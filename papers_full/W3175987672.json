{
  "title": "Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level",
  "url": "https://openalex.org/W3175987672",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2767776798",
      "name": "Ruiqi Zhong",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2227982659",
      "name": "Dhruba Ghosh",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2056960045",
      "name": "Dan Klein",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2071240084",
      "name": "Jacob Steinhardt",
      "affiliations": [
        "University of California, Berkeley"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3034491534",
    "https://openalex.org/W3099624838",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3034255912",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W3008851394",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W2110065044",
    "https://openalex.org/W3034340181",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W3100511085",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2251485414",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3021336872",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963508788",
    "https://openalex.org/W3098467034",
    "https://openalex.org/W3001279689"
  ],
  "abstract": "Larger language models have higher accuracy on average, but are they better on every single instance (datapoint)?Some work suggests larger models have higher out-ofdistribution robustness, while other work suggests they have lower accuracy on rare subgroups.To understand these differences, we investigate these models at the level of individual instances.However, one major challenge is that individual predictions are highly sensitive to noise in the randomness in training.We develop statistically rigorous methods to address this, and after accounting for pretraining and finetuning noise, we find that our BERT-LARGE is worse than BERT-MINI on at least 1-4% of instances across MNLI, SST-2, and QQP, compared to the overall accuracy improvement of 2-10%.We also find that finetuning noise increases with model size, and that instance-level accuracy has momentum: improvement from BERT-MINI to BERT-MEDIUM correlates with improvement from BERT-MEDIUM to BERT-LARGE .Our findings suggest that instance-level predictions provide a rich source of information; we therefore recommend that researchers supplement model weights with model predictions.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3813–3827\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3813\nAre Larger Pretrained Language Models Uniformly Better? Comparing\nPerformance at the Instance Level\nRuiqi Zhong Dhruba Ghosh Dan Klein Jacob Steinhardt\nComputer Science Division, University of California, Berkeley\n{ruiqi-zhong, djghosh13, klein, jsteinhardt}@berkeley.edu\nAbstract\nLarger language models have higher accu-\nracy on average, but are they better on ev-\nery single instance (datapoint)? Some work\nsuggests larger models have higher out-of-\ndistribution robustness, while other work sug-\ngests they have lower accuracy on rare sub-\ngroups. To understand these differences, we\ninvestigate these models at the level of indi-\nvidual instances. However, one major chal-\nlenge is that individual predictions are highly\nsensitive to noise in the randomness in train-\ning. We develop statistically rigorous meth-\nods to address this, and after accounting for\npretraining and ﬁnetuning noise, we ﬁnd that\nour BERT-LARGE is worse than BERT- MINI\non at least 1 −4% of instances across MNLI,\nSST-2, and QQP, compared to the overall ac-\ncuracy improvement of 2 −10%. We also\nﬁnd that ﬁnetuning noise increases with model\nsize, and that instance-level accuracy has mo-\nmentum: improvement from BERT- MINI to\nBERT-MEDIUM correlates with improvement\nfrom BERT-MEDIUM to BERT-LARGE . Our\nﬁndings suggest that instance-level predictions\nprovide a rich source of information; we there-\nfore recommend that researchers supplement\nmodel weights with model predictions.\n1 Introduction\nHistorically, large deep learning models (Peters\net al., 2018; Devlin et al., 2019; Lewis et al., 2020;\nRaffel et al., 2019) have improved the state of\nthe art on a wide range of tasks and leaderboards\n(Schwartz et al., 2014; Rajpurkar et al., 2016; Wang\net al., 2018), and empirical scaling laws predict\nthat larger models will continue to increase per-\nformance (Kaplan et al., 2020). However, little is\nunderstood about such improvement at the instance\n(datapoint) level. Are larger models uniformly bet-\nter? In other words, are larger pretrained models\nbetter at every instance, or are they better at some\ninstances, but worse at others?\nPrior works hint at differing answers. Hendrycks\net al. (2020) and Desai and Durrett (2020) ﬁnd\nthat larger pretrained models consistently improve\nout-of-distribution performance, which implies that\nthey might be uniformly better at a ﬁner level.\nHenighan et al. (2020) claim that larger pretrained\nimage models have lower downstream classiﬁca-\ntion loss for the majority of instances, and they\npredict this trend to be true for other data modal-\nities (e.g. text). On the other hand, Sagawa et al.\n(2020) ﬁnd that larger non-pretrained models per-\nform worse on rare subgroups; if this result gener-\nalizes to pretrained language models, larger models\nwill not be uniformly better. Despite all the in-\ndirect evidence, it is still inconclusive how many\ninstances larger pretrained models perform worse\non.\nA na¨ıve solution is to ﬁnetune a larger model,\ncompare it to a smaller one, and ﬁnd instances\nwhere the larger model is worse. However, this\napproach is ﬂawed, since model predictions are\nnoisy at the instance level. On MNLI in-domain\ndevelopment set, even the same architecture with\ndifferent ﬁnetuning seeds leads to different pre-\ndictions on ∼8% of the instances. This is due to\nunder-speciﬁcation (D’Amour et al., 2020), where\nthere are multiple different solutions that can mini-\nmize the training loss. Since the accuracy improve-\nment from our BERT-BASE 1 to BERT-LARGE is\n2%, most signals across different model sizes will\nbe dominated by noise due to random seeds.\nTo account for the noise in pretraining and ﬁne-\ntuning, we deﬁne instance accuracyas “how often\na model correctly predicts an instance” (Figure 1\nleft) in expectation across pretraining and ﬁnetun-\ning seeds. We estimate this quantity by pretraining\n10 models with different seeds, ﬁnetuning 5 times\nfor each pretrained models (Figure 1 middle), and\n1This is not the original release by Devlin et al. (2019); we\npretrained models ourselves.\n3814\nSeed 1Seed 2Seed 3Instance AccuracyInstance 1X ✓ ✓ 66%Instance 2X X X 0%Instance 3✓ X X 33%Instance 4✓ ✓ ✓ 100%\nAverage across seeds\nF(*)1F(*)2\nP1 X✓P2 ✓XP3 X✓\nFinetuning Seeds\nPretrain-ing SeedsMINI \nLARGE\nInstance 1Instance i\nModelSizes\n… ……… …✓✓✓X✓✓\nX✓✓XXX\nXX✓X✓✓\nInstances\nFigure 1: Left: Each column represents the same architecture trained with a different seed. We calculate accuracy\nfor each instance (row) by averaging across seeds (column), while it is usually calculated for each model by\naveraging across instances. Middle: A visual layout of the model predictions we obtain, which is a binary-valued\ntensor with 4 axes: model size s, instance i, pretraining seeds P and ﬁnetuning seeds F. Right: for each instance,\nwe calculate the accuracy gain from MINI to LARGE and plot the histogram in blue, along with a random baseline\nin red. Since the blue distribution has a bigger left tail, smaller models are better at some instances.\naveraging across them.\nHowever, this estimate is still inexact, and we\nmight falsely observe smaller models to be better\nat some instances by chance. Hence, we propose\na random baseline to estimate the fraction of false\ndiscoveries (Section 3, Figure 1 right) and formally\nupper-bound the false discoveries in Section 4. Our\nmethod provides a better upper bound than the clas-\nsical Benjamini-Hochberg procedure with Fisher’s\nexact test.\nUsing the 50 models for each size and our im-\nproved statistical tool, we ﬁnd that, on the MNLI\nin-domain development set, the accuracy “decays”\nfrom BERT-LARGE to BERT-MINI on at least ∼4%\nof the instances, which is signiﬁcant given that the\nimprovement in overall accuracy is 10%. These\ndecaying instances contain more controversial or\nwrong labels, but also correct ones (Section 4.2).\nTherefore, larger pretrained language models are\nnot uniformly better.\nWe make other interesting discoveries at the in-\nstance level. Section 5 ﬁnds that instance-level\naccuracy has momentum: improvement from MINI\nto MEDIUM correlates with improvement from\nMEDIUM to LARGE . Additionally, Section 6 at-\ntributes variance of model predictions to pretrain-\ning and ﬁnetuning random seeds, and ﬁnds that\nﬁnetuning seeds cause more variance for larger\nmodels. Our ﬁndings suggest that instance-level\npredictions provide a rich source of information;\nwe therefore recommend that researchers supple-\nment model weights with model predictions. In this\nspirit, we release all the pretrained models, model\npredictions, and code here: https://github.com/\nruiqi-zhong/acl2021-instance-level .\n2 Data, Models, and Predictions\nTo investigate model behavior, we considered dif-\nferent sizes of the BERT architecture and ﬁne-\ntuned them on Quora Question Pairs (QQP 2),\nMulti-Genre Natural Language Inference (MNLI;\nWilliams et al. (2020)), and the Stanford Sen-\ntiment Treebank (SST-2; Socher et al. (2013)).\nTo account for pretraining and ﬁnetuning noise,\nwe averaged over multiple random initializations\nand training data order, and thus needed to pre-\ntrain our own models rather than downloading\noff the internet. Following Turc et al. (2019) we\ntrained 5 architectures of increasing size: MINI\n(L4/H256, 4 Layers with hidden dimension 256),\nSMALL (L4/H512), MEDIUM (L8/H512), BASE\n(L12/H768), and LARGE (L24/H1024). For each\narchitecture we pre-trained models with 10 differ-\nent random seeds and ﬁne-tuned each of them 5\ntimes (50 total) on each task; see Figure 1 middle.\nSince pretraining is computationally expensive, we\nreduced the context size during pretraining from\n512 to 128 and compensated by increasing train-\ning steps from 1M to 2M. Appendix A includes\nmore details about pretraining and ﬁnetuning and\ntheir computational cost, and Appendix B veriﬁes\nthat our cost-saving changes do not affect accuracy\nqualitatively.\nNotation. We use i to index an instance in the\nevaluation set, sfor model sizes, P for pretraining\nseeds and F for ﬁnetuning seeds. cis a random\nvariable of value 0 or 1 to indicate whether the\nprediction is correct. Given the pretraining seed P\nand the ﬁnetuning seed F, cs\ni = 1 if the model of\n2https://www.quora.com/q/quoradata/First-Quora-\nDataset-Release-Question-Pairs\n3815\nDiffFTune DiffPTrain Stdall\nMINI 7.2% 10.7% 0.2%\nSMALL 7.2% 10.7% 0.3%\nMEDIUM 8.0% 10.7% 0.3%\nBASE 8.5% 10.6% 0.2%\nLARGE 8.6% 10.1% 0.2%\nTable 1: Larger model sizes are at the bottom rows.\nDiffFTune: how much do the predictions differ, if two\nmodels have the same pretraining seed but different\nﬁnetuning seeds F? Diff PTrain: the difference if the\npretraining seeds P are different. Std all: the standard\ndeviation of overall accuracy, around 40 times smaller\nthan DiffFTune.\nsize sis correct on instance i, 0 otherwise. To keep\nthe notation uncluttered, we sometimes omit these\nsuperscripts or subscripts if they can be inferred\nfrom context.\nUnless otherwise noted, we present results on\nthe MNLI in-domain development set in the main\npaper.\n3 Comparing Instance Accuracy\nTo ﬁnd the instances where larger models are worse,\na na¨ıve approach is to ﬁnetune a larger pretrained\nmodel, compare it to a smaller one, and ﬁnd in-\nstances where the larger is incorrect but the smaller\nis correct. Under this approach, BERT-LARGE is\nworse than BERT-BASE on 4.5% of the instances\nand better on 7%, giving an overall accuracy im-\nprovement of 2.5%.\nHowever, this result is misleading: even if we\ncompare two BERT- BASE model with different\nﬁnetuning seeds, their predictions differ on 8% of\nthe instances, while their accuracies differ only by\n0.1%; Table 1 reports this baseline randomness\nacross model sizes. Changing the pretraining seed\nalso changes around 2% additional predictions be-\nyond ﬁnetuning.\nTable 1 also reports the standard deviation of\noverall accuracy, which is about 40 times smaller.\nSuch stability starkly contrasts with the noisiness at\nthe instance level, which poses a unique challenge.\nInstance-Level Metrics To reﬂect this noisiness,\nwe deﬁne the instance accuracyAccs\ni to be how\noften models of size spredict instance icorrectly,\nAccs\ni := EP,F [cs\ni ]. (1)\nThe expectation is taken with respect to the pre-\ntraining and ﬁnetuning randomness P and F. We\nestimate Accs\ni via the empirical average ˆAcc\ns\ni ac-\ncross 10 pretraining ×5 ﬁnetuning runs.\nWe histogram ˆAcc\ns\ni in Figure 2 (a). On most\ninstances the model always predicts correctly or\nincorrectly ( ˆAcc = 0 or 1), but a sizable fraction\nof accuracies lie between the two extremes.\nRecall that our goal is to ﬁnd instances where\nlarger models are less accurate, which we refer\nto as decaying instances. We therefore study the\ninstance differencebetween two model sizess1 and\ns2, deﬁned as\ns1\ns2 ∆Acci := Accs2\ni −Accs1\ni , (2)\nwhich is estimated by the difference between the\naccuracy estimates ˆAcc\ns\ni , i.e.\ns1\ns2\nˆ∆Acci := ˆAcc\ns2\ni − ˆAcc\ns1\ni . (3)\nWe histogram BASE\nLARGE\nˆ∆Acci in Figure 2 (b). We\nobserve a unimodal distribution centered near 0,\nwith tails on both sides. Therefore, the estimated\ndifferences for some instances are negative.\nHowever, due to estimation noise, we might\nfalsely observe this accuracy decay by chance.\nTherefore, we introduce a random baseline\ns1\ns2 ∆Acc′to control for these false discoveries. Re-\ncall that we have 10 smaller pretrained models and\n10 larger ones. Our baseline splits these into a\ngroup Aof 5 smaller + 5 larger, and another group\nBof the remaining 5 + 5. Then the empirical accu-\nracies ˆAcc\nA\nand ˆAcc\nB\nare identically distributed,\nso we take our baseline s1\ns2 ∆Acc′to be the differ-\nence ˆAcc\nA\n− ˆAcc\nB\n. We visualize and compare\nhow to calculate s1\ns2\nˆ∆Acc and s1\ns2 ∆Acc′in Figure 3.\nWe histogram this baseline BASE\nLARGE ∆Acc′ in\nFigure 2 (b), and ﬁnd that our noisy estimate\nBASE\nLARGE\nˆ∆Acc has a larger left tail than the baseline.\nThis suggests that decaying instances exist. We\nsimilarly compare MINI to LARGE in Figure 2 (c)\nand ﬁnd an even larger left tail.\n4 Quantifying the Decaying Instances\nThe left tail of ˆ∆Acc noisily estimates the frac-\ntion of decaying instances, and the left tail of the\nrandom baseline ∆Acc′ counts the false discov-\nery fraction due to the noise. Intuitively, the true\nfraction of decaying instances can be captured by\nthe difference of these left tails, and we formally\nquantify this below.\n3816\n(a) BASE vs. LARGE , Acc\n (b) BASE vs. LARGE , ∆Acc\n (c) MINI vs. LARGE , ∆Acc\nFigure 2: (a) The distribution of instance accuracy ˆAcci. (b, c) Histogram of instance difference estimate (x-axis),\nˆ∆Acc (blue) and its baseline ∆Acc′ (red) compares BASE and LARGE . To better visualize, we truncated the\ndensity (y-axis) above 2. Since the blue histogram has a larger left tail than the red one, there are indeed instances\nwhere larger models are worse.\nF(*)1F(*)2 F(*)3\nP1 X✓✓P2 ✓XXP3 XXXP4 X✓✓\nMINI\nPretrain-ing Seeds\n̂AccA=0.58\n̂AccB=0.58MiniLargeΔAcc′\u0000 =0=\n̂AccMINI=0.42̂AccLARGE=0.75F(*)1F(*)2 F(*)3\nP1 X✓✓P2 ✓✓XP3 ✓✓XP4 ✓✓✓\nLARGE - MINILARGE ̂ΔAcc=.33=\n-\nGroup A\nGroup B\nFigure 3: The tables are model predictions with visual\nnotations established in Figure 1 middle. ˆ∆Acc (blue)\nis the mean difference between the left and the right\ntable, each corresponding to a model size. The random\nbaseline ∆Acc′ (red) is the mean difference between\ngroup A(orange) cells and group B(green), which are\nidentically and independently distributed.\nSuppose instance iis drawn from the empirical\nevaluation distribution. Then we can deﬁne the true\ndecaying fraction Decay as\nDecay := Pi[∆Acci <0]. (4)\nSince ∆Acci is not directly observable and\nˆ∆Acci is noisy, we add a buffer and only consider\ninstances with ˆ∆Acci ≤t, which makes it more\nlikely (but still uncertain) that the true ∆Acci <0.\nWe denote this “discovery fraction” ˆDecay(t) as\nˆDecay(t) := Pi[ ˆ∆Acci ≤t]. (5)\nSimilarly, we deﬁne a baseline control (false\ndiscovery fraction) Decay′(t) := Pi[∆Acc′\ni ≤t].\nHence, ˆDecay and Decay′are the cumulative dis-\ntribution function of ˆ∆Acc and ∆Acc′(Figure 4).\nWe have the following theorem, which we for-\nmally state and prove in Appendix D:\nTheorem 1 (Informal) If all the random seeds are\nindependent, then for all thresholdst,\nDecay ≥E[ ˆDecay(t) −Decay′(t)] (6)\nProof Sketch Suppose we observe cs1\nR1...2k\nand\ncs2\nR2k+1...4k\n, where there are 2k different random\nseeds for each model size 3. Then\nˆ∆Acci := 1\n2k(\n2k∑\nj=1\ncs1\nRj,i −\n4k∑\nj=2k+1\ncs2\nRj,i), (7)\nand hence the discovery rate ˆDecay(t) is deﬁned\nas\nˆDecay(t) := 1\n|T|\n|T|∑\ni=1\n1[ ˆ∆Acc ≤t]. (8)\nFor the random baseline estimator, we have\n∆Acc′\ni := 1\n2k(\nk∑\nj=1\ncs1\nRj,i +\n3k∑\nj=2k+1\ncs2\nRj,i (9)\n−\n2k∑\nj=k+1\ncs1\nRj,i −\n4k∑\nj=3k+1\ncs2\nRj,i),\nand the false discovery controlDecay′is deﬁned\nas\nDecay′(t) := 1\n|T|\n|T|∑\ni=1\n1[∆Acc′\ni ≤t]. (10)\nFormally, the theorem states that\nDecay ≥ER1...R4k[ ˆDecay(t)−Decay′(t)], (11)\nwhich is equivalent to\n|T|∑\ni=1\n(1[∆Acci <0] −P[ ˆ∆Acci ≤t] (12)\n+P[∆Acc′\ni ≤t]) ≥0\n3We assumed even number of random seeds since we will\nmix half of the models from each size to compute the random\nbaseline\n3817\n6%\nFigure 4: The cumulative distribution function of the\nhistogram in Figure 2 (c); only the negative x-axis is\nshown because it corresponds to decays. The maxi-\nmum difference between the two curves (6%) is a lower\nbound of the true decaying fraction.\nHence, we can declare victory if we can prove\nthat for all i, if ∆Acci ≥0,\nP[∆Acc′\ni ≤t] ≥P[ ˆ∆Acci ≤t].\nThis is easy to see, since ∆Acc′\ni and ˆ∆Acci are\nboth binomial distributions with the same n, but\nthe ﬁrst has a larger rate. 4 □\nRoughly speaking, the true decaying fraction\nis at least the difference between ˆDecay(t) and\nDecay′(t) at every threshold t. Therefore, we take\nthe maximum difference between ˆDecay(t) and\nDecay′(t) to lower-bound the fraction of decaying\ninstances.5 For example, Figure 4 estimates the\ntrue decaying fraction between MINI and LARGE\nto be at least 6%.\nWe compute this lower bound for other pairs of\nmodel sizes in Table 2, and the full results across\nother tasks and model size pairs are in Appendix C.\nIn all of these settings we ﬁnd a non-zero fraction\nof decaying instances, and larger model size differ-\nences usually lead to more decaying instances.\nUnfortunately, applying Theorem 1 as above\nis not fully rigorous, since some ﬁnetuning runs\nshare the same pretraining seeds and hence are de-\npendent.6 To obtain a statistically rigorous lower\nbound, we slightly modify our target of interest. In-\nstead of examining individual ﬁnetuning runs, we\nensemble our model across 5 different ﬁnetuning\nruns for each pretraining seed; these predictions\nare essentially the same as individual ﬁnetuning\nruns, except that the ﬁnetuning randomness is av-\neraged out. Hence we obtain 10 independent sets\n4More details are in Appendix D.\n5Adaptively picking the best threshold t depending on the\ndata may incur a slight upward bias. Appendix E estimates\nthat the relative bias is at most 10% using a bootstrap method.\n6Although we anticipate such dependencies do not cause a\nsubstantial difference, as discussed in Appendix D.1.\ns1 \\s2 MINI SMALL BASE LARGE\nMINI N/A 9% 18% 21%\nSMALL 3% N/A 14% 18%\nBASE 6% 5% N/A 10%\nLARGE 6% 5% 2% N/A\nTable 2: We lower-bound the fraction of instances that\nimprove when model size changes from s1 (row) to s2\n(column). For example, when model size decreases\nfrom LARGE to MINI , 6% of instances improve (i.e.\ndecays).\nThreshold ˆDecay Decay ′ Diff\nt= −0.4 4.22% 3.49e−3 3.87%\n. . . . . . . . . . . .\nt= −0.9 0.91% 1.44e−7 0.91%\nt= −1.0 0.48% 2.06e−8 0.48%\nTable 3: Comparing MINI vs. LARGE by calculating\nthe discovery fraction ˆDecay, the false discovery con-\ntrol Decay′, and their difference (Diff) under different\nthresholds t. LARGE is worse on at least ∼4% (maxi-\nmum Diff) of instances.\nof model predictions with different random seeds,\nwhich allows us to apply Theorem 1.\nWe compare MINI to LARGE using these ensem-\nbles and report the discovery ˆDecay and the base-\nline Decay′in Table 3. Taking the maximum differ-\nence across thresholds, we estimate at least∼4% of\ndecaying instances. This estimate is lower than the\nprevious 6% estimate, which used the full set of 50\nmodels’ predictions assuming they were indepen-\ndent. However, this is still a meaningful amount,\ngiven that the overall accuracy improvement from\nMINI to LARGE is 10%.\n4.1 Fisher’s Test + Benjamini-Hochberg\nHere is a more classical approach to lower-bound\nthe decaying fraction. For each instance, we com-\npute a signiﬁcance levelαunder the null hypothesis\nthat the larger model is better, using Fisher’s exact\ntest. We sort the signiﬁcance levels ascendingly,\nand call the pth percentile αp. Then we pick a\nfalse discovery rate q(say, 25%), ﬁnd the largest\nps.t. αp <pq, and estimate the decaying fraction\nto be at least p(1 −q). This calculation is known\nas the Benjamini-Hochberg procedure (Benjamini\nand Hochberg, 1995).\nTo compare our method with this classical ap-\nproach, we estimate the lower bound of the decay-\ning fraction for different pairs of model sizes with\ndifferent numbers of pretrained models available.\n3818\ns1 s2 2 6 10\nMINI LARGE ours 1.9% 3.1% 4.0%\nMINI LARGE BH 0.0% 0.9% 1.9%\nBASE LARGE ours 0.4% 0.9% 1.2%\nBASE LARGE BH 0.0% 0.0% 0.0%\nTable 4: We compare our method to the Fisher’s exact\ntest + Benjamin-Hochberg (BH) procedure described in\nSection 4. For all different model size pairs and number\nof pretrained models available, ours always provides a\nhigher (better) lower bound of the decaying fraction.\nTo make sure our choice of the false discovery rate\nqdoes not bias against the classical approach, we\nadaptively choose qto maximize its performance.\nAppendix F includes the full results and Table 4 is\na representative subset.\nWe ﬁnd that our approach is more powerful, par-\nticularly when the true decaying fraction is likely\nto be small and only a few models are available,\nwhich is usually the regime of interest. For exam-\nple, across all pairs of model sizes, our approach\nonly needs 2 random seeds (i.e. pretrained models)\nto provide a non-zero lower bound on the decaying\nfraction, while the classical approach sometimes\nfails to do this even with 10 seeds. Intuitively, when\nfewer seeds are available, the smallest possible sig-\nniﬁcance level for each instance is larger than the\ndecaying fraction, hence hurting the classical ap-\nproach.\n4.2 Understanding the Decaying Instances\nWe next manually examine the decaying instances\nto see whether we can ﬁnd any interpretable pat-\nterns. One hypothesis is that all the decaying frac-\ntions are in fact mislabeled, and hence larger mod-\nels are not in fact worse on any instances.\nTo investigate this hypothesis, we examined the\ngroup of instances where MINI\nLARGE\nˆ∆Acci ≤ −0.9.\nMINI is almost always correct on these instances,\nwhile LARGE is almost always wrong, and the false\ndiscovery fraction is tiny. For each instance, we\nmanually categorize it as either: 1) Correct, if the\nlabel is correct, 2) Fine, if the label might be con-\ntroversial but we could see a reason why this label\nis reasonable, 3) Wrong, if the label is wrong, or\n4) Unsure, if we are unsure about how to label\nthis instance. Each time we annotate, with 50%\nprobability we randomly sample either a decaying\ninstance or an instance from the remaining dataset\nas a control. We are blind to which group it comes\nfrom.\nCorrect Fine Wrong Unsure\nMNLID 66% 17% 9% 5%\nMNLIC 86% 5% 5% 1%\nSST-2D 55% 8% 10% 25%\nSST-2C 88% 4% 0% 6%\nQQPD 60% 26% 10% 2%\nQQPC 87% 10% 1% 0%\nTable 5: MINI vs. LARGE . We examine whether there\nare mislabels for theDecaying fractions (superscript D)\nand the rest of the dataset ( Control group C). The de-\ncaying fraction contains more mislabels, but includes\ncorrect labels as well.\nFor each task of MNLI, QQP, and SST-2, the ﬁrst\nauthor annotated 100 instances (decay + control\ngroup) (Table 5). We present all the annotated\ndecaying instances in Appendix J.\nConclusion We ﬁnd that the decaying fraction\nhas more wrong or controversial labels, compared\nto the remaining instances. However, even after\nwe adjust for the fraction of incorrect labels, the\nDecay fraction still exceeds the false discovery\ncontrol. This implies that MINI models are bet-\nter than LARGE models on some correctly labeled\ninstances. The second author followed the same\nprocedure and reproduced the same qualitative re-\nsults.\nHowever, we cannot ﬁnd an interpretable pattern\nfor these correctly labeled decaying instances by\nsimply eyeballing. We discuss future directions to\ndiscover interpretable categories in Section 7.\n5 Correlation of Instance Difference\nWe next investigate whether there is a momen-\ntum of instance accuracy increase: for example,\nif the instance accuracy improves from MINI (s1)\nto MEDIUM (s2), is it more likely to improve from\nMEDIUM (s2) to LARGE (s3)?\nThe na¨ıve approach is to calculate the Pearson\ncorrelation coefﬁcient between MINI\nMEDIUM\nˆ∆Acc and\nMEDIUM\nLARGE\nˆ∆Acc, and we ﬁnd the correlation to be zero.\nHowever, this is partly an artifact of accuracies be-\ning bounded in [0,1]. If MEDIUM drastically im-\nproves over MINI from 0 to 1, there is no room for\nLARGE to improve over MEDIUM . To remove this\ninherent negative correlation, we calculate the cor-\nrelation conditioned on the accuracy of the middle-\nsized model, ˆAcc\nMEDIUM\n.\nTherefore, we bucket instances by their esti-\nmated MEDIUM accuracy into intervals of size 0.1,\n3819\n(s1,s2,s3) ↓Buckets→ 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00\nSMALL ,MEDIUM ,BASE 0.07 0.22 0.29 0.40 0.35 0.33 0.38 0.27 0.24 0.13\nMINI ,MEDIUM ,LARGE 0.03 0.15 0.18 0.33 0.17 0.16 0.22 0.20 0.19 0.09\nTable 6: Each row corresponds to a triplet of model sizes. Each column t represents a bucket that contains\ninstances with ˆAcc\ns2\n∈[t−0.1,t]. Within each bucket, we calculate the Pearson correlation coefﬁcient between\nthe estimated accuracy improvements: s1\ns2\nˆ∆Acc and s2\ns3\nˆ∆Acc. These correlations are positive and become higher\nwhen model size differences are small.\nand we ﬁnd the correlation to be positive within\neach bucket (Table 6, row 2). This ﬁxes the prob-\nlem with the na¨ıve approach by getting rid of the\nnegative correlation, which could have misled us\nto believe that improvements by larger models are\nuncorrelated.\nWe additionally ﬁnd that the correlations be-\ntween improvements become stronger when model\nsize differences are smaller. Table 6 row 1 re-\nports results for another model size triplet with\nsmaller size difference, i.e. (s1, s2, s3) = (SMALL ,\nMEDIUM , BASE ), and the correlation is larger for\nall buckets. Results for more tasks and size triplets\nare in Appendix G and the same conclusions hold\nqualitatively.\n6 Variance at the Instance Level\nSection 3 found that the overall accuracy has rela-\ntively low variance, but model predictions are noisy.\nThis section formally analyzes variance at the in-\nstance level. For each instance, we decompose its\nloss into three components: Bias2, variance due to\npretraining randomness, and variance due to ﬁne-\ntuning randomness. Formally, we consider the 0/1\nloss:\nLi := 1 −ci = (1 −ci)2, (13)\nwhere ci is a random variable 0/1 indicating\nwhether the prediction is correct or incorrect, with\nrespect to randomness in pretraining and ﬁnetun-\ning. Therefore, by bias-variance decomposition\nand total variance decomposition, we have\nLi = Bias2i + PretVari + FineVari, (14)\nwhere, by using P and F as pretraining and ﬁne-\ntuning random seeds:\nBias2i := (1 −EP,F [ci])2, (15)\nPretVari := VarP [EF [ci]],\nFineVari := EP [VarF [ci]],\ncapturing “how wrong is the average prediction”,\nvariance due to pretraining, and variance due to\nﬁnetuning seeds, respectively.\nBias2 PretVar FineVar\nMINI 0.203 0.017 0.036\nSMALL 0.179 0.017 0.036\nMEDIUM 0.157 0.014 0.040\nBASE 0.134 0.010 0.043\nLARGE 0.111 0.007 0.043\nTable 7: The bias, pretraining variance, and ﬁnetuning\nvariance for each model size, averaged across all test\ninstances. Finetuning variance is much larger than pre-\ntraining variance; larger models have larger ﬁnetuning\nvariance.\nWe can directly estimate FineVar by ﬁrst calcu-\nlating the sample variance across ﬁnetuning runs\nfor each pretraining seed, and then averaging the\nvariances across the pretraining seeds. Estimating\nPretVar is more complicated. A na¨ıve approach is\nto calculate the empirical variance, across pretrain-\ning seeds, of the average accuracy across ﬁnetuning\nseeds. However, the estimated average accuracy for\neach pretraining seed is noisy itself, which causes\nan upward bias on the PretVar estimate. We cor-\nrect this bias by estimating the variance of the esti-\nmated average accuracy and subtracting it from the\nna¨ıve estimate; see Appendix H for details, as well\nas a generalization to more than two sources of ran-\ndomness. Finally, we estimateBias2 by subtracting\nthe two variance estimates from the estimated loss.\nFor each of these three quantities, Bias2,\nPretVar and FineVar, we estimate it for each in-\nstance, average it across all instances in the evalua-\ntion set, and report it in Table 7. The variances at\nthe instance level are much larger than the variance\nof overall accuracy, by a factor of 1000.\nWe may conclude from Table 7 that larger mod-\nels have larger ﬁnetuning variance and smaller pre-\ntraining variance. However, lower bias also inher-\nently implies lower variance. To see this, suppose\na model has perfect accuracy and hence zero bias;\nthen it always predicts the same label (the correct\none) and hence has zero variance. This might favor\nlarger models and “underestimate” their variance,\n3820\nFigure 5: The pretraining variance conditioned on\nBias2 (the level of correctness). Each curve represents\na model size. Larger models have lower pretraining\nvariance across all levels of bias.\nsince they have lower bias. Therefore, we calculate\nand compare the variances conditioned on the bias,\ni.e. PretVar(b2) := Ei[PretVari|Bias2i = b2].\nWe estimate PretVars(b2) using Gaussian pro-\ncess regression and plot it against b2 in Figure 5.\nWe ﬁnd that larger models still have lower pre-\ntraining variance across all levels of bias on the\nspeciﬁc task of MNLI under the 0/1 loss. To fur-\nther check whether our conclusions are general, we\ntested them on other tasks and under the squared\nloss Li := (1 −pi)2, where pi is the probability\nassigned to the correct class. Below are the conclu-\nsions that generally hold across different tasks and\nloss functions.\nConclusion We ﬁnd that 1) larger models have\nlarger ﬁnetuning variance, 2) LARGE has smaller\npretraining variance than BASE ; however, the or-\ndering between other sizes varies across tasks and\nlosses, and 3) ﬁnetuning variance is 2−8 times as\nlarge as pretraining variance, and the ratio is bigger\nfor larger models.\n7 Discussion and Future Directions\nTo investigate model behaviors at the instance level,\nwe produced massive amounts of model predictions\nin Section 2 and treated them as raw data. To ex-\ntract insights from them, we developed better met-\nrics and statistical tools, including a new method\nto control the false discoveries, an unbiased estima-\ntor for the decomposed variances, and metrics that\ncompute variance and correlation of improvements\nconditioned on instance accuracy. We ﬁnd that\nlarger pretrained models are indeed worse on a non-\ntrivial fraction of instances and have higher vari-\nance due to ﬁnetuning seeds; additionally, instance\naccuracy improvements from MINI to MEDIUM cor-\nrelate with improvements from MEDIUM to LARGE\n.\nOverall, we treated model prediction data as the\ncentral object and built analysis tools around them\nto obtain a ﬁner understanding of model perfor-\nmance. We therefore refer to this paradigm as\n“instance level understanding as data mining ”.\nWe discuss three key factors for this paradigm to\nthrive: 1) scalability and the cost of obtaining pre-\ndiction data, 2) other information to collect for each\ninstance, and 3) better statistical tools. We analyze\neach of these aspects below.\nScalability and Cost of Data Data mining is\nmore powerful with more data. How easy is it\nto obtain more model predictions? In our paper,\nthe main bottleneck is pretraining. However, once\nthe pretrained models are released, individual re-\nsearchers can download them and only need to\nrepeat the cheaper ﬁnetuning procedure.\nFurthermore, model prediction data are under-\nshared: while many recent research papers share\ntheir code or even model weights to help reproduce\nthe results, it is not yet a standard practice to share\nall the model predictions. Since many researches\nfollow almost the same recipe of pretraining and\nﬁnetuning (McCoy et al., 2020; Desai and Durrett,\n2020; Dodge et al., 2020), much computation can\nbe saved if model predictions are shared. On the\nother hand, as the state of the art model size is\nincreasing at a staggering speed7, most researchers\nwill not be able to run inference on a single instance.\nThe trend that models are becoming larger and\nmore similar necessitate more prediction sharing.\nMeta-Labels and Other Predictions Data min-\ning is more powerful with more types of informa-\ntion. One way to add information to each instance\nis to assign “meta-labels”. In the HANS (McCoy\net al., 2019) dataset, the authors tag each instance\nwith a heuristic 8 that holds for the training distri-\nbution but fails on this instance. Naik et al. (2018a)\nand Ribeiro et al. (2020) associate each instance\nwith a particular stress test type or subgroup, for ex-\nample, whether the instance requires the model to\nreason numerically or handle negations. Nie et al.\n7e.g. BERT (Devlin et al., 2019) has 340M parameters,\nwhile Switch-Transformer has over 1 trillion parameters (Fe-\ndus et al., 2021).\n8For example, “the label [entailment] is likely if the\npremise and the hypothesis have signiﬁcant lexical overlap”.\n3821\n(2020) collects multiple human responses to esti-\nmate human disagreement for each instance. This\nmeta-information can potentially help us identify\ninterpretable patterns for the disagreeing instances\nwhere one model is better than the other. On the\nﬂip side, identifying disagreeing instances between\ntwo models can also help us generate hypothesis\nand decide what subgroup information to annotate.\nWe can also add performance information on\nother tasks to each instance. For example, Pruk-\nsachatkun et al. (2020) studied the correlation\nbetween syntactic probing accuracy (Hewitt and\nLiang, 2019) and downstream task performance.\nTurc et al. (2019) and Kaplan et al. (2020) studied\nthe correlation between language modelling loss\nand the downstream task performance. However,\nthey did not analyze correlations at the instance\nlevel. We may investigate whether their results\nhold on the instance level: if an instance is easier\nto tag by a probe or easier to predict by a larger\nlanguage model, is the accuracy likely to be higher?\nStatistical Tools Data mining is more powerful\nwith better statistical tools. Initially we used the\nBenjamini-Hochberg procedure with Fisher’s ex-\nact test, which required us to pretrain 10 models\nto formally verify that the decaying instances ex-\nist. However, we later realized that 2 is in fact\nenough by using our approach introduced in Sec-\ntion 4. We could have saved 80% of the compu-\ntation for pretraining if this approach was known\nbefore we started.\nFuture work can explore more complicated met-\nrics and settings. We compared at most 3 different\nmodel sizes at a time, and higher order comparisons\nrequire novel metrics. We studied two sources of\nrandomness, pretraining and ﬁnetuning, but other\nsources of variation can be interesting as well, e.g.\ndifferences in pretraining corpus, different model\ncheckpoints, etc. To deal with more sophisticated\nmetrics, handle different sources and hierarchies of\nrandomness, and reach conclusions that are robust\nto noises at the instance level, researchers need to\ndevelop new inference procedures.\nTo conclude, for better instance level understand-\ning, we need to produce and share more prediction\ndata, annotate more diverse linguistic properties,\nand develop better statistical tools to infer under\nnoises. We hope our work can inform researchers\nabout the core challenges underlying instance level\nunderstanding and inspire future work.\nAcknowledgement\nWe thank Steven Cao, Cathy Chen, Frances Ding,\nDavid Gaddy, Colin Li, and Alex Wei for giving\ncomments on the initial paper draft. We would also\nlike to thank the Google Cloud TPU team for their\nhardware support.\nReferences\nYoav Benjamini and Yosef Hochberg. 1995. Control-\nling the false discovery rate: a practical and pow-\nerful approach to multiple testing. Journal of the\nRoyal statistical society: series B (Methodological),\n57(1):289–300.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nAlexander D’Amour, Katherine Heller, Dan Moldovan,\nBen Adlam, Babak Alipanahi, Alex Beutel,\nChristina Chen, Jonathan Deaton, Jacob Eisen-\nstein, Matthew D Hoffman, et al. 2020. Un-\nderspeciﬁcation presents challenges for credibil-\nity in modern machine learning. arXiv preprint\narXiv:2011.03395.\nShrey Desai and Greg Durrett. 2020. Calibration of\npre-trained transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 295–302, Online.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv\npreprint arXiv:2101.03961.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\n3822\npages 2744–2751, Online. Association for Computa-\ntional Linguistics.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun,\nTom B Brown, Prafulla Dhariwal, Scott Gray, et al.\n2020. Scaling laws for autoregressive generative\nmodeling. arXiv preprint arXiv:2010.14701.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nWuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017.\nA continuously growing dataset of sentential para-\nphrases. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1224–1234, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin,\nKurt Keutzer, Dan Klein, and Joseph E. Gonzalez.\n2020. Train large, then compress: Rethinking model\nsize for efﬁcient training and inference of transform-\ners. In International Conference on Machine Learn-\ning.\nR. Thomas McCoy, Junghyun Min, and Tal Linzen.\n2020. BERTs of a feather do not generalize to-\ngether: Large variability in generalization across\nmodels with similar test set performance. In Pro-\nceedings of the Third BlackboxNLP Workshop on An-\nalyzing and Interpreting Neural Networks for NLP,\npages 217–227, Online. Association for Computa-\ntional Linguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018a.\nStress test evaluation for natural language inference.\nIn The 27th International Conference on Computa-\ntional Linguistics (COLING), Santa Fe, New Mex-\nico, USA.\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018b.\nStress test evaluation for natural language inference.\nIn Proceedings of the 27th International Conference\non Computational Linguistics, pages 2340–2353,\nSanta Fe, New Mexico, USA. Association for Com-\nputational Linguistics.\nYixin Nie, Xiang Zhou, and Mohit Bansal. 2020. What\ncan we learn from collective human opinions on nat-\nural language inference data? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9131–9143,\nOnline. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nYada Pruksachatkun, Jason Phang, Haokun Liu,\nPhu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe\nPang, Clara Vania, Katharina Kann, and Samuel R.\nBowman. 2020. Intermediate-task transfer learning\nwith pretrained language models: When and why\ndoes it work? In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5231–5247, Online. Association for\nComputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902–\n4912, Online. Association for Computational Lin-\nguistics.\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and\nPercy Liang. 2020. An investigation of why over-\nparameterization exacerbates spurious correlations.\n3823\nIn Proceedings of the 37th International Conference\non Machine Learning, volume 119 of Proceedings\nof Machine Learning Research, pages 8346–8356.\nPMLR.\nLane Schwartz, Timothy Anderson, Jeremy Gwinnup,\nand Katherine Young. 2014. Machine translation\nand monolingual postediting: The AFRL WMT-\n14 system. In Proceedings of the Ninth Workshop\non Statistical Machine Translation, pages 186–194,\nBaltimore, Maryland, USA. Association for Compu-\ntational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962v2.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAdina Williams, Tiago Pimentel, Hagen Blix, Arya D.\nMcCarthy, Eleanor Chodroff, and Ryan Cotterell.\n2020. Predicting declension class from form and\nmeaning. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6682–6695, Online. Association for Computa-\ntional Linguistics.\n3824\nSummary\nIn this 4-page appendix, we include 1) details on\nmodel training and the datasets we used 2) high-\nlevel ideas underlying the theoretical results, and 3)\nrelatively more important discussions. We also\nhave a longer version of this appendix on our\ngithub, which contains the full proofs, discussions,\nplots, and tables.\nA Pretraining and Finetuning Details\nTo obtain model predictions under the “pretraining\nand ﬁnetuning” framework (Devlin et al., 2019), we\nneed to decide a model size, perform pretraining,\nﬁnetune on a training set with a choice of hyper-\nparameters, and test the model on an evaluation\nset. We discuss each bolded aspects below.\nSize Similar to Turc et al. (2019), we exper-\nimented with the following ﬁve model sizes,\nlisted in increasing order: MINI (L4/H256) 9 ,\nSMALL (L4/H512), MEDIUM (L8/H512), BASE\n(L12/H768), and LARGE (L24/H1024).\nPretraining We used the pretraining code from\nDevlin et al. (2019) and the pre-training corpus\nfrom Li et al. (2020). Compared to the original\nBERT release, we used context size 128 instead of\n512, since computation cost grows quadratically\nwith respect to context size; we also pretrained for\n2M steps instead of 1M.\nTraining Set We consider 3 datasets: Quora\nQuestion Pairs (QQP) 10, Multi-Genre Natural Lan-\nguage Inference (MNLI; Williams et al. (2020)),\nand the Stanford Sentiment Treebank (SST-2;\n(Socher et al., 2013)). For QQP we used the of-\nﬁcial training split. For MNLI we used 350K out\nof 400K instances from the original training split,\nand added the remaining 50K to the evaluation set,\nsince the original in-domain development set only\ncontains 10K examples. For SST-2, we mix the\ntraining and development set of the original split,\nsplit the instances into 5 folds, train on four of them,\nand evaluate on the remaining fold.\nHyperparameters As in Turc et al. (2019), we\nﬁnetune 4 epochs for each dataset. For each task\nand model size, we tune hyperparameters in the\nfollowing way: we ﬁrst randomly split our new\ntraining set into 80% and 20%; then we ﬁnetune on\n94 Layers with hidden dimension 256\n10https://www.quora.com/q/quoradata/First-Quora-\nDataset-Release-Question-Pairs\nthe 80% split with all 9 combination of batch size\n[16, 32, 64] and learning rate [1e-4, 5e-5, 3e-5],\nand choose the combination that leads to the best\naverage accuracy on the remaining 20%.\nEvaluation Set After ﬁnetuning our pretrained\nmodels, we evaluate them on a range of in-domain,\nout-of-domain, or challenging datasets to obtain\nmodel predictions. Models trained on MNLI are\nalso evaluated on Stanford Natural Language In-\nference (SNLI; (Bowman et al., 2015)), Heuristic\nAnalysis for NLI Systems (HANS; (McCoy et al.,\n2019)), and stress test evaluations (STRESS; (Naik\net al., 2018b)). Models trained on QQP are also\nevaluated on Twitter Paraphrase Database (Twit-\nterPPDB; (Lan et al., 2017)).\nSince pretraining introduces randomness, for\neach model size s, we pretrain 10 times with dif-\nferent random seed P; since ﬁnetuning also intro-\nduces noise, for each pretrained model we pretrain\n5 times with different random seed F; besides, we\nalso evaluate the model at the checkpoints after E\nepochs, where E ∈[3,31\n3 ,32\n3 ,4].\nPretraining 10 models for all 5 model sizes alto-\ngether takes around 3840 hours on TPU v3 with 8\ncores. Finetuning all of them 5 times for all three\ntasks in our paper requires around 1200 hours.\nB Compare Our Models to the Original\nSince we decreased the pre-training context length\nto save computation, these models are not exactly\nthe same as the original BERT release by Devlin\net al. (2019) and Turc et al. (2019). We need to\nbenchmark our model against theirs to ensure that\nthe performance of our model is still reasonable\nand the qualitative trend still holds. For each each\nsize and task, we ﬁnetune the original model 5\ntimes and calculate the average of overall accuracy.\nThe comparison can be seen in Table 8. We ﬁnd\nthat our model does not substantially differ from\nthe original ones on QQP and SST-2. On MNLI,\nthe performance of our BERT- BASE and BERT-\nLARGE is 2∼3% below the original release, but\nthe qualitative trend that larger models have better\naccuracy still holds robustly.\nC More Instance Difference Results\nSimilar to Figure 4, for all 10 pairs of model sizes\nand all in-distribution instances of MNLI, SST-2,\nand QQP, we plot the cumulative density of ˆ∆Acc\nand ∆Acc′, or say, ˆDecay(t) and Decay′(t). See\nthe long appendix for the ﬁgures.\n3825\nQQP MNLI SST-2\nMINI orig 88.2% 74.6% 92.8%\nMINI ours 87.3% 74.3% 92.8%\nSMALL orig 89.1% 77.3% 93.9%\nSMALL ours 88.7% 76.7% 93.9%\nMEDIUM orig 89.8% 79.6% 94.2%\nMEDIUM ours 89.5% 78.9% 94.2%\nBASE orig 90.8% 83.8% 95.0%\nBASE ours 90.6% 81.2% 94.6%\nLARGE orig 91.3% 86.8% 95.2%\nLARGE ours 91.0% 83.8% 94.8%\nTable 8: Comparing our pretrained model (superscript\norig) to the original release by Devlin et al. (2019) and\nTurc et al. (2019) (superscript ours). All pretrained\nmodels are ﬁnetuned with the training set and tested on\nthe in-distribution evaluation set described in Appendix\nA.\nAdditionally, for each pair of model sizes s1 and\ns2, we estimate “how much instances are getting\nbetter/worse accuracy?” by taking the maximum\ndifference between the red curve and the blue curve.\nWe report these results for MNLI, SST-2, and QQP\nin Table 9. We ﬁnd that larger model size gaps\nlead to larger decaying fraction, but also larger\nimproving fraction as well.\nD Proof of Theorem 1\nFormal Setup Suppose each instance is indexed\nby i, the set of all instances is T, and the random\nseed is R; then cs\nR ∈{0,1}|T| is a random |T|\ndimensional vector, where cs\nR,i = 1 if the model of\nsize scorrectly predicts instance iunder the ran-\ndom seed R. We are comparing model size s1 and\ns2, where s2 is larger; to keep notation uncluttered,\nwe omit these indexes whenever possible.\nSuppose we observe cs1\nR1...2k\nand cs2\nR2k+1...4k\n,\nwhere there are 2kdifferent random seeds for each\nmodel size 11. Then\nˆ∆Acci := 1\n2k(\n2k∑\nj=1\ncs1\nRj,i −\n4k∑\nj=2k+1\ncs2\nRj,i), (16)\nand hence the discovery rate ˆDecay(t) is deﬁned\nas\nˆDecay(t) := 1\n|T|\n|T|∑\ni=1\n1[ ˆ∆Acc ≤t]. (17)\n11We assumed even number of random seeds since we will\nmix half of the models from each size to compute the random\nbaseline\nFor the random baseline estimator, we have\n∆Acc′\ni := 1\n2k(\nk∑\nj=1\ncs1\nRj,i +\n3k∑\nj=2k+1\ncs2\nRj,i (18)\n−\n2k∑\nj=k+1\ncs1\nRj,i −\n4k∑\nj=3k+1\ncs2\nRj,i),\nand the false discovery controlDecay′is deﬁned\nas\nDecay′(t) := 1\n|T|\n|T|∑\ni=1\n1[∆Acc′\ni ≤t]. (19)\nFormally, theorem states that\nDecay ≥ER1...R4k[ ˆDecay(t) −Decay′(t)] (20)\nProof By re-arranging terms and linearity of ex-\npectation, Equation 20 is equivalent to the follow-\ning\n|T|∑\ni=1\n(1[∆Acci <0] −P[ ˆ∆Acci ≤t] (21)\n+P[∆Acc′\ni ≤t]) ≥0\nHence, we can declare victory if we can prove\nthat for all i,\n1[∆Acci <0] −P[ ˆ∆Acci ≤t] (22)\n+P[∆Acc′\ni ≤t] ≥0\nTo prove Equation 22, we observe that ifAcci <\n0, since the probabilities are bounded by 0 and 1,\nits left-hand side must be positive. Therefore, we\nonly need to prove that\n∆Acci ≥0 (23)\n⇒P[∆Acc′\ni ≤t] ≥P[ ˆ∆Acci ≤t],\nwhich will be proved in Lemma 1.□\nLemma 1\n∆Acci ≥0 (24)\n⇒P[∆Acc′\ni ≤t] ≥P[ ˆ∆Acci ≤t],\nFor m= 1,2, deﬁne\npsm\ni := ER[csm\ni ], (25)\nthen\nps1\ni ≤ps2\ni (26)\n3826\nMNLI s1 \\s2 MINI SMALL MEDIUM BASE LARGE\nMINI 0.000 0.087 0.136 0.179 0.214\nSMALL 0.033 0.000 0.089 0.139 0.180\nMEDIUM 0.050 0.028 0.000 0.090 0.143\nBASE 0.060 0.048 0.026 0.000 0.101\nLARGE 0.059 0.052 0.040 0.021 0.000\nQQP s1 \\s2 MINI SMALL MEDIUM BASE LARGE\nMINI 0.000 0.057 0.076 0.100 0.107\nSMALL 0.019 0.000 0.039 0.073 0.084\nMEDIUM 0.029 0.014 0.000 0.044 0.063\nBASE 0.034 0.027 0.016 0.000 0.032\nLARGE 0.036 0.031 0.027 0.016 0.000\nSST-2 s1 \\s2 MINI SMALL MEDIUM BASE LARGE\nMINI 0.000 0.037 0.043 0.052 0.057\nSMALL 0.010 0.000 0.015 0.031 0.036\nMEDIUM 0.016 0.008 0.000 0.020 0.028\nBASE 0.019 0.014 0.009 0.000 0.014\nLARGE 0.020 0.017 0.015 0.008 0.000\nTable 9: On QQP, MNLI in domain development set and SST-2 we lowerbound the fraction of instances that\nimproves when model size changes from s1 (row) to s2 (column).\nSince cs1\ni and cs2\ni are both Bernoulli random vari-\nables with rate ps1\ni and ps2\ni respectively, we can\nwrite down the probability distribution of ˆ∆Acci\nand ∆Acc′\ni as the sum/difference of several bino-\nmial variables, i.e.\nˆ∆Acci ∼(Binom(k,ps2\ni ) + Binom(k,ps2\ni ) (27)\n−Binom(k,ps1\ni ) −Binom(k,ps1\ni ))/2k,\nand\n∆Acc′i ∼(Binom(k,ps2,i) + Binom(k,ps1,i)\n(28)\n−Binom(k,ps1,i) −Binom(k,ps2,i))/2k\nps1\ni ≤ps2\ni , Binom(k,ps2,i)) ﬁrst order stochas-\ntically dominates Binom(k,ps1,i). Therefore,\n∆Acc′i dominates ˆ∆Acci, hence completing the\nproof. □\nD.1 Independent Seed Assumption\nSee the long appendix for discussion.\nE Upward Bias of Adaptive Thresholds\nIn section 3 we picked the best threshold that can\nmaximize the lowerbound, which can incur a slight\nupward bias. Here we estimate that the bias is at\nmost 10% relative to the unbiased lowerbound with\na bootstrapping method.\nWe use the empirical distribution of 10 pre-\ntrained models as the ground truth distribution for\nbootstrapping. We ﬁrst compute a best threshold\nwith 10 sampled smaller and larger pretrained mod-\nels, and then compute the lowerbound Lwith this\nthreshold on another sample of 10 smaller and\nlarger models. Intuitively, we use one bootstrap\nsample (which contains 10 smaller pretrained mod-\nels and 10 larger pretrained models) as the devel-\nopment set to “tune the threshold”, and then use\nthis threshold on a fresh bootstrap sample to com-\npute the lowerbound. We refer to the lowerbound\nthat uses the best threshold as L∗, and compute the\nrelative error E[(L∗−L)]/E[L)], where the expec-\ntation is taken with respect to bootstrap samples.\nSee the long appendix for more detailed results.\nIn general, we ﬁnd that the upward bias is negligi-\nble, which is at most around 10%.\nF Comparison with Signiﬁcance Testing\nSee the long appendix for the Table that compares\nour procedure with the classical method that uses\nthe Fisher’s exact test and Benjamini-Hochberg\nprocedure for other tasks and model size pairs.\nIn general, we ﬁnd that our method always pro-\nvide a tighter (higher) lowerbound than the classical\nmethod, and 2 models are sufﬁcient to verify the\nexistence (i.e. lowerbound >0) of the decaying\nfraction; in contrast, the classical method some-\n3827\ntimes fails to do this even with 10 models, e.g.,\nwhen comparing BASE to LARGE .\nIntuitively, our approach provides a better lower-\nbound because it better makes use of the infor-\nmation that on most instances, both the smaller\nand the larger models agree and predict completely\ncorrectly or incorrectly12. For an extreme exam-\nple, suppose we only observe 2 smaller models\nand 2 larger models, and inﬁnite number of dat-\napoints, whose predictions are independent. On\n99.98% datapoints, both models have instance ac-\ncuracy 1; on 0.01% datapoints, smaller model is\ncompletely correct while bigger completely wrong,\nwhile on the rest 0.01% smaller completely wrong\nbut bigger completely correct. Setting threshold\nto be 2, our decay estimate ˆDecay is 0.01%, while\nDecay′ = 0: since the models either completely\npredict correct or wrongly, there is never a false\ndiscovery. Therefore, our method can provide the\ntightest lowerbound 0.01% in this case. On the\nother hand, since we only have 4 models in total,\nthe lowest signiﬁcance-level given by the ﬁsher ex-\nact test is 17% ≫0.1%, hence the discovery made\nby the Benjamin-Hochberg procedure is 0.\nG More Results on Momentum\nSee the long appendix for correlations with other\nmodel size triplets on other tasks.\nH Loss Decomposition and Estimation\nThe core of the PretVar estimator builds on the\nfollowing theorem:\nTheorem 2 Suppose Dk,k ∈ [F] are indepen-\ndently sampled from the same distributionΞ, which\nis a distribution of distributions;ˆµk is an unbiased\nestimator ofEX∈Dk[X], andˆφk to be an unbiased\nestimator of the variance ofˆµk, then\nˆVarF = 1\nF− 1\n∑\nk∈[F]\n(ˆµk −ˆµ)2 (29)\n−1\nF\n∑\nk∈[F]\nˆφk\nis an unbiased estimator for\nV = VarD∼Ξ[EX∼D[X]], (30)\n12This is for intuition, though, and we do not need any\nassumption on the prior of instance accuracy, which requires\na Bayes interpretation.\nwhere\nˆµ:= 1\nF\n∑\nk∈[F]\nˆµk (31)\nIn this estimator, the ﬁrst term “pretends” that ˆµ·\nare perfect estimator for the population mean and\ncalculate the variance, while the second term cor-\nrects for the fact that the empirical mean estimation\nis not perfect. Notice the theorem only requires\nthat ˆµand ˆφare unbiased, and is agnostic to the\nactual computation procedure by these estimators.\nTo estimate PretVar, we need ˆµk and ˆφk. The\nﬁrst term is the empirical accuracy for each pre-\ntraining seed; the second is an unbiased estimator\nof the variance of empirical accuracy for each pre-\ntraining seed, which can be estimated by the sam-\nple variance of ﬁnetuning divided by the number\nof ﬁnetuning runs.\nSee the long Appendix for a more detailed proof,\nand also how to generalize this estimator to esti-\nmate arbitrary level of variance decomposition.\nI Variance Conditioned on Bias\nSee the long appendix for more ﬁgures.\nJ Example Decaying Instances\nHere we present some random annotated decaying\ninstances on MNLI.\nPremise : and that you’re very much right\nbut the jury may or may not see it that way so you\nget a little anticipate you know anxious there and\ngo well you know\nHypothesis : Jury’s operate without the beneﬁt of\nan education in law.\nLabel : Neutral, Category : Correct\nPremise : In ﬁscal year 2000, it reported\nestimated improper Medicare Fee-for-Service\npayments of $11.\nHypothesis : The payments were improper.\nLabel : Entailment, Category : Fine\nPremise : INTEREST RATE - The price\ncharged per unit of money borrowed per year,\nor other unit of time, usually expressed as a\npercentage.\nHypothesis : Interest rate is deﬁned as the total\namount of money borrowed.\nLabel : Entailment, Category : Wrong",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7348346710205078
    },
    {
      "name": "Language model",
      "score": 0.5563675761222839
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4767873287200928
    },
    {
      "name": "Natural language processing",
      "score": 0.47563862800598145
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ],
  "cited_by": 23
}