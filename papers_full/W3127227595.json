{
  "title": "Measuring and Improving Consistency in Pretrained Language Models",
  "url": "https://openalex.org/W3127227595",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4286749036",
      "name": "Elazar, Yanai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282188277",
      "name": "Kassner, Nora",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224706977",
      "name": "Ravfogel, Shauli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287442545",
      "name": "Ravichander, Abhilasha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2186945576",
      "name": "Hovy, Eduard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2764041618",
      "name": "Schütze, Hinrich",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221504774",
      "name": "Goldberg, Yoav",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2978491132",
    "https://openalex.org/W2970746059",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2950733407",
    "https://openalex.org/W2953039212",
    "https://openalex.org/W2798935874",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W3037363161",
    "https://openalex.org/W2962684341",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2010497813",
    "https://openalex.org/W3098613713",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W3035267217",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2985797697",
    "https://openalex.org/W2950761309",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W3033176962",
    "https://openalex.org/W3100879603",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W1514391388",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W1972647152",
    "https://openalex.org/W3014784959",
    "https://openalex.org/W2016172157",
    "https://openalex.org/W2945531126",
    "https://openalex.org/W2605717780",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2891012317",
    "https://openalex.org/W2799007037",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W3104100020",
    "https://openalex.org/W1532097860",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W3092932788",
    "https://openalex.org/W2914795094",
    "https://openalex.org/W3127729136",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3116216579",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2971600926",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2949849869",
    "https://openalex.org/W2108505829",
    "https://openalex.org/W3153457449",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2143206572",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3110879614",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2108061372",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3034649382",
    "https://openalex.org/W3159900299"
  ],
  "abstract": "Consistency of a model -- that is, the invariance of its behavior under meaning-preserving alternations in its input -- is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel, we show that the consistency of all PLMs we experiment with is poor -- though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.",
  "full_text": "Measuring and Improving Consistency in Pretrained Language Models\nYanai Elazar1,2 Nora Kassner3 Shauli Ravfogel1,2 Abhilasha Ravichander4\nEduard Hovy4 Hinrich Schütze3 Yoav Goldberg1,2\n1Computer Science Department, Bar Ilan University\n2Allen Institute for Artiﬁcial Intelligence\n3Center for Information and Language Processing (CIS), LMU Munich\n4Language Technologies Institute, Carnegie Mellon University\n{yanaiela,shauli.ravfogel,yoav.goldberg}@gmail.com\nkassner@cis.lmu.de {aravicha,hovy}@cs.cmu.edu\nAbstract\nConsistency of a model — that is, the in-\nvariance of its behavior under meaning-\npreserving alternations in its input — is\na highly desirable property in natural lan-\nguage processing. In this paper we study\nthe question: Are Pretrained Language Mod-\nels (PLMs) consistent with respect to fac-\ntual knowledge? To this end, we create\nPARA REL\n , a high-quality resource of\ncloze-style query English paraphrases. It\ncontains a total of 328 paraphrases for 38\nrelations. Using PARA REL\n , we show that\nthe consistency of all PLMs we experiment\nwith is poor – though with high variance be-\ntween relations. Our analysis of the represen-\ntational spaces of PLMs suggests that they\nhave a poor structure and are currently not\nsuitable for representing knowledge robustly.\nFinally, we propose a method for improv-\ning model consistency and experimentally\ndemonstrate its effectiveness.1\n1 Introduction\nPretrained Language Models (PLMs) are large neu-\nral networks that are used in a wide variety of\nNLP tasks. They operate under a pretrain-ﬁnetune\nparadigm: models are ﬁrst pretrained over a large\ntext corpus and then ﬁnetuned on a downstream\ntask. PLMs are thought of as good language en-\ncoders, supplying basic language understanding\ncapabilities that can be used with ease for many\ndownstream tasks.\nA desirable property of a good language under-\nstanding model is consistency: the ability to make\nconsistent decisions in semantically equivalent con-\ntexts, reﬂecting a systematic ability to generalize\nin the face of language variability.\nExamples of consistency include: predicting\nthe same answer in question answering and read-\n1The code and resource are available at: https://\ngithub.com/yanaiela/pararel.\nHomeland originally aired on[MASK]\nShowtime\n⇕Inconsistent\nHomeland premiered on[MASK]\nABC\nSeinfeld originally aired on[MASK]\nNBC\n⇕Consistent\nSeinfeld premiered on[MASK]\nNBC\nFigure 1: Overview of our approach. We expect\nthat a consistent model would predict the same\nanswer for two paraphrases. In this example, the\nmodel is inconsistent on the Homeland and consis-\ntent on the Seinfeld paraphrases.\ning comprehension tasks regardless of paraphrase\n(Asai and Hajishirzi, 2020); making consistent as-\nsignments in coreference resolution (Denis and\nBaldridge, 2009; Chang et al., 2011); or making\nsummaries factually consistent with the original\ndocument (Kryscinski et al., 2020). While consis-\ntency is important in many tasks, nothing in the\ntraining process explicitly targets it. One could\nhope that the unsupervised training signal from\nlarge corpora made available to PLMs such as\nBERT or RoBERTa (Devlin et al., 2019; Liu et al.,\n2019) is sufﬁcient to induce consistency and trans-\nfer it to downstream tasks. In this paper, we show\nthat this is not the case.\nThe recent rise of PLMs has sparked a discus-\nsion about whether these models can be used as\nKnowledge Bases (KBs) (Petroni et al., 2019, 2020;\nDavison et al., 2019; Peters et al., 2019; Jiang et al.,\n2020; Roberts et al., 2020). Consistency is a key\nproperty of KBs and is particularly important for\nautomatically constructed KBs. One of the biggest\nappeals of using a PLM as a KB is that we can\nquery it in natural language – instead of relying on a\nspeciﬁc KB schema. The expectation is that PLMs\nabstract away from language and map queries in\nnatural language into meaningful representations\narXiv:2102.01017v2  [cs.CL]  29 May 2021\nsuch that queries with identical intent but different\nlanguage forms yield the same answer. For exam-\nple, the query “Homeland premiered on [MASK]”\nshould produce the same answer as “ Homeland\noriginally aired on [MASK]”. Studying inconsisten-\ncies of PLM-KBs can also teach us about the orga-\nnization of knowledge in the model or lack thereof.\nFinally, failure to behave consistently may point\nto other representational issues such as the simi-\nlarity between antonyms and synonyms (Nguyen\net al., 2016), and overestimating events and actions\n(reporting bias) (Shwartz and Choi, 2020).\nIn this work, we study the consistency of factual\nknowledge in PLMs, speciﬁcally in Masked Lan-\nguage Models (MLM) – these are PLMs trained\nwith the MLM objective (Devlin et al., 2019; Liu\net al., 2019), as opposed to other strategies such as\nstandard language modeling (Radford et al., 2019)\nor text-to-text (Raffel et al., 2020). We ask: Is the\nfactual information we extract from PLMs invari-\nant to paraphrasing? We use zero-shot evaluation\nsince we want to inspect models directly, without\nadding biases through ﬁnetuning. This allows us to\nassess how much consistency was acquired during\npretraining and to compare the consistency of dif-\nferent models. Overall, we ﬁnd that the consistency\nof the PLMs we consider is poor, although there is\na high variance between relations.\nWe introduce PARA REL\n , a new benchmark\nthat enables us to measure consistency in PLMs\n(§3), by using factual knowledge that was found to\nbe partially encoded in them (Petroni et al., 2019;\nJiang et al., 2020). PARA REL\n is a manually\ncurated resource that provides patterns – short tex-\ntual prompts – that are paraphrases of one another,\nwith 328 paraphrases describing 38 binary rela-\ntions such as X born-in Y, X works-for Y (§4). We\nthen test multiple PLMs for knowledge consistency,\ni.e., whether a model predicts the same answer\nfor all patterns of a relation. Figure 1 shows an\noverview of our approach. Using PARA REL\n , we\nprobe for consistency in four PLM types: BERT,\nBERT-whole-word-masking, RoBERTa and AL-\nBERT (§5). Our experiments with PARA REL\nshow that current models have poor consistency,\nalthough with high variance between relations (§6).\nFinally, we propose a method that improves\nmodel consistency by introducing a novel consis-\ntency loss (§8). We demonstrate that trained with\nthis loss, BERT achieves better consistency perfor-\nmance on unseen relations. However, more work\nis required to achieve fully consistent models.\n2 Background\nThere has been signiﬁcant interest in analyzing\nhow well PLMs (Rogers et al., 2020) perform on\nlinguistic tasks (Goldberg, 2019; Hewitt and Man-\nning, 2019; Tenney et al., 2019; Elazar et al., 2021),\ncommonsense (Forbes et al., 2019; Da and Kasai,\n2019; Zhang et al., 2020) and reasoning (Talmor\net al., 2020; Kassner et al., 2020), usually assessed\nby measures of accuracy. However, accuracy is just\none measure of PLM performance (Linzen, 2020).\nIt is equally important that PLMs do not make con-\ntradictory predictions (cf. Figure 1), a type of error\nthat humans rarely make. There has been relatively\nlittle research attention devoted to this question,\ni.e., to analyze if models behave consistently. One\nexample concerns negation: Ettinger (2020) and\nKassner and Schütze (2020) show that models tend\nto generate facts and their negation, a type of incon-\nsistent behavior. Ravichander et al. (2020) propose\npaired probes for evaluating consistency. Our work\nis broader in scope, examining the consistency of\nPLM behavior across a range of factual knowledge\ntypes and investigating how models can be made\nto behave more consistently.\nConsistency has also been highlighted as a de-\nsirable property in automatically constructed KBs\nand downstream NLP tasks. We now brieﬂy review\nwork along these lines.\nConsistency in knowledge bases (KBs) has\nbeen studied in theoretical frameworks in the con-\ntext of the satisﬁability problem and KB construc-\ntion, and efﬁcient algorithms for detecting incon-\nsistencies in KBs have been proposed (Hansen\nand Jaumard, 2000; Andersen and Pretolani, 2001).\nOther work aims to quantify the degree to which\nKBs are inconsistent and detects inconsistent state-\nments (Thimm, 2009, 2013; Muiño, 2011).\nConsistency in question answering was stud-\nied by Ribeiro et al. (2019) in two tasks: visual\nquestion answering (Antol et al., 2015) and read-\ning comprehension (Rajpurkar et al., 2016). They\nautomatically generate questions to test the consis-\ntency of QA models. Their ﬁndings suggest that\nmost models are not consistent in their predictions.\nIn addition, they use data augmentation to create\nmore robust models. Alberti et al. (2019) generate\nnew questions conditioned on context and answer\nfrom a labeled dataset and by ﬁltering answers that\ndo not provide a consistent result with the origi-\nnal answer. They show that pretraining on these\nsynthetic data improves QA results. Asai and Ha-\njishirzi (2020) use data augmentation that comple-\nments questions with symmetricity and transitivity,\nas well as a regularizing loss that penalizes incon-\nsistent predictions. Kassner et al. (2021b) propose\na method to improve accuracy and consistency of\nQA models by augmenting a PLM with an evolving\nmemory that records PLM answers and resolves\ninconsistency between answers.\nWork on consistency in other domains in-\ncludes (Du et al., 2019) where prediction of con-\nsistency in procedural text is improved. Ribeiro\net al. (2020) use consistency for more robust evalu-\nation. Li et al. (2019) measure and mitigate incon-\nsistency in natural language inference (NLI), and\nﬁnally, Camburu et al. (2020) propose a method\nfor measuring inconsistencies in NLI explanations\n(Camburu et al., 2018).\n3 Probing PLMs for Consistency\nIn this section, we formally deﬁne consistency and\ndescribe our framework for probing consistency of\nPLMs.\n3.1 Consistency\nWe deﬁne a model as consistent if, given\ntwo cloze-phrases such as “ Seinfeld originally\naired on [MASK]” and “ Seinfeld premiered on\n[MASK]” that arequasi-paraphrases, it makes non-\ncontradictory predictions2 on N-1 relations over a\nlarge set of entities. A quasi-paraphrase – a con-\ncept introduced by Bhagat and Hovy (2013) – is\na more fuzzy version of a paraphrase. The con-\ncept does not rely on the strict, logical deﬁnition of\nparaphrase and allows to operationalize concrete\nuses of paraphrases. This deﬁnition is in the spirit\nof the RTE deﬁnition (Dagan et al., 2005), which\nsimilarly supports a more ﬂexible use of the notion\nof entailment. For instance, a model that predicts\nNBC and ABC on the two aforementioned patterns,\nis not consistent, since these two facts are contra-\ndictory. We deﬁne acloze-pattern as a cloze-phrase\nthat expresses a relation between a subject and an\nobject. Note that consistency does not require the\nanswers to be factually correct. While correctness\nis also an important property for KBs, we view it as\n2We refer to non-contradictory predictions as predictions\nthat, as the name suggest, do not contradict one another. For\ninstance, predicting as the birth place of a person two differ-\nence cities is considered to be contradictory, but predicting a\ncity and its country, is not.\na separate objective and measure it independently.\nWe use the termsparaphrase and quasi-paraphrase\ninterchangeably.\nMany-to-many (N-M) relations (e.g. shares-\nborder-with) can be consistent even with different\nanswers (given they are correct). For instance, two\npatterns that express the shares-border-with rela-\ntion and predict Albania and Bulgaria for Greece\nare both correct. We do not consider such rela-\ntions for measuring consistency. However, another\nrequirement from a KB is determinism, i.e., return-\ning the results in the same order (when more than\na single result exists). In this work, we focus on\nconsistency, but also measure determinism of the\nmodels we inspect.\n3.2 The Framework\nAn illustration of the framework is presented in\nFigure 2. Let Di be a set of subject-object KB\ntuples (e.g., <Homeland, Showtime>) from some\nrelation ri (e.g., originally-aired-on), accompanied\nwith a set of quasi-paraphrases cloze-patterns Pi\n(e.g., X originally aired on Y). Our goal is to test\nwhether the model consistently predicts the same\nobject (e.g., Showtime) for a particular subject (e.g.,\nHomeland).3 To this end, we substitute X with a\nsubject from Di and Y with [MASK] in all of the\npatterns Pi of that relation (e.g., Homeland origi-\nnally aired on [MASK] and Homeland premiered\non [MASK]). A consistent model must predict the\nsame entity.\nRestricted Candidate Sets Since PLMs were\nnot trained for serving as KBs, they often predict\nwords that are not KB entities; e.g., a PLM may\npredict, for the pattern “Showtime originally aired\non [MASK]”, the noun ‘tv’ – which is also a likely\nsubstitution for the language modeling objective,\nbut not a valid KB fact completion. Therefore,\nfollowing (Xiong et al., 2020; Ravichander et al.,\n2020; Kassner et al., 2021a), we restrict the PLMs’\noutput vocabulary to the set of possible gold ob-\njects for each relation from the underlining KB. For\nexample, in the born-in relation, instead of inspect-\ning the entire vocabulary of a model, we only keep\nobjects from the KB, such as Paris, London, Tokyo,\netc.\nNote that this setup makes the task easier for the\n3Although it is possible to also predict the subject from\nthe object, in the cases of N-1 relations more than a single\nanswer would be possible, making it impossible to test for\nconsistency, but determinism.\n(D1, r1, P1), . . . ,(Di, ri, Pi), . . . ,(Dn, rn, Pn)\nri =originally-aired-on\nHomeland originally aired on[MASK]\nHomeland premiered on[MASK]\n. . .\nSeinfeld originally aired on[MASK]\nSeinfeld premiered on[MASK]\nData Pairs (D)\n(Lou Reed, Brooklyn)\n(Masako Natsume, Tokyo). . .\n. . .\n(Seinfeld, NBC )\n(Homeland, Showtime). . .\n. . .\nD1\nDi\nPatterns (P)\n(X was born inY )\n(X is native toY ). . .\n. . .\n(X originally aired inY )\n(X premiered onY ). . .\n. . .\nP1\nPi\nFigure 2: Overview of our framework for assessing model consistency. Di (“Data Pairs (D)” on the left)\nis a set of KB triplets of some relation ri, which are coupled with a set of quasi-paraphrase cloze-patterns\nPi (“Patterns (P)” on the right) that describe that relation. We then populate the subjects from Di as well\nas a mask token into all patterns Pi (shown in the middle) and expect a model to predict the same object\nacross all pattern pairs.\nPLM, especially in the context of KBs. However,\npoor consistency in this setup strongly implies that\nconsistency would be even lower without restrict-\ning candidates.\n4 The P ARA REL\n Resource\nWe now describePARA REL\n , a resource designed\nfor our framework (cf. Section 3.2).PARA REL\n is\ncurated by experts, with a high level of agreement.\nIt contains patterns for 38 relations 4 from T-REx\n(Elsahar et al., 2018) — a large dataset containing\nKB triples aligned with Wikipedia abstracts — with\nan average of 8.63 patterns per relation. Table 1\ngives statistics. We further analyse the paraphrases\nused in this resource, partly based on the types\ndeﬁned in Bhagat and Hovy (2013), and report this\nanalysis in Appendix B.\nConstruction Method PARA REL\n was con-\nstructed in four steps. (1) We began with the pat-\nterns provided by LAMA (Petroni et al., 2019) (one\npattern per relation, referred to as base-pattern).\n(2) We augmented each base-pattern with other pat-\nterns that are paraphrases from LPAQA (Jiang et al.,\n2020). However, since LPAQA was created auto-\nmatically (either by back-translation or by extract-\ning patterns from sentences that contain both sub-\nject and object), some LPAQA patterns are not cor-\nrect paraphrases. We therefore only include the sub-\nset of correct paraphrases. (3) Using SPIKE (Shlain\n4using the 41 relations from LAMA (Petroni et al., 2019),\nleaving out three relations that are poorly deﬁned, or consist\nof mixed and heterogeneous entities.\net al., 2020), 5 a search engine over Wikipedia\nsentences that supports syntax-based queries, we\nsearched for additional patterns that appeared in\nWikipedia and added them to PARA REL\n . Specif-\nically, we searched for Wikipedia sentences con-\ntaining a subject-object tuple from T-REx and then\nmanually extracted patterns from the sentences. (4)\nLastly, we added additional paraphrases of the base-\npattern using the annotators’ linguistic expertise.\nTwo additional experts went over all the patterns\nand corrected them, while engaging in a discussion\nuntil reaching agreement, discarding patterns they\ncould not agree on.\nHuman Agreement To assess the quality of\nPARA REL\n , we run a human annotation study.\nFor each relation, we sample up to ﬁve paraphrases,\ncomparing each of the new patterns to the base-\npattern from LAMA. That is, if relation ri contains\nthe following patterns: p1,p2,p3,p4, and p1 is the\nbase-pattern, then we compare the following pairs\n(p1,p2),(p1,p3),(p1,p4).\nWe populate the patterns with random subjects\nand objects pairs from T-REx (Elsahar et al., 2018)\nand ask annotators if these sentences are para-\nphrases. We also sample patterns from different re-\nlations to provide examples that are not paraphrases\nof each other, as a control. Each task contains ﬁve\npatterns that are thought to be paraphrases and two\nthat are not.6 Overall, we collect annotations for\n5https://spike.apps.allenai.org/\n6The controls contain the same subjects and objects so that\nonly the pattern (not its arguments) can be used to solve the\ntask.\n# Relations 38\n# Patterns 328\nMin # patterns per rel. 2\nMax # patterns per rel. 20\nAvg # patterns per rel. 8.63\nAvg syntax 4.74\nAvg lexical 6.03\nTable 1: Statistics of PARA REL\n . Last two rows:\naverage number of unique syntactic/lexical varia-\ntions of patterns for a relation.\n156 paraphrase candidates and 61 controls.\nWe asked NLP graduate students to annotate the\npairs and collected one answer per pair.7 The agree-\nment scores for the paraphrases and the controls\nare 95.5% and 98.3%, which is high and indicates\nPARA REL\n ’s high quality. We also inspected the\ndisagreements and ﬁxed many additional problems\nto further improve quality.\n5 Experimental Setup\n5.1 Models & Data\nWe experiment with four PLMs: BERT, BERT\nwhole-word-masking8 (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019) and ALBERT (Lan\net al., 2019). For BERT, RoBERTa and ALBERT,\nwe use a base and a large version.9 We also report\na majority baseline that always predicts the most\ncommon object for a relation. By construction, this\nbaseline is perfectly consistent.\nWe use knowledge graph data from T-REx (Elsa-\nhar et al., 2018).10 To make the results comparable\nacross models, we remove objects that are not repre-\nsented as a single token in all models’ vocabularies;\n26,813 tuples remain.11 We further split the data\ninto N-M relations for which we report determin-\nism results (seven relations) and N-1 relations for\nwhich we report consistency (31 relations).\n7We asked the annotators to re-annotate any mismatch with\nour initial label, to allow them to ﬁx random mistakes.\n8BERT whole-word-masking is BERT’s version where\nwords that are tokenized into multiple tokens are masked\ntogether.\n9For ALBERT we use the smallest and largest versions.\n10We discard three poorly deﬁned relations from T-REx.\n11In a few cases, we ﬁlter entities from certain relations that\ncontain multiple ﬁne-grained relations to make our patterns\ncompatible with the data. For instance, most of the instances\nfor the genre relation describes music genres, thus we remove\nsome of the tuples were the objects include non-music genres\nsuch as ‘satire’, ‘sitcom’ and ‘thriller’.\nModel Succ-Patt Succ-Objs Unk-Const Know-Const\nmajority 97.3+-7.3 23.2+-21.0 100.0+-0.0 100.0+-0.0\nBERT-base 100.0+-0.0 63.0+-19.9 46.5+-21.7 63.8+-24.5BERT-large 100.0+-0.065.7+-22.1 48.1+-20.2 65.2+-23.8BERT-large-wwm100.0+-0.0 64.9+-20.349.5+-20.1 65.3+-25.1\nRoBERTa-base100.0+-0.0 56.2+-22.7 43.9+-15.8 56.3+-19.0RoBERTa-large100.0+-0.0 60.1+-22.3 46.8+-18.0 60.5+-21.1\nALBERT-base100.0+-0.0 45.8+-23.7 41.4+-17.3 56.3+-22.0ALBERT-xxlarge100.0+-0.0 58.8+-23.8 40.5+-16.4 57.5+-23.8\nTable 2: Extractability Measures in the different\nmodels we inspect. Best model for each measure\nhighlighted in bold.\n5.2 Evaluation\nOur consistency measure for a relation ri (Consis-\ntency) is the percentage of consistent predictions\nof all the pattern pairs pi\nk,pi\nl ∈Pi of that relation,\nfor all its KB tuples di\nj ∈Di. Thus, for each KB\ntuple from a relation ri that contains npatterns, we\nconsider predictions for n(n−1)/2 pairs.\nWe also report Accuracy, that is, the acc@1 of\na model in predicting the correct object, using the\noriginal patterns from Petroni et al. (2019). In con-\ntrast to Petroni et al. (2019), we deﬁne it as the accu-\nracy of the top-ranked object from the candidate set\nof each relation. Finally, we report Consistent-Acc,\na new measure that evaluates individual objects as\ncorrect only if all patterns of the corresponding\nrelation predict the object correctly. Consistent-\nAcc is much stricter and combines the requirements\nof both consistency (Consistency) and factual cor-\nrectness (Accuracy). We report the average over\nrelations, i.e., macro average, but notice that the\nmicro average produces similar results.\n6 Experiments and Results\n6.1 Knowledge Extraction through Different\nPatterns\nWe begin by assessing our patterns as well as the\ndegree to which they extract the correct entities.\nThese results are summarized in Table 2.\nFirst, we report Succ-Patt, the percentage of pat-\nterns that successfully predicted the right object\nat least once. A high score suggests that the pat-\nterns are of high quality and enable the models\nto extract the correct answers. All PLMs achieve\na perfect score. Next, we report Succ-Objs, the\npercentage of entities that were predicted correctly\nby at least one of the patterns. Succ-Objs quan-\ntiﬁes the degree to which the models “have” the\nrequired knowledge. We observe that some tuples\nModel Accuracy Consistency Consistent-Acc\nmajority 23.1+-21.0 100.0+-0.0 23.1+-21.0\nBERT-base 45.8+-25.6 58.5+-24.2 27.0+-23.8\nBERT-large 48.1+-26.1 61.1+-23.0 29.5+-26.6\nBERT-large-wwm48.7+-25.0 60.9+-24.2 29.3+-26.9\nRoBERTa-base 39.0+-22.8 52.1+-17.8 16.4+-16.4\nRoBERTa-large 43.2+-24.7 56.3+-20.4 22.5+-21.1\nALBERT-base 29.8+-22.8 49.8+-20.1 16.7+-20.3\nALBERT-xxlarge 41.7+-24.9 52.1+-22.4 23.8+-24.8\nTable 3: Knowledge and Consistency Results. Best\nmodel for each measure in bold.\nare not predicted correctly by any of our patterns:\nthe scores vary between 45.8% for ALBERT-base\nand 65.7% for BERT-large. With an average num-\nber of 8.63 patterns per relation, there are multiple\nways to extract the knowledge, we thus interpret\nthese results as evidence that a large part of T-REx\nknowledge is not stored in these models.\nFinally, we measure Unk-Const, a consistency\nmeasure for the subset of tuples for which no pat-\ntern predicted the correct answer; and Know-Const,\nconsistency for the subset where at least one of\nthe patterns for a speciﬁc relation predicted the\ncorrect answer. This split into subsets is based on\nSucc-Objs. Overall, the results indicate that when\nthe factual knowledge is successfully extracted, the\nmodel is also more consistent. For instance, for\nBERT-large,Know-Const is 65.2% and Unk-Const\nis 48.1%.\n6.2 Consistency & Knowledge\nIn this section, we report the overall knowledge\nmeasure that was used in Petroni et al. (2019) (Ac-\ncuracy), the consistency measure ( Consistency),\nand Consistent- Acc, which combines knowledge\nand consistency (Consistent-Acc). The results are\nsummarized in Table 3.\nWe begin with the Accuracy results. The results\nrange between 29.8% (ALBERT-base) and 48.7%\n(BERT-large whole-word-masking). Notice that\nour numbers differ from Petroni et al. (2019) as\nwe use a candidate set (§3) and only consider KB\ntriples whose object is a single token in all the\nPLMs we consider (§5.1).\nNext, we report Consistency (§5.2). The BERT\nmodels achieve the highest scores. There is a con-\nsistent improvement from base to large versions of\neach model. In contrast to previous work that ob-\nserved quantitative and qualitative improvements\nof RoBERTa-based models over BERT (Liu et al.,\n2019; Talmor et al., 2020), in terms of consistency,\nBERT is more consistent than RoBERTa and AL-\nBERT. Still, the overall results are low (61.1% for\nthe best model), even more remarkably so because\nthe restricted candidate set makes the task easier.\nWe note that the results are highly variant between\nmodels (performance on original-language varies\nbetween 52% and 90%), and relations (BERT-large\nperformance is 92% on capital-of and 44% on\nowned-by).\nFinally, we report Consistent-Acc: the results\nare much lower than for Accuracy, as expected,\nbut follow similar trends: RoBERTa-base performs\nworse (16.4%) and BERT-large best (29.5%).\nInterestingly, we ﬁnd strong correlations be-\ntween Accuracy and Consistency, ranging from\n67.3% for RoBERTa-base to 82.1% for BERT-large\n(all with small p-values ≪0.01).\nA striking result of the model comparison is the\nclear superiority of BERT, both in knowledge ac-\ncuracy (which was also observed by Shin et al.\n(2020)) and knowledge consistency. We hypothe-\nsize this result is caused by the different sources\nof training data: although Wikipedia is part of the\ntraining data for all models we consider, for BERT\nit is the main data source, but for RoBERTa and\nALBERT it is only a small portion. Thus, when\nusing additional data, some of the facts may be\nforgotten, or contradicted in the other corpora; this\ncan diminish knowledge and compromise consis-\ntency behavior. Thus, since Wikipedia is likely the\nlargest uniﬁed source of factual knowledge that ex-\nists in unstructured data, giving it prominence in\npretraining makes it more likely that the model will\nincorporate Wikipedia’s factual knowledge well.\nThese results may have a broader impact on mod-\nels to come: Training bigger models with more\ndata (such as GPT-3 (Brown et al., 2020)) is not\nalways beneﬁcial.\nDeterminism We also measure determinism for\nN-M relations, i.e., we use the same measure\nas Consistency, but since difference predictions\nmay be factually correct, these do not necessarily\nconvey consistency violations, but indicate non-\ndeterminism. For brevity, we do not present all\nresults, but the trend is similar to the consistency\nresult (although not comparable, as the relations\nare different): 52.9% and 44.6% for BERT-large\nand RoBERTa-base, respectively.\nModel Acc Consistency Consistent-Acc\nmajority 23.1+-21.0 100.0+-0.0 23.1+-21.0\nRoBERTa-med-small-1M 11.2+-9.4 37.1+-11.0 2.8+-4.0\nRoBERTa-base-10M 17.3+-15.8 29.8+-12.7 3.2+-5.1\nRoBERTa-base-100M 22.1+-17.1 31.5+-13.0 3.7+-5.3\nRoBERTa-base-1B38.0+-23.4 50.6+-19.8 18.0+-16.0\nTable 4: Knowledge and consistency results for\ndifferent RoBERTas, trained on increasing amounts\nof data. Best model for each measure in bold.\nEffect of Pretraining Corpus Size Next, we\nstudy the question of whether the number of to-\nkens used during pretraining contributes to con-\nsistency. We use the pretrained RoBERTa mod-\nels from Warstadt et al. (2020) and repeat the ex-\nperiments on four additional models. These are\nRoBERTa-based models, trained on a sample of\nWikipedia and the book corpus, with varying train-\ning sizes and parameters. We use one of the three\npublished models for each conﬁguration and re-\nport the average accuracy over the relations for\neach model in Table 4. Overall, Accuracy and\nConsistent-Acc improve with more training data.\nHowever, there is an interesting outlier to this trend:\nThe model that was trained on one million tokens\nis more consistent than the models trained on ten\nand one-hundred million tokens. A potentially cru-\ncial difference is that this model has many fewer\nparameters than the rest (to avoid overﬁtting). It is\nnonetheless interesting that a model that is trained\non signiﬁcantly less data can achieve better consis-\ntency. On the other hand, it’s accuracy scores are\nlower, arguably due to the model being exposed to\nless factual knowledge during pretraining.\n6.3 Do PLMs Generalize Over Syntactic\nConﬁgurations?\nMany papers have found neural models (especially\nPLMs) to naturally encode syntax (Linzen et al.,\n2016; Belinkov et al., 2017; Marvin and Linzen,\n2018; Belinkov and Glass, 2019; Goldberg, 2019;\nHewitt and Manning, 2019). Does this mean that\nPLMs have successfully abstracted knowledge and\ncan comprehend and produce it regardless of syn-\ntactic variation? We consider two scenarios. (1)\nTwo patterns differ only in syntax. (2) Both syntax\nand lexical choice are the same. As a proxy, we\ndeﬁne syntactic equivalence when the dependency\npath between the subject and object are identical.\nWe parse all patterns from PARA REL\n using a\nModel Diff-Syntax No-Change\nmajority 100.0+-0.0 100.0+-0.0\nBERT-base 67.9+-30.3 76.3+-22.6\nBERT-large 67.5+-30.2 78.7+-14.7\nBERT-large-wwm 63.0+-31.781.1+-9.7\nRoBERTa-base 66.9+-10.1 80.7+-5.2\nRoBERTa-large 69.7+-19.2 80.3+-6.8\nALBERT-base 62.3+-22.8 72.6+-11.5\nALBERT-xxlarge 51.7+-26.0 67.3+-17.1\nTable 5: Consistency and standard deviation when\nonly syntax differs (Diff-Syntax) and when syntax\nand lexical choice are identical (No-Change). Best\nmodel for each metric is highlighted in bold.\ndependency parser (Honnibal et al., 2020) 12 and\nretain the path between the entities. Success on (1)\nindicates that the model’s knowledge processing\nis robust to syntactic variation. Success on (2) in-\ndicates that the model’s knowledge processing is\nrobust to variation in word order and tense.\nTable 5 reports results. While these and the\nmain results on the entire dataset are not compa-\nrable as the pattern subsets are different, they are\nhigher than the general results: 67.5% for BERT-\nlarge when only the syntax differs and 78.7% when\nthe syntax is identical. This demonstrates that\nwhile PLMs have impressive syntactic abilities,\nthey struggle to extract factual knowledge in the\nface of tense, word-order, and syntactic variation.\nMcCoy et al. (2019) show that supervised mod-\nels trained on MNLI (Williams et al., 2018), an\nNLI dataset (Bowman et al., 2015), use superﬁcial\nsyntactic heuristics rather than more generalizable\nproperties of the data. Our results indicate that\nPLMs have problems along the same lines: they\nare not robust to surface variation.\n7 Analysis\n7.1 Qualitative Analysis\nTo better understand the factors affecting consistent\npredictions, we inspect the predictions of BERT-\nlarge on the patterns shown in Table 6. We high-\nlight several cases: The predictions in Example #1\nare inconsistent, and correct for the ﬁrst pattern\n(Amsterdam), but not for the other two (Madagas-\ncar and Luxembourg). The predictions in Example\n#2 also show a single pattern that predicted the right\nobject; however, the two other patterns, which are\n12https://spacy.io/\n# Subject Object Pattern #1 Pattern #2 Pattern #3 Pred #1 Pred #2 Pred #3\n1 Adriaan Pauw Amsterdam [X] was born in [Y]. [X] is native to [Y]. [X] is a [Y]-born person.AmsterdamMadagascarLuxembourg2 Nissan Livina Geniss Nissan [X] is produced by [Y]. [X] is created by [Y]. [X], created by [Y].Nissan Renault Renault3 Albania Serbia [X] shares border with [Y]. [Y] borders with [X]. [Y] shares the border with [X]Greece Turkey Kosovo4 iCloud Apple [X] is developed by [Y]. [X], created by [Y]. [X] was created by [Y]MicrosoftGoogle Sony\n5 Yahoo! Messenger Yahoo [X], a product created by [Y] [X], a product developed by [Y] [Y], that developed [X]MicrosoftMicrosoftMicrosoft6 Wales Cardiff The capital of [X] is [Y] . [X]’s capital, [Y]. [X]’s capital city, [Y].Cardiff Cardiff Cardiff\nTable 6: Predictions of BERT-large-cased. “Subject” and “Object” are from T-REx (Elsahar et al., 2018).\n“Pattern #i” / “Pred #i”: three different patterns from our resource and their predictions. The predictions\nare colored in blue if the model predicted correctly (out of the candidate list), and in red otherwise. If\nthere is more than a single erroneous prediction, it is colored by a different red.\n60\n 40\n 20\n 0 20 40\n40\n20\n0\n20\n40\nCapital\nFigure 3: t-SNE of the encoded patterns from the\ncapital relation. The colors represent the different\nsubjects, while the shapes represent patterns. A\nknowledge-focused representation should cluster\nbased on identical subjects (color), but instead the\nclustering is according to identical patterns (shape).\nlexically similar, predicted the same, wrong answer\n– Renault. Next, the patterns of Example #3 pro-\nduced two factually correct answers out of three\n(Greece, Kosovo), but simply do not correspond\nto the gold object in T-REx ( Albania), since this\nis an M-N relation. Note that this relation is not\npart of the consistency evaluation, but the deter-\nminism evaluation. The three different predictions\nin example #4 are all incorrect. Finally, the two\nlast predictions demonstrate consistent predictions:\nExample #5 is consistent but factually incorrect\n(even though the correct answer is a substring of\nthe subject), and ﬁnally, Example #6 is consistent\nand factual.\n7.2 Representation Analysis\nTo provide insights on the models’ representations,\nwe inspect these after encoding the patterns.\nMotivated by previous work that found that\nwords with the same syntactic structure cluster to-\ngether (Chi et al., 2020; Ravfogel et al., 2020) we\nperform a similar experiment to test if this behav-\nior replicates with respect to knowledge: We en-\ncode the patterns, after ﬁlling the placeholders with\nsubjects and masked tokens and inspect the last\nlayer representations in the masked token position.\nWhen plotting the results using t-SNE (Maaten and\nHinton, 2008) we mainly observe clustering based\non the patterns, which suggests that encoding of\nknowledge of the entity is not the main component\nof the representations. Figure 3 demonstrates this\nfor BERT-large encodings of the capital relation,\nwhich is highly consistent. 13 To provide a more\nquantitative assessment of this phenomenon, we\nalso cluster the representations and set the number\nof centroids based on:14 (1) the number of patterns\nin each relation, which aims to capture pattern-\nbased clusters, and (2) the number of subjects in\neach relation, which aims to capture entity-based\nclusters. This would allow for a perfect cluster-\ning, in the case of perfect alignment between the\nrepresentation and the inspected property. We mea-\nsure the purity of these clusters using V-measure\nand observe that the clusters are mostly grouped\nby the patterns, rather than the subjects. Finally,\nwe compute the spearman correlation between the\nconsistency scores and the V-measure of the repre-\nsentations. However, the correlation between these\nvariables is close to zero,15 therefore not explaining\nthe models’ behavior. We repeated these experi-\nments while inspecting the objects instead of the\nsubjects, and found similar trends. This ﬁnding is\ninteresting since it means that (1) these represen-\ntations are not knowledge-focused, i.e., their main\ncomponent does not relate to knowledge, and (2)\nthe representation by its entirety does not explain\nthe behavior of the model, and thus only a subset of\nthe representation does. This ﬁnding is consistent\n13While some patterns are clustered based on the subjects\n(upper-left part), most of them are clustered based on patterns.\n14Using the KMeans algorithm.\n15Except for BERT-large whole-word-masking, where the\ncorrelation is 39.5 (p <0.05).\nwith previous work that observed similar trends for\nlinguistic tasks (Elazar et al., 2021). We hypothe-\nsize that this disparity between the representation\nand the behavior of the model may be explained by\na situation where the distance between representa-\ntions largely does not reﬂect the distance between\npredictions, but rather other, behaviorally irrelevant\nfactors of a sentence.\n8 Improving Consistency in PLMs\nIn the previous sections, we showed PLMs are gen-\nerally not consistent in their predictions, and pre-\nvious works have noticed the lack of this property\nin a variety of downstream tasks. An ideal model\nwould exhibit the consistency property after pre-\ntraining, and would then be able to transfer it to\ndifferent downstream tasks. We therefore ask: Can\nwe enhance current PLMs and make them more\nconsistent?\n8.1 Consistency Improved PLMs\nWe propose to improve the consistency of PLMs\nby continuing the pretraining step with a novel\nconsistency loss. We make use of the T-REx tuples\nand the paraphrases from PARA REL\n .\nFor each relationri, we have a set of paraphrased\npatterns Pi describing that relation. We use a PLM\nto encode all patterns in Pi, after populating a sub-\nject that corresponds to the relation ri and a mask\ntoken. We expect the model to make the same\nprediction for the masked token for all patterns.\nConsistency Loss Function As we evaluate the\nmodel using acc@1, the straight-forward consis-\ntency loss would require these predictions to be\nidentical:\nmin\nθ\nsim(arg max\ni\nfθ(Pn)[i],arg max\nj\nfθ(Pm)[j])\nwhere fθ(Pn) is the output of an encoding func-\ntion (e.g., BERT) parameterized by θ (a vector)\nover input Pn, and fθ(Pn)[i] is the score of the ith\nvocabulary item of the model.\nHowever, this objective contains a comparison\nbetween the output of two argmax operations, mak-\ning it discrete and discontinuous, and hard to opti-\nmize in a gradient-based framework. We instead\nrelax the objective, and require that the predicted\ndistributions Qn = softmax(fθ(Pn)), rather than\nthe top-1 prediction, be identical to each other.\nWe use two-sided KL Divergence to measure sim-\nilarity between distributions: DKL(Qri\nn||Qri\nm) +\nDKL(Qri\nm||Qri\nn) where Qri\nn is the predicted distri-\nbution for pattern Pn of relation ri.\nAs most of the vocabulary is not relevant for the\npredictions, we ﬁlter it down to the ktokens from\nthe candidate set of each relation (§3.2). We want\nto maintain the original capabilities of the model –\nfocusing on the candidate set helps to achieve this\ngoal since most of the vocabulary is not affected\nby our new loss.\nTo encourage a more general solution, we make\nuse of all the paraphrases together, and enforce all\npredictions to be as close as possible. Thus, the\nconsistency loss for all pattern pairs for a particular\nrelation ri is:\nLc =\nk∑\nn=1\nk∑\nm=n+1\nDKL(Qri\nn||Qri\nm)+DKL(Qri\nm||Qri\nn)\nMLM Loss Since the consistency loss is differ-\nent from the Cross-Entropy loss the PLM is trained\non, we ﬁnd it important to continue the MLM loss\non text data, similar to previous work (Geva et al.,\n2020).\nWe consider two alternatives for continuing the\npretraining objective: (1) MLM on Wikipedia and\n(2) MLM on the patterns of the relations used for\nthe consistency loss. We found that the latter works\nbetter. We denote this loss by LMLM.\nConsistency Guided MLM Continual Training\nCombining our novel consistency loss with the reg-\nular MLM loss, we continue the PLM training by\ncombining the two losses. The combination of the\ntwo losses is determined by a hyperparameter λ,\nresulting in the following ﬁnal loss function:\nL= λLc + LMLM\nThis loss is computed per relation, for one KB tuple.\nWe have many of these instances, which we require\nto behave similarly. Therefore, we batch together\nl= 8tuples from the same relation and apply the\nconsistency loss function to all of them.\n8.2 Setup\nSince we evaluate our method on unseen relations,\nwe also split train and test by relation type (e.g.,\nlocation-based relations, which are very common\nin T-REx). Moreover, our method is aimed to\nbe simple, effective, and to require only mini-\nmal supervision. Therefore, we opt to use only\nthree relations: original-language, named-after,\nand original-network; these were chosen randomly,\nModel Accuracy Consistency Consistent-Acc\nmajority 24.4+-22.5 100.0+-0.0 24.4+-22.5\nBERT-base 45.6+-27.6 58.2+-23.9 27.3+-24.8\nBERT-ft 47.4+-27.3 64.0+-22.9 33.2+-27.0\n-consistency 46.9+-27.6 60.9+-22.6 30.9+-26.3\n-typed 46.5+-27.1 62.0+-21.2 31.1+-25.2\n-MLM 16.9+-21.1 80.8+-27.1 9.1+-11.5\nTable 7: Knowledge and consistency results for\nthe baseline, BERT base, and our model. The re-\nsults are averaged over the 25 test relations. Un-\nderlined: best performance overall, including abla-\ntions. Bold: best performance for BERT-ft and the\ntwo baselines (BERT-base, majority).\nout of the non-location related relations.16 For val-\nidation, we randomly pick three relations of the\nremaining relations and use the remaining 25 for\ntesting.\nWe perform minimal tuning of the parameters\n(λ ∈0.1,0.5,1) to pick the best model, train for\nthree epochs, and select the best model based on\nConsistent-Acc on the validation set. For efﬁciency\nreasons, we use the base version of BERT.\n8.3 Improved Consistency Results\nThe results are presented in Table 7. We report\naggregated results for the 25 relations in the test.\nWe again report macro average (mean over rela-\ntions) and standard deviation. We report the results\nof the majority baseline (ﬁrst row), BERT-base\n(second row) and our new model (BERT-ft, third\nrow). First, we note that our model signiﬁcantly im-\nproves consistency: 64.0% (compared with 58.2%\nfor BERT-base, an increase of 5.8 points). Accu-\nracy also improves compared to BERT-base, from\n45.6% to 47.4%. Finally, and most importantly,\nwe see an increase of 5.9 points in Consistent-Acc,\nwhich is achieved due to the improved consistency\nof the model. Notably, these improvements arise\nfrom training on merely three relations, meaning\nthat the model improved its consistency ability and\ngeneralized to new relations. We measure the sta-\ntistical signiﬁcance of our method compared to the\nBERT baseline, using McNemar’s test (following\nDror et al. (2018, 2020)) and ﬁnd all results to be\nsigniﬁcant (p≪0.01).\nWe also perform an ablation study to quantify\nthe utility of the different components. First, we\nreport on the ﬁnetuned model without the consis-\ntency loss (-consistency). Interestingly, it does im-\n16Many relations are location-based – not training on them\nprevents train-test leakage.\nprove over the baseline (BERT-base), but it lags\nbehind our ﬁnetuned model. Second, applying our\nloss on the candidate set rather than on the entire\nvocabulary is beneﬁcial (-typed). Finally, by not\nperforming the MLM training on the generated\npatterns (-MLM), the consistency results improve\nsigniﬁcantly (80.8%); however, this also hurts Ac-\ncuracy and Consistent-Acc. MLM training seems\nto serve as a regularizer that prevents catastrophic\nforgetting.\nOur ultimate goal is to improve consistency in\nPLMs for better performance on downstream tasks.\nTherefore, we also experiment with ﬁnetuning on\nSQuAD (Rajpurkar et al., 2016), and evaluating\non paraphrased questions from SQuAD (Gan and\nNg, 2019) using our consistency model. How-\never, the results perform on par with the baseline\nmodel, both on SQuAD and the paraphrase ques-\ntions. More research is required to show that con-\nsistent PLMs can also beneﬁt downstream tasks.\n9 Discussion\nConsistency for Downstream Tasks The rise\nof PLMs has improved many tasks but has also\nbrought a lot of expectations. The standard usage\nof these models is pretraining on a large corpus of\nunstructured text and then ﬁnetuning on a task of\ninterest. The ﬁrst step is thought of as providing a\ngood language-understanding component, whereas\nthe second step is used to teach the format and the\nnuances of a downstream task.\nAs discussed earlier, consistency is a crucial\ncomponent of many NLP systems (Du et al., 2019;\nAsai and Hajishirzi, 2020; Denis and Baldridge,\n2009; Kryscinski et al., 2020) and obtaining this\nskill from a pretrained model would be extremely\nbeneﬁcial and has the potential to make specialized\nconsistency solutions in downstream tasks redun-\ndant. Indeed, there is an ongoing discussion about\nthe ability to acquire “meaning” from raw text sig-\nnal alone (Bender and Koller, 2020). Our new\nbenchmark makes it possible to track the progress\nof consistency in pretrained models.\nBroader Sense of Consistency In this work we\nfocus on one type of consistency, that is, consis-\ntency in the face of paraphrasing; however, consis-\ntency is a broader concept. For instance, previous\nwork has studied the effect of negation on factual\nstatements, which can also be seen as consistency\n(Ettinger, 2020; Kassner and Schütze, 2020). A\nconsistent model is expected to return different an-\nswers to the prompts: “ Birds can [MASK]” and\n“Birds cannot [MASK]”. The inability to do so, as\nwas shown in these works, also shows the lack of\nmodel consistency.\nUsage of PLMs as KBs Our work follows the\nsetup of Petroni et al. (2019); Jiang et al. (2020),\nwhere PLMs are being tested as KBs. While it is an\ninteresting setup for probing models for knowledge\nand consistency, it lacks important properties of\nstandard KBs: (1) the ability to return more than\na single answer and (2) the ability to return no\nanswer. Although some heuristics can be used for\nallowing a PLM to do so, e.g., using a threshold on\nthe probabilities, it is not the way that the model\nwas trained, and thus may not be optimal. Newer\napproaches that propose to use PLMs as a starting\npoint to more complex systems have promising\nresults and address these problems (Thorne et al.,\n2020).\nIn another approach, Shin et al. (2020) suggest\nusing AUTO PROMPT to automatically generate\nprompts, or patterns, instead of creating them man-\nually. This approach is superior to manual patterns\n(Petroni et al., 2019), or aggregation of patterns that\nwere collected automatically (Jiang et al., 2020).\nBrittleness of Neural Models Our work also re-\nlates to the problem of the brittleness of neural\nnetworks. One example of this brittleness is the\nvulnerability to adversarial attacks (Szegedy et al.,\n2014; Jia and Liang, 2017). The other problem,\ncloser to the problem we explore in this work, is the\npoor generalization to paraphrases. For example,\nGan and Ng (2019) created a paraphrase version\nfor a subset of SQuAD (Rajpurkar et al., 2016), and\nshowed that model performance drops signiﬁcantly.\nRibeiro et al. (2018) proposed another method for\ncreating paraphrases and performed a similar anal-\nysis for visual question answering and sentiment\nanalysis. Recently, Ribeiro et al. (2020) proposed\nCHECK LIST , a system that tests a model’s vulnera-\nbility to several linguistic perturbations.\nPARA REL\n enables us to study the brittleness\nof PLMs, and separate facts that are robustly en-\ncoded in the model from mere ‘guesses’, which\nmay arise from some heuristic or spurious corre-\nlations with certain patterns (Poerner et al., 2020).\nWe showed that PLMs are susceptible to small per-\nturbations, and thus, ﬁnetuning on a downstream\ntask – given that training datasets are typically not\nlarge and do not contain equivalent examples – is\nnot likely to perform better.\nCan we Expect from LMs to be Consistent?\nThe typical training procedure of an LM does\nnot encourage consistency. The standard training\nsolely tries to minimize the log-likelihood of an un-\nseen token, and this objective is not always aligned\nwith consistency of knowledge. Consider for ex-\nample the case of wikipedia texts, as opposed to\nreddit; their texts and styles may be very different\nand they may even describe contradictory facts. An\nLM can exploit the styles of each text to best ﬁt the\nprobabilities given to an unseen word, even if the\nresulting generations contradict each other.\nSince the pretraining-ﬁnetuning procedure is\nthe dominating one in our ﬁeld currently, a great\namount of the language capabilities that were\nlearned during pre-training also propagates to the\nﬁne-tuned models. As such, we believe it is impor-\ntant to measure and improve consistency already in\nthe pretrained models.\nReasons Behind the (In)Consistency Since\nLMs are not expected to be consistent, what are\nthe reasons behind their predictions, when being\nconsistent, or inconsistent?\nIn this work, we presented the predictions of\nmultiple queries, and the representation space of\none of the inspected models. However, this does\nnot point to the origins of such behavior. In future\nwork, we aim to inspect this question more closely.\n10 Conclusion\nIn this work, we study the consistency of PLMs\nwith regard to their ability to extract knowl-\nedge. We build a high-quality resource named\nPARA REL\n that contains 328 high-quality pat-\nterns for 38 relations. Using PARA REL\n , we\nmeasure consistency in multiple PLMs, including\nBERT, RoBERTa, and ALBERT, and show that\nalthough the latter two are superior to BERT in\nother tasks, they fall short in terms of consistency.\nHowever, the consistency of these models is gener-\nally low. We release PARA REL\n along with data\ntuples from T-REx as a new benchmark to track\nknowledge consistency of NLP models. Finally, we\npropose a new simple method to improve model\nconsistency, by continuing the pretraining with a\nnovel loss. We show this method to be effective\nand to improve both the consistency of models as\nwell as their ability to extract the correct facts.\nAcknowledgements\nWe would like to thank Tomer Wolfson, Ido Da-\ngan, Amit Moryossef and Victoria Basmov for\ntheir helpful comments and discussions, and Alon\nJacovi, Ori Shapira, Arie Cattan, Elron Bandel,\nPhilipp Dufter, Masoud Jalili Sabet, Marina Sper-\nanskaya, Antonis Maronikolakis, Aakanksha Naik,\nAishwarya Ravichander, Aditya Potukuchi for the\nhelp with the annotations. We also thank the anony-\nmous reviewers and the action editor, George Fos-\nter, for their valuable suggestions.\nYanai Elazar is grateful to be supported by the\nPBC fellowship for outstanding PhD candidates\nin Data Science and the Google PhD fellowship.\nThis project has received funding from the Eu-\nropoean Research Council (ERC) under the Eu-\nropoean Union’s Horizon 2020 research and inno-\nvation programme, grant agreement No. 802774\n(iEXTRACT). This work has been funded by the\nEuropean Research Council (#740516) and by the\nGerman Federal Ministry of Education and Re-\nsearch (BMBF) under Grant No. 01IS18036A.\nThe authors of this work take full responsibil-\nity for its content. This research was also sup-\nported in part by grants from the National Sci-\nence Foundation Secure and Trustworthy Comput-\ning program (CNS-1330596, CNS15-13957, CNS-\n1801316, CNS-1914486) and a DARPA Brandeis\ngrant (FA8750-15-2-0277). The views and con-\nclusions contained herein are those of the authors\nand should not be interpreted as necessarily repre-\nsenting the ofﬁcial policies or endorsements, either\nexpressed or implied, of the NSF, DARPA, or the\nUS Government.\nReferences\nChris Alberti, Daniel Andor, Emily Pitler, Jacob\nDevlin, and Michael Collins. 2019. Synthetic qa\ncorpora generation with roundtrip consistency.\nIn Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 6168–6173.\nKim Allan Andersen and Daniele Pretolani. 2001.\nEasy cases of probabilistic satisﬁability. An-\nnals of Mathematics and Artiﬁcial Intelligence,\n33(1):69–91.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu,\nMargaret Mitchell, Dhruv Batra, C Lawrence Zit-\nnick, and Devi Parikh. 2015. Vqa: Visual ques-\ntion answering. In Proceedings of the IEEE\ninternational conference on computer vision ,\npages 2425–2433.\nAkari Asai and Hannaneh Hajishirzi. 2020. Logic-\nguided data augmentation and regularization for\nconsistent question answering. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 5642–5650,\nOnline. Association for Computational Linguis-\ntics.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi,\nHassan Sajjad, and James Glass. 2017. What do\nneural machine translation models learn about\nmorphology? In Proceedings of the 55th Annual\nMeeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 861–\n872.\nYonatan Belinkov and James Glass. 2019. Anal-\nysis methods in neural language processing: A\nsurvey. Transactions of the Association for Com-\nputational Linguistics, 7:49–72.\nEmily M Bender and Alexander Koller. 2020.\nClimbing towards nlu: On meaning, form, and\nunderstanding in the age of data. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 5185–5198.\nRahul Bhagat and Eduard Hovy. 2013. What\nis a paraphrase? Computational Linguistics ,\n39(3):463–472.\nLukas Biewald. 2020. Experiment tracking with\nweights and biases. Software available from\nwandb.com.\nSamuel R. Bowman, Gabor Angeli, Christopher\nPotts, and Christopher D. Manning. 2015. A\nlarge annotated corpus for learning natural lan-\nguage inference. In Proceedings of the 2015\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP). Association for\nComputational Linguistics.\nTom B Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, et al. 2020. Language\nmodels are few-shot learners. arXiv preprint\narXiv:2005.14165.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli:\nNatural language inference with natural lan-\nguage explanations. In NeurIPS.\nOana-Maria Camburu, Brendan Shillingford,\nPasquale Minervini, Thomas Lukasiewicz, and\nPhil Blunsom. 2020. Make up your mind! adver-\nsarial generation of inconsistent natural language\nexplanations. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 4157–4165.\nKai-Wei Chang, Rajhans Samdani, Alla Ro-\nzovskaya, Nick Rizzolo, Mark Sammons, and\nDan Roth. 2011. Inference protocols for corefer-\nence resolution. In Proceedings of the Fifteenth\nConference on Computational Natural Language\nLearning: Shared Task, pages 40–44.\nEthan A. Chi, John Hewitt, and Christopher D.\nManning. 2020. Finding universal grammatical\nrelations in multilingual BERT. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 5564–5577,\nOnline. Association for Computational Linguis-\ntics.\nJeff Da and Jungo Kasai. 2019. Cracking the\ncontextual commonsense code: Understanding\ncommonsense reasoning aptitude of deep contex-\ntual representations. In Proceedings of the First\nWorkshop on Commonsense Inference in Natural\nLanguage Processing, pages 1–12, Hong Kong,\nChina. Association for Computational Linguis-\ntics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges\nWorkshop, pages 177–190. Springer.\nJoe Davison, Joshua Feldman, and Alexander M\nRush. 2019. Commonsense knowledge min-\ning from pretrained models. In Proceedings\nof the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the\n9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pages\n1173–1178.\nPascal Denis and Jason Baldridge. 2009. Global\njoint models for coreference resolution and\nnamed entity classiﬁcation. Procesamiento del\nLenguaje Natural, 42.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the\nAssociation for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long\nand Short Papers), pages 4171–4186.\nRotem Dror, Gili Baumer, Segev Shlomov, and Roi\nReichart. 2018. The hitchhiker’s guide to testing\nstatistical signiﬁcance in natural language pro-\ncessing. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1383–\n1392.\nRotem Dror, Lotem Peled-Cohen, Segev Shlomov,\nand Roi Reichart. 2020. Statistical signiﬁcance\ntesting for natural language processing. Synthe-\nsis Lectures on Human Language Technologies,\n13(2):1–116.\nXinya Du, Bhavana Dalvi, Niket Tandon, Antoine\nBosselut, Wen-tau Yih, Peter Clark, and Claire\nCardie. 2019. Be consistent! improving proce-\ndural text comprehension using label consistency.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 2347–2356.\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and\nYoav Goldberg. 2021. Amnesic Probing: Behav-\nioral Explanation with Amnesic Counterfactuals.\nTransactions of the Association for Computa-\ntional Linguistics, 9:160–175.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-rex: A\nlarge scale alignment of natural language with\nknowledge base triples. In Proceedings of the\nEleventh International Conference on Language\nResources and Evaluation (LREC 2018).\nAllyson Ettinger. 2020. What bert is not: Lessons\nfrom a new suite of psycholinguistic diagnostics\nfor language models. Transactions of the Asso-\nciation for Computational Linguistics, 8:34–48.\nMaxwell Forbes, Ari Holtzman, and Yejin Choi.\n2019. Do neural language representations learn\nphysical commonsense? In CogSci.\nWee Chung Gan and Hwee Tou Ng. 2019. Improv-\ning the robustness of question answering systems\nto question paraphrasing. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6065–6075.\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting numerical reasoning skills into lan-\nguage models. In Association for Computational\nLinguistics (ACL).\nYoav Goldberg. 2019. Assessing bert’s syntactic\nabilities. arXiv preprint arXiv:1901.05287.\nPierre Hansen and Brigitte Jaumard. 2000. Proba-\nbilistic satisﬁability. In Handbook of Defeasible\nReasoning and Uncertainty Management Sys-\ntems, pages 321–367. Springer.\nJohn Hewitt and Christopher D. Manning. 2019.\nA structural probe for ﬁnding syntax in word\nrepresentations. In North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies (NAACL). Asso-\nciation for Computational Linguistics.\nMatthew Honnibal, Ines Montani, Soﬁe Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy:\nIndustrial-strength Natural Language Processing\nin Python.\nRobin Jia and Percy Liang. 2017. Adversarial ex-\namples for evaluating reading comprehension\nsystems. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 2021–2031.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Gra-\nham Neubig. 2020. How can we know what\nlanguage models know? Transactions of the As-\nsociation for Computational Linguistics, 8:423–\n438.\nNora Kassner, Philipp Dufter, and Hinrich Schütze.\n2021a. Multilingual lama: Investigating knowl-\nedge in multilingual pretrained language models.\nNora Kassner, Benno Krojer, and Hinrich Schütze.\n2020. Are pretrained language models symbolic\nreasoners over knowledge? In Proceedings of\nthe 24th Conference on Computational Natural\nLanguage Learning, pages 552–564, Online. As-\nsociation for Computational Linguistics.\nNora Kassner and Hinrich Schütze. 2020. Negated\nand misprimed probes for pretrained language\nmodels: Birds can talk, but cannot ﬂy. In Pro-\nceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages\n7811–7818, Online. Association for Computa-\ntional Linguistics.\nNora Kassner, Oyvind Tafjord, Hinrich Schütze,\nand Peter Clark. 2021b. Enriching a model’s no-\ntion of belief using a persistent memory. CoRR,\nabs/2104.08401.\nWojciech Kryscinski, Bryan McCann, Caiming\nXiong, and Richard Socher. 2020. Evaluating\nthe factual consistency of abstractive text sum-\nmarization. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 9332–9346.\nZhenzhong Lan, Mingda Chen, Sebastian Good-\nman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. 2019. Albert: A lite bert for self-\nsupervised learning of language representations.\nIn International Conference on Learning Repre-\nsentations.\nTao Li, Vivek Gupta, Maitrey Mehta, and Vivek\nSrikumar. 2019. A logic-driven framework for\nconsistency of neural models. In Proceedings\nof the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the\n9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pages\n3924–3935, Hong Kong, China. Association for\nComputational Linguistics.\nTal Linzen. 2020. How can we accelerate progress\ntowards human-like linguistic generalization?\nIn Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics,\npages 5210–5217.\nTal Linzen, Emmanuel Dupoux, and Yoav Gold-\nberg. 2016. Assessing the ability of lstms to\nlearn syntax-sensitive dependencies. Transac-\ntions of the Association for Computational Lin-\nguistics, 4:521–535.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei\nDu, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin\nStoyanov. 2019. Roberta: A robustly opti-\nmized bert pretraining approach. arXiv preprint\narXiv:1907.11692.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. Journal of Ma-\nchine Learning Research, 9:2579–2605.\nRebecca Marvin and Tal Linzen. 2018. Targeted\nsyntactic evaluation of language models. In Pro-\nceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages\n1192–1202, Brussels, Belgium. Association for\nComputational Linguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syn-\ntactic heuristics in natural language inference.\nIn Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 3428–3448.\nDavid Picado Muiño. 2011. Measuring and re-\npairing inconsistency in probabilistic knowledge\nbases. International Journal of Approximate\nReasoning, 52(6):828–840.\nKim Anh Nguyen, Sabine Schulte im Walde, and\nNgoc Thang Vu. 2016. Integrating distribu-\ntional lexical contrast into word embeddings for\nantonym-synonym distinction. In Proceedings\nof the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short\nPapers), pages 454–459.\nF. Pedregosa, G. Varoquaux, A. Gramfort,\nV . Michel, B. Thirion, O. Grisel, M. Blondel,\nP. Prettenhofer, R. Weiss, V . Dubourg, J. Van-\nderplas, A. Passos, D. Cournapeau, M. Brucher,\nM. Perrot, and E. Duchesnay. 2011. Scikit-learn:\nMachine learning in Python. Journal of Machine\nLearning Research, 12:2825–2830.\nMatthew E Peters, Mark Neumann, Robert Logan,\nRoy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A Smith. 2019. Knowledge enhanced con-\ntextual word representations. In Proceedings of\nthe 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 43–54.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus,\nTim Rocktäschel, Yuxiang Wu, Alexander H.\nMiller, and Sebastian Riedel. 2020. How context\naffects language models’ factual predictions. In\nAutomated Knowledge Base Construction.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as\nknowledge bases? In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 2463–2473, Hong\nKong, China. Association for Computational\nLinguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2020. E-bert: Efﬁcient-yet-effective entity em-\nbeddings for bert. In Proceedings of the 2020\nConference on Empirical Methods in Natural\nLanguage Processing: Findings, pages 803–818.\nAlec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multitask\nlearners. OpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. 2020. Ex-\nploring the limits of transfer learning with a uni-\nﬁed text-to-text transformer. Journal of Machine\nLearning Research, 21:1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopy-\nrev, and Percy Liang. 2016. Squad: 100,000+\nquestions for machine comprehension of text. In\nProceedings of the 2016 Conference on Empir-\nical Methods in Natural Language Processing,\npages 2383–2392.\nShauli Ravfogel, Yanai Elazar, Jacob Goldberger,\nand Yoav Goldberg. 2020. Unsupervised distil-\nlation of syntactic information from contextual-\nized word representations. In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and\nInterpreting Neural Networks for NLP , pages\n91–106, Online. Association for Computational\nLinguistics.\nAbhilasha Ravichander, Eduard Hovy, Kaheer Sule-\nman, Adam Trischler, and Jackie Chi Kit Che-\nung. 2020. On the systematicity of probing con-\ntextualized word representations: The case of\nhypernymy in BERT. In Proceedings of the\nNinth Joint Conference on Lexical and Compu-\ntational Semantics, pages 88–102, Barcelona,\nSpain (Online). Association for Computational\nLinguistics.\nMarco Tulio Ribeiro, Carlos Guestrin, and Sameer\nSingh. 2019. Are red roses red? evaluating\nconsistency of question-answering models. In\nProceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 6174–6184, Florence, Italy. Association\nfor Computational Linguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2018. Semantically equivalent adver-\nsarial rules for debugging nlp models. In Pro-\nceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume\n1: Long Papers), pages 856–865.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos\nGuestrin, and Sameer Singh. 2020. Beyond ac-\ncuracy: Behavioral testing of NLP models with\nCheckList. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 4902–4912, Online. Associa-\ntion for Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer.\n2020. How much knowledge can you pack into\nthe parameters of a language model? In Proceed-\nings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP),\npages 5418–5426.\nAnna Rogers, Olga Kovaleva, and Anna\nRumshisky. 2020. A primer in bertology: What\nwe know about how bert works. Transactions of\nthe Association for Computational Linguistics,\n8:842–866.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Auto-\nPrompt: Eliciting Knowledge from Language\nModels with Automatically Generated Prompts.\nIn Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP), pages 4222–4235, Online. Associa-\ntion for Computational Linguistics.\nMicah Shlain, Hillel Taub-Tabib, Shoval Sadde,\nand Yoav Goldberg. 2020. Syntactic search by\nexample. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics: System Demonstrations, pages 17–\n23, Online. Association for Computational Lin-\nguistics.\nVered Shwartz and Yejin Choi. 2020. Do neural\nlanguage models overcome reporting bias? In\nProceedings of the 28th International Confer-\nence on Computational Linguistics, pages 6863–\n6870.\nChristian Szegedy, Wojciech Zaremba, Ilya\nSutskever, Joan Bruna, Dumitru Erhan, Ian\nGoodfellow, and Rob Fergus. 2014. Intriguing\nproperties of neural networks. In International\nConference on Learning Representations.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. olmpics-on what lan-\nguage model pre-training captures. Transactions\nof the Association for Computational Linguistics,\n8:743–758.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. In\nProceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 4593–4601.\nMatthias Thimm. 2009. Measuring Inconsistency\nin Probabilistic Knowledge Bases. In Proceed-\nings of the Twenty-Fifth Conference on Uncer-\ntainty in Artiﬁcial Intelligence (UAI’09), pages\n530–537. AUAI Press.\nMatthias Thimm. 2013. Inconsistency measures\nfor probabilistic logics. Artiﬁcial Intelligence,\n197:1–24.\nJames Thorne, Majid Yazdani, Marzieh Saeidi,\nFabrizio Silvestri, Sebastian Riedel, and Alon\nHalevy. 2020. Neural databases. arXiv preprint\narXiv:2010.06973.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun\nLiu, and Samuel R. Bowman. 2020. Learning\nwhich features matter: RoBERTa acquires a pref-\nerence for linguistic generalizations (eventually).\nIn Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP), pages 217–235, Online. Association\nfor Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference.\nIn Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages\n1112–1122. Association for Computational Lin-\nguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Rémi Louf, Morgan\nFuntowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexan-\nder M. Rush. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demon-\nstrations, pages 38–45, Online. Association for\nComputational Linguistics.\nWenhan Xiong, Jingfei Du, William Yang Wang,\nand Veselin Stoyanov. 2020. Pretrained encyclo-\npedia: Weakly supervised knowledge-pretrained\nlanguage model. In 8th International Confer-\nence on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. Open-\nReview.net.\nXikun Zhang, Deepak Ramachandran, Ian Tenney,\nYanai Elazar, and Dan Roth. 2020. Do language\nembeddings capture scales? In Proceedings of\nthe 2020 Conference on Empirical Methods in\nNatural Language Processing: Findings, pages\n4889–4896.\nA Appendix\nWe heavily rely on Hugging Face’s Transformers\nlibrary (Wolf et al., 2020) for all experiments in-\nvolving the PLMs. We used Weights & Biases\nfor tracking and logging the experiments (Biewald,\n2020). Finally, we used sklearn (Pedregosa et al.,\n2011) for other ML-related experiments.\nB Paraphrases Analysis\nWe provide a characterization of the paraphrase\ntypes included in our dataset.\nWe analyze the type of paraphrases in\nPARA REL\n . We sample 100 paraphrase pairs\nfrom the agreement evaluation that were labeled\nas paraphrases and annotate the paraphrase type.\nNotice that the paraphrases can be complex, as\nsuch, multiple transformations can be annotated for\neach pair. We mainly use a subset of paraphrase\ntypes categorized by Bhagat and Hovy (2013), but\nalso deﬁne new types which were not covered by\nthat work. We begin by brieﬂy deﬁning the types\nof paraphrases found in PARA REL\n from Bhagat\nand Hovy (2013) (more thorough deﬁnitions can\nbe found in their paper), and then deﬁne the new\ntypes we observed.\n1. Synonym substitution: Replacing a\nword/phrase by a synonymous word/phrase,\nin the appropriate context.\n2. Function word variations: Changing the func-\ntion words in a sentence/phrase without affect-\ning its semantics, in the appropriate context.\n3. Converse substitution: Replacing a\nword/phrase with its converse and inverting\nthe relationship between the constituents of a\nsentence/phrase, in the appropriate context,\npresenting the situation from the converse\nperspective.\n4. Change of tense: Changing the tense of a verb,\nin the appropriate context.\n5. Change of voice: Changing a verb from its\nactive to passive form and vice versa results in\na paraphrase of the original sentence/phrase.\n6. Verb/Noun conversion: Replacing a verb by\nits corresponding nominalized noun form and\nvice versa, in the appropriate context.\n7. External knowledge: Replacing a word/phrase\nby another word/phrase based on extra-\nlinguistic (world) knowledge, in the appro-\npriate context.\n8. Noun/Adjective conversion: Replacing a verb\nby its corresponding adjective form and vice\nversa, in the appropriate context.\n9. Change of aspect: Changing the aspect of a\nverb, in the appropriate context.\nWe also deﬁne several other types of paraphrases\nnot covered in Bhagat and Hovy (2013) (potentially\nbecause they did not occur in the corpora they have\ninspected).\na. Irrelevant addition: addition or removal of a\nword or phrase, that does not affect the mean-\ning of the sentence (as far as the relation of in-\nterest is concerned), and can be inferred from\nthe context independently.\nb. Topicalization transformation: a transforma-\ntion from or to a topicalization construction.\nTopicalization is a construction in which a\nclause is moved to the beginning of its enclos-\ning clause.\nParaphrase Type Pattern #1 Pattern #2 Relation N.\nSynonym substitution [X] died in [Y]. [X] expired at [Y]. place of death 41Function words variations [X] is [Y] citizen. [X], who is a citizen of [Y]. country of citizenship 16Converse substitution [X] maintains diplomatic relations with [Y]. [Y] maintains diplomatic relations with [X]. diplomatic relation 10Change of tense [X] is developed by [Y]. [X] was developed by [Y]. developer 10Change of voice [X] is owned by [Y]. [Y] owns [X]. owned by 7Verb/Noun conversion The headquarter of [X] is in [Y]. [X] is headquartered in [Y]. headquarters location 7External knowledge [X] is represented by music label [Y]. [X], that is represented by [Y]. record label 3Noun/Adjective conversion The ofﬁcial language of [X] is [Y]. The ofﬁcial language of [X] is the [Y] language. ofﬁcial language 2Change of aspect [X] plays in [Y] position. playing as an [X], [Y] position played on team 1\nIrrelevant addition [X] shares border with [Y]. [X] shares a common border with [Y]. shares border with 11Topicalization transformation [X] plays in [Y] position. playing as a [Y], [X] position played on team 8Apposition transformation [X] is the capital of [Y]. [Y]’s capital, [X]. capital of 4Other syntactic movements [X] and [Y] are twin cities. [X] is a twin city of [Y]. twinned administrative body 10\nTable 8: Different types of paraphrases in PARA REL\n . We report examples from each paraphrase type,\nalong with the type of relation, and the number of examples from the speciﬁc transformation from a\nrandom subset of 100 pairs. Each pair can be classiﬁed into more than a single transformation (we report\none for brevity), thus the sum of transformation is more than 100.\nc. Apposition transformation: a transformation\nfrom or to an apposition construction. In\nan apposition construction, two noun phrases\nwhere one identiﬁes the other are placed one\nnext to each other.\nd. Other syntactic movements: includes other\ntypes of syntactic transformations that are not\npart of the other categories. This includes\ncases such as moving an element from a coor-\ndinate construction to the subject position as\nin the last example in Table 8. Another type of\ntransformation is in the following paraphrase:\n“[X] plays in [Y] position.” and “[X] plays\nin the position of [Y].” where a compound\nnoun-phrase is replaced with a prepositional\nphrase.\nWe report the percentage of each type, along\nwith examples of paraphrases in Table 8. The most\ncommon paraphrase is the ‘synonym substitution’,\nfollowing ‘function words variations’ which oc-\ncurred 41 and 16 times, respectively. The least\ncommon paraphrase is ‘change of aspect’, which\noccurred only once in the sample.\nThe full PARA REL\n resource can be found\nat: https://github.com/yanaiela/\npararel/tree/main/data/pattern_\ndata/graphs_json.",
  "topic": "Consistency (knowledge bases)",
  "concepts": [
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.8746464252471924
    },
    {
      "name": "Computer science",
      "score": 0.7250902652740479
    },
    {
      "name": "Variance (accounting)",
      "score": 0.6186530590057373
    },
    {
      "name": "Property (philosophy)",
      "score": 0.6095302700996399
    },
    {
      "name": "Natural language processing",
      "score": 0.6004977226257324
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5759719014167786
    },
    {
      "name": "Meaning (existential)",
      "score": 0.5505424737930298
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5148407816886902
    },
    {
      "name": "Style (visual arts)",
      "score": 0.4344167709350586
    },
    {
      "name": "Psychology",
      "score": 0.1626165211200714
    },
    {
      "name": "Epistemology",
      "score": 0.07316520810127258
    },
    {
      "name": "History",
      "score": 0.05804404616355896
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Accounting",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    }
  ]
}