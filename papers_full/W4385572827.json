{
  "title": "CPL: Counterfactual Prompt Learning for Vision and Language Models",
  "url": "https://openalex.org/W4385572827",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2228361739",
      "name": "Xuehai He",
      "affiliations": [
        "University of California, Santa Cruz"
      ]
    },
    {
      "id": "https://openalex.org/A2909773484",
      "name": "Diji Yang",
      "affiliations": [
        "University of California, Santa Cruz"
      ]
    },
    {
      "id": "https://openalex.org/A2231959515",
      "name": "Weixi Feng",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A4314316502",
      "name": "Tsu-Jui Fu",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A5020005244",
      "name": "Arjun Akula",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1235729806",
      "name": "Varun Jampani",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2611786991",
      "name": "Pradyumna Narayana",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2169880767",
      "name": "Sugato Basu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2110566271",
      "name": "William Yang Wang",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2045332956",
      "name": "Xin Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2887997457",
    "https://openalex.org/W3104788521",
    "https://openalex.org/W4226150044",
    "https://openalex.org/W3207941483",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W12634471",
    "https://openalex.org/W4285191490",
    "https://openalex.org/W3035512383",
    "https://openalex.org/W3202933889",
    "https://openalex.org/W3103857453",
    "https://openalex.org/W4287812705",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3035517717",
    "https://openalex.org/W4301372783",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W1576445103",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4312480274",
    "https://openalex.org/W3170914142",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4226345839",
    "https://openalex.org/W4306755431",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W3102363610",
    "https://openalex.org/W3196778703",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2977235550",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W4312310776",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W3212456749",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W3200253633",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W3104591237",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4287113019",
    "https://openalex.org/W3181158454",
    "https://openalex.org/W3206477178",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3099641295",
    "https://openalex.org/W2995191098",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W3106806814",
    "https://openalex.org/W4287121067",
    "https://openalex.org/W2936695845"
  ],
  "abstract": "Xuehai He, Diji Yang, Weixi Feng, Tsu-Jui Fu, Arjun Akula, Varun Jampani, Pradyumna Narayana, Sugato Basu, William Yang Wang, Xin Wang. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3407–3418\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nCPL: Counterfactual Prompt Learning for Vision and Language Models\nXuehai He1 Diji Yang1 Weixi Feng2 Tsu-Jui Fu2 Arjun Akula3 Varun Jampani3\nPradyumna Narayana3 Sugato Basu3 William Yang Wang2 Xin Eric Wang1\n1UC Santa Cruz, 2UC Santa Barbara, 3Google\n{xhe89,dyang39,xwang366}@ucsc.edu\n{weixifeng,tsu-juifu,william}@ucsb.edu\n{arjunakula,varunjampani,pradyn,sugato}@google.com\nAbstract\nPrompt tuning is a new few-shot transfer\nlearning technique that only tunes the learn-\nable prompt for pre-trained vision and lan-\nguage models such as CLIP. However, ex-\nisting prompt tuning methods tend to learn\nspurious or entangled representations, which\nleads to poor generalization to unseen con-\ncepts. Towards non-spurious and efficient\nprompt learning from limited examples, this\npaper presents a novel Counterfactual Prompt\nLearning (CPL) method for vision and lan-\nguage models, which simultaneously employs\ncounterfactual generation and contrastive learn-\ning in a joint optimization framework. Partic-\nularly, CPL constructs counterfactual by iden-\ntifying minimal non-spurious feature change\nbetween semantically-similar positive and neg-\native samples that causes concept change and\nlearns more generalizable prompt representa-\ntion from both factual and counterfactual exam-\nples via contrastive learning. Extensive exper-\niments demonstrate that CPL can obtain supe-\nrior few-shot performance on different vision\nand language tasks than previous prompt tuning\nmethods on CLIP. On image classification, we\nachieve a 3.55% average relative improvement\non unseen classes across seven datasets; on\nimage-text retrieval and visual question answer-\ning, we gain up to 4.09% and 25.08% relative\nimprovements across three few-shot scenarios\non unseen test sets respectively.1\n1 Introduction\nPre-trained vision and language foundation mod-\nels (Radford et al., 2021; Jia et al., 2021) have\nshown encouraging results toward open-domain\nvisual-concept matching. Benefiting from prompt\nengineering (Song et al., 2022a; Liu et al., 2022),\nwhere free-form text prompts are designed for spe-\ncific task goals, those foundation models can be\neasily transferred to a wide array of tasks under\n1Our code is released at https://github.com/\neric-ai-lab/CPL.\nA:Alargelongtrainonasteeltrack B:Alargelongtrainonasteeltracknearabarn\nWhatifweaddabarntoimageA(orremovethebarnfromimageB)?Willthepromptbechanged?\nFigure 1: A conceptual overview of counterfactual\nprompt learning. CPL constructs counterfactuals by\nidentifying non-spurious feature change that causally\ncauses the prompt change. In this case, the “barn” fea-\nture is the essential cause between Prompt A and B.\nzero-shot and few-shot scenarios, including im-\nage classification (Deng et al., 2009), visual ques-\ntion answering (Shen et al., 2021), image-text re-\ntrieval (Jia et al., 2021), etc. But manually con-\nstructing prompts for vision and language models\nsuch as CLIP is a tedious, time-consuming process,\nwhich usually requires prior domain knowledge\nand leads to suboptimal solutions.\nPrompt tuning (Lester et al., 2021), on the other\nhand, liberates us from manual prompt engineering\nand automates this process. Prompt tuning meth-\nods (Ju et al., 2021; Lin et al., 2014; Zhou et al.,\n2022) are proposed to effectively transfer CLIP to\nimage recognition tasks after tuning a learnable\nprompt with a few examples of the classes. How-\never, those methods purely conduct empirical risk\nminimization (ERM) and optimize for predictive\naccuracy, which often produces spurious, ineffi-\ncient, or entangled representations (Wang and Jor-\ndan, 2021). Therefore, the generalization ability\nof existing prompt tuning methods for vision and\nlanguage models is limited, and they often fail to\ntransfer well to unseen classes or concepts. For\n3407\nexample, the image classification performance of\nthe SOTA method CoCoOp (Zhou et al., 2022) is\nsimilar or even degrades on unseen classes when\ncompared with zero-shot CLIP.\nLearning non-spurious representation for better\ngeneralization requires disentangling features that\ncausally determine the prompts. One solution is\ncounterfactual reasoning. Counterfactual (“counter\nto the facts”) is a concept that describes the human\ncapacity to learn from limited prior experiences by\nimagining the outcome of an alternative action that\ncould have been taken. So we can do counterfac-\ntual intervention by asking “what if ...” questions\nin prompt learning. For example, as shown in Fig-\nure 1, a change in the visual feature of the barn\nwould cause the label to change (if we view the\ntwo prompts as two labels).\nTherefore, we introduce a new causality-based\napproach, Counterfactual Prompt Learning (CPL),\nfor non-spurious and efficient prompt learning.\nFirst, we introduce a text-based negative sampling\nstrategy to discover the most semantically-similar\nnegative sample based on text similarity. Then\nwe generate a counterfactual example by identify-\ning minimal non-spurious feature change between\nsemantically-similar positive and negative samples\nthat causally causes prompt change. Finally, we\nadopt contrastive learning in the joint optimization\nframework (with counterfactual construction) to\ntune the learnable prompts using both factual and\ncounterfactual examples. The causally fine-tuned\nprompts will eventually guide vision-and-language\nfoundation models to distinguish images from un-\nseen concepts, thereby improving the generaliza-\ntion ability of prompt learning.\nWe extensively evaluate CPL using seven stan-\ndard datasets for image classification, two for\nimage-text-retrieval, and one for visual question\nanswering (VQA). We show that CPL outper-\nforms the baseline on all three tasks: on im-\nage classification, our method achieves 3.55%\naverage relative improvement on unseen classes\nacross the seven datasets in terms of accuracy;\non image-text retrieval, our method improves the\nmost ( 4.09% relative improvement in terms of\nRecall@1) when using 0.5% of total training\ninstances on MSCOCO (Lin et al., 2014) and\nFlickr30K (Plummer et al., 2015); on VQA, we\ngain up to 25.08% relative improvement on the\nVQAv2 (Goyal et al., 2017a) dataset.\nOur main contributions are summarized below:\n• We introduce Counterfactual Prompt\nLearning (CPL), a task-agnostic causality-\nbased prompt learning method to effectively\ntransfer CLIP to unseen concepts for different\ndownstream tasks.\n• We propose a text-based negative sam-\npling strategy, where we compute\nBERTScore (Zhang et al., 2019) between text\nprompts, based on which we sample the most\nsemantically-similar negative images.\n• We introduce a optimization framework that\nsimultaneously constructs counterfactuals by\nidentifying minimal non-spurious feature\nchange, and learns the generalized prompt rep-\nresentation from both factual and counterfac-\ntual examples.\n• We conduct extensive experiments on image\nclassification, image-text retrieval, and visual\nquestion answering, and validate the superior-\nity of CPL to existing prompt tuning methods\nin transferring effectiveness on unseen con-\ncepts.\n2 Related Work\nVision-and-Language Models. Vision-and-\nLanguage models pre-trained on large-scale\nimage-text pairs have demonstrated great potential\nin multimodal representation learning (Jia et al.,\n2021; Yao et al., 2021; Yuan et al., 2021). Among\nthem, the representative CLIP (Radford et al.,\n2021) benefits from 400M curated data and defines\nvarious prompt templates to carry out zero-shot\nimage classification. However, those prompts still\nrequire hand-crafted designs. In this work, we\nautomatically learn task-agnostic and task-relevant\nprompts without human priors. In addition, by\nconsidering the counterfactual examples, we\ncan further improve various vision-and-language\ntasks, including visual question answering and\nimage-text retrieval in a few-shot scenario.\nPrompt Tuning. Many works focus on learning\nfrom discrete natural language prompts, e.g., Auto-\nPrompt (Shin et al., 2020) elicits knowledge from\nlanguage models with automatically generated dis-\ncrete prompts. Lately, many other works (Zhou\net al., 2021, 2022) directly tune prompts in con-\ntinuous vector forms. Guo et al. (2021) intro-\nduces Q-Learning to optimize the soft prompt. P-\nTuning v2 (Liu et al., 2021) shows that continuous\n3408\nprompt tuning achieves the same performance as\nfine-tuning in various settings. Prompt tuning also\nreceives great interest in the computer vision do-\nmain. For example, CoOp proposes a continuous\nprompt optimization strategy to avoid prompt de-\nsign. CoCoOp (Zhou et al., 2022) extends CoOp by\nfurther learning an instance-conditional network to\ngenerate an input-conditional token for each image.\nHowever, these methods trained with empirical risk\nminimization (ERM) may learn to rely on correla-\ntions between class labels and spurious attributes\nby minimizing average training error (Zhang et al.,\n2022). They usually learn spurious, inefficient, and\nentangled representation, lacking generalization\nability to unseen scenarios.\nCounterfactual Reasoning. A number of re-\ncent works have investigated generating counterfac-\ntual images (Besserve et al., 2020), or counterfac-\ntual text in specific language domains (e.g., court\nview (Wu et al., 2020), dialogue generation (Zhu\net al., 2020), Natural Language Inference (Kaushik\net al., 2019; Gokhale et al., 2021), named entity\nrecognition (Zeng et al., 2020)); On the vision end,\nZhang et al. (2021) proposes to add intervention\nover the changed domain on images during the\ndata-generation process and steer the generative\nmodel to produce counterfactual features to aug-\nment the training process. Agarwal et al. (2020)\nuses automated semantic image manipulations to\ngenerate synthetic data to make models more ro-\nbust against spurious correlations; On the vision\nand language end, Chen et al. (2020) proposes to\ngenerate counterfactual VQA samples by masking\ncritical objects in images or words in questions to\naugment the training data and gain a huge improve-\nment on the VQAv2 dataset. Gokhale et al. (2020)\nproposes template-based counterfactual image aug-\nmentation methods. Fu et al. (2020) proposes a\nnovel training strategy for visual language naviga-\ntion that dynamically generates counterfactuals to\naccount for unseen scenarios. To our best knowl-\nedge, CPL is the first to apply counterfactual gener-\nation to prompt-based few-shot learning for vision\nand language models.\nFew-shot Learning. Recently, several few-shot\nefficient learners on vision (He et al., 2022) and lan-\nguage (Brown et al., 2020) tasks were proposed in-\ncluding CLIP. GPT (Brown et al., 2020), as a strong\nfew-shot learner, is capable of performing a new\nlanguage task by learning from only a few training\ninstances. Frozen (Tsimpoukelli et al., 2021) is de-\nveloped based on GPT and made into a multimodal\nfew-shot learner by expanding the soft prompting\nto include a collection of images and text. Their\nmethod demonstrates strong few-shot capabilities\non visual question answering and image classifi-\ncation tasks. Similarly, CoCa (Yu et al., 2022) is\npre-trained from scratch and end-to-end using both\nweb-scale data and annotated images by consider-\ning all labels as text, therefore unifying supervi-\nsion for learning representations through natural\nlanguage. It can achieve state-of-the-art perfor-\nmance with few-shot transfer or by minimal task-\nspecific adaptation on a wide range of downstream\nvision-and-language tasks, including visual recog-\nnition, multimodal understanding, crossmodal re-\ntrieval, and image captioning. SimVLM (Wang\net al., 2021b) is pre-trained with prefix language\nmodeling on datasets with weak supervision. It\nexhibits its efficacy on few-shot captioning tasks.\nEven though all these models mentioned above can\nalready achieve improvement on some few-shot\ntasks, how to exploit their few-shot reasoning abil-\nity using limited training examples still deserves\nthe effort. In this work, we study this direction\nvia the lens of prompt learning utilizing CLIP as a\nstarting point.\n3 Counterfactual Prompt Learning\n3.1 Problem Formulation\nOur goal is to learn generalizable prompt repre-\nsentation with limited data. The prompt in CLIP\nis divided into two parts: task-agnostic prompt p\nand task-relevant prompt h. Task-agnostic prompt\np is learned end-to-end automatically. The set of\ntask-relevant prompts H = {h0,h1,..., hC}is\nmapped from the label space Y with some prede-\nfined rules hinging on the task type, where Cis the\ntotal number of classes. The final prompt tc is the\nconcatenation of the task-agnostic prompt and the\ntask-relevant prompt fed into CLIP’s text encoder:\ntc = [p,hc].\nExisting works to this problem (Zhou et al., 2021,\n2022) propose to first extract visual feature v of\neach input image by feeding it into CLIP’s vision\nencoder F; and text embeddings are generated by\nfeeding {tc}C\nc=1 into the CLIP’s text encoder G.\nThe probability of i-th class is computed as\np(ti |x) = e\n<G(ti),v>\nτ\n∑C\nc=1 e\n<G(tc),v>\nτ\n, (1)\n3409\nFigure 2: The counterfactual prompt learning framework. We freeze the vision encoder F and the text encoder\nG, and only optimize the task-agnostic prompts and the instance-conditioned net M (blue blocks). Please refer to\nSection 3.2 for the explanation.\nwhere τ is the temperature parameter, < ·>de-\nnotes the cosine similarity. Cross-entropy loss is\nthen minimized and the gradients can be back-\npropagated via the text encoder G to update the\nlearnable prompt representation p. During training,\nthe weights of CLIP always remain frozen. During\ninference, Eq. 1 is used to compute the probability\nfor each class.\n3.2 Method Overview\nAn overview of the Counterfactual Prompt Learn-\ning (CPL) framework is shown in Figure 2. For\npre-processing, we construct task-relevant prompts\nfor all training samples. The goal is to optimize\nthe task-agnostic prompt p.2 During training,\ngiven a positive image-prompt pair, we first per-\nform text-based negative samplingto find the most\nsemantically-similar negative sample based on text\nsimilarity scores. Then we adopt a controllable\ncounterfactual generationstrategy to construct the\ncounterfactual from the positive and negative sam-\nples in the visual feature space. Finally, we perform\ncontrastive learning using both generated counter-\nfactual image features and factual image features\nin a joint optimization framework to fine-tune the\ntask-agnostic prompt p, allowing the model to un-\n2Together with the instance-conditional net M as intro-\nduced in Zhou et al. (2022). For simplicity, we will only use\np hereafter as p and M are always optimized together.\nderstand non-spurious semantic information and\nlearn generalized prompt representations.\n3.3 Controllable Counterfactual Generation\nBy viewing image feature v as a potential cause\nof the label, a non-spurious feature shall be a suf-\nficient cause of the label. So we would like to\ngenerate counterfactuals by identifying minimal\nnon-spurious feature change that causes the label\nchange. The illustration of the counterfactual con-\nstruction process is shown in Figure 3. Given posi-\ntive image features v and negative image features\nv−, we can generate negative counterfactual image\nfeatures v′as below:\nv′= (1−u) ◦v + u ◦v−, (2)\nwhere ◦is the element-wise multiplication and u\nis the parameter controlling the amount of nega-\ntive image feature that replaces the positive image\nfeature. The negative image features are extracted\nfrom those images similar to the original image\nat the semantic level, which we will introduce in\nSection 3.4.\nTo capture the non-spuriousness, we would like\nto construct counterfactuals by replacing essential\nnon-spurious features only. This can be achieved\nby minimizing the amount of feature change u∗\nto the original image that can causally incur label\n3410\n3\n+  =\nFigure 3: Counterfactual generation process. v and\nc are the positive image feature and label, while v−\nand c− are the negative image feature and label. ◦is\nelement-wise multiplication. By mixing v and v−, the\ncounterfactual image feature v′ is predicted as a nega-\ntive label c− by the discriminator D. u is minimized\nso a minimal change to the positive image feature u is\ncaptured here to causally change the label.\nchange:\nminimize\nu∗ ∥u∗∥1\ns.t. u∗ = arg max\nu\nDc−(v′). (3)\nGiven the factual and counterfactual features v\nand v′, we aim to learn the prompt that can help\nCLIP better align visual features v and textual fea-\ntures G(t) with same semantic meanings. This can\nbe achieved by maximizing the mutual information\n(MI) between v and G(t). Therefore, by minimiz-\ning the InfoNCE loss (Hjelm et al., 2018), we can\nmaximize the lower bound on MI(v,G(t)). To this\nend, we define the contrastive objective function\nbased on the InfoNCE estimator following Khosla\net al. (2020):\nLCL(p,u∗) =−log( e\nS(v,G(t))\nτ\ne\nS(v,G(t))\nτ + e\nS(v′,G(t))\nτ\n),\n(4)\nwhere S(·,·) is normally the cosine similarity func-\ntion and τ is the temperature value.\n3.4 Text-based Negative Sampling\nWe then discuss how to perform negative sampling\nfor constructing counterfactual features. As sug-\ngested in Robinson et al. (2020), good negative\nsamples have different labels and are difficult to be\ndistinguished from an anchor point, while their se-\nmantic representations are close (Suresh and Ong,\n2021). Since not all negative samples can serve\nas useful negatives (Chuang et al., 2020), indis-\ncriminate leverage of these data may harm model\nrobustness and algorithm efficiency. Therefore, dur-\ning training, in each batch, we only utilize the most\nsemantically-similar one to generate counterfactual\nimage features. Other image samples are filtered\nout.\nSemantic concepts may be highly complex in the\nvisual representations, and thus it is hard to directly\nmeasure semantic similarity in the visual space.\nWhile language is more expressive and naturally\npreserves semantic meanings. Therefore, we pro-\npose a text-based negative sampling method. We\nfirst measure the text similarity between prompts\nwith BERTScore (Zhang et al., 2019), which com-\nputes pairwise cosine similarity between reference\nsentences and candidate sentences using BERT\ncontextual embedding (Devlin et al., 2019). We\ncompute a similarity matrix with the value of each\nelement being:\nsim(i,j) = BERTScore(hi,hj). (5)\nDenote Bas the collection of sampled instances.\nDuring training, each prompt hc ∈B (1 ≤c≤C,\nwhere C is the size of sampled instances) can be\ntreated as a query. Given a query prompt hq, its\nmost semantically similar prompt (the one with\nthe highest BERTScore) hk is searched from B.\nThen we use the CLIP vision encoder to obtain the\nfeatures of the corresponding positive and negative\nimages v and v−.\n3.5 Joint Optimization\nIn addition to the contrastive learning loss as intro-\nduced in Eq. 4, we also adopt the standard cross-\nentropy loss for training:\nLCE(p) =−\n∑\nc\nyc log p(tc |x) , (6)\nwhere yc denotes the one-hot ground-truth an-\nnotation of the label. We treat all downstream\ntasks in this work as classification tasks, where\nthe model predicts if the image and text prompt\npair is matched or not.\nThen the task-agnostic prompt p is learned\nby minimizing the weighted combination of con-\ntrastive learning loss and cross-entropy loss:\nL(p) =LCE (p) +λ·LCL(p,u∗), (7)\nwhere λdetermines the weight of LCL.\nIn fact, we can seek to put Eq. 3 and Eq. 7 in\na single-stage optimization framework. The in-\ntuition is that we generate counterfactual image\n3411\nAlgorithm 1 Counterfactual Prompt Learning\n1: X: image space\n2: Y: label space\n3: hc: task-relevant prompt for the c-th class\n4: H: the set of task-relevant prompts\n5: p: the task-agnostic prompt\n6: v: image features\n7: v−: negative image features\n8: u: parameter controls the generation of counterfactual\nimage features\n9: function CPL(X, Y)\n10: H ←Y\n11: tc ←[p, hc]\n12: for each i, jdo\n13: sim(i, j) = BERTScore(hi, hj) ▷ Eq. 5\n14: end for\n15: for q in the batch do\n16: v ←vq\n17: Find the index k that maximize sim(q, k) with the\ngiven index q\n18: v− ←vk\n19: Generate counterfactual image features ▷ Eq. 2\n20: LCE ←cross-entropy loss ▷ Eq. 6\n21: LCL ←contrastive loss ▷ Eq. 4\n22: Update p and u with the joint optimization loss ▷\nEq. 7\n23: end for\n24: end function\nfeatures with minimal feature change that can max-\nimize the negative prediction probability, and at\nthe same time, utilize contrastive learning to learn\nthe prompt that can guide CLIP to explicitly distin-\nguish between factual images and counterfactual\nimages. Putting all pieces together, we have:\nminimize\np,u∗ LCE (p) +λ·LCL(p,u∗) +∥u∗∥1\ns.t. u∗ = arg max\nu\nDc−(v′)\nwhere v′ = (1−u) ◦v + u ◦v−.\n(8)\nIn Eq. 8, the gradients can be back-propagated all\nthe way through the text encoder G to the task-\nagnostic prompt, making use of the rich knowledge\nencoded in the pre-trained CLIP model to optimize\nthe prompt.\nAlgorithm 1 presents the learning algorithm of\nCPL. In summary, given few input training samples\n{(x1,y1) ,..., (xn,yn)}, CPL consists of three\nmain steps: (1) compute the similarity matrix be-\ntween different text prompts within the sampled\nbatch; (2) generate counterfactual image features;\n(3) optimize p and u with contrastive learning loss\nand cross-entropy loss.\n3.6 Task-relevant Prompt Construction\nWe construct task-relevant prompts H for image\nclassification, image-text retrieval, and visual ques-\ntion answering, respectively. For image classifi-\ncation, the prompts are class labels for each task;\nfor image-text retrieval, captions for each image\nare adopted as prompts; for visual question an-\nswering, we first use a pre-trained generative T5\nmodel (Raffel et al., 2019) to convert the question-\nanswer pairs into declarative sentences referring\nto the VQA prompt generation method proposed\nin Song et al. (2022b). Then, motivated by Wei et al.\n(2022), we add additional category information into\nthe prompt generated from templates based on the\nquestion type to help the model perform interme-\ndiate reasoning steps. Specifically, we add “The\nquestion is asking about others” for Other ques-\ntions before the generated declarative sentence. In\na similar vein, “The question is asking about yes\nor no” and “The question is asking about numbers”\nare added for Yes/No and Number questions.\n4 Experiments\n4.1 Tasks and Datasets\nImage Classification. We employ seven pub-\nlicly available image classification datasets used\nin CLIP: SUN397 (Xiao et al., 2010), Cal-\ntech101 (Griffin et al., 2007), ImageNet (Deng\net al., 2009), OxfordPets (Parkhi et al., 2012),\nStandfordCars (Krause et al., 2013), Flow-\ners102 (Nilsback and Zisserman, 2008), and\nFood101 (Bossard et al., 2014). These datasets\nconstitute a comprehensive benchmark, which cov-\ners a diverse set of vision tasks including the clas-\nsification of generic objects, fine-grained image\nrecognition, action classification, etc. To evaluate\nthe generalization ability of methods, we split those\ndatasets into seen and unseen classes. Only images\nin the seen classes will be used for training. The\nsetting follows the few-shot evaluation protocol in\nCLIP, where we use 16 shots for training and full\ntest sets for testing.\nImage-Text Retrieval. We consider two datasets\nfor image-text retrieval: MSCOCO (Lin et al.,\n2014) and Flickr30K (Plummer et al., 2015). We\nadopt the widely used Karpathy split (Karpathy\nand Fei-Fei, 2015) for both the MSCOCO and\nFlickr30K datasets, where MSCOCO contains\n113/5K/5K for train/validation/test. Flickr30K con-\ntains 29K/1K/1K images for train/validation/test.\nWe construct few-shot setting subsets for both Co-\nCoOp and CPL by taking 0.5%, 1%, and 3% of\ntraining instances. We train the model with the sub-\nsets and evaluate its performance on the complete\n3412\nClasses Method SUN397 Caltech101 ImageNet OxfordPets StanfordCars Flowers102 Food101 Average\nSeen\nCLIP 69.40 96.51 72.46 91.33 74.85 72.17 90.12 80.98\nCoCoOp 79.08 [+13.95] 97.66 [+1.19] 76.01 [+4.90] 95.18 [+4.22] 70.91 [-5.26]94.65[+31.15] 90.67 [+0.61] 86.31 [+6.58]\nCPL (ours)81.05[+16.79]97.70[+1.23]78.81[+8.76]96.69[+5.87]75.51[+0.88] 93.91 [+30.12]93.01[+3.21]88.10[+8.79]\nUnseen\nCLIP 75.40 94.10 68.09 97.04 74.95 77.87 91.30 82.68\nCoCoOp 76.83 [+1.90] 93.92 [-0.19] 70.44 [+3.45] 97.78 [+0.76] 73.09 [-2.48] 69.24 [-11.08] 91.53 [+0.25] 81.83 [-1.02]\nCPL (ours)80.19[+6.35]94.94[+0.89]73.17[+7.46]98.81[+1.82]78.90[+5.27] 72.30 [-7.15]93.44[+2.34]84.54[+2.25]\nTable 1: Result comparison between CPL and CoCoOp (Zhou et al., 2022) on seen and unseen classes across\nseven image classification datasets in terms of accuracy (%) under the few-shot setting. The relative difference (%)\ncompared with CLIP is reported in color.\nTraining data used Method Flickr30k MSCOCO Average\n0 CLIP 83.00 53.35 68.18\n0.5% CoCoOp 82.40 [-0.72] 55.55 [+4.12] 68.98 [+1.17]CPL (ours)85.64[+3.18]57.91[+8.55]71.78[+5.28]\n1% CoCoOp 84.80 [+2.17] 56.62 [+6.13] 70.71 [+3.71]CPL (ours)86.91[+4.71]58.43[+9.52]72.67[+6.59]\n3% CoCoOp 85.90 [+3.49] 58.08 [+8.87] 71.99 [+5.59]CPL (ours)87.74[+5.71]59.96[+12.39]73.85[+8.32]\nTable 2: Result comparison between CPL and CoCoOp\non two image-text retrieval datasets, Flickr30k (Plum-\nmer et al., 2015) and MSCOCO (Lin et al., 2014), on the\nunseen test sets in terms of Recall@1 (%). The relative\ndifference (%) over CLIP is reported in color.\nTraining data used Method VQAv2\n0 CLIP 11.83\n0.5%\nCoCoOp 27.98 [+136.52]\nCPL w/o. Category Information 31.68 [+167.79]\nCPL 33.39[+182.25]\n1%\nCoCoOp 28.51 [+141.00]\nCPL w/o. Category Information 34.70 [+193.32]\nCPL 35.66[+201.44]\n3%\nCoCoOp 30.18 [+155.11]\nCPL w/o. Category Information 35.41 [+199.32]\nCPL 36.32[+207.02]\nTable 3: Result comparison on the VQAv2\ndataset (Goyal et al., 2017a) in terms of accuracy (%).\nThe relative improvements over CLIP are reported in\ncolor. Incorporating category information into task-\nrelevant prompts can further improve the performance.\ntest set. We use Recall at 1 (R@1) as the default\nevaluation metric.\nVisual Question Answering. VQAv2 (Goyal\net al., 2017b) is an extended dataset from the\nVQA (Antol et al., 2015) dataset. The questions are\ncategorized into three types: Number, Yes/No, and\nOther. We set up the experiments following An-\nderson et al. (2018), which treats visual question\nanswering as a classification problem: for each\nquestion, the model picks the corresponding an-\nswer from a given set of predefined most frequent\ncandidate answers and matches it with the image.\nThe questions are first converted into a masked\ntemplate using the pre-trained T5 model and pre-\ndefined rules. The infilled template along with the\nquestions will be turned into prompts that naturally\nconnect questions and answers. The model will\npredict whether the given prompt and image pairs\nare matched. We construct the few-shot setting by\ntaking 0.5%, 1%, and 3% instances for training.\n4.2 Implementation Details\nBaselines. We mainly compare CPL with Co-\nCoOp (Zhou et al., 2022), one of the earliest prompt\ntuning methods proposed for vision-and-language\npre-trained models. CoCoOp considers each input\nimage and injects the learnable instance-aware to-\nkens into the context vectors as the final prompt.\nFor a fair comparison, both CPL and CoCoOp\nadopt CLIP (Radford et al., 2021) as the pre-trained\nvision-and-language backbone and are compared\nwith respect to their relative improvements over\nzero-shot CLIP.\nPrompt Tuning. The task-agnostic prompt is ran-\ndomly initialized from a zero-mean Gaussian dis-\ntribution with the standard deviation 0.02, where\nwe set length L = 4 by default. For vision and\nlanguage tasks, in contrast to image classification,\nwhere an image is labeled by a category, the task-\nrelevant prompts comprise more fine-grained de-\ntails, usually a sentence. We here similarly to-\nkenize the whole sentence using the CLIP word\nembedding (Radford et al., 2021), and feed the tok-\nenized results to the text encoder with task-agnostic\nprompt vectors, to generate the language embed-\nding for each prompt. In both the image-text re-\ntrieval and visual question answering, all data in\nthe test set can be treated as belonging to unseen\nclasses.\n3413\n4.3 Main Results\nImage Classification. The experimental results\nfor image classification are shown in Table 1. With\nbetter prompts learned from counterfactual exam-\nples, our CPL method achieves clear advantages\nover CoCoOp for both seen and unseen classes\nacross almost all datasets. Particularly on unseen\nclasses, we gain an average relative improvement\nof 3.55%.\nMeanwhile, CoCoOp shows its poor generaliza-\ntion ability. Specifically, we found that CoCoOp\nperforms worse than CLIP on StandfordCars on\nboth seen and unseen classes, and on Caltech101\nand Flower102 on unseen classes, indicating that it\ntends to learn and leverage spurious relations and\ncould not generalize well on unseen classes in some\ncases. We believe all these mentioned above can\nbe sufficient evidence that the main idea of CPL,\nlearning non-spurious prompt representation can\naid CLIP adapting at test time, is practical.\nImage-Text Retrieval. Table 2 reports results on\nimage-text retrieval on the unseen test set. CPL\ncan beat the zero-shot CLIP consistently across\nthe three different settings, demonstrating that CPL\ncan also learn better prompt representation and\nmore effectively exploit the limited amount of data\non image-text retrieval. Meanwhile, CoCoOp per-\nforms even worse than CLIP on Flickr30k using\n0.5% training data, which suggests that a tiny quan-\ntity of training data for image-text retrieval can lead\nto spurious prompt representation if using naïve\ninstance-conditional prompt tuning method.\nVisual Question Answering. For visual ques-\ntion answering, the results are shown in Table 3.\nAs can be seen, CPL surpasses the baseline Co-\nCoOp with a relative improvement of up to25.08%\nwhen using 1% instances for training. This proves\nthe concept that CPL can be effective on more\ncomplicated vision-and-language tasks. In fact, vi-\nsual question answering is more challenging for\nzero-shot CLIP which is pre-trained for image-text\nmatching. During pre-training, CLIP sees most\nsentences similar to captions in image-text retrieval\nand those captions can be directly used as prompts;\nwhile for VQA, question-answer pairs have to be\nadapted into declarative prompts. Therefore, zero-\nshot CLIP has poor performance on VQA, but few-\nshot prompt tuning via CPL can help reduce the\nprompt domain gap significantly. Apart from the\nvanilla CPL method, we examined another variant\nPositive Examples BERTScore Sampled Random Sampled\nImage\nClassification\nImage-text\nRetrieval\nTiger cat\n(BERTScore = 0.9126)\nTabby cat\nJeep\n(BERTScore = 0.8556)\nA big bunch of ripe \nyellow bananas on \ndisplay\nThe plate is empty on \nthe table\n(BERTScore = 0.8908)\nBunches of bananas are \nneatly arranged on a \ndisplay\n(BERTScore = 0.9313)\nFigure 4: Visualization of the weights of the con-\ntroller parameter u on images. The first column is\nthe original positive examples; the second column is\nBERT-sampled negative examples; the third column is\nrandomly-sampled negative examples for comparison.\nThe BERTScore between the text prompts of positive\nexamples and sampled examples are shown at the bot-\ntom.\nof CPL where we do not add additional category\ninformation into the prompt (denoted as CPL w/o.\nCategory Information), the results indicate that con-\nstructing task-relevant prompts by adding categori-\ncal information contributes to the improvement.\n4.4 Ablation Analysis\nNegative Sampling. We compare the random\nsampling vs. BERTScore sampling over ImageNet\nfor image classification, MSCOCO for image-text\nretrieval, and VQAv2 for visual question answering\nin Table 4. With more challenging negative exam-\nples, BERTScore sampling leads to more effective\nprompt tuning and overbeats random sampling on\nall three tasks. The qualitative visualizations of\nthe two sampling strategies are shown in Figure 4,\nfrom which it can be seen that BERTScore-sampled\nimages are much more semantically similar to the\noriginal images.\nNon-spurious Feature Visualization. We visual-\nize the heatmap of the learned non-spurious feature\nweights in the image level in Figure 4. The weights\nare mainly centralized on the semantically mean-\ningful regions that are aligned to the text prompts.\nNumber of Shots in Image Classification. We\nthen study the effects of the number of shots on\nCPL for image classification. Following the few-\nshot evaluation protocol adopted in CLIP, we use\n4, 8, and 16 shots for training on ImageNet. From\nFigure 5, increasing the number of shots keeps\nimproving the performance of both two methods\n3414\nMethod ImageNet MSCOCO VQAv2\nRandom sampling 75.28 57.78 33.01\nBERTScore sampling76.02 58.43 35.66\nTable 4: Random sampling vs. BERTScore sampling\nfor CPL over three tasks. On ImageNet, we measure the\naverage accuracy across seen and unseen classes. On\nMSCOCO and VQAv2, we both use 1% instances for\nfew-shot learning.\n72.8372.94 73.17\n70.2570.32 70.44\nFigure 5: Accuracy comparison on ImageNet (Deng\net al., 2009) unseen classes under three different shots.\nCPL performs better than CoCoOp consistently and has\nlower standard errors.\non unseen classes. Meanwhile, CPL outperforms\nCoCoOp under the three different settings and has\nlower standard errors.\nContribution of Contrastive Learning. In Sec-\ntion 3, we use the coefficient λto weigh the con-\ntrastive learning loss and combine it with the cross-\nentropy loss. It is observed that the scale of con-\ntrastive learning loss is smaller, hence we try to use\na larger λto balance the two loss terms. Figure 6\nshows the average accuracy result across seen and\nunseen classes on the SUN397 dataset under four\ndifferent λvalues. Note that when λis zero, there\nis no contribution from the contrastive loss and the\nmethod actually learns the prompt using standard\ncross-entropy loss. From experimental results ob-\ntained on the SUN397 dataset, we can observe that\nusing λ= 1leads to the best performance.\n5 Conclusion\nIn this paper, we propose a Counterfactual\nPrompt Learning (CPL) framework to avoid time-\nconsuming prompt engineering and learn more gen-\neralizable prompt representation for vision and lan-\nguage models. We conduct abundant experiments\non seven widely used image classification datasets,\ntwo image-text retrieval datasets, and one visual\nquestion answering dataset. Our proposed CPL\n80.62\n77.96\n79.8280.34\nFigure 6: Ablation of four different λ values on the\nSUN397 dataset in terms of average accuracy (%). The\nperformance of CPL peaks at λ= 1.\nmethod outperforms the previous prompt tuning\nbaseline and the zero-shot CLIP across the three\ntasks. In the future, we plan to develop more so-\nphisticated methods based on CPL and extend CPL\nto other vision and language tasks.\nLimitations\nThere are fairness issues in large pre-trained vision\nand language models such as CLIP. The proposed\nprompt learning method in this study automatically\nlearns the prompt and does not address those issues\nin the pre-trained model. Considering the method\nis proposed for the few-shot setting, careful inspec-\ntion and tuning are also needed when testing our\nmethod on other biased datasets. The methodolo-\ngies proposed in Booth et al. (2021) and Wang et al.\n(2021a) may possibly be paired with CPL to po-\ntentially address the issues. Another limitation is\nthe absence of explainability in CPL, which is a\ncommon problem with existing soft prompt tun-\ning methods. Back-mapping tuned soft prompts\nrepresentation to natural language is a way for in-\nterpretation; however, due to the limited size of\nvocabulary used by CLIP during the training, prior\nmethods such as searching for the nearest words in\nthe embedding space can not accurately match the\nvector to natural language. Expanding the dictio-\nnary size for CLIP embedding or developing more\nadvanced back-mapping techniques can possibly\naddress the limitation.\nAcknowledgments\nWe would like to thank the support of the Google\nAds Faculty Research Award. We also thank the\nanonymous reviewers for their thought-provoking\ncomments. The views and conclusions contained in\nthis document are those of the authors and should\nnot be interpreted as representing the sponsor.\n3415\nReferences\nVedika Agarwal, Rakshith Shetty, and Mario Fritz. 2020.\nTowards causal vqa: Revealing and reducing spuri-\nous correlations by invariant and covariant semantic\nediting. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n9690–9698.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\n2018. Bottom-up and top-down attention for image\ncaptioning and visual question answering. In CVPR.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\nDevi Parikh. 2015. Vqa: Visual question answering.\nIn Proceedings of the IEEE international conference\non computer vision, pages 2425–2433.\nM Besserve, A Mehrjou, R Sun, and B Schölkopf. 2020.\nCounterfactuals uncover the modular structure of\ndeep generative models. In Eighth International Con-\nference on Learning Representations (ICLR 2020).\nBrandon M Booth, Louis Hickman, Shree Krishna Sub-\nburaj, Louis Tay, Sang Eun Woo, and Sidney K\nD’Mello. 2021. Bias and fairness in multimodal\nmachine learning: A case study of automated video\ninterviews. In Proceedings of the 2021 International\nConference on Multimodal Interaction, pages 268–\n277.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\n2014. Food-101 – mining discriminative components\nwith random forests. In European Conference on\nComputer Vision.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nLong Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shil-\niang Pu, and Yueting Zhuang. 2020. Counterfactual\nsamples synthesizing for robust visual question an-\nswering. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 10800–10809.\nChing-Yao Chuang, Joshua Robinson, Yen-Chen Lin,\nAntonio Torralba, and Stefanie Jegelka. 2020. Debi-\nased contrastive learning. Advances in neural infor-\nmation processing systems, 33:8765–8775.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai\nLi, and Li Fei-Fei. 2009. Imagenet: A large-scale\nhierarchical image database. In CVPR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL.\nTsu-Jui Fu, Xin Eric Wang, Matthew F Peterson, Scott T\nGrafton, Miguel P Eckstein, and William Yang Wang.\n2020. Counterfactual vision-and-language naviga-\ntion via adversarial path sampler. In European Con-\nference on Computer Vision, pages 71–86. Springer.\nTejas Gokhale, Pratyay Banerjee, Chitta Baral, and\nYezhou Yang. 2020. Mutant: A training paradigm for\nout-of-distribution generalization in visual question\nanswering. arXiv preprint arXiv:2009.08566.\nTejas Gokhale, Abhishek Chaudhary, Pratyay Baner-\njee, Chitta Baral, and Yezhou Yang. 2021. Se-\nmantically distributed robust optimization for\nvision-and-language inference. arXiv preprint\narXiv:2110.07165.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017a. Making the V in VQA\nmatter: Elevating the role of image understanding\nin Visual Question Answering. In Conference on\nComputer Vision and Pattern Recognition (CVPR).\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017b. Making the v in vqa\nmatter: Elevating the role of image understanding in\nvisual question answering. In CVPR.\nGregory Griffin, Alex Holub, and Pietro Perona. 2007.\nCaltech-256 object category dataset.\nHan Guo, Bowen Tan, Zhengzhong Liu, Eric P Xing,\nand Zhiting Hu. 2021. Text generation with efficient\n(soft) q-learning. arXiv preprint arXiv:2106.07704.\nXuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei\nYang, and Xin Eric Wang. 2022. Parameter-efficient\nfine-tuning for vision transformers. arXiv preprint\narXiv:2203.16329.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-\nMarchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. 2018. Learning deep\nrepresentations by mutual information estimation and\nmaximization. arXiv preprint arXiv:1808.06670.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision. In International Conference on\nMachine Learning, pages 4904–4916. PMLR.\nChen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and\nWeidi Xie. 2021. Prompting visual-language models\nfor efficient video understanding. arXiv preprint\narXiv:2112.04478.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3128–\n3137.\n3416\nDivyansh Kaushik, Eduard Hovy, and Zachary Lipton.\n2019. Learning the difference that makes a differ-\nence with counterfactually-augmented data. In Inter-\nnational Conference on Learning Representations.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\nSarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\npervised contrastive learning. Advances in Neural\nInformation Processing Systems, 33:18661–18673.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-\nFei. 2013. 3d object representations for fine-grained\ncategorization. In 4th International IEEE Workshop\non 3D Representation and Recognition (3dRR-13),\nSydney, Australia.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In ECCV.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du,\nZhilin Yang, and Jie Tang. 2021. P-tuning v2:\nPrompt tuning can be comparable to fine-tuning\nuniversally across scales and tasks. arXiv preprint\narXiv:2110.07602.\nYuhang Liu, Wei Wei, Daowan Peng, and Feida\nZhu. 2022. Declaration-based prompt tuning\nfor visual question answering. arXiv preprint\narXiv:2205.02456.\nMaria-Elena Nilsback and Andrew Zisserman. 2008.\nAutomated flower classification over a large number\nof classes. In 2008 Sixth Indian Conference on Com-\nputer Vision, Graphics & Image Processing, pages\n722–729. IEEE.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman,\nand CV Jawahar. 2012. Cats and dogs. In 2012\nIEEE conference on computer vision and pattern\nrecognition, pages 3498–3505. IEEE.\nBryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. 2015. Flickr30k entities: Collecting\nregion-to-phrase correspondences for richer image-\nto-sentence models. In Proceedings of the IEEE\ninternational conference on computer vision, pages\n2641–2649.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. arXiv preprint\narXiv:2103.00020.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nJoshua Robinson, Ching-Yao Chuang, Suvrit Sra,\nand Stefanie Jegelka. 2020. Contrastive learn-\ning with hard negative samples. arXiv preprint\narXiv:2010.04592.\nSheng Shen, Liunian Harold Li, Hao Tan, Mohit\nBansal, Anna Rohrbach, Kai-Wei Chang, Zhewei\nYao, and Kurt Keutzer. 2021. How much can clip\nbenefit vision-and-language tasks? arXiv preprint\narXiv:2107.06383.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with\nautomatically generated prompts. arXiv preprint\narXiv:2010.15980.\nHaoyu Song, Li Dong, Wei-Nan Zhang, Ting Liu, and\nFuru Wei. 2022a. Clip models are few-shot learners:\nEmpirical studies on vqa and visual entailment.arXiv\npreprint arXiv:2203.07190.\nHaoyu Song, Li Dong, Wei-Nan Zhang, Ting Liu, and\nFuru Wei. 2022b. Clip models are few-shot learners:\nEmpirical studies on vqa and visual entailment.arXiv\npreprint arXiv:2203.07190.\nVarsha Suresh and Desmond C Ong. 2021. Not all\nnegatives are equal: Label-aware contrastive loss\nfor fine-grained text classification. arXiv preprint\narXiv:2109.05427.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi,\nSM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-\ntimodal few-shot learning with frozen language mod-\nels. Advances in Neural Information Processing Sys-\ntems, 34:200–212.\nJialu Wang, Yang Liu, and Xin Eric Wang.\n2021a. Assessing multilingual fairness in pre-\ntrained multimodal representations. arXiv preprint\narXiv:2106.06683.\nYixin Wang and Michael I Jordan. 2021. Desiderata for\nrepresentation learning: A causal perspective. arXiv\npreprint arXiv:2109.03795.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2021b. Simvlm: Simple\nvisual language model pretraining with weak super-\nvision. arXiv preprint arXiv:2108.10904.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nYiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu,\nChanglong Sun, Jun Xiao, Yueting Zhuang, Luo Si,\nand Fei Wu. 2020. De-biased court’s view generation\nwith causality. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 763–780.\n3417\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude\nOliva, and Antonio Torralba. 2010. Sun database:\nLarge-scale scene recognition from abbey to zoo. In\n2010 IEEE computer society conference on computer\nvision and pattern recognition, pages 3485–3492.\nIEEE.\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu,\nMinzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo\nLi, Xin Jiang, and Chunjing Xu. 2021. Filip:\nFine-grained interactive language-image pre-training.\narXiv preprint arXiv:2111.07783.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\nCoca: Contrastive captioners are image-text founda-\ntion models. arXiv preprint arXiv:2205.01917.\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong\nHuang, Boxin Li, Chunyuan Li, et al. 2021. Florence:\nA new foundation model for computer vision. arXiv\npreprint arXiv:2111.11432.\nXiangji Zeng, Yunliang Li, Yuchen Zhai, and Yin\nZhang. 2020. Counterfactual generator: A weakly-\nsupervised method for named entity recognition. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7270–7280.\nMichael Zhang, Nimit S Sohoni, Hongyang R Zhang,\nChelsea Finn, and Christopher Ré. 2022. Correct-\nn-contrast: A contrastive approach for improving\nrobustness to spurious correlations. arXiv preprint\narXiv:2203.01517.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\nXiheng Zhang, Yongkang Wong, Xiaofei Wu, Juwei\nLu, Mohan Kankanhalli, Xiangdong Li, and Wei-\ndong Geng. 2021. Learning causal representation for\ntraining cross-domain pose estimator via generative\ninterventions. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages\n11270–11280.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2021. Learning to prompt for vision-\nlanguage models. arXiv preprint arXiv:2109.01134.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy,\nand Ziwei Liu. 2022. Conditional prompt learn-\ning for vision-language models. arXiv preprint\narXiv:2203.05557.\nQingfu Zhu, Weinan Zhang, Ting Liu, and\nWilliam Yang Wang. 2020. Counterfactual\noff-policy training for neural dialogue generation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3438–3448.\n3418",
  "topic": "Counterfactual thinking",
  "concepts": [
    {
      "name": "Counterfactual thinking",
      "score": 0.7753260135650635
    },
    {
      "name": "Natural language processing",
      "score": 0.6250085830688477
    },
    {
      "name": "Computer science",
      "score": 0.5995540618896484
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5504445433616638
    },
    {
      "name": "Linguistics",
      "score": 0.3931918144226074
    },
    {
      "name": "Cognitive science",
      "score": 0.3667837381362915
    },
    {
      "name": "Psychology",
      "score": 0.241656094789505
    },
    {
      "name": "Philosophy",
      "score": 0.20404329895973206
    },
    {
      "name": "Epistemology",
      "score": 0.16159576177597046
    }
  ]
}