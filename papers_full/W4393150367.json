{
    "title": "Learning Content-Enhanced Mask Transformer for Domain Generalized Urban-Scene Segmentation",
    "url": "https://openalex.org/W4393150367",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2119616303",
            "name": "Qi Bi",
            "affiliations": [
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A2300542389",
            "name": "Shaodi You",
            "affiliations": [
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A1935704050",
            "name": "Theo Gevers",
            "affiliations": [
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A2119616303",
            "name": "Qi Bi",
            "affiliations": [
                "Amsterdam University of Applied Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2300542389",
            "name": "Shaodi You",
            "affiliations": [
                "Amsterdam University of Applied Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A1935704050",
            "name": "Theo Gevers",
            "affiliations": [
                "Amsterdam University of Applied Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4383503856",
        "https://openalex.org/W3082928408",
        "https://openalex.org/W4312880823",
        "https://openalex.org/W3217112505",
        "https://openalex.org/W3180659539",
        "https://openalex.org/W3150373995",
        "https://openalex.org/W2340897893",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W4377865048",
        "https://openalex.org/W4313192503",
        "https://openalex.org/W2927893015",
        "https://openalex.org/W4386075995",
        "https://openalex.org/W3182906273",
        "https://openalex.org/W4224587847",
        "https://openalex.org/W3035086574",
        "https://openalex.org/W4225348769",
        "https://openalex.org/W3212519405",
        "https://openalex.org/W4307006204",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6779928312",
        "https://openalex.org/W4224291315",
        "https://openalex.org/W2781228439",
        "https://openalex.org/W4221152726",
        "https://openalex.org/W2884366600",
        "https://openalex.org/W2935856147",
        "https://openalex.org/W6810767744",
        "https://openalex.org/W3181515393",
        "https://openalex.org/W3190457880",
        "https://openalex.org/W6775572947",
        "https://openalex.org/W6722836162",
        "https://openalex.org/W2431874326",
        "https://openalex.org/W3158147101",
        "https://openalex.org/W3107499124",
        "https://openalex.org/W6795103355",
        "https://openalex.org/W3171920444",
        "https://openalex.org/W2806187986",
        "https://openalex.org/W3042345653",
        "https://openalex.org/W4283803529",
        "https://openalex.org/W3202168743",
        "https://openalex.org/W2971911570",
        "https://openalex.org/W3103937687",
        "https://openalex.org/W4226108054",
        "https://openalex.org/W4285397058",
        "https://openalex.org/W3162566581",
        "https://openalex.org/W3039444641",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W4386065565",
        "https://openalex.org/W3213165621",
        "https://openalex.org/W3214608036",
        "https://openalex.org/W4292828330",
        "https://openalex.org/W3034373371",
        "https://openalex.org/W4310030745",
        "https://openalex.org/W3109986233",
        "https://openalex.org/W4319300814",
        "https://openalex.org/W3035762155",
        "https://openalex.org/W4280556796",
        "https://openalex.org/W4312243476",
        "https://openalex.org/W4214893857",
        "https://openalex.org/W2487365028",
        "https://openalex.org/W4319300373",
        "https://openalex.org/W2979475574",
        "https://openalex.org/W4283796551",
        "https://openalex.org/W2981488580",
        "https://openalex.org/W2981540341",
        "https://openalex.org/W2799352588",
        "https://openalex.org/W3109635165",
        "https://openalex.org/W3095799614",
        "https://openalex.org/W4313045831",
        "https://openalex.org/W4287028247",
        "https://openalex.org/W4312815172",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W4312719508",
        "https://openalex.org/W4312657985"
    ],
    "abstract": "Domain-generalized urban-scene semantic segmentation (USSS) aims to learn generalized semantic predictions across diverse urban-scene styles. Unlike generic domain gap challenges, USSS is unique in that the semantic categories are often similar in different urban scenes, while the styles can vary significantly due to changes in urban landscapes, weather conditions, lighting, and other factors. Existing approaches typically rely on convolutional neural networks (CNNs) to learn the content of urban scenes. In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for domain-generalized USSS. The main idea is to enhance the focus of the fundamental component, the mask attention mechanism, in Transformer segmentation models on content information. We have observed through empirical analysis that a mask representation effectively captures pixel segments, albeit with reduced robustness to style variations. Conversely, its lower-resolution counterpart exhibits greater ability to accommodate style variations, while being less proficient in representing pixel segments. To harness the synergistic attributes of these two approaches, we introduce a novel content-enhanced mask attention mechanism. It learns mask queries from both the image feature and its down-sampled counterpart, aiming to simultaneously encapsulate the content and address stylistic variations. These features are fused into a Transformer decoder and integrated into a multi-resolution content-enhanced mask attention learning scheme. Extensive experiments conducted on various domain-generalized urban-scene segmentation datasets demonstrate that the proposed CMFormer significantly outperforms existing CNN-based methods by up to 14.0% mIoU and the contemporary HGFormer by up to 1.7% mIoU. The source code is publicly available at https://github.com/BiQiWHU/CMFormer.",
    "full_text": "Learning Content-Enhanced Mask Transformer for Domain Generalized\nUrban-Scene Segmentation\nQi Bi, Shaodi You, Theo Gevers\nComputer Vision Research Group, University of Amsterdam, Netherlands\n{q.bi, s.you, th.gevers}@uva.nl\nAbstract\nDomain-generalized urban-scene semantic segmentation\n(USSS) aims to learn generalized semantic predictions across\ndiverse urban-scene styles. Unlike generic domain gap chal-\nlenges, USSS is unique in that the semantic categories are\noften similar in different urban scenes, while the styles\ncan vary significantly due to changes in urban landscapes,\nweather conditions, lighting, and other factors. Existing ap-\nproaches typically rely on convolutional neural networks\n(CNNs) to learn the content of urban scenes. In this paper,\nwe propose a Content-enhanced Mask TransFormer (CM-\nFormer) for domain-generalized USSS. The main idea is to\nenhance the focus of the fundamental component, the mask\nattention mechanism, in Transformer segmentation models\non content information. We have observed through empiri-\ncal analysis that a mask representation effectively captures\npixel segments, albeit with reduced robustness to style vari-\nations. Conversely, its lower-resolution counterpart exhibits\ngreater ability to accommodate style variations, while be-\ning less proficient in representing pixel segments. To har-\nness the synergistic attributes of these two approaches, we\nintroduce a novel content-enhanced mask attention mech-\nanism. It learns mask queries from both the image fea-\nture and its down-sampled counterpart, aiming to simulta-\nneously encapsulate the content and address stylistic vari-\nations. These features are fused into a Transformer de-\ncoder and integrated into a multi-resolution content-enhanced\nmask attention learning scheme. Extensive experiments con-\nducted on various domain-generalized urban-scene segmen-\ntation datasets demonstrate that the proposed CMFormer sig-\nnificantly outperforms existing CNN-based methods by up\nto 14.0% mIoU and the contemporary HGFormer by up to\n1.7% mIoU. The source code is publicly available at https:\n//github.com/BiQiWHU/CMFormer.\nIntroduction\nUrban-scene semantic segmentation (USSS) is a challenging\nproblem because of the large scene variations due to chang-\ning landscape, weather, and lighting conditions (Sakaridis,\nDai, and Van Gool 2021; Mirza et al. 2022; Bi, You, and\nGevers 2023; Chen et al. 2022). Unreliable USSS can pose\na significant risk to road users. Nevertheless, a segmenta-\ntion model trained on a specific dataset cannot encompass\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nCityScapes BDD Mapillary GTAV SYNTHIA\n(b) generalization on unseen domains\n61.5%\n62.6%\n+  1.1%\nHGFormer*\nOurs\n72.1%\n73.6%\n+1.5%\n59.4%\n60.7%\n+1.3%\n41.3%\n43.0%\n+1.7%\n(a) style variation of urban-scenes \nFigure 1: (a) In domain generalized USSS, the domain gap is\nmainly from the extremely-varied styles. (b) A segmentation\nmodel is supposed to show good generalization on unseen\ntarget domains.\nall urban scenes across the globe. As a result, the segmenta-\ntion model is prone to encountering unfamiliar urban scenes\nduring the inference stage. Hence, domain generalization\nis essential for robust USSS (Pan et al. 2018; Huang et al.\n2019a; Choi et al. 2021), where a segmentation model can\neffectively extrapolate its performance to urban scenes that\nit hasn‚Äôt encountered before (Fig. 1). In contrast to com-\nmon domain generalization, domain generalized USSS re-\nquires special attention because the domain gap is mainly\ncaused by large style variations whereas changes in seman-\ntics largely remain consistent (example in Fig. 2).\nExisting approaches can be divided into two groups. One\ngroup focuses on the style de-coupling. This is usually\nachieved by a normalization (Pan et al. 2018; Huang et al.\n2019a; Peng et al. 2022) or whitening (Pan et al. 2019; Choi\net al. 2021; Xu et al. 2022; Peng et al. 2022) transforma-\ntion. However, the de-coupling methodology falls short as\nthe content is not learnt in a robust way. The other group\nis based on adverse domain training (Zhao et al. 2022; Lee\net al. 2022; Zhong et al. 2022). However, these methods usu-\nally do not particularly focus on urban styles and therefore\ntheir performance is limited.\nRecent work has shown that mask-level segmentation\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n819\nSimilar Content Varied Styles\nBDD100K GTA5\ntree, cars, \nroad, building,\n‚Ä¶‚Ä¶ \ntree, cars, \nroad, building,\n‚Ä¶‚Ä¶ \nBDD100K GTA5\nrainy, day-time,\nEuropean street, \n‚Ä¶‚Ä¶ \nclear, night-time,\nAmerican street, \n‚Ä¶‚Ä¶ \nFigure 2: Domain-generalized USSS demonstrates a distinc-\ntive feature of consistent content with diverse styles. An ex-\nample is given for BDD100K and GTA5.\nTransformer (e.g., Mask2Former) (Ding et al. 2023) is a\nscalable learner for domain generalized semantic segmenta-\ntion. However, based on our empirical observations, a high-\nresolution mask-level representation excels at capturing con-\ntent down to pixel semantics but is more susceptible to style\nvariations. Conversely, its down-sampled counterpart is less\nproficient in representing content down to pixel semantics\nbut exhibits greater resilience to style variations.\nA novel content-enhanced mask attention (CMA) mech-\nanism is proposed. It jointly leverages both mask represen-\ntation and its down-sampled counterpart, which show com-\nplementary properties on content representing and handling\nstyle variation. Jointly using both features helps the style to\nbe uniformly distributed while the content to be stabilized in\na certain cluster. The proposed CMA takes the original im-\nage feature together with its down-sampled counterpart as\ninput. Both features are fused to learn a more robust content\nfrom their complementary properties.\nThe proposed content-enhanced mask attention (CMA)\nmechanism can be integrated into existing mask-level seg-\nmentation Transformer in a learnable fashion. It consists\nof three key steps, namely, exploiting high-resolution prop-\nerties, exploiting low-resolution properties, and content-\nenhanced fusion. Besides, it can also be seamlessly adapted\nto multi-resolution features. A novel Content-enhanced\nMask TransFormer (CMFormer) is proposed for domain-\ngeneralized USSS.\nLarge-scale experiments are conducted with various do-\nmain generalized USSS settings, i.e., trained on one dataset\nfrom (Richter et al. 2016; Ros et al. 2016; Cordts et al. 2016;\nNeuhold et al. 2017; Yu et al. 2018) as the source domain,\nand validated on the rest of the four datasets as the unseen\ntarget domains. All the datasets contain the same 19 se-\nmantic categories as the content, but vary in terms of scene\nstyles. The experiments show that the proposed CMFormer\nachieves up to 14.00% mIoU improvement compared to the\nstate-of-the-art CNN based methods (e.g., SAW (Peng et al.\n2022), WildNet (Lee et al. 2022)). Furthermore, it demon-\nstrates a mIoU improvement of up to 1.7% compared to the\nmodern HGFormer model (Ding et al. 2023). It also shows\nstate-of-the-art performance on synthetic-to-real and clear-\nto-adverse generalization.\nOur contribution is summarized as follows:\n‚Ä¢ A content-enhanced mask attention (CMA) mechanism\nis proposed to leverage the complementary content and\nstyle properties from mask-level representation and its\ndown-sampled counterpart.\n‚Ä¢ On top of CMA, a Content-enhanced Mask Transformer\n(CMFormer) is proposed for domain generalized urban-\nscene semantic segmentation.\n‚Ä¢ Extensive experiments show a large performance im-\nprovement over existing SOTA by up to 14.0% mIoU,\nand HGFormer by up to 1.7% mIoU.\nRelated Work\nDomain Generalization has been studied on no task-\nspecific scenarios in the field of both machine learning and\ncomputer vision. Hu et al. (Hu and Lee 2022) proposed a\nframework for image retrieval in an unsupervised setting.\nZhou et al. (Zhou et al. 2020) proposed a framework to gen-\neralize to new homogeneous domains. Qiao et al. (Qiao,\nZhao, and Peng 2020) and Peng et al. (Peng, Qiao, and\nZhao 2022) proposed to learn domain generalization from\na single source domain. Many other methods have also been\nproposed (Zhao et al. 2020; Mahajan, Tople, and Sharma\n2021; Wang et al. 2020; Chattopadhyay, Balaji, and Hoff-\nman 2020; Segu, Tonioni, and Tombari 2023).\nDomain Generalized Semantic Segmentation is more\npractical than conventional semantic segmentation (Pan\net al. 2022; Ji et al. 2021; Li et al. 2021; Ji et al. 2022; Zhou,\nYi, and Bi 2021; Ye et al. 2021), which focuses on the gener-\nalization of a segmentation model on unseen target domains.\nExisting methods focus on the generalization of in-the-wild\n(Piva, de Geus, and Dubbelman 2023), scribble (Tjio et al.\n2022) and multi-source images (Kim et al. 2022; Lambert\net al. 2020), where substantial alterations can occur in both\nthe content and style.\nDomain Generalized USSS focuses on the generaliza-\ntion of driving-scenes (Cordts et al. 2016; Yu et al. 2018;\nNeuhold et al. 2017; Ros et al. 2016; Richter et al. 2016).\nThese methods use either normalization transformation\n(e.g., IBN (Pan et al. 2018), IN (Huang et al. 2019a), SAN\n(Peng et al. 2022)) or whitening transformation (e.g., IW\n(Pan et al. 2019), ISW (Choi et al. 2021), DIRL (Xu et al.\n2022), SAW (Peng et al. 2022)) on the training domain, to\nenable the model to generalize better on the target domains.\nOther advanced methods for domain generalization in seg-\nmentation typically rely on external images to incorporate\nmore diverse styles (Lee et al. 2022; Zhao et al. 2022; Zhong\net al. 2022; Li et al. 2023), and leverage content consistency\nacross multi-scale features (Yue et al. 2019). To the best of\nour knowledge, all of these methods are based on CNN.\nMask Transformer for Semantic Segmentation uses the\nqueries in the Transformer decoder to learn the masks,\ne.g., Segmenter (Strudel et al. 2021), MaskFormer (Cheng,\nSchwing, and Kirillov 2021). More recently, Mask2Former\n(Cheng et al. 2022) further simplifies the pipeline of Mask-\nFormer and achieves better performance.\nPreliminary\nProblem Definition Domain generalization can be formu-\nlated as a worst-case problem (Li, Namkoong, and Xia 2021;\nZhong et al. 2022; V olpi et al. 2018). Given a source do-\nmain S, and a set of unseen target domains T1, T2, ¬∑ ¬∑¬∑, a\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n820\n(b) objective\nstyle\ncontent\n(a) observation\nstyle\ncontent\nBDD\nMapillary\nGTA5\nSYNTHIA\nFigure 3: (a) In the domain-generalized USSS setting, within\nthe content-style space, samples from various domains tend\nto cluster closely along the content dimension while display-\ning dispersion along the style dimension. (b) An optimal\ngeneralized semantic segmentation scenario would involve\nuniform distribution of styles while maintaining content sta-\nbility (as indicated by the brown bounding box).\nmodel parameterized by Œ∏ with the task-specific loss Ltask,\nthe generic domain generalization task can be formulated as\na worst-case problem, given by\nmin\nŒ∏\nsupp\nT :D(S;T1,T2,¬∑¬∑¬∑)‚â§œÅ\nET [Ltask(Œ∏; T1, T2, ¬∑ ¬∑¬∑)], (1)\nwhere Œ∏ denotes the model parameters, D(S; T1, T2, ¬∑ ¬∑¬∑)\ncorresponds to the distance between the source S and tar-\nget domain T , and œÅ denotes the constraint threshold.\nContent-style Feature Space Here we analyze the feature\nspace. Figure 3a illustrates that in the context of domain-\ngeneralized USSS, samples from distinct domains might ex-\nhibit analogous patterns and cluster tightly along the content\ndimension. Conversely, samples from diverse domains may\nsegregate into separate clusters along the style dimension.\nAn optimal and adaptable segmentation representation\nshould achieve content stability while simultaneously ex-\nhibiting resilience in the face of significant style varia-\ntions. Illustrated in Figure 3b, our objective is to cultivate\na content-style space wherein: 1) samples from diverse do-\nmains can occupy analogous positions along the content di-\nmension, and 2) samples can be uniformly dispersed across\nthe style dimension. Both learning objectives allow us to\ntherefore minimize the domain gap.\nOverall Idea Recent work has shown that mask-level seg-\nmentation Transformer (e.g., Mask2Former) (Ding et al.\n2023) is a scalable learner for domain generalized seman-\ntic segmentation. However, we empirically observe that, a\nmask-level representation is better at representing content,\nbut more sensitive to style variations (similar to Fig. 3a); its\nlow-resolution counterpart, on the contrary, is less capable\nto represent content, but more robust to the style variations\n(similar to the style dimensions in Fig. 3b).\nOverall, the mask representation and its down-sampled\ncounterpart shows complementary properties when handling\nsamples from different domains. Thus, it is natural to jointly\nleverage both mask representation and its down-sampled\ncounterparts, so as to at the same time stabilize the content\nand be insensitive to the style variation.\nDifference between Existing Pipelines Existing methods\nusually focus on decoupling the styles from urban scenes,\nso that along the style dimension the samples from different\ndomains are more uniformly distributed.\nIn contrast, the proposed method intends to leverage the\ncontent representation ability of mask-level features and the\nstyle handling ability of its down-sampled counterpart, so as\nto realize the aforementioned learning objective.\nMethodology\nRecap on Mask Attention\nRecent studies show that the mask-level pipelines (Strudel\net al. 2021; Cheng, Schwing, and Kirillov 2021; Cheng et al.\n2022) have stronger representation ability than conventional\npixel-wise pipelines for semantic segmentation, which can\nbe attributed to the mask attention mechanism.\nIt learns the query features as the segmentation masks\nby introducing a mask attention matrix based on the self-\nattention mechanism. Let Fl and Xl denote the image fea-\ntures from the image decoder and the features of thelth layer\nin a Transformer decoder, respectively. When l = 0, X0\nrefers to the input query features of the Transformer decoder.\nThe key Kl and value Vl on Fl‚àí1 are computed by lin-\near transformations fK and fV , respectively. Similarly, the\nquery Ql on Xl‚àí1 is computed by linear transformation fQ.\nThen, the query feature Xl is computed by\nXl = softmax(Ml‚àí1 + QlKT\nl )Vl + Xl‚àí1, (2)\nwhere Ml‚àí1 ‚àà {0,1}N√óHlWl is a binary mask attention\nmatrix from the resized mask prediction of the previous\n(l ‚àí 1)th layer, with a threshold of 0.5. M0 is binarized\nand resized from X0. It filters the foreground regions of an\nimage, given by\nMl‚àí1(x, y) =\n\u001a0 if Ml‚àí1(x, y)=1\n-‚àû else . (3)\nExploiting High-Resolution Properties\nHighlighted within the green block in Figure 4, our empir-\nical observations reveal that the high-resolution mask rep-\nresentation exhibits the following characteristics: 1) greater\nproficiency in content representation, and 2) reduced sus-\nceptibility to domain variation. Achieving uniform mixing\nof samples from four domains presents a challenge.\nTo leverage the properties from high-resolution mask rep-\nresentations, we use the self-attention mechanism to exploit\nthe amplified content representation fromXl. Let QXl , VXl\nand KXl denote its query, value and key, and dk denotes\ntheir dimension. Then, the self-attention is computed as\nAttention(QXl , KXl , VXl ) = Softmax(QXl KXl\n‚àödk\n)VXl , (4)\nwhere Softmax denotes the softmax normalization function,\nand the final output is denoted as ÀúXl.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n821\nQueries\n‚Ä¶\navgpool\nùêêùëô\nùêêùëô\nùëë\nùêóùëô\nùëë\nùêóùëô\nFramework Overview\nmask attention\nùêåùëô‚àí1\nùêÖùëô‚àí1\nùêÖùëô‚àí1\nmask attention\n‚Ä¶\nùêåùëô‚àí1\nùëë\nùêïùëô\nùëë\nùêäùëô\nùëë\nùêäùëô\nùêïùëô\nself-attention mining\nself-attention mining\nexploiting high-resolution properties\n‚Ä¶‚Ä¶\nMLP\ncontent enhanced fusion\nrefined feature space\n‚Ä¶\nBDD\nMapillary\nGTA5\nSYNTHIA\nCMA\nCMA\nCMA\nCMA\nCMA\nCMA\nCMA\nCMA\nCMA\nimage encoder\nimage decoder\nTransformer \ndecoder\nexploiting low-resolution properties\nstyle  √ó\ncontent √ó\ncontent ‚àö   content ‚àö   style ‚àö   \nstyle ‚àö   \nFigure 4: (a) The proposed Content-enhanced Mask Attention (CMA) consists of three key steps, namely, exploiting high-\nresolution properties (in green), exploiting low-resolution properties (in brown), and content enhanced fusion (in gray). (b)\nFramework overview (in yellow) of the proposed Content-enhanced Mask TransFormer (CMFormer) for domain generalized\nsemantic segmentation. The image decoder is directly inherited from the Mask2Former (Cheng et al. 2022).\nExploiting Low-Resolution Properties\nAs shown in the brown block of Fig. 4, the low-resolution\nmask-level representation has the following properties: 1)\nless qualified to represent the content; 2) more capable to\nhandle the style variation. In the feature space, samples from\ndifferent domains are more uniformly distributed. We pro-\npose to build a low-resolution mask representation derived\nfrom its high-resolution counterpart. This approach capital-\nizes on the attributes of the low-resolution representation to\neffectively address domain variations.\nThe low-resolution counterpart Fd\nl is computed by aver-\nage pooling avgpool from the original image feature Fl by\nFd\nl = avgpool(Fl), (5)\nwhere the width and height ofFl is both twice the width and\nheight of Fd\nl .\nSimilarly, the key and value from Fd\nl are computed by\nlinear transformations, and can be denoted as Kd\nl and Vd\nl ,\nrespectively. The query from Xd\nl‚àí1 is also computed by lin-\near transformation, and can be denoted as Kd\nl . The mask\nattention on the low-resolution feature Xd\nl is computed as\nXd\nl = softmax(Md\nl‚àí1 + QlKdT\nl )Vd\nl + Xd\nl‚àí1. (6)\nTo exploit the properties from the low-resolution mask\nrepresentation Xd\nl , we use the self-attention mechanism. Let\nQXd\nl\n, VXd\nl\nand KXd\nl\ndenote its query, value and keys. Then,\nthe self-attention is computed by\nAttention(QXd\nl\n, KXd\nl\n, VXd\nl\n) = Softmax(\nQXd\nl\nKXd\nl\n‚àödk\n)VXd\nl\n. (7)\nThe final output is denoted as ÀúXd\nl . It inherits the charac-\nteristics of the low-resolution mask representation, which is\nadept at accommodating style variations while being less re-\nsilient in capturing pixel-level intricacies.\nContent-enhanced Fusion\nOur idea is to leverage the complementary properties of\nmask-level representation and its down-sample counterpart,\nso as to enhance both the pixel-wise representing and style\nvariation handing (shown in the gray box of Fig. 4). The joint\nuse of both representations aids the segmentation masks in\nconcentrating on scene content while reducing sensitivity to\nstyle variations.\nTo this end, we fuse both representations ÀúXl and ÀúXd\nl in a\nsimple and straight-forward way. The fused feature Xfinal\nl\nserves as the final output of thelth Transformer decoder, and\nit is computed as\nXfinal\nl = hl([ ÀúXl, ÀúXd\nl ]), (8)\nwhere [¬∑, ¬∑] represents the concatenation operation, and hl(¬∑)\nrefers to a linear layer.\nNetwork Architecture and Implementation Details\nThe overall framework is shown in the yellow box of Fig. 4.\nThe Swin Transformer (Liu et al. 2021) is used as the back-\nbone. The pre-trained backbone from ImageNet (Deng et al.\n2009) is utilized for initialization.\nThe image decoder from (Cheng et al. 2022) uses the\noff-the-shelf multi-scale deformable attention Transformer\n(MSDeformAttn) (Zhu et al. 2021) with the default setting\nin (Zhu et al. 2021; Cheng et al. 2022). By considering the\nimage features from the Swin-Based encoder as input, every\n6 MSDeformAttn layers are used to progressively up-sample\nthe image features in √ó32, √ó16, √ó8, and √ó4, respectively.\nThe 1/4 resolution feature map is fused with the features\nfrom the Transformer decoder for dense prediction.\nThe Transformer decoder is also directly inherited from\nMask2Former (Cheng et al. 2022), which has 9 self-\nattention layers in the Transformer decoder to handle the\n√ó32, √ó16 and √ó8 image features, respectively.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n822\nFollowing the default setting of MaskFormer (Cheng,\nSchwing, and Kirillov 2021) and Mask2Former (Cheng et al.\n2022), the final loss functionL is a linear combination of the\nbinary cross-entropy loss Lce, dice loss Ldice, and the clas-\nsification loss Lcls, given by\nL = ŒªceLce + ŒªdiceLdice + ŒªclsLcls, (9)\nwith hyper-parameters Œªce = Œªdice = 5.0, Œªcls = 2.0 as\nthe default of Mask2Former without any tuning. The Adam\noptimizer is used with an initial learning rate of 1 √ó 10‚àí4.\nThe weight decay is set 0.05. The training terminates after\n50 epochs.\nExperiment\nDataset & Evaluation Protocols\nBuilding upon prior research in domain-generalized USSS,\nour experiments utilize five different semantic segmentation\ndatasets. Specifically, CityScapes (Cordts et al. 2016) pro-\nvides 2,975 and 500 well-annotated samples for training and\nvalidation, respectively. These driving-scenes are captured\nin Germany cities with a resolution of 2048√ó1024. BDD-\n100K (Yu et al. 2018) also provides diverse urban driving\nscenes with a resolution of 1280√ó720. 7,000 and 1,000 fine-\nannotated samples are provided for training and validation\nof semantic segmentation, respectively. Mapillary (Neuhold\net al. 2017) is also a real-scene large-scale semantic segmen-\ntation dataset with 25,000 samples. SYNTHIA (Ros et al.\n2016) is large-scale synthetic dataset, and provides 9,400\nimages with a resolution of 1280√ó760. GTA5 (Richter et al.\n2016) is a synthetic semantic segmentation dataset rendered\nby the GTA V game engine. It provides 24,966 simulated\nurban-street samples with a resolution of 1914√ó1052. We\nuse C, B, M, S and G to denote these five datasets.\nFollowing prior domain generalized USSS works (Pan\net al. 2018, 2019; Choi et al. 2021; Peng et al. 2022), the\nsegmentation model is trained on one dataset as the source\ndomain, and is validated on the rest of the four datasets as\nthe target domains. Three settings include: 1) G to C, B, M,\nS; 2) S to C, B, M, G; and 3) C to B, M, G, S. mIoU (in\npercentage %) is used as the validation metric. All of our\nexperiments are performed three times and averaged for fair\ncomparison. All the reported performance is directly cited\nfrom prior works under the ResNet-50 backbone (Pan et al.\n2018, 2019; Choi et al. 2021; Peng et al. 2022).\nExisting domain generalized USSS methods are included\nfor comparison, namely, IBN (Pan et al. 2018), IW (Pan\net al. 2019), Iternorm (Huang et al. 2019b), DRPC (Yue\net al. 2019), ISW (Choi et al. 2021), GTR (Peng et al. 2021),\nDIRL (Xu et al. 2022), SHADE (Zhao et al. 2022), SAW\n(Peng et al. 2022), WildNet (Lee et al. 2022), AdvStyle\n(Zhong et al. 2022), SPC (Huang et al. 2023), and HG-\nFormer (Ding et al. 2023).\nComparison with State-of-the-art\nGTA5 Source Domain Table 1 reports the performance on\ntarget domains of C, B, M and S, respectively. The proposed\nCMFormer shows a performance improvement of 10.66%,\nMethod Trained on GTA5 (G)\n‚Üí C ‚Üí B ‚Üí M ‚Üí S\nIBN 33.85 32.30 37.75 27.90\nIW 29.91 27.48 29.71 27.61\nIternorm 31.81 32.70 33.88 27.07\nDRPC 37.42 32.14 34.12 28.06\nISW 36.58 35.20 40.33 28.30\nGTR 37.53 33.75 34.52 28.17\nDIRL 41.04 39.15 41.60 -\nSHADE 44.65 39.28 43.34 -\nSAW 39.75 37.34 41.86 30.79\nWildNet 44.62 38.42 46.09 31.34\nAdvStyle 39.62 35.54 37.00 -\nSPC 44.10 40.46 45.51 -\nCMFormer (Ours) 55.31 49.91 60.09 43.80\nTable 1: G ‚Üí {C, B, M, S}setting. Performance compari-\nson between the proposed CMFormer and existing domain\ngeneralized USSS methods. ‚Äô-‚Äô: The metric is either not re-\nported or the official source code is not available. Evaluation\nmetric mIoU is given in (%).\n9.45%, 14.00% and 12.46% compared to existing state-\nof-the-art CNN based methods on each target domain, re-\nspectively. These outcomes demonstrate the feature gener-\nalization ability of the proposed CMFormer. Notice that the\nsource domain GTA5 is a synthetic dataset, while the target\ndomains are real images. It further validates the performance\nof the proposed method.\nSYNTHIA Source Domain Table 2 reports the perfor-\nmance. The proposed CMFormer shows a 5.67%, 8.73%\nand 11.49% mIoU performance gain against the best CNN\nbased methods, respectively. However, on the BBD-100K\n(B) dataset, the semantic-aware whitening (SAW) method\n(Peng et al. 2022) outperforms the proposed CMFormer by\n1.80% mIoU. Nevertheless, the proposed CMFormer still\noutperforms the rest methods. The performance gain of the\nproposed CMFormer when trained on SYNTHIA dataset is\nnot as significant as it is trained on CityScapes or GTA5\ndataset. The explanation may be that the SYNTHIA dataset\nhas much fewer samples than GTA5 dataset, i.e., 9400 v.s.\n24966, and a transformer may be under-trained.\nCityScapes Source Domain Table 3 reports the per-\nformance. As HGFormer only reports one decimal results\n(Ding et al. 2023), we also report one decimal results\nwhen compared with it. The proposed CMFormer (with\nSwin-Base backbone) shows a performance gain of 6.32%,\n10.43%, 9.50% and 12.11% mIoU on the B, M, G and S\ndataset against the state-of-the-art CNN based method. As\nBDD100K dataset contains many nigh-time urban-street im-\nages, it is particularly challenging for existing domain gen-\neralized USSS methods. Still, a performance gain of 6.32%\nis observed by the proposed CMFormer.\nOn the other hand, when comparing ours with the contem-\nporary HGFormer with the Swin-Large backbone, it shows\nan mIoU improvement of 1.1%, 1.5%, 1.3% and 1.7% on\nthe B, M, G and S target domain, respectively.\nFrom Synthetic Domain to Real Domain We also test\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n823\nMethod Trained on SYNTHIA (S)\n‚Üí C ‚Üí B ‚Üí M ‚Üí G\nIBN 32.04 30.57 32.16 26.90\nIW 28.16 27.12 26.31 26.51\nDRPC 35.65 31.53 32.74 28.75\nISW 35.83 31.62 30.84 27.68\nGTR 36.84 32.02 32.89 28.02\nSAW 38.92 35.24 34.52 29.16\nAdvStyle 37.59 27.45 31.76 -\nCMFormer (Ours) 44.59 33.44 43.25 40.65\nTable 2: S ‚Üí {C, B, M, G}setting. Performance compari-\nson between the proposed CMFormer and existing domain\ngeneralized USSS methods. ‚Äô-‚Äô: The metric is either not re-\nported or the official source code is not available. Evaluation\nmetric mIoU is given in (%).\nMethod Backbone Trained on Cityscapes\n(C)\n‚Üí B ‚Üí M ‚Üí G ‚Üí S\nIBN Res50 48.56 57.04 45.06 26.14\nIW Res50 48.49 55.82 44.87 26.10\nIternorm Res50 49.23 56.26 45.73 25.98\nDRPC Res50 49.86 56.34 45.62 26.58\nISW Res50 50.73 58.64 45.00 26.20\nGTR Res50 50.75 57.16 45.79 26.47\nDIRL Res50 51.80 - 46.52 26.50\nSHADE Res50 50.95 60.67 48.61 27.62\nSA\nW Res50 52.95 59.81 47.28 28.32\nW\nildNet Res50 50.94 58.79 47.01 27.95\nHGF\normer Swin-T 53.4 66.9 51.3 33.6\nOurs Swin-B 59.27 71.10 58.11 40.43\nHGFormer Swin-L 61.5 72.1 59.4 41.3\nOurs Swin-L 62.6 73.6 60.7 43.0\nTable 3: C ‚Üí {B, M, G, S}setting. Performance compari-\nson between the proposed CMFormer and existing domain\ngeneralized USSS methods. ‚Äô-‚Äô: the metric is either not re-\nported or the official source code is not available. Evaluation\nmetric mIoU is given in (%). ‚Ä†: HGFormer only reports one\ndecimal results (Ding et al. 2023).\nthe generalization ability of the CMFormer when trained on\nthe synthetic domains (G+S) and validated on the three real-\nworld domains B, C and M, respectively. The results are\nshown in Table 4. The proposed CMFormer significantly\noutperforms the instance normalization based (IBN (Pan\net al. 2018)), whitening transformation based (ISW (Choi\net al. 2021)) and adversarial domain training based (SHADE\n(Zhao et al. 2022), AdvStyle (Zhong et al. 2022)) methods\nby >10% mIoU.\nFrom Clear to Adverse Conditions we further vali-\ndate the proposed CMFormer‚Äôs performance on the adverse\nconditions dataset with correspondance (ACDC) (Sakaridis,\nDai, and Van Gool 2021). We set the fog, night, rain and\nsnow as four different unseen domains, and directly use the\nmodel pre-trained on CityScapes for inference. The results\nare shown in Table 5. It significantly outperforms existing\ndomain generalized segmentation methods (Pan et al. 2018;\nBackbone Trained on Two Synthetic Domains (G+S)\n‚Üí Citys ‚Üí BDD ‚Üí MAP mean\nRes50 35.46 25.09 31.94 30.83\nIBN 35.55 32.18 38.09 35.27\nISW 37.69 34.09 38.49 36.75\nSHADE 47.43 40.30 47.60 45.11\nAdvStyle 39.29 39.26 41.14 39.90\nSPC 46.36 43.18 48.23 45.92\nOurs 59.70 53.36 61.61 58.22\nTable 4: Generalization of the proposed CMFormer when\ntrained on two synthetic datasets and generalized on real do-\nmains. Evaluation metric mIoU is presented in (%).\nMethod Trained on Cityscapes (C)\n‚Üí Fog ‚Üí Night ‚Üí Rain ‚Üí Snow\nIBN 63.8 21.2 50.4 49.6\nIW 62.4 21.8 52.4 47.6\nISW 64.3 24.3 56.0 49.8\nISSA 67.5 33.2 55.9 53.2\nOurs 77.8 33.7 67.6 64.3\nTable 5: Generalization of the proposed CMFormer to the\nadverse condition domains (rain, fog, night and snow) on\nACDC dataset (Sakaridis, Dai, and Van Gool 2021).\nContent Enhancement Trained on CityScapes (C)\n√ó32 √ó16 √ó8 ‚Üí B ‚Üí M ‚Üí G ‚Üí S\n55.43 66.12 55.05 38.19\n‚úì 56.17 67.55 55.42 38.83\n‚úì ‚úì 58.10 69.72 55.54 39.41\n‚úì ‚úì ‚úì 59.27 71.10 58.11 40.43\nTable 6: Ablation studies on each component of the pro-\nposed CMFormer. √ó32, √ó16 and √ó8 denote the image fea-\ntures of √ó32, √ó16 and √ó8 resolution. ‚úì refers to the content\nenhancement is implemented. Evaluation metric mIoU.\nHuang et al. 2019a; Pan et al. 2019; Choi et al. 2021; Li\net al. 2023) by up to 10.3%, 0.5%, 11.6%, 11.1% on the fog,\nnight, rain and snow domains, respectively.\nAblation Studies\nOn Content-enhancement of Each Resolution Table 6 re-\nports the performance of the proposed CMFormer when\n√ó32, √ó16 and √ó8 image features are or are not imple-\nmented with content enhancement. The content enhance-\nment on a certain resolution feature allows the exploiting\nof its low-resolution properties. When no image features\nare implemented with content enhancement, CMFormer de-\ngrades into a Mask2Former (Cheng et al. 2022) which only\nincludes the high-resolution properties. When only imple-\nmenting content enhancement on the √ó32 image feature,\nthe down-sampled √ó128 image feature may propagate lit-\ntle content information to the segmentation mask, and only\na performance gain of 0.74%, 1.43%, 0.37% and 0.64% on\nB, M, G and S target domain is observed. When further im-\nplementing content enhancement on the √ó16 image feature,\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n824\nUnseen images Ground truth IBN SAW OursISW\nIW\nFigure 5: Unseen domain segmentation prediction of existing CNN based domain generalized semantic segmentation methods\n(IBN (Pan et al. 2018), IW (Pan et al. 2019), ISW (Choi et al. 2021), SAW (Peng et al. 2022)) and the proposed CMFormer\nunder the C ‚Üí B, M, G, S setting.\nUnseen images Ground truth IBN SAW OursIW\n ISW\nFigure 6: Unseen domain segmentation prediction of existing CNN based domain generalized semantic segmentation methods\n(IBN (Pan et al. 2018), IW (Pan et al. 2019), ISW (Choi et al. 2021), SAW (Peng et al. 2022)) and the proposed CMFormer\nunder the C ‚Üí adverse domain setting.\nthe enhanced content information begins to play a role, and\nan additional performance gain of 1.93%, 2.17%, 0.12% and\n0.58% is observed. Then, the content enhancement on the\n√ó8 image feature also demonstrates a significant impact on\nthe generalization ability.\nQuantitative Segmentation Results\nSome segmentation results on the C ‚Üí B, M, G, S set-\nting and C ‚Üí adverse domain setting are visualized in\nFig. 5 and 6. Compared with the CNN based methods, the\nproposed CMFormer shows a better segmentation predic-\ntion, especially in terms of the completeness of objects.\nConclusion\nIn this paper, we explored the feasibility of adapting the\nmask Transformer for domain-generalized urban-scene se-\nmantic segmentation (USSS). To address the challenges of\nstyle variation and robust content representation, we pro-\nposed a content-enhanced mask attention (CMA) mecha-\nnism. This mechanism is designed to capture more resilient\ncontent features while being less sensitive to style variations.\nFurthermore, we integrate it into a novel framework called\nthe Content-enhanced Mask TransFormer (CMFormer).\nExtensive experiments on multiple settings demonstrated the\nsuperior performance of CMFormer compared to existing\ndomain-generalized USSS methods.\nBoarder Social Impact. The proposed method has the\npotential to enhance the accuracy and reliability of seman-\ntic segmentation models, thereby contributing to safer and\nmore efficient autonomous systems. Overall, the proposed\ncontent-enhanced mask attention mechanism offers promis-\ning advancements in domain-generalized USSS.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n825\nReferences\nBi, Q.; You, S.; and Gevers, T. 2023. Interactive Learning\nof Intrinsic and Extrinsic Properties for All-Day Semantic\nSegmentation. IEEE Transactions on Image Processing, 32:\n3821‚Äì3835.\nChattopadhyay, P.; Balaji, Y .; and Hoffman, J. 2020. Learn-\ning to balance specificity and invariance for in and out of\ndomain generalization. In European Conference on Com-\nputer Vision, 301‚Äì318. Springer.\nChen, W.-T.; Huang, Z.-K.; Tsai, C.-C.; Yang, H.-H.; Ding,\nJ.-J.; and Kuo, S.-Y . 2022. Learning Multiple Adverse\nWeather Removal via Two-Stage Knowledge Learning and\nMulti-Contrastive Regularization: Toward a Unified Model.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 17653‚Äì17662.\nCheng, B.; Misra, I.; Schwing, A. G.; Kirillov, A.; and Gird-\nhar, R. 2022. Masked-attention mask transformer for univer-\nsal image segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 1290‚Äì1299.\nCheng, B.; Schwing, A.; and Kirillov, A. 2021. Per-pixel\nclassification is not all you need for semantic segmenta-\ntion. Advances in Neural Information Processing Systems,\n34: 17864‚Äì17875.\nChoi, S.; Jung, S.; Yun, H.; Kim, J.; Kim, S.; and Choo,\nJ. 2021. RobustNet: Improving Domain Generalization in\nUrban-Scene Segmentation via Instance Selective Whiten-\ning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 11580‚Äì\n11590.\nCordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler,\nM.; Benenson, R.; Franke, U.; Roth, S.; and Schiele, B.\n2016. The cityscapes dataset for semantic urban scene un-\nderstanding. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 3213‚Äì3223.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248‚Äì255. Ieee.\nDing, J.; Xue, N.; Xia, G.-S.; Schiele, B.; and Dai, D. 2023.\nHGFormer: Hierarchical Grouping Transformer for Domain\nGeneralized Semantic Segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 15413‚Äì15423.\nHu, C.; and Lee, G. H. 2022. Feature Representation Learn-\ning for Unsupervised Cross-Domain Image Retrieval. InEu-\nropean Conference on Computer Vision, 529‚Äì544. Springer.\nHuang, L.; Zhou, Y .; Zhu, F.; Liu, L.; and Shao, L. 2019a. It-\nerative Normalization: Beyond Standardization towards Ef-\nficient Whitening. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n4874‚Äì4883.\nHuang, L.; Zhou, Y .; Zhu, F.; Liu, L.; and Shao, L. 2019b.\nIterative normalization: Beyond standardization towards ef-\nficient whitening. In Proceedings of the ieee/cvf conference\non computer vision and pattern recognition, 4874‚Äì4883.\nHuang, W.; Chen, C.; Li, Y .; Li, J.; Li, C.; Song, F.; Yan,\nY .; and Xiong, Z. 2023. Style Projected Clustering for Do-\nmain Generalized Semantic Segmentation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 3061‚Äì3071.\nJi, W.; Li, J.; Bi, Q.; Liu, J.; Cheng, L.; et al. 2022. Pro-\nmoting Saliency From Depth: Deep Unsupervised RGB-D\nSaliency Detection. In International Conference on Learn-\ning Representations.\nJi, W.; Yu, S.; Wu, J.; Ma, K.; Bian, C.; Bi, Q.; Li, J.; Liu, H.;\nCheng, L.; and Zheng, Y . 2021. Learning calibrated medi-\ncal image segmentation via multi-rater agreement modeling.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 12341‚Äì12351.\nKim, J.; Lee, J.; Park, J.; Min, D.; and Sohn, K. 2022. Pin\nthe memory: Learning to generalize semantic segmentation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 4350‚Äì4360.\nLambert, J.; Liu, Z.; Sener, O.; Hays, J.; and Koltun, V . 2020.\nMSeg: A composite dataset for multi-domain semantic seg-\nmentation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2879‚Äì2888.\nLee, S.; Seong, H.; Lee, S.; and Kim, E. 2022. WildNet:\nLearning Domain Generalized Semantic Segmentation from\nthe Wild. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 9936‚Äì9946.\nLi, J.; Ji, W.; Bi, Q.; Yan, C.; Zhang, M.; Piao, Y .; Lu, H.;\net al. 2021. Joint semantic mining for weakly supervised\nRGB-D salient object detection. Advances in Neural Infor-\nmation Processing Systems, 34: 11945‚Äì11959.\nLi, M.; Namkoong, H.; and Xia, S. 2021. Evaluating model\nperformance under worst-case subpopulations. Advances in\nNeural Information Processing Systems, 34: 17325‚Äì17334.\nLi, Y .; Zhang, D.; Keuper, M.; and Khoreva, A. 2023. Intra-\nSource Style Augmentation for Improved Domain General-\nization. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision, 509‚Äì519.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (CVPR), 10012‚Äì10022.\nMahajan, D.; Tople, S.; and Sharma, A. 2021. Domain gen-\neralization using causal matching. In International Confer-\nence on Machine Learning, 7313‚Äì7324. PMLR.\nMirza, M. J.; Masana, M.; Possegger, H.; and Bischof, H.\n2022. An Efficient Domain-Incremental Learning Approach\nto Drive in All Weather Conditions. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 3001‚Äì3011.\nNeuhold, G.; Ollmann, T.; Rota Bulo, S.; and Kontschieder,\nP. 2017. The mapillary vistas dataset for semantic under-\nstanding of street scenes. In Proceedings of the IEEE inter-\nnational conference on computer vision, 4990‚Äì4999.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n826\nPan, J.; Bi, Q.; Yang, Y .; Zhu, P.; and Bian, C. 2022. Label-\nefficient hybrid-supervised learning for medical image seg-\nmentation. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence, volume 36, 2026‚Äì2034.\nPan, X.; Luo, P.; Shi, J.; and Tang, X. 2018. Two at Once:\nEnhancing Learning and Generalization Capacities via IBN-\nNet. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), 464‚Äì479.\nPan, X.; Zhan, X.; Shi, J.; Tang, X.; and Luo, P. 2019.\nSwitchable Whitening for Deep Representation Learning. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 1863‚Äì1871.\nPeng, D.; Lei, Y .; Hayat, M.; Guo, Y .; and Li, W. 2022.\nSemantic-aware domain generalized segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2594‚Äì2605.\nPeng, D.; Lei, Y .; Liu, L.; Zhang, P.; and Liu, J. 2021. Global\nand local texture randomization for synthetic-to-real seman-\ntic segmentation. IEEE Transactions on Image Processing,\n30: 6594‚Äì6608.\nPeng, X.; Qiao, F.; and Zhao, L. 2022. Out-of-Domain Gen-\neralization From a Single Source: An Uncertainty Quantifi-\ncation Approach. IEEE Transactions on Pattern Analysis\nand Machine Intelligence.\nPiva, F. J.; de Geus, D.; and Dubbelman, G. 2023. Empiri-\ncal Generalization Study: Unsupervised Domain Adaptation\nvs. Domain Generalization Methods for Semantic Segmen-\ntation in the Wild. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, 499‚Äì508.\nQiao, F.; Zhao, L.; and Peng, X. 2020. Learning to learn sin-\ngle domain generalization. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n12556‚Äì12565.\nRichter, S. R.; Vineet, V .; Roth, S.; and Koltun, V . 2016.\nPlaying for data: Ground truth from computer games. InEu-\nropean conference on computer vision, 102‚Äì118. Springer.\nRos, G.; Sellart, L.; Materzynska, J.; Vazquez, D.; and\nLopez, A. M. 2016. The synthia dataset: A large collec-\ntion of synthetic images for semantic segmentation of urban\nscenes. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 3234‚Äì3243.\nSakaridis, C.; Dai, D.; and Van Gool, L. 2021. ACDC:\nThe adverse conditions dataset with correspondences for se-\nmantic driving scene understanding. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 10765‚Äì10775.\nSegu, M.; Tonioni, A.; and Tombari, F. 2023. Batch normal-\nization embeddings for deep domain generalization. Pattern\nRecognition, 135: 109115.\nStrudel, R.; Garcia, R.; Laptev, I.; and Schmid, C. 2021.\nSegmenter: Transformer for semantic segmentation. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, 7262‚Äì7272.\nTjio, G.; Liu, P.; Zhou, J. T.; and Goh, R. S. M. 2022. Adver-\nsarial semantic hallucination for domain generalized seman-\ntic segmentation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, 318‚Äì327.\nV olpi, R.; Namkoong, H.; Sener, O.; Duchi, J. C.; Murino,\nV .; and Savarese, S. 2018. Generalizing to unseen domains\nvia adversarial data augmentation. Advances in neural in-\nformation processing systems, 31.\nWang, S.; Yu, L.; Li, C.; Fu, C.-W.; and Heng, P.-A. 2020.\nLearning from extrinsic and intrinsic supervisions for do-\nmain generalization. In European Conference on Computer\nVision, 159‚Äì176. Springer.\nXu, Q.; Yao, L.; Jiang, Z.; Jiang, G.; Chu, W.; Han, W.;\nZhang, W.; Wang, C.; and Tai, Y . 2022. DIRL: Domain-\ninvariant representation learning for generalizable semantic\nsegmentation. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 36, 2884‚Äì2892.\nYe, Q.; Shen, X.; Gao, Y .; Wang, Z.; Bi, Q.; Li, P.; and Yang,\nG. 2021. Temporal cue guided video highlight detection\nwith low-rank audio-visual fusion. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n7950‚Äì7959.\nYu, F.; Xian, W.; Chen, Y .; Liu, F.; Liao, M.; Madhavan,\nV .; and Darrell, T. 2018. Bdd100k: A diverse driving video\ndatabase with scalable annotation tooling. arXiv preprint\narXiv:1805.04687, 2(5): 6.\nYue, X.; Zhang, Y .; Zhao, S.; Sangiovanni-Vincentelli, A.;\nKeutzer, K.; and Gong, B. 2019. Domain Randomization\nand Pyramid Consistency: Simulation-to-Real Generaliza-\ntion Without Accessing Target Domain Data. InProceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 2100‚Äì2110.\nZhao, S.; Gong, M.; Liu, T.; Fu, H.; and Tao, D. 2020. Do-\nmain generalization via entropy regularization. Advances in\nNeural Information Processing Systems, 33: 16096‚Äì16107.\nZhao, Y .; Zhong, Z.; Zhao, N.; Sebe, N.; and Lee, G. H.\n2022. Style-hallucinated dual consistency learning for do-\nmain generalized semantic segmentation. In Computer\nVision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Is-\nrael, October 23‚Äì27, 2022, Proceedings, Part XXVIII, 535‚Äì\n552. Springer.\nZhong, Z.; Zhao, Y .; Lee, G. H.; and Sebe, N. 2022. Adver-\nsarial Style Augmentation for Domain Generalized Urban-\nScene Segmentation. In Advances in Neural Information\nProcessing Systems.\nZhou, B.; Yi, J.; and Bi, Q. 2021. Differential convolution\nfeature guided deep multi-scale multiple instance learning\nfor aerial scene classification. In ICASSP 2021-2021 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing, 4595‚Äì4599.\nZhou, K.; Yang, Y .; Hospedales, T.; and Xiang, T. 2020.\nLearning to generate novel domains for domain generaliza-\ntion. In European conference on computer vision, 561‚Äì578.\nSpringer.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J.\n2021. Deformable DETR: Deformable Transformers for\nEnd-to-End Object Detection. In International Conference\non Learning Representations.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n827"
}