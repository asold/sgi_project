{
  "title": "How to Index Item IDs for Recommendation Foundation Models",
  "url": "https://openalex.org/W4376312036",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4311992004",
      "name": "Hua, Wenyue",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2229576381",
      "name": "Xu, Shuyuan",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A4222152839",
      "name": "Ge, Yingqiang",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A1865257807",
      "name": "Zhang Yongfeng",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2171960770",
    "https://openalex.org/W3024683329",
    "https://openalex.org/W2512971201",
    "https://openalex.org/W4296591867",
    "https://openalex.org/W2219888463",
    "https://openalex.org/W2746011824",
    "https://openalex.org/W2054141820",
    "https://openalex.org/W2951645301",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W2783272285",
    "https://openalex.org/W2788728386",
    "https://openalex.org/W2966483207",
    "https://openalex.org/W2767724106",
    "https://openalex.org/W3094127838",
    "https://openalex.org/W3065542300",
    "https://openalex.org/W2783666221",
    "https://openalex.org/W3100260481",
    "https://openalex.org/W3106181667",
    "https://openalex.org/W3124675547",
    "https://openalex.org/W2963367478",
    "https://openalex.org/W1597703625",
    "https://openalex.org/W4229641819",
    "https://openalex.org/W3153088333"
  ],
  "abstract": "Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item as in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text and hallucinated recommendations when deciding which item(s) to recommend, creating LLM-compatible item IDs to uniquely identify each item is essential for recommendation foundation models. In this study, we systematically examine the item ID creation and indexing problem for recommendation foundation models, using P5 as an example of the backbone LLM. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as random indexing, title indexing, and independent indexing. We then propose four simple yet effective solutions, including sequential indexing, collaborative indexing, semantic (content-based) indexing, and hybrid indexing. Our study highlights the significant influence of item indexing methods on the performance of LLM-based recommendation, and our results on real-world datasets validate the effectiveness of our proposed solutions. The research also demonstrates how recent advances on language modeling and traditional IR principles such as indexing can help each other for better learning and inference. Source code and data are available at https://github.com/Wenyueh/LLM-RecSys-ID.",
  "full_text": "How to Index Item IDs for Recommendation Foundation Models\nWenyue Hua\nRutgers University\nwenyue.hua@rutgers.edu\nShuyuan Xu\nRutgers University\nshuyuan.xu@rutgers.edu\nYingqiang Ge\nRutgers University\nyingqiang.ge@rutgers.edu\nYongfeng Zhang\nRutgers University\nyongfeng.zhang@rutgers.edu\nABSTRACT\nRecommendation foundation model utilizes large language models\n(LLM) for recommendation by converting recommendation tasks\ninto natural language tasks. It enables generative recommendation\nwhich directly generates the item(s) to recommend rather than\ncalculating a ranking score for each and every candidate item as\nin traditional recommendation models, simplifying the recommen-\ndation pipeline from multi-stage filtering to single-stage filtering.\nTo avoid generating excessively long text and hallucinated recom-\nmendations when deciding which item(s) to recommend, creating\nLLM-compatible item IDs to uniquely identify each item is essential\nfor recommendation foundation models. In this study, we system-\natically examine the item ID creation and indexing problem for\nrecommendation foundation models, using P5 as an example of\nthe backbone LLM. To emphasize the importance of item indexing,\nwe first discuss the issues of several trivial item indexing methods,\nsuch as random indexing, title indexing, and independent indexing.\nWe then propose four simple yet effective solutions, including se-\nquential indexing, collaborative indexing, semantic (content-based)\nindexing, and hybrid indexing. Our study highlights the signifi-\ncant influence of item indexing methods on the performance of\nLLM-based recommendation, and our results on real-world datasets\nvalidate the effectiveness of our proposed solutions. The research\nalso demonstrates how recent advances on language modeling and\ntraditional IR principles such as indexing can help each other for\nbetter learning and inference. Source code and data are available at\nhttps://github.com/Wenyueh/LLM-RecSys-ID.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíRecommender systems ; ‚Ä¢ Computing\nmethodologies ‚ÜíMachine learning ; Natural language processing .\nKEYWORDS\nLarge Language Model; Recommendation; Item ID and Indexing\nACM Reference Format:\nWenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. 2023. How\nto Index Item IDs for Recommendation Foundation Models. In Annual\nInternational ACM SIGIR Conference on Research and Development in In-\nformation Retrieval in the Asia Pacific Region (SIGIR-AP ‚Äô23), November\n26‚Äì28, 2023, Beijing, China. ACM, New York, NY, USA, 10 pages. https:\n//doi.org/10.1145/3624918.3625339\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR-AP ‚Äô23, November 26‚Äì28, 2023, Beijing, China\n¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0408-6/23/11. . . $15.00\nhttps://doi.org/10.1145/3624918.3625339\n1 INTRODUCTION\nFoundation Models such as Large Language Models (LLMs) [3, 4, 27]\nhave significantly impacted research areas such as natural language\nprocessing (NLP) and computer vision (CV) [ 19], and have been\napplied to various recommender system (RS) tasks. Recent research\nsuch as P5 [9] and M6Rec [6] leverage the advantages of pre-trained\nLLMs for recommendation [17]: they incorporate rich user behavior\nand knowledge information into pre-training and benefit from the\nstrong learning ability of foundation models for recommendation.\nPre-trained LLMs also have improved reasoning ability [11] to in-\nfer user interests based on the context. Therefore, these models\naim to utilize LLMs pre-trained on extensive natural language cor-\npora for RS by transforming recommendation tasks into language\ngeneration tasks, enabling generative recommendation.\nSince item description may include a large number of words\n(e.g., a product title/description could include tens/hundreds of\nwords and a news article could include thousands of words), we\ncan hardly expect an LLM to generate the complete and exact item\ndescription when deciding which item(s) to recommend, because\nthe generated text may not even correspond to a real existing item\nin the item database, leading to the hallucination problem [8, 18] in\nLLM-based recommendation. As a result, it is important to assign a\nunique ID to each item so that each item is represented by a small\nnumber of characteristic tokens while being distinguishable from\neach other. For example, a business location in Yelp may be assigned\nthe ID ‚Äúlocation_4332‚Äù and be further represented as a sequence\nof tokens such as ‚ü®location‚ü©‚ü®_‚ü©‚ü®43‚ü©‚ü®32‚ü©[9]. Note that the item ID\nmay not necessarily be number tokens, rather, as long as it is a\nunique identifier for an item, then it may be considered as an ID\nfor the item. For example, the title of the movie ‚ÄúThe Lord of the\nRings‚Äù can be considered as the ID of the movie, which consists\nof a sequence of word tokens rather than number tokens. The ID\nmay even be a sequence of words that do not convey an explicit\nmeaning, e.g., ‚Äúring epic journey fellowship adventure‚Äù.\nHowever, assigning LLM-compatible IDs to items is not a trivial\ntask. First, there could be a huge amount of or even infinite items\nwhile each item should be assigned a unique ID so that items are\ndistinguishable from each other for the foundation model. Second,\nitem IDs should be compatible with natural language so that IDs can\nbe integrated into natural language instructions for the pre-training,\nfine-tuning and prompting of LLMs. Third, trivial item indexing\nmethods such as random indexing may not help and may even hurt\nthe recommendation foundation models since they may mistakenly\nassign related IDs to unrelated items, misleading the training and\nprompting of LLMs. As a result, a comprehensive examination for\nLLM-oriented item indexing is needed, which enables the seamless\nadaptation of recommendation tasks to be compatible with LLMs,\nharnessing the potential of LLMs for recommendation.\nBesides, a natural idea to ensure the generated text align with\nreal items so as to avoid the hallucination problem is to employ a\narXiv:2305.06569v6  [cs.IR]  26 Sep 2023\nSIGIR-AP ‚Äô23, November 26‚Äì28, 2023, Beijing, China Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang\nconstrained decoding method [7]. However, utilizing constrained\ngeneration for free-form long text is impractical. This is because\nconstrained decoding essentially prescribes a singular mode of\nexpressing the content, negating the flexible nature of long-text\nnarratives. By compelling the model to adhere to specific descriptive\npatterns, the model is required to memorize rigid text patterns in\naddition to recommendation-specific knowledge. This additional\ncomplexity can dilute the primary purpose of the model and hinder\nits efficacy in performing the core recommendation task.\nMotivated by the above reasons, this paper concentrates on the\nitem indexing problem for LLM-based recommenders: how to assign\na unique ID (i.e., token sequence) for each item. We study the issue\nbased on P5 [9], a representative LLM for RS model. P5 employs\npre-training over foundation models and converts recommenda-\ntion tasks into natural language sentences based on personalized\nprompts. We first experiment on three trivial indexing methods and\nshow their limitations, some of which were employed in previous\nmodels: Independent Indexing (IID), Title Indexing (TID), and Ran-\ndom Indexing (RID). Based on the analysis, we further explore four\nnovel indexing techniques: Sequential Indexing (SID), Collaborative\nIndexing (CID), Semantic (content-based) Indexing (SemID), and\nHybrid Indexing (HID). To ensure the generated IDs align with\nreal items during the recommendation stage so as to avoid hallu-\ncination, we develop a constrained decoding method [ 7], which\nis facilitated by crafting a prefix tree (a.k.a a trie) from the set of\nvalid IDs and setting the generation probability of non-existent IDs\nto zero during the decoding phase. We show the performance of\nvarious ID methods on three widely-used datasets (Amazon Beauty,\nAmazon Sports, Yelp) and provide insights about the performance\nof different methods for LLM-based recommendation models.\n2 RELATED WORK\nMany traditional recommendation models use a matching-based\nparadigm [1, 2, 13, 15, 29, 30, 32]. They project users (or user be-\nhavior history) and items into a shared embedding space and then\nestimate a user‚Äôs preference for an item by calculating the ranking\nscore using their embedding vectors, such as the inner product\nbetween the user and item vectors in matrix factorization [15]. Usu-\nally, this involves calculating ranking scores for each and every\ncandidate item, making the matching and sorting process time con-\nsuming when the item pool is large [ 34]. As a result, industrial\nRS usually has to use the multi-stage (usually two-stage) filtering\npipeline [5], where simple and efficient filtering methods such as\nrule-based filtering methods are used at early stages, while ad-\nvanced filtering methods are used at later stages where candidate\nitems are fewer. As a result, the most advanced models are only\napplied on a small subset of items.\nRecently, there have been multiple attempts to pre-train foun-\ndational models for generative recommendation, which spare the\nexpensive one-by-one candidate item matching process and in-\nstead directly generate the item to recommend. For example, P5 [9]\nunifies diverse recommendation tasks as natural language genera-\ntion tasks within a sequence-to-sequence generation framework.\nRecommendation data such as user-item interactions, user descrip-\ntions, item metadata, and user reviews are converted to a common\nformat‚Äînatural language sequences‚Äîusing multiple personalized\nprompt templates. Each user or item is represented by a unique\nsequence of tokens as the user or item ID. M6Rec [6] converts vari-\nous recommendation tasks, such as content supply, delivery, and\npresentation, into natural language understanding or generation\ntasks. Input prompts incorporate user attributes, past behaviors,\nand detailed item descriptions provided by sellers. Users and items\nare represented as pre-computed embeddings from their attributes\nand descriptions. LMRecSys [33] converts item-based recommen-\ndation tasks to text-based cloze tasks. The model is tested on the\nMovieLens-1M dataset [10], which includes movies that pre-trained\nLLMs may have seen in web text. Items are represented by their\ntitles that function as indices. This indexing method negatively af-\nfects the model performance as reported in the original paper: LLMs\nare not only ineffective for inferring the probability distribution of\na multi-token span, but also the linguistic bias contained in titles\nmay mislead the model as the title could contain little information\nabout the content of the movie.\nThe three models use different methods to index items: P5 uses\nnumber tokens, M6Rec uses metadata-based embeddings, and LM-\nRecSys uses item titles. This paper studies different item index-\ning methods under the LLM-based generative recommendation\nframework using P5 as an example backbone, which compares the\neffectiveness of different indexing methods, sheds light on the rela-\ntionship between item indexing and foundation model pre-training,\nand also provides insights about which item indexing methods are\nmost suitable for pre-training recommendation foundation models.\n3 PRELIMINARIES AND PRECEDING STUDY\n3.1 Introduction of P5 Paradigm\nThis paper studies the indexing problem based on P5 [ 9]. P5 is a\nrepresentative recommendation foundation model which enhances\nthe generalization capabilities of existing recommender systems\nby integrating various tasks and personalized instruction prompts\nto pre-train a foundation model for recommendation. These tasks\ninclude sequential recommendation, rating prediction, explanation\ngeneration, review summarization, and direct recommendation.\nP5 is trained using input-target pairs of texts generated from a\ncollection of prompt templates featuring personalized fields for\ndistinct users and items: an example input prompt for sequential\nrecommendation can be a description of user-item interactions\nsuch as ‚ÄúAccording to the places user_1 has visited: location_1123,\nlocation_4332, location_8463, location_12312, can you recommend\nanother place for the user?‚Äù and the output text is the next generated\nitem such as ‚ÄúOutput: location_1934‚Äù. In this study, we focus on\nthe sequential recommendation task since it explicitly relies on the\nitem interactions presented in the input prompt, making it highly\nsensitive to different indexing methods.\n3.2 The Angle Bracket Notation\nIn this paper, we need to introduce Out-of-vocabulary (OOV) tokens\nto construct item indices in some indexing methods, which are\ntokens that are not part of the normal vocabulary of the language\nmodel. In our case, they are tokens that do not exist in the default\nT5 vocabulary [23]. To distinguish the newly created OOV tokens\nfrom existent tokens, we use angle brackets ‚Äú‚ü®‚ü©‚Äù to represent the\nnewly created OOV token, and use text without ‚Äú‚ü®‚ü©‚Äù to represent\nHow to Index Item IDs for Recommendation Foundation Models SIGIR-AP ‚Äô23, November 26‚Äì28, 2023, Beijing, China\n#User #Item #Interactions Sparsity( %)\nSports 35,598 18,357 296,337 0.0453\nBeauty 22,363 12,101 198,502 0.0734\nYelp 30,431 20,033 316,354 0.0519\nTable 1: Basic statistics of datasets\nan existent token in the default tokenizer. All OOV tokens are\nrandomly initialized in the model and thus the text enclosed in ‚Äú‚ü®‚ü©‚Äù\ndoes not influence embeddings of the OOV tokens. The text within\nangle brackets ‚Äú‚ü®‚ü©‚Äù could be words or numbers, but no matter which\ncase, the text within angle brackets only functions to distinguish\ndifferent OOV tokens and it is irrelevant to the existent tokens.\nFor example, ‚ü®restaurant‚ü©‚ü®Greek‚ü©‚ü®2‚ü©is the index for an item in\nYelp consisting three OOV tokens, where ‚ü®restaurant‚ü©is a different\ntoken from the plain English word ‚Äúrestaurant‚Äù, and‚ü®2‚ü©is a different\ntoken from the number ‚Äú2‚Äù. When we need to use the existent plain\nword tokens, we will use them without the angle brackets, such as\n‚Äúrestaurant‚Äù and ‚Äú2‚Äù.\n3.3 Data Format and Prepossessing\nExperiments are conducted on Amazon Sports & Outdoors, Ama-\nzon Beauty, and the Yelp dataset. The Amazon datasets [22]1 are\nsourced from Amazon.com for product recommendations, while\nthe Yelp dataset2 provides a collection of user ratings and reviews\nfor business recommendation. We use transaction records from Jan\n1, 2019 to Dec 31, 2019, as in the original P5 paper [9]. The detailed\nstatistics for these datasets can be found in Table 1.\nThese datasets organize user-item interactions by individual\nusers. We split the datasets into training, validation, and testing by\nthe frequently used leave-one-out setting: for each user‚Äôs interac-\ntion sequence, we put the second-to-last item into the validation\nset, put the last item into the testing set, and put all other items of\nthe sequence into the training set. For example, suppose the inter-\naction sequence of userùëñ is {itemùëñ,1, itemùëñ,2, itemùëñ,3, ¬∑¬∑¬∑ , itemùëñ,ùëò ‚àí1,\nitemùëñ,ùëò }. Then the prediction of itemùëñ,ùëò ‚àí1 based on the sequence\n{itemùëñ,1, itemùëñ,2, itemùëñ,3, ¬∑¬∑¬∑ , itemùëñ,ùëò ‚àí2} is used for validation and the\nprediction of itemùëñ,ùëò based on the sequence {itemùëñ,1, itemùëñ,2, itemùëñ,3,\n¬∑¬∑¬∑ , itemùëñ,ùëò ‚àí1} is used for testing.\n3.4 Motivating Analysis of Item Indexing\nWe motivate the exploration of indexing methods starting from\nthree trivial indexing methods:\n‚Ä¢Random Indexing (RID): Assigning each item with a random\nnumber as the item ID. The number is further tokenized into a se-\nquence of sub-tokens based on the SentencePiece tokenizer [24],\nas did in P5 [9]. For example, a Yelp item is randomly assigned\nthe number ‚Äú4332‚Äù, and ‚Äú4332‚Äù is represented as a sequence of\ntokens ‚Äú43‚Äù‚Äú32‚Äù.\n‚Ä¢Title Indexing (TID): Using the item title to represent the item\nwhich is also tokenized by SentencePiece [24]. For example, the\nYelp item ‚ÄúLas Vegas Cigar Outlet‚Äù is represented as a sequence\nof tokens ‚ÄúLas‚Äù‚ÄúVegas‚Äù‚ÄúCi‚Äù‚Äúgar‚Äù‚ÄúOutlet‚Äù.\n‚Ä¢Independent Indexing (IID): Creating an independent OOV extra\ntoken that needs to be learned for each item. For example, a Yelp\n1https://cseweb.ucsd.edu/ jmcauley/datasets/amazon_v2/\n2https://www.yelp.com/dataset\nitem is represented as‚ü®IID5‚ü©which is an independent extra token\nspecifically allocated for this item. In the rest of the paper, tokens\ncreated for IID will always start with the letters ‚ÄúIID‚Äù.\nRID generates random numerical indices, leading to potential\noverlaps between unrelated items after tokenization. For example,\ntwo items ‚Äú4332‚Äù and ‚Äú4389‚Äù would be tokenized into ‚Äú43‚Äù‚Äú32‚Äù and\n‚Äú43‚Äù‚Äú89‚Äù, respectively, which means that they always share the same\nsub-token ‚Äú43‚Äù even though the two items may be totally unrelated\nwith each other. This unintended overlap may establish arbitrary\nrelationships among items, introducing unwanted bias to model\ntraining. As the overlaps stem from the index structure, they are\nimpossible to eliminate no matter how the model learns from data.\nConsequently, RID is considered an unfavorable method.\nTID makes the task more challenging since the model needs to\nmemorize and generate lengthy item titles. Besides, certain words\nor expressions in the title could be unrelated to the real content of\nthe item, also, very different items may share overlapping tokens in\ntheir title, and thus semantics derived from the titles may introduce\nstrong linguistic biases [33]. For example, the movies ‚ÄúThe Lord\nof the Rings‚Äù and ‚ÄúThe Lord of War‚Äù share many tokens in their\ntitles (‚Äúthe‚Äù, ‚Äúlord‚Äù, ‚Äúof‚Äù), but they are two very different movies:\nthe former is an epic fantasy, while the later is a crime drama. In\ngeneral, two irrelevant items could have very similar titles, such as\nApple the fruit and Apple the company, while two closely related\nitems may have very different titles, such as the classic ‚Äúbeer and\ndiaper‚Äù example in data mining [ 17]. As a result, using title as\nID may encode misleading semantics into the generation process,\nsimilar as the problem of random indexing.\nIID uses single-token indices for items without assuming any\nprior information about the items, making the item representations\neasier for language models to learn compared to RID and TID.\nThough better than RID and TID, it still has limited performance\ndue to considering all items independent from each other when\ncreating item IDs. It could also incur prohibitively long training\ntime if a large number of new tokens are required to create.\nThe aforementioned analysis implies that none of the three meth-\nods is optimal. To validate this, we provide experimental results to\nshow their suboptimal performance. We evaluate the three indexing\nmethods against two strong and widely-used baselines: SASRec\n[14] and S3-Rec [35]. Results are shown in Table 2, where the best\nresult for each metric is highlighted in bold and the second-best\nresult is underlined with wavy lines. Based on Table 2, RID and\nTID underperform relative to the baselines, while IID offers minor\ngains at the cost of introducing more learnable tokens because each\nitem is considered as an independent new token. As a result, these\nindexing methods are considered suboptimal and we will further\nexplore nontrivial indexing methods in the next section.\n4 NONTRIVIAL INDEXING METHODS\nBased on the above analysis, an optimal item indexing method\nshould meet two criteria to enable an effective learning process:\n(1) Maintaining a suitable length to mitigate the text generation\ndifficulty.\n(2) Integrating prior information to item index structure to ensure\nthat similar items share a maximum number of tokens while\ndistinguishable, and dissimilar items share minimal tokens.\nSIGIR-AP ‚Äô23, November 26‚Äì28, 2023, Beijing, China Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang\nMethod Amazon Sports Amazon Beauty Yelp\nHR@5 NCDG@5 HR@10 NCDG@10 HR@5 NCDG@5 HR@10 NCDG@10 HR@5 NCDG@5 HR@10 NCDG@10\nSASRec 0.0233 ::::0.0154 0.0350 0.0192 0.0387 ::::0.0249 0.0605 0.0318 0.0170 0.0110 0.0284 0.0147\nS3-Rec ::::0.0251 0.0161 ::::0.0385 0.0204 ::::0.0387 0.0244 0.0647 :::::0.0327 0.0201 0.0123 ::::0.0341 0.0168\nRID 0.0208 0.0122 0.0288 0.0153 0.0213 0.0178 0.0479 0.0277 ::::0.0225 0.0159 0.0329 :::::0.0193\nTID 0.0000 0.0000 0.0000 0.0000 0.0182 0.0132 0.0432 0.0254 0.0058 0.0040 0.0086 0.0049\nIID 0.0268 0.0151 0.0386 :::::0.0195 0.0394 0.0268 ::::0.0615 0.0341 0.0232 ::::0.0146 0.0393 0.0197\nTable 2: Performances of the trivial indexing methods for P5 as well as the baselines. The numbers in bold represent\nthe best results, while the numbers with a wave represent the second-best results. The results for RID and TID are\nsignificantly worse on Sports and Beauty, with a ùëù-value < 0.05 under the paired Student‚Äôs t-test protocol.\nTo achieve these objectives, we introduce and explore four index-\ning methods of increasing complexity: Sequential Indexing (SID),\nCollaborative Indexing (CID), Semantic (content-based) Indexing\n(SemID), and Hybrid Indexing (HID). SID and CID leverage collab-\norative information, enabling co-occurring items to share tokens.\nSemID employs metadata in natural language, allowing semanti-\ncally similar items to share tokens. HID combines multiple indexing\nmethods, seeking to capitalize on the strengths of each approach\nin order to generate optimal indices. In the following subsections,\nwe will provide details of the four indexing methods.\n4.1 Sequential Indexing\nSequential indexing is a straightforward method to leverage collabo-\nrative information for item indexing. Items interacted consecutively\nby a user are assigned consecutive numerical indices, reflecting\ntheir co-occurrence. Take Table 3 as an example, items are assigned\nwith IDs consecutively starting from the first user and all the way to\nthe last user. If an item has already been indexed in previous users‚Äô\ninteraction sequence, such as item 1001 in User 2‚Äôs sequence (and all\nother squared items in the table), then the item‚Äôs already assigned\nID will be used, otherwise, an incremental new ID will be created\nand assigned to the item. Notice that the item indexing process only\ndepends on the training sequences, while the validate and testing\nitems do not participate in the indexing process. After the indexing\nprocess is finished, the validation and testing items are assigned the\ncorresponding IDs that have already been established during the\nindexing process. Upon tokenization based on the SentencePiece\ntokenizer [24], an ID such as ‚Äú1001‚Äù will be tokenized into ‚Äú100‚Äù‚Äú1‚Äù,\nwhile ‚Äú1002‚Äù will be tokenized into ‚Äú100‚Äù‚Äú2‚Äù, resulting in the shared\ntoken ‚Äú100‚Äù for these two consecutive items. This gives us encoding\nsimilarity between those items that co-appear in at least one user‚Äôs\nsequence. As a result, this simple sequential indexing method is\nable to capture collaborative information on some occasions.\nOne minor note is that we initiate item index enumeration at 1001.\nWe initiate at 1001 instead of 1 for two reasons: 1) the SentencePiece\ntokenizer does not tokenize some numbers smaller than 1000 into\nmultiple sub-tokens, such as the number 12, and thus items assigned\nwith these small numbers will be completely independent of each\nother, 2) after tokenization, smaller numbers could become complete\nsubsets of larger tokenized numbers, e.g., ID ‚Äú12‚Äù can be a subset of\nID ‚Äú12‚Äù‚Äú34‚Äù, which may enforce false correlation between items.\nNevertheless, sequential indexing also has limitations: 1) Ad-\njacently indexed items not interacted together by the same user\nmay erroneously share tokens; for instance, the last item of User\n2 is indexed as 1014 (tokenized as ‚Äú10‚Äù‚Äú14‚Äù) and the first item of\nUser 3 is indexed as 1015 (tokenized as ‚Äú10‚Äù‚Äú15‚Äù), then the token\n‚Äú10‚Äù will be shared despite a lack of co-occurrence between the two\nitems, 2) it cannot capture similarities based on the frequency of\nco-occurrence; for example, suppose items 1001 and 1002 co-occur\nonce while items 1002 and 1003 co-occur ten times, both pairs will\nstill share only one token, failing to convey frequency information,\nand 3) user ordering in the training data affects the results; for\nexample, if we exchange the rows of User 1 and User 2 in Table 3,\nthen the indexing result would be different. Although sequential\nindexing has its shortcomings, it can still yield relatively favorable\nresults that are close to, or even surpass, the baselines.\n4.2 Collaborative Indexing\nSequential Indexing is a preliminary method for integrating col-\nlaborative information into item indexing. To effectively capture\nthe essence of collaborative filtering, we explore the Collabora-\ntive Indexing (CID) approach, which employs spectral clustering\nbased on Spectral Matrix Factorization (SMF) [21, 28] to generate\nitem indices. This method is based on the premise that items with\nmore frequent co-occurrence are more similar and should share\nmore overlapping tokens in index construction. The core concept\ninvolves constructing a co-occurrence graph for all items based on\nthe training dataset and using spectral clustering to group items\ninto clusters, ensuring that items within the same cluster share\ntokens when constructing indices.\n4.2.1 Spectral Clustering based on Spectral Matrix Factorization.\nTo elaborate, we create a graph based on the training set, as exam-\npled in Figure 1(a): each item serves as a node, edges between two\nitems represent their co-occurrence (i.e., two items co-appear in\na user‚Äôs interaction sequence), and the edge weights indicate the\nfrequency of co-occurrence (i.e., the number of user interaction\nsequences in which two items co-appear). The adjacency matrix\ncorresponding to the graph (Figure 1(b)) indicates the similarity be-\ntween items in terms of co-appearance frequency, and the Laplacian\nmatrix corresponding to the graph (Figure 1(c)) can be factorized to\nenable spectral clustering [21, 28]. The spectral clustering process\ngroups items into clusters so that items sharing more co-appearance\nsimilarity are grouped into the same cluster; each cluster can be\nfurther grouped into finer-grained clusters by recursively applying\nthe spectral clustering process within the big cluster, resulting in\nhierarchical levels of clusters, as shown in Figure 1(a).\nHow to Index Item IDs for Recommendation Foundation Models SIGIR-AP ‚Äô23, November 26‚Äì28, 2023, Beijing, China\nTraining Sequence Validation Testing\nUser 1 1001 1002 1003 1004 1005 1006 1007 1008 1009 1018 1019\nUser 2 1010 1011 1001 1012 1008 1009 1013 1014 1022 1023\nUser 3 1015 1016 1017 1007 1018 1019 1020 1021 1009 1015 1016\nUser 4 1022 1023 1005 1002 1006 1024 1002 1008\nUser 5 1025 1026 1027 1028 1029 1030 1024 1020 1021 1031 1033 1034\nTable 3: An illustration of Sequential Indexing method. Numbers in boxes represent previously indexed items.\n(a) Recursive spectral clustering on item co-appearance graph\n (b) Adjacency matrix\n (c) Laplacian matrix\nFigure 1: Illustration of spectral clustering on the item co-appearance graph based on spectral matrix factorization\nMore specifically, spectral clustering leverages the eigenvectors\nof the Laplacian matrix to group nodes into clusters [ 21, 28]. It\nensures that items within the same cluster have a higher degree\nof similarity while items in different clusters exhibit lower simi-\nlarity. We use the standard spectral clustering implementation in\nthe Python scikit-learn package3. We do not expand too many de-\ntails of the spectral clustering algorithm since it is considered a\ntextbook-level algorithm for data analysis [ 16]. However, we do\nwant to discuss the two important parameters that are used to con-\ntrol the recursive clustering process: 1) ùëÅ : we divide the items into\nùëÅ clusters at each level of the clustering, and 2) ùëò: the maximum\nnumber of items allowed in the final cluster, which serves as the\nstopping criterion of the recursive clustering process, i.e., when a\ncluster contains at most ùëò items, we will not further reduce its size.\nFinally, the clustering result can be formulated into a hierarchical\ntree structure, as shown in Figure 2. In this figure, each non-leaf\nnode (large yellow nodes in the graph) represents the clusters cre-\nated at the corresponding level, and each leaf node (small blue\nnodes) represents an item in the corresponding final cluster. In the\nnext subsection, we will introduce how to create item IDs based on\nthe hierarchical tree structure.\n4.2.2 Item Indexing based on the Spectral Clustering Tree.As men-\ntioned above, the recursive clustering process generates a tree struc-\nture for the clusters and items, as shown in Figure 2 usingùëÅ = 4 and\nùëò = 20 as an example, which means that each iteration of spectral\nclustering divides items into 4 clusters, and the process is recur-\nsively applied on each cluster until the cluster size is smaller than\nor equal to 20. Each non-leaf node (large yellow node) represents\na cluster while all items present as leaf nodes (small blue nodes)\nunder the final cluster. Note that since the maximum number of\nitems allowed in the final cluster is ùëò, it means that we only need\nat most ùëò independent extra tokens to distinguish the items within\n3https://scikit-learn.org/stable/modules/generated/sklearn.cluster.\nSpectralClustering.html\nFigure 2: Collaborative indexing based on the spectral\nclustering tree ( ùëÅ = 4, ùëò = 20).\nthe same final cluster (i.e., the small blue nodes under the same\nyellow node is at most ùëò). As a result, we introduce ùëò independent\nextra tokens into the vocabulary, noted as ‚ü®0‚ü©, ‚ü®1‚ü©, ‚ü®2‚ü©, ¬∑¬∑¬∑ , ‚ü®ùëò ‚àí1‚ü©.\nWe first assign tokens to the non-leaf nodes. The non-leaf nodes\nare enumerated level by level across the whole tree using the ùëò\nindependent tokens beginning from ‚ü®0‚ü©to ‚ü®ùëò ‚àí1‚ü©, as shown in\nFigure 2. Once all ùëò tokens are used, we simply restart from ‚ü®0‚ü©. As\nmentioned before, each parent cluster node has ùëÅ children cluster\nnodes. However, if ùëÅ > ùëò, then we would not have enough tokens\nto distinguish the different children under the same parent node. As\na result, we requireùëÅ ‚â§ùëò for collaborative indexing. Together with\nthe level-by-level token assignment process, this can guarantee that\ndifferent children nodes under the same parent node are assigned\ndifferent tokens.\nWe then assign tokens to leaf nodes (small blue nodes), where\neach leaf node is an item. This is rather straightforward: for each\nSIGIR-AP ‚Äô23, November 26‚Äì28, 2023, Beijing, China Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang\nfinal cluster, we assign each of its children item node with an inde-\npendent extra token, beginning from ‚ü®0‚ü©and on-wards. Since the\nclustering process ensures that each final cluster contains at mostùëò\nitems, so the ùëò independent extra tokens are enough to distinguish\ndifferent items under the same final cluster.\nFinally, the ID of an item is the concatenation of its non-leaf an-\ncestor nodes‚Äô tokens and its own leaf node token. For example, the\nitem under the bolded path in Figure 2 is indexed as ‚ü®1‚ü©‚ü®9‚ü©‚ü®5‚ü©‚ü®4‚ü©.\nThis indexing process guarantees that any two items within the\nsame final cluster will share tokens until their own token within\nthe final cluster, which means that the more frequently two items\nco-occur, the more tokens they will share, well leveraging the col-\nlaborative information hidden in user behavior sequences.\n4.3 Semantic (Content-based) Indexing\nSemantic (content-based) Indexing (SemID) utilizes item metadata\nto construct IDs for items. As shown in Figure 3, items‚Äô categories\nform a hierarchical structure [36], with each non-leaf node (large\nyellow node) representing a category and each leaf node (small\nblue node) representing an item. Each non-leaf node is assigned an\nindependent extra token, and each leaf node receives a unique extra\ntoken under its parent node. To create an item index, the tokens of\nnon-leaf nodes and leaf nodes are concatenated along the path from\nroot to leaf. Take the bolded path in Figure 3 as an example, the\nitem‚Äôs categories range from coarse to fine-grained as ‚ü®Makeup‚ü©,\n‚ü®Lips‚ü©, ‚ü®Lip_Liners‚ü©, and its leaf node token is ‚ü®5‚ü©, which differenti-\nates this item from other items under the Lip Liners category, then\nthe item would be indexed as ‚ü®Makeup‚ü©‚ü®Lips‚ü©‚ü®Lip_Liners‚ü©‚ü®5‚ü©.\n4.4 Hybrid Indexing\nHybrid Indexing (HID) is not a single specific indexing method\nbut a category of methods. It concatenates multiple indices intro-\nduced above into one index, such as SID+IID, CID+IID, SemID+IID,\nSemID+CID, etc. This approach aims to leverage the advantages\nof different indexing techniques to produce better indices. In this\npaper we implement four combinations and here are the details:\nFor SID+IID: we append an independent extra token at the end\nof the sequential ID for each item. Suppose the SID of an item\nafter tokenization is ‚Äú10‚Äù‚Äú18‚Äù, and its IID index is ‚ü®IID982‚ü©, then\nthe HID index will be ‚Äú10‚Äù‚Äú18‚Äù‚ü®IID982‚ü©. Thus it contains some item\nco-appearance information from SID and meanwhile ensure the\nitem distinction through IID.\nFor CID and SemID, before we concatenate them with IID, we\nfirst remove the last token (the leaf node token) from them since\nthe last token simply functions to differentiate an item from others\nunder the same parent non-leaf node. For CID+IID: suppose an\nitem‚Äôs CID is ‚ü®1‚ü©‚ü®9‚ü©‚ü®5‚ü©‚ü®4‚ü©, and its IID is ‚ü®IID28‚ü©, then the item‚Äôs\nHID would be ‚ü®1‚ü©‚ü®9‚ü©‚ü®5‚ü©‚ü®IID28‚ü©. For SemID+IID: suppose an item‚Äôs\nSemID is ‚ü®Makeup‚ü©‚ü®Lips‚ü©‚ü®Lip_Liners‚ü©‚ü®5‚ü©, and its IID is ‚ü®IID1023‚ü©,\nthen the HID is ‚ü®Makeup‚ü©‚ü®Lips‚ü©‚ü®Lip_Liners‚ü©‚ü®IID1023‚ü©. The final\nindex incorporates both collaborative information from CID (or\nmetadata content information in SID), and a special IID token that\ndifferentiates the item from all others, ensuring item distinction\nwhile preserving the advantages of the CID (or SID).\nFor SemID+CID: we concatenate the SemID and CID in either\norder, hoping to combine both metadata content information and\nFigure 3: An example of semantic indexing\ncollaborative information. Since both SemID and CID contain leaf\nnode tokens to distinguish items under one parent node, we only\nneed to retain one of them, e.g., we retain the CID leaf node to-\nken. Suppose the SemID is ‚ü®Makeup‚ü©‚ü®Lips‚ü©‚ü®Lip_Liners‚ü©‚ü®5‚ü©and\nthe CID is ‚ü®1‚ü©‚ü®9‚ü©‚ü®5‚ü©‚ü®4‚ü©. If we put SemID first, the final HID index\nis ‚ü®Makeup‚ü©‚ü®Lips‚ü©‚ü®Lip_Liners‚ü©‚ü®1‚ü©‚ü®9‚ü©‚ü®5‚ü©‚ü®4‚ü©; otherwise, the HID\nindex is ‚ü®1‚ü©‚ü®9‚ü©‚ü®5‚ü©‚ü®4‚ü©‚ü®Makeup‚ü©‚ü®Lips‚ü©‚ü®Lip_Liners‚ü©.\nIn the following experiments, we will evaluate and compare the\nvarious different HIDs.\n5 EXPERIMENTS\n5.1 Dataset and Baselines\nThe datasets and their pre-processing methods have been intro-\nduced in Section 3.3. In this section, we introduce the baselines.\nWe apply the various item indexing methods into the P5 frame-\nwork [9] for sequential recommendation and compare with several\nrepresentative sequential recommendation methods as baselines:\nCaser [26]: This approach treats sequential recommendation as a\nMarkov Chain and utilizes convolutional neural network to model\nuser interests. HGN [20]: This approach leverages hierarchical\ngating networks to learn user behaviors from both long-term and\nshort-term perspectives. GRU4Rec [12]: Originally proposed for\nsession-based recommendation, this approach employs GRU to\nmodel the user click history sequence. BERT4Rec [25]: This ap-\nproach mimics BERT-style masked language modeling, learning a\nbidirectional representation for sequential recommendation. FDSA\n[31]: Focusing on feature transition patterns, this approach models\nthe feature sequence with a self-attention module. SASRec [14]:\nAdopting a self-attention mechanism in a sequential recommen-\ndation model, this approach reconciles the properties of Markov\nChains and RNN-based approaches. S3-Rec [35]: Leveraging self-\nsupervised objectives on meta information of items, this approach\nhelps the sequential recommendation model to better discover the\ncorrelations among different items and their attributes. For com-\nparison, we utilize the implementation of S3-Rec and its baselines.\n5.2 Implementation Details\nFollowing the P5 framework [9], our implementation utilizes T5 as\nthe backbone [23]: there are 6 layers for both encoder and decoder,\nHow to Index Item IDs for Recommendation Foundation Models SIGIR-AP ‚Äô23, November 26‚Äì28, 2023, Beijing, China\nMethod Amazon Sports Amazon Beauty Yelp\nHR@5 NCDG@5 HR@10 NCDG@10 HR@5 NCDG@5 HR@10 NCDG@10 HR@5 NCDG@5 HR@10 NCDG@10\nCaser 0.0116 0.0072 0.0194 0.0097 0.0205 0.0131 0.0347 0.0176 0.015 0.0099 0.0263 0.0134\nHGN 0.0189 0.0120 0.0313 0.0159 0.0325 0.0206 0.0512 0.0266 0.0186 0.0115 0.0326 0.159\nGRU4Rec 0.0129 0.0086 0.0204 0.0110 0.0164 0.0099 0.0283 0.0137 0.0176 0.0110 0.0285 0.0145\nBERT4Rec 0.0115 0.0075 0.0191 0.0099 0.0203 0.0124 0.0347 0.0170 0.0051 0.0033 0.0090 0.0090\nFDSA 0.0182 0.0122 0.0288 0.0156 0.0267 0.0163 0.0407 0.0208 0.0158 0.0098 0.0276 0.0136\nSASRec 0.0233 0.0154 0.0350 0.0192 0.0387 0.0249 0.0605 0.0318 0.0170 0.0110 0.0284 0.0147\nS3-Rec 0.0251 0.0161 0.0385 0.0204 0.0387 0.0244 0.0647 0.0327 0.0201 0.0123 0.0341 0.0168\nRID 0.0208 0.0122 0.0288 0.0153 0.0213 0.0178 0.0479 0.0277 0.0225 0.0159 0.0329 0.0193\nTID 0.000 0.000 0.000 0.000 0.0182 0.0132 0.0432 0.0254 0.0058 0.0040 0.0086 0.0049\nIID 0.0268 0.0151 0.0386 0.0195 0.0394 0.0268 0.0615 0.0341 0.0232 0.0146 0.0393 0.0197\nSID 0.0264 0.0186 0.0358 0.0216 0.0430 0.0288 0.0602 0.0368 0.0346 0.0242 0.0486 0.0287\nCID ::::0.0313 ::::0.0224 :::::0.0431 :::::0.0262 0.0489 0.0318 0.0680 0.0357 0.0261 0.0171 0.0428 0.0225\nSemID 0.0274 0.0193 0.0406 0.0235 0.0433 0.0299 0.0652 0.0370 0.0202 0.0131 0.0324 0.0170\nSID+IID 0.0235 0.0161 0.0339 0.0195 0.0420 0.0297 0.0603 0.0355 ::::0.0329 ::::0.0236 0.0465 :::::0.0280\nCID+IID 0.0321 0.0227 0.0456 0.0270 0.0512 0.0356 0.0732 0.0427 0.0287 0.0195 :::::0.0468 0.0254\nSemID+IID 0.0291 0.0196 0.0436 0.0242 ::::0.0501 ::::0.0344 :::::0.0724 :::::0.0411 0.0229 0.0150 0.0382 0.0199\nSemID+CID 0.0043 0.0031 0.0070 0.0039 0.0355 0.0248 0.0545 0.0310 0.0021 0.0016 0.0056 0.0029\nTable 4: Performance of all baseline results and all indexing methods under P5. Numbers in bold represent the best\nresults, numbers with a wavy underline represent the second-best results, and numbers with a straight underline\nindicate that they are better than the best baseline result. Results better than baselines here have been tested to be\nsignificant under the paired Student‚Äôs t-test protocol with ùëù-value < 0.05.\nthe model dimensionality is 512 with 8-headed attention. For to-\nkenization, we use the default SentencePiece tokenizer [24] with\na vocabulary size of 32,128 for parsing sub-word units. All inde-\npendent extra tokens are not further tokenized. We use the same\nsequential recommendation prompts as P5 [9] to convert sequential\ninformation into texts. We pre-train P5 for 20 epochs using AdamW\noptimizer on two NVIDIA RTX A5000 GPUs with a batch size of\n64, a peak learning rate of 1e-3. We apply warm-up for the first 5%\nof all training steps to adjust the learning rate.\nRID, TID, and SID do not involve creating OOV tokens since\ntheir item indices comprise tokens from the default T5 tokenizer,\nwhile IID, CID, SemID, and HID involve creating extra OOV to-\nkens, extending the original vocabulary. All tokens used in these\nindexing methods, excluding TID, are randomly initialized rather\nthan using T5‚Äôs pre-trained embeddings for initialization. This is\ndue to our observation that the pre-trained T5‚Äôs a priori semantics\nabout numbers adversely impact the learning of item semantics\nand the recommendation performance during experimentation. We\nuse T5‚Äôs pre-trained token embeddings for initializing TID tokens\nsince TID only involves plain word tokens.\n5.3 Overall Results\nThe overall experimental results are presented in Table 4 with all\nbaselines. The best result for each metric is highlighted in bold,\nwhile the second-best result is underlined with wavy lines. For each\nindexing method, if the result surpasses the best baseline result, it is\nemphasized by underlining with straight lines. In general, RID, TID\nand IID cannot beat the baseline results in most cases, while most\nof the advanced indexing methods (SID, CID, SemID and the HIDs)\nsurpass the baseline results. A more detailed breakdown analysis is\nas follows.\nIn Table 4, the first block contains all the baseline results. The\nsecond block contains the basic indexing methods, where RID and\nTID consistently perform worse than baselines, while IID in general\nperforms better. The third block contains three advanced indexing\nmethods. We can see that SID performs worse than CID and SemID\non Amazon datasets but better on Yelp, while CID performs better\nthan SemID across different datasets, indicating that constructing\nindices using collaborative information is more beneficial than\nusing metadata, because CID can better capture item relationships\nfrom user behaviors by collaborative learning from the wisdom of\nthe crowd, which could be more effective than only using items‚Äô\nmetadata. The fourth block in the table contains HID results with\nseveral different implementations: SID+IID, CID+IID, SemID+IID,\nand SemID+CID. CID+IID and SemID+IID perform much better\nthan all other indexing methods while SID+IID and SemID+CID\nperform worse. In the following subsections, we will further analyze\nthe results in the third and fourth blocks in detail based on more\ncomprehensive experiments.\n5.4 Different Settings of Sequential Indexing\nTable 4 shows that though simple in nature, SID can generate favor-\nable results that are close to or surpass baselines. In Section 4.1, we\nexplored the construction of SID and its limitations, specifically, the\nindexing result can be influenced by the user ordering, e.g., if we\nexchange the rows of User 1 and User 2 in Table 3, then the indexing\nresult would be different. In this section, we present the results of\nSID using four different user orderings, which substantiate this\nclaim and also suggest the most effective ordering to use:\nSIGIR-AP ‚Äô23, November 26‚Äì28, 2023, Beijing, China Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang\nMethod Amazon Sports Amazon Beauty Yelp\nHR@5 NCDG@5 HR@10 NCDG@10 HR@5 NCDG@5 HR@10 NCDG@10 HR@5 NCDG@5 HR@10 NCDG@10\nSASRec 0.0233 0.0154 0.0350 0.0192 0.0387 0.0249 0.0605 0.0318 0.0170 0.0110 0.0284 0.0147\nS3-Rec 0.0251 0.0161 :::::0.0385 0.0204 0.0387 0.0244 0.0647 0.0327 0.0201 0.0123 0.0341 0.0168\nSID-TSO ::::0.0264 ::::0.0186 0.0358 :::::0.0216 0.0430 0.0288 :::::0.0602 0.0368 0.0346 0.0242 0.0486 0.0287\nSID-RO 0.0214 0.0150 0.0291 0.0175 0.0392 0.0257 0.0512 0.0335 0.0324 0.0219 0.0461 0.0263\nSID-S2LO 0.0304 0.0230 0.0395 0.0259 0.0395 0.0259 0.0520 0.0337 ::::0.0335 ::::0.0237 0.0442 :::::0.0277\nSID-L2SO 0.0244 0.0176 0.0356 0.0209 ::::0.0409 ::::0.0286 0.0586 :::::0.0343 0.0316 0.0215 :::::0.0472 0.0265\nTable 5: Different settings of Sequential Indexing for P5 compared with two baselines on three datasets. The\nnumbers in bold represent the best results, while the numbers with a wave represent the second-best results. TSO\nresults in Amazon Beauty and Yelp are tested to be significant with respect to other settings.\nFigure 4: CID Beauty ablations on ùëÅ (number of clus-\nters at each level) and ùëò (maximum number of items\nallowed in the final cluster).\nFigure 5: CID average length on Beauty.\n(1) Time-Sensitive Ordering (TSO) : Users are ordered chrono-\nlogically in the original dataset based on their initial interaction\nwith the system. Subsequent interactions are recorded and new\nrecords are created for previously unrecorded users upon their\nfirst interaction with the system. By sorting and processing\ninteractions based on their timestamps, we ensure that users\nwith earlier initial interactions are recorded first.\n(2) Random Ordering (RO) : Users are ordered randomly.\n(3) Short-to-Long Ordering (S2LO) : Users are organized accord-\ning to their number of interactions, arranged in ascending order\nfrom the fewest to the most interactions.\n(4) Long-to-Short Ordering (L2SO) : Users are sorted in descend-\ning order from the most to the fewest interactions.\nTable 5 presents the performance of the four settings. Our ob-\nservations indicate that, in general, the relative performance is as\nfollows: Time-Sensitive > {Long-to-Short, Short-to-Long}> Ran-\ndom. The observations imply that time plays an important role in\nDataset Sports Beauty Yelp\nSASRec 0.0350 0.0605 0.0284\nS3-Rec 0.0385 0.0647 0.0341\nùëÅ = 10 ùëÅ = 20 ùëÅ = 10 ùëÅ = 20 ùëÅ = 10 ùëÅ = 20\nùëò=200 0.0302 0.0423 0.0566 0.0635 :::::0.0416 0.0428\nùëò=500 0.0400 :::::0.0431 0.0680 :::::0.0668 0.0388 0.0403\nùëò=1000 0.0435 0.0416 0.0658 0.0638 0.0385 0.0388\nTable 6: CID hit@10 results under different parame-\nters and datasets. Bold numbers are best results and\nunder-wave numbers are second-best results. The high-\nest scored settings in all datasets are tested to be signif-\nicant with respect to other settings under the paired\nStudent‚Äôs t-test with ùëù-value < 0.05.\nDataset Sports Beauty Yelp\nùëÅ = 10 ùëÅ = 20 ùëÅ = 10 ùëÅ = 20 ùëÅ = 10 ùëÅ = 20\nùëò=200 4.25 3.35 4.31 3.23 :::3.88 3.25\nùëò=500 3.66 :::3.66 3.80 :::2.94 3.57 2.91\nùëò=1000 3.31 2.78 3.54 3.54 3.21 2.76\nTable 7: Average ID lengths under different parame-\nters. Bold numbers in this table correspond to the best\nresults in Table 6 (i.e., bold numbers in Table 6).\nsequential indexing: items that are interacted at similar times, even\nby different users, may be more similar to each other compared to\nitems being interacted at vastly different times. As a result, items\nthat occurred at similar times are more likely to be co-interacted\nby certain users. Thus, using the time-related information when\nordering users is likely to improve the performance.\nConsidering these observations, we recommend that fu-\nture implementations of the simple SID method consider\nusing the time-sensitive user ordering strategies to enhance\nperformance. Note that the original Amazon and Yelp datasets\nalready used a time-sensitive ordering to arrange the users. As a re-\nsult, to generate indices using SID, we simply need to incrementally\nindex the items from the first user all the way to the last user.\n5.5 Different Settings of Collaborative Indexing\nCID involves two hyper-parameters: ùëÅ and ùëò, where ùëÅ is the num-\nber of clusters at each level of the clustering, andùëò is the maximum\nHow to Index Item IDs for Recommendation Foundation Models SIGIR-AP ‚Äô23, November 26‚Äì28, 2023, Beijing, China\nBeauty > Skin Care > Eyes > Combinations Beauty > Skin Care > Eyes > Creams\nBeauty > Makeup > Makeup Remover > Eyes Beauty > Makeup > Body > Moisturizers > Creams\nTable 8: Examples of non-tree structure categories in Amazon Beauty dataset.\nMethod Amazon Sports Amazon Beauty Yelp\nHR@5 NCDG@5 HR@10 NCDG@10 HR@5 NCDG@5 HR@10 NCDG@10 HR@5 NCDG@5 HR@10 NCDG@10\nSASRec 0.0233 0.0154 0.0350 0.0192 0.0387 0.0249 0.0605 0.0318 0.0170 0.0110 0.0284 0.0147\nS3-Rec 0.0251 0.0161 0.0385 0.0204 0.0387 0.0244 0.0647 0.0327 ::::0.0201 ::::0.0123 0.0341 :::::0.0168\nSemID-non-tree 0.0281 ::::0.0192 0.0410 :::::0.0233 ::::0.0423 ::::0.0288 ::::0.0632 :::::0.0354 0.0028 0.0019 0.0050 0.0025\nSemID-tree ::::0.0274 0.0193 ::::0.0406 0.0235 0.0433 0.0299 0.0652 0.0370 0.0202 0.0131 ::::0.0324 0.0170\nTable 9: SemID results under different settings. Bold numbers are best results and under-wave numbers are second-\nbest. Tree setting results in Amazon Beauty and Yelp are tested to be significant with respect to non-tree setting.\nnumber of items allowed in the final cluster. Varying these hyper-\nparameters results in different numbers of independent extra tokens\nand recommendation performances.\nIn Figure 4, we present hit@10 results for various ùëÅ and ùëò value\ncombinations on the Beauty dataset. When ùëò = 50, the perfor-\nmance is below 4.5%, which is significantly lower than the baselines\nand some basic indexing methods. However, when ùëò is greater\nthan 100, the performances improve considerably. Furthermore,\nTable 6 shows hit@10 results for multiple configurations with\nùëò ‚àà {200, 500, 1000}, ùëÅ ‚àà {10, 20}and on all three datasets. In\nthese different settings, nearly all the CID results outperform the\nbaselines, indicating that CID is relatively easy to fine-tune with\nrespect to its hyper-parameters.\nBased on our observations, we can draw the following conclu-\nsions: (1) Extremely small ùëò values lead to suboptimal performance\nregardless of the chosen ùëÅ . When ùëò = 50, the performance is below\nthe baselines. This can be attributed to the limited expressiveness of\na small number of new tokens, which cannot adequately capture the\ndiversity of items. (2) Differentùëò and ùëÅ combinations yield varying\nID lengths (i.e., the number of tokens in an ID). We compute the\naverage ID length for each ùëò and ùëÅ hyper-parameter setting, and\nthe results are shown in Figure 5 (for Beauty) and Table 7 (for all\ndatasets). Combining Figure 4 and 5, as well as Table 6 and 7, we\nfind that the optimal recommendation results are usually observed\nwhen the average ID length is between 3 and 4. For example, the\nsquared points in Figure 5 shows all cases whose average ID length\nis between 3 and 4 for the Beauty dataset, and we can see that these\npoints also correspond to the optimal performance on each line in\nFigure 4. Similarly, the best or second-best results in Table 6 also\ncorresponds to 3‚àº4 ID lengths in Table 7 in most cases.\nBased on these observations, we recommend that future\nCID implementations use hyperparameters that generate\nan average ID length between 3 and 4. However, it is worth\nnoting that different datasets may require slightly different\nlengths for optimal performance.\n5.6 When will Semantic Indexing Work\nSemID uses metadata to construct item indices. In our experiments,\nwe observe that if the categories follow a hierarchical tree structure,\nthen the performance tends to improve. Category information in\ndatasets is usually not a tree structure because in some cases, one\ncategory name can occur under different parent categories, which\nmakes the categories into a graph but not a tree. Table 8 are two\nexamples in Amazon Beauty, where the category ‚ÄúEyes‚Äù occurs\nunder both ‚ÄúSkin Care‚Äù and ‚ÄúMakeup Remover‚Äù, and the category\n‚ÄúCreams‚Äù occurs under both ‚ÄúSkin Care‚Äù and ‚ÄúMoisturizers‚Äù.\nTo test whether the tree structure in categories is crucial, we\ncompare two different settings in our experiments:\n(1) Non-tree-structure setting: we directly use the category names\nto create the corresponding independent OOV extra tokens.\nFor example, an item under ‚ÄúBeauty‚Äù, ‚ÄúSkin Care‚Äù, ‚ÄúEyes‚Äù, and\nanother item under ‚ÄúBeauty‚Äù, ‚ÄúMakeup‚Äù, ‚ÄúMakeup Remover‚Äù,\n‚ÄúEyes‚Äù will share the token ‚ü®Eyes‚ü©.\n(2) Tree-structure setting: we enforce a tree structure on the cat-\negories by creating different OOV tokens when the same cat-\negory name occurs at different places. For example, the cat-\negory ‚ÄúEyes‚Äù under ‚ÄúBeauty‚Äù, ‚ÄúSkin Care‚Äù will correspond to\ntoken ‚ü®Eyes1‚ü©while that under ‚ÄúBeauty‚Äù, ‚ÄúMakeup‚Äù, ‚ÄúMakeup\nRemover‚Äù corresponds to ‚ü®Eyes2‚ü©.\nTable 9 illustrates the importance of hierarchical information for\nSemID‚Äôs effectiveness. The more closely the categories adhere to\na hierarchical structure, the better the performance of the model.\nThis is likely because a hierarchically organized category list helps\nreduce the search space during the generation process. Conse-\nquently, this finding highlights the importance of properly\norganizing and structuring category information when im-\nplementing SemID in recommendation foundation models.\n5.7 What Types of HID Work and Why\nBased on the results presented in Table 4, CID+IID and SemID+IID\nshow much better performance compared to their respective CID\nand SemID counterparts. But SID+IID does not improve on SID,\nand SemID+CID not only does not improve but decreases the per-\nformance a lot. Both CID+IID and SemID+IID are constructed by\nassigning each item an independent extra token and concatenating\nit after the sequence of cluster IDs or category IDs. These combi-\nnations maintain the original index lengths while preserving the\nhierarchical structure. The improved performance can be attributed\nto the increased expressiveness of the indices provided by the extra\ntoken, as well as the retention of either collaborative information\nor metadata information within the hybrid index. This combination\nSIGIR-AP ‚Äô23, November 26‚Äì28, 2023, Beijing, China Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang\nof factors contributes to the performance enhancement observed\nin CID+IID and SemID+IID methods.\nSID+IID is created by appending an independent extra token\nafter the original sequential index, increasing the ID length by 1.\nSID+IID does not improve the performance possibly because the\nadditional token interferes the time-sensitive information encoded\nas a numerical style in the original sequential indices.SemID+CID,\nwhich is created by concatenating category IDs with cluster IDs or\nvice versa, exhibits suboptimal performance, as shown in Table 4.\nThis holds true for both concatenation orders: category IDs followed\nby CID indices and cluster IDs followed by SemID indices. The\nreason behind this suboptimal performance is that it generates\nexcessively long indices and disrupts the hierarchical structure\nencoded in both SemID and CID. Considering our findings, we\nrecommend employing CID+IID and SemID+IID as hybrid\nindices for recommendation foundation models, as they have\ndemonstrated superior performance in such scenarios.\n6 CONCLUSION\nThis paper examines various indexing methods using P5 as an ex-\nample backbone model. We examine three trivial indexing methods:\nRandom Indexing (RID), Title Indexing (TID), and Independent In-\ndexing (IID), and emphasize their limitations. This highlights the\nimportance of selecting an appropriate indexing method for foun-\ndation recommendation models, as it greatly impacts the model\nperformance. We then examine four simple yet effective indexing\nmethods: Sequential Indexing (SID), Collaborative Indexing (CID),\nSemantic Indexing (SemID), and Hybrid Indexing (HID). Experimen-\ntal results on Amazon Sports, Amazon Beauty, and Yelp datasets\ndemonstrate their strong performance. The four effective index-\ning methods satisfy the two criteria introduced in this paper: (1)\nmaintaining a suitable ID length, and (2) integrating useful prior\ninformation into item ID construction. We hope this study serves\nas an inspiration for future research on indexing methods for rec-\nommendation foundation models and beyond.\nAcknowledgement: The work was supported in part by NSF IIS-\n2046457 and IIS-2007907.\nREFERENCES\n[1] Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the next gen-\neration of recommender systems: A survey of the state-of-the-art and possible\nextensions. ICDE 17, 6 (2005), 734‚Äì749.\n[2] Hanxiong Chen, Shaoyun Shi, Yunqi Li, and Yongfeng Zhang. 2021. Neural\ncollaborative reasoning. In Proceedings of the Web Conference 2021 . 1516‚Äì1527.\n[3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311 (2022).\n[4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\nEric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling\ninstruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n[5] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks\nfor youtube recommendations. In Proceedings of the 10th ACM conference on\nrecommender systems . 191‚Äì198.\n[6] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6-\nRec: Generative Pretrained Language Models are Open-Ended Recommender\nSystems. arXiv preprint arXiv:2205.08084 (2022).\n[7] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Au-\ntoregressive entity retrieval. arXiv preprint arXiv:2010.00904 (2020).\n[8] Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit,\nPeter Wills, Luca Righetti, and William Saunders. 2021. Truthful AI: Developing\nand governing AI that does not lie. arXiv preprint arXiv:2110.06674 (2021).\n[9] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.\nRecommendation as language processing (rlp): A unified pretrain, personalized\nprompt & predict paradigm (p5). In RecSys. 299‚Äì315.\n[10] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History\nand context. ACM TIIS 5, 4 (2015), 1‚Äì19.\n[11] Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards Reasoning in Large\nLanguage Models: A Survey. arXiv preprint arXiv:2212.10403 (2022).\n[12] Dietmar Jannach and Malte Ludewig. 2017. When recurrent neural networks\nmeet the neighborhood for session-based recommendation. In RecSys. 306‚Äì310.\n[13] Dietmar Jannach, Markus Zanker, Alexander Felfernig, and Gerhard Friedrich.\n2010. Recommender systems: an introduction . Cambridge University Press.\n[14] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\nmendation. In ICDM. IEEE, 197‚Äì206.\n[15] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-\nniques for recommender systems. Computer 42, 8 (2009), 30‚Äì37.\n[16] Jure Leskovec, Anand Rajaraman, and Jeffrey David Ullman. 2020. Mining of\nmassive data sets . Cambridge university press.\n[17] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023. Large Language Models\nfor Generative Recommendation: A Survey and Visionary Discussions. arXiv\npreprint arXiv:2309.01157 (2023).\n[18] Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how\nmodels mimic human falsehoods. arXiv preprint arXiv:2109.07958 (2021).\n[19] Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang,\nand Lorenzo Torresani. 2022. Learning to recognize procedural activities with\ndistant supervision. In CVPR. 13853‚Äì13863.\n[20] Chen Ma, Peng Kang, and Xue Liu. 2019. Hierarchical gating networks for\nsequential recommendation. InProceedings of the 25th ACM SIGKDD international\nconference on knowledge discovery & data mining . 825‚Äì833.\n[21] Andrew Ng, Michael Jordan, and Yair Weiss. 2001. On spectral clustering: Analysis\nand an algorithm. Advances in neural information processing systems 14 (2001).\n[22] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations\nusing distantly-labeled reviews and fine-grained aspects. In EMNLP-IJCNLP.\n[23] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text transformer. JMLR 21, 1 (2020).\n[24] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine\nTranslation of Rare Words with Subword Units. InACL. 1715‚Äì1725.\n[25] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-\nresentations from transformer. In Proceedings of the 28th ACM international\nconference on information and knowledge management . 1441‚Äì1450.\n[26] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommenda-\ntion via convolutional sequence embedding. In Proceedings of the eleventh ACM\ninternational conference on web search and data mining . 565‚Äì573.\n[27] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[28] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and\ncomputing 17 (2007), 395‚Äì416.\n[29] Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen.\n2017. Deep matrix factorization models for recommender systems.. In IJCAI,\nVol. 17. Melbourne, Australia, 3203‚Äì3209.\n[30] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based rec-\nommender system: A survey and new perspectives. ACM Computing Surveys\n(CSUR) 52, 1 (2019), 1‚Äì38.\n[31] Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Deqing\nWang, Guanfeng Liu, Xiaofang Zhou, et al . 2019. Feature-level Deeper Self-\nAttention Network for Sequential Recommendation.. In IJCAI. 4320‚Äì4326.\n[32] Yongfeng Zhang, Qingyao Ai, Xu Chen, and W Bruce Croft. 2017. Joint repre-\nsentation learning for top-n recommendation with heterogeneous information\nsources. In Proceedings of the 2017 ACM on Conference on Information and Knowl-\nedge Management . 1449‚Äì1458.\n[33] Yuhui Zhang, Hao Ding, Zeren Shui, Yifei Ma, James Zou, Anoop Deoras, and\nHao Wang. 2021. Language models as recommender systems: Evaluations and\nlimitations. (2021).\n[34] Wayne Xin Zhao, Junhua Chen, Pengfei Wang, Qi Gu, and Ji-Rong Wen. 2020.\nRevisiting alternative experimental settings for evaluating top-n item recommen-\ndation algorithms. In Proceedings of the 29th ACM International Conference on\nInformation & Knowledge Management . 2329‚Äì2332.\n[35] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,\nZhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for\nsequential recommendation with mutual information maximization. In CIKM.\n[36] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai.\n2018. Learning tree-based deep model for recommender systems. In KDD.",
  "topic": "Search engine indexing",
  "concepts": [
    {
      "name": "Search engine indexing",
      "score": 0.9121475219726562
    },
    {
      "name": "Computer science",
      "score": 0.8509262800216675
    },
    {
      "name": "Information retrieval",
      "score": 0.7000367641448975
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.6114703416824341
    },
    {
      "name": "Index (typography)",
      "score": 0.5138288140296936
    },
    {
      "name": "Inference",
      "score": 0.48578837513923645
    },
    {
      "name": "Recommender system",
      "score": 0.4634625017642975
    },
    {
      "name": "Learning to rank",
      "score": 0.43731117248535156
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3378673791885376
    },
    {
      "name": "Data mining",
      "score": 0.33463937044143677
    },
    {
      "name": "World Wide Web",
      "score": 0.15705463290214539
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I102322142",
      "name": "Rutgers, The State University of New Jersey",
      "country": "US"
    }
  ]
}