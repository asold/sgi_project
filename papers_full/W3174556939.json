{
  "title": "Attention Calibration for Transformer in Neural Machine Translation",
  "url": "https://openalex.org/W3174556939",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1593108345",
      "name": "Yu Lu",
      "affiliations": [
        "Shandong Institute of Automation",
        "University of Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2344356506",
      "name": "Jiali Zeng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2120994304",
      "name": "Jiajun Zhang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shandong Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2770339140",
      "name": "Shuangzhi Wu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2103262105",
      "name": "Mu Li",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3211259717",
    "https://openalex.org/W3094519420",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2903268415",
    "https://openalex.org/W2963333747",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2977162702",
    "https://openalex.org/W3103729510",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2051840895",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2962708992",
    "https://openalex.org/W3101609372",
    "https://openalex.org/W3088181395",
    "https://openalex.org/W2954730351",
    "https://openalex.org/W2996061341",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963798744",
    "https://openalex.org/W2979200397",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4288103164",
    "https://openalex.org/W2970045405",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W3037212200",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2963260202",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W3035281110",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3106185885",
    "https://openalex.org/W3027886415",
    "https://openalex.org/W2963598809",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W3173506780",
    "https://openalex.org/W2998655810",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2972918955",
    "https://openalex.org/W2962834107"
  ],
  "abstract": "Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, Mu Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 1288â€“1298\nAugust 1â€“6, 2021. Â©2021 Association for Computational Linguistics\n1288\nAttention Calibration for Transformer in Neural Machine Translation\nYu Lu1,2âˆ—, Jiali Zeng3, Jiajun Zhang1,2â€ , Shuangzhi Wu3 and Mu Li3\n1 National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China\n2 School of Artiï¬cial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n3 Tencent Cloud Xiaowei, Beijing, China\n{yu.lu, jjzhang}@nlpr.ia.ac.cn\n{lemonzeng, frostwu, ethanlli}@tencent.com\nAbstract\nAttention mechanisms have achieved substan-\ntial improvements in neural machine transla-\ntion by dynamically selecting relevant inputs\nfor different predictions. However, recent stud-\nies have questioned the attention mechanismsâ€™\ncapability for discovering decisive inputs. In\nthis paper, we propose to calibrate the attention\nweights by introducing a mask perturbation\nmodel that automatically evaluates each in-\nputâ€™s contribution to the model outputs. We in-\ncrease the attention weights assigned to the in-\ndispensable tokens, whose removal leads to a\ndramatic performance decrease. The extensive\nexperiments on the Transformer-based transla-\ntion have demonstrated the effectiveness of our\nmodel. We further ï¬nd that the calibrated at-\ntention weights are more uniform at lower lay-\ners to collect multiple information while more\nconcentrated on the speciï¬c inputs at higher\nlayers. Detailed analyses also show a great\nneed for calibration in the attention weights\nwith high entropy where the model is uncon-\nï¬dent about its decision1.\n1 Introduction\nAttention mechanisms have been ubiquitous in neu-\nral machine translation (NMT) (Bahdanau et al.,\n2015; Vaswani et al., 2017). It dynamically en-\ncodes source-side information by inducing a con-\nditional distribution over inputs, where the ones\nthat are most relevant to the current translation are\nexpected to receive more attention.\nHowever, many studies doubt whether highly-\nattended inputs have a large impact on the model\noutputs. On the one hand, erasing the representa-\ntions accorded high attention weights do not neces-\nsarily lead to a performance decrease (Serrano and\nâˆ—Work done while the author was an intern at Tencent.\nâ€ Corresponding author.\n1https://github.com/yulu-dada/Attention-calibration-\nNMT\nè¿œ\néƒŠ\nè¿ æ—¥ å¤§\né›ª\nå¤š äºº æ­»\näº¡\näº¤\né€š\nä¸­\næ–­\n<EOS>\nè¿œéƒŠ è¿ æ—¥ å¤§é›ª å¤šäºº æ­»äº¡ äº¤é€š ä¸­æ–­\nheavy snow in countryside caused many deaths\nSrc:\nBase:\ndeaths _ [before]\ndeaths _ [after]\ndeaths _ [mask]\nin _ [before]\nin _ [after]\nin _ [mask]\nheavy snow in countryside has caused many \ndeaths and traffic interruption\nOurs:\ndays of heavy snow in countryside left many \ndeaths and transportation disrupted\nRef:\nè¿œ\néƒŠ\nè¿ æ—¥ å¤§\né›ª\nå¤š äºº æ­»\näº¡\näº¤\né€š\nä¸­\næ–­\n<EOS>\nFigure 1: Examples of the attention weights before and\nafter calibration. â€œin â€ denotes the timestep after the\nprediction â€œinâ€. The dashed boxes indicate the inputs\nwhich should receive more attention measured by our\nmask perturbation model.\nSmith, 2019), which can be attributed to that unim-\nportant words (e.g., punctuations) are frequently\nassigned with high attention weights (Mohanku-\nmar et al., 2020). On the other hand, Jain and Wal-\nlace (2019) state that attention weights are incon-\nsistent with other feature importance metrics in text\nclassiï¬cation tasks. It further proves that attention\nmechanisms are incapable of precisely identifying\ndecisive inputs for each prediction, which would\nresult in wrong-translation or over-translation in\nNMT (Tu et al., 2016). We take Figure 1 as an\nexample. After producing the target-side word\nâ€œdeathsâ€, attention mechanisms wrongly attribute\nmost attention to the â€œ âŸ¨EOSâŸ©â€, making parts of\nthe source sentence untranslated.\nIn this paper, we propose to calibrate the vanilla\nattention mechanism by focusing more on key in-\n1289\nputs. To test what inputs affect the model prediction\nmost, we tend to observe how the model decision\nchanges as perturbing parts of inputs. We deï¬ne\nthe perturbation operation as applying a learnable\nmask to scale each attention weight. Then, we per-\nform a â€œdeletion gameâ€, which aims to ï¬nd the\nsmallest perturbation extents that cause the signif-\nicant quality degradation. In this manner, we can\nï¬nd the most informative inputs for the prediction.\nBased on the results detected by the mask pertur-\nbation model, we further calibrate attention weights\nby reallocating more attention to informative inputs.\nWe design three fusion methods to incorporate the\ncalibrated attention weights into original attention\nweights: (1) ï¬xed weighted sum, (2) annealing\nlearning, and (3) gating mechanism. The mask per-\nturbation model and NMT model are jointly trained,\nwhile the attention weights in NMT are corrected\nbased on the actual contributions measured by the\nmask perturbation model.\nRecall the example in Figure 1. After producing\nthe target word â€œinâ€, our mask perturbation model\nï¬nds that the source word â€œ è¿œéƒŠ[countryside]â€\nwith a high attention weight is exactly the decisive\ninput for the prediction. Therefore, we strengthen\nthe corresponding attention weight of â€œè¿œéƒŠ[coun-\ntryside]â€. However, after the prediction â€œdeathsâ€,\nthe highly-attended â€œâŸ¨EOSâŸ©â€ is not the decisive in-\nput at the current step. We redistribute the attention\nweights to the source words (â€œäº¤é€š[trafï¬c]â€ and\nâ€œä¸­æ–­ [interruption]â€) which receive little attention\nbut are important for the subsequent translation dis-\ncovered by our mask perturbation model. After\ncalibration, the missing source information â€œtrafï¬c\ninterruptionâ€ is well-translated.\nWe conduct extensive experiments to verify our\nmethodâ€™s effectiveness on Transformer-based trans-\nlation (NIST Zhâ‡’En, WMT14 Enâ‡’De, WMT16\nEnâ‡”Ro, WMT17 Enâ‡”Fi, and Enâ‡”Lv). Experi-\nmental results show that our calibration methods\ncan signiï¬cantly boost performance. We further vi-\nsualize calibrated attention weights and investigate\nwhen attention weights need to be corrected.\nThe contributions of this paper are three-fold:\nâ€¢ We propose a mask perturbation model to au-\ntomatically assess each inputâ€™s contribution\nfor translation, which is simple yet effective.\nâ€¢ We design three methods to calibrate original\nattention weights by highlighting the informa-\ntive inputs, which are experimentally proved\nto outperform strong baselines.\nâ€¢ Detailed analyses show that calibrated atten-\ntion weights are more uniform at lower layers\nwhile more focused at the higher layers. High-\nentropy attention weights are found to have\ngreat needs for calibration at all layers.\n2 Background\nIn this section, we ï¬rst brieï¬‚y introduce the frame-\nwork of Transformer (Vaswani et al., 2017) with a\nfocus on the Multi-head attention (MHA). Then we\npresent an analysis of the learned attention weights,\nthe correlation with feature importance measures,\nwhich motivates our ideas discussed afterward.\n2.1 Transformer Architecture\nThe Transformer is an encoder-decoder framework\nwith stacking layers of attention blocks. The en-\ncoder ï¬rst transforms an input x= {x1,x2,...xn}\nto a sequence of continues representations h =\n{h1,h2,...hn}, from which the decoder gener-\nates an output sequence y= {y1,y2,...ym}.\nMulti-head attention between encoder and de-\ncoder enables each prediction to attend overall\ninputs from different representation subspaces\njointly. For the single head, we ï¬rst project h=\n{h1,h2,...hn}to keys K and values V using\ndifferent linear projections. At the t-th position,\nwe project the hidden state of the previous decoder\nlayer to the query vector qt. Then we multiply qt\nby keys Kto obtain an attention at, which is used\nto calculate a weighted sum of values V.\nAttn (qt,K,V) =at âˆ—V\nat = softmax\n(qtKT\nâˆšdk\n) (1)\nwhere dk is the dimension of the keys. For MHA,\nwe use different projections to obtain the queries,\nkeys, and values representations for each head.\nIt is noted that Transformer (base model) per-\nforms N = 6cross-lingual attention layers and em-\nploys h= 8parallel attention heads for each time.\nThus we implement our methods on N Ã—hatten-\ntion operations separately. For simplicity, we next\ndenote the query, keys, and values asqt,K,V re-\ngardless of what layers and heads they come from.\n2.2 Disagreement Between Attention Weights\nand Feature Importance Metrics\nAttention mechanisms provide a distribution over\nthe context representations of inputs, which are\n1290\n1 2 3 4 5 6\nLayer\n0.165\n0.170\n0.175\n0.180\nKendall-  correlation\nwith it\n0.1768\n0.1685 0.1683\n0.1665\n0.1691 0.1697\nFigure 2: The mean Kendall-Ï„ correlation between at-\ntention weights ( a) and gradient importance metrics\n(Ï„it) on Zhâ‡’En translation.\noften presented as communicating the relative im-\nportance of inputs. However, recent work has cau-\ntioned against whether the inputs accorded high\nattention weights decide the model outputs (Jain\nand Wallace, 2019). Our analysis examines the\ncorrelation with attention weights and feature im-\nportance metrics in NMT to test if the attention\nmechanisms focus on the decisive inputs. We ap-\nply gradient-based methods (Simonyan et al., 2014;\nLi et al., 2016) to measure the importance of each\ncontextual representation hi for model output yt:\nÏ„it = |âˆ‡hi p(yt|x1:n)| (2)\nWe train a baseline Transformer model on NIST\nZhâ‡’En dataset and extract the averaged attention\nweights over heads.\nFigure 2 reports the statistics of Kendall-Ï„ corre-\nlation for each attention layer, where the observed\ncorrelation is all modest (0 indicates no correlation,\nwhile 1 implies perfect concordance). The inconsis-\ntency with feature importance metrics reveals that\nthe high-attention inputs are not always responsible\nfor the model prediction. It further motivates us\nwhether we can calibrate the attention weights to\nfocus more on the decisive inputs to achieve better\ntranslation.\n3 Our Method\nWe aim to make the attention mechanism more fo-\ncused on the informative inputs. The ï¬rst step is to\ndiscover what inputs are essential for the model pre-\ndiction. As shown in Figure 3, we design a Mask\nPerturbation Modelto worsen the performance\nwith limited perturbation on the original attention\nweights. By doing this, we can automatically detect\nwhat inputs decide the model outputs. Then, we\ndesign an Attention Calibration Network (ACN)\nto correct the original attention weights, highlight-\ning the decisive inputs based on what inputs are\nperturbed by the mask perturbation model.\n3.1 Mask Perturbation Model\nTo search the source-side inputs that the model\nrelies on to produce the output, we can observe\nhow the model prediction changes as perturbing\ndifferent parts of the input sentence. We apply a\nmask to scale each inputâ€™s attention weight, which\nsimulates the process of perturbation.\nFormally, let mt be a mask at t-th step. The\nperturbed attention weightap\nt is calculated as:\nap\nt = mt âŠ™at + (1 âˆ’mt) âŠ™Âµ0 (3)\nwhere Âµ0 is a uniform distribution (an average vec-\ntor of 1\nn) and âŠ™denotes element-wise multiplica-\ntion. The mask mt is obtained based on the hidden\nstate in the decoder qt and keys K:\nmt = Ïƒ\n(\nqtWQ(KWK)T\nâˆšdk\n)\n(4)\nHere, Ïƒ(Â·) is the sigmoid function. A smaller value\nof mt means a larger perturbation extent on orig-\ninal attention weights. Considering the structure\nof multi-head attention in Transformer, WQ and\nWK differ among layers and heads.\nTo test the effect of perturbing distinct regions of\ninputs, we borrow the idea â€œdeletion gameâ€ to ï¬nd\nthe smallest perturbation extent, which leads to a\nsigniï¬cant performance decrease. The objective\nfunction of mask perturbation model is:\nL(Î¸m) =âˆ’LNMT (ap\nt ,Î¸) +Î±Lc (Î¸m) (5)\nwhere Î¸ denotes the parameters of the original\nTransformer. LNMT (ap\nt ,Î¸) is the cross-entropy\nloss of the translation model when using perturbed\nattention weights ap\nt . Î¸m = {WQ,WK}repre-\nsents the parameters of mask perturbation model.\nThe ï¬rst term indicates that the perturbation op-\neration aims to harm the translation quality. The\nsecond one serves as a penalty term to encourage\nmost of the mask to be turned off (perturb inputs\nas few as possible).\nLc (Î¸m) =âˆ¥1 âˆ’mtâˆ¥2 (6)\nThe perturbation extent is determined by the hyper-\nparameter Î±. Notably, earlier studies employ masks\nand â€œdeletion gameâ€ as the analytical tools to ex-\nplore the importance of each attention head (Fong\nand Vedaldi, 2017) or the contributions of the pix-\nels in the ï¬gure to the model outputs (V oita et al.,\n2019). However, we extend to probing the inputsâ€™\ncontributions to the model prediction in NMT and\nfurther use the masks to calibrate the attention\nmechanisms based on the analytical results.\n1291\nAttention\nMechanisms\nAttention Calibration \nNetwork (ACN)\nMask Perturbation \nModel \nOriginal \nAttention Weights mask\nCalibrated \nAttention Weights\nPerturbed \nAttention Weights\nK\nQ\nNMT \ndecoder Worse performance \nafter perturbation\nBetter performance\nafter calibration\nï¼ˆğ‘ğ‘ï¼‰\nï¼ˆğ‘ğ‘ï¼‰\nï¼ˆğ‘ï¼‰\nï¼ˆğ‘šï¼‰\nFigure 3: The overview of the framework. The mask perturbation model is trained to perturb the attention weights\nof decisive inputs to harm the performance. ACN looks for what inputs are perturbed and enhance the correspond-\ning attention weights.\n3.2 Attention Calibration Network\nAs aforementioned, our mask perturbation model\nremoves the most informative input to deteriorate\nthe translation by setting the corresponding masks\nto zero. In other words, a smaller mask means a\nlarger perturbation, namely a more signiï¬cant im-\npact on the prediction. We propose to calibrate the\noriginal attention weights in NMT by highlighting\nthe essential inputs for each model prediction.\nFormally, the calibrated attention weightac\nt\ncan be designed as:\nac\nt = at âŠ™e1âˆ’mt (7)\nWe increase the attention weights of key inputs\nwhich suffer large perturbation extents. The atten-\ntion weights of other less-informative inputs are\ncorrespondingly decreased. We design three meth-\nods to incorporate ac\nt into the original one at to\nobtain combined attention weightsacomb\nt :\nâ€¢ Fixed Weighed Sum. In this method, the cal-\nibrated attention weights are added to the orig-\ninal attention weights of ï¬xed ratio Î»as:\nacomb\nt = softmax(at + Î»âˆ—ac\nt) (8)\nâ€¢ Annealing Learning. Considering the mask\nperturbation model is not well-trained at the\nearly stage, we expect the effect of ac\nt to be\nsmaller at ï¬rst and gradually grow with the\ntraining step s. To this end, we use annealing\nlearning to control the ratio of ac\nt as:\nacomb\nt = Î³(s) âˆ—at + (1âˆ’Î³(s)) âˆ—ac\nt\nÎ³(s) =eâˆ’s/105 (9)\nâ€¢ Gating Mechanism. We propose a calibra-\ntion gate to dynamically select the amount of\nthe information from the perturbation model\nin the decoding process.\nacomb\nt = gt âˆ—at + (1âˆ’gt) âˆ—ac\nt\ngt = Ïƒ(qtWg + bg) (10)\nwhere Wg and bg are trainable parameters\nvary among different layers and heads.\n3.3 Training\nOur mask perturbation model and NMT model are\njointly optimized. As shown in Figure 3, the mask\nperturbation model is trained to worsen the per-\nformance by limited perturbation on the attention\nweights (Equation 5). Given what inputs are per-\nturbed, we can ï¬gure out the decisive inputs for\neach model prediction and calibrate the original\nattention weights in the NMT model by ACN. With\nthe calibrated attention weights, the NMT model is\nï¬nally optimized by:\nLNMT (Î¸) =âˆ’\nmâˆ‘\nt=1\nlogp(yt|y<t,x; acomb\nt ,Î¸)\n(11)\nDuring testing, the mask perturbation model also\nhelps identify the informative inputs based on the\nhidden state in the decoder at each step (as seen\nin Equation 4). The NMT model decodes with the\ncalibrated attention weights. Moreover, our method\ncan provide the saliency map between inputs and\noutputs based on the generated mask, an accessible\nmeasurement of the inputsâ€™ contributions to the\nmodel predictions.\n4 Experiments\nWe evaluate our method in LDC Chinese-English\n(Zhâ‡’En), WMT14 English-German (En â‡’De),\nWMT16 English-Romanian (En â‡”Ro), WMT17\nEnglish-Finnish (En â‡”Fi) and English-Latvian\n(Enâ‡”Lv).\n1292\nSource Lang. Train Dev. Test V ocab.\nLDC1 Zhâ‡’En 2.09M 878 4789 32k\nWMT142 Enâ‡’De 4.54M 3000 3003 37k\nWMT173\nEnâ‡’Lv\nLvâ‡’En 4.46M 2003 2001 37k\nEnâ‡’Fi\nFiâ‡’En 2.63M 3000 3002 32k\nWMT164 Enâ‡’Ro\nRoâ‡’En 0.61M 1999 1999 32k\nTable 1: Statistics of the datasets.\n4.1 Dataset\nWe tokenize the corpora using a script from\nMoses (Koehn et al., 2007). Byte pair encoding\n(BPE) (Sennrich et al., 2016) is applied to all lan-\nguage pairs to construct a join vocabulary except\nfor Zhâ‡’En where the source and target languages\nare separately encoded.\nFor Zhâ‡’En, we remove the sentences of more\nthan 50 words. We use NIST 2002 as validation\nset, NIST 2003-2006 as the testbed. For Enâ‡’De,\nnewstest2013 and newstest2014 are set as valida-\ntion and test sets. We use the standard 4-gram\nBLEU (Papineni et al., 2002) on the true-case out-\nput to score the performance. For En â‡”Ro, we\nuse newsdev2016 and newstest2016 as develop-\nment and test sets. For Enâ‡”Lv and Enâ‡”Fi, news-\ndev2017 and newstest2017 are validation set and\ntest set. See Table 1 for statistics of the data.\n4.2 Settings\nWe implement the described models with fairseq5\ntoolkit for training and evaluating. We experiment\nwith Transformer Base (Vaswani et al., 2017): hid-\nden size dmodel = 512, 6 encoder and decoder lay-\ners, 8 attention heads and 2048 feed-forward inner-\nlayer dimension. The dropout rate of the residual\nconnection is 0.1 except for Zhâ‡’En (0.3). During\ntraining, we use label smoothing of value Ïµls = 0.1\nand employ the Adam (Î²1 = 0.9,Î²2 = 0.998) for\nparameter optimization with a scheduled learning\nrate of 4,000 warm-up steps. All the experiments\nlast for 150k steps except for small-scale Enâ‡”Ro\ntranslation tasks (100k). For evaluation, we aver-\nage the last ten checkpoints and use beam search\n1The corpora includes LDC2000T50, LDC2002T01,\nLDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17\nand LDC2004T07. Following previous work, we use case-\ninsensitive tokenized BLEU to evaluate the performance.\n2http://www.statmt.org/wmt14/translation-task.html\n3http://www.statmt.org/wmt17/translation-task.html\n4http://www.statmt.org/wmt16/translation-task.html\n5https://github.com/pytorch/fairseq\nModel TEST\nGNMT (Wu et al., 2016)â€¡ 24.61\nConv (Gehring et al., 2017)â€¡ 25.16\nAttIsAll (Vaswani et al., 2017)â€¡ 27.3\n(Feng et al., 2020)â€¡ 27.55\n(Weng et al., 2020)â€¡ 27.7\nOur Implemented Baseline 27.37\nOurs\nFixed 27.38\nAnneal 28.1âˆ—\nGate 27.75\nTable 2: The comparison of our model, Transformer\nbaselines and related work on the WMT14 Enâ‡’De us-\ning case-sensitive BLEU. Results withâ€¡mark are taken\nfrom the corresponding papers. â€œâˆ—â€ indicates the gains\nare statistically signiï¬cant than baselines with p<0.05.\n(beam size 4, length penalty 0.6) for inference.\nBesides, the hyperparameter Î»in Equation 8 de-\ncides how much the calibrated attention weights are\nincorporated in the Fixed Weighted Sum method.\nWe set Î»= 0.1 in all experiments for comparison.\n4.3 Main Results\nTo comprehensively compare with the existing\nbaselines and similar work, we report the results\nof some competitive models including GNMT (Wu\net al., 2016), Conv (Gehring et al., 2017) and At-\ntIsAll (Vaswani et al., 2017) on WMT14 Enâ‡’De\ntranslation task. Besides, we also compare our\nmethod against related researches about introduc-\ning word alignment information to guide transla-\ntion (Weng et al., 2020; Feng et al., 2020). As\npresented in Table 2, our method exhibits better\nperformance than the above models. Unlike su-\npervised attention with external word alignment,\nour model yields a signiï¬cant gain by looking into\nwhat inputs affect the modelâ€™s internal training.\nTable 3 shows the translation quality measured\nin BLEU score for NIST Zh â‡’En. Our proposed\nmodel signiï¬cantly outperforms the baseline by\n0.96 (MT02), 0.84 (MT03), 0.58 (MT04), 1.02\n(MT05) and 0.76 (MT06), respectively.\nWe also conduct our experiments on WMT17\nEnâ‡”Fi and En â‡”Lv. As shown in Table 4, our\nmethods improve the performance over baseline\nby 0.54 BLEU (Enâ‡’Fi), 0.6 BLEU (Fiâ‡’En), 0.57\nBLEU (Enâ‡’Lv) and 0.95 BLEU (Lv â‡’En). For\nthe small-scale WMT16 En â‡”Ro, our methods\nachieve a substantial improvement of 1.44 more\nBLEU (Enâ‡’Ro) and 0.95 BLEU (Roâ‡’En). Com-\n1293\nModel DEV MT03 MT04 MT05 MT06 A VE\nBaseline 48.56 49.58 48.58 49.95 47.22 48.24\nOurs\nFixed 48.42 49.41 48.56 50.32 47.89 48.44\nAnneal 48.22 49.73 48.85 50.97âˆ— 47.49 48.74\nGate 49.52âˆ— 50.42âˆ— 49.16âˆ— 50.78âˆ— 47.98âˆ— 49.00âˆ—\nTable 3: Evaluation of translation quality for Zhâ‡’En Translation using case-insensitive BLEU score. â€œâˆ—â€ indicates\nthe gains are statistically signiï¬cant than baselines with p<0.05.\nModel Enâ‡’Lv Lvâ‡’En Enâ‡’Fi Fiâ‡’En Enâ‡’Ro Roâ‡’En\nBaseline 16.26 17.76 22.01 26.07 22.56 27.53\nOurs\nFixed 16.54 18.45âˆ— 22.42 26.2 23.1 28.02\nAnneal 16.35 18.12 22.4 26.39 23.27âˆ— 28.2âˆ—\nGate 16.83âˆ— 18.71âˆ— 22.55âˆ— 26.67âˆ— 24.00âˆ— 28.48âˆ—\nTable 4: Evaluation of translation quality for WMT17 En â‡”Fi, WMT17 En â‡”Lv and WMT16 En â‡”Ro using\ncase-insensitive BLEU score. â€œâˆ—â€ indicates the gains are statistically signiï¬cant than baselines with p<0.05.\n0.020 0.025 0.030 0.035 0.040 0.045\n46\n47\n48\n49\n50BLEU\n47.73\n49.1\n49.52\n48.91 48.79\n47.74\n0.30\n0.35\n0.40\n0.45\nAveraged Value of Masks\nBLEU\nAveraged Value of Masks\nFigure 4: Experimental results on the validation set and\nthe averaged value of generated masks with respect to\ndifferent hyperparameter Î±on Zhâ‡’En translation task\n(Gate Mechanism).\npared to the large-scale dataset, the insufï¬cient\ntraining data make it harder to learn the relation-\nship between inputs and outputs, leaving a greater\nneed for calibrating attention weights.\nOverall, our proposed model signiï¬cantly out-\nperforms the strong baselines, especially for the\nsmall-scale dataset. More importantly, the parame-\nter size is tiny (6M), which cannot add much cost\nto the training and inference process.\nEffect of Fusion Methods For three fusion\nmethods, the ï¬xed weighted sum has a limited gain.\nAnnealing learning is comparatively more stable,\nwhich reduces the impact of ACN when the mask\nperturbation model is not well-trained at the initial\nstage. But it is challenging to design an annealing\nstrategy that can be applied to all language pairs.\nGate mechanism mostly achieves the best perfor-\nmance for dynamically controlling the proportions\nof original and calibrated attention weights.\n1 2 3 4 5 6\nLayer\n0.16\n0.18\n0.20\n0.22\n0.24\n0.26Kendall-  correlation with it\n0.1768\n0.1685 0.1683 0.1665 0.1691 0.1697\n0.2206 0.2217 0.2261\n0.2195 0.2187 0.2211\nAttention Weights\nGenerated Masks\nFigure 5: The mean Kendall-Ï„ correlation between at-\ntention weights ( a), the masks ( m) generated by our\nmask perturbation model and gradient importance mea-\nsures (Ï„it) on Zhâ‡’En.\nEffect of Hyperparameter The hyperparameter\nÎ± in the loss function of the mask perturbation\nmodel (as in Equation 5) decides how much masks\nwould turn on to perturb the original attention\nweights. Figure 4 exhibits the average value of\ngenerated masks across heads as the function of\nthe setting of Î±. A larger Î± forces the model to\nturn off most masks, which makes the value of the\nmask closer to 1, resulting in a smaller perturbation\nextent on the attention weights.\nCorrelation with Feature Importance Metrics\nFigure 5 reports the correlation between our gener-\nated mask (m) and the gradient-based importance\nmeasures6 (Ï„it). We ï¬nd that the masks are rela-\ntively closer to the gradient-based importance mea-\nsures than the original attention weights, which\n6Though these measures are insufï¬cient for telling what\ninputs are important (Kindermans et al., 2019), they do pro-\nvide measures of individual feature importance with known\nsemantics (Ross et al., 2017).\n1294\n1 2 3 4 5 6\n0.024\n0.026\n0.028\n0.030JSD\n0.0249\n0.0268\n0.0243\n0.0296 0.0295\n0.0276\nZh En\n1 2 3 4 5 6\nLayer\n0.04\n0.06\n0.08JSD\n0.0784\n0.0695 0.0708\n0.0581\n0.0428\n0.0548\nEn De\nFigure 6: The JSD between attention weights before\nand after calibration at different layers on Zhâ‡’En and\nEnâ‡’De translation. Note that the overall JSD for each\nlanguage pair is decided by the hyperparameter Î±, but\nthe calibration extents of layers are learned by ACN.\nprove the effectiveness of our mask perturbation\nmodel to discover decisive inputs.\n5 Analysis\nIn this section, we explain how our proposed\nmethod helps produce better translation by investi-\ngating: (1) what attention weights need to calibrate\nand (2) calibrated attention weights are more fo-\ncused or more uniform. Speciï¬cally, we delve into\nthe differences between layers, which give insights\ninto the attention mechanismâ€™s inner working. We\nconduct analyses on Zhâ‡’En NIST03 and Enâ‡’De\nnewstest2014 to understand our model from differ-\nent perspectives.\nWe apply Jensen-Shannon Divergence (JSD) be-\ntween attention weights before and after calibration\nto measure the calibration extent:\nJSD (a1,a2) =1\n2KL[a1âˆ¥a] +1\n2KL[a2âˆ¥a] (12)\nwhere a = a1+a2\n2 . A high JSD means the cali-\nbrated attention weights are distant from the orig-\ninal one. Besides, we use the entropy changes of\nattention weights to test whether the calibrated at-\ntention weights become more uniform or focused.\nâ–³Ent (a1,a2) = ent (a1) âˆ’ent (a2) (13)\nwhere ent (a) = âˆ’âˆ‘m\ni=1 ailogai, a metric to de-\nscribe the uncertainty of the distribution.\n5.1 What attention weights need to calibrate?\nHigh or low layers?Concerning the roles of dif-\nferent attention layers, one natural question is what\n0-0.8 0.8-1.6 1.6-2.4 2.4-3.2 3.2-4\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nJSD\nThe entropy of the original attention weights\nZh En\n0-0.5 0.5-0.75 0.75-1 1-2 2-4\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nEn De\n(a) 1-st layer\n0-0.8 0.8-1.6 1.6-2.4 2.4-3.2 3.2-4\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nJSD\nThe entropy of the original attention weights\nZh En\n0-0.5 0.5-0.75 0.75-1 1-2 2-4\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nEn De\n(b) 6-th layer\nFigure 7: The JSD between attention weights before\nand after calibration with respect to the entropy of orig-\ninal attention distributions.\nattention layers are not well-trained in the original\nNMT model and have an urgent need to calibrate.\nFigure 6 depicts the JSD between original and cal-\nibrated attention weights. We ï¬nd high JSD for\nhigh layers and low JSD for low layers in Zhâ‡’En\ntask. However, a different pattern is observed in\nEnâ‡’De task, where JSD in the high layer is lower\nthan in the low layers. We speculate that the dif-\nference is due to the language discrepancy and we\nwill explore this phenomenon in our future work.\nHigh or low entropy? More focused contribu-\ntions of inputs suggest that the model is more con-\nï¬dent about the choice of important tokens (V oita\net al., 2020). We attempt to validate whether the\nattention weights are more likely to be calibrated\nwhen the NMT model is uncertain about its de-\ncision. Figure 7 shows the positive relationship\nbetween calibration extent and the entropy of at-\ntention weights. Take the 6-th attention layer in\nZhâ‡’En translation as an example (as seen in Fig-\nure 7(b)). The averaged JSD is 0.0084 for the\nattention weights in rang [0,0.8], while the value\nis 0.0324 for the attention weights where the en-\ntropy is larger than 3.2. These ï¬ndings can also be\nobserved at different attention layers and language\npairs.\nWe infer that a higher entropy indicates the NMT\nmodel relies on multiple inputs to generate the\n1295\nlayer Zhâ‡’En Enâ‡’De\n1 + 0.0203 + 0.1846\n2 - 0.011 + 0.0762\n3 - 0.0023 + 0.0207\n4 - 0.0224 - 0.0336\n5 - 0.0303 - 0.0595\n6 - 0.0083 - 0.01\nAll - 0.0336 - 0.0224\nTable 5: Entropy differences (â–³Ent) between the orig-\ninal and calibrated attention weights. â€œ+â€ means the\ncalibrated attention weights are more disperse. â€œ-â€ in-\ndicates attention weights are sharper after calibration.\ntranslation, which increases the probability of infor-\nmation redundancy or error signals. Our proposed\nmodel is more likely to calibrate these attention\nweights to makes the NMT model pay more atten-\ntion to the informative inputs.\n5.2 Calibrated attention weights are more\ndispersed or focused?\nThere are multiple reasons why the calibrated atten-\ntion weights can boost performance. Section 4.3\nstates that our generated masks are much closer\nto the gradient-based feature importance measures\ncompared with attention weights. On the other\nhand, we present the entropy differences of the\noriginal and calibrated attention weights in Table 5\nwhere the entropy of attention weights are overall\nsmaller after calibration. However, the changes\nvary across layers. For Enâ‡’De translation, the cal-\nibrated attention weights are more uniform at 1-3\nlayers and more focused at 4-6 layers, while the at-\ntention weights become more focused for all layers\nexcept the 1-st layer on Zhâ‡’En task. These ï¬nd-\nings prove that each attention layer plays a different\nrole in the decoding process. The low layers gener-\nally grasp information from various inputs, while\nthe high layers look for some particular words tied\nto the model predictions.\n6 Related Work\nThe attention mechanism is ï¬rst introduced to aug-\nment vanilla recurrent network (Bahdanau et al.,\n2015; Luong et al., 2015), which are then the back-\nbone of state-of-the-art Transformer (Vaswani et al.,\n2017) for NMT. It yields better performance and\nprovides a window into how a model is operat-\ning (Belinkov and Glass, 2019; Du et al., 2020).\nThis section reviews the recent researches on ana-\nlyzing and improving attention mechanisms.\nThe Attention Debate Many recent studies have\nspawned interest in whether attention weights faith-\nfully represent each input tokenâ€™s responsibility\nfor model prediction. Serrano and Smith ï¬‚ip\nthe modelâ€™s decision by permuting some small at-\ntention weights, with high-weighted components\nnot being the reason for the decision. Some\nwork (Jain and Wallace, 2019; Vashishth et al.,\n2019) ï¬nd a weak correlation between attention\nscores and other well-ground feature importance\nmetrics, specially gradient-based and leave-one-out\nmethods, in various text classiï¬cation tasks. We\nalso present the correlation analysis in the less-\ndiscussed Transformer-based NMT and reach a\nsimilar conclusion. As opposed to the critiques of\nregarding attention weights as explanation, Wiegr-\neffe and Pinter claim that the trained attention\nmechanisms do learn something meaningful about\nthe relationship between inputs and outputs, such\nas syntactic information (Raganato and Tiedemann,\n2018; Vig and Belinkov, 2019; Pham et al., 2019).\nCan Attention be improved?There is plenty of\nwork on supervising attention weights with lexi-\ncal probabilities (Arthur et al., 2016), word align-\nment (Chen et al., 2016; Liu et al., 2016; Mi et al.,\n2016; Cohn et al., 2016; Garg et al., 2019; Feng\net al., 2020), human rationales (Strout et al., 2019)\nand sparsity regularization (Zhang et al., 2019). Un-\nlike them, we never introduce any external knowl-\nedge but highlight the inputs whose removal would\nsigniï¬cantly decrease Transformerâ€™s performance.\nAnother work line aims to make attention better\nindicative of the inputsâ€™ importance (Kitada and\nIyatomi, 2020; Tutek and Snajder, 2020; Mohanku-\nmar et al., 2020) which is designed for analysis\nwith no signiï¬cant performance gain, while our\nmethods incorporate the analytical results to en-\nhance the NMT performance.\n7 Conclusion\nIn this paper, we present a mask perturbation model\nto automatically discover the decisive inputs for the\nmodel prediction. We propose three methods to cal-\nibrate the attention mechanism by focusing on the\ndiscovered vital inputs. Extensive experimental re-\nsults show that our approaches obtain signiï¬cant\nimprovements over the state-of-the-art system. An-\nalytical results indicate that our proposed meth-\nods make the low layerâ€™s attention weights more\ndispersed to grasp multiple information. In con-\ntrast, high-layer attention weights become more\n1296\nfocused on speciï¬c essential inputs. We further\nï¬nd a greater need for calibration in the original\nattention weights with high entropy. Our work\nprovides insights on future work about learning\nmore useful information via attention mechanisms\nin other attention-based frameworks.\nAcknowledgments\nThe research work has been funded by the Natu-\nral Science Foundation of China under Grant No.\nU1836221 and the National Key Research and\nDevelopment Program of China under Grant No.\n2018YFC0823404. The research work in this paper\nhas also been supported by Beijing Academy of\nArtiï¬cial Intelligence (BAAI2019QN0504). This\nwork is also supported by Youth Innovation Promo-\ntion Association CAS No. 2017172.\nReferences\nPhilip Arthur, Graham Neubig, and Satoshi Nakamura.\n2016. Incorporating discrete translation lexicons\ninto neural machine translation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1557â€“1567, Austin,\nTexas. Association for Computational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nYonatan Belinkov and James R. Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-\n7, 2019, Volume 1 (Long and Short Papers) , pages\n3348â€“3354. Association for Computational Linguis-\ntics.\nWenhu Chen, Evgeny Matusov, Shahram Khadivi,\nand Jan-Thorsten Peter. 2016. Guided alignment\ntraining for topic-aware neural machine translation.\nCoRR, abs/1607.01628.\nTrevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-\nmolova, Kaisheng Yao, Chris Dyer, and Gholamreza\nHaffari. 2016. Incorporating structural alignment\nbiases into an attentional neural translation model.\nIn NAACL HLT 2016, The 2016 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, San Diego California, USA, June 12-17, 2016 ,\npages 876â€“885. The Association for Computational\nLinguistics.\nMengnan Du, Ninghao Liu, and Xia Hu. 2020. Tech-\nniques for interpretable machine learning. Commun.\nACM, 63(1):68â€“77.\nYang Feng, Wanying Xie, Shuhao Gu, Chenze Shao,\nWen Zhang, Zhengxin Yang, and Dong Yu. 2020.\nModeling ï¬‚uency and faithfulness for diverse neu-\nral machine translation. In The Thirty-Fourth AAAI\nConference on Artiï¬cial Intelligence, AAAI 2020,\nThe Thirty-Second Innovative Applications of Arti-\nï¬cial Intelligence Conference, IAAI 2020, The Tenth\nAAAI Symposium on Educational Advances in Arti-\nï¬cial Intelligence, EAAI 2020, New York, NY, USA,\nFebruary 7-12, 2020, pages 59â€“66. AAAI Press.\nRuth C Fong and Andrea Vedaldi. 2017. Interpretable\nexplanations of black boxes by meaningful perturba-\ntion. In Proceedings of the IEEE International Con-\nference on Computer Vision, pages 3429â€“3437.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly learning to align\nand translate with transformer models. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing, EMNLP-IJCNLP 2019, Hong Kong, China,\nNovember 3-7, 2019, pages 4452â€“4461. Association\nfor Computational Linguistics.\nJonas Gehring, Michael Auli, David Grangier, De-\nnis Yarats, and Yann N Dauphin. 2017. Convolu-\ntional sequence to sequence learning. In Proceed-\nings of the 34th International Conference on Ma-\nchine Learning, volume 70, pages 1243â€“1252.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543â€“3556. Association for Computa-\ntional Linguistics.\nPieter-Jan Kindermans, Sara Hooker, Julius Ade-\nbayo, Maximilian Alber, Kristof T. Sch Â¨utt, Sven\nDÂ¨ahne, Dumitru Erhan, and Been Kim. 2019.\nThe (un)reliability of saliency methods. In Woj-\nciech Samek, Gr Â´egoire Montavon, Andrea Vedaldi,\nLars Kai Hansen, and Klaus-Robert M Â¨uller, edi-\ntors, Explainable AI: Interpreting, Explaining and\nVisualizing Deep Learning , volume 11700 of Lec-\nture Notes in Computer Science , pages 267â€“280.\nSpringer.\nShunsuke Kitada and Hitoshi Iyatomi. 2020. At-\ntention meets perturbations: Robust and inter-\npretable attention with adversarial training. CoRR,\nabs/2009.12064.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\n1297\nsource toolkit for statistical machine translation. In\nACL 2007, Proceedings of the 45th Annual Meet-\ning of the Association for Computational Linguistics,\nJune 23-30, 2007, Prague, Czech Republic. The As-\nsociation for Computational Linguistics.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 681â€“691. Association for Computa-\ntional Linguistics.\nLemao Liu, Masao Utiyama, Andrew Finch, and Ei-\nichiro Sumita. 2016. Neural machine translation\nwith supervised attention. In Proceedings of COL-\nING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers, pages\n3093â€“3102. The COLING 2016 Organizing Com-\nmittee.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2015, Lisbon, Portu-\ngal, September 17-21, 2015 , pages 1412â€“1421. As-\nsociation for Computational Linguistics.\nHaitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.\nSupervised attentions for neural machine translation.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2283â€“2288. Association for Computational Linguis-\ntics.\nAkash Kumar Mohankumar, Preksha Nema, Sharan\nNarasimhan, Mitesh M. Khapra, Balaji Vasan Srini-\nvasan, and Balaraman Ravindran. 2020. Towards\ntransparent and explainable attention models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4206â€“\n4216. Association for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311â€“318. Association for\nComputational Linguistics.\nThuong-Hai Pham, Dominik Mach Â´acek, and Ondrej\nBojar. 2019. Promoting the knowledge of source\nsyntax in transformer NMT is not needed. Com-\nputaciÂ´on y Sistemas, 23(3).\nAlessandro Raganato and J Â¨org Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n287â€“297. Association for Computational Linguis-\ntics.\nAndrew Slavin Ross, Michael C. Hughes, and Finale\nDoshi-Velez. 2017. Right for the right reasons:\nTraining differentiable models by constraining their\nexplanations. In Proceedings of the Twenty-Sixth\nInternational Joint Conference on Artiï¬cial Intelli-\ngence, IJCAI-17, pages 2662â€“2670.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715â€“1725.\nAssociation for Computational Linguistics.\nSoï¬a Serrano and Noah A. Smith. 2019. Is attention\ninterpretable? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2931â€“2951. Association for Compu-\ntational Linguistics.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. 2014. Deep inside convolutional networks: Vi-\nsualising image classiï¬cation models and saliency\nmaps. In 2nd International Conference on Learn-\ning Representations, ICLR 2014, Banff, AB, Canada,\nApril 14-16, 2014, Workshop Track Proceedings.\nJulia Strout, Ye Zhang, and Raymond Mooney. 2019.\nDo human rationales improve machine explana-\ntions? In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 56â€“62. Association for\nComputational Linguistics.\nZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,\nand Hang Li. 2016. Modeling coverage for neural\nmachine translation. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2016, August 7-12, 2016, Berlin,\nGermany, Volume 1: Long Papers. The Association\nfor Computer Linguistics.\nMartin Tutek and Jan Snajder. 2020. Staying true to\nyour word: (how) can attention become explanation?\nIn Proceedings of the 5th Workshop on Representa-\ntion Learning for NLP, pages 131â€“142. Association\nfor Computational Linguistics.\nShikhar Vashishth, Shyam Upadhyay, Gaurav Singh\nTomar, and Manaal Faruqui. 2019. Atten-\ntion interpretability across NLP tasks. CoRR,\nabs/1909.11218.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, undeï¬ne-\ndukasz Kaiser, and Illia Polosukhin. 2017. Attention\nis all you need. In Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing\nSystems, pages 6000â€“6010. Curran Associates Inc.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing\nthe structure of attention in a transformer language\nmodel. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 63â€“76. Association for\nComputational Linguistics.\n1298\nElena V oita, Rico Sennrich, and Ivan Titov. 2020.\nAnalyzing the source and target contributions to\npredictions in neural machine translation. CoRR,\nabs/2010.10907.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers , pages\n5797â€“5808. Association for Computational Linguis-\ntics.\nRongxiang Weng, Heng Yu, Xiangpeng Wei, and Wei-\nhua Luo. 2020. Towards enhancing faithfulness for\nneural machine translation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2675â€“2684,\nOnline. Association for Computational Linguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is\nnot not explanation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 11â€“20. Association for Computa-\ntional Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Googleâ€™s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nJiajun Zhang, Yang Zhao, Haoran Li, and Chengqing\nZong. 2019. Attention with sparsity regularization\nfor neural machine translation and summarization.\nIEEE ACM Trans. Audio Speech Lang. Process. ,\n27(3):507â€“518.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.7683628797531128
    },
    {
      "name": "ZhÃ ng",
      "score": 0.7548009753227234
    },
    {
      "name": "Transformer",
      "score": 0.6504743695259094
    },
    {
      "name": "Computer science",
      "score": 0.6252732276916504
    },
    {
      "name": "Calibration",
      "score": 0.5649035573005676
    },
    {
      "name": "Natural language processing",
      "score": 0.5221436619758606
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4700079560279846
    },
    {
      "name": "Artificial neural network",
      "score": 0.4507283866405487
    },
    {
      "name": "Computational linguistics",
      "score": 0.4135363698005676
    },
    {
      "name": "Engineering",
      "score": 0.20929980278015137
    },
    {
      "name": "Electrical engineering",
      "score": 0.17121177911758423
    },
    {
      "name": "Mathematics",
      "score": 0.13036873936653137
    },
    {
      "name": "History",
      "score": 0.10867524147033691
    },
    {
      "name": "Voltage",
      "score": 0.09784555435180664
    },
    {
      "name": "Statistics",
      "score": 0.06501501798629761
    },
    {
      "name": "China",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}