{
  "title": "Automatic Composition of Guitar Tabs by Transformers and Groove Modeling",
  "url": "https://openalex.org/W3047571292",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100384510",
      "name": "Yu‐Hua Chen",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5048096590",
      "name": "Yu-Hsiang Huang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5086833741",
      "name": "Wen-Yi Hsiao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5061291906",
      "name": "Yi‐Hsuan Yang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2394782790",
    "https://openalex.org/W2989636112",
    "https://openalex.org/W2574842964",
    "https://openalex.org/W2019335092",
    "https://openalex.org/W2296756383",
    "https://openalex.org/W2991108091",
    "https://openalex.org/W2036197063",
    "https://openalex.org/W2613740626",
    "https://openalex.org/W2296378237",
    "https://openalex.org/W3123961192",
    "https://openalex.org/W2991248973",
    "https://openalex.org/W2995416527",
    "https://openalex.org/W2809716692",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3092879656",
    "https://openalex.org/W1501078361",
    "https://openalex.org/W2552880089",
    "https://openalex.org/W2023973629",
    "https://openalex.org/W1991655173",
    "https://openalex.org/W2958623036",
    "https://openalex.org/W1987986948",
    "https://openalex.org/W2771567333",
    "https://openalex.org/W2902076983",
    "https://openalex.org/W2095428299",
    "https://openalex.org/W2963408210",
    "https://openalex.org/W3131643527",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2402815563",
    "https://openalex.org/W2953717317",
    "https://openalex.org/W2404247810",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2982753834",
    "https://openalex.org/W2294724140",
    "https://openalex.org/W2296230291",
    "https://openalex.org/W168712871",
    "https://openalex.org/W1971334401",
    "https://openalex.org/W1977556410",
    "https://openalex.org/W2921495374"
  ],
  "abstract": "Deep learning algorithms are increasingly developed for learning to compose music in the form of MIDI files. However, whether such algorithms work well for composing guitar tabs, which are quite different from MIDIs, remain relatively unexplored. To address this, we build a model for composing fingerstyle guitar tabs with Transformer-XL, a neural sequence model architecture. With this model, we investigate the following research questions. First, whether the neural net generates note sequences with meaningful note-string combinations, which is important for the guitar but not other instruments such as the piano. Second, whether it generates compositions with coherent rhythmic groove, crucial for fingerstyle guitar music. And, finally, how pleasant the composed music is in comparison to real, human-made compositions. Our work provides preliminary empirical evidence of the promise of deep learning for tab composition, and suggests areas for future study.",
  "full_text": "AUTOMATIC COMPOSITION OF GUITAR TABS BY TRANSFORMERS\nAND GROOVE MODELING\nYu-Hua Chen1,2,3, Yu-Hsiang Huang1, Wen-Yi Hsiao1, and Yi-Hsuan Yang1,2\n1 Taiwan AI Labs, Taiwan, 2 Academia Sinica, Taiwan, 3 National Taiwan University, Taiwan\nr08946011@ntu.edu.tw, {yshuang,wayne391,yhyang}@ailabs.tw\nABSTRACT\nDeep learning algorithms are increasingly developed for\nlearning to compose music in the form of MIDI ﬁles. How-\never, whether such algorithms work well for composing\nguitar tabs, which are quite different from MIDIs, remain\nrelatively unexplored. To address this, we build a model for\ncomposing ﬁngerstyle guitar tabs with Transformer-XL, a\nneural sequence model architecture. With this model, we\ninvestigate the following research questions. First, whether\nthe neural net generates note sequences with meaningful\nnote-string combinations, which is important for the gui-\ntar but not other instruments such as the piano. Second,\nwhether it generates compositions with coherent rhythmic\ngroove, crucial for ﬁngerstyle guitar music. And, ﬁnally,\nhow pleasant the composed music is in comparison to real,\nhuman-made compositions. Our work provides prelimi-\nnary empirical evidence of the promise of deep learning\nfor tab composition, and suggests areas for future study.\n1. INTRODUCTION\nThanks to the cumulative efforts in the community, in re-\ncent years we have seen great progress in using deep learn-\ning models for automatic music composition [8]. An im-\nportant body of research has been invested on creating pi-\nano compositions, or more generally keyboard style music.\nFor instance, the “Music Transformer” presented by Huang\net al. [19] employs 172 hours of piano performances to\nlearn to compose classical piano music. Another group\nof researchers extends that model to generate pop piano\ncompositions from 48 hours of human-performed piano\ncovers [20]. They both use a MIDI-derived representa-\ntion of music and describe music as a sequence of event\ntokens such as NOTE-ON and NOTE-VELOCITY. While\nthe MIDI format works the best for representing keyboard\ninstruments and less for other instruments (for reasons de-\nscribed below), Donahue et al. [14] and Payne [31] show\nrespectively that it is possible for machines to learn from a\nset of MIDI ﬁles to compose multi-instrument music.\nThere are, however, many other forms of musical nota-\ntion that are quite different from the staff notation assumed\nc⃝ Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: “Automatic Composi-\ntion of Guitar Tabs by Transformers and Groove Modeling ”, 21st Inter-\nnational Society for Music Information Retrieval Conference, Montréal,\nCanada, 2020.\nFigure 1. An example of ﬁngerstyle guitar tab composed\nby human, along with the corresponding staff notation.\nby keyboard music. For example, the tabulature, or “tab”\nfor short, is a notation format that indicates instrument ﬁn-\ngering rather than musical pitches. It is common for fret-\nted stringed instruments such as the guitar and ukulele, and\nfree reed aerophones such as the harmonica. It makes more\nsense for people playing such instruments to read the tabs,\nas they suggest how to move the ﬁngers.\nAs shown in Figure 1, a tab contains information such\nas the ﬁngering conﬁguration on the fretboard (six strings\nfor the case of the guitar) as well as usage of the left-hand\nor right-hand playing techniques. Such information is usu-\nally missing in the corresponding staff notation and MIDI\nﬁles. Learning to automatically compose guitar music di-\nrectly from MIDI ﬁles, though possible, has the limitation\nof ignoring the way people play these instruments. How-\never, to our best knowledge, little has been done to use tabs\nto train a deep generative model.\nTo investigate the applicability of modern deep learn-\ning architectures for composing tabs, we compile a new\ndataset of 333 TAB ﬁles of “ﬁngerstyle guitar” (including\noriginally ﬁngerstyle guitar music and ﬁngerstyle adapta-\ntion) [3], and modify the data representation of the Mu-\nsic Transformer [19] to make the extended model learn to\ncompose guitar tabs. With this model, we aim to answer\nthree research questions (RQs):\n• Whether the neural network learns to generate not\nonly the note sequences but also the ﬁngering of the\nnotes to be played on a fretboard, from reading only\nthe tabs (instead of, for example, watching videos\ndemonstrating how people play the guitar)?\n• Whether the neural network generates compositions\nwith coherent “groove,” or the use of rhythmic pat-\nterns over time [13, 32, 39]? It is generally assumed\nthat the layers of a neural network learn abstrac-\ntions of data on their own to perform the intended\narXiv:2008.01431v1  [cs.SD]  4 Aug 2020\ntask, e.g., to predict the next events given the his-\ntory. However, in music, groove is usually indirectly\nimplied according to the arrangement of notes along\nthe time axis, instead of explicitly speciﬁed in either\na MIDI or TAB ﬁle. Therefore, it remains to be stud-\nied whether the model can do better if it has to ex-\nplicitly handle bar-levelGROOVING events, inserted\ninto the training data as a high-level information in\nsome way, or if such a modiﬁcation is not needed.\nThis is in particular relevant in the context of ﬁnger-\nstyle composition, as in ﬁngerstyle a guitarist has to\ntake care of the melody, chord comping, bass line\nand rhythm simultaneously [3].\n• Finally, how the compositions generated by the neu-\nral network compare with human-composed guitar\ntabs, when both rendered into audio waveforms and\npresented to human listeners? This gives us a direct\nevaluation of the effectiveness of the neural network\nin modeling guitar music.\nWe provide audio rendition of examples of the gener-\nated tabs (using a guitar synthesizer of a DAW called Am-\nple Sound [1]) at https://ss12f32v.github.io/\nGuitar-Transformer-Demo/, along with a video\nrecording of a guitarist playing a generated tab.\nIn what follows, we review some related work in Sec-\ntion 2, and then present the tab dataset in Section 3. After\nthat, we describe in Section 4 the methodology for model-\ning and learning to compose guitar tabs. We present the re-\nsult of objective and subjective evaluations addressing the\naforementioned research questions in Section 5.\n2. RELATED WORK\n2.1 Guitar-related Research in MIR\nIn the music information retrieval (MIR) community, re-\nsearch concerning guitar is often related to automatic gui-\ntar transcription [5, 7, 9, 16, 18, 21, 22, 27, 34, 46] and play-\ning technique detection [4, 10, 37, 38]. For example, Su et\nal. [38] built a convolution neural network (CNN) model\nfor detecting the playing techniques associated with the\nstring-pressing hand, and incorporated that for transcribing\naudio recordings of unaccompanied electric guitar perfor-\nmances. Rodríguez et al. [34] presented a model for tran-\nscribing Flamenco guitar falsetas, and Abeßer and Schuller\n[5] dealt with the transcription of solo bass guitar record-\nings. We note that, while automatic transcription concerns\nwith recovering the tab underlying an audio guitar perfor-\nmance, our work deals with automatic composition of orig-\ninal guitar tabs in the symbolic domain, and therefore does\nnot consider audio signals.\nAs there are multiple fret positions to play the same note\non a guitar, it may not be easy for a novice guitar learner\nto play a guitar song without the corresponding tab. Au-\ntomatic suggestion of the ﬁngering given a human-made\n“lead sheet,” a symbolic format that speciﬁes the melody\nand chord sequence but not their ﬁngering, has therefore\nbeen a subject of research. Existing work has explored\nthe use of hidden Markov models, genetic algorithm, and\nneural networks to predict the ﬁngering by examining its\nplaying difﬁculty for a guitarist, viewing the task as an op-\ntimal path ﬁnding problem [6,28,35,40]. While such prior\narts can be considered as performing a MIDI-to-TAB con-\nversion, our work aims to model TABs directly.\nXi et al. developed the GuitarSet [45], a set of 360 au-\ndio recordings of a guitar equipped with the hexaphonic\npickup. The special pickup is able to capture the sound\nfrom each string individually, making it possible for a\nmodel to learn to perform multipitch estimation and tab-\nulature ﬁngering arrangement at the same time. Using the\ndataset, Wiggins and Kim [43] built such a model with\nCNN, achieving 0.83 F-score (i.e., the harmonic average\nof precision and recall) for multipitch estimation, and 0.90\nfor identifying the string-fret combinations of the notes.\nWhile the dataset is relevant for guitar transcription, its\nrecordings are all around 12–16 bars in length only, which\nseems to be too short for deep generative modeling.\nMcVicar et al. [24–26] used to build sophisticated prob-\nabilistic systems to algorithmically compose rhythm and\nlead guitar tabs from an input chord and key sequence. Our\nwork differs from theirs in that we aim to build a general-\npurpose tab composition model using modern deep gener-\native networks. An extra complexity of our work is that we\nexperiment with ﬁngerstyle guitar, a type of performance\nthat can be accomplished by a single guitarist.\n2.2 Transformer Models for Automatic Composition\nThe Transformer [41] is a deep learning model that is de-\nsigned to handle ordered sequences of data, such as natu-\nral language. It models a word sequence (w1, w2, . . . wT )\nseen in the training data by factorizing the joint prob-\nability into a product of conditionals, namely, P(w1) ·\nP(w2|w1) ····· P(wT |w1, . . . , wT−1) . During the train-\ning process, the model optimizes its parameters so as to\ncorrectly predict the next word wt given its preceding his-\ntory (w1, w2, . . . wt−1), for each position t in a sequence.\nFollowing some recent work on recurrent neural net-\nwork (RNN)-based automatic music composition [29, 42],\nHuang et al. [19] viewed music as a language and for the\nﬁrst time employed the Transformer architecture for mod-\neling music. Given a collection of MIDI performances,\nthey converted each MIDI ﬁle to a time-ordered sequence\nof musical “events,” so as to model the joint probability of\nevents as if they are words in natural language (see Section\n4.1 for details of such events). The Transformer with rel-\native attention was shown to greatly outperform an RNN-\nbased model, called PerformanceRNN [29], in a subjective\nlistening test [19], inspiring the use of Transformer-like ar-\nchitectures, such as Transformer or Transformer-XL [12],\nin follow-up research [11, 14, 20, 31, 44].1\nThere are lots of approaches to automatic music com-\nposition, deep learning- and non-deep learning based in-\ncluded [8, 15, 30]. We choose to consider only the Trans-\nformer architecture here, to study whether we can translate\nits strong result in modeling MIDIs to modeling TABs.\n1 We note that it is debatable whether music and language are related.\nWe therefore envision that some other new architectures people will come\nup with in the future might do a much better job than Transformers in\nmodeling music. This is, however, beyond the scope of the current work.\n# tabs # bars # bars # events\nper tab per tab\ntraining 303 24,381 80 ±41 5,394 ±3,116\nvalidation 30 2,593 74 ±35 5,244 ±3,183\nTable 1. Statistics of the dataset; the last two columns\nshow the mean and standard deviation values across each\nset. Please see Table 2 for deﬁnitions of the events.\n3. FINGERSTYLE GUITAR TAB DATASET\nThere have been some large-scale MIDI datasets out there,\nsuch as the Lakh MIDI dataset [33] and BitMidi [2]. The\nformer, for example, contains 176,581 unique MIDI ﬁles\nof full songs. In contrast, existing datasets of tabs are usu-\nally smaller and shorter, as they are mainly designed for\nlearning the mapping between tabs and audio (i.e., for tran-\nscription research), rather than for generative modeling of\nthe structure of tabs. The tabs in the GuitarSet [45], for\nexample, are performances of short excerpts of songs, typ-\nically 12–16 bars in length, which are not long.\nFor the purpose of this research, we compile a guitar\ntab dataset on our own, focusing on the speciﬁc genre of\nﬁngerstyle guitar. Speciﬁcally, we collect digital TABs of\nfull songs, to facilitate language modeling of guitar tabs.\nWe go through all the collected TABs one-by-one and ﬁl-\nter out those that are of low quality (e.g., with wrong ﬁn-\ngering, obvious annotation errors), or are not ﬁngerstyle\n(e.g., have more than one tracks). We also discard TABs\nthat are not in standard tuning, to avoid inconsistent map-\nping between notes and ﬁngering. As shown in Table 1,\nthis leads to a collection of 333 TABs, each with around\n80 bars. This includes TABs of famous professional ﬁn-\ngerstyle players such as Tommy Emmanuel and Sungha\nJung. All the TABs are in 4/4 time signature, and they can\nbe in various keys. We reserve 30 TABs for validation and\nperformance evaluation, and use the rest for training.\nPlease note that, similar to the MIDI ﬁles available in\nLakh MIDI [33], the TAB ﬁles we collect do not contain\nperformance information such as expressive variations in\ndynamics (i.e., note velocity) and micro-timing [23, 29].\nTo increase velocity variation, we use Ample Sound [1] to\nadd velocity to each note by its humanization feature. We\ndo not deal with micro-timing in this work.\n3.1 Fingerstyle\nIt is interesting to focus on only ﬁngerstyle guitar in the\ncontext of this work, as we opt for validating the effec-\ntiveness of Transformers for single-track TABs ﬁrst, before\nmoving to modeling multi-track performances that involve\nat least a guitar (e.g., a rock song). We give a brief intro-\nduction of ﬁngerstyle guitar below.\nFingerstyle [3] is at ﬁrst a term that describes using ﬁn-\ngertips or ﬁngernails to pluck the strings to play the guitar.\nNowadays, the term is often used to describe an arrange-\nment method to blend multiple parts of musical elements\nor tracks, which are initially played by several instruments,\ninto the composition of one guitar track. Therefore, a gui-\ntarist playing ﬁngerstyle has to simultaneously take care of\ncategory/type description\nNOTE-ON 45 different pitches (E2–C6)\nNOTE-DURATION multiples of the 32th note (1–64)\nNOTE-VELOCITY note velocity as 32 levels (1–32)\nPOSITION temporal position within a bar;\nmultiples of the 16th note (1–16)\nBAR marker of the transition of bars\nSTRING 6 strings on a tab\nFRET 20 fret positions per string\nTECHNIQUE 5 playing techniques: slap, press\nupstroke, downstroke, and hit-top\nGROOVING 32 grooving patterns\nTable 2. The list of events adopted for representing a tab as\nan event sequence. The ﬁrst ﬁve are adapted from [19,20],\nwhereas the last four are tab-speciﬁc and are new. We have\nin total 45+64+32+16+1+6+20+5+32=231 unique events.\nFigure 2. An example of the result of “TAB-to-event” con-\nversion needed for modeling a tab as a sequence. Here, we\nshow the resultant event representation of a C chord.\nthe melody line, bass line, chord comping and the rhythmic\ngroove. Groove, in particular, is important in ﬁngerstyle,\nas it is now only possible to work on the rhythmic ﬂow of\nmusic with a single guitar and the use of the two hands.\nWe hence pay special attention to groove modeling in this\nwork (see Section 4.3).\n4. MODELING GUITAR TABS\nIn this section, we elaborate how we design an event repre-\nsentation for modeling guitar tabs, or more generally tabs\nof instruments played by string strumming.\n4.1 Event Representation for MIDIs: A Quick Recap\nIn representing MIDIs as a sequence of “events,” Huang\net al. [20] considered, amongst others, the following event\ntokens. Each note is represented by a triplet of NOTE-ON,\nNOTE-DURATION, and NOTE-VELOCITY events, repre-\nsenting the MIDI note number, quantized duration as an in-\nteger multiple of a minimum duration, and discrete level of\nnote dynamics, respectively. The minimum duration is set\nto the 32th note. The onset time of the notes, on the other\nhand, is marked (again after quantization) on a time grid\nwith a speciﬁc resolution, which is set to the 16th note as\nin [19]. Speciﬁcally, to place the notes over the 16-th note\ntime grid, they use a combination of POSITION and BAR\nevents, indicating respectively the position of a note onset\nwithin a bar, among the 16 possible locations, and the be-\nginning of a new bar as the music unfolds over time. This\nevent representation has been shown effective in modeling\npop piano [20]. We note that the time grid outlined with\nthis combination of POSITION and BAR events can also\ncontribute to modeling the rhythm of ﬁngerstyle guitar.\n4.2 Event Representation for Tabs\nTo represent TABs, we propose to add, on top of the afore-\nmentioned ﬁve types of events for MIDIs, 2 the follow-\ning three new types of ﬁngering-related events: STRING,\nFRET, TECHNIQUE, and a type of rhythm-related events:\nGROOVING. We introduce the ﬁrst three below, and the last\nin the next subsection. Table 2 lists all the events consid-\nered, whereas Figure 2 gives an example of how we repre-\nsent a C chord with such an event representation.\nWe use the ﬁrst 20 frets of the 6 strings in the collected\nTABs, i.e., each string can play 20 notes. The pitch range\nof the strings overlaps, so a guitarist can play the same\npitch on different strings, with moderate but non-negligible\ndifference in timbre. The ﬁngering of the notes also affects\nplayability [46]. In standard tuning, the strings can play 45\ndifferent pitches, from E2 to C6.\nIn our implementation, we adopt the straightforward ap-\nproach to account for the various possible playing positions\nof the notes—to add STRING and FRET tokens right after\nthe NOTE-ON tokens in the event sequence representing a\ntab. We note that the FRET tokens are actually redundant,\nin that the combination of NOTE-ON and STRING alone\nis sufﬁcient to determine the fret position to use. However,\nin pilot studies we found the inclusion of FRET makes the\nmodel converges faster at the training time.\nSpeciﬁcally, instead of a 3-tuple representation of a note\nas the case in MIDIs, we use a 5-tuple note representation\nthat consists of successive tokens of NOTE-VELOCITY,\nNOTE-ON, NOTE-DURATION, STRING and FRET for\nTABs. As such ﬁve tokens always occur one after another\nin the training sequences, it is easy for a Transformer not to\nmiss any of them when generating a new NOTE-ON event\nat the inference time, according to our empirical observa-\ntion of the behavior of the Transformers.\nHowever, as we do not impose constraints on the asso-\nciation between NOTE-ON and STRING, it remains to be\nstudied whether a Transformer can learn to compose tabs\nwith reasonable note-string combinations. This is the sub-\nject of the 1st RQoutlined in Section 1.\nAs for the TECHNIQUEs, we consider the following\nﬁve right-hand techniques: slap, press, upstroke, down-\nstroke, and hit-top, which account for ∼1% of the events\nin our training set. The inclusion of other techniques, such\nas sliding and bending, is left as a future work.\nSimilar to [19,20], we consider the 16th note as the res-\nolution of onset times, which is okay for 4/4 time signature.\nIncreasing the resolution further to avoid quantization er-\nrors and to enhance expressivity is also left to the future.\n4.3 Groove Modeling\nGroove can be in general considered as a rhythmic feeling\nof a changing or repeated pattern, or “humans’ pleasurable\n2 Huang et al. [20] actually considered the Chord and Tempo events\nadditionally; we found these two types of event less useful in modeling\ntabs, according to preliminary experiments.\n(a) (b)\nFigure 3. Samples of 16-dim hard grooving patterns as-\nsigned to 2 different clusters (a), (b) by kmeans clustering.\nurge to move their bodies rhythmically in response to mu-\nsic” [36]. Unlike the note-related or time-related events,\ngroove is usually implicitly implied as a result of the ar-\nrangement of note onsets over time, instead of be explicitly\nspeciﬁed in either a MIDI or TAB ﬁle. Hence, it might be\npossible for a Transformer to learn to compose music with\nreasonable groove, without we explicitly inform it what\ngroove is. We refer to this baseline variant of our Trans-\nformer as the no groovingversion, which considers all the\nevents listed in Table 2 butGROOVING.\nHowever, as a tab is now represented as a sequence of\nevents, it is possible to add groove-related events to help\nthe model make sense of this phenomenon. Since our event\nrepresentation has the BAR events to mark the bar lines,\nwe can ask the model to learn to generate a “bar-level”\nGROOVING event right after a BAR event, before proceed-\ning to generate the actual content of the bar. Whether such\na groove-aware approach beneﬁts the quality of the gener-\nated tabs is the subject of our 2nd RQ.\nTo implement such an approach, we need to come up\nwith 1) a bar-level grooving representation of symbolic\nmusic, and 2) a method to convert the grooving represen-\ntation, which might be a vector, to a discrete event token.\nIn this work, we represent groove by the occurrence of\nnote onset over the 16-th time grid, leading to the following\nfour grooving representations of music.\n• Hard grooving: A 16-dim binary vector marking\nthe presence of (at least one) onset per each 16 po-\nsitions of a bar. A popular pattern in our dataset, for\nexample, is [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\nmeaning onsets on beats only.\n• Soft grooving: A soft version that considers the\nnumber of onsets (but disregarding the velocity val-\nues) for each position, normalized by the maximum\nin the bar, leading to a 16-dim real-valued vector.\n• Multi-resolution hard (or soft) grooving: Variants\nof the last two that additionally consider correspond-\ning down-sampled 8-dim and 4-dim vectors to em-\nphasize the beats (e.g., counting only the onsets on\nbeats), and then concatenate the vectors together,\nyielding a 28-dim vector (i.e., 16+8+4).\nTo convert the aforementioned grooving patterns to\nevents, a discretization is needed. Among various possi-\nble approaches, we experiment with the simplest idea of\ngrouping the grooving patterns seen in the training set into\na number of clusters. We can then use the ID of the cluster\na grooving pattern is associated with for the GROOVING\nevent of that grooving pattern. For simplicity, we employ\nthe classic kmeans algorithm [17] here, setting k to 32.\nPlease see Figure 3 for an example of the clustering result.\nFigure 4. Distributions of (a) the error rate for each note\nin the string arrangement prediction of our model, and (b)\nthe counts of each note in the training set.\nstring(high-pitched↔low-pitched)\n1st 2nd 3rd 4th 5th 6th\n(a) accuracy 100% 99% 97% 94% 91% 90%\n(b) pitch 42 ∼0% ∼0% 10% ∼0% 27% 63%\n(c) pitch 57 ∼0% 6% 65% 26% ∼0% ∼0%\n(d) pitch 69 85% 14% ∼0% ∼0% ∼0% ∼0%\nTable 3. (a) The average accuracy of our model in as-\nsociating each STRING with a NOTE-ON, broken down\nby string; (b–d) The string-relevant output probability esti-\nmated by our model for three different pitches.\n4.4 Transformer-XL-based Architecture\nFollowing [14, 20], we use the Transformer-XL [12] for\nthe architecture of our model. Unlike the Transformer\nused in [19], the Transformer-XL gains a longer receptive\nﬁled with a segment-level recurrence mechanism, thereby\nseeing further into the history and beneﬁting from the ex-\ntended memory. We base our implementation on the open\nsource code of [20], adopting many of their settings. For\nexample, we also set the sequence length and recurrence\nlength to 512 events, and use 12 self-attention layers and 8\nattention heads. The model has in total∼41M trainable pa-\nrameters. The training process converges within 12 hours\non a single NVIDIA V100 GPU, with batch size 32.\n5. EV ALUATION\n5.1 Experiment 1: On Fingering\nThe 1st RQ explores how a Transformer learns the associa-\ntion between notes and ﬁngering, without human-assigned\nprior knowledge/constraints on the association. For sim-\nplicity, we use the no groovingvariant of our model here.\nA straightforward approach to address this RQ is to let\nthe model generates randomly a large number of event se-\nquences (i.e., compositions) and examine how often it gen-\nerates a plausible STRING event after a NOTE-ON event.\nTable 3(a) shows the average note-string association ac-\ncuracy calculated from 50 generated 16-bar tabs, broken\ndown into six values according to STRING. To our mild\ndisappointment, the accuracy, though generally high, is not\nHard accuracy↑ Soft distance↓\nmean max mean min\nhard grooving 76.2% 82.4% 56.3 44.6\nsoft grooving 76.9% 83.0% 56.2 43.7\nmulti-hard 79.0% 85.7% 57.8 44.3\nmulti-soft 74.6% 81.1% 64.7 52.9\nno grooving 70.0% 80.1% 58.6 47.7\ntraining data 82.1% 89.5% 43.8 28.6\nrandom 64.9% 71.3% 70.6 59.6\nTable 4. Objective evaluation on groove coherence.\nperfect. This indicates that some post-processing is still\nneeded to ensure the note-string association is correct.\nAs Table 3(a) shows larger errors toward the 6th string,\nwe also examine how the errors distribute over the pitches.\nInterestingly, Figure 4(a) shows that the model makes mis-\ntakes only in the low end; the ﬁngering prediction is good\nfor pitches (i.e., MIDI numbers) from 64 to 84.\nIt is hard to ﬁnd out why exactly this is the case, but we\npresent two more observations here. First, we plot in Fig-\nure 4(b) the popularity of these pitches in the training set.\nThe Pearson correlation coefﬁcient between the note quan-\ntity and the error rate is weak, at 0.299, suggesting that this\nmay not be due to the sparseness of the low-pitched notes.\nSecond, we show in Table 3(b)–(d) the note-string associ-\nation output probability estimated by our model for three\ndifferent pitches. Interestingly, it seems the model has the\ntendency to use neighboring strings for each pitch. For ex-\nample, pitch 42 is actually a bass note playable on the 6th\nstring, and it erroneously “leaks” mostly to the 5th string.\n5.2 Experiment 2: On Groove\nFigure 5 gives two examples of tabs generated by the hard\ngrooving model. It seems the grooving is consistent across\ntime in each tab. But, how good it is?\nThe 2nd RQ tests whether the addedGROOVING events\nhelp a Transformer compose tabs with better rhythmic co-\nherence. We therefore intend to compare the performance\nof models trained with or without GROOVING for generat-\ning “continuations” of a given “prompt.”\nWe consider both objective and subjective evaluations\nhere. For the former, we compare the models trained with\nGROOVING events obtained with each of the four vector-\nquantized grooving representations described in Section\n4.3. We ask the models to generate 16-bar continuations\nfollowing the ﬁrst 4 bars of the 30 tabs in the validation\nset. The performance of the models is compared against\nthat of the ‘no-grooving’ baseline, the ‘real’ continuations\n(of these 30 tabs), and a ‘random’ baseline that picks the\nnext 16 bars from another tab at random from the valida-\ntion set. The last two are meant to set the high-end and\nlow-end performances, respectively. For fair comparison,\nwe also project the note onsets of the validation data onto\nthe 16th-note grid underlying our training data.\nWe consider the following two simple objective metrics:\n• Hard accuracy: Given the hard grooving patterns\nX = (x1, . . . ,xN ) of the prompt, and those of the\nFigure 5. Segments of 2 tabs randomly generated by the hard grooving model; below each tab—the soft grooving patterns.\nFigure 6. Result of the ﬁrst user study asking subjects to\nchoose the best among the three continuations generated\nby different models, with or without GROOVING, given a\nman-made prompt. The result is broken down according to\nthe self-report guitar proﬁciency level of the subjects.\ncontinuation Y = (y1, . . . ,yM ), where both xi and\nyj are in {0, 1}K, N = 4, M = 16, K = 16, we\ncompare the similarity between X and Y by\nmeani=(1,...,N)\n1\nMK\nM∑\nj=1\nK∑\nk=1\nXNOR(x(k)\ni , y(k)\nj ) ,\n(1)\nwhere XNOR(·, ·) returns, element-wisely, whether\nthe k-th element of xi and yj are the same. Alter-\nnatively, we replace the mean aggregator bymax, to\nsay it is good enough for yj to be close to any xi.\n• Soft distance: We consider instead the soft groov-\ning patterns ˜xi and ˜yj, and compute the distance be-\ntween them as meani=(1,...,N)\n1\nM\n∑M\nj=1 ∥˜xi −˜yj∥2\n2 .\nWe can similarly replace mean by the min function.\nTable 4 shows that, consistently across different metrics,\ngroove-aware models outperform the no-grooving model.\nMoreover, the scores of the groove-aware models are\ncloser to the high end than to the low end. It is also im-\nportant to note that, there is still a moderate gap between\nthe best model’s composition and the real data, which has\nto be further addressed in the future work.\nFigure 6 shows the result of the subjective evaluation,\nwhere we present the audio rendition (using a guitar syn-\nthesizer) of the aforementioned 16-bar continuations to hu-\nman listeners, and ask them to choose the one they like\nthe most, among those generated by the ‘no-grooving,’\nReal No grooving Hard grooving\nMOS 3.48±1.16 2.80 ±1.03 3.43 ±1.12\nTable 5. Result of the second user study (in mean opinion\nscore, from 1 to 5) comparing audio renditions of real tabs\nand machine-composed tabs by two variants of our model.\n‘soft-grooving,’ and ‘hard-grooving’ models. We divide\nthe response from 57 participants by their self-report pro-\nﬁciency level in guitar. Figure 6 shows that professionals\nare aware of the difference between groove-aware and no-\ngrooving models. According to their optional verbal re-\nsponse, groove-aware models continue the prompts better,\nand generate more pleasant melody lines.\n5.3 Experiment 3: On Comparison with Real Tabs\nFinally, our last RQ involves another user study where we\nask participants to rate, on a Likert ﬁve-point scale how\nthey like the audio rendition of the continuations, this time\nincluding the result of real continuations. For groove-\naware models, we consider hard-grooving only, for its sim-\nplicity and also for reducing the load on the subjects. Much\nto our surprise, the average result from 23 participants (see\nTable 5) suggests that hard-grooving compositions are ac-\ntually on par with real compositions. We believe this re-\nsult has to be taken with a grain of salt, as it concerns\nwith only fairly short pieces (i.e., 16 bars) that do not con-\ntain performance-level variations. Yet, it provides evidence\nshowing the promise of deep learning for tab composition.\n6. CONCLUSION\nIn this paper, we have presented a series of evaluations\nsupporting the effectiveness of a modern neural sequence\nmodel, called Transformer-XL, for automatic composition\nof ﬁngerstyle guitar tabs. The model still has troubles in\nensuring the note-string association and the rhythmic co-\nherence of the generated tabs. How well the model gen-\nerates tabs of plausible long-term structure is not yet stud-\nied. And, much of the expression in guitar music is left\nunaddressed. Much work are yet to be done to possibly re-\ndesign the network architecture and the tab representation.\nYet, we hope this work shows promises that inspire more\nresearch on this intriguing area of research.\n7. REFERENCES\n[1] Ample sound. https://www.amplesound.\nnet/en/index.asp.\n[2] The BitMidi dataset. https://github.com/\nfeross/bitmidi.com.\n[3] Fingerstyle guitar. https://en.wikipedia.\norg/wiki/Fingerstyle_guitar/.\n[4] J. Abeßer, H. Lukashevich, and G. Schuller. Feature-\nbased extraction of plucking and expression styles of\nthe electric bass guitar. In Proc. IEEE International\nConference on Acoustics, Speech, & Signal Process-\ning, pages 2290–2293, 2010.\n[5] J. Abeßer and G. Schuller. Instrument-centered music\ntranscription of solo bass guitar recordings.IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing, 25(9):1741–1750, 2017.\n[6] S. Ariga, S. Fukayama, and M. Goto. Song2Guitar:\nA difﬁculty-aware arrangement system for generating\nguitar solo covers from polyphonic audio of popular\nmusic. In Proc. International Society for Music Infor-\nmation Retrieval Conference, pages 568–574, 2017.\n[7] A. M. Barbancho, A. Klapuri, L. J. Tardon, and I. Bar-\nbancho. Automatic transcription of guitar chords and\nﬁngering from audio. IEEE Trans. Audio, Speech and\nLanguage Processing, 20(3):915–921, 2012.\n[8] J.-P. Briot, G. Hadjeres, and F. Pachet. Deep Learning\nTechniques for Music Generation, Computational Syn-\nthesis and Creative Systems. Springer, 2019.\n[9] G. Burlet and I. Fujinaga. Robotaba guitar tablature\ntranscription framework. In Proc. International Soci-\nety for Music Information Retrieval Conference, 2013.\n[10] Y .-P. Chen, L. Su, and Y .-H. Yang. Electric guitar play-\ning technique detection in real-world recordings based\non F0 sequence pattern recognition. In Proc. Interna-\ntional Society for Music Information Retrieval, 2015.\n[11] K. Choi, C. Hawthorne, I. Simon, M. Dinculescu, and\nJ. Engel. Encoding musical style with transformer au-\ntoencoders. arXiv preprint arXiv:1912.05537, 2019.\n[12] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. Le, and\nR. Salakhutdinov. Transformer-XL: Attentive language\nmodels beyond a ﬁxed-length context. In Proc. Annual\nMeeting of the Association for Computational Linguis-\ntics, pages 2978–2988, 2019.\n[13] S. Dixon, F. Gouyon, and G. Widmer. Towards char-\nacterisation of music via rhythmic patterns. In Proc.\nInternational Society for Music Information Retrieval,\npages 509–516, 2004.\n[14] C. Donahue, H. H. Mao, Y . E. Li, G. W. Cottrell, and\nJ. McAuley. LakhNES: Improving multi-instrumental\nmusic generation with cross-domain pre-training. In\nProc. International Society for Music Information Re-\ntrieval, pages 685–692, 2019.\n[15] J. D. Fernández and F. Vico. AI methods in algorith-\nmic composition: A comprehensive survey. Journal of\nArtiﬁcial Intelligence Research, 48(1):513–582, 2013.\n[16] S. Gorlow, M. Ramona, and F. Pachet. Decision-based\ntranscription of Jazz guitar solos using a harmonic\nbident analysis ﬁlter bank and spectral distribution\nweighting. arXiv preprint arXiv:1611.06505, 2016.\n[17] J. A. Hartigan and M. A. Wong. A k-means clustering\nalgorithm. JSTOR: Applied Statistics , 28(1):100–108,\n1979.\n[18] A. Hrybyk and Y . E. Kim. Combined audio and video\nanalysis for guitar chord identiﬁcation. In Proc. Inter-\nnational Society for Music Information Retrieval Con-\nference, 2010.\n[19] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon,\nC. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoff-\nman, M. Dinculescu, and D. Eck. Music Transformer:\nGenerating music with long-term structure. InProc. In-\nternational Conference on Learning Representations ,\n2019.\n[20] Y .-S. Huang and Y .-H. Yang. Pop Music Transformer:\nBeat-based modeling and generation of expressive Pop\npiano compositions. In Proc. ACM International Con-\nference on Multimedia, 2020.\n[21] E. J. Humphrey and J. P. Bello. From music audio\nto chord tablature: Teaching deep convolutional net-\nworks toplay guitar. In Proc. IEEE International Con-\nference on Acoustics, Speech & Signal Processing ,\npages 6974–6978, 2014.\n[22] C. Kehling, J. Abeßer, C. Dittmar, and G. Schuller. Au-\ntomatic tablature transcription of electric guitar record-\nings by estimation of score- and instrument-related pa-\nrameters. In Proc. International Conference on Digital\nAudio Effects, 2014.\n[23] A. Lerch, C. Arthur, A. Pati, and S. Gururani. Music\nperformance analysis: A survey. In Proc. International\nSociety for Music Information Retrieval Conference ,\npages 33–43, 2019.\n[24] M. McVicar, S. Fukayama, and M. Goto. AutoLead-\nGuitar: Automatic generation of guitar solo phrases in\nthe tablature space. In Proc. International Conference\non Signal Processing, pages 599–604, 2014.\n[25] M. McVicar, S. Fukayama, and M. Goto. AutoRhythm-\nGuitar: Computer-aided composition for rhythm guitar\nin the tab space. In Proc. International Computer Mu-\nsic Conference, 2014.\n[26] M. McVicar, S. Fukayama, and M. Goto. AutoGui-\ntarTab: Computer-aided composition of rhythm and\nlead guitar parts in the tablature space. IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing, 23(7):1105–1117, 2015.\n[27] J. Michelson, R. M. Stern, and T. M. Sullivan. Auto-\nmatic guitar tablature transcription from audio using\ninharmonicity regression and bayesian classiﬁcation.\nJournal of The Audio Engineering Society, 2018.\n[28] E. Mistler. Generating guitar tablatures with neural net-\nworks. Master of Science Dissertation, The University\nof Edinburgh, 2017.\n[29] S. Oore, I. Simon, S. Dieleman, D. Eck, and K. Si-\nmonyan. This time with feeling: Learning expressive\nmusical performance. Neural Computing and Applica-\ntions, 2018.\n[30] G. Papadopoulos and G. Wiggins. AI methods for al-\ngorithmic composition: A survey, a critical view and\nfuture prospects. In Proc. AISB Symposium on Musical\nCreativity, pages 110–117, 1999.\n[31] C. Payne. MuseNet. https://openai.com/\nblog/musenet/, 2019.\n[32] G. Peeters. Rhythm classiﬁcation using spectral\nrhythm patterns. In Proc. International Society for Mu-\nsic Information Retrieval, pages 644–647, 09 2005.\n[33] C. Raffel and D. P. W. Ellis. Extracting ground\ntruth information from MIDI ﬁles: A MIDIfesto. In\nProc. International Society for Music Information Re-\ntrieval, pages 796–802, 2016. [Online] https://\ncolinraffel.com/projects/lmd/.\n[34] S. Rodríguez, E. Gómez, and H. Cuesta. Automatic\ntranscription of Flamenco guitar falsetas. In Proc. In-\nternational Workshop on Folk Music Analysis, 2018.\n[35] S. I. Sayegh. Fingering for string instruments with\nthe optimum path paradigm. Computer Music Journal,\n13(3):76–84, 1989.\n[36] O. Senn, L. Kilchenmann, T. Bechtold, and F. Hoesl.\nGroove in drum patterns as a function of both rhythmic\nproperties and listeners’ attitudes. PLOS ONE, 13:1–\n33, 06 2018.\n[37] L. Su, L.-F. Yu, and Y .-H. Yang. Sparse cepstral and\nphase codes for guitar playing technique classiﬁcation.\nIn Proc. International Society for Music Information\nRetrieval, 2014.\n[38] T.-W. Su, Y .-P. Chen, L. Su, and Y .-H. Yang. TENT:\nTechnique-embedded note tracking for real-world gui-\ntar solo recordings. International Society for Music In-\nformation Retrieval, 2(1):15–28, 2019.\n[39] L. Thompson, S. Dixon, and M. Mauch. Drum tran-\nscription via classiﬁcation of bar-level rhythmic pat-\nterns. In Proc. International Society for Music Infor-\nmation Retrieval, pages 187–192, 2014.\n[40] D. R. Tuohy and W. D. Potter. Guitar tablature creation\nwith neural networks and distributed genetic search. In\nProc. International Conference on Industrial and Engi-\nneering Applications of Artiﬁcial Intelligence and Ex-\npert Systems, 2006.\n[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. In Proc. Advances in Neu-\nral Information Processing Systems, pages 5998–6008,\n2017.\n[42] E. Waite, D. Eck, A. Roberts, and D. Abo-\nlaﬁa. Project Magenta: Generating long-term\nstructure in songs and stories, 2016. https:\n//magenta.tensorflow.org/blog/2016/\n07/15/lookback-rnn-attention-rnn/ .\n[43] A. Wiggins and Y . E. Kim. Guitar tablature estimation\nwith a convolutional neural network. In Proc. Inter-\nnational Conference on Music Information Retrieval ,\npages 284–291, 2019.\n[44] S.-L. Wu and Y .-H. Yang. The Jazz transformer on\nthe front line: Exploring the shortcomings of AI-\ncomposed music through quantitative measures. In\nProc. International Society for Music Information Re-\ntrieval Conference, 2020.\n[45] Q. Xi, R. M. Bittner, J. Pauwels, X. Ye, and J. P.\nBello. GuitarSet: A dataset for guitar transcription. In\nProc. International Conference on Music Information\nRetrieval, pages 453–460, 2018. [Online] https://\ngithub.com/marl/guitarset/.\n[46] K. Yazawa, D. Sakaue, K. Nagira, K. Itoyama, and\nH. Okuno. Audio-based guitar tablature transcription\nusing multipitch analysis and playability constraints.\nIn Proc. IEEE International Conference on Acoustics,\nSpeech & Signal Processing, pages 196–200, 2013.",
  "topic": "Guitar",
  "concepts": [
    {
      "name": "Guitar",
      "score": 0.9797884225845337
    },
    {
      "name": "MIDI",
      "score": 0.7242764234542847
    },
    {
      "name": "Transformer",
      "score": 0.7139371037483215
    },
    {
      "name": "Computer science",
      "score": 0.6125580072402954
    },
    {
      "name": "Piano",
      "score": 0.6108468770980835
    },
    {
      "name": "Musical composition",
      "score": 0.5317144989967346
    },
    {
      "name": "Bass (fish)",
      "score": 0.4867822825908661
    },
    {
      "name": "Deep learning",
      "score": 0.48581838607788086
    },
    {
      "name": "Composition (language)",
      "score": 0.45171406865119934
    },
    {
      "name": "Speech recognition",
      "score": 0.44279155135154724
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41478264331817627
    },
    {
      "name": "Artificial neural network",
      "score": 0.41224798560142517
    },
    {
      "name": "Natural language processing",
      "score": 0.3615204095840454
    },
    {
      "name": "Musical",
      "score": 0.2482297122478485
    },
    {
      "name": "Engineering",
      "score": 0.22299250960350037
    },
    {
      "name": "Visual arts",
      "score": 0.19545456767082214
    },
    {
      "name": "Art",
      "score": 0.17424073815345764
    },
    {
      "name": "Acoustics",
      "score": 0.15663492679595947
    },
    {
      "name": "Literature",
      "score": 0.10280728340148926
    },
    {
      "name": "Voltage",
      "score": 0.08673596382141113
    },
    {
      "name": "Electrical engineering",
      "score": 0.07800483703613281
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Art history",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}