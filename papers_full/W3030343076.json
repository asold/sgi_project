{
  "title": "Cascaded Text Generation with Markov Transformers",
  "url": "https://openalex.org/W3030343076",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2369628830",
      "name": "Deng, Yuntian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222402211",
      "name": "Rush, Alexander M.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2122262818",
    "https://openalex.org/W2962969034",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2890438841",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2948381505",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W2101714644",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2963946353",
    "https://openalex.org/W2962897886",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2990389671",
    "https://openalex.org/W2972034672",
    "https://openalex.org/W2947334393",
    "https://openalex.org/W3087983746",
    "https://openalex.org/W2970690146",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W3004162225",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2948975009",
    "https://openalex.org/W2178449955",
    "https://openalex.org/W2986772082",
    "https://openalex.org/W2152966212",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2963536265",
    "https://openalex.org/W2508316494",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W3041866211",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3022593918",
    "https://openalex.org/W2514713644",
    "https://openalex.org/W3021914396",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2947898088",
    "https://openalex.org/W2952509486",
    "https://openalex.org/W2937808806",
    "https://openalex.org/W2951655021",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2949417144",
    "https://openalex.org/W3008388412",
    "https://openalex.org/W2970206392",
    "https://openalex.org/W3008170323",
    "https://openalex.org/W2969263964",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3141754740",
    "https://openalex.org/W2119052294",
    "https://openalex.org/W2920538220",
    "https://openalex.org/W3100753857",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W3000840023",
    "https://openalex.org/W2985694911",
    "https://openalex.org/W2945928904",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2125712079",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W2251610689",
    "https://openalex.org/W2463507112"
  ],
  "abstract": "The two dominant approaches to neural text generation are fully autoregressive models, using serial beam search decoding, and non-autoregressive models, using parallel decoding with no output dependencies. This work proposes an autoregressive model with sub-linear parallel time generation. Noting that conditional random fields with bounded context can be decoded in parallel, we propose an efficient cascaded decoding approach for generating high-quality output. To parameterize this cascade, we introduce a Markov transformer, a variant of the popular fully autoregressive model that allows us to simultaneously decode with specific autoregressive context cutoffs. This approach requires only a small modification from standard autoregressive training, while showing competitive accuracy/speed tradeoff compared to existing methods on five machine translation datasets.",
  "full_text": "Cascaded Text Generation\nwith Markov Transformers\nYuntian Deng\nHarvard University\ndengyuntian@seas.harvard.edu\nAlexander M. Rush\nCornell University\narush@cornell.edu\nAbstract\nThe two dominant approaches to neural text generation are fully autoregressive\nmodels, using serial beam search decoding, and non-autoregressive models, using\nparallel decoding with no output dependencies. This work proposes an autore-\ngressive model with sub-linear parallel time generation. Noting that conditional\nrandom ﬁelds with bounded context can be decoded in parallel, we propose an\nefﬁcient cascaded decoding approach for generating high-quality output. To param-\neterize this cascade, we introduce a Markov transformer, a variant of the popular\nfully autoregressive model that allows us to simultaneously decode with speciﬁc\nautoregressive context cutoffs. This approach requires only a small modiﬁcation\nfrom standard autoregressive training, while showing competitive accuracy/speed\ntradeoff compared to existing methods on ﬁve machine translation datasets.\n1 Introduction\nProbabilistic text generation is a ubiquitous tool in natural language processing. Originally primarily\nstudied with respect to machine translation [1, 27], its progress has led to applications in document\nsummarization [40, 45], data-to-text [60], image captioning [61], etc. State-of-the-art text generation\napproaches rely on fully autoregressive models such as RNNs and transformers [53], in which the\nprobability of an output word depends on all previous words. At inference time, beam search is\nused for decoding, a left-to-right serial procedure. To speed up decoding, researchers have proposed\nalternative parallel generation models. One class of non-autoregressive probabilistic models assumes\nthat each word’s output probability is independent of other words [13, 67, 28]. While it is impressive\nthat these models perform well, this independence assumption is very strong and often results in\nnoticeable artifacts such as repetitions [13, 51].\nWe note that non-autoregressive models, while sufﬁcient, are not necessary for fast probabilistic\nparallel generation. On parallel hardware, inference in models with bounded Markov dependencies is\ntrivial to parallelize and requires sub-linear time w.r.t. sequence length [43, 39]. In practice, given\nthe right parameterization, we can explore any level of autoregressive dependencies to achieve a\nspeed/accuracy tradeoff.\nIn this work, we exploit this property by proposing cascaded decoding with a Markov transformer\narchitecture. Our approach centers around a graphical model representation of the output space of\ntext generation. Given this model, we can employ cascaded decoding [7, 8, 58, 41] for parallel text\ngeneration, using an iterative procedure that starts from a non-autoregressive model and introduces\nincreasingly higher-order dependencies. We combine this approach with a Markov transformer, an\nextension to the fully autoregressive transformer architecture. This network uses barriers during\ntraining to ensure it learns ﬁxed high-order dependencies. At test time, a single network can be\nused to parameterize a cascade of different graphical models. The Markov transformer only changes\nself-attention masks and inputs at training, and is applicable to all transformer variants.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2006.01112v2  [cs.CL]  5 Dec 2020\nExperiments on ﬁve machine translation datasets compare this approach to other beam search and non-\nautoregressive baselines. Our inference approach is comparably fast to non-autoregressive methods\nwhile allowing for local dependencies in a principled, probabilistic way. Results validate the competi-\ntive accuracy/speed tradeoff of our approach compared to existing methods. The code for reproducing\nall results is available at https://github.com/harvardnlp/cascaded-generation.\n2 Related Work\nThere has been extensive interest in non-autoregressive/parallel generation approaches, aiming at\nproducing a sequence in parallel sub-linear time w.r.t. sequence length [13, 54, 26, 67, 55, 14, 11,\n12, 49, 15, 28, 16, 51, 57, 30, 42, 66, 64, 50]. Existing approaches can be broadly classiﬁed as latent\nvariable based [13, 26, 67, 28, 42], reﬁnement-based [25, 49, 14, 15, 11, 30, 12, 64] or a combination\nof both [42].\nLatent-variable approaches factor out the dependencies among output words, such that we can\ngenerate each word independently of each other conditioned on those latent variables. The training\nof these approaches usually employs variational autoencoders, since the log marginal is intractable\n[21, 38, 31]. The introduced latent variables enable generation in a single forward pass, achieving\nO(1) time complexity regardless of sequence length, but many of them suffer from generation\nartifacts such as repetitions [13]. While not using latent variables, our approach could be extended to\nincorporate them. A notable difference is that the parallel time complexity of this work is not O(1)\nbut O(log L) w.r.t. sequence length. In practice though, the only O(log L) part in our approach takes\na negligible fraction of total time [51], and our approach reaches comparable speedup compared to\nexisting approaches with O(1) time complexity.\nAnother line of research uses reﬁnement-based methods, where the model learns to iteratively reﬁne\na partially/fully completed hypothesis. Training usually takes the form of masked language modeling\n[11, 12] or imitating hand-crafted reﬁnement policies [25, 49, 15]. Reﬁnement-based approaches can\nsometimes reach better performance after multiple forward passes compared to latent variable based\napproaches which mostly use a single forward pass [ 15, 11, 42]. While our method superﬁcially\nresembles reﬁnement, our approach is probabilistic, model-based, and conceptually simpler. Training\nis by maximum likelihood, requires no hand-designed rules, and allows for activations to be preserved\nbetween iterations. A ﬁnal beneﬁt of our approach is that multiple lengths can be considered at no\nextra cost, as opposed to generating candidates under different lengths and reranking [11, 51, 28].\nOur approach is motivated by structured prediction cascades (SPC) [ 58]. SPC is a technique in\ngraphical models for graphical model type tasks, where we can specify the length of the sequence\nbeforehand [58]. To the best of our knowledge, we are the ﬁrst to adapt it to neural text generation.\nWe also go beyond SPC, which uses multiple models, and show how to adapt a single Markov\ntransformer model to learn the entire cascade. While [51] shares our motivation and combines a 0th\norder model with a 1st order graphical model, they do not consider higher-order models or cascades,\nor show how to achieve parallel sublinear time. In addition, we use a single Markov transformer to\nparameterize all log potentials, instead of using additional parameters for pairwise potentials.\n3 Cascaded Decoding for Conditional Random Fields\nNeural text decoding can be viewed as a conditional random ﬁeld (CRF) [ 24] over a sequence of\nwords x1:L, where xi ∈V with |V|= V, and X= VL is the set of all sequences. This model deﬁnes\na conditional probability distribution P(x1:L|c), where cis an arbitrary conditioning term, e.g., a\nsource sentence. Deﬁne an m-th (Markov) order CRF model as,\nP(m)(x1:L |c; θ) ∝exp\nL−m∑\nl=1\nf(m)\nl (xl:l+m,c; θ(m)),\nwhere f(m)\nl (·)’s are any parameterized log potentials looking at m+ 1 words, for example local\nlog-probabilities. For simplicity, we omit cand θ(m) through the rest of this paper. We can deﬁne two\nimportant special cases of this CRF model. With m= L−1, we can recover fully autoregressive neu-\nral text generation models such as RNNs and transformers. Using m= 0 gives us non-autoregressive\nmodels.\n2\n(a) m = 0\n (b) m = 1\n“this is actually” (c) m = 2\nFigure 1: Illustration of cascaded decoding (K = 10, iters = 4) for X1, X2, X3. The axes correspond\nto x1, x2 and x3. (a) 0th-order (non-autoregressive) model prunes unigrams to produce X1; (b)\n1st-order model prunes bigrams to Kper size-2 span (seen in 2D projection); (c) 2nd-order model\nprunes trigrams to Ktotal in size-3 span. Colors represent max-marginals MM(m)\nXm (x1:3), with pink\nbeing higher and blue being lower. Fixed limit Kallows for efﬁcient parallel (GPU) implementation.\nDecoding aims to ﬁnd the sequence with the highest model score, maxx′∈XP(m)(x′). Computing\nthis exactly can be done with the Viterbi algorithm in O(Vm+1L); however, even for m = 1 this\nis intractable since V is typically on the order of 104. Beam search is commonly used instead\nto approximate this value, but it cannot be parallelized, and alternatives to beam search remain\nunder-explored in the literature.\nWe propose an alternativecascaded decoding approach based on max-marginals [58], which are used\nas a metric to prune “unlikely” n-grams at each position based on the score of the “best” sequence\nwith a given n-gram. To be precise, deﬁne the notation X(xi:j) to be the set of sequences that contain\na span xi:j, i.e. {x′∈X : x′\ni:j = xi:j}. The max-marginal of xi:j is the maximum score in this set:\nMM(m)\nX (xi:j) =\n{ max\nx′∈X(xi:j)\nP(m)(x′\n1:L) X(xi:j) ̸= ∅\n0 o.w.\n.\nCascaded decoding, illustrated in Figure 1, proceeds by iteratively computing max-marginals for\nprogressively higher-order models while ﬁltering out unlikely spans. Starting with a complete initial\nset X0 = X, for all single word spans xl:l, we compute M(0)\nX0 and collect the top K max-marginal\nvalues at each step to prune the search space,\nX1 = {x1:L ∈X0 : xl:l ∈K arg max\nx′\nl:l∈V1\nMM(0)\nX0 (x′\nl:l) for all l}.\nWe then apply a 1st order model (m= 1) and collect the top K xl:l+1 values with the highest max\nmarginals M(1)\nX1 (xl:l+1) to further prune the search space,\nX2 = {x1:L ∈X1 : xl:l+1 ∈K arg max\nx′\nl:l+1∈V2\nMM(1)\nX1 (x′\nl:l+1) for all l}.\nWe repeat the above process M times with increasing m, and prune the search space to XM. It can\nbe shown that based on properties of max marginals this set is always non-empty [58]. We decode by\nﬁnding the sequence x1:L with the highest score P(M)(x1:L) in XM.\nImplementation The only non-parallel component of cascaded decoding is calculation of max-\nmarginals for m≥1. With m= 1, max-marginals xl:l+1 can be exactly computed using a variant of\nthe forward-backward algorithm. This algorithm requires O(K2L) time when performed serially.\nWe can reduce this complexity on parallel hardware by leveraging the commutative property ofmax\n[43, 39], and computing an inside-outside preﬁx sum. First we pad the sequence to a power of 2 and\nconstruct a balanced binary tree with words as leaves. We then perform max operations bottom-up\nand top down. The height of the tree dictates the parallel time of this approach, O(K2 log L). More\ndetails can be found in the TREE MM function in Algorithm 1, where Ci\nj,k1,k2 is the max possible\n3\nAlgorithm 1Parallel Cascaded Decoding\nGiven: max length L, limit K, log potentials f(m) for min {0,...,M }, parameters θ\nfunction CASCADE ( )\nfor m= 0 →M −1 do\nCompute potentials f(m)\nl (xl:l+m; θ) for all Xm(xl:l+m) ̸= ∅(K) ⊿O(K2)\nCompute ﬁrst-order state relabeling Φ(m)\nl for all positions l= 1 ...L −m ⊿O (K)\nCompute max-marginals MM(m)\nXm using TREE MM ⊿O(K2 log L)\nSet Xm+1 =\n{\nx1:L ∈Xm : xl:l+1 ∈ K arg max\nx′\nl:l+m∈Vm+1\nMM(m)\nXm (x′\nl:l+m) for all l\n}\n⊿O(K2)\nreturn arg maxx′∈XM P(M)(x′). ⊿O(K2 log L)\nfunction TREE MM(First-order scores C0\n···of size (L−1) ×K×K)\nAll Ci,Si,Pi size 2log(L−1)−i ×K×K, all j ∈{0 ... 2log(L−1)−i −1}\nPlog(L−1),Slog(L−1) ←0\nfor i= 0 →log(L−1) −2 do ⊿Chart max-scores computed bottom-up\nCi+1\nj·· ←maxkCi\n2j·k + Ci\n(2j+1)k·\nfor i= log(L−1) →1 do ⊿Preﬁx and sufﬁx MM scores computed top-down\nPi−1\n2j·· ←Pi\nj··; Pi−1\n2j+1··←maxkPi\nj·k + Ci−1\n2jk·\nSi−1\n2j+1··←Si\nj··; Si−1\n2j·· ←maxkCi−1\n(2j+1)·k + Si\njk·\nreturn exp[(maxkP0\njk·) + C0\nj··+ (maxkS0\nj·k)] ⊿O(K2 log L)\nscore of spans xj∗2i+1:(j+1)∗2i+1, with the constraint of the left end being word k1 and the right end\nbeing word k2. We compute Ci bottom-up, starting from i= 0 (C0 is the log potentials) and merging\nadjacent spans in Ci to get Ci+1. The preﬁx score Pi\nj,k1,k2 stores the max possible score of x1:j∗2i+1\n(also with end constraints), which we compute iteratively top-down using Pi+1 and Ci. Similarly,\nthe sufﬁx score Si\nj,k1,k2 is the max score of x(j+1)∗2i+1: computed top-down. Finally, we combine\nthe preﬁx scores P0, sufﬁx scores S0, and log potentials C0 to calculate max marginals of any edge.\nFor higher-order models with m> 1, we can compute max-marginals for xl:l+m using a reduction to\nan m= 1 CRF. By construction, Xm has exactly K spans xl:l+m such that X(xl:l+m) ̸= ∅for all\npositions l. We relabel these spans xl:l+m as 1 ...K for each position, using a mapping Φ(m)\nl . This\nmapping implies that there are at max K2 transitions between Φ(m)\nl (xl:l+m) to Φ(m)\nl+1 (xl+1:l+m+1),\nresembling an m= 1 model over Φ. Therefore, the total parallel computation cost is O(K2 log L).\nThe full procedure is given in Algorithm 1. As opposed to O(VM+1 log L) of exact search, the\ncascaded approximation can be computed in parallel in O(MK2 log L). We note that this yields a\nsub-linear time yet (partially) autoregressive decoding algorithm.\nHandling Length A common issue in parallel generation is the need to specify the length of the\ngeneration beforehand [13, 28]. It is hard to predict the exact length and constraining search with\nstrict length limits the maximum achievable score. We can relax the length constraint by considering\nmultiple lengths simultaneously. We introduce a special padding symbol pad to Vat inference time,\nand add log-potentials to force pad and end-of-sentence tokens eos to transition to pad. Candidate\nsequences of different lengths are padded to the same length, but trailing pad’s do not affect scores.\nThe CRF parameterization allows us to consider all these lengths simultaneously, where extending\nthe length only introduces log additional time. More details can be found at supplementary materials.\n4 Model Parameterization: Markov Transformer\nThe cascaded decoding approach can be applied to any cascades of CRF models that obey the\nproperties deﬁned above, i.e., m-th order log-potentials. Given a training set (cj,xj)1:J we would\n4\nx 1 x 2 x 4 x 5 x 7 x 8✏ ✏ ✏\nf\n(2)\n7 ( · )f\n(1)\n7 ( · )f\n(0)\n7 ( · )f\n(0)\n4 ( · ) f\n(1)\n4 ( · ) f\n(2)\n4 ( · )f\n(0)\n1 ( · ) f\n(1)\n1 ( · ) f\n(2)\n1 ( · )\n(a)\nx l x l +1\nX 1\nX 2\nX 3\n✏\nf\n(0)\nl ( · ) f\n(1)\nl ( · ) f\n(2)\nl ( · ) (b)\nFigure 2: Markov transformer with M = 2 and L= 9. (a) At training, model state is reset with a\nbarrier every M + 1 words. (b) At decoding, potential f(0)\nl is computed at each position to get X1,\nand the dependency order is increased by introducing more columns to compute X2 and X3.\nlike M + 1 different parameters that satisfy the following MLE objectives:\nθ(m) = arg max\nθ(m)\n∑\nj\nlog P(m)(xj\n1:L |cj; θ(m)) for all m∈{0,...M }\nNaive approaches for cascading would require training M + 1 different models that are calibrated or\ntrained together to produce similar outputs [58]. These also cannot be standard translation models\nsuch as RNNs or transformers [18, 52, 53], since they have m= L−1.\nWe propose a training and modeling strategy to ﬁx both of these issues. First, to reduce from M + 1\nmodels to 1, we rewrite the above objective in the form:\n(θ(0),...,θ (M)) = arg max\nθ(0)...θ(M)\n1\nM + 1\nM∑\nm=0\n∑\nj\nlog P(m)(xj\n1:L |cj; θ(m))\nWe then make simplifying assumptions that we only want one set of model parameters θand that the\nMarkov order mis sampled through training:\nθ= arg max\nθ\nEm\n∑\nj\nlog P(m)(xj\n1:L |cj; θ)\nIn order to approximate this sampling, we train θ by starting with an autoregressive model and\nresetting the model’s state everyM+1 words with a hard barrier. The ﬁrst barrier is placed uniformly\nat random from words 1 to M + 1.\nNext, we need a model that can be trained under this hard constraint in order to parameterize f(m)\nl (·).\nWe propose a variant of the transformer, which we call the Markov transformer (Figure 2), that\ncan satisfy the necessary properties. The model is trained with (M + 1)-spaced reset barriers with\nthe constraint that self-attention does not cross those barriers. Transformer is particularly suited to\nlearning with this constraint, given that it has positional encodings that encode leven with explicit\nbarriers. In order to ensure that the model can parameterize P(0), i.e., the prediction immediately\nafter the barrier, we replace the ﬁrst input word by a special token ϵ.\nTo perform cascaded decoding, we simply start the computation off(0)\nl at each position l. A beneﬁt of\nusing a single model is that we can reuse the transformer state (neural activations) between iterations,\ni.e., for f(m)\nl (xl:l+m) we can reuse the cached states from f(m−1)\nl (xl:l+m−1). We use the output of\nthe transformer as the log-potentials. This means each log-potential requires computing one column\nof the transformer, with length mself-attention, requiring O(mK) parallel time per iteration.\n5\n5 Experiments\nDatasets We evaluate our approach on ﬁve commonly used machine translation benchmark datasets:\nIWSLT14 De-En [6] (∼160k parallel sentences), WMT14 En-De/De-En1 [29] (∼4M parallel sen-\ntences) and WMT16 En-Ro/Ro-En2 [3] (∼610k parallel sentences). To process the data, we use Byte\nPair Encoding (BPE) [46, 23] learned on the training set with a shared vocabulary between source and\ntarget. For IWSLT14 the vocabulary size is 10k; for WMT14 the vocabulary size 40k. For WMT16\nwe use the processed data provided by [25]. We sample all validation datasets to be at most 3k.\nModel SettingsMarkov transformer uses the same hyperparameters as standard transformers. The\nbase settings are from FAIRSEQ3 [34]: For IWSLT14 De-En, we use 6 layers, 4 attention heads,\nmodel dimension 512, hidden dimension 1024; for WMT14 En-De/De-En and WMT16 En-Ro/Ro-En\nwe use 6 layers, 8 attention heads, model dimension 512, hidden dimension 2048. We tie the decoder\noutput projection matrix on all datasets [36], and we share source and target embeddings on WMT14\nEn-De/De-En and WMT16 En-Ro/Ro-En. It differs only in the application of attention barriers,\nwhere we set M = 4. The optimization settings can be found at supplementary materials.\nAt generation time, we predict the length Lusing linear regression based on source length. We\nconsider hypotheses of length L−∆Lto L+ ∆Lwhere we vary ∆Lfrom 0 to 5. Since the Markov\ntransformer was trained with M = 4, we consider applying cascaded decoding for 2 to 5 iterations\n(2 iterations corresponds to M = 1 in Algorithm 1), where more iterations consider higher local\ndependency orders at the cost of more computations. The limit Kis chosen from 16, 32, 64, 128.\nBaselines For the fully autoregressive baseline, we use the same model setting and use beam size\n5. We also compare to other parallel generation methods. These include a latent variable approach:\nFlowSeq [28]; reﬁnement-based approaches: CMLM [11], Levenshtein transformer [15] and SMART\n[12]; a mixed approach: Imputer [ 42]; reinforcement learning: Imitate-NAT [ 57]; and another\nsequence-based approach: NART-DCRF [51] which combines a non-autoregressive model with a\n1st-order CRF. Several of these methods use fully autoregressive reranking [13], which generally\ngives further improvements but requires a separate test-time model.\nEvaluation We evaluate the BLEU score of different approaches. Following prior works [28, 51, 66],\nwe use tokenized cased BLEU for WMT14 En-De/De-En and tokenized uncased BLEU for IWSLT14\nDe-En and WMT16 En-Ro/Ro-En, after removing BPE. We measure the average decoding time of a\nsingle sentence [13, 25, 16, 15, 55, 51] on a 12GB Nvidia Titan X GPU.\nExtension Knowledge distillation [17, 19, 65] is a commonly used technique to improve the perfor-\nmance of parallel generation [13, 25, 28]. In knowledge distillation, we translate the training set using\na fully autoregressive transformer and use the translated sentences as the new target for training.\n5.1 Results\nResults are presented in Table 1. We show the tradeoff between speedup and BLEU score by\nﬁnding the conﬁguration that gives the best BLEU score with more than 1×, 2×, ... , 7×validation\nspeedup. We presented our results in terms of the number of iterations, which is equal to M + 1, for\ncomparability to reﬁnement-based approaches.\nUsing knowledge distillation, our results get close to the fully autoregressive baseline: on WMT14\nEn-De, the gap between our approach and transformer is 0.5 BLEU, while being2.4×faster (K = 32,\niters=5). Our results are also competitive to previous works, even those using a reranker. For example,\non WMT14 En-De, we can get 26.52 BLEU score at a 4.68×speedup, compared to NART-DCRF\nthat reaches 26.80 BLEU at a 4.39×speedup using 19 candidate sentences to rerank. On IWSLT14,\nour BLEU scores are much better than previous works: we can reach within 0.54 BLEU score\ncompared to transformer at a 5.88×speedup (K = 16, iters=2), 6 BLEU points better than FlowSeq.\nOur approach is also competitive against previous works without distillation: at a speedup of 2.06×,\nwe achieved a better BLEU score than FlowSeq-large using 30 candidates to rerank, which also has\nmany more parameters (66M vs. 258M excluding the reranker). The one model that outperforms our\napproach is the Levenshtein Transformer. We note though that this model requires hand-crafted rules\n1http://www.statmt.org/wmt14/translation-task.html\n2http://www.statmt.org/wmt16/translation-task.html\n3https://github.com/pytorch/fairseq/tree/master/examples/translation\n6\nTable 1: Main results. †: latency numbers not directly comparable due to platform differences.\nApproach Latency (Speedup)\nWMT14 En-De\nWMT14 WMT16 IWSLT14\nModel Settings En-De De-En En-Ro Ro-En De-En\nTransformer (beam 5) 318.85ms ( ×1.00) 27.41 31.49 33.89 33.82 34.44\nWith Distillation\nCascaded Generation with Speedup\n>×7 (K=16, iters=2) 50.28ms ( ×6.34) 26.34 30.69 32.70 32.66 33.90\n>×6/5 (K=32, iters=2) 52.93ms ( ×6.02) 26.43 30.72 32.73 32.70 34.01\n>×4 (K=64, iters=2) 68.09ms ( ×4.68) 26.52 30.73 32.77 32.76 34.02\n>×3 (K=32, iters=4) 107.14ms ( ×2.98) 26.80 31.22 33.14 33.22 34.43\n>×2 (K=32, iters=5) 132.64ms ( ×2.40) 26.90 31.15 33.08 33.13 34.43\n>×1 (K=64, iters=5) 189.96ms ( ×1.68) 26.92 31.23 33.23 33.28 34.49\nLiterature\nFlowSeq-base [28] - 21.45 26.16 29.34 30.44 27.55\nFlowSeq-large [28] - 23.72 28.39 29.73 30.72 -\nBase CMLM[11] (iters=10) - 27.03 30.53 33.08 33.31 -\nLevenshtein [15] 92ms ( ×4.01)† 27.27 - - 33.26 -\nSMART [12] (iters=10) - 27.65 31.27 - - -\nImputer [42] (iters=1) - 25.8 28.4 - - -\nimitate-NAT [57] - ( ×18.6)† 22.44 25.67 28.61 28.90 -\nNART-DCRF [51] 37ms ( ×10.4)† 23.44 27.22 27.44 - -\nLiterature+Reranking\nFlowSeq-large [28] (rescoring=30) - 25.31 30.68 - - -\nBase CMLM [11] (iters=4, rescoring 2) - ( ×3.0-3.1)† 25.6-25.7 - - - -\nimitate-NAT [57] (rescoring=7) - ( ×9.70)† 24.15 27.28 31.45 31.81 -\nNART-DCRF [51] (rescoring=9) 63ms ( ×6.14)† 26.07 29.68 29.99 - -\nNART-DCRF [51] (rescoring=19) 88ms ( ×4.39)† 26.80 30.04 30.36 - -\nWithout Distillation\nCascaded Generation with Speedup\n>×7 (K=16, iters=2) 47.05ms ( ×6.78) 21.34 26.91 32.11 32.53 32.95\n>×6/5 (K=32, iters=2) 54.36ms ( ×5.87) 22.55 27.56 32.62 32.44 33.14\n>×4 (K=64, iters=2) 69.19ms ( ×4.61) 23.09 27.79 32.78 32.43 33.25\n>×3 (K=32, iters=3) 78.29ms ( ×4.07) 23.35 28.64 33.12 33.11 33.74\n>×2/1 (K=64, iters=4) 154.45ms ( ×2.06) 24.40 29.43 33.64 33.19 34.08\nLiterature\nFlowSeq-base [28] - 18.55 23.36 29.34 30.44 24.75\nFlowSeq-large [28] - 20.85 25.40 29.73 30.72 -\nLevenshtein [15] 126ms ( ×2.93)† 25.20 - - 33.02 -\nLiterature+Reranking\nFlowSeq-large [28] (rescoring=30) - 23.64 28.29 32.20 32.84 -\nfor training, and uses global communication, while our approach is probabilistic and only requires\ncommunicating log potentials between adjacent positions.\n5.2 Analysis\nCandidates SearchedUnlike beam search, which is limited to a ﬁxed number (KL) of candidates,\ncascaded search can explore an exponential number of sequences [63]. Figure 3 (a) shows the number\nof candidate sequences scored by cascaded decoding (f(2), f(3), f(4)) and beam search (f(L−1)\nAR ). We\nadditionally note that max-marginal computations are in practice extremely fast relative to transformer\ncomputation and take less than 1% of the total time, so the bottleneck is computing potentials.\nVariable Length GenerationCascaded decoding allows for relaxing the length constraint. Figure 3\n(b) shows the effect of varying ∆Lfrom {0,3,5}, where ∆L = 0 corresponds to a hard length\nconstraint, and ∆L= 3 sequences of 7 possible length values fromL−3 to L+3. By using ∆L= 3,\nwe get more than 1 BLEU improvement at any given speedup. Therefore, we use∆L= 3 for Table 1.\nRatio of RepetitionsThe independence assumption of non-autoregressive models often leads to\nvisible artifacts in generation such as n-gram repetitions. By introducing higher-order dependencies,\nwe can reduce the ratio of repetitions, as shown in Figure 3 (c), where we measure the extent of\n7\n(a)\n (b)\n (c)\nFigure 3: Analysis on WMT14 En-De val. (a) Box plot of the number of candidate sequences at\ndifferent dependency orders with K = 16. Results include cascaded decoding with 3 iterations\n(scored with f(2)), 4 iterations ( f(3)) and 5 iterations ( f(4)), and beam baseline ( f(L−1)\nAR ). (b)\nBLEU/speedup tradeoff as we vary ∆L. The plot is drawn by varying Kfrom {16,32,64,128}and\nvarying iterations from {2,3,4,5}. (c) The ratio of n-gram repetitions evaluated using the ratio of\nunique n-grams as a proxy (K = 16, ∆L= 0).\nTable 2: Markov transformer with different search strategies on IWSLT14 De-En val w/o distillation.\nColumn ∆Lshows the length constraint (L−∆Lto L+ ∆L), where None denotes no constraint.\nModel Search Parallel Time ∆L Model Score BLEU\nTransformer [53] Beam (K= 5) N O(KL2) None -11.82 35.63\nMarkov Trans.\nBeam (K=5) N O(KML ) None -12.05 35.07\nBeam (K=64) N - 0 -17.79 33.14\nBeam (K=1024) N - 0 -16.77 33.33\nCascade (K=64, iters=5) Y - 0 -17.44 33.45\nCascade (K=64, iters=5) Y - 3 -13.87 35.03\nrepetitions using the ratio of unique n-grams [59]. Cascaded decoding with more than 1 iterations\nsigniﬁcantly reduces the number of repetitions.\nMarkov Transformer AnalysisTable 2 shows different search algorithms for the Markov trans-\nformer. We can observe that 1) a 4th-order Markov transformer is very expressive by itself: using\nbeam search with K = 5, the BLEU score (35.07) is close to the BLEU score of a transformer (35.63);\n2) Cascaded decoding is less effective without distillation than serial beam search; 3) With length\nconstraint, cascaded decoding is more effective than beam search; 4) Variable length generation can\nimprove upon enforcing strict length constraints. Finally, we want to note that Markov transformer’s\ncomplexity is lower than normal transformer, since it attends to at most M past words.\nMulti-GPU Scaling on multiple GPUs is becoming more important, given the recent trend in bigger\nmodels [47, 5]. For multi-GPU parallelization4, each GPU takes a chunk of the sequence and forwards\ndecoder for that chunk, while each GPU maintains full encoder states. The only communications\nbetween GPUs are the log potentials of size L×K×K at each iteration. By using 4 GPUs, our\napproach can reach speedup of 2.79×compared to 1.68×using only 1 GPU when K = 64 and\niters = 5 on WMT14 En-De test set with distillation. Note that we use batch size 1, while for most\nother approaches due to the global communication required between different parts of the target\nsentence, it is hard to reach this level of parallelism.\nMax-Marginals To prune “unlikely” n-grams at each position, we used max-marginals instead of\nn-gram scores. The problem with using n-gram scores is that they do not consider compatibility\nwith other positions. Max-marginal ﬁxes this issue with negligible extra time. On WMT14 En-De\nvalidation set, using n-gram scores would get a BLEU score of 28.42 at 123.48ms, while using\nmax-marginals reaches 29.24 at 128.58ms (iters = 5, K = 32, ∆L= 3).\n4We use https://pytorch.org/docs/stable/multiprocessing.html.\n8\n6 Conclusion\nWe demonstrate that probabilistic autoregressive models can achieve sub-linear decoding time while\nretaining high ﬁdelity translations by replacing beam search with a cascaded inference approach. Our\napproach, based on [58], iteratively prunes the search space using increasingly higher-order models.\nTo support this inference procedure, we utilize Markov transformers, a variant of transformer that can\nparameterize cascades of CRFs. Experiments on ﬁve commonly used machine translation benchmark\ndatasets validate that our approach is competitive in terms of accuracy/speed tradeoff with other\nstate-of-the-art parallel decoding methods, and practically useful with distillation.\nOur work opens up a number of exciting future directions, such as applying this approach to longer-\nform text generation using latent variables, extending the Markov transformer to mimic any speciﬁed\ngraphical model, or using more powerful globally normalized energy models instead of locally\nnormalized ones.\nBroader Impact\nOur work proposes an alternative approach to beam search that enables more efﬁcient text generation.\nThis work primarily uses machine translation as an application, but in the long run, it might be applied\nto longer-form text generation such as summarizing or translating entire documents, or be deployed\nto edge devices due to its faster inference and lower computational costs.\nOn the positive side, more efﬁcient text generation can make these technologies more accessible\nto the general public. For example, machine translation can help overcome language barriers [37];\ndocument summarization makes data more interpretable [ 33]. However, there are potential risks.\nFaster text generation has provoked concerns about generating fake news and targeted propaganda\n[56, 9] and might pose safety concerns if it was used to generate hate speech or to harass people [48].\nAnother potential problem is that it might generate language that appears ﬂuent but fabricates facts\n[22].\nTo mitigate those issues, there have been works trying to detect machine-generated text [10, 62, 2].\nWhile these works address some concerns over the abuse of text generation, we should be cautious\nthat fake news detection is still a mostly unsolved technical problem and requires active future\nresearch [44, 4] as well as non-technical mitigation efforts.\nAcknowledgments and Disclosure of Funding\nWe would like to thank Justin Chiu, Demi Guo, Yoon Kim, David Rosenberg, Zachary Ziegler, and\nJiawei Zhou for helpful feedback. This project was supported by NSF SHF 1704834, CAREER\nIIS-1845664, and Intel. YD is supported by a Baidu AI fellowship.\nReferences\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473, 2014.\n[2] Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc’Aurelio Ranzato, and Arthur Szlam. Real or\nfake? learning to discriminate machine from human generated text. arXiv preprint arXiv:1906.03351,\n2019.\n[3] Ondˇrej Bojar, Yvette Graham, Amir Kamran, and Miloš Stanojevi´c. Results of the wmt16 metrics shared\ntask. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages\n199–231, 2016.\n[4] Alessandro Bondielli and Francesco Marcelloni. A survey on fake news and rumour detection techniques.\nInformation Sciences, 497:38–55, 2019.\n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners, 2020.\n9\n[6] Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa Bentivogli, and Marcello Federico. Report on the\n11th iwslt evaluation campaign, iwslt 2014. In Proceedings of the International Workshop on Spoken\nLanguage Translation, Hanoi, Vietnam, volume 57, 2014.\n[7] Eugene Charniak and Mark Johnson. Coarse-to-ﬁne n-best parsing and maxent discriminative reranking.\nIn Proceedings of the 43rd annual meeting on association for computational linguistics, pages 173–180.\nAssociation for Computational Linguistics, 2005.\n[8] Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil, David Ellis, Isaac Haxton, Catherine\nHill, R Shrivaths, Jeremy Moore, Michael Pozar, et al. Multilevel coarse-to-ﬁne pcfg parsing. In\nProceedings of the main conference on Human Language Technology Conference of the North American\nChapter of the Association of Computational Linguistics, pages 168–175. Association for Computational\nLinguistics, 2006.\n[9] Robert Faris, Hal Roberts, Bruce Etling, Nikki Bourassa, Ethan Zuckerman, and Yochai Benkler. Partisan-\nship, propaganda, and disinformation: Online media and the 2016 us presidential election. Berkman Klein\nCenter Research Publication, 6, 2017.\n[10] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. Gltr: Statistical detection and visualization\nof generated text. arXiv preprint arXiv:1906.04043, 2019.\n[11] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Constant-time machine translation\nwith conditional masked language models. arXiv preprint arXiv:1904.09324, 2019.\n[12] Marjan Ghazvininejad, Omer Levy, and Luke Zettlemoyer. Semi-autoregressive training improves mask-\npredict decoding. arXiv preprint arXiv:2001.08785, 2020.\n[13] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural\nmachine translation. arXiv preprint arXiv:1711.02281, 2017.\n[14] Jiatao Gu, Qi Liu, and Kyunghyun Cho. Insertion-based decoding with automatically inferred generation\norder. Transactions of the Association for Computational Linguistics, 7:661–676, 2019.\n[15] Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. In Advances in Neural Information\nProcessing Systems, pages 11179–11189, 2019.\n[16] Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and Tie-Yan Liu. Non-autoregressive neural machine\ntranslation with enhanced decoder input. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 3723–3730, 2019.\n[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\n[18] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\n1997.\n[19] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation.arXiv preprint arXiv:1606.07947,\n2016.\n[20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[21] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In Proceedings of ICLR, 2014.\n[22] Wojciech Kry´sci´nski, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the factual\nconsistency of abstractive text summarization. arXiv, pages arXiv–1910, 2019.\n[23] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\n[24] John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random ﬁelds: Probabilistic\nmodels for segmenting and labeling sequence data. 2001.\n[25] Jason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-autoregressive neural sequence\nmodeling by iterative reﬁnement. arXiv preprint arXiv:1802.06901, 2018.\n[26] Jindˇrich Libovick`y and Jindˇrich Helcl. End-to-end non-autoregressive neural machine translation with\nconnectionist temporal classiﬁcation. arXiv preprint arXiv:1811.04719, 2018.\n[27] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based\nneural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n[28] Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy. Flowseq: Non-autoregressive\nconditional sequence generation with generative ﬂow. arXiv preprint arXiv:1909.02480, 2019.\n[29] Matouš Macháˇcek and Ondˇrej Bojar. Results of the wmt14 metrics shared task. In Proceedings of the\nNinth Workshop on Statistical Machine Translation, pages 293–301, 2014.\n[30] Elman Mansimov, Alex Wang, and Kyunghyun Cho. A generalized framework of sequence generation\nwith application to undirected sequence models. arXiv preprint arXiv:1905.12790, 2019.\n10\n[31] Andriy Mnih and Karol Gregor. Neural Variational Inference and Learning in Belief Networks. In\nProceedings of ICML, 2014.\n[32] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? In Advances\nin Neural Information Processing Systems, pages 4696–4705, 2019.\n[33] Ephraim Nissan. Digital technologies and artiﬁcial intelligence’s present and foreseeable impact on\nlawyering, judging, policing and law enforcement. Ai & Society, 32(3):441–464, 2017.\n[34] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038,\n2019.\n[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library. In Advances in Neural Information Processing Systems, pages 8024–8035, 2019.\n[36] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint\narXiv:1608.05859, 2016.\n[37] Georg Rehm. Cracking the language barrier for a multilingual europe.\n[38] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approxi-\nmate Inference in Deep Generative Models. In Proceedings of ICML, 2014.\n[39] Alexander M Rush. Torch-struct: Deep structured prediction library. arXiv preprint arXiv:2002.00876,\n2020.\n[40] Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence\nsummarization. arXiv preprint arXiv:1509.00685, 2015.\n[41] Alexander M Rush and Slav Petrov. Vine pruning for efﬁcient multi-pass dependency parsing. In\nProceedings of the 2012 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 498–507. Association for Computational Linguistics,\n2012.\n[42] Chitwan Saharia, William Chan, Saurabh Saxena, and Mohammad Norouzi. Non-autoregressive machine\ntranslation with latent alignments. arXiv preprint arXiv:2004.07437, 2020.\n[43] Simo Särkkä and Ángel F García-Fernández. Temporal parallelization of bayesian ﬁlters and smoothers.\narXiv preprint arXiv:1905.13002, 2019.\n[44] Tal Schuster, Roei Schuster, Darsh J Shah, and Regina Barzilay. Are we safe yet? the limitations of\ndistributional features for fake news detection. arXiv preprint arXiv:1908.09805, 2019.\n[45] Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-\ngenerator networks. arXiv preprint arXiv:1704.04368, 2017.\n[46] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909, 2015.\n[47] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv\npreprint arXiv:1909.08053, 2019.\n[48] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec Radford,\nand Jasmine Wang. Release strategies and the social impacts of language models. arXiv preprint\narXiv:1908.09203, 2019.\n[49] Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. Insertion transformer: Flexible sequence\ngeneration via insertion operations. arXiv preprint arXiv:1902.03249, 2019.\n[50] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive\nmodels. In Advances in Neural Information Processing Systems, pages 10086–10095, 2018.\n[51] Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He, Zi Lin, and Zhihong Deng. Fast structured decoding for\nsequence models. In Advances in Neural Information Processing Systems, pages 3011–3020, 2019.\n[52] Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. Lstm neural networks for language modeling. In\nThirteenth annual conference of the international speech communication association, 2012.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing\nsystems, pages 5998–6008, 2017.\n[54] Chunqi Wang, Ji Zhang, and Haiqing Chen. Semi-autoregressive neural machine translation.arXiv preprint\narXiv:1808.08583, 2018.\n11\n[55] Yiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Non-autoregressive machine\ntranslation with auxiliary regularization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 5377–5384, 2019.\n[56] Claire Wardle and Hossein Derakhshan. Information disorder: Toward an interdisciplinary framework for\nresearch and policy making. Council of Europe report, 27, 2017.\n[57] Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, and Xu Sun. Imitation learning for non-\nautoregressive neural machine translation. arXiv preprint arXiv:1906.02041, 2019.\n[58] David Weiss and Benjamin Taskar. Structured prediction cascades. In Proceedings of the Thirteenth\nInternational Conference on Artiﬁcial Intelligence and Statistics, pages 916–923, 2010.\n[59] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text\ngeneration with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.\n[60] Sam Wiseman, Stuart M Shieber, and Alexander M Rush. Challenges in data-to-document generation.\narXiv preprint arXiv:1707.08052, 2017.\n[61] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In\nInternational conference on machine learning, pages 2048–2057, 2015.\n[62] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin\nChoi. Defending against neural fake news. In Advances in Neural Information Processing Systems, pages\n9051–9062, 2019.\n[63] Wen Zhang, Liang Huang, Yang Feng, Lei Shen, and Qun Liu. Speeding up neural machine translation\ndecoding by cube pruning. arXiv preprint arXiv:1809.02992, 2018.\n[64] Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, and Bill Dolan. Pointer: Constrained\ntext generation via insertion-based generative pre-training. arXiv preprint arXiv:2005.00558, 2020.\n[65] Chunting Zhou, Graham Neubig, and Jiatao Gu. Understanding knowledge distillation in non-\nautoregressive machine translation. arXiv preprint arXiv:1911.02727, 2019.\n[66] Jiawei Zhou and Phillip Keung. Improving non-autoregressive neural machine translation with monolingual\ndata. arXiv preprint arXiv:2005.00932, 2020.\n[67] Zachary M Ziegler and Alexander M Rush. Latent normalizing ﬂows for discrete sequences. arXiv preprint\narXiv:1901.10548, 2019.\n12\nSupplementary Materials for\nCascaded Text Generation with Markov Transformers\nAppendix A: Cascaded Decoding Examples\nWe show a decoding example in Table 3 (K = 5, ∆L= 1, iters=5). We sort states by max-marginals\nin descending order and use - to denote invalid states (with −∞log max-marginals). In this simple\nsentence, using 1 iteration (m= 0, non-autoregressive model) repeats the word “woman” (m= 0,\nﬁrst row, x4:4+m). Introducing higher order dependencies ﬁxes this issue.\nTable 3: Cascaded Decoding Example. When m= 4, Viterbi in X4 returns “an amazing woman .\neos”. The source is “eine erstaunliche frau . eos” and the target is “an amazing woman . eos”.\nm x1:1+m x2:2+m x3:3+m x4:4+m x5:5+m x6:6+m x7:7+m x8\n0\nan amazing woman woman eos eos eos pad\namazing woman amazing . . pad pad -\nincredible an an amazing woman . . -\nthis remarkable . eos amazing woman woman -\nremarkable incredible women an women women women -\n1\nan amazing amazing woman woman . . eos eos pad pad pad pad pad\nan incredible incredible woman amazing woman woman . . eos eos pad eos pad\nthis amazing remarkable woman women . amazing woman woman . . eos -\nan remarkable woman amazing woman woman . . women . woman eos -\namazing woman amazing women an amazing . woman . . - -\n2\nan amazing woman amazing woman . woman . eos . eos pad eos pad pad pad pad pad\nan incredible woman incredible woman . women . eos woman . eos . eos pad eos pad pad\nthis amazing woman remarkable woman . woman woman . . . eos woman . eos . eos pad\nan remarkable woman amazing women . woman . . . woman . . . eos -\nan amazing women amazing woman woman woman . woman woman . . - -\n3\nan amazing woman . amazing woman . eos woman . eos pad . eos pad pad eos pad pad pad\nan incredible woman . incredible woman . eos women . eos pad woman . eos pad. eos pad pad\nthis amazing woman . remarkable woman . eos woman woman . eos . . eos pad woman . eos pad\nan remarkable woman . amazing women . eos woman . . eos . woman . eos . . eos pad\nan amazing women . amazing woman woman . woman . woman . woman . . eos -\nTable 4: Cascaded Decoding Example. When m= 4, Viterbi in X4 returns “what has happened ?\neos”. The source is “was ist passiert ? eos” and the target is “what happened ? eos”.\nm x1:1+m x2:2+m x3:3+m x4:4+m x5:5+m x6:6+m x7:7+m x8\n0\nwhat happened happened ? eos eos eos pad\nso has ? eos ? pad pad -\nnow did what happened happened ? ? -\nand what happen happen happen happened . -\nwell ’s eos happens happens . happened -\n1\nwhat has has happened happened ? ? eos eos pad pad pad pad pad\nso what what happened what happened happened ? ? eos eos pad eos pad\nand what ’s happened happen ? happens ? happens ? ? eos -\nwhat ’s did what what ? happen ? happen ? . eos -\nnow what did happened what happens ? ? happened ? happened eos -\n2\nwhat has happened has happened ? happened ? eos ? eos pad eos pad pad pad pad pad\nso what happened what happened ? happened ? ? ? ? eos ? eos pad eos pad pad\nwhat ’s happened ’s happened ? happen ? ? happen ? eos happened ? eos happened eos pad\nand what happened did what ? happen ? eos happens ? eos happen ? eos . eos pad\nnow what happened did what happened what happened ? happened ? eos happens ? eos ? eos pad\n3\nwhat has happened ? has happened ? eos happened ? eos pad ? eos pad pad eos pad pad pad\nso what happened ? what happened ? eos happened ? ? eos ? ? eos pad ? eos pad pad\nand what happened ? ’s happened ? eos what happened ? eos happened ? eos padhappens ? eos pad\nwhat ’s happened ? has happened ? ? happen ? eos pad happens ? eos pad happen ? eos pad\nnow what happened ? what happened ? ? happen ? ? eos happen ? eos pad happened ? eos pad\nIn Tables 4, 5, 6, 7, 8 we show more examples from IWSLT14 De-En val.\n13\nTable 5: Cascaded Decoding Example. When m= 4, Viterbi in X4 returns “you ’re happy .eos”.\nThe source is “du bist glücklich . eos” and the target is “you ’re happy .eos”.\nmx1:1+m x2:2+m x3:3+m x4:4+m x5:5+m x6:6+m x7:7+m x8\n0\nyou ’re happy . eos eos eos pad\nhappy are lucky eos . pad pad -\nyour you gla@@ happy happy . . -\nand ’s good lucky ? happy happy -\ni be fortun@@ ful you ? ? -\n1\nyou ’re ’re happy happy . . eos eos pad pad pad pad pad\nyou are are happy lucky . . . . eos eos pad eos pad\nyou be are lucky good . happy . happy . . eos -\nyou ’s be happy happy happy ful . ? eos ? eos -\nand you ’re lucky happy ful lucky . you . happy eos -\n2\nyou ’re happy ’re happy . happy . eos . eos pad eos pad pad pad pad pad\nyou are happy are happy . lucky . eos . . eos . eos pad eos pad pad\nyou be happy be happy . happy . . happy . eos you . eos happy eos pad\nyou ’re lucky ’re lucky . happy happy . ful . eos ? eos pad ? eos pad\nyou are lucky are lucky . happy ful . lucky . eos happy . eos . eos pad\n3\nyou ’re happy . ’re happy .eos happy . eos pad . eos pad pad eos pad pad pad\nyou are happy . are happy . eos lucky . eos pad . . eos pad . eos pad pad\nyou be happy . be happy . eos happy . . eos lucky . eos pad happy . eos pad\nyou ’re lucky . ’re lucky . eos happy ful . eos ful . eos pad ? eos pad pad\nyou are lucky . are lucky . eos happy happy . eos happy . eos padyou . eos pad\nTable 6: Cascaded Decoding Example. When m= 4, Viterbi in X4 returns “let ’s move .eos”. The\nsource is “bewe@@ g dich . eos” and the target is “move it .eos”.\nmx1:1+m x2:2+m x3:3+m x4:4+m x5:5+m x6:6+m x7:7+m x8\n0\nmove move . eos eos eos eos pad\nlet . eos . . pad pad -\nso moving move ? ? . . -\njust ’s forward forward here ? ? -\nnow let moving it forward here here -\n1\nlet ’s ’s move move . . eos eos pad pad pad pad pad\njust move ’s moving moving . it . . eos eos pad eos pad\nso move move forward move it forward . here . . eos -\nmove . . forward move forward ? eos ? eos ? eos -\nmove ’s . moving move ? . . forward . - -\n2\nlet ’s move ’s move . move . eos . eos pad eos pad pad pad pad pad\nlet ’s moving ’s move it move it . it . eos . eos pad eos pad pad\nmove ’s move ’s move forward move forward . forward . eos ? eos pad ? eos pad\nmove . moving ’s moving . moving . eos ? eos pad here . eos . eos pad\nmove ’s moving ’s move ? move ? eos . . eos - -\n3\nlet ’s move . ’s move . eos move . eos pad . eos pad pad eos pad pad pad\nlet ’s move it ’s move it . move it . eos it . eos pad . eos pad pad\nlet ’s moving . ’s moving . eos moving . eos pad forward . eos padhere . eos pad\nlet ’s move forward ’s move forward . move forward .eos ? eos pad pad ? eos pad pad\nlet ’s move ? ’s move ? eos move ? eos pad . . eos pad -\n14\nTable 7: Cascaded Decoding Example. When m= 4, Viterbi in X4 returns “very , very hard . eos”.\nThe source is “sehr sehr schwer . eos” and the target is “very very hard .eos”.\nmx1:1+m x2:2+m x3:3+m x4:4+m x5:5+m x6:6+m x7:7+m x8\n0\nvery difﬁcult difﬁcult . eos eos eos pad\nit hard hard eos . pad pad -\nreally very . difﬁcult difﬁcult . . -\nextremely tough very hard hard difﬁcult difﬁcult -\nthat , tough very very hard hard -\n1\nvery , , very very difﬁcult difﬁcult . . eos eos pad pad pad\nvery very very hard very hard hard . eos pad pad pad eos pad\nreally , very difﬁcult hard . . eos difﬁcult . . eos -\nit very , hard difﬁcult . hard eos hard . difﬁcult eos -\nextremely , , difﬁcult tough . difﬁcult eos . . hard eos -\n2\nvery , very , very hard very hard . hard . eos . eos pad eos pad pad\nvery very difﬁcult , very difﬁcult very difﬁcult . difﬁcult . eos eos pad pad pad pad pad\nvery very hard very difﬁcult . difﬁcult . eos . eos pad . . eos . eos pad\nreally , very very hard . hard . eos hard eos pad hard . eos hard eos pad\nit very difﬁcult , hard . very hard eos difﬁcult eos pad difﬁcult . eos difﬁcult eos pad\n3\nvery , very hard , very hard . very hard . eos hard . eos pad . eos pad pad\nvery , very difﬁcult , very difﬁcult . very difﬁcult . eos difﬁcult . eos pad eos pad pad pad\nvery very difﬁcult . very difﬁcult . eos difﬁcult . eos pad . eos pad pad difﬁcult . eos pad\nvery very hard . very hard . eos hard . eos pad hard eos pad pad hard . eos pad\nreally , very hard , very hard eos very hard eos paddifﬁcult eos pad pad. . eos pad\nTable 8: Cascaded Decoding Example. When m = 4, Viterbi in X4 returns “the opposite thing\nhappened . eos”. The source is “das gegenteil passierte . eos” and the target is “the opposite\nhappened . eos”.\nm x1:1+m x2:2+m x3:3+m x4:4+m x5:5+m x6:6+m x7:7+m x8\n0\nthe opposite opposite happened eos eos eos pad\nand contr@@ thing was . pad pad -\nso other ary thing happened . . -\nbut the happened did happening happened happened -\nwell conver@@ was opposite happen happen happen -\n1\nthe opposite opposite thing thing happened happened . . eos eos pad pad pad\nthe contr@@ contr@@ ary ary happened was happening happening . . eos eos pad\nand the the opposite opposite happened thing happened happened . pad pad -\nthe other other thing thing was did . eos pad . happened eos -\nso the opposite opposite was happened was happened . . - -\n2\nthe opposite thing opposite thing happened thing happened . happened . eos . eos pad eos pad pad\nthe contr@@ ary contr@@ ary happened ary happened . was happening . happening . eos . eos pad\nand the opposite the opposite happened opposite happened . was happened . happened . eos happened eos pad\nthe other thing other thing happened thing was happening happened . . . . eos pad pad pad\nso the opposite opposite thing was thing was happened thing happened . - -\n3\nthe opposite thing happened opposite thing happened . thing happened . eos happened . eos pad . eos pad pad\nthe contr@@ ary happened contr@@ ary happened . ary happened . eos was happening . eos happening . eos pad\nand the opposite happened the opposite happened . opposite happened . eos was happened . eos happened . eos pad\nthe other thing happened other thing happened . thing was happening . happened . . eos . . eos pad\nthe opposite thing was opposite thing was happening thing was happened . - -\n15\nAppendix B: More Visualizations\n(a) m = 0\n (b) m = 1\n“and that made” (c) m = 2\nFigure 4: Illustration of cascaded decoding (K = 10, iters=4) for X1, X2, X3.\n(a) m = 0\n (b) m = 1\n“my retire@@” (c) m = 2\nFigure 5: Illustration of cascaded decoding (K = 10, iters=4) for X1, X2, X3.\nWe include more visualizations of X1, X2 and X3 in Figure 4 and Figure 5. These examples are taken\nfrom IWSLT14 De-En val.\nAppendix C: Variable Length Generation Potentials\nTo handle length, we introduce an additional padding symbol pad to V, and change the log potentials\nto enforce the considered candidates are of length L−∆Lto L+ ∆L. Note that we can only enforce\nthat for m≥1, and for m= 0 we manually add pad to the pruned vocabulary.\nWe start cascaded search using a sequence of length L+ ∆L+ 1. The main ideas are: 1) We make\neos and pad to always transition to pad such that sequences of different lengths can be compared; 2)\nWe disallow eos to appear too early or too late to satisfy the length constraint; 3) We force the last\ntoken to be pad such that we don’t end up with sentences withouteos endings. Putting these ideas\ntogether, the modiﬁed log potentials we use are:\n16\nf′(m)\nl (xl:l+m)\n=\n\n\n\n0, if xl+m−1 = eos ∧xl+m = pad\n−∞, if xl+m−1 = eos ∧xl+m ̸= pad (eos →pad)\n0, if xl+m−1 = pad ∧xl+m = pad\n−∞, if xl+m−1 = pad ∧xl+m ̸= pad (pad →pad)\n−∞, if xl+m−1 ̸= pad ∧xl+m−1 ̸= eos ∧xl+m = pad (nothing else →pad)\n−∞, if l+ m<L −∆L∧xl+m = eos (eos cannot appear too early)\n0, if l+ m= L+ ∆L+ 1 and xl+m = pad\n−∞, if l+ m= L+ ∆L+ 1 and xl+m ̸= pad (the last token must be pad)\nf(m)\nl (xl:l+m), o.t.\n.\nNote that we only considered a single sentence above, but batching is straightforward to implement\nand we refer interested readers to our code5 for batch implementations.\nAppendix D: Full Results\nIn the main experiment table we showed latency/speedup results for WMT14 En-De. In Table 9,\nTable 10, Table 11 and Table 12 we show the latency/speedup results for other datasets. Same as in\nthe main experiment table, we use the validation set to choose the conﬁguration with the best BLEU\nscore under speedup >×1, >×2, etc.\nTable 9: Results on WMT14 De-En.\nModel Settings Latency (Speedup) BLEU\nTransformer (beam 5) 294.64ms ( ×1.00) 31.49\nWith Distillation\nCascaded Generation with Speedup\n>×7 (K=16, iters=2) 43.41ms ( ×6.79) 30.69\n>×6 (K=32, iters=2) 52.06ms ( ×5.66) 30.72\n>×5 (K=16, iters=3) 62.06ms ( ×4.75) 30.96\n>×4/3 (K=32, iters=3) 79.01ms ( ×3.73) 31.08\n>×2/1 (K=32, iters=5) 129.67ms ( ×2.27) 31.15\nWithout Distillation\nCascaded Generation with Speedup\n>×6/5 (K=32, iters=2) 53.83ms ( ×5.47) 27.56\n>×4 (K=32, iters=3) 81.10ms ( ×3.63) 28.64\n>×3 (K=32, iters=4) 106.97ms ( ×2.75) 28.73\n>×2 (K=64, iters=4) 154.15ms ( ×1.91) 29.43\n>×1 (K=128, iters=4) 269.59ms ( ×1.09) 29.66\n5https://github.com/harvardnlp/cascaded-generation\n17\nTable 10: Results on WMT16 En-Ro.\nModel Settings Latency (Speedup) BLEU\nTransformer (beam 5) 343.28ms ( ×1.00) 33.89\nWith Distillation\nCascaded Generation with Speedup\n>×7 (K=16, iters=2) 49.38ms ( ×6.95) 32.70\n>×6 (K=32, iters=2) 54.56ms ( ×6.29) 32.73\n>×5 (K=16, iters=3) 66.33ms ( ×5.18) 32.89\n>×4 (K=32, iters=3) 77.39ms ( ×4.44) 33.16\n>×3 (K=64, iters=3) 108.57ms ( ×3.16) 33.23\n>×2 (K=64, iters=4) 142.23ms ( ×2.41) 33.30\n>×1 (K=64, iters=5) 179.07ms ( ×1.92) 33.23\nWithout Distillation\nCascaded Generation with Speedup\n>×7 (K=16, iters=2) 45.18ms ( ×7.60) 32.11\n>×6 (K=32, iters=2) 51.38ms ( ×6.68) 32.62\n>×5 (K=16, iters=3) 60.34ms ( ×5.69) 32.67\n>×4 (K=32, iters=3) 73.99ms ( ×4.64) 33.12\n>×3 (K=64, iters=3) 105.46ms ( ×3.26) 33.48\n>×2 (K=64, iters=4) 145.18ms ( ×2.36) 33.64\n>×1 (K=128, iters=5) 325.42ms ( ×1.05) 33.52\nTable 11: Results on WMT16 Ro-En.\nModel Settings Latency (Speedup) BLEU\nTransformer (beam 5) 318.57ms ( ×1.00) 33.82\nWith Distillation\nCascaded Generation with Speedup\n>×6/5 (K=16, iters=2) 46.84ms ( ×6.80) 32.66\n>×4 (K=16, iters=3) 62.57ms ( ×5.09) 33.00\n>×3 (K=16, iters=5) 99.25ms ( ×3.21) 33.04\n>×2 (K=64, iters=3) 103.85ms ( ×3.07) 33.17\n>×1 (K=64, iters=5) 181.18ms ( ×1.76) 33.28\nWithout Distillation\nCascaded Generation with Speedup\n>×6 (K=16, iters=2) 47.58ms ( ×6.70) 32.53\n>×5 (K=32, iters=2) 54.05ms ( ×5.89) 32.44\n>×4 (K=16, iters=3) 60.94ms ( ×5.23) 33.00\n>×3 (K=32, iters=4) 100.29ms ( ×3.18) 33.10\n>×2 (K=64, iters=3) 105.21ms ( ×3.03) 33.22\n>×1 (K=128, iters=4) 282.76ms ( ×1.13) 33.29\n18\nTable 12: Results on IWSLT14 De-En.\nModel Settings Latency (Speedup) BLEU\nTransformer (beam 5) 229.76ms ( ×1.00) 34.44\nWith Distillation\nCascaded Generation with Speedup\n>×6/5 (K=16, iters=2) 39.38ms ( ×5.83) 33.90\n>×4 (K=32, iters=3) 60.27ms ( ×3.81) 34.33\n>×3 (K=32, iters=4) 78.27ms ( ×2.94) 34.43\n>×2/1 (K=64, iters=5) 117.90ms ( ×1.95) 34.49\nWithout Distillation\nCascaded Generation with Speedup\n>×5 (K=64, iters=2) 48.59ms ( ×4.73) 33.25\n>×4 (K=32, iters=3) 60.09ms ( ×3.82) 33.74\n>×3 (K=64, iters=3) 75.64ms ( ×3.04) 33.96\n>×2 (K=64, iters=5) 121.95ms ( ×1.88) 34.08\n>×1 (K=128, iters=5) 189.10ms ( ×1.22) 34.15\nAppendix E: Optimization Settings\nTable 13: Optimization settings. We use the same settings for knowledge distillation experiments.\nDataset dropout fp16 GPUs batch accum warmup steps max steps max lr weight decay\nWMT14 En-De/De-En 0.1 Y 3 4096 3 4k 240k 7e-4 0\nWMT16 En-Ro/Ro-En 0.3 Y 3 5461 1 10k 240k 7e-4 1e-2\nIWSLT14 De-En 0.3 N 1 4096 1 4k 120k 5e-4 1e-4\nOur approach is implemented in PyTorch [35], and we use 16GB Nvidia V100 GPUs for training.\nWe used Adam optimizer [20], with betas 0.9 and 0.98. We use inverse square root learning rate\ndecay after warmup steps [34]. We train with label smoothing strength 0.1 [32]. For model selection,\nwe used BLEU score on validation set. For Markov transformers, we use cascaded decoding with\nK = 16 and ∆L= 3 to compute validation BLEU score. Other hyperparameters can be found at\nTable 13.\n19",
  "topic": "Autoregressive model",
  "concepts": [
    {
      "name": "Autoregressive model",
      "score": 0.9277669191360474
    },
    {
      "name": "Computer science",
      "score": 0.7235735654830933
    },
    {
      "name": "Decoding methods",
      "score": 0.7096108198165894
    },
    {
      "name": "STAR model",
      "score": 0.6757574677467346
    },
    {
      "name": "Hidden Markov model",
      "score": 0.5406455993652344
    },
    {
      "name": "Transformer",
      "score": 0.4987025260925293
    },
    {
      "name": "Markov chain",
      "score": 0.48855122923851013
    },
    {
      "name": "Algorithm",
      "score": 0.4823698103427887
    },
    {
      "name": "Cascade",
      "score": 0.4684469997882843
    },
    {
      "name": "Context (archaeology)",
      "score": 0.45051613450050354
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35082072019577026
    },
    {
      "name": "Speech recognition",
      "score": 0.3400197923183441
    },
    {
      "name": "Machine learning",
      "score": 0.25552430748939514
    },
    {
      "name": "Time series",
      "score": 0.2163209617137909
    },
    {
      "name": "Mathematics",
      "score": 0.19490447640419006
    },
    {
      "name": "Autoregressive integrated moving average",
      "score": 0.17712649703025818
    },
    {
      "name": "Econometrics",
      "score": 0.12970265746116638
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    }
  ]
}