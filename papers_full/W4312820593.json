{
  "title": "Towards Optimal Patch Size in Vision Transformers for Tumor Segmentation",
  "url": "https://openalex.org/W4312820593",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3139192876",
      "name": "Ramtin Mojtahedi",
      "affiliations": [
        "Queen's University"
      ]
    },
    {
      "id": "https://openalex.org/A306089521",
      "name": "Mohammad Hamghalam",
      "affiliations": [
        "Qazvin Islamic Azad University",
        "Queen's University"
      ]
    },
    {
      "id": "https://openalex.org/A2152710113",
      "name": "Richard K. G. Do",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2125637486",
      "name": "Amber L. Simpson",
      "affiliations": [
        "Queen's University"
      ]
    },
    {
      "id": "https://openalex.org/A3139192876",
      "name": "Ramtin Mojtahedi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A306089521",
      "name": "Mohammad Hamghalam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152710113",
      "name": "Richard K. G. Do",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125637486",
      "name": "Amber L. Simpson",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2762561229",
    "https://openalex.org/W2759180643",
    "https://openalex.org/W2950605145",
    "https://openalex.org/W3028437692",
    "https://openalex.org/W3026468359",
    "https://openalex.org/W3202196779",
    "https://openalex.org/W2996837403",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3172681723",
    "https://openalex.org/W2620349183"
  ],
  "abstract": null,
  "full_text": "Towards Optimal Patch Size in Vision\nTransformers for Tumor Segmentation\nRamtin Mojtahedi1[0000−0002−3953−3256], Mohammad\nHamghalam1,2[0000−0003−2543−0712], Richard K. G. Do 3[0000−0002−6554−0310],\nand Amber L. Simpson 1,4[0000−0002−4387−8417]\n1 School of Computing, Queen’s University, Kingston, ON, Canada\namber.simpson@queensu.ca\n2 Department of Electrical Engineering, Qazvin Branch, Islamic Azad University,\nQazvin, Iran\n3 Department of Radiology, Memorial Sloan Kettering Cancer Center, New York,\nNY, USA\n4 Department of Biomedical and Molecular Sciences, Queen’s University, Kingston,\nON, Canada\nAbstract. Detection of tumors in metastatic colorectal cancer (mCRC)\nplays an essential role in the early diagnosis and treatment of liver\ncancer. Deep learning models backboned by fully convolutional neural\nnetworks (FCNNs) have become the dominant model for segmenting\n3D computerized tomography (CT) scans. However, since their convo-\nlution layers suffer from limited kernel size, they are not able to cap-\nture long-range dependencies and global context. To tackle this restric-\ntion, vision transformers have been introduced to solve FCNN’s local-\nity of receptive fields. Although transformers can capture long-range\nfeatures, their segmentation performance decreases with various tumor\nsizes due to the model sensitivity to the input patch size. While finding\nan optimal patch size improves the performance of vision transformer-\nbased models on segmentation tasks, it is a time-consuming and chal-\nlenging procedure. This paper proposes a technique to select the vi-\nsion transformer’s optimal input multi-resolution image patch size based\non the average volume size of metastasis lesions. We further validated\nour suggested framework using a transfer-learning technique, demon-\nstrating that the highest Dice similarity coefficient (DSC) performance\nwas obtained by pre-training on training data with a larger tumour\nvolume using the suggested ideal patch size and then training with a\nsmaller one. We experimentally evaluate this idea through pre-training\nour model on a multi-resolution public dataset. Our model showed consis-\ntent and improved results when applied to our private multi-resolution\nmCRC dataset with a smaller average tumor volume. This study lays\nthe groundwork for optimizing semantic segmentation of small objects\nusing vision transformers. The implementation source code is available\nat: https://github.com/Ramtin-Mojtahedi/OVTPS.\nKeywords: CT Segmentation · Vision Transformer · Liver Tumor ·\narXiv:2308.16598v1  [eess.IV]  31 Aug 2023\n2 R. Mojtahedi et al.\n1 Introduction\nColorectal cancer is the third most common cancer diagnosed in the United\nStates, with 100,000 new cases and 50,000 deaths expected in 2022 [1]. The sur-\nvival rate of these patients is over 90% [2]. However, up to 70% of them will\ndevelop liver metastasis [3], with a roughly 5-year survival rate of 11% [4]. The\nsegmentation of metastatic colorectal cancer (mCRC) liver tumours on com-\nputed tomography (CT) images is essential for evaluating tumour response to\nchemotherapy and surgical planning [5], especially for detecting small metastasis\ntumor volumes in the liver tissue. To achieve this objective, it is imperative to\nbuild and develop a reliable and automated machine-learning (ML) model.\nConvolutional neural networks (CNNs)-based [6,7,8,9,10] and vision trans-\nformers (ViTs)-based[11] architectures are the major machine learning segmen-\ntation approaches. Since the introduction of the pioneering U-shaped encoder-\ndecoder architecture, dubbed U-Net [12], CNN-based architectures have achieved\nstate-of-the-art performance on a variety of medical image segmentation tasks\n[13]. The U-Net is a densely supervised encoder-decoder network where the en-\ncoder and decoder sub-networks are connected by densely supervised skip path-\nways. Adapting the U-Net to new challenges entails a range of design, preprocess-\ning, training, and assessment strategies for the network. These hyperparameters\nare interconnected and have a substantial effect on the outcome. The nnU-Net\nframework was developed by Isensee et al. [14] to address these limitations.\nBased on 2D and 3D vanilla U-Nets, they suggested nnU-Net as a robust and\nself-adaptive architecture.\nDespite the effectiveness of fully convolutional networks, these networks have\na drawback in learning global context and long-range spatial relationships due to\ntheir confined kernel size and receptive fields. To tackle this limitation, Dosovit-\nsky et al. [15] proposed using transformers in computer vision tasks, called ViTs,\nresulting from their successful performance in the language domain, their ability\nto capture long-range dependencies, and their self-attention mechanisms. Com-\npared to state-of-the-art convolutional networks, ViT-based models achieve sig-\nnificant outcomes while using fewer computing resources for the training phase.\nBy integrating an additional control mechanism in the self-attention module, a\ngated axial-attention model was presented by Valanarasu et al., [16], extending\nthe previous transformer-based architectures. A novel ViT-based on a hierarchi-\ncal structure was introduced by Liu et al. [17] to represent the image features\nthrough shifted windows. Their proposed structure improved performance as\nself-attention processing is limited to non-overlapping local windows, but cross-\nwindow connections are still allowed. Hatamizadeh et al. [18] proposed UNEt\nTRansformers (UNETR) to capture global multi-scale information. This unique\nU-Net-based architecture employs a transformer as the encoder to learn sequence\nrepresentations of the input volume. The extracted features from the transformer\nencoder are integrated with the CNN-based decoder through skip connections\nto predict the segmentation outputs.\nAlthough the UNETR achieved state-of-the-art performance in 3D volumet-\nric segmentation, it used an isotropic network topology with fixed-size feature\nOptimal ViT Patch Size for Tumor Segmentation 3\nresolution and an inflexible embedding size. Therefore, UNETR could not de-\nscribe context at various sizes or assign computations at different resolutions.\nWhile their proposed network could be performant for segmenting large-sized\nobjects such as the liver, it did not show the same level of performance in seg-\nmenting small objects such as small liver tumors.\nOur goal in this paper was to detect and segment colorectal liver metas-\ntases on abdominal CT scans. The contribution of this work is twofold: First,\nwe introduce a framework to find an optimal patch size for the vision trans-\nformer models to improve ViT-based structures in segmenting small objects.\nSpecifically, based on the liver lesion volume, we designed a framework for ViT\npatch size, using the UNETR architecture as the backbone of our experiments to\nachieve higher segmentation performance. We also validated our proposed frame-\nwork in a transfer-learning approach and showed that pre-training on training\ndata that has a larger tumor volume using the proposed optimal patch size and\nthen training with a smaller one achieved the best Dice similarity coefficient\n(DSC) performance. Second, we show that our pipeline outperforms the UN-\nETR baseline ViT-based model in terms of DSC for segmenting liver metastasis\nand validates our results on LiTS and mCRC datasets.\n2 Method\nIn the following subsections, the structure of the ViT-based model, our proce-\ndure to select optimal patch size, and a novel training technique to improve\nperformance on segmenting of small tumors are elaborated.\n2.1 ViT-based Model Structure\nTransformer Patch. In the ViT transformer framework, the input image is\nsplit into patches, and a series of linear embeddings of these patches is passed to\nthe vanilla transformer [15]. These image patches are processed and considered\nsimilar to tokens (words) in natural language processing. Specifically, transform-\ners operate on a 1D sequence of input embeddings. Similarly, the given 3D input\nimages are mapped to 1D embedding vectors in our pipeline. In the utilized\nframework, the 3D CT volumes are provided with an input size of (H, W, L)\nare the input image dimensions. The transformer patches are represented as\nM. Accordingly, flattened uniform sequences are being created with the size\nof N = (H×W×L)/(M×M×M) using non-overlapping patches that are shown\nwith yv ∈ RN×M3\n.\nTo maintain the retrieved patches’ spatial information, 1D learnable posi-\ntional embeddings (Epos ∈ RN×P ) are added to the projected patch embeddings\nwith dimensional embedding size. This process is shown in Eq. (1).\nz0 = [y1\nvE; y2\nvE; ...; yN\nv E] + Epos (1)\n4 R. Mojtahedi et al.\nFig. 1.Comparing tumor sizes in mCRC and LiTS sample data where (a) is the LiTS\ndataset with larger tumor volume size, and (b) is the mCRC dataset (Green: liver,\nYellow: tumor).\nwhere E ∈ RM3×P is flattened uniform non-overlapping patches embedding [13].\nThe transformer encoder consists of alternating layers of multi-headed self-\nattention (MSA) and multilayer perceptron (MLP) blocks. As proposed in [9],\nthese blocks can be shown as Eq. (2) and Eq. (3).\n´zj = MSA(Lnorm(zj−1)) + zj−1, j∈ [1, ..., T] (2)\n´zj = MLP(Lnorm( ´zj)) + ´zj, j∈ [1, ..., T] (3)\nwhere T is the number of transformer layers, Lnorm function denotes layer\nnormalization, and the MLP block consists of two linear layers with GELU\nactivation functions. There are parallel self-attention (SA) heads in the MSA\nsublayer, and attention weights are calculated as (4). The SA block uses standard\nqkv self-attention, which uses query ( q) and the sequence’s associated key ( k)\nand input sequence value ( v) representations. Eq. (4) is also included where\nKh = K/n is the scaling factor and the outputs of the MSA are achieved as Eq.\n(5) using MSA weights.\nAttention(q,k,v) = Softmax( qKT\n√Kh\n)v (4)\n[Attention1(z); ...; Attentionn(z)]WMSA (5)\nLoss Function. The loss function employed is a mix of soft Dice loss and cross-\nentropy loss, which could be calculated in a voxel-by-voxel approach, as shown\nin Eq. (6).\nLoss(G, O) = 1 − 2\nC\nCX\nr=1\nΣA\nx=1Gx,rOx,r\nΣC\nx=1G2x,r + ΣU\nK=1O2x,r\n− 1\nA\nAX\nx=1\nCX\nr=1\nGx,rlog(Ox,r) (6)\nOptimal ViT Patch Size for Tumor Segmentation 5\nwhere A represents the voxel’s number; C denotes the number of classes. The\nprobability output and one-hot encoded ground truth for class r at voxel x are\nrepresented by Ox,r and Gx,r, respectively [19].\nUNETR uses a contracting-expanding pattern with a stack of transformers,\nViT, as the encoder, and skip connections to the decoder. It considers the patch\nsize as a hyperparameter. In this sense, choosing an optimal patch size ( M∗) is\ncritical due to its impact on the features’ receptive field. This is because patches\nare reshaped into a tensor with the size of H\nM∗ × W\nM∗ × L\nM∗ × P, where P is\nthe transformer’s embedding size. To assess the impact of patch size, we did\nour experiments on two clinical datasets, LiTS, and our private mCRC. LiTS\nhas a larger tumors volume size than the mCRC dataset. Samples of these two\ndatasets are shown in Fig. 1.\n2.2 Choosing Optimal Patch size\nThe proposed framework tries to find the best patch size for our tumor segmen-\ntation. As shown in Fig. 2, the average volume of the tumors is first computed\non the training input. Then, the optimal patch size is determined based on a\nmathematical relationship between the average volume size of tumors and the\nperformance. The experimented patch sizes must be a factor of the input image\ndimensions, 256 × 256 × 96, and were selected based on our computational re-\nsources and with respect to the sizes proposed in [15], M ∈ [8, 12, 16, 24]. This\nensures that the model performs best when segmenting small objects such as\ntumors.\nThe average volume size of tumors in LiTS datasets was reported as 17.56\ncm3 [20]. Through histogram analysis on our private mCRC dataset, the average\ntumor volume was achieved as 10.42 cm3. Empirically, the relationship between\noptimal patch size ( M∗) and the average volume size of the tumors attained as\nEq. (7).\nM∗ = Argmin\nM∈[8,12,16,24]\n(|\n3√\nV × S − M|) (7)\nwhere M is the patch size;S is the voxel spacing, andV is the average volume size\nof tumors for LiTS and mCRC datasets. The optimal patch is achieved by finding\nthe patch that makes the absolute differentiation of cube root multiplication of\nvoxel spacing and average tumors volume size with patch size be at a minimum.\n2.3 Pre-training Technique to Improve Segmentation of Small\nTumors\nTo increase the segmentation performance of ViT-based structures on small le-\nsions, we suggested pre-training on a dataset with a large tumor volume (LiTS\ndata) using the optimal patch size achieved by the proposed framework and sub-\nsequently training on a smaller one (mCRC dataset) to achieve the best DSC\nperformance. This idea was experimentally tested, as shown in the next section,\nand could increase the DSC significantly compared to when the dataset with a\nsmall tumor volume was trained by scratch.\n6 R. Mojtahedi et al.\nFig. 2.The model’s pipeline is shown for the proposed framework. In step (a), the\nframework receives the raw 3D CT images of the abdomen, consisting of the liver\nand its primary and secondary tumors. Then, the average tumor volume is computed\nthrough histogram analysis in step (b). Through assessment of the averaged volume of\ntumors and the three determined patch sizes in step (c), M ∈ [8,12,16,24] the optimal\npatch size is selected in step (d). In step (e), we train the model and segment it in our\nbackboned UNETR network to achieve segmented tumors and liver organs in step (f).\n3 Experimental Results\n3.1 Datasets\nWe conducted our experiments on two multi-resolution 3D abdominal CT liver\ndatasets, including training with large tumor volumes (LiTS) and later smaller\nones (mCRC). For the former, we used the LiTS, which consisted of 201 CT im-\nages with liver and liver tumors annotations: 131 for training and 70 for testing.\nThe number of tumors detected in the scans varied between 0 and 75, exhibiting\na half-normal distribution. The dataset was created to closely show real-world\nclinical data and contains a range of cancer types, including primary tumors\nsuch as hepatocellular carcinoma (HCC) and metastasis from colorectal, breast,\nand lung cancer. The collection includes scans with voxel spacings ranging from\nOptimal ViT Patch Size for Tumor Segmentation 7\n0.56 mm to 1.0 mm in the axial plane and slice thicknesses ranging from 0.70\nmm to 5.0 mm [20,21]. In the private data, we employed CT volume of the col-\norectal liver metastasis (mCRC) [22]. This dataset contained 198 CT scans of\npatients who underwent hepatic resection for CRLM between 2003 and 2007.\nThe number of tumors in the scans ranged from 1 to 17. Voxel spacing in the\nscans in the dataset ranged from 0.61 mm to 0.98 mm in the axial plane, and\nslice thickness was 0.80 mm to 7.5 mm. For data pre-processing, all image voxel\nspacing was normalized to the range [0–1]. In addition, all foreground images\nwere resampled to a voxel spacing of 0 .765 × 0.765 × 1.5mm3, achieved by the\nmedian of the range of spacings and availability of computational resources. The\ndata is also transformed using 90 orientation, flipping, random rotations, and\nintensity shifting.\n3.2 Implementation details\nTable 1 illustrates the important model parameters and hyperparameters we\nemployed to conduct the experiments on our datasets. The hyperparameters\nused for the ViT network were selected based on the ViT-base discussed in [15].\nWe also ran the experiments with various input image sizes and discovered that\n256 × 256 × 96 produced the best results compatible with our computational\nresources. Implementations of experiments and code will also be available. For\nall experiments, the training and validation split considered as 80:20.\nTable 1.Summary of employed parameters and critical hyperparameters.\nParameter Description of the Value/Method\nInput Image Size [ H × W × L] [256 × 256 × 96]\nOptimizer Adam\nLearning Rate 0.0001\nWeight-Decay 1e-5\nViT: [Layers, Hidden Size, MLP size, Heads,\nNumber of Parameters] [12, 768, 3072, 12, 86M]\nBatch Size 1\nComputational Resource NVIDIA A100 – 40GB\n3.3 Liver and Lesion Segmentation Results\nSegmentation Results for Optimal Patch size. We calculated the optimal\npatch size, M∗, based on eq. (7) for both datasets. We also compared our results\nwith smaller and larger patch sizes than the calculated one to validate our pro-\nposed technique. As shown in Table 2, the best performance results were achieved\nfor the computed patch size of 16 and 12 for the LiTS and mCRC datasets, re-\nspectively. In addition to these experiments, we tested the DSC performance\nusing a combination of both datasets, which did not outperform the following\nresults. Moreover, as our main focus was to find the optimal patch size, we didn’t\n8 R. Mojtahedi et al.\nprovide results with respect to the CNN-based architectures, which inherently\nhave different structures with no utilized transformer.\nTable 2.Highest segmentation performance results for the models built on LiTS and\nmCRC datasets using multiple patch sizes.\nDataset Patch SizeTumor DSC [%]Liver DSC [%] Loss Training\nTime [Min.]\nLiTS\nM=8 48.62 81.3 0.2105 1883.93\nM=12 51.19 87.37 0.2297 2464.83\nM*=16 53.08 88.06 0.1805 2811.70\nM=24 51.91 87.93 0.1717 4106.87\nmCRC\nM=8 39.64 89.51 0.1893 3745.30\nM*=12 41.44 92.35 0.1020 2221.24\nM=16 40.14 87.77 0.1060 ⁄tieacce♪tlowercase2566.82\nM=24 38.82 87.85 0.2050 3758.87\nEffectiveness of the Proposed Vision-based Model Training. Table 3 in-\ndicates that employing LiTS pre-trained models significantly improves segmen-\ntation performance. The patch size of 16 showed the best outcomes, with a DSC\nof 44.94 percent for tumor segmentation. This indicates that training on a large\ntumor volume dataset successfully learns tumor representations that improve\nmodel performance on an mCRC dataset with small tumor volume mCRC.\nTable 3.Comparison of the highest segmentation performance (DSC(%)) results using\nthe pre-trained model on the dataset with larger tumor volumes (LiTS) to the dataset\nwith smaller tumor volumes (mCRC).\nPatch SizePre-trained ModelNon Pre-trained ModelImprovement\nTumor Liver Tumor Liver Tumor Liver\n8 42.2 93.97 39.64 89.51 2.56 4.46\n12 44.46 94.42 41.44 92.35 3.02 2.07\n16 (M*) 44.94 94.61 40.14 87.77 4.8 6.84\nFig. 3 visually compares the segmentation performance between pre-trained\nmodels on the LiTS dataset with larger tumor volumes to the mCRC dataset\nwith smaller tumor volumes both for tumor and liver organ.\n4 Discussion and Conclusion\nThis paper proposed a novel framework to find an optimal patch size for se-\nmantic segmentation, particularly practical for small liver lesion segmentation.\nBased on the volume size of metastasis, we introduced a procedure to calculate\nOptimal ViT Patch Size for Tumor Segmentation 9\nFig. 3.The performance results for the tumor and liver organ segmentation tasks were\nobtained using LiTS pre-trained models and mCRC itself for training. All pre-trained\nmodels could improve performance, while the model with a patch size of 16 achieved\nthe best results, improving the tumor segmentation performance by 4.8%.\npatch size methodically in transformer-based segmentation models. In addition,\nthe optimal patch size computed by the proposed method showed the best per-\nformance on large objects such as a liver organ. Furthermore, a significant part\nof the small tumor information was missed when we trained on a large patch\nsize. However, when we pre-trained a model on our public dataset of LiTS with\nlarger tumors, the model could learn tumors representations with higher per-\nformance. Consistent with our first novelty and in a transfer-learning approach,\nthe pre-trained model demonstrated its most effective performance in learning\nrepresentations and segmentation performance when it utilized the computed\noptimal patch size defined by (7), M∗. The results of this study could be used\nfor further development in vision transformer-based networks with multi-patch\nsizes. We also showed that our pipeline outperforms the ViT-based models in\nterms of DSC for segmenting liver metastasis tumors and validated our results\non LiTS and mCRC datasets.\n5 Acknowledgement\nThis work was funded in part by National Institutes of Health R01CA233888.\nReferences\n1. Colorectal cancer - statistics, https://www.cancer.net/cancer-types/\ncolorectal-cancer/statistics. last accessed 31 May 2022\n2. Colorectal cancer survival rates: Colorectal cancer prognosis, https://www.\ncancer.org/cancer/colon-rectal-cancer/detection-diagnosis-staging/\nsurvival-rates. last accessed 1 March 2022\n3. Liver metastases (secondary liver cancer), https://www.mskcc.org/cancer-care/\ntypes/liver-metastases\n10 R. Mojtahedi et al.\n4. Valderrama-Trevi˜ no, A.I., Barrera-Mera, B., Ceballos-Villalva, J.C.,\nMontalvo-Jav´ e, E.E.: Hepatic metastasis from colorectal cancer. Eu-\nroasian Journal of Hepato-Gastroenterology. 7, 166–175 (2016). https:\n//doi.org/10.50052Fjp-journals-10018-1241\n5. Wu, W., Wu, S., Zhou, Z., Zhang, R., Zhang, Y.: 3D liver tumor segmentation\nin CT images using improved fuzzy c-means and graph cuts. BioMed Research\nInternational, 1–11 (2017). https://doi.org/10.1155/2017/5207685\n6. Soleymanifard, M., Hamghalam, M.: Segmentation of whole tumor using localized\nactive contour and trained neural network in boundaries. 2019 5th Conference on\nKnowledge Based Engineering and Innovation (KBEI). (2019)\n7. Hamghalam, M., Wang, T., Qin, J., Lei, B.: Transforming intensity distribution of\nbrain lesions via conditional gans for segmentation. 2020 IEEE 17th International\nSymposium on Biomedical Imaging (ISBI). (2020)\n8. Hamghalam, M., Lei, B., Wang, T.: Convolutional 3D to 2D patch conversion\nfor pixel-wise glioma segmentation in MRI scans. Brainlesion: Glioma, Multiple\nSclerosis, Stroke and Traumatic Brain Injuries. 3–12 (2020)\n9. Hamghalam, M., Frangi, A.F., Lei, B., Simpson, A.L.: Modality completion via\ngaussian process prior variational autoencoders for multi-modal glioma segmenta-\ntion. Medical Image Computing and Computer Assisted Intervention – MICCAI\n2021. 442–452 (2021)\n10. Hamghalam, M., Lei, B., Wang, T.: High tissue contrast MRI synthesis using\nmulti-stage attention-gan for segmentation. Proceedings of the AAAI Conference\non Artificial Intelligence. 34, 4067–4074 (2020)\n11. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang,\nT., Torr, P.H.S., Zhang, L.: Rethinking semantic segmentation from a sequence-\nto-sequence perspective with Transformers. 2021 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR). (2021)\n12. Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional Networks for\nBiomedi-cal Image Segmentation. Lecture Notes in Computer Science. 234–241\n(2015)\n13. Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J.: UNet++: A nested\nU-Net Architecture for Medical Image segmentation. Deep Learning in Medical Im-\nage Analysis and Multimodal Learning for Clinical Decision Support. 3–11 (2018).\nhttps://doi.org/10.1007/97830300088951\n14. Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: NNU-\nnet: A self-configuring method for deep learning-based biomedical image seg-\nmentation. Nature Methods. 18, 203–211 (2020). https://doi.org/10.1038/\ns4159202001008z\n15. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\nICLR 2021. (2021)\n16. Valanarasu, J.M., Oza, P., Hacihaliloglu, I., Patel, V.M.: Medical transformer:\nGated axial-attention for medical image segmentation. Medical Image Computing\nand Computer-Assisted Intervention – MICCAI 2021. 36–46 (2021)\n17. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin Trans-\nformer: Hierarchical Vision Transformer using Shifted Windows. arXiv preprint\narxXiv:2103.14030 (2021)\n18. Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B.,\nRoth, H.R., Xu, D.: UNETR: Transformers for 3D Medical Image segmentation.\nOptimal ViT Patch Size for Tumor Segmentation 11\n2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),\n(2022)\n19. Milletari, F., Navab, N., Ahmadi, S.-A.: V-net: Fully convolutional neural networks\nfor volumetric medical image segmentation. 2016 Fourth International Conference\non 3D Vision (3DV), (2016). https://doi.org/10.1109/3DV.2016.79\n20. Antonelli, M., Reinke, A., Bakas, S., Farahani, K., Kopp-Schneider, A., Landman,\nB.A., Litjens, G., Menze, B., Ronneberger, O., Summers, R.M., van Ginneken, B.,\nBilello, M., Bilic, P., Christ, P.F., Do, R.K., Gollub, M.J., Heckers, S.H., Huisman,\nH., Jarnagin, W.R., McHugo, M.K., Napel, S., Pernicka, J.S., Rhode, K., Tobon-\nGomez, C., Vorontsov, E., Meakin, J.A., Ourselin, S., Wiesenfarth, M., Arbel´ aez,\nP., Bae, B., Chen, S., Daza, L., Feng, J., He, B., Isensee, F., Ji, Y., Jia, F., Kim, I.,\nMaier-Hein, K., Merhof, D., Pai, A., Park, B., Perslev, M., Rezaiifar, R., Rippel,\nO., Sarasua, I., Shen, W., Son, J., Wachinger, C., Wang, L., Wang, Y., Xia, Y., Xu,\nD., Xu, Z., Zheng, Y., Simpson, A.L., Maier-Hein, L., Cardoso, M.J.: The Medical\nSegmentation Decathlon. Nature Communications. 13, (2022)\n21. Bilic, P., Christ, P.F., Vorontsov, E., Chlebus, G., Chen, H., Dou, Q., Fu, C.-W.,\nHan, X., Heng, P.-A., Hesser, J., Kadoury, S., Konopczynski, T., Le, M., Li, C., Li,\nX., Lipkov` a, J., Lowengrub, J., Meine, H., Moltz, J.H., Pal, C., Piraud, M., Qi, X.,\nQi, J., Rempfler, M., Roth, K., Schenk, A., Sekuboyina, A., Zhou, P., H¨ ulsemeyer,\nC., Beetz, M., Ettlinger, F., Gruen, F., Kaissis, G., Loh¨ ofer, F., Braren, R., Holch,\nJ., Hofmann, F., Sommer, W., Heinemann, V., Jacobs, C., Mamani, G.E.H., van\nGinneken, B., Chartrand, G., Tang, A., Drozdzal, M., Ben-Cohen, A., Klang, E.,\nAmitai, M.M., Konen, E., Greenspan, H., Moreau, J., Hostettler, A., Soler, L., Vi-\nvanti, R., Szeskin, A., Lev-Cohain, N., Sosna, J., Joskowicz, L., Menze, B.H.: The\nLiver Tumor Segmentation Benchmark (LiTS). arXiv preprint arXiv:1901.04056\n(2019)\n22. Simpson, A.L., Doussot, A., Creasy, J.M., Adams, L.B., Allen, P.J., DeMatteo,\nR.P., G¨ onen, M., Kemeny, N.E., Kingham, T.P., Shia, J., Jarnagin, W.R., Do, R.K.,\nD’Angelica, M.I.: Computed tomography image texture: A noninvasive prognostic\nmarker of hepatic recurrence after hepatectomy for metastatic colorectal cancer.\nAnnals of Surgical Oncology. 24, 2482–2490 (2017)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8053381443023682
    },
    {
      "name": "Computer vision",
      "score": 0.5826953649520874
    },
    {
      "name": "Transformer",
      "score": 0.5497956871986389
    },
    {
      "name": "Segmentation",
      "score": 0.5479207634925842
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5315836071968079
    },
    {
      "name": "Image segmentation",
      "score": 0.49677973985671997
    },
    {
      "name": "Electrical engineering",
      "score": 0.1477677822113037
    },
    {
      "name": "Voltage",
      "score": 0.053653866052627563
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ]
}