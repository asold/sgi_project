{
  "title": "Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling",
  "url": "https://openalex.org/W4385573912",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2914372190",
      "name": "Vidhisha Balachandran",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A91410043",
      "name": "Hannaneh Hajishirzi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1989912291",
      "name": "William Cohen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2234266251",
      "name": "Yulia Tsvetkov",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3159259047",
    "https://openalex.org/W3101551503",
    "https://openalex.org/W4288289173",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2127978399",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2971034336",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3099766584",
    "https://openalex.org/W4385573357",
    "https://openalex.org/W3099474967",
    "https://openalex.org/W4281659037",
    "https://openalex.org/W3024131638",
    "https://openalex.org/W3170432046",
    "https://openalex.org/W4385574044",
    "https://openalex.org/W4385573712",
    "https://openalex.org/W3169283369",
    "https://openalex.org/W4287887233",
    "https://openalex.org/W3187952778",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W3176869007",
    "https://openalex.org/W3199926081",
    "https://openalex.org/W3034188538",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W3158419028",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W4280625699",
    "https://openalex.org/W3100258764",
    "https://openalex.org/W3102645206",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4385573830",
    "https://openalex.org/W3135012552",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W3102489149",
    "https://openalex.org/W2768957049"
  ],
  "abstract": "ive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of non-factual summaries through infilling language models. With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency. Through quantitative and qualitative experiments on two popular summarization datasets— CNN/DM and XSum—we show that our approach vastly outperforms prior methods in correcting erroneous summaries. Our model—FactEdit—improves factuality scores by over ~11 points on CNN/DM and over ~31 points on XSum on average across multiple summarization models, producing more factual summaries while maintaining competitive summarization quality.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9818–9830\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nCorrecting Diverse Factual Errors in Abstractive Summarization via\nPost-Editing and Language Model Infilling\nVidhisha Balachandran♣ Hannaneh Hajishirzi♢♡\nWilliam W. Cohen♠ Yulia Tsvetkov♡\n♣Language Technologies Institute, Carnegie Mellon University\n♢Allen Institute for Artificial Intelligence\n♡Paul G. Allen School of Computer Science & Engineering, University of Washington\n♠Google Research\nvbalacha@cs.cmu.edu, hannaneh@cs.washington.edu, wcohen@google.com, yuliats@cs.washington.edu\nAbstract\nAbstractive summarization models often gen-\nerate inconsistent summaries containing fac-\ntual errors or hallucinated content. Recent\nworks focus on correcting factual errors in gen-\nerated summaries via post-editing. Such correc-\ntion models are trained using adversarial non-\nfactual summaries constructed using heuristic\nrules for injecting errors. However, generat-\ning non-factual summaries using heuristics of-\nten does not generalize well to actual model\nerrors. In this work, we propose to generate\nhard, representative synthetic examples of non-\nfactual summaries through infilling language\nmodels. With this data, we train a more ro-\nbust fact-correction model to post-edit the sum-\nmaries to improve factual consistency. Through\nquantitative and qualitative experiments on two\npopular summarization datasets— CNN/DM\nand XSum—we show that our approach vastly\noutperforms prior methods in correcting erro-\nneous summaries. Our model— FACT EDIT —\nimproves factuality scores by over ∼11 points\non CNN/DM and over ∼31 points on XSum on\naverage across multiple summarization models,\nproducing more factual summaries while main-\ntaining competitive summarization quality.1\n1 Introduction\nWhile modern summarization models generate\nhighly fluent summaries that appear realistic (Lewis\net al., 2020; Zhang et al., 2020), these models are\nprone to generating non-factual and sometimes en-\ntirely fabricated content (Cao et al., 2018; Goodrich\net al., 2019; Maynez et al., 2020). With the in-\ncreasing adoption of language generation tools in\nuser-facing products, such unreliability poses se-\nvere risks, including the spread of misinformation,\npanic and other potentially harmful effects (Ranade\net al., 2021; Hutson et al., 2021).\nSince it is difficult to control for factuality at\ntraining or inference time (Huang et al., 2021;\n1Code and data available at https://github.com/\nvidhishanair/FactEdit.\nThe ﬁrst vaccine for Ebola was approved by the FDA in \n2019 in the US, ﬁve years after the initial outbreak in \n2014. To produce the vaccine, scientists had to sequence \nthe DNA of Ebola, then identify possible vaccines, and \nﬁnally show successful clinical trials. Scientists say a \nvaccine for COVID-19 is unlikely to be ready this year, \nalthough clinical trials have already started. \nScientists believe a \nvaccine for Covid-19 \nmight not be ready \nthis year. The ﬁrst \nvaccine for Ebola took \n5 years to be \napproved by the FDA.\nScientists believe a \nvaccine for Ebola \nmight not be ready \nthis year. The ﬁrst \nvaccine for Ebola took \n5 years to be \nproduced by the CBP.\nIncorrect  \nEntity\nIncorrect  \nPredicate Hallucination\nSource\nGenerated   \nSummary\nCorrected   \nSummary\nError  \nCorrection\nFigure 1: Model generated summaries often produce\ncontent which is factually inconsistent w.r.t. to the\nsource. FACT EDIT rewrites these summaries by main-\ntaining the abstractiveness but correcting factual errors.\nDreyer et al., 2021), a popular approach to fix the\nfactual inconsistencies is via post-editing generated\nsummaries (Cao et al., 2020; Dong et al., 2020).\nThis allows summarization models to focus on flu-\nency and content-relevance while improving fac-\ntual consistency. However, there is no suitable\ndata for training post-editing models to directly\n“translate” an incorrect summary to a correct one.\nPrior work constructed synthetic training data by\nintroducing simple heuristic errors like replacing\nentities or numbers in reference summaries (Cao\net al., 2020), but it is not clear whether such syn-\nthetic errors have sufficient coverage and accurately\nrepresent the types and distribution of actual errors\nmade by language models. Further, with increas-\ning language generation capabilities, models make\nmore complex factual errors involving discourse\nstructures and paraphrasing which cannot be easily\ncaptured with heuristics (Pagnoni et al., 2021). The\ngoal of our work is to develop post-editing models\n9818\nVaccine for [MASK] is \nunlikely to be ready this year. \n[SEP] The ﬁrst vaccine for \nEbola …….. ready this year, \nalthough clinical trials have \nalready started. \nCovid-19\nCoronavirus\nEbola\nthe virus\nVaccine for Ebola \nis unlikely to be \nready this year.\nVaccine for the virus \nis unlikely to be \nready this year.\nWorld leaders met Ban Ki Moon for \nUN meeting in 2020[SEP] Pandemic \nresponse … [SEP] UN Sec. Gen. \nAntonio Gutteres met the leaders\nWorld leaders met \nAntonio Gutteres for \nUN meeting in 2020\nInput: masked ref summary \n(smasked) [SEP] source context \n(ctx)\nInput: incorrect summary sent (sg’) \n[SEP] summary context (g’) \n[SEP] relevant source passages\nOutput: corrected summary  \nsent (g)\nInﬁlling  \nLanguage Model \n(MI)\nSeq-to-Seq \nModel (MC)\nInﬁlling-Based Generation of Adversarial Summaries\ntrain\ndecode\nOutput: beam search \ncandidates\nFactEdit: Factual Error Correction\n incorrect summary  \ncandidates (r’)\nTraining  \nData\nTraining  \nData\nFigure 2: Architecture framework for FACT EDIT . Using masked versions of existing reference summaries, we use\nan infilling language model to produce alternative candidates for the mask position. We construct factually incorrect\nsummaries by replacing the mask with the lower ranked candidates. Finally, we train a sequence-to-sequence model\nfor fact correction using the synthetically constructed data.\nthat generalize over a wider range of factual errors\n(example in Figure 1) in generated summaries from\ndiverse summarization model types.\nWe propose FACT EDIT —a novel approach to\npost-editing text, to control for content factuality in\ngenerated summaries. Rather than manually defin-\ning a list of heuristic errors, it incorporates a new\nalgorithm to generate adversarial (non-factual) ex-\namples using infilling language models (Donahue\net al., 2020). We use lower ranked beam-search\ncandidates from the language model as a source\nfor potentially factually-incorrect summary facts,\nthereby producing a set of plausible, likely, and\nfluent, incorrect synthetic summaries for a partic-\nular correct reference summary. In this way, we\nleverage the capabilities of large language models\nto produce multiple candidates of alternative, er-\nroneous summaries. These examples, along with\nfactually correct references, are then used to train\na sequence-to-sequence fact-correction model that\naims at generating a factually consistent version of\nthe candidate summary (§2).\nWe evaluate FACT EDIT on two datasets -\nCNN/DailyMail (Hermann et al., 2015) and XSum\n(Narayan et al., 2018) and across nine summariza-\ntion models with the FRANK benchmark (Pagnoni\net al., 2021) for evaluating various categories of\nfactual errors in generated summaries (§3). The\ntwo summarization datasets represent varied distri-\nbutions of factual errors in models trained on them\nand hence constitute a good test bed to evaluate\nthe generalizability of our model. We show that\nFACT EDIT substantially improves factuality scores\nacross two metrics - Ent-DAE (Goyal and Durrett,\n2021) and FactCC (Kryscinski et al., 2020). On the\nEnt-DAE metric, FACT EDIT improves results by\n∼11 points (CNN/DM) and ∼31 points (XSum),\nand on the FactCC metric we show improvements\nof ∼6 points (CNN/DM) and ∼24 (XSum) points\non average across models (§4). Further, our anal-\nysis shows that FACT EDIT effectively corrects di-\nverse error categories without the need for special\nheuristics or annotations (§5). An important ap-\nplication of FACT EDIT is to audit summarization\nsystems and facilitate their reliability.\n2 Model\nAssume a summarization model trained to process\na document d and generate a coherent and fluent\nsummary2 g′ which has been shown to often mis-\nrepresent facts from the document. FACT EDIT is\n2We denote incorrect input (to fact correction model) sum-\nmaries using ′ and corrected output (from fact correction\nmodel) without the ′ throughout this paper. For E.g: g′ is in-\ncorrect summary, r′ is the incorrect reference summary while\ng is the corrected summary and r′ is the corrected reference\nsummary.\n9819\na fact correction model MC which takes the gener-\nated summary g′ and document d, identifies factual\nerrors and generates a rewritten summary g by cor-\nrecting them (as outlined in Figure 2).\nWe present an adversarial data generation ap-\nproach which leverages the power of pre-trained\nlanguage models to produce fluent and complex\nfactually incorrect summaries. We train an infill-\ning language model MI using documents from\nsummarization training data and use the model to\nintroduce diverse factual errors in sentences from\nthem (§2.1). Using the trained model, we intro-\nduce factual errors in reference summaries of the\ntraining data r producing an incorrect summary\nr′ resulting in a synthetic dataset {r′, r, d}train of\nerroneous summaries mapped to their corrected\nversions (pink section in Figure 2). We train a\nsequence-to-sequence model MC for factual er-\nror correction using the generated synthetic data\n(§2.2). Finally, we use the trained correction model\nto rewrite model generated summaries g′ produc-\ning a corrected version g (§2.3 - green section in\nFigure 2).\n2.1 Infilling Data Generator MI\nOur data generation process leverages infilling lan-\nguage models (Donahue et al., 2020) to produce\ncandidates to fill masked phrases in a summary\nsentence. We mask parts of the input and use the\ninfilling model to generate multiple candidates for\nthe masked position. We then use lower order beam\ncandidates as potential incorrect candidates to gen-\nerate an incorrect version of the input. We hypoth-\nesize that, given the relevant context of a source\ndocument, a strong language model generates rel-\nevant and factual sequences at higher probabili-\nties, compared to lower probability sequences. For\nthe infilling model, we hypothesize that the lower\nranked candidates are often alternative phrases of\nsimilar types (in case of entities) or parts-of-speech\nwhich are plausible but often not factually correct.\nMotivated by prior work (Goyal and Durrett, 2020)\nusing lower ranked beam search candidates as a\nsource for adversarial data, we use the lower ranked\ncandidates to construct erroneous summaries from\nreference summaries.\nTraining: Our infilling model MI is trained to\ntake a masked sentence smasked and its relevant\ncontext ctx as input and generate a correct phrase to\nfill in the masked span. To trainMI, we construct a\ndataset using documents d from the training data of\nexisting summarization datasets. For each sentence\ns in the first-k (k=5) positional sentences of a doc-\nument d, we identify the subjects, objects and rela-\ntions {sub, obj, rel} in them using OpenIE (Banko\net al., 2007). By iteratively masking each phrase p\nin {sub,obj,rel}, we create a masked query smasked\nand its corresponding context ctx by removing the\nmasked sentence from the document, resulting in\nour training data {smasked, p, ctx}, where p is the\nmasked span text. We train a sequence-to-sequence\nmodel MI on this data which takes smasked [SEP]\nctx as input and learns to generate p as the out-\nput. We intentionally use only sentences from the\ndocument as masked queries and do not use sen-\ntences from the reference summaries, to ensure\nthat the model does not memorize phrases from\nthe references. Thus, when applied to unseen ref-\nerence sentences during inference, the model will\nproduces richer beam search candidates.\nAdversarial Data Generation: We use the\ntrained infilling model to generate the synthetic\ndataset for fact correction using the document\nreference pairs {d, r}train from the summariza-\ntion training data. For each sentence in the ref-\nerence sr, we use OpenIE to extract {sub, obj, rel}\nand iteratively mask one phrase at a time to con-\nstruct masked sentences smasked from the refer-\nences. We provide this masked reference summary\nsentence and document d as input to the model\nand perform beam-search decoding for generation.\nWe then consider lower ranked beam candidates\n(rank=[5,15])3 as non-factual alternatives for the\ncorresponding masked phrase. We then use these\ncandidates as the replacements for the mask produc-\ning an erroneous summary r′. Running this on the\n{d, r}train training data, we construct a synthetic\ndata {r′, r, d}train of factually incorrect summaries\npaired with their correct version where r′ and r dif-\nfer by an incorrect phrase. To train the model to\nnot perform any corrections on factual summaries,\nwe keep original reference summaries for 20% of\nthe data points (r′ = r).\n2.2 Fact Correction Model MC\nUsing the parallel data {r′, r, d}train produced by\nthe above infilling method, we train models for\nfactual error correction. In contrast to prior work\n3We chose this range of ranks based on a manual analysis\nof 500 generated adversarial examples where our method\nproduced factually incorrect replacements over 90% of the\ntime.\n9820\nwhich used pointer based models to copy phrases\nfrom the source document, we use a sequence-to-\nsequence model like BART (Lewis et al., 2020) to\npreserve the abstractive content in the input. The\nmodel MC is trained with an erroneous reference\nsummary sentence sr′ produced by the infilling\ndata generator and the corresponding document d\nas input and the correct reference summary sen-\ntence sr as output. A straightforward option is to\nprovide sr′ , dconcatenated as inputs to the model.\nBut we hypothesize that providing the right context\ncan help the model better correct the errors. Below\nwe outline input structures that provide better con-\ntext in the input:\nRelevant Supporting Passages: To help the\nmodel better connect the relevant facts in the source\ndocument to the summary sentence being corrected,\nwe experiment with providing only the most rele-\nvant parts of the document as input context instead\nof the entire document. Using a scoring function\n(ROUGE), we identify sentences from the docu-\nment which have high overlap with the generated\nsummary sentence and extract the top-k (k=3 for\nour work) such sentences. We provide these sen-\ntences along with a window of wk (wk=2) sen-\ntences before and after each as the input context to\nthe model.\nSurrounding Summary Context: While simple\nerrors like incorrect entities can be detected and cor-\nrected with only the context of the current sentence\nbeing corrected, more complex discourse level er-\nrors like incorrect pronouns require the context of\nthe rest of the sentences of the summary. To enable\nthis, we additionally give the complete generated\nsummary (other sentences from the summary) as\nadditional context. For single sentence summaries\nlike headline generation, this does not change the\noriginal setting, but for longer summaries this set-\nting helps with discourse level errors.\nIn essence, our model MC takes the input as Incor-\nrect Reference Sentence (sr′ ) [SEP] Full Reference\nSummary (r′) [SEP] Relevant Passages and gener-\nates the corrected summary r as output.\n2.3 Inference\nOur trained fact correction model MC can be di-\nrectly applied to any model-generated summaries\ng′, without access to the underlying model. For\neach sentence in a generated summary, we iden-\ntify the relevant passages using ROUGE and pro-\nvide it as an input to the model (in the form Gen-\nerated Summary Sentence (sg′ ) [SEP] Generated\nFull Summary (g′) [SEP] Relevant Passages).\n3 Experiments and Data\n3.1 Datasets\nWe use two news summarization datasets CNN-\nDailyMail (Hermann et al., 2015) and XSum\n(Narayan et al., 2018). The two datasets have\nbeen extensively studied for the factual consistency\nin their generated summaries across a variety of\nmodels (Goodrich et al., 2019; Cao et al., 2018).\nReference summaries from CNN/DM are longer,\nhaving on average three sentences, and more ex-\ntractive in nature. XSum on the other hand has\nshorter, single sentence summaries and is signifi-\ncantly more abstractive in nature. The summaries\nin these datasets are qualitatively different, and\nhence models trained on the two datasets present\nvaried levels of challenges in maintaining factual\nconsistencies.\nPrior work have studied summaries generated\nfrom different language models and characterized\nthe factual errors in them (Pagnoni et al., 2021).\nGenerated summaries on the CNN/DM dataset are\nmore extractive in nature and hence are more fac-\ntual ( ∼70% of summaries are factual) than the\nmore abstractive generated summaries of XSum\n(∼20% of summaries are factual). The longer sum-\nmaries in CNN/DM display discourse level incon-\nsistencies while summaries from XSum often hallu-\ncinate content which is not supported by the source\ndocument. Hence, the two datasets present a var-\nied setting for evaluating the efficacy of our model\nacross different kinds of errors. For our main evalu-\nation, we evaluate the overall capability of FACT E-\nDIT in correcting errors in summaries generated by\na BART model.\nWe further evaluate our model on the FRANK\nbenchmark (Pagnoni et al., 2021) which contains\ngenerated summaries obtained using multiple lan-\nguage models for both datasets annotated with hu-\nman judgements on their factuality and the category\nof factual error. As different language models have\ndifferent distribution of factual error types, this\nevaluation helps us study the generalizability of\nFACT EDIT in correcting errors across them.4 For\nthe CNN/DM dataset, it contains model outputs\nfrom a LSTM Seq-to-Seq model (S2S) (Rush et al.,\n4As the benchmark has publicly available model outputs,\nthe summaries across different datasets are from different\nmodels owing to their availability.\n9821\n2015), a Pointer-Generator Network (PGN) model\n(See et al., 2017), a Bottom-Up Summarization\n(BUS) model (Gehrmann et al., 2018), a Bert based\nExtractive-Abstractive model (BertSum) (Liu and\nLapata, 2019) and a jointly pretrained transformer\nbased encoder-decoder model BART (Lewis et al.,\n2020). For the XSum dataset, it contains model\noutputs from a Topic-Aware CNN Model (Narayan\net al., 2018), a Pointer-Generator Network (PGN)\nmodel, a randomly initialized (TransS2S) (Vaswani\net al., 2017) and one initialized with Bert-Base\n(BertS2S) (Devlin et al., 2019).\n3.2 Experimental Settings and Evaluation\nSetup: We use OpenIE (Banko et al., 2007) to\npre-process each summary and extract subject, ob-\nject, predicate triples for each summary sentence.\nWe use BART-base (Lewis et al., 2020) as our\nsequence-to-sequence model for the infilling based\ndata generator and the fact correction model. Both\nmodels were trained with a batch size of 48, a learn-\ning rate of 3e-5, and warm-up of 1000 for 1 epoch.\nThe maximum input sequence length was 512 and\nmaximum output sequence length was 128. Using\nthe infilling data generator, we generate 1233329\nnegative, 308332 positive examples for CNN/DM\nand 724304 negative, 181076 positive, examples\nfor XSum as training data for fact correction. Mod-\nels were trained on 4 Nvidia GeForce GTX TITAN\nX GPUs and each training run took ∼15 hours. All\nhyperparameters were chosen based on generated\ndev set ROUGE-L (Lin, 2004) on each dataset.\nEvaluation Setup: Evaluating factual consistency\nof generated summaries is challenging, with rela-\ntively recent metrics developed to detect it. These\nmetrics unfortunately do not correlate highly with\nhuman judgements yet. We therefore evaluate\nour model using two metrics - FactCC (Kryscin-\nski et al., 2020) and Ent-DAE (Goyal and Durrett,\n2021); each captures different error types. FactCC\nis a binary classifier, trained on a synthetic, heuris-\ntic error dataset, which is better at detecting simple\nsemantic errors like incorrect entities or numbers.\nEnt-DAE is a classifier trained on synthetic data\nconstructed using the dependency structure of the\ntext. In addition to semantic errors, it is better\nat detecting more complex discourse-level errors\n(Pagnoni et al., 2021). We also report ROUGE\n(Lin, 2004) to evaluate if our model maintains the\nfluency of summaries. While ROUGE is less corre-\nlated with factuality (Pagnoni et al., 2021; Maynez\net al., 2020), it helps evaluate if the corrected sum-\nmary is fluent and aligned with the reference sum-\nmary. However, with factual corrections of outputs\nwe expect small drops in ROUGE, since generation\nmodels were specifically optimized to maximize\nROUGE presumably at the expense of factuality.\nOur evaluation has two settings: i) FACT EDIT\n- correct all generated summaries in the test set\nand ii) FACT EDIT + FactCC Filter (FF) - using\nthe FactCC metric we identify factually incorrect\nsummaries, and only correct the incorrect ones.\nBaselines: We compare our approach with (Cao\net al., 2020) as the baseline. The baseline uses a\nheuristic set of rules proposed by Kryscinski et al.\n(2020) to introduce simple errors (Entity, Num-\nber, Date, and Pronoun) in reference summaries\nand trains a BART-base model for error correction.\nComparing our model with (Cao et al., 2020) helps\nus evaluate the benefit of our Infilling LM based\nadversarial data generator. 5\n4 Results\n4.1 Factuality Results\nWe first evaluate FACT EDIT ’s ability to correct\nerrors in summaries generated by a BART-base\nsummarization model on the entire test set. We\nfirst generate summaries using a BART-base model\nfinetuned on each dataset and then provide the gen-\nerated summaries and their corresponding source\ndocuments as inputs to FACT EDIT for correction.\nTable 1 shows results for this experiment. Our\nresults show that correcting factual errors using\nour model improves the factuality results. The\nbaseline model performs poorly with the FactCC\nmetric showing lower scores than the BART model\ngenerated summaries, especially in the more ab-\nstractive XSum setting. The DAE metric for the\nbaseline model is slightly lower than the BART\nmodel scores in the CNN/DM setting and has no\nimprovement in the XSum setting showing that it\ndoes not perform corrections on complex errors.\nThese results confirm our hypothesis that the base-\nline model trained on adversarial data based on\nheuristic errors does not transfer well to real errors\nin model generated summaries. In contrast, our\nmodel improves both metrics across both datasets.\nOn the more challenging XSum dataset, our model\nhas a ∼17 point improvement on FactCC and ∼0.1\n5While (Dong et al., 2020) is also a factual error correction\nmethod, we were unable to reproduce it as no public code was\navailable.\n9822\nDataset Method R1 R2 RL FactCC Ent-DAE\nCNN/DM\nBart (Lewis et al., 2020) 44.07 21.08 41.01 75.78 74.85\nCao et al. (2020) 42.72 20.59 39.92 49.98 74.83\nFACT EDIT 42.17 20.22 39.37 75.49 75.71\nFACT EDIT + FactCC Filter (FF) 42.53 20.48 39.74 76.03 75.36\nXSum\nBart (Lewis et al., 2020) 34.71 15.04 27.40 21.93 20.03\nCao et al. (2020) 33.64 14.71 26.49 7.01 20.03\nFACT EDIT 33.58 14.68 26.71 23.91 20.13\nFACT EDIT + FactCC Filter (FF) 33.58 14.68 26.71 23.91 20.13\nTable 1: FACT EDIT performance for correcting BART outputs (best performance in bold). FACT EDIT ourperforms\nfactuality related baselines on FactCC and DAE scores, while maintaining competitive summarization quality.\nMethod RL FactCC Ent-DAE\nCNN/DM\nBart 41.53 46.29 72.57\nFACT EDIT 37.73 42.29 78.86\nFACT EDIT (FF) 37.73 53.14 81.71\nBertSum 38.74 58.86 82.29\nFACT EDIT 35.6 55.43 79.43\nFACT EDIT (FF) 35.6 61.71 82.86\nBUS 38.59 49.71 70.28\nFACT EDIT 33.79 48.00 76.00\nFACT EDIT (FF) 33.79 56.57 80.00\nPointGen 35.62 80.57 93.14\nFACT EDIT 32.54 75.43 90.29\nFACT EDIT (FF) 32.54 78.29 90.86\nSeq2Seq 27.15 19.43 29.71\nFACT EDIT 24.78 23.43 48.00\nFACT EDIT (FF) 24.78 24.00 54.29\nXSum\nBertS2S 29.05 22.29 05.71\nFACT EDIT 28.93 50.43 40.00\nFACT EDIT (FF) 28.95 50.43 40.00\nTConvS2S 25.69 17.71 04.00\nFACT EDIT 25.64 47.16 29.14\nFACT EDIT (FF) 25.64 47.16 29.14\nPointGen 23.12 18.29 00.57\nFACT EDIT 23.02 43.75 32.00\nFACT EDIT (FF) 23.04 43.75 32.00\nTranS2S 23.93 18.86 2.86\nFACT EDIT 23.86 31.73 36.00\nFACT EDIT (FF) 23.86 31.73 36.00\nTable 2: Performance of FACT EDIT across different\nmodel generated summaries in the FRANK setting. Best\nperformance is indicated in Bold. FACT EDIT model\nvastly improves factuality across multiple models on\nboth FactCC and DAE scores.\nimprovement on DAE over the baseline model.\nThe BART generated summaries on CNN/DM are\n∼70% factual and hence using the FactCC Filter to\ncorrect only non-factual summaries helps improve\nresults on FactCC. As XSum has more than 80%\nnon-factual summaries, the FactCC filter does not\nchange results and correcting all generated sum-\nmaries is beneficial. In Table 6 we present exam-\nples of corrections made by FACT EDIT and present\na discussion in §A.\nPrior works have shown that improving fac-\ntual consistency in summaries leads to a drop in\nROUGE scores (Maynez et al., 2020; Cao and\nWang, 2021; Cao et al., 2020). Our ROUGE re-\nsults do not drop significantly and are consistent\nwith prior work. These results show that our model\ndoes not significantly change the summaries and\nthe corrected summaries contain the relevant infor-\nmation w.r.t. to the source.\n4.2 Factuality Results across Model Types\nTable 2 shows results of usingFACT EDIT to correct\nsummaries generated by different types of language\nmodels using the FRANK benchmark (Pagnoni\net al., 2021). We provide the generated summaries\ncollected in the benchmark along with their source\ndocument as input to our trained fact corrector.\nThis setting evaluates the generalizability of our\nadversarial training data in handling different error\ndistributions from different summarization models.\nOur results show our model significantly improves\nthe factuality in generated summaries across 8 out\nof 9 test models. The FactCC Filter helps improves\nresults in CNN/DM setting but does not change\nresults in XSum similar to results in §4.1. In the\nmore extractive CNN/DM setting, fact correction\nimproves FactCC scores by ∼5.3 points and DAE\nscores by ∼10.9 points on average across mod-\nels. In the more challenging and abstractive XSum\ndataset, we improve FactCC scores by ∼24 points\nand DAE scores by ∼31 points on average. Our\nresults show that our model trained using Infilling\nLM based adversarial data is able to generalize\nand correct errors in generated summaries across\ndifferent model types. Further, the significant im-\nprovement in XSum suggests that using LMs to\ngenerate factually incorrect candidates produces\nrich negative examples which help correct errors in\n9823\nFigure 3: Performance of FactEdit across different error categories in comparison to baseline (Cao et al., 2020)\n. FACT EDIT improves the percentage of factual summaries across diverse types of factual errors.\nmore abstractive summaries.\nPretrained models like BART, BertSum and\nBertS2S have improved generation capabilities and\nmake lesser mistakes in generating the right en-\ntity or predicate and more mistakes in discourse\nstructuring (Pagnoni et al., 2021). FACT EDIT cor-\nrespondingly shows larger improvements in DAE\nscores than FactCC scores in these pretrained mod-\nels. The Pointer-Generator model being highly ex-\ntractive in nature scores highly in factuality metrics\nin the CNN/DM setting and FACT EDIT reduces\nresults in this setting showing that our model is\nnot beneficial in copy-based model settings. On\nthe other hand, in the XSum setting, the base\nPointer-Generator model scores poorly and cor-\nrecting factual errors in them improves factuality\nscores. Non-pretrained sequence-to-sequence mod-\nels like Seq2Seq and TransSeq2Seq score poorly in\nboth ROUGE and Factuality scores due to their lim-\nited language generation capabilities. By correct-\ning factual errors in them, we improve factuality\nmetrics significantly without changes in ROUGE,\nindicating that the gains are due to fact correction\nand not just rewriting the summary using a strong\nlanguage model.\n5 Analysis\n5.1 Performance across Error Categories\nThe FRANK benchmark proposes a typology of\nthree coarse categories of error types and collects\nhuman annotations on the error category: i) Se-\nmantic Frame Errors - This category covers factual\nerrors in a sentence due to incorrect entity or pred-\nicate being generated ii) Discourse Errors - This\ncovers discourse level factual errors like incorrect\npronouns or sentence ordering iii) Content Verifi-\nability Errors - This category is for errors whose\nfactuality cannot be judged either due to grammati-\ncal errors or hallucinated content. We evaluate our\nmodel on its ability to correct different types of\nerrors. We use the generated summaries from the\nbest pretrained model in FRANK for each dataset -\nBART for CNN/DM and BertS2S for XSum. For\neach subset of summaries of a particular error type,\nwe correct the summaries using FACT EDIT and re-\nport the percentage of factual summaries in the out-\nput as predicted by Ent-DAE. We compare FACT E-\nDIT with the baseline to study whether our model\nimproves error correction for each type.\nFrom Figure 3, we see that across both datasets\nFACT EDIT increases the percentage of factual sum-\nmaries across all three error categories, showing\nthat the data generation process in FACT EDIT can\ngeneralize across multiple error types without the\nneed for special heuristics or annotations. We see\nthe largest improvements in the Semantic Frame\nError category with an increase of ∼8 points on\nCNN/DM and ∼13 points on XSum. On the more\ncomplex Discourse Errors we see an improvement\nof ∼5 points on both datasets. Finally, on Content\nVerifiability Errors, we see a ∼8 point improve-\nment on CNN/DM and ∼2 point improvement on\nXSum. XSum has a high proportion of hallucina-\ntion errors and our results highlight the challenge\nin correcting this error type.\n5.2 Transferrability across Datasets\nIt is not always feasible to train specialized fact\ncorrection models for each dataset or style of sum-\nmaries. While CNN/DM and XSum contain doc-\numents of the news domain, they both have differ-\nent summary characteristics. Certain applications\nmight benefit from a single model which can gen-\neralize to different summary styles. We evaluate\n9824\nMethod FactCC Ent-DAE\nBertS2S 22.29 05.71\nFACT EDIT (FF) - CNN Model 33.71 22.29\nTConvS2S 17.71 04.00\nFACT EDIT (FF) - CNN Model 30.29 22.29\nPointGen 18.29 00.57\nFACT EDIT (FF) - CNN Model 28.57 19.43\nTranS2S 18.86 2.86\nFACT EDIT (FF) - CNN Model 18.86 21.14\nTable 3: Transfer results of FACT EDIT . FACT EDIT\ntrained using CNN/DM data transfers well to summaries\ngenerated for documents in XSum.\nMethod Fluency Factuality\nCao et al. (2020) 4.58 3.10\nFACT EDIT 4.75 3.33\nTable 4: Results of Human Evaluation on Fluency and\nFactuality of corrected summaries. Human judges rate\nsummaries corrected by FACT EDIT higher in fluency\nand factuality than the baseline.\nthe ability of FACT EDIT trained on CNN/DM data\n(FACT EDIT FF - CNN Model) to transfer and cor-\nrect summaries generated for XSum documents\nusing FRANK benchmark. Table 3 shows results\nfor this experiment. Our results show significant\nimprovement in factuality scores across all model\ntypes in this setting, showing that our data gener-\nation process produces rich and diverse factually\nincorrect examples which can generalize to factual\nerrors in other data settings. By using only the\nsource documents, our training data is agnostic of\nthe styles, lengths and characteristics of reference\nsummaries and hence is able to generalize to the\nheadline style abstractive summaries of XSum.\n5.3 Human Evaluation\nTo further study whether the factuality corrections\nperformed by our model align with human expec-\ntations of automated summaries, we conduct a hu-\nman study. Two annotators evaluated 20 randomly\nsampled summaries generated from the test set of\nthe XSum dataset using the BertS2S model and\ncorrected by FACT EDIT and the baseline. The an-\nnotators were shown the entire source document\nand one corrected summary at a time and asked to\nrate the fluency and factuality of the summary on a\n1-5 Likert scale. In manual evaluation, annotators\nrated FACT EDIT an average of 3.3 on factuality\nand 4.8 on fluency, compared to the baseline which\nwas rated 3.1 and 4.6 scores respectively, showing\nthat FACT EDIT improves on both factuality and\nfluency.\nMethod FactCC E-DAE\nCNN/DM\nFACT EDIT 76.03 75.36\nFACT EDIT -SummCtxt 75.73 74.23\nFACT EDIT -SummCtxt-RelevPass 75.89 75.03\nXsum\nFACT EDIT 23.91 20.13\nFACT EDIT -SummCtxt 22.89 20.06\nFACT EDIT -SummCtxt-RelevPass 23.48 20.08\nTable 5: Results of Ablation study with components of\nfact correction pipeline removed. SummCtxt includes\nthe generated summary as additional context. RelevPass\nincludes relevant passages from the source as additional\ncontext. FACT EDIT setup ourperforms the ablated ver-\nsions on FactCC and DAE scores.\n5.4 Ablation Study\nOur model corrects each sentence in a summary\ngiven context of the rest of the summary and rele-\nvant passages in the source document. We ablate\nthis setup by removing parts of the context one at\na time. In Table 5 we present the results. We ob-\nserve a a drop in results when using the entire sum-\nmary as context (-RelevPass) and when removing\nthe context of the summary in which the sentence\noccurs (-SummCtxt). Our results show the impor-\ntance of having the appropriate context to enable\nthe model to perform fact correction well.\n6 Related Work\nFactuality Evaluation Standard n-gram based met-\nrics do not correlate well with human judgements\nof factuality and are unsuitable for evaluating fac-\ntuality (Kryscinski et al., 2019; Fabbri et al., 2020).\nSeveral automated metrics were proposed to detect\nfactual errors in generated summaries. They primar-\nily fall in two paradigms—Entailment based and\nQA based metrics. Goodrich et al. (2019); Kryscin-\nski et al. (2020); Maynez et al. (2020); Goyal and\nDurrett (2021) model factuality as an entailment\nverifying whether the summary is entailed by the\nsource. Lee et al. (2022b) use similar masked infill-\ning to generate training data for such metrics. QA\nmodels can be used to answer questions about the\ndocument, separately using the article and the out-\nput summary as context and compare the answers\nto score the factuality of summaries (Durmus et al.,\n2020; Wang et al., 2020). To evaluate these metrics,\nrecent work collec human judgements for factuality\n(Fabbri et al., 2020; Maynez et al., 2020; Pagnoni\net al., 2021). Additionally, (Pagnoni et al., 2021)\nalso obtain annotations on factual error categories,\nwhich we use for our evaluations. This paper con-\n9825\nsiders the problem of improving factuality, not mea-\nsuring it. While this is a different task, it is related:\ne.g., measuring the number of corrections made by\nFACT EDIT might be useful as a factuality measure.\nImproving Factuality of Summaries: There are\ntwo paradigms of work to ensure generated sum-\nmaries are factually consistent: i) imposing factual-\nity constraints during training or generation and ii)\npost-editing generated summaries to correct factual\nerrors. Wan and Bansal (2022) add factuality con-\ntraints during pretraining by using factually consis-\ntent summaries. Model designs and factuality spe-\ncific objectives help optimize for factuality during\ntraining (Gabriel et al., 2019; Cao and Wang, 2021;\nDong et al., 2022; Rajagopal et al., 2022). During\ndecoding beam search candidates can be ranked\nbased on factuality measures (King et al., 2022;\nZhao et al., 2020). Work on correcting factual er-\nrors post generation is relatively nascent. Cao et al.\n(2020) and Lee et al. (2022a) train fact correction\nmodels on synthetic data based on heuristic errors\nwhich we show is less effective than LM based er-\nror generation (Table 1). Dong et al. (2020) use a\nQA model to replace phrases in the summary with\nspans in the source text. This requires multiple\ninference iterations, making them very expensive\nfor correction. In contrast our approach corrects\nerrors in one iteration, making it a faster and more\npractical approach for error correction. Tangen-\ntially, work on correcting errors in reference sum-\nmaries to make the training data more reliable has\nalso been explored Adams et al. (2022); Wan and\nBansal (2022). In dialog generation, Gupta et al.\n(2021) explore using mask-fill approaches to gen-\nerate synthetic data for response ranking, showing\nthat using language models to generate adversarial\ndata might be applicable beyond summarization.\nConclusion\nWe present an adversarial data generation process\nto generate rich synthetic data for a post editing\nmodel, which can be applied to correct factual er-\nrors generated summaries. Our data generation\nprocess leverages Infilling Language Models to\nproduce alternative candidate summaries. Using\nthe generated data, we train models to rewrite\nsummaries by correcting factual errors in them.\nThrough extensive experiments across two datasets\nand nine models, we show that our fact corrector\nmodel improves the factual consistency of the sum-\nmaries, making them more reliable.\nLimitations\nOur model is trained to rewrite generated sum-\nmaries by correcting factual errors in them. A limi-\ntation in our current setup is accurate detection of\nfactual errors. We rely on off-the-shelf metrics for\nidentifying summaries with factual errors to correct.\nOur model does not perform detection and correc-\ntion together and often rewrites correct summaries\nas well if fed to the model. Therefore for settings\nlike CNN/DM, it’s beneficial to filter summaries\nusing a factuality metric before giving summaries\nto our model as input. As our fact corrector is a\nsequence-to-sequence model, it could potentially\nintroduce new factual errors in the summaries. It is\nessential to use factually detectors to ensure sum-\nmaries are factual before real world usage of any\ncorrected summary.\nEthical Considerations\nState-of-the-art language generation models, in-\ncluding summarization, are not yet powerful\nenough to facilitate fine-grained control over gener-\nated content. This leads to problems with content\nfidelity and safety; our work aims to ameliorate\nissues related to factual reliability of the models.\nHowever, existing approaches, including ours, can-\nnot guarantee this yet. Furthermore, there is a risk\nof dual use, since the same techniques can be used\nto post-edit models to produce non-factual, harm-\nful content to mislead, impersonate, or manipulate\nopinions. Future research should focus on devel-\noping better defenses methods against mis-using\nlanguage generators maliciously.\nAcknowledgements\nWe would like to thank Lucille Njoo, Xiaochuang\nHan, Sachin Kumar, Dheeraj Rajagopal and other\nmembers of the Tsvetshop Lab for their valuable\nfeedback on this work. This material is based upon\nwork supported by the DARPA CMO under Con-\ntract No. HR001120C0124. Any opinions, findings\nand conclusions or recommendations expressed\nin this material are those of the author(s) and do\nnot necessarily state or reflect those of the United\nStates Government or any agency thereof. Y .T. also\ngratefully acknowledges support from NSF CA-\nREER Grant No. IIS2142739 and Alfred P. Sloan\nFoundation Fellowship.\n9826\nReferences\nGriffin Adams, Han-Chin Shing, Qing Sun, Christo-\npher Winestock, Kathleen McKeown, and Noémie\nElhadad. 2022. Learning to revise references\nfor faithful summarization. arXiv preprint\narXiv:2204.10290.\nMichele Banko, Michael J. Cafarella, Stephen Soder-\nland, Matt Broadhead, and Oren Etzioni. 2007. Open\ninformation extraction from the web. In Proceedings\nof the 20th International Joint Conference on Artif-\nical Intelligence, IJCAI’07, pages 2670–2676, San\nFrancisco, CA, USA. Morgan Kaufmann Publishers\nInc.\nMeng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit\nCheung. 2020. Factual error correction for abstrac-\ntive summarization models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6251–6258,\nOnline. Association for Computational Linguistics.\nShuyang Cao and Lu Wang. 2021. Cliff: Con-\ntrastive learning for improving faithfulness and fac-\ntuality in abstractive summarization. arXiv preprint\narXiv:2109.09209.\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.\nFaithful to the original: Fact aware neural abstractive\nsummarization. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 4784–4791. AAAI Press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nChris Donahue, Mina Lee, and Percy Liang. 2020. En-\nabling language models to fill in the blanks. arXiv\npreprint arXiv:2005.05339.\nYue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie\nChi Kit Cheung, and Jingjing Liu. 2020. Multi-\nfact correction in abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9320–9331, Online. Association for Computa-\ntional Linguistics.\nYue Dong, John Wieting, and Pat Verga. 2022. Faithful\nto the document or to the world? mitigating hallu-\ncinations via entity-linked knowledge in abstractive\nsummarization. arXiv preprint arXiv:2204.13761.\nMarkus Dreyer, Mengwen Liu, Feng Nan, Sandeep\nAtluri, and Sujith Ravi. 2021. Analyzing\nthe abstractiveness-factuality tradeoff with nonlin-\near abstractiveness constraints. arXiv preprint\narXiv:2108.02859.\nEsin Durmus, He He, and Mona Diab. 2020. FEQA: A\nquestion answering evaluation framework for faith-\nfulness assessment in abstractive summarization. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5055–\n5070, Online. Association for Computational Lin-\nguistics.\nAlexander R Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2020. Summeval: Re-evaluating summariza-\ntion evaluation. arXiv preprint arXiv:2007.12626.\nSaadia Gabriel, Antoine Bosselut, Jeff Da, Ari Holtz-\nman, Jan Buys, Kyle Lo, Asli Celikyilmaz, and Yejin\nChoi. 2019. Discourse understanding and factual con-\nsistency in abstractive summarization. arXiv preprint\narXiv:1907.01272.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4098–4109, Brussels, Belgium. Association for Com-\nputational Linguistics.\nBen Goodrich, Vinay Rao, Peter J. Liu, and Mohammad\nSaleh. 2019. Assessing the factual accuracy of gener-\nated text. In Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery\n& Data Mining, KDD 2019, Anchorage, AK, USA,\nAugust 4-8, 2019, pages 166–175. ACM.\nTanya Goyal and Greg Durrett. 2020. Evaluating factu-\nality in generation with dependency-level entailment.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pages 3592–3603, Online.\nAssociation for Computational Linguistics.\nTanya Goyal and Greg Durrett. 2021. Annotating and\nmodeling fine-grained factuality in summarization.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1449–1462, Online. Association for Computa-\ntional Linguistics.\nPrakhar Gupta, Yulia Tsvetkov, and Jeffrey P. Bigham.\n2021. Synthesizing adversarial negative responses\nfor robust response ranking and evaluation. In FIND-\nINGS.\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Proceedings of the Advances in\nNeural Information Processing Systems 28: Annual\nConference on Neural Information Processing Sys-\ntems 2015, December 7-12, 2015, Montreal, Quebec,\nCanada, pages 1693–1701.\n9827\nYichong Huang, Xiachong Feng, Xiaocheng Feng, and\nBing Qin. 2021. The factual inconsistency problem\nin abstractive text summarization: A survey. arXiv\npreprint arXiv:2104.14839.\nMatthew Hutson et al. 2021. Robo-writers: the\nrise and risks of language-generating ai. Nature,\n591(7848):22–25.\nDaniel King, Zejiang Shen, Nishant Subramani,\nDaniel S Weld, Iz Beltagy, and Doug Downey. 2022.\nDon’t say what you don’t know: Improving the con-\nsistency of abstractive summarization by constraining\nbeam search. arXiv preprint arXiv:2203.08436.\nWojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-\nCann, Caiming Xiong, and Richard Socher. 2019.\nNeural text summarization: A critical evaluation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 540–551, Hong\nKong, China. Association for Computational Linguis-\ntics.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346, Online. Association for Computa-\ntional Linguistics.\nHwanhee Lee, Cheoneum Park, Seunghyun Yoon,\nTrung Bui, Franck Dernoncourt, Juae Kim, and Ky-\nomin Jung. 2022a. Factual error correction for ab-\nstractive summaries using entity retrieval. arXiv\npreprint arXiv:2204.08263.\nHwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran\nLee, and Kyomin Jung. 2022b. Masked summa-\nrization to generate factually inconsistent summaries\nfor improved factual consistency checking. arXiv\npreprint arXiv:2205.02035.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807, Brussels, Bel-\ngium. Association for Computational Linguistics.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with FRANK: A benchmark for\nfactuality metrics. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4812–4829, Online. As-\nsociation for Computational Linguistics.\nDheeraj Rajagopal, Siamak Shakeri, Cicero\nNogueira dos Santos, Eduard Hovy, and Chung-\nChing Chang. 2022. Counterfactual data augmenta-\ntion improves factuality of abstractive summarization.\narXiv preprint arXiv:2205.12416.\nPriyanka Ranade, Anupam Joshi, and Tim Finin. 2021.\nStudy shows ai-generated fake reports fool experts.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 379–389, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the Advances in Neural\nInformation Processing Systems 30: Annual Confer-\nence on Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA, pages\n5998–6008.\nDavid Wan and Mohit Bansal. 2022. Factpega-\nsus: Factuality-aware pre-training and fine-tuning\nfor abstractive summarization. arXiv preprint\narXiv:2205.07830.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of the\n9828\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5008–5020, Online. Asso-\nciation for Computational Linguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages\n11328–11339. PMLR.\nZheng Zhao, Shay B Cohen, and Bonnie Webber. 2020.\nReducing quantity hallucinations in abstractive sum-\nmarization. arXiv preprint arXiv:2009.13312.\nA Qualitative Analysis\nIn Table 6 we present examples of corrections made\nby our models. In the first example, the generated\nsummary contains an entire sentence which is un-\nsupported by the source document. FACT EDIT\nchooses to remove the entire sentence instead of\nrewriting or correcting it. In the second example,\nthe generated summary contains hallucinated enti-\nties of reporter names which are not present in the\nsource. Here, the FACT EDIT rewrites by replacing\nthe incorrect entity with the correct ones.\n9829\nSource Generated Summary Corrected Summary\nAn endangered bottlenose dolphin has died off the coast of SpainIt had lost its mother and become stranded in shallow watersoff the coast of Mojacar in Andalucia. Tourists then began touchingand photographing the animal rather than seeking help for it.Equinac, a local NGO, said \"selfishness\" caused the dolphin\"suffering and stress.\" The organisation, which works to protectmarine wildlife in the area, wrote in a Facebook post (in Spanish):\"The animal was subjected to the curious who wanted to photographand touch it.\" \"These animals are highly protected; to disturb them, toharm them, to manipulate them and to harass them is prohibitedby law, and we always ask for respect and consideration.\"\nAn endangered bottlenose dolphin has diedoff the coast of Spain after being harassed by touristswho wanted to take pictures of it, an animal welfaregroup has said in a Facebook post, in part becauseof their \"selfish\" behaviour.774,000 people havevisited the animal since it was stranded.\nAn endangered bottlenose dolphin has diedoff the coast of Spain after being harassed by touristswho wanted to take pictures of it, an animal welfaregroup has said in a Facebook post, in part becauseof their \"selfish\" behaviour.\nBBC Wales Sport understands the Gills made an enquiry for Byrne, before Edinburgh changed his mind over a player he broughtto Rodney Parade. Edinburgh opted to end the pursuit out of respectfor his old club, who he managed from 2011-2015, so as not to weakentheir battle against relegation. They are currently 20th in League Two,seven points above the relegation zone. Meanwhile, Newporthave confirmed the release of defender Matt Taylor. The 34-yearold joined from Cheltenham in the summerbut made just five appearances for the south Wales club.\"Further to recent discussions, we can confirm that Matt Taylorwill leave the Club with immediate effect having mutuallyagreed to terminate his contract,\" a statement from the club read.\nNewport County manager Justin Edinburgh has endedhis interest in signing Gillingham midfielder Luke Byrne,BBC Wales Sport has learned, after talks between the twoclubs ended in stalemate on Tuesdaynight, reports BBC Radio Wales’ Mark McGheeand BBC Radio Newport’s Steve Davies.\nNewport County manager Justin Edinburgh has endedhis interest in signing Gillingham midfielder Luke Byrne,BBC Wales Sport has learned, after talks between the twoclubs ended in stalemate on Tuesday night,reports BBC Wales Sport.\nTable 6: Examples of corrections made by our fact corrector.\n9830",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9550368785858154
    },
    {
      "name": "Computer science",
      "score": 0.8533841371536255
    },
    {
      "name": "Heuristics",
      "score": 0.7238664031028748
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6440539360046387
    },
    {
      "name": "Natural language processing",
      "score": 0.6201847791671753
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.60286945104599
    },
    {
      "name": "Focus (optics)",
      "score": 0.5487939119338989
    },
    {
      "name": "Language model",
      "score": 0.5415965914726257
    },
    {
      "name": "Hallucinating",
      "score": 0.47944512963294983
    },
    {
      "name": "Heuristic",
      "score": 0.4608258008956909
    },
    {
      "name": "Machine learning",
      "score": 0.3826353847980499
    },
    {
      "name": "Information retrieval",
      "score": 0.3731905221939087
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ],
  "cited_by": 24
}