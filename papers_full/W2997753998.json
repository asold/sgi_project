{
  "title": "Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection",
  "url": "https://openalex.org/W2997753998",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4228079001",
      "name": "Zhao, Guangxiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352364427",
      "name": "Lin, Junyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1700595713",
      "name": "Zhang Zhi-yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221880362",
      "name": "Ren, Xuancheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110308624",
      "name": "Su Qi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106641529",
      "name": "Sun Xu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2888010645",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2896495706",
    "https://openalex.org/W2950858167",
    "https://openalex.org/W3041866211",
    "https://openalex.org/W2907121943",
    "https://openalex.org/W2890964657",
    "https://openalex.org/W2525246036",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2900014366",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2964352247",
    "https://openalex.org/W2886490473",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2946462349",
    "https://openalex.org/W2540419089",
    "https://openalex.org/W2859444450",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W2964190861",
    "https://openalex.org/W2510842514",
    "https://openalex.org/W2755806193",
    "https://openalex.org/W2964269252",
    "https://openalex.org/W2963983719",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2989571009",
    "https://openalex.org/W2888321701",
    "https://openalex.org/W2795151422",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2964302946",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2963123301",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2962729168",
    "https://openalex.org/W2970777192",
    "https://openalex.org/W2964059481",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W1847088711"
  ],
  "abstract": "Self-attention based Transformer has demonstrated the state-of-the-art performances in a number of natural language processing tasks. Self-attention is able to model long-term dependencies, but it may suffer from the extraction of irrelevant information in the context. To tackle the problem, we propose a novel model called \\textbf{Explicit Sparse Transformer}. Explicit Sparse Transformer is able to improve the concentration of attention on the global context through an explicit selection of the most relevant segments. Extensive experimental results on a series of natural language processing and computer vision tasks, including neural machine translation, image captioning, and language modeling, all demonstrate the advantages of Explicit Sparse Transformer in model performance. We also show that our proposed sparse attention method achieves comparable or better results than the previous sparse attention method, but significantly reduces training and testing time. For example, the inference speed is twice that of sparsemax in Transformer model. Code will be available at \\url{https://github.com/lancopku/Explicit-Sparse-Transformer}",
  "full_text": "EXPLICIT SPARSE TRANSFORMER : C ONCENTRATED\nATTENTION THROUGH EXPLICIT SELECTION\nGuangxiang Zhaoâ€ , Junyang Linâ€¡, Zhiyuan Zhangâ€¡, Xuancheng Renâ€¡, Qi Suâ€¡, Xu Sunâ€ â€¡\nâ€ Center for Data Science, Peking University\nâ€¡MOE Key Lab of Computational Linguistics, School of EECS, Peking University\n{zhaoguangxiang,linjunyang,zzy1210,renxc,sukia,xusun}@pku.edu.cn\nABSTRACT\nSelf-attention based Transformer has demonstrated the state-of-the-art perfor-\nmances in a number of natural language processing tasks. Self-attention is\nable to model long-term dependencies, but it may suffer from the extraction\nof irrelevant information in the context. To tackle the problem, we propose a\nnovel model called Explicit Sparse Transformer. Explicit Sparse Transformer\nis able to improve the concentration of attention on the global context through\nan explicit selection of the most relevant segments. Extensive experimental re-\nsults on a series of natural language processing and computer vision tasks, in-\ncluding neural machine translation, image captioning, and language modeling,\nall demonstrate the advantages of Explicit Sparse Transformer in model per-\nformance. We also show that our proposed sparse attention method achieves\ncomparable or better results than the previous sparse attention method, but sig-\nniï¬cantly reduces training and testing time. For example, the inference speed is\ntwice that of sparsemax in Transformer model. Code will be available at https:\n//github.com/lancopku/Explicit-Sparse-Transformer\n1 I NTRODUCTION\nUnderstanding natural language requires the ability to pay attention to the most relevant information.\nFor example, people tend to focus on the most relevant segments to search for the answers to their\nquestions in mind during reading. However, retrieving problems may occur if irrelevant segments\nimpose negative impacts on reading comprehension. Such distraction hinders the understanding\nprocess, which calls for an effective attention.\nThis principle is also applicable to the computation systems for natural language. Attention has been\na vital component of the models for natural language understanding and natural language generation.\nRecently, Vaswani et al. (2017) proposed Transformer, a model based on the attention mechanism\nfor Neural Machine Translation(NMT). Transformer has shown outstanding performance in natural\nlanguage generation tasks. More recently, the success of BERT (Devlin et al., 2018) in natural\nlanguage processing shows the great usefulness of both the attention mechanism and the framework\nof Transformer.\nHowever, the attention in vanilla Transformer has a obvious drawback, as the Transformer assigns\ncredits to all components of the context. This causes a lack of focus. As illustrated in Figure 1,\nthe attention in vanilla Transformer assigns high credits to many irrelevant words, while in Explicit\nSparse Transformer, it concentrates on the most relevant k words. For the word â€œtimâ€, the most\nrelated words should be â€heartâ€ and the immediate words. Yet the attention in vanilla Transformer\ndoes not focus on them but gives credits to some irrelevant words such as â€œhimâ€.\nRecent works have studied applying sparse attention in Transformer model. However, they either add\nlocal attention constraints (Child et al., 2019) which break long term dependency or hurt the time\nefï¬ciency (Martins & Astudillo, 2016). Inspired by Ke et al. (2018) which introduce sparse credit\nassignment to the LSTM model, we propose a novel model called Explicit Sparse Transformer\nwhich is equipped with our sparse attention mechanism. We implement an explicit selection method\nbased on top-kselection. Unlike vanilla Transformer, Explicit Sparse Transformer only pays attention\n1\narXiv:1912.11637v1  [cs.CL]  25 Dec 2019\nFigure 1: Illustration of self-attention in the models. The orange bar denotes the attention score of\nour proposed model while the blue bar denotes the attention scores of the vanilla Transformer. The\norange line denotes the attention between the target word â€œtimâ€ and the selected top-kpositions in the\nsequence. In the attention of vanilla Transformer, â€timâ€ assigns too many non-zero attention scores to\nthe irrelevant words. But for the proposal, the top-klargest attention scores removes the distraction\nfrom irrelevant words and the attention becomes concentrated.\nto the kmost contributive states. Thus Explicit Sparse Transformer can perform more concentrated\nattention than vanilla Transformer.\nWe ï¬rst validate our methods on three tasks. For further investigation, we compare our methods\nwith previous sparse attention methods and experimentally answer how to choose k in a series of\nqualitative analyses. We are surprised to ï¬nd that the proposed sparse attention method can also help\nwith training as a regularization method. Visual analysis shows that Explicit Sparse Transformer\nexhibits a higher potential in performing a high-quality alignment. The contributions of this paper are\npresented below:\nâ€¢We propose a novel model called Explicit Sparse Transformer, which enhances the concen-\ntration of the Transformerâ€™s attention through explicit selection.\nâ€¢We conducted extensive experiments on three natural language processing tasks, including\nNeural Machine Translation, Image Captioning and Language Modeling. Compared with\nvanilla Transformer, Explicit Sparse Transformer demonstrates better performances in the\nabove three tasks.\nâ€¢Compared to previous sparse attention methods for transformers, our methods are much\nfaster in training and testing, and achieves comparable results.\n2 E XPLICIT SPARSE TRANSFORMER\nThe review to the attention mechanism and the attention-based framework of Transformer can be\nfound in Appendix A.1.\nLack of concentration in the attention can lead to the failure of relevant information extraction. To\nthis end, we propose a novel model, Explicit Sparse Transformer, which enables the focus on only\na few elements through explicit selection. Compared with the conventional attention, no credit will\nbe assigned to the value that is not highly correlated to the query. We provide a comparison between\nthe attention of vanilla Transformer and that of Explicit Sparse Transformer in Figure 2.\n2\nğ’’ğŸ\nğ’’ğŸ\n...\nğ’’ğ’ğ’’\nğ’ŒğŸ ğ’ŒğŸ â€¦ ğ’Œğ’ğ’Œ\nğ’‘ğŸğŸ ğ’‘ğŸ2 â€¦ ğ’‘ğŸğ‘™ğ‘˜\nğ’‘ğŸğŸ ğ’‘ğŸğŸ â€¦ ğ’‘2ğ‘™ğ‘˜\n... ... ... ...\nğ’‘ğ’ğ’’ğŸ ğ’‘ğ’ğ’’ğŸ â€¦ ğ’‘ğ’ğ’’ğ‘™ğ‘˜\nğ¾\nğ‘„\nğ’•ğŸ\nğ’•ğŸ\n...\nğ’•ğ’ğ’’\nğ‘¡\nğŸ ğŸ â€¦ ğŸ\nğŸ ğŸ â€¦ ğŸ\n... ... ... ...\nğŸ ğŸ â€¦ ğŸ\n-\nsign\nğ•„\n+ 1âˆ’ğ•„\nâˆ’âˆ\nx\nğ’‘ğŸğŸ âˆ’âˆ â€¦ âˆ’âˆ\nâˆ’âˆ ğ’‘ğŸğŸ â€¦ ğ’‘ğŸğ’ğ’Œ\n... ... ... ...\nğ’‘ğ’ğ’’ğŸ âˆ’âˆ â€¦ âˆ’âˆ\nğˆ\nğœ¶ğŸğŸ ğŸ â€¦ ğŸ\nğŸ ğœ¶ğŸğŸ â€¦ ğœ¶ğŸğ’ğ’Œ\n... ... ... ...\nğ’‚ğ’ğ’’ğŸ ğŸ â€¦ ğŸ\nğ´\nSoftmax\nnormalization\nğ‘ƒ\nTop-k\nselection\nFigure 2: The comparison between the attentions of vanilla Transformer and Explicit Sparse Trans-\nformer and the illustration of the attention module of Explicit Sparse Transformer. With the mask\nbased on top-kselection and softmax function, only the most contributive elements are assigned with\nprobabilities.\nExplicit Sparse Transformer is still based on the Transformer framework. The difference is in\nthe implementation of self-attention. The attention is degenerated to the sparse attention through\ntop-k selection. In this way, the most contributive components for attention are reserved and\nthe other irrelevant information are removed. This selective method is effective in preserving\nimportant information and removing noise. The attention can be much more concentrated on the most\ncontributive elements of value. In the following, we ï¬rst introduce the sparsiï¬cation in self-attention\nand then extend it to context attention.\nIn the unihead self-attention, the key components, the queryQ[lQ,d], key K[lK,d] and value V[lV ,d],\nare the linear transformation of the source context, namely the input of each layer, where Q= WQx,\nK = WKxand V = WV x. Explicit Sparse Transformer ï¬rst generates the attention scores P as\ndemonstrated below:\nP = QKT\nâˆš\nd\n(1)\nThen the model evaluates the values of the scores P based on the hypothesis that scores with larger\nvalues demonstrate higher relevance. The sparse attention masking operation M(Â·) is implemented\nupon P in order to select the top-kcontributive elements. Speciï¬cally, we select the klargest element\nof each row in P and record their positions in the position matrix (i,j), where kis a hyperparameter.\nTo be speciï¬c, say the k-th largest value of row iis ti, if the value of the j-th component is larger\nthan ti, the position (i,j) is recorded. We concatenate the threshold value of each row to form a\nvector t= [t1,t2,Â·Â·Â· ,tlQ]. The masking functions M(Â·,Â·) is illustrated as follows:\nM(P,k)ij =\n{ Pij if Pij â‰¥ti (k-th largest value of row i)\nâˆ’âˆ if Pij <ti (k-th largest value of row i) (2)\nWith the top-k selection, the high attention scores are selected through an explicit way. This is\ndifferent from dropout which randomly abandons the scores. Such explicit selection can not only\nguarantee the preservation of important components, but also simplify the model since kis usually a\nsmall number such as 8, detailed analysis can be found in 4.2. The next step after top-kselection is\nnormalization:\nA= softmax(M(P,k)) (3)\nwhere Arefers to the normalized scores. As the scores that are smaller than the top k largest scores\nare assigned with negative inï¬nity by the masking function M(Â·,Â·), their normalized scores, namely\nthe probabilities, approximate 0. We show the back-propagation process of Top-k selection in A.3.\nThe output representation of self-attention Ccan be computed as below:\nC = AV (4)\n3\nModel En-De En-Vi De-En\nConvS2S (Gehring et al., 2017) 25.2 - -\nActor-Critic (Bahdanau et al., 2017) - - 28.5\nNPMT+LM (Huang et al., 2017) - 28.1 30.1\nSACT (Lin et al., 2018) - 29.1 -\nVar-Attn (Deng et al., 2018) - - 33.7\nNP2MT Feng et al. (2018) - 30.6 31.7\nTransformer (Vaswani et al., 2017) 28.4 - -\nRNMT (Chen et al., 2018) 28.5 - -\nFixup (Zhang et al., 2019) 29.3 - 34.5\nWeighted Transformer (Ahmed et al., 2017) 28.9 - -\nUniversal Transformer (Dehghani et al., 2018) 28.9 - -\nLayer-wise Coordination (He et al., 2018) 29.1 - -\nTransformer(relative position) (Shaw et al., 2018) 29.2 - -\nTransformer (Ott et al., 2018) 29.3 - -\nDynamicConv (Wu et al., 2019) 29.7 - 35.2\nLocal Joint Self-attention (Fonollosa et al., 2019) 29.7 - 35.7\nTransformer(impl.) 29.1 30.6 35.3\nExplicit Sparse Transformer 29.4 31.1 35.6\nTable 1: Results on the En-De, En-Vi and De-En test sets. Compared with the baseline models, â€œimpl.â€\ndenotes our own implementation.\nThe output is the expectation of the value following the sparsiï¬ed distribution A. Following the\ndistribution of the selected components, the attention in the Explicit Sparse Transformer model can\nobtain more focused attention. Also, such sparse attention can extend to context attention. Resembling\nbut different from the self-attention mechanism, the Qis no longer the linear transformation of the\nsource context but the decoding states s. In the implementation, we replace Qwith WQs, where WQ\nis still learnable matrix.\nIn brief, the attention in our proposed Explicit Sparse Transformer sparsiï¬es the attention weights.\nThe attention can then become focused on the most contributive elements, and it is compatible\nto both self-attention and context attention. The simple implementation of this method is in the\nAppendix A.4.\n3 R ESULTS\nWe conducted a series of experiments on three natural language processing tasks, including neural\nmachine translation, image captioning and language modeling. Detailed experimental settings are in\nAppendix A.2.\n3.1 N EURAL MACHINE TRANSLATION\nDataset To evaluate the performance of Explicit Sparse Transformer in NMT, we conducted\nexperiments on three NMT tasks, English-to-German translation (En-De) with a large dataset,\nEnglish-to-Vietnamese (En-Vi) translation and German-to-English translation (De-En) with two\ndatasets of medium size. For En-De, we trained Explicit Sparse Transformer on the standard dataset\nfor WMT 2014 En-De translation. The dataset consists of around 4.5 million sentence pairs. The\nsource and target languages share a vocabulary of 32K sub-word units. We used the newstest 2013\nfor validation and the newstest 2014as our test set. We report the results on the test set.\nFor En-Vi, we trained our model on the dataset in IWSLT 2015 (Cettolo et al., 2014). The dataset\nconsists of around 133K sentence pairs from translated TED talks. The vocabulary size for source\nlanguage is around 17,200 and that for target language is around 7,800. We usedtst2012 for validation,\nand tst2013 for testing and report the testing results. For De-En, we used the dataset in IWSLT\n2014. The training set contains 160K sentence pairs and the validation set contains 7K sentences.\nFollowing Edunov et al. (2018), we used the same test set with around 7K sentences. The data were\npreprocessed with byte-pair encoding (Sennrich et al., 2016). The vocabulary size is 14,000.\n4\nModel BLEU-4 METEOR CIDEr\nSAT Bazzani et al. (2018b) 28.2 24.8 92.3\nSCST Rennie et al. (2017) 32.8 26.7 106.5\nNBT Lu et al. (2018) 34.7 27.1 107.2\nAdaAtt Lu et al. (2017) 33.2 26.6 108.5\nARNN Bazzani et al. (2018a) 33.9 27.6 109.8\nTransformer 35.3 27.7 113.1\nUpDown Anderson et al. (2018a) 36.2 27.0 113.5\nExplicit Sparse Transformer 35.7 28.0 113.8\nTable 2: Results on the MSCOCO Karpathy test split.\nResult Table 1 presents the results of the baselines and our Explicit Sparse Transformer on the\nthree datasets. For En-De, Transformer-based models outperform the previous methods. Compared\nwith the result of Transformer (Vaswani et al., 2017), Explicit Sparse Transformer reaches 29.4 in\nBLEU score evaluation, outperforming vanilla Transformer by 0.3 BLEU score. For En-Vi, vanilla\nTransformer1 reaches 30.2, outperforming previous best method (Huang et al., 2017). Our model,\nExplicit Sparse Transformer, achieves a much better performance, 31.1, by a margin of 0.5 over\nvanilla Transformer. For De-En, we demonstrate that Transformer-based models outperform the other\nbaselines. Compared with Transformer, our Explicit Sparse Transformer reaches a better performance,\n35.6. Its advantage is +0.3. To the best of our knowledge, Explicit Sparse Transformer reaches a top\nline performance on the dataset.\n3.2 I MAGE CAPTIONING\nDataset We evaluated our approach on the image captioning task. Image captioning is a task that\ncombines image understanding and language generation. We conducted experiments on the Microsoft\nCOCO 2014 dataset (Chen et al., 2015a). It contains 123,287 images, each of which is paired 5\nwith descriptive sentences. We report the results and evaluate the image captioning model on the\nMSCOCO 2014 test set for image captioning. Following previous works (Anderson et al., 2018b; Liu\net al., 2018), we used the publicly-available splits provided by Karpathy & Li (2015). The validation\nset and test set both contain 5,000 images.\nResult Table 2 shows the results of the baseline models and Explicit Sparse Transformer on the\nCOCO Karpathy test split. Transformer outperforms the mentioned baseline models. Explicit Sparse\nTransformer outperforms the implemented Transformer by +0.4 in terms of BLEU-4, +0.3 in terms of\nMETEOR, +0.7 in terms of CIDEr. , which consistently proves its effectiveness in Image Captioning.\n3.3 L ANGUAGE MODELING\nDataset Enwiki82 is large-scale dataset for character-level language modeling. It contains 100M\nbytes of unprocessed Wikipedia texts. The inputs include Latin alphabets, non-Latin alphabets,\nXML markups and special characters. The vocabulary size 205 tokens, including one for unknown\ncharacters. We used the same preprocessing method following Chung et al. (2015). The training set\ncontains 90M bytes of data, and the validation set and the test set contains 5M respectively.\nResult Table 3 shows the results of the baseline models and Explicit Sparse Transformer-XL on the\ntest set of enwiki8. Compared with the other strong baselines, Transformer-XL can reach a better\nperformance, and Explicit Sparse Transformer outperforms Transformer-XL with an advantage.\n1While we did not ï¬nd the results of Transformer on En-Vi, we reimplemented our vanilla Transformer with\nthe same setting.\n2http://mattmahoney.net/dc/text.html\n5\nModel Params BPC\nLN HyperNetworks (Ha et al., 2016) 27M 1.34\nLN HM-LSTM (Chung et al., 2016) 35M 1.32\nRHN (Zilly et al., 2017) 46M 1.27\nLarge FS-LSTM-4 (Mujika et al., 2017) 47M 1.25\nLarge mLSTM (Krause et al., 2016) 46M 1.24\nTransformer (Al-Rfou et al., 2018) 44M 1.11\nTransformer-XL (Dai et al., 2019) 41M 1.06\nAdaptive-span (Sukhbaatar et al., 2019) 39M 1.02\nExplicit Sparse Transformer-XL 41M 1.05\nTable 3: Comparison with state-of-the-art results on enwiki8. Explicit Sparse Transformer-XL refers\nto the Transformer with our sparsiï¬cation method.\nMethod En-Vi De-En Training Speed (tokens/s) Inference Speed (tokens/s)\nTransformer 30.6 35.3 49K 7.0K\nSparsemax (Martins & Astudillo, 2016) - 31.2 39K 3.0K\nEntmax-1.5 (Peters et al., 2019) 30.9 35.6 40K 4.9K\nEntmax-alpha (Correia et al., 2019) - 35.5 13K 0.6K\nProposal 31.1 35.6 48K 6.6K\nTable 4: In the Transformer model, the proposed method, top-k selection before softmax is faster\nthan previous sparse attention methods and is comparable in terms of BLEU scores.\n4 D ISCUSSION\nIn this section, we performed several analyses for further discussion of Explicit Sparse Transformer.\nFirst, we compare the proposed method of topk selection before softmax with previous sparse\nattention method including various variants of sparsemax (Martins & Astudillo, 2016; Correia et al.,\n2019; Peters et al., 2019). Second, we discuss about the selection of the value of k. Third, we\ndemonstrate that the top-k sparse attention method helps training. In the end, we conducted a series\nof qualitative analyses to visualize proposed sparse attention in Transformer.\n4.1 C OMPARISON WITH OTHER SPARSE ATTENTION METHODS\nWe compare the performance and speed of our method with the previous sparse attention methods3 on\nthe basis of strong implemented transformer baseline. The training and inference speed are reported\non the platform of Pytorch and IWSLT 2014 De-En translation dataset, the batch size for inference is\nset to 128 in terms of sentence and half precision training(FP-16) is applied.\nAs we can see from Table 4, the proposed sparse attention method achieve the comparable results as\nprevious sparse attention methods, but the training and testing speed is 2x faster than sparsemax and\n10x faster than Entmax-alpha during the inference. This is due to the fact that our method does not\nintroduce too much computation for calculating sparse attention scores.\nThe other group of sparse attention methods of adding local attention constraints into attention (Child\net al., 2019; Sukhbaatar et al., 2019), do not show performance on neural machine translation, so we\ndo not compare them in Table 4.\n3We borrow the implementation of Entmax1.5 in Tensorï¬‚ow from https://github.com/\ndeep-spin/entmax, and the implementation of Sparsemax, Entmax-1.5, Entmax-alpha in Pytorch from\nhttps://gist.github.com/justheuristic/60167e77a95221586be315ae527c3cbd. We\nhave not found a reliable Tensorï¬‚ow implementation of sparsemax and entmax-alpha in the transformer (we\ntried to apply the ofï¬cial implementation of sparsemax in Tensorï¬‚ow to tensor2tensor, but it reports loss of\nNaN.)\n6\nFigure 3: Analyse the value of K on IWSLT En-Vi and De-En datasets. â€infâ€ denotes the special\ncase of the Explicit Sparse Transformer where all positions may be attended, same as the origin\nTransformer.\nTask Base T T&P\nEn-Vi (BLEU) 27.4 27.7 27.8\nTable 5: Results of the ablation study of the sparsiï¬cation at different phases on the En-Vi test set.\nâ€œBaseâ€ denotes vanilla Transformer. â€œTâ€ denotes only adding the sparsiï¬cation in the training phase,\nand â€œT&Pâ€ denotes adding it at both phases as the implementation of Explicit Sparse Transformer\ndoes.\n4.2 H OW TO SELECT A PROPER K ?\nThe natural question of how to choose the optimal kcomes with the proposed method. We compare\nthe effect of the value of kat exponential scales. We perform experiments on En-Vi and De-En from\n3 different initializations for each value of K, and report the mean BLEU scores on the valid set.\nThe ï¬gure 3 shows that regardless of the value of 16 on the En-Vi dataset, the model performance\ngenerally rises ï¬rst and then falls as kincreases. For k âˆˆ{4,8,16,32}, setting the value of kto 8\nachieves consistent improvements over the transformer baseline.\n4.3 D O THE PROPOSED SPARSE ATTENTION METHOD HELPS TRAINING ?\nWe are surprised to ï¬nd that only adding the sparsiï¬cation in the training phase can also bring an\nimprovement in the performance. We experiment this idea on IWSLT En-Vi and report the results on\nthe valid set in Table 5, . The improvement of 0.3 BLEU scores shows that vanilla Transformer may\nbe overparameterized and the sparsiï¬cation encourages the simpliï¬cation of the model.\n4.4 D O THE EXPLICIT SPARSE TRANSFORMER ATTEND BETTER ?\nTo perform a thorough evaluation of our Explicit Sparse Transformer, we conducted a case study and\nvisualize the attention distributions of our model and the baseline for further comparison. Speciï¬cally,\nwe conducted the analysis on the test set of En-Vi, and randomly selected a sample pair of attention\nvisualization of both models.\n7\ntÃ´ i\ncáº£m\nÆ¡n\nÃ´ ng\náº¥y\nvÃ¬  \nÄ‘Ã£\nnghÄ©\nvá» \ntrÃ¡ i \ntim \ntÃ´ i \n,\nvÃ   \ntÃ´ i \nhá»i \nÃ´ ng \n, \n&quot; \ntáº¡i \nsao \nÃ´ ng \nláº¡i \ngiÃº p \ntÃ´ i \n? \n&quot; \nEOS\ntÃ´ i \ncáº£m \nÆ¡n \nÃ´ ng \nvá»›i \ntáº¥t \ncáº£ \ntrÃ¡ i \ntim\ntÃ´ i\n,\nvÃ   \ntÃ´ i \nhá»i \nÃ´ ng \n, \n&quot; \ntáº¡i \nsao \nÃ´ ng \nláº¡i \ngiÃº p \ntÃ´ i \n? \n&quot; \nEOS\nfor\nthinking\nabout\nmy \nheart\nwith\nall\nmy \nheart\nI \nthanked\nhim\nwith \nall\nmy\nheart \n, \nand \nI \nasked \nhim \n, \n&quot; \nwhy\nare\nyou\nhelping\nme\n?\n&quot;\nI \nthanked\nhim\nwith \nall\nmy\nheart \n, \nand \nI \nasked \nhim \n, \n&quot; \nwhy\nare\nyou\nhelping\nme\n?\n&quot;\n(a) Attention of the bottom layer\nI \nthanked\nhim\nwith \nall\nmy\nheart \n, \nand \nI \nasked \nhim \n, \n&quot; \nwhy\nare\nyou\nhelping\nme\n?\n&quot;\ntÃ´ i\ncáº£m\nÆ¡n\nÃ´ ng\náº¥y\nvÃ¬  \nÄ‘Ã£\nnghÄ©\nvá» \ntrÃ¡ i \ntim \ntÃ´ i \n,\nvÃ   \ntÃ´ i \nhá»i \nÃ´ ng \n, \n&quot; \ntáº¡i \nsao \nÃ´ ng \nláº¡i \ngiÃº p \ntÃ´ i \n? \n&quot; \nEOS\ntÃ´ i \ncáº£m \nÆ¡n \nÃ´ ng \nvá»›i \ntáº¥t \ncáº£ \ntrÃ¡ i \ntim\ntÃ´ i\n,\nvÃ   \ntÃ´ i \nhá»i \nÃ´ ng \n, \n&quot; \ntáº¡i \nsao \nÃ´ ng \nláº¡i \ngiÃº p \ntÃ´ i \n? \n&quot; \nEOS\nI \nthanked\nhim\nwith \nall\nmy\nheart \n, \nand \nI \nasked \nhim \n, \n&quot; \nwhy\nare\nyou\nhelping\nme\n?\n&quot; (b) Attention of the top layer\nFigure 4: Figure 4(a) is the attention visualization of Transformer and Figure 4(b) is that of the\nExplicit Sparse Transformer. The red box shows that the attentions in vanilla Transformer at most\nsteps are concentrated on the last token of the context.\nThe visualization of the context attention of the decoderâ€™s bottom layer in Figure 4(a). The attention\ndistribution of the left ï¬gure is fairly disperse. On the contrary, the right ï¬gure shows that the sparse\nattention can choose to focus only on several positions so that the model can be forced to stay focused.\nFor example, when generating the phrase â€œfor thinking about my heartâ€(Word-to-word translation\nfrom Vietnamese), the generated word cannot be aligned to the corresponding words. As to Explicit\nSparse Transformer, when generating the phrase â€with all my heartâ€, the attention can focus on the\ncorresponding positions with strong conï¬dence.\nThe visualization of the decoderâ€™s top layer is shown in Figure 4(b). From the ï¬gure, the context\nattention at the top layer of the vanilla Transformer decoder suffers from focusing on the last source\ntoken. This is a common behavior of the attention in vanilla Transformer. Such attention with wrong\nalignment cannot sufï¬ciently extract enough relevant source-side information for the generation. In\ncontrast, Explicit Sparse Transformer, with simple modiï¬cation on the vanilla version, does not suffer\nfrom this problem, but instead focuses on the relevant sections of the source context. The ï¬gure\non the right demonstrating the attention distribution of Explicit Sparse Transformer shows that our\nproposed attention in the model is able to perform accurate alignment.\n5 R ELATED WORK\nAttention mechanism has demonstrated outstanding performances in a number of neural-network-\nbased methods, and it has been a focus in the NLP studies (Bahdanau et al., 2014). A number of\nstudies are proposed to enhance the effects of attention mechanism (Luong et al., 2015; Vaswani et al.,\n2017; Ke et al., 2018; Zhao et al., 2019). Luong et al. (2015) propose local attention and Yang et al.\n(2018) propose local attention for self-attention. Xu et al. (2015) propose hard attention that pays\ndiscrete attention in image captioning. Chandar et al. (2016) propose a combination soft attention\nwith hard attention to construct hierarchical memory network. Lin et al. (2018) propose a temperature\nmechanism to change the softness of attention distribution. Shen et al. (2018) propose an attention\nwhich can select a small proportion for focusing. It is trained by reinforcement learning algorithms\n(Williams, 1992). In terms of memory networks, Rae et al. (2016) propose to sparse access memory.\nChild et al. (2019) recently propose to use local attention and block attention to sparsify the trans-\nformer. Our approach differs from them in that our method does not need to block sentences and still\ncapture long distance dependencies. Besides, we demonstrate the importance of Explicit Sparse Trans-\nformer in sequence to sequence learning. Although the variants of sparsemax (Martins & Astudillo,\n2016; Correia et al., 2019; Peters et al., 2019) improve in machine translation tasks, we empirically\ndemonstrate in 4.1 that our method introduces less computation in the standard transformer and is\nmuch faster than those sparse attention methods on GPUs.\n8\n6 C ONCLUSION\nIn this paper, we propose a novel model called Explicit Sparse Transformer. Explicit Sparse Trans-\nformer is able to make the attention in vanilla Transformer more concentrated on the most contributive\ncomponents. Extensive experiments show that Explicit Sparse Transformer outperforms vanilla Trans-\nformer in three different NLP tasks. We conducted a series of qualitative analyses to investigate the\nreasons why Explicit Sparse Transformer outperforms the vanilla Transformer. Furthermore, we ï¬nd\nan obvious problem of the attention at the top layer of the vanilla Transformer, and Explicit Sparse\nTransformer can alleviate this problem effectively with improved alignment effects.\nREFERENCES\nKarim Ahmed, Nitish Shirish Keskar, and Richard Socher. Weighted transformer network for machine\ntranslation. CoRR, abs/1711.02132, 2017.\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language\nmodeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei\nZhang. Bottom-up and top-down attention for image captioning and visual question answering. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6077â€“6086,\n2018a.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei\nZhang. Bottom-up and top-down attention for image captioning and visual question answering. In\nCVPR 2018, pp. 6077â€“6086. IEEE Computer Society, 2018b.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C.\nCourville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In 5th Interna-\ntional Conference on Learning Representations, ICLR 2017, 2017.\nLoris Bazzani, Tobias Domhan, and Felix Hieber. Image captioning as neural machine translation\ntask in sockeye. arXiv preprint arXiv:1810.04101, 2018a.\nLoris Bazzani, Tobias Domhan, and Felix Hieber. Image captioning as neural machine translation\ntask in SOCKEYE. CoRR, abs/1810.04101, 2018b.\nMauro Cettolo, Jan Niehues, Sebastian StÂ¨uker, Luisa Bentivogli, and Marcello Federico. Report on\nthe 11th iwslt evaluation campaign, iwslt 2014. In Proceedings of the International Workshop on\nSpoken Language Translation, Hanoi, Vietnam, 2014.\nSarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, and Yoshua Bengio.\nHierarchical memory networks. CoRR, abs/1605.07427, 2016.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. The best of both worlds: Combining\nrecent advances in neural machine translation. In Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2018, pp. 76â€“86, 2018.\nX. Chen, H. Fang, TY Lin, R. Vedantam, S. Gupta, P. Dollr, and C. L. Zitnick. Microsoft coco\ncaptions: Data collection and evaluation server. arXiv:1504.00325, 2015a.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr DollÂ´ar, and\nC. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. CoRR,\nabs/1504.00325, 2015b.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n9\nJunyoung Chung, C Â¸aglar GÂ¨ulc Â¸ehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback recurrent\nneural networks. In Proceedings of the 32nd International Conference on Machine Learning,\nICML 2015, pp. 2067â€“2075, 2015.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.\narXiv preprint arXiv:1609.01704, 2016.\nGonalo M. Correia, Vlad Niculae, and Andr F. T. Martins. Adaptively sparse transformers. Proceed-\nings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019. doi:\n10.18653/v1/d19-1223. URL http://dx.doi.org/10.18653/v1/d19-1223.\nZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ï¬xed-length context. arXiv\npreprint arXiv:1901.02860, 2019.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\ntransformers. CoRR, abs/1807.03819, 2018.\nYuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander M. Rush. Latent alignment\nand variational attention. In Advances in Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems 2018, NeurIPS 2018, pp. 9735â€“9747, 2018.\nMichael J. Denkowski and Alon Lavie. Meteor universal: Language speciï¬c translation evaluation\nfor any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation,\nWMT@ACL 2014, pp. 376â€“380, 2014.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nSergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marcâ€™Aurelio Ranzato. Classical\nstructured prediction losses for sequence to sequence learning. In Proceedings of the 2018\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2018 1 (Long Papers), pp. 355â€“364, 2018.\nJiangtao Feng, Lingpeng Kong, Po-Sen Huang, Chong Wang, Da Huang, Jiayuan Mao, Kan Qiao,\nand Dengyong Zhou. Neural phrase-to-phrase machine translation. CoRR, abs/1811.02172, 2018.\nJosÂ´e AR Fonollosa, Noe Casas, and Marta R Costa-juss `a. Joint source-target self attention with\nlocality constraints. arXiv preprint arXiv:1905.06596, 2019.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional\nsequence to sequence learning. In ICML 2017, volume 70 of Proceedings of Machine Learning\nResearch, pp. 1243â€“1252. PMLR, 2017.\nDavid Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.\nTianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, and Tie-Yan Liu. Layer-wise\ncoordination between encoder and decoder for neural machine translation. In S. Bengio, H. Wal-\nlach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural\nInformation Processing Systems 31, pp. 7944â€“7954. Curran Associates, Inc., 2018.\nPo-Sen Huang, Chong Wang, Sitao Huang, Dengyong Zhou, and Li Deng. Towards neural phrase-\nbased machine translation. arXiv preprint arXiv:1706.05565, 2017.\nAndrej Karpathy and Fei-Fei Li. Deep visual-semantic alignments for generating image descriptions.\nIn CVPR 2015, pp. 3128â€“3137. IEEE Computer Society, 2015.\nNan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael C. Mozer, Chris Pal,\nand Yoshua Bengio. Sparse attentive backtracking: Temporal credit assignment through reminding.\nIn NeurIPS 2018, pp. 7651â€“7662, 2018.\nBen Krause, Liang Lu, Iain Murray, and Steve Renals. Multiplicative lstm for sequence modelling.\narXiv preprint arXiv:1609.07959, 2016.\n10\nJunyang Lin, Xu Sun, Xuancheng Ren, Muyu Li, and Qi Su. Learning when to concentrate or divert\nattention: Self-adaptive attention temperature for neural machine translation. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, pp. 2985â€“2990, 2018.\nFenglin Liu, Xuancheng Ren, Yuanxin Liu, Houfeng Wang, and Xu Sun. simnet: Stepwise image-\ntopic merging network for generating detailed and comprehensive image captions. Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing, 2018. doi:\n10.18653/v1/d18-1013. URL http://dx.doi.org/10.18653/v1/d18-1013.\nJiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Knowing when to look: Adaptive\nattention via a visual sentinel for image captioning. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 375â€“383, 2017.\nJiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pp. 7219â€“7228, 2018.\nThang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based\nneural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp.\n1412â€“1421, 2015.\nAndrÂ´e F. T. Martins and RamÂ´on FernÂ´andez Astudillo. From softmax to sparsemax: A sparse model\nof attention and multi-label classiï¬cation. In ICML 2016, pp. 1614â€“1623, 2016.\nAsier Mujika, Florian Meier, and Angelika Steger. Fast-slow recurrent neural networks. In Advances\nin Neural Information Processing Systems, pp. 5915â€“5924, 2017.\nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.\nIn Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018,\nBelgium, Brussels, October 31 - November 1, 2018, pp. 1â€“9, 2018.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In ACL 2002, pp. 311â€“318, 2002.\nBen Peters, Vlad Niculae, and Andr F. T. Martins. Sparse sequence-to-sequence models. Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, 2019. doi: 10.18653/\nv1/p19-1146. URL http://dx.doi.org/10.18653/v1/p19-1146.\nJack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex\nGraves, and Timothy Lillicrap. Scaling memory-augmented neural networks with sparse reads and\nwrites. In Advances in Neural Information Processing Systems, pp. 3621â€“3629, 2016.\nSteven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical\nsequence training for image captioning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 7008â€“7024, 2017.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics, ACL 2016, 2016.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana,\nUSA, June 1-6, 2018, Volume 2 (Short Papers), pp. 464â€“468, 2018.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen Wang, and Chengqi Zhang. Reinforced\nself-attention network: a hybrid of hard and soft attention for sequence modeling. In IJCAI 2018,\npp. 4345â€“4352, 2018.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention\nspan in transformers. Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, 2019. doi: 10.18653/v1/p19-1032. URL http://dx.doi.org/10.18653/v1/\np19-1032.\n11\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS 2017, pp. 6000â€“6010, 2017.\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In CVPR 2015, pp. 4566â€“4575, 2015.\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. In Machine Learning, pp. 229â€“256, 1992.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with\nlightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,\nRichard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation\nwith visual attention. In ICML 2015, pp. 2048â€“2057, 2015.\nBaosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong Meng, Lidia S. Chao, and Tong Zhang.\nModeling localness for self-attention networks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, pp. 4449â€“4458, 2018.\nHongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without\nnormalization. CoRR, abs/1901.09321, 2019.\nGuangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, and Liangchen Luo. Muse: Parallel\nmulti-scale attention for sequence to sequence learning. arXiv preprint arXiv:1911.09483, 2019.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan KoutnÂ´Ä±k, and JÂ¨urgen Schmidhuber. Recurrent\nhighway networks. In Proceedings of the 34th International Conference on Machine Learning-\nVolume 70, pp. 4189â€“4198. JMLR. org, 2017.\nA A PPENDIX\nA.1 B ACKGROUND\nA.1.1 A TTENTION MECHANISM\nBahdanau et al. (2014) ï¬rst introduced the attention mechanism to learn the alignment between the\ntarget-side context and the source-side context, and Luong et al. (2015) formulated several versions\nfor local and global attention. In general, the attention mechanism maps a query and a key-value pair\nto an output. The attention score function and softmax normalization can turn the query Qand the\nkey K into a distribution Î±. Following the distribution Î±, the attention mechanism computes the\nexpectation of the value V and ï¬nally generates the output C.\nTake the original attention mechanism in NMT as an example. Both key K âˆˆRnÃ—d and value\nV âˆˆRnÃ—d are the sequence of output states from the encoder. Query QâˆˆRmÃ—d is the sequence of\noutput states from the decoder, where mis the length of Q, nis the length of Kand V, and dis the\ndimension of the states. Thus, the attention mechanism is formulated as:\nC = softmax(f(Q,K))V (5)\nwhere f refers to the attention score computation.\nA.1.2 T RANSFORMER\nTransformer (Vaswani et al., 2017), which is fully based on the attention mechanism, demonstrates\nthe state-of-the-art performances in a series of natural language generation tasks. Speciï¬cally, we\nfocus on self-attention and multi-head attention.\nThe ideology of self-attention is, as the name implies, the attention over the context itself. In the\nimplementation, the query Q, key K and value V are the linear transformation of the input x, so\n12\nthat Q = WQx, K = WKxand V = WV xwhere WQ, WK and WV are learnable parameters.\nTherefore, the computation can be formulated as below:\nC = softmax\n(QKT\nâˆš\nd\n)\nV (6)\nwhere drefers to the dimension of the states.\nThe aforementioned mechanism can be regarded as the unihead attention. As to the multi-head\nattention, the attention computation is separated into gheads (namely 8 for basic model and 16 for\nlarge model in the common practice). Thus multiple parts of the inputs can be computed individually.\nFor the i-th head, the output can be computed as in the following formula:\nC(i) = softmax\n(Q(i)K(i)T\nâˆšdk\n)\nV(i) (7)\nwhere C(i) refers to the output of the head, Q(i), K(i) and V(i) are the query, key and value of\nthe head, and dk refers to the size of each head ( dk = d/g). Finally, the output of each head are\nconcatenated for the output:\nC = [C(1),Â·Â·Â· ,C(i),Â·Â·Â· ,C(g)] (8)\nIn common practice, Cis sent through a linear transformation with weight matrix Wc for the ï¬nal\noutput of multi-head attention.\nHowever, soft attention can assign weights to a lot more words that are less relevent to the query.\nTherefore, in order to improve concentration in attention for effective information extraction, we study\nthe problem of sparse attention in Transformer and propose our model Explicit Sparse Transformer.\nA.2 E XPERIMENTAL DETAILS\nWe use the default setting in Vaswani et al. (2017) for the implementation of our proposed Explicit\nSparse Transformer. The hyper parameters including beam size and training steps are tuned on the\nvalid set.\nNeural Machine Translation Training For En-Vi translation, we use default scripts and hyper-\nparameter setting of tensor2tensor4 v1.11.0 to preprocess, train and evaluate our model. We use the\ndefault scripts of fairseq5 v0.6.1 to preprocess the De-En and En-De dataset. We train the model on\nthe En-Vi dataset for 35Ksteps with batch size of 4K. For IWSLT 2015 De-En dataset, batch size\nis also set to 4K, we update the model every 4 steps and train the model for 90epochs. For WMT\n2014 En-De dataset, we train the model for 72 epochs on 4 GPUs with update frequency of 32 and\nbatch size of 3584. We train all models on a single RTX2080TI for two small IWSLT datasets and\non a single machine of 4 RTX TITAN for WMT14 En-De. In order to reduce the impact of random\ninitialization, we perform experiments with three different initializations for all models and report the\nhighest for small datasets.\nEvaluation We use case-sensitive tokenized BLEU score (Papineni et al., 2002) for the evaluation of\nWMT14 En-De, and we use case-insensitive BLEU for that of IWSLT 2015 En-Vi and IWSLT 2014\nDe-En following Lin et al. (2018). Same as Vaswani et al. (2017), compound splitting is used for\nWMT 14 En-De. For WMT 14 En-De and IWSLT 2014 De-En, we save checkpoints every epoch\nand average last 10 checkpoints every 5 epochs, We select the averaged checkpoint with best valid\nBLEU and report its BLEU score on the test set. For IWSLT 2015 En-Vi, we save checkpoints every\n600 seconds and average last 20 checkpoints.\nImage Captioning We still use the default setting of Transformer for training our proposed Explicit\nSparse Transformer. We report the standard automatic evaluation metrics with the help of the COCO\ncaptioning evaluation toolkit6 (Chen et al., 2015b), which includes the commonly-used evaluation\nmetrics, BLEU-4 Papineni et al. (2002), METEOR Denkowski & Lavie (2014), and CIDEr Vedantam\net al. (2015).\n4https://github.com/tensorï¬‚ow/tensor2tensor\n5https://github.com/pytorch/fairseq\n6https://github.com/tylin/coco-caption\n13\nLanguage Models We follow Dai et al. (2019) and use their implementation for our Explicit\nSparse Transformer. Following the previous work (Chung et al., 2015; Dai et al., 2019), we use BPC\n(E[log2P(xt+ 1|ht)]), standing for the average number of Bits-Per-Character, for evaluation. Lower\nBPC refers to better performance. As to the model implementation, we implement Explicit Sparse\nTransformer-XL, which is based on the base version of Transformer-XL.7 Transformer-XL is a model\nbased on Transformer but has better capability of representing long sequences.\nA.3 T HE BACK -PROPAGATION PROCESS OF TOP-K SELECTION\nThe masking function M(Â·,Â·) is illustrated as follow:\nM(P,k)ij =\n{ Pij if Pij â‰¥ti (k-th largest value of row i)\nâˆ’âˆ if Pij <ti (k-th largest value of row i) (9)\nDenote M = M(P,k). We regard ti as constants. When back-propagating,\nâˆ‚Mij\nâˆ‚Pkl\n= 0 (iÌ¸= kor j Ì¸= l) (10)\nâˆ‚Mij\nâˆ‚Pij\n=\n{1 if Pij â‰¥ti (k-th largest value of row i)\n0 if Pij <ti (k-th largest value of row i) (11)\nThe next step after top-kselection is normalization:\nA= softmax(M(P,k)) (12)\nwhere Arefers to the normalized scores. When backpropagating,\nâˆ‚Aij\nâˆ‚Pkl\n=\nlQâˆ‘\nm=1\nlKâˆ‘\nn=1\nâˆ‚Aij\nâˆ‚Mmn\nâˆ‚Mmn\nâˆ‚Pkl\n(13)\n= âˆ‚Aij\nâˆ‚Mkl\nâˆ‚Mkl\nâˆ‚Pkl\n(14)\n=\nï£±\nï£²\nï£³\nâˆ‚Aij\nâˆ‚Mkl\nif Pij â‰¥ti (k-th largest value of row i)\n0 if Pij <ti (k-th largest value of row i)\n(15)\nThe softmax function is evidently differentiable, therefore, we have calculated the gradient involved\nin top-k selection.\nA.4 I MPLEMENTATION\nFigure 5 shows the code for the idea in case of single head self-attention, the proposed method is\neasy to implement and plug in the successful Transformer model.\n7Due to our limited resources (TPU), we did not implement the big version of Explicit Sparse Transformer-\nXL.\n14\nFigure 5: Code for the main idea in Pytorch\n15",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6028699278831482
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5360143184661865
    },
    {
      "name": "Computer science",
      "score": 0.4563908576965332
    },
    {
      "name": "Psychology",
      "score": 0.3563825488090515
    },
    {
      "name": "Arithmetic",
      "score": 0.3521904945373535
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3518846035003662
    },
    {
      "name": "Mathematics",
      "score": 0.2768174707889557
    },
    {
      "name": "Artificial intelligence",
      "score": 0.25409626960754395
    },
    {
      "name": "Electrical engineering",
      "score": 0.16859611868858337
    },
    {
      "name": "Engineering",
      "score": 0.1280522346496582
    },
    {
      "name": "Voltage",
      "score": 0.05738970637321472
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ],
  "cited_by": 77
}