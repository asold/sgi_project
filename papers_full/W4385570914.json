{
  "title": "Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making",
  "url": "https://openalex.org/W4385570914",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101207492",
      "name": "Xuanjie Fang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A5110750792",
      "name": "Sijie Cheng",
      "affiliations": [
        "Fudan University",
        "National Engineering Research Center for Information Technology in Agriculture",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100355692",
      "name": "Yang Liu",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence",
        "National Engineering Research Center for Information Technology in Agriculture",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100391896",
      "name": "Wei Wang",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3169948074",
    "https://openalex.org/W2962818281",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W3034397670",
    "https://openalex.org/W2609368435",
    "https://openalex.org/W2049763295",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3090117266",
    "https://openalex.org/W2022470916",
    "https://openalex.org/W1673923490",
    "https://openalex.org/W2963083752",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2107726111",
    "https://openalex.org/W4205758343",
    "https://openalex.org/W3035736465",
    "https://openalex.org/W4287210714",
    "https://openalex.org/W3101449015",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W2785699986",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964232431",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2795727551",
    "https://openalex.org/W4388154749",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W2893554781",
    "https://openalex.org/W3005742798",
    "https://openalex.org/W2964301649",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2893051557",
    "https://openalex.org/W4289145467",
    "https://openalex.org/W2799007037",
    "https://openalex.org/W2972619028",
    "https://openalex.org/W3105604018"
  ],
  "abstract": "Pre-trained language models (PLMs) have been widely used to underpin various downstream tasks. However, the adversarial attack task has found that PLMs are vulnerable to small perturbations. Mainstream methods adopt a detached two-stage framework to attack without considering the subsequent influence of substitution at each step.In this paper, we formally model the adversarial attack task on PLMs as a sequential decision-making problem, where the whole attack process is sequential with two decision-making problems, i.e., word finder and word substitution. Considering the attack process can only receive the final state without any direct intermediate signals, we propose to use reinforcement learning to find an appropriate sequential attack path to generate adversaries, named SDM-ATTACK. Our experimental results show that SDM-ATTACK achieves the highest attack success rate with a comparable modification rate and semantic similarity to attack fine-tuned BERT. Furthermore, our analyses demonstrate the generalization and transferability of SDM-ATTACK.Resources of this work will be released after this paper's publication.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 7322‚Äì7336\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nModeling Adversarial Attack on Pre-trained Language Models as\nSequential Decision Making\nXuanjie Fang1‚àó, Sijie Cheng1,2,3,4‚àó, Yang Liu2,3,4,5, Wei Wang1‚Ä†\n1School of Computer Science, Fudan University, Shanghai, China\n2Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\n3Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China\n4Beijing National Research Center for Information Science and Technology, Beijing, China\n5Shanghai Artificial Intelligence Laboratory, Shanghai, China\n{xjfang20,sjcheng20,weiwang1}@fudan.edu.cn, liuyang2011@tsinghua.edu.cn\nAbstract\nPre-trained language models (PLMs) have been\nwidely used to underpin various downstream\ntasks. However, the adversarial attack task has\nfound that PLMs are vulnerable to small pertur-\nbations. Mainstream methods adopt a detached\ntwo-stage framework to attack without consid-\nering the subsequent influence of substitution\nat each step. In this paper, we formally model\nthe adversarial attack task on PLMs as a se-\nquential decision-making problem, where the\nwhole attack process is sequential with two\ndecision-making problems, i.e., word finder\nand word substitution. Considering the attack\nprocess can only receive the final state with-\nout any direct intermediate signals, we propose\nto use reinforcement learning to find an ap-\npropriate sequential attack path to generate ad-\nversaries, named SDM-A TTACK . Extensive\nexperimental results show that SDM-A TTACK\nachieves the highest attack success rate with\na comparable modification rate and semantic\nsimilarity to attack fine-tuned BERT. Further-\nmore, our analyses demonstrate the general-\nization and transferability of SDM-A TTACK .\nThe code is available at https://github.\ncom/fduxuan/SDM-Attack.\n1 Introduction\nNowadays, pre-trained language models (PLMs)\nhave shown strong potential in various downstream\ntasks (Devlin et al., 2018; Brown et al., 2020). How-\never, a series of studies about adversarial attack\n(Jin et al., 2020; Li et al., 2020a,b) have found that\nPLMs are vulnerable to some small perturbations\nbased on the original inputs. The adversarial attack\nis essential to develop trustworthy and robust PLMs\nin Artificial Intelligence (AI) community (Thiebes\net al., 2021; Marcus, 2020).\nDespite the adversarial attack achieving success\nin both image and speech domains (Chakraborty\n‚àóEqual Contribution\n‚Ä†Corresponding Author\nDavis is so enamored of her own creation that she can‚Äôt see how insufferable the character is.Step1. VictimModel\nü§ñ NegativeDecision1.1WordFinderComputeProbability‚Ä¶howinsufferablethe‚Ä¶\nSynonymVocabulary ‚Ä¶howindefensiblethe‚Ä¶\nDecision1.2WordSubstitution\nDavis is so enamored of her own creation that she can‚Äôt see how indefensiblethe character is.Step2.\nVictimModel\nü§ñ NegativeAttackFailurei\nüëøDecision2.1WordFinderComputeProbability‚Ä¶soenamoredofher‚Ä¶\nDavis is so captivatedof her own creation that she can‚Äôt see how indefensiblethe character is.Stepn. ‚Ä¶\nVictimModel\nü§ñ PositiveAttackSuccessi\nüòà\nFigure 1: Illustrative example of modeling the adversar-\nial attack into sequential decision making. The entire\nattack process is a sequence with two decision-making\nproblems, i.e., word finder and substitution, until the\nadversary against the victim model is successful.\net al., 2018; Kurakin et al., 2018; Carlini and Wag-\nner, 2018), it is still far from perfect in the natural\nlanguage processing (NLP) field due to the dis-\ncrete nature of language (Studdert-Kennedy, 2005;\nArmstrong et al., 1995). The main problem is to\nfind an appropriate search algorithm that can make\nperturbations to mislead the victim models (i.e.,\nPLMs) successfully (Morris et al., 2020; Yoo and\nQi, 2021). As mentioned in recent studies (Jin et al.,\n2020), the challenges are preserving the following\nproperties: 1) human prediction consistency, mis-\nleading the PLMs while keeping human judges\nunchanged; 2) semantic similarity, keeping the se-\nmantics of the original inputs; 3) language fluency,\nensuring the correctness of grammar.\n7322\nMainstream solutions are typically a detached\ntwo-stage framework. Specifically, they first rank\nthe importance scores of all tokens according to\nthe original input and then orderly substitute these\ntokens via heuristic rules. Previous studies pro-\npose different strategies to rank the editing order\nof tokens, such as temporal-based algorithm (Gao\net al., 2018), probability-weighted saliency (Ren\net al., 2019; Li et al., 2020b,a; Jin et al., 2020), and\ngradient-based ranking (Yoo and Qi, 2021). How-\never, these methods face two limitations. On the\none hand, they use a threshold to filter the unsatis-\nfactory substitutions at last, but neglect to integrally\nconsider the properties during computing impor-\ntance scores. On the other hand, their editing order\nonly depends on the original input without con-\nsidering the subsequent influence of substitution,\nas computing the importance score at each step is\ncomputationally burdensome in practice.\nTo solve the issues mentioned above, in this pa-\nper, we formally propose to transform the adver-\nsarial attack problem into a sequential decision-\nmaking task as shown in Figure 1. Rather than\ncomputing the importance scores all at once based\non the original input, we regard the entire attack\nprocess as a sequence, where scores in the next step\nare influenced by the editing results in the current\nstep. Furthermore, there are two types of decision-\nmaking problems during each step in the attack\nsequential process: 1) word finder, choosing the ap-\npropriate token to edit; 2) word substitution, replac-\ning the token with a suitable substitution. Mean-\nwhile, selecting edited tokens at each step should\ntake the attack success rate and crucial properties,\nsuch as fluency, into account.\nAs a sequential decision-making task without\na direct signal in each step, we naturally leverage\nreinforcement learning (RL) to find an appropriate\nsequential attack path to generate adversaries. In\nthis paper, we propose a model-agnostic method\nbased on policy-based RL for modeling the ad-\nversarial attack into Sequential Decision Making,\nentitled SDM-A TTACK . Given the victim model\nas the environment with designed reward functions\nand the original input text as the initial state, the re-\ninforced agent needs to decide on tokens to edit and\nsynonyms to replace sequentially, until it attacks\nsuccessfully. The experimental results show that\nSDM-A TTACK achieves the highest attack success\nrate with a comparable modification rate and se-\nmantic similarity to attack fine-tuned BERT against\nstate-of-the-art baselines. Furthermore, we also\ndemonstrate the effectiveness, generalizability, and\ntransferability of SDM-A TTACK in our analysis.\nThe main contributions of this work are summa-\nrized as the following:\n‚Ä¢ To the best of our knowledge, we are the\nfirst to model the adversarial attack on PLMs\ninto a sequential decision-making problem,\nwhere the whole attack process is sequen-\ntial with two decision-making problems, i.e.,\nword finder and word substitution.\n‚Ä¢ Considering the sequential attack process can\nreceive the final state without any direct inter-\nmediate signals, we propose SDM-A TTACK\nto use reinforcement learning to ask the agent\nto find an appropriate attack path based on our\ndesigned indirect reward signals yielded by\nthe environment.\n2 Preliminaries\nAs for NLP tasks, given a corpus of N input texts\nX = {x1,x2,x3,¬∑¬∑¬∑ ,xN}and an output space\nY = {y1,y2,y3,¬∑¬∑¬∑ ,yK}containing Klabels, the\nlanguage model F learns a mapping f : x ‚Üíy ,\nwhich learns to classify each input sample x ‚ààX\nto the ground-truth label ygold ‚ààY:\nF(x) = arg max\nyi‚ààY\nP(yi|x) (1)\nThe adversary of text x ‚ààX can be formulated\nas xadv = x + œµ, where œµis a slight perturbation\nto the input x. The goal is to mislead the victim\nmodel F within a certain constraint C(xadv):\nF(xadv) = arg max\nyi‚ààY\nP(yi|xadv) Ã∏= F(x),\nand C(xadv,x) ‚â•Œª\n(2)\nwhere Œªis the coefficient, and C(xadv,x) usually\ncalculates the semantic or syntactic similarity (Cer\net al., 2018; Oliva et al., 2011) between the input x\nand its corresponding adversary xadv.\nRecently, the adversarial attack task has been\nframed as a combinatorial optimization problem.\nHowever, previous studies (Gao et al., 2018; Ren\net al., 2019; Yoo and Qi, 2021) address this prob-\nlem without considering the subsequent influence\nof substitution at each step, making attack far from\nthe most effective. In this paper, we formally de-\nfine the adversarial attack as a sequential decision-\nmaking task, where the decisions in the next step\nare influenced by the results in the current step.\n7323\nMLM\nBinaryRepresentationùëè!\"‚Ñé!\" ùëè#\"‚Ñé#\" ùëè$\"‚Ñé$\"‚Ä¶ Fully Connected Layer\nùë§%\"\nùë§!\" ùë§%\"‚Ä¶ùë§&\" ‚Ä¶ùë§'\"‚Ä¶ùë§(\"ùíôùíï ‚Ä¶\nùë§!\" ùë†‚Ä¶ùë§&\" ‚Ä¶ùë§'\"‚Ä¶ùë§(\"ùíôùíï\"ùüè ‚Ä¶\n VictimModelLanguageModelSimilarityModelùëü!\"\" ùëü#$% ùëü&'(\nùëü$Environment\nUpdateState RewardTerminate?Yi\nSoftMax\nùëù! ùëù$ùëù#‚Ä¶Probability\nS!S# S)‚Ä¶SynonymVocabularyùíôùíîùíï\nùëü)Ni\nWordFinder\nWordSubstitution\nùë§',ùë§&,ùïé\nùë§%-\nFigure 2: The framework of SDM-A TTACK .\n3 Methodology\nIn this section, we model the adversarial attack on\nPLMs problem as a sequential decision-making\ntask as shown in Figure 1, where the entire attack\nprocess is a sequence with two decision-making\nproblems. Considering the lack of direct signal in\neach step during the attack process, we propose\na model-agnostic method, named SDM-A TTACK ,\nbased on policy-based reinforcement learning. The\nillustration is shown in Figure 2. During each step\nin the attack process, the reinforced agent needs\nto take two actions: 1) word finder, choosing the\nappropriate token to edit; and 2) word substitu-\ntion, replacing the token with a suitable substitution.\nThrough an attack sequence toward the input, we\nobtain its adversary until the attack is successful.\n3.1 Environment and Rewards\nWe regard the victim models (i.e., PLMs) as the\nwhole environment. Intuitively, the agent needs to\ngenerate adversaries against the environment and\nachieve as high a reward as possible. The t-step\nenvironment state is our intermediate generation\nxt = [wt\n1,wt\n2,..., wt\nn] containing nwords, where\nthe initial state x0 is the original input.\nConsidering the lack of direct signal in each step,\nour reward consists of a final discriminant signal\nrd to present the state of termination and an instant\nreward rt on every step. As for the final signal\nrd, once the model prediction of t-step state is dif-\nferent from the initial state, the environment will\nterminate this episode and yield a success signal.\nHowever, if the model prediction does not change\nwhen all the tokens are replaced or the maximum\nnumber of steps is reached, a failure signal will be\ngiven. Overall, the final signal rd is denoted as:\nrd =\n{\n1, success\n‚àí1, failure (3)\nAs for the instant reward ri for each step, we\nhope that the t-step state xt can not only mislead\nthe victim model but also ensure semantics simi-\nlarity and fluency. Firstly, we design one instant\nreward to evaluate attack success rates:\nratt\nt =\n{\nrd, terminated\nP(ygold|xt‚àí1) ‚àíP(ygold|xt), survive\n(4)\nwhere rd is the final reward if the current episode\nis terminated. Secondly, we define a punishment\nby using an auto-regressive language model (LM)\nto measure fluency:\nrflu\nt =\n‚àë\ni\n1\n|xt|(LM(xi|xt)‚àíLM(xi|xt‚àí1)) (5)\nwhere LM(xi|xt) is the cross-entropy loss of the to-\nken xi in sentence xt. Thirdly, we also add seman-\ntic similarity constraints as another punishment:\nrsim\nt = Sim(x,xt‚àí1) ‚àíSim(x,xt) (6)\nFinally, our overall instant rewardrt is defined as:\nrt = Œ≤1ratt\nt ‚àíŒ≤2rflu\nt ‚àíŒ≤3rsim\nt (7)\n3.2 Decision Making\nDuring each step in the whole attack process, there\nare two types of decision-making problems. The\nfirst is choosing the appropriate token to edit, while\nthe second is replacing the token with a suitable\nsubstitution. In RL, the agent needs to determine\nthe decisions according to the yielded rewards.\n7324\nWord Finder To find the appropriate token to\nedit, we first employ the masked language mod-\nels (MLM) as an encoder to represent the state xt.\nDue to the setup of the sub-word tokenizer in MLM,\nthe encoder first converts xt to a token sequence\nxt\ntoken = [ot\n1,ot\n2,..., ot\nm]. We reverse the conver-\nsion mapping œï : xt\ntoken ‚Üíxt to recover tokens\ninto words in need. Then we obtain the hidden\nstates ht = [ht\n1,ht\n2,..., ht\nm], where ht\ni ‚ààRd is the\nhidden state of token ot\ni with ddimensions.\nFurthermore, we maintain a word set W to re-\nstore the words of x that have been already modi-\nfied as well as stop words and punctuation. We then\nadopt a simple binary representation bt according\nto the word set W:\nbt\ni =\n{\n0 ‚ààRd œï(ot\ni) ‚ààW\n1 ‚ààRd œï(ot\ni) /‚ààW (8)\nThen, we fuse both the hidden states ht\ni and the\nbinary representation bt\ni to obtain the final repre-\nsentation et\ni of the environment states:\net\ni = [ht\ni; bt\ni] (9)\nwhere [; ] denotes the concatenation operation.\nDuring the process of training, we first adopt a\nsimple linear layer to obtain the probability and\nfurther normalize it into a distribution. The proba-\nbility distribution p(ot\ni|xt) of each token at t-step\ncan be calculated as follows:\np(ot\ni|xt) = softmax(W ¬∑et\ni + b) (10)\nwhere W,b are the weight matrix and the bias vec-\ntor, respectively. Then the agent samples the word\nwt to substitute according to the distribution and\nensures the sampled word is not in the word set W.\nDuring the evaluation, the agent will directly\nselect the token with the maximum probability at\neach step, which is formulated as follows:\nwt = arg maxp(ot\ni|xt),œï(ot\ni) /‚ààW (11)\nIf the selected token wt is a sub-word, we re-\nverse the sub-word into a complete word via the\nconversion mapping œïas the newly selected word.\nWord Substitution Following Jin et al. (2020),\nwe adopt synonym substitution as our strategy af-\nter obtaining selected word wt in t-step. Firstly,\nwe gather a synonym set Swt for wt that con-\ntains top- k candidates from the external vocab-\nulary, computing via cosine similarity (Mrk≈°i ¬¥c\net al., 2016). Then, for each s ‚ààSwt , we replace\nwt\np with s in the sentence xt to get a substitution\nxt\ns = [w1,..., wp‚àí1,s,wp+1,...wn]. Finally, ac-\ncording to the instant reward rt in the Equation 4,\nwe select the substitution with the highest reward\nas the final adversaries xt\nadv. Meanwhile, the envi-\nronment states further updates as follows:\n{\nxt+1 = xt\nadv\nW = W ‚à™{wt\np} (12)\n3.3 Agent Training\nThe training target is to maximize the total return\nG(œÑ), which is an accumulated reward based on\nthe instant reward rt , defined in Equation 7, with\na discount factor Œ≥ ‚àà[0,1):\nG(œÑ) =\nT‚àë\nt=1\nŒ≥trt (13)\nThe expected return of the decision trajectory,\ni.e., attack path, is defined as follows:\nJ(Œ∏) =E[G(œÑ)] (14)\nFurthermore, we regard the agent as œÄŒ∏ with\nparameters Œ∏ and the attack path as œÑ =\n[(af\n1 ,as\n1),¬∑¬∑¬∑ ,(af\nT,as\nT)], where af\nt and as\nt rep-\nresent actions of word finder and substitution\nin t-th step, respectively. The probability\nof this attack path is calculated as œÄŒ∏(œÑ) =‚àèT\nt=1 œÄŒ∏((af\nt,as\nt)|st), where œÄŒ∏((af\nt,as\nt)|st) is the\nprobability of actions in step t based on current\nenvironment state st. Meanwhile, we consider as\nt a\nprior knowledge so that this probability can be sim-\nplified. The gradient is calculated by REINFORCE\nalgorithm (Kaelbling et al., 1996):\n‚àáJ(Œ∏) =‚àáE[log œÄŒ∏(œÑ) ¬∑G(œÑ)] (15)\nDetailed information of reinforce training is shown\nin appendix B.\n4 Experiments\n4.1 Experimental Setups\nTasks and Datasets Following Li et al. (2020b);\nJin et al. (2020), we evaluate the effectiveness of\nSDM-A TTACK mainly on two standard NLP tasks,\ntext classification and textual entailment. As for\ntext classification, we use diverse datasets from dif-\nferent aspects, including news topic classification\n(AG‚Äôs News; Zhang et al., 2015), sentence-level\nsentiment analysis (MR; Pang and Lee, 2005) and\ndocument-level sentiment analysis (IMDB 1 and\n1https://datasets.imdbws.com/\n7325\nDataset Method A-rate ‚Üë Mod‚Üì Sim‚Üë Dataset Method A-rate ‚Üë Mod ‚Üì Sim‚Üë\nYelp\nA2T 88.3 8.1 0.68\nIMDB\nA2T 89.9 4.4 0.79\nTextFooler 90.5 9.0 0.69 TextFooler 88.7 7.6 0.76\nBERT-Attack 89.8 12.4 0.66 BERT-Attack 88.2 5.3 0.78\nSDM-A TTACK 95.8 8.2 0.71 SDM-A TTACK 91.4 4.1 0.82\nAG‚Äôs News\nA2T 53.7 13.5 0.57\nMR\nA2T 58.5 12.6 0.55\nTextFooler 66.2 18.4 0.52 TextFooler 80.5 15.8 0.50\nBERT-Attack 74.6 15.6 0.52 BERT-Attack 83.2 12.8 0.52\nSDM-A TTACK 77.9 15.3 0.53 SDM-A TTACK 85.6 12.3 0.57\nSNLI\nA2T 70.8 17.2 0.35\nMNLI\nA2T 66.0 14.4 0.45\nTextFooler 84.3 17.2 0.38 TextFooler 76.5 15.0 0.45\nBERT-Attack 81.9 16.5 0.38 BERT-Attack 78.1 14.0 0.46\nSDM-A TTACK 85.5 15.9 0.43 SDM-A TTACK 78.7 13.8 0.49\nTable 1: Automatic evaluation results of attack success rate (A-rate), modification rate (Mod), and semantic\nsimilarity (Sim). ‚Üërepresents the higher the better and ‚Üìmeans the opposite. The results of MNLI dataset are the\naverage performance of MNLI-matched and MNLI-mismatched. The best results are bolded, and the second-best\nones are underlined.\nYelp Polarity; Zhang et al., 2015). As for tex-\ntual entailment, we use a dataset of sentence pairs\n(SNLI; Bowman et al., 2015) and a dataset with\nmulti-genre (MultiNLI; Williams et al., 2017). The\nstatistics of datasets and more details can be found\nin Appendix A. Following Jin et al. (2020); Alzan-\ntot et al. (2018), we attack 1k samples randomly\nselected from the test set of each task.\nBaselines We compare SDM-A TTACK with re-\ncent state-of-the-art studies: 1) TextFooler (Jin\net al., 2020): find important words via probabil-\nity weighted word saliency and then apply sub-\nstitution with counter-fitted word embeddings. 2)\nBERT-Attack (Li et al., 2020b): use mask-predict\napproach to generate adversaries. 3) A2T (Yoo\nand Qi, 2021): adopt faster search with gradient-\nbased word importance ranking algorithm. We use\nopen-source codes provided by the authors and\nTextAttack tools (Morris et al., 2020) to implement\nthese baselines. Furthermore, to ensure fairness in\ncomparing baselines and SDM-A TTACK , we apply\nconstraints to all methods following Morris et al.\n(2020) in Appendix C.\nVictim Models We conduct the main experi-\nments on a standard pre-trained language model\nBERT following (Jin et al., 2020; Li et al., 2020b).\nTo detect the generalization of SDM-A TTACK , we\nexplore the effects on more typical models as dis-\ncussed in Section 5.1. All victim models are pre-\ntrained from TextAttack (Morris et al., 2020).\nImplementation Details We adopt BERT as the\nMLM model in word finder and GPT-2 (Radford\net al., 2019) to measure fluency when computing\nrewards. To keep instant reward and punishment\nin a similar range, we set the hyper-parameters Œ≤1\nto be 1, Œ≤2 to be 1 and Œ≤3 to be 0.2. Moreover,\nthe discount factor Œ≥ is set to be 0.9 to achieve\na trade-off between instant reward and long-term\nreturn. We set the episode number as M = 200\nand the learning rate as Œ± = 3e‚àí6 with Adam as\nthe optimizer. In word substitution, the parameter\nKof the synonyms number is 50. Our experiments\nare conducted on a single NVIDIA 2080ti.\nAutomatic Evaluation Metrics Following pre-\nvious studies (Jin et al., 2020; Morris et al., 2020),\nwe use the following metrics as the evaluation cri-\nteria. 1) Attack success rate (A-rate): the degraded\nperformance after attacking target model. 2) Mod-\nification rate (Mod): the percentage of modified\nwords comparing to original text. 3) Semantic sim-\nilarity (Sim): the cosine similarity between the\noriginal text and its adversary, computing via the\nuniversal sentence encoder (USE; Cer et al., 2018).\nManual Evaluation Metrics We further manu-\nally validate the quality of the adversaries from\nthree challenging properties. 1) Human prediction\nconsistency (Con): the rate of human judgement\nwhich is consistent with ground-truth label; 2) Lan-\nguage fluency (Flu): the fluency score of the sen-\ntence, measured on a Likert scale of 1 to 5 from un-\ngrammatical to coherent (Gagnon-Marchand et al.,\n2019); 3) Semantic similarity (Simhum): the seman-\ntic consistency between each input-adversary pair,\nwhere 1 means unanimous, 0.5 means ambiguous,\n0 means inconsistent.\n7326\nDataset Con ‚Üë Flu‚Üë Simhum ‚Üë\nIMDB Original 0.95 4.5 0.95SDM-A TTACK 0.90 4.3\nMNLI Original 0.88 4.0 0.83SDM-A TTACK 0.79 3.7\nTable 2: Manual evaluation results comparing the origi-\nnal input and generated adversary by SDM-A TTACK of\nhuman prediction consistency (Con), language fluency\n(Flu), and semantic similarity (Simhum).\n4.2 Results\nAutomatic Evaluation As shown in Table 1,\nSDM-A TTACK consistently achieves the highest\nattack success rate to attack BERT in both text\nclassification and textual entailment tasks, which\nindicates the effectiveness of SDM-A TTACK . Fur-\nthermore, SDM-A TTACK mostly obtains the best\nperformance of modification and similarity met-\nrics, except for AG‚Äôs News, where SDM-A TTACK\nachieves the second-best. For instance, our frame-\nwork only perturbs 4.1% of the words on the IMDB\ndatasets, while the attack success rate is improved\nto 91.4% with a semantic similarity of 0.82. Al-\nthough A2T performs better in modification and\nsimilarity metrics in Yelp and AG‚Äôs News, their at-\ntack success rate is always much lower than SDM-\nATTACK , even other baselines. Because the mod-\nification and similarity metrics only consider the\nsuccessful adversaries, we conjecture that A2T can\nonly solve the inputs which are simpler to attack.\nIn general, our method can simultaneously satisfy\nthe high attack success rate with a lower modifica-\ntion rate and higher similarity. Furthermore, We\nfind that the attack success rate on document-level\ndatasets, i.e., Yelp and IMDB, are higher than the\nother sentence-level datasets, which indicates that\nit is easier to mislead models when the input text\nis longer. The possible reason is the victim model\ntends to use surface clues rather than understand\nthem to make predictions when the context is long.\nManual evaluation In manual evaluation, we\nfirst randomly select 100 samples from successful\nadversaries in IMDB and MNLI datasets and then\nask three crowd-workers to evaluate the quality of\nthe original inputs and our generated adversaries.\nThe results are shown in Table 2. As for the human\nprediction consistency, we regard the original in-\nputs as a baseline. Taking IMDB as an example, hu-\nmans can correctly judge 95% of the original inputs\nwhile they can maintain 90% accuracy to our gen-\nDataset Model A-rate ‚Üë Mod‚Üì Sim‚Üë\nRoBERTa 84.4 13.9 0.52\nMR WordCNN 72.1 10.3 0.48\nWordLSTM 80.7 8.9 0.56\nRoBERTa 88.3 8.3 0.70\nIMDB WordCNN 89.2 3.3 0.85\nWordLSTM 89.8 5.4 0.75\nSNLI InferSent 78.7 17.0 0.42\nESIM 79.0 17.2 0.41\nTable 3: Attack results against other models.\nDataset Method A-rate ‚Üë Mod‚Üì Sim‚Üë\nAG‚Äôs News BERT-Attack 74.6 15.6 0.52\nSDM-ATTACK-mlm 76.2 15.0 0.51\nMR BERT-Attack 83.2 12.8 0.52\nSDM-ATTACK-mlm 84.3 11.5 0.53\nTable 4: Attack results of different substitution strate-\ngies, where SDM-A TTACK -mlm is replaced with the\nsame strategy of word finder as BERT-Attack.\nerated adversaries, which indicates SDM-A TTACK\ncan mislead the PLMs while keeping human judges\nunchanged. The language fluency scores of adver-\nsaries are close to the original inputs, where the\ngap scores are within 0.3 on both datasets. Further-\nmore, the semantic similarity scores between the\noriginal inputs and our generated adversaries are\n0.95 and 0.83 in IMDB and MNLI, respectively. In\ngeneral, SDM-A TTACK can satisfy the challeng-\ning demand of preserving the three aforementioned\nproperties. Detailed design of manual evaluation\nand more results are shown in appendix E.\n5 Analyses\n5.1 Generalization\nWe detect the generalization of SDM-A TTACK in\ntwo aspects, 1) attack more language models and\n2) adapt to more substitution strategies. Firstly, we\napply SDM-A TTACK to attack extensive victim\nmodels, such as traditional language models (e.g.,\nWordCNN) and other state-of-the-art PLMs (e.g.,\nRoBERTa; Liu et al., 2019). The results of text clas-\nsification tasks in table 3 show that SDM-A TTACK\nnot only has better attack effects against Word-\nCNN and WordLSTM, but also misleads RoBERTa,\nwhich is a more robust model. For example, on\nthe IMDB datasets, the attack success rate is up\nto 89.2% against WordCNN with a modification\nrate of only about 3.3% and a high semantic sim-\nilarity of 0.85. As for the textual entailment task,\nSDM-A TTACK can also achieve remarkable attack\n7327\nFigure 3: The time cost according to varying sentence\nlengths in the IMDB dataset, smoothed with Gaussian\nfunction where kernel size is 5.\nsuccess rates against InferSent and ESIM.\nSecondly, although we directly adopt the word\nsubstitution strategy in Textfooler, this strategy\ncan actually be replaced by other methods. To\ndemonstrate this assumption, we further replace\nour word substitution strategy with the mask-fill\nway in BERT-attack, namedSDM-A TTACK -mlm.\nAs shown in Table 4, SDM-A TTACK -mlm com-\npletely beat BERT-Attack, indicating the part of\nword substitution of SDM-A TTACK has generaliza-\ntion ability to extend to different types of strategies\nand archives high performance. More results are\ndisplayed in appendix E.\n5.2 Efficiency\nIn this section, we probe the efficiency according\nto varying sentence lengths in the IMDB dataset\nas shown in Figure 3. The time cost of SDM-\nATTACK is surprisingly mostly better than A2T,\nwhich mainly targets obtaining cheaper compu-\ntation costs with lower attack success rates in\nTable 1. Meanwhile, SDM-A TTACK can obvi-\nously beat BERT-attack and TextFooler, which\nneed to conduct a model forward process for each\ntoken. Furthermore, with the increase of sentence\nlengths, SDM-A TTACK and A2T maintain a sta-\nble time cost, while the time cost of BERT-attack\nand TextFooler is exploding. These phenomena\nshow the efficiency advantage of SDM-A TTACK ,\nespecially in dealing with long texts.\n5.3 Transferability\nWe evaluate the transferability ofSDM-A TTACK to\ndetect whether the SDM-A TTACK trained on one\ndataset can perform well on other datasets. We con-\nduct experiments on a series of text classification\ntasks and use the randomly initialized BERT as a\nYelp IMDB MR AG‚Äôs News\nYelp 87.6 85.8 40.5 43.6\nIMDB 82.9 89.3 51.4 43.4\nMR 81.8 88.2 66.5 39.6\nAG‚Äôs News 62.4 59.2 29.9 53.2\nRandom 58.9 56.1 27.8 38.3\nTable 5: Transferability evaluation of SDM-A TTACK\ngenerator on text classification task against BERT. Row\ni and column j is the attack success rate of SDM-\nATTACK trained on dataset ievaluated on dataset j.\nDataset Acc ‚Üë A-rate‚Üë Mod‚Üì Sim‚Üë\nYelp 97.4 95.8 8.2 0.71\n+Adv Train 97.0 82.5 13.5 0.63\nIMDB 91.6 91.4 4.1 0.82\n+Adv Train 90.5 79.2 8.5 0.74\nSNLI 89.1 85.5 15.9 0.43\n+Adv Train 88.2 78.6 17.1 0.42\nTable 6: The results of comparing the original training\nwith adversarial training with our generated adversaries.\nMore results can be found in Appendix D.\nbaseline. As shown in Table 5, SDM-A TTACK has\nhigh transferability scores across different datasets,\nwhich are consistently higher than random. In de-\ntail, the performances among Yelp, IMDB and MR,\nwhich all belong to sentiment analysis, are higher\nthan AG‚Äôs News. Moreover, IMDB and MR are\ncorpora about movies where SDM-A TTACK tends\nto learn a general attack strategy in this field and\ncan transfer well to each other.\n5.4 Adversarial Training\nWe further investigate to improve the robustness\nof victim models via adversarial training. Specif-\nically, we fine-tune the victim model with both\noriginal training datasets and our generated adver-\nsaries, and evaluate it on the same test set. As\nshown in Table 6, compared to the results with\nthe original training datasets, adversarial training\nwith our generated adversaries can maintain close\naccuracy, while improving performance on attack\nsuccess rates, modification rates, and semantic sim-\nilarity. The victim models with adversarial training\nare more difficult to attack, which indicates that our\ngenerated adversaries have the potential to serve as\nsupplementary corpora to enhance the robustness\nof victim models.\n7328\nMethod Text (MR; Negative) Result Mod ‚Üì Sim‚Üë Flu‚Üë\nOriginal Davis is so enamored of her own creation that she can not see\nhow insufferable the character is. - - - 5\nA2T Davis is so enamored of her own institution that she can not\nbehold how unforgivable the hallmark is. Failure 22.2 0.16 3\nTextFooler Davis is well enamored of her own infancy that she could not\nadmire how infernal the idiosyncrasies is. Success 33.3 0.23 3\nBERT-Attack Davis is often enamoted of her own generation that she can not\nsee how insuffoure the queen is. Failure 27.8 0.09 2\nSDM-A TTACK Davis is so captivated of her own creation that she can‚Äôt see how\nindefensible the character is. Success 11.1 0.57 5\nTable 7: Adversaries generated by SDM-A TTACK and baselines in MR dataset. The replaced words are highlighted\nin blue. Failure indicates the adversary fails to attack the victim model and success means the opposite.\n5.5 Case Study\nTable 7 shows adversaries produced by SDM-\nATTACK and the baselines. Overall, the perfor-\nmance of SDM-A TTACK is significantly better\nthan other methods. For this sample from the MR\ndataset, only TextFooler and SDM-A TTACK suc-\ncessfully mislead the victim model, i.e., changing\nthe prediction from negative to positive. However,\nTextFooler modifies twice as many words asSDM-\nATTACK , demonstrating our work has found a more\nsuitable modification path. Adversaries generated\nby A2T and BERT-Attack are failed samples due to\nthe low semantic similarity. BERT-Attack even gen-\nerates an invalid word ‚Äúenamoted\" due to its sub-\nword combination algorithm. We also ask crowd-\nworkers to give a fluency evaluation. Results show\nSDM-A TTACK obtains the highest score of 5 as\nthe original sentence, while other adversaries are\nconsidered difficult to understand, indicatingSDM-\nATTACK can generate more natural sentences.\n6 Related Work\nAdversarial attack has been well-studied in im-\nage and speech domains (Szegedy et al., 2013;\nChakraborty et al., 2018; Kurakin et al., 2018;\nCarlini and Wagner, 2018). However, due to the\ndiscrete nature of language, the adversarial attack\nagainst pre-trained language models is much more\ndifficult. Earlier works mainly focus on designing\nheuristic rules to generate adversaries, including\nswapping words (Wei and Zou, 2019), transform-\ning syntactic structure (Coulombe, 2018), and para-\nphrasing by back-translation (Ribeiro et al., 2018;\nXie et al., 2020). However, these rule-based meth-\nods are label-intensive and difficult to scale.\nRecently, adversarial attack in NLP is framed as\na combinatorial optimization problem. Mainstream\nstudies design a series of search algorithms with\ntwo detached stages In the first stage, they itera-\ntively search for modification positions, including\nsaliency-based ranking (Liang et al., 2017; Ren\net al., 2019; Jin et al., 2020; Garg and Ramakrish-\nnan, 2020), gradient-based descent algorithm (Sato\net al., 2018; Yoo and Qi, 2021), and temporal-based\nsearcher (Gao et al., 2018). In the second stage,\na series of studies designs different substitution\nstrategies, including dictionary method (Ren et al.,\n2019), word embeddings (Kuleshov et al., 2018; Jin\net al., 2020) or language models (Li et al., 2020b;\nGarg and Ramakrishnan, 2020; Li et al., 2020a). In\nthis paper, we formally propose to define the adver-\nsarial attack task as a sequential decision-making\nproblem, further considering that scores in the next\nstep are influenced by the editing results in the\ncurrent step.\nThe other line of recent studies is sampling-\nbased methods. Alzantot et al. (2018) and Wang\net al. (2019) apply genetic-based algorithm, Zang\net al. (2019) propose a particle swarm optimization-\nbased method, and Guo et al. (2021) generate adver-\nsaries via distribution approximate sampling. How-\never, their execution time is much more expensive\ndue to the properties of sampling, so it is unlikely\nto generate large-scale adversarial samples. In ad-\ndition, Zou et al. (2019) conducts reinforcement\nlearning on attacking the neural machine transla-\ntion task, but their search path is fixed from left to\nright. In this paper, SDM-A TTACK can determine\nany search order to find the appropriate attack path.\n7 Conclusion\nIn this paper, we formally define the adversarial at-\ntack task as a sequential decision-making problem,\n7329\nconsidering the entire attack process as sequence\nwith two types of decision-making problems, i.e.,\nword finder and substitution. To solve this problem\nwithout any direct signals of intermediate steps,\nwe propose to use policy-based RL to find an\nappropriate attack path, entitled SDM-A TTACK .\nOur experimental results show that SDM-A TTACK\nachieves the highest attack success rate. In this\npaper, we use our designed rewards as instant sig-\nnals to solve these two decision-making problems\napproximately. We will further try to adopt hierar-\nchical RL to optimize the solution.\n8 Limitations\nWe define the adversarial attack task as a sequential\ndecision-making problem and apply policy-based\nreinforcement learning to model it. This work must\nfollow this assumption: the decision process con-\nforms to Markov decision process (MDP) that the\nconditional probability distribution of the future\nstate depends only on the current state. Meanwhile,\nreinforcement learning training requires additional\ntime costs and the results may be unstable.\nWe only conduct the experiments on two NLP\ntasks with six selected datasets, which are all En-\nglish corpus. Furthermore, our experimental results\nare mainly for BERT, with RoBERTa supplemented\nin the analysis. Thus, we lack the evaluation of\nother novel pre-trained language models, such as\nELECTRA (Clark et al., 2020) and XLNET (Yang\net al., 2019). Therefore, our work lacks multi-task,\nmulti-model and multilingual verification in terms\nof generalization and transferability.\n9 Ethics Statement\nWe declare that this article is in accordance with the\nethical standards of ACL Code of Ethics. Any third\nparty tools used in this work are licensed from their\nauthors. All crowd-workers participating in the\nexperiments are paid according to the local hourly\nwages.\n10 Acknowledgment\nWe would like to thank anonymous reviewers for\ntheir insightful and constructive feedback. We ap-\npreciate Peng Li and Shuo Wang for their valuable\ndiscussions. We thank Qianlin Liu, Yanqi Jiang and\nYiwen Xu for the crowdsourced work. This work\nis supported by the National Key R&D Program\nof China (2022ZD0160502) and the National Nat-\nural Science Foundation of China (No. 61925601,\n62276152, 62236011).\nReferences\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary,\nBo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.\n2018. Generating natural language adversarial exam-\nples. arXiv preprint arXiv:1804.07998.\nDavid F Armstrong, William C Stokoe, and Sherman E\nWilcox. 1995. Gesture and the nature of language.\nCambridge University Press.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. arXiv\npreprint arXiv:1508.05326.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. CoRR,\nabs/2005.14165.\nNicholas Carlini and David Wagner. 2018. Audio adver-\nsarial examples: Targeted attacks on speech-to-text.\nIn 2018 IEEE security and privacy workshops (SPW),\npages 1‚Äì7. IEEE.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\net al. 2018. Universal sentence encoder. arXiv\npreprint arXiv:1803.11175.\nAnirban Chakraborty, Manaar Alam, Vishal Dey, Anu-\npam Chattopadhyay, and Debdeep Mukhopadhyay.\n2018. Adversarial attacks and defences: A survey.\narXiv preprint arXiv:1810.00069.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors.\nClaude Coulombe. 2018. Text data augmentation made\nsimple by leveraging nlp cloud apis. arXiv preprint\narXiv:1812.04718.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJules Gagnon-Marchand, Hamed Sadeghi, Md Haidar,\nMehdi Rezagholizadeh, et al. 2019. Salsa-text: self\n7330\nattentive latent space based adversarial text genera-\ntion. In Canadian Conference on Artificial Intelli-\ngence, pages 119‚Äì131. Springer.\nJi Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun\nQi. 2018. Black-box generation of adversarial text\nsequences to evade deep learning classifiers. In 2018\nIEEE Security and Privacy Workshops (SPW), pages\n50‚Äì56. IEEE.\nSiddhant Garg and Goutham Ramakrishnan. 2020. Bae:\nBert-based adversarial examples for text classifica-\ntion. arXiv preprint arXiv:2004.01970.\nChuan Guo, Alexandre Sablayrolles, Herv√© J√©gou, and\nDouwe Kiela. 2021. Gradient-based adversarial\nattacks against text transformers. arXiv preprint\narXiv:2104.13733.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is bert really robust? a strong base-\nline for natural language attack on text classification\nand entailment. In Proceedings of the AAAI con-\nference on artificial intelligence, volume 34, pages\n8018‚Äì8025.\nLeslie Pack Kaelbling, Michael L Littman, and An-\ndrew W Moore. 1996. Reinforcement learning: A\nsurvey. Journal of artificial intelligence research ,\n4:237‚Äì285.\nV olodymyr Kuleshov, Shantanu Thakoor, Tingfung Lau,\nand Stefano Ermon. 2018. Adversarial examples for\nnatural language classification problems.\nAlexey Kurakin, Ian Goodfellow, Samy Bengio, Yin-\npeng Dong, Fangzhou Liao, Ming Liang, Tianyu\nPang, Jun Zhu, Xiaolin Hu, Cihang Xie, et al. 2018.\nAdversarial attacks and defences competition. In The\nNIPS‚Äô17 Competition: Building Intelligent Systems,\npages 195‚Äì231. Springer.\nDianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris\nBrockett, Ming-Ting Sun, and Bill Dolan. 2020a.\nContextualized perturbation for textual adversarial\nattack. arXiv preprint arXiv:2009.07502.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,\nand Xipeng Qiu. 2020b. Bert-attack: Adversar-\nial attack against bert using bert. arXiv preprint\narXiv:2004.09984.\nBin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian,\nXirong Li, and Wenchang Shi. 2017. Deep\ntext classification can be fooled. arXiv preprint\narXiv:1704.08006.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nGary Marcus. 2020. The next decade in ai: four steps\ntowards robust artificial intelligence. arXiv preprint\narXiv:2002.06177.\nJohn X Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby,\nDi Jin, and Yanjun Qi. 2020. Textattack: A frame-\nwork for adversarial attacks, data augmentation,\nand adversarial training in nlp. arXiv preprint\narXiv:2005.05909.\nNikola Mrk≈°i¬¥c, Diarmuid O S√©aghdha, Blaise Thom-\nson, Milica Ga≈°i¬¥c, Lina Rojas-Barahona, Pei-Hao Su,\nDavid Vandyke, Tsung-Hsien Wen, and Steve Young.\n2016. Counter-fitting word vectors to linguistic con-\nstraints. arXiv preprint arXiv:1603.00892.\nJes√∫s Oliva, Jos√© Ignacio Serrano, Mar√≠a Dolores\nDel Castillo, and √Ångel Iglesias. 2011. Symss: A\nsyntax-based measure for short-text semantic similar-\nity. Data & Knowledge Engineering, 70(4):390‚Äì405.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. arXiv preprint cs/0506075.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.\n2019. Generating natural language adversarial ex-\namples through probability weighted word saliency.\nIn Proceedings of the 57th annual meeting of the as-\nsociation for computational linguistics, pages 1085‚Äì\n1097.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2018. Semantically equivalent adversarial\nrules for debugging nlp models. In Annual Meet-\ning of the Association for Computational Linguistics\n(ACL).\nMotoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji\nMatsumoto. 2018. Interpretable adversarial perturba-\ntion in input embedding space for text.arXiv preprint\narXiv:1805.02917.\nMichael Studdert-Kennedy. 2005. How did language\ngo discrete. Language origins: Perspectives on evo-\nlution, ed. M. Tallerman, pages 48‚Äì67.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever,\nJoan Bruna, Dumitru Erhan, Ian Goodfellow, and\nRob Fergus. 2013. Intriguing properties of neural\nnetworks. arXiv preprint arXiv:1312.6199.\nScott Thiebes, Sebastian Lins, and Ali Sunyaev. 2021.\nTrustworthy artificial intelligence. Electronic Mar-\nkets, 31(2):447‚Äì464.\nX Wang, H Jin, and K He. 2019. Natural language\nadversarial attacks and defenses in word level. arXiv\npreprint arXiv:1909.06723.\nJason Wei and Kai Zou. 2019. Eda: Easy data augmenta-\ntion techniques for boosting performance on text clas-\nsification tasks. arXiv preprint arXiv:1901.11196.\n7331\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\nand Quoc Le. 2020. Unsupervised data augmenta-\ntion for consistency training. Advances in Neural\nInformation Processing Systems, 33:6256‚Äì6268.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nJin Yong Yoo and Yanjun Qi. 2021. Towards improving\nadversarial training of nlp models. arXiv preprint\narXiv:2109.00544.\nYuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan\nLiu, Meng Zhang, Qun Liu, and Maosong Sun.\n2019. Word-level textual adversarial attacking\nas combinatorial optimization. arXiv preprint\narXiv:1910.12196.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text classi-\nfication. Advances in neural information processing\nsystems, 28.\nWei Zou, Shujian Huang, Jun Xie, Xinyu Dai, and Jia-\njun Chen. 2019. A reinforced generation of adversar-\nial examples for neural machine translation. arXiv\npreprint arXiv:1911.03677.\n7332\nA Datasets\nWe conduct experiments on the following datasets\nof two NLP tasks and detailed statistics are dis-\nplayed in Table 8:\n‚Ä¢ Text Classification: (1) Yelp (Zhang et al.,\n2015): A dataset for binary sentiment classi-\nfication on reviews, constructed by consider-\ning stars 1 and 2 negative, and 3 and 4 pos-\nitive. (2) IMDB: A document-level movie\nreview dataset for binary sentiment analysis.\n(3) MR (Pang and Lee, 2005): A sentence-\nlevel binary classification dataset collected\nfrom Rotten Tomatoes movie reviews. (4)\nAG‚Äôs News (Zhang et al., 2015): A collec-\ntion of news articles. There are four topics\nin this dataset: World, Sports, Business, and\nScience/Technology.\n‚Ä¢ Textual Entailment: (1) SNLI (Bowman\net al., 2015): A dataset of human-written En-\nglish sentence pairs and manually annotated\nlabels of entailment, neutral and contradiction.\n(2) MNLI (Williams et al., 2017): Another\ncrowd-sourced collection of sentence pairs\nlabeled with textual entailment information.\nCompare to SNLI, it includes more complex\nsentences, e.g, enres of spoken and written\ntext.\nB Training Algorithm\nThe training process is shown in Algorithm 1.\nSince af\nt is chosen through a probability distri-\nbution, the agent is encouraged to explore more\npossible paths. The instant reward rt is obtained\nfrom environment after performing both two ac-\ntions actions. Once the termination signal is raised,\nthe environment will terminate this current episode\nand update the agent‚Äôs parameters via a policy gra-\ndient approach. The expected return of decision\ntrajectory is defined as follows:\nJ(Œ∏) =E[G(œÑ)] (16)\nThus the gradient is calculated by REINFORCE\nalgorithm (Kaelbling et al., 1996):\n‚àáJ(Œ∏) =‚àáE[log œÄŒ∏(œÑ) ¬∑G(œÑ)] (17)\nThen the expectation over the whole sequence is\napproximated by Monte Carlo simulations and can\nbe expressed as follows:\n‚àáJ(Œ∏) = 1\nM\nM‚àë\nm=1\n‚àálog œÄŒ∏(œÑ(m))G(œÑ(m)) (18)\nDataset Train Test Avg Len Classes\nYelp 560k 38k 152 2\nIMDB 25k 25k 215 2\nAG‚Äôs News 120k 7.6k 73 4\nMR 9k 1k 20 2\nSNLI 570k 3k 8 3\nMNLI 433k 10k 11 3\nTable 8: Overall statistics of datasets.\nAlgorithm 1 Reinforce Training\n1: Initialization: agent œÄŒ∏ with parameters Œ∏,\nepisode number M\n2: for i‚Üê1 to M do\n3: initialize t‚Üê1\n4: while not receive termination signal do\n5: get environment state st\n6: compute œÄŒ∏((af\nt,as\nt)|st) ‚àΩ œÄŒ∏(af\nt|st)\n7: sample af\nt based on probability\n8: select as\nt from prior knowledge\n9: compute reward rt\n10: update t‚Üêt+ 1\n11: end while\n12: initialize G(œÑ) ‚Üê 0\n13: for j ‚ÜêT to 1 do\n14: G(œÑ) ‚ÜêŒ≥G(œÑ) +rj\n15: accumulate Jj(Œ∏)\n16: end for\n17: update Œ∏‚ÜêŒ∏+ Œ±‚àáJ(Œ∏)\n18: end for\nwhere [œÑ(1),œÑ(2),...,œÑ (M)] are M samples of tra-\njectories. The discount factor Œ≥enables both long-\nterm and immediate effects to be taken into account\nand trajectories with shorter lengths are encour-\naged.\nWe randomly select 2500 items from the train-\ning corpus for training the agent of each dataset.\nThe average convergence time is approximately be-\ntween 2-16 hours, related to the length of the input.\nWhen attacking large batches of samples, the im-\npact of training cost is negligible compared to the\ncumulative attack time cost. During training, We\nadopt random strategies and short-sighted strate-\ngies in the initial stage for early exploration and to\nobtain better seeds.\nC Implementation Constraint\nIn order to make the comparison fairer, we set the\nfollowing constraints for SDM-A TTACK as well as\nall baselines: (1) Max modification rate: To better\n7333\nDataset Acc ‚Üë A-rate‚Üë Mod‚Üì Sim‚Üë\nYelp 97.4 95.8 8.2 0.71\n+Adv Train 97.0 82.5 13.5 0.63\nIMDB 91.6 91.4 4.1 0.82\n+Adv Train 90.5 79.2 8.5 0.74\nAG‚Äôs News 94.6 77.9 15.3 0.53\n+Adv Train 91.8 50.6 23.3 0.50\nMR 96.9 85.6 12.3 0.57\n+Adv Train 92.4 72.0 16.7 0.57\nSNLI 89.1 85.5 15.9 0.43\n+Adv Train 88.2 78.6 17.1 0.42\nMNLI 84.5 78.7 13.8 0.49\n+Adv Train 76.8 58.6 15.2 0.49\nTable 9: Adversarial training results.\nmaintain semantic consistency, we only keep ad-\nversarial samples with less than 40% of the words\nto be perturbed. (2) Part-of-speech (POS): To\ngenerate grammatical and fluent sentences, we use\nNLTK tools2 to filter candidates that have a differ-\nent POS from the target word. This constraint is\nnot employed on BERT-Attack. (3) Stop words\npreservation: the modification of stop words is\ndisallowed and this constraint helps avoid gram-\nmatical errors. (4) Word embedding distance :\nFor Textfooler, A2T and SDM-A TTACK , we only\nkeep candidates with word embedding cosine simi-\nlarity higher than 0.5 from synonyms dictionaries\n(Mrk≈°i¬¥c et al., 2016). Formask-fill methods, follow-\ning BERT-Attack, we filter out antonyms (Li et al.,\n2020b) via the same synonym dictionaries for sen-\ntiment classification tasks and textual entailment\ntasks.\nD Tuning with Adversaries\nTable 9 displays adversarial training results of all\ndatasets. Overall, after fine-turned with both origi-\nnal training datasets and adversaries, victim model\nis more difficult to attack. Compared to original re-\nsults, accuracy of all datasets is barely affected,\nwhile attack success rate meets an obvious de-\ncline. Meanwhile, attacking model with adversarial\ntraining leads to higher modification rate, further\ndemonstrating adversarial training may help im-\nprove robustness of victim models.\nE Supplementary Results\nAt the beginning of manual evaluation, we provided\nsome data to allow crowdsourcing workers to unify\n2https://www.nltk.org/\nDataset Con ‚Üë Flu‚Üë Simhum ‚Üë\nIMDB\nOriginal 0.95 4.5\nTextFooler 0.84 4.0 0.88\nBert-Attack 0.83 4.2 0.90\nSDM-A TTACK 0.90 4.3 0.95\nMNLI Original 0.88 4.0\nTextFooler 0.77 3.5 0.80\nBert-Attack 0.77 3.6 0.81\nSDM-A TTACK 0.79 3.7 0.83\nTable 10: Manual evaluation results comparing the orig-\ninal input and generated adversary by attack method of\nhuman prediction consistency (Con), language fluency\n(Flu), and semantic similarity (Simhum).\nDataset Method A-rate ‚Üë Mod‚Üì Sim‚Üë\nYelp BERT-Attack 89.8 12.4 0.66\nSDM-ATTACK-mlm 90.0 10.6 0.65\nIMDB BERT-Attack 88.2 5.3 0.78\nSDM-ATTACK-mlm 88.5 5.1 0.78\nAG‚Äôs News BERT-Attack 74.6 15.6 0.52\nSDM-ATTACK-mlm 76.2 15.0 0.51\nMR BERT-Attack 83.2 12.8 0.52\nSDM-ATTACK-mlm 84.3 11.5 0.53\nTable 11: Attack results of different substitution strate-\ngies, where SDM-A TTACK -mlm is replaced with the\nsame strategy of word finder as BERT-Attack.\nthe evaluation standards. We also remove the data\nwith large differences when calculating the average\nvalue to ensure the reliability and accuracy of the\nevaluation results. More manual evaluation results\nare shown in Table 10.\nTable 11 displays the generalization ability of\nSDM-A TTACK with mask-fill strategy. However,\nthe improvement effect is not particularly obvi-\nous. The mask-fill method makes the current candi-\ndate synonyms also affected by the sequence states.\nCompared to a fixed synonym dictionary, it has a\nlarger prior knowledge and changing action space,\nwhich makes it harder to train the agent. Only in-\ncreasing the size of the training corpus is not very\neffective. We will try adopting hierarchical RL to\nfurther solve this problem in the future.\n7334\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\n8\n‚ñ°\u0017 A2. Did you discuss any potential risks of your work?\nThere is no risks in our work.\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\n1\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nWe write this paper ourselves.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\n4,9\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\n4\n‚ñ°\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\n9\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n4\n‚ñ°\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nOur experiments are performed on public datasets.\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n4, appendix\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nWe provide statistics of datasets in appendix and detail information about baselines in section 4.\nC ‚ñ°\u0013 Did you run computational experiments?\n4\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7335\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4\nD ‚ñ°\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\n4\n‚ñ°\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\n4\n‚ñ°\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\n9\n‚ñ°\u0017 D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nWe have used publicly available datasets.\n‚ñ°\u0017 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nOur work does not involve collecting ethically relevant information.\n‚ñ°\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\n4\n7336",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8071397542953491
    },
    {
      "name": "Adversarial system",
      "score": 0.6118959784507751
    },
    {
      "name": "Generalization",
      "score": 0.5883113145828247
    },
    {
      "name": "Task (project management)",
      "score": 0.5757720470428467
    },
    {
      "name": "Language model",
      "score": 0.5555925369262695
    },
    {
      "name": "Reinforcement learning",
      "score": 0.5169750452041626
    },
    {
      "name": "Artificial intelligence",
      "score": 0.509369969367981
    },
    {
      "name": "Process (computing)",
      "score": 0.46148601174354553
    },
    {
      "name": "Word (group theory)",
      "score": 0.4506622850894928
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.44650956988334656
    },
    {
      "name": "Attack model",
      "score": 0.4413844645023346
    },
    {
      "name": "Machine learning",
      "score": 0.40534529089927673
    },
    {
      "name": "Computer security",
      "score": 0.2666912078857422
    },
    {
      "name": "Mathematics",
      "score": 0.07684451341629028
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210156423",
      "name": "National Engineering Research Center for Information Technology in Agriculture",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    }
  ]
}