{
  "title": "Guidance for researchers and peer-reviewers on the ethical use of Large Language Models (LLMs) in scientific research workflows",
  "url": "https://openalex.org/W4376643711",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2116947593",
      "name": "Ryan Watkins",
      "affiliations": [
        "George Washington University"
      ]
    },
    {
      "id": "https://openalex.org/A2116947593",
      "name": "Ryan Watkins",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6812490683",
    "https://openalex.org/W6600553734",
    "https://openalex.org/W6605584388",
    "https://openalex.org/W2098846919",
    "https://openalex.org/W2007256608",
    "https://openalex.org/W3039885489",
    "https://openalex.org/W1970588407",
    "https://openalex.org/W3203321135",
    "https://openalex.org/W6600164255",
    "https://openalex.org/W6635750829"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)1 3\nAI and Ethics (2024) 4:969–974 \nhttps://doi.org/10.1007/s43681-023-00294-5\nOPINION PAPER\nGuidance for researchers and peer‑reviewers on the ethical use \nof Large Language Models (LLMs) in scientific research workflows\nRyan Watkins1 \nReceived: 18 April 2023 / Accepted: 2 May 2023 / Published online: 16 May 2023 \n© The Author(s), under exclusive licence to Springer Nature Switzerland AG 2023\nAbstract\nFor researchers interested in exploring the exciting applications of Large Language Models (LLMs) in their scientific inves-\ntigations, there is currently limited guidance and few norms for them to consult. Similarly, those providing peer-reviews on \nresearch articles where LLMs were used are without conventions or standards to apply or guidelines to follow. This situation \nis understandable given the rapid and recent development of LLMs that are capable of valuable contributions to research \nworkflows (such as OpenAI’s ChatGPT). Nevertheless, now is the time to begin the development of norms, conventions, and \nstandards that can be applied by researchers and peer-reviewers. By applying the principles of Artificial Intelligence (AI) \nethics, we can better ensure that the use of LLMs in scientific research aligns with ethical principles and best practices. This \neditorial hopes to inspire further dialogue and research in this crucial area of scientific investigation.\nKeywords Large Language Model · LLM · Research · Science · Norms · Conventions · Standards\n1 Introduction\nRecent advancements in Large Language Models (LLMs), \nsuch as OpenAI’s ChatGPT 4 [1 ] and Google’s LaMDA \n[2], have inspired developers and researchers alike to find \nnew applications and uses for these groundbreaking tools. \n[3] From applications that summarize one, or one thou-\nsand, research papers, to those that let users \"chat\" with a \nresearch publication, many innovative techniques and crea-\ntive products have been developed in the past few months. \nMost recently, the first wave of research articles that use \nLLMs in their scientific research workflows have started to \nshow up – primarily as preprints at this stage (for instance, \n[4–7]). As with many new research methods, statistical \ntechniques, or technologies, the use of new tools \"in the \nwild\" routinely precedes agreement on the norms, conven-\ntions, and standards that guide their application. LLMs are \nno exception, with many researchers exploring their pos-\nsible applications at numerous phases of scientific research \nworkflows. Therefore, now is the time to start establish-\ning norms, conventions, and standards [ 8, 9] for the use of \nLLMs in scientific research, both as guidance for researchers \nand peer-reviewers, and as a starting place to guide future \nresearch into establishing these as foundations for apply -\ning the principles of Artificial Intelligence (AI) ethics in \nresearch practice.\nThe ethical use of LLMs in scientific research requires \nthe development of norms, conventions, and standards. Just \nas researchers apply norms, conventions, and/or standards \nto hypothesis testing, regression, or CRISPR applications, \nresearchers can benefit from guidance on how to both use, \nand report on their use, of LLMs in their research. 1 Simi-\nlarly, for those providing peer-reviews of scientific research \npapers that use LLMs in their methods, guidance on current \nconventions and standards will be valuable. The implemen-\ntation of norms, conventions, and standards plays a critical \nrole in ensuring the ethical use of artificial intelligence (AI) \nin scientific research, bridging the gap between theoretical \nframeworks and their practical application. This is particu-\nlarly relevant in research involving Large Language Models \n(LLMs)..\n * Ryan Watkins \n rwatkins@gwu.edu\n1 Educational Technology Leadership, George Washington \nUniversity, Washington, DC, USA\n1 For example, a norm in international economics research is com-\nparability (i.e.,the desire to compare statistics across countries) [10], \nwhere as a long-standing convention in the social sciences is to use \na value of /u1D6FC = 0.05 to define a statistically significant finding [11]. \nWhile IEEE’s P11073-10426 is a standard that defines a communica-\ntion framework for interoperability with personal respiratory equip-\nment [12].\n970 AI and Ethics (2024) 4:969–974\n1 3\nThe creation and study of LLMs is a rapidly advanc-\ning field [3]. With the growing use of LLMs it is expected \nthat the norms, conventions, and standards will evolve as \nnew tools and techniques are introduced. Nevertheless, it is \nimportant to begin the foundation building process so that \ninitial guidance can be systematically improved over time. \nIn this editorial I propose an initial set of considerations that \ncan (i) be applied by researchers to guide their use of LLMs \nin their workflows, and (ii) be utilized by peer-reviewers \nto assess the quality and ethical implications of LLMs use \nin the articles they review. These initial norms, conven-\ntions, and standards for what should be considered during \nthe research process, and included in reports or articles on \nresearch that used LLMs, are a starting place with the goal \nof providing an ethical foundation for future dialogue on \nthis topic.2 The proposed foundation should ideally identify \nkey research questions that will be explored in the com-\ning months, such as determining the appropriate conven -\ntions for setting LLM temperature parameters and assessing \npotential disciplinary and field-specific variations in these \nconventions.\n2  Framework\nThe following is an initial framework of proposed norms \nthat researchers and peer-reviewers should consider when \nusing LLMs in scientific research. While this framework is \nnot intended to be comprehensive, it provides a foundation \non which researchers can build and develop conventions and \nstandards.\nThe proposed framework (which includes, context, \nembeddings, fine tuning, agents, ethics) was derived from \nthe key considerations of researchers using LLMs. These \nconsiderations range from determining if LLMs are going \nto used in combination with other research tools and decid-\ning when to customize LLMs with embedding models, to \nfine tuning the performance of LLMs and ensuring that \nresearch retains ethical rigor. As such, the proposed frame-\nwork captures many unique considerations to using LLMs \nin the workflows of scientific research. Described first are \nthe up-front considerations for researchers who plan to use \nLLMs in their workflows, followed by a checklist of ques-\ntions (within the same framework) peer-reviewers should \nconsider when reviewing articles or reports that apply LLMs \nin their methods.\n2.1  Context\nThe context in which LLMs are used in research workflows \nis important to their appropriate and ethical application. Ini-\ntial considerations of researchers should include:\n• Are LLMs appropriate for the research questions and \ndata?\n• Will LLMs be used along with other methods or tools?\n• Will the study be pre registered?\nLLMs are not, of course, appropriate for all research ques-\ntions or data types. Researchers should begin with their \nresearch question(s) and then determine if/how LLMs might \nbe applied. LLMs may, for instance, be an appropriate com-\nponent of data collection (e.g., writing interview questions), \ndata preparation (e.g., fuzzy joining of data sets), and/or \ndata analysis (e.g., sentiment analysis, optimizing code). \nFor example, in analyzing qualitative data a researcher \nmay choose to use traditional qualitative data analysis \nsoftware and techniques (such as, coding or word counts \nwith Nvivo or Atlas TI) along with a LLM for comparing \nsemantics across samples. Within this context, the use of \nthe LLM complements other analysis techniques, allowing \nthe researcher to explore more diverse questions of inter -\nest. Whereas in other contexts all of the research questions \nmay be best explored with just LLMs or another traditional \nmethod. In their reporting, researchers should describe and \njustify the complete methods applied in their research and \nthe full list of LLM tools selected since each may be spe-\ncialized for a different task. Likewise, if the research study \nwas pre registered, any subsequent articles or reports should \ninclude both the pre registration URL and discussion of any \nchanges made from the original pre registered research \nplan—especially when those changes are based on the test-\ning and fine tuning of LLMs.\n2.2  Embedding Models\nAdding a custom embedding model(s) to complement the \nbase LLM (such as OpenAI’s ChatGPT) can enhance the \nvalue of LLMs for specific research task(s). Initial consid-\nerations of researchers should include:\n• Will a custom embedding model(s) help meet the goals \nof the research?\n• What tool(s) will be used to create the embedding \nmodel(s)?\n• Will multiple embedding models created and tested (i.e., \nchained)?\n• What size of chunks will be used in preparing the data \nfor the embedding(s)?\n2 Research and updated guidance for using LLMs in scientific \nresearch workflows are available on the clearinghouse website: \nhttps:// LLMin Scien ce. com.\n971AI and Ethics (2024) 4:969–974 \n1 3\n• Will overlap across chunks be permitted?\n• What tool will be used for similarity matching (i.e., vec-\ntor database)?\n• Will the code for creating embedding model(s) be made \npublicly available?\nWhile the web interface for some LLMs (such as ChatGPT) \ncan be valuable for some research questions, many times \nsupplemental content (in addition to a base LLM, such as \nGPT-3.5 or GPT-4) is important to the research. Custom \nembedding models allow researchers to extent the base LLM \nwith content of their choosing. Technically, \"Embeddings \nare vectors or arrays of numbers that represent the mean-\ning and the context of the tokens that the model processes \nand generates. Embeddings are derived from the parameters \nor the weights of the model, and are used to encode and \ndecode the input and output texts. Embeddings can help the \nmodel to understand the semantic and syntactic relationships \nbetween the tokens, and to generate more relevant and coher-\nent texts\" [13]. While LLMs use embeddings to create their \nbase models (such as, GPT-4), researchers can also create \nembeddings with specialized content (such as a corpus of \nresearch articles on a topic, a drive of interview transcripts, \nor a database of automobile descriptors) to expand the inputs \nused by the LLM. Researchers can also chain together mul-\ntiple embedding models in improve LLM performance [14].\nThere are numerous embedding models [algorithms] that \ncan be used by researchers to create an embeddings file for \nuse in their research [ 15]. Embedding models use a vari -\nety of algorithms to create the custom embeddings file, and \ntherefore it is important for researchers to be transparent \nabout their procedures in selecting and creating embeddings \nfor use in their workflow. The preparation of data for creat-\ning the embedding model(s) can also influence the result-\ning embeddings and thereby the outputs of the LLMs when \nused in the workflow. For example, text has be divided into \nchunks in preparation for creating the embeddings and the \nsize of chunks used will define the cut-off points for creating \nvectors. Researchers can, for instance, divide the text data \ninto chunks of 1000 tokens, or 500 tokens. Depending on the \ncontext of the research, one dividing point for chunking may \nbe more valuable than another. Chunking can also be done \nusing sentence splitting in order to keep sentences together, \nor not. Likewise, researchers can allow for some overlap \nbetween chunks in order to maintain semantic context [16]. \nEach of these decisions can influence the output of the LLM \nwhen using additional embeddings, and thus should be con-\nsidered in the research procedures and included in subse-\nquent reporting.\nAfter a embeddings are created for the additional content \nto be used in conjunction with the base LLM, the embed -\ndings have to be stored in a database so that the data can be \nmanaged and searched. Vector databases (or vectorestores) \nare used, and there are many options researchers can choose \namongst [17]. Vector databases use different heuristics and \nalgorithms to index and search vectors, and can perform \ndifferently. Vector databases may use different neural search \nframeworks, such as FAISS, Jina.AI, or Haystack, and cus-\ntom algorithms [18]. While the selection of a vector data-\nbase mostly influences performance (i.e., speed, more than \nLLM outputs) it is useful for researchers to be transparent \non their selection. In the future, differences in neural search \nframeworks, algorithms, and vector database technologies \nmay lead to substantive differences in LLM outputs as well.\n2.3  Fine Tuning\nThere are many Large Language Models (LLMs) available \nto researchers [19] and the selection of which LLM to use \nin a specific research workflow requires several decisions, \nincluding:\n• Which language model will used (e.g., OpenAI’s GPT-\n3.5, GPT-4, open source alternative)?\n• Will multiple language models be tested for performance \nin the research task(s)?\n• Will completion parameters be applied (e.g., temperature, \npresence penalty, frequency penalty, max tokens, logit \nbias, stops)?\n• Will multiple combinations of completion parameters be \ntested before or during the research?\n• Will systematic “prompt engineering” be done as part of \nthe research?\n• What quality review and validation checks will be per -\nformed on LLM-generated results?\n• Will the LLM’s performance be compared with bench-\nmarks or standards for the field or discipline?\n• Will the code for fine tuning the LLM be made publicly \navailable?\nBeyond the standard user interface and default settings \noffered by many LLMs (such as the ChatGPT website), by \nusing an Application Programming Interface (API) research-\ners can fine tune LLMs for their research. Fine tuning can \nbe done with or without using a embedding model(s), and \nis currently done primarily through setting the completion \nparameters (e.g., temperature) and by conducting “prompt \nengineering” (i.e., systematically improving LLM prompts \nto provide outputs with desired characteristics). Additional \nfine tuning options should however be expected as LLMs \nevolve and more competing LLMs become available to \nresearchers.\nCurrently there are no conventions or standards for set-\nting completion parameters when using LLMs in scientific \nresearch. For instance, two common parameters used to \ninfluence the outputs of LLMs are tokens and temperature.\n972 AI and Ethics (2024) 4:969–974\n1 3\n2.3.1  Tokens\nTokens are unit of analysis of LLMs, and they are roughly \nequivalent to about a word, but not always. Researchers can \nselect the number of tokens to be returned to complete a \nrequest, and the LLM will complete the request within that \nconstraint [20]. Depending on the size of the LLM there \nmay be limits on the total number of tokens that can be \nrequested. There are no conventions or standards at this \ntime for the ideal maximum number of tokens a researcher \nshould request in order to get results, and this will routinely \nbe dependent on the research context in which they are using \nthe LLM. In general however, LLMs have been observed \nto ramble on at time (i.e., filling the maximum number of \ntokens) and to provide less accurate outputs toward the end \nwhen the maximum token parameter is set too high.\n2.3.2  Temperature\nTemperature [20] is used to provide the LLM with additional \nflexibility in how it completes a request. At the lowest tem-\nperature setting (e.g., 0) then the LLM is limited to selecting \nthe next word/token that has the highest probability in the \nmodel (also see, “top p” parameter [20]). As the researcher \nincreases the temperature ( ≤ 2 with OpenAI’s LLMs), the \nLLM may select from an increasing range of probabilities \nfor the next word/token. Setting an appropriate temperature \nfor the unique research context is therefore important, and \nin the future we will hopefully have conventions (by field \nand/or disciplines) on appropriate temperature parameters \nfor research.\nOther completion parameters can also influence the out-\nputs of LLMs (e.g., “presence penalty”, “frequency pen-\nalty”, “logit bias”) and we should expect that new LLMs will \nexpand the range of completion parameters that researchers \ncan apply. It should be the norm, therefore, for researchers to \nclearly state the applied completion parameters used in their \nresearch, and describe any testing of different parameter set-\ntings done in evaluating and selecting the final parameter \nsettings.\nPrompts are the inputs provided by researchers to request \na LLM response. Prompts are converted to tokens and used \nto inform predictions about what the following words/tokens \nshould be in the output. Behind the curtain, LLMs are using \nprobabilities for the various permutations and combinations \nof tokens/words that could follow. Changing the prompt, for \ninstancing changing the wording of the prompt or including \nmore prior prompts from the history of a conversation, can \nsubstantially influence the LLM’s outputs [21, 22]. Prompt \nengineering is the systematic manipulation of prompts in \norder to improve outputs, and researchers should be trans-\nparent about both their prompt engineering procedures and \nthe final prompts used to in the research.\nAt this time, however, “There are no reliable techniques \nfor steering the behavior of LLMs” [3]. While transparency \nof research “prompt engineering” practices is essential, \nwhen using LLMs in research transparency may not lead to \nreproducability—and therefore limit generalizability.\n2.4  Agents\nThe automation of LLM tasks can be important in some \nresearch contexts. If using automated LLM tools (i.e., \nagents) researcher considerations should include:\n• Will LLM agent(s) used in the research?\n• How many and in what sequence will LLM agent(s) \nused?\n• Will the code for creating the agents be made publicly \navailable?\nMany research workflows can utilize a predetermined \nsequence of prompts or chains of LLMs. Other workflows, \nhowever, can’t rely on predetermined sequences and/or deci-\nsions to achieve their goals. In these later cases, LLM agents \ncan be used to make decisions about which LLMs and tools \n(including, for instance, internet searches [23]) to use in \nachieving a goal [ 24]. A LLM agent utilizes prompts, or \nLLM responses, as inputs to their (the agent’s) reasoning \nand decisions about which LLMs or tools to utilize next. \nFurther, LLM agents can learn from their past performance \n(i.e., successes or failures) leading to improved performance \n[25, 26]. If researchers apply LLM agents in their workflow, \ndetails on the agents and tools used in the research should be \ndescribed. Any intermediate steps, and the sequence of those \nsteps, should also be described since these are essential to \nhow the final outputs of the LLM were achieved.\n2.5  Ethics\nThe use of LLMs in scientific research workflows is a new \narea of AI ethics that requires emerging considerations for \nresearchers, including:\n• Is the organization (e.g., company, open source commu-\nnity) that created the LLM transparent about the choices \nthey made in its development and fine tuning?\n• How will training data for additional embedding model(s) \nbe acquired in a transparent and ethical manner?\n• What steps for data privacy and protections will be \ntaken?\n• What will be done to identify and mitigate potential \nbiases in LLM-generated results?\n• Are there any potential conflicts of interest related to the \nuse of LLMs?\n973AI and Ethics (2024) 4:969–974 \n1 3\n• Are there any applicable institutional and/or regulatory \nguidelines that will be followed?\n• What steps will be taken for the research to be reproduc-\nible and transparent?\n• Will LLM outputs be described in a non-anthropomor -\nphic manner?\nThe ethical use of LLMs in research workflows is a crucial \nconsideration that cuts across multiple disciplines. From \nsociology and psychology to engineering management and \nbusiness, LLMs have diverse applications in research, and \nthis necessitates attention to a range of issues. These issues \ninclude technical concerns such as data privacy and bias, as \nwell as philosophical considerations such as anthropomor -\nphism and the epistemological challenges posed by machine-\ngenerated knowledge. Therefore, it is essential to address \nethical considerations when using LLMs in research work -\nflows to ensure that the research remains unbiased, trans-\nparent, and scientifically rigorous. While researchers may \nhave little control, for example, over the ethical collection \nof data for the initial training of an LLM (such as OpenAI’s \nGPT-3.5), they do have choices in which LLMs to utilize in \nTable 1  Peer-reviewer’s checklist\nContext\n ◻ Was the study pre-registered?\n ◻ Were LLMs used to complement other research methods, or as the sole method?\n ◻ Were the research questions and data appropriate for LLM methods?\nEmbedding Models\n ◻ Were embedding(s) used in the research?\n ◻ Is the tool used to create the embedding model(s) provided and described?\n ◻ Were multiple embedding models created, tested, or used (i.e., chained)?\n ◻ Is the size of chunks used in preparing the data for embedding provided?\n ◻ Were different sizes of chunks tested for influence on the LLMs performance?\n ◻ Is the size of overlap permitted when creating chunks provided?\n ◻ Is the tool used for similarity matching (i.e., vector database) provided and described (e.g., FAISS)?\n ◻ Is the code for creating embedding(s) available?\nFine Tuning\n ◻ Which language model was used (e.g., OpenAI’s GPT−3.5 model)?\n ◻ Were multiple language models tested for performance?\n ◻ Are the completion parameters applied (e.g., temperature, presence penalty, frequency penalty, max tokens, logit bias, stops) provided?\n ◻ Were multiple combinations of completion parameters tested?\n ◻ Is any “prompt engineering” described in detail?\n ◻ Did the researcher(s) include the final prompts used?\n ◻ Were quality review checks performed on LLM-generated results?\n ◻ Did the researcher(s) validate the LLM-generated results through experimentation or simulation?\n ◻ Did the researcher(s) evaluate the LLM’s performance against other benchmarks or standards?\n ◻ Is the code for fine tuning available?\nAgents\n ◻ Were LLM agent(s) used in the research?\n ◻ Were the intermediate steps of the LLM agent(s) described?\n ◻ Is the code for creating the agents available?\nEthics\n ◻ Does the researcher(s) describe ethical considerations applied when selecting an appropriate base LLM for the research?\n ◻ Were training data for additional embedding model(s) acquired in a transparent and ethical manner?\n ◻ Were proper steps for data privacy and protections taken?\n ◻ Did the research methods address potential biases in LLM-generated results?\n ◻ Did the researcher(s) disclose any conflicts of interest related to the use of LLMs?\n ◻ Did the researcher(s) comply with applicable institutional and/or regulatory guidelines?\n ◻ Were proper citations and credit given?\n ◻ To the extent possible are the LLM methods done in a manner that is reproducible and transparent?\n ◻ Were LLM outputs described in a non-anthropomorphic manner?\n974 AI and Ethics (2024) 4:969–974\n1 3\ntheir research and the ethical collection of data used in creat-\ning any custom embedding models used in their workflows. \nLikewise, while there are currently limited institutional and/\nor regulatory policies guiding the use of LLMs in scien-\ntific research, researchers will be responsible for adhering \nto those AI policies (such as the EU AI Act [27]) when they \nare established. In the interim, researchers must be detailed \nand transparent about their practices, provide proper cita-\ntions and credit, and disclose any conflicts of interest.\n3  Conclusions\nAs LLMs continue to advance, their potential uses, benefits, \nand limitations in scientific research workflows are emerg-\ning. This presents an opportune moment to establish norms, \nconventions, and standards for their application in research \nand reporting their use in scientific publications. In this \neditorial, I have proposed an initial framework and set of \nnorms for researchers to consider, including a peer-reviewer \nchecklist (see Table  1) for assessing research reports and \narticles that employ LLMs in their methods. These propos-\nals are not meant to be definitive, as we are still in the early \nstages of learning about the potential uses and limitations \nof LLMs. Rather, it is hoped that this foundation will stimu-\nlate research questions and inform future decisions about the \nnorms, conventions, and standards that should be applied \nwhen using LLMs in scientific research workflows.\nReferences\n 1. OpenAI: GPT-4 Technical Report. https:// cdn. openai. com/  \npapers/ gpt-4. pdf (2023)\n 2. Romal Thoppilan, E.A.: LaMDA: language models for dialog \napplications. arXiv preprint arXiv: 2201. 08239 (2022)\n 3. Bowman, S.R.: Eight things to know about large language mod-\nels. arXiv preprint arXiv: 2304. 00612 (2023)\n 4. Crokidakis, N., de Menezes, M.A., Cajueiro, D.O.: Questions of \nscience: chatting with ChatGPT about complex systems arXic \npreprint arXiv: 2303. 16870 (2023)\n 5. Wang, Z., Xie, Q., Ding, Z., Feng, Y., Xia, R.: Is ChatGPT a \ngood sentiment analyzer? A preliminary study. arXiv preprint \narXiv: 2304. 04339 (2023)\n 6. Qi, Y., Zhao, X., Huang, X.: safety analysis in the era of large \nlanguage models: a case study of STPA using ChatGPT. arXiv \npreprint arXiv: 2304. 04339 (2023)\n 7. Khademi, A.: Can ChatGPT and bard generate aligned assess-\nment items? A reliability analysis against human performance. \narXiv preprint arXiv: 2304. 05372 (2023)\n 8. Southwood, N., Eriksson, L.: Norms and conventions. Philos. \nExplor. 14(2), 195–217 (2011). https:// doi. org/ 10. 1080/ 13869 \n795. 2011. 569748\n 9. Bowdery, G.J.: Conventions and norms. Philos. Sci. 8 (4), 493–\n505 (1941). https:// doi. org/ 10. 1086/ 286731\n 10. Mügge, D., Linsi, L.: The national accounting paradox: how \nstatistical norms corrode international economic data. Eur. J. \nInt. Relat. 27(2), 403–427 (2021). https://  doi. org/ 10. 1177/ 13540 \n66120 936339. (PMID: 34040493)\n 11. Johnson, V.E.: Revised standards for statistical evidence. Proc. \nNatl. Acad. Sci. U.S.A. 110(48), 19313–19317 (2013)\n 12. Chang, M.: IEEE standards used in your everyday life-IEEE SA—\nstandards.ieee.org. https:// stand ards. ieee. org/ beyond- stand ards/ \nieee- stand ards- used- in- your- every day- life. Accessed 16 Apr 2023\n 13. Maeda, J.: LLM Ai Embeddings. https:// learn. micro soft. com/  \nen- us/ seman tic- kernel/ conce pts- ai/ embed dings\n 14. Wu, T., Terry, M., Cai, C.J.: AI chains: transparent and control-\nlable human-AI interaction by chaining large language model \nprompts. arXiv preprint arXiv: 2110. 01691 (2022)\n 15. Chase, H.: Text embedding models (2023). https:// python. langc \nhain. com/ en/ latest/ modul es/ models/ text_ embed ding. html? highl \night= embed ding\n 16. Chunking Strategies for LLM Applications. https:// www. pinec  \none. io/ learn/ chunk ing- strat egies/\n 17. Chase, H.: Vectorstores (2023). https:// python. langc  hain. com/ \nen/ latest/ modul es/ index es/ vecto rstor es. html\n 18. Kan, D.: Not all vector databases are made equal. Towards Data \nScience (2022). https:// towar  dsdat ascie nce. com/ milvus- pinec \none- vespa- weavi ate- vald- gsi- what- unites- these- buzz- words- \nand- what- makes- each- 9c65a 3bd06 96\n 19. Hannibal046: Hannibal046/Awesome-LLM: Awesome-LLM: a \ncurated list of large language model. https:// github. com/ Hanni  \nbal046/ Aweso me- LLM\n 20. OpenAI: OpenAI API—platform.openai.com. https://  platf orm. \nopenai. com/ docs/ api- refer ence/ compl etions/ create. Accessed 16 \nApr 2023\n 21. Si, C.: Prompting gpt-3 to be reliable. In: ICLR 2023 Proceed-\nings. https:// openr eview. net/ pdf? id= 98p5x 51L5af (2023)\n 22. White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., \nElnashar, A., Spencer-Smith, J., Schmidt, D.C.: A prompt pat-\ntern catalog to enhance prompt engineering with ChatGPT. arXiv \npreprint arXiv: 2302. 11382 (2023)\n 23. Significant-Gravitas: GitHub-Significant-Gravitas/Auto-GPT: \nAn experimental open-source attempt to make GPT-4 fully \nautonomous.—github.com. https:// github. com/ Signi ficant- Gravi \ntas/ Auto- GPT. Accessed 16 Apr 2023 (2023)\n 24. Chase, H.: Agents. https:// python. langc  hain. com/ en/ latest/ \nmodul es/ agents. html (2023)\n 25. Shinn, N., Labash, B., Gopinath, A.: Reflexion: an autonomous \nagent with dynamic memory and self-reflection. arXiv preprint \narXiv: 2303. 11366 (2023)\n 26. Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., \nZettlemoyer, L., Cancedda, N., Scialom, T.: Toolformer: language \nmodels can teach themselves to use tools. arXiv preprint arXiv: \n2302. 04761 (2023)\n 27. Union, E.: Artificial Intelligence Act (2023). https:// artifi  cial intel \nligen ceact. eu/\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Engineering ethics",
  "concepts": [
    {
      "name": "Engineering ethics",
      "score": 0.5858033895492554
    },
    {
      "name": "Workflow",
      "score": 0.5366527438163757
    },
    {
      "name": "Political science",
      "score": 0.3449404239654541
    },
    {
      "name": "Psychology",
      "score": 0.3410428464412689
    },
    {
      "name": "Computer science",
      "score": 0.3060545027256012
    },
    {
      "name": "Engineering",
      "score": 0.13228711485862732
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I193531525",
      "name": "George Washington University",
      "country": "US"
    }
  ]
}