{
  "title": "Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer",
  "url": "https://openalex.org/W4385574244",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2331575848",
      "name": "Javier Ferrando",
      "affiliations": [
        "Universitat Politècnica de Catalunya"
      ]
    },
    {
      "id": "https://openalex.org/A3096720572",
      "name": "Gerard I. Gállego",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3182384549",
      "name": "Belen Alastruey",
      "affiliations": [
        "Universitat Politècnica de Catalunya"
      ]
    },
    {
      "id": "https://openalex.org/A2660413250",
      "name": "Carlos Escolano",
      "affiliations": [
        "Universitat Politècnica de Catalunya"
      ]
    },
    {
      "id": "https://openalex.org/A4227926241",
      "name": "Marta R. Costa‐jussà",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2398041834",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2909737760",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W3169369929",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3197138502",
    "https://openalex.org/W3200182126",
    "https://openalex.org/W4221154557",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W4230405732",
    "https://openalex.org/W3099143320",
    "https://openalex.org/W3173506780",
    "https://openalex.org/W3200704197",
    "https://openalex.org/W2977944219",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970045405",
    "https://openalex.org/W4283809028",
    "https://openalex.org/W3104881680",
    "https://openalex.org/W2952682849",
    "https://openalex.org/W2912070261",
    "https://openalex.org/W3093871477",
    "https://openalex.org/W2988249555",
    "https://openalex.org/W2971296520",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W2799051177",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1787224781"
  ],
  "abstract": "In Neural Machine Translation (NMT), each token prediction is conditioned on the source sentence and the target prefix (what has been previously translated at a decoding step). However, previous work on interpretability in NMT has mainly focused solely on source sentence tokens' attributions. Therefore, we lack a full understanding of the influences of every input token (source sentence and target prefix) in the model predictions. In this work, we propose an interpretability method that tracks input tokens' attributions for both contexts. Our method, which can be extended to any encoder-decoder Transformer-based model, allows us to better comprehend the inner workings of current NMT models. We apply the proposed method to both bilingual and multilingual Transformers and present insights into their behaviour.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8756–8769\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nTowards Opening the Black Box of Neural Machine Translation:\nSource and Target Interpretations of the Transformer\nJavier Ferrando1, Gerard I. Gállego1, Belen Alastruey1,\nCarlos Escolano1, Marta R. Costa-jussà2\n1TALP Research Center, Universitat Politècnica de Catalunya\n2Meta AI\n{javier.ferrando.monsonis,gerard.ion.gallego,\nbelen.alastruey,carlos.escolano}@upc.edu\ncostajussa@meta.com\nAbstract\nIn Neural Machine Translation (NMT), each\ntoken prediction is conditioned on the source\nsentence and the target prefix (what has been\npreviously translated at a decoding step). How-\never, previous work on interpretability in NMT\nhas mainly focused solely on source sentence\ntokens’ attributions. Therefore, we lack a full\nunderstanding of the influences of every input\ntoken (source sentence and target prefix) in the\nmodel predictions. In this work, we propose an\ninterpretability method that tracks input tokens’\nattributions for both contexts. Our method,\nwhich can be extended to any encoder-decoder\nTransformer-based model, allows us to bet-\nter comprehend the inner workings of current\nNMT models. We apply the proposed method\nto both bilingual and multilingual Transformers\nand present insights into their behaviour.\n1 Introduction\nTransformers (Vaswani et al., 2017) have become\nthe state-of-the-art architecture for natural language\nprocessing (NLP) tasks (Devlin et al., 2019; Raffel\net al., 2020; Brown et al., 2020). With its suc-\ncess, the NLP community has experienced an urge\nto understand the decision process of the model\npredictions (Jain and Wallace, 2019; Serrano and\nSmith, 2019).\nIn Neural Machine Translation (NMT), attempts\nto interpret Transformer-based predictions have\nmainly focused on analyzing the attention mecha-\nnism (Raganato and Tiedemann, 2018; V oita et al.,\n2018). A large number of works in this line have in-\nvestigated the capabilities of the cross-attention to\nperform source-target alignment (Kobayashi et al.,\n2020; Zenkel et al., 2019; Chen et al., 2020), com-\npared with human annotations. Gradient-based\n(Ding et al., 2019) and occlusion-based methods\n(Li et al., 2019) have also been evaluated against\nhuman word alignments. The former computes gra-\ndients with respect to the input token embeddings\nFigure 1: ALTI+ results for a De-En translation example.\nWe obtain source sentence and target prefix (columns)\ninterpretations for every predicted token (row).\nto measure how much a change in the input changes\nthe output, the latter generates input attributions by\nmeasuring the change in the predicted probability\nafter deleting specific tokens. However, there is\na tension between finding a faithful explanation\nand observing human-like alignments, since one\ndoes not imply the other (Ferrando and Costa-jussà,\n2021).\nThe decoding process of NMT systems consists\nof generating tokens in the target vocabulary based\non the information provided by the source sequence\nand the previously generated tokens (target prefix).\nHowever, most of the work on interpretability of\nNMT models only analyses source tokens. Re-\ncently, V oita et al. (2021a) proposed using Layer\nRelevance Propagation (LRP) (Bach et al., 2015)\nto analyze the source and target contributions to the\nmodel prediction, and later analyzed its behaviour\nduring training (V oita et al., 2021b). Nonethe-\nless, they apply their method to obtain global ex-\nplanations, as an average over the entire dataset,\nnot to get input attributions of a single prediction.\nGradient-based methods have also been extended to\nthe target prefix (Ferrando and Costa-jussà, 2021),\nalthough they do not quantify the relative contribu-\ntion of source and target inputs.\n8756\nConcurrently, encoder-based Transformers, such\nas BERT (Devlin et al., 2019) and RoBERTa (Liu\net al., 2019), have been analysed with attention\nrollout (Abnar and Zuidema, 2020), which models\nthe information flow in the model with a Directed\nAcyclic Graph, where nodes are token representa-\ntions and edges, attention weights. In the computer\nvision literature, Chefer et al. (2021b,a) combined\nthis method with gradient information. Recently,\nFerrando et al. (2022) have presented ALTI (Aggre-\ngation of Layer-wise Tokens Attributions), which\napplies the attention rollout method by substitut-\ning attention weights with refined token-to-token\ninteractions. In this work, we present the first ap-\nplication of a rollout-based method to sequence\nto sequence Transformers. Our key contributions\nare1:\n• We propose a method that measures the contri-\nbutions of each input token (source and target\nprefix) to the encoder-decoder Transformer\npredictions;\n• We show how contextual information is mixed\nacross the encoder of NMT models, with the\nmodel keeping up to 47% of token identity;\n• We evaluate the role of residual connections\nin the cross-attention, and show that attention\nto uninformative source tokens (EOS and final\npunctuation mark) is used to let information\nflow from the target prefix;\n• We analyze the role of both input contexts in\nlow and high-resource scenarios, and show\nthe model behaviour under hallucinations.\n2 Background\nIn this section, we provide the background to under-\nstand our proposed method by briefly explaining\nthe encoder-decoder Transformer-based model in\nthe context of NMT (Vaswani et al., 2017) and the\nAggregation of Layer-wise Token-to-token Interac-\ntions (ALTI) method (Ferrando et al., 2022).\n2.1 Encoder-Decoder Transformer\nGiven a source sequence of tokens x =\n(x1,...,x J), and a target sequence y =\n1Code available at https://github.com/mt-upc/\ntransformer-contributions-nmt.\nMHA\nLN\nMLP\nLN\nMHA\nLN\nMHA\nLN\nMLP\nLN\nEsta es la figura 2 . This is figure\n2\nFigure 2: Encoder-Decoder Transformer.\n(y1,...,y T), an NMT system models the condi-\ntional probability:\nP(y|x) =\nT∏\nt=1\nP(yt|y<t,x) (1)\nwhere y<t = (y0,...,y t−1) represents the prefix\nof yt, with xJ = y0 = </s> used as a special token\nto mark the beginning and end of sentence. The\nTransformer is composed by a stack of encoder\nand decoder layers (Figure 2). The encoder gener-\nates a contextualized sequence of representations\ne = (e1,..., eJ) of the source sentence. The de-\ncoder, at each time step t, uses both the encoder\noutputs (e) and the target prefix (y<t) to compute a\nprobability distribution over the target vocabulary,\nfrom which a prediction is sampled.\nMulti-head attention. The Transformer core\nbuilding block, the multi-head attention mechanism\n(MHA) is in charge of combining contextual infor-\nmation in the hidden representations. Consider\nhere x = (x1,..., xJ) as the sequence of token\nrepresentations2 of dimension d entering layer l,\nand ˜x = (˜x1,..., ˜xJ) the output layer representa-\ntions. Each of the H heads inside MHA computes\nvectors of dimension dh = d/H:\nzh\ni =\nJ∑\nj=1\nαh\ni,jWh\nVxj (2)\nwith αh\ni,j referring to the attention weight where\ntoken i attends token j, and Wh\nV ∈Rdh×d to a\nlearned weight matrix3.\n2We consider xi as a column vector.\n3The bias vector associated with Wh\nV is omitted for the\nsake of simplicity.\n8757\nThe output of MHA for the i-th token (MHAi) is\ncalculated by concatenating each zh\ni and projecting\nthe joint vector through WO ∈ Rd×d. This is\nequivalent to a sum over heads where each zh\ni is\nprojected through the partitioned weight matrix\nWh\nO ∈Rd×dh and adding the bias bO ∈Rd:\nMHAi(x) =WO Concat(z1\ni,..., zH\ni ) +bO\n=\nH∑\nh=1\nWh\nOzh\ni + bO\n(3)\nLayer normalization. Finally, a layer normaliza-\ntion (LN) is applied over the sum of the residual\nvector xi and the output of the multi-head attention\nmodule, giving as output ˜xi:\n˜xi = LN(MHAi(x) +xi) (4)\nMerging Equations (2) to (4), we get:\n˜xi = LN\n( J∑\nj=1\nH∑\nh=1\nWh\nOαh\ni,jWh\nVxj + bO + xi\n)\nConsidering Fi(xj) =∑H\nh=1 Wh\nOαh\ni,jWh\nVxj, we\ncan formulate the previous equation as:\n˜xi = LN\n( J∑\nj=1\nFi(xj) +bO + xi\n)\n(5)\n2.2 Aggregation of Layer-wise Token-to-token\nInteractions (ALTI)\nThe layer normalization operation over a sum of\nvectors LN(∑\nj uj), as in Equation (5), can be re-\nformulated as ∑\nj L(uj) +β, where L: Rd ↦→Rd\n(see Appendix A.1). This allows us to express\nEquation (5) (Kobayashi et al., 2021) as an inter-\npretable expression of the layer input representa-\ntions (Figure 3):\n˜xi =\nJ∑\nj=1\nTi(xj) +ϵ (6)\nwhere ϵcontains bias terms (see Appendix A.2\nfor full derivation) andTitransforms the layer input\nvectors:\nTi(xj) =\n{L(Fi(xj)) if j ̸= i\nL(Fi(xj) +xi) if j = i (7)\nwith the residual connection xi only considered\nin the transformed vector Ti(xj=i). Ferrando et al.\nLN \nFigure 3: The self-attention block (left) at each position\nican be decomposed as a summation of transformed\ninput vectors (right). The closest vector ( T2(x2)) con-\ntributes the most to ˜x2.\n(2022) propose to use the Manhattan distance be-\ntween the output vector and the transformed vector\nas a measure of the impact of xj on ˜xi:\ndi,j = ∥˜xi −Ti(xj)∥1 (8)\nBy taking −di,j, larger distances reflect lower\n(more negative) influence. Then, distances are nor-\nmalized ∈[0,1] to obtain the contribution of token\nrepresentation j to token representation i4:\nc˜xi←xj = max(0,−di,j + ∥˜xi∥1)∑J\nk=1 max(0,−di,k + ∥˜xi∥1)\n(9)\ngiving the matrix of layer-wise contributions\nC˜xi←x ∈RJ×J, where each row contains the con-\ntribution, or influence, of each xj in ˜xi.\nALTI method (Ferrando et al., 2022) follows\nthe Transformer’s modeling approach proposed by\nAbnar and Zuidema (2020), where the informa-\ntion flow in the model is simplified as a Directed\nAcyclic Graph, where nodes are token represen-\ntations, and edges represent the influence of each\ninput layer token xj in the output token ˜xi. ALTI\nproposes using token contributions C instead of\nraw attention weights α. The amount of informa-\ntion flowing from one node to another in different\nlayers is computed by summing over the different\npaths connecting both nodes, where each path is\nthe result of the multiplication of every edge in the\npath. This is computed by the matrix multiplica-\ntion of the layer-wise contributions, giving the full\nencoder contribution matrix:\nCenc\ne←x = CL\ne←x ·CL−1\n˜x←x · ··· ·C1\n˜x←x (10)\nWe refer to CL\ne←x as the contributions in the last\nlayer of the encoder, where output vectors are e.\n4We use the term ‘contribution’ to refer to influences be-\ntween token representations. ‘Relevance’ is used to allude to\nthe influence of input tokens to model predictions.\n8758\nLNs\nLNc\n \nFigure 4: Self-attention and cross-attention modules\nin a decoder layer together with its contribution matri-\nces.5In green, it’s shown the information coming from\nthe encoder (source), and in red, the information from\nthe decoder (target prefix). Highlighted is shown contri-\nbutions at a single time step t.\n3 ALTI for the Encoder-Decoder\nTransformer (ALTI+)\nThe attention rollout and ALTI methods work for\nencoder-based Transformers. However, in the\nencoder-decoder Transformer, the cross-attention\nhinders its integration. In this section, we present\nALTI+, which is the adaptation of ALTI method to\nthe encoder-decoder Transformer.\n3.1 Decoder Layer Decomposition\nWe decompose the self-attention and cross-\nattention of a decoder layer into interpretable ex-\npressions (Equation (6)), from which we can get\nthe degree of interaction between input and out-\nput token representations (Equation (9)). Consider\ny<t = (y0,..., yj,..., yt−1) the set of vector rep-\nresentations of the target prefix tokens as input of a\ndecoder layer, and ˜yt the layer output (Figure 4).\nDecoder self-attention. The layer normalization\nin the decoder self-attention (LNs) is applied over\nthe sum of the multi-head attention output and the\nresidual yt−1. The self-attention block 6 can be\nwritten as:\n˜ys\nt = LNs(MHAs\nt(y<t) +yt−1)\n= LNs\n\n\nt−1∑\nj=0\nFs\nt (yj) +bO + yt−1\n\n (11)\n5We omit the MLP and its LN of the decoder layer.\n6We refer as ‘block’ to the multi-head attention, residual,\nand layer normalization.\nwhere Fs\nt considers α,Wh\nV and Wh\nO of the de-\ncoder self-attention. Analogous to Equation (7) we\ncan obtain the transformed vectors of yj:\nTs\nt (yj) =\n{Ls(Fs\ni (yj)) if j ̸= t−1\nLs(Fs\ni (yj) +yt−1) if j = t−1\nFollowing Equations (8) and (9) we get the decoder\nself-attention contributions C˜ys←y<t ∈RT×T re-\nflecting the strength of the interaction between y<t\nand ˜ys\nt.\nDecoder cross-attention. The output of the\ncross-attention block at time step tcan be decom-\nposed as:\n˜yt = LNc(MHAc\nt( e ) + ˜ys\nt )\n= LNc\n\n\nJ∑\nj=1\nFc\nt( ej ) +bO + ˜ys\nt\n\n (12)\nwhere ˜ys\nt, the residual connection, is the output of\nthe self-attention block, and e the encoder outputs.\nWe can obtain the transformed vectors of the en-\ncoder outputs ej and the residual connection ˜ys\nt:\nTc\nt(ej) =Lc(Fc\nt(ej))\nTc\nt(ys\nt) =Lc(˜ys\nt) (13)\nFollowing Equation (8), we can compute the Man-\nhattan distance between the transformed vectors\nand ˜yt and get the contributions [C˜y←e; C˜y←˜ys\nt ],\nwith C˜y←e ∈RT×J and C˜y←˜ys\nt ∈RT×1.\nThe cross-attention residual ˜ys\nt contribution to\n˜yt reflects the total influence of the self-attention\ninputs y<t to the decoder layer output ˜yt. Thus,\nwe can get the full decoder layer contribution ma-\ntrix [C˜y←e; C˜y←y<t] (Figure 5) by substituting\nthe residual contributions (C˜y←˜ys\nt ) with the self-\nattention contributions ( C˜y←˜ys\nt ), and weighting\nevery row of C˜y←y<t by the corresponding value\nof the residual contribution of each time step.\nFigure 5: Full decoder layer contributions.\n8759\n... \n... \nFigure 6: Source input attributions Rmodel\n˜yt←x.\n... \n... Figure 7: Target prefix input attributions Rmodel\n˜yt←y<t\n.\n3.2 Aggregating Contributions Through the\nEncoder-Decoder Transformer\nIn order to get input token attributions, we apply\nthe same principle as attention rollout method. As\ndescribed in §2.2, ALTI builds a graph where nodes\nare token representations and edges represent the\ncontributions between tokens in each layer. The\namount of information flowing from one node to\nanother in different layers is computed by summing\nover the different paths connecting both nodes,\nwhere each path is the result of the multiplication\nof every edge in the path (Figures 6 and 7).\nAlgorithm 1: ALTI+ source relevance.\nInput: Cenc\ne←x – encoder contributions\nCl\n˜yt←e – contributions decoder\nlayers\nL– number of layers\nOutput: Rmodel\n˜yt←x – source input relevancies\nfor l ←[1,2...L] do\nC∗l\n˜yt←x = Cl\n˜yt←e ·Cenc\ne←x\nR1\n˜yt←x = C∗1\n˜yt←x\nfor l ←[2,3...L] do\nRl\n˜yt←x = Cl\n˜yt←y<t ·Rl−1\n˜yt←x + C∗l\n˜yt←x\nRmodel\n˜yt←x = RL\n˜yt←x\nreturn Rmodel\n˜yt←x\nALTI+ source tokens relevance. Algorithm 1\nshows the process to obtain source sentence to-\nkens relevance for the model prediction Rmodel\n˜yt←x\n(Figure 6). We first update the cross-attention con-\ntribution matrices (to C∗l\n˜yt←x) by multiplying each\nof them with the contributions of the entire encoder\nCenc\ne←x to account for all the paths in the encoder\nand cross-attentions. We then iteratively aggregate\nedges from paths of the target prefix contributions\nCl\n˜yt←y<t.\nALTI+ target prefix tokens relevance. Target\nprefix input attributions (Figure 7) are computed\nby multiplying C˜y←y<t in each layer:\nRmodel\n˜yt←y<t = CL\n˜y←y<t ·CL−1\n˜y←y<t\n· ··· ·C1\n˜y←y<t\n(14)\n4 Experimental Setup\nWe analyze input token attributions in both bilin-\ngual and multilingual Machine Translation mod-\nels. For the bilingual setting, we train a 6-layer\nTransformer model for the German-English (De-\nEn) translation task. We use Europarl v7 corpus7\nand follow Zenkel et al. (2019) and Ding et al.\n(2019) data setup 8. We use byte-pair encoding\n(BPE) (Sennrich et al., 2016) with 10k merge oper-\nations. For the multilingual model, we use M2M\nTransformer (Fan et al., 2021), a many-to-many\nmultilingual translation model that can translate\ndirectly between any pair of 100 languages. We\nuse FAIRSEQ (Ott et al., 2019) implementations,\nand the provided checkpoint for the M2M model\n(418M). We perform the quantitative analysis in\n1000 sentences of the test set of IWSLT’14 German-\nEnglish dataset. For the analysis in §5.5 we use\nFLORES -101 (Goyal et al., 2022) devtest split.\n5 Analysis\nIn this section, we perform a set of experiments\nto measure the quality of the obtained contribu-\n7http://www.statmt.org/europarl/v7\n8https://github.com/lilt/alignment-scripts/\ntree/master/preprocess\n8760\n(a) Attention weights\n (b) Contributions\n (c) Decoder layer contributions\nFigure 8: (a) Cross-attention weights. (b) Cross-attention contributions[C˜y←e; C˜y←˜ys\nt ] of the encoder outputse and\nresidual ˜ys\nt to the decoder layer output, as described in §3.1. (c) Total decoder layer contributions [C˜y←e; C˜y←y<t]\nwith the self-attention contributions included.\nFigure 9: Contribution of the source input token to the\nencoder output representation at the same position. We\nshow mean and SD for each layer of the bilingual and\nmultilingual models.\ntions, and unveil different aspects of bilingual and\nmultilingual NMT models.\n5.1 Information Mix in the Encoder\nInformation from input source tokens gets mixed\nthroughout the encoder. Intermediate layer rep-\nresentations acquire contextual information from\nother tokens in the sentence due to the self-attention\nmechanism. Brunner et al. (2020) analyze, for\nan encoder-based model, the contribution of input\nsource tokens to its intermediate layer represen-\ntations. They conclude that input source tokens\ncontribute little (around 10% on average) to its\ncorresponding last layer representation (encoder\noutput). However, by training a linear classifier\nand, with nearest neighbor lookup based on the co-\nsine distance, they are able to recover input token\nidentity 93% of the times. We apply ALTI method\n(Equation (10)) across the Transformer encoder\nand analyze the input relevance of source tokens\nto intermediate encoder representations (Figure 9).\nOur results in the bilingual and multilingual models\nMethod AER ( ↓)\nAttention weights 47.7±1.7\nVector-Norms 41.4±1.4\nVector-Norms + LN + Res42.5±0.8\nOur contributionsC˜y←e 38.8±1.3\nTable 1: AER of the cross-attention contributions in the\n5th layer of the bilingual model. We show mean and SD\nfor models trained on five different seeds.\nshow that, indeed, input tokens highly contribute\nto their associated layer representations. In the last\nlayer, 41% of the input contribution comes from\nthe input token at the same position. The multilin-\ngual model is able to retain above 47% despite its\n12 layers. The curves of both models in Figure 9\nclosely match the results obtained by V oita et al.\n(2019) relying on the mutual information between\nthe input tokens and tokens representations across\nlayers.\n5.2 Alignment in Cross-attention\nIn order to evaluate the quality of the proposed\ncross-attention contributions (§3.1), we measure\nAlignment Error Rate (AER) against human-\nannotated alignments. As found out by Garg et al.\n(2019), the penultimate layer of Transformers tends\nto focus on learning the source-target alignment of\nwords. Therefore, we analyze the cross-attention\ncontributions C˜y←e extracted from the 5th layer\nfrom the bilingual 6-layer model. We use gold\nalignments from Vilar et al. (2006), containing 508\nsentence pairs. For comparison, we compute the\nAER of the raw attention weights and previous\nmethods based on vector norms. Vector-Norms\n(Kobayashi et al., 2020) compute ∥F∥2 from Equa-\n8761\nFigure 10: Pearson’s r correlation between attention\nweight values given to EOS token (</s>) and the contri-\nbution of the residual in the cross attention.\ntion (5), and Vector-Norms + LN + Res(Kobayashi\net al., 2021) ∥T∥2 from Equation (6). As shown in\nTable 1, our method for estimating layer-wise con-\ntributions obtain the lowest AER, outperforming\nsimilar previous methods by at least 2.6 points on\naverage. As can be observed in Figure 8, attention\nweights fail at showing alignments, with the </s>\ntoken concentrating large attention weights. Our\nmethod is able to filter this noise, showing almost\nno contribution from </s>. In §5.3, we analyze\nthis phenomenon and try to find an explanation for\nit.\n5.3 The Role of the End-of-Sentence Token\nIt has been hypothesized that attention given to spe-\ncial tokens is used by the model as a ‘no-op’ (Clark\net al., 2019). Ferrando and Costa-jussà (2021) an-\nalyze attention weights of the cross-attention to\nsource finalizing tokens (final punctuation mark\nand </s>), and find the value vectors (see Ap-\npendix B) associated with these tokens to be almost\nzero norm. Additionally, they find that attention\nweights to source finalizing tokens tend to increase\nwhen predicting tokens that heavily rely on the\ntarget prefix, such as postpositions, particles, or\nclosing subwords. The proposed cross-attention\ndecomposition in §3.1 allows us to analyze both\nthe contributions of source tokens, and the residual\nconnection (Figure 8 (b)). We measure the Pearson\ncorrelation between attention weights to </s> to-\nken and the contribution of the residual connection\nin the cross-attention. We can see in Figure 10 that\nthere is a high correlation in almost every layer,\nespecially in the last layers. This demonstrates\nthat finalizing tokens are used to skip source at-\ntention, since the higher their attention score, the\nmore information is flowing from the decoder (in\nthe residual) coming from the target prefix.\nFigure 11: ALTI+ results for a hallucination after in-\nduced perturbation in the bilingual model.\nFigure 12: Source contribution in sentences without and\nwith induced perturbation in the bilingual model.\n5.4 Analyzing Hallucinations\nA common issue of NMT models is hallucination,\nwhich are translations that are disconnected from\nthe source text, despite being fluent in the target lan-\nguage (Müller et al., 2020). Hallucinations should\nbe reflected in our method as a drop in the contri-\nbution of the source sentence. Thus, in this section,\nwe induce hallucination and measure the source\nsentence contribution with ALTI+.\nTo induce hallucination, we perturb the target\nprefix sequence of the bilingual model by adding\nthe <unk> token. Then, we follow the algorithm\nproposed by Lee et al. (2018) to detect which per-\nturbed translations are hallucinations. They mea-\nsure BLEU score of the generated translation with\nand without perturbation. They fix a minimum\nthreshold BLEU score for the original translations\n(20 BLEU in our experiments), and a maximum\nscore for the perturbed translations (3 BLEU in our\nexperiments). The model is considered to halluci-\nnate when both translations satisfy the thresholds.\nAnalyzing ALTI+ contributions, we can confirm\nthat the bilingual model largely ignores source to-\n8762\nFigure 13: ALTI+ for a Es-En example in the multilingual model.\nFigure 14: Source sentence contribution in different\nlanguage directions from the FLORES -101 devtest split.\nkens during hallucinations (Figures 11 and 12).\n5.5 Multilingual Model Analysis\nWe analyze the behaviour of the multilingual model\nin different language pairs of FLORES -101 dataset.\nWe include in the analysis high-resource languages,\nEnglish (En), Spanish (Es), and French (Fr) and\nlow-resource languages, Zulu (Zu) and Xhosa (Xh).\nHigh-resource languages have been defined in\n(Goyal et al., 2022) as languages with available bi-\ntext data beyond 100M samples, and low-resource\nlanguages are those with less than 1M.\nFigure 13 shows an Es-En example in the mul-\ntilingual model. We observe an almost uniform\ncontribution of the language tags across different\noutputs. The only drop in its contribution seems to\nhappen when translating proper nouns (e.g., \"Mr.\nWilliams\") or anglicisms (e.g., \"hobby\"), which is\nobserved for other language pairs too (Appendix C),\nand repeated across the dataset. We hypothesize\nthat the model doesn’t need to rely on the lan-\nguage tag since these words appear across different\nlanguages. Dependencies between generated to-\nkens are also observed, the prediction \"for\" relies\non \"thanks\", \"Williams\" on \"Mr.\" and \"into\" on\n\"introducing\". The same example can be found in\nAppendix C for En-Zu and Zu-En pairs.\nFigure 14 shows results of the source sentence\ncontribution for En-Zu, En-Xh, En-Fr and En-\nEs pairs. We observe similar source contribution\npatterns between the high-resource pairs, and be-\ntween those pairs involving a low-resource lan-\nguage. However, in the low-resource scenario, the\nsource contribution is remarkably lower when trans-\nlating from English. We hypothesize that, when the\nlow-resource language is in the target prefix, the\nmodel tends to behave similarly to when it halluci-\nnates (Figure 12), ignoring the source. But, when\na high-resource language (En) is in the target pre-\nfix, it is less likely to lose track of the source and,\nthus, less prone to enter hallucination mode. Low-\nresource language sentences in the target side may\nbe seen by the model as target prefix perturbations\n(§5.4), although further research is required.\n6 Conclusions\nWe propose ALTI+, an interpretability method for\nthe encoder-decoder Transformer that provides to-\nken influences to the model predictions for the two\ninput contexts: source sentence and target prefix.\nBy applying ALTI+ to a bilingual and a multilin-\ngual NMT model we are able to discover insights\ninto the behaviour of these black-box models. Un-\nlike previous methods, we can now observe depen-\ndencies between tokens in the predicted sentence,\nand quantify the total contribution of each of the\ncontexts. This allows a deeper exploration of cur-\nrent NMT models. Our findings include: the role\nof the source EOS (</s>) token as a mean to avoid\n8763\nincorporating source information, the absence of\nsource contribution when producing hallucinations,\nand the lack of source contributions when trans-\nlating from English to a low-resource language.\nALTI+ overcomes the limitations of previous inter-\npretability methods in NMT, and we believe it can\nhelp researchers and practitioners to better under-\nstand any encoder-decoder Transformer model.\nLimitations\nALTI+ is able to measure the amount of contex-\ntual information in each layer representation of the\nTransformer. We use the influences of each input\ntoken to the last layer representation for evaluating\ninput attributions for the model prediction. How-\never, our method does not consider the softmax\nlayer on top of the Transformer. Therefore, ALTI+\ndoesn’t provide explanations for each of the output\nclasses (target vocabulary), as opposed to gradient-\nbased methods.\nEthical Considerations\nALTI+ provides explanations about input attribu-\ntions in the Encoder-Decoder Transformer. By it-\nself, we are not aware of any ethical implications of\nthe methodology, which does not take into account\nany subjective priors. We perform experiments in\nMachine Translation. While we do not study biases\nin this application, we know they exist (Costa-jussà\net al., 2022). In the future, we plan to further ex-\nplore and mitigate them by using the information\nof source input attributions that ALTI+ provides.\nAlso, understanding hallucinations by means of\nALTI+ can help to avoid catastrophic and unsafe\ntranslations.\n7 Acknowledgements\nWe would like to thank the anonymous review-\ners for their useful comments. Javier Fer-\nrando and Gerard I. Gállego are supported by\nthe Spanish Ministerio de Ciencia e Innovación\nthrough the project PID2019-107579RB-I00 / AEI\n/ 10.13039/501100011033.\nReferences\nSamira Abnar and Willem Zuidema. 2020. Quantify-\ning attention flow in transformers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4190–4197, On-\nline. Association for Computational Linguistics.\nSebastian Bach, Alexander Binder, Grégoire Montavon,\nFrederick Klauschen, Klaus-Robert Müller, and Wo-\njciech Samek. 2015. On pixel-wise explanations\nfor non-linear classifier decisions by layer-wise rele-\nvance propagation. PLOS ONE, 10(7):1–46.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Wat-\ntenhofer. 2020. On identifiability in transformers. In\nInternational Conference on Learning Representa-\ntions.\nHila Chefer, Shir Gur, and Lior Wolf. 2021a. Generic\nattention-model explainability for interpreting bi-\nmodal and encoder-decoder transformers. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), pages 397–406.\nHila Chefer, Shir Gur, and Lior Wolf. 2021b. Trans-\nformer interpretability beyond attention visualization.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n782–791.\nYun Chen, Yang Liu, Guanhua Chen, Xin Jiang, and\nQun Liu. 2020. Accurate word alignment induction\nfrom neural machine translation. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 566–576,\nOnline. Association for Computational Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\nMarta R. Costa-jussà, Carlos Escolano, Christine\nBasta, Javier Ferrando, Roser Batlle, and Ksenia\nKharitonova. 2022. Interpreting gender bias in neu-\nral machine translation: Multilingual architecture\nmatters. Proceedings of the AAAI Conference on\nArtificial Intelligence, 36(11):11855–11863.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\n8764\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nShuoyang Ding, Hainan Xu, and Philipp Koehn. 2019.\nSaliency-driven word alignment interpretation for\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n1: Research Papers) , pages 1–12, Florence, Italy.\nAssociation for Computational Linguistics.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, Naman Goyal, Tom Birch, Vitaliy\nLiptchinsky, Sergey Edunov, Michael Auli, and Ar-\nmand Joulin. 2021. Beyond english-centric multilin-\ngual machine translation. Journal of Machine Learn-\ning Research, 22(107):1–48.\nJavier Ferrando and Marta R. Costa-jussà. 2021. Atten-\ntion weights in transformer NMT fail aligning words\nbetween sequences but largely explain model predic-\ntions. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021 , pages 434–443,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJavier Ferrando, Gerard I. Gállego, and Marta R. Costa-\njussà. 2022. Measuring the mixing of contextual\ninformation in the transformer.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly learning to align\nand translate with transformer models. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4453–4462, Hong\nKong, China. Association for Computational Linguis-\ntics.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022. The Flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522–538.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is not only a weight:\nAnalyzing transformers with vector norms. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7057–7075, Online. Association for Computa-\ntional Linguistics.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2021. Incorporating Residual and Nor-\nmalization Layers into Analysis of Masked Language\nModels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4547–4568, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nKatherine Lee, Orhan Firat, Ashish Agarwal, Clara Fan-\nnjiang, and David Sussillo. 2018. Hallucinations in\nneural machine translation. NIPS 2018 Interpretabil-\nity and Robustness for Audio, Speech and Language\nWorkshop.\nXintong Li, Guanlin Li, Lemao Liu, Max Meng, and\nShuming Shi. 2019. On the word alignment from\nneural machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 1293–1303, Florence, Italy.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nMathias Müller, Annette Rios, and Rico Sennrich. 2020.\nDomain robustness in neural machine translation. In\nProceedings of the 14th Conference of the Associa-\ntion for Machine Translation in the Americas (Volume\n1: Research Track), pages 151–164, Virtual. Associa-\ntion for Machine Translation in the Americas.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48–53, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAlessandro Raganato and Jörg Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n287–297, Brussels, Belgium. Association for Com-\nputational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\n8765\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nSofia Serrano and Noah A. Smith. 2019. Is attention in-\nterpretable? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2931–2951, Florence, Italy. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nDavid Vilar, Maja Popovic, and Hermann Ney. 2006.\nAER: do we need to “improve” our alignments? In\nProceedings of the Third International Workshop on\nSpoken Language Translation: Papers, Kyoto, Japan.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\nElena V oita, Rico Sennrich, and Ivan Titov. 2021a. An-\nalyzing the source and target contributions to predic-\ntions in neural machine translation. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1126–1140, Online.\nAssociation for Computational Linguistics.\nElena V oita, Rico Sennrich, and Ivan Titov. 2021b. Lan-\nguage modeling, lexical translation, reordering: The\ntraining process of NMT through the lens of classi-\ncal SMT. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 8478–8491, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1264–1274, Melbourne, Australia. Association\nfor Computational Linguistics.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2019. Adding interpretable attention to neural trans-\nlation models improves word alignment. CoRR,\nabs/1901.11359.\n8766\nA ALTI\nA.1 Layer Normalization\nThe Layer normalization operation over inputxcan\nbe defined as: LN(x) =x−µ(x)\nσ(x) ⊙γ+ β, where µ\ncomputes the mean,σthe standard deviation, andγ\nand βrefer to an element-wise transformation and\nbias respectively. LN(x) can be decomposed into\n1\nσ(x) Lx+ β, where L is a linear transformation in-\ncluding the mean and element wise multiplication.\nGiven a sum of vectors ∑\nj xj as input to LN\nwe can rewrite the expression as:\nLN(\n∑\nj\nxj) = 1\nσ(∑\nj xj)L\n∑\nj\nxj + β\n=\n∑\nj\n1\nσ(∑\nj xj)Lxj + β\n=\n∑\nj\nL(xj) +β\nA.2 Full derivation\n˜xi = LN\n( J∑\nj=1\nH∑\nh=1\nWh\nOαh\ni,jWh\nVxj + bO + xi\n)\n= LN\n( J∑\nj=1\nFi(xj) +bO + xi\n)\n=\nJ∑\nj=1\nL(Fi(xj)) +L(bO) +L(xi) +β\nDefining ϵ= L(bO)+βwe get to the expression\nin Equation (6):\n˜xi =\nJ∑\nj=1\nTi(xj) +ϵ (15)\nB Values Norms\nFigure 15: Norm of the value vectors (from encoder\noutputs) in the cross-attention of the alignment layer.\nWe provide mean and SD for each head in the bilingual\nmodel. Similar patterns are observed across layers, and\nin the multilingual model.\nC Examples\nWe include examples for the En-Zu language pair in\nthe multilingual model in Figure 16 and 17, as well\nas for Es-En in Figure 18 and Fr-En in Figure 19.\n8767\nFigure 16: ALTI+ for a En-Zu example in the multilingual model.\nFigure 17: ALTI+ for a Zu-En example in the multilingual model.\n8768\nFigure 18: ALTI+ for a Es-En example in the multilingual model.\nFigure 19: ALTI+ for a Fr-En example in the multilingual model.\n8769",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.786919116973877
    },
    {
      "name": "Computer science",
      "score": 0.7838367223739624
    },
    {
      "name": "Security token",
      "score": 0.7491318583488464
    },
    {
      "name": "Machine translation",
      "score": 0.7094951868057251
    },
    {
      "name": "Sentence",
      "score": 0.6958414316177368
    },
    {
      "name": "Transformer",
      "score": 0.6872568130493164
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4931625723838806
    },
    {
      "name": "Prefix",
      "score": 0.49144041538238525
    },
    {
      "name": "Decoding methods",
      "score": 0.48437437415122986
    },
    {
      "name": "Natural language processing",
      "score": 0.4574446380138397
    },
    {
      "name": "Encoder",
      "score": 0.4219932556152344
    },
    {
      "name": "Speech recognition",
      "score": 0.4003846049308777
    },
    {
      "name": "Algorithm",
      "score": 0.14054161310195923
    },
    {
      "name": "Linguistics",
      "score": 0.09516656398773193
    },
    {
      "name": "Voltage",
      "score": 0.08481359481811523
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9617848",
      "name": "Universitat Politècnica de Catalunya",
      "country": "ES"
    }
  ],
  "cited_by": 18
}