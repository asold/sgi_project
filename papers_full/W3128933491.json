{
  "title": "Text Compression-aided Transformer Encoding",
  "url": "https://openalex.org/W3128933491",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2663852672",
      "name": "Li, Zuchao",
      "affiliations": [
        "Shanghai Municipal Education Commission",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2367810317",
      "name": "Zhang Zhuosheng",
      "affiliations": [
        "Shanghai Municipal Education Commission",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2084083973",
      "name": "Zhao, Hai",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Municipal Education Commission"
      ]
    },
    {
      "id": "https://openalex.org/A1686891224",
      "name": "Wang Rui",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Municipal Education Commission"
      ]
    },
    {
      "id": "https://openalex.org/A2361524982",
      "name": "Chen, Kehai",
      "affiliations": [
        "National Institute of Information and Communications Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1216073853",
      "name": "Utiyama, Masao",
      "affiliations": [
        "National Institute of Information and Communications Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2662569190",
      "name": "Sumita Eiichiro",
      "affiliations": [
        "National Institute of Information and Communications Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6680532216",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4299923722",
    "https://openalex.org/W2962931466",
    "https://openalex.org/W2952524847",
    "https://openalex.org/W2963652649",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W2963615251",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2133182690",
    "https://openalex.org/W1501931667",
    "https://openalex.org/W2963104691",
    "https://openalex.org/W2962805889",
    "https://openalex.org/W2890116189",
    "https://openalex.org/W6727690538",
    "https://openalex.org/W6746208923",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2962996600",
    "https://openalex.org/W2467173223",
    "https://openalex.org/W2307381258",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2949925034",
    "https://openalex.org/W6697028822",
    "https://openalex.org/W6632455782",
    "https://openalex.org/W2115167129",
    "https://openalex.org/W6762521896",
    "https://openalex.org/W6725207838",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2785910460",
    "https://openalex.org/W2025768430",
    "https://openalex.org/W6737778391",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964302946",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W6762122294",
    "https://openalex.org/W6771915120",
    "https://openalex.org/W3100439847",
    "https://openalex.org/W2162245945",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3001393026",
    "https://openalex.org/W6755691338",
    "https://openalex.org/W2963547127",
    "https://openalex.org/W6773706223",
    "https://openalex.org/W6759961675",
    "https://openalex.org/W6757517122",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2126129064",
    "https://openalex.org/W2519284111",
    "https://openalex.org/W6601244113",
    "https://openalex.org/W6638238318",
    "https://openalex.org/W2964335437",
    "https://openalex.org/W2564861257",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3002535714",
    "https://openalex.org/W2964089333",
    "https://openalex.org/W2799124508",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W1792850142",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2019413183",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2293778248",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2905927205",
    "https://openalex.org/W2953830716",
    "https://openalex.org/W2903728819",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2897076808",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W30193166",
    "https://openalex.org/W2999210089",
    "https://openalex.org/W2920665390",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W4285719527"
  ],
  "abstract": "Text encoding is one of the most important steps in Natural Language Processing (NLP). It has been done well by the self-attention mechanism in the current state-of-the-art Transformer encoder, which has brought about significant improvements in the performance of many NLP tasks. Though the Transformer encoder may effectively capture general information in its resulting representations, the backbone information, meaning the gist of the input text, is not specifically focused on. In this paper, we propose explicit and implicit text compression approaches to enhance the Transformer encoding and evaluate models using this approach on several typical downstream tasks that rely on the encoding heavily. Our explicit text compression approaches use dedicated models to compress text, while our implicit text compression approach simply adds an additional module to the main model to handle text compression. We propose three ways of integration, namely backbone source-side fusion, target-side fusion, and both-side fusion, to integrate the backbone information into Transformer-based models for various downstream tasks. Our evaluation on benchmark datasets shows that the proposed explicit and implicit text compression approaches improve results in comparison to strong baselines. We therefore conclude, when comparing the encodings to the baseline models, text compression helps the encoders to learn better language representations.",
  "full_text": "IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 1\nText Compression-aided Transformer Encoding\nZuchao Li, Zhuosheng Zhang, Hai Zhao‚àó, Rui Wang‚àó, Kehai Chen, Masao Utiyama, and Eiichiro Sumita\nAbstract‚ÄîText encoding is one of the most important steps in Natural Language Processing (NLP). It has been done well by the\nself-attention mechanism in the current state-of-the-art Transformer encoder, which has brought about signiÔ¨Åcant improvements in the\nperformance of many NLP tasks. Though the Transformer encoder may effectively capture general information in its resulting\nrepresentations, the backbone information, meaning the gist of the input text, is not speciÔ¨Åcally focused on. In this paper, we propose\nexplicit and implicit text compression approaches to enhance the Transformer encoding and evaluate models using this approach on\nseveral typical downstream tasks that rely on the encoding heavily. Our explicit text compression approaches use dedicated models to\ncompress text, while our implicit text compression approach simply adds an additional module to the main model to handle text\ncompression. We propose three ways of integration, namely backbone source-side fusion, target-side fusion, and both-side fusion, to\nintegrate the backbone information into Transformer-based models for various downstream tasks. Our evaluation on benchmark\ndatasets shows that the proposed explicit and implicit text compression approaches improve results in comparison to strong baselines.\nWe therefore conclude, when comparing the encodings to the baseline models, text compression helps the encoders to learn better\nlanguage representations.\nIndex Terms‚ÄîNatural Language Processing, Text Compression, Transformer Encoding, Neural Machine Translation, Machine\nReading Comprehension.\n!\n1 I NTRODUCTION\nT\nExt encoding plays an important role in NLP , especially\nin Natural Language Understanding (NLU). NLU,\nwhich requires computers to read and understand human\nnatural language texts, has been a long-standing goal\nof machine intelligence since the pioneering work of\ndistributional text encoding by a neural network starting\nin 2003 [1], [2], etc. Recently, statistical contextualized\ntext encoding that effectively integrates contextual features\nand language model training objectives have set a\nseries of state-of-the-art benchmarks in several NLP\ntasks [3], [4], [5]. So far, most of these existing works\nconsider text encoding only from a language symbol\ndistributional standpoint. Namely, these works use either\ntoken (word/subword/character) embedding or sentence-\nlevel encoders rooted in statistical co-occurrence. But some\nexplicit experience of human understanding of the language\nis less taken into consideration to enrich the resulting\nrepresentation.\n‚Ä¢ This paper was partially supported by National Key Research and\nDevelopment Program of China (No. 2017YFB0304100), Key Projects\nof National Natural Science Foundation of China (U1836222 and\n61733011), Huawei-SJTU long term AI project, Cutting-edge Machine\nReading Comprehension and Language Model (Corresponding author: Hai\nZhao and Rui Wang).\n‚Ä¢ Z. Li, Z. Zhang, H. Zhao, and R. Wang are with the Department\nof Computer Science and Engineering, Shanghai Jiao Tong University,\nand also with Key Laboratory of Shanghai Education Commission for\nIntelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong\nUniversity, and also with MoE Key Lab of ArtiÔ¨Åcial Intelligence, AI\nInstitute, Shanghai Jiao Tong University. E-mail: {charlee, zhangzs,\n}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn, wangrui.nlp@gmail.com.\nK. Chen, M. Utiyama, and E. Sumita are with National Institute of\nInformation and Communications Technology (NICT), Kyoto, Japan, 619-\n0289. E-mail: {khchen, mutiyama, eiichiro.sumita }@nict.go.jp. Tel:+81-\n0774-6986.\n‚Ä¢ Part of this work was Ô¨Ånished when Z. Li and Z. Zhang visited National\nInstitute of Information and Communications Technology (NICT) and R.\nWang was with NICT.\nIn human languages, a sentence is a minimal unit\nthat may delivers a complete piece of knowledge [6], [7].\nIntuitively, when a human reads a sentence, especially a\nlong sentence, he/she often selectively focuses on some\nimportant words with basic sentence meaning and re-\nreads the sentence to understand its meaning completely;\nthat is, some words (i.e., backbone words) are more\nimportant for understanding the basic meaning of this\nsentence than others (i.e., detail words). In the state-of-the-\nart Transformer-based encoders, self-attention mechanisms\neffectively capture the general information in the input\nsentence/passage [8], [9], [10]; however, it is difÔ¨Åcult to\ndistinguish which information in the input is really salient\nfor encoding. Taking examples from the inputs of WMT14\nEnglish-to-German translation task (sentence-level) and\nSQuAD 2.0 reading comprehension task (paragraph-level)\nin Table 1, we manually annotate the text‚Äôs basic meaning\nas a sequence of words shorter than that of the original\ninput and call this ‚Äúbackbone‚Äù information. Obviously, these\nwords in the backbone annotation contain more important\ninformation for human understanding than do the other\nwords in the text.\nWe argue that such backbone information is helpful\nfor text encoding since it is underutilized by Transformer\nencoders. In other words, there are often redundancies\nin self-attentional token-level representations generated\nby Transformer encoders because semantically dominant\nwords treated as being equal to less semantically signiÔ¨Åcant\ntokens. In this paper, this process of obtaining text\nbackbone information is called text compression. We\npropose utilizing an Explicit Text Compression (ETC)\napproach (text summarization) and a novel Implicit Text\nCompression (ITC) approach to enhance Transformer-based\nmodels‚Äô text encoding with backbone information. To\nthis end, we Ô¨Årst build three text compression settings,\nincluding supervised, unsupervised, and semi-supervised,\narXiv:2102.05951v1  [cs.CL]  11 Feb 2021\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 2\nTABLE 1\nExamples of text and backbone information.\nSentence Both the US authorities and the Mexican security forces are engaged in an ongoing battle against the drug cartels.\nSentence-level Backbone US authorities and Mexican forces battle against drug cartels\nPassage The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and\n11th centuries gave their name to Normandy, a region in France. They were descended from Norse (‚ÄùNorman‚Äù\ncomes from ‚ÄùNorseman‚Äù) raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo ,\nagreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native\nFrankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures\nof West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the Ô¨Årst half of\nthe 10th century, and it continued to evolve over the succeeding centuries.\nParagraph-level Backbone The Normans were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They\nwere descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo. The\ndistinct cultural and ethnic identity of the Normans emerged initially in the Ô¨Årst half of the 10th century.\nto accommodate the needs of various scenarios where the\ncompression sequences or features are learnt from the input.\nThen, three methods of backbone fusion are proposed (BEF,\nBDF, and BBF), to integrate the backbone features into the\nTransformer encoder to improve the text encoding.\nEmpirical results on the widely used WMT14 English-\nto-German and English-to-French translation tasks and\nSQuAD 2.0 and RACE reading comprehension tasks show\nthat the proposed approaches improve the performances of\nseveral NLP tasks over the strong and even state-of-the-art\nbaselines.\nIn addition, in order to further analyze the improvement\nsource of proposed text compression approaches, we trained\nthe baseline and the ETC-aided model on the Natural\nLanguage Inference (NLI) dataset and evaluated them using\na controlled evaluation set, HANS (Heuristic Analysis for\nNLI Systems) [11], which can effectively assess how much\nthe model depends on the heuristic rules of the dataset and\nhow much actual improvement it produces. The evaluation\nresults show that our proposed ETC-aided model relies less\non the heuristic rules. Using 10 linguistic tasks, further\nprobing evaluation on the sentence embeddings derived\nfrom the NLI model veriÔ¨Åes that the representations with\nthe aids of text compression are improved.\n2 B ACKGROUNDS\n2.1 Text Representations Learning\nRecently, deep contextual Language Models (LMs) have\nbeen shown effective for text representation learning (i.e.,\ntext encoding) and achieving state-of-the-art results in a\nseries of Ô¨Çagship NLU tasks. Some prominent examples are\nEmbedding from Language models (ELMo) [4], Generative\nPre-trained Transformer (OpenAI GPT) [5], Bidirectional\nEncoder Representations from Transformers (BERT) [3],\nGeneralized Autoregressive Pre-training (XLNet) [12], and\nA Lite BERT for Self-supervised Learning of Language\nRepresentations (ALBERT) [13]. Providing Ô¨Åne-grained\ncontextual text embedding, these pre-trained LMs can be\neasily applied to downstream models by serving as the\nencoder or being used for Ô¨Ånetuning.\nA number of studies have found that deep learning\nmodels might not really understand natural language\n[14] and be vulnerable to adversarial attacks [15]. Deep\nlearning models pay great attention to non-signiÔ¨Åcant\nwords and ignore important ones, which suggests\nthat the current NLU models suffer from insufÔ¨Åcient\ncontextual semantic representation and learning. Natural\nlanguage understanding tasks require a comprehensive\nunderstanding of natural languages and the ability to do\nfurther inference and reasoning. A common trend among\nNLU studies is that models are becoming more and more\nsophisticated with stacked attention mechanisms or are\ntraining from a larger amount of data, resulting in an\nexplosive growth of computational cost.\nDistributed embeddings have been widely used as a\nstandard part of NLP models due to their ability to\ncapture the local co-occurrence of words from large-scale\nunlabeled text [2]; however, these approaches for learning\nword vectors only involve a single, context-independent\nembedding for each word with little consideration of\ncontextual encoding at the sentence level. Thus, recently\nintroduced contextual language models including ELMo,\nGPT, BERT, XLNet, and ALBERT Ô¨Åll this contextual gap\nby strengthening the contextual sentence modeling and\nconsequently produce better representations. Among these\nmodels, BERT pioneered a powerful pre-training objective,\nmasked language modeling, which allows capturing both sides\nof context - the text preceding and following a word.\nBesides, BERT also introduces a next sentence prediction\nobjective that jointly pre-trains text-pair representations.\n2.2 Typical NLU Tasks\nThe human ability to understand language is general,\nÔ¨Çexible, and robust. Typical NLU tasks require models\nthat can learn to represent linguistic knowledge in a\nway that facilitates sample-efÔ¨Åcient learning and effective\nknowledge-transfer across tasks. Presently, the neural\nnetwork-based NLP framework consistently achieves new\nlevels of quality and has become the dominating approach\nfor NLP tasks such as machine translation, machine reading\ncomprehension, sentiment analysis, and textual entailment.\nIn this work, we choose three tasks as research objectives:\nNeural Machine Translation (NMT), Machine Reading\nComprehension (MRC), and Natural Language Inference\n(NLI). It is worth noting that in the era of traditional\nrule-based and phrase-based statistical machine translation,\ntranslation relied on rules and statistical probability and\ndid not possibly constitute true language understanding;\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 3\ntherefore, we do not consider statistical machine translation\nto fall under the category of NLU. On the contrary, NMT,\nwhich has shown much recent success, does not rely on\nsuch rules and thus requires a much more comprehensive\nunderstanding of the language used. Therefore, we argue\nthat in addition to being a task of natural language\ngeneration, NMT can also be regarded as a task of NLU,\nas it reÔ¨Çects the quality of understanding in two or\nmore languages with the intuition that the better the\nunderstanding, the higher the quality of translation.\nNMT is popularly implemented as an encoder-decoder\nframework [16] in which the encoder handles source\nsentence representation. Typically, the input sentence is\nencoded as a contextualized source representation using\ndeep learning networks. By feeding this to the decoder, the\nsource representation is used to learn time-step dependent\ncontext vectors for predicting target translations [17].\nMRC is a Ô¨Åeld of question-answering in which\ncomputers understand passages and answer related\nquestions. Most of the previous MRC models are composed\nof two modules: an encoder and a pointing module.\nA passage (paragraph) is encoded as a contextualized\nrepresentation, and the starting and ending positions of\nanswer phrases are determined in the pointing module.\nThe data for Multi-Genre Natural Language Inference\n(MNLI) [18] consists of a crowd-sourced collection of\nsentence pairs with textual entailment annotations. Given\na premise sentence and a hypothesis sentence, the task\nis to predict whether the premise entails the hypothesis\n(entailment), contradicts the hypothesis (contradiction), or\nneither (neutral).\n2.3 Transformer Encoder\nRecurrent Neural Networks (RNNs) (or Long Short-Term\nMemory, LSTMs) have been established as advanced\napproaches to language modeling. Before 2017, they were\nthe most optimal architecture for a sequence encoder.\nBecause of their sequential nature, recurrent encoders pose\na problem for learning long-term memory dependencies\ndue to gradient explosion and a lack of parallelization\nduring training. Vaswani et al. [16] introduced a novel\narchitecture called Transformer, which provides a more\nstructured memory for handling long-term dependencies in\ntext, resulting in robust performance across diverse tasks.\nA Transformer encoder fully relies on self-attention\nnetworks (SANs) to encode an input sequence into a latent\nrepresentation space. Formally, a input language sequence\nx={x1,¬∑¬∑¬∑ ,xJ}of length J is Ô¨Årst mapped into embedding\nvectors. Then, the vectors and its position embeddings add\nup to form the input representation vx = {vx\n1 ,¬∑¬∑¬∑ ,vx\nJ}. The\ninput representation vx is then packed into a query matrix\nQx, a key matrix Kx, and a value matrix Vx. For the SAN-\nbased encoder, the self-attention sub-layer is Ô¨Årst performed\nover Q, K, and V to the matrix of outputs as:\nSelfAtt(Q,K,V) =Softmax( QKT\n‚àödmodel\n)V, (1)\nwhere dmodel represents the dimensions of the model.\nGenerally, the self-attention function is further reÔ¨Åned as\nmulti-head self-attention to jointly consider information\nEncoder\nITC Module\nETC Pipeline\n(Compressed Text)    (Vector Representation) (Vector Representation)\n/ Joint ITC Joint\nETC Model\nEncoder\nTask-specific Integration Task-specific Integration\n1 2 3\n1 2 1 3\nETC\nITC\nsupervised\nunsupervised\nsemi-supervised\nBEF\nBDF\nBBF\nMethodology\nTC Training\nIntegration\nFig. 1. An overview of our text compression settings (left) and overall\narchitectures of ETC and ITC (right). In the right sub-Ô¨Ågures, the solid\narrows represent differentiable operations and the dashed represent\nnon-differentiable operations. ETC Pipeline, ETC Joint, and ITC Joint\nare three manners explored for text compression-aided Transformer\nencoding in this paper. In ETC Pipeline and ETC Joint manners, text\nis compressed using an independent model, denoted as ETC Model. In\nthe ITC Jointmanner, text compression is handled in the full model in a\nmodule we denote ITC Module.\nfrom different representation subspaces at different\npositions:\nMultiHead(Q,K,V) =Concat(head1,¬∑¬∑¬∑ ,headH)WO,\nheadh = SelfAtt(QWQ\nh,kWK\nh ,VWV\nh),\n(2)\nwhere the projections are parameter matrices\nWQ\nh‚ààRdmodel√ódk , WK\nh ‚ààRdmodel√ódk , WV\nh‚ààRdmodel√ódv , and\nWO‚ààRdv√ódmodel . For example, a common setup is H=8\nheads, dmodel is 512, and dk=dv=512/8=64. A position-wise\nfeed-forward network (FFN) layer is applied over the\noutput of multi-head self-attention, and is then added with\nthe matrix V to generate the Ô¨Ånal source representation\nHx={Hx\n1 ,¬∑¬∑¬∑ ,Hx\nJ}:\nHx = FFN(MultiHead(Q,K,V)) +V. (3)\n3 O VERVIEW\nOur approach aims to integrate the products of compression\n(text sequence or feature vectors) into the encoding of\nvanilla Transformer to improve the resulting representa-\ntions. We classify two types of text compression, Explicit\nText Compression (ETC) and Implicit Text Compression\n(ITC), which signify whether the compressed sequence is\nexplicitly generated before being passed to the downstream\nmodel or implicitly generated as feature vectors by a\npart of the downstream model, respectively. In Figure 1,\nwe show an overview of our approach and the overall\narchitectures of ETC and ITC. In our framework, there are\ntwo models/modules: compression model (ETC/ITC) and\ndownstream task models. We will Ô¨Årst introduce two types\nof compression models (modules) and then show how to\nincorporate these compression models (modules) into task\nmodels.\nFor ETC, since compression is handled by an external\nmodel, we have two options: use a pipeline manner,\nwhereby a text compression model is trained before training\nthe downstream model ( ETC Pipeline ) or train the text\ncompression model and the downstream task model jointly\n(ETC Joint ). For ITC, since the compressed features is\ndirectly generated by a module inside the downstream\ntask‚Äôs model, jointly training the downstream task model\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 4\nwith the objective of text compression is the only solution.\nWe thus call this method ITC Joint. Thus, we have three text\ncompression-aided model training manners: ETC Pipeline ,\nETC Joint, and ITC Joint.\nDepending on whether or not the text compression\ntraining requires human annotated data , the text\ncompression can be trained in three settings: supervised,\nunsupervised, and semi-supervised. Generally, downstream\nmodels can be roughly divided into two types: encoder-\ndecoder and encoder-classiÔ¨Åer. For the downstream models,\nwe propose different methods of integrating the compressed\ntext and fusing its representation with that of the\noriginal Transformer: Backbone Encoder-side Fusion ( BEF),\nBackbone Decoder-side Fusion ( BDF), and Backbone Both-\nside Fusion (BBF).\n4 T EXT COMPRESSION\n4.1 Explicit Text Compression\nGenerally, text summarization is a typical sequence\ngeneration task that aims to maximize the absorption\nand long-term retention of large amounts of data over a\nrelatively short sequence for text understanding [19], [20] 1.\nTo distinguish the importance of words in a sentence or\nparagraph and, more importantly, to dig out the most salient\nparts of the source representation and emphasize these\nparts, we use text summarization, and term this task ETC\nin this paper.\nETC can be conducted by a typical sequence-to-sequence\nmodel. The encoder represents the input sentence x as a\nsequence of vectors, and the autoregressive decoder uses\nthe attention mechanism to learn the context vector for\ngenerating a text compressed sequence xc, which contains\nthe key meaning of the input sequence. Recently, the new\nTransformer architecture proposed by Vaswani et al. [16],\nwhich fully relies on self-attention networks, has exhibited\nstate-of-the-art translation performance for several language\npairs. We attempt to apply the Transformer architecture to\nsuch a compression task.\nWhen applying ETC to downstream tasks, the ETC\nmodel and downstream task model are optimized\nseparately, forming a pipeline manner, and we refer to this\nmanner as ETC Pipeline. For the downstream task, the ETC\nPipeline is simple and less invasive to the downstream\nmodel; however, the outputs of the low-level ETC model\nproduce errors that can propagate to the downstream task‚Äôs\nmodel and hamper performance.\nMulti-Task Joint Learning (MTL) is popular learning\nparadigm and promising technique in machine learning\nthat aims to leverage useful information contained in\nmultiple related tasks to help improve the generalization\nperformance of all the tasks. Since both the downstream\ntask and text compression can beneÔ¨Åt from being performed\njointly, it is very intuitive to train the ETC model with\nthe downstream model jointly, namely ETC Joint . In ETC\nJoint, the Transformer output vectors in the decoder are\ndirectly input to the downstream model as a compressed\n1. Text summarization typically consists of two types: extractive and\nabstractive. The extractive type has been well studied. We focus on\nabstractive summarization in this paper.\ntext feature instead of re-encoding the decoded compressed\ntext sequence.\nETC Compression Rate Control Explicit compression rate\n(length) control is a common method that has been used\nin previous text compression works. Kikuchi et al. [21]\nexamined several methods of introducing target output\nlength information and found that they were effective\nwithout negatively impacting summarization quality. Fan\net al. [22] introduced a length marker token that induces the\nmodel to target an output of a desired length that is coarsely\ndivided into discrete bins. Fevry and Phang [23] augmented\nthe decoder with an additional length countdown input,\nwhich is a single scalar that ticks down to 0 when the\ngeneration reaches the desired length.\nIn ETC Pipeline , different from the length marker or\nlength countdown input, to induce our ETC model to\noutput a compressed sequence with a desired length, we\nuse beam search during generation to Ô¨Ånd the sequence xc\nthat maximizes a score function s(xc,x) given a trained\nETC model. The length normalization is introduced to\naccount for the fact that we have to compare hypotheses\nof different lengths. Without some form of length-\nnormalization LenNorm, beam search will favor shorter\nsequences over longer ones on average since a negative\nlog-probability is added at each step, yielding lower (more\nnegative) scores for longer sentences. Moreover, a coverage\npenalty cp is also added to favor the sequence that covers\nthe source sentence meaning as much as possible according\nto the attention weights [24].\ns(xc,x) =log(P(xc|x))/LenNorm(xc) +cp(x; xc), (4)\nLenNorm(xc) =(5 +|xc|)Œ±/(5 + 1)Œ±, (5)\ncp(x; xc) =Œ≤√ó\n|x|‚àë\ni=1\nlog(min(\n|xc|‚àë\nj=1\npi,j,1.0)), (6)\nwhere pi,j is the attention probability of the j-th target\nword on the i-th source word. Parameters Œ± and Œ≤ control\nthe strength of the length normalization and the coverage\npenalty. Although Œ±can be used to control the compression\nratio softly, we use the compression ratio Œ≥ to control\nthe maximum length of decoding generation by hard\nrequirements. When the decoding length |xc|is greater than\nŒ≥|x|, decoding stops.\nIn ETC Joint , for the feasibility of mini-batch training,\ngreedy decoding is performed based on the maximum\ncompression length in a batch, and then the compressed\nrepresentations are masked according to their respective\ndesired lengths to ensure that the compression rate is\ncorrectly controlled.\n4.2 Implicit Text Compression\nImplicit Text Compression (ITC) is a method that allows text\ncompression to be trained jointly with the downstream task.\nThe errors generated by ETC Pipeline ‚Äôs text compression\nmodel are not differentiable across the two models used,\nas these models pass hard text sequences rather than\nsoft representations, and though ETC Joint ‚Äôs gradient is\ndifferentiable for the self-attention layer of the ETC decoder,\nthe encoder and encoder-decoder attention of the method‚Äôs\ntext compression model will not be updated during the\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 5\nEncoderSelf-Attn\nùëÜ!ùëÜ\"ùëÜ#ùëÜ$ùëÜ%\nùë†!ùë†\"ùë†#ùë†$ùë†%\nDecoderInter-Attn+Self-Attn\nrankingcopy\nùëá!ùëá\"ùëá#\n(a)Autoregressivedecoding (b)Non-autoregressivedecoding\nEncoderSelf-Attn\nùëÜ!ùëÜ\"ùëÜ#ùëÜ$ùëÜ%\nùëá! ùêµùëÇùëÜ\nÃÉùë°\"ùëá\" argmax\nÃÉùë°#ùëá# argmax\nÃÉùë°$ùëá$ argmax\nDecoderSelf-Attn\nùë†!ùë†\"ùë†#ùë†$ùë†%\nDecoderCross-Attn\nFig. 2. Comparison between autoregressive decoding in ETC Jointand\nnon-autoregressive decoding in ITC Joint. The solid line represents\ndifferentiable operations, and the dashed line represents non-\ndifferentiable ones. The gray background represents the compressed\ntext features. s is used for tokens in the source side, ÀÜt for predicted\ntokens in the target side, and BOS for tokens to start the prediction.\nautoregressive decoding process, as shown in Figure 2(a).\nIn addition to this lack of differentiability, the left-to-right\nsequential decoding method of autoregressive decoding\nmeans hidden states must be generated one by one\nand cannot be parallelized, meaning time cost is another\nconsideration. To address this issue, ITC uses a non-\nautoregressive decoder [25] that allows full integration of\ntext compression into the downstream model and allows\nfor a fully differentiable gradient, as seen in Figure 2(b). We\nname this approach ITC Joint. With this novel approach, we\nno longer rely on generating a compressed text sequence\n(which is needed for ETC Joint , as the generation of a\nsubsequent token depends on the previous one).\nCompared with autoregressive decoding, non-\nautoregressive decoding not only has the advantage of\nremoving non-differentiable operations and thus allowing\nthe whole model to be optimized at the same time, but it\nhas excellent decoding time complexity ( O(1) compared to\nO(n) in autoregressive decoding) that greatly speeds up the\njoint training and inference of the full model.\nWe present the architecture of the ITC module in Figure\n3. The ITC module consists of three main components: a\nfertility predictor, a Non-Autoregressive Transformer (NAT)\ndecoder, and an optional text compression predictor.\nFertility Predictor Unlike the non-autoregressive machine\ntranslation model in [25], the fertility predictor in the ITC\nmodule we proposed is not used to solve multimodality\nproblems in different languages. In machine translation,\na word in the source language may correspond to\nzero or more words in the target language due to the\ndifferences between languages. Because of the conditional\nindependence assumption of NAT machine translation,\npredicting this correspondence relationship before inputting\nit to the decoder is necessary. Thus, the fertility predictor is\ndesigned to predict the number of copies of words, known\nas the fertility, in the source language. The NAT decoder\nthen uses this prediction to copy each encoder input as a\ndecoder input zero or more times.\nIn text compression, because the text needs to be\ncompressed and thus shorter, the input word usually does\nnot correspond to a word in the target sentence or only\ncorresponds to one single word. Our fertility predictor is\nused to predict the probability that a word appears in the\ntarget sequence rather than predicting the number of copies\nas is done in typical fertility predictors. Thus, we adopt a\none-layer neural network with a sigmoid function instead\nof a softmax classiÔ¨Åer. It is formulated as follows:\npf = œÉ(MLP(Hx)). (7)\nSince text compression is simpler than machine\ntranslation, the order of the compressed text will not change\ntoo much. In the text compression process, the model will\nextract the backbone of the sentence and paraphrase it to\nproduce a shorter sentence.\nNAT Decoder The input for the NAT decoder is a copy of\nthe encoder input that is compressed to its top- K words\naccording to the fertility sort, where\nK = Œ≥|x|. (8)\nIn the NAT decoder, without the constraint of an\nautoregressive factorization of the output sequence, earlier\ndecoding steps do not risk accessing information in later\nsteps. Thus, we can avoid the causal mask used in the self-\nattention layer of the conventional Transformer‚Äôs decoder\nand use the same multi-head self-attention layer as in the\nencoder. The compressed representation Hc\nx is\nHitc = FFN(MultiHead(Q,K,V)) +V,\nHc\nx = FFN(MultiHead(Hitc,Hx,Hx)) +Hitc. (9)\nThis is unlike the original NAT Decoder implementation\nin [25], which masks each query position, preventing it\nfrom attending to itself. In addition, because the word order\nof compressed text does not change much, the additional\npositional attention mechanism is not included.\nOptional Text Compression Predictor For ITC, compressed\nrepresentations can be obtained using only the fertility\npredictor and the NAT decoder, which are optimized\nalong with the downstream task models. To expose the\nITC module to true text compression signals for pre-\ntraining, we added an optional compression predictor to\noutput compressed text sequence distributions and enabled\nstraightforward maximum likelihood training with a text\ncompression target. More precisely, given a source sentence\nx, the conditional probability of a target compression\nsequence yc is:\np(yc|x; Œ∏) =\n‚àë\nfi‚ààF\n(pf(fi)\nK‚àè\ni=1\npc(yc\ni|x{fi}\n1 ,...,x {fi}\nK )); Œ∏)),\n(10)\nwhere F is the set of all fertility sequences, and x{fi}\ndenotes tokens in the top-K Ô¨Åltered sequence fi.\n5 T EXT COMPRESSION TRAINING\nTypically, downstream task can be done on several levels,\nincluding on an individual sentence level or paragraph\nlevel or on the entire document as a whole. To adapt\ntext compression to various downstream tasks, we train\ntext compression at the sentence and paragraph levels.\nThese two types of models are only distinguished by the\nmaximum input length limit and the training datasets.\nSupervised Training Text compression usually relies on\nlarge-scale raw data together with human-labeled data,\nwhich acts as supervision, to train a compression model [26],\n[27], [28], [29], [30], [31].\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 6\nSupervised Text Compression Predictor  Compressed Representation\nITC Module\nFig. 3. The architecture of the ITC module.\nFor supervised training, we used the Annotated\nGigaword corpus [32] as the sentence-level training dataset\nand CNN/DailyMail (CNN/DM) [33] as the paragraph-\nlevel training dataset. The Gigaword dataset is derived\nfrom news articles and consists of pairs of the main\nsentences in the article (longer) and the headline (shorter).\nIt includes approximately 3.8M training samples, 400K\nvalidation samples, and 2K test samples. The CNN/DM\ndataset is collected from the CNN and the Daily Mail news\nwebsites. Both news providers supplement their articles\nwith a number of bullet points, summarizing aspects of the\ninformation contained in the article. These summary points\nare abstractive and do not simply copy sentences from the\ndocuments.\nUnsupervised Training A major challenge in supervised\ntext compression training is the scarcity of high-quality\nhuman annotated parallel data. In practice, supervised\ntraining often cannot be done due to a lack of annotated\ndata. In particular, models also often suffer from poor\ndomain adaptation as the domain variety of the available\ndata is also insufÔ¨Åcient. Therefore, the effectiveness of\nmodels relies heavily on the availability of large amounts\nof parallel original sentences and human-annotated\ncompressed sentences. This hinders text compression from\nfurther improvements for many low-resource scenarios.\nStudies of human summarizers show that it is common\nto apply various other operations while compressing, such\nas paraphrasing, generalization, and reordering [34]. Fevry\nand Phang [23] added noise to extend the original sentences\nto train the text compression model. Adding noise creates\nparallel pseudo data, which is used for unsupervised\ntraining. In this setup, the model has to exclude and\nreorder the noisy sentence input and hence learn to output\nmore semantically important and shorter but grammatically\ncorrect sentences. Inspired by this, we use three types of\nnoise for data synthesis: Additive Sampling Noise, ShufÔ¨Çe\nNoise, and Word Dropout Noise.\n‚Ä¢ Additive Sampling Noise: To extend the original\ninput, we sample additional instances from the\ntraining dataset randomly and then sub-sample\na subset of words from each of these instances\nwithout replacement. The newly sampled words are\nappended to the end of the original sequence.\n‚Ä¢ ShufÔ¨Çe Noise: In the sentence-level model, to\nencourage the model to learn to rephrase the input\nsequence to make the output shorter, we shufÔ¨Çe the\nresultant additive noisy sequence. In the paragraph-\nlevel model, a paragraph is divided into sentences\nbased on full stops, and these sentences, together\nwith the sampling sequence, are then shufÔ¨Çed in\nrandom order.\n‚Ä¢ Word Dropout Noise: In addition to the above two\nnoises introduced by Fevry and Phang [23], we\nintroduce word dropout noise to enable the model\nto predict tokens unseen in the input, which leads to\nabstract outputs.\nSemi-supervised Training As noted in Song et al.\n[35], the sequence-to-sequence framework has attracted\nmuch attention recently due to the advances of deep\nlearning using large-scale data. Many language generation\ntasks have only a small amount of paired data, which\nis insufÔ¨Åcient for training a deep model with good\ngeneralization ability. In comparison, there is a lot of\nunpaired data, which is easier to obtain.\nWe observe difference performance in different domains\nin the supervised training. According to the experimental\nresults of Fevry and Phang [23], the accuracy of\nunsupervised training is currently lower than that of\nsupervised training. Therefore, we adopt semi-supervised\ntraining to alleviate this problem. SpeciÔ¨Åcally, unsupervised\ntraining (often referred to as pre-training) is performed\nusing the unpaired data Ô¨Årst before Ô¨Ånetuning with the\nsmall amount of paired data (supervised training) to obtain\na model with good performance and generalization ability.\n6 T EXT COMPRESSION INTEGRATION\nIn this section, we introduce the general downstream\nmodels and the methods of integrating compressed text in\nin our approach. To meet the diverse needs of downstream\nmodels, based on the fusion position of the backbone\ninformation, we propose three novel methods of integrating\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 7\ncompressed text: Backbone Encoder-side Fusion (BEF),\nBackbone Decoder-side Fusion (BDF), and Backbone Both-\nside Fusion (BBF).\n6.1 Downstream Model Paradigm\nFirst, we introduce two commonly used downstream model\nparadigms: encoder-decoder and encoder-classiÔ¨Åer.\nEncoder-Decoder A Transformer NMT model is a typical\nencoder-decoder downstream model which consists of an\nencoder and a decoder. It fully relies on self-attention\nnetworks ( SANs) to translate a sentence in one language\nto another language while maintaining meaning.\nThe encoder is described in section 2.3. Here, we focus\non the decoder. The SAN of the decoder uses both encoder\noutput Hx and target context hidden state Htgt to learn the\ncontext vector oi by ‚Äúencoder-decoder cross-attention‚Äù:\nHtgt = FFN(MultiHeadmasked(Q,K,V)) +V,\nci = FFN(MultiHead(Htgt,Hx,Hx)), (11)\noi = ci + Htgt. (12)\nThen, the context vectoroiis used to compute translation\nprobabilities of the next target word yi by a linear,\npotentially multi-layered function:\nP(yi|y<i,x) ‚àùSoftmax(LoGeLU(Lwoi)), (13)\nwhere Lo and Lw are projection matrices.\nEncoder-ClassiÔ¨Åer The encoder-decoder is a very general\nmodel paradigm, but most of the tasks in NLP follow the\nparadigm of encoder-classiÔ¨Åer. We take two popular styles\nof MRC tasks, span-based and multi-choice, as examples for\nillustration. For the span-based style, the query is a question\nwhose answers are spans of texts. For the multi-choice style,\nthe model is requested to choose the right answer from\na set of candidate ones according to given passages and\nquestions.\nFormally, we describe the reading comprehension task\nas a tuple <P,Q, (O),A> , where P is a passage (context),\nQ is a query over the contents of P, and a span or option\nO is the right answer A. Given the original passage P and\na question Q, we organize its input xfor the encoder as the\nfollowing sequence:\nSpan: [CLS] P [SEP] Q [SEP],\nChoice: [CLS] P||Q [SEP] O [SEP].\nwhere ‚Äú ||‚Äù denotes concatenation operation, and both of\n[CLS] and [SEP] denote special marks.\nThe sequence (Span or Choice) is then fed to the encoder\nto attain the contextualized representation Hx. In the span-\nbased style model, Hxis fed to answer position network [36]\nto compute the start position As and end position Ae for the\nanswer span:\nAs = Ptr-Nets(Hx),Ae = Ptr-Nete(Hx). (14)\nFor the tasks such as SQuAD 2.0 challenge [37] that\ncontains unanswerable questions, we deÔ¨Åne an answer of 0\nfor both the start and end positions as a prediction of it being\nunanswerable, or we pass Hx to one additional veriÔ¨Åer\nmodule to predict whether the question is answerable or\nnot.\nIn the multi-choice style, Hx is fed to one MLP classiÔ¨Åer\nto predict the choice label Ac:\nP(Ac) ‚àùSoftmax(MLP(Hx)). (15)\n6.2 Backbone Encoder-side Fusion\nIn BEF, the backbone sequence (in ETC Pipeline) or hidden\nstates (in ETC Joint and ITC Joint ) are integrated with\nthe original Transformer representations in the encoder\nside. Figure 5 shows BEF on the Transformer NMT\nmodel with ETC Pipeline . In ETC Pipeline , given an input\nsequence x={x1,¬∑¬∑¬∑ ,xJ}, there is an additional compressed\nsequence xc={xc\n1,¬∑¬∑¬∑ ,xc\nK}of length K generated by the\nproposed ETC model. This compressed sequence is also\ninput to the SAN shared with the original encoder with\nword vectors vc = {vc\n1,¬∑¬∑¬∑ ,vc\nK}in a shared vocabulary\nto learn the compressed sequence‚Äôs Ô¨Ånal representation\nHc={Hc\n1,¬∑¬∑¬∑ ,Hc\nK}. In ETC Joint and ITC Joint, the hidden\nstates in text compression decoder are directly used as the\nÔ¨Ånal representation Hc of the compressed sequence. We\nintroduce an additional multi-head attention layer to fuse\nthe compressed representation and the original Transformer\nencoding to learn a more effective representation.\nFor the multi-head attention-fusion layer, a compressed\ntext representation Hc\nx is computed by the multi-head\nattention on the original sequence representation Hx and\nthe compressed sequence representation Hc:\nHc\nx = FFN(MultiHead(Hx,Hc,Hc)). (16)\nHc\nx and Hx are added to form a fusion source\nrepresentation H\n‚Ä≤\nx:\nH\n‚Ä≤\nx = Hx + Hc\nx. (17)\nFinally, the H\n‚Ä≤\nx (instead of Hx) is input to Eq. (11) for\npredicting the target translations or Eq. (14) for predicting\nthe start position As and end position Ae for the answer\nspan. Similarly, theH\n‚Ä≤\nx is also input to Eq. (15) for predicting\nthe choice label Ac.\n6.3 Backbone Decoder-side Fusion\nBEF is applicable to both encoder-decoder and encoder-\nclassiÔ¨Åer downstream models as long as a encoder is\nused; however, because the encoder-decoder paradigm uses\ncross-attention (‚Äúencoder-decoder inter-attention‚Äù) to select\nthe encoder‚Äôs representation (i.e. source representation),\nwe propose an additional method of integration. In this\nmethod, the compressed representation is regarded as a new\nindependent source representation, and the decoder will\nhandles the two representations separately. We call this BDF.\nWe present BDF with the Transformer NMT model andETC\nPipeline in Figure 5.\nIn BDF, the original Transformer encoding and the\ncompressed representation are represented as Hx and Hc\nrespectively. We then use a tuple ( Hx,Hc) instead of the\nencoder-side fusion representation H\n‚Ä≤\nx as the input to the\ndecoder. SpeciÔ¨Åcally, we introduce an additional ‚Äúencoder-\ndecoder inter-attention‚Äù module into the decoder to learn\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 8\nSource Source\nFig. 5. The architecture of our NMT model with ETC PipelineBEF.\nAdd & Norm\nSource Source Fig. 6. The architecture of our NMT model with ETC PipelineBDF.\nthe compressed sequence context bi at the current time-step\ni:\nbi = FFN(MultiHead(Htgt,Hc,Hc)). (18)\nSince our goal is to treat the original encoding and the\ncompressed representation as two independent contexts,\nwe use a context gate gc for integrating two independent\ncontexts: original context ci and compression context bi. The\ngate gi is calculated by:\ngi = œÉ(MLP([ci; bi])). (19)\nTherefore, the Ô¨Ånal decoder-side fusion context c‚Ä≤\ni is:\nc‚Ä≤\ni = gi ‚äóci + (1‚àígi) ‚äóbi, (20)\nwhere œÉis the logistic sigmoid function, ‚äóis the point-wise\nmultiplication, and [¬∑] represent the concatenation operation.\nThe context c‚Ä≤\ni is input to replace the ci the Eq. (12) to\ncompute the probabilities of next target prediction.\n6.4 Backbone Both-side Fusion\nAs discussed in the BEF and BDF, the compressed\nrepresentation in encoder-decoder paradigm can be used both\nto improve source-side encoding and to provide additional\nrepresentation for the decoder, so a novel BBF can be\nobtained by combining these two methods of integration.\nIn BBF, both the original representation Hx and text\ncompression-aided representation H\n‚Ä≤\nx are input to the\ndecoder. Similarly, we introduce an additional ‚Äúencoder-\ndecoder inter-attention‚Äù module into the decoder to learn\na text compression-aided context b\n‚Ä≤\ni at the current time-step\ni:\nb\n‚Ä≤\ni = FFN(MultiHead(Htgt,H\n‚Ä≤\nx,H\n‚Ä≤\nx)). (21)\nThen, the context gate gi in BBF (consistent with BDF) is\napplied to combine the two contexts ci and b\n‚Ä≤\ni.\n7 E XPERIMENTAL SETUP\n7.1 Text Compression\nIn ETC, under the supervised setting, we use the Trans-\nformer (big) architecture [16]. The number of layers/hidden\nsize/FFN size/number of heads is 6/1024/4096/16, and\nonly the text compression objective is used for model\ntraining (without any pre-training approaches). Under\nthe unsupervised and semi-supervised settings, in order\nto make full use of the existing pre-trained resources,\nwe use the same architecture as BART (large) [38] with\nnumber of layers/hidden size/FFN size/number of heads\n12/1024/4096/16, and the model is initialized with a pre-\ntrained BART checkpoint before training.\nIn ITC, since the ITC module is part of the downstream\nmodel and the encoder is shared, we only need to add\nan additional NAT decoder. The number of layers/hidden\nsize/FFN size/number of heads is 6/512/2048/8. An\nadditional linear mapping layer is added to handle the\ninconsistency of hidden size with the downstream model.\nThe ITC-aided model is two-stage trained. The Ô¨Årst stage\nuses text compression data for pre-training, and the second\nstage uses downstream task targets to train the model\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 9\nas a whole. Under the supervised/unsupervised/semi-\nsupervised settings, only the text compression training data\nused in the Ô¨Årst stage is different.\nFor the text compression training data, we use\nGigaword for sentence-level and CNN/DM for paragraph-\nlevel in supervised training, use a pseudo-parallel corpus\nsynthesized from 190M English monolingual unpaired data\nfrom the WMT News Crawl datasets in the unsupervised\nsetting (same as [35]), and combine the real and synthesized\ntext compression data for semi-supervised model training.\nTo evaluate the quality of text compression, the F1 score\nof ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) [39]\nwas used to evaluate the model. In ETC Pipeline , we used\nbeam search with a beam size of 5, a length normalization\nof 0.5, and a coverage penalty of 0.2. Baseline systems\nin the sentence-level text compression include AllText and\nF8W [26], [40]. F8W is simply the Ô¨Årst 8 words of the\ninput, and AllText uses the whole text as the compression\noutput. We also present the Lead-3 baseline, which simply\nselects the Ô¨Årst three sentences for a given input in the\nparagraph-level text compression. We also included the\nfollowing other pre-training methods: masked language\nmodeling (MLM, BERT) [3], denoising auto-encoder (DAE)\n[41], and masked sequence to sequence (MASS) [35] to\ncompare with our unsupervised pre-training method in the\nsemi-supervised setting. Since ETC Joint can not be fully\ntrained and was slow in training, we did not include this in\nthe main experiments. Only ETC Pipeline is used in the main\nexperiment and ETC Joint is left for analysis and comparison\nin the ablation experiment. Because ITC does not exist as an\nindependent model, the model obtained from the Ô¨Årst stage\ntraining of the ITC-aided Transformer-base model in the\nfollowing WMT14 EN-DE machine translation experiment\nwas selected as the sentence-level evaluation object, and\nITC-aided RACE reading comprehension model (BERT base\nbaseline) was selected as the paragraph-level evaluation\nobject.\nIn sentence-level ETC, we set the compression rate Œ≥ to\n0.6, and in paragraph-level ETC, we set the compression\nrate to 0.3. The compression rates of sentence-level and\nparagraph-level ITC are both set to 0.4.\n7.2 Machine Translation\nThe proposed NMT model was evaluated on the WMT14\nEnglish-to-German (EN-DE) and English-to-French (EN-FR)\ntasks, which are both standard large-scale corpora for NMT\nevaluation. For the EN-DE translation task, 4.43M bilingual\nsentence pairs from the WMT14 shared task, which includes\nCommon Crawl, News Commentary, and Europarl v7, were\nused as training data. The newstest2013 and newstest2014\nsets were used as the dev set and test set, respectively.\nFor the EN-FR translation task, 36M bilingual sentence\npairs from the WMT14 shared task were used as training\ndata. The newstest12 and newstest13 were combined for\nvalidation, and the newstest14 was the test set, following\nthe setting of [42]. The BPE algorithm [43] was also adopted,\nand the joint vocabulary size was set at 40K. For the hyper-\nparameters of our Transformer (base/large) models, we\nfollowed the settings used in [16].\nIn addition, we also reported the state-of-the-art results\nin recent literature, including modeling local dependencies\n(Localness) [44], fusing multiple-layer representations\nin SANs ( Context-Aware) [10], and fusing all global\ncontext representations in SANs ( global-deep context ) [8].\nMultiBLEU was used to evaluate the translation task. Since\nthe machine translation task we used is a sentence level task,\nwe used sentence-level text compression for enhancement.\nIn addition, for the NMT model with the encoder-decoder\nparadigm, we empirically demonstrated that BBF is a more\neffective method of integration for ETC Pipeline ; therefore,\non ITC Joint , only the results of BBF are reported to save\nspace.\nTABLE 2\nROUGE performance of the sentence-level text compression models\ntrained on the Gigaword dataset.\nModel R-1 R-2 R-L\nBaselines:\nAll text 28.91 10.22 25.08\nF8W 26.90 9.65 25.19\nUnsupervised:\nFevry et al. [23] 28.42 7.82 24.95\nETC 31.37 8.25 28.01\nITC 30.42 8.06 27.44\nSupervised:\nRNN-based Seq2seq 35.50 15.54 32.45\nNallapati et al. [30] 34.97 17.17 32.70\nETC 37.53 18.48 34.79\nITC 36.95 17.32 34.01\nSemi-supervised:\nMLM Pre-training 37.75 18.45 34.85\nDAE Pre-training 35.97 17.17 33.14\nMASS [35] 38.73 19.71 35.96\nETC 39.54 20.35 36.79\nITC 38.45 19.12 35.63\n7.3 Machine Reading Comprehension\nSpan-based MRC As a widely used MRC benchmark\ndataset, SQuAD 2.0 [37] extends the 100,000 questions\nin SQuAD 1.1 [45] with over 50,000 new, unanswerable\nquestions that are written adversarially by crowd-sourced\nworkers to resemble answerable questions. For the SQuAD\n2.0 challenge, systems must not only answer questions when\npossible but also abstain from answering when no answer\nis supported by the paragraph, making SQuAD 2.0 an even\nmore challenging natural language understanding task for\nmodels. Two ofÔ¨Åcial metrics are selected to evaluate model\nperformance: Exact Match (EM) and a softer metric, F1\nscore, which measures a weighted average of the precision\nand recall rate at the character level.\nMulti-choice MRC Our Multi-choice MRC is evaluated\non the Large-scale ReAding Comprehension Dataset From\nExaminations (RACE) dataset [46], which consists of two\nsubsets, RACE-M and RACE-H, corresponding to middle\nschool and high school difÔ¨Åculty levels. RACE contains\n27,933 passages and 97,687 questions in total and is\nrecognized as one of the largest and most difÔ¨Åcult datasets\nin multi-choice reading comprehension. The evaluation\nmetric is accuracy.\nSince the two MRC tasks are both paragraph-level,\nparagraph-level text compression and BEF are adopted.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 10\nTABLE 3\nROUGE performance of the paragraph-level text compression models\non the CNN/DM dataset.\nModel R-1 R-2 R-L\nBaselines:\nLead-3 40.34 17.70 36.57\nUnsupervised:\nETC 38.95 17.04 35.70\nITC 38.77 16.25 34.13\nSupervised:\nPTGEN [47] 36.44 15.66 33.42\nPTGEN+COV [47] 39.53 17.28 36.38\nTransformerABS [48] 40.21 17.76 37.09\nETC 40.45 17.92 37.14\nITC 38.87 16.98 36.01\nSemi-supervised:\nUniLM [49] 43.33 20.21 40.51\nBERTSUMABS [48] 41.72 19.39 38.76\nBERTSUMEXTABS [48] 42.13 19.60 39.18\nBART [38] 44.16 21.28 40.90\nPEGASUS [50] 44.17 21.47 41.11\nProphetNet [51] 44.20 21.17 41.30\nETC 44.35 21.32 41.05\nITC 40.74 18.19 37.25\n8 M AIN RESULTS\n8.1 Text Compression\nWe conducted a comparison between our proposed text\ncompression model and other text compression models in\ndifferent settings.\nTable 2 shows the results at the sentence-level. We\nobserved that the proposed unsupervised ETC model\nperformed substantially better than Fevry and Phang [23]‚Äôs\nunsupervised method. The proposed supervised ETC model\nalso substantially outperformed the RNN-based Seq2seq\nand Nallapati et al. [30]‚Äôs baseline method; that is, our\nsupervised model gave +2.0 improvements on R-1, R-2, and\nR-L scores over the RNN-based Seq2seq. This means that\nthe proposed Transformer-based approaches can generate\ncompressed sentences of high quality.\nWe further compared our semi-supervised model with\nthe semi-supervised pre-training methods of MLM [3],\nDAE [41], and MASS [35]. Our unsupervised ETC pre-\ntraining method consistently outperformed the other\nunsupervised pre-training ones on the text compression\ntask. Comparing the results of ETC and ITC, because the\nITC model is relatively smaller and a pre-trained model\nsuch as BART is not used for initialization, the effect is\nworse. In addition, because the non-autoregressive decoder\nhas some weaknesses in explicit decoding compared\nto other autoregressive decoders, it only achieves an\neffect comparable to other explicit autoregressive decoding\nmethods; however, since the strength of ITC lies in the joint\ntraining of representations rather than explicit decoding,\nthis is only for rough reference, and the real enhancement is\nexpected to occur on downstream tasks.\nTable 3 summarizes the results of paragraph-level\ntext compression models on the CNN/DM dataset. First,\nour unsupervised paragraph-level ETC model does not\noutperform Lead-3 as it did F8W in the sentence level,\nsuggesting that the paragraph-level text compression task\nis more complex than at the sentence-level. Second,\nour supervised paragraph-level ETC model outperforms\nthe TransformerABS reported in [48], which indicates\nthat joint training with MLM pre-training objectives\ncan bring some performance improvement during the\ndownstream task target training, which is consistent\nwith the conclusion of work [53]. Third, in the semi-\nsupervised setting, our model is slightly better than\nour base, BART [38], in performance, demonstrating\nthe effectiveness of our proposed unsupervised ETC\npre-training. Fourth, comparing the unsupervised and\nsupervised results, our unsupervised system outperforms\nsome supervised systems, like PTGEN and PTGEN + COV .\nThis is mainly because our unsupervised model is based on\nthe Transformer structure and initialized with BART‚Äôs pre-\ntrained parameters, while these supervised systems are built\non an RNN structure. This shows that the feature extraction\nability of Transformer is stronger than that of RNN and\nthat the BART pre-training is very effective for sequence-\nto-sequence (seq2seq) generation. This is consistent with\nresults on sentence-level compression. At the paragraph\nlevel, especially in the semi-supervised setting, when the\ngoal is a compressed sequence, the advantages of ETC\nin comparison to ITC are more obvious because of the\nautoregressive decoding. Since the semi-supervised text\ncompression performs the best, we adopt semi-supervised\ntext compression models for downstream task evaluation\nwhen not otherwise speciÔ¨Åed.\n8.2 Machine Translation\nThe main results on the WMT14 EN-DE and EN-FR\ntranslation tasks are shown in Table 4. In the EN-DE task,\nwe made the following observations:\n1) The baseline Transformer (base) in this work achieved\nperformance comparable to the original Transformer\n(base) [16]. This indicates that it is a strong baseline NMT\nsystem.\n2) All BEF, BDF, and BBF in ETC Pipeline signiÔ¨Åcantly\noutperformed the baseline Transformer (base/big) and only\nintroduced a very small amount of extra parameters. This\nindicates that the compressed backbone information was\nbeneÔ¨Åcial for the Transformer translation system.\n3) ETC Pipeline BDF performed better than ETC\nPipeline BEF. This indicates that the backbone fusion on\nthe decoder-side is better than that on the encoder-side.\nIn addition, ETC Pipeline BBF (base/big) outperformed\nthe comparison systems +Localness and +Context-Aware\nSANs. This indicates that additional context (backbone\ninformation) is also an effective option for improving the\nNMT baselines.\n4) ETC Pipeline BBF (base) is comparable to the +global-\ndeep context, the best comparison system, while BBF (big)\nslightly outperformed +global-deep context by 0.16 BLEU\nscore. In particular, the parameters of ETC Pipeline BBF\n(base/big) model, which just increased 12.1/7.9M over the\nTransformer (base/big), were only 70% of the +global-deep\ncontext model. This shows that the ETC Pipeline BBF\nmodel is more efÔ¨Åcient than the +global-deep context model,\nthough the training speed of the proposed models slightly\ndecreased (8%), compared to the corresponding baselines.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 11\nTABLE 4\nComparison with existing NMT systems on the WMT14 EN-DE and EN-FR translation tasks. ‚Äú++/+‚Äù after the BLEU score indicates that the\nproposed method was signiÔ¨Åcantly better than the corresponding baseline Transformer (base or big) at signiÔ¨Åcance level p<0.01/0.05 [52].\n‚Äú#Speed‚Äù denotes the decoding speed measured in target tokens per second.\nSystem EN-DE #Speed #Params EN-FR #Speed #Params\nExisting NMT systems\nTransformer (base) [16] 27.3 N/A 65.0M 38.1 N/A N/A\n+Localness [44] 28.11 N/A 88.8M N/A N/A N/A\n+Context-Aware SANs [10] 28.26 N/A 194.9M N/A N/A N/A\n+global-deep context [8] 28.58 N/A 111M N/A N/A N/A\nTransformer (big) [16] 28.4 N/A 213.0M 41.0 N/A N/A\n+Localness [44] 28.89 N/A 267.4M N/A N/A N/A\n+Context-Aware SANs [10] 28.89 N/A 339.6M N/A N/A N/A\n+global-deep context [8] 29.21 N/A 396M N/A N/A N/A\nOur NMT systems\nTransformer (base) 27.24 131K 66.5M 38.21 130K 85.7M\n+ETC Pipeline BEF 27.75++ 121K 72.1M 39.09++ 120K 89.0M\n+ETC Pipeline BDF 28.14+ 120K 72.7M 39.22++ 119K 89.8M\n+ETC Pipeline BBF 28.35++ 119K 78.6M 39.40++ 116K 91.4M\n+ITC Joint BBF 28.57++ 120K 109.2M 39.68++ 116K 133.5M\nTransformer (big) 28.23 11K 221.0M 41.15 11K 222.3M\n+ETC Pipeline BEF 28.52+ 10K 225.2M 41.92+ 9K 227.1M\n+ETC Pipeline BDF 29.16++ 9K 225.7M 42.22++ 8K 227.5M\n+ETC Pipeline BBF 29.37++ 8K 228.9M 42.52++ 8K 230.3M\n+ITC Joint BBF 29.60++ 8K 273.2M 42.77++ 8K 275.4M\n5) The proposed ETC Pipeline BBF (base) slightly\noutperformed Transformer (big), this contains much more\nparameters than ETC Pipeline BBF (base). This indicates\nthat our improvement is not likely to be due to the increased\nnumber of parameters.\nFor the EN-FR translation task, the proposed models\ngave similar improvements over the baseline systems\nand other top models (though Transformer (big) had a\nmuch greater improvement over Transformer (base)). These\nresults show that our method is robust in improving the\ntranslation of other language pairs.\nIn the text compression evaluation, we found ITC did\nnot perform as well as ETC and was even inferior to some\nother text summarization models; however, since the ITC\nmodule will be optimized in the training of downstream\ntasks, ts performance on only text compression is not\nnecessarily indicative of its performance on the downstream\ntask, so we also conducted experiments on ITC Joint BBF.\nComparing the results of ITC Joint BBF, ETC Pipeline\nBBF, and the baselines, ITC Joint brings improvements to\nthe baseline model, which again illustrates the contribution\nof text compression to machine translation. In addition,\nITC Joint obtains greater improvements than ETC Pipeline ,\nindicating that the joint training can take advantage of the\nsimilarities in tasks and reduce error propagation. Besides,\nthe number of parameters in the ITC Joint model is not\ndirectly comparable to that of the ETC Pipeline model as the\nparameters of the ETC model are not included in the total\nof the machine translation model. Additionally, the speed of\ntranslation did not slow down and even increased, which is\ndue to the fact that ITC eliminates the redundant encoding\nprocess and adopts a non-autoregressive decoder instead of\nan autoregressive decoder.\n8.3 Machine Reading Comprehension\nIn addition to the evaluation on the encoder-decoder\ndownstream NMT model, we also conducted experiments\nwith the encoder-classiÔ¨Åer MRC models. Table 5 shows\nthe results for reading comprehension on the SQuAD\n2.0 dev and test sets 2. ETC solidly improves over the\nstrong BERT baseline in both EM and F1. With the more\npowerful ALBERT, ETC still obtains improvements and\nalso outperforms all the published works and achieves\ncomparable performance with a few unpublished models\nfrom the leaderboard 3. In particular, our model is slightly\nbetter than the state-of-the-art ALBERT+Retro-Reader [54].\nThis means that text compression is beneÔ¨Åcial for the\nreading comprehension task. In addition, comparing ETC\nPipeline and ITC Joint, the ITC approach deÔ¨Ånitely beneÔ¨Åts\nfrom joint training and can generally obtain better\nperformance.\nTable 6 shows the results on the RACE task. The\nre-implemented BERT and ALBERT baselines performed\ncomparably to the Leaderboard, indicating that they are\nstrong baselines. Our model boosted the BERT baseline\nsubstantially, with an increase of +0.8% accuracy, and\nit boosted the ALBERT baseline with +0.6% accuracy\non the test set. This improvement further veriÔ¨Åes that\nthe proposed method is a robust method for improving\nperformance on the MRC task. In addition, although there\nis still a gap between our model and the state-of-the-art\nALBERT+DUMA [57], which improves the performance\nwith a more complicated neural network structure, on\nthe current leaderboard, we obtain improvement from\n2. To focus on the evaluation of ETC and keep simplicity, we only\ncompared with single models instead of ensemble ones.\n3. The test set needs to be evaluated online, and the results on the\ntest set were not available at the time of submission.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 12\nTABLE 5\nExact Match (EM) and F1 scores on the SQuAD 2.0 dev and test sets\nfor single models.\nModel Dev Test\nEM F1 EM F1\nRegular Track\nJoint SAN\nU-Net [55] 70.3 74.0 69.2 72.6\nRMR+ELMo+VeriÔ¨Åer [56] 72.3 74.8 71.7 74.2\nLeaderboard (Feb. 23st, 2020)\nHuman N/A N/A 86.83 89.45\nBERT 78.70 81.90 80.00 83.06\nXLNet [12] 87.90 90.60 87.92 90.68\nALBERT [13] 87.40 90.20 88.10 90.90\nALBERT+VeriÔ¨Åer N/A N/A 88.35 91.01\nALBERT+Retro-Reader [54] 87.80 90.90 88.10 91.41\nOur implementation\nBERT Baseline 78.57 81.84 ‚àí ‚àí\n+ETC Pipeline 79.33 82.59 ‚àí ‚àí\n+ITC Joint 79.62 82.94 ‚àí ‚àí\nBERT + VeriÔ¨Åer 79.52 82.61 ‚àí ‚àí\n+ETC Pipeline 79.94 83.23 ‚àí ‚àí\n+ITC Joint 80.15 83.54 ‚àí ‚àí\nALBERT Baseline 87.00 90.15 ‚àí ‚àí\n+ETC Pipeline 87.50 90.50 ‚àí ‚àí\n+ITC Joint 87.75 90.85 ‚àí ‚àí\nALBERT + VeriÔ¨Åer 87.42 90.45 ‚àí ‚àí\n+ETC Pipeline 87.98 90.97 ‚àí ‚àí\n+ITC Joint 87.95 91.00 ‚àí ‚àí\nthe representation learning, which is orthogonal to their\nimprovement, so our results could be further improved\nthrough their complicated network structure. Similar to its\nresults on the SQuAD task, ITC Joint outperforms ETC\nPipeline on the RACE task. These results indicate that\nalthough the compressed text obtained by ETC is better than\nthe ITC for text compression on its own with automated\nmetrics such as F1, its results are not as good as those of ITC\non the downstream task. This may shows that the backbone\ninformation for different tasks is not static. Finetuning text\ncompression and downstream tasks together can lead to\nmore task-relevant backbone information, which can better\nimprove the performance of downstream tasks.\n9 A BLATION STUDY\n9.1 Effect of Different Levels of Text Compression\nQuality\nTo show effect of different levels of text compression\nquality on the improvement of downstream tasks, we\nconducted experiments on the WMT14 EN-DE translation\ntask using Transformer (base) +ETC Pipeline BBF and\n+ITC Joint BBF. We considered six different levels of\ncompression quality resulting from the following settings:\nAllText, F8W, RandSample (random sampling), Supervised,\nUnsupervised, and Semi-supervised. The performance\ncomparison is shown in Table 7.\nWe made the following observations:\n1) Simply introducing AllText and F8W achieved few\nimprovements, and RandSample was outperformed by the\nTABLE 6\nAccuracy on the RACE test set for single models.\nModel Middle High All\nHuman Performance\nTurkers 85.1 69.4 73.3\nCeiling 95.4 94.2 94.5\nLeaderboard (Feb. 23st, 2020)\nBERT [58] 75.6 64.7 67.9\nXLNet [12] 85.5 80.2 81.8\nALBERT [13] 89.0 85.5 86.5\nALBERT+DUMA [57] 90.9 86.7 88.0\nOur implementation\nBERT Baseline 76.6 70.1 72.0\n+ETC Pipeline 77.8 70.7 72.8\n+ITC Joint 78.3 70.9 73.4\nALBERT Baseline 88.7 85.6 86.5\n+ETC Pipeline 89.3 86.2 87.1\n+ITC Joint 89.6 86.3 87.3\nTABLE 7\nThe effect of levels of text compression quality on translation baseline.\nThe results are reported on the newstest2014 test set.\nModel ETC Pipeline ITC Joint\nBaseline 27.24 27.24\n+AllText 27.24 ‚àí\n+F8W 27.40 ‚àí\n+RandSample 26.53 ‚àí\n+Supervised 27.80 27.64\n+Unsupervised 27.97 28.10\n+Semi-supervised 28.35 28.57\nbaseline. In comparison, the +Supervised, +Unsupervised,\nand +Semi-supervised models substantially improved the\nperformance over the baseline Transformer (base). This\nmeans that our text compression approach provides richer\nand useful source information for machine translation tasks.\n2) +Unsupervised models can lead to better im-\nprovements compared to +Supervised models, although\nsupervised models outperform unsupervised models in\nthe text compression benchmark. This may be due to\nthe fact that the annotated text compression training\ndata is in a domain different from the WMT EN-DE\ntraining data. +Semi-supervised models with annotated\ndata Ô¨Ånetuning outperformed both +Unsupervised and\n+Supervised models.\n3) Comparing ETC Pipeline and ITC Joint , ETC Pipeline\nis more effective under a supervised setting, while ITC\nJoint performs better under other settings. This may\nbe because in the case of limited (annotated) training\ndata, ETC Pipeline can be optimized better by using an\nindependent compression model, but in unsupervised and\nsemi-supervised settings, ETC Pipeline loses this advantage\nas there is more data available.\n9.2 Effect of Encoder Parameters\nIn ETC Pipeline, representations of the original sentence and\nits compressed sequence were learned by a shared encoder.\nTo explore the effect of the encoder parameters, we also\ndesigned a BBF model with two independent encoders\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 13\nto learn respective representations of the original sentence\nand its compressed version. Table 8 shows results on the\nnewstest2014 test set for the WMT14 EN-DE translation\ntask.\nTABLE 8\nThe effect of encoder parameters.\nModel BLEU #Params\nTransformer (base) 27.24 66.4M\n+ETC Pipeline BBF w/ Shared encoder 28.35 78.6M\n+ETC Pipeline BBF w/ Independent encoders 28.50 91.6M\nThe BBF (w/ independent params) slightly outper-\nformed the proposed shared encoder model by a BLEU\nscore of 0.15, but its parameters increased by approximately\n30%. In contrast, the parameters in our model are\ncomparable to those of Transformer (base). To account for\nthe number of parameters, we used a shared encoder to\nlearn source representations, which makes it easy to verify\nthe effectiveness of the additional backbone information.\n9.3 Different Compression Ratio Evaluation\nIn order to verify the impact of different compression\nratios on the downstream task, we conducted experiments\non the WMT14 EN-DE translation task with ETC Pipeline\nBBF and ITC Joint BBF using Transformer (base) models\nunder a semi-supervised text compression training setting.\nWe gradually increased the compression ratio Œ≥ from\n0 to 1.0. When the compression ratio Œ≥ = 0 , no\ncompression sequence is generated, which is the same\nas the vanilla Transformer. Setting the compression ratio\nŒ≥ = 1.0 is equivalent to re-paraphrasing the source sentence\n(maintaining the same length).\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n27.5\n28\n28.5\nCompression Ratio\nBLEU score\nETC Pipeline\nITC Joint\nFig. 6. Performances on EN-DE newstest2014 with different text\ncompression ratios.\nThe experimental results are shown in Fig. 6. As can\nbe seen from the results, text compression can bring\nperformance improvement. When the compression ratio\nŒ≥ = 1.0 and the sentence length is not shortened, re-\nparaphrasing can still bring a slight improvement in\ntranslation quality. Additionally, the experimental results\nshow that the best compression ratio of ITC Joint is lower\nthan that of ETC Pipeline. This may be because ITC and the\ndownstream tasks are optimized together, and information\nthat is not relevant to the speciÔ¨Åc downstream task is more\noften excluded from the compression, which leads to a\nlower compression ratio in ITC Joint achieving the best\nperformance compared with ETC Pipeline.\n9.4 ETC Pipeline, ETC Joint, ITC Joint, and DAE pre-\ntraining\nETC Pipeline , ETC Joint , and ITC Joint are the three\ncompression methods we studied. In this section, we\ncompared their performance, training time, and inference\nspeed to visually demonstrate their differences. The Ô¨Årst\nstage of training in ITC Joint is a bit similar to DAE pre-\ntraining of a model‚Äôs encoder, so we take this as one\nof the baselines for comparison. We only compare the\nactual downstream model training and inference speed. The\ntime in pre-training and compression generation are not\nconsidered. The baseline model is still Transformer (base),\nand the evaluation task is WMT14 EN-DE translation. All\nmodels are trained for 20w steps to make the training time\ncomparable, and the results are shown in Table 9.\nTABLE 9\nComparison of ETC Pipeline, ETC Joint, ITC Joint, and DAE\npre-training approaches.\nSystems BLEU #Training #Speed\nTransformer (base) 27.24 16h 131k\n+DAE 27.65 16h 131k\n+ETC Pipeline BBF 28.35 18h 119k\n+ETC Joint BBF 28.04 46h 65k\n+ITC Joint BBF 28.57 20h 120k\nThe comparison results show that the training time of\nETC Joint is much longer than those of ETC Pipeline and ITC\nJoint because of the autoregressive decoding for each batch\nin training. Secondly, although the ETC Joint outperforms\nthe baseline, the improvement is less than that of the ETC\nPipeline. The reason may be that only the self attention part\nin the ETC Model is jointly updated in the downstream\ntask training, which is no better than re-encoding using\nthe trainable encoder in the downstream model. Also,\nthe DAE pre-training method can improve the baseline‚Äôs\nperformance to a certain extent, but the improvement is less\nthan that of our text compression, which veriÔ¨Åes that does\nnot only rely on pre-training, and backbone information is\nindeed useful.\n10 F URTHER EXPLORATION\n10.1 Transfer Learning Effects of ETC\nIn our experiments on machine translation, we found that\nthe semi-supervised setting was best when enhancing the\nbaseline NMT model with ETC. We hypothesize that this\nresults from the inconsistent domains of the training data\nin machine translation and text compression. To verify this\nhypothesis, we carried out a domain adaptation experiment\non the IWSLT 2015 [59] EN-DE task.\nThe training data of IWSLT 2015 EN-DE is taken as the\nin-domain (TED domain) training set for the experiment,\nand TED tst2010 is used as the development set. TED\ntst2011, tst2012, tst2013 are concatenated to make the in-\ndomain test set. The WMT 14 EN-DE newstest2014 is used\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 14\nas the out-of-domain test set. We use the Transformer (base)\nETC Pipeline BBF model of the previous experiment for the\nNEWS domain. For the TED domain, we Ô¨Ånetune the NEWS\ndomain ETC model using the source corpus of IWSLT15 EN-\nDE with an unsupervised text compression target.\nTABLE 10\nThe transfer learning results of ETC-aided models.\nSystems ETC domain IWSLT WMT\n(TED) (NEWS)\nTransformer (base) - 30.76 5.24\n+ETC Pipeline BBF NEWS 30.54 7.88\nTED 31.28 4.91\nDue to the small scale of the IWSLT15 training set, we\nchoose Transformer (base) as the baseline. The experimental\nresults are shown in Table 10. +ETC Pipeline BBF brings\n0.52 and 2.64 BLEU points improvements over its baseline\nmodel when the domain of ETC model is the same as the test\nset. This shows the proposed ETC method has the function\nof transfer learning to a certain extent when the domain of\nETC model is the same as the test set. We suspect that this\nis because the ETC model can extract a sentence‚Äôs useful\nbackbone information. Although the training and test sets\nhave different domains and different styles, the backbone\nstructure of a sentence is a consistent property that can\nbeneÔ¨Åt the model in different domains. +ETC Pipeline BBF\nenhances the robustness of the model by normalizing the\nchanges caused by different domains.\nIn addition, when the domain of the ETC model is\ninconsistent with the test set, the results on the test set show\nsome decline compared to the baseline. The reason for this\nphenomenon may be due to the inconsistency in domain,\nwhich the compression generated to worsen, and the error\npropagates to downstream models. In this case, Ô¨Ånetuning\nthe ETC model using the monolingual corpus of the target\ndomain is an effective and necessary operation.\n10.2 Language Representation Improvement Contribu-\ntion\nTo evaluate the contributions of the language representation\nimprovement in the text compression-aided models, we\nperformed an ablation study on an NLI task. NLI involves\nreading a pair of sentences ‚ü®premise,hypothesis‚ü©and judging\nthe relationship between their meanings, such as entailment,\nneutral, or contradiction. We evaluated our ETC-aided\nmodel on and alongside a strong ALBERT baseline and train\nboth the ETC-aided model and the baseline on the MNLI\ndataset using adversarial evaluation of natural language\ninference with the Heuristic Analysis for the NLI Systems\n(HANS) dataset proposed by McCoy et al. [11].\nMcCoy et al. [11], aiming to give the model some hints\nduring inference, suggest using three heuristics for good\nperformance for models in NLI:\n‚Ä¢ lexical overlap : assume that the premise entails all\nhypotheses constructed from words in the premise.\n‚Ä¢ subsequence: assume that the hypothesis is the\nsubstring of premise.\n‚Ä¢ constituent: assume that the premise entails all\ncomplete subtrees in its parse tree.\nThe instances labeled ‚Äúentailed‚Äù and ‚Äúnon-entailed‚Äù in\nHANS that meet with the three heuristics, obey a uniform\ndistribution. Table 11 shows results on the MNLI dev dataset\nand HANS dataset. From the results, we can see that:\n1) The performance of models on ‚Äúentailed‚Äù is better\nthan on ‚Äúnon-entailed.‚Äù This suggests that the model is\nmaking excessive use of heuristic information. According to\nMcCoy et al.‚Äôs [11] explanation, MNLI training data might\nnot provide not enough supervision to learn the desired\nlevel of NLI.\n2) From BERT base to BERT large to ALBERT xxlarge, the\nmodel parameters gradually increased, and the results on\nthe MNLI dev set improved. The results of entailed and\nnon-entailed data were generally better, but the result of\nBERT on non-entailed ‚Äúconstituent‚Äù data was stronger than\nthat of ALBERT. This may be due to the loss of syntactic\ninformation due to the sharing of ALBERT layer parameters.\n3) The ETC-aided model we proposed can enhance\nperformance on ‚Äúnon-entailed‚Äù data, indicating that the\nquality of language representation has been improved.\n10.3 Probing Evaluation for the Language Representa-\ntion\nThe assessment of the MNLI model on the HANS dataset\nshows that the improvement of results does come from a\nbetter quality language representation rather than tricks.\nIn order to understand what parts of the language\nrepresentations improved, we further evaluate the MNLI\nmodel using ten widely-used language probing tasks [60].\nSpeciÔ¨Åcally, we use the BERT base and ALBERT base\nmodels trained for the MNLI task to generate a sentence\nrepresentation (embedding) of the inputs to evaluate the\nquality of the linguistic properties are encoded in them.\nThe evaluation results are shown in Table 12. Comparing\nthe two baselines, BERT and ALBERT, shows that ALBERT\nuses only about 1/10 the parameters of BERT and achieves a\ncomparable results. +ETC Pipeline BEF achieves statistically\nsigniÔ¨Åcant improvement on the WC, TopConst, and Tense\ntasks compared to the two baselines, which demonstrates\nthat the ETC-aided model focuses on important words,\nconstituents, and verbs more in its sentence representations,\nwhich is consistent with our hypothesis.\n10.4 ETC Enhancement on Different MAXSEQ LEN\nIn Transformer encoders, theO(N2) self-attention operation\nis a process where every output element is connected to\nevery input element, and the weightings between them\nare dynamically calculated based upon the circumstances.\nTransformer encoders are very Ô¨Çexible compared to models\nwith Ô¨Åxed connectivity patterns because of this nature, but\nthey consume large amounts of memory and inference time\nwhen applied to data types with many elements, such as the\nMRC task.\nCurrently, there is an upper limit on the memory of\na single NVIDIA GPU, so the mainstream approach to\nreducing the memory requirement for a long sequence is\nto truncate long sentences or adopt a sliding window with a\nlimited window size for them. The Ô¨Årst method is typically\nused for choice-style MRC tasks (RACE), and the second\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 15\nTABLE 11\nAccuracy results on the MNLI dev dataset, including match (m) and mismatch (mm) parts, and results on the HANS dataset.\nMNLI Heuristic Entailed Heuristic Non-entailed\nm/mm(acc) lexical overlap subsequence constituent lexical overlap subsequence constituent\nBERTbase 84.61/83.43 93.04 97.76 96.82 59.90 3.96 11.80\nBERTlarge 86.24/85.96 97.02 99.42 99.62 70.40 29.08 33.48\n+ETC Pipeline BEF 86.69/86.35 98.10 99.22 99.48 75.46 29.22 34.12\nALBERTxxlarge 90.62/90.59 100 99.92 99.98 88.48 29.86 10.34\n+ETC Pipeline BEF 90.85/90.74 99.90 99.96 99.72 94.24 35.42 14.80\nTABLE 12\nTen probing task accuracies. ‚Äù++/+‚Äù after the accuracy score indicate that the score is statistically signiÔ¨Åcant at levelp <0.01/0.05. The tasks are\nabbreviated: SentLen: sentence length, WC: important word, TreeDepth: syntactic tree depth, TopConst: top constituents, BShift: word order,\nTense: verb tense, SubjNum: subject number, ObjNum: object number, SOMO: semantic replacement, CoordInv: coordinated clausal conjoints\norder.\nTask SentLen WC TreeDepth TopConst BShift Tense SubjNum ObjNum SOMO CoordInv #Params\nBERTbase 65.84 50.35 29.97 53.31 72.22 86.35 77.63 76.23 61.18 61.14 109M\n+ETC PipelineBEF 65.78 59.24++ 30.01 55.42++ 71.79 86.83+ 77.79 76.58 60.78 60.06 110M\nALBERTbase 65.46 50.02 31.11 52.99 71.82 86.28 77.89 76.51 60.36 60.34 12M\n+ETC PipelineBEF 65.45 58.78++ 31.02 55.07++ 71.15 86.68+ 77.95 76.81 59.98 60.41 13M\nis used for span-style MRC tasks (SQuAD); however, both\nmethods have an impact on the performance of our model.\nTo explore the effect of ETC enhancements at different\nmaximum sequence lengths (MAX SEQ LEN), we report\nthe results of the ALBERT baseline and our proposed\nETC-aided model on the SQuAD 2.0 development set\nand the RACE test set, as shown in Table 13. The\nexperimental results show a downward trend in model\nperformance with a decrease of the maximum sequence\nlength. Under different MAX SEQ LEN, the performance\nimprovements on SQuAD are 0.52, 0.46 and 0.50, and\n0.6, 0.8 and 0.9 on RACE, respectively. Adopted by\nSQuAD to predict the answer position, the sliding window\nmechanism alleviates the problem caused by the maximum\nlength, and thus, the relationship between ETC and\nthe magnitude of performance variation could not be\nclearly seen. In the RACE task, the model truncates\nlength over MAX SEQ LEN. With this reduction of the\nmaximum length, the performance degradation is reduced,\nshowing that ETC improves the quality of truncated text\nrepresentations.\nTABLE 13\nResults on the SQuAD2.0 dev and RACE test sets with different\nMAX SEQ LEN (MSL). M./H. represent middle and high splits of the\nRACE test set.\nModel MSL SQuAD RACE\nEM F1 M./H. All\nALBERT Baseline 512 87.42 90.45 88.7/85.6 86.5\n+ETC Pipeline BEF 512 87.98 90.97 89.3/86.2 87.1\nALBERT Baseline 384 86.87 89.98 87.9/84.2 85.2\n+ETC Pipeline BEF 384 87.36 90.44 88.7/84.9 86.0\nALBERT Baseline 256 86.33 89.51 87.0/82.6 83.8\n+ETC Pipeline BEF 256 86.85 90.01 87.7/83.6 84.7\n11 R ELATED WORK\nExploiting sentence segmentation, sentence simpliÔ¨Åcation,\nand text compression have all been used to emphasize\nthe source sentence information for machine translation.\n[61] presented an approach to integrating the sentence\nskeleton information into a phrase-based statistic machine\ntranslation system. [62] proposed an approach to modeling\nsyntactically-motivated skeletal structure of a source\nsentence for statistic machine translation. [63] described\nan early approach to skeleton-based translation, which\ndecomposes input sentences into syntactically meaningful\nchunks. The central part of the sentence is identiÔ¨Åed\nand remains unaltered while other parts of the sentence\nare simpliÔ¨Åed. This process produces a set of partial,\npotentially overlapping translations that are recombined\nto form the Ô¨Ånal translation. [64] described a ‚Äúdivide\nand translate‚Äù approach to dealing with complex input\nsentences. They parse the input sentences, replace\nsubclauses with placeholders, and later substitute them\nwith separately translated clauses. Their method requires\ntraining translation models on clause-level aligned parallel\ndata with placeholders in order for the translation model\nto deal with the placeholders correctly. [65] experimented\nwith automatically segmenting the source sentence to\novercome problems with overly long sentences. [66] showed\nthat the spaces of original and simpliÔ¨Åed translations\ncould be effectively combined using translation lattices and\ncompared two decoding approaches to process both inputs\nat different levels of integration.\nDifferent from these works, our proposed text\ncompression model does not rely on any known linguistics-\nmotivated (such as syntax) skeleton simpliÔ¨Åcation but\ndirectly trains a computation-motivated text compression\nmodel to learn to compress sentences and re-paraphrase\nthem directly in a seq2seq model. Our text compression\nmodel can surprisingly generate more grammatically correct\nand reÔ¨Åned sentences, and the words in the compressed\nsentence do not have to be the same as those in the\noriginal sentence. At the same time, our text compression\nmodel can give source backbone representation exempt\nfrom the unstable performance of a syntactic parser, which\nis especially important as stable performance is essential\nfor syntactic skeleton simpliÔ¨Åcation. Our text compression\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 16\nmodel can perform unsupervised training on large-scale\ndatasets and then use the supervised data for Ô¨Ånetuning,\nwhich produces more impressive results.\n12 C ONCLUSION\nThis paper presents explicit and implicit text compression\nmethods for improving representations from Transformer\nencoders. We aim to use the backbone knowledge of text to\ngenerate more accurate characteristic statistical distributions\nin language representation learning. To demonstrate that the\nproposed text compression enhancement serves a general\npurpose in NLP tasks, we evaluate the impact of the\nproposed approach in two major NLP tasks, machine\ntranslation and machine reading comprehension, and in\nan extra natural language inference task for natural\nlanguage understanding evaluation. The experimental\nresults show that our proposed model can yield signiÔ¨Åcant\nimprovements over strong baselines on Ô¨Çagship datasets\nand benchmark leaderboards for these challenging tasks.\nACKNOWLEDGMENTS\nWe thank Kevin Parnow, from the Department of Computer\nScience and Engineering, Shanghai Jiao Tong University\n(parnow@sjtu.edu.cn) for editing a draft of this manuscript.\nREFERENCES\n[1] Y. Bengio, R. Ducharme, P . Vincent, and C. Jauvin, ‚ÄúA neural\nprobabilistic language model,‚Äù Journal of machine learning research ,\nvol. 3, no. Feb, pp. 1137‚Äì1155, 2003.\n[2] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\n‚ÄúDistributed representations of words and phrases and their\ncompositionality,‚Äù in Advances in neural information processing\nsystems, 2013, pp. 3111‚Äì3119.\n[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT:\nPre-training of deep bidirectional transformers for language\nunderstanding,‚Äù in Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers). Minneapolis, Minnesota: Association for Computational\nLinguistics, Jun. 2019, pp. 4171‚Äì4186.\n[4] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, ‚ÄúDeep contextualized word representations,‚Äù\nin Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers) . New Orleans, Louisiana:\nAssociation for Computational Linguistics, Jun. 2018, pp. 2227‚Äì\n2237.\n[5] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever,\n‚ÄúImproving language understanding by generative pre-\ntraining,‚Äù URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding\npaper. pdf, 2018.\n[6] M. Halliday, C. M. Matthiessen, and C. Matthiessen, An\nintroduction to functional grammar. Routledge, 2014.\n[7] UKEssays, ‚ÄúDistinguishing a sentence from an\nutterance,‚Äù ukessays.com, 2018. [Online]. Available:\nhttps://www.ukessays.com/essays/english-language/\ndistinguish-a-sentence-from-an-utterance-english-language-essay.\nphp?vref=1\n[8] Z.-Y. Dou, Z. Tu, X. Wang, S. Shi, and T. Zhang, ‚ÄúExploiting deep\nrepresentations for neural machine translation,‚Äù in Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language\nProcessing. Brussels, Belgium: Association for Computational\nLinguistics, Oct.-Nov. 2018, pp. 4253‚Äì4262.\n[9] X. Wang, Z. Tu, L. Wang, and S. Shi, ‚ÄúExploiting sentential context\nfor neural machine translation,‚Äù in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics . Florence,\nItaly: Association for Computational Linguistics, Jul. 2019, pp.\n6197‚Äì6203.\n[10] B. Yang, J. Li, D. F. Wong, L. S. Chao, X. Wang, and Z. Tu,\n‚ÄúContext-aware self-attention networks,‚Äù in Proceedings of the\nAAAI Conference on ArtiÔ¨Åcial Intelligence, vol. 33, 2019, pp. 387‚Äì394.\n[11] T. McCoy, E. Pavlick, and T. Linzen, ‚ÄúRight for the wrong reasons:\nDiagnosing syntactic heuristics in natural language inference,‚Äù\nin Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics . Florence, Italy: Association for\nComputational Linguistics, Jul. 2019, pp. 3428‚Äì3448.\n[12] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov,\nand Q. V . Le, ‚ÄúXlnet: Generalized autoregressive pretraining\nfor language understanding,‚Äù in Advances in neural information\nprocessing systems, 2019, pp. 5754‚Äì5764.\n[13] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P . Sharma, and\nR. Soricut, ‚ÄúAlbert: A lite bert for self-supervised learning of\nlanguage representations,‚Äù in International Conference on Learning\nRepresentations, 2019.\n[14] P . K. Mudrakarta, A. Taly, M. Sundararajan, and K. Dhamdhere,\n‚ÄúDid the model understand the question?‚Äù in Proceedings of the\n56th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) . Melbourne, Australia: Association for\nComputational Linguistics, Jul. 2018, pp. 1896‚Äì1906.\n[15] R. Jia and P . Liang, ‚ÄúAdversarial examples for evaluating reading\ncomprehension systems,‚Äù in Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing . Copenhagen,\nDenmark: Association for Computational Linguistics, Sep. 2017,\npp. 2021‚Äì2031.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nin Advances in neural information processing systems, 2017, pp. 5998‚Äì\n6008.\n[17] O. Bojar, C. Federmann, M. Fishel, Y. Graham, B. Haddow,\nP . Koehn, and C. Monz, ‚ÄúFindings of the 2018 conference\non machine translation (WMT18),‚Äù in Proceedings of the Third\nConference on Machine Translation: Shared Task Papers . Belgium,\nBrussels: Association for Computational Linguistics, Oct. 2018, pp.\n272‚Äì303.\n[18] A. Williams, N. Nangia, and S. Bowman, ‚ÄúA broad-coverage\nchallenge corpus for sentence understanding through inference,‚Äù\nin Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers) . New Orleans, Louisiana:\nAssociation for Computational Linguistics, Jun. 2018, pp. 1112‚Äì\n1122.\n[19] K. Knight and D. Marcu, ‚ÄúSummarization beyond sentence\nextraction: A probabilistic approach to sentence compression,‚Äù\nArtiÔ¨Åcial Intelligence, vol. 139, no. 1, pp. 91‚Äì107, 2002.\n[20] W. Che, Y. Zhao, H. Guo, Z. Su, and T. Liu, ‚ÄúSentence compression\nfor aspect-based sentiment analysis,‚Äù IEEE/ACM Transactions on\naudio, speech, and language processing, vol. 23, no. 12, pp. 2111‚Äì2124,\n2015.\n[21] Y. Kikuchi, G. Neubig, R. Sasano, H. Takamura, and M. Okumura,\n‚ÄúControlling output length in neural encoder-decoders,‚Äù in\nProceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing . Austin, Texas: Association for\nComputational Linguistics, Nov. 2016, pp. 1328‚Äì1338.\n[22] A. Fan, D. Grangier, and M. Auli, ‚ÄúControllable abstractive\nsummarization,‚Äù in Proceedings of the 2nd Workshop on Neural\nMachine Translation and Generation . Melbourne, Australia:\nAssociation for Computational Linguistics, Jul. 2018, pp. 45‚Äì54.\n[23] T. Fevry and J. Phang, ‚ÄúUnsupervised sentence compression using\ndenoising auto-encoders,‚Äù in Proceedings of the 22nd Conference\non Computational Natural Language Learning . Brussels, Belgium:\nAssociation for Computational Linguistics, Oct. 2018, pp. 413‚Äì422.\n[24] Y. Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey et al. , ‚ÄúGoogle‚Äôs neural\nmachine translation system: Bridging the gap between human and\nmachine translation,‚Äù arXiv preprint arXiv:1609.08144, 2016.\n[25] J. Gu, J. Bradbury, C. Xiong, V . O. Li, and R. Socher,\n‚ÄúNon-autoregressive neural machine translation,‚Äù in International\nConference on Learning Representations, 2018.\n[26] A. M. Rush, S. Chopra, and J. Weston, ‚ÄúA neural attention model\nfor abstractive sentence summarization,‚Äù in Proceedings of the 2015\nConference on Empirical Methods in Natural Language Processing .\nLisbon, Portugal: Association for Computational Linguistics, Sep.\n2015, pp. 379‚Äì389.\n[27] B. Hu, Q. Chen, and F. Zhu, ‚ÄúLCSTS: A large scale Chinese short\ntext summarization dataset,‚Äù in Proceedings of the 2015 Conference\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 17\non Empirical Methods in Natural Language Processing . Lisbon,\nPortugal: Association for Computational Linguistics, Sep. 2015,\npp. 1967‚Äì1972.\n[28] S. Chopra, M. Auli, and A. M. Rush, ‚ÄúAbstractive sentence\nsummarization with attentive recurrent neural networks,‚Äù in\nProceedings of the 2016 Conference of the North American\nChapter of the Association for Computational Linguistics: Human\nLanguage Technologies . San Diego, California: Association for\nComputational Linguistics, Jun. 2016, pp. 93‚Äì98.\n[29] J. Cheng and M. Lapata, ‚ÄúNeural summarization by extracting\nsentences and words,‚Äù in Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers).\nBerlin, Germany: Association for Computational Linguistics, Aug.\n2016, pp. 484‚Äì494.\n[30] R. Nallapati, B. Zhou, C. dos Santos, C ¬∏ . Guehre, and B. Xiang,\n‚ÄúAbstractive text summarization using sequence-to-sequence\nRNNs and beyond,‚Äù in Proceedings of The 20th SIGNLL Conference\non Computational Natural Language Learning . Berlin, Germany:\nAssociation for Computational Linguistics, Aug. 2016, pp. 280‚Äì\n290.\n[31] X. Duan, M. Yin, M. Zhang, B. Chen, and W. Luo, ‚ÄúZero-shot cross-\nlingual abstractive sentence summarization through teaching\ngeneration and attention,‚Äù in Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics . Florence, Italy:\nAssociation for Computational Linguistics, Jul. 2019, pp. 3162‚Äì\n3172.\n[32] C. Napoles, M. Gormley, and B. Van Durme, ‚ÄúAnnotated Giga-\nword,‚Äù in Proceedings of the Joint Workshop on Automatic Knowledge\nBase Construction and Web-scale Knowledge Extraction (AKBC-\nWEKEX). Montr ¬¥eal, Canada: Association for Computational\nLinguistics, Jun. 2012, pp. 95‚Äì100.\n[33] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay,\nM. Suleyman, and P . Blunsom, ‚ÄúTeaching machines to read and\ncomprehend,‚Äù in Advances in neural information processing systems ,\n2015, pp. 1693‚Äì1701.\n[34] H. Jing, ‚ÄúUsing hidden Markov modeling to decompose human-\nwritten summaries,‚Äù Computational Linguistics , vol. 28, no. 4, pp.\n527‚Äì543, 2002.\n[35] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, ‚ÄúMass: Masked\nsequence to sequence pre-training for language generation,‚Äù in\nInternational Conference on Machine Learning, 2019, pp. 5926‚Äì5936.\n[36] O. Vinyals, M. Fortunato, and N. Jaitly, ‚ÄúPointer networks,‚Äù in\nAdvances in neural information processing systems , 2015, pp. 2692‚Äì\n2700.\n[37] P . Rajpurkar, R. Jia, and P . Liang, ‚ÄúKnow what you don‚Äôt know:\nUnanswerable questions for SQuAD,‚Äù in Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers) . Melbourne, Australia: Association for\nComputational Linguistics, Jul. 2018, pp. 784‚Äì789.\n[38] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,\nO. Levy, V . Stoyanov, and L. Zettlemoyer, ‚ÄúBart: Denois-\ning sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension,‚Äù arXiv preprint\narXiv:1910.13461, 2019.\n[39] C.-Y. Lin, ‚ÄúROUGE: A package for automatic evaluation of\nsummaries,‚Äù in Text Summarization Branches Out . Barcelona,\nSpain: Association for Computational Linguistics, Jul. 2004, pp.\n74‚Äì81.\n[40] Y. Wang and H.-Y. Lee, ‚ÄúLearning to encode text as human-\nreadable summaries using generative adversarial networks,‚Äù\nin Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing . Brussels, Belgium: Association for\nComputational Linguistics, Oct.-Nov. 2018, pp. 4187‚Äì4195.\n[41] P . Vincent, H. Larochelle, Y. Bengio, and P .-A. Manzagol,\n‚ÄúExtracting and composing robust features with denoising\nautoencoders,‚Äù in Proceedings of the 25th international conference on\nMachine learning, 2008, pp. 1096‚Äì1103.\n[42] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin,\n‚ÄúConvolutional sequence to sequence learning,‚Äù in Proceedings of\nthe 34th International Conference on Machine Learning-Volume 70 .\nJMLR. org, 2017, pp. 1243‚Äì1252.\n[43] R. Sennrich, B. Haddow, and A. Birch, ‚ÄúNeural machine\ntranslation of rare words with subword units,‚Äù in Proceedings\nof the 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). Berlin, Germany: Association\nfor Computational Linguistics, Aug. 2016, pp. 1715‚Äì1725.\n[44] B. Yang, Z. Tu, D. F. Wong, F. Meng, L. S. Chao, and T. Zhang,\n‚ÄúModeling localness for self-attention networks,‚Äù in Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language\nProcessing. Brussels, Belgium: Association for Computational\nLinguistics, Oct.-Nov. 2018, pp. 4449‚Äì4458.\n[45] P . Rajpurkar, J. Zhang, K. Lopyrev, and P . Liang, ‚ÄúSQuAD:\n100,000+ questions for machine comprehension of text,‚Äù in\nProceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing . Austin, Texas: Association for\nComputational Linguistics, Nov. 2016, pp. 2383‚Äì2392.\n[46] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, ‚ÄúRACE: Large-\nscale ReAding comprehension dataset from examinations,‚Äù in\nProceedings of the 2017 Conference on Empirical Methods in Natural\nLanguage Processing . Copenhagen, Denmark: Association for\nComputational Linguistics, Sep. 2017, pp. 785‚Äì794.\n[47] A. See, P . J. Liu, and C. D. Manning, ‚ÄúGet to the point:\nSummarization with pointer-generator networks,‚Äù in Proceedings\nof the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) . Vancouver, Canada:\nAssociation for Computational Linguistics, Jul. 2017, pp. 1073‚Äì\n1083.\n[48] Y. Liu and M. Lapata, ‚ÄúText summarization with pretrained\nencoders,‚Äù in Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP) .\nHong Kong, China: Association for Computational Linguistics,\nNov. 2019, pp. 3730‚Äì3740.\n[49] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao,\nM. Zhou, and H.-W. Hon, ‚ÄúUniÔ¨Åed language model pre-training\nfor natural language understanding and generation,‚Äù in Advances\nin Neural Information Processing Systems, 2019, pp. 13 042‚Äì13 054.\n[50] J. Zhang, Y. Zhao, M. Saleh, and P . J. Liu, ‚ÄúPegasus: Pre-training\nwith extracted gap-sentences for abstractive summarization,‚Äù\narXiv preprint arXiv:1912.08777, 2019.\n[51] Y. Yan, W. Qi, Y. Gong, D. Liu, N. Duan, J. Chen, R. Zhang, and\nM. Zhou, ‚ÄúProphetnet: Predicting future n-gram for sequence-to-\nsequence pre-training,‚Äù arXiv preprint arXiv:2001.04063, 2020.\n[52] M. Collins, P . Koehn, and I. Ku Àácerov¬¥a, ‚ÄúClause restructuring for\nstatistical machine translation,‚Äù in Proceedings of the 43rd Annual\nMeeting of the Association for Computational Linguistics (ACL‚Äô05) .\nAnn Arbor, Michigan: Association for Computational Linguistics,\nJun. 2005, pp. 531‚Äì540.\n[53] S. Gururangan, A. Marasovi ¬¥c, S. Swayamdipta, K. Lo, I. Beltagy,\nD. Downey, and N. A. Smith, ‚ÄúDon‚Äôt stop pretraining: Adapt\nlanguage models to domains and tasks,‚Äù in Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics .\nOnline: Association for Computational Linguistics, Jul. 2020, pp.\n8342‚Äì8360.\n[54] Z. Zhang, J. Yang, and H. Zhao, ‚ÄúRetrospective reader for machine\nreading comprehension,‚Äù arXiv preprint arXiv:2001.09694, 2020.\n[55] F. Sun, L. Li, X. Qiu, and Y. Liu, ‚ÄúU-net: Machine reading\ncomprehension with unanswerable questions,‚Äù arXiv preprint\narXiv:1810.06638, 2018.\n[56] M. Hu, F. Wei, Y. Peng, Z. Huang, N. Yang, and D. Li, ‚ÄúRead+\nverify: Machine reading comprehension with unanswerable\nquestions,‚Äù in Proceedings of the AAAI Conference on ArtiÔ¨Åcial\nIntelligence, vol. 33, 2019, pp. 6529‚Äì6537.\n[57] P . Zhu, H. Zhao, and X. Li, ‚ÄúDual multi-head co-attention\nfor multi-choice reading comprehension,‚Äù arXiv preprint\narXiv:2001.09415, 2020.\n[58] Q. Ran, P . Li, W. Hu, and J. Zhou, ‚ÄúOption comparison\nnetwork for multiple-choice reading comprehension,‚Äù arXiv\npreprint arXiv:1903.03033, 2019.\n[59] M. Cettolo, N. Jan, S. Sebastian, L. Bentivogli, R. Cattoni,\nand M. Federico, ‚ÄúThe iwslt 2015 evaluation campaign,‚Äù in\nInternational Workshop on Spoken Language Translation, 2015.\n[60] A. Conneau, G. Kruszewski, G. Lample, L. Barrault, and\nM. Baroni, ‚ÄúWhat you can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties,‚Äù inProceedings of the\n56th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) . Melbourne, Australia: Association for\nComputational Linguistics, Jul. 2018, pp. 2126‚Äì2136.\n[61] T. Xiao, J. Zhu, and C. Zhang, ‚ÄúA hybrid approach to skeleton-\nbased translation,‚Äù in Proceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers) .\nBaltimore, Maryland: Association for Computational Linguistics,\nJun. 2014, pp. 563‚Äì568.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, MARCH 2020 18\n[62] T. Xiao, J. Zhu, C. Zhang, and T. Liu, ‚ÄúSyntactic skeleton-based\ntranslation,‚Äù in Thirtieth AAAI Conference on ArtiÔ¨Åcial Intelligence ,\n2016.\n[63] B. Mellebeek, K. Owczarzak, D. Groves, J. Van Genabith, and\nA. Way, ‚ÄúA syntactic skeleton for statistical machine translation,‚Äù\n2006.\n[64] K. Sudoh, K. Duh, H. Tsukada, T. Hirao, and M. Nagata, ‚ÄúDivide\nand translate: Improving long distance reordering in statistical\nmachine translation,‚Äù in Proceedings of the Joint Fifth Workshop\non Statistical Machine Translation and MetricsMATR . Uppsala,\nSweden: Association for Computational Linguistics, Jul. 2010, pp.\n418‚Äì427.\n[65] J. Pouget-Abadie, D. Bahdanau, B. van Merri ¬®enboer, K. Cho, and\nY. Bengio, ‚ÄúOvercoming the curse of sentence length for neural\nmachine translation using automatic segmentation,‚Äù in Proceedings\nof SSST-8, Eighth Workshop on Syntax, Semantics and Structure in\nStatistical Translation. Doha, Qatar: Association for Computational\nLinguistics, Oct. 2014, pp. 78‚Äì85.\n[66] E. Hasler, A. de Gispert, F. Stahlberg, A. Waite, and\nB. Byrne, ‚ÄúSource sentence simpliÔ¨Åcation for statistical machine\ntranslation,‚Äù Computer Speech & Language , vol. 45, pp. 221‚Äì235,\n2017.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8233769536018372
    },
    {
      "name": "Encoder",
      "score": 0.7451202869415283
    },
    {
      "name": "Transformer",
      "score": 0.7151384949684143
    },
    {
      "name": "Natural language processing",
      "score": 0.5835123062133789
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5534246563911438
    },
    {
      "name": "Language model",
      "score": 0.4641597867012024
    },
    {
      "name": "Question answering",
      "score": 0.45184046030044556
    },
    {
      "name": "Data compression",
      "score": 0.4439026713371277
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165726",
      "name": "Shanghai Municipal Education Commission",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I90023481",
      "name": "National Institute of Information and Communications Technology",
      "country": "JP"
    }
  ],
  "cited_by": 46
}