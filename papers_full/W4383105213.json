{
  "title": "AraBERTopic: A Neural Topic Modeling Approach for News Extraction from Arabic Facebook Pages using Pre-trained BERT Transformer Model",
  "url": "https://openalex.org/W4383105213",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5054471561",
      "name": "Nassera Habbat",
      "affiliations": [
        "University of Hassan II Casablanca"
      ]
    },
    {
      "id": "https://openalex.org/A5013033870",
      "name": "Houda Anoun",
      "affiliations": [
        "University of Hassan II Casablanca"
      ]
    },
    {
      "id": "https://openalex.org/A5034046389",
      "name": "Larbi Hassouni",
      "affiliations": [
        "University of Hassan II Casablanca"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6768084152",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2594155836",
    "https://openalex.org/W3045210825",
    "https://openalex.org/W2769501189",
    "https://openalex.org/W4320077570",
    "https://openalex.org/W4327723648",
    "https://openalex.org/W6765908918",
    "https://openalex.org/W6757993438",
    "https://openalex.org/W4362703898",
    "https://openalex.org/W4381568698",
    "https://openalex.org/W3112012747",
    "https://openalex.org/W6684152816",
    "https://openalex.org/W6784847355",
    "https://openalex.org/W3015202090",
    "https://openalex.org/W3038046198",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W1539309091",
    "https://openalex.org/W3046666383",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W2250533720",
    "https://openalex.org/W2972110483",
    "https://openalex.org/W2911250003",
    "https://openalex.org/W2952478253",
    "https://openalex.org/W2165279024",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W2955328590",
    "https://openalex.org/W3176380929",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3099531031",
    "https://openalex.org/W2962686197",
    "https://openalex.org/W3116641301"
  ],
  "abstract": "Topic modeling algorithms can better understand data by extracting meaningful words from text collection, but the results are often inconsistent, and consequently difficult to interpret.Enrich the model with more contextual knowledge can improve coherence.Recently, neural topic models have emerged, and the development of neural models, in general, was pushed by BERT-based representations.We propose in this paper, a model named AraBERTopic to extract news from Facebook pages.Our model combines the Pre-training BERT transformer model for the Arabic language (AraBERT) and neural topic model ProdLDA.Thus, compared with the standard LDA, pre-trained BERT sentence embeddings produce more meaningful and coherent topics using different embedding models.Results show that our AraBERTopic model gives 0.579 in topic coherence.",
  "full_text": "International Journal of Computing and Digital Systems\nISSN (2210-142X)\nInt. J. Com. Dig. Sys.14, No.1 (Jul-23)\nhttp://dx.doi.org/10.12785/ijcds/140101\nAraBERTopic: A Neural Topic Modeling Approach for News\nExtraction from Arabic Facebook Pages using Pre-trained\nBERT Transformer Model\nNassera HABBAT1, Houda ANOUN1 and Larbi HASSOUNI1\n1RITM Laboratory, CED ENSEM Ecole Superieure de Technologie Hassan II University, Casablanca, Morocco\nReceived 21 Jun. 2022, Revised 6 May. 2023, Accepted 7 May. 2023, Published 1 Jul. 2023\nAbstract: Topic modeling algorithms can better understand data by extracting meaningful words from text collection, but the\nresults are often inconsistent, and consequently di fficult to interpret. Enrich the model with more contextual knowledge can improve\ncoherence. Recently, neural topic models have emerged, and the development of neural models, in general, was pushed by BERT-based\nrepresentations. We propose in this paper, a model named AraBERTopic to extract news from Facebook pages. Our model combines\nthe Pre-training BERT transformer model for the Arabic language (AraBERT) and neural topic model ProdLDA. Thus, compared with\nthe standard LDA, pre-trained BERT sentence embeddings produce more meaningful and coherent topics using di fferent embedding\nmodels. Results show that our AraBERTopic model gives 0.579 in topic coherence.\nKeywords: Neural topic model, ProdLDA, AraBERT, Topic coherence.\n1. Introduction\nNowadays, because of the exponential development of\nthe Internet, a huge quantity of documents, such as online\nnews are produced every day, especially in social media\nlike Facebook which is one of the most important social\nnetworks in Africa. Concerning Morocco, there were 22\n010 000 Facebook users in January 2021 [1]. Mining the\nknowledge and topics from social media posts have attracted\na lot of attention these last years. To discover hidden topical\npatterns which are present in large collections of texts,\nmany unsupervised techniques are usually used to generate\nprobabilistic topic models. Among these models, we find\nNon-negative Matrix Factorization (NMF), Latent Semantic\nIndexing (LSI), and Latent Dirichlet Allocation (LDA) [2]\n.\nHowever, those probabilistic models do not take ad-\nvantage of language model pre-training benefits. Many\nextensions were proposed to integrate di fferent types of\ninformation and add contextual knowledge to the topic\nmodels. The most prominent architecture in this category is\nBidirectional Encoder Representations from Transformers\n(BERT) which allows us to extract representations from\npre-trained documents to easily reach state-of-the-art per-\nformance through numerous tasks [3].\nRecently, some neural topic models were explored and\nhave shown promising results. For instance, ProdLDA,\nwhich is a developed version of LDA based on deep\nlearning, is an expert products instead of mixture model\nin LDA to yield much more interpretable topics [4].\nThe main contribution of this paper is to present a\nproposed model (AraBERTopic) to extract topics from Ara-\nbic news published on Facebook pages using AraBERT as\nlanguage model pre-training and ProdLDA as a neural topic\nmodel. To prove the high performance of our model; we\ncompared, on the one hand, its feature extraction phase with\ndifferent embedding models (Glove, Doc2Vec, and Asafaya\nas a bert-base-arabic model [5]), and we compared, on the\nother hand, its topic model phase with standard LDA. The\nresults demonstrate that our proposed model is superior to\nother models in terms of Normalized topic coherence, Point-\nwise Mutual Information (NPMI), and perplexity metrics.\nThe rest of this paper is organized as follows: Section 2\nprovides a brief review of literature. The proposed model\nis described in 3rd Section. In section 4, we present the\nresults and discusses. Finally, the paper is summarized and\nthe future work has prospected.\n2. Related Works\nWe introduce in this part some topic extraction methods,\nincluding traditional feature extraction methods and deep\nlearning methods.\nProbabilistic topic models are much used in natural\nE-mail address: nassera.habbat@gmail.com, houda.anoun@gmail.com, lhassouni@hotmail.com http:// journals.uob.edu.bh\n2\n Nassera Habbat, et al.: AraBERTopic: A Neural Topic Modeling Approach for News Extraction ...\nlanguage processing (NLP), and among the most commonly\nused methods, we find LDA. In [6]; the author reviews the\nacademic papers on LDA topic modeling published from\n2003 to 2016. In [7], the authors examine the feedback\non Duolingo, a free and enjoyable language-learning pro-\ngram. They employed LDA to figure out the points that\nconsumers bring up and to create a rough outline for both\nsoftware developers and people looking for apps that are\ncomparable based on these points. Similarly, the authors\nin [8] examined the content of startups using LDA to see\nhow their communication approaches can change as they\nscale. They found that the contents may be categorized into\nfive different topics using data from Twitter as a source of\ninformation.\nTo imitate the statistical process of LDA, the authors\nin [9] investigate the possibility of using deep neural\nnetwork to model the statistical process to minimize the\ncomputational time in LDA; Therefore, they proposed two\ndeep neural network variants: two and three Neural Network\n(NN) DeepLDA, in their experiments they used Reuters-\n21578 as a dataset, and some standard libraries in Python\nlike genism, NLTK and Keras and to record the accuracy\nof the models, a Support Vector Classifier (SVC) was used.\nTheir results showed that 3NN DeepLDA outperforms 2NN\nDeepLDA and LDA.\nIn recent years, deep learning has become a powerful\nmachine learning technology, which allows learning multi-\nlevel representation and several methods exist that are\nparticularly adapted for learning meaningful hidden repre-\nsentations. Among those models, we find the Variational\nAutoencoder (V AE) which is a deep generative model.\nIn [4] the authors present the first e fficient autoencoding\nvariational Bayes (AEVB) which is an inference method\nbased on latent Dirichlet allocation (LDA)-(A VITM), in this\npaper, they proposed a novel topic model named ProdLDA\nwhich replace the hybrid model in LDA with expert prod-\nucts, After applying their model on 20 Newsgroup dataset\nthey obtained that A VITM outperforms baseline methods\nin term of accuracy and reasoning time, and the topics\ngiven by ProdLDA are more explanatory, Similarly, the\nauthors in [10] presented Neural Variational Correlated\nTopic Model consisting of two main parts; the 1st one\nis the inference network with Centralized Transformation\nFlow and the 2nd one is the multinomial softmax generative\nmodel. To evaluate their model they used NPMI topic\ncoherence. Their results showed that the model enhances the\nperformance of topic modeling and can e ffectively capture\ntopic correlation. However, the authors in [11] used LDA\nto identify the topics of discussion in Tweets, resulting\nin a directed multilayer network in which users (in one\nlayer) are linked to discussions and topics (in a second\nlayer) in which they contributed, with interlayer connections\nindicating user participation in discussions. Although there\nare other methods for topic modeling on short texts, such\nas the Biterm Topic Model. The authors in [12] used\nthis technique with part-of-speech tagging on noun-only\nto analyze the public mapping review as a data source\nregarding hospitals in order to identify the topics in the\nreview with a low score so that it could be used as a\nsuggestion for enhancing health services.\nOnly some recent studies have used semantic em-\nbeddings like Bidirectional Encoder Representations from\nTransformers (BERT) and ELMO (Embeddings from Lan-\nguage Models) in topic analysis. In [13] the authors pro-\nposed Variational Auto-Encoder Topic Model (V AETM)\nwhich combines entity vector representation and word\nvector representation. The model uses large-scale external\ncorpus and manually edited large-scale knowledge graph\nto learn the embedding representation of each word and\nentity, then, those embedding representations are integrated\ninto the V AE framework to deduct the hidden representation\nof topic distributions; To prove the performance of their\nmodel they compared it with various of baseline algorithms\n(LDA, Sparse Additive Generative Model (SAGE) [14] and\nSCHOLAR [15]), and 20Newsgroups. IMDB and Chinese\nStandard Literature (96,000 national and industry standards\nof China) using as datasets; based on perplexity, NPMI,\nand accuracy measures; they showed that the model better\nmine the hidden semantic of short texts and improve topic\nmodeling.\nMany researchers opt for BERT [3] as contextualized\nword representations because it progresses the state of the\nart for di fferent NLP tasks by pushing the MultiNLI accu-\nracy up to 86%, score to 80.5% for the General Language\nUnderstanding Evaluation (GLUE), the Stanford Question\nAnswering Dataset (SQuAD v1.1) question answering Test\nF1-score to 93.2 and Test F1-score to 83.1 for SQuAD v2.0,\ncompared to Glove, ELMOs, and OpenAI GPT; Among\nNLP tasks we found Topic modeling. The authors in [16]\ninclude in a neural topic model, the contextualized BERT\nembeddings to get more consistent topics compared to\nNeural-ProdLDA, NVDM, and LDA.\nMoreover, BERT creates word embeddings in multiple\nlanguages, in [17] the authors used LDA topic model\nand multilingual pre-trained BERT embeddings to analyze\nthe evolution of topics in Chinese, English, and multi-\nlingual in scientific publications using Google-pre-trained\nBERT models: “bert-base-chinese” for Chinese, “bert-base-\nuncased” for English and ¨bert-base-multilingual-uncased”\nfor multilingual text. The results showed that the model can\nwell analyze the scientific evolution of similar relationships\nbetween monolingual and multilingual disciplines. In most\ncases, 80% of the relationships are related to the key topics\nof each language.\nCompared with English, Arabic is a language with\nrich forms, less syntactic exploration, and fewer resources.\nThe pre-trained AraBERT model [18] is very e ffective in\nlanguage understanding compared with multilingual BERT.\nIt achieves the most advanced performance in most Arabic\nNLP tasks.\nhttp:// journals.uob.edu.bh\nInt. J. Com. Dig. Sys. 14, No.1, 1-8 (Jul-23)\n 3\n3. Proposed approach\nWe will describe in this part, our model (AraBERTopic),\nthe global architecture is exposed in Figure 1 . There are\nfour principal components:\n(1) Data Acquisition: This component aims to collect\ndata from Facebook pages using web scraping;\n(2) Pre-training model using AraBERT and extracting\nthe features.\n(3) Extraction of topics using Neural Prod-LDA\n(4) Evaluation of model comparing it with baselines\nmodels.\nFigure 1. The global architecture of AraBERTopic model.\nA. Word Embedding\nAraBERT [18] is a pre-training BERT transformer\nmodel for the Arabic language. It uses Transformer to\nlearn the context between words (or sub-words) in texts.\nThe Transformer encoder is considered to be bidirectional,\nwhich means that it reads the whole word sequence at one\ntime, instead of reading the text input-oriented model in\norder (from right to left or from left to right).\nThe input is a token sequence, which is first embedded\ninto a vector and then processed in the neural network to\nobtain a vector sequence as output. As shown in Figure 2,\n[CLS] is a special symbol added before each sample input\nand [SEP] is a special separator mark. Generally, the\nembedding models include the following three layers:\n• The token embeddings layer changes each word tag\ninto a 768-dimensional vector representation.\n• The segment embeddings layer has two representations\nVector: the 1st vector (index 0) is attributed to all tokens\nof input 1, and the last vector (index 1) is attributed to all\ntokens of input 2.\n• The Position embeddings layer: AraBERT is designed\nto process up to 512 input sequences. Therefore, AraBERT\nmust learn a vector representation of each position. This\nmeans that the position integration layer is a size look-up\ntable (512, 768) where :\n- The 1st row is the representation of the vector of each\nword in position 1.\n- The 2nd row is the representation of the vector of each\nword in position 2, etc.\nThese representations are added in a single representa-\ntion by the elements in the generated form (1, n, 768). The\nfollowing Figure shows the input representation passed to\nthe AraBERT encoder layer.\nFigure 2. AraBERT input representation.\nThe learning of AraBERT takes place in two phases:\nThe 1st one is the pre-training, it is done only once, it\nallows the creation of a neural network that has a certain\ngeneral understanding of the language. Then, the 2nd phase\nis called the fine-tuning phase, which allows the network to\nbe trained on a specific task like classification, question\nanswering, and Topic modeling as shown in Figure 3.\nFigure 3. General pre-training and fine-tuning processes for\nBERT [3].\nConcerning Pre-training Dataset, it was scraped from\nArabic news websites for articles. In addition there are two\nmajor publicly accessible Arabic corpora:\n- Open Source International Arabic News (OSLAN)\nCorpus [18] composed of 3.5 million articles (about 1\nbillion tokens) from 31 news sources in 24 Arab countries,\n- An Arabic corpus of 1.5 billion word [17], which is\na contemporary corpus composed of more than 5 million\narticles from ten major information sources in 8 countries,\nAfter deleting the repeated sentences, the final size of\nthe pre-training dataset in AraBERT is 70 million phrases\n(about 24 GB), which can represent a large number of topics\nhttp:// journals.uob.edu.bh\n4\n Nassera Habbat, et al.: AraBERTopic: A Neural Topic Modeling Approach for News Extraction ...\ndiscussed in news from di fferent Arab regions. AraBERT\nwas assessed on three tasks concerning Arabic language\nunderstanding: Named Entity Recognition, Sentiment Anal-\nysis, and Question Answering [12]. In our work, we used\nAraBERT in the Topic modeling task to enrich the repre-\nsentations and provide a significant augmentation in topic\ncoherence by adding to neural topic models, the contextual\ninformation.\nB. Neural Topic Model\nProdLDA [4] solved the problem of p(w|ϕ,γ) (a mixture\nof multinomials) distribution of LDA, which consists of\nnever making predictions that are more precise than the\nmixed components. This led to low-quality subjects and\npeople’s judgment is not consistent. The way to resolve\nthis problem is by using the weighted product of experts\nto convert mixed words into word level. According to the\ndefinition, the weighted product of experts can make clearer\npredictions than any combination of experts. ProdLDA\nuses the weighted product of experts to replace the mixed\nword hypothesis in LDA [19], which greatly improves the\nconsistency of topics.\nProdLDA employed a V AE for LDA using a Laplace\napproximation for the Dirichlet distribution, which makes\nit possible to train a Dirichlet variational autoencoder.\nMoreover, this model does not directly reparametrize the\nDirichlet distribution.\nTo successfully apply V AE to LDA, an encoder network\nis used that approximates the Dirichlet prior p(w|ϕ,γ) with\na softmax-normal distribution LN. In other words:\np(ϕ,γ)p(ϕ|µ,Σ) = LN(ϕ|µ,Σ) (1)\nWhere µ and Σ are the encoder network outputs. In\naddition, the Adam optimizer, batch normalization, and\ndropout units in the encoder network are used to component\ncollapse. The only modifications to pass from LDA to\nProdLDA are that ProdLDA is not normalized, and the\nconditional distribution of wm is interpreted as wm|γ,ϕ ∼\nMultinomial(1,σ(γϕ)).\nFor the multinomial, the relation to an expert products\nis very simple; if we take 2 N-dimensional multinomials\nparameterized by mean vectors p and q and set the natural\nparameters to p = σ(v) and q = σ(r), we can show that\n(where δ∈[0,1] ):\nP (x|δv+ (1 −δ) r) = α\nNY\ni=1\nσ(δvi + (1 −δ) ri )xi\nNY\ni=1\n[vδ\ni . r(1−δ)\ni ]\nxi\n(2)\nWhere the notation α(β) means applying the softmax\nfunction separately to each column of the matrix β and δ\nrepresents the output of each network is a vector in RK .\nIn brief; the used approach trains an encoding neural net-\nwork to map pre-trained contextualized word embeddings\n(AraBERT) to latent representations which are variably\nsampled from a Gaussian distribution and transmitted to a\nnetwork of decoders. This network of decoders must rebuild\nthe bag-of-words representation of the document.\n4. EXPERIMENTS AND RESULTS\nWe will describe in this part, our dataset (collect and\npreprocessing), then we will present baselines and used\nmetrics to compare our model with baseline methods.\nA. Dataset\nFor almost 10 years, the Application Programming In-\nterface (API) provided by Facebook has been the primary\ntool for researchers to collect data on Facebook. These\ndata contain public information about user profiles, com-\nments and reactions to public messages. However, after the\nCambridge Analytics (CA) scandal in early 2018, Facebook\nsignificantly tightened access to its API [20].\nTo pull data from Facebook pages, we used web scrap-\ning techniques with Python language (Version 3.8) namely\nRequests (Version 2.25.0) and BeautifulSoup4 (Version\n4.9.3) as external libraries for automatic browsing. In our\nstudy, we were interested in Arabic posts published on Face-\nbook by Moroccan news pages. We chose seven Facebook\npages (Hespress, Medi1TV , aljarida24.ma, alakhbar.maroc,\nAlyaoum24, JARIDATACHCHAAB, al3omk), and col-\nlected 81 598 posts published from 04 October 2020 to 05\nMarch 2021 (details of collected posts are shown in Table I)\nTABLE I. DATA COLLECTION\nFacebook pages Number of collected posts\nHespress 22 854\nMedi1TV 18 176\naljarida24.ma 8 133\nalakhbar.maroc 6 400\nAlyaoum24 15 489\nJARIDATACHCHAAB 1 571\nal3omk 8 975\nTotal 81 598\nB. Data preprocessing\nWe performed the common pre-processing steps in\nexisting approaches which consist of:\n- Removal of Arabic stopwords.\n- Removing hyperlinks, hashes to keep only Arabic text.\n- Lemmatization of words using Farasapy Lemma-\ntizer [21] for Arabic text.\n- Selecting the most frequent 2,000 words as the vocab-\nulary\nhttp:// journals.uob.edu.bh\nInt. J. Com. Dig. Sys. 14, No.1, 1-8 (Jul-23)\n 5\nC. Word embedding\nIt designates a set of learning methods in NLP where\nvocabulary words (or phrases) are converted to numerical\nvectors. In our research, we used BERT word embeddings.\nAraBERT is an Arabic pre-trained language model that\ngives a contextual embeddings, which is used to generate\nword embeddings. Each word is interpreted as a vector of\nsize 768 and consequently each sentence is a list of word\nembeddings are extracted. The sequence of the embeddings\nis completed so that they have the same size. For our\nAraBERTopic model, we used the pretrained bert-base-\narabert Arabic embedding with 12 encoder blocks /layer,\n768 hidden dimensions, 12 attention heads, and 110 M\nparameters. It can be found on the Google Bert model\nwebsite [22].\nD. LDA and ProdLDA settings\nFigure 4. Choosing the optimal number of LDA and ProdLDA\ntopics.\nConcerning LDA and prodLDA parameters, we selected\ntwenty topics and the top-ten words for each topic. For the\nnumber of topics (N), we tested N values from 5 to 80.\nWhen the N value was 20, the results were good with the\nhighest topic coherence value as shown in Figure 4.\nE. Experimental Setup\nIn our experiments, we used the implementation of\nAraBERTopic in Pytorch Library (Version 1.6.0). Adam\noptimizer was used, with a batch size of 64 and 2e-3 for\nthe learning rate. Our model was fine-tuned after 10 epochs\nover the data.\nF . Baselines\nWe compared the two levels of our approach (feature\nextraction and topic modeling) to other models to prove its\nperformance. Concerning feature extraction level, we com-\npared AraBERT Embedding with three embedding models:\n- asafaya/bert-base-arabic model [5]: This model is con-\ntextual, it was pretrained on 8.2 Billion words. Glove [23]:\nThe model gives the vector representation of words using\nan unsupervised learning method. We choose 60% of the\ndata set to train the model, because it captures the overall\nmeaning of sentences in a relatively small memory.\n- Doc2vec [24]: This model converts sentences or para-\ngraphs to numeric vectors. In our work, we used Doc2vec\nGensim implementation. To train our Doc2vec model, we\nused the same dataset as for the Glove model. Concerning\nthe second level of topic modeling, we compared ProdLDA\nto LDA used with different word embedding models already\nmentioned.\nG. Metrics\nTo assess the performance of the topic model is usually\nusing the following metrics:\n1. Subject coherence based on NPMI algorithm,\n2. Topic coherence measure,\n3. Confusion or Perplexity evaluation.\n1) NPMI\nThis metric [25] gives an automatic measure of the\nquality of the topics to evaluate our proposed model as well\nas baselines. It derives from Pointwise Mutual Information\n(PMI) and measures the e ffect of one xmvariable on another\nxn . Its formal definition is as follows:\nPMI (xm,xn) = log p(xm,xn)\np(xm)p(xn) (3)\np(xn) : The likelihood that the word xn appearing in the\ncorpus,\np(xm,xn) : The likelihood that the word xm and word xn\nappear together in the corpus.\nWe took in our experiment the top-five words of each\ntopic, and for each word; we compute the NPMI score\nfollowing this equation:\nNPMI =\njX\nm=1\njX\nn=m+1\nPMI (xm,xn)\n−log P(xm,xn) (4)\nThe topics that scored higher in NPMI are the most\nlikely words to seem more often in the same document m\nthan those who occasionally appeared.\n2) Topic coherence\nTopic coherence measure is also derived from PMI,\nwhich is used to evaluate the semantic similarity between\nhigh-resolution words of a topic. Topic coherence score is\nhttp:// journals.uob.edu.bh\n6\n Nassera Habbat, et al.: AraBERTopic: A Neural Topic Modeling Approach for News Extraction ...\ncomputed as follows:\nS coreUCI (xi,xj) = log p(xi,xj) + ε\np(xi)p(xj) (5)\n3) Perplexity\nPerplexity (PPL) is a statistical measure used to assess\nthe quality of model subject modeling. It is computed as\nfollows:\nPerplexity (w|z,θ,β ) = exp (−PM\nm=1\nPNm\nn=1 logp(wmn|zmn,θm,β)\nPM\nm=1 Nm\n)\n(6)\nWhere: Nm : The number of words in the document M.\nθ : Document-topic density\nβ : Topic-word density.\nH. Results analysis\nIn our experiments, we used experimental parameter\nsummarized in the following Table II:\nTABLE II. SETTINGS OF OUR IMPLEMENTED MODEL\nParameter Value\nN Components 20\nTopic Prior Mean 0.0\nTopic Prior Variance 0.95\nBatch size 64\nNum epochs 10\nHidden Sizes (100, 100)\nActivation softplus\nSolver or optimizer Adam\nDropout 0.2\nLearning Rate 0.002\nMomentum (momentum to use for training) 0.99\nReduce On Plateau (reduce learning rate by\n10x on plateau of 10 epochs) False\nThose parameters are used to implement our AraBER-\nTopic using contextualized-topic-models Python library\n[16].\n1) Quantitative Evaluation\nWe calculated NPMI, topic coherence, and perplexity\nmeasure of each model to better compare their performance\nwith di fferent embedding models on short Arabic text.\nAs can be seen in Figure 5, the NPMI of AraBERTopic\n(AraBERT + ProdLDA) is 0.553, which is higher than\nthe other models. Overall, our model outperforms baselines\nmethods in terms of topic coherence value, perplexity score\nas shown in Table III.\n2) Qualitative evaluation\nTo prove the quality of the topics extracted by our\nmodels using ProdLDA which has the highest score within\nTABLE III. EV ALUATION OF PERFORMANCE OF TOPIC\nMODELS PRODLDA AND LDA WITH DIFFERENT EMBED-\nDING MODELS.\nModels Metrics\nWord\nEmbedding Topic Model NPMI CV PPL\nAraBERT ProdLDA 0.553 0.579 11.25\nLDA 0.137 0.497 20.65\nAsafaya ProdLDA 0.484 0.543 56.20\nLDA 0.128 0.494 61.28\nDoc2Vec ProdLDA 0.358 0.482 63.79\nLDA -0.16 0.477 78.63\nGlove ProdLDA 0.333 0.534 86.9\nLDA 0.109 0.434 90\nFigure 5. NPMI of di fferent models.\ndifferent embedding models as shown in Table III, we\ndisplay, in Table IV , examples of the top-six words of three\ntopics from all the models. We have added the translation\nof each word in English.\nThe topics generated using AraBERTopic are more\ncoherent than those generated by other models. In the\nsecond position, we find Asafaya’s topics which sound to be\ncoherent, but are influenced by additional mixed topics; for\nexample, the 1st topic is about Coronavirus but it includes\nthe term ‘Geographic’ which is a bit far from the topic.\nhttp:// journals.uob.edu.bh\nInt. J. Com. Dig. Sys. 14, No.1, 1-8 (Jul-23)\n 7\nTABLE IV. TOP SIX-WORDS OF THREE TOPICS EXTRACTED\nBY ALL THE MODELS..\nFigure 6. Topics sampled by AraBERTopic.\nFinally, we present in Figure 6 the wordcloud of the first\nsix topics extracted by our model AraBERTopic.\n5. CONCLUSION AND FUTURE WORK\nWe proposed in this paper a contextualized and neural\nAraBERT Topic Model (we named AraBERTopic) which\nintegrates contextual knowledge to the neural topic model\nto capture more coherent and meaningful topics published\nin pages on the net.\nFor that, we collected 81 598 Arabic posts from Face-\nbook pages of 7 Moroccan electronic press newspapers, then\nwe preprocessed our dataset and extracted features using\ndifferent embeddings models (Glove, Doc2Vec, and Asafaya\n– Arabic BERT). Finally, we extracted hidden topics in\nthese pages using ProdLDA as a neural topic model and\nstandard LDA.\nTo verify our contributions quantitatively, we performed\nexperiments in terms of perplexity, topic coherence, and\nNPMI measures. The results proved that our proposed\nmodel can e ffectively capture meaningful topics and en-\nhance the performance of topic modeling.\nAs part of our future work, we aim to enrich our\nArabBERTopic model with new components to apply it to\nother tasks such as sentiment analysis using di fferent deep\nlearning algorithms (CNN, LSTM . . . ).\nReferences\n[1] “Social Media users in Morocco - January 2021.” [Online]. Avail-\nable: https: //napoleoncat.com/stats/social-media-users-in-morocco /\n2021/01\n[2] S. K. Ray, A. Ahmad, and C. A. Kumar, “Review and\nImplementation of Topic Modeling in Hindi,” Applied Artificial\nIntelligence, vol. 33, no. 11, pp. 979–1007, Sep. 2019. [Online].\nAvailable: https://www.tandfonline.com/doi/full/10.1080/08839514.\n2019.1661576\n[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of Deep Bidirectional Transformers for Language\nUnderstanding,” arXiv:1810.04805 [cs] , May 2019, arXiv:\n1810.04805. [Online]. Available: http: //arxiv.org/abs/1810.04805\n[4] A. Srivastava and C. Sutton, “Autoencoding Variational Inference\nFor Topic Models,” arXiv:1703.01488 [stat] , Mar. 2017, arXiv:\n1703.01488. [Online]. Available: http: //arxiv.org/abs/1703.01488\n[5] A. Safaya, M. Abdullatif, and D. Yuret, “KUISAIL at SemEval-\n2020 Task 12: BERT-CNN for O ffensive Speech Identification in\nSocial Media,” in Proceedings of the Fourteenth Workshop on\nSemantic Evaluation. Barcelona (online): International Committee\nfor Computational Linguistics, Dec. 2020, pp. 2054–2059.\n[6] H. Jelodar, Y . Wang, C. Yuan, X. Feng, X. Jiang, Y . Li, and\nL. Zhao, “Latent Dirichlet allocation (LDA) and topic modeling:\nmodels, applications, a survey,” Multimedia Tools and Applications,\nvol. 78, no. 11, pp. 15 169–15 211, Jun. 2019. [Online]. Available:\nhttp://link.springer.com/10.1007/s11042-018-6894-4\n[7] M. Polatgil, “Analyzing comments made to the duolingo\nmobile application with topic modeling,” International Journal of\nComputing and Digital Systems , vol. 13, no. 1, pp. 223–230, Jan.\n2023. [Online]. Available: https: //doi.org/10.12785/ijcds/130118\n[8] A. R. Peixoto, A. de Almeida, N. Ant ´onio, F. Batista, and R. Ribeiro,\n“Diachronic profile of startup companies through social media,”\nSocial Network Analysis and Mining , vol. 13, no. 1, Mar. 2023.\n[Online]. Available: https://doi.org/10.1007/s13278-023-01055-2\n[9] M. R. Bhat, M. A. Kundroo, T. A. Tarray, and B. Agarwal, “Deep\nLDA : A new way to topic model,” Journal of Information and\nOptimization Sciences , vol. 41, no. 3, pp. 823–834, Apr. 2020.\n[Online]. Available: https: //www.tandfonline.com/doi/full/10.1080/\n02522667.2019.1616911\nhttp:// journals.uob.edu.bh\n8\n Nassera Habbat, et al.: AraBERTopic: A Neural Topic Modeling Approach for News Extraction ...\n[10] L. Liu, H. Huang, Y . Gao, X. Wei, and Y . Zhang, “Neural Variational\nCorrelated Topic Modeling,” p. 11.\n[11] A. P. Logan, P. M. LaCasse, and B. J. Lunday, “Social\nnetwork analysis of twitter interactions: a directed multilayer\nnetwork approach,” Social Network Analysis and Mining , vol. 13,\nno. 1, Apr. 2023. [Online]. Available: https: //doi.org/10.1007/\ns13278-023-01063-2\n[12] M. Makruf and A. Bramantoro, “Public hospital review on\nmap service with part of speech tagging and biterm topic\nmodeling,” International Journal of Computing and Digital\nSystems, vol. 13, no. 1, pp. 1097–1106, Apr. 2023. [Online].\nAvailable: https://doi.org/10.12785/ijcds/130188\n[13] X. Zhao, D. Wang, Z. Zhao, W. Liu, C. Lu, and F. Zhuang,\n“A neural topic model with word vectors and entity vectors\nfor short texts,” Information Processing & Management, vol. 58,\nno. 2, p. 102455, Mar. 2021. [Online]. Available: https:\n//linkinghub.elsevier.com/retrieve/pii/S030645732030947X\n[14] J. Eisenstein, A. Ahmed, and E. P. Xing, “Sparse Additive Gener-\native Models of Text,” p. 8.\n[15] D. Card, C. Tan, and N. A. Smith, “Neural Models for Documents\nwith Metadata,” Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 2031–2040, 2018, arXiv: 1705.09296. [Online].\nAvailable: http://arxiv.org/abs/1705.09296\n[16] F. Bianchi, S. Terragni, and D. Hovy, “Pre-training is a Hot Topic:\nContextualized Document Embeddings Improve Topic Coherence,”\narXiv:2004.03974 [cs] , Apr. 2020, arXiv: 2004.03974. [Online].\nAvailable: http://arxiv.org/abs/2004.03974\n[17] Q. Xie, X. Zhang, Y . Ding, and M. Song, “Monolingual and\nmultilingual topic analysis using LDA and BERT embeddings,”\nJournal of Informetrics , vol. 14, no. 3, p. 101055, Aug.\n2020. [Online]. Available: https: //linkinghub.elsevier.com/retrieve/\npii/S1751157719305127\n[18] W. Antoun, F. Baly, and H. Hajj, “AraBERT: Transformer-based\nmodel for Arabic language understanding,” pp. 9–15, May 2020.\n[Online]. Available: https://aclanthology.org/2020.osact-1.2\n[19] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants,\nP. Koehn, and T. Robinson, “One Billion Word Benchmark\nfor Measuring Progress in Statistical Language Modeling,”\narXiv:1312.3005 [cs] , Mar. 2014, arXiv: 1312.3005. [Online].\nAvailable: http://arxiv.org/abs/1312.3005\n[20] M. Mancosu and F. Vegetti, “What You Can Scrape and\nWhat Is Right to Scrape: A Proposal for a Tool to Collect\nPublic Facebook Data,” Social Media + Society, vol. 6,\nno. 3, p. 205630512094070, Jul. 2020. [Online]. Available:\nhttp://journals.sagepub.com/doi/10.1177/2056305120940703\n[21] MagedSaeed, “farasapy: A Python Wrapper for the well\nFarasa toolkit.” [Online]. Available: https://github.com/MagedSaeed/\nfarasapy\n[22] “google-research /bert,” Apr. 2021, original-date: 2018-\n10-25T22:57:34Z. [Online]. Available: https: //github.com/\ngoogle-research/bert\n[23] J. Pennington, R. Socher, and C. Manning, “GloVe: Global Vectors\nfor Word Representation,” in Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP) .\nDoha, Qatar: Association for Computational Linguistics, Oct. 2014,\npp. 1532–1543.\n[24] Q. V . Le and T. Mikolov, “Distributed Representations of\nSentences and Documents,” arXiv:1405.4053 [cs] , May 2014,\narXiv: 1405.4053 version: 2. [Online]. Available: http: //arxiv.org/\nabs/1405.4053\n[25] J. H. Lau, D. Newman, and T. Baldwin, “Machine Reading\nTea Leaves: Automatically Evaluating Topic Coherence and Topic\nModel Quality,” in Proceedings of the 14th Conference of the\nEuropean Chapter of the Association for Computational Linguistics.\nGothenburg, Sweden: Association for Computational Linguistics,\nApr. 2014, pp. 530–539.\nMrs. Nassera HABBAT PhD student in\nHassan II University Morocco, obtained his\nMaster degree in Information Systems En-\ngineering from Caddi Ayyad University, and\nLicence degree in Technology and Web Pro-\ngramming from the same University in 2015\nand 2013 respectively. Her research interest\nare: Big data and Machine learning.\nDr. Houda ANOUNreceived her engineer-\ning degree in Software Engineering from\nENSEIRB Bordeaux in 2003 and her PhD\nin computational linguistics from Bordeaux\nI University in 2007. And actually she is\na professor in the Computer Science De-\npartment at Ecole Sup´erieure de Technologie\n(Hassan II University), where she has been\nsince 2009. Her current research interest lie\nin the area of IA especially deep learning,\nmachine learning, and Big Data.\nDr. Larbi Hassouni got his engineer de-\ngree in 1983 from the “ ´Ecole Centrale de\nMarseille”. He prepared his Phd degree in\n1987 at the University of Aix Marseille III\nin France. Among his research work, there is\nthe development of a list unification software\nwith LeLisp language of INRIA in order to\ncontribute to the realization of the inference\nengine of an expert system for the digital\ncircuits’ diagnossis. He also developed a\nbehavioral symbolic simulator of digital circuits using the C, FRL,\nand LeLisp languages in order to contribute to the development of\na “formal proof” tool for correcting a design of material produced\nby a Hardware Design Language (HDL). Currently, his research\nwork mainly concerns Data Sciences.\nhttp:// journals.uob.edu.bh",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7294458150863647
    },
    {
      "name": "Arabic",
      "score": 0.7235012054443359
    },
    {
      "name": "Computer science",
      "score": 0.6503137350082397
    },
    {
      "name": "Artificial neural network",
      "score": 0.4476972818374634
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42051059007644653
    },
    {
      "name": "World Wide Web",
      "score": 0.40036237239837646
    },
    {
      "name": "Natural language processing",
      "score": 0.3877885937690735
    },
    {
      "name": "Engineering",
      "score": 0.19789913296699524
    },
    {
      "name": "Linguistics",
      "score": 0.1416650414466858
    },
    {
      "name": "Electrical engineering",
      "score": 0.09431779384613037
    },
    {
      "name": "Voltage",
      "score": 0.04013890027999878
    },
    {
      "name": "Philosophy",
      "score": 0.034126222133636475
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99297268",
      "name": "University of Hassan II Casablanca",
      "country": "MA"
    }
  ],
  "cited_by": 31
}