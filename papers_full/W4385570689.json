{
  "title": "Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity",
  "url": "https://openalex.org/W4385570689",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2800557345",
      "name": "Gabriel Simmons",
      "affiliations": [
        "University of California, Davis"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3137423266",
    "https://openalex.org/W3120860016",
    "https://openalex.org/W2966551734",
    "https://openalex.org/W4302306219",
    "https://openalex.org/W2119432837",
    "https://openalex.org/W3104041537",
    "https://openalex.org/W3133108979",
    "https://openalex.org/W4287890491",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W2946711516",
    "https://openalex.org/W2622292886",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2023418922",
    "https://openalex.org/W3130772487",
    "https://openalex.org/W3097654613",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4386566829",
    "https://openalex.org/W3042401844",
    "https://openalex.org/W4297847082",
    "https://openalex.org/W2991236417",
    "https://openalex.org/W1954045752",
    "https://openalex.org/W3183435370",
    "https://openalex.org/W4312050653",
    "https://openalex.org/W3171808334",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3147808590",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4292947474",
    "https://openalex.org/W4285484119",
    "https://openalex.org/W2096910467",
    "https://openalex.org/W4385574282",
    "https://openalex.org/W2798751230",
    "https://openalex.org/W2148165142",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W4214607966",
    "https://openalex.org/W2948518250",
    "https://openalex.org/W4224866872",
    "https://openalex.org/W2234292000",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3047185145",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2953982927",
    "https://openalex.org/W1467293599"
  ],
  "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases. This work investigates whether LLMs reproduce the moral biases associated with political groups in the United States, an instance of a broader capability herein termed moral mimicry. This work explores this hypothesis in the GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral Foundations Theory, this work shows that these LLMs are indeed moral mimics. When prompted with a liberal or conservative political identity, the models generate text reflecting corresponding moral biases. This study also explores the relationship between moral mimicry and model size, and similarity between human and LLM moral word use.",
  "full_text": "Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics - Student Research Workshop, pages 282–297\nJuly 10-12, 2023 ©2023 Association for Computational Linguistics\nMoral Mimicry: Large Language Models Produce Moral Rationalizations\nTailored to Political Identity\nGabriel Simmons\nUC Davis\ngsimmons@ucdavis.edu\nAbstract\nLarge Language Models (LLMs) have demon-\nstrated impressive capabilities in generating flu-\nent text, as well as tendencies to reproduce\nundesirable social biases. This study investi-\ngates whether LLMs reproduce the moral biases\nassociated with political groups in the United\nStates, an instance of a broader capability herein\ntermed moral mimicry. This hypothesis is ex-\nplored in the GPT-3/3.5 and OPT families of\nTransformer-based LLMs. Using tools from\nMoral Foundations Theory, it is shown that\nthese LLMs are indeed moral mimics. When\nprompted with a liberal or conservative politi-\ncal identity, the models generate text reflecting\ncorresponding moral biases. This study also ex-\nplores the relationship between moral mimicry\nand model size, and similarity between human\nand LLM moral word use.\n1 Introduction\nRecent work suggests that Large Language Model\n(LLM) performance will continue to scale with\nmodel and training data sizes (Kaplan et al., 2020).\nAs LLMs advance in capability, it becomes more\nlikely that they will be capable of producing text\nthat influences human opinions (Tiku, 2022), po-\ntentially lowering barriers to disinformation (Wei-\ndinger et al., 2022). More optimistically, LLMs\nmay play a role in bridging divides between social\ngroups (Alshomary and Wachsmuth, 2021; Jiang\net al., 2022). For better or worse, we should under-\nstand how LLM-generated content will impact the\nhuman informational environment - whether this\ncontent is influential, and to whom.\nMorality is an important factor in persuasiveness\nand polarization of human opinions (Luttrell et al.,\n2019). Moral argumentation can modulate willing-\nness to compromise (Kodapanakkal et al., 2022),\nand moral congruence between participants in a\ndialogue influences argument effectiveness (Fein-\nberg and Willer, 2015) and perceptions of ethicality\n(Egorov et al., 2020).\nTherefore, it is important to characterize the ca-\npabilities of LLMs to produce apparently-moral\ncontent1. This requires a framework from which\nwe can study morality; Moral Foundations The-\nory (MFT) is one such framework. MFT pro-\nposes that human morals rely on five foundations:\nCare/Harm, Fairness/Cheating, Loyalty/Betrayal,\nAuthority/Subversion, and Sanctity/Degradation2.\nEvidence from MFT supports the “Moral Foun-\ndations Hypothesis” that political groups in the\nUnited States vary in their foundation use - lib-\nerals rely primarily on the individualizing founda-\ntions (Care/Harm and Fairness/Cheating), while\nconservatives make more balanced appeals to all 5\nfoundations, appealing to the binding foundations\n(Authority/Subversion, Sanctity/Degradation, and\nLoyalty/Betrayal) more than liberals (Graham et al.,\n2009; Do˘gruyol et al., 2019; Frimer, 2020).\nExisting work has investigated the moral foun-\ndational biases of language models that have been\nfine-tuned on supervised data (Fraser et al., 2022),\ninvestigated whether language models reproduce\nother social biases (see (Weidinger et al., 2022)\nsection 2.1.1), and probed LLMs for differences\nin other cultural values (Arora et al., 2023). Con-\ncurrent work has shown that LLMs used as dialog\nagents tend to repeat users’ political views back\nto them, and that this happens more frequently in\nlarger models (Perez et al., 2022). To my knowl-\nedge, no work yet examines whether language mod-\nels can perform moral mimicry - that is, reproduce\nthe moral foundational biases associated with social\n1Anthropomorphization provides convenient ways to talk\nabout system behavior, but can also distort perception of un-\nderlying mechanisms (Bender and Koller, 2020). To be clear,\nI ascribe capabilities such as “moral argumentation” or “moral\ncongruence” to language models only to the extent that their\noutputs may be perceived as such, and make no claim that\nLLMs might generate such text with communicative intent.\n2Liberty/Oppression was proposed as a sixth foundation\n- for the sake of this analysis I consider only the original 5\nfoundations, as these are the ones available in the Moral Foun-\ndations Dictionaries (Graham et al., 2009; Frimer, 2019; Hopp\net al., 2021).\n282\nFigure 1: An example of the experimental methods. Prompts 2 are constructed from scenarios 1a, identity phrases\n1b, and stances 1c, combined in a template (Section 2). Text completions 3 are generated by LLMs based on the\nprompts (Section 2). The completions are analyzed for their foundational contents 4 using the moral foundations\ndictionaries (Section 2). Differences between texts generated from liberal and conservative prompting are used to\ncalculate effect sizes 5 .\ngroups such as political identities.\nThe present study considers whether LLMs use\nmoral vocabulary in ways that are situationally-\nappropriate, and how this compares to human foun-\ndation use. I find that LLMs respond to the salient\nmoral attributes of scenario descriptions, increas-\ning their use of the appropriate foundations, but\nstill differ from human consensus foundation use\nmore than individual humans (Section 2.1). I then\nturn to the moral mimicry phenomenon. I inves-\ntigate whether conditioning an LLM with a politi-\ncal “identity” influences the model’s use of moral\nfoundations in ways that are consistent with human\nmoral biases. I find confirmatory results for text\ngenerated based on“liberal” and “conservative” po-\nlitical identities (Section 2.2). Finally, I ask how the\nmoral mimicry phenomenon varies with model size.\nResults show that the extent to which LLMs can\nreproduce moral biases increases with model size,\nin the OPT family (Section 2.2). This is also true\nfor the GPT-3 and -3.5 models considered together,\nand to a lesser extent for the GPT-3 models alone.\n2 Methods\nData Generation All experiments follow the\nsame pattern for data generation, described in the\nfollowing sections and illustrated in Figure 1. Meth-\nods accompanying specific research questions are\npresented alongside results in Sections 2.1 - 2.3.\nPrompt Construction I constructed prompts that\nencourage the language model to generate apparent\nmoral rationalizations. Each prompt conditions the\nmodel with three variables: a scenario s, a political\nidentity phrase i, and a moral stancer. Each prompt\nconsists of values for these variables embedded in\na prompt template t.\nScenarios are text strings describing situations\nor actions apt for moral judgement. I used three\ndatasets (Moral Stories 3 (Emelin et al., 2021),\nETHICS4 (Hendrycks et al., 2021), and Social\nChemistry 101 5 (Forbes et al., 2020)) to obtain\nfour sets of scenarios, which I refer to as Moral\nStories, ETHICS, Social Chemistry Actions, and\nSocial Chemistry Situations. Appendix Section A.2\nprovides specifics on how each dataset was con-\nstructed. I use S and s to a set of scenarios, and a\nsingle scenario, respectively.\nPolitical identity phrasesare text strings refer-\nring to political ideologies (e.g. “liberal”). I use I\nand i to refer to a set of political identities and an\nindividual identity, respectively.\nMoral StancesThe moral stance presented in\neach prompt conditions the model to produce an\napparent rationalization indicating approval or dis-\napproval of the scenario. I useR, rto refer to the set\nof stances {moral, immoral}, and a single stance,\nrespectively. The datasets used herein contain la-\nbels indicating the normative moral acceptability\nof each scenario. For a scenario s, I refer to its\nnormative moral acceptability as rH(s).\nPrompt Templatesare functions that convert a\ntuple of scenario, identity phrase, and moral stance\ninto a prompt. To check for sensitivity to any par-\nticular phrasing, five different styles of prompt tem-\nplate were used (see Appendix Tables 2 and 3).\n3Downloaded from https://github.com/demelin/moral_stories\n4Downloaded from https://github.com/hendrycks/ethics\n5Downloaded from https://github.com/mbforbes/social-\nchemistry-101\n283\nPrompts were constructed by selecting a template\nt for a particular style, and populating it with a\nstance, scenario, and political identity phrase.\nText Generation with LLMsLanguage models\nproduce text by autoregressive decoding. Given a\nsequence of tokens, the model assigns likelihoods\nto all tokens in its vocabulary indicating how likely\nthey are to follow the sequence. Based on these\nlikelihoods, a suitable next token is appended to\nthe sequence, and the process is repeated until a\nmaximum number of tokens is generated, or the\nmodel generates a special “end-of-sequence” token.\nI refer to the text provided initially to the model as\na “prompt” and the text obtained through the decod-\ning process as a “completion”. In this work I used\nthree families of Large Language Models: GPT-3,\nGPT-3.5, and OPT (Table 1). GPT-3 is a family\nof Transformer-based (Vaswani et al., 2017) au-\ntoregressive language models with sizes up to 175\nbillion parameters, pre-trained in self-supervised\nfashion on web text corpora (Radford et al., 2019).\nThe largest 3 of the 4 GPT-3 models evaluated\nhere also received supervised fine-tuning on high-\nquality model samples and human demonstrations\n(OpenAI, 2022). The GPT-3.5 models are also\nTransformer-based, pre-trained on text and code\nweb corpora, and fine-tuned using either supervised\nfine-tuning or reinforcement learning from human\npreferences (OpenAI, 2022). I accessed GPT-3/3.5\nthrough the OpenAI Completions API (OpenAI,\n2021). I used the engine parameter to indicate a spe-\ncific model. GPT-3 models “text-ada-001”, “text-\nbabbage-001”, “text-curie-001”, and “text-davinci-\n001”, and GPT-3.5 models “text-davinci-002” and\n“text-davinci-003” were used. The OPT models are\nTransformer-based pre-trained models released by\nMeta AI, with sizes up to 175B parameters (Zhang\net al., 2022). Model sizes up to 30B parameters\nwere used herein. OPT model weights were ob-\ntained from the HuggingFace Model Hub. I ob-\ntained completions from these models locally using\nthe HuggingFace Transformers (Wolf et al., 2020)\nand DeepSpeed ZeRo-Inference libraries (Deep-\nSpeed, 2022), using a machine with a Threadripper\n3960x CPU and two RTX3090 24GB GPUs. For\nall models, completions were produced with tem-\nperature=0 for reproducibility. The max_tokens\nparameter was used to stop generation after 64 to-\nkens (roughly 50 words). All other settings were\nleft as default 6.\nMeasuring Moral Content\nMoral Foundations Dictionaries I estimated the\nmoral foundational content of each completion\nusing three dictionaries: the Moral Foundations\nDictionary version 1.0 (MFDv1) (Graham et al.,\n2009), Moral Foundations Dictionary version 2.0\n(MFDv2) (Frimer, 2019), the extended Moral Foun-\ndations Dictionary (eMFD) (Hopp et al., 2021).\nMFDv1 consists of a lexicon containing 324\nword stems, with each word stem associated to one\nor more categories. MFDv2 consists of a lexicon of\n2014 words, with each word associated to a single\ncategory. In MFDv1, the categories consist of a\n“Vice” and “Virtue” category for each of the five\nfoundations, plus a “MoralityGeneral” category, for\n11 categories in total. MFDv2 includes all cate-\ngories from MFDv1 except “MoralityGeneral”, for\na total of 10 categories. The eMFD (Hopp et al.,\n2021) contains 3270 words and differs slightly from\nMFDv1 and MFDv2. Words in the eMFD are as-\nsociated with all foundations by scores in [0, 1].\nScores were derived from annotation of news arti-\ncles, and indicate how frequently each word was\nassociated to each foundation, divided by the to-\ntal word appearances. Word overlap between the\ndictionaries is shown in Appendix Figure 5.\nRemoving Valence InformationAll three dic-\ntionaries indicate whether a word is associated with\nthe positive or negative aspect of a foundation. In\nMFDv1 and MFDv2 this is indicated by word as-\nsociation to the “Vice” or “Virtue” category for\neach foundation. In the eMFD, each word has sen-\ntiment scores for each foundation. In this work I\nwas interested in the foundational contents of the\ncompletions, independent of valence. Accordingly,\n“Vice” and “Virtue” categories were merged into a\nsingle category for each foundation, in both MFDv1\nand MFDv2. The “MoralityGeneral” score from\nMFDv1 was unused as it does not indicate asso-\nciation with any particular foundation. Sentiment\nscores from eMFD were also unused.\nApplying the DictionariesApplying dictionary\nd to a piece of text produces five scores{wd f|f ∈\nF}. For MFDv1 and MFDv2, these are inte-\nger values representing the number of foundation-\nassociated words in the text. The eMFD produces\n6Default values for unused parameters of the OpenAI\nCompletions API were suffix: null; top_p: 1; n: 1;\nstream: false; logprobs: null; echo: false; stop: null;\npresence_penalty: 0; frequency_penalty: 0; best_of: 1;\nlogit_bias: null; user: null\n284\ncontinuous values in [0, ∞] - the foundation-wise\nsums of scores for all eMFD words in the text.\nI am interested in the probability P that a human\nor language model (apparently) expresses founda-\ntion f, which I write as Ph(ef ) and PLM (ef ), re-\nspectively. I use Pd(ef |s, r, i) to denote this prob-\nability conditioned on a scenario s, stance r, and\npolitical identity i, using a dictionary d for mea-\nsurement.\nI use F to refer to the set of moral foundations,\nand f for a single foundation. I use D to refer to the\nset of dictionaries. In each dictionary, Wd refers\nto all words in the dictionary. For MFDv1 and\nMFDv2, Wd frefers to all the words in d belonging\nto foundation f. I approximate Pd(ef |s, r, i) as the\nfoundation-specific score wd fobtained by applying\nthe dictionary d to the model’s response to a prompt,\nnormalized by the total score across all foundations,\nas shown in Equation 1 below.\nPd(ef |s, r, i) ≈ wfd∑\nf′∈F wf′d\n(1)\nCalculating Effect Sizes Effect sizes capture\nhow varying political identity alters the likelihood\nthat the model will express foundation f, given\nthe same stance and scenario. Effect sizes were\ncalculated as the absolute difference in foundation\nexpression probabilities for pairs of completions\nthat differ only in political identity (Equation 2 be-\nlow). Equation 3 calculates the average effect size\nfor foundation f over scenarios S and stances R,\nmeasured by dictionary d. Equation 4 gives one av-\nerage effect size by the results across dictionaries.\n∆Pd\ni1,i2 (ef |s,r)=Pd(ef |s,i1,r)−Pd(ef |s,i2,r) (2)\n∆Pd\ni1,i2 (ef )=Es,r∈S×R ∆Pd\ni1,i2 (ef |s,r) (3)\n∆Pi1,i2 (ef )=Ed∈D ∆Pd\ni1,i2 (ef ) (4)\n2.1 LLM vs. Human Moral Foundation Use\nExperiment Details This experiment considers\nwhether LLMs use foundation words that are situa-\ntionally appropriate7. LLMs would satisfy a weak\ncriterion for this capability if they were more likely\nto express foundation f in response to scenarios\nwhere foundation f is salient, compared to their av-\nerage use of f across a corpus of scenarios contain-\ning all foundations in equal proportion. I formalize\nthis with Criterion A below.\nCriterion A Average use of foundation f is\ngreater across scenarios Sf that demonstrate only\n7e.g. using the Care/Harm foundation when prompted with\na violent scenario\nfoundation f, in comparison to average use of foun-\ndation f across a foundationally-balanced corpus\nof scenarios S (Equation 5).\nEsf ,r∈Sf ×R PLM (ef |sf ,r)>Es,r∈S×R PLM (ef |s,r)\nA stronger criterion would require LLMs to not\nto deviate from human foundation use beyond some\nlevel of variation that is expected among humans. I\nformalize this with Criterion 2b below.\nCriterion BThe average difference between lan-\nguage model and consensus human foundation use\nis less than the average difference between individ-\nual human and consensus human foundation use.\nDIFFLM,CH ≤DIFFH,CH (5)\nDIFFLM,CH =Es∈S[|PLM (ef |s,rH(s))−CH(s)|] (6)\nDIFFH,CH =Es∈S[EH[|Ph(ef |s)−CH(s)|]] (7)\nCH(s)=Eh[Ph(ef |s)] (8)\nStance rHs is the normative moral acceptability\nof scenario s - the human-written rationalizations\nare “conditioned” on human normative stance for\neach scenario, so I only compare these with model\noutputs that are also conditioned on human norma-\ntive stance.\nCriterion A requires a corpus with ground-truth\nknowledge that only a particular foundation f is\nsalient for each scenario. To obtain such clear-\ncut scenarios, I select the least ambiguous actions\nfrom the Social Chemistry dataset, according to the\nfiltering methods described in Appendix Section\nA.2.3. Estimating human consensus foundation use\n(Criterion B) requires a corpus of scenarios that are\neach annotated in open-ended fashion by multiple\nhumans. I obtain such a corpus from the Social\nChemistry dataset using the methods described in\nAppendix Section A.2.4.\nResults\nFigure 2 (left) shows average values of P(ef |s)\nfor each foundation. For all five foundations, the\nmodel increases its apparent use of foundation-\nassociated words appropriate to the ground truth\nfoundation label, satisfying Criterion A. Figure\n2 (right) shows LM differences from human con-\nsensus |PLM (ef |s, rHs) −CH(s)|obtained from\nthe text-davinci-002 model, and human differences\nfrom human consensus EH [|Ph(ef |s) −CH(s)|],\non the Social Chemistry Situations dataset. In gen-\neral the LM-human differences are greater than the\nhuman-human differences.\n285\nFigure 2: Left: Foundation expression probabilities for\nfoundation-specific examples vs. average foundation use\nacross all examples. Text-davinci-002; Social Chemistry\nActions scenarios. Right: LM and individual human\ndifferences from human consensus foundation use, in\nresponse to scenarios from the Social Chemistry Situa-\ntions dataset; text-davinci-002.\n2.2 Are LLMs Moral Mimics?\nExperiment Details I consider whether condi-\ntioning LLMs with political identity influences their\nuse of moral foundations in a way that reflects hu-\nman moral biases. To investigate this question I\nused a corpus of 2,000 scenarios obtained from the\nMoral Stories dataset and 1,000 scenarios obtained\nfrom the ETHICS dataset, described in Appendix\nSection A.2.\nPrompts were constructed with template style\n2 from table 2. For each scenario, four prompts\nwere constructed based on combinations of “lib-\neral” and “conservative” political identity and moral\nand immoral stance, for a total of 12,000 prompts.\nCompletions were obtained from the most capa-\nble model in each family that our computational\nresources afforded: text-davinci-001 (GPT-3), text-\ndavinci-002 and text-davinci-003 (GPT-3.5) and\nOPT-30B. One generation was obtained from each\nmodel for each prompt. I calculated average effect\nsize ∆Pi1,i2 (ef ) with i1 = “liberal” and i2 = “con-\nservative” for all five foundations. Effect sizes were\ncomputed separately for each dictionary, for a total\nof 18,000 effect sizes computed per model.\nResults Figure 3 shows effect sizes for liberal\nvs. conservative political identity, for the most capa-\nble models tested from the OPT, GPT, and GPT-3.5\nmodel families, measured using the three moral\nfoundations dictionaries. The shaded regions in\neach plot represent the effects that would be ex-\npected based on the Moral Foundations Hypothesis\n- namely that prompting with liberal political iden-\ntity would result in more use of the individualizing\nfoundations (positive ∆Pi1,i2 ) and prompting with\nconservative political identity would result in more\nuse of the binding foundations (negative ∆Pi1,i2 ).\nThe majority of effect sizes coincide with the\nMoral Foundations Hypothesis. Of 60 combina-\ntions of 5 foundations, 4 models, and 3 dictionaries,\nonly 11 effect sizes are in the opposite direction\nfrom expected, and all of these effect sizes have\nmagnitude of less than 1 point absolute difference.\n2.3 Is Moral Mimicry Affected By Model\nSize?\nExperiment Details In this section, I consider\nhow moral mimicry relates to model size. I used\ntext-ada-001, text-babbage-001, text-curie-001, and\ntext-davinci-001 models from the GPT-3 family,\ntext-davinci-002 and text-davinci-003 from the\nGPT-3.5 family (OpenAI, 2022), and OPT-350m,\nOPT-1.3B, OPT-6.7B, OPT-13B, and OPT-30B\n(Zhang et al., 2022). The GPT-3 models have\nestimated parameter counts of 350M, 1.3B, 6.7B\nand 175B, respectively (OpenAI, 2022; Gao, 2021).\nText-davinci-002 and text-davinci-003 also have\n175B parameters (OpenAI, 2022). Parameters in\nbillions for the OPT models are indicated in the\nmodel names.\nTo analyze to what extent each model demon-\nstrates the moral mimicry phenomenon, I define a\nscoring function MFH-S CORE that scores a model\nm as follows:\nMFH-S CORE (m)=∑\nf∈F signMFH (f)∆Pm(ef ) (9)\nsignMFH =\n\n\n\n−1, iff ∈{A/S, S/D, L/B}\n+1, iff ∈{C/H, F/C}\n(10)\nA/S: Authority/Subversion; S/D: Sanctity/Degradation;\nL/B: Loyalty/Betrayal; C/H: Care/Harm; F/C; Fairness/Cheating\nThe MFH-S CORE calculates the average effect\nsize for each model in the direction predicted by\nthe Moral Foundations Hypothesis.\nResults Figure 4 above shows effect sizes∆(Pef )\nfor each foundation and MFH-S CORE s vs. model\nsize (number of parameters). Effect sizes are aver-\naged over the three moral foundations dictionaries.\nFor the OPT model family, we can see that model\nparameter count and MFH-S CORE show some rela-\ntionship (r=0.69, although statistical power is lim-\n286\nFigure 3: Effect sizes for liberal vs. conservative political identity for OPT-30B, text-davinci-001, text-davinci-002,\nand text-davinci-003. Dot markers represent average effect size. Error bars represent 95% CI. Shaded regions\nrepresent directions of expected effect size based on the Moral Foundations Hypothesis.\nited due to the limited number of models). In par-\nticular, the Sanctity/Degradation foundation main-\ntains a non-zero effect size in the expected direc-\ntion for all models 6.7B parameters or larger. Sur-\nprisingly, OPT-13B shows decreased effect sizes\nfor Fairness/Cheating and Care/Harm in compari-\nson to the smaller OPT-6.7B. The relationship be-\ntween model size and effect size is weaker for\nGPT-3 (r=0.23). Care/Harm, Fairness/Cheating,\nSanctity/Degradation, and Authority/Subversion\nhave effect size in the expected direction for Bab-\nbage, Curie, and DaVinci models, though the effect\nsizes are smaller than for the OPT family. Mod-\nels from the GPT-3.5 family show the largest ef-\nfect sizes overall. Unfortunately, no smaller model\nsizes are available for this family. If we include\nthe GPT-3 and GPT-3.5 models together (indi-\ncated by †in Figure 4), the correlation between\nMFH-S CORE and model parameters increases to\nr=0.84. Interestingly, the OPT and GPT-3 families\nshow Sanctity/Degradation as the most pronounced\neffect size for conservative prompting, and Fair-\nness/Cheating as the most pronounced effect size\nfor liberal prompting. GPT-3.5 instead shows the\nlargest effect sizes for Authority/Subversion and\nCare/Harm, respectively.\n3 Discussion\nSection 2.1 posed two criteria to judge whether\nLLMs use moral foundations appropriately. For\nthe weaker Criterion A, results show that LLMs\ndo increase use of foundation words relevant to\nthe foundation that is salient in a given scenario,\nat least for scenarios with clear human consensus\non foundation salience. However, for Criterion B,\nresults show that LLMs differ more from human\nconsensus foundation use than humans do in terms\nof foundation use.\nSection 2.2 compared LM foundation use with\nfindings from moral psychology that identify dif-\nferences in the moral foundations used by lib-\neral and conservative political groups. Specif-\nically, according to the Moral Foundations Hy-\npothesis, liberals rely mostly on the Care/Harm\nand Fairness/Cheating foundations, while conser-\nvatives use all 5 foundations more evenly, using\nAuthority/Subversion, Loyalty/Betrayal, and Fair-\nness/Cheating more than liberals. This finding was\nfirst presented in (Graham et al., 2009), and has\nsince been supported with confirmatory factor anal-\nysis in (Do˘gruyol et al., 2019), and partially repli-\ncated (though with smaller effect sizes) in (Frimer,\n2020).\nResults indicate that models from the GPT-3,\nGPT-3.5 and OPT model families are more likely\nto use the binding foundations when prompted\nwith conservative political identity, and are more\nlikely to use the individualizing foundations when\nprompted with liberal political identity. Emphasis\non individual foundations in each category differs\nby model family. OPT-30B shows larger effect\nsizes for Fairness/Cheating than Care/Harm and\nlarger effect sizes for Sanctity/Degradation vs. Au-\nthority/Subversion, while GPT-3.5 demonstrates\nthe opposite. I suspect that this may be due to dif-\nferences in training data and/or training practices\nbetween the model families. This opens an interest-\ning question of how to influence the moral mimicry\n287\nFigure 4: Top: Effect size vs. model parameters, based on completions obtained from Moral Stories dataset. Dark\nlines show mean effect size. Error bars show 95% CI. Effect sizes are averaged over the three moral foundations\ndictionaries.; 002: text-davinci-002; 003: text-davinci-003.; Bottom: MFH-S CORE vs. model parameters; r,p: value\nand p-value for Pearson’s Correlation between MFH-S CORE and model parameters.; †results of correlation analysis\nwith GPT-3 and GPT-3.5 models analyzed together\ncapabilities that emerge during training, via dataset\ncuration or other methods.\nThe results from Section 2.3 show some relation-\nship between moral mimicry and model size. Effect\nsizes tend to increase with parameter count in the\nOPT family, and less so in the GPT-3 family. Both\n175B-parameter GPT-3.5 models show relatively\nstrong moral mimicry capabilities, moreso than the\n175B GPT-3 model text-davinci-001. This suggests\nthat parameter count is not the only factor lead-\ning to moral mimicry. The GPT-3.5 models were\ntrained with additional supervised fine-tuning not\napplied to the GPT-3 family, and used text and code\npre-training rather than text alone (OpenAI, 2022).\n4 Limitations\nThis work used the moral foundations dictionar-\nies to measure the moral content of text produced\nby GPT-3. While studies have demonstrated cor-\nrespondence between results from the dictionaries\nand human labels of moral foundational content\n(Mutlu et al., 2020; Graham et al., 2009), dictionary-\nbased analysis is limited in its ability to detect nu-\nanced moral expressions. Dictionary-based analy-\nsis could be complemented with machine-learning\napproaches (Garten et al., 2016; Johnson and Gold-\nwasser, 2018; Pavan et al., 2020; Roy et al., 2022)\nas well as human evaluation. This study attempted\nto control for variations in the prompt phrasing by\naveraging results over several prompt styles (Tables\n2 and 3). These prompt variations were chosen\nby the author. A more principled selection proce-\ndure could result in a more diverse set of prompts.\nThe human studies that this study refers to (Graham\net al., 2009; Frimer, 2020) were performed on popu-\nlations from the United States. The precise political\nconnotations of the terms “liberal” and “conserva-\ntive” differ across demographics. Future work may\nexplore how language model output varies when\nadditional demographic information is provided, or\nwhen multilingual models are used. Documenta-\ntion for the datasets used herein indicates that the\ncrowd workers leaned politically left, and morally\ntowards the Care/Harm and Fairness/Cheating foun-\ndations (Forbes et al., 2020; Hendrycks et al., 2021;\nFraser et al., 2022). However, bias in the marginal\nfoundation distribution does not hinder the present\nanalysis, since the present experiments experiments\nfocus primarily on the difference in foundation\nuse resulting from varying political identity. The\nanalysis in Section 2.1 relies more heavily on the\nmarginal foundation distribution; a foundationally-\nbalanced dataset was constructed for this experi-\nment. This study used GPT-3 (Brown et al., 2020),\nGPT-3.5 (OpenAI, 2022), and OPT (Zhang et al.,\n2022). Other pre-trained language model families\nof similar scale and architecture include BLOOM8,\nwhich I was unable to test due to compute bud-\nget, and LLaMA (Touvron et al., 2023), which was\nreleased after the experiments for this work con-\ncluded. While the OPT model weights are available\nfor download, GPT-3 and GPT-3.5 model weights\nare not; this may present barriers to future work\nthat attempts to connect the moral mimicry phe-\nnomenon to properties of the model. On the other\nhand, the hardware required to run openly-available\nmodels may be a barrier to experimentation that is\nnot a concern for models hosted via an API.\nCriticisms of Moral Foundations Theory include\ndisagreements about whether a pluralist theory of\nmorality is parsimonious (Suhler and Churchland,\n2011; Dobolyi, 2016); Ch. 6 of (Haidt, 2013), dis-\nagreements about the number and character of the\n8https://bigscience.huggingface.co/blog/bloom\n288\nfoundations (Yalçında˘g et al., 2019; Harper and\nRhodes, 2021), disagreements about stability of the\nfoundations across cultures (Davis et al., 2016), and\ncriticisms suggesting bias in the Moral Foundations\nQuestionnaire (Dobolyi, 2016). Moral foundations\ntheory was used in this study because it provides\nestablished methods to measure moral content in\ntext, and because MFT-based analyses have identi-\nfied relationships between political affiliation and\nmoral biases, offering a way to compare LLM and\nhuman behavior. The methods presented here may\nbe applicable to other theories of morality; this is\nleft for future work.\nWork that aims to elicit normative moral or eth-\nical judgement from non-human systems has re-\nceived criticism. Authors have argued that non-\nhuman systems lack the autonomy and communica-\ntive intent to be moral agents (Talat et al., 2022;\nBender and Koller, 2020). Criticisms have also\nbeen raised about the quality and appropriateness\nof data used to train such systems. Notably, crowd-\nsourced or repurposed data often reflects a priori\nopinions of individuals who may not be informed\nabout the topics they are asked to judge, and who\nmay not have had the opportunity for discourse or\nreflection before responding (Talat et al., 2022; Eti-\nenne, 2021). Some have argued that systems that ag-\ngregate moral judgements from descriptive datasets\ncannot help but be seen as normative, since their re-\nproduction of the popular or average view tends to\nbe implicitly identified with a sense of correctness\n(Talat et al., 2022). Finally, several authors argue\nthat the use of non-human systems that produce\napparent or intended normative judgements sets a\ndangerous precedent by short-circuiting the discur-\nsive process by which moral and ethical progress is\nmade, and by obscuring accountability should such\na system cause harm (Talat et al., 2022; Etienne,\n2021).\nThe present study investigates the apparent moral\nrationalizations produced by prompted LLMs. This\nstudy does not intend to produce a system for nor-\nmative judgement, and I would discourage a nor-\nmative use or interpretation of the methods and\nresults presented here. The recent sea change in nat-\nural language processing towards general-purpose\nLLMs prompted into specific behaviors enables end\nusers to produce a range of outputs of convincing\nquality, including apparent normative moral or eth-\nical judgements. Anticipating how these systems\nwill impact end users and society requires study-\ning model behaviors under a variety of prompting\ninputs. The present study was conducted with this\ngoal in mind, under the belief that the benefit of\nunderstanding the moral mimicry phenomenon out-\nweighs the risk of normative interpretation.\n5 Related Work\nSeveral machine ethics projects have assessed the\nextent to which LLM-based systems can mimic\nhuman normative ethical judgement, for example\n(Hendrycks et al., 2021) and (Jiang et al., 2021).\nOther projects evaluate whether LLMs can pro-\nduce the relevant moral norms for a given scenario\n(Forbes et al., 2020; Emelin et al., 2021), or whether\nthey can determine which scenarios justify moral\nexceptions (Jin et al., 2022). Yet other works fo-\ncus on aligning models to normative ethics (Ziems\net al., 2022), and investigating to what extent soci-\netal biases are reproduced in language models (see\nSection 5.1 of Bommasani et al. 2022). As an exam-\nple, Fraser, Kiritchenko, and Balkir (2022) analyze\nresponses of the Delphi model (Jiang et al., 2021)\nto the Moral Foundations Questionnaire (Graham\net al., 2011), finding that its responses reflect the\nmoral foundational biases of the groups that pro-\nduced the model and its training data.\nThe aforementioned research directions typically\ninvestigate language models not prompted with any\nparticular identity. This framing implies the pre-\ntrained model itself as the locus where a cohesive\nset of biases might exist. Recent work suggests an\nalternative view that a single model may be capable\nof simulating a multitude of “identities”, and that\nthese apparent identities may be selected from by\nconditioning the model via prompting (Argyle et al.,\n2023; Aher et al., 2023). Drawing on the latter view,\nthe present study prompts LLMs to simulate behav-\nior corresponding to opposed political identities,\nand evaluates the fidelity of these simulacra with\nrespect to moral foundational bias. Relations be-\ntween the present work and other works taking this\n“simulation” view are summarized below.\nArora et. al. probe for cultural values using Hof-\nstede’s six-dimenension theory (Hofstede, 2001)\nand the World Values Survey (Survey, 2022), and\nuse prompt language rather than prompt tokens\nto condition the model with a cultural “identity”.\nAlshomary et al. 2021 and Qian et al. 2021 fine-\ntune GPT-2 models (1.5B parameters) on domain-\nspecific corpora, and condition text generation with\nstances on social issues. The present work, in con-\ntrast, conditions on political identity rather than\n289\nstance, evaluates larger models without domain-\nspecific fine-tuning, and investigates LLM capabili-\nties to mimic moral preferences. Concurrent work\nprobes language models for behaviors including\nsycophancy, the tendency to mirror users’ politi-\ncal views in a dialog setting (Perez et al., 2022).\nPerez et. al. find that this tendency increases with\nscale above ~10B parameters. While sycophancy\ndescribes how model-generated text appears to ex-\npress political views, conditioned on dialog user po-\nlitical views, moral mimicry describes how model-\ngenerated text appears to express moral founda-\ntional salience, conditioned on political identity\nlabels. Argyle et. al. propose the concept of “algo-\nrithmic fidelity” - an LLM’s ability to “accurately\nemulate the response distribution . . . of human sub-\ngroups” under proper conditioning (Argyle et al.,\n2023). Moral mimicry can be seen as an instance of\nalgorithmic fidelity where moral foundation use is\nthe response variable of interest. Argyle et. al. study\nother response variables: partisan descriptors, vot-\ning patterns, and correlational structure in survey\nresponses.\n6 Conclusion\nThis study evaluates whether LLMs can reproduce\nthe moral foundational biases associated with social\ngroups, a capability herein coined moral mimicry.\nI measure the apparent use of five moral founda-\ntions in the text generated by pre-trained language\nmodels conditioned with a political identity. I show\nthat LLMs reproduce the moral foundational biases\nassociated with liberal and conservative political\nidentities, modify their moral foundation use situ-\nationally, although not indistinguishably from hu-\nmans, and that moral mimicry may relate to model\nsize.\nAcknowledgements\nI would like to thank the anonymous reviewers who\nprovided valuable comments on this paper. I would\nalso like to thank Professors Dipak Ghosal, Jiawei\nZhang, and Patrice Koehl, who provided valuable\nfeedback on this work, and colleagues, friends, and\nfamily for insightful discussions.\nReferences\nGati Aher, Rosa I. Arriaga, and Adam Tauman Kalai.\n2023. Using Large Language Models to Simulate\nMultiple Humans and Replicate Human Subject Stud-\nies.\nMilad Alshomary, Wei-Fan Chen, Timon Gurcke, and\nHenning Wachsmuth. 2021. Belief-based Generation\nof Argumentative Claims. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 224–233, Online. Association for Computa-\ntional Linguistics.\nMilad Alshomary and Henning Wachsmuth. 2021. To-\nward audience-aware argument generation. Patterns,\n2(6):100253.\nLisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R.\nGubler, Christopher Rytting, and David Wingate.\n2023. Out of One, Many: Using Language Mod-\nels to Simulate Human Samples. Political Analysis,\npages 1–15.\nArnav Arora, Lucie-aimee Kaffee, and Isabelle Augen-\nstein. 2023. Probing pre-trained language models for\ncross-cultural differences in values. In Proceedings\nof the First Workshop on Cross-Cultural Considera-\ntions in NLP (C3NLP), pages 114–130, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nEmily M. Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On Meaning, Form, and Understand-\ning in the Age of Data. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5185–5198, Online. Association\nfor Computational Linguistics.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dal-\nlas Card, Rodrigo Castellon, Niladri Chatterji, An-\nnie Chen, Kathleen Creel, Jared Quincy Davis,\nDora Demszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy,\nKawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil\nJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, Omar\nKhattab, Pang Wei Koh, Mark Krass, Ranjay Krishna,\nRohith Kuditipudi, Ananya Kumar, Faisal Ladhak,\nMina Lee, Tony Lee, Jure Leskovec, Isabelle Lev-\nent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali\nMalik, Christopher D. Manning, Suvir Mirchandani,\nEric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika\nNarayan, Deepak Narayanan, Ben Newman, Allen\nNie, Juan Carlos Niebles, Hamed Nilforoshan, Julian\nNyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou,\nJoon Sung Park, Chris Piech, Eva Portelance, Christo-\npher Potts, Aditi Raghunathan, Rob Reich, Hongyu\nRen, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack\nRyan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa,\nKeshav Santhanam, Andy Shih, Krishnan Srinivasan,\nAlex Tamkin, Rohan Taori, Armin W. Thomas, Flo-\nrian Tramèr, Rose E. Wang, William Wang, Bohan\nWu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michi-\nhiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael\n290\nZhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang,\nLucia Zheng, Kaitlyn Zhou, and Percy Liang. 2022.\nOn the Opportunities and Risks of Foundation Mod-\nels.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. 2020. Language\nmodels are few-shot learners. In Advances in Neural\nInformation Processing Systems, volume 33, pages\n1877–1901. Curran Associates, Inc.\nDon E. Davis, Kenneth Rice, Daryl R. Van Tongeren,\nJoshua N. Hook, Cirleen DeBlaere, Everett L. Wor-\nthington Jr., and Elise Choe. 2016. The moral foun-\ndations hypothesis does not replicate well in Black\nsamples. Journal of Personality and Social Psychol-\nogy, 110(4):e23–e30.\nDeepSpeed. 2022. ZeRO-Inference: De-\nmocratizing massive model inference.\nhttps://www.deepspeed.ai/2022/09/09/zero-\ninference.html.\nDavid Dobolyi. 2016. Critiques | Moral Foundations\nTheory.\nBurak Do ˘gruyol, Sinan Alper, and Onurcan Yilmaz.\n2019. The five-factor model of the moral founda-\ntions theory is stable across WEIRD and non-WEIRD\ncultures. Personality and Individual Differences ,\n151:109547.\nMaxim Egorov, Karianne Kalshoven, Armin Pircher Ver-\ndorfer, and Claudia Peus. 2020. It’s a Match: Moral-\nization and the Effects of Moral Foundations Congru-\nence on Ethical and Unethical Leadership Perception.\nJournal of Business Ethics, 167(4):707–723.\nDenis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell\nForbes, and Yejin Choi. 2021. Moral Stories: Situ-\nated Reasoning about Norms, Intents, Actions, and\ntheir Consequences. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 698–718, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nHubert Etienne. 2021. The dark side of the ‘Moral\nMachine’ and the fallacy of computational ethical\ndecision-making for autonomous vehicles. Law, In-\nnovation and Technology, 13(1):85–107.\nMatthew Feinberg and Robb Willer. 2015. From Gulf\nto Bridge: When Do Moral Arguments Facilitate Po-\nlitical Influence? Personality and Social Psychology\nBulletin, 41(12):1665–1681.\nMaxwell Forbes, Jena D. Hwang, Vered Shwartz,\nMaarten Sap, and Yejin Choi. 2020. Social chem-\nistry 101: Learning to reason about social and moral\nnorms. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 653–670, Online. Association for\nComputational Linguistics.\nKathleen C. Fraser, Svetlana Kiritchenko, and Esma\nBalkir. 2022. Does Moral Code have a Moral Code?\nProbing Delphi’s Moral Philosophy. In Proceedings\nof the 2nd Workshop on Trustworthy Natural Lan-\nguage Processing (TrustNLP 2022) , pages 26–42,\nSeattle, U.S.A. Association for Computational Lin-\nguistics.\nJeremy Frimer. 2019. Moral Foundations Dictionary\n2.0.\nJeremy A. Frimer. 2020. Do liberals and conservatives\nuse different moral languages? Two replications and\nsix extensions of Graham, Haidt, and Nosek’s (2009)\nmoral text analysis. Journal of Research in Personal-\nity, 84:103906.\nLeo Gao. 2021. On the Sizes of OpenAI API Models.\nhttps://blog.eleuther.ai/gpt3-model-sizes/.\nJustin Garten, Reihane Boghrati, J. Hoover, Kate M.\nJohnson, and Morteza Dehghani. 2016. Morality\nBetween the Lines : Detecting Moral Sentiment In\nText.\nJesse Graham, Jonathan Haidt, and Brian A. Nosek.\n2009. Liberals and conservatives rely on different\nsets of moral foundations. Journal of Personality and\nSocial Psychology, 96(5):1029–1046.\nJesse Graham, Brian A. Nosek, Jonathan Haidt, Ravi\nIyer, Spassena Koleva, and Peter H. Ditto. 2011. Map-\nping the Moral Domain. Journal of personality and\nsocial psychology, 101(2):366–385.\nJonathan Haidt. 2013. The Righteous Mind: Why Good\nPeople Are Divided by Politics and Religion. Vintage\nBooks.\nCraig A. Harper and Darren Rhodes. 2021. Reanalysing\nthe factor structure of the moral foundations ques-\ntionnaire. The British Journal of Social Psychology,\n60(4):1303–1329.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2021. Aligning AI with shared human values. In\n9th International Conference on Learning Represen-\ntations, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. OpenReview.net.\nGeert Hofstede. 2001. Culture’s Recent Consequences:\nUsing Dimension Scores in Theory and Research.\nInternational Journal of Cross Cultural Management,\n1(1):11–17.\n291\nFrederic R. Hopp, Jacob T. Fisher, Devin Cornell,\nRichard Huskey, and René Weber. 2021. The ex-\ntended Moral Foundations Dictionary (eMFD): Devel-\nopment and applications of a crowd-sourced approach\nto extracting moral intuitions from text. Behavior Re-\nsearch Methods, 53(1):232–246.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,\nTianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,\nBrian O’Horo, Gabriel Pereyra, Jeff Wang, Christo-\npher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,\nand Ves Stoyanov. 2023. OPT-IML: Scaling Lan-\nguage Model Instruction Meta Learning through the\nLens of Generalization.\nHang Jiang, Doug Beeferman, Brandon Roy, and Deb\nRoy. 2022. CommunityLM: Probing partisan world-\nviews from language models. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 6818–6826, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nLiwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ro-\nnan Le Bras, Jenny Liang, Jesse Dodge, Keisuke\nSakaguchi, Maxwell Forbes, Jon Borchardt, Saadia\nGabriel, Yulia Tsvetkov, Oren Etzioni, Maarten Sap,\nRegina Rini, and Yejin Choi. 2021. Can Machines\nLearn Morality? The Delphi Experiment.\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,\nOjasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada\nMihalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to make exceptions: Exploring language\nmodels as accounts of human moral judgment. In\nNeurIPS.\nKristen Johnson and Dan Goldwasser. 2018. Classifi-\ncation of Moral Foundations in Microblog Political\nDiscourse. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 720–730, Melbourne,\nAustralia. Association for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling Laws for Neural Language Models.\nRabia I. Kodapanakkal, Mark J. Brandt, Christoph\nKogler, and Ilja van Beest. 2022. Moral Frames Are\nPersuasive and Moralize Attitudes; Nonmoral Frames\nAre Persuasive and De-Moralize Attitudes. Psycho-\nlogical Science, 33(3):433–449.\nAndrew Luttrell, Aviva Philipp-Muller, and Richard E.\nPetty. 2019. Challenging Moral Attitudes With Moral\nMessages. Psychological Science, 30(8):1136–1150.\nEce Çi˘gdem Mutlu, Toktam Oghaz, Ege Tütüncüler, and\nIvan Garibay. 2020. Do Bots Have Moral Judgement?\nThe Difference Between Bots and Humans in Moral\nRhetoric. In 2020 IEEE/ACM International Confer-\nence on Advances in Social Networks Analysis and\nMining (ASONAM), pages 222–226.\nOpenAI. 2021. OpenAI API. https://openai.com/api/.\nOpenAI. 2022. Model Index for Researchers.\nMatheus C. Pavan, Vitor G. Dos Santos, Alex G. J.\nLan, Joao Martins, Wesley R. Santos, Caio Deutsch,\nPablo B. Costa, Fernando C. Hsieh, and Ivandre\nParaboni. 2020. Morality Classification in Natural\nLanguage Text. IEEE Transactions on Affective Com-\nputing, pages 1–1.\nEthan Perez, Sam Ringer, Kamil ˙e Lukoši ¯ut˙e, Karina\nNguyen, Edwin Chen, Scott Heiner, Craig Pettit,\nCatherine Olsson, Sandipan Kundu, Saurav Kadavath,\nAndy Jones, Anna Chen, Ben Mann, Brian Israel,\nBryan Seethor, Cameron McKinnon, Christopher\nOlah, Da Yan, Daniela Amodei, Dario Amodei, Dawn\nDrain, Dustin Li, Eli Tran-Johnson, Guro Khun-\ndadze, Jackson Kernion, James Landis, Jamie Kerr,\nJared Mueller, Jeeyoon Hyun, Joshua Landau, Ka-\nmal Ndousse, Landon Goldberg, Liane Lovitt, Mar-\ntin Lucas, Michael Sellitto, Miranda Zhang, Neerav\nKingsland, Nelson Elhage, Nicholas Joseph, Noemí\nMercado, Nova DasSarma, Oliver Rausch, Robin\nLarson, Sam McCandlish, Scott Johnston, Shauna\nKravec, Sheer El Showk, Tamera Lanham, Timothy\nTelleen-Lawton, Tom Brown, Tom Henighan, Tristan\nHume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark,\nSamuel R. Bowman, Amanda Askell, Roger Grosse,\nDanny Hernandez, Deep Ganguli, Evan Hubinger,\nNicholas Schiefer, and Jared Kaplan. 2022. Discover-\ning Language Model Behaviors with Model-Written\nEvaluations.\nMing Qian, Jaye Laguardia, and Davis Qian. 2021.\nMorality Beyond the Lines: Detecting Moral Senti-\nment Using AI-Generated Synthetic Context. In Arti-\nficial Intelligence in HCI, Lecture Notes in Computer\nScience, pages 84–94, Cham. Springer International\nPublishing.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nShamik Roy, Nishanth Sridhar Nakshatri, and Dan Gold-\nwasser. 2022. Towards Few-Shot Identification of\nMorality Frames using In-Context Learning. In\nProceedings of the Fifth Workshop on Natural Lan-\nguage Processing and Computational Social Science\n(NLP+CSS), pages 183–196, Abu Dhabi, UAE. Asso-\nciation for Computational Linguistics.\nChristopher Suhler and Pat Churchland. 2011. Can\nInnate, Modular “Foundations” Explain Morality?\nChallenges for Haidt’s Moral Foundations Theory.\nJournal of cognitive neuroscience, 23:2103–16; dis-\ncussion 2117.\nWorld Values Survey. 2022. WVS Database.\nhttps://www.worldvaluessurvey.org/wvs.jsp.\nZeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira\nGanesh, Ryan Cotterell, and Adina Williams. 2022.\nOn the Machine Learning of Ethical Judgments from\n292\nNatural Language. In Proceedings of the 2022 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 769–779, Seattle, United\nStates. Association for Computational Linguistics.\nNitasha Tiku. 2022. The Google engineer who thinks\nthe company’s AI has come to life. Washington Post.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand Efficient Foundation Language Models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh,\nConor Griffin, Po-Sen Huang, John Mellor, Amelia\nGlaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,\nCourtney Biles, Sasha Brown, Zac Kenton, Will\nHawkins, Tom Stepleton, Abeba Birhane, Lisa Anne\nHendricks, Laura Rimell, William Isaac, Julia Haas,\nSean Legassick, Geoffrey Irving, and Iason Gabriel.\n2022. Taxonomy of Risks posed by Language Mod-\nels. In 2022 ACM Conference on Fairness, Account-\nability, and Transparency, FAccT ’22, pages 214–229,\nNew York, NY , USA. Association for Computing Ma-\nchinery.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020. Hug-\ngingFace’s Transformers: State-of-the-art Natural\nLanguage Processing.\nBilge Yalçında˘g, Türker Özkan, Sevim Cesur, Onurcan\nYilmaz, Beyza Tepe, Zeynep Ecem Piyale, Ali Furkan\nBiten, and Diane Sunar. 2019. An Investigation of\nMoral Foundations Theory in Turkey Using Different\nMeasures. Current Psychology, 38(2):440–457.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. OPT: Open Pre-\ntrained Transformer Language Models.\nCaleb Ziems, Jane Yu, Yi-Chia Wang, Alon Halevy,\nand Diyi Yang. 2022. The moral integrity corpus: A\nbenchmark for ethical dialogue systems. In Proceed-\nings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 3755–3773, Dublin, Ireland. Association for\nComputational Linguistics.\nA Appendix A: Additional Details Related\nto Experimental Methods\nA.1 Additional Details Related to LLMs Used\nin the Study\nModel FamilyModel VariantNumber of ParametersInstruction Fine-tuningGPT-3 text-ada-001350M NoneGPT-3 text-babbage-0011.3B FeedMEGPT-3 text-curie-0016.7B FeedMEGPT-3 text-davinci-001175B FeedMEGPT-3.5 text-davinci-002175B ?GPT-3.5 text-davinci-003175B PPOOPT opt-350m 350M NoneOPT opt-1.3b 1.3B NoneOPT opt-6.7b 6.7B NoneOPT opt-13b 13B NoneOPT opt-30b 30B None\nTable 1: Models evaluated in this study. Information for\nGPT-3 and GPT-3.5 from (OpenAI, 2022). Information\nfor OPT from (Zhang et al., 2022). Information for OPT-\nIML from (Iyer et al., 2023). FeedME: “Supervised\nfine-tuning on human-written demonstrations and on\nmodel samples rated 7/7 by human labelers on an overall\nquality score” (OpenAI, 2022); PPO: “Reinforcement\nlearning with reward models trained from comparisons\nby humans” (OpenAI, 2022); ?: use of instruction fine-\ntuning is uncertain based on documentation.\nA.2 Additional Details Related to Datasets\nUsed in the Study\nA.2.1 Preprocessing Details for Moral Stories\nDataset\nEach example in Moral Stories consists of a moral\nnorm (a normative expectation about moral behav-\nior), a situation which describes the state of some\ncharacters, an intent which describes what a partic-\nular character wants, and two paths, a moral path\nand immoral path. Each path consists of a moral\nor immoral action (an action following or violating\nthe norm) and a moral or immoral consequence\n(a likely outcome of the action). For the present\nexperiments, I construct scenarios as the string con-\ncatenation of an example’s situation, intent, and\neither moral action or immoral action. We do not\nuse the consequences or norms, as they often in-\nclude a reason why the action was moral/immoral,\nand thus could bias the moral foundational contents\nof the completions.\nWe used 2,000 scenarios produced from\nthe Moral Stories dataset, consisting of 1,000\nrandomly-sampled moral scenarios and 1,000\nrandomly-sampled immoral scenarios.\n293\nA.2.2 Preprocessing Details for ETHICS\nDataset\nThe ETHICS dataset contains five subsets of data,\neach corresponding to a particular ethical frame-\nwork (deontology, justice, utilitarianism, common-\nsense, and virtue), each further divided into a “train”\nand “test” portion. For the present experiments, I\nuse the “train” split of the “commonsense” portion\nof the dataset, which contains 13,910 examples of\nscenarios paired with ground-truth binary labels of\nethical acceptability. Of these, 6,661 are “short”\nexamples, which are 1-2 sentences in length. These\nshort examples were sourced from Amazon Me-\nchanical Turk workers and consist of 3,872 moral\nexamples, and 2,789 immoral examples. From\nthese, I randomly select 1,000 examples split evenly\naccording to normative acceptability, resulting in\n500 moral scenarios and 500 immoral scenarios.\nThe train split of the commonsense portion of the\nETHICS dataset also contains 7,249 “long” exam-\nples, 1-6 paragraphs in length, which were obtained\nfrom Reddit. These were unused in the present ex-\nperiment, primarily due to the increased costs of\nusing longer scenarios.\nA.2.3 Preprocessing Details for Social\nChemistry Actions Dataset\nThe Social Chemistry 101 (Forbes et al., 2020)\ndataset contains 355,922 structured annotations of\n103,692 situations, drawn from four sources (Dear\nAbby, Reddit AITA, Reddit Confessions, and sen-\ntences from the ROCStories corpus; see (Forbes\net al., 2020) for references). Situations are brief\ndescriptions of occurrences in everyday life where\nsocial or moral norms may dictate behavior, for\nexample “pulling out of a group project at the last\nminute”. Situations are annotated with Rules-of-\nThumb (RoTs), which are judgements of actions\nthat occur in the situation, such as “It’s bad to not\nfollow through on your commitments”. Some sit-\nuations may contain more than one action, but I\nconsider situations that are unanimously annotated\nas having only one action for the present experi-\nment, as this simplifies interpretation of the moral\nfoundation annotations. RoTs in the dataset are\nannotated with “RoT breakdowns”. RoT break-\ndowns parse each RoT into its constituent action\n(e.g. “not following through on commitments”) and\njudgement (“it’s bad”). Judgements are standard-\nized to five levels of approval/disapproval: very\nbad, bad, expected/OK, good, very good. I discard\nactions labeled with “expected/OK”, and collapse\n“very bad” and “bad” together, and “very good” and\n“good” together to obtain actions annotated with\nbinary normative acceptability. Actions are also\nannotated with moral foundation labels (the exam-\nple in the previous sentence was annotated with\nthe Fairness/Cheating and Loyalty/Betrayal foun-\ndations). Additionally, each RoT belongs to one of\nthe following categories - morality-ethics, social-\nnorms, advice, description. I use RoTs belonging\nto the “morality-ethics” category, since this is the\ncategory indicating that the RoT contains moral\nreasoning rather than advice or etiquette recom-\nmendations. After filtering RoTs and situations by\ncategory, and selecting examples with unanimous\nratings for moral foundation and normative accept-\nability, I obtain a dataset of 1300 actions - 130\nnormatively moral actions and 130 normatively im-\nmoral actions for each of the five moral foundations.\nThese scenarios are used in the experiment related\nto Criterion A in Section 2.1.\nA.2.4 Preprocessing Details for Social\nChemistry Situations Dataset\nCriterion B requires comparing PH(ef |s) and\nPLM (ef |s), for human- and LLM-written open-\nended text responses containing moral reasoning\nabout some scenarios. I use situations from the So-\ncial Chemistry 101 dataset (Forbes et al., 2020), and\nuse the human-written RoTs to estimate PH(ef |s)\nusing the moral foundations dictionaries. To es-\ntimate consensus human judgement CH(s), I use\nsituations that are multiply annotated. Specifically,\nI filter the Social Chemistry 101 dataset to situa-\ntions with 4 or more RoTs, and 4 or more RoT\nbreakdowns per RoT. This results in a corpus of\n170 scenarios. Unlike the Social Chemistry Actions\ndataset, this Social Chemistry Situations dataset is\nnot foundationally-balanced - I encountered a trade-\noff between the minimum number of annotations\nper situation, and the final corpus size - balancing\nthe dataset in terms of foundations would have re-\nduced the dataset size further. The set of scenarios\nis used for the experiment related to Criterion B in\nSection 2.1.\nA.3 Additional Details Related to Moral\nFoundations Dictionaries\nA.4 Additional Details Related to Prompt\nConstruction\nTemplates from Table 2 were used for the Moral\nStories, ETHICS, and Social Chemistry Situations\ndatasets, where the scenarios are longer descrip-\n294\nFigure 5: Venn diagram of word overlap between\nMFDv1, MFDv2 and eMFD. Since some entries in\nMFDv1 are regexes, I represent MFDv1 in this diagram\nby all non-compound words in WordNet matching a\nregex in MFDv1.\ntions of events, with length one sentence or longer.\nTemplates from Table 3 were used for the Social\nChemistry Actions dataset, where scenarios are\nbrief action descriptions (sentence fragments). This\nwas done to ensure grammaticality.\nTemplate StyleStanceTemplate1 moral[SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], this is a moral thing to do because1 immoral[SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], this is an immoral thing to because2 moral[SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here are the moral arguments for why this is a moral thing to do:2 immoral[SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here are the moral arguments for why this is an immoral thing to do:3 moral[SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is acceptable behavior:3 immoral[SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is unacceptable behavior:4 moral[SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is a good thing to do:4 immoral[SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is a bad thing to do:5 moral[SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is the right thing to do:5 immoral[SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is the wrong thing to do:\nTable 2: Prompt template styles for situations\nTemplate StyleStanceTemplate1 moralAs a/an [POLITICAL_IDENTITY_PHRASE], [ACTION] is a moral thing to do because1 immoralAs a/an [POLITICAL_IDENTITY_PHRASE], [ACTION] is an immoral thing to because2 moralAs a/an [POLITICAL_IDENTITY_PHRASE], here are the moral arguments for why [ACTION] is a moral thing to do:2 immoralAs a/an [POLITICAL_IDENTITY_PHRASE], here are the moral arguments for why [ACTION] is an immoral thing to do:3 moralAs a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is acceptable behavior:3 immoralAs a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is unacceptable behavior:4 moralAs a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is a good thing to do:4 immoralAs a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is a bad thing to do:5 moralAs a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is the right thing to do:5 immoralAs a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is the wrong thing to do:\nTable 3: Prompt template styles for actions\nB Appendix B: Additional Experimental\nResults\nB.1 Effect Size vs. Dataset\nFigure 6 shows effect sizes for liberal vs. conser-\nvative prompting, based on completions obtained\nfrom 2000 scenarios produced from Moral Stories\nand 1000 scenarios produced from ETHICS. Scores\nFigure 6: Effect sizes, liberal vs. conservative prompt\nidentity, by dataset and dictionary\nare separated by dictionary and dataset. See Section\n2 for the methods used to calculate effect sizes.\nEffect sizes and directions are consistent\nacross datasets for the Care/Harm and Author-\nity/Subversion foundations.\nB.2 Effect Size vs. Prompt Template Style\nFigure 7 shows the results obtained from analysis of\ncompeletions obtained from five different prompt\nstyles, as described in 2.\nEffects of liberal vs. conservative political iden-\ntity are uniform in direction for the Care/Harm and\nAuthority/Subversion foundations. Regardless of\nthe prompt style or dictionary used, the comple-\ntions contain more Care/Harm words when the lib-\neral political identity is used, and more Author-\nity/Subversion words when the conservative polit-\nical identity is used. Effects are nearly uniform\nin direction for the Fairness/Cheating foundation,\nwith liberal political identity resulting in increased\nuse of this foundation for thirteen of fifteen com-\nbinations of prompt style and dictionary. Liberal\nprompting resulted in decreased use of the Fair-\nness/Cheating foundation for prompt styles 1 and\n2, when measured using MFDv2.\n295\nFigure 7: Effect sizes, liberal vs. conservative prompt\nidentity, by prompt style and dictionary.\nResults for the Sanctity/Degradation and Loy-\nalty/Betrayal foundations are more varied. Effect\ndirections are uniform for the Sanctity/Degradation\nfoundation when measured with MFDv2 - lib-\neral political identity results in lower Sanc-\ntity/Degradation use by 1-2 percent score across\nall prompt styles. Effects on Sanctity/Degradation\nare less consistent when measured using MFDv1\nor eMFD - liberal prompting resulted in decreased\nuse of Sanctity/Degradation words for only three\nout of five prompt styles. Measured by the eMFD,\nliberal prompting results in decreased use of Sanc-\ntity/degradation words for four of five prompt\nstyles.\nEffect directions are uniform for Loy-\nalty/Betrayal when measured with MFDv1 -\nprompting with liberal political identity results in\ngreater percent scores for Loyalty for all prompt\nstyles. Results are varied when measured with\nMFDv1 - liberal prompting results in decreased\nuse for only three of five prompt styles. When mea-\nsured using the eMFD, liberal prompting results\nin decreased or equal use of the Loyalty/Betrayal\nfoundation across the prompt styles, which is\nconsistent within the dictionary, but is opposite in\neffect direction in comparison to MFDv1.\n296\nC Appendix C: LLM Output Examples\nFigure 8: Examples of completions obtained from Moral Stories dataset, from OpenAI models of increasing size.\nExamples were randomly selected\n297",
  "topic": "Mimicry",
  "concepts": [
    {
      "name": "Mimicry",
      "score": 0.7009049654006958
    },
    {
      "name": "Social cognitive theory of morality",
      "score": 0.6121004819869995
    },
    {
      "name": "Politics",
      "score": 0.608056902885437
    },
    {
      "name": "Moral reasoning",
      "score": 0.5099595785140991
    },
    {
      "name": "Moral disengagement",
      "score": 0.46819576621055603
    },
    {
      "name": "Social psychology",
      "score": 0.4539607763290405
    },
    {
      "name": "Identity (music)",
      "score": 0.4357920289039612
    },
    {
      "name": "Moral development",
      "score": 0.42238619923591614
    },
    {
      "name": "Sociology",
      "score": 0.4128755033016205
    },
    {
      "name": "Epistemology",
      "score": 0.3924143612384796
    },
    {
      "name": "Psychology",
      "score": 0.3784256875514984
    },
    {
      "name": "Political science",
      "score": 0.2579346299171448
    },
    {
      "name": "Law",
      "score": 0.15732476115226746
    },
    {
      "name": "Aesthetics",
      "score": 0.09974762797355652
    },
    {
      "name": "Philosophy",
      "score": 0.07768452167510986
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I84218800",
      "name": "University of California, Davis",
      "country": "US"
    }
  ],
  "cited_by": 26
}