{
    "title": "Keyphrase Prediction With Pre-trained Language Model",
    "url": "https://openalex.org/W3017406139",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1930211867",
            "name": "Liu Rui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2009932017",
            "name": "Lin Zheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1926383388",
            "name": "Wang Wei-ping",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2949877232",
        "https://openalex.org/W2949041962",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W2914076857",
        "https://openalex.org/W1525595230",
        "https://openalex.org/W2976344774",
        "https://openalex.org/W2954579984",
        "https://openalex.org/W1528825546",
        "https://openalex.org/W2888593125",
        "https://openalex.org/W32253530",
        "https://openalex.org/W2948655914",
        "https://openalex.org/W3098636015",
        "https://openalex.org/W2950265271",
        "https://openalex.org/W2969740599",
        "https://openalex.org/W2296283641",
        "https://openalex.org/W2963531963",
        "https://openalex.org/W2949360683",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2030903088"
    ],
    "abstract": "Recently, generative methods have been widely used in keyphrase prediction, thanks to their capability to produce both present keyphrases that appear in the source text and absent keyphrases that do not match any source text. However, the absent keyphrases are generated at the cost of the performance on present keyphrase prediction, since previous works mainly use generative models that rely on the copying mechanism and select words step by step. Besides, the extractive model that directly extracts a text span is more suitable for predicting the present keyphrase. Considering the different characteristics of extractive and generative methods, we propose to divide the keyphrase prediction into two subtasks, i.e., present keyphrase extraction (PKE) and absent keyphrase generation (AKG), to fully exploit their respective advantages. On this basis, a joint inference framework is proposed to make the most of BERT in two subtasks. For PKE, we tackle this task as a sequence labeling problem with the pre-trained language model BERT. For AKG, we introduce a Transformer-based architecture, which fully integrates the present keyphrase knowledge learned from PKE by the fine-tuned BERT. The experimental results show that our approach can achieve state-of-the-art results on both tasks on benchmark datasets.",
    "full_text": "Keyphrase Prediction With Pre-trained Language Model\nRui Liu1,2 , Zheng Lin1 , Weiping Wang1\n1Institute of Information Engineering, Chinese Academy of Sciences\n2School of Cyber Security, University of Chinese Academy of Sciences\n{liurui1995, linzheng, wangweiping}@iie.ac.cn,\nAbstract\nRecently, generative methods have been widely\nused in keyphrase prediction, thanks to their capa-\nbility to produce both present keyphrases that ap-\npear in the source text and absent keyphrases that\ndo not match any source text. However, the absent\nkeyphrases are generated at the cost of the perfor-\nmance on present keyphrase prediction, since pre-\nvious works mainly use generative models that rely\non the copying mechanism and select words step\nby step. Besides, the extractive model that directly\nextracts a text span is more suitable for predicting\nthe present keyphrase. Considering the different\ncharacteristics of extractive and generative meth-\nods, we propose to divide the keyphrase prediction\ninto two subtasks, i.e., present keyphrase extraction\n(PKE) and absent keyphrase generation (AKG), to\nfully exploit their respective advantages. On this\nbasis, a joint inference framework is proposed to\nmake the most of BERT in two subtasks. For PKE,\nwe tackle this task as a sequence labeling prob-\nlem with the pre-trained language model BERT. For\nAKG, we introduce a Transformer-based architec-\nture, which fully integrates the present keyphrase\nknowledge learned from PKE by the Ô¨Åne-tuned\nBERT. The experimental results show that our ap-\nproach can achieve state-of-the-art results on both\ntasks on benchmark datasets.\n1 Introduction\nKeyphrase prediction aims to automatically obtain many con-\ndensed phrases or words, which can highly summarize the\nprimary information of a document. A solution to this task is\nessential for numerous downstream NLP tasks, e.g., recom-\nmendation, information retrieval [Ushiku et al., 2017 ], and\nsummarization [Pasunuru and Bansal, 2018]. In practical ap-\nplications, people can quickly gain the required content from\nthe Internet through keyphrases.\nExisting keyphrase prediction approaches mostly focus on\neither extractive or generative methods. Extractive methods\naim to select present keyphrases (e.g., ‚Äúabstract machines‚Äù\nin Figure 1), which appear in the document. However, an\nissue of these methods is that they cannot produce absent\nDocument:(total 173 words)\nOn the syntactic and functional correspondence between\nhybrid (or layered) normalisers and abstract machines.\nWe show how to connect the syntactic and the functional\ncorrespondence for normalisers and abstract machines\nimplementing hybrid (or layered) reduction strategies,\n... Many fundamental strategies in the literature are hy-\nbrid, in particular, many full reducing strategies ... If we\nfollow the standard program transformationsteps the ...\nHowever, a solution is possible based on establishing the\nshape invariant of well formed continuation stacks. We il-\nlustrate the problem and the solution with the derivation\nof substitution based ... The machine we obtain is a sub-\nstitution based, eval apply, open terms version of Pierre\ncregut‚Äôs...\nPresent Keyphrases: abstract machines; reduction strate-\ngies; program transformation\nAbsent Keyphrases: operational semantics; full reduc-\ntion\nFigure 1: A sample document with labeled keyphrases. The present\nkeyphrases are shown in bold. There are no present keyphrases in\nthe sentences underlined in blue.\nkeyphrases (e.g., ‚Äúoperational semantics‚Äù in Figure 1), which\ndo not exist in the document. Another line of works [Meng\net al., 2017; Chen et al., 2019b] treats the keyphrase predic-\ntion as a sequence-to-sequence learning problem and uses the\nencoder-decoder framework to generate present and absent\nkeyphrases simultaneously. These works reveal that adopting\nthe copy mechanism is more effective than directly generat-\ning words from the vocabulary. However, the copying mech-\nanism generates a word at each time step and does not take\ndependencies between the selected words into consideration.\nMeanwhile, Chen et al. [2019a] focused on improving the\nperformance of the generative model with the assistance of an\nextractive model. Nevertheless, instead of directly extracting\nkeyphrases from the original document, their proposed ex-\ntraction model aimed to identify the importance of each word\nin the document, and the importance score was used to assist\nthe generation of keyphrases. As a result, the potential of the\nextractive model has not been fully exploited.\nTo fully exploit the power of extraction and generation, we\ndivide the keyphrase prediction problem into two processes:\npresent keyphrase extraction (PKE) and absent keyphrase\ngeneration (AKG). For PKE, we address this work as a se-\narXiv:2004.10462v1  [cs.CL]  22 Apr 2020\nquence labeling problem using a BiLSTM-CRF architecture;\nmeanwhile, we employ the pre-trained model BERT [Devlin\net al., 2019] to obtain the contextual embedding. Moreover,\nthere are some sentences in the document that do not con-\ntain present keyphrases, and these noisy data can impair the\nperformance of PKE. To tackle the issue above, we design\na sentence Ô¨Ålter module to select sentences that may contain\npresent keyphrases to extract keyphrases more accurately.\nFor AKG, we exploit the extractive information from the\nshared BERT model Ô¨Åne-tuned on the PKG task. Further-\nmore, the present keyphrase information provides an explicit\nsummary of the topic of the article, and it can be used to\nguide the generation of absent keyphrases. To achieve this\ngoal, we employ a Transformer-based model [Vaswani et al.,\n2017] with the copying mechanism. Rather than simply us-\ning the Ô¨Ånd-tuned BERT as the encoder, we propose a gated\nfusion attention module, in which we use Transformer en-\ncoder representations to interact with the BERT representa-\ntions. Afterwards, a gated fusion layer is introduced to fuse\nthe present keyphrase knowledge and Transformer encoder\nrepresentations.\nThe main contributions of this paper are listed as follows:\n‚Ä¢We divide the keyphrase prediction task into two sub-\ntasks, i.e., PKE and AKG, to combine the advantages\nof both extractive models and generative models. To\nbe speciÔ¨Åc, the extractive model thoroughly considers\nthe dependencies between words to enhance the perfor-\nmance. Meanwhile, additional present keyphrase infor-\nmation enables the generation model to generate absent\nkeyphrases that are close to the topic.\n‚Ä¢A shared BERT is utilized in the two subtasks (PKE and\nAKG) to beneÔ¨Åt from the prior knowledge learned from\ndata-rich corpora. SpeciÔ¨Åcally, to take advantage of the\nBERT Ô¨Åne-tuned on PKE, we propose a gated fusion at-\ntention module to integrate the present keyphrase infor-\nmation in AKG. In this way, both the PKE and AKG\ntasks can be further reÔ¨Åned through the pre-trained lan-\nguage model.\n‚Ä¢Experimental results on three benchmark datasets show\nthat our model outperforms the state-of-the-art models\nsigniÔ¨Åcantly.\n2 Model\nThe structure of our model, including two submodels (i.e.,\nBERT-PKE and BERT-AKG), is shown in Figure 2. During\ntraining, we Ô¨Årst train our labeling model into convergence.\nThen, the generative model is trained with BERT that is Ô¨Åne-\ntuned on our present keyphrase extraction task. In testing, we\njointly conduct two subtasks, a document is converted into\nhidden states via BERT encoder and Transformer encoder re-\nspectively, then we simultaneously extract present keyphrases\nand generate absent keyphrases.\n2.1 Problem DeÔ¨Ånition\nGiven a document x = {x1,...,x Lx }, the goal is to obtain\nkeyphrases Y= {y1 ..., yM }including present keyphrases\nand absent keyphrases, where Lx is the length of the docu-\nment, and M is the number of keyphrases.\n2.2 Present Keyphrase Extraction\nThe overall framework of our sequence labeling model (i.e.,\nBERT-PKE) consists of two components: (i) BERT-based\nsentence Ô¨Ålter, (ii) BiLSTM-CRF sequence labeling module.\nBERT-based Sentence Filter\nGiven a document x, we Ô¨Årst split it into some sentences\nS = {sent1,...,sent Ls }by punctuation marks, where Ls\nis the number of the sentences. To Ô¨Ålter noisy sentences\nwhich do not contain present keyphrases, we add special to-\nkens [CLS] and [SEP] at the start and the end of the sentence\nrespectively, inspired by [Liu and Lapata, 2019 ]. BERT en-\ncodes the processed document into contextual representations\nH. The vector h[cls] of the token [CLS] before each sentence\nis used as the sentence representation. We denote those rep-\nresentations as G0 = {g0\ni }Ls\ni=1, where g0\ni is the vector of the\ni-th sentence. The sentence representations G0 are then fed\ninto 2-layer Transformer blocks. Each block contains two\nsub-layers: a multi-head self-attention layer and a fully con-\nnected feed-forward network:\nGl = FFN(MultiHeadAtt(Gl‚àí1,Gl‚àí1,Gl‚àí1)). (1)\nThe three inputs of the multi-head self-attention layer are\nquery matrix, key matrix and value matrix from left to right.\nA residual connection is employed around each of the two\nsub-layers, followed by layer normalization.\nAfterwards, we obtain the conÔ¨Ådence score for each sen-\ntence through a sigmoid function:\nscore = œÉ(wT G2) ‚ààRLs . (2)\nAccording to the sentence scores, we choose the top-K candi-\ndate sentences for the subsequent sequence labeling process.\nHere, K is set to 7, and we investigate the inÔ¨Çuence of the\nhyperparameter K in section 3.6. Each sentence is associated\nwith a label Àúyi ‚àà{0,1}, indicating whether the senti con-\ntains any present keyphrase. We can train the sentence Ô¨Ålter\nby minimizing the negative log-likelihood loss:\nLf = ‚àí\nLs‚àë\ni=1\nÀúyi log scorei. (3)\nwhere Ls is the number of the sentences in the document.\nBi-LSTM CRF Sequence Labeling Architecture\nThe contextualized vectors of the words from the selected\nsentences are fed into a BiLSTM to add sufÔ¨Åcient expres-\nsive power. Then, a conditional random Ô¨Åeld (CRF) scores\nand labels the output of the BiLSTM network. As described\nin [Lample et al., 2016], given an input documentx, the score\nof the sequence of tag predictions t can be deÔ¨Åned as:\ns(x,t) =\nn‚àë\ni=0\nAti,ti+1 +\nn‚àë\ni=1\nPi,ti , (4)\nwhere Ai,j is the transition score from tag ito tag j, and Pi,j\nis the score of the j-th tag of the i-th word. The CRF model\ncan be trained by minimizing the negative log-probability of\nFor example , nuclear physics , and deep learning\nBERT\nSentence \nFilter\nL x\nL x\nL x\nBi-LSTM CRF\n0  1  1 O B I O O O O B I O 1 - gate\ngate\nTFEnc\nTFDec\nFusion attention\nSelf-\nAttention\nFeed Forward\nMulti-Head \nAttention\nFeed Forward\nSelf-\nAttention\nFeed Forward\nEnc-Dec \nAttention\nnuclear deep learningphysics\nùë¶ùë°\nùë¶1 ‚Ä¶ùë¶ùë°‚àí1\nExtraction\nGeneration\nùëí4 ùëí5 ùëí6 ùëí7 ùëí8 ùëí9ùëí3ùëí2ùëí1ùëí[ùëêùëôùë†] ùëí[ùë†ùëíùëù] ùëí[ùëêùëôùë†] ùëí[ùë†ùëíùëù] ùëí[ùëêùëôùë†] ùëí[ùë†ùëíùëù]\n‚Ñé[ùëêùëôùë†] ‚Ñé2‚Ñé1 ‚Ñé5‚Ñé4 ‚Ñé6‚Ñé3 ‚Ñé9‚Ñé8‚Ñé7 ‚Ñé[ùë†ùëíùëù]‚Ñé[ùë†ùëíùëù] ‚Ñé[ùëêùëôùë†]‚Ñé[ùëêùëôùë†] ‚Ñé[ùë†ùëíùëù]\nFigure 2: The architecture of our proposed model. The ei and hi denote the embedding vector and the bert representation of the i-th word\nrespectively. Here, yt is the word predicted by the generative model at the t-th step according to the previously generated sequence.\nthe ground-truth tag sequence t:\nLc = ‚àílog(es(x,t)/\n‚àë\nÀÜt‚ààT\nes(x,ÀÜt))\n= ‚àís(x,t) +log(\n‚àë\nÀÜt‚ààT\nes(x,ÀÜt)).\n(5)\nThe best sequence path can be found using the Viterbi decod-\ning algorithm.\nIn this work, rather than training the BiLSTM-CRF mod-\nule on sentences selected by the sentence Ô¨Ålter module, we\ntrain it exclusively on ground-truth positive sentences that\ncontain present keyphrases. This strategy removes most of\nthe responsibility of content selection and allows the module\nto focus its efforts on labeling the document. During testing,\nwe Ô¨Årst utilize the sentence Ô¨Ålter to select top- K sentences\nby calculating the conÔ¨Ådence score. Thereafter, BiLSTM-\nCRF tags those selected sentences. Finally, we extract all the\npresent keyphrases according to the IOB format [Lample et\nal., 2016].\nThe Ô¨Ånal loss of the overall extractive model BERT-PKE\ncan be expressed as: LPKE = Lf + Lc.\n2.3 Absent Keyphrase Generation\nThe basic architecture of BERT-AKG is Transformer\n[Vaswani et al., 2017 ], which consists of an encoder and a\ndecoder. Encoder and decoder both contain L-layer Trans-\nformer blocks. But the Transformer model has its vocabu-\nlary Aand the words in this vocabulary are not tokenized\nby WordPiece. Besides, the BERT model we used is Ô¨Åne-\ntuned on the present keyphrase extraction task. To prevent\nthe shared BERT from forgetting the knowledge of present\nkeyphrases, we do not train the BERT model with the Trans-\nformer. In other words, we only treat the output vector of the\nshared BERT as a Ô¨Åxed supplementary knowledge to guide\nthe generation procedure.\nTransformer Encoder with Fusion Attention Module\nGiven a document x, the Transformer encodes it into U. We\ndenote the word embedding of the document x as U0 =\nEmbedding(x), and in the l-th layer:\nUl = FFN(MultiHeadAtt(Ul‚àí1,Ul‚àí1,Ul‚àí1)). (6)\nMeanwhile, the BERT model encodes x into representation\nH. We adopt another multi-head attention module with L\nlayers to Ô¨Ånd the useful information of BERT representations\nwhich are conducive to generate absent keyphrases:\nÀÜUl = FFN(MultiHeadAtt( ÀÜUl‚àí1,H,H)), (7)\nwhere ÀÜU0 = UL. Then, we use a soft gating weight to effec-\ntively merge the integrated BERT representation ÀÜUL with the\nTransformer encoding representation UL.\ngate= œÉ(Wu[UL; ÀÜUL]), (8)\nV = gate‚äôUL + (1‚àígate) ‚äôÀÜUL, (9)\nwhere V is the Ô¨Ånal encodings of the document x, and ‚äôis\nan element-wise multiplication.\nTransformer Decoder\nThe Transformer decoder is also composed of a stack of L\nidentical layers. Except the self-attention sub-layer in the\nencoder module, each decoder layer contains another multi-\nhead encoder-decoder attention sub-layer to perform attention\nover the output representationV of the encoder stack. We de-\nnote the input of the decoder asD0. Each decoder block is as\nfollows:\nCl = MultiHeadAtt(Dl‚àí1,Dl‚àí1,Dl‚àí1), (10)\nDl = FFN(MultiHeadAtt(Cl,V ,V )), (11)\nwhere, Dl is the output of the l-th decoder block.\nTo further improve the generation ability of the model, we\nincorporate the copying mechanism [See et al., 2017 ] with\nthe Transformer decoder, where the attention distribution at\nfrom the last decoding layer indicates the probability of copy-\ning a word from the source text. Hence, the Ô¨Ånal predicted\ndistribution P at time step tcan be computed as:\nP = pgenPvocab + (1‚àípgen)\n‚àë\ni:wi=w\nai\nt, (12)\nwhere pgen = œÉ(wT\nd dL\nt + bt) ‚àà[0,1] is a switch that con-\ntrols the probability of generating a word from the vocab-\nulary Aor copying a word from the document, and dL\nt is\nt-step‚Äôs output vector of the decoder. The vocabulary dis-\ntribution Pvocab over the Ô¨Åxed vocabulary Ais computed as\nPvocab = softmax(WvdL\nt + bv).\nWe train the generator by minimizing the cross entropy\nloss:\nLAKG = ‚àí\n|y|‚àë\nt=1\nlog P(yt|y1...t‚àí1,x), (13)\nwhere yt is the t-th word of keyphrasey, and |y|is the length\nof ground-truth keyphrase y.\n3 Experiment\n3.1 Dataset\nWe choose three datasets of scholarly documents for\nevaluation, which includes KP20k [Meng et al., 2017 ],\nNUS [Nguyen and Kan, 2007 ], and Krapivin [Krapivin et\nal., 2009]. KP20k is a large-scale scholarly articles dataset\nwith 528K articles for training, 20K articles for validation and\n20K articles for testing. All the models are trained with the\ndata from KP20k. We conduct zero-shot evaluations on the\nremaining two datasets following the previous work [Meng\net al., 2017; Chen et al., 2019b]. The statistics of the three\ndatasets are shown in Table 1.\nDataset #Total #Training #Testing\nKP20k 567,830 527,830 20,000\nKrapivin 2,304 1904 400\nNUS 211 - 211\nTable 1: The statistics of three datasets.\n3.2 Implementation Details\nDue to the limitations of time and GPU resources, all of our\nmodels are built on the BERTbase model. All the models are\ntrained on 3 GTX 1080Ti GPUs. During training and test-\ning procedures, the maximum length of the document is 512.\nMoreover, we convert all the documents into lowercase and\nreplace digits with token <digit>. We train our model using\nan Adam optimizer with a learning rate of 0.001, Œ≤1 = 0.9,\nŒ≤2 = 0.998 and œµ = 10‚àí9. A dropout rate of 0.1 is ap-\nplied to avoid overÔ¨Åtting. Gradient clipping is utilized with a\nmaximum norm of 2.0. For PKE, the dimension of the BiL-\nSTM hidden states is set to 512, and we use a linear warm-up\nstrategy with 1000 warm-up steps. For AKG, the encoder\nand decoder of the Transformer model are all composed of\nL = 4 layers, H = 768 hidden size and A = 8 attention\nheads. The warm-up step is set to 8000. For the evaluation\nof absent keyphrase generation, the beam size is set to 200\non three datasets. We set beam depth to 6. We implement\nour model with OpenNMT and the Pytorch implementation\nof BERT. Our code will be released on GitHub.\n3.3 Baseline Models and Evaluation Metrics\nWe compare our models with three extractive algorithms\n(Tf-Idf, TextRank, BiLSTM-CRF [Alzaidy et al., 2019 ])\nand six state-of-the-art generative baselines, including Copy-\nRNN [Meng et al., 2017 ], TG-Net [Chen et al., 2019b ],\nKG-KE-KR [Chen et al., 2019a ], CatSeqTG-2RF [Chan\net al., 2019 ], KG-GAN [Swaminathan et al., 2019 ] and\nParaNetT +CoAtt [Zhao and Zhang, 2019 ]. Following the\nprevious works [Chen et al., 2019b; Meng et al., 2017], we\nadopt the macro-averaged precision, recall and F-measure\n(F1) as evaluation metrics. In the present extraction task,\nthe generation model using beam search ranks the results,\nwhile the results of the labeling model are consistent with\ntheir original position in the document. Therefore, the Ô¨Årst\nkkeyphrases extracted by labeling models cannot be directly\nused to calculate F@k. We use F1@M as the evaluation met-\nric following [Chan et al., 2019], where F1@M computes an\nF1 score by comparing all the keyphrase predictions with the\nground-truth, i.e., k= the number of predictions.\n3.4 Main Results and Analysis\nIn this section, we evaluate our model on two subtasks, i.e.,\nPKE and AKG. We conduct experiments to demonstrate the\neffectiveness of our approach by comparing it with several\nstate-of-the-art methods on three benchmark datasets.\nPresent Keyphrase Extraction\nTable 2 presents the results of present keyphrase extraction on\nthree datasets. We Ô¨Ånd that our proposed model outperforms\nall the generative models and conventional extractive models\nby a large margin, and our model achieves the highest scores\nin all test datasets. In particular, our model achieves an im-\nprovement of 15.6%, 13.2% and 10.6% over the state-of-the-\nart model ‚ÄúCopyRNN‚Äù, ‚ÄúTG-Net‚Äù, and ‚ÄúKG-KE-KR‚Äù in the\nlarge-scale dataset KP20k respectively. Although the gener-\native models have strong generative capabilities and are as-\nsisted with the copying mechanism, the results still show that\nour model is more capable of extracting present keyphrases\nfrom the original document.\nAbsent Keyphrase Generation\nThe results of the absent keyphrase generation measured by\nR@50 is shown in Table 3. The results reveal that our\nmodel consistently outperforms other baseline methods in all\nthe test datasets again. For example, our model achieves\nan improvement of 24.7% and 13.2% over ‚ÄúTG-Net‚Äù and\n‚ÄúKG-KE-KR,‚Äù respectively in KP20k. Besides, the vanilla\nTransformer model performs better than RNN-based mod-\nels. Note that our fusion module effectively integrates present\nkeyphrase information and BERT knowledge into the Trans-\nformer model, bringing substantial improvements on the\nmodel performance.\n3.5 Ablation Study\nHere we conduct some ablation studies for PKE and AKG\nto explore the effectiveness of our proposed methods. The\nrelevant results of our models are shown in Table 4.\nPKE Ablation. Table 4 illustrates that removing our sen-\ntence Ô¨Ålter module results in a signiÔ¨Åcant decline in the per-\nformance. This suggests that our sentence Ô¨Ålter module is a\ncore component of our model, which can ameliorate the in-\nÔ¨Çuences of noisy sentences to improve the labeling ability of\nModel KP20k NUS Krapivin\nF1@5 F1@10 F1@5 F1@10 F1@5 F1@10\nTF-IDF 0.105 0.130 0.139 0.181 0.113 0.143\nTextRank 0.180 0.150 0.195 0.190 0.172 0.147\nCopyRNN [Meng et al., 2017] 0.378 0.310 0.418 0.369 0.339 0.281\nTG-Net [Chen et al., 2019b] 0.386 0.321 0.425 0.368 0.356 0.289\nKG-KE-KR [Chen et al., 2019a] 0.395 0.325 0.421 0.377 0.355 0.287\nCatSeqTG-2RF [Chan et al., 2019] 0.385 - 0.422 - 0.369 -\nParaNetT +CoAtt [Zhao and Zhang, 2019] 0.360 0.289 0.360 0.350 0.329 0.282\nKG-GAN [Swaminathan et al., 2019] 0.370 - 0.401 - 0.357 -\nBiLSTM-CRF [Alzaidy et al., 2019]‚Ä† 0.335 0.351 0.316\nBERT-PKE‚Ä† 0.437 0.447 0.407\nTable 2: The performance of present keyphrase prediction on three testing datasets. We highlight the best results in bold. ‚Ä†We report the\nresult of F1@M for the sequence labeling models.\nModel KP20k NUS Krapivin\nCopyRNN[Menget al., 2017] 0.222 0.175 0.202\nTG-Net[Chenet al., 2019b] 0.226 0.164 0.169\nKG-KE-KR[Chenet al., 2019a] 0.249 0.190 0.252\nCatSeqTG-2RF[Chanet al., 2019] 0.029 0.026 0.044\nParaNetT+CoAtt[Zhao and Zhang, 2019] 0.228 0.125 0.214\nKG-GAN[Swaminathanet al., 2019] 0.027 0.027 0.037\nTransformer 0.262 0.188 0.241\nBERT-AKG 0.282 0.219 0.268\nTable 3: The performance ( R@50) of absent keyphrase prediction\non three testing datasets. We highlight the best results in bold.\nModel KP20k NUS Krapivin\nExtractive\nBERT-PKE 0.437 0.447 0.407\nw/o Sentence Filter 0.425 0.438 0.391\nReplace CRF with Linear 0.350 0.339 0.267\nGenerative\nBERT-AKG (Fine-tuned‚ÜíFixed BERT) 0.282 0.219 0.268\nw/o BERT 0.262 0.188 0.241\nw/o Fusion Attention 0.257 0.151 0.254\nFine-tuned‚ÜíFine-tuned BERT 0.268 0.182 0.257\nOriginal BERT 0.278 0.209 0.264\nTable 4: Ablation analysis of our extractive and generative approach\non three testing datasets. ‚ÄúFine-tuned ‚ÜíFixed BERT‚Äù means that\nduring the training of the generative model, we Ô¨Åx the parameters of\nBERT which has been Ô¨Åne-tuned on PKE. ‚ÄúFine-tuned‚ÜíFine-tuned\nBERT‚Äù means that we train the BERT Ô¨Åne-tuned on PKE, together\nwith the Transformer model. ‚ÄúOriginal BERT‚Äù denotes replacing\nthe BERT Ô¨Åne-tuned on PKE with an original BERT which is not\nÔ¨Åne-tuned.\nthe model further. Besides, we Ô¨Ånd that the performance de-\ngrades by a large margin if the CRF module is replaced with a\nLinear tagging layer. This indicates that the CRF architecture\nis crucial for capturing the label dependencies in keyphrase\nextraction scenarios.\nAKG Ablation. As shown in Table 4, ‚ÄúFine-tuned‚ÜíFine-\ntuned BERT‚Äù shows that using a Ô¨Åxed BERT as contextual\nfeatures is better than Ô¨Åne-tuning it on AKG. This may be\nbecause excessive training makes BERT forget the useful\nknowledge obtained from PKE. The ‚Äúw/o Fusion attention‚Äù\nresult illustrates that our proposed fusion attention module\ncan better integrate the useful information than simply merge\nthem by a weighted average. Besides, removing the BERT\nencoder deteriorates the performance, which demonstrates\nthat the present keyphrase information and BERT knowledge\ncan facilitate the generation of absent keyphrases. Further-\nmore, we Ô¨Ånd that replacing our BERT Ô¨Åne-tuned on PKE\nwith an original BERT, which is never Ô¨Åne-tuned, also results\nin a worse performance, which implies that the extractive task\ncan provide useful information to promote the performance of\nthe generative task.\nFigure 3: The inÔ¨Çuence of the number of the selected sentences ( F1\nof predicting present keyphrases).\n3.6 Impact of the Number of Selected Sentences\nTo further investigate the inÔ¨Çuence of the hyperparameter K\n(i.e., the number of the selected sentences), we plot F1 curve\nwith respect to different choices of K. As shown in Figure 3,\nwe notice that the curve Ô¨Årst rises and then declines with the\nincrease of K. The possible explanation for this phenomenon\nmight be that the recall and precision of the positive sentences\nreach a balance when K=7. In other words, the recall of pos-\nitive sentences increases as the number of selected sentences\nincreases, but the precision declines accordingly. Hence, the\nnoisy data in the selected sentences is also increasing, leading\nto the error of labeling. There is also a consistent trend in the\nother two datasets.\n3.7 Case Study\nFigure 4 presents a case of the input document and outputs\nof different methods. For convenience, we only choose two\n(1) Macrophages, Oxidation, and Endometriosis. (2) Retrograde menstruation has been suggested to be the cause for the presence of\nendometrial cells in the peritoneal cavity. (3) However, little is known about the events that lead to the adhesion and growth of these\ncells that ultimately ... women despite the common occurrence of retrograde menstruation in most women. (4)We postulate that, in nor-\nmal women, the endometrial cells ... resident tissue macrophages in the peritoneal cavity. (5) In contrast, the peritoneal macrophages in\nwomen with endometriosis are nonadherent and ineffectively scavenged, resulting in the sustained presence and growth of the endome-\ntrial cells. (6) We also postulate that the peritoneal Ô¨Çuid is not a passive reservoir ... , but actively promotes endometriosis. (7) The\nperitoneal Ô¨Çuid is rich in lipoproteins, particularly low density lipoprotein, which generates oxidized... (8) The oxidants exacerbate\nthe growth of endometriosis by inducing chemoattractants such as mcp ... (9) We provide evidence for the presence of oxidative milieu\nin the peritoneal cavity of women with endometriosis, the nonscavenging properties of macrophages that are nonadherent, and the syn-\nergistic interaction ... (10) For example, the peritoneal Ô¨Çuid lipoproteins of subjects with endometriosis have increased the propensity\nto undergo oxidation as compared with plasma lipoproteins, ... (11) If the oxidative proinÔ¨Çammatory nature of the peritoneal Ô¨Çuid is\nan important mediator of endometriosis growth, ... against endometriosis.\nPresent Keyphrases: {macrophages; oxidation; endometriosis; endometrial cells; peritoneal Ô¨Çuid; lipoproteins}\nTG-Net: 1. endometriosis; 2. antioxidants; 3. macrophages; 4. oxidation; 5. oxidative stress; 6. menstruation; ... ; 10. women\nKG-KE-KR: 1. endometriosis; 2. antioxidants; 3. macrophages; 4. oxidation; 5. oxidative stress; ... ; 10. peritoneal Ô¨Çuid;\nBERT-PKE: 1. macrophages; 2. oxidation; 3. endometriosis; 4. retrograde menstruation; 5. endometrial cells; 6. peritoneal Ô¨Çuid\nAbsent Keyphrases: {growth factors; cytokines}\nTG-Net: 1. inÔ¨Çammation; 2. apoptosis; 3. mitochondria; 4. autoimmunity; 5. retrograde oxidation; ... ; 12. cytokines; ...\nKG-KE-KR: 1. inÔ¨Çammation; 2. nonadherent scavenged; 3. nonscavenging scavenged; 4. apoptosis; ...; 8. cytokines; ...\nBERT-AKG: 1. inÔ¨Çammation; 2. endometrial growth; 3. retrograde macrophages; 4. cytokines; ... ; 11. growth factors; ...\nFigure 4: Examples of the generated keyphrases by our approach and other models. The underlined sentences in blue are selected by our\nmodel. Phrases in bold are true keyphrases and we omit some incorrect predicted keyphrases for brevity. In PKE task, our labeling model\nonly extracts six keyphrases, but for the other two baseline models, we exhibits the top 10 results.\nstrong baselines (i.e., TG-Net and KG-KE-KR) for compari-\nson. For PKE, our model can decide the appropriate number\nof keyphrases to be predicted, while the most conventional\ngenerative approaches need to select a Ô¨Åxed number of top-\nranked candidates as the Ô¨Ånal results. According to Figure 4,\nour model only extracts six candidates, Ô¨Åve of which are cor-\nrect answers. In comparison, the two baseline models se-\nlect top-10 keyphrases among which only three are correct.\nFurthermore, all the sentences we selected contain present\nkeyphrases. It indicates that our sentence Ô¨Ålter can effectively\nchoose positive sentences as much as possible to alleviate the\neffect of the noisy sentences. For AKG, all the ground-truth\nabsent keyphrases are included in the results predicted by\nour model, while the two RNN-based models and the vanilla\nTransformer model only predict one of them. We observe that\nour model assigns the keyphrase ‚Äúcytokines‚Äù a higher rank\ncompared with other models. With the help of the present\nkeyphrase knowledge, our model is capable of generating ab-\nsent keyphrase more accurately.\n4 Related Work\nKeyphrase Extraction. Extractive methods aim at extract-\ning present keyphrases from the document. In most unsu-\npervised methods [Wan and Xiao, 2008; Mihalcea and Tarau,\n2004; Medelyan et al., 2009 ], they Ô¨Årst constructed lots of\ncandidate phrases based on some heuristic methods. Then,\nthese candidate phrases were ranked to select those phrases\nwith high scores as the Ô¨Ånal results. Zhang et al. [2016]\nand Alzaidy et al.[2019] tackled the extractive task as a se-\nquence labeling problem. Sun et al. [2019] and Prasad and\nKan [2019] adopted the Graph Neural Networks to extract\nkeyphrases by encoding the graph of a document. However,\na common drawback is that only relying on these extractive\nsystems can not generate absent keyphrases.\nKeyphrase Generation. Generative methods make it pos-\nsible to generate absent keyphrase by modeling the keyphrase\nprediction task as a sequence-to-sequence learning prob-\nlem. Meng et al. [2017] Ô¨Årst built CopyRNN, a seq2seq\nframework with attention and copying mechanism [See\net al., 2017 ]. Subsequently, many variations of Copy-\nRNN appeared. Ye and Wang [2018] investigated a semi-\nsupervised method for exploitation of the unlabeled data.\nCorrRNN [Chen et al., 2018] employed a review mechanism\nto the correlation among keyphrases. TG-Net [Chen et al.,\n2019b] leveraged the information of the title to identify the\nimportant parts of the document. Chen et al.[2019a] focused\non using an extractive model to enhance the performance of\nanother generative model. Wang et al.[2019] utilized the la-\ntent topics of the document to enrich useful features. Zhao\nand Zhang [2019] exploited linguistic constraints to prevent\nthe model from generating overlapping phrases. However, all\nof them cannot break through the limitation of their generat-\ning ability to present keyphrase prediction.\n5 Conclusion\nIn this study, we divide the keyphrase prediction into two sub-\ntasks: PKE and AKG. We introduce a novel joint inference\nframework to make the most of the power of extractive and\ngenerative models. SpeciÔ¨Åcally, we apply a shared BERT in\nthe two subtasks to make full use of the prior knowledge from\nthe pre-trained language model and share useful information\nbetween two subtasks. The proposed generative model em-\nploys the gated fusion attention module to effectively incor-\nporate the updated BERT and Transformer model for better\nperformance on AKG. The experimental results demonstrate\nthat our approach outperforms the state-of-the-art methods on\nboth PKE and absent AKG tasks.\nReferences\n[Alzaidy et al., 2019] Rabah Alzaidy, Cornelia Caragea, and\nC. Lee Giles. Bi-lstm-crf sequence labeling for keyphrase\nextraction from scholarly documents. In WWW, pages\n2551‚Äì2557, 2019.\n[Chan et al., 2019] Hou Pong Chan, Wang Chen, Lu Wang,\nand Irwin King. Neural keyphrase generation via rein-\nforcement learning with adaptive rewards. In ACL, pages\n2163‚Äì2174, 2019.\n[Chen et al., 2018] Jun Chen, Xiaoming Zhang, Yu Wu,\nZhao Yan, and Zhoujun Li. Keyphrase generation with\ncorrelation constraints. In EMNLP, pages 4057‚Äì4066,\n2018.\n[Chen et al., 2019a] Wang Chen, Hou Pong Chan, Piji Li,\nLidong Bing, and Irwin King. An integrated approach for\nkeyphrase generation via exploring the power of retrieval\nand extraction. In NAACL-HLT, pages 2846‚Äì2856, 2019.\n[Chen et al., 2019b] Wang Chen, Yifan Gao, Jiani Zhang, Ir-\nwin King, and Michael R. Lyu. Title-guided encoding for\nkeyphrase generation. In AAAI, pages 6268‚Äì6275, 2019.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understand-\ning. In NAACL, pages 4171‚Äì4186, Minneapolis, Min-\nnesota, 2019. Association for Computational Linguistics.\n[Krapivin et al., 2009] Mikalai Krapivin, Aliaksandr Au-\ntaeu, and Maurizio Marchese. Large dataset for keyphrases\nextraction. Technical report, University of Trento, 2009.\n[Lample et al., 2016] Guillaume Lample, Miguel Balles-\nteros, Sandeep Subramanian, Kazuya Kawakami, and\nChris Dyer. Neural architectures for named entity recog-\nnition. In NAACL-HLT, pages 260‚Äì270, 2016.\n[Liu and Lapata, 2019] Yang Liu and Mirella Lapata. Text\nsummarization with pretrained encoders. In EMNLP,\npages 3721‚Äì3731, 2019.\n[Medelyan et al., 2009] Olena Medelyan, Eibe Frank, and\nIan H. Witten. Human-competitive tagging using auto-\nmatic keyphrase extraction. In EMNLP, pages 1318‚Äì1327,\n2009.\n[Meng et al., 2017] Rui Meng, Sanqiang Zhao, Shuguang\nHan, Daqing He, Peter Brusilovsky, and Yu Chi. Deep\nkeyphrase generation. In ACL, pages 582‚Äì592, 2017.\n[Mihalcea and Tarau, 2004] Rada Mihalcea and Paul Tarau.\nTextrank: Bringing order into text. InEMNLP, pages 404‚Äì\n411, 2004.\n[Nguyen and Kan, 2007] Thuy Dung Nguyen and Min-Yen\nKan. Keyphrase extraction in scientiÔ¨Åc publications. In\nICADL, pages 317‚Äì326, 2007.\n[Pasunuru and Bansal, 2018] Ramakanth Pasunuru and Mo-\nhit Bansal. Multi-reward reinforced summarization with\nsaliency and entailment. In NAACL-HLT, pages 646‚Äì653,\n2018.\n[Prasad and Kan, 2019] Animesh Prasad and Min-Yen Kan.\nGlocal: Incorporating global information in local convo-\nlution for keyphrase extraction. In NAACL-HLT, pages\n1837‚Äì1846, 2019.\n[See et al., 2017] Abigail See, Peter J. Liu, and Christo-\npher D. Manning. Get to the point: Summarization with\npointer-generator networks. In ACL, pages 1073‚Äì1083,\n2017.\n[Sun et al., 2019] Zhiqing Sun, Jian Tang, Pan Du, Zhi-Hong\nDeng, and Jian-Yun Nie. Divgraphpointer: A graph\npointer network for extracting diverse keyphrases. In SI-\nGIR, pages 755‚Äì764, 2019.\n[Swaminathan et al., 2019] Avinash Swaminathan,\nRaj Kuwar Gupta, Haimin Zhang, Debanjan Mahata,\nRakesh Gosangi, and Rajiv Ratn Shah. Keyphrase\ngeneration for scientiÔ¨Åc articles using gans. CoRR,\nabs/1909.12229, 2019.\n[Ushiku et al., 2017] Atsushi Ushiku, Shinsuke Mori, Hiro-\ntaka Kameko, and Yoshimasa Tsuruoka. Game state re-\ntrieval with keyword queries. In SIGIR, pages 877‚Äì880,\n2017.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NeurIPS, pages 5998‚Äì6008, 2017.\n[Wan and Xiao, 2008] Xiaojun Wan and Jianguo Xiao. Sin-\ngle document keyphrase extraction using neighborhood\nknowledge. In AAAI, pages 855‚Äì860, 2008.\n[Wang et al., 2019] Yue Wang, Jing Li, Hou Pong Chan, Ir-\nwin King, Michael R. Lyu, and Shuming Shi. Topic-aware\nneural keyphrase generation for social media language. In\nACL, pages 2516‚Äì2526, 2019.\n[Ye and Wang, 2018] Hai Ye and Lu Wang. Semi-supervised\nlearning for neural keyphrase generation. In EMNLP,\npages 4142‚Äì4153, 2018.\n[Zhang et al., 2016] Qi Zhang, Yang Wang, Yeyun Gong,\nand Xuanjing Huang. Keyphrase extraction using deep\nrecurrent neural networks on twitter. In EMNLP, pages\n836‚Äì845, 2016.\n[Zhao and Zhang, 2019] Jing Zhao and Yuxiang Zhang. In-\ncorporating linguistic constraints into keyphrase genera-\ntion. In ACL, pages 5224‚Äì5233, 2019."
}