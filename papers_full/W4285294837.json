{
  "title": "Emergent Structures and Training Dynamics in Large Language Models",
  "url": "https://openalex.org/W4285294837",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3156757639",
      "name": "Ryan Teehan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3185265269",
      "name": "Miruna Clinciu",
      "affiliations": [
        "University of Edinburgh",
        "Centre de Robotique"
      ]
    },
    {
      "id": "https://openalex.org/A2962956864",
      "name": "Oleg Serikov",
      "affiliations": [
        "AIR Institute",
        "Air University",
        "National Research University Higher School of Economics"
      ]
    },
    {
      "id": "https://openalex.org/A2808175158",
      "name": "Eliza Szczechla",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2202327057",
      "name": "Natasha Seelam",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2145024286",
      "name": "Shachar Mirkin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2807682323",
      "name": "Aaron Gokaslan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2920945696",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W3099668342",
    "https://openalex.org/W3104350794",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3104723404",
    "https://openalex.org/W2045336620",
    "https://openalex.org/W3117696238",
    "https://openalex.org/W1591962818",
    "https://openalex.org/W4205830770",
    "https://openalex.org/W3092292656",
    "https://openalex.org/W3174088532",
    "https://openalex.org/W3099023595",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W3035305735",
    "https://openalex.org/W4205292755",
    "https://openalex.org/W1668605814",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3106325613",
    "https://openalex.org/W4226251122",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W2969372718",
    "https://openalex.org/W4298574599",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2963246595",
    "https://openalex.org/W3197839114",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2885396331",
    "https://openalex.org/W3205187613",
    "https://openalex.org/W79417630",
    "https://openalex.org/W3013563411",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2951941824",
    "https://openalex.org/W4301785137",
    "https://openalex.org/W3170826848",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W1979559232",
    "https://openalex.org/W2983461979",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W3207139774",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3171097489",
    "https://openalex.org/W4285704109",
    "https://openalex.org/W1990687200",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3214210379",
    "https://openalex.org/W3192478068",
    "https://openalex.org/W4287646439",
    "https://openalex.org/W3033072407",
    "https://openalex.org/W2963084773",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2159636675",
    "https://openalex.org/W3119175506",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2914924671",
    "https://openalex.org/W4298392860",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W3173681001",
    "https://openalex.org/W3101284630",
    "https://openalex.org/W3099658661",
    "https://openalex.org/W4300687381",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W4299566640",
    "https://openalex.org/W3034917890",
    "https://openalex.org/W2773956126",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4287597717",
    "https://openalex.org/W2052055672",
    "https://openalex.org/W3099624838",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W2032254348",
    "https://openalex.org/W2963087868",
    "https://openalex.org/W3044818641",
    "https://openalex.org/W3009095382",
    "https://openalex.org/W4237769672",
    "https://openalex.org/W3123796128",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W4236950558",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W1983276223",
    "https://openalex.org/W3175001078",
    "https://openalex.org/W2103714988",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3012981624",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4205276337",
    "https://openalex.org/W2952542841"
  ],
  "abstract": "Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, Aaron Gokaslan. Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models. 2022.",
  "full_text": "Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models, pages 146 - 159\nMay 27, 2022c⃝2022 Association for Computational Linguistics\nEmergent Structures and Training Dynamics in Large Language Models\nRyan Teehan*1,5, Miruna Clinciu*2,3,4,5, Oleg Serikov*5,6,7,8, Eliza Szczechla*5,\nNatasha Seelam5,9, Shachar Mirkin5,10, Aaron Gokaslan5,11\n1Charles River Analytics 2Edinburgh Centre for Robotics\n3Heriot-Watt University 4University of Edinburgh 5BigScience 6AIR Institute\n7DeepPavlov lab, MIPT 8HSE University 9Sherlock Biosciences 10Lawgeex 11Cornell University\nContact: rsteehan@gmail.com\nAbstract\nLarge language models have achieved success\non a number of downstream tasks, particularly\nin a few and zero-shot manner. As a conse-\nquence, researchers have been investigating\nboth the kind of information these networks\nlearn and how such information can be encoded\nin the parameters of the model. We survey\nthe literature on changes in the network during\ntraining, drawing from work outside of NLP\nwhen necessary, and on learned representations\nof linguistic features in large language models.\nWe note in particular the lack of sufficient re-\nsearch on the emergence of functional units –\nsubsections of the network where related func-\ntions are grouped or organized – within large\nlanguage models, and motivate future work that\ngrounds the study of language models in an\nanalysis of their changing internal structure dur-\ning training time.\n1 Introduction\nRecent advances in self-supervised learning, dis-\ntributed training, and architecture improvements\nhave enabled training massive language mod-\nels (Devlin et al., 2019; Brown et al., 2020; Radford\net al., 2019; Ma et al., 2020; Liu et al., 2019). As\nthese models have grown larger, so has their per-\nformance and generalization to new tasks. Further-\nmore, these techniques have also shown substan-\ntial improvements in learning multilingual (Chen\net al., 2020) and multimodal representations (Rad-\nford et al., 2021). These large language models\n(LLMs) have advanced the state of the art in few-\nand zero-shot tasks (Radford et al., 2019; Brown\net al., 2020; Radford et al., 2021). However, the\nsize of these models makes them difficult to eval-\nuate, examine, and audit. What structures emerge\nfrom training these neural networks? What internal\nrepresentations do these networks learn?\nIn part, this opacity is implicit in the models\nthemselves. Many of the fascinating capabilities of\nLLMs are “implicitly induced, not explicitly con-\nstructed” emergent properties (Bommasani et al.,\n2021). Emergent properties are those that result\nfrom the structural relations and interactions be-\ntween a system 's components (Ablowitz, 1939;\nCallebaut and Rasskin-Gutman, 2005). One way\nof characterizing the emergence of useful proper-\nties from complexity is through self-organization,\nwherein complex systems come to develop ordered\npatterns from the interactions of their components\n(Gershenson et al., 2020). Interactions between\nthe parts of a system can produce complex global\nbehavior, for example in the collective behavior of\nants, flocking in birds (Cucker and Smale, 2007),\nor in the brain and central nervous system (Dresp-\nLangley, 2020; Brown, 2013). In the context of\ndeep learning models, qualitatively different be-\nhavior has been observed during phase transitions\nin model size or training steps (Steinhardt, 2022).\nCurrent research on understanding the generaliza-\ntion abilities of LLMs has largely focused on the\ndegree to which they learn various linguistic fea-\ntures (e.g. syntax) that would support performance\non diverse downstream tasks. Our goal instead is to\nmotivate research that grounds the learning of these\nhigher-level representations, and from there, LLMs\ngeneralization abilities, in the emergent structures\nthat result from self-organization within the net-\nworks.\nTo analyze LLMs themselves, we survey current\nresearch on the following topics and identify gaps\nin the literature. First, we turn to the development\nof internal representations of important features of\nlanguage (e.g. syntax). Second, we look at the\nstructure of the network (neurons, weights, etc.),\nhow it evolves over time, and the emergence of\nfunctional units therein. In each case, we include\nnot only research related to trained models, but also\nthe changes that result over training time (termed\ntraining dynamics). Most research has focused\non the aforementioned internal representations and\n146\ntheir connection to the downstream performance\nand generalization ability of LLMs, with only lim-\nited work on how the network structure changes\nover time and that change’s connection to those\nrepresentations. We aim to motivate research that\nnot only applies work on the emergent structures\nwithin networks from outside of NLP to LLMs, but\nalso develops a language-specific account of useful\nfunctional units that emerge in LLMs. Moreover,\nwe identify methods for studying emergence and\nself-organization in complex systems with poten-\ntial applications to analyzing LLM training dynam-\nics and behavior. We conclude with a survey of\nexplainability methods that allow researchers to\nconnect structure with function.\n2 Internal Representations\nLinguistic Structure RepresentationsA signifi-\ncant current area of research is dedicated to inter-\npreting language models from a linguistic point of\nview. The motivation is to know to what extent\nmodels “understand” language, and more specif-\nically, to what extent their generalizations over\nlanguage agree with the generalizations about lan-\nguage described by linguistics. Following the\nhierarchy of language levels (morphology, syn-\ntax, discourse) (Dalrymple, 2001), experiments\nin probing studies typically address models’ pro-\nficiency on a certain level of language. This\nline of research typically comes down to analyz-\ning how linguistic structures are represented in\na model’s knowledge. Such structures represent\nsyntagmatic/paradigmatic mechanisms of language\n(how language units combine and alternate, respec-\ntively). It is believed (McCoy et al., 2020) that\nrediscovering these structures would help models\nget closer to humans performance on a variety of\ntasks.\nProbing Methods to Test for Linguistic Struc-\nture Probing tasks measure the linguistic aware-\nness of a model’s components, such as layers (Ten-\nney et al., 2019) or groups of neurons (Durrani\net al., 2020), by training an auxiliary model, the\nprobe, on annotated data. Datasets providing such\nlinguistically annotated data are called probing\ndatasets, and cover a wide variety of properties\n(parts of speech, parse trees, etc.). A high perfor-\nmance of a probe model on a linguistic task implies\nthat the representation tested encodes the property\nof interest. Several studies using probing methods\nhave reported high accuracy predictions in identi-\nfying the underlying linguistic structure (Belinkov\net al., 2017a,b; Peters et al., 2018; Tenney et al.,\n2019; Conneau et al., 2018; Zhang and Bowman,\n2018; Alain and Bengio, 2017; Hewitt and Man-\nning, 2019; Hewitt and Liang, 2019).\nHowever, high performance may have confound-\ning factors; there is uncertainty on whether the\nprobing tasks properly test if representations ac-\ntually encode linguistic structure and on how to\ninterpret the results of probes (Hewitt and Liang,\n2019; Zhang and Bowman, 2018; V oita and Titov,\n2020; Pimentel et al., 2020b). Toward that end,\nthe following section reviews several probing ap-\nproaches in the context of language models, and\nthe evaluation criteria used to determine the profi-\nciency of a probe.\nGrammatical and Semantic Probing Given the\nexcellent performance of pre-trained representa-\ntions on numerous linguistic tasks (Kitaev and\nKlein, 2018; He et al., 2018; Strubell et al., 2018;\nLee et al., 2018), several studies have explored how\nsemantic and grammatical knowledge are encoded\nwithin language models. Syntactic and morpho-\nlogical probing encompasses tasks that identify\ngrammatical structure underlying the vector rep-\nresentations within pre-trained models, whereas\nsemantic probing tasks investigate what meaning\nis conveyed within the representation.\nEarlier work using part of speech (POS) and\nmorphological tagging (Belinkov et al., 2017a) in-\ndicated that syntactic information may be encoded\nin layers of neural models. More recently, investi-\ngations have considered whether models learn to\nembed entire parse trees in their representations.\nIn Hewitt and Manning (2019), the authors out-\nline structural probing as a method to identify hi-\nerarchical, tree-like, structures from vector repre-\nsentations of language via the syntactic distance\nbetween embeddings. Their results across several\nlarge language models suggested that Transformer\nmodel encodings possess some hierarchical linguis-\ntic structure.\nSeveral studies conducted probing experiments\nin multilingual settings. Chi et al. (2020) high-\nlighted syntactic generalizations in multilingual\nlanguage models via structured probing, and ¸ Sahin\net al. (2020) propose a framework for multilingual\nmorpho-syntactic probing, with 15 probing tasks\nfor multiple languages, showing that, while cross-\nlingual typological regularities can be found with\nprobing, probing dataset properties strongly impact\n147\nthe results (see Section 2.2 for more details about\nmultilingual models).\nProbes have also been used to measure semantic\ninformation within language model representations.\nThe authors of Belinkov et al. (2017b) posed a\nsemantic-class labeling task and found that higher\nlayers of a model tend to perform better at seman-\ntic tagging. Similarly, semantic labeling tasks have\nbeen used to indicate that contextualized represen-\ntations may encode multiple meanings within a\nsingle vector (Yaghoobzadeh et al., 2019). Con-\ntrarily, edge probing, developed by Tenney et al.\n(2019), implied that contextualized embeddings\nshow larger gains on syntactic tasks as opposed\nto semantic ones (with only modest performance\ngains against non-contextualized baselines). There\nis no general evidence on how exactly language\nlevels are distributed across model layers (Rogers\net al., 2020).\nInformation Theoretic Probing Information-\ntheoretic probing characterize tasks as a way of esti-\nmating the mutual information between an internal\nrepresentation and the linguistic property of inter-\nest (Pimentel et al., 2020b; Pimentel and Cotterell,\n2021; V oita and Titov, 2020; Pimentel et al., 2020a).\nMany of these approaches highlight the need to for-\nmalize the “effort” required in encoding a linguistic\nproperty, often via some form of a control function\n(Pimentel et al., 2020b). Counter-intuitively, work\nfrom Pimentel et al. (2020b) suggest that the “best”\nprobes are ones that always perform highest on the\ntask; their argument is that “learning” the task is\nequivalent to encoding the linguistic property in\nthe initial representations. They provide approxi-\nmations to calculate information gain, finding that\nBERT models contain only 12% more information\nthan non-contextualized baselines.\nCriticisms of accuracy-based performance met-\nrics have argued that these methods are sensitive\nto structure, randomization, and hyperparameter\nselection (V oita and Titov, 2020; Hewitt and Liang,\n2019; Zhang and Bowman, 2018; Pimentel et al.,\n2020b). As an alternative, the minimum description\nlength (MDL) offers an information theoretic view\non probe quality (V oita and Titov, 2020). Formally,\nit describes the “minimum number of bits required\nto transmit labels, knowing the representations”,\nwhere better probes are those with smaller code-\nlengths, as they suggest the information available\nin the representation is sufficiently accessible to\nsolve the task. Prior studies have shown the MDL\nmetric is robust and resilient to randomness (V oita\nand Titov, 2020). In comparison to the original\nPOS tagging of Hewitt and Liang (2019), the MDL\nmetric consistently distinguishes between the lin-\nguistic versus the control tasks across differences\nin hyperparameters and random seeds. Similarly,\nfollowing Zhang and Bowman (2018), evaluation\nusing MDL revealed longer codelengths for ran-\ndomly initialized models as opposed to pre-trained\nones.\n2.1 Evaluating Probing Performance\nSeveral studies have highlighted the need for inter-\npretable performance scores on probes (Belinkov\net al., 2017b; Peters et al., 2018; Tenney et al., 2019;\nConneau et al., 2018; Zhang and Bowman, 2018;\nAlain and Bengio, 2017; Hall Maudslay and Cot-\nterell, 2021). Two common themes have emerged\nfor evaluating the proficiency of a probe: selec-\ntivity through control tasks and high informatic\noverlap via control functions (Hewitt and Liang,\n2019; Pimentel et al., 2020b; Zhu and Rudzicz,\n2020). Recent work suggests that both approaches\nyield comparable results empirically with similar\nerror terms theoretically (Zhu and Rudzicz, 2020).\nControl Tasks Selectivity is the trade-off be-\ntween complexity and performance of the linguistic\ntask. A “good” probe refers to one that performs\nhighly on linguistic tasks, but poorly on control\ntasks, thus limiting the ability for a probe to “mem-\norize” the task (Hewitt and Liang, 2019).\nArguments preferring “simpler” probes claim\nthat these models should find “accessible” informa-\ntion within the representations (Shi et al., 2016).\nThe simplest probes employ linear functions, yet\nmore complex probes have been commonly used,\nincluding multi-layer perceptrons (MLP) or kernel\nmethods (Belinkov et al., 2017a; Conneau et al.,\n2018; White et al., 2021; Adi et al., 2017), suggest-\ning that some linguistic properties may be encoded\nnon-linearly. Linear functions and MLPs are still\ncommonly in use (Tenney et al., 2019).\nPrior works within the probing literature have\nalso explored how the size of training data can in-\nfluence the performance of the probe (Zhang and\nBowman, 2018; Hewitt and Liang, 2019). In an\ninvestigation considering probes of pre-trained lan-\nguage models and an untrained baseline on two\nsyntactic tasks: POS tagging and Combinatorial\nCategorical Grammar (CCG) super-tagging (Hock-\nenmaier and Steedman, 2007), probes with an un-\n148\ntrained baseline model could surprisingly attain\nhigh performance compared to pre-trained models\n(Zhang and Bowman, 2018). However, the probe\nperformance decreased dramatically when reduc-\ning the amount of available training data when com-\npared to the pre-trained models. This suggested\ntrained encoders captured enough syntactic infor-\nmation, beyond simple word-identities, which en-\nabled these representations to achieve high perfor-\nmance on the linguistic tasks.\nAn extensive study on selectivity proposed sev-\neral control tasks for POS tagging and dependency\nedge prediction (Hewitt and Liang, 2019). Across\nan array of probe architectures (linear, MLP-1,\nMLP-2) and hyperparameters, this investigation\nconsidered the effect of the hidden state dimen-\nsionality (size), number of training examples, regu-\nlarization, and early stopping. The most effective\nprobes were those with constrained hidden dimen-\nsions, yielding the most selective probes.\nControl Functions Control functions compare\nthe mutual information against a property of inter-\nest and the representation before and after the func-\ntion is applied. The objective is used to measure the\ninformation gain of the representation. In Pimentel\net al. (2020b), control functions were used to com-\npare BERT contextualized models against FastText\n(Bojanowski et al., 2017) and a one-hot encoding\non POS tagging. Curiously, their results suggested\nthat BERT models only marginally improved infor-\nmation gain against these simpler baselines.\n2.2 Emerging Multilingual Structures\nMultilingual large language models, such as multi-\nlingual BERT (mBERT) (Devlin et al., 2019; De-\nvlin, 2018) XLM (Conneau and Lample, 2019) or\nXLM-R (Conneau et al., 2020a) have shown im-\npressive results when used for (zero-shot) cross-\nlingual transfer; that is, when the pre-trained mul-\ntilingual language model is used as the basis for a\ntask-specific model that is applied to a language in\nwhich it was not trained for. Their efficiency was\nproven in a wide variety of tasks, such as sentiment\nanalysis, natural language inference, and question\nanswering, to name a few.\nPrior to the immense popularity of Transformer-\nbased models, two approaches of using word\nembeddings for cross-lingual tasks have shown\npromising results. In the first, representations are\nlearned separately from individual languages and\nthen aligned to a shared space, thus producing\ncross-lingual word embeddings(Ruder et al., 2019),\nthat in turn, are used on the target language. In the\nsecond, multilingual representations are learned by\njointly training over multiple languages. Artetxe\nand Schwenk (2019), for example, trained a BiL-\nSTM over 93 languages using parallel corpora, pro-\nducing “universal” embeddings that were success-\nfully used in various tasks.\nThe same two approaches are being explored\nwith large language models. In Conneau et al.\n(2020b), monolingual BERT models that were\ntrained separately for different languages produced\nsimilar (easily-aligned) representations. Pires et al.\n(2019) and Vuli´c et al. (2020) further showed – as\nexpected – that the similarity depends on the typo-\nlogical distance between the languages. Universal\nlanguage-agnostic embeddings also emerge when\ntraining multilingual models, even when no explicit\nconnection (such as parallel corpora or bilingual\ndictionaries) between the languages is used during\ntraining, such as in the case of mBERT.\nMultiple works looked into the factors that con-\ntribute to the successful transfer. These include\ndomain and language similarity, shared parame-\nters, and perhaps the most straightforward factor:\ncommon (sub-) words between the languages (Wu\nand Dredze, 2019; Conneau et al., 2020b; Pires\net al., 2019). Interestingly, Conneau et al. (2020b)\nand K et al. (2020) showed that the universal rep-\nresentations do not heavily depend on shared vo-\ncabulary; instead, multilinguality emerges directly\nfrom the fact that parameters are shared in training,\nfrom the structure of the network, and is affected\nby common characteristics of the languages, such\nas word order (Dufter and Schütze, 2020). Pires\net al. (2019) discovered that mBERT can also suc-\ncessfully transfer between languages with differ-\nent scripts, and that generalization goes beyond\nthe lexical level, and Chi et al. (2020) found that\nsyntactic features representations in mBERT over-\nlap between languages. Still, Ahmad et al. (2021)\nhave shown that augmenting mBERT with syntac-\ntic information can improve cross-lingual transfer\nperformance.\nThe size of each language 's corpus in the lan-\nguage model's training set has been shown to be\ndecisive for transfer to that language. Thus, low-\nresource languages often benefit more from the\njoint training (Wu and Dredze, 2020), while lan-\nguages with abundant resources often achieve bet-\nter performance when trained on their own (Nozza\n149\net al., 2020; Lewis et al., 2020).\n2.3 Training Dynamics of Internal\nRepresentation Development\nTraining dynamics is an emerging field of research,\npromising to improve our understanding of knowl-\nedge acquisition in neural networks and offering in-\nsights into the utility of pre-trained models and em-\nbedded representations for downstream tasks. Most\nstudies of Transformers (e.g. RoBERTa (Zhuang\net al., 2021)) and LSTMs (Hochreiter and Schmid-\nhuber, 1997) agree that models acquire linguistic\nknowledge early in the learning process.\nLocal syntactic information, such as parts of\nspeech, is learned earlier than information encod-\ning long-distance dependencies (e.g. topic) (Liu\net al., 2021; Saphra, 2021). Exploration of AL-\nBERT (Lan et al., 2019) and LSTM-based networks\nreveals different learning patterns for function and\ncontent words with more fine-grained distinctions\nwithin these categories including part of speech\nand verb form (Saphra, 2021; Chiang et al., 2020).\nDifferences in learning trajectory were also ob-\nserved between layers. In LSTMs, recurrent lay-\ners become more task-independent over the course\nof training, while embeddings become more task-\nspecific (Saphra, 2021). In Transformer-based ar-\nchitectures, i.e.: ALBERT and ELECTRA, Chiang\net al. (2020) observe differences in performance\npatterns between the top and last layers. Simi-\nlarly to other areas of research in NLP, most of\nthe literature on training dynamics concentrate on\nEnglish-language models. Another possible direc-\ntion for future work is extending studies conducted\non LSTMs to more widely used Transformers.\n2.4 Critique of Testing Methods\nRecent research has complicated the picture of\ngrammar learning presented in Sections 2, 2.2,\nand 2.3. Specifically, there have been two sepa-\nrate but related types of critique leveled at probing\nand grammar learning. First, specific to probing,\nresearchers question whether probes really iden-\ntify linguistic representations at all. Secondly, and\nmore fundamentally, it is unclear to what degree\nlanguage models even learn grammar.\nHall Maudslay and Cotterell (2021) suggest that\nsemantic “cues” may contaminate syntax probes,\nmaking it difficult to evaluate their scores. By\nemploying “Jabberwocky probing”, where pseudo-\nwords with no lexical meaning replace the original\ncomponents of the sentence in a way that preserves\ngrammar, the authors discovered that performance\nof syntactic probes considerably dropped for large\nlanguage models, calling into question whether syn-\ntactic probes actually isolate syntactic knowledge\nwithing language models.\nA more fundamental issue for syntax learning\nin language models has been their performance\nwhen trained on perturbed or permuted data. Sinha\net al. (2021) use a variety of word order permu-\ntations that preserve distributional information to\nisolate whether what language models learn is ac-\ntually syntax. Word order has been assumed to\nbe important not only for natural language under-\nstanding by humans but also by language models,\nparticularly for learning syntax. Surprisingly then,\nword order appears to have less influence than one\nwould expect on the downstream performance of\nlanguage models and their performance on prob-\ning tasks. In part, the authors note that some syn-\ntax information can be acquired during fine-tuning\nto sufficiently answer tasks that require it. More-\nover, in the context of syntax probes, the authors\nnote that “while natural word order is useful for at\nleast some probing tasks, the distributional prior of\nrandomized models alone is enough to achieve a\nreasonably high accuracy on syntax sensitive prob-\ning”. Furthermore, the results distinguish between\nparametric and non-parametric probes, where per-\nformance on the latter using randomization models\ndegrades significantly. This degradation provides\nevidence that non-parametric probes are able to test\nfor syntax learning in ways that parametric probes\ncannot. Similarly, O’Connor and Andreas (2021)\nuse syntax-level perturbations and ablations to con-\nclude that the information in context windows most\nuseful to language models are local ordering statis-\ntics and content words, e.g. nouns, verbs, adverbs,\nand adjectives. In other words, it does not appear\nthat language models make use of syntactic or other\nstructural information in the context window.\n2.5 Further Research\nDespite recent probing studies providing a closer\nlook at how linguistic structures are distributed in\nlanguage models, it is an open question to what ex-\ntent this knowledge acquisition differs from that of\nhumans. While grammatical structures tend to be\nlearned much faster than downstream knowledge\n(Conneau et al., 2018), there is still room for the\nstudy of more specific questions, such as whether\nmodels require more time to acquire the grammar\n150\nof polysynthetic languages, as has been reported\nfor humans (Kelly et al., 2014).\nAnother remaining open question is whether lin-\nguistic structure knowledge can be transferred be-\ntween models with the neurons initialization mech-\nanism (Durrani et al., 2021). While rough re-use of\nneurons is proven to be helpful in model initializa-\ntion (Sanh et al., 2019), for instance, such neuron\n“surgery” would potentially lead to even quicker\nacquisition of grammatical knowledge.\nGenerally speaking, the performance of multi-\nlingual models is inferior to that of monolingual\nones, especially when enough resources are avail-\nable. Yet, high-quality multilingual models remain\na desired objective that can particularly benefit low-\nresource languages. Further understanding the fac-\ntors that enable learning language-independent rep-\nresentations is key for developing better multilin-\ngual training or cross-lingual fine-tuning strategies,\nespecially for transfer between less similar lan-\nguage pairs. A particularly interesting question is\nwhether some tasks require more language-specific\nadaptation, because, for instance, they depend on\nlinguistic information that is currently not general-\nized well enough in multilingual LLMs.\n3 Self-Organization and the Emergent\nStructure of Networks\n3.1 Network Structure\nInspired by the architecture of biological neural\nnetworks (BNNs) and their adaptability to various\ntasks, where neurons and circuits are capable of\nself-organization, many researchers have investi-\ngated how Artificial Neural Networks (ANNs) can\nbe seen as emergent structures, where interpretabil-\nity of an ANN's parameters can help us to inspect\ntheir functional modularity. Broadly, researchers\nhave approached this by identifying patterns in the\nweights or neurons especially through subgraphs\nof the network.\nBranch specialization is the organization of\nbranches – or “sequences of layers which tem-\nporarily don’t have access to ‘parallel’ information\nwhich is still passed to later layers” (V oss et al.,\n2021) – of the network into functional units, across\ndifferent architectures and tasks (Zhang et al., 2020;\nBunel et al., 2020; V oss et al., 2021; Rössig and\nPetkovic, 2021). It is somewhat similar to how\nneurons are connected by synapses, forming small\nfunctional units called neural circuits that can be\nspecialized for specific tasks, such as to “medi-\nate reflexes, process sensory information, gener-\nate locomotion and mediate learning and memory”\n(Byrne et al., 2012; Luo, 2021). In their work on\nAlexNet, V oss et al. (2021) provided initial evi-\ndence of self-organization of neurons and circuits\n(subgraphs) into functional units in a neural net-\nwork. This self-organized emergent structure is\nconsistent “across different architectures and tasks”.\nA look at evolving neural structures gives another\nperspective. Inspired by neural architecture search\n(NAS), So et al. (2019) presented “a first neural ar-\nchitecture search conducted to find improved feed-\nforward sequence models”, where the search space\ncontains five branch-level search fields. Recently,\nSo et al. (2021) introduced Primer (PRIMitives\nsearched TransformER), which can add improve-\nments in the pre-training and one-shot downstream\ntask transfer regime. However, branches are used\njust for the initialized multi-head attention.\nWeight bandingis the uniformity in the orga-\nnization of the weights in a final layer. In neural\nnetworks, weights are parameters that can trans-\nform the input data between the network's hidden\nlayers. Weight banding resembles another biolog-\nical phenomenon when a neuron multiplies each\ninput with a synaptic weight, which is represented\nas a number that highlights the importance assigned\nto that input. The weighted inputs are summed up\nin what represents the neuron's output (Iyer et al.,\n2013). Petrov et al. (2021) note that many vision\nmodels display a uniform pattern in their final layer.\nThey investigate the nature of this structural phe-\nnomenon, connecting it ultimately to architectural\nchoices in the network and noting that weight band-\ning can serve as a method of preserving spatial\ninformation.\nClustering is the grouping of neurons or subnet-\nworks into units that can be used for specific tasks\n(Hod et al., 2021). Starting from the fact that modu-\nlar systems allow us to have a better understanding\nof a system if we can inspect the function of indi-\nvidual modules, different clustering methods for\nneural networks were proposed. Li et al. (2020) de-\nsigned a modular neural network based on feature\nclustering to decompose features into clusters with\neach module processing different features. These\nmodules work in parallel for a singular task. Filan\net al. (2021) proposed a spectral clustering algo-\nrithm for decomposition of trained networks into\nclusters, finding that networks can have some sense\nof modularity and suggested further work related\n151\nto clusterability in various domains.\nModularity focuses on the reusability of sub-\nnetworks for multiple tasks (Happel and Murre,\n1994; Shukla et al., 2010; Csordás et al., 2021). In\nCsordás et al. (2021) neural networks trained on\nalgorithmic tasks appear to fail to learn general,\nmodular, compositional algorithms, and require\nspecific subset weights to handle a particular com-\nbination of the input tokens. With these findings,\nCsordás et al. (2021) suggest further research about\n“function dependent weight sharing in the neural\nnetworks”. Reusable multi-task subnetworks may\nalso be discovered via Neural Architecture Search\n(NAS) methods (Pham et al., 2018). Pasunuru and\nBansal (2019) leverage a technique called multi-\ntask architecture search (MAS) to find multi-task\ncell structures in RNNs, capable of generalization\nto unseen tasks.\n3.2 Training Dynamics of Network Changes\nUnderstanding the change in network structure over\ntime is equally as important as identifying structure\nin trained models. Here, the focus is on how the\nparameters of the model change over the course\nof training, which can give insight into the types\nof inductive biases that develop and shed light on\nthe nature of LLMs’ abilities to generalize. The\nmost recent work covering this in the context of\nLLMs focuses on parameter norm growth, which\nrefers to the growth of the ℓ2 norm during training\ntime. According to Merrill et al. (2021), neural\nnetworks learn successfully due to inductive biases\nintroduced during training. Norm growth induces\nsaturation in Transformer models, which reduces\nthe attention heads to “generalized hard attention”.\nThe authors find that computations for argmax and\nmean are reducible to saturated attention, which\npartially explains why saturated Transformer mod-\nels can learn counter languages, a kind of formal\nlanguage, and may play a broader role in explaining\ntheir generalization abilities.\n3.3 Further Research\nAs we have noted, most of the work on network\nstructure is currently outside of NLP, either dealing\nwith general ANNs or specific to Computer Vision\nwith AlexNet and general convolutional networks\ntrained on ImageNet (V oss et al., 2021; Petrov et al.,\n2021). This work should be replicated in the con-\ntext of LLMs to test for the existence of language-\nspecific functional units and, more generally, deter-\nmine whether there are internal network structures\nthat support the learned representations we discuss\nin Section 2. Likewise, since this research is still in\nits infancy, it is focused on simple emergent struc-\ntures. Future research can incorporate higher-order\nemergent structures (Baas, 2000), new methods of\nstructure detection in networks (Aktas et al., 2019),\nand even detection of structures whose form is not\nexplicitly specified (Shalizi et al., 2006).\nAdditionally, by viewing the neural networks\nin question as time-evolving complex systems we\ncan leverage older research on self-organization\nthat has yet to be applied to understanding LLMs.\nIn particular, Ball et al. (2010) provide a method\nfor quantifying self-organization based on persis-\ntent mutual information. Likewise, Shalizi et al.\n(2004) ground self-organization in information the-\nory and Shalizi (2003) extends this method to a\ngeneral class of undirected graphs. Methods such\nas these can be used to identify and quantify self-\norganization in LLMs and better understand their\nemergent behavior.\n4 Connecting Structure to Function:\nExplainable AI (XAI)\nThe rapid increase in the adoption of AI models in\nrecent years and their growing impact on human\nlives created a need for techniques that offer insight\ninto the models internal operations.\nSince attention-based models (Vaswani et al.,\n2017) have become state-of-the-art tools in NLP,\nthere have been numerous attempts to provide some\nunderstanding of their predictions by visualizing\nthe attention layer. However, these approaches\nhave been criticized for their inability to produce\nmeaningful and coherent interpretations (Wiegreffe\nand Pinter, 2019; Bastings and Filippova, 2020;\nSerrano and Smith, 2019). To address these limi-\ntations, Ghaeini et al. (2018) examine the saliency\nof attention and LSTM gating signal in the inter-\nmediate layers of ESIM models, an architecture de-\nsigned for natural language inference tasks (Chen\net al., 2017). Their results show that visualizing\nattention saliency allows identifying which parts\nof the premise and hypothesis contribute most to\nthe final score. Moreover, attention saliency maps\ncompared across different ESIM models reveal dif-\nferences in focus that reflect the differences in their\npredictions. According to this study, using saliency\nis much more effective than using attention alone.\nAnother approach to revealing how decisions are\nformed across network layers is erasure, where fea-\n152\ntures are deemed irrelevant if their removal has a\nminor effect on the prediction. De Cao et al. (2020)\nextend this method to learned masking and adapt\nit to measure the importance of intermediate states\nrather than the inputs. They run the proposed DIFF-\nMASK method on BERT (Devlin et al., 2019) and\nfind that separator tokens play an important role\nin the input layer for question answering but not\nfor sentiment classification, a task where adjectives\nand nouns are kept for much longer. Given that\nseparators serve as delimiters between the ques-\ntion and the context, these differences shed light on\nthe connection between the internal latent structure\nand the task, marking a step toward gaining some\nunderstanding of the information flow in the model.\nApplying neural models to the NLP domain\nposes specific challenges. This opens the way for\nresearch on the extent to which language-specific\ncharacteristics, such as compositionality of mean-\ning, are reflected in the internal representations of\nneural networks. The work by Li et al. (2016) lever-\nages several methods including variance-based and\nfirst-derivative saliency (a technique inspired by\nback-propagation), to study how models deal with\ncompositionality of meaning, e.g., negation, in-\ntensification and combining meaning from differ-\nent parts of the sentence. The study of recurrent,\nLSTM and bi-LSTM networks across time steps\nfinds that, as decoding proceeds, the task (language\nmodelling) gradually prevails overbuilding word\nrepresentations.\nAn integrated gradients (Sundararajan et al.,\n2017) based method of finding neurons that en-\ncode individual facts has been proposed by Dai\net al. (2021). This approach builds on the ob-\nservation that large pre-trained language models\ncan remember factual knowledge from the training\ncorpus. The authors find that knowledge neurons\nare located in the feed-forward network of BERT\nand view these two-layer perceptron modules as\nknowledge memories in the Transformer architec-\nture. The method allows for explicit editing of spe-\ncific factual knowledge by manipulating the corre-\nsponding knowledge neurons with only a moderate\ninfluence on unrelated knowledge. These findings\nare in line with a work by Meng et al. (2022) that\nlocalizes factual knowledge to the feed-forward\nlayer. Further, this approach makes a distinction\nbetween the notions of knowing and saying a fact\nand concludes that, while the feed-forward layers\nencode the former, the latter is attended to by the\nlate self-attention.\nOther approaches, e.g. SHAP, DeepLift and\nLIME (Lundberg and Lee, 2017; Shrikumar et al.,\n2019; Ribeiro et al., 2016) can reveal dependencies\nmissed by the methods discussed here. In NLP,\nthe key challenges include performance and, where\napplicable, choosing an adequate baseline for word\nembeddings. The dynamic progress of research\nin natural language processing has led researchers\nto review and analyze existing methods of inter-\npreting neural models (Belinkov and Glass, 2019;\nDanilevsky et al., 2020). While the emerging field\nof explainable AI (XAI) is seeing faster growth, a\npath for research and discussion on the desired eval-\nuation criteria of interpretation methods is opening\nup (Jacovi and Goldberg, 2020).\n5 Conclusion and Future Directions\nIn this paper, we provide an overview of research\non network structure, linguistic feature learning,\ntheir training dynamics, and explainability research\nthat aims to connect network structure and func-\ntion. In doing so, we highlight gaps in the liter-\nature and opportunities for future research, both\nin each individual research area and as a broad\nproposal for grounding research in understanding\nlarge language models. We highlight a few areas\nof future research as particularly important given\nthe gaps in the current literature. For the study of\nhow, and whether, linguistic structures are learned\nby language models, more work is needed to under-\nstand the training dynamics of this learning across\na variety of model scales and architectures. More\nfundamentally, there is disagreement about what\nit means for a model to “encode” linguistic struc-\ntures such as syntax, particularly in a multilingual\nsetting.\nMore broadly, nascent work on the self-\norganization of neurons and subnetwork structures\nthat emerge during training time has largely not\nbeen applied to LLMs, or neural networks in NLP\nmore generally. Research in Computer Vision has\nshown the existence of emergent functional units\nwith functions that are semantically meaningful\nto humans. In the context of LLMs, such struc-\ntures may provide a basis for understanding the\nnature of linguistic features that LLMs purportedly\nlearn, especially when comparing the development\nof each during training time. Additional research\nis needed to not only determine whether such struc-\ntures emerge in LLMs, but also to apply and ex-\n153\ntend the literature on self-organization in complex\nsystems. This research can also be used for ex-\nplainability. Currently, assessment of the quality\nof interpretations of the information flow in neu-\nral models is not straightforward. Identification of\nmodular and emergent structures within networks\nmay be viewed as a way of moving away from\nthe binary definition of faithfulness as postulated\nby Jacovi and Goldberg (2020). Evidence for the\nexistence of structures aligning with human per-\nception of language, if found, can help to enable\nseparate consideration of plausibility from a human\nperspective, as proposed in the same study. More\nbroadly, we propose grounding the study of LLMs\nproperties in the analysis of the self-organization\nof weights and neurons into emergent structures.\nReferences\nReuben Ablowitz. 1939. The theory of emergence. Phi-\nlosophy of Science, 6(1):1–16.\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi,\nand Yoav Goldberg. 2017. Fine-grained analysis\nof sentence embeddings using auxiliary prediction\ntasks. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nWasi Ahmad, Haoran Li, Kai-Wei Chang, and Yashar\nMehdad. 2021. Syntax-augmented multilingual\nBERT for cross-lingual transfer. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 4538–4554, Online.\nAssociation for Computational Linguistics.\nMehmet Aktas, Esra Akbas, and Ahmed El Fatmaoui.\n2019. Persistence homology of networks: methods\nand applications. Applied Network Science, 4.\nGuillaume Alain and Yoshua Bengio. 2017. Under-\nstanding intermediate layers using linear classifier\nprobes. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Workshop Track Proceedings. OpenRe-\nview.net.\nMikel Artetxe and Holger Schwenk. 2019. Massively\nMultilingual Sentence Embeddings for Zero-Shot\nCross-Lingual Transfer and Beyond. Transactions of\nthe Association for Computational Linguistics, 7:597–\n610.\nNils A. Baas. 2000. Emergence, hierarchies, and hyper-\nstructures. In Christopher G. Langton, editor, Artifi-\ncial Life III, page 515–537. Addison-Wesley Long-\nman Publishing Co., Inc., USA.\nRobin C. Ball, Marina Diakonova, and Robert S.\nMacKay. 2010. Quantifying emergence in terms of\npersistent mutual information. Adv. Complex Syst.,\n13:327–338.\nJasmijn Bastings and Katja Filippova. 2020. The ele-\nphant in the interpretability room: Why use attention\nas explanation when we have saliency methods? In\nProceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for\nNLP, pages 149–155, Online. Association for Com-\nputational Linguistics.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan\nSajjad, and James Glass. 2017a. What do neural\nmachine translation models learn about morphology?\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 861–872, Vancouver, Canada.\nAssociation for Computational Linguistics.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nYonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James Glass. 2017b. Eval-\nuating layers of representation in neural machine\ntranslation on part-of-speech and semantic tagging\ntasks. In Proceedings of the Eighth International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1–10, Taipei, Taiwan.\nAsian Federation of Natural Language Processing.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri Chatterji, Annie S.\nChen, Kathleen Creel, Jared Quincy Davis, Dorottya\nDemszky, Chris Donahue, Moussa Doumbouya, Esin\nDurmus, Stefano Ermon, John Etchemendy, Kawin\nEthayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale,\nLauren Gillespie, Karan Goel, Noah D. Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil\nJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, Omar\nKhattab, Pang Wei Koh, Mark S. Krass, Ranjay Kr-\nishna, Rohith Kuditipudi, and et al. 2021. On the\nopportunities and risks of foundation models. CoRR,\nabs/2108.07258.\nSteven Ravett Brown. 2013. Emergence in the central\nnervous system. Cognitive Neurodynamics, 7(3).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n154\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nRudy Bunel, Ilker Turkaslan, Philip H.S. Torr, M. Pawan\nKumar, Jingyue Lu, and Pushmeet Kohli. 2020.\nBranch and bound for piecewise linear neural net-\nwork verification. Journal of Machine Learning Re-\nsearch, 21.\nJohn H Byrne, D Ph, and The Ut. 2012. Introduction\nto Neurons and Neuronal Networks. Cellular and\nMolecular Neurobiology, 1.\nWerner Callebaut and Diego Rasskin-Gutman. 2005.\nModularity: Understanding the Development and\nEvolution of Natural Complex Systems . The MIT\nPress.\nPeng-Jen Chen, Ann Lee, Changhan Wang, Naman\nGoyal, Angela Fan, Mary Williamson, and Jiatao\nGu. 2020. Facebook AI’s WMT20 news translation\ntask submission. In Proceedings of the Fifth Confer-\nence on Machine Translation, pages 113–125, Online.\nAssociation for Computational Linguistics.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced LSTM for\nnatural language inference. In Proceedings of the\n55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1657–1668, Vancouver, Canada. Association\nfor Computational Linguistics.\nEthan A. Chi, John Hewitt, and Christopher D. Man-\nning. 2020. Finding universal grammatical relations\nin multilingual BERT. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5564–5577, Online. Association\nfor Computational Linguistics.\nCheng-Han Chiang, Sung-Feng Huang, and Hung-yi\nLee. 2020. Pretrained language model embryology:\nThe birth of ALBERT. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6813–6828, On-\nline. Association for Computational Linguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Emerging\ncross-lingual structure in pretrained language mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020 , pages 6022–6034.\nAssociation for Computational Linguistics.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Emerging\ncross-lingual structure in pretrained language mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6022–6034, Online. Association for Computational\nLinguistics.\nRóbert Csordás, Sjoerd van Steenkiste, and Jürgen\nSchmidhuber. 2021. Are neural nets modular? in-\nspecting functional modularity through differentiable\nweight masks. In International Conference on Learn-\ning Representations.\nFelipe Cucker and Steve Smale. 2007. Emergent be-\nhavior in flocks. IEEE Transactions on Automatic\nControl, 52(5):852–862.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu\nWei. 2021. Knowledge neurons in pretrained trans-\nformers. ArXiv, abs/2104.08696.\nMary Dalrymple. 2001. Lexical functional grammar.\nBrill.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yan-\nnis Katsis, Ban Kawas, and Prithviraj Sen. 2020. A\nsurvey of the state of explainable AI for natural lan-\nguage processing. In Proceedings of the 1st Confer-\nence of the Asia-Pacific Chapter of the Association\nfor Computational Linguistics and the 10th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 447–459, Suzhou, China. Association\nfor Computational Linguistics.\nNicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz,\nand Ivan Titov. 2020. How do decisions emerge\nacross layers in neural models? interpretation with\ndifferentiable masking. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 3243–3255, On-\nline. Association for Computational Linguistics.\nJacob Devlin. 2018. Multilingual bert.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n155\nBirgitta Dresp-Langley. 2020. Seven properties of self-\norganization in the human brain. Big Data and Cog-\nnitive Computing, 4(2).\nPhilipp Dufter and Hinrich Schütze. 2020. Identifying\nelements essential for BERT’s multilinguality. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4423–4437, Online. Association for Computa-\ntional Linguistics.\nNadir Durrani, Hassan Sajjad, and Fahim Dalvi. 2021.\nHow transfer learning impacts linguistic knowledge\nin deep NLP models? In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4947–4957, Online. Association for Computa-\ntional Linguistics.\nNadir Durrani, Hassan Sajjad, Fahim Dalvi, and\nYonatan Belinkov. 2020. Analyzing individual neu-\nrons in pre-trained language models. arXiv preprint\narXiv:2010.02695.\nDaniel Filan, Stephen Casper, Shlomi Hod, Cody Wild,\nAndrew Critch, and Stuart Russell. 2021. Cluster-\nability in neural networks. CoRR, abs/2103.03386.\nCarlos Gershenson, Vito Trianni, Justin Werfel, and Hi-\nroki Sayama. 2020. Self-Organization and Artificial\nLife. Artificial Life, 26(3):391–408.\nReza Ghaeini, Xiaoli Fern, and Prasad Tadepalli. 2018.\nInterpreting recurrent and attention-based neural\nmodels: a case study on natural language inference.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4952–4957, Brussels, Belgium. Association for Com-\nputational Linguistics.\nRowan Hall Maudslay and Ryan Cotterell. 2021. Do\nsyntactic probes probe syntax? experiments with\njabberwocky probing. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 124–131, Online. As-\nsociation for Computational Linguistics.\nBart L.M. Happel and Jacob M.J. Murre. 1994. Design\nand evolution of modular neural network architec-\ntures. Neural Networks, 7(6-7).\nLuheng He, Kenton Lee, Omer Levy, and Luke Zettle-\nmoyer. 2018. Jointly predicting predicates and argu-\nments in neural semantic role labeling. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 364–369, Melbourne, Australia. Association\nfor Computational Linguistics.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2733–2743, Hong Kong,\nChina. Association for Computational Linguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nJulia Hockenmaier and Mark Steedman. 2007. Ccg-\nbank: A corpus of ccg derivations and dependency\nstructures extracted from the penn treebank. Comput.\nLinguist., 33(3):355–396.\nShlomi Hod, Stephen Casper, Daniel Filan, Cody Wild,\nAndrew Critch, and Stuart J. Russell. 2021. De-\ntecting modularity in deep neural networks. ArXiv,\nabs/2110.08058.\nRamakrishnan Iyer, Vilas Menon, Michael Buice,\nChristof Koch, and Stefan Mihalas. 2013. The Influ-\nence of Synaptic Weight Distribution on Neuronal\nPopulation Dynamics. PLoS Computational Biology,\n9(10).\nAlon Jacovi and Yoav Goldberg. 2020. Towards faith-\nfully interpretable NLP systems: How should we\ndefine and evaluate faithfulness? In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4198–4205, On-\nline. Association for Computational Linguistics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and Dan\nRoth. 2020. Cross-lingual ability of multilingual bert:\nAn empirical study. In International Conference on\nLearning Representations.\nBarbara Kelly, Gillian Wigglesworth, Rachel\nNordlinger, and Joseph Blythe. 2014. The acqui-\nsition of polysynthetic languages. Language and\nLinguistics Compass, 8(2):51–64.\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2676–2686, Melbourne, Australia. Association\nfor Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018.\nHigher-order coreference resolution with coarse-to-\nfine inference. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 2 (Short Papers), pages\n687–692, New Orleans, Louisiana. Association for\nComputational Linguistics.\n156\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 681–691, San Diego, California.\nAssociation for Computational Linguistics.\nWenjing Li, Meng Li, Junfei Qiao, and Xin Guo. 2020.\nA feature clustering-based adaptive modular neural\nnetwork for nonlinear system modeling. ISA Trans-\nactions, 100:185–197.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nZeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Ha-\njishirzi, and Noah A. Smith. 2021. Probing across\ntime: What does RoBERTa know and when? In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 820–842, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nScott M Lundberg and Su-In Lee. 2017. A unified\napproach to interpreting model predictions. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 30. Curran Associates, Inc.\nLiqun Luo. 2021. Architectures of neuronal circuits.\nScience, 373(6559):eabg7285.\nShuming Ma, Jian Yang, Haoyang Huang, Zewen Chi,\nLi Dong, Dongdong Zhang, Hany Hassan Awadalla,\nAlexandre Muzio, Akiko Eriguchi, Saksham Singhal,\net al. 2020. Xlm-t: Scaling up multilingual machine\ntranslation with pretrained cross-lingual transformer\nencoders. arXiv preprint arXiv:2012.15547.\nR. Thomas McCoy, Junghyun Min, and Tal Linzen.\n2020. BERTs of a feather do not generalize together:\nLarge variability in generalization across models with\nsimilar test set performance. In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP, pages 217–227,\nOnline. Association for Computational Linguistics.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual knowl-\nedge in gpt.\nWilliam Merrill, Vivek Ramanujan, Yoav Goldberg, Roy\nSchwartz, and Noah A. Smith. 2021. Effects of pa-\nrameter norm growth during transformer training:\nInductive bias from gradient descent. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1766–1781, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2020.\nWhat the [mask]? making sense of language-specific\nbert models.\nJoe O’Connor and Jacob Andreas. 2021. What context\nfeatures can transformer language models use? In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 851–864,\nOnline. Association for Computational Linguistics.\nRamakanth Pasunuru and Mohit Bansal. 2019. Contin-\nual and multi-task architecture search.\nMatthew E. Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018. Dissecting contextual word\nembeddings: Architecture and representation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1499–\n1509, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nMichael Petrov, Chelsea V oss, Ludwig Schu-\nbert, Nick Cammarata, Gabriel Goh, and\nChris Olah. 2021. Weight banding. Distill.\nHttps://distill.pub/2020/circuits/weight-banding.\nHieu Pham, Melody Y . Guan, Barret Zoph, Quoc V . Le,\nand Jeff Dean. 2018. Efficient neural architecture\nsearch via parameter sharing.\nTiago Pimentel and Ryan Cotterell. 2021. A bayesian\nframework for information-theoretic probing.\nTiago Pimentel, Naomi Saphra, Adina Williams, and\nRyan Cotterell. 2020a. Pareto probing: Trading off\naccuracy for complexity. CoRR, abs/2010.02180.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020b. Information-theoretic probing for linguistic\nstructure. CoRR, abs/2004.03061.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4996–5001, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. arXiv preprint\narXiv:2103.00020.\n157\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nMarco Ribeiro, Sameer Singh, and Carlos Guestrin.\n2016. “why should I trust you?”: Explaining the pre-\ndictions of any classifier. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Demon-\nstrations, pages 97–101, San Diego, California. As-\nsociation for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics, 8:842–866.\nAnsgar Rössig and Milena Petkovic. 2021. Advances\nin verification of ReLU neural networks. Journal of\nGlobal Optimization, 81(1).\nSebastian Ruder, Ivan Vuli´c, and Anders Søgaard. 2019.\nA survey of cross-lingual word embedding models.\nJournal of Artificial Intelligence Research, 65:569–\n631.\nGözde Gül ¸ Sahin, Clara Vania, Ilia Kuznetsov, and Iryna\nGurevych. 2020. Linspector: Multilingual probing\ntasks for word representations. Computational Lin-\nguistics, 46(2):335–385.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nNaomi Saphra. 2021. Training dynamics of neural lan-\nguage models. Ph.D. thesis, The University of Edin-\nburgh.\nSofia Serrano and Noah A. Smith. 2019. Is attention in-\nterpretable? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2931–2951, Florence, Italy. Association for\nComputational Linguistics.\nCosma Rohilla Shalizi. 2003. Optimal nonlinear predic-\ntion of random fields on networks. In DMCS.\nCosma Rohilla Shalizi, Robert Haslinger, Jean-Baptiste\nRouquier, Kristina Lisa Klinkner, and Cristopher\nMoore. 2006. Automatic filters for the detection of\ncoherent structure in spatiotemporal systems. Physi-\ncal review. E, Statistical, nonlinear, and soft matter\nphysics, 73 3 Pt 2:036104.\nCosma Rohilla Shalizi, Kristina Lisa Shalizi, and Robert\nHaslinger. 2004. Quantifying self-organization with\noptimal predictors. Physical review letters , 93\n11:118701.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural MT learn source syntax? In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1526–\n1534, Austin, Texas. Association for Computational\nLinguistics.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kun-\ndaje. 2019. Learning important features through\npropagating activation differences.\nAnupam Shukla, Ritu Tiwari, and Rahul Kala. 2010.\nModular Neural Networks, pages 307–335. Springer\nBerlin Heidelberg, Berlin, Heidelberg.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional hy-\npothesis: Order word matters pre-training for little.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2888–2913, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nDavid So, Quoc Le, and Chen Liang. 2019. The evolved\ntransformer. In Proceedings of the 36th International\nConference on Machine Learning, volume 97 of Pro-\nceedings of Machine Learning Research, pages 5877–\n5886. PMLR.\nDavid R. So, Wojciech Manke, Hanxiao Liu, Zihang\nDai, Noam Shazeer, and Quoc V . Le. 2021. Primer:\nSearching for efficient transformers for language\nmodeling. CoRR, abs/2109.08668.\nJacob Steinhardt. 2022. Future ml systems will\nbe qualitatively different. Https://bounded-\nregret.ghost.io/future-ml-systems-will-be-\nqualitatively-different/.\nEmma Strubell, Patrick Verga, Daniel Andor, David\nWeiss, and Andrew McCallum. 2018. Linguistically-\ninformed self-attention for semantic role labeling.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5027–5038, Brussels, Belgium. Association for Com-\nputational Linguistics.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Pro-\nceedings of the 34th International Conference on\nMachine Learning, volume 70 of Proceedings of Ma-\nchine Learning Research, pages 3319–3328. PMLR.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam\nPoliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R. Bowman, Dipanjan Das, and\nEllie Pavlick. 2019. What do you learn from con-\ntext? Probing for sentence structure in contextu-\nalized word representations. arXiv e-prints, page\narXiv:1905.06316.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nElena V oita and Ivan Titov. 2020. Information-theoretic\nprobing with minimum description length. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) ,\npages 183–196, Online. Association for Computa-\ntional Linguistics.\n158\nChelsea V oss, Gabriel Goh, Nick Cammarata,\nMichael Petrov, Ludwig Schubert, and Chris\nOlah. 2021. Branch specialization. Dis-\ntill. Https://distill.pub/2020/circuits/branch-\nspecialization.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Probing\npretrained language models for lexical semantics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7222–7240, Online. Association for Computa-\ntional Linguistics.\nJennifer C. White, Tiago Pimentel, Naomi Saphra, and\nRyan Cotterell. 2021. A non-linear structural probe.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 132–138, Online. Association for Computa-\ntional Linguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 11–20, Hong Kong, China. Association for\nComputational Linguistics.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 833–844, Hong\nKong, China. Association for Computational Linguis-\ntics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nYadollah Yaghoobzadeh, Katharina Kann, T. J. Hazen,\nEneko Agirre, and Hinrich Schütze. 2019. Probing\nfor semantic classes: Diagnosing the meaning con-\ntent of word embeddings. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5740–5753, Florence, Italy. Asso-\nciation for Computational Linguistics.\nHongyang Zhang, Junru Shao, and Ruslan Salakhutdi-\nnov. 2020. Deep neural networks with multi-branch\narchitectures are intrinsically less non-convex. In\nAISTATS 2019 - 22nd International Conference on\nArtificial Intelligence and Statistics.\nKelly Zhang and Samuel Bowman. 2018. Language\nmodeling teaches you more than translation does:\nLessons learned through auxiliary syntactic task anal-\nysis. In Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP , pages 359–361, Brussels, Bel-\ngium. Association for Computational Linguistics.\nZining Zhu and Frank Rudzicz. 2020. An informa-\ntion theoretic view on selecting linguistic probes. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9251–9262, Online. Association for Computa-\ntional Linguistics.\nLiu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021. A\nrobustly optimized BERT pre-training approach with\npost-training. In Proceedings of the 20th Chinese\nNational Conference on Computational Linguistics,\npages 1218–1227, Huhhot, China. Chinese Informa-\ntion Processing Society of China.\n159",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6314520239830017
    },
    {
      "name": "Dynamics (music)",
      "score": 0.5552991032600403
    },
    {
      "name": "Cognitive science",
      "score": 0.3769277036190033
    },
    {
      "name": "Linguistics",
      "score": 0.34235680103302
    },
    {
      "name": "Library science",
      "score": 0.3377404510974884
    },
    {
      "name": "Psychology",
      "score": 0.22953489422798157
    },
    {
      "name": "Pedagogy",
      "score": 0.07875838875770569
    },
    {
      "name": "Philosophy",
      "score": 0.06050899624824524
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4403386650",
      "name": "Centre de Robotique",
      "country": null
    },
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4387154232",
      "name": "AIR Institute",
      "country": null
    },
    {
      "id": "https://openalex.org/I118501908",
      "name": "National Research University Higher School of Economics",
      "country": "RU"
    },
    {
      "id": "https://openalex.org/I1294991024",
      "name": "Air University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    }
  ],
  "cited_by": 7
}