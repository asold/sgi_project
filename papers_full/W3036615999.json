{
  "title": "Memory Transformer.",
  "url": "https://openalex.org/W3036615999",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3078001183",
      "name": "Mikhail Burtsev",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2803864553",
      "name": "Yuri Kuratov",
      "affiliations": [
        "Moscow Institute of Physics and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3129495158",
      "name": "Anton Peganov",
      "affiliations": [
        "Moscow Institute of Physics and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3036148235",
      "name": "Grigory V. Sapunov",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2257408573",
    "https://openalex.org/W2798998913",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2949626814",
    "https://openalex.org/W2803728898",
    "https://openalex.org/W2951777936",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963310665"
  ],
  "abstract": "Transformer-based models have achieved state-of-the-art results in many natural language processing tasks. The self-attention architecture allows transformer to combine information from all elements of a sequence into context-aware representations. However, information about the context is stored mostly in the same element-wise representations. This might limit the processing of properties related to the sequence as a whole more difficult. Adding trainable memory to selectively store local as well as global representations of a sequence is a promising direction to improve the Transformer model. Memory-augmented neural networks (MANNs) extend traditional neural architectures with general-purpose memory for representations. MANNs have demonstrated the capability to learn simple algorithms like Copy or Reverse and can be successfully trained via backpropagation on diverse tasks from question answering to language modeling outperforming RNNs and LSTMs of comparable complexity. In this work, we propose and study few extensions of the Transformer baseline (1) by adding memory tokens to store non-local representations, (2) creating memory bottleneck for the global information, (3) controlling memory update with dedicated layer. We evaluate these memory augmented Transformers and demonstrate that presence of memory positively correlates with the model performance for machine translation and language modelling tasks. Augmentation of pre-trained masked language model with memory tokens shows mixed results for tasks from GLUE benchmark. Visualization of attention patterns over the memory suggest that it improves the model's ability to process a global context.",
  "full_text": "MEMORY TRANSFORMER\nMikhail S. Burtsev\nNeural Networks and Deep Learning Lab\nMoscow Institute of Physics and Technology\nDolgoprudny, Russia\nburtcev.ms@mipt.ru\nYuri Kuratov\nNeural Networks and Deep Learning Lab\nMoscow Institute of Physics and Technology\nDolgoprudny, Russia\nyurii.kuratov@phystech.edu\nAnton Peganov\nNeural Networks and Deep Learning Lab\nMoscow Institute of Physics and Technology\nDolgoprudny, Russia\npeganov@phystech.edu\nGrigory V . Sapunov\nIntento, Inc.\nBerkeley, CA 94704\ngs@inten.to\nABSTRACT\nTransformer-based models have achieved state-of-the-art results in many natural\nlanguage processing tasks. The self-attention architecture allows transformer to\ncombine information from all elements of a sequence into context-aware repre-\nsentations. However, information about the context is stored mostly in the same\nelement-wise representations. This might limit the processing of properties related\nto the sequence as a whole more difﬁcult. Adding trainable memory to selectively\nstore local as well as global representations of a sequence is a promising direc-\ntion to improve the Transformer model. Memory-augmented neural networks\n(MANNs) extend traditional neural architectures with general-purpose memory\nfor representations. MANNs have demonstrated the capability to learn simple\nalgorithms like Copy or Reverse and can be successfully trained via backpropaga-\ntion on diverse tasks from question answering to language modeling outperform-\ning RNNs and LSTMs of comparable complexity. In this work, we propose and\nstudy few extensions of the Transformer baseline (1) by adding memory tokens to\nstore non-local representations, (2) creating memory bottleneck for the global in-\nformation, (3) controlling memory update with dedicated layer. We evaluate these\nmemory augmented Transformers and demonstrate that presence of memory posi-\ntively correlates with the model performance for machine translation and language\nmodelling tasks. Augmentation of pre-trained masked language model with mem-\nory tokens shows mixed results for tasks from GLUE benchmark. Visualization\nof attention patterns over the memory suggest that it improves the model’s ability\nto process a global context.\n1 I NTRODUCTION\nTransformers (Vaswani et al., 2017) are extremely successful in a wide range of natural language\nprocessing and other tasks. Due to the self-attention mechanism transformer layer can be trained\nto update a vector representation of every element with information aggregated over the whole\nsequence. As a result, rich contextual representation for every token is generated at the end of\nencoding. However, a combination of local and global information in the same vector has its lim-\nitations. Distributed storage of global features results in ”blurring” and makes it harder to access\nthem. Another well-known deﬁciency of Transformers is poor scaling of attention span that hurts its\napplications to long sequences.\nIn our work, we propose and study a simple technique to augment Transformer with memory rep-\nresentation (MemTransformer). We extend the Transformer baseline by adding [mem] tokens at\nthe beginning of the input sequence and train the model to see if it is able to use them as universal\nmemory storage. To assess the capacity of proposed memory augmentation, we additionally applied\nit to a number of other architectures. In the MemCtrl model update of [mem] tokens is controlled\n1\narXiv:2006.11527v2  [cs.CL]  16 Feb 2021\nby dedicated Transformer layer. MemBottleneck model has removed attention between sequence el-\nements, thus making memory the only channel to access global information about the sequence. We\nalso tested memory augmented BERT (Devlin et al., 2019) and Transformer XL (Dai et al., 2019)\nmodels.\nOur work lies at the intersection of two research directions Memory-augmented neural networks\n(MANNs) and Transformers. The history of memory augmentation in neural networks is pretty\nlong. Classic Long-Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) can be seen as\na simple yet powerful form of ﬁne-grained memory augmentation with a single memory value per\nLSTM cell and memory control logic implemented by internal learnable gates. Thus, in LSTMs,\ncomputations are heavily intertwined with memory. In contrast to that, memory-augmented neu-\nral networks incorporate external-memory, which decouples memory capacity from the number of\nmodel parameters. Neural Turing Machines (NTMs) (Graves et al., 2014) and Memory Networks\n(Weston et al., 2014) are among the best-knows MANNs that provide powerful random access op-\nerations over external memory. Memory Networks (Weston et al., 2014; Sukhbaatar et al., 2015)\nare trained to iteratively reason by combining sequence representation and embeddings in long-\nterm memory with the help of attention. NTMs, and their successors Differentiable Neural Com-\nputer (DNC) (Graves et al., 2016) and Sparse DNC (Rae et al., 2016) are recurrent neural networks\nequipped with a content-addressable memory, similar to Memory Networks, but with the additional\ncapability to write to memory over time. The memory is accessed by a controller network, typi-\ncally an LSTM. The full model is differentiable and can be trained via back-propagation through\ntime (BPTT). There is also a line of work to equip neural networks (typically, LSTMs) with data\nstructures like stacks, lists, or queues (Joulin & Mikolov, 2015; Grefenstette et al., 2015). MANN\narchitectures with a more advanced addressing mechanisms such as address-content separation and\nmulti-step addressing were proposed in (Gulcehre et al., 2016; 2017; Meng & Rumshisky, 2018).\nFamily of Transformer models have been recently applied to many deep learning tasks and proved\nto be very powerful for the language modeling tasks. The core element of Transformers is self-\nattention that allows updating representation for every element with information aggregated over the\nwhole sequence. Self-attention scales as O(N2) with a sequence length, and as a result, it is severely\nlimited in application to long sequences.\nThere is a separate line of work dedicated to reducing the computational cost of the transformer\nattention to O(N\n√\nN) using sparsity (Child et al., 2019), O(N log N) with local-sensitive hashing\n(Kitaev et al., 2020) or even O(N) with low-rank approximations (Wang et al., 2020), kernel-based\nformulation (Katharopoulos et al., 2020), or sparse attention with randomness (Zaheer et al., 2020).\nSeveral recent approaches try to solve this problem by adding some kinds of memory elements to\ntheir architecture. Transformer-XL (Dai et al., 2019) adds segment-level recurrence with state reuse,\nwhich can be seen as a sort of memory. During training, the hidden state sequence computed for the\nprevious segment is ﬁxed and cached to be reused as an extended context when the model processes\nthe next segment. Compressive Transformer (Rae et al., 2019) extends the ideas of Transformer-\nXL by incorporating the second level of the memory into the architecture. Memory on the second\nlevel stores information from the short-term memory of the ﬁrst level in compressed form. Memory\nLayers (Lample et al., 2019) replace a feed-forward layer with a product key memory layer, that can\nincrease model capacity for a negligible computational cost.\nSome transformers introduce different sorts of global representations. Among the most recent archi-\ntectures with global representations are Star-Transformer (Guo et al., 2019), Longformer (Beltagy\net al., 2020), Extended Transformer Construction (ETC) (Ainslie et al., 2020) and its successor Big\nBird (Zaheer et al., 2020). All these architectures reduce full self-attention to some local or patterned\nattention and combine it with a sparse global attention bottleneck. For example, Longformer uses\nselected tokens such as [CLS] or tokens for question marks to accumulate and redistribute global\ninformation to all other elements of the sequence. Among these, the BigBird-ETC with dedicated\n”global” tokens is the most similar to our MemTransformer approach.\nOur MemTransformer, MemCtrl and MemBottleneck Transformer models can be seen as more gen-\neral limit cases for this class of models. They have dedicated general purpose[mem] tokens that can\nbe used by the model as a placeholders to store and process global or copy of local representations.\nMemTransformer has full self-attention over the memory+input sequence. In contrast, MemBottle-\n2\nFigure 1: Memory modiﬁcations of Transformer architecture. (a) Transformer layer. For ev-\nery element of a sequence (solid arrow), self-attention produces aggregate representation from all\nother elements (dashed arrow). Then this aggregate and the element representations are combined\nand updated with a fully-connected feed-forward network layer. (b) Memory Transformer (Mem-\nTransformer) prepends input sequence with dedicated [mem] tokens. This extended sequence is\nprocessed with a standard Transformer layer without any distinction between [mem] and other ele-\nments of the input. (c) Compared to MemTransformer MemCtrl Transforemer has dedicated mem-\nory controller sub-network. (d) Memory Bottleneck Transformer (MemBottleneck Transformer)\nuses [mem] tokens but separates memory and input attention streams. At the ﬁrst step, representa-\ntions of [mem] tokens are updated (2) with the attention span (1) covering both memory and input\nsegments of the sequence. Then representations of input elements are updated (4) with memory\nattention (3) only. Thus information ﬂow is distributed to representations of elements only through\nthe memory.\nneck has full both-way attention between the input sequence and memory but no attention between\nsequence tokens.\n2 M EMORY IN TRANSFORMER\n2.1 B ACKGROUND : T RANSFORMER ARCHITECTURE\nThe process of calculating single Transformer self-attention layer can be seen as a two-step process-\ning ﬂow (see ﬁg. 1a).\n1. Self-attention. Calculate normalized sum of input X with multi-head attention\nMH (Q, K, V) between all elements of the sequence:\nA = LN(X + MH (X, X, X)). (1)\n2. Update. For every element of the sequence update aggregated representation A with FF\nfeed-forward sub-layer then add skip connection and normalize:\nH = LN(A + FF (A)). (2)\n3\n2.2 S IMPLE MEMTRANSFORMER\nThe ﬁrst proposed model is a simple extension of a baseline Transformer we call MemTransformer.\nThe idea is to add m special [mem]ory tokens to the standard input (see ﬁg. 1b) then process them\nin a standard way. So, the input vectors X became the concatenation of the memory token vectors\nXmem and the original input token vectors Xseq:\nXmem+seq = [Xmem; Xseq] ∈ R(n+m)×d, Xmem ∈ Rm×d, Xseq ∈ Rn×d.\nThis modiﬁcation can be applied independently to encoder and/or decoder. The rest of the Trans-\nformer stays the same with the multi-head attention layer processing the extended input.\n2.3 M EMCTRL TRANSFORMER\nIn the simple MemTransformer tokens of the memory and the sequence are processed by layers with\nthe same parameters. Thus memory and sequence updated in a similar way. To test if dedicated sub-\nnetwork for memory update might improve performance we introduce a separate memory control\nlayer (see ﬁg. 1c). Thus, memory representation of MemCtrl Transformer is updated as:\nAmem = LN(Xmem + MH mem(Xmem, Xmem+seq, Xmem+seq)),\nHmem = LN(Amem + FF mem(Amem)).\nSequence representation is updated as:\nAseq = LN(Xseq + MH seq(Xseq, Xmem+seq, Xmem+seq)),\nHseq = LN(Aseq + FF seq(Aseq)).\n2.4 M EMBOTTLENECK TRANSFORMER\nIn the MemTransformer input and [mem] tokens are updated inside the same traditional self-attend\nand update processing ﬂow. In this case, representations of the input sequence elements potentially\nmight be updated “as usual” without attending to the content of the memory. Here, global infor-\nmation can propagate in a “peer to peer” manner. To block this distributed information ﬂow and\nseparate storage and processing of global and local representations, we add a memory bottleneck.\nThe resulting MemBottleneck Transformer has two-staged processing ﬂow (see ﬁg. 1d).\n1. Memory update. First, calculate attention between every memory token and full sequence of\nmemory Xmem and input Xseq (see Step 1 on the ﬁg. 1d), and update memory token representations\n(see Step 2 on the ﬁg. 1d):\nAmem = LN(Xmem + MH mem(Xmem, Xmem+seq, Xmem+seq)),\nHmem = LN(Amem + FF mem(Amem)).\n2. Sequence update. Calculate attention between sequence and memory (Step 3 on the ﬁg. 1d),\nand update sequence token representations (Step 4 on the ﬁg. 1d):\nAseq = LN(Xseq + MH seq(Xseq, Hmem, Hmem)),\nHseq = LN(Aseq + FF seq(Aseq)).\nIn other words, the memory “attends” to itself and a sequence, and the sequence “attends” only to\nthe memory. This should force the model to accumulate and re-distribute global information through\nmemory. Computations for MemBottleneck Transformer scales linearly with the size of the input\nsequence or memory O(NM ), when the traditional transformer scales as O(N2).\nFor all encoder-decoder variants of the memory transformers the decoder part was the same as in\nthe baseline. Output of the last encoder layer [Hmem; Hseq] passed to the decoder layers.\n4\nTable 1: Performance of baseline and memory models on WMT-14 DE-EN translation. Values represent\nan average of BLEU 4 scores for 3 runs of every model evaluated on 2000 samples from WMT-14 DE-EN\nvalidation set.\nSmall models Base models\n4 layers per encoder/decoder, 20 epochs 6 layers per encoder/decoder, 10 epochs\nTransformer (baseline) 19.01 Transformer (baseline) 24.65\nMemTransformer 5 19.17 - -\nMemTransformer 10 19.15 MemTransformer 10 25.07\nMemTransformer 20 19.14 MemTransformer 20 25.58\nMemBottleneck Transformer 10 11.20 MemCtrl Transformer 20 24.13\nMemBottleneck Transformer 20 10.41 MemCtrl Shared Transformer 20 25.73\nMemBottleneck Skip Transformer 20 16.45 - -\n3 R ESULTS AND DISCUSSION\nAs a reference model for a machine translation task we use a vanilla Transformer from ofﬁcial\nTensorFlow tutorial1. Two model sizes were studied for a machine translation task small2 with N\n= 4 and base3 with N = 6 layers in the encoder. The decoder has the same number of layers as the\nencoder. For a language modeling task we augmented Transformer XL (Dai et al., 2019) base4 with\n20 [mem]tokens. For a masked language model memory augmentation we used pre-trained BERT5\n(Devlin et al., 2019). All values reported in the paper are averaged over 3 runs if otherwise stated.\n3.1 P ERFORMANCE METRICS\nThe main hypothesis of the study says that adding memory to multilayered encoder-decoder archi-\ntectures should result in better performance for sequence processing tasks such as machine transla-\ntion. BLEU scores for WMT-14 DE-EN translation task (Bojar et al., 2014) are presented in Table 1.\nAfter 20 epochs of training, small MemTransformer models have similar scores and clearly outper-\nform the Transformer baseline. Base 6-layer MemTransformer with 10 memory tokens improves\nthe baseline, and doubling the memory up to 20 tokens results in an even higher score of 25.58.\nThis is a modest but solid performance given no hyperparameters ﬁne tuning and beam search were\nused. MemTransformer results supports our intuition that self-attention could be trained to utilize\nrepresentations of extra memory elements that are not related to the input sequence to improve the\nquality of encoding. Surprisingly, adding separate layer for memory control decreases scores below\nbaseline (see MemCtrl 20 in Table 1). On the other hand, memory controller with shared parameters\nfor all 6 encoder layers (MemCtrl Shared 20 in Table 1) demonstrates the best performance among\nmodiﬁcations we studied for this task.\nThe MemTransformer results suggest that if memory extends but not intervene in the Transformer\nsequence processing, then it is beneﬁcial. But to what extent processing of relations between ele-\nments of the sequence can be abstracted to memory? Experiments with MemBottleneck Transformer\n(Table 1) shows that it is possible, but performance suffers. This can be due to the more complex\narchitecture of the MemBottleneck that has twice more layers in the encoder part (see ﬁg. 1c.). So, it\nis more difﬁcult and longer to train compared to baseline. On the other hand, degraded performance\ncan also be attributed to the insufﬁcient throughput of the memory bottleneck. Then, there might\nbe a trade-off between the size of the bottleneck and the complexity of learning a deeper network.\nFrom the experiments, we see that MemBottleneck 10 learns faster and has lower loss compared\nto MemBottleneck 20, which points to the complexity of training but not the bottleneck width as a\nmajor factor limiting performance.\nThe limit scenario for the MemBottleneck model is when only memory representations are pro-\ncessed. In MemBottleneck Skip modiﬁcation of Transformer, representations for sequence tokens\n1https://www.tensorflow.org/tutorials/text/transformer\n2dmodel = 128, dff = 512, h= 8, Pdrop = 0.1, batch= 64, warmupsteps = 4000\n3dmodel = 512, dff = 2048, h= 8, Pdrop = 0.1, batch= 64, warmupsteps = 32000\n4https://github.com/kimiyoung/transformer-xl\n5bert-base-cased checkpoint from HuggingFace Transformers (Wolf et al., 2020) was trained with\nDeepPavlov (Burtsev et al., 2018) on GLUE tasks.\n5\nTable 2: Memory lesions. Performance of the models trained with memory gradually degrades if the memory\nsize is changed during inference.\nmemory size at inference\n0 2 5 10 20 30\nMemTransformer 10 11.75 15.91 18.22 25.07 12.39 7.87\nMemTransformer 20 3.87 8.58 9.75 14.51 25.58 7.51\nTable 3: Memory extension. Values represent an average of BLEU 4 scores for 3 runs of every model.\n20 epochs +5 epochs +10 epochs +15 epochs\nmem 5 mem 10 mem 15 mem 20\nMemTransformer 5 (small) 19.17 19.18 19.19 19.41\nTable 4: Memory augmentation for the language modeling task. Average performance after training on\nWikiText-103 (Merity et al., 2016) over 3 runs.\nTransformer-XL + 20 mem ﬁxed pos. emb. + 20 mem rel. pos. emb.\nbpc 3.182 3.179 3.176\nppl 24.09 24.02 23.95\nare not updated at all (steps 3 and 4 on the ﬁg. 1d are skipped) and encoder output consists of input\nsequence embeddings and memory representations. Quite unexpectedly, leaving only in memory\nprocessing in encoder signiﬁcantly improves 10.41 BLEU score of MemBottleneck 20 to 16.45\n(MemBottleneck Skip in Table 1).\nMemory models have better scores after training, but do they require memory for inference? If\nthe performance of trained MemTransformer will stay the same for the inference without [mem]\ntokens, then memory was only needed to improve training and not used for the processing of an input\nsequence. Results of memory lesion experiments presented in Table 2 demonstrate that removing\n[mem] tokens from MemTransformer input leads to a dramatic drop in BLEU score, from 25.07\nto 11.75 for the MemTransformer 10 and from 25.58 to 3.87 for MemTransformer 20 (both models\nhave 6-layers in the encoder). This is an indicator that the presence of [mem] tokens is critical for\nMemTransformer during inference.\nAnother important question is related to the universality of the learned memory controller. Is it\nable to utilize memory of arbitrary size, or can work only with the memory capacity it was trained?\nMemory lesions data (see Table 2) suggest that MemTransformer learns a solution that is partially\nrobust to the variations in the size of the memory. BLEU score of MemTransformer 10 with 5\n[mem] tokens shows it is still able to produce translation that makes sense . On the other hand, if\nwe add 20 more [mem] tokens to the same model, it will have scores that are lower even compared\nto the case when the model is evaluated without memory at all. Interestingly, the model trained with\na larger memory size of 20 has weaker generalization abilities. It is evident from the more steep\ndecline of performance with the deviation of memory size from the one used during training.\nAs we see from memory ablation results (Table 2) increasing memory without ﬁne tuning hurts\nperformance. But, what will happen if the model will be ﬁne-tuned after memory extension? To\nanswer this question we take small MemTransformer 5 pre-trained for 20 epochs and grow it’s\nmemory in a few stages up to 20 [mem] tokens. On every stage 5 [mem] tokens were added and\nthe model was ﬁne tuned for 5 epochs. Results are presented in Table 3. Extension of the memory\nfollowed by ﬁne tuning proved to be beneﬁcial and resulted in the model with the highest BLEU\nscore among all small sized modiﬁcations.\nTo test an effect of mem tokens on performance in a language modelling task we trained Trans-\nformer XL base augmented with memory of size 20. Original Transformer XL has ﬁxed and relative\npositional encodings, so results for the both options and the baseline are presented in the Table 4.\nMemory augmentation allows the model to achieve better perplexity.\nPositive memory extension results suggested experiments with memory augmentation of an already\npre-trained encoders. We took a BERT-base model and augmented it with a memory of different\n6\nsizes. The model was trained on datasets from the GLUE (Wang et al., 2018) benchmark. Adding\n[mem] tokens to BERT-base model improved its performance on 6 / 9 tasks as shown in the Table 5.\n3.2 A TTENTION PATTERNS IN MEMORY\nGeneric system with memory relies on three types of operations, such as writing, reading and pro-\ncessing. In this section we present results of analysis of the inner workings of memory augmented\ntransformers to localize these operations. Following previous studies (Kovaleva et al., 2019; Clark\net al., 2019), we visually explored attention patterns. Kovaleva et al. (2019) introduced ﬁve cate-\ngories of self-attention and suggested that only ”heterogeneous” patterns that spread attention across\nall input tokens might extract non-trivial information about the linguistic structure. Numerous ob-\nservations of attention maps across baseline Transformer and MemTransformer models allow us to\nconclude that the overall structure and distribution of pattern types in the sequence to sequence part\nof the attention mechanism are similar. Thus, for further analysis, we skip sequence to sequence\nattention and focus on memory to sequence, memory to memory, and sequence to memory attention\npatterns. All attention maps for selected models are presented in the Appendix.\nMemory to sequence attention makes it possible to selectively update vectors stored in [mem]\ntoken positions with representations of input sequence elements. Such an update is a form of soft\nwrite to memory operation. Indeed, we found many patterns consistent with writing from sequence\nto memory in all MemTransformer models. One of them is shown in Figure 2a. Write to memory\ntype of attention is more frequent in the ﬁrst layers and almost absent in the deeper part of the\nencoder.\nMemory to memory attention allows recombining vectors of [mem] tokens. We found a few\ncommon patterns related to in-memory processing. The most common arrangement of activity is\ndiagonal. The diagonal can be blurred (or ”soft”), making local fusion of the neighboring memory\nrepresentations possible. Examples of this operation can be seen in the left top corner of the ﬁgures\n2a., 2b. and 2c. If diagonal attention is sharp (see ﬁg. 2c. in the middle), then corresponding memory\nvectors are added to themselves, so their content is ampliﬁed and became more error-tolerant. This\ncan be seen as a store operation. Another possible operation is a block copy (examples can be\nfound in the Appendix). It is usually manifested as a vertical block of attention. In this case,\na number of consequent [mem] vectors are updated with the same values aggregated from some\nanother sequence of [mem] vectors. A copy operation can also be performed in a[mem] to [mem]\nmanner with preserving a source order as in ﬁgure 2b. or with reversing the order (see Appendix for\nexamples).\nSequence to memory attention implements read from memory operation and can be found in the\nﬁrst layers of the encoder, but it is more pronounced in the middle layers of the decoder. A typical\nexample of the memory ”reading” is presented in Figure 2d. Note, that during decoding token\nrepresentation is updated by reading from a block of subsequent [mem] tokens.\nThe overall pipeline of memory processingis similar for the different runs and sizes of MemTrans-\nformer. It consists of writing some information from the input sequence to the memory in the ﬁrst\nlayers of the encoder, then followed by memory processing in the intermediate layers and ampliﬁca-\ntion in the output layers of the encoder. During decoding, information is read from memory. Here,\nthe highest ”reading” activity is commonly observed in the intermediate decoder layers. Interest-\nTable 5: Results on GLUE dev set with [mem] tokens added only for end task ﬁne-tuning. Each [mem]\nwas randomly initialized and trained only on the GLUE task. All runs were repeated 5 times and average scores\nare reported. +pool stands for using concatenation of max and avg pooling over outputs for [mem] tokens\ninstead of the output from [CLS] token for classiﬁcation.\nCoLA SST-2 MRPC STS-B QQP MNLI-m/mm QNLI RTE\nBERT-base 62.9 92.7 90.2/85.8 86.0/85.8 86.6/89.8 83.0/ 83.5 90.5 65.0\n5mem 61.3 92.4 90.4/86.4 86.0/85.8 86.8/90.1 82.7/83.3 90.7 68.0\n5mem+pool 62.1 92.3 89.4/84.8 85.8/85.6 86.9/ 90.2 83.3 /83.3 90.8 60.2\n10mem 60.6 92.5 91.3/87.6 86.6/86.4 86.4/89.8 82.8/83.3 90.5 66.8\n10mem+pool 62.6 92.6 90.2/86.0 86.7/86.5 87.1/90.2 83.1/83.0 90.7 61.2\n20mem 60.9 92.4 91.2/87.5 86.4/86.2 86.8/90.1 82.8/83.1 90.7 65.3\n7\nFigure 2: Operations with memory learned by MemTransformer 10. (a) The pattern of self-\nattention in the 3 rd encoder layer. Here, [mem] tokens in the central segment of memory (on the\nleft) attend to the vector representations of tokens Technik, Entwicklung, Intelligenz\n(and some others). This attention values are consistent with the writing of selected token vectors to\nthe [mem] tokens. Activity in the left top corner that involves ﬁrst four tokens might indicatefusion\nof neighbour vectors by pairwise summation of [mem] tokens. (b) In the next 4th layer of the same\nencoder similar fusion operation with the same [mem]’s is repeated. A parallel diagonal activity\njust below the fusion pattern can be attributed to copy operation. (c) Another attention head in the\nsame encoder layer demonstrates combination of fusion andstore operations. Sharp self-attention of\nthree tokens in the middle results in adding vectors to themselves. (d) Attention pattern in decoder\nlayer 4 over the output of 6 th encoder layer suggest that vectors of [mem] tokens are selectively\nread and added to the output token representations during decoding.\ningly, memory-related attention patterns usually have a block structure. For example, patterns form\nthe particular MemTransformer 10 presented in Figure 2 suggest that the model had learned to split\nmemory into three blocks. Commonly, the same memory operation is applied to all [mem]s of the\nsame block by one particular head. During memory processing, the model can operate in a block-\nwise manner, as in Figure 2b, where the ”block(1-3)” is copying to the ”block(4-6)”. We speculate\nthat the block structure of memory processing might reduce error rate because the memory repre-\nsentation is ”averaged” over the[mem]s of the block during reading (see ﬁg. 2d). Experiments with\nMemBottleneck architecture show that the model might be able to learn how to copy representations\nof the input sequence into the memory of the ﬁxed size and use only this memory during decoding.\n4 C ONCLUSIONS\nWe proposed and studied a series of memory augmented transformer based architecturesMemTrans-\nformer, MemCtrl and MemBottleneck transformers. Qualitative analysis of attention patterns pro-\nduced by the transformer heads trained to solve machine translation task suggests that both models\nsuccessfully discovered basic operations for memory control. Attention maps show evidence for the\npresence of memory read/write as well as some in-memory processing operations such as copying\nand summation.\nA comparison of machine translation quality shows that adding general-purpose memory in Mem-\nTransformer improves performance over the baseline. Moreover, the ﬁnal quality positively corre-\nlates with the memory size. On the other hand, MemBottleneck Transformer, with all self-attention\nrestricted to the memory only, has signiﬁcantly lower scores after training.\n8\nMemory lesion tests demonstrate that the performance of the pre-trained MemTransformer model\ncritically depends on the presence of memory. Still, the memory controller learned by the model\ndegrades only gradually when memory size is changed during inference. This indicates that the\ncontroller has some robustness and ability for generalization. We also found, that extension of\nmemory followed by ﬁne tuning leads to better performance.\nApplication of proposed technique to language model training as well as ﬁne-tuning of BERT based\nencoder for a battery of GLUE tasks further demonstrated beneﬁcial effect of memory augmentation.\nThis suggests that simple addition of [mem] tokens can extend almost any ”encoder-decoder with\nattention” framework. It can be also applied to the tasks that depend on the multi-hop reasoning or\nplanning. In this cases memory should help to store and process representations for the intermediate\nstages of the solution.\n9\nREFERENCES\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Philip Pham, Anirudh Ravula, and Sumit Sanghai.\nEtc: Encoding long and structured data in transformers, 2020.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer,\n2020.\nOndrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes\nLeveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia\nSpecia, and Ale s Tamchyna. Findings of the 2014 workshop on statistical machine trans-\nlation. In Proceedings of the Ninth Workshop on Statistical Machine Translation , pp. 12–\n58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. URL\nhttp://www.aclweb.org/anthology/W/W14/W14-3302.\nMikhail Burtsev, Alexander Seliverstov, Rafael Airapetyan, Mikhail Arkhipov, Dilyara Baymurzina,\nNickolay Bushkov, Olga Gureenkova, Taras Khakhulin, Yurii Kuratov, Denis Kuznetsov, et al.\nDeeppavlov: Open-source library for dialogue systems. In Proceedings of ACL 2018, System\nDemonstrations, pp. 122–127, 2018.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers, 2019.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What Does BERT\nLook at? An Analysis of BERT’s Attention. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 276–286, Florence,\nItaly, 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL\nhttps://www.aclweb.org/anthology/W19-4828.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V . Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and Short Papers) , pp. 4171–4186, 2019. URL\nhttps://aclweb.org/anthology/papers/N/N19/N19-1423/.\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.\nAlex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwi´nska, Sergio G ´omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,\nAdri`a Puigdom `enech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain,\nHelen King, Christopher Summerﬁeld, Phil Blunsom, Koray Kavukcuoglu, and Demis Hass-\nabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538\n(7626):471–476, October 2016. ISSN 00280836. URL http://dx.doi.org/10.1038/\nnature20101.\nEdward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to\ntransduce with unbounded memory, 2015.\nCaglar Gulcehre, Sarath Chandar, Kyunghyun Cho, and Yoshua Bengio. Dynamic neural turing\nmachine with soft and hard addressing schemes. arXiv preprint arXiv:1607.00036, 2016.\nCaglar Gulcehre, Sarath Chandar, and Yoshua Bengio. Memory augmented neural networks with\nwormhole connections. arXiv preprint arXiv:1701.08718, 2017.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. Star-\ntransformer, 2019.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural Comput. , 9(8):\n1735–1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL\nhttps://doi.org/10.1162/neco.1997.9.8.1735.\n10\nArmand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent\nnets, 2015.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ¸ois Fleuret. Transformers are\nrnns: Fast autoregressive transformers with linear attention, 2020.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer, 2020.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. Attention Module is Not Only\na Weight: Analyzing Transformers with Vector Norms. arXiv:2004.10102 [cs] , 2020. URL\nhttp://arxiv.org/abs/2004.10102.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the Dark Se-\ncrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pp. 4356–4365, Hong Kong, China, 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/D19-1445. URL https://www.aclweb.org/anthology/\nD19-1445.\nGuillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv ´e\nJ´egou. Large memory layers with product keys, 2019.\nYuanliang Meng and Anna Rumshisky. Context-aware neural model for temporal information ex-\ntraction. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pp. 527–536, 2018.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843, 2016.\nJack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex\nGraves, and Timothy P Lillicrap. Scaling memory-augmented neural networks with sparse reads\nand writes, 2016.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. Compressive\ntransformers for long-range sequence modelling, 2019.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks,\n2015.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in neural informa-\ntion processing systems, pp. 5998–6008, 2017. URL http://papers.nips.cc/paper/\n7181-attention-is-all-you-need .\nAlex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP , pp. 353–355, Brussels, Belgium, 2018. Association for Computational\nLinguistics. URL http://aclweb.org/anthology/W18-5446.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity, 2020.\nJason Weston, Sumit Chopra, and Antoine Bordes. Memory networks, 2014.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, and Jamie Brew. HuggingFace’s Trans-\nformers: State-of-the-Art Natural Language Processing. arXiv:1910.03771 [cs], February 2020.\nURL http://arxiv.org/abs/1910.03771.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers\nfor longer sequences, 2020.\n11\nA A TTENTION MAPS FOR MEMORY AUGMENTED TRANSFORMERS\nIn this section we present attention maps for two representative cases of MemTransformer and Mem-\nBottleneck transformer models. For both models we use the same input sequence.\nInput sequence: Langes Kurzzeitged¨achtnis ist eine Technik, die zur Verbesserung der Entwicklung\nvon k¨unstlicher Intelligenz wesentlich beigetragen hat.6\nPredicted translation MemTransformer 10 : Long, short-term memory is a technique that has\ncontributed signiﬁcantly to improving the development of artiﬁcial intelligence.\nPredicted translation MemBottleneck 20: The short time memory is a technique that has helped\nto improve the development of artiﬁcial intelligence in a lot of sense.\nReference: Long-term short-term memory is a technique that has contributed signiﬁcantly to im-\nproving the development of artiﬁcial intelligence.7\nA short guide to help with interpretation of attention maps is shown on the Figure 3.\nFigure 3: How to read Memory Transformer attention map. Attention values indicate how ele-\nments of input sequence (on the top) contribute to the update of representation for speciﬁc output el-\nement (on the left). Attention map for memory augmented transformer can be split into four blocks:\n(1) update - [sequence] to [sequence]; (2) write - [sequence] to [memory]; (3) read - [memory] to\n[sequence]; (4) process - [memory] to [memory].\nA.1 M EMTRANSFORMER ATTENTION MAPS\nVisualisation of attention maps for MemTransformer 10 (see Section 2.2) with a memory size of\n10 is presented on the Figure 4 for 6 layers of encoder and on the Figure 5 for 6 layers of decoder.\nEvery transformer layer has 8 attention heads. The model was trained for 10 epochs on WMT-14\nDE-EN (Bojar et al., 2014) dataset.\n6https://de.wikipedia.org/wiki/Long_short-term_memory\n7https://translate.google.com\n12\nFigure 4: MemTransformer 10 encoder attention maps. As the model encodes an input se-\nquence the change of attention patterns related to memory can be interpreted as aread-process-store\npipeline. Heads in layers 1 to 3 have manyread to memorypatterns. Patterns consistent with in mem-\nory processing are more frequent in layers 3-6. The last layer is dominated by diagonal attention\nthat can be seen as an ampliﬁcation of calculated representations.\n13\nFigure 5: MemTransformer 10 decoder attention maps. Every layer of the decoder has heads\nwith signs of memory reading activity. Reading patterns suggest that the representations in memory\nare locally grouped in 3 blocks.\n14\nA.2 M EMBOTTLENECK TRANSFORMER ATTENTION MAPS\nAttention patterns generated by MemBottleneck Transformer architecture (see Section 2.4) strongly\nsuggest that the model learned to copy a given sequence into a memory, process it and use only this\nrepresentations of input for decoding. The main idea of MemBottleneck is a restriction of global\ninformation exchange to memory. Therefore, an update for representations of the input sequence\nelements can access representations of other elements only by writing into and then reading them\nfrom memory. To do that, MemBottleneck uses two different transformer sub-layers each with its’\nown set of attention heads (see ﬁg. 1c).\nEncoder attention maps (see ﬁg. 6) suggest that, as expected, representations for the input elements\nare copied into memory in layers 1 and 2. Surprisingly, after that they are not properly updated\nanymore and the decoder mostly attends to the content of memory (see ﬁg. 7). This impressive\noutcome shows that transformer can be trained to read and process all the information about the\ninput sequence in memory only.\n15\nFigure 6: MemBottleneck 20 encoder attention maps. In the 1st layer, all attention heads of\nmemory sub-layer ([memory+sequence] to [memory]) read from the input sequence. Only 2 heads\nof memory sub-layer in the layer 2 reads from the input, but all others are diagonal to amplify content\nof the memory. No more input reading is present in layers 3 and 4. Notably, all heads of the 1st\nlayer memory attention have patterns that split into three blocks. The top block has sparse attention\nover the whole sequence without preserving the order. The middle block reads the ﬁrst half of the\nsequence in the reverse order, and the bottom block reads the rest in the proper order. This suggests\nencoding of global information in the top block and local information in the middle and bottom\nblocks. Layer 3 of memory sub-layer has sharp amplifying diagonals, and something like shifting\noperations represented by broken diagonals. Layer 4 of memory sub-layer demonstrates mainly\nheterogeneous patterns which indicates in memory processing. Maps of attention which belongs to\nthe sequence sub-layer ([memory] to [sequence]) of MemBottleneck layer degrade to vertical lines\nin layers 3 and 4. This is a sing that these attention heads are bypassed as follows from (Kobayashi\net al., 2020).\n16\nFigure 7: MemBottleneck 20 decoder attention maps. At the decoding phase, almost all heads\nattend to the content of memory but not on the representations of sequence elements.\n17",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.851050078868866
    },
    {
      "name": "Transformer",
      "score": 0.7239083647727966
    },
    {
      "name": "Language model",
      "score": 0.5974283218383789
    },
    {
      "name": "Auxiliary memory",
      "score": 0.5060519576072693
    },
    {
      "name": "Question answering",
      "score": 0.4839399456977844
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4805322289466858
    },
    {
      "name": "Bottleneck",
      "score": 0.44330480694770813
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4413122832775116
    },
    {
      "name": "Memory map",
      "score": 0.41403627395629883
    },
    {
      "name": "Artificial neural network",
      "score": 0.4046834409236908
    },
    {
      "name": "Natural language processing",
      "score": 0.3466240167617798
    },
    {
      "name": "Parallel computing",
      "score": 0.18795940279960632
    },
    {
      "name": "Shared memory",
      "score": 0.14545974135398865
    },
    {
      "name": "Embedded system",
      "score": 0.10588020086288452
    },
    {
      "name": "Computer hardware",
      "score": 0.09943005442619324
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I153845743",
      "name": "Moscow Institute of Physics and Technology",
      "country": "RU"
    }
  ],
  "cited_by": 4
}