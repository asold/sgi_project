{
    "title": "A vision transformer architecture for the automated segmentation of retinal lesions in spectral domain optical coherence tomography images",
    "url": "https://openalex.org/W4315490300",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2551983448",
            "name": "Daniel Philippi",
            "affiliations": [
                "Universidade Nova de Lisboa"
            ]
        },
        {
            "id": "https://openalex.org/A2094732573",
            "name": "Kai Rothaus",
            "affiliations": [
                "St. Franziskus Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2116914403",
            "name": "Mauro Castelli",
            "affiliations": [
                "Universidade Nova de Lisboa",
                "University of Ljubljana"
            ]
        },
        {
            "id": "https://openalex.org/A2551983448",
            "name": "Daniel Philippi",
            "affiliations": [
                "Universidade Nova de Lisboa"
            ]
        },
        {
            "id": "https://openalex.org/A2094732573",
            "name": "Kai Rothaus",
            "affiliations": [
                "St. Franziskus Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2116914403",
            "name": "Mauro Castelli",
            "affiliations": [
                "University of Ljubljana",
                "Universidade Nova de Lisboa"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2753708825",
        "https://openalex.org/W2565136318",
        "https://openalex.org/W2988969159",
        "https://openalex.org/W3015121411",
        "https://openalex.org/W4247214019",
        "https://openalex.org/W2745385871",
        "https://openalex.org/W2778338360",
        "https://openalex.org/W2967073593",
        "https://openalex.org/W2061240737",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2974844314",
        "https://openalex.org/W2917393555",
        "https://openalex.org/W2965153464",
        "https://openalex.org/W3136606386",
        "https://openalex.org/W3113024057",
        "https://openalex.org/W3089571734",
        "https://openalex.org/W3165736781",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W4221163766",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4295025017",
        "https://openalex.org/W4284975712",
        "https://openalex.org/W3216035653",
        "https://openalex.org/W2949122205",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W2985276900",
        "https://openalex.org/W6763367864",
        "https://openalex.org/W64421947",
        "https://openalex.org/W2888358068",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2917124146",
        "https://openalex.org/W1976468890",
        "https://openalex.org/W4241153917",
        "https://openalex.org/W4283160212",
        "https://openalex.org/W1909740415",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2991139962",
        "https://openalex.org/W3015788359",
        "https://openalex.org/W2884436604",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W1994616650",
        "https://openalex.org/W6601235158",
        "https://openalex.org/W6813345622",
        "https://openalex.org/W2974263490",
        "https://openalex.org/W2919070891",
        "https://openalex.org/W4205087751",
        "https://openalex.org/W3189659869",
        "https://openalex.org/W3010066814",
        "https://openalex.org/W3092531815",
        "https://openalex.org/W3033158565",
        "https://openalex.org/W3207698969",
        "https://openalex.org/W4212774754",
        "https://openalex.org/W4226187698",
        "https://openalex.org/W3016052689",
        "https://openalex.org/W4206057698",
        "https://openalex.org/W4251663120",
        "https://openalex.org/W3091933671",
        "https://openalex.org/W2738996050",
        "https://openalex.org/W2474683155",
        "https://openalex.org/W4246891456"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports\nA vision transformer architecture \nfor the automated segmentation \nof retinal lesions in spectral domain \noptical coherence tomography \nimages\nDaniel Philippi 1, Kai Rothaus 2 & Mauro Castelli 1,3*\nNeovascular age-related macular degeneration (nAMD) is one of the major causes of irreversible \nblindness and is characterized by accumulations of different lesions inside the retina. AMD biomarkers \nenable experts to grade the AMD and could be used for therapy prognosis and individualized \ntreatment decisions. In particular, intra-retinal fluid (IRF), sub-retinal fluid (SRF), and pigment \nepithelium detachment (PED) are prominent biomarkers for grading neovascular AMD. Spectral-\ndomain optical coherence tomography (SD-OCT) revolutionized nAMD early diagnosis by providing \ncross-sectional images of the retina. Automatic segmentation and quantification of IRF, SRF, and \nPED in SD-OCT images can be extremely useful for clinical decision-making. Despite the excellent \nperformance of convolutional neural network (CNN)-based methods, the task still presents some \nchallenges due to relevant variations in the location, size, shape, and texture of the lesions. This work \nadopts a transformer-based method to automatically segment retinal lesion from SD-OCT images \nand qualitatively and quantitatively evaluate its performance against CNN-based methods. The \nmethod combines the efficient long-range feature extraction and aggregation capabilities of Vision \nTransformers with data-efficient training of CNNs. The proposed method was tested on a private \ndataset containing 3842 2-dimensional SD-OCT retina images, manually labeled by experts of the \nFranziskus Eye-Center, Muenster. While one of the competitors presents a better performance in \nterms of Dice score, the proposed method is significantly less computationally expensive. Thus, future \nresearch will focus on the proposed network’s architecture to increase its segmentation performance \nwhile maintaining its computational efficiency.\nAge-related macular degeneration (AMD) is a disease of the human visual system that causes a progressive loss \nof central vision, leading to total blindness in its late stages. It is one of the major causes of severe vision loss in \ndeveloped countries and mainly occurs in the population above 50 years of  age1. AMD is a complex disease influ-\nenced by genetic, behavioral, environmental factors and their  interactions2. In addition to a variety of influencing \nfactors, AMD may have different  manifestations3. One type, denoted as neovascular AMD, is characterized by \nabnormal vessel growth in the macular region, resulting in fluid leakages across various areas within, or under-\nneath the  retina4. Early stages of AMD show the presence of drusen and abnormalities in the retinal pigment \nepithelium (RPE). Later stages can be further subdivided into neovascular AMD and dry  AMD5. Neovascular \nAMD (nAMD) is characterized by the presence of various lesions that occur in addition to the signs of the early \nstages. Primarily, these include intra-retinal fluid (IRF) and sub-retinal fluid (SRF), as well as the detachment of \nthe RPE from the Bruch’s membrane (BM), called pigment epithelium detachment (PED)5. In 2022, the interna-\ntional AMD research group published a consensus AMD  nomenclature3. In that work, several AMD biomarkers \nin optical coherence tomography images are outlined, including different kinds of fluids (intraretinal, subretinal, \nsub-RPE), different kinds of pigment epithelium detachment (serous, fibrovascular, drusenoid), drusen, sub-\nretinal drusenoid deposits (pseudo-drusen), types of retinal atrophies (complete or incomplete, with or without \nOPEN\n1NOVA Information Management School (NOVA IMS), Universidade Nova de Lisboa, 1070-312 Lisbon, \nPortugal. 2Department of Ophthalmology, St. Franziskus Hospital, 48145 Muenster, Germany. 3School of \nEconomics and Business, University of Ljubljana, Ljubljana, Slovenia. *email: mcastelli@novaims.unl.pt\n2\nVol:.(1234567890)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nRPE-defect), SHRM (subretinal hyperreflective material), HRF (hyperreflective foci), fibrosis, RIP (rip of the \nretinal pigment epithelium). It turned out that for our investigated prognostic targets, IRF , SRF , and PED are \nthe biomarkers with the highest relevance. For this reason, in this paper, we investigate the segmentation per -\nformance of our proposed approach considering the three features (IRF , SRF , and PED) with the highest impact \non prognostic targets. IRF , SRF , and PED are also considered as the most relevant biomarkers for managing the \ntreatment of  nAMD6. The main type of treatment is the so-called antivascular endothelial growth factor (anti-\nVEGF), which is injected into the vitreous. Progress and response of the treatment are monitored by an imaging \ntechnique called optical coherence tomography (OCT), which allows for the visualization and quantification of \nbiomarkers in the macula (= central retinal region)1,4,7. OCT is a non-invasive imaging technique that produces \ncross-sectional images of the  retina7 and is state-of-the-art in the diagnosis and monitoring of  nAMD6. A light \nbeam traverses the frontal plane centered around the point of interest, usually the fovea centralis (foveola) or \nthe papillary. At every position, it generates a depth profile, called amplitude scan (A-scan), by analyzing the \nspectrum of the back-reflection of the  beam8. There exist various scan patterns (e.g. 3d-scan, radial scan, and \nraster scan) that specify the movement pattern of the light beam across the frontal  plane7. In this work, we use the \n3d-scan pattern, which gives the most holistic view of the retina and we focus on the area around the foveola as \nthe point of interest. To initiate, continue, stop, or change the treatment of nAMD, the identification of presence \nand changes concerning IRF , SRF , and PED is  crucial7. Automated systems to segment and quantify those lesions \ncan be extremely useful to support clinical decision-making for an early diagnosis and a quantifiable treatment \nof nAMD. Ultimately, this can lead up to a patient individual predictions of treatment intensity.\nHowever, the huge variations in the location, size, and texture of those lesions, as well as the presence of \nspeckle noise in the images, make accurate segmentation a challenging task, which thus remains an active field \nof research. In OCT images, speckle is both the source of noise and the carrier of information. Arguably, without \nspeckle, an OCT image visualizing dense tissue would be empty. However, speckle noise poses a considerable \nchallenge to the segmentation of an OCT image, as it results in reduced contrast and unclear boundaries within \nthe  image9. Recent years have witnessed the implementation and adoption of deep  learning10 (DL)-based systems \nin the domain of retinal lesion segmentation in OCT images. In particular, DL-based  methods11–13 repeatedly \noutperformed conventional methods. They are unanimously based on convolutional neural networks (CNNs), \ndeep architecture specifically designed for feature extraction in grid-like data, including image data. Those \nmethods include multi-stage approaches consisting of a pre-processing stage followed by the actual segmenta-\ntion network and an optional post-processing stage. In the pre-processing stage, the localization prior to the \nregion of interest (ROI) is passed to the network by limiting the image boundaries to the retinal region. The \napplication of post-processing helps to reject falsely segmented lesions by training an additional classifier or to \ncombine lesion instances falsely interrupted by the background class. However, with the design of increasingly \nadvanced  architectures14–16, recent single-stage networks can learn the information induced through pre- or post-\nprocessing by themselves. This comes with the advantage of reduced computational complexity and, consequently, \nmaking these methods more clinically  applicable17. Despite the advances provided by DL, there is still a need \nfor more accurate automatic segmentation tools. To answer this call, our objective is to improve CNN-based \nmethods’ performance in modeling long-range contextual information and reduce speckle noise to cope with \ndomain-specific challenges. To achieve this objective, we rely on Vision Transformers (ViTs)18. ViTs have a larger \nreceptive field and perform learned feature aggregation. Thus, they have a stronger localization ability and better \npropagation of contextual and semantic information through the network compared to CNN-based  networks19. \nThese advantages come at the cost of a higher demand for training data and higher computational costs. For this \nreason, amongst many recent publications, we rely on Swin-UNETR20 which combines all the advantages of ViTs, \nwhile compensating for their weaknesses. It uses a Swin  Transformer21 as the backbone architecture to reduce \ncomputational complexity and combines it with a CNN-based decoder to reduce the hunger for training data.\nIn this work, we evaluate ViTs on the segmentation of retinal lesions in OCTs.\nThis work belongs to a recent research strand in which transformer-based  architectures22 were considered for \nthe analysis of OCT images. However, when compared to the existing contributions, several differences emerged. \nPlayout and  coauthors23 thoroughly investigated the performance of several transformers and compared it to \ntraditional CNN-based models for retinal disease classification. Experimental results showed that Vision Trans-\nformers achieve comparable performance to CNNs in retinal disease classification and scale better with larger \ntraining sets than CNNs. Different from our study, Playout and coauthors did not consider segmentation tasks. \nHowever, the idea of adding adjustable stride and focused attention looks interesting, as it addresses the issue of \nefficiently using finer-grained image patches to increase performance. The shifted window mechanism considered \nin our approach addresses the same issue but looks more suited to be integrated into a segmentation network \ndue to its cascading structure. Another recent contribution was proposed by Wang et al. 24, who developed an \narchitecture for the automatic segmentation of retinal edema lesions in OCT images. Thus, the authors focused \non a different disease than the AMD we examined. Moreover, their manuscript differs in at least two aspects from \nour study: (1) concerning the encoding of long-range, multi-scale features, it used a CNN-based backbone and \nadded a transformer at the bottleneck. On the other hand, in our study, we continuously extract global informa-\ntion throughout the encoder levels relying on the transformer’s self-attention mechanism. (2) A CNN-based \nfeature extractor produces a semantic gap that is subsequently bridged by adaptively fusing multi-scale features, \nwhile our study assumes a weaker semantic gap thanks to a transformer-based feature extractor. Other relevant \nstudies are the ones of  Kihara25 and  Jiang26, where transformers have been considered for analyzing OCT images. \nKihara and coauthors focused on nonexudative macular neovascularization and proposed a transformer-based \nencoder-decoder architecture for the segmentation task. Similarly, Jiang and coauthors proposed a transformer-\nbased model to classify OCT images of AMD and Diabetic Macular Edema (DME). In our study, we rely on a \nhybrid Transformer-CNN that, unlike the pure transformer used by Kihara, is less training data demanding and \nthus does not require pretraining. In particular, we evaluate the use of a shifted window transformer backbone \n3\nVol.:(0123456789)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nwhich, contrary to the transformer backbones in the aforementioned  studies25,26, can efficiently process more \nfine-grained image patches through the use of local self-attention. This follows the concept exploited in the work \nof  Playout23, which increased the number of patches drawn from the image through convolutions with adjust -\nable strides to improve classification performance and used focused attention to compensate for the additional \nmemory requirements. Other contributions (like the work of Lee and  coauthors27) concerning the use of less \nrecent deep learning methods for AMD diagnosis and OCT image analysis have been proposed in recent years. \nHowever, considering that they are based on traditional CNNs architectures, we did not discuss their differ -\nences to our transformer-based approach. The key contributions of this study include C.1—the adaptation of \nSwin-UNETR to the purpose of automated segmentation of retinal lesions in nAMD SD-OCT scans; C.2—the \nquantitative and qualitative validation of the proposed method on unseen testing data and comparison with other \nstate-of-the-art methods; C.3—the investigation of the effect of lesion size on the segmentation performance.\nTechnical background\nRecent years have witnessed the development of new deep architectures enriching CNNs with attention \n mechanisms28–31. Mathematically, attention is the process of mapping a query and a set of key-value pairs to an \noutput. A compatibility function is used to calculate a set of weights based on the query and the corresponding \nkey. The weights are used to compute the output as the weighted sum of the  values22. Attention is commonly used \nin computer vision tasks to adaptively aggregate features that capture long-range contextual  information28 and \nto suppress irrelevant parts of features while highlighting the relevant ones for a given  task32. Attention can be \ndivided into trainable and non-trainable, and the former can be further divided into hard and soft-attention28. In \nthis work, we will focus on soft attention, as it is differentiable and thus can be trained in an end-to-end network \narchitecture. The learned parameters come from three individual linear transformations, where sets of weights \nare learned to map the input into key, value, and query,  respectively22. Soft attention can be further characterized \ndepending on the size of the neighborhood (local or global)31, the type of compatibility function used to compute \nthe weights (additive or multiplicative)31, and the input source (self, encoder-decoder)22.\nThe transformer  architecture22 is the first model relying entirely on self-attention to compute representations \nof its input and output without using recurrence or convolutions. The original transformer architecture follows \nan encoder-decoder structure. In more detail, the encoder maps an input sequence to a sequence of continuous \nrepresentations, which is then fed into a decoder. The decoder receives the output of the encoder together with the \ndecoder output at the previous time step and generates an output sequence. This architecture was first developed \nfor addressing natural language processing  tasks33. Subsequently,  Dosovitskiy34 proposed the first transformer \nfor vision tasks, dubbed ViT. ViT was obtained by adapting the original transformer architecture to move from \ntext processing to image processing. First, to simulate a sequence of words as input, an image of size H × W × C \nis reshaped into a sequence of flattened, non-overlapping 2d patches of size N × (P × P × C), where P is the pixel \nsize of each patch, C is the number of input channels, and N the number of patches. Second, the fixed positional \nencoding was replaced with a learnable 1d  encoding35.\nThe application of attention mechanisms in image segmentation improves the representational performance \nof the network. In particular, it results in a richer feature space, capturing long-range dependencies. The network \nlearns to focus on relevant spatial regions (the “where”) or channels (the “what”) within a certain  context31. \nContrary to encoder-decoder attention mechanisms, self-attention is based solely on the given input feature \nmaps and aggregates contextual information across the different dimensions of this input.\nFrom ViT to Swin transformer. The original ViT heavily relies on global self-attention for feature extrac-\ntion with very little image-specific bias induced a priori. To improve its performance, Liu and coauthors pro-\nposed Swin  Transformer21. It is designed as a general-purpose backbone for dense prediction tasks like object \ndetection and image segmentation. The main observation is that for dense prediction tasks, translational invari-\nance and the ability to process finer-grained patches are more important than instant global attention at every \nlayer. Compared to the original ViT architecture, the Swin Transformer varies in three main aspects, namely the \napplication of local self-attention, hierarchical feature maps, and relative position bias. In more detail, in local \nself-attention, heads (i.e., attention modules) of each layer only attend to a restricted non-overlapping group of \npatches, the attention windows. This reduces the computational complexity from quadratic to linear to the image \nsize over global self-attention. Consequently, it allows for processing more fine-grained patches at comparable \ncomputational costs, which is important for dense prediction tasks like image segmentation. Hierarchical feature \nmaps are used to create multi-scale outputs during feature extraction. After a user-defined number of consecu-\ntive transformer blocks with a constant patch resolution, the patch merging operation is performed at the start \nof a new layer. In patch merging, features related to a group of neighboring patches are concatenated, and a \nlinear layer allows obtaining the vectorized embedding. This way, feature resolution is gradually decreased with \ndeeper encoder levels, allowing for an in-place replacement of CNN-based backbones like ResNet and, thus, a \nsmoother integration with decoders in U-Net-like architectures. Finally, instead of absolute positional embed-\ndings, Swin Transformers learn a relative position bias inside the self-attention mechanism, achieving significant \nperformance improvements. Here, an additional set of parameters is added to the computation of the attention \ncoefficients, which learns the relative distance of each patch to every other patch.\nMaterials and methods\nDataset. In this work, we use a private dataset developed in collaboration with Franziskus Eye-Center in \nMuenster, Germany. All procedures performed in this study involving human participants were in accordance \nwith the ethical standards of the institutional and/or national research committee and with the 1964 Helsinki \nDeclaration and its later amendments or comparable ethical standards. Approval for the study was obtained \n4\nVol:.(1234567890)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nfrom the University of Muenster Ethical Committee. Being a retrospective study, the Muenster Ethical Com-\nmittee waived the need for informed consent. A cohort of nAMD patients treated with anti-VEGF therapy was \nselected. Out of those patients, we only consider OCT volumes that do not date back more than 18 months from \nthe start of the treatment. Furthermore, we subsampled the OCTs to obtain an even distribution across these 18 \nmonths. All the images were acquired using a Spectralis SD-OCT from Heidelberg Engineering. Each volume \ncontains 49 B-scans (i.e., cross-sectional images obtained from the combination of multiple A-scans) with a spa-\ntial resolution of 512 × 496. The training labels were generated through a manual annotation process performed \nby a group of professional ophthalmologists. We have defined a set of 17 AMD biomarkers and three retinal slab \nlimits. We randomly selected 3842 B-scans (from 1400 randomly selected SD-OCTs) within the central 3 mm \naround the fovea. Four junior and one senior annotator annotated the B-scans. All graders have clinical ophthal-\nmological experience and have undergone certification, with the annotation process documented in a detailed \nmanual. The annotation was performed in a reading center environment using the COCO  annotator36. Their \nwork was supervised by a senior annotator who also performed a quality assessment on a certification dataset \ncontaining 50 slices that were selected to capture a broad spectrum of cases. To make the time-consuming \nprocess of manual annotation as efficient as possible, we focus on slices that cut through the macula. This is the \nregion where IRF , SRF , and PED have the highest clinical relevance, and it should roughly be positioned around \nthe center slice. Therefore, we randomly select five B-scans (slices) of every volume, using a gaussian probability \ndistribution centered around the middle slice. In terms of preprocessing, to define the ROI, we used the internal \nlimiting membrane (ILM) as the upper limit, which was segmented by the OCT machine and extracted from \nthe raw OCT file. In particular, the area in the depth dimension of the retinal region, where lesions can occur, \nis defined between the ILM and the  BM4,7. The BM was pre-segmented using polynomial regression. Inspired \nby the work of  Russakoff37, we considered a fixed offset of 390µ m below the BM as the lower limit of the ROI to \ncapture choroidal information of a fixed area below the BM. The pixel values outside the ROI were replaced with \nzero. To improve the image quality, we applied contrast-limited adaptive histogram equalization (CLAHE) 38. \nHere, an image is equally divided by a horizontal and a vertical factor ( grid_size ) into non-overlapping regions. \nFor each region, a histogram is calculated and redistributed to not exceed a clip_limit value. Based on the result-\ning contrast-limited histograms, a grayscale mapping is  determined39. Finally, each pixel of the image is mapped \nto the grayscale mappings of the four nearest regions. In this work, we used the implementation by OpenCV 4 \n(https:// opencv. org/), with a grid_size of 8 × 8 and a clip_limit of 1.\nFurthermore, we applied a horizontal and vertical registration of the images to reduce the variance of the \ntraining data. We took the left and rightmost points of the RPE layer and used shearing so that both RPE end -\npoints lay on the same horizontal axis. We subsequently translated the image in the vertical direction, so that the \nRPE endpoints are positioned at 65% from the top of the image. Concerning training, validation, and testing, \nwe only consider B-scans that contain at least one type of lesion (for a total of 3842 B-scans). Table  1 reports \nthe distribution over the classes. Furthermore, each B-scan is resized to a dimension of 224 × 224 pixel. To add \nsome regularization, we augment the training data by rotating the image by ± 20 degrees at a probability of 50%.\nEvaluation metrics. We consider the problem at hand as a multi-class, single-label task. The four indi-\nvidual classes are background, IRF , SRF , and PED. However, when generating the labels, the class masks are over-\nlayed, resulting in a single label passed to the network, with pixel values chosen from the set of 0 = BG, 1 = IRF , \n2 = SRF , and 3 = PED respectively. Thus, a co-occurrence of multiple classes at a particular pixel is excluded. \nWe argue that the information loss due to overlaying the class masks is very limited since different types of \nlesions generally do not occur in the same position. In the selection and calculation of the metrics, we follow the \n guidelines40 of evaluation metrics for medical image segmentation: We use the Dice  score41 as the main metric \nfor performance evaluation. Additionally, we report sensitivity and specificity for method comparability, both \nindividually per class and as a weighted aggregation. We use the image-wise averaging to aggregate the metrics \nto allow for an even contribution of all images and to account for the class imbalance. Here, the metrics are cal-\nculated for each image separately, and per-class metrics are averaged over the dataset. To obtain a final metric, \nwe average the scores over the classes, weighted by the class distribution of the dataset. When computing the \nscores, we ignore images where either reference or prediction masks for a class are empty to avoid the zero divi-\nsion  issue42. Furthermore, we will provide boxplots depicting the distribution of the metrics over the dataset, as \nwell as sample visualizations of references and predictions for the considered methods.\nMetric comparison and interpretation.  Low-density predictions show numerous tiny holes inside the seg-\nmented area, while the area of the reference is solid. If density and the general shape and location are more \nimportant than the precise contours of an object, overlap-based metrics, like Dice, are recommended, as, unlike \ndistance-based metrics, they penalize low  density41. In this work, the segmentation task is primarily aimed at \na precise quantification of the segments’ volume. Consequently, we will use the Dice score as our main metric.\nTable 1.  Number of B-scans per class. The relative values are based on the total number of B-scans (3842).\nClass Absolute Relative\nPED 3203 0.84\nIRF 1783 0.47\nSRF 1416 0.37\n5\nVol.:(0123456789)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nThe proposed framework. In this work, we use as segmentation network Swin UNEt TRansformers \n(Swin-UNETR)20, which is specifically designed for the task of medical image segmentation. It is an encoder-\ndecoder-based Transformer-CNN hybrid, with a transformer-based encoder, skip connections, and a CNN-\nbased decoder. Though the original  paper20 described a 3d segmentation problem, we adapt the following \ndescription to the 2d task to align with the problem at hand. The encoder uses a Swin Transformer backbone. \nIt consists of five decoder stages and four encoder stages with a total of 12 layers ([2, 2, 6, 2] per stage). At each \nstage, patch resolution is increased, starting at 2  × 2 pixels per patch and reaching 32  × 32 pixels. The model’s \nfeature (i.e., embedding) size is set to 24 and is doubled at every encoder stage. The remaining network repeat-\nedly makes use of a residual block composed of two 3 × 3 convolutions followed by an instance normalization. \nAt each encoder stage, the output is reshaped into the spatial dimensions and run through a residual block to be \nconcatenated with the output of the previous decoder stage. The result of the concatenation is then run through \nanother residual block, followed by a transposed convolution layer, that increases the resolution of the feature \nmaps by factor 2, to conclude the current decoder stage. The resulting network has a decoder depth of 5 and a \ntotal of 6.3 Mio. parameters.\nConsidered CNNs. We compared the performance achieved by the considered Swin-UNETR against state-\nof-the-art CNNs specifically developed for medical image segmentation.\nU‑Net. The U-Net43 is a commonly used, CNN-based, encoder-decoder architecture designed for medical \nimage segmentation. It consists of a contracting path (encoder) and a symmetric expansive path (decoder). The \nencoder consists of cascaded convolutional blocks, ReLU activations, and a max-pooling operation. It generates \nlow-level but high-resolution features in the early layers and increasingly higher-level semantic features in the \ndeeper layers. The decoder consists of transposed convolutions, an un-pooling layer, and ReLU activations sym-\nmetric to the ones in the encoder. The decoder is built to aggregate multi-level features and capture multi-scale \ncontext information. It continuously increases the spatial resolution of the feature maps to regain the original \ninput’s resolution. Furthermore, skip connections are used to concatenate features from the decoder with the \nones of the corresponding encoder level. Finally, the last layer is a 1 × 1 convolution, which maps the features of \nthe previous layer to the desired number of  classes30. Despite its success, the original U-Net fails to fully recover \nthe spatial information lost during down-sampling and flawlessly to bridge the semantic gap between low and \nhigh-level features. Thus, many adaptations like spatial pyramid pooling (SPP), atrous spatial pyramid pooling \n(ASPP), or input pyramids have been proposed to improve segmentation  accuracy44.\nU‑Net3+. U-Net3+45 builds on top of the original U-Net and the U-Net++46 to propose a position and bound-\nary-aware network for medical image segmentation. The primary change is the redesigned, full-scale, skip con-\nnection mechanism to improve multi-scale feature aggregation and the deep supervision that learns hierarchi-\ncal representations. Low-level, high-resolution features are important for a precise segmentation of boundary \nregions, while high-level features embody positional information needed to locate the lesions. Multi-scale fea-\nture extraction and aggregation is a widespread solution to merge these features. Full-scale skip connections \nconsider features of all scales at each particular decoder stage, to improve multi-scale feature fusion. At one \nparticular decoder, like in the vanilla U-Net, the feature maps of the corresponding encoder are passed directly. \nAdditionally, feature maps of earlier encoder stages are passed through max pooling operations of increasing \nscale to match the spatial resolution of the current stage. Likewise, coarser feature maps are taken from the pre-\nvious decoder layers and are up-sampled through bilinear interpolation. To unify the depth across the feature \nmaps and reduce redundant information, feature maps of every scale are run through individual 3 × 3 convolu-\ntions with 64 kernels before being concatenated. Finally, the concatenated feature maps are run through a block \nof 3 × 3 convolutions with 320 kernels, batch normalization, and ReLu, to obtain the output of that decoder \nstage. This way, feature maps at every decoder stage have a depth of 320 and contain information on every scale.\nExperimental settings\nAll experiments were performed on an Ubuntu 20.04 server, using an AMD Ryzen 9 3900X 12-Core CPU with \n64GB system memory and a single NVIDIA Titan RTX 1330 24GB GPU. The software was implemented in \npython 3.7 using PyTorch 1.5.0 and is based on the work of  Cao47. It has been extensively modified to address \nthe problem at hand.\nModel selection. In model selection, we perform hyper-parameter tuning (HPT) to increase model per -\nformance and reduce training time. The objective is to adapt to the model to achieve the ideal balance between \nover- and underfitting. To save training time, we will use a single training/validation split for model selection. \nWe monitor loss and Dice scores. We will do that for our method and for all the other considered techniques \ntaken into account. The hyper-parameters can generally be divided into two groups, the model design, and the \noptimizer hyper-parameters.\nOn the one hand, the model design hyper-parameters define the model’s capacity in terms of depth (number \nof recurrent blocks or layers) and width (number of filters), the loss function, dropout rate, and the optimizer. \nConcerning the model capacity, we will keep the depth and width equal to the default  values20 while maintain-\ning an even model capacity across the architectures throughout the experimental phase. The dropout rate is the \nprimary source of regularization on the model design side. We keep this value to the default and focus on the \noptimizer hyper-parameter to adjust regularization strength. Concerning the loss function, we use a combination \n6\nVol:.(1234567890)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nof Cross-Entropy and  Dice48 during model selection and comparison, as commonly seen in medical image \nsegmentation, with /afii98381 = /afii98382 = 0.5.\nWe choose stochastic gradient descent (SGD)49 as the optimizer. In an empirical comparison of common types of \noptimizers, Choi and  coauthors50 found similar performance between SGD and other optimizers (after learning \nrate tuning) and a less severe increase of training time with increased batch size.\nOptimizer’s hyper-parameters primarily consist of batch size, learning rate (LR), momentum, weight decay \n(WD), and the number of training epochs. Especially the first four parameters have a significant impact on \ngeneralizability and are strongly  interconnected 51. In this work, we consider a maximum number of epochs \nequal to 50 and an early stopping criterion to stop the training phase if the loss on the validation set does not \nimprove for six consecutive epochs. Focusing on the optimizer’s hyper-parameters, to find suitable values within \nthe hyper-parameter space, we perform a 3-stage manual search individually for each method. We adopt the \ntuning strategy described  in51 and choose a batch size as large as possible, only restricted by the GPU. In stage 1, \nwe identify the best learning rate range for the cyclic LR-scheduler while keeping the values for momentum and \nWD constant. We try different LR ranges within the boundaries of [5e−5, 1.5]  where each LRbase is a tenth of the \ncorresponding LRmax . In stage 2, based on the best values for the learning rate range for the respective method, \nwe evaluate the best value for the momentum range. Finally, stage 3 builds on the best values for the learning rate \nand momentum ranges. The values of the hyper-parameters resulting from the HPT phase are reported in Table 2.\nModel comparison. In model comparison, we compare different types of architectures, all of which were \npreviously fine-tuned in the HPT phase, on a set of test images (that were not considered during the model selec-\ntion stage). To calculate the test metrics, we take the model with the smallest validation loss for each model and \nfold. To obtain statistically significant results, we compare the test metrics over a 5-fold cross-validation.\nloss= /afii98381 ·lossCE + /afii98382 ·lossDice\nTable 2.  Manual search results of optimizer hyper-parameter tuning, performed in 3 consecutive stages. In \nstage 1 (Tab. a), we find the best values for the learning rate range, in stage 2 (Tab. b) for the momentum range \nand in stage 3 (Tab. c) for weight decay. The tables show the weighted mean Dice score over the classes of \nthe training epoch calculated on the validation set with the lowest validation loss for the considered hyper-\nparameter configurations. The best configuration for each method is displayed in bold.\nLRbase LRmax U-Net U-Net3+ Swin-UNETR-24 Swin-UNETR-48\nLR range: (a) Mean Dice scores for different learning rate ranges and for the four considered \nmethods. LRbase and LRmax represent the lower and upper boundary of the learning rate range \nrespectively. In the case where there is not LRmax provided, we use a constant LR. In any other \ncase, we use a cyclic LR scheduler. For all methods, the momentum is set to 0.97 and weight decay \nis set to 1e−4. Concerning Swin-UNETR-24, we selected the configuration with LRmax = 5e−2 \ndespite the slightly worse mean Dice score over the next bigger LR range by 1%. The main reason \nis the strongly increased runtime by a factor of 2 at a small improvement of the validation loss by \n0.001 points\n5e−5 5e−4 0.389 – 0.370 0.393\n5e−4 5e−3 0.438 0.478 0.449 0.452\n5e −3.5 – 0.421 0.444 0.423 0.468\n5e−3 5e−2 0.432 0.537 0.454 0.485\n5e−2 5e−1 0.505 0.362 0.463 0.459\n5e−1 1.5 0.359 0.169 0.403 0.402\n M base Mmax U-Net U-Net3+ Swin-UNETR-24 Swin-UNETR-48\n(b) Momentum range: Mean Dice scores for different momentum ranges and for the four con-\nsidered methods. Mbase and Mmax represent the lower and upper boundary of the momentum \nrange respectively. For the LRbase and LRmax, we consider values that yielded the best results in the \nprevious stage (displayed in bold). For all methods, the weight decay is set to 1e−4\n0.80 0.90 0.504 0.536 0.476 0.489\n0.80 0.95 0.480 0.539 0.496 0.461\n0.85 0.97 0.505 0.537 0.454 0.485\n0.85 0.99 0.513 0.524 0.460 0.497\n Weight decay U-Net U-Net3+ Swin-UNETR-24 Swin-UNETR-48\n(c) Weight decay: Mean Dice scores for different weight decay coefficients and for the four con-\nsidered methods. For all methods, we consider the values for the learning rate and momentum \nranges that yielded the best results in the previous stages\n1e−6 0.474 0.520 0.448 0.481\n1e−5 0.509 0.534 0.451 0.499\n1e−4 0.513 0.539 0.496 0.497\n1e−3 0.360 0.531 0.477 0.470\n1e−2 0 0.463 0.435 0.356\n7\nVol.:(0123456789)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nDataset splitting. We split the dataset at the patient level, and B-scans of a particular patient can not simul-\ntaneously occur in the training and test datasets. Two B-scans of a particular patient taken at different points in \ntime but at the same position of the retina are expected to show a high structural correlation. As a consequence, \nallowing them to occur in both train and test sets would have resulted in information leakage. Furthermore, we \nstratified the splits according to groups of classes. Considering the three classes IRF , SRF , and PED, this results \nin seven groups: IRF , PED, SRF , IRF+PED, IRF+SRF , PED+SRF , IRF+PED+SRF . The stratified split guarantees a \nsimilar distribution of those groups to generate homogeneity across the datasets and reduce selection bias. For \nboth model selection and model comparison, we relied on nested k-fold cross-validation (CV), as proposed by \n Raschka52. In this work, we apply a slight variation to this approach. We maintain the outer 5-fold CV but we \nreplaced the inner CV with a single split. In this way, we reduce the overall training time, at the cost of losing the \npossibility of training on every data point at least once.\nResults\nModel selection. In the model selection, we perform hyper-parameter tuning for each of the methods \nunder analysis. To align the models’ capacity in terms of their total number of parameters, we consider two dif-\nferent settings for the Swin-UNETR. First, a version with the default settings as proposed by the original authors, \nwith a feature size of 24, dubbed Swin-UNETR-24. With only 7 Mio. parameters, this version has a far smaller \ncapacity than U-Net (24 Mio.) and U-Net3+ (27 Mio.). We propose a second version of the Swin-UNETR, \ndubbed Swin-UNETR-48, with a feature size set to 48, which increases the model’s capacity to 27 Mio. param-\neters, making it comparable to the CNN-based methods.\nThe results are shown in Table  2. In general, the choice of LR is more relevant for U-Net3+ than for Swin-\nUNETR. In return, Swin-UNETR is much more sensitive to the choice of momentum and weight decay. In \nterms of the LR scheduler, a constant LR of 5e −3.5 achieves lower Dice scores for all but Swin-UNETR-48 in \ncomparison to the cyclic LR that oscillates around the same LR value of 5e −3.5 . As far as the LR range (a), the \nU-Net performed best at a range of 5e−2 to 5e−1 and the remaining methods at a step to the power of ten smaller. \nIn terms of momentum, we choose a rather small range of [0.8, 0.95] for U-Net3+ and Swin-UNETR-24 and \na high range of [0.85, 0.99] for U-Net and Swin-UNETR-48. For the weight decay, we choose a slightly lower \nvalue of 1e−5 for Swin-UNETR-48 while the remaining three methods all work best at a medium value of 1e−4.\nModel comparison. Quantitative. Overall performance. The results of the model comparison are summa-\nrized in Table 3. Across all classes, U-Net3+ has the highest Dice scores. In terms of mean Dice, it outperforms \nthe other methods by 0.05, while the remaining performers lie within a range of 0.01. Considering the different \nclasses, PED clearly shows the best overall Dice scores followed by SRF when segmented by CNN-based meth-\nods or IRF in the case of Swin-Transformer. The latter, however, only shows a weak variance between IRF and \nSRF of less than 0.01. The two versions of the Swin-UNETR both achieve similar results.\nLooking at the value distribution of the Dice scores depicted in Fig.  1, the general tendencies described in \nthe previous paragraph can be confirmed. However, we can observe a significant variance in the scores across \nthe cases of the dataset. This is especially true for the Dice score of IRF and SRF .\nOver‑ and under‑segmentation: sensitivity and specificity. To better understand the methods’ tendency to over- \nor under-segment a class, we report sensitivity and specificity in Table 4. Sensitivity measures the rate of correctly \npredicted foreground classes, while specificity measures the same quantity for the background class. Reduced \nsensitivity is due to an increase in false negatives (FNs), meaning the algorithm fails to predict the specific class \nand suggests the pixel belongs to the background or to an incorrect foreground class. Analog, a reduced specific-\nity is caused by an increase in false positives (FPs). In other words, pixels that belong to the background or an \nalternate class were falsely predicted to belong to one of the specific foreground classes. Consequently, poor sen-\nsitivity represents under-segmentation and poor specificity over-segmentation. Overall, all methods show a clear \ntendency towards a higher specificity than sensitivity. However, there exist some differences across the classes as \nwell as the methods. The sensitivity values across classes and methods vastly align with the observations reported \nTable 3.  Method comparison results. Mean and (standard deviation (std)) of the Dice scores for the four \nconsidered methods calculated on the test set over a 5-fold cross-validation (CV). Results reported per class \n(columns 2-4) and as a weighted average (column 5 (MEAN)). The best performer is displayed in bold, \nfollowed by the second best in italic.\nMethod IRF PED SRF MEAN\n U-Net 0.354\n(0.023)\n0.515\n(0.027)\n0.391\n(0.042)\n0.443\n(0.029)\n U-Net3+ 0.423\n(0.028)\n0.582\n(0.023)\n0.444\n(0.030)\n0.508\n(0.026)\nSwin-\nUNETR-\n24\n0.379\n(0.036)\n0.537\n(0.013)\n0.372\n(0.028)\n0.457\n(0.023)\nSwin-\nUNETR-\n48\n0.365\n(0.031)\n0.538\n(0.014)\n0.359\n(0.043)\n0.451\n(0.025)\n8\nVol:.(1234567890)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nfor Dice. PED achieves the highest values, followed by SRF and IRF across all methods. U-Net3+ shows the high-\nest values across all classes, followed by the Swin-UNETRs, while U-Net clearly shows the poorest performance. \nAcross all methods, PED is the only class achieving a specificity of lower than 0.999. U-Net3+ has the highest \nvalue of 0.995, while the values of the Swin-UNETRs are the lowest, with a difference of 0.002.\nImpact of lesion size. Figure 2 shows the segmentation performance of the evaluated methods regarding dif-\nferent lesion sizes. The sizes were approximated by the number of pixels per image and class. Generally, per -\nformance increases with the lesion size while the variance decreases. Across all lesion sizes, U-Net performs \nnoticeably weaker than the other methods, and U-Net3+ is superior, especially for the PED class. The advantage \nof U-Net3+ over both Swin-UNETR methods becomes particularly clear for the larger lesion sizes. Within the \ntwo Swin-UNETR methods, there exist only slight differences in mean performance. However, a reduced per -\nformance variance can be observed for the larger model.\nMethods computational efficiency. Figure 3 compares the methods’ performance against their computational \nefficiency measured by four different metrics. The leftmost two metrics, namely the total number of trainable \nparameters ( n_params ) and the number of multiply-accumulate operations (MACs), are both characteristics of \na particular architecture, while the rightmost two metrics are measured during the actual training.\nIn terms of n_params , Swin-UNETR-24 uses far fewer parameters than the remaining three methods. While \nU-Net and Swin-UNETR-48 have a lower Dice score, U-Net3+ outperforms the remaining methods. As a result, \nSwin-UNETR-24 and U-Net3+ dominate the remaining two methods in terms of n_params.\nMACs represent the linear transformation operations of a neural network, including matrix multiplications \nand convolutions. Unlike non-linear functions, such as rectified linear units (ReLU) and pooling operations, \ntheir computational costs grow quadratically with the network size, making them the main hardware bottle-\nneck.53 Here, the value for U-Net3+ is about 25 times higher than the remaining competitors while also showing \nFigure 1.  Boxplots of model comparison results per class for Dice score across the entire dataset. The boxes \nshow the middle 50% of the values. Whiskers reach at most 1.5 times the interquartile range (= box height) on \ntop of either end of the box. Points outside the whiskers are considered outliers and are not shown here. The \nhorizontal bar within the box shows the median. The mean is depicted with a yellow diamond.\nTable 4.  Method comparison results. Mean and (std) of the sensitivity and specificity calculated on the test set \nover a 5-fold CV per class and as a weighted average.\nMethod\nSensitivity Specificity\nIRF PED SRF MEAN IRF PED SRF MEAN\n U-Net 0.389\n(0.078)\n0.553\n(0.080)\n0.534\n(0.093)\n0.504\n(0.082)\n0.999\n(0.001)\n0.994\n(0.002)\n0.999\n(0.001)\n0.997\n(0.001)\n U-Net3+ 0.526\n(0.047)\n0.640\n(0.034)\n0.600\n(0.014)\n0.600\n(0.033)\n0.999\n(0.000)\n0.995\n(0.001)\n0.999\n(0.000)\n0.997\n(0.001)\nSwin-\nUNETR-\n24\n0.497\n(0.033)\n0.616\n(0.061)\n0.589\n(0.035)\n0.578\n(0.048)\n0.999\n(0.000)\n0.993\n(0.003)\n0.999\n(0.001)\n0.996\n(0.002)\nSwin-\nUNETR-\n48\n0.504\n(0.039)\n0.628\n(0.028)\n0.587\n(0.045)\n0.585\n(0.035)\n0.999\n(0.000)\n0.993\n(0.002)\n0.999\n(0.001)\n0.996\n(0.001)\n9\nVol.:(0123456789)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nan increased Dice score. This hugely increased value can be explained by the expensive skip connections of \nU-Net3+ where at each decoder level one convolution per encoder level plus one extra convolution are added \nto the network, compared to simple concatenate operations in the U-Net. Amongst the remaining methods, \nSwin-UNETR-24 has the highest Dice score, again leading to the domination of U-Net3+ and Swin-UNETR-48.\nFor memory usage, please recall that we used an equal batch size of 16 across the methods. Consequently, the \nobserved differences are fully due to the architecture design and implementation. The dotted line connects the \nU-Net as the method with the lowest to U-Net3+ with the highest memory usage. Swin-UNETR-24 lies slightly \nabove and Swin-UNETR-48 slightly below that line. We argue that this result corresponds to the slightly more \nefficient memory usage of Swin-UNETR-24 compared to the remaining competitors.\nLast, in terms of training time, three methods roughly lie on the same dotted line. Only the Swin-UNETR-48 \nlies slightly below that line, reflecting a negligible disadvantage in training time efficiency.\nQualitative. Figure 4 shows typical example cases of misclassifications and differences between the methods. \nFor instance, we can observe that U-Net confuses IRF (light blue) either with the background (row 6) or with \nSRF (dark red) (row 4), while U-Net3+ performs best in all cases. Concerning the PED (yellow) class, we can \nFigure 2.  Model comparison results per class and lesion size. The boxplots show the value distribution of \nthe entire dataset for the Dice scores. Lesion sizes are discretized into quartiles from small (q1) to large (q4). \nThe boxes show the middle 50% of the values. Whiskers reach at most 1.5 times the interquartile range (= box \nheight) on top of either end of the box. Points outside the whiskers are considered outliers and are not shown \nhere. The horizontal bar within the box shows the median. The mean is depicted with a yellow diamond.\nFigure 3.  Comparison of the methods’ computational efficiency across four different metrics. The mean cv5-\nDice score over the classes is plotted against the total number of trainable parameters (top-left), the number of \nmultiply-accumulate operations (MACs) (top-right), the measured GPU memory used during training (bottom-\nleft), and the mean training duration over the folds (bottom-right). The dotted line represents the relationship \nbetween the two methods with the lowest and highest Dice scores.\n10\nVol:.(1234567890)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nobserve misclassifications with the background for all methods but U-Net3+ (row 2). Furthermore, ViTs show \nsigns of over-segmentation of PED (rows 4 and 5). SRF gets confused with IRF by Swin-UNETR-48 (row 5). Row \n3 shows a case with the presence of all three classes but with a very small SRF area compared to IRF and PED. \nHere, the U-Net fails to separate the SRF from the IRF areas above, while the other methods perform equally \nwell in detecting the SRF area. However, Swin-UNETR-24 shows advantages for PED and IRF , whereas U-Net3+ \ntends to over-segment the IRF and under-segment the PED class. For the case shown in row 5, U-Net3+ per -\nforms well in detecting the SRF areas while U-Net performs slightly worse, failing to detect the entire area \nand partially mistaking SRF for IRF . Here, both ViTs perform drastically worse, with Swin-UNETR-24 hardly \nmanaging to distinguish the area from the background and Swin-UNETR-48 mistaking large parts with IRF . \nFurthermore, ViTs confuse SRF with PED (row 1), where the SRF area is uncharacteristically large compared to \nthe PED area. Finally, there exist cases with holes in the segmented areas of IRF (row 6), SRF (row 5), and PED \n(rows 2 and 4), where U-Net3+ is least affected.\nDiscussion\nModel comparison. Overall performance. In absolute performance, Swin-UNETR is inferior to the ad-\nvanced CNN-based architecture of the U-Net3+ in the segmentation of retinal lesions on our dataset. We argue \nthat the complex skip connections in U-Net3+ successfully compensate for the smaller initial receptive field of \nFigure 4.  Qualitative model comparison. The columns represent the original image, the reference (GT), and \nthe predictions of the four considered methods from left to right. Each row represents an example case that \nwas selected from the cases with the biggest variance of the mean Dice scores over the folds amongst the four \nmethods. The best and the worst method are highlighted in green and red respectively.\n11\nVol.:(0123456789)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nthe CNN encoder, thus resulting in higher segmentation performance. This is supported by the observed perfor-\nmance advantages of Swin-UNETR over the less advanced CNN-based architecture of the U-Net.\nHowever, it is worth mentioning that the version of Swin-UNETR used in this work is far less computationally \nexpensive than U-Net3+. This motivates further investigations into the network’s architecture to increase Swin-\nUNETR’s model capacity and the resulting impact on segmentation performance. In this work, we investigated \nthe increase in the feature size, which did not yield any relevant performance improvement. Possible further \noptions include the increase of the model depth, patch resolution, or the number of attention heads that can be \nexplored in future work.\nFurthermore, a known limitation of ViTs is their need for large amounts of training data. Even though this \ncaveat is already addressed by the hybrid structure of the Swin-UNETR, we suggest further investigations of \nthe effect of differently sized training data, data augmentation, or the use of pre-training on medical image data \nfrom other domains.\nClass‑wise performance. In terms of Dice score, all methods performed best in the PED class followed by either \nSRF (CNNs) or IRF (Swin-UNETRs). The differences between the classes are bigger for the Swin-UNETRs than \nfor U-Net and U-Net3+, with values for the std across the classes ranging from 0.084 for the U-Net and 0.086 \nfor the U-Net3+ to 0.093 and 0.010 for Swin-UNETR-24 and Swin-UNETR-48 respectively. In the related work, \nthe order between IRF , SRF , and PED varies for each method also when tested on the same dataset. This lets us \nassume that it depends not only on the dataset used to obtain the score but also on the segmentation method \nitself. The differences between the classes tend to be smaller than observed in this work. Contrary to the order, \nhowever, we can observe a dependency solely due to the characteristics of the dataset. The related works that \nused the publicly available  RETOUCH54 test dataset all report relatively small differences in Dice score between \nthe classes with a std of 0.020 to 0.025 55–58 while the ones using private datasets report larger values of 0.075 59 \nand 0.08760. Consequently, we assign the reason for the spread of the Dice scores between the classes primarily \nto the characteristic of the dataset. To achieve more general results, future work can repeat our experiments on \npublicly available datasets.\nFurthermore, we can observe a strong imbalance between sensitivity and specificity. While the sensitivity \nranges between 0.389 and 0.640 depending on the class and method, the specificity mostly reaches a value of \n0.999. We argue that the overall very high specificity and resulting rare occurrence of over-segmentation is a \nresponse to the highly unbalanced dataset with a strongly overrepresented background class with an average \narea coverage of nearly 97%. Most signs of over-segmentation can be observed in the PED class, which can be \nmeasured by a slightly reduced specificity. This can be explained by PED being the most represented foreground \nclass. On average, PED covers 2.66% of the image area while IRF and SRF only cover 0.95% and 1.23% respec-\ntively. Swin-UNETRs show the tendency of PED over-segmentation slightly stronger than U-Net3+. Examples \nof this behavior can be observed in Fig. 4 rows 4 and 5.\nImpact of lesion size. Across all methods, we observe an increase in the Dice score with the lesion size. This \nconfirms the findings of previous related  work59. The performance advantage of U-Net3+ becomes obvious with \nlarger lesion sizes. This is particularly true for the PED class, which tends to cover larger areas in the image than \nthe two remaining classes. It is somewhat counterintuitive to the theoretically high robustness of ViTs to varia-\ntions in lesion sizes and proves once more the power of U-Net3+’s advanced skip connections. We know, from \nrelated work, that multi-scale feature extraction methods like advanced skip connections can improve the detec-\ntion of lesions of different sizes. However, we expected the dynamically sized receptive field of ViTs’ attention \nheads to be even more potent. Possibly, the local attention mechanism of the Swin Transformer backbone used in \nSwin-UNETR hinders that advantage. We recommend future work to experiment with different ViT backbones \nthat use global instead of local attention heads. A further limitation of our approach is the proxy used to measure \nthe lesion size. We used a simple pixel count approach that is unaware of multiple instances of one class within a \nparticular image. As we reached the same overall results as related work, we do not expect fundamentally differ-\nent findings with an improved proxy. Still, future work can reproduce the experiments with more precise proxies \nthat are agnostic to class instances.\nFurther limitations. Limitations of this work are mainly related to the private dataset used here.\nIn this work, we consider every 2d image separately, without including their relationship in the 3-dimensional \nspace. This is due to the limited capacity of the reading center, where the manual labeling of the reference is only \nperformed on five, non-consecutive slices per volume. This makes embedding them into a 3d context impossible \nconsequently preventing the precise quantification of the lesion volume. Coarsely sampled OCT volumes can \neven miss certain lesion instances  entirely61. Also, related work shows possible improvements in segmentation \nperformance when adding 3d context, which we can not reproduce based on the dataset used in this  work62.\nFurthermore, our data is obtained from devices of a single vendor only. Related work has shown strong \ndependencies of segmentation performance across image data from different  vendors56–58,62. In future work, we \nwill consider image data from different vendors to compare the cross-device generalizability capabilities of ViTs \nto CNN-based methods.\nConclusion\nRegarding the key contributions of this work, we conclude the following. C.1—In adapting Swin-UNETR to the \ndomain of automatic retinal OCT lesion segmentation, this work contributed to a recent research strand that \nfocuses on the use of ViTs for the automated segmentation of retinal lesions in OCT images. In particular, in our \nstudy, we used a hybrid Transformer-CNN that, unlike the pure transformer used in recent  contributions25,26, \n12\nVol:.(1234567890)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nis less training data demanding and thus does not require pretraining. In this work, the adaptation of the Swin-\nUNETR is limited to experimenting with different feature sizes. We demonstrated that increasing feature size \ndoes not yield any performance increase. As the next steps, we suggest investigating the effect of the model depth, \npatch resolution, or the number of attention heads. C.2—Even though we were not able to demonstrate the \nsuperiority of ViTs over CNN-based approaches in terms of absolute performance, the demonstrated advantages \nin computational efficiency motivate future work to investigate changes in the model’s architecture for further \nperformance improvements. C.3—As far as analyzing the effect of lesion size on the segmentation performance, \nwe can confirm the overall findings of related work of increasing performance with the lesion size. Comparing \nthe segmentation performance of ViTs with CNN-based approaches we found that Swin-UNETR shows room \nfor improvement particularly with larger lesions.\nIn the clinical context of retinal lesion segmentation of OCT images, this work is a valuable contribution for \nimproving current DL-based segmentation algorithms. Possible applications include the support of practition-\ners with segmentation suggestions made by the algorithm and the use of the segmentation results as a source \nof input for subsequent prediction questions like the treatment intensity. At this point, a full replacement of the \npractitioner in identifying the lesions is not recommendable until a further reduction of the segmentation error \nis reached. Considering the shown imbalance between positive and negative prediction errors, it is up to the \nspecific field of application or to the individual practitioner even, to decide. For the decision-making support in \nthe clinic, on the one hand, we argue that the here observed imbalance towards the specificity is generally not \nfavorable. The system should be more balanced or even lean towards an over-segmentation to raise the practi -\ntioner’s attention towards a certain anomaly, that the human can then reject if false. The greatest danger is failing \nto detect early signs of nAMD resulting in a delayed initialization of the treatment, which can dramatically worsen \nthe visual outcome. As input to subsequent prediction algorithms, on the other hand, we suggest identifying the \nbalance between over- and under-segmentation that leads to the best overall Dice score. Finally, to foster the use \nof automated segmentation systems, it would be essential to assess whether an automated suggestion can speed \nup the diagnostic process and lead to more uniform results across the practitioners.\nData availability\nThe data that support the findings of this study were not publicly available. Data are, however, available from \nKai Rothaus upon reasonable request and after the permission of the St. Franziskus Eye-Center in Muenster.\nReceived: 11 November 2022; Accepted: 4 January 2023\nReferences\n 1. Silva, R. et al. Treat-and-extend versus monthly regimen in neovascular age-related macular degeneration: results with ranibizumab \nfrom the trend study. Ophthalmology 125, 57–65 (2018).\n 2. Pennington, K. L. & DeAngelis, M. M. Epidemiology of age-related macular degeneration (amd): associations with cardiovascular \ndisease phenotypes and lipid factors. Eye and vision 3, 1–20 (2016).\n 3. Spaide, R. F . et al. Consensus nomenclature for reporting neovascular age-related macular degeneration data: consensus on neo-\nvascular age-related macular degeneration nomenclature study group. Ophthalmology 127, 616–636 (2020).\n 4. de Moura, J. et al. Intraretinal fluid pattern characterization in optical coherence tomography images. Sensors 20, 2004 (2020).\n 5. Lim, L. S., Mitchell, P ., Seddon, J. M., Holz, F . G. & Wong, T. Y . Age-related macular degeneration. The Lancet 379, 1728–1738 \n(2012).\n 6. Klimscha, S. et al. Spatial correspondence between intraretinal fluid, subretinal fluid, and pigment epithelial detachment in neo -\nvascular age-related macular degeneration. Investigative ophthalmology & visual science 58, 4039–4048 (2017).\n 7. Bhende, M., Shetty, S., Parthasarathy, M. K. & Ramya, S. Optical coherence tomography: A guide to interpretation of common \nmacular diseases. Indian journal of ophthalmology 66, 20 (2018).\n 8. Aumann, S., Donner, S., Fischer, J. & Müller, F . Optical coherence tomography (oct): principle and technical realization. High \nResolution Imaging in Microscopy and Ophthalmology 59–85 (2019).\n 9. Schmitt, J. M., Xiang, S. & Yung, K. M. Speckle in optical coherence tomography. Journal of biomedical optics 4, 95–105 (1999).\n 10. LeCun, Y ., Bengio, Y . & Hinton, G. Deep learning. nature 521, 436–444 (2015).\n 11. Pekala, M. et al. Deep learning based retinal oct segmentation. Computers in biology and medicine 114, 103445 (2019).\n 12. Lu, D. et al. Deep-learning based multiclass retinal fluid segmentation and detection in optical coherence tomography images \nusing a fully convolutional neural network. Medical image analysis 54, 100–110 (2019).\n 13. Li, M.-X. et al. Segmentation of retinal fluid based on deep learning: application of three-dimensional fully convolutional neural \nnetworks in optical coherence tomography images. International journal of ophthalmology 12, 1012 (2019).\n 14. Tan, T., Wang, Z., Du, H., Xu, J. & Qiu, B. Lightweight pyramid network with spatial attention mechanism for accurate retinal \nvessel segmentation. International Journal of Computer Assisted Radiology and Surgery 16, 673–682 (2021).\n 15. Xie, H., Tang, C., Zhang, W ., Shen, Y . & Lei, Z. Multi-scale retinal vessel segmentation using encoder-decoder network with \nsqueeze-and-excitation connection and atrous spatial pyramid pooling. Applied Optics 60, 239–249 (2021).\n 16. Li, K. et al. Accurate retinal vessel segmentation in color fundus images via fully attention-based networks. IEEE Journal of Bio‑\nmedical and Health Informatics 25, 2071–2081 (2020).\n 17. Sappa, L. B. et al. Retfluidnet: Retinal fluid segmentation for sd-oct images using convolutional neural network. Journal of Digital \nImaging 34, 691–704 (2021).\n 18. Khan, S. et al. Transformers in vision: A survey. ACM computing surveys (CSUR) 54, 1–41 (2022).\n 19. Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C. & Dosovitskiy, A. Do vision transformers see like convolutional neural net -\nworks?. Advances in Neural Information Processing Systems 34, 12116–12128 (2021).\n 20. Hatamizadeh, A. et al. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. arXiv preprint \narXiv: 2201. 01266 (2022).\n 21. Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International \nConference on Computer Vision, 10012–10022 (2021).\n 22. Vaswani, A. et al. Attention is all you need. Advances in neural information processing systems 30 (2017).\n 23. Playout, C., Duval, R., Boucher, M. C. & Cheriet, F . Focused attention in transformers for interpretable classification of retinal \nimages. Medical Image Analysis 82, 102608 (2022).\n13\nVol.:(0123456789)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\n 24. Wang, M. et al. Reliable joint segmentation of retinal edema lesions in oct images. arXiv preprint arXiv:  2212. 00330 (2022).\n 25. Kihara, Y . et al. Detection of nonexudative macular neovascularization on structural oct images using vision transformers. Oph ‑\nthalmology Science 2, 100197 (2022).\n 26. Jiang, Z. et al. Computer-aided diagnosis of retinopathy based on vision transformer. Journal of Innovative Optical Health Sciences \n15, 2250009 (2022).\n 27. Lee, C. S. et al. Deep-learning based, automated segmentation of macular edema in optical coherence tomography. Biomedical \noptics express 8, 3440–3448 (2017).\n 28. Oktay, O. et al. Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv: 1804. 03999 (2018).\n 29. Fu, J. et al. Dual attention network for scene segmentation. In Proceedings of the IEEE/CVF conference on computer vision and \npattern recognition, 3146–3154 (2019).\n 30. Pang, Y ., Li, Y ., Shen, J. & Shao, L. Towards bridging semantic gap to improve semantic segmentation. In Proceedings of the IEEE/\nCVF International Conference on Computer Vision, 4230–4239 (2019).\n 31. Ramachandran, P . et al. Stand-alone self-attention in vision models. Advances in Neural Information Processing Systems 32 (2019).\n 32. Schlemper, J. et al. Attention gated networks: Learning to leverage salient regions in medical images. Medical image analysis  53, \n197–207 (2019).\n 33. Wolf, T. et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods \nin natural language processing: system demonstrations, 38–45 (2020).\n 34. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010. \n11929 (2020).\n 35. Gehring, J., Auli, M., Grangier, D., Y arats, D. & Dauphin, Y . N. Convolutional sequence to sequence learning. In International \nconference on machine learning, 1243–1252 (PMLR, 2017).\n 36. Brooks, J. COCO Annotator. https:// github. com/ jsbro ks/ coco- annot ator/ (2019).\n 37. Russakoff, D. B., Lamin, A., Oakley, J. D., Dubis, A. M. & Sivaprasad, S. Deep learning for prediction of amd progression: a pilot \nstudy. Investigative ophthalmology & visual science 60, 712–722 (2019).\n 38. Reza, A. M. Realization of the contrast limited adaptive histogram equalization (CLAHE) for real-time image enhancement. Journal \nof VLSI signal processing systems for signal, image and video technology 38, 35–44 (2004).\n 39. Pizer, S. M., Zimmerman, J. B. & Staab, E. V . Adaptive grey level assignment in ct scan display. Journal of Computer Assisted \nTomography 8, 300–305 (1984).\n 40. Müller, D., Soto-Rey, I. & Kramer, F . Towards a guideline for evaluation metrics in medical image segmentation. arXiv preprint \narXiv: 2202. 05273 (2022).\n 41. Taha, A. A. & Hanbury, A. Metrics for evaluating 3d medical image segmentation: analysis, selection, and tool. BMC medical \nimaging 15, 1–28 (2015).\n 42. Reinke, A. et al. Common limitations of image processing metrics: A picture story. arXiv preprint arXiv: 2104. 05642 (2021).\n 43. Ronneberger, O., Fischer, P . & Brox, T. U-net: Convolutional networks for biomedical image segmentation. In International Confer‑\nence on Medical image computing and computer‑assisted intervention, 234–241 (Springer, 2015).\n 44. Chen, Y . et al. Channel-unet: a spatial channel-wise convolutional neural network for liver and tumors segmentation. Frontiers in \ngenetics 10, 1110 (2019).\n 45. Huang, H. et al. Unet 3+: A full-scale connected unet for medical image segmentation. In ICASSP 2020 ‑2020 IEEE International \nConference on Acoustics, Speech and Signal Processing (ICASSP), 1055–1059 (IEEE, 2020).\n 46. Zhou, Z., Rahman Siddiquee, M. M., Tajbakhsh, N. & Liang, J. Unet++: A nested u-net architecture for medical image segmenta-\ntion. In Deep learning in medical image analysis and multimodal learning for clinical decision support, 3–11 (Springer, 2018).\n 47. Cao, H. et al. Swin-unet: Unet-like pure transformer for medical image segmentation. arXiv preprint arXiv: 2105. 05537 (2021).\n 48. Milletari, F ., Navab, N. & Ahmadi, S.-A. V-net: Fully convolutional neural networks for volumetric medical image segmentation. \nIn 2016 fourth international conference on 3D vision (3DV), 565–571 (IEEE, 2016).\n 49. Robbins, H. & Monro, S. A stochastic approximation method. The annals of mathematical statistics 400–407 (1951).\n 50. Choi, D. et al. On empirical comparisons of optimizers for deep learning. arXiv preprint arXiv: 1910. 05446 (2019).\n 51. Smith, L. N. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight \ndecay. arXiv preprint arXiv: 1803. 09820 (2018).\n 52. Raschka, S. Model evaluation, model selection, and algorithm selection in machine learning. arXiv preprint arXiv: 1811. 12808  \n(2018).\n 53. Nahmias, M. A. et al. Photonic multiply-accumulate operations for neural networks. IEEE Journal of Selected Topics in Quantum \nElectronics 26, 1–18 (2019).\n 54. Bogunović, H. et al. Retouch: the retinal oct fluid detection and segmentation benchmark and challenge. IEEE transactions on \nmedical imaging 38, 1858–1874 (2019).\n 55. Xing, G. et al. Multi-scale pathological fluid segmentation in oct with a novel curvature loss in convolutional neural network. IEEE \nTransactions on Medical Imaging (2022).\n 56. Lu, D. et al. Deep-learning based multiclass retinal fluid segmentation and detection in optical coherence tomography images \nusing a fully convolutional neural network. Medical image analysis 54, 100–110 (2019).\n 57. Hassan, B. et al. Deep learning based joint segmentation and characterization of multi-class retinal fluid lesions on oct scans for \nclinical use in anti-vegf therapy. Computers in Biology and Medicine 136, 104727 (2021).\n 58. Liu, W ., Sun, Y . & Ji, Q. Mdan-unet: multi-scale and dual attention enhanced nested u-net architecture for segmentation of optical \ncoherence tomography images. Algorithms 13, 60 (2020).\n 59. De Zanet, S. et al. Automated detection and quantification of pathological fluid in neovascular age-related macular degeneration \nusing a deep learning approach. Investigative Ophthalmology & Visual Science 61, 1655 (2020).\n 60. Sappa, L. B. et al. Retfluidnet: Retinal fluid segmentation for sd-oct images using convolutional neural network. Journal of Digital \nImaging 34, 691–704 (2021).\n 61. Guo, Y . et al. Automated segmentation of retinal fluid volumes from structural and angiographic optical coherence tomography \nusing deep learning. Translational vision science & technology 9, 54 (2020).\n 62. Ma, D. et al. Lf-unet-a novel anatomical-aware dual-branch cascaded deep neural network for segmentation of retinal layers and \nfluid from optical coherence tomography images. Computerized Medical Imaging and Graphics 94, 101988 (2021).\nAcknowledgements\nThis work was supported by national funds through FCT (Fundação para a Ciência e a Tecnologia), under the \nproject - UIDB/04152/2020 - Centro de Investigação em Gestão de Informação (MagIC)/NOV A IMS. Mauro \nCastelli acknowledges the financial support from the Slovenian Research Agency (research core funding no. \nP5-0410).\n14\nVol:.(1234567890)Scientific Reports |          (2023) 13:517  | https://doi.org/10.1038/s41598-023-27616-1\nwww.nature.com/scientificreports/\nAuthor contributions\nM.C. and D.P .: Conceptualization, Writing—Review and Editing, Methodology, Investigation; D.P . Software; \nD.P . Formal analysis; M.C. Validation; M.C. Supervision and Project administration; K.R. Data collection; K.R. \nData curation. All authors reviewed the work, contributed to its overall scientific content and approved the final \nversion of the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to M.C.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023"
}