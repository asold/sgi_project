{
  "title": "Chinese Spelling Error Detection and Correction Based on Language Model, Pronunciation, and Shape",
  "url": "https://openalex.org/W2250444785",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A2122749030",
      "name": "Junjie Yu",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2105297065",
      "name": "Zhenghua Li",
      "affiliations": [
        "Soochow University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2099143268",
    "https://openalex.org/W1510106835",
    "https://openalex.org/W2010595692",
    "https://openalex.org/W2252231918",
    "https://openalex.org/W2034038326",
    "https://openalex.org/W7062651059",
    "https://openalex.org/W2251157340",
    "https://openalex.org/W2070944431",
    "https://openalex.org/W2350627527",
    "https://openalex.org/W2165945467"
  ],
  "abstract": "Spelling check is an important preprocessing task when dealing with user generated texts such as tweets and product comments.Compared with some western languages such as English, Chinese spelling check is more complex because there is no word delimiter in Chinese written texts and misspelled characters can only be determined in word level.Our system works as follows.First, we use character-level n-gram language models to detect potential misspelled characters with low probabilities below some predefined threshold.Second, for each potential incorrect character, we generate a candidate set based on pronunciation and shape similarities.Third, we filter some candidate corrections if the candidate cannot form a legal word with its neighbors according to a word dictionary.Finally, we find the best candidate with highest language model probability.If the probability is higher than a predefined threshold, then we replace the original character; or we consider the original character as correct and take no action.Our preliminary experiments shows that our simple method can achieve relatively high precision but low recall.",
  "full_text": "Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 220–223,\nWuhan, China, 20-21 October 2014\nChinese\nSpelling\nError\nDetection\nand\nCorrection\nBased\non\nLanguage\nModel,\nPronunciation\n,\nand\nShape\nJunjie\nYu\nand\nZhenghua\nLi\nProvincial\nKey\nLaboratory\nfor\nComputer\nInformation\nProcessing\nTechnology\nSoochow\nUniversity\n,\nChina\n20144227010@stu.suda.edu.cn\n;\nzhli13@suda.edu.cn\nAbstract\nS\npell\ning\ncheck\nis\nan\nimportant\npreprocessing\ntask\nwhen\ndealing\nwith\nuser\ngenerated\ntexts\nsuch\nas\ntweets\nand\nproduct\ncomments.\nCompared\nwith\nsome\nwestern\nlanguages\nsuch\nas\nEnglish,\nChinese\nspelling\ncheck\nis\nmore\ncomplex\nbecause\nthere\nis\nno\nword\ndelimiter\nin\nChinese\nwritten\ntexts\nand\nmisspelled\ncharacters\ncan\nonly\nbe\ndetermined\nin\nword\nlevel.\nOur\nsystem\nworks\nas\nfollows.\nFirst,\nwe\nuse\ncharacter-level\nn-gram\nlanguage\nmodel\ns\nto\ndetect\npotential\nmisspelled\ncharacters\nwith\nlow\nprobabilities\nbelow\nsome\npredefined\nthreshold.\nSecond,\nfor\neach\npotential\nincorrect\ncharacter,\nwe\ngenerate\na\ncandidate\nset\nbased\non\npronunciation\nand\nshape\nsimilarities.\nThird,\nwe\nfilter\nsome\ncandidate\ncorrections\nif\nthe\ncandidate\ncannot\nform\na\nlegal\nword\nwith\nits\nneighbors\naccording\nto\na\nword\ndictionary.\nFinally,\nwe\nfind\nthe\nbest\ncandidate\nwith\nhighest\nlanguage\nmodel\nprobability.\nIf\nthe\nprobability\nis\nhigher\nthan\na\npredefined\nthreshold,\nthen\nwe\nreplace\nthe\noriginal\ncharacter;\nor\nwe\nconsider\nthe\noriginal\ncharacter\nas\ncorrect\nand\ntake\nno\naction.\nOur\npreliminary\nexperiments\nshows\nthat\nour\nsimple\nmethod\ncan\nachieve\nrelatively\nhigh\nprecision\nbut\nlow\nrecall.\n1\nIntroduction\nSpelling\ncheck\nis\na\ntraditional\nand\nimportant\npreprocessing\ntask\nfor\nnatural\nlanguage\nprocessing\n,\nsince\nspelling\nerrors\nhappen\nin\nwritten\ntexts\n,\nsuch\nas\nshort\nmessages,\nemails\n,\nand\nso\non\n.\nLots\nof\nresearch\nhas\nbeen\ndevoted\nto\nEnglish\nspelling\nerror\ndetection\nand\ncorrection.\nIn\nEnglish\nspelling\nerror\ndetection\nand\ncorrection,\nthe\nerror\ns\ncan\nbe\nclassified\ninto\n“\nnon-\nword\n”\nerror\nand\n“\nreal-word\n”\nerror\n(\nKukich,\n1992\n)\n.\nUnlike\nEnglish,\nChinese\nwords\nare\nnot\nseparated\nby\nspace\nand\nall\ncharacter\ns\nin\nChinese\nare\n“\nreal-word\n”\n.\nTherefore\n,\nautomatic\nword\nsegmentation\nneed\nto\nbe\napplied\nin\norder\nto\nproduce\nwords\n(\nZhang\net\nal.,\n2000\n)\n.\nThere\nare\nmany\nChinese\ninput\nmethods\n(\nZhang\net\nal.,\n2005\n)\n.\nDifferent\ninput\nmethods\nlead\nto\ndifferent\ntypes\nof\nspelling\nerrors\n.\nFor\nexample,\ninput\nmethod\ns\nbased\non\npinyin\nwhich\nusually\nlead\nto\nspelling\ner\nr\nors\nof\ncharacters\nsharing\ns\nimilar\npronunciations\n;\nwhile\ninput\nmethods\nbased\non\nradical\nmethods\nusually\nlead\nto\nerrors\nrelated\nto\ncharacter\nshapes.\nHuang\net\nal.\n(\n2007\n)\nproposed\na\nlearning\nmodel\nbased\non\nChinese\nphonemic\nalphabet\nto\ndetect\nChinese\nspelling\nerrors\n.\nYeh\net\nal.\n(\n2013\n)\npresent\ned\na\nmethod\nbased\non\nN-\ngram\nranked\ninverted\nindex\nlist\nto\ndeal\nwith\nthis\nproblem\n.\n2\nS\nystem\nArchitecture\nOur\nsystem\nincludes\ntwo\ncascaded\ncomponent\ns\n:\nspelling\nerror\ndetection\nand\nspelling\nerror\ncorrection\n,\nas\nshown\nin\nFigure\n1.\n2.1\nR\nesource\ns\nTo\ntrain\nour\nlanguage\nmode,\nwe\nuse\na\nportion\nof\nChinese\nGigaword\nversion\n2.0\n(\nLDC2009T14)\n,\nwhich\ncontains\nabout\n12\nmillion\ntraditional\nChinese\nsentences.\nWe\ndo\nnot\nsplit\nsentence\ninto\nwords,\nbut\ntreat\ne\nach\ncharacter\nas\na\nn\nindividual\nunit\n.\nIn\nother\nwords,\nour\nlanguage\nmodel\nis\nbased\non\ncharacter\n.\nIn\norder\nto\ntake\nadvantage\nof\nthe\ncontext\ninformation,\nwe\ntrain\na\nnew\nlanguage\nmodel\nby\nrevers\ning\nall\nsentences\nin\nthe\ncorpus\n.\nSo,\nw\ne\nwill\ncalculate\ntwice\nfor\none\ncharacter\nbased\non\nthis\ntwo\nlanguage\nmodel\ns\n.\nAnd\nthe\ntotal\nscore\nis\nthe\ncombination\nof\nboth\n.\nAs\nmisspelled\ncharacters\nin\na\nsentence\ncan\nonly\nbe\ndetected\nin\nword\nlevel\n,\nwe\nconstruct\na\nword\ndictionary\nwhich\ncontains\nabout\n300\nthousand\nwords\ncollected\nfrom\nInternet.\nAnd\nthe\nSIGHAN\norganize\nr\nprovid\nes\na\ndictionary\ninclud\ning\nabout\n5000\nChinese\ncharacters\nwith\nother\ncharacters\nin\nsimilar\npronunciation\nor\nshape\nwhich\ncan\nbe\nused\nin\ncandidate\ngeneration.\n2.2\nSpelling\nError\nDetection\nIn\nspelling\nerror\ndetection\nphase,\nwe\npropose\ntwo\nmethod\ns\nto\ndeal\nwith\nthis\nproblem.\nOne\nis\nto\ngather\nthe\ncharacters\nwhich\nget\na\nlow\nscore\n220\nFigure\n1\n:\nF\nramework\nof\nour\nproposed\nsystem\nunder\nlanguage\nmodel\n.\nA\nnother\nis\nto\nrecord\nany\nindependent\ncharacters\nafter\nautomatic\nword\nsegmentation.\nHowever,\nwe\nfind\nboth\nwill\nbring\nin\nlots\nof\nirrelevant\ncharacters\nthough\nmost\nerror\ns\nhave\nbeen\ndiscovered\n.\nBecause\nChen\net\nal.\n(2011)\nf\nin\nd\nthe\naverage\namount\nof\nerrors\nin\na\nlearners\n’\ncorpus\nfor\na\nstudent\nessay\nis\nonly\n2\n,\nw\ne\ndo\nnot\nwant\nto\nmark\ntoo\nmany\nerror\ncharacters\nto\ncause\nfalse-alarm\nproblem\nheavily\n.\nIn\norder\nto\nmake\nthe\nbest\nof\nthe\ntwo\nmethods,\nwe\nprepare\ntwo\nsteps\nto\ncombine\nboth.\nStep\n1,\nwe\ncalculate\nthe\nscore\nof\ne\nach\ncharacter\nin\na\nsentence\nby\na\nforward-backward\n5-gram\nlanguage\nmodel.\nWhile\nthe\nscore\nis\nless\nthan\nthe\nthreshold,\nthe\ncharacter\nand\nits\nlocation\nare\nsent\nto\nStep\n2.\nT\no\nfind\nas\nmore\nerrors\nas\npossible,\nwe\nset\nthe\nthreshold\nin\na\nquit\ne\ntight\nvalue.\nHowever,\nthis\nwill\nresult\nin\nmore\nirrelevant\ncharacters\nwhich\nconfuse\nthe\nsystem.\nIn\nStep\n2,\nwe\nneed\nto\nfilter\nthe\ncharacters\ngenerated\nin\nStep\n1.\nW\ne\nwill\njudge\nthe\ncharacter\nwhether\nit\ncan\nconstruct\na\nword\n.\nO\ntherwise,\nwe\nmake\nthe\nassumption\nthat\nit\nmay\nbe\na\nspelling\nerror\nwhich\nmeans\nwe\nare\nstill\nnot\nsure\nabout\nit.\nAnyhow,\nwe\nwill\nsen\nd\nthe\nresult\ns\nto\nnext\nphase.\n2.3\nSpelling\nError\nCorrection\nIn\nspelling\nerror\ncorrection\nphase,\nwe\nfirst\nly\ngenerate\na\ncandidate\nset\nfor\nthe\nerror\ncharacter.\nCharacters\nof\nsimilar\npronunciations\nare\nthe\nmost\ncommon\nsource\nof\nspelling\nerrors\n(Wu\net\nal.,\n2013)\n.\nBut\nthere\nstill\nexist\nsome\nerrors\nfrom\nsimilar\nshape\n(\nLiu\ne\nt\nal.,\n2011\n)\n.\nSo,\nthe\ncandidate\ngeneration\nis\nbased\non\na\nsimilar\npronunciation\nor\nshape\ndictionary.\nFor\nmore\ndetails\nabout\nthe\ndictionary,\nplease\nrefer\nto\nYeh\net\nal.,\n(2013).\nSecond\nly\n,\neach\ncharacter\nin\nthe\ncandidate\nset\nwill\nbe\ntested\nwhether\nit\ncan\nform\na\nlegal\nword\nwith\nits\nneighbors.\nHere,\nthe\ncharacter\nwhich\ncan\nconstruct\na\nlegal\nword\nwith\nits\nneighbors\nwill\nbe\nleft\nfor\ncalculating\nits\nscore\nby\nthe\nlanguage\nmodel.\nAfter\nfiltering\n,\nthe\nnumber\nof\ncandidates\nhas\nbeen\nreduced\nwhich\nwill\nbring\ntwo\nbenefits:\nmost\ncandidates\nthat\nhave\nbeen\ncut\nare\nirrelevant\ncharacters\nand\nless\ncandidates\nmakes\nthe\nsystem\nbe\nmore\nefficient.\nAt\nlast,\nthe\nbest\ncandidate\nmeans\none\ncharacter\ngets\nthe\nhighest\nscore\nunder\na\nforward-backward\n5-gram\nlanguage\nmodel\nand\nthe\nscore\nis\nhigh\ne\nr\nthan\nthe\nthreshold\n.\nIf\nexisting,\nthe\noriginal\ncharacter\nfinally\nwill\nbe\nrecognized\nas\na\nn\nerror\ncharacter\nand\nit\nwill\nbe\nreplaced\nby\nthe\nbest\ncandidate.\nWe\nonly\nuse\nthe\nlanguage\nmodel\nto\nchoose\nthe\nbest\ncandidate\nbecause\nwe\nf\nin\nd\nthat\nthe\nlanguage\nmodel\nc\nan\nget\na\nquite\nhigh\naccuracy\nif\nwe\nc\nan\nprovide\na\nsuitable\ncandidate\nset\nsuccessfully\n.\n3\nExperiment\nal\nAnalysis\nIn\nthis\npaper,\nwe\nuse\n300\nsentences\nfrom\nthe\nfinal\ntest\nof\nSIGHAN\nBake-off\n2013\nas\nour\ntraining\ndata\nand\n1000\nsentences\nprovided\nby\nthe\nSIGHAN\norganizer\nare\nour\ntest\ndata\n.\nIn\nour\ntraining\ndata,\nthere\nare\n402\nerror\ncharacters\nin\ntotal.\nWe\nfirst\ntest\nthe\nrecall\nof\nthe\nspelling\nerror\ndetection\nbased\non\nlanguage\nmodel.\nFunction\nt\nhreshold\nL\nanguage\nmodel\nRecall\n(%)\n#\nC\nharacters\n-4\n26.67\n2\n-3\n57.00\n6\n-2\n86.67\n18\n-1\n96.32\n38\nTable\n1\n:\nResults\non\nerror\ndetection\nT\nable\n1\nshows\nthat\nwhen\nthreshold\nbecome\ntight\ner\n,\nthe\nrecall\nis\nhigher.\nH\nowever,\nthe\naverage\nnumber\nof\ncharacters\nincrease\ns\nquickly.\nA\nverage\nnumber\nof\ncharacters\nmeans\nhow\nmany\n221\nFigure\n2:\nE\nxample\nto\nshow\nhow\nto\nconstruct\na\nword\nRun\nFalse\nPositive\nRate\nDetection\nLevel\nCorrection\nLevel\nAccuracy\nPrecision\nRecall\nF1\nAccuracy\nPrecision\nRecall\nF1\n1\n0.2524\n0.4539\n0.3881\n0.1601\n0.2267\n0.4426\n0.3527\n0.1375\n0.1978\n2\n0.032\n0.5292\n0.7385\n0.0904\n0.1611\n0.5235\n0.7119\n0.0791\n0.1424\nTable\n2\n:\nResults\nof\nour\nerror\ndetection\nand\ncorrection\nsubtask\ncharacters\nare\nmarked\nas\nerror\ncharacters\nby\nour\nsystem.\nThe\naverage\nlength\nof\nsentences\nin\nour\ntraining\ndata\nis\nabout\n70\ncharacters.\nW\nhen\nthe\nthreshold\nhas\nbeen\nset\nto\nbe\n-\n1,\nmore\nthan\nhalf\nof\nthe\ncharacters\nin\na\nsentence\nhave\nbeen\nmarked\nas\nerror\ns\non\naverage\n.\nThough\nthe\nrecall\nis\nvery\nhigh\nin\nthis\ncase,\ntoo\nmany\ncorrect\ncharacters\nhave\nbeen\nrecognized\nas\nerrors.\nSo\nwe\nprefer\nto\ngive\nup\nthe\nhigh\nrecall\nrather\nthan\nreserve\ntoo\nmany\nirrelevant\ncharacters.\nAs\nwe\nmentioned\nin\nS\nection\n2.2,\nthe\naverage\nnumber\nof\nspelling\nerrors\nin\na\nsentence\nis\nquit\ne\nlow.\nThreshold\n=\n-2\nonly\nleads\nto\na\nslight\nreduce\nin\nrecall\nbut\nthe\naverage\nnumber\nof\ncharacters\nhave\nbeen\ncut\ndown\nby\nhalf.\nAs\nshown\nin\nF\nigure\n1,\nwe\nfirst\nly\nprepare\ntwo\nresources\n:\na\nforward-backward\n5-gram\nlanguage\nmodel\nand\na\nword\ndictionary.\nAs\ndescribed\nin\nprevious\nsections,\nsuch\ntwo\nresources\nwill\nbe\napplied\ninto\nboth\nspelling\ncheck\ndetection\nand\ncorrection.\nThen,\nwe\nstart\nto\ndetect\nthe\nerror\ncharacters\nin\na\nsentence.\nFor\neach\ncharacter\nin\na\nsentence\n,\nif\nits\nscore\nwhich\ncalculated\nby\nthe\nforward-backward\n5-gram\nlanguage\nmodel\nis\nless\nthan\nthe\nthreshold\nvalue\n,\nit\nwill\nbe\nsent\nto\nnext\nphase\n.\nA\nnd\nthe\nthreshold\nis\nset\nat\n-2\nas\nwe\ndiscussed\nbefore.\nNext,\nwe\nwill\ntest\nthe\ncharacter\nfor\nconstructing\na\nword.\nWe\nset\nthe\nsize\nof\nthe\nwindow\nat\n4\nwhich\nmeans\nthe\ntarget\ncharacter\nc\nan\nbe\ncombined\nwith\nits\nneighbors\nat\na\ndistance\nof\n4\ncharacters.\nFor\nexample,\nF\nigure\n2\ndescribe\ns\nthe\ndetails\n.\nAfter\nthe\ntarget\ncharacter\nis\ncombined\nwith\nits\nneighbors,\nwe\nwill\nlook\nup\nthe\nword\ndictionary.\nWhile\nno\nne\nof\ncombination\ns\nc\nan\nbe\nfound\nin\nthe\nword\ndictionary,\nwe\nmake\nthe\nassumption\nthat\nthe\ntarget\ncharacter\nmay\nbe\na\nn\nerror.\nIn\nthis\nexample,\nnone\nof\nthese\n7\nwords\nc\nan\nbe\nfound\nin\nword\ndictionary.\nSo,\nthe\ncharacter\n“\n竟\n”\nin\nthis\nsentence\nwould\nbe\nmarked\nas\nan\nerror\nand\nsent\nto\nnext\nphase.\nIn\nspelling\ncheck\ncorrection\nphase,\nwe\nfirst\ngenerate\ncandidates\nby\nsimilar\npronunciation\nor\nshape\n.\nT\nhen\nthe\ncandidates\nare\nfiltered\nby\nconstructing\na\nword\n.\nThis\ntime,\nwe\nreserve\nthe\ncandidate\ns\nwhich\ncan\nconstruct\na\nword\nwith\nits\nneighbors.\nAt\nlast,\nthe\nrest\ncandidates\nwill\nbe\nranked\nby\nlanguage\nmodel.\nThe\nbest\ncandidate\nwith\nits\nscore\nhigher\nthan\nthreshold\nwill\nreplace\nthe\noriginal\ncharacter\nin\nthe\nsentence.\nHere,\nthe\nthreshold\nis\nthe\nsame\nwith\nthe\nvalue\nin\ndetection\nlevel.\n4\nFinal\nResults\nIn\nthis\nbake-off,\nthere\nare\n1000\nsentences\nand\nall\nsentences\ncontain\nat\nleast\nmore\nthan\none\nerror.\nTable\n2\nshows\nthat\nthe\nF1\nscore\nis\nvery\nlow\nbecause\nwe\ncan\nonly\nfind\na\nsmall\nportion\nof\nall\nerrors.\nHowever,\nthe\nfalse\npositive\nrate\nand\nprecision\nis\nsatisf\nactory\nespecially\nfor\nthe\nfalse\npositive\nrate.\nSuch\nresults\nare\nconsistent\nwith\nour\nmain\nidea\nthat\nw\ne\nchoose\nto\nunder-\ncorrect\nrather\nthan\nover-correct.\n222\nWe\nc\nan\nsee\nthat\nthe\nperformance\nin\ndetection\nlevel\nand\ncorrection\nlevel\nare\nsimilar\n.\nAs\ndescribed\nin\nprevious\nsections,\nonly\nwhen\nthe\nbest\ncandidate\nhas\nbeen\nfound,\nwe\nwill\nmake\nthe\nconclusion\nthat\nthe\ntarget\ncharacter\nis\na\nspelling\nerror.\nThe\nperformance\nin\ncorrection\nlevel\nonly\nha\ns\na\nslight\ndecrease\ncompared\nwith\nthe\ndetection\nlevel.\nBut\nthe\nunavoidable\nreality\nis\nthat\nthe\nrecall\nis\nnot\ngood.\n5\nConclusions\nBased\non\nn-gram\nlanguage\nmodel\nand\njudging\na\ncharacter\nwhether\nit\ncan\nform\na\nlegal\nword\nwith\nits\nneighbors,\na\nsimple\napproach\nis\nproposed\nto\ndetect\nand\ncorrect\nthe\nspelling\nerrors\nin\ntraditional\nChinese\ntext.\nTo\nfind\nthe\nspelling\nerrors\nin\nsentence,\nthe\nlanguage\nmodel\nand\na\nword\ndictionary\nare\nboth\nused.\nAnd\nin\norder\nto\nreduce\nthe\nfalse\npositive\nrate,\nthe\nsystem\nonly\ntreat\ns\nthe\ncharacter\nas\na\nspelling\nerror\nwhen\nthe\nbest\ncandidate\nhas\nbeen\nfound.\nAcknowledgment\ns\nThis\nwo\nrk\nwas\nsupported\nby\nNational\nNatural\nScience\nFoundation\nof\nChina\n(Grant\nNo.\n61373095,\n61333018).\nReference\nChen,\nY.\nZ.,\nWu,\nS.\nH.,\nYang,\nP.\nC.,\n&\nKu,\nT.\n(2011).\nImprove\nthe\ndetection\nof\nimproperly\nused\nChinese\ncharacters\nin\nstudents'\nessays\nwith\nerror\nmodel\n.\nInternational\nJournal\nof\nContinuing\nEngineering\nEducation\nand\nLife\nLong\nLearning,\n21(1),\n103-116.\nHuang,\nC.\nM.,\nWu,\nM.\nC.,\n&\nChang,\nC.\nC.\n(2007).\nError\ndetection\nand\ncorrection\nbased\non\nChinese\nphonemic\nalphabet\nin\nChinese\ntext\n.\nIn\nModeling\nDecisions\nfor\nArtificial\nIntelligence\n(pp.\n463-476).\nSpringer\nBerlin\nHeidelberg.\nKukich,\nK.\n(1992).\nTechniques\nfor\nautomatically\ncorrecting\nwords\nin\ntext\n.\nACM\nComputing\nSurveys\n(CSUR),\n24(4),\n377-439.\nLiu,\nC.\nL.,\nLai,\nM.\nH.,\nTien,\nK.\nW.,\nChuang,\nY.\nH.,\nWu,\nS.\nH.,\n&\nLee,\nC.\nY.\n(2011).\nVisually\nand\nphonologically\nsimilar\ncharacters\nin\nincorrect\nChinese\nwords:\nAnalyses,\nidentification,\nand\napplications.\nACM\nTransactions\non\nAsian\nLanguage\nInformation\nProcessing\n(TALIP),\n10(2),\n10.\nWu,\nS.\nH.,\nLiu,\nC.\nL.,\n&\nLee,\nL.\nH.\nChinese\nSpelling\nCheck\nEvaluation\nat\nSIGHAN\nBake-\noff\n2013.\nIn\nSixth\nInternational\nJoint\nConference\non\nNatural\nLanguage\nProcessing\n(p.\n35)\n.\nYeh,\nJ.\nF.,\nLi,\nS.\nF.,\nWu,\nM.\nR.,\nChen,\nW.\nY.,\n&\nSu,\nM.\nC.\n(2013).\nChinese\nWord\nSpelling\nCorrection\nBased\non\nN-gram\nRanked\nInverted\nIndex\nList.\nIn\nSixth\nInternational\nJoint\nConference\non\nNatural\nLanguage\nProcessing\n(p.\n43).\nZhang,\nL.,\nHuang,\nC.,\nZhou,\nM.,\n&\nPan,\nH.\n(2000).\nAutomatic\ndetecting/correcting\nerrors\nin\nChinese\ntext\nby\nan\napproximate\nword-\nmatching\nalgorithm\n.\nIn\nProceedings\nof\nthe\n38th\nAnnual\nMeeting\non\nAssociation\nfor\nComputational\nLinguistics\n(pp.\n248-254).\nAssociation\nfor\nComputational\nLinguistics.\nZHANG,\nY.\nS.\n,\nYU\nShi-wen\n.\n(2006).\nSummary\nof\nText\nAutomatic\nProofreading\nTechnology\n.\nApplication\nResearch\nof\nComputers,\n6.\n223",
  "topic": "Spelling",
  "concepts": [
    {
      "name": "Spelling",
      "score": 0.8409088253974915
    },
    {
      "name": "Pronunciation",
      "score": 0.8061239123344421
    },
    {
      "name": "Computer science",
      "score": 0.7334518432617188
    },
    {
      "name": "Natural language processing",
      "score": 0.6471397280693054
    },
    {
      "name": "Error detection and correction",
      "score": 0.6327441334724426
    },
    {
      "name": "Artificial intelligence",
      "score": 0.581369936466217
    },
    {
      "name": "Speech recognition",
      "score": 0.5086127519607544
    },
    {
      "name": "Error analysis",
      "score": 0.48526665568351746
    },
    {
      "name": "Language model",
      "score": 0.46514761447906494
    },
    {
      "name": "Linguistics",
      "score": 0.4302102327346802
    },
    {
      "name": "Algorithm",
      "score": 0.14509806036949158
    },
    {
      "name": "Mathematics",
      "score": 0.12549468874931335
    },
    {
      "name": "Philosophy",
      "score": 0.058472633361816406
    },
    {
      "name": "Applied mathematics",
      "score": 0.0
    }
  ]
}