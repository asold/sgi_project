{
  "title": "FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning",
  "url": "https://openalex.org/W4393148063",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2338464104",
      "name": "Hao-Kun Chen",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "Siemens (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2096325748",
      "name": "Yao Zhang",
      "affiliations": [
        "Munich Center for Machine Learning",
        "Machine Science",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2229751183",
      "name": "Denis Krompass",
      "affiliations": [
        "Siemens (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2786186765",
      "name": "Jindong Gu",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A175204660",
      "name": "Volker Tresp",
      "affiliations": [
        "Munich Center for Machine Learning",
        "Ludwig-Maximilians-Universität München",
        "Machine Science"
      ]
    },
    {
      "id": "https://openalex.org/A2338464104",
      "name": "Hao-Kun Chen",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum",
        "Siemens (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2096325748",
      "name": "Yao Zhang",
      "affiliations": [
        "LMU Klinikum",
        "Ludwig-Maximilians-Universität München",
        "Munich Center for Machine Learning"
      ]
    },
    {
      "id": "https://openalex.org/A2229751183",
      "name": "Denis Krompass",
      "affiliations": [
        "Siemens (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2786186765",
      "name": "Jindong Gu",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A175204660",
      "name": "Volker Tresp",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum",
        "Munich Center for Machine Learning"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6640773114",
    "https://openalex.org/W3098071563",
    "https://openalex.org/W3020257313",
    "https://openalex.org/W6795288823",
    "https://openalex.org/W3081991540",
    "https://openalex.org/W2788643321",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W2950104027",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W4293326769",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W3103802018",
    "https://openalex.org/W3016923549",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W3105324058",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2541884796",
    "https://openalex.org/W3211770985",
    "https://openalex.org/W6756577627",
    "https://openalex.org/W6634232107",
    "https://openalex.org/W4225750637",
    "https://openalex.org/W4283332761",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2899335602",
    "https://openalex.org/W4200629441",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W4297899355",
    "https://openalex.org/W2912371042",
    "https://openalex.org/W4283811427",
    "https://openalex.org/W2903073381",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4382450654",
    "https://openalex.org/W4376163524",
    "https://openalex.org/W4389459523",
    "https://openalex.org/W4312884055",
    "https://openalex.org/W4223561012",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3162926177",
    "https://openalex.org/W2963622213",
    "https://openalex.org/W4377371500",
    "https://openalex.org/W4292720139",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W4327673103",
    "https://openalex.org/W4309208182",
    "https://openalex.org/W3127724266",
    "https://openalex.org/W4286336838",
    "https://openalex.org/W4375957738",
    "https://openalex.org/W4312950667",
    "https://openalex.org/W4322717062",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W1575833922",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2560674852",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3045674654",
    "https://openalex.org/W4318619660",
    "https://openalex.org/W4382492045",
    "https://openalex.org/W2981720610",
    "https://openalex.org/W4367046615",
    "https://openalex.org/W3133825286",
    "https://openalex.org/W4293141861",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W4379538795",
    "https://openalex.org/W3113151582",
    "https://openalex.org/W4302307546",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W4321472314",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W4309202064",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4321392831",
    "https://openalex.org/W2963530300"
  ],
  "abstract": "Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning framework tailored to heterogeneous multi-modal FL, called Federated Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the client local updates and applying Mutual Knowledge Distillation (MKD) for an efficient knowledge transfer. FedDAT is the first approach that enables an efficient distributed finetuning of foundation models for a variety of heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we conduct extensive experiments on four multi-modality FL benchmarks with different types of data heterogeneity, where FedDAT substantially outperforms the existing centralized PEFT methods adapted for FL.",
  "full_text": "FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal\nHeterogeneous Federated Learning\nHaokun Chen1,2 , Yao Zhang1,4 , Denis Krompass2, Jindong Gu3*, Volker Tresp1,4\n1 LMU Munich, Munich, Germany\n2 Siemens AG, Munich, Germany\n3 University of Oxford, Oxford, England\n4 Munich Center for Machine Learning (MCML), Munich, Germany\n{haokun.chen, denis.krompass}@siemens.com, yzhang@dbs.ifi.lmu.de\njindong.gu@outlook.com, volker.tresp@lmu.de\nAbstract\nRecently, foundation models have exhibited remarkable ad-\nvancements in multi-modal learning. These models, equipped\nwith millions (or billions) of parameters, typically require a\nsubstantial amount of data for finetuning. However, collect-\ning and centralizing training data from diverse sectors be-\ncomes challenging due to distinct privacy regulations. Feder-\nated Learning (FL) emerges as a promising solution, enabling\nmultiple clients to collaboratively train neural networks with-\nout centralizing their local data. To alleviate client computa-\ntion burdens and communication overheads, previous works\nhave adapted Parameter-efficient Finetuning (PEFT) methods\nfor FL. Hereby, only a small fraction of the model parameters\nare optimized and communicated during federated communi-\ncations. Nevertheless, most previous works have focused on\na single modality and neglected one common phenomenon,\ni.e., the presence of data heterogeneity across the clients.\nTherefore, in this work, we propose a finetuning framework\ntailored to heterogeneous multi-modal FL, called Federated\nDual-Aadapter Teacher (FedDAT). Specifically, our approach\nleverages a Dual-Adapter Teacher (DAT) to address data het-\nerogeneity by regularizing the client local updates and apply-\ning Mutual Knowledge Distillation (MKD) for an efficient\nknowledge transfer. FedDAT is the first approach that enables\nan efficient distributed finetuning of foundation models for a\nvariety of heterogeneous Vision-Language tasks. To demon-\nstrate its effectiveness, we conduct extensive experiments on\nfour multi-modality FL benchmarks with different types of\ndata heterogeneity, where FedDAT substantially outperforms\nthe existing centralized PEFT methods adapted for FL.\nIntroduction\nRecent works have shown the power of foundation mod-\nels with millions (billions) of parameters (Zhou et al. 2023;\nDu et al. 2022). These models, represented by Transfomers\n(Vaswani et al. 2017), achieve promising results when fine-\ntuned for real-world multi-modal tasks, including Visual\nQuestion Answering (VQA) (Antol et al. 2015), Visual\nCommonsense Reasoning (VCR) (Zellers et al. 2019), etc.\nTo improve the generalization ability of the foundation\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Schematic illustration of the training procedure for\nVisual Question Answering (VQA) in Federated Learning.\nmodels, a substantial amount of data from diverse sectors\nand application scenarios is typically required for extensive\nfinetuning. However, it becomes challenging to aggregate\nall training data and perform centralized model finetuning.\nFor instance, collecting data from different clinical centers\nacross multiple countries becomes infeasible due to distinct\nprivacy regulations, such as GDPR in the EU and PDPA in\nSingapore.\nTo address this problem, Federated Learning (FL)\nemerges as a promising solution, which allows a shared\nmodel to be collaboratively optimized using decentralized\ndata sources. In the classical FL approaches, e.g., FedAvg\n(McMahan et al. 2017), the central server obtains the model\nby iteratively averaging the optimized model weights up-\nloaded from the active clients. FL offers several advantages,\nincluding improved efficiency in client-server communica-\ntion and enhanced data confidentiality, as it eliminates the\nneed for direct access to the client’s local dataset. FL pro-\nvides promising solutions for various application areas, such\nas healthcare (Sheller et al. 2020) and industry (Liu et al.\n2020), where data privacy is crucial.\nDespite its promising prospects, traditional FL is unsuit-\nable for finetuning the entire foundation model. The opti-\nmization and transmission of billions of parameters would\nimpose significant client computation burdens and substan-\ntial communication overheads. To overcome this challenge,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11285\nparameter-efficient finetuning (PEFT) methods provide a\npossible solution, where only a small fraction of the model\nparameters is optimized and communicated during FL.\nExisting works have predominantly explored a basic com-\nbination of centralized PEFT algorithms and FedAvg. For\ninstance, some approaches focus on training and communi-\ncating only the tiny adaptation modules (adapter) (Houlsby\net al. 2019; Su et al. 2022) or a small amount of trainable\ninput tokens (Guo et al. 2022; Guo, Guo, and Wang 2023).\nHowever, these investigations are limited to single modality\nscenarios, where only visual or textual tasks are considered.\nMost importantly, none of these works address the problem\nof data heterogeneity, in which the data of different clients\nare not independent and identically distributed ( non-IID).\nData heterogeneity may lead to model drifts during the client\nlocal update, as well as an unstable and sub-optimal con-\nvergence of the aggregated server model (Li et al. 2020a;\nMendieta et al. 2022). Therefore, in this paper, we pro-\npose Federated Dual-Adapter Teacher (FedDAT), as the first\nframework to address this challenging yet practical prob-\nlem, PEFT of foundation models for multi-modal (Vision-\nLanguage) heterogeneous FL.\nFedDAT incorporates a global adapter in the foundation\nmodel, which is optimized and transmitted during federated\ncommunications. FedDAT utilizes a Dual-Adapter Teacher\n(DAT) module, comprising two parallel adapters: one is a\ncopy of the global adapter, kept frozen, while the other is\nlocally optimized at each client. This configuration enables\nthe local adapter to capture client-specific knowledge, which\nserves to regularize the global adapter and address data het-\nerogeneity. Meanwhile, the frozen adapter preserves client-\nagnostic knowledge, thereby mitigating the catastrophic for-\ngetting of the global adapter during knowledge transfer. To\nprevent overfitting of DAT to the limited client local dataset,\nwe implement Mutual Knowledge Distillation (MKD) be-\ntween DAT and the global adapter. This mechanism ensures\nefficient knowledge transfer while maintaining the general-\nization ability of both modules.\nThe proposed method FedDAT achieves state-of-the-art\nresults on four multi-modality benchmarks that include a va-\nriety of Vision-Language (VL) tasks with data heterogene-\nity. Our contributions can be summarized as follows:\n• We propose a novel method FedDAT for multi-modal\nheterogeneous FL, which is the first FL framework\naddressing distributed PEFT of foundation models for\nVision-Language tasks.\n• We conduct comprehensive experiments on four het-\nerogeneous FL benchmarks with a variety of Vision-\nLanguage tasks. The results demonstrate that FedDAT\nachieves SOTA results, indicating better convergence rate\nand scalability compared to existing PEFT methods.\nRelated Work\nParameter-Efficient Finetuning (PEFT) for Federated\nLearning. PEFT has been well studied in centralized ma-\nchine learning (Houlsby et al. 2019; Liu et al. 2022; Sung,\nCho, and Bansal 2022), while its application on FL re-\nmains under-explored. Most of the prior work rudimentarily\nadapted PEFT for FL and focused on single-modal tasks:\n(1) Image classification. (Chen et al. 2022; Sun et al.\n2022) evaluate the existing PEFT baselines combined with\nFL, while (Guo et al. 2022; Guo, Guo, and Wang 2023; Li\net al. 2023; Lu et al. 2023) finetune the CLIP model (Rad-\nford et al. 2021) via tuning and communicating only small\namount of learnable (personalized) prompts. (Su et al. 2022)\naddresses the problem of heterogeneous client images by in-\njecting lightweight adaptation modules (adapters) (Houlsby\net al. 2019). (Yang et al. 2023) explores the possibility of\nfinetuning generative foundation models (diffusion models)\n(Dhariwal and Nichol 2021) via FL.\n(2) Language tasks. (Yu, Mu˜noz, and Jannesari 2023) re-\nquires public server dataset and optimize adapter for few-\nshot finetuning of BERT-like language models (Devlin et al.\n2018). (Zhang et al. 2023) builds a distributed instruction\ntuning (Wei et al. 2021) datasets and finetunes the language\nmodel via Low-Rank Adaptation (LoRA) (Hu et al. 2021).\n(Zhuang, Chen, and Lyu 2023) systematically analyzes the\nchallenges of finetuning large language models in FL.\n(Yu et al. 2023) is the first to analyze the situation of\nhaving multi-modal client datasets and conducts contrastive\nrepresentation learning. However, the visual data and the\nlanguage data are processed by separate networks, i.e., no\nVision-Language Foundation Model is involved. In this\nwork, we focus on the under-explored PEFT for large-scale\nvision-language models in FL and address the problem of\nclient local datasets with heterogeneity in both vision and/or\nlanguage modality.\nVision-Language Foundation Model. Vision-Language\nfoundation models have significantly advanced the Vision-\nLanguage tasks (Antol et al. 2015; Zellers et al. 2019; Suhr\net al. 2019; Xie et al. 2019a). Based on the perspective of\nintra-modality data handling, there are two types of main-\nstream Vision-Language Foundation model structures: (1)\nSingle-stream Vision-Language Foundation models (Li et al.\n2019; Chen et al. 2020; Li et al. 2020b; Su et al. 2020;\nKim, Son, and Kim 2021a; Singh et al. 2022), which di-\nrectly fuse the initial language/visual representation by us-\ning the joint cross-modal encoder at the initial state, and (2)\nDual-stream Vision-Language foundation models (Lu et al.\n2019; Tan and Bansal 2019; Li et al. 2021b; Huo et al.\n2021), which separately apply the intra-modality processing\nto two modalities along with a shared cross-modal encoder.\nTo showcase the applicability of our proposed FedDAT to a\nwide range of Vision-Language foundation models, we care-\nfully select ViLT (Kim, Son, and Kim 2021a) as a repre-\nsentative single-stream Vision-Language foundation model,\nand ALBEF (Li et al. 2021b) as a representative dual-stream\nVision-Language foundation model. By employing these di-\nverse models, we effectively demonstrate the versatility and\nrobustness of FedDAT in Vision-Language learning.\nMethodology\nProblem Statement\nIn this work, we address a heterogeneous FL problem set-\nting with K clients: Each client k owns its private multi-\nmodal dataset Dk, containing data from visual modality (im-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11286\nages) and textual modality (texts). Specifically, we focus\non the vision-language tasks and take Visual Question An-\nswering (VQA) as an example. Hereby, the local datasetDk\ncan be further decomposed into Nk image-question-answer\ntriplets {(vk\ni , qk\ni , ak\ni )|i ∈ {1, ..., Nk}}. We assume that the\nmarginal distribution of vk\ni and/or qk\ni , ak\ni varies across the\nclients, i.e., there exists data heterogeneity in the visual\nspace and/or in the textual space. We define the answer pool\nAk = {ak\n1, ..., ak\nCk } with Ck ground-truth answers for client\nk and define our task as aCk-way classification problem fol-\nlowing (Antol et al. 2015). Note that the size of the answer\npool could be different for different clients. The objective\nof FL is to collaboratively finetune one global foundation\nmodel fθ in a parameter-efficient manner (PEFT) within a\npre-defined communication budget, which produces promis-\ning results on all client’s local data.\nPEFT Method: Adapter\nIn this section, we introduce a traditional parameter-efficient\nfinetuning (PEFT) method, i.e., Adapter (Houlsby et al.\n2019), adjusted for FL applications. Here, we adopt the\nfoundation models with common Transformer architecture\n(Vaswani et al. 2017) consisting of multiple repeated Trans-\nformer blocks. Specifically, each block contains a self-\nattention sub-layer, a fully connected feed-forward network\n(FFN), and residual connections around the sub-layers fol-\nlowed by layer normalization.\nAdapter is a bottleneck network consisting of a down-\nsample linear layer Wdown ∈ Rd×r and an up-sampling lin-\near layer Wup ∈ Rr×d, where r denotes the down-sampled\ndimension (r < d). A nonlinear activation function ϕ(·),\nsuch as ReLU, is inserted in between. The adapter is injected\nafter the FFN of each Transformer block and its computation\ncan be formulated as\nh′ = h + ϕ(hWdown)Wup, (1)\nwhere h is the normalized output of FFN.\nRecap: Federated Averaging\nIn this section, we formally describe the combination of the\nconventional federated learning algorithm, FedAvg (McMa-\nhan et al. 2017), and the centralized PEFT algorithm, i.e.,\nAdapter. Before the client-server communication starts, we\ndeploy the same pre-trained foundation model fθ at differ-\nent clients. Afterwards, the server randomly initializes the\nparameter w of the learnable lightweight module, which are\nMethod DomainNet\nC I P Q R S avg\nclf-L 72.43 36.13 86.35 55.70 74.07 74.70 66.56\nAdapter-L 76.05 36.93 88.03 72.40 66.53 78.74 69.78\nclf 80.80 44.61 83.47 60.10 84.21 71.69 70.81\nAdapter 88.59 50.95 87.12 76.00 84.99 74.08 76.96\nTable 1: Evaluation results of ViT finetuned for DomainNet\nwith/without FL. ”L” indicates independent client training,\ni.e., no federated communication involved.\nthe weight matrices of the linear layers Wdown and Wup in\nthe adapters. w is then distributed to all clients for commu-\nnication and local optimization. We illustrate the procedure\nof one communication round in the following.\nAs shown in Figure 1, each active client k first execute\nlocal training to optimize the light-wegiht module wk com-\nbined with the frozen foundation model fθ (➀) in parallel,\nwhere the following loss Lk is minimized:\nLk(wk) = 1\nNk\nNkX\ni=1\nL(yi, fθ∪wk (xi)), (2)\nwhere yi is the ground-truth label of input data xi, and\nL is the loss function, e.g., Cross-Entropy for classification\ntasks. After the local updates, the central server aggregates\n{wk|1 ≤ k ≤ K}, uploaded (➁) by all active clients, and\nexecutes a parameter aggregation (➂):\nˆw ← 1\nPK\nk=1 Nk\nKX\nk=1\nNk · wk. (3)\nFinally, the aggregated weight ˆw will be distributed (➃)\nto the active clients for optimization in the next communi-\ncation round. Note that after exhausting all communication\nbudgets, the global model fθ∪w is deployed for the testing.\nMotivational Case Study\nTo motivate the architecture design of FedDAT, we present\nan empirical analysis to address the following research ques-\ntion: Which type of knowledge is more crucial for optimizing\na promising ML model in heterogeneous FL, client-specific\nor client-agnostic? Therefore, we follow the experiment de-\nsign proposed in (Tan et al. 2022). Specifically, we take the\ndown-sampled version of DomainNet (Peng et al. 2019),\nwhich is an image classification benchmark and contains\ndata from 6 different styles: Clipart (C), Infograph (I), Paint-\ning (P), Quickdraw (Q), Real (R), and Sketch (S). By assign-\ning data from one style to each client, we simulate data het-\nerogeneity in the feature space across different clients. We\nfinetune the foundation model, i.e., ViT (Dosovitskiy et al.\n2020), with different PEFT methods via FL.\nIn Table 1, we provide the results of finetuning the clas-\nsification head (clf ) and finetuning with Adapter. We also\ndisplay the performance of client local finetuning (L), i.e.,\nno federated communication involved. We conclude three\nobservations from the results: (1) Adapter is an effective\nPEFT method in both federated setting and independent\nfinetuning setting compared with clf, providing an aver-\nage performance increase of3.22% and 6.15%, respectively.\n(2) Collaborative training via FL, i.e., finetuning a client-\nagnostic foundation model, generally outperforms local in-\ndependent finetuning. This can be observed by comparing\nthe average accuracy of models with and without ”L”. (3)\nClient-specific classification head and adapters show bene-\nfits on certain clients (marked with underlines), i.e., clients\nwith Painting (P) and Sketch (S) data and optimized inde-\npendently. We assume this is due to the large distribution\nshift in the feature space across different clients’ local data,\ngiven their different image appearances. This phenomenon\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11287\n(a) DAT Architecture Design\n (b) Communication\nFigure 2: Schematic illustration of the Dual-Adapter Teacher\n(DAT) with local Ac and frozen ˆAs. Only the shared adapter\nAs is transmitted during federated communication.\nanswers the previous research question: Both client-specific\nand client-agnostic knowledge are crucial and should not be\nforgotten during federated communication. These observa-\ntions motivate the proposed method and serve as evidence\nfor its promising applicability and effectiveness.\nProposed Method\nIn this section, we introduce the proposed method Federated\nDual-Adapter Teacher (FedDAT). As shown in Algorithm\n1, the training process of FedDAT can be divided into two\nfunctions, which will be introduced in the following:\nAt the beginning of the training, the server initializes a\nshared adapter As. In each communication round, all active\nclients receive As and conduct Client Update in parallel.\nSubsequently, the server aggregates and averages the opti-\nmized parameters {Ak\ns|1 ≤ k ≤ K} uploaded from all\nclients, which will be used as the initialization of As for the\nnext communication round.\nThe client local update comprises 2 main components,\nwhich will be introduced in the following:\n(1) Dual-Adapter Teacher (DAT). Before the first com-\nmunication round, each client locally initializes the local\nadapter Ac as well as the foundation modelfθ with the same\npre-trained weights θ. Subsequently, each client receives the\nparameters of As from the server, which is then copied asˆAs\nand kept frozen during the client local update. We combine\nˆAs and Ac as the Dual-Adapter Teacher (DAT) and provide\nits schematic illustration in Figure 2a.\nIn DAT, we constrain the parameters of Ac strictly lo-\ncal for each client. By personalizing Ac, we force it to fo-\ncus solely on client-specific knowledge, which is crucial for\nclient data heterogeneity. Meanwhile, the frozen ˆAs is uti-\nlized to retain the client-agnostic knowledge captured by the\nshared adapter As. Similar to traditional adapters (Equation\n1), given the normalized output of FFN h in a Transformer\nlayer, DAT performs the following transformation:\nh′ ← h + 1\n2ϕ(h · ˆWdown\ns ) · ˆWup\ns + 1\n2ϕ(h · Wdown\nc ) · Wup\nc , (4)\nwhere ˆWs and Wc are the weight matrices for ˆAs and Ac,\nrespectively. Afterwards, T local update steps will be exe-\n(a) Optimization of As\n (b) Optimization of DAT\nFigure 3: Schematic illustration of the Mutual Knowledge\nDistillation (MKD) between DAT and As.\ncuted, in which the shared adapter As and the DAT module\nis optimized.\nBy utilizing DAT as a guidance for the local optimiza-\ntion of As at each client, our goal is to distill client-specific\nknowledge into As and mitigate the forgetting of As on\nits client-agnostic knowledge. Hereby, we apply Mutual\nKnowledge Distillation (MKD) for an efficient knowledge\ntransfer, which will be introduced in the following.\n(2) Mutual Knowledge Distillation (MKD).A schematic\nillustration of MKD is provided in Figure 3. MKD executes\nbi-directional knowledge distillation between As and DAT\nvia Ls\nKL and LDAT\nKL , respectively:\nLs\nKL = KL(zs(x)||zDAT(x)), L DAT\nKL = KL(zDAT(x)||zs(x)),\n(5)\nwhere KL denotes the Kullback-Leibler divergence, zs\nand zDAT are the predicted logits of the foundation model\ninjected with As and DAT, respectively. Hereby, this setup\nallows the shared adapter As to capture both client-specific\nknowledge and client-agnostic stored in DAT (Ls\nKL). Addi-\ntionally, we apply As as guidance for the optimization DAT\n(LDAT\nKL ) to prevent possible overfitting, considering the scarce\nlocal data of each client (McMahan et al. 2017).\nMKD is utilized together with the guidance from ground-\ntruth labels of the training data, i.e.,\nLs\nCE =\nCX\nc=1\nI(x, c) · log(σ(zs(x))(c)),\nLDAT\nCE =\nCX\nc=1\nI(x, c) · log(σ(zDAT(x))(c)),\n(6)\nwhere, I(x, c) is a binary indicator (0 or 1) if c is the\nground-truth label for x, σ is the softmax function. Hereby,\nwe aim at training the foundation model, injected with ei-\nther As or DAT, to correctly classify the training sample x.\nFinally, combining MKD and LCE produces the optimiza-\ntion objective for As and DAT:\nLs =Ls\nCE + αLs\nKL,\nLDAT =LDAT\nCE + βLDAT\nKL ,\n(7)\nwhere, α and β are the weighting coefficient. While both\nDAT and As are randomly initialized, they become more in-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11288\nformative as the training progresses. To reflect this observa-\ntion, we apply an exponential ramp-up schedule for α and\nβ. Despite the sophisticated design of our method, FedDAT\nindicates the same inference cost and communication over-\nhead as the PEFT method Adapter, where only As is trans-\nmitted and applied at deployment.\nExperiments and Analyses\nWe conduct extensive empirical analyses to investigate the\nproposed method. Firstly, we compare FedDAT with other\ncentralized PEFT methods on four heterogeneous FL bench-\nmarks containing different Vision-Language tasks. After-\nwards, we demonstrate the effectiveness ofFedDAT compo-\nnents via ablation study. Finally, we analyze the promising\nconvergence rate and scalability of FedDAT.\nBenchmark Experiments\nDatasets Description. We conduct experiments on differ-\nent Vision-Language (VL) benchmarks with different types\nof data heterogeneity, including visual, textual, and task het-\nerogeneity. We introduce these benchmarks in the following.\n• Domain. We adopt 5 common VQA datasets from differ-\nent domains, i.e., VizWiz (Gurari et al. 2018), COCO QA\n(Ren, Kiros, and Zemel 2015), Art (Garcia et al. 2020),\nGQA (Hudson and Manning 2019) and Abstract (An-\ntol et al. 2015). We assign one of the datasets to each\nclient, leading to heterogeneity in both vision and lan-\nguage modality. Example VQA triplets from the bench-\nmark are provided in Figure 4.\n• Function & Scene. We adopt and split the CLOVE\nbenchmark (Lei et al. 2023) into Scene and Function\nbenchmark, which contains VQA triplets collected from\n6 different visual environments and 5 different functions,\nrespectively. Triplets from one scene (function) are allo-\ncated to one client, resulting in visual (textual) hetero-\ngeneity in the Scene (Function) benchmark.\n• Task. We adopt and modify the CLiMB benchmark\n(Srinivasan et al. 2022), which contains 4 VL tasks,\nnamely VQA (Antol et al. 2015), Natural Language for\nVisual Reasoning (NLVR) (Suhr et al. 2018), Visual En-\ntailment (VE ) (Xie et al. 2019b), and Visual Common-\nsense Reasoning (VCR) (Zellers et al. 2019). Each client\nowns data from one of the datasets, introducing task het-\nerogeneity across different clients.\nWe downsample the original dataset to simulate client lo-\ncal data scarcity described in prior arts (McMahan et al.\n2017) and provide more details in the Appendix.\nImplementation Details. For the task-heterogeneous\nbenchmark (Task), we adopt the Transformer encoder-only\nbackbones following (Srinivasan et al. 2022), i.e., ViLT\n(Kim, Son, and Kim 2021b) and V AuLT (Chochlakis et al.\n2022). For the rest three benchmarks, we add another\nencoder-decoder backbone, i.e., ALBEF (Li et al. 2021a).\nWe compare FedDAT with various centralized PEFT meth-\nods adapted for FL, including LoRA (Hu et al. 2021),\nprompt-tuning (Guo et al. 2022), andbias-tuning (Cai et al.\nAlgorithm 1: Training procedure of FedDAT\nServerUpdate\n1: Randomly initialize As\n2: for round r = 1 to R do\n3: for client k = 1 to K do {in parallel}\n4: Ak\ns ← ClientUpdate(As, k, r)\n5: end for\n6: As ← 1\nK\nPK\nk=1 Ak\ns\n7: end for\nClientUpdate(As, k, r)\n1: if r = 1then\n2: Randomly initialize Ac\n3: end if\n4: ˆAs ← As\n5: for local step t = 1 to T do\n6: Sample {X, y} from Dk\n7: Optimize As via minimizing Ls\n8: Optimize DAT via minimizing LDAT\n9: end for\n10: return As\n2020). We also provide results of independent client opti-\nmization (marked by ”L”) of the classification head clf and\nAdapter. Moreover, we provide the results of fully finetun-\ning the models (full ) as an oracle method (marked by ∗),\ngiven the infeasibility of transmitting the entire foundation\nmodel in FL.\nTo handle the different answer pools in different clients,\nwe incorporate client-specific classification heads for ViLT\nand V AuLT, and apply client-specific answer lists for AL-\nBEF. To make a fair comparison between different central-\nized PEFT algorithms and FedDAT, we apply the same\nhyperparameters search for all methods in different bench-\nmarks. All experiments are repeated with 3 random seeds.\nThe hyperparameters are detailed in the Appendix.\nResults and Analyses. In Table 2, we provide the re-\nsults of FedDAT and the other FL-adapted PEFT methods\non our Domain benchmark. We observe FedDAT outper-\nforms all the baselines with all the architectures, achieving\nan average performance improvement of up to 4.55% com-\npared with the most promising baseline Adapter. This in-\ndicates the easy adaptability of FedDAT for both encoder-\nbased and encoder-decoder-based VL models. Moreover,\nFedDAT depicts the same communication overhead as a\nsingle Adapter, which adds and optimizes only less than\nFigure 4: Example VQA triplets of different datasets in Do-\nmain benchmark with heterogeneity in both Vision and Lan-\nguage modality.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11289\nBackbone Method Comm. Overhead VizWiz COCO Art GQA Abstract Average\nViLT\nclf-L − 63.13±1.07 36.15±2.92 63.22±0.99 34.90±3.16 52.81±2.67 50.04±1.81\nLoRA 0.60M(0.48%) 60.47±1.25 43.28±1.37 62.98±0.75 36.57±2.01 52.04±1.62 51.07±1.41\nprompt 0.60M(0.48%) 60.13±1.05 52.13±0.87 63.02±1.58 39.09±0.37 52.88±3.07 53.45±2.04\nbias 0.10M(0.08%) 61.83±2.41 49.41±2.36 69.38±1.69 40.43±0.66 60.36±1.92 56.28±1.97\nAdapter-L − 61.72±1.42 46.27±4.58 67.69±0.42 43.62±0.93 54.02±2.16 54.67±2.54\nAdapter 0.89M(0.75%) 61.39±1.11 52.39±6.20 68.72±3.20 43.72±0.65 59.43±2.94 57.13±4.08\nFedDAT 0.89M(0.75%) 60.99±2.81 63.81±2.90 71.36±3.34 48.65±2.93 60.75±2.67 61.11±2.98\nfull-L∗ − 55.52±1.42 72.97±1.53 73.16±0.28 44.41±3.98 58.78±0.25 60.97±1.45\nfull∗ 87.40M(100%) 56.12±2.55 73.87±0.83 76.24±1.82 50.28±1.59 61.26±0.78 63.55±1.35\nV AuLT\nclf-L − 61.83±1.85 32.42±0.04 64.52±1.55 35.08±5.57 48.48±0.77 48.46±1.15\nLoRA 0.60M(0.29%) 62.17±1.32 40.56±0.86 63.08±1.13 33.47±3.08 47.34±1.04 49.32±1.16\nprompt 0.60M(0.29%) 62.93±0.87 46.52±1.45 64.26±1.03 35.33±2.12 48.91±0.68 51.59±1.63\nbias 0.21M(0.10%) 61.12±2.84 43.81±0.35 67.00±1.41 33.30±4.81 51.22±2.07 51.29±1.08\nAdapter-L − 62.33±1.42 47.72±2.83 67.50±2.11 33.75±2.79 54.09±0.93 53.07±1.34\nAdapter 1.79M(0.77%) 52.53±3.65 53.63±0.28 66.80±0.53 35.65±1.84 50.03±1.77 51.73±0.46\nFedDAT 1.79M(0.77%) 62.19±1.01 54.83±2.04 67.86±1.93 40.06±3.08 54.48±0.49 55.88±1.79\nfull-L∗ − 57.41±2.13 55.68±1.24 70.27±2.11 41.31±1.46 52.66±0.57 55.47±1.85\nfull∗ 227.77M(100%) 45.79±2.12 64.64±3.05 67.89±1.82 41.93±3.85 49.58±0.66 53.97±2.09\nALBEF\nLoRA 1.52M(0.53%) 60.49±1.32 28.32±0.65 57.04±3.69 28.71±0.42 58.06±2.42 46.52±1.75\nprompt 0.92M(0.32%) 63.13±0.65 32.50±1.20 63.45±0.42 32.08±1.07 59.45±1.78 50.12±0.95\nbias 0.93M(0.32%) 63.23±0.14 31.23±0.28 61.23±1.12 35.93±1.73 57.88±0.28 49.90±0.87\nAdapter-L − 61.72±1.12 56.32±1.50 65.21±0.35 40.96±2.27 59.51±1.58 56.74±1.38\nAdapter 2.86M(0.98%) 59.52±2.44 69.35±2.78 68.32±0.89 41.02±3.12 60.83±2.66 59.81±1.87\nFedDAT 2.86M(0.98%) 61.52±1.51 76.36±0.63 71.04±0.50 49.22±1.60 63.65±1.19 64.36±1.39\nfull-L∗ − 61.22±0.14 77.80±1.39 74.45±0.7 50.09±1.06 63.58±2.79 65.43±1.37\nfull∗ 290.34M(100%) 51.91±1.42 78.38±1.11 75.65±0.14 55.91±0.54 70.47±0.83 66.46±0.96\nTable 2: Evaluation results of different finetuning methods on our FL benchmark with distribution shift in both Vision and\nLanguage space. ”L” indicates client local finetuning where no communication is involved. We report the mean±std accuracy of\neach client from 3 runs with different seeds.\n1% of the total parameters in the foundation model. This\nfurther illustrates its applicability to the FL system with\nconstrained communication bandwidths. Besides, FedDAT\nnarrows the performance gap between the PEFT methods\nand fully-finetuning methods. Interestingly, our approach\noutperforms the oracle methods full -L when applied on\nViLT and V AuLT, which demonstrates the effectiveness of\nintroducing client-specific knowledge into the client local\noptimization. We also note that applying Adapter-L for\nV AuLT, i.e., optimizing adapters for each client indepen-\ndently, achieves better results thanAdapter, which provides\nadditional evidence for our observation in Section .\nAfterwards, we provide the comparison of clients’ aver-\nage accuracy between FedDAT and different PEFT methods\non the other benchmarks. As shown in Table 3, FedDAT\nprovides promising improvements of up to 6.02%, 7.94%\nand 1.09% on Function, Scene, and Task benchmark, re-\nspectively. More details of the client specific performance\nare provided in the Appendix.\nAblation Study\nTo illustrate the importance of different components used in\nFedDAT, we conduct an ablation study for ViLT on three\nbenchmarks. The results are shown in Table 4. We first in-\nvestigate the optimization process, where we notice that op-\ntimizing without DAT, i.e., applying solely the local adapter\nAc or the frozen adapter ˆAs as the teacher, leads to only\nminimal performance increase, which indicates the effec-\ntiveness of our Dual-Adapter Teacher design. Besides, dis-\ntilling only the knowledge from DAT to the shared adapter\nAs, i.e., omitting the bi-directional MKD, brings visible per-\nformance gain. Combining both strategies achieves the best\nresults, which further demonstrates their complementarity.\nAdditionally, we validate other inference choices. Specifi-\ncally, we evaluate the final DAT module (combination of\nAc and ˆAs) and the local adapter Ac at each client. We\nagain note that we are addressing the problem of finetuning a\nglobal foundation model via FL, where no further personal-\nization is required. Considering the inference efficiency and\nthe problem setting, we adopt the shared adapter As for in-\nference, which also achieves the most promising results.\nConvergence Analysis\nIn Figure 5, we display the convergence analysis ofFedDAT\ncompared with the most promising PEFT method Adapter\non Domain benchmark. Hereby, we report the accuracy of\nthe clients on their corresponding local testing set after each\ncommunication round. As shown in the figure, even though\nFedDAT utilizes a more sophisticated optimization schema,\ni.e., a combination of DAT and MKD, the learning curves\nof FedDAT still exhibit faster convergence rates than sin-\ngle Adapter. It is also worth noticing that FedDAT already\nachieves distinct performance gain after 5 communication\nrounds, i.e., 25% of the total communication budgets.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11290\nFigure 5: Convergence analysis of ViLT model on different clients inDomain benchmark.\nBackbone Method Function Scene Task\nViLT\nclf-L 31.58±1.97 24.52±0.95 49.46±0.39\nLoRA 32.04±1.12 28.47±1.03 47.82±1.42\nprompt 40.53±1.56 30.53±1.30 49.55±1.14\nbias 43.81±1.39 33.65±1.87 50.71±1.26\nAdapter-L 39.68±2.19 31.91±2.05 49.59±1.74\nAdapter 48.37±1.56 31.07±1.08 51.44±1.34\nFedDAT 54.39±2.36 39.35±1.25 52.37±0.52\nfull-L∗ 56.81±2.97 38.00±1.48 50.64±1.42\nfull∗ 59.62±2.56 40.62±3.76 53.17±0.69\nV AuLT\nclf-L 27.72±3.05 21.22±2.08 39.63±1.07\nLoRA 29.87±1.86 23.08±1.09 38.35±1.47\nprompt 36.32±2.07 25.63±1.54 38.75±1.34\nbias 36.11±3.05 24.89±2.17 39.46±0.99\nAdapter-L 37.22±2.38 28.57±1.98 40.42±1.21\nAdapter 41.50±3.24 29.39±2.65 40.19±0.89\nFedDAT 44.54±2.08 34.31±2.87 41.28±0.57\nfull-L∗ 49.13±2.68 35.11±1.99 41.66±1.32\nfull∗ 46.38±1.57 36.72±2.57 42.44±0.71\nTable 3: Evaluation results of different methods on Func-\ntion, Scene, and Task benchmark. ”L” indicates independent\nclient finetuning. We report the mean±stdaccuracy of 3 trials.\nScalability Analysis of FedDAT\nTo show the effectiveness ofFedDAT under various applica-\ntion scenarios, we further conduct experiments with differ-\nent numbers of clients. More specifically, we split the data\nof each function in the original CLOVE dataset (Lei et al.\n2023) into 5 subsets, where each subset has an equal number\nof training data and is assigned to one client, following the\nclient data scarcity described in (McMahan et al. 2017). We\nconduct experiments where 1, 2, 3, 4, and 5 clients (subsets)\nfrom each function are selected, which gives in total 5, 10,\n15, 20, and 25 clients joining the federated communication\nfor the Function benchmark, respectively. We apply also the\nsame split strategy for the 6 different visual environments\nfor the Scene benchmark and conduct the same experiment.\nMore details regarding the experimental setups are provided\nin Appendix.\nWe observe that FedDAT consistently outperforms\nAdapter across all setups with small or large quantities of\ntraining data. Notably, a performance gap of up to 10% for\nALBEF and 6% for ViLT is evident. These results indicate\nthe scalability of FedDAT in handling complex FL appli-\ncations involving a larger number of clients and increased\ncommunication budgets.\nFigure 6: Scalability analysis of FedDAT with different\nnumber of clients on Funciton and Scene benchmarks.\nConclusion\nIn this work, we propose the first FL framework to ad-\ndress the parameter-efficient finetuning (PEFT) of the foun-\ndation model in heterogeneous FL, where various Vision-\nLanguage tasks are investigated. The proposed method,\nnamed FedDAT, optimizes a shared adapter utilizing the\nDual-Adapter Teacher (DAT ) and Mutual Knowledge Dis-\ntillation (MKD). Compared with existing centralized PEFT\nmethods, FedDAT achieves promising results on the four FL\nbenchmarks with various Vision-Language tasks, demon-\nstrating its effectiveness. Additional experiments indicate\nits applicability to complex FL setups involving larger dis-\ntributed systems and training budgets.\nStage Method Domain Function Scene\n- Adapter 57.13±4.08 48.37±1.56 31.07±1.08\nOptimization\nw/o ˆAs 58.24±0.98 50.62±1.45 33.04±0.65\nw/o Ac 57.87±1.24 50.93±0.85 32.45±0.27\nw/o MKD 58.41±1.57 52.82±2.98 36.98±1.07\nFedDAT 61.11±2.98 54.39±2.36 39.35±1.25\nInference\nAc + As 58.45±1.57 50.42±1.87 35.61±2.41\nAc 55.87±3.35 46.14±2.60 32.84±0.78\nAs (FedDAT) 61.11±2.98 54.39±2.36 39.35±1.25\nTable 4: Ablation study for different components in opti-\nmization and inference stage of FedDAT on three bench-\nmark datasets.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11291\nReferences\n[3] Shen, Y .; and et al. 2022. Cd2-pfed: Cyclic distillation-guided\nchannel decoupling for model personalization in federated learn-\ning. In CVPR.\nAntol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.; Zitnick,\nC. L.; and Parikh, D. 2015. VQA: Visual Question Answering. In\nInternational Conference on Computer Vision (ICCV).\nCai, H.; Gan, C.; Zhu, L.; and Han, S. 2020. Tinytl: Reduce mem-\nory, not parameters for efficient on-device learning. Advances in\nNeural Information Processing Systems, 33: 11285–11297.\nChen, J.; Xu, W.; Guo, S.; Wang, J.; Zhang, J.; and Wang, H. 2022.\nFedTune: A Deep Dive into Efficient Federated Fine-Tuning with\nPre-trained Transformers. arXiv preprint arXiv:2211.08025.\nChen, S.; Gu, J.; Han, Z.; Ma, Y .; Torr, P.; and Tresp, V . 2023.\nBenchmarking Robustness of Adaptation Methods on Pre-trained\nVision-Language Models. arXiv preprint arXiv:2306.02080.\nChen, Y .; Li, L.; Yu, L.; Kholy, A. E.; Ahmed, F.; Gan, Z.; Cheng,\nY .; and Liu, J. 2020. UNITER: UNiversal Image-TExt Representa-\ntion Learning. In Vedaldi, A.; Bischof, H.; Brox, T.; and Frahm, J.,\neds., Computer Vision - ECCV 2020 - 16th European Conference,\nGlasgow, UK, August 23-28, 2020, Proceedings, Part XXX, volume\n12375 of Lecture Notes in Computer Science, 104–120. Springer.\nChochlakis, G.; Srinivasan, T.; Thomason, J.; and Narayanan, S.\n2022. Vault: Augmenting the vision-and-language transformer\nwith the propagation of deep language representations. arXiv\npreprint arXiv:2208.09021.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert:\nPre-training of deep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nDhariwal, P.; and Nichol, A. 2021. Diffusion models beat gans\non image synthesis. Advances in neural information processing\nsystems, 34: 8780–8794.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.;\nZhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold,\nG.; Gelly, S.; et al. 2020. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929.\nDu, Y .; Liu, Z.; Li, J.; and Zhao, W. X. 2022. A survey of vision-\nlanguage pre-trained models. arXiv preprint arXiv:2202.10936.\nGarcia, N.; Ye, C.; Liu, Z.; Hu, Q.; Otani, M.; Chu, C.; Nakashima,\nY .; and Mitamura, T. 2020. A Dataset and Baselines for Visual\nQuestion Answering on Art. In Proceedings of the European Con-\nference in Computer Vision Workshops.\nGuo, T.; Guo, S.; and Wang, J. 2023. pFedPrompt: Learning Per-\nsonalized Prompt for Vision-Language Models in Federated Learn-\ning. In Proceedings of the ACM Web Conference 2023, 1364–1374.\nGuo, T.; Guo, S.; Wang, J.; and Xu, W. 2022. PromptFL: Let Feder-\nated Participants Cooperatively Learn Prompts Instead of Models–\nFederated Learning in Age of Foundation Model. arXiv preprint\narXiv:2208.11625.\nGurari, D.; Li, Q.; Stangl, A. J.; Guo, A.; Lin, C.; Grauman, K.;\nLuo, J.; and Bigham, J. P. 2018. VizWiz Grand Challenge: An-\nswering Visual Questions from Blind People. CVPR.\nHoulsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.; De Larous-\nsilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly, S. 2019.\nParameter-efficient transfer learning for NLP. In International\nConference on Machine Learning, 2790–2799. PMLR.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang, S.;\nWang, L.; and Chen, W. 2021. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685.\nHudson, D. A.; and Manning, C. D. 2019. Gqa: A new dataset for\nreal-world visual reasoning and compositional question answering.\nIn Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 6700–6709.\nHuo, Y .; Zhang, M.; Liu, G.; Lu, H.; Gao, Y .; Yang, G.; Wen, J.;\nZhang, H.; Xu, B.; Zheng, W.; et al. 2021. WenLan: Bridging vi-\nsion and language by large-scale multi-modal pre-training. arXiv\npreprint arXiv:2103.06561.\nKim, W.; Son, B.; and Kim, I. 2021a. ViLT: Vision-and-Language\nTransformer Without Convolution or Region Supervision. In\nMeila, M.; and Zhang, T., eds., Proceedings of the 38th Interna-\ntional Conference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, volume 139 ofProceedings of Machine Learn-\ning Research, 5583–5594. PMLR.\nKim, W.; Son, B.; and Kim, I. 2021b. Vilt: Vision-and-language\ntransformer without convolution or region supervision. In Interna-\ntional Conference on Machine Learning, 5583–5594. PMLR.\nLe, H. Q.; Nguyen, M. N.; Thwal, C. M.; Qiao, Y .; Zhang, C.;\nand Hong, C. S. 2023. FedMEKT: Distillation-based Embedding\nKnowledge Transfer for Multimodal Federated Learning. arXiv\npreprint arXiv:2307.13214.\nLei, S. W.; Gao, D.; Wu, J. Z.; Wang, Y .; Liu, W.; Zhang, M.; and\nShou, M. Z. 2023. Symbolic replay: Scene graph as prompt for\ncontinual learning on vqa task. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 37, 1250–1259.\nLi, G.; Wu, W.; Sun, Y .; Shen, L.; Wu, B.; and Tao, D. 2023. Visual\nPrompt Based Personalized Federated Learning. arXiv preprint\narXiv:2303.08678.\nLi, J.; Selvaraju, R.; Gotmare, A.; Joty, S.; Xiong, C.; and Hoi, S.\nC. H. 2021a. Align before fuse: Vision and language representation\nlearning with momentum distillation. Advances in neural informa-\ntion processing systems, 34: 9694–9705.\nLi, J.; Selvaraju, R. R.; Gotmare, A.; Joty, S. R.; Xiong, C.; and\nHoi, S. C. 2021b. Align before Fuse: Vision and Language Repre-\nsentation Learning with Momentum Distillation. In Ranzato, M.;\nBeygelzimer, A.; Dauphin, Y . N.; Liang, P.; and Vaughan, J. W.,\neds., Advances in Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, 9694–9705.\nLi, L. H.; Yatskar, M.; Yin, D.; Hsieh, C.; and Chang, K. 2019.\nVisualBERT: A Simple and Performant Baseline for Vision and\nLanguage. CoRR, abs/1908.03557.\nLi, T.; Sahu, A. K.; Talwalkar, A.; and Smith, V . 2020a. Federated\nlearning: Challenges, methods, and future directions. IEEE Signal\nProcessing Magazine, 37(3): 50–60.\nLi, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.; Wang, L.; Hu,\nH.; Dong, L.; Wei, F.; Choi, Y .; and Gao, J. 2020b. Oscar: Object-\nSemantics Aligned Pre-training for Vision-Language Tasks. In\nVedaldi, A.; Bischof, H.; Brox, T.; and Frahm, J., eds., Computer\nVision - ECCV 2020 - 16th European Conference, Glasgow, UK,\nAugust 23-28, 2020, Proceedings, Part XXX, volume 12375 ofLec-\nture Notes in Computer Science, 121–137. Springer.\nLiu, H.; Tam, D.; Muqeeth, M.; Mohta, J.; Huang, T.; Bansal, M.;\nand Raffel, C. A. 2022. Few-shot parameter-efficient fine-tuning\nis better and cheaper than in-context learning. Advances in Neural\nInformation Processing Systems, 35: 1950–1965.\nLiu, Y .; Garg, S.; Nie, J.; Zhang, Y .; Xiong, Z.; Kang, J.; and Hos-\nsain, M. S. 2020. Deep anomaly detection for time-series data\nin industrial IoT: A communication-efficient on-device federated\nlearning approach. IEEE Internet of Things Journal, 8(8): 6348–\n6358.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11292\nLu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. ViLBERT: Pretrain-\ning Task-Agnostic Visiolinguistic Representations for Vision-and-\nLanguage Tasks. In Wallach, H. M.; Larochelle, H.; Beygelzimer,\nA.; d’Alch ´e-Buc, F.; Fox, E. B.; and Garnett, R., eds., Advances\nin Neural Information Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, 13–23.\nLu, W.; Hu, X.; Wang, J.; and Xie, X. 2023. FedCLIP: Fast Gen-\neralization and Personalization for CLIP in Federated Learning.\narXiv preprint arXiv:2302.13485.\nMcMahan, B.; Moore, E.; Ramage, D.; Hampson, S.; and y Arcas,\nB. A. 2017. Communication-efficient learning of deep networks\nfrom decentralized data. In Artificial intelligence and statistics,\n1273–1282. PMLR.\nMendieta, M.; Yang, T.; Wang, P.; Lee, M.; Ding, Z.; and Chen,\nC. 2022. Local learning matters: Rethinking data heterogeneity in\nfederated learning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 8397–8406.\nPeng, X.; Bai, Q.; Xia, X.; Huang, Z.; Saenko, K.; and Wang, B.\n2019. Moment matching for multi-source domain adaptation. In\nProceedings of the IEEE International Conference on Computer\nVision, 1406–1415.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agar-\nwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021.\nLearning transferable visual models from natural language supervi-\nsion. In International conference on machine learning, 8748–8763.\nPMLR.\nRen, M.; Kiros, R.; and Zemel, R. 2015. Exploring models and\ndata for image question answering.Advances in neural information\nprocessing systems, 28.\nSheller, M. J.; Edwards, B.; Reina, G. A.; Martin, J.; Pati, S.;\nKotrotsou, A.; Milchenko, M.; Xu, W.; Marcus, D.; Colen, R. R.;\net al. 2020. Federated learning in medicine: facilitating multi-\ninstitutional collaborations without sharing patient data. Scientific\nreports, 10(1): 12598.\nSingh, A.; Hu, R.; Goswami, V .; Couairon, G.; Galuba, W.;\nRohrbach, M.; and Kiela, D. 2022. FLA V A: A Foundational Lan-\nguage And Vision Alignment Model. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2022, New Or-\nleans, LA, USA, June 18-24, 2022, 15617–15629. IEEE.\nSrinivasan, T.; Chang, T.-Y .; Pinto Alva, L.; Chochlakis, G.; Ros-\ntami, M.; and Thomason, J. 2022. Climb: A continual learning\nbenchmark for vision-and-language tasks. Advances in Neural In-\nformation Processing Systems, 35: 29440–29453.\nSu, S.; Yang, M.; Li, B.; and Xue, X. 2022. Cross-domain\nFederated Adaptive Prompt Tuning for CLIP. arXiv preprint\narXiv:2211.07864.\nSu, W.; Zhu, X.; Cao, Y .; Li, B.; Lu, L.; Wei, F.; and Dai, J. 2020.\nVL-BERT: Pre-training of Generic Visual-Linguistic Representa-\ntions. In 8th International Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. Open-\nReview.net.\nSuhr, A.; Zhou, S.; Zhang, A.; Zhang, I.; Bai, H.; and Artzi, Y .\n2018. A corpus for reasoning about natural language grounded in\nphotographs. arXiv preprint arXiv:1811.00491.\nSuhr, A.; Zhou, S.; Zhang, A.; Zhang, I.; Bai, H.; and Artzi, Y .\n2019. A Corpus for Reasoning about Natural Language Grounded\nin Photographs. In Korhonen, A.; Traum, D. R.; and M `arquez,\nL., eds., Proceedings of the 57th Conference of the Association for\nComputational Linguistics, ACL 2019, Florence, Italy, July 28- Au-\ngust 2, 2019, Volume 1: Long Papers, 6418–6428. Association for\nComputational Linguistics.\nSun, G.; Mendieta, M.; Yang, T.; and Chen, C. 2022. Exploring\nParameter-Efficient Fine-tuning for Improving Communication Ef-\nficiency in Federated Learning. arXiv preprint arXiv:2210.01708.\nSung, Y .-L.; Cho, J.; and Bansal, M. 2022. Vl-adapter: Parameter-\nefficient transfer learning for vision-and-language tasks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 5227–5237.\nTan, H.; and Bansal, M. 2019. LXMERT: Learning Cross-Modality\nEncoder Representations from Transformers. In Inui, K.; Jiang, J.;\nNg, V .; and Wan, X., eds.,Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019,\n5099–5110. Association for Computational Linguistics.\nTan, Y .; Long, G.; Ma, J.; Liu, L.; Zhou, T.; and Jiang, J. 2022.\nFederated learning from pre-trained models: A contrastive learning\napproach. Advances in Neural Information Processing Systems, 35:\n19332–19344.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nall you need. Advances in neural information processing systems,\n30.\nWei, J.; Bosma, M.; Zhao, V . Y .; Guu, K.; Yu, A. W.; Lester, B.; Du,\nN.; Dai, A. M.; and Le, Q. V . 2021. Finetuned language models are\nzero-shot learners. arXiv preprint arXiv:2109.01652.\nXie, N.; Lai, F.; Doran, D.; and Kadav, A. 2019a. Visual En-\ntailment: A Novel Task for Fine-Grained Image Understanding.\nCoRR, abs/1901.06706.\nXie, N.; Lai, F.; Doran, D.; and Kadav, A. 2019b. Visual Entail-\nment: A Novel Task for Fine-grained Image Understanding. arXiv\npreprint arXiv:1901.06706.\nYang, M.; Su, S.; Li, B.; and Xue, X. 2023. Exploring One-shot\nSemi-supervised Federated Learning with A Pre-trained Diffusion\nModel. arXiv preprint arXiv:2305.04063.\nYang, X.; Xiong, B.; Huang, Y .; and Xu, C. 2022. Cross-Modal\nFederated Human Activity Recognition via Modality-Agnostic and\nModality-Specific Representation Learning. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 36, 3063–3071.\nYu, Q.; Liu, Y .; Wang, Y .; Xu, K.; and Liu, J. 2023. Multi-\nmodal Federated Learning via Contrastive Representation Ensem-\nble. arXiv preprint arXiv:2302.08888.\nYu, S.; Mu˜noz, J. P.; and Jannesari, A. 2023. Federated Foundation\nModels: Privacy-Preserving and Collaborative Learning for Large\nModels. arXiv preprint arXiv:2305.11414.\nZellers, R.; Bisk, Y .; Farhadi, A.; and Choi, Y . 2019. From Recog-\nnition to Cognition: Visual Commonsense Reasoning. InThe IEEE\nConference on Computer Vision and Pattern Recognition (CVPR).\nZhang, J.; Vahidian, S.; Kuo, M.; Li, C.; Zhang, R.; Wang, G.; and\nChen, Y . 2023. Towards Building the Federated GPT: Federated\nInstruction Tuning. arXiv preprint arXiv:2305.05644.\nZhou, C.; Li, Q.; Li, C.; Yu, J.; Liu, Y .; Wang, G.; Zhang, K.; Ji, C.;\nYan, Q.; He, L.; et al. 2023. A comprehensive survey on pretrained\nfoundation models: A history from bert to chatgpt. arXiv preprint\narXiv:2302.09419.\nZhuang, W.; Chen, C.; and Lyu, L. 2023. When Foundation Model\nMeets Federated Learning: Motivations, Challenges, and Future\nDirections. arXiv preprint arXiv:2306.15546.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11293",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.6982673406600952
    },
    {
      "name": "Computer science",
      "score": 0.6729599833488464
    },
    {
      "name": "Modal",
      "score": 0.6325639486312866
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4415702521800995
    },
    {
      "name": "Distributed computing",
      "score": 0.37355464696884155
    },
    {
      "name": "Computer architecture",
      "score": 0.3527209162712097
    },
    {
      "name": "Materials science",
      "score": 0.08926591277122498
    },
    {
      "name": "Composite material",
      "score": 0.06139528751373291
    },
    {
      "name": "Political science",
      "score": 0.04768812656402588
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}