{
    "title": "TCL: Transformer-based Dynamic Graph Modelling via Contrastive Learning",
    "url": "https://openalex.org/W3160404663",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2110760413",
            "name": "Wang Lu",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Chang, Xiaofu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1949867640",
            "name": "Li Shuang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2375397178",
            "name": "Chu Yun-fei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1885612739",
            "name": "Li Hui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1809037978",
            "name": "Zhang Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2000296914",
            "name": "He Xiaofeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2132743963",
            "name": "Song Le",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2348046973",
            "name": "Zhou, Jingren",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2034242580",
            "name": "Yang Hong-xia",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3094559936",
        "https://openalex.org/W2998313947",
        "https://openalex.org/W2961138157",
        "https://openalex.org/W2624431344",
        "https://openalex.org/W1665214252",
        "https://openalex.org/W3007404067",
        "https://openalex.org/W2805177834",
        "https://openalex.org/W2998116985",
        "https://openalex.org/W3101588560",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2798918712",
        "https://openalex.org/W3021393209",
        "https://openalex.org/W2964420626",
        "https://openalex.org/W2949888546",
        "https://openalex.org/W2787927827",
        "https://openalex.org/W3036685317",
        "https://openalex.org/W2806983170",
        "https://openalex.org/W2900041539",
        "https://openalex.org/W2766453196",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2949759968",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W2907192581",
        "https://openalex.org/W2755088640",
        "https://openalex.org/W2786016794",
        "https://openalex.org/W2594230395",
        "https://openalex.org/W3045200674",
        "https://openalex.org/W2965556524",
        "https://openalex.org/W2893944917",
        "https://openalex.org/W2951050019"
    ],
    "abstract": "Dynamic graph modeling has recently attracted much attention due to its extensive applications in many real-world scenarios, such as recommendation systems, financial transactions, and social networks. Although many works have been proposed for dynamic graph modeling in recent years, effective and scalable models are yet to be developed. In this paper, we propose a novel graph neural network approach, called TCL, which deals with the dynamically-evolving graph in a continuous-time fashion and enables effective dynamic node representation learning that captures both the temporal and topology information. Technically, our model contains three novel aspects. First, we generalize the vanilla Transformer to temporal graph learning scenarios and design a graph-topology-aware transformer. Secondly, on top of the proposed graph transformer, we introduce a two-stream encoder that separately extracts representations from temporal neighborhoods associated with the two interaction nodes and then utilizes a co-attentional transformer to model inter-dependencies at a semantic level. Lastly, we are inspired by the recently developed contrastive learning and propose to optimize our model by maximizing mutual information (MI) between the predictive representations of two future interaction nodes. Benefiting from this, our dynamic representations can preserve high-level (or global) semantics about interactions and thus is robust to noisy interactions. To the best of our knowledge, this is the first attempt to apply contrastive learning to representation learning on dynamic graphs. We evaluate our model on four benchmark datasets for interaction prediction and experiment results demonstrate the superiority of our model.",
    "full_text": "TCL: Transformer-based Dynamic Graph Modelling via\nContrastive Learning\nLu Wang\nEast China Normal University\nChina\nluwang@stu.ecnu.edu.cn\nXiaofu Chang\nDamo Academy, Alibaba Group\nChina\nchangxiaofu123@163.com\nShuang Li\nHarvard University\nUSA\nshuangli@fas.harvard.edu\nYunfei Chu\nDamo Academy, Alibaba Group\nChina\nfay.cyf@alibaba-inc.com\nHui Li\nAnt Group\nChina\nken.lh@antgroup.com\nWei Zhang\nEast China Normal University\nChina\nzhangwei.ltt@gmail.com\nXiaofeng He\nEast China Normal University\nChina\nxfhe@cs.ecnu.edu.cn\nLe Song\nGatech\nUSA\nlesong@cc.gatech.edu\nJingren Zhou\nDamo Academy, Alibaba Group\nChina\njingren.zhou@alibaba-inc.com\nHongxia Yang\nDamo Academy, Alibaba Group\nChina\nyang.yhx@alibaba-inc.com\nABSTRACT\nDynamic graph modeling has recently attracted much attention\ndue to its extensive applications in many real-world scenarios, such\nas recommendation systems, financial transactions, and social net-\nworks. Although many works have been proposed for dynamic\ngraph modeling in recent years, effective and scalable models are\nyet to be developed. In this paper, we propose a novel graph neural\nnetwork approach, called TCL, which deals with the dynamically-\nevolving graph in a continuous-time fashion and enables effective\ndynamic node representation learning that captures both the tempo-\nral and topology information. Technically, our model contains three\nnovel aspects. First, we generalize the vanilla Transformer to tem-\nporal graph learning scenarios and design a graph-topology-aware\ntransformer. Secondly, on top of the proposed graph transformer,\nwe introduce a two-stream encoder that separately extracts repre-\nsentations from temporal neighborhoods associated with the two\ninteraction nodes and then utilizes a co-attentional transformer\nto model inter-dependencies at a semantic level. Lastly, we are in-\nspired by the recently developed contrastive learning and propose\nto optimize our model by maximizing mutual information (MI)\nbetween the predictive representations of two future interaction\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nÂ© 2018 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\nhttps://doi.org/10.1145/1122445.1122456\nnodes. Benefiting from this, our dynamic representations can pre-\nserve high-level (or global) semantics about interactions and thus\nis robust to noisy interactions. To the best of our knowledge, this\nis the first attempt to apply contrastive learning to representation\nlearning on dynamic graphs. We evaluate our model on four bench-\nmark datasets for interaction prediction and experiment results\ndemonstrate the superiority of our model.\nCCS CONCEPTS\nâ€¢ Information systems â†’Recommender systems.\nKEYWORDS\ndynamic graph, transformer, contrastive learning, mutual informa-\ntion\nACM Reference Format:\nLu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng\nHe, Le Song, Jingren Zhou, and Hongxia Yang. 2018. TCL: Transformer-\nbased Dynamic Graph Modelling via Contrastive Learning . In Woodstock\nâ€™18: ACM Symposium on Neural Gaze Detection, June 03â€“05, 2018, Woodstock,\nNY. ACM, Singapore, 11 pages. https://doi.org/10.1145/1122445.1122456\n1 INTRODUCTION\nRepresentation learning on graphs is gaining increasing interests\nsince it has exhibited great potentials in many real-world applica-\ntions, ranging from e-commerce [ 9, 36], drug discovery [ 35, 38],\nsocial networks [ 33, 36] to financial transactions [ 12]. Previous\nworks on graph representation learning mainly focus on static set-\ntings where the topological structures are assumed fixed. However,\ngraphs in practice are often constantly evolving, i.e., the nodes and\ntheir associated interactions (edges) can emerge and vanish over\ntime. For instance, sequences of interactions such as following new\narXiv:2105.07944v1  [cs.LG]  17 May 2021\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng He, Le Song, Jingren Zhou, and Hongxia Yang\nfriends, sharing news with friends on Twitter, or daily user-item\npurchasing interactions on Amazon can naturally yield a dynamic\ngraph. The graph neural networks (GNNs) methods [5, 7, 30] tai-\nlored for static graphs usually perform poorly on such dynamic\nscenarios because of the inability of utilizing temporal evolutionary\ninformation that is lost in the static settings.\nDynamic graph modeling aims to learn a low-dimensional em-\nbedding for each node that can effectively encode temporal and\nstructural properties on dynamic graphs. This is an appealing yet\nchallenging task due to the sophisticated time-evolving graph struc-\ntures. Several works have been proposed these days. According to\nthe way that dynamic graphs are constructed, these works can be\nroughly divided into discrete-time methods and continuous-time\nmethods. The former methods [6, 20, 24] rely on discrete-time dy-\nnamic graph construction that approximates dynamic graph as a\nseries of graph snapshots over time. Usually, static graph encoding\ntechniques like GCN [7] or GAT [30] are applied to each snapshot,\nand then a recurrent neural network (RNN) [10] or a self-attention\nmechanism [29] is introduced to capture complicated temporal de-\npendency among snapshots. However, the discrete-time methods\ncan be sub-optimal since they ignore the fine-grained temporal\nand structural information which might be critical in real-world\napplications. Meanwhile, it is unclear how to specify the size of\nthe time intervals in different applications. To tackle these chal-\nlenges, the latter continuous-time methods [ 2, 4, 14, 23, 34] are\nconsidered and have achieved state-of-the-art results. Methods in\nthis direction focus on designing different temporal neighborhood\naggregation techniques applied to the fine-grained temporal graphs\nwhich are represented as a sequence of temporally ordered interac-\ntions. For instance, TGAT [34] proposes a continuous-time kernel\nencoder combined with a self-attention mechanism to aggregate\ninformation from temporal neighborhood. TGNs [23] introduces a\ngeneric temporal aggregation framework with a node-wise mem-\nory mechanism. Chang et. al. propose a dynamic message passing\nneural network to capture the high-order graph proximity on their\ntemporal dependency interaction graph (TDIG) [2].\nAlthough continuous-time methods have achieved impressive\nresults, there exist some limitations: First, when aggregating infor-\nmation from temporal neighborhoods of the two target interaction\nnodes, most of the aforementioned-methods [2, 4, 14] employ RNNs-\nlike architectures. Such methods suffer from vanishing gradient\nin optimization and are unable to capture long-term dependen-\ncies. The learned dynamic representations will degrade especially\nwhen applied to complicated temporal graphs. Secondly, these\nmethods typically compute dynamic embeddings of the two target\ninteractions nodes separately without considering the semantic\nrelatedness between their temporal neighborhoods (i.e. history\nbehaviors), which may also be a causal factor for the target interac-\ntion. For example, in a temporal co-author network, the fact is that\nnodes A and B previously co-authored with node C respectively\ncan promote a new potential collaboration between nodes A and\nB. Therefore, modeling mutual influences between the two tempo-\nral neighborhoods can aid informative dynamic representations.\nLastly, in optimization, most prior works typically model exact\nfuture by reconstructing future states [14] or leveraging a Temporal\nPoint Process (TPP) framework [ 2, 4] to model complicated sto-\nchastic processes of future interactions. However, they may learn\nthe noisy information when trying to fit the next interactions. Be-\nsides, computing the survival function of an intensity function in\nTPP-based methods is expensive when the integration cannot be\ncomputed in closed-form.\nTo address the above limitations, we propose a novel continuous-\ntime Transformer-based dynamic graph modeling framework via\ncontrastive learning, called TCL. The main contributions of our\nwork are summarized as follows:\nâ€¢We generalize the vanilla Transformer and enable it to handle\ntemporal-topological information on the dynamic graph repre-\nsented as a sequence of temporally cascaded interactions.\nâ€¢To obtain informative dynamic embeddings, We design a two-\nstream encoder that separately processes temporal neighbor-\nhoods associated with the two target interaction nodes by our\ngraph-topology-aware Transformer and then integrate them at a\nsemantic level through a co-attentional Transformer.\nâ€¢To ensure robust learning, we leverage a contrastive learning\nstrategy that maximizes the mutual information (MI) between\nthe predictive representations of future interaction nodes. To the\nour best knowledge, this is the first attempt to apply contrastive\nlearning to dynamic graph modeling.\nâ€¢Our model is evaluated on four diverse interaction datasets for\ninteraction prediction. Experimental results demonstrate that\nour method yields consistent and significant improvements over\nstate-of-the-art baselines.\n2 RELATED WORK\nThis section reviews state-of-the-art approaches for dynamic graph\nlearning. Since CL loss is used as our optimization objective, we also\nreview recent works on CL-based graph representation learning.\n2.1 Dynamic Graph Modeling\nAccording to how the dynamic graph is constructed, we roughly di-\nvide the existing modeling approaches into two categories: discrete-\ntime methods and continuous-time methods.\nDiscrete-time Methods. Methods in this category deal with a se-\nquence of discretized graph snapshots that coarsely approximates a\ntime-evolving graph. The authors in [37] utilize temporally regular-\nized weights to enforce the smoothness of nodesâ€™ dynamic embed-\ndings from adjacent snapshots. However, this method may break\ndown when nodes exhibit significantly varying evolutionary be-\nhaviors. DynGEM [6] is an autoencoding approach that minimizes\nthe reconstruction loss and learns incremental node embeddings\nthrough initialization from the previous time steps. However, this\nmethod may not capture the long-term graph similarities. Inspired\nby the self-attention mechanism [ 29], DySAT [24] computes dy-\nnamic embeddings by employing structural attention layers on each\nsnapshot followed by temporal attention layers to capture temporal\nvariations among snapshots. Recently, EvolveGCN [20] leverages\nRNNs to regulate the GCN model (i.e., network parameters) at every\ntime step to capture the dynamism in the evolving network param-\neters. Despite progress, the snapshots-based methods inevitably\nfail to capture the fine-grained temporal and structural information\ndue to the coarse approximation of continuous-time graphs. It is\nalso challenging to specify a suitable aggregation granularity.\nTCL: Transformer-based Dynamic Graph Modelling via Contrastive Learning Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nContinuous-time Methods. Methods in this category directly op-\nerate on time-evolving graphs without time discretization and focus\non designing different temporal aggregators to extract information.\nThe dynamic graphs are represented as a series of chronological in-\nteractions with precise timestamps recorded. DeepCoevolve [4] and\nits variant JODIE [14] employ two coupled RNNs to update dynamic\nnode embeddings given each interaction. They provide an implicit\nway to construct the dynamic graph where only the historical inter-\naction information of the two involved nodes of the interactions at\ntime ğ‘¡are utilized. The drawbacks are that they are limited to model-\ning first-order proximity while ignoring the higher-order temporal\nneighborhood structures. To exploit the topology structure of the\ntemporal graph explicitly, TDIG-MPNN [2] proposes a graph con-\nstruction method, named Temporal Dependency Interaction Graph\n(TDIG), which generalizes the above implicit construction and is\nconstructed from a sequence of cascaded interactions. Based on the\ntopology of TDIG, they employ a graph-informed Long Short Term\nMemory (LSTM) [11] to obtain the dynamic embeddings. However,\nthe downside of the above methods is that it is not good at cap-\nturing long-range dependencies and is difficult to train, which is\nalso the intrinsic weaknesses of RNNs. Recent work TGAT [34] and\nTGNs [23] adopt a different graph construction technique, i.e., a\ntime-recorded multi-graph, which accommodates more than one\ninteraction (edge) between a pair of nodes. TGAT uses a time encod-\ning kernel combined with a graph attention layer [30] to aggregate\ntemporal neighbors. Just like the encoding process in static models\n(e.g., GraphSAGE [7]), a single TGAT layer is used to aggregate\none-hop neighborhoods and by stacking several TGAT layers, it can\ncapture high-order topological information. TGNs generalizes the\naggregation of TGAT and utilizes a node-wise memory to capture\nlong-term dependency.\n2.2 Contrastive Learning On Graph\nRepresentation Learning\nRecently, state-of-the-art results in unsupervised graph representa-\ntion learning have been achieved by leveraging a contrastive learn-\ning loss that contrasts samples from a distribution that contains\ndependencies of interest and the distribution that does not. Deep\ngraph Infomax [31] learns node representations by contrasting rep-\nresentations of nodes that belong to a graph and nodes coming from\na corrupted graph. InfoGraph [25] learns graph-level representa-\ntions by contrasting the representations at graph level and that of\nsub-structures at different scales. Motivated by recent advances\nin multi-view contrastive learning for visual representation learn-\ning, [8] proposes a contrastive multi-view representation learning\nat both node and graph levels. According to [22], contrastive objec-\ntives used in these methods can be seen as maximizing the lower\nbounds of MI. A recent study [28] has shown that the success of\nthese models is not only attributed to the properties of MI but is\nalso influenced by the choice of the encoder and the MI estimators.\n3 PRELIMINARIES\nTo make our paper self-contained, we start with introducing the\nbasic knowledge of Temporal Dependency Interaction Graph [2],\nTransformer [29] and Contrastive Learning [19], upon which we\nbuild our new method.\nFigure 1: Illustration of a Temporal Dependency Interaction\nGraph induced from a sequence of six chronological interac-\ntions.\n3.1 Temporal Dependency Interaction Graph\nTemporal Dependency Interaction Graph (TDIG) proposed by [2]\nis constructed by a sequence of temporally cascaded chronological\ninteractions. Compared with other construction methods, TDIG\nas illustrated in Figure 1 maintains the fine-grained temporal and\nstructural information of dynamic graphs. Therefore, in this paper,\nwe select TDIG as our backbone modeling .\nFormally, a temporal dependency interaction graph Gğ‘¡ = (Vğ‘¡,Eğ‘¡)\n(ğ‘‡ğ·ğ¼ğº ) consists of a node set Vğ‘¡ and an edge set Eğ‘¡ indexed by\ntime ğ‘¡ âˆˆR+and is constructed based on a sequence of chrono-\nlogical interactions up to time ğ‘¡. An interaction occurring at time\nğ‘¡ is denoted as ğ‘™ğ‘¢,ğ‘£,ğ‘¡, where nodes ğ‘¢, ğ‘£ âˆˆVğ‘¡ are the two parties\ninvolved in this interaction. Since one node can have multiple in-\nteractions happening at different time points, for convenience, we\nlet ğ‘¢ğ‘¡ represent the node ğ‘¢ at time ğ‘¡ who was involved in ğ‘™ğ‘¢,ğ‘£,ğ‘¡.\nThere are two types of edges in Eğ‘¡. One is the interaction edge\nthat corresponds to an interaction ğ‘™ğ‘¢,ğ‘£,ğ‘¡. The other is the depen-\ndency edge that links the current node ğ‘¢ğ‘¡ to the two dependency\nnodes that were involved in ğ‘¢ğ‘¡â€™s last interaction at time ğ‘¡ğ‘¢âˆ’(just\nbefore time t). The dependency edge represents the evolution and\nthe causality between temporal relevant nodes. The two dependen-\ncies nodes ofğ‘¢ğ‘¡ are denoted asğ‘¢(ğ‘¡ğ‘¢âˆ’)(1)and ğ‘¢(ğ‘¡ğ‘¢âˆ’)(2)respectively.\nThe time interval between ğ‘™ğ‘¢,ğ‘£,ğ‘¡ and the node ğ‘¢â€™s last interaction,\ndenoted as Î”(ğ‘¢,ğ‘¡)= ğ‘¡âˆ’ğ‘¡ğ‘¢âˆ’, is treated as a dependency edge feature.\nTo reduce the computational cost in practice, history information\npresent in the sub-graphs rather than a whole TDIG is utilized to\nform dynamic embeddings. Gğ‘¡(ğ‘¢,ğ‘˜)is denoted as the max ğ‘˜-depth\nsub-graph(i.e., temporal neighborhoods) rooted at ğ‘¢ğ‘¡, where ğ‘˜ is\na hyper-parameter. For more details, we refer the readers to the\noriginal paper [2] for a complete description.\n3.2 Transformer\nTransformer [29] has achieved state-of-the-art performance and\nefficiency on many NLP tasks that have been previously dominated\nby RNN/CNN-based [16, 26] approaches. A Transformer block relies\nheavily on a multi-head attention mechanism to learn the context-\naware representation for sequences, which can be defined as:\nğ‘¶ = MultiHead(ğ‘¸,ğ‘²,ğ‘½)= Concat \u0000ğ‘¶1,ğ‘¶2,..., ğ‘¶ğ‘‘â„\n\u0001 ğ‘¾ğ‘‚ (1)\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng He, Le Song, Jingren Zhou, and Hongxia Yang\nwhere ğ‘¾ğ‘‚ is a trainable parameter. ğ‘‘â„ is the number of heads. ğ‘¶ğ‘–\nis computed as:\nğ‘¶ğ‘– = Attention \u0000ğ‘¸ğ‘¾ğ‘„,ğ‘–,ğ‘²ğ‘¾ ğ¾,ğ‘–,ğ‘½ğ‘¾ğ‘‰,ğ‘–\n\u0001 (2)\nwhere ğ‘¾âˆ—,ğ‘– âˆˆRğ‘‘Ã—ğ‘‘ are trainable parameters andAttention(Â·,Â·,Â·)\nis the scaled dot-product attention defined as:\nAttention(ğ‘¸,ğ‘²,ğ‘½)= softmax\n\u0012ğ‘¸ğ‘²ğ‘‡\nâˆš\nğ‘‘\n\u0013\nğ‘½ (3)\nAs input to Transformer, an element in a sequence is represented by\nan embedding vector. The multi-head attention mechanism works\nby injecting the Positional Embedding into the element embeddings\nto be aware of element orders in a sequence.\n3.3 Contrastive Learning\nContrastive learning approaches have attracted much attention\nfor learning word embeddings in natural language models [ 13],\nimage classification [3], and static graph modeling [31]. Contrastive\nlearning optimizes an objective that enforces the positive samples\nto stay in a close neighborhood and negative samples far apart:\nLcontrast = âˆ’E\n\"\nlog ğ‘’Simğœƒ(x,x+)\nğ‘’Simğœƒ(x,x+)+Ã\nxâˆ’ğ‘’Simğœƒ(x,xâˆ’)\n#\n(4)\nwhere the positive pair (x,x+)is contrasted with negative pair\n(x,xâˆ’). A discriminating function Simğœƒ(Â·)with parameters ğœƒ is\ntrained to achieve a high value for congruent pairs and low for\nincongruent pairs. According to the proofs [ 22], minimizing the\nobjective Lcontrast can maximize the lower bounds of MI.\n4 PROPOSED METHOD\nGiven the topology of TDIG, we first describe how to generalize the\nvanilla Transformer to handle temporal and structural information.\nTo obtain informative dynamic embeddings, we design a two-stream\nencoder that first processes historical graphs associated with the\ntwo interaction nodes by a graph-topology-aware Transformer and\nthen propose a co-attentional Transformer to fuse them at the level\nof semantics. Finally, we will introduce our temporal contrastive\nobjective function used in model training.\n4.1 Graph Transformer\nConsidering the advantages of the Transformer in capturing long-\nterm dependencies and in computational efficiency, we propose\nto extract temporal and structural information of TDIG by Trans-\nformer type of architecture. The vanilla Transformer architecture\ncannot be directly applied to dynamic graphs due to two reasons:\n(a) The vanilla Transformer is designed to process equally spaced\nsequences and cannot deal with the irregularly spaced time in-\ntervals between interactions that can be critical in analyzing the\ninteractive behaviors [29]. (b) The vanilla Transformer considers\nthe attention with a flat structure, which is not suitable to capture\nthe hierarchical and structured dependency relationships exhibited\nin TDIG. To address these challenges, we make two adaptions: (1)\nWe leverage the depths of nodes together with the time intervals\nto encode nodesâ€™ positions or hierarchies. (2) We inject the graph\nstructure into the attention mechanism by performing the masked\noperation. We will elaborate as follows.\n4.1.1 Embedding Layer. Let Sğ‘¢ =\n\u0002\nğ‘ ğ‘¢\n1 ,...,ğ‘  ğ‘¢\nğ‘– ,...,ğ‘  ğ‘¢ğ‘š\n\u0003\nbe the set of\nnodes {ğ‘ ğ‘¢\nğ‘– }ğ‘–=1,...,ğ‘š in Gğ‘¡(ğ‘¢,ğ‘˜)where the nodes are listed according\nto a pre-determined order of graph traversal (e.g., the breadth-first-\nsearch) and ğ‘šis the number of nodes in Gğ‘¡(ğ‘¢,ğ‘˜). We create a node\nembedding matrix M âˆˆR(|Vğ‘¡|)Ã—ğ‘‘, where each row of the matrix\nM corresponds to a d-dimensional embedding of a specific node in\nnode set Vğ‘¡. We then retrieve the node embedding matrices for the\nnode sequences Sğ‘¢:\nEğ‘›ğ‘œğ‘‘ğ‘’\nğ‘¢ =\nï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\nMğ‘ ğ‘¢\n1\nMğ‘ ğ‘¢\n2\nÂ·Â·Â·\nMğ‘ ğ‘¢ğ‘š\nï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»\n(5)\nwhere Eğ‘›ğ‘œğ‘‘ğ‘’ğ‘¢ âˆˆRğ‘šÃ—ğ‘‘ is the concatenation of the corresponding\nembeddings of nodes in Sğ‘¢.\nPositional Embedding: Since the self-attention mechanism can\nnot be aware of the nodesâ€™ positions or hierarchies on the TDIG,\nwe first learn a nodesâ€™ depth embedding P âˆˆRğ‘˜Ã—ğ‘‘, where the ğ‘—-th\nrow of the matrix P denotes a d-dimensional embedding for the\ndepth ğ‘—. Denote\nEğ‘‘ğ‘’ğ‘ğ‘¡â„\nğ‘¢ =\nï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\nPğ‘ ğ‘¢\n1\nPğ‘ ğ‘¢\n2\nÂ·Â·Â·\nPğ‘ ğ‘¢ğ‘š\nï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»\n(6)\nwhere Eğ‘‘ğ‘’ğ‘ğ‘¡â„\nğ‘¢ âˆˆRğ‘šÃ—ğ‘‘ is the concatenation of the corresponding\ndepth embeddings of nodes in Sğ‘¢. The depths of nodes in the TDIG\nindicate temporal orders of the observed interactions in which\nthese nodes are involved. However, only considering the depths of\nnodes is not enough, because the nodes in the TDIG have multi-\nple instances with the same depths. To enhance nodesâ€™ positional\ninformation, we also employ time intervals (i.e., dependency edge\nfeatures) that usually convey important behaviour information.\nSpecifically, we use a time-interval projection matrix WÎ” âˆˆRğ‘‘Ã—1\nto get the following time-interval embedding matrices:\nEğ‘¡ğ‘–ğ‘šğ‘’\nğ‘¢ =\nï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\nWÎ”Î”(ğ‘ ğ‘¢\n1 ,ğ‘¡)\nWÎ”Î”(ğ‘ ğ‘¢\n2 ,ğ‘¡)\nÂ·Â·Â·\nWÎ”Î”(ğ‘ ğ‘¢ğ‘š,ğ‘¡)\nï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»\n(7)\nwhere Etimeu âˆˆRğ‘šÃ—ğ‘‘ is the concatenation of the corresponding\ntime-interval embeddings of nodes in Sğ‘¢.\nFinally, by injecting the TDIG-based positional embedding into\nthe above node embeddings, we get the input embedding matrices\nË†Eğ‘¢ âˆˆRğ‘šÃ—ğ‘‘ for the next structure-aware attention layer:\nË†Eğ‘¢ = Eğ‘›ğ‘œğ‘‘ğ‘’\nğ‘¢ +Eğ‘‘ğ‘’ğ‘ğ‘¡â„\nğ‘¢ +Eğ‘¡ğ‘–ğ‘šğ‘’\nğ‘¢ (8)\n4.1.2 Structure-Aware Attention Layer. The self-attention mecha-\nnism above used in the NLP domain usually allows every token to\nattend to every other token, but directly applying it to nodes of the\nTDIG will lead to a loss of the structural information. Instead, we\npropose to inject the TDIG structural information into the attention\nmask. Specifically, the structure attention mask ğš¿ğ‘¢ âˆˆRğ‘šÃ—ğ‘š for\nTCL: Transformer-based Dynamic Graph Modelling via Contrastive Learning Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nthe subgraph Gğ‘¡(ğ‘¢,ğ‘˜)is defined as:\nğš¿ğ‘¢\nğ‘–,ğ‘— =\n\u001a 0 if ğ‘˜ğ‘— âˆˆSG (ğ‘ğ‘–)\nâˆ’âˆ ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’ (9)\nwhere ğ‘ğ‘– and ğ‘˜ğ‘— denote the query node indexed by ğ‘– and the key\nnode indexed by ğ‘— respectively. The ğš¿ğ‘¢\nğ‘–,ğ‘— is set by 0 only for the\naffinity pairs whose key node ğ‘˜ğ‘— belongs to the sub-graphs rooted\nat the query node ğ‘ğ‘–, i.e., SG(ğ‘ğ‘–). We then inject the structure\nattention mask ğš¿ğ‘¢ âˆˆRğ‘šÃ—ğ‘š into the scaled dot-product attention\nlayer:\nğ‘¸ğ‘\nğ‘¢,ğ‘²ğ‘\nğ‘¢,ğ‘½ğ‘\nğ‘¢ = ğ‘¯ğ‘\nğ‘¢ğ‘¾ğ‘„,ğ‘¯ğ‘\nğ‘¢ğ‘¾ğ¾,ğ‘¯ğ‘\nğ‘¢ğ‘¾ğ‘‰\nM-Attention(ğ‘¸ğ‘\nğ‘¢,ğ‘²ğ‘\nğ‘¢,ğ‘½ğ‘\nğ‘¢,ğš¿ğ‘¢)= softmax\n \nğ‘¸ğ‘ğ‘¢(ğ¾ğ‘ğ‘¢)ğ‘‡ +ğš¿ğ‘¢\nâˆš\nğ‘‘\n!\nğ‘½ğ‘\nğ‘¢\n(10)\nwhere ğ‘¯ğ‘ğ‘¢ represents the (ğ‘ +1)-th block input (when stacking\nB blocks) and ğ‘¯ğ‘=0ğ‘¢ = Ë†ğ‘¬ğ‘¢. It could be observed that the attention\noutput will be zero for the mask ğš¿ğ‘¢\nğ‘–,ğ‘— with negative infinity values\n(-âˆ). In other words, each query node only attends to this query\nnodeâ€™ structural dependency nodes whose occurrence time are\nearlier than the occurrence time of this query node. The masked\nmulti-head attention is denoted as\nM-MultiHead(ğ‘¸ğ‘\nğ‘¢,ğ‘²ğ‘\nğ‘¢,ğ‘½ğ‘\nğ‘¢,ğš¿ğ‘¢) (11)\n4.1.3 Graph Transformer Block. Our graph-topology-aware Trans-\nformer block is defined with the following operation:\nğ‘¶ğ‘\nğ‘¢ = M-MultiHead(ğ‘¸ğ‘âˆ’1\nğ‘¢ ,ğ‘²ğ‘âˆ’1\nğ‘¢ ,ğ‘½ğ‘âˆ’1\nğ‘¢ ,ğš¿ğ‘¢) (12)\nğ‘¯ğ‘\nğ‘¢ = LN(FFN(LN(ğ‘¶ğ‘\nğ‘¢ +ğ‘¸ğ‘\nğ‘¢))+ LN(ğ‘¶ğ‘\nğ‘¢ +ğ‘¸ğ‘\nğ‘¢)) (13)\nwhere \"+\" means a residual connection operation. LN denotes a\nlayer normalization module [1] which usually stabilizes the training\nprocess. FFN represents a two-layer fully-connected feed-forward\nnetwork module defined as:\nFFN(ğ‘¥)= ğœ(ğ‘¥ğ‘Š1 +ğ‘1)ğ‘Š2 +ğ‘2 (14)\nwhere ğœ is an activation function usually implemented with a recti-\nfied linear unit [17]. We sequentially apply these operations to get\nğ‘¯ğ‘ğ‘¢, which is the output of our graph Transformer block.\n4.2 Encoders for Dynamic Embeddings\nGiven the topology of the Gğ‘¡, our work aims to obtain the dynamic\nembeddings of the interaction nodes ğ‘¢(ğ‘¡)and ğ‘¤(ğ‘¡), i.e., hğ‘¢(ğ‘¡) âˆˆRğ‘‘\nand hğ‘¤(ğ‘¡) âˆˆRğ‘‘, by leveraging the history information in their\nsub-graphs Gğ‘¡(ğ‘¢,ğ‘˜)and Gğ‘¡(ğ‘¤,ğ‘˜). To achieve this, we devise an\nencoder with a two-stream architecture that first processes the\nGğ‘¡(ğ‘¢,ğ‘˜)and Gğ‘¡(ğ‘¤,ğ‘˜)in two separate streams by the above graph-\ntopology-aware Transformer layer and then interacts them at the\nlevel of semantics through a co-attentional Transformer layer. The\nencoder architecture is depicted in Fig.2 As shown in Fig. 2, our\ntwo-stream encoder includes two successive parts. The first part is\na graph Transformer block which we utilize to obtain intermediate\nembeddings of nodes in the Gğ‘¡(ğ‘¢,ğ‘˜)and Gğ‘¡(ğ‘¤,ğ‘˜), i.e., Ëœğ‘¯ğ‘\nğ‘¢ and Ëœğ‘¯ğ‘\nğ‘¤.\nThe second part is a cross-attentional Transformer block that first\nmodels information correlation between Ëœğ‘¯ğ‘\nğ‘¢ and Ëœğ‘¯ğ‘\nğ‘¤, and then\ngets final informative dynamic embeddings matrices , ğ‘¯ğ‘ğ‘¢ and ğ‘¯ğ‘ğ‘¤,\nwhere we retrieval dynamic embeddings hğ‘¢(ğ‘¡)and hğ‘¤(ğ‘¡)for the\nFigure 2: Overview of Our Two-stream Encoder Framework.interactions nodes ğ‘¢(ğ‘¡)and ğ‘¤(ğ‘¡). The core of the cross-attentional\nTransformer block is a cross-attention operation by exchanging\nkey-value pairs in the multi-headed attention. Formally, we have\nthe following equations:\nğ‘¸ğ‘\nğ‘¢,ğ‘²ğ‘\nğ‘¢,ğ‘½ğ‘\nğ‘¢ = Ëœğ‘¯ğ‘\nğ‘¢ğ‘¾ğ‘„, Ëœğ‘¯ğ‘\nğ‘¢ğ‘¾ğ¾, Ëœğ‘¯ğ‘\nğ‘¢ğ‘¾ğ‘‰ (15)\nğ‘¸ğ‘\nğ‘¤,ğ‘²ğ‘\nğ‘¤,ğ‘½ğ‘\nğ‘¤ = Ëœğ‘¯ğ‘\nğ‘¤ğ‘¾ğ‘„, Ëœğ‘¯ğ‘\nğ‘¤ğ‘¾ğ¾, Ëœğ‘¯ğ‘\nğ‘¤ğ‘¾ğ‘‰ (16)\nğ‘¶ğ‘\nğ‘¢ = MultiHead(ğ‘¸ğ‘\nğ‘¢,ğ‘²ğ‘\nğ‘¤,ğ‘½ğ‘\nğ‘¤) (17)\nğ‘¶ğ‘\nğ‘¤ = MultiHead(ğ‘¸ğ‘\nğ‘¤,ğ‘²ğ‘\nğ‘¢,ğ‘½ğ‘\nğ‘¢) (18)\nğ‘¯ğ‘\nğ‘¢ = LN(FFN(LN(ğ‘¶ğ‘\nğ‘¢ +ğ‘¸ğ‘\nğ‘¢))+ LN(ğ‘¶ğ‘\nğ‘¢ +ğ‘¸ğ‘\nğ‘¢)) (19)\nğ‘¯ğ‘\nğ‘¤ = LN(FFN(LN(ğ‘¶ğ‘\nğ‘¤ +ğ‘¸ğ‘\nğ‘¤))+ LN(ğ‘¶ğ‘\nğ‘¤ +ğ‘¸ğ‘\nğ‘¤)) (20)\nThe co-attention operations by exchanging key-value pairs in Eq.15\nand Eq.16 enable our encoder to highlight relevant semantics shared\nby the two sub-graphs Gğ‘¡(ğ‘¢,ğ‘˜)and Gğ‘¡(ğ‘¤,ğ‘˜), and suppress the\nirrelevant semantics. Meanwhile, the attentive information from\nGğ‘¡(ğ‘¤,ğ‘˜)is incorporated into the final representation ğ‘¯ğ‘ğ‘¢ and vice\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng He, Le Song, Jingren Zhou, and Hongxia Yang\nversa. By doing so, the obtained embeddings matrices ğ‘¯ğ‘ğ‘¢ and ğ‘¯ğ‘ğ‘¤\ncan be semantic and informative. We then retrieval dynamic node\nembeddings hğ‘¢(ğ‘¡)and hğ‘¤(ğ‘¡)for the two interactions nodes ğ‘¢(ğ‘¡)\nand ğ‘¤(ğ‘¡)from ğ‘¯ğ‘ğ‘¢ and ğ‘¯ğ‘ğ‘¤.\nRelation to previous work. In comparison with the RNN-based\ninformation aggregation mechanism, such as JODIE, DeepCoevolve\nand TDIG-MPNN, our Transformer-based model excels in captur-\ning the complex dependency structures over sophisticated time-\nevolving graphs. It successfully tackles the vanishing gradient is-\nsues inherited from RNN and eases optimization. Whatâ€™s more, all\nthese previous methods learn node embeddings separately with-\nout considering the semantic relatedness between their temporal\nneighborhoods (i.e. history behaviors), which may also trigger new\ninteractions. In contrast, we design a two-stream encoder to sep-\narately process temporal neighborhoods of the interaction nodes\nand integrate them at a semantic level through a co-attentional\nTransformer. As a result, we obtain more informative dynamic\nembeddings.\n4.3 Dynamic Graph Contrastive Learning\nFor many generative time series models, the training strategies\nare formulated to maximize the prediction accuracy. For example,\nJODIE exploits the local smoothness of the data and optimizes the\nmodel by minimizing the ğ¿2 distance between the predicted node\nembedding and the ground truth nodeâ€™s embedding at every interac-\ntion. Based on the TPP framework, methods like DeepCoevolve [4]\nand TDIG-MPNN [2] model the occurrence rate of new interaction\nat any time ğ‘¡ and optimize the models by maximizing the likeli-\nhood. However, their training performance relies on the modeling\naccuracy of future interactions and could be vulnerable to noise.\nRegarding this, instead of reconstructing the exact future interac-\ntions or employing the complicated generative model to learn the\nstochastic processes of future interactions, we attempt to maximize\nthe mutual information between the latent representations of inter-\naction nodes in the future. In this way, we can learn the underlying\nlatent representations that the interaction nodes have in common.\nMoreover, these underlying latent representations can preserve the\nhigh-level semantics of interactions and focus less on the low-level\ndetails, which are robust to noise information [32]. We apply the\ncontrastive objective function to our dynamic embedding learning.\nAccording to proofs in [22], minimizing this training objective func-\ntion is equivalent to maximizing the lower bound of the mutual\ninformation between interaction nodes.\n4.3.1 Future Prediction. Since we are leveraging the future inter-\naction as our supervisory signal, we first construct the predictive\nfuture states of nodes ğ‘¢ and ğ‘£ associated with future interaction\nğ‘™ğ‘¢,ğ‘£,ğ‘¡. Considering one node has two dependency nodes in TDIG, the\npredictive nodeâ€™ representation at future timeğ‘¡is constructed based\non the dynamic embeddings of this nodeâ€™ two dependency nodes\ninvolved in the previous interaction just before time ğ‘¡. Formally,\nwe have the following equations:\nÂ¯hğ‘¢(ğ‘¡)= ğœ™([hğ‘¢(1)(ğ‘¡ğ‘¢âˆ’); hğ‘¢(2)(ğ‘¡ğ‘¢âˆ’)]) (21)\nÂ¯hğ‘£(ğ‘¡)= ğœ™([hğ‘£(1)(ğ‘¡ğ‘£âˆ’); hğ‘£(2)(ğ‘¡ğ‘£âˆ’)]) (22)\nwhere [; ]denotes concatenation. ğœ™(Â·)is a projection head used to\npredict the future node representation. hğ‘¢(1)(ğ‘¡ğ‘¢âˆ’)and hğ‘¢(2)(ğ‘¡ğ‘¢âˆ’)are\nthe dynamic embeddings of ğ‘¢(ğ‘¡)â€™ two dependency nodes ğ‘¢(1)(ğ‘¡ğ‘¢âˆ’)\nand ğ‘¢(2)(ğ‘¡ğ‘¢âˆ’)respectively, which are obtained by extracting the his-\ntory information from sub-graphs Gğ‘¡ğ‘¢âˆ’(ğ‘¢(1),ğ‘˜)and Gğ‘¡ğ‘¢âˆ’(ğ‘¢(2),ğ‘˜)\nusing our two-stream encoder. The ğœ™(Â·)function takes the con-\ncatenation of hğ‘¢(1)(ğ‘¡ğ‘¢âˆ’) and hğ‘¢(2)(ğ‘¡ğ‘¢âˆ’) as input to forecast future\nrepresentation Â¯hğ‘¢(ğ‘¡) âˆˆRğ‘‘. Similarly, we have the predictive rep-\nresentation Â¯hğ‘£(ğ‘¡) âˆˆRğ‘‘ for the node ğ‘£ at future time ğ‘¡. In practice,\nğœ™(Â·)is an MLP with two hidden layers and PReLU non-linearity.\n4.3.2 Contrastive Objective Function. To train our two-stream en-\ncoders in an end-to-end fashion and learn the informative dynamic\nembeddings that aid future interaction prediction, we utilize the fu-\nture interaction as our supervisory signal and maximize the mutual\ninformation between two nodes involved in the future interac-\ntion by contrasting their predictive representations. A schematic\noverview of our proposed method is in Fig. 3\nGiven a list of interactions D= {(ğ‘¢ğ‘–,ğ‘£ğ‘–,ğ‘¡ğ‘–)}ğ‘\nğ‘–=1 observed in a\ntime window [0,ğ‘‡], our contrastive learning is to minimize:\nL= âˆ’E\nï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\nlog ğ‘’Sim\n\u0010\nÂ¯hğ‘¢ğ‘–(ğ‘¡ğ‘–),Â¯hğ‘£ğ‘–(ğ‘¡ğ‘–)\n\u0011\nğ‘’Sim\n\u0010\nÂ¯hğ‘¢ğ‘–(ğ‘¡ğ‘–),Â¯hğ‘£ğ‘–(ğ‘¡ğ‘–)\n\u0011\n+Ã\nğ‘¤â‰ ğ‘£ğ‘– ğ‘’Sim\n\u0010\nÂ¯hğ‘¢ğ‘–(ğ‘¡ğ‘–),Â¯hğ‘¤(ğ‘¡ğ‘–)\n\u0011\nï£¹ï£ºï£ºï£ºï£ºï£ºï£»\n(23)\nwhere Sim(Â·): Rğ‘‘ Ã—Rğ‘‘ â†¦âˆ’â†’R denotes discriminator function that\ntakes two predictive representations as the input and then scores\nthe agreement between them. Our discriminator function Sim(Â·)is\ndesigned to explore both the additive and multiplicative interaction\nrelations between Â¯hğ‘¢ğ‘–(ğ‘¡ğ‘–)and Â¯hğ‘£ğ‘–(ğ‘¡ğ‘–):\nSim\n\u0010\nÂ¯hğ‘¢ğ‘–(ğ‘¡ğ‘–),Â¯hğ‘£ğ‘–(ğ‘¡ğ‘–)\n\u0011\n= SoftPlus\n\u0010\nwâŠ¤\nğ‘ğ‘‘ğ‘‘(Â¯hğ‘¢ğ‘–(ğ‘¡ğ‘–)+Â¯hğ‘£ğ‘–(ğ‘¡ğ‘–))\n+wâŠ¤\nğ‘šğ‘¢ğ‘™(Â¯hğ‘¢ğ‘–(ğ‘¡ğ‘–)âŠ™Â¯hğ‘£ğ‘–(ğ‘¡ğ‘–)\n\u0011\n(24)\nEssentially, the contrastive objective function acts as a multi-way\nclassifier that distinguishes the positive pairs out of all other nega-\ntive pairs. In our case, the positive pair is (Â¯hğ‘¢ğ‘–(ğ‘¡ğ‘–),Â¯hğ‘£ğ‘–(ğ‘¡ğ‘–)), i.e., the\npredictive representations of two nodes ğ‘¢ğ‘– and ğ‘£ğ‘– involved in an in-\nteraction at timeğ‘¡ğ‘–. All the other pairs (Â¯hğ‘¢ğ‘–(ğ‘¡ğ‘–),Â¯hğ‘¤(ğ‘¡ğ‘–)) whereğ‘¤ â‰  ğ‘£ğ‘–\nare negative pairs, which means all the other items that donâ€™t have\ninteractions with ğ‘¢ğ‘– at time ğ‘¡ğ‘– will be treated as negative items.\nThe aim of our contrastive loss here is to train our two-stream\nencoder that maximises the shared information between positive\npairs, while minimizing the shared information between negative\npairs that are well separated. Compared with existing training\nmethods that model the exact future, our proposed contrastive loss\nmakes the obtained dynamic embeddings more informative and\nrobust to noise information. A schematic overview of our training\nmethod is shown in Fig. 3. We describe the pseudocode of training\nin Appendix.\n5 EXPERIMENTS\nWe evaluate the effectiveness of TCL by comparing with baselines\non four diverse interaction datasets. Meanwhile, the ablation study\nis conducted to understand which sub-module of TCL contributes\nmost to the overall performance. We also conduct experiments of\nparameter sensitivity.\nTCL: Transformer-based Dynamic Graph Modelling via Contrastive Learning Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nFigure 3: Illustration of CL-based optimization strategy for proposed two-stream encoder.\nTable 1: Dataset Statistics.\nDataset CollegeMsg Wikipedia LastFM Reddit\n#u 999 7619 1000 9986\n#v 1352 998 1000 984\n#Interactions 48,771 128,271 1,293,103 672,447\nInteraction Repetition Density 67.28% 87.78% 88.01% 53.97%\nDuration (day) 193.63 30.00 1586.89 3882.00\n5.1 Experimental Setting\n5.1.1 Datasets. We evaluate our proposed model for temporal in-\nteraction prediction on four diverse interaction datasets as shown\nin Table 1.\nCollegeMsg [15]. This dataset consists of message sending activi-\nties on a social network at the University of California, Irvine. The\ndataset we use here contains 999 senders and 1,352 receivers, with\ntotally 48,771 activities.\nReddit [14]. This public dataset consists of one month of posts\nmade by users on subreddits. We select the 984 most active sub-\nreddits as items and the 9,986 most active users. This results in\n672,447 interactions.\nLastFM [15]. We use the dataset from top 1000 active users and\n1000 popular songs, where the interactions indicate the user listen\nto the song.\nWikipedia [15]. We use the data from 998 frequent edited pages\nand 7619 active users on Wikipedia, yielding totally 128271 interac-\ntions.\nThese four datasets are from different real-world scenes and\nvary in terms of the number of interactions, interaction repetition\ndensity and time duration, where interaction repetition density is\nthe number of the intersection of training set and test set divides\nby the number of test set.\n5.1.2 Baselines. We compare TCL with the following algorithms\nspanning three algorithmic categories:\nâ€¢Static Methods. We consider GraphSAGE [7] with four aggre-\ngators, namely, GCN, MEAN, MAX and LSTM. A GAT [30]\naggregator is also implemented base on the GraphSAGE frame-\nwork [7]. For convenience, we report the best results among the\nabove five aggregators denoted as GraphSage*.\nâ€¢Discrete-Time Methods. Three snapshot-based dynamic meth-\nods are considered here. CTDNE [18] is an extension to Deep-\nWalk[21] with time constraint for sampling orders;DynGEM [6]\nis an autoencoding approach that minimizes the reconstruction\nloss and learns incremental node embeddings through initializa-\ntion from the previous time steps;DySAT[24] computes dynamic\nembeddings by using structural and temporal attentions.\nâ€¢Continuous-Time Methods . Five continuous-time baselines\nare considered here. DeepCoevolve [4] learns the dynamic em-\nbedding via utilizing a TPP framework to estimate the intensity\nof two nodes interacting in the future time; JODIE[14] utilizes\ntwo RNN models and a projection function to learn dynamic\nembeddings; TDIG-MPNN [2] proposes a dynamic message\npassing network combined a selection mechanism to obtain dy-\nnamic embeddings; TGAT[34] proposes temporal graph atten-\ntion layer to aggregate temporal-topological neighborhood fea-\ntures; TGNs[23] extends TGAT by adding a memory modules\nto capture long-term interaction relations.\n5.1.3 Evaluation Metrics. Given an interaction ğ‘™ğ‘¢,ğ‘£,ğ‘¡, each method\noutputs the nodeğ‘¢â€™s preference scores over all the items at timeğ‘¡in\ntest set. We sort scores in a descending order and record the rank of\nthe paired node ğ‘£. We report the average ranks for all interactions\nin test data, which is denoted as Mean Rank(MR). We also report\nthe Hit@10 defined as the proportion of times that a test tuple\nappears in the top 10.\n5.1.4 Reproducibility. For GraphSage, CTDNE, DynGEM, DySAT,\nJODIE, TGAT, and TGNs, we use their open-source implemen-\ntations. For DeepCoevole and TDIG-MPNN, we use the source\ncodes provided by their authors. More details about parameter\nsettings and implementation can be found in the Appendix.\n5.2 Overall Performances\nIn this section, we evaluate our method and the baselines on tem-\nporal interaction prediction task. All experiments are repeated five\ntimes and averaged results are reported in Table 2, from which\nwe have the following observations: (1)TCL consistently outper-\nforms all the competitors on all the datasets. The improvement of\nTCL over the second-best results(underlined in Table 2) are 14.49%,\n21.58%, 16.01% and 24.74% respectively in terms of MR scores. The\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng He, Le Song, Jingren Zhou, and Hongxia Yang\nTable 2: Overall Comparison on Temporal Interaction Prediction.\nGroup Method CollegeMsg Wikipedia LastFM Reddit\nMRâ†“ Hit@10â†‘ MRâ†“ Hit@10â†‘ MRâ†“ Hit@10â†‘ MRâ†“ Hit@10â†‘\nStatic GraphSage* [7, 30] 269.35Â±6.12 11.56 Â±1.27 165.39 Â±3.59 33.47 Â±3.81 261.67 Â±2.17 11.02 Â±1.04 50.49 Â±1.14 58.63 Â±1.54\nDiscrete-time\nCTDNE[18] 607.27Â±10.65 1.85 Â±0.53 350.68 Â±6.80 11.89 Â±0.62 461.59 Â±7.49 1.17 Â±0.05 51.30 Â±1.37 58.55 Â±1.08\nDynGEM[6] 337.64Â±9.30 3.47 Â±0.38 216.56 Â±5.27 16.79 Â±0.21 457.68 Â±8.37 3.67 Â±0.04 52.43 Â±1.75 45.37 Â±0.90\nDySAT[24] 354.35Â±8.47 2.69 Â±0.47 146.59 Â±2.36 17.64 Â±0.89 292.91 Â±4.28 4.63 Â±0.36 55.79 Â±1.78 56.150 Â±1.14\nContinuous-\ntime\nJODIE[14] 286.62Â±7.14 24.09 Â±0.92 125.88 Â±1.96 41.27 Â±1.03 289.53 Â±4.91 27.97 Â±1.35 56.13 Â±1.78 59.11 Â±1.94\nDeepCoevolve [4] 439.63Â±11.71 2.53 Â±0.21 227.74 Â±6.41 15.37 Â±0.26 376.23 Â±5.38 20.44 Â±0.68 49.70 Â±1.53 62.06 Â±1.02\nTGAT[34] 206.79Â±6.37 35.49 Â±1.15 87.03 Â±2.41 72.05 Â±1.37 199.24 Â±2.51 30.27 Â±0.92 45.11 Â±2.19 64.33 Â±2.66\nTGNs[23] 196.69Â±4.36 36.07 Â±1.39 75.47 Â±2.02 79.74 Â±1.67 192.36 Â±3.02 31.07 Â±0.85 56.34 Â±2.04 61.61 Â±2.73\nTDIG-MPNN [2] 160.89Â±1.58 39.37 Â±1.31 69.28 Â±1.34 79.82 Â±0.49 180.07 Â±2.43 33.29 Â±1.63 46.91 Â±0.49 71.46 Â±0.08\nTCL 137.57Â±1.70 45.53Â±0.37 54.33Â±1.05 81.17Â±0.16 149.31Â±0.72 35.32Â±0.48 33.95Â±1.27 75.95Â±0.47\nImprovement 14.49% 15.64% 21.58% 1.69% 16.01% 9.38% 24.74% 6.28%\nstrong performance verifies the superiority of TCL. (2)On average,\nthe continuous-time methods perform better than the static and\ndiscrete-time methods, which can be explained by the fact that the\nfine-grained temporal and structural information is critical for dy-\nnamic scenarios. (3)In some scenarios, the performance of discrete-\ntime dynamic methods is not better than that of static methods as\nexpected. Similar phenomenons have also been observed by previ-\nous work DyRep [27] and TDIG-MPNN. A possible explanation is\nthat itâ€™s non-trivial to specify the appropriate aggregation granular-\nity(i.e., the number of the snapshots) for these scenarios. (5)In some\nscenarios, the static methods GraphSage* perform competitive with\nthe continuous-time baselines. One possible reason could be that\nthere are many repetitive interactions in our datasets and recurring\ninteraction information can help models predict easily, especially\nfor static methods which can make full use of structural information.\n(6)TCL and recent work including TGAT, TGNs and TDIG-MPNN\nsurpass JODIE and DeepCoevolve by a large margin, which indi-\ncates the importance of exploiting information from high-order\ntemporal neighborhood(i.g., the k-depth sub-graph information\nused in TCL). (7) TCL performs relatively better than the recent\nwork TGAT, TGNs and TDIG-MPNN. The reasons could be two\nfolds. First, all these baseline methods treat temporal information\naggregations of two interactions nodes separately without consider-\ning the semantic relatedness between their temporal neighborhoods\n(i.e. history behaviors), which may be a causal factor for the tar-\nget interaction, while the proposed two-stream encoder can utilize\nthe co-attentional Transformer to capture inter-dependencies at\nsemantic level. Second, we use the contrastive learning loss as our\noptimization objective that enable our dynamic embeddings to pre-\nserve high-level (or global)semantics about interactions which is\nrobust to noise information.\n5.3 Ablation Study\nWe perform ablation studies on our TCL framework by removing\nspecific module one at a time to explore their relative importance.\nThe components validated in this section are the positional em-\nbedding, the cross-attentional Transformer, and the contrastive\nlearning strategy.\nPositional Embedding . We design the graph positional embed-\nding with the aim of enhancing the positional information of nodes.\nTo this end, the positional embedding module encodes both the\nTable 3: The comparison of TCL with its variants.\nDataset CollegeMsg Wikipedia LastFM Reddit\nMetrics MR â†“ MRâ†“ MRâ†“ MRâ†“\nTCL w/o TE 140.20 54.49 157.86 34.09\nTCL w/o DE 139.11 55.85 151.48 35.05\nTCL w/o CA-Transformer 146.18 57.42 152.31 35.39\nTwo-Stream-Encoder+TPP 143.31 62.78 153.28 40.61\nTCL 136.13 52.97 149.27 33.67\ntime interval information and depth information. To evaluate their\nimportance, we test the removal of time embedding (i.e., TCL w/o\nTE) and the removal of depth embedding (i.e., TCL w/o DE ). From\nthe results shown in Table 3, it can be observed that the perfor-\nmance degrades when removing either TE or DE, confirming the\neffectiveness of both time intervals between adjacent interactions\nand depth of the nodes.\nCross-attentional Transformer. To aid informative dynamic rep-\nresentations, we propose a two-stream encoder which includes a\ncross-attentional Transformer module to aggregate information\nfrom temporal neighborhood with the mutual influence captured.\nTo verify the effectiveness, we test the removal of the cross-attentional\nTransformer (i.e., TCL w/o CA-Transformer) in our two-stream\nencoder. As shown in Table 3, we find that TCL with the default\nsetting outperforms TCL w/o CA-Transformer over all datasets by\n5.04% on average, demonstrating the important role of the cross-\nattentional in our encoder.\nContrastive learning . To improve robustness to noisy interac-\ntions, we utilize the contrastive learning as the objective function\nthat maximizes the mutual information between the predictive rep-\nresentations of future interaction nodes. To evaluate its effective-\nness, we compare TCL with a variant that replaces the contrastive\nlearning by a TPP objective (i.e., Two-Stream-Encoder+TPP). It\ncan be observed that TCL outperforms Two-Stream-Encoder+TPP\nby a large margin, i.e., 9.76% on average, which demonstrates the\neffectiveness of our optimization strategy.\n5.4 Parameter Sensitivity\nWe investigate the impact of parameters on the future interaction\nprediction performance.\nTCL: Transformer-based Dynamic Graph Modelling via Contrastive Learning Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\n1 2 3 4 5\nDepth\n150\n200\n250MRR\nCollegeMsg\n1 2 3 4 5\nDepth\n55\n60\n65\nWikipedia\n1 2 3 4 5\nDepth\n160\n170\nLastFM\n1 2 3 4 5\nDepth\n35.0\n37.5\n40.0\n42.5\nReddit\nFigure 4: Performance of TCL w.r.t. different depths of the ğ‘˜-depth sub-graph.\n1 2 4 8 16\n#Head\n137\n138MRR\nCollegeMsg\n1 2 4 8 16\n#Head\n53\n54\n55\nWikipedia\n1 2 4 8 16\n#Head\n152\n154\n156\nLastFM\n1 2 4 8 16\n#Head\n33.0\n33.2\n33.4\n33.6\nReddit\nFigure 5: Performance of TCL w.r.t different number of heads.\n5.4.1 Impact of Sub-graph Depth. We explore how the depth of the\nsub-graph impacts the performance. We plot the MR metric of TCL\nin different depth on four datasets. The results are summarized in\nFig 4. We find that the performance gradually improves with the\nincreasing of depth, which verifies that exploiting information of\nhigh-order temporal neighborhood can benefit the performance.\n5.4.2 Impact of Attention Head Number. Multi-head attention al-\nlows the model to jointly attend to information from different rep-\nresentation subspaces. We attempt to see how the attention head\nnumber in our two-stream encoder impacts the performance. We\nplot the MR metric with different number of heads in Fig 5. We\nobserve that in most cases the performance improves when the\nhead number increases, which demonstrates the effectiveness of\nmulti-head attention. However, at some cases, more heads lead to\ndegraded performance due to the possible over-fitting problem.\n6 CONCLUSION\nIn this paper, we propose a novel continuous-time dynamic graph\nrepresentation learning method, called TCL. TCL generalizes the\nvanilla Transformer and obtains temporal-topological information\non dynamic graphs via a two-stream encoder. The proposed con-\ntrastive learning can preserve the high-level semantics of inter-\nactions and focus less on the low-level details, which is robust to\nnoise. Extensive experiments verify the effectiveness and stabil-\nity of TCL. In the future, there are still two important problems\nto be considered, i.e., how to effectively model long-term history\ninformation on a time-evolving graph and how to scale well.\nREFERENCES\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-\ntion. arXiv preprint arXiv:1607.06450 (2016).\n[2] Xiaofu Chang, Xuqin Liu, Jianfeng Wen, Shuang Li, Yanming Fang, Le Song, and\nYuan Qi. 2020. Continuous-Time Dynamic Graph Learning via Neural Interaction\nProcesses. In Proceedings of the 29th ACM International Conference on Information\n& Knowledge Management . 145â€“154.\n[3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.\nA simple framework for contrastive learning of visual representations. In ICML.\n1597â€“1607.\n[4] Hanjun Dai, Yichen Wang, Rakshit Trivedi, and Le Song. 2016. Deep coevolu-\ntionary network: Embedding user and item features for recommendation. arXiv\npreprint arXiv:1609.03675 (2016).\n[5] Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolu-\ntional neural networks on graphs with fast localized spectral filtering. neural\ninformation processing systems (2016), 3844â€“3852.\n[6] Palash Goyal, Nitin Kamra, Xinran He, and Yan Liu. 2018. Dyngem: Deep em-\nbedding method for dynamic graphs. arXiv preprint arXiv:1805.11273 (2018).\n[7] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation\nlearning on large graphs. In Advances in Neural Information Processing Systems .\n1024â€“1034.\n[8] Kaveh Hassani and Amir Hosein Khasahmadi. 2020. Contrastive multi-view\nrepresentation learning on graphs. In ICML. 4116â€“4126.\n[9] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\nWang. 2020. Lightgcn: Simplifying and powering graph convolution network for\nrecommendation. In SIGIR. 639â€“648.\n[10] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory.Neural\ncomputation 9, 8 (1997), 1735â€“1780.\n[11] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory.Neural\ncomputation 9, 8 (1997), 1735â€“1780.\n[12] Anish Khazane, Jonathan Rider, Max Serpe, Antonia Gogoglou, Keegan Hines,\nC Bayan Bruss, and Richard Serpe. 2019. Deeptrax: Embedding graphs of financial\ntransactions. In ICMLA. 126â€“133.\n[13] Tassilo Klein and Moin Nabi. 2020. Contrastive self-supervised learning for\ncommonsense reasoning. arXiv preprint arXiv:2005.00669 (2020).\n[14] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic em-\nbedding trajectory in temporal interaction networks. In Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery & Data Mining .\nACM, 1269â€“1278.\n[15] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network\nDataset Collection. http://snap.stanford.edu/data.\n[16] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effec-\ntive approaches to attention-based neural machine translation. arXiv preprint\narXiv:1508.04025 (2015).\n[17] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted\nboltzmann machines. In Icml.\n[18] Giang Hoang Nguyen, John Boaz Lee, Ryan A. Rossi, Nesreen K. Ahmed, Eunyee\nKoh, and Sungchul Kim. 2018. Continuous-Time Dynamic Network Embeddings.\nIn Companion of the The Web Conference 2018 on The Web Conference 2018, WWW\n2018, Lyon , France, April 23-27, 2018 . 969â€“976. https://doi.org/10.1145/3184558.\n3191526\n[19] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning\nwith contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).\n[20] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura,\nHiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. 2020. Evolvegcn:\nEvolving graph convolutional networks for dynamic graphs. In AAAI, Vol. 34.\n5363â€“5370.\n[21] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning\nof social representations. In Proceedings of the 20th ACM SIGKDD international\nconference on Knowledge discovery and data mining . ACM, 701â€“710.\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, Xiaofeng He, Le Song, Jingren Zhou, and Hongxia Yang\n[22] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker.\n2019. On variational bounds of mutual information. In ICML. 5171â€“5180.\n[23] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico\nMonti, and Michael Bronstein. 2020. Temporal Graph Networks for Deep Learning\non Dynamic Graphs. arXiv preprint arXiv:2006.10637 (2020).\n[24] Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. 2020.\nDySAT: Deep Neural Representation Learning on Dynamic Graphs via Self-\nAttention Networks. In WSDM. 519â€“527.\n[25] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. 2019. Infograph: Un-\nsupervised and semi-supervised graph-level representation learning via mutual\ninformation maximization. arXiv preprint arXiv:1908.01000 (2019).\n[26] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning\nwith neural networks. arXiv preprint arXiv:1409.3215 (2014).\n[27] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. 2019.\nDyrep: Learning representations over dynamic graphs. In ICLR.\n[28] Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario\nLucic. 2019. On mutual information maximization for representation learning.\narXiv preprint arXiv:1907.13625 (2019).\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762 (2017).\n[30] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\narXiv:1710.10903 (2017).\n[31] Petar Velickovic, William Fedus, William L Hamilton, Pietro LiÃ², Yoshua Bengio,\nand R Devon Hjelm. 2019. Deep Graph Infomax.. In ICLR.\n[32] Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation\nlearning through alignment and uniformity on the hypersphere. In ICML. 9929â€“\n9939.\n[33] Le Wu, Peijie Sun, Richang Hong, Yanjie Fu, Xiting Wang, and Meng Wang. 2018.\nSocialGCN: an efficient graph convolutional network based model for social\nrecommendation. arXiv preprint arXiv:1811.02815 (2018).\n[34] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.\n2020. Inductive Representation Learning on Temporal Graphs. arXiv preprint\narXiv:2002.07962 (2020).\n[35] Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. 2018. Graph\nconvolutional policy network for goal-directed molecular graph generation.arXiv\npreprint arXiv:1806.02473 (2018).\n[36] Jun Zhao, Zhou Zhou, Ziyu Guan, Wei Zhao, Wei Ning, Guang Qiu, and Xiaofei\nHe. 2019. Intentgc: a scalable graph convolution framework fusing heterogeneous\ninformation for recommendation. In SIGKDD. 2347â€“2357.\n[37] Lekui Zhou, Yang Yang, Xiang Ren, Fei Wu, and Yueting Zhuang. 2018. Dynamic\nnetwork embedding by modeling triadic closure process. In Thirty-Second AAAI\nConference on Artificial Intelligence .\n[38] Marinka Zitnik, Monica Agrawal, and Jure Leskovec. 2018. Modeling polyphar-\nmacy side effects with graph convolutional networks.Bioinformatics 34, 13 (2018),\ni457â€“i466.\nTCL: Transformer-based Dynamic Graph Modelling via Contrastive Learning Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nAlgorithm 1: TCL\nRequire: The dynamic interaction set D= {ğ‘™ğ‘¢ğ‘–,ğ‘£ğ‘–,ğ‘¡ğ‘–}ğ‘\nğ‘–=1; Depth ğ‘˜;\n# Heads ğ‘‘â„; # Blocks ğµ; # Epochs ğ¸, number of Negative\nsamples # NS . Initialize the parameters of Two-Stream\nEncoder ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ(Â·), and projection function ğœ™(Â·).\n1: for ğ‘’ in 1,2,...,ğ¸ do\n2: for (ğ‘¢ğ‘–,ğ‘£ğ‘–,ğ‘¡ğ‘–) in Ddo\n3: Select # NS negative items ğ‘¤ â‰  ğ‘£ğ‘– and extract the\ndependency k-depth sub-graphs of ğ‘¢ğ‘–, ğ‘£ğ‘–, ğ‘¤ as\n(Gğ‘¡ğ‘¢ğ‘–âˆ’(ğ‘¢(1)\nğ‘– ,ğ‘˜), Gğ‘¡ğ‘¢ğ‘–âˆ’(ğ‘¢(2)\nğ‘– ,ğ‘˜)),\n(Gğ‘¡ğ‘£ğ‘–âˆ’(ğ‘£(1)\nğ‘– ,ğ‘˜), Gğ‘¡ğ‘£ğ‘–âˆ’(ğ‘£(2)\nğ‘– ,ğ‘˜)),\n(Gğ‘¡ğ‘¤âˆ’(ğ‘¤(1),ğ‘˜), Gğ‘¡ğ‘¤âˆ’(ğ‘¤(2),ğ‘˜)).\n4: Obtain the behavior sequences of each sub-graph via BFS,\nSğ‘¢(1)\nğ‘– â†Gğ‘¡ğ‘¢ğ‘–âˆ’(ğ‘¢(1)\nğ‘– ,ğ‘˜), Sğ‘¢(2)\nğ‘– â†Gğ‘¡ğ‘¢ğ‘–âˆ’(ğ‘¢(2)\nğ‘– ,ğ‘˜),\nSğ‘£(1)\nğ‘– â†Gğ‘¡ğ‘¢ğ‘–âˆ’(ğ‘£(1)\nğ‘– ,ğ‘˜), Sğ‘£(2)\nğ‘– â†Gğ‘¡ğ‘¢ğ‘–âˆ’(ğ‘£(2)\nğ‘– ,ğ‘˜),\nSğ‘¤(1)\nâ†Gğ‘¡ğ‘¢ğ‘–âˆ’(ğ‘¤(1),ğ‘˜), Sğ‘¤(2)\nâ†Gğ‘¡ğ‘¢ğ‘–âˆ’(ğ‘¤(2),ğ‘˜).\n5: Encode each pair of dependency sub-graph via\nTwo-Stream Encoder ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ,\nhğ‘¢(1)\nğ‘– (ğ‘¡ğ‘¢ğ‘–âˆ’),hğ‘¢(2)\nğ‘– (ğ‘¡ğ‘¢ğ‘–âˆ’)â†ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ(Sğ‘¢(1)\nğ‘– ,Sğ‘¢(2)\nğ‘– ),\nhğ‘£(1)\nğ‘– (ğ‘¡ğ‘£ğ‘–âˆ’),hğ‘£(2)\nğ‘– (ğ‘¡ğ‘£ğ‘–âˆ’)â†ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ(Sğ‘£(1)\nğ‘– ,Sğ‘£(2)\nğ‘– ),\nhğ‘¤(1)(ğ‘¡ğ‘¤âˆ’),hğ‘¤(2)(ğ‘¡ğ‘¤âˆ’)â†ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ(Sğ‘¤(1)\n,Sğ‘¤(2)\n)\n6: Project dependency node embeddings to node embedding\nvia Future Prediction Function ğœ™,\n7: Â¯hğ‘¢ğ‘–(ğ‘¡)â†ğœ™([hu(1)\ni (tuiâˆ’); hu(2)\ni (tuiâˆ’)])\nÂ¯hğ‘£(ğ‘¡)â†ğœ™([hv(1)\ni (tviâˆ’); hv(2)\ni (tviâˆ’)\nÂ¯hğ‘¤(ğ‘¡)â†ğœ™([hğ‘¤(1)(ğ‘¡ğ‘¤âˆ’); hğ‘¤(2)(ğ‘¡ğ‘¤âˆ’)])\n8: Calculate the CL loss according to Eq. 23 and update the\nparameters of ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ and ğœ™.\n9: end for\n10: end for\nA REPRODUCIBILITY SUPPLEMENT\nA.1 Settings for Baselines\nTo enhance the reproducibility of this paper, we first give the pseu-\ndocode of our training process. Then we describe the implementa-\ntion of the baselines. Finally, we introduce the experimental envi-\nronment and hyperparameters settings.\nImplementation of Baselines . We compare TCL with three\ncategories of methods: (1) Static Methods: GraphSage1. (2) Discrete-\nTime Methods: CTDNE2, DynGEM3 and DySAT4. (3) Continuous-\ntime Methods: JODIE5, DeepCoevole, TGAT6, TGNs7 and TDIG-\nMPNN.\nFor GraphSage*, the maximum number of 1/2/3/4/5-hop neigh-\nbor nodes is set to be 25/10/10/10/10. For discrete-time methods,\nwe search the number of snapshots in {1,5,10,15} for all datasets.\nWe search learning rates in {0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1},\nbatchsize in {128, 256, 512} and keep the other hyper-parameters\nthe same as their published version to obtain the best results of\nthese methods. For continuous-time methods, we search the learn-\ning rates in {0.0001, 0.0005, 0.001, 0.005}, batch-size in {128, 256,\n512} to obtain the best results of these methods and keep the other\nhyper-parameters same with their published version. We set the\nembedding dimension as ğ‘‘ = 64 for all the methods, maximum\ntraining epochs 20 and 5 negative samples (except for DynGEM and\nJODIE which do not require negative samples) for a fair comparison.\nA.2 Settings for TCL\nTable 4: Hyper-parameters Settings.\nHyper-parameters Setting\nLearning rate 0.0005\nOptimizer Adam\nMini-batch size 512\nNode embedding dimension 64\nNumber of attention heads 16\nThe depth of the k sub-graph 5\nDropout ratio in the input 0.6\nNumber Training Epoch 20\nNumber of negative samples 5\nNumber of blocks ğµ 1\nDataSet Split. We split CollegeMsg into training/validation/test\nsets by 60/20/20. For a fair comparison with JODIE, we split dataset\nLastFM by 80/10/10. For a fair comparison with TGNT and TGNs,\nwe split Reddit and Wiki by 70/15/15.\nA.3 Pseudocode for TCL\nThe pseudocode of the training procedure for TCL is detailed in\nAlgorithm 1.\n1https://github.com/williamleif/GraphSAGE\n2https://github.com/stellargraph/stellargraph\n3https://github.com/palash1992/DynamicGEM\n4https://github.com/aravindsankar28/DySAT\n5https://github.com/srijankr/jodie\n6https://github.com/StatsDLMathsRecomSys/Inductive-representation-learning-on-\ntemporal-graphs\n7https://github.com/twitter-research/tgn"
}