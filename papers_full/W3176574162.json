{
    "title": "Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation",
    "url": "https://openalex.org/W3176574162",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2783116390",
            "name": "Shizhe Diao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109717562",
            "name": "Ruijia Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3175217892",
            "name": "Hongjin Su",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2475564498",
            "name": "Yilei Jiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103229335",
            "name": "Yan Song",
            "affiliations": [
                "Shenzhen Research Institute of Big Data"
            ]
        },
        {
            "id": "https://openalex.org/A1973384194",
            "name": "Tong Zhang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3099008231",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2963012544",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W2251584595",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2988937804",
        "https://openalex.org/W2983577274",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2124793550",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3035193825",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2955041501",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W4294554825",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W2279376656",
        "https://openalex.org/W3106165216",
        "https://openalex.org/W3098065087",
        "https://openalex.org/W2912630292",
        "https://openalex.org/W2963639288",
        "https://openalex.org/W3101638898",
        "https://openalex.org/W3094382446",
        "https://openalex.org/W3101662419",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4288087322",
        "https://openalex.org/W3171291687",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3159622706",
        "https://openalex.org/W3101893044",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2801930304",
        "https://openalex.org/W2790250716",
        "https://openalex.org/W3034379414",
        "https://openalex.org/W2995154514",
        "https://openalex.org/W2945864679",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2963718112",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2952867657",
        "https://openalex.org/W2808556605",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2027731328",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3034238904"
    ],
    "abstract": "Large pre-trained models such as BERT are known to improve different downstream NLP tasks, even when such a model is trained on a generic domain. Moreover, recent studies have shown that when large domain-specific corpora are available, continued pre-training on domain-specific data can further improve the performance of in-domain tasks. However, this practice requires significant domain-specific data and computational resources which may not always be available. In this paper, we aim to adapt a generic pretrained model with a relatively small amount of domain-specific data. We demonstrate that by explicitly incorporating the multi-granularity information of unseen and domain-specific words via the adaptation of (word based) n-grams, the performance of a generic pretrained model can be greatly improved. Specifically, we introduce a Transformer-based Domain-aware N-gram Adaptor, T-DNA, to effectively learn and incorporate the semantic representation of different combinations of words in the new domain. Experimental results illustrate the effectiveness of T-DNA on eight low-resource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities.(1)",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3336–3349\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3336\nTaming Pre-trained Language Models with N-gram Representations\nfor Low-Resource Domain Adaptation\nShizhe Diao♦, Ruijia Xu ♦, Hongjin Su ♣, Yilei Jiang ♣\nYan Song♠♥, Tong Zhang ♦\n♦The Hong Kong University of Science and Technology\n{sdiaoaa, rxuaq, tongzhang}@ust.hk\n♣The Chinese University of Hong Kong\n♠The Chinese University of Hong Kong (Shenzhen)\n♥Shenzhen Research Institute of Big Data\nsongyan@cuhk.edu.cn\nAbstract\nLarge pre-trained models such as BERT are\nknown to improve different downstream NLP\ntasks, even when such a model is trained on\na generic domain. Moreover, recent studies\nhave shown that when large domain-speciﬁc\ncorpora are available, continued pre-training\non domain-speciﬁc data can further improve\nthe performance of in-domain tasks. How-\never, this practice requires signiﬁcant domain-\nspeciﬁc data and computational resources\nwhich may not always be available. In this\npaper, we aim to adapt a generic pretrained\nmodel with a relatively small amount of\ndomain-speciﬁc data. We demonstrate that by\nexplicitly incorporating the multi-granularity\ninformation of unseen and domain-speciﬁc\nwords via the adaptation of (word based) n-\ngrams, the performance of a generic pretrained\nmodel can be greatly improved. Speciﬁcally,\nwe introduce a Transformer-based Domain-\naware N-gram Adaptor, T-DNA, to effectively\nlearn and incorporate the semantic represen-\ntation of different combinations of words in\nthe new domain. Experimental results illus-\ntrate the effectiveness of T-DNA on eight low-\nresource downstream tasks from four domains.\nWe show that T-DNA is able to achieve sig-\nniﬁcant improvements compared to existing\nmethods on most tasks using limited data with\nlower computational costs. Moreover, further\nanalyses demonstrate the importance and ef-\nfectiveness of both unseen words and the in-\nformation of different granularities.1\n1 Introduction\nPre-trained language models have achieved great\nsuccess and shown promise in various applica-\ntion scenarios across natural language understand-\ning (Devlin et al., 2019; Liu et al., 2019; Tian et al.,\n2020a) and generation (Lewis et al., 2020; Zhang\n1Our code is available at https://github.com/\nshizhediao/T-DNA.\net al., 2020; Yang et al., 2020). Normally applying\npre-trained language models to different applica-\ntions follows a two-stage paradigm: pre-training on\na large unlabeled corpus and then ﬁne-tuning on a\ndownstream task dataset. However, when there are\ndomain gaps between pre-training and ﬁne-tuning\ndata, previous studies (Beltagy et al., 2019; Lee\net al., 2020) have observed a performance drop\ncaused by the incapability of generalization to new\ndomains. Towards ﬁlling the gaps, the main re-\nsearch stream (Beltagy et al., 2019; Alsentzer et al.,\n2019; Huang et al., 2019; Lee et al., 2020) on\nadapting pre-trained language models starts from\na generic model (e.g., BERT, RoBERTa) and then\ncontinues pre-training with similar objectives on\na large-scale domain-speciﬁc corpus. However,\nwithout providing sufﬁcient understanding of the\nreason for the performance drop during the domain\nshift, it is prone to failure of adaptation. There-\nfore, many aspects of continuous pre-training are\nexpected to be enhanced. First, although generic\npre-trained models offer better initialization for\ncontinuous pre-training models, it still costs con-\nsiderable time (and money) that are beyond the\nreach of many institutions.2 Second, it is clumsy\nto pre-train domain-speciﬁc models repeatedly for\neach domain on large-scale corpora.3 Therefore, it\nis helpful to have an efﬁcient and ﬂexible method\nfor being able to adapt pre-trained language models\nto different domains requiring limited resources.\nStarting from the observed vocabulary mismatch\nproblem (Gururangan et al., 2020), we further show\nempirically that the domain gap is largely caused by\ndomain-speciﬁc n-grams.4 Motivated by this ﬁnd-\n2For example, BioBERT (Lee et al., 2020), initialized by\ngeneric BERT, was trained on biomedical corpora for 23 days\non eight NVIDIA V100 GPUs.\n3For example, SciBERT (Beltagy et al., 2019) needs to\nbe trained from scratch if one wants to use a domain-speciﬁc\nvocabulary (i.e., SciV ocab in their paper).\n4We explain it in detail in the following section.\n3337\ning, we propose a light-weight Transformer-based\nDomain-aware N-gram Adaptor (T-DNA) by in-\ncorporating n-gram representations to bridge the\ndomain gap between source and target vocabulary.\nSpeciﬁcally, the proposed model is able to explic-\nitly learn and incorporate better representations of\ndomain-speciﬁc words and phrases (in the form of\nn-grams) by the adaptor networks with only requir-\ning small pieces of data. With this adaptor, once\nentering a new domain, one can choose to train the\nadaptor alone or train it with a Transformer-based\nbackbone (e.g., BERT) together, where the joint\ntraining paradigm could provide more improve-\nment. In addition, although it is designed for a low-\nresource setting, the adaptor is still able to work\nwith enough data, which ensures its generalization\nability in different scenarios.\nExperimental results demonstrate that T-DNA\nsigniﬁcantly improves domain adaptation perfor-\nmance based on a generic pre-trained model and\noutperforms all baselines on eight classiﬁcation\ntasks (on eight datasets). The results conﬁrm that\nincorporating domain-speciﬁc n-grams with the\nproposed T-DNA is an effective and efﬁcient solu-\ntion to domain adaptation, showing that the infor-\nmation carried by larger text granularity is highly\nimportant for language processing across domains.\nMoreover, further analyses investigate the factors\nthat may inﬂuence the performance of our model,\nsuch as the amount of available data, the train-\ning time cost and efﬁciency, and the granularity\nof domain-speciﬁc information, revealing the best\nway and setting for using the model.\n2 The Motivation\nAs observed in Gururangan et al. (2020), the trans-\nfer gain of domain-speciﬁc pre-training becomes\nincreasingly signiﬁcant when the source and tar-\nget domain are vastly dissimilar in terms of the\nvocabulary overlap. Motivated by this association\nbetween transfer gain and vocabulary distribution,\nwe further investigate the shift of words and phrases\nacross domains and attempt to alleviate the degra-\ndation of language models without large domain-\nspeciﬁc corpora.\nIn particular, we start with a RoBERTa-base\nmodel from the generic domain and then ﬁne-tune\nit on the IMDB (Maas et al., 2011) dataset. We\ninvestigate the outputs predicted by the [CLS] em-\nbedding on the IMDB development set and divide\nthem into two categories: correct predictions (true\n1-gram 2-gram 3-gram 4-gram 5-gram\nGranularity\n40\n50\n60\n70\n80\n90\n100Ratio\nLabel\ncorrect\nfalse\nFigure 1: The proportion of domain-speciﬁc n-grams\nin correct predictions and false predictions over 10 dif-\nferent random seeds.\npositive/negative) and false predictions (false pos-\nitive/false negative). To examine the vocabulary\nmismatch problem during the domain shift, we\nextract the top 1K most frequent n-grams 5 from\nthese two categories respectively. We identify the\nn-grams not in the top 10K most frequent n-grams\nof source data 6 as domain-speciﬁc n-grams. As\nrevealed in Figure 1, a larger proportion of domain-\nspeciﬁc n-grams are captured when the model is\nmisled to make wrong predictions, which suggests\nthat the shifts in semantic meaning for both words\nand phrases might account for the domain shift.\nFurthermore, we conjecture that the representations\nof domain-speciﬁc n-grams are unreliable, which\nexacerbates the model degradation. While more\ndetails will be presented in §6.3, we brieﬂy men-\ntion here that the tokens usually improperly attend\nto other tokens in the sentence but omit the most\nimportant words and phrases.\nIn light of this empirical evidence, we are moti-\nvated to design a framework to not only capture the\ndomain-speciﬁc n-grams but also reliably embed\nthem to extrapolate in the novel domain.\n3 The T-DNA\nOur approach follows the standard recipe of pre-\ntraining and ﬁne-tuning a language model, which\nreceives a sentence X = t1t2 ···ti ···tT with\nti indicating the i-th token, and outputs the rep-\nresentation of each token. The overall architec-\nture of our approach is shown in Figure 2. In\nthe middle, a generic pre-trained encoder, such\n5Here we set n to 5.\n6We sample a subset from English Wikipedia.\n3338\nTokenEmbeddingLayer\nInputSubjectiveeffects,psychomotortaskperformance,andphysiologicalmeasureswere…\nsub\nsubjective\neffects\njectiveeffects,psychomotor\nsubjectivepsychomotorphychomotor\ntask…\nphysiological\nmeasuresphysiological\npsychomotor\ntaskperformance\ntask\n…\nTokenization\nPositional Encoding+\nAdd & NormFeedForwardAdd & NormMulti-Head Attention●\n●\n+!\nAdd & NormFeedForwardAdd & NormMulti-Head Attention●\n●\nN-gramExtractionModule\nN-gramEmbeddingLayersubjectivepsychomotorphysiologicalPsychomotortaskperformancesubjectiveeffects\n……DomainLexicon\n\"\n…!′\nTokenization\nperformance…\nFigure 2: The overall architecture of our model.\nas BERT or RoBERTa, provides a representation\nat the subword-level without any target domain\nknowledge. The right-hand side shows the pro-\nposed T-DNA to enhance the backbone pre-trained\nencoder, where word based n-grams in X are ex-\ntracted from a pre-constructed lexicon L, and are\nrepresented through n-gram attention module. The\nleft-hand side shows the n-gram matching matrix\nand the integrating process of domain-speciﬁc rep-\nresentation and generic encoding.\nIn this section, we start with a detailed descrip-\ntion of lexicon construction, then introduce our\nn-gram encoding module and how to integrate n-\ngram encoding with the backbone model to get\ndomain-aware representation, and end with an il-\nlustration of two training strategies.\n3.1 Lexicon Construction and N-gram\nExtraction\nTo better represent and incorporate unseen and\ndomain-speciﬁc n-grams, we ﬁrst need to ﬁnd and\nextract them. Here we propose to use an unsuper-\nvised method, pointwise mutual information (PMI),\nto ﬁnd domain-speciﬁc words and phrases by col-\nlocations and associations between words.\nGiven a sentence X = x1x2 ···xK with K\nwords, for any two adjacent words (e.g., ¯x, ˜x)\nwithin the sentence, their PMI is calculated by\nPMI (¯x,˜x) = log p(¯x˜x)\np(¯x)p(˜x), (1)\nwhere p(x) is the probability of an n-gramx. When\na high PMI score is detected between the adja-\ncent ¯xand ˜x, it suggests they are good collocation\npairs, because they have a high probability of co-\noccurrence and are more likely to form an n-gram.\nOn the contrary, a delimiter is inserted between\nthe two adjacent words if their PMI (¯x,˜x) is less\nthan a threshold σ, i.e., X= x1x2 ··· ¯x/˜x···xK.\nAs a result, those consecutive words without a de-\nlimiter are identiﬁed as candidate domain-speciﬁc\nn-grams. After using PMI to segment each sen-\ntence in the training set of a target task, we could\nselect among candidate n-grams to obtain the ﬁnal\nn-gram lexicon L, where each n-gram appears with\na frequency of at least f.\nIn light of this lexicon, for each training in-\nput sentence X= t1t2 ···ti ···tT with T tokens,\nwhere ti denotes the i-th token of X, we extract\nthose sub-strings of X that exist in the lexicon\nto form domain-speciﬁc n-gram sequence S =\ns1s2,··· ,sj,··· ,sN , with sj indicating the j-th\nn-gram of X. At the same time, an n-gram match-\ning matrix, M∈ RT×N , can be built to record the\n3339\npositions of the extracted domain-speciﬁc n-gram\nset and its associated tokens, where mij = 1 for\nti ∈sj and mij = 0 for ti /∈sj. The matching\nmatrix is shown in the left hand size of Figure 2.\n3.2 Domain-aware Representation\nThe backbone pre-trained encoder is a Transformer\narchitecture (Vaswani et al., 2017) withLlayers, S\nself-attention heads and H hidden dimensions ini-\ntialized from any pre-trained encoder (e.g., BERT\nor RoBERTa). The input sentence is passed through\nit, resulting in a generic hidden state hi for each\ninput token xi. To get the domain-aware hidden\nrepresentation, the n-gram adaptor network is im-\nplemented by a Transformer encoder with llayers,\nS self-attention heads and H hidden dimensions.\nFirst, the embeddings of domain-speciﬁc n-grams\ncould be obtained by an n-gram embedding layer\nand then they are fed into the n-gram encoder to\nget a sequence of hidden states gvia a multi-head\nattention mechanism. The n-gram encoder is able\nto model the interactions among all extracted n-\ngrams and dynamically weighs n-grams to empha-\nsize truly useful n-grams and ignores noisy infor-\nmation. The combination of the generic representa-\ntion and domain-speciﬁc n-gram representation are\ncomputed by\nh′\ni = hi +\n∑\nk\ngi,k, (2)\nwhere h′\ni is the desired domain-aware representa-\ntion, and gi,k is the resulting hidden state for the\ni-th token and the k-th n-gram associated with this\ntoken according to the matching matrix M. The n-\ngram encoding process and hidden state integration\nis repeated layer-by-layer along with the generic\nencoder for llayers from the bottom.\n3.3 Training Strategies\nSeveral training strategies could be used and we\nadopt two in our experiments: ﬁne-tuning (FT)\nand task-adaptive pre-training (TAPT). For ﬁne-\ntuning, we operate on the hidden state of the special\nclassiﬁcation token [CLS]. Following the tradition\ncitation, we simply add a fully-connected layer\nas a classiﬁer on top of the model and obtain the\nprobabilities via a softmax layer. The classiﬁer and\nthe whole model are ﬁne-tuned on the labeled task\ndata in the target domain with cross-entropy loss.\nTo inject unsupervised target domain knowledge,\nwe leverage the task-adaptive pre-training proposed\nin (Gururangan et al., 2020) which strips the labels\nin downstream task training data and trains the\nmodel on this unlabeled data. We use the masked\nlanguage model (MLM) as our objective and do\nnot include the next sentence prediction (NSP) task\nfollowing Liu et al. (2019); Lan et al. (2020).\nNote that, our model also supports other train-\ning strategies such as domain-adaptive pre-training,\nwhich proves to be effective in Gururangan et al.\n(2020). One can pre-train our model on a far larger\ndomain corpus (normally beyond 10GB) at the be-\nginning, and then do the task-adaptive pre-training\nand ﬁne-tuning. Because our main goal is to adapt\nour model in a low-resource setting in terms of data\nsize and time cost, we leave it for future research.7\n4 Experiment Settings\nIn this section, we ﬁrst introduce eight benchmark-\ning datasets. Then the baseline models, evaluation\nmetrics, and implementation details are presented\nin the following three subsections, respectively.\n4.1 Datasets\nFollowing Gururangan et al. (2020), we conduct\nour experiments on eight classiﬁcation tasks from\nfour domains including biomedical sciences, com-\nputer science, news and reviews. The datasets are\ndescribed as follows.\n•CHEM PROT (Kringelum et al., 2016), a man-\nually annotated chemical–protein interaction\ndataset extracted from 5,031 abstracts for rela-\ntion classiﬁcation.\n•RCT (Dernoncourt and Lee, 2017), which con-\ntains approximately 200,000 abstracts from pub-\nlic medicine with the role of each sentence\nclearly identiﬁed.\n•CITATION INTENT (Jurgens et al., 2018), which\ncontains around 2,000 citations annotated for\ntheir function.\n•SCIERC (Luan et al., 2018), which consists\nof 500 scientiﬁc abstracts annotated for relation\nclassiﬁcation.\n•HYPER PARTISAN (Kiesel et al., 2019), which\ncontains 645 articles from Hyperpartisan news\nwith either extreme left-wing or right-wing stand-\npoint used for partisanship classiﬁcation.\n•AGNEWS (Zhang et al., 2015), consisting of\n127,600 categorized articles from more than\n2000 news source for topic classiﬁcation.\n7We show some analyses and discussion of data size in\nSection 6.2.\n3340\nDOMAIN BIOMED CS N EWS REVIEWS\nDATASET CP RCT CI SE HP AG AM IMDB\nTRAIN\nS# 4.1K 1.8K 1.6K 3.2K 516 1.1K 1.1K 2.0K\nT# 895K 267K 376K 619K 1.7M 213K 1.0M 2.6M\nO.S# 4.1K 180K 1.6K 3.2K 516 115K 115K 20K\nO.T# 895K 27.4M 376K 619K 1.7M 21.4M 98.9M 25.9M\nDEV S# 2.4K 30K 114 455 64 5K 5K 5K\nT# 547K 4.6M 24K 89K 194K 929K 4.4M 6.6M\nTEST S# 3.4K 30K 139 974 65 7.6K 25K 25K\nT# 773K 4.6M 31K 187K 238K 1.4M 21.5M 31.8M\nCLASSES 13 5 6 7 2 4 2 2\nTable 1: The statistics of the eight task datasets in four target domains. To limit the computational resources and\nmaintain all datasets on thousand-level, we only take 10% of IMDB training set, and 1% of RCT, AG and AM\ntraining sets. O.S# and O.T# refer to the number of sentences and the number of tokens in the original datasets,\nrespectively. S# denotes the number of sentences and T# is the number of tokens. CP, CI, SE, HP, AG and AM\ndenote CHEM PROT, CITATION INTENT , SCIERC, H YPER PARTISAN ,AGN EWS and AMAZON , respectively.\n•AMAZON (McAuley et al., 2015), consisting of\n145,251 reviews on Women’s and Men’s Cloth-\ning & Accessories, each representing users’ im-\nplicit feedback on items with a binary label sig-\nnifying whether the majority of customers found\nthe review helpful.\n•IMDB (Maas et al., 2011), 50,000 balanced\npositive and negative reviews from the Internet\nMovie Database for sentiment classiﬁcation.\nTo create a low-resource setting, we constrain\nthe size of all datasets into thousand-level. To do so,\nwe randomly select a subset for RCT, AG, Amazon,\nIMDB with the ratio 1%, 1%, 1%, 10%, respec-\ntively. The details can be found in Table 1.\n4.2 Baselines\nIn our experiments, the following two models serve\nas the main baselines.\n•ROBERTA+FT: ﬁne-tuned off-the-shelf\nRoBERTa-base model for downstream tasks.\n•ROBERTA+TAPT : task-adaptive pre-trained\non unlabeled task data starting from RoBERTa\nand then ﬁne-tuned on labeled data.\n4.3 Evaluation Metrics\nFollowing Beltagy et al. (2019), we adopt macro-\nF1 for CitationIntent, SciERC, HyperPartisan,\nAGNews, Amazon, IMDB, and micro-F1 for\nChemProt and RCT as evaluation metrics. Macro-\nF1 will compute the F1 metric independently for\neach class and then take the average, whereas\nmicro-F1 will aggregate the contributions of all\nclasses to compute the average metric. In a\nmulti-class classiﬁcation setup, micro-F1 is prefer-\nable if there is class imbalance, which is true for\nChemProt and RCT.\n4.4 Implementation\nWe implement the RoBERTa-base architecture and\ninitialize it with pre-trained weights by Hugging-\nface’s Transformers library8. In order to obtain\na fast and warm start for n-gram representations,\nwe utilize fastText (Bojanowski et al., 2017) to ini-\ntialize n-gram embeddings. Considering the small\namount of data and based on our experience, the\nnumber of N-gram encoding layers lis set to 1.\nFor unsupervised task-adaptive pre-training\n(TAPT), the batch size is set to 16 and training\nepochs range from 10 to 15. We adopt Adam\n(Kingma and Ba, 2015) as the optimizer , where the\ncorresponding learning rates of different datasets\ncan be found in our code. The dropout rate is set to\n0.5. For the task-speciﬁc ﬁne-tuning (FT), we use\nsimilar hyperparameter settings and the details are\nelaborated in the Appendix. All the experiments\nare implemented on Nvidia V100 GPUs.\n5 Experimental Results\nWe compare the performance of the RoBERTa\nmodel with and without T-DNA on the aforemen-\ntioned datasets. In both ﬁne-tuning and task adap-\ntive pre-training experiments, T-DNA shows sig-\nniﬁcant improvements over the pre-trained generic\nRoBERTa.\n8https://github.com/huggingface/transformers\n3341\nDOMAIN BIOMED CS N EWS REVIEWS\nDATASET CP RCT CI SE HP AG AM IMDB\nRoBERTa+FT 81.100.70 80.720.40 56.745.47 74.065.25 88.151.51 88.600.01 63.040.69 92.290.23\n+T-DNA 82.660.31 81.520.41 64.954.98 78.612.00 92.490.69 88.910.06 63.920.62 92.910.71\nRoBERTa+TAPT 82.241.33 82.730.23 63.442.30 77.851.12 92.700.73 88.840.01 64.130.22 92.770.25\n+T-DNA 83.890.76 83.940.27 69.732.87 79.400.48 93.911.48 89.050.03 64.360.34 93.130.15\nTable 2: The overall performance of T-DNA and the comparison against existing models on eight target down-\nstream datasts. We report average scores across ﬁve random seeds, with standard deviations as subscripts.\n5.1 Fine-Tuning\nThe results of ﬁne-tuning on eight datasets are re-\nported in Table 4. In general, the RoBERTa model\nwith T-DNA outperforms that without T-DNA on\nall datasets, clearly indicating the effectiveness of\nT-DNA by emphasizing multi-granularity infor-\nmation. On average, T-DNA is able to bring an\nimprovement of performance by around 2.66%.\nAcross all eight datasets, it is observed that T-\nDNA achieves the greatest improvement (8.21%)\non the CitationIntent dataset and the least improve-\nment on the AGNews dataset. One reasonable ex-\nplanation for different improvements is that the\ndomain gap between the RoBERTa pre-training\ndomain and the CS domain is the greatest so that\nfar more gains could be obtained by an effective\nadaptation strategy. To conﬁrm this, we follow Gu-\nrurangan et al. (2020) to characterize the domain\nsimilarity by analyzing vocabulary overlap and we\ndraw the same conclustion that RoBERTa’s pre-\ntraining domain has a similar vocabulary to News\nand Reviews, but far more dissimilar vocabulary to\nBioMed and CS. In light of this observation, we\nrecognize that the proposed method is more appli-\ncable when the domain gap is large. In this sce-\nnario, the potential of incorporating multi-grained\ninformation by domain-speciﬁc n-grams is greatly\nexploited to boost the performance of adaptation.\nWhen comparing the improvements over four\ndomains, T-DNA is able to offer 1.18%, 6.38%,\n2.33%, 0.75% gains on BioMed, CS, News, Re-\nviews, respectively. The improvement on the CS\ndomain is the best while on the Reviews domain\nit is the poorest, which is consistent with previous\nanalyses across datasets for similar reasons.\n5.2 Task-Adaptive Pre-Training\nIn the previous section, we show that T-DNA is\nhelpful in ﬁne-tuning. Additionally, we would like\nto explore whether T-DNA is complementary to\nmore training strategies, such as task-adaptive pre-\ntraining (TAPT). TAPT has been shown useful for\n0-gram 1-gram 2-gram 3-gram\ngranularity of n-grams\n55\n60\n65\n70\n75\n80\n85\n90performance\nCP\nRCT\nCI\nSE\nHP\nAG\nAM\nIMDB\nFigure 3: Effects of Different Granularities\n(N=0,1,2,3).\npre-trained models in previous studies (Howard\nand Ruder, 2018; Gururangan et al., 2020), by pre-\ntraining on the unlabeled task dataset drawn from\nthe task distribution. The experimental results of\ntwo models with and without T-DNA are reported\nin the bottom two rows in Table 4. From the re-\nsults, we can clearly see that the model with T-\nDNA achieves better performance on all datasets\ncompared to the generic RoBERTa model with-\nout T-DNA. The T-DNA helps to improve the\nperformance by approximately 1.59% on average,\nwhich shows that the effectiveness of T-DNA does\nnot vanish when combined with TAPT. Instead,\nit further leads to a large performance boost for\npre-trained models, indicating that T-DNA is a\ncomplementary approach, where explicitly model-\ning domain-speciﬁc information helps the unsuper-\nvised learning of representations (i.e., the masked\nlanguage model (MLM) pre-training objective).\nOverall, for both FT and TAPT experiments, the\nresults show that T-DNA signiﬁcantly improves\ndomain adaptation performance based on a generic\npre-trained model. We attribute this improvement\nto the essential domain-speciﬁc semantic informa-\ntion that is carried by n-grams and the valid repre-\nsentation of n-grams from the T-DNA network.\n6 Analyses\nWe analyze several aspects of T-DNA, including\nthe effects of different granularities and the effects\n3342\nTask RCT AG AM IMDB\nModel w.o w. w.o w. w.o w. w.o w.\n10% 80.78 82.23↑1.45 90.11 92.01↑1.90 63.13 64.10↑0.97 92.29 92.91↑0.62\n20% 85.22 86.16↑0.94 91.71 92.14↑0.43 64.01 65.12↑1.11 92.11 92.89↑0.78\n50% 87.10 87.69↑0.59 92.17 92.58↑0.41 65.52 66.10↑0.58 93.13 93.32↑0.19\n100% 87.31 87.69↑0.38 93.75 94.00↑0.25 66.79 67.14↑0.35 94.34 94.81↑0.47\nTable 3: Performance gains of T-DNA w.r.t. different sampling ratios of RCT, AG, AM and IMDB datasets. w.\nand w.o indicate whether the model is equipped with T-DNA or not. The uparrow marks where a positive gain is\nobtained.\nof data size. In addition, we examine the attention\nmechanism to verify the effects of n-gram repre-\nsentations during the domain shift. The details are\nillustrated in this section.\n6.1 Effects of Different Granularities\nThe lexical unit in RoBERTa is a subword obtained\nfrom byte pair encoding (BPE) (Sennrich et al.,\n2016) tokenization, resulting in a smaller token\nspace and more training data for each token. Our\napproach provides coarse-grained information car-\nried by the larger lexical units, n-gram.\nTo verify the contribution of larger granularity\ninformation, we compare the improvement brought\nby T-DNA with information of different granular-\nities, for n from 0 to 3. Note that here n means\nthat we extract and incorporate all n-grams with a\nlength smaller or equal to n(within a certain granu-\nlarity). For example, n= 3 means that we include\nall unigrams, bigrams and trigrams. Two consis-\ntent observations could be made. First, adding\nonly 1-gram is able to bring improvements over\n0-gram (i.e., without T-DNA) on all eight datasets,\nas shown in Figure 3. As we know, the tokens in the\ngeneric encoder are at the subword-level and our\nunigrams are at the word-level, which can be seen\nas a combination of subwords. Therefore, the re-\nsults suggest that adding unseen words through our\nadaptor network is effective, which could enhance\nthe interaction between subwords of the same word,\nespecially for the new words in the target domain.\nMoreover, based on 1-gram, involving larger\ngranularity offer further gains. Comparing 2-gram\nand 3-gram v.s. 1-gram, the consistent improve-\nments of T-DNA demonstrate that the potential\nboundary information presented by n-grams plays\nan essential role in learning representations by pro-\nviding explicit and better guidance.\n6.2 Effects of Data Size\nIn the previous section, we explored the virtue\nof incorporating multi-grained information under\nresource-limited settings, where only a small sub-\nset of speciﬁc datasets can be accessed. In addition,\nwe are curious whether T-DNA could work well\non a larger scale. To this end, we sample differ-\nent ratios (i.e., 10%, 20%, 50%, 100%) of four\ndatasets (i.e., RCT, AGNews, Amazon and IMDB)\nand investigate how T-DNA performs at different\ndata scales. As shown in Table 3, the model with\nT-DNA always outperforms that without T-DNA\nw.r.t. any subsets of four datasets. This demon-\nstrates that models with T-DNA could easily adapt\nto any size of dataset with the help of domain-\nspeciﬁc n-gram information. However, it is also\nnoted that the performance gains of our method\ndecayed with the increase of the amount of training\ndata, dropping from 1.24% (proportion=10%) to\n0.36% (proportion=100%). It is not surprising be-\ncause with adequate data, a model is able to learn a\ngood representation with supervised learning with-\nout the need of prior knowledge. However, since\nsufﬁcient data normally could not be accessed in re-\nality, especially labeled data, we argue thatT-DNA\nis desirable and necessary for domain adaptation.\n6.3 Visualization of N-gram Representations\nTo verify the effects of n-gram representations dur-\ning the domain shift, we examine the attention\nmechanism of RoBERTa andT-DNA by plotting\nthe attention maps and salience maps using the\nLIT tool (Tenney et al., 2020). In the attention\nmap of RoBERTa withoutT-DNA, we found that\nthe tokens usually improperly attend to other to-\nkens in the sentence. For example, in Figure 4,\n“Barbie” attributes more attentions to “animated”\nand “scary” but omits “creepy” and fails to capture\n“scary as hell” as an integrated phase. In contrast,\nwhen the model is equipped with T-DNA, this vari-\nant will shift its attention to include “creepy” and\n3343\nmodel attentionmapsandsaliencemaps predictionlabel\nRoBERTa positivenegative\nRoBERTa+T-DNA negativenegative\nThatcreepyanimatedBarbieisscaryashell!I wanttostoptalkingabouthernow\nThatcreepyanimatedBarbieisscaryashell!Iwanttostoptalkingabouthernow.\n.\nThatcreepyanimatedBarbieisscaryashell!Iwanttostoptalkingabouthernow.\nThatcreepyanimatedBarbieisscaryashell!I wanttostoptalkingabouthernow\nThatcreepyanimatedBarbieisscaryashell!Iwanttostoptalkingabouthernow.\n.\nThatcreepyanimatedBarbieisscaryashell!Iwanttostoptalkingabouthernow.\nFigure 4: The visualization of attention maps and salience maps of RoBERTa and RoBERTa+T-DNA. The upper\nregion of each row shows the attention map, where thicker lines denote higher attention weights. The bottom region\nillustrates the salience map, where the darker color box denotes the more dominant weights for the prediction.\nforce the model to focus on the informative phrase\n“scary as hell ”. Furthermore, the salience map\nof RoBERTa without T-DNA suggests that “an-\nimated” and “scary” dominate its prediction while\n“creepy” and “scary as hell” are captured by our T-\nDNA, which is consistent with the decision process\nof human beings.\nDue to the space limitations, more visualized\nexamples are not shown here. However, based on\nconsiderable empirical evidence, we conclude that\nthe unreliable representations of domain-speciﬁc\nn-grams (words and phrases) might be one of the\nmain causes for model degradation.\n7 Related Work\nA large performance drop of pre-trained models\ncaused by domain shift has been observed and\nmany domain-speciﬁc BERT models (Beltagy et al.,\n2019; Alsentzer et al., 2019; Huang et al., 2019;\nLee et al., 2020) have been introduced to bridge the\ndomain gap. For example, SciBERT (Beltagy et al.,\n2019) is trained on 1.14M scientiﬁc papers from\nSemantic Scholar corpus (Ammar et al., 2018) for\n7 days on TPU v3-8 machine and BioBERT (Lee\net al., 2020) is trained on PubMed abstracts and\nPMC full text articles for 23 days on eight NVIDIA\nV100 GPUs. ClinicalBERT (Alsentzer et al., 2019)\nis trained on about 2 million notes in the MIMIC-III\nv1.4 database (Johnson et al., 2016) for 17-18 days\non a single GeForce GTX TITAN X 12 GB GPU.\nHowever, they all incur a huge computational cost,\nwhich is not affordable for many university labs\nor institutions. This is precisely why we believe\nthat our efﬁcient adaptor is useful to the commu-\nnity. Although Gururangan et al. (2020) introduced\ntask-adaptive pre-training (TAPT) to save time by\ntraining on unlabeled downstream task data, we\ndemonstrate that our plug-in adaptor is faster and\nmore effective because of the explicit learning strat-\negy and efﬁcient model architecture.\nOut of vocabulary (OOV) words refer to those\nwords that are not in the vocabulary list and have\nreceived a lot of attention in recent years. One way\nto handle OOV words is to simply utilize and learn\nan “unknown” embedding during training. Another\nway is to add in-domain words into the original vo-\ncabulary list and learn their representation by pre-\ntraining from scratch (Beltagy et al., 2019; Gu et al.,\n2020), which requires substantial resources and\ntraining data. Moreover, SciBERT (Beltagy et al.,\n2019) found that in-domain vocabulary is helpful\nbut not signiﬁcant while we attribute it to the inefﬁ-\nciency of implicit learning of in-domain vocabulary.\nTo represent OOV words in multilingual settings,\nthe mixture mapping method (Wang et al., 2019)\nutilized a mixture of English subwords embedding,\nbut it has been shown useless for domain-speciﬁc\n3344\nwords by Tai et al. (2020). ExBERT (Tai et al.,\n2020) applied an extension module to adapt an aug-\nmenting embedding for the in-domain vocabulary\nbut it still needs large continuous pre-training. Sim-\nilar to our work, they highlight the importance of\nthe domain-speciﬁc words but all of these work nei-\nther explore the understanding of performance drop\nduring a domain shift nor examine the importance\nof multi-grained information. Large granularity\ncontextual information carried by spans or n-grams\nhas proven to be helpful to enhance text representa-\ntion for Chinese (Song et al., 2009; Song and Xia,\n2012; Ouyang et al., 2017; Kim et al., 2018; Peng\net al., 2018; Higashiyama et al., 2019; Tian et al.,\n2020e,b; Li et al., 2020; Diao et al., 2020; Song\net al., 2021) and English (Joshi et al., 2020; Xiao\net al., 2020; Tian et al., 2020c,d). In addition to text\nencoders on pre-training, the kNN-LM (Khandel-\nwal et al., 2019) proposes to augment the language\nmodel for effective domain adaptation, by varying\nthe nearest neighbor datastore of similar contexts\nwithout further training. However, all of the previ-\nous studies focused on either general pre-training\nprocedures or different tasks (e.g., language model-\ning), and did not explore the effectiveness of multi-\ngrained information for domain adaptation. We\nhence view them as orthogonal to our work.\n8 Conclusion\nIn this work, we ﬁrst reveal a novel discovery be-\nhind the performance drop during a domain shift,\ndemonstrating that an unreliable representation of\ndomain-speciﬁc n-grams causes the failure of adap-\ntation. To this end, we propose an innovative\nadaptor network for generic pre-trained encoders,\nsupporting many training strategies such as task-\nadaptive pre-training and ﬁne-tuning, both leading\nto signiﬁcant improvements to eight classiﬁcation\ndatasets from four domains (biomedical, computer\nscience, news and reviews). Our method is easy\nto implement, simple but effective, implying that\nexplicitly representing and incorporating domain-\nspeciﬁc n-grams offer large gains. In addition, fur-\nther analyses consistently demonstrate the impor-\ntance and effectiveness of both unseen words and\nthe information carried by coarse-grained n-grams.\nAcknowledgments\nThis work was supported by the General Research\nFund (GRF) of Hong Kong (No. 16201320). The\nauthors also want to thank the Sinovation Ventures\nfor their great support. Y . Song was supported by\nNSFC under the project “The Essential Algorithms\nand Technologies for Standardized Analytics of\nClinical Texts” (12026610) and Shenzhen Institute\nof Artiﬁcial Intelligence and Robotics for Society\nunder the project “Automatic Knowledge Enhanced\nNatural Language Understanding and Its Applica-\ntions” (AC01202101001). R. Xu was supported by\nthe Hong Kong PhD Fellowship Scheme (HKPFS).\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly Available\nClinical BERT Embeddings. In Proceedings of the\n2nd Clinical Natural Language Processing Work-\nshop, pages 72–78.\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu Ha, et al. 2018. Construction of the Lit-\nerature Graph in Semantic Scholar. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 3 (Industry\nPapers), pages 84–91.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A Pretrained Language Model for Scientiﬁc\nText. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3606–3611.\nPiotr Bojanowski, Édouard Grave, Armand Joulin, and\nTomáš Mikolov. 2017. Enriching Word Vectors with\nSubword Information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nFranck Dernoncourt and Ji Young Lee. 2017. PubMed\n200k RCT: a Dataset for Sequential Sentence Clas-\nsiﬁcation in Medical Abstracts. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 308–313.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186.\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and\nYonggang Wang. 2020. ZEN: Pre-training Chinese\nText Encoder Enhanced by N-gram Representations.\nIn Proceedings of the 2020 Conference on Empirical\n3345\nMethods in Natural Language Processing: Findings,\npages 4729–4740.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nSpeciﬁc Language Model Pretraining for Biomed-\nical Natural Language Processing. arXiv e-prints ,\npages arXiv–2007.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t Stop Pretraining:\nAdapt Language Models to Domains and Tasks.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360.\nShohei Higashiyama, Masao Utiyama, Eiichiro Sumita,\nMasao Ideuchi, Yoshiaki Oida, Yohei Sakamoto,\nand Isaac Okada. 2019. Incorporating Word Atten-\ntion into Character-Based Word Segmentation. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , pages 2699–\n2709, Minneapolis, Minnesota.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classiﬁcation.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 328–339, Melbourne, Aus-\ntralia.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. ClinicalBERT: Modeling Clinical Notes and\nPredicting Hospital Readmission. arXiv preprint\narXiv:1904.05342.\nAE Johnson, TJ Pollard, L Shen, LW Lehman, M Feng,\nM Ghassemi, B Moody, P Szolovits, LA Celi, and\nRG Mark. 2016. MIMIC-III, a Freely Accessible\nCritical Care Database. Scientiﬁc data, 3:160035–\n160035.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving Pre-training by Representing and Predict-\ning Spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Dan Mc-\nFarland, and Dan Jurafsky. 2018. Measuring the\nEvolution of a Scientiﬁc Field through Citation\nFrames. Transactions of the Association for Com-\nputational Linguistics, 6:391–406.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough Memorization: Nearest Neighbor Language\nModels. In International Conference on Learning\nRepresentations.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. Semeval-\n2019 Task 4: Hyperpartisan News Detection. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation, pages 829–839.\nGeewook Kim, Kazuki Fukui, and Hidetoshi Shi-\nmodaira. 2018. Word-like Character N-gram Em-\nbedding. In Proceedings of the 2018 EMNLP Work-\nshop W-NUT: The 4th Workshop on Noisy User-\ngenerated Text, pages 148–152.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nMethod for Stochastic Optimization. In Interna-\ntional Conference on Learning Representations.\nJens Kringelum, Sonny Kim Kjaerulff, Søren Brunak,\nOle Lund, Tudor I Oprea, and Olivier Taboureau.\n2016. ChemProt-3.0: a Global Chemical Biology\nDiseases Mapping. Database, 2016.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In Interna-\ntional Conference on Learning Representations.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. BioBERT: A Pre-Trained\nBiomedical Language Representation Model\nfor Biomedical Text Mining. Bioinformatics,\n36(4):1234–1240.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7871–7880.\nXiaonan Li, Hang Yan, Xipeng Qiu, and Xuanjing\nHuang. 2020. FLAT: Chinese NER using Flat-\nLattice Transformer. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 6836–6842, Online.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-Task Identiﬁcation of En-\ntities, Relations, and Coreference for Scientiﬁc\nKnowledge Graph Construction. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 3219–3232.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n3346\n2011. Learning Word Vectors for Sentiment Analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nJulian McAuley, Christopher Targett, Qinfeng Shi, and\nAnton Van Den Hengel. 2015. Image-based Recom-\nmendations on Styles and Substitutes. In Proceed-\nings of the 38th international ACM SIGIR confer-\nence on research and development in information re-\ntrieval, pages 43–52.\nEn Ouyang, Yuxi Li, Ling Jin, Zuofeng Li, and Xi-\naoyan Zhang. 2017. Exploring N-gram Character\nPresentation in Bidirectional RNN-CRF for Chinese\nClinical Named Entity Recognition. In CEUR Work-\nshop Proc, volume 1976, pages 37–42.\nHaiyun Peng, Yukun Ma, Yang Li, and Erik Cambria.\n2018. Learning Multi-grained Aspect Target Se-\nquence for Chinese Sentiment Analysis. Knowledge-\nBased Systems, 148:167–176.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725.\nYan Song, Chunyu Kit, and Xiao Chen. 2009. Translit-\neration of Name Entity via Improved Statistical\nTranslation on Character Sequences. In Proceedings\nof the 2009 Named Entities Workshop: Shared Task\non Transliteration (NEWS 2009), pages 57–60, Sun-\ntec, Singapore.\nYan Song and Fei Xia. 2012. Using a Goodness Mea-\nsurement for Domain Adaptation: A Case Study on\nChinese Word Segmentation. In LREC, pages 3853–\n3860.\nYan Song, Tong Zhang, Yonggang Wang, and Kai-Fu\nLee. 2021. ZEN 2.0: Continue Training and Adap-\ntion for N-gram Enhanced Text Encoders. arXiv\npreprint arXiv:2105.01279.\nWen Tai, HT Kung, Xin Luna Dong, Marcus Comiter,\nand Chang-Fu Kuo. 2020. exBERT: Extending Pre-\ntrained Models with Domain-speciﬁc V ocabulary\nUnder Constrained Training Resources. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: Findings , pages\n1433–1439.\nIan Tenney, James Wexler, Jasmijn Bastings, Tolga\nBolukbasi, Andy Coenen, Sebastian Gehrmann,\nEllen Jiang, Mahima Pushkarna, Carey Radebaugh,\nEmily Reif, et al. 2020. The Language Inter-\npretability Tool: Extensible, Interactive Visualiza-\ntions and Analysis for NLP Models. arXiv preprint\narXiv:2008.05122.\nYuanhe Tian, Yan Song, Xiang Ao, Fei Xia, Xi-\naojun Quan, Tong Zhang, and Yonggang Wang.\n2020a. Joint Chinese Word Segmentation and Part-\nof-Speech Tagging via Two-way Attentions of Auto-\nanalyzed Knowledge. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 8286–8296.\nYuanhe Tian, Yan Song, Xiang Ao, Fei Xia, Xi-\naojun Quan, Tong Zhang, and Yonggang Wang.\n2020b. Joint Chinese Word Segmentation and Part-\nof-speech Tagging via Two-way Attentions of Auto-\nanalyzed Knowledge. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 8286–8296, Online.\nYuanhe Tian, Yan Song, and Fei Xia. 2020c. Supertag-\nging combinatory categorial grammar with attentive\ngraph convolutional networks. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6037–6044.\nYuanhe Tian, Yan Song, Fei Xia, and Tong Zhang.\n2020d. Improving Constituency Parsing with Span\nAttention. In Findings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning.\nYuanhe Tian, Yan Song, Fei Xia, Tong Zhang, and\nYonggang Wang. 2020e. Improving Chinese Word\nSegmentation with Wordhood Memory Networks.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8274–8285, Online.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nHai Wang, Dian Yu, Kai Sun, Jianshu Chen, and\nDong Yu. 2019. Improving Pre-Trained Multilin-\ngual Model with V ocabulary Expansion. InProceed-\nings of the 23rd Conference on Computational Natu-\nral Language Learning (CoNLL), pages 316–327.\nDongling Xiao, Yu-Kun Li, Han Zhang, Yu Sun,\nHao Tian, Hua Wu, and Haifeng Wang. 2020.\nERNIE-Gram: Pre-Training with Explicitly N-Gram\nMasked Language Modeling for Natural Language\nUnderstanding. arXiv preprint arXiv:2010.12148.\nZe Yang, Wei Wu, Can Xu, Xinnian Liang, Jiaqi\nBai, Liran Wang, Wei Wang, and Zhoujun Li. 2020.\nStyleDGPT: Stylized Response Generation with Pre-\ntrained Language Models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing: Findings, pages 1548–1559.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level Convolutional Networks for Text\nClassiﬁcation. Advances in neural information pro-\ncessing systems, 28:649–657.\n3347\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and William B Dolan. 2020. DIALOGPT:\nLarge-Scale Generative Pre-training for Conversa-\ntional Response Generation. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics: System Demonstrations , pages\n270–278.\n3348\nA Description of Computing Infrastructure\nAll the experiments are implemented on Nvidia V100 GPUs with 32GB memory.\nB Run Time\nDOMAIN BIOMED CS N EWS REVIEWS\nDATASET CP RCT CI SE HP AG AM IMDB\nRoBERTa+FT 95 40 37 74 50 102 130 114\n+T-DNA 93 39 40 72 52 104 131 113\nRoBERTa+TAPT 300 132 117 234 285 389 402 392\n+T-DNA 320 128 114 240 290 390 400 394\nTable 4: Running time per epoch of models, in the unit of second.\nC Validation Performance\nDOMAIN BIOMED CS N EWS REVIEWS\nDATASET CP RCT CI SE HP AG AM IMDB\nRoBERTa+FT 80.08 81.21 58.06 75.33 93.50 88.70 62.50 93.04\n+T-DNA 81.17 82.00 62.98 79.62 91.81 88.64 63.40 92.83\nRoBERTa+TAPT 81.27 80.98 60.11 77.08 93.50 88.90 64.30 92.38\n+T-DNA 82.58 83.24 67.89 80.69 93.74 89.31 64.27 93.11\nTable 5: The validation performance.\nD Evaluation Measures\nWe use manual tuning and adopt macro-F1 for CitationIntent, SciERC, HyperPartisan, AGNews, Amazon,\nIMDB, and micro-F1 for ChemProt and RCT as evaluation metrics. Macro-F1 will compute the F1 metric\nindependently for each class and then take the average, whereas micro-F1 will aggregate the contributions\nof all classes to compute the average metric. In a multi-class classiﬁcation setup, micro-F1 is preferable if\nthere is class imbalance, which is true for ChemProt and RCT.\nE Bounds of Hyperparameters\nHyperparameter Assaignment\nnumber of epochs 3(FT) or 15(TAPT)\npatience 1\nbatch size [4,8,16,32,64]\nlearning rate [1e-5,1e-4]\ndropout 0.5\nclassiﬁcation layer [1,2]\nlearning rate optimizer Adam\nAdam epsilon 1e-8\nAdam beta 0.9, 0.999\nlearning rate optimizer Adam\nTable 6: Bounds of hyperparameters.\n3349\nF Conﬁguration of Best Model\nHyperparameter Assaignment\nnumber of epochs 3(FT) or 15(TAPT)\npatience 1\nbatch size 32\nlearning rate 4e-5\ndropout 0.5\nclassiﬁcation layer 1\nlearning rate optimizer Adam\nAdam epsilon 1e-8\nAdam beta 0.9, 0.999\nlearning rate optimizer Adam\nTable 7: Conﬁguration of the best model."
}