{
  "title": "RobuTrans: A Robust Transformer-Based Text-to-Speech Model",
  "url": "https://openalex.org/W2997540646",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2891909358",
      "name": "Naihan Li",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2122251602",
      "name": "Yan-qing Liu",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2126641399",
      "name": "Yu Wu",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2118325933",
      "name": "Shujie Liu",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2029189534",
      "name": "Sheng Zhao",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2098006703",
      "name": "Ming Liu",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2891909358",
      "name": "Naihan Li",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "University of Electronic Science and Technology of China",
        "People's Government of Shaanxi Province"
      ]
    },
    {
      "id": "https://openalex.org/A2122251602",
      "name": "Yan-qing Liu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2126641399",
      "name": "Yu Wu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2118325933",
      "name": "Shujie Liu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2029189534",
      "name": "Sheng Zhao",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2098006703",
      "name": "Ming Liu",
      "affiliations": [
        "People's Government of Shaanxi Province",
        "Beijing Institute of Big Data Research",
        "University of Electronic Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2148228080",
    "https://openalex.org/W2150658333",
    "https://openalex.org/W2285182995",
    "https://openalex.org/W2898847420",
    "https://openalex.org/W2162433174",
    "https://openalex.org/W2605141709",
    "https://openalex.org/W2946200149",
    "https://openalex.org/W6756197946",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6676641785",
    "https://openalex.org/W2898322826",
    "https://openalex.org/W2168510624",
    "https://openalex.org/W1991133427",
    "https://openalex.org/W2102003408",
    "https://openalex.org/W6679146927",
    "https://openalex.org/W6910745590",
    "https://openalex.org/W2942807473",
    "https://openalex.org/W2111284386",
    "https://openalex.org/W2129142580",
    "https://openalex.org/W2963300588",
    "https://openalex.org/W4298580827",
    "https://openalex.org/W2963975282",
    "https://openalex.org/W2941814890",
    "https://openalex.org/W2584032004",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2903739847",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2604184139",
    "https://openalex.org/W4294619240",
    "https://openalex.org/W2519091744",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2964243274",
    "https://openalex.org/W2901997113",
    "https://openalex.org/W2963091184"
  ],
  "abstract": "Recently, neural network based speech synthesis has achieved outstanding results, by which the synthesized audios are of excellent quality and naturalness. However, current neural TTS models suffer from the robustness issue, which results in abnormal audios (bad cases) especially for unusual text (unseen context). To build a neural model which can synthesize both natural and stable audios, in this paper, we make a deep analysis of why the previous neural TTS models are not robust, based on which we propose RobuTrans (Robust Transformer), a robust neural TTS model based on Transformer. Comparing to TransformerTTS, our model first converts input texts to linguistic features, including phonemic features and prosodic features, then feed them to the encoder. In the decoder, the encoder-decoder attention is replaced with a duration-based hard attention mechanism, and the causal self-attention is replaced with a \"pseudo non-causal attention\" mechanism to model the holistic information of the input. Besides, the position embedding is replaced with a 1-D CNN, since it constrains the maximum length of synthesized audio. With these modifications, our model not only fix the robustness problem, but also achieves on parity MOS (4.36) with TransformerTTS (4.37) and Tacotron2 (4.37) on our general set.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nRobuTrans: A Robust Transformer-Based Text-to-Speech Model\nNaihan Li,∗1,4,5 Yanqing Liu,2 Yu Wu,3 Shujie Liu,3 Sheng Zhao,2 Ming Liu1,4,5\n1School of Computer Science and Engineering, University of Electronic Science and Technology of China\n2Microsoft STC Asia\n3Microsoft Research Asia\n4CETC Big Data Research Institute Co.,Ltd, Guiyang\n5Big Data Application on Improving Government Governance CapabilitiesNational Engineering Laboratory, Guiyang\nlnhzsbls1994@163.com\n{yanqliu, Wu.Y u, shujliu, szhao}@microsoft.com\ncsmliu@uestc.edu.cn\nAbstract\nRecently, neural network based speech synthesis has achieved\noutstanding results, by which the synthesized audios are of\nexcellent quality and naturalness. However, current neural\nTTS models suffer from the robustness issue, which results\nin abnormal audios (bad cases) especially for unusual text\n(unseen context). To build a neural model which can synthe-\nsize both natural and stable audios, in this paper, we make\na deep analysis of why the previous neural TTS models are\nnot robust, based on which we propose RobuTrans (Robust\nTransformer), a robust neural TTS model based on Trans-\nformer. Comparing to TransformerTTS, our model ﬁrst con-\nverts input texts to linguistic features, including phonemic\nfeatures and prosodic features, then feed them to the encoder.\nIn the decoder, the encoder-decoder attention is replaced with\na duration-based hard attention mechanism, and the causal\nself-attention is replaced with a ”pseudo non-causal atten-\ntion” mechanism to model the holistic information of the in-\nput. Besides, the position embedding is replaced with a 1-\nD CNN, since it constrains the maximum length of synthe-\nsized audio. With these modiﬁcations, our model not only\nﬁx the robustness problem, but also achieves on parity MOS\n(4.36) with TransformerTTS (4.37) and Tacotron2 (4.37) on\nour general set.\n1 Introduction\nSpeech synthesis (also known as text to speech, TTS) has a\npivotal role in a wide range of speech-related applications.\nOwing to the development of deep learning techniques,\nmodern TTS pipelines make a step from HMM-based statis-\ntical parametric TTS models (Maia, Zen, and Gales 2010),\nneural acoustic models (Ze, Senior, and Schuster 2013) to\nneural end-to-end TTS models (Wang et al. 2017; Shen et\nal. 2017; Li et al. 2018).\nAlong with the success of neural machine translation,\nneural sequence to sequence models are applied to TTS\ntasks, such as Tacotron2 (Shen et al. 2017) and Trans-\nformerTTS (Li et al. 2018). The neural sequence to sequence\nmodel usually contains three components: an encoder, a de-\n∗Work done during internship at Microsoft STC Asia.\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\ncoder and an attention mechanism between them. The en-\ncoder is used to convert the input text into a semantic space,\nbased on which the decoder generates the spectrums, with\nthe guidance of the attention mechanism to decide when to\npronounce which word. Based on the generated spectrums,\na vocoder (V an Den Oord et al. 2016) is leveraged to synthe-\nsizes the ﬁnal audios.\nBy this method, the synthesized audio has excellent nat-\nuralness on general input texts like those in the training set,\nsome even achieve close-to-human quality (Li et al. 2018).\nHowever, when the input texts are unusual\n1, abnormal spec-\ntrums are generated, leading to bad audios. We summarize\nand categorize these bad audios into following major types:\n1) some words may be unclear even missed, or on the con-\ntrary, duplicated; 2) the decoding procedure stops too early\nor too late; 3) the decoding procedure can’t stop until the\npre-deﬁned maximum length is reached. The vulnerability\nand instability of these models limit their applicability to a\nbroader range of tasks, which require robust performance on\ndiverse inputs, such as voice assistant and vehicle navigator.\nWe conduct a detailed study on the above bad audios gen-\nerated by TransformerTTS, ﬁnding that abnormal audios al-\nways appear with disordered attention alignments. To deal\nwith such a problem and ensure the monotonic correspon-\ndence from the phoneme sequence to the acoustic sequence,\nZhang, Ling, and Dai (2018) propose a forward attention\nmethod for higher attention stability, in which the attention\nprobabilities at each time step are computed recursively us-\ning a forward algorithm. Raffel et al. (2017) propose a forced\nmonotonic attention mechanism, where at each output time\nstep, the decoder inspects memory entries in left-to-right\nmanner starting from where it left off at the previous out-\nput time step and chooses a single one to attend to.\nWe test them and ﬁnd none of them can completely get\nrid of abnormal cases, instead gives rise to other issues such\nas higher speech rate and weird rhythm.\nIn this paper, we remove the encoder-decoder attention\nand apply a duration-based hard attention to copy encoder\nhidden states to their corresponding frames, forcing the de-\n1Such as URL, a sequence of numbers, and other texts which\nare out of the domain of the training data\n8228\ncoder to generate correct content. To have a holistic view\nof the whole input as the original attention mechanism, we\nreplace the causal self-attention layer in the decoder with a\npseudo non-causal attention (PNCA) to not only consider\npreviously decoded results, but also attend to subsequential\ncontexts. To improve the audio naturalness for long input,\nwe remove the position embedding and rely on a 1-D CNN\nto model the relative position information. Furthermore, in-\nstead of using the text, we leverage linguistic features (in-\ncluding phonemic and prosodic features) as the input of the\nencoder, of which the prosodic features have a great con-\ntribution to the prosody of results. With the above adapta-\ntions, our model manages to thoroughly eliminate abnormal\nresults, meanwhile achieve the on parity naturalness with\nprevious neural TTS models. We conduct experiments on\ntwo test sets, including a general set and a bad-case set. Our\nmodel doesn’t make any mistake for the samples in the bad-\ncase set, at the same time, it achieves on parity MOS (4.36)\nwith TransformerTTS (4.37) and to Tacotron2 (4.37) on the\ngeneral set.\nIn summary, our contributions can be listed as follows:\n1) We conduct a detailed study to show why previous neu-\nral TTS models are not robust. 2) We employ a duration-\nbased hard attention to effectively improve the robustness\nof our model, meanwhile propose a pseudo non-causal at-\ntention, which signiﬁcantly contributes to the naturalness of\nsynthesized audio by providing a holistic view of the input\nsequence for each decoding step.\nBesides, the ability to synthesize long sequences is im-\nproved by dispensing the position embedding and relying on\n1-D CNN instead, and the naturalness of synthesized audio\nis further enhanced by leveraging linguistic features.\n3) Our model achieves comparable performance on qual-\nity and naturalness to previous neural TTS models, mean-\nwhile shows excellent robustness for various input patterns.\n2 Why Previous Neural Models Unstable\nIn this section, we ﬁrst brieﬂy introduce TransformerTTS\n(Li et al. 2018), the state-of-the-art Neural TTS model; base\non this model, we make a deep analysis on three factors\nwhich makes it unstable.\nTransformerTTS (Li et al. 2018) is a neural TTS model\nwhich combines Transformer (V aswani et al. 2017) and\nTacotron2 (Shen et al. 2017). As shown in Figure 1, given\nthe input text, a text-to-phoneme converter is ﬁrst used to get\nthe phoneme sequence. With a CNN as the encoder pre-net,\ncontext features are extracted to be the input of the encoder.\nThe mel spectrum frames are also processed by a 2-layer\nfully connected network withrelu activation. Position infor-\nmation is injected by adding two position embeddings to the\noutput of the encoder and decoder pre-nets respectively. The\nencoder is built with stacks of several identity blocks, each\ncontains two sub-networks: a self-attention and a feed for-\nward network. The decoder has the similar structure, while\nthe self-attention is causal to attend to only the previously\ndecoded frames, and an extra encoder-decoder attention is\nleveraged to attend to encoder hidden states.\nBased on the ﬁnal hidden states of the decoder, mel spec-\ntrum frames are generated autogressively with a linear layer\nFigure 1: Architecture of TransfomerTTS.\nfollowed by a post-net, which stops when a stop token is\npredicted by a separate linear projection.\nSimilar to Tacotron2, TransformerTTS also borrows tech-\nniques from neural machine translation (NMT) community.\nSome designs for NMT, however, do not ﬁt TTS tasks, which\nis the root cause of the instability and robustness problem. In\nthe following section, we summarize three main drawbacks.\n2.1 Unconstrained Encoder-decoder Attention\nTransformerTTS borrows the soft attention mechanism from\nNMT, which enables the decoder to attend to arbitrary parts\nat the source side for each step. The mechanism is reason-\nable for NMT because the word order of two languages\nmay be different, which means the last word on the target\nside may correspond to the ﬁrst word on the source side.\nIn contrast, text to speech has a unique property, which is\nmonotonous continuous correspondence, meaning that if\nthe i-th step attends to the j-th word at the source side,\nthe (i +1 )-th step must attend to the (j + n)-th word\n(1 ≥ n ≥ 0), as shown in the left picture in Figure 2.\nPrevious models ignore this constraint, and learn the\nalignment from the data totally, resulting in incorrect align-\nments for special inputs. The right picture in Figure 2 shows\nan example of an abnormal alignment. On the one hand, the\nattention mechanism skips the second word and attends to\n8229\nFigure 2: Normal and abnormal alignments of encoder-\ndecoder attention. Mel spectrum frames (queries) are ranged\nhorizontally, while encoder hidden states (keys) are vertical.\nLeft: normal alignment; the focus along keys are continuous\nand monotonous. Right: Abnormal alignment; the red line\nrepresents the skipping as well as retreating advance.\nthe third word at the third step, while on the other hand, af-\nter attending to the forth word at the fourth step, it attends\nto the third word again. These two cases deﬁnitely output\nbad cases. Although some variation of attention mechanisms\n(e.g. forward attention) has tried to construct a monotonous\ncontinuous correspondence between encoder and decoder,\nthey cannot completely eliminate bad cases. Details will be\nshown in Section 4.6.\n2.2 Imprecise Stop Prediction\nDifferent from NMT models which predict one token each\ntime until a stop token is predicted, TTS models predict mel\nvectors and require a separate classiﬁer to decide when to\nstop. However, this stop predictor is usually unreliable due\nto two reasons: 1) Each case of TTS consists of hundreds\nof decoding steps but only one stop step, leading to an im-\nbalance continue/stop classiﬁcation problem and an unde-\nsirable performance on stop prediction. Although imposing\na larger weight on the ”stop” class can efﬁciently relieve this\nissue, the stop prediction still makes mistakes for some spe-\nciﬁc inputs. For example, if the input text is twenty consec-\nutive ”0”s (”00 ... 0”), the generated audio is likely to con-\ntain more or less than twenty. 2) The stop token is autore-\ngressively predicted only conditioning on the hidden state of\neach step, which contains no explicit information of whether\nthe input is all and once pronounced.\n2.3 Unseen position embedding\nTransformer injects the position information by adding po-\nsition embeddings to the inputs of its encoder and decoder:\nPE (pos, 2i)=s i n ( pos\n10000\n2i\ndmodel\n) (1)\nPE (pos, 2i + 1) = cos( pos\n10000\n2i\ndmodel\n) (2)\nHowever, when the input sequence is longer than the nor-\nmal length as it is in the training set, the position indexes\ncan be extremely large, of which the corresponding position\nembeddings are unseen for both the encoder and decoder.\nTherefore, the input sequences to which these position em-\nbeddings are added may confuse the model, leading to the\noccurrences of abnormal outputs.\nFigure 3: Architecture of RobuTrans.\n3 Robust Transformer-based TTS\nIn this section, we will introduce RobuTrans, a robust neu-\nral TTS model based on Transformer, as shown in Figure\n3. The main processing pipeline is, on the one hand, the in-\nput text is ﬁrstly converted into the sequences of linguistic\nfeatures, then through Encoder Pre-net (a 3-layer CNN) and\nEncoder. Encoder hidden states are fed into Duration Pre-\ndictor, which predicts their durations, then they are tiled into\nframe-level. On the other hand, the shifted mel spectrum is\nﬁrstly processed by Decoder Pre-net, which is a two-layer\nfully connected network withrelu activation. The tiled en-\ncoder hidden states and processed mel spectrum frames are\nconcatenated and then through a linear projection to be fused\nand have the appropriate dimension for Decoder. The De-\ncoder hidden states are processed by a linear projection to\nobtain the decoded spectrum, then through Post-net (a 5-\nlayer CNN) to obtain the ﬁnal spectrum.\nRobuTrans differs from TransformerTTS in following as-\npects: 1) The input of Encoder is linguistic features, which\nconsists of phonemic and prosodic features; 2) The position\nembedding in the Encoder and Decoder is removed; 3) The\nencoder-decoder attention is replaced with a duration based\nhard attention; 4) The causal self-attention in Decoder is re-\nplaced with pseudo non-causal attention.\n8230\n3.1 Text-to-Linguistic-Feature Converter\nWe ﬁrst convert the input text into linguistic features, which\nconsist of phonemic and prosodic features and then con-\nsumed by Encoder. To obtain the phonemic features, a rule-\nbased system is used for the grapheme-to-phoneme conver-\nsion, which generates the phonemic categorical features\n2.\nThe prosodic feature includes tone and break index, which\nare predicted by a conditional random ﬁeld (CRF) model\nwith syntactic and contextual information as in Qian et al.\n(2010). The prosodic feature plays a critical role in Robu-\nTrans for synthesizing expressive speech, while, on the con-\ntrary, harms the quality of TransformerTTS. Two ablation\nstudies will be demonstrated in Section 4.8.\n3.2 Duration Predictor\nWe adopt the structure of the duration predictor as in Fast-\nSpeech (Ren et al. 2019), where there are two convolutional\nlayers (kernel size is 3, hidden size is 256) with layer nor-\nmalization and dropout, together with a linear projection\nto predict the logarithmic duration of each encoder hidden\nstate. Mean squared error (MSE) is employed as the loss\nfunction. To generate the ground truth duration for the model\ntraining, speech recognition tools are employed to make the\nforced alignment between the audio and the phoneme se-\nquence. With the predicted duration, the phoneme-level fea-\ntures are copied and expanded to frame-level features ac-\ncordingly, as illustrated in Figure 4.\n3.3 Pseudo Non-causal Attention\nAs discussed in Section 2.1, the encoder-decoder attention\nmechanism is a crucial factor for the instability. However,\nsimply removing this attention will also discard the advan-\ntages it brings to the TTS model. The advantages can be\nconsidered as the following two aspects. On the one hand,\nthe encoder-decoder attention provides a holistic view of in-\nput sequence for the decoder, while on the other hand, it\ncomposes frame-level context vectors according to decoder\ninputs (which are mel frames). These two advantages make\ngreat contribution to the decoding procedure, and we pro-\npose ”pseudo non-causal attention” (PNCA) to replace the\ncausal self-attention layers as shown in Figure 4, which not\nonly inherits the two features above, but also makes the de-\ncoding procedure robust.\nLet T be the total length of mel spectrum to be decoded,\nx\nl\ni be the autoregressive output of stepi and layer l, hi be\nthe tiled encoder hidden state of stepi. For the time step\nt, the PNCA of layer l takes\n[\nxl−1\n1 ,x l−1\n2 , ..., xl−1\nt\n] 3 and\n[ht,h t+1...hT ] as input. Speciﬁcally, letAttention(Q, K)\nbe the multi-head attention,\n2We group the phonemes into different categories, and the cat-\negorical feature indicates which groups the phoneme belongs to.\n3if l =1 , x0\ni is the fusion of padded mel spectrum frame and\nencoder hidden of stepi. We concatenate thehi and the(i −1)-th\nmel spectrum frame processed by Decoder Pre-net, then through a\nlinear projection.\nFigure 4: Phoneme-level to frame-level conversion and\npseudo non-causal attention (PNCA). The left part of PNCA\nis causal self-attention, which takes the encoder hidden\nstates fused with padded mel spectrum frames by a linear\nprojection as input, while the right part consumes only the\nencoder hidden states.\ny\nl\nt = PNCA(xl−1\nt ,X l−1\n1...t,H t...T ) (3)\n= Attention(xl−1\nt ,X l−1\n1...t) + Attention(xl−1\nt ,H t...T )\n(4)\nThen yl\nt is added toxl−1\nt and consumed byFFN and follow-\ning residual connection to obtainxl\nt.\nBy using pseudo non-causal attention, RobuTrans enjoys\nthe following advantages comparing to TransformerTTS: 1)\nDecoder gets a holistic view of the input sequence, as well as\nframe-level context vectors, thus the two beneﬁts of vanilla\nencoder-decoder attention is kept; 2) the output length is de-\ntermined by the sum of durations of phonemes, thus the stop\npredictor is removable and the issue in Section 2.2 is ad-\ndressed; 3) copying encoder hidden states according to their\ndurations explicitly builds a monotonic continuous corre-\nspondence between encoder and decoder steps, which pro-\nvides the instruction for each decoding step and helps deal\nwith the abnormal alignment problem in Section 2.1. With\nthese three advantages, RobuTrans becomes robust and man-\nages to synthesize stable audios, meanwhile the audio qual-\nity has no regression.\n3.4 Removing Position Embedding\nAs it is demonstrated in Section 2.3, we ﬁnd that the posi-\ntion embedding severely constrains the valid length of syn-\nthesized audio in our experiments. Speciﬁcally, when a text\nis much longer than those in the training set, the synthesized\naudio becomes unclear and its prosody is very strange.\nInstead of adding this positional embedding, as inves-\ntigated in the speech recognition community (Mohamed,\nOkhonko, and Zettlemoyer 2019), we can simply remove the\nposition embedding and count on the CNN used in Encoder\nPre-net to model the relative position information in a ﬁxed\nwindow. By this alteration, RobuTrans can synthesize longer\nsequences than those in the training set.\n8231\n4 Experiment\n4.1 Baseline Model\nThere are three baseline models to verify both the natural-\nness and robustness of RobuTrans respectively, which are\nTransformerTTS, Tacotron2, and FastSpeech. These three\nmodels are all trained with the same dataset introduced in\nSection 4.2.\n4.2 Training Setup\nWe use 4 Nvidia Tesla P100 to train our model. Since the\nlengths of training samples vary greatly, a ﬁxed batch size\nwill either run out of memory when the batch size is large, or\nmakes the training procedure inefﬁcient and unstable if the\nbatch is small. Therefore, a dynamic batch size is adapted.\nEach GPU has a memory of 16GB, which can hold 6000\nframes (total length of 10∼40 samples), and thus the batch\nsize is 40∼160. For the training set, we use an internal US\nEnglish dataset, which contains 20 hours of speech from a\nsingle professional speaker. 80-channel mel scaled spectrum\nis extracted from 16k normalized wave, and all the train-\ning texts are also normalized. The time consuming for a sin-\ngle training step is 0.55 seconds, and it takes 150,000 steps\n(about 23 hours) to converge.\n4.3 Test Setup\nRobuTrans aims to be not only natural but also robust for\nall input text. Therefore, we respectively test our model with\ntwo different test sets for these two aspects.\nRobustness test: To test the model robustness, we have\na bad-case set consists of 327 sentences, which covers the\nmain categories. We collect them from a large corpus con-\nsisting of tens of thousands of sentences. A Tacotron2 model\nis employed to generate all the audios, of which the cor-\nresponding phoneme sequences are then recognized by a\nspeech recognition tool. We calculate the WER of these\nphoneme sequences with ground truths. Those texts with the\nhighest error rate are collected as our bad case set, which has\nno overlap with our training set. Some typical error-prone\nexamples are listed in Table 1. The results of this test con-\nsists of ”has” and ”doesn’t have” bad cases.\nNaturalness test: A 81-sentence test set is randomly se-\nlected from the general domain of a large internal corpus\n(containing millions of sentences). Sentences in these set are\nsimilar as training set but have no overlap. To evaluate the\nnaturalness, we conduct both a MOS test (among RobuTrans\nand baseline models) and CMOS\n4 tests (between RobuTrans\nand baseline models respectively).\nAll tests are conducted on a crowd-sourcing platform,\nwhere the testers are registered by themselves. We didn’t\nspecify anything except for the maximum number of audios\n(40 sentences for MOS, 30 sentence pairs for CMOS) each\ntester could listen to in one test, and the tester number (12\nfor MOS, 9 for CMOS) each sample is listened by.\n4Comparison mean option score, in which the annotator listens\nto two audios from different models with the same text each time\nand evaluates how the latter is better than the former with an integer\nscore ranging in[−3, 3]. Since the order of the two audios changes\nrandomly, the tester has no idea about their sources.\n4.4 WaveNet Vocoder\nTo obtain audios with high quality, we employ an autore-\ngressive WaveNet vocoder to synthesize the audio with mel\nsequence as input for all the models, which is trained sepa-\nrately conditioning on ground truth mel spectrums extracted\nby the audios. The sample rate of ground truth audios is\n16000 and frame rate (frames per second) of ground truth\nmel spectrums is 80. Our autoregressive WaveNet contains\n2 QRNN layers and 20 dilated layers, and the sizes of all\nresidual channels and dilation channels are 256. Each frame\nof QRNN’s ﬁnal output is copied 200 times to build the con-\nditional input of the 20 dilated layers to ﬁt the same length\nwith audio samples.\n4.5 Result\nGenerated audio samples are accessible in the supplemen-\ntary materials, including those from the general set gener-\nated by RobuTrans and three baseline models, as well as\nthose by RobuTrans from the bad-case set synthesized by\nRobuTrans listed in Table 1 .\nWe ﬁnd that RobuTrans can not only synthesize unusual\nsentences like a single letter, number and letter series, but\nalso robust for URLs, command lines, and even for some\nlong and meaningless sentences, which are completely out\nof the domain of our training set.\nRobustness test: On our bad-case set (327 sentences),\nRobuTrans can always synthesize completely correct au-\ndios, including single letter, number and letter series, as\nwell as URLs, command lines, and even for some long\nand meaningless sentences, which are completely out of\nthe domain of our training set. FastSpeech also generates\nno bad cases, while TransformerTTS and Tacotron2 have\n237 and 35 bad cases respectively. Note that, on the one\nhand, we can qualitatively conclude that RobuTrans and\nFastSpeech are two robust TTS models on these input pat-\nterns, while TransformerTTS and Tacotron2 are not; mean-\nwhile, TransformerTTS is most likely to generate abnor-\nmal results among these models. We think the reason for\nthe poor robustness of Transformer is that its decoder has\n6 encoder-decoder attention among its 6 blocks, each in-\ncluding 8 heads; each of them has a chance to make mis-\ntakes, which makes the model more error-prone. On the\nother hand, these numbers of bad cases cannot be used to\nquantify the robustness of our model as well as baselines.\nSpeciﬁcally, each pattern includes inﬁnite samples, such as\nduplicate ”zero” for 20, 50 and 100 times, etc., on which\nTransformer and Tacotron2 always generates bad cases.\nNaturalness test: All models generate correct results on\nthis set. Test results are shown in Table 2, and we have fol-\nlowing three conclusions. 1) RobuTrans is on parity with\nTacotron2, as MOS4.36 verse 4.37 and CMOS −0.062.2 )\nRobuTrans is also on parity with TransformerTTS, as MOS\n4.36 verse 4.37 and CMOS −0.051. 3) RobuTrans outper-\nforms FastSpeech, as MOS 4.36 verse 4.31 and CMOS\n+0.187. Acousticly, the audio synthesized by FastSpeech\nwith WaveNet vocoder has background noise and unclear\npronounciation, which is more obvious when pairwise lis-\ntened to in CMOS test.\n8232\nTable 1: Categories of error-prone text and corresponding examples\nCategory Example\nSingle Letter W\nNumber zero zero zero zero zero zero zero zero two seven nine eight F three forty zero zero zero zero zero six\nfour two eight zero one eight\nSpelling backslashinternald o texchanged o tmanagementd o tsystemmanage\nURL http://ofﬁce/c16/specs/Specs2/Forms/All%20Ofﬁce%20Specs.aspx?\nRootFolder=/c16/specs/Specs2/FrontPage&View={33888BDC-E0CB-4928-AEB7-26607D28009F}\nCommand Line $runtime.windows\\Speech OneCore\\Engines\\TTS\\ar-EG\\ArEGDiacModel.Bin\nSpelling &\nNumber (long, up\nto 30 secs)\nDUB - OW A - zero one JPN - OW A - zero one RED - OW A - zero one RED - OW A - zero two SIN -\nOW A - zero one SIN - OW A - zero two SYD - OW A - zero one SYD - OW A - zero two Corporate\nMSG Servers Server Name Exchange V ersion OS V ersion Able to Upgrade to WS2003 ?\nModel MOS CMOS\nTacotron2 4.37 (0.06) −0.062 (0.088)\nTransformerTTS 4.37 (0.06) −0.051 (0.079)\nFastSpeech 4.31 (0.06) +0.187 (0.085)\nRobuTrans 4.36 (0.06) -\nRecording 4.69 (0.06) -\nTable 2: MOS and CMOS test results. In ”MOS” and\n”CMOS” column, the number in brackets is conﬁdence in-\nterval radius with conﬁdence level 0.95. Note that in the\n”CMOS” column, all scores measure the improvement of\nRobuTrans comparing to the three baseline models.\n4.6 Other Attention Mechanisms\nWe conduct experiments investigating the robustness of\nother attention mechanisms, including forward attention\n(Zhang, Ling, and Dai 2018), GMM attention (Graves\n2013), forced monotonic mechanism (Raffel et al. 2017) and\nguided attention (Zhu et al. 2019). All these mechanisms\ngenerate bad cases, therefore none of them could be part\nof our robust model. Besides, we ﬁnd that GMM attention\nmakes the model more stable on long sequences comparing\nto the vanilla attention mechanism. Forward attention makes\ninference procedure more unstoppable-prone, forced mono-\ntonic mechanism makes the speech rate higher, and guided\nattention produces audios with weird rhythm.\n4.7 Bad Case Analysis\nAs reported above, RobuTrans can generate completely cor-\nrect samples in our bad-case set, while both Transformer and\nTacotron2 has some errors. To visualize these bad cases, we\nshow alignments from certain heads of certain layers of the\nTransformerTTS encoder-decoder attention, which are obvi-\nously disordered and account for the bad cases.\nMissed/duplicated Words: As demonstrated in Section\n2.1, there isn’t any constraint for the monotonous continu-\nous correspondence of the encoder-decoder alignment, thus\nsome words may be missed or duplicated. The disconnec-\ntion of the alignment, shown in Figure 5(a), results in the\nmissed words; on the contrary, some words are pronounced\nfor more than once, which can be interpreted by the repeated\nattention in Figure 5(b).\n(a) Skipping alignment, causing missed words.\n(b) Retreating alignment, causing duplicated words.\n(c) Alignment of early stop.\n(d) Alignment of unstoppable prediction.\nFigure 5: Alignments of some typical bad cases.\nImprecise Stop: Figure 5(c) shows the alignment of a\nsample, of which the text is ”zero zero zero ...” (22 repeated\n”zero”s). However, there are only 21 ”zero”s are pronounced\nin the synthesized audio. It can be observed that the align-\nment becomes confused in the rear part of the decoding pro-\ncedure (the only one line at the beginning becomes ﬁve lines\nat the end). The reason could be that the decoder has no idea\nwhich word it is decoding since all the words are the same,\nwhich results in the early stop (similar alignment can also\nlead to late stop).\nUnstoppable Prediction: In Figure 5(d), it can be ob-\nserved that the alignment keeps repeating the rear words, and\ncannot stop until the pre-deﬁned maximum decoding length\n8233\nis reached (which is 1000 in this sample).\n4.8 Ablation Study\nTo better understand the impact of the components in Robu-\nTrans, we conduct ablation studies as below.\nPseudo Non-Causal Attention: Pseudo non-causal atten-\ntion provides a view of subsequential context. We quantify\nthis innovation by an ablation study where the pseudo non-\ncausal attention is changed back to the causal self-attention.\nThe CMOS is −0.291 (CI (95%): 0.077), proving that at-\ntending to subsequential frames contributes signiﬁcantly to\nthe quality of synthesized audio.\nProsodic Feature: To verify the impact of prosodic fea-\ntures, we evaluate the performance without them. We ﬁnd\nthat the prosody of generated audios becomes weird and un-\nnatural. We conduct a CMOS test and ﬁnd that removing\nprosodic features results in a regression with CMOS−0.134\n(CI (95%): 0.068 ), which conﬁrms that the prosodic fea-\ntures play a signiﬁcant role in RobuTrans.\nLinguistic feature for TransformerTTS: When com-\nparing RobuTrans with Transformer, the extra information\nadded by Text-to-Linguistic-Feature Converter may be a fac-\ntor of an unfair comparison. Therefore, we add the same\nextra information to TransformerTTS\n5, and test its CMOS\ncomparing to the original version. The result is−0.047 (CI\n(95%): 0.11), which means adding extra information doesn’t\nimprove but severely harms the quality (similar result is also\nobtained on Tacotron2).\n5 Related Work\nTraditional speech synthesis methods can be categorized\ninto two classes: concatenative systems and parametric sys-\ntems. Concatenative TTS systems (Hunt and Black 1996)\nsplit original waves into small units, and stitch them by\nsome algorithms such as Viterbi (Viterbi 1967) followed\nby signal process methods (Charpentier and Stella 1986;\nV erhelst and Roelands 1993) to generate new waves. Para-\nmetric TTS systems (Zen, Tokuda, and Black 2009; Ze, Se-\nnior, and Schuster 2013; Tokuda et al. 2013) convert speech\nwaves into spectrograms, as well as acoustic parameters,\nsuch as fundamental frequency and duration, which are em-\nployed to synthesize new audio results.\nTraditional speech synthesis methods require extensive\ndomain expertise and may contain brittle design choices.\nThis may be time-consuming and require a lot of resources\nfor manpower. On the other hand, with the rapid develop-\nment of neural networks, neural TTS has become the main-\nstream.\nChar2Wav (Sotelo et al. 2017) integrates the front-end\nand the back-end with a seq2seq (Sutskever, Vinyals, and\nLe 2014; Bahdanau, Cho, and Bengio 2014) model, predict-\ning acoustic parameters for a following SampleRNN (Mehri\net al. 2016). This simpliﬁes the complex traditional pipeline.\nAfter that, end-to-end TTS models become the research fo-\ncus, aiming to directly learn the text-to-audio procedure.\nIn the common pipeline, the text is ﬁrst converted into the\n5Employ this converter to process the input of TransformerTTS\nthen feed into it.\nspectrum, a highly-compressed representation of the audio,\nby acoustic models, such as Tacotron (Wang et al. 2017),\nTacotron2 (Shen et al. 2017), TransformerTTS (Li et al.\n2018) and ClariNet (Ping, Peng, and Chen 2018), then the\nspectrum is converted into the audio by a neural vocoder. As\nfor the neural vocoder, WaveNet (V an Den Oord et al. 2016)\nis a powerful model which can generate high-quality au-\ndios. Combine the acoustic model and vocoder, neural TTS\nachieves extraordinary results and signiﬁcantly outperforms\ntraditional TTS systems.\nThough neural TTS shows promising ability, there are still\ntwo barriers preventing them from being widely applied to\napplication especially in industry. On the one hand, the infer-\nence cannot be real-time since both the acoustic model and\nvocoder are autoregressive, which mean the prediction of\neach time step depends on previous steps, thus the inference\nis serial, and the acoustic model faces the same situation. To\ntackle this problem, fast vocoders are ﬁrstly proposed, such\nas Parallel WaveNet (Oord et al. 2017), WaveGlow (Prenger,\nV alle, and Catanzaro 2019), WaveRNN (Kalchbrenner et al.\n2018) and LPCNet (V alin and Skoglund 2019). After the\nspeedup on vocoders, parallelization is then investigated on\nthe acoustic model. FastSpeech (Ren et al. 2019) breaks the\nautoregressive connection in its decoder, and employs Wave-\nGlow, a parallel vocoder, making the inference completely\nnon-autoregressive and two orders faster.\n6 Conclusion\nIn this paper, we ﬁrst give a deep analysis of why pre-\nvious neural TTS models are unstable. Among these rea-\nsons, the encoder-decoder attention borrowed from NMT\nis the most critical factor. TransformerTTS and Tacotron2\nalways generates bad cases on certain patterns, and Trans-\nformerTTS is more error-prone since it employs more such\nattention mechanisms. Besides, the position embedding also\nconstrains the maximum length of generated audio. Based\non this analysis, we propose RobuTrans, a robust neural\nTTS model based on Transformer, which is not only robust\neven for unseen context but also capable to synthesize nat-\nural speech audios, of which the quality is on parity with\nTransformerTTS and Tacotron2 on a general test domain.\nWe ﬁnd that FastSpeech is also robust since it employs simi-\nlar duration-based hard encoder-decoder attention, while our\nmodel outperforms it on audio quality and requires no addi-\ntional teacher model.\n7 Acknowledgments\nThis work is supported in part by National Science Foun-\ndation of China under Grant No.61572113, and Big Data\nApplication on lmproving Government Governance Capa-\nbilitiesNational Engineering Laboratory Open Fund Project.\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural ma-\nchine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473.\nCharpentier, F., and Stella, M. 1986. Diphone synthesis us-\ning an overlap-add technique for speech waveforms concate-\n8234\nnation. In Acoustics, Speech, and Signal Processing, IEEE\nInternational Conference on ICASSP’86., volume 11, 2015–\n2018. IEEE.\nGraves, A. 2013. Generating sequences with recurrent neu-\nral networks. arXiv preprint arXiv:1308.0850.\nHunt, A. J., and Black, A. W. 1996. Unit selection in a\nconcatenative speech synthesis system using a large speech\ndatabase. In Acoustics, Speech, and Signal Processing,\n1996. ICASSP-96. Conference Proceedings., 1996 IEEE In-\nternational Conference on, volume 1, 373–376. IEEE.\nKalchbrenner, N.; Elsen, E.; Simonyan, K.; Noury, S.;\nCasagrande, N.; Lockhart, E.; Stimberg, F.; Oord, A. v. d.;\nDieleman, S.; and Kavukcuoglu, K. 2018. Efﬁcient neural\naudio synthesis. arXiv preprint arXiv:1802.08435.\nLi, N.; Liu, S.; Liu, Y .; Zhao, S.; Liu, M.; and Zhou, M.\n2018. Neural speech synthesis with transformer network.\narXiv preprint arXiv:1809.08895.\nMaia, R.; Zen, H.; and Gales, M. J. 2010. Statistical para-\nmetric speech synthesis with joint estimation of acoustic and\nexcitation model parameters. InSeventh ISCA W orkshop on\nSpeech Synthesis.\nMehri, S.; Kumar, K.; Gulrajani, I.; Kumar, R.; Jain, S.;\nSotelo, J.; Courville, A.; and Bengio, Y . 2016. Samplernn:\nAn unconditional end-to-end neural audio generation model.\narXiv preprint arXiv:1612.07837.\nMohamed, A.; Okhonko, D.; and Zettlemoyer, L. 2019.\nTransformers with convolutional context for asr. arXiv\npreprint arXiv:1904.11660.\nOord, A. v. d.; Li, Y .; Babuschkin, I.; Simonyan, K.;\nVinyals, O.; Kavukcuoglu, K.; Driessche, G. v. d.; Lock-\nhart, E.; Cobo, L. C.; Stimberg, F.; et al. 2017. Parallel\nwavenet: Fast high-ﬁdelity speech synthesis.arXiv preprint\narXiv:1711.10433.\nPing, W.; Peng, K.; and Chen, J. 2018. Clarinet: Parallel\nwave generation in end-to-end text-to-speech.arXiv preprint\narXiv:1807.07281.\nPrenger, R.; V alle, R.; and Catanzaro, B. 2019. Waveg-\nlow: A ﬂow-based generative network for speech synthesis.\nIn ICASSP 2019-2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 3617–\n3621. IEEE.\nQian, Y .; Wu, Z.; Ma, X.; and Soong, F. 2010. Automatic\nprosody prediction and detection with conditional random\nﬁeld (crf) models. In2010 7th International Symposium on\nChinese Spoken Language Processing, 135–138. IEEE.\nRaffel, C.; Luong, M.-T.; Liu, P . J.; Weiss, R. J.; and Eck, D.\n2017. Online and linear-time attention by enforcing mono-\ntonic alignments. In Proceedings of the 34th International\nConference on Machine Learning-V olume 70, 2837–2846.\nJMLR. org.\nRen, Y .; Ruan, Y .; Tan, X.; Qin, T.; Zhao, S.; Zhao, Z.; and\nLiu, T. 2019. Fastspeech: Fast, robust and controllable text\nto speech. CoRR abs/1905.09263.\nShen, J.; Pang, R.; Weiss, R. J.; Schuster, M.; Jaitly, N.;\nYang, Z.; Chen, Z.; Zhang, Y .; Wang, Y .; Skerry-Ryan,\nR.; et al. 2017. Natural tts synthesis by conditioning\nwavenet on mel spectrogram predictions. arXiv preprint\narXiv:1712.05884.\nSotelo, J.; Mehri, S.; Kumar, K.; Santos, J. F.; Kastner, K.;\nCourville, A.; and Bengio, Y . 2017. Char2wav: End-to-end\nspeech synthesis. ICLR 2017 workshop.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence\nto sequence learning with neural networks. InAdvances in\nneural information processing systems, 3104–3112.\nTokuda, K.; Nankaku, Y .; Toda, T.; Zen, H.; Yamagishi, J.;\nand Oura, K. 2013. Speech synthesis based on hidden\nmarkov models. Proceedings of the IEEE 101(5):1234–\n1252.\nV alin, J.-M., and Skoglund, J. 2019. Lpcnet: Improving neu-\nral speech synthesis through linear prediction. InICASSP\n2019-2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 5891–5895. IEEE.\nV an Den Oord, A.; Dieleman, S.; Zen, H.; Simonyan, K.;\nVinyals, O.; Graves, A.; Kalchbrenner, N.; Senior, A. W.;\nand Kavukcuoglu, K. 2016. Wavenet: A generative model\nfor raw audio. InSSW, 125.\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. InAdvances in Neural Information\nProcessing Systems, 5998–6008.\nV erhelst, W., and Roelands, M. 1993. An overlap-add tech-\nnique based on waveform similarity (wsola) for high quality\ntime-scale modiﬁcation of speech. InAcoustics, Speech, and\nSignal Processing, 1993. ICASSP-93., 1993 IEEE Interna-\ntional Conference on, volume 2, 554–557. IEEE.\nViterbi, A. 1967. Error bounds for convolutional codes\nand an asymptotically optimum decoding algorithm.IEEE\ntransactions on Information Theory13(2):260–269.\nWang, Y .; Skerry-Ryan, R.; Stanton, D.; Wu, Y .; Weiss, R. J.;\nJaitly, N.; Yang, Z.; Xiao, Y .; Chen, Z.; Bengio, S.; et al.\n2017. Tacotron: A fully end-to-end text-to-speech synthesis\nmodel. arXiv preprint.\nZe, H.; Senior, A.; and Schuster, M. 2013. Statistical para-\nmetric speech synthesis using deep neural networks. In\nAcoustics, Speech and Signal Processing (ICASSP), 2013\nIEEE International Conference on, 7962–7966. IEEE.\nZen, H.; Tokuda, K.; and Black, A. W. 2009. Statisti-\ncal parametric speech synthesis. Speech Communication\n51(11):1039–1064.\nZhang, J.; Ling, Z.; and Dai, L. 2018. Forward attention\nin sequence-to-sequence acoustic modelling for speech syn-\nthesis. CoRR abs/1807.06736.\nZhu, X.; Zhang, Y .; Yang, S.; Xue, L.; and Xie, L. 2019. Pre-\nalignment guided attention for improving training efﬁciency\nand model stability in end-to-end speech synthesis. IEEE\nAccess 7:65955–65964.\n8235",
  "topic": "Naturalness",
  "concepts": [
    {
      "name": "Naturalness",
      "score": 0.7664954662322998
    },
    {
      "name": "Transformer",
      "score": 0.7338497042655945
    },
    {
      "name": "Computer science",
      "score": 0.7227357625961304
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.662295401096344
    },
    {
      "name": "Encoder",
      "score": 0.6085933446884155
    },
    {
      "name": "Artificial neural network",
      "score": 0.5501884818077087
    },
    {
      "name": "Embedding",
      "score": 0.524236261844635
    },
    {
      "name": "Speech recognition",
      "score": 0.4761659801006317
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4452645182609558
    },
    {
      "name": "Engineering",
      "score": 0.12159863114356995
    },
    {
      "name": "Voltage",
      "score": 0.11702403426170349
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150229711",
      "name": "University of Electronic Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}