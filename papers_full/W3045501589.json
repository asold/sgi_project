{
  "title": "Metaphor Detection Using Contextual Word Embeddings From Transformers",
  "url": "https://openalex.org/W3045501589",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2098513951",
      "name": "Jerry Liu",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A4213597035",
      "name": "Nathan O’Hara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2334185799",
      "name": "Alexander Rubin",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A3131724768",
      "name": "Rachel Draelos",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A2141705163",
      "name": "Cynthia Rudin",
      "affiliations": [
        "Duke University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2250915238",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2949442961",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2126530744",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W773678586",
    "https://openalex.org/W2252128138",
    "https://openalex.org/W2806685047",
    "https://openalex.org/W2806592685",
    "https://openalex.org/W3045533700",
    "https://openalex.org/W574867956",
    "https://openalex.org/W2805103385",
    "https://openalex.org/W2052417512",
    "https://openalex.org/W2806273110"
  ],
  "abstract": "The detection of metaphors can provide valuable information about a given text and is crucial to sentiment analysis and machine translation. In this paper, we outline the techniques for word-level metaphor detection used in our submission to the Second Shared Task on Metaphor Detection. We propose using both BERT and XLNet language models to create contextualized embeddings and a bi-directional LSTM to identify whether a given word is a metaphor. Our best model achieved F1-scores of 68.0% on VUA AllPOS, 73.0% on VUA Verbs, 66.9% on TOEFL AllPOS, and 69.7% on TOEFL Verbs, placing 7th, 6th, 5th, and 5th respectively. In addition, we outline another potential approach with a KNN-LSTM ensemble model that we did not have enough time to implement given the deadline for the competition. We show that a KNN classifier provides a similar F1-score on a validation set as the LSTM and yields different information on metaphors.",
  "full_text": "Proceedings of the Second Workshop on Figurative Language Processing, pages 250–255\nJuly 9, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n250\nMetaphor Detection Using Contextual Word Embeddings From\nTransformers\nJerry Liu, Nathan O’Hara, Alexander Rubin, Rachel Draelos, Cynthia Rudin\nDepartment of Computer Science, Duke University\n{jwl50, nmo4, acr43, rlb61}@duke.edu, cynthia@cs.duke.edu\nAbstract\nThe detection of metaphors can provide valu-\nable information about a given text and is cru-\ncial to sentiment analysis and machine trans-\nlation. In this paper, we outline the tech-\nniques for word-level metaphor detection used\nin our submission to the Second Shared Task\non Metaphor Detection. We propose using\nboth BERT and XLNet language models to\ncreate contextualized embeddings and a bi-\ndirectional LSTM to identify whether a given\nword is a metaphor. Our best model achieved\nF1-scores of 68.0% on VUA AllPOS, 73.0%\non VUA Verbs, 66.9% on TOEFL AllPOS,\nand 69.7% on TOEFL Verbs, placing 7th, 6th,\n5th, and 5th respectively. In addition, we out-\nline another potential approach with a KNN-\nLSTM ensemble model that we did not have\nenough time to implement given the deadline\nfor the competition. We show that a KNN clas-\nsiﬁer provides a similar F1-score on a valida-\ntion set as the LSTM and yields different infor-\nmation on metaphors.\n1 Introduction\nA metaphor is a form of ﬁgurative language that\ncreates a link between two different concepts and\nconveys rich linguistic information (Lakoﬁ and\nJohnson, 1980). The complex information that ac-\ncompanies a metaphorical text is often overlooked\nin sentiment analysis, machine translation, and in-\nformation extraction. Therefore, the detection of\nmetaphors is an important task in order to achieve\nthe full potential of many applications in natural\nlanguage processing (Tsvetkov et al., 2014).\nThe differences between a metaphorical text and\na non-metaphorical text can be subtle and require\nspeciﬁc domain information. For instance, in the\nphrase\nthe trajectory of your legal career\nthe word trajectory is used metaphorically. To iden-\ntify this metaphor, both the meaning of the word\nin the context of the sentence and its literal deﬁ-\nnition must be recognized and compared. In this\ncase, the word trajectory is used to describe the\npath of a legal career in the sentence, whereas its\nbasic deﬁnition involves the path of a projectile.\nAs a result of the ambiguity present in determining\nthe basic meaning of a word, as well as whether\nit deviates signiﬁcantly from a contextual use, de-\ntecting metaphors at a word-level can be challeng-\ning even for humans. Additionally, the Metaphor\nIdentiﬁcation Procedure used to label the datasets\n(MIPVU) accounts for multiple kinds of metaphors\n(Steen et al., 2010). Capturing implicit, complex\nmetaphors may require different information than\ncapturing direct, simple metaphors.\nThis paper describes the techniques that we uti-\nlized in the Second Shared Task on Metaphor De-\ntection. The competition provided two datasets: a\nsubset of ETS Corpus of Non-Native Written En-\nglish, which contains essays written by test-takers\nfor the TOEFL test and was annotated for argumen-\ntation relevant metaphors, and the VU Amsterdam\nMetaphor Corpus (VUA) dataset, which consists of\ntext fragments sampled across four genres from the\nBritish National Corpus (BNC) – Academic, News,\nConversation, and Fiction. For each dataset, par-\nticipants could compete in two tracks: identifying\nmetaphors of all parts of speech (AllPOS) or verbs\nonly (Verbs).\nOur ﬁnal submission uses pretrained BERT (De-\nvlin et al., 2018) and XLNet (Yang et al., 2019)\ntransformer models, part-of-speech (POS) labels,\nand a two-layer bi-directional long short-term mem-\nory (Bi-LSTM) neural network architecture. BERT\nand XLNet are used to generate contextualized\nword embeddings, which are then combined with\nPOS tags and fed through the Bi-LSTM to predict\nmetaphoricity for each word. By creating contex-\n251\ntualized word embeddings using transformers, we\nhoped to capture more long-range interdependen-\ncies between words than would be possible using\nmethods such as word2vec, GloVe, or fastText. In-\ndeed, our model achieved F1-scores of 68.0% on\nVUA AllPOS and 73.0% on VUA Verbs, improv-\ning upon results from the First Shared Task (Leong\net al., 2018). On the TOEFL task, we achieved F1-\nscores of 66.9% on AllPOS, and 69.7% on Verbs.\nOur scores placed 7th, 6th, 5th, and 5th respectively\nin the Second Shared Task on Metaphor Detection\n(Leong et al., 2020).\n2 Related Works\nHistorically, approaches to automatic metaphor de-\ntection have focused on hand-crafting a set of in-\nformative features for every word and applying\na supervised machine learning algorithm to clas-\nsify words as metaphorical or non-metaphorical.\nPrevious works have explored features including\nPOS tags, concreteness, imageability, semantic dis-\ntributions, and semantic classes as characterized\nthrough SUMO ontology, WordNet, and VerbNet\n(Beigman Klebanov et al., 2014; Tsvetkov et al.,\n2014; Dunn, 2013; Mohler et al., 2013).\nDeep learning methods have also been em-\nployed for automatic metaphor detection. In the\nFirst Shared Task on Metaphor Detection, the\ntop three highest scoring teams all employed an\nLSTM model with word embeddings and addi-\ntional features (Leong et al., 2018). Stemle and\nOnysko (2018) trained fastText word embeddings\non various native and non-native English corpora,\nand passed the sequences of embeddings to an\nBi-LSTM. The highest-performing model from\nBizzoni and Ghanimifard (2018) employed a Bi-\nLSTM on GloVe embeddings and concreteness rat-\nings for each word. Wu et al. (2018) appended\nPOS and semantic class information to pretrained\nword2vec word embeddings, and utilized a CNN\nin addition to a Bi-LSTM in order to better cap-\nture local and global contextual information. In all\nthese cases, the word embeddings used are context-\nindependent: the same word appearing in two dif-\nferent sentences will nonetheless have the same\nembedding. Thus, these embeddings may not be\nable to fully capture information about multi-sense\nwords (for example, the word bank in river bank\nand bank robber), which is crucial for properly\nidentifying metaphors.\nMore recently, Mao et al. (2019) proposed two\nRNN models for word-level metaphor detection\nbased on linguistic theories of metaphor identiﬁ-\ncation. GloVe and ELMo embeddings are used\nas input features that capture literal meanings of\nwords, which are compared with the hidden states\nof Bi-LSTMs that capture contextual meaning. We\nchose to explore transformer-based embeddings as\nan alternative way to capture contextual informa-\ntion.\nTransformer-based models have shown state-of-\nthe-art results on a wide variety of language tasks,\nincluding sentence classiﬁcation, question answer-\ning, and named entity recognition. These models\nrely on self-attention mechanisms to capture global\ndependencies, and can be used to generate contex-\ntualized word embeddings. We chose to examine\nthe models BERT, GPT2, and XLNet. These three\nmodels all achieve remarkable performances on var-\nious NLP tasks, but they capture long-distance re-\nlationships within the text in different ways. BERT\nis an autoencoder model, consisting of a stack of\nencoder layers, and is able to capture bi-directional\ncontext using masking during training (Devlin et al.,\n2018). GPT2 is an autoregressive model, consisting\nof a stack of decoder layers, and thus is only able\nto capture unidirectional context (Radford et al.,\n2018). XLNet is also autoregressive, but it captures\nbi-directional context by considering all permuta-\ntions of the given words (Yang et al., 2019). Each\nof these models has its advantages and disadvan-\ntages that are worth exploring in the context of\nmetaphor detection.\n3 Methodology\nOur method for metaphor detection begins with\ngenerating contextualized word embeddings for\neach word in a sentence using the hidden states\nof pretrained BERT and XLNet language models.\nNext, those embeddings are concatenated together,\nPOS tags for each word are appended to the embed-\ndings, and a Bi-LSTM reads the features as input\nand classiﬁes each word in the sentence.\nWord Embeddings Due to limited metaphor-\nannotated data, rather than training a transformer\nmodel on our downstream task, we instead opted to\ntake a feature-based approach to generating contex-\ntualized word embeddings from pretrained trans-\nformer models. This idea was inspired by the ap-\nproach to the token-level named entity recogni-\ntion task described in Devlin et al. (2018), which\nused a number of strategies for combining hidden\n252\nFigure 1: Our model architecture. Sentences are fed through pretrained BERT and XLNet models, concatenated\nalong with POS tags, passed to a Bi-LSTM, and a sigmoid layer outputs probabilities.\nstate representations of words from a pretrained\nBERT model to generate contextualized word em-\nbeddings.\nWe installed the Python transformers library de-\nveloped by huggingface (Wolf et al., 2019), which\nincludes a PyTorch (Paszke et al., 2019) imple-\nmentation of BERT and several pretrained BERT\nmodels. We opted to use the BERT base uncased\nmodel, which consists of 12-layers, 768-hidden,\n12-heads, and 110M parameters. For each line in\nthe VUA and TOEFL datasets, we use the BERT\ntokenizer included in the transformers package to\npre-process the text, then generate hidden-state rep-\nresentations for each word by inputting each line\ninto the pretrained BERT model. Each token is\ngiven a 12x768 hidden-state representation from\nBERT. We generate 768-dimension word embed-\ndings by summing the values from each of the 12\nhidden layers for each token. Words out-of-vocab\nfor BERT are split into multiple tokens represent-\ning subwords. To generate embeddings for these\nwords, embeddings are generated for each subword\ntoken, then averaged together.\nSimilarly, we installed the huggingface imple-\nmentation of XLNet and used its pretrained XLNet\nbase uncased model to generate embeddings for\neach word in the dataset using the same method as\nwith BERT.\nOnce both embeddings are generated, we con-\ncatenate the BERT and XLNet embeddings for each\nword to generate 1536-dimensional word embed-\ndings. By combining word embeddings from mul-\ntiple high-performing pretrained transformers, we\nare able to capture more contextual information\nfor each word. Additionally, we supplement these\nword embeddings with the POS tag for each word\nas generated by the Stanford parser (Toutanova\net al., 2003). POS tags were shown to improve\nmetaphor detection in the 2018 Metaphor Detec-\ntion Shared Task (Leong et al., 2018), and we ﬁnd\na small improvement by including them here.\nNeural Network We pass the features from each\nsentence into a Bi-LSTM. The purpose of this net-\nwork is to capture long-range relationships between\nwords in the same sentence which may reveal the\npresence of metaphors. We use a dense layer with\na sigmoid activation function to obtain the pre-\ndicted probability of being a metaphor for each\nword in the sentence. During training, we employ\na weighted binary cross entropy loss function to\naddress the extreme class imbalance, since non-\nmetaphors occur signiﬁcantly more frequently than\nmetaphors. Hyperparameters were tuned via cross-\nvalidation. For the testing phase, we use an en-\nsemble strategy which was effective for Wu et al.\n(2018): we trained four copies of this Bi-LSTM\nwith different initializations and averaged the pre-\n253\ndictions from each model.\nAdditionally, we noted that our model tended to\nassign similar probabilities to different instances\nof the same word in different contexts, and that\na prediction signiﬁcantly higher than the average\nprediction for that word was a good indicator of\nthe presence of metaphor, even if the prediction\nfell lower than the ideal threshold. Thus, we used\nthe following procedure for the testing phase: label\nthe word as a metaphor if its predicted probability\nis higher than the threshold, or if its probability is\nthree orders of magnitude higher than the median\npredicted probability for that word in the evalu-\nation set. We found this to be a useful way of\naddressing the domain shift between the training\nand the test data. This concept is further explored\nin Section 4.1.\n4 Experiments\nWord Embeddings Devlin et al. (2018) suggest\nthat for different token-level classiﬁcation tasks,\ndifferent methods for combining hidden states from\nBERT may prove effective in generating contextu-\nalized word embeddings. For our task, to determine\nthe optimal embedding strategy, we evaluated four\ndifferent methods of combining information from\nhidden states of the transformer models. To deter-\nmine which performed best prior to training LSTM\nmodels, we tested each strategy using logistic re-\ngression on the word embeddings with an 80/20\ntraining-test split. Results from logistic regression\non BERT embeddings from the VUA AllPOS data\nare in Table 1. We note that the F1 scores using dif-\nferent methods of generating contextualized word\nembeddings differ substantially. We use the ”sum-\nall-layers” method of generating word embeddings\nfor our further experiments.\nVUA AllPOS TOEFL AllPOS\nMethod P R F1 P R F1\nSum alllayers 0.672 0.531 0.593 0.569 0.596 0.582\nConcat. last4 layers 0.614 0.552 0.581 0.644 0.473 0.546\nSum last4 layers 0.623 0.534 0.575 0.594 0.550 0.571\nSecond tolast layer 0.580 0.547 0.563 0.633 0.482 0.547\nLast layer 0.628 0.493 0.553 0.542 0.551 0.546\nTable 1: Logistic regression on various BERT word em-\nbeddings, VUA and TOEFL AllPOS.\nTransformers Table 2 compares the perfor-\nmance of the Bi-LSTM using the embeddings from\nBERT, GPT2, and XLNet. Because the true test\nlabels were not made available to us, here we report\nresults on an 80/20 training-test split of the given\ntraining data. We make the following observations.\n•The LSTM models perform far better than\ntheir logistic regression counterparts. Of the\nsingle embedding LSTM models, the BERT\nand XLNet embeddings have the best perfor-\nmances. Combining BERT and XLNet embed-\ndings and using an ensemble strategy further\nimproved our performance.\n•In general, the AllPOS task is more challeng-\ning than the Verbs task. Different parts of\nspeech are used metaphorically in different\nways, and these multiple varieties of metaphor\nmust all be captured by a single model in the\nAllPOS task. Correspondingly, all models\nperform worse on AllPOS than Verbs in both\nVUA and TOEFL datasets.\n•Additionally, the models achieve a lower F1\nscore on the TOEFL dataset than the VUA\nin both AllPOS and Verbs track. We believe\nthis is in part due to the smaller size of the\nTOEFL dataset, and in part because linguis-\ntic characteristics can differ substantially be-\ntween native and non-native text. Since we\nused transformer models pretrained on a na-\ntive corpus, the word embeddings were likely\nless informative for the TOEFL track.\n•GPT2 and XLNet are both autoregressive lan-\nguage models, but GPT2+LSTM performs sig-\nniﬁcantly worse than the other LSTM models.\nThis result suggests that bi-directional rela-\ntionships between words play a crucial role\nin metaphor detection. Because XLNet con-\nsiders every possible permutation of the given\nwords during training, the XLNet embeddings\nlikely contain more bi-directional context than\nthe GPT2 embeddings.\n4.1 A Promising Future Approach:\nK-Nearest Neighbors\nIn our experiments, we noted that our LSTM mod-\nels tended to output similar probabilities for dif-\nferent instances of the same word independent of\ncontext. For example, although 4 out of 14 of the\noccurrences of the word capacity in the validation\nset were metaphor-related, all of the LSTM predic-\ntions were less than 10−5. This suggested that al-\nthough word embeddings from transformer models\n254\nModel VUA AllPOS VUA Verbs TOEFL AllPOS TOEFL Verbs\nP R F1 P R F1 P R F1 P R F1\nBaseline* 0.608 0.700 0.651 0.600 0.763 0.672 N/A N/A N/A N/A N/A N/A\nBERT+LSTM 0.644 0.689 0.666 0.662 0.730 0.694 0.618 0.648 0.633 0.611 0.670 0.639\nGPT2+LSTM 0.592 0.573 0.582 0.618 0.648 0.633 0.579 0.589 0.584 0.555 0.681 0.612\nXLNet+LSTM 0.622 0.622 0.622 0.650 0.684 0.667 0.644 0.646 0.645 0.633 0.681 0.656\nBERT+XLNet\n+LSTM 0.665 0.688 0.676 0.655 0.736 0.693 0.649 0.664 0.656 0.618 0.724 0.667\nBERT+XLNet\n+LSTM\n(ensemble) 0.675 0.710 0.692 0.656 0.768 0.708 0.686 0.654 0.669 0.722 0.659 0.689\nTable 2: Performance of LSTM models. The baseline is the highest achieved score from the First Shared Task on\nMetaphor Detection.\ncontain more contextual information than embed-\ndings from word2vec or GloVe, the model could\nbe improved by including even more contextual\ninformation. We explored the idea of ensembling\nan LSTM with a K-Nearest Neighbors (KNN) clas-\nsiﬁcation approach. We believe that the LSTM\napproach would give information as to which types\nof words tend to be metaphors in context, whereas\nthe KNN approach would clue into whether a spe-\nciﬁc use of a speciﬁc word is more likely to be\nmetaphorical. We were unable to fully implement\nsuch an ensemble model for the competition, but\nwe detail some promising results below.\nWe trained a KNN-only model using our contex-\ntualized word embeddings. First, we lemmatized\neach word in the VUA and TOEFL datasets. For\nVUA, we classiﬁed each word based on a KNN\nclassiﬁer trained on all instances of the same lem-\nmatized word in the training data. If no such lem-\nmatized word existed in the training data, we clas-\nsiﬁed that word using a prediction from an LSTM\nmodel, though that occurred in only 2% of cases.\nFor TOEFL, we compared using training data from\nTOEFL combined with VUA due to the limited\ndataset. We achieved F1 scores of 0.642 and 0.608\non 80/20 training-test splits of VUA and TOEFL\nrespectively, not much worse than our LSTM mod-\nels.\nThere is reason to believe the LSTM and KNN\napproaches capture signiﬁcantly different informa-\ntion on metaphors. On the VUA validation data, the\nLSTM method predicted 3751 metaphors and the\nKNN predicted 3190. However, only 2372 words\nwere predicted as metaphors by the two models\ntogether. Since both models have similar F1 scores,\nthis implies that a superior classiﬁer can be con-\nstructed using information from both classiﬁers.\nFor our ﬁnal submissions, we were able to adopt\na simpliﬁed implementation of this approach, la-\nbeling an instance of a word as metaphorical if its\nLSTM prediction either was higher than a certain\nthreshold, or higher by a signiﬁcant amount than\nthe median LSTM prediction of all instances of\nthat word. This procedure improved our F1 scores\nby about 1% during the testing phase.\nk Precision Recall F1\n1 0.665 0.599 0.630\n2 0.722 0.514 0.600\n3 0.676 0.611 0.642\n4 0.703 0.538 0.610\n5 0.679 0.604 0.639\nTable 3: KNN using sum-all BERT word embeddings,\nVUA AllPOS\n5 Conclusion\nIn this paper, we describe the best performing\nmodel that we submitted for the Second Shared\nTask on Metaphor Detection. We used BERT and\nXLNet language models to create contextualized\nembeddings, and fed these embeddings into a bi-\ndirectional LSTM with a sigmoid layer that used\nboth local and global contextual information to out-\nput a probability. Our experimental results verify\nthat contextualized embeddings outperform previ-\nous state-of-the-art word embeddings for metaphor\ndetection. We also propose an ensemble model\ncombining a bi-directional LSTM and a KNN, and\nshow promising results that suggest the two models\nencode complementary information on metaphors.\nAcknowledgments\nThe authors thank the organizers of the Second\nShared Task on Metaphor Detection and the rest\nof the Duke Data Science Team. We also thank\nthe anonymous reviewers for their insightful com-\nments.\n255\nReferences\nBeata Beigman Klebanov, Ben Leong, Michael Heil-\nman, and Michael Flor. 2014. Different texts, same\nmetaphors: Unigrams and beyond. In Proceedings\nof the Second Workshop on Metaphor in NLP, pages\n11–17, Baltimore, MD. Association for Computa-\ntional Linguistics.\nYuri Bizzoni and Mehdi Ghanimifard. 2018. Bigrams\nand BiLSTMs two neural networks for sequential\nmetaphor detection. In Proceedings of the Workshop\non Figurative Language Processing, pages 91–101,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv, arXiv:1810.04805.\nJonathan Dunn. 2013. What metaphor identiﬁcation\nsystems can tell us about metaphor-in-language. In\nProceedings of the First Workshop on Metaphor in\nNLP, pages 1–10, Atlanta, Georgia. Association for\nComputational Linguistics.\nGeorge Lakoﬁ and Mark Johnson. 1980. Metaphors we\nlive by. University of Chicago Press, Chicago, IL.\nChee Wee Leong, Beata Beigman Klebanov, Chris\nHamill, Egon Stemle, Rutuja Ubale, and Xianyang\nChen. 2020. A report on the 2020 vua and toeﬂ\nmetaphor detection shared task. In Proceedings of\nthe Second Workshop on Figurative Language Pro-\ncessing, Seattle, W A.\nChee Wee (Ben) Leong, Beata Beigman Klebanov, and\nEkaterina Shutova. 2018. A report on the 2018 VUA\nmetaphor detection shared task. In Proceedings of\nthe Workshop on Figurative Language Processing ,\npages 56–66, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nRui Mao, Chenghua Lin, and Frank Guerin. 2019. End-\nto-end sequential metaphor identiﬁcation inspired by\nlinguistic theories. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3888–3898, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMichael Mohler, David Bracewell, Marc Tomlinson,\nand David Hinote. 2013. Semantic signatures for\nexample-based linguistic metaphor detection. In\nProceedings of the First Workshop on Metaphor in\nNLP, pages 27–35, Atlanta, Georgia. Association for\nComputational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018. Language\nmodels are unsupervised multitask learners.\nGerard J. Steen, Aletta G. Dorst, J. Berenike Herrmann,\nAnna A. Kaal, Tina Krennmayr, and Trijntje Pasma.\n2010. A Method for Linguistic Metaphor Identiﬁca-\ntion: From MIP to MIPVU . John Benjamins Pub-\nlishing.\nEgon Stemle and Alexander Onysko. 2018. Using\nlanguage learner data for metaphor detection. In\nProceedings of the Workshop on Figurative Lan-\nguage Processing , pages 133–138, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nKristina Toutanova, Dan Klein, Christopher D. Man-\nning, and Yoram Singer. 2003. Feature-rich part-of-\nspeech tagging with a cyclic dependency network.\nIn Proceedings of the 2003 Human Language Tech-\nnology Conference of the North American Chapter\nof the Association for Computational Linguistics ,\npages 252–259.\nYulia Tsvetkov, Leonid Boytsov, Anatole Gershman,\nEric Nyberg, and Chris Dyer. 2014. Metaphor detec-\ntion with cross-lingual model transfer. In Proceed-\nings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 248–258, Baltimore, Maryland. Associ-\nation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. arXiv, arXiv:1910.03771.\nChuhan Wu, Fangzhao Wu, Yubo Chen, Sixing Wu,\nZhigang Yuan, and Yongfeng Huang. 2018. Neu-\nral metaphor detecting with CNN-LSTM model. In\nProceedings of the Workshop on Figurative Lan-\nguage Processing , pages 110–114, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. arXiv, arXiv:1906.08237.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8154922723770142
    },
    {
      "name": "Metaphor",
      "score": 0.7935123443603516
    },
    {
      "name": "Natural language processing",
      "score": 0.6974928379058838
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6914153099060059
    },
    {
      "name": "Transformer",
      "score": 0.648003339767456
    },
    {
      "name": "Machine translation",
      "score": 0.6441332697868347
    },
    {
      "name": "Word (group theory)",
      "score": 0.5655789971351624
    },
    {
      "name": "Classifier (UML)",
      "score": 0.531362771987915
    },
    {
      "name": "Linguistics",
      "score": 0.2160840630531311
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170897317",
      "name": "Duke University",
      "country": "US"
    }
  ]
}