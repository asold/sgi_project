{
  "title": "An Efficient Transformer Decoder with Compressed Sub-layers",
  "url": "https://openalex.org/W3120658156",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2711656284",
      "name": "Li Yanyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102314178",
      "name": "Lin Ye",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097229227",
      "name": "Xiao Tong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114650367",
      "name": "Zhu Jingbo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2997347790",
    "https://openalex.org/W2963112338",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2949454572",
    "https://openalex.org/W2919290281",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2806697599",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W2799001369",
    "https://openalex.org/W2952509486",
    "https://openalex.org/W2971191050",
    "https://openalex.org/W2948798935",
    "https://openalex.org/W2955646770",
    "https://openalex.org/W2890964657",
    "https://openalex.org/W3035083896"
  ],
  "abstract": "The large attention-based encoder-decoder network (Transformer) has become prevailing recently due to its effectiveness. But the high computation complexity of its decoder raises the inefficiency issue. By examining the mathematic formulation of the decoder, we show that under some mild conditions, the architecture could be simplified by compressing its sub-layers, the basic building block of Transformer, and achieves a higher parallelism. We thereby propose Compressed Attention Network, whose decoder layer consists of only one sub-layer instead of three. Extensive experiments on 14 WMT machine translation tasks show that our model is 1.42x faster with performance on par with a strong baseline. This strong baseline is already 2x faster than the widely used standard baseline without loss in performance.",
  "full_text": "arXiv:2101.00542v4  [cs.CL]  11 May 2023\nAn Efﬁcient T ransformer Decoder with Compressed Sub-layer s\nY anyang Li1* , Y e Lin 1 ∗, T ong Xiao 1,2† , Jingbo Zhu 1,2\n1 NLP Lab, School of Computer Science and Engineering, Northe astern University, Shenyang, China\n2 NiuTrans Research, Shenyang, China\n{blamedrlee, linye2015 }@outlook.com, {xiaotong,zhujingbo }@mail.neu.edu.cn\nAbstract\nThe large attention-based encoder-decoder network (Trans -\nformer) has become prevailing recently due to its effective -\nness. But the high computation complexity of its decoder\nraises the inefﬁciency issue. By examining the mathematic\nformulation of the decoder, we show that under some mild\nconditions, the architecture could be simpliﬁed by compres s-\ning its sub-layers, the basic building block of Transformer ,\nand achieves a higher parallelism. W e thereby propose Com-\npressed Attention Network , whose decoder layer consists of\nonly one sub-layer instead of three. Extensive experiments\non 14 WMT machine translation tasks show that our model is\n1.42× faster with performance on par with a strong baseline.\nThis strong baseline is already 2 × faster than the widely used\nstandard baseline without loss in performance.\nIntroduction\nTransformer is an attention-based encoder-decoder model\n(V aswani et al. 2017). It has shown promising results in ma-\nchine translation tasks recently (W ang et al. 2019; Li et al.\n2020a; Zhang et al. 2020; Li et al. 2020b). Nonetheless,\nTransformer suffers from the inefﬁciency issue at inferenc e.\nThis problem is attributed to the Transformer decoder for\ntwo reasons: 1) the decoder is deep (Kasai et al. 2020). It\nconsists of multiple layers and each layer contains three\nsub-layers, including two attentions and a feed-forward ne t-\nwork; 2) the attention has a high (quadratic time) complex-\nity (Zhang, Xiong, and Su 2018), as it needs to compute the\ncorrelation between any two input words.\nPrevious work has focused on improving the complex-\nity of the attention in the decoder to accelerate the in-\nference. For example, A A N uses the averaging operation\nto avoid computing the correlation between input words\n(Zhang, Xiong, and Su 2018). S A N share the attention re-\nsults among layers (Xiao et al. 2019). On the other hand,\nwe learn that vanilla attention runs faster in training than\nin inference thanks to its parallelism. This offers a new di-\nrection: a higher degree of parallelism could speed up the\ninference. The most representative work of this type is the\nnon-autoregressive approach (Gu et al. 2018). Its decoder\n* Authors contributed equally .\n† Corresponding author.\nCopyright © 2021, Association for the Advancement of Artiﬁc ial\nIntelligence (www .aaai.org). All rights reserved.\npredicts all words in parallel, but fails to model the word\ndependencies. Despite of their successes, all these system s\nstill have a deep decoder.\nIn this work, we propose to parallelize the sub-layers to\nobtain a shallow autoregressive decoder. This way does not\nsuffer from the poor result of directly reducing depths and\navoids the limitation of non-autoregressive approaches. W e\nprove that the two attention sub-layers in a decoder layer\ncould be parallelized if we assume their inputs are close to\neach other. This assumption holds and thereby we compress\nthese two attentions into one. Furthermore, we show that\nthe remaining feed-forward network could also be merged\ninto the attention due to their linearity. T o the end, we pro-\npose Compressed Attention Network (C A N for short). The\ndecoder layer of C A N possesses a single attention sub-layer\nthat does the previous three sub-layers’ jobs in parallel.\nAs another “bonus”, C A N is simple and easy to be imple-\nmented.\nIn addition, Kasai et al. (2020) empirically discover that\nexisting systems are not well balancing the encoder and de-\ncoder depths. Based on their work, we build a system with a\ndeep encoder and a shallow decoder, which is 2 × faster than\nthe widely used standard baseline without loss in perfor-\nmance. It requires neither the architecture modiﬁcation no r\nadding extra parameters. This system serves as a stronger\nbaseline for a more convincing comparison.\nW e evaluate C A N and the stronger baseline in 14 machine\ntranslation tasks, including WMT14 English ↔{ German,\nFrench} (En↔{ De, Fr }) and WMT17 English ↔{ German,\nFinnish, Latvian, Russian, Czech } (En↔{ De, Fi, Lv, Ru,\nCs}). The experiments show that C A N is up to 2.82 × faster\nthan the standard baseline with almost no loss in perfor-\nmance. Even comparing to our stronger baseline, C A N still\nhas a 1.42 × speed-up, while other acceleration techniques\nsuch as S A N and A A N are 1.12 ∼ 1.16× in the same case.\nT o summarize, our contributions are as follows:\n• W e propose C A N, a novel architecture that accelerates\nTransformer by compressing its sub-layers for a higher\ndegree of parallelism. C A N is easy to be implemented.\n• Our work is based on a stronger baseline, which is 2 ×\nfaster than the widely used standard baseline.\n• The extensive experiments on 14 WMT machine transla-\ntion tasks show that C A N is 1.42 × faster than the stronger\n×N\nwø3 hˇ en hˇ ao .\nEncoder\nI am ﬁne .\nam ﬁne . ⟨eos⟩\nSelf-Attention\nCross-Attention\nFFNT arget T oken\nSource T oken\nDecoder\n(a) Transformer\n×N\nwø3 hˇ en hˇ ao .\nEncoder I am ﬁne .\nam ﬁne . ⟨eos⟩\nCompressed-Attention\nT arget T oken\nSource T oken\nDecoder\n(b) C A N\nFigure 1: Transformer vs. C A N (Chinese pinyin → English: “wø3 h ˇen h ˇao . ” → “I am ﬁne . ” ).\nbaseline and 2.82 × for the standard baseline. C A N also\noutperforms other approaches such as S A N and A A N.\nBackground: T ransformer\nTransformer is one of the state-of-the-art neural models in\nmachine translation. It consists of a N-layer encoder and a\nN-layer decoder, where N = 6 in most cases. The encoder\nmaps the source sentence to a sequence of continuous rep-\nresentations and the decoder maps these representations to\nthe target sentence. All layers in the encoder or decoder are\nidentical to each other.\nThe layer in the decoder consists of three sub-layers, in-\ncluding the self-attention, the cross-attention and the fe ed-\nforward network (FFN). The self-attention takes the output\nX of the previous sub-layer as its input and produces a ten-\nsor with the same size as its output. It computes the attentio n\ndistribution Ax and then averages X by Ax. W e denote the\nself-attention as Yx = Self( X), where X ∈ Rt×d, t is the\ntarget sentence length and d is the dimension of the hidden\nrepresentation:\nAx = SoftMax( XWq1W T\nk1XT\n√\nd\n) (1)\nYx = AxXWv1 (2)\nwhere Wq1, Wk1, Wv1 ∈ Rd×d.\nThe cross-attention is similar to the self-attention, exce pt\nthat it takes the encoder output H as an additional input.\nW e denote the cross-attention as Yh = Cross( X, H), where\nH ∈ Rs×d, s is the source sentence length:\nAh = SoftMax( XWq2W T\nk2HT\n√\nd\n) (3)\nYh = AhHWv2 (4)\nwhere Wq2, Wk2, Wv2 ∈ Rd×d.\nThe FFN applies non-linear transformation to its input X.\nW e denote the FFN as Yf = FFN( X):\nYf = ReLU( XW1 + b1)W2 + b2 (5)\n1 2 3 4 5 6\n1\n2\n3\n4\n5\n6\nCross-Attention\nSelf-Attention\n1 2 3 4 5 6\n1\n2\n3\n4\n5\n6\nFFN\nCross-Attention\nFigure 2: The cosine similarity of inputs for every two ad-\njacent sub-layers on WMT14 En-De translation task (a dark\ncell means the inputs are dissimilar).\nwhere W1 ∈ Rd×4d, b1 ∈ R4d, W2 ∈ R4d×d and b2 ∈ Rd.\nAll sub-layers are coupled with the residual connection\n(He et al. 2016a), i.e., Y = f(X)+ X where f could be any\nsub-layer. Their inputs are also preprocessed by the layer\nnormalization ﬁrst (Ba, Kiros, and Hinton 2016). Fig. 1(a)\nshows the architecture of Transformer decoder. For more de-\ntails, we refer the reader to V aswani et al. (2017).\nCompressed Attention Network\nCompressing Self-Attention and Cross-Attention\nAs suggested by Huang et al. (2016), the output of one layer\nin the residual network can be decomposed into the sum\nof all outputs from previous layers. For the adjacent self-\nattention and cross-attention, we can write their ﬁnal outp ut\nas Y = X + Self( X) + Cross( X′, H), where X is the in-\nput of self-attention and X′ = X + Self( X) is the input\nof cross-attention. If X and X′ are identical, we are able to\naccelerate the computation of Y by parallelizing these two\nattentions, as X′ do not need to wait Self(X) to ﬁnish.\nPrevious work (He et al. 2016b) has shown that inputs of\nadjacent layers are similar. This implies that X and X′ are\nclose and the parallelization is possible. W e empirically v er-\nify this in the left part of Fig. 2 by examining the cosine\nsimilarity between inputs of every self-attention and cros s-\nattention pairs. It shows that X and X′ are indeed close to\n[H, X]\nY\nd 4d\nA\nd\nY=\n[\nH˜Wv2, X˜Wv1\n]\nEq. 11\nY = AY\nY = XW1 + Y + b1\nY = ReLU( Y )W2 + b2\nInput\nOutputT arget T oken X\nSource T oken H\nFigure 3: Compressed-Attention.\neach other (a high similarity > 0.9 for the diagonal entries).\nTherefore we could assume X and X′ are identical (we omit\nthe layer normalization for simplicity):\nY = X + Self( X) + Cross( X, H) (6)\nBy observing that Eq. 2 and Eq. 4 are essentially matrix\nmultiplications, we could rewrite Self(X) + Cross( X, H)\nas a single matrix multiplication:\nA =\n[\nAT\nx , AT\nh\n] T\n(7)\nSelf(X) + Cross( X, H) = A [XWv1, HWv2] (8)\n[·] is the concatenation operation along the ﬁrst dimension.\nXiao et al. (2019) shows that some attention distributions\nAx and Ah are duplicate. This means that there exists a cer-\ntain redundancy in {Wq1, Wk1} and {Wq2, Wk2}. Thus we\ncould safely share Wq1 and Wq2 to parallelize the computa-\ntion of the attention distribution A:\n¯A =\n(\nXWq [XWk1, HWk2]T\n)\n/\n√\nd (9)\nA =\n[\nSoftMax( ¯AT\n·,1...t),SoftMax( ¯AT\n·,t+1...t+s)\n] T\n(10)\nHowever, A consists of two SoftMax distributions and\nis used in Eq. 8 without normalization. The output vari-\nance is then doubled and leads to poor optimization\n(Glorot and Bengio 2010). It is advised to divide A by\n√\n2\nto preserve the variance. This way resembles a single distri -\nbution. So we use one SoftMax instead and this works well:\nA = SoftMax( XWq [XWk1, HWk2]T\n√\nd\n) (11)\nNow , we can compute Y in Eq. 6 efﬁciently by using Eq.\n11 as well as Eq. 8 to compute Self(X) + Cross( X, H).\nCompressing Attention and FFN\nIt is natural to consider to merge the attention and FFN with\nthe same approach for further speed-up. As suggested by\nthe right part of Fig. 2, the similarities between inputs of\n6/6 9/4 12/2 14/1\n26\n27\n28\n# of Encoder Layers/# of Decoder Layers\nBLEU [%]\n100\n200\n300\nSpeed\nBLEU Speed\nFigure 4: Performance (BLEU) and translation speed (to-\nken/sec) vs. the numbers of encoder and decoder layers on\nWMT14 En-De translation task.\nthe adjacent cross-attention and FFN are low (dark diagonal\nentries). This implies that it is not ideal to make the identi cal\ninput assumption to parallelize the cross-attention and FF N.\nHere we provide another solution. Given that attention is\nmerely a weighted sum and FFN performs a linear projection\nﬁrst, we can merge them by exploiting the linearity. This\nway not only parallelizes the computation of attention and\nFFN but also removes redundant matrix multiplications.\nW e substitute X in Eq. 5 by Y in Eq. 6:\nYf = ReLU( XW1 + A [XWv1, HWv2] W1 + b1)W2 + b2\n(12)\nW e can combine W1 with Wv1 as well as Wv2 into\n˜Wv1,˜Wv2 ∈ Rd×4d, as these matrices are learnable and ma-\ntrix multiplied together:\nYf = ReLU( XW1 + A\n[\nX˜Wv1, H˜Wv2\n]\n+ b1)W2 + b2\n(13)\nFurthermore, XW1 can be computed in parallel with other\ntransformations such as XWq.\nThis eventually gives us an more efﬁcient decoder layer\narchitecture, named Compressed-Attention. The whole com-\nputation process is shown in Fig. 3: it ﬁrst computes the at-\ntention distribution A by Eq. 11, then performs the attention\noperation via Eq. 13, and produces Yf as the ﬁnal result.\nThe proposed Compressed Attention Network (C A N) stacks\ncompressed-attentions to form its decoder. Fig. 1 shows the\ndifference between Transformer and C A N.\nBalancing Encoder and Decoder Depths\nBased on the ﬁndings of Kasai et al. (2020), we learn that\na shallow decoder could offer a great speed gain, while\na deep encoder could make up of the loss of a shal-\nlow decoder without adding a heavy computation over-\nhead. Since their work is based on knowledge distillation\n(Hinton, V inyals, and Dean 2015), here we re-examine this\nidea under the standard training setting (without knowledg e\ndistillation).\nFig. 4 shows the performance and speed if we gradually\nreduce the decoder depth while adding more encoder layers.\nW e see that although the overall number of parameters re-\nmains the same, the baseline can be 2 × faster without losing\nSource Lang. Train V alid T est\nsent. word sent. word sent. word\nWMT14 En↔ De 4.5M 220M 3000 110K 3003 114K\nEn↔ Fr 35M 2.2B 26K 1.7M 3003 155K\nWMT17\nEn↔ De 5.9M 276M 8171 356K 3004 128K\nEn↔ Fi 2.6M 108M 8870 330K 3002 110K\nEn↔ Lv 4.5M 115M 2003 90K 2001 88K\nEn↔ Ru 25M 1.2B 8819 391K 3001 132K\nEn↔ Cs 52M 1.2B 8658 354K 3005 118K\nT able 1: Data statistics (# of sentences and # of words).\nany performance (12/2 vs. 6/6). This justiﬁes the previous\nidea. W e thereby choose a stronger baseline with a 12-layer\nencoder and a 2-layer decoder for a more convincing com-\nparison. This setting is also applied to C A N.\nExperiments\nExperimental Setup\nDatasets W e evaluate our methods on 14 machine trans-\nlation tasks (7 datasets × 2 translation directions each), in-\ncluding WMT14 En ↔{ De, Fr } and WMT17 En ↔{ De, Fi,\nLv, Ru, Cs }.\nWMT14 En ↔{ De, Fr } datasets are tokenized by a script\nfrom Moses 1 . W e apply BPE (Sennrich, Haddow , and Birch\n2016) with 32K merge operations to segment words into\nsubword units. Sentences with more than 250 subword units\nare removed. The ﬁrst two rows of T able 1 are the detailed\nstatistics of these two datasets. For En-De, we share the\nsource and target vocabularies. W e choose newstest-2013 as\nthe validation set and newstest-2014 as the test set. For En-\nFr, we validate the system on the combination of newstest-\n2012 and newstest-2013, and test it on newstest-2014.\nAll WMT17 datasets are the ofﬁcial preprocessed version\nfrom WMT17 website 2 . BPE with 32K merge operations is\nsimilarly applied to these datasets. W e use the concatena-\ntion of all available preprocessed validation sets in WMT17\ndatasets as our validation set:\n• En ↔ De. W e use the concatenation of newstest2014, new-\nstest2015 and newstest2016 as the validation set.\n• En ↔ Fi. W e use the concatenation of newstest2015, news-\ndev2015, newstest2016 and newstestB2016 as the valida-\ntion set.\n• En ↔ Lv. W e use newsdev2016 as the validation set.\n• En ↔ Ru. W e use the concatenation of newstest2014, new-\nstest2015 and newstest2016 as the validation set.\n• En ↔ Cs. W e use the concatenation of newstest2014, new-\nstest2015 and newstest2016 as the validation set.\nW e use newstest2017 as the test set for all WMT17 datasets.\nDetailed statistics of these datasets are shown in T able 1. F or\nall 14 translation tasks, we report case-sensitive tokeniz ed\nBLEU scores 3.\n1 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl\n2 http://data.statmt.org/wmt17/translation-task/preprocessed/\n3 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nSystem T est ∆ BLEU V alid Speed ∆ Speed\nEn-De\nBaseline 27.32 - 26.56 104.27 -\nBalanced 27.46 0.00 26.81 219.53 0.00%\nSA N 26.91 -0.55 26.04 229.89 +4.72%\nAA N 27.36 -0.10 26.11 233.58 +6.40%\nCA N 27.32 -0.14 26.47 290.08 +32.14%\nDe-En\nBaseline 30.50 - 30.34 103.97 -\nBalanced 30.76 0.00 30.37 206.00 0.00%\nSA N 30.09 -0.67 30.11 240.52 +16.76%\nAA N 30.15 -0.61 30.07 232.08 +12.66%\nCA N 30.37 -0.39 30.17 293.16 +42.31%\nEn-Fr\nBaseline 40.82 - 46.80 104.65 -\nBalanced 40.55 0.00 46.87 206.54 0.00%\nSA N 40.45 -0.10 46.69 208.68 +1.04%\nAA N 40.50 -0.05 46.57 210.29 +1.82%\nCA N 40.25 -0.30 46.56 263.83 +27.74%\nFr-En\nBaseline 36.33 - 47.03 105.85 -\nBalanced 36.86 0.00 46.89 201.13 0.00%\nSA N 36.73 -0.13 46.82 213.30 +6.05%\nAA N 36.52 -0.34 46.74 215.97 +7.38%\nCA N 36.67 -0.19 46.63 266.63 +32.57%\nT able 2: Comparison of BLEU scores [%] and translation\nspeeds (token/sec) of different attention models on WMT14\nEn↔{ De, Fr } translation tasks.\nModel Setup Our baseline system is based on the open-\nsource implementation of the Transformer model presented\nin Ott et al. (2019). For all machine translation tasks, the\nstandard Transformer baseline (Baseline) consists of a 6-\nlayer encoder and a 6-layer decoder. The embedding size\nis set to 512. The number of attention heads is 8. The FFN\nhidden size equals to 4 × embedding size. Dropout with the\nvalue of 0.1 is used for regularization. W e adopt the inverse\nsquare root learning rate schedule with 8,000 warmup steps\nand 0.0007 learning rate. W e stop training until the model\nstops improving on the validation set. All systems are train ed\non 8 NVIDIA TITIAN V GPUs with mixed-precision train-\ning (Micikevicius et al. 2018) and a batch size of 4,096 to-\nkens per GPU. W e average model parameters in the last 5\nepochs for better performance. At test time, the model is de-\ncoded with a beam of width 4 and half-precision. For an ac-\ncurate speed comparison, we decode with a batch size of 1 to\navoid paddings. The stronger balanced baseline (Balanced)\nshares the setting with this standard baseline, except that its\nencoder depth is 12 and decoder depth is 2.\nW e compare C A N and other model acceleration ap-\nproaches with our baselines. W e choose Sharing Atten-\ntion Network (S A N) (Xiao et al. 2019) and A verage Atten-\ntion Network (A A N) (Zhang, Xiong, and Su 2018) for com-\nparison, as they have been proven to be effective in vari-\nous machine translation tasks (Birch et al. 2018). All hyper -\nparameters of C A N, S A N and A A N are identical to the bal-\nanced baseline system. Results are the average of 3 runs.\nResults\nT able 2 shows the results of various systems on WMT14\nEn↔{ De, Fr }. Our balanced baseline has nearly the same\nSystem T est ∆ BLEU V alid Speed ∆ Speed\nEn-De\nBaseline 28.40 - 31.30 106.58 -\nBalanced 28.65 0.00 31.39 218.35 0.00%\nCA N 28.30 -0.35 30.94 280.57 +28.50%\nDe-En\nBaseline 34.48 - 35.36 103.04 -\nBalanced 34.38 0.00 35.16 220.05 0.00%\nCA N 33.99 -0.39 34.82 286.23 +30.07%\nEn-Fi\nBaseline 21.28 - 18.31 103.84 -\nBalanced 21.38 0.00 18.67 207.73 0.00%\nCA N 21.14 -0.24 18.19 286.36 +37.85%\nFi-En\nBaseline 25.54 - 21.32 106.59 -\nBalanced 25.63 0.00 21.29 209.88 0.00%\nCA N 25.25 -0.38 21.31 287.57 +37.02%\nEn-Lv\nBaseline 16.14 - 21.33 107.20 -\nBalanced 15.98 0.00 21.21 219.02 0.00%\nCA N 15.90 -0.08 20.75 287.33 +31.19%\nLv-En\nBaseline 18.74 - 24.79 106.25 -\nBalanced 18.69 0.00 24.54 216.06 0.00%\nCA N 18.21 -0.48 24.16 275.89 +27.69%\nEn-Ru\nBaseline 30.44 - 30.67 106.46 -\nBalanced 30.28 0.00 30.59 214.52 0.00%\nCA N 29.89 -0.39 30.28 287.13 +33.85%\nRu-En\nBaseline 34.44 - 32.39 107.24 -\nBalanced 34.24 0.00 32.22 213.78 0.00%\nCA N 33.95 -0.29 31.92 287.86 +34.65%\nEn-Cs\nBaseline 24.00 - 28.09 106.18 -\nBalanced 23.69 0.00 28.03 212.65 0.00%\nCA N 23.59 -0.10 27.71 272.37 +28.08%\nCs-En\nBaseline 30.00 - 33.01 104.00 -\nBalanced 30.06 0.00 32.86 202.96 0.00%\nCA N 29.87 -0.19 32.99 269.70 +32.88%\nT able 3: BLEU scores [%] and translation speeds (token/sec)\non WMT17 En ↔{ De, Fi, Lv, Ru, Cs } translation tasks.\nperformance as the standard baseline, but its speed is 2 ×\nfaster on average. A similar phenomenon is also observed\nfrom WMT17 experiments in T able 3. This observation in-\ndicates that existing systems do not well balance the encode r\nand decoder depths. W e also report the performance of A A N,\nSA N and the proposed C A N. All three approaches have sim-\nilar BLEU scores and slightly underperform the balanced\nbaseline. C A N is more stable than the others, as its maxi-\nmum ∆ BLEU is -0.39, while S A N is -0.67 and A A N is -0.61.\nFor speeds of these systems, S A N and A A N have a similar\nlevel of acceleration (1 ∼ 16%) over the balanced baseline.\nCA N, on the other hand, provides a higher level of accel-\neration (27 ∼ 42%). Interestingly, we ﬁnd that the accelera-\ntion is more obvious in De-En than in others, e.g., 42% in\nDe-En and 27% in En-Fr for C A N. W e ﬁnd that the length\nratio between the translation and the source sentence in De-\nEn is higher than others, e.g., 1.0 for De-En and 0.981 for\nEn-Fr. In this case the decoder tends to predict more words\nand consumes more time in De-En, and thus acceleration\napproaches that work on the decoder are more effective. In\naddition, though not reported in T able 2, we ﬁnd that ap-\nplying C A N on the standard baseline hurt the performance\nSystem Before KD After KD\nT est ∆ BLEU T est ∆ BLEU\nBalanced 27.46 0.00 27.82 0.00\nSA N 26.91 -0.55 27.76 -0.06\nAA N 27.36 -0.10 27.85 +0.03\nCA N 27.32 -0.14 28.08 +0.26\nT able 4: BLEU scores [%] of applying knowledge distilla-\ntion (KD) on WMT14 En-De translation task.\nSystem T est ∆ BLEU Speed ∆ Speed\nBalanced 27.46 0.00 219.53 0.00%\n+ Compress Attention 27.09 -0.37 263.64 +20.09%\n+ Compress FFN 27.69 +0.23 233.17 +6.21%\n+ Compress All 27.32 -0.14 290.08 +32.14%\nT able 5: Ablation study on WMT14 En-De translation task\n(Compress Attention: compress the self-attention and cros s-\nattention only; Compress FFN: compress the cross-attentio n\nand FFN only; Compress All: compress the self-attention,\ncross-attention and FFN).\nless (-0.09 BLEU points on average) than on the balanced\nbaseline (-0.25 BLEU points on average).\nMore experimental results to justify the effectiveness of\nCA N are presented in T able 3. W e evaluate the balanced\nbaseline as well as C A N on ﬁve WMT17 language pairs.\nThe results again show that the balanced baseline is indeed a\nstrong baseline with BLEU scores close to the standard base-\nline and is consistently 2 × faster. C A N also shows a simi-\nlar trend that it slightly underperforms the balanced basel ine\n(< 0.5 BLEU scores) but is > 27% faster.\nAnalysis\nKnowledge Distillation\nAlthough S A N, A A N and C A N offer considerable speed\ngain over the balanced baseline, they all suffer from the\nperformance degradation as shown in T able 2 and T able\n3. The popular solution to this is knowledge distillation\n(KD). Here we choose sequence-level knowledge distilla-\ntion (Kim and Rush 2016) for better performance in ma-\nchine translation tasks. The balanced baseline is used to ge n-\nerate the pseudo data for KD.\nT able 4 shows that KD closes the performance gap be-\ntween the fast attention models (S A N, A A N and C A N) and\nthe balanced baseline. This fact suggests that all three sys -\ntems have enough capacity for a good performance, but\ntraining from scratch is not able to reach a good conver-\ngence state. It suggests that these systems might require a\nmore careful hyper-parameters tuning or a better optimiza-\ntion method.\nAblation Study\nT o investigate in which part C A N contributes the most to the\nacceleration as well as the performance loss, we only com-\npress the self-attention and cross-attention or compress t he\ncross-attention and FFN for study. T able 5 shows the results\n1 4 16 64 256\n0\n200\n400\nBeam Size\nSpeed\nBalanced\nCA N\n10 20 30 40 50+\n50\n100\n150\n200\nLength\nSpeed\nBalanced\nCA N\nCAN SAN AAN\n26.9\n27.0\n27.1\n30.3\n30.4\n30.4\nTranslation Length\nEn-De En-Fr\nFigure 5: Translation speed (token/sec) vs. beam size and tr anslation length on WMT14 En-De translation task.\nof this ablation study. W e can see that compressing the two\nattentions provides a 20.09% speed-up, while only 6.21%\nfor compressing attention and FFN. This is because FFN is\nalready highly parallelized and accelerating itself does n ot\nbring much gain. Note that the second row of T able 5 is\nexactly Zhang, Titov, and Sennrich (2019)’s work (without\nAAN). W e see that C A N is faster and performs better. On\nthe other hand, compressing attentions brings the most per-\nformance loss, which shows that the identical input assump-\ntion is strong. Fig. 2 shows that inputs of the adjacent layer s\nare not very similar in lower layers. Therefore using C A N in\nlow layers might bring a great loss. W e also ﬁnd that com-\npressing attention and FFN has an even better result. This\nmight be that we remove the redundant parameters in the\nmodel.\nSensitivity Analysis\nW e study how the speed could be affected by other factors\nin Fig. 5, e.g., the beam size and the translation length. The\nleft of Fig. 5 shows that C A N is consistently faster than the\nbalanced baseline with different beam size. As the accelera -\ntion provided by C A N is constantly proportional to the speed\nof the baseline, it becomes less obvious when the baseline is\nslow , i.e., translating with a large beam. An opposite trend\nhappens in the middle of Fig. 5 for the translation length.\nThis is because overheads such as data preparation domi-\nnate the translation time of short sentences. This way resul ts\nin a slow speed even when the translation time is short. As\nboth the baseline and C A N get faster when generating longer\ntranslations, one might suspect that the superior accelera tion\nof C A N over other approaches comes from the fact that C A N\ngenerates longer translations. Further analysis is conduc ted\nand shown in the right of Fig. 5. W e see that C A N, S A N\nand A A N generate translations with similar lengths in two\nWMT14 translation tasks. This observation justiﬁes that th e\nsuperior acceleration brought by C A N did come from its de-\nsign rather than translation lengths.\nError Analysis\nAs shown in T able 2 and T able 3, the acceleration of C A N\ncomes at the cost of performance. Here we conduct experi-\nments to better understand in which aspect C A N scariﬁes for\nspeed-up. W e ﬁrst evaluate the sentence-level BLEU score\nfor each translation, then cluster these translations acco rd-\ning to their averaged word frequencies or lengths.\n6 12 18 24 30+\n35\n40\n45\nFrequency ( ×105)\nBLEU [%]\nBalanced\nCA N\n10 20 30 40 50+\n32\n34\n36\n38\n40\nLength\nBLEU [%]\nBalanced\nCA N\nFigure 6: BLEU score [%] vs. word frequency ( × 105) and\ntranslation length on WMT14 En-De translation task.\nFig. 6 shows the results. The left of Fig. 6 indicates that\nCA N did well on sentences with low frequencies, but not\non those with high frequencies. The right of Fig. 6 shows\nthat C A N does not translate short sentences well but is quite\ngood at translating long sentences. These facts are counter -\nintuitive as one might expect a poor model could do well\non easy samples (high frequency and short sentences) but\nnot on hard ones (low frequency and long sentences). This\nmight due to the identical input assumptions we used to de-\nrive C A N are critical to easy samples. W e left this for the\nfuture exploration.\nParallelism Study\nA simple approach to obtain a higher parallelism without\nmodifying the architecture is to increase the batch size at i n-\nference. Fig. 7 compares the inference time of the balanced\nbaseline and C A N by varying the batch size. W e can see that\nboth systems run faster with a larger batch size and C A N is\nconsistently faster than the balanced baseline. But the acc el-\neration of C A N over the baseline ∆ Speed diminishes when\nthe batch size gets larger. In this case we observe that C A N\nreaches the highest parallelism (a nearly 100% GPU utility)\nin a smaller batch size ( ≥ 32) than the baseline ( > 64). This\nmeans that enlarging the batch size no longer provides ac-\nceleration for C A N, while the baseline can still be further\nspeeded up. W e expect C A N could be faster if more tensor\ncores are available in the future.\n0 20 40 60\n0\n2,000\n4,000\n6,000\n8,000\n10,000\nBatch Size\nSpeed\n0\n10\n20\n30\n40\n50\n∆ Speed [%]\nBalanced CA N\n∆ Speed\nFigure 7: Speed (token/sec) and ∆ Speed [%] vs. batch size\non WMT14 En-De translation task.\nT raining Study\nW e plot the training and validation loss curve of the stan-\ndard baseline, the balanced baseline and C A N in Fig. 8 for\nstudying their convergence. W e can see that all systems con-\nverge stably. The balanced baseline has a higher loss than th e\nstandard baseline in both the training and validation sets, but\ntheir BLEU scores are close as shown in T able 2. This is due\nto the shallow decoder in the balanced baseline. Since the\nloss is determined by the decoder, a shallow decoder with\nless capacity would have a higher loss. W ang et al. (2019)\nindicates that the encoder depth has a greater impact than th e\ndecoder on BLEU scores, therefore the deep encoder makes\nup the performance loss of the shallow decoder. W e also see\nthat C A N has a higher loss than the balanced baseline be-\ncause we compress the decoder. Since we do not enhance\nthe encoder, the BLEU score drops accordingly.\nRelated W ork\nModel Acceleration\nLarge Transformer has demonstrated its effectiveness on\nvarious natural language processing tasks, including ma-\nchine translation (V aswani et al. 2017), language modellin g\n(Baevski and Auli 2019) and etc. The by-product brought by\nthis huge network is the slow inference speed. Previous work\nfocuses on improving model efﬁciency from different per-\nspectives. For example, knowledge distillation approache s\ntreat the large network output as the ground truth to train\na small network (Kim and Rush 2016). Low-bit quantiza-\ntion approaches represent and run the model with 8-bit in-\nteger (Lin et al. 2020). Our work follows another line of re-\nsearches, which purses a more efﬁcient architecture.\nChen et al. (2018) show that the attention of Transformer\nbeneﬁts the encoder the most and the decoder could be\nsafely replaced by a recurrent network. This way reduces\nthe complexity of the decoder to linear time but incurs a\nhigh cost in training. Zhang, Xiong, and Su (2018) show\nthat the self-attention is not necessary and a simple aver-\naging is enough. Xiao et al. (2019) indicate that most at-\ntention distributions are redundant and thus share these\ndistributions among layers. Kitaev, Kaiser, and Levskaya\n(2020) use locality-sensitive hashing to select a con-\nstant number of words and perform attention on them.\n0 3 6 9 12 15 18 21\n4\n6\n8\n#Epoch\nLoss\nBaseline\nBalanced\nCA N\nFigure 8: Loss vs. # of epochs on WMT14 En-De translation\ntask (solid lines are the training losses, dashed lines are t he\nvalidation losses).\nFan, Grave, and Joulin (2020) train a large Transformer and\ndrop some layers at testing for fast inference. Gu et al.\n(2018) use a non-autoregressive decoder to predict the\nwhole sentence at one time instead of generating it word\nby word. This approach makes a linear time transla-\ntion process to constant time via the parallel computa-\ntion. Perhaps the most related works are He et al. (2018);\nZhang, Titov, and Sennrich (2019). They merge the self-\nattention and cross-attention and share their parameters. W e,\non the other hand, use different sets of parameters for each\nattention and mathematically prove that this way is equiv-\nalent to the standard Transformer under some mild condi-\ntions. W e further show that the attention and FFN could also\nbe merged together due to their linearity.\nDeep T ransformer\nRecent studies have shown that deepening the Transformer\nencoder is more beneﬁcial than widening the encoder\nor deepening the decoder (Bapna et al. 2018). W ang et al.\n(2019) show that placing the layer normalization before\n(Pre-Norm) rather than behind (Post-Norm) the sub-layer al -\nlows us to train deep Transformer. Xiong et al. (2020) prove\nthat the success of the Pre-Norm network relies on its well-\nbehaved gradient. Zhang, Titov, and Sennrich (2019) sug-\ngest that a proper initialization is enough to train a deep Po st-\nNorm network. Kasai et al. (2020) similarly exploit this ob-\nservation but to build a faster instead of a better model. The y\nshow that using knowledge distillation, a deep encoder and\nshallow decoder model could run much faster without losing\nany performance. Based on their work, we use this model as\nour baseline system and evaluate it on extensive machine\ntranslation tasks without knowledge distillation.\nConclusion\nIn this work, we propose C A N, whose decoder layer consists\nof only one attention. C A N offers consistent acceleration by\nproviding a high degree of parallelism. Experiments on 14\nWMT machine translation tasks show that C A N is 2.82 ×\nfaster than the baseline. W e also use a stronger baseline for\ncomparison. It employs a deep encoder and a shallow de-\ncoder, and is 2 × faster than the standard Transformer base-\nline without loss in performance.\nAcknowledgments\nThis work was supported in part by the National Sci-\nence Foundation of China (Nos. 61876035 and 61732005),\nand the National Key R&D Program of China (No.\n2019QY1801). The authors would like to thank anonymous\nreviewers for their comments.\nReferences\nBa, L. J.; Kiros, J. R.; and Hinton, G. E. 2016.\nLayer Normalization. CoRR abs/1607.06450. URL\nhttp://arxiv.org/abs/1607.06450 .\nBaevski, A.; and Auli, M. 2019. Adaptive Input Represen-\ntations for Neural Language Modeling. In 7th International\nConference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019 . OpenReview .net. URL\nhttps://openreview .net/forum?id=ByxZX20qFQ .\nBapna, A.; Chen, M. X.; Firat, O.; Cao, Y .; and Wu,\nY . 2018. Training Deeper Neural Machine Translation\nModels with Transparent Attention. In Riloff, E.; Chi-\nang, D.; Hockenmaier, J.; and Tsujii, J., eds., Proceedings\nof the 2018 Conference on Empirical Methods in Natu-\nral Language Processing, Brussels, Belgium, October 31\n- November 4, 2018 , 3028–3033. Association for Com-\nputational Linguistics. doi:10.18653/v1/d18-1338. URL\nhttps://doi.org/10.18653/v1/d18-1338 .\nBirch, A.; Finch, A. M.; Luong, M.; Neubig, G.; and\nOda, Y . 2018. Findings of the Second W orkshop on\nNeural Machine Translation and Generation. In Birch,\nA.; Finch, A. M.; Luong, M.; Neubig, G.; and Oda, Y .,\neds., Proceedings of the 2nd W orkshop on Neural Machine\nT ranslation and Generation, NMT@ACL 2018, Melbourne,\nAustralia, July 20, 2018 , 1–10. Association for Compu-\ntational Linguistics. doi:10.18653/v1/w18-2701. URL\nhttps://doi.org/10.18653/v1/w18-2701 .\nChen, M. X.; Firat, O.; Bapna, A.; Johnson, M.; Macherey,\nW .; Foster, G. F .; Jones, L.; Schuster, M.; Shazeer, N.; Par-\nmar, N.; V aswani, A.; Uszkoreit, J.; Kaiser, L.; Chen, Z.;\nWu, Y .; and Hughes, M. 2018. The Best of Both W orlds:\nCombining Recent Advances in Neural Machine Transla-\ntion. In Gurevych, I.; and Miyao, Y ., eds., Proceedings\nof the 56th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2018, Melbourne, Australia, Jul y\n15-20, 2018, V olume 1: Long P apers , 76–86. Association\nfor Computational Linguistics. doi:10.18653/v1/P18-100 8.\nURL https://www .aclweb.org/anthology/P18-1008/ .\nFan, A.; Grave, E.; and Joulin, A. 2020. Re-\nducing Transformer Depth on Demand with Struc-\ntured Dropout. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview .net. URL\nhttps://openreview .net/forum?id=SylO2yStDr .\nGlorot, X.; and Bengio, Y . 2010. Understanding the\ndifﬁculty of training deep feedforward neural networks.\nIn T eh, Y . W .; and Titterington, D. M., eds., Proceed-\nings of the Thirteenth International Conference on Ar-\ntiﬁcial Intelligence and Statistics, AISTATS 2010, Chia\nLaguna Resort, Sardinia, Italy, May 13-15, 2010 , vol-\nume 9 of JMLR Proceedings , 249–256. JMLR.org. URL\nhttp://proceedings.mlr.press/v9/glorot10a.html .\nGu, J.; Bradbury, J.; Xiong, C.; Li, V . O. K.; and Socher,\nR. 2018. Non-Autoregressive Neural Machine Translation.\nIn 6th International Conference on Learning Representa-\ntions, ICLR 2018, V ancouver , BC, Canada, April 30 - May\n3, 2018, Conference T rack Proceedings . OpenReview .net.\nURL https://openreview .net/forum?id=B1l8BtlCb .\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016a. Deep Resid-\nual Learning for Image Recognition. In 2016 IEEE Confer-\nence on Computer V ision and P attern Recognition, CVPR\n2016, Las V egas, NV , USA, June 27-30, 2016 , 770–778.\nIEEE Computer Society. doi:10.1109/CVPR.2016.90. URL\nhttps://doi.org/10.1109/CVPR.2016.90 .\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016b. Iden-\ntity Mappings in Deep Residual Networks. In Leibe, B.;\nMatas, J.; Sebe, N.; and W elling, M., eds., Computer V ision\n- ECCV 2016 - 14th European Conference, Amsterdam, The\nNetherlands, October 11-14, 2016, Proceedings, P art IV ,\nvolume 9908 of Lecture Notes in Computer Science , 630–\n645. Springer. doi:10.1007/978-3-319-46493-0 \\\n38. URL\nhttps://doi.org/10.1007/978-3-319-46493-0 38.\nHe, T .; T an, X.; Xia, Y .; He, D.; Qin, T .; Chen, Z.; and Liu,\nT . 2018. Layer-Wise Coordination between Encoder and\nDecoder for Neural Machine Translation. In Bengio, S.;\nW allach, H. M.; Larochelle, H.; Grauman, K.; Cesa-Bianchi,\nN.; and Garnett, R., eds., Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neural\nInformation Processing Systems 2018, NeurIPS 2018,\n3-8 December 2018, Montr ´eal, Canada , 7955–7965. URL\nhttp://papers.nips.cc/paper/8019-layer-wise-coordin ation-between-encoder-and-decoder-for-neural-machin e-translation .\nHinton, G. E.; V inyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network. CoRR abs/1503.02531.\nURL http://arxiv.org/abs/1503.02531 .\nHuang, G.; Sun, Y .; Liu, Z.; Sedra, D.; and W einberger,\nK. Q. 2016. Deep Networks with Stochastic Depth. In Leibe,\nB.; Matas, J.; Sebe, N.; and W elling, M., eds., Computer\nV ision - ECCV 2016 - 14th European Conference, Amster-\ndam, The Netherlands, October 11-14, 2016, Proceedings,\nP art IV, volume 9908 of Lecture Notes in Computer Science ,\n646–661. Springer. doi:10.1007/978-3-319-46493-0 \\\n39.\nURL https://doi.org/10.1007/978-3-319-46493-0 39.\nKasai, J.; Pappas, N.; Peng, H.; Cross, J.; and Smith, N. A.\n2020. Deep Encoder, Shallow Decoder: Reevaluating the\nSpeed-Quality Tradeoff in Machine Translation. CoRR\nabs/2006.10369. URL https://arxiv.org/abs/2006.10369 .\nKim, Y .; and Rush, A. M. 2016. Sequence-Level Knowledge\nDistillation. In Su, J.; Carreras, X.; and Duh, K., eds., Pro-\nceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin, T exas,\nUSA, November 1-4, 2016 , 1317–1327. The Association\nfor Computational Linguistics. doi:10.18653/v1/d16-113 9.\nURL https://doi.org/10.18653/v1/d16-1139 .\nKitaev, N.; Kaiser, L.; and Levskaya, A. 2020. Reformer:\nThe Efﬁcient Transformer. In 8th International Con-\nference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020 . OpenReview .net. URL\nhttps://openreview .net/forum?id=rkgNKkHtvB .\nLi, B.; W ang, Z.; Liu, H.; Jiang, Y .; Du, Q.; Xiao, T .;\nW ang, H.; and Zhu, J. 2020a. Shallow-to-Deep Training\nfor Neural Machine Translation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2020, Online, November 16-\n20, 2020 , 995–1005. Association for Computational Lin-\nguistics. doi:10.18653/v1/2020.emnlp-main.72. URL\nhttps://doi.org/10.18653/v1/2020.emnlp-main.72 .\nLi, Y .; W ang, Q.; Xiao, T .; Liu, T .; and Zhu, J. 2020b.\nNeural Machine Translation with Joint Representation. In\nThe Thirty-F ourth AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Appli-\ncations of Artiﬁcial Intelligence Conference, IAAI 2020,\nThe T enth AAAI Symposium on Educational Advances in\nArtiﬁcial Intelligence, EAAI 2020, New Y ork, NY , USA,\nF ebruary 7-12, 2020 , 8285–8292. AAAI Press. URL\nhttps://aaai.org/ojs/index.php/AAAI/article/view/6344 .\nLin, Y .; Li, Y .; Liu, T .; Xiao, T .; Liu, T .; and Zhu, J. 2020.\nT owards Fully 8-bit Integer Inference for the Transformer\nModel. In Bessiere, C., ed., Proceedings of the T wenty-Ninth\nInternational Joint Conference on Artiﬁcial Intelligence , IJ-\nCAI 2020 , 3759–3765. ijcai.org. doi:10.24963/ijcai.2020/\n520. URL https://doi.org/10.24963/ijcai.2020/520 .\nMicikevicius, P .; Narang, S.; Alben, J.; Diamos, G. F .; Else n,\nE.; Garc´ ıa, D.; Ginsburg, B.; Houston, M.; Kuchaiev, O.;\nV enkatesh, G.; and Wu, H. 2018. Mixed Precision Train-\ning. In 6th International Conference on Learning Represen-\ntations, ICLR 2018, V ancouver , BC, Canada, April 30 - May\n3, 2018, Conference T rack Proceedings . OpenReview .net.\nURL https://openreview .net/forum?id=r1gs9JgRZ .\nOtt, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.;\nGrangier, D.; and Auli, M. 2019. fairseq: A Fast, Extensi-\nble T oolkit for Sequence Modeling. In Ammar, W .; Louis,\nA.; and Mostafazadeh, N., eds., Proceedings of the 2019\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language T ech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June\n2-7, 2019, Demonstrations , 48–53. Association for Com-\nputational Linguistics. doi:10.18653/v1/n19-4009. URL\nhttps://doi.org/10.18653/v1/n19-4009 .\nSennrich, R.; Haddow , B.; and Birch, A. 2016. Neural Ma-\nchine Translation of Rare W ords with Subword Units. In\nProceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2016, August 7-12,\n2016, Berlin, Germany, V olume 1: Long P apers . The Associ-\nation for Computer Linguistics. doi:10.18653/v1/p16-116 2.\nURL https://doi.org/10.18653/v1/p16-1162 .\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017.\nAttention is All you Need. In Guyon, I.; von Luxburg,\nU.; Bengio, S.; W allach, H. M.; Fergus, R.; V ishwanathan,\nS. V . N.; and Garnett, R., eds., Advances in Neural In-\nformation Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA , 5998–6008. URL\nhttp://papers.nips.cc/paper/7181-attention-is-all-y ou-need .\nW ang, Q.; Li, B.; Xiao, T .; Zhu, J.; Li, C.; W ong, D. F .;\nand Chao, L. S. 2019. Learning Deep Transformer Mod-\nels for Machine Translation. In Korhonen, A.; Traum,\nD. R.; and M ` arquez, L., eds., Proceedings of the 57th Con-\nference of the Association for Computational Linguistics,\nACL 2019, Florence, Italy, July 28- August 2, 2019, V ol-\nume 1: Long P apers , 1810–1822. Association for Com-\nputational Linguistics. doi:10.18653/v1/p19-1176. URL\nhttps://doi.org/10.18653/v1/p19-1176 .\nXiao, T .; Li, Y .; Zhu, J.; Y u, Z.; and Liu, T . 2019.\nSharing Attention W eights for Fast Transformer. In\nKraus, S., ed., Proceedings of the T wenty-Eighth Inter-\nnational Joint Conference on Artiﬁcial Intelligence, IJ-\nCAI 2019, Macao, China, August 10-16, 2019 , 5292–\n5298. ijcai.org. doi:10.24963/ijcai.2019/735. URL\nhttps://doi.org/10.24963/ijcai.2019/735 .\nXiong, R.; Y ang, Y .; He, D.; Zheng, K.; Zheng, S.; Xing,\nC.; Zhang, H.; Lan, Y .; W ang, L.; and Liu, T . 2020. On\nLayer Normalization in the Transformer Architecture. CoRR\nabs/2002.04745. URL https://arxiv.org/abs/2002.04745 .\nZhang, B.; Titov, I.; and Sennrich, R. 2019. Improv-\ning Deep Transformer with Depth-Scaled Initialization and\nMerged Attention. In Inui, K.; Jiang, J.; Ng, V .; and\nW an, X., eds., Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language\nProcessing, EMNLP-IJCNLP 2019, Hong Kong, China,\nNovember 3-7, 2019 , 898–909. Association for Compu-\ntational Linguistics. doi:10.18653/v1/D19-1083. URL\nhttps://doi.org/10.18653/v1/D19-1083 .\nZhang, B.; Xiong, D.; and Su, J. 2018. Accelerating Neu-\nral Transformer via an A verage Attention Network. In\nGurevych, I.; and Miyao, Y ., eds., Proceedings of the 56th\nAnnual Meeting of the Association for Computational Lin-\nguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018 ,\nV olume 1: Long P apers , 1789–1798. Association for Com-\nputational Linguistics. doi:10.18653/v1/P18-1166. URL\nhttps://www .aclweb.org/anthology/P18-1166/ .\nZhang, Y .; W ang, Z.; Cao, R.; W ei, B.; Shan, W .;\nZhou, S.; Reheman, A.; Zhou, T .; Zeng, X.; W ang,\nL.; Mu, Y .; Zhang, J.; Liu, X.; Zhou, X.; Li, Y .; Li,\nB.; Xiao, T .; and Zhu, J. 2020. The NiuTrans Ma-\nchine Translation Systems for WMT20. In Proceed-\nings of the Fifth Conference on Machine T ranslation,\nWMT@EMNLP 2020, Online, November 19-20, 2020 , 338–\n345. Association for Computational Linguistics. URL\nhttps://www .aclweb.org/anthology/2020.wmt-1.37/ .",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7434991598129272
    },
    {
      "name": "Transformer",
      "score": 0.7396248579025269
    },
    {
      "name": "Encoder",
      "score": 0.6398830413818359
    },
    {
      "name": "Computation",
      "score": 0.6390625238418579
    },
    {
      "name": "Decoding methods",
      "score": 0.475040465593338
    },
    {
      "name": "Inefficiency",
      "score": 0.451738178730011
    },
    {
      "name": "Parallel computing",
      "score": 0.43763267993927
    },
    {
      "name": "Computer engineering",
      "score": 0.422096312046051
    },
    {
      "name": "Algorithm",
      "score": 0.38560622930526733
    },
    {
      "name": "Voltage",
      "score": 0.1062752902507782
    },
    {
      "name": "Engineering",
      "score": 0.10596197843551636
    },
    {
      "name": "Electrical engineering",
      "score": 0.08375087380409241
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 10
}