{
  "title": "Employing large language models safely and effectively as a practicing neurosurgeon",
  "url": "https://openalex.org/W4409309006",
  "year": 2025,
  "authors": [
    {
      "id": null,
      "name": "Patil, Advait",
      "affiliations": [
        "Mass General Brigham",
        "Harvard University"
      ]
    },
    {
      "id": null,
      "name": "Serrato, Paul",
      "affiliations": [
        "Yale University",
        "Mass General Brigham"
      ]
    },
    {
      "id": null,
      "name": "Cleaver, Gracie",
      "affiliations": [
        "Mass General Brigham",
        "Harvard University"
      ]
    },
    {
      "id": null,
      "name": "Limbania, Daniela",
      "affiliations": [
        "Harvard University",
        "Mass General Brigham"
      ]
    },
    {
      "id": null,
      "name": "See, Alfred Pokmeng",
      "affiliations": [
        "Harvard University",
        "Boston Children's Hospital",
        "Boston Children's Museum"
      ]
    },
    {
      "id": null,
      "name": "Huang, Kevin T.",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Mass General Brigham",
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385827730",
    "https://openalex.org/W4380291159",
    "https://openalex.org/W4401610221",
    "https://openalex.org/W4362519903",
    "https://openalex.org/W4389137712",
    "https://openalex.org/W4400136387",
    "https://openalex.org/W4388585663",
    "https://openalex.org/W4379057984",
    "https://openalex.org/W4391260037",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4385988139",
    "https://openalex.org/W4392787378",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4393942918",
    "https://openalex.org/W4387641283",
    "https://openalex.org/W4390704928",
    "https://openalex.org/W4388656085",
    "https://openalex.org/W4402796316",
    "https://openalex.org/W4405728682",
    "https://openalex.org/W4392195265",
    "https://openalex.org/W4390811243",
    "https://openalex.org/W4404651681",
    "https://openalex.org/W4406778928",
    "https://openalex.org/W4389513630",
    "https://openalex.org/W4403362497",
    "https://openalex.org/W4365503720",
    "https://openalex.org/W4391774330",
    "https://openalex.org/W4401412884",
    "https://openalex.org/W4385377333",
    "https://openalex.org/W4404143589",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W4389792145",
    "https://openalex.org/W4385564466",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W4391426382",
    "https://openalex.org/W4391484746",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4391221150",
    "https://openalex.org/W4402348202",
    "https://openalex.org/W4401397240"
  ],
  "abstract": "LLMs present promising opportunities to advance neurosurgical practice, but their clinical adoption necessitates careful consideration of technical, ethical, and regulatory hurdles. By thoughtfully evaluating model selection, deployment approaches, and compliance requirements, neurosurgeons can leverage the benefits of LLMs while minimizing potential risks.",
  "full_text": "Vol.:(0123456789)\nActa Neurochirurgica         (2025) 167:101  \nhttps://doi.org/10.1007/s00701-025-06515-6\nMINI REVIEW\nEmploying large language models safely and effectively \nas a practicing neurosurgeon\nAdvait Patil1,3 · Paul Serrato3,4 · Gracie Cleaver1,3 · Daniela Limbania1,3 · Alfred Pokmeng See1,2 · Kevin T. Huang1,3,5\nReceived: 15 March 2025 / Accepted: 3 April 2025 \n© The Author(s) 2025\nAbstract\nBackground Large Language Models (LLMs) have demonstrated significant capabilities to date in working with a neuro-\nsurgical knowledge-base and have the potential to enhance neurosurgical practice and education. However, their role in the \nclinical workspace is still being actively explored. As many neurosurgeons seek to incorporate this technology into their local \npractice environments, we explore pertinent questions about how to deploy these systems in a safe and efficacious manner.\nMethods The authors performed a literature search of LLM studies in neurosurgery in the PubMed database (“LLM” and \n“neurosurgery”). Papers were reviewed for LLM use cases, considerations taken for selection of specific LLMs, and chal-\nlenges encountered, including processing of private health information.\nResults The authors provide a review of core principles underpinning model selection, including technical considerations \nsuch as model access, context windows, multimodality, retrieval-augmented generation, and benchmark performance, as \nwell as relative advantages of current LLMs. Additionally, the authors discuss safety considerations and paths for institu-\ntional support in safe LLM inference on private health data. The resulting discussion forms a framework for key dimensions \nneurosurgeons employing LLMs should consider.\nConclusions LLMs present promising opportunities to advance neurosurgical practice, but their clinical adoption necessitates \ncareful consideration of technical, ethical, and regulatory hurdles. By thoughtfully evaluating model selection, deployment \napproaches, and compliance requirements, neurosurgeons can leverage the benefits of LLMs while minimizing potential risks.\nKeywords Large language models · Artificial intelligence · Neurosurgery\nAbbreviations\nLLM  Large language model\nAI  Artificial intelligence\nML  Machine learning\nNLP  Natural language processing\nGPT  Generative pre-trained transformer\nLLAMA  Large language model from Meta AI\nRAG   Retrieval-augmented generation\nHER  Electronic health record\nHIPAA  Health Insurance Portability and Accountabil-\nity Act\nPHI  Protected Health Information\nIntroduction\nLarge language models (LLMs) are a form of artificial intel-\nligence that have recently garnered widespread popularity \ndue to their ability to generate fluent human language in a \nversatile and robust manner. They have demonstrated highly \naccurate capabilities to date in working with a neurosurgical \nknowledge-base and have the potential to enhance neuro-\nsurgical practice and education [12, 20, 25, 29, 32, 34, 43]. \nHowever, their role in the clinical workspace is still being \nactively explored. As many neurosurgeons seek to incorpo-\nrate this technology into their local practice environments, \n * Kevin T. Huang \n khuang@bwh.harvard.edu\n1 Department of Neurosurgery, Harvard Medical School, \nBoston, MA 02467, USA\n2 Department of Neurosurgery, Boston Children’s Hospital, \nBoston, MA 02467, USA\n3 Department of Neurosurgery, Mass General Brigham, \nBoston, MA 02467, USA\n4 Department of Neurosurgery, Yale School of Medicine, \nNew Haven, CT 06510, USA\n5 Department of Neurosurgery, Brigham and Women’s \nHospital, 60 Fenwood Road, Hale Building, 4th Floor, \nBoston, MA 02115, USA\n Acta Neurochirurgica         (2025) 167:101 \n  101  Page 2 of 9\nquestions remain on how to deploy these systems in a safe \nand efficacious manner. These are myriad, ranging from how \nto select an appropriate model for the given use case, to how \nto navigate local privacy and data safety regulations. Thus, \nwhile previous reviews have focused on LLMs in neurosur-\ngery writ-large [9 , 27, 35], to our knowledge no previous \nworks have focused on surmounting the practical barriers \ntowards deploying LLMs for the practicing neurosurgeon.\nIn this review, we seek to explore this question from a \nfew different angles. First, we will review potential LLM use \ncases, discussing use cases both with and without incorpo-\nrating patient protected health information (PHI). Second, \nwe will summarize the technical components of deploy -\nment, including model selection and LLM-specific features \nto consider. Finally, we will consider broader patient and \ndata safety topics and regulatory compliance.\nUse cases: how might LLMs be employed \nby a neurosurgeon?\nUse cases of LLMs in healthcare can be broadly divided into \ntwo categories by the nature of the data being processed. \n“Outside-the-firewall” applications do not involve the trans-\nmission of PHI, and thus are naturally compliant with local \nprivacy regulations such as the United States Health Insur -\nance Portability and Accountability Act (HIPAA). Appli-\ncation areas include literature review, medical education, \npatient education, and commonly banned uses such as for \npeer review. In contrast, “behind-the-firewall” uses like bill-\ning and coding, clinical document generation, report simpli-\nfication, and data structuring are necessarily reliant on PHI, \nand thus subject to a greater degree of scrutiny. Here, we \nreview progress and opportunities in each domain.\nUse cases not involving PHI\nOutside the firewall, LLMs have been employed successfully \nin simplifying patient-facing text. Doing so can improve \nreadability, comprehension, and retention of material. \nSeveral authors have studied this use case on a variety of \npatient-facing materials. Mirza et al. found that the Kincaid \nreadability score of consent forms decreased from that of \na college student to that of a middle school student after \nsimplification with ChatGPT, demonstrating how LLMs can \nbe used to increase patient understanding in consent forms \n[20]. Similarly to Mirza et al., Swisher et al. demonstrated \nhow LLMs can also be utilized to improve the readability \nof patient handouts and other patient-facing materials [20, \n34]. Nian et al. evaluated ChatGPT’s response to frequently \nasked questions (FAQs) about spinal fusion and lumbar \nlaminectomy. ChatGPT’s answers had a higher Flesch-Kin-\ncaid grade level score (11.6) compared to Google searches \n(8.8) with longer responses [24]. These contradictory find-\nings between studies highlight both the stochastic nature of \nLLMs and the importance of specific prompting towards \ndesired output.\nLLMs also hold significant potential in medical educa-\ntion. Not only have they demonstrated competent command \nof basic neurosurgical facts, but also have the potential to \nexplain neurosurgical concepts to new learners, as well as \ngenerate a nearly infinite amount of practice questions to aid \nin understanding. Numerous benchmarking studies, includ-\ning Gilson et al. demonstrated that LLMs like ChatGPT, can \npass the United States Medical Licensing Exam (USMLE), \nfunctioning at the level of a third-year medical student [13]. \nThe author further demonstrated how ChatGPT could help \nwith learning as a virtual medical tutor. In a similar study, \nKung et al. Found that ChatGPT can perform at a level at or \napproaching the passing score of the USMLE exams, which \ninclude Step 1, Step 2 CK, and Step 3 [16]. Notably, this per-\nformance is achieved without further fine-tuning or further \ntraining on neurosurgery-specific text. This study demon-\nstrated the improving accuracy of LLMs on USMLE exams \nand how LLMs such as ChatGPT can provide nuanced \ninsights that may be useful in medical education.\nOne important limitation is the use of LLM in research \nand publication – whether that be in peer review or manu-\nscript preparation. Currently, the US National Institutes of \nHealth (NIH) expressly prohibits the use of LLMs in funding \napplication peer review. Majovsky et al. found that there is \npotential for AI to create fraudulent nonscientific articles \nthat are highly convincing.14 As Alkaissi et al. and many \nothers highlight, the risk for hallucinations in LLM-gener -\nated text not subjected to human review is significant.15 Edi-\ntorial attitudes towards using LLMs in manuscript writing \nare currently varied; some journals forbid it, many require a \ndisclosure during submission, and some (such as NEJM AI) \nhave taken a vocal stance to encourage it. Rapid changes in \nthis technology also elicit changing judgements of the merits \nand shortcomings, and surgeons should be meticulous about \nreading individual organizational policies before employing \nLLMs in their scientific communication.\nUse cases involving PHI\nIncorporating PHI into LLMs significantly adds to their \npower and utility, but comes with additional regulatory \nscrutiny as well as privacy risks. While some organizations \nare working to adopt data-safe implementations of language \nmodels that area available for clinician use, efforts are still in \nearly phases and are constrained by limited information tech-\nnology staff and infrastructure. Nevertheless, many research-\ners have demonstrated strong use cases for incorporating \nPHI and LLM in several domains.\nActa Neurochirurgica         (2025) 167:101  \n Page 3 of 9   101 \nOne area of active developing interest involves using \nlanguage models to automate procedural billing. O’Malley \net al. assessed the accuracy of various LLMs in looking up \nCurrent Procedural Terminology (CPT) codes for cranial, \nspine, and endovascular procedures using user-generated \nprompts, with reported accuracies ranging from 20 to 75% \n[25]. In a subsequent study, Roy et al. showed the capabili-\nties of AtlasGPT and ChatGPT 4.0 in deriving CPT codes \ndirectly from operative notes in endovascular neurosurgery. \nThey reported partial accuracies of 98.3% and 86.7%, albeit \nwith complete accuracies of only 35.3% and 35.1%, respec-\ntively [28]. While current findings demonstrate the poten-\ntial streamline efficiency, LLMs will inevitably need to be \nimproved with further experience before broad deployment. \nThey have also been proposed as a means to reviewing cod-\ning to reduce human error and it is purported that health \ninsurers are already applying LLM in utilization manage-\nment [40].\nAnother application focuses on processing clinical text, \nspecifically in summarizing previously written documents \nor generating new notes. Van Veen et al. highlighted the use \nof GPT- 3.5 to synthesize key information from electronic \nhealth records into cohesive narratives, including summariz-\ning imaging findings into impressions and creating problem \nlists from provider progress notes. Their findings indicated \nthat a large percentage of these AI-generated summaries \nwere either equivalent to (45%) or surpassed (36%) those \ngenerated by medical experts [41]. Dubinski et al. inves-\ntigated the application of ChatGPT for creating discharge \nsummaries and operative reports for neurosurgical patients, \ncomparing its efficiency to standard speech recognition \nsoftware [12]. They found that the median time for drafting \ndischarge summaries for complication-free chronic subdural \nhematoma dropped from 16 min with standard software to \njust 1.85 min, achieving a median accuracy of 83% as evalu-\nated by senior physicians. In another study, Ali et al. trained \nGPT- 4 to draft detailed operative notes using surgeon-spe-\ncific templates and key parameter inputs, with independent \nevaluations by three blinded neurosurgeons on a scale of 1 \nto 5, showing no significant differences in accuracy between \nAI-generated notes (4.44) and those created by surgeons \n(4.33) [3]. These studies suggest that large language models \n(LLMs) have the potential to streamline clinical documenta-\ntion, particularly for routine and uncomplicated procedures \nin neurosurgery.\nLeveraging LLMs for mining and structuring patient \ndata from electronic health records for research purposes is \nanother area of interest. Truhn et al. demonstrated the abil-\nity of GPT- 4 to extract structured data from unstructured \npathology reports for diffuse adult-type glioma [38]. They \nfound that GPT- 4 achieved an error rate of just 1.0 ± 0.4% \nin extracting the IDH mutation status, which underscores \nits precision and efficiency in data extraction tasks. This \nallowed them to observe significant correlations between the \nKi- 67 labeling index and the presence of necrosis, as well \nas increased Ki- 67 labeling when IDH wild-type was pre-\nsent. Similarly, Wang et al. explored a biomedical language \nmodel, BRLM, to classify genomic variants and construct \nnetworks linking clinical symptoms, biomarkers, and related \ndiseases [42]. Their model successfully identified genetic \nmutations and used a cumulative perturbation measurement \nfor pathways. The model’s perturbation scores helped in \nunderstanding the impact of genetic mutations, potentially \nenhancing the precision of treatment strategies through clear \ninsights into pathogenicity mechanisms. By structuring and \nanalyzing vast amounts of patient data, these models can \nidentify patterns and trends that might otherwise remain \nobscured, thereby supporting evidence-based practice and \npersonalized medicine.\nModel selection: how do I go about selecting \na model?\nCore principles\nChoosing the appropriate LLM for a neurosurgical task \nnecessitates a comprehensive evaluation of its accuracy, \nreliability, interpretability, and compliance with regulatory \nstandards. In addition, the specific nature of the task—which \nincludes parameters such as the amount of text being pro-\ncessed and the need to update a model’s knowledge base \nregularly—are critical factors to consider. Here, we discuss \nthe two main dichotomies of model types (open source ver-\nsus closed source models) and survey technical parameters \ninfluencing model selection, performance on medical bench-\nmarks, handling of data types, advanced LLM frameworks \nsuch as Retrieval Augmented Generation (RAG), and neu-\nrosurgery-specific benchmark performance. A summary of \ncurrent leading LLMs and their strengths is given in Table 1.\nOpen source vs. closed source\nLLMs can be broadly categorized as open source or closed \nsource models, with each presenting unique advantages and \nchallenges [46]. Open source models are defined as those \nwhose weights are publicly available and can be run on \nlocal devices or servers. Prominent examples include Meta’s \nLlama series of models and a large collection on Hugging \nFace (a private company which develops machine learning \ntools and hosts libraries of models, datasets on its platform) \n[18, 44]. In contrast, closed source models like OpenAI’s \nChatGPT and Google’s Gemini do not release model param-\neters or weights to the public [37].\nThese two LLM paradigms come with unique hosting con-\nsiderations, or in other words, where a model’s computations \n Acta Neurochirurgica         (2025) 167:101 \n  101  Page 4 of 9\ndirectly occur. Local hosting provides enhanced control of \nthe data, making it a good option for applications involving \nsensitive patient data due to user-determined data security \npractices. However, only open source models can be hosted \non local computing devices. In addition, local hosting often \nincurs significant upfront capital investment to develop ade-\nquate computing infrastructure (e.g., GPUs) to fulfil user \nneeds [22].\nCloud-based solutions offer better scalability, essentially \nincurring costs for only what you use and having nearly \nno upper limit on use, and access to continuously updated \nmodel versions and incur reduced costs through payment on \na per-token basis with inference being handled by a dedi-\ncated server. However, careful assessment of regulatory \ncompliance before adoption is needed as most cloud-based \ncomputing resources do not comply with health data pri-\nvacy guidelines. Options such as Microsoft Azure OpenAI \nService can be used in a privacy compliant manner through \nBusiness Associate Agreements (BAAs) [33].\nContext window\nAn LLM’s context window refers to the amount of text that \ncan be processed by the model in a given request. Mod-\nern long-context LLMs with capacities exceeding 100,000 \ntokens can enable comprehensive analysis of full patient his-\ntories without requiring fragmented input, thereby improv -\ning contextual continuity. A model with a limited token \nwindow may fail to recognize key longitudinal data trends, \npotentially impacting clinical decision-making. However, \nwhile models with longer context windows have greater \ncapabilities, they also incur greater computational costs, \nwhich may manifest as greater resources needed or longer \nresponse times. Additionally, long-context models strug-\ngle with “needle in a haystack” problems where important \ninformation is buried in the middle of a large input sequence \n[17]. These factors present an essential tradeoff that should \nbe navigated through determination of the minimum context \nlength required to comfortably satisfy user queries.\nMultimodality\nAdvancements in multimodal AI allow models to process \nboth textual and visual data. LLMs with integrated vision \ncapabilities, such as GPT- 4-Vision, can potentially inter -\npret imaging studies in conjunction with clinical notes, \nfacilitating enhanced diagnostic support and decision-\nmaking [7, 36, 47]. This capability holds promise in areas \nlike neuroradiology, where automated AI-driven image-\ntext synthesis can accelerate workflow efficiency. How -\never, studies have shown that leading vision-language \nmodels (VLMs) disproportionately exploit text over imag-\ning input when generating their output [8 ]. Additionally, \nperformance of VLMs is often poorer on text-only tasks \nwhen compared to LLMs. Therefore, a suggested princi-\nple is to only use VLMs when interpretation of imaging \nor video data is necessary until performance equilibrates \nor improves.\nRetrieval‑augmented generation\nOne inherent limitation of LLMs is the difficulty asso-\nciated with updating the core knowledge of an LLM as \nguidelines change, which involves either completely \nretraining or finetuning the LLM on updated datasets. \nThis is also often challenging or impossible with closed-\nsource models, which cannot be retrained by users, and \nis extremely computationally (and financially) expensive \nfor open-source models. Retrieval-Augmented Generation \n(RAG) is a technique that retrieves relevant documents \nfrom a user-created database based on a user query and \nprovides that to an LLM, increasing the accuracy and reli-\nability of LLM output and grounding it in external facts \n(Fig.  1) [23]. A prominent closed-source example of a \nRAG-enabled LLM in neurosurgery is AtlasGPT, and a \npopular open-source framework to create custom RAG \nframeworks that are compatible with any LLM is Lang-\nChain [10, 15].\nTable 1  Example LLM models and their relative advantages\nModel family Developer Private or open source Advantages\nGPT- 4o OpenAI Private High zero-shot performance across tasks\no1, o3 OpenAI Private Advanced reasoning capabilities\nGemini Google Private 1-million-token context window with Gemini 1.5 Flash\nDeep research OpenAI Private Multi-step tasks and information synthesis\nClaude Anthropic Private Strong performance with writing and coding assistance\nLlama 2 Meta AI Open Source Focus on safety and factual accuracy\nDeepSeek-R1 Deep Seek Open Source Open source model rivaling large private LLM performance\nPhi Microsoft Open Source Low-cost small LLMs\nActa Neurochirurgica         (2025) 167:101  \n Page 5 of 9   101 \nPerformance on neurosurgical benchmarks\nMany studies have benchmarked the performance of LLMs \non standardized neurosurgical questions [1, 2, 6, 14, 19, 21, \n29, 31, 43]. Several studies have explored LLM performance \non medical licensing exams, particularly in neurosurgery, \nwith varied outcomes. Most investigations focused on U.S. \nlicensing tests, often using the Self-Assessment Neurosur -\ngery Examinations (SANS) from the American Board of \nNeurological Surgery. Ali et al. reported that GPT- 4 out-\nperformed both GPT- 3.5 and Bard, surpassing the aver -\nage human score, while all tested LLMs achieved a passing \nmark on the SANS question set [1]. Other research examined \nLLMs in international neurosurgical exams. Stengel et al. \nassessed multiple models on the European Board Examina-\ntion in Neurological Surgery (EANS) and found that Bard \nhad the highest accuracy at 62%, exceeding the human aver-\nage of 59% [32]. Similarly, Sahin et al. evaluated Turkish \nneurosurgical licensing exams, noting that GPT- 3.5 scored \nabove the average candidate and provided clearer responses \n[29].\nLLMs have struggled outside of conventional multiple-\nchoice assessments, however. In an adapted Turing Test, \nWilliams et al. had evaluators—who were unaware of the \nsources—assess interview transcripts from both ChatGPT \nand U.K. physicians applying for a neurosurgical National \nTraining Number [43]. Their findings revealed that Chat-\nGPT underperformed compared to six of the eight human \ncandidates, never surpassed the average score of those who \nsecured training positions, and sometimes produced inac-\ncurate responses that could be dangerous in a clinical setting.\nMonitoring and regulatory compliance: \nhow do we keep patients safe?\nPrivacy/data safety compliance\nPolicies around privacy and data safety compliance in \nthe use of AI systems within healthcare are guided by a \ncomplex regulatory framework that varies across regions. \nIn the United States, adherence to the mandates set by \nthe Food and Drug Administration (FDA) and the Health \nInsurance Portability and Accountability Act (HIPAA) \nis crucial for safeguarding patient health data privacy. \nIn December 2024, the U.S. Department of Health and \nHuman Services (HHS) suggested updates to both the \nHIPAA Privacy Rule and the HIPAA Security Rule to \nbolster cybersecurity and patient data [26]. Many of these \nupdates align with the General Data Protection Regulation \n(GDPR) in the European Union, which demands robust \nFig. 1  Example workflow for a custom HIPAA-compliant retrieval-augmented generation LLM system in the cloud\n Acta Neurochirurgica         (2025) 167:101 \n  101  Page 6 of 9\nprotections for personal data, and therefore pertain to AI \nsystems that handle patient information [5 ].\nCompliance with these guidelines should be thought \nthrough carefully, as there are several nuances to be aware \nof. For instance, there is a vigorous debate on the distinc-\ntion between\"mandatory\"and\"strongly encouraged\"—lan -\nguage that appears in numerous policies and dictates the \nlevel at which the policies should be implemented [11]. \nIn addition, the GDPR emphasizes that patients should \nbe informed about the usage of their health data in AI \napplications and should provide consent for any AI-driven \nanalysis or decision-making. Data minimization is also \nprioritized, ensuring that only the essential data is col-\nlected for AI purposes. Critically, both the GDPR and \nHIPAA mandate a de-identification strategy to facilitate \nthe safe application of AI, thus minimizing the risk of \ncompromising patient privacy (European Parliamentary \nResearch Service, 2020; U.S. Department of Health and \nHuman Services, 2025). This de-identification process \ncan be executed through either the Expert Determination \nmethod or the Safe Harbor method, involving the removal \nof specific identifiers (45 C.F.R. § 164.514(b)(2)).\nEncryption is a key aspect of regulatory compliance. \nThe GDPR regulations, particularly those outlined in Arti-\ncle 32, strongly advocate for the encryption of personal \npatient data both at rest and in transit. They encourage the \nadoption of advanced techniques like field-level, homo-\nmorphic, or tokenization to ensure secure data processing \nwithout the necessity of decrypting sensitive information \n[4]. Encrypting specific components of electronic health \nrecords can be particularly beneficial for researchers utiliz-\ning language models to access and analyze relevant patient \ninformation while ensuring the privacy of personal iden-\ntifying information. Moreover, the Fairness of Artificial \nIntelligence Recommendations statement underscores the \nimportance of incorporating fairness and transparency in \nAI healthcare applications. It supports the encryption of \ndata that may pose a risk to patient privacy and promotes \nthe implementation of best practices to guarantee equitable \nAI deployment [39].\nTo advance data safety and privacy compliance in AI \nhealthcare applications, the existing regulatory frame-\nworks must evolve to address current gaps and to antici -\npate future technological advancements. Although HIPAA \nprovides foundational privacy protections, it currently \nlacks the comprehensive scope of the GDPR, indicating a \nneed for reinforced legislation to close existing loopholes \nand ensure stronger data protection in the U.S. The chal-\nlenges are mounting due to evolving uses and potential \npolicies in this space. Additionally, a significant hurdle in \nthe U.S. is the variation in state-by-state privacy policies \nfor patient data, with many states lacking applicable laws.\nInstitutional‑specific support and institutional \npolicies\nIn our opinion, institutional-level support for LLM deploy -\nment is critical. Institutional-scale initiatives can ensure \nproper compliance, as well as provide a partner for testing, \nscaling, and even commercialization of LLM systems.\nAt one level, institutional support for basic technology \ninfrastructure can be immensely helpful in dealing with \nPHI safely. Our own institution and many of its peers host \nHIPAA-compliant LLM instances available to all affiliated \nphysicians and researchers that can interface with protected \ndata for both research and non-research purposes. These are \nsupported by in-house technical support staff, and are acces-\nsible by general program language (e.g., Python) application \nprograming interfaces (APIs). In our experience, these insti-\ntutionally sponsored instances are some of the best ways for \nneurosurgeons to transition to PHI-specific deployments, as \nmany of the regulatory and compliance issues are standard-\nized, freeing the individual surgeon to focus on the clinical \nimplementation.\nCoincident with this, ethical guidance through institu-\ntional review boards (IRBs) is necessary for any research \napplications. Not only is this a critical regulatory step for \nconducting research, but can also provide a valuable part-\nners in helping to protect vulnerable populations that may \nbe impacted by the work. Furthermore, IRBs can serve \nas a conduit to community leaders and group representa-\ntives if the research impacts populations that cannot practi-\ncally be consented for use with LLM data (e.g., in trauma \napplications).\nIncreasingly, institutions are also important partners in \nintellectual property protection and commercialization of \nideas. If successful, surgeon-led AI initiatives can be the \nbasis for valuable intellectual property, and surgeons must \nconsider how they want their idea to be protected and dis-\nseminated. Commercialization not only is a financial consid-\neration for surgeons personally but can be an important way \nto promote and disseminate key discoveries. As part of this, \nsurgeons’ employment contracts should be reviewed care-\nfully, and professional legal advice can be very helpful in \nunderstanding how any developed intellectual works relate \nto their employer. Many times, hospitals and universities will \nhave a monetary interest in promoting the surgeon’s work \nand have developed in-house “incubators” or “accelerators” \nto support commercialization, protection, and dissemination \nof surgeons’ ideas.\nHandling hallucinations and omissions\nAs of the writing of this article, current LLMs are still \nsusceptible to producing statements that are either erro-\nneous (i.e., “hallucinations”) or missing critical requested \nActa Neurochirurgica         (2025) 167:101  \n Page 7 of 9   101 \ninformation (“omissions”). These can stem from a combina-\ntion of idiosyncrasies in their original training data and the \ninherent probabilistic nature of LLMs. These pose signifi-\ncant problems for safe clinical deployment, as LLM prompt \nresponses do not always come with proper confidence mark-\ners (i.e., the problem of being “confidently wrong”) and the \npotentially onerous nature of fact-checking the produced \ntext, which may negate the utility of utilizing the LLM in \nthe first place.\nAs such, it is paramount to design systems that can effi-\nciently and effectively mitigate errors in any LLM-based \nsystem. Strategies for this fall into a few different catego -\nries. There are increasingly sophisticated technical solu-\ntions. These include anchoring responses in relevant “ground \ntruth” text, via RAG-integration, as well as producing confi-\ndence measures along with text output – allowing for more \ntargeted fact-checking [30, 45]. In addition, limiting the \nresponse length to only the information necessary to com-\nplete the task, and providing as much structure to LLM-\nfed prompts, are best practices that can limit the chance for \nerrors and make fact-checking more efficient. There even \nhave been several proposed methods whereby LLMs can \ntry to fact-check themselves, which may hold near-term \npromise.\nHowever, in clinical decision-making settings, where \npatient safety is reliant on LLM output, we strongly advo-\ncate for direct human oversight and accountability for all \nLLM-produced output. Much like the use of any other piece \nof technology in the operating room, surgeons are ultimately \nresponsible for understanding the limits of their tools and for \nmaximizing patient safety. Designing ways for clinicians to \nefficiently monitor LLM output is key to operational success.\nConclusions\nLLMs offer exciting opportunities to enhance neurosurgi-\ncal practice, but their clinical integration requires careful \nnavigation of technical, ethical, and regulatory challenges. \nBy systematically addressing model selection, deployment \nstrategies, and compliance considerations, neurosurgeons \ncan harness the potential of LLMs while mitigating associ-\nated risks. As the field continues to evolve, ongoing research \nand institutional collaboration will be essential to ensure the \nsafe and effective use of this transformative technology in \nneurosurgical care.\nAuthor contribution Study conception: A.P., A.P.S., K.H. Develop-\nment of methodology: A.P., A.P.S., K.H. Data acquisition, interpreta-\ntion, and analysis: all authors. Manuscript writing: all authors. Study \noversight: A.P., A.P.S., K.H. All authors reviewed the manuscript.\nFunding No funding was received for this research.\nData availability No datasets were generated or analysed during the \ncurrent study.\nDeclarations \nResearch involving human participants and/or animals This article \ndoes not contain any studies with animals or human participants per -\nformed by any of the authors.\nInformed consent N/A.\nCompeting Interests The authors declare no competing interests.\nOpen Access This article is licensed under a Creative Commons \nAttribution-NonCommercial-NoDerivatives 4.0 International License, \nwhich permits any non-commercial use, sharing, distribution and repro-\nduction in any medium or format, as long as you give appropriate credit \nto the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if you modified the licensed material. \nYou do not have permission under this licence to share adapted material \nderived from this article or parts of it. The images or other third party \nmaterial in this article are included in the article’s Creative Commons \nlicence, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the article’s Creative Commons licence and \nyour intended use is not permitted by statutory regulation or exceeds \nthe permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http://crea-\ntivecommons.org/licenses/by-nc-nd/4.0/.\nReferences\n 1. Ali R, Tang OY, Connolly ID et al (2023) Performance of Chat-\nGPT and GPT-4 on neurosurgery written board examinations. \nNeurosurgery 93(6):1353–1365. https:// doi. org/ 10. 1227/ neu. \n00000 00000 002632\n 2. Ali R, Tang OY, Connolly ID et al (2023) Performance of Chat-\nGPT, GPT-4, and google bard on a neurosurgery oral boards prep-\naration question bank. Neurosurgery 93(5):1090–1098. https:// doi. \norg/ 10. 1227/ neu. 00000 00000 002551\n 3. Ali A, Kumar RP, Polavarapu H et al (2024) Bridging the gap: can \nlarge language models match human expertise in writing neuro-\nsurgical operative notes? World Neurosurg 192:e34–e41. https:// \ndoi. org/ 10. 1016/j. wneu. 2024. 08. 062\n 4. Almalawi A, Khan AI, Alsolami F, Abushark YB, Alfakeeh AS \n(2023) Managing security of healthcare data for a modern health-\ncare system. Sensors 23(7):3612. https:// doi. org/ 10. 3390/ s2307 \n3612\n 5. Artificial Intelligence Act: MEPs adopt landmark law | News | \nEuropean Parliament. March 13, 2024. https:// www. europ arl.  \neuropa. eu/ news/ en/ press- room/ 20240 308IP R19015/ artifi  cial- intel \nligen ce- act- meps- adopt- landm ark- law. Accessed 9 Mar 2025\n 6. Bartoli A, May AT, Al-Awadhi A, Schaller K (2023) Probing \nartificial intelligence in neurosurgical training: ChatGPT takes \na neurosurgical residents written exam. Brain Spine 4:102715. \nhttps:// doi. org/ 10. 1016/j. bas. 2023. 102715\n 7. Blankemeier L, Cohen JP, Kumar A et al Merlin: a vision lan-\nguage foundation model for 3D computed tomography. Res Sq. \nPublished online June 28, 2024:rs.3.rs-4546309. https:// doi. org/ \n10. 21203/ rs.3. rs- 45463 09/ v1\n 8. Buckley T, Diao JA, Rajpurkar P, Rodman A, Manrai AK Mul-\ntimodal foundation models exploit text to make medical image \npredictions. Published online November 25, 2024. https:// doi. org/ \n10. 48550/ arXiv. 2311. 05591\n Acta Neurochirurgica         (2025) 167:101 \n  101  Page 8 of 9\n 9. Bydon M, Shin JH, Timmons SD, Potts EA, Chan AK (2023) \nIntroduction. Machine learning in neurosurgery: transition-\ning to a new era of contemporary medicine. Neurosurg Focus \n54(6):E1. https:// doi. org/ 10. 3171/ 2023.3. FOCUS 23210\n 10. Chase H LangChain. Published online October 2022. https://  \ngithub. com/ langc hain- ai/ langc hain. Accessed 9 Mar 2025\n 11. Derisking AI: risk management in AI development | McKinsey. \nhttps:// www. mckin sey. com/ capab iliti es/ quant umbla ck/ our- insig \nhts/ deris king- ai- by- design- how- to- build- risk- manag ement- into- \nai- devel opment. Accessed 9 Mar 2025\n 12. Dubinski D, Won SY, Trnovec S et al (2024) Leveraging arti-\nficial intelligence in neurosurgery—unveiling ChatGPT for \nneurosurgical discharge summaries and operative reports. \nActa Neurochir (Wien) 166(1):38. https:// doi. org/ 10. 1007/  \ns00701- 024- 05908-3\n 13. Gilson A, Safranek CW, Huang T et al (2023) How does Chat-\nGPT perform on the United States Medical Licensing Examina-\ntion (USMLE)? The implications of large language models for \nmedical education and knowledge assessment. JMIR Med Educ \n9:e45312. https:// doi. org/ 10. 2196/ 45312\n 14. Guerra GA, Hofmann H, Sobhani S et al (2023) GPT-4 artificial \nintelligence model outperforms ChatGPT, medical students, and \nneurosurgery residents on neurosurgery written board-like ques-\ntions. World Neurosurg 179:e160–e165. https:// doi. org/ 10. 1016/j. \nwneu. 2023. 08. 042\n 15. Hopkins BS, Carter B, Lord J, Rutka JT, Cohen-Gadol AA Edito-\nrial. AtlasGPT: dawn of a new era in neurosurgery for intelligent \ncare augmentation, operative planning, and performance. Pub-\nlished online February 27, 2024. https:// doi. org/ 10. 3171/ 2024.2. \nJNS23 2997\n 16. Kung TH, Cheatham M, Medenilla A et al (2023) Performance of \nChatGPT on USMLE: potential for AI-assisted medical education \nusing large language models. PLOS Digit Health 2(2):e0000198. \nhttps:// doi. org/ 10. 1371/ journ al. pdig. 00001 98\n 17. Li T, Zhang G, Do QD, Yue X, Chen W Long-context LLMs \nstruggle with long in-context learning. Published online June 12, \n2024. https:// doi. org/ 10. 48550/ arXiv. 2404. 02060\n 18. Llama. Meta Llama. https:// www. llama. com/. Accessed 9 Mar \n2025\n 19. Mannam SS, Subtirelu R, Chauhan D et al (2023) Large language \nmodel-based neurosurgical evaluation matrix: a novel scoring cri-\nteria to assess the efficacy of ChatGPT as an educational tool \nfor neurosurgery board preparation. World Neurosurg 180:e765–\ne773. https:// doi. org/ 10. 1016/j. wneu. 2023. 10. 043\n 20. Mirza FN, Tang OY, Connolly ID et al (2024) Using Chat-\nGPT to facilitate truly informed medical consent. NEJM AI. \n1(2):AIcs2300145. https:// doi. org/ 10. 1056/ AIcs2 300145\n 21. Murphy Lonergan R, Curry J, Dhas K, Simmons BI Stratified \nevaluation of GPT’s question answering in surgery reveals Arti-\nficial Intelligence (AI) knowledge gaps. Cureus 15(11):e48788. \nhttps:// doi. org/ 10. 7759/ cureus. 48788\n 22. Nagarajan R, Kondo M, Salas F et al (2024) Economics and equity \nof large language models: health care perspective. J Med Internet \nRes 26:e64226. https:// doi. org/ 10. 2196/ 64226\n 23. Ng KKY, Matsuba I, Zhang PC (2025) RAG in health care: a novel \nframework for improving communication and decision-making by \naddressing LLM limitations. NEJM AI 2(1):AIra2400380. https:// \ndoi. org/ 10. 1056/ AIra2 400380\n 24. Nian PP, Saleet J, Magruder M et al (2024) ChatGPT as a source \nof patient information for lumbar spinal fusion and laminectomy: \na comparative analysis against google web search. Clin Spine Surg \n37(10):E394–E403. https:// doi. org/ 10. 1097/ BSD. 00000 00000 \n001582\n 25. O’Malley GR, Sarwar SA, Cassimatis ND et al (2024) Can pub-\nlicly available artificial intelligence successfully identify cur -\nrent procedural terminology codes for common procedures in \nneurosurgery? World Neurosurg 183:e860–e870. https:// doi. org/ \n10. 1016/j. wneu. 2024. 01. 043\n 26. Office for Civil Rights. U.S. Department of Health and Human \nServices. “HIPAA Security Rule.” HIPAA Security Rule NPRM. \nDecember 27, 2024. https:// www. hhs. gov/ hipaa/ for- profe ssion als/ \nsecur ity/ hipaa- secur ity- rule- nprm/ index. html. Accessed 9 Feb \n2025\n 27. Patil A, Serrato P, Chisvo N, Arnaout O, See PA, Huang KT \n(2024) Large language models in neurosurgery: a systematic \nreview and meta-analysis. Acta Neurochir (Wien) 166(1):475. \nhttps:// doi. org/ 10. 1007/ s00701- 024- 06372-9\n 28. Roy JM, Self DM, Isch E et al (2025) Evaluating large language \nmodels for automated CPT code prediction in endovascular \nneurosurgery. J Med Syst 49(1):15. https:// doi. org/ 10. 1007/  \ns10916- 025- 02149-4\n 29. Sahin MC, Sozer A, Kuzucu P et al (2024) Beyond human in neu-\nrosurgical exams: ChatGPT’s success in the Turkish neurosurgical \nsociety proficiency board exams. Comput Biol Med 169:107807. \nhttps:// doi. org/ 10. 1016/j. compb iomed. 2023. 107807\n 30. Savage T, Wang J, Gallo R et al (2025) Large language model \nuncertainty proxies: discrimination and calibration for medi-\ncal diagnosis and treatment. J Am Med Inform Assoc JAMIA \n32(1):139–149. https:// doi. org/ 10. 1093/ jamia/ ocae2 54\n 31. Sevgi UT, Erol G, Doğruel Y, Sönmez OF, Tubbs RS, Güngor \nA (2023) The role of an open artificial intelligence platform in \nmodern neurosurgical education: a preliminary study. Neurosurg \nRev 46(1):86. https:// doi. org/ 10. 1007/ s10143- 023- 01998-2\n 32. Stengel FC, Stienen MN, Ivanov M et al (2024) Can AI pass the \nwritten European Board Examination in Neurological Surgery? \n- Ethical and practical issues. Brain Spine 4:102765. https:// doi.  \norg/ 10. 1016/j. bas. 2024. 102765\n 33. stevevi. HIPAA - Azure Compliance. April 6, 2023. https:// learn. \nmicro soft. com/ en- us/ azure/ compl iance/ offer ings/ offer ing- hipaa- \nus. Accessed 9 Mar 2025\n 34. Swisher AR, Wu AW, Liu GC, Lee MK, Carle TR, Tang DM \n(2024) Enhancing health literacy: evaluating the readability of \npatient handouts revised by ChatGPT’s large language model. \nOtolaryngol Neck Surg 171(6):1751–1757. https:// doi. org/ 10.  \n1002/ ohn. 927\n 35. Tangsrivimol JA, Schonfeld E, Zhang M et al (2023) Artificial \nintelligence in neurosurgery: a state-of-the-art review from past \nto future. Diagnostics 13(14):2429. https:// doi. org/ 10. 3390/ diagn \nostic s1314 2429\n 36. Tanno R, Barrett DGT, Sellergren A et al (2025) Collaboration \nbetween clinicians and vision-language models in radiology report \ngeneration. Nat Med 31(2):599–608. https://  doi. org/ 10. 1038/ \ns41591- 024- 03302-1\n 37. Team G, Anil R, Borgeaud S et al Gemini: a family of highly capa-\nble multimodal models. Published online June 17, 2024. https://  \ndoi. org/ 10. 48550/ arXiv. 2312. 11805\n 38. Truhn D, Loeffler CM, Müller-Franzes G et al (2024) Extracting \nstructured information from unstructured histopathology reports \nusing generative pre-trained transformer 4 ( GPT  -4). J Pathol \n262(3):310–319. https:// doi. org/ 10. 1002/ path. 6232\n 39. Ueda D, Kakinuma T, Fujita S et al (2024) Fairness of artificial \nintelligence in healthcare: review and recommendations. Jpn J \nRadiol 42(1):3–15. https:// doi. org/ 10. 1007/ s11604- 023- 01474-3\n 40. Use generative AI for utilization management | Cloud Architecture \nCenter. Google Cloud. https:// cloud. google. com/ archi tectu re/ use- \ngener ative- ai- utili zation- manag ement. Accessed 13 Mar 2025\n 41. Van Veen D, Van Uden C, Blankemeier L et al (2024) Adapted \nlarge language models can outperform medical experts in clinical \ntext summarization. Nat Med 30(4):1134–1142. https:// doi. org/ 10. \n1038/ s41591- 024- 02855-5\n 42. Wang Y, Zuo J, Duan C et al (2024) Large language models \nassisted multi-effect variants mining on cerebral cavernous \nActa Neurochirurgica         (2025) 167:101  \n Page 9 of 9   101 \nmalformation familial whole genome sequencing. Comput Struct \nBiotechnol J 23:843–858. https:// doi. org/ 10. 1016/j. csbj. 2024. 01. \n014\n 43. Williams SC, Starup-Hansen J, Funnell JP et al (2024) Can Chat-\nGPT outperform a neurosurgical trainee? A prospective compar -\native study. Br J Neurosurg 0(0):1–10. https:// doi. org/ 10. 1080/ \n02688 697. 2024. 23082 22\n 44. Wolf T, Debut L, Sanh V et al HuggingFace’s transformers: state-\nof-the-art natural language processing. Published online July 14, \n2020. https:// doi. org/ 10. 48550/ arXiv. 1910. 03771\n 45. Zakka C, Shad R, Chaurasia A et al (2024) Almanac — retrieval-\naugmented language models for clinical medicine. NEJM AI 1(2). \nhttps:// doi. org/ 10. 1056/ aioa2 300068\n 46. Zhang G, Jin Q, Zhou Y et al (2024) Closing the gap between \nopen source and commercial large language models for medical \nevidence summarization. NPJ Digit Med 7(1):239. https:// doi. org/ \n10. 1038/ s41746- 024- 01239-w\n 47. Zhang K, Zhou R, Adhikarla E et al (2024) A generalist vision-\nlanguage foundation model for diverse biomedical tasks. Nat Med \n30(11):3129–3141. https:// doi. org/ 10. 1038/ s41591- 024- 03185-2\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Neurosurgery",
  "concepts": [
    {
      "name": "Neurosurgery",
      "score": 0.8652940988540649
    },
    {
      "name": "Medicine",
      "score": 0.8430088758468628
    },
    {
      "name": "Neuroradiology",
      "score": 0.7684645652770996
    },
    {
      "name": "Interventional radiology",
      "score": 0.7308893799781799
    },
    {
      "name": "Neurology",
      "score": 0.5716007947921753
    },
    {
      "name": "Medical physics",
      "score": 0.4631347954273224
    },
    {
      "name": "General surgery",
      "score": 0.435660719871521
    },
    {
      "name": "Radiology",
      "score": 0.36146557331085205
    },
    {
      "name": "Psychiatry",
      "score": 0.12912127375602722
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I48633490",
      "name": "Mass General Brigham",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1288882113",
      "name": "Boston Children's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1283280774",
      "name": "Brigham and Women's Hospital",
      "country": "US"
    }
  ]
}