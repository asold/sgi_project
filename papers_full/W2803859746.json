{
  "title": "Role-specific Language Models for Processing Recorded Neuropsychological Exams",
  "url": "https://openalex.org/W2803859746",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2337577423",
      "name": "Tuka Al Hanai",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2123775110",
      "name": "Rhoda Au",
      "affiliations": [
        "Framingham Heart Study",
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A2154846939",
      "name": "James Glass",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2114851258",
    "https://openalex.org/W2063042856",
    "https://openalex.org/W2021554924",
    "https://openalex.org/W3014251530",
    "https://openalex.org/W2250357346",
    "https://openalex.org/W2486500903",
    "https://openalex.org/W2084397926",
    "https://openalex.org/W2514643877",
    "https://openalex.org/W2560091445",
    "https://openalex.org/W2096451472",
    "https://openalex.org/W2265553328",
    "https://openalex.org/W2510117016",
    "https://openalex.org/W2091746061",
    "https://openalex.org/W4230133114",
    "https://openalex.org/W2149522871",
    "https://openalex.org/W2408352576",
    "https://openalex.org/W2949651102",
    "https://openalex.org/W2285653139",
    "https://openalex.org/W2003502731",
    "https://openalex.org/W2030322782",
    "https://openalex.org/W2159591770",
    "https://openalex.org/W2102320098",
    "https://openalex.org/W2128970689",
    "https://openalex.org/W2081074144",
    "https://openalex.org/W2021461139",
    "https://openalex.org/W2115017507",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2116950438",
    "https://openalex.org/W2765969799",
    "https://openalex.org/W1524333225"
  ],
  "abstract": "Tuka Al Hanai, Rhoda Au, James Glass. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). 2018.",
  "full_text": "Proceedings of NAACL-HLT 2018, pages 746–752\nNew Orleans, Louisiana, June 1 - 6, 2018.c⃝2018 Association for Computational Linguistics\nRole-speciﬁc Language Models for\nProcessing Recorded Neuropsychological Exams\nTuka Alhanai†, Rhoda Au‡§ , and James Glass†\n†Computer Science and Artiﬁcial Intelligence Laboratory,\nMassachusetts Institute of Technology, Cambridge MA, USA\n‡Departments of Anatomy & Neurobiology, Neurology, and Epidemiology,\nBoston University School of Medicine and Public Health, Boston MA, USA\n§The Framingham Heart Study, Framingham MA, USA\ntuka@mit.edu, rhodaau@bu.edu, glass@mit.edu\nAbstract\nNeuropsychological examinations are an im-\nportant screening tool for the presence of cog-\nnitive conditions (e.g. Alzheimer’s, Parkin-\nson’s Disease), and require a trained tester to\nconduct the exam through spoken interactions\nwith the subject. While audio is relatively\neasy to record, it remains a challenge to auto-\nmatically diarize (who spoke when?), decode\n(what did they say?), and assess a subject’s\ncognitive health. This paper demonstrates a\nmethod to determine the cognitive health (im-\npaired or not) of 92 subjects, from audio that\nwas diarized using an automatic speech recog-\nnition system trained on TED talks and on the\nstructured language used by testers and sub-\njects. Using leave-one-out cross validation\nand logistic regression modeling we show that\neven with noisily decoded data (81% WER)\nwe can still perform accurate enough diariza-\ntion (0.02 % confusion rate) to determine the\ncognitive state of a subject (0.76 AUC).\n1 Introduction\nCognitive impairment is a decline in mental abil-\nities that is severe enough to interfere with daily\nlife (Nussbaum and Ellis, 2003). Such conditions\nare particularly debilitating, with costs of up to\n$200 billion in the USA alone (Prince et al., 2011;\nLeifer, 2003; Alzheimers, 2015), and come second\nonly to spinal-cord injuries and terminal cancer in\nthe severity of their effects (Organization, 2003;\nFerri et al., 2006).\nSeveral methods exist to screen for cognitive\nconditions (e.g. Alzheimer’s, Parkinson’s), rang-\ning from laboratory measures to brain imaging\nscans (Quadri et al., 2004; Van Himbergen et al.,\n2012), with the baseline being set by neuropsy-\nchological examinations. These exams are com-\nposed of multiple components that measure a spe-\nciﬁc domain of cognition such as: thinking, recall,\nspeech, and physical movement. Each exam com-\nponent is assigned a score by the tester according\nto the established rubric. While this exam can be\ncomprehensive, there is an additional dimension\nof information that can be passively recorded - the\naudio of the spoken interactions. Utilizing such\ndata would allow for the identiﬁcation of spoken\nlanguage biomarkers of cognitive impairment.\nHowever, with richer information comes addi-\ntional complexity (Fitch et al., 2016). The appli-\ncation of automatic speech processing technolo-\ngies to medical domains requires a pipeline with\nmultiple stages. Such a system requires audio\npre-processing to locate speech and speaker seg-\nments (i.e. diarization) (Anguera et al., 2012), the\ntranscription of spoken utterances (Besacier et al.,\n2014), and feature representation and modeling of\nthe speaker’s latent condition to determine disease\nbiomarkers for classiﬁcation purposes (Cummins\net al., 2015).\nResearch in this domain can be categorized\ninto two areas. First is the utilization of acous-\ntic and linguistic information to perform speaker\ndiarization and veriﬁcation using standard cor-\npora (e.g. Switchboard, NIST) (Stolcke et al.,\n2006; Reynolds et al., 2003). The second category\nof work seeks to evaluate speech and language\nbiomarkers for the detection of cognitive impair-\nment utilizing measures such as speaking rate,\npauses, n-grams, and Word Error Rates (WERs)\n(Pakhomov et al., 2010; Lehr et al., 2012; Fraser\net al., 2014; Pakhomov and Hemmy, 2014; Vincze\net al., 2016), as well as Automatic Speech Recog-\nnition (ASR) for phonetic alignment and acoustic\nfeature extraction (T ´oth et al., 2015). However,\nsystems from the speech community are devel-\noped using well-curated data with healthy speak-\ners, while the clinical community develops sys-\ntems using manually transcribed data, with some\nexceptions (T´oth et al., 2015; Weiner et al., 2016).\n746\nOur paper seeks to bridge the two areas by au-\ntomating data curation for clinical use.\nWe hypothesize that it is possible to automate\ndata curation for clinical use by conditioning on\nspeaker roles, because speakers (subject/tester)\nduring neuropsychological exams have different\nword usage and speaking patterns due to the ques-\ntion and answer nature of the evaluation. We\nalso hypothesize that not all segments of the exam\nwill be equally valuable in evaluating for cognitive\nconditions, due to potential confusion between\nspeakers when automatically annotating speaker\nsegments, polluting the features used for model-\ning cognitive conditions.\nOur study differentiates itself from prior work\nby combining speaker-speciﬁc language modeling\nand ASR for speaker diarization, with the ulti-\nmate goal of assessing the cognitive condition of\nthe subjects using the acoustic information con-\ntained in the hypothesized (and less than ideal)\nsegments. This is an extension of work by Alhanai\net al. that used gold standard speaker segmenta-\ntions and transcriptions to evaluate cognitive out-\ncomes. Further details on feature selection, mod-\neling, and the relation to previous work in that do-\nmain are described in (Alhanai et al., 2017). This\napproach captures real-world scenarios where au-\ntomatically diarized and transcribed data may not\nbe at human parity but its usage is necessary for\ndeploying screening technologies at scale. More-\nover, audio recordings are often sub-optimal, us-\ning digital recorders on a desk, which is the case\nof the data used in this study. Therefore the ability\nto detect cognitive conditions must accommodate\nthe presence of noisy data, of which we sought to\nevaluate.\n1.1 Objectives\nOur objectives were to (1) automatically extract\nand identify segments of speech that were most\nlikely to belong to the subject, and (2) to evalu-\nate the type of segments that were most predictive\nof a subject’s cognitive condition.\n2 Methods\n2.1 Data\nThe data used in this work was collected from the\nFramingham Heart Study, an on-going longitudi-\nnal population study of 15,447 subjects from 1948\nto the present (Mahmood et al., 2014). Since 1999\na subset of subjects have undergone neuropsycho-\nlogical examinations (Satizabal et al., 2016), and\nas of 2005, it became standard to record audio of\nthese examinations. The neuropsychological ex-\naminations include multiple components to assess\nmemory, attention, executive function, language,\nreasoning, visuoperceptual skills, and premorbid\nintelligence. All participants provided written in-\nformed consent, with study protocols and consent\nforms approved by the institutional review board\nat the Boston University Medical Center.\nOur study used 92 mono-channel audio record-\nings of neuropsychological examinations that had\navailable text transcripts. The exams were com-\nposed of several tests measuring memory, recall,\nlogical and thinking. Further details and a full ex-\nample are found in (Satizabal et al., 2016). The\nrecordings were on average, 65 minutes in dura-\ntion, contained 2,496 words, with a vocabulary\nsize of 527 words.\nTranscripts for each audio ﬁle were generated\nmanually. Transcribers were instructed to include\ntimestamps for each speaker turn (subject/tester),\nindicate who spoke when, transcribe speech ortho-\ngraphically (e.g. nineteen dollars instead of $19),\ninclude tags to highlight moments such as ﬁlled\npauses (<um>), and to insert punctuation.\n2.2 Outcome of Interest\nOur overarching goal was to determine whether\nthe subject being evaluated was cognitively im-\npaired, but we also needed to determine who spoke\nwhen (subject or tester). To this end, we modeled\ntwo levels of outcomes. Our ﬁrst outcome of in-\nterest was a binary indicator of the speaker type\n(subject or tester), with the subject coded as 1.\nOur second outcome of interest was a binary\nindicator of cognitive impairment, with impair-\nment coded as 1. We labeled subjects as cogni-\ntively impaired if the date of impairment (as con-\ncluded by the dementia diagnostic review panel\n(Seshadri et al., 2006) was on or before the date\nof the neuropsychological examination where the\naudio recording took place. Using this criteria, 21\nsubjects (22.8%) were cognitively impaired. Ten\nof these subjects had a severity rating less than\nmild, six were mild, ﬁve were moderate, and none\nwere severe . Fourteen subject were diagnosed as\nhaving Alzheimer’s disease using the NINCDS-\nADRDA criteria (McKhann et al., 2011), and\nﬁve were diagnosed with Vascular dementia based\non the NINCDS-AIRENS criteria (Rom ´an et al.,\n747\n1993).\n2.3 Model Choice and Evaluation Metrics\nTo evaluate speaker diarization we used the Di-\narization Error Rate (DER) metric, as well as\nthe percentage of speech classiﬁed as non-speech\n(Miss), the percentage of non-speech classiﬁed\nas speech (False Alarm), and the percentage of\nspeech misclassiﬁed as belonging to the other\nspeaker (Confusion Rate) (Tranter and Reynolds,\n2006). We used a time-based diarization ap-\nproach, ignoring segments less than 250ms in du-\nration. To evaluate the performance of the ASR\nsystem we used the Word Error Rate (WER) met-\nric. Given the importance of model interpretability\nfor detecting spoken language biomarkers, logis-\ntic regression was chosen as our modeling frame-\nwork. The evaluation metrics we used for detect-\ning cognitive impairment was the Area Under the\nReceiver Operating Characteristic Curve (AUC)\nwhich has the advantage of evaluating model per-\nformance across the whole range of probability\ncutoffs, rather than a single point estimate such as\naccuracy or F1 score (Huang and Ling, 2005). To\nassess the generalizability and robustness of our\nmodeling techniques, we performed leave-one-out\ncross-validation.\n3 Experiment 1: Speaker ID from Text\nWe ﬁrst investigated the language patterns of\nspeakers to determine whether a subject or tester\nwas speaking (i.e., a 2 class problem). We started\nwith the segmentation from the speaker turns la-\nbeled in the transcripts. We trained a trigram lan-\nguage model with Knesser Ney discounting for\neach speaker type. The language models were\nthen used to generate the language perplexity of\nthe spoken (text) segment. The training and test-\ning was performed with leave-one-out validation\n(i.e. 92 folds, one fold for each of the 92 subject-\ntester interactions). Six features were used in the\nlogistic regression model:\n• OOV-rate (x2): The Out-of-V ocabulary rate\nof the subjects’ and testers’ vocabulary (from\ntheir respective training sets).\n• Perplexity (x2): The language model per-\nplexity for the subjects and testers.\n• Perplexity sans <s> (x2): The language\nmodel perplexity for the subjects and testers,\nexcluding the start and end of sentence tags\n(<s>,</s>).\nThis resulted in a classiﬁcation accuracy of 84%\n(±0.06), and an AUC of 0.93 ( ±0.07) These re-\nsults motivated further investigation into classify-\ning speakers from the audio directly.\n4 Experiment 2: Speaker ID from ASR\nFor this experiment, we decoded the audio using\nan Automatic Speech Recognition (ASR) system\nwith a language model trained on each speaker\n(subject/tester), and an acoustic model trained on\nthe TEDLIUM corpus. Each component of the\nASR system was developed as follows:\n• Acoustic Model: The TEDLIUM corpus\ncontains over 1,400 audio recordings and text\ntranscription of TED talks, for a total of 120\nhours of data and 1.7M words (Rousseau\net al., 2012). Using this corpus, we trained\nthe acoustic model as a feedforward Neu-\nral Network (6 layers x 2048 hidden units)\nwith the Minimum Bayes Risk (MBR) cri-\nterion using 40 mel ﬁlterbank features, via\nthe Kaldi speech recognition toolkit using the\n‘s5’ TEDLIUM recipe (Povey et al., 2011;\nRousseau et al., 2012).\n• Language Model: A tri-gram language\nmodel was trained for each of the speaker and\ntester using the SRILM toolkit (Stolcke et al.,\n2002).\n• Lexicon: We generated the word pronuncia-\ntions using the LOGIOS lexical tool1.\nWe decoded the audio in three ways:\n1. Oracle: A language model was trained\nacross all 92 transcripts, and utterances were\nsegmented according to manually generated\nspeaker turns.\n2. Leave-one-out: A language model was\ntrained on all transcripts excluding the tran-\nscript of the audio being decoded. Utterances\nwere segmented according to manually gen-\nerated speaker turns.\n3. Leave-one-out + automatic segmentation :\nA language model was trained on all tran-\nscripts excluding the transcript of the audio\n1http://www.speech.cs.cmu.edu/tools/\nlextool.html\n748\nbeing decoded. Utterances were not seg-\nmented by speaker turns, the full audio was\ndecoded as a single segment.\nThe results are displayed in Table 4. Our Oracle\nsystem performed with a WER of 66.7%, while\ndecoding without language modeling information\n(of the audio being decoded) resulted in a WER of\n68.6%. This relatively small difference in perfor-\nmance (68.6% vs. 66.7%) indicated that the lan-\nguage usage across the audio recordings was con-\nsistent.\nWe also compared the Diariziation Error Rate\n(DER) across the different setups (Table 4). This\nhelped us evaluate how well a speaker could\nbe identiﬁed given various levels of information\nabout the underlying segments being decoded.\nALL SEGMENTS\nOracle Loocv Loocv\nauto seg.\nWER(%) 66.7 68.6 81.3\nDER(%) 35.8 ( ±5.9) 37.2 (±5.5) 40.5 (±05.4)\nMiss 00.2 ( ±0.4) 00.2 (±0.4) 00.2 (±15.3)\nFalse Alarm 03.9 (±1.2) 04.1 (±1.3) 03.7 (±01.3)\nConfusion 31.7 (±5.9) 32.9 (±5.5) 36.7 (±05.3)\nCognitive ID\nAUC 0.72 0.70 0.68\nOPTIMUM SEGMENTS\n95% subj.\n& 10+ words\nTop 9\nlongest\nDER(%) - 98.2 ( ±1.6) 99.9 (±0.2)\nMiss - 97.9 ( ±2.1) 99.9 (±0.4)\nFalse Alarm - 00.0 ( ±0.0) 00.0 (±0.0)\nConfusion - 00.3 ( ±0.6) 0.02 (±0.2)\nCognitive ID\nAUC - 0.75 0.76\nTable 1: ASR, Speaker ID, and Cognitive ID\n5 Experiment 3: Cognitive ID\nUsing the classiﬁed speaker segments, we were\ninterested in determining the subject’s cognitive\ncondition (impaired or not). We modeled each\nsegment using logistic regression and 220 acoustic\nfeatures capturing prosody (pitch, zero-crossing\nrate, jitter, harmonic-to-noise ration) and energy\nin the speech (energy, spectral energy, shimmer).\nFull details on the acoustic feature set, and method\nfor extraction can be found in (Alhanai et al.,\n2017). To calculate model performance, we took\nthe mean predicted probability across all seg-\nments as a single value representing the probabil-\nity of a subject’s cognitive impairment. For this\nexperiment, we performed leave-one-out cross-\nvalidation.\n5.0.1 Speaker Turn Segmentations\nFor the experimental setup that used segmenta-\ntions by speaker turn, we modeled cognitive im-\npairment within a grid search space along two di-\nmensions: (a) the total number of words that were\ndecoded, and (b) by the percentage of words de-\ncoded that were hypothesized to belong to the sub-\nject. The results of evaluating cognitive impair-\nment in this search space can be viewed in Figure\n1. The highest AUC (of 0.75) was found when\nmodeling with segments that had been decoded\nwith at least 10 words, and 95% of which were\nhypothesized to belong to the subject.\nAUC Heatmap\n0.50.550.60.650.70.750.80.850.90.95 1\n% of words decoded as subject`s\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n# of words decoded\n0.66\n0.67\n0.68\n0.69\n 0.7\n0.71\n0.72\n0.73\n0.74\n0.75\nFigure 1: Cognitive ID Heatmap. Heatmap of Subject\nAUC across two thresholds, (y-axis) minimum number\nof words decoded, and (x-axis) percentage of words in\na segment classiﬁed as the subject’s.\n5.0.2 Discarding Speaker Segmentations\nFor the experimental setup that was decoded with-\nout oracle speaker turn segmentation, we ﬁrst seg-\nmented the decoded hypothesis along silences that\nwere longer than 1.5 seconds, and then segmented\naccording to the hypothesized speaker. For mod-\neling, we selected the top N longest segments that\nwere hypothesized to be the subject’s, where N\nwas evaluated from 1 to 15. 99% of segments hy-\npothesized were under 25 seconds in duration, and\nas a pre-processing step we discarded the longest\n1% of hypothesized segments, which were many\nminutes long and several standard deviations be-\nyond the mean (i.e. spurious decodings). The\nhighest AUC (of 0.76) was found when modeling\nthe 9 longest segments hypothesized as the sub-\n749\nject’s. This was an average of 150 seconds ( ±20\nsec) of audio per subject, or 7% of a subject’s total\naudio duration.\n0 5 10 15\nnumber of subject segments\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nAUC\nauto seg.\noracle\nFigure 2: Cognitive ID by Number of Segments. Plot\nof AUC (y-axis) with respect to the number of seg-\nments per-subject (by descending order of length) used\nfor modeling their cognitive impairment (x-axis). Red\npoints indicate best performance (AUC 0.72 and 0.75\nfor oracle and automatic segmentation systems respec-\ntively).\n6 Discussion\nUtilizing audio recordings of spoken interactions\nbetween subjects and tester, the work in this paper\nsought to: (1) automatically extract and identify\nsegments of speech that were most likely to be-\nlong to the subject, and (2) to evaluate the type of\nsegments that were most predictive of a subject’s\ncognitive condition.\n6.1 Experiment 1: Speaker ID from Text\nOur results from the ﬁrst experiment showed that\nlanguage usage between the subject and tester dif-\nfered signiﬁcantly, and that each speaker’s lan-\nguage style was consistent across recordings (i.e.\nsubjects consistently spoke like other subjects,\nand testers consistently spoke like other testers).\nTherefore, with the availability of highly accurate\ntranscriptions of the same structure (neuropsy-\nchological exams), a highly accurate text-based\nspeaker diarization can be conducted.\n6.2 Experiment 2: Speaker ID from ASR\nOur second set of experiments validated the obser-\nvation from the previous experiment on language\nusage patterns across speaker roles (i.e. subjects\nconsistently spoke like other subjects, testers con-\nsistently spoke like other testers, and subjects and\ntesters did not speak like each other). Also, seem-\ningly high WERs (between 66.7% and 81.3%) still\ncontained information that was robust enough for\nfurther usage in diarization and modeling of cog-\nnitive impairment.\n6.3 Experiment 3: Cognitive ID\nOur last experiment showed that it was possible to\nperform modeling of cognitive impairment utiliz-\ning automatically segmented subject speaker turns\nthat was on par with the oracle speaker segmenta-\ntion, and that 9 segments was sufﬁcient for evalu-\nation. As shown in Figure 2, we also found that\nnot all diarization was equal, nor were all seg-\nment lengths equally powerful at modeling sub-\njects’ cognitive state. In the case where no or-\nacle segmentation was available, and automatic\nsegmentation was utilized, longer segments con-\ntained information that was more discriminative\n(AUC 0.68 vs. 0.76). For the oracle system, the\nlongest system was the most and equally predic-\ntive of cognitive impairment, as all segments taken\ntogether. This highlights that tests that elicited\nlonger responses allowed for more robust diariza-\ntion, were evaluating cognitive performance that\nwas (via speech) most strongly associated with the\noutcome, and/or that longer spoken segments pro-\nvided more opportunity to capture patterns associ-\nated with cognitive impairment.\nFurthermore, the modeling paradigm we ex-\nplored was robust enough that neither the under-\nlying neuropsychological test need be explicitly\nmodeled (Lehr et al., 2012), nor do the features\nutilized require word or phone alignments (align-\nments which require accurate transcriptions in or-\nder to generate) (T´oth et al., 2015).\nAcknowledgments\nThe authors thank David Stuck, Maggie San-\ndoval, Alessio Signorini, and Christine Lemke\nfrom Evidation Health Inc., and Ida Xu, Brynna\nWasserman, Maulika Kohli, Nancy Heard-Costa,\nYulin Liu, Karen Mutalik, Mia Lavallee, Christina\nNowak, Alvin Ang, and Spencer Hardy from\nBoston University and The Framingham Heart\nStudy. Data curation was sponsored by\nDARPA and NIH grants AG016495, AG008122,\nAG033040. T. Alhanai thanks the Abu Dhabi Ed-\nucation Council for sponsoring her studies.\n750\nReferences\nTuka Alhanai, Rhoda Au, and James Glass. 2017. Spo-\nken language biomarkers for detecting cognitive im-\npairment.\nAssociation Alzheimers. 2015. 2015 alzheimer’s dis-\nease facts and ﬁgures. Alzheimer’s & dementia: the\njournal of the Alzheimer’s Association11(3):332.\nXavier Anguera, Simon Bozonnet, Nicholas Evans,\nCorinne Fredouille, Gerald Friedland, and Oriol\nVinyals. 2012. Speaker diarization: A review of re-\ncent research. IEEE Transactions on Audio, Speech,\nand Language Processing 20(2):356–370.\nLaurent Besacier, Etienne Barnard, Alexey Karpov,\nand Tanja Schultz. 2014. Automatic speech recog-\nnition for under-resourced languages: A survey.\nSpeech Communication 56:85–100.\nNicholas Cummins, Stefan Scherer, Jarek Krajewski,\nSebastian Schnieder, Julien Epps, and Thomas F\nQuatieri. 2015. A review of depression and suicide\nrisk assessment using speech analysis. Speech Com-\nmunication 71:10–49.\nCleusa P Ferri, Martin Prince, Carol Brayne, Henry\nBrodaty, Laura Fratiglioni, Mary Ganguli, Kath-\nleen Hall, Kazuo Hasegawa, Hugh Hendrie, Yue-\nqin Huang, et al. 2006. Global prevalence of de-\nmentia: a delphi consensus study. The lancet\n366(9503):2112–2117.\nW Tecumseh Fitch, Bart de Boer, Neil Mathur, and\nAsif A Ghazanfar. 2016. Monkey vocal tracts are\nspeech-ready. Science advances 2(12):e1600723.\nKathleen C Fraser, Jed A Meltzer, Naida L Graham,\nCarol Leonard, Graeme Hirst, Sandra E Black, and\nElizabeth Rochon. 2014. Automated classiﬁcation\nof primary progressive aphasia subtypes from narra-\ntive speech transcripts. cortex 55:43–60.\nJin Huang and Charles X Ling. 2005. Using auc and\naccuracy in evaluating learning algorithms. IEEE\nTransactions on knowledge and Data Engineering\n17(3):299–310.\nMaider Lehr, Emily Prud’hommeaux, Izhak Shafran,\nand Brian Roark. 2012. Fully automated neuropsy-\nchological assessment for detecting mild cognitive\nimpairment. In Thirteenth Annual Conference of the\nInternational Speech Communication Association.\nBennett P Leifer. 2003. Early diagnosis of alzheimer’s\ndisease: clinical and economic beneﬁts. Journal of\nthe American Geriatrics Society 51(5s2).\nSyed S Mahmood, Daniel Levy, Ramachandran S\nVasan, and Thomas J Wang. 2014. The framing-\nham heart study and the epidemiology of cardiovas-\ncular disease: a historical perspective. The Lancet\n383(9921):999–1008.\nGuy M McKhann, David S Knopman, Howard\nChertkow, Bradley T Hyman, Clifford R Jack, Clau-\ndia H Kawas, William E Klunk, Walter J Ko-\nroshetz, Jennifer J Manly, Richard Mayeux, et al.\n2011. The diagnosis of dementia due to alzheimers\ndisease: Recommendations from the national in-\nstitute on aging-alzheimers association workgroups\non diagnostic guidelines for alzheimer’s disease.\nAlzheimer’s & dementia7(3):263–269.\nRobert L Nussbaum and Christopher E Ellis. 2003.\nAlzheimer’s disease and parkinson’s disease. New\nEngland Journal of Medicine 348(14):1356–1364.\nWorld Health Organization. 2003. The world health\nreport 2003: shaping the future . World Health Or-\nganization.\nSerguei VS Pakhomov and Laura S Hemmy. 2014. A\ncomputational linguistic measure of clustering be-\nhavior on semantic verbal ﬂuency task predicts risk\nof future dementia in the nun study. Cortex 55:97–\n106.\nSerguei VS Pakhomov, Glenn E Smith, Susan Marino,\nAngela Birnbaum, Neill Graff-Radford, Richard\nCaselli, Bradley Boeve, and David S Knopman.\n2010. A computerized technique to assess language\nuse patterns in patients with frontotemporal demen-\ntia. Journal of neurolinguistics 23(2):127–144.\nDaniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas\nBurget, Ondrej Glembek, Nagendra Goel, Mirko\nHannemann, Petr Motlicek, Yanmin Qian, Petr\nSchwarz, et al. 2011. The kaldi speech recogni-\ntion toolkit. In IEEE 2011 workshop on automatic\nspeech recognition and understanding. IEEE Signal\nProcessing Society, EPFL-CONF-192584.\nMartin Prince, Renata Bryce, and Cleusa Ferri. 2011.\nWorld Alzheimer Report 2011: The beneﬁts of early\ndiagnosis and intervention. Alzheimer’s Disease In-\nternational.\nPierluigi Quadri, Claudia Fragiacomo, Rita Pezzati,\nEnrica Zanda, Gianluigi Forloni, Mauro Tettamanti,\nand Ugo Lucca. 2004. Homocysteine, folate, and vi-\ntamin b-12 in mild cognitive impairment, alzheimer\ndisease, and vascular dementia. The American jour-\nnal of clinical nutrition 80(1):114–122.\nDouglas Reynolds, Walter Andrews, Joseph Campbell,\nJiri Navratil, Barbara Peskin, Andre Adami, Qin Jin,\nDavid Klusacek, Joy Abramson, Radu Mihaescu,\net al. 2003. The supersid project: Exploiting high-\nlevel information for high-accuracy speaker recog-\nnition. In Acoustics, Speech, and Signal Processing,\n2003. Proceedings.(ICASSP’03). 2003 IEEE Inter-\nnational Conference on. IEEE, volume 4, pages IV–\n784.\nGustavo C Rom´an, Thomas K Tatemichi, T Erkinjuntti,\nJL Cummings, JC Masdeu, JHet al Garcia, L Ama-\nducci, J-M Orgogozo, A Brun, Aea Hofman, et al.\n751\n1993. Vascular dementia diagnostic criteria for re-\nsearch studies: Report of the ninds-airen interna-\ntional workshop. Neurology 43(2):250–250.\nAnthony Rousseau, Paul Del ´eglise, and Yannick Es-\nteve. 2012. Ted-lium: an automatic speech recog-\nnition dedicated corpus. In LREC. pages 125–129.\nClaudia L Satizabal, Alexa S Beiser, Vincent Chouraki,\nGenevi`eve Ch ˆene, Carole Dufouil, and Sudha Se-\nshadri. 2016. Incidence of dementia over three\ndecades in the framingham heart study. New Eng-\nland Journal of Medicine 374(6):523–532.\nSudha Seshadri, Alexa Beiser, Margaret Kelly-Hayes,\nCarlos S Kase, Rhoda Au, William B Kannel, and\nPhilip A Wolf. 2006. The lifetime risk of stroke.\nStroke 37(2):345–350.\nAndreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-\nbeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul\nTaylor, Rachel Martin, Carol Van Ess-Dykema, and\nMarie Meteer. 2006. Dialogue act modeling for au-\ntomatic tagging and recognition of conversational\nspeech. Dialogue 26(3).\nAndreas Stolcke et al. 2002. Srilm-an extensible lan-\nguage modeling toolkit. In Interspeech. volume\n2002, page 2002.\nLaszl´o T´oth, G´abor Gosztolya, Veronika Vincze, Ildik´o\nHoffmann, Gr ´eta Szatl ´oczki, Edit Bir ´o, Fruzsina\nZsura, Magdolna P´ak´aski, and J´anos K´alm´an. 2015.\nAutomatic detection of mild cognitive impairment\nfrom spontaneous speech using asr. In Sixteenth An-\nnual Conference of the International Speech Com-\nmunication Association.\nSue E Tranter and Douglas A Reynolds. 2006. An\noverview of automatic speaker diarization systems.\nIEEE Transactions on audio, speech, and language\nprocessing 14(5):1557–1565.\nThomas M Van Himbergen, Alexa S Beiser, Masumi\nAi, Sudha Seshadri, Seiko Otokozawa, Rhoda Au,\nNuntakorn Thongtang, Philip A Wolf, and Ernst J\nSchaefer. 2012. Biomarkers for insulin resistance\nand inﬂammation and the risk for all-cause dementia\nand alzheimer disease: results from the framingham\nheart study. Archives of neurology 69(5):594–600.\nVeronika Vincze, Gabor Gosztolya, Laszlo Toth, Ildiko\nHoffmann, Greta Szatloczki, Zoltan Banreti, Mag-\ndolna Pakaski, and Janos Kalman. 2016. Detecting\nmild cognitive impairment by exploiting linguistic\ninformation from transcripts. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics. volume 2, pages 181–187.\nJochen Weiner, Christian Herff, and Tanja Schultz.\n2016. Speech-based detection of alzheimer’s dis-\nease in conversational german. In INTERSPEECH.\npages 1938–1942.\n752",
  "topic": "Neuropsychology",
  "concepts": [
    {
      "name": "Neuropsychology",
      "score": 0.6550145745277405
    },
    {
      "name": "Computer science",
      "score": 0.6206867098808289
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.48594629764556885
    },
    {
      "name": "Computational linguistics",
      "score": 0.4839661419391632
    },
    {
      "name": "Language technology",
      "score": 0.42491063475608826
    },
    {
      "name": "Cognitive science",
      "score": 0.41040733456611633
    },
    {
      "name": "Natural language processing",
      "score": 0.40932410955429077
    },
    {
      "name": "Linguistics",
      "score": 0.38035809993743896
    },
    {
      "name": "Artificial intelligence",
      "score": 0.377754271030426
    },
    {
      "name": "Psychology",
      "score": 0.2722882628440857
    },
    {
      "name": "Natural language",
      "score": 0.15738987922668457
    },
    {
      "name": "Cognition",
      "score": 0.15333375334739685
    },
    {
      "name": "Philosophy",
      "score": 0.11526244878768921
    },
    {
      "name": "Comprehension approach",
      "score": 0.1103036105632782
    },
    {
      "name": "Neuroscience",
      "score": 0.08554252982139587
    },
    {
      "name": "Physics",
      "score": 0.07680609822273254
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}