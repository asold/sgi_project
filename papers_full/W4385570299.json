{
    "title": "The Turing Quest: Can Transformers Make Good NPCs?",
    "url": "https://openalex.org/W4385570299",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5073251562",
            "name": "Qi Gao",
            "affiliations": [
                "Brock University"
            ]
        },
        {
            "id": "https://openalex.org/A5064694944",
            "name": "Ali Emami",
            "affiliations": [
                "Brock University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3163108010",
        "https://openalex.org/W3028484854",
        "https://openalex.org/W2899586196",
        "https://openalex.org/W2001771035",
        "https://openalex.org/W2099465598",
        "https://openalex.org/W2726070921",
        "https://openalex.org/W2343030434",
        "https://openalex.org/W3009308378",
        "https://openalex.org/W2625977873",
        "https://openalex.org/W4313564992",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4255523714",
        "https://openalex.org/W138032095",
        "https://openalex.org/W4283461431",
        "https://openalex.org/W2063077274",
        "https://openalex.org/W2250545287",
        "https://openalex.org/W4376860094",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2977128309",
        "https://openalex.org/W4287327402",
        "https://openalex.org/W3134354193",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W2182280645",
        "https://openalex.org/W2005814556",
        "https://openalex.org/W3214109107",
        "https://openalex.org/W2611049140",
        "https://openalex.org/W2136350355",
        "https://openalex.org/W2963167310",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3035084718",
        "https://openalex.org/W2935711174",
        "https://openalex.org/W3111693342"
    ],
    "abstract": "In this paper, we study the viability of the deployment of language models towards non-playable character (NPC) scripts, by introducing a novel pipeline for the automatic construction of NPC scripts using Transformer-based believable scripts for a variety of game genres and specifications. In addition, we propose a self-diagnosis method inspired by previous work to develop language models, tailored specifically to desirable NPC qualities such as coherency, believability, and degree of repetition. Finally, we propose a new benchmark, called The Turing Quest, which we use to show that the pipeline, when applied to GPT-3, can generate for a variety of game genres and contexts, NPC scripts that can fool judges in thinking they have been written by humans. We believe that these findings can greatly benefit both the gaming industry and its global community of users, since many current games continue to base their NPCs on manually-curated scripts that are resource-demanding and may curb the immersiveness and enjoyment of the user.",
    "full_text": "Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics - Student Research Workshop, pages 93–103\nJuly 10-12, 2023 ©2023 Association for Computational Linguistics\nThe TuringQuest: Can Transformers Make Good NPCs?\nQi chen Gao\nBrock University\n1812 Sir Isaac Brock Way\nSt. Catharines, ON, Canada\nqgao@brocku.ca\nAli Emami\nBrock University\n1812 Sir Isaac Brock Way\nSt. Catharines, ON, Canada\naemami@brocku.ca\nAbstract\nIn this paper, we investigate the potential of\nusing large pre-trained language models to gen-\nerate non-playable character (NPC) scripts in\nvideo games. We introduce a novel pipeline\nthat automatically constructs believable NPC\nscripts for various game genres and specifica-\ntions using Transformer-based models. More-\nover, we develop a self-diagnosis method, in-\nspired by prior research, that is tailored to es-\nsential NPC characteristics such as coherence,\nbelievability, and variety in dialogue. To evalu-\nate our approach, we propose a new benchmark,\nThe Turing Quest, which demonstrates that our\npipeline, when applied to GPT-3, generates\nNPC scripts across diverse game genres and\ncontexts that can successfully deceive judges\ninto believing they were written by humans.\nOur findings hold significant implications for\nthe gaming industry and its global community,\nas the current reliance on manually-curated\nscripts is resource-intensive and can limit the\nimmersiveness and enjoyment of players.\n1 Introduction\nOver the past decade, there has been a growing in-\nterest in applying deep learning models to Natural\nLanguage Generation (NLG) for open-domain dia-\nlogue systems and conversational agents. In paral-\nlel, the gaming industry has been striving to create\nmore immersive experiences for players by enhanc-\ning their interactions with non-playable characters\n(NPCs). However, the potential of utilizing state-of-\nthe-art deep learning models, such as Transformer-\nbased models, to create NPC scripts remains largely\nunexplored.\nPre-trained Transformer-based language mod-\nels (PLMs) like OpenAI’s GPT-3 (Brown et al.,\n2020) and ChatGPT (Schulman et al., 2022) have\ndemonstrated impressive conversational abilities\n(Milne-Ives et al., 2020). In certain contexts, the\ntext generated by these models can be nearly indis-\ntinguishable from human-written text (M Alshater,\n2022) without the aid of external tools or water-\nmarks (Gambini et al., 2022). The use of these\nmodels in real-world applications has been expand-\ning in areas such as customer service automation\n(Xu et al., 2017) (Zou et al., 2021), educational\nconversational agents (Molnár and Szüts, 2018),\nand mental health dialogue systems (Abd-Alrazaq\net al., 2019).\nDespite their growing prevalence, the effective-\nness and generalization capabilities of PLMs in\nvarious contexts remain uncertain. One such un-\ncharted domain is the creation of “non-playable\ncharacters” or NPCs in video games.\nWhen comparing chatbots to NPCs, the latter\ncan be considered as a narrative-driven variant of\ngoal-oriented chatbots. However, NPCs and chat-\nbots serve different purposes and operate in distinct\nenvironments. Generating NPC scripts presents\nunique challenges, as the dialogue must be consis-\ntent with the game’s plot, genre, and the NPC’s\ncharacter to maintain player immersion and suspen-\nsion of disbelief (Kerr and Szafron, 2009). Accord-\ning to Lee and Heeter (2015), NPC believability\nhinges on “ the size and nature of the cognitive\ngap between the [NPC that] players experience\nand the [NPC] they expect ”. Players anticipate\nNPCs with individualized and possibly dynamic\ntraits, which should be reflected in their dialogue.\nWhile incorporating personality into dialogue sys-\ntems is well-studied (Qian et al., 2017) (Smestad\nand V olden, 2019) (de Haan et al., 2018), the chal-\nlenge of generating goal-oriented, believable NPC\nscripts that align with a game’s narrative and the-\nmatic elements, while preserving player immersion,\nremains substantial.\nThe ability to automatically generate contextu-\nally appropriate dialogue for a specified character\ncould have an effect on the design paradigms of\nfuture video games. While manually scripted nar-\nratives and plot points will continue to hold their\nvalue, developers could augment player immersion\n93\nFigure 1: A sample output of our NPC construction pipeline.\nby allowing an array of NPCs to dynamically re-\nspond to a player’s in-game progression.\nTraditionally, game design involves scripted di-\nalogues only for NPCs that contribute directly to\na quest or story line, thereby limiting the extent\nof player interaction. It is not often possible for a\nplayer to initiate a conversation with a companion\nabout an ongoing quest or solicit their views, creat-\ning an impression that, from an NPC’s perspective,\nthe player’s existence is confined to the quests they\nundertake.\nSimply implementing an interactive compan-\nion system necessitates writing dialogues for ev-\nery quest for all possible companions—a labor-\nintensive task. Expanding this system to encom-\npass a majority of a game’s NPCs would fur-\nther compound these challenges, increasing the\namount of labour to an unreasonable degree. The\nvast amount of dialogue required for each narra-\ntive stage would significantly exceed typical time\nand resource constraints of most developers. De-\nspite the potential enrichment of the player ex-\nperience, the practicality of creating such an im-\nmersive, dialogue-rich environment using solely\nhuman-authored dialogue in game development re-\nmains questionable.\nIn this study, we investigate the application\nof Transformer-based models like GPT-3 to the\ntask of creating NPCs and generating believable\nscripts. To this end, we develop an NPC construc-\ntion pipeline capable of generating dialogue based\non the NPC’s attributes alone. Our pipeline com-\nprises three key modules: a) a Feature Charac-\nterization Schema that classifies NPCs based on\npersonality traits and world descriptions, b) an Au-\ntomatic Prompt Creation process that employs the\nschema to generate tailored prompts for condition-\ning language models, and c) aDialogue Generation\nphase that uses the customized prompts to generate\nscripts with Transformer-based PLMs. Figure 1\nprovides an example of dialogue generated through\nthis pipeline. We also devise and automate an eval-\nuation metric for NPC dialogue quality, drawing\ninspiration from related literature (Brown et al.,\n2020). Lastly, we propose the Turing Quest: a test\nusing human judges to assess the believability and\nquality of generated NPC scripts.\n2 Related Work\nIn recent years, there has been a growing inter-\nest in dialogue systems and conversational agents.\nHowever, the exploration of dialogue generation\nfor NPCs in video games, despite their similarities\nto chatbots, remains limited. Although most video\ngames in the past decade include NPC dialogue,\nresearch on automating its creation using Artificial\nIntelligence (AI) is still in its infancy.\nNPC Dialogue generation. In the early 2000s,\nefforts in NLP to create better NPC dialogue re-\nlied on hand-crafted algorithms and manually au-\nthored grammars (Schlünder and Klabunde, 2013)\n(Ryan et al., 2016). Schlünder and Klabunde (2013)\nsucceeded in generating greetings that players per-\nceived as more polite and appropriate than in-game\n94\ngreetings. However, their rule-based method re-\nlied on labor-intensive, discrete human-defined\nsteps that were difficult to scale into full branching\nconversations. With recent advancements in goal-\noriented chatbots utilizing machine learning tech-\nniques such as reinforcement learning (Liu et al.,\n2020) and dialogue generation through deep re-\ninforcement learning (Li et al., 2016) (Li, 2020),\nautomating NPC dialogue generation becomes in-\ncreasingly feasible.\nThe introduction of AI into games has led to\nthe application of various AI techniques and algo-\nrithms to enhance gameplay experiences through\nimproved bots (Nareyek, 2004) and adaptive ex-\nperiences (Raifer et al., 2022). There has been\nsignificant research into using machine learning to\ncreate bots that provide challenging and entertain-\ning opponents for players (Håkansson and Fröberg,\n2021). However, this trend of applying machine\nlearning to different game design tasks does not\nextend to dialogue generation for NPCs.\nAlthough pre-trained language models such as\nGPT-3 continue to expand their applicability, gen-\neralization remains an unsolved problem. While\nPLMs like GPT-3 have shown natural language gen-\neration capabilities (Topal et al., 2021), research\ninto NLG with Transformer-based models trained\non NPC dialogue has revealed that the generated di-\nalogue “compared rather poorly to human-written\n[dialogue]” in terms of purpose and coherence\n(Kalbiyev, 2022). Nevertheless, generalization dif-\nficulty for LMs is not unique to NPC dialogue (Ye\net al., 2021). We hypothesize that NPC dialogue\nis not merely another generalization problem but a\ndistinct task. This hypothesis is supported by the\ninadequacy of chatbot evaluation metrics (Peras,\n2018) when applied to NPC dialogue.\nNPC Dialogue Metrics. Metrics proposed for\nchatbots do not directly translate to suitable metrics\nfor NPC dialogue. While chatbot success is often\ndetermined by how “human” they sound and their\nability to maintain a conversation with a human\n(Turing, 1950), NPC dialogue is always directed\nand goal-oriented. Generating dialogue for NPCs\npresents unique challenges compared to text gener-\nation in fictional settings. The generated dialogue\nmust be consistent with the game world and the\nNPC’s specific traits and personality, and it should\nensure coherence and contextual relevance in re-\nlation to the player’s input. No test equivalent to\nthe Turing test or its alternatives, such as the Wino-\ngrad schema (WSC) (Winograd, 1972; Levesque\net al., 2011) exists specifically for NPC dialogue.\nTo our knowledge, there is no standard metric to\nevaluate the quality of generated NPC dialogue.\nOne suggested metric for NPC dialogue is “coher-\nence, relevance, human-likeness, and fittingness”\n(Kalbiyev, 2022). While coherence, relevance, and\nhuman-likeness can be applied to chatbots, fitting-\nness—defined by Kalbiyev (2022) as how well the\nresponse fits the game world—is unique to NPCs.\n3 NPC Construction Pipeline\nThe objective of the NPC construction pipeline\nis to automatically generate coherent, contextu-\nally appropriate, and engaging utterances for an\nNPC, given the dialogue history between the NPC\nand a player, as well as the contextual informa-\ntion about the NPC and the game. The pipeline\nconsists of three modules, which serve to a) char-\nacterize the NPC according to a generalized rep-\nresentation schema that captures crucial informa-\ntion about the NPC’s role, personality, and game\ncontext, b) generate short prompts based on the\ncharacterization, providing contextually relevant\npretexts for the language model (LM), and c) gen-\nerate utterances based on these prompts using an\nLM optimized for NPC dialogue generation.\n3.1 Module 1: Feature Characterization\nSchema\nThe first module in the pipeline involves develop-\ning a schema that characterizes a given NPC ac-\ncording to a number of game- and NPC-relevant\nfeatures. Identifying the most concise set of fea-\ntures needed to define any NPC is a challenging\ntask, as NPCs not only exhibit vastly different per-\nsonalities but can also serve different purposes for\nthe player and the game world. For example, in\nthe action role-playing game, “The Elder Scrolls\nV: Skyrim” (Bethesda Game Studios, 2011), the\nNPC Balgruuf the Greater is a Jarl, i.e., a king or\nruler who assigns quests to the player to maintain\npeace. In contrast, a character like KL-E-0 from\n“Fallout 4” (Bethesda Game Studios, 2015), a robot\narms dealer in a post-nuclear apocalyptic world,\nhas little concern for peace. Based on (Warpefelt,\n2016), NPCs should possess both a ludic function\nand a narrative framing for their actions to be coher-\nent and believable. That is, an NPC should fulfill\na gameplay or mechanical purpose—i.e., a ludic\nfunction—while advancing the narrative through\n95\ntheir actions.\nTo develop a characterization of NPCs that cap-\ntures their differences across various games and\ngenres, we should consider several important fea-\ntures, such as their relationship and role with re-\nspect to the player (e.g., buying and selling, pro-\nviding quests, etc.) and their individual personality\nand values. Taking into account narrative purpose,\nludic purposes, and the personality and character-\nistic differences of NPCs, we propose five game-\nspecific features to characterize and distinguish\nNPCs:\nNarrative Ludic\nfunction\nWorld Desc. /Check-mark\nNPC Role /Check-mark\nNPC Personality /Check-mark\nGame State /Check-mark/Check-mark\nNPC Objective /Check-mark/Check-mark\nTable 1: The features and their purpose(s).\nEach of these five features either fulfills a ludic\nfunction or contributes to the game’s narrative, and\nin some cases, a feature serves both purposes. This\nschema enables us to classify NPCs based on their\nin-game mechanics (Hunicke et al., 2004) while\nalso capturing their role in the game’s story. By\nincorporating these features into the NPC construc-\ntion pipeline, we can create NPCs that not only\nadhere to the context and constraints of the game\nworld but also exhibit distinct and engaging per-\nsonalities, which can significantly enhance players’\nimmersion and overall gaming experience.\nWorld Description. A world description pro-\nvides a summary of the story thus far, including\ninformation about the game world and its unique\ncharacteristics. Without this information, actions,\nthoughts, and utterances may be incoherent or unfit-\nting, as they lack awareness of the setting and genre.\nThis may result in dialogue or actions that con-\nflict with the player’s expectations. For instance,\nif Balgruuf from the previous example, originat-\ning from a fantasy adventure game, were placed\nin a sci-fi horror set in space, his actions, appear-\nance, and dialogue would clash with the rest of\nthe game. NPCs become “essentially incompre-\nhensible if they are not framed according to the\nnarrative” (Warpefelt, 2016). Ignoring information\nrelated to the setting, genre, and themes present\nin the NPC’s world may affect the believability\nand fittingness of the NPC. More importantly, the\nnarrative dissonance generated could shatter the\nwillful suspension of disbelief —coined by Samuel\nTaylor Coleridge (1971)—and break the player’s\nimmersion in the game’s world and story.\nRole. Each unique NPC is created to fulfill\na purpose. Continuing from the previous ex-\nample, Balgruuf primarily functions as a quest-\ngiver—facilitating the player’s progression through\nthe main quest line and occasionally offering side\nquests to enrich the narrative experience. Omitting\nhis role would fail to represent a critical function of\nhis character. Defining the role of an NPC, whether\nas a vendor, quest giver, or storyteller, etc., is thus\ncrucial. We selected these roles based on the ty-\npology of NPCs and the NPC model proposed in\n(Warpefelt, 2016). We adapted the types of NPCs\nfrom (Warpefelt, 2016) and simplified the set of\nNPC types to those that would feasibly have a con-\nversation with the player while also merging entries\nthat were similar in their roles. This resulted in\neight types of NPCs, six neutral or friendly roles,\nand two non-friendly roles, as shown below, in\nTable 2.\nMetatype Role\nFunctional\nVendor\nService Provider\nQuestgiver\nProviders Story teller\nFriendly Ally\nCompanion\nAdversaries Enemy\nVillain\nTable 2: Adapted NPC types.\nThe role an NPC occupies influences their ex-\npected dialogue. Although these roles are not mu-\ntually exclusive within a single NPC (e.g., some\nNPCs can be vendors at times while providing a\nquest at another time), at any given point during a\ndialogue with a player, the NPC occupies only one\nof these roles.\nPersonality. To describe any given NPC, it is nec-\nessary to elaborate on their personality and unique\ncharacteristics that distinguish them from other\ncharacters. These characteristics include physical\nattributes and appearances, psychological and per-\nsonality traits such as the strength of the OCEAN\npersonality traits proposed in (Digman, 1990), likes\n96\nand dislikes, etc. This feature focuses on the details\nof the NPC’s character, such as their occupation,\nbeliefs, and other related details. NPCs are charac-\nters at their core, making it essential to incorporate\nthese details into their depiction.\nGame State. This describes the progression of\nthe game and changes to the NPC’s location. The\nNPC’s dialogue may change based on the objec-\ntives completed by the player and the current state\nof the in-game world. The addition of this feature\nallows us to focus on the NPC during any single\ntime frame during the course of the game. This\nenables better classification of dynamic NPCs that\nchange over the course of the game and react to the\nplayer’s actions. This feature also allows specify-\ning details such as the current location of the NPCs\nand the scope of information the NPC possesses.\nGame state serves both a narrative and ludic pur-\npose; for example, a shopkeeper may offer more\ngoods depending on the player’s actions, and the\nNPC’s location also aids in framing their actions\nand dialogue, as a vendor may only offer certain\ngoods in specific towns.\nObjective. The NPC Objective is the purpose of\nthe NPC apart from the player. According to Den-\nnett Daniel (1981), personhood consists of six dif-\nferent themes: Rationality, Intentionality, Stance,\nReciprocity, Communication, and Consciousness.\nProviding an NPC with a role satisfies intention-\nality, as each action should be motivated by what\nthe NPC was designed to achieve. However, giving\nthem goals and aspirations allows the NPC to have\na stance and perhaps evenconsciousness (Kalbiyev,\n2022). If a blacksmith’s objective is to raise enough\nmoney for their family, they should act and speak\naccordingly. Their actions and dialogue should not\nsolely reflect their personality but also their objec-\ntive. This feature allows the schema to capture\ncomplex and dynamic NPCs with intricate values\nand goals not fully represented by their role or per-\nsonality. The addition of this feature enables the\nNPC to have a greater purpose than merely serving\nas an outlet for exposition or facilitating a game\nfunction.\nWith these features, we propose that each unique\nNPC can be encapsulated and represented wholly,\nas shown in figure 2. Each one of these features\nis independent of one another, allowing for mod-\nularity when designing NPCs. However, clashing\ncombinations may still exist regardless of the mod-\nWorld A fantasy world of Dragons and\nmagic; Skyrim\nRole Questgiver\nPersonality Nord, Jarl of Whiterun, Loyal,\nNoble, Blonde, reasonable\nState Sitting on throne in dragonsreach.\nContemplating the war and re-\ncent reports of dragons\nGoal The safety and prosperity of the\npeople of whiterun and a solution\nto the looming dragon threat.\nFigure 2: Completed features for “Balgruuf the\nGreater”.\nular nature of this schema.\n3.2 Module 2: Prompt Creation\nPrompt creation was designed with the feature rep-\nresentation schema in mind. Providing the LM\nwith sufficient information about an NPC is cru-\ncial to ensure that the generated dialogue remains\nconsistent with the character’s identity. These re-\nquirements are akin to the challenges faced by the\nfeature representation schema. Consequently, the\nprompt creation module integrates the various fea-\ntures present in the schema and uses them as a\nprompt. The first line of each prompt begins with\nthe sentence “You are an NPC in a game”, followed\nby optional details such as a name, some details\nabout the world that the NPC inhabits, the role of\nthe NPC, basic personal characteristics, their cur-\nrent state (e.g., sitting outside thinking about their\ndaughter), and finally their goal(s). Most of these\ncategories are optional, except for the NPC type\n(i.e., their role), which must always be present. By\nincorporating these features, the prompt creation\nmodule empowers users to guide the LM in generat-\ning diverse NPCs with individualized personalities,\nallowing for greater customization without the need\nfor prior fine-tuning or training.\nNPC Header. Utilizing this prompt creation\nmethod, we created the NPC header, a represen-\ntative example is depicted in figure 3. This header\nplays a pivotal role in dialogue generation by pro-\nviding essential information about the character.\nFor our needs, we also created a player header us-\ning the same information used in the NPC header,\nguiding the LM to mimic a player’s behavior and\nfacilitate automated dialogue generation. The gen-\nerated player dialogue is less creative and more\nprone to repetition compared to human-written dia-\n97\nFigure 3: Example of an NPC header.\nlogue. This issue is beyond the scope of this paper,\nas our focus lies on NPC dialogue generation.\n3.3 Module 3: Dialogue Generation\nDialogue generation was executed automatically\nand iteratively. The prompt was structured as a\ncombination of the header and the current dialogue\nhistory. The header section is continually swapped\ndepending on which agent’s dialogue—NPC or\nplayer—is currently being generated. By placing\nthe header at the top of the prompt and swapping it\nfor the active agent, PLMs can generate dialogue\nthat is coherent with the current speaker and their\ntraits.\nFirst Sentences. In early development-stage re-\nsults, GPT-3 demonstrated difficulty in generating\neffective first sentences. Combined with the inher-\nent challenge of generating human-like responses,\nthis led to a significant drop in the overall quality of\ndialogue—often resulting in both NPC and player\ngenerating blank lines or constantly repeating the\nsame responses. A workaround was developed by\nemploying a small set of hand-written first sen-\ntences based on the genre and NPC type. This\nworkaround allowed the conversation to avoid im-\nmediate repetition while minimizing interference\nwith dialogue generation.\nRepetition. In our preliminary testing, we found\nthat PLMs struggle to avoid repetition when the\nplayer dialogue is similar to a past query or sen-\ntence. This often caused the NPC’s response to be\nsimilar or even identical to its previous response.\nTo circumvent this issue, we implemented a dy-\nnamic frequency penalty. The dynamic frequency\npenalty incrementally increases when the NPC or\nplayer generates a response that already exists in\nthe conversation. After detecting a repetition and\nincrementing the frequency penalty, the LM at-\ntempts to regenerate with the same prompt, exclud-\ning the repeated sentence. This process occurs up\nto three times or until a new sentence is generated\nbefore resetting the frequency penalty to the orig-\ninal value before any increments. This technique\nsignificantly reduced overall repetitions and drasti-\ncally decreased the occurrence of loops appearing\nearly in the conversation.\n4 Evaluation\nTo assess the performance of the NPC construction\npipeline and the resulting generated dialogue, we\ndesigned a comprehensive evaluation metric that\nexamines dialogue quality based on coherency, be-\nlievability, degree of repetition, alignment of the\nNPC’s dialogue with their role, and fittingness of\nthe NPC’s dialogue within their world. These cat-\negories draw from and adapt Kalbiyev (2022)’s\nmetric for evaluating video game dialogue. Each\nmetric is assigned a score between one and five,\nwith the sum of these scores indicating the overall\nquality of the dialogue.\nSelf-diagnosis harnesses the capacity of\nTransformer-based language models to detect\npatterns within text and their few-shot learning\nperformance to enable rapid, automated evaluation\nof dialogue without prior fine-tuning. We con-\nducted a human evaluation of 66 different NPC\nscripts to assess the accuracy and reliability of our\nself-diagnosis approach. After each conversation\nwas evaluated and scored, we found a correlation\nbetween parameters and their average score. By\nincluding our full NPC header, we were able to\ngenerate dialogue of higher quality. We then\nconducted a single-blind test where human judges\nwere asked to determine whether an NPC script\nwas generated by AI or written manually by a\nhuman.\n4.1 Self-Diagnosis\nWe investigated the ability of pretrained language\nmodels, such as GPT-3, to understand, evaluate,\nand diagnose dialogue when given a specific non-\ntrivial query (e.g., “whether an NPC behaved co-\nherently”). Schick et al. (2021) demonstrate that\nPLMs can identify socially undesirable attributes in\ntext, such as racism and violence. We propose that\nthis self-diagnosis capability is not only applicable\nto socially undesirable attributes but also enables\nPLMs to self-diagnose a broader and more general\nset of attributes, themes, and behaviors without fur-\n98\nther fine-tuning. For simple questions, such as if a\ngenre was clearly distinguishable in text, PLMs per-\nform accurately in a zero-shot environment without\nexamples and further guidance. This behavior is\nsupported by Sanh et al. (2022). However, this per-\nformance does not hold when dealing with more\ncomplicated and potentially subjective questions.\nFigure 4: Prompt structure of self-diagnosis.\nOur self-diagnosis approach consists of provid-\ning examples of different scoring dialogue for each\nmetric that needed further clarification. By scoring\ndialogue”, we mean, for example, giving the LM\na prompt like “What a perfect score looks like” or\n“What a 3 should look like”. In preliminary tests,\nwe found that simply inputting a script and posing\na question led to relatively reliable results; however,\nthe output occasionally did not align with human\nresponses or logic. By formulating the question\nmore precisely and asking for a numeric response\nrather than a free-form sentence response, we were\nable to obtain a numeric answer more accurately.\nTo account for potential variability in the responses,\nwe set the temperature to 0 for each test, yielding a\ndeterministic model devoid of stochastic behavior.\nWe leveraged the PLM’s few-shot learning abili-\nties by adding three examples of different scoring\nsample dialogue before the prompt. This approach\naligns scores obtained through self-diagnosis more\nclosely with human scores on queries that a PLM\nwould otherwise have difficulties with.\n4.2 The Turing Quest\nTo evaluate the performance of our NPC Construc-\ntion pipeline and the degree to which the resulting\ngenerated dialogue appears human-written, we pro-\npose a test tailored to NPC dialogue—the Turing\nQuest. Inspired by the Turing test (Turing, 1950),\nthe goal of this test is to determine whether a gener-\nated NPC script can be distinguished from human-\nwritten dialogue by human judges. A script passes\nthe Turing Quest if the judge deems it human-\nwritten, and fails if perceived as AI-generated.\nConducting this test on multiple NPC script sam-\nples helps assess the proficiency of state-of-the-art\nPLMs in generating convincing NPC dialogue.\nThe Turing Quest is a self-administered question-\nnaire. For each script, it asks the judge to determine\nif the NPC’s dialogue is written by a human or an\nAI. Since the scope of this test is to determine the\nbelievability of an NPC’s dialogue, the player’s\ndialogue can be manually written by a human.\nFor our test, six NPC scripts were evaluated by\n12 individual judges. Four of the six scripts were\ngenerated by GPT-3, one was manually written, and\nthe final script was sampled from the game Skyrim.\nOur test group comprised twelve people familiar\nwith video games and NPCs. From the responses\nof our judges, we determined the average passing\nrate was 64.58% for all AI-generated scripts. The\nbest performing generated script had a pass rate of\n75%. Interestingly, 75% of judges believed that the\ndialogue sampled from Skyrim was AI-generated\nand 50% thought the same for the manually written\nscript. This could highlight the expectations of\nplayers regarding the current state and abilities of\nLMs and conversational agents. These findings\nprovide strong empirical evidence that our pipeline,\nwhen applied to PLMs, is capable of producing\nNPC scripts that resemble and perhaps even surpass\nhuman-written NPC dialogue.\n5 Experiments and Results\n5.1 Parameter Search and Model Selection\nWe conducted a comprehensive random grid pa-\nrameter search to identify the optimal model and\nparameters for generating high-quality NPC dia-\nlogue. Three key parameters influenced the quality\nand score of the generated dialogue: the language\nmodel, temperature setting, and the integration of\nour NPC construction pipeline prompt.\nUtilizing different versions of GPT-3 (OpenAI’s\ntext-davinci-002, text-curie-001, and text-babbage-\n001 models) and a range of temperatures (0 to 1,\nincremented by 0.1), we compared the quality of\ndialogue generated with our full prompt and a min-\nimal version without the world description, NPC\nPersonality, game state, and NPC objective sec-\ntions. We repeated the experiment with another\nNPC role to ensure generalizability1.\n1The code to reproduce all of our experimental results\nare available at https://github.com/FieryAced/-NPC-Dialogue-\nGeneration.\n99\nFigure 5: Evaluation Scores of varying models and\ntemperatures.\nOur analysis revealed a significant decline in\nquality from the text-davinci-002 to text-curie-001\nmodels, and an even more pronounced decrease\nbetween text-curie-001 and text-babbage-001. This\nis consistent with recent research which has shown\nthat larger and more complex models, such as GPT-\n3’s text-davinci-002 model, have the ability to learn\nand generalize more complex patterns from larger\nand more diverse datasets, resulting in better per-\nformance across a wide range of natural language\nprocessing tasks (Brown et al., 2020).\nFurthermore, the recently proposed InstructGPT\nframework by Ouyang et al. (2022) allows for tar-\ngeted fine-tuning of pre-trained language models\nto better suit the task at hand. This approach in-\nvolves providing additional instructions during fine-\ntuning, such as providing task-specific prompts or\ndata augmentation techniques, which results in im-\nproved performance for downstream tasks. With\nthe success of InstructGPT, it is becoming increas-\ningly clear that language models can be further\noptimized for specific use-cases by adjusting their\narchitecture or fine-tuning process. Thus, it is rea-\nsonable to assume that newer and more advanced\nmodels, such as text-davinci-003, should gener-\nally perform better than their predecessors. Finally,\nour analysis shows that full-prompt models outper-\nformed minimal prompt ones, with an average 4.06\npoint higher score, demonstrating the effectiveness\nof our prompting method.\nA Pearson correlation test (excluding the atyp-\nical data point with a temperature of 0) showed a\npositive correlation between temperature and score,\nr(8) = .7055, p= .022646. Higher temperature\nvalues yielded better results, with the highest aver-\nage scores at temperatures of 0.9 and 0.8.\nBased on these findings, we recommend using\nadvanced Transformer-based LMs like OpenAI’s\nGPT-3 “text-davinci-002” at a temperature around\n0.9, along with our NPC construction pipeline, for\noptimal NPC script generation.\n5.2 Results\nSelf-Diagnosis: To assess the reliability of the\nself-diagnosis module, we manually evaluated 66\nNPC scripts using the same metrics applied in self-\ndiagnosis. A Pearson correlation test showed a\nstrong positive correlation between self-diagnosed\nand human-evaluated scores, r(64) = .8092, p <\n.00001. This demonstrates the module’s con-\nsistency and correlation with human evaluation\nscores.\nTuring Quest Results: Our NPC construction\npipeline, when using the recommended parameters,\ngenerates dialogue that not only passes as human-\nwritten but also scores highly on the evaluation\nmetric. On average, our generated dialogue was\nthought to be hand-written64.58% of the time with\nthe best performing script passing as human writ-\nten 75% of the time. The generated NPC scripts\nexhibit goal-oriented behavior and adherence to the\nin-game world and genre, maintaining player im-\nmersion. The Turing Quest results further confirm\nthe high quality of the generated dialogue.\n6 Conclusion\nWe developed a novel pipeline capable of auto-\nmatically generating NPC scripts comparable or of\nsuperior quality to human-written NPC dialogue\nusing Transformer-based PLMs. We then created a\nself-diagnosis module which provides a method to\nevaluate and compare the quality of NPC dialogue\nquantitatively. Finally, our proposal of the Turing\nQuest allows us to determine the capabilities of a\nlanguage model when applied to the task of NPC\ndialogue generation and whether a script passes\nas human-written. While the NPC construction\npipeline allows for modularity even in between re-\nsponses, that aspect was not explored in depth in\nthis paper. We will explore dialogue generation for\ndynamic NPCs with evolving roles or attributes in\nfuture research.\nLimitations\nThe dialogue generated for the player exhibits a\nhigher degree of repetition and has a tendency to-\n100\nwards looping. This limitation exists as we did\nnot focus on generating player dialogue as that is\na different problem of its own. To account for this\nlimitation, both the self-diagnosis and the Turing\nQuest only evaluate the NPC’s dialogue.\nCurrently, the maximum context window for the\ndialogue history portion is limited by the max to-\nkens of a given model minus the tokens required for\nthe NPC header. Despite being a rare occurrence,\nit is possible that the dialogue history becomes so\nlong that the model may not be able to generate\nany responses as there is no more remaining space.\nWe did not experience this problem; however, a\nworkaround would be to discard the oldest dialogue\nhistory entry as needed. This approach however\nmay cause the NPC to lose out on information that\nit would otherwise be able to leverage in dialogue.\nEthics Statement\nThe presence of bias within NPC models/systems\nposes a significant risk particularly as the demo-\ngraphic of young individuals, still in the age of\ndevelopment, who enjoy playing video games con-\ntinues to expand. In 2006, 92% of children in\nthe ages of 2-17 had played video games (Do˘gan,\n2006). 97% of players under the age of of 18 play\nmore that an hour of games daily (Granic et al.,\n2014). According to recent statistics, the global\ndemographic of active video game players is pro-\njected to increase over 5% year-over-year (Do˘gan,\n2006), reaching over 3 billion active players world-\nwide in 2023 2. This means, in the future, video\ngames will reach more young children and adoles-\ncents. If the presence of bias is not addressed, it\ncould subconsciously normalize problematic be-\nhaviours seen in games in children as humans are a\nproduct of both nature and nurture (Plomin and As-\nbury, 2005). This in turn may lead to more biases\nbeing overlooked or ignored by the next generation\nof researchers, creating a vicious cycle.\nAcknowledgements\nThis work was supported by the Natural Sciences\nand Engineering Research Council of Canada and\nby the New Frontiers in Research Fund.\n2https://www.statista.com/statistics/748044/number-\nvideo-gamers-world\nReferences\nAlaa A Abd-Alrazaq, Mohannad Alajlani, Ali Abdal-\nlah Alalwan, Bridgette M Bewick, Peter Gardner,\nand Mowafa Househ. 2019. An overview of the\nfeatures of chatbots in mental health: A scoping re-\nview. International Journal of Medical Informatics,\n132:103978.\nBethesda Game Studios. 2011. The elder scrolls v:\nSkyrim.\nBethesda Game Studios. 2015. Fallout 4.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nHayco de Haan, Joop Snijder, Christof van Nimwegen,\nand Robbert Jan Beun. 2018. Chatbot personality\nand customer satisfaction. Info Support Research.\nC Dennett Daniel. 1981. Conditions of personhood.\nThe Identities of Persons, 175.\nJohn M Digman. 1990. Personality structure: Emer-\ngence of the five-factor model. Annual review of\npsychology, 41(1):417–440.\nFiliz Öztütüncü Do ˘gan. 2006. Video games and chil-\ndren: violence in video games. In New/Yeni Sympo-\nsium Journal, volume 44, pages 161–164.\nMargherita Gambini, Tiziano Fagni, Fabrizio Falchi,\nand Maurizio Tesconi. 2022. On pushing deepfake\ntweet detection capabilities to the limits. In 14th\nACM Web Science Conference 2022 , WebSci ’22,\npage 154–163, New York, NY , USA. Association for\nComputing Machinery.\nIsabela Granic, Adam Lobel, and Rutger CME Engels.\n2014. The benefits of playing video games. Ameri-\ncan psychologist, 69(1):66.\nCarl Håkansson and Johan Fröberg. 2021. Applica-\ntion of machine learning to construct advanced npc\nbehaviors in unity 3d.\nRobin Hunicke, Marc Leblanc, and Robert Zubek. 2004.\nMda: A formal approach to game design and game\nresearch. AAAI Workshop - Technical Report, 1.\nA Kalbiyev. 2022. Affective dialogue generation for\nvideo games. Master’s thesis, University of Twente.\n101\nChristopher Kerr and Duane Szafron. 2009. Support-\ning dialogue generation for story-based games. In\nProceedings of the AAAI Conference on Artificial\nIntelligence and Interactive Digital Entertainment ,\nvolume 5, pages 154–160.\nMichael Sangyeob Lee and Carrie Heeter. 2015. Cogni-\ntive intervention and reconciliation: Npc believability\nin single-player rpgs. International Journal of Role-\nPlaying, 5:47–65.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2011. The winograd schema challenge. In AAAI\nSpring Symposium: Logical Formalizations of Com-\nmonsense Reasoning.\nJiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jian-\nfeng Gao, and Dan Jurafsky. 2016. Deep reinforce-\nment learning for dialogue generation. arXiv preprint\narXiv:1606.01541.\nPiji Li. 2020. An empirical investigation of pre-trained\ntransformer language models for open-domain dia-\nlogue generation. CoRR, abs/2003.04195.\nJianfeng Liu, Feiyang Pan, and Ling Luo. 2020. Gochat:\nGoal-oriented chatbots with hierarchical reinforce-\nment learning. In Proceedings of the 43rd Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval , SIGIR ’20,\npage 1793–1796, New York, NY , USA. Association\nfor Computing Machinery.\nMuneer M Alshater. 2022. Exploring the role of artifi-\ncial intelligence in enhancing academic performance:\nA case study of chatgpt. Available at SSRN.\nMadison Milne-Ives, Caroline de Cock, Ernest Lim,\nMelissa Harper Shehadeh, Nick de Pennington, Guy\nMole, Eduardo Normando, and Edward Meinert.\n2020. The effectiveness of artificial intelligence con-\nversational agents in health care: Systematic review.\nJ Med Internet Res, 22(10):e20346.\nGyörgy Molnár and Zoltán Szüts. 2018. The role of\nchatbots in formal education. In 2018 IEEE 16th\nInternational Symposium on Intelligent Systems and\nInformatics (SISY), pages 000197–000202. IEEE.\nAlexander Nareyek. 2004. Ai in computer games:\nSmarter games are making for a better user expe-\nrience. what does the future hold? Queue, 1(10):58–\n65.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nDijana Peras. 2018. Chatbot evaluation metrics. Eco-\nnomic and Social Development: Book of Proceedings,\npages 89–97.\nRobert Plomin and Kathryn Asbury. 2005. Nature and\nnurture: Genetic and environmental influences on\nbehavior. The Annals of the American Academy of\nPolitical and Social Science, 600(1):86–98.\nQiao Qian, Minlie Huang, Haizhou Zhao, Jingfang\nXu, and Xiaoyan Zhu. 2017. Assigning personal-\nity/identity to a chatting machine for coherent con-\nversation generation. CoRR, abs/1706.02861.\nMaya Raifer, Guy Rotman, Reut Apel, Moshe Tennen-\nholtz, and Roi Reichart. 2022. Designing an auto-\nmatic agent for repeated language–based persuasion\ngames. Transactions of the Association for Computa-\ntional Linguistics, 10:307–324.\nJames Ryan, Michael Mateas, and Noah Wardrip-Fruin.\n2016. Characters who speak their minds: Dialogue\ngeneration in talk of the town. In Twelfth Artificial\nIntelligence and Interactive Digital Entertainment\nConference.\nSamuel Taylor Coleridge. 1971. Biographia Literaria,\n1817.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,\nHan Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Tali Bers, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-Diagnosis and Self-Debiasing: A Proposal for\nReducing Corpus-Based Bias in NLP. Transactions\nof the Association for Computational Linguistics ,\n9:1408–1424.\nBjörn Schlünder and Ralf Klabunde. 2013. Greetings\ngeneration in video role playing games. In Proceed-\nings of the 14th European Workshop on Natural Lan-\nguage Generation, pages 167–171, Sofia, Bulgaria.\nAssociation for Computational Linguistics.\nJ Schulman, B Zoph, C Kim, J Hilton, J Menick, J Weng,\nJFC Uribe, L Fedus, L Metz, M Pokorny, et al. 2022.\nChatgpt: Optimizing language models for dialogue.\nTuva Lunde Smestad and Frode V olden. 2019. Chatbot\npersonalities matters. In International conference on\ninternet science, pages 170–181. Springer.\nM. Onat Topal, Anil Bas, and Imke van Heerden. 2021.\nExploring transformers in natural language genera-\ntion: Gpt, bert, and xlnet. CoRR, abs/2102.08036.\n102\nA. M. Turing. 1950. I.—COMPUTING MACHINERY\nAND INTELLIGENCE. Mind, LIX(236):433–460.\nHenrik Warpefelt. 2016. The Non-Player Character:\nExploring the believability of NPC presentation and\nbehavior. Ph.D. thesis, Stockholm University.\nTerry Winograd. 1972. Understanding natural language.\nCognitive Psychology, 3(1):1–191.\nAnbang Xu, Zhe Liu, Yufan Guo, Vibha Sinha, and\nRama Akkiraju. 2017. A new chatbot for customer\nservice on social media. In Proceedings of the 2017\nCHI conference on human factors in computing sys-\ntems, pages 3506–3510.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\nCrossfit: A few-shot learning challenge for cross-\ntask generalization in nlp.\nYicheng Zou, Lujun Zhao, Yangyang Kang, Jun Lin,\nMinlong Peng, Zhuoren Jiang, Changlong Sun,\nQi Zhang, Xuanjing Huang, and Xiaozhong Liu.\n2021. Topic-oriented spoken dialogue summariza-\ntion for customer service with saliency-aware topic\nmodeling. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 35, pages 14665–\n14673.\n103"
}