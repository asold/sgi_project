{
  "title": "Discovering sentiment insights: streamlining tourism review analysis with Large Language Models",
  "url": "https://openalex.org/W4406545980",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2918679402",
      "name": "Dario Guidotti",
      "affiliations": [
        "University of Sassari"
      ]
    },
    {
      "id": "https://openalex.org/A2101543707",
      "name": "Laura Pandolfo",
      "affiliations": [
        "University of Sassari"
      ]
    },
    {
      "id": "https://openalex.org/A1230122370",
      "name": "Luca Pulina",
      "affiliations": [
        "University of Sassari"
      ]
    },
    {
      "id": "https://openalex.org/A2918679402",
      "name": "Dario Guidotti",
      "affiliations": [
        "University of Sassari"
      ]
    },
    {
      "id": "https://openalex.org/A2101543707",
      "name": "Laura Pandolfo",
      "affiliations": [
        "University of Sassari"
      ]
    },
    {
      "id": "https://openalex.org/A1230122370",
      "name": "Luca Pulina",
      "affiliations": [
        "University of Sassari"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4286216949",
    "https://openalex.org/W4385417666",
    "https://openalex.org/W2962903510",
    "https://openalex.org/W3162241272",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W3163841364",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2276215713",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2803436629",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2946565880",
    "https://openalex.org/W2985875790",
    "https://openalex.org/W2050075801",
    "https://openalex.org/W1608357024",
    "https://openalex.org/W4283168218",
    "https://openalex.org/W2890831498",
    "https://openalex.org/W4205248469",
    "https://openalex.org/W2922162322",
    "https://openalex.org/W4379986648",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4285190530",
    "https://openalex.org/W2735294628",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W2832586745",
    "https://openalex.org/W2990436242",
    "https://openalex.org/W2953771450",
    "https://openalex.org/W4391855109",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W2130901086",
    "https://openalex.org/W4300555876",
    "https://openalex.org/W2138854216",
    "https://openalex.org/W4213072767",
    "https://openalex.org/W4293528099",
    "https://openalex.org/W2963486920",
    "https://openalex.org/W2910453440",
    "https://openalex.org/W3174544005",
    "https://openalex.org/W4295857769",
    "https://openalex.org/W4210827551",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963499153",
    "https://openalex.org/W3131290958"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)\nInformation Technology & Tourism (2025) 27:227–261\nhttps://doi.org/10.1007/s40558-024-00309-9\nORIGINAL RESEARCH\nDiscovering sentiment insights: streamlining tourism \nreview analysis with Large Language Models\nDario Guidotti1  · Laura Pandolfo1 · Luca Pulina1\nReceived: 10 April 2024 / Revised: 24 December 2024 / Accepted: 27 December 2024 /  \nPublished online: 17 January 2025 \n© The Author(s) 2025\nAbstract\nWith digital technology increasingly shaping the tourism industry, understanding \ncustomer sentiment and identifying key themes in reviews is crucial for enhanc-\ning service quality. However, traditional sentiment analysis and keyword extraction \nmodels typically demand significant time, computational resources, and labelled \ndata for training. In this paper, we explore how Large Language Models (LLMs) \ncan be leveraged to automatically classify reviews as positive or negative and extract \nrelevant keywords without the need for dedicated training. Additionally, we frame \nthe keyword extraction task as a tool to assist human users in comprehending and \ninterpreting review content, especially in scenarios where ground truth labels for \nkeywords are unavailable. To evaluate our approach, we conduct an experimental \nanalysis using several datasets of tourism reviews and various LLMs. Our results \ndemonstrate the reliability of LLMs as zero-shot classifiers for sentiment analy -\nsis and showcase the efficacy of the approach in extracting meaningful keywords \nfrom reviews, providing valuable insights into customer sentiments and preferences. \nOverall, this research contributes to the intersection of information technology \nand tourism by presenting a practical solution for sentiment analysis and keyword \nextraction in tourism reviews, leveraging the capabilities of LLMs as versatile tools \nfor enhancing decision-making processes in the tourism industry.\nKeywords AI for tourism · Sentiment analysis · Natural language processing · \nKeyword extraction\n * Dario Guidotti \n dguidotti@uniss.it\n Laura Pandolfo \n lpandolfo@uniss.it\n Luca Pulina \n lpulina@uniss.it\n1 Artificial Intelligence and Formal Methods Laboratory, Department of Humanities and Social \nSciences, University of Sassari, Via Roma 151, 07100 Sassari, Sardinia, Italy\n228 D. Guidotti et al.\n1 Introduction\nThe tourism industry, like many other sectors, is experiencing profound shifts \ndue to the pervasive integration of digital technology into its operations  (Pen-\ncarelli 2020). This ongoing evolution has prompted a critical imperative for tour -\nism businesses: the ability to effectively interpret and respond to the sentiments \nconveyed within customer reviews. As travellers increasingly rely on digital plat-\nforms to share their experiences, the wealth of information contained within these \nreviews presents a valuable opportunity for businesses to gain insights into cus-\ntomer preferences, satisfaction levels, and areas for improvement (Rossetti et al. \n2016; Jardim and Mora 2021).\nSentiment analysis and keyword extraction stand as two Natural language pro-\ncessing (NLP) tasks that provide these capabilities. Specifically, sentiment anal-\nysis facilitates the automatic classification of texts (e.g., reviews, social media \ncomments) into positive, negative, or neutral categories, while keyword extrac-\ntion enables the distillation of primary themes and topics expressed within such \ntexts. However, traditional methods of sentiment analysis and keyword extraction \npresent considerable obstacles in this endeavour. These approaches often demand \nsignificant investments of time, computational resources, and access to labelled \ndata for training (Sharma et al. 2022). Such requirements can prove prohibitive, \nparticularly for smaller businesses or those operating within niche markets where \ndata availability may be limited. Consequently, businesses face challenges in har -\nnessing the full potential of customer feedback to inform strategic decision-mak -\ning and enhance service quality. Zero-shot learning (ZSL) emerges as a potential \nsolution to overcome these limitations: this methodology capitalises on seman-\ntic embeddings to deduce insights about unseen data based on their correlation \nwith known data. Consequently, these models can be directly applied without the \nnecessity for task-specific training.\nIn this paper, we showcase the potential of Large Language Models (LLMs) to \nstreamline sentiment analysis and keyword extraction in tourism reviews. Unlike \nconventional approaches, which often rely on laborious processes involving the \nmanual labelling of vast datasets for training, these models harnesses the power \nof zero-shot learning to automatically classify reviews as positive or negative \nand extract pertinent keywords without the need for specialised training data. To \nassess the effectiveness of our approach, we conduct an experimental analysis \nevaluating multiple datasets of tourism reviews and various LLM architectures. \nThe findings of our study assess the reliability of LLMs as zero-shot classifiers \nfor sentiment analysis and underscore the efficacy of our approach in extract-\ning meaningful keywords from reviews. These insights offer valuable perspec-\ntives into customer sentiments and preferences, thereby providing tourism Small \nMedium Enterprises (SMEs) with actionable intelligence for enhancing decision-\nmaking processes. To the extent of out knowledge, the presented study is the first \nto explore the utilisation of zero-shot learning models for our tasks of interest or \nto conduct a comparison between such models.\n229\nDiscovering sentiment insights: streamlining tourism review…\nIn summary, this study contributes to the intersection of digital technology \nand tourism by presenting a practical solution for sentiment analysis and keyword \nextraction in tourism reviews. By harnessing the versatility of LLMs, we lay the \ngroundwork for informed decision-making and enhanced service delivery within \nthe ever-evolving tourism sector. Furthermore, by emphasising open-source LLM \narchitectures that do not necessitate specialised training from end-users, the pro-\nposed solution can be readily utilised, even by SMEs that may struggle to acquire \nthe financial and computational resources required to train NLP models indepen-\ndently. In the framework of the proposed work, one of our objectives is to prioritise \ntechnology transfer to actively involve local communities in digital innovation, thus \npropelling towards a knowledge-based economy. This involves supporting SMEs, \nwhich are crucial components of our regional economy, in the adoption of new tech-\nnologies to bolster their current and future competitiveness.\nThe rest of the paper is structured as follows. In Sect.  2 we introduce some basic \nconcepts and definitions, while in Sect.  3 we present the related work. In Sect.  4 \nwe describe the presented approach and the experimental setup. Finally, in Sects.  5 \nand 6 we present the results of our experimental evaluation and summarise our con-\nclusions, respectively.\n2  Background\n2.1  Sentiment analysis and keyword extraction\nSentiment analysis is a NLP task aimed at determining the sentiment expressed in a \npiece of text. With the proliferation of user-generated content on digital platforms, \nsentiment analysis has emerged as a vital tool for businesses to understand and \nrespond to customer opinions, feedback, and emotions. By automatically classify -\ning text as positive, negative, or neutral, sentiment analysis enables organisations to \ngauge customer satisfaction, identify areas for improvement, and tailor their prod-\nucts or services to meet customer expectations. Traditional approaches to sentiment \nanalysis often rely on Machine Learning (ML) algorithms trained on labelled data-\nsets, where each text sample is annotated with its corresponding sentiment polarity. \nThese algorithms learn to classify new text samples based on patterns and features \nextracted from the training data. While effective, this supervised learning paradigm \nrequires substantial amounts of labelled training data and may struggle to general-\nise to new domains or languages where labelled data is scarce. For further explora-\ntion of the various approaches, we refer to Birjali et al. (2021) and Wankhade et al. \n(2022).\nKeyword extraction, on the other hand, involves the automatic identification of \nsignificant words or phrases that capture the essence of a document or text corpus. \nKeywords play a crucial role in summarising the main themes, topics, or sentiments \nexpressed within textual content, facilitating information retrieval, analysis, and \ninterpretation. Traditional keyword extraction methods often rely on statistical met-\nrics, such as term frequency-inverse document frequency (TF-IDF) or graph-based \nalgorithms like TextRank, to identify important keywords based on their frequency, \n230 D. Guidotti et al.\nimportance, or connectivity within a document  (Firoozeh et  al. 2020). However, \ntraditional keyword extraction methods may face challenges in handling noisy or \nunstructured text data, as well as in identifying contextually relevant keywords that \ncapture the nuances of the text. Moreover, the effectiveness of keyword extraction \napproaches may be limited by the availability of labelled training data or the need \nfor manual parameter tuning.\nIn recent years, advancements in deep learning and the availability of large-scale \npre-trained language models, have revolutionised the field of sentiment analysis and \nkeyword extraction.\n2.2  Large Language Models\nLarge Language Models stand at the forefront of contemporary natural language \nprocessing, embodying a substantial leap in the field’s capabilities. These models, \nrooted in transformer architectures, have emerged as revolutionary tools for com-\nprehending and generating human-like text across a diverse array of tasks and \ndomains (Abdullah and Ahmet 2023). Transformers, introduced by Vaswani et al. \n(2017), depart from earlier sequence-to-sequence models by dispensing with Recur -\nrent Neural Networks (RNNs) in favour of self-attention mechanisms. This mecha-\nnism allows the model to weigh the importance of each token in the input sequence, \nfacilitating the capture of long-range dependencies and contextual relationships \nwithin the text.\nOne of the defining features of LLMs is their extensive pre-training on a vast cor-\npora of text data. During pre-training, the model learns to predict missing words or \ntokens within text sequences, leveraging the wealth of linguistic patterns and struc-\ntures present in the data. This pre-training phase instils LLMs with a rich under -\nstanding of language semantics, syntax, and context, enabling them to generalise \nacross a wide spectrum of language tasks and domains. Following pre-training, \nLLMs can be fine-tuned on task-specific datasets to adapt to particular applications \nor domains. Fine-tuning involves updating the model’s parameters using labelled \ndata from the target task, thereby tailoring the model to excel in specific linguistic \ntasks such as sentiment analysis, text classification, or language generation. Further-\nmore, LLMs support transfer learning, where knowledge acquired during pre-train-\ning can be transferred to downstream tasks with minimal task-specific training data, \nenhancing their versatility and applicability.\nLLMs have garnered widespread acclaim for their scalability and performance, \nwith state-of-the-art models such as BERT (Devlin et al. 2019) and GPTs (Gen-\nerative Pre-trained Transformers) comprising billions of parameters (Wang et al. \n2022). These large-scale models exhibit remarkable capabilities in generating \ncoherent and contextually relevant text, surpassing previous benchmarks on a \nmyriad of language understanding and generation tasks. In the realm of sentiment \nanalysis and keyword extraction, LLMs offer unparalleled advantages. Their deep \nunderstanding of language semantics and context enables them to accurately infer \nsentiment polarity and identify relevant keywords within textual data. Moreover, \nthe zero-shot learning capabilities of LLMs allow them to perform these tasks \n231\nDiscovering sentiment insights: streamlining tourism review…\nwithout explicit training on labelled datasets, obviating the need for extensive \ntask-specific training. For a more detailed overview regarding LLMs we refer to \nRaiaan et al. (2024).\n2.3  Zero‑shot learning\nZero-shot learning stands as a transformative approach within the realm of machine \nlearning, revolutionising the traditional paradigm of supervised learning by enabling \nmodels to generalise to unseen classes or tasks without requiring explicit training on \nlabelled examples (Xian et al. 2019; Wang et al. 2019). Unlike conventional super -\nvised methods, which rely heavily on annotated data for each class or task of inter -\nest, ZSL leverages auxiliary information or semantic embedding to infer knowledge \nabout unseen classes based on their relationships to known classes. Initially con-\nceived within the domain of computer vision to address challenges in image clas-\nsification, ZSL has since expanded its applicability to various fields, including NLP. \nIn this area, the principles of ZSL have opened new avenues for models to perform \ntasks such as text classification, sentiment analysis, and language generation without \nthe need for labelled training data specific to each task.\nThe foundation of ZSL lies in its utilisation of semantic representations to bridge \nthe gap between known and unseen classes or tasks. These semantic embedding, \nwhich may include word embedding, language embedding, or ontological knowl-\nedge graphs, encode the semantic relationships between different classes or concepts \nwithin a continuous vector space (Wang et al. 2018). By leveraging these represen-\ntations, ZSL models can generalise across tasks or domains, transferring knowl-\nedge from known classes to unseen ones. A key advantage of ZSL is its reliance on \ntransfer learning principles, where knowledge acquired from pre-training on large-\nscale datasets or auxiliary tasks can be transferred to related tasks or domains. This \nenables ZSL models to acquire generalised knowledge about language semantics, \nsyntax, and context, which can then be adapted to perform specific tasks with mini-\nmal task-specific training data. Furthermore, ZSL often involves task decomposition \ntechniques, where complex tasks are broken down into simpler sub-tasks or compo-\nnents. By learning to perform these sub-tasks independently, models can generalise \nto unseen tasks by combining their knowledge of known sub-tasks in novel ways, \nthereby enhancing their adaptability and versatility. A particularly relevant task is \nNatural Language Inference (NLI), which involves determining whether the infor -\nmation presented in one text can be inferred from another text. This task can be uti-\nlised to develop models capable of categorising text into generic classes as specified \nby users, without the necessity for training or fine-tuning. Further details on NLI are \nprovided in the subsequent section.\nIn recent years, advancements in ZSL have led to the development of sophisti-\ncated models capable of zero-shot text classification, sentiment analysis, and key -\nword extraction. These models leverage semantic embedding and transfer learn-\ning techniques to generalise across tasks and domains, enabling them to perform \neffectively in scenarios where labelled training data is scarce or unavailable.\n232 D. Guidotti et al.\n2.4  Natural language inference\nNatural language inference, also known as Textual Entailment, is a central task \nin natural language processing that focuses on determining whether a given text, \nreferred to as the premise, logically entails another text, known as the hypothesis. \nThis task plays a crucial role in assessing a model’s ability to understand and reason \nabout language, which is fundamental for a wide range of NLP applications such \nas question answering, summarisation, and information retrieval. It is important \nto note that textual entailment differs from classical logical entailment, as it has a \nmore relaxed definition: a text t entails an hypothesis h if a human reading t would \ninfer that h is most likely true (Dagan et al. 2005). In traditional NLI tasks, the rela-\ntionship between the premise and the hypothesis is classified into three categories: \nentailment, contradiction, and neutral. If the hypothesis logically follows from the \npremise, it is classified as entailment. If the hypothesis is inconsistent with the prem-\nise, it is classified as contradiction. If there is no significant relationship between \nthe premise and the hypothesis, it is classified as neutral. In this work, we focus on \nmodels that leverage NLI for zero-shot text classification. These models treat the \nclasses as hypotheses: if the model determines that the text entails a hypothesis, the \ntext is classified under that category. Conversely, if the model finds a contradiction \nor neutrality between the text and the hypothesis, it indicates that the text does not \nbelong to that category. Despite significant progress, NLI remains challenging due \nto the inherent ambiguity and vagueness of natural language. Many entailment deci-\nsions require commonsense knowledge or external world information, which cur -\nrent models may lack. Multi-hop reasoning, where a hypothesis requires integrating \ninformation from multiple premises, adds another layer of complexity to the task. \nHowever the introduction of transformer models marked a significant breakthrough, \nsetting new benchmarks in NLI by leveraging pre-training on large corpora followed \nby fine-tuning on NLI datasets.\n3  Related work\nIn the current state-of-the-art, sentiment analysis classification approaches are typi-\ncally divided into lexicon-based and ML-based methods.\nLexicon-based approaches utilise sentiment lexicons, which consist of opinion \nwords and phrases annotated with positive or negative scores, to determine the senti-\nment polarity of terms in text (Bagherzadeh et al. 2021; Bucur 2015; Gräbner et al. \n2012; Hnin et al. 2018). Their primary advantage lies in their independence from \nlabelled data. However, they rely heavily on linguistic resources and struggle to \naccount for context. In this study, we do not further focus on this kind of methodolo-\ngies due to the scope of our current investigation. For a more thorough examination \nof these approaches, we recommend referring to Ameur et al. (2024) for additional \ninsights and details.\nML-based methods do not necessitate a predefined dictionary and demonstrate \ngreater proficiency in handling ambiguities and adapting to different domains. \nHowever, training such models is often time-consuming and typically relies on \n233\nDiscovering sentiment insights: streamlining tourism review…\nlabelled data. These approaches can be further categorised into shallow learning \nmethods and deep learning methods.\nSome of the main shallow learning methods used in sentiment analysis are \nNaive Bayes (NB) and Support Vector Machines (SVMs). NB-supervised clas-\nsification utilises Bayes’ rule to assign the most probable class to a certain text. \nNotably, NB exhibits acceptable precision and low computational cost, as noted \nby Martins et al. (2017), and is robust against noise and overfitting, making it a \npopular choice for sentiment classification in hotel reviews  (Farisi et  al. 2019; \nGhorpade and Ragha 2012; Martins et  al. 2017). SVMs, unlike probabilistic \nclassifiers, work by finding the optimal hyperplane that best separates different \nclasses of data. Such hyperplane is positioned to maximise the margin between \nthe closest data points of each class, resulting in a robust and efficient classifica-\ntion model. Studies suggest that SVM outperforms other shallow ML algorithms \nin accuracy for sentiment analysis (Shi and Li 2011).\nDeep learning methodologies involve complex neural network architectures \nwith multiple hidden layers, enabling them to autonomously learn and refine their \nown representations of the data. The primary deep learning models employed \nin sentiment analysis tasks comprise Convolutional Neural Networks (CNNs), \nRecurrent Neural Networks (RNNs), and transformer language models. Jiang \net al. (2019) applied CNN layers to identify local features for aspect sentiment \nclassification in Chinese hotel reviews, while de  Souza et  al. (2018) employed \nCNNs for classifying Brazilian hotel reviews, yielding promising results. How -\never, while CNNs are proficient in analysing short sentences, RNNs may be \nmore suitable for longer ones  (de  Souza et  al. 2018). Long Short-Term Mem-\nory (LSTM), a popular RNN architecture, maintains information in memory for \nextended periods, offering exceptional performance in sentiment analysis  (Pal \net al. 2018). Priyantina and Sarno (2019) combined word embedding with LSTM \nfor sentiment classification of reviews, achieving high accuracy for aspect-based \nsentiment analysis. Despite LSTM architectures’ capability to capture contex-\ntual information, they struggle with identifying crucial corpus parts (Jiang et al. \n2019). Younas et al. (2020) examined the performance of two transformer mod-\nels, multilingual BERT (mBERT) and XLM-RoBERTa (XLM-R), for senti-\nment analysis of multilingual social media text. Tan et al. (2022) introduced an \nadvanced sentiment analysis approach by merging the strengths of two prominent \nmodels: the robustly optimised BERT approach (RoBERTa) and LSTM. Finally, \nTesfagergish et al. (2022) proposed a two-stage emotion detection methodology \nfor sentiment analysis leveraging zero-shot learning models and trained ensem-\nbles. Evaluation across three benchmark datasets yielded the best accuracy of \n87% for binary sentiment classification and 63% for the three-class sentiment \nclassification.\nWhile our methodology shares some similarities with this last work, it is \nimportant to highlight that Tesfagergish et al. (2022) trains the ensembles from \nscratch, whereas we directly employ zero-shot learning models without requiring \nfine-tuning or specifically trained models. For a more detailed survey of senti-\nment analysis methodologies we refer to Ameur et al. (2024).\n234 D. Guidotti et al.\n4  Materials and methods\nIn the following, we introduce the models and datasets examined in our experimen-\ntal evaluation, offering insight into the rationale behind our selections. Additionally, \nwe focus on the proposed methodologies for sentiment analysis and keyword extrac-\ntion tasks with greater details.\n4.1  Models\nIn our experimental evaluation, we used a selection of state-of-the-art NLP zero-\nshot classifiers, readily available from the Hugging Face model hub, 1 a well-known \nplatform for NLP model deployment and experimentation. These models were cho-\nsen for their accessibility and their capabilities specifically tailored for zero-shot \nclassification tasks, which allow them to generalise effectively to unseen classes or \ncategories without the need for task-specific training data. Moreover, we prioritise \nmodels sourced from reputable origins, preferably with accompanying references, \nand not tailored for languages other than English. In addition to the zero-shot mod-\nels, we also considered a selection of four State-Of-The-Art (SOTA) models opti-\nmised for sentiment analysis. This approach aims to elucidate the trade-off between \nthe accuracy of models specifically trained for sentiment analysis and the versatility \nof the zero-shot classifiers, which can be used for both sentiment analysis and key -\nword extraction tasks. For clarity, we refer to the zero-shot classifiers as M1-M9 and \nto the SOTA models as S1-S4. It should be noted that S4 is fine-tuned to recognise \nonly positive and negative sentiment polarity, whereas all the other models can be \nused to also recognize the neutral polarity.\nThe models shown in Table  1 offer varying degrees of complexity, from large-\nscale architectures capable of handling extensive text data to smaller, more light-\nweight models suitable for less resource-intensive tasks. As can be seen, many of \nthem originate from the same base models: this is due to the limited availability \nof open source LLMs which consequently constrains the variety of their derived \nmodels. However, it should be noted that the training process is non-deterministic—\nmeaning, two models trained on the same datasets may exhibit different behaviours. \nAdditionally, smaller models are not necessarily inferior to larger ones across vari-\nous tasks of interest. Specifically, when evaluating a single review, the response of \nthe most complex model may not always be more reliable than that of the small-\nest model. Furthermore, it is essential to consider that larger models require corre-\nspondingly higher computational resources for inference execution. For these latter \nreasons, we have chosen to include all the models presented, even those belonging \nto the same “family” and that present decreasing levels of complexity. In particular, \nwe considered models originating from four different LLMs:\n1 https:// huggi ngface. co.\n235\nDiscovering sentiment insights: streamlining tourism review…\nTable 1  Models considered in our experimental evaluation\nColumn Mod ID represent the identifiers used for the models. Column Description provides a brief \ndescription of the corresponding models\nahttps:// huggi ngface. co/ Morit zLaur er/ deber ta- v3- large- zeros hot- v1\nbhttps:// huggi ngface. co/ Morit zLaur er/ deber ta- v3- base- zeros hot- v1\nchttps:// huggi ngface. co/ Morit zLaur er/ DeBER Ta- v3- xsmall- mnli- fever- anli- ling- binary\ndhttps:// huggi ngface. co/ faceb ook/ bart- large- mnli\nehttps:// huggi ngface. co/ cross- encod er/ nli- MiniL M2- L6- H768\nf https:// huggi ngface. co/ cross- encod er/ nli- deber ta- v3- large\nghttps:// huggi ngface. co/ cross- encod er/ nli- deber ta- v3- xsmall\nhhttps:// huggi ngface. co/ cross- encod er/ nli- deber ta- v3- small\ni https:// huggi ngface. co/ cross- encod er/ nli- deber ta- v3- base\nj https:// huggi ngface. co/ cardi ffnlp/ twitt er- rober ta- base- senti ment- latest\nkhttps:// huggi ngface. co/ cardi ffnlp/ twitt er- xlm- rober ta- base- senti ment\nl https:// huggi ngface. co/ finit eauto mata/ bertw eet- base- senti ment- analy sis\nmhttps:// huggi ngface. co/ siebe rt/ senti ment- rober ta- large- engli sh\nMod ID Description\nM1 a A model by Laurer et al. (2023) based on the large version of the DeBERTa model (He et al. \n2021) and fine-tuned on several general classification dataset\nM2  b A model based on the base version of the DeBERTa model and fine-tuned on several general \nclassification dataset\nM3 c A model based on the extra small version of the DeBERTa model and fine-tuned on several \ngeneral classification dataset\nM4 d A model based on the large version of the BART model (Lewis et al. 2020) and fine-tuned on \nthe MNLI dataset (Williams et al. 2018)\nM5 e A smaller model based on MiniLM2 (Wang et al. 2021) and trained on the SNLI (Bowman \net al. 2015) and MNLI datasets\nM6 f A model based on the large version of the DeBERTa model and trained on the SNLI and MNLI \ndatasets\nM7 g A model based on the extra small version of the DeBERTa model and trained on the SNLI and \nMNLI datasets\nM8 h A model based on the small version of the DeBERTa model and trained on the SNLI and \nMNLI datasets\nM9 i A model based on the base version of the DeBERTa model and trained on the SNLI and MNLI \ndatasets\nS1  j A model (Loureiro et al. 2022) based on the base version of the RoBERTa (Liu et al. 2019) \nmodel and fine-tuned for sentiment analysis with the TweetEval benchmark (Barbieri et al. \n2020)\nS2 k A multilingual model (Barbieri et al. 2021) based on the base version of the RoBERTa model \nand fine-tuned for sentiment analysis\nS3 l A model based on a specialised version (Nguyen et al. 2020) of the RoBERTa model and fine-\ntuned on the SemEval-2017 dataset (Rosenthal et al. 2017)\nS4 m A model (Hartmann et al. 2023) based on the large version of the RoBERTa model and fine-\ntuned for sentiment analysis on 15 datasets\n236 D. Guidotti et al.\n• BART  (Lewis et al. 2020) is designed as a versatile sequence-to-sequence model \ntailored for tasks such as text generation, summarisation, translation, and ques-\ntion answering. It combines features from both bidirectional transformers (like \nBERT) and autoregressive transformers (like GPT), allowing it to excel in a wide \nrange of NLP applications.\n• DeBERTa (He et al. 2021) builds upon BERT by introducing a novel disentan-\ngled attention mechanism, where each word is represented by one vector for con-\ntent and one for position. This enhances the model’s ability to capture intricate \nrelationships between words within a sentence. These advancements contribute \nto DeBERTa achieving state-of-the-art performance across various NLP bench-\nmarks.\n• MiniLM2 (Wang et al. 2021) is engineered to be a more compact and efficient \nalternative to larger language models like BERT, while still delivering competi-\ntive performance. It achieves this through a process of knowledge distillation \nfrom larger models, which allows it to retain much of the performance benefits \nwhile reducing computational demands. Despite its smaller size, MiniLM2 dem-\nonstrates strong performance across a range of NLP tasks, making it suitable for \ndeployment in resource-constrained environments.\n• RoBERTa (Liu et al. 2019) builds upon BERT (Devlin et al. 2019) by optimising \nits pre-training procedure. This includes training on more data, longer sequences, \nand removing the next sentence prediction task. These enhancements lead to \nimproved language understanding capabilities and robust performance across \nmultiple NLP benchmarks.\nBy leveraging this diverse set of pre-trained NLP models, we aimed to comprehen-\nsively evaluate their performance in the context of sentiment analysis and keyword \nextraction tasks within tourism domain reviews. This approach enabled us to assess \nthe effectiveness and robustness of zero-shot learning techniques across a range of \nmodel architectures, providing valuable insights into their suitability for real-world \napplications in the tourism industry. Furthermore, by considering together the \nresponses of the separate models as an ensemble, we are able to gather more insight \nboth on their performances and on the reviews considered.\n4.2  Datasets\nIn our experimental evaluation, we selected diverse datasets to ensure a comprehen-\nsive assessment of the proposed methodology across different contexts. The datasets \nchosen for analysis include: \n1. European Castles Dataset.2 This dataset comprises 2550 Google reviews encom-\npassing 529 European castles. These reviews provide useful insights into tour -\nists’ experiences and perceptions of historical landmarks, allowing us to explore \n2 https:// www. kaggle. com/ datas ets/ datas cienc edonut/ europ ean- castl es.\n237\nDiscovering sentiment insights: streamlining tourism review…\nsentiment analysis and keyword extraction within the realm of cultural heritage \ntourism. We will use the identifier Castles to refer to this dataset from now on.\n2. Google Maps Restaurant Reviews Dataset.3 With 1100 Google reviews span-\nning 100 different restaurants, this dataset provides a rich source of information \non dining experiences and culinary preferences. Analysing sentiment and extract-\ning keywords from restaurant reviews enables us to focus on the gastronomic and \nculinary tourism domains. We will use the identifier Restaurants to refer to this \ndataset.\n3. Hotel Reviews Dataset.4 Consisting of 10000 Google reviews pertaining to 1000 \nhotels across the United States, this dataset offers an overview of accommodation \nexperiences in the hospitality sector. By examining sentiment and identifying \nkeywords within hotel reviews, we collect insights into travellers’ preferences, \nsatisfaction levels, and expectations within the lodging industry. We will use the \nidentifier Hotels to refer to this dataset.\nIn Fig. 1, we provide a concise summary of the distribution of reviews within each \ndataset across different rating scores. It is evident that the datasets we examined \ndisplay relevant imbalances concerning the quantity of positive (greater than three \nstars), neutral (three stars), and negative (less than three stars) reviews. Recognising \nthese imbalances is crucial for accurately interpreting the findings derived from our \nexperimental evaluation. Furthermore, it is important to note that such disparities \nare typical in datasets encompassing reviews even across different domains.\nIn Fig.  2, we present a graphical representation of the distribution of reviews \nbased on their text length. The distributions of the datasets are quite similar, yet \nthey are centred around different lengths. In particular, the Restaurants dataset pre-\ndominantly features shorter reviews, whereas the Castles and Hotels datasets tend \nto contain significantly longer reviews. This observation aligns with the nature of \nthe respective domains: there is generally more to discuss regarding a hotel stay or a \nvisit to a cultural heritage site than a single dining experience.\nFig. 1  Histograms of the distributions of reviews for each possible score and dataset\n3 https:// www. kaggle. com/ datas ets/ deniz bilgi nn/ google- maps- resta urant- revie ws.\n4 https:// www. kaggle. com/ datas ets/ datafi  niti/ hotel- revie ws.\n238 D. Guidotti et al.\nBy leveraging these diverse datasets, we aim to evaluate the effectiveness and \nrobustness of the proposed methodology across different tourism domains. The \ninclusion of datasets spanning castles, restaurants, and hotels allows for a holis-\ntic assessment of sentiment analysis and keyword extraction techniques, providing \nvaluable insights applicable to a wide range of tourism-related businesses and des-\ntinations. Moreover, it is noteworthy that the language and communication styles \nemployed in reviews may exhibit substantial variation across these datasets, thereby \naugmenting the reliability and applicability of our findings to diverse contexts. \nFinally, the significant variation in review lengths further contributes to the breadth \nof our experimental evaluation.\nWhile our focus on dataset of reviews, which typically include a related number \nof stars, may make sentiment analysis appear superfluous, it remains relevant. The \nreliability of scores is generally lower than the sentiment expressed in the textual \nportion of the reviews due to potential errors or misunderstandings by the writer. \nTherefore, sentiment analysis is essential for accurately measuring the true senti-\nment conveyed in the text. Furthermore, while we did not consider at this time data-\nset originating from social networks, we expect the results obtained in this work to \ndirectly apply also to social media texts related to particular tourist attractions. Note \nthat the datasets used for this work do not include ground truth annotations spe-\ncifically tailored for keyword extraction tasks. Currently, there is a notable absence \nof datasets designed specifically for keyword extraction within the tourism domain. \nExisting datasets typically originate from specialised fields [e.g., extracting key -\nwords from scientific literature (Kim et al. 2010; Augenstein et al. 2017)] and are \nlimited in terms of manually annotated texts. This lack of domain-specific datasets \nunderscores the rationale behind our approach of leveraging LLMs for automated \nkeyword extraction. Establishing the reliability of this methodology could facilitate \nautomatic annotation for future datasets in tourism research.\n4.3  Methodology\nIn this subsection, we offer a comprehensive overview of the methodology proposed \nin our study for conducting sentiment analysis and keyword extraction tasks within \nFig. 2  Histograms of the distributions of reviews with respect to their length for each dataset. Each bin \nreports the number of reviews with length less or equal to the corresponding tick. For the sake of clarity, \nthe last bin also includes reviews with length greater than the corresponding bin\n239\nDiscovering sentiment insights: streamlining tourism review…\ntourism reviews. For sentiment analysis, we classify reviews as positive, neutral, or \nnegative based on their provided scores, aiming to assess the accuracy of NLP mod-\nels in discerning sentiment. In keyword extraction, we preselect five keywords of \ninterest per dataset and employ NLP models to determine the probabilities of each \nkeyword to be relevant for each review. We then evaluate the results using statistical \nanalysis.\nIn Fig. 3 we present the workflow diagram of our experimental evaluation. It pro-\nvides an overview of the experimental framework applied to the three distinct data-\nsets. It should be noted that the inputs of our models consist of reviews expressed \nas raw texts, without other specific prompts. In our analysis, we incorporate two \ncategories of models: nine zero-shot models and four SOTA models. For sentiment \nanalysis, we employ all models, both zero-shot and SOTA, to assess the sentiment \nconveyed in the textual data across each dataset. In contrast, keyword extraction \nused only the nine zero-shot models to identify and extract significant terms from \nthe textual content within each dataset.\n4.3.1  Sentiment analysis\nIn our study, the sentiment analysis task revolves around the classification of reviews \nbased on their sentiment polarity. To achieve this, we employ the transformer pipe-\nline,5 which is a user-friendly library for inference and fine-tuning using transform-\ners models provided by Hugging Face. Utilising Python scripts, we interface with \nthe transformer pipeline to apply the models of interest to every review within the \ndatasets. For the NLI based models (M1-M9), we provide them with positive, nega-\ntive, or neutral as three mutually exclusive hypothesis. Therefore the actual output of \nthese models consists in three score between 0 and 1 corresponding to the likelihood \nFig. 3  Workflow diagram illustrating the experimental setup for sentiment analysis and keyword extrac-\ntion across Castles, Restaurants, and Hotels datasets using nine zero-shot and four state-of-the-art models\n5 https:// huggi ngface. co/ docs/ trans forme rs/ index.\n240 D. Guidotti et al.\nthat the input text entails the specific hypothesis. As the classes are mutually exclu-\nsive, the sum of the three scores is equal to 1. To further clarify, for all the NLI mod-\nels considered, the prompt consisted of the text of the review under consideration \nand a list of possible hypotheses. The model’s output was a set of mutually exclusive \nlikelihoods p i ∈[ 0, 1] ⊂ ℝ , such that ∑\ni p i = 1 , representing the probabilities that \nthe review corresponded to each specific class. For a visual representation of the \nrelationship between the prompt, the model, and the corresponding output, please \nrefer to Fig. 4.\nThe SOTA models (S1–S4) are fine-tuned for sentiment analysis, therefore their \noutput already consists by default in the likelihood that the input text could be asso-\nciated to the sentiment label of interest. However, S4 supports only the positive and \nnegative classes, and therefore has been excluded from the evaluation when the \nneutral sentiment is considered. The response of the model is then determined as \nthe sentiment label (positive, neutral, or negative) associated with the highest likeli-\nhood. Then this response is compared with the ground truth to provide an estimation \nof the accuracy of the models on the task of interest. After this initial experimental \nevaluation, we opted to expand our analysis to include scenarios where the reviews \npresenting a score of three stars, corresponding to a neutral polarity, were removed \nfrom the dataset.\nIn addition to assessing the accuracy of the models, we compute standard NLP \nmeasurements of Precision, Recall, and F Score. Precision refers to the propor -\ntion of correctly classified reviews out of all reviews classified as a certain class by \nthe model. Recall, on the other hand, represents the proportion of correctly identi-\nfied reviews out of all the reviews actually belonging to the class of interest in the \ndataset. Finally, F Score is the harmonic mean of Precision and Recall, providing a \nbalanced measure of the model’s performance. We compute these metrics to gain a \nFig. 4  Illustration of the NLI model’s input-output process. The prompt consists of the review text and \na set of possible hypotheses. The model processes this input to produce likelihoods for each hypothesis, \nwhich are mutually exclusive and sum to 1\n241\nDiscovering sentiment insights: streamlining tourism review…\nmore comprehensive understanding of the models’ effectiveness in sentiment anal-\nysis. While accuracy provides a general overview, Precision, Recall, and F Score \noffer more insights into the models’ ability to correctly identify the sentiment polar -\nity of the reviews of interest. Similar to accuracy, these three metrics range between \n0 and 1, where a higher value indicates better performance.\nAs our ground truth for sentiment classification, we adopt a straightforward crite-\nrion: a review is considered positive if its score is greater than 4 stars, neutral if its \nscore is 3 stars, and negative otherwise.\n4.3.2  Keywords extraction\nThe first step in our methodology for identifying relevant keywords in specific \nreviews is to define a set of target keywords. This approach is necessary because \nusing LLMs directly for keyword extraction may lead to the detection of uninforma-\ntive terms (e.g., “food” for restaurant reviews, “room” for hotel reviews, etc.). In our \nexperiments, the target keywords were selected through a systematic process involv-\ning collaboration with domain experts in the tourism sector as part of a technol-\nogy transfer initiative aimed at supporting SMEs. Initial steps involved conducting \ntargeted surveys among a representative sample of 50 SMEs from the tourism and \ncultural heritage sectors. These surveys were designed to gather insights into the \nkeywords that SMEs themselves identified as most relevant and impactful for their \ndomains. To ensure a comprehensive experimental evaluation, two different sets \nof keywords were selected: the first set was chosen without giving any aid to the \nexperts and the SMEs, whereas for the second set they were aided with the word \nclouds presented in Fig.  5, thus incorporating a data-driven perspective. This dual \napproach balances expert intuition with empirical evidence. It is important to note \nthat although the word clouds were presented to the SMEs, they were not restricted \nto choosing only the keywords contained within them. For instance, they selected \nthe term “price” for the Castles dataset upon observing that “ticket” was present in \nthe corresponding word cloud, as they deemed it to be more informative within the \nsame thematic context.\nTable 2 presents the sets of keywords chosen for each dataset: SET1 denotes the \nkeywords selected by domain experts without additional aid, while SET2 represents \nthose chosen with the assistance of word clouds presented in Fig.  5. It is important \nto note that the algorithm generating the word cloud already eliminates less informa-\ntive terms such as articles and pronouns. As can be seen, relying solely on NLP for \ndirect keyword selection may lead to sub-optimal choices: the word clouds might \nsuggest “Hotel”, “Room”, and “Thank” as prominent keywords for the Hotel Data-\nset; “Good”, “Place”, and “Taste” for the Restaurant Dataset; “Castle”, “Place”, \n“Inside” for the Castle Dataset. Clearly, these keywords do not provide useful infor-\nmation to the end user. As anticipated, there is a degree of overlap between the two \nsets of keywords chosen for each dataset.\nOnce the set of keywords of interest is identified, the models are employed to assess \nevery review in our datasets, determining the likelihood of each keyword in the sets \nbeing associated with each review. To accomplish this, we provide to each model the \nselected keywords as not mutually exclusive hypotheses. Therefore their outputs consist \n242 D. Guidotti et al.\nTable 2  Keyword Sets \nconsidered in our experimental \nevaluation\nColumn Dataset represent the dataset of interest. Column Key Set \nreports the identifiers of the keyword sets. Note that SET1 denotes \nthe keywords selected by domain experts without additional aid, \nwhile SET2 represents those chosen with the assistance of word \nclouds. Finally, Column Keywords reports the keywords correspond-\ning to each set\nDataset Key set Keywords\nCastles SET 1 Tour, History, Panorama, Staff, Food\nSET 2 Tour, History, View, Garden, Price\nRestaurants SET 1 Price, Staff, Clean, Comfortable, Location\nSET 2 Price, Service, Pizza, Meat, Breakfast\nHotels SET 1 Price, Staff, Location, Food, Ambience\nSET 2 Price, Staff, Location, Breakfast, Clean\nFig. 5  Word Clouds generated \nfor the Datasets of interest. \nLarger font sizes correspond to \nhigher frequencies of the respec-\ntive words in the dataset. Differ-\nent font colours are utilised for \nreadability purposes, with no \nadditional information conveyed\n\n243\nDiscovering sentiment insights: streamlining tourism review…\nin the scores, between 0 and 1, corresponding to the likelihood that the input text entails \neach specific hypothesis, and more than one hypothesis, or even none, can be entailed \nat the same time. The prompt schema is the same as the one in Fig. 4 with the only dif-\nference that the hypotheses of interest are our selected keywords and the likelihoods are \nnot mutually exclusive.\nIt is important to highlight that, unlike the sentiment analysis task, we lack a ground \ntruth in this scenario. Therefore, once we obtain the probabilities provided by each \nmodel for each review and keyword, we conduct statistical analysis to analyse the dis-\ntribution of these probabilities. Specifically, we compute the median and interquartile \n(IQ) range of the probabilities provided by the models for each keyword in each review, \nand we analyse the distribution of these measurements over the whole dataset. Consid-\nering two sets of keywords also allows us to gather more insight in the behaviour of the \nmodels. This analysis enables us to gain insights into the consistency and variability \nof model responses, providing a comprehensive understanding of the models’ perfor-\nmance in keyword extraction. Additionally, we analyse the distribution of probabili-\nties for each keyword considering selected reviews to better understand the differences \nbetween the responses of various models. This targeted analysis allows us to identify \npotential patterns or discrepancies in model behaviour and performance.\n5  Results and discussion\nIn this section, we present and discuss the outcomes of our experimental assessment \nfor sentiment analysis and keyword extraction. Furthermore, we offer hypotheses \nand considerations concerning our results, exploring specific reviews for thorough \nanalysis.\n5.1  Sentiment analysis\nIn this sub-section we present our results considering three potential polarities for \nthe reviews: positive, neutral, and negative. As depicted in Table  3, all the mod-\nels under consideration struggle to accurately identify reviews with a neutral polar -\nity, with the best case being model M2 with the Castles dataset reaching 34.5% of \naccuracy. Conversely, they demonstrate relative proficiency in identifying positive \nreviews, albeit slightly less so in the case of negative reviews. In particular, in the \ncase of the Castle dataset, the models exhibit greater success in identifying nega-\ntive reviews compared to positive ones, contrasting with results from other datasets. \nRegarding the performance of the various models, no clear hierarchy in accuracy \nemerges, except for models M6 and M8 and S2, which appear to exhibit lower aver -\nage accuracy compared to others. It is also interesting to note that the SOTA mod-\nels do not seem to be significantly more accurate than the zero-shot models, even \nif their performances seem more consistent across different datasets. S4 has not be \nconsidered in this first evaluation as it does not support the neutral polarity.\nFurther insight can be obtained by analysing Table  4. Indeed, the models appear \nto be proficient in recognising positive polarity, as evidenced by the presented \n244 D. Guidotti et al.\nPrecision. The frequency with which they misclassified neutral or negative reviews \nas positive ones seems negligible compared to the number of correctly classified \npositive reviews. Concerning neutral polarity, the results from Table  3 are corrobo-\nrated. As indicated by the Recall, the models struggle to identify neutral reviews, \nand furthermore, they frequently misclassified positive and negative reviews as neu-\ntral. This challenge may arise from the inherent difficulty in detecting true neutral \nsentiment, as human users seldom express sentiments that are entirely neutral. Even \nreviews that may appear neutral to a human reader often contain a mix of positive \nand negative aspects regarding the location of interest, making it challenging for \nLLMs to interpret them as truly neutral.\nFinally, the findings regarding negative polarity are particularly noteworthy. \nDespite the high Recall, suggesting proficiency in recognising negative reviews, the \nlimited Precision implies that the models often misclassified positive and neutral \nreviews as negative. This phenomenon is especially pronounced in the Castles data-\nset, previously noted for its anomaly wherein models found it easier to recognise \nnegative reviews than positive ones. This suggests a tendency among models to clas-\nsify positive reviews originating from this dataset as negative. However, it is impor-\ntant to note that the low Precision could also be partially attributed to the higher \nimbalance between negative and positive reviews in the Castles dataset, potentially \nleading to a higher number of false negatives due to the greater number of positive \nreviews. It is noteworthy to notice that most of the SOTA models appears to present \nslightly higher Precision than the zero-shot counterparts when considering the nega-\ntive polarity in all the dataset of interest.\nBased on the presented results, we opted to deepen our experimental evalua-\ntion by excluding neutral reviews from the datasets of interest, focusing solely on \nrecognising positive and negative polarities. As we would expect, Table  5 reveals \nan observable increase in average accuracy, attributable to the simplified task of \nTable 3  Results of the sentiment analysis task when considering reviews with a score of three stars as \nneutral\nColumn Mod ID is the same of Table  1. Columns Acc, Pos, Neut, and Neg represent the accuracy com-\nputed on all the reviews, on only the positive ones, on only the neutral ones, and only the negative ones \nrespectively, for the three datasets of interest Castles, Restaurants, and Hotels. The table is best inter -\npreted row by row to identify the most effective models. Models with overall better performance are indi-\ncated by a higher concentration of green cells, while those with poorer performance have more red cells. \nFor example, notice that models M6 and M8 have a higher quantity of red across their rows, suggesting \nthey perform worse compared to other models\n245\nDiscovering sentiment insights: streamlining tourism review…\nidentifying only two possible polarities. Moreover, these results generally align with \nour conjectures regarding the models under scrutiny: although on the Castles and \nHotels datasets, models M1 and M2 appear to outperform others. Furthermore, as \nwe would expect due to its fine-tuning focused on distinguishing between the posi-\ntive and negative polarities, S4 outperforms all the other models when we consider \nthe overall accuracy. However, it is interesting to notice that when the accuracy is \ncomputed only over the negative reviews, M1 and M2 still present slightly better \nperformances.\nTable 6 provides further validation of the findings presented in Table  4, indicat-\ning that the models demonstrate more reliable recognition of positive sentiment. \nTable 4  Results of the sentiment analysis task when considering reviews with a score of three stars as \nneutral\nColumns P, R and F1 represent the Precision, Recall and F Score measurements respectively. Column \nSent represent the sentiment considered to compute the measurements. For the other Columns refer to \nTable 3\n246 D. Guidotti et al.\nThe observed increase in Precision, particularly evident in the Restaurants and \nHotels datasets compared to Table 4, suggests that a considerable number of neutral \nreviews may have been previously misclassified as negative. The relatively modest \nincrease in Precision for the Castles dataset could further corroborate the hypoth-\nesis that the greater imbalance between positive and negative reviews may, to some \nextent, contribute to the lower Precision observed. S4 still best performances overall \neven if, interestingly, S1 and S3 present significantly higher Precision when applied \nto the Castles dataset.\nOverall, our experimental evaluation indicates that, particularly when focusing on \nrecognising positive and negative polarity, the models exhibit sufficient reliability \nfor the task at hand. However, the pronounced imbalance between the number of \npositive and negative reviews characteristic of such datasets renders it challenging \nto accurately evaluate the Precision and, consequently, the F Score of the models \nfor both positive and negative polarities. Concerning the performance differences \nbetween the zero-shot and SOTA models, the latter appears to perform slightly bet-\nter than the former. However, when evaluating the entire dataset, this difference does \nnot seem to be substantial, with the exception of model S4. The specialisation of \nS4 in distinguishing positive and negative polarity allows it to excel when neutral \nreviews are excluded.\n5.2  Keywords extraction\nIn this sub-section we present our results considering the keyword extraction task. \nIn Figs. 6, 7, 8, 9, 10, and 11, we depict the medians and IQ ranges computed across \nthe responses of our models of interest for the entire datasets. These histograms \nTable 5  Results of the sentiment analysis task when removing reviews with a score of three stars from \nthe datasets\nThe Columns are the same presented in Table  3. The table should be read row by row to identify the \nbest models. Green cells indicate better performance, while red cells indicate poorer performance. Mod-\nels like M1 and M2 show higher concentrations of green cells, suggesting overall better performance, \nwhereas models like M6 show lower performance, highlighted by a higher presence of red cells.\n247\nDiscovering sentiment insights: streamlining tourism review…\nillustrate the distribution of samples presenting median and IQ ranges within cer -\ntain value intervals for each specific keyword. The objective is to observe whether \nthe models exhibit unanimity and confidence in their selection of the keyword, as \nindicated by polarised medians towards 0 (indicating a mismatch) or 1 (indicat-\ning a match). Conversely, the spread of the likelihood predicted by the models, as \nreflected by the IQ range, should primarily tend towards zero, as higher IQ ranges \nsuggest strong disagreement among the models regarding the likelihood. As can be \nseen, the results are strictly dependant on the particular keyword and dataset con-\nsidered, and they do not seem to always indicate high confidence in the prediction \nprovided by the models considered. The most significant example of this behaviour \nseems to be “tour” for the Castle Dataset which seems to present an almost uniform \ndistribution for the medians and a nearly Gaussian distribution centred between 0.4 \nand 0.5 for the IQ ranges.\nFor most of the other keywords and datasets, the results align with our expecta-\ntions, with some noteworthy exceptions. Particularly, we observe that the keyword \n“view” in the Castle Dataset, chosen with the assistance of the related word cloud, \nexhibits a distribution of IQ ranges that is less skewed than desired. However, it is \nintriguing to note that, compared to the “panorama” keyword, we observe a signifi-\ncantly higher number of samples with very high medians. This indicates that while \nthe “panorama” keyword was infrequently matched to reviews with a high degree of \nTable 6  Results of the sentiment analysis task when removing reviews with a score of three stars from \nthe datasets\nThe Columns are the same presented in Table 4.\n248 D. Guidotti et al.\ncertainty, the “view” keyword is matched to more samples but with a lower degree \nof certainty. It is important to emphasise that while we seek high certainty in our \npredictions, a high degree of certainty regarding a keyword that rarely appears in the \nreviews of interest may not provide much useful information. Contrarily, a keyword \nFig. 6  Histograms illustrating the sample distribution and the median and interquartile range of prob-\nabilities calculated by the models for the Castles Dataset across the different keywords of SET0\n249\nDiscovering sentiment insights: streamlining tourism review…\nthat is identified less reliably but more frequently may still offer valuable insights to \nthe end user.\nRegarding the difference in results obtained for the keywords of SET1 and those \nof SET2, it does not appear that the models of interest consistently provide more \nreliable or relevant results for keywords selected with the assistance of the word \nFig. 7  Histograms illustrating the sample distribution and the median and interquartile range of prob-\nabilities calculated by the models for the Castles Dataset across the different keywords of SET1\n250 D. Guidotti et al.\nclouds. However, specific cases demonstrate instances where the keywords chosen \nwith the support of the word cloud yield significantly better results than the original \nones. Two such examples are the keywords “breakfast” and “clean” in the Hotels \nDataset. As depicted in Fig.  10 and 11, both of these keywords exhibit distributions \nof medians that are more polarised towards 0 and 1, as well as distributions of IQ \nFig. 8  Histograms illustrating the sample distribution and the median and interquartile range of prob-\nabilities calculated by the models for the Restaurants Dataset across the different keywords of SET0\n251\nDiscovering sentiment insights: streamlining tourism review…\nranges that are more skewed towards 0 compared to the original keywords “food” \nand “ambience”.\nWe conclude our experimental evaluation with some considerations regard-\ning the behaviour of the models of interest when applied to specific reviews. \nFig. 9  Histograms illustrating the sample distribution and the median and interquartile range of prob-\nabilities calculated by the models for the Restaurants Dataset across the different keywords of SET1\n252 D. Guidotti et al.\nSpecifically, we focus briefly on the first and third reviews of the Castle Dataset, \nwhich we believe can provide valuable insights. As depicted in Fig.  12, the mod-\nels exhibit varying degrees of agreement regarding which keywords should be \nFig. 10  Histograms illustrating the sample distribution and the median and interquartile range of prob-\nabilities calculated by the models for the Hotels Dataset across the different keywords of SET0\n253\nDiscovering sentiment insights: streamlining tourism review…\nmatched to the first review, whereas they appear to be more confident in pairing \nthe third review with the keywords “staff” and “food”. To understand the ration-\nale behind these results, we need to examine the reviews in question more closely:\nFig. 11  Histograms illustrating the sample distribution and the median and interquartile range of prob-\nabilities calculated by the models for the Hotels Dataset across the different keywords of SET1\n254 D. Guidotti et al.\n• Sample 0: “More than a great and magical place. Clean, tasty , unique for \nwalks, suitable for children ... there was a big party downstairs in the restau-\nrants ... everything was again very clean, tasty, wonderful ... the staff is great \npolite and smiling, ... The experience with the ducks and the swan is very nice!”.\n• Sample 2: “Very tasty large cup of cappuccino and very good fresh ice cream. \nAnd for a 5 star hotel in a castle on an island on a river for very reasonable and \naffordable price. Oh yes, and very friendly staff.”.\nFirstly, it is important to note that we have provided only brief excerpts of Sample \n0 for the sake of conciseness. The complete review spans 192 words and describes \nvarious aspects of the user’s stay at the hotel associated with the castle. Conversely, \nSample 2 is presented in its entirety as it comprises only 39 words.\nWhile the behaviour of the models under consideration is generally not formally \nexplainable, we can posit some hypotheses by comparing the two reviews. Spe-\ncifically, we observe that the word “tasty”, which we would semantically associate \nwith the keyword “food”, appears to be used as a generic adjective to denote a posi-\ntive sentiment in the first review. This unexpected usage of the term, coupled with \nthe presence of the term “restaurants”, may contribute to the models’ uncertainty \nregarding the keyword “food”. Even to human observers, it is not entirely clear \nwhether the “tasty” mentioned in the first review refers to some type of food con-\nsumed by the reviewer at the castle or if it is simply an unintended use of the term, \npossibly attributable to English being a second language for the reviewer or to errors \nin machine translation applied to the review in its original language. Conversely, in \nthe second review, the reference to food is much clearer: the user mentions a “tasty \nlarge cup of cappuccino” and “very good fresh ice cream”, leading to a more unani-\nmous and certain match with the keyword “food” by the models.\nThe variance in the models’ confidence levels regarding the keyword “staff” \nappears to arise from different factors. Both reviews explicitly mention the staff  \nof the respective locations: the first review states “... the staff is great, polite, and \nsmiling...”, while the second mentions “... very friendly staff”. Consequently, it is \nFig. 12  Boxplots illustrating the probabilities for each keyword in SET1 predicted by the models of inter-\nest when applied to the first (Sample 0) and third (Sample 2) samples of the Castle Dataset, respectively\n255\nDiscovering sentiment insights: streamlining tourism review…\nplausible that the disparity in the models’ confidence levels may be attributed to \nthe length discrepancy between the two reviews – the first being nearly five times \nlonger than the second – and the discussion of various events at the hotel, present \nin the first review, which the models struggle to interpret. Specifically, the reviewer \nrecounts instances of noise disturbances occurring at different times of the day, a \nnarrative that necessitates a certain level of contextual understanding, a challenge \nfor LLMs, which are known for their difficulty in replicating common sense rea-\nsoning. As a result, this narrative segment of the review appears to complicate the \nmodels’ comprehension of the text, consequently leading to lower confidence levels \nin their predictions.\nOverall, our experimental assessment revealed that while NLP technologies can \naid domain experts in selecting sets of relevant keywords, their usage should be sup-\nplemented with human oversight to prevent the selection of sub optimal keywords. \nAdditionally, we verified that the models under scrutiny can reliably match reviews \nwith keywords of interest to a certain extent. However, the overall reliability of these \nmodels may fluctuate depending on the specific keywords involved. Lastly, our anal-\nysis of two particular reviews highlighted that the confidence of the models’ predic-\ntions appears to be significantly influenced by both the length and writing style of \nthe reviews.\n6  Conclusions\nIn this paper, we focused on showcasing how LLMs’ capabilities can enhance deci-\nsion-making and service delivery in the tourism industry. Particularly, we explored \ntwo applications we believe could be of interest for tourism enterprises: sentiment \nanalysis and keyword extraction. We examined models that are readily available and \ndo not necessitate additional training, which could be prohibitively costly in terms \nof resources and thus unfeasible for SMEs. In the subsequent sub-sections, we will \ndiscuss the theoretical contribution of the study, analyse the practical implications \nof the presented research in the tourism domain, examine the limitations of LLMs \nidentified through our research, and outline potential future research directions in \nthis domain.\n6.1  Theoretical contribution\nThis paper contributes theoretically by exploring the capabilities of LLMs in senti-\nment analysis and keyword extraction using zero-shot learning within the domain \nof tourism and hospitality. Our experimental evaluation encompassed three distinct \ndatasets and thirteen models, demonstrating their ability to identify positive and \nnegative sentiments in the majority of reviews analysed. However, the significant \nimbalance between positive and negative reviews in the datasets posed challenges in \nprecisely evaluating the models’ Precision in sentiment analysis. Overall, the mod-\nels showed effectiveness in aligning reviews with keywords identified by domain \n256 D. Guidotti et al.\nexperts, though their success varied depending on the specific keywords. Notably, \nkeywords supported by NLP techniques often yielded better alignment with the \nreviews. Further analysis of individual reviews provided insights into how fac-\ntors such as review length, writing style, and language nuances impact the models’ \npredictive accuracy. In summary, this paper highlights how integrating LLM tech-\nnologies can empower tourism enterprises, including SMEs, to extract actionable \ninsights from customer feedback more efficiently, thereby enhancing their competi-\ntive edge in the market. This research can contribute to expanding the understanding \nof LLM applications in tourism and lays a foundation for future studies leveraging \nadvanced NLP techniques for analysing tourism-related data.\n6.2  Practical implications\nFrom a practical perspective, the findings of our research can offer relevant insights \nfor SMEs in the tourism domain. By using LLMs for sentiment analysis and key -\nword extraction, tourism managers can gain a deeper understanding of customer \nsentiments and preferences with minimal computational resources. Additionally, the \nuse of open-source LLM architectures democratises access to advanced analytical \ntools, enabling even resource-constrained organisations to benefit from cutting-edge \ntechnology. Our collaboration with SMEs within a technology transfer project has \ndemonstrated the practical feasibility of integrating LLMs into everyday business \npractices. The positive feedback received from these collaborations underlines the \nvalue of tailored solutions in meeting specific business needs, thereby enhancing \noperational efficiency and customer engagement. Such partnerships not only vali-\ndate the applicability of LLM-driven insights but also foster ongoing innovation and \nadaptation in response to industry demands. This mutual exchange of knowledge \nand expertise ensures that SMEs can harness the full potential of LLM technologies \nto drive sustainable growth and competitive advantage in the tourism sector.\n6.3  Limitations of LLMs\nDespite the promising results, it is essential to acknowledge the limitations of LLMs \nin this context. One primary limitation is the variability in model performance \nacross different datasets and keywords, indicating that LLMs may not consistently \nprovide accurate results in all scenarios. Additionally, the lack of a ground truth for \nkeyword extraction poses challenges in evaluating the models’ performance rigor -\nously. Furthermore, LLMs can sometimes struggle with neutral sentiment detection \nand may exhibit biases inherent in the training data used to develop them. These \nconstraints emphasise the importance of interpreting the results carefully and pin-\npoint areas where future advancements in LLM development can be focused.\n257\nDiscovering sentiment insights: streamlining tourism review…\n6.4  Future directions for research\nFirstly, our immediate objective involves creating meticulously annotated datasets spe-\ncifically designed for training purposes. These datasets will not only aid in refining key-\nword extraction models but also strengthen the reliability of model assessments, even in \nscenarios where ground truth data is unavailable. Secondly, there is a need to develop \nmore robust evaluation metrics for keyword extraction tasks that do not rely on ground \ntruth data. Enhancing these metrics would significantly improve the accuracy and trust-\nworthiness of model evaluations. Furthermore, exploring the performance of LLMs \nacross a broader range of datasets and within diverse tourism contexts is essential. This \nbroader exploration will provide a more comprehensive understanding of how LLMs \ncan be effectively applied in real-world scenarios within the tourism industry. Addition-\nally, there is a critical need for research aimed at mitigating biases inherent in LLMs \nand enhancing their capability to accurately detect neutral sentiments. Addressing these \nchallenges will be instrumental in advancing the reliability and fairness of LLM appli-\ncations across various domains. These future directions are crucial for overcoming cur-\nrent limitations and advancing the field of NLP within the tourism sector and beyond.\nAcknowledgements This work has been developed within the framework of the project e.INS- Ecosys-\ntem of Innovation for Next Generation Sardinia (cod. ECS 00000038) funded by the Italian Ministry for \nResearch and Education (MUR) under the National Recovery and Resilience Plan (NRRP) - MISSION 4 \nCOMPONENT 2, “From research to business” INVESTMENT 1.5, “Creation and strengthening of Eco-\nsystems of innovation” and construction of “Territorial R&D Leaders”, CUP J83C21000320007.\nAuthor contributions All authors whose names appear on the submission: 1. made substantial contribu-\ntions to the conception or design of the work; or the acquisition, analysis, or interpretation of data; or \nthe creation of new software used in the work; 2. drafted the work or revised it critically for important \nintellectual content; 3. approved the version to be published; 4. agree to be accountable for all aspects of \nthe work in ensuring that questions related to the accuracy or integrity of any part of the work are appro-\npriately investigated and resolved.\nData availability and access The datasets used in this work are freely available \non Kaggle and can be found at the following links: 1. European Castles Dataset: \nhttps:// www. kaggle. com/ datas ets/ datas cienc edonut/ europ ean- castl es (License: CC0 \n(https:// creat iveco mmons. org/ publi cdoma in/ zero/1. 0/)). 2. Google Maps Restaurant \nReviews Dataset: https:// www. kaggle. com/ datas ets/ deniz bilgi nn/ google- maps- resta \nurant- revie ws (License: ODbL (https:// opend ataco mmons. org/ licen ses/ odbl/1- 0/)). \n3. Hotel Reviews Dataset: https:// www. kaggle. com/ datas ets/ datafi  niti/ hotel- revie ws \n(License: CC BY-NC-SA 4.0(https:// creat iveco mmons. org/ licen ses/ by- nc- sa/4. 0/)).\nCode availability The code required to replicate our experiments is available online \nat https:// github. com/ AIMet- Lab/ ITT- 2024- PNRR- ZSSA.\nDeclarations \nConflict of interest The authors declare no competing interests.\n258 D. Guidotti et al.\nEthical and informed consent for data used No informed consent is needed for the data used.\nOpen Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDeriv -\natives 4.0 International License, which permits any non-commercial use, sharing, distribution and repro-\nduction in any medium or format, as long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate if you modified the licensed mate-\nrial. You do not have permission under this licence to share adapted material derived from this article or \nparts of it. The images or other third party material in this article are included in the article’s Creative \nCommons licence, unless indicated otherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view \na copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.\nReferences\nAbdullah T, Ahmet A (2023) Deep learning in sentiment analysis: recent architectures. ACM Comput \nSurv 55(8):159:1-159:37. https:// doi. org/ 10. 1145/ 35487 72\nAmeur A, Hamdi S, Yahia SB (2024) Sentiment analysis for hotel reviews: a systematic literature review. \nACM Comput Surv 56(2):51:1-51:38. https:// doi. org/ 10. 1145/ 36051 52\nAugenstein I, Das M, Riedel S, et  al (2017) Semeval 2017 task 10: Scienceie-extracting keyphrases \nand relations from scientific publications. In: Proceedings of the 11th International Workshop on \nSemantic Evaluation, SemEval@ACL 2017, Vancouver, Canada, August 3-4, 2017. Association for \nComputational Linguistics, pp 546–555, https:// doi. org/ 10. 18653/ V1/ S17- 2091\nBagherzadeh S, Shokouhyar S, Jahani H et al (2021) A generalizable sentiment analysis method for creat-\ning a hotel dictionary: using big data on tripadvisor hotel reviews. J Hosp Tour Technol 12(2):210–\n238. https:// doi. org/ 10. 1108/ JHTT- 02- 2020- 0034\nBarbieri F, Anke LE, Camacho-Collados J (2021) XLM-T: a multilingual language model toolkit for twit-\nter. CoRR abs/2104.12250\nBarbieri F, Camacho-Collados J, Anke LE, et al (2020) Tweeteval: unified benchmark and comparative \nevaluation for tweet classification. In: Findings of the Association for Computational Linguistics: \nEMNLP 2020, Online Event, 16-20 November 2020, Findings of ACL, vol EMNLP 2020. Associa-\ntion for Computational Linguistics, pp 1644–1650, https:// doi. org/ 10. 18653/ V1/ 2020. FINDI NGS- \nEMNLP. 148\nBirjali M, Kasri M, Hssane AB (2021) A comprehensive survey on sentiment analysis: approaches, chal-\nlenges and trends. Knowl Based Syst 226:107134. https:// doi. org/ 10. 1016/J. KNOSYS. 2021. 107134\nBowman SR, Angeli G, Potts C, et al (2015) A large annotated corpus for learning natural language infer-\nence. In: Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-\ning, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. The Association for Computational \nLinguistics, pp 632–642, https:// doi. org/ 10. 18653/ V1/ D15- 1075\nBucur C (2015) Using opinion mining techniques in tourism. Proc Econ Finance 23:1666–1673. https:// \ndoi. org/ 10. 1016/ S2212- 5671(15) 00471-2\nDagan I, Glickman O, Magnini B (2005) The PASCAL recognising textual entailment challenge. In: \nMachine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and \nRecognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW \n2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, Lecture Notes in Computer \nScience, vol 3944. Springer, pp 177–190, https:// doi. org/ 10. 1007/ 11736 790_9\nde Souza JGR, de Paiva Oliveira A, de Andrade GC, et al (2018) A deep learning approach for senti-\nment analysis applied to hotel’s reviews. In: Natural language processing and information systems-\n23rd International Conference on Applications of Natural Language to Information Systems, NLDB \n2018, Paris, France, June 13-15, 2018, Proceedings, Lecture Notes in Computer Science, vol 10859. \nSpringer, pp 48–56, https:// doi. org/ 10. 1007/ 978-3- 319- 91947-8_5\nDevlin J, Chang M, Lee K, et al (2019) BERT: pre-training of deep bidirectional transformers for lan-\nguage understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, \n259\nDiscovering sentiment insights: streamlining tourism review…\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). Association for Com-\nputational Linguistics, pp 4171–4186, https:// doi. org/ 10. 18653/ V1/ N19- 1423\nFarisi AA, Sibaroni Y, Faraby SA (2019) Sentiment analysis on hotel reviews using multinomial naÃ¯ve \nbayes classifier. J Phys: Conf Ser 1192(1):012024. https:// doi. org/ 10. 1088/ 1742- 6596/ 1192/1/ \n012024\nFiroozeh N, Nazarenko A, Alizon F et al (2020) Keyword extraction: Issues and methods. Nat Lang Eng \n26(3):259–291. https:// doi. org/ 10. 1017/ S1351 32491 90004 57\nGhorpade T, Ragha L (2012) Featured based sentiment classification for hotel reviews using nlp and \nBayesian classification. In: 2012 International Conference on Communication, Information & Com-\nputing Technology (ICCICT), pp 1–5, https:// doi. org/ 10. 1109/ ICCICT. 2012. 63981 36\nGräbner D, Zanker M, Fliedl G, et al (2012) Classification of customer reviews based on sentiment analy-\nsis. In: Information and Communication Technologies in Tourism 2012, ENTER 2012, Proceed-\nings of the International Conference in Helsingborg, Sweden, January 25-27, 2012. Springer, pp \n460–470, https:// doi. org/ 10. 1007/ 978-3- 7091- 1142-0_ 40\nHartmann J, Heitmann M, Siebert C et al (2023) More than a feeling: accuracy and application of senti-\nment analysis. Int J Res Mark 40(1):75–87. https:// doi. org/ 10. 1016/j. ijres mar. 2022. 05. 005\nHe P, Liu X, Gao J, et al (2021) DeBERTa: decoding-enhanced bert with disentangled attention. In: 9th \nInternational Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May \n3-7, 2021. OpenReview.net\nHnin CC, Naw N, Win A (2018) Aspect level opinion mining for hotel reviews in Myanmar language. \nIn: IEEE International Conference on Agents, ICA 2018, Singapore, July 28-31, 2018. IEEE, pp \n132–135, https:// doi. org/ 10. 1109/ AGENTS. 2018. 84600 40\nJardim SVB, Mora C (2021) Customer reviews sentiment-based analysis and clustering for market ori-\nented tourism services and products development or positioning. In: CENTERIS 2021 - Interna-\ntional Conference on ENTERprise Information Systems / ProjMAN 2021 - International Confer -\nence on Project MANagement / HCist 2021 - International Conference on Health and Social Care \nInformation Systems and Technologies 2021, Braga, Portugal, Procedia Computer Science, vol 196. \nElsevier, pp 199–206, https:// doi. org/ 10. 1016/J. PROCS. 2021. 12. 006\nJiang M, Zhang W, Zhang M et al (2019) An LSTM-CNN attention approach for aspect-level sentiment \nclassification. J Comput Methods Sci Eng 19(4):859–868. https:// doi. org/ 10. 3233/ JCM- 190022\nKim SN, Medelyan O, Kan M, et al (2010) Semeval-2010 task 5 : automatic keyphrase extraction from \nscientific articles. In: Proceedings of the 5th International Workshop on Semantic Evaluation, \nSemEval@ACL 2010, Uppsala University, Uppsala, Sweden, July 15-16, 2010. The Association for \nComputer Linguistics, pp 21–26\nLaurer M, Van Atteveldt W, Casas A et al (2023) Less annotating, more classifying: addressing the data \nscarcity issue of supervised machine learning with deep transfer learning and BERT-NLI. Polit \nAnal. https:// doi. org/ 10. 1017/ pan. 2023. 20\nLewis M, Liu Y, Goyal N, et al (2020) BART: denoising sequence-to-sequence pre-training for natural \nlanguage generation, translation, and comprehension. In: Proceedings of the 58th Annual Meeting \nof the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020. Association \nfor Computational Linguistics, pp 7871–7880, https:// doi. org/ 10. 18653/ V1/ 2020. ACL- MAIN. 703\nLiu Y, Ott M, Goyal N, et al (2019) RoBERTa: a robustly optimized BERT pretraining approach. CoRR \nabs/1907.11692\nLoureiro D, Barbieri F, Neves L, et  al (2022) Timelms: diachronic language models from twitter. In: \nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL \n2022 - System Demonstrations, Dublin, Ireland, May 22-27, 2022. Association for Computational \nLinguistics, pp 251–260, https:// doi. org/ 10. 18653/ V1/ 2022. ACL- DEMO. 25\nMartins GS, de Paiva Oliveira A, Moreira A (2017) Sentiment analysis applied to hotels evaluation. In: \nComputational Science and Its Applications - ICCSA 2017 - 17th International Conference, Tri-\neste, Italy, July 3-6, 2017, Proceedings, Part VI, Lecture Notes in Computer Science, vol 10409. \nSpringer, pp 710–716, https:// doi. org/ 10. 1007/ 978-3- 319- 62407-5_ 52\nNguyen DQ, Vu T, Nguyen AT (2020) Bertweet: A pre-trained language model for English tweets. In: \nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Sys-\ntem Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020. Association for Com-\nputational Linguistics, pp 9–14, https:// doi. org/ 10. 18653/ V1/ 2020. EMNLP- DEMOS.2\nPal S, Ghosh S, Nag A (2018) Sentiment analysis in the light of LSTM recurrent neural networks. Int J \nSynth Emot 9(1):33–39. https:// doi. org/ 10. 4018/ IJSE. 20180 10103\n260 D. Guidotti et al.\nPencarelli T (2020) The digital revolution in the travel and tourism industry. J Inf Technol Tour \n22(3):455–476. https:// doi. org/ 10. 1007/ S40558- 019- 00160-3\nPriyantina RA, Sarno R (2019) Sentiment analysis of hotel reviews using latent Dirichlet allocation, \nsemantic similarity and lstm. Int J Intell Eng Syst 12(4):142–155. https:// doi. org/ 10. 22266/ ijies \n2019. 0831. 14\nRaiaan MAK, Mukta MSH, Fatema K et al (2024) A review on large language models: Architectures, \napplications, taxonomies, open issues and challenges. IEEE Access 12:26839–26874. https:// doi. \norg/ 10. 1109/ ACCESS. 2024. 33657 42\nRosenthal S, Farra N, Nakov P (2017) Semeval-2017 task 4: sentiment analysis in twitter. In: Proceed-\nings of the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Vancouver, \nCanada, August 3-4, 2017. Association for Computational Linguistics, pp 502–518, https:// doi. org/ \n10. 18653/ V1/ S17- 2088\nRossetti M, Stella F, Zanker M (2016) Analyzing user reviews in tourism with topic models. J Inf Tech-\nnol Tour 16(1):5–21. https:// doi. org/ 10. 1007/ S40558- 015- 0035-Y\nSharma S, Diwakar M, Joshi K, et al (2022) A critical review on sentiment analysis techniques. In: 2022 \n3rd International Conference on Intelligent Engineering and Management (ICIEM), IEEE, pp 741–\n746, https:// doi. org/ 10. 1109/ ICIEM 54221. 2022. 98531 40\nShi H, Li X (2011) A sentiment analysis model for hotel reviews based on supervised learning. In: Inter -\nnational Conference on machine learning and cybernetics, ICMLC 2011, Guilin, China, July 10-13, \n2011, Proceedings. IEEE, pp 950–954, https:// doi. org/ 10. 1109/ ICMLC. 2011. 60168 66\nTan KL, Lee C, Anbananthen KSM et al (2022) RoBERTa-LSTM: a hybrid model for sentiment analysis \nwith transformer and recurrent neural network. IEEE Access 10:21517–21525. https:// doi. org/ 10. \n1109/ ACCESS. 2022. 31528 28\nTesfagergish SG, Kapočiūtė-Dzikienė J, Damaševičius R (2022) Zero-shot emotion detection for \nsemi-supervised sentiment analysis using sentence transformers and ensemble learning. Appl Sci \n12(17):8662. https:// doi. org/ 10. 3390/ app12 178662\nVaswani A, Shazeer N, Parmar N, et al (2017) Attention is all you need. In: Advances in Neural Informa-\ntion Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, \nDecember 4-9, 2017, Long Beach, CA, USA, pp 5998–6008\nWang X, Ye Y, Gupta A (2018) Zero-shot recognition via semantic embeddings and knowledge graphs. \nIn: 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake \nCity, UT, USA, June 18-22, 2018. Computer Vision Foundation / IEEE Computer Society, pp 6857–\n6866, https:// doi. org/ 10. 1109/ CVPR. 2018. 00717\nWang W, Zheng VW, Yu H et al (2019) A survey of zero-shot learning: settings, methods, and applica-\ntions. ACM Trans Intell Syst Technol 10(2):13:1-13:37. https:// doi. org/ 10. 1145/ 32933 18\nWang W, Bao H, Huang S, et al (2021) Minilmv2: Multi-head self-attention relation distillation for com-\npressing pretrained transformers. In: Findings of the association for computational linguistics: ACL/\nIJCNLP 2021, Online Event, August 1-6, 2021, Findings of ACL, vol ACL/IJCNLP 2021. Associa-\ntion for Computational Linguistics, pp 2140–2151, https:// doi. org/ 10. 18653/ V1/ 2021. FINDI NGS- \nACL. 188\nWang H, Li J, Wu H et al (2022) Pre-trained language models and their applications. Engineering 25:51–\n65. https:// doi. org/ 10. 1016/j. eng. 2022. 04. 024\nWankhade M, Rao ACS, Kulkarni C (2022) A survey on sentiment analysis methods, applications, and \nchallenges. Artif Intell Rev 55(7):5731–5780. https:// doi. org/ 10. 1007/ S10462- 022- 10144-1\nWilliams A, Nangia N, Bowman SR (2018) A broad-coverage challenge corpus for sentence understand-\ning through inference. In: Proceedings of the 2018 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Language Technologies, NAACL-HLT \n2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers). Association for \nComputational Linguistics, pp 1112–1122, https:// doi. org/ 10. 18653/ V1/ N18- 1101\nXian Y, Lampert CH, Schiele B et  al (2019) Zero-shot learning—a comprehensive evaluation of the \ngood, the bad and the ugly. IEEE Trans Pattern Anal Mach Intell 41(9):2251–2265. https:// doi. org/ \n10. 1109/ TPAMI. 2018. 28577 68\nYounas A, Nasim R, Ali S, et al (2020) Sentiment analysis of code-mixed roman Urdu-English social \nmedia text using deep learning approaches. In: 23rd IEEE International Conference on Computa-\ntional Science and Engineering, CSE 2020, Guangzhou, China, December 29, 2020 - January 1, \n2021. IEEE, pp 66–71, https:// doi. org/ 10. 1109/ CSE50 738. 2020. 00017\n261\nDiscovering sentiment insights: streamlining tourism review…\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.",
  "topic": "Tourism",
  "concepts": [
    {
      "name": "Tourism",
      "score": 0.7641909718513489
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5456304550170898
    },
    {
      "name": "Computer science",
      "score": 0.4970085918903351
    },
    {
      "name": "Data science",
      "score": 0.46157917380332947
    },
    {
      "name": "Knowledge management",
      "score": 0.35680508613586426
    },
    {
      "name": "Business",
      "score": 0.32533401250839233
    },
    {
      "name": "Natural language processing",
      "score": 0.251379132270813
    },
    {
      "name": "Geography",
      "score": 0.14907169342041016
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I159650629",
      "name": "University of Sassari",
      "country": "IT"
    }
  ]
}