{
  "title": "Unsupervised Paraphrasing with Pretrained Language Models",
  "url": "https://openalex.org/W3199834251",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2166596042",
      "name": "Tong Niu",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2528646980",
      "name": "Semih Yavuz",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2116467378",
      "name": "Yingbo Zhou",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2337946745",
      "name": "Nitish Shirish Keskar",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2095600284",
      "name": "Huan Wang",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2095665791",
      "name": "Caiming Xiong",
      "affiliations": [
        "Salesforce (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1566289585",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2515295520",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2250225488",
    "https://openalex.org/W3035144493",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2927746189",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2557480356",
    "https://openalex.org/W2131726681",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2116900724",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3102273025",
    "https://openalex.org/W2962953307",
    "https://openalex.org/W2903376039",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3023986361",
    "https://openalex.org/W3099942180",
    "https://openalex.org/W2931212643",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2992787485",
    "https://openalex.org/W4295253143",
    "https://openalex.org/W3035051717",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W3099309639",
    "https://openalex.org/W2143927888",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2090243146",
    "https://openalex.org/W2964212550",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2903430190",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W2945232141",
    "https://openalex.org/W2988853560",
    "https://openalex.org/W2963508788",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W1980095184",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2252001469",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2996403597",
    "https://openalex.org/W2963564796",
    "https://openalex.org/W2082980366",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3034466976",
    "https://openalex.org/W2949832505",
    "https://openalex.org/W2970559004",
    "https://openalex.org/W2161374612",
    "https://openalex.org/W2531908596",
    "https://openalex.org/W2788277448",
    "https://openalex.org/W2970677802",
    "https://openalex.org/W2966746916",
    "https://openalex.org/W3035368872",
    "https://openalex.org/W2134273450",
    "https://openalex.org/W2963463583",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2952481978",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W3034531294",
    "https://openalex.org/W2963558220",
    "https://openalex.org/W2755124548"
  ],
  "abstract": "Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on supervised methods, which require a large amount of labeled data that is costly to collect. To address this drawback, we adopt a transfer learning approach and propose a training pipeline that enables pre-trained language models to generate high-quality paraphrases in an unsupervised setting. Our recipe consists of task-adaptation, self-supervision, and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a surface form dissimilar from the input, whenever the language model emits a token contained in the source sequence, DB prevents the model from outputting the subsequent source token for the next generation step. We show with automatic and human evaluations that our approach achieves state-of-the-art performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and is robust to domain shift between the two datasets of distinct distributions. We also demonstrate that our model transfers to paraphrasing in other languages without any additional finetuning.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5136–5150\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n5136\nUnsupervised Paraphrasing with Pretrained Language Models\nTong Niu Semih Yavuz Yingbo Zhou\nNitish Shirish Keskar Huan Wang\nSalesforce Research\n{tniu, syavuz, yingbo.zhou,\nnkeskar, huan.wang, cxiong}@salesforce.com\nCaiming Xiong\nAbstract\nParaphrase generation has beneﬁted exten-\nsively from recent progress in the designing\nof training objectives and model architectures.\nHowever, previous explorations have largely\nfocused on supervised methods, which require\na large amount of labeled data that is costly to\ncollect. To address this drawback, we adopt\na transfer learning approach and propose a\ntraining pipeline that enables pre-trained lan-\nguage models to generate high-quality para-\nphrases in an unsupervised setting. Our recipe\nconsists of task-adaptation, self-supervision,\nand a novel decoding algorithm named Dy-\nnamic Blocking (DB). To enforce a surface\nform dissimilar from the input, whenever the\nlanguage model emits a token contained in\nthe source sequence, DB prevents the model\nfrom outputting the subsequent source token\nfor the next generation step. We show with\nautomatic and human evaluations that our ap-\nproach achieves state-of-the-art performance\non both the Quora Question Pair (QQP) and\nthe ParaNMT datasets and is robust to domain\nshift between the two datasets of distinct distri-\nbutions. We also demonstrate that our model\ntransfers to paraphrasing in other languages\nwithout any additional ﬁnetuning.\n1 Introduction\nParaphrase generation restates text input in a differ-\nent surface form while preserving its semantics. It\nhas various applications on downstream NLP tasks\nincluding text summarization (Cao et al., 2016), se-\nmantic parsing (Berant and Liang, 2014), as well as\ndiversifying text generation for user-facing systems\nsuch as chatbots. To evaluate model robustness, a\nparaphraser can be used to generate adversarial\nexamples, which also serve as augmented data to\ntrain the target neural networks (Iyyer et al., 2018a).\nBesides, paraphrasing queries makes Question An-\nswering systems more likely to match with key-\nwords in a knowledge base (Fader et al., 2014; Yin\net al., 2015).\nLanguage Model Task-adaptive Training\nUnsupervised Data\nSynthetic Parallel Data\nCorrupt and \nShufﬂe\nGenerate with \nDynamic Blocking\nSelf-supervised Model\nSelf-supervision Data\nTask-adapted Model  \nSelf-supervised Training\nFigure 1: Training pipeline of our paraphrasing model.\nWe ﬁrst train a task-adapted model with a denoising\nobjective so that it is able to reconstruct input text. We\nthen use Dynamic Blocking (DB) to generate pseudo-\npairs of paraphrasing data. Finally, the generated data\nis used to train the self-supervised model.\nHowever, it is expensive to annotate para-\nphrases, resulting in only a few human-labeled\ndatasets. The existing ones are either small-scale\nlike MRPC (Dolan and Brockett, 2005), or of\nclosed domains like QQP1 which consists entirely\nof questions. Consequently, previous work ex-\nplored automatically (hence noisily) annotated\ndatasets such as PIT-2015 (Xu et al., 2013), Twit-\nter URL Paraphrase Corpus (Lan et al., 2017),\nParaNMT (Wieting and Gimpel, 2018), and Para-\nBank (Hu et al., 2019), or re-purposed datasets\nincluding MSCOCO (Lin et al., 2014) and WikiAn-\nswers (Fader et al., 2013). The scarcity of high-\nquality datasets motivates us to consider unsuper-\nvised alternatives. In this work, we explore a trans-\nfer learning approach, which leverages unsuper-\nvised large-scale pretrained models like T5 (Raffel\net al., 2019) and BART (Lewis et al., 2019).\nThe effectiveness of BERT-score (Zhang et al.,\n2019) in identifying text similarity hints that pre-\ntrained language models are equipped with exten-\nsive knowledge in paraphrasing. This knowledge\nmay be attributed to the fact that text spans shar-\n1https://www.kaggle.com/c/\nquora-question-pairs\n5137\ning similar context usually stay semantically close\ntogether – word embedding (Mikolov et al., 2013)\nbeing a classic example. In other words, the para-\nphrasing capability of language models stems from\nthe strong correlation between context and seman-\ntic similarity. In this work, we use pre-trained au-\ntoregressive LMs to leverage such implicit knowl-\nedge for paraphrasing in an unsupervised setting.2\nFor paraphrasing, decoder-only LMs merely out-\nput a continuation of the input, while Sequence-\nto-Sequence models like BART tend to copy the\ninput through even when paired with popular de-\ncoding algorithms such as greedy decoding, beam\nsearch or top-k/psampling (Holtzman et al., 2020)\nbecause the probabilities of the input tokens during\ngeneration are all peaked. To address this issue,\nwe propose Dynamic Blocking (DB), a decoding\nalgorithm that effortlessly transforms pre-trained\nautoregressive language models into natural para-\nphrasers with the help of task-adaption and self-\nsupervision (Figure 1). To obtain a surface form\ndifferent from the input, whenever we emit a to-\nken that is present in the source sequence, this\nalgorithm prevents the model from outputting its\nimmediate successor for the next generation step.\nThe algorithm is based on the intuition that during\ninference, although the top candidate at each gener-\nation step corresponds to a peaked probability, the\nrest of the distribution still contains rich linguis-\ntic knowledge suitable for paraphrasing. This is\nin similar spirit with using soft targets for model\ndistillation (Hinton et al., 2015).\nThrough automatic and human evaluations, we\ndemonstrate that our approach outperforms pre-\nvious models (including supervised, in-domain\nmodels and the ground-truth targets) on both\nQQP and ParaNMT datasets and incurs no per-\nformance loss under domain shifts (i.e., ﬁnetuned\non QQP and evaluated on ParaNMT, and vice\nversa). For automatic evaluations, we propose\na reference-independent automatic metric named\nBERT-iBLEU, which is a harmonic mean of BERT-\nscore and one minus self -BLEU. We show that\nthis new metric correlates signiﬁcantly better with\nhuman evaluation than traditional metrics. On the\nqualitative side, we illustrate with concrete exam-\nples that our model generates paraphrases that ex-\nhibit diverse syntactic structures. Finally, we ob-\nserve that our model can generate paraphrases in\nother languages without any additional training.\n2We will release all codes.\nOur contributions are: (1) a training pipeline\nthat leads to a strong, unsupervised paraphrasing\nmodel; (2) a novel decoding algorithm that effec-\ntively diversiﬁes paraphrase generation; (3) a new\nautomatic metric that evaluates paraphrasing qual-\nity more accurately.\n2 Model\nFigure 1 shows the training pipeline of our para-\nphrasing model, which consists of three key com-\nponents, namely task-adaptation, self-supervision\nand Dynamic Blocking. Overall we decode the task-\nadapted model with Dynamic Blocking to generate\nself-supervision data, which is in turn used to train\nthe ﬁnal model.\n2.1 Task-Adaptation\nInspired by Gururangan et al. (2020), we apply\ntask-adaptive training on the target dataset, treat-\ning its training set as a non-parallel collection of\nsentences. We perform task-adaptation by recon-\nstructing the original sequence from its corrupted\nversion with a denoising auto-encoder objective.\nUnlike previous work (Devlin et al., 2019; Lewis\net al., 2019), we do not corrupt inputs with masks,\nbut rather directly remove the corrupted tokens.\nThis is to avoid pretrain-ﬁnetune discrepancy in\ndenoising autoencoding models (Yang et al., 2019).\nAfter the deletions, we randomly shufﬂe all remain-\ning tokens to encourage the model to learn different\nalignments for better syntactic diversity.3 Note that\nwe perform both deletions and shufﬂing on the\nword-level. This is similar to whole-word masking\nintroduced in later versions of BERT (Devlin et al.,\n2019). To demonstrate the beneﬁt of our corrup-\ntion strategy, we present ablation study results in\nSection 4.3 by either adding masks or not shufﬂing.\n2.2 Dynamic Blocking\nUnlike previous diversity-promoting work which\nmainly focuses on the target side and encourages\ndissimilarity among beams (Vijayakumar et al.,\n2018; Kumar et al., 2019; Holtzman et al., 2020),\nDynamic Blocking takes the source input into ac-\ncount to guide the model toward generating in a\ndifferent surface form (Figure 2). As illustrated in\nAlgorithm 1, we represent the source sequenceSas\na list of tokens S = (S0,S1,...,S M ) and similarly\n3For example, consider an input sentence “I want to lose\nweight in a healthy way.” where we sample words “to” and\n“way” to delete and shufﬂe the rest. This may give us “weight\nin want a lose I healthy .” as the corrupted sentence.\n5138\nG1\npopulation\nG8\nFull Block Dictionary\n<s>\nThe\nchart\nbelow\nillustrates\nthroughout\nThe\nchart\nbelow\nillustrates\nhistory\nhow\nthroughout\nEncoder\n<s> The chart below illustrates how world population has changed throughout history.\nS0\n S1\n S2\n Sk\n<s>\nG0\nAutoregressive Decoding\nhas\nG9\nThe  following chart  depicts how  world ’s population  has  evolved over time.\nThe chart\nhow world\nworld\n population\nhistory\nhas\n changed\nBlocking\nthe\nthe\nfollowing\n has\n evolved\nfollowing\nchart\nbelow\nBlocked\ndid\nhas\ndoes\nNot \nBlocked\nOut of Dictionary\nDynamic Blocking\nevolved\nchanged\nshifted\nBlocked\nIn Dictionary\nIn Dictionary\n<s>\n the\n chart\n .\nActive Block Dictionary\nSample each entry \nwith probability p\nFigure 2: Illustration of the Dynamic Blocking algorithm on real outputs. The algorithm ﬁrst constructs afull block\ndictionary based on the input, which maps each token to its immediate successor to be blocked, and then samples\nfrom this dictionary to build multiple active block dictionaries , each used for generating a distinct paraphrase.\nWhen establishing an active dictionary, each entry in the full dictionary has a probability of p to be sampled.\nDuring generation, the blocking takes place whenever an item in the active dictionary is triggered.\nthe generated sequence as G= (G0,G1,...,G N ).\nSuppose that during generation, the model emits\nGj that is identical to some Si (it is not necessary\nthat i= j). Then for the next generation stepGj+1,\nthe algorithm forbids the model to generate Si+1.\nNote that we block Si+1 for only one step. After\nGj+1 is generated, we perform a different blocking\nfor Gj+2 iff Gj+1 ∈S.\nAlgorithm 1: Dynamic Blocking\ninput : A source sequence S consisting of a list of\ntokens S = (S0, S1, ..., SM ), and a\nG0 = BOS to start the decoding process\n1 Initialize j ←0\n2 while Gj ̸= EOS do\n3 if Gj = Si ∈S for some i then\n4 P(Gj+1 = Si+1|S, (G0, G1, ..., Gj) ←0\n5 end\n6 Generate Gj+1\n7 j ←j + 1\n8 end\noutput :G = (G0, G1, ..., GN )\nThe motivation to block for only one generation\nstep is to allow the possibility of pure syntactic\nvariation of the original sequence, meaning that all\ntokens are kept but their order is permuted. For\nexample, let us consider a decoding algorithm that\ncompletely prevents the model from generating a\nsource token at all generation steps – a popular\nn-gram blocking strategy we call Static Blocking.\nSuppose that we intend to paraphrase “I like apples\nand oranges.” as “ I like oranges and apples. ”.\nThis is a valid paraphrase, but if we completely\nblock the word “ apples” at all generation steps,\nit will be impossible to arrive at this paraphrase.\nHowever, with Dynamic Blocking the model will\nstill be able to generate the word “ apples” later\non even though this word has been temporarily\nblocked for one step after “and” is generated. As\nshown in Figure 2, Dynamic Blocking builds a\nblock dictionary which maps each token in the\nsource sequence to its immediate successor. We\nthen sample from this dictionary with a probability\npfor each entry. This hyperparameter controls how\ndifferent we want the paraphrase to be from the\nsource input. In two extreme cases: when p =\n0.0, the model does not block any tokens and most\nlikely copies through the source sequence; when\np = 1.0, the model always blocks the immediate\nnext token, leading to a drastically different surface\nform. In this work, we take the middle ground and\nset p= 0.5 so that for each blocking action, there\nwill be half of the candidates taking that path. Note\nthat if a word is tokenized into several subwords,\nonly the ﬁrst subword is allowed to be blocked.\nWe sample multiple block dictionaries to ensure\ndiversity among candidates, while leveraging beam\nsearch to ensure coherence. For each sampled\nblock dictionary, we use beam search to generate\nfour candidates and keep the top-ranked two. It is\n5139\nbeneﬁcial to combine the two decoding methods\nbecause beam search helps to weed out ungram-\nmatical or semantically invalid candidates.4\nNote that we only adopt bi-gram blocking be-\ncause it is a superset of all higher-gram blockings.\nConsider, e.g., a tri-gram blocking entry ab→cin\nthe block dictionary. If this entry is triggered, then\nthe bi-gram blocking entry b →cwill also have\nbeen triggered. Hence we found it unnecessary to\ninclude higher-order n-grams.\n2.3 Self-Supervision\nTo help the model internalize patterns learned\nfrom task-adaption, we pseudo-label the training\nset (Siddhant et al., 2020) by decoding the task-\nadapted model with Dynamic Blocking. Hav-\ning obtained the self-supervision data, we discard\nthe task-adapted model and start from the pre-\ntrained language model to avoid catastrophic forget-\nting (Chronopoulou et al., 2019; Chen et al., 2020).\nWe also include reversed data (i.e., swapping\nsource and target) because during task-adaptation\nthe target is always longer than the input, and in-\ncluding reversed data helps to offset this bias of\nsequence length.\n3 Experimental Setup\n3.1 BERT- iBLEU\nTo evaluate paraphrasing quality, we propose a new\nmetric named BERT-iBLEU which encourages se-\nmantic closeness while penalizing surface-form\nsimilarity. For semantic closeness we use the unsu-\npervised metric BERT-score (Zhang et al., 2019),\nwhich leverages a pretrained language model to\ncompute the cosine similarity between each to-\nken in the candidate and that in the reference us-\ning contextual embeddings. 5 To ensure that the\nkey information (often conveyed through relatively\nrare words) is retained in the paraphrase, we apply\nIDF-reweighing on each token.6 To measure the\nsurface-form dissimilarity, we use one minus self -\nBLEU, where self -BLEU is the BLEU score be-\ntween the source and the candidate. Hence BERT-\n4For more details on Dynamic Blocking, please refer to\nAppendix D.\n5In early experiments we tried another unsupervised metric\nUniversal Sentence Encoder (Cer et al., 2018) and supervised\nmetrics including RUSE (Shimanaka et al., 2018), Sentence-\nBERT (Reimers and Gurevych, 2019), and BLEURT (Sellam\net al., 2020). We observed that BERT-score worked better at\nevaluating semantic similarity compared to these metrics.\n6We use the BookCorpus dataset (Zhu et al., 2015) to\ncompute the IDF weights.\niBLEU (where i stands for inverse) is a weighted\nharmonic mean of the BERT-score and one minus\nself -BLEU.\nBERT-iBLEU=\n(β∗BERT-score−1+ 1.0∗(1−self-BLEU)−1\nβ+ 1.0\n)−1\nself-BLEU=BLEU(source,candidate)\nAs an extreme case, though copying through\nthe input leads to a perfect BERT- score,\n1 −self -BLEU = 0; hence BERT-iBLEU = 0.\nThis is the reason that we do not use the BERT-\nscore directly to evaluate paraphrases. βis used to\ncontrol the relative importance between semantic\nsimilarity and surface-form dissimilarity. In\nour experiments we set β = 4 .0 to scale up\nBERT-score so that it has a similar range with\nself -BLEU. Note that because BERT- iBLEU\nis reference-independent, it serves both as a\nmetric to evaluate paraphrasing quality and as a\ncriterion to re-rank generated candidates during\ntask-adaptation and self-supervision.\n3.2 Dataset\nWe evaluate on the Quora Question Pair (QQP)\nand the ParaNMT datasets. QQP contains 140K\nquestion pairs that are marked as a duplicate to\neach other and 640K non-parallel questions. The\nsizes of dev and test sets are 3K and 20K, respec-\ntively. The ParaNMT dataset was constructed by\nback-translating sentences in Czech in the CzEng\ndataset (Bojar et al., 2016). We directly ob-\ntained the test set of SOW-REAP from the authors\nof Goyal and Durrett (2020). To match the size\nof their training set, for task-adaptation we sample\n350K non-parallel sentences from ParaNMT-5M,\nwhile to generate self-supervision data we sample\n350K sentences from the same corpus as inputs.\nWe ﬁlter out any sentences in SOW-REAP’s test\nset to avoid training on test examples.\n3.3 Reproduction of Previous Models\nFor the experiments on QQP we reproduce the su-\npervised Transformer with the pre-trained T5-base\nmodel, which is stronger than the usual setting\nwhere the paraphraser trains from scratch. We also\nreproduce the model from Hegde and Patil (2020),\nwhich we refer to as CorruptLM. This model is\nsimilar to our task-adaptive phase (Section 2.1),\nexcept that they corrupt the inputs by removing all\nstop words rather than a ﬁxed percentage of arbi-\n5140\ntrary words.7 Instead of GPT-2 as used by their\nwork, we use BART which shows stronger results\non downstream tasks. The rest of the settings re-\nmain the same.8 For the experiments on ParaNMT\nwe use the SOW-REAP model released by Goyal\nand Durrett (2020).9\n3.4 Automatic Evaluation\nTo evaluate paraphrasing quality, we follow Li et al.\n(2019) to report iBLEU (Sun and Zhou, 2012),\nBLEU (Papineni et al., 2002) and ROUGE (Lin,\n2004) on QQP, and report BLEU and ROUGE on\nParaNMT. Follwing Goyal and Durrett (2020), for\nParaNMT both BLEU and ROUGE are calculated\nby ﬁrst selecting the candidate that achieves the\nbest sentence-level score with the ground-truth, and\nthen compute the corpus-level score of all these can-\ndidates. We use py-rouge10 to compute ROUGE\nand the Datasets library from HuggingFace11 to\ncompute BLEU. We also report BERT-iBLEU for\nthe models we reproduced.\n3.5 Human Evaluation\nWe conduct human evaluations on MTurk.12 For\neach experiment, we compare our model with the\nstrongest models reported in both supervised and\nunsupervised settings. On QQP, we compare with\nsupervised Transformer, unsupervised CorruptLM,\nand the ground-truth. On ParaNMT, we compare\nwith SOW-REAP and the ground-truth. To con-\nstruct holistic human studies, we opt for both head-\nto-head binary comparison and Likert-scale scor-\ning. The former provides straightforward results\non which model is stronger, while the latter is used\nto consolidate their relative positions.\nWe only worked with annotators who had com-\npleted more than 10K assignments, had an ap-\nproval rate of > 98%, and resided in the US. We\nalso required that the annotators be native English\nspeakers. When comparing between two model\n7Because the original paper did not provide the source\nof the stop words, we extract the ﬁrst 252 words from The\nCorpus of Contemporary American English (Davies, 2010) to\nmatch the number.\n8To encourage the model to output new words in the recon-\nstructed sentence, CorruptLM starts by randomly replacing\n20% of the words in the source sequence with synonyms using\nSyn-net (Miller, 1998) (also applied during inference).\n9https://github.com/tagoyal/\nsow-reap-paraphrasing/\n10https://pypi.org/project/py-rouge/\n11https://huggingface.co/metrics/\nsacrebleu\n12Screenshots of the interfaces used by our MTurk studies\nare presented in Appendix F.\noutputs based on the same input, we asked the an-\nnotators to identify which paraphrase they prefer in\nterms of overall quality.13 For each experiment, we\nrandomly sampled 200 examples from the QQP’s\nor ParaNMT’s test set and shufﬂed the order of\neach example to anonymize the model identities.\nEach assignment was scored by two annotators.\n4 Results\n4.1 Human Evaluation\nTable 1 and 2 present human evaluation results on\nour ﬁnal model compared with other baselines. On\nQQP our model outperforms both Transformer and\nCorruptLM. Recall that CorruptLM also leverages\na pre-trained language model. This indicates the\neffectiveness of our training pipeline when hold-\ning the LM factor as a constant. On ParaNMT\nour model outperforms SOW-REAP in both head-\nto-head and Likert-based evaluations. Moreover,\nour model outperforms the ground-truth on both\ndatasets. For ParaNMT, the result indicates that\nour approach also outperforms a supervised round-\ntrip translation baseline since that is how ParaNMT\ndata was generated in the ﬁrst place. For QQP,\nwe note two reasons why these scores do not in-\ndicate that our model can generate paraphrases\nwith human-level quality. First, QQP is human-\nlabeled, not human-generated. Second, QQP anno-\ntates duplicate questions rather than paraphrases.\nQuestions referring to the same topic but are not\nsemantically equivalent may still be marked as du-\nplicates.14\nWe use Cohen’s Kappa to evaluate the inter-\nannotator agreement. For head-to-head evaluations,\nwe obtained kappa = 0.35, indicating fair agree-\nment. Note that when calculating kappa, we leave\nout all cases where either of the two annotators\ngives a “tie” because this usually signiﬁes that they\nare unsure about which paraphrase is better.\n4.2 Advantage of the Proposed Metric\nTo facilitate a better understanding of the automatic\nevaluation results, we investigate how each of the\nautomatic metrics correlates with human evalua-\ntion. Table 3 shows that BERT-iBLEU agrees sig-\n13We intentionally did not ask them to separately evaluate\nsemantic similarity and surface-form diversity because the\nlatter is easy to check with self -BLEU.\n14For instance, the question pair “I’m 27, is it too late for\nme to go to medical school?” and “How old is too old to start\nmedical school?” has a positive label even though they do not\nshare the same meaning.\n5141\nDataset Ours v.s. Win( %) Tie( %) Loss( %) W-L( %)\nQQP\nTransformer 40.75 28 .25 31 .00 12 .50\nCorruptLM 46.00 26 .25 27 .75 18 .00\nGround-truth 43.00 16 .75 40 .25 2 .75\nParaNMT SOW-REAP 40.50 28 .50 31 .00 9 .50\nGround-truth 49.50 14 .50 36 .00 13 .50\nTable 1: Head-to-head human evaluation results. Each\nexperiment is performed over 200 samples with 2 anno-\ntators each. “ Ours” stands for the model trained with\nself-supervision and decoded with Dynamic Blocking.\nNote that both Transformer and SOW-REAP are su-\npervised models, and we are also comparing our unsu-\npervised model outputs with the ground-truth. “ W-L”\nstands for the difference between Win and Loss.\nDataset Model Avg. Score\nQQP\nSupervised Transformer 4.04 ±1.01\nUnsupervised CorruptLM 3.74 ±1.26\nOurs 4.19 ±0.99\nParaNMT Supervised SOW-REAP 3.78 ±1.15\nUnsupervised Ours 3.94 ±1.09\nTable 2: Likert-scale human evaluation results. Both\naverages and standard deviations are reported.\nniﬁcantly better with human perceptions. The rea-\nson that BLEU does not correlate well with human\nevaluation is that there are two conﬂicting objec-\ntives. The ﬁrst comes from keeping the important\ninformation, such as named entities, which should\nbe copied verbatim, while the second comes from\nusing different wordings to express the same se-\nmantics – the better the model is at this, the lower\nthe BLEU becomes. For a model good at both, the\ngain in BLEU for matching key entities and the\nloss for using different wordings cancel each other\nout, preventing BLEU from faithfully evaluating\nthe paraphrasing quality. Consequently, BLEU is\nonly useful for checking extreme cases: very low\nor high BLEU usually signals bad paraphrases, but\nfor the middle-ground cases BLEU alone is less\nindicative. A similar argument holds for ROUGE.\nIn contrast, BERT-score encourages the ﬁrst objec-\ntive and is not penalized by the second. However,\nparroting the input will still fool BERT-score alone.\nHence we pair it with self -BLEU to encourage\nsurface-form diversity.\n4.3 Automatic Evaluation\nOn QQP, our model outperforms both the su-\npervised Transformer and the unsupervised Cor-\nruptLM on BERT-iBLEU (Table 4).15 Recall that\n15We tried combining supervised Transformer with DB, and\nobtained a BERT-iBLEU of80.1 on QQP, indicating that DB\nitself is an effective diversity-promoting decoding strategy.\nBERT-iBLEU iBLEU BLEU ROUGE-1/2/L\nAgree % 68.9 39.4 45.3 21.8/5.4/21.4\nTable 3: The percentage of times where the ranking\ngiven by each metric agrees with that given by hu-\nman evaluation in the head-to-head studies. Only cases\nwhere two annotators agree are counted.\nboth Transformer and CorruptLM leverage a strong\npretrained language model, indicating that the per-\nformance gain stems mainly from our proposed\npipeline rather than the language model itself. On\nParaNMT, our model outperforms the supervised\nSOW-REAP (Table 5).16 As ablation studies on\ntask-adaptation and self-supervision, we can see in\nTable 4 and 5 that our model (TA+SS+DB) beats\nthe one that is either task-adapted only (TA) or\nself-supervised but decoded without DB (TA+SS),\nshowing that both self-supervision and Dynamic\nBlocking are crucial to paraphrasing quality.\nOn the traditional metrics in Table 4, our models\nalso obtain competitive results with the supervised\nmodels. However, as we move down to the last\nrow, we see that Copy-input achieves state-of-the-\nart results on all metrics except BERT-iBLEU, in-\ndicating that iBLEU, BLEU, and ROUGE scores\nare not reliable for evaluating paraphrasing qual-\nity.17 In contrast, our best model on BERT-iBLEU\n(TA+SS+DB) achieves much lower iBLEU and\nBLEU scores as compared to other models, show-\ning the inconsistency between these traditional met-\nrics and human evaluation. We also note one spe-\ncial aspect of Table 5 to make it easier to interpret.\nUnlike on QQP, the performance of Copy-input on\nParaNMT is the lowest among all models. How-\never, we need to take this comparison with a grain\nof salt because all the other results are based on\n10 candidates where only the ones with the high-\nest sentence-level scores are retained. In contrast,\nCopy-input only has one candidate. Thus Copy-\ninput and the other results are not directly compa-\nrable. Plus, SOW-REAP ﬁlters the dataset to only\ninclude syntactically diverse targets and then splits\nit into the train, dev and test sets, which makes\nCopy-input less effective.\n4.4 Robustness to Domain Shift\nOn the ParaNMT dataset, we notice that Cor-\nruptLM, when ﬁnetuned on non-parallel QQP,\n16Please refer to Appendix A for results of our model com-\npared with all previous ones on the traditional metrics.\n17Mao and Lee (2019) also observe that parroting often\nachieves competitive results.\n5142\nModel BERT- iBLEU iBLEU BLEU ROUGE-1 ROUGE-2 ROUGE-L\nSupervised Transformer 68.7 17.0 22.3 55.8 32.3 57.5\nUnsupervised\nCorruptLM 61.5 12.1 16.8 49.1 26.2 51.7\nTA 76.2 16.0 21.2 61.9 35.1 61.7\nTA+SS 78.9 15.6 20.7 61.5 32.8 60.7\nTA+SS+DB (NMT) 82.5 10.1 14.6 60.1 28.5 58.6\nTA+SS+DB 83.1 9.6 14.1 59.9 28.5 58.8\nNo Model Copy-input 0.0 24.3 30.4 65.7 41.7 66.5\nTable 4: Automatic evaluation results on QQP. TA =Task-Adaptation, SS = Self-Supervision and DB = Dynamic\nBlocking. “NMT” stands for model ﬁnetuned on non-parallel ParaNMT and evaluated cross-domain on QQP. Both\nour ﬁnal model (TA+SS+DB) and the best result for each metric are boldfaced. Please refer to Section A in the\nAppendix for a comparison with 12 supervised models and 5 unsupervised models from previous work.\nModel BERT- iBLEU BLEU ROUGE-1 ROUGE-2 ROUGE-L\nSupervised SOW-REAP 54.2 30.9 62.3 40.2 61.7\nUnsupervised\nCorruptLM (QQP) 39.7 7.6 31.9 11.6 31.6\nTA 72.0 20.2 59.0 32.3 53.8\nTA+SS 74.0 22.9 58.9 33.3 54.1\nTA+SS+DB (QQP) 76.8 22.0 60.1 33.8 54.9\nTA+SS+DB 78.0 22.6 59.8 33.2 54.5\nNo Model Copy-input 0.0 18.4 54.4 27.2 49.2\nTable 5: Automatic evaluation results on ParaNMT. “QQP” stands for models ﬁnetuned on non-parallel QQP and\nevaluated cross-domain on ParaNMT. Note that BLEU and ROUGE scores are based on top-10 candidates where\nonly the ones with the highest sentence-level scores are retained for the ﬁnal score computation.\nachieves much worse results than the other models\n(CorruptLM (QQP) row in Table 5), indicating that\nit is less robust to domain shift. In contrast, our\nmodel achieves similar results compared to the in-\ndomain one under the same setting (TA+SS+DB\n(QQP) row). Conversely, we also ﬁnetune our\nmodel on non-parallel ParaNMT and evaluate on\nQQP (TA+SS+DB (ParaNMT) row in Table 4). We\nobserve that this model again achieves performance\nsimilar to that of the in-domain model. These re-\nsults show that our model may be able to perform\ntask-adaptation using an arbitrary out-of-domain\ncorpus and still work well on the target domain.\n4.5 Ablation Studies on Corruption\nStrategies\nDuring task-adaptation, our corruption strategies\ninvolve both deletions and shufﬂing. In Table 6\nwe provide ablation study results where we either\nreplace words with masks instead of deleting them\nor delete words without shufﬂing. We can see that\nour delete-and-shufﬂe strategy achieves the best\nBERT-iBLEU score among the three settings.\nAddMask NoShufﬂe Delete-Shufﬂe\nBERT-iBLEU 80.7 81.7 83.1\nTable 6: Ablation studies on different corruption strate-\ngies for task-adaptation on QQP. AddMask stands for\nthe strategy where corrupted words are replaced with\nMASK tokens; NoShufﬂe corresponds to “no shufﬂing”\nafter sentence corruption.\n5 Analysis\n5.1 Syntactic Diversity\nIn Table 7, we qualitatively demonstrate para-\nphrases generated by our model that exhibit syntac-\ntic structure variance. Unlike previous work relying\non explicit syntactic scaffolding (Goyal and Dur-\nrett, 2020), our model achieves syntactic diversity\n“for free” from shufﬂing during task-adaptation.18\n5.2 Generalization to Other Languages\nDynamic Blocking on BART without Finetun-\ning Though we focus on T5 throughout the pa-\nper, we do note a unique ability of BART: it can\n18We present in Appendix B that shufﬂing also makes the\nmodel robust to grammar errors, enabling it to paraphrase and\nperform text normalization at the same time.\n5143\nInput Generated paraphrase\nWe got to spend the rest of the weekend at the track. yeah. We got to stay at the track for the rest of the weekend. yeah.\nAre predictions of the future based on the present too much? Are future predictions too much based on the present?\nWhat is the best way to reduce belly and arm fat? What is the easiest way to reduce arm and belly fat?\nYou can seduce enemy soldiers, though. You can, though, seduce enemy troops.\nWell, why would your buddy be in the shower with you?! Okay, why would you be in the shower with your friend?!\nTable 7: Selected paraphrases generated by our ﬁnal model that shows syntactic variance at different extents. Only\nthe top candidate is shown for each input.\ndirectly work with Dynamic Blocking to generate\nparaphrases (i.e., without domain-adaptation and\nself-supervision), though of lower quality than the\nself-supervised model. We demonstrate such exam-\nples in Appendix D.\nParaphrasing in Other Languages We observe\nthat although BART is trained almost exclusively\non English text, it is able to paraphrase in multi-\nple other languages. We adopt the aforementioned\nBART setting and present an example in German\n(Table 13 in Appendix E). To our best knowledge,\nthis is the ﬁrst unsupervised model that can para-\nphrase in a non-English language. The reasoning\nbehind this observation is twofold. First, although\nBART was trained on English corpora, there is\na small portion of the content in German due to\nmislabeled language identiﬁcation, allowing the\nmodel to observe German data; second, previous\nwork has shown that large-scale language models\nare able to perform zero-shot cross-lingual trans-\nfer on a variety of downstream classiﬁcation tasks,\nsuch as Named Entity Recognition (Moon et al.,\n2019), Natural Language Inference, and Document\nClassiﬁcation (Artetxe and Schwenk, 2019). Our\nwork hence demonstrates that it is possible to per-\nform such a transfer even for generative tasks like\nparaphrasing. We also hypothesize that the para-\nphrasing quality should improve if we apply our\ntraining pipeline to mBART or mT5 (Xue et al.,\n2020). We leave this as future work.\n6 Related Work\nParaphrase generation has been a long-standing\ntask that has several applications on downstream\nNLP tasks including text summarization (Cao et al.,\n2016), semantic parsing (Berant and Liang, 2014),\nand question answering (Yu et al., 2018). Early\nworks on paraphrase generation mostly rely on\nrule-based or statistical machine translation sys-\ntems (McKeown, 1980; Meteer and Shaked, 1988;\nBannard and Callison-Burch, 2005).\nSupervised Approaches Neural sequence-to-\nsequence (Seq2Seq) models have been used to ad-\ndress this task (Prakash et al., 2016; Li et al., 2017;\nSee et al., 2017; Vaswani et al., 2017; Gupta et al.,\n2018); sometimes such models are also used to\nevaluate paraphrasing quality (Thompson and Post,\n2020). Round-trip translation between two lan-\nguages (i.e., back-translation) with strong neural\nmachine translation (NMT) models has also be-\ncome a widely used approach for paraphrase gener-\nation (Yu et al., 2018). Consequently, supervised\nmodels using datasets like ParaNMT obtain their\nperformance mainly from sequence-level distilla-\ntion (Kim and Rush, 2016), where the data comes\nfrom the underlying supervised translation mod-\nels. There have been several previous works (Iyyer\net al., 2018b; Chen et al., 2019; Li et al., 2019;\nKumar et al., 2019; Goyal and Durrett, 2020) that\nmake use of syntactic structures to produce more di-\nverse paraphrases. More recently, Qian et al. (2019)\nemploy distinct generators to produce diverse para-\nphrases. Retrieval-augmented generation methods\nhave also been investigated (Kazemnejad et al.,\n2020; Lewis et al., 2020). However, most of these\napproaches require parallel data.\nUnsupervised Approaches Unsupervised para-\nphrasing, on the other hand, is a rather less explored\nand more challenging problem in NLP. Bowman\net al. (2016) train a variational autoencoder (V AE)\nto maximize the lower bounds for the reconstruc-\ntion log-likelihood of the input sentence without\nrequiring any parallel corpora. Sampling from the\ntrained V AE’s decoder leads to sentences that can\npractically be considered as paraphrases as the de-\ncoder aims to reconstruct the input sentence by its\ntraining objective. Miao et al. (2018) introduce a\nconstrained sentence generation approach by us-\ning Metropolis-Hastings sampling, which allows\nfor decoding with complicated discrete constraints\nsuch as the occurrence of multiple keywords, hence\nnot requiring any parallel corpora. Roy and Grang-\nier (2019) introduce a model that allows interpo-\n5144\nlation from continuous auto-encoders to vector-\nquantized auto-encoders. Liu et al. (2020) cast the\nparaphrasing as an optimization problem, where\nit searches the sentence space to ﬁnd the optimal\npoint for an objective function that takes semantic\nsimilarity, expression diversity, and language ﬂu-\nency into account. Siddique et al. (2020) optimize a\nsimilar objective with deep reinforcement learning.\nTransfer Learning There have been few works\nleveraging pre-trained language models for para-\nphrasing, either in a supervised (Witteveen and\nAndrews, 2019) or an unsupervised (Hegde and\nPatil, 2020) setting. Both works employ GPT-2 as\ntheir backbone generation model. Similarly, we opt\nfor more recent large-scale pre-trained models like\nBART and T5.\n7 Conclusion\nWe design an effective training pipeline that en-\nables large-scale pre-trained models to generate\nhigh-quality paraphrases in an unsupervised set-\nting through task-adaptation, self-supervision, and\na novel decoding algorithm named Dynamic Block-\ning. We demonstrate with automatic and human\nevaluations that our model achieves state-of-the-\nart results on benchmark datasets. We also show\nthat our model generates paraphrases that exhibit\nsyntactic diversity, as well as generalizes to other\nlanguages without any additional training. Over-\nall our work motivates a deeper investigation into\nself-supervised techniques for paraphrase genera-\ntion as well as extensions such as context-aware\nparaphrasing, where the output conditions not only\non the sentences to be paraphrased, but also on the\ncontext around them. We leave this as future work.\nReferences\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nColin Bannard and Chris Callison-Burch. 2005. Para-\nphrasing with bilingual parallel corpora. In Proceed-\nings of the 43rd Annual Meeting of the Association\nfor Computational Linguistics (ACL’05), pages 597–\n604.\nJonathan Berant and Percy Liang. 2014. Semantic pars-\ning via paraphrasing. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1415–\n1425.\nOndˇrej Bojar, Ondˇrej Dušek, Tom Kocmi, Jindˇrich Li-\nbovick`y, Michal Novák, Martin Popel, Roman Su-\ndarikov, and Dušan Variš. 2016. Czeng 1.6: en-\nlarged czech-english parallel corpus with processing\ntools dockered. In International Conference on Text,\nSpeech, and Dialogue, pages 231–238. Springer.\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Confer-\nence on Computational Natural Language Learning.\nZiqiang Cao, Chuwei Luo, Wenjie Li, and Sujian Li.\n2016. Joint copying and restricted generation for\nparaphrase. arXiv preprint arXiv:1611.09235.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nBrian Strope, and Ray Kurzweil. 2018. Universal\nsentence encoder for English. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 169–174, Brussels, Belgium. Association for\nComputational Linguistics.\nMingda Chen, Qingming Tang, Sam Wiseman, and\nKevin Gimpel. 2019. A multi-task approach for dis-\nentangling syntax and semantics in sentence repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers).\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7870–7881, Online. As-\nsociation for Computational Linguistics.\nAlexandra Chronopoulou, Christos Baziotis, and\nAlexandros Potamianos. 2019. An embarrassingly\nsimple approach for transfer learning from pre-\ntrained language models. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers).\nMark Davies. 2010. The corpus of contemporary amer-\nican english as the ﬁrst reliable monitor corpus of en-\nglish. Literary and linguistic computing, 25(4):447–\n464.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\n5145\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni.\n2013. Paraphrase-driven learning for open question\nanswering. In Proceedings of the 51st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1608–1618.\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni.\n2014. Open question answering over curated and ex-\ntracted knowledge bases. In Proceedings of the 20th\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining, pages 1156–1165.\nTanya Goyal and Greg Durrett. 2020. Neural syntactic\npreordering for controlled paraphrase generation. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 238–\n252, Online. Association for Computational Linguis-\ntics.\nAnkush Gupta, Arvind Agarwal, Prawaan Singh, and\nPiyush Rai. 2018. A deep generative framework for\nparaphrase generation. In AAAI Conference on Arti-\nﬁcial Intelligence.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nChaitra Hegde and Shrikumar Patil. 2020. Unsuper-\nvised paraphrase generation using pre-trained lan-\nguage models. arXiv preprint arXiv:2006.05477.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In The Ninth International Conference\non Learning Representations.\nJ. Edward Hu, Abhinav Singh, Nils Holzenberger, Matt\nPost, and Benjamin Van Durme. 2019. Large-\nscale, diverse, paraphrastic bitexts via sampling and\nclustering. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 44–54, Hong Kong, China. Associ-\nation for Computational Linguistics.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018a. Adversarial example gener-\nation with syntactically controlled paraphrase net-\nworks. arXiv preprint arXiv:1804.06059.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018b. Adversarial example gener-\nation with syntactically controlled paraphrase net-\nworks. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers) , pages 1875–1885,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAmirhossein Kazemnejad, Mohammadreza Salehi, and\nMahdieh Soleymani Baghshah. 2020. Paraphrase\ngeneration by learning how to edit from samples. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6010–\n6021.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1317–1327, Austin,\nTexas. Association for Computational Linguistics.\nAshutosh Kumar, Satwik Bhattamishra, Manik Bhan-\ndari, and Partha Talukdar. 2019. Submodular\noptimization-based diverse paraphrasing and its ef-\nfectiveness in data augmentation. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers), pages 3609–3619, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nWuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017.\nA continuously growing dataset of sentential para-\nphrases. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1224–1234, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nZichao Li, Xin Jiang, Lifeng Shang, and Hang Li.\n2017. Paraphrase generation with deep reinforce-\nment learning. arXiv preprint arXiv:1711.00279.\n5146\nZichao Li, Xin Jiang, Lifeng Shang, and Qun Liu.\n2019. Decomposable neural paraphrase generation.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n3403–3414, Florence, Italy. Association for Compu-\ntational Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nXianggen Liu, Lili Mou, Fandong Meng, Hao Zhou,\nJie Zhou, and Sen Song. 2020. Unsupervised para-\nphrasing by simulated annealing. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 302–312, Online. As-\nsociation for Computational Linguistics.\nHong-Ren Mao and Hung-Yi Lee. 2019. Polly want\na cracker: Analyzing performance of parroting on\nparaphrase generation datasets. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5960–5968, Hong Kong,\nChina. Association for Computational Linguistics.\nKathleen R McKeown. 1980. Paraphrasing using given\nand new information in a question-answer system.\nTechnical Reports (CIS), page 723.\nMarie Meteer and Varda Shaked. 1988. Strategies for\neffective paraphrasing. In Coling Budapest 1988\nVolume 2: International Conference on Computa-\ntional Linguistics.\nNing Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li.\n2018. Cgmh: Constrained sentence generation by\nmetropolis-hastings sampling.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nGeorge A Miller. 1998. WordNet: An electronic lexical\ndatabase. MIT press.\nTaesun Moon, Parul Awasthy, Jian Ni, and Radu\nFlorian. 2019. Towards lingua franca named\nentity recognition with bert. arXiv preprint\narXiv:1912.01389.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek\nDatla, Ashequl Qadir, Joey Liu, and Oladimeji Farri.\n2016. Neural paraphrase generation with stacked\nresidual LSTM networks. In Proceedings of COL-\nING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers, pages\n2923–2934, Osaka, Japan. The COLING 2016 Orga-\nnizing Committee.\nLihua Qian, Lin Qiu, Weinan Zhang, Xin Jiang, and\nYong Yu. 2019. Exploring diverse expressions\nfor paraphrase generation. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for\nComputational Linguistics.\nAurko Roy and David Grangier. 2019. Unsupervised\nparaphrasing without translation. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6033–6039, Florence,\nItaly. Association for Computational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers).\nThibault Sellam, Dipanjan Das, and Ankur Parikh.\n2020. BLEURT: Learning robust metrics for text\ngeneration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7881–7892, Online. Association for Computa-\ntional Linguistics.\nHiroki Shimanaka, Tomoyuki Kajiwara, and Mamoru\nKomachi. 2018. RUSE: Regressor using sentence\nembeddings for automatic machine translation eval-\nuation. In Proceedings of the Third Conference on\nMachine Translation: Shared Task Papers , pages\n751–758, Belgium, Brussels. Association for Com-\nputational Linguistics.\nAditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Fi-\nrat, Mia Chen, Sneha Kudugunta, Naveen Arivazha-\ngan, and Yonghui Wu. 2020. Leveraging mono-\nlingual data with self-supervision for multilingual\nneural machine translation. In Proceedings of the\n5147\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 2827–2835, Online. As-\nsociation for Computational Linguistics.\nAB Siddique, Samet Oymak, and Vagelis Hristidis.\n2020. Unsupervised paraphrasing via deep rein-\nforcement learning. In Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge\nDiscovery & Data Mining, pages 1800–1809.\nHong Sun and Ming Zhou. 2012. Joint learning of a\ndual SMT system for paraphrase generation. In Pro-\nceedings of the 50th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 38–42, Jeju Island, Korea. Associa-\ntion for Computational Linguistics.\nBrian Thompson and Matt Post. 2020. Automatic ma-\nchine translation evaluation in many languages via\nzero-shot paraphrasing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 90–121, Online.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nAshwin Vijayakumar, Michael Cogswell, Ramprasaath\nSelvaraju, Qing Sun, Stefan Lee, David Crandall,\nand Dhruv Batra. 2018. Diverse beam search for\nimproved description of complex scenes. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 32.\nJohn Wieting and Kevin Gimpel. 2018. ParaNMT-\n50M: Pushing the limits of paraphrastic sentence em-\nbeddings with millions of machine translations. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 451–462, Melbourne, Australia.\nAssociation for Computational Linguistics.\nSam Witteveen and Martin Andrews. 2019. Paraphras-\ning with large language models. arXiv preprint\narXiv:1911.09661.\nWei Xu, Alan Ritter, and Ralph Grishman. 2013. Gath-\nering and generating paraphrases from twitter with\napplication to normalization. In Proceedings of the\nsixth workshop on building and using comparable\ncorpora, pages 121–128.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.\nPengcheng Yin, Nan Duan, Ben Kao, Junwei Bao, and\nMing Zhou. 2015. Answering questions with com-\nplex semantic constraints on open knowledge bases.\nIn Proceedings of the 24th ACM International on\nConference on Information and Knowledge Manage-\nment, pages 1301–1310.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V .\nLe. 2018. QANet: Combining Local Convolution\nwith Global Self-Attention for Reading Comprehen-\nsion. In 6th International Conference on Learning\nRepresentations (ICLR).\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\n5148\nModel\nQuora\niBLEU BLEU ROUGE-1 ROUGE-2\nSupervised\nResidualLSTM 12.67 17.57 59.22 32.40\nV AE-SVG-eq 15.17 20.04 59.98 33.30\nPointer-generator 16.79 22.65 61.96 36.07\nTransformer 16.25 21.73 60.25 33.45\n+ Copy 17.98 24.77 63.34 37.31\nDNPG 18.01 25.03 63.73 37.75\nSupervised (Wiki)\nPointer-generator 5.04 6.96 41.89 12.77\nTransformer + Copy 6.17 8.15 44.89 14.79\nShallow fusion 6.04 7.95 44.87 14.79\nMulti-task learning 4.90 6.37 37.64 11.83\n+ Copy 7.22 9.83 47.08 19.03\nDNPG 10.39 16.98 56.01 28.61\nUnsupervised\nV AE 8.16 13.96 44.55 22.64\nCGMH 9.94 15.73 48.73 26.12\nUPSA 12.02 18.18 56.51 30.69\nPUP 14.91 19.68 59.77 30.47\nCorruptLM 12.08 16.80 49.13 26.15\nTA 16.02 21.18 61.90 35.07\nTA+SS 15.57 20.68 61.51 32.78\nTA+SS+DB 9.67 14.12 60.06 28.91\nNo model Copy-input 24.79 30.98 65.60 42.09\nTable 8: Automatic evaluation results on the QQP\ndataset. Models we (re)produced and SOTA results\nin each category are boldfaced. “Supervised (Wiki)”\nstands for models trained on WikiAnswers and evalu-\nated on QQP.\nModel\nOracle Quality (10 sentences)\nBLEU ROUGE-1 ROUGE-2 ROUGE-L\nSupervised\ncopy-input 18.4 54.4 27.2 49.2\nSCPN 21.3 53.2 30.3 51.0\nTransformer seq2seq 32.8 63.1 41.4 63.3\n+ diverse-decoding 24.8 56.8 33.2 56.4\nSOW-REAP (LSTM) 27.0 57.9 34.8 57.5\nSOW-REAP 30.9 62.3 40.2 61.7\nUnsupervised\nCorruptLM (QQP) 7.6 31.9 11.6 31.6\nTA+SS+DB (QQP) 22.0 60.1 33.8 54.9\nTA+SS+DB 22.6 59.8 33.2 54.5\nTable 9: Automatic metrics results on the Para-NMT\ndataset. “(QQP)” stands for models ﬁnetuned on\nthe non-parallel QQP dataset and evaluated on the\nParaNMT dataset.\nA Automatic Metric Results\nWe present automatic evaluation results on the\nprevious metrics for QQP in Table 8 and for\nParaNMT in Table 9. We can see that for QQP our\ntask-adaptation model without Dynamic Blocking\nduring inference achieves state-of-the-art results\namong unsupervised approaches. Had we based\nour judgments on Table 8, we would have mistak-\nenly selected this one as our ﬁnal model.\nB Robustness to Grammar Errors\nDuring the task-adaptation phase, our model in\nmost cases has a grammatically correct sentence as\nthe target sequence. Additionally, shufﬂing during\nthat phase encourages the model to attend to the\ncontext during generation. These setups make our\nmodel reasonably robust to grammar errors so that\nit can paraphrase and normalize the input at the\nsame time. Table 10 shows a case where we inten-\ntionally introduce grammar errors on subject-verb\nagreement, singular vs. plural, and verb inﬂections.\nInput Our approach are data-driven and can be apply across various situation.\nOutput\nOur approach is data-driven and can be applied across various situations.\nOur approach is data-driven and can be applied across different situations.\nOur approach is data-driven and can be applied across diverse situations.\nOur approaches are data-driven and can be applied across various situations.\nOur data-driven approach can be applied across different situations.\nOur approaches are data-driven and can be applied across different situations.\nOur data-driven approach can be applied across diverse situations.\nOur approaches are data-driven and can be applied across diverse situations.\nTable 10: Selected example of output candidates pro-\nduced by our model where we intentionally introduce\ngrammar errors (marked with underlines). We observe\nthat all paraphrase candidates have these errors cor-\nrected.\nWe ﬁnd that our model is in most cases robust to\nsuch errors. This trait is desired because we may\nface noisy inputs from users. Through early ab-\nlation studies, we observed that without shufﬂing\nduring task-adaptation, the model was much less ro-\nbust to grammar errors. Hence shufﬂing does more\nthan just improving on the BERT- iBLEU metric\n(Table 6).\nC Failure Modes\nThough only occurring occasionally, our model ex-\nhibits multiple failure patterns. Hence we perform\n“anti-cherry-picking” and present in Table 11 some\nof such examples and the respective modes we out-\nline. We hypothesize that the Antonym mode can\nbe partially addressed by a lookup in the dictionary\nto additionally block the antonyms. Grammar er-\nrors are harder to resolve because they are usually\napparent only after the whole sentence is generated.\nA grammar checker on the candidates may improve\nthe situation. The swapping of subject and object\nshows that unsupervised approaches based on pre-\ntrained language models could only carry us so far\ntill the syntactic-level. In its current form, it cannot\nhandle semantic mistakes. For missing named enti-\nties, an NER tagger can help ﬁlter candidates that\nmiss important entities. We leave addressing these\nfailure modes as future work.\nD Details of Dynamic Blocking\nBlock surface-form variations In our early ex-\nperiments, we observed that when blocking a word\n(e.g. “give”), the model usually tries to generate\nits capitalized (“Give”) or upper (“GIVE”) version.\nFrom we human’s perspective, these are usually\nnot good paraphrases – intuitively we would prefer\na different word. Similar to whole-word masking\nintroduced in later versions of BERT, 19 we only\n19https://github.com/google-research/\nbert\n5149\nFailure mode Input Output\nAntonym How do I gain weight in a healthy way? How do I lose weight in healthy ways?\nRepeated words What is the funniest movie to watch? What is the most funniest ﬁlm to see?\nGrammar errors Do spirits or ghosts exist? Do ghost or spirit exist?\nSubject ↔object How will you know you love someone? How will you tell if someone loves you?\nMissing named entity A look of dismay came into luzhin ’s face. A look of disappointment came into the face.\nTable 11: Typical examples where our model fails to generate correct paraphrases. Words related to each failure\nmode are underlined.\nblock the beginning of the word rather than any\nsubword.\nBlock Closed-Class Words We also leverage\nlinguistic knowledge to help boost the quality of\nthe paraphrases by avoiding blocking closed-class\nwords, or functional words.20 The closed classes\nin English include pronouns, determiners, conjunc-\ntions, and prepositions while open-class words cor-\nrespond to nouns, lexical verbs, adjectives, and\nadverbs. There are two justiﬁcations for blocking\nthese words. First, because they are closed-class,\nthere are fewer synonyms available; second, block-\ning such words is error-prone. For example, chang-\ning determiners (e.g. from “you” to “I”) may lead\nto syntactic or semantic errors, while modifying\nconjunctions (e.g. from “and” to “or”) may lead to\nchange in logical relationships.\nBlock Inﬂections In Section 5.2, we mentioned\nthat BART can directly work with Dynamic Block-\ning without task-adaptation or self-supervision, but\nthat results in lower quality, especially lacking syn-\ntactic variance because it is not trained with the\nshufﬂing strategy during task-adaptation. In ad-\ndition, we found that without ﬁnetuning, BART\ntries to generate inﬂections of a word when it is\nblocked. To partially remedy this drawback, we use\nthe pattern library21 to enumerate all inﬂections of\na word to block (e.g. for “ give” we should also\nblock “gives”, “gave”, “giving” and “given”) in ad-\ndition to all the other blocking schemes introduced\nin Section 3. This is available for most languages\nthat involve inﬂections. We show in Table 12 the\noutput candidates of a selected example with and\nwithout blocking inﬂections.\nRetain Named Entities We also explore a vari-\nation of the system where we employ a separate\nNamed Entity Recognition model to identify the\nnamed entities in the source sequence and prevent\n20https://mailman.uib.no/public/\ncorpora/attachments/20111124/6c58cb02/\nattachment.txt\n21https://github.com/clips/pattern\nany tokens in these entities from appearing in the\nfull block dictionary. This change ensures that all\nnamed entities are copied verbatim.\nE Paraphrasing in German\nWe pair BART directly with Dynamic Blocking\nto generate paraphrases in German. In Table 13,\nwe can see that all candidates (left column) have\ndifferent surface forms, while all translations in\nEnglish (right column)22 share similar meanings.\nF MTurk Instructions\nTo facilitate reproducibility, we include our MTurk\ninstructions for the head-to-head and the Likert-\nbased human studies (Figure 3 and 4). As men-\ntioned in Section 3.5, we only provide guidelines\non which paraphrases are better in general and\nleave the rest to the annotator’s intuition.\n22By Google Translator: https://translate.\ngoogle.com/\n5150\nInput The random selection of pages must be performed by someone other than the player.\nOutput\nBlocking inﬂections\nThe random choice of the pages must be performed by someone else than the player.\nThe random selection of the pages must be performed by someone else than the user.\nThe random selection of the pages must be executed by someone other than the user.\nThe random collection of these pages must be performed by someone else than the player.\nThe random selection of these pages must be executed by someone other than the user.\nNo blocking inﬂections\nThe randomly selection of page must be perform by someone else than the players.\nThe random choice of page must be performed by someone else than the player.\nThe randomly selection of page must be perform by someone rather than the players.\nThe random choice of page must be performed by someone rather than the player.\nThe random collection of pages must be performed by someone else than the players.\nTable 12: Selected example of output candidates produced by BART + Dynamic Blocking with and without block-\ning inﬂections. Grammar errors made by the latter due to wrong inﬂections are underlined.\nGerman Translation from German\nInput Warum ﬁnden keine Brandschutzbelehrungen statt ? Why are there no ﬁre instructions?\nCandidates\nWarum lieen keine Geschutzbelehrungen statt? Why were there no protection instructions?\nWarum ﬁnden keine Geschutzbelehrungen statt? Why are there no protection instructions?\nWarum lieen keine Brandschutzbelehrungen statt? Why weren’t there any ﬁre safety instructions?\nWarum ﬁnden keine Geschutzbelehrungen statt? Why are there no protection instructions?\nWarum ﬁnden wir keine Brandschutzbelehrungen statt? Why are we not giving ﬁre safety instructions?\nTable 13: Paraphrasing German input by directly applying Dynamic Blocking to BART. Translations on the right\nare given by the Google Translator, except that the ﬁrst one is the ground-truth translation. Note that the candidates\nare ranked by multi-lingual BERT rather than RoBERTa-base which is only used to rank English outputs.\nFigure 3: Interface of our MTurk studies for head-to-head comparisions with other models.\nFigure 4: Interface of our MTurk studies for head-to-head comparisions with other models.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8783259987831116
    },
    {
      "name": "Paraphrase",
      "score": 0.8107121586799622
    },
    {
      "name": "Pipeline (software)",
      "score": 0.7297300696372986
    },
    {
      "name": "Security token",
      "score": 0.7025368213653564
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6469073295593262
    },
    {
      "name": "Language model",
      "score": 0.6464380025863647
    },
    {
      "name": "Natural language processing",
      "score": 0.5894879102706909
    },
    {
      "name": "Decoding methods",
      "score": 0.5669677257537842
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5528962016105652
    },
    {
      "name": "Task (project management)",
      "score": 0.5425546765327454
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4768460988998413
    },
    {
      "name": "Transfer of learning",
      "score": 0.46019840240478516
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4554221034049988
    },
    {
      "name": "Machine learning",
      "score": 0.4404156804084778
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4153084456920624
    },
    {
      "name": "Algorithm",
      "score": 0.14414232969284058
    },
    {
      "name": "Programming language",
      "score": 0.12120962142944336
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210155268",
      "name": "Salesforce (United States)",
      "country": "US"
    }
  ]
}