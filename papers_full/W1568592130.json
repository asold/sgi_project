{
  "title": "Position / Force Hybrid Control of a Manipulator with a Flexible Tool Using Visual and Force Information",
  "url": "https://openalex.org/W1568592130",
  "year": 2005,
  "authors": [
    {
      "id": "https://openalex.org/A5066790771",
      "name": "Jian Huang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5020942077",
      "name": "Isao Todo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5018894428",
      "name": "Tetsuro Yabut",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2150254119",
    "https://openalex.org/W2335642528",
    "https://openalex.org/W302761957",
    "https://openalex.org/W2325696102",
    "https://openalex.org/W2322792318",
    "https://openalex.org/W2145405144",
    "https://openalex.org/W2323224157",
    "https://openalex.org/W2320185712",
    "https://openalex.org/W1844670856",
    "https://openalex.org/W1935175493",
    "https://openalex.org/W2508888294",
    "https://openalex.org/W2116645259",
    "https://openalex.org/W2517084686",
    "https://openalex.org/W2329704946",
    "https://openalex.org/W2081179887",
    "https://openalex.org/W2336026487",
    "https://openalex.org/W2097824192",
    "https://openalex.org/W285212305",
    "https://openalex.org/W2113283711",
    "https://openalex.org/W2498864155"
  ],
  "abstract": "Several experiments have been performed to prove the effectiveness of the proposed method as shown in Fig.9. The control parameters used in the experiments are given in Table 1.",
  "full_text": " 611\n \n \nPosition / Force Hybrid Control of a Manipulator \nwith a Flexible Tool Using Visual  \nand Force Information \n \nJian Huang, Isao Todo & Tetsuro Yabuta \n \n \n  \n1. Introduction \n \n1.1 Background \n \nRobots used for industrial applications such as welding, painting and object handling have \nbeen common for many years. In recent years , the development of domestic robots has \nbecome more and more important because of the large and growing population of aged \npeople, especially in Japan. To assist people in their daily lives, a robot must have the ability \nto deal with not only rigid objects but also  the deformable and fragile objects usually \nencountered in our daily life. Many contro l algorithms have been developed for the \nmanipulation of rigid objects, and in the recent past many studies related to robotic \nmanipulation of deformable objects have also been reported (Hirai, 1998).  \n \n1.2 Recent Researches on Manipulating Deformable Objects \n \nIn recent years, the robotic manipulation of flexible objects has been demonstrated in a \nvariety of applications. Manipulation of a deformable tube based on human demonstration \n(Hirai & Noguchi, 1997), wire manipulation (Nakagaki et al., 1997, Nakagaki, 1998), a study \nof manipulating a linear objects (Remde et  al., 1999, Acker & Henrich, 2003, Schlechter & \nHenrich, 2002, Schmidt & Henrich, 2001, Yue & Henrich, 2002), one-handed knotting \nmanipulation (Wakamatsu et al., 2002, 2004), hand ling of fabric objects (Ono, 1998), ticket \nhandling (Itakura, 1998) and contact task on a fl exible plate (Wu, et. al., 1997) have been \ndeveloped,.  \nIn general, deformable objects display a wide range of responses to applied forces because \nof their different physical properties. Therefore, different control strategies are required for \na robot to manipulate different kind of object s. Deformation model analysis is one of the \nfundamental methods available. Trials have been made of methods based on establishing a \ndeformation model for robot manipulation (Wakamatsu & Wada, 1998, Hisada, 1998), and a \nstate-transition method (Henrich , et. al., 1999, Remde, et al., 1999, Abegg, et al., 2000).  \nWhen a human manipulates a deformable ob ject, he will actively combine visual \ninformation from his eyes with contact fo rce information obtained by his hands. \nVision-based state detection for a linear obje ct (Acker & Henrich, 2003, Abegg, et al., 2000) \nand visual tracking of a wire deformation (Chen & Zheng, 1992, Nakagaki et al., 1997) have \nbeen reported. \nSource: Cutting Edge Robotics, ISBN 3-86611-038-3, pp. 784, ARS/plV, Germany, July 2005 Edited by: Kordic, V.; Lazinica, A. & Merdan, M.\nOpen Access Database www.i-techonline.com\n 612\nFurthermore, a control method based on high -speed visual detection and visual/force \nsensor fusion was proposed to enable a robot to complete the task of inserting an aluminum \npeg held by a manipulator into an acrylic hole fixed on a deformable plate (Huang & Todo, \n2001). In a continuing study, a stereovision-based compliance control enabling a robot arm \nto manipulate an unknown deformable beam  object (Huang, et al., 2003) has been \ndemonstrated, in which stereovision and forc e information were combined by an online \nlearning neural network so that the robot co uld adjust its position and orientation in \nresponse to the deformation state of the unknown beam object. \n \n1.3 Description of the Study in this Chapter \n \nA robot with a flexible tool is considered the first choice for manipulating a fragile object. \nHowever, in contrast with the extensive lite rature on robot manipulation of rigid and \ndeformable objects, the study of how to contro l a robot arm with a flexible tool has been \nneglected. Therefore, a great need exists to develop fundamental control methods for robots \nwith flexible tools.  \nWhen a robot arm with a flexible tool is used to complete a contact task, deformation of the \nflexible tool always occurs so that the task can not be adequately completed by using solely \nthe type of control methods designed for a robo t with a rigid tool. As the position of the \ntool’s tip can not be computed from the ro bot’s kinematics, establishing a deformation \nmodel of the flexible tool is generally needed to  estimate the position of the flexible tool’s \ntip. \nIn this chapter, a position/force hybrid control method that incorporates visual information \nis proposed to enable a robot arm with a flexib le tool to complete a contact task. To detect \nthe position of the flexible tool’s tip, two CCD cameras are used and a real time image \nprocessing algorithm is developed. Furthermor e, an online learning neural network is \nintroduced to the position/force hybrid contro l loop so as to improve the tracing accuracy \nof the flexible tool. An advantage of the proposed method is that establishing a deformation \nmodel of the flexible tool is not necessary.  \nWith the proposed method, a flexible tool held by the end-effector of a 6 DOF robot is used \nto trace a given curve with a specified vertical pressing force.  \n \n2. Overview of the Robot System \n \nThe robot control system considered in this  chapter includes a manipulator with seven \ndegrees of freedom (PA-10, Mitsubishi Heavy Industry Co.), a force/torque sensor (10/100, \nB.L. Autotec Ltd.) for force detection, two compact CCD cameras (XC-EI50, Sony Co.) for \nstereo visual detection and a personal co mputer (Dimension XPS B733r, Dell Co.) for \nalgorithm calculation. However, the redundan t joint of the manipulator is fixed. A \nschematic diagram of the robot system is shown in Fig.1. \nAs shown in Fig.1, a cylindrical tube made from polyvinyl chloride (10.7 mm diameter) is \nused as the tool, and is held by the end-effector of the mani pulator. The tip of the flexible \ntool is wrapped with a light-reflective tape for image detection. A infrared ring type \nradiators (IRDR-110, Nissin Electronic Co.) is set up in front of each camera’s lens. \nThe required task is for the flexible tool he ld by the robot to trace a given curve with a \nspecified pressing force. As deformation of th e flexible tool always occurs on contact, \ncontrol of the flexible tool’s tip to make it trace a desired trajectory is very difficult.  \nIn order to estimate the tool’s tip position , establishing a deform able model is usually \nconsidered. \n 613\nRobot\n(PA-10)\nCCD camera 1\nForce/Torque sensor\n(F/T 10/100)\nImage processing board\n(Genesis)\nComputer\nyb\nxb\nzb Σb\nCCD camera 2\nReflective marker\nFlexible tool\nDesired trajectory\nxh\nyh\nzh\nΣh\n \n \nFigure 1.The schematic diagram of the robot system with a flexible tool \n 614\nTherefore, some physical parameters like the stiffness and Young's modulus of the flexible \ntool are needed for model analysis. Howev er, such physical parameters are usually \nunknown for many of the materials encountered in daily life. \n \n3. Method of Image Processing  \n \n3.1 Real Time Image Processing \n \nAs shown in Fig.1, two synchronized compact cameras are used to construct a stereovision \nsystem. Each camera outputs im ages at the rate of 30 frames  per second. For image input \nand processing, a graphic board with a C80 digital signal processor (GENESIS \nGEN/F/64/8 /STD, Matrox Co.) is installed in the personal computer. \nGenerally, real time image processing means that one frame image from a camera must be \nprocessed within the camera’s frame time so that image input and processing can be carried \nout simultaneously. Real time image processing is commonly achieved with parallel \nprocessing. \nIn this chapter, we use a method called do uble buffering to achieve parallel image \nprocessing. As shown in Fig.2, images are su ccessively output from cameras 1 and 2. At \ntimes n = 0, 2, 4, ···, new images from cameras  1 and 2 are taken into buffers 11 and 21 \nrespectively, while the last images saved in buffers 12 and 22 are processed. At times n = 1, 3, \n5, ···, functions of the buffers are  exchanged. Images from cameras 1 and 2 are taken into \nbuffers 12 and 22 respectively, while the la st images saved in buffers 11 and 21 are \nprocessed.  \nBy using the parallel processing, a stereo im age can be processed within the camera frame \ntime Tc (33.33 ms). Therefore, the tool tip’s position 31()t nR\n×∈p  (n = 0, 1, 2, ···) in the robot \nbase coordinate Σb can be detected at each camera’s frame time T c. Thus, real time image \nprocessing is achieved. \n \n \nImage\nbuffer 22\nImage\nbuffer 12\nImage\nbuffer 11\nImage\nbuffer 21\nDSP embedded image procesing board GENSIS\nTc\nBuffer\nselectorA/D\nData bus\nTc\nCamera 1\nCamera 2\nn=0,2,4,6...\nn=1,3,5,7...\nn=0,2,4,6...\nn=1,3,5,7...\nA/D Buffer\nselector\n \nFigure 2. Parallel image processing in GENESIS using double buffering method \n 615\n(1 ) (2 )(1 ) ( (1 ) )\n             ( ( 1) ), \n() ( 1 )() ( )\n( )              (( 1) < ( 2) ),\n(1 ) ( ) (1 ) ( (1 ) )\n                           \ntt\ntc\nc\ncc\ntt\ntc\nc\ntc c\ntt\ntc\nc\nnnnk T n T T\nnT kT n T\nnnnk T n T T\nkn T kT n T\nnnnk T n T T\n− −−−+ − −\n≤< +\n−−+−\n=+ < +\n+−++ − +\nppp\nppp\np\nppp\n      (( 2) < ( 3) ),\n                            \nccnT k T nT\n⎧\n⎪\n⎪\n⎪\n⎪\n⎪\n⎪\n⎪\n⎪\n⎨\n⎪\n⎪\n⎪\n⎪ +< +⎪\n⎪\n⎪\n⎪\n⎩\n\"\"\"\"\"\"\"\"\"\"\n3. 2 Interpolation of the Image Detection Results \n \nThe robot control sampling period T is 5 milliseconds. At time t = k T, the position of the \nflexible tool’s tip in the robot base coordinate Σb is assumed to be 31()t kR ×∈p . By using the \nreal time image processing method described in section 3.1, the tool’s tip position ()t np  in \nΣb can be detected within the camera frame time T c. However, the image processing \nsampling period Tc is still longer than the robot contro l sampling period T. A time chart of \nimage processing and robot control is shown in Fig. 3, where T n (100 ms) is the least \ncommon multiple of T c and T. Therefore, interpolation is necessary to convert the result \n()t np  of image processing to ()t kp  at every sampling period T for robot control. In the \nperiod Tn, the position ()t kp  can be calculated by \n \n \n \n \n \n \n \n \n (1) \n \n \n \n \n \n \n \n \n                                 (k = 0,1,2, ···),(n = 0,1,2, ···) . \n \nIn equation (1), (1 )t −p  and (2 )t −p  are set equal to 0 for n = 0.  \n \nTc\nT\nImage\nsynchronization\nRobot control\nsynchronization\nTn\nkT kT+T c kT+ 3TckT+ 2Tc  \n \nFigure 3. The time chart of image processing and robot control \n 616\n4. Generation of Desired Trajectory \n \nIf a rigid tool held by a manipulator is used to complete the task as shown in Fig.1, the tool \nwill not deform, so the position of the tool’s tip can easily be calculated from the kinematics \nof the manipulator. However, when a flexible tool is used, it deforms on contact so that the \nposition of the tool’s tip cannot be dire ctly computed from the kinematics of the \nmanipulator. To explain the principle of the generation of the desired trajectory, a model is \nassumed as shown in Figs.4 and 5, where the tool’s tip moves along the x axis with a \nconstant pressing force in the z direction.  \n \n \nL\nd d1\nd d\nptd(k+1)\npt(k)pt(k−1)\nphd(k+1)ph(k)ph(k−1)\n∆z\nz0\nL0L0\npt(k+1)\nPosition of end\neffector of the arm\nTip position of the\nflexible tool\nMoving\ndirection\nx\nz\ny\n \n \nFigure 4. Moving end-effector of the robot forward with a distance d \n \nd d\nd d2\nptd(k+1)\npt(k)pt(k−1)\nphd(k+1)ph(k)ph(k−1)\n∆z\nz0\nL0L0\nPosition of end\neffector of the arm\nTip position of the\nflexible tool\nMoving\ndirection L0\nφ\nx\nz\ny\n \n \nFigure 5. Moving the flexible tool forward with a distance d \n 617\nFurther assumptions of the task are the following: \n- The desired trajectory traced by the flexible tool is in a given plane. \n- The orientation around the y axis of the end-e ffector shown in Figs.4 and 5 is kept to be \nunchanged at its initial value until the task is completed.  \n \n4.1 Desired Trajectory Generation of the Robot’s End-Effector \n \nAt time t = k T, when the end-effector of the robot moves by a distance d from position \nphd(k) ∈R3×1 to position phd(k+1) as shown in Fig.4, the variation ∆z along the z axis of the \nend-effctor caused by deformation will lead to the result that the flexible tool’s tip moves by \nthe distance d1 from position pt(k)∈R3×1 to position pt(k+1)  rather than to the desired \nposition ptd(k+1).  \nAs shown in Fig.5, in order to move the tool’s tip to the desired position ptd(k+1) ahead of \nposition pt(k) by the distance d, the end-effector of the arm should be moved by the distance \nd2 from position ph(k) to position phd(k+1) to compensate for the position error caused by \nthe variation ∆z of the robot end-effector.  \nHere, we assume that the distance between the tool tip and the end-effector is L0 when the \ntool presses against the contact surface with a constant force fzd. At time t = kT, the desired \nposition of the tool’s  tip is expressed as ptd(k+1)=[x td(k+1),  y td(k+1),  z td(k+1)]T, and \nthe angle between line L0 and the x-y plane is \nφ. Thus, the moving direction vector ()td k\u0004p  of \nthe tool’s tip can be computed by \n                                 \n(1 ) ( )() td td\ntd\nkkk d\n+ −=\u0004 ppp\n, (2) \n \nwhere d is given by \n                   \n22(( 1 ) ( ) ) (( 1 ) ( ) )td td td tddx k x k y k y k=+ −+ + − . (3) \n \nThe projection vector ∆pL(k) ∈R3×1 of the tool on the x-y plane is \n \n                                    0( ) cos( ) ( )Lt dkL k\n∆φ = ⋅ \u0004pp , (4) \nwhere angle Φ  can be calculated by  \n                                       \n1 0\n0\nsin ( )zz\nL\n∆φ − −=\n (5) \nIf the position error ∆pt(k) of the tool’s tip is considered, the end-effector’s desired position \nin the x-y plane is given by \n                                () ( 1 ) () ()xy\nhd td L tkk k k\n∆∆=+ + +pp p p ,  (6) \nTherefore, the desired position phd(k) of the end-effector is generated by \n                                 11(1 ) ( ) xy\nhd hd dkk z+ =⋅ +⋅pS ps ,  (7) \nwhere zd is equal to the initial coordinate z0, and matrix S1 and vector s1 are given by \n \n                                      \n1\n100\n010\n000\n⎡ ⎤\n⎢ ⎥= ⎢ ⎥\n⎢ ⎥⎣ ⎦\nS\n,  (8) \n                                       1 [0 0 1] T=s . (9) \n 618\n4.2 Desired Orientation of the Arm’s End-Effector  \n \nAt time t = kT, the desired orientation of the end-effector is expressed as \n                                 () [ () () () ] T\nhd d d dkk k k\nαβγ=r , (10) \nwhere ǂd(k), ǃd(k) and γ d(k) are the rotation angle of the end-effector around the xb axis, \nyb axis and zb axis respectively, defined by the x-y-z fixed-angle method. If the end-effector \npushes the tool to move along the desired curv e as shown in Fig.6, a twisting moment will \ncause the rotation of the tool so that the tool’s tip position will be uncontrollable.  \n \n \npt(k)pt(k+1)ptd(k+1)\nph(k)\nphd(k+1)\nTwist moment\nDesired trajectory\nMoving direction of\nthe robot end effector\n \nFigure 6. Rotation of the flexible tool caused by twist moment \n \nMoving direction of\nthe robot end effector\npt(k)ptd(k+1)\nph(k)\nphd(k+1)\nDesired trajectory\n \nFigure 7. Rotation avoidance by pulling the flexible tool \n 619\nIn order to avoid rotation of the tool, a me thod of pulling the flexible tool with the \nend-effector is determined as shown in Fig .7. Therefore, the desired orientation of the \nend-effector is computed by \n                                \n1 ()() [ t a n ( ) ] ()\nTtd\nhd d d\ntd\nykk xkαβ −=r\n (11) \nThen, by equations (7) and (11), the desired position and orientation prd(k+1)\u001f\u001fR6×1 of the \nend-effector can be calculated by \n                                (1 ) ( ) ( )rd p hd r hdkk k+= ⋅ + ⋅pS p S r , (12) \nwhere  \n \n                          \n100\n010\n001\n000\n000\n000\np\n⎡⎤\n⎢⎥\n⎢⎥\n⎢⎥= ⎢⎥\n⎢⎥\n⎢⎥\n⎢⎥\n⎢⎥\n⎣⎦\nS\n, \n000\n000\n000\n100\n010\n001\nr\n⎡ ⎤\n⎢ ⎥\n⎢ ⎥\n⎢ ⎥= ⎢ ⎥\n⎢ ⎥\n⎢ ⎥\n⎢ ⎥⎢ ⎥⎣ ⎦\nS\n. \n \n4.3 Desired Pressing Force \n \nAs a constant force is required for the tip of the flexible tool to press the contact surface, the \ndesired force f(k) is determined by \n                                   \n[ ]() 0 0\nT\nzdkf=f ,  (12) \nwhere fzd is the constant force along the zb axis. If the end-effector pushes the tool as shown \nin Fig.6, a large pressing force acting between  the tool’s tip and th e contact surface will \nincrease friction so as to easily bring about rotation of the tool caused by the twist moment. \n \n5. Hybrid Control Algorithm \n \n5.1 Online Learning Neural Network \n \nTo complete the required task, position cont rol in the xb-yb plane and force control along \nthe zb axis are considered. However, unspecified factors such as friction at the tool’s tip and \ndeformation of the tool will impair the tracing accuracy of the tool.  \nTo improve the tracing accuracy of the tool’s  tip in the xb-yb plane, two online learning \nneural networks are introduced, one each for th e x and y directions as shown in Fig.8. For \nthe neural network NNx, the desired position xhd(k), desired velocity ()hd\nx k\u0005  and desired \nacceleration ()hdx k\u0005\u0005  are used as the input quantities . Just as in the NNx, the desired \nposition yhd(k), desired velocity ()hdyk\u0005  and desired acceleration ()hdyk\u0005\u0005  are used as the \ninput quantities for the neural network NNy. The position errors of ∆xr(k) on the xb axis \nand ∆yr(k) on the yb axis are used as error signals for the neural network’s learning.  \nThe neural networks NNx and NNy have same 3- layer structure. The neurons in the input \nlayer, hidden layer and output layer are de signated NA, NB and NC respectively. The \nsigmoid function F(x) is given as  \n 620\n                                      \n/\n/\n1() 1\nx\nx\neFx e\nµ\nµ\n−\n−\n−=\n+ ,  (13) \nwhere µ is the annealing parameter. Learning of the weighting coefficients is obtained with \nback propagation method [Cichocki & Unbehauen, 1993]. \n \nxhd(k)\n()hdxk\u0005\n()hdxk\u0005\u0005\nyhd(k)\n()hdy k\u0005\n()hdy k\u0005\u0005\nNN x\n()rxk∆\nNN y\n()ryk∆\nMultilayer\nneural\nnetwork\nMultilayer\nneural\nnetwork\n \n \nFigure 8. The proposed online learning neural networks \n \n5.2 The Proposed Hybrid Control Method \n \nA position/force hybrid control method us ing two online learning neural networks is \nproposed to enable a robot with a flexible t ool to trace a given curve. The control block \ndiagram is shown in Fig.9, where Λ is the kinematics, J is the Jacobian and Rf is the rotation \nmatrix for calculating fz from fenv(k) detected by the force/torque sensor.  \nMatrix S2 and vector sf are given as \n \n                  \n2\n100000\n010000\n⎡⎤= ⎢⎥\n⎣⎦\nS\n,        \n[ ]001000\nT\nf =s  \nThe PID controller Gp(z) of the position loop is networks \n \n                               \n1() ( 1 ) 1\npp p\npP I D\nzz zz\n−=+ + − −GK K K\n, (14) \nand the PID controller Gf(z) of the force loop is \n \n                               \n1() ( 1 ) 1\nff f\nfP I D\nzGz K K K z z\n−=+ + − −  (15) \n \nFor the position control loop in Fig.9, at time t = kT, interpolation is made by equation (1) for \nthe image detection result p(n −1) so as to obtain the tool tip’s position pt(k). After the \nprojection of the vector ∆pL(k) on the x-y plane is calculated with equation (4), the desired \nposition phd(k) can be generated by equation (7). Thus, the desired position and orientation \nprd(k) are computed by equation (12). \nPosition errors between the desired positi on prd(k) and the present position pr(k) \ncalculated from the kinematics are used as error signals for the neural network’s learning.  \n 621\n()hd k\u0005p\n()d k\u0005θ\nΛ\n(1 )td k +p\n()td kp\n()hd kr\n()hd kp\n()hdxk\u0005\n()hdxk\u0005\u0005\n()hdyk\u0005\n()hdyk\u0005\u0005\n()xy\nhd kp\n()t k\n∆ p\n()L k\n∆ p\n()t kp\n()rd kp\n()r kp\n()r k\n∆ p\n \nFigure 9. The proposed position/force hybrid control algorithm with using the online learning \n 622\nFor the force control loop, the desired force fzd and the contact force fz in the z directrion are \ncompared, and the force error is input to the PID controller Gf(z). \nThen, the desired joint velocity ()d k\u0005\nθ  is computed by using the inverse of Jacobian J and is \ninput to the servo driver of the manipulator. \n \n6. Experiments and Discussion \n \nSeveral experiments have been performed to  prove the effectiveness of the proposed \nmethod as shown in Fig.9. The control parame ters used in the experiments are given in \nTable 1. \n \n1 1 222diag[ 0 ]p p p ppp\nP P P PPP\nK KK K K=K  \n \n 1\np\nPK =5.01/s, 2\np\nPK =2.01/s \n \n1 1 222diag[ 0 ]pp p p p p\nII I I I IK KK K K=K  \n \n 1\np\nI\nK =0.51/s, 2\np\nIK =0.21/s \n \n1 1 222diag[ 0 ]p p p ppp\nD D D DDD KK KKK=K  \n \n 1\np\nDK =0.11/s, 2\np\nDK =0.11/s \n \nf\nPK =0.001 m/(s·N), \nf\nIK =0.0005 m/(s·N), \nf\nDK =0.0001 m/(s·N) \nTable 1. Control parameters used in the experiments \n \n6.1 Influences on Tracing Accuracy of the Tool Caused by Twist Moment (Without \nApplying Neural Networks and Orientation Control) \n \nA circular trajectory is given in Fig.10. The radiu s R of the circle is 0.1 meters. In order to \ninvestigate the influence of twis t moments on the flexible tool , position control is imposed \non the end-effector, while the orientation of the end-effector is fixed at its initial value. In \nthis experiment, the stereovision detection an d neural networks descr ibed in Fig.9 are not \nused. \nOther parameters used are as follows. \nFor position/orientation control:zd=0.750,ǂd=0, ǃd=1.222 rad, Ǆd=0. \nFor force control:fzd = 2 N. \nResults for the end-effector’s position ph(k) an d the tool’s tip position pt(k) are shown in \nFig.11. Because the tool’s deformation is not considered, a position error in the starting point \ncaused by deformation of the flexible tool is observed in Fig.11. On the path through points \np0, p1 and p2 as shown in Fig.11, the tool’s tip follows the movement of the end-effector but \na large position error is generated. Rotation of the flexible tool caused by a twist moment is \nnot encountered on this route because the flexible tool is pulled by the end-effector. \nHowever, on the path along points p2, p3 and p0, the end-effector pushes the flexible tool, so \nthat a twist moment causes rotation of the tool . As a result, the tool’s tip is uncontrollable. \nThe above results demonstrate that the tool’s  deformation and the effect of the twist \nmoment must be considered. \n 623\nDesired trajectory prd\nx\nStarting\npoint\nMoving direction\ny\nR=100 mm\np0\np1\np2\np3\n \nFigure 10. Desired trajectory of the tool’s tip \n \n0\n0.1\n0.2\n0.3\n0.25 0.35 0.45 0.55 0.65\nPosition in x direction   m\nphd\nph\nPosition in y direction   m\nptd\npt\np0\np1\np2\np3\n \nFigure 11. The tool’s tip position pt and end-effector’s position ph \n \n6.2 Tracing Experiment (Without Using Neural Networks) \n \nIn order to avoid twist-moment-induced rotation  of the flexible tool, it is pulled by the \nend-effector to trace the desired trajectory as shown in Fig.7. However, due to the arm’s \nmechanical structure, the end-effector can not rotate 360 degrees to trace the circular \ntrajectory shown in Fig.10. Therefore, a sinusoidal curve is given as the desired trajectory in \nFig.12. In this experiment, stereovision detection by real time image processing is used, but \nthe neural networks described in Fig.9 are not included in position control. Other \nparameters are used as follows. \nFor position/orientation control:zd=0.690,ǂd=0, ǃd=1.047 rad. \nFor force control:fzd = 0.5 N. \nThe end-effector’s position ph(k) and the tool ’s tip position pt(k) are shown in Fig.13. \nCompared with the results shown in Fig.11, the tool’s tip traces the desired sinusoidal curve \nwith the movement of the end-effector. Furthermore, rotation of the flexible tool caused by \nthe twist moment is successfully avoided by the proposed motion control of the end-effector \n 624\nas shown in Fig.7. However, in Fig.13, the ma ximum position error between tool tip’s \nposition pt(k) and the desired trajectory ptd(k) along the x axis is 8 mm, and the maximum \nposition error between ptd(k) and pt(k) along the y axis is 12 mm. \n \n \nDesired trajectory ptd\nx\nStarting point\nMoving direction\ny\nEnd point\n \nFigure 12. Desired trajectory of the tool’s tip \n \n0\n0.2\n0.4\n0.6\n0 0.1 0.2 0.3 0.4 0.5\nPosition in x direction   m\nphd\nph\nPosition in y direction   m\nptd\npt\n \nFigure 13. The tool tip’s position pt and end effector’s position ph \n \n6.3 Tracing Experiment Using the Proposed Method \n \nIn order to improve the tracing accuracy of th e flexible tool, the proposed control method \nusing online neural network learning is applie d. The desired trajectory is same sinusoidal \ncurve as shown in Fig.12. Other parameters are as follows. \n 625\nFor position/orientation control:zd=0.690,ǂd=0, ǃd=1.047 rad. \nFor force control:fzd = 0.5 N. \nThe following parameters are used for the proposed neural networks. \nNeuron number in the input layer NA= 3. \nNeuron number in the hidden layer NB= 6. \nNeuron number in the output layer NA= 1. \nAnnealing rate µ = 1.0. \nLearning rate η = 0.85. \nThe initial values of weighting coefficients of the neural networks are set randomly, and all \nweighting coefficients are saved in a file for continued learning in the next tracing. It is \nknown that a number of trials are generally needed before the learning error converges [Liu \n& Todo, 1991]. Results of the error signals for the neural network’s learning are shown in \nFig.14, where the learning errors settled near zero at the end of the first tracing after 10,000 \ntrials. Furthermore, compared with the error obtained in the first tracing, the tracing \naccuracy is obviously improved in the second tracing. \n \n0           1            2           3           4\n□~104\nLearning trials\n0\n−0.006\n0.006\n0\n−0.008\n0.008\nPosition error   m\nError in x axis\nError in y axis\n1st experiment 2nd experiment\n \nFigure 14. The error signals for neural network learning \n \nThe tracing trajectories of the tool’s tip and th e end-effector’s position in the x-y plane are \nshown in Fig.15, and result of the forces fz is shown in Fig.16. Compared with the results \nshown in Fig.12, the tracing accuracy of the t ool’s tip is greatly improved. The maximum \nposition errors between ptd(k) and pt(k) on both the x and y axes are decreased to 4 mm, \nwhich is almost half of the error in the expe riment without neural networks. Furthermore, \nthe contact force between the tool’s tip and the surface is accurately controlled at the desired \nforce fzd. \n \n7. Conclusions and Future Work \n \nFor robotic manipulation of a fragile object, usin g a flexible tool fixed to a robot arm is the \nobvious choice. However, a flexible tool deforms on contact and control of the flexible tool is \ndifficult because the position of the tool’s tip cannot be calculated from the kinematics of the \nrobot arm. We have developed a new approach  that is not based on establishing a \n 626\ndeformation model to calculate the tool tip’ s position, but that uses real time image \nprocessing with stereovision. Furthermore, an interpolation algorithm is proposed to \nconvert the image-processing results detected in each camera frame time to the results used \nat every sampling period for robot control. \nVisual detection is a convenient and effectiv e way to obtain inform ation on a deformable \nobject with unknown parameters, which is generally required for deformation model \nanalysis. \n \n \n0\n0.2\n0.4\n0.6\n0 0.1 0.2 0.3 0.4 0.5\nPosition in x direction   m\nphdph\nPosition in y direction   m\nptd\npt\n \nFigure 15. The tool’s tip position pt and end effector’s position ph \n \n \n-1.5\n-1\n-0.5\n0\n0.5\n01 0 2 0 3 0\nTime   s\nForce    N fzd\nfz\n \nFigure 16. The force results \n 627\nAs shown in Fig.6, control of the deformable tool is difficu lt because twist moment will \ncause a rotation of the tool’s body so that posi tion of the tool is un controllable. Therefore, \npulling the flexible tool by the end-effector is  considered. In this chapter, a position and \nforce hybrid control method using visual and force information is proposed to enable a \nflexible tool held by a manipulator to trace a specified curve in a plane. To improve tracing \naccuracy, online learning neural networks are  introduced to construct a feed-forward \ncontroller so that the position error of the tool ’s tip is decreased. The proposed method is \nused for a manipulator with a flexible tool to trace a sinusoidal curve and it effectiveness is \nexperimentally demonstrated. \nFor future work, the influence of the pressing force and the physical characteristics of the \ncontact surface will be investigated. In this chapter, a method of avoiding tool rotation \ncaused by the twist moment is proposed. In practice, because the friction coefficients of the \ncontact surfaces vary greatly, any torque can easily cause rotation of the flexible tool. \nTherefore, a control strategy should be develo ped to adjust the end-effector’s orientation \nwhen rotation of the tool occurs. To achieve th is control, the visual detection will first be \ni m p r o v e d  b y  u s i n g  a  n u m b e r  o f  f e a t u r e  p o i n t s  o n  t h e  t o o l  b o d y  so  a s  t o  o b t a i n  i t s  \ndeformation state. A control algorithm for the end-effector of the arm is also needed for \nadequate adjustment of the flexible tool.  \n \n8. References \n \nAcker, J. & Henrich, D. (2003). Manipulating  deformable linear objects: characteristic \nfeatures for vision-based detection of contact state transitions, Proc. IEEE Int. Symp. on \nAssembly and Task Planning, pp.204–209, July 10-11, 2003 \nAbegg, F.; Remde, A. & Henrich, D. (2000). Force and vision based detection of contact state \ntransition, Robot manipulation of deformable objects, Springer, ISBN 1852332506 \nCichocki, A. & Unbehauen, R. (1993). Neural networks for optimization and \nsignalprocessing, John Wiley & Sons□ ISBN 0471930105,  \nChen, C. Y. & Zheng,Y. F. (1992). Deformat ion identification and estimation of \none-dimension objects by vision sensors, Journal of Robotic Systems, Vol.9, No.5, \npp.595-612 \nHenrich, D.; Ogasawara, T. & Worn, H. M. ( 1999). Manipulating deformable linear objects \n-Contact states and point contacts-, Proc. IEEE Int. Symp. on Assembly and Task \nPlanning, pp.198 - 204, July 21-24, 1999 \nHirai, S. (1998). Deformable object manipulation, J. of the Robotics Society of Japan, Vol.16, \nNo.2, pp.136-139 \nHirai, S. & Noguchi, H. (1997). Human-demonstration based approach to the object motion \ndesign and the recognition of process state transitions in insertion of deformable tubes, \nJ. of the Robotics Society of Japan, Vol.18, No.8, pp.1172-1179 \nHisada, T. (1998). Finite element modeling, J. of the Robotics Society of Japan, Vol.16, No.2, \npp.140-144 \nHuang, J. & Todo, I. (2001). Control of a robo t based on fusion of visual and force/torque \nsensor information (Manipulation of a deform able object), Trans. Japan. Soc. Mech. \nEng., Series C, Vol.67, No.660, pp.2616-2623 \nHuang, J.; Todo, I. & Muramatsu, I. (2003). Neuro-control of a robot using visual and \nforce/torque sensor information (Manipulation of a flexible beam object), Trans. Japan. \nSoc. Mech. Eng., Series C, Vol.69, No.684, pp.2085-2092 \nItakura, O. (1998). Manipulation  of paper material – ticket  handling in station business \nmachine –, J. of the Robotics Society of Japan, Vol.16, No.2, pp.154-158 \n 628\nLiu, M. H. & Todo, I. (1991). Digital control of servo systems using neural networks (A \nmethod of off-line learning), Trans. Japan. Soc. Mech. Eng., Series C, Vol.57, No.539, \npp.2256-2262 \nNakagaki, H. (1998). Insertion task of a flexib le beam or a flexible wire, Journal of the \nRobotics Society of Japan, Vol.16, No.2, pp.159-162 \nNakagaki, H.; Kitagaki, K.; Ogasawara, T. & Tsukune, H. (1997). Estimation of a force acting \non a flexible wire by using visual tracking and its application to insertion task, J. of the \nRobotics Society of Japan, Vol.15, No.3, pp.422-430 \nOno, E. (1998). Fabric Manipulation, J. of  the Robotics Society of Japan, Vol.16, No.2, \npp.149-153 \nRemde, A.; Henrich, D. & Wom, H. (1999). Ma nipulating deformable linear objects-contact \nstate transitions and transition conditions , Proc. IEEE/RSJ Int. Conf. on Intelligent \nRobots and Systems,Vol.3, pp.1450-1455, Oct.17-21,1999 \nSchlechter, A. & Henrich, D. (2002). Manipula ting deformable linear objects: manipulation \nskill for active damping of oscillations, Proc. IEEE/RSJ Int. Conf. on Intelligent Robots \nand System, Vol.2, pp.1541-1546, Sept.30-Oct.5, 2002  \nSchmidt, T. W. & Henrich, D. (2001). Manipulating deformable linear objects: robot motions \nin single and multiple contact points, Pr oc. IEEE Int. Symp. on  Assembly and Task \nPlanning, pp.435-441, May 28-29, 2001  \nWakamatsu, H. & Wada, T. (1998). Modeling of string object for their manipulation, J. of the \nRobotics Society of Japan, Vol.16, No.2, pp.145-148 \nWakamatsu, H.; Tanaka, Y.; Tsumaya, A.; Shirase, K. & Arai, E. (2002). Representation and \nplanning of deformable linear object manipu lation including knotting, Proc. IEEE Int. \nConf. on Industrial Technology, Vol.2, pp.1321-1326, Dec.11-14, 2002 \nWakamatsu, H.; Tsumaya, A.; Arai, E. & Hi rai, S. (2004). Planning of one-handed \nknotting/raveling manipulation of linear objects, Proc. IEEE International Conference \non Robotics and Automation, Vol.2, pp.1719-1725, April 26-May 1, 2004 \nWu, J. Q.; Luo, Z. W.; Yamakita, M. & Ito, K. (1997). Dynamic position/force control of \nmanipulators for contact tasks on unknown fl e x i b l e  p l a t e ,  T r a n s .  J a p a n .  S o c .  M e c h .  \nEng., Vol.63, No.607, pp.937-944 \nYue, S. & Henrich, D. (2002). Manipulating deformable linear objects: sensor-based fast \nmanipulation during vibratio n, Proc. IEEE Int. Conf. on Robotics and Automation, \nVol.3, pp.2467-2472, May11-15, 2002  \nCutting Edge Robotics\nEdited by Vedran Kordic, Aleksandar Lazinica and Mu nir Merdan\nISBN 3-86611-038-3\nHard cover, 784 pages\nPublisher Pro Literatur Verlag, Germany\nPublished online 01, July, 2005\nPublished in print edition July, 2005\nInTech Europe\nUniversity Campus STeP Ri \nSlavka Krautzeka 83/A \n51000 Rijeka, Croatia \nPhone: +385 (51) 770 447 \nFax: +385 (51) 686 166\nwww.intechopen.com\nInTech China\nUnit 405, Office Block, Hotel Equatorial Shanghai \nNo.65, Yan An Road (West), Shanghai, 200040, China \nPhone: +86-21-62489820 \nFax: +86-21-62489821\nThis book is the result of inspirations and contrib utions from many researchers worldwide. It presents  a\ncollection of wide range research results of roboti cs scientific community. Various aspects of current  research\nin robotics area are explored and discussed. The bo ok begins with researches in robot modelling & desi gn, in\nwhich different approaches in kinematical, dynamica l and other design issues of mobile robots are disc ussed.\nSecond chapter deals with various sensor systems, b ut the major part of the chapter is devoted to robo tic\nvision systems. Chapter III is devoted to robot nav igation and presents different navigation architect ures. The\nchapter IV is devoted to research on adaptive and l earning systems in mobile robots area. The chapter V\nspeaks about different application areas of multi-r obot systems. Other emerging field is discussed in chapter VI\n- the human- robot interaction. Chapter VII gives a  great tutorial on legged robot systems and one res earch\noverview on design of a humanoid robot.The differen t examples of service robots are showed in chapter VIII.\nChapter IX is oriented to industrial robots, i.e. r obot manipulators. Different mechatronic systems or iented on\nrobotics are explored in the last chapter of the bo ok.\nHow to reference\nIn order to correctly reference this scholarly work , feel free to copy and paste the following:\nJian Huang, Isao Todo and Tetsuro Yabuta (2005). Po sition / Force Hybrid Control of a Manipulator with  a\nFlexible Tool Using Visual and Force Information, C utting Edge Robotics, Vedran Kordic, Aleksandar Laz inica\nand Munir Merdan (Ed.), ISBN: 3-86611-038-3, InTech , Available from:\nhttp://www.intechopen.com/books/cutting_edge_roboti cs/position___force_hybrid_control_of_a_manipulator _\nwith_a_flexible_tool_using_visual_and_force_informa\n\n© 2005 The Author(s). Licensee IntechOpen. This chapter is distributed under the terms of the\nCreative Commons Attribution-NonCommercial-ShareAlike-3.0 License\n, which permits use,\ndistribution and reproduction for non-commercial purposes, provided the original is properly cited\nand derivative works building on this content are distributed under the same license.",
  "topic": "Robot",
  "concepts": [
    {
      "name": "Robot",
      "score": 0.6142469048500061
    },
    {
      "name": "Object (grammar)",
      "score": 0.5984907150268555
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5195989012718201
    },
    {
      "name": "Position (finance)",
      "score": 0.5174767971038818
    },
    {
      "name": "Computer science",
      "score": 0.5046511888504028
    },
    {
      "name": "Robot manipulator",
      "score": 0.4548507332801819
    },
    {
      "name": "Computer vision",
      "score": 0.45347148180007935
    },
    {
      "name": "Control (management)",
      "score": 0.45227155089378357
    },
    {
      "name": "Manipulator (device)",
      "score": 0.43710583448410034
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4050520062446594
    },
    {
      "name": "Engineering",
      "score": 0.34577441215515137
    },
    {
      "name": "Business",
      "score": 0.08800670504570007
    },
    {
      "name": "Finance",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 7
}