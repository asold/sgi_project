{
    "title": "Unlearning Bias in Language Models by Partitioning Gradients",
    "url": "https://openalex.org/W4385570172",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2157172445",
            "name": "Charles Yu",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A4287889259",
            "name": "Sullam Jeoung",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A3021164492",
            "name": "Anish Kasi",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A1987378291",
            "name": "Pengfei Yu",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2127420617",
            "name": "Heng Ji",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4286897388",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W2725155646",
        "https://openalex.org/W2889624842",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W2972413484",
        "https://openalex.org/W4285192297",
        "https://openalex.org/W3105882417",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2963457723",
        "https://openalex.org/W2971307358",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3035241006",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W3093211917",
        "https://openalex.org/W4385574250",
        "https://openalex.org/W2950866572",
        "https://openalex.org/W3155655882",
        "https://openalex.org/W4206292552",
        "https://openalex.org/W4287820586",
        "https://openalex.org/W4298882136",
        "https://openalex.org/W2971015127",
        "https://openalex.org/W4206118214",
        "https://openalex.org/W3104142662",
        "https://openalex.org/W4382202906",
        "https://openalex.org/W2099115159",
        "https://openalex.org/W2997588435",
        "https://openalex.org/W2888161220",
        "https://openalex.org/W3206487987",
        "https://openalex.org/W2963526187",
        "https://openalex.org/W4385573981",
        "https://openalex.org/W3134354193",
        "https://openalex.org/W2805003733",
        "https://openalex.org/W2972572477",
        "https://openalex.org/W3037831233",
        "https://openalex.org/W4385570906",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W3035591180",
        "https://openalex.org/W2950018712",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W230405319",
        "https://openalex.org/W3172415559",
        "https://openalex.org/W3131157458"
    ],
    "abstract": "Recent research has shown that large-scale pretrained language models, specifically transformers, tend to exhibit issues relating to racism, sexism, religion bias, and toxicity in general. Unfortunately, these pretrained language models are used almost universally in downstream tasks, and natural language processing is often applied to make real-world predictions. Thus, debiasing these language models as early in development as possible is increasingly crucial for preventing unintentional harms caused by natural language systems. To this end, we propose a new technique called partitioned contrastive gradient unlearning (PCGU), a gray-box method for debiasing pretrained masked language models. PCGU aims to optimize only the weights that contribute most to a specific domain of bias, doing so by computing a first-order approximation based on the gradients of contrastive sentence pairs. Our experiments show that PCGU is both low-cost and seems particularly effective at pinpointing the sources of implicit social bias in large pretrained transformers. Although we train using PCGU in the gender-profession domain only, we find that doing so can also partially mitigate bias across other domains. All code for our implementation and experiments can be found at https://github.com/CharlesYu2000/PCGU-UnlearningBias.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 6032–6048\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nUnlearning Bias in Language Models by Partitioning Gradients\nCharles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, Heng Ji\nUniversity of Illinois at Urbana-Champaign\n{ctyu2,sjeoung2,anishk4,pengfei4,hengji}@illinois.edu\nAbstract\nRecent research has shown that large-scale\npretrained language models, specifically\ntransformers, tend to exhibit issues relating\nto racism, sexism, religion bias, and toxicity\nin general. Unfortunately, these pretrained\nlanguage models are used almost universally\nin downstream tasks, and natural language\nprocessing is often applied to make real-world\npredictions. Thus, debiasing these language\nmodels as early in development as possible\nis increasingly crucial for preventing unin-\ntentional harms caused by natural language\nsystems. To this end, we propose a new tech-\nnique called partitioned contrastive gradient\nunlearning (PCGU), a gray-box method for\ndebiasing pretrained masked language models.\nPCGU aims to optimize only the weights that\ncontribute most to a specific domain of bias,\ndoing so by computing a first-order approxi-\nmation based on the gradients of contrastive\nsentence pairs. Our experiments show that\nPCGU is both low-cost and seems particularly\neffective at pinpointing the sources of implicit\nsocial bias in large pretrained transformers.\nAlthough we train using PCGU in the gender-\nprofession domain only, we find that doing so\ncan also partially mitigate bias across other\ndomains. All code for our implementation\nand experiments can be found at https:\n//github.com/CharlesYu2000/\nPCGU-UnlearningBias.\n1 Introduction\nIn the past few years, extraordinary improvements\nhave been made to most applications of natural lan-\nguage processing due to the prevalence of large\npretrained language models, particularly Trans-\nformers (Vaswani et al., 2017). These language\nmodels achieve remarkable performance not only\nbecause of mechanisms like attention (Bahdanau\net al., 2016), but because of rich and diverse nat-\nural language corpora scraped from literature and\nthe internet. However, in spite of some measures\nto ensure that these natural language sentences\nare high quality (Radford et al., 2019), recent\nwork has shown that pretraining corpora contain\nmany toxic/biased sentences and that neural mod-\nels trained on such data readily capture and exhibit\nthese biases (Caliskan et al., 2017; May et al., 2019;\nGehman et al., 2020; Kurita et al., 2019).\nPrevious studies suggest that embeddings and\nmodels encode harmful social biases (Bolukbasi\net al., 2016; Caliskan et al., 2017; Kaneko and Bol-\nlegala, 2021; Dev et al., 2019; Nangia et al., 2020;\nKurita et al., 2019; Nadeem et al., 2020). This\ncan be problematic, as the lack of interpretability\nin modern language models means that negative\nstereotypes and social biases encoded in models\nmay lead to unfairness and harms in production\nsystems. Without effective mitigation techniques,\nfinetuned models utilizing these flawed language\nrepresentations might accidentally inherit spurious\ncorrelations not representative of the real world or\ntheir target task.\nTo mitigate the representational harms explained\nin Barocas et al. (2017); Blodgett et al. (2020), we\nmight aim for two goals of different granularities.\nThe first goal proposes to debias a model such that\nits predictions encode the least bias. The second\naims to remove social bias throughout a model such\nthat the model minimally represents constructs that\ncan cause itself to be biased in its predictions. Re-\ngardless of the debiasing goal, the north star is to\neliminate harms caused by the model, so we must\nbe motivated by how pretrained language models\nare used.\nMinimizing the cost of adoption for debiased\nlanguage models is a high priority for debiasing, as\nany barriers may cause people to be skeptical of the\nsocietal benefits. To ensure that people have little\nreason not to use our debiased model, we aim to\nminimize representing bias while still maximizing\nthe representation ability of the model. In this study,\nwe focus on debiasing pretrained language models\n6032\nused directly for masked language modeling. Cru-\ncially, we modify only their weights post-hoc with-\nout any changes to the architecture or additional\nmodules. In this way, we enable key stakeholders\nto swap out their masked language models (by sim-\nply loading a different set of weights) but still use\nthe exact same code for masked predictions, just\nas they might with any other finetuned model. Fur-\nthermore, stakeholders need not rely on the people\npretraining the model to have incorporated debias-\ning procedures during the pretraining process. We\nrestrict our study to masked language modeling, as\nthe use cases of language models for other down-\nstream tasks are disparate, and extrinsic evaluation\nof bias in those tasks is often be confounded by\ntask-specific finetuning (Meade et al., 2022).\nWe expect, based on the results from Kaneko and\nBollegala (2021); Vig et al. (2020), that problem-\natic social biases propagate throughout large por-\ntions of language models. Furthermore, based on\nthe Lottery Ticket Hypothesis (Frankle and Carbin,\n2019), we hypothesize that most bias is encoded\nby specific groups of neurons rather than individ-\nual weights throughout the model. So, we propose\na gradient-based debiasing method called parti-\ntioned contrastive gradient unlearning (PCGU)\nto locate where in the model these problematic\ninferences originate from and to systematically re-\ntrain those parts of the model to unlearn this bi-\nased behavior. In our experiments, we use PCGU\nto unlearn biases in the gender-profession domain\nand evaluate our approach using prior association\ntests for bias/stereotypes. We find that PCGU is\nseemingly effective both in mitigating bias for the\ngender-profession domain that it is applied to as\nwell as for generalizing these effects to other un-\nseen domains. In addition, we observe that the\nprocedure exhibits results quickly, requiring very\nfew iterations over the tuning dataset and very little\nreal time until convergence. The hyperparameter\nsearch space can be found in Appendix A.\n2 Related Work\nMotivated by the idea that the words in sentences\nare the root of all the information flowing through\nlanguage models, static word embeddings were\nthe first target for debiasing (Bolukbasi et al., 2016;\nZhao et al., 2018b; Sheng et al., 2019; Nangia et al.,\n2020; Dev et al., 2019; Karve et al., 2019; Zhang\net al., 2018). These methods typically operate via\nprojection onto some subspace that does not en-\ncode the targeted bias. However, modern language\nmodels do not use external embeddings, so it is\nnot immediately clear that such methods can be\napplied to transformers.\nFurther efforts have been made to extend those\npatterns for contextualized embeddings (Dev et al.,\n2019; Karve et al., 2019; Ravfogel et al., 2020;\nKaneko and Bollegala, 2021). However, such stud-\nies typically do not account for interactions be-\ntween different parts of the model when used in\nactual sentences. Instead, they focus either on the\n(static) word embedding layer or on aggregate rep-\nresentations of specific words.\nMethods that propose debiasing models beyond\nthe word level have also been proposed (Liang et al.,\n2020; Cheng et al., 2021). However, most of these\nmethods aim only to improve the case where an-\nother model will further use the sentence represen-\ntations generated by the text encoder. Crucially,\nthis does not solve any word-level problems such\nas masked language modeling. For example, meth-\nods like Cheng et al. (2021) add on extra modules,\nwhich means that the cost of adoption is more than\nsimply loading a new weights file. In a different\nvein, methods like Schick et al. (2021) utilize mul-\ntiple iterative prompts to debias generations only.\nRecently, much work in this field has been fo-\ncused on changing the pretraining or finetuning\nprocess to prevent bias from being learned by the\nlanguage model. Many approaches aim to change\nthe training process for embeddings, classifiers, or\nencoders, either through changing the training pro-\ncedure or adding bias-aware terms to the training\nloss function (Zhao et al., 2018a; Lauscher et al.,\n2021). Some of this work has achieved success\nby attempting to “neutralize\" the language models’\nrepresentation of biased words over some bias sub-\nspace by finetuning (Kaneko and Bollegala, 2021)\nor prompt tuning (Yang et al., 2023), or by extend-\ning these ideas by reformulating the bias dimen-\nsions as a set of implicit dimensions from social\npsychology (Omrani et al., 2023). Other methods\npropose changing or augmenting the training data\nin some way, typically by adding high-quality un-\nbiased or antistereotypical sentences, eliminating\nblatantly biased or stereotypical sentences, or a\ncombination of the two by replacing texts in the\ntraining corpus (Elazar and Goldberg, 2018; Guo\net al., 2022; Qian et al., 2022). Yet other techniques\nutilize counterfactual or adversarial signals to dis-\nsuade models from encoding biases (Zhao et al.,\n6033\nLayer n\nLayer 2\nLayer 1 x\nWK\nk = xWK\nx\nWQ\nq = xWQ\nw11 w12 w13\nw21 w22 w23\nw31 w32 w33\nWK =\nw11 w12 w13\nw21 w22 w23\nw31 w32 w33\nWQ =\nComputation Graph ViewMatrix View\n---\nThe surgeon could not\noperate on [MASK]\npatient. \np(his) = 0.92\np(her) = 0.07\nBERT \nStep 1:\nPartition\nParameter\nSet\nIndividual\nWeight Vectors\nStep 2:\nFind the Weight\nVectors Encoding\nBias\nget\ngradients\nBERT\nbackpropagate\nStep 3:\nUnlearn Bias\nVecAngle\n180\n176\n153\n92\n32\n12\nLeast Bias - Most Bias\nBias: \nNon-Bias: \ngradient descent to\nminimize advantaged\nprobability for bias\nweight vectorsChoose top-k\n(k=2 here)\nnon-bias\nweight\nvectors\nare frozen\nFigure 1: This illustration shows the framework of PCGU, which follows 3 steps as described in Section 3.\n2018a; Elazar and Goldberg, 2018; Zhang et al.,\n2018; Zmigrod et al., 2019; Hall Maudslay et al.,\n2019; Webster et al., 2020).\nPerhaps most similar to our method is actually\nwork done in the knowledge editing space. Such\ntasks propose explicitly editing specific knowledge\nin a model without affecting unrelated knowledge\n(Sinitsin et al., 2020; Zhu et al., 2020). This is quite\nsimilar to our task in that we aim to remove spe-\ncific social bias from our model without affecting\nunrelated inference ability. However, our method\nattempts to remove generalized forms of these bi-\nases, as opposed to removing/changing the more\ntargeted and specific knowledge that knowledge\nediting methods attempts to do. Recent studies in-\nclude gradient-based methods that train separate\nnetworks to predict efficient gradient updates for\nremoving or replacing models’ knowledge (Cao\net al., 2021; Mitchell et al., 2021).\n3 Methods\nAt a high-level, PCGU is composed of three parts.\nFirst, gradients must be computed for a contrast-\ning pair of sentences whose difference is in the\ndomain that the model is biased in. Next, we apply\na weight importance algorithm, based on gradients,\nto compute a ranked ordering of weights that are\nmost important to our criterion (i.e., the weights\nthat seem to most encode the biases we wish to\nunlearn). Finally, taking the earlier gradients and\nordered weights as input, we compute a first-order\napproximation of the bias gradient and perform a\nstandard optimization step of our language model.\nIn our experiments, we apply this procedure to\ndebias a group of masked transformer language\nmodels for the gender-profession domain such that\ntheir final parameters encode less inequality for\nMLM predictions. Specifically, we aim to update\nthe models such that they are not generally biased\ntoward a stereotypical sentence nor an antistereo-\ntypical sentence, since even antistereotypes can be\nharmful (McGowan and Lindgren, 2006). We later\nevaluate PCGU’s efficacy using existing evaluation\nbenchmarks.\n3.1 Contrastive Gradients\nFormally, we can consider BERT (Devlin et al.,\n2019), or any masked language model in this class,\nas a probability function M parameterized by its\nweights θ ∈Rd (d is the number of parameters\nof the model). M computes the probability of a\ntoken (which should be masked due to contextual\nembeddings) conditioned on its right and left con-\ntexts. So, given a sentence si = [w1\ni ,w2\ni ,...,w n\ni ]\nwhere wj\ni = [MASK], we can compute the proba-\nbility distribution of all possible tokens at index j\nto investigate the model’s biases.\nTo calculate contrastive gradients in the gender-\nprofession domain, we will employ a subset of\nthe Winogender Schemas dataset (Rudinger et al.,\n2018). This subset is composed of 240 minimal\nsentence pairs, where the only difference between\nthe sentences is the gender, either male or female1,\n1We do not claim that gender is binary. However, as the\ndataset only consists of three pronouns (male, female, neutral\nsuch as “they\"), we use only the male and female versions\n6034\nof the pronoun coreferent with the subject of the\nsentence. The subject of the sentence is always a\nperson referred to by their occupation, so we can\ninterpret the probabilities assigned to the male and\nfemale pronouns as the model’s stereotype for each\noccupation. For example, we may have a pair of\nsentences\ns1 = “The professor could not attend the talk\nbecause he was preparing for the keynote.\"\ns2 = “The professor could not attend the talk\nbecause she was preparing for the keynote.\"\nThe pronoun must be assumed by the model, as\nnone of the context entails a gender. For domains\nother than gender-profession, an analogous dataset\nwith minimally different sentence pairs could be\nutilized (or sentence tuples for non-binary domains,\nas described in Appendix E).\nFor each of the sentences in the minimal pair, we\ncompute the probability that the model assigns to\nthe differing token. Using standard backpropaga-\ntion, we then calculate the gradients, ∇1,∇2 ∈Rd,\nof the probabilities with respect to the model’s\nweights θ.\n3.2 Determining Importance of Weights\nPartitioning the Weights. Now, using ∇1 and ∇2,\nwe will determine which dimensions of θare the\nones that seem most important to the representa-\ntion of bias. To make this method robust, we parti-\ntion θinto a set of weight vectors θ1 ∈Rd1 ,θ2 ∈\nRd2 ,...,θ m ∈Rdm (where d1 + ··· + dm = d).\nThe gradient ∇i is partitioned into ∇1\ni ,..., ∇m\ni in\nthe same way.\nTo determine how to partition θ, we hypothesize\nthat a subset of neurons of the model should encode\nall the biases/preferences of the model in different\ncontexts. This is motivated by the Lottery Ticket\nHypothesis (Frankle and Carbin, 2019), which\nposited that neural networks often contain highly\nactive subnetworks that can be solely trained to\nsolve a task. Here, we propose two related forms of\npartitioning: input aggregation and output aggrega-\ntion. In transformers, input aggregation partitions\nattention matrices by grouping together the weights\nthat determine how much each element in the in-\nput embedding contributes to the key/query/value\nvectors. Output aggregation partitions the attention\nmatrices by grouping the weights that determine\nto simplify experiments by using “disjoint\" terms. A natural\nextension beyond binary gender words should be possible\ninductively, as discussed in Appendix E.\nhow much each element in the key/query/value\nvectors is influenced by the input embedding. For\nnon-attention weight matrices such as those used\nfor dense layers, the same concepts apply but for\nthe output embedding rather than the attention vec-\ntors. Note that we do not partition bias vectors for\neither partitioning method.\nAs an example, consider an r×cweight matrix\nW and a 1 ×r input embedding vector − →i . The\nleft multiplication of − →i by Wresults in the 1 ×C\noutput embedding vector − →o = − →i ·W. Input ag-\ngregation partitioning would partition W into r\nvectors (− →v1,− →v2,..., − →vr), where each of the vectors\n− →vi determines how much the ith index of − →i con-\ntributes to − →o (since each index jof − →o is computed\nas − →oj = ∑r\ni=1\n− →i i ·− →vi j). Output aggregation par-\ntitioning would instead partition Winto cvectors\n(− →v1,− →v2,..., − →vc), where each of the vectors − →vj de-\ntermines how much − →i contributes to the jth index\nof − →o (since − →oj is the dot product of − →vj and − →i ).\nTherefore, input aggregation partitioning is equiva-\nlent to partitioning the right-multiplied matrix by\nits rows, as illustrated in Figure 1. Similarly, output\naggregation partitioning is splitting by its columns.\nIn the 110M parameter version of BERT, using\ninput aggregation partitioning to partition θgives\nus approximately 114k weight vectors and using\noutput aggregation partitioning results in about 88k\nweight vectors.\nComputing Importance of Weight Blocks. Next,\nwe will calculate which vectors of the partition\n{θ1,θ2,...,θ m}seem to most encode the bias.\nSince our minimal pairs differ only in the gen-\nder of the subject noun working in the profes-\nsion, the gradients will encode the direction of\nmaximal increase in probability for the associ-\nated gender term. We expect that some parts of\nthe gradient may encode concepts like grammar,\nsemantics, and syntax, and be similar for both\ngradients. On the other hand, we expect a few\nparts of the gradient to be drastically different,\nas those are the parts of the model that the gen-\nder of the pronoun is highly relevant to. With\n{∇i\n1}m\n1 and {∇i\n2}m\n1 being the partitioned gradi-\nents for the two minimally different sentences, we\norder the weight vectors θr1 ,θr2 ,...,θ rm , where\nthe ordering {r1,r2,...,r m}is determined by how\ndifferent each of the corresponding gradient pieces\nis. Since the magnitude of each gradient piece is\nhighly dependent on unrelated values, we use only\nthe directions of the vectors to determine the dif-\n6035\nference between corresponding pieces in the two\ngradients. Thus, θ1,θ2,...,θ m are ordered by im-\nportance, as computed by cosine similarity:\nImportance(θi) = ∇i\n1 ·∇i\n2\n∥∇i\n1∥∥∇i\n2∥ (1)\nWeight vectors where the associated contrasting\ngradient pieces have low cosine similarity are thus\ndetermined to be most important for the targeted\nbias. In contrast, the ones with high similarity\nare determined to be least important to that bias,\nbut may be more relevant to unrelated concepts or\ndifferent types of bias.\n3.3 First-order Gradient Optimization Step\nFinally, we take some subset of the partition of\nweight vectors and only optimize those parts of θ\nto approximate reducing bias. We choose the sub-\nset θr1 ,θr2 ,...,θ rk as the kmost important weight\nvectors. To determine the actual values of the gradi-\nent used in this optimization step, we consider the\ngradients of each pair of sentences in our tuning\nset. In each pair, we denote one sentence to be the\n“advantaged\" sentence and the other to be the “dis-\nadvantaged\" sentence. The advantaged sentence is\nthe one that is expected to be more preferred by a\nbiased model and the disadvantaged sentence to be\nthe one less preferred. In our experiments tuning\nwith Winogender, we use the included statistics\nabout the proportion of gender-occupation corefer-\nence pairs in news sentences where the gender is\nfemale (Bergsma and Lin, 2006). From these pro-\nportions, we choose the sentence with the pronoun\nthat is less often coreferent to be the disadvantaged\nsentence and the other to be the advantaged sen-\ntence.\nWe then relabel the sentence pair s1,s2 to be\nsa1 ,sa2 where a1 is the index of the advantaged\nsentence and a2 is the index of the disadvantaged\nsentence. For example, since the reported propor-\ntion of the male-surgeon pair is 0.9566, a1 = 1 is\nthe index of the advantaged sentence and a2 = 2 is\nthe disadvantaged sentence.\nFinally, to compose our bias gradient, we will\ntake the gradient parts associated with the advan-\ntaged sentence (i.e., ∇r1\na1 ,∇r2\na1 ,..., ∇rk\na1 ) and ap-\nply a negative optimization step. In this negative\noptimization step, we perform gradient descent,\nmoving the parameters in the direction of maximal\nprobability decrease of the advantaged term:\nθri ←θri −α1 {i≤k}∇ri\na1 , (2)\nwhere αis the learning rate.\nIt is also reasonable to move the parameters in\nthe direction of maximal probability increase of\nthe disadvantaged term, which we discuss more in\nSection 4.6. Similarly, we may decide that using\nhistoric counts is too rigid and non-representative\nof modern models. Instead of using the coreferent\npairs proportions, we can decide which word is\nadvantaged and which is disadvantaged based on\ntheir logits at inference time. We discuss this more\nin Section 4.7.\n4 Experiments and Discussion\n4.1 Datasets\nWe evaluate our proposed PCGU method on two\nrecent social bias benchmarks: StereoSet (Nadeem\net al., 2020) and CrowS Pairs (Nangia et al., 2020).\nSince PCGU is intended for masked language mod-\neling within a sentence, we use only the intrasen-\ntence subset of StereoSet. Each instance in Stere-\noSet is a sentence triplet: one sentence is stereotyp-\nical, the second is antistereotypical, and the final\nsentence is a semantically non-sensical sentence.\nGirls tend to be more soft than boys. (stereo)\nGirls tend to be more determined than boys. (anti)\nGirls tend to be more fish than boys. (nonsense)\nEach instance in CrowS Pairs is a minimally differ-\nent pair of stereotypical and antistereotypical sen-\ntences. Using these two datasets, masked language\nmodels can be evaluated for bias by comparing the\nprobabilities associated with each sentence.\n4.2 Evaluation Metrics\nThe three StereoSet metrics are the Stereotype\nScore (SS), the Language Modeling Score (LMS),\nand the Idealized Context Association Test\nscore (ICAT). These metrics are computed by com-\nparing the probability assigned to the contrasting\nportion of each sentence conditioned on the shared\nportion of the sentence. The CrowS metric is simi-\nlar to SS except that it computes the probability of\nthe shared portion of the sentence conditioned on\nthe contrasting portions of each sentence instead.\nSS and CrowS both measure the proportion of ex-\namples where the stereotypical sentence is assigned\na higher probability than the antistereotypical sen-\ntence. The ideal score is 0.5, indicating no general\nbias toward either stereotypes or antistereotypes.\nTo measure the language modeling abilities of\nthe model, LMS is proposed as the proportion of\n6036\nModel SS→0.5(∆) LMS↑ICAT↑CrowS→0.5(∆)\nbert-base-cased0.569 (0.069)0.873 0.752 0.551 (0.051)+ PCGU (ours)0.534 (0.034)0.837 0.781 0.548 (0.048)+ DPCE 0.624 (0.124)0.785 0.590 0.458 (0.042)+ AutoDebias 0.530 (0.030)0.507 0.476 0.465 (0.035)\n+ PCGU then DPCE0.581 (0.081)0.849 0.712 0.452 (0.048)+ DPCE then PCGU0.569 (0.069)0.726 0.625 0.486 (0.014)\nModel SS→0.5(∆) LMS↑ICAT↑CrowS→0.5(∆)\nroberta-base 0.625 (0.125)0.917 0.689 0.593 (0.093)+ PCGU (ours)0.570 (0.070)0.839 0.722 0.584 (0.084)+ DPCE 0.641 (0.141)0.930 0.667 0.405 (0.095)+ AutoDebias 0.596 (0.096)0.685 0.554 0.467 (0.033)\n+ PCGU then DPCE0.561 (0.061)0.860 0.755 0.311 (0.189)+ DPCE then PCGU0.588 (0.088)0.853 0.703 0.516 (0.016)\nTable 1: PCGU compared with DPCE (Kaneko and Bollegala, 2021) and AutoDebias (Guo et al., 2022), two recent\nand similar debiasing methods. Bolded values are the best in their class. The ideal score for both SS and CrowS is\n0.50, so we additionally include the delta between score and ideal in parentheses for those two columns to facilitate\ngrokking. The reported SS, LMS, and ICAT scores are based on our full test set (across all domains). Our validation\nand test sets are created as a random 50/50 split of the intrasentence portion of the original development set of\nStereoSet.\nModel Name k Partition methodSS→0.5(∆) LMS↑ ICAT↑ CrowS→0.5(∆)\nBERT (base, uncased)\n0 (pretrained) - 0.5138 (0.0138)0.77240.7510 0.6048 (0.1048)\n14000 Input 0.4959 (0.0041)0.76750.7612 0.5968 (0.0968)\n11000 Output 0.5122 (0.0122)0.76260.7440 0.6021 (0.1021)\nAll - 0.4846 (0.0154)0.65120.6311 0.6021 (0.1021)\nBERT (base, cased)\n0 (pretrained) - 0.5693 (0.0693)0.87290.7519 0.5511 (0.0511)\n3000 Input 0.5336 (0.0336)0.83720.7809 0.5477 (0.0477)\n9500 Output 0.5609 (0.0609)0.85710.7527 0.5424 (0.0424)\nAll - 0.5126 (0.0126)0.59560.5806 0.5444 (0.0444)\nRoBERTa (base)\n0 (pretrained) - 0.6246 (0.1246)0.91700.6885 0.5928 (0.0928)\n22000 Input 0.5698 (0.0698)0.83890.7218 0.5842 (0.0842)\n8000 Output 0.6130 (0.1130)0.89530.6931 0.6114 (0.1114)\nAll - 0.5415 (0.0415)0.68270.6260 0.5358 (0.0358)\nALBERT (base)\n0 (pretrained) - 0.5000 (0.0000)0.56690.5669 0.5676 (0.0676)\n1000 Input 0.4806 (0.0194)0.53710.5163 0.4483 (0.0517)\n1300 Output 0.4790 (0.0210)0.43150.4134 0.4894 (0.0106)\nAll - 0.4839 (0.0161)0.44520.4308 0.6068 (0.1068)\nTable 2: Models are chosen at the epoch at which they achieve an average (across the gender and profession\ndomains) SS closest to 0.5 on our validation set. Formatting and evaluation details are as in Table 1. k= 0 models\nare the original pretrained model and k= All models are models tuned using the full gradient without partitioning\n(i.e., tuning all weights).\nexamples where the stereotypical/antistereotypical\nsentences are assigned a higher probability than the\nnon-sensical one. So, an ideal model achieves a\nscore of 1, and debiasing methods should aim to\nminimally decrease this score during debiasing.\nIn order to measure the tradeoff between better\nSS and worse LMS after debiasing, ICAT combines\nthe two into a score between 0 and 1 such that a\nperfectly debiased and accurate model achieves a\nscore of 1 (also, a fully random model achieves a\nscore of 0.5).\nFull formulations of these metrics can be found\nin Appendix D.\n4.3 Experiments\nWe test PCGU on four masked language models:\nthe uncased and cased versions of 110M BERT (De-\nvlin et al., 2019), the 125M version of RoBERTa\n(Liu et al., 2019), and the 11M version of ALBERT\n(Lan et al., 2020), all pretrained from the Hugging-\nFace library (Wolf et al., 2020). For each of the\nmodels, we report the results of the best-performing\nmodel tuned via PCGU using each of the two (in-\nput and output) aggregation partitioning methods.\nInput aggregation models were tuned for at most\n15 epochs using a learning rate of α= 2e−6 and\noutput aggregation models were tuned for at most\n10 epochs using a learning rate of α= 1e−5. On\na single NVIDIA Tesla V100 GPU (16GB), using\na batch size of 64 pairs from Winogender (so there\nare 4 batches per epoch), PCGU tuning of BERT\nwith PyTorch takes around 4 seconds per batch us-\ning input aggregation partitioning and 50 seconds\nper batch for output aggregation partitioning2. The\nmain cost of PCGU, other than the partitioning\nmethod which is implementation dependent (and\ncan be quite fast if not made to be a general inter-\nface) is only a cosine similarity, so the cost of a\nsingle step PCGU is on the order of a single step\nof finetuning, implying scalability to modern large\nlanguage models.\n2The extra runtime of output aggregation is due only to the\nspecific implementation we used, which indexed into tensors\nusing the range() function to allow for a more generic interface\nrather than slicing. Slicing indices is much more efficient.\n6037\nNotably, we re-compute weight importance for\neach batch of bsentence pairs by computing the\nimportance using the batched gradients. This is\nas opposed to computing the importance for each\nexample pair (i.e., b= 1) or using a static selection\nof weights computed based on the full dataset. In\nour testing, we found little discernible difference in\nusing different batch sizes, provided that they were\nreasonably large (b> 16). Evidently, larger batch\nsizes allowed the weight importance computation\nto be more robust.\nWe report the results of these experiments in\nTable 2. Although the reported PCGU models do\nnot achieve the perfect SS of 0.5, we tend to see\nsignificant improvement to the SS compared to\nrelatively little decrease in LMS, leading to an in-\ncrease in the overall ICAT score for both BERT\nand RoBERTa. However, this was not the case for\nALBERT, whose pretrained version achieved a per-\nfect SS, which might suggest that this method is\nmore effective when knowledge is more distributed\n(i.e., for larger models) or that our stopping criteria\nare imprecise. Perhaps unsurprisingly, the CrowS\nscore does not seem to be as affected by PCGU\n(although it does seem to have slightly improved\nin all cases). We attribute this observation to the\nfact that the gradient used for PCGU more closely\nresembles the probability used for the StereoSet\nmetrics than the probability calculation used for\nthe CrowS metric.\nBased on our random validation/test split of\nStereoSet, we find that apparently the dataset is\nnot uniform. Therefore, the performance for either\nSS or LMS of a model on the validation set was not\na great indicator of its performance on the test set.\nThe average SS of each of the reported PCGU mod-\nels on the validation set is within 0.016 of perfect,\nand mostly within 0.001 of perfect. However, not\nonly do we find that many different models achieve\nperfect or near-perfect SS on the test set (but not\non the validation set as well), but there exist yet\nother models that achieve high SS across the entire\nset but poor SS over each of the validation and test\nsets (Simpson’s paradox).\nAs part of a qualitative analysis, we find that\nmost random examples from StereoSet and even\nour own examples follow the trends shown in Fig-\nure 2. This suggests that PCGU debiases by aiming\nfor equality of genders in the sense used in Beu-\ntel et al. (2017); Zhang et al. (2018), where the\nodds of either gender are mostly uncorrelated with\nthe context. In fact, variants of the sentence in\nFigure 2, such as the sentence “The professor had\nto write [MASK] keynote” further showcase that\nnon-gendered infills can be minimally affected by\nPCGU debiasing. Prior to debiasing, the LM pre-\ndicts “a” and “the” with 88% probability while\npredicting “his” and “her” with only 7% of the\nprobability mass. After applying PCGU, the prob-\nability of “a” and “the” decreases only slightly,\nto 86%, while the gendered predictions “his” and\n“her” only increase to 10% of the total probability\nmass. Notably, PCGU seems effective at targeting\nactual biases, not simply differences in gender, a\nphenomenon discussed more in Appendix F.\n4.4 Comparison with Similar Debiasing\nMethods\nWe also compare models debiased using PCGU\nwith those debiased by DPCE (Kaneko and Bolle-\ngala, 2021) and AutoDebias (Guo et al., 2022), two\nrecent methods that update only the weights of the\nlanguage model without changes in architecture, in\nTable 1:\nDPCE (Kaneko and Bollegala, 2021) is a\nmethod that finetunes layers of the model according\nto their novel objective function seeking to mini-\nmize bias in the contextualized word embedding\nproduced at that layer. Their objective function\ndepends on finding sentences in the corpus that\nutilize bias attribute words and creating a proto-\ntype from those words’ contexts. Then, DPCE at-\ntempts to minimize the shared dimension between\nthe attribute prototype and the contextualized word\nembedding (similar to the projection-based debias-\ning methods that subtract from embeddings their\nprojections onto the bias subspace).\nAutoDebias (Guo et al., 2022) is a method that\nfirst searches for MLM prompts whose masked\ntoken distribution has the highest disagreement\namong the demographics chosen for debiasing (for\nexample, the probability of the words “he” and\n“she” being very different). Then, they use a Jensen-\nShannon divergence-based objective function to\nfinetune the model to equalize the demographic\ndistribution across all the generated prompts.\nWe find that PCGU tends to be far more ef-\nfective than DPCE while AutoDebias produces a\nclose-to-random model. Also, PCGU can signifi-\ncantly debias a model even after applying DPCE,\nbut the opposite is less notable. Thus, as a stan-\ndalone method, PCGU seems superior to the others.\n6038\nHowever, since they seem to have different effects\n(DPCE actually causes LMS to improve in some\ncases), it may be most effective to chain multiple\nmethods together.\nThe main methodological difference that seems\nto allow PCGU to perform better than DPCE is that\nPCGU does a very targeted finetuning by identify-\ning the weight partitions in the model that should be\naltered, whereas DPCE finetuning is guided by the\nloss function only and is dependent on using high-\nquality attribute prototypes. In practice, DPCE\nconverges much slower than PCGU does, possibly\ndue to this reliance on the prototypes.\nAn explanation for the relatively poor perfor-\nmance of AutoDebias may be due to the way it\nfinds the prompts with the highest distributional\ndisagreement. This heuristic does not account for\nthe fact that those prompts with the largest distribu-\ntional disagreement in a strong PLM are often those\nwhose context necessitates one version of a word\nand may not have anything to do with bias (“The\n[MASK] tied his shoes” should have a much higher\nprobability for “man” than for “woman” and “The\n[MASK] person prayed at the synagogue” would\nhave much higher probability for “Jewish” than for\n“Muslim”).\n4.5 Weight Importance Ablations\nAs an ablation test for the weight importance step,\nwe also perform PCGU using all the weights (basi-\ncally, taking a backward optimization step for the\nadvantaged sentence). We find that, although the\nprocedure generally is able to debias the language\nmodel well, the language modeling functionality is\ngreatly crippled (similar to AutoDebias). This is in\nstark contrast to the weight partitioning versions,\nwhich incur a much smaller decrease in language\nmodeling ability. These results suggest that some\nform of partitioning is clearly necessary; not all\nweights of the model contribute equally to bias.\nWe also find that the choice of input vs output\naggregation partitioning does not obviously affect\nthe performance of the debiased models. How-\never, across the experiments, the input partitioning\nmethod maintained a slight edge over the output\npartitioning method.\n4.6 Decreasing the Advantaged Probability vs\nIncreasing the Disadvantaged Probability\nWe also investigate the difference between taking\nthe optimization step in PCGU to decrease the prob-\nability of the advantaged sentence compared to\nFigure 2: Predictions when prompting BERT with a\nsentence that would cause stereotypes, before and after\ndebiasing using PCGU. In this [random unseen] exam-\nple, PCGU seems to equalize the probabilities of the\ngendered predictions.\nincreasing the probability of the disadvantaged sen-\ntence. We find that the former results in faster con-\nvergence, although the latter does not take much\nlonger to converge to similar performance. In gen-\neral, the difference in performance depended more\non the model selection criteria than on which gradi-\nent was used for the tuning. For example, selecting\nthe model based on the SS over the gender and\nprofession domains rather than based on the macro-\naveraged SS (compute SS for each domain and then\naverage it) resulted in as much fluctuation in SS\non the test set as using the disadvantaged gradient\ninstead of the advantaged gradient did.\nThere are some interesting implications related\nto the difference in goals for using each gradient.\nBy decreasing the probability of the advantaged\nsentence, we are more directly teaching the model\nto be less biased. On the other hand, by increas-\ning the probability of the disadvantaged sentence,\nwe are instead teaching the model to be equally\nas biased toward both forms (compared to other\noptions). In reality, bias comes in many shapes,\nand our work is motivated by the idea that we want\nto unlearn the entire class of bias, not just specific\nexamples. Unfortunately, a pair of options is not\nenough to represent the full distribution of options.\nTherefore, it seems reasonable to believe that de-\ncreasing the probability of the advantaged sentence\nshould be more applicable for general forms of bias.\nThus, all our experiments report results from this\nmethod only.\n6039\n4.7 Dynamically Determining the Advantaged\nand Disadvantaged Sentence\nModel NameSS (Dynamic|Static|Pretrained)LMS (Dynamic|Static|Pretrained)BERT (base, uncased)0.5106 |0.4959| 0.5138 0.7659 | 0.7675 |0.7724BERT (base, cased)0.5777 |0.5336| 0.5693 0.8687 | 0.8372 |0.8729RoBERTa (base)0.6213 |0.5698| 0.6246 0.9128 | 0.8389 |0.9170ALBERT (base)0.5048 | 0.4806 |0.5000 0.5613 | 0.5371 |0.5669\nTable 3: PCGU with dynamic sentence classification\n(i.e., choosing which sentence is advantaged and which\nis disadvantaged based on the PLM’s own prediction\nlogits) vs static sentence classification (as reported in Ta-\nble 2) and the original pretrained model. Bolded values\ndenote the most effective version. Dynamic determina-\ntion seems to be very similar to not changing original\npretrained model, as opposed to the static sentence clas-\nsification, which actually debiases.\nWe also consider the differences between using\na static determination of which sentence is advan-\ntaged and a dynamic determination, as alluded to\nin Section 3.3. A pretrained model’s state is highly\ncomplex so the model may need to improve greatly\nfor one region of the bias space and less so for an-\nother region. Therefore, it seems likely that one\nspace may become debiased before another space\nhas been debiased. By using a static determina-\ntion, we resign ourselves to the likelihood that an\nalready debiased space may become biased in the\nopposite direction while we debias the other space.\nIn other words, it seems likely that the model may\novershoot and fail to achieve an ideal overall per-\nformance when using the static determination.\nThis is, in experimentation, not the case, and\nwe report the results of PCGU using a dynamic\ndetermination in Table 3. At each training step, we\ndynamically choose the advantaged and disadvan-\ntaged sentences based on the logits of the masked\ntoken. Since this now allows us to simply aim for\nequality in the sentences, we then perform the op-\ntimization step using the difference in gradients\n(such that the advantaged sentence probability is\ndecreased and the disadvantaged sentence proba-\nbility is increased). In all cases, the model’s perfor-\nmance both for SS and LMS remained similar to\nthe original pretrained model. Thus, we can con-\nclude that this dynamic determination is not usable\nfor debiasing with PCGU.\n4.8 Cross-Domain Effects of PCGU\nThe scores for our experiments suggest that PCGU\nis effective at mitigating the amount of bias in a\nmodel without greatly affecting the transformer’s\nability to perform language modeling. Interestingly,\nModel Name Race Religion\nBERT (base, uncased)0.3799 - 0.47730.3636 - 0.5455\nBERT (base, cased)0.4372 - 0.53680.3750 - 0.7500\nRoBERTa (base)0.4146 - 0.65160.3500 - 0.7500\nALBERT (base)0.3571 - 0.60710.1429 - 0.6667\nTable 4: SS ranges for out-of-domain biases after PCGU.\nObserve that the perfect SS of 0.5 is contained in most of\nthese ranges, suggesting that the weight vectors selected\nfor unlearning by PCGU are, in some way, related to\nbiases in general, not just the gender-profession biases\nencoded in the training data.\ndespite the fact that our tuning set for PCGU only\ncontained information related to gender and profes-\nsion, we see that this procedure is able to change\nthe amount of bias in other domains as well (to\nvarying degrees), as shown in Table 4.\nThis suggests that perhaps some of the parame-\nters/neurons governing different domains of bias\nare potentially overlapping, causing some cross-\ndomain convergence during training. However, it is\njust as possible that the difference in SS may be due\nonly to noise or factors unrelated to bias. An exten-\nsion of this experiment may be able to determine\nif different domains of bias can be concurrently\nor sequentially debiased, possibly via coordinate\ndescent. It also seems reasonable, using the anal-\nogous data for other domains of bias mentioned\nin Section 3.1, to determine which weights are im-\nportant for separate domains of bias and which are\nshared.\n5 Conclusion\nIn this paper, we introduced PCGU, a method to\nsystematically search through a pretrained masked\nlanguage model to find the origins of its bias and\nmitigate them. The positive results in our paper sug-\ngest that, with the proper data, post-hoc removal\nof problematic social biases can be efficient and\ntargeted via PCGU. Our findings also support the\nnotion that different types of bias arise from differ-\nent areas in pretrained transformers.\nWe believe that by focusing on the language\nmodel holistically, rather than as a collection of\nindividual pieces, we can more effectively remove\nrepresentational harms from pretrained language\nmodels. It is our hope that future studies are able\nto leverage PCGU to fully debias language models\nand increase adoption of fair pretrained models.\n6040\n6 Limitations\nWe acknowledge that the StereoSet and CrowS\ndatasets and metrics are not ideal evaluation mea-\nsures for debiasing work (see Blodgett et al. (2021)\nfor more details about their pitfalls). We advise\npractitioners to conduct careful analysis for their\nspecific use case rather than interpreting the scores\nfrom our experiments as clear measures of bias\nmitigation or removal.\nFurthermore, we realize that in discussion of\nharms, we should also ensure that allocative harms\ndo not arise from dependency on a PCGU-debiased\nmodel. In this paper, we do not report experiments\non models finetuned for other downstream tasks, as\nfinetuning is generally more prone to spurious cor-\nrelations and accidentally encoding bias, so evalu-\nating such models obfuscates the procedure’s effect\non the pretrained model. Instead, we focused only\non the masked language modeling task such that\nintrinsic and extrinsic evaluations both use the pre-\ntrained model directly and only. In the modern age\nof large language models, this is arguably more ap-\nplicable, but this setting doesn’t take into account\nthe effects of prompts on the prediction distribu-\ntion. An interesting extension of this study would\nbe to debias using some form of PCGU in the pure\ngeneration setting and evaluating with high quality\ngeneration-based resources such as HolisticBias\n(Smith et al., 2022). However, the base form of\nPCGU is not directly applicable due to the diffi-\nculty in attaining and using minimal pairs/tuples in\ngenerations.\nAnother related limitation is that our experi-\nments were only conducted in English. However,\nmany languages, such as Spanish or other Romance\nlanguages, have a much richer concept of grammat-\nical/lexical gender sometimes affecting multiple\nwords per sentence.\nUnfortunately, a fundamental problem with in-\nterpretability arises if we wish to evaluate the lan-\nguage model’s bias implicitly. For example, the\nprediction in Figure 2 suggests that the debiased\nmodel is less biased than a model predicting the\nfull probability mass for the female term. Discrete\nmetrics fail to account for this behavior, so better\nevaluation metrics would also give us a better sense\nof the efficacy of our proposed method.\nWe further note that gender, which has histori-\ncally been treated as a binary construct, is likely\nto be a relatively easy domain to work with. Other\nmore complicated social biases like racism and\nclassism are similarly harmful, and an ideal debias-\ning procedure should work for all of them. Similar\nquestions may arise about if we can ever compre-\nhensively cover all domains without a better way\nto generalize across domains. It is also to be seen\nif PCGU can be directly used for other domains, as\nour experiments only touched on the intersection\nof gender and profession biases while observing\nthat this has effects on other domains. Further work\nwould be required to understand why, and in what\ncontexts, PCGU can affect unseen domains; are\nthe cross-domain results in the main paper arti-\nfacts of intersectionality (between seen and unseen\ndomains) or is this truly generalizations across a\nbroader notion of bias?\nDue to the complexity of social bias, it is not ob-\nvious if a properly modeled dataset for such other\ndomains of bias can be easily constructed for us-\nage with PCGU. A natural thought would be to\nattempt to generate training data for PCGU. We\nattempted this but found that the generations were\nnot reliable in terms of providing signal for what\nconstituted bias. By using a templated dataset like\nWinoGender, we can ensure that every instance in\nthe training set encodes bias by an assumption of\ngender based on only the profession.\nObviously, partitioning at the most granular\nlevel where each single parameter is its own part\nwould make our directional comparison meaning-\nless. However, we did not extensively study how\nimportant the specific partitioning method was. An\ninteresting class of experiments would be using\nsome sort of random partitioning, where each in-\ndividual parameter is assigned to its group of pa-\nrameters not according to any architectural reason\nbut according to some sort of randomness. Our\nimplementation of this made the gradient selec-\ntion extremely expensive because it required too\nmuch indexing into tensors as opposed to a full\nreplacement of specific dimensions. A better imple-\nmentation or experiment would be needed to draw\nactionable conclusions about different partitioning\nmethods. However, our baseline experiments for\nthis matched with the intuition that sampling each\nweight as being a bias or non-bias weight using\na Bernoulli distribution yields a similar effect as\nregular training with dropout, similar to the k=All\nexperiments in Table 2.\n6041\n7 Other Ethical Considerations\nThis study employed a binary classification of gen-\nder in our experimentation and description of the\nmethodology. It is our firm stance that such beliefs\nhave no place in the community, especially consid-\nering that language evolves with its users. However,\nwe believe that this narrow view of gender is neces-\nsary as a step in the broader direction of full equity.\nWe hope that when high quality datasets displaying\nnon-binary genders are released in a form usable\nby PCGU, researchers may revisit this paper and\nstudy an inductive extension of PCGU.\nWe also recognize the fact that any method used\nfor debiasing may possibly be reversed to train\nextremely bigoted models, possibly for trolling or\ntargeted harassment. However, we believe that any\nsuch practice for PCGU would not be more harmful\nthan existing training methods. As observed in our\nexperiments, even when looking to increase the\nprobability of logits only (as opposed to explicitly\ndecreasing the advantaged sentence), the language\nmodeling score still suffers. Therefore, there seems\nto be no reason to believe that PCGU could create a\nmore biased model than simply finetuning on many\nbigoted examples.\nDue to the problems with StereoSet and CrowS\nalluded to in Section 6, we recognize that experi-\nmental results based on those metrics are not con-\nclusive evidence that a model is biased or unbiased\n(or good at modeling). We urge any reader to make\ntheir own judgment about these models through\ntheir own qualitative analyses.\nAcknowledgement\nWe are extremely grateful for and would like\nto thank all our anonymous reviewers for their\ninsights and feedback. This research is based\nupon work supported by U.S. DARPA CCU\nProgram No. HR001122C0034 and INCAS\nProgram No. HR001121C0165. The views and\nconclusions contained herein are those of the\nauthors and should not be interpreted as necessarily\nrepresenting the official policies, either expressed\nor implied, of DARPA, or the U.S. Government.\nThe U.S. Government is authorized to reproduce\nand distribute reprints for governmental purposes\nnotwithstanding any copyright annotation therein.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2016. Neural machine translation by jointly\nlearning to align and translate.\nSolon Barocas, Kate Crawford, Aaron Shapiro, and\nHanna Wallach. 2017. The problem with bias: Al-\nlocative versus representational harms in machine\nlearning. In In Proceedings of SIGCIS.\nShane Bergsma and Dekang Lin. 2006. Bootstrapping\npath-based pronoun resolution. In Proceedings of the\n21st International Conference on Computational Lin-\nguistics and 44th Annual Meeting of the Association\nfor Computational Linguistics, pages 33–40, Sydney,\nAustralia. Association for Computational Linguistics.\nAlex Beutel, Jilin Chen, Zhe Zhao, and Ed H. Chi. 2017.\nData decisions and theoretical implications when ad-\nversarially learning fair representations.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyping\nNorwegian salmon: An inventory of pitfalls in fair-\nness benchmark datasets. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1004–1015, Online. Association\nfor Computational Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. Advances\nin neural information processing systems, 29:4349–\n4357.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science,\n356(6334):183–186.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models.\nPengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si,\nand Lawrence Carin. 2021. Fairfil: Contrastive neu-\nral debiasing method for pretrained text encoders.\nSunipa Dev, Tao Li, Jeff Phillips, and Vivek Srikumar.\n2019. On measuring and mitigating biased inferences\nof word embeddings.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\n6042\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYanai Elazar and Yoav Goldberg. 2018. Adversarial\nremoval of demographic attributes from text data.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n11–21, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nJonathan Frankle and Michael Carbin. 2019. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. Realtoxic-\nityprompts: Evaluating neural toxic degeneration in\nlanguage models.\nYue Guo, Yi Yang, and Ahmed Abbasi. 2022. Auto-\ndebias: Debiasing masked language models with\nautomated biased prompts. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1012–1023, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nRowan Hall Maudslay, Hila Gonen, Ryan Cotterell, and\nSimone Teufel. 2019. It’s all in the name: Mitigating\ngender bias with name-based counterfactual data sub-\nstitution. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5267–5275, Hong Kong, China. Association for Com-\nputational Linguistics.\nMasahiro Kaneko and Danushka Bollegala. 2021. De-\nbiasing pre-trained contextualised embeddings. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 1256–1266, Online.\nAssociation for Computational Linguistics.\nSaket Karve, Lyle Ungar, and João Sedoc. 2019. Con-\nceptor debiasing of word representations evaluated\non WEAT. In Proceedings of the First Workshop\non Gender Bias in Natural Language Processing,\npages 40–48, Florence, Italy. Association for Com-\nputational Linguistics.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in con-\ntextualized word representations. In Proceedings of\nthe First Workshop on Gender Bias in Natural Lan-\nguage Processing, pages 166–172, Florence, Italy.\nAssociation for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learn-\ning of language representations. In ICLR. OpenRe-\nview.net.\nAnne Lauscher, Tobias Lueken, and Goran Glavaš. 2021.\nSustainable modular debiasing of language models.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 4782–4797, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020. Towards debiasing sentence\nrepresentations. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5502–5515, Online. Association for\nComputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 622–628, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMiranda Oshige McGowan and James Lindgren. 2006.\nTesting the model minority myth. Nw. UL REv.,\n100:331.\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2022. An empirical survey of the effectiveness of\ndebiasing techniques for pre-trained language models.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1878–1898, Dublin, Ireland.\nAssociation for Computational Linguistics.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D. Manning. 2021. Fast model\nediting at scale.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2020.\nStereoset: Measuring stereotypical bias in pretrained\nlanguage models. arXiv preprint arXiv:2004.09456.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nAli Omrani, Alireza Salkhordeh Ziabari, Charles Yu,\nPreni Golazizian, Brendan Kennedy, Mohammad\nAtari, Heng Ji, and Morteza Dehghani. 2023. Social-\ngroup-agnostic bias mitigation via the stereotype\n6043\ncontent model. In Proc. The 61st Annual Meet-\ning of the Association for Computational Linguistics\n(ACL2023).\nRebecca Qian, Candace Ross, Jude Fernandes,\nEric Michael Smith, Douwe Kiela, and Adina\nWilliams. 2022. Perturbation augmentation for fairer\nNLP. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9496–9521, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\nTwiton, and Yoav Goldberg. 2020. Null it out: Guard-\ning protected attributes by iterative nullspace projec-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7237–7256, Online. Association for Computational\nLinguistics.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers),\npages 8–14, New Orleans, Louisiana. Association for\nComputational Linguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for re-\nducing corpus-based bias in NLP. Transactions of the\nAssociation for Computational Linguistics, 9:1408–\n1424.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as\na babysitter: On biases in language generation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3407–\n3412, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin,\nSergei Popov, and Artem Babenko. 2020. Editable\nneural networks.\nEric Michael Smith, Melissa Hall, Melanie Kambadur,\nEleonora Presani, and Adina Williams. 2022. “I’m\nsorry to hear that”: Finding new biases in language\nmodels with a holistic descriptor dataset. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 9180–9211,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\nart Shieber. 2020. Investigating gender bias in lan-\nguage models using causal mediation analysis. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 12388–12401. Curran Associates,\nInc.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel,\nEmily Pitler, Ellie Pavlick, Jilin Chen, Ed H. Chi, and\nSlav Petrov. 2020. Measuring and reducing gendered\ncorrelations in pre-trained models. Technical report.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020. Hug-\ngingface’s transformers: State-of-the-art natural lan-\nguage processing.\nKe Yang, Charles Yu, Yi R. Fung, Manling Li, and Heng\nJi. 2023. Adept: A debiasing prompt framework.\nAAAI.\nBrian Hu Zhang, Blake Lemoine, and Margaret Mitchell.\n2018. Mitigating unwanted biases with adversarial\nlearning.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018a. Gender bias\nin coreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 15–20, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-\nWei Chang. 2018b. Learning gender-neutral word\nembeddings. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 4847–4853, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.\n2020. Modifying memories in transformer models.\nRan Zmigrod, Sabrina J. Mielke, Hanna Wallach, and\nRyan Cotterell. 2019. Counterfactual data augmenta-\ntion for mitigating gender stereotypes in languages\nwith rich morphology. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1651–1661, Florence, Italy. Asso-\nciation for Computational Linguistics.\n6044\nA Hyperparameter Search\nFor the models reported in Table 2, the only hyper-\nparameter search performed was for the value of\nk. In general, fewer attempts were made for output\naggregation methods, as those took much longer\nto perform. Also, output aggregation and input\naggregation resulted in different maximum values\nof k. The range of kexperimented on was based\non being near to 10% of available vectors. All k\nvalues were chosen uniformly over the provided\nrange (both bounds inclusive) based on the step\nsize.\nSummary statistics are not included as each kis\nessentially a different value.\n1. bert (both bert-base-uncased and bert-base-\ncased). For input aggregation, k from 2000\nto 22000 with a step size of 1000. For output\naggregation, kfrom 5000 to 11000 with a step\nsize of 1500.\n2. roberta-base. For input aggregation, kfrom\n2000 to 26000 with a step size of 1000. For\noutput aggregation, k from 5000 to 11000\nwith a step size of 1500.\n3. albert-base-v2. For input aggregation, kfrom\n1000 to 8000 with a step size of 250. For\noutput aggregation, kfrom 500 to 1500 with\na step size of 200.\nB Dataset Download Links\nCrowS Pairs: https://github.com/nyu-mll/crows-\npairs\nStereoSet: https://stereoset.mit.edu/\nC Dataset Statistics\nThe full CrowS dataset of 1508 examples is used\nfor evaluation.\nInstances from StereoSet where any of the\nmasked words tokenized to more than one to-\nken were discarded, since the masked language\nmodels we use do not support joint mask predic-\ntion/infilling. In the remaining set, there were 765\ninstances in the gender domain, 2430 in the profes-\nsion domain, 2886 in the race domain, and 237 in\nthe religion domain.\nD Evaluation Metrics\nGiven a sentence si = [ w1\ni ,w2\ni ,...,w n\ni ] where\nwj\ni = [MASK], we can compute the probability\ndistribution of the tokens in the masked index by\ntaking\nM(·|left = [w1\ni ,...,w j−1\ni ],\nright = [wj+1\ni ,...,w n\ni ],θ). (3)\nSo, we can compute the probability that the model\nprefers a specific word in the context of sen-\ntence si, where si is understood to have a sin-\ngle [MASK] token at position j, by the notation\nM(si) = M(wj\ni |left = [w1\ni ,...,w j−1\ni ],right =\n[wj+1\ni ,...,w n\ni ],θ).\nSentence st is stereotypical, sa is antistereotyp-\nical, and the final sentence sn is the non-sensical\nsentence. As a reminder, for StereoSet we have all\nthree sentences and for CrowS we have only the\nsensical two sentences.\nStereoset. There are three evaluation met-\nrics proposed in the StereoSet dataset: the\nStereotype Score (SS), the Language Modeling\nScore (LMS), and the Idealized Context Associ-\nation Test score (ICAT).\nThe SS of a model M is the proportion of the\nsentence pairs in which the model tends to prefer\nthe stereotypical sentence over the antistereotypical\nsentence. For an evaluation set E,\nss(M) = E(st,sa,sn)∈E1 [M(st) >M (sa)] (4)\nAn ideal model without bias is claimed to have\nan SS score of 0.5 meaning that it does not prefer\neither a stereotype or an antistereotype in general.\nThe LMS score measures the basic language\nmodeling capability of a model and is intended to\nmimic a regression test. It is calculated as how\noften the model M prefers an acceptable sentence\nover a meaningless one.\nlms(M) = 1\n2E(st,sa,sn)∈E1 [M(st) >M (sn)]\n+ 1\n2E(st,sa,sn)∈E1 [M(sa) >M (sn)],\n(5)\nwhere we consider both stereotypical and antis-\ntereotypical sentences to be informative. A perfect\nlanguage model should have a score of 1 and a de-\nbiased language model should have a score similar\nto the original language model.\nICAT combines SS and LMS as\nicat(M) = lms(M)∗min{ss(M),1 −ss(M)}\n0.5 .\n(6)\n6045\nA perfect model achieves an ICAT of 1, a fully\nbiased model achieves an ICAT of 0, and a random\nmodel achieves an ICAT of 0.5.\nCrowS Pairs. The CrowS score is also based\non the masked language modeling probabilities but\ncomputed to condition on the prior probabilities\nof words. Given a pair of stereotypical and anti-\nstereotypical sentences (st,sa), we first split the\ntokens of each of them into constrastive tokens\nCt,Ca (soft vs determined in the example from\nSection 4.1) and overlapping tokens O. We then\ncompute the probability of each sentence via a sum-\nmation of masked language modeling log probabil-\nities of all overlapping tokens conditioned on the\nnon-overlapping tokens:\nQ(M,C) =\n∑\nj∈O\nlog P(j|C,O\\{j}) (7)\nFinally, the CrowS metric measures the proportion\nof CrowS pairs where the model assigned a higher\nprobability to the stereotypical sentence compared\nto the antistereotypical one:\ncrows(M) = E(st,sa)∈E1\n[\nQ(M,Ct) >Q(M,Ca)\n]\n(8)\nE Non-binary Bias Domains\nTo handle the multi-class setting (e.g., religion\nbias), we can adjust the weight block importance\ncalculation to be based on variance rather than only\ndirection (i.e., run PCA, then choose the weight\nvectors where the first few principal components\nexplain the most variance in the gradients) and ad-\njust the gradient optimization step to be based on a\nweighted average of the projection of the gradients.\nA weighted average of the gradients encodes the\nsame philosophy as the proposed binary form of\nPCGU from the main paper; consider that the cur-\nrent gradient update of decreasing the advantaged\nsentence would be identical (other than some scal-\ning) to a weighted average in the case where the\ngradients point in completely opposite directions\n(when they are slightly off opposite, it becomes ap-\nproximate). Also, with a weighted vector average,\nwe can still utilize the philosophy of decreasing the\nadvantaged forms (as suggested in Section 4.6).\nF Facts vs Bias\nThe boundary between fact and bias can often be\nblurry. Although we know some sentences may\ncontain unalienable truths, an LM without world\nknowledge may not. However, it should at least\nrecognize that these sentences represent facts. In\nthis sense, both the sentence “Men run faster at the\nOlympics” and the sentence “Women run faster at\nthe Olympics” could be reasonable (even if one is\nfalse).\nBy using WinoGender, we guarantee that all ex-\namples for PCGU contain bias, because they neces-\nsarily assume a gender. When probing our MLMs\nwith\n1. The runner tied [MASK] shoes.\n2. The fast runner tied [MASK] shoes.\n3. Men run [MASK] than women do.\n4. Women run [MASK] than men do.\nwe find that PCGU debiases the distribution of {his,\nher} for the first two sentences (both of which start\nout with “his” having the highest probability of\nall predicted words) but does not touch the distri-\nbution of the top words for the last two sentences\nwhich are shaped like facts (the distributions for\nboth sentences before PCGU have “faster” with\naround 90% of the probability mass, followed by\n“better,” “more,” and “longer.” After PCGU, the\norder of the words remains the same, and the prob-\nabilities remain constant as well, other than slight\nvariations on the order of <1%). So, it seems that\neven without explicitly differentiating “facts” from\n“bias,” the choice of training data allows PCGU to\nunlearn ideas that are clearly biased and leave those\ncloser to fact untouched. This may also suggest\nthat such facts and biases are encoded in separate\nparts of the PLM.\nOne nice feature of PCGU is that the decision of\nwhich sentence is advantaged/disadvantaged is de-\ncoupled from the rest of the method. If one wanted\nto use training data which may or may not contain\nfact, it seems reasonable that they could incorpo-\nrate some fact-checking/NLI model in the scoring\nfunction when determining which sentence is ad-\nvantaged/disadvantaged. Of course, this runs into\nthe problem that a biased scorer may incorrectly\nperceive an opinion to be factual, so that model it-\nself should be debiased, possibly via a self-training\nloop with PCGU.\n6046\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n6\n□\u0013 A2. Did you discuss any potential risks of your work?\n7\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n3,4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n3,4\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n3, Limitations\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n3, Appendix\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n6047\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4, Appendix\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n6048"
}