{
    "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
    "url": "https://openalex.org/W3159134453",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4221967483",
            "name": "Goyal, Naman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2748019825",
            "name": "Du, Jingfei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226508943",
            "name": "Ott, Myle",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226508949",
            "name": "Anantharaman, Giri",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221359244",
            "name": "Conneau, Alexis",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2982180741",
        "https://openalex.org/W2799054028",
        "https://openalex.org/W2983040767",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W2980404057",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2427527485",
        "https://openalex.org/W2989539713",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2958953787",
        "https://openalex.org/W3093517588",
        "https://openalex.org/W2963979492"
    ],
    "abstract": "Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.",
    "full_text": "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 29–33\nBangkok, Thailand (Online), August 6, 2021. ©2021 Association for Computational Linguistics\n29\nLarger-Scale Transformers for Multilingual Masked Language Modeling\nNaman Goyal Jingfei Du Myle Ott Giri Anantharaman Alexis Conneau\n{naman, jingfeidu, myleott, giriman, aconneau}@fb.com\nFacebook AI\nAbstract\nRecent work has demonstrated the effective-\nness of cross-lingual language model pretrain-\ning for cross-lingual understanding. In this\nstudy, we present the results of two larger mul-\ntilingual masked language models, with 3.5B\nand 10.7B parameters. Our two new mod-\nels dubbed XLM-RXL and XLM-RXXL outper-\nform XLM-R by 1.8% and 2.4% average ac-\ncuracy on XNLI. Our model also outperforms\nthe RoBERTa-Large model on several English\ntasks of the GLUE benchmark by 0.3% on av-\nerage while handling 99 more languages. This\nsuggests larger capacity models for language\nunderstanding may obtain strong performance\non both high- and low-resource languages. We\nmake our code and models publicly available.1\n1 Introduction\nThe goal of this paper is to present a study of\nthe impact of larger capacity models on cross-\nlingual language understanding (XLU). We scale\nthe capacity of XLM-R by almost two orders\nof magnitude while training on the same CC100\ndataset (Wenzek et al., 2019). Our two new multi-\nlingual masked language model dubbed XLM-RXL\nand XLM-RXXL, with 3.5 and 10.7 billion parame-\nters respectively, signiﬁcantly outperform the previ-\nous XLM-R model on cross-lingual understanding\nbenchmarks and obtain competitive performance\nwith the multilingual T5 models (Raffel et al., 2019;\nXue et al., 2020). We show that they can even out-\nperform RoBERTa-Large (Liu et al., 2019) on the\nGLUE benchmark (Wang et al., 2018).\nRecent multilingual masked language models\n(MLM) like mBERT (Devlin et al., 2018) or\nXLM (Lample and Conneau, 2019) improved cross-\nlingual language understanding by pretraining large\nTransformer models (Vaswani et al., 2017) on mul-\n1https://github.com/anonymous\ntiple languages at once. The XLM-R model (Con-\nneau et al., 2019) extended that approach by scal-\ning the amount of data by two orders of magni-\ntude, from Wikipedia to Common-Crawl and train-\ning longer, similar to RoBERTa (Liu et al., 2019).\nThese models are particularly effective for low-\nresource languages, where both labeled and un-\nlabeled data is scarce. They enable supervised\ncross-lingual transfer, where labeled data in one\nlanguage can be used to solve the same task in other\nlanguages, and unsupervised cross-lingual transfer,\nwhere low-resource language self-supervised repre-\nsentations are improved using additional unlabeled\ndata from higher-resource languages. Furthermore,\nthey reduce the need for training one model per\nlanguage, and allows the use of a single - poten-\ntially much larger - pretrained model that is then\nﬁne-tuned on annotated data from many languages.\nThe better performance of self-supervised cross-\nlingual models on low-resource languages comes\nhowever at the cost of lower performance on higher-\nresource languages (Arivazhagan et al., 2019).\nWhen the number of languages becomes large,\nConneau et al. (2019) even observed an overall\ndecrease of performance on all languages. It was\nhypothesized that when multilingual models get\nmore capacity, they may showcase strong perfor-\nmance on both high-resource languages and low-\nresource languages. With only 550M parameters,\nthe XLM-R model is now relatively small com-\npared to new standards. Recent work scaled lan-\nguage models to hundreds of billions (Brown et al.,\n2020) or even multiple trillion parameters (Fedus\net al., 2021), showing consistent gains in doing\nso. Recently, multilingual T5 showed impressive\nincrease in performance by scaling the model ca-\npacity to tens of billions of parameters. Our study\ncomplements these ﬁndings by showing the impact\nof larger capacity models on the important pretrain-\ning task of multilingual masked language model-\n30\ning. We show promising results for cross-lingual\nunderstanding: XLM-RXXL can both obtain a new\nstate of the art on some cross-lingual understanding\nbenchmarks and outperform the RoBERTa-Large\nmodel on the English GLUE benchmark (Wang\net al., 2018). This suggests that very large-scale\nmultilingual models may be able to beneﬁt from the\nbest of both worlds: obtaining strong performance\non high-resource languages while still allowing for\nzero-shot transfer and low-resource language un-\nderstanding. We make the following contributions:\n• We scale XLM capacity by two orders of mag-\nnitude, and publicly release XLM-R XL and\nXLM-RXXL with 3.5B and 10.7B parameters.\n• We show that those two models obtain very\nstrong performance on cross-lingual bench-\nmarks while outperforming RoBERTaLarge on\nthe GLUE benchmark.\n2 Pretraining and evaluation\nIn this section, we describe the model we use and\nhow we scale it, as well as the data and tasks we\nuse for pretraining and evaluation.\n2.1 Multilingual masked language models\nWe use a Transformer model (Vaswani et al., 2017)\ntrained with the multilingual MLM objective (De-\nvlin et al., 2018; Lample and Conneau, 2019) using\nonly monolingual data. We sample streams of text\nfrom each language and train the model to predict\nthe masked tokens in the input. We use the same\nlearning procedure as XLM-R. We apply subword\ntokenization directly on raw text data using Sen-\ntence Piece (Kudo and Richardson, 2018) with a\nunigram language model (Kudo, 2018) just like\nin XLM-R. We sample batches from different lan-\nguages using the same sampling distribution as\nConneau et al. (2019), with α= 0.3, and without\nlanguage embeddings. We use a large vocabulary\nsize of 250K with a full softmax and train two dif-\nferent models: XLM-R XL (L = 36, H = 2560, A\n= 32, 3.5B params) and XLM-R XXL (L = 48, H\n= 4096, A = 32, 10.7B params). We pretrain the\nmodels on the CC100 dataset, which corresponds\nto 167B tokens in 100 languages. We compare our\napproach to previous results as well as the mT5\nbaselines, which were pretrained on the larger mC4\ncorpus of 6.4T tokens.\n2.2 Evaluation\nWe consider three evaluation benchmarks. For\ncross-lingual understanding, we use cross-lingual\nnatural language inference and question answer-\ning, and use the GLUE benchmark to evaluate the\nEnglish performance.\nCross-lingual Natural Language Inference.\nThe XNLI dataset (Conneau et al., 2018) comes\nwith ground-truth dev and test sets in 15 languages,\nand a ground-truth English training set. The train-\ning set has been machine-translated to the remain-\ning 14 languages, providing synthetic training data\nfor these languages as well. We evaluate our model\non cross-lingual transfer from English to other lan-\nguages. We also consider two machine translation\nbaselines: (i) translate-test: dev and test sets are\nmachine-translated to English and a single English\nmodel is used (ii) translate-train-all: the English\ntraining set is machine-translated to each language\nand we ﬁne-tune a multilingual model on all train-\ning sets. For translations, we use the original XNLI\ndata for consistency.\nCross-lingual Question Answering. We use the\nMLQA and XQuad benchmark from Lewis et al.\n(2019) and Artetxe et al. (2019), which extends\nthe English SQuAD benchmark to more languages.\nWe report the F1 score as well as the exact match\n(EM) score for cross-lingual transfer from English.\nThe English GLUE Benchmark. Finally, we\nevaluate the English performance of our model\non the GLUE benchmark (Wang et al., 2018)\nwhich gathers multiple classiﬁcation tasks, such\nas MNLI (Williams et al., 2017), SST-2 (Socher\net al., 2013), or QNLI (Rajpurkar et al., 2018).\n2.3 Training details\nWe use model parallelism based on tensor paral-\nlel (Shoeybi et al., 2019) for scaling models. XLM-\nRXL uses model parallel size of 2 and XLM-RXXL\nused 8. Compared to previous XLM-R models, we\nreduce the batch size and number of updates sig-\nniﬁcantly to keep the compute of the new models\nsimilar (see Table 5). For both models, we use\nbatch size of 2048 and train for 500,000 updates.\nWe use pre-LayerNorm setting for both the models\nwhich was more stable during training.\nFor all the tasks in ﬁnetuning, we use batch size\nof 32 and train for 10 epochs. We do early stop-\nping based on the average valid metrics across all\nlanguages and report test results.\n31\nModel Data (#tok)en fr es de el bg ru tr ar vi th zh hi sw ur Avg\nFine-tune multilingual model on English training set (Cross-lingual Transfer)\nmBERT Wikipedia80.8 64.3 68.0 70.0 65.3 73.5 73.4 58.9 67.8 49.7 54.1 60.9 57.2 69.3 67.8 65.4\nXLM 83.2 76.5 76.3 74.2 73.1 74.0 73.1 67.8 68.5 71.2 69.2 71.9 65.7 64.6 63.4 71.5\nmT5-Base\nmC4\n84.7 73.3 78.6 77.4 77.1 80.3 79.1 70.8 77.1 69.4 73.2 72.8 68.3 74.2 74.1 75.4\nmT5-Large 89.4 79.8 84.1 83.4 83.2 84.2 84.1 77.6 81.5 75.4 79.4 80.1 73.5 81.0 80.3 81.1\nmT5-XL (6.4T) 90.6 82.2 85.4 85.8 85.4 81.3 85.3 80.4 83.7 78.6 80.9 82.0 77.0 81.8 82.7 82.9\nmT5-XXL 91.6 84.5 87.7 87.3 87.3 87.8 86.9 83.2 85.1 80.3 81.7 83.8 79.8 84.6 83.6 84.5\nXLM-RBase\nCC100\n85.8 79.7 80.7 78.7 77.5 79.6 78.1 74.2 73.8 76.5 74.6 76.7 72.4 66.5 68.3 76.2\nXLM-RLarge 89.1 84.1 85.1 83.9 82.9 84.0 81.2 79.6 79.8 80.8 78.1 80.2 76.9 73.9 73.8 80.9\nXLM-RXL (167B) 90.7 85.5 86.5 84.6 84.0 85.2 82.7 81.7 81.6 82.4 79.4 81.7 78.5 75.3 74.3 82.3\nXLM-RXXL 91.6 86.2 87.3 87.0 85.1 85.7 82.5 82.0 82.5 83.0 79.5 82.6 79.8 76.2 74.9 83.1\nTranslate everything to English and use English-only model (TRANSLATE-TEST)\nRoBERTa CC-En 91.3 82.9 84.3 81.2 81.7 83.1 78.3 76.8 76.6 74.2 74.1 77.5 70.9 66.7 66.8 77.8\nFine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)\nmT5-Base\nmC4\n82.0 74.4 78.5 77.7 78.1 79.1 77.9 72.2 76.5 71.5 75.0 74.8 70.4 74.5 76.0 75.9\nmT5-Large 88.3 80.3 84.1 84.0 83.7 84.9 83.8 79.8 82.0 76.4 79.9 81.0 75.9 81.3 81.7 81.8\nmT5-XL (6.4T) 90.9 84.2 86.8 86.8 86.4 87.4 86.8 83.1 84.9 81.3 82.3 84.4 79.4 83.9 84.0 84.8\nmT5-XXL 92.7 87.2 89.4 89.8 89.5 90.0 89.1 86.5 87.6 84.3 85.6 87.1 83.8 87.5 86.5 87.8\nXLM-RBase\nCC100\n85.4 81.4 82.2 80.3 80.4 81.3 79.7 78.6 77.3 79.7 77.9 80.2 76.1 73.1 73.0 79.1\nXLM-RLarge 89.1 85.1 86.6 85.7 85.3 85.9 83.5 83.2 83.1 83.7 81.5 83.7 81.6 78.0 78.1 83.6\nXLM-RXL (167B) 91.1 87.2 88.1 87.0 87.4 87.8 85.3 85.2 85.3 86.2 83.8 85.3 83.1 79.8 78.2 85.4\nXLM-RXXL 91.5 87.6 88.7 87.8 87.4 88.2 85.6 85.1 85.8 86.3 83.9 85.6 84.6 81.7 80.6 86.0\nTable 1: Results on cross-lingual classiﬁcation (XNLI).We report the accuracy on each of the 15 XNLI languages\nand average accuracy, and specify the dataset and its corresponding size in number of tokens. We report results of\nXLM-R models with increasing capacity, from 270M (Base), 550M (Large), 3.5B (XL) to 10.7B (XXL) parameters.\n3 Analysis and Results\nIn this section, we present our results and compare\nXLM-RXL and XLM-RXXL performance to other\nmethods from previous work.\nCross-lingual understanding results. On\nXNLI, we observe in Table 1 that scaling the\ncapacity from XLM-R Large to XLM-R XL leads\nto an average accuracy improvement of 1.4 on\nzero-shot cross-lingual transfer and 1.8 on mul-\ntilingual ﬁne-tuning. When scaling even further\nto XLM-RXXL, we observe a total improvement\nof 2.2 on zero-shot and 2.4 on translate-train-all\ncompared to XLM-R XL, with a new state of\nthe art on French, Vietnamese and Hindi. On\nMLQA, in Table 4, we observe even larger\ngains for cross-lingual zero-shot transfer, where\nscaling from XLM-R Large to XLM-R XXL leads\nto improvements of 4.1 F1 and 3.9 EM scores\non average. Similarly, on XQuad we observe\nimprovements of 4.4 F1 and 5.5 scores, with new\nstate-of-the-art results on Arabic, German, Greek\nand Russian (see Table 3).\nComparison to monolingual English model.\nFor smaller-capacity models like the Base and\nLarge version of XLM-R, it was shown that the\nmore languages are considered the lower the perfor-\nmance (Conneau et al., 2019), in particular on high-\nresource languages. For instance, XLM-R Large\nwas outperformed by RoBERTa Large by 1% ac-\ncuracy on average on several downstream tasks\nfrom the GLUE benchmark, as illustrated in Ta-\nble2. With larger capacity, we now observe that\nXLM-RXXL is able to outperform RoBERTaLarge\nby 0.3 dev points, going from 92.9 to 93.2 aver-\nage accuracy, while handling 99 more languages.\nWhile a RoBERTa XXL model may outperform\nXLM-RXXL, we believe it interesting to notice that\nwith more capacity, a multilingual model can get\nstrong high-resource performance while not losing\nits cross-lingual transfer ability for lower-resource\nlanguages. Given the compute needed for training\nsuch large-scale models, the possibility of training\na single very large model on hundreds of languages\nwith state-of-the-art performance on high-resource\nlanguages is an encouraging and positive result.\nModel #lgs MNLI QNLI QQP SST MRPCAvg\nRoBERTa† 1 90.2 94.7 92.2 96.4 90.9 92.9\nXLM-RLarge 100 88.9 93.8 92.3 95.0 89.5 91.9\nXLM-RXL 100 90.4 94.9 92.5 96.6 90.4 93.0\nXLM-RXXL 100 90.9 95.0 92.6 96.7 90.7 93.2\nTable 2: GLUE dev results\n32\nModel en ar de el es hi ru th tr vi zh avg\nCross-lingual zero-shot transfer (models ﬁne-tune on English data only)\nmT5-Large88.4 / 77.3 75.2 / 56.7 80.0 / 62.9 77.5 / 57.6 81.8 / 64.2 73.4 / 56.6 74.7 / 56.9 73.4 / 62.0 76.5 / 56.3 79.4 / 60.3 75.9 / 65.5 77.8 / 61.5mT5-XL88.8 / 78.1 77.4 / 60.8 80.4 / 63.5 80.4 / 61.2 82.7 / 64.5 76.1 / 60.3 76.2 / 58.8 74.2 / 62.5 77.7 / 58.4 80.5 / 60.8 80.5 / 71.0 79.5 / 63.6mt5-XXL90.9 / 80.1 80.3/ 62.683.1/ 65.5 83.3 /65.5 85.1 / 68.1 81.7 / 65.979.3 / 63.677.8 / 66.1 80.2/ 60.983.1 / 63.6 83.1 / 73.4 82.5 / 66.8\nXLM-RLarge86.5 / 75.7 68.6 / 49.0 80.4 / 63.4 79.8 / 61.7 82.0 / 63.9 76.7 / 59.7 80.1 / 64.3 74.2 / 62.8 75.9 / 59.3 79.1 / 59.0 59.3 / 50.0 76.6 / 60.8XLM-RXL 89.5 / 79.0 78.4 / 61.6 81.3 / 64.1 82.3 / 63.9 84.6 / 66.2 78.8 / 63.2 81.5 / 65.0 76.0 / 65.5 73.9 / 57.9 81.7 / 61.8 72.3 / 66.1 80.0 / 64.9XLM-RXXL 89.3 / 79.4 80.1 /63.782.7 /65.8 83.4 / 65.583.8 / 66.0 80.7 / 65.482.4 / 65.476.6 / 65.6 76.8 /61.782.2 / 63.0 74.1 / 67.4 81.1 / 66.3\nTable 3: XQuad results (F1/EM) for each language.\nModel en es de ar hi vi zh Avg\nCross-lingual zero-shot transfer (models ﬁne-tune on English data only)\nmT5-Large84.9 / 70.7 65.3 / 44.6 68.9 / 51.8 73.5 / 54.1 66.9 / 47.7 72.5 / 50.7 66.2 / 42.0 71.2 / 51.7\nmT5-XL 85.5 / 71.9 68.0 / 47.4 70.5 / 54.4 75.2 / 56.3 70.5 / 51.0 74.2 / 52.8 70.5 / 47.2 73.5 / 54.4\nmT5-XXL86.7 / 73.5 70.7 / 50.4 74.0 / 57.8 76.8 / 58.4 75.6 / 57.3 76.4 / 56.0 71.8/ 48.876.0 / 57.4\nXLM-RLarge 80.6 / 67.8 74.1 / 56.0 68.5 / 53.6 63.1 / 43.5 69.2 / 51.6 71.3 / 50.9 68.0 / 45.4 70.7 / 52.7\nXLM-RXL 85.1 / 72.6 66.7 / 46.2 70.5 / 55.5 74.3 / 56.9 72.2 / 54.7 74.4 / 52.9 70.9 / 48.5 73.4 / 55.3\nXLM-RXXL 85.5 / 72.4 68.6 / 48.4 72.7 /57.8 75.4 / 57.6 73.7 / 55.8 76.0 / 55.0 71.7 /48.9 74.8 / 56.6\nTable 4: MLQA results (F1/EM) for each language.\nDiscussion and comparison to mT5. Both mT5\nand XLM-R models obtain strong performance on\ncross-lingual understanding benchmarks, as well\nas high performance on English benchmarks (see\nthe score of 91.6 of mT5 XXL on English XNLI).\nMany hyperparameters are however different be-\ntween mT5 and XLM-R models which makes difﬁ-\ncult an apple-to-apple comparison. First, as shown\nin Table 5, the mT5 models are pretrained on the\nmuch larger mC4 dataset which contains around\n6.4T tokens, which is 38 times bigger than CC100\n(167B tokens). While XLM-RLarge was pretrained\nwith more updates (6T tokens), the XLM-RXL and\nXLM-RXXL models have seen less tokens (0.5T)\nduring pretraining than their mT5 counterparts, al-\nthough it also uses a bigger batch size (2048 over\n1024 for mT5). Another difference is the context\nsequence length of 512 for XLM-R and 1024 for\nmT5. The mT5-XXL model also has slightly more\nparameters (13B over 10.7B). The larger number\nof updates combined with the larger dataset size\nmay explain the larger improvement from the XL\nmodel to the XXL model in the case of mT5 (+3 av-\nerage accuracy on XNLI), in which the additional\nModel Number ofDatasetDatasetNumber ofBatchSequenceparametersname size training tokenssize length\nXLM-RLarge 550M CC100167B 6T 8192 512XLM-RXL 3.5B CC100167B 0.5T 2048 512XLM-RXXL 10.7B CC100167B 0.5T 2048 512mt5-XL 3.7B mC4 6.4T 1T 1024 1024mt5-XXL 13B mC4 6.4T 1T 1024 1024\nTable 5: Comparison of datasets and pretraining details\nbetween XLM-R and mT5. We report dataset sizes and\nnumber of updates in terms of number of tokens.\ncapacity can exploit the large quantity of unlabeled\nmC4 data. We note however that the mT5 XL is\noutperformed by XLM-RXL on XNLI by 0.6% on\naverage, on XQuad by 1.3% and on MLQA by\n0.9% when considering average EM score. In com-\nparison, gains of XLM-R from the XL to the XXL\narchitecture are only of 0.6 on average. Another\nexplanation may be that generative models scale\nbetter than masked language models. The differ-\nence in the nature of the pretraining dataset is par-\nticularly striking when looking at the variance of\nperformance across languages. For example the\nmT5XXL outperforms XLM-RXXL by 8.4 points on\nSwahili on XNLI zero-shot, while it only outper-\nforms XLM-RXXL by 1.4 average accuracy. These\nresults may suggest that the CC100 dataset gets\nsaturated with current larger-capacity models.\n4 Conclusion\nIn this study, we scaled the model capacity of the\nXLM-R model up to 10.7B parameters and ob-\ntained stronger performance than previous XLM-\nR models on cross-lingual understanding bench-\nmarks. We also show that the additional capac-\nity allows a multilingual model to outperform a\nthe RoBERTaLarge baseline on English benchmarks.\nOur technical study thus suggests that larger capac-\nity multilingual model can obtain state-of-the-art\ncross-lingual understanding results while maintain-\ning strong performance on high-resource languages.\nOur work provides an alternative to mT5 models,\nwith new state-of-the-art performance on some lan-\nguages. We release our code and models publicly.\n33\nReferences\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat,\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George Foster, Colin\nCherry, et al. 2019. Massively multilingual neural\nmachine translation in the wild: Findings and chal-\nlenges. arXiv preprint arXiv:1907.05019.\nMikel Artetxe, Sebastian Ruder, and Dani Yo-\ngatama. 2019. On the cross-lingual transferabil-\nity of monolingual representations. arXiv preprint\narXiv:1910.11856.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Proc. of NeurIPS.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In EMNLP. Asso-\nciation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. NAACL.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv\npreprint arXiv:2101.03961.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In ACL, pages 66–75.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\nEMNLP.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. NeurIPS.\nPatrick Lewis, Barlas O ˘guz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2019. Mlqa: Eval-\nuating cross-lingual extractive question answering.\narXiv preprint arXiv:1910.07475.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. ACL.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In EMNLP, pages 1631–1642.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzman, Ar-\nmand Joulin, and Edouard Grave. 2019. Ccnet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. arXiv preprint arXiv:1911.00359.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus\nfor sentence understanding through inference. Pro-\nceedings of the 2nd Workshop on Evaluating Vector-\nSpace Representations for NLP.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934."
}