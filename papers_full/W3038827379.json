{
    "title": "Data Movement Is All You Need: A Case Study on Optimizing Transformers",
    "url": "https://openalex.org/W3038827379",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2314101779",
            "name": "Ivanov, Andrei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4284460739",
            "name": "Dryden, Nikoli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227709863",
            "name": "Ben-Nun, Tal",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2146932660",
            "name": "Li ShiGang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2744306648",
            "name": "Hoefler, Torsten",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2972073717",
        "https://openalex.org/W2981758446",
        "https://openalex.org/W2969388332",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2992192853",
        "https://openalex.org/W2141599568",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2982413405",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2102976251",
        "https://openalex.org/W1753482797",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2806891462",
        "https://openalex.org/W3035435378",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3034609440",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W1530262073",
        "https://openalex.org/W2972087877",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W2901541570",
        "https://openalex.org/W2036551003",
        "https://openalex.org/W2911935295",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W2769856846",
        "https://openalex.org/W2963703618",
        "https://openalex.org/W2995359496",
        "https://openalex.org/W2786320458",
        "https://openalex.org/W1218987319",
        "https://openalex.org/W2970777192",
        "https://openalex.org/W3007772124",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2785452945",
        "https://openalex.org/W2338908902",
        "https://openalex.org/W2966258469",
        "https://openalex.org/W3034340181",
        "https://openalex.org/W3037749908",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970389371",
        "https://openalex.org/W2162390675",
        "https://openalex.org/W2622263826",
        "https://openalex.org/W2942460556",
        "https://openalex.org/W2920668770",
        "https://openalex.org/W2984085861",
        "https://openalex.org/W2510697685",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2963112338",
        "https://openalex.org/W1972501001",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2767935072",
        "https://openalex.org/W2962747323",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2962700998",
        "https://openalex.org/W2946567085",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2594730095",
        "https://openalex.org/W2899366056",
        "https://openalex.org/W2996151541",
        "https://openalex.org/W3021423880",
        "https://openalex.org/W2617411258",
        "https://openalex.org/W2977720775",
        "https://openalex.org/W3037639655",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2899971035",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W3037847693",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W1667652561",
        "https://openalex.org/W2991040477",
        "https://openalex.org/W1922123711",
        "https://openalex.org/W2055312318",
        "https://openalex.org/W2558748708",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W1442374986",
        "https://openalex.org/W2294929133",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2784561332",
        "https://openalex.org/W2963351145",
        "https://openalex.org/W2983101505",
        "https://openalex.org/W2988394319",
        "https://openalex.org/W2994759459",
        "https://openalex.org/W1828163288",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2798341898",
        "https://openalex.org/W1902237438"
    ],
    "abstract": "Transformers are one of the most important machine learning workloads today. Training one is a very compute-intensive task, often taking days or weeks, and significant attention has been given to optimizing transformers. Despite this, existing implementations do not efficiently utilize GPUs. We find that data movement is the key bottleneck when training. Due to Amdahl's Law and massive improvements in compute performance, training has now become memory-bound. Further, existing frameworks use suboptimal data layouts. Using these insights, we present a recipe for globally optimizing data movement in transformers. We reduce data movement by up to 22.91% and overall achieve a 1.30x performance improvement over state-of-the-art frameworks when training a BERT encoder layer and 1.19x for the entire BERT. Our approach is applicable more broadly to optimizing deep neural networks, and offers insight into how to tackle emerging performance bottlenecks.",
    "full_text": "DATA MOVEMENT IS ALL YOU NEED : A C ASE STUDY ON OPTIMIZING\nTRANSFORMERS\nAndrei Ivanov * 1 Nikoli Dryden * 1 Tal Ben-Nun1 Shigang Li 1 Torsten Hoeﬂer1\nABSTRACT\nTransformers are one of the most important machine learning workloads today. Training one is a very compute-\nintensive task, often taking days or weeks, and signiﬁcant attention has been given to optimizing transformers.\nDespite this, existing implementations do not efﬁciently utilize GPUs. We ﬁnd that data movement is the key\nbottleneck when training. Due to Amdahl’s Law and massive improvements in compute performance, training has\nnow become memory-bound. Further, existing frameworks use suboptimal data layouts. Using these insights,\nwe present a recipe for globally optimizing data movement in transformers. We reduce data movement by up to\n22.91% and overall achieve a 1.30×performance improvement over state-of-the-art frameworks when training a\nBERT encoder layer and 1.19×for the entire BERT. Our approach is applicable more broadly to optimizing deep\nneural networks, and offers insight into how to tackle emerging performance bottlenecks.\n1 I NTRODUCTION\nTransformers (Vaswani et al., 2017) are a class of deep neu-\nral network architecture for sequence transduction (Graves,\n2012), with similar applicability as RNNs (Rumelhart et al.,\n1986) and LSTMs (Hochreiter & Schmidhuber, 1997). They\nhave recently had a major impact on natural language pro-\ncessing (NLP), including language modeling (Radford et al.,\n2018; Wang et al., 2018; 2019), question-answering (Ra-\njpurkar et al., 2018), translation (Vaswani et al., 2017), and\nmany other applications. The signiﬁcant improvement in\naccuracy brought by transformers to NLP tasks is compa-\nrable to the improvement brought to computer vision by\nAlexNet (Krizhevsky et al., 2012) and subsequent convo-\nlutional neural networks. Transformers have also begun to\nbe applied to domains beyond NLP, including vision (Doso-\nvitskiy et al., 2020), speech recognition (Yeh et al., 2019),\nreinforcement learning (Parisotto et al., 2019), molecular\nproperty prediction (Maziarka et al., 2020), and symbolic\nmathematics (Lample & Charton, 2019).\nTraining transformers is very compute-intensive, often tak-\ning days on hundreds of GPUs or TPUs (Devlin et al., 2019;\nYang et al., 2019; Liu et al., 2019; Keskar et al., 2019;\nShoeybi et al., 2019; Lan et al., 2019; Raffel et al., 2019).\nFurther, generalization only improves with model size (Rad-\nford et al., 2019; Shoeybi et al., 2019; Raffel et al., 2019;\n*Equal contribution 1Department of Computer Science, ETH\nZürich, Switzerland. Correspondence to: Andrei Ivanov <ani-\nvanov@inf.ethz.ch>.\nProceedings of the 4 th MLSys Conference, San Jose, CA, USA,\n2021. Copyright 2021 by the author(s).\nMicrosoft, 2020b), number of training samples (Raffel et al.,\n2019; Liu et al., 2019), and total iterations (Liu et al., 2019;\nKaplan et al., 2020). These all signiﬁcantly increase com-\npute requirements. Indeed, transformers are becoming the\ndominant task for machine learning compute where training\na model can cost tens of thousands to millions of dollars and\neven cause environmental concerns (Strubell et al., 2019).\nThese trends will only accelerate with pushes toward models\nwith tens of billions to trillions of parameters (Microsoft,\n2020b;c), their corresponding compute requirements (Ope-\nnAI, 2018), and increasing corporate investment towards\nchallenges such as artiﬁcial general intelligence (OpenAI,\n2019). Thus, improving transformer performance has been\nin the focus of numerous research and industrial groups.\nSigniﬁcant attention has been given to optimizing transform-\ners: local and ﬁxed-window attention (Bahdanau et al., 2014;\nLuong et al., 2015; Shen et al., 2018; Parmar et al., 2018; Tay\net al., 2019), more general structured sparsity (Child et al.,\n2019), learned sparsity (Correia et al., 2019; Sukhbaatar\net al., 2019; Tay et al., 2020), and other algorithmic tech-\nniques (Lan et al., 2019; Kitaev et al., 2020) improve\nthe performance of transformers. Major hardware efforts,\nsuch as Tensor Cores and TPUs (Jouppi et al., 2017) have\naccelerated tensor operations like matrix-matrix multipli-\ncation (MMM), a core transformer operation. Despite\nthis, existing implementations do not efﬁciently utilize\nGPUs. Even optimized implementations such as Mega-\ntron (Shoeybi et al., 2019) report achieving only 30% of\npeak GPU ﬂoating point operations per second (ﬂop/s).\nWe ﬁnd that the key bottleneck when training transform-\ners is data movement. Improvements in compute perfor-\narXiv:2007.00072v3  [cs.LG]  8 Nov 2021\nData Movement Is All You Need\n@dace.program\ndef mha_forward(\nwq: dace.float16[P, H, I], bq: dace.float16[P, H],\nq: dace.float16[I, B, J],\nwk: dace.float16[P, H, I], bk: dace.float16[P, H],\nk: dace.float16[I, B, K],\nwv: dace.float16[W, H, I], bv: dace.float16[W, H],\nv: dace.float16[I, B, K],\nwo: dace.float16[W, H, I], bo: dace.float16[I],\nscaler: dace.float16\n):\nqq = np.einsum(\"phi,ibj->phbj\", wq, q) + bq[:,:,None,None]\nkk = np.einsum(\"phi,ibk->phbk\", wk, k) + bk[:,:,None,None]\nvv = np.einsum(\"whi,ibk->whbk\", wv, v) + bv[:,:,None,None]\nbeta = scaler * np.einsum(\"phbk,phbj->hbjk\", kk, qq)\nalpha = dropout(softmax(beta))\ngamma = np.einsum(\"whbk,hbjk->whbj\", vv, alpha)\nout = np.einsum(\"whi,whbj->ibj\", wo, gamma)+bo[:,None,None]\nreturn out\n(a) Input Code\nWV V\nwhi,ibk->whbk\nwhbk,hbjk->whbj\nwhi,whbj->ibj\nOUT\nWQ Q\nphi,ibj->phbj\nWK K\nphi,ibk->phbk\nphbk,phbj->hbjk\nsoftmax [k]\n9G 910\n4G 102\n9G 910\n9G 9109G 910\n4G 102\n168M 2.5 ﬂop\nﬂop / IO\nMHA \ndropout\nbias [ph]bias [ph]\n4M 0.54M 0.5\n34M 0.5\nbias [wh]\n4M 0.5\nbias [i]\n4M 0.5\nscalerWO\nIO > ﬂop\nIO ≈ ﬂop\nIO < ﬂop\nTensor contraction\nNormalization\nElementwise\nVV\nKK QQ\nGAMMA\nALPHA\nBETA (b) Resulting dataﬂow\nFigure 1.Input code and stateful dataﬂow multigraph (SDFG) for Multi-Head Attention. Throughout the paper, if not stated otherwise, the\nvalues are given for the following set of parameters: P =W =64, H =16, I =P ⋅H =1024, B=8, J =K =512. P/W: key/value\nprojection size; H: # heads; I: embedding size; B: batch size; J/K: input/output sequence length.\nmance have reached the point that, due to Amdahl’s Law\nand the acceleration of tensor contractions, training is now\nmemory-bound. Over a third (37%) of the runtime in a\nBERT training iteration is spent in memory-bound opera-\ntors: While tensor contractions account for over 99% of the\narithmetic operations performed, they constitute only 61%\nof the runtime. By optimizing these, we show that the over-\nhead of data movement can be reduced by up to 22.91%.\nFurther, while MMM is highly tuned by BLAS libraries\nand hardware, we also ﬁnd that existing frameworks use\nsuboptimal data layouts. Using better layouts enables us\nto speed up MMM by up to 52%. Combining these insights\nrequires moving beyond peephole-style optimizations and\nglobally optimizing data movement, as selecting a single\nlayout is insufﬁcient. Overall, we achieve at least 1.30×\nperformance improvements in training over general-purpose\ndeep learning frameworks, and 1.08×over DeepSpeed (Mi-\ncrosoft, 2020a), the state of the art manually-tuned imple-\nmentation of BERT. For robustly training BERT (Liu et al.,\n2019), this translates to a savings of over $85,000 on AWS\nusing PyTorch. For the GPT-3 transformer model (Brown\net al., 2020) with a training cost of $12M (Wiggers, 2020),\nour optimizations could save $3.6M and more than 120\nMWh energy. To go beyond this, we develop a recipe for\nsystematically optimizing data movement in DNN training.\nOur approach constructs a dataﬂow graph for the training\nprocess, which we use to identify operator dependency pat-\nterns and data volume. With this representation, we identify\nopportunities for data movement reduction to guide opti-\nmization. We aim to maximize data reuse using various\nforms of fusion. Then we select performant data layouts,\nwhich is particularly impactful for normalization and ten-\nsor contraction operators, where it provides opportunities\nfor vectorization and different tiling strategies. The perfor-\nmance data gathered is then used to ﬁnd operator conﬁgura-\ntions that produce an optimized end-to-end implementation.\nWe evaluate these implementations ﬁrst for multi-head atten-\ntion, a core primitive within transformers and one that has\nsigniﬁcant applications beyond transformers (Bello et al.,\n2019; Parmar et al., 2019; Cordonnier et al., 2020). We then\nconsider the encoder layer from BERT (Devlin et al., 2019),\na widely-used transformer architecture. In each case, we\ncompare against existing highly optimized implementations\nto provide strong baselines. Using this recipe, we are able to\ndemonstrate signiﬁcant performance improvements in both\nmicrobenchmarks and end-to-end training, outperforming\nPyTorch (Paszke et al., 2019), TensorFlow+XLA (Abadi\net al., 2015), cuDNN (Chetlur et al., 2014), and Deep-\nSpeed (Microsoft, 2020a). While we focus our work on\nparticular transformer models, our approach is generally\napplicable to other DNN models and architectures. We\nsummarize our contributions as follows:\n• We ﬁnd transformer training to be memory-bound and\nsigniﬁcantly underperforming on GPUs.\n• We develop a generic recipe for optimizing training using\ndataﬂow analyses.\n• Using this recipe, we systematically explore performance\nof operators and the beneﬁts of different optimizations.\n• We demonstrate signiﬁcant performance improvements,\nreducing data movement overheads by up to 22.91% over\nexisting implementations, and achieving at least 1.08×\nperformance improvements over specialized libraries, and\n1.30×over general-purpose frameworks.\n• We make our code publicly available at https://github.\ncom/spcl/substation.\nData Movement Is All You Need\nLinear+Bias\nDropout\nLayerNorm\nMHA\nReLU\nX\nY\nReLU dX\nMHA back\nLinear+Bias dXLinear+Bias dW\nLayerNorm dXLayerNorm dW\nDropout dX\ndY\ndX\nDropout\nLinear+Bias\nDropout\nLayerNorm\nDropout dX\nLinear+Bias dXLinear+Bias dW\nLayerNorm dXLayerNorm dW\nDropout dX\nln1 dW\nlinear1 dW\nlinear2 dW\nln2 dW\n1/3\n3.5\n585\n1/3\n1024\n1/3\n2.33 2 3\n1/3\n1/3\n1/3\n1170\n819\n1365\n1365\n2 3\n153 28043G\n4M\n29M\n34G\n17M\n34G\n4M\n29M 17M 38M\n4M\n34G 34G\n17M\n34G34G\n17M 38M\n4M\n86G\nﬂop\nﬂop / IO\nIO > ﬂop\nIO ≈ ﬂop\nIO < ﬂop\n1/34M\n4M 1/3\n4M\n4M 1/3\n1/3\nTensor contraction\nNormalization\nElementwise\nFigure 2.Forward and backpropagation for a BERT-large encoder\nlayer, and the ratio of ﬂop to memory accesses (words) when\ntraining on a batch B=8 and sequence length J =K =512.\n2 B ACKGROUND\nHere we provide a brief overview of our terminology, trans-\nformers, and data-centric programming. We assume the\nreader is generally familiar with training deep neural net-\nworks (see Goodfellow et al. (2016) for an overview) and\ntransformers (see Sec. A.1 for a more complete overview).\n2.1 Transformers\nWe use the standard data-parallel approach to training,\nwherein a mini-batch of samples is partitioned among many\nGPUs. During backpropagation, we distinguish between\ntwo stages: computing the gradients with respect to a layer’s\ninput (dX), and computing the gradients with respect to the\nlayer’s parameters (dW). Note that the second stage is\nrelevant only for layers with learnable parameters.\nThe transformer architecture (Vaswani et al., 2017) con-\nsists of two main components: multi-head attention (MHA)\nand a feed-forward network. We provide Python code and\nan illustration of MHA forward propagation in Fig. 1. At-\ntention has three input tensors, queries ( q), keys (k), and\nvalues (v). The inputs are ﬁrst multiplied by weight ten-\nsors wq, wk, wv, respectively, as a learned input projection\n(we use Einstein-notation sums, or einsums, for tensor\ncontractions). The query and key tensors are subsequently\nmultiplied together and scaled (stored in beta), followed\nby a softmax operation. This is then multiplied with vv to\nproduce the per-head output (gamma). The outputs of all\nheads are ﬁnally concatenated and linearly projected back\nto the input dimension size (i).\nThe respective dataﬂow graph (Fig. 1b) immediately ex-\nposes coarse- (whole graph) and ﬁne-grained (within rect-\nangular nodes) parallelism, as well as data reuse. As every\nedge represents exact data movement, their characteristics\n(access sets and movement volume) can be inspected for\nbottlenecks and potential solution analysis.\nWe focus on BERT (Devlin et al., 2019). Fig. 2 illustrates\nthe forward and backward pass for a single BERT encoder\nlayer. The layer essentially consists of MHA followed by a\nfeed-forward neural network (two linear layers with bias and\nReLU activations after the ﬁrst layer). Dropout (Srivastava\net al., 2014), layer normalization (Ba et al., 2016), and\nresidual connections (He et al., 2016) are also used.\n2.2 Data-Centric Programming\nAs DNN processing is among the most popular compute-\nintensive applications today, considerable efforts have been\nmade to optimize its core operators (Ben-Nun & Hoeﬂer,\n2019). This has driven the ﬁeld to the point where opti-\nmization today is almost exclusively performed beyond the\nindividual operator, either on the whole network (Google,\n2020; PyTorch Team, 2020) or repeated modules.\nPerformance optimization on modern architectures consists\nof mutations to the original code, sometimes algorithmic\n(Chellapilla et al., 2006; Mathieu et al., 2013; Lavin & Gray,\n2016) but mostly relating to hardware-speciﬁc mapping of\ncomputations and caching schemes. This includes tiling\ncomputations for speciﬁc memory hierarchies, using spe-\ncialized units (e.g., Tensor Cores) for bulk-processing of\ntiles, modifying data layout to enable parallel reductions,\nhiding memory latency via multiple buffering, pipelining,\nand using vectorized operations. It is thus apparent that\nall current optimization techniques revolve around careful\ntuning of data movement and maximizing data reuse.\nThe Data-Centric (DaCe) parallel programming frame-\nwork (Ben-Nun et al., 2019) enables performance optimiza-\ntion on heterogeneous architectures by deﬁning a develop-\nment workﬂow that enforces a separation between com-\nputation and data movement. The core concept powering\nprogram transformation is the Stateful Dataﬂow multiGraph\n(SDFG), a graph intermediate representation that deﬁnes\ncontainers and computation as nodes, and data movement\nas edges. DaCe takes input code written in Python or DSLs,\nData Movement Is All You Need\nand outputs corresponding SDFGs. Programmers can mu-\ntate the dataﬂow via user-extensible graph-rewriting trans-\nformations to change the schedule of operations, the layout\nof data containers, mapping of data and computation to\ncertain processing elements, or any adaptation to the data\nmovement that does not change the underlying computation.\nAs opposed to traditional optimizing compilers and deep\nlearning frameworks (e.g., XLA, TorchScript), DaCe pro-\nmotes a white-box approach for performance optimization.\nThe framework provides an API to programmatically instru-\nment and explore, e.g., layouts and kernel fusion strategies,\nwithout modifying the original code. DaCe can map appli-\ncations to different hardware architectures, including CPUs,\nGPUs, and FPGAs (Ben-Nun et al., 2019), enabling both\nwhole-program and micro-optimizations of nontrivial appli-\ncations to state-of-the-art performance (Ziogas et al., 2019).\nThe combination of the separation of the algorithm from\nthe transformed representation and white-box approach for\noptimization enables us to inspect and optimize the data\nmovement characteristics of Transformer networks. As we\nshall show in the next sections, this drives a methodical\napproach to optimizing a complex composition of linear\nalgebraic operations beyond the current state of the art.\n3 O PTIMIZING TRANSFORMERS\nWe now apply our recipe to optimize data movement in\ntraining, using a BERT encoder layer as an example. We\nfocus on a single encoder layer, since these are simply re-\npeated throughout the network, and other components (e.g.,\nembedding layers) are not a signiﬁcant component of the\nruntime. In this section, we discuss dataﬂow and our opera-\ntor classiﬁcation. Sections 4 and 5 discuss our optimizations\nand Section 6 presents end-to-end results for transformers.\nAt a high level, our recipe consists of the following steps:\n1. Construct a dataﬂow graph for training and analyze the\ncomputation to identify common operator classes.\n2. Identify opportunities for data movement reduction\nwithin each operator class using data reuse as a guide.\n3. Systematically evaluate the performance of operators\nwith respect to data layout to ﬁnd near-optimal layouts.\n4. Find the best conﬁgurations to optimize end-to-end per-\nformance of the training process.\n3.1 Dataﬂow Analysis\nWe use SDFGs and the DaCe environment to construct and\nanalyze dataﬂow graphs. Fig. 2 provides a simpliﬁed repre-\nsentation of dataﬂow in a transformer encoder layer. Each\nnode represents an operator, which is a particular computa-\ntion along with its associated input and output, which may\nvary in size. An operator may be implemented as multi-\nple compute kernels, but is logically one operation for our\nTable 1.Proportions for operator classes in PyTorch.\nOperator class % ﬂop % Runtime\n△Tensor contraction 99.80 61.0\n⬜Stat. normalization 0.17 25.5\n○Element-wise 0.03 13.5\nanalysis. To produce an SDFG, all that is required is a\nsimple implementation using NumPy. As the goal of this\nstage is to understand the dataﬂow, we do not need to opti-\nmize this implementation: It is simply a speciﬁcation of the\ncomputations and data movement.\nUsing DaCe, we can estimate data access volume and the\nnumber of ﬂoating point operations (ﬂop) required for each\ncomputation. Fig. 2 is annotated with ﬂop and the ratio of\nﬂop to data volume, and we provide a precise comparison\nwith PyTorch in Tab. A.1. The key observation is that the\nratio of data movement to operations performed varies sig-\nniﬁcantly among operators. In many cases, the runtime of\nan operator is dominated by data movement, rather than\ncomputation, and this should be the target for optimization.\n3.2 Operators in Transformers\nWith this analysis, we can now identify high-level patterns\nthat allow us to classify operators. We base our classiﬁca-\ntion both on the data movement to operation ratio and the\nstructure of the computations. This classiﬁcation is useful\nas it allows us to consider optimizations at a more general\nlevel, as opposed to working on each operator (or kernel)\nindividually. For transformers, we ﬁnd three classes: tensor\ncontractions, statistical normalizations, and element-wise\noperations. The border of each operator in Fig. 2 indicates\nits class and Tab. 1 gives the proportion of ﬂop and runtime\nfor a BERT encoder layer for each class.\nTensor Contractions △These are matrix-matrix multipli-\ncations (MMMs), batched MMMs, and in principle could\ninclude arbitrary tensor contractions. We consider only\nMMMs and batched MMMs for simplicity, as these are ef-\nﬁciently supported by cuBLAS. In transformers, these are\nlinear layers and components of MHA. These operations are\nthe most compute-intensive part of training a transformer.\nFor good performance, data layout and algorithm selection\n(e.g., tiling strategy) are critical.\nStatistical Normalizations ⬜These are operators such as\nsoftmax and layer normalization. These are less compute-\nintensive than tensor contractions, and involve one or more\nreduction operation, the result of which is then applied via\na map. This compute pattern means that data layout and\nvectorization is important for operator performance.\nElement-wise Operators ○These are the remaining oper-\nators: biases, dropout, activations, and residual connections.\nData Movement Is All You Need\nDlinB2\nDlin2\nbji->i add [bji]\nDresid2 Dlin1_linW1\nDsb1ln1\nDS1\nbji,bji->i\nfusion pattern 1\nfusion pattern 4\nmul [bju]\nDdrop2\ndrop2mask\nDact\nbackward activation [bju]\nlin1\nfusion pattern 2\nDlin1\nresid1\nsub [bju] - [bj]\nbju->bj\nln1mean\nln1diff\nfusion pattern 3\nFigure 3.Examples of operator fusion.\nThese are the least compute-intensive operations.\n3.3 Memory Usage Efﬁciency (MUE)\nThe MUE metric (Fuhrer et al., 2018) provides a measure of\nthe memory efﬁciency of an operation, both with respect to\nits implementation and achieved memory performance. This\nprovides another method for understanding performance\nbeyond ﬂop/s that is particularly relevant for applications\nthat are bottlenecked by data movement. MUE evaluates the\nefﬁciency of a particular implementation by comparing the\namount of memory moved (D) to the theoretical I/O lower\nbound (Jia-Wei & Kung, 1981) (Q) and the ratio of achieved\n(B) to peak (ˆB) memory bandwidth: MUE =Q/D⋅B/ˆB⋅100.\nIf an implementation both performs the minimum number\nof operations and fully utilizes the memory bandwidth, it\nachieves MUE =100. This can be thought of as similar to\nthe percent of peak memory bandwidth.\n3.4 Evaluation Details\nWe use the Lassen supercomputer (Livermore Computing\nCenter, 2020), where each node has four 16 GB Nvidia\nV100 GPUs with NVLINK2. For comparison, we use\ncuDNN 8.0.4, PyTorch 1.6.0 (PT), TensorFlow 2.1 from\nIBM PowerAI with XLA, and a recent development version\nof DeepSpeed (DS). See Sec. A.3 for more details. Our run-\nning example is training BERT-large. We use a mini-batch\nsize of B = 8, sequence length L = 512, embedding size\nN=1024, H=16 heads, and a projection size P =64.\n4 F USION\nA signiﬁcant portion of the runtime in existing transformer\nimplementations is in statistical normalization and element-\nwise operators (Tab. 1). Thus, fusion is a major opportunity\nfor promoting data reuse, as when operators cover identical\niteration spaces, global memory writes and subsequent reads\nbetween them can be removed.\nWe develop a set of fusion rules applicable to any application\nwith operators similar to the three here. The process consists\nof two steps: detecting which operations can be fused and\nthen deciding whether it is beneﬁcial to fuse them.\nTo discover fusion opportunities, we analyze iteration\nspaces. Every operator has independent dimensions. Statisti-\ncal normalization operators also have reduction dimensions;\ntensor contractions also have reduction dimensions and spe-\ncial independent dimensions for the two input tensors.\nThe type of iteration space implementation determines\nwhich tools are used to make them. Independent dimensions\ncan be implemented using GPU block or thread parallelism,\nor \"for\" loops within a single thread. Reduction dimensions\nuse these techniques but also specify how the reduction is to\nbe performed: accumulating to the same memory location\n(\"for\" loops), or as grid-, block-, or warp-reductions.\nTwo operators can be fused if their iteration space imple-\nmentations are compatible: They are either the same or the\nonly difference is that one operator performs a reduction.\nThe order and size of dimensions and the implementation\nfor each must match. If the ﬁrst operator produces output\nthe second uses as input, partial fusion can be done: The\noutermost independent dimensions can be shared, but the\ninnermost iteration spaces are put in sequential order inside.\nWhen a fusion opportunity is detected, we take it in two\ncases: First, if we can perform fewer kernel launches by\nmerging iteration spaces. Second, if we achieve less data\nmovement by avoiding loads and stores between operators.\nTheoretically, the ﬁrst case could increase memory pressure,\nbut we observe it provides beneﬁts in practice.\nWe attempt to fuse maximally. There are four structural\npatterns (Fig. 3) in the dataﬂow graph for the encoder layer\nwhen fusion rules above are applied to a pair of non-tensor\ncontraction operators. Using the SDFG, we fuse two adja-\ncent operators whenever we detect these patterns and con-\ntinue until we cannot fuse further. This means we fuse until\neither a reduction dimension or iteration space changes. As\na further constraint, we only fuse simple element-wise scal-\ning operations into tensor contraction operators.\nEach fused operator is implemented as a CUDA kernel spe-\ncialized for a speciﬁc data layout. Due to space limitations,\nwe detail our implementation and fused kernels in Sec. A.2.\n4.1 Results\nTab. A.1 presents our comprehensive results, including op-\nerator fusion. In this, we show a detailed breakdown of the\nrequired and observed ﬂop, data movement, runtime, and\nMUE for each operator within the encoder layer, for both\nData Movement Is All You Need\nPyTorch and our implementation, with our fused operators\nmarked. We can easily observe that while the vast majority\nof ﬂop are in tensor contractions, much of the runtime is\nin statistical normalization and element-wise operators (see\nalso Tab. 1). These operators are indeed memory-bound.\nIn forward propagation, every fused operator outperforms\nPyTorch’s. In backpropagation, this trend generally holds,\nbut EBSB and BAOB are slower due to our conﬁguration\nselection algorithm choosing a suboptimal layout for some\noperators to optimize the overall performance (see Sec. 6).\nBy studying the MUE and ﬂop/s, we can reason about the\nbottlenecks behind each operator. For the fused operators,\nwe see that high MUE rates are often achieved. In fact,\nthe MUE from Tab. A.1 and the theoretical ﬂop/IO ratio\nfrom Fig. 2 are highly correlated across operators. We say\nthat a kernel is memory bound if its MUE is larger than the\nachieved peak ﬂop/s, and compute bound otherwise. This\ninsight aids in analyzing the bottlenecks of general DNNs\nand automated tuning of operators, prior to measuring their\nperformance. We note that for our operators, which involve\nmultiple tensors of different shapes, 100% MUE is poten-\ntially unattainable as achieving peak memory bandwidth\nrequires a highly regular access pattern into DRAM.\nAs for the tensor contraction results, we see that the attained\nMUE is consistently under 50%. This is acceptable, since\nthe underlying matrix multiplications are generally compute-\nbound. However, we note that some cases, such as QKT ,\nare both low in ﬂop/s and MUE. A more in-depth look into\nthe dimensions of the contraction reveals that the dimensions\nare small, which then indicates that the tensor cores are\nunderutilized. This may result from insufﬁcient scheduled\nthreads, or low memory throughput to compute ratio. We\nthus try to increase hardware utilization by fusing additional\noperators into the contractions next.\nAdditional Fusion Approaches We considered two addi-\ntional fusion approaches, fusing operators into tensor con-\ntractions and algebraic fusion, which we discuss in Sec. A.5\ndue to limited space. We ﬁnd that fusing into tensor contrac-\ntions is not proﬁtable, but algebraic fusion to combine input\nprojection operations in MHA is.\n5 D ATA LAYOUT\nWe now consider data layout selection, which enables efﬁ-\ncient access patterns, vectorization, and tiling strategies for\ntensor contraction and statistical normalization operators.\nTo study this systematically, for each operator, including\nfused operators produced in the prior step, we benchmark ev-\nery feasible data layout, as well as varying other parameters\ndepending on the speciﬁc operator. The best parameteriza-\ntion of an operator is highly dependent on the GPU model\nand tensor sizes, and may not be obvious a priori; hence it\nis important to consider this empirically.\nBecause there are a myriad of potential conﬁgurations for\neach operator, we summarize the distribution of runtimes\nover all conﬁgurations using violin plots. The width of the\nviolin represents the relative number of conﬁgurations with\nthe same runtime. This allows us to see not only the best\nruntime but sensitivity of operators to layouts, an important\nfactor for global layout optimization in Step 4 of our recipe.\n5.1 Tensor Contractions\nUsing the Einsum notation for tensor contractions, we con-\nsider all equivalent permutations of the summation string.\nThe einsum is then mapped to a single cuBLAS MMM or\nbatched MMM call. While this notation allows us to express\narbitrary tensor contractions, as cuBLAS does not support\nall conﬁgurations, we limit ourselves to these two types.\nIn addition, we consider every possible cuBLAS algo-\nrithm for each layout, as we have observed that the heuris-\ntic default selection is not always optimal. We use the\ncublasGemmEx API to manually select algorithms. We\nuse both regular and Tensor Core operations, and perform all\naccumulations at single-precision, in line with best practices\nfor mixed-precision training (Micikevicius et al., 2018).\nFig. 4 presents performance distributions over all data lay-\nouts for every tensor contraction in the encoder layer train-\ning, including algebraic fusion variants. Each plot is for\ndifferent tensor sizes and shows the performance with and\nwithout Tensor Cores. Since input matrices for cuBLAS\nMMMs can be easily swapped, results for both orders are\nmerged into the plot and labeled withM >N. Interestingly,\nin several cases (where N or K is 64) the performance\nis quite close to the regular ﬂoating point units, due to a\nfailure to saturate the tensor cores. Among the tensor core\nresults, we can typically see there are several modes in the\nperformance distributions; these correspond to particularly\nimportant axes for data layout. Indeed, for many conﬁgura-\ntions, one of these is near to or contains the best-performing\nconﬁguration, indicating that many slightly different data\nlayouts could be used with little impact on performance\ndepending on the needs of our global optimization pass.\nHowever, this does not mean that any data layout is ac-\nceptable; in every case, the majority of the layouts do not\nperform well, illustrating the importance of careful tuning.\nWe also investigated how the default cuBLAS algorithm\ncompares to the best-performing conﬁguration. On half\nprecision, we found that the algorithm chosen by cuBLAS’s\nheuristic was up to 14.24% worse than the best algorithm\n(in dX1 QKT ). We also investigated the performance at\nsingle-precision and found similar results with up to 7.18%\nworse performance. This demonstrates the importance of\ntuning for particular hardware and workload.\nData Movement Is All You Need\nbest: 2.32 ms\nbest: 0.38 ms\nworst: 3.30 ms\nworst: 0.65 ms\nbest: 0.31 ms\nbest: 0.14 ms\nworst: 0.82 ms\nworst: 0.18 ms\nbest: 1.19 ms\nbest: 0.21 ms\nworst: 1.66 ms\nworst: 0.33 ms\nbest: 0.59 ms\nbest: 0.12 ms\nworst: 0.87 ms\nworst: 0.23 ms\nbest: 0.32 ms\nbest: 0.12 ms\nworst: 0.79 ms\nworst: 0.40 ms\nbest: 1.75 ms\nbest: 0.29 ms\nworst: 2.48 ms\nworst: 0.56 ms\nbest: 1.14 ms\nbest: 0.20 ms\nworst: 1.77 ms\nworst: 0.34 ms\nbest: 2.32 ms\nbest: 0.37 ms\nworst: 3.29 ms\nworst: 2.46 ms\nbest: 1.19 ms\nbest: 0.20 ms\nworst: 1.67 ms\nworst: 0.33 ms\nbest: 1.70 ms\nbest: 0.29 ms\nworst: 2.63 ms\nworst: 0.50 ms\nbest: 0.60 ms\nbest: 0.11 ms\nworst: 0.85 ms\nworst: 0.18 ms\nbest: 1.74 ms\nbest: 0.28 ms\nworst: 2.48 ms\nworst: 0.48 ms\ndXQK\nM: 4096, N: 1024, K: 2048, B: 1\ndXQKV\nM: 4096, N: 1024, K: 3072, B: 1\nKV\nM: 4096, N: 2048, K: 1024, B: 1\nQKV\nM: 4096, N: 3072, K: 1024, B: 1\ndX1gamma, QKT\nM: 512, N: 512, K: 64, B: 128\ndX1QKT, dX2gamma, dX2QKT, gamma\nM: 512, N: 64, K: 512, B: 128\ndXlin2, lin1\nM: 4096, N: 4096, K: 1024, B: 1\ndXout, dXQ, out, Q\nM: 4096, N: 1024, K: 1024, B: 1\ndWlin1, dWlin2, dXlin1, lin2\nM: 4096, N: 1024, K: 4096, B: 1\ndWout, dWQ\nM: 1024, N: 1024, K: 4096, B: 1\ndWQK\nM: 2048, N: 1024, K: 4096, B: 1\ndWQKV\nM: 3072, N: 1024, K: 4096, B: 1\n0 25 50 75 100 0 25 50 75 100 0 25 50 75 100 0 25 50 75 100\n% of peak performance16-bit FPUs Tensor Cores\nFigure 4.Tensor contraction performance. Tensor Core peak: 125 Tﬂop/s; FP16 peak: 31.4 Tﬂop/s.\n5.2 Fused Operators\nFor our fused operators, we consider all combinations of\ninput and output layout permutations. This enables us to\ninclude transposes of the output data as part of the operator,\nshould the next operator perform better in a different layout\nthan the input. The data layout is especially critical for sta-\ntistical normalization operators, where the appropriate data\nlayout can enable vectorization opportunities, especially\nvectorized loads and stores for more efﬁcient memory ac-\ncess. We therefore also consider vectorization dimensions,\nthe mapping of dimensions to GPU warps, etc. Our imple-\nmentation takes advantage of these layouts when feasible.\nFig. 5 presents the runtime distribution for all conﬁgurations\nof our fused operators (note that some are used twice; see\nSec. A.2 for details of each operator). The distributions here\nare qualitatively similar to those in Fig. 4, except these have\nmuch longer tails: A bad conﬁguration is, relatively, much\nworse, potentially by orders of magnitude.\nAll kernels support changing the layouts of tensors they use.\nThis change is done via template parameterization, so the\ncompiler can generate efﬁcient code. All kernels support\nselecting different vectorization dimensions. The BRD and\nBEI kernels can select the dimension used for CUDA thread\ndistribution; BSB , EBSB , and BDRB can select the warp\nreduction dimension, as they reduce over two dimensions.\nThe most noticeable performance improvement is made by\nlayouts that enable vectorized memory access, showing that\nthe main performance limitation is the amount of moved\ndata. The second notable category are layouts with the same\nreduce and vector dimensions. Joining these dimensions\nbest: 0.065\nworst: 5.317\nbest: 0.101\nworst: 7.643\nbest: 0.396\nworst: 45.379\nbest: 0.033\nworst: 0.863\nbest: 0.034\nworst: 0.876\nbest: 0.037\nworst: 1.271\nbest: 0.014\nworst: 0.146\nbest: 0.071\nworst: 3.521\nbest: 0.078\nworst: 3.334\nbest: 0.167\nworst: 14.453\nbest: 0.176\nworst: 6.583\nbest: 0.402\nworst: 81.286\nBS BSB EBSB SM\nBDRLN BEI BLNRD BRD\nAIB BAIB BAOB BDRB\n0\n5\n10\n15\n0\n2\n4\n6\n0\n20\n40\n60\n80\n0.04\n0.08\n0.12\n0\n1\n2\n3\n0\n1\n2\n3\n0.00\n0.25\n0.50\n0.75\n0.00\n0.25\n0.50\n0.75\n0.0\n0.5\n1.0\n0\n2\n4\n0\n2\n4\n6\n8\n0\n10\n20\n30\n40 Time [ms] Figure 5.Performance of fused kernels for element-wise and sta-\ntistical normalization operators.\nData Movement Is All You Need\ndecreases the number of registers required to store partially\nreduced values from the vector size (eight at FP16) to one.\nWe can expect to get good performance restricting ourselves\nto conﬁgurations in the two groups described above. Usu-\nally, the best layout discovered follows them. For example,\nthe SM kernel has the same warp and reduction dimensions,\nand these dimensions are the last and sequential ones for in-\nvolved arrays. However, this intuition is not always correct.\nFrom the results in Fig. 5, we ﬁnd that there are conﬁgura-\ntions that both satisfy these intuitive rules yet are very slow.\nFor example, the best conﬁguration of AIB takes 65 µs, and\nthe worst \"intuitively good\" conﬁguration takes 771 µs.\nConﬁgurations discovered through exhaustive search can\nenhance our intuitive understanding of what is required for\ngood performance. For example, the BRD kernel uses four\n3D tensors, which can use six possible layouts. Intuitively,\nwe want to place the vectorized dimension last to make it\nsequential for all tensors. Surprisingly, however, the best\nconﬁguration has only two tensors vectorized. With this\ninformation, intuition can be reﬁned: the likely factor that\nlimits vectorization over all tensors is excessive register\nusage. However, unlike the exhaustive search, intuition does\nnot help to identify which tensors to vectorize.\n6 E ND-TO-END OPTIMIZATION\nThe ﬁnal step is to assemble fused components and select\ndata layouts for each operator to yield a complete implemen-\ntation. This is the culmination of the prior steps performing\ndataﬂow analysis, fusion, and layout selection. From these,\nwe have performance data for all data layouts and algebraic\nfusion strategies. One cannot simply pick a single layout a\npriori, as the beneﬁt of running two operators in different\nlayouts may outweigh the overhead of transposing data. Our\nassembled implementation is structured using the SDFGs\nproduced in Step 1. We integrate it into PyTorch (Paszke\net al., 2019) via its C++ operator API.\n6.1 Conﬁguration Selection\nWe develop an automatic conﬁguration selection algorithm\nto globally optimize our implementation using the perfor-\nmance data. We construct a directed graph (Fig. 6) based\non the dataﬂow graph of the operators. Beginning from the\ninput data and proceeding in a topological order, we add\na node to the graph for each input and output data layout\nof the operator. An edge is added from the input to the\noutput layout, weighted with the minimum runtime of any\nconﬁguration with that layout. Determining this simply re-\nquires a linear search over the matching performance data.\nThis allows us to select both the best data layout and any\nother operator knobs (e.g., vectorization dimension). To\nminimize the size of the graph, we only add a conﬁguration\n0\n0\n0\nsource\nt1\nt2\nijb\nt4\nibj\nt3 \nbji\n⋅⋅⋅ \nt5 \nt6 \nt7 \nphjb\nt13 \nt12 \nt1 1 \nhpjb\nt9 \nt8 \nt10 \nphbj\n⋅⋅⋅ \n0\nphjb\n0\nhpjb\n0\nphbj\n⋅⋅⋅ \ntarget\nQKV-fused AIB\nin in/out out\nFigure 6.Example conﬁguration selection graph for SSSP.\nfrom an operator when it has at least one input and output\nedge. We then run a single-source shortest-path (SSSP)\nalgorithm from the input to the output in the graph; the\nresulting path gives our ﬁnal conﬁguration. Because this\ngraph is a DAG and the number of feasible input/output\nconﬁgurations for each operator is small, SSSP takes lin-\near time asymptotically and seconds for BERT. The path is\nsaved to a conﬁguration ﬁle which is used to automatically\ndeﬁne tensor layouts at the start of training.\nTo simplify the implementation, we omit dataﬂow connec-\ntions between forward and backpropagation operators. This\nassumption could be relaxed in a future version of this algo-\nrithm. Although this means we are not guaranteed to ﬁnd a\nglobally optimal data layout, the runtime of our conﬁgura-\ntion is nevertheless within 6% of an ideal (incorrect) layout\nconﬁguration that ignores data layout constraints.\n6.2 Multi-head Attention\nWe ﬁrst analyze the performance of multi-head self-\nattention. While it is a key primitive in BERT, MHA is\nalso used outside of transformers, so understanding its per-\nformance in isolation can inform other models too. Tab. 2\ncompares our globally-optimized implementation with Py-\nTorch, TensorFlow+XLA, and cuDNN. cuDNN’s MHA im-\nplementation (cudnnMultiHeadAttnForward and re-\nlated) supports six data layouts; we report the fastest.\ncuDNN’s performance is signiﬁcantly worse than the others;\nas it is a black box, our ability to understand it is limited.\nHowever, proﬁling shows that its implementation launches\nvery large numbers of softmax kernels, which dominate\nthe runtime, indicating additional fusion would be prof-\nitable. TensorFlow+XLA ﬁnds several fusion opportunities\nfor softmax. However, its implementation does not perform\nalgebraic fusion for the queries, keys, and values, and it uses\nsubpar data layouts for tensor contractions.\nOur performance results in Tab. A.1 illustrate the source of\nour performance advantage over PyTorch: Our data layout\nand algorithm selection results in faster tensor contractions\noverall. This is despite the Gamma stage actually being\nslower than PyTorch’s: Sometimes locally suboptimal lay-\nData Movement Is All You Need\nTable 2.Multi-head attention performance for BERT.\nTF+XLA PT cuDNN Ours\nForward (ms) 1.60 1.90 3.86 1.25\nBackward (ms) 2.25 2.77 680 1.86\nouts need to be selected to improve performance globally.\n6.3 End-to-End Performance\nWe present overall performance results for the encoder layer\nin Tab. 3. For forward and backpropagation combined, we\nare 1.30×faster than PyTorch and 1.20×faster than Ten-\nsorFlow+XLA, including framework overheads. At a high\nlevel, this is because we perform a superset of the optimiza-\ntions used by both frameworks, and globally combine them\nto get all the advantages while minimizing drawbacks. As a\ngeneral guideline, we use ﬂop and MUE rates as proxies for\nwhich operators require the most attention and their corre-\nsponding bottlenecks. This ensures a guided optimization\nrather than tuning all operators aggressively.\nWe also include performance results from DeepSpeed,\nwhich we are 1.08×faster than. This is despite DeepSpeed\nbeing a manually-optimized library tuned speciﬁcally for\nBERT on V100s. Note also that DeepSpeed modiﬁes some\noperations, e.g., to be reversible or to exploit output spar-\nsity, and so is not always strictly comparable to the other\nimplementations. This also provides it opportunities for\noptimization that we do not consider.\nThe total data movement reduction we attain is ∼22.91%\nover the standard implementation. We obtain this informa-\ntion from Tab. A.1, where for each fused kernel we omit the\ninterim outputs and inputs that are not part of the overall I/O\nthat the fused kernels perform. TensorFlow+XLA’s auto-\nmatic kernel fusion reduces data movement similarly to ours.\nHowever, the data layouts used for tensor contractions are\nnot optimal, and its BERT encoder implementation does not\nuse algebraic fusion in MHA. PyTorch’s data layouts enable\nfaster tensor contractions and it implements the algebraic\nfusion, but it has higher overheads for other operators.\nOur fusion pass ﬁnds all the opportunities that TF+XLA\ndoes, plus several additional ones; for example, we imple-\nment layernorm as a single fused kernel. Our data layout\nselection picks better layouts than PyTorch in nearly every\nindividual instance; when it does not, this is because the lay-\nout change enables greater performance gains downstream.\nIn Tab. A.1, we also see that PyTorch performs more ﬂop\nthan predicted. Some of this is due to padding in cuBLAS\noperations, and generic methods performing excess oper-\nations. However, we also discovered that some cuBLAS\nGEMM algorithms, including ones called by PyTorch, in-\ncorrectly perform twice as many FLOPs as necessary; our\nTable 3.Full BERT encoder layer performance.\nTF+XLA PT DS Ours\nForward (ms) 3.2 3.45 2.8 2.63\nBackward (ms) 5.2 5.69 4.8 4.38\n0 100 200 300 400 500 600 700\nTime (min)\n0\n2\n4\n6Pretraining loss\n11.4 hours\nLoss: 1.386\n9.57 hours\nLoss: 1.398\nInitial loss: 11.06\nInitial loss: 11.26\nBaseline phase 1\nBaseline phase 2\nOurs phase 1\nOurs phase 2\nFigure 7.Pretraining curve for BERT-large.\nrecipe avoids these cases automatically.\nWe also considered another conﬁguration for training BERT,\nwhere we change the batch size to B =96 and the sequence\nlength to L = 128, and retuned our layout selection. In\nthis case, forward and backpropagation for a single encoder\nlayer takes 18.43 ms in PyTorch, 16.19 ms in DeepSpeed,\nand 15.42 ms in our implementation. We outperform both\nPyTorch and DeepSpeed in this case (even with its addi-\ntional optimizations). We believe that with improvements\nto our layout selection algorithm, the performance of our\nimplementation will improve further.\nFinally, we performed end-to-end training of BERT at scale.\nWe observe an overall speedup of 1.19×, despite additional\ntraining overheads. We show pretraining curves in Fig. 7\nand present full details of the experiment in Sec. A.6.\nBeyond BERT, other transformers have very similar layers,\nsuch as decoder layers in GPT-2/3. With very few changes,\nour recipe and implementations are directly applicable to\nthese. Our implementation can also be extended to support\na full training pipeline by stacking our optimized layers.\n7 R ELATED WORK\nWe provide a brief overview of related work here, and a\nmore comprehensive discussion in Sec. A.7.\nMost directly relevant are other approaches speciﬁcally to\naccelerate transformer training. Distributed-memory tech-\nniques, such as ZeRO (Rajbhandari et al., 2019), Mega-\ntron (Shoeybi et al., 2019), and Mesh-TensorFlow (Shazeer\net al., 2018) scale training to many GPUs to accelerate it.\nMesh-TensorFlow also presents a classiﬁcation of opera-\ntors similar to ours. Large batches have also been used\nto accelerate training via LAMB (You et al., 2020) or\nNVLAMB (NVIDIA AI, 2019). None of these directly\naddress the performance of a single GPU as done in this\npaper. DeepSpeed (Microsoft, 2020a), which we compare\nwith in Section 6.3, is closest to our work, but performs all\nData Movement Is All You Need\noptimizations and layout selections manually.\nMany frameworks provide optimizations that can also be\napplied to transformers, such as XLA or TVM (Chen et al.,\n2018). None of these works provide all the optimizations\nor the systematic study of data movement and its impact\non performance provided here. Beyond deep learning, data\nmovement reduction is a core component of high-level opti-\nmization (Unat et al., 2017) and polyhedral optimizations\n(e.g., Grosser et al. (2012)). Other white-box approaches\nfor optimization exist (e.g., Halide (Ragan-Kelley et al.,\n2013) and MLIR (Lattner et al., 2020)). The data-centric\noptimization approach proposed here using dataﬂow graphs\nallows us to perform and tune complex data layout and fu-\nsion transformations that span granularity levels, surpassing\nthe optimization capabilities of the aforementioned tools\nand achieving state-of-the-art performance.\n8 D ISCUSSION\nThe recipe we propose in this paper can be directly adopted\nin other DNN architectures. Additional transformer net-\nworks, such as Megatron-LM (Shoeybi et al., 2019) and\nGPT-3 (Brown et al., 2020), only differ by dimensions\nand minor aspects in the encoder and decoder blocks (e.g.,\ndropout position, biases). Once a data-centric graph is con-\nstructed from them, the recipe remains unchanged.\n8.1 Beyond Transformers\nThe classiﬁcation of operators into three groups covers a\nwide variety of operators that span beyond transformers.\nLarge tensors and their contraction are ubiquitous in modern\nDNNs. For MLPs and recurrent neural networks (RNNs),\nthere is little change, as the core operator types are essen-\ntially the same. Convolutions, pooling, and other local spa-\ntial operators can be treated similarly to tensor contractions,\nowing to their arithmetic intensity properties and abundance\nof optimized implementations. Therefore, the same consid-\nerations we take here are just as critical in CNNs. However,\nas opposed to contractions (see Section A.5.1), further fu-\nsion is typically considered for convolutions.\nStatistical normalization also takes different forms in DNNs.\nThis includes a variety of reductions, as well as Instance,\nGroup, and Batch Normalization, where the latter consti-\ntutes the second largest computation in ResNets after con-\nvolutions. When varying data layouts, these operators share\nproperties (normalizing a dimension) and are optimized\nin exactly the same way. Lastly, element-wise operators\nalways exist in DNNs and beneﬁt from the same fusion\nand bulk data movement optimizations as we perform here.\nFor graph neural networks (Bronstein et al., 2017), capsule\nneural networks (Sabour et al., 2017), and other emerging\narchitectures, the operators change more signiﬁcantly, but\nthe basic procedure applies.\nDue to the prohibitively large search space of conﬁgura-\ntions in transformers, writing manually-optimized kernels\nbecomes infeasible. Instead, each of our data-centric imple-\nmentations chooses an optimization scheme (e.g., tiling, vec-\ntorization, warp-aggregated reductions) automatically, ac-\ncording to the input data layout and the operator type. Com-\nbined with automated conﬁguration selection (Section 6.1),\nwe rely only on the dataﬂow graph structure to choose the\nbest feasible data layout conﬁguration.\n8.2 Hardware Implications\nThe implications of data movement reduction extend beyond\nsoftware. Given that the best performance for different oper-\nators is achieved with different data layouts, there would be\nsigniﬁcant beneﬁts if future machine learning accelerators\nincluded built-in support for fast data layout changes. We\nconﬁrm this in our MUE results (Tab. A.1), as even the most\ncompute-intensive tensor contractions are bounded by the\nhardware’s capability of transferring data to Tensor Cores.\nHardware trends indicate a similar situation. New architec-\ntures are moving towards reducing data format conversion\n(e.g., TF32 (Nvidia, 2020)), increased on-chip memory and\nlow-latency interconnects (Jia et al., 2019c), and coarse-\ngrained spatial hardware (Cerebras, 2020). For the latter\ntwo, the recipe and analysis provided here is crucial to main-\ntain high utilization in pipelined DNN execution. More\ngenerally, we expect our recipe to be applicable to any load-\nstore architecture (e.g., TPUs (Jouppi et al., 2017)). The %\nof peak and MUE are fundamental quantities and analyz-\ning them will allow one to study bottlenecks and optimize\nappropriately.\n9 C ONCLUSIONS\nDespite the importance of transformer neural networks,\ntraining them is memory-bound and underutilizes GPUs.\nUsing our recipe for data movement analysis, we identiﬁed\nbottlenecks and optimizations, yielding improved implemen-\ntations that outperform the already highly tuned state-of-the-\nart. As training transformers is already a major workload\nthat will only grow larger, our improvements offer signiﬁ-\ncant real-world impacts for both research and industry.\nOur approach is applicable more broadly to deep learning;\nmany DNNs easily ﬁt within our operator classiﬁcation.\nThis is especially important for guiding the optimization of\nemerging model architectures, which do not beneﬁt from\nexisting acceleration libraries. Our results also highlight the\nimportance of considering data movement at every level of\nthe training stack—from the application down to hardware.\nData Movement Is All You Need\nACKNOWLEDGMENTS\nThis project received funding from the European Research\nCouncil (ERC) under the European Union’s Horizon 2020\nprogramme (grant agreements DAPP, No. 678880, and\nEPiGRAM-HS, No. 801039). N.D. is supported by the ETH\nPostdoctoral Fellowship. T.B.N. is supported by the Swiss\nNational Science Foundation (Ambizione Project #185778).\nExperiments were performed at the Livermore Computing\nfacility.\nREFERENCES\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,\nM., Jia, Y ., Jozefowicz, R., Kaiser, L., Kudlur, M., Lev-\nenberg, J., Mané, D., Monga, R., Moore, S., Murray, D.,\nOlah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,\nI., Talwar, K., Tucker, P., Vanhoucke, V ., Vasudevan,\nV ., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M.,\nWicke, M., Yu, Y ., and Zheng, X. TensorFlow: Large-\nscale machine learning on heterogeneous systems, 2015.\nURL https://www.tensorflow.org/.\nAkiba, T., Suzuki, S., and Fukuda, K. Extremely large\nminibatch SGD: Training ResNet-50 on ImageNet in 15\nminutes. In NeurIPS 2017 Workshop: Deep Learning at\nSupercomputer Scale, 2017.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016.\nBaghdadi, R., Debbagh, A. N., Abdous, K., Zohra, B. F.,\nRenda, A., Frankle, J. E., Carbin, M., and Amarasinghe,\nS. TIRAMISU: A polyhedral compiler for dense and\nsparse deep learning. In Workshop on Systems for ML at\nNeurIPS, 2019.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate. In\nProceedings of the Third International Conference on\nLearning Representations (ICLR), 2014.\nBauer, M., Treichler, S., Slaughter, E., and Aiken, A. Le-\ngion: Expressing locality and independence with logical\nregions. In Proceedings of the International Conference\non High Performance Computing, Networking, Storage\nand Analysis (SC), 2012.\nBello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V .\nAttention augmented convolutional networks. InProceed-\nings of the IEEE International Conference on Computer\nVision (CVPR), 2019.\nBen-Nun, T. and Hoeﬂer, T. Demystifying parallel and dis-\ntributed deep learning: An in-depth concurrency analysis.\nACM Comput. Surv., 52(4), 2019.\nBen-Nun, T., de Fine Licht, J., Ziogas, A. N., Schneider, T.,\nand Hoeﬂer, T. Stateful dataﬂow multigraphs: A data-\ncentric model for performance portability on heteroge-\nneous architectures. In Proceedings of the International\nConference for High Performance Computing, Network-\ning, Storage and Analysis (SC), 2019.\nBondhugula, U., Baskaran, M., Krishnamoorthy, S., Ra-\nmanujam, J., Rountev, A., and Sadayappan, P. Auto-\nmatic transformations for communication-minimized par-\nallelization and locality optimization in the polyhedral\nmodel. In International Conference on Compiler Con-\nstruction (ETAPS CC), 2008.\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,\nC., Maclaurin, D., and Wanderman-Milne, S. JAX: com-\nposable transformations of Python+NumPy programs,\n2018. URL http://github.com/google/jax.\nBronstein, M. M., Bruna, J., LeCun, Y ., Szlam, A., and Van-\ndergheynst, P. Geometric deep learning: going beyond\nEuclidean data. IEEE Signal Processing Magazine, 34\n(4), 2017.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\nBuchlovsky, P., Budden, D., Grewe, D., Jones, C., Aslanides,\nJ., Besse, F., Brock, A., Clark, A., Colmenarejo, S. G.,\nPope, A., et al. TF-Replicator: Distributed machine learn-\ning for researchers. arXiv preprint arXiv:1902.00465 ,\n2019.\nCerebras. Cerebras CS-1 Product Overview, 2020. URL\nhttps://www.cerebras.net/product.\nChellapilla, K., Puri, S., and Simard, P. High performance\nconvolutional neural networks for document processing.\nIn Tenth International Workshop on Frontiers in Hand-\nwriting Recognition, 2006.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training\ndeep nets with sublinear memory cost. arXiv preprint\narXiv:1604.06174, 2016.\nChen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H.,\nCowan, M., Wang, L., Hu, Y ., Ceze, L., Guestrin, C., and\nKrishnamurthy, A. TVM: An end-to-end optimization\nstack for deep learning. In 13th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI),\n2018.\nChen, X., Eversole, A., Li, G., Yu, D., and Seide, F.\nPipelined back-propagation for context-dependent deep\nneural networks. In Thirteenth Annual Conference of the\nInternational Speech Communication Association (IN-\nTERSPEECH), 2012.\nData Movement Is All You Need\nChetlur, S., Woolley, C., Vandermersch, P., Cohen, J.,\nTran, J., Catanzaro, B., and Shelhamer, E. cuDNN:\nEfﬁcient primitives for deep learning. arXiv preprint\narXiv:1410.0759, 2014.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gen-\nerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019.\nChilimbi, T., Suzue, Y ., Apacible, J., and Kalyanaraman, K.\nProject Adam: Building an efﬁcient and scalable deep\nlearning training system. In 11th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI),\n2014.\nCho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D.,\nBougares, F., Schwenk, H., and Bengio, Y . Learning\nphrase representations using RNN encoder-decoder for\nstatistical machine translation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Language\nProcessing (EMNLP), 2014.\nCoates, A., Huval, B., Wang, T., Wu, D., Catanzaro, B., and\nAndrew, N. Deep learning with COTS HPC systems. In\nInternational Conference on Machine Learning (ICML),\n2013.\nCordonnier, J.-B., Loukas, A., and Jaggi, M. On the rela-\ntionship between self-attention and convolutional layers.\nIn Proceedings of the Eighth International Conference\non Learning Representations (ICLR), 2020.\nCorreia, G. M., Niculae, V ., and Martins, A. F. Adap-\ntively sparse transformers. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), 2019.\nCyphers, S., Bansal, A. K., Bhiwandiwalla, A., Bobba, J.,\nBrookhart, M., Chakraborty, A., Constable, W., Convey,\nC., Cook, L., Kanawi, O., et al. Intel nGraph: An inter-\nmediate representation, compiler, and executor for deep\nlearning. arXiv preprint arXiv:1801.08058, 2018.\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and\nSalakhutdinov, R. Transformer-XL: Attentive language\nmodels beyond a ﬁxed-length context. In Proceedings of\nthe 57th Annual Meeting of the Association for Computa-\ntional Linguistics (ACL), 2019.\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao,\nM., Ranzato, M., Senior, A., Tucker, P., Yang, K., et al.\nLarge scale distributed deep networks. In Advances in\nneural information processing systems (NeurIPS), 2012.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics (NAACL), 2019.\nDong, X., Liu, L., Zhao, P., Li, G., Li, J., Wang, X., and\nFeng, X. Acorns: A framework for accelerating deep\nneural networks with input sparsity. In 2019 28th Inter-\nnational Conference on Parallel Architectures and Com-\npilation Techniques (PACT), 2019.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale.arXiv\npreprint arXiv:2010.11929, 2020.\nDryden, N., Maruyama, N., Benson, T., Moon, T., Snir, M.,\nand Van Essen, B. Improving strong-scaling of CNN\ntraining by exploiting ﬁner-grained parallelism. In 2019\nIEEE International Parallel and Distributed Processing\nSymposium (IPDPS), 2019a.\nDryden, N., Maruyama, N., Moon, T., Benson, T., Snir,\nM., and Van Essen, B. Channel and ﬁlter parallelism for\nlarge-scale CNN training. In Proceedings of the Inter-\nnational Conference for High Performance Computing,\nNetworking, Storage and Analysis (SC), 2019b.\nElango, V ., Rubin, N., Ravishankar, M., Sandanagobalane,\nH., and Grover, V . Diesel: DSL for linear algebra and\nneural net computations on GPUs. In Proceedings of the\n2nd ACM SIGPLAN International Workshop on Machine\nLearning and Programming Languages, 2018.\nFacebook. Caffe2, 2020. URL https://caffe2.ai/.\nFrostig, R., Johnson, M. J., and Leary, C. Compiling ma-\nchine learning programs via high-level tracing. Systems\nfor Machine Learning, 2018.\nFuhrer, O., Chadha, T., Hoeﬂer, T., Kwasniewski, G., Lapil-\nlonne, X., Leutwyler, D., Lüthi, D., Osuna, C., Schär, C.,\nSchulthess, T. C., et al. Near-global climate simulation\nat 1 km resolution: establishing a performance baseline\non 4888 GPUs with COSMO 5.0. Geoscientiﬁc Model\nDevelopment, 11(4), 2018.\nGholami, A., Azad, A., Jin, P., Keutzer, K., and Buluc, A. In-\ntegrated model, batch, and domain parallelism in training\nneural networks. In Proceedings of the 30th on Sym-\nposium on Parallelism in Algorithms and Architectures\n(SPAA), 2018.\nGomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. The\nreversible residual network: Backpropagation without\nstoring activations. In Advances in neural information\nprocessing systems (NeurIPS), 2017.\nData Movement Is All You Need\nGoodfellow, I., Bengio, Y ., and Courville, A. Deep\nLearning. MIT Press, 2016. http://www.\ndeeplearningbook.org.\nGoogle. XLA: Optimizing compiler for machine learn-\ning, 2020. URL https://www.tensorflow.org/\nxla.\nGoyal, P., Dollár, P., Girshick, R., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He,\nK. Accurate, large minibatch SGD: training ImageNet in\n1 hour. arXiv preprint arXiv:1706.02677, 2017.\nGraves, A. Sequence transduction with recurrent neural\nnetworks. International Conference on Machine Learning\n(ICML) Workshop on Representation Learning, 2012.\nGrosser, T., Größlinger, A., and Lengauer, C. Polly - per-\nforming polyhedral optimizations on a low-level interme-\ndiate representation. Parallel Processing Letters, 22(4),\n2012.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nInternational Conference on Computer Vision (CVPR),\n2016.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural Computation, 9(8), 1997.\nHuang, Y ., Cheng, Y ., Bapna, A., Firat, O., Chen, D., Chen,\nM., Lee, H., Ngiam, J., Le, Q. V ., Wu, Y ., et al. GPipe:\nEfﬁcient training of giant neural networks using pipeline\nparallelism. In Advances in Neural Information Process-\ning Systems (NeurIPS), 2019.\nJain, P., Jain, A., Nrusimha, A., Gholami, A., Abbeel, P.,\nKeutzer, K., Stoica, I., and Gonzalez, J. E. Checkmate:\nBreaking the memory wall with optimal tensor remate-\nrialization. In Proceedings of the Third Conference on\nMachine Learning and Systems (MLSys), 2020.\nJia, Z., Padon, O., Thomas, J., Warszawski, T., Zaharia, M.,\nand Aiken, A. TASO: optimizing deep learning computa-\ntion with automatic generation of graph substitutions. In\nProceedings of the 27th ACM Symposium on Operating\nSystems Principles (SOSP), 2019a.\nJia, Z., Thomas, J., Warszawski, T., Gao, M., Zaharia, M.,\nand Aiken, A. Optimizing DNN computation with relaxed\ngraph substitutions. In Proceedings of the 2nd Conference\non Systems and Machine Learning (SysML), 2019b.\nJia, Z., Tillman, B., Maggioni, M., and Scarpazza, D. P. Dis-\nsecting the Graphcore IPU architecture via microbench-\nmarking, 2019c.\nJia, Z., Zaharia, M., and Aiken, A. Beyond data and model\nparallelism for deep neural networks. In Proceedings of\nthe 2nd Conference on Systems and Machine Learning\n(SysML), 2019d.\nJia-Wei, H. and Kung, H.-T. I/O complexity: The red-blue\npebble game. In Proceedings of the thirteenth annual\nACM symposium on Theory of computing, 1981.\nJouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,\nG., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,\nA., et al. In-datacenter performance analysis of a tensor\nprocessing unit. In Proceedings of the 44th Annual Inter-\nnational Symposium on Computer Architecture (ISCA),\n2017.\nKalchbrenner, N. and Blunsom, P. Recurrent continuous\ntranslation models. In Proceedings of the 2013 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), 2013.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKeskar, N. S., McCann, B., Varshney, L. R., Xiong, C.,\nand Socher, R. CTRL: A conditional transformer lan-\nguage model for controllable generation. arXiv preprint\narXiv:1909.05858, 2019.\nKitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The\nefﬁcient transformer. In Proceedings of the Eighth Inter-\nnational Conference on Learning Representations (ICLR),\n2020.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet\nclassiﬁcation with deep convolutional neural networks.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2012.\nLample, G. and Charton, F. Deep learning for symbolic\nmathematics. arXiv preprint arXiv:1912.01412, 2019.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.,\nand Soricut, R. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In Proceedings of\nthe Seventh International Conference on Learning Repre-\nsentations (ICLR), 2019.\nLattner, C., Pienaar, J., Amini, M., Bondhugula, U., Riddle,\nR., Cohen, A., Shpeisman, T., Davis, A., Vasilache, N.,\nand Zinenko, O. MLIR: A compiler infrastructure for the\nend of moore’s law. arXiv preprint arXiv:2002.11054,\n2020.\nLavin, A. and Gray, S. Fast algorithms for convolutional\nneural networks. In Proceedings of the IEEE Conference\nData Movement Is All You Need\non Computer Vision and Pattern Recognition (CVPR) ,\n2016.\nLethin, R. Polyhedral optimization of tensorﬂow compu-\ntation graphs. In Sixth Workshop on Extreme-scale Pro-\ngramming Tools (ESPT), 2017.\nLi, C., Yang, Y ., Feng, M., Chakradhar, S., and Zhou, H.\nOptimizing memory efﬁciency for deep convolutional\nneural networks on GPUs. In Proceedings of the Inter-\nnational Conference for High Performance Computing,\nNetworking, Storage and Analysis (SC), 2016.\nLi, Y ., Yu, M., Li, S., Avestimehr, S., Kim, N. S., and\nSchwing, A. Pipe-SGD: A decentralized pipelined SGD\nframework for distributed deep net training. In Advances\nin Neural Information Processing Systems (NeurIPS) ,\n2018.\nLi, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein,\nD., and Gonzalez, J. E. Train large, then compress: Re-\nthinking model size for efﬁcient training and inference of\ntransformers. arXiv preprint arXiv:2002.11794, 2020.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoBERTa: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint arXiv:1907.11692, 2019.\nLivermore Computing Center. Lassen, 2020.\nURL https://hpc.llnl.gov/hardware/\nplatforms/lassen.\nLuong, M.-T., Pham, H., and Manning, C. D. Effective ap-\nproaches to attention-based neural machine translation. In\nProceedings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), 2015.\nMathieu, M., Henaff, M., and LeCun, Y . Fast training of\nconvolutional networks through FFTs. arXiv preprint\narXiv:1312.5851, 2013.\nMaziarka, Ł., Danel, T., Mucha, S., Rataj, K., Tabor, J., and\nJastrz˛ ebski, S. Molecule attention transformer. arXiv\npreprint arXiv:2002.08264, 2020.\nMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev,\nO., Venkatesh, G., et al. Mixed precision training. In\nProceedings of the Sixth International Conference on\nLearning Representations (ICLR), 2018.\nMicrosoft. DeepSpeed, 2020a. URL deepspeed.ai.\nMicrosoft. Turing-NLG: A 17-billion-parameter language\nmodel by Microsoft, 2020b. URL https://www.\nmicrosoft.com/en-us/research/blog/\nturing-nlg-a-17-billion-parameter-language-model-by-microsoft/ .\nMicrosoft. ZeRO & DeepSpeed: New system op-\ntimizations enable training models with over 100\nbillion parameters, 2020c. URL https://www.\nmicrosoft.com/en-us/research/blog/\nzero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/ .\nMicrosoft. ONNX Runtime, 2020. URL https://\nmicrosoft.github.io/onnxruntime/.\nMikami, H., Suganuma, H., U-chupala, P., Tanaka,\nY ., and Kageyama, Y . Massively distributed SGD:\nImageNet/ResNet-50 training in a ﬂash. arXiv preprint\narXiv:1811.05233, 2018.\nMikolov, T., Chen, K., Corrado, G., and Dean, J. Efﬁcient\nestimation of word representations in vector space. arXiv\npreprint arXiv:1301.3781, 2013a.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. Distributed representations of words and phrases\nand their compositionality. In Advances in neural infor-\nmation processing systems (NeurIPS), 2013b.\nMikolov, T., Yih, W.-t., and Zweig, G. Linguistic regularities\nin continuous space word representations. In Proceedings\nof the 2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics (NAACL),\n2013c.\nMirhoseini, A., Pham, H., Le, Q. V ., Steiner, B., Larsen,\nR., Zhou, Y ., Kumar, N., Norouzi, M., Bengio, S., and\nDean, J. Device placement optimization with reinforce-\nment learning. In Proceedings of the 34th International\nConference on Machine Learning, 2017.\nNarayanan, D., Harlap, A., Phanishayee, A., Seshadri, V .,\nDevanur, N. R., Ganger, G. R., Gibbons, P. B., and Za-\nharia, M. PipeDream: generalized pipeline parallelism for\ndnn training. In Proceedings of the 27th ACM Symposium\non Operating Systems Principles (SOSP), 2019.\nNvidia. Apex (A PyTorch Extension), 2020a. URL https:\n//nvidia.github.io/apex/.\nNvidia. CUTLASS: CUDA templates for linear algebra\nsubroutines, 2020b. URL https://github.com/\nNVIDIA/cutlass.\nNVIDIA. NVIDIA deep learning examples,\n2020a. URL https://github.com/NVIDIA/\nDeepLearningExamples.\nNVIDIA. NVIDIA TensorRT, 2020b. URL https://\ndeveloper.nvidia.com/tensorrt.\nNvidia. TensorFloat-32 in the A100 GPU Accelerates\nAI Training, HPC up to 20x, 2020. URL https:\n//blogs.nvidia.com/blog/2020/05/14/\ntensorfloat-32-precision-format .\nData Movement Is All You Need\nNVIDIA AI. A guide to optimizer imple-\nmentation for BERT at scale, 2019. URL\nhttps://medium.com/nvidia-ai/\na-guide-to-optimizer-implementation-for-bert-at-scale-8338cc7f45fd .\nOpenAI. AI and compute, 2018. URLhttps://openai.\ncom/blog/ai-and-compute/.\nOpenAI. Microsoft invests in and partners with OpenAI to\nsupport us building beneﬁcial AGI, 2019. URL https:\n//openai.com/blog/microsoft/.\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,\nN., Grangier, D., and Auli, M. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of NAACL-\nHLT 2019: Demonstrations, 2019.\nOyama, Y ., Ben-Nun, T., Hoeﬂer, T., and Matsuoka, S. Ac-\ncelerating deep learning frameworks with micro-batches.\nIn 2018 IEEE International Conference on Cluster Com-\nputing (CLUSTER), 2018.\nParisotto, E., Song, H. F., Rae, J. W., Pascanu, R., Gulcehre,\nC., Jayakumar, S. M., Jaderberg, M., Kaufman, R. L.,\nClark, A., Noury, S., et al. Stabilizing transformers for\nreinforcement learning. arXiv preprint arXiv:1910.06764,\n2019.\nParmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer,\nN., Ku, A., and Tran, D. Image transformer. In Proceed-\nings of the 35th International Conference on Machine\nLearning (ICML), 2018.\nParmar, N., Ramachandran, P., Vaswani, A., Bello, I., Lev-\nskaya, A., and Shlens, J. Stand-alone self-attention in\nvision models. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), 2019.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. PyTorch: An imperative style, high-performance\ndeep learning library. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2019.\nPaul G. Allen School of Computer Science & Engineer-\ning, University of Washington, Amazon Web Service\nAI team, and DMLC open-source community. NNVM\ncompiler: Open compiler for AI frameworks, 2017.\nURL https://tvm.apache.org/2017/10/06/\nnnvm-compiler-announcement.\nPyTorch Team. TorchScript, 2020. URL https://\npytorch.org/docs/stable/jit.html.\nRadford, A., Narasimhan, K., Salimans, T., and\nSutskever, I. Improving language understand-\ning by generative pre-training, 2018. URL\nhttps://s3-us-west-2.amazonaws.\ncom/openai-assets/research-covers/\nlanguage-unsupervised/language_\nunderstanding_paper.pdf.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners, 2019. URL https://openai.com/blog/\nbetter-language-models/.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nRagan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand,\nF., and Amarasinghe, S. Halide: A language and compiler\nfor optimizing parallelism, locality, and recomputation in\nimage processing pipelines. ACM SIGPLAN Notices, 48\n(6), 2013.\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y . ZeRO:\nMemory optimization towards training a trillion parame-\nter models. arXiv preprint arXiv:1910.02054, 2019.\nRajpurkar, P., Jia, R., and Liang, P. Know what you don’t\nknow: Unanswerable questions for SQuAD. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (ACL), 2018.\nRosenfeld, J. S., Rosenfeld, A., Belinkov, Y ., and Shavit,\nN. A constructive prediction of the generalization error\nacross scales. In Proceedings of the Eighth International\nConference on Learning Representations (ICLR), 2020.\nRotem, N., Fix, J., Abdulrasool, S., Catron, G., Deng,\nS., Dzhabarov, R., Gibson, N., Hegeman, J., Lele, M.,\nLevenstein, R., et al. Glow: Graph lowering com-\npiler techniques for neural networks. arXiv preprint\narXiv:1805.00907, 2018.\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. Learn-\ning representations by back-propagating errors. Nature,\n323(6088), 1986.\nSabour, S., Frosst, N., and Hinton, G. E. Dynamic routing\nbetween capsules. In Advances in neural information\nprocessing systems (NeurIPS), 2017.\nSanh, V ., Debut, L., Chaumond, J., and Wolf, T. DistilBERT,\na distilled version of BERT: smaller, faster, cheaper and\nlighter. In Fifth Workshop on Energy Efﬁcient Machine\nLearning and Cognitive Computing at NeurIPS 2019 ,\n2019.\nShazeer, N. Fast transformer decoding: One write-head is\nall you need. arXiv preprint arXiv:1911.02150, 2019.\nData Movement Is All You Need\nShazeer, N., Cheng, Y ., Parmar, N., Tran, D., Vaswani, A.,\nKoanantakool, P., Hawkins, P., Lee, H., Hong, M., Young,\nC., et al. Mesh-TensorFlow: Deep learning for supercom-\nputers. In Advances in Neural Information Processing\nSystems (NeurIPS), 2018.\nShen, T., Zhou, T., Long, G., Jiang, J., and Zhang, C.\nBi-directional block self-attention for fast and memory-\nefﬁcient sequence modeling. In Proceedings of the Sixth\nInternational Conference on Learning Representations\n(ICLR), 2018.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J.,\nand Catanzaro, B. Megatron-LM: Training multi-billion\nparameter language models using GPU model parallelism.\narXiv preprint arXiv:1909.08053, 2019.\nSivathanu, M., Chugh, T., Singapuram, S. S., and Zhou, L.\nAstra: Exploiting predictability to optimize deep learn-\ning. In Proceedings of the Twenty-Fourth International\nConference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS), 2019.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\nand Salakhutdinov, R. Dropout: a simple way to prevent\nneural networks from overﬁtting. Journal of machine\nlearning research (JMLR), 15(1), 2014.\nSteuwer, M., Remmelg, T., and Dubach, C. Lift: A func-\ntional data-parallel ir for high-performance gpu code gen-\neration. In Proceedings of the 2017 International Sym-\nposium on Code Generation and Optimization (CGO) ,\n2017.\nStrubell, E., Ganesh, A., and McCallum, A. Energy and\npolicy considerations for deep learning in NLP. In Pro-\nceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics (ACL), 2019.\nSukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A.\nAdaptive attention span in transformers. In Proceedings\nof the 57th Annual Meeting of the Association for Com-\nputational Linguistics (ACL), 2019.\nSutskever, I., Vinyals, O., and Le, Q. V . Sequence to se-\nquence learning with neural networks. In Advances in\nneural information processing systems (NeurIPS), 2014.\nTay, Y ., Wang, S., Tuan, L. A., Fu, J., Phan, M. C., Yuan,\nX., Rao, J., Hui, S. C., and Zhang, A. Simple and effec-\ntive curriculum pointer-generator networks for reading\ncomprehension over long narratives. In Proceedings of\nthe 57th Annual Meeting of the Association for Computa-\ntional Linguistics (ACL), 2019.\nTay, Y ., Bahri, D., Yang, L., Metzler, D., and Juan,\nD.-C. Sparse sinkhorn attention. arXiv preprint\narXiv:2002.11296, 2020.\nTruong, L., Barik, R., Totoni, E., Liu, H., Markley, C., Fox,\nA., and Shpeisman, T. Latte: A language, compiler, and\nruntime for elegant and efﬁcient deep neural networks.\nIn Proceedings of the 37th ACM SIGPLAN Conference\non Programming Language Design and Implementation\n(PLDI), 2016.\nUnat, D., Dubey, A., Hoeﬂer, T., Shalf, J., Abraham, M.,\nBianco, M., Chamberlain, B. L., Cledat, R., Edwards,\nH. C., Finkel, H., Fuerlinger, K., Hannig, F., Jeannot, E.,\nKamil, A., Keasler, J., Kelly, P. H. J., Leung, V ., Ltaief,\nH., Maruyama, N., Newburn, C. J., , and Pericas, M.\nTrends in Data Locality Abstractions for HPC Systems.\nIEEE Transactions on Parallel and Distributed Systems\n(TPDS), 28(10), 2017.\nVan Essen, B., Kim, H., Pearce, R., Boakye, K., and Chen,\nB. LBANN: Livermore big artiﬁcial neural network HPC\ntoolkit. In Proceedings of the Workshop on Machine\nLearning in High-Performance Computing Environments\n(MLHPC), 2015.\nVasilache, N., Zinenko, O., Theodoridis, T., Goyal, P., De-\nVito, Z., Moses, W. S., Verdoolaege, S., Adams, A., and\nCohen, A. Tensor comprehensions: Framework-agnostic\nhigh-performance machine learning abstractions. arXiv\npreprint arXiv:1802.04730, 2018.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2017.\nVenkat, A., Rusira, T., Barik, R., Hall, M., and Truong,\nL. SWIRL: High-performance many-core CPU code\ngeneration for deep neural networks. The International\nJournal of High Performance Computing Applications,\n33(6), 2019.\nV oita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I.\nAnalyzing multi-head self-attention: Specialized heads\ndo the heavy lifting, the rest can be pruned. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics (ACL), 2019.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. GLUE: A multi-task benchmark and anal-\nysis platform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP ,\n2018.\nWang, A., Pruksachatkun, Y ., Nangia, N., Singh, A.,\nMichael, J., Hill, F., Levy, O., and Bowman, S. Su-\nperGLUE: A stickier benchmark for general-purpose lan-\nguage understanding systems. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2019.\nData Movement Is All You Need\nWei, R., Schwartz, L., and Adve, V . DLVM: A modern com-\npiler infrastructure for deep learning systems. In Proceed-\nings of the Sixth International Conference on Learning\nRepresentations - Workshop Track (ICLR), 2018.\nWiggers, K. OpenAI’s massive GPT-3 model is im-\npressive, but size isn’t everything, 2020. URL\nhttps://venturebeat.com/2020/06/01/\nai-machine-learning-openai-gpt-3-size-isnt-everything/ .\nWolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue,\nC., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz,\nM., and Brew, J. HuggingFace’s transformers: State-\nof-the-art natural language processing. arXiv preprint\narXiv:1910.03771, 2019.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov,\nR. R., and Le, Q. V . XLNet: Generalized autoregressive\npretraining for language understanding. In Advances in\nNeural Information Processing Systems (NeurIPS), 2019.\nYeh, C.-F., Mahadeokar, J., Kalgaonkar, K., Wang, Y ., Le,\nD., Jain, M., Schubert, K., Fuegen, C., and Seltzer, M. L.\nTransformer-transducer: End-to-end speech recognition\nwith self-attention. arXiv preprint arXiv:1910.12977 ,\n2019.\nYing, C., Kumar, S., Chen, D., Wang, T., and Cheng, Y .\nImage classiﬁcation at supercomputer scale. In NeurIPS\nSystems for ML Workshop, 2018.\nYou, Y ., Zhang, Z., Hsieh, C.-J., Demmel, J., and Keutzer, K.\nImageNet training in minutes. In Proceedings of the 47th\nInternational Conference on Parallel Processing (ICPP),\n2018.\nYou, Y ., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojana-\npalli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh,\nC.-J. Large batch optimization for deep learning: Train-\ning BERT in 76 minutes. In Proceedings of the Eighth\nInternational Conference on Learning Representations\n(ICLR), 2020.\nZiogas, A. N., Ben-Nun, T., Fernández, G. I., Schneider, T.,\nLuisier, M., and Hoeﬂer, T. A data-centric approach to\nextreme-scale ab initio dissipative quantum transport sim-\nulations. In Proceedings of the International Conference\nfor High Performance Computing, Networking, Storage\nand Analysis (SC), 2019.\nData Movement Is All You Need\nA S UPPLEMENTARY MATERIAL\nA.1 Additional Background\nWe use the standard data-parallel approach to training,\nwherein a mini-batch of samples is partitioned among many\nGPUs. During backpropagation, we distinguish between\ntwo stages: computing the gradients with respect to a layer’s\ninput (dX), and computing the gradients with respect to the\nlayer’s parameters (dW). Note that the second stage is\nrelevant only for layers with learnable parameters.\nA.1.1 Transformers\nThe transformer architecture (Vaswani et al., 2017), orig-\ninally developed for machine translation, is a neural net-\nwork architecture for sequence transduction, or transform-\ning an input sequence into an output sequence. Transformers\nbuild upon a long sequence of work within natural language\nprocessing, most relevantly beginning with word embed-\ndings (Mikolov et al., 2013c;a;b), neural machine transla-\ntion (Kalchbrenner & Blunsom, 2013; Cho et al., 2014), and\nsequence-to-sequence learning (Sutskever et al., 2014). A\nkey component is attention (Bahdanau et al., 2014; Luong\net al., 2015), which enables a model to learn to focus on\nparticular parts of a sequence.\nThe transformer makes two key contributions. First, it gen-\neralizes attention to multi-head attention, which we discuss\nbelow. Second, the transformer neglects recurrent or convo-\nlutional mechanisms for processing sequences, and relies\nentirely on attention. Critically, this enables signiﬁcantly\nmore parallelism during training, as the model can process\nevery element of a sequence simultaneously, instead of hav-\ning a serial dependence on the prior element.\nA.1.2 Multi-head Attention\nMulti-head attention (MHA) generalizes attention, and uses\nh attention “heads” in parallel to attend to different learned\nprojections of a sequence. We provide Python code and an\nillustration of MHA forward propagation in Fig. 1.\nEach attention head is an instance of scaled dot-product\nattention, with input tensors: queries (q), keys (k), and val-\nues (v). Conceptually, attention ﬁnds values corresponding\nto the keys closest to the input queries. Heads are aug-\nmented with learned linear layers that project their inputs\ninto a lower-dimensional space. The three inputs are ﬁrst\nmultiplied by weight tensors wq, wk, wv, respectively, as a\nlearned input projection (we use Einstein-notation sums, or\neinsums, for tensor contractions). The query and key ten-\nsors are subsequently multiplied together and scaled (stored\nin beta), followed by applying the softmax operation in\norder to weight and select the most important results. This\nis then multiplied with vv to produce the per-head output\n(gamma). The outputs of all heads are ﬁnally concatenated\nand linearly projected back to the input dimension size (i).\nThe respective dataﬂow graph (Fig. 1b) immediately ex-\nposes coarse- (whole graph) and ﬁne-grained (within rect-\nangular nodes) parallelism, as well as data reuse. As every\nedge represents exact data movement, their characteristics\n(access sets and movement volume) can be inspected for\nguided bottlenecks and potential solution analysis.\nThere are three broad classes of MHA, distinguished by\ntheir inputs. General attention uses distinct tensors as the\nqueries, keys, and values. Encoder/decoder attention uses\nthe same tensor for both keys and values. Self-attention uses\nthe same tensor for all three inputs. MHA may also have\na masking step, which is used during training to prevent a\nmodel from “seeing the future” and using information from\na later part of a sequence.\nA.1.3 Transformer Architectures\nBERT (Devlin et al., 2019) is a widely-used transformer\nfor NLP tasks. Fig. 2 illustrates the forward and backward\npass for a single BERT encoder layer. The layer essentially\nconsists of MHA (as self-attention) followed by a feed-\nforward neural network (two linear layers with bias and\nReLU activations after the ﬁrst layer). Dropout (Srivastava\net al., 2014), layer normalization (Ba et al., 2016), and\nresidual connections (He et al., 2016) are also used.\nTransformers also incorporate several other layers that we\nwill not discuss in detail: embedding layers for input se-\nquences and various output layers, depending on the task.\nOther transformer architectures, such as the original Trans-\nformer and GPT-2/3 (Radford et al., 2019; Brown et al.,\n2020) have very similar architectures.\nA.2 Fusion Implementation\nWe implement each fused operator as a single custom CUDA\nkernel and specialize it for a speciﬁc data layout using tem-\nplates to maximize opportunities for compiler optimization.\nTo correctly handle data dependencies, if a reduction is the\nﬁrst operator in a fusion kernel, it is implemented with two\nloops: the ﬁrst computes the reduction and the second uses\nit. Otherwise, each kernel is implemented as a single loop.\nReduction operations in statistical normalizations use a warp\nallreduce among all threads in a warp, implemented with\nCUDA intrinsics. If the number of elements to be reduced\nexceeds the warp size, we perform a sequential reduction\nover smaller chunks ﬁrst. Layout-permuting, we use vec-\ntorized loads, stores, and arithmetic within a single thread,\nand fall back to word-wise implementations otherwise. For\ndropout operators, which must generate a random mask, we\nuse cuRAND for random number generation.\nAfter fusion, we have the following fused element-wise and\nData Movement Is All You Need\nnormalization operations. Fig. 3 illustrates several cases.\n• AIB : Attention input bias.\n• BAOB : Backward attention output bias.\n• BAIB : Backward attention input bias.\n• SM: Softmax with scaling and dropout.\n• BRD : Bias, ReLU, and dropout.\n• BDRLN : Bias, dropout, residual, and layernorm.\n• BSB : Backward layernorm scale and bias.\n• BLNRD : Backward layernorm dX and dropout, saving the\nintermediate result for the residual connection.\n• BDRB : Backward dropout, ReLU, and bias.\n• EBSB : Backward residual and layernorm scale and bias.\n• BS: Backward dropout and softmax with scaling.\n• BEI : Backward encoder input residual connection.\nA.3 Evaluation Details\nAll our results were produced on the Lassen supercom-\nputer (Livermore Computing Center, 2020), which consists\nof 795 nodes, each with two IBM Power9 CPUs and four\nNvidia V100 GPUs with NVLINK2 and 16 GB of mem-\nory. We use CUDA 10.1.243 and GCC 7.3.1 to build our\ncode. For comparison, we use cuDNN 7.6.5, PyTorch 1.6.0\n(PT) (built-in transformer implementation), TensorFlow 2.1\nfrom IBM PowerAI with XLA enabled (transformer adapted\nfrom Wolf et al. (2019)) (TF+XLA), and a recent develop-\nment version of DeepSpeed (DS). Unless noted, our results\nuse mixed-precision training (Micikevicius et al., 2018),\nwith FP16 data and accumulations performed at FP32. In\nPyTorch, we use Apex (Nvidia, 2020a) for mixed-precision;\nin TensorFlow and DeepSpeed, we use the built-in automatic\nmixed-precision. Results are the mean of 100 measurements.\nWhen we compute the percent of peak performance, we use\nthe 125 Gﬂop/s Tensor Core peak on our V100s for tensor\ncontraction operations, and the 31.4 Gﬂop/s half-precision\npeak for other operations.\nA.4 Flop Analysis\nTab. A.1 presents our complete ﬂop analysis. See Sec. 4.1\nfor details.\nA.5 Additional Fusion Opportunities\nA.5.1 Fusing into Tensor Contractions\nBecause cuBLAS does not support fusing arbitrary operators\ninto (batched) MMMs, we evaluated CUTLASS (Nvidia,\n2020b) version 2.1 as an alternative, which does support\nfusing element-wise operators. We conducted a simple\nbenchmark comparing cuBLAS with a separate bias kernel\nto CUTLASS for the ﬁrst linear layer of BERT. We found\nthat the batched MMM in CUTLASS is approximately 40µs\nslower than cuBLAS. The reduction in runtime by fusing the\nbias is less than this. Hence, we only consider cuBLAS for\ntensor contractions. cuBLAS does support simple scaling\noperations, which we use to implement the scaling for the\nscaled softmax operator.\nA.5.2 Algebraic Fusion\nThere is an additional opportunity for fusion among certain\ntensor contraction operators. Using domain knowledge and\nthe dataﬂow graph, we can identify some operations that can\nbe combined into a single algebraically equivalent operation.\nFor example, there are several different ways to perform the\ninput projections (batched MMMs) in self-attention, since\nthe input queries, keys, and values are the same tensor, X:\n1. X can be multiplied by each of the projection matrices:\nWQX, WKX, and WV X.\n2. The WQ and WK matrices can be stacked and two mul-\ntiplications performed: [WQ WK]X and WV X. Simi-\nlarly, the WK and WV matrices can be stacked.\n3. All three can be stacked: [yQyKyV] =[WQ WK WV ]X.\nIn backpropagation, the dW and dX computations for each\nprojection matrix can be similarly fused: X[dyQd yK dyV]\nand [WQ WK WV ][dyQd yKdyV], respectively.\nThere are different tradeoffs to these approaches, which\nmust be determined empirically. Performing separate\nMMMs may enable task parallelism. On the other hand,\nthis stacking enables data reuse, since X is used only once.\nTab. A.2 presents results for each of these cases. Fully fusing\nthis batched MMM performs best. Unfortunately, cuBLAS\nlaunches kernels that utilize the entire GPU, so task paral-\nlelism is not proﬁtable. This example can also be adapted\nto fuse keys and values in encoder/decoder attention.\nA.6 End-to-end training\nWe conducted end-to-end training of BERT-large on the\nWikipedia data corpus. We adapted the BERT implemen-\ntation from the Nvidia deep learning examples (NVIDIA,\n2020a) and performed training on Lassen using 32 nodes\nwith 128 GPUs. The training consists of two phases, one us-\ning a sequence length of 128 and the other with a sequence\nlength of 512.\nPretraining loss versus time is presented in Fig. A.1. We\nachieve a 1.18×speedup in phase 1 and a 1.22×speedup in\nphase 2, for an overall speedup of 1.19×. Note this is less\nthan our 1.30×speedup over PyTorch for an encoder layer\nfor several reasons: The Nvidia implementation includes\nsome further optimizations, such as fused layer normaliza-\ntion kernels; there are additional operations (e.g., embedding\nlayers, loss functions) that we do not currently optimize;\nand overheads from distributed training and data loading.\nData Movement Is All You Need\nTable A.1.Flop analysis for BERT encoder layer. △– tensor contraction, ⬜– statistical normalization, ○ – element-wise. MHA\noperators are ﬁlled black. We bold whichever is greater, % peak (compute-bound) or MUE (memory-bound). The speedup is computed\nfor kernels in isolation, overall speedup may be different due to measurement overheads.\nInput Output PyTorch Ours\nOperator Gﬂop (1e6) (1e6) Gﬂop Time ( µs) % peak Time ( µs) % peak MUE Speedup Kernel\nForward\n▲Q, K, V 25.770 7.34 12.58 25.782 333 56.2 306 61.2 12 1.08 —\n●Input bias 0.013 12.59 12.58 0.025 90 0.4 66 0.5 78 1.35 yAIB\n▲QKT 4.295 8.39 33.55 4.329 189 16.5 143 21.8 50 1.32 —\n⬛Scaled softmax 0.201 33.55 100.66 0.956 453 1.3 433 1.3 32 1.04 ySM\n▲Gamma 4.295 37.75 4.19 8.598 142 21.9 160 19.4 6 0.88 —\n▲Out 8.590 5.24 4.19 8.686 136 45.9 120 52 10 1.13 —\n●Output bias 0.004 4.20 4.19 0.008 34 0.4\n102 0.1 42 1.68\n⎫⎪⎪⎪⎪⎬⎪⎪⎪⎪⎭\nDRLN○Dropout 0.004 4.19 8.39 0.014 37 0.3\n○Residual 0.004 8.39 4.19 0.008 36 0.3\n⬜LayerNorm 0.029 4.20 4.19 0.052 63 1.3\n△Linear 34.360 8.39 16.78 34.377 451 55.4 402 62.1 12 1.12 —\n○Bias 0.017 16.78 16.78 0.034 116 0.4\n183 0.3 76 1.90\n⎫⎪⎪⎪⎬⎪⎪⎪⎭\nBRD○ReLU — 16.78 16.78 — 112 0\n○Dropout 0.017 16.78 33.55 0.052 120 0.4\n△Linear 34.360 20.97 4.19 34.456 449 55.6 369 67.6 6 1.21 —\n○Bias 0.004 4.20 4.19 0.008 35 0.3\n101 0.1 43 1.70\n⎫⎪⎪⎪⎪⎬⎪⎪⎪⎪⎭\nBDRLN○Dropout 0.004 4.19 8.39 0.014 37 0.3\n○Residual 0.004 8.39 4.19 0.008 37 0.3\n⬜LayerNorm 0.029 8.39 4.19 0.052 63 1.3\nBackward\n⬜LayerNorm dW 0.017 8.39 <0.01 0.021 184 0.3 150 0.3 6 1.22 yBSB\n⬜LayerNorm dX 0.038 8.40 4.19 0.064 78 1.4 71 1.5 37 1.58 |BLNRD○Dropout dX 0.004 8.39 4.19 0.008 34 0.4\n△Linear+Bias dX 34.360 8.39 16.78 34.377 427 58.4 414 60.3 5 1.03 —\n△Linear dW 34.360 20.97 4.19 34.389 424 58.9 378 66 13 1.11 —\n⬜Bias dW 0.004 4.19 <0.01 0.005 24 0.5\n362 <0.1 38 1.05\n⎫⎪⎪⎪⎪⎬⎪⎪⎪⎪⎭\nBDRB○Dropout dX 0.017 33.55 16.78 0.034 129 0.4\n○ReLU dX — 33.55 16.78 — 166 0\n⬜Bias dW 0.017 16.78 <0.01 0.021 61 0.8\n△Linear+Bias dX 34.360 20.97 4.19 34.389 417 59.9 398 62.7 6 1.04 —\n△Linear dW 34.360 20.97 4.19 34.456 437 57.2 372 67.2 6 1.17 —\n○Residual 0.004 8.39 4.19 0.008 36 0.3 250 <0.1 17 0.89 |EBSB⬜LayerNorm dW 0.017 8.39 <0.01 0.021 186 0.3\n⬜LayerNorm dX 0.038 8.40 4.19 0.064 80 1.4 69 1.6 37 1.64 |BLNRD○Dropout dX 0.004 8.39 4.19 0.008 34 0.4\n⬛Output bias dW 0.004 4.19 <0.01 0.005 23 0.5 38 0.3 22 0.60 yBAOB\n▲Out dX 8.590 4.33 4.19 8.637 131 47.6 119 52.2 10 1.09 —\n▲Out dW 8.590 8.39 1.05 8.686 136 45.9 113 54.8 5 1.19 —\n▲Gamma dX1 4.295 8.39 33.55 8.598 136 22.8 147 21.2 7 0.93 —\n▲Gamma dX2 4.295 67.11 33.55 4.329 188 16.6 123 25.2 8 1.52 —\n⬛Scaled softmax dX 0.168 12.58 4.19 0.214 790 0.6 426 1.1 49 1.85 yBS\n▲QKT dX1 4.295 37.75 4.19 4.299 135 23.1 155 20 7 0.86 —\n▲QKT dX2 4.295 37.75 4.19 8.598 139 22.3 115 26.9 9 1.20 —\n▲Q, K, V dX 25.770 15.73 4.19 25.799 344 54.4 274 68.2 6 1.25 —\n▲Q, K, V dW 25.770 20.46 1.05 25.911 329 57 293 64 6 1.12 —\n⬛Input bias dW 0.013 12.58 <0.01 0.016 52 0.7 39 0.9 66 1.32 yBAIB\n○Residual 0.004 8.39 4.19 0.008 35 0.3 31 0.4 83 1.14 yBEI\n△Tensor contractions 335.007 — — 348.698 4951 43.1 4411 48.5 — 1.12\n⬜Stat. normalizations 0.575 — — 1.492 2063 0.9 1591 0.6 — 1.29\n○Element-wise 0.105 — — 0.239 1096 0.3 735 0.1 — 1.49\nTotal 335.687 — — 350.429 8110 31.1 6739 35 — 1.20\nData Movement Is All You Need\nTable A.2.Algebraic fusion for MHA Q/K/V (µs).\nUnfused QKfused QKV fused\nForward 345 294 275\nBackward 342 312 291\n0 100 200 300 400 500 600 700\nTime (min)\n2\n4\n6\n8\n10Pretraining loss\n11.4 hours\nLoss: 1.386\n9.57 hours\nLoss: 1.398\nBaseline phase 1\nBaseline phase 2\nOurs phase 1\nOurs phase 2\nFigure A.1.Pretraining loss and time curves for BERT.\nA.7 Additional Related Work\nThere has been signiﬁcant work optimizing both transform-\ners in particular and deep learning in general. For a com-\nprehensive overview, see Ben-Nun & Hoeﬂer (2019). To\nhelp guide training regimes for transformers, recent work\nhas provided empirical recommendations on model size,\nbatch size, and so on (Li et al., 2020; Kaplan et al., 2020;\nRosenfeld et al., 2020). Many of the subsequent techniques\nwe review are complementary to our work.\nMost directly relevant are other approaches speciﬁcally to\naccelerate transformer training. Distributed-memory tech-\nniques, such as ZeRO (Rajbhandari et al., 2019), Mega-\ntron (Shoeybi et al., 2019), and Mesh-TensorFlow (Shazeer\net al., 2018) scale training to many GPUs to accelerate it.\nMesh-TensorFlow also presents a classiﬁcation of opera-\ntors similar to ours. Large batches have also been used\nto accelerate training via LAMB (You et al., 2020) or\nNVLAMB (NVIDIA AI, 2019). None of these directly\naddress the performance of a single GPU as done in this\npaper. DeepSpeed (Microsoft, 2020a), which we compare\nwith in Section 6.3, is closest to our work, but performs all\noptimizations and layout selections manually.\nTransformer architectures to enable improved training have\nalso been the subject of signiﬁcant recent work. AL-\nBERT (Lan et al., 2019) used a combination of weight\nsharing and factorized embedding layers to reduce mem-\nory requirements; however compute times are unaffected.\nTransformer-XL (Dai et al., 2019) caches prior hidden states\nto learn long sequences. RevNets (Gomez et al., 2017), a\nvariant of ResNets which allow activations to be recon-\nstructed during backpropagation, have been applied to trans-\nformers. Notably, Reformer (Kitaev et al., 2020) combines\nreversible residual layers with locality-sensitive hashing to\nimprove the efﬁciency of multi-head attention. Sparsity opti-\nmizations for attention (Bahdanau et al., 2014; Luong et al.,\n2015; Shen et al., 2018; Parmar et al., 2018; Tay et al., 2019;\nChild et al., 2019; Correia et al., 2019; Sukhbaatar et al.,\n2019; Tay et al., 2020) reduce memory and compute require-\nments. We view these as orthongal to our work: the same\nprinciples of data-ﬂow analysis can be applied to optimize\nfor sparsity and reuse.\nThere has also been much work on optimizing deep learn-\ning in general. Many frameworks provide implementa-\ntions of transformers or their components, such as Py-\nTorch (Paszke et al., 2019), TensorFlow (Abadi et al.,\n2015), cuDNN (Chetlur et al., 2014), and others built atop\nthese (Ott et al., 2019; Wolf et al., 2019). Optimizing frame-\nworks can also be applied to transformers (Google, 2020;\nFrostig et al., 2018; Bradbury et al., 2018; PyTorch Team,\n2020; Rotem et al., 2018; Jia et al., 2019d;b;a; Chen et al.,\n2018; Paul G. Allen School of Computer Science & Engi-\nneering, University of Washington et al., 2017; Sivathanu\net al., 2019; Mirhoseini et al., 2017; Cyphers et al., 2018;\nBaghdadi et al., 2019; Vasilache et al., 2018; Lethin, 2017;\nWei et al., 2018; Truong et al., 2016; Venkat et al., 2019;\nDong et al., 2019; Elango et al., 2018). None of these frame-\nworks provide all the optimizations or the systematic study\nof data movement and its impact on performance. We have\nspeciﬁcally compared against some of the most popular pro-\nduction frameworks: PyTorch, TensorFlow with XLA, and\ncuDNN. Beyond these, TASO (Jia et al., 2019a) targets sim-\nilar optimizations to ours by using graph substitutions, but\nconsiders only inference and does not exhaustively explore\nthe search space.\nOther optimizations, including model parallelism (Van Es-\nsen et al., 2015; Jia et al., 2019d; Gholami et al., 2018;\nDean et al., 2012; Chilimbi et al., 2014; Shazeer et al., 2018;\nBuchlovsky et al., 2019), pipeline parallelism (Chen et al.,\n2012; Li et al., 2018; Narayanan et al., 2019; Huang et al.,\n2019), microbatching (Oyama et al., 2018), and recompu-\ntation for memory reduction (Chen et al., 2016; Jain et al.,\n2020) are all also applicable. Communication can also be\na major bottleneck for training transformers, due to the\nlarge model size (Shoeybi et al., 2019; Shazeer et al., 2018).\nFrameworks for inference, including TensorRT (NVIDIA,\n2020b), Caffe2 (Facebook, 2020), and the ONNX Run-\ntime (Microsoft, 2020), all help to enable a suite of op-\ntimizations primarily applicable during inference. Prun-\ning (Shazeer, 2019; V oita et al., 2019) and distillation (Sanh\net al., 2019) has also been used to accelerate inference.\nNeural architecture-speciﬁc optimizations have a long his-\nData Movement Is All You Need\ntory outside of transformers, and have primarily targeted\nCNNs (Krizhevsky et al., 2012; Coates et al., 2013; Goyal\net al., 2017; Akiba et al., 2017; You et al., 2018; Mikami\net al., 2018; Ying et al., 2018; Dryden et al., 2019a;b). No-\ntably, Li et al. (2016) optimized data layouts for convolution.\nIn general, data movement reduction is a core component\nof high-level optimization (Unat et al., 2017). Optimizing\ncompilers, most notably components that specialize in poly-\nhedral programs (Grosser et al., 2012; Bondhugula et al.,\n2008), apply loop transformations (e.g., tiling, skewing) that\nbelong to the class of data movement optimization. Other\nwhite-box approaches for separation of program deﬁnition\nfrom data optimization passes include Halide (Ragan-Kelley\net al., 2013), JAX (Frostig et al., 2018; Bradbury et al.,\n2018), Legion (Bauer et al., 2012), Lift (Steuwer et al.,\n2017), and MLIR (Lattner et al., 2020). The data-centric\napproach proposed here enables user-extensible coarse- and\nﬁne-grained data movement optimization via the ﬂat (yet\nparametric) dataﬂow graph representation. This allows us to\nperform and tune complex data layout and fusion transfor-\nmations that span multiple granularity levels, surpassing the\noptimization capabilities of the aforementioned tools and\nachieving state-of-the-art performance."
}