{
    "title": "Performance and Reproducibility of Large Language Models in Named Entity Recognition: Considerations for the Use in Controlled Environments",
    "url": "https://openalex.org/W4405274408",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2493588529",
            "name": "Jurgen Dietrich",
            "affiliations": [
                "Bayer (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2020194650",
            "name": "André Hollstein",
            "affiliations": [
                "Bayer (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2493588529",
            "name": "Jurgen Dietrich",
            "affiliations": [
                "Bayer (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2020194650",
            "name": "André Hollstein",
            "affiliations": [
                "Bayer (Germany)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4392526270",
        "https://openalex.org/W3143491409",
        "https://openalex.org/W4386121529",
        "https://openalex.org/W2962411443",
        "https://openalex.org/W3005745869",
        "https://openalex.org/W4295959876",
        "https://openalex.org/W2896962189",
        "https://openalex.org/W4380290764",
        "https://openalex.org/W2897647360",
        "https://openalex.org/W3004708926",
        "https://openalex.org/W4280534881",
        "https://openalex.org/W3126506274",
        "https://openalex.org/W4280590646",
        "https://openalex.org/W4365814040",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W4385571451",
        "https://openalex.org/W2129767020",
        "https://openalex.org/W4391292768",
        "https://openalex.org/W4393063345",
        "https://openalex.org/W3199307809",
        "https://openalex.org/W4381309010",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3156470785",
        "https://openalex.org/W4287891464",
        "https://openalex.org/W2895296143",
        "https://openalex.org/W4389524506",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W6857464309",
        "https://openalex.org/W6772383348",
        "https://openalex.org/W3205349317",
        "https://openalex.org/W6858023062",
        "https://openalex.org/W6852449896",
        "https://openalex.org/W6863109351"
    ],
    "abstract": "We demonstrated that zero-shot GPT 4 performance is comparable with a fine-tuned T5, and Zephyr performed better than zero-shot GPT 3.5, but the recognition of product combinations such as product event combination was significantly better by using a fine-tuned T5. Although Open AI launched recently GPT versions to improve the generation of consistent output, both GPT variants failed to demonstrate reproducible results. The lack of reproducibility together with limitations of external hosted systems to keep validated systems in a state of control may affect the use of closed and proprietary models in regulated environments. However, due to the good NER performance, we recommend using GPT for creating annotation proposals for training data as a basis for fine-tuning.",
    "full_text": "Vol.:(0123456789)\nDrug Safety (2025) 48:287–303 \nhttps://doi.org/10.1007/s40264-024-01499-1\nORIGINAL RESEARCH ARTICLE\nPerformance and Reproducibility of Large Language Models \nin Named Entity Recognition: Considerations for the Use in Controlled \nEnvironments\nJürgen Dietrich1  · André Hollstein1\nAccepted: 11 November 2024 / Published online: 11 December 2024 \n© The Author(s) 2024\nAbstract\nIntroduction Recent artificial intelligence (AI) advances can generate human-like responses to a wide range of queries, mak-\ning them a useful tool for healthcare applications. Therefore, the potential use of large language models (LLMs) in controlled \nenvironments regarding efficacy, reproducibility, and operability will be of paramount interest.\nObjective We investigated if and how GPT 3.5 and GPT 4 models can be directly used as a part of a GxP validated system \nand compared the performance of externally hosted GPT 3.5 and GPT 4 against LLMs, which can be hosted internally. \nWe explored zero-shot LLM performance for named entity recognition (NER) and relation extraction tasks, investigated \nwhich LLM has the best zero-shot performance to be used potentially for generating training data proposals, evaluated the \nLLM performance of seven entities for medical NER in zero-shot experiments, selected one model for further performance \nimprovement (few-shot and fine-tuning: Zephyr-7b-beta), and investigated how smaller open-source LLMs perform in con-\ntrast to GPT models and to a small fine-tuned T5 Base.\nMethods We performed reproducibility experiments to evaluate if LLMs can be used in controlled environments and uti-\nlized guided generation to use the same prompt across multiple models. Few-shot learning and  quantized low rank adapter \n(QLoRA) fine-tuning were applied to further improve LLM performance.\nResults and Conclusion We demonstrated that zero-shot GPT 4 performance is comparable with a fine-tuned T5, and Zephyr \nperformed better than zero-shot GPT 3.5, but the recognition of product combinations such as product event combination \nwas significantly better by using a fine-tuned T5. Although Open AI launched recently GPT versions to improve the genera-\ntion of consistent output, both GPT variants failed to demonstrate reproducible results. The lack of reproducibility together \nwith limitations of external hosted systems to keep validated systems in a state of control may affect the use of closed and \nproprietary models in regulated environments. However, due to the good NER performance, we recommend using GPT for \ncreating annotation proposals for training data as a basis for fine-tuning.\n1  Introduction and Objective\nOver the last few years artificial intelligence/machine \nlearning (AI/ML) has been increasingly used throughout \ndrug development and across therapeutic areas from drug \ndiscovery, non-clinical and clinical research, clinical trial \nmanagement, and drug manufacturing to post marketing \nsafety surveillance [1 –8]. With the increased use of AI in \nhealthcare, ethical challenges become more apparent. In \n2021 the WHO issued a report on AI in health with six \nguiding principles for its design and use to address ethi-\ncal considerations [9 ]. Managing the permanently increas-\ning volume, variety, and velocity of pharmacovigilance \n(PV) data in the framework of regulatory requirements \npresents a remarkable challenge. One important AI/ML \nbenefit resides in its ability to learn from real-world use \nand experience, and the AI/ML capability for performance \nimprovement. AI-based automation in PV and especially \nin individual case safety report (ICSR) processing repre-\nsents a significant opportunity for efficiency improvement \nand cost reduction [10– 12].\nRecent publications are addressing quality aspects of \nAI/ML use as software as a medical device (SaMD) in \n * Jürgen Dietrich \n juergen.dietrich@bayer.com\n1 Pharmaceuticals, Medical Affairs and Pharmacovigilance, \nData Science and Insights, Bayer AG, Müllerstr. 178, \n13353 Berlin, Germany\n288 J. Dietrich, A. Hollstein \nKey Points \nReproducibility of generated text is a prerequisite when \nusing LLMs in regulated environments.\nOpen-source LLMs ensure reproducibility and provide \ngood results by fine-tuning, few-shot learning, and \nguided generation.\nLLM zero-shot performance is a good starting point to \nsupport training-data generation.\nhealth care focused on risk considerations [13– 15] and \nthe use of AI in the medicinal product life cycle [16]. \nAlthough requirements for validation largely remain the \nsame, additional activities tailored to AI/ML systems \nare required to document evidence that those systems \nare fit for purpose [17]. US Food and Drug Administra-\ntion (FDA) proposes to place AI/ML-based systems into \none of four categories to reflect the risk associated with \nthe clinical situation and the significance of information \nprovided [13], and introduced a total product lifecycle \n(TPLC) approach for the regulation of AI/ML-based soft-\nware products [18]. Beside common computer validation \naspects, Kompa et al. described in a systematic review \nthat a lot of AI/ML implementations do not reflect cur -\nrent characteristics of good machine learning practices \nand trends in machine learning [19]. Intensifying efforts \nto identify potential adverse drug reactions (ADRs), and \nsubsequently create, store, analyze, and communicate \nresults in individual case safety reports (ICSRs), could \npotentially produce more reports, but would not neces-\nsarily result in more rapid or accurate identification or \nunderstanding of true safety issues [20].\nBesides classical computer validation aspects like \nresult reproducibility, system performance, controlled \nstate persistence, the investigation of which system, \nmethodology, and appropriate training data are best \nfor the intended AI/ML system use is of paramount \nimportance.\nIn machine learning, the system evaluation analyzes \nhow well a model performs its intended task. Accu-\nracy may be insufficient in situations with imbalanced \nclasses, which is mainly given for instance in adverse \nevent (AE) detection in real-world datasets. In pharma-\ncovigilance, model evaluation is therefore focusing on \nF1 score as the harmonic mean of precision and recall \n(i.e., with increased precision and maintained F1 score \nrecall is decreased) and preferably on recall, since recall \nmeasures indicate how well the model finds all positive \noccurrences in a dataset.\nA GxP validated system must be kept in a state of \ncontrol. Therefore, it is important to establish appropri-\nate measures for process and system maintenance to keep \nthe validated state during routine operation. This ongo-\ning process and system verification provides documented \nevidence that the system remains in a state of control \nduring production use [21].\nThe system remains in a GxP validated state as long \nas conditions and control parameters remain unchanged. \nBy using the current externally hosted OpenAI model \nfamily, this status can hardly be preserved due to the lack \nof direct control over system infrastructure, which may \npotentially affect system quality by provider’s system or \ninfrastructure changes.\nReproducibility is an essential ingredient of com-\nputer system validation, meant to verify the documented \nresults and claims and to enable a continuous life cycle \nmanagement process. AI methods and algorithms are \nincreasingly used in critical systems or applications, \nwhere their decisions can have an impact on people and \nsociety, and their improper operation may cause harm. \nTherefore, ensuring reproducibility of algorithm results \nis a method to test their quality and a signal of their \ncredibility [22].\nMeasuring the closeness of results under the same \nrepeatability conditions (e.g., same sample, measurement \nprocedure, operating conditions) establishes scientific \nevidence during validation that the process is reproduc-\nible and will consistently deliver the requested quality \n[23].\nThe validation protocol should specify a sufficient \nnumber of replicate process runs to demonstrate repro-\nducibility and provide an accurate measure of variability \namong successive runs [24].\nIn our study we investigated the performance (partial \nF1 score and partial recall) of several externally hosted \nOpenAI models versus internally hosted large language \nmodels (LLMs) to explore, by example of AE detection \ntasks in pharmacovigilance, if pre-trained or fine-tuned \nsmaller LLMs would reach the efficacy of OpenAI mod-\nels like GPT 4. We considered NLP tasks that go beyond \nnamed-entity recognition (NER) (e.g., detection of a drug \nor dose) such as linking two entities with each other (e.g., \nextracting product dose combinations). Recent advances \nin text-generating models make it possible to cast many \ntasks relevant for the medical domain as text-generation \ntasks to solve multiple use-cases with a unified approach. \nWe emphasize here named-entity recognition and link -\ning, as they are important applications with many down-\nstream use-cases such as detection of adverse events or \nentity-based search.\n289\nLLM Performance and Reproducibility in Named Entity Recognition in Controlled Environments\nAs a baseline model, we used in our experiments \nT5-Base since it was shown that T5 outperformed other \nAE detection models based on bidirectional encoder \nrepresentations from transformers (BERT) variants and \ncan be used to apply the same model for multiple and \ndiverse datasets [25, 26]. In addition, we investigated \nthe reproducibility of OpenAI models as since November \n2023 OpenAI introduced new methods to ensure better \nreproducibility of their models.\nA recent publication by Wadhwa et al. [27] demonstrated \nthat relation extraction on the adverse drug effect (ADE) \ncorpus [28] and non-medical datasets can be achieved using \nGPT-3 and few-shot learning, Hu et al. [29] showed good \nresults for named entity recognition on data from vaccine \nadverse event reporting system (VAERS) using GPT 3.5 and \n4 with prompt engineering and few-shot learning. Li et al. \n[30] focused on LLMs to extract adverse events from sur -\nveillance reports on influenza vaccine adverse events from \nVAERS.\nSince those publications have already demonstrated that \ncurrent LLMs can deliver results that match with or improve \nprevious methods, we do not focus in our paper on further \nimproving model performance, but\n• to investigate if and how GPT 3.51 and GPT 4 models can \nbe directly used as a part of a GxP-validated system,\n• to investigate if the expected better performance of GPT \n4 versus GPT 3.5 justifies the higher costs and longer \nexecution time per request,\n• to investigate the LLM zero-shot NER performance to \nfind the best open-source model for further performance \nimprovement steps (i.e., few-shot learning, fine-tuning),\n• to investigate which LLM has the best zero-shot perfor -\nmance to be used potentially for generating training data \nproposals,\n• to investigate how smaller open-source LLMs perform \nin contrast to GPT models and to a small fine-tuned T5 \nBase, since smaller open-source LLMs are easier to \nimplement in local infrastructures, easier to maintain, \nand faster to train with less resources and costs,\n• and to recommend a solution for LLMs for the use in a \ncontrolled environment.\n2  Methods\n2.1  Overview\nIn our study we were using a fine-tuned T5 Base as a refer -\nence model which shows good performance in PV-related \nNER tasks [31, 32]. Our assumption was that all larger \nmodel sizes compared to T5 Base would typically result in \nbetter performance, but it would also require more comput-\ning resources to train and run. In our experiments we strove \nto find the best balance between efforts and performance \ndepending on the results of our experiments.\nLLMs are trained with large amounts of text data to \ndetermine the most likely continuation of an input text. \nThe model proposes a list of the most probable subsequent \nwords, and a text generation algorithm is employed to select \nthem. Algorithms that do not consistently select the most \nlikely word can enhance the overall probability of the com-\nplete output and generate more variable and natural text. \nTemperature is a common parameter, which regulates the \nprobability of selecting the most likely next word. Its value \ntypically starts at 0 and progress to higher values, which \nintroduce greater randomness or creativity. In our study, the \ntemperature parameter for all models was set to 0 to reduce \nthe degree of randomness in response generation.\nThe term zero-shot refers to a prompting technique \nwherein the model is presented with a task without any addi-\ntional examples, whereas in the case of a few-shot prompt, \nthe prompt comprises multiple examples of the task and the \nanticipated outcome [33–35].\nExperiments were executed between October 2023 and \nFebruary 2024. We started with prompt engineering2 of GPT \n3.5 Turbo [33] (gpt-3.5-turbo-1106) and GPT 4 Turbo [36] \n(gpt-4-1106-preview) models [37]. Although GPT 3.5 and \nGPT 4 experiments were originally executed using Micro-\nsoft Azure, we repeated our experiments with Open AIs \npublic API (version 1.6.1) to allow reproducibility for the \nresearch community. Results obtained by the OpenAI API \nPython SDK for text generation did not principally deviate \nfrom those executed in Azure.\nBoth GPT models return a maximum of 4096 output \ntokens3, while the context window of GPT 3.5 Turbo with \n16,385 tokens is lower when compared with GPT 4 Turbo \ncontext window of 128,000 tokens. We performed qualita-\ntive prompt experiments independent of entity performance. \nWe focused on modifying the system and user prompt to \n1 Please note that ‘T5’, ‘GPT 3.5’, and ‘GPT 4’ refer in our experi-\nments to the models ‘T5-Base’, ‘GPT 3.5 Turbo’, and ‘GPT 4 Turbo’, \nrespectively.\n2 Prompt engineering is the process of writing, refining, and optimiz-\ning inputs to allow AI models to create specific, high-quality outputs.\n3 Language models read and write text in chunks called tokens. In \nEnglish, a token can be as short as one character or as long as one \nword.\n290 J. Dietrich, A. Hollstein \nachieve adequate prompt interpretability by OpenAI models \nin terms of result format and complete and correct interpre-\ntation of all prompt information.\nThere are various sizes available for pre-trained T5 mod-\nels [26] based on a Colossal Clean Crawled Corpus (C4), \nincluding small (60 million parameters), base (220 million \nparameters), large (770 million parameters), 3B (3 billion \nparameters), and 11B (11 billion parameters).\nFor evaluating the model performance (see section 2.5.1), \nwe used several LLMs published on the HuggingFace hub, \nsuch as argilla/notus-7b-v1, epfl-llm/meditron-7b, Hug-\ngingFaceH4/zephyr-7b-beta, meta-llama/Llama-2-13b-hf, \nand togethercomputer/Llama-2-7B-32K-Instruct [38– 42].\nTable 1 summarizes methods and expected objectives of \nour study.\n2.2  Dataset\nThe annotated dataset used for all experiments consists \nof four sources: Bayer Literature Database (PubMed), \nADECorpus (PubMed) [28], Social Media Mining for \nHealth Applications (SMM4H, Twitter) [43], and drugs.\ncom[44] and 890 records and is publicly available as sup-\nplementary material in [32]. Each record consists of one or \nseveral sentences. The dataset was consistently annotated \nwith controlled quality, contains diverse data sources, and \nthe entities are relevant for PV. The dataset is small, diverse, \nand very clean and allows to train ML models with limited \nresources. The detailed entity description, data selection, \nand the annotation process are described in [32].\n2.3  Prompting\nWe performed manual prompt optimization for OpenAI \nmodels to obtain the best results based on a few samples of \ninput texts and describe our approach below. Many addi-\ntional methods exist, and a growing body of literature is \ndiscussing additional methods like adding personas or chain-\nof-thought prompting [ 45]. We decided to stop spending \nadditional effort in prompt engineering once we reached \nsatisficing results.\nWe used OpenAI’s prompt engineering guide [46] to \nachieve better results in terms of adequate prompt inter -\npretability of the text provided. We applied some strategies \nwhich are adequate for the given tasks:\n• Ask the model to adopt a persona\n• Use delimiters to clearly indicate distinct parts of the \ninput\n• Instruct the model to answer using a reference text\nIn contrast to Hu et al. [29], who provided annotation \nguideline-based prompts and annotated samples via few-\nshot learning, we intentionally started with zero shot experi-\nments and without any further explanation what is meant by \na given entity description (e.g., indication or adverse event) \nto investigate the native LLM entity extraction capability.\nWe realized that the task definition in our prompt should \nbe as exact as possible (e.g., three directly consecutive \nhashtags instead of three consecutive hashtags) and prefer -\nably with an example (e.g., for product event combination: \n“Aspirin|Rash”).\nTable 1  Overview about the experiments executed and their related objective\nExperiment Objective\nGPT 3.5 and GPT 4 prompt engineering Proper prompt interpretability in terms of result format, completeness, \nand correct interpretation of all prompt information\nGPT reproducibility experiments Investigating whether GPT provides deterministic output as a prereq-\nuisite for computer system validation to demonstrate that a system is \nsuitable for its intended purpose\nOpen-source LLM selection by executing zero-shot performance and \ncomparison with zero-shot GPT 3.5 and GPT 4\na) Investigation of the performance of 7B and 13B open-source LLMs \nby using “guided generation” to select the best performing LLM for \nfurther performance improvement steps\nb) Demonstration which entities can be identified by LLMs out-of-the-\nbox\nc) Investigation which LLM has the best zero-shot performance to be \nused potentially for generating training data proposals\nd) Investigation if the expected better performance of GPT 4 versus \nGPT 3.5 justifies the higher costs and longer execution time per \nrequest\nComparison of selected best performing open-source LLM processed \n(i.e., fine-tuned, few-shot) against zero-shot model, fine-tuned T5, and \nzero-shot GPT\na) Investigation of potential LLM performance improvement by few-\nshot learning and QLoRA fine-tuning compared to the zero-shot \nmode, T5, and GPT\nb) Proposal for use of LLMs in a regulated environment (as a conclu-\nsion of all results)\n291\nLLM Performance and Reproducibility in Named Entity Recognition in Controlled Environments\nFor all our experiments, the following prompt example \nwas used with the variable part embedded by three hashtags \nas start and end tag (except for fine-tuned T5 which did not \nrequire any prompting). We used all entity labels which are \ndescribed in [32]:\n• System prompt (only OpenAI models, system prompt \nrefers to the set of instructions that guide the output): “I \nwant you to act as an experienced and diligent annotator \nin a Pharmacovigilance department. Follow the user's \ninstructions carefully. Respond using plain text.”\n• User prompt (with an input text example used for all \nreproducibility experiments, the user prompt is the spe-\ncific input or question you want the AI to respond to):\n• “Use the following items and identify all items in the \ntext marked by three directly consecutive hashtags (e.g., \n###text to be analyzed###) put in front and at the end of \nthe text:\n01. Adverse event\n02. Mode of action\n03. Administration form / Primary packaging\n04. Administration route\n05. Comorbidity\n06. Dosage\n07. Drug / Device\n08. Indication\n09. Intended effect\n10. Medical history / condition\n11. Method / Procedure / Administration\n12. Outcome\n13. Product Dose Combination (PDC)\n14. Product Event Combination (PEC)\n15. Product Indication Combination (PIC)\n16. Product Technical Complaint (PTC)\n17. Target parameter\n18. Target population\nPresent the results in the following form:\n- Per item use a new line with the format: 'item: hit(s)'\n- List multiple hits separated by semi-colon\n- When you do not find an entry fill the field with 'None'\n- When you find a product combination (PEC, PIC, \nPDC) separate Drug / Device from the other Adverse \nEvent, Indication, or Dose term by a pipe character (e.g., \n'Aspirin|Rash')\n- List all items with preceding numbers\n- Stop generating after the list is complete\nInput text:\n“###After TACE for intrahepatic metastasis, localized \nCCRT (45 Gy over 5 weeks with conventional fractionation \nand hepatic artery infusional chemotherapy using 5-fluo-\nrouracil as a radiosensitizer, administered during the first \nand fifth weeks of radiotherapy) was used to treat main HCC \nwith PVT.###”\n2.4  Reproducibility Experiments\nReproducibility is of paramount importance for produc-\ntion use of AI models in a regulated environment. This \nwas also acknowledged by the OpenAI developers. In \nNovember 2023, two new OpenAI model versions were \nreleased addressing this need (gpt-4-1106-preview, gpt-35-\nturbo-1106). Both models were used for our reproducibility \nexperiments.\nIn the OpenAI Cookbook [47] the following explanation \nis provided:\n“Reproducibility has always been a big request from \nuser communities when using our APIs … Develop-\ners can now specify seed parameter in the Chat Com-\npletion request to receive (mostly) consistent outputs. \nTo help you keep track of these changes, we expose \nthe system_fingerprint field… The system fingerprint \nis an identifier for the current combination of model \nweights, infrastructure, and other configuration options \nused by OpenAI servers to generate the completion.”\nThe reproducibility of model results (used models: gpt-4-\n1106-preview, gpt-35-turbo-1106, HuggingFaceH4/zephyr-\n7b-beta) was evaluated by providing a single record multiple \ntimes as the same request (n = 100) and analyzing the num-\nber of different model responses. The prompting described \nin section 2.3 was used.\nFor reproducibility assessment, the number of different \nresponses was counted. In all experiments and all models, \nthe temperature was set to zero and the seed number was \nomitted or set to 42 to analyze result differences.\n2.5  Model Performance\nOur motivation for this experiment was to compare the \nperformance of externally hosted OpenAI models versus \ninternally hosted LLMs. In our experiments, we were focus-\ning on partial scores on the positive class (F1 and recall) \nto evaluate overlapping hits between prediction and truth \n[32]. We started by running zero-shot performance studies \nwith a range of transformer models to select the best model \nfor further development steps like few-shot prompting or \nfine-tuning and compared the results of the selected model \nagainst two OpenAI models (gpt-4-1106-preview, gpt-35-\nturbo-1106) as well as a fine-tuned T5.\nWe assumed that GPT 4 would outperform GPT 3.5, but \nwe also wanted to investigate if the potentially better perfor-\nmance justifies the higher costs and longer execution time \nper request.\n292 J. Dietrich, A. Hollstein \n2.5.1  Model Selection\nTransformer models were selected, which could be poten-\ntially run and fine-tuned with reasonable technical efforts \nand costs. Therefore, we focused on recent 7B and 13B \nparameter models which were at the time of selecting \nLLMs for our study (November / December 2023) ranking \nhigh on Hugging Face leader boards [48]: argilla/notus-\n7b-v1, epfl-llm/meditron-7b, HuggingFaceH4/zephyr-7b-\nbeta, meta-llama/Llama-2-13b-hf, and togethercomputer/\nLlama-2-7B-32K-Instruct.\nAny selection of models will be an extreme reduction \ncompared to the models published on the Hugging Face \nhub, which hosts, as of June 2024, over 100k models for \nthe Text Generation task. It's common for models to be \npublished in different versions, like meta-llama/Llama-2-\n13b-hf or meta-llama/Llama-2-13b-chat-hf. When mul -\ntiple versions of a candidate model were available, we \nfocused on the instruct models in favor of chat models. \nThis was because instruct models tend to perform better \nin use cases that require adherence to structured output to \nmake use of the generated text. Chat models tend to excuse \nthemselves and are wordy in their outputs, which makes \nthem suitable for use in chat applications but less favorable \nfor data processing tasks.\nWe compared the model performance against a T5 model \nfine-tuned on splits of the published dataset [32]. The per -\nformance evaluation was executed to identify the candidate \nmodel for further processing (few-shot and fine-tuning \nexperiments). We focused a) on the named entities indica-\ntion, adverse event, drug/device, dose, product event com-\nbination, product indication combination, and product dose \ncombination due to business relevance and since the dataset \nprovides the highest number of occurrences for those enti -\nties and b) on one data source (ADE Corpus), because the \nmajority of dataset records originated from this source (n = \n500 of 890 records). All model selection experiments were \nperformed as zero-shot and executed only once (n = 1).\n2.5.2  Guided Generation\nInitial experiments indicated that the prompt for GPT \n3.5 and GPT 4 was not generating useful results with the \nselected 7B or 13B LLMs. The major issue was that open-\nsource models did not adhere to the desired output format \nand output termination.\nPerformance of LLMs is highly sensitive to how they \nare prompted, and a growing body of literature is address-\ning various techniques to optimize results [45, 46]. One \nimportant issue of open-source models is adherence to the \nexpected output format, which is unlikely to be fixed by \nprompt optimization alone.\nOne possible option to address the issue with the output \nformat is to use optimized single prompts per entity, but with \nthe disadvantage of additional work and complexity.\nWe used guided generation (version 0.1.10, 22 December \n2023), which offers a guaranteed and stable solution for this \nissue without the need for additional prompting efforts to \ncompare multiple open-source LLMs with the same user \nprompt developed for GPT. There are several approaches of \ncontrolling the output of LLMs, e.g., [49, 50]. In our paper \nwe evaluated two approaches of guidance-ai for guided gen-\neration [50]:\n• Constrained generation to input words: Enforcing out-\nput structure, hard constraints on text generation to words \noccurring in the input text and in addition, generation \ntermination after newlines. This approach is based on the \nidea that the output of a NER task can be limited to the \ninput text since generated words which do not occur in \nthe input text are by definition non-valid responses. This \nconstraint works well for NER tasks but breaks down \nquickly when more free form answers are needed (as \nrequired e.g., for relation extraction).\n• Guided generation: Enforcing output structure and \nunconstrained generation within free text blocks, with \ntermination of generation after newlines. This approach \nentirely focuses on adherence to a consistent output \nstructure, which greatly simplifies downstream process-\ning of model inference results and separates the model \nability to adhere to a described output format and gener-\nating meaningful answers.\nDue to the use of product combinations and the use of \npipe characters in the expected results, we focused on guided \ngeneration for the following analysis. Our experiments with \nthese two approaches showed that imposing known con -\nstraints on LLM inference greatly simplifies further usage \nand processing of LLM results and can bridge the gap \nbetween model performance of vastly different parameter \nsizes.\n2.6  T5 and Zephyr Fine‑Tuning\nT5 was fine-tuned as described in [32] by using the pub-\nlished dataset, executing stratified five-fold cross valida-\ntion with 80:20 splits. Stratification was performed based \non data source and entity type. HuggingFaceH4/zephyr-\n7b-beta was selected for fine-tuning as we saw the most \nconsistent and promising results from this model. The \nsame dataset as for the T5 models was used for training \nand testing to create 5-fold cross validation Zephyr mod-\nels. The quantized low rank adapter (QLoRA) method \n[51] was used to quantize a pre-trained model to 4-bit and \nadds a small set of learnable (QLoRA) weights which are \n293\nLLM Performance and Reproducibility in Named Entity Recognition in Controlled Environments\ntuned by back propagating gradients through the quan-\ntized weights. QLoRA reduces the average GPU memory \nrequirements significantly by using novel high-precision \ntechniques such as 4-bit NormalFloat, double quantization, \nand paged optimizers without significant performance deg-\nradation [51]. We used the X-LLM library for streamlining \nmodel training optimization and LLM finetuning [52].\nFor the fine-tuning of HuggingFaceH4/zephyr-7b-beta, \nwe used an Adam optimizer (paged_adamw_8bit) and set \nthe maximum quantization samples to 1024. The learn-\ning rate was set to 0.0002, batch size was set to 1, and \nthe epoch was set to 1. The T5 fine-tuning parameter are \ndescribed in [32].\n2.6.1  Few‑Shot Experiments\nAs a first step towards optimizing model performance, \nwe performed few-shot experiments which were based on \nthe same conditions as for the zero-shot experiments. In \ntotal, eight samples (about 1% of dataset) were selected \n(two samples per data source) which represented per \ndata source the highest number of entities per record. We \nensured that “None” was present in our few-shot experi-\nments to teach the models that “None” is a valid option. \nWe executed the experiments with the original Hugging-\nFaceH4/zephyr-7b-beta model and our fine-tuned Zephyr \nmodels. For the evaluation we focused on the source ADE \nCorpus (which contains the most records, n = 500).\n3  Results\n3.1  Prompting\nTable  2 shows two examples of GPT 4 model responses \nrepresenting the two most frequent model results by using \nsystem and user prompts introduced in section 2.3 [Input \ntext: “After TACE for intrahepatic metastasis, localized \nCCRT (45 Gy over 5 weeks with conventional fractionation \nand hepatic artery infusional chemotherapy using 5-fluo-\nrouracil as a radiosensitizer, administered during the first \nand fifth weeks of radiotherapy) was used to treat main \nHCC with PVT.”].\nBoth GPT models created a string as a response. In a \npost-processing step, we searched for the entity label and \nused, for comparison of prediction and truth, the informa-\ntion after the colon until the end of line. The T5 procedure \nis described in [32]. For all other models, we used guided \ngeneration and directly created structured data in the same \nformat.\n3.2  Reproducibility Experiments\nAs described in section 2.4, reproducibility experiments \nwere executed to evaluate the number of different responses \nwithout a post-processing step on three different models with \na temperature of 0.0. The model HuggingFaceH4/zephyr-\n7b-beta showed the expected behavior, i.e., in case the seed \nnumber was omitted we observed different responses (n  = \n7, data not shown), when seed number was set, we observed \nonly one unique response (i.e., n = 1, data not shown). Using \ngpt-35-turbo-1106 and gpt-4-1106-preview we observed \nwhile omitting the seed number different results as expected \n(gpt-35-turbo-1106, n = 10, number of different system fin-\ngerprints = 1, and gpt-4-1106-preview n  = 33, number of \ndifferent system fingerprints = 6, data not shown), but when \nwe set the seed number, GPT 3.5 and GPT 4 did not show \na significant reduction in the response variability (gpt-35-\nturbo-1106, n = 8, number of system fingerprints = 1, data \nnot shown, and gpt-4-1106-preview n = 24, number of sys-\ntem fingerprints = 4). In Figure 1, the GPT 4 distribution of \ndifferent responses and the number of system fingerprints \nper response is shown. About 1/3 of all system responses \n(n = 33 of 100) contain the same contents but distributed \nover all four available system fingerprints (see Fig.  1, left \nblue bar and left red bar). In Fig. 2 the total number and the \nunique number of responses is displayed. Nearly half of all \nresponses (blue bar, n = 48 of 100) are related to one system \nfingerprint and represent about 44 % of all unique responses \n(red bar, n  = 20 of 45). Figures  1 and 2  demonstrate that \ndifferent fingerprints can generate the same result, and one \nfingerprint can generate different responses. It is obvious \nthat the established OpenAI concept of system fingerprint \nand seed number provides only insufficient capabilities to \nachieve consistent reproducibility. As a consequence of our \nresults, we will not invest in further performance improve-\nments of GPT 3.5 and 4 since in their current state they are \nnot fit for application in a regulated environment. We do not \nclaim this to be a discovery and are not speculating about \npotential explanations for this behavior.\nWe do not see any inherent reason why these models \ncould not be technically made reproducible with technical \nor legal efforts. Especially since this issue does not occur \nwhen running models locally and controlling for reproduc-\nibility, as our application of open models show in other parts \nof this paper.\n3.3  Measurement of Model Performance\n3.3.1  Zeroshot Experiments\nFigures  3 and 4  show the partial F1 and partial recall of \nselected 7B and 13B LLMs for seven entities for all mod-\nels under consideration and by using the same prompt. \n294 J. Dietrich, A. Hollstein \nTable 2  Two GPT 4 examples are shown to demonstrate the responses selected from the highest and second highest numbers of total response shown in Fig 1. The differences between both \nresults are shown in bold faces\nGy, Gray (SI unit); HCC, hepatocellular carcinoma; PVT, portal vein thrombosis; TACE , transarterial chemoembolization; CCRT, concurrent chemoradiotherapy; PDC, product dose combina-\ntion; PEC, product event combination; PIC, product indication combination; PTC, product technical complaint\nResponse 1 Response 2 Truth\n1 01. Adverse event: none 01. Adverse event: none 01. Adverse event: none\n2 02. Mode of action: none 02. Mode of action: none 02. Mode of action: radiosensitizer\n3 03. Administration form/Primary packaging: none 03. Administration form/primary packaging: None 03. Administration form/primary packaging: None\n4 04. Administration route: hepatic artery infusional chemo-\ntherapy\n04. Administration route: hepatic artery infusional chemo-\ntherapy\n04. Administration route: hepatic artery infusional chemo-\ntherapy\n5 05. Comorbidity: intrahepatic metastasis 05. Comorbidity: intrahepatic metastasis 05. Comorbidity: intrahepatic metastasis, PVT\n6 06. Dosage: 45 Gy over 5 weeks; during the first and fifth \nweeks of radiotherapy\n06. Dosage: 45 Gy over 5 weeks; during the first and fifth \nweeks of radiotherapy\n06. Dosage: 45 Gy over 5 weeks; during the first and fifth \nweeks of radiotherapy\n7 07. Drug/device: 5-fluorouracil 07. Drug/device: 5-fluorouracil 07. Drug/device: 5-fluorouracil\n8 08. Indication: main HCC with PVT 08. Indication: main HCC with PVT 08. Indication: intrahepatic metastasis; main HCC with PVT\n9 09. Intended effect: none 09. Intended effect: none 09. Intended effect: chemotherapy, treat\n10 10. Medical history/condition: none 10. Medical history/condition: none 10. Medical history/condition: none\n11 11. Method/procedure/administration: TACE; localized \nCCRT; conventional fractionation\n11. Method/procedure/administration: TACE; CCRT; \nhepatic artery infusional chemotherapy\n11. Method/procedure/administration: TACE; localized \nCCRT; conventional fractionation; hepatic artery infusional \nchemotherapy; radiotherapy\n12 12. Outcome: none 12. Outcome: None 12. Outcome: none\n13 13. Product dose combination (PDC): 5-fluorouracil|45 Gy \nover 5 weeks\n13. Product dose combination (PDC): 5-fluorouracil|45 Gy \nover 5 weeks\n13. Product dose combination (PDC): 5-fluorouracil|45 Gy \nover 5 weeks\n14 14. Product event combination (PEC): none 14. Product event combination (PEC): None 14. Product event combination (PEC): none\n15 15. Product indication combination (PIC): \n5-fluorouracil|main HCC with PVT\n15. Product indication combination (PIC): \n5-fluorouracil|main HCC with PVT\n15. Product indication combination (PIC): \n5-fluorouracil|intrahepatic metastasis; 5-fluorouracil|main \nHCC with PVT\n16 16. Product technical complaint (PTC): none 16. Product technical complaint (PTC): none 16. Product technical complaint (PTC): none\n17 17. Target parameter: none 17. Target parameter: none 17. Target parameter: none\n18 18. Target population: none 18. Target population: none 18. Target population: none\n295\nLLM Performance and Reproducibility in Named Entity Recognition in Controlled Environments\nFig. 1  Distribution of gpt-4-\n1106-preview responses (blue) \nand of system fingerprints (red) \nobserved by providing 100 \ntimes the same request. The \nbars are sorted by unique total \nresponse count in descending \norder\nFig. 2  Distribution of gpt-4-\n1106-preview total (blue) and \nunique responses (red) observed \nper system fingerprint. The \nbars are sorted by the total \nresponse count per fingerprint \nin descending order\nFig. 3  Partial F1 entity performance of open-source LLMs. AE, adverse event; PDC, product dose combination; PEC, product event combina-\ntion; PIC, product indication combination\n296 J. Dietrich, A. Hollstein \nHuggingFaceH4/zephyr-7b-beta and argilla/notus-7b-v1 \nperformed best compared to the other three open-source \nLLMs. The meta-llama/Llama-2-13b-hf performed sig-\nnificantly worse than the other LLM models. As shown in \nFig. 4, the Zephyr model in our experiment performs better \nregarding partial recall than the Notus model, and therefore, \nwas selected for further investigation (i.e., fine-tuning and \nfew-shot inference).\n3.3.2  Comparison Between Fine‑Tuned Zephyr \nand Fine‑Tuned T5 Models\nFigure 5 shows the partial F1 and partial recall results of the \nfine-tuned T5 and fine-tuned Zephyr models. The partial F1 \nscore of a fine-tuned T5 is higher compared with Zephyr for \nall selected entities. Especially for product combinations, T5 \nperforms much better than Zephyr with the given prompt.\nFig. 4  Partial recall entity performance of open-source LLMs. AE, adverse event; PDC, product dose combination; PEC, product event combi-\nnation; PIC, product indication combination\nFig. 5  Partial F1 score and recall results (median and standard devia-\ntion) of entities from stratified cross-validation with fine-tuned T5 \nand fine-tuned Zephyr models. AE, adverse event; PDC, product dose \ncombination; PEC, product event combination; PIC, product indica-\ntion combination\n297\nLLM Performance and Reproducibility in Named Entity Recognition in Controlled Environments\n3.3.3  Comparison of Fine‑Tuned T5 with Few‑Shot \nPre‑trained and Fine‑Tuned Zephyr\nFigure  6 shows the partial F1 score of few-shot experi-\nments for seven selected entities by comparing the zero-\nshot Zephyr model with fine-tuned Zephyr and fine-tuned \nT5 models. Please note that zero-shot experiments were exe-\ncuted once (n = 1), while each model of the cross-validated \nfine-tuned model types was tested (n  = 5). The fine-tuned \nZephyr model shows slightly higher F1 scores compared to \nthe zero-shot Zephyr model and when referring to the non-\noverlapping standard deviations, the performance of T5 may \nbe considered as significantly higher than with fine-tuned \nZephyr. Independent of any possible prompt optimization, \nwe assume the reason for the better performance observed \nmay originate from the fact that in contrast to fine-tuned \nZephyr all T5 weights were adjusted by fine-tuning. Please \nnote that we did not execute a complete adjustment of all \nZephyr weights since the efforts and computation costs \nwould be significantly increased due to the vast differences \nin model parameters (T5 Base: 220 million parameters ver-\nsus 7 billion parameters).\nIn Figs. 7 and 8 the median partial F1 scores and partial \nrecall of all zero- and few-shot experiments are displayed. \nFigures 7 and 8 demonstrate that the scores for non-product \ncombination entities of GPT 4 and T5 are similar, while the \nFig. 6  Few-shot median partial F1 score of fine-tuned T5 and Zephyr compared with zero-shot Zephyr. Standard deviation is shown for all fine-\ntuned models. AE, adverse event; PDC, product dose combination; PEC, product event combination; PIC, product indication combination\nFig. 7  Comparison of median partial F1 scores of fine-tuned T5 and Zephyr versus zero-shot Zephyr, GPT 3.5, and GPT 4. AE, adverse event; \nPDC, product dose combination; PEC, product event combination; PIC, product indication combination; zephyr-FT, fine-tuned Zephyr-7b-beta\n298 J. Dietrich, A. Hollstein \nscores for product combination entities of T5 are signifi-\ncantly higher compared to all other models. The F1 score \nof AE detection with GPT 4 is significantly higher than \nwith GPT 3.5. The recall of the fine-tuned Zephyr model \nwith regard to AE, drug/device, and dose seems to perform \nslightly better than the scores of other models. Comparing \nzero-shot Zephyr with GPT 3.5, the Zephyr AE F1 and recall \nperformance is better. Figure 8 demonstrates that the recall \nperformance can be increased even by a small number of \nsamples (n = 8) in few-shot learning.\n4  Discussion\n4.1  Selection of the Best Performing Open‑Source \nLLM\nIn section 3.3.1 we investigated the performance of selected \nLLM models [53–55] in zero-shot experiments. We selected \nmodels with seven and thirteen billion parameters which \nallows to run and fine-tune those models with reasonable \ntechnical efforts and costs. We could demonstrate that a \nhigher parameter count (number of model parameters) does \nnot necessarily imply a better performance for a given NER \ntask.\nParameter count is an obvious but alluring proxy to pre-\ndict model performance on unseen tasks, which can be sum-\nmarized as the “bigger is better” heuristic [56]. This heu-\nristic holds badly if applied too broadly but can be a good \nguiding principle when applied in a narrower sense where \none focuses on models that are based on the same general \narchitecture, similar training data, similar training tasks, as \nwell as objective functions.\nWe demonstrated that model parameter count is not a \nsuitable proxy metric for downstream task performance. \nAs shown in this paper, by considering our study frame-\nwork as prompts and text generation techniques, the LLMs \ninvestigated were not able to outperform a relatively small \nmodel by today's standards, such as T5 for the task dis-\ncussed here. This only considers task performance; once \none factors in that smaller models greatly simplify the \ncomplexity and effort to develop and maintain the overall \nsystem, the benefit of small local models starts to become \nmore and more significant in many industry-relevant \nsettings.\nOne example of this observation is that a popular open \nmodel like LLAMA-2 13B can be outperformed by smaller \nmodels based on similar architecture when the training data \nis chosen carefully. Especially instruction tuning, such as for \nZephyr-7b-beta, can efficiently bridge the gap in model size.\nFigure 3 shows two models with a similar architecture, \nsuch as Llama-2-7B-32K-Instruct and Llama-2-13b-hf, but \ndifferent pre-training data, where the smaller model outper-\nforms the bigger one, but the bigger one was not trained \nwith instruction tuning using reinforcement learning with \nhuman feedback (RLHF). This highlights that various meth-\nods of instruction tuning can increase model performance \non unseen tasks since they shape the trained model in a way \nthat focuses less on next token or masked token predictions, \nbut more on fulfilling a task that is given to the model in the \nform of text input. Kirstain et al. demonstrated that in open \nquestion answering tasks, enlarging the training set does not \nimprove performance. In contrast, classification, extractive \nquestion answering, and multiple choice tasks benefit so \nmuch from additional examples that collecting a few hun-\ndred examples is often “worth” billions of parameters [57].\nAs shown in Figs.  3 and 4, the Zephyr model performs \nbetter than all other open-source models, and therefore, was \nselected for further investigation (i.e., fine-tuning and few-\nshot inference)\nFig. 8  Comparison of median partial recall of fine-tuned T5 and Zephyr versus zero-shot Zephyr, GPT 3.5, and GPT 4. AE, adverse event; PDC, \nproduct dose combination; PEC, product event combination; PIC, product indication combination; zephyr-FT, fine-tuned Zephyr-7b-beta\n299\nLLM Performance and Reproducibility in Named Entity Recognition in Controlled Environments\nZephyr-7b-beta was derived from a strong baseline LLM \nMistral-7B [58] and was further fine-tuned with the aim of \nfulfilling tasks using direct preference optimization (DPO) \nas a novel and promising fine-tuning approach [59]. We are \nusing in our study a small, but well curated dataset [32]. We \nfine-tuned a small T5 model with this dataset, which shows \nin our study an excellent performance for designated NER \ntasks (see Figs. 6, 7, 8). This is in line with recent publi -\ncations which demonstrates that remarkably strong perfor -\nmance can be achieved from only a handful of examples in \nthe training data [32, 60].\n4.2  Comparison of Zephyr, GPT 3.5, and GPT 4 \nZero‑Shot Performance\nIn comparing the OpenAI models, GPT 4 shows a better \nNER performance in general in Fig.  7, but only the GPT 4 \nAE F1 score is significantly higher than the GPT 3.5 score. \nAs shown in Fig.  8, the better recall performance of GPT \n4 is even more pronounced. Both GPT and Zephyr mod-\nels demonstrate good NER zero-shot performance, while \nZephyr shows a very good recall performance. Zero-shot \nGPT performance of named entity linking with product com-\nbination entities could be improved by few-shot learning as \nshown in relation extraction experiments [27]. AE detection \nis considered as a more difficult task for LLMs compared \nwith other entities used in this study (as drug, dose, or indi-\ncation) [32]. For complex NER tasks, the significant perfor-\nmance increase would likely justify the higher GPT 4 costs \nand longer execution time per request compared with GPT \n3.5. For AE detection, zero-shot Zephyr demonstrates better \nperformance than GPT 3.5 and could alternatively be used.\n4.3  Comparison of Best Performing Open‑Source \nLLM (Zero‑Shot, Few‑Shot, Fine‑Tuned) \nwith Fine‑Tuned T5, and Zero‑Shot GPT\nWith the given prompt, we observed similar F1 perfor -\nmances of zero-shot GPT 4 and fine-tuned T5 for non-\nproduct combination entities, while the product combina -\ntion scores of T5 are significantly higher compared with \nall other models. The results imply that compared with a \nsmall T5-Base with complete weight fine-tuning perfor -\nmance, QLoRA fine-tuned models could not achieve com-\nparable performance especially with regard to production \ncombination entities. To improve non-T5 models, further \nefforts in prompt engineering are possible (e.g., few-shot \ntechniques or prompt optimization). All quantitative results \nregarding model performance are lower bounds and must \nbe interpreted as such. They do not inform about a poten-\ntial performance ceiling that could be achieved with more \nefforts on prompting or fine-tuning. Zero-shot experiments \nwith minimal prompting efforts are an adequate measure \nto predict how well a model will work with minimal effort.\nIn Figs.  7 and 8  we found that the F1 performances of \nzero-shot, few-shot, and fine-tuned Zephyr model show simi-\nlar results, but the recall performance of fine-tuned Zephyr \nimproved significantly. We demonstrated that a zero-shot \nZephyr model performance could be increased by fine-\ntuning and few-shot learning, even by using a very small \nsample (n = 8).\nThe same data were used for fine-tuning T5 and fine-\ntuning Zephyr. When comparing fine-tuned Zephyr with T5, \nT5 had a better F1 performance, especially with product \ncombination entities.\nAll approaches used in this paper offer potential for addi-\ntional gains in task performance, and a balance needs to \nbe found between time spent on specific approaches. From \nthe experiments we performed, we expect that few-shot \napproaches offer the potential for additional performance \ngains with relatively little additional investment, which \ncould be addressed in future research.\nThe technical content of this study is concerned with \nnamed entity recognition and linking (i.e., product combi-\nnation entities), but the conclusions drawn here may hold \nin principle for many more use cases in the text modality, \nwhich is due to the new paradigm opened by LLMs. Tradi-\ntionally, different use cases (e.g., sentiment classification, \nquestion answering) were approached using different meth-\nods, but LLMs allow casting any use case that operates in a \nmodality covered by a model as a text generation task based \non a prompt and input data.\n4.4  Proposal for Use of LLMs in a Regulated \nEnvironment\nTranslating such results into qualified applications running \nas regulated systems will prove challenging, and several \nhurdles are needed to be overcome. One such hurdle is the \nreproducibility of model inference, which is discussed in this \npaper. Reproducibility is a cornerstone of a computer vali-\ndation process [21–24], as it allows to independently verify \nand validate the documented findings of previous validation \nruns. In our study we investigated the overall performance \nand reproducibility capability of the responses produced by \nLLMs, regarding NER conducted upon PV texts. We demon-\nstrated in section 3.2 that in contrast to a fine-tuned Zephyr, \nrecent OpenAI model implementations failed in showing \nreproducible results, which is a prerequisite for computer \nsystem validation to demonstrate that a system is suitable \nfor its intended purpose.\nOur results clearly show that this point needs special \nattention, which might include and is not limited to legal \nefforts and technical considerations. Given the time at which \nthe experiments were done and presented here, it is evident \n300 J. Dietrich, A. Hollstein \nthat the current level of publicly available GPT 4 and 3.5 \nmodels do not provide the level of reproducibility needed \nin these contexts.\nDepending on the provider of an external LLM, one might \nalso need to consider the additional need to keep a validated \nmodel in a controlled state and available over longer time \nperiods. Additional documentation of the training data and \ninference logic might also be useful for developing suffi-\ncient levels of trust in the model's performance on new and \nunseen data.\nWhen models are running internally, one can control \nhardware as well as inference code and can technically \nensure reproducibility. Open models [61] can additionally \noffer better understanding in terms of training data and pro-\ncedures used for model training and offer paths towards the \nexplainability of specific model inferences.\nRunning inference locally on LLMs is demanding in \nterms of hardware requirements such as CPU inference \nspeed or available GPU memory. Today’s data centers and \ncloud providers offer the capability to run LLMs from a few \nbillion to much larger number of parameters, but the com -\nplexity and costs increase steeply with the number of model \nparameters. Currently, models in the range of 7B to 13B \nare in a reasonable spot of potentially sufficient capabili-\nties and simplicity of operation, system development, and \nexperimentation.\nDepending on the task and available experience, even in \nsmaller organizations, one can be confident that open models \nmay be the most reasonable choice for many tasks. This is \nespecially true when dedicated domain-relevant prior knowl-\nedge is deployed within the overall system of model infer -\nence and additional components.\nOut-of-the-box and without changing prompts, smaller \nopen models show a big capability gap compared with state-\nof-the-art hosted models such as GPT 4 and GPT 3.5. Our \nanalysis clearly showed that for this task, the gap can be \nclosed using guided generation and fine-tuning with mod-\nest amounts of training data. Creating this training data are \ncostly and need to be factored in when developing AI sys-\ntems. However, using hosted models does not mean one can \nsafely develop a system without the generation of training \ndata, since it is needed to evaluate and monitor model per -\nformance. Data-efficient training regimes emphasize that \nthe combination of open and smaller models with dedicated \ntask-specific data can reach and outperform closed models’ \nperformance.\nGenerating training data requires domain expertise, \nadherence to well-described labeling protocols, and meas-\nures for quality control like inter-annotator agreement. Time \nand cost requirements of generating high-quality data can \npotentially be reduced by using the best available hosted \nmodel to generate data candidates, which are mostly quality \ncontrolled by domain experts. Our findings and those of \nother publications clearly demonstrate that LLMs possess \nthe capability to accomplish this task [27, 29, 30]. This \napproach shows that initially opposing ideas of open versus \nclosed and internal versus external models are not that rel-\nevant for many practical applications.\n5  Conclusion\nLLM-based AI systems have great innovative potential for \napplications in the pharmaceutical industry. Our case study \nshows that, notwithstanding the current rate of progress, \ngeneral rules of system implementation in a controlled \nenvironment have not lost their importance. At the center \nof developing a pharmacovigilance system, efficacy (i.e., \nsystem performance metrics), reproducibility of results, and \noperability (i.e., to remain a validated system in a controlled \nstatus) should be the focus. For system deployment a robust \ntechnical and legal guardrail framework needs to ensure that \nmodels remain in a controlled status and provide availability \nover time, as well as reproducible model inference, as mini-\nmum criteria for them to be deployed operationally. These \nissues aside, they could be used to greatly reduce the effort \nto generate proposals for fine-tuning data or automated sys-\ntem evaluation and monitoring. This is clear from the fact \nthat GPT 3.5 and 4 showed promising results with limited \nprompting efforts in zero-shot settings. Hence, their results \ncould replace annotating efforts to generate training data \nneeded to train bespoke models like T5.\nHere we took the traditional route of using data labeled \nby domain experts to ensure high data quality, as it is most \nsuitable for a regulated environment. We were unable to \nreach sufficient levels of reproducible model inference with \nGPT 3.5 and GPT 4 models in contrast to locally maintained \nopen-source models, which implies that for the current GPT \nversions an independent verification in computer system \nvalidation of previous validation runs is hardly to achieve.\nSmaller model sizes offer great advantages in effort and \ncomplexity when developing and maintaining an LLM-\nbased AI system. It seems plausible that model size and \nthe size of fine-tuning data could be traded off to reach the \nrequired levels of system performance. However, we have \nnot shown this explicitly here.\nBased on this case study, we recommend using the model \nwhich performs best for NER and relation extraction (e.g., \nGPT), irrespective of reproducibility to generate proposals \nfor data annotation or to generate synthetic data for fine-\ntuning. Then, one should fine-tune, evaluate, and deploy the \nmost operationally feasible LLM that reaches the required \nlevel of performance on the task to find the optimal balance \n301\nLLM Performance and Reproducibility in Named Entity Recognition in Controlled Environments\nof complexity, stability, and quality for the overall system in \na regulated environment.\nThe key contributions of our paper to the research com-\nmunity are as follows:\n• GPT 3.5 and GPT 4, as of today, should not be used as a \npart of a GxP validated system due to missing guarantees \nof reproducibility.\n• Due to promising results of Zephyr or GPT in terms of \nNER tasks tested in our study it seems in many relevant \ncases to be sufficient to use only zero-shot experiments to \ngenerate training data for further manual review.\n• This training data could be used to train a smaller, e.g., T5 \nBase system. This would be a straightforward process to \ntrain a high-quality and resource-efficient model, which \ncould be used as a part of a GxP system.\nAcknowledgements We would like to thank, in alphabetic order, \nPhilipp Kazzer, Katrin Manlik, Nikola Milosevic, John Pietsch, The-\nresa Schmitt, and Angelo Ziletti for constructive comments.\nDeclarations \nFunding Not applicable.\nConflict of interest Jürgen Dietrich holds shares in Bayer AG. Jürgen \nDietrich is a full-time employee of Bayer AG. André Hollstein holds \nshares in Bayer AG. André Hollstein is a full-time employee of Bayer \nAG. Jürgen Dietrich and André Hollstein have no conflict of interest \nthat are directly relevant to the content of this experiment. The views \nexpressed in this paper are those of the authors and do not necessarily \nreflect the official policy or position of Bayer AG.\nEthics approval Not applicable.\nConsent to participate Not applicable.\nConsent for publication Not applicable.\nAvailability of data and material The dataset used is publicly avail -\nable in publication [32] under Supplementary file 1,https:// static- conte \nnt. sprin ger. com/ esm/ art% 3A10. 1007% 2Fs40 264- 023- 01322-3/ Media \nObjec ts/ 40264_ 2023_ 1322_ MOESM1_ ESM. xlsx\nCode availability The code used for this experiment is not provided, \nas a patent is pending.\nAuthors’ contributions Jürgen Dietrich and André Hollstein were \ninvolved in the conception and design of the experiments and result \ninterpretation. André Hollstein was responsible for code generation \nof guidance AI implementation and the execution of few-shot experi-\nments, Jürgen Dietrich was responsible for the rest of the code genera-\ntion tasks and the execution of cross-validation and LLM finetuning \nexperiments. All authors contributed to the interpretation of the data \nanalysis results and assisted with the concept and draft revisions of the \nmanuscript. All authors reviewed and approved the final manuscript \nand accept full responsibility for its overall content.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution-NonCommercial 4.0 International License, which permits any \nnon-commercial use, sharing, adaptation, distribution and reproduction \nin any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other \nthird party material in this article are included in the article’s Creative \nCommons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons \nlicence and your intended use is not permitted by statutory regula-\ntion or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit \nhttp://creativecommons.org/licenses/by-nc/4.0/.\nReferences\n 1. Mak K-K, Wong Y-H, Pichika MR. Artificial intelligence in \ndrug discovery and development. Drug Discov Eval Saf Phar -\nmacokinet Assays. 2023:1–38.\n 2. Jiménez-Luna J, Grisoni F, Weskamp N, Schneider G. Artifi-\ncial intelligence in drug discovery: recent advances and future \nperspectives. Expert Opin Drug Discov. 2021;16(9):949–59.\n 3. Nasnodkar S, Cinar B, Ness S. Artificial intelligence in toxicol-\nogy and pharmacology. J Eng Res Rep. 2023;25(7):192–206.\n 4. Harrer S, Shah P, Antony B, Hu J. Artificial intelligence for \nclinical trial design. Trends Pharmacol Sci. 2019;40(8):577–91.\n 5. Angus DC. Randomized clinical trials of artificial intelligence. \nJAMA. 2020;323(11):1043–5.\n 6. Rathore AS, Nikita S, Thakur G, Mishra S. Artificial intelli-\ngence and machine learning applications in biopharmaceutical \nmanufacturing. Trends Biotechnol. 2023;41(4):497–510.\n 7. Abatemarco D, Perera S, Bao SH, Desai S, Assuncao B, \nTetarenko N, et al. Training augmented intelligent capabili-\nties for pharmacovigilance: applying deep-learning approaches \nto individual case safety report processing. Pharm Med. \n2018;32:391–401.\n 8. Wang H, Ding YJ, Luo Y. Future of ChatGPT in pharmacovigi-\nlance. Drug Saf. 2023;46(8):711–3.\n 9. Ethics and governance of artificial intelligence for health—\nwho guide. 2021. https:// iris. who. int/ bitst ream/ handle/ 10665/ \n341996/ 97892 40029 200- eng. pdf. Accessed 29 July 2024.\n 10. Schmider J, Kumar K, LaForest C, Swankoski B, Naim K, \nCaubel PM. Innovation in pharmacovigilance: use of artificial \nintelligence in adverse event case processing. Clin Pharmacol \nTher. 2019;105(4):954–61.\n 11. Ghosh R, Kempf D, Pufko A, Barrios Martinez LF, Davis CM, \nSethi S. Automation opportunities in pharmacovigilance: an \nindustry survey. Pharm Med. 2020;34:7–18.\n 12. Kassekert R, Grabowski N, Lorenz D, Schaffer C, Kempf D, Roy \nP, et al. Industry perspective on artificial intelligence/machine \nlearning in pharmacovigilance. Drug Saf. 2022;45(5):439–48.\n 13. Proposed regulatory framework for modifications to artificial \nintelligence/machine learning (AI/ML)-based software as a \nmedical device (SaMD)—discussion paper and request for \nFeedbac. 2019. https:// www. fda. gov/ media/ 122535/ downl oad. \nAccessed 26 Feb 2024.\n 14. Informal innovation network horizon scanning assessment \nreport—artificial intelligence. 2021. https:// www. icmra. info/  \ndrupal/ sites/ defau lt/ files/ 2021- 08/ horiz on_ scann ing_ report_ \nartifi  cial_ intel ligen ce. pdf. Accessed 29 July 2024.\n302 J. Dietrich, A. Hollstein \n 15. Medical device software: considerations for device and risk \ncharacterization, IMDRF/SaMD WG/N81 DRAFT: 2024. \n2024. https:// www. imdrf. org/ sites/ defau lt/ files/ 2024- 02/ IMDRF \nSaMD% 20WGN 81% 20DRA FT% 202024% 2C% 20Med ical% \n20Dev ice% 20Sof tware% 20Con sider ations% 20for% 20Dev ice% \n20and% 20Risk% 20Cha racte rizat ion% 20-% 20fin al% 20dra ft. pdf. \nAccessed 29 June 2024.\n 16. Reflection paper on the use of Artificial Intelligence (AI) in \nthe medicinal product lifecycle (Draft). 2023. https:// www. ema. \neuropa. eu/ en/ docum ents/ scien tific- guide line/ draft- refle ction- \npaper- use- artifi  cial- intel ligen ce- ai- medic inal- produ ct- lifec ycle_ \nen. pdf. Accessed 29 July 2024.\n 17. Huysentruyt K, Kjoersvik O, Dobracki P, Savage E, Mishalov \nE, Cherry M, et al. Validating intelligent automation systems \nin pharmacovigilance: insights from good manufacturing prac-\ntices. Drug Saf. 2021;44:261–72.\n 18. Developing a software precertification program: a working \nmodel v1.0 2019. https:// www. fda. gov/ media/ 119722/ downl  \noad. Accessed 26 Feb 2024.\n 19. Kompa B, Hakim JB, Palepu A, Kompa KG, Smith M, Bain PA, \net al. Artificial intelligence based on machine learning in pharma-\ncovigilance: a scoping review. Drug Saf. 2022;45(5):477–91.\n 20. Bate A, Stegmann J-U. Artificial intelligence and pharmacovigi-\nlance: what is happening, what could happen and what should hap-\npen? Health Policy Technol. 2023;12(2): 100743.\n 21. Guideline on process validation for finished products—information \nand data to be provided in regulatory submissions. 2016. https:// \nwww. ema. europa. eu/ en/ docum ents/ scien tific- guide line/ guide line- \nproce ss- valid ation- finis hed- produ cts- infor mation- and- data- be- provi \nded- regul atory- submi ssions- revis ion-1_ en. pdf. Accessed 01 Feb \n2024.\n 22. Albertoni R, Colantonio S, Skrzypczyński P, Stefanowski J. Repro-\nducibility of machine learning: Terminology, recommendations and \nopen issues. 2023. arXiv:230212691.\n 23. Guidance for Industry Process Validation: General Principles and \nPractices. 2011. https:// www. fda. gov/ files/ drugs/ publi shed/ Proce ss- \nValid ation-- Gener al- Princ iples- and- Pract ices. pdf. Accessed 01 Feb \n2024.\n 24. Katz P, Campbell C. FDA 2011 process validation guidance: process \nvalidation revisited. J Valid Technol. 2012;18(4):33.\n 25. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. \nExploring the limits of transfer learning with a unified text-to-text \ntransformer. J Mach Learn Res. 2020;21(140):1–67.\n 26. Hugging Face T5 V4.24.0. https:// www. huggi ngface. co/ docs/ trans \nforme rs/ model_ doc/ t5# trans forme rs. T5Mod el. Accessed 10 Nov \n2023.\n 27. Wadhwa S, Amir S, Wallace BC. Revisiting relation extraction in \nthe era of large language models. In: Proceedings of the conference \nassociation for computational linguistics meeting; 2023: NIH public \naccess; 2023. p. 15566.\n 28. Gurulingappa H, Rajput AM, Roberts A, Fluck J, Hofmann-Apitius \nM, Toldo L. Development of a benchmark corpus to support the \nautomatic extraction of drug-related adverse effects from medical \ncase reports. J Biomed Inform. 2012;45(5):885–92.\n 29. Hu Y, Chen Q, Du J, Peng X, Keloth VK, Zuo X, et al. Improv -\ning large language models for clinical named entity recognition via \nprompt engineering. J Am Med Inform Assoc. 2024.\n 30. Li Y, Li J, He J, Tao C. AE-GPT: using large language models \nto extract adverse events from surveillance reports-a use case \nwith influenza vaccine adverse events. PLoS ONE. 2024;19(3): \ne0300919.\n 31. Raval S, Sedghamiz H, Santus E, Alhanai T, Ghassemi M, Chersoni \nE. Exploring a unified sequence-to-sequence transformer for medical \nproduct safety monitoring in social media. 2021 November; Punta \nCana, Dominican Republic: Association for Computational Linguis-\ntics; 2021. pp. 3534–46.\n 32. Dietrich J, Kazzer P. Provision and characterization of a corpus \nfor pharmaceutical, biomedical named entity recognition for phar-\nmacovigilance: evaluation of language registers and training data \nsufficiency. Drug Saf. 2023;46(8):765–79. https:// doi. org/ 10. 1007/ \ns40264- 023- 01322-3.\n 33. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, \net al. Language models are few-shot learners. Adv Neural Inf Pro-\ncess Syst. 2020;33:1877–901.\n 34. Lu Y, Bartolo M, Moore A, Riedel S, Stenetorp P. Fantastically \nordered prompts and where to find them: overcoming few-shot \nprompt order sensitivity. 2021. arXiv:210408786.\n 35. Rubin O, Herzig J, Berant J. Learning to retrieve prompts for in-\ncontext learning. 2021. arXiv:211208633.\n 36. Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, \net al. Gpt-4 technical report. 2023. arXiv:230308774.\n 37. OpenAI Platform. https:// platf orm. openai. com/ docs/ models. \nAccessed 05 Dec 2023.\n 38. Notus 7B v1. https:// huggi ngface. co/ argil la/ notus- 7b- v1. Accessed \n15 Oct 2023.\n 39. Meditron-7B-v1.0. https:// huggi ngface. co/ epfl- llm/ medit ron- 7b. \nAccessed 15 Oct 2023.\n 40. Zephyr 7B β. https:// huggi ngface. co/ Huggi ngFac eH4/ zephyr- 7b- \nbeta. Accessed 15 Oct 2023.\n 41. Llama 2 13B. https:// huggi ngface. co/ meta- llama/ Llama-2- 13b- hf. \nAccessed 15 Oct 23.\n 42. Llama-2-7B-32K-Instruct. https:// huggi ngface. co/ toget herco mputer/ \nLlama-2- 7B- 32K- Instr uct. Accessed 15 Oct 2023.\n 43. Sarker A, Belousov M, Friedrichs J, Hakala K, Kiritchenko S, Mehr-\nyary F, et al. Data and systems for medication-related text classifi-\ncation and concept normalization from Twitter: insights from the \nSocial Media Mining for Health (SMM4H)-2017 shared task. J Am \nMed Inform Assoc. 2018;25(10):1274–83.\n 44. New drug approvals archive for 2010–2021. https:// www. drugs. com/ \nnewdr ugs- archi ve/ 2021. html. Accessed 30 Sept 2021\n 45. Schulhoff S, Ilie M, Balepur N, Kahadze K, Liu A, Si C, et al. \nThe prompt report: a systematic survey of prompting techniques. \narXiv:240606608. 2024.\n 46. Prompt engineering guide. https:// www. promp  tingg uide. ai/. \nAccessed 15 June 2024.\n 47. Anadkat S. How to make your completions outputs consistent with \nthe new seed parameter. https:// cookb ook. openai. com/ examp les/ \nrepro ducib le_ outpu ts_ with_ the_ seed_ param eter. Accessed 01 Mar \n2024.\n 48. Wolf EBaCFaNHaSHaNLaNRaOSaLTaT. Open LLM Leaderboard. \n2023. https:// huggi ngface. co/ spaces/ Huggi ngFac eH4/ open_ llm_ \nleade rboard. Accessed 05 Dec 2023.\n 49. Rebedea T, Dinu R, Sreedhar MN, Parisien C, Cohen J. NeMo \nguardrails: a toolkit for controllable and safe LLM applications \nwith programmable rails. In: Proceedings of the 2023 conference \non empirical methods in natural language processing: system dem-\nonstrations; 2023; 2023. pp. 431–45.\n 50. A guidance language for controlling large language models. https:// \ngithub. com/ guida nce- ai/ guida nce. Accessed 01 Mar 2024.\n 51. Dettmers T, Pagnoni A, Holtzman A, Zettlemoyer L. QLoRA: effi-\ncient finetuning of quantized llms. Adv Neural Inf Process Syst. \n2024;36.\n 52. X—LLM: cutting edge & easy LLM finetuning. https:// github. com/ \nBobaZ ooba/ xllm. Accessed 20 Dec 2023.\n 53. Taori R, Gulrajani I, Zhang T, Dubois Y, Li X, Guestrin C, et al. \nAlpaca: a strong, replicable instruction-following model. Stanford \nCenter Res Found Models. 2023;3(6):7.\n303\nLLM Performance and Reproducibility in Named Entity Recognition in Controlled Environments\n 54. Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, \net al. Llama 2: open foundation and fine-tuned chat models. 2023. \narXiv:230709288.\n 55. Tunstall L, Beeching E, Lambert N, Rajani N, Rasul K, Belkada \nY, et  al. Zephyr: direct distillation of lm alignment. 2023. \narXiv:231016944.\n 56. Kaplan J, McCandlish S, Henighan T, Brown TB, Chess B, \nChild R, et al. Scaling laws for neural language models. 2020. \narXiv:200108361.\n 57. Kirstain Y, Lewis P, Riedel S, Levy O. A few more examples may \nbe worth billions of parameters. 2021. arXiv:211004374.\n 58. Jiang AQ, Sablayrolles A, Mensch A, Bamford C, Chaplot DS, \nCasas Ddl, et al. Mistral 7B. 2023. arXiv:231006825.\n 59. Rafailov R, Sharma A, Mitchell E, Manning CD, Ermon S, Finn C. \nDirect preference optimization: your language model is secretly a \nreward model. Adv Neural Inf Process Syst. 2024;36.\n 60. Zhou C, Liu P, Xu P, Iyer S, Sun J, Mao Y, et al. Lima: Less is more \nfor alignment. Adv Neural Inf Process Syst. 2024;36.\n 61. Kapoor S, Bommasani R, Klyman K, Longpre S, Ramaswami A, \nCihon P, et al. On the societal impact of open foundation models. \n2024."
}