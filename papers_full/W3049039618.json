{
  "title": "Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition",
  "url": "https://openalex.org/W3049039618",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5056054750",
      "name": "Henry Tsai",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5069722453",
      "name": "Jayden Ooi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5030156442",
      "name": "Chun-Sung Ferng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5051828575",
      "name": "Hyung Won Chung",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015547427",
      "name": "Jason Riesa",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3177265267",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2962847160",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2970557265",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2964212578",
    "https://openalex.org/W2964217527",
    "https://openalex.org/W2581624817",
    "https://openalex.org/W2764043458",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963453233",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3011650341",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Transformer-based models have achieved stateof-the-art results in many tasks in natural language processing. However, such models are usually slow at inference time, making deployment difficult. In this paper, we develop an efficient algorithm to search for fast models while maintaining model quality. We describe a novel approach to decompose the Transformer architecture into smaller components, and propose a sampling-based one-shot architecture search method to find an optimal model for inference. The model search process is more efficient than alternatives, adding only a small overhead to training time. By applying our methods to BERT-base architectures, we achieve 10% to 30% speedup for pre-trained BERT and 70% speedup on top of a previous state-of-the-art distilled BERT model on Cloud TPU-v2 with a generally acceptable drop in performance.",
  "full_text": "Finding Fast Transformers: One-Shot Neural Architecture Search by\nComponent Composition\nHenry Tsai, Jayden Ooi, Chun-Sung Ferng, Hyung Won Chung, Jason Riesa\nGoogle Research\n{henrytsai, jayden, csferng, hwchung, riesa}@google.com\nAbstract\nTransformer-based models have achieved state-\nof-the-art results in many tasks in natural lan-\nguage processing. However, such models are\nusually slow at inference time, making deploy-\nment difﬁcult. In this paper, we develop an efﬁ-\ncient algorithm to search for fast models while\nmaintaining model quality. We describe a novel\napproach to decompose the Transformer archi-\ntecture into smaller components, and propose\na sampling-based one-shot architecture search\nmethod to ﬁnd an optimal model for inference.\nThe model search process is more efﬁcient than\nalternatives, adding only a small overhead to\ntraining time. By applying our methods to\nBERT-base architectures, we achieve 10% to\n30% speedup for pre-trained BERT and 70%\nspeedup on top of a previous state-of-the-art\ndistilled BERT model on Cloud TPU-v2 with a\ngenerally acceptable drop in performance.\n1 Introduction\nDeep residual models like the Transformer\n(Vaswani et al., 2017) have achieved state-of-the-\nart results on many tasks. However, the most accu-\nrate models are usually slow at inference time, mak-\ning real-world deployment prohibitive for many\napplications.\nIn this paper we describe a novel approach to\nﬁnding the optimal architecture for Transformer\nnetworks, optimizing for inference time and main-\ntaining accuracy. The ﬁnal model can be trained\nfrom scratch or used in conjunction with techniques\nlike distillation (Hinton et al., 2015).\nWe propose a component-wise network selection\napproach for Transformer-based networks. We use\nBERT (Devlin et al., 2018) as a running example\nto show how to construct components based on the\nneed for high-speed models and formulate an ob-\njective that is directly relevant to inference speed.\nThen, we propose a simple sampling-based model\nselection algorithm that automatically selects hy-\nperparameters in one-shot. Our contributions are\nas follows:\n•We propose a novel re-parameterization of the\nTransformer which enables us to search the\ndepth and width of the model at the same time.\n•We design an objective to integrate external\ncomputation proﬁling information and formu-\nlate the objective to directly optimize for high-\nspeed models.\n•We propose a sampling-based algorithm for\none-shot model selection that yields state-\nof-the-art results while adding little memory\noverhead and small (1.4x) training time over-\nhead.\nWe evaluate our methods on a wide variety of tasks\nand show how fast model architectures can vary for\ndifferent tasks.\n2 Background and Related Work\n2.1 Smaller Models\nSmaller models do not always mean faster models:\nthe total number of parameters does not necessarily\ncorrelate with the amount of computation needed.\nFor example, the softmax operation itself is not\nassigned parameters but can be expensive when\nthe output dimension is large; the embedding di-\nmension can be large, but the embedding-lookup\noperation can be light-weight if the operation is\noptimized.\nA signiﬁcant amount of previous work is con-\ncerned with model-size minimization. There are\nseveral typical approaches, and we brieﬂy examine\nhow each contributes to model inference speedup.\nMost work described below is orthogonal to this\nwork, and as a result can be applied on top of the\nmethods we introduce in this paper.\narXiv:2008.06808v1  [cs.LG]  15 Aug 2020\nQuantization Previous work has shown that in-\nstead of using 32-bit ﬂoats to store the weights,\nmodels can be quantized to 8-bit or even 4-bit ﬂoat-\ning point numbers of integers without much accu-\nracy loss (Zafrir et al., 2019; Fan et al., 2020). By\ndoing that, one can easily obtain a model that is 4x\nsmaller or more. Quantized models may run faster\non hardware that support quantized arithmetic (Ja-\ncob et al., 2018).\nSparsity Model sparsity, or zeroing-out model\nparameters, is another common minimization ap-\nproach (Zhu and Gupta, 2017); zero-weights typi-\ncally do not need to be stored. However, keeping\ntrack of sparse matrices may add additional com-\nputational overhead. Usually speedups are only\nobserved if the model is very sparse (e.g. 90%\nsparsity) and is running on hardware that supports\nsparse operations well.1 Alternatively, structured\nsparsity (Gordon et al., 2017) which adds con-\nstraints to introduce sparsity on each tensor row\nmay help achieve better speedup since row prun-\ning reduces the dimensionality of the tensors. One\nproblem with such methods is that it is limited by\nthe existing network architecture.\nParameter Sharing We can obtain smaller mod-\nels by sharing parameters across layers (Lan et al.,\n2020). However, resulting models are typically\nnot much faster because computation is not shared.\nIn addition, a larger architecture may be required,\naffecting the computation graph, in order to com-\npete in tasks with models that have no parameter\nsharing.\n2.2 Models with Fewer FLOPs\nThere are many existing works that try to minimize\nFLOPs2 (Gordon et al., 2017). However, fewer\nFLOPs do not always mean faster speed. A model\ncan have increased FLOPs but still run faster at\ninference time because it uses the computational\nhardware more effectively.\n2.3 One-Shot Neural Architecture Search\nThere are many ways to speed up the traditionally\nslow neural architecture search process (Elsken\net al., 2019). One recent focus is one-shot search.\nIn such methods, only one model is trained, and the\nﬁnal model is just a sub-network of the one-shot\nmodel.\n1For example, Intel’s sparse matrix kernel library.\n2ﬂoating-point operations per second\nWe can think of a neural network as a directed\nacyclic graph with different functions on the edges\nof the graph. Given nnodes and k sub-network\ncandidates for each edge, including dropout of the\nedge connection itself, we can search all O(n2k)\ncombinations to ﬁnd the optimal model. Doing one-\nshot search means that one needs to search all sub-\nnetwork candidates at the same time. As a result,\nsuch methods usually use a lot of memory to store\nthe weights of all candidates, and applying them\non state-of-the-art large networks can be difﬁcult.\nThere are two categories of algorithms to search\nthe combinations: direct pruning methods (Gor-\ndon et al., 2017) and sampling-based methods\n(Shazeer et al., 2017), including reinforcement-\nlearning methods (Xie et al., 2019). Regularizers\ncan be added to the network to make the ﬁnal model\nhaving certain properties. Many works are pro-\nposed in both categories, but there are few works\nthat compare them directly. To our knowledge, this\nis the ﬁrst work to compare the two in a controlled\nsetting.\n3 Architecture Search Space\nThe standard BERT model architecture consists\nof a series of Transformer blocks, each contain-\ning a multi-headed attention followed by a 2-layer\nposition-wise feedforward block. There is a resid-\nual connection around each attention and feed-\nforward block, after which the output is passed\nthrough layer normalization.\nBased on proﬁling results, we have identiﬁed the\nfollowing network components and hyperparame-\nters as having substantial impact on inference efﬁ-\nciency, and design our search space around them:\n•Attention query key and value dimensions\n•Width and depth of feedforward layers\n•Number of attention heads\n•Layer normalization mean computation\nWe do not require each Transformer block to\nshare the same structure, as required by some pre-\nvious work (So et al., 2019). Unlike the standard\nTransformer, we explore dimensions for feedfor-\nward and attention key-value, independent of the\nhidden layer size and the number of attention heads.\nTo manage the search space due to combinatorial\nexplosion, we formulate the model as a sequence of\nfunction compositions and represent choices of the\nhyperparameters as searching the composition of\nsmaller components to avoid searching all possible\nsizes. By allowing components to share weights as\nmodel architecture changes, we can search many ar-\nchitectures without retraining and with little mem-\nory overhead.\n3.1 Network Component\nThis section deﬁnes the key network components\nand the corresponding hyperparameters that are\ncrucial for constructing our search space. In or-\nder to search for different component dimensions,\nwe derive the decomposition for each component\nthat enables the search algorithms to optimize for\ndifferent dimensions later.\nWe use component to refer to any learnable func-\ntion or sub-network. To simplify the notations, we\nonly describe the component type and omit the\ndifferent learnable parameters. Let X denotes the\ninput with dimension [ℓ,h], corresponding to the\nsequence length and hidden layer size respectively.\nFeedforward Network The position-wise feed-\nforward network in a Transformer block has2 fully-\nconnected layers. Both input and output size are\nﬁxed to h, while the intermediate dimension dis\nﬂexible. Let FFddenote a 2-layer with intermediate\ndimension d. We have the decomposition\nFFd(X) = Denseh(Activation(Densed(X)))\n=\nm∑\ni=1\nDenseh(Activation(Dense d\nm\n(X)))\n=\nm∑\ni=1\nFF d\nm\n(X),\nwhich is a summation of mfeedforward networks\nof size d/meach.\nQuery-Key Similarity Query-key similarity is\nthe core operation in the attention mechanism.\nGiven the component with key dimension dk, we\ncan decompose it into\nSimdk (X) = Densedk (X)(Densedk (X))T\n=\nm∑\ni=1\nSimdk\nm\n(X),\nwhere each of the mparts just have a smaller key\ndimension of dk/m.\nMulti-Head Attention Let Atta,dk,dv denote an\na-head self-attention with key and value dimension\nof dk, dv respectively. As multi-head attention is\nthe concatenation of all heads’ output followed by\na linear projection, we can naturally divide it to a\nsummation of single-head attentions.\nAtta,dk,dv (X) =Denseh(Concat(Head1,..., Heada))\n=\na∑\ni=1\nAtt1,dk,dv (X)\nSingle-Head Attention Similarly, the attention\nvalue computation of single-head attention can be\nbroken into mequal parts.\nAtt1,dk,dv (X)\n= Denseh\n(\nSoftmax\n(Simdk (X)√dk\n)\n·Densedv (X)\n)\n=\nm∑\ni=1\nAtt1,dk,dv\nm\n(X)\nWe have shown that all components mentioned\nabove can be decomposed into summation of\nm equal parts. Now, for each sub-component\nfi(X), deﬁne a corresponding binary variablewiof\nwhether to keep that sub-component, so component\noutput can be written as ∑\niwifi(X). The selec-\ntion parameters can be optimized by the search al-\ngorithm, and setting any wi to 0 effectively reduces\nthe component dimension (e.g. dfor feedforward,\nand a, dk, dv for attention).\nLayer Normalization Some existing works have\nshown that zero-mean normalization in batch nor-\nmalization is not needed (Shen et al., 2020). We\nexplore the same for layer normalization by replac-\ning the mean with µ′(X) = w·µ(X) conditioned\non a selection parameter w, giving\nLN(X) = α·(X−µ′(X))\nσ + β,\nwhere σ=\n√\n(X−µ′)2/N. The search algorithm\ncan disable zero-mean normalization by assigning\nw= 0.\n3.2 Architecture Connection\nFinding the best layer width and depth for a given\nnetwork size is another different challenge. The\nsearch space consists of exponentially many possi-\nble conﬁgurations that we need to be able to repre-\nsent and optimize on.\nTo achieve that, imagine a sequence of kiden-\ntical components (e.g., FF) to be assembled in a\nnetwork. Each component fi, except the last, can\nFigure 1: Ψ(f2,w2) ◦Ψ(f1,w1). The dashed arrows\nare multiplied by edge weights. When w1 = 0, f1 and\nf2 share the same input, giving a horizontal connection.\nWhen w1 = 1, f2 takes X+ f1(X) + Ras input, thus\nf2 is in the next layer and the connection is vertical.\neither be placed in the same layer as its succes-\nsor (horizontal connection), or in a different layer\n(vertical connection). Similarly, we deﬁne a con-\nnection parameterwi ∈{0,1}to represent these\ntwo choices respectively. Notice that the 2k−1 pos-\nsible choices corresponds exactly with all possible\nlayer conﬁgurations. This view provides a useful\nmean for constructing our search space.\nWe also need an accumulating mechanism that\ncan combine output of all componentsfi(X) in the\nsame layer. This can be implemented in a network\nby passing an accumulated memoryRas additional\ninput across the components.\nTo be able to represent any residual networks,\nit’s crucial to include residual connection in the\nsearch space.\n3.2.1 Connector Unit\nWe deﬁne connector unitΨ as a higher-order func-\ntion that takes a basic component f and connection\nparameter w, and outputs the function\nΨ(f,w)(X,R) =\n(X+ w·(f(X) + R),(1 −w) ·(f(X) + R)).\nOutput of Ψ(f,w) is a tuple to be fed to the next\nconnector unit, as illustrated in Figure 1.\nInput Rcontains the cumulative output for cur-\nrent layer up until current component. When\nw = 0, f(X) is added to Rto continue accumu-\nlating current layer’s output, while X is passed\nthrough unchanged. When w= 1, current layer is\nconcluded by summing the input X, current output\nf(X), and cumulative output Rtogether. A more\ndetailed example can be found in Appendix C.\nIn order to interoperate Ψ(·) with unary func-\ntions, we deﬁne Ω(X,R) = X + R, and with\na slight abuse of notation, let Ψ(f,w)(X) =\nΨ(f,w)(X,0) when only a single input is given.\n3.2.2 Residual Connection\nAs shown earlier, feedforward networks with di-\nmension dis equivalent to a summation of mfeed-\nforward networks with dimension d/m. Using\nthe connector unit deﬁned above, a residual-added\nfeedforward network can be expressed as mhori-\nzontally connected networks.\nRes(FFd)(X) = X+\nm∑\ni=1\nFF d\nm\n(X)\n=\n(\nΩ ◦\n(m\n⃝\ni=1\nΨ\n(\nFF d\nm\n,0\n)))\n(X)\nDetailed derivation is provided in Appendix B. Sim-\nilarly we can write multi-head attention as horizon-\ntally connected single-head attentions.\nRes(Atta,dk,dv )(X) = X+\na∑\ni=1\nAtt1,dk,dv (X)\n=\n(\nΩ ◦\n( a\n⃝\ni=1\nΨ(Att1,dk,dv ,0)\n))\n(X)\nThus a Transformer block can be rewritten as\n(\nLN ◦Ω ◦\n(m\n⃝\ni=1\nΨ\n(\nFF d\nm\n,wi\n))\n◦\nLN ◦Ω ◦\n(\na\n⃝\nj=1\nΨ(Att1,dk,dv ,wj)\n))\n(X),\nwhere wi = wj = 0 for all i, j. One can choose\na different m, or change any of wi and wj to get\nmodel architectures with various width and depth\nof the feedforward and attention networks for a\ngiven model size.\n3.3 Search Space Considerations\nWe have established our architecture search space,\nparameterized by selection parameters that deter-\nmines which sub-components to retain, and con-\nnection parameters that controls the connection\norientations. We will refer to them more generally\nas architecture parameters, still denoted by w. Let\nθ be other network parametersthat do not affect\nthe architecture. Both wand θare jointly trained\nto produce a complete model.\nThe choice of mto divide the sub-components\ninto allows us to control the granularity of the\nsearch space. If mis too small, each component\nmay be too large that even dropping one hurts the\nmodel quality. If mis too large, besides expanding\nthe search space, it can result in many small com-\nponents that each require fewer FLOPS to compute,\nbut may have worse device utilization overall.\nWe also exclude incompatible or known inefﬁ-\ncient settings from the search space. For example,\ntwo heads with different dv in the same multi-head\nattention will limit parallelization of multi-head\nattention, and therefore not considered.\n4 One-Shot Search\nFor each distinct component fi, we run ofﬂine pro-\nﬁling on the target device to measure its computa-\ntion cost ci. Let Lc(w) be the total network cost,\nwhich depends on the costs c= {ci}and architec-\nture parameters w, but not θ. Let Lo(w,θ) denote\nthe loss function in the original problem that is a\nfunction of both network weights and architecture.\nThe goal is to ﬁnd the optimal\nw∗,θ∗= arg min\nw,θ\nLc(w),\nsubjected to the constraint that Lo(w,θ) is no\nworse than some baseline. To simplify computa-\ntion, we relax the constraint optimization problem\ninto minimizing\nL(w,θ) = Lo(w,θ) + λLc(w),\nwhere λis a tunable hyperparameter.\nNotice that in our search space formulations,\nwi = 0 corresponds to either dropping the com-\nponent or connecting horizontally, both of which\ndon’t add incremental cost to the network. Using\nthis observation, the total cost can be approximated\nas\nLc(w) =\n∑\ni\nwi ·ci, where wi ∈{0,1}.\nThis approximation correlates well with the com-\nputation time from our experiments.\n4.1 Direct Optimization (DO)\nOptimizing the cost with integer constraints is in-\ntractable in general. In this method, we relax the\nconstraints on wand use Lc(w) = ∑\ni|wi|·ci in\nminimizing the total loss L(w,θ). This resembles\nL1 norm regularization that encourages sparse so-\nlution, which is a desirable outcome. After training,\nwe prune components with |wi|below threshold\n(10−6) to get a leaner model.\nThe optimization may yield some wi /∈{0,1},\nwhich make Lc(w) a less accurate estimate of the\ncomputation cost. Nevertheless, the ﬁnal network\nis still valid with the following interpretation. For\nselection parameters, as they simply scale the com-\nponents’ output, that is equivalent to wi = 1 and\nre-scaling θ accordingly. If a connection param-\neter is not 0 or 1, it represents a scaled residual\nconnection to the next connector unit.\n4.2 Sampling Distribution Optimization\n(SDO)\nA disadvantage of DO is the inability to enforce\nwi ∈{0,1}and having many components with\nsmall weight could be a source of inefﬁciency. Be-\nsides, it is not possible to coordinate selection /\nconnection decisions by incorporating more sophis-\nticated dependencies between decisions.\nInstead of learning witself, we learn a sampling\ndistribution or policy π(w|φ) to sample wfor train-\ning, where φare learnable parameters. The policy\nis continuously improved alongside the model to\njointly optimize the expected loss\nL′(θ,φ) = E\nw∼π(·|φ)\n[L(w,θ)].\nDuring training, the sampling policy is initialized\nto explore randomly at ﬁrst, and converges to more\npromising parameter region over time.\nCompared to DO, this formulation is more gen-\neral in that Lc(w) can be any differentiable cost\nfunction, and π(w|φ) can be modeled as more so-\nphisticated distribution to capture dependencies be-\ntween variables.\nNevertheless, computing the gradients of\nL′(θ,φ) analytically by enumerating all possible w\nis generally intractable, depending on structure of\nπ(w|φ). Estimating the expectation from samples\nof w, on the other hand, does not provide gradients\nw.r.t φfor updating π(w|φ). Fortunately, using the\nidentity ∇φπ(w|φ) = π(w|φ)∇φlog π(w|φ), we\ncan rewrite the gradient as\n∇L′(θ,φ) = ∇\n∑\nw\nπ(w|φ) ·L(w,θ)\n=\n∑\nw\n∇π(w|φ) ·L(w,θ) + π(w|φ) ·∇L(w,θ)\n= E\nw∼π(·|φ)\n[∇log π(w|φ) ·L(w,θ) + ∇L(w,θ)].\n(1)\nNotice that the ﬁnal form is the sum of the origi-\nnal L(w,θ) gradient and a term involving gradient\nof the sampling distribution, and their expectation\ncan be estimated from batch samples. After train-\ning, we output the model that corresponds to maxi-\nmum likelihood w.\n5 Experiments\nWe run our experiments on different BERT models\nand tasks to evaluate our proposed methods. We\ninitialize wto be the baseline BERT network.\nTo study the effect of model modiﬁcations, we\nalso re-train the models with selected architecture\nfor comparison. These models are given a “-R”\nsufﬁx in the experiments. All of our models use\nbﬂoat16 and run inference on batches of 16 on\nTPU-v2 hardware.\n5.1 Additional Input\nTo use our model, we need two additional pieces\nof information. First, one needs to run proﬁling of\na base model once to estimate the cost ci of each\ncomponent. Note that the component costs can vary\ndepending on the sequence lengths. From Table 1,\nwe observed that the costs are similar across dif-\nferent sequence lengths, so we just use the highest\ncost across all proﬁled sequence length. Second,\nwe need to decide on an acceptable metric drop.\nOnce we do, we can increase λuntil the drop be-\ncomes unacceptable.\n5.2 Hyperparameters\nAll hyperparameters, including number of training\nsteps, are the same for the selected models and the\nbase models for fair comparison. See Appendix A\nfor more details.\n5.3 BERT Base\nWe consider training the BERT-base structure in\ntwo different scenarios: English-only BERT and\nmultilingual BERT, and investigate the effective-\nness of the methods proposed above under different\nsettings.\nRegarding search space, we divide each compo-\nnent into two equally sized parts. We measure TPU\nrun time of each component as shown in Table 1\nto compute the ci’s above. We report the inference\ntime averaged across sequence lengths to report\nspeedup.\n5.3.1 English BERT\nWe follow Devlin et al. (2018)’s setting to pre-train\nEnglish BERT-base. We pick the fastest model\nwith MNLI dev set accuracy drop less than 1%.\nWe evaluate our model on three datasets of the\nGLUE benchmark (Wang et al., 2019). Table 2\nshows that SDO-R is 1.31 times faster with compa-\nrable quality to the baseline model.\nSequence Length\nComponent 32 128 512\nFeedforward 43.3% 58.6% 51.0%\nAttention Head 54.9% 40.6% 48.7%\nQuery-Key Similarity 28.9% 20.6% 21.6%\nAttention Value 22.8% 19.9% 21.6%\nLayer Normalization Mean 0.8% 0.8% 0.7%\nVertical Feedforward 0.9% 1.3% 0.1%\nTable 1: The computation time of each category of\ncomponents in a full BERT-base network.\nTask Metric\nModel MNLI MRPC SST2 Speed\nBERTBase 84.5% 83.0% 93.7% 1\nDO 83.2% 82.3% 93.4% 1.06\nDO-R 84.5% 83.1% 93.6% 1.06\nSDO 83.0% 83.5% 92.8% 1.31\nSDO-R 84.0% 82.4% 93.5% 1.31\nTable 2: English benchmark tasks and performance\nmetrics of the base model and the selected models.\nFigure 2: Value Mean Pooling: instead of doing stan-\ndard self-attention, we can remove query-key similar-\nity in the dashed line. With that, we get a uniformly\nweighted average of values across the sequence.\n5.3.2 Multilingual BERT\nWe pre-train mutilingual BERT-base using Senten-\ncePiece (Kudo and Richardson, 2018) and 120k vo-\ncab size on Wikipedia. We pick the fastest model\nwith pretraining dev set accuracy drop less than\n1%. We evaluate our models on two datasets of the\nXTREME benchmark (Hu et al., 2020) for zero-\nshot learning. Table 3 show that we can get a 14%\nfaster model with similar accuracy to the baseline\nmodel after retraining.\n5.3.3 Architecture Choices\nThe selected architectures are shown in Fig-\nure 3. We can see English BERT and multilin-\ngual BERT have different network architectures\nchosen. Across all selected models and different\npre-training tasks, some observations are\n•Zero-mean layer normalization was never cho-\nsen, raising doubts about its effectiveness.\n•Sometimes, the whole query-key similarity\nTask Metric Performance Metric\nModel XNLI WikiAnn Speed # Params\nBERTBase 70.3% 68.7% 1 172M\nDO 69.3% 65.5% 1.08 165M\nDO-R 71.4% 70.4% 1.08 165M\nSDO 70.3% 65.7% 1.14 161M\nSDO-R 70.2% 69.6% 1.14 161M\nTable 3: Multilingual tasks metrics and performance\nmetrics of the base model and the selected models.\nSequence Length\nComponent 32 128 512\nFeedforward 32.2% 36.2% 30.2%\nAttention Head 41.2% 36.7% 47.1%\nQuery-Key Similarity 21.3% 16.4% 21.3%\nAttention Value 18.9% 15.5% 21.3%\nLayer Normalization Mean 6.6% 6.4% 4.6%\nVertical Feedforward 19.1% 22.4% 14.7%\nTable 4: The computation time of each category of\ncomponents in a full MiniBERT network.\nbranch of an attention can be dropped, espe-\ncially in earlier layers, making it aValue Mean\nPooling component as shown in Figure 2.\n•Attention and feedforward components can\nbe dropped at the bottom and the top of the\nmodel.\n•dk can be smaller, but not dv.\n•Vertically connected feedforward is better.\n5.4 MiniBERT\nDistillation is a very effective approach to reduce\nmodel size and increase model speed if one can\naccess a large amount of unlabeled data. However,\nﬁnding an efﬁcient and accurate distilled model ar-\nchitecture can be difﬁcult and may require exhaus-\ntive search. Here, we apply our model architecture\nsearch method to shrink a previous state-of-the-art\ndistilled model for part-of-speech tagging and mor-\nphology (Tsai et al., 2019) and show that we can\nmake the model more efﬁcient with our one-shot\nsearch algorithms.\n5.4.1 Model Proﬁle and Search Space\nWe proﬁled MiniBERT with results in Table 4. No-\ntice that, unlike BERT-base, the operations that\nare expensive here are different: vertical feedfor-\nward connection and layernorm are relatively more\nexpensive.\nRegarding the search space, we divide the feed-\nforward layer to eight equally-sized components.\nEach query-key similarity and attention value are\nModel Accuracy Speed # params\nTeacher 94.3% 1 172M\nTsai et al. (2019) 93.7% 20 33M\nMiniBERT (Ours) 94.1% 20 33M\nDO 93.5% 34 32M\nDO-R 93.7% 34 32M\nSDO 93.7% 36 31M\nSDO-R 93.8% 36 31M\nTable 5: Multilingual part-of-speech tagging accuracy\nand performance metrics of the base model and the\nselected models.\nModel Accuracy Speed # params\nTeacher 91.1% 1 172M\nTsai et al. (2019) 88.6% 20 33M\nMiniBERT (Ours) 90.7% 20 33M\nDO 89.8% 26 33M\nDO-R 90.4% 26 33M\nSDO 90.2% 33 32M\nSDO-R 90.2% 33 32M\nTable 6: Multilingual morphology accuracy and per-\nformance metrics of the base model and the selected\nmodels.\ndivided into two components. Each attention head\nis one component.\n5.4.2 More Accurate Distilled Model\nFirst, we found the distilled model trained by (Tsai\net al., 2019) can be improved by better distillation\ntechniques: we ﬁx the teacher model and improve\nthe distilled model by removing all dropouts and\napply linear weight ramp-up of labeled data during\ndistillation, closing more than half of the distilla-\ntion gap.\n5.4.3 Even Faster Distilled Model\nWe search for the fastest distilled model with dev\naccuracy drop less than 0.3%. Table 5 and Table 6\nshow that the MiniBERT is already 20 times faster\nthan the BERT-base. We show our selected models\ncan further improve the distilled model to be 1.7\ntimes faster than the state-of-the-art distilled model\nand 33 to 36 times faster than the base model with\nsmall accuracy drop. The scale of the change is\nmuch larger than pretrained models where we see\nabout 1.1 to 1.2 times speedup. We conjecture this\nis due to that BERT needs a lot of model capacity\nto learn the pre-training tasks and it is difﬁcult to\nachieve bigger speedups there.\nAll the selected architecture are shown in Fig-\nure 4. We observed that different architectures are\nselected for different tasks. The morphology task,\nFigure 3: Selected architectures: (a) BERT-base. (b) English BERT selected by SDO. (c) English BERT selected by\nDO. (d) multilingual BERT selected by SDO. (e) multilingual BERT selected by DO. For illustration simplicity, we\nomit the layernorm after each block in this ﬁgure. All of the zero-mean normalization in layernorms are removed.\nFigure 4: Distilled BERT architectures comparison: (a) MiniBERT architecture (b) Selected architecture by SDO for\nthe part-of-speed task. (c) Selected architecture by DO for the part-of-speed task. (d) Selected architecture by SDO\nfor the morphology task. (e) Selected architecture by DO for the morphology task. For illustration simplicity, we\nomit the layernorm after each block in this ﬁgure. All of the zero-mean normalization in layernorms are removed.\nwhich has 1000 times more classes than part-of-\nspeech tagging, needs a bigger model to keep the\nhigh accuracy.\n5.5 Comparing One-Shot Search Algorithms\nObserving the results above, we can see that SDO,\nwhich optimizes the speed objective directly with-\nout relaxation, usually achieve bigger speedup than\nDO given the same model quality constraint.\nIn the pretraining cases, re-training may be\nneeded depending on the downstream task. We con-\njecture this is because needing to make architecture\nexploration makes the models not have enough ef-\nfective training steps as reported by previous work\n(Liu et al., 2019) to achieve SOTA accuracy. In\nthe distillation case, we see that SDO has the same\nquality as SDO-R, so we can remove the retrain-\ning step and save 2 times the resources for training\nanother model.\n6 Conclusion\nWe have described a way to deﬁne the model archi-\ntecture space of the Transformer based on compo-\nnent composition, and we have proposed a sample-\nbased one-shot search algorithm to ﬁnd efﬁcient\nmodel architectures efﬁciently. We show empiri-\ncally that our methods work well with both BERT-\nbase and an already-small distilled BERT on a va-\nriety of tasks.\nReferences\nKevin Clark, Minh-Thang Luong, Urvashi Khandel-\nwal, Christopher D. Manning, and Quoc V . Le. 2019.\nBam! born-again multi-task networks for natural\nlanguage understanding. CoRR, abs/1907.04829.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter.\n2019. Neural architecture search: A survey. Journal\nof Machine Learning Research, 20(55):1–21.\nAngela Fan, Pierre Stock, Benjamin Graham, Edouard\nGrave, Remi Gribonval, Herve Jegou, and Armand\nJoulin. 2020. Training with quantization noise for\nextreme model compression.\nAriel Gordon, Elad Eban, Oﬁr Nachum, Bo Chen, Tien-\nJu Yang, and Edward Choi. 2017. Morphnet: Fast\n& simple resource-constrained structure learning of\ndeep networks. CoRR, abs/1711.06798.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.\nDistilling the knowledge in a neural network. In\nNIPS Deep Learning and Representation Learning\nWorkshop.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A Massively Multilingual Multi-\ntask Benchmark for Evaluating Cross-lingual Gener-\nalization. arXiv e-prints, page arXiv:2003.11080.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Meng-\nlong Zhu, Matthew Tang, Andrew Howard, Hartwig\nAdam, and Dmitry Kalenichenko. 2018. Quanti-\nzation and training of neural networks for efﬁcient\ninteger-arithmetic-only inference. In The IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR).\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\nCoRR, abs/1808.06226.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Confer-\nence on Learning Representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc V . Le, Geoffrey E. Hinton, and\nJeff Dean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. CoRR,\nabs/1701.06538.\nSheng Shen, Zhewei Yao, Amir Gholami, Michael Ma-\nhoney, and Kurt Keutzer. 2020. Rethinking batch\nnormalization in transformers.\nDavid R. So, Chen Liang, and Quoc V . Le. 2019. The\nevolved transformer. CoRR, abs/1901.11117.\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-\nvazhagan, Xin Li, and Amelia Archer. 2019. Small\nand practical BERT models for sequence labeling. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3632–\n3636, Hong Kong. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Interna-\ntional Conference on Learning Representations.\nSirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin.\n2019. SNAS: stochastic neural architecture search.\nIn International Conference on Learning Representa-\ntions.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learning:\nTraining bert in 76 minutes. In International Confer-\nence on Learning Representations.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert.\nMichael Zhu and Suyog Gupta. 2017. To prune, or not\nto prune: exploring the efﬁcacy of pruning for model\ncompression.\nA Training Setting\nA.1 Implementation Formulation\nIn our SDO implementation, we model the sam-\npling policy as Bernoulli distribution with mean\nparameter represented by E[wi] = Sigmoid(φi)\nand that\nlog π(wi|φ) ∝(−1)1−wi φi.\nTo allow different update size toπ(w|φ) relative\nto L(w,θ), we modiﬁed Equation 1 to introduce\na tuneable hyperparameter ν. The ﬁnal gradients\ncomputed on sampled batch data is given by\nν(−1)1−wi ∇φi ·L(w,θ) + ∇L(w,θ),\nwhere\nL(w,θ) = Lo(w,θ) + λ\n∑\ni\nwici.\nA.2 One-Shot Search Algorithm Tuning\nWe do a grid search with ν = {0.001,0.01}and\nλ = {0.01,0.001,0.0001,0.00001,0.000001}to\nﬁnd the fastest model with an acceptable accuracy.\nWe do a comprehensive search for English BERT\nto understand the hyperparameter impacts better.\nThe results are in Table 7. Overall, we see that\nas we increase λ, the quality start to gradually de-\ncrease and some components are dropped. Then,\nafter a certain point, the model will collapse with\nreally low accuracy and most of the components\ndropped for both DO and SDO.\nFrom our experiments, we also found that λ=\n0.001 for DO, and (λ,ν) = (0 .00001,0.01) for\nSDO work out-of-the-box for other experiments.\nWe conjecture this is because the model is relative\nstable when the hyperparameters are at the ”saddle”\narea. Thus, in most of our experiments, we use the\nabove hyperparameter values, and only retune if\nthe model quality is off.\nA.3 Pretraining Details\nFor English BERT, we use the following pretrain-\ning hyperparameters:\n•Pretraining steps: 250k (90% sequence length\n128, then 10% sequence length 512.)\n•Public BERT wordpieces.\n•Batch size: 4096\n•Optimizer: LAMB (You et al., 2020)\n•Learning rate: 0.0018\n•Num warmup steps: 2500\nFor multilingual-BERT, we use the following hy-\nperparameters:\n•Sequence length: 128\n•Num vocabs: 120k\n•Tokenization: sentecnepiece\n•Pretraining steps: 1M\n•Batch size: 4096\n•Optimizer: LAMB (You et al., 2020)\n•Learning rate: 0.0018\n•Num warmup steps: 1250\nWe notice that while multilingual BERT results\nmatch the state-of-the-art but the English BERT\ndoes not. This may be ﬁxed by training longer, but\nit should not affect our neural architecture search\nstudy.\nA.4 Distillation Details\nCompared to (Tsai et al., 2019), we made a couple\nmodiﬁcations to the distillation algorithm. First,\nwe do not use logits in distillation. We ﬁnd using\nlogits does not help model quality but make the\ndistillation pipeline run much longer due to passing\nhuge logits tensors. Thus, we remove the logits loss\ncomputation and just use the silver labels generated\nby the teacher model. That enables us to train the\nmodels for longer in less time. Second, we remove\nall the dropouts in the model to make the student\nhave more model capacity and overﬁt the teacher\nbetter. Finally, we linearly rampup the weight ratio\nof the labeled data from zero after the training has\nprogressed p% and stop at q% (meaning we only\nuse labeled data after that). The intuition is to let\nthe student model slowly adapt to the gold data dis-\ntribution. The idea is similar to Teacher Annealing\n(Clark et al., 2019).\nTo summarize, here are the hyperparameters\n•Distillation data: de-duplicated multilingual\nWikipedia without upsampling.\n•Train steps: 2M\n•Learning rate: 0.0005\n•Optimizer: ADAM\n•Warmup steps: 10k\n•Batch size: 768 (704 silver and 64 gold in\neach batch.)\n•p = 80, q = 100\nB Derivations for Connecting Residual\nComponents\nHere we show the equivalence between Res(FFd)\nand mhorizontally connected FF d\nm\n. The equiva-\nlence between Res(Atta,dk,dv ) and ahorizontally\nconnected Att1,dk,dv follows the same logic.\nOne horizontal connector puts FF d\nm\nat the sec-\nond output:\nΨ\n(\nFF d\nm\n,0\n)\n(X,0) =\n(\nX,FF d\nm\n(X)\n)\nWhen we horizontally connect another component,\nboth FF d\nm\nare accumulated at the second output.\n(\nΨ\n(\nFF d\nm\n,0\n)\n◦Ψ\n(\nFF d\nm\n,0\n))\n(X,0)\n=\n(\nX,FF d\nm\n(X) + FF d\nm\n(X)\n)\nBy repeating mtimes, the second output becomes\nFFd(X).\n(m\n⃝\ni=1\nΨ(FF d\nm\n,0)\n)\n(X,0)\n=\n(\nX,\nm∑\ni=1\nFF d\nm\n(X)\n)\n= (X,FFd(X))\nFinally, we can combine both output using\nΩ(X,R) = X+R, and the equivalence is straight-\nforward.\nRes(FFd)(X) = X+ FFd(X)\n= Ω(X,FFd(X))\n=\n(\nΩ ◦\n(m\n⃝\ni=1\nΨ(FF d\nm\n,0)\n))\n(X)\nC Example of Connector Units\nFigure 5 shows a [3,2] residual network and its\nequivalent expression in connector units(Ψ(f5,1)◦\nΨ(f4,0) ◦Ψ(f3,1) ◦Ψ(f2,0) ◦Ψ(f1,0))(X,0).\nD Detailed Experiment Results\nHere, we provide detailed experiment results and\nthe ﬁnal model hyperparameters.\nAlgorithm λ ν Accuracy Speed\nBertBASE 82.8% 1\nDO 1e-2 79.1% 1.3\nDO 1e-3 82.2% 1.06\nDO 1e-4 82.6% 1.03\nDO 1e-5 83.1% 1.03\nSDO 1e-4 1e-2 74.4% 3\nSDO 1e-5 1e-2 82.2% 1.31\nSDO 1e-6 1e-2 83.1% 1\nTable 7: MNLI dev set accuracy of different model\narchitectures. The models are trained for 125k steps.\nThe selected model is marked bold.\nD.1 English BERT\nWe report the hyperparameters and the MNLI dev\nset accuracy used to select the best model architec-\nture of all tasks in Table 7. After getting the best\nmodel architecture, we search over learning rate\n{2e-5, 3e-5, 4e-5}, train epochs {6, 7}on GLUE\ndata sets to ﬁnd the model with the best dev set\naccuracy. We report the test set accuracy of the\nbest models in Table 8.\nD.2 Multilingual BERT\nThe fatest DO model with acceptable accuracy drop\nuses λ = 0.01 at 60% MLM accuracy. The best\nSDO model usesλ= 10−5 and ν = 0.01 with 60%\nMLM accuracy. The BERT base model without\nmodel selection has 61% MLM accuracy.\nWe compiled detailed ﬁne-tuning results in Ta-\nble 9.\nVertical\nXin\n0\nXout\nXin\nf3\nf2\nf1\nf5\nf4\n+ Xout+\n(a)\n(b)\n+\n+\nf3\nHorizontal\n+\n+\nf4\nVertical\n+\n+\nf5\nHorizontal\n+\n+\nf2\nHorizontal\n+\n+\nf1\nFigure 5: (a) An example 2-layer network. (b) Equivalent network expressed in connector units.\nTask Metric Performance Metric\nModel MNLI MRPC SST2 Speed # Params\nBERTBase 84.4% / 84.5% 86.3% / 83.0% 91.4% / 93.7% 1 110M\nDO 83.2% / 83.2% 86.8% / 82.3% 91.6% / 93.4% 1.06 106M\nDO-R 84.8% / 84.5% 85.7% / 83.1% 92.1% / 93.6% 1.06 106M\nSDO 83.4% / 83.0% 87.3% / 83.5% 91.6% / 92.8% 1.31 98M\nSDO-R 83.6% / 84.0% 86.0% / 82.4% 92.4% / 93.5% 1.31 98M\nTable 8: English BERT: detailed (dev/test) benchmark tasks metrics and performance metrics of the base model and\nthe selected models.\nTask Metric Performance Metric XNLI Hyperparameters\nModel XNLI Accuracy WikiAnn F1 Speed # Params epochs learning rate\nBERTBase 70.7% / 70.3% 68.3% / 68.7% 1 172M 3 3e-5\nDO 69.0% / 69.3% 65.1% / 65.5% 1.08 165M 3 5e-5\nDO-R 71.2% / 71.4% 70.2% / 70.4% 1.08 165M 3 2e-5\nSDO 70.3% / 70.3% 65.5% / 65.7% 1.14 161M 3 3e-5\nSDO-R 70.3% / 70.2% 69.3% / 69.6% 1.14 161M 3 3e-5\nTable 9: Multilingual BERT: Detailed (dev/test) benchmark tasks metrics and performance metrics of the base\nmodel and the selected models. For WikiAnn, all runs train for 10 epochs with learning rate 3e-5.",
  "topic": "Speedup",
  "concepts": [
    {
      "name": "Speedup",
      "score": 0.8400377035140991
    },
    {
      "name": "Computer science",
      "score": 0.7794404029846191
    },
    {
      "name": "Inference",
      "score": 0.6962810754776001
    },
    {
      "name": "Transformer",
      "score": 0.6629437208175659
    },
    {
      "name": "Architecture",
      "score": 0.4814023971557617
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3738749325275421
    },
    {
      "name": "Computer engineering",
      "score": 0.3503269553184509
    },
    {
      "name": "Parallel computing",
      "score": 0.29142969846725464
    },
    {
      "name": "Engineering",
      "score": 0.11498072743415833
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ]
}