{
  "title": "A Stock Price Prediction Method Based on BiLSTM and Improved Transformer",
  "url": "https://openalex.org/W4384518871",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2130010543",
      "name": "Shuzhen Wang",
      "affiliations": [
        "Xiamen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2593842564",
    "https://openalex.org/W3110420963",
    "https://openalex.org/W2621280324",
    "https://openalex.org/W2057882659",
    "https://openalex.org/W3016414288",
    "https://openalex.org/W3088738441",
    "https://openalex.org/W3202315245",
    "https://openalex.org/W6775882456",
    "https://openalex.org/W2774559076",
    "https://openalex.org/W2995741266",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2997421965",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4285402398",
    "https://openalex.org/W6725336789",
    "https://openalex.org/W3035414307",
    "https://openalex.org/W4205156336",
    "https://openalex.org/W6790346436",
    "https://openalex.org/W3110378470",
    "https://openalex.org/W2084095064",
    "https://openalex.org/W2936404407",
    "https://openalex.org/W2999622530",
    "https://openalex.org/W4224220755",
    "https://openalex.org/W3015644358",
    "https://openalex.org/W1509168122",
    "https://openalex.org/W2510642588",
    "https://openalex.org/W3126170988",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "How to maximize shareholder returns has always been a focus of research in the financial field. In order to improve the accuracy and stability of stock price prediction, this article proposes a new method, BiLSTM-MTRAN-TCN. Improve the transformer model and introduce TCN (Temporary Revolution Network) to construct a new transformer model (MTRAN-TCN), making it suitable for stock price prediction. This method consists of BiLSTM (Bi-directional Long Short-Term Memory) and MTRAN-TCN, which can fully utilize the advantages of the three models: BiLSTM, transformer and TCN. Transformer is good at obtaining full range distance information, but its ability to capture sequence information is weak. BiLSTM can capture bidirectional information in sequences, while TCN can capture sequence dependencies and improve the model&#x2019;s generalization ability. Not only did the improvement effect of the transformer and the effectiveness of introducing the BiLSTM model be verified, but the effectiveness of the method was also verified using 5 index stocks and 14 Shanghai and Shenzhen stocks. Compared with other existing methods in the literature, this method has the best fit on each index stock, and the <inline-formula> <tex-math notation=\"LaTeX\">$\\text{R}^{2}$ </tex-math></inline-formula> of this method is the best in 85.7&#x0025; of the stock dataset. RMSE decreases by 24.3&#x0025; to 93.5&#x0025;, and <inline-formula> <tex-math notation=\"LaTeX\">$\\text{R}^{2}$ </tex-math></inline-formula> increases by 0.3&#x0025; to 15.6&#x0025;. In addition, this method has relatively stable prediction performance at different time periods and does not have timeliness issues. The results indicate that the BiLSTM-MTRAN-TCN method performs better in predicting stock prices, with high accuracy and generalization ability.",
  "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nA Stock Price Prediction Method Based on \nBiLSTM and Improved Transformer  \nSHUZHEN Wang1,2 \n1 School of Information Science and Technology, Xiamen University Tan Kah Kee College, Zhangzhou 363105, China   \nCorresponding author: Shuzhen Wang (wangsz@xujc.com) \nThis research was funded by the Educational Research Project of Young and Middle-Aged Teachers of Fujian \nProvincial Department of Education (grant number JAT191086).  \nABSTRACT How to maximize shareholder returns has always been a focus of research in the financial field. \nIn order to improve the accuracy and stability of stock price prediction, this article proposes a new method, \nBiLSTM-MTRAN-TCN. Improve the transformer model and introduce TCN (Temporary Revolution \nNetwork) to construct a new transformer model (MTRAN-TCN), making it suitable for stock price prediction. \nThis method consists of BiLSTM (Bi-directional Long Short-Term Memory) and MTRAN-TCN, which can \nfully utilize the advantages of the three models: BiLSTM, transformer and TCN.  Transformer is good at \nobtaining full range distance information, but its ability to capture sequence information is weak. BiLSTM \ncan capture bidirectional information in sequences, while TCN can capture sequence dependencies and \nimprove the model's generalization ability.  Not only did the improvement effect of the transformer and the \neffectiveness of introducing the BiLSTM model be verified, but the effectiveness of the method was also \nverified using 5 index stocks and 14 Shanghai and Shenzhen stocks.  Compared with other existing methods \nin the literature, this method has the best fit on each index stock, and the R2 of this method is the best in 85.7% \nof the stock dataset. RMSE decreases by 24.3% to 93.5%, and R 2 increases by 0.3% to 15.6%. In addition, \nthis method has relatively stable prediction performance at different time periods and does not have timeliness \nissues. The results indicate that the BiLSTM-MTRAN-TCN method performs better in predicting stock prices, \nwith high accuracy and generalization ability. \nINDEX TERMS Transformer, BiLSTM, TCN, Stock price prediction, Deep learning, Hybrid neural network \nI. INTRODUCTION \nFor decades, stock price prediction has attracted attention \nfrom investors and researchers due to their enormous value \n[1]. More and more investors pay attention to the changing \ntrend of stock price [2]. For economists, predicting stock \nprice changes in advance is a very important task  [3],[4]. It \ncan help investors maximize the ir investment income.  \nHowever, due to the high volatility of the stock market and \nthe impact of random noise, its trend is complex and \ndifficult to predict [5]. Although the financial time series is \ndifficult to predict, it gen erally shows predictability is a n \nessential task.  \nIn the early days, the most famous is the moving average \nautoregressive model ARIMA  [6]. Later, Narendra et al. [7] \napplied ARIMA model and GARCH model (autoregressive \nconditional heteroskedasticity model) to the NSE Indian \nstock market data forecast. In addition to these two models, \nthere are Bayesian vector autoregression model and \nKalman filter model. Although these techniques can be \nsuccessfully used for short -term prediction, they are not \nsuitable for nonlinear problems and have poor long -term \nprediction performance [8]. To solve this problem, machine \nlearning is introduced to analyze time series, and they  are \nsuccessfully applied to stock price forecasting [9], [10], [11]. \nThe advantages of machine learning in processing complex \nand large amounts of data have solved many limitations of \ntraditional methods [12]. Machine learning methods include \nsupport vector machine (SVM), decision tree, naive Bayes, \nrandom forest, etc. Wang et al. [13] mixed decision trees and \nSVM models to predict future price trends. Chen et al. [14] \nestablished feature weighted SVM and K -nearest neighbor \nalgorithm to predict the stock market index.  Experimental \nresults have shown that the model has good short -term, \nmedium-term, and long-term prediction capabilities. In 2021, \nYan Zhengxu et al. [15] proposed a new combination model \nmethod of Random forest based on Pearson coefficient on the \nbasis of Random forest to achieve short -term forecasting \nregression of stock price. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \n2 VOLUME XX, 2017 \nin recent years. deep learning methods that rely solely on \ndatasets can be used to predict stock price without the need for \nprofessional knowledge. Therefore, their application in the \nfield of stock prediction has gradually become a resear ch \nhotspot for scholars. Deep learning methods include gated \ncyclic unit (GRU), recurrent neural network (RNN), \nConvolutional neural network (CNN), Long short -term \nmemory (LSTM) and bidirectional Long short-term memory \n(BiLSTM). In 2017 and 2018, wavelet neural networks and \nCNN were successively used for stock price prediction [16], \n[17]. Experiments have shown that CNN is effective in \npredicting time series. In 2018, the authors proposed Conv1D-\nLSTM model, wh ich combines one -dimensional CNN and \nLSTM. It can integrate the advantages of the two networks: \nCNN can effectively extract features, and LSTM can well \nprocess sequence data. The results indicate that the prediction \nresults are more accurate than machine l earning prediction \nmodels [18]. In 2019, Yang Qing et al. conducted predictive \nresearch on global stock ind exes using BiLSTM, indicating \nthat BiLSTM has excellent predictive accuracy and strong \ngeneralization ability [19]. In 2020, one study [20] used LSTM \nregression model to forecast India‚Äôs NIFTY 50 index. The \nresults showed that LSTM models based on deep learning \nperformed better than traditional machine learning methods. \nNow, i n most of the current studies, the attention \nmechanism has begun to be the main structure to solve the \nproblem of financial market forecasting, focusing on the key \nposition that has a greater impact on the results. In 2020, Lu et \nal. introduced an attention mechanism (AM) based on CNN \nand BiLSTM, proposed CNN -BiLSTM-AM model , which \nproved to be more accurate than the existing models [8]. In \n2022, a wavelet transform was used to denoise historical stock \ndata based on LSTM and an attention mechanism [21]. \nMoreover, the transformer is the state-of-the-art model based \non the attention mechanism, which was proposed for sequence \nmodeling [22]. New methods based on the transformer were \nproposed to tackle the stock movement prediction task. In \n2020, Ding et al. demonstrated that the model based on the \ntransformer with the enhancement of Multi-Gaussian prior can \nbe used for stock movement  prediction [23]. In 2021, a \ntransformer neural network based on self -attention was \nproposed, which has the special ability in forecasting time \nseries, and the electricity consumption and traffic data were \nused to validate the proposed model [24] . \nIn 2022, Zhang et al. proposed a novel transformer encoder-\nbased attention network framework with the fusion of media \ntext and stock price, and it has been shown to be effective to \npredict the rise or fall of stock price [8]. In 2022, Peng et al. \nused a data organization method with LSTM and transformer \nto predict Chinese bank stock price [25]. In 2022, Wang \nutilized the latest deep learning framewor k, transformer, to \npredict the stock market index  and it demonstrated that the \ntransformer can outperform other classical methods [26]. \nWe can see that different LSTM -based and transformer -\nbased models have been proposed for stock prediction. But so \nfar, it is rare to use transformer-based models to predict price \nwithout considering the significance of stock data or social \nmedia text. Most of the models based on transformer are used \nto predict the stock trend of up and down, rat her than \npredicting the stock price. Furthermore, now most of the \nproposed methods usually only target specific stocks or a \nsingle stock index, while the prediction model has timeliness \nissues. Therefore, there is still a lot of room for optimization \nin terms of accuracy and depth in the network structure of \nstock prediction models. \nIn order to improve the stability and accuracy of stock price \nprediction, this article proposes a novel method BiLSTM -\nMTRAN-TCN based on transformer. It can not only predict \nthe index price, but also the individual stock price. This \nmethod is formed by introducing BiLSTM and TCN \n(Temporary Revolution Network) on the basis of transformer \nencoder. Transformer is a mechanism that can extract deep \nfeatures of small samples to obtain key information. BiLSTM \ncan capture bidirectional information in sequences, while \nTCN can capture sequence dependencies and improve the \nmodel's generalization ability.  \nThe main work completed in this article is as follows: \n‚ö´ Improve the transformer model and introduce TCN to \nconstruct a new transformer model (MTRAN-TCN), \nmaking it suitable for stock price prediction. \n‚ö´ This method consists of BiLSTM and MTRAN-TCN, \nwhich can fully utilize the advantages of the three \nmodels: BiLSTM, transformer and TCN. \n‚ö´ Propose a bidirectional stock selection strategy to \nselect stock experimental subjects. \n‚ö´ Compare with other existing models in the literature \nto verify the effectiveness of the method. \n‚ö´ Experimental results have shown that the proposed \nmethod has good generalization ability and solves the \nproblem of timeliness. \nII. ALGORITHM INTRODUCTION \nA. TRANSFORMER \nTransformer is a classic NLP model propos ed by Google's \nteam in 2017  [27], and Bert, which is popular now, is also \nbased on the transformer . The transformer uses the self -\nattention mechanism and does not use the RNN sequential \nstructure, so that the model can be parallelized and have global \ninformation. Different fro m the recurrent networks, the \ntransformer has no problem of gradient disappearance, and can \naccess any point in the past, regardless of the distance between \nwords.  \nTransformer consists of an encoder and a decoder section. \nThe encoder section contains a stack of encoders, as shown in \nFigure 1. It encodes the input data according to a specific mode, \nand the decoder section decodes the output according to the \nencoded input to generate the required output. The most \nimportant part of the encoder section is the multi-head self-\nattention mechanism, through which the transformer captures \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \n2 VOLUME XX, 2017 \nlong-term and short -term dependency. Different attention is \nfocused on different aspects of the time pattern, so that more \nfeature information can be captured. The output of self -\nattention is mainly calculated by Q, K and V matrices, where \ndk is the dimension of K vector, as shown in Formula (1). \nùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùëÑ, ùêæ, ùëâ) = ùë†ùëúùëìùë°ùëöùëéùë• (\nùëÑùêæùëá\n‚àöùëëùëò\n) ùëâ             (1) \nTransformer has a better understanding of the context, so \nthat it has special capabilities in predicting time series \nproblems. However, the transformer model is designed for \nmachine translation, and it cannot be directly used to predict \ntime series. To predict the stock price, the encoder section of \ntransformer is used as the basic model, as shown in Figure 3. \n \nFIGURE 1. Transformer overall structure. \nA. TCN (Time convolutional network) \nTCN refers to a time convolutional network, a new algorithm \nthat can be used to solve time series prediction. This algorithm \nwas first proposed by Lea et al. in 2016  [28]. The network \nstructure of TCN m ainly consists of causal convolution, \ndilation convolution  and residual connections. Each \nconvolution layer is a causal convolution with a unidirectional \nstructure, as shown in Figure 2.  Causal convolution ensures \nthat the output at time T is only convolved with elements that \noccurred before time T, which avoids future information \nleakage. And it can accept input sequences of any length and \noutput the same length. The calculation of causal convolution, \nas shown in Formula (2). \nùêπ(ùë†) = ‚àë ùëì(ùëñ)ùë•ùë†‚àíùëëùëñ\nùëò‚àí1\nùëñ=0\n       (2) \nThe dilation convolution  is used to capture longer \ndependency information without stacking more layers or \nadding pooling layers to obtain larger a receptive field. \nFIGURE 2. Time Convolutional Neural Network (TCN ).  \nIII. BiLSTM-MTRAN-TCN METHOD OF THIS ARTICLE \nMTRAN-TCN, which is an improved model of transformer . \nIn this section, we first provide a detailed introduction on how \nto improve the transformer, and then describe the entire \nnetwork structure of the proposed method. \nA. IMPROVE TRANSFORMER TO MTRAN-TCN \nModify the transformer model to better predict stock price , \nmainly by modifying the decoder of the transformer model, as \nshown in Figure 3. \n‚ö´ Delete the first module Input Embedding (see Figure 1). \nThis module is  to vectorization the language and text, \nwhich is a module required for machine language \ntranslation, while the stock price does not need to \nvectorization the text. \n‚ö´ Adjust the position of the Position Encoding module, \nremove it from the MTRAN-TCN, and move  it to the \nfront of BiLSTM, as shown in Figure 4. \n‚ö´ Replace the transformer decoder with TCN layer, full \nconnection layer and the activation function Tanh. \n‚ö´ Removed other inputs of the decoder, leaving only the \noutput of the encoder as the only input to the decoder. \nTCN has been proven to be suitab le for sequence data \nprediction. In 2022, Chen Zhe et al. proposed using time \nconvolutional networks to mine time series features in traffic \ndatasets[29]. In 2022, Wang Jun et al. prop osed a \nmultivariable TCN-Attention model that combines TCN and \nattention mechanism to predict daily average traffic  [30]. In \n2023, Yang Zhiyong et al.  proposed a two -way short-term \nmemory neural network integrating self-attention mechanism \nand TCN to predict stocks [31]. These all indicate that TCN \ncan be used for sequence data prediction with good results. \nTherefore, this article introduces TCN to improve the \ntransformer model. \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \n2 VOLUME XX, 2017 \nFIGURE 3. Modified overall structure of transformerÔºàMTRAN-TCNÔºâ. \nModified transformer, abbreviated as MTRAN-TCN. \nB. NETWORK STRUCTURE \nThis study is based on the transformer model and introduces \nTCN to modify it to make it suitable for predicting stock \nseries data. The transformer model can achieve pa rallel \ncomputing and obtain global signals well. However, the \nability to capture sequence information is weak, and the \neffect of directly using transformer for stock prediction is not \nideal. TCN can capture advanced and low-level features with \nstable gradients. And it can enable the model to process time \nseries information in parallel, improving the model \nprediction accuracy and training efficiency . Its introduction \ncan fully utilize the advantages of t ransformer and TCN to \nbetter predict sequence data. At the same time, BiLSTM has \na strong ability to capture sequence signals, and BiLSTM is \nintroduced to achieve better prediction results. Combining \nBiLSTM with the improved transformer to construct a \nhybrid network BiLSTM-MTRAN-TCN, as shown in Figure \n4.  \nIn the process of stock prediction analysis, the time range \nof stock sequence data is usually large, and BiLSTM may \nencounter the problem of rapid gradient decay when \nprocessing long time series. This can  affect the BiLSTM \nnetwork to learn important feature information in stock data, \nleading to the loss of important feature information in the \nmodel. So combining it with MTRAN-TCN can improve the \nprediction efficiency of the model.  The MTRAN-TCN can \ncapture short -term and long -term dependencies, prioritize \nobtaining important feature information of stock data, ignore \nother irrelevant information, and improve the prediction \naccuracy of the model by calculating the importance of each \nelement in the multi-source feature fusion stock sequence. \nAs the number of network layers and iterations increases, \nthe weight changes too quickly, which can lead to network \ndegradation effects in the model and poor performance when \nprocessing new data. For this reason, this article introduces the \nTCN network layer to handle variable length time series, \ncapturing the dependencies of the sequence through the \nconvolutional layer in TCN. And its residual connections can \nreduce the network depth and number of parameters, which \nimprove the model's generalization ability and training \nefficiency. \nTherefore, introducing TCN to improve the transformer \nmodel and constructing a hybrid network with BiLSTM can \ngreatly improve the model's expression performance and \ntraining efficiency.  \nUsing stock trading data and technical indexes data as the \ninput, the output is the closing price of the next day. The input \nis a 3D tensor, which includes samples, time_steps and \nfeatures. After the input is processed by the P ositional \nEncoding layer, the s equence features are captured by the \nBiLSTM, and then processed by the encoder of the \ntransformer. After that,  the features are further extracted \nthrough the TCN network layer. Finally, the full connection \nlayer and the activation function are used for dim ension \nreduction processing. The main structure includes Positional \nEncoding layer, BiLSTM layer, transformer encoder layer, \nTCN layer and Dense layer (Full Connection layer), as shown \nin Figure 4. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \n2 VOLUME XX, 2017 \n  \nFIGURE 4. BiLSTM-MTRAN-TCN model structure diagram \nIV. EXPERIMENT ENVIRONMENT \nIn this section, we introduce the experimental dataset, \nevaluation indexes and experimental parameters. \nA. DATASET  \nPrevious studies [32], [33], [34], [35] only selected a few \nstocks from the Shanghai and Shenzhen ind exes or the \nShanghai and Shenzhen stock markets for experiments, and \nthe price trends of the selected stocks were relatively stable \nand lacked volatility. The experimental coverage is poor, and \nthe model experimental results lack persuasiveness.  \nTo increase coverage, select 5 index stocks and 14 Shanghai \nand Shenzhen stocks for experiments. When selecting index \nstocks, representative index  stocks were selected : A-share \nIndex, Shanghai Compos ite Index, Shenzhen Component \nIndex, CSI 3 00 and Growth Enterprise Board  Index. The \nselected index stocks are shown in Table 1. \nWhen selecting stocks in Shanghai and Shenzhen markets, \na bidirectional stock selection strategy is adopted, which \nbroadens the coverage of the experiment. Horizontally, select \nbased on the size of the company's market value, divided into \nlarge-cap stock and small -cap stock; vertically, select \naccording to stock classification, including 7 general \ncategories including finance, real  estate, coal, steel, non -\nferrous metals, petrochemical and automotive. The selected \nstocks are shown in Table 2. \nThe data of the index stocks comes from J oinQuant \n(https://www.joinquant.com/research). And  the data of 14 \nShanghai and Shenzhen stocks is sourced from Tushare \n(https://tushare.pro/).  \nTABLE 1 SELECTED INDEX STOCKS \nNUMBER Index Name Index Code \n1 A-share 000002.XSHG \n2 Shanghai Composite Index  000001.XSHG \n3 Shenzhen Component Index 399001.XSHE \n4 CSI 300  399300.XSHE \n5 Growth Enterprise Board.Index 399006.XSHE \nThis experiment select ed stock data from the last 2700 \ntrading days of each stock, covering the period from February \n2012 to May 2023. Index stocks use historical trading data as \nthe dataset, including: closing price, highest price, lowest price, \nopening price, ups and downs, change, turnover and volume. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \n2 VOLUME XX, 2017 \nThe Shanghai and Shenzhen stocks use the first six fields  \nabove and two relevant technical indexes (5-day moving \naverage, 10-day moving average) as the stock dataset. \nThere are significant differences in each feature of the stock \ndataset, and the data needs to be normalized  [36]. The \nstandardization method used in this article is the Z -score, as \nshown in formula (3).  \nùë¶ùëñ =\nùë•ùëñ‚àíùë•ÃÖ\nùë† ‚Ä¶‚Ä¶‚Ä¶.‚Ä¶‚Ä¶(3) \n Where yi is the standardized value, ùë•ùëñ is the input data, ùë•ÃÖ is \nthe average of the input data, and s is the standard deviation of \nthe input data. \nTABLE 2 SELECTED STOCKS FROM  SHANGHAI AND SHENZHEN MARKETS \nCategory Large-cap stock Small-cap stock \nFinance China Merchants Bank \n(SH600036) \nGuojin Securities \n(SH600109) \nReal Estate Poly Development \n(SH600048) \nTibet Urban Investment \n(SH600773) \nCoal China Shenhua \n(SH601088) \nPower Investment Energy \n(SZ002128) \nSteel Zhongxin Special \nSteel (SZ000708) \nFangda Carbon \n(SH600516) \nNonferrous \nMetal \nTianqi Lithium \nIndustry (SZ002466) \nYunnan Copper Industry \n(SZ000878) \nPetrochemical China Petroleum \n(SH601857) \nYueyang Xingchang \n(SZ300164) \nAutomotive BYD (SZ002594) Dongfeng Motor \n(SH600006) \nB. PERFORMANCE EVALUATION \nThe mean square error (M SE), the mean absolute error \n(MAE), root mean square error (RMSE)  and R-square (R2) \nare used as the evaluation criteria of methods. The \ncalculation method of these error evaluation indexes is \nshown in Formula (4). \nùëÄùëÜùê∏ =\n1\nùëõ ‚àë (ùë¶ùëñ ‚àíùëõ\nùëñ=1 ùë¶ùëñÃÇ)2 \nùëÄùê¥ùê∏ = 1\nùëõ ‚àë |ùë¶ùëñ ‚àí\nùëõ\nùëñ=1\nùë¶ùëñÃÇ| \nùëÖùëÄùëÜùê∏ = ‚àö\n1\nùëõ ‚àë (ùë¶ùëñ ‚àíùëõ\nùëñ=1 ùë¶ùëñÃÇ)2 \nùëÖ2 = 1 ‚àí\n‚àë (ùë¶ùëñ ‚àíùëõ\nùëñ=1 ùë¶ùëñÃÇ)2\n‚àë (ùë¶ÃÖùëõ\nùëñ=1 ‚àí ùë¶ùëñÃÇ)2  \nWhere yi is the real value, yiÃÇ is the predicted value, and yÃÖ is \nthe average value. The smaller the MSE, MAE, and RMSE \nvalues, the better the performance.  And t he value of R2  is \nbetween 0 and 1. The closer it is to 1, the better its performance. \nC. NETWORK PARAMETERS \nThe parameter set tings of the BiLSTM -MTRAN-TCN \nmethod in this experiment are shown in Table 3 . T he loss \nfunction is M SE, the optimizer chooses Adam , and the \nlearning rate is 0.00001. The window size is 10, which predicts \nthe closing price of stocks on the following day based on stock \ndata from 10 days ago. \nTABLE 3. PARAMETER‚ÄôS SETTING OF BILSTM-MTRAN-TCN METHOD \nParameters Value \nBatch size 5 \nSequence length of training data 10 \nHidden size of BiLSTM 64 \nNumber of BiLSTM layer 3 \nNumber of transformer encoder head 8 \nNumber of transformer enconder layer 6 \nNumber of TCN layer neurons 32 \nTCN layer kernel size 1 \nNumber of TCN hidden layer 4 \nKernel size of TCN layer  7 \nActivation function of TCN layer  RELU \nV. EXPERIMENT AND ANALYSIS  \nIn this section, we validate the effectiveness of our proposed \nmethod from four aspects: comparative analysis with \nmainstream methods in the existing literature, validation of \nthe effectiveness of introducing MTRAN-TCN and BiLSTM \nmodels, whether this method has timeliness issues, and the \ngeneralization ability of the method. \nA. EFFECTIVENESS OF MTRAN-TCN AND BiLSTM \nUsing the index stocks in Table 1 as the dataset, verify the \nimprovement effect of the transformer model and the \neffectiveness of introducing BiLSTM. Compare the BiLSTM-\nMTRAN-TCN method wi th MTRAN -TCN, MTRAN, \nBiLSTM-TRAN, BiLSTM, and BiLSTM -TCN, where \nMTRAN refers to the transformer encoder  plus a fully \nconnected layer. The comparison results of the above six \nmethods are shown in Table 4. \nThe predictive performance of MTRAN and MTRAN-TCN \nis similar, indicating that using the improved MTRAN-TCN \nalone did not result in much improvement in performance. \nHowever, by adding BiLSTM to the MTRAN -TCN, the \nprediction performance of BiLSTM -MTRAN-TCN is \nsignificantly better than that of MTRAN -TCN. Its RMSE \n(0.118 compared to 0.231) decreases by 49.1%, while its R2 \n(compared to 0.986 and 0.945) increase s by 4.28%. \nMeanwhile, the prediction performance of using BiLSTM \nalone is second only to BiLSTM -MTRAN-TCN, indicating \nthat the BiLSTM model itself has a relatively good effect in \npredicting stock price . Its addition greatly improves the \npredictive performance of BiLSTM -MTRAN-TCN. This \nindicates that the introduction of BiLSTM is effective. \nBut when BiLSTM constructs a hybrid network with \nMTRAN or TCN, the prediction performance actually \ndecreases. Compared to BiLSTM, the RMSE of BiLSTM-\nMTRAN and BiLSTM-TCN increases by 13.59% and 20.23%, \nrespectively, while R 2 decreases by 0.85% and 1.13%, \nrespectively. When building a hybrid network with BiLSTM \nand MTRAN-TCN, its RMSE is less by 27.46% and R 2 is \nmore by 1.5% compared to BiLSTM. It can be seen that the \nhybrid network constructed by BiLSTM and MTARN -TCN \ncan achieve the best results. \n(4) \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \n2 VOLUME XX, 2017 \nIn addition, compared to BiLSTM -MTRAN, BiLSTM-\nMTRAN-TCN reduces MAE from 0.137 to 0.087, MSE from \n0.038 to 0.014, RMSE from 0.184 to 0.118, and increases R2 \nfrom 0.963 to 0.986, indicating that the introduction of TCN \nto improve the transformer has a significant effect.  \nIn conclusion , this article has shown good results in \nimproving the transformer, and using BiLSTM to extract \nsequence features before MTRAN -TCN processing has \nshown significant results. \nTABLE 4. COMPARISON OF SIX METHODS \nMethod MAE MSE RMSE R2 \nMTRAN 0.175  0.056  0.229  0.945  \nMTRAN-TCN 0.173  0.055  0.231  0.945  \nBILSTM-MTRAN-TCN 0.087  0.014  0.118  0.986  \nBILSTM 0.121  0.028  0.162  0.971  \nBILSTM-MTRAN 0.137  0.038  0.184  0.963  \nBILSTM-TCN 0.147  0.040  0.195  0.960  \nB. COMPARISON WITH OTHER METHODS \nThe BiLSTM-MTRAN-TCN method is compared with five \nother methods: LSTM, BiLSTM in literature  [38], CNN-\nBiLSTM in literature [37], CNN-BiLSTM-AM in literature \n[8], and BiLSTM -SA-TCN in literature [31]. CNN-\nBiLSTM-AM, is a hybrid network of CNN, BiLSTM and \nAM (Attention Mechanism). BiLSTM-SA-TCN is a hybrid \nnetwork of BiLSTM, SA (Self-Attention) and TCN.  \nComparative analysis was conducted using index stocks \nand individual stocks from Shanghai and Shenzhen.  Each \nstock is tested 5 times and the average of the five results is \ntaken. All data is first standardized by Formula (3) before \ntraining, so the experimental results  are all standardized \nvalues. \n1) SHANGHAI AND SHENZHEN STOCKS \nTo further verify the effectiveness and progressiveness of \nthe BiLSTM-MTRAN-TCN method, compare with other four \nmethods mentioned above: BiLSTM, CNN BiLSTM, CNN \nBiLSTM-AM and BiLSTM-SA-TCN. Select 14 stocks from \nthe Shanghai and Shenzhen stock markets as the dataset, from \n7 major categories of large and small cap stocks, as shown in \nTable 2. The experimental results are shown in Tables 5-11. \nIt can be observed that the evaluation error indexes of CNN-\nBiLSTM and CNN -BiLSTM-AM methods are significantly \nlower than those of BiLSTM, BiLSTM -SA-TCN, and \nBiLSTM-MTRAN-TCN methods. This indicates that these \ntwo methods can only roughly fit the stock trend, and there is \na significant error, resulting in a low fitting degree of the \nmodel. Compared with CNN -BiLSTM and CNN -BiLSTM-\nAM, BiLSTM has a smaller error, indicating that using CNN \nto extract stock data features first and then learning through \nthe BiLSTM network does not improve the prediction \naccuracy, but rather reduces the prediction accuracy. How to \nimprove the accuracy of prediction models requires neural \nnetwork models to focus on feature information related to \nstock prices. The use of transformer's multi -head attention \nmechanism can effectively solve this problem. \nAlthough BiLSTM -SA-TCN also uses a self -attention \nmechanism, its predictive performance is lower than \nBiLSTM-MTRAN-TCN in 100% of datasets. Compared with \nBiLSTM-SA-TCN, BiLSTM -MTRAN-TCN has a 40.2% \nreduction in MAE, 34.6% reduction in MSE, 55.6% reduction \nin RMSE, and 1.2% in crease in R 2. This indicates that the \nmulti-head attention mechanism of the transformer can better \nimprove prediction accuracy than SA (Self-Attention). \nOverall, for the BiLSTM -MTRAN-TCN method, the \nexperimental results show that the R2 evaluation index is the \nbest in 85.7% of the stock dataset, and the RMSE evaluation \nindex is the best in 78.6% of the dataset. Compared with the \nCNN-LSTM, CNN-BiLSTM-AM, BiLSTM, and BiLSTM -\nSA-TCN methods, its RMSE decrease s by 93.2%, 93.5%, \n55.6%, and 24.3%, respectively. And R2 increases by 0.3% to \n15.6%. This indicates that the BiLSTM -MTRAN-TCN \nmethod has more accurate expression ability than other \nmethods and is in line with the fundamental trend of stock \nprice fluctuations.  \n \nTABLE 5. COMPARISON OF EVALUATION INDEXES OF DIFFERENT METHODS IN CHINA MERCHANTS BANK AND GUOJIN SECURITIES \nMethod \nCHINA MERCHANTS BANK (SH600036) GUOJIN SECURITIES (SH600109) \nMAE MSE RMSE R2 MAE MSE RMSE R2 \nBILSTM 0.081  0.012  0.109  0.988  0.081  0.012  0.109  0.988  \nCNN-BILSTM 0.265  0.134  0.366  0.861  0.264  0.132  0.364  0.862  \nCNN-BILSTM-AM 0.277  0.143  0.378  0.851  0.240  0.121  0.348  0.874  \nBILSTM-SA-TCN 0.110  0.020  0.142  0.979  0.116  0.021  0.146  0.978  \nBILSTM-MTRAN-TCN 0.042  0.004  0.064  0.996  0.062  0.007  0.083  0.993  \n \n \n \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \n2 VOLUME XX, 2017 \nTABLE 6. COMPARISON OF EVALUATION INDEXES OF DIFFERENT METHODS IN POLY DEVELOPMENT AND TIBET URBAN INVESTMENT \nMethod \nPOLY DEVELOPMENT (SH600048) TIBET URBAN INVESTMENT (SH600773) \nMAE MSE RMSE R2 MAE MSE RMSE R2 \nBILSTM 0.081  0.012  0.108  0.988  0.080  0.011  0.107  0.988  \nCNN-BILSTM 0.265  0.134  0.366  0.860  0.257  0.129  0.359  0.866  \nCNN-BILSTM-AM 0.293  0.149  0.387  0.845  0.241  0.123  0.351  0.872  \nBILSTM-SA-TCN 0.119  0.022  0.148  0.977  0.107  0.018  0.136  0.981  \nBILSTM-MTRAN-TCN 0.056  0.009  0.096  0.990  0.096  0.017  0.131  0.982  \n \nTABLE 7 COMPARISON OF EVALUATION INDEXES OF DIFFERENT METHODS IN SHENHUA AND POWER INVESTMENT ENERGY IN CHINA \nMethod \nCHINA SHENHUA (SH601088) POWER INVESTMENT ENERGY (SZ002128) \nMAE MSE RMSE R2 MAE MSE RMSE R2 \nBILSTM 0.084  0.012  0.111  0.987  0.083  0.012  0.111  0.987  \nCNN-BILSTM 0.260  0.132  0.363  0.863  0.264  0.133  0.365  0.861  \nCNN-BILSTM-AM 0.243  0.122  0.349  0.873  0.272  0.138  0.372  0.856  \nBILSTM-SA-TCN 0.110  0.020  0.140  0.980  0.115  0.021  0.145  0.978  \nBILSTM-MTRAN-TCN 0.093  0.013  0.112  0.987  0.079  0.010  0.098  0.990  \n \nTABLE 8. COMPARISON OF EVALUATION INDEXES OF DIFFERENT METHODS IN ZHONGXIN SPECIAL STEEL AND FANGDA CARBON \nMethod \nZHONGXIN SPECIAL STEEL (SZ000708) FANGDA CARBON (SH600516) \nMAE MSE RMSE R2 MAE MSE RMSE R2 \nBILSTM 0.082  0.012  0.109  0.988  0.081  0.012  0.109  0.988  \nCNN-BILSTM 0.256  0.129  0.359  0.866  0.263  0.133  0.365  0.862  \nCNN-BILSTM-AM 0.255  0.131  0.362  0.864  0.265  0.135  0.367  0.860  \nBILSTM-SA-TCN 0.109  0.019  0.137  0.981  0.122  0.023  0.153  0.976  \nBILSTM-MTRAN-TCN 0.075  0.010  0.099  0.990  0.104  0.016  0.128  0.983  \n \nTABLE 9. COMPARISON OF EVALUATION INDEXES OF DIFFERENT METHODS IN TIANQI LITHIUM INDUSTRY AND YUNNAN COPPER INDUSTRY \nMethod \nTIANQI LITHIUM INDUSTRY (SZ002466) YUNNAN COPPER INDUSTRY (SZ000878) \nMAE MSE RMSE R2 MAE MSE RMSE R2 \nBILSTM 0.081  0.012  0.108  0.988  0.081  0.012  0.109  0.988  \nCNN-BILSTM 0.258  0.131  0.361  0.864  0.269  0.137  0.370  0.857  \nCNN-BILSTM-AM 0.277  0.141  0.376  0.853  0.303  0.157  0.396  0.837  \nBILSTM-SA-TCN 0.113  0.020  0.140  0.980  0.114  0.020  0.142  0.979  \nBILSTM-MTRAN-TCN 0.059  0.007  0.082  0.993  0.057  0.008  0.088  0.992  \n \nTABLE 10. COMPARISON OF EVALUATION INDEXES OF DIFFERENT METHODS IN CHINA PETROLEUM AND YUEYANG XINGCHANG \nMethod \nCHINA PETROLEUM (SH601857) YUEYANG XINGCHANG (SZ300164) \nMAE MSE RMSE R2 MAE MSE RMSE R2 \nBILSTM 0.081  0.012  0.108  0.988  0.083  0.012  0.110  0.987  \nCNN-BILSTM 0.261  0.133  0.365  0.861  0.262  0.132  0.364  0.862  \nCNN-BILSTM-AM 0.284  0.146  0.382  0.849  0.303  0.157  0.396  0.837  \nBILSTM-SA-TCN 0.117  0.021  0.145  0.978  0.119  0.022  0.147  0.977  \nBILSTM-MTRAN-TCN 0.059  0.006  0.080  0.993  0.051  0.006  0.076  0.994  \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \n2 VOLUME XX, 2017 \nTABLE 11. COMPARISON OF EVALUATION INDEXES OF DIFFERENT METHODS IN BYD AND DONGFENG MOTOR \nMethod \nBYD (SZ002594) DONGFENG MOTOR (SH600006) \nMAE MSE RMSE R2 MAE MSE RMSE R2 \nBILSTM 0.082  0.012  0.109  0.988  0.081  0.012  0.109  0.988  \nCNN-BILSTM 0.271  0.137  0.370  0.857  0.256  0.130  0.360  0.865  \nCNN-BILSTM-AM 0.258  0.130  0.361  0.865  0.257  0.131  0.362  0.863  \nBILSTM-SA-TCN 0.111  0.019  0.140  0.980  0.121  0.022  0.149  0.977  \nBILSTM-MTRAN-TCN 0.068  0.008  0.087  0.992  0.048  0.006  0.076  0.994  \n2) INDEX STOCKS  \nIn addition to the Shanghai and Shenzhen stocks, this article \nalso uses index stocks to further compare a nd analyze the \nabove five methods  and the method of LS TM, further \nverifying that the proposed method is superior to other \nmethods and enhancing the coverage of the experiment. Using \nthe 5 index stocks in Table 1 as the test dataset, conduct 5 \nexperiments on each stock and each method. Then, calculate \nthe average value of five stocks for each method, as shown in \nTable 12. \nThe prediction ability of the six methods is ranked in \ndescending order:  BiLSTM-MTRAN-TCN, BiLSTM, \nBiLSTM-SA-TCN, LSTM, CNN-BiLSTM-AM a nd CNN -\nBiLSTM. The MAE, MSE, and RMSE values of BiLSTM -\nMTRAN-TCN are the smallest among all methods, while the \nR2 value is the largest among all methods. This shows that the \nprediction accuracy of the BiLSTM-MTRAN-TCN method is \noptimal. And the prediction performance of CNN-BiLSTM or \nCNN-BiLSTM-AM is not very ideal.  \nCompared with other methods, BiLSTM-MTRAN-TCN \nreduces the error by 27.36% to 88.4%, and increases R2  by \n1.5% to 12.4%. Compared to LSTM, BiLSTM has smal ler \nMAE and RMSE, while R2 is larger. Its MAE (0.121 vs 0.138) \ndecreases by 12.3%, RMSE ( 0.162 vs 0.185) decreases by \n12.2%, and R2 increases by 0.9% . Therefore, BiLSTM is \nsuperior to LSTM. When compared to BiLSTM-SA-TCN, the \nMAE of BiLSTM -MTRAN-TCN decrea ses from 0.139 to \n0.098, while RMSE decrease s from 0.185 to 0.118, and R 2 \nincreases from 0.963 to 0.986, an increase of 2.4%. This also \nindicates that the transformer's self -attention mechanism \nperforms better than SA's. \nTherefore, this method can better fit th e stock trend, \nincluding the stock index and individual stock trend, to better \npredict the stock price. \n \n \nTABLE 12. AVERAGE OF EVALUATION INDEXES  OF THE SIX METHODS \nMethod MAE MSE RMSE R2 \nCNN-BILSTM 0.268  0.120  0.342  0.877  \nCNN-BILSTM-AM 0.259 0.116 0.335 0.882 \nLSTM 0.138  0.037  0.185  0.962  \nBILSTM-SA-TCN 0.139  0.036  0.185  0.963  \nBILSTM 0.121  0.028  0.162  0.971  \nBILSTM-MTRAN-TCN 0.087  0.014  0.118  0.986  \nC. VALIDATION OF GENERALIZATION ABILITY  \nBased on the abo ve 14 Shanghai and Shenzhen stocks  \nexperimental data, conduct in -depth analysis on the \ngeneralization ability and accuracy of each method. It can be \nseen from Figure 5  that the R 2 of all six methods are \nconcentrated between 0.8 and 1.0 , and there are no singular \nvalues. The R2 of BiLSTM is the most concentrated, with most \nconcentrated at 0.988. The R2 of BiLSTM is between 0.987 \nand 0.988, with a median of 0.988 and an average of 0.988.  \nBut the R2 of BiLSTM-MTRAN-CNN is the closest to 1.0, \nindicating that its prediction accuracy is higher.  \nThe R 2 value of the BiLSTM -MTRAN-CNN method is \nbetween 0.983 and 0.99 6, with a median of 0.9 92 and an \naverage of 0.9 91. The R2 value of the BiLSTM -SA-TCN \nmethod is between 0.976 and 0.981, with a median of 0.9 77 \nand an average of 0.9 79. The R2 value of CNN -BiLSTM \nmethod is between 0.857 and 0.866, with a median of 0. 862 \nand an average of 0.862. The R2 value of CNN-BiLSTM-AM \nmethod is between 0.837 and 0.874, with a median of 0. 858 \nand an average of 0.857. It can be seen from the distribution \nof R2 that the BiLSTM-MTRAN-CNN method has no singular \nvalues and the values are relatively concentrated, indicating its \ngood generalization ability. In addition, it has a higher mean \nand median, and is closer to 1.0, indicating better accuracy.  \n \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \n2 VOLUME XX, 2017 \n \nFIGURE 5. Box diagram of R2 .for each method. The graph drawn based on the analysis of the test results of 14 Shanghai and Shenzhen stocks . \nD. VALIDATION OF TIMELINESS ISSUES \nExperiments were conducted on five index stocks in Table 1, \nand stock data from four different periods (2009.1~2020.12, \n2010.1~2021.12, 2011.1~2022.12, 2012.1~2023.5) were used \nto verify whether there is a problem with timeliness. That is to \nverify the stability of the method in the time dimension. \nFrom Figure 6, it can be observed that the error values of \nthe BiLSTM-MTRAN-TCN m ethod vary little at different \ntime periods. The average values of RMSE during different \nperiods were 0.120, 0.112, 0.160 and 0.137, respectively. The \naverage values of MAE were 0.086, 0.083, 0.114 and 0.098, \nrespectively. The average values of MSE were 0.016, 0.013, \n0.028 and 0.021, respectively. Their R2 differences are also \nvery small. From Figure 7, it can be seen in detail that the R2 \nof each index stock in the four time periods have very small \nchanges (each time period is marked with 'a', 'b', 'c',  'd' for \ndifferentiation, and the same identifier means the same time \nperiod). The average values of R2 at different time periods are \n0.984, 0.987, 0.973 and 0.978, respectively. It can be seen that \nthe R2 fluctuations of different stocks during different time \nperiods are not significant and relatively stable. \n \nFIGURE 6. Error indicator values for each time period. MSE uses the secondary coordinates on the right  \n \nFIGURE 7. The R2 of different stocks at different time periods. Each time period is marked with 'a', 'b', 'c','d' for differentiation, and the same identifier \nis used for the same time period. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \n2 VOLUME XX, 2017 \nIn addition, by calculating the variance of each index stock \nover four time periods, the dispersion of different time periods \ncan be reflected, as shown in Table 13. The variance of MAE \nis between 0.0002 and 0.0008, the variance of MSE is between \n0.0000 and 0.0002, the variance of RMSE is between 0.0003 \nand 0.0016, and the variance of R2 is between 0.0000 and \n0.0002. The variance of each index approaches 0, indicating \nthat the deviation of the five index stocks at different time \nperiods is very small. \nTABLE 13. VARIANCE OF EVALUATION INDEXES FOR 5 INDEX STOCKS \n(OVER FOUR TIME PERIODS) \nINDEX CODE  MAE MSE RMSE R2 \n000001 S2 0.0006  0.0001  0.0009  0.0001 \n000002 S2 0.0008  0.0002  0.0016  0.0002 \n399001 S2 0.0002  0.0000  0.0003  0.0000 \n399006 S2 0.0002  0.0000  0.0005  0.0000 \n399300 S2 0.0002  0.0000  0.0004  0.0000 \nAccording to the above analysis, the error indexes values of \ndifferent stocks at different time periods only show small \nfluctuations, and the prediction results are relatively stable. \nThis indicates that the BiLSTM -MTRAN-TCN method \nperforms well in processing new data, has high accuracy and \ngeneralization ability, and does not have timeliness issues. \nE. OPTIMAL EPOCH VALUE \nThe training performance of neural network models usually \nincreases with the number of training rounds, but when it \nreaches a certain level, the performance tends to stabilize or \nactually decreases. In order to obtain the optimal epoch value \nand achieve the best prediction effect of this method,  14 \nShanghai and Shenzhen stocks from Table 2 were used fo r \nexperiments. Comparative analysis was conducted on 5 \ndifferent scenarios as shown in Figure 8. The closer the R2 is \nto 1, the better the performance of the method. When \nepoch=500, there is a significant increase in R2 compared to \nepoch=200 and epoch=300. However, when epoch=600, the \nR2 of most stocks is basically the same as when epoch=500, \nand it will not increase due to the increase in rounds. So it can \nbe seen that epoch=500 is the optimal parameter value. \n \nFIGURE 8. The R2 of 14 Shanghai and Shenzhen stocks in epoch=200, 300,500 and 600. \nVI. CONCLUSION \nThe BiLSTM-MTRAN-TCN method proposed in this article \nis used to predict the closing price of stocks . This method \nmodifies the transformer model, removes Input Embedding, \nreplaces the original decoder part with TCN layer a nd fully \nconnected layer, treats the output of the encoder as the only \ninput of the decoder, and cancels other inputs. After Position \nEncoding Layer processing, the data is first processed by \nBiLSTM to capture sequence dependent signals, and then sent \nto the modified transformer (MTRAN-TCN) for processing. \nMixing multiple models can fully utilize the advantages of \neach model while avoiding their drawbacks and improving \nprediction accuracy. \nIn this article, experiments were conducted on the \neffectiveness of introducing BiLSTM, the improvement effect \nof transformer, the accuracy of the method, the generalization \nability and timeliness issues. Experiments have shown that the \nimproved transformer (MTRAN -TCN) needs to be mixed \nwith BiLSTM to achieve optimal performance, and the \nintroduction of BiLSTM has a significant effect on stock \nprediction. \nCompared with LSTM„ÄÅBiLSTM, CNN-BiLSTM, CNN-\nBiLSTM-AM, BiLSTM -SA-TCN in the literature , t he \nBiLSTM-MTRAN-TCN method has the best predictive \nperformance. Five representative index stocks and 14 \nShanghai and Shenzhen stocks were selected for the \nexperiments, among which 14 stocks were selected from 7 \nmajor categories using a bidirectional stock selection strategy. \nIn the index stock experiment, the results show that the R2 of \nBiLSTM-MTRAN-TCN method is increased by 1.5% to 12.4% \ncompared to other methods . In the Shanghai and Shenzhen \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \nVOLUME XX, 2017 7 \nstock experiments, the R2  value of this method is the best in \n85.7% of the stock dataset, and RMSE is the best in 78.6% of \nthe dataset. RMSE decrease s by 24.3% to 9 3.5%, and R 2 \nincreases by 0.3% to 15.6%. \nIn the timeliness experiment of 5 index stocks  over four \ndifferent time periods, the error indexes values of different \nstocks at different time periods only show small fluctuations, \nand the prediction results are relatively stable. This indicates \nthat the BiLSTM -MTRAN-TCN method performs well in \nprocessing new data, has high accuracy and generalization \nability, and does not have timeliness issues. \nVII. DICUSSION \nHowever, this method can further improve the prediction \nperformance of some individual stocks. Future research work \nwill continue in the following three aspects: \n‚ö´ Further optimize the neural network structure. \n‚ö´ Integrating multiple data sources for prediction: stock \nprices, various i ndexes data or fundamental \ninformation. \n‚ö´ Considering multiple time scale information: \nCurrently, only one type of time window length data \nis considered for its impact. In the future, not only \nshould we consider the impact of stock data from 10 \ndays ago, but we can also consider the impact of stock \ndata from 7, 30 and 150 days ago. \nACKNOWLEDGMENT \nAcknowledge Xiaomen University Tan Kah Kee College \nHongnet Lab for providing support for GPU server resources. \nREFERENCES \n[1] Hu. Z, Liu. W, Bian. J, Liu. X and Liu. T. Y, ‚ÄúListening to chaotic \nwhispers: A deep learning framework for news -oriented stock trend \nprediction,‚Äù in WSDM 2018  - proceedings of the 11th ACM \ninternational conference on web search and data mining, Marina Del \nRey, CA, pp. 261‚Äì269, 2018. \n[2] Li. J, Pan. S, Huang. L and Zhu. X, ‚ÄúA machine learning based method \nfor customer behavior prediction ,‚Äù Tehnicki Vjesnik -Technical \nGazette, vol. 26, no.6, pp. 1670‚Äì1676, 2019. \n[3] Xiao. C, Xia. W and Jiang. J, ‚ÄúStock price forecast based on combined \nmodel of ARI -MA-LS-SVM,‚Äù Neural Computing & Applications m, \nvol. 32, no.10, pp. 5379-5388, 2020.  \n[4] Yu. P and Yan. X, ‚ÄúStock price prediction based on deep neural \nnetworks,‚Äù Neural Computing & Applications m, vol.  32, no.6, pp. \n1609-1628, 2020. \n[5] Zhang. Q, Qin . C, Zhang . Y, Bao . F, Zhang . C and Liu. P, \n‚ÄúTransformer-based attention network for stock movement prediction,‚Äù \nExpert Systems with Applications, vol. 202, no. 117239, 2022. \n[6] Rao T S, Gabr M M, ‚ÄúAn introduction to bispectral analysis and \nbilinear time series models,‚Äù New York: Springer, 2012. \n[7] C. N. Babu, B. E. Reddy, ‚ÄúPrediction of selected Indian stock using a \npartitioning-interpolation based ARIMA -GARCH model,‚Äù. Applied \nComputing and Informatics, vol. 11, no. 2, pp. 130-143, 2015. \n[8] Lu. W, Li. J, Wang. J, and Qin. L, ‚ÄúA CNN-BiLSTM-AM method for \nstock price prediction,‚Äù Neural Computing & Applicationsm, vol. 33, \nno.10, pp. 4741-4753, 2020.  \n[9] Xu Haoran, Xu Bo, Xu Kewen. Overview of the application of \nmachine learning in stock forecasting[J]. Computer Engineering and \nApplications, vol. 56, no.12, pp. 19‚Äì24, 2020. \n[10] Li. J, Pan. S, Huang. L and Zhu. X, ‚ÄúA machine learning based method \nfor customer behavior prediction ,‚Äù Tehnicki Vjesnik -Technical \nGazette, vol. 26, no.6, pp. 1670‚Äì1676, 2019. \n[11] S. Mehtab, J. Sen and A. Dutta, ‚ÄúStock price prediction using machine \nlearning and LSTM-based deep learning models,‚Äù in: S. M. Thampi, S. \nPiramuthu, KC. Li, S. Berretti, M. Wozniak, D. Singh (eds.), Machine \nLearning and Metaheuristics Algorithms, and Applications. SoMMA \n2020. Communications in Computer and Information Science , vol. \n1366, pp. 88‚Äì106, 2021.  \n[12] Xu Haoran, Xu Bo, Xu Kewen , ‚ÄúOverview of the application of \nmachine learning in stock forecasting ,‚Äù Computer Engineering and \nApplications, vol. 56, no. 12, pp. 19‚Äì24, 2020.  \n[13] WANG D X, LIU X, WANG M D, ‚ÄúA DT-SVM Strategy for Stock \nFutures Pr ediction with Big Data ‚Äù Computational Science and \nEngineering. Sydney, NSW, Australia: IEEE, 2013: 1005-1012.  \n[14] CHEN Y J,  HAO Y T, ‚ÄúA feature weighted support vector machine \nand K-nearest neighbor algorithm for stock market indices prediction, ‚Äù \nExpert Systems with Applications, vol. 80, pp. 340-355, 2017. \n[15] Yan Zhengxu, Qin Chao, Song Gang , ‚Äú Stock price prediction of \nstochastic forest model based on Pearson feature selection,‚Äù Computer \nEngineering and Application, vol. 57, no. 15, pp. 286‚Äì296, 2021. \n[16] Wang. P, Lou. Y and Lei. L, ‚ÄúResearch on Stock Price Prediction \nBased on BP Wavelet Neural Network with Mexico Hat Wavelet \nBasis,‚Äù in Proceedings of the 2017 International Conference on \nEducation, Economics and Management Research (ICEEMR 2017) , \nSingapore, pp. 99-102, 2017. \n[17] Hu.Y, ‚ÄúStock market timing model based on convolutional neural \nnetwork‚ÄîTaking Shanghai Composite Index as an example,‚Äù \nFinance Econ, vol. 26, no.4, pp. 71‚Äì74, 2018. \n[18] S. Jain, R. Gupta and A. Moghe, ‚ÄúStock price prediction on daily stock \ndata using  deep neural networks,‚Äù in Proceedings of the 2018 \nInternational Conference on Advanced Computation and \nTelecommunication (ICACAT), Bhopal, India, December 2018. \n[19] Yang Qing, Wang Chenwei , ‚Äú Research on global stock index \nprediction based on deep learning LSTM neural network, ‚Äù Statistical \nResearch, vol. 36, no.3, pp. 65‚Äì77, 2019. \n[20] S. Mehtab, J. Sen and A. Dutta, ‚ÄúStock price prediction using machine \nlearning and LSTM-based deep learning models,‚Äù in S. M. Thampi, S. \nPiramuthu, KC. Li, S. Berretti, M. Wozniak, D. Singh (eds.), Machine \nLearning and Metaheuristics Algorithms, and Applications. SoMMA \n2020, Communications in Computer and Information Science , vol. \n1366, pp. 88‚Äì106, 2021. \n[21] J. Qiu, B. Wang  and C. Zhou, ‚ÄúForecasting stock prices with long -\nshort term memor y neural network based on attention mechanism,‚Äù \nPLoS One, vol. 15, no. 1, Article ID e0227222, 2020. \n[22] A. Vaswani, ‚ÄúAttention is all you need,‚Äù in Proceedings of the 31st \nConference on Neural Information Processing Systems (NIPS 2017) , \nLong Beach, CA, USA, 2017. \n[23] Ding. Q, Wu. S, Sun. H, Guo. J and Guo. J, ‚ÄúHierarchical Multi-Scale \nGaussian Transformer for Stock Movement Prediction ,‚Äù in \nProceedings of the Twenty -Ninth International Joint Conference on \nArtificial Intelligence, Electr Network, pp.4640-4646, 2020. \n[24] Farsani. R and Pazouki. E, ‚ÄúA Transformer Self -attention Model for \nTime Series Forecasting,‚Äù J. Electr. Comput. Eng. Innovations, vol. 9, \nno.1, pp. 1‚Äì10, 2021. \n[25] Peng. Z.-Y and Guo. P.-C, ‚ÄúA Data Organization Method for LSTM \nand Transformer When Predicting Chinese Banking Stock Prices,‚Äù \nDiscrete Dynamics in Nature and Society, vol. 2022, pp. 1‚Äì8, 2022. \n[26] Wang. C, Chen . Y, Zhang . S and Zhang. Q, ‚ÄúStock market index \nprediction using deep Transformer model,‚Äù Expert Systems with \nApplications, vol. 208, 118128, 2022. \n[27] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural \nComputation, vol. 9, no. 8, pp. 1735‚Äì1780, 1997. \n[28] Lea C, Vidal R, Reiter A, et al. ‚ÄúTemporal convolutional networks: a \nunified approach to action segmentation ,‚Äù in Lecture Notes in \nComputer Science. Cham: Springer International Publishing, \nBaltimore, USA, 2016: 47-54 \n[29] Chen Zh e, Liu Jiahua, Zhao Bin, et al , ‚ÄúMulti factor urban road \nnetwork taxi demand pred iction based on GCN and TCN ,‚Äù Control \nand Decision, vol. 38, no. 04, pp. 1031-1038, 2023. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                               Shuzhen Wang: A Stock Price Prediction Method Based on BiLSTM and Improved Transformer  \nVOLUME XX, 2017 7 \n[30] Wang Jun, Gao Zixun, Shan Chunyi. , ‚ÄúMultivariate prediction of the \nYellow River runoff base d on the TCN -Attention model,‚Äù People's \nYellow River, vol. 11, pp. 20-25, 2022. \n[31] Zhiyong Yang, Yuxi Ye and Yu Zhou, \"Application of BiLSTM-SA-\nTCN time series model  in stock forecasting\", Journal of Nanjing \ninformation engineering University (Natural Science Edition), pp. 1‚Äì\n12, April 2023. \n[32] Deng. J, Zhao. F, Wang. X. ‚ÄúMTICA-AEO-SVR model for stock price \nforecasting,‚Äù Computer Engineering and Applications, vol. 58, no. 8, \npp. 257-263, 2022. \n[33] Li. X, Cui. C, Song. G, et al. ‚ÄúStock trend prediction method based on \ntemporal hypergraph  convolutional neural network, ‚Äù. Journal of \nComputer Applications, vol.42, no. 3, pp. 797-803, 2022. \n[34] Zhang.T, Tang. Z, Wu. J. ‚Äú Forecasting method of stock index based \non optimized KELM model,‚Äù Statistics& Decision, vol. 37, no. 13, pp. \n148-150, 2021. \n[35] Gao Y, Wang R, Zhou E M. ‚ÄúStock prediction based on optimized \nLSTM and GRU models,‚Äù. Scientific Programming, vol. 2021, pp. 1-\n8, 2021. \n[36] Yadav A, Jha C K, Sharan A.  ‚ÄúOptimizing LSTM for time series \nprediction in Indian stock market, ‚Äù . Procedia Computer Science, vol. \n167, pp. 2091-2100, 2020. \n[37] Lu. W, Li. J, Li. Y, Sun. A and Wang. J, ‚ÄúA CNN-LSTM-Based Model \nto Forecast Stock Prices,‚Äù Complexity, vol. 2020, pp. 1‚Äì10, 2020. \n[38] Zeng. A and Nie. W, ‚ÄúStock Recommendation System Based on Deep \nBidirectional LSTM,‚Äù Computer Science, vol. 46, no.10, pp. 84 ‚Äì89, \n2019 \n \n  \nSHUZHEN WANG. received the B.S. degree in \nelectrical and automation from ChangAn \nUniversity, Xi'an, China, in 200 6 and the M.S. \ndegree in detection and automation device  from \nXiamen University, Xiamen, China, in 2009. She \nhas been working in data analysis and processing \nfor an internet company for about eight years. Her \nresearch interests  include facial detection and \napplications, data processing and stock prediction. \nFrom 2009 to 2018, she worked in data analysis \nand processing for an internet company.From 2018 \nto now, She works in Xiamen University Tan Kah Kee college. And she has \nbeen researching the deep learning networks for stock prediction. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3296308\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6717269420623779
    },
    {
      "name": "Computer science",
      "score": 0.585246741771698
    },
    {
      "name": "Stock (firearms)",
      "score": 0.43209266662597656
    },
    {
      "name": "Data mining",
      "score": 0.3579341173171997
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35624366998672485
    },
    {
      "name": "Engineering",
      "score": 0.10928419232368469
    },
    {
      "name": "Electrical engineering",
      "score": 0.08140075206756592
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I191208505",
      "name": "Xiamen University",
      "country": "CN"
    }
  ],
  "cited_by": 73
}