{
    "title": "Word Frequency Does Not Predict Grammatical Knowledge in Language Models",
    "url": "https://openalex.org/W3102684881",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2157172445",
            "name": "Charles Yu",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A3096153771",
            "name": "Ryan Sie",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A2040888565",
            "name": "Nicolas Tedeschi",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A2190439507",
            "name": "Leon Bergen",
            "affiliations": [
                "University of California, San Diego"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2607879133",
        "https://openalex.org/W2060277733",
        "https://openalex.org/W2962961857",
        "https://openalex.org/W2971044268",
        "https://openalex.org/W2340992439",
        "https://openalex.org/W2910243263",
        "https://openalex.org/W2963751529",
        "https://openalex.org/W3035295386",
        "https://openalex.org/W2039217078",
        "https://openalex.org/W2531882892",
        "https://openalex.org/W2918996109",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2193149",
        "https://openalex.org/W4288631803",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W4233424333",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2964222268",
        "https://openalex.org/W170935930",
        "https://openalex.org/W1579035156",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2038721957",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2888922637",
        "https://openalex.org/W2036963181",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2921890305",
        "https://openalex.org/W75403801",
        "https://openalex.org/W2798727047"
    ],
    "abstract": "Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models’ accuracy. Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models. Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun’s performance on grammatical tasks. Finally, we find that a novel noun’s grammatical properties can be few-shot learned from various types of training data. The results present a paradox: there should be less variation in grammatical performance than is actually observed.",
    "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4040–4054,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n4040\nWord Frequency Does Not Predict Grammatical Knowledge in Language\nModels\nCharles Yu, Ryan Sie, Nico Tedeschi, and Leon Bergen\n{cty002,rdsie,njtedeschi,lbergen}@ucsd.edu\nUniversity of California, San Diego\nAbstract\nNeural language models learn, to varying de-\ngrees of accuracy, the grammatical properties\nof natural languages. In this work, we in-\nvestigate whether there are systematic sources\nof variation in the language models’ accuracy.\nFocusing on subject-verb agreement and re-\nﬂexive anaphora, we ﬁnd that certain nouns\nare systematically understood better than oth-\ners, an effect which is robust across grammati-\ncal tasks and different language models. Sur-\nprisingly, we ﬁnd that across four orders of\nmagnitude, corpus frequency is unrelated to a\nnoun’s performance on grammatical tasks. Fi-\nnally, we ﬁnd that a novel noun’s grammatical\nproperties can be few-shot learned from vari-\nous types of training data. The results present\na paradox: there should be less variation in\ngrammatical performance than is actually ob-\nserved.\n1 Introduction\nNeural language models (Howard and Ruder, 2018;\nDevlin et al., 2019; Dai et al., 2019; Yang et al.,\n2019; Radford et al., 2019) have achieved success\nin both text prediction and downstream tasks such\nas question-answering, text classiﬁcation, and natu-\nral language inference. The strong performance of\nthese models raises scientiﬁc questions about the\nknowledge they have acquired, in particular, about\nthe abstractness and generality of their linguistic\nrepresentations.\nPrevious work has investigated the linguistic rep-\nresentations of neural language models in several\ndomains, and found varying evidence for how lin-\nguistically adequate these representations are (Lau\net al., 2017; Marvin and Linzen, 2018; Goldberg,\n2019; Futrell et al., 2019). This work has em-\nployed psycholinguistic methodology in order to\nelicit grammatical judgments from these models,\ninferring the models’ underlying representations\nfrom the patterns of judgments.\nIn the current work, we focus on the variation\nin grammatical knowledge that potentially exists\nwithin a neural language model. Just as in human\npsycholinguistic tasks, previous work on neural\nLMs has observed variability in grammatical judg-\nments between different sentences; not all viola-\ntions of a grammatical constraint are judged to be\nequally bad. It is not clear, however, whether there\nare systematic sources of variation in these judg-\nments, and if so, what the sources are.\nWe will focus on variation among lexical items,\nusing English subject-verb agreement and reﬂexive\nanaphora as a case study. We ﬁrst ask whether lan-\nguage models learn the grammatical properties of\nsome nouns more accurately than for others. We do\nthis by measuring the accuracy of language models\nwhen making grammatical judgments involving dif-\nferent nouns. We ﬁnd systematic variation among\nnouns: nouns that perform well on one task or lan-\nguage model are more likely to perform well on\nother tasks or other language models. We then\nconsider possible sources of the observed varia-\ntion between nouns, ﬁnding that the grammatical\nproperties of nouns are paradoxically easy to learn;\nour results suggest that there should be much less\nvariation than is actually observed.1\nRelated work\nA number of other studies have investigated the\nlinguistic representations of neural models, both\nlanguage models speciﬁcally and networks trained\nusing other objectives. Linzen et al. (2016); Gulor-\ndava et al. (2018); Kuncoro et al. (2018) probe the\nability of LSTMs to learn hierarchical structures.\nWarstadt et al. (2019b) introduces a large-scale cor-\npus of grammatical acceptability judgements, trains\nRNNs to predict these judgments, and concludes\n1All code and experimental materials are avail-\nable at https://github.com/CharlesYu2000/\nlm-variation\n4041\nthat the models outperform unsupervised baselines,\nbut fall far short of human performance. Lepori\net al. (2020) ﬁnds that tree-based RNNs outper-\nform sequential RNNs on number prediction tasks,\nbut that ﬁne-tuning on an artiﬁcially-generated aug-\nmentation set can bring the models closer to parity.\nOther work has focused on probing whether\nneural language models have acquired adequate\nrepresentations of speciﬁc linguistic phenomena.\nMarvin and Linzen (2018) and Goldberg (2019)\nuse a minimal pair methodology to assess the\ngrammatical knowledge of RNNs and BERT, look-\ning at subject-verb number agreement, reﬂexive\nanaphora, and negative polarity items. Wilcox et al.\n(2018) examines whether RNN language models\nexhibit wh-licensing interactions on surprisal as-\nsociated with gaps, concluding they can represent\nlong-distance ﬁller-gap dependencies and learn cer-\ntain island constraints. Futrell et al. (2019) studies\nwhether neural language models show evidence\nfor incremental syntactic state representations us-\ning psycholinguistic methodology. Warstadt et al.\n(2019a) studies BERT’s knowledge of NPI’s, fo-\ncusing on differences between tasks: boolean clas-\nsiﬁcation (e.g. Linzen et al. 2016 and Warstadt\net al. 2019b), minimal pair comparisons (e.g. Mar-\nvin and Linzen 2018 and Wilcox et al. 2019), and\nprobing tasks (e.g. Giulianelli et al. 2018).\n2 Approach\nWe use the minimal pair methodology of Mar-\nvin and Linzen (2018) in order to investigate the\ngrammatical judgments of neural language models.\nGiven a minimal pair of sentences, i.e. a pair that\ndiffer from each other in their acceptability due to a\ndifference in just one grammatical property. If the\nmodel understands the grammatical phenomenon\nbeing studied, it should assign higher probability to\nthe grammatical sentence than to the ungrammati-\ncal sentence.\n2.1 Grammatical tasks\nTable 1 shows the 10 grammatical tasks (Marvin\nand Linzen, 2018) and the templates used for gener-\nating minimal pairs. The tasks fall into two general\ncategories: subject-verb agreement (SV A) and re-\nﬂexive anaphora (RA). The ﬁrst SV A task, SV A\nSimple, probes whether the model understands that\nsubject number must agree with the number of\nthird-person present verbs:\n(1) a. The cat walks.\nb. *The cat walk.\nThe other SV A tasks probe whether the models\nhave more sophisticated representations of number\nagreement. For example, the SV A PP task mea-\nsures whether the model is able to ignore distrac-\ntors (“boys”) which occur between the head of the\nsubject and the verb:\n(2) a. The cat next to the boys jumps.\nb. *The cat next to the boys jump.\nThe object relative clause tasks probe whether the\nmodel accurately maintains the head’s number in\nthe presence of an embedded clause. Marvin and\nLinzen (2018) provide extensive discussion of the\nlinguistic motivation for these tasks.\nThe RA tasks measure whether the language\nmodel understands the structural conditions on the\nbinding of reﬂexive pronouns. The tasks make use\nof the following property of English reﬂexives: a\nreﬂexive pronoun needs to agree in number with\nits antecedent. The RA Sent.Comp task evaluates\nwhether the model understands that reﬂexives must\nbe in the same clause as their antecedents:\n(3) a. The lawyers said the defendant incrim-\ninated himself.\nb. * The lawyers said the defendant incrim-\ninated themselves.\nThe RA tasks involving object relative clauses eval-\nuate whether the models understand that reﬂexive\nanaphora do not bind to the noun in an embedded\nclause but rather to the head noun.\n2.2 Measuring the performance of a noun\nWe use these tasks in order to measure how well\nthe model understands the grammatical properties\nof a particular target noun. Given a speciﬁc target\nnoun, it is substituted as the TargetNoun in each of\nthe task templates shown in Table 1. This gives a\npartially speciﬁed template. For example, substi-\ntuting the target noun “zombie” in the SV A Simple\ntemplate results in:\n(4) The zombie ⟨Verb⟩.\nGiven each of these partially speciﬁed templates,\n500 minimal pairs are randomly sampled by ﬁlling\nin the remaining lexical items. Finally, the model’s\n4042\nTask Template\nSV A Simple The⟨TargetNoun⟩⟨Verb⟩.\nSV A Subj.Rel.Clause The⟨TargetNoun⟩that liked the⟨Noun⟩⟨Verb⟩.\nSV A Sent.Comp. The⟨Noun⟩said the⟨TargetNoun⟩⟨Verb⟩.\nSV A PP The⟨TargetNoun⟩next to the⟨Noun⟩⟨Verb⟩.\nSV A Obj.Rel.Clause.That The⟨TargetNoun⟩that the⟨Noun⟩liked⟨Verb⟩.\nSV A Obj.Rel.Clause.NoThatThe⟨TargetNoun⟩the⟨Noun⟩liked⟨Verb⟩.\nRA Simple The⟨TargetNoun⟩⟨PastTransVerb⟩⟨himself/themselves⟩.\nRA Sent.Comp. The⟨NonGenderedNoun⟩said the⟨TargetNoun⟩⟨PastTransVerb⟩⟨himself/themselves⟩.\nRA Obj.Rel.Clause.That The⟨TargetNoun⟩that the⟨NonGenderedNoun⟩liked⟨PastTransVerb⟩⟨himself/themselves⟩.\nRA Obj.Rel.Clause.NoThatThe⟨TargetNoun⟩the⟨NonGenderedNoun⟩liked⟨PastTransVerb⟩⟨himself/themselves⟩.\nTable 1: Templates used for sentence generation. TargetNoun indicates the position of the target noun whose\nperformance score is being calculated.\ngrammatical judgments on the 500 minimal pairs\nare computed (by taking the difference in scores\nbetween the grammatical and ungrammatical vari-\nants) and averaged, resulting in a task performance\nscore for the noun.\n2.3 Limitations\nThese analyses are limited in several respects. First,\nonly two grammatical tasks are used. By using a\nwider range of tasks, it will be possible to investi-\ngate a larger set of grammatical phenomena outside\nof number agreement.\nSecond, while the study focuses on the gram-\nmatical information carried by nouns, other lexical\ntypes such as verbs are likely to carry this informa-\ntion as well. Future work can determine whether\nthe approach generalizes to verbs and other lexical\ntypes.\nFinally, while the study uses acceptability judg-\nments in order to determine the models’ grammati-\ncal knowledge, other probing tasks exist and may\nproduce different results (Warstadt et al., 2019a).\nWe use acceptability judgments because, to the best\nof our knowledge, feature probing has not been ex-\ntensively studied for GPT-2 or Transformer-XL.\nDifferent probing architectures may produce differ-\nent results for these models. It would be desirable\nto understand the robustness of the current results\nto the choice of experimental readout.\n3 Methods\nIn this section we describe the process of calculat-\ning a target noun’s task performance score in more\ndetail.\n3.1 Sentence generation\nUsing WordNet (Fellbaum, 1998) and VerbNet\n(Schuler, 2005), we compiled a list of lexical items\nas shown in Table 2. The target nouns were drawn\nfrom the Noun list, which consisted of animate\nnouns. Only nouns with distinct singular and plu-\nral forms were included. All verbs in the Verb set\nhave an intransitive reading. For each pair of task\ntemplate and target noun, 500 sentences were ran-\ndomly sampled by choosing lexical items from the\nappropriate word lists.\nFor each sampled sentence, 2*2 or 2*2*2 ver-\nsions were generated (depending on the template).\nThese versions varied the grammaticality of the\nsentence and the plurality of the target noun and\nany distractor nouns. For example, for the SV A\nSimple task, 2*2 versions are generated for every\nsampled sentence:\n(5) a. Singular-Grammatical: The horse walks.\nb. Singular-Ungrammatical: *The horse\nwalk.\nc. Plural-Grammatical: The horses walk.\nd. Plural-Ungrammatical: *The horses\nwalks.\n3.2 Models\nOur experiments use three models, Transformer-\nXL (Dai et al., 2019), GPT-2 (Radford et al., 2019),\nand BERT (Devlin et al., 2019). We use the Hug-\nging Face implementations (Wolf et al., 2019) with\nthe pre-trained models transfo-xl-wt103, which is\ntrained on the WikiText-103 dataset,gpt2-xl, which\nis trained on the WebText dataset, and bert-base-\nuncased, which is trained on BookCorpus and En-\nglish Wikipedia.\n3.3 Sentence scoring\nWe now describe how a score was calculated for\na particular sampled sentence. For each of the\n4043\nSet Name Transformer-XLGPT-2 BERT\nNoun 916 723 704\nVerb 615 228 406\nNonGenderedNoun 870 679 663\nPastTransVerb 1298 1034 1298\nTable 2: Size of word sets for each model.\nsentence variants (e.g. Example 5), the model com-\nputes a score. In the case of Transformer-XL and\nGPT-2, this score is simply the the log probability\nof the string. For example, for Transformer-XL:\nScorestring(s) = logPTXL(s) (1)\nwhere PTXL is the Transformer-XL language model\nprobability distribution.\nFor BERT, given its masked language model ar-\nchitecture, we follow the approach of Goldberg\n(2019). For the SV A tasks, we compute the log\nconditional probability of the verb whose number\nmust agree with the target noun. For the RA tasks,\nwe compute the log conditional probability of the\nreﬂexive pronoun. Both conditional probabilities\nare computed conditional on the left and right con-\ntexts.\nGiven the scores for a sentence’s variants, we\ncompute an overall score for the sentence, which\ncaptures how much the model prefers the grammat-\nical variants to the ungrammatical variants. For\neach sampled sentence S, there are either 2 or 4\nminimal pairs among its variants. In Example 5, a.\nand b. is a minimal pair, and c. and d. is a minimal\npair. Letting sa, ..., sd denote these variants, the\noverall score for the sentence is given by:\nScoresent(S) =1\n2(Scorestring(sa) −Scorestring(sb)\n+Scorestring(sc) −Scorestring(sd))\nThe formula when there are four minimal pairs is\nsimilar.\n3.4 Noun scoring\nWe next compute an overall score for the target\nnoun. As described in Section 3.1, for a speciﬁc\ntarget noun n and task, we sample 500 sentences\nS1, ..., S500. The noun’s score for this task is then\ngiven by:\nScorenoun(n) = 1\n500\n500∑\ni=1\nScoresent(Si) (2)\n3.5 Word ﬁltering and tokenization\nWords were removed from a particular model if\neither their singular or plural form was tokenized\nto unk, or if their singular and plural forms were\nassigned different numbers of tokens.2 For BERT,\nwords in the Verb set were removed if they were\nassigned more than one token, as BERT does not\nmodel the joint distribution over multiple masked\ntokens.\nFor Transformer-XL, we add a padding text 3\nand a start-of-sentence-token (⟨SOS⟩) to the begin-\nning of the sentence and an end-of-sentence token\n(⟨EOS⟩) to the end of the sentence. For GPT-2, we\nmake no modiﬁcations to the generated sentence\n(although preﬁx spaces are added to the strings for\ntokenization purposes). For BERT, since it is a\nmasked language model, we replace the Verb (for\nSV A) or reﬂexive pronoun (for RA) with a [MASK]\ntoken after tokenization. Thus, each sentence will\nhave a single mask token corresponding to the word\nthat should agree with the target noun.\n4 Results\n4.1 Noun performance is correlated across\ntasks\nWe ﬁrst examine how each noun’s performance\nvaries across the grammatical tasks. For each noun-\ntask pair, we measure the average performance of\nthe noun on that task, as described above. This\ngives 10 features per noun, corresponding to the 10\ngrammatical tasks.\nFigure 1 shows the pairwise comparisons be-\ntween performance on the different tasks for\nTransformer-XL. Results for BERT and GPT-2 are\nsimilar and are shown in the appendix. The ﬁg-\nure shows that performance is correlated across the\ntasks; for many pairs of tasks, nouns which have\nhigher performance on one task are likely to have\nhigher performance on the other.\nUsing principal component analysis, we found\nthat a single principal component explains 47% of\ntask variance for Transformer-XL, and two prin-\ncipal components explain 73%. Results are sim-\nilar for BERT and GPT-2, and are shown in the\nappendix. The ﬁrst PC primarily measures per-\nformance on the four reﬂexive anaphora tasks,\nwhile the second PC measures performance on the\nsubject-verb agreement across relative clause tasks.\n2The latter constraint was used in order to simplify batch-\ning.\n3https://tinyurl.com/y9kjuj5q\n4044\nFigure 1: Pairwise comparisons between tasks with\nTransformer-XL. Rows and columns represent tasks,\nand one point represents a single noun’s performance\non a pair of tasks. The four tasks on the lower\nright, with strongest correlations, all involve reﬂexive\nanaphora.\nThis suggests that there is a dimension that char-\nacterizes whether the model understands how re-\nﬂexive binding constraints operate for a noun, and\na dimension for whether the model understands\nsubject-verb agreement for the noun. Note that\nFigure 1 additionally demonstrates correlations be-\ntween the reﬂexive tasks and the subject-verb agree-\nment tasks.\nThese results provide evidence that language\nmodels’ variation in performance on the grammat-\nical tasks is, in part, explained by properties of\nthe nouns which are stable across tasks. The mod-\nels understand number agreement better for some\nnouns, and worse for others.\n4.2 Noun performance is correlated across\nmodels\nWe next investigate whether nouns exhibit stable\nbehavior across different neural language models.\nFor each pair of the three language models, we\nmeasured how well a noun’s task performance in\none language model predicted its task performance\nin the other language model.\nFigure 2 shows comparisons between pairs of\nlanguage models on the 10 grammatical tasks. Of\nthe 30 comparisons, 24 show signiﬁcant positive\ncorrelations between the pairs of language mod-\nels. 22 of the correlations remain signiﬁcant after\nBonferroni correction.\nGPT-2 and Transformer-XL show the strongest\ncorrelation in performance. It is possible that this is\ndue to methodological differences between the task\nsetup for GPT-2 and Transformer-XL compared to\nBERT: GPT-2 and Transformer-XL are performing\na language modeling task in which the probability\nof a full sentence is queried, while BERT performs\nmasked language modeling on a single target word.\nThe difference may also be due to corresponding\ntraining differences between BERT and the autore-\ngressive language models.\nThe results provide evidence that nouns exhibit\nstable task performance across language models.\nThe source of the correlation across language mod-\nels must come from features of the training data.\nProperties of the natural text distribution of nouns\nlead some of these nouns to be better understood\nthan others.\n4.3 Effect of frequency on task performance\nIn Sections 4.1 and 4.2, we found evidence that\nnouns exhibit stable performance across different\ngrammatical tasks and language models. One obvi-\nous explanation of these results is that nouns vary\nin their frequency in natural text, and language\nmodels learn more accurate grammatical represen-\ntations for more frequent nouns.\nIn order to investigate this, we measured the fre-\nquency of each noun in two corpora: WikiText-103,\na 103 million token subset of Wikipedia, which was\nused for training Transformer-XL; and Open Web-\nText (Gokaslan and Cohen, 2019), an open-source\nimplementation of the web corpus used to train\nGPT-2.4 Word frequencies were measured sepa-\nrately for singular and plural noun forms. Figure 3\nshows the relationship between frequency and task\nperformance on each of the ten grammatical tasks.\nThe appendix shows the results broken down by\ntask type.\nThe results show no clear relationship between\nnoun frequency and task performance. Frequency\nexplains no more than 0.1% of the variation in\nperformance. This holds true over more than four\norders of magnitude in frequency. This provides\nevidence that 1) differences in corpus frequency\ndo not explain the systematic differences observed\nbetween nouns, and 2) relatively few observations\nsufﬁce for transformer language models to learn\n4BERT was trained on a mix of Wikipedia text and Book-\nCorpus. Because, as of this writing, BookCorpus is no longer\ndistributed, WikiText-103 was used as a proxy for BERT train-\ning frequencies.\n4045\nFigure 2: Pairwise comparisons between GPT-2, Transformer-XL, and BERT on the 10 grammatical tasks. Each\nrow corresponds to a pair of language models, and each column is a single task. One point represents the perfor-\nmance of a noun on a single task.\nFigure 3: Relationship between corpus frequency and task performance for Transformer-XL, BERT, and GPT-2.\nPerformance scores are z-normalized. Colors indicate the ten grammatical tasks and singular/plural form of the\nnoun (s indicates singular, p indicates plural). Each point represents task performance for a single noun.\ncorrect number agreement behavior for a noun. In\nthe next section, we investigate this ﬁnding further.\n5 Few-shot learning for novel lexical\nitems\nThe results in the previous section provide evidence\nthat nouns systematically vary in their performance\non grammatical tasks; some nouns perform better\nthan others across tasks and language models. How-\never, this variation is not explained by frequency of\noccurrence in natural text. Nouns that occur on the\norder of 100 times in a corpus do not have system-\natically worse performance than nouns that occur\n106 times.\nThe results raise a question: if frequency does\nnot inﬂuence how well a noun is understood, what\ndoes? If low frequency nouns are understood as\nwell as higher frequency nouns, then this suggests\nthat language models few-shot learn the grammati-\ncal properties of nouns. We suggest that by study-\ning what makes a noun learnable in a few-shot\nsetting, it may be psosible to better understand the\nsources of the observed variation.\nWe use a few-shot learning paradigm, introduc-\ning a new lexical item into the vocabulary of the\nlanguage model, either “wug” (intended as a new\nsingular noun), or “wuz” (intended as a plural). We\nthen ﬁne-tune the language model using several\nexample sentences containing this word. Note that\nthis paradigm is distinct from nearly all of the few-\nshot learning experiments performed in Radford\net al. (2019); Brown et al. (2020), which operate\non a known vocabulary.5\n5.1 Learning agreement from syntactic data\nWe ﬁrst look at whether training data containing\nexplicit syntactic markers of number agreement is\nsufﬁcient for few-shot learning. Table 3 describes\n5Brown et al. (2020) perform several experiments on novel\nvocabulary items.\n4046\nTraining data typeTemplate\nSimple The wug/wuz⟨PresentTenseVerb⟩.\nPred-adj The wug/wuz is/are⟨Adj⟩.\nReﬂexive The wug/wuz⟨Verb⟩himself/themselves.\nTable 3: The three types of training data used for syn-\ntactic ﬁne-tuning.\nthe types of training data we examine. The three\ntypes of training data use different syntactic mark-\ners of plurality to indicate whether the new noun is\nsingular or plural.\nThe language models are ﬁne-tuned with 5 sen-\ntences drawn from a single training data type. GPT-\n2 was ﬁne-tuned for 2 epochs, and BERT was\nﬁne-tuned for 4 epochs. 6 Transformer-XL was\nnot used for the ﬁne-tuning experiments, due to is-\nsues with introducing new vocabulary items given\nTransformer-XL’s adaptive weight embedding.\nAfter ﬁne-tuning, each model was evaluated on\nthe 10 grammatical tasks in Table 1. For each gram-\nmatical task, 500 sentences were sampled from\nthe task template, and a performance score was\ncalculated by averaging scores of the samples, as\ndescribed in Section 3.4.\nFigure 4 shows results for ﬁne-tuning on the\nthree types of syntactic data. Compared to model\nperformance on real lexical items (shown in the left-\nmost column), both BERT and GPT-2 achieve qual-\nitatively similar performance given the Pred-adj\nand Reﬂexive training data, but worse performance\ngiven the Simple training data. Performance is\nweakest on subject-verb agreement (SV-agreement)\ntasks involving relative clauses. When trained on\ndata containing reﬂexive anaphora, both models\nachieve notably higher performance on the gram-\nmatical tasks involving reﬂexive anaphora.\nThe results provide evidence that small amounts\nof syntactic training data support learning the agree-\nment properties of novel nouns. They also pro-\nvide evidence of heterogeneity among different\ntypes of training data. Training from bare present\ntense verbs is least effective, and training from\nsentences containing reﬂexives leads to improved\nperformance on tasks which require understanding\nof the conditions on reﬂexive binding.\n5.2 Learning agreement from semantic data\nWe next examine whether purely semantic indica-\ntors of plurality are sufﬁcient for learning a noun’s\n6Prior to more systematic experiments, we informally opti-\nmized the number of ﬁne-tuning epochs.\nFigure 4: Few-shot learning from syntactic examples\n(averaging over plural and singular results). Columns\nshow different types of training data, and rows show\nthe 10 grammatical tasks. The bert-base and gpt2-xl\ncolumns indicate model performance on known lexi-\ncal items, i.e. summarizing results from Section 4.\nThe baseline columns indicate performance of non-\nﬁne-tuned models on the novel wug/wuz lexical items.\nScores are differences of log-probabilities between\ngrammatical and ungrammatical. The 95% conﬁdence\ninterval around each point estimate is always smaller\nthan ±0.25.\nnumber agreement properties. We look at several\ntypes of constructions which provide information\nabout the plurality of a noun, but using predicates\nwith past tense verbs that don’t inﬂect for number\nso that there is no grammatical number agreement.\nIn particular, we note the different possible read-\nings with reference to the distributive and collec-\ntive distinction described in the semantics literature\n(Lønning, 1997; Lasersohn, 2011; Champollion,\n2015). For documentation of predicates that re-\nquire a collective NP subject, see Levin (1993).\nWe use the ﬁne-tuning method from Section 5.1.\nSingular constructions\nIn order to induce singular noun interpretations,\nwe use the singular-biased constructions shown at\nthe top of Table 4. For example, if a wug worked\nall alone or came unaccompanied, it is likely that\n“wug” is both semantically and grammatically sin-\ngular. However, these constructions do not gramat-\nically require the head noun to be singular: they\nare compatible with distributive readings where\nthe predicate individually applies to members of a\ngroup (e.g. “the lawyers worked all alone” means\neach lawyer worked alone).\nBERT and GPT-2 were ﬁne-tuned on 5 exam-\nples of each of the singular constructions. Figure 5\nshows the results. None of the constructions con-\n4047\nTraining data type Example\nSingular\nall-alone The wug worked all alone.unaccompaniedThe wug came unaccompanied.separated-entireThe wug became separated from the entire group.personally The wug personally thanked me.\nPlural\nunison The wuz nodded in unison.together The wuz ate together.simultaneouslyThe wuz jumped simultaneously.outnumbered The wuz outnumbered the cats.constituted The wuz constituted a majority of the team.gathered The wuz gathered quietly.\nTable 4: Types of training data used for semantic ﬁne-\ntuning.\nFigure 5: Few-shot learning from singular semantic ex-\namples. The bert-base and gpt2-xl columns indicate\nmodel performance on known lexical items, i.e. sum-\nmarizing results from Section 4. The baseline columns\nindicate performance of non-ﬁne-tuned models on the\nnovel wug token.\nsistently induced correct performance on the gram-\nmatical tasks across both models. Three of the con-\nstructions — all-alone, unaccompanied, and per-\nsonally — led to strong performance on the reﬂex-\nive anaphora tasks (stronger than the average per-\nformance calculated in Section 4). The separated-\nentire construction consistently decreased perfor-\nmance on the tasks relative to baseline.\nPlural constructions\nIn order to provide the models with data indicating\nthat a novel noun is plural, we use constructions\nwhich force either collective or distributive read-\nings. For example, in Table 4, if the wuz constituted\nthe majority of the team, then the word “wuz” must\nbe semantically plural. The construction consti-\ntuted a majorityis collective because it must apply\nto the group as a whole:\n(6) The doctors constituted a majority of the\nteam.\na. *Distributive reading: each of the doctors\nconstituted a majority.\nFigure 6: Few-shot learning from plural semantic ex-\namples. The baseline columns indicate performance of\nnon-ﬁne-tuned models on the novel wuz token.\nb. Collective reading: the doctors as a group\nconstituted a majority.\nWhile the argument of a collective predicate must\nbe semantically plural, it is not necessarily gram-\nmatically plural. For example, the singular “the\ngroup” could constitute the majority of the team.\nThree of the constructions in Table 4 are col-\nlective: outnumbered, constituted, and gathered.\nThe other three are distributive phrasal predicates,\nwhich force distributive readings:\n(7) The architects nodded in unison.\na. Distributive reading: each of the archi-\ntects nodded.\nb. *Collective reading: the group of archi-\ntects itself nodded.\nFigure 6 shows the plural learning results. The\n6 types of training data perform comparably on\nthe subject-verb agreement tasks (and similar\nto the baseline model, which represents perfor-\nmance prior to ﬁne-tuning). The three distribu-\ntive phrasal constructions perform better on the\nreﬂexive anaphora tasks than the three collective\nconstructions, though all constructions improve rel-\native to the baseline.\n6 Discussion\nWe have investigated the sources of variation in neu-\nral language models’ grammatical judgments. We\nfound that there are systematic differences between\nnouns: when a language model exhibits knowledge\nof a noun’s grammatical properties in one task, it\nis more likely to do so in other tasks. Moreover,\nwhen one language model exhibits this knowledge,\nother language models are more likely to as well.\n4048\nThe study found two latent dimensions of variation\nbetween nouns: one corresponding to how well the\nmodels understood its behavior with reﬂexive pro-\nnouns, and the other corresponding to subject-verb\nagreement.\nSubsequent analyses demonstrate a pair of em-\npirical phenomena:\n1. It is relatively easy to learn the number agree-\nment properties of a noun. The models learn\nthe agreement properties of a novel noun from\njust a few samples, and the data supporting\nfew-shot learning appears to be densely dis-\ntributed; nearly all types of syntactic and se-\nmantic data examined lead to improvements\non the reﬂexive pronoun or subject-verb agree-\nment tasks.\n2. Nouns that occur more frequently during train-\ning are not learned more accurately. Many\nnouns that occur with high frequency are not\nlearned accurately.\nThese results suggest that nouns should vary less\nin their grammatical performance than is actually\nobserved; the study ﬁnds excess variationin gram-\nmatical performance. If number agreement can be\ncorrectly learned from a few samples (FSL sam-\nples), then one would expect model performance to\neither a) improve with more data, as more FSL sam-\nples are observed, or b) improve with more data\nup to some threshold, and then asymptote after\nlearning has saturated. In either case, for high fre-\nquency nouns, a sufﬁcient number of FSL samples\nshould be observed for these nouns to be learned\nvery accurately.\nA potential explanation of the results is that\nthey are caused by catastrophic forgetting (Ratcliff,\n1990; French, 1999): although a sufﬁcient num-\nber of FSL samples are observed for a noun, these\nsamples are forgotten during training, causing the\nperformance of the noun to degrade. This expla-\nnation is implausible. If catastrophic forgetting is\noccurring, then the problem should be more severe\nfor infrequent nouns than for frequent nouns, as the\ninterval between training samples will be longer\nfor infrequent nouns. This would predict better\nperformance for frequent nouns.\nAcknowledgements\nWe thank the Google Cloud Platform research pro-\ngram for support. The Titan V used for this re-\nsearch was donated by the NVIDIA Corporation.\nReferences\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nLucas Champollion. 2015. Distributivity, collectivity\nand cumulativity. In L. Matthewson, C. Meier, H.\nRullmann, and T. E. Zimmermann, editors, Com-\npanion to Semantics. Wiley-Blackwell.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nC. Fellbaum, editor. 1998. WordNet: An electronic lex-\nical database. MIT Press.\nRobert M French. 1999. Catastrophic forgetting in con-\nnectionist networks. Trends in cognitive sciences,\n3(4):128–135.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic sub-\njects: Representations of syntactic state. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 32–42, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Un-\nder the hood: Using diagnostic classiﬁers to in-\nvestigate and improve how language models track\nagreement information. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, pages 240–248,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nYoav Goldberg. 2019. Assessing BERT’s syntactic\nabilities. arXiv preprint arXiv:1901.05287.\n4049\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1195–1205, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLSTMs can learn syntax-sensitive dependencies\nwell, but modeling structure makes them better. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1426–1436, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nPeter Nathan Lasersohn. 2011. Mass nouns and plu-\nrals. In Semantics: An International Handbook of\nNatural Language Meaning, pages 1131–1153. De\nGruyter.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality, acceptability, and probabil-\nity: A probabilistic view of linguistic knowledge.\nCognitive Science, 41(5):1202–1241.\nMichael A Lepori, Tal Linzen, and R Thomas Mc-\nCoy. 2020. Representations of syntax [MASK] use-\nful: Effects of constituency and dependency struc-\nture in recursive LSTMs. In Proceedings of the\n58th Annual Meeting of the Association for Com-\nputational Linguistics, Seattle, Washington. Associ-\nation for Computational Linguistics.\nBeth Levin. 1993. English verb classes and alterna-\ntions: A preliminary investigation. University of\nChicago press.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics, 4:521–\n535.\nJan Tore Lønning. 1997. Plurals and collectivity. In\nHandbook of logic and language, pages 1009–1053.\nElsevier.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nRoger Ratcliff. 1990. Connectionist models of recog-\nnition memory: constraints imposed by learning\nand forgetting functions. Psychological review,\n97(2):285.\nKarin Kipper Schuler. 2005. Verbnet: A broad-\ncoverage, comprehensive verb lexicon.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019a. Investi-\ngating BERT’s knowledge of language: Five anal-\nysis methods with NPIs. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2877–2887, Hong Kong,\nChina. Association for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019b. Neural network acceptability judg-\nments. Transactions of the Association for Compu-\ntational Linguistics, 7:625–641.\nEthan Wilcox, Roger Levy, Takashi Morita, and\nRichard Futrell. 2018. What do RNN language\nmodels learn about ﬁller–gap dependencies? In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 211–221, Brussels, Belgium.\nAssociation for Computational Linguistics.\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019. Structural super-\nvision improves learning of non-local grammatical\ndependencies. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3302–3312, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nA Further analyses\nThis section contains several additional analyses:\nprincipal components for task performance among\n4050\nthe three language models (Tables 5-8); pairwise\ncomparison between task performance for BERT\nand GPT-2 (Figures 7 and 8); and more ﬁne-grained\ncomparisons between word frequency and model\nperformance (Figures 9 and 10).\n4051\nPC Number Transformer-XL BERT GPT-2\n1 0.4663 0.3865 0.4146\n2 0.7299 0.5619 0.6873\n3 0.8511 0.7073 0.8059\n4 0.9083 0.8034 0.8720\n5 0.9499 0.8919 0.9175\nTable 5: Cumulative proportion of variance explained by the top (of 10) PCs for each model as detailed in Section\n4.1.\nContributor by RankPC 1 PC 2 PC 3\n1 RA ObjRelClauseNoThat - 0.386980SV SubjRelClause - 0.449504 SV SentComp - 0.534710\n2 RA ObjRelClauseThat - 0.376354SV ObjRelClauseNoThat - 0.442445SV Simple - 0.516031\n3 RA SentComp - 0.359096 SV ObjRelClauseThat - 0.439015RA ObjRelClauseThat - 0.402452\n4 RA Simple - 0.347978 RA Simple - 0.376192 SV ObjRelClauseThat - 0.312466\nTable 6: Top contributors (tasks) to top few (of 10) PCs for Transformer-XL’s noun performance as detailed in\nSection 4.1. Cells contain the task name followed by their (absolute) component value in the eigenvector.\nContributor PC 1 PC 2 PC 3\n1 RA ObjRelClauseNoThat - 0.456096SV ObjRelClauseThat - 0.577452SV SentComp - 0.686402\n2 RA ObjRelClauseThat - 0.444272SV ObjRelClauseNoThat - 0.576572SV Simple - 0.499513\n3 RA Simple - 0.383953 SV PP - 0.353213 SV SubjRelClause - 0.346437\n4 RA SentComp - 0.383866 RA Simple - 0.288091 SV PP - 0.248977\nTable 7: Top contributors (tasks) to top few (of 10) PCs for BERT’s noun performance as detailed in Section 4.1.\nCells contain the task name followed by their (absolute) component value in the eigenvector.\nContributor PC 1 PC 2 PC 3\n1 RA ObjRelClauseNoThat - 0.454492SV ObjRelClauseNoThat - 0.477923SV SentComp - 0.549969\n2 RA Simple - 0.447148 SV SubjRelClause - 0.444648 SV Simple - 0.525072\n3 RA SentComp - 0.441366 SV ObjRelClauseThat - 0.426385SV ObjRelClauseThat - 0.478782\n4 RA ObjRelClauseThat - 0.425359SV SentComp - 0.385486 SV ObjRelClauseNoThat - 0.362165\nTable 8: Top contributors (tasks) to top few (of 10) PCs for GPT-2’s noun performance as detailed in Section 4.1.\nCells contain the task name followed by their (absolute) component value in the eigenvector.\n4052\nFigure 7: BERT: Pairwise comparisons between tasks.\n4053\nFigure 8: GPT-2: Pairwise comparisons between tasks.\n4054\nFigure 9: Noun Frequency vs. Model Performance on Subject-Verb Agreement Tasks\nFigure 10: Noun Frequency vs. Model Performance on Reﬂexive Anaphora Tasks"
}