{
  "title": "Take an Irregular Route: Enhance the Decoder of Time-Series Forecasting Transformer",
  "url": "https://openalex.org/W4389538588",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5080198746",
      "name": "Li Shen",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5079044133",
      "name": "Yuning Wei",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5114067558",
      "name": "Yangzhu Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5038659681",
      "name": "Huaxin Qiu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3208189341",
    "https://openalex.org/W4234761883",
    "https://openalex.org/W4280550128",
    "https://openalex.org/W4382203079",
    "https://openalex.org/W6838200255",
    "https://openalex.org/W4382239356",
    "https://openalex.org/W6839002298",
    "https://openalex.org/W4280531713",
    "https://openalex.org/W6851631877",
    "https://openalex.org/W2017377889",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W6846825190",
    "https://openalex.org/W6846376059",
    "https://openalex.org/W6889955440",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2963857746",
    "https://openalex.org/W4297095251",
    "https://openalex.org/W3176916588",
    "https://openalex.org/W6764679822",
    "https://openalex.org/W6797155008",
    "https://openalex.org/W6810637551",
    "https://openalex.org/W6810853030",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W6797297377",
    "https://openalex.org/W6763309814",
    "https://openalex.org/W6853416803",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4382239131",
    "https://openalex.org/W4385764384",
    "https://openalex.org/W6845694969",
    "https://openalex.org/W4238629135",
    "https://openalex.org/W6798224550",
    "https://openalex.org/W3188872815",
    "https://openalex.org/W6838461383",
    "https://openalex.org/W3199148273",
    "https://openalex.org/W6811289536",
    "https://openalex.org/W2808800115",
    "https://openalex.org/W6857279128",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4221148002",
    "https://openalex.org/W4389664922",
    "https://openalex.org/W4386901726",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4229539396",
    "https://openalex.org/W4387561305"
  ],
  "abstract": "With the development of Internet of Things (IoT) systems, precise long-term forecasting method is requisite for decision makers to evaluate current statuses and formulate future policies. Currently, Transformer and MLP are two paradigms for deep time-series forecasting and the former one is more prevailing in virtue of its exquisite attention mechanism and encoder-decoder architecture. However, data scientists seem to be more willing to dive into the research of encoder, leaving decoder unconcerned. Some researchers even adopt linear projections in lieu of the decoder to reduce the complexity. We argue that both extracting the features of input sequence and seeking the relations of input and prediction sequence, which are respective functions of encoder and decoder, are of paramount significance. Motivated from the success of FPN in CV field, we propose FPPformer to utilize bottom-up and top-down architectures respectively in encoder and decoder to build the full and rational hierarchy. The cutting-edge patch-wise attention is exploited and further developed with the combination, whose format is also different in encoder and decoder, of revamped element-wise attention in this work. Extensive experiments with six state-of-the-art baselines on twelve benchmarks verify the promising performances of FPPformer and the importance of elaborately devising decoder in time-series forecasting Transformer. The source code is released in https://github.com/OrigamiSL/FPPformer.",
  "full_text": "IEEE INTERNET OF THINGS JOURNAL, 1\nTake an Irregular Route: Enhance the Decoder of\nTime-Series Forecasting Transformer\nLi Shen, Yuning Wei, Yangzhu Wang and Huaxin Qiu\nAbstractâ€”With the development of Internet of Things (IoT)\nsystems, precise long-term forecasting method is requisite for\ndecision makers to evaluate current statuses and formulate future\npolicies. Currently, Transformer and MLP are two paradigms\nfor deep time-series forecasting and the former one is more\nprevailing in virtue of its exquisite attention mechanism and\nencoder-decoder architecture. However, data scientists seem to be\nmore willing to dive into the research of encoder, leaving decoder\nunconcerned. Some researchers even adopt linear projections in\nlieu of the decoder to reduce the complexity. We argue that both\nextracting the features of input sequence and seeking the relations\nof input and prediction sequence, which are respective functions\nof encoder and decoder, are of paramount significance. Motivated\nfrom the success of FPN in CV field, we propose FPPformer\nto utilize bottom-up and top-down architectures respectively in\nencoder and decoder to build the full and rational hierarchy.\nThe cutting-edge patch-wise attention is exploited and further\ndeveloped with the combination, whose format is also different in\nencoder and decoder, of revamped element-wise attention in this\nwork. Extensive experiments with six state-of-the-art baselines\non twelve benchmarks verify the promising performances of\nFPPformer and the importance of elaborately devising decoder in\ntime-series forecasting Transformer. The source code is released\nin https://github.com/OrigamiSL/FPPformer.\nIndex Termsâ€”Deep-learning, neural network, time-series fore-\ncasting, Transformer.\nI. I NTRODUCTION\nA. Background\nT\nHE advent of Big Data era has brought immense volume\nand variety of data in the 21st century, especially in\nInternet of Things (IoT) systems with tons of sensors [1].\nConsequently, it necessitates long-term time-series forecasting\nmethods with demanding accuracy and efficiency to assist\ndecision makers and engineers in the appraisal of sensor\nstatuses and future plans. Since traditional forecasting methods\nbased on statistics [2], [3] are no longer sufficient for such so-\nphisticated situations, more and more data scientists pay their\nattention to deep time-series forecasting [4]. After decades of\ndevelopment and competition, Time-Series Forecasting MLP\n(TSFM) [5]â€“[7] and Time-Series Forecasting Transformer\n(TSFT) [8]â€“[11] become the mainstream.\nB. Problems\nTSFM and TSFT have different pros and cons. TSFM is\nknown for its parsimonious but efficient architecture so that\nManuscript received xxxx; revised xxxx. (Corresponding author: Li Shen)\nLi Shen, Yuning Wei, Yangzhu Wang and Huaxin Qiu are with Beihang\nUniversity, Beijing, China. (email: shenli@buaa.edu.cn; yuning@buaa.edu.cn;\nwangyangzhu@buaa.edu.cn; qiuhuaxin@buaa.edu.cn)\nforecasting models based on TSFM excel in resisting non-\nstationarity brought by distribution shifts [12] and concept\ndrifts [13]. Conversely, forecasting models based on TSFT\nown more complicated architecture and better capability of\ncapturing long-term dependencies of time-series at the expense\nof being more vulnerable to over-fitting problem caused by\nnon-stationarity [5]. Fortunately, pioneers have striven to get\naround plenty of problems of TSFT. Direct forecasting strategy\n[14] reduces the time complexity and alleviates the error\naccumulation problem [15]. RevIN [12] solves the problem of\ndistribution shifts among windows with distinct time spans.\nThe channel-independent [16] forecasting method renders\nTSFT refraining from extracting vague inter-relationships of\ndifferent variables. Patch-wise attention mechanism [10], [16]\nfurther attenuates the space complexity and brings the capabil-\nity of local feature extraction to TSFT. Indeed, recent works\nhave proven that TSFT models [9], [17] can also be stable\nand robust in forecasting. Evidently, the majority of these\nenhancement focus on improving the encoder architecture and\ntackling input sequence features. It cannot be denied that they\nare very important, but not solely. The connections of input\nand prediction sequences, manifested by decoder in TSFT, are\nalso of paramount significance, especially for pursuing precise\nforecasting in IoT. However, its significance is frequently\nomitted and itself is inadequately explored. Normally, the\ndecoder architectures of existing TSFT models are simply\nduplicates of their encoder architectures, barring with little\nindispensable modifications, such like changing self-attentions\nto cross-attentions [8], [15]. Furthermore, some researchers\nhave gone so far as to substitute decoder in TSFT with simple\nlinear projection [16], [18], which is analogous to TSFM,\nfor the sake of enhancing their efficiency. Now it is time to\nenhance the decoder of TSFT to fully develop its potential and\npush its forecasting performances to a new altitude.\nC. Contributions\nDifferent from existing TSFT models, we Fully develop the\ntried-and-tested Patch-wise attention mechanism and Pyramid\narchitecture in both encoder and decoder and thereby propose\nFPPformer. Alike FPN [19] and PAN [20] architectures which\nare prevalent in CV fields, FPPformer hierarchically extracts\ninput sequence features from fine to coarse and constructs\nprediction sequence from coarse to fine . To strengthen the\nfeature extraction capability of patch-wise attention, we further\ninsert an element-wise attention block into each patch-wise\nattention block to extract fine-grained inner-relations of each\npatch in encoder and decoder with merely linear complexity .\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2 IEEE INTERNET OF THINGS JOURNAL,\nA channel-independent and temporal-independent embedding\nmethod is utilized to modify the size of feature maps in\nFPPformer to meet the needs of element-wise attention and\npatch-wise attention. Within each attention block in encoder,\nthe diagonal line of query-key matching matrix is masked\nto ensure the generality of features extracted from input\nsequences. Primary contributions of this work are five folds:\n1) We propose a novel time-series forecasting Transformer,\ni.e., FPPformer, which uncommonly and efficaciously\nimproves the decoder architecture of TSFT to break its\nfetters and excavate its potential.\n2) We renovate the decoder architecture of TSFT and\nchange it into top-down architecture for the sake of\nrationally constructing the prediction sequence in a\nhierarchical manner.\n3) Motivated by a pioneer anomaly detection method, we\npropose diagonal-masked self-attention to mitigate the\nnegative impacts of the outliers in input sequences.\n4) A new combination of element-wise attention and patch-\nwise attention is proposed by us to compensate the\nweakness of conventional patch-attention in extracting\nthe inner-features of each patch, with only additional\nlinear complexity.\n5) Extensive experiments under diverse settings validate\nthat FPPformer is capable of reaching state-of-the-art\non twelve benchmarks with peerless accuracy and ro-\nbustness.\nII. R ELATED WORKS\nThe past few years have witnessed the development and the\nsuccess of deep-learning based forecasting methods. Thanks\nto the help of neural network, the long-term multivariate\nforecasting is no more a dream so that even IoT systems with\nplenty of sensors and explosive data can be predicted [1], [21],\n[22]. Researchers have developed deep forecasting methods\nbuilt upon diverse networks and Transformer is a hot topic\namong corresponding literature.\na) Time-Series Forecasting Transformer: Traditionally,\nTime-Series Forecasting Transformer (TSFT) executes the\nforecasting via encoder-decoder architecture. The Transformer\nencoder is used to extract the features of input sequence, then\nthe Transformer decoder is able to construct the prediction\nsequence by the extracted features of encoder and the pre-\ndiction sequence, which is initialized with a certain number\nsince it is unknown at the beginning. These two processes\nare completed predominantly by attention mechanism, thereby\nresearchers always keep an eye on it. LogSparse Transformer\n[23] and Informer [15] discover the sparsity of query-key\nmatching matrix and they force the elements of query to attach\nto the partial elements of key for the sake of reducing the\ncomplexity. Autoformer [24], FEDformer [25] and ETSformer\n[26] combine the TSFT with seasonal-trend decomposition and\nsignal processing method, e.g., Fourier Analysis, in attention\nmechanism to enhance their interpretability. Patch-wise atten-\ntion is more popular and proven to be more useful recently.\nTSFTs with patch-wise attention, including Triformer [10],\nCrossformer [8] and PatchTST [16], achieve more promising\nperformances than preceding models. However, whichever\nTSFT always emphasizes that the modified architecture is\nintended for more efficient or effective feature extraction for\ninput sequence. Hardly ever can statements involved with the\nprofits of decoder be found. Indeed, their decoders seem to\nplay the role of requisite appendages in the entire Transformer\narchitecture. Once some parts of encoders are changed by their\nproposed methods, mirrored changes are made to their de-\ncoders. Some researches [16], [18] even abandon the decoder\nto circumvent these changes. Contrary to them, studying and\nfiguring out the correct way of designing decoders in TSFT is\nexactly what this work is supposed to do.\nb) Other Miscellaneous Deep Forecasting Methods: Bar-\nring TSFT, there are plenty of other types of deep forecasting\nmethods. Forecasting methods based on RNN and CNN are\nfeasible ones. Their respective representatives LSTNet [27]\nand SCINet [28] both achieved shiny performances during\ntheir periods. However, compared with the foregoing two\ntypes of deep forecasting methods, Time-Series Forecasting\nMLP (TSFM) relatively receives more attentions. These fore-\ncasting networks are solely comprised of linear-projection\nlayers, whereas they still achieve promising performances.\nDue to their simple architectures, it is convenient for them to\ncombine with statistics models for the objective of improving\ntheir interpretability and forecasting capability. NBEATS [29]\nand DLinear [5] adopt seasonal-trend decomposition methods\nin their networks more concisely than FEDformer [25] but\nachieve better results in general. C. Challu et al [7] further\npresented N-HiTS that employs sampling and interpolation\nstrategies on the basis of NBEATS for more precise and\nhierarchical prediction. Reconstruction method motivated from\nLegendre Polynomials is taken into account by T. Zhou et\nal [6] to come out with FiLM. TSMixer proposed by V .\nEkambaram et al [30] considers the temporal patterns, cross-\nvariate information and additional auxiliary information to\nrender TSFM ready for more complicated forecasting cases.\nThey are challenging competitors for TSFT and we chiefly\ncompare FPPformer with other TSFTs and these TSFMs in\nforthcoming experiments.\nIII. P RELIMINARY\na) Problem Statement: This work primarily concentrates\non multivariate forecasting problem. As the term suggests, a\nmultivariate forecasting problem is to predict a certain window\n{xt2:t3 }1:V with time duration of (t3 âˆ’t2) and variable number\nof V with its anterior window {xt1:t2 }1:V . Each xv\nt âˆˆ R,\nwhere t âˆˆ [t1 : t3) and v âˆˆ [1, V], denotes an element at\ntimestamp t and stemming from variable v. There are quite a\nfew nomenclature style to name the dimension of t and v. In\nthis work, the dimension of t is termed the temporal dimension\nand the dimension of v is termed the variable dimension. Note\nthat the word â€˜channel-independentâ€™ mentioned above refers\nto the independence at the variable dimension. Moreover, we\nstill discuss several univariate forecasting cases where V = 1\nin our experiments since the variable treatment strategies of\ndifferent forecasting methods can be very distinctive, making\nmultivariate forecasting comparison, albeit prevailing, not per-\nsuasive enough.\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nSHEN et al.: TAKE AN IRREGULAR ROUTE: ENHANCE THE DECODER OF TIME-SERIES FORECASTING TRANSFORMER 3\nEncoder\nğ‘¥ğ‘¥ğ‘–ğ‘–ğ‘–ğ‘–\nEmbed\nSelf-attention\nFeed Forward\n1st stage\nSelf-attention\nFeed Forward\n2nd stage\nğ‘‹ğ‘‹ğ‘–ğ‘–ğ‘–ğ‘–Position Embedding ğ‘ƒğ‘ƒğ‘’ğ‘’ğ‘–ğ‘–ğ‘’ğ‘’\n0ğ‘ğ‘ğ‘ğ‘ğ‘’ğ‘’ğ‘ğ‘\nEmbed\nğ‘‹ğ‘‹ğ‘ğ‘ğ‘ğ‘ğ‘’ğ‘’ğ‘ğ‘Position Embedding ğ‘ƒğ‘ƒğ‘ğ‘ğ‘’ğ‘’ğ‘’ğ‘’\nMasked \nSelf-attention\nFeed Forward\nCross-attention\n1st stage\nMasked \nSelf-attention\nFeed Forward\nCross-attention\n2nd stage\nProjection\nDecoder\nğ‘¥ğ‘¥ğ‘ğ‘ğ‘ğ‘ğ‘’ğ‘’ğ‘ğ‘\nğ‘‹ğ‘‹ğ‘’ğ‘’ğ‘–ğ‘–ğ‘’ğ‘’\nğ‘‹ğ‘‹ğ‘ğ‘ğ‘’ğ‘’ğ‘’ğ‘’\nFig. 1. A schematic of a vanilla TSFT with two-stage encoder (Green dashed\nbox containing green solid boxes in the left) and two-stage decoder (Orange\ndashed box containing orange solid boxes in the right).\nb) Vanilla TSFT Architecture: The architecture of vanilla\nTSFT is chiefly composed of an encoder, a decoder and\na projection layer . An example of a vanilla TSFT with 2-\nstage encoder and 2-stage decoder is sketched in Fig. 1. We\ncan see that input sequence xin passes through the encoder\nembedding layer, then the superimposition of the embedded\ninput sequence Xin and its position embedding Penc, which\ncovers the input time span, is sent to the encoder. The encoder\nprocesses Xin + Penc with M (M = 2 in Fig. 1) stages,\neach consisting of a self-attention block and a feed forward\nlayer, and the ultimate output feature map of the encoder is\nXenc. Typically, the canonical self-attention conducts scaled\ndot-products with the formula of (1):\nAttn(q, k, v) =Softmax(qkâŠ¤\nâˆš\nD\nv) (1)\nwhere q, k, vâˆˆ RLÃ—D are the linear projections of the\nidentical sequence tensor, L is the number of tokens (or se-\nquence tensor length) and D is the hidden dimension. Readers\ncan refer to [31] to be familiar with attention mechanism\nand feed forward layer. Correspondingly, the decoder receives\nboth Xenc and the zero-initialized prediction sequence 0pred.\n0pred propagates through the decoder embedding layer to\nobtain Xpred and its position embedding Pdec, which covers\nthe prediction time span, is superimposed on it. Afterwards,\nthey are sequentially sent into N (N = 2 in Fig. 1) stages,\neach consisting of a masked self-attention block, a cross-\nattention block and a feed forward layer. Causality is essential\nas the prediction sequence is unknown, so that the masked\nself-attention, rather than normal self-attention, is utilized in\ndecoder. The cross-attention block is intended to construct\nthe prediction sequence via the encoder feature map Xenc.\nEventually, a projection layer maps the output feature map of\ndecoder Xdec to the prediction sequence xpred.\nc) Employed Mechanisms: Barring our proposed meth-\nods, which will be introduced in the upcoming section, we also\nemploy several advanced time-series forecasting mechanisms\nin FPPformer:\n1) Direct forecasting method [14], which is widely em-\nployed by recent deep forecasting method, performs the\nprediction of the entire sequence with only one forward\nprocess to alleviate the error accumulation.\n2) Channel-independent forecasting method , which has\nbeen mentioned in the foregoing sections, treats the\nsequences of different variables as different instances.\nThe sequences of different variables are parallel sent into\nthe network without interfering with each other so that\nthe network can seek shared characteristics of different\nvariable sequences without imposing any inductive bias\nto the correlations of different variables.\n3) RevIN [12], which is devised for non-stationary time-\nseries forecasting, normalizes each input sequence with\nits own statistics before sent into the network and\nrestores the original statistics to the prediction sequence\nvia the reverse instance normalization to handle the\ndistribution shifts of real-world long time-series.\n4) Patch-wise attention [16], which segments the sequence\ninto patches of the same length, treats each single patch,\nrather than each single element, as a token in (1) and\ntreats the elements inside each patch or their latent\nrepresentations as the hidden features of each token\n(patch) for better efficiency and generality.\nNote that the majority of recent deep forecasting models\n[5]â€“[8], [10], [16], [28], [30], [32], including those employed\nin our experiments, at least adopt two of above mechanisms\nso that they are not something that distinguish our methods\nfrom others.\nIV. M ETHODOLOGY\nA. Analysis of Decoder in TSFT\nBefore commencing the introduction to our proposed FPP-\nformer, we point out some deficiencies of current decoder\narchitecture in TSFT to clarify the necessity and rationality\nof the decoder improvement in FPPformer.\nWe first discover the redundant self-attention problem in\ndecoder. To elaborate, we notice that the input to decoder in\nFig. 1 is a zero-initialized prediction sequence 0pred owing\nto the unknown future. Consequently, the first (masked) self-\nattention is performed only on the position embedding of the\nprediction sequence. No matter it is fixed [15] or learnable [8],\nit is completely independent of input sequence. Keep in mind\nthat time-series forecasting is an auto-regressive problem.\nIt makes no sense to perform (masked) self-attention only\non position embedding with the attempt of deducing some\ngroundless relations. Moreover, position embedding is always\nstatic after training while the input sequence can be dynamic\nand non-stationary, shattering the last hope that the position\nembedding can fit the statistics of time-series sequences due\nto some sort of assumptions with respect to homogeneity [33],\n[34]. Start token [15] can be a solution, however short start\ntoken is not enough for long-term forecasting and longer start\ntoken brings about excessive complexity.\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4 IEEE INTERNET OF THINGS JOURNAL,\nEncoder\nğ‘¥ğ‘¥ğ‘–ğ‘–ğ‘–ğ‘–\nEmbed\nDM Element-wise\nSelf-attention\nFeed Forward\n1st stage\nğ‘‹ğ‘‹ğ‘–ğ‘–ğ‘–ğ‘–Position Embedding ğ‘ƒğ‘ƒğ‘’ğ‘’ğ‘–ğ‘–ğ‘’ğ‘’\nProjection\nDecoder\nğ‘¥ğ‘¥ğ‘ğ‘ğ‘ğ‘ğ‘’ğ‘’ğ‘ğ‘\nDM Patch-wise\nSelf-attention\nIN\nDM Element-wise\nSelf-attention\nFeed Forward\n2nd stage\nDM Patch-wise\nSelf-attention\nMerging patches\nğ‘‹ğ‘‹ğ‘’ğ‘’ğ‘–ğ‘–ğ‘’ğ‘’ğ‘’\nğ‘‹ğ‘‹ğ‘’ğ‘’ğ‘–ğ‘–ğ‘’ğ‘’ğ‘’\nProjection\nğ‘‹ğ‘‹ğ‘ğ‘ğ‘’ğ‘’ğ‘’ğ‘’\nRevIN\nPosition Embedding\nPatch-wise \nCross-attention\n1st stage\nElement-wise\nSelf-attention\nFeed Forward\nSplitting patches\nPatch-wise \nCross-attention\n2nd stage\nElement-wise\nSelf-attention\nFeed Forward\nğ‘ƒğ‘ƒğ‘ğ‘ğ‘’ğ‘’ğ‘’ğ‘’\nâ— \nFig. 2. An overview of FPPformerâ€™s hierarchical architecture with two-stage\nencoder and two-stage decoder. Different from the vanilla one in Fig. 1, the\nencoder owns bottom-up structure while the decoder owns top-down structure.\nNote that the direction of the propagation flow in decoder is opposite to that\nin Fig. 1 to highlight the top-down structure. â€˜DMâ€™ in the stages of encoder\nmeans â€˜Diagonal-Maskedâ€™.\nBesides, the connection of encoder and decoder is unitary,\nleading to the multi-scale insufficiency problem. As shown in\nFig. 1, the encoder with M stages can produce M feature\nmaps of input sequence, however merely the last one is\nsent to decoder. This problem is more noteworthy when it\ncomes to some modified TSFT with hierarchical architecture\nin encoder . For instance, Informer [15] employs convolution\nlayers between every two stages in encoder and FEDformer\n[25] keeps decomposing input sequences to acquire more\nprecise seasonal features but neither of them apply the same\noperations to decoder and only the feature map of the last\nstage in encoder is sent to decoder . Crossformer [8] merges\nadjacent segments to obtain bigger patches in deeper stages\nin both encoder and decoder. However, just as what we claim\nin foregoing sections, the architecture of decoder is merely\nthe replica of encoder with an additional cross-attention in\nCrossformer. By merging patches from small to big in decoder,\nCrossformer attempts to construct the unknown prediction\nsequence from fine to coarse , whose irrationality is self-\nevident.\nB. Model Architecture\nThe overview of our proposed FPPformer is illustrated in\nFig. 2 and its major enhancement on vanilla TSFT concen-\ntrates on addressing the preceding two problems of decoder.\nComparing the schematics in Fig. 1 and 2, the differences with\nrespect to the overall architecture can be readily noticed. To\nhandle the first redundant self-attention problem in decoder,\nwe change the order of the self-attention and cross-attention\nin decoder. Thereby, before embarking upon deducing any\nrelations within unknown prediction sequence, the prediction\nsequence receives the auto-regressive parts from the deepest\nencoder feature map , which serves as a better role for pre-\ndiction sequence initialization before the first self-attention in\ndecoder than simple zero-initialization with start token [15],\nrandomly generated parameters [8], the trend decomposition\nof raw input sequence [25], and so forth. It is evident that the\nlatter initialization formats of other TSFTs are either relatively\nsimple or inefficient.\nWe employ the hierarchical pyramids both in encoder and\ndecoder with lateral connections to tackle the second multi-\nscale insufficiency problem. As we adopt the patch-wise\nattention in FPPformer, the patches are merged before sent\nto the next stages in the bottom-up architecture of encoder\nand opposite operations, i.e., the splitting, are performed in\nthe top-down architecture of decoder. The feature map of\ninput sequence gets deeper and more coarse-grained within\nlater stages, which is also the property shared by encoders of\nmany TSFTs. However, things get different when we attempt\nto construct the prediction sequence from the position embed-\nding and encoder feature maps. Recall how we decompose\nand reconstruct an arbitrary sequence from a certain multi-\nresolution analysis {Vj}jâˆˆZ of L2(R) and wavelet spaces\n{Wj}jâˆˆZ in wavelet theory [35], which owns a transcendent\nposition in signal processing. When we decompose certain\nsequence fi+1(t) âˆˆ Vj+1, we decompose it into coarser spaces\nVj and Wj. Whereas the reconstruction is opposite, i.e., we\nrecover the sequence in finer space fi+2(t) âˆˆ Vj+2 from Vj+1\nand Wj+1. Omitting the existence of wavelet space Wj, which\ncontains the information of details or noises, we can find that\nthe encoder and decoder processes separately correspond to\nthe decomposition and (re)construction processes in multi-\nresolution analysis. From another perspective, the unknown\nprediction sequence is initialized with zero or other parameters\nnot pertaining to the ground truths at first. Thereby, when\nwe strive to construct it from input sequence features, it\nis natural to commence with the most universal features to\nensure the exactitude of general characteristics of prediction\nsequence features , then we can prudently take steps to seek\nfiner features of prediction sequence to avoid over-fitting . The\nsuccess of FPN [19], which also employs bottom-up and top-\ndown architectures, in CV fields further confirms the preceding\nidea. Therefore, we keep splitting the patches in decoder and\ncommence the hierarchical prediction sequence constructions\nwith the feature maps from encoder, separately with identical\nresolutions, via lateral connections. The encoder in FPPformer\npresents a bottom-up architecture while the decoder presents\na top-down architecture. Differences between the hierarchical\ndesign in Crossformer [8], whose decoder architecture is\nmerely a replication of that encoder 1. M. A. Shabani et al [9]\nalso notice the analogous thing but they neither expound the\nreasons of doing so nor they carry out any change to decoder.\n1In effect, the decoder of Crossformer even does not own a pyramid\narchitecture. We use this statement since the hierarchical process of con-\nstructing the unknown prediction sequence is determined by how the model\nhierarchically uses the encoder features and for the more vivid comparison\nbetween FPPformer and Crossformer in Fig.3.\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nSHEN et al.: TAKE AN IRREGULAR ROUTE: ENHANCE THE DECODER OF TIME-SERIES FORECASTING TRANSFORMER 5\nEncoder: Keep merging Decoder:Keep splitting\nInput Sequence Prediction Sequence\n1st stage\n2nd stage\n3rd stage 1st stage\n2nd stage\n3rd stage\nDecoder input\n(a) FPPformer\nEncoder: Keep merging Decoder: Keep â€˜mergingâ€™\nInput Sequence\nPrediction Sequence\n1st stage\n2nd stage\n3rd stage\nDecoder input\n1st stage\n2nd stage\n3rd stageLinear\nLinear\nLinear (b) Crossformer\nFig. 3. The comparison of hierarchical architecture of FPPformer (a) and Crossformer (b). The discrepancies are highlighted with red. Obviously, the decoder\nstructure of Crossformer is nearly a duplicate of encoderâ€™s bottom-up structure whereas the decoder of FPPformer owns a different â€˜top-downâ€™ structure.\nC. Combined Element-wise Attention and Patch-wise Atten-\ntions\nThe preceding two problems are shared by the majority of\nTSFTs and we would like to mention another specific problem\nof TSFT with patch-wise attention. We name this type of\nTransformer PTFST for brevity. Different from the element-\nwise attention of vanilla TSFT, which seeks the correlations\nof sequence elements, the patch-wise attention seeks the\ncorrelations of different patches or segments of sequences\nto improve the efficiency and reduce the risk of over-fitting.\nPatchTST [16] and Crossformer [8] are such PTSFTs and\ntheir experiments have proven the superiority of patch-wise\nattention. However, they neglect the inner-relations of the\nelements inside the patches or only employ simple linear\nprojections to mix them up. Therefore, we make different\nchanges in employed patch-wise attention in encoder and\ndecoder for the sake of extracting more fine-grained features\nin encoder or pursuing finer prediction sequence construction\nin decoder.\nAs shown in the schematic of Fig. 2, a element-wise\nattention block is arranged before each patch-wise block in\nevery encoder stage to extract the inner-relationships of all\npatches before seeking their inter-relationship. This element-\nwise self-attention is patch-independent so that the additional\ncomplexity is O(P2 Ã—L/P) =O(LÃ—P), which is linear with\ninput sequence length. P is the patch size and L is the input\nsequence length in the last complexity expression. Observ-\ning that the element-wise attention requires the preservation\nof independent sequence elements information, therefore we\ncannot directly map the initially segmented patches into the\nlatent space like other PTSFTs otherwise the element-wise\ninformation is no longer preserved and element-wise attention\ncannot be implemented. To address this issue, we adopt\na channel-independent and element-independent embedding\nmethod. As illustrated in Fig. 4 and Table I, the input sequence\nelements of different timestamps do not interfere with each\nother during the embedding process and reshaping operations\nare performed for the different needs of tensor shapes of\nelement-wise attention and patch-wise attention.\nThe changes to decoderâ€™s attention are analogous but not\ncompletely. Since decoder itself has already owned two at-\ntention blocks, we maintain the patch-wise attention in cross-\nattention block to ensure the general construction of predic-\ntion sequence via auto-regressive process. Simultaneously, the\nmasked self-attention block of vanilla TSFT is transformed\nL\nInput sequence\nEmbedding\nLÃ—D SÃ—PÃ—D SÃ—PÃ—D SÃ—PD\nSegment to \nS patches\nS = L / P Element-wise\nself-attention\nPatch-wise\nself-attention\nReshape\nğ·ğ·\nğ¿ğ¿L\nğ‘ƒğ‘ƒ\nğ·ğ·\nğ‘ƒğ‘ƒ\nS\nğ‘ƒğ‘ƒğ·ğ·\nğ‘†ğ‘†\nğ·ğ·\nğ‘†ğ‘†\nFig. 4. The changes in the size of a single input sequence when propagating\nthrough the first encoder stage. The batch size and the variable dimension\nare omitted. The red and blue letters in the last two sizes separately refer to\nthe token dimension and its latent representation dimension. The reshaping\noperation is used to treat the features of all elements in a single patch as a\nunity for the sake of connecting element-wise self-attention and patch-wise\nattention.\nTABLE I\nTHE ARCHITECTURE OF THE FIRST STAGE IN ENCODER\nLayer/Operation Name Output Size\nInput L\nEmbedding L Ã— D\nSegmentation S Ã— P Ã— D, P= L/S\nElement-wise self-attention S Ã— P Ã— D\nReshape S Ã— PD\nPatch-wise self-attention S Ã— PD\nto element-wise self-attention block, which is also patch-\nindependent, in FPPformer. Just as we mentioned in the\npreceding sections, the prediction sequence is unknown so that\nwe need to foremost guarantee the correctness of its general\ncharacteristics, manifested by placing patch-wise attention be-\nfore the element-wise attention in decoder, then we can pursue\nthe fine-grained features of prediction sequences without over-\nfitting. As the patch-wise cross-attention treats each patch as a\nunity, the respect to causality within prediction sequence, i.e.,\nthe masking to the upper triangular parts of query-key match\nmatrix, is superfluous in the decoder of FPPformer.\nD. Diagonal-Masked (DM) Self-Attention\nIt is known that outliers always occur in real-world systems,\nespecially for IoT systems owning immense and diverse data.\nThese anomalies sometimes exist in the form of small patches\n[36] so that patch-attention cannot be immune to them. Com-\npared with smoothing with filters [6], [7], which is natural but\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6 IEEE INTERNET OF THINGS JOURNAL,\nğ‘ğ‘1\nğ‘ğ‘2\nğ‘ğ‘3\nğ‘ğ‘4\nğ‘˜ğ‘˜1 ğ‘˜ğ‘˜2 ğ‘˜ğ‘˜3 ğ‘˜ğ‘˜4\nDiagonal-masked patch-wise \nself-attention\nDiagonal-masked element-wise \nself-attention (The 3rd patch)\n{ğ‘˜ğ‘˜1:12}ğ‘ğ‘=3\n{ğ‘ğ‘1:12}ğ‘ğ‘=3\nFig. 5. An example of query-key matching matrices in diagonal-masked\nelement-wise self-attention and patch-wise self-attention. The total sequence\nlength is 48 and the sequence is divided into 4 patches, each with the length\nof 12, in this example. The white hollow boxes denote the normal unchanged\nmatrix elements while the black solid boxes, i.e., the matrix elements at the\ndiagonal, denote the masked matrix elements.\nnot flexible enough, it is better to devise mechanisms inside\nthe networks to circumvent the negative effects of outliers\nin the latent space. Representation learning is a fair answer\n[16] but needs heavy parameter tuning and not very stable.\nDirectly masking the input sequence [37] gives rise to another\nproblem analogous to the preceding position embedding prob-\nlem in decoder since the fixed parameters cannot sufficiently\nrepresent the dynamic sequence features. Enlightened by [38],\nwe mask the diagonal of query-key matching matrix of both\nelement-wise and patch-wise self-attention blocks in encoder,\nas sketched in Fig. 5. Thereby, any element(patch) during the\nattention can merely be expressed by the values of the rest\nof the elements(patches). Those elements or patches whose\ncharacteristics confine with the general ones are scarcely\naffected but the outliers are impossible to be expressed by\nnormal elements(patches), hence their values are restored to\napproach the general level and the negative effects of them\nare mitigated.\nE. Projection\nThe prediction sequence is acquired through the summation\nof the linear projections of encoder output and decoder output.\nThe first linear projection is supposed to represent the linear\ncorrelations of input and prediction sequence while the second\nlinear projection, together with the entire decoder, is supposed\nto represent the non-linear correlations. The loss function (2)\nis the summation of MSE function (3) and MAE function (4)\naccording to [39], [40].\nLoss = MSE(x1:V\nt2:t3 , y1:V\nt2:t3 ) +MAE(x1:V\nt2:t3 , y1:V\nt2:t3 ) (2)\nMSE(x, y) = 1\nn\nXn\ni=1\n(xi âˆ’ yi)2 (3)\nMAE(x, y) = 1\nn\nXn\ni=1\n|xi âˆ’ yi| (4)\nV. E XPERIMENTS\nWe attempt to answer three questions via the experiments\non FPPformer:\n1) Can FPPformer outperform temporarily state-of-the-art\nTSFTs and TSFMs on commonly-used benchmarks with\nthe settings of both short input sequence length and long\ninput sequence length (Section V-C)?\n2) Are the unique mechanisms proposed to be applied in\nFPPformer literally effective or useful (Section V-D) and\nwhatâ€™s about their parameter sensitivity (Section V-E)?\n3) Why does FPPformer own better or worse performances\nthan other baselines? Can we figure it out via visualiza-\ntion (Section V-G)?\nA. Baselines and Datasets\nTo unveil the empirical forecasting capability of FPPformer,\nwe perform multivariate forecasting experiments on eight\nbenchmarks involved in four types of IoT systems, includ-\ning electricity consumption (ETTh 1, ETTh2, ETTm1, ETTm2\n[15], ECL [41]), traffic flow (Traffic [42]), meteorological\nconditions (Weather [43]) and solar power production (Solar\n[44]). Their numerical details are presented in Table II. Eight\ntemporarily state-of-the-art forecasting baselines, including\nfour TSFTs (Triformer [10], Crossformer [8], Scaleformer [9],\nPatchTST [16]) and two TSFMs (FiLM [6] and TSMixer\n[30]), are employed to make comparison with FPPformer.\nIt is worth mentioning that they are all superb forecasting\nmethods proposed in the recent two years. Specially, besides\nScaleformer, the other three of four TSFTs are PTSFTs so that\nFPPformer does not have an edge on pure attention mechanism\ndesign. Furthermore, we notice that these six forecasting\nbaselines own different variable treatment strategies, e.g., some\nof them are channel-independent while some of them are not,\nwhich means that multivariate forecasting results cannot fully\ntypify their forecasting capabilities. Therefore, we additionally\nperform univariate forecasting results on M4 dataset [45],\nwhich is a competition dataset qualified for univariate fore-\ncasting, rather than delibrately choosing a variate within the\nabove multivariate forecasting datasets to perform univariate\nTABLE II\nTHE NUMERICAL DETAILS OF EIGHT MULTIVARIATE DATASETS\nDatasets Sizes Dimensions Frequencies\nETTh1 17420 7 1h\nETTh2 17420 7 1h\nETTm1 69680 7 15min\nETTm2 69680 7 15min\nECL 26304 321 1h\nTraffic 17544 862 1h\nWeather 52696 21 10min\nSolar 52560 137 10min\nTABLE III\nTHE NUMERICAL DETAILS OF FOUR M4 SUB -DATASETS\nSub-datasets Prediction Lengths Periodicities Frequencies Instances\nM4-Monthly 18 12 1month 48,000\nM4-Weekly 13 1 1week 359\nM4-Daily 14 1 1day 4,227\nM4-Hourly 48 24 1hour 414\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nSHEN et al.: TAKE AN IRREGULAR ROUTE: ENHANCE THE DECODER OF TIME-SERIES FORECASTING TRANSFORMER 7\nforecasting experiments like many other researches [6], [8]â€“\n[10]. Its details are elaborated in Table III.\nB. Implementation Details\nWe would like to present a persuasive and fair comparison\nof FPPformer and other baselines, therefore we set the hyper-\nparameters of FPPformer identical to the commonly-used\nones. The input sequence lengths are different in different\nsub-experiments but are kept identical for all baselines. The\nnumber of stages are 3 in both encoder and decoder of\nFPPformer, and the size of the initial segmented patch is\n6, which are in accordance with those of Crossformer [8].\nThe embedding dimension D (in Fig. 4) is 32. As for hyper-\nparameters with respect to the training process, FPPformer is\ntrained via an Adam optimizer with the learning rate of 1e-4,\nwhich decays by half per epoch with totally ten epochs and\nthe patience of one. The batch size is 16 and the dropout rate\nis 0.1. These are all commonly employed settings. All exper-\niments, which are conducted on a single NVIDIA GeForce\nRTX 3090 24GB GPU, are repeated for five times with\ncasual seeds and the average results are presented. The source\ncodes are implemented by Python 3.8.8 and Pytorch 1.11.0\nin https://github.com/OrigamiSL/FPPformer. Correspondingly,\nthe other baselines used in this work also merely employ the\nfixed hyper-parameters and settings, which are chosen after\nreferencing their default ones. As for those with multiple\nchoices and versions, we choose the one that owns the\nbest general performance. How we use the other baselines\nfor experiment can all be found in our provided GitHub\nrepository. The best results in each table are highlighted with\nbold and italic and the second best are highlighted with\nunderline and italic , barring a special table in Section V-D.\nC. Quantitative Results\nWe commence with the multivariate forecasting experi-\nments, whose results are shown in Table IV and Table V.\nUnder many real-world occasions, training samples are limited\nso that long input sequence length is not always available\nfor some deep forecasting methods needing it for satisfactory\nperformances. Therefore, we first measure the performances of\nFPPformer and its six competitors in eight multivariate bench-\nmarks with input sequence length of 96, which is ascribable\nto well-known Autoformer [24] in Table IV. The prediction\nlengths are commonly agreed-upon {96, 192, 336, 720}. Then\nwe evaluate the performances of the same seven models and\ndatasets using longer input sequence length within {192, 384,\n576}, whose results are shown in Table V. MSE (3) and MAE\n(4) are utilized as the evaluation metrics. The average results\nTABLE IV\nMULTIVARIATE FORECASTING RESULTS WITH SHORT INPUT LENGTH\nMethods Datasets ETTh1 ETTh2 ETTm1 ETTm2\nMetrics/Prediction Lengths 96 192 336 720 96 192 336 720 96 192 336 720 96 192 336 720\nFPPformer MSE 0.373 0.425 0.470 0.479 0.296 0.372 0.418 0.422 0.313 0.362 0.393 0.448 0.176 0.243 0.302 0.398\nMAE 0.391 0.421 0.442 0.463 0.343 0.392 0.427 0.435 0.350 0.377 0.401 0.437 0.258 0.301 0.340 0.396\nTriformer MSE 0.419 0.484 0.513 0.711 0.742 1.028 1.049 1.223 0.362 0.419 0.466 0.531 0.240 0.387 0.545 1.928\nMAE 0.446 0.486 0.489 0.638 0.585 0.708 0.745 0.836 0.402 0.443 0.484 0.513 0.326 0.449 0.532 0.924\nCrossformer MSE 0.472 0.533 0.698 0.847 0.884 2.835 2.339 4.387 0.444 0.522 0.649 0.818 0.350 0.643 1.051 3.988\nMAE 0.493 0.516 0.602 0.713 0.690 1.381 1.241 1.810 0.467 0.524 0.592 0.672 0.421 0.596 0.774 1.426\nScaleformer MSE 0.443 0.430 0.457 0.498 0.328 0.434 0.489 0.474 0.327 0.394 0.416 0.467 0.175 0.251 0.316 0.438\nMAE 0.460 0.451 0.472 0.503 0.380 0.448 0.490 0.495 0.381 0.422 0.441 0.474 0.268 0.319 0.361 0.436\nPatchTST MSE 0.393 0.438 0.482 0.492 0.296 0.380 0.417 0.425 0.332 0.377 0.407 0.473 0.177 0.243 0.305 0.401\nMAE 0.427 0.459 0.494 0.519 0.346 0.395 0.428 0.443 0.366 0.385 0.405 0.442 0.274 0.324 0.363 0.420\nFiLM MSE 0.423 0.474 0.510 0.557 0.299 0.384 0.425 0.443 0.354 0.390 0.422 0.481 0.183 0.249 0.310 0.409\nMAE 0.423 0.452 0.468 0.521 0.345 0.397 0.436 0.454 0.371 0.387 0.408 0.441 0.266 0.307 0.344 0.399\nTSMixer MSE 0.441 0.491 0.531 0.536 0.320 0.406 0.438 0.455 0.339 0.380 0.410 0.472 0.184 0.248 0.308 0.406\nMAE 0.441 0.468 0.488 0.509 0.365 0.415 0.446 0.463 0.370 0.388 0.407 0.441 0.267 0.307 0.345 0.399\nMethods Datasets ECL Traffic Weather Solar\nMetrics/Prediction Lengths 96 192 336 720 96 192 336 720 96 192 336 720 96 192 336 720\nFPPformer MSE 0.179 0.186 0.203 0.246 0.481 0.479 0.486 0.518 0.174 0.219 0.275 0.352 0.234 0.263 0.270 0.273\nMAE 0.254 0.264 0.281 0.317 0.295 0.290 0.291 0.309 0.210 0.251 0.291 0.342 0.261 0.276 0.277 0.281\nTriformer MSE 0.179 0.191 0.209 0.253 â€” â€” â€” â€” 0.174 0.219 0.272 0.357 0.185 0.203 0.241 0.220\nMAE 0.277 0.289 0.308 0.344 â€” â€” â€” â€” 0.242 0.290 0.323 0.378 0.247 0.253 0.285 0.269\nCrossformer MSE 0.234 0.285 0.342 0.543 0.532 0.543 0.562 0.571 0.156 0.210 0.260 0.355 0.167 0.203 0.216 0.216\nMAE 0.323 0.351 0.381 0.471 0.295 0.296 0.303 0.321 0.227 0.278 0.321 0.385 0.214 0.239 0.251 0.249\nScaleformer MSE 0.180 0.195 0.215 0.257 0.551 0.578 0.587 0.611 0.198 0.313 0.395 0.595 0.206 0.218 0.266 0.283\nMAE 0.297 0.308 0.328 0.363 0.341 0.350 0.354 0.364 0.273 0.372 0.420 0.549 0.305 0.303 0.330 0.333\nPatchTST MSE 0.196 0.199 0.214 0.256 0.589 0.579 0.589 0.631 0.188 0.231 0.285 0.359 0.240 0.274 0.295 0.295\nMAE 0.279 0.283 0.299 0.331 0.379 0.368 0.371 0.391 0.228 0.263 0.301 0.349 0.285 0.308 0.317 0.317\nFiLM MSE 0.198 0.199 0.217 0.279 0.648 0.600 0.610 0.694 0.197 0.240 0.290 0.361 0.314 0.355 0.395 0.397\nMAE 0.274 0.278 0.300 0.357 0.385 0.362 0.367 0.427 0.239 0.272 0.307 0.352 0.334 0.356 0.371 0.364\nTSMixer MSE 0.222 0.226 0.244 0.287 0.725 0.706 0.727 0.775 0.191 0.235 0.286 0.358 0.263 0.302 0.332 0.335\nMAE 0.308 0.315 0.332 0.363 0.453 0.449 0.458 0.477 0.236 0.271 0.307 0.352 0.305 0.328 0.345 0.345\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8 IEEE INTERNET OF THINGS JOURNAL,\nTABLE V\nAVERAGE MULTIVARIATE RESULTS WITH LONG INPUT LENGTHS\nMethods Metrics/Input Lengths 192 384 576\nFPPformer MSE (Avg. of all) 0.358 0.347 0.345\nMAE (Avg. of all) 0.354 0.352 0.350\nTriformer MSE (Avg. of all) 0.932 0.966 1.546\nMAE (Avg. of all) 0.621 0.639 0.828\nCrossformer MSE (Avg. of all) 1.319 1.192 1.290\nMAE (Avg. of all) 0.729 0.707 0.747\nScaleformer MSE (Avg. of all) 0.605 0.555 0.598\nMAE (Avg. of all) 0.539 0.517 0.520\nPatchTST MSE (Avg. of all) 0.377 0.357 0.354\nMAE (Avg. of all) 0.378 0.368 0.366\nFiLM MSE (Avg. of all) 0.397 0.381 0.383\nMAE (Avg. of all) 0.391 0.385 0.390\nTSMixer MSE (Avg. of all) 0.397 0.372 0.367\nMAE (Avg. of all) 0.392 0.379 0.377\nTABLE VI\nCOMPARISON OF MODELS EMPLOYING CROSS -VARIABLE ATTENTION\nMethods (Dataset: Solar) Metrics/Prediction Length 96 192 336 720\nFPPformer-Cross MSE 0.162 0.174 0.191 0.199\nMAE 0.209 0.232 0.237 0.238\niTransformer MSE 0.215 0.247 0.267 0.268\nMAE 0.255 0.279 0.294 0.297\nCrossformer MSE 0.167 0.203 0.216 0.216\nMAE 0.214 0.239 0.251 0.249\nof eight benchmarks with prediction length of 720 are given\nin Table V to refrain from tedious data stacking. Full results\nare given in our released repository provided in Section V-B.\nâ€˜â€”â€™ refers to the fact that certain model is out of the memory\n(24GB) even batch size is set to 1.\nAs Table IV and V show, FPPformer outperforms other\nbaselines in most of situations with both short and long input\nsequence lengths. When input sequence length is set to 96,\nFPPformer obtains 31.7%/60.0%/10.5%/6.4%/12.8%/14.7%\nMSE reduction compared with Triformer/Crossformer/ Scale-\nformer/PatchTST/FiLM/TSMixer, which illustrates the superb\nforecasting capability of FPPformer with the setting of short\ninput sequence length. Though it seems that FPPformer fails\nto own a superior performance when experimenting on Solar\ndataset, FPPformer reconquers its leading position with longer\ninput length when handling the same dataset (Concrete results\nare available at github repository provided in Section V-B).\nFurthermore, if also equiped with cross-variable attention 2\nlike Crossformer, which means that a cross-variable attention\nmodule proposed by Crossformer is arranged at the end of\neach stage of the encoder and decoder in FPPformer, the\nmodified FPPformer, denoted by FPPformer-Cross in Table\nVI, is capable of completely outperforming Crossformer and\niTransformer [46], which is another state-of-the-art model\nemploying cross-variable attention, under Solar dataset.\nWhen prolonging the input sequence length within {192,\n2We use the statement of â€˜cross-variableâ€™, rather than â€˜cross-dimensionâ€™ in\nCrossformer, to maintain the identical description of the variable dimension\nthroughout this work.\n384, 576 }, FPPformer achieves better forecasting perfor-\nmances and respectively obtains 9.5%/12.3%/13.0% general\nMSE reduction when compared with the FPPformer with\nshort input length in Table IV. Moreover, FPPformer ob-\ntains 67.4%/72.3%/40.2%/3.5%/9.5%/7.5% MSE reduction\ncompared with Triformer/Crossformer/Scaleformer/PatchTST/\nFiLM/TSMixer with longer input lengths in general. These\nphenomena illustrate the superb forecasting performances of\nFPPformer especially handling longer input sequence.\nThen we compare the univariate forecasting capability of\nFPPformer with other six baselines on M4. We omit the first\ntwo subsets with sampling frequencies of a year and a quarter\nsince many of their instance lengths are too short, and only\nperform experiments on the rest of four subsets. The prediction\nlengths, which are regulated by [45], are {18, 13, 14, 48 } for\n{M4-Monthly, M4-Weekly, M4-Daily, M4-Hourly}. The input\nsequence lengths for them are separately {72, 65, 84, 336 }\nafter consulting [29]. We change these four input sequence\nlengths a little to {72, 72, 96, 384 } for the sake of rendering\nthem fitting the patch-wise attention in FPPformer. The M4-\nspecifical metrics SMAPE (5) and OW A (7) are used for\nmeasurement. m refers to the periodicity of series and na Â¨Ä±ve2\nrefers to the results of a seasonally-adjusted forecast model by\n[45] for scaling in OW A.\nSMAPE = 200\nt3 âˆ’ t2\nX\ntâˆˆ[t2,t3)\n|yt âˆ’ xt|\n|yt| + |xt| (5)\nMASE = 1\nt3âˆ’t2\nX\ntâˆˆ[t2,t3)\n|ytâˆ’xt|\n1\nt3âˆ’t1âˆ’m\nP\njâˆˆ[t1+m+ 1,t3) |xj âˆ’xjâˆ’m| (6)\nOW A= 1\n2( SMAPE\nSMAPENaÂ¨Ä±ve2\n+ MASE\nMASENaÂ¨Ä±ve2\n) (7)\nThe univariate forecasting results are shown in Table VII.\nIt could be observed that FPPformer gains 88.6%/47.8%/\n21.0%/54.1%/50.8%/20.8% MSE reduction in general when\ncompared with Triformer/Crossformer/Scaleformer/PatchTST/\nFiLM/TSMixer, which is more persuasive to verify the fore-\ncasting capability of FPPformer.\nTABLE VII\nUNIVARIATE FORECASTING RESULTS\nMethods Metrics M4-Monthly M4-Weekly M4-Daily M4-Hourly\nFPPformer SMAPE 8.674 8.711 3.214 17.468\nOW A 0.891 1.464 1.057 1.373\nTriformer SMAPE 65.161 106.905 82.288 78.169\nOW A 11.068 19.181 41.565 5.756\nCrossformer SMAPE 24.801 12.702 6.901 28.471\nOW A 2.704 3.021 3.077 2.706\nScaleformer SMAPE 14.275 10.591 3.362 19.939\nOW A 1.163 1.409 1.163 2.017\nPatchTST SMAPE 15.127 15.687 4.457 47.657\nOW A 1.237 2.679 1.519 7.526\nFiLM SMAPE 32.329 15.029 8.376 21.681\nOW A 3.489 3.966 3.899 2.240\nTSMixer SMAPE 13.886 10.962 3.429 19.769\nOW A 0.947 1.515 1.131 1.516\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nSHEN et al.: TAKE AN IRREGULAR ROUTE: ENHANCE THE DECODER OF TIME-SERIES FORECASTING TRANSFORMER 9\nTABLE VIII\nABLATION RESULTS WITH PREDICTION LENGTHS OF 720\nArchitecture Attention Decoder Sturcture DM ETTh1 ETTh2 ETTm1 ETTm2 ECL Traffic Weather Solar Avg. MSE(Increase rate)\nFPPformer\nPoint-wise top-down âœ“ 0.706 0.446 0.708 0.404 0.818 1.298 0.343 0.371 0.637(+84.5%)\nPatch-wise top-down âœ“ 0.479 0.422 0.441 0.384 0.227 0.501 0.336 0.248 0.380(+10.0%)\nPoint-wise & Patch-wise bottom-up\n(e.g. Crossformer) âœ“ 0.471 0.427 0.446 0.405 0.235 0.522 0.340 0.264 0.389(+12.6%)\nPoint-wise & Patch-wise Linear\n(e.g. PatchTST) âœ“ 0.473 0.425 0.437 0.385 0.229 0.488 0.238 0.333 0.376(+9.0%)\nPoint-wise & Patch-wise top-down Ã— 0.469 0.421 0.439 0.385 0.227 0.500 0.337 0.254 0.379(+9.8%)\n(Complete FPPformer) Point-wise & Patch-wise top-down âœ“ 0.449 0.394 0.411 0.366 0.199 0.432 0.316 0.194 0.345\nTABLE IX\nMSE RESULTS OF PARAMETER SENSITIVITY ON STAGE NUMBERS\nMethods/Stages M1( ËœM1) M2( ËœM2) M4( ËœM3) M4( ËœM4) Dev\nFPPformer 0.349(1.00) 0.344(0.99) 0.345(0.99) 0.351(1.01) 0.03\nScaleformer 0.417(1.00) 0.425(1.02) 0.434(1.04) 0.433(1.04) 0.10\nTriformer 0.787(1.00) 0.797(1.01) 0.964(1.23) 0.814(1.03) 0.27\nPatchTST 0.370(1.00) 0.369(1.00) 0.385(1.04) 0.382(1.03) 0.07\nCrossformer 1.475(1.00) 1.339(0.91) 1.466(0.99) 1.498(1.02) 0.12\nD. Ablation Study\nWe conduct ablation studies to validate the functions of the\narchitecture of FPPformer and its components. All variants are\nexperimented with multivariate benchmarks with prediction\nsequence length of 720. The results of all eight benchmarks\nare presented in Table VIII. Each value is the average of\nfour sub-experiment results with input sequence lengths of\n{96, 192, 384, 576 }. As expected, only using point-wise\nattention like canonical TSFT gives rise to a losses increasing\nof 84.6% (Average loss: 0.345â†’0.637). Meanwhile, only\nusing patch-wise attention like other PTSFTs does not suffer\na severe performance degradation but still owns apparently\nworse performance than our proposed combined patch-wise\nattention and point-wise attention (Average loss: 0.380 vs.\n0.345). As for the decoder architecture design, FPPformer\nsurpasses the same models but replacing the decoder archi-\ntecture of FPPformer with that of Crossformer (Average loss:\n0.345 vs. 0.389) or simply substituting the entire decoder\nwith a linear projection like PatchTST (Average loss: 0.345\nvs. 0.376). Besides, removing DM mechanisms in the self-\nattention blocks of encoder also results in worse performances\n(Average loss: 0.345â†’0.379). Conclusively, the efficiency and\nnecessity of all unique parts and architectures of FPPformer\nare verified.\nE. Parameter Sensitivity Analysis\nIt is well-known that TSFT cannot own too many layers\nor stages, otherwise the risk of over-fitting substantially rises.\nThereby, we perform parameter analysis on the stage number\nof FPPformer in this section to check out whether FPPformer\nis capable of handling this problem. The parameter analysis\nof patch size is no longer tested as it has been well studied by\n[8], [10], [16]. The input sequence length is chosen as 576 for\nusing more stages for FPPformer and other models use their\ndefault input sequence lengths, which are supposed to be best\nfor them (96 for {Scaleformer, Triformer}; 512 for PatchTST;\n336 for Crossformer). The number of stages are chosen in {1,\n2, 3, 4 }. The prediction sequence length is set as 720 and the\naverage results (MSEs) of all eight multivariate benchmarks\nare presented in Table IX. The result of each model with stage\nnumber of one is utilized as the normalization factor to further\nmeasure their absolute performance deviations with more stage\nnumbers which are manifested by (8).\nDev =\nX4\ni=2\n| ËœMi âˆ’ ËœM1|, ËœMi = Mi/M1 (8)\n96 192 384 576\nInput length\n96 192 384 576\nInput length Input length\n96 192 384 576\n(b1) (b3)(b2)\n96 192 384 576\nInput length\n96 192 384 576\nInput length Input length\n96 192 384 576\n(a1) (a3)(a2)\n0.5\n1\n1.5\n2\nValue/MB\n104 GPU Memory Computation\nFPPformer\nCrossformer\n200\n400\n600\n800\n1000Value/s\nFPPformer-Enc\nPatchTST\n0.5\n1\n1.5\n2\nValue/MB\n104\nFPPformer-Enc\nPatchTST\n5\n10\n15Value/ms\nFPPformer-Enc\nPatchTST\n200\n400\n600\n800\n1000Value/s\nTraining Time Per Epoch\nFPPformer\nCrossformer\n5\n10\n15\n20\n25Value/ms\nInference Time Per Instance on GPU\nFPPformer\nCrossformer\nFig. 6. The results of training time per epoch (1), GPU memory computation (2) and inference time per instance (3) in two experiments (a)(b).\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10 IEEE INTERNET OF THINGS JOURNAL,\nValueValue\nTimestamp \nï¼ˆa) ETTh1 ï¼ˆb) ETTm2 ï¼ˆc) Solar\nTimestamp\nï¼ˆf) ECL\nTimestamp\nï¼ˆe) Weather\nTimestamp \nï¼ˆd) Traffic\n0 48 96 144 192\n-2\n-1.5\n-1\n-0.5\n0\nTruth\nFPPformer\nPatchTST\nCrossformer\nScaleformer\nTriformer\n0 48 96 144 192\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1 Truth\nFPPformer\nPatchTST\nCrossformer\nScaleformer\nTriformer\n0 48 96 144 192\n-1\n0\n1\n2\n3\n4\n5\n6\n7 Truth\nFPPformer\nPatchTST\nCrossformer\nScaleformer\nTriformer\n0 48 96 144 192\n-1\n0\n1\n2\n3\n4\n5\nTruth\nFPPformer\nPatchTST\nCrossformer\nScaleformer\n0 48 96 144     192 240 288-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n2\n2.5\nTruth\nFPPformer\nPatchTST\nCrossformer\nScaleformer\nTriformer\n0 48 96 144 192\n-1\n-0.5\n0\n0.5\n1 Truth\nFPPformer\nPatchTST\nCrossformer\nScaleformer\nTriformer\nTimestamp Timestamp \nFig. 7. Forecasting windows of FPPformer and other four TSFTs from six datasets. The black line is the ground truth, the red line is the forecasting curve\nof FPPformer and the rest are the forecasting curves of other TSFTs.\nMi(i = 1, 2, 3, 4) are the original average MSE results and\nËœMi(i = 1, 2, 3, 4) are the normalized ones. When comparing\nthe performances and performance deviations of five TSFTs,\nit is evident that FPPformer not only keeps its leading po-\nsition (smaller errors) among all TSFTs with different stage\nnumbers, but also maintains its robustness ascendancy (smaller\ndeviations) over other TSFTs.\nF . Complexity Analysis\nWe compare the training time per epoch/the GPU memory\ncomputation/the inference time per instance on GPU of FPP-\nformer and Crossformer [8], and the identical four measuring\ncriteria of FPPformer without decoder and PatchTST [16] dur-\ning multivariate forecasting under Solar dataset. Solar dataset\nis selected in that it owns the intermediate variable number\namong all datasets. Crossformer and PatchTST are chosen as\nthey are also patch-wise attention based models. The decoder\nof FPPformer is removed when compared with PatchTST\nsince PatchTST does not employ the decoder architecture.\nThe input sequence lengths are chosen within {96, 192, 384,\n576} and the prediction sequence length is 96. The other\nhyper-parameters and settings are identical with those used\nin the quantitative multivariate results, barring the size of\nthe hidden(embedding) dimension. The size of the hidden\ndimension, which is the one that exceedingly affects the model\ncomplexity, is identical for all baselines in this experiment so\nthat the model architecture design can determine the model\ncomplexity to the utmost extent. These two experiment results\nare shown in Fig. 6(a)(b) respectively.\nAs Fig. 6(a) shows, the full FPPformer owns linear com-\nputation and space complexity with input sequence length.\nMoreover, Fig. 6(b) illustrates that the encoder of FPPformer\n(FPPformer-Enc for short) also only owns linear complexity\nand the complexity of FPPformer and PatchTST are analogous,\ndemonstrating that the element-wise attention, which is almost\nthe only difference between PatchTST and FPPformer-Enc,\nmerely brings minuscule additional complexity.\nG. Case Study\nTo more vividly illustrate the outstanding forecasting perfor-\nmance of FPPformer, we visualize several forecasting windows\nof FPPformer and other TSFTs from different datasets in\nFig. 7. Benefitting from exploiting better decoder and atten-\ntion mechanism, FPPformer excels in capturing the features\nof trends (Fig. 7(a)), seasons (Fig. 7(b)) and their hybrids\n0 1 2 3 4 5 6 7 0.0\n0.2\n0.4\n0.6\n0.8\n1.00\n1\n2\n3\n4\n5\n6\n7\n0 1 2 3 4 5 6 70 1 2 3 4 5 6 7\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7 Query\nKey\n(c1)\nKey\n(c2)\nKey\n(c3)\nQuery\nQuery\nTimestamp\n(a)\n0 2 4 8 6 10 0.0\n0.2\n0.4\n0.6\n0.8\n1.00\n2\n4\n6\n8\n10\nQuery\nKey\n(b1)\nKey\n(b2)\nKey\n(b3)\nQuery\nQuery\n0 2 4 8 6 100 2 4 8 6 10\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nDM Attention Remove DM Attention \nDuring Test Phase\nRemove DM Attention \nDuring Train and Test Phase\nPatch-wise AttentionElement-wise Attention\nInput Sequence\nValue\n0 12 24 36 48 60 72 84 96-1.8\n-1.6\n-1.4\n-1.2\n-1\n-0.8\n-0.6\n-0.4\nFig. 8. The visualization of DM patch-wise (b) and element-wise (c)\nself-attention score distributions via heat map. All figures are obtained by\nFPPformer or its variants with the identical input sequence (a). Specially, the\nelement-wise attention score heat maps stem from the fifth patch, which is\nsupposed to be anomalous and marked with red in (a).\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nSHEN et al.: TAKE AN IRREGULAR ROUTE: ENHANCE THE DECODER OF TIME-SERIES FORECASTING TRANSFORMER 11\n(Fig. 7(c)) so that smaller forecasting errors than others\ncan be obtained. Moreover, its preponderance of robustness\nand immunity against outliers are revealed in Fig. 7(d)(e)(f)\nwhere certain distribution shifts occur in partial distincts. In\naddition, we present some visualizations of the feature maps\nof FPPformer and several competitors in the latent space to\nvalidate the functions of its unique modules.\na. We visualize the attention score distribution of the first\nDM patch-wise and DM element-wise attention in FPPformer\nwith a certain input window of length 96 in ETTh 1 dataset,\nvia heat map in Fig.8. As illustrated in Fig. 8(b1), the attention\nscore distribution is uniformly distributed if applying the DM\npatch-wise attention during the training phase. Even substi-\ntuting the DM patch-wise attention with the normal attention\nsolely in the testing phase (Fig.8(b2)) will not lead to the\nself-matches with high scores, demonstrating the enhancement\nof DM attention mechanism on universal feature extraction.\nHowever, it can be observed in Fig.8(b3) that the highest\nattention score chiefly lies in the fifth patch, which corresponds\nto an outlier patch with a exorbitant dip. The visualization\n(Fig.8(c)) of the three different element-wise attention score\nheat maps of that outlier patch in Fig.8(b) also manifests the\nsame phenomenon,i.e., the excessive high or low values can\nresult in the excessive high attention scores which burden the\nuniversal feature extraction.\nb. As the additional element-wise attention is employed\nin FPPformer to reinforce the inner-patch feature extraction,\nwe visualize the feature maps outputted by each patch-wise\nattention in the encoders of FPPformer, PatchTST and Cross-\nformer by T-SNE [47] in Fig.9. To get rid of the influences\nfrom other different modules, this experiment is performed\nas a reconstruction experiment, which reconstructs the input\nsequences via the ultimate encoder features, and we remove\n-40 -20 0 20 40\nf1\n-40\n-20\n0\n20\n40f2\nCrossformer (Stage 1)\n10\n20\n30\n40\n50\n60\n70\n80\n90\n-30 -20 -10 0 10 20 30\nf1\n-30\n-20\n-10\n0\n10\n20\n30\n40f2\nCrossformer (Stage 2)\n5\n10\n15\n20\n25\n30\n35\n40\n45\n-60 -40 -20 0 20 40\nf1\n-40\n-20\n0\n20\n40\n60\n80f2\nCrossformer (Stage 3)\n5\n10\n15\n20\n-60 -40 -20 0 20 40\nf1\n-40\n-30\n-20\n-10\n0\n10\n20\n30f2\nFPPformer (Stage 1)\n10\n20\n30\n40\n50\n60\n70\n80\n90\n-40 -30 -20 -10 0 10 20\nf1\n-40\n-20\n0\n20\n40f2\nFPPformer (Stage 2)\n5\n10\n15\n20\n25\n30\n35\n40\n45\n-2000 -1000 0 1000 2000\nf1\n-1000\n-500\n0\n500\n1000\n1500\n2000f2\nFPPformer (Stage 3)\n5\n10\n15\n20\n-30 -20 -10 0 10 20\nf1\n-20\n-10\n0\n10\n20\n30f2\nPatchTST\n10\n20\n30\n40\n50\n60\n70\n(b)\n(c1) (c2) (c3)\n(d1) (d2) (d3)\n0 192 336 432 528-2\n-1\n0\n1\n2\n3\n4\n5 Input Sequence\nTimestamp\n(a)\nValue\n288 384 480 57648 96 144 240\nFig. 9. The visualization of the feature maps of the patches in different\nencoder stages (a) in PatchTST (b), Crossformer (c) and FPPformer (d) via\nT-SNE. The points with different colors denote different patches. Only the\nfeature map of the last encoder stage of PatchTST is shown since it is not\nhierarchical.\n0\n2\n4\n6\n8\n10\n12\n14 Query\n0 2 4 6 8 10 12 14\nKey\n0\n2\n4\n6\n8\n10\n12\n14 Query\n0 2 4 6\nKey\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n2\n4\n6\n8\n10\n12\n14 Query\n0 2\nKey\n0 1 2 3\n0\n1\n2\n3 Query\n0\n1\n2\n3\n4\n5\n6\n7 Query\n0 1 2 3 4 5 6 7\nKeyKey\n 0 2 4 6 8 10 12 14\nKey\n0.0\n0.2\n0.4\n0.6\n0.8\n1.00\n2\n4\n6\n8\n10\n12\n14\nFPPformer (Stage 1) FPPformer (Stage 2) FPPformer (Stage 3)\n(a1) (a2) (a3)\nCrossformer (Stage 1) Crossformer (Stage 2) Crossformer (Stage 3)\nQuery\n(b1) (b2) (b3)\nFig. 10. The visualization of the cross-attention score distribution of different\ndecoder stages in Crossformer (a) and FPPformer (b) via heat map.\nthe DM technique in the patch-wise attentions of FPPformer\nbut keep the DM element-wise attention. The input sequence\nlength is set as 576 to provide enough data points in the T-\nSNE figures. Apparently, the distances of the data points in\nthree FPPformer-related figures are the smallest among three\nmodels, indicating that the DM element-wise attentions can\nliterally assist in better representing the features of each patch\nin the latent space so that it is easier for the following patch-\nwise attentions in FPPformer encoder to extract the universal\nfeatures among different patches and FPPformer can have an\nedge on extracting universal features compared with other two\nbaselines.\nc. To vividly illustrate that the top-down architecture in\ndecoder can genuinely render the construction of prediction\nsequence feature maps more general in the latent space, we\nvisualize the patch-wise cross-attention score distributions of\ndifferent decoder layers in FPPformer and Crossformer via\nheat maps in Fig. 10. Obviously, with the stage number\ngrows, the attention score matrix size is getting bigger in FPP-\nformer and smaller in Crossformer, illustrating the differences\nbetween â€˜bottom-upâ€™ and â€˜top-downâ€™ decoder architecture.\nMeanwhile, it can be observed that the highest attention scores\nin Crossformer primarily locate at the last patch, especially\n8%\n 6%\n13%\n73%\nCrossformer\n0 1 2 3\n35%\n15%\n17%\n33%\nFPPformer\n0 1 2 3Position: Position:\n(a) (b)\nFig. 11. The distribution of the highest cross-attention score positions of the\nthird stage in Crossformer (a) and the first stage in FPPformer (b).\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12 IEEE INTERNET OF THINGS JOURNAL,\nthe third stage, indicating that the construction of the pre-\ndiction sequence features heavily rely on the rear end of\ninput sequence features and it fails to build up the prediction\nsequence in an universal manner. On contrary, the cross-\nattention scores in the decoder uniformly distribute along the\ntemporal dimension, which implies the preponderance of the\ntop-down architecture in FPPformer decoder. In effect, the\ninstance in Fig. 10. is not a particular situation. We collect\nthe highest attention score positions of the third stage in\nCrossformer and the first stage in FPPformer, where the most\ncoarse-grained features lie in, when handling the whole ETTh1\ndataset, i.e., with over 100, 000 instances. The result is shown\nin Fig.11. Apparently, the highest attention score distribution\nof FPPformer is much more uniform than that of Crossformer,\nindicating the success of the top-down decoder design in\nFPPformer.\nVI. D ISCUSSION\nThough FPPformer has achieved state-of-the-art perfor-\nmances, it still owns at least two limits:\n1. The hierarchy in FPPformer can be more exquisitely\ndevised. The â€˜mergingâ€™ operation in the encoder of FPPformer\nis too simple to well represent the feature map of the bigger\npatch via the two smaller patch ingredients. So does the\nâ€˜splittingâ€™ operation in the decoder. The cutting-edge methods\nto handle the combination or the split of patches, e.g., Swin-\nTransformer [48], in CV field, where patch-wise attention is\nalso prevailing, can be learned, imitated and modified in time-\nseries forecasting Transformer.\n2. Currently, the outlier is tackled via DM self-attention,\nwhich roughly mask the entire diagonal of the self-attention\nscore matrix, in FPPformer. Notice that the outliers shall be\nfewer than the normal segments of time-series sequences,\nwhich implies that the majority of masked patches are indeed\nnormal and the masking behavior can negatively influences\nthe feature extraction of input sequences. We believe that\napplying a prior anomaly detection method to each input se-\nquence before forecasting and then only masking the detected\nanomalous patches can be a better format of utilizing the DM\nself-attention.\nBoth of the foregoing two limits and potential solutions will\nbe our future research directions.\nVII. C ONCLUSION\nIn this work, we attempt to further develop the time-\nseries forecasting Transformer from the perspective of decoder.\nWe lucubrate the existing decoder designs, point out their\ndrawbacks and propose our solutions. The ultimate product,\ni.e., FPPformer, achieves state-of-the-art performaces in mul-\ntiple benchmarks, including multivariate and univariate ones,\nleveraging from refined attention mechanism and enhanced\nencoder-decoder architecture proposed by us.\nACKNOWLEDGMENT\nThis work was partially supported by National Natural\nScience Foundation of China under grant #U19B2033 and\nNo.62103414.\nREFERENCES\n[1] J. Han, G. H. Lee, S. Park, J. Lee, and J. K. Choi, â€œA multivariate-\ntime-series-prediction-based adaptive data transmission period control\nalgorithm for iot networks,â€ IEEE Internet of Things Journal , vol. 9,\nno. 1, pp. 419â€“436, 2022.\n[2] G. E. P. Box, G. M. Jenkins, and J. F. MacGregor, â€œSome recent\nadvances in forecasting and control,â€ Journal of the Royal Statistical\nSociety: Series C (Applied Statistics) , vol. 23, no. 2, pp. 158â€“179,\n1974. [Online]. Available: https://rss.onlinelibrary.wiley.com/doi/abs/10.\n2307/2346997\n[3] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series\nanalysis: forecasting and control . John Wiley & Sons, 2015.\n[4] K. Benidis, S. S. Rangapuram, V . Flunkert, Y . Wang, D. Maddix,\nC. Turkmen, J. Gasthaus, M. Bohlke-Schneider, D. Salinas, L. Stella, F.-\nX. Aubet, L. Callot, and T. Januschowski, â€œDeep learning for time series\nforecasting: Tutorial and literature survey,â€ ACM Comput. Surv., vol. 55,\nno. 6, dec 2022. [Online]. Available: https://doi.org/10.1145/3533382\n[5] A. Zeng, M. Chen, L. Zhang, and Q. Xu, â€œAre transformers effective\nfor time series forecasting?â€ 2023.\n[6] T. Zhou, Z. Ma, xue wang, Q. Wen, L. Sun, T. Yao, W. Yin, and R. Jin,\nâ€œFiLM: Frequency improved legendre memory model for long-term\ntime series forecasting,â€ in Advances in Neural Information Processing\nSystems, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022.\n[Online]. Available: https://openreview.net/forum?id=zTQdHSQUQWc\n[7] C. Challu, K. G. Olivares, B. N. Oreshkin, F. Garza, M. Mergenthaler-\nCanseco, and A. Dubrawski, â€œNhits: Neural hierarchical interpolation\nfor time series forecasting,â€ 2023.\n[8] Y . Zhang and J. Yan, â€œCrossformer: Transformer utilizing cross-\ndimension dependency for multivariate time series forecasting,â€ in The\nEleventh International Conference on Learning Representations , 2023.\n[Online]. Available: https://openreview.net/forum?id=vSVLM2j9eie\n[9] M. A. Shabani, A. H. Abdi, L. Meng, and T. Sylvain, â€œScaleformer:\nIterative multi-scale refining transformers for time series forecasting,â€\nin The Eleventh International Conference on Learning Representations ,\n2023. [Online]. Available: https://openreview.net/forum?id=sCrnllCtjoE\n[10] R.-G. Cirstea, C. Guo, B. Yang, T. Kieu, X. Dong, and S. Pan,\nâ€œTriformer: Triangular, variable-specific attentions for long sequence\nmultivariate time series forecasting,â€ in Proceedings of the Thirty-First\nInternational Joint Conference on Artificial Intelligence, IJCAI-22, L. D.\nRaedt, Ed. International Joint Conferences on Artificial Intelligence\nOrganization, 7 2022, pp. 1994â€“2001, main Track. [Online]. Available:\nhttps://doi.org/10.24963/ijcai.2022/277\n[11] H. Wang, J. Peng, F. Huang, J. Wang, J. Chen, and Y . Xiao,\nâ€œMICN: Multi-scale local and global context modeling for long-\nterm series forecasting,â€ in The Eleventh International Conference\non Learning Representations , 2023. [Online]. Available: https:\n//openreview.net/forum?id=zt53IDUR1U\n[12] T. Kim, J. Kim, Y . Tae, C. Park, J.-H. Choi, and J. Choo,\nâ€œReversible instance normalization for accurate time-series forecasting\nagainst distribution shift,â€ in International Conference on Learning\nRepresentations, 2022. [Online]. Available: https://openreview.net/\nforum?id=cGDAkQo1C0p\n[13] Z. Liu, R. Godahewa, K. Bandara, and C. Bergmeir, â€œHandling concept\ndrift in global time series forecasting,â€ ArXiv, vol. abs/2304.01512,\n2023. [Online]. Available: https://api.semanticscholar.org/CorpusID:\n257921602\n[14] S. B. Taieb and A. F. Atiya, â€œA bias and variance analysis for multistep-\nahead time series forecasting,â€ IEEE Transactions on Neural Networks\nand Learning Systems , vol. 27, no. 1, pp. 62â€“76, 2016.\n[15] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang,\nâ€œInformer: Beyond efficient transformer for long sequence time-series\nforecasting,â€ in The Thirty-Fifth AAAI Conference on Artificial Intelli-\ngence, AAAI 2021, Virtual Conference , vol. 35, no. 12. AAAI Press,\n2021, pp. 11 106â€“11 115.\n[16] Y . Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam, â€œA time series\nis worth 64 words: Long-term forecasting with transformers,â€ in The\nEleventh International Conference on Learning Representations , 2023.\n[Online]. Available: https://openreview.net/forum?id=Jbdc0vTOcol\n[17] Y . Liu, H. Wu, J. Wang, and M. Long, â€œNon-stationary transformers:\nExploring the stationarity in time series forecasting,â€ in Advances\nin Neural Information Processing Systems , A. H. Oh, A. Agarwal,\nD. Belgrave, and K. Cho, Eds., 2022. [Online]. Available: https:\n//openreview.net/forum?id=ucNDIDRNjjv\n[18] S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A. X. Liu, and\nS. Dustdar, â€œPyraformer: Low-complexity pyramidal attention for\nlong-range time series modeling and forecasting,â€ in International\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nSHEN et al.: TAKE AN IRREGULAR ROUTE: ENHANCE THE DECODER OF TIME-SERIES FORECASTING TRANSFORMER 13\nConference on Learning Representations , 2022. [Online]. Available:\nhttps://openreview.net/forum?id=0EXmFzUn5I\n[19] T.-Y . Lin, P. DollÂ´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,\nâ€œFeature pyramid networks for object detection,â€ in 2017 IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) , 2017, pp.\n936â€“944.\n[20] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, â€œPath aggregation network for\ninstance segmentation,â€ in 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2018, pp. 8759â€“8768.\n[21] B. Huang, H. Dou, Y . Luo, J. Li, J. Wang, and T. Zhou, â€œAdaptive\nspatiotemporal transformer graph network for traffic flow forecasting by\niot loop detectors,â€ IEEE Internet of Things Journal , vol. 10, no. 2, pp.\n1642â€“1653, 2023.\n[22] Y . Jiang, S. Niu, K. Zhang, B. Chen, C. Xu, D. Liu, and H. Song, â€œSpa-\ntialâ€“temporal graph data mining for iot-enabled air mobility prediction,â€\nIEEE Internet of Things Journal , vol. 9, no. 12, pp. 9232â€“9240, 2022.\n[23] S. Li, X. Jin, Y . Xuan, X. Zhou, W. Chen, Y .-X. Wang,\nand X. Yan, â€œEnhancing the locality and breaking the\nmemory bottleneck of transformer on time series forecasting,â€ in\nAdvances in Neural Information Processing Systems , H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alch Â´e-Buc, E. Fox,\nand R. Garnett, Eds., vol. 32. Curran Associates, Inc.,\n2019. [Online]. Available: https://proceedings.neurips.cc/paper/2019/\nfile/6775a0635c302542da2c32aa19d86be0-Paper.pdf\n[24] H. Wu, J. Xu, J. Wang, and M. Long, â€œAutoformer: Decomposition\ntransformers with auto-correlation for long-term series forecasting,â€\nin Advances in Neural Information Processing Systems , M. Ranzato,\nA. Beygelzimer, Y . Dauphin, P. Liang, and J. W. Vaughan,\nEds., vol. 34. Curran Associates, Inc., 2021, pp. 22 419â€“\n22 430. [Online]. Available: https://proceedings.neurips.cc/paper/2021/\nfile/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf\n[25] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, and R. Jin, â€œFEDformer:\nFrequency enhanced decomposed transformer for long-term series\nforecasting,â€ in Proceedings of the 39th International Conference on\nMachine Learning , ser. Proceedings of Machine Learning Research,\nK. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and\nS. Sabato, Eds., vol. 162. PMLR, 17â€“23 Jul 2022, pp. 27 268â€“27 286.\n[Online]. Available: https://proceedings.mlr.press/v162/zhou22g.html\n[26] G. Woo, C. Liu, D. Sahoo, A. Kumar, and S. C. H. Hoi, â€œEtsformer:\nExponential smoothing transformers for time-series forecasting,â€ ArXiv,\nvol. abs/2202.01381, 2022.\n[27] G. Lai, W.-C. Chang, Y . Yang, and H. Liu, â€œModeling long- and\nshort-term temporal patterns with deep neural networks,â€ in The 41st\nInternational ACM SIGIR Conference on Research &amp; Development\nin Information Retrieval , ser. SIGIR â€™18. New York, NY , USA:\nAssociation for Computing Machinery, 2018, p. 95â€“104. [Online].\nAvailable: https://doi.org/10.1145/3209978.3210006\n[28] M. LIU, A. Zeng, M. Chen, Z. Xu, Q. LAI, L. Ma, and Q. Xu, â€œSCINet:\nTime series modeling and forecasting with sample convolution and\ninteraction,â€ in Advances in Neural Information Processing Systems ,\nA. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022. [Online].\nAvailable: https://openreview.net/forum?id=AyajSjTAzmg\n[29] B. N. Oreshkin, D. Carpov, N. Chapados, and Y . Bengio, â€œN-beats:\nNeural basis expansion analysis for interpretable time series forecasting,â€\nin International Conference on Learning Representations , 2020.\n[Online]. Available: https://openreview.net/forum?id=r1ecqn4YwB\n[30] V . Ekambaram, A. Jati, N. H. Nguyen, P. Sinthong, and J. Kalagnanam,\nâ€œTsmixer: Lightweight mlp-mixer model for multivariate time series\nforecasting,â€ Proceedings of the 29th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining , 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:259187817\n[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin, â€œAttention is all you\nneed,â€ in Advances in Neural Information Processing Systems ,\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates,\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/\n2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[32] W. Fan, P. Wang, D. Wang, D. Wang, Y . Zhou, and Y . Fu, â€œDish-ts: A\ngeneral paradigm for alleviating distribution shift in time series forecast-\ning,â€ in Proceedings of the AAAI Conference on Artificial Intelligence ,\nvol. 37, no. 6, 2023, pp. 7522â€“7529.\n[33] S. Chen, G. Long, T. Shen, and J. Jiang, â€œPrompt federated\nlearning for weather forecasting: Toward foundation models on\nmeteorological data,â€ in Proceedings of the Thirty-Second International\nJoint Conference on Artificial Intelligence, IJCAI-23 , E. Elkind, Ed.\nInternational Joint Conferences on Artificial Intelligence Organization,\n8 2023, pp. 3532â€“3540, main Track. [Online]. Available: https:\n//doi.org/10.24963/ijcai.2023/393\n[34] H. Xue and F. D. Salim, â€œPromptCast: A New Prompt-based\nLearning Paradigm for Time Series Forecasting,â€ arXiv e-prints , p.\narXiv:2210.08964, Sep. 2022.\n[35] . Ruch, David K., Wavelet theory an elementary approach with\napplications. Hoboken, N.J. : John Wiley &amp; Sons, 2009.,\n2009, description based upon print version of record.;Includes\nbibliographical references and indexes.;English. [Online]. Available:\nhttps://search.library.wisc.edu/catalog/9911067192202121\n[36] K.-H. Lai, D. Zha, J. Xu, Y . Zhao, G. Wang, and X. Hu,\nâ€œRevisiting time series outlier detection: Definitions and benchmarks,â€\nin Thirty-fifth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 1) , 2021. [Online]. Available:\nhttps://openreview.net/forum?id=r8IvOsnHchr\n[37] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff, â€œA\ntransformer-based framework for multivariate time series representation\nlearning,â€ in Proceedings of the 27th ACM SIGKDD Conference on\nKnowledge Discovery & Data Mining , ser. KDD â€™21. New York,\nNY , USA: Association for Computing Machinery, 2021, p. 2114â€“2124.\n[Online]. Available: https://doi.org/10.1145/3447548.3467401\n[38] Z. You, L. Cui, Y . Shen, K. Yang, X. Lu, Y . Zheng, and\nX. Le, â€œA unified model for multi-class anomaly detection,â€ in\nAdvances in Neural Information Processing Systems , A. H. Oh,\nA. Agarwal, D. Belgrave, and K. Cho, Eds., 2022. [Online]. Available:\nhttps://openreview.net/forum?id=bMYU8 qD8PW\n[39] Z. Yue, Y . Wang, J. Duan, T. Yang, C. Huang, Y . Tong, and B. Xu,\nâ€œTs2vec: Towards universal representation of time series,â€ in AAAI,\n2022.\n[40] G. Woo, C. Liu, D. Sahoo, A. Kumar, and S. Hoi, â€œCoST:\nContrastive learning of disentangled seasonal-trend representations for\ntime series forecasting,â€ in International Conference on Learning\nRepresentations, 2022. [Online]. Available: https://openreview.net/\nforum?id=PilZY3omXV2\n[41] A. Trindade, â€œElectricityLoadDiagrams20112014,â€ UCI Machine Learn-\ning Repository, 2015, DOI: https://doi.org/10.24432/C58C86.\n[42] (n.d.) Caltrans pems. [Online]. Available: http://pems.dot.ca.gov/\n[43] (n.d.) Max-planck-institut fuer biogeochemie - wetterdaten. [Online].\nAvailable: http://pems.dot.ca.gov/\n[44] (n.d.) Solar power data for integration studies. [Online]. Available:\nhttps://www.nrel.gov/grid/solar-power-data.html\n[45] S. Makridakis, E. Spiliotis, and V . Assimakopoulos, â€œThe m4\ncompetition: Results, findings, conclusion and way forward,â€\nInternational Journal of Forecasting , vol. 34, no. 4, pp. 802â€“\n808, 2018. [Online]. Available: https://www.sciencedirect.com/science/\narticle/pii/S0169207018300785\n[46] Y . Liu, T. Hu, H. Zhang, H. Wu, S. Wang, L. Ma, and M. Long,\nâ€œitransformer: Inverted transformers are effective for time series\nforecasting,â€ ArXiv, vol. abs/2310.06625, 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:263830644\n[47] L. van der Maaten and G. Hinton, â€œVisualizing data using t-sne,â€ Journal\nof Machine Learning Research , vol. 9, no. 86, pp. 2579â€“2605, 2008.\n[Online]. Available: http://jmlr.org/papers/v9/vandermaaten08a.html\n[48] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, â€œSwin transformer: Hierarchical vision transformer using shifted\nwindows,â€ in 2021 IEEE/CVF International Conference on Computer\nVision (ICCV), 2021, pp. 9992â€“10 002.\nThis article has been accepted for publication in IEEE Internet of Things Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JIOT.2023.3341099\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8661912083625793
    },
    {
      "name": "Encoder",
      "score": 0.8117935061454773
    },
    {
      "name": "Transformer",
      "score": 0.5754412412643433
    },
    {
      "name": "Decoding methods",
      "score": 0.5463486313819885
    },
    {
      "name": "Source code",
      "score": 0.4623706340789795
    },
    {
      "name": "Computer engineering",
      "score": 0.4165210425853729
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3427436053752899
    },
    {
      "name": "Real-time computing",
      "score": 0.3283935487270355
    },
    {
      "name": "Data mining",
      "score": 0.32467415928840637
    },
    {
      "name": "Algorithm",
      "score": 0.2721569538116455
    },
    {
      "name": "Programming language",
      "score": 0.0984170138835907
    },
    {
      "name": "Voltage",
      "score": 0.08809599280357361
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    }
  ],
  "cited_by": 8
}