{
    "title": "Surgical-DINO: adapter learning of foundation models for depth estimation in endoscopic surgery",
    "url": "https://openalex.org/W4392579942",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2368815896",
            "name": "Cui, Beilei",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A4223722506",
            "name": "Islam, Mobarakol",
            "affiliations": [
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences",
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A2185378682",
            "name": "Bai Long",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2222319403",
            "name": "Ren, Hongliang",
            "affiliations": [
                "National University of Singapore",
                "Chinese University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2989184872",
        "https://openalex.org/W4225372979",
        "https://openalex.org/W4390874575",
        "https://openalex.org/W6608025442",
        "https://openalex.org/W4389213248",
        "https://openalex.org/W6600655081",
        "https://openalex.org/W4390190100",
        "https://openalex.org/W6601630192",
        "https://openalex.org/W4382567565",
        "https://openalex.org/W6601158483",
        "https://openalex.org/W2963760790",
        "https://openalex.org/W4200072305",
        "https://openalex.org/W3179159099",
        "https://openalex.org/W2609883120",
        "https://openalex.org/W3009257710",
        "https://openalex.org/W3035434014",
        "https://openalex.org/W2985775862",
        "https://openalex.org/W3152803807",
        "https://openalex.org/W125693051"
    ],
    "abstract": "Abstract Purpose Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results Our model is extensively validated on a MICCAI challenge dataset of SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically show that Surgical-DINO significantly outperforms all the state-of-the-art models in endoscopic depth estimation tasks. The analysis with ablation studies has shown evidence of the remarkable effect of our LoRA layers and adaptation. Conclusion Surgical-DINO shed some light on the successful adaptation of the foundation models into the surgical domain for depth estimation. There is clear evidence in the results that zero-shot prediction on pre-trained weights in computer vision datasets or naive fine-tuning is not sufficient to use the foundation model in the surgical domain directly.",
    "full_text": "International Journal of Computer Assisted Radiology and Surgery (2024) 19:1013–1020\nhttps://doi.org/10.1007/s11548-024-03083-5\nORIGINAL ARTICLE\nSurgical-DINO: adapter learning of foundation models for depth\nestimation in endoscopic surgery\nBeilei Cui 1 · Mobarakol Islam 2 · Long Bai 1 · Hongliang Ren 1,3\nReceived: 12 January 2024 / Accepted: 16 February 2024 / Published online: 8 March 2024\n© The Author(s) 2024\nAbstract\nPurpose Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visu-\nalization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation\n(e.g., DINOv2), recent works observed its limitations in medical and surgical domain-speciﬁc applications. This work presents\na low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation.\nMethods We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation\nof the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with\nsurgery-speciﬁc domain knowledge instead of conventional ﬁne-tuning. During training, we freeze the DINO image encoder,\nwhich shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate\nfeatures from the surgical scene.\nResults Our model is extensively validated on a MICCAI challenge dataset of SCARED, which is collected from da Vinci\nXi endoscope surgery. We empirically show that Surgical-DINO signiﬁcantly outperforms all the state-of-the-art models in\nendoscopic depth estimation tasks. The analysis with ablation studies has shown evidence of the remarkable effect of our\nLoRA layers and adaptation.\nConclusion Surgical-DINO shed some light on the successful adaptation of the foundation models into the surgical domain\nfor depth estimation. There is clear evidence in the results that zero-shot prediction on pre-trained weights in computer vision\ndatasets or naive ﬁne-tuning is not sufﬁcient to use the foundation model in the surgical domain directly.\nKeywords Surgical scene understanding · Foundation models · Depth estimation · Adapter learning\nBeilei Cui and Mobarakol Islam have contributed equally to this work.\nB Hongliang Ren\nhlren@ee.cuhk.edu.hk\nBeilei Cui\nbeileicui@link.cuhk.edu.hk\nMobarakol Islam\nmobarakol.islam@ucl.ac.uk\nLong Bai\nb.long@link.cuhk.edu.hk\n1 The Chinese University of Hong Kong, Hong Kong, China\n2 Wellcome/EPSRC Centre for Interventional and Surgical\nSciences (WEISS), University College London, London, UK\n3 Department of BME, National University of Singapore,\nSingapore, Singapore\nIntroduction\n3D scene reconstruction in endoscopic surgery has a sig-\nniﬁcant impact on the development of automated surgery\nand promotes the advancement of various downstream\napplications such as surgical navigation, depth perception,\naugmented reality [ 1–3]. However, there are still many unre-\nsolved challenges in dense depth estimation tasks within\nendoscopic scenes. The variability of soft tissues and occlu-\nsion by surgical tools in the surgical environment poses high\ndemands on the model’s ability to reconstruct dynamic depth\nmaps [4]. Recent methods have focused on utilizing binocular\ninformation to obtain disparity maps and reconstruct depth\ninformation [1, 4]. However, apart from the da Vinci surgical\nrobot system, most endoscopic surgical robot systems only\nconsist of a monocular camera, which is a more cost-effective\nand easily implementable hardware solution. Therefore, pre-\n123\n1014 International Journal of Computer Assisted Radiology and Surgery (2024) 19:1013–1020\ncise depth estimation tasks based on monocular endoscopy\nare still an area that requires further exploration.\nRecently, foundation models have become one of the\nmost popular terms in the ﬁeld of deep learning [ 5, 6].\nThanks to their large number of model parameters, foun-\ndation models have the ability to build long-term memory of\nmassive training data, achieving state-of-the-art performance\non various downstream tasks involving vision, text and\nmultimodal inputs. However, when encountering domain-\nspeciﬁc scenarios such as surgical scenes, the predictive\nability of foundation models tends to decline signiﬁcantly\n[7]. Due to the limited availability of annotated data in\nmedical scenes and insufﬁcient computational resources,\ntraining a medical-speciﬁc foundation model from scratch\nposes various challenges. Therefore, there has been exten-\nsive discussion on adapting existing foundation models to\ndifferent sub-domains, maximizing the utilization of exist-\ning model parameters, and ﬁne-tuning foundation models for\ntarget application scenarios based on limited computational\nresources [ 7–9]. Chen et al. [ 8] constructed their adapter\nusing two MLP layers and an activation function without\ninputting any prompt for ﬁne-tuning the Segment Anything\n(SAM) model. On the other hand, Wu et al. [ 9] used a simple\npixel classiﬁer as a self-prompt to achieve zero-shot seg-\nmentation based on SAM. However, the adapter layer shall\nslow down inference speed, and prompts cannot be directly\noptimized through training. Therefore, we have designed our\nadaptation solution based on Low-Rank Adaptation (LoRA)\n[10]. LoRA adds a bypass next to the original foundation\nmodel, which performs a dimensionality reduction and then\nan elevation operation to simulate the intrinsic rank. When\ndeployed in a production environment, LoRA can be intro-\nduced without introducing inference delays, and only the\npre-trained model parameters need to be combined with the\nLoRA parameters. Therefore, LoRA can serve as an efﬁcient\nadaption tool in real-world applications of foundation mod-\nels.\nAdditionally, current works on ﬁne-tuning vision founda-\ntion models to the medical domain have focused on common\ntasks such as segmentation and detection, with limited explo-\nration in pixel-wise regression tasks like depth estimation.\nIn this case, supervised training paradigms for visual foun-\ndation models are typically applied to common semantic\nunderstanding tasks and may not be suitable for our needs.\nTherefore, we have chosen DINOv2 [ 6] as the starting point\nfor our study in this paper. DINOv2 is a self-supervised\ntrained foundation model for multiple vision tasks. The self-\nsupervised training paradigm enables DINOv2 to effectively\nlearn uniﬁed visual features, thereby requiring only cus-\ntomized decoders to adapt DINOv2 to various downstream\nvisual tasks including depth estimation. Therefore, we aim to\nexplore the ﬁne-tuning of the DINOv2 encoder to fully utilize\nthe pre-trained extensive parameters and beneﬁt downstream\ndepth estimation tasks in the surgical domain. Speciﬁcally,\nour key contributions and ﬁndings are:\n• We ﬁrstly extend the foundation model in computer\nvision, DINOv2, to explore its capability on medical\nimage depth estimation problems.\n• We present an adaptation and ﬁne-tuning strategy of\nDINOv2 based on the Low-Rank Adaptation technique\nwith low additional training costs toward the surgical\nimage domain.\n• Our method, Surgical-DINO, is validated on two publicly\navailable datasets and obtained superior performance\nover other state-of-the-art depth estimation methods for\nsurgical images. We also investigate that the zero-shot\nfoundation model is not yet ready for use in surgical\napplications, and LoRA adaptation is crucial, which out-\nperformed naive ﬁne-tuning.\nMethodology\nPreliminaries\nDINOv2\nLearning pre-trained representations without regard to spe-\nciﬁc tasks has been proven extremely effective in Natural\nLanguage Processing (NLP) [ 11]. One can use features\nfrom these pre-trained representations without ﬁne-tuning\nfor downstream tasks and obtain signiﬁcantly better perfor-\nmances than those task-speciﬁc models. Oquab et al. [ 6]\ndeveloped a similar \"foundation\" model, named DINOv2,\nin computer vision where vision features at both image level\nand pixel level generated from it can work without any task\nlimitation. They proposed an automatic pipeline to build a\nlarge, curated and dedicated image dataset and an unsuper-\nvised learning method to learn robust vision features. A ViT\nmodel [ 12] with 1B parameters was trained in a discrimi-\nnative self-supervised training manner and distilled into a\nseries of smaller models that were evaluated to have sur-\npassing ability against the best available all-purpose features\non most of the benchmarks at image and pixel levels. Depth\nestimation task was also tested as a classical dense prediction\ntask in computer vision by training a simple depth decoder\nhead following DINOv2 and gained excellent performance\nin the general computer vision realm. The huge domain gap\nbetween medical and natural images may impede the utiliza-\ntion of such a foundation model; thus, we ﬁrst attempt to\ndevelop a simple but effective adaptation method to exploit\nDINOv2 for the surgical domain.\n123\nInternational Journal of Computer Assisted Radiology and Surgery (2024) 19:1013–1020 1015\nFig. 1 The proposed Surgical-DINO framework. The input image is\ntransformed into tokens by extracting scaled-down patches followed by\na linear projection. A positional embedding and a patch-independent\nclass token (red) are used to augment the embedding subsequently. We\nfreeze the image encoder and add trainable LoRA layers to ﬁne-tune\nthe model. We extract tokens from different layers, then up-sample and\nconcatenate them to form the embedding features. Another trainable\ndecode head is used on top of the frozen model to estimate the ﬁnal\ndepth\nLoRA\nLow-Rank Adaptation (LoRA) was ﬁrst proposed in [ 10]\nto ﬁne-tune large-scale foundation models in NLP to down-\nstream tasks. It was inspired by the low “intrinsic dimension”\nof the pre-trained large model that random projection to a\nsmaller subspace does not affect its ability to learn effec-\ntively. By injecting trainable rank decomposition matrices\ninto each layer of the Transformer architecture and freezing\nthe pre-trained model weights, LoRA signiﬁcantly reduces\nthe amount of trainable parameters for downstream tasks.\nTo be speciﬁc, for a pre-trained weight matrix W\n0 ∈ Rd×k ,\nLoRA utilizes a low-rank decomposition to restrict its update\nby W\n0 + /Delta1W = W0 + BA where B ∈ Rd×r , A ∈ Rr ×k\nwith the rank r ≪ min (d,k). W0 does not receive gradient\nupdates during the training process, while only A and B con-\ntain trainable parameters. The modiﬁed forward pass is then\ndescribed as:\nh = W0 x + /Delta1Wx = W0 x + BA x . (1)\nThis implementation can signiﬁcantly reduce the memory\nand storage usage for training thus very suitable for ﬁne-\ntuning large-scale foundational models to downstream tasks.\nSurgical-DINO\nAs illustrated in Fig. 1, the architecture of our proposed\nSurgical-DINO depth estimation framework inherits from\nDINOv2. Given a surgical image x ∈ RH ×W ×C with spatial\nresolution H ×W and channels C, we aim to predict its depth\nmap ˆD ∈ H × W as close to ground truth depth as possible.\nDINOv2 serves as an image encoder where images are ﬁrst\nsplit into patches of size p2 and then ﬂattened with linear pro-\njection. A positional embedding is augmented for the tokens\nand another learnable class token is added which aggregates\nthe global image information for subsequent missions. The\nimage embeddings will then go through a series of Trans-\nformer blocks to generate new token representations. All\nparameters in the DINOv2 image encoder are frozen during\ntraining, and we added additional LoRA layers to each Trans-\nformer block to capture the learnable information. These side\nLoRA layers, as illustrated in the previous section, compress\nthe Transformer vision features to the low rank space and\nthen re-project back to match the output features’ channels\nin the frozen transformer blocks. LoRA layers in each Trans-\nformer block work independently and do not share weights.\nSeveral intermediate and the ﬁnal output token representa-\ntions will be resized and bi-linearly upsampled by a factor of\n4 ﬁrst and then concatenated to output the overall feature rep-\nresentation. A simple trainable depth decoder head is utilized\nat the end to predict the depth map.\nLoRA layers\nDifferent from ﬁne-tuning the whole model, freezing the\nmodel and adding trainable LoRA layers will largely reduce\nthe required memory and computation resources for train-\ning and also beneﬁt conveniently deploying the model. The\nLoRA design in Surgical-DINO is presented in Fig. 2.W ef o l -\nlowed [13] where the low-rank approximation is only applied\nfor q and v projection layers to avoid excessive inﬂuence on\n123\n1016 International Journal of Computer Assisted Radiology and Surgery (2024) 19:1013–1020\nFig. 2 The LoRA design in Surgical-DINO. We apply LoRA only to q\nand v projection layers in each transformer block. Wq , Wk , Wv and Wo\ndenote the projection layer of q,k,v and o, respectively\nattention scores. With the aforementioned fundamental for-\nmulation of LoRA, for an encoded token embedding x ,t h e\nprocessing of q,k and v projection layers within a multi-head\nself-attention block will become:\nQ = ˆWq a = Wq a + Bq Aq a,\nK = Wk a,\nV = ˆWva = Wva + Bv Ava,\n(2)\nwhere Wq , Wk and Wv are frozen projection layers for q,k\nand v; Aq , Bq , Av and Bv are trainable LoRA layers. The\nself-attention mechanism remains unchanged that described\nby:\nAtt(Q, K , V ) = Softmax\n( QK\nT\n√Cout\n+ B\n)\nV (3)\nwhere Cout denotes the numbers of output tokens.\nNetwork architecture\nImage Encoder . The image is ﬁrst separated into non-\noverlapping patches and then projected to image embeddings\nwith the Embedding process. The image embeddings are a set\nof t 0 =\n{\nt 0\n0 ,..., t 0\nN p\n}\n,t 0\nn ∈ RD tokens, where p is the patch\nsize, N p = HW\np2 , t0 is the class token, and D represents the\nfeature dimensions of each token. L Transformers are then\nused to transform the image tokens into feature representa-\ntions tl where l denotes the output of l-th Transformer block.\nWe utilized the pre-trained ViT-Base model from DINOv2 as\nour image encoder with 12 Transformer blocks and a feature\ndimension of 784.\nDepth Decoder . We ﬁrst extract the layers from l =\n{3,6,9,12}, unﬂatten them to ﬁt the patch resolution and\nup-sample tokens by a factor of 4 to increase the resolu-\ntion. We treat depth prediction as a classiﬁcation problem by\ndividing the depth range into 256 uniformly distributed bins\nwith a linear layer to predict the depth. The predicted map is\nscaled to align the input resolution eventually.\nLoss functions\nSurgical-DINO utilizes Scale-invariant depth loss [ 14] and\nGradient loss [ 15] as the supervision constraints for the ﬁne-\ntuning process. They can be described by:\nL\npixel = λ1\n\n√\n1\nn\n∑\ni\n(gi )2 − λ2\nn2\n( ∑\ni\ngi\n) 2\nLgrad = λ3\n1\nn\n∑\nk\n∑\ni\n(\n|∇x gk\ni |+| ∇y gk\ni |\n) (4)\nwhere n denotes the number of valid pixels; gk\ni = log ˜dk\ni −\nlogd k\ni is the value of the log-depth difference map at position\ni and scale k. Lpixel guides the network to predict truth depth,\nwhile Lgrad encourages the network to predict smoother gra-\ndient changes. The ﬁnal loss is then described as:\nL = Lpixel + Lgrad . (5)\nExperiment\nDataset\nSCARED1 dataset is collected with a da Vinci Xi endoscope\nfrom fresh porcine cadaver abdominal anatomy and contains\n35 endoscopic videos with 22,950 frames. A projector is used\nto obtain high-quality depth maps of the scene. Each video\nhas ground truth depth and ego-motion, while we only used\ndepth to evaluate our method. We followed the split scheme in\n[16] where the SCARED dataset is split into 15,351, 1705 and\n551 frames for the training, validation and test sets, respec-\ntively.\nHamlyn\n2 is a laparoscopic and endoscopic video dataset\ntaken from various surgical procedures with challenging in\nvivo scenes. We followed the selection in [ 17] with 21 videos\nfor validation.\nImplementation details\nThe framework is implemented with PyTorch on NVIDIA\nRTX 3090 GPU. We adopt the AdamW [ 18] optimizer with\n1 https://endovissub2019-scared.grand-challenge.org/ .\n2 https://hamlyn.doc.ic.ac.uk/vision/ .\n123\nInternational Journal of Computer Assisted Radiology and Surgery (2024) 19:1013–1020 1017\nTable 1 Quantitative depth\ncomparison on the SCARED\ndataset of SOTA depth\nestimation methods\nMethod Abs Rel ↓ Sq Rel ↓ RMSE ↓ RMSE log ↓ δ ↑\nSfMLearner [ 19] 0.079 0.879 6.896 0.110 0.947\nFang et al. [ 20] 0.078 0.794 6.794 0.109 0.946\nDefeat-Net [ 21] 0.077 0.792 6.688 0.108 0.941\nSC-SfMLearner [ 22] 0.068 0.645 5.988 0.097 0.957\nMonodepth2 [ 23] 0.071 0.590 5.606 0.094 0.953\nEndo-SfM [ 24] 0.062 0.606 5.726 0.093 0.957\nAF-SfMLearner [ 16] 0.059 0.435 4.925 0.082 0.974\nDINOv2 [ 6] (zero-shot) 0.088 0.963 7.447 0.120 0.933\nDINOv2 [ 6] (ﬁne-tuned) 0.060 0.459 4.692 0.081 0.963\nSurgical-DINO SSL (Ours) 0.059 0.427 4.904 0.081 0.974\nSurgical-DINO (Ours) 0.053 0 .377 4 .296 0 .074 0 .975\nThe best results are in bold. The second-best results are in italic\nTable 2 Quantitative depth\ncomparison on Hamlyn dataset Method Abs Rel ↓ Sq Rel ↓ RMSE ↓ RMSE log ↓ δ ↑\nEndo-depth-and-motion [ 17] 0.185 5.424 16.100 0.225 0.732\nAF-SfMLearner [ 16] 0.168 4.440 13.870 0.204 0.770\nSurgical-DINO (ours) 0.146 3 .216 11 .974 0 .178 0 .801\nThe best results are in bold. The second-best results are in italic\nan initial learning rate of 1 × 10−5 and weight decay of\n1 × 10−4. The batch size is set to 8 with 50 epochs in total.\nWe can achieve our evaluation results with the following\nweights set: λ1 = 1.0,λ2 = 0.85,λ3 = 0.5. The images are\nresized to 224 × 224 pixels. We also trained our proposed\nmodel in a Self-Supervised Learning (SSL) manner with the\nbaseline of AF-SfMLearner [ 16]. We replace the encoder in\nAF-SfMLearner with Surgical-DINO and resize the image\nto 224 × 224 pixels to ﬁt the patch size of DINOv2.\nPerformance metrics\nWe evaluate our method with ﬁve common metrics used in\ndepth estimation problems: Abs Rel, Sq Rel, RMSE, RMSE\nlog and δ in which lower is better for the ﬁrst four metrics\nand larger is better for the last one. During evaluation, we re-\nscale the predicted depth map by a median scaling method\nintroduced by SfM-Leaner [ 19], which can be expressed by\nD\nscaled =\n(\nDpred ∗\n(\nmedian\n(\nDgt\n)\n/ median\n(\nDpred\n)))\n.(6)\nWe capped the depth map at 150 mm which can cover\nalmost all depth values.\nResults\nQuantitative results on SCARED. We compared our pro-\nposed method with several SOTA self-supervised methods\n[16, 19–24] as well as zero-shot, self-supervised and super-\nvised method, and the results are shown in Table 1.A l l\nof these baseline methods were reproduced with the origi-\nnal implementation under the same dataset splits mentioned\nabove. The zero-shot performance of pre-trained DINOv2\nis evaluated on model size ViT-Base with a same depth\ndecoder head ﬁne-tuned on NYU Depth V2 [ 25]. Our method\nobtained superior performances in all the evaluation metrics\ncompared to all of the methods. It is worth noting that the\nzero-shot performance of DINOv2 has the worst results indi-\ncating that vision features and depth decoder that are highly\neffective in natural images are unsuitable for medical images\ndue to the large domain gap. While the ﬁne-tuned DINOv2\nexceeds other SOTA self-supervised methods in RMSE and\nRMSE log, it did not gain better performance in the other\nthree metrics proving its prediction to have more large depth\nerrors. Only ﬁne-tuning a depth decoder head is not enough\nto transfer the vision features to geometric relations within\nmedical images. With the adaptation method of LoRA, the\nnetwork is able to learn medical domain-speciﬁc vision fea-\ntures and relate them with depth information, thus resulting\nin an improvement in the estimation accuracy.\nQuantitative results on Hamlyn. We made zero-shot valida-\ntion for our model trained on SCARED in Hamlyn dataset\nwithout any ﬁne-tuning. For comparison, we zero-shot vali-\ndate AF-SfMLearner with their best model and obtain the\nresults of Endo-Depth-and-Motion [ 17] by averaging the\n21-fold cross-validation results trained on Hamlyn. As pre-\nsented in Table 2, our method achieves superior performance\nagainst other methods, unveiling the good generalization\nability across different cameras and surgical scenes.\n123\n1018 International Journal of Computer Assisted Radiology and Surgery (2024) 19:1013–1020\nTable 3 Comparison of encoder parameters, trainable parameters, trainable parameters’ ratio and full model inference speed\nMethod Params. (M) ↓ trainable Params. (M) ↓ trainable ratio (%) ↓ Speed (ms) ↓\nAF-SfMLearner [ 16] 11.68 11.68 100.00 9.9\nSurgical-DINO (Ours) 86.72 0.14 0 .17 18.2\nThe best results are in bold\nFig. 3 Qualitative depth\ncomparison on the SCARED\ndataset\nTable 4 Ablation study on the\nrank size on the LoRA layer Rank size Abs Rel ↓ Sq Rel ↓ RMSE ↓ RMSE log ↓ δ ↑\n1 0.058 0.389 4.513 0.081 0.962\n4 0.053 0 .375 4 .296 0.074 0.975\n8 0.053 0.376 4.324 0.074 0.974\n16 0.053 0.377 4.325 0.073 0.974\nThe best results are in bold\nModel complexity and speed evaluation. The proposed\nmodel’s parameters, trainable parameters, trainable parame-\nters ratio and inference speed are evaluated on an NVIDIA\nRTX 3090 GPU compared to AF-SfMLearner. Table 3 shows\nthat while Surgical-DINO has a larger amount of parameters,\nonly a very small part of parameters are trainable mak-\ning it faster to train and converge. The inference speed of\nSurgical-DINO is slower than AF-SfMLearner, but still in\nan acceptable range for real-time applications.\nQualitative results. We also show some qualitative results in\nFig. 3. Our method can depict anatomical structure well com-\npared to other methods. Nevertheless, the qualitative results\nof our proposed Surgical-DINO also have drawbacks like\nlack of continuity which can motivate future modiﬁcation\ndirection.\nAblation studies\nEffects of the rank size on the LoRA layer . A set of com-\nparative experiments is performed to evaluate the effects of\nrank size on the LoRA layer. We evaluated four different\nsizes of rank of the LoRA layer, and the results are shown in\nTable 4. We notice that the performance of Surgical-DINO\nwill increase with the increase of rank size within a certain\nlow range and start to drop when the rank size exceeds a\ncertain value. This phenomenon implies that despite being\ndesigned to utilize low-rank decomposition to make the\napproximation, LoRA still requires certain training param-\neters to ﬁt downstream tasks. However, too many trainable\nparameters may mislead the original weights resulting in per-\nformance degradation.\nEffects of the size of pre-trained foundation model. DINOV2\npublished four pre-trained ViT foundation models and named\nthem by their size. Table 5 presents the ablation study to\ninvestigate the effect of the size of the pre-trained founda-\ntion model. We discover that the performance increases with\nthe increase of the pre-trained model size. Larger models\ninherently have better integration and generalization abil-\nity of vision features, thus better ﬁtting downstream tasks.\nBut larger models are also accompanied by larger memory\nTable 5 Ablation study on the\nsize of pre-trained foundation\nmodel\nModel size Abs Rel ↓ Sq Rel ↓ RMSE ↓ RMSE log ↓ δ ↑\nViT-small 0.055 0.416 4.513 0.075 0.971\nViT-base 0.053 0.377 4.296 0.074 0.975\nViT-large 0.051 0.363 4.256 0.070 0.979\nViT-giant 0.050 0 .342 4 .120 0 .069 0 .980\n123\nInternational Journal of Computer Assisted Radiology and Surgery (2024) 19:1013–1020 1019\noccupancy and training costs, so we chose ViT-Base for our\ndepth estimation method in consideration of the compromise\nbetween performance and cost.\nConclusions\nDepth estimation is a vital task in robotic surgery and ben-\neﬁts many downstream tasks like surgical navigation and\n3D reconstruction. Vision Foundation model that captures\nuniversal vision features has been proven to be both effec-\ntive and convenient in many vision tasks but yet needs\nmore exploration in the surgical domain. We have presented\nSurgical-DINO, an adapter learning method that utilizes\nDINOv2, a vision foundation model, for surgical scene depth\nestimation. We design LoRA layers to ﬁne-tune the network\nwith a small number of additional parameters to adapt to the\nsurgical domain. Experiments have been made on a publicly\navailable dataset and demonstrate the superior performance\nof the proposed Surgical-DINO. We ﬁrst explore the direction\nof deploying the vision foundation model to surgical depth\nestimation tasks and reveal its enormous potential. Future\nworks could explore the foundation model in a supervised,\nself-supervised and unsupervised manner to investigate the\nrobustness and reliability in the surgical domain.\nFunding This work was supported by Hong Kong Research Grants\nCouncil (RGC) Collaborative Research Fund (C4026-21G), Gen-\neral Research Fund (GRF 14211420 & 14203323), Shenzhen-Hong\nKong-Macau Technology Research Programme (Type C) STIC Grant\nSGDX20210823103535014 (202108233000303).\nCode availability The source code is available at https://github.com/\nBeileiCui/SurgicalDINO.\nDeclarations\nConﬂict of interest The authors declare that they have no conﬂict of\ninterest.\nEthical approval This article does not contain any studies with human\nparticipants or animals performed by any of the authors.\nInformed consent This article does not contain patient data.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\n1. Zha R, Cheng X, Li H, Harandi M, Ge Z (2023) Endosurf: neural\nsurface reconstruction of deformable tissues with stereo endoscope\nvideos. International conference on medical image computing and\ncomputer-assisted intervention. Springer, Berlin, pp 13–23\n2. Liu X, Sinha A, Ishii M, Hager GD, Reiter A, Taylor RH, Unberath\nM (2019) Dense depth estimation in monocular endoscopy with\nself-supervised learning methods. IEEE Trans Med Imaging\n39(5):1438–1447\n3 . W e iX ,W a n gY ,G eL ,P e n gB ,H eQ ,W a n gR ,H u a n gL ,X u\nY , Luo J (2022) Unsupervised convolutional neural network for\nmotion estimation in ultrasound elastography. IEEE Trans Ultrason\nFerroelectr Freq Control 69(7):2236–2247\n4. Wang Y , Long Y , Fan SH, Dou Q (2022) Neural rendering\nfor stereo 3d reconstruction of deformable tissues in robotic\nsurgery. International conference on medical image computing and\ncomputer-assisted intervention. Springer, Berlin, pp 431–441\n5. Kirillov A, Mintun E, Ravi N, Mao H, Rolland C, Gustafson L,\nXiao T, Whitehead S, Berg AC, Lo W-Y , Dollár P , Girshick R\n(2023) Segment anything. arXiv preprint arXiv:2304.02643\n6. Oquab M, Darcet T, Moutakanni T, V o H, Szafraniec M, Khalidov\nV , Fernandez P , Haziza D, Massa F, El-Nouby A, Assran M, Ballas\nN, Galuba W, Howes R, Huang P-Y , Li S-W, Misra I, Rabbat M,\nSharma V , Synnaeve G, Xu H, Jegou H, Mairal J, Labatut P , Joulin\nA, Bojanowski P (2023) Dinov2: learning robust visual features\nwithout supervision. arXiv preprint arXiv:2304.07193\n7. Wang A, Islam M, Xu M, Zhang Y , Ren H (2023) Sam meets\nrobotic surgery: an empirical study on generalization, robustness\nand adaptation. arXiv preprint arXiv:2308.07156\n8. Chen T, Zhu L, Ding C, Cao R, Zhang S, Wang Y , Li Z, Sun L, Mao\nP , Zang Y (2023) Sam fails to segment anything?–sam-adapter:\nadapting sam in underperformed scenes: Camouﬂage, shadow, and\nmore. arXiv preprint arXiv:2304.09148\n9. Wu Q, Zhang Y , Elbatel M (2023) Self-prompting large vision mod-\nels for few-shot medical image segmentation. MICCAI workshop\non domain adaptation and representation transfer. Springer, Berlin,\npp 156–167\n10. Hu EJ, yelong shen Wallis P , Allen-Zhu Z, Li Y , Wang S, Wang\nL, Chen W (2022) LoRA: low-rank adaptation of large language\nmodels. In: International conference on learning representations\n11. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou\nY , Li W, Liu PJ (2020) Exploring the limits of transfer learning with\na uniﬁed text-to-text transformer. J Mach Learn Res 21(1):5485–\n5551\n12. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X,\nUnterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S,\nUszkoreit J, Houlsby N (2021) An image is worth 16x16 words:\ntransformers for image recognition at scale. In: International con-\nference on learning representations\n13. Zhang K, Liu D (2023) Customized segment anything model for\nmedical image segmentation. arXiv preprint arXiv:2304.13785\n14. Bhat SF, Alhashim I, Wonka P (2021) Adabins: depth estimation\nusing adaptive bins. In: Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp 4009–4018\n15. Li Z, Snavely N (2018) Megadepth: learning single-view depth\nprediction from internet photos. In: Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pp 2041–2050\n16. Shao S, Pei Z, Chen W, Zhu W, Wu X, Sun D, Zhang B (2022)\nSelf-supervised monocular depth and ego-motion estimation in\nendoscopy: appearance ﬂow to the rescue. Med Image Anal\n77:102338\n17. Recasens D, Lamarca J, Fácil JM, Montiel J, Civera J (2021)\nEndo-depth-and-motion: reconstruction and tracking in endo-\n123\n1020 International Journal of Computer Assisted Radiology and Surgery (2024) 19:1013–1020\nscopic videos using depth networks and photometric constraints.\nIEEE Robot Autom Lett 6(4):7225–7232\n18. Loshchilov I, Hutter F (2019) Decoupled weight decay regulariza-\ntion. In: International conference on learning representations\n19. Zhou T, Brown M, Snavely N, Lowe DG (2017) Unsupervised\nlearning of depth and ego-motion from video. In: Proceedings of\nthe IEEE conference on computer vision and pattern recognition,\npp 1851–1858\n20. Fang Z, Chen X, Chen Y , Gool LV (2020) Towards good practice\nfor CNN-based monocular depth estimation. In: Proceedings of\nthe IEEE winter conference on applications of computer vision, pp\n1091–1100\n21. Spencer J, Bowden R, Hadﬁeld S (2020) Defeat-net: general\nmonocular depth via simultaneous unsupervised representation\nlearning. In: Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pp 14402–14413\n22. Bian J, Li Z, Wang N, Zhan H, Shen C, Cheng M-M, Reid I (2019)\nUnsupervised scale-consistent depth and ego-motion learning from\nmonocular video. Adv Neural Inf Process Syst 32\n23. Godard C, Mac Aodha O, Firman M, Brostow GJ (2019) Digging\ninto self-supervised monocular depth estimation. In: Proceedings\nof the IEEE international conference on computer vision, pp 3828–\n3838\n24. Ozyoruk KB, Gokceler GI, Bobrow TL, Coskun G, Incetan K,\nAlmalioglu Y , Mahmood F, Curto E, Perdigoto L, Oliveira M, Sahin\nH, Araujo H, Alexandrino H, Durr NJ, Gibert HB, Mehmet T (2021)\nEndoslam dataset and an unsupervised monocular visual odometry\nand depth estimation approach for endoscopic videos. Med Image\nAnal 71:102058\n25. Silberman N, Hoiem D, Kohli P , Fergus R (2012) Indoor seg-\nmentation and support inference from RGBD images. In: Com-\nputer vision–ECCV 2012: 12th European conference on computer\nvision, Florence, Italy, October 7–13, 2012, Proceedings, Part V\n12, pp 746–760. Springer\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123"
}