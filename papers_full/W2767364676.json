{
    "title": "Language Modeling for Code-Switched Data: Challenges and Approaches",
    "url": "https://openalex.org/W2767364676",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2782440736",
            "name": "Sreeram Ganji",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746979148",
            "name": "Sinha Rohit",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2176685020",
        "https://openalex.org/W2187953100",
        "https://openalex.org/W1995085962",
        "https://openalex.org/W2057814390",
        "https://openalex.org/W2031292349",
        "https://openalex.org/W2250750541",
        "https://openalex.org/W2002019621",
        "https://openalex.org/W1977560964",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2039387825",
        "https://openalex.org/W1631260214"
    ],
    "abstract": "Lately, the problem of code-switching has gained a lot of attention and has emerged as an active area of research. In bilingual communities, the speakers commonly embed the words and phrases of a non-native language into the syntax of a native language in their day-to-day communications. The code-switching is a global phenomenon among multilingual communities, still very limited acoustic and linguistic resources are available as yet. For developing effective speech based applications, the ability of the existing language technologies to deal with the code-switched data can not be over emphasized. The code-switching is broadly classified into two modes: inter-sentential and intra-sentential code-switching. In this work, we have studied the intra-sentential problem in the context of code-switching language modeling task. The salient contributions of this paper includes: (i) the creation of Hindi-English code-switching text corpus by crawling a few blogging sites educating about the usage of the Internet (ii) the exploration of the parts-of-speech features towards more effective modeling of Hindi-English code-switched data by the monolingual language model (LM) trained on native (Hindi) language data, and (iii) the proposal of a novel textual factor referred to as the code-switch factor (CS-factor), which allows the LM to predict the code-switching instances. In the context of recognition of the code-switching data, the substantial reduction in the PPL is achieved with the use of POS factors and also the proposed CS-factor provides independent as well as additive gain in the PPL.",
    "full_text": "Language Modeling for Code-Switched Data: Challenges and\nApproaches\nGanji Sreeram‚àó, Rohit Sinha\nDepartment of Electronics and Electrical Engineering,\nIndian Institute of Technology Guwahati, Guwahati-781039, India.\nAbstract\nLately, the problem of code-switching has gained a lot of attention and has emerged as an\nactive area of research. In bilingual communities, the speakers commonly embed the words\nand phrases of a non-native language into the syntax of a native language in their day-to-day\ncommunications. The code-switching is a global phenomenon among multilingual commu-\nnities, still very limited acoustic and linguistic resources are available as yet. For developing\neÔ¨Äective speech based applications, the ability of the existing language technologies to deal\nwith the code-switched data can not be over emphasized. The code-switching is broadly\nclassiÔ¨Åed into two modes: inter-sentential and intra-sentential code-switching. In this work,\nwe have studied the intra-sentential problem in the context of code-switching language mod-\neling task. The salient contributions of this paper includes: (i) the creation of Hindi-English\ncode-switching text corpus by crawling a few blogging sites educating about the usage of the\nInternet (ii) the exploration of the parts-of-speech features towards more eÔ¨Äective modeling\nof Hindi-English code-switched data by the monolingual language model (LM) trained on\nnative (Hindi) language data, and (iii) the proposal of a novel textual factor referred to\nas the code-switch factor (CS-factor), which allows the LM to predict the code-switching\ninstances. In the context of recognition of the code-switching data, the substantial reduction\nin the PPL is achieved with the use of POS factors and also the proposed CS-factor provides\nindependent as well as additive gain in the PPL.\nKeywords: Code-switching, factored language model, textual features, recurrent neural\nnetworks\nPreprint submitted to arXiv.org November 13, 2017\narXiv:1711.03541v1  [cs.CL]  9 Nov 2017\n1. Introduction\nCode-switching is deÔ¨Åned as the alternate use of two or more languages by a speaker\nwithin the same utterance during conversation [1]. Over the years, due to urbanization\nand geographical distribution, people have moved from one place to another for a better\nlivelihood. Communicating in two or more languages helps to interact better with people\nfrom diÔ¨Äerent places and cultures. Code-switching diÔ¨Äers from other types of language\nmixing such as borrowing, pidgin, etc. Code-switching refers to the phenomena where the\nbilingual speaker switches between the two languages while conversing with another person\nwho also have the knowledge about both the languages. On the other hand, borrowing is\nreferred as embedding words from the foreign language into the native language due to the\nabsence of those words in the vocabulary of the native language [2]. Pidgin is a grammatically\nsimpliÔ¨Åed means of communication that develops between two or more groups that do not\nhave a language in common. After independence, though the Indian constitution declared\nHindi as the primary oÔ¨Écial language, the usage of English was still continued as secondary\nlanguage for its dominance in administration, education and law [3, 4]. Thereby, creating\na trend among the urban population to communicate in English for economic and social\npurposes. Over the years, substantial code-switching to English while speaking Hindi as\nwell as other dominant Indian languages has become a common feature. Despite extensive\ncode-switching, the speakers and the listeners mostly agree upon which language the code-\nswitched sentence mainly belongs to. That particular language is referred to as native\n(matrix) language and the other is the non-native (embedded) language [5]. The people\nbelonging to the bilingual communities say that the main reason for code-switching between\nlanguages is due to the lack of words in the vocabulary of that particular native language [6].\nOther possible reasons for code-switching are: (i) to qualify the message by emphasizing\nspeciÔ¨Åc words, (ii) to convey a personalized message, (iii) to maintain conÔ¨Ådentiality during\nverbal communication, (iv) to show expertise, authority, status, etc.\n‚àóCorresponding author\nEmail addresses: s.ganji@iitg.ernet.in (Ganji Sreeram), rsinha@iitg.ernet.in (Rohit Sinha)\n2\nThe switching of languages can happen within the sentence or at its boundary. The\nswitching within the sentence is referred to as the intra-sentential code-switching and the one\nhappening at the sentence boundary is referred to as the inter-sentential code-switching [6].\nIn this paper, we address the intra-sentential code-switching mode in Indian context. Here,\nwe consider the code-switching between Hindi and English languages, where the native lan-\nguage is Hindi and the non-native (foreign) language is English [7]. In literature, there\nexists many works addressing the intra-sentential code-switching problem [8, 9]. For train-\ning the bilingual language model (LM), in [1] the authors merged the training texts with\npossible tagging of words, which have the same spelling in more than one of the considered\nlanguages. Thus, they distinguish these words and build a language model of the merged\ntraining texts. A few other works also employed the interpolated LM where the vocabulary\nof both the languages are merged, but estimate the LM probabilities for each individual\nlanguage. Word entries of one language are included in the language models of other lan-\nguages with zero counts and received a small backoÔ¨Ä probability. In that approach, all the\nwords have non-zero probabilities which enabled them to handle the intra-sentential code-\nswitching. But in most of those works, the non-native language contained word sequences\nhaving contextual information [10, 11]. In this work, we consider a case where most of the\nnative (Hindi) language sentences are embedded with non-native (English) language words\nwithout much context information. Figure 1 shows the examples of inter-sentential code-\nswitching and diÔ¨Äerent types of intra-sentential code-switching sentences highlighting the\ndiÔ¨Äerence in the contextual information carried by the switched foreign words. Due to lack\nof suÔ¨Écient context information, the existing approaches become less eÔ¨Äective in recognizing\nsuch code-switching data. To address these challenges, one way is to tediously collect large\namount of text corpus in code-switching domain for training the models as attempted in\n[12]. Alternatively, we should explore augmenting the monolingual LM with code-switching\ninformation.\nThe remainder of this paper is organized as follows: In Section 2, the motivation of the\nwork is presented followed by the discussion on the explored parts-of-speech (POS) and the\nproposed code-switching textual features in training recurrent neural network based language\n3\nInter-sentential She is the daughter of CEO,         ‡§Ø‡§π‡§π‡§π ‡§¶‡§¶ ‡§¶‡§¶‡§®‡§ï ‡§ï ‡§¶‡§≤‡§è‡§Ü‡§à‡§π‡§π| \n      ‡§Æ‡§Æ‡§ù‡§ï ‡§Ö‡§Æ‡§ï‡§∞‡§∞‡§ï‡§π ‡§Æ‡§Æ ‡§ö‡§π‡§∞‡§∏‡§π‡§≤‡§π‡§¶‡§ó‡§è, but I still miss my country.\nIntra-sentential\nType 1   ‡§Æ‡§Æ‡§ù‡§ï ‡§Æ‡§ï‡§∞‡§πcurrent account balance   ‡§ú‡§π‡§®‡§®‡§π ‡§π‡§π| \n‡§≠‡§≠‡§æ‡§∞‡§§ ‡§Æ‡§Æ‡•á‡§Ç popular free virtual credit card services ‡§ï‡§ï‡§ø‡§§‡§®‡§®‡•Ä  ‡§π‡§π |\nType 2  ‡§Ö‡§™‡§®‡§ïbudget   ‡§ï ‡§ï ‡§Ö‡§®‡§Æ‡§∏‡§π‡§∞investments    ‡§ï‡§∞‡§∏‡§ï‡§§‡§ï ‡§π‡§π |\nClass  ‡§î‡§∞object  ‡§ï ‡§ï ‡§¨‡§∞‡§ö relationship   ‡§ï‡§Ø‡§π‡§π‡§π |\nHinglish class ‡§î‡§∞ object ‡§ï ‡§ï ‡§¨‡§∞‡§ö relationship ‡§ï‡§Ø‡§π ‡§π‡§π\nHindi ‡§µ‡§ó‡§∞ ‡§î‡§∞ ‡§µ‡§∏‡§§‡§Æ ‡§ï ‡§ï ‡§¨‡§∞‡§ö ‡§¶‡§∞‡§∂‡§§‡§π ‡§ï‡§Ø‡§π ‡§π‡§π\nPOS Noun Adj. Noun Post pos. Adj. Noun Ques. Aux.Verb\n ‡§Ü‡§™‡§ï ‡§ïcontent  ‡§ï‡§∞length   ‡§¨‡•ú‡§∞‡§π‡§¶‡§®‡§∞‡§ö‡§π‡§¶‡§π‡§è\n      ‡§Ü‡§™‡§ï‡§∞‡§∏‡§π‡§Æ‡§ó‡§ó‡§∞‡§∞ ‡§ï‡§∞‡§≤‡§≤‡§¨‡§π‡§à‡§¨‡•ú‡§∞‡§π‡§¶‡§®‡§∞‡§ö‡§π‡§¶‡§π‡§è\nclass  ‡§î‡§∞object ‡§ï‡§ø ‡§ï‡•á  ‡§¨‡§®‡•Ä‡§ö relationship  ‡§ï‡§Ø‡§π ‡§π‡§π\n‡§≠‡§≠‡§æ‡§∞‡§§ ‡§Æ‡§Æ‡•á‡§Ç popular free virtual credit card services\n‡§∏‡§≠‡§æ‡§≤\nShe is the daughter of CEO, ‡§Ø‡§π‡§≠‡§æ‡§π‡§æ‡§Å ‡§¶‡§¶‡•ã  ‡§ï‡§¶‡§® ‡§ï‡§ø ‡§ï‡•á  ‡§ï‡§≤‡§è ‡§Ü‡§à ‡§π‡§π‡•à | \n      ‡§Æ‡§Æ‡§ù‡§ï ‡§Ö‡§Æ‡§ï‡§∞‡§∞‡§ï‡§π‡§Æ‡§Æ ‡§ö‡§π‡§∞‡§∏‡§≤‡§π‡§¶‡§ó‡§è, but I still miss my country.   ‡§µ‡§ó‡§∞‡§î‡§∞‡§µ‡§∏‡§§‡§Æ‡§ï‡§ø ‡§ï‡•á  ‡§¨‡§®‡•Ä‡§ö ‡§ï‡§∞‡§∂‡•ç‡§§‡§≠‡§æ  ‡§ï‡§Ø‡§π ‡§π‡§π\n‡§Ø‡§≠‡§æ‡§π ‡§¶‡§¶‡•ã ‡§ö‡§≠‡§æ‡§∞ ‡§ï‡§¶‡§® ‡§ï‡§ø ‡§ï‡•á  ‡§ï‡§≤‡§è ‡§ï‡§ø‡§≠‡§®‡•Ä ‡§π‡§π‡•à\nIn a bilingual speech community, there is a natural\ntendency among speakers to mix lexical items, phrases,\nclauses and sentences during conversations. Bilingual\nspeakers tend to have the ability to switch linguistically\naccording to situational changes. When a bilingual speaker\narticulates two languages successively in the same\ndiscourse, there is interference between the two modes of\nspeaking. When two languages are articulated in this\nmanner, they differ significantly from the same languages\nas spoken in their separate social systems. This kind of\ninterference on the discourse level is known as code-\nswitching and this phenomenon is one of the major aspects\nof bilingualism (Malhotra, 1980). Code-switching can also\nbe seen as the alternate use of lexical items, phrases, clauses,\nand sentences from the non-native language (e.g. English\nfor native Hindi speakers) into the system of the native After independence from the British in the \nyear 1947,\nEnglish was given the status of secondary official language\nFigure 1: Example sentences showing the inter-sentential code-switching and variants of intra-sentential\ncode-switching. Type 1 and Type 2 refer to those cases where the embedded English words carry high\nand low contextual information, respectively. In this work, we mainly deal with Type 2 intra-sentential\ncode-switching problem.\nmodel (RNN-LM) in 3. The description of the text corpora and the system parameters\ninvolved in this study are presented in Section 4. The evaluation results of the proposed\ncode-switch factor (CS-factor) based RNN-LM in contrast with the existing approaches has\nbeen presented in Section 5. The paper is concluded in Section 6.\n2. Motivation of the work\nIntra-sentential code-switching is not a random mixing of one language with the other,\nrather the switching happens with the systematic interactions between the two languages.\nIt is hypothesized that, if the semantic and syntactic information about the native words\nthat are being code-switched with the foreign words can be captured, the same could en-\nhance the ability of the monolingual LM to deal with code-switching task. Based on this\nintuition, we explore the parts-of-speech (POS) features from the native Hindi language text\ndata. Factored language models (LMs) have been introduced to incorporate morphological\ninformation in language modeling [13, 14, 15]. In the factored language model, each word\nwt in the vocabulary V is represented as a collection of k factors: f1\nt , f2\nt , . . . , fk\nt . The\nfactors of a word can be anything, including morphological classes, stems, roots, and any\nother linguistic features that might correspond to that word. A factor can even be a word\nitself, so that the probabilistic language modeling covers both words and their factors. Fac-\ntored language models can be applied to any language which is semantically accepted since\nthe semantic factors like the parts-of-speech can be used as factors. The recurrent neural\n4\n                Input layer, x \n \n \n \n \n                                                Hidden layer, s  \n \n \n \n \n  \n  Output layer, y \ny \nùêπ(ùë§ùë°‚àí1) \nùëìùë°‚àí1\n1  \nùëìùë°‚àí1\n2  \nùëìùë°‚àí1\nùëò  \n ùë†ùë°‚àí1 \n)\nùëÉ(ùë§ùë° ùêπ(ùë§ùë°‚àí1),  ùë†ùë°‚àí1, ùëê(ùë§ùë°)) \nùëÉ(ùëê(ùë§ùë°) ùêπ(ùë§ùë°‚àí1),  ùë†ùë°‚àí1)  \nWord \nClass \nùë§ùë°‚àí1 \nFigure 2: Block diagram of the factored RNN-LM.\nnetworks (RNNs) possess the ability to model the long-term dependencies and also the se-\nmantics information more eÔ¨Éciently compared to the traditional n-gram scheme in context\nof LMs [16, 17, 18]. Recently, the RNNs are also employed in training the factored LMs and\nhave shown signiÔ¨Åcant gains in terms of the perplexity (PPL) and the error rate in speech\nrecognition task [19, 20]. Motivated by those facts, we have explored the parts-of-speech\n(POS) factors in training the native (Hindi) language LM using RNNs in this work. Later,\nthe Hindi-English code-switching data is tested over that factored RNN-LM. Following that,\nwe introduce a novel textual factor that allows to include the code-switching information in\ntraining of the factored RNN-LMs.\n3. Proposed Approach\nIn this work, we have explored the factored RNN-LM for code-switching task. For that\npurpose, the POS textual factors are extracted from Hindi data and are used along with the\nwords in modeling. The network architecture of the factored RNN-LM is given in Figure 2,\nwhere the input, hidden and the output layers are denoted by x, s and y, respectively. The\n5\nType 1   ‡§Æ‡§Æ‡§ù‡§ù ‡§Æ‡§ù‡§∞‡§∞current account balance   ‡§ú‡§∞‡§®‡§®‡§∞‡§π‡§π|\nGrammarly  ‡§è‡§ïadvanced grammar and spell checker  ‡§π‡§π|\nType 2  ‡§Ö‡§™‡§®‡§ùbudget   ‡§ï‡§ù ‡§Ö‡§®‡§Æ‡§∏‡§∞‡§∞investments    ‡§ï‡§∞‡§∏‡§ï‡§§‡§ù ‡§π‡§π |\nMarket    ‡§Æ‡§Æ ‡§¨‡§π‡§Æ‡§§‡§∏‡§ù paid  ‡§î‡§∞free cdn   ‡§â‡§™‡§≤‡§¨‡§ß‡§π‡§π |\nPOS Noun Adjective Post position Adjective Noun Question Verb Aux\nHindi ‡§ï‡§∞‡§∞‡§∞‡§µ‡§§‡§§ ‡§§‡§§‡§µ‡§µ‡§∞‡§ó‡§∞‡§Æ‡§§ ‡§ï‡§∞ ‡§Ü‡§ó‡§Æ‡§® ‡§∏‡§Æ‡§Ø ‡§ï‡§Ø‡§∞ ‡§π‡§π\nHinglish ‡§ï‡§∞‡§∞‡§∞‡§µ‡§§‡§§ express ‡§ï‡§∞ arrival time ‡§ï‡§Ø‡§∞ ‡§π‡§π\nCS No Yes No Yes Yes No No\nFigure 3: An example pair of Hindi and Hindi-English (Hinglish) sentences and their factors. It is worth\nhighlighting that the POS labels extracted for Hindi text remain mostly valid for Hinglish text too. By\naligning Hinglish and Hindi sentences on word basis, the code-switch (CS) factors can be labeled.\nfactored RNN-LM predicts the posterior probability of the current word as\nP(wt|F(wt‚àí1), st‚àí1) = P(wt|F(wt‚àí1), st‚àí1, c(wt)) √óP(c(wt)|F(wt‚àí1), st‚àí1) (1)\nwhere F(wt‚àí1) =\n[\nf1\n(t‚àí1), f2\n(t‚àí1), . . . , fk\n(t‚àí1)\n]\n.\n3.1. Parts-of-Speech (POS) Factors\nIn natural language processing, the parts-of-speech of the words are derived given the\ncontext information. The POS mainly depends on the adjacent words in a given sentence.\nThe POS tagging is not just tagging a list of words with their respective parts-of-speech\ninformation. Based on the context, the same word might have more than one POS factors\nat diÔ¨Äerent locations. When these POS factors are used as the features along with the words\nwhile training the LM, it is expected to model the contextual information in more eÔ¨Äective\nway for the code-switched data. We hypothesize that, in majority of the code-switching\ncases only the native language words are replaced by the foreign language words. Whereas\nthe context and the parts-of-speech of that replaced foreign language word will be mostly\nsame as that of the native language word. This can be visualized from the example given in\nFigure 3. Also, while training the factored RNN-LM, we have added all the foreign (English)\nlanguage words from the code-switching training data to the native (Hindi) language text\ndata to avoid the out-of-vocabulary (OOV) issue. However, no eÔ¨Äort has been made to\naddress the OOV for the native (Hindi) language. The parts-of-speech tagging of the Hindi\ntraining data has been done using an existing Hindi POS tagger [21].\nThe POS tagger is a tool that helps achieve the part-of-speech labeling in a sentence.\nMost of the Indian languages do not have POS taggers due to lack of linguistic resources\n6\nsuch as text corpora, morphological analyzers, lexicons, etc. Recent research has shown\nthat if the low resourced language is typologically related to a rich resourced language, it\nis possible to build taggers for the former using the linguistic resources of the later [22].\nOn studying the topological similarity between Hindi-English and their Hindi translated\nversions, we notice a good match. In our collected data, we Ô¨Ånd that about 70% sentences\ninvolve only the replacement of English words with their Hindi counterparts without need\nof any change in the syntax of the sentences. Motivated by that the POS factors for the\nHindi-English code-switched data are extracted using the POS tagger trained on the Hindi\nlanguage database. Wherever the said tool is not able to identify any part of the given\nsentence correctly, it outputs ‚Äôunknown‚Äô (UNK) label.\n3.2. Code-Switch (CS) Factor\nDuring code-switching the foreign language words are inserted into the native language\nsentences mostly without altering the semantics and syntactics of the native language. Thus,\nlike the POS, the code-switching is also expected to adhere certain semantic and syntactic\nrules. To capture this information, we have proposed a novel factor that identiÔ¨Åes the\nlocations where the code-switching can potentially occur or not. This factor is referred to\nas the CS-factor in this work. To address the OOV issue due to foreign (English) words,\nwe have followed the same procedure which is discussed in the context of the POS factors.\nWhen a monolingual LM is trained by adding the CS-factor along with the words and tested\nusing the code-switched data, a signiÔ¨Åcant improvement in terms of the perplexity is noted\nover the LMs created with and without the POS factors.\nTo introduce the CS-factor in the LM training data, a separate tagger is required but the\nsame is yet to be developed. For quick validation of the idea, in this work, the CS-factors\nare derived by following a simple scheme. For doing that, we have aligned the Hindi-English\ntraining sentences with their Hindi translated versions, to capture the information about\nthe positions where code-switching occurs. The words which undergo code-switching are\nmarked as ‚ÄòYes‚Äô while the remaining non-switched words are marked as ‚ÄòNo‚Äô as shown in\nFigure 3. The Hindi training set is then tagged using the CS-factor based on the mapping\n7\n    \n  \n Hindi-English  \n  \n Booch  methodology ‚Äã  ‚Äã‚Äã‡§¶‡•ã    development ‚Äã  ‚Äã process ‚Äã  ‚Äã‚Äã‡§ï‡•ã    suggest ‚Äã  ‚Äã‚Äã‡§ï‡§∞‡§§‡§æ   ‡§π‡•à   |  \n Installation ‚Äã  ‚Äã complete ‚Äã   ‡§π‡•ã   ‡§ú‡§æ‡§®‡•á   ‡§ï‡•á   ‡§¨‡§æ‡§¶   ‡§Ü‡§™‡§ï‡•ã   ‡§è‡§ï   email   ‡§≠‡•Ä   \u0000‡§Æ‡§≤‡•á‡§ó‡§æ   |  \n‡§Æ‡•á‡§∞‡§æ    ATM ‚Äã  ‚Äã card ‚Äã   ‡§ñ‡•ã   ‡§ó‡§Ø‡§æ   ‡§π‡•à   ‡§§‡•ã   ‡§Æ‡•à   ‡§Ö‡§™‡§®‡•á    payment ‚Äã   ‡§ï‡•ã   ‡§ï‡•à‡§∏‡•á    block ‚Äã   ‡§ï‡§∞   ‡§∏‡§ï‡§§‡§æ   ‡§π‡•Ç‡§Å   |  \n\u0000‡§Ø‡§æ   ‡§Ü‡§™   ‡§Æ‡•Å‡§ù‡•á   ‡§ö‡§æ‡§≤‡•Å\u0000‡§Ø    express ‚Äã   ‡§ï‡§æ    arrival ‚Äã  ‚Äã time ‚Äã   ‡§¨‡§§‡§æ   ‡§∏‡§ï‡§§‡•á   ‡§π‡•à   |  \n  \n Hindi  \n  \n Booch  ‡§™\u0000‡§ß\u0000‡§§      ‡§¶‡•ã   \u0000‡§µ‡§ï‡§æ‡§∏   \u0000\u0000\u0000‡§Ø‡§æ‡§ì‡§Ç   ‡§ï‡§æ   ‡§∏‡•Å‡§ù‡§æ‡§µ   ‡§ï‡§∞‡§§‡§æ   ‡§π‡•à   |  \n\u0000‡§•‡§æ‡§™‡§®‡§æ   ‡§™‡•Ç‡§£\u0000   ‡§π‡•ã   ‡§ú‡§æ‡§®‡•á   ‡§ï‡•á   ‡§¨‡§æ‡§¶   ‡§Ü‡§™‡§ï‡•ã   ‡§è‡§ï   \u0000‡§µ‡§™\u0000   ‡§≠‡•Ä   \u0000‡§Æ‡§≤‡•á‡§ó‡§æ   |  \n‡§Æ‡•á‡§∞‡§æ       ATM ‚Äã   ‡§™\u0000‡§ï   ‡§ñ‡•ã   ‡§ó‡§Ø‡§æ   ‡§π‡•à   ‡§§‡•ã   ‡§Æ‡•à   ‡§Ö‡§™‡§®‡•á   ‡§≠‡•Å‡§ó‡§§‡§æ‡§®   ‡§ï‡•ã   ‡§ï‡•à‡§∏‡•á   ‡§∞‡•ã‡§ï   ‡§∏‡§ï‡§§‡§æ   ‡§π‡•Ç‡§Å   |  \n\u0000‡§Ø‡§æ   ‡§Ü‡§™   ‡§Æ‡•Å‡§ù‡•á   ‡§ö‡§æ‡§≤‡•Å\u0000‡§Ø   ‡§§‡•Ä\u0000‡§ó‡§æ‡§Æ‡•Ä      ‡§ï‡§æ   ‡§Ü‡§ó‡§Æ‡§®   ‡§∏‡§Æ‡§Ø   ‡§¨‡§§‡§æ   ‡§∏‡§ï‡§§‡•á   ‡§π‡•à   |  \n  \n  \n  \n  \n  \n  \n ‚Äã  ‚Äã‚Äã  ‚Äã‚Äã  ‚Äã‚Äã  ‚Äã‚Äã  ‚Äã‚Äã  ‚Äã‚Äã  ‚Äã‚Äã  ‚Äã‚Äã  ‚Äã‚Äã  ‚Äã‚Äã  ‚Äã‚Äã   ‡§Æ‡§æ‡§®‡§µ   ‡§∏‡§Ç‡§∏‡§æ‡§ß‡§®   \u0000‡§µ‡§ï‡§æ‡§∏   ‡§™‡§∞   \u0000‡§µ‡§≠‡§æ‡§ó   ‡§∏‡•á   ‡§∏‡§Ç‡§¨‡§Ç\u0000‡§ß‡§§   ‡§∏‡§Ç‡§∏‡§¶\u0000‡§Ø   \u0000‡§•‡§æ‡§Ø‡•Ä   ‡§∏\u0000‡§Æ\u0000‡§§    ‡§ï‡§æ    ‡§¶‡•å‡§∞‡§æ  \n  \n  \nFigure 4: Typical Hindi-English code-switched sentences in the database and their respective Hindi trans-\nlated versions. It is to note that proper-nouns and abbreviations are retained in Hindi version.\nlearnt. We note that the data collected for our experiments involve 70% sentences where the\nEnglish words are replaced with their Hindi counterparts without any change in the syntax.\nHence, the CS factors for the Hindi-English code-switched data are kept the same as that\nof Hindi data while training the Hindi-English code-switched LMs.\n4. Experimental Setup\nThis section describes the text data used in the experimentation, the extraction of the\nPOS and the proposed CS factors, and the parameter used for training the factored LMs.\n4.1. Database preparation\nFor the experimental purpose, the code-switched Hindi-English text data has been col-\nlected by crawling a few blogging websites educating about the Internet usage 1,2. The\ncrawled text data has been normalized into meaningful sentences while removing the spe-\ncial characters, emoticons, spaces, etc. Later, these Hindi-English sentences are translated\nto Hindi sentences primarily with the help of Google translate3 online translation tool. In\nsome cases, the Google translate failed to produce correct Hindi translation possibly due\n1https://shoutmehindi.com\n2https://notesinhinglish.blogspot.in\n3https://translate.google.com/\n8\nHinglish next ‡§™‡§∞ click ‡§ï‡§ï‡•Ä‡§ú‡§ú‡§ø‡§Ø‡§Ø\nHindi ‡§Ö‡§ó‡§≤‡§Ø ‡§™‡§∞ ‡§ü‡§ï ‡§ï‡§ï‡•Ä‡§ú‡§ú‡§ø‡§Ø‡§Ø\nword+POS W-‡§Ö‡§ó‡§≤‡§Ø:P-Adj W-‡§™‡§∞:P-PostposW-‡§ü‡§ï:P-Noun W-‡§ï‡§ï‡•Ä‡§ú‡§ú‡§ø‡§Ø‡§Ø:P-Verb\nword+CS W-‡§Ö‡§ó‡§≤‡§Ø:C-Yes W-‡§™‡§∞:C-No W-‡§ü‡§ï:C-Yes W-‡§ï‡§ï‡•Ä‡§ú‡§ú‡§ø‡§Ø‡§Ø:C-No\nword+POS+CS W-‡§Ö‡§ó‡§≤‡§Ø:P-Adj:C-Yes W-‡§™‡§∞:P-Postpos:C-No W-‡§ü‡§ï:P-Noun:C-Yes W-‡§ï‡§ï‡•Ä‡§ú‡§ú‡§ø‡§Ø‡§Ø:P-Verb:C-No\nFigure 5: The syntax of diÔ¨Äerent transcriptions that are used for training the factor-based RNNLM. Here,\nP and C refers to the POS and the CS factors respectively.\nTable 1: Description of the data sets created for evaluating the code-switching task. The reason behind\nEnglish words in Hindi data lies in the proper-nouns and abbreviations being left unchanged while translating\nHindi-English (Hinglish) sentences to Hindi.\nData\nTrain Set Test Set\nNo. of\nSentences\nNo. of WordsNo. of Unique WordsNo. of\nSentences\nNo. of WordsNo. of Unique Words\nHindi EnglishHindi English Hindi EnglishHindi English\nHinglish 1050 10484 5936 916 1418 105 1088 533 284 263\nHindi 1050 15604 1036 1938 381 105 1563 59 508 29\nto inadequate Hindi vocabulary. In such cases, the Hindi translation has been done man-\nually with the help of a few online Hindi vocabulary resources 4,5. It is to note that the\nproper-nouns and the abbreviations present in the Hindi-English sentences are kept un-\nchanged while translating them to Hindi. A few example Hindi-English sentences and their\nHindi translated versions are shown in Figure 4. The obtained Hindi text data is divided\ninto training and test sets consisting of 1050 and 105 sentences, respectively. Similarly, the\nHindi-English text data is also partitioned into identical sized training and test sets while\nmaintaining one-to-one correspondence with those of Hindi data sets. The salient details of\ndiÔ¨Äerent Hindi-English and Hindi data sets are summarized in Table 1. Also, the syntax\nof various transcriptions that are used for training the factor-based RNNLMs is given in\nFigure 5.\nThe Hindi training data is used for developing the proposed and the contrast language\n4http://www.rajbhasha.nic.in/hi/hindi-vocabulary\n5https://hi.wiktionary.org/wiki\n9\n0 100 200 300 400 50076\n78\n80\n82\n84\n86\n88\nNo. of hidden nodes\nPerplexity\n*\n0 50 100 150 200 250 30075\n80\n85\n90\n95\n100\n105\nNo. of classes\nPerplexity\nFigure 6: Tuning experiments for Ô¨Åxing the number of nodes in the hidden layer and the number of classes\nin training the RNN-LMs. First, the number of nodes is tuned using default settings of the toolkit used.\nLater, using the optimal value of the number of nodes as 300, the number of classes is tuned.\nmodels for the code-switching task. The Hindi test data is used for tuning the parameters\nwhile training the RNN-LMs. The recognition performances of the trained LMs are evaluated\non the Hindi-English test data. Further, another set of class-based and POS-factor based\nRNN-LMs are trained on the Hindi-English training data and are used to benchmark the\nperformance on the proposed approach. Also, the performances for the 5-gram LMs trained\non Hindi and Hindi-English data using SRILM toolkit [23] are computed for the reference\npurpose.\n4.2. Parameter tuning\nRNN-based language models used in the experimental evaluation are developed using the\nRNNLM toolkit [24]. These LMs are trained with a single hidden layer having 300 nodes\nwith sigmoid as the non-linearity function. By conducting tuning experiments on Hindi test\ndata, the number of classes are set as 50 and the variable corresponding to BPTT is set as\n5. Note that, the tuning of the number of classes selection has been done by considering the\noptimal value of the number of nodes in hidden layer. The perplexity values for diÔ¨Äerent\nnumber of nodes in the hidden layer and for diÔ¨Äerent number of classes is shown in Figure 6.\n10\n5. Results and Discussion\nThe evaluation of both the POS and the proposed CS factors have been done in the\ncontext of Hindi-English code-switching task. For this purpose, 5-gram LMs and both\nthe class-based and the factor-based LMs are developed separately using Hindi and Hindi-\nEnglish training data. The performances of these systems in terms of perplexity (PPL) on\nHindi-English and Hindi test data sets are reported in Table 2. To avoid the paucity of\ndata aÔ¨Äect the Ô¨Åndings, the 3-fold cross-validation is done and the average performance of\nall those test sets has been reported.\nWhen the Hindi-English data is tested over the 5-gram LM or the normal class-based\nHindi RNN-LM, a huge degradation in PPL has been observed in comparison to that of\nthe Hindi data. This is attributed to the fact that whenever a English words occur during\ntesting, there is no context information with them. Note that, the OOV issue is taken care\nby adding all the English words in to the vocabulary of Hindi LM training data. Whereas,\nwhen the Hindi-English data is tested over factored Hindi RNN-LM trained using the POS\ninformation as a factor, a signiÔ¨Åcant reduction in PPL is achieved. This is because, the\nPOS factor is not only tagging a list of words with their respective parts-of-speech, but also\nis based on the context. Even when the native word is replaced by the foreign word, the\nPOS factor will mostly remain unchanged. Thus, by employing the POS factors as a feature\nalong with the words while training the RNN-LM, the context information will help predict\nthe Hindi-Englihs word sequences. The parameters of all kinds of LMs are trained on Hindi\ntest set, therefore the Hindi test set performances given in Table 2 are for reference purpose\nonly. Further, from Table 2, it can be noted that the PPL values of the Hindi-English test\nset on all factored RNN-LMs have turned out to be slightly better than those of the Hindi\ntest set. This is attributed to the inclusion of English words towards addressing the OOVs\nin all types of Hindi RNN-LMs. As a result, the Hindi test set has higher OOV than the\nHindi-English test set. From Table 2 we can see that the proposed CS-factor also resulted in\nsigniÔ¨Åcant improvement in the system performance over the conventional RNN-LM and also\nover the explored POS-based factored RNN-LM. Later, when the RNN-LM is trained by\n11\nTable 2: The performances (in terms of perplexity) of the POS and the proposed CS-factors in training the\nHindi RNN-LM in the context of Hindi-English (Hinglish) code-switching task. The performances of Higlish\nRNN-LMs are given for benchmarking. Since, the Hindi RNN-LMs being tuned on Hindi test set, those\nperformances are for reference purpose only. Note that, the factored RNNLM is referred to as F-RNNLM.\nLM Factor Training data Test data PPL\n5-gram word\nHinglish Hinglish 92.11\nHindi\nHinglish 334.20\nHindi 80.11\nRNNLM word\nHinglish Hinglish 89.67\nHindi\nHinglish 318.23\nHindi 78.23\nF-RNNLM\nword + POS\nHinglish Hinglish 39.03\nHindi\nHinglish 58.12\nHindi 58.91\nword + CS\nHinglish Hinglish 36.52\nHindi\nHinglish 51.67\nHindi 52.85\nword + POS + CS\nHinglish Hinglish 22.97\nHindi\nHinglish 38.89\nHindi 39.55\n12\ncombining CS-factor along with the POS factors, further improvement in PPL is achieved.\nThis result shows that the information captured by the CS-factor is additive to that of the\nPOS factors.\nFor contrast purpose, Table 2 also lists the performances of the class-based RNN-LM\ncreated using Hindi-English training data when evaluated on the Hindi-English test data.\nSince the Hindi-English training data carries the Hindi language syntax for deriving the POS\ntags, it is processed with the same Hindi POS tagger [22]. The obtained POS factors are\nthen used to create the Hindi-English factored RNN-LM. Also for comparison purpose, the\nrecognition performances over the 5-gram LMs trained using Hindi-English and Hindi text\ndata are also reported. Interestingly, the proposed CS-factor inclusive factored monolingual\nRNN-LM has resulted in PPL value of 38 .89.\n6. Conclusion\nIn this work, we have explored the factored language model in the context of code-\nswitching task. The factored native (Hindi) language RNN-LM when created using both\nPOS and proposed code-switching factors has shown signiÔ¨Åcant reduction in perplexity for\ncode-switched Hinglish test data. The proposed code-switching factor is simple to estimate\nyet found to be quite eÔ¨Äective in handling the code-switched data. There is very little data\navailable for the task considered in this work. In our experimentation, the collected data is\nstill quite small for training the language models. So the validation on a bigger datasets is\nwarranted and the same will be pursued in the future work.\nReferences\n[1] John J Gumperz, Discourse Strategies, Cambridge University Press, 1982.\n[2] Carol Myers Scotton, ‚ÄúComparing codeswitching and borrowing,‚Äù Journal of Multilingual & Multicul-\ntural Development, vol. 13, no. 1-2, pp. 19‚Äì39, 1992.\n[3] Anik Dey and Pascale Fung, ‚ÄúA Hindi-English Code-Switching Corpus.,‚Äù in Proc. of Language Re-\nsources and Evaluation Conference (LREC), 2014, pp. 2410‚Äì2413.\n[4] Sunita Malhotra, ‚ÄúHindi-English, Code Switching and Language choice in urban, uppermiddle-class\nIndian Families,‚Äù Kansas Working Papers in Linguistics, 1980.\n13\n[5] Aravind K Joshi, ‚ÄúProcessing of sentences with intra-sentential code-switching,‚Äù in Proc. of 9th\nconference on Computational Linguistics, 1982, vol. 1, pp. 145‚Äì150.\n[6] Fran¬∏ cois Grosjean, Life with Two Languages: An Introduction to Bilingualism, Harvard University\nPress, 1982.\n[7] Kiran Bhuvanagirir and Sunil Kumar Kopparapu, ‚ÄúMixed language speech recognition without explicit\nidentiÔ¨Åcation of language,‚Äù American Journal of Signal Processing, vol. 2, no. 5, pp. 92‚Äì97, 2012.\n[8] Basem HA Ahmed and Tien-Ping Tan, ‚ÄúAutomatic speech recognition of code switching speech using\n1-best rescoring,‚Äù in Proc. of International Conference on Asian Language Processing (IALP). IEEE,\n2012, pp. 137‚Äì140.\n[9] Houwei Cao, PC Ching, Tan Lee, and Yu Ting Yeung, ‚ÄúSemantics-based language modeling for\nCantonese-English code-mixing speech recognition,‚Äù in Proc. of 7th International Symposium on Chi-\nnese Spoken Language Processing (ISCSLP). IEEE, 2010, pp. 246‚Äì250.\n[10] Dau Cheng Lyu, Ren Yuan Lyu, Yuang Chin Chiang, and Chun Nan Hsu, ‚ÄúSpeech recognition on\ncode-switching among the Chinese dialects,‚Äù in Proc. of International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2006, vol. 1.\n[11] Ching Feng Yeh, Chao Yu Huang, Liang Che Sun, Che Liang, and Lin Shan Lee, ‚ÄúAn integrated\nframework for transcribing Mandarin-English code-mixed lectures with improved acoustic and language\nmodeling,‚Äù in Proc. of 7th International Symposium on Chinese Spoken Language Processing (ISCSLP).\nIEEE, 2010, pp. 214‚Äì219.\n[12] Dau Cheng Lyu and Ren Yuan Lyu, ‚ÄúLanguage identiÔ¨Åcation on code-switching utterances using mul-\ntiple cues,‚Äù in Proc. of 9th Annual Conference of the International Speech Communication Association,\n2008.\n[13] Katrin KirchhoÔ¨Ä, JeÔ¨Ä Bilmes, and Kevin Duh, ‚ÄúFactored Language Models Tutorial,‚Äù Tech. Rep.\nUWEETR-2007-0003, Dept of EE, University of Washington, 2007.\n[14] JeÔ¨Ä A Bilmes and Katrin KirchhoÔ¨Ä, ‚ÄúFactored language models and generalized parallel backoÔ¨Ä,‚Äù in\nProc. of Conference on Computational Linguistics on Human Language Technology, 2003, pp. 4‚Äì6.\n[15] Jan Gebhardt, ‚ÄúSpeech recognition on English-Mandarin code-switching data using factored language\nmodels,‚Äù M.S. thesis, Department of Informatics, Karlsruhe Institute of Technology, 2011.\n[16] Tomas Mikolov, Kai Chen, Greg Corrado, and JeÔ¨Ärey Dean, ‚ÄúEÔ¨Écient estimation of word representa-\ntions in vector space,‚Äù arXiv preprint arXiv:1301.3781, 2013.\n[17] Ilya Oparin, Martin Sundermeyer, Hermann Ney, and Jean Luc Gauvain, ‚ÄúPerformance analysis of\nneural networks in combination with n-gram language models,‚Äù in Proc. of International Conference\non Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2012, pp. 5005‚Äì5008.\n[18] Xie Chen, Xunying Liu, Yanmin Qian, MJF Gales, and Philip C Woodland, ‚ÄúCUED-RNNLM‚ÄìAn\n14\nopen-source toolkit for eÔ¨Écient training and evaluation of recurrent neural network language models,‚Äù\nin Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,\n2016, pp. 6000‚Äì6004.\n[19] Youzheng Wu, Xugang Lu, Hitoshi Yamamoto, Shigeki Matsuda, Chiori Hori, and Hideki Kashioka,\n‚ÄúFactored language model based on recurrent neural network,‚Äù in Proc. of International Conference\non Computational Linguistics, 2012, pp. 2835‚Äì2850.\n[20] Heike Adel, Ngoc Thang Vu, Franziska Kraus, Tim Schlippe, Haizhou Li, and Tanja Schultz, ‚ÄúRecurrent\nneural network language modeling for code switching conversational speech,‚Äù in Proc. of International\nConference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 8411‚Äì8415.\n[21] ‚ÄúHindi POS tagger,‚Äù [Online] http://sivareddy.in/downloads, Accessed: 2017-09-30.\n[22] Siva Reddy and Serge SharoÔ¨Ä, ‚ÄúCross language POS taggers (and other tools) for Indian languages:\nAn experiment with Kannada using Telugu resources,‚Äù in Proc. of the 5th International Workshop on\nCross Lingual Information Access, 2011, pp. 11‚Äì19.\n[23] Andreas Stolcke, ‚ÄúSRILM: an extensible language modeling toolkit,‚Äù in Proc. of Interspeech, 2002.\n[24] Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Burget, and Jan Cernocky, ‚ÄúRNNLM: Recur-\nrent neural network language modeling toolkit,‚Äù in Proc. of the ASRU Workshop, 2011, pp. 196‚Äì201.\n15"
}