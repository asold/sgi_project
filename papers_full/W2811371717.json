{
    "title": "Striving to Earn More: A Survey of Work Strategies and Tool Use Among Crowd Workers",
    "url": "https://openalex.org/W2811371717",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2810192077",
            "name": "Toni Kaplan",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2108253311",
            "name": "Susumu Saito",
            "affiliations": [
                "Waseda University"
            ]
        },
        {
            "id": "https://openalex.org/A2136232910",
            "name": "Kotaro Hara",
            "affiliations": [
                "Singapore Management University"
            ]
        },
        {
            "id": "https://openalex.org/A4225043513",
            "name": "Jeffrey Bigham",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2810192077",
            "name": "Toni Kaplan",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2108253311",
            "name": "Susumu Saito",
            "affiliations": [
                "Waseda University"
            ]
        },
        {
            "id": "https://openalex.org/A2136232910",
            "name": "Kotaro Hara",
            "affiliations": [
                "Singapore Management University"
            ]
        },
        {
            "id": "https://openalex.org/A4225043513",
            "name": "Jeffrey Bigham",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2759323897",
        "https://openalex.org/W1681309541",
        "https://openalex.org/W6678799499",
        "https://openalex.org/W7056074122",
        "https://openalex.org/W6793654474",
        "https://openalex.org/W6675633350",
        "https://openalex.org/W2794424581",
        "https://openalex.org/W2103240268",
        "https://openalex.org/W7053643155",
        "https://openalex.org/W7043615826",
        "https://openalex.org/W6652021361",
        "https://openalex.org/W1984022436",
        "https://openalex.org/W1788738712",
        "https://openalex.org/W6661058033",
        "https://openalex.org/W2399394062",
        "https://openalex.org/W2114269021",
        "https://openalex.org/W3126123353",
        "https://openalex.org/W2147603330",
        "https://openalex.org/W2007018772",
        "https://openalex.org/W4230005269",
        "https://openalex.org/W2042882066",
        "https://openalex.org/W2240322187",
        "https://openalex.org/W2083429555",
        "https://openalex.org/W3123895079",
        "https://openalex.org/W3124258878",
        "https://openalex.org/W2124503193",
        "https://openalex.org/W2136606893",
        "https://openalex.org/W3149399274",
        "https://openalex.org/W2777647957",
        "https://openalex.org/W2055699460"
    ],
    "abstract": "Earning money is a primary motivation for workers on Amazon Mechanical Turk, but earning a good wage is difficult because work that pays well is not easily identified and can be time-consuming to find. We explored the strategies that both low- and high-earning workers use to find and complete tasks via a survey of 360 workers. Nearly all workers surveyed had earning money as their primary goal, and workers used many of the same tools (browser extensions and scripts) and strategies in an attempt to earn more money, regardless of earning level. However, high-earning workers used more tools, were more involved in worker communities, and more heavily used batch completion strategies. A natural next step is to use automated systems to assist workers with finding and completing tasks. Workers found this idea interesting, but expressed concerns about impact on the quality of their work and whether using automated tools to support them would violate platform rules. We conclude with ideas for future work in supporting workers to earn more and design considerations for such tools.",
    "full_text": "Striving to Earn More: A Survey of Work\nStrategies and Tool Use Among Crowd Workers\nToni Kaplan,1∗ Susumu Saito,2∗ Kotaro Hara,3 Jeffrey P . Bigham1\n1Carnegie Mellon University, Pittsburgh, PA\n2Waseda University, Tokyo, Japan\n3Singapore Management University, Singapore\ntkaplan@andrew.cmu.edu, susumu@pcl.cs.waseda.ac.jp, kotarohara@smu.edu.sg, jbigham@cs.cmu.edu\nAbstract\nEarning money is a primary motivation for workers on Ama-\nzon Mechanical Turk, but earning a good wage is difﬁcult\nbecause work that pays well is not easily identiﬁed and can\nbe time-consuming to ﬁnd. We explored the strategies that\nboth low- and high-earning workers use to ﬁnd and complete\ntasks via a survey of 360 workers. Nearly all workers sur-\nveyed had earning money as their primary goal, and workers\nused many of the same tools (browser extensions and scripts)\nand strategies in an attempt to earn more money, regardless\nof earning level. However, high-earning workers used more\ntools, were more involved in worker communities, and more\nheavily used batch completion strategies. A natural next step\nis to use automated systems to assist workers with ﬁnding and\ncompleting tasks. Workers found this idea interesting, but ex-\npressed concerns about impact on the quality of their work\nand whether using automated tools to support them would vi-\nolate platform rules. We conclude with ideas for future work\nin supporting workers to earn more and design considerations\nfor such tools.\nIntroduction\nCrowd work is an increasingly important component of the\ndigital economy. It provides an opportunity for people to\nearn income by completing online tasks issued by taskre-\nquesters via crowd work marketplaces. Types of tasks vary\nwidely; common tasks include video and audio transcrip-\ntion, translation, image tagging, data retrieval, and usability\ntesting of websites (Ipeirotis 2010; Difallah et al. 2015). We\nfocus here on the Amazon Mechanical Turk (AMT) crowd\nwork marketplace due to both its scale and its ubiquity in\nresearch and machine learning applications.\nMuch of the prior research examining crowdsourcing\nmarketplaces from the workers’ perspective emphasize low\nwages and an uneven distribution of power between work-\ners and requesters. Crowd workers on AMT are not pro-\nvided a ﬁxed hourly wage. Instead, earnings are allotted\nbased solely onhuman intelligence tasks(HITs) completed\nand approved by requesters in a piece rate. Low per-task re-\nwards and unpaid task search time contribute to more than\nhalf of the AMT workers currently earning less than $5\n∗These authors contributed equally to this work\nCopyright c⃝ 2018, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nan hour (Ipeirotis 2010; Irani and Silberman 2013; 2016;\nHitlin 2016; Horton and Chilton 2010; Martin et al. 2014;\nHara et al. 2018).\nPrior research suggests that workers use online communi-\nties and external tools to aid their work (Mason and Suri\n2012; Schmidt 2015; Huang and Bigham 2017). For ex-\nample, to avoid unfair requesters, workers use tools like\nTurkopticon (Irani and Silberman 2013), Crowd Work-\ners (Callison-Burch 2014), and online forums such as Turk-\nerNation\n1. To reduce the unpaid work due to task search\ntime, people employ strategies like “Preview and Accept”\n(known by workers as PandA), to accept a manually spec-\niﬁed batch of similar HITs in parallel, assuring they have\na constant stream of HITs to progress through during their\nwork session. Researchers have also created prototype tools\nthat automatically queues and visualizes available work\n(Hanrahan et al. 2015). Outside of the research community,\ncrowd workers themselves have produced a plethora of tools\nto help augment the process of ﬁltering HITs and automate\nthe queuing process. However, there is limited research on\nwhat techniques and tools the workers are currently using,\nhow it affects their income, and how strategies may differ\nbetween novice and experienced workers.\nIn this paper, we seek to better understand the challenges\ncrowd workers face in wage-efﬁcient task selection, and\nwhat strategies, tools and information high-earning work-\ners are using to overcome these obstacles. We conducted a\nsurvey on AMT to explore how low- and high-earning work-\ners are leveraging information about HITs to select tasks to\ncomplete, and to make inferences about where further re-\nsearch could be best focused to improve crowd workers’\nearnings. We examined the task-selection habits and types\nof external tools utilized by high-earning workers in com-\nparison to their low-earning peers. By investigating these\nfactors, we aim to provide informed design considerations\nfor future tools and task-recommendation systems for im-\nproving crowd workers’ earnings.\nIn summary, our contributions are:\n• An overview of worker strategies and tool use, which may\ninform future research in teaching workers to be more ef-\nfective and tools to help workers earn higher wages;\n1http://turkernation.com/\nThe Sixth AAAI Conference on Human Computation and Crowdsourcing  (HCOMP 2018)\n70\n• An analysis of how high- and low-earning crowd work-\ners different in working strategies, engagement with so-\ncial communities, and tool usage; and,\n• Design considerations for researchers and others devel-\noping tools to support crowd workers, especially tools\nthat would bring to bear automated technology for recom-\nmending tasks for workers to do and help workers com-\nplete tasks more efﬁciently.\nRelated Work\nThe AMT marketplace demonstrates a severely uneven dis-\ntribution of power and information between workers and\nrequesters (Salehi et al. 2015; Irani and Silberman 2013).\nRequesters have the ability to reject submitted tasks. Work-\ners are not compensated at all if work is rejected. Generally\nthe mechanism is used by requesters to avoid compensating\nworkers for poor quality or incomplete work. This is a point\nof contention, as requesters are able to keep data from un-\ncompensated work. Previous explorations of ethics in crowd\nwork have noted that workers feel this is an unfair practice,\nas requesters can independently and subjectively curate re-\nsults (Martin et al. 2014). Researchers have made efforts\nto promote a more fair crowd work marketplace by address-\ning such unethical treatment of workers. Approaches ranged\nfrom working collaboratively with crowd workers to amass\na collection of letters to Jeff Bezos (Salehi et al. 2015), the\nfounder of AMT, to the creation of a separate crowdsourcing\nmarketplace based on open-governance in which the work-\ners needs and rights are prioritized (Gaikwad et al. 2015).\nWhile these efforts push for fair treatment of crowd work-\ners, reasonable wages, and open communication, the collec-\ntive letter has not had signiﬁcant effects on the environment\nand the open platform is in a process of taking off.\nThe information imbalance between requesters and work-\ners on AMT limits workers ability to effectively ﬁlter\nand search for HITs that will be completable and wage-\nefﬁcient (Chilton et al. 2010). While requesters are able to\njudge workers by a number of metrics within AMT (e.g., by\nqualiﬁcations, location, number of tasks completed), work-\ners do not have access to similar information regarding re-\nquesters. The current AMT search interface allows workers\nto sort tasks by criteria like creation date and reward amount,\nbut does not provide more advanced features like sorting and\nﬁltering available HITs by wage efﬁciency, level of difﬁ-\nculty, interests or other preference metrics (Silberman, Irani,\nand Ross 2010). The lack of such advanced search features\nlimit workers’ ability to judge the quality of requesters and\nthe wage and feasibility their tasks. Tasks may be impossi-\nble to complete (e.g., due to unclear instructions, interface\nglitches or insufﬁcient time to complete a task). This forces\na worker to abandon it or return it to the pool of available\ntasks, resulting in wasted and unpaid work time. This makes\nit difﬁcult for workers to optimize wages, forcing them to\nbalance per-task reward with completion time, while also at-\ntempting to minimize unpaid time spent searching for tasks.\nPrior work has explored ways to overcome this informa-\ntion imbalance. Arguably, the most widely adopted tool is\nTurkopticon (Irani and Silberman 2013), a browser exten-\nsion that enables crowd workers to collaboratively rate and\nreview requesters. The ratings and reviews are publicly vis-\nible and can be used to avoid a speciﬁc requester if s/he has\na poor reputation. The information from Turkopticon’s API\nis integrated into tools designed by workers themselves. For\nexample, a browser extension such as HitScraper allows its\nusers to ﬁlter and prioritize search results based on Turkop-\nticon ratings. Although we know these tools are widely used\namong crowd workers, to our knowledge, there has not been\nformal research investigating the types and prevalence of\nworker tools or their impact on workers’ income. This leaves\nus with little information about how workers are currently\naddressing the challenges they face earning a viable wage\nin the workplace, providing little foundation from which to\ndevelop new tools. In this paper, we investigate the current\nstate-of-the-art in worker tools and strategies on AMT to\nproviding necessary grounding for future tool development.\nIn the research survey presented here, we examine the role\nof HIT content, search features, and tools in wage-efﬁcient\ntask selection. HIT-content based task selection on AMT\nis sparsely studied and rarely implemented. Previous task-\nrecommendation systems have leveraged information read-\nily available in the AMT search, such as task keywords, re-\nward, qualiﬁcations, etc., in combination with Turkopticon\nratings to queue wage-efﬁcient tasks (Hanrahan et al. 2015;\nAlsayasneh et al. 2018). We hypothesize that efﬁcient HIT\nselection hinges on additional content dependent factors that\naffect work duration, such as the types of media included in\nthe HIT and the type and number of inputs required.\nMethod\nWe created and deployed a survey to gather information\nabout AMT worker earnings and demographics, HIT selec-\ntion criteria, work strategies and tools. The survey was cre-\nated and hosted using Qualtrics\n2, and 400 HITs including the\nsurvey were posted to Amazon Mechanical Turk for United\nStates-based workers to complete. The survey contained 67\nrequired questions and took between 10 and 30 minutes to\ncomplete. Participants were compensated $3.50 upon com-\npletion to provide a mean hourly wage of $10.\nWe staggered the release of HITs in order to sample work-\ners with varying levels of crowd work experience, as fol-\nlows: The ﬁrst batch of 100 HITs was made available to\nworkers with over 10,000 HITs completed. The following\nthree batches of 100 HITs were made available to workers\nwith more than 5000, 1000, and then 100 HITs completed.\nThe survey was limited to workers in the United States, and\nwas posted from January 23, 2018 to January 31, 2018.\nThe survey began with general demographic questions,\nincluding gender, age, employment status, education level\nand income. The following survey sections included ques-\ntions on AMT related demographic information, such as\ntime spent working and estimated earnings. Workers were\nasked if they had the Masters Qualiﬁcation on AMT. A\n“Masters Qualiﬁcation” is a qualiﬁcation that is automati-\ncally granted to a selection of workers by AMT based on\nstatistical models used to identify workers who “consistently\n2https://www.qualtrics.com/\n71\nTable 1: Description of Mechanical Turk related browser ex-\ntension tools (as of February 2018)\nExtension name Description\nTurkopticon\nA web platform (with API) for reviewing\nand evaluating requesters and HITs. Also\nrefers to a browser extension that displays\npop-ups of the evaluation status on AMT\nsearch pages.\nPanda Crazy\nA userscript that provides an interface\nfor managing and PandA-ing batches of\nHITs.\nMTurk Suite\nAn extension enhancing AMT pages with\nfeatures from various scripts and ex-\ntensions. Includes of Turkopticon, Turk-\nerview, and minor work history and earn-\nings tracking features.\nHIT Scraper\nA userscript that provides a an augmented\nsearch interface for HITs. Hit Scraper in-\ncludes additonal search ﬁlters and can au-\ntomatication search for new HITs at set in-\ntervals.\nMTurk Engine\nAn extension combining HIT Scraper and\nPanda Crazy features, with an automatic\nHIT watcher and improved dashboard for\nmanaging earnings.\nTurkmaster\nA userscript that adds a side bar in Me-\nchanical Turk dashboard page. Automati-\ncally runs a watcher for new HITs based\nsaved requesters and search keywords.\nAlso supports PandAing HITs.\nGreasemonkey/\nTampermonkey\nExtensions that enable userscripts. (Re-\nquired for some userscripts, such as HIT\nScraper, HITForker, Overwatch, Panda\nCrazy and Turkmaster)\ndemonstrate a high degree of success in performing a wide\nrange of HITs across a large number of Requesters”3.W e\nalso asked if workers felt day of the week was a factor in\nearnings on AMT, and, if so, which days were the best and\nworst for earnings.\nWorkers were then asked 5-point Likert scale questions\nabout what factors they consider when selecting HITs. These\nincluded rating the importance of HIT reward, HIT media\ntype, predicted HIT completion time, and recommendations\nfrom other workers when selecting a HIT. We asked similar\nquestions about reasons why a worker may choose to avoid\nor return a HIT, and why they may choose to end a work\nsession. Workers were then asked about preferred task types,\ntheir usage of AMT related tools (see Table 1), and website\nforums (see Table 2). Questions regarding AMT tools and\nwebsites also included and ”Other” option with a text ﬁeld\nin which participants could provide additional details. We\nalso asked workers to indicate how time-consuming and also\nhow frustrating they found task search, spending time on re-\nturned HITs, and spending time on rejected HITs. Four addi-\ntional questions were asked to gauge worker sentiment about\nthe possible future of automation in crowd work. Each set\nof Likert scale responses were followed by optional open-\nended ﬁelds in which workers could provide additional com-\nments.\n3https://www.mturk.com/worker/help\nThe survey closed with more speciﬁc questions regard-\ning workers’ experience and income on Mechanical Turk.\nWorkers were asked to access their AMT dashboard and re-\nport the number of HITs approved/rejected/pending, their\nHIT approval rate, earnings from 2017 and total AMT earn-\nings. These values are available in the AMT dashboard in-\nterface, and thus should be more reliable than self-reported\nestimated wages.\nAll 400 survey HITs were completed. Of these, 360 were\nkept for analysis. Forty responses were omitted due to vi-\nolations of our spam ﬁltering and validation criteria, which\nchecked worker responses for non-zero total AMT earnings\nand internal consistency (e.g., workers’ reported approval\nrate should be consistent with the their reported approved\nHITs divided by reported total HITs submitted). Researchers\nthen manually evaluated the optional open-ended responses\nto identify obvious spammers (e.g. random strings, repeated\nquestions, consistently unrelated responses). All but two re-\nmaining persons completed at least one open-ended mean-\ningful response (”no, none, and nope” were not consid-\nered meaningful responses). No additional spammers were\nidentiﬁed. All 360 remaining responses reported a HIT ac-\nceptance rate within 1% of what would be expected based\non their reported HIT submission history, and thus were\ndeemed valid responses.\nResults\nIn this section, we provide and discuss the results of the\nsurvey. We ﬁrst describe high-level results such as the sur-\nvey respondents’ demographics, their income levels, and the\ntools they use. We then perform a more detailed analysis\nto uncover how and why workers selected particular tasks,\nthe challenges they face, and tools they use. To investigate\nthe effects of external tools and work strategies on workers’\nearnings, we split the workers into 2 groups based on their\ntotal reported 2017 earnings on AMT and compare between\ngroups when relevant. We use total income as opposed to\nhourly wage as it is available in the AMT dashboard and\ntherefore not prone to estimation errors among reliable re-\nspondents. We compute the median 2017 earnings ($948.18)\namong the workers who responded our survey, and assign\nthem to the high-earning group if they earn more than the\nmedian, and low-earning group otherwise. This results in\n180 respondents in each group.\nWe then deﬁne the top 10% of earners in our survey as\nhigh-earning extremes and further examine how their habits\nand strategies differ in comparison to the top 50% of work-\ners. Via these additional comparisons, we aim to further elu-\ncidate successful work strategies.\nDemographics. The composition of our survey respon-\ndents is similar to the worker demographics reﬂected in\nprior research (Ross et al. 2010). Women represented 47.8%\nof respondents, and the most common age group was 25-\n34, comprising 39.7% of respondents. More than half the\nrespondents (61.7%) reported that they are employed full-\ntime, and 50.2% reported having completed a four year\ndegree or higher. Reported approximate household income\n(from all sources, including AMT) ranged from “Less than\n$10,000” to “Over $150,000.” The median income bracket\n72\nTable 2: Description of Mechanical Turk related website forums (as of February 2018)\nWebsite name Description\nMTurk Crowd\n(https://www.mturkcrowd.com/)\nA community with forum topics such as sharing HIT links, requesters’ reputation, scripts/extensions,\nand AMT news. There are “mentors” for novice workers. 1,130,000+ messages have been posted and\n5,200+ members have joined.\nMturk Forum\n(http://www.mturkforum.com/)\nA community with forum topics such as sharing HIT links, requesters’ reputation, worker know-\nhows and habits. The largest platform among our choices; 1,650,000+ messages have been posted\nand 64,000+ members have joined.\nMturkgrind\n(http://www.mturkgrind.com)\nA community with multiple forum topics such as sharing HIT links and other general discussions.\nPosts have slowed signiﬁcantly in the past year. 1,100,000+ messages have been posted and 14,000+\nmembers have joined.\n[Reddit] Hits Worth Turking For\n(https://www.reddit.com/r/HITsWorthTurkingFor/)\nA community with a single forum, for sharing good HIT links between workers. 42,000+ members\nhave joined.\n[Reddit] Hits NOT Worth Turking For\n(https://www.reddit.com/r/hNOTwtf/)\nA community with a single forum, for warning other workers about bad HITs. 500+ members have\njoined.\n[Reddit] Amazon Mechanical Turk\n(https://www.reddit.com/r/mturk/)\nA community with a single forum, for general conversations/discussions (e.g., various comments on\nHITs, tips for better tasking, warnings for bad requesters, etc.) 26,000+ members have joined.\nTurker Hub\n(https://turkerhub.com/)\nA community with forum topics such as sharing HIT links, scripts/extensions, and wiki information.\nThe newest among our choices; established in Nov. 2016. 559,000+ messages have been posted and\n2,200+ members have joined.\nTurker Nation\n(http://turkernation.com/)\nA community with multiple forum topics such as sharing HIT links (by workers/requesters) and other\ngeneral discussions. This forum has 640,000+ posts and 20,000+ members.\nHIT Notiﬁer\n(http://hitnotiﬁer .com/)\nAggregates good HIT links posted on Turker Hub, MTurk Crowd, MTurk Forum, and HITs Worth\nTurking For and provides an audio notiﬁcation when new recommended HITs appear.\nwas $40-49,000, and 3% of total respondents reported less\nthan $10,000.\nIncome Tracking. Of 360 workers, 258 (71.7% of work-\ners) reported that they think about their earnings per day.\nThis was the most common measurement interval, followed\nby wages per week (35% of workers), and earnings per hour\n(17.2% of workers).\nReported Earnings. Self-reported hourly workers’ earn-\nings averaged $5.12 per hour ( SD =3 .23) and ranged\nbetween $0.01 and $25 per hour. Seventeen percent of re-\nspondents (62 workers) reported earnings above the current\nUnited States federal minimum wage ($7.25 per hour). Note\nthat given the above details on tracked earnings, hourly re-\nported earning alone may not be an effective means of de-\nscribing workers’ earnings. Another measure of hourly earn-\nings can be computed per respondent by dividing daily earn-\nings by average hours worked per day, resulting in a calcu-\nlated hourly wage. The average calculated hourly wage was\n$4.73 (SD =3 .27) and ranged between $0.01 and $26.67.\nGiven average calculated hourly wage, 16.39% of workers\nreported earnings above the federal minimum hourly wage.\nSelf-reported daily workers’ earnings averaged $17.3\n(SD =1 6.84) and ranged between $0.03 and $100 per day.\nThe low daily earnings may be due to the low hours worked\nper day. Reported hours worked per day ranged between .5\nto 15 hours (SD =2 .41), and averaged 3.8 hours per day.\nThese ﬁgures are slightly higher than those reported in\nprevious research (Ross et al. 2010; Hara et al. 2018). We\nbelieve this is due to the staggered distribution of the survey\nbased on the number of HITs a worker has had approved,\nwhich resulted in an increase of experienced worker respon-\ndents. In fact, individual Spearman non-parametric corre-\nlations indicate a positive correlation between experience\n(r(360) = .39,p<. 001) and hourly earnings, as well as\nbetween experience and daily earnings (r(360) = .58,p<\n.001), suggesting that these ﬁgures are slightly inﬂated due\nto the sampling method that we employed.\nImpact of Day of the W eek. Eighty-nine percent\n(321/360) of respondents agreed that day of the week “Prob-\nably” or “Deﬁnitely” had an effect on their earnings and\nopportunities on AMT. Workers reported the most prof-\nitable day was Monday (31%), which was closely followed\nby Tuesday (29%). The least proﬁtable days were Sunday\n(59%) and Saturday (34%).\nWhile more low-earners found Sunday to be the least\nproﬁtable day (60.57%), followed by Saturday (32%), equal\namounts of high-earners found Saturday (45.71%) and Sun-\nday (45.71%) unproﬁtable. Our survey data does not allow\nus to investigate why workers think they earn more early\nin the week. We suspect this is because requesters who are\nnot active during weekends become more active early in the\nweek, so there is a greater number and variety of HITs avail-\nable to workers.\nPandA Strategy.“Preview and Accept” (PandA) is a strat-\negy to reduce unpaid work and task search time, in which\nworkers automatically accept a worker-speciﬁed batch of\nsimilar HITs in parallel, assuring they have a constant stream\nof HITs to work through. PandA is a work strategy facil-\nitated and augmented by a wide array of extensions and\nscripts. In total, 156 workers (43.3%) reported using the\nPandA strategy in their work.\nA Chi-square test of independence comparing the fre-\nquency of PandA strategy use between the high and low-\nearning groups showed PandA was more prevalent among\nthe high-earning group ( χ\n2(1) = 23 .927,p<. 0001).\n101 of 180 workers in the high-earning group reported us-\ning PandA, in comparison to only 55 workers in the low-\nearning group. Given the prevalence of PandA usage overall\n73\nand among high-earners, we believe that this strategy is one\nof the most important factors in efﬁcient work on AMT, and\nthat support for this strategy should be a design considera-\ntion for future crowd worker tools.\nExtension Usage. See Figure 1. 213 (59.2%) of respon-\ndents reported using extensions to aid their work on AMT.\nThe number of extensions used ranged from 0 to 8 and av-\neraged 2.2 (SD =2 .24). Among workers using at least\none extension, the average number of extensions used was\n3.75. The most commonly used extensions were Tamper-\nmonkey, Turkopticon and MTurk Suite. ”Other” extensions\nincluded HITForker (12) , Turkerview (4), Overwatch (4),\nHIT Database (4) and Task Archive (4). Note that HIT-\nForker, HIT Database and Overwatch are Greasemonkey\nscripts. Four high-earning workers also reported using their\nown custom scripts.\nHigh-earning workers were more likely to use scripts such\nas MTurk Engine and Tampermonkey. A Wilcoxon Rank-\nSum Test indicated that high-earners used signiﬁcantly more\nextensions, Mdn =3 , than low-earners, Mdn =0 (Z =\n4.49,p<. 0001).\nSocial Platform Usage. See Figure 1. More than 60%\nof workers (222 respondents) reported at least occasionally\nposting or browsing in AMT related online social spaces.\nThe most popular social platform among workers was the\nMTurk subreddit where 99 of the surveyed workers used the\nplatform, followed by the HITsWorthTurkingFor subreddit\nwith 80 users, MTurk Crowd with 77 users, and Turker Hub\nwith 49 users. ”Other” websites included Facebook groups\n(7) and the Turkopticon website (5).\nMTurk Crowd was signiﬁcantly more popular among\nhigh-earners ( Z =2 .44,p < . 05). Twenty-seven per-\ncent (48) of high-earners used MTurk Crowd, in compari-\nson to 16.11% (29) of low-earners. Similarly, Turker Hub\nwas more popular among high-earners, with 20% (36) high-\nearners using the site, while only 7.2% (13) of low-earners\n(Z =3 .53,p<. 001) used Turker Hub.\nT ask Search: Time and Frustration.30% of respondents\nindicated via 5-point Likert scale that ﬁnding HITs to com-\nplete was “4 - V ery” or “5 - Extremely” time consuming.\nResults did not differ signiﬁcantly between high- and low-\nearners (Z = .30,p = .766). Regarding frustration, 22% of\nparticipants (81) reported that task search was “4 - V ery” or\n“5 - Extremely” frustrating.\nNotably, the most important reason for both high and low-\nearning workers ending a work session was that workers\n“Can’t ﬁnd more HITs worth doing.” Nearly half of partic-\nipants (48%) indicated that this was a “5 - Extremely Im-\nportant” motivation in ending a work session. In combina-\ntion, these ﬁndings suggest that the search for HITs on AMT\nposes challenges for workers of all levels, and improvement\nto the task search and selection process could potentially im-\nprove earnings for all workers.\nRejected / Returned T asks: Time and Frustration.44%\n(161) of participants indicated via 5-point Likert scale that\nhaving to return a HIT was “4 - V ery Time Consuming” or\n“5 - Extremely Time Consuming.” Similarly, 58% (205) of\nparticipants indicated that having to return a HIT was “4 -\nV ery Frustrating” or “5 - Extremely Frustrating.”\nFigure 1: Workers used a number of browser extensions and\nsocial websites related to their work. High-earning workers\nwere more likely to use extensions and used more exten-\nsions overall. High-earning workers also made heavier use\nof social web sites related to their work. Error bars represent\nstandard error.\n62% of workers found rejected HITs “Extremely Time\nConsuming” and 80% of workers indicated they rejected\nHITs are “Extremely Frustrating.” This means that workers\nfound that Rejected HITs were the most time consuming as\nwell as the most frustrating.\nThere were no reliable differences between the high- and\nlow-earning groups in level of frustration (Z =1 .81,p =\n.0707) or reported time consumption (Z = .43,p = .6670)\nfor rejected tasks, nor were there any differences in frustra-\ntion (Z = −1.04,p = .2975) or reported time consumption\n(Z = −1.48,p = .1380) for returned tasks.\nMasters Qualiﬁcation. 37 (10.28%) workers reported\nthey had the Masters qualiﬁcation. The majority, 28\n(75.68%) of them were in the high-earning group. A chi-\nsquare test of independence was performed to examine the\n74\nFigure 2: (a) HIT selection / (b) HIT avoidance criteria of all workers. While some of the features used to select or avoid HITs\nare readily available on the platform (e.g., pay per HIT, Time allotted), others are only available with the use of extensions (e.g.,\nRequester reputation), and yet others require workers to guess (e.g., expected completion time, unclear instructions). Error bars\nrepresent standard error.\nrelation between earnings group (high-vs. low-earning) and\nMasters Qualiﬁcation status (withvs. without Masters Qual-\niﬁcation). This was signiﬁcant, (χ2(1) = 10.87,p<. 01).\nHigh-earning workers were more likely to have the Masters\nQualiﬁcation than low-earners. This may be due to increased\naccess to wage efﬁcient tasks among those with Masters\nQualiﬁcation. Workers with Masters qualiﬁcations reported\nworking an average of just under 2.5 years on AMT before\nachieving the qualiﬁcation. This time period ranged between\n1 and 5 years of work on AMT.\nHIT Type Preference. The most popular HIT type was\nsurveys and extended reading tasks, while the least pop-\nular was image transcriptions. High-earners had less ex-\ntreme preferences overall across all HIT types,M =2 .25\non a 5-point Likert scale from 1-Not at All Preferred to\n5-Extremely Preferred in comparison to the low-earning\ngroup, M =2 .41. A Wilcoxon Rank-Sum Test indicated\nlow-earners were signiﬁcantly more likely to prefer sur-\nveys (Z = −4.08,p<. 0001) and image transcriptions\n(Z = −2.93,p<. 01) in comparison to high-earners.\nHIT Selection Criteria.Survey respondents indicated the\nimportance of HIT selection criteria on 5-point Likert scales,\nranging from 1 - Not at all Important to 5 - Extremely Im-\nportant. See Figure 2(a). Results indicated that the most im-\nportant HIT selection criteria was “Pay per HIT”, followed\nby “Expected Task Completion Time” and then “Requester\nReputation.” Given importance of HIT pay and time per HIT\nin task selection, we can infer that workers are concerned\nwith wage in addition to earnings.\nThe least important were “Opportunities to Learn New\nSkills” and the “Number of HITs Available in a Batch”. The\nlow importance reported for the number of HITs in a batch\nis surprising, given the prevalence of the PandA technique\nfor quickly working through HITs in a batch. In addition,\n54 unique respondents (35 in high-earning group and 19 in\nlow-earning group) mentioned working on batches of HITs\nas part of their work strategy in the open-response questions.\nGiven this, we believe that workers are working through\nbatches of HITs, but generally batches are fairly abundant,\nand batch size is not something that workers must deliber-\nately consider. Instead, in the open-ended responses, work-\ners seemed more concerned about their personal opportunity\nto seize HITs in a good quality batch. One respondent clari-\nﬁes, ”I prefer to have something I can work on consistently\nfor a long period of time more than anything, which I’m\nnot sure is answered by any of the above options. It kind\nof matches ”Number of HITs available in batch” but 10000\nHITs can be taken in 10 minutes, whereas a batch of 200\nmight last an hour.” Seven workers expressed sentiments\nabout how task quality and requester reputation can take\nprecedence over batch size, with users noting that ”when try-\ning a batch with a new requester, I will usually only do 5-10\nhits at the most until they approve.” Others mentioned pre-\nviewing multiple HITs in the batch before accepting, only\naccepting batches from a requester they have worked with\nin the past, or accepting batches only from requesters with\nhigh T.O. ratings.\nHigh-earners were signiﬁcantly less likely to consider\nthe type of media (Z = −3.02,p<. 01), input mecha-\nnism (Z = −2.62,p<. 001), opportunities to learn new\nskills (Z = −2.57,p<. 05), or their interest in the task\n(Z = −2.17,p<. 05) when selecting a HIT. This suggests\nthat workers who are less selective about types of HITs to\nwork on tend to earn more, though we cannot argue causal\nrelationship between the two. That is, it is not clear if work-\ners being less selective is enabling them to earn more, or if\nthere is a hidden factor affecting worker selectivity and/or\nearnings.\nHIT Avoidance / Return / Abandonment Criteria.Survey\nrespondents indicated the importance of various factors in\ntheir decision to avoid, return or abandon (ARA) a HIT. Re-\nsponses were via 5-point Likert scales, ranging from 1 - Not\nat all Important to 5 - Extremely Important (Figure 2(b)).\nResults indicated that the most important ARA factor was\nthat a task “Requires too much Time for the Pay”, followed\nby “Unclear Instructions” and then “Glitches”. The least im-\nportant were “Accidents Resulting In Returns / Rejections”\nand “Interrupted Work”.\nUnclear instructions (Z = −3.51,p<. 001), Unclear\nAudio / Images: (Z = −3.21,p<. 05), Glitches (Z =\n75\n−2.61,p<. 05) and Not Being in the Mood For this Type\nof Task (Z = −2.21,p<. 05) were signiﬁcantly more\nimportant ARA decision factors for low-earners than high-\nearners.\nAutomation. Workers were asked via Likert scale the\nextent to which they agreed or disagreed with statements\nabout automation. Sentiment toward the use of automation\ndiffered between the high and low-earning groups ( Z =\n−2.09,p<. 05), with low-earning workers being more in-\nclined to use a tool that automates some of the work in a\nHIT (M =4 .17,S D =1 .18), than high-earners (M =\n3.85,S D =1 .40). In open-ended responses, 78 workers\nexpressed various concerns regarding automation. The most\ncommon concern, mentioned by 18 respondents, was the\nrole of the human in Human Intelligence Tasks (HITs). One\nworker concisely summarized that, “the whole purpose of\na HIT is to complete a Human Intelligence Task, which by\ndeﬁnition is a task that cannot or should not be automated.”\nOther workers noted that they are being paid for their “opin-\nions and thoughts” and, if they were using AI, they would\n“feel that it wasn’t really [their] work.” Seventeen workers\nexpressed a lack of trust in the quality of AI output, wor-\nrying that they “wouldn’t trust it to work correctly,” noting\nthat if they “don’t trust it, it would add more time to go back\nand check to see if it was right.” Twelve participants also\nmentioned that use of work automation tools would violate\nthe AMT terms of service, and nine participants reported it\nwould be a violation of their personal ethics. Twelve work-\ners discussed how automation would be unfair to requesters.\nWorkers speciﬁed that requesters post work on AMT ex-\npecting and valuing a human response, and using automa-\ntion “doesn’t feel right towards the requesters” because they\n“aren’t trying to hire robots.”\nWorkers were generally somewhat concerned ( M =\n3.43) about how automation could effect the availability of\ntasks on AMT, and this did not differ reliably between high-\nand low-earning workers (Z = −1.62,p = .1037). When\nasked about the possibility of automated systems complet-\ning the types of work currently on AMT, only 34% of re-\nspondents agreed that this would eventually be plausible in\nthe future. In the open-ended questions workers emphasized\nthat some tasks would always require human input, such as\nacademic or opinion surveys and tasks involving evaluating\nart or music.\nTo gauge workers’ awareness of their role in AI and ma-\nchine learning, participants were asked if they felt that their\nwork is being used to improve automated systems. The ma-\njority, 52% of respondents indicated that they did not think\nor did not know if that their work was being used to improve\nautomated systems.\nHigh-Earning Extremes. We deﬁne the top 10% of earn-\ners in our survey as high-earning extremes and further ex-\namine what habits and strategies these workers are using\n(“90-100%” of Figure 3.) The top 10% of workers was com-\nprised of 36 people whose earnings ranged from $8,500 to\n$26,593 (M =1 3,030.29,S D =4 ,818.12). Their esti-\nmated hourly wage averaged $46.81 and varied between $20\nand $100 (SD =2 3\n.27).\nThe PandA work strategy was very common among the\nFigure 3: Distribution of workers’ total earnings in 2017\n(split into 10 groups based on earnings.) We deﬁne the top\n10%, indicated as “90-100%”, as high-earning extremes.\nhigh-earning extremes, with 33 of the 36 (91.7%) high-\nearning extreme workers reporting using PandA. A Chi-\nsquare test of independence comparing the frequency of\nPandA between the high-earning and high-earning extreme\ngroup showed PandA was more prevalent among the high-\nearning extremes group (χ\n2(1) = 22.63,p<. 0001).\nThe high-earning extremes were also more likely than\nhigh-earners to using browser scripts or extensions when\nworking on AMT ( χ\n2(1) = 11 .47,p<. 001), with\n91.7% of high-earning extremes using scripts of exten-\nsions to augment their work experience. High-earning ex-\ntremes also reported using a greater number of extensions\n(M =4 .11,S D =1 .96) than high-earning workers (M =\n2.42,S D =2 .42)( Z =3 .48,p<. 001).\nThe most popular extensions used among the high-\nearning extremes were Tampermonkey (77.78%), MTurk\nSuite (77.78%), Panda Crazy (77.78%) and Turkopticon\n(72.22%). The usage of Panda Crazy is signiﬁcantly higher\namong high-earning extreme workers than high-earners\n(Z =5 .21,p<. 001).\nThe importance of pay rate was evident in task selection\nbased on the open ended questions. One respondent noted\nthat they, “don’t care what the task is, as long as it pays at\nleast $12 an hour.” Twenty of the 36 high-earning extremes\nincluded similar sentiments in their open-ended responses.\nMTurk Crowd was signiﬁcantly more popular among\nhigh-earning extremes. Over 70.22% (26) over the high-\nearning extreme workers used MTurk Crowd. This was sig-\nniﬁcantly more than the 20% (36) of high-earners who used\nthe site (Z =6 .86,p<. 0001). Turker Hub was also more\npopular among high-earning extremes ( Z =4 .52,p<\n.0001). Forty-seven percent (17) of high-earning extremes\nused Turker Hub, in comparison to only 16.11% (29) of\nhigh-earners who used the website.\nThree workers mentioned how private qualiﬁcations af-\nfected their earnings. Qualiﬁcations on AMT can be as-\nsigned to an AMT worker based on demographics, number\nof HITs completed, qualiﬁcation tasks (e.g. demonstration\nof language proﬁciency) or assigned to workers as needed\nby requesters (private qualiﬁcations). For a private qualiﬁ-\n76\ncation a requester might assign a custom qualiﬁcation to a\nset of workers who completed part 1 of a study, or who had\ndone quality work in the past. Then they may post new HITs\nrestricted to only workers with that qualiﬁcation. Given that\nrequesters desire a speciﬁc subset of workers, these tasks\ngenerally pay higher than those without prior qualiﬁcations.\nOpen-ended responses among high-earning extreme\nworkers also included multiple references to workers track-\ning their HITs and earnings history and their previous\nwork per requester. In addition, four high-earning extreme\nworkers mentioned a Greasemonkey / Tampermonkey script\ncalled MTurk HIT Database which provides this function-\nality. They were the only workers surveyed who mentioned\nthis script. These responses may indicate the high-earner ex-\ntreme workers are leveraging information about their previ-\nous work to inform current work selection patterns.\nDiscussion\nExtensions and T ools.Our survey results indicate that ex-\ntension and tool usage is prevalent among AMT workers.\nHigh-earning workers are generally using more extensions\nin their work, and they are using tools that facilitate the\nPandA strategy for queuing batch work. High-earners are\nalso using these tools to monitor their previous work. Fu-\nture tools may beneﬁt from supporting similar batch work\nstrategies and work tracking practices among crowd work-\ners, especially if they can extend these successful strategies\nto low-earning workers who do not yet use these tools.\nSince workers using extensions are, on average, using\nmore than one extension to facilitate their work on AMT,\nextensions and tools should be designed to run in parallel\nwith other scripts. If an extension conﬂicts with another pop-\nular tool, or provides redundant information that clutters the\nAMT interface, it will likely not be favored among workers.\nT ask Selection.Workers are frustrated with unpaid time\nspent working on HITs that are eventually returned or re-\njected by the requester. To explore why workers are experi-\nencing HIT returns and rejections, we examined HIT avoid-\nance, rejection and abandonment factors. The most impor-\ntant reasons for ARA can be broken down into two cate-\ngories: poor compensation (for time and/or effort) and im-\npossible HITs. HITs may be impossible to complete due to\nunclear instructions or media, glitches, or qualiﬁcation tasks\nembedded within the HIT. Embedded qualiﬁcation tasks\nmay involve a pre-survey, allowing only those who meet\ncertain demographic criteria to proceed with and be paid\nfor the HIT. Those who do not meet the criteria are forced\nto return the HIT without receiving compensation for time\nspent on the embedded qualifying task. Presently, it is difﬁ-\ncult for workers to determine whether a task is completable\nand will provide worthwhile compensation via the exist-\ning AMT interface without wasting time attempting HITs.\nWorkers are currently using extensions to address this con-\ncern, using information from other workers to identify rec-\nommended tasks and make inferences about task completion\ntime (and thus pay rate). Still, the results of this survey indi-\ncate the task selection strategies are not adequately reducing\nunpaid work time and frustration, and there is room for im-\nprovement of worker tools. Future systems should include a\nmeans for predicting a HITs completability and wage.\nSentiment T owards Automation.In our survey, we asked\nworkers about the role they thought automation could play\nin their work. Workers expressed concerns about the ethics\nof using automation for partial task completion in a market-\nplace focused on “human” output. Workers noted that they\nwould feel they were “cheating the requester” and that they\nmay spend too much time checking over automated output\nto assure quality. From this feedback, we take away design\nconcerns in creating automated systems for crowd workers.\nThese systems should not make the worker feel as if they\nare being replaced or dishonest. We propose that providing\nauto-ﬁll options for workers as they progress through a task,\ninstead of providing automatically generated output upon\npage load, may help workers complete tasks quicker with-\nout compromising their output quality or minimizing their\npersonal contribution. Automation should likely be oriented\nnot toward the human intelligence part of the task, but rather\nto the mechanics of completing it.\nFuture Work\nMicrotask Recommendation.Our analysis shows that work-\ners’ earnings would likely beneﬁt from access to a constant\nstream of tasks, as seen in the PandA technique, in which\nworkers must manually identify batches to queue. Future\nwork might therefore look to develop systems that automati-\ncally identify and recommend wage-efﬁcient and complete-\nable tasks to queue, reducing task searching time. We be-\nlieve there is an opportunity for Machine Learning (ML) to\nreduce unpaid work time. For instance, an ML model might\nbe trained to predict feasibility and completion time of HITs\nbased on HIT content (HTML) and metadata. While previ-\nous approaches to AMT task recommendation (Hanrahan et\nal. 2015) exist, there are opportunities to utilize HIT con-\ntent in addition to HIT metadata to make better predictions\n(and, thus, pay rate). Task feasibility may be able to be deter-\nmined via web automation, enabling our system to identify\nand recommend worthwhile tasks. A recommendation sys-\ntem for HITs would reduce search time, unpaid work time,\nand frustration due to returned or rejected tasks. Later iter-\nations of this automated task recommendation system may\ncapture and leverage information about workers’ personal\nwork history to recommend similar and preferred tasks.\nMasters Qualiﬁcation.Another beneﬁcial focus would be\nto help workers achieve the Masters Qualiﬁcation, which\nsigniﬁcantly impacted the earning potential for workers. Un-\nfortunately, Mechanical Turk does not clearly state the re-\nquirements to get Masters Qualiﬁcation, although the AMT\ndocumentation notes that their statistical models consider\nthe “variety of tasks” preformed. Perhaps, workers could\nbe empowered by capturing and aggregating worker perfor-\nmance and task selection behavior, and then analyzing it to\nunderstand what leads to Masters Qualiﬁcation attainment.\nWork selection trends identiﬁed here could then be incorpo-\nrated into task recommendation.\n77\nConclusion\nIn this paper, we explored the strategies that low- and high-\nearning workers use to ﬁnd and complete tasks. Workers\nidentiﬁed pay per HIT as their primary task selection fac-\ntor, and used a variety of worker tools in an attempt to earn\nhigher wages, regardless of their earning level. High-earning\nworkers used more tools and were more involved in worker\ncommunities. High earners were also more likely to use\nbatch completion strategies. Through our survey, rejected\nand returned HITs appeared as key factors in unnecessary\nunpaid work time and worker frustration.\nThese ﬁndings suggest several avenues of future research\nin optimizing task selection for improved wages and qualiﬁ-\ncation achievement. Notably, automated task recommenda-\ntion systems may beneﬁt from collecting HIT content infor-\nmation that allows for automatic feasibility evaluation and\nwork time predictions. Such measures would reduce unpaid\nwork time and improve user access to wage-efﬁcient HITs.\nAlthough workers were wary of using automation in their\nwork in general, they seemed more open to using automa-\ntion to improve efﬁciency in ﬁnding work and completing\nother tasks unrelated to the perceived core human intelli-\ngence task. We believe these augmentations are likely to im-\nprove the overall crowdwork experience, and lead to more\nworkers achieving the higher wages that they seek.\nReferences\nAlsayasneh, M.; Amer-Y ahia, S.; Gaussier, E.; Leroy, V .; Pi-\nlourdault, J.; Borromeo, R. M.; Toyama, M.; and Renders,\nJ.-M. 2018. Personalized and diverse task composition in\ncrowdsourcing. IEEE Transactions on Knowledge and Data\nEngineering 30(1):128–141.\nCallison-Burch, C. 2014. Crowd-workers: Aggregating in-\nformation across turkers to help them ﬁnd higher paying\nwork. In Second AAAI Conference on Human Computation\nand Crowdsourcing.\nChilton, L. B.; Horton, J. J.; Miller, R. C.; and Azenkot, S.\n2010. Task search in a human computation market. InPro-\nceedings of the ACM SIGKDD workshop on human compu-\ntation, 1–9. ACM.\nDifallah, D. E.; Catasta, M.; Demartini, G.; Ipeirotis, P . G.;\nand Cudr´e-Mauroux, P . 2015. The dynamics of micro-task\ncrowdsourcing: The case of amazon mturk. InProceedings\nof the 24th International Conference on World Wide Web,\n238–247. International World Wide Web Conferences Steer-\ning Committee.\nGaikwad, S. N.; Morina, D.; Nistala, R.; Agarwal, M.; Cos-\nsette, A.; Bhanu, R.; Savage, S.; Narwal, V .; Rajpal, K.;\nRegino, J.; et al. 2015. Daemo: A self-governed crowdsourc-\ning marketplace. InAdjunct Proceedings of the 28th Annual\nACM Symposium on User Interface Software & Technology,\n101–102. ACM.\nHanrahan, B. V .; Willamowski, J. K.; Swaminathan, S.; and\nMartin, D. B. 2015. Turkbench: Rendering the market for\nturkers. In Proceedings of the 33rd Annual ACM Confer-\nence on Human Factors in Computing Systems, 1613–1616.\nACM.\nHara, K.; Adams, A.; Milland, K.; Savage, S.; Callison-\nBurch, C.; and Bigham, J. P . 2018. A data-driven anal-\nysis of workers’ earnings on amazon mechanical turk. In\nProceedings of SIGCHI Conference on Human Factors in\nComputing Systems, CHI ’18. New Y ork, NY , USA: ACM.\nHitlin, P . 2016. Research in the crowdsourcing age,\na case study. Pew Research Center . http://www. pewin-\nternet. org/2016/07/11/research-in-the-crowdsourcing-age-\na-casestudy.\nHorton, J. J., and Chilton, L. B. 2010. The labor economics\nof paid crowdsourcing. In Proceedings of the 11th ACM\nconference on Electronic commerce, 209–218. ACM.\nHuang, T.-H. K., and Bigham, J. P . 2017. A 10-month-long\ndeployment study of on-demand recruiting for low-latency\ncrowdsourcing. In In Proceedings of The ﬁfth AAAI Confer-\nence on Human Computation and Crowdsourcing (HCOMP\n2017). AAAI, AAAI.\nIpeirotis, P . G. 2010. Analyzing the amazon mechanical\nturk marketplace. XRDS: Crossroads, The ACM Magazine\nfor Students 17(2):16–21.\nIrani, L. C., and Silberman, M. 2013. Turkopticon: Inter-\nrupting worker invisibility in amazon mechanical turk. In\nProceedings of the SIGCHI conference on human factors in\ncomputing systems, 611–620. ACM.\nIrani, L. C., and Silberman, M. 2016. Stories we tell about\nlabor: Turkopticon and the trouble with design. InProceed-\nings of the 2016 CHI conference on human factors in com-\nputing systems, 4573–4586. ACM.\nMartin, D.; Hanrahan, B. V .; O’Neill, J.; and Gupta, N. 2014.\nBeing a turker. InProceedings of the 17th ACM conference\non Computer supported cooperative work & social comput-\ning, 224–235. ACM.\nMason, W., and Suri, S. 2012. Conducting behavioral\nresearch on amazons mechanical turk. Behavior research\nmethods 44(1):1–23.\nRoss, J.; Irani, L.; Silberman, M.; Zaldivar, A.; and Tomlin-\nson, B. 2010. Who are the crowdworkers?: shifting demo-\ngraphics in mechanical turk. InCHI’10 extended abstracts\non Human factors in computing systems, 2863–2872. ACM.\nSalehi, N.; Irani, L. C.; Bernstein, M. S.; Alkhatib, A.; Ogbe,\nE.; Milland, K.; et al. 2015. We are dynamo: Overcoming\nstalling and friction in collective action for crowd workers.\nIn Proceedings of the 33rd annual ACM conference on hu-\nman factors in computing systems, 1621–1630. ACM.\nSchmidt, G. B. 2015. Fifty days an mturk worker: The social\nand motivational context for amazon mechanical turk work-\ners. Industrial and Organizational Psychology 8(2):165–\n171.\nSilberman, M.; Irani, L.; and Ross, J. 2010. Ethics and\ntactics of professional crowdwork.XRDS: Crossroads, The\nACM Magazine for Students17(2):39–43.\n78"
}