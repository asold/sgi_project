{
  "title": "Multi-task Learning based Pre-trained Language Model for Code Completion",
  "url": "https://openalex.org/W3114589574",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1953448656",
      "name": "Liu Fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2031667691",
      "name": "Li, Ge",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1768715567",
      "name": "Zhao Yun-fei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2016811418",
      "name": "Jin Zhi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2605202003",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2142403498",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2091432990",
    "https://openalex.org/W3014797428",
    "https://openalex.org/W2172140247",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3005598269",
    "https://openalex.org/W2954950681",
    "https://openalex.org/W1994573369",
    "https://openalex.org/W2549401308",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2753108589",
    "https://openalex.org/W2888557792",
    "https://openalex.org/W2557805692",
    "https://openalex.org/W2949737566",
    "https://openalex.org/W3126095862",
    "https://openalex.org/W2973083087",
    "https://openalex.org/W2295072214",
    "https://openalex.org/W2884276923",
    "https://openalex.org/W2899384793",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2143861926",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W582055897",
    "https://openalex.org/W1517055698",
    "https://openalex.org/W2165747537",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963935794",
    "https://openalex.org/W3008088841",
    "https://openalex.org/W2971008324",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2741705590",
    "https://openalex.org/W2740130862",
    "https://openalex.org/W3100026183",
    "https://openalex.org/W3021206621",
    "https://openalex.org/W3099302725",
    "https://openalex.org/W3105398568",
    "https://openalex.org/W2533695286",
    "https://openalex.org/W2955426500",
    "https://openalex.org/W2964285114"
  ],
  "abstract": "Code completion is one of the most useful features in the Integrated Development Environments (IDEs), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this paper, we develop a multi-task learning based pre-trained language model for code understanding and code generation with a Transformer-based neural architecture. We pre-train it with hybrid objective functions that incorporate both code understanding and code generation tasks. Then we fine-tune the pre-trained model on code completion. During the completion, our model does not directly predict the next token. Instead, we adopt multi-task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction. Experiments results on two real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.",
  "full_text": "Multi-task Learning based Pre-trained Language Model for\nCode Completion\nFang Liu\nKey Lab of High Confidence Software\nTechnology, MoE (Peking University)\nBeijing, China\nliufang816@pku.edu.cn\nGe Liâˆ—\nKey Lab of High Confidence Software\nTechnology, MoE (Peking University)\nBeijing, China\nlige@pku.edu.cn\nYunfei Zhao\nKey Lab of High Confidence Software\nTechnology, MoE (Peking University)\nBeijing, China\nzhaoyunfei@pku.edu.cn\nZhi Jinâˆ—\nKey Lab of High Confidence Software\nTechnology, MoE (Peking University)\nBeijing, China\nzhijin@pku.edu.cn\nABSTRACT\nCode completion is one of the most useful features in the Integrated\nDevelopment Environments (IDEs), which can accelerate software\ndevelopment by suggesting the next probable token based on the\ncontextual code in real-time. Recent studies have shown that statis-\ntical language modeling techniques can improve the performance\nof code completion tools through learning from large-scale soft-\nware repositories. However, these models suffer from two major\ndrawbacks: a) Existing research uses static embeddings, which map\na word to the same vector regardless of its context. The differences\nin the meaning of a token in varying contexts are lost when each to-\nken is associated with a single representation; b) Existing language\nmodel based code completion models perform poor on completing\nidentifiers, and the type information of the identifiers is ignored in\nmost of these models. To address these challenges, in this paper, we\ndevelop a multi-task learning based pre-trained language model for\ncode understanding and code generation with a Transformer-based\nneural architecture. We pre-train it with hybrid objective functions\nthat incorporate both code understanding and code generation\ntasks. Then we fine-tune the pre-trained model on code completion.\nDuring the completion, our model does not directly predict the\nnext token. Instead, we adopt multi-task learning to predict the\ntoken and its type jointly and utilize the predicted type to assist the\ntoken prediction. Experiments results on two real-world datasets\ndemonstrate the effectiveness of our model when compared with\nstate-of-the-art methods.\nâˆ—Corresponding authors.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nASE â€™20, September 21â€“25, 2020, Virtual Event, Australia\nÂ© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-6768-4/20/09. . . $15.00\nhttps://doi.org/10.1145/3324884.3416591\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Artificial intelligence; â€¢ Soft-\nware and its engineering â†’Software maintenance tools .\nKEYWORDS\ncode completion, multi-task learning, pre-trained language model,\ntransformer networks\nACM Reference Format:\nFang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task Learning based\nPre-trained Language Model for Code Completion. In 35th IEEE/ACM Inter-\nnational Conference on Automated Software Engineering (ASE â€™20), September\n21â€“25, 2020, Virtual Event, Australia. ACM, New York, NY, USA, 13 pages.\nhttps://doi.org/10.1145/3324884.3416591\n1 INTRODUCTION\nAs the complexity and scale of the software development continue\nto grow, large corpora of open source software projects present\nan opportunity for modeling source code on machine learning [1].\nMost of these approaches are based on the observation of source\ncodeâ€™s naturalness [19], that is, source code is written by humans\nand for humans to read, it displays some of the statistical proper-\nties as natural language. Thus, statistical language models have\nbeen used for source code modeling [19, 42, 46], benefiting many\nsoftware engineering tasks, including code summarization [23, 49],\ncode clone detection [51, 52] program repair [15, 47], especially, in\ncode completion [17, 19, 27, 46].\nCode completion is an essential feature of Integrated Develop-\nment Environments (IDEs). It speeds up the process of software\ndevelopment by suggesting the next probable token based on exist-\ning code. In recent years, as the success of deep learning, Recurrent\nNeural Network (RNN)-based language models have been applied\nto source code modeling [3, 27]. In these models, a piece of source\ncode is represented as a source code token sequence or an Ab-\nstract Syntactic Tree (AST) node sequence. Given a partial code\nsequence, the model computes the probability of the next token or\nAST node and recommends the one with the highest probability.\nFurthermore, these language models can also learn useful word\nembeddings, which can be used for other downstream tasks in the\nsame way as word2vec-style embeddings [ 37]. However, source\ncode has some special properties, which have not been exploited in\narXiv:2012.14631v1  [cs.SE]  29 Dec 2020\nASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Liu et al.\n1 p u b l i c long getMaximumTime ( IoEventType t y p e ) {\ni f ( ! timerManager . c o n t a i n s K e y ( t y p e ) )\n3 throw new I l l e g a l A r g u m e n t E x c e p t i o n ( \" P l e a s e add\nt h i s e v e n t f i r s t . \" ) ;\nr e t u r n timerManager . g e t ( t y p e ) . getMaximum ( ) ;\n5 }\nCode 1: A Java method example.\nFigure 1: Java IDE completion example.\nexisting statistical language models. We discuss two critical issues\nin detail below.\nThe contextual information is not well considered in the existing\ncode completion models. Writing clean and readable code that con-\nforms to the specification has been paid more attention in software\ndevelopment, which helps the developers reuse and maintain the\ncode. When programming, developers tend to use meaningful and\nconventional identifier names and natural language documentation\n[36]. As a result, information contained in the source code can be\nexploited by machine learning algorithms. Most of these models\nare based on learned representations called embeddings, which\ntransform words into a continuous vector space [ 37]. However,\nexisting research [3, 25, 27] uses static embeddings, which map a\nword to the same vector regardless of its context. For example, in\nJava method overloading, the same function name can have dif-\nferent meanings based on the number and type of the parameters.\nHowever, the static embedding will map it to the same vector. The\ndifferences in the meaning of a token in varying contexts are lost\nwhen each token is associated with a single representation. The\nsurrounding tokens of the program entities usually contain certain\ninformation that reflects the roles of the entities. For instance, for\na method name, the surrounding tokens might include the vari-\nables/fields/methods that are used/accessed/invoked to implement\nthe method. Taking the Java method in Code 1 as an example, the\nfunction name getMaximumTime can be inferred from the variablesâ€™\nnames and method calls in the body, e.g., getMaximum, timerMan-\nager. These tokens provide information about possible values the\nfunction could take, and so should affect its representation.\nIdentifier completions are challenging, and existing statistical\nLanguage Model (LM) based code completion models perform\npoorly on completing identifiers. These approaches consider ev-\nery token in the source code file as targets for completion. More\nthan two-thirds of the completions do not refer to identifiers. In-\nstead, the majority concern punctuation-like tokens (e.g., operators,\nbraces), which are much easier to complete than identifiers, but\nthese completions are not that beneficial to developers [ 25]. Be-\nsides, the type information of the identifiers is ignored in most\nof the models. Modern IDEs for most languages heavily rely on\ntypes to make helpful suggestions for completing partial code. For\nexample, when accessing the field of an object in a Java IDE, code\ncompletion suggests suitable field names based on the objectâ€™s type\n[35]. Taking the code completion example of a Java IDE ( ItelliJ\nIDEA) in Figure 1 as an example, the IDE suggests â€œScannerâ€ as the\nnext token based on its type (i.e., java.util), and not just predict the\nfrequent token in the corpus. For those dynamic languages, such as\nPython and JavaScript, IDEs often fail to make accurate suggestions\nbecause the types of code elements are unknown, which further\ndemonstrates the importance of the type information. However,\nmost of the existing LM-based source code modeling techniques\nand code completion studies do not take the type information into\nconsideration.\nIn response to the observations and concerns raised above, we\nhave developed a Code Understanding and Generation pre-trained\nLanguage Model (CugLM) for source code modeling. Recent work\non pre-trained language models has found that the contextual em-\nbeddings produced by these models can lead to better performance\nfor many natural language processing (NLP) tasks [10, 22, 39, 40]. In\nthese models, the representation for each word is learned using the\nlanguage models, where the vector of the word is computed based\non the context it is used. Thus, the vector of the same word under\ndifferent contexts can be different. In particular, BERT [10] proposes\na bidirectional Transformer Encoder with two new pre-training\nobjective: â€œmasked language modelâ€ and â€œnext sentence predictionâ€,\nwhere â€œmasked language modelâ€ randomly masks some of the to-\nkens from the input, and the objective is to predict the masked word\nbased only on its context, and â€œnext sentence predictionâ€ predicts\nwhether two sentences follow each other in a natural discourse. By\nusing these two objectives, BERT can produce powerful bidirec-\ntional contextual representations and advances the state-of-the-art\nfor many NLP tasks. Inspired by the success of pre-trained language\nmodels in NLP, we propose a multi-task learning based pre-trained\nlanguage model to produce general and contextual representations\nfor programs that can broadly support code understanding and\ngeneration tasks, and then apply it to code completion. During the\npre-training period, we adopt the multi-task learning framework\nto learn the following three training objectives jointly:\n1) Masked bidirectional Language Modeling : The identifiers\nare more informative for understanding the program and correctly\nsuggesting the identifiers is challenging in existing code completion\nresearch [25]. Thus, producing contextual and general represen-\ntations for tokens, especially for identifiers, would be helpful for\nsource code modeling and code completion. For these reasons, we\nmask the identifiers from the programs, and the objective is to\npredict the masked tokens based on their bidirectional context.\n2) Next Code segment Predicting : We argue that understand-\ning relationships between code segments can help in source code\nmodeling. In order to achieve this, we pre-train a binarized next\ncode segment prediction task, that is, predicting whether two seg-\nments of code tokens follow each other in a piece of code snippet.\nMulti-task Learning based Pre-trained Language Model for Code Completion ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia\n3) Unidirectional Language Modeling: a left-to-right language\nmodeling task, where the representation of each token encodes only\nthe leftward context tokens and itself. This training objective is\nadded because for the generation tasks (e.g., code completion), only\nleftward contextual tokens are allowed.\nAfter the model has been pre-trained, we fine-tune it (directly ap-\nply the pre-trained model and adapt the model on downstream tasks\nby fine-tuning the pre-trained parameters) on the code completion\ntask. During the code completion, our model does not directly pre-\ndict next token, instead, we adopt a multi-task learning framework\nto predict the token and its type. We first predict the type of the\ntoken, and then use predicted type to assist the token prediction.\nWe create two massive corpora of Java and TypeScript programs\ncollected from GitHub to pre-train and fine-tune the model. We\ncompare our model with two state-of-the-art code completion ap-\nproaches: Byte Pair Encoding based Neural Language Model (BPE\nNLM) [25] and Pointer Mixture Network [27]. For completing all\ntypes of tokens, our model achieves the accuracy of 80% and 81% on\nJava and TypeScript datasets, respectively, which improves Pointer\nMixture Network by 17% and 24%, and improves BPE NLM by 19%\nand 24%, in terms of relative improvements. For identifier comple-\ntion, our model achieves the accuracy of 48% and 39%, respectively,\nwhich improves Pointer Mixture Network by 29% and 34%, and im-\nproves BPE NLM by 11% and 9%, in terms of relative improvements.\nThe main contributions of this paper are summarized as follows:\nâ€¢We present the first attempt at pre-training a language model\nwith a transformer-based architecture for code completion.\nâ€¢We take advantage of the type information to help our model\nmake better suggestions on identifiers.\nâ€¢We compare our model with state-of-the-art code completion\nmodels and evaluate the performance of these models on two\nreal-world datasets. Experimental results demonstrate that\nour model achieves the best performance compared with the\nbaseline models.\n2 BACKGROUND\n2.1 Statistical Language Model\nStatistical language models capture the statistical patterns in lan-\nguages by assigning occurrence probabilities to a sequence of words\nin a particular sequence, which will score an utterance high, if it\nsounds â€œnaturalâ€ to a native speaker, and score low the unnatural (or\nwrong) sentences. Programming languages are kind of languages\nthat contain predictable statistical properties [ 19], which can be\nmodeled by statistical language models. Given a token sequence S\n= ğ‘ 1,ğ‘ 2,...,ğ‘  ğ‘¡, the probability of the sequence is computed as:\nğ‘(ğ‘†)= ğ‘(ğ‘ 1)ğ‘(ğ‘ 2|ğ‘ 1)ğ‘(ğ‘ 3|ğ‘ 1ğ‘ 2),...,ğ‘ (ğ‘ ğ‘¡|ğ‘ 1ğ‘ 2,...,ğ‘  ğ‘¡âˆ’1) (1)\nThe probabilities are hard to estimate when the number of the\ncontext tokens ğ‘ 1,ğ‘ 2,...,ğ‘  ğ‘¡âˆ’1 is tremendous. The N-gram model\nbased on the Markov assumption is proposed to address this chal-\nlenge, where the probability of a token is dependent only on the\nğ‘›âˆ’1 most recent tokens. N-gram based models have been generally\napplied to code completion [17, 19, 46]. These models have been\nproved to capture the repetitive regularities in the source code ef-\nfectively. In recent years, deep recurrent neural networks, including\nLong Short-Term Memory (LSTM) [20] and Gate Recurrent Unit\n(GRU) [6], have shown great performance on modeling program-\nming languages [ 3, 27, 28]. By using recurrent connections and\ngate mechanisms, information can cycle inside these networks for\na long time, which loosens the fixed context size and can capture\nlonger dependencies than the N-gram model.\nHowever, the introduction of the gating mechanism in LSTMs\nand GRUs might not be sufficient to address the gradient vanishing\nand explosion issue fully. To ease this issue, attention mechanisms\n[2, 48], which add direct connections between long-distance word\npairs, are proposed. For example, the Transformer [ 48] is an ar-\nchitecture based solely on attention mechanism. It uses a multi-\nheaded self-attention mechanism to replace the recurrent layers\nto reduce sequential computation and capture longer-range depen-\ndency. Later, Transformer-XL [8] is proposed by introducing the\nnotion of recurrence into the deep self-attention network. Thus it\nenables the Transformer networks to capture the very long-term\ndependency during language modeling.\n2.2 Multi-task Learning\nMulti-task learning is an approach for knowledge transfer across\nrelated tasks. It improves generalization by leveraging the domain-\nspecific information contained in the training signals of related\ntasks [4]. Through sharing hidden layers among tasks, the model\ncan capture the common features among all the tasks. Furthermore,\nby preferring the representation that all tasks prefer, the risk of\nover-fitting is reduced, and the model can be more general to new\ntasks in the future. Multi-task learning has been successfully used\nin many fields including natural language processing [10, 14, 31],\nspeech recognition [9] and computer vision [32, 34].\n2.3 Pre-trained Language Models\nLanguage model pre-training has shown to be effective for NLP,\nand has achieved the state-of-the-art results across many NLP tasks\n[7, 10, 22, 39, 40]. The advantages of the pre-trained model can\nbe summarized as follows: (1 By pre-training on the huge corpus,\nthe model can learn universal representations and help with the\ntarget tasks; 2) The pre-trained model can provide a better model\ninitialization, which leads to a better generalization performance\non the downstream tasks. 3) Pre-training can be regarded as a\nkind of regularization to avoid over-fitting on small data. To apply\nthe pre-trained language representations to downstream tasks, the\nfeature-based approaches use the pre-trained representations as\nadditional features [39], and the fine-tuning approaches directly\nadapt the model on the downstream tasks by simply fine-tuning\nthe pre-trained parameters [10, 40]. Generative Pre-trained Trans-\nformer (GPT) [40] and Bidirectional Encoder Representations from\nTransformers (BERT) [10] are the widely used fine-tuning approach,\nwhere BERT has significantly improved the performance of a wide\nrange of natural language understanding tasks. However, the bidi-\nrectionality nature of BERT makes it difficult to be applied to natural\nlanguage generation tasks. To overcome this limitation, UNIfied\npre-trained Language Model (UNILM) [11] that can be applied to\nboth natural language understanding (NLU) and natural language\ngeneration (NLG) tasks was proposed. Inspired by these models,\nwe build a pre-trained language model for code understanding and\ngeneration, and then fine-tune it on code completion.\nASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Liu et al.\n[CLS]\n x2\n x3\n xn...\ntoken embeddings\nsegment embeddings\nposition embeddings\nTransformer Layer 1\nTransformer Layer 2\nTransformer Layer L\n...\nattend to all tokens\nattend to left context\nNCP\nSelf-attention Masks\nCode Understanding and Generation Pre-trained LM with Shared Parameters\n[MASK]\n xn-1\n[MASK]\nx5\nh[CLS]\n h2\n h3\n hn...\nh[MASK]\n hn-1\nh[MASK]\nh5\nh[CLS]\n h2\n h3\n hn...\nh[MASK]\n hn-1\nh[MASK]\nh5\ny4\n yn-2\nTransformer\nTransformer\n(a) Masked bidirectional LM (MLM)\nh[CLS]\n h2\n h3\n hn...\nh[MASK]\n hn-1\nh[MASK]\nh5\ny[CLS]\nTransformer\nTransformer\n(b) Next code segment prediction(NCP)\nTransformer\nTransformer\nh[CLS]\n h2\n h3\n hn...\nh[MASK]\n hn-1\nh[MASK]\nh5\ny4\n yn-2\ny2\n y3\n y5\n yn-1\n yn\n(c) Unidirectional LM (ULM)\n Prevent from Attending\nFigure 2: Overview of CugLM pre-training. The model parameters are shared across the pre-training objectives (i.e., MLM, NCP,\nand ULM). We use different self-attention masks to control the access to context for each token.\n3 CugLM\nWe describe the details about our proposedCode understanding and\ngeneration pre-trained Language Model (CugLM) in this section.\n3.1 Model Architecture\nGiven an input program token sequences ğ‘¥ = ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘›, CugLM\nobtains a contextualized vector representation for each token. The\nmodel architecture is shown in Figure 2. We adopt an ğ¿-layer\nTransformer as the language model to encode the input vectors\nğ‘¥ = ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘› into contextual representations at different levels\nğ»ğ‘™ = [â„ğ‘™\n1,â„ğ‘™\n2,...,â„ ğ‘™ğ‘›], where ğ»ğ‘™ = Transformerğ‘™(ğ»ğ‘™âˆ’1),ğ‘™ âˆˆ[1,ğ¿]. In\nFigure 2 and later sections, we omit the superscriptğ¿for the hidden\nvectors of the final layer â„ğ¿\nğ‘– to make the illustration less cluttered.\nFor each transformer layer (block), multi-attention heads are used\nto aggregate the output of the previous layer, and the output of a\nself-attention head ğ´ğ‘™ is computed as:\nğ‘„ = ğ»ğ¿âˆ’1ğ‘Šğ‘„\nğ‘™ , ğ¾= ğ»ğ¿âˆ’1ğ‘Šğ¾\nğ‘™ , ğ‘‰= ğ»ğ¿âˆ’1ğ‘Šğ‘‰\nğ‘™\nğ‘€ğ‘–ğ‘— =\n\u001a 0, allow to attend\nâˆ’âˆ,prevent from attending\nğ´ğ‘™ = softmax(ğ‘„ğ¾ğ‘‡\nâˆšï¸\nğ‘‘ğ‘˜\n+ğ‘€)ğ‘‰\n(2)\nwhere ğ»ğ‘– âˆˆR|ğ‘¥|Ã—ğ‘‘â„ denotes the ğ‘–-th layerâ€™s output. The queries ğ‘„,\nkeys ğ¾, and values ğ‘‰ are computed by linearly projecting the pre-\nvious layerâ€™s output ğ»ğ‘™âˆ’1 using parameter matrices ğ‘Šğ‘„\nğ‘™ ,ğ‘Šğ¾\nğ‘™ ,ğ‘Šğ‘‰\nğ‘™ .\nğ‘€ âˆˆR|ğ‘¥|Ã—|ğ‘¥|is the mask matrix that determines whether a pair\nof tokens can be attended to each other. For different pre-training\nobjectives, we use different mask matrices ğ‘€ to control how many\ncontextual tokens can a token attend to when computing its contex-\ntualized representations, as illustrated in Figure 2. For bidirectional\nLM, the elements of the mask matrix are all 0s, which means that\nall the tokens have access to each other. For unidirectional LM,\nthe upper triangular part of the mask is set to âˆ’âˆ, indicating that\neach token can only access the leftward context tokens and itself.\nThe output of CugLM includes (1) contextual vector representation\nof each input token, and (2) the representation of [CLS], which is\nshort for â€œCLaSsificationâ€ and works as the aggregated sequence\nrepresentation and can be used for classification tasks.\nDuring the pre-training period, the modelâ€™s parameters are shared\nand optimized with several objectives, namely, Masked bidirectional\nLM, Next Code segment Prediction, and Unidirectional LM. After\nthe model is pre-trained, we can then fine-tune it for downstream\ntasks. In this paper, we fine-tune CugLM on code completion.\n3.2 Input Representation\nThe input ğ‘¥is a token sequence, which is a pair of segments packed\ntogether. As shown in Figure 3, for a given token, its vector represen-\ntation is computed by summing the corresponding token, segment\nand position embeddings.\nâ€¢For token embeddings, the embedding matrix is randomly\ninitialized, and then is adjusted as part of the training process.\nTwo special tokens [CLS], [SEP]are defined, where [CLS],\nwhich is short for â€œCLaSsificationâ€, always appears at the\nbeginning of the input. The final hidden state corresponding\nto [CLS] can be used as the aggregate sequence representa-\ntion for classification tasks, for example, in next sentence\nprediction task. [SEP], which is short for â€œSEPerationâ€, is\nused to separate the sentence pairs.\nâ€¢The segment embeddings, i.e., ğ¸ğ´ and ğ¸ğµ are also used to\ndifferentiate the code segment pairs. For each token of the\nfirst code segment, a learned embedding ğ¸ğ´ is added, and\na learned embedding ğ¸ğµ is added to each token of the sec-\nond code segment. The embedding matrix for the segment\nembeddings is also randomly initialized\nMulti-task Learning based Pre-trained Language Model for Code Completion ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia\nthread\n =\n new\n Thread\n (\n ct\n )\n ;\n thread\n .\n start\n (\n )\n ;\n[CLS]\n [SEP]input tokens\ntoken \nembeddings\nsegment \nembeddings\nposition \nembeddings\nE[CLS]\n Ethread\n E=\n Enew\n EThread\n E(\n Ect\n E)\n E;\n E[SEP]\n Ethread\n E.\n E[tart\n E(\n E)\n E;\nEA\n EA\n EA\n EA\n EA\n EA\n EA\n EA\n EA\n EA\n EB\n EB\n EB\n EB\n EB\n EB\nE0\n E1\n E2\n E3\n E4\n E5\n E6\n E7\n E8\n E9\n E10\n E11\n E12\n E13\n E14\n E15\nFigure 3: Input representation. The input embeddings is the sum of the token embeddings, the segment embeddings, and the\nposition embeddings.\nâ€¢To make use of the order of the sequence, we use learned po-\nsitional embeddings with sequence lengths up to 128 tokens.\n3.3 Pre-training Procedure\nTo pre-train CugLM, we adopt multi-task learning to learn three\ntasks jointly, as shown in Figure 2, including Masked bidirectional\nLanguage Modeling (MLM), Next Code segment Predicting (NCP),\nand Unidirectional Language Modeling (ULM). For the first two ob-\njectives, the Transformer network is under the bidirectional settings,\nand for the last objective, the Transformer network is unidirectional.\na) Masked bidirectional Language Modeling : In order to train\ndeep bidirectional representations for the program, we adopt a sim-\nilar objective with BERT, that is, masking some percentage of the\ninput tokens and then predicting only those masked tokens. Differ-\nent from BERT, we only mask the identifiers with type information,\nwhere the type information can be extracted by static analysis or be\nannotated by developers, considering that these identifiers are more\ninformative for understanding the program. Then the objective is to\npredict the masked identifiers based on their bidirectional contex-\ntual tokens, where all tokens can attend to each other in prediction.\nIt encodes contextual information from both directions and can\ngenerate better contextual representations of the masked identifiers\nas well as the other tokens than its unidirectional counterpart. The\nfinal hidden vectors corresponding to the mask tokens are fed into\nthe output softmax layer to produce the probability distribution of\nthe outputs.\nb) Next Code segment Predicting : Understanding the relation-\nship between two sentences is quite important for many NLP tasks,\nfor example, Question Answering (QA) and Natural Language In-\nference (NLI), which can help to understand the input text in more\ndepth. We argue that understanding relationships between code\nsegments also help in source code modeling. In order to achieve\nthis, we pre-train a binarized next code segment prediction task,\nthat is, predicting whether two segments of code tokens follow each\nother in a piece of code snippet. Specifically, when choosing the\ncode segments A and B for each pre-training example, 50% of the\ntime B is the actual next code segment that follows A, and 50% of\nthe time it is a random code segment from the corpus. For example:\nInput = [CLS] public void setTextDirection ( int textDirection ) {\n[SEP] this . mTextDirection = textDirection ; }\nLabel = 1\nInput = [CLS] public void setTextDirection ( int textDirection ) {\n[SEP] this . request = request ;\nLabel = 0\nThe final hidden vector corresponding to [CLS], which works as\nthe aggregated sequence representation, is fed into the output soft-\nmax layer to produce the probability distribution of classification\nresults.\nc) Unidirectional Language Modeling : For language generation\ntasks, for example, code completion, the context of the predicted\ntoken should only consist of the token on its left. Thus, we create\nthe left-to-right language modeling task as another pre-training\nobjective, namely predicting the next token ğ‘¥ğ‘¡+1 given the preced-\ning context tokens ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘¡. The representation of each token\nencodes only the leftward context tokens and itself. This can be\ndone using a triangular matrix for self-attention maskğ‘€, where the\nupper triangular part of the self-attention mask is set to âˆ’âˆ, and\nothers to 0. At each time step ğ‘¡, the final hidden vector correspond-\ning to ğ‘¥ğ‘¡ is fed into the softmax layer to produce the probability\ndistribution of the predicted token ğ‘¦ğ‘¡.\nThe pre-training procedure follows the existing language model\npre-training approaches. The parameters of CugLM are learned\nto minimize the sum of the cross-entropy losses of the three pre-\ntraining tasks, and are shared among all the tasks. The final loss\nfunction is given below:\nmin\nğœƒ\nLğ‘€ğ¿ğ‘€(ğœƒ)+L ğ‘ğ¶ğ‘ƒ (ğœƒ)+Lğ‘ˆğ¿ğ‘€ (ğœƒ) (3)\n3.4 Fine-tuning Procedure\nWhen the model is pre-trained, we fine-tune it on code completion\ntask. In code completion, the context of the predicted token should\nonly consist of all the token on its left. Thus, the representation of\neach token can encode only the leftward context tokens and itself.\nDuring the fine-tuning procedure, the following two objectives are\noptimized:\na) Unidirectional Masked Language Modeling (UMLM) : Dif-\nferent from the MLM objective in pre-training, the UMLM objective\nin fine-tuning is to predict the masked token based only on its\nleftward context, where all tokens can only attend to the tokens on\nits left in prediction. The transformer network is set to unidirec-\ntional using a triangular matrix for the self-attention mask. All the\nidentifiers that have type information are masked in each sequence.\nASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Liu et al.\nBesides, our model not directly predicts the masked token. Instead,\nwe adopt the multi-task learning framework to predict the token\nand its type. We first predict the type of the token, and then the\npredicted type is used to assist the token prediction, as shown in\nFigure 4. The reason for formulating the code completion task as a\ntwo-step prediction instead of predicting the type and token jointly\nlies in that, by predicting the type firstly and then use the predicted\nresults as extra input for the token prediction can constraint our\nmodel to make more accurate prediction on the type and further\nenhance the token prediction performance.\nh1\n h2\n h3\n hn...\nh[MASK]\n hn-1\nh[MASK]\nh5\nToken4\nType4\nToken prediction\nType prediction\nTokenn-2\nTypen-2\nTransformer\nTransformer\nFigure 4: Model architecture for UMLM.\n1) Type prediction: The final hidden vector (i.e., the output of the\nTransformer) corresponding to the mask token â„[ğ‘€ğ´ğ‘†ğ¾]is used\nto compute the output vector for the tokenâ€™s type ğ‘‚ğ‘¡ğ‘¦ğ‘ğ‘’. We use\nthe softmax function to produce the probability distribution of the\noutputs ğ‘Œğ‘¡ğ‘¦ğ‘ğ‘’:\nğ‘‚ğ‘¡ğ‘¦ğ‘ğ‘’ = tanh(ğ‘Šğ‘œâ„[ğ‘€ğ´ğ‘†ğ¾])\nğ‘Œğ‘¡ğ‘¦ğ‘ğ‘’ = softmax(ğ‘Šğ‘¦ğ‘‚ğ‘¡ğ‘¦ğ‘ğ‘’ +ğ‘ğ‘¦) (4)\nwhere ğ‘Šğ‘œ âˆˆRğ»Ã—ğ»ğ‘¡ğ‘¦ğ‘ğ‘’ ,ğ‘Šğ‘¦ âˆˆRğ‘‰ğ‘¡ğ‘¦ğ‘ğ‘’Ã—ğ»ğ‘¡ğ‘¦ğ‘ğ‘’ ,ğ‘ğ‘¦ âˆˆRğ‘‰ğ‘¡ğ‘¦ğ‘ğ‘’ are train-\nable parameters. ğ‘‰ğ‘¡ğ‘¦ğ‘ğ‘’ is the vocabulary size of the tokenâ€™s type, ğ»\nis the hidden size of the transformer network, ğ»ğ‘¡ğ‘¦ğ‘ğ‘’ is the embed-\nding size of type vector.\n2) Token prediction: After predicting the tokenâ€™s type, we use the\npredicted type to assist the token prediction. The vector of the\npredicted type ğ¸ğ‘¡ğ‘¦ğ‘ğ‘’ and the hidden vector of the mask token\nâ„[ğ‘€ğ´ğ‘†ğ¾]are concatenated to compute the output vector for the\ntoken ğ‘‚ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›. Then the output vector is fed into the outputsoftmax\nlayer to compute the output vector for the token ğ‘Œğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›:\nğ‘‚ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› = tanh(ğ‘Šğ‘œ(â„[ğ‘€ğ´ğ‘†ğ¾]; ğ¸ğ‘¡ğ‘¦ğ‘ğ‘’))\nğ‘Œğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› = softmax(ğ‘Šğ‘¦ğ‘‚ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› +ğ‘ğ‘¦) (5)\nwhere ğ¸ğ‘¡ğ‘¦ğ‘ğ‘’ is the embedding of the predicted type,ğ‘Šğ‘œ âˆˆRğ»ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›Ã—ğ»,\nğ‘Šğ‘¦ âˆˆRğ‘‰ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›Ã—ğ»ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ,ğ‘ğ‘¦ âˆˆRğ‘‰ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› are trainable parameters.ğ‘‰ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›\nis the vocabulary size of the token, and â€œ;â€ denotes the concatenation\noperation.\nb) Unidirectional Language Modeling (ULM) : This objective is\na left-to-right language modeling task that is the same as the pre-\ntraining procedure. Given the preceding context tokensğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘¡,\nthe model predicts the next token ğ‘¥ğ‘¡+1, where the representation\nof each token encodes only the leftward context tokens and itself.\nTable 1: Statistics of the datasets.\nJava TypeScript\nProjects 9,708 8,446\nFiles 800,983 227,424\nLines 5.4 * 107 8.8 * 106\n# of Tokens 6.9 * 106 1.1 * 106\n# of Types 6.4 * 106 1.7 * 105\nMasked ID proportion 21.04% 9.74%\nDuring the fine-tuning procedure, the parameters of CugLM are\nlearned to minimize the sum of the cross-entropy losses of the two\nfine-tuning tasks and are shared among all the tasks. The final loss\nfunction is given below:\nmin\nğœƒ\nLğ‘ˆğ‘€ğ¿ğ‘€ (ğœƒ)+Lğ‘ˆğ¿ğ‘€ (ğœƒ) (6)\nThrough learning these two objectives jointly, we hope the model\ncan make better predictions on both the identifiers and the other\ntokens.\n4 EXPERIMENTS AND ANALYSIS\n4.1 Data preparation\nWe pre-train and fine-tune our model across two programming\nlanguages: Java and TypeScript. The programs in the corpus are\ncollected from publicly available open-source GitHub repositories\nby removing duplicate files and project forks. Each program is\ntokenized into token sequence. The detailed information is shown\nin Table 1. We use 60% of the projects for pre-training, and 40% of\nthe projects for fine-tuning on code completion task. During the\nfine-tuning, we split the projects into train/validation/test sets in\nthe proportion 8:1:1. For the other baselines, all the programs used\nin pre-training and the training programs used in fine-tuning are\nused as the training set, and the validation and test sets are the\nsame as in our fine-tuning procedure. We also randomly sample 200\nprogram files from both Java and TypeScript test sets as the small\ntest sets for Byte Pair Encoding based Neural Language Model (BPE\nNLM) [25] evaluation since when performing completion (testing)\nin their model, they use a variation of the beam search algorithm\nto combine the sub-units to complete tokens, which is very time-\nconsuming. It takes several minutes to complete a single program\nfile and will take tens of days to perform completion on the large\ntest sets (e.g., the Java test set contains 14,600 files). Thus, we create\nsmall test sets.\nFor Java programs, we extract the identifiersâ€™ type information\nthrough static analysis. For TypeScript programs, we apply the\napproach in Hellendoorn et al . [16] to extract type annotations\nof the identifiers. We filter the programs to make sure at least\n10% of type annotations are user-defined types in each TypeScript\nfile. Figure 5 shows the examples for Java and TypeScript code,\nwhere the identifiers that have type are marked with underlines,\nand the green tokens next to the identifiers are the corresponding\ntypes. To generate each training input sequence for pre-training,\nwe sample two spans of tokens from the corpus, which we refer\nto as segments ğ‘†1 and ğ‘†2. Each segment contains several lines of\nsource code tokens. For the first segment ğ‘†1, we sample the first ğ‘\nlines from one program file, where ğ‘ is randomly sampled from\nMulti-task Learning based Pre-trained Language Model for Code Completion ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia\nTable 2: Performance of baseline models and our approach.\nModel\nJava TypeScript\nLarge Test Small Test Large Test Small Test\nAll Tokens Identifiers All Tokens Identifiers All Tokens Identifiers All Tokens Identifiers\nVanilla LSTM 64.28% 33.84% 64.73% 33.11% 64.42% 28.11% 63.31% 23.91%\nPointer Mixture Network 68.30% 38.41% 68.49% 37.54% 68.75% 33.76% 65.75% 29.26%\nBPE NLM - - 67.17% 43.67% - - 65.39% 36.16%\nTransformer-XL 72.12% 43.63% 70.96% 40.92% 73.94% 37.46% 68.88% 34.90%\nCugLM 84.06% 55.19% 80.07% 48.47% 82.14% 41.85% 81.36% 39.28%\n type NamespaceName = |'s'|'s'|'s'|'s'|'s'|'s'|'s';\n interface Signature {\n     name: string;\n     email: string;\n     when: Date;\n }\n interface Commit {\n     author: Signature;\n     committer: Signature;\n     sha: string;\n     message: string;\n }\n interface NamespaceInfo {\n     count: number;\n     namespace: NamespaceName;\n     data: { [key]: {\n             intro:string; \n             name: string;\n         };\n     }; \n }\n package com.labo.kaji.swipeawaydialog;\n import android.app.Application;\n import android.test.ApplicationTestCase;\n public class ApplicationTest extends ApplicationTestCase <Application> {\n     public ApplicationTest ( ) {\n         super(Application.class);\n     }\n }\n ApplicationTestCase: android.test.ApplicationTestCase\n Application: android.app.Application\n ApplicationTest: com.labo.kaji.swipeawaydialog.ApplicationTest\n Application: android.app.Application\nTypeScript\nJava\nFigure 5: Code examples for type annotations.\n1 to the length of the code lines of the program file. 50% of the\ntime, the second segment ğ‘†2 is the rest of the lines from the same\nprogram file that followsğ‘†1, and 50% of the time it is a random code\nsegment sampled from other program files of the corpus, which\nis done for the â€œnext code segment prediction (NCP)â€ task. They\nare sampled such that the combined length is â‰¤128 tokens. For the\nâ€œMasked bidirectional Language Modeling (MLM)â€ task, we only\nmask those identifiers that have type information. For example, the\nunderlined tokens in Figure 5.\n4.2 Experimental Setup\nParameter configuration. We use Transformer with 6 layers, 516\ndimensional hidden states and 6 attention heads. The inner hidden\nsize of the feed-forward layer is 3072. We pre-train our model\nwith batch size of 16 sequences for 600,000 steps. We use Adam\nwith learning rate of 5e-5, ğ›½1 = 0.9, ğ›½2 = 0.999, L2 weight decay\nof 0.01, learning rate warmup over the first 1,000 steps, and linear\ndecay of the learning rate. We use a dropout probability of 0.1 on\nall layers. We use a gelu activation [ 18] following OpenAI GPT.\nThe training loss is the sum of the cross-entropy losses of the pre-\ntraining objectives or fine-tuning objectives. Training of CugLM\nwas performed on 3 GeForce GTX 1080 Ti GPUs with 12GB memory.\nFor each dataset, the model is pre-trained for 600,000 steps and takes\n4 days to complete, and is fine-tuned for 300,000 steps and takes 2\ndays to complete.\nMetric. We use accuracy to evaluate the performance of code com-\npletion. Our model provides an ordered list of suggestions for each\ntoken in the source code file given the context. We compute the\ntop-1 accuracy, i.e., the fraction of times the correct suggestion\nappears in the first of the predicted list.\nVocabulary. As shown in Table 1, in the datasets, the number\nof unique tokens and types is too large to build neural models\nto learn directly. We choose ğ¾ (50,000) most frequent tokens in\neach training set to build the token vocabulary, which is the same\nas Li et al. [27]â€™s study. For those tokens outside the vocabulary,\nwe use UNK (unknow values) to represent them. The size of type\nvocabulary is also set to 50,000. In both the training and test process,\nthe predictions of the UNK targets are treated as wrong predictions.\nThe token UNK rates for Java, and TypeScript test sets are 10%, 5%,\nand the type UNK rates are 9%, 1%, respectively.\n4.3 Research Questions and Results\nTo evaluate our proposed approach, in this section, we conduct\nexperiments to investigate the following research questions:\nRQ1: How does our proposed approach perform in code com-\npletion when compared with state-of-the-art models? To an-\nswer this research question, we compare our model with the fol-\nlowing baseline models:\nâ€¢vanilla LSTM: a vanilla LSTM neural network-based lan-\nguage model.\nâ€¢Pointer Mixture Network [ 27]: an attention and pointer-\ngenerator network-based code completion model.\nâ€¢Byte Pair Encoding based Neural Language Model (BPE\nNLM) [25]: a large-scale open-vocabulary NLM for code\ncompletion, which leverage BPE [ 13] algorithm to keep\nvocabulary size low and successfully predict OoV (Out-of-\nVocabulary) tokens.\nâ€¢Transformer-XL [8]: a self-attentional neural network-based\nlanguage model for code completion.\nASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Liu et al.\nTable 3: Effects of each pre-training task, fine-tuning task, and the type prediction in our proposed model.\nModel\nJava TypeScript\nLarge Test Small Test Large Test Small Test\nAll Tokens Identifiers All Tokens Identifiers All Tokens Identifiers All Tokens Identifiers\nFull Model 84.06% 55.19% 80.07% 48.47% 82.14% 41.85% 81.36% 39.28%\nPre-training tasks\n- ULM 78.64% 50.10% 77.78% 44.18% 77.83% 38.44% 76.77% 37.38%\n- MLM 77.42% 49.86% 76.41% 43.82% 78.93% 36.89% 78.28% 35.15%\n- NCP 81.24% 52.56% 79.79% 46.54% 78.52% 40.71% 79.02% 38.49%\nFine-tuning tasks\n- UMLM 80.93% 45.70% 77.21% 41.66% 78.86% 33.26% 77.58% 31.81%\n- ULM - 49.50% - 43.31% - 38.25% - 35.33%\n- Type Prediction 80.14% 52.05% 77.28% 46.83% 80.99% 40.73% 79.85% 38.31%\n1) Comparison with LSTM based closed vocabulary models (the\nfirst two baselines): To compare with Pointer Mixture Network, we\ndownloaded their publicly available source code1. In their model,\nthe programs in the datasets are parsed into ASTs, and they build\nthe model to perform code completion on AST node sequences.\nAlthough the ASTs can provide more information, representing the\nprograms as AST node sequences is not the natural order of typing,\nand the precision does not directly reflect the productivity gain of\nthe code completion tool. More importantly, in practice, the code is\nincomplete, so the software project might not be compilable (code\nis not parsable into ASTs, or parsed ASTs miss a lot of information).\nThus, representing programs as token sequences and performing\ncode completion on the token-level might be more practical. In this\npaper, we focus on token-level code completion. In our corpus, the\nprograms are tokenized into token sequences. To compare with\nthem, we train their model within our tokenized programs using\nthe command line arguments given in the artifactâ€™s README file 2.\nTheir base model is a single layer LSTM network with an unrolling\nlength of 50 and hidden unit size of 1500. The initial learning rate\nis 0.001 and is decayed by multiplying 0.6 after every epoch. The\ngradientsâ€™ norm is clipped to 5. The size of the attention window is\n50. Since the Pointer Mixture Network is based on LSTM language\nmodel, we also list the results of the vanilla LSTM, where the pa-\nrameter configuration of the vanilla LSTM network is set the same\nas the Pointer Mixture Network.\nAs shown from the results, our model outperforms the two\nLSTM-based models on both Java and TypeScript datasets by a\nlarge margin, especially in identifier completion. On the Java large\ntest set, our model achieves the accuracy of 84.06% and 55.19% on\ntokenâ€™s completion and identifierâ€™s completion, respectively, which\noutperforms Pointer Mixture Network by 23.07% and 43.69%, in\nterms of relative improvement. On the TypeScript large test set,\nour model achieves the accuracy of 82.14% and 41.85% on tokenâ€™s\ncompletion and identifierâ€™s completion, respectively, which outper-\nforms Pointer Mixture Network by 19.47% and 23.96%. The results\non small test sets are similar to the large test set. We can find\n1https://github.com/jack57lee/neuralCodeCompletion\n2Since the Pointer Mixture Network also makes use of the additional information\nderived from ASTs, the results of using token sequence as input might understate the\naccuracy of the plain Pointer Mixture Network.\nthat the improvements on the TypeScript dataset are smaller than\nJava, especially in identifier completion. The reason lies in that, the\n(masked) identifier proportion in TypeScript (9.74%) is smaller than\nJava (21.04%) because the type information in TypeScript is anno-\ntated by developers, and only a part of the identifiers are annotated.\nIn the MLM pre-training task, these identifiers are masked and are\npredicted based on their contextual tokens aiming at generating bet-\nter contextual and informative representations for these identifiers\nas well as other tokens. During fine-tuning, the type information\nof these identifiers is used to assist the identifiersâ€™ prediction. Due\nto the lower masked proportion, the pre-training and fine-tuning\nprocedure can offer less information than Java, thus resulting in\nsmaller improvements.\n2) Comparison with open vocabulary model (BPE NLM): To com-\npare with BPE NLM, we downloaded their publicly available source\ncode3 and train their model on our datasets. They use a single layer\nGRU NLM with an unrolling length of 200 built upon sub-word\nunits learned from BPE. The embedding size and the hidden unit\nsize are both set to 512 in their model. To keep the number of param-\neters comparable with our model and other baselines, we increase\nthe hidden unit size and the embedding size of their model to 1500.\nThere are three scenarios: static, dynamic, and maintenance, where\nthe dynamic and maintenance settings update modelâ€™s parameters\nduring testing. Since our model and other baselines do not update\nparameters during the test process, we present the results of the\nstatic scenario to make the comparison fair, and realize that evaluat-\ning dynamically may improve accuracy. As shown from the results,\nBPE NLM performs best on completing identifiers among all the\nbaseline models on both datasets, which proves the power of the\nopen vocabulary LM for predicting the identifiers. Even though,\nour model still outperforms the BPE NLM on completing identifiers.\nWhen evaluating on completing all kinds of tokens, the perfor-\nmance of BPE NLM is not as well as the identifier completion. Our\nmodel outperforms BPE NLM on completing all kinds of tokens by\na large margin.\n3) Comparison with transformer network based model (Transformer-\nXL): To find out if CugLMâ€™s promising results derive more from\nusing a Transformer-based model for code completion, or from the\n3https://github.com/mast-group/OpenVocabCodeNLM\nMulti-task Learning based Pre-trained Language Model for Code Completion ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia\nmulti-task learning based pre-training and fine-tuning, we compare\nour results to a Transformer-based model trained from scratch, i.e.,\nwithout the benefit of a pre-trained embedding. Transformer-XL is\na Transformer network based language model, which introduces\nthe notion of recurrence to model the long-term dependency of the\ninput sequence. We use a 6-layer Transformer-XL network with\n5 parallel heads. The dimension of each head is set to 64. We set\nthe segment length to be 128, and the length of the cached seg-\nments to 256. The dimensionality of the model (hidden unit) and\nthe embedding size is set to 800. The dimension of the feed-forward\nlayer is set to 1024. As seen from Table 2, transformer-XL model\noutperforms the other baseline models that are based on the recur-\nrent neural networks on both datasets, which demonstrates that\nthe Transformer-based network is more powerful than recurrent\nneural networks on this task. The performance of our model is\nsubstantially higher than the Transformer-XL model trained from\nscratch. We therefore conclude that pre-training and fine-tuning\nare crucial to CugLMâ€™s success.\nRQ2: What are the contributions of the pre-training tasks?\nWe perform an ablation study to examine the effects of the three\npre-trained tasks: ULM, MLM, and NCP. We conduct experiments\non pre-training the model without each task, and the fine-tuning\nprocedure remains unchanged. The results are shown in Table 3.\nThe first row shows the results of our full model. The second to the\nfourth rows present the results of removing ULM, MLM, and NCP\nfrom the full model during pre-training, respectively.\n- ULM Removing the ULM task during pre-training. The loss func-\ntion of the pre-training procedure consists ofLğ‘€ğ¿ğ‘€ and Lğ‘ğ¶ğ‘ƒ, and\nboth these tasks are based on the bidirectional transformer. As seen\nfrom the results, removing this task hurts the modelâ€™s performance.\nDuring fine-tuning, the objectives are based on the unidirectional\ntransformer. Thus, adding the ULM task during pre-training makes\nthe learned text representations more general because they are opti-\nmized for both bidirectional and unidirectional language modeling\nobjectives jointly, mitigating over-fitting to bidirectional language\nmodeling task. Removing the ULM task would make the parameters\nhard to optimized when fine-tuned on the unidirectional objectives.\nThus, the accuracy drops.\n- NCP Removing the Next Code segment Prediction task during the\npre-training. The loss function consists of Lğ‘ˆğ¿ğ‘€ and Lğ‘€ğ¿ğ‘€. The\nNCP tasks are added to help the model understand the relationships\nbetween the code segments. The model removing NCP performs\nworse than the full model, but performs better than removing ULM,\nwhich demonstrates that the NCP task is necessary to improve the\nperformance but contributes less than the ULM task.\n- MLM Pre-training the model without the Masked bidirectional\nLanguage Modeling objective, and the loss function consists of\nLğ‘ˆğ¿ğ‘€ and Lğ‘ğ¶ğ‘ƒ. As shown from the results, removing the MLM\nhurts the performance more than the other two tasks, especially on\nidentifier completion. MLM task can help the model generate better\ncontextual representations of the tokens, especially the identifiers,\nthus can improve the modelâ€™s performance significantly.\nThe above results demonstrate that all of the pre-training tasks\nare necessary to improve the performance, and MLM contributes\nmost to the improvements.\nRQ3: What are the contributions of the fine-tuning tasks?\nTo figure out the effectiveness of the fine-tuning procedure, we also\nconduct experiments by removing each of the fine-tuning task. The\nresults are shown in fifth and sixth rows of Table 3.\n- UMLM Removing the Unidirectional Masked Language Modeling\ntask during fine-tuning procedure. Only the left-to-right language\nmodeling task is performed and the loss function becomes Lğ‘ˆğ¿ğ‘€.\nAs seen from the results, removing this task hurts the modelâ€™s\nperformance on both two datasets, especially for the identifier pre-\ndiction. UMLM task can help the model generate better contextual\nrepresentations for the tokens. Besides, it can also utilize the type\ninformation of the identifiers during the fine-tuning. Thus, this\nfine-tuning task is necessary for improving the performance of the\ncode completion.\n- ULM Removing the Unidirectional Language Modeling task dur-\ning fine-tuning procedure. Under this setting, the model can only\nproduce the results of the masked identifier prediction. The loss\nfunction becomes Lğ‘ˆğ‘€ğ¿ğ‘€ . As seen from the results, when remov-\ning ULM task, the performance of the identifier prediction drops a\nlot, which demonstrates that the language modeling task can offer\nmuch help for the identifier prediction. Through optimizing the\nmodel on this task jointly, the model can capture the semantic of\nthe input code segment better, which serves as the basis of the\nimprovement on identifier prediction.\nRQ4: Could the predicted type help the model on token pre-\ndiction? When fine-tuning our model on code completion task, we\nutilize multi-task learning to predict the token and its type jointly.\nWe first predict the type and then use the type to assist the tokenâ€™s\nprediction. To confirm whether our model can correctly predict the\nidentifierâ€™s type, we present the accuracy of the type prediction.\nOur model achieves the accuracy of 68.89% and 79.31% on Java and\nTypeScript large test sets, respectively. The results demonstrate that\nour model can correctly predict the identifiersâ€™ type in most cases.\nTo find out whether the type prediction really helps, we conduct ex-\nperiment by removing the type prediction. The results are shown in\nthe last row of Table 3. As shown from the results, when removing\nthe type prediction, the model performs worse than the full model\non completing both identifiers and all tokens, which demonstrates\nthat the predicted type information can help the model achieve\nbetter performance on code completion.\n5 DISCUSSION\n5.1 The type of completions\nExcept for identifiers, we also give a detailed breakdown of the\naccuracies for completing different types of tokens on both our\nmodel and BPE NLM [25], and also present these tokensâ€™ propor-\ntion. The results are shown in Table 4. Punctuations make up the\nmajority of the completions, and the accuracies of both our model\nand BPE NLM on predicting punctuations are high, where BPE\nNLM performs better than CugLM. The punctuation tokens are\nmuch easier to complete than identifiers, but these completions\nare not that useful for developers [25]. For keyword completions,\nour model outperforms BPE LM by a large margin. The keywords\nare predefined, reserved words used in programming that have\nspecial meanings to the compiler, which contain the syntactic in-\nformation or the attribute information of the objects. The great\nperformance of CugLM on completing keywords further demon-\nstrates that through multi-task learning based pre-training and\nASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Liu et al.\nTable 4: Performance of completing different types of tokens.\nJava TypeScript\nType Proportion CugLM BPE NLM Proportion CugLM BPE NLM\nIdentifiers 28.99% 48.47% 42.27% 16.62% 39.28% 36.16%\nKeyword 7.69% 86.78% 72.57% 6.49% 79.47% 56.67%\nPunctuation 31.98% 87.38% 90.30% 45.42% 82.64% 82.93%\nNumerals 0.62% 72.62% 58.83% 1.22% 89.42% 82.44%\nOperator 3.80% 85.65% 76.92% 4.03% 75.98% 65.84%\nfine-tuning, the representations generated by our model can cap-\nture syntactic and semantic information better. For numeral and\noperator completions, which are more related to the semantic of\nthe programs, our model also outperforms BPE NLM substantially.\n5.2 Model complexity comparison\nTo analyze the complexity of our model and the baseline models,\nwe present the number of trainable parameters for all the models,\nshown in Table 5. The number of trainable parameters of our model\nis less than all the baselines. Although we adopt multi-task learn-\ning for both pre-training and fine-tuning, the number of trainable\nparameters does not increase much as all of the tasks share one\nmulti-layer transformer network. To improve training efficiency\nand avoid over-fitting, we do not use large parameter settings. Even\nthough, our model still outperforms the other baselines by a large\nmargin thanks to the pre-training and fine-tuning.\nTable 5: Parameters of the baseline models and our model.\nModel # of Parameters\nVanilla LSTM 168M\nPointer Mixture Network 177M\nBPE NLM 145M\nTransformer-XL 173M\nCugLM 104M\n5.3 Effect of applying BPE algorithm\nTo further improve the performance of our model, we also conduct\nexperiments on applying Byte Pair Encoding (BPE) algorithm to\nbuild up the vocabulary of sub-words as in [ 25], where the rare\ntokens will be segmented into more common sub-word units, and\nno word is OoV. However, the performance on Java corpus is compa-\nrable with the origin model, and the accuracy decreases slightly on\nTypeScript corpus. We analyze the possible reasons are as follows.\nDuring pre-training, we mask the identifiers with type information.\nWhen we apply BPE algorithm, most of these masked identifiers\nwill be split into sub-word units. Thus, all of these units will be\nmasked, which leads to the high mask proportion and increased the\ndifficulty of learning the semantics of embeddings. Besides, during\nfine-tuning, our model utilizes the predicted type information to\nassist the tokenâ€™s prediction. After splitting the tokens into sub-\nword units, all of the units from one token correspond to the same\ntype, resulting in the semantic inconsistencies between the type\ninformation and the sub-word units. For example, the same unit\nfrom different tokens might correspond to different types. Thus,\napplying BPE does not improve the performance of our model.\n5.4 Threats to Validity\nThreats to external validity relate to the quality of the datasets\nwe used and the generalizability of our results. We create two\nmassive datasets (Java and TypeScript) to pre-train and fine-tune\nour model. All of the programs in the datasets are collected from\nGitHub repositories. The reasons for using these two languages are\nas follows. These two languages are commonly used for software\ndevelopment, and we can get the identifiersâ€™ type through static\nanalysis or through the developersâ€™ annotations. However, further\nstudies are needed to validate and generalize our findings to other\nprogramming languages.\nThreats to internal validity include the influence of the hyper-\nparameters used in our model. The performance of our model would\nbe affected by different hyper-parameter settings, which are tuned\nempirically in our experiments. Thus, there is little threat to the\nhyper-parameter choosing, and there might be room for further im-\nprovement. However, current settings have achieved a considerable\nperformance increase. Another threat to internal validity relates\nto the implementation of the baseline methods. For Li et al. [27]â€™s\nmodel, we apply their model to the token-level code completion,\nwhich is originally used for AST-level code completion. In their\nmodel, the additional information derived from ASTs is utilized to\nimprove the performance. The results of using token sequence as\ninput might understate the accuracy of the plain Pointer Mixture\nNetwork. However, in practice, the code is incomplete, so the code\nis not parsable into ASTs, or parsed ASTs miss a lot of information.\nThus, representing programs as token sequences and performing\ncode completion on the token-level is more practical. Under this\nsetting, we have tried our best to make fair comparison with Li\net al. [27] by only changing the format of the input, and keeping\nthe model unchanged. For BPE NLM [25], we compare our model\nwith the static setting of their model considering the fairness of the\ncomparison. We realize that evaluating dynamically may improve\naccuracy. The dynamic and maintenance scenarios are not imple-\nmented and compared in this work, which will be considered as\nour future work.\nThreats to construct validity relate to the suitability of our eval-\nuation measure. We use accuracy as the metric which evaluates\nthe proportion of correctly predicted next token. It is a classical\nevaluation measure for code completion and is used in almost all\nthe previous code completion work [17, 19, 27, 41, 46].\n6 RELATED WORK\nStatistical Code Completion Code completion is a hot research\ntopic in the field of software engineering. Early work in code com-\npletion mainly bases on heuristic rules and static type information\nMulti-task Learning based Pre-trained Language Model for Code Completion ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia\nto make suggestions [21]. Since Hindle et al. [19] found that source\ncode contained predictable statistical properties, statistical language\nmodels began to be used for modeling source code [17, 27, 30, 38, 50],\nwhere N-gram is the most widely used model. [46] observed that\nsource code has a unique property of localness, which could not be\ncaptured by the traditional N-gram model. They improved N-gram\nby adding a cache mechanism to exploit localness and achieved bet-\nter performance than other N-gram based models. Hellendoorn and\nDevanbu [17] introduced an improved N-gram model that consid-\nered the unlimited vocabulary, nested scope, locality, and dynamism\nin source code.\nIn recent years, deep recurrent neural network-based language\nmodels have been applied to learning source code and have made\ngreat progress [3, 5, 27â€“29]. Liu et al. [28] proposed a code com-\npletion model based on a vanilla LSTM network. Li et al. [27] pro-\nposed a pointer mixture network to address the OoV issue. Liu\net al. [29] propose a multi-task learning and transformer based\nlanguage model for AST-level code completion. They built model\nto predict the AST nodeâ€™s type and value jointly and also utilized\nthe hierarchical structural information in the programâ€™s represen-\ntation, which achieves state-of-the-art results on AST-level code\ncompletion. Kim et al. [26] presented a transformer model for code\nprediction and incorporated syntactic structure into the transformer\nto further improve the modelâ€™s performance. Svyatkovskiy et al .\n[44] proposed a code completion system based on LSTM for recom-\nmending Python method calls. Their system is deployed as part of\nthe Intellicode extension in Visual Studio Code IDE. Karampatsis\net al. [25] proposed a large-scale open-vocabulary neural language\nmodel for source code, which leverages the BPE algorithm, beam\nsearch algorithm, and cache mechanism to both keep vocabulary\nsize low and successfully predict OoV tokens. The experimental\nresults demonstrate that their open vocabulary model outperforms\nboth N-gram models and closed vocabulary neural language mod-\nels, and achieve state-of-the-art performance on token-level code\ncompletion. Most recently, Svyatkovskoy et al. [45] implemented\nand evaluated a number of neural code completion models, which\noffer varying trade-offs in terms of memory, speed and accuracy.\nThey provided a well-engineered approach to deep-learning based\ncode completion, which is important to the software engineering\ncommunity.\nPre-trained Language Models Language model pre-training has\nshown to be effective for NLP, and has achieved the state-of-the-\nart results across many NLP tasks [ 7, 10, 22, 39, 40]. Pre-trained\nlanguage models can learn token contextualized representations\nby predicting tokens based on their context by training on large\namounts of data, and then can be adapted to downstream tasks.\nBidirectional Encoder Representations from Transformers (BERT)\n[10] is the widely used approach in NLP, which learns to predict\nthe masked words of a randomly masked word sequence given\nsurrounding contexts. BERT has significantly improved the perfor-\nmance of a wide range of natural language understanding tasks.\nKanade et al . [24] extended this idea to programming language\nunderstanding tasks. They derived contextual embedding of source\ncode by training a BERT model on source code. They evaluate their\nmodel on a benchmark of five classification tasks in programs. Re-\nsults show that their model outperforms the baseline LSTM models\nsupported by Word2Vec embeddings, and Transformers trained\nfrom scratch. The bidirectionality nature of BERT makes it difficult\nto be applied to natural language generation tasks. To overcome\nthis limitation, Dong et al. [11] proposed a unified pre-trained lan-\nguage model (UNILM) that can be applied to both natural language\nunderstanding and natural language generation tasks. UNILM can\nbe configured using different self-attention masks to aggregate con-\ntext for different types of language models, and thus can be used\nfor both language understanding and generation tasks.\nIn the above work, the models are learned from the input of a\nsingle modal, for example, only from natural languages or source\ncode. In recent years, multi-modal pre-trained models that can learn\nimplicit alignment between inputs of different modalities are pro-\nposed. These models are learned from bi-modal data, such as pairs\nof language-image [33], language-video [43], or language-code [12].\nFeng et al. [12] proposed CodeBERT, a bimodal pre-trained model\nfor natural language and programming language, aiming at captur-\ning the semantic connection between natural language (NL) and\nprogramming language (PL). They trained CodeBERT with masked\nlanguage modeling task and replaced token detection task, and\nevaluated it on two downstream NL-PL tasks, including natural\nlanguage code search and code documentation generation.\nInspired by the above models, we propose a code understanding\nand generation pre-trained language model with a transformer-\nbased architecture and tailored it for code completion, which is the\nfirst attempt at pre-training a language model for code completion.\n7 CONCLUSIONS AND FUTURE WORK\nIn this paper, we propose a multi-task learning based code under-\nstanding and generation pre-trained language model for source\ncode modeling with a Transformer-based neural architecture. We\npre-train our model on two massive datasets and with three objec-\ntive functions and then fine-tune it on code completion. Experimen-\ntal results demonstrate that the proposed model achieves better\nresults than previous state-of-the-art models on completing tokens,\nespecially on completing identifiers. To the best of our knowledge,\nwe are the first to apply the pre-trained language model to code\ncompletion. We believe this work represents a significant advance\nin source code modeling, which will be beneficial as a building\nblock for many other applications in this area.\nIn the future, we plan to apply our model to other programming\nlanguages and fine-tune our model to adapt to other tasks.\nACKNOWLEDGMENTS\nThis research is supported by the National Key R&D Program un-\nder Grant No. 2018YFB1003904, and the National Natural Science\nFoundation of China under Grant Nos. 61832009, 61620106007, and\n61751210.\nREFERENCES\n[1] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.\nA survey of machine learning for big code and naturalness. ACM Computing\nSurveys (CSUR) 51, 4 (2018), 1â€“37.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine\nTranslation by Jointly Learning to Align and Translate. (2015).\n[3] Avishkar Bhoopchand, Tim RocktÃ¤schel, Earl Barr, and Sebastian Riedel. 2016.\nLearning python code suggestion with a sparse pointer network. arXiv preprint\narXiv:1611.08307 (2016).\n[4] Rich Caruana. 1997. Multitask Learning. Machine Learning 28, 1 (1997), 41â€“75.\nASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Liu et al.\n[5] Hao Chen, Triet Huynh Minh Le, and Muhammad Ali Babar. 2020. Deep Learning\nfor Source Code Modeling and Generation: Models, Applications and Challenges.\nACM Computing Surveys (CSUR) (2020).\n[6] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Ben-\ngio. 2014. On the Properties of Neural Machine Translation: Encoder-Decoder\nApproaches. (2014), 103â€“111.\n[7] Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In\nAdvances in neural information processing systems . 3079â€“3087.\n[8] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and\nRuslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models beyond\na Fixed-Length Context. In Proceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019,\nVolume 1: Long Papers . 2978â€“2988.\n[9] Li Deng, Geoffrey E. Hinton, and Brian Kingsbury. 2013. New types of deep neural\nnetwork learning for speech recognition and related applications: an overview. In\nIEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP\n2013, Vancouver, BC, Canada, May 26-31, 2013 . IEEE, 8599â€“8603.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL-HLT (1). Association for Computational Linguistics, 4171â€“4186.\n[11] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng\nGao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-\ntraining for natural language understanding and generation. In Advances in\nNeural Information Processing Systems . 13042â€“13054.\n[12] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming\nGong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al . 2020. CodeBERT:\nA Pre-Trained Model for Programming and Natural Languages. arXiv preprint\narXiv:2002.08155 (2020).\n[13] Philip Gage. 1994. A new algorithm for data compression. C Users Journal 12, 2\n(1994), 23â€“38.\n[14] Han Guo, Ramakanth Pasunuru, and Mohit Bansal. 2018. Soft Layer-Specific\nMulti-Task Summarization with Entailment and Question Generation. InProceed-\nings of the 56th Annual Meeting of the Association for Computational Linguistics,\nACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers . Associa-\ntion for Computational Linguistics, 687â€“697.\n[15] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. Deepfix: Fix-\ning common c language errors by deep learning. In Thirty-First AAAI Conference\non Artificial Intelligence .\n[16] Vincent J Hellendoorn, Christian Bird, Earl T Barr, and Miltiadis Allamanis. 2018.\nDeep learning type inference. In Proceedings of the 2018 26th acm joint meeting\non european software engineering conference and symposium on the foundations of\nsoftware engineering . 152â€“162.\n[17] Vincent J Hellendoorn and Premkumar Devanbu. 2017. Are deep neural networks\nthe best choice for modeling source code?. In Proceedings of the 2017 11th Joint\nMeeting on Foundations of Software Engineering . 763â€“773.\n[18] Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic\nregularizers with gaussian error linear units. (2016).\n[19] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar T. De-\nvanbu. 2012. On the naturalness of software. In 34th International Conference\non Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland . IEEE\nComputer Society, 837â€“847.\n[20] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long Short-Term Memory.\nNeural Computation 9, 8 (1997), 1735â€“1780.\n[21] Daqing Hou and David M Pletcher. 2010. Towards a better code completion system\nby API grouping, filtering, and popularity-based ranking. In Proceedings of the\n2nd International Workshop on Recommendation Systems for Software Engineering .\n26â€“30.\n[22] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-\ntuning for Text Classification. InACL (1). Association for Computational Linguis-\ntics, 328â€“339.\n[23] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment\ngeneration. In Proceedings of the 26th Conference on Program Comprehension .\n200â€“210.\n[24] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2019. Pre-\ntrained Contextual Embedding of Source Code. arXiv preprint arXiv:2001.00059\n(2019).\n[25] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and\nAndrea Janes. 2020. Big Code!= Big Vocabulary: Open-Vocabulary Models for\nSource Code. ICSE.\n[26] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2020. Code Predic-\ntion by Feeding Trees to Transformers. arXiv preprint arXiv:2003.13848 (2020).\n[27] Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018. Code Completion with\nNeural Attention and Pointer Networks. In Proceedings of the Twenty-Seventh\nInternational Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19,\n2018, Stockholm, Sweden. 4159â€“4165.\n[28] Chang Liu, Xin Wang, Richard Shin, Joseph E Gonzalez, and Dawn Song. 2016.\nNeural Code Completion. (2016).\n[29] Fang Liu, Ge Li, Bolin Wei, Xin Xia, Ming Li, Zhiyi Fu, and Zhi Jin. 2019. A Self-\nAttentional Neural Architecture for Code Completion with Multi-Task Learning.\narXiv preprint arXiv:1909.06983 (2019).\n[30] Fang Liu, Lu Zhang, and Zhi Jin. 2020. Modeling programs hierarchically with\nstack-augmented LSTM. Journal of Systems and Software 164 (2020), 110547.\n[31] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang.\n2015. Representation Learning Using Multi-Task Deep Neural Networks for\nSemantic Classification and Information Retrieval. In NAACL HLT 2015, The 2015\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Denver, Colorado, USA, May 31 - June\n5, 2015 . The Association for Computational Linguistics, 912â€“921.\n[32] Mingsheng Long and Jianmin Wang. 2015. Learning Multiple Tasks with Deep\nRelationship Networks. CoRR abs/1506.02117 (2015). arXiv:1506.02117 http:\n//arxiv.org/abs/1506.02117\n[33] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining\ntask-agnostic visiolinguistic representations for vision-and-language tasks. In\nAdvances in Neural Information Processing Systems . 13â€“23.\n[34] Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and\nRogÃ©rio Schmidt Feris. 2017. Fully-Adaptive Feature Sharing in Multi-Task\nNetworks with Applications in Person Attribute Classification. In 2017 IEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI,\nUSA, July 21-26, 2017 . IEEE Computer Society, 1131â€“1140.\n[35] Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. 2019. NL2Type: inferring\nJavaScript function types from natural language information. In 2019 IEEE/ACM\n41st International Conference on Software Engineering (ICSE) . IEEE, 304â€“315.\n[36] Robert C. Martin. 2009. Clean Code - a Handbook of Agile Software Craftsmanship .\nPrentice Hall.\n[37] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.\nDistributed representations of words and phrases and their compositionality. In\nAdvances in neural information processing systems . 3111â€“3119.\n[38] Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen.\n2013. A statistical semantic language model for source code. In Joint Meeting of\nthe European Software Engineering Conference and the ACM SIGSOFT Symposium\non the Foundations of Software Engineering, ESEC/FSEâ€™13, Saint Petersburg, Russian\nFederation, August 18-26, 2013 . ACM, 532â€“542.\n[39] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word\nRepresentations. In NAACL-HLT. Association for Computational Linguistics,\n2227â€“2237.\n[40] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. 2018. Improving language understanding by genera-\ntive pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper.\npdf (2018).\n[41] Veselin Raychev, Pavol Bielik, and Martin T. Vechev. 2016. Probabilistic model for\ncode with decision trees. In Proceedings of the 2016 ACM SIGPLAN International\nConference on Object-Oriented Programming, Systems, Languages, and Applications,\nOOPSLA 2016, part of SPLASH 2016, Amsterdam, The Netherlands, October 30 -\nNovember 4, 2016 . ACM, 731â€“747.\n[42] Veselin Raychev, Martin T. Vechev, and Eran Yahav. 2014. Code completion\nwith statistical language models. In ACM SIGPLAN Conference on Programming\nLanguage Design and Implementation, PLDI â€™14, Edinburgh, United Kingdom - June\n09 - 11, 2014 . ACM, 419â€“428.\n[43] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.\n2019. Videobert: A joint model for video and language representation learning. In\nProceedings of the IEEE International Conference on Computer Vision . 7464â€“7473.\n[44] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundaresan. 2019. Pythia:\nAI-assisted Code Completion System. In Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining . 2727â€“2735.\n[45] Alexey Svyatkovskoy, Sebastian Lee, Anna Hadjitofi, Maik Riechert, Juliana\nFranco, and Miltiadis Allamanis. 2020. Fast and Memory-Efficient Neural Code\nCompletion. arXiv preprint arXiv:2004.13651 (2020).\n[46] Zhaopeng Tu, Zhendong Su, and Premkumar T. Devanbu. 2014. On the localness\nof software. In Proceedings of the 22nd ACM SIGSOFT International Symposium on\nFoundations of Software Engineering, (FSE-22), Hong Kong, China, November 16 -\n22, 2014 . ACM, 269â€“280.\n[47] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh.\n2019. Neural program repair by jointly learning to localize and repair. arXiv\npreprint arXiv:1904.01720 (2019).\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998â€“6008.\n[49] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and\nPhilip S Yu. 2018. Improving automatic source code summarization via deep rein-\nforcement learning. In Proceedings of the 33rd ACM/IEEE International Conference\non Automated Software Engineering . 397â€“407.\n[50] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code generation as a\ndual task of code summarization. In Advances in Neural Information Processing\nMulti-task Learning based Pre-trained Language Model for Code Completion ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia\nSystems. 6563â€“6573.\n[51] Huihui Wei and Ming Li. 2017. Supervised Deep Features for Software Functional\nClone Detection by Exploiting Lexical and Syntactical Information in Source\nCode.. In IJCAI. 3034â€“3040.\n[52] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong\nLiu. 2019. A novel neural source code representation based on abstract syntax\ntree. In 2019 IEEE/ACM 41st International Conference on Software Engineering\n(ICSE). IEEE, 783â€“794.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8471819162368774
    },
    {
      "name": "Security token",
      "score": 0.7392011284828186
    },
    {
      "name": "Language model",
      "score": 0.62364661693573
    },
    {
      "name": "Identifier",
      "score": 0.555635392665863
    },
    {
      "name": "Code (set theory)",
      "score": 0.5428287386894226
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5292301177978516
    },
    {
      "name": "Context (archaeology)",
      "score": 0.47367364168167114
    },
    {
      "name": "Code generation",
      "score": 0.44937068223953247
    },
    {
      "name": "Natural language processing",
      "score": 0.4396667778491974
    },
    {
      "name": "Machine learning",
      "score": 0.4321633279323578
    },
    {
      "name": "Task (project management)",
      "score": 0.42559266090393066
    },
    {
      "name": "Programming language",
      "score": 0.20987647771835327
    },
    {
      "name": "Engineering",
      "score": 0.08189254999160767
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}