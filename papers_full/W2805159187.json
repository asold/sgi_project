{
  "title": "Second Language Acquisition Modeling",
  "url": "https://openalex.org/W2805159187",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2911267484",
      "name": "Burr Settles",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5045815593",
      "name": "Chris Brust",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2212413516",
      "name": "Erin Gustafson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2194948529",
      "name": "Masato Hagiwara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1640398473",
      "name": "Nitin Madnani",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4312883844",
    "https://openalex.org/W2471174856",
    "https://openalex.org/W2015040676",
    "https://openalex.org/W650350307",
    "https://openalex.org/W2963492065",
    "https://openalex.org/W4300842610",
    "https://openalex.org/W2963117210",
    "https://openalex.org/W2127374659",
    "https://openalex.org/W1592805114",
    "https://openalex.org/W2162283255",
    "https://openalex.org/W1562878411",
    "https://openalex.org/W2098297786",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2806424637",
    "https://openalex.org/W2181262297",
    "https://openalex.org/W1997802248",
    "https://openalex.org/W2805245825",
    "https://openalex.org/W2917881729",
    "https://openalex.org/W1999187286",
    "https://openalex.org/W2806656092",
    "https://openalex.org/W2251743902",
    "https://openalex.org/W2805432432",
    "https://openalex.org/W1647671624",
    "https://openalex.org/W4239456069",
    "https://openalex.org/W2963572611",
    "https://openalex.org/W4292414119",
    "https://openalex.org/W2514897959",
    "https://openalex.org/W2806310915",
    "https://openalex.org/W2806208744",
    "https://openalex.org/W2805465457",
    "https://openalex.org/W2805963248",
    "https://openalex.org/W4249934679",
    "https://openalex.org/W2158698691",
    "https://openalex.org/W2328176404",
    "https://openalex.org/W2045385178",
    "https://openalex.org/W28412257",
    "https://openalex.org/W2029507764",
    "https://openalex.org/W2807624977"
  ],
  "abstract": "We present the task of second language acquisition (SLA) modeling. Given a history of errors made by learners of a second language, the task is to predict errors that they are likely to make at arbitrary points in the future. We describe a large corpus of more than 7M words produced by more than 6k learners of English, Spanish, and French using Duolingo, a popular online language-learning app. Then we report on the results of a shared task challenge aimed studying the SLA task via this corpus, which attracted 15 teams and synthesized work from various fields including cognitive science, linguistics, and machine learning.",
  "full_text": "Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 56–65\nNew Orleans, Louisiana, June 5, 2018.c⃝2018 Association for Computational Linguistics\nSecond Language Acquisition Modeling\nBurr Settles∗ Chris Brust∗ Erin Gustafson∗ Masato Hagiwara∗ Nitin Madnani†\n∗Duolingo, Pittsburgh, PA, USA†ETS, Princeton, NJ, USA\n{burr,chrisb,erin,masato}@duolingo.com nmadnani@ets.org\nAbstract\nWe present the task ofsecond language acqui-\nsition (SLA) modeling. Given a history of er-\nrors made by learners of a second language, the\ntask is to predict errors that they are likely to\nmake at arbitrary points in the future. We de-\nscribe a large corpus of more than 7M words\nproduced by more than 6k learners of English,\nSpanish, and French using Duolingo, a popular\nonline language-learning app. Then we report\non the results of a shared task challenge aimed\nstudying the SLA task via this corpus, which\nattracted 15 teams and synthesized work from\nvarious ﬁelds including cognitive science, lin-\nguistics, and machine learning.\n1 Introduction\nAs computer-based educational apps increase in\npopularity, they generate vast amounts of student\nlearning data which can be harnessed to drive per-\nsonalized instruction. While there have been some\nrecent advances for educational software in do-\nmains like mathematics, learning a language is\nmore nuanced, involving the interaction of lexi-\ncal knowledge, morpho-syntactic processing, and\nseveral other skills. Furthermore, most work that\nhas applied natural language processing to lan-\nguage learner data has focused on intermediate-to-\nadvanced students of English, particularly in as-\nsessment settings. Much less work has been de-\nvoted to beginners, learners of languages other\nthan English, or ongoing study over time.\nWe proposesecond language acquisition (SLA)\nmodeling as a new computational task to help\nbroaden our understanding in this area. First, we\ndescribe a new corpus of language learner data,\ncontaining more than 7.1M words, annotated for\nproduction errors that were made by more than\n6.4k learners of English, Spanish, and French, dur-\ning their ﬁrst 30 days of learning with Duolingo\n(a popular online language-learning app).\nThen we report on the results of a “shared task”\nchallenge organized by the authors using this SLA\nmodeling corpus, which brought together 15 re-\nsearch teams. Our goal for this work is three-\nfold: (1) to synthesize years of research in cog-\nnitive science, linguistics, and machine learning,\n(2) to facilitate cross-dialog among these disci-\nplines through a common large-scale empirical\ntask, and in so doing (3) to shed light on the most\neffective approaches to SLA modeling.\n2 Shared Task Description\nOur learner trace data comes from Duolingo:\na free, award-winning, online language-learning\nplatform. Since launching in 2012, more than\n200 million learners worldwide have enrolled in\nDuolingo’s game-like courses, either via the web-\nsite1 or mobile apps.\nFigure 1(a) is a screen-shot of the home screen,\nwhich speciﬁes the game-like curriculum. Each\nicon represents a skill, aimed at teaching themati-\ncally or grammatically grouped words or concepts.\nLearners can tap an icon to access lessons of new\nmaterial, or to review material once all lessons are\ncompleted. Learners can also choose to get a per-\nsonalized practice session that reviews previously-\nlearned material from anywhere in the course by\ntapping the “practice weak skills” button.\n2.1 Corpus Collection\nTo create the SLA modeling corpus, we sampled\nfrom Duolingo users who registered for a course\nand reached at least the tenth row of skill icons\nwithin the month of November 2015. By limit-\ning the data to new users who reach this level of\nthe course, we hope to better capture beginners’\nbroader language-learning process, including re-\npeated interaction with vocabulary and grammar\n1https://www.duolingo.com\n56\n(a) home screen\n (b) reverse_translate\n (c) reverse_tap\n (d) listen\nFigure 1:Duolingo screen-shots for an English-speaking student learning French (iPhone app, 2017). (a) The home\nscreen, where learners can choose to do a “skill” lesson to learn new material, or get a personalized practice session\nby tapping the “practice weak skills” button. (b–d) Examples of the three exercise types included in our shared task\nexperiments, which require the student to construct responses in the language they are learning.\nover time. Note that we excluded all learners who\ntook a placement test to skip ahead in the course,\nsince these learners are likely more advanced.\n2.2 Three Language Tracks\nAn important question for SLA modeling is: to\nwhat extent does an approach generalize across\nlanguages? While the majority of Duolingo users\nlearn English—which can signiﬁcantly improve\njob prospects and quality of life (Pinon and Hay-\ndon, 2010)—Spanish and French are the second\nand third most popular courses. To encourage re-\nsearchers to explore language-agnostic features,\nor uniﬁed cross-lingual modeling approaches, we\ncreated three tracks: English learners (who speak\nSpanish), Spanish learners (who speak English),\nand French learners (who speak English).\n2.3 Label Prediction Task\nThe goal of the task is as follows: given a his-\ntory of token-level errors made by the learner in\nthe learning language (L2), accurately predict the\nerrors they will make in the future. In particular,\nwe focus on three Duolingo exercise formats that\nrequire the learners to engage inactive recall, that\nis, they must construct answers in the L2 through\ntranslation or transcription.\nFigure 1(b) illustrates a reverse translate item,\nwhere learners are given a prompt in the language\nthey know (e.g., their L1 or native language), and\nlearner: wen can help\nreference: when can I help ?\nlabel: \u0017 \u0013 \u0017 \u0013\nFigure 2: An illustration of how data labels are gener-\nated. Learner responses are aligned with the most simi-\nlar reference answer, and tokens from the reference that\ndo not match are labeled errors.\ntranslate it into the L2. Figure1(c) illustrates are-\nverse tap item, which is a simpler version of the\nsame format: learners construct an answer using a\nbank of words and distractors. Figure1(d) is alis-\nten item, where learners hear an utterance in the L2\nthey are learning, and must transcribe it. Duolingo\ndoes include many other exercise formats, but we\nfocus on these three in the current work, since con-\nstructing L2 responses through translation or tran-\nscription is associated with deeper levels of pro-\ncessing, which in turn is more strongly associated\nwith learning (Craik and Tulving, 1975).\nSince each exercise can have multiple correct\nanswers (due to synonyms, homophones, or ambi-\nguities in tense, number, formality, etc.), Duolingo\nuses a ﬁnite-state machine to align the learner’s re-\nsponse to the most similar reference answer form\na large set of acceptable responses, based on token\nstring edit distance (Levenshtein, 1966). For ex-\nample, Figure1(b) shows an example of corrective\nfeedback based on such an alignment.\n57\nFigure 2 shows how we use these alignments to\ngenerate labels for the SLA modeling task. In this\ncase, an English (from Spanish) learner was asked\nto translate, “¿Cuándo puedo ayudar?” and wrote\n“wen can help” instead of “When can I help?” This\nproduces two errors (a typo and a missing pro-\nnoun). We ignore capitalization, punctuation, and\naccents when matching tokens.\n2.4 Data Set Format\nSample data from the resulting corpus can be found\nin Figure 3. Each token from the reference an-\nswer is labeled according to the alignment with the\nlearner’s response (the ﬁnal column: 0 for cor-\nrect and 1 for incorrect). Tokens are grouped\ntogether by exercise, including user-, exercise-,\nand session-level meta-data in the previous line\n(marked by the# character). We included all ex-\nercises done by the users sampled from the 30-day\ndata collection window.\nThe overall format is inspired by the Universal\nDependencies (UD) format2. Column 1 is a unique\nB64-encoded token ID, column 2 is a token (word),\nand columns 3–6 are morpho-syntactic features\nfrom the UD tag set (part of speech, morphology\nfeatures, and dependency parse labels and edges).\nThese were generated by processing the aligned\nreference answers with Google SyntaxNet (Andor\net al., 2016). Because UD tags are meant to be\nlanguage-agnostic, it was our goal to help make\ncross-lingual SLA modeling more straightforward\nby providing these features.\nExercise meta-data includes the following:\n• user: 8-character unique anonymous user ID\nfor each learner (B64-encoded)\n• countries: 2-character ISO country codes\nfrom which this learner has done exercises\n• days: number of days since the learner started\nlearning this language on Duolingo\n• client: session device platform\n• session: session type (e.g., lesson or practice)\n• format: exercise format (see Figure1)\n• time: the time (in seconds) it took the learner\nto submit a response for this exercise.\nLesson sessions (about 77% of the data set)\nare where new words or concepts are introduced,\nalthough lessons also include previously-learned\nmaterial (e.g., each exercise attempts to introduce\nonly one new word or inﬂection, so all other to-\nkens should have been seen by the student be-\n2http://universaldependencies.org\nTRAIN DEV TEST\nTrack Users Tokens (Err) Tokens (Err) Tokens (Err)\nEnglish 2.6k 2.6M (13%) 387k (14%) 387k (15%)\nSpanish 2.6k 2.0M (14%) 289k (16%) 282k (16%)\nFrench 1.2k 927k (16%) 138k (18%) 136k (18%)\nOverall 6.4k 5.5M (14%) 814k (15%) 804k (16%)\nTable 1:Summary of the SLA modeling data set.\n.\nfore). Practice sessions (22%) should contain only\npreviously-seen words and concepts. Test sessions\n(1%) are mini-quizzes that allow a student to skip\nout of a single skill in the curriculum (i.e., the stu-\ndent may have never seen this content before in the\nDuolingo app, but may well have had prior knowl-\nedge before starting the course).\nIt is worth mentioning that for the shared task,\nwe did not provide actual learner responses, only\nthe closest reference answers. Releasing such data\n(at least in the TEST set) would by deﬁnition give\naway the labels and might undermine the task.\nHowever, we plan to release a future version of the\ncorpus that is enhanced with additional meta-data,\nincluding the actual learner responses.\n2.5 Challenge Timeline\nThe data were released in two phases. In phase 1\n(8 weeks), TRAIN and DEV partitions were re-\nleased with labels, along with a baseline system\nand evaluation script, for system development. In\nphase 2 (10 days), the TEST partition was released\nwithout labels, and teams submitted predictions to\nCodaLab3 for blind evaluation. To allow teams to\ncompare different system parameters or features,\nthey were allowed to submit up to 10 predictions\ntotal (up to 2 per day) during this phase.\nTable 1 reports summary statistics for each of\nthe data partitions for all three tracks. We created\nTRAIN, DEV , and TEST partitions as follows. For\neach user, the ﬁrst 80% of their exercises were\nplaced in the TRAIN set, the subsequent 10% in\nDEV , and the ﬁnal 10% in TEST. Hence the three\ndata partitions are sequential, and contain ordered\nobservations for all users.\nNote that because the three data partitions are\nsequential, and the DEV set contains observations\nthat are potentially valuable for making TEST\nset predictions, most teams opted to combine the\nTRAIN and DEV sets to train their systems in ﬁ-\nnal phase 2 evaluations.\n3http://codalab.org\n58\n# user:XEinXf5+ countries:CO days:2.678 client:web session:practice format:reverse_translate time:6\noMGsnnH/0101 When ADV PronType=Int|fPOS=ADV++WRB advmod 4 1\noMGsnnH/0102 can AUX VerbForm=Fin|fPOS=AUX++MD aux 4 0\noMGsnnH/0103 I PRON Case=Nom|Number=Sing|Person=1|PronType=Prs|fPOS=PRON++PRP nsubj 4 1\noMGsnnH/0104 help VERB VerbForm=Inf|fPOS=VERB++VB ROOT 0 0\n# user:XEinXf5+ countries:CO days:5.707 client:android session:practice format:reverse_translate time:22\nW+QU2fm70301 He PRON Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs|fPOS=PRON++PRP nsubj 3 0\nW+QU2fm70302 's AUX Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|fPOS=AUX++VBZ aux 3 1\nW+QU2fm70303 wearing VERB Tense=Pres|VerbForm=Part|fPOS=VERB++VBG ROOT 0 0\nW+QU2fm70304 two NUM NumType=Card|fPOS=NUM++CD nummod 5 0\nW+QU2fm70305 shirts NOUN Number=Plur|fPOS=NOUN++NNS dobj 3 0\n# user:XEinXf5+ countries:CO days:10.302 client:web session:lesson format:reverse_translate time:28\nvOeGrMgP0101 We PRON Case=Nom|Number=Plur|Person=1|PronType=Prs|fPOS=PRON++PRP nsubj 2 0\nvOeGrMgP0102 eat VERB Mood=Ind|Tense=Pres|VerbForm=Fin|fPOS=VERB++VBP ROOT 0 1\nvOeGrMgP0103 cheese NOUN Degree=Pos|fPOS=ADJ++JJ dobj 2 1\nvOeGrMgP0104 and CONJ fPOS=CONJ++CC cc 2 0\nvOeGrMgP0105 they PRON Case=Nom|Number=Plur|Person=3|PronType=Prs|fPOS=PRON++PRP nsubj 6 0\nvOeGrMgP0106 eat VERB Mood=Ind|Tense=Pres|VerbForm=Fin|fPOS=VERB++VBP conj 2 1\nvOeGrMgP0107 ﬁsh NOUN fPOS=X++FW dobj 6 0\nFigure 3:Sample exercise data from an English learner over time: roughly two, ﬁve, and ten days into the course.\n2.6 Evaluation\nWe use area under the ROC curve (AUC) as\nthe primary evaluation metric for SLA model-\ning (Fawcett, 2006). AUC is a common measure of\nranking quality in classiﬁcation tasks, and can be\ninterpreted as the probability that the system will\nrank a randomly-chosen error above a randomly-\nchosen non-error. We argue that this notion of\nranking quality is particularly useful for evaluating\nsystems that might be used for personalized learn-\ning, e.g., if we wish to prioritize words or exer-\ncises for an individual learner’s review based on\nhow likely they are to have forgotten or make er-\nrors at a given point in time.\nWe also report F1 score—the harmonic mean of\nprecision and recall—as a secondary metric, since\nit is more common in similar skewed-class label-\ning tasks (e.g.,Ng et al., 2013). Note, however,\nthat F1 can be signiﬁcantly improved simply by\ntuning the classiﬁcation threshold (ﬁxed at 0.5 for\nour evaluations) without affecting AUC.\n3 Results\nA total of 15 teams participated in the task, of\nwhich 13 responded to a brief survey about their\napproach, and 11 submitted system description pa-\npers. All but two of these teams submitted predic-\ntions for all three language tracks.\nOfﬁcial shared task results are reported in Ta-\nble 2. System ranks are determined by sorting\nteams according to AUC, and using DeLong’s test\n(DeLong et al., 1988) to identify statistical ties.\nFor the remainder of this section, we provide a\nsummary of each team’s approach, ordered by the\nteam’s average rank across all three tracks. Certain\nteams are marked with modeling choice indicators\n(♢,♣ ,‡), which we discuss further in§5.\nSanaLabs (Nilsson et al., 2018) used a combi-\nnation of recurrent neural network (RNN) predic-\ntions with those of a Gradient Boosted Decision\nTree (GBDT) ensemble, trained independently for\neach track. This was motivated by the observa-\ntion that RNNs work well for sequence data, while\nGBDTs are often the best-performing non-neural\nmodel for shared tasks using tabular data. They\nalso engineered several token context features, and\nlearner/token history features such as number of\ntimes seen, time since last practice, etc.\nsingsound (Xu et al., 2018) used an RNN ar-\nchitecture using four types of encoders, represent-\ning different types of features: token context, lin-\nguistic information, user data, and exercise for-\nmat. The RNN decoder integrated information\nfrom all four encoders. Ablation experiments re-\nvealed the context encoder (representing the token)\ncontributed the most to model performance, while\nthe linguistic encoder (representing grammatical\ninformation) contributed the least.\nNYU (Rich et al., 2018) used an ensemble of\nGBDTs with features engineered based on psy-\nchological theories of cognition. Predictions for\neach track were averaged between a track-speciﬁc\nmodel and a uniﬁed model (trained on data from all\nthree tracks). In addition to the word, user, and ex-\nercise features provided, the authors included word\nlemmas, corpus frequency, L1-L2 cognates, and\nfeatures indicating user motivation and diligence\n(derived from usage patterns), and others. Abla-\ntion studies indicated that most of the performance\nwas due to the user and token features.\n59\nEnglish Track\n↑ Team AUC F1\n1 SanaLabs ♢♣ .861 .561\n1 singsound ♢ .861 .559\n3 NYU ♣‡ .859 .468\n4 TMU ♢‡ .848 .476\n5 CECL ‡ .846 .414\n6 Cambridge ♢ .841 .479\n7 UCSD ♣ .829 .424\n8 nihalnayak .821 .376\n8 LambdaLab ♣ .821 .389\n10 Grotoco .817 .462\n11 jilljenn .815 .329\n12 ymatusevych .813 .381\n13 renhk .797 .448\n14 zlb241 .787 .003\n15 SLAM_baseline .774 .190\nSpanish Track\n↑ Team AUC F1\n1 SanaLabs ♢♣ .838 .530\n2 NYU ♣‡ .835 .420\n2 singsound ♢ .835 .524\n4 TMU ♢‡ .824 .439\n5 CECL ‡ .818 .390\n6 Cambridge ♢ .807 .435\n7 UCSD ♣ .803 .375\n7 LambdaLab ♣ .801 .344\n9 Grotoco .791 .452\n9 nihalnayak .790 .338\n11 ymatusevych .789 .347\n11 jilljenn .788 .306\n13 renhk .773 .432\n14 SLAM_baseline .746 .175\n15 zlb241 .682 .389\nFrench Track\n↑ Team AUC F1\n1 SanaLabs ♢♣ .857 .573\n2 singsound ♢ .854 .569\n2 NYU ♣‡ .854 .493\n4 CECL ‡ .843 .487\n5 TMU ♢‡ .839 .502\n6 Cambridge ♢ .835 .508\n7 UCSD ♣ .823 .442\n8 LambdaLab ♣ .815 .415\n8 Grotoco .813 .502\n10 nihalnayak .811 .431\n10 jilljenn .809 .406\n10 ymatusevych .808 .441\n13 simplelinear .807 .394\n14 renhk .796 .481\n15 SLAM_baseline .771 .281\nTable 2: Final results. Ranks (↑) are determined by statistical ties (see text). Markers indicate which systems\ninclude recurrent neural architectures (♢), decision tree ensembles (♣ ), or a multitask model across all tracks (‡).\nTMU (Kaneko et al., 2018) used a combination\nof two bidirectional RNNs—the ﬁrst to predict po-\ntential user errors at a given token, and a second to\ntrack the history of previous answers by each user.\nThese networks were jointly trained through a uni-\nﬁed objective function. The authors did not engi-\nneer any additional features, but did train a single\nmodel for all three tracks (using a track ID feature\nto distinguish among them).\nCECL (Bestgen, 2018) used a logistic regres-\nsion approach. The base feature set was expanded\nto include many feature conjunctions, including\nword n-grams crossed with the token, user, format,\nand session features provided with the data set.\nCambridge (Yuan, 2018) trained two RNNs—\na sequence labeler, and a sequence-to-sequence\nmodel taking into account previous answers—and\nfound that averaging their predictions yielded the\nbest results. They focused on the English track, ex-\nperimenting with additional features derived from\nother English learner corpora. Hyper-parameters\nwere tuned for English and used as-is for other\ntracks, with comparable results.\nUCSD (Tomoschuk and Lovelett, 2018) used a\nrandom forest classiﬁer with a set of engineered\nfeatures motivated by previous research in mem-\nory and linguistic effects in SLA, including “word\nneighborhoods,” corpus frequency, cognates, and\nrepetition/experience with a given word. The sys-\ntem also included features speciﬁc to each user,\nsuch as mean and variance of error rates.\nLambdaLab (Chen et al., 2018) used GBDT\nmodels independently for each track, deriv-\ning their features from conﬁrmatory analysis\nof psychologically-motivated hypotheses on the\nTRAIN set. These include proxies for student en-\ngagement, spacing effect, response time, etc.\nnihalnayak (Nayak and Rao, 2018) used a lo-\ngistic regression model similar to the baseline,\nbut added features inspired by research in code-\nmixed language-learning where context plays an\nimportant role. In particular, they included word,\npart of speech, and metaphone features for previ-\nous:current and current:next token pairs.\nGrotoco (Klerke et al., 2018) also used logis-\ntic regression, including word lemmas, frequency,\ncognates, and user-speciﬁc features such as word\nerror rate. Interestingly, the authors found that ig-\nnoring each user’s ﬁrst day of exercise data im-\nproved their predictions, suggesting that learners\nﬁrst needed to familiarize themselves with app be-\nfore their data were reliable for modeling.\njilljenn (Vie, 2018) used a deep factorization\nmachine (DeepFM), a neural architecture devel-\noped for click-through rate prediction in recom-\nmender systems. This model allows learning from\nboth lower-order and higher-order induced fea-\ntures and their interactions. The DeepFM outper-\nformed a simple logistic regression baseline with-\nout much additional feature engineering.\nOther teams did not submit system description\npapers. However, according to a task organizer\nsurvey ymatusevych used a linear model with\nmultilingual word embeddings, corpus frequency,\nand several L1-L2 features such as cognates. Ad-\nditionally, simplelinear used an ensemble of some\nsort (for the French track only).renhk and zlb241\nprovided no details about their systems.\n60\nSLAM_baseline is the baseline system pro-\nvided by the task organizers. It is a simple logis-\ntic regression using data set features, trained sepa-\nrately for each track using stochastic gradient de-\nscent on the TRAIN set only.\n4 Related Work\nSLA modeling is a rich problem, and presents a\nopportunity to synthesize work from various sub-\nﬁelds in cognitive science, linguistics, and ma-\nchine learning. This section highlights a few key\nconcepts from these ﬁelds, and how they relate to\nthe approaches taken by shared task participants.\nItem response theory (IRT)is a common psy-\nchometric modeling approach used in educational\nsoftware (e.g., Chen et al., 2005). In its simplest\nform (Rasch, 1980), an IRT model is a logistic re-\ngression with two weights: one representing the\nlearner’s ability (i.e., user ID), and the other rep-\nresenting the difﬁculty of the exercise or test item\n(i.e., token ID). An extension of this idea is thead-\nditive factor model(Cen et al., 2008) which adds\nadditional “knowledge components” (e.g., lexical,\nmorphological, or syntactic features). Teams that\nemployed linear models (including our baseline)\nare essentially all additive factor IRT models.\nFor decades, tutoring systems have also em-\nployed sequence models like HMMs to perform\nknowledge tracing(Corbett and Anderson, 1995),\na way of estimating a learner’s mastery of knowl-\nedge over time. RNN-based approaches that en-\ncode user performance over time (i.e., that span\nacross exercises) are therefore variants of deep\nknowledge tracing(Piech et al., 2015).\nRelatedly, the spacing effect (Dempster, 1989)\nis the observation that people will not only learn\nbut also forget over time, and they remember more\neffectively through scheduled practices that are\nspaced out.Settles and Meeder(2016) andRidge-\nway et al.(2017) recently proposed non-linear re-\ngressions that explicitly encode the rate of forget-\nting as part of a decision surface, however none of\nthe current teams chose to do this. Instead, forget-\nting was either modeled through engineered fea-\ntures (e.g., user/token histories), or opaquely han-\ndled by sequential RNN architectures.\nSLA modeling also bears some similarity to re-\nsearch in grammatical error detection (Leacock\net al., 2010) andcorrection (Ng et al., 2013). For\nthese tasks, a model is given a (possibly ill-formed)\nsequence of words produced by a learner, and\nthe task is to identify which are mistakes. SLA\nmodeling is in some sense the opposite: given\na well-formed sequence of words that a learner\nshould be able to produce, identify where they are\nlikely to make mistakes. Given these similarities, a\nfew teams adapted state-of-the-art GEC/GED ap-\nproaches to create their SLA modeling systems.\nFinally, multitask learning(e.g., Caruana, 1997)\nis the idea that machine learning systems can do\nbetter at multiple related tasks by trying to solve\nthem simultaneously. For example, recent work\nin machine translation has demonstrated gains\nthrough learning to translate multiple languages\nwith a uniﬁed model (Dong et al., 2015). Simi-\nlarly, the three language tracks in this work pre-\nsented an opportunity to explore a uniﬁed multi-\ntask framework, which a few teams did with posi-\ntive results.\n5 Meta-Analyses\nIn this section, we analyze the various modeling\nchoices explored by the different teams in order to\nshed light on what kinds of algorithmic and feature\nengineering decisions appear to be useful for the\nSLA modeling task.\n5.1 Learning Algorithms\nHere we attempt to answer the question of whether\nparticular machine learning algorithms have a sig-\nniﬁcant impact on task performance. For example,\nthe results in Table2 suggest that the algorithmic\nchoices indicated by (♢,♣ ,‡) are particularly ef-\nfective. Is this actually the case?\nTo answer this question, we partitioned the\nTEST set into 6.4k subsets (one for each learner),\nand computed per-user AUC scores for each\nteam’s predictions (83.9k observations total). We\nalso coded each team with indicator variables to\ndescribe their algorithmic approach, and used a re-\ngression analysis to determine if these algorithmic\nvariations had any signiﬁcant effects on learner-\nspeciﬁc AUC scores.\nTo analyze this properly, however, we need to\ndetermine whether the differences among model-\ning choices are actually meaningful, or can simply\nbe explained by sampling error due to random vari-\nations among users, teams, or tracks. To do this,\nwe use alinear mixed-effects model(cf., Baayen,\n2008, Ch. 7). In addition to modeling theﬁxed\neffects of the various learning algorithms, we can\nalso model therandom effects represented by the\n61\nFixed effects (algorithm choices) Effect p-value\nIntercept .786 <.001 ***\nRecurrent neural network (♢) +.028 .012 *\nDecision tree ensemble (♣) +.018 .055 .\nLinear model (e.g., IRT) −.006 .541\nMultitask model (‡) +.023 .017 *\nRandom effects St. Dev.\nUser ID ±.086\nTeam ID ±.013\nTrack ID ±.011\nTable 3:Mixed-effects analysis of learning algorithms.\nuser ID (learners may vary by ability), the team ID\n(teams may differ in other aspects not captured by\nour schema, e.g., the hardware used), and the track\nID (tracks may vary inherently in difﬁculty).\nTable 3 presents a mixed-effects analysis for the\nalgorithm variations used by at least 3 teams. The\nintercept can be interpreted as the “average” AUC\nof .786. Controlling for the random effects of user\n(which exhibits a wide standard deviation of± .086\nAUC), team (± .013), and track (± .011), three of\nthe algorithmic choices are at least marginally sig-\nniﬁcant (p < .1). For example, we might expect\na system that uses RNNs to model learner mas-\ntery over time would add+.028 to learner-speciﬁc\nAUC (all else being equal). Note that most teams’\nsystems that were not based on RNNs or tree en-\nsembles used logistic regression, hence the “linear\nmodel” effect is negligible (effectively treated as a\ncontrol condition in the analysis).\nThese results suggest two key insights for SLA\nmodeling. First,non-linear algorithmsare particu-\nlarly desirable4, and second,multitask learningap-\nproaches that share information across tracks (i.e.,\nlanguages) are also effective.\n5.2 Feature Sets\nWe would also like to get a sense of which fea-\ntures, if any, signiﬁcantly affect system perfor-\nmance. Table 4 lists features provided with the\nSLA modeling data set, as well as several newly-\nengineered feature types that were employed by at\nleast three teams (note that the precise details may\nvary from team to team, but in our view aim to cap-\n4 Interestingly, the only linear model to rank among the\ntop 5 (CECL) relied on combinatorial feature conjunctions—\nwhich effectively alter the decision surface to be non-linear\nwith respect to the original features. The RNN hidden nodes\nand GBDT constituent trees from other top systems may in\nfact be learning to represent these same feature conjunctions.\nFeatures used Popularity Effect\nWord (surface form) +.005\nUser ID +.014\nPart of speech −.008\nDependency labels −.011\nMorphology features −.021\nResponse time +.028 *\nDays in course +.023 .\nClient +.005\nCountries +.012\nDependency edges −.000\nSession +.014\nWord corpus frequency +.008\nSpaced repetition features +.013\nL1-L2 cognates +.001\nWord embeddings +.020\nWord stem/root/lemma +.007\nTable 4: Summary of system features—both provided\n(top) and team-engineered (bottom)—with team popu-\nlarity and univariate mixed-effects estimates.\n.\nture the same phenomena). We also include each\nfeature’s popularity and an effect estimate5.\nBroadly speaking, results suggest that feature\nengineering had a much smaller impact on system\nperformance than the choice of learning algorithm.\nOnly “response time” and “days in course” showed\neven marginally signiﬁcant trends.\nOf particular interest is the observation that\nmorpho-syntactic features (described in§2.4) ac-\ntually seem to have weakly negative effects. This\nechoes singsound’s ﬁnding that their linguistic en-\ncoder contributed the least to system performance,\nand Cambridge determined through ablation stud-\nies that these features in fact hurt their system. One\nreasonable explanation is that these automatically-\ngenerated features contain too many systematic\nparsing errors to provide value. (Note thatNYU\nartiﬁcially introduced punctuation to the exercises\nand re-parsed the data in their work.)\nAs for newly-engineered features, word infor-\nmation such as frequency, semantic embeddings,\nand stemming were popular. It may be that these\nfeatures showed such little return because our cor-\npus was too biased toward beginners—thus rep-\nresenting a very narrow sample of language—for\nthese features to be meaningful. Cognate features\nwere an interesting idea used by a few teams, and\nmay have been more useful if the data included\n5This is similar to the analysis in§5.1, except that we\nregress on each feature separately. That is, a feature is the\nonly ﬁxed effect in the model (alongside intercept), while still\ncontrolling for user, team, and track random effects.\n62\nusers from a wider variety of different L1 lan-\nguage backgrounds. Spaced repetition features\nalso exhibited marginal (but statistically insignif-\nicant) gains. We posit that the 30-day window\nwe used for data collection was simply not long\nenough for these features to capture more long-\nterm learning (and forgetting) trends.\n5.3 Ensemble Analysis\nAnother interesting research question is: what is\nthe upper-bound for this task? This can be esti-\nmated by treating each team’s best submission as\nan independent system, and combining the results\nusing ensemble methods in a variety of ways. Such\nanalyses have been previously applied to other\nshared task challenges and meta-analyses (e.g.,\nMalmasi et al., 2017).\nThe oracle system is meant to be an upper-\nbound: for each token in the TEST set, the oracle\noutputs the team prediction with the lowest error\nfor that particular token. We also experiment with\nstacking (Wolpert, 1992) by training a logistic re-\ngression classiﬁer using each team’s prediction as\nan input feature6. Finally, we also pool system pre-\ndictions together by taking theiraverage (mean).\nTable5 reports AUC for various ensemble meth-\nods as well as some of the top performing team sys-\ntems for all three tracks. Interestingly, the oracle\nis exceptionally accurate (>.993 AUC and>.884\nF1, not shown). This indicates that thepotential\nupper limit of performance on this task is quite\nhigh, since there exists a near-perfect ranking of\ntokens in the TEST set based only on predictions\nfrom these 15 diverse participating teams.\nThe stacking classiﬁer produces signiﬁcantly\nbetter rankings than any of the constituent sys-\ntems alone, while the average (over all teams)\nranked between the 3rd and 4th best system in all\nthree tracks. Inspection of stacking model weights\nrevealed that it largely learned to trust the top-\nperforming systems, so we also tried simply av-\neraging the top 3 systems for each track, and this\nmethod was statistically tied with stacking for the\nEnglish and French tracks (p =0.002 for Spanish).\nInterestingly, the highest-weighted team in each\ntrack’s stacking model was singsound (+2.417\non average across the three models), followed\n6Note that we only have TEST set predictions for each\nteam. While we averaged stacking classiﬁer weights across\n10 folds using cross-validation, the reported AUC is still\nlikely an over-estimate, since the models were in some sense\ntrained on the TEST set.\nSystem English Spanish French\nOracle .995 .996 .993\nStacking .867 .844 .863\nAverage (top 3) .867 .843 .863\n1st team .861 .838 .857\n2nd team .861 .835 .854\n3rd team .859 .835 .854\nAverage (all) .857 .832 .852\n4th team .848 .824 .843\nTable 5:AUC results for the ensemble analysis.\nby NYU (+1.632), whereas the top-performing\nteam SanaLabs had a surprisingly lower weight\n(+0.841). This could be due to the fact that their\nsystem was itself an ensemble of an RNN and\nGBDT models, which were used (in isolation) by\neach of the other two teams. This seems to add\nfurther support for the effectiveness of combining\nthese algorithms for the task.\n6 Conclusion and Future Work\nIn this work, we presented the task of second\nlanguage acquisition (SLA) modeling, described a\nlarge data set for studying this task, and reported on\nthe results of a shared task challenge that explored\nthis new domain. The task attracted strong par-\nticipation from 15 teams, who represented a wide\nvariety of ﬁelds including cognitive science, lin-\nguistics, and machine learning.\nAmong our key ﬁndings is the observation that,\nfor this particular formulation of the task, the\nchoice of learning algorithm appears to be more\nimportant than clever feature engineering. In par-\nticular, the most effective teams employed se-\nquence models (e.g., RNNs) that can capture user\nperformance over time, and tree ensembles (e.g.,\nGBDTs) that can capture non-linear relationships\namong features. Furthermore, using a multitask\nframework—in this case, a uniﬁed model that\nleverages data from all three language tracks—can\nprovide further improvements.\nStill, many teams opted for a simpler algo-\nrithm (e.g., logistic regression) and concentrated\ninstead on more psychologically-motivated fea-\ntures. While these teams did not always perform as\nwell, several demonstrated through ablation stud-\nies that these features can be useful within the lim-\nitations of the algorithm. It is possible that the\nconstraints of the SLA modeling data set (beginner\nlanguage, homogeneous L1 language background,\nshort 30-day time frame, etc.) prevented these\nfeatures from being more useful across different\n63\nteams and learning algorithms. It would be inter-\nesting to revisit these ideas using a more diverse\nand longitudinal data set in the future.\nTo support ongoing research in SLA mod-\neling, current and future releases of our data\nset will be publicly maintained online at:\nhttps://doi.org/10.7910/DVN/8SWHNO.\nAcknowledgments\nThe authors would like to acknowledge Bożena\nPająk, Joseph Rollinson, and Hideki Shima for\ntheir help planning and co-organizing the shared\ntask. Eleanor Avrunin and Natalie Glance made\nsigniﬁcant contributions to early versions of the\nSLA modeling data set, and Anastassia Loukina\nand Kristen K. Reyher provided helpful advice\nregarding mixed-effects modeling. Finally, we\nwould like to thank the organizers of the NAACL-\nHLT 2018 Workshop on Innovative Use of NLP\nfor Building Educational Applications (BEA) for\nproviding a forum for this work.\nReferences\nD. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta,\nK. Ganchev, S. Petrov, and M. Collins. 2016.Glob-\nally normalized transition-based neural networks.\nCoRR, abs/1603.06042.\nR.H. Baayen. 2008. Analyzing Linguistic Data: A\nPractical Introduction to Statistics using R. Cam-\nbridge University Press.\nY . Bestgen. 2018. Predicting second language learner\nsuccesses and mistakes by means of conjunctive fea-\ntures. InProceedings of the NAACL-HLT Workshop\non Innovative Use of NLP for Building Educational\nApplications (BEA). ACL.\nR. Caruana. 1997. Multitask learning.Machine Learn-\ning, 28:41–75.\nH. Cen, K. Koedinger, and B. Junker. 2008. Compar-\ning two IRT models for conjunctive skills. InPro-\nceedings of the Conference on Intelligent Tutoring\nSystems (ITS), pages 796–798. Springer.\nC.M. Chen, H.M. Lee, and Y .H. Chen. 2005. Person-\nalized e-learning system using item response theory.\nComputers & Education, 44(3):237–255.\nG. Chen, C. Hauff, and G.J. Houben. 2018. Feature\nengineering for second language acquisition model-\ning. In Proceedings of the NAACL-HLT Workshop\non Innovative Use of NLP for Building Educational\nApplications (BEA). ACL.\nA.T. Corbett and J.R. Anderson. 1995. Knowledge\ntracing: Modeling the acquisition of procedural\nknowledge. User Modeling and User-Adapted In-\nteraction, 4(4):253–278.\nF.I.M. Craik and E. Tulving. 1975. Depth of process-\ning and the retention of words in episodic memory.\nJournal of Experimental Psychology, 104:268–294.\nE.R. DeLong, D.M. DeLong, and D.L. Clarke-Pearson.\n1988. Comparing the areas under two or more corre-\nlated receiver operating characteristic curves: a non-\nparametric approach. Biometrics, 44:837–845.\nF.N. Dempster. 1989. Spacing effects and their impli-\ncations for theory and practice. Educational Psy-\nchology Review, 1(4):309–330.\nD. Dong, H. Wu, W. He, D. Yu, and H. Wang. 2015.\nMulti-task learning for multiple language transla-\ntion. In Proceedings of the Association for Compu-\ntational Linguistics (ACL), pages 1723–1732. ACL.\nT. Fawcett. 2006. An introduction to ROC analysis.\nPattern Recognition Letters, 27(8):861–874.\nM. Kaneko, T. Kajiwara, and M. Komachi. 2018. TMU\nsystem for SLAM-2018. In Proceedings of the\nNAACL-HLT Workshop on Innovative Use of NLP\nfor Building Educational Applications (BEA). ACL.\nS. Klerke, H.M. Alonso, and B. Plank. 2018. Gro-\ntoco@SLAM: Second language acquisition mod-\neling with simple features, learners and task-wise\nmodels. In Proceedings of the NAACL-HLT Work-\nshop on Innovative Use of NLP for Building Educa-\ntional Applications (BEA). ACL.\nC. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.\n2010. Automated grammatical error detection for\nlanguage learners. Synthesis Lectures on Human\nLanguage Technologies, 3(1):1–134.\nV .I. Levenshtein. 1966. Binary codes capable of cor-\nrecting deletions, insertions, and reversals. Soviet\nPhysics Doklady, 10(8):707–710.\nS. Malmasi, K. Evanini, A. Cahill, J. Tetreault, R. Pugh,\nC. Hamill, D. Napolitano, and Y . Qian. 2017.A\nreport on the 2017 Native Language Identiﬁcation\nshared task. In Proceedings of the EMNLP Work-\nshop on Innovative Use of NLP for Building Edu-\ncational Applications (BEA), pages 62–75, Copen-\nhagen, Denmark. ACL.\nN.V . Nayak and A.R. Rao. 2018. Context based ap-\nproach for second language acquisition. In Pro-\nceedings of the NAACL-HLT Workshop on Innova-\ntive Use of NLP for Building Educational Applica-\ntions (BEA). ACL.\nH.T. Ng, S.M. Wu, Y . Wu, C. Hadiwinoto, and\nJ. Tetreault. 2013. The CoNLL-2013 shared task\non grammatical error correction. In Proceedings of\nthe Conference on Computational Natural Language\nLearning (CoNLL), pages 1–12. ACL.\n64\nS. Nilsson, A. Osika, A. Sydorchuk, F. Sahin, and\nA. Huss. 2018. Second language acquisition mod-\neling: An ensemble approach. InProceedings of the\nNAACL-HLT Workshop on Innovative Use of NLP\nfor Building Educational Applications (BEA). ACL.\nC. Piech, J. Bassen, J. Huang, S. Ganguli, M. Sahami,\nL.J. Guibas, and J. Sohl-Dickstein. 2015. Deep\nknowledge tracing. InAdvances in Neural Informa-\ntion Processing Systems (NIPS), pages 505–513.\nR. Pinon and J. Haydon. 2010. The beneﬁts of the En-\nglish language for individuals and societies: Quanti-\ntative indicators from Cameroon, Nigeria, Rwanda,\nBangladesh and Pakistan. Technical report, Eu-\nromonitor International for the British Council.\nG. Rasch. 1980. Probabilistic models for some in-\ntelligence and attainment tests. The University of\nChicago Press.\nA. Rich, P.O. Popp, D. Halpern, A. Rothe, and\nT. Gureckis. 2018. Modeling second-language\nlearning from a psychological perspective. InPro-\nceedings of the NAACL-HLT Workshop on Innova-\ntive Use of NLP for Building Educational Applica-\ntions (BEA). ACL.\nK. Ridgeway, M.C. Mozer, and A.R. Bowles. 2017.\nForgetting of foreign-language skills: A corpus-\nbased analysis of online tutoring software.Cognitive\nScience, 41(4):924–949.\nB. Settles and B. Meeder. 2016. A trainable spaced\nrepetition model for language learning. In Proceed-\nings of the Association for Computational Linguis-\ntics (ACL), pages 1848–1858. ACL.\nB. Tomoschuk and J. Lovelett. 2018. A memory-\nsensitive classiﬁcation model of errors in early sec-\nond language learning. In Proceedings of the\nNAACL-HLT Workshop on Innovative Use of NLP\nfor Building Educational Applications (BEA). ACL.\nJ.J. Vie. 2018. Deep factorization machines for knowl-\nedge tracing. In Proceedings of the NAACL-HLT\nWorkshop on Innovative Use of NLP for Building Ed-\nucational Applications (BEA). ACL.\nD.H. Wolpert. 1992. Stacked generalization. Neural\nNetworks, 5(2):241–259.\nS. Xu, J. Chen, and L. Qin. 2018. CLUF: A neural\nmodel for second language acquisition modeling. In\nProceedings of the NAACL-HLT Workshop on Inno-\nvative Use of NLP for Building Educational Appli-\ncations (BEA). ACL.\nZ. Yuan. 2018. Neural sequence modelling for learner\nerror prediction. InProceedings of the NAACL-HLT\nWorkshop on Innovative Use of NLP for Building Ed-\nucational Applications (BEA). ACL.\n65",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8330159187316895
    },
    {
      "name": "Task (project management)",
      "score": 0.7683371305465698
    },
    {
      "name": "Natural language processing",
      "score": 0.6574144959449768
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5772461295127869
    },
    {
      "name": "Second-language acquisition",
      "score": 0.5376362204551697
    },
    {
      "name": "Task analysis",
      "score": 0.4873982071876526
    },
    {
      "name": "Language acquisition",
      "score": 0.47475793957710266
    },
    {
      "name": "Language model",
      "score": 0.47333982586860657
    },
    {
      "name": "Linguistics",
      "score": 0.3090583384037018
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}