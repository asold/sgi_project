{
    "title": "Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis",
    "url": "https://openalex.org/W2893141505",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4225791641",
            "name": "Zhang, Kelly W.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221820538",
            "name": "Bowman, Samuel R.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2740711318",
        "https://openalex.org/W1985123443",
        "https://openalex.org/W2025768430",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2159636675",
        "https://openalex.org/W2962777840",
        "https://openalex.org/W2124807415",
        "https://openalex.org/W2618512726",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2605717780",
        "https://openalex.org/W2171865010",
        "https://openalex.org/W2170973209",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2775461895",
        "https://openalex.org/W2799124508",
        "https://openalex.org/W2164891816",
        "https://openalex.org/W2963212250",
        "https://openalex.org/W2962776659",
        "https://openalex.org/W2625092622",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2963918774"
    ],
    "abstract": "Recent work using auxiliary prediction task classifiers to investigate the properties of LSTM representations has begun to shed light on why pretrained representations, like ELMo (Peters et al., 2018) and CoVe (McCann et al., 2017), are so beneficial for neural language understanding models. We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn. With this in mind, we compare four objectives---language modeling, translation, skip-thought, and autoencoding---on their ability to induce syntactic and part-of-speech information. We make a fair comparison between the tasks by holding constant the quantity and genre of the training data, as well as the LSTM architecture. We find that representations from language models consistently perform best on our syntactic auxiliary prediction tasks, even when trained on relatively small amounts of data. These results suggest that language modeling may be the best data-rich pretraining task for transfer learning applications requiring syntactic information. We also find that the representations from randomly-initialized, frozen LSTMs perform strikingly well on our syntactic auxiliary tasks, but this effect disappears when the amount of training data for the auxiliary tasks is reduced.",
    "full_text": "LANGUAGE MODELING TEACHES YOU MORE SYN-\nTAX THAN TRANSLATION DOES : L ESSONS LEARNED\nTHROUGH AUXILIARY TASK ANALYSIS\nKelly W. Zhang∗\nSchool of Engineering and Applied Sciences\nHarvard University\nCambridge, MA 02420, USA\nkellywzhang@seas.harvard.edu\nSamuel R. Bowman\nCenter for Data Science & Dept. of Linguistics\nNew York University\nNew York, NY 10003 USA\nbowman@nyu.edu\nABSTRACT\nRecent work using auxiliary prediction task classiﬁers to investigate the properties\nof LSTM representations has begun to shed light on why pretrained representa-\ntions, like ELMo (Peters et al., 2018) and CoVe (McCann et al., 2017), are so ben-\neﬁcial for neural language understanding models. We still, though, do not yet have\na clear understanding of how the choice of pretraining objective affects the type\nof linguistic information that models learn. With this in mind, we compare four\nobjectives—language modeling, translation, skip-thought, and autoencoding—on\ntheir ability to induce syntactic and part-of-speech information. We make a fair\ncomparison between the tasks by holding constant the quantity and genre of the\ntraining data, as well as the LSTM architecture. We ﬁnd that representations from\nlanguage models consistently perform best on our syntactic auxiliary prediction\ntasks, even when trained on relatively small amounts of data. These results sug-\ngest that language modeling may be the best data-rich pretraining task for transfer\nlearning applications requiring syntactic information. We also ﬁnd that the rep-\nresentations from randomly-initialized, frozen LSTMs perform strikingly well on\nour syntactic auxiliary tasks, but this effect disappears when the amount of train-\ning data for the auxiliary tasks is reduced.\n1 I NTRODUCTION\nRepresentation learning with deep recurrent neural networks has revolutionized natural language\nprocessing and replaced many of the expert-designed, linguistic features used previously. Recently,\nresearchers have begun to investigate the properties of learned representations by training auxiliary\nclassiﬁers that use the hidden states of frozen, pretrained models to perform other tasks. These\ninvestigations have shown that when deep LSTMs (Hochreiter & Schmidhuber, 1997) are trained\non tasks like translation, they learn substantial syntactic and semantic information about their input\nsentences, including part-of-speech (Shi et al., 2016; Belinkov et al., 2017a;b; Blevins et al., 2018).\nThese intriguing ﬁndings lead us to ask the following questions:\n1. How does the training task affect how well models learn syntactic properties? Which tasks\nare better at inducing these properties?\n2. How does the amount of data the model is trained on affect these results? When does\ntraining on more data help?\nWe investigate these questions by holding the data source and model architecture constant, while\nvarying both the training task and the amount of training data. Speciﬁcally, we examine models\ntrained on English-German (En-De) translation 1 , language modeling, skip-thought (Kiros et al.,\n∗Majority of work done while at New York University.\n1Although Belinkov et al. (2017a) ﬁnd that translating into morphologically poorer languages leads to a\nslight improvement in encoder representations, we expect that our study of English-German translation will\nprovide a reasonable overall picture of the representations that can be learned in data-rich translation.\n1\narXiv:1809.10040v2  [cs.CL]  7 Jan 2019\n2015), and autoencoding, and also compare to an untrained LSTM model as a baseline. We con-\ntrol for the data domain by exclusively training on datasets from the 2016 Conference on Machine\nTranslation (WMT; Bojar et al., 2016). We train models on all tasks using the parallel En-De cor-\npus, which allows us to make fair comparisons across tasks. We also train models on a subset of the\nthis corpus to examine the effect of training data volume on learned representations. Additionally,\nwe augment the parallel dataset with a large monolingual corpus from WMT to examine how the\nperformance of the unsupervised tasks (all but translation) scale with more data.\nThroughout our work, we focus on the syntactic evaluation tasks of part-of-speech (POS) tagging\nand Combinatorial Categorical Grammar (CCG) supertagging. Supertagging is considered a build-\ning block for parsing as these tags constrain the ways in which words can compose, largely deter-\nmining the parse of the sentence. CCG supertagging thus allows us to measure the degree to which\nmodels learn syntactic structure above the word. We focus our analysis on representations learned by\nlanguage models and by the encoders of sequence-to-sequence models, as translation encoders have\nbeen found to learn richer representations of POS and morphological information than translation\ndecoders (Belinkov et al., 2017a).\nWe ﬁnd that for POS and CCG tagging, bidirectional language models (BiLMs)—created by sep-\narately training forward and backward language models, and concatenating their hidden states—\noutperform models trained on all other tasks. Even BiLMs trained on relatively small amounts of\ndata (1 million sentences) outperform translation and skip-thought models trained on larger datasets\n(5 million and 63 million sentences respectively).\nOur inclusion of an untrained LSTM baseline allows us to study the effect of training on hidden\nstate representations of LSTMs. We ﬁnd, surprisingly, that when we use all of the available labeled\ntag data to train our auxiliary task classiﬁers, our best trained models (BiLMs) only outperform the\nrandomly initialized, untrained LSTMs by a few percentage points. When we reduce the amount of\nclassiﬁer training data though, the performance of the randomly initialized LSTM model drops far\nbelow those of trained models. We hypothesize that this occurs because training the classiﬁers on\nlarge amounts of auxiliary task data allows them to memorize conﬁgurations of words seen in the\ntraining set and their associated tags. We test this hypothesis by training classiﬁers to predict the\nidentity of neighboring words from a given hidden state, and ﬁnd that randomly initialized models\noutperform all trained models on this task. Our ﬁndings demonstrate that our best trained models\ndo well on the tagging tasks because they are truly learning representations that conform to our\nnotions of POS and CCG tagging, and not simply because the classiﬁers we train are able to recover\nneighboring word identity information.\n2 R ELATED WORK\nEvaluating Learned Representations Adi et al. (2016) introduce the idea of examining sentence\nvector representations by training auxiliary classiﬁers to take sentence encodings and predict at-\ntributes like word order. Belinkov et al. (2017a) build on this work by examining the hidden states\nof LSTMs trained on translation and ﬁnd that they learn substantial POS and morphological infor-\nmation without direct supervision for these linguistic properties. Beyond translation, Blevins et al.\n(2018) ﬁnd that deep LSTMs learn hierarchical syntax when trained on a variety of tasks—including\nsemantic role labeling, language modeling, and dependency parsing. However, the models exam-\nined by Blevins et al. (2018) were also trained on different datasets, so it’s unclear if the differences\nin syntactic task performance are due to the training objectives or simply differences in the training\ndata. By controlling for model size and the quantity and genre of the training data, we we are able\nto make direct comparisons between tasks on their ability to induce syntactic information.\nTransfer Learning of Representations Much of the work on sentence-level pretraining has fo-\ncused on sentence-to-vector models and evaluating learned representations on how well they can\nbe used to perform sentence-level classiﬁcation tasks. A prominent early success in this area with\nunlabeled data is skip-thought (Kiros et al., 2015), the technique of training a sequence-to-sequence\nmodel to predict the sentence preceding and following each sentence in a running text. InferSent\n(Conneau et al., 2017)—the technique of pretraining encoders on natural language inference data—\nyields strikingly better performance when such labeled data is available.\n2\nTask Layer Size Attn. 1 Million 5 Million 15 Million 63 Million\nTranslation 2 ×500D Y 13.2 (17.6 BLEU) 9.1 (21.4 BLEU) – –\nTranslation 2 ×500D N 25.2 (6.8 BLEU) 13.0 (12.3 BLEU) – –\nLM Forward 1 ×500D – 104.8 81.2 82.3 76.9\nLM Backward 1 ×500D – 103.2 80.8 81.1 77.3\nLM Forward 1 ×1000D – 103.8 73.6 69.2 66.5\nSkip-Thought 2 ×500D Y 99.0 69.2 68.7 67.9\nSkip-Thought 2 ×500D N 104.1 72.0 68.1 66.7\nAutoencoder 2 ×500D Y 1.0 1.0 1.0 1.0\nAutoencoder 2 ×500D N 1.0 1.1 1.2 1.1\nTable 1: Perplexity of trained models by number of training sentences. All but the language models\nare 1000D BiLSTMs (500D per direction). The 500D forward and backward language models are\ncombined into a single bidirectional language model for analysis experiments.\nWork in transfer learning of representations has recently moved beyond strict sentence-to-vector\nmappings. Dai & Le (2015) was one of the earliest works to use unsupervised pretraining of LSTMs\nfor transfer learning on text classiﬁcation tasks. Newer models that incorporate LSTMs pretrained\non data-rich tasks, like translation and language modeling, have achieved state-of-the-art results\non many tasks—including semantic role labeling and coreference resolution (Peters et al., 2018;\nMcCann et al., 2017; Howard & Ruder, 2018). Although comparisons have previously been made\nbetween translation and language modeling as pretraining tasks (Peters et al., 2018), we investigate\nthis issue more thoroughly by controlling for the quantity and content of the training data.\nTraining Dataset Size The performance of neural models depends immensely on the amount of\ntraining data used. Koehn & Knowles (2017) ﬁnd that when training machine translation models on\ncorpora with fewer than 15 million words (English side), statistical machine translation approaches\noutperform neural ones. Similarly, Hestness et al. (2017) study the affect of training data volume\non performance for several tasks—including translation and image classiﬁcation. They ﬁnd that for\nsmall amounts of data, neural models perform about as well as chance. After a certain threshold,\nmodel performance improves logarithmically with the amount of training data, but this eventually\nplateaus. With this in mind, we also vary the amount of training data to investigate the relationship\nbetween performance and data volume for each task.\nRandomly Initialized Models Conneau et al. (2018) use randomly initialized LSTMs as a base-\nline when studying sentence-to-vector embedding models. They ﬁnd that untrained models outper-\nform many trained models on several auxiliary tasks, including predicting word content. Similarly in\nvision, untrained convolutional networks have been shown to capture many low-level image statis-\ntics and can be used for image denoising (Ulyanov et al., 2017). Our method of training auxiliary\nclassiﬁers on randomly initialized RNNs builds on the tradition of reservoir computing, in which\nrandomly initialized networks or “reservoirs” are ﬁxed and only “read-out” classiﬁer networks are\ntrained (Lukoˇseviˇcius & Jaeger, 2009). Echo state networks—reservoir computing with recurrent\nmodels—have been used for tasks like speech recognition, language modeling, and time series pre-\ndiction (Verstraeten et al., 2006; Tong et al., 2007; Sun et al., 2017).\n3 M ETHODS\n3.1 M AIN TRAINING DATA\nWe use the parallel English-German (En-De) dataset from the 2016 ACL Conference on Machine\nTranslation (WMT) shared task on news translation (Bojar et al., 2016). This dataset contains 5\nmillion ordered sentence translation pairs. We also use the 2015 English monolingual news dis-\ncussion dataset from the same WMT shared task, which contains approximately 58 million ordered\nsentences. To examine how the volume of training data affects learned representations, we use four\ncorpus sizes: 1, 5, 15, and 63 million sentences (translation is only trained on the smaller two sizes).\nWe create the 1 million sentence corpora from the 5 million sentence dataset by sampling (i) sen-\ntence pairs for translation, (ii) English sentences for autoencoders, and (iii) ordered English sentence\n3\nSoon\nRB\nshe\nPRP\nwas\nVBD\nrunning\nVBG\nthe\nDT\nofﬁce\nNN\n(a) POS tags\nSoon\nS/S\nshe\nNP\nwas\n(S\\NP)/NP\nrunning\nNP >\n(S\\NP) / NP\nthe\nNP/N\nofﬁce\nN >\nNP >\nS\\NP <\nS >\nS\n(b) A CCG parse, with supertags shown immediately\nbelow the words.\nFigure 1: An annotated PTB example sentence.\npairs for skip-thought and language models 2. Similarly, we create the 15 million sentence corpora\nfor the unsupervised tasks by sampling sentences from the entire corpus of 63 million sentences.\nWe use word-level representations throughout and use the Moses package (Koehn et al., 2007) to\ntokenize and truecase our data. Finally, we limit both the English and German vocabularies to the\n50k most frequent tokens in the training set.\n3.2 M ODEL ARCHITECTURE AND TRAINING\nWe train all our models using OpenNMT-py (Klein et al., 2017) and use the default options for model\nsizes, hyperparameters, and training procedure—except we increase the size of the LSTMs, make\nthe encoders bidirectional, and use validation-based learning rate decay instead of a ﬁxed sched-\nule. Speciﬁcally, all our models (except language models) are 1000D, two-layer encoder-decoder\nLSTMs with bidirectional encoders (500D per direction) and 500D embeddings. We train models\nboth with and without attention (Bahdanau et al., 2015). For language models, we train a 1000D\nforward language model and a bidirectional language model—two 500D language models (forward\nand backward) trained separately, whose hidden states are concatenated. All models, including our\nuntrained baseline, are initialized from a uniform distribution (−0.1, 0.1), the default in OpenNMT.\nWe use the same training procedure for all our models. We evaluate on the validation set every\nepoch when training on the 1 and 5 million sentence datasets, and evaluate approximately every 5\nmillion sentences when training on the larger datasets. We use SGD with an initial learning rate\nof 1. Whenever a model’s validation loss increases relative to the previous evaluation, we halve\nthe learning rate and stop training when the learning rate reaches 0.515. For each training task and\ndataset size, we select the model with the lowest validation perplexity to perform auxiliary task\nevaluations on. We report model performance in terms of perplexity and BLEU (Papineni et al.,\n2002) in Table 1. For translation we use beam search (B = 5) when decoding.\n3.3 C LASSIFIER DATA AND ARCHITECTURE\nPOS and CCG For Part-of-Speech (POS) tagging evaluation, we use the Wall Street Journal\n(WSJ) portion of the Penn Treebank (PTB; Marcus et al., 1993) We follow the standard WSJ split\n(train 2-21; dev 22; test 23). The dataset contains approximately 50k sentences and 45 tag types.\nFor CCG supertagging, we use CCG Bank (Hockenmaier & Steedman, 2007), which is based on\nPTB WSJ. CCG supertagging provides ﬁne-grained information about the role of each word in its\nlarger syntactic context and is considered almost parsing, since sequences of tags map sentences to\nsmall subsets of possible parses. The entire dataset contains approximately 50k sentences and 1327\ntag types. We display POS and CCG tags for an example sentence in Figure 1.\nTo study the impact of auxiliary task training data volume, for both datasets we create smaller\nclassiﬁer training sets by sampling 10% and 1% of the sentences. We truecase both datasets using\nthe same truecase model trained on WMT and restrict the vocabularies to the 50k tokens used in\npretraining our LSTM models. In addition to the untrained LSTM baseline, we also compare to the\nword-conditional most frequent class (WC-MFC)—the most frequently assigned tag class for each\ndistinct word in the training set. For this baseline we restrict the vocabulary to that of our LSTM\n2Note, in training we initialize the language model LSTM hidden states with the ﬁnal state after reading the\nprevious sentence.\n4\nmodels and map all out-of-vocabulary words to a single UNK token. Note that while PTB and WMT\nare both drawn from news text, there is slight genre mismatch.\nFigure 2: POS and CCG tagging accuracies for different amounts of LSTM encoder and classiﬁer\ntraining data. We show results for the best performing layer of each model. Note, BiLMs are dis-\nplayed with the attention models and forward LMs are displayed with the models without attention.\nWord Identity For this task, the classiﬁer takes a single LSTM hidden state as input and predicts\nthe identity of the word at a different time step. For example, for the sentence I love NLP and a time\nstep shift of -2, we would train the classiﬁer to take the hidden state for NLP and predict the word I.\nWe use the WSJ dataset for this task. Following Conneau et al. (2018), we take all words that occur\nbetween 100 and 1000 times (about 1000 words total) as the possible targets for neighboring word\nprediction.\nClassiﬁer Training Procedure We train multi-layer perceptron (MLP) classiﬁers that take an\nLSTM hidden state (from one time step and one layer) and output a distribution over the possi-\nble labels (tags or word identities). The MLPs we train have a single 1000D hidden layer with a\nReLU activation. For classiﬁer training, we use the same training and learning rate decay procedure\nused for pretraining the LSTM encoders.\n5\n(a) WC-MFC baselines for different amounts of PTB\ntraining data: 1% PTB: 81.8%; 10% PTB: 88.6%;\n100% PTB: 89.9%.\n(b) WC-MFC baselines for different amounts of CCG\ntraining data: 1% CCG: 62.3%; 10% CCG: 68.3%;\n100% CCG: 71.6%.\nFigure 3: POS and CCG tagging accuracies for different amounts of classiﬁer training data in terms\nof percentage points over the word-conditional most frequent class (WC-MFC) baseline. We show\nresults for the best performing layer and model for each task.\n4 C OMPARING PRETRAINING TASKS\nIn this section we discuss the main POS and CCG tagging results displayed in Figure 2. Overall,\nPOS and CCG tagging accuracies tend to increase with the amount of data the LSTM encoders are\ntrained on, but the marginal improvement decreases as the amount of training data increases.\nLanguage Modeling and Translation For all pretraining dataset sizes, bidirectional language\nmodel (BiLM) and translation encoder representations perform best on both POS and CCG tagging.\nTranslation encoders, however, slightly underperform BiLMs, even when both models are trained on\nthe same amount of data. In fact, even BiLMs trained on the smallest amount of data (1 million sen-\ntences) outperform models trained on all other tasks and dataset sizes (up to 5 million sentences for\ntranslation, and 63 million sentences for skip-thought and autoencoding). Especially since BiLMs\ndo not require aligned data to train, the superior performance of BiLM representations on these tasks\nsuggests that BiLMs (like ELMo; Peters et al., 2018) are better than translation encoders (like CoVe;\nMcCann et al., 2017) for transfer learning of syntactic information. One reason BiLMs perform rel-\natively well on these syntactic tasks could be that in contrast to the encoders for the other tasks, LM\nencoders have a per-token loss. Note also that since our evaluation tasks also predict a single label\nfor each token, this could be one reason that BiLMs perform so well on these tasks in particular.\nFor all amounts of training data, the BiLMs signiﬁcantly outperform the 1000D forward-only lan-\nguage models. The gap in performance between bidirectional and forward language models is\ngreater for CCG supertagging than for POS tagging. When using all available auxiliary training\ndata, there is a 2 and 8 percentage point performance gap in POS and CCG tagging respectively.\nThis difference in relative performance suggests that bidirectional context information is more im-\nportant for identifying syntactic structure than for identifying part of speech.\nFigure 2 illustrates how the best performing BiLMs and translation models tend to be more robust to\ndecreases in classiﬁer data than models trained on other tasks. Also, when training on less auxiliary\ntask data, POS tagging performance tends to drop less than CCG supertagging performance. For the\nbest model (BiLM trained on 63 million sentences), when using 1% rather than all of the auxiliary\ntask training data, CCG accuracy drops 9 percentage points, while POS accuracy only drops 2 points.\nFurther examinations of the effect of classiﬁer data volume are displayed in Figure 3.\nSkip-Thought Although skip-thought encoders consistently underperform both BiLMs and trans-\nlation encoders in all data regimes we examine, skip-thought models improve the most when increas-\ning the amount of pretraining data, and are the only models whose performance does not seem to\nhave plateaued by 63 million training sentences. Since we train our language models on ordered sen-\ntences, as we do for skip-thought, our language models can be interpreted as a regularized versions\n6\n(a) POS tagging baseline: 89.9%\n (b) CCG supertagging baseline: 71.6%\nFigure 4: POS and CCG tagging accuracies in terms of percentage points over the word-conditional\nmost frequent class baseline. We display results for the best performing models for each task.\nof skip-thought, in which the weights of the encoder and decoder are shared. The increased model\ncapacity of skip-thought, compared to language models, could explain the difference in learned\nrepresentation quality—especially when these models are trained on smaller amounts of data.\nRandom Initialization For our randomly initialized, untrained LSTM encoders, we use the de-\nfault weight initialization technique in OpenNMT-py, a uniform distribution between -0.1 and 0.1;\nthe only change we make is to set all biases to zero. We ﬁnd that this baseline performs quite well\nwhen using all auxiliary data, and is only 3 and 8 percentage points behind the BiLM on POS and\nCCG tagging, respectively. We ﬁnd that decreasing the amount of classiﬁer data leads to a signif-\nicantly greater drop in the untrained encoder performance compared to trained models. In the 1%\nclassiﬁer data regime, the performance of untrained encoders on both tasks drops below that of all\ntrained models and below even the word-conditional most-frequent class baseline.\nWe hypothesize that the randomly initialized baseline is able to perform well on tagging tasks with\nlarge amounts of auxiliary task training data, because the classiﬁer can learn the identity of neigh-\nboring words from a given time step’s hidden state, and simply memorize word conﬁgurations and\ntheir associated tags from the training data. We test this hypothesis directly in Section 6 and ﬁnd that\nuntrained LSTM representations are in fact better at capturing neighboring wordidentity information\nthan any trained model.\nAutoencoder Models trained on autoencoding are the only ones that do not consistently improve\nwith the amount of training data, which is unsurprising as unregularized autoencoders are prone to\nlearning identity mappings (Vincent et al., 2008). When training on 10% and 1% of the auxiliary\ntask data, autoencoders outperform randomly initialized encoders and match the word-conditional\nmost frequent class baseline. When training on all the auxiliary data though, untrained encoders\noutperform autoencoders. These results suggest that autoencoders learn some useful structure that\nis useful in the low auxiliary data regime. However, the representations autoencoders learn do not\ncapture syntactically rich features, since random encoders outperform them in the high auxiliary\ndata regime. This conclusion is further supported by the extremely poor performance of the second\nlayer of an autoencoder without attention on POS tagging (almost 10 percentage points below the\nmost frequent class baseline), as seen in Figure 4a.\n5 C OMPARING LAYERS\nEmbeddings (Layer 0) We ﬁnd that randomly initialized embeddings consistently perform as\nwell as the word-conditional most frequent class baseline on POS and CCG tagging, which serves\nas an upper bound on performance for the embedding layer. As these embeddings are untrained, the\nauxiliary classiﬁers are learning to memorize and classify the random vectors. When using all the\nauxiliary classiﬁer data, there is no signiﬁcant difference in the performance of trained and untrained\nembeddings on the tagging tasks. Only for smaller amounts of classiﬁer data do trained embeddings\nconsistently outperform randomly initialized ones.\n7\nFigure 5: Performance of classiﬁers trained to predict the identity of the word a ﬁxed number of\ntimesteps away. Note, the forward LM has asymmetrical access to this information in its input.\nUpper Layers Belinkov et al. (2017a) ﬁnd that, for translation models, the ﬁrst layer consistently\noutperforms the second on POS tagging. We ﬁnd that this pattern holds for all our models, except\nin BiLMs, for which the ﬁrst and second layers perform equivalently. The pattern holds even for\nuntrained models, suggesting that POS information is stored on the lower layer, not necessarily\nbecause the training task encourages this, but because of properties of the deep LSTM architecture.\nWe also ﬁnd that for CCG supertagging, the ﬁrst layer also outperforms the second layer on untrained\nmodels. For the trained models though, the second layer performs better than the ﬁrst in some cases.\nWhich layer performs best appears to be independent of absolute performance on the supertagging\ntask. Our layer analysis results are displayed in Figure 4.\n6 W ORD IDENTITY PREDICTION\nOur results on word identity prediction are summarized in Figure 5 and given in more detail in\nAppendix A. While trained encoders outperform untrained ones on both POS and CCG tagging, we\nﬁnd that all trained LSTMs underperform untrained ones on word identity prediction. This ﬁnding\nconﬁrms that trained encoders genuinely capture substantial syntactic features, beyond mere word\nidentity, that the auxiliary classiﬁers can use.\nWe ﬁnd that for both trained and untrained models, the ﬁrst layer outperforms the second layer\nwhen predicting the identity of the immediate neighbors of a word. However, the second layer\ntends to outperform the ﬁrst at predicting the identity of more distant neighboring words. This\neffect is especially apparent for the randomly initialized encoders. Our ﬁnding suggests that, as\nis the case for convolutional neural networks, depth in recurrent neural networks has the effect\nof increasing the receptive ﬁeld and allows the upper layers to have representations that capture a\nlarger context. These results reﬂect the ﬁndings of Blevins et al. (2018) that for trained models,\nupper levels of LSTMs encode more abstract syntactic information, since more abstract information\ngenerally requires larger context information.\n7 C ONCLUSION\nBy controlling for the genre and quantity of the training data, we make fair comparisons between\nseveral data-rich training tasks in their ability to induce syntactic information. We ﬁnd that bidirec-\ntional language models (BiLMs) do better than translation and skip-thought encoders at extracting\nuseful features for POS tagging and CCG supertagging. Moreover, this improvement holds even\n8\nwhen the BiLMs are trained on substantially less data than competing models. Our results suggest\nthat for transfer learning, BiLMs like ELMo (Peters et al., 2018) capture more useful features than\ntranslation encoders—and that this holds even on genres for which data is not abundant.\nWe also ﬁnd that randomly initialized encoders extract usable features for POS and CCG tagging—\nat least when the auxiliary POS and CCG classiﬁers are themselves trained on reasonably large\namounts of data. The performance of untrained models drops sharply relative to trained ones when\nusing smaller amounts of the classiﬁer data. We investigate further and ﬁnd that untrained models\noutperform trained ones on the task of neighboring word identity prediction, which conﬁrms that\ntrained encoders do not perform well on tagging tasks because the classiﬁers are simply memorizing\nword identity information. We also ﬁnd that both trained and untrained LSTMs store more local\nneighboring word identity information in lower layers and moredistant word identity information in\nupper layers, which suggests that depth in LSTMs allow them to capture larger context information.\nACKNOWLEDGMENTS\nWe thank Nikita Nangia, Alex Wang, Phu Mon Htut, Katharina Kann, Chris Barker, and Sebastian\nGehrmann for feedback on this project and paper. We acknowledge the support of NVIDIA\nCorporation with the donation of the Titan X Pascal GPU used for this research. SB acknowledges\nsupport from Google and Samsung Research.\nREFERENCES\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained Analysis\nof Sentence Embeddings Using Auxiliary Prediction Tasks.ICLR, 2016. URLhttp://arxiv.\norg/abs/1608.04207.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly\nLearning to Align and Translate. ICLR, 2015.\nYonatan Belinkov, Llu´ıs M`arquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass.\nEvaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Se-\nmantic Tagging Tasks. IJCNLP, 2017b. URL https://arxiv.org/abs/1801.07772.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James R. Glass. What do Neural\nMachine Translation Models Learn about Morphology? ACL, 2017a. URL http://arxiv.\norg/abs/1704.03471.\nTerra Blevins, Omer Levy, and Luke Zettlemoyer. Deep RNNs Learn Hierarchical Syntax. ACL,\n2018.\nOndrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias\nHuck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Ne-\ngri, Aurelie Neveol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton,\nLucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. Findings of the 2016 Con-\nference on Machine Translation (WMT16). ACL, 2016.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised\nLearning of Universal Sentence Representations from Natural Language Inference Data. ACL,\n2017.\nAlexis Conneau, Germ`an Kruszewski, Guillaume Lample, Lo ¨ı Barrault, and Marco Baroni. What\nyou can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.\nACL, 2018.\nAndrew M. Dai and Quoc V . Le. Semi-supervised Sequence Learning.NIPS, 2015. URL https:\n//arxiv.org/abs/1511.01432.\n9\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory F. Diamos, Heewoo Jun, Hassan Kian-\ninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep Learning Scaling is Pre-\ndictable, Empirically. arXiv preprint 1712.00409, 2017. URL http://arxiv.org/abs/\n1712.00409.\nSepp Hochreiter and J¨uergen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):\n1735–1780, 1997.\nJulia Hockenmaier and Mark Steedman. CCGbank: A Corpus of CCG Derivations and Dependency\nStructures Extracted from the Penn Treebank. Computational Linguistics, 2007. URL http:\n//www.aclweb.org/anthology/J07-3004.\nJeremy Howard and Sebastian Ruder. Universal Language Model Fine-tuning for Text Classiﬁca-\ntion. ACL, 2018. URL http://arxiv.org/abs/1801.06146.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urta-\nsun, and Sanja Fidler. Skip-Thought Vectors. NIPS, 2015. URL http://arxiv.org/abs/\n1506.06726.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. OpenNMT:\nOpen-Source Toolkit for Neural Machine Translation. ACL, 2017. doi: 10.18653/v1/P17-4012.\nURL https://doi.org/10.18653/v1/P17-4012.\nPhilip Koehn and Rebecca Knowles. Six Challenges for Neural Machine Translation. ACL, 2017.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola\nBertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar,\nAlexandra Constantin, and Evan Herbst. Moses: Open Source Toolkit for Statistical Machine\nTranslation. ACL, 2007. URL http://www.statmt.org/moses/.\nMantas Lukoˇseviˇcius and Herbert Jaeger. Reservoir computing approaches to recurrent neural net-\nwork training. In Computer Science Review, volume 3, pp. 127–149, 2009.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a Large Annotated\nCorpus of English: The Penn Treebank. Computational Linguistics, 1993. URL http://dl.\nacm.org/citation.cfm?id=972470.972475.\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in Translation:\nContextualized Word Vectors. NIPS, 2017.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a Method for Auto-\nmatic Evaluation of Machine Translation. ACL, 2002. URL http://www.aclweb.org/\nanthology/P02-1040.pdf.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. Deep contextualized word representations. NAACL, 2018. URL http:\n//arxiv.org/abs/1802.05365.\nXing Shi, Inkit Padhi, and Kevin Knight. Does String-Based Neural MT Learn Source\nSyntax? EMNLP, 2016. URL https://www.isi.edu/natural-language/mt/\nemnlp16-nmt-grammar.pdf.\nXiaochuan Sun, Tao Li, Qun Li, Yue Huang, and Yingqi Li. Deep belief echo-state network and its\napplication to time series prediction. Knowl.-Based Syst., 130:17–29, 2017.\nMatthew H. Tong, Adam D. Bickett, Eric M. Christiansen, and Garrison W. Cottrell. Learning\ngrammatical structure with echo state networks. Neural Networks, 20(3):424–432, 2007. doi: 10.\n1016/j.neunet.2007.04.013. URL https://doi.org/10.1016/j.neunet.2007.04.\n013.\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep Image Prior. NIPS, 2017.\nDavid Verstraeten, Benjamin Schrauwen, and Dirk Stroobandt. Reservoir-based techniques for\nspeech recognition. In The 2006 IEEE International Joint Conference on Neural Network Pro-\nceedings, pp. 1050–1053, July 2006. doi: 10.1109/IJCNN.2006.246804.\n10\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and\nComposing Robust Features with Denoising Autoencoders. InMachine Learning, Proceedings of\nthe Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008, pp.\n1096–1103, 2008.\n11\nA R ANDOMLY INITIALIZED ENCODERS\nFigure 6: Here we display results for the word identity prediction task with randomly initialized\nLSTM encoders with up to 4 layers. Lower layers have a more peaked shape and upper layers a\nmore ﬂat shape, meaning that the lower layers encode relatively more nearby neighboring word\ninformation, while upper layers encode relatively more distant neighboring word information.\n12\nB POS AND CCG E VALUATION FULL RESULTS\nB.1 T RAINING CLASSIFIERS ON ALL DATA\nTraining task Data Attention POS L2 POS L1 POS L0 CCG L2 CCG L1 CCG L0\nRandom Init 1 None N/A 90.5 93.7 90.2 83.5 85.4 71.6\nRandom Init 2 None N/A 90.3 93.8 90.1 83.3 85.3 71.5\nTranslation 1M Yes 95.6 95.7 90.0 91.4 91.2 71.5\nTranslation 1M No 92.5 95.0 90.0 88.2 90.1 71.3\nLM (Bidir) 1M No 96.4 96.1 90.2 92.5 92.0 71.6\nLM (Forward) 1M No 94.3 94.5 90.1 83.5 83.1 71.5\nSkip-thought 1M Yes 44.3 88.6 89.9 45.3 81.0 71.1\nSkip-thought 1M No 78.1 90.8 89.9 74.5 84.4 71.1\nAutoencoder 1M Yes 80.8 92.4 89.6 73.6 83.7 71.2\nAutoencoder 1M No 79.8 90.8 89.9 79.2 84.0 71.1\nTranslation 5M Yes 96.0 95.9 90.2 92.2 91.6 71.5\nTranslation 5M No 92.9 95.8 90.2 89.6 91.2 71.5\nLM (Bidir) 5M No 96.6 96.2 90.3 92.6 92.4 71.6\nLM (Forward) 5M No 94.6 94.7 90.2 84.0 83.5 71.5\nSkip-thought 5M Yes 76.4 92.2 90.0 68.4 86.4 71.1\nSkip-thought 5M No 86.1 94.3 90.0 81.2 88.6 71.2\nAutoencoder 5M Yes 88.1 91.8 89.6 76.5 82.5 70.8\nAutoencoder 5M No 70.7 92.1 89.8 72.7 83.7 71.0\nLM (Bidir) 15M No 97.0 96.8 90.6 93.1 92.9 72.0\nLM (Forward) 15M No 95.3 95.3 90.6 84.9 84.5 72.0\nSkip-thought 15M Yes 82.3 93.8 90.2 70.4 87.6 71.6\nSkip-thought 15M No 90.1 95.1 90.3 85.8 89.8 71.5\nAutoencoder 15M Yes 91.9 93.1 90.1 82.6 84.5 71.4\nAutoencoder 15M No 71.6 92.0 89.8 71.0 83.7 71.2\nLM (Bidir) 63M No 96.9 96.7 90.6 93.1 93.0 72.0\nLM (Forward) 63M No 95.3 95.4 90.6 84.9 84.5 72.0\nSkip-thought 63M Yes 90.6 95.5 90.3 80.9 90.1 71.6\nSkip-thought 63M No 91.6 95.6 90.3 86.8 90.3 71.6\nAutoencoder 63M Yes 89.4 91.8 89.6 78.4 83.2 71.2\nAutoencoder 63M No 70.2 91.7 89.9 70.5 83.1 71.3\nTable 2: Here we display results for training on all of auxiliary task data. Word-conditional most\nfrequent class baselines for this amount of training data are 89.9% for POS tagging and 71.6% for\nCCG supertagging. For each task, we underline the best performance for each training dataset size\nand bold the best overall performance.\n13\nB.2 T RAINING CLASSIFIERS ON 10% OF DATA\nTraining task Data Attention POS L2 POS L1 POS L0 CCG L2 CCG L1 CCG L0\nRandom Init 1 None N/A 85.0 90.5 88.3 71.8 77.0 68.3\nRandom Init 2 None N/A 84.9 90.6 88.3 72.7 77.0 68.3\nTranslation 1M Yes 93.4 94.3 89.1 88.4 87.6 69.5\nTranslation 1M No 89.9 93.4 89.0 82.9 86.0 69.5\nLM (Bidir) 1M No 95.5 95.2 89.7 89.4 88.6 70.1\nLM Forward 1M No 93.2 93.5 89.5 80.8 80.2 69.9\nSkip-thought 1M Yes 34.3 84.1 88.2 36.7 74.0 68.3\nSkip-thought 1M No 71.3 86.9 88.2 64.9 78.0 68.1\nAutoencoder 1M Yes 77.9 89.6 87.7 71.5 77.4 68.3\nAutoencoder 1M No 71.2 87.9 88.6 71.8 78.1 68.8\nTranslation 5M Yes 94.1 94.8 89.5 88.9 88.2 69.8\nTranslation 5M No 89.2 94.4 89.5 85.4 87.6 69.9\nLM (Bidir) 5M No 95.7 95.3 89.8 89.6 88.9 70.2\nLM Forward 5M No 93.3 93.7 89.7 81.4 80.6 70.1\nSkip-thought 5M Yes 66.8 89.6 88.7 60.8 81.0 68.7\nSkip-thought 5M No 81.2 92.1 88.7 73.4 83.7 68.7\nAutoencoder 5M Yes 84.9 89.0 87.6 71.8 76.1 67.9\nAutoencoder 5M No 65.6 89.6 88.4 65.8 77.9 68.3\nLM (Bidir) 15M No 96.1 95.9 90.2 89.7 89.9 70.6\nLM Forward 15M No 94.1 94.5 90.1 82.1 81.8 70.6\nSkip-thought 15M Yes 72.8 91.4 89.0 63.2 82.6 68.9\nSkip-thought 15M No 84.6 93.2 89.0 79.8 85.5 69.1\nAutoencoder 15M Yes 88.3 90.3 88.4 76.6 78.9 68.7\nAutoencoder 15M No 68.5 89.2 88.3 68.6 78.1 68.6\nLM (Bidir) 63M No 96.1 96.0 90.2 90.0 90.1 70.7\nLM Forward 63M No 94.3 94.4 90.2 82.3 81.8 70.6\nSkip-thought 63M Yes 85.0 94.0 89.2 73.9 86.0 69.4\nSkip-thought 63M No 88.0 94.0 89.3 81.6 86.1 69.3\nAutoencoder 63M Yes 82.8 88.9 87.4 72.7 77.3 68.4\nAutoencoder 63M No 67.2 89.5 88.5 66.1 77.2 68.5\nTable 3: Here we display results for training on 10% of auxiliary task data. Word-conditional most\nfrequent class baselines for this amount of training data are 88.6% for POS tagging and 68.3% for\nCCG supertagging. For each task, we underline the best performance for each training dataset size\nand bold the best overall performance.\n14\nB.3 T RAINING CLASSIFIERS ON 1% OF DATA\nTraining task Data Attn. POS L2 POS L1 POS L0 CCG L2 CCG L1 CCG L0\nRandom Init 1 None N/A 68.7 74.5 79.1 54.4 60.9 59.3\nRandom Init 2 None N/A 68.8 74.5 79.5 55.5 62.0 58.8\nTranslation 1M Yes 90.8 91.7 87.2 79.1 81.0 65.4\nTranslation 1M No 82.5 89.9 86.9 69.0 78.3 65.0\nLM (Bidir) 1M No 93.5 93.8 89.0 82.8 81.6 67.1\nLM Forward 1M No 90.8 91.8 88.5 74.3 74.1 66.5\nSkip-thought 1M Yes 27.2 73.2 81.4 28.7 63.3 60.7\nSkip-thought 1M No 57.8 77.5 81.3 47.4 67.9 61.0\nAutoencoder 1M Yes 71.2 81.4 81.8 59.0 67.4 61.9\nAutoencoder 1M No 62.2 78.7 84.2 60.2 69.4 63.5\nTranslation 5M Yes 92.1 92.9 88.2 77.3 81.2 65.7\nTranslation 5M No 82.7 91.7 88.0 73.5 80.7 65.9\nLM (Bidir) 5M No 93.7 94.0 89.1 83.0 82.4 67.1\nLM Forward 5M No 90.7 92.1 88.8 74.3 74.3 66.7\nSkip-thought 5M Yes 55.3 83.4 84.8 44.5 72.4 63.0\nSkip-thought 5M No 69.6 86.0 84.4 53.5 75.1 62.7\nAutoencoder 5M Yes 67.6 79.5 80.8 58.8 64.6 61.0\nAutoencoder 5M No 60.7 81.1 82.6 56.0 68.7 61.8\nLM (Bidir) 15M No 94.4 94.7 89.6 82.8 83.7 67.5\nLM Forward 15M No 91.7 93.1 89.3 74.8 75.8 67.3\nSkip-thought 15M Yes 50.7 85.4 84.9 29.6 73.8 63.5\nSkip-thought 15M No 75.2 88.1 84.9 63.5 77.4 63.7\nAutoencoder 15M Yes 77.9 82.3 81.9 66.9 68.7 62.6\nAutoencoder 15M No 61.2 80.4 82.3 56.6 69.8 62.0\nLM (Bidir) 63M No 94.3 94.8 89.7 82.9 83.9 67.5\nLM Forward 63M No 92.1 93.3 89.4 74.9 76.2 67.6\nSkip-thought 63M Yes 69.8 90.2 86.3 55.4 78.1 64.4\nSkip-thought 63M No 77.9 89.6 86.1 64.8 78.4 64.0\nAutoencoder 63M Yes 72.1 80.1 81.5 58.7 66.8 61.3\nAutoencoder 63M No 60.6 80.6 82.3 55.7 68.6 61.7\nTable 4: Here we display results for training on 1% of auxiliary task data. Word-conditional most\nfrequent class baselines for this amount of training data are 81.8% for POS tagging and 62.3% for\nCCG supertagging. For each task, we underline the best performance for each training dataset size\nand bold the best overall performance.\n15"
}