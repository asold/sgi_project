{
  "title": "Training Multilingual Pre-trained Language Model with Byte-level Subwords",
  "url": "https://openalex.org/W3124386064",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5041900541",
      "name": "Victor Junqiu Wei",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100426170",
      "name": "Qun Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5011761798",
      "name": "Yinpeng Guo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5086603207",
      "name": "Xin Jiang",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2788760202",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2118434577",
    "https://openalex.org/W2757141711",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2965210982",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2972176762",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2971871542",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963208801",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2787998955",
    "https://openalex.org/W2964002616"
  ],
  "abstract": "The pre-trained language models have achieved great successes in various natural language understanding (NLU) tasks due to its capacity to capture the deep contextualized information in text by pre-training on large-scale corpora. One of the fundamental components in pre-trained language models is the vocabulary, especially for training multilingual models on many different languages. In the technical report, we present our practices on training multilingual pre-trained language models with BBPE: Byte-Level BPE (i.e., Byte Pair Encoding). In the experiment, we adopted the architecture of NEZHA as the underlying pre-trained language model and the results show that NEZHA trained with byte-level subwords consistently outperforms Google multilingual BERT and vanilla NEZHA by a notable margin in several multilingual NLU tasks. We release the source code of our byte-level vocabulary building tools and the multilingual pre-trained language models.",
  "full_text": "TRAINING MULTILINGUAL PRE-TRAINED LANGUAGE MODELS\nWITH BYTE -LEVEL SUBWORDS\nTECHNICAL REPORT\nJunqiu Wei, Qun Liu, Yinpeng Guo, Xin Jiang\nNoah’s Ark Lab, Huawei Technologies\n{weijunqiu, qun.liu, guo.yinpeng, jiang.xin}@huawei.com\nJune 4, 2021\nABSTRACT\nThe pre-trained language models have achieved great successes in various natural language under-\nstanding (NLU) tasks due to its capacity to capture the deep contextualized information in text by\npre-training on large-scale corpora. One of the fundamental components in pre-trained language\nmodels is the vocabulary, especially for training multilingual models on many different languages. In\nthe technical report, we present our practices on training multilingual pre-trained language models\nwith BBPE: Byte-Level BPE (i.e., Byte Pair Encoding). BBPE has been adopted by pretrained\nlanguage models like GPT-2/3 [1, 2] and Roberta [3] and its usage in machine translation has been\ndiscussed in [4]. We compared the byte-level vocabulary with the character-level vocabulary adopted\nin Google’s multilingual BERT model through intensive case studies on the tokenization in a variety\nof languages. In the experiment, we adopted the architecture of NEZHA [ 5] as the underlying\npre-trained language model and the results show that NEZHA trained with byte-level subwords\nconsistently outperforms Google multilingual BERT and vanilla NEZHA by a notable margin in\nseveral multilingual NLU tasks. We release the source code of our byte-level vocabulary building\ntools and the multilingual pre-trained language models at the URLs 12 .\nKeywords Pre-trained Language Models · Tokenization · Multilingual · Byte-Level Subwords\n1 Introduction\nPre-trained language models has demonstrated marvelous success by its excellent performance in a variety of natural\nlanguages understanding (NLU) tasks. In the pretraining phase, it employs language modeling tasks and learns\ncontextualized word representations by utilizing the massive amount of training text. A large body of research\nefforts has been devoted to pre-trained language models such as ELMo [6], BERT [7], ERNIE-Baidu [8, 9], ERNIE-\nTsinghua [10], XLNet [ 11], RoBERTa [3], NEZHA [ 5], ALBERT [12], ELECTRA [ 13] and MegatronLM3. As a\nfundamental technique in natural language processing (NLP), the language models pre-trained on text could be easily\ntransferred to learn downstream NLP tasks with ﬁnetuning, which achieve the state-of-the-art performances on many\ntasks including sentiment analysis, machine reading comprehension, sentence matching, named entity recognition and\nnatural language inference.\nThe multilingual pre-trained language model works as an universal cross-lingual encoder which embeds any sen-\ntence/words into a shared embedding space. The ﬁrst attempt on this topic is Google’s multilingual BERT [7] which\nembeds more than 100 languages and highly improved the performance on low-resource languages. Then, XLM\n(Cross-Lingual Language Pre-training) [14] further incorporates supervised parallel data (with a new cross-lingual\nlanguage model objective) into the multilingual pre-trained language models. In this technical report, we detail our\npractice in training multilingual pre-trained language models with Byte-Level Subwords. A key component in the\n1https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BBPE\n2https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow\n3https://nv-adlr.github.io/MegatronLM\narXiv:2101.09469v2  [cs.CL]  3 Jun 2021\nTECHNICAL REPORT - JUNE 4, 2021\nFigure 1: Subwords Frequencies in Google Multilingual V ocabulary (mBERT) and Our Byte-Level V ocabulary (BBPE)\npre-trained language models, esp. multilingual models, is the vocabulary. We observe that the character-level vocabulary,\ne.g., goolge multilingual BERT vocabulary (mBERT vocab), has two weaknesses. First of all, it has too many rare\nsubwords contained whose representations are hard to learn in deep learning models. As shown in Figure 1, the mBERT\nvocab has around 30% subwords with the frequency less than 100 and roughly 18% subwords which never occur. We\ncalculate the frequencies of the subwords in the Wikipedia datasets with 14 languages as shown in Section 4.4.3 and\nsorted the subwords by its frequency. Note that we only consider the subwords which belong to the 14 languages\nin mBERT vocab. We observe that these rare subwords are mainly rare Chinese characters and rare Latin subwords.\nThese subwords waste the slots of the vocabulary. Secondly, even though mBERT vocab has many rare subwords, it is\nimpossible to avoid the unknown words problem (i.e., [UNK]). By the most up-to-date standard of Unicode4, there\nare more than 14 millions of characters (most of which rarely occur). And thus, it is not possible to include them all\nin a vocabulary which will lead to [UNK] problem in the text containing these non-included characters. For example\n\"\n \" is the name of a famous Chinese writer , however, the character \"\n \" is not in the mBERT vocabulary.\nMotivated by this, we employed a technique, namely Byte-Level Subwords which shows marvelous success in neural\nmachine translation [4], in building the vocabulary for multilingual pre-trained language models. Speciﬁcally, this\ntechnique ﬁrst converts the text into its corresponding UTF-8 codes and then applies a byte-level vocabulary building\nalgorithm on the UTF-8 codes. In this study, we employed Byte Pair Encoding (BPE) as a show case of this underlying\ncharacter-level vocabulary building algorithm and thus, it is called \"Byte-Level BPE\" (BBPE). There are strong\nexperimental evidences [4] showing that building the multilingual vocabulary in a byte-level will largely encourage\nthe sharing of subwords among different languages in a ﬁne-grain, as a result of which, the subwords obtained in the\nvocabulary have much higher frequencies and the learning of the representations of the subwords could be improved.\nAs shown in Figure 1, our BBPE vocab has all its subwords with the frequency more than 10,000 on the Wikipedia\ndatasets of 14 languages. Besides, we thoroughly avoid the unknown words problem (i.e., [UNK]) by including all\nbytes into the vocabulary. It is worth mentioning that there are only 256 bytes in total and thus, the cost of including\nthem all in a vocabulary is negligible. To intuitively give an idea of this Byte-Level Subwords, we demonstrate an\nexample in Figure 2. In this ﬁgure, there are four lines which correspond to the raw text, the UTF-8 encoding of the text,\nthe tokenized UTF-8 encoding of the text and the corresponding text of the tokenized UTF-8 encoding, respectively.\nEach word contains several characters and we convert each character into bytes and preserve the boundary of each\nword (i.e., whitespace). By applying a vocabulary building algorithm (e.g., BPE) to the UTF-8 encoding of the text, we\nobtain a byte-level vocabulary. The tokenization using the vocabulary is shown in the third line and the last line shows\nthe corresponding text of the tokenized UTF-8 encoding. Note that the symbol \"##\" denotes the subword is an trailing\nsubword (i.e., the subword is not in beginning of the word it belongs to). In this work, we handcraft several techniques\nto better make sense of the byte-level subwords learned which will be presented in details in later sections. We also\nprovide some insightful comments on these techniques which shed light on the practice of multilingual model training.\nThe contribution of this technical report is three-folds. First, we detail the deployment of the Byte-Level Subwords in\nthe vocabulary training together with several effective handcrafted techniques integrated for the multilingual pre-trained\nlanguage models. Secondly, we conducted comprehensive case studies on the tokenziation on multiple languages and\ndemonstrate the effectiveness of the BBPE, esp. for the low-resource languages. Thirdly, we conducted experiment\n4https://en.wikipedia.org/wiki/Unicode\n2\nTECHNICAL REPORT - JUNE 4, 2021\nFigure 2: An Example of Tokenization with Byte-Level Subwords\nbased on our previously released architecture NEZHA and our result shows that NEZHA with BBPE consistently\noutperform the vanilla NEZHA and Google multilingual BERT by a notable margin in natural language inference\n(XNLI) task. Fourthly, we open sourced our multilingual pre-trained language models and the BBPE tool.\nThe remainder of this paper is organized as follows. In Section 2, we review the related works of our work. Section 3\nand Section 4 present our proposed byte-level vocabulary building algorithm and its empirical performance in training\nmultilingual pre-trained language models. Finally, Section 5 concludes this paper.\n2 Related Works\nThis section reviews the literature related to our work. Section 2.1, Section 2.2 and Section 2.3 present the existing\nworks on the pre-trained language models and the vocabulary building algorithms, respectively.\n2.1 Unicode and UTF-8 Encoding\nThe unicode 5 is a uniﬁed standard for the consistent encoding, representation and handling of text data. Its success in\nunifying the representation of text renders it widely spread and applied in computer softwares. UTF-8 (8-bit Unicode\nTransformation Format) 6 is one of the most important formats in unicode. And UTF-8 is a variable-length character\nencoding and it encodes characters with 1-4 bytes. It is worth mentioning that UTF-8 was designed for backward\ncompatibility with ASCII. Speciﬁcally, the ﬁrst byte of UTF-8 are encoded using a single byte with the same binary\nvalue as ASCII. As such, each valid ASCII text is valid UTF-8-encoded Unicode as well. And the ﬁrst byte of UTF-8\ndetermines the length of the character encoding. Up to 2021, UTF-8 is by far the most commonly used encoding for the\nWorld Wide Web7.\n2.2 Pre-trained Language Models\nThe pre-trained language models [6, 7, 8, 9, 10, 11, 3, 5, 12, 13] utilizes a massive amount of unsupervized text data in\nthe pre-training phase and thus, the deep contextualized representations of words are obtained. In the ﬁnetuning phase,\nthe representations could be successfully transferred to a wide range of NLP tasks including Text Classiﬁcation, Natural\nLanguage Inference, Machine Comprehension, etc. The multilingual pre-trained language models [7, 14] works as a\nuniversal encoder which applies to many different languages and embedds them into a uniﬁed vector space. Compared\nwith the monolingual models, the multilingual models enjoy the sharing of semantic information among different\nlanguages (esp., the language-agnostic part) and thus, highly boost the performance on low-resource languages.\n2.3 Vocabulary Building Algorithms\nThis section surveys the existing works of vocabulary building which has two categories, namely Character-Level\nVocabularyand Byte-Level Vocabulary. The character-level vocabulary treats each character as the most fundamental\nunit in the vocabulary building and builds the vocabulary on the raw text representation of the corpus directly. But the\nbyte-level vocabulary treats each byte instead as the most fundamental unit instead and build the vocabulary based on\nthe byte representation of the corpus correspondingly.\n5https://en.wikipedia.org/wiki/Unicode\n6https://en.wikipedia.org/wiki/UTF-8\n7https://w3techs.com/technologies/cross/character_encoding/ranking\n3\nTECHNICAL REPORT - JUNE 4, 2021\n2.3.1 Character-Level Vocabulary\nWord Vocabularies.This type of vocabularies were deployed in word-based Neural Machine Translation models in the\nearlier days. The word vocabularies consist of several words and simply treats each word as a token in the tokenization\nprocess [15, 16, 17, 18]. But this method suffers from the unknown or rare words and highly discourages the subword\nsharing among different words and thus, become less popular in deep learning models which is vulnerable to unknown\nor rare words.\nCharacter Vocabularies.Due to the limitations of the word vocabularies, character vocabularies are deployed which\ncontain several characters and treat each character as a token in the procedure of text segmentation [19]. The advantages\nof this type of vocabularies are two folds. First, it has very small size and thus, could reserve much more GPU memory\nfor storing more training samples (i.e., the batch size could be increased). Secondly, it highly encourages the sharing\nof subword information among different words. Despite its advantages, its limitation is also obvious. The character\nvocabularies lead to much longer sequences of tokens after the tokenization which renders the training of the models\nharder.\nSubword Vocabularies. The subword vocabularies consist of several subwords and thus, properly balanced the\nword vocabularies and the character vocabularies and becomes de facto standard of building vocabularies. There are\nthree major subword vocabulary building algorithms, namely Byte Pair Encoding (BPE) [20, 21], Unigram [22] and\nWordPiecein the subword vocabulary building. BPE and WordPiece initialize the vocabulary as a set of characters and\nthen iteratively merges a pair of tokens in the vocabulary and insert the merged pair (i.e., a newly created token) into the\nvocabulary until the vocabulary size reaches a predeﬁned value. Their difference lies on their selection method of the\ntoken pair in each iteration. BPE iteratively replaces the most frequently occurred pair of consecutive characters in\na text with a single character that does not occur in the text. Besides, it maintains a mapping table to link each pair\nreplaced to their corresponding character for the usage of decoding. In the vocabulary training, each character is treated\nas the most fundamental unit. Consider the example as shown in Table 1. The raw text is \"ABABABCABC\" and the\nmost frequent pair of characters is \"AB\". Thus, in the ﬁrst iteration, the pair \"AB\" is replaced with a new character Z\nand \"Z=AB\" is inserted into the mapping table (i.e., vocabulary). Similarly, in the last three iterations, \"ZC\", \"ZZ\",\n\"YY\" are replaced by \"Y\", \"X\" and \"M\", respectively. It is worth mentioning that the whitespace is not allowed to be\nmerged with any other character. This means that the characters merged in any iteration must be within the same word\nand each item in the vocabulary must be a sub-string of a word. WordPiece is the same as BPE except the selection of\nthe character pair in each iteration. WordPiece selects the one which maximizes the likelihood of a given language\nmodel after the merge of the pair. In a word, BPE and WordPiece construct the vocabulary in a bottom-up fashion\nwhich start from character-level vocabulary and iteratively enrich the vocabulary by merging two existing tokens.\nTable 1: An Example of BPE\nText Mapping Table (V ocabulary)\nRaw Text ABABABCABC -\nIteration 1 ZZZCZC Z=AB\nIteration 2 ZZYY Z=AB, Y=ZC\nIteration 3 XYY Z=AB, Y=ZC, X=ZZ\nIteration 4 XM Z=AB, Y=ZC, X=ZZ, M=YY\nConversely, Unigram constructs the vocabulary in a top-down fashion which starts from a set of words/subwords and\nenriches the vocabulary by splitting the existing tokens instead of merging them. Speciﬁcally, it initially maintains a\nset of candidates of tokens (typically, all words in corpus) and then iteratively splits the candidates by a probabilistic\nmodel and insert the split ones into the candidate list until the candidate list reaches a certain size. Several pruning\ntechniques are also incorporated to prune the candidates and boost the building of the vocabulary. BPE and Unigram\nare integrated into the renowned tool, namely SentencePiece library [23]. But the implementation of WordPiece which\nis developed by Google has not been released due to commercial issues and is not available. The subword vocabulary\nwas ﬁrst deployed to the ﬁeld of Neural Machine Translation for training vocabulary [21] and was later on introduced\nto pre-trained language models. BERT [ 7] uses WordPiece as their underlining vocabulary building algorithm and\nERNIE-Baidu, ERNIE-Tsinghua, NEZHA, ELECTRA simply use the vocabulary of BERT released by Google in\ntraining their models. ALBERT [12] and XLNET [11] use Unigram and BPE respectively by using the SentencePiece\nlibrary [23].\n4\nTECHNICAL REPORT - JUNE 4, 2021\n2.3.2 Byte-Level Vocabularies\nAs shown in Figure 2, the second line shows the byte-level text (in UTF-8 codes) of the original sentence shown in\nthe ﬁrst line. Byte-Level vocabularies are built and applied in the converted byte-level text [1, 3, 4]. In the byte-level\nrepresentation, each character in the original text is converted to multiple bytes in the byte-level text and thus, byte-level\nvocabularies allow the sharing of different words in a ﬁner grain (i.e., in a sub-character level), as a result of which, the\nvocabularies obtained have less rare tokens and more high-frequency tokens (e.g., each rare Chinese/Japanese/Korean\ncharacter is tokenized into multiple high-frequency bytes). [ 4] deploys the byte-level BPE on the neural machine\ntranslation systems and GPT-2 /3 [1, 2] and RoBERTa [3] claims their vocabularies are constructed by using BPE over\nbyte-level text. But they did not provide the details of their vocabulary building procedure. A vocabulary consisting\npurely of 256 bytes has also been used in many language and speech tasks such as part-of-speech tagging and named\nentity recognition [24], translation [25], machine reading [26] and speech recognition [27] but these work did not adopt\nBPE in text encoding.\n3 Building Vocabulary with Byte-Level Subwords\nIn this section, we present our practice on building the vocabulary with byte-level subwords in details. Section 3.1\npresents the overall byte-level vocabulary algorithm step by step. Section 3.2 provides the insights on the technical\ndetails involved in our algorithm to give a high level idea. Section 3.3 discusses from the theoretical perspective the\nreasons why byte-level subwords work and when this technique works.\n3.1 Vocabulary Building Algorithm\nIn this section, we present the details of our byte-level vocabulary building algorithm and provide the insights on the\nimportant tricks deployed which sheds light on the practice of building the byte-level subwords. It is worthy mentioning\nthat in this section, we use BPE as our underlining vocabulary building tool on the byte-level text but our algorithm\nis compatible with any other vocabulary building tool (character-level algorithm mentioned in Section 2.3.1) such as\nUnigram, WordPiece etc. (i.e., the BPE mentioned in this section could be replaced with any other character-level\nvocabulary building algorithm in Section 2.3.1 such as WordPiece, etc.). Since we consider the text as a byte sequence,\nwe call the training method Byte-Level Byte Pair Encoding (BBPE). Note that we assume that our algorithm takes the\nraw text as input and thus, the vocabulary building mentioned in this section contains the procedure of converting the\nraw text into byte sequence and applying BPE to the byte-level text. In the remainder of this paper, we refer the term\n\"word\" to a sequence of characters/bytes which is allowed to be merged in BPE (i.e., the sequence has no whitespace\nwithin but has a whitespace before and after itself).\nAlgorithm 1:V ocabulary Building Algorithm with Byte-Level BPE\nData: Raw Multilingual Text Data\nResult: A V ocabulary Containing Byte-Level Subwords\nStep 1: Text Preprocessing;\n• Step 1.1: Insert Whitespace before and after each CJK character and each punctuation;\n• Step 1.2: Convert each character into its corresponding UTF-8 byte sequence except for whitespaces and\nsentence stops. This step converts the raw text into byte-level representation. But we keep whitespaces and\nsentence stops intact which provides the boundary information of each word and sentence, respectively;\n• Step 1.3: Assign ﬁrst byte in each converted byte-level word a \"leading\" label denoting it is the ﬁrst byte of a\nword;\nStep 2: Apply BPE to the byte-level text;\n• This step applies BPE as mentioned in Section 2.3.1 to the byte-level text given by Step 3 and obtain a\nvocabulary containing byte-level subwords.\nStep 3: Vocabulary Postprocessing;\n• Step 3.1: Insert all bytes (totally 256) into vocabulary if they are not in the vocabulary;\n• Step 3.2: Insert all bytes (totally 256) each of which is assigned a \"leading\" label into vocabulary if they are\nnot in the vocabulary;\n• Step 3.3: Insert \"##\" before each trailing subword and remove the labels added in Step 1.3 and Step 3.2.\n5\nTECHNICAL REPORT - JUNE 4, 2021\nWe proceed to present the step-by-step description of the algorithm which consists of the following three steps as shown\nin Algorithm 1. Step 1 preprocesses the raw input data and converts it into its corresponding byte-level representation. It\ninvolvs three sub-steps which (1) insert whitespaces before and after each CJK (Chinese, Japanese or Korean) character\nand each punctuation, (2) covert each character into its UTF-8 byte codes (except for whitespaces and sentence stops)\n(3) assign a label to each leading byte (as shown in Step 1.1, Step 1.2 and Step 1.3). Step 2 applies BPE to the byte-level\ntext and ﬁnally obtains a vocabulary containing byte-level subwords. Step 3 postprocesses the vocabulary to make\nsure (1) the vocabulary has 256 single-byte leading subwords and 256 single-byte trailing subwords (2) each trailing\nsubword has the symbol ’##’ in its beginning.\n3.2 Technical Details for Building A Byte-Level Vocabulary\nIn this section, we present three technical details deployed in our BBPE algorithm and provide the insights on each one\nas follows.\n3.2.1 Basic Units: Bytes\nTreat each byte as the basic unit.The UTF-8 encoding represents each unicode character with 1-4 bytes. Following [4],\nin our algorithm, we consider the UTF-8 encoding of the text and the basic unit used is byte. This decomposes each\ncharacter into multiple pieces (i.e., bytes) and thus, allows all words to share the subwords in a ﬁner level (i.e., up to\nsub-character level) and also have more chances of sharing. Besides, it decomposes the rare words/characters (esp.\ncharacters) into several bytes with a fairly high frequencies so that pre-trained language models could learn their\ncontextualized representations well enough. We will see in Section 4.2 through case studies that even we split each\ncharacter into bytes, the unicode of most character could be merged as a subword in our BBPE vocabulary. And the\nunicode of each rare character is split into multiple subwords in BBPE. On the other hand, we do not break each byte\ninto more detailed pieces (i.e., bits) since byte-level is ﬁne enough (note that there are only 256 distinct bytes).\nVocabulary must contain all single-byte subwords (in both leading and trailing versions mentioned in Sec-\ntion 3.2.3).Following [4], in case that the corpus in the pre-training or ﬁnetuning phase contains unseen words/characters\nthat never occur in the corpus for training vocabulary, we make sure that the vocabulary contains all single-byte char-\nacters (in both versions aforementioned and thus, there are totally 512 single-byte subwords in our vocabulary). As\na result, we could always decompose any given sequence of a UTF-8 characters into a sequence of tokens contained\nin our vocabulary (i.e., there will be no unknown tokens in the tokenization results with our vocabulary). This trick\ncorresponds to Step 3.1 and Step 3.2 which ensure the vocabulary contains all single-byte subwords in both version and\nas such, the unknown token problem will be avoided.\n3.2.2 Maximum Units: Words, CJK Characters and Punctuations\nThe UTF-8 encoding of each word is treated as a maximum unit in BBPE. Consider a word in the original text\nwhich has a whitespace before and after itself. We keep the boundary of the word intact (i.e., whitespaces) when\nconverting the raw text into UTF-8 codes. The same as the vanilla BPE, we do not allow the cross-word merges\nand assume that word is the largest token to be used, as a result of which, it reduces the occurrences of long tokens\nwhich normally has low frequencies. The languages such as Latin (including English, French, Italic) and Slavic\n(including Russian, Polish) naturally has the boundary for each word and we preserve the word boundary (i.e., denoted\nby whitespace) in the data preprocessing. This trick corresponds to Step 1.2 which preserves the word boundary in the\nraw text. This trick was also deployed in [4] in the byte-level neural machine translation.\nThe UTF-8 encoding of each CJK (Chinese, Japanese, Korean) character and each punctuation is also treated\nas a maximum unit.Different from Latin languages, the raw text of CJK does not have the information of the\nboundaries of words and each sentence in CJK is simply a sequence of characters (without whitespace contained).\nConsider that there are ambiguities on the segmentation (i.e., tokenization) of CJK text and a poor segmentation on the\ntext could lead to signiﬁcant performance loss on many downstream tasks, esp. Named Entity Recognition (NER), in\npre-trained language models such as BERT [7], RoBERTa [3], etc., it is a common practice that each CJK character is\ntreated as a token. We also follow this practice and simply treat each CJK character as a word in the vocabulary training\nand safely let the pre-training language models learn the word-level/phrase-level information (i.e., the dependencies\namong characters) with the attention mechanism. Besides, in the non-CJK text, each number such as “233000”, “2019”\nis a word (i.e., surrounded by two whitespaces before and after it) but in the CJK text, each number is not separated\nwith other characters. Thus, another beneﬁt of this trick is to make each number in CJK text separated from other\ntext and treated as a word. The trick corresponds to Step 1.1 which ensures that the bytes inside each CJK character\n(punctuation) will not be merged with any byte outside this CJK character (punctuation).\n6\nTECHNICAL REPORT - JUNE 4, 2021\nFigure 3: Case Study on English\n3.2.3 Distinguish Trailing Subwords from Leading Subwords\nConsider a subword sequence {\"whats\", \"app\"}. Its original text has two possibilities: \"whats app\" and \"whatsapp\" if\nwe do not distinguish a trailing subword and a leading subword with the same spelling. Thus, we observe that although\n\"app\" and \"##app\" have the same spelling, their semantic information is different. Besides, another beneﬁt of this\npractice is to make tokenization lossless. This means that after a sentence is tokenized into several tokens, we could\nrecover the original sentence by using the symbol (\"##\") denoting an trailing subword. Motivated by this, we distinguish\nthe two cases in our algorithm. It is worthy mentioning that this is a commonly used technique in Neural Machine\nTranslation [21, 4] which we also deploy in this report. In Section 4.4.2, we also conduct ablation study on the BBPE\nwithout using this technique. This technical detail corresponds to Step 1.3 which ensures that we could distinguish a\ntrailing subword and a leading subword with the same spelling. We will show in the experiment that this technique\nis highly effective in training multilingual pre-trained language models which implies that a trailing subword and a\nleading subword have different semantic meanings.\n3.3 Discussion on Why and When Byte-Level Subwords Work\nIt is a common sense that deep neural networks for natural language processing, esp. the pre-trained language models\nwhich has millions or even trillions of parameters, are vulnerable to rare and unknown words. This is because the\nscarcity of the rare words renders the learning of their representations quite hard in deep neural networks since they\nare rarely exposed to the model in the training procedure and waste quite a lot of slots in the vocabulary. And the\nunknown words are simply treated as a special token such as ’[UNK]’ without further distinguishing the spelling. This\nproblem is especially severe in character-rich languages such as Thai, Arabic, Japanese, etc. Representing the text in\nbyte-level is an effective solution since each original character is converted into 1-4 bytes and thus, the sub-character\nlevel sharing among different words or original characters are possible. With the byte-level subwords, one original rare\nor unknown character could be split into several frequent bytes and equivalently speaking, the slots of the rare words in\nthe vocabulary could be freed for storing more frequently used and more meaningful symbols. Thus, the problem of\nrare/unknown tokens is largely mitigated. And the byte representation is language agnostic and even allows the sharing\nbetween languages without any overlap on their character sets.\nBy the above discussion, we could obtain that the byte-level subwords works in the scenarios where (1) there are rare\ncharacters in the character-level text (esp. character-rich languages such as Thai, Arabic, Japanese, etc.) (2) the rare\ncharacter has more than 1 bytes in the byte representation (so that it could be decomposed into more than one bytes\nwhich occur frequently in the training corpus). As we will present in the experiment, byte-level subwords get marvelous\nsuccess in the languages whose character corresponds to multiple bytes and have a large number of characters but\nobtain almost ignorable improvement on Latin languages (since Latin languages has very limited number of characters\nand the UTF-8 unicode of most character in Latin have only 1 byte) compared with character-level subwords. This\nresult conﬁrms our discussion in this section.\nFinally, it is worth mentioning that in the BBPE vocabulary, it is possible that a byte-level token has no corresponding\noriginal text (e.g., a sub-string of a UTF-8 unicode of a Chinese character which contains more than 1 bytes). This\nwill not lead to any problem in the natural language understanding as studied in this paper. But in the natural language\ngeneration tasks, it may result in infeasible word in the decoding procedure. But this problem could be highly alleviated\nby many effective solutions such as data augmentation which reduces the probability of generating infeasible words.\n7\nTECHNICAL REPORT - JUNE 4, 2021\nFigure 4: Case Study on Thai\nFigure 5: Case Study on Russian\n4 Experiments\nIn this section, we report the experimental results of our methods. The remainder of this section is organized as\nfollows. Section 4.1 provides the information of our experimental settings. Section 4.2 presents the case studies (i.e.,\nthe tokenization) on many different languages by using our vocabulary and comparison with the vocabularies used in\nother works. Section 4.3 presents the experimental results on the multilingual XNLI tasks. Finally, Section4.4 further\nstudies the effect of several variants of our vocabulary building algorithm and also presents the empirical study on the\nexperiments with more languages.\n4.1 Experimental Setting\nPre-training Details We train the NEZHA models on 10 servers on Huawei Cloud 8, each of which has 8 NVIDIA\nTesla V100 GPUs with 32GB memory. The distributed training algorithm is theRing-AllReduce9 and was employed\nwith the framework named Horovod [28]. We trained each model from scratch and pre-train for 80k steps. The batch\nsize on each GPU is 64, and thus the total batch size is 64 * 8 * 10 = 5120. For the each model tested, we set the\nmaximum learning rate to be 1e − 4 (with 1800 warm-up steps and polynomial decay). In addition, we adopted the\nmixed-precision training using FP16 [ 29] in the pre-training phase. We employ wikipedia datasets containing 11\n8https://www.huaweicloud.com/product/modelarts.html\n9https://github.com/baidu-research/baidu-allreduce\nFigure 6: Case Study on Arabic\n8\nTECHNICAL REPORT - JUNE 4, 2021\nFigure 7: Comparison between Google V ocabulary and BBPE V ocabulary\nlanguages including English, Arabic, Spanish, Thai, Russian, German, Italian, French, Malay, Portuguese and Polish\nand upsample the low-resource languages to make the dataset more balanced.\n4.2 Case Studies On Tokenization Results and Vocabulary Analysis\nFigure 3-Figure 6 demonstrates the tokenization of many different languages by using our vocabulary and that used\nin Google multilingual BERT. Our BBPE vocabulary is built by using wikipedia datasets containing 11 languages\nincluding English, Arabic, Spanish, Thai, Russian, German, Italian, French, Malay, Portuguese and Polish and upsample\nthe low-resource languages to make the dataset more balanced. Each ﬁgure has four lines which corresponds to raw\ntext, the UTF-8 encoding of the raw text converted by the aforementioned 6 steps, the tokenization result on the UTF-8\nencoding, the corresponding text of the tokenized UTF-8 encoding and the tokenization result on the raw text by using\nGoogle’s vocabulary. From Figure 3, Figure 4 and Figure 6, we observe that for the same sentence in English, Thai or\nArabic, the output of our tokenizer has less tokens and this implies that our deployed BBPE enjoys better sharing of\nsubwords.\nFigure 7 shows the language-wise comparison between Google multilingual vocabulary and our BBPE vocabulary.\nEach bar in the ﬁgure shows the relative difference of the tokens in the corresponding language (i.e., (A - B) / A, where\nA (B resp.) is the number of tokens in the language in BBPE (Google multilingual resp.) vocabulary). As observed from\nthe ﬁgure, BBPE vocabulary has more tokens in Spanish, Arabic, Malay and Thai and this is the reason why our BBPE\nvocabulary gives less tokens in the tokenization of Arabic and Thai compared with Google multilingual vocabulary.\nAnd we also observe that although BBPE vocabulary has less tokens in English and Russian, it has almost the same\ntokenization result on the two languages as that of Google vocabulary. This implies that our BBPE vocabulary removes\nredundant tokens (esp., rare words/subwords) in languages like English, Russian, French etc. and reserves the tokens for\nlow-resource languages like Arabic and Thai. Consider again the Chinese word \"\n \" mentioned in Section 1 which\n9\nTECHNICAL REPORT - JUNE 4, 2021\nFigure 8: Comparison between Google V ocabulary and BBPE V ocabulary (on 14 Languages)\nis the name of a famous Chinese writer , however, the character \"\n \" is not in the mBERT vocabulary and will be treated\nas \"[UNK]\" (i.e., unknown token). But with our BBPE vocabulary, the UTF-8 encoding of the character (i.e., E8A992)\nwill be tokenized into \"E8\", \"##A9\" and \"##92\" and the unknown word problem is properly avoided. Consider another\nexample \"\n \" which is one sentence from an ancient Chinese poem. The tokenization result\nof mBERT is \"\n \", in which the word \"\n \" is not in its vocabulary and treated as\n\"[UNK]\". But the tokenization result of our BBPE is \"E585B0 E58FB6 E698A5 E891 ##B3 E895 ##A4 EFBC8C\nE6A182 E58D8E E7A78B E79A ##8E E6B481 E38082\" where the word \"\n \" is tokenized into two subwords \"E895\"\nand \"##A4\" and the \"[UNK]\" problem is properly circumvented again. Besides, \"\n \" and \"\n \" are two rare Chinese\ncharacters but they are included in the Google mBERT vocabulary which occupy two entries of the vocabulary and\nwastes the space. Our BBPE vocabulary does not contain the two rare characters which reserves the space of vocabulary\nfor more frequent subwords and each of the two characters is split into two subwords with higher frequencies in the\ntokenization.\n4.3 Experimental Results\nWe tested our methods and several baselines on Cross-Lingual Natual Language Inference (XNLI) task on seven\nlanguages, namely English (En), Arabic (Ar), German (De), Spanish (Es), French (Fr), Russian (Ru) and Thai (Th). We\ntrained two BBPE-based vocabularies with different sizes 50k and 100k, respectively and trained NEZHA with the two\nbyte-level vocabularies. And we compared with Google’s multilingual BERT, vanilla NEZHA (trained with the same\nvocabulary as Google’s multilingual BERT) and NEZHA trained with original BPE-based vocabulary. The result is as\nshown in Table 2. As the table shows, NEZHA (BBPE) with 100k-size vocabulary consistently outperforms BERT\n(Google), vinilla NEZHA and NEZHA (BPE) on the seven languages tested by a notable margin. The improvements\n10\nTECHNICAL REPORT - JUNE 4, 2021\nof our NEZHA (BBPE) over BERT (Google) and NEZHA (BPE) are more signiﬁcant on Ar, Ru and Th which are\nlow-resource languages and this experiment results demonstrate that these languages enjoys the byte-level subword\nsharing. And this results are also consistent with the case studies of tokenization as shown in Figure 4.2 (our BBPE\ngives less tokens in the tokenization results on these languages compared with those by using Google multilingual\nvocabulary). The improvement brought by BBPE on the Latin languages such as English is not very signiﬁcant since\nmost Latin character is single-byte character and there is no difference between character-level and byte-level. Besides,\nNEZHA (BBPE) which contains only 50k subwords in its vocabulary (i.e., less than or equal to half of the Google’s\nvocabulary and that of NEZHA (BPE)) outperforms BERT (Google) and NEZHA (BPE) and its performance is very\nclose to vinilla NEZHA.\nTable 2: Results on XNLI\nModel En Ar De Es Fr Ru Th Avg. V ocab\nBERT (google) 82.4 72.0 76.1 78.4 76.9 74.5 67.1 75.3 120k\nNEZHA 83.0 75.9 79.1 81.1 79.3 76.3 68.8 77.6 120k\nNEZHA (BPE) 81.2 72.7 77.0 78.8 79.0 73.8 67.3 77.0 100k\nNEZHA (BBPE) 83.2 73.9 79.8 81.5 80.8 77.1 71.5 78.3 100k\nNEZHA (BBPE) 82.6 74.1 78.3 80.5 79.6 76.1 69.6 77.3 50k\n4.4 Abalation Study\n4.4.1 Comparison between BBPE and BUnigram\nIn this section, we empirically compare two different word segmentation algorithms on the segmentation/tokenization of\nbyte-level text. One is BPE which we adopted in our experiment and the other one is Unigram [22]. The two algorithms\nare commonly used for word segmentation and are integrated in a renowned tool, namely SentencePiece 10. Since they\nboth handle byte-level text, we call them BBPE and BUnigram, respectively. It is worth mentioning that BUnigram\nis space-consuming and could not affort to process all multilingual corpora. Thus, for each of the 11 languages we\nadopted for the pre-training, we sampled a subset of it and constructed a small version of the pre-training corpora. The\nresult is as shown in Table 3. From the result, we could observe that BBPE consistently ourperforms BUnigram on the\nseven languages tested.\nTable 3: Comparison between BBPE and BUnigram (Based on a Small Version of Data)\nModel En Ar De Es Fr Ru Th V ocab\nNEZHA (wiki sample, BBPE) 80.3 67.6 75.3 78.1 76.6 72.6 69.5 100k\nNEZHA (wiki sample, BUnigram) 78.5 60.6 72.4 76.6 76.2 62.3 53.7 100k\n4.4.2 Comparison Between BBPE and That without Distinguishing A Leading and A Trialing Subword with\nSame Spelling\nIn this section, we compare the experimental results on BBPE without using the technique in Section 3.2.3 (we refer this\nversion as BBPE, w/o D in short) and our original BBPE presented before. We consider three different implementations\nof BBPE, w/o D which are shown as follows.\n• (BBPE, w/o D, WWB (Without Word Boundary)): in the building of vocabulary, we do not employ the\ntechnique in Section 3.2.3.\n• (BBPE, w/o D, WBL (Word Boundary Label)): in the building of vocabulary, we do not employ the technique\nin Section 3.2.3 but in the tokenization, we add a special character \"##\" before each trailing subword. As a\nresult, each trailing subword will have a different context compared with the leading subword with the same\nspelling.\n• (BBPE, w/o D, STE (Subword Type Embedding)): in the building of vocabulary, we do not employ the\ntechnique in Section 3.2.3 but assign a trainable subword type embedding for each type of subword which is\nadded with word embedding in the input of the model. In this work, we consider 7 different subword types:\ntrailing subword, leading subword, ending subword, whole word, \"[SEP]\", \"[CLS]\" and others.\n10https://github.com/google/sentencepiece\n11\nTECHNICAL REPORT - JUNE 4, 2021\nTable 4 demonstrates the experimental results. The above three methods have worse performances compared with\nour original BBPE method. This implies that the explicit information of leading/trailing subwords in the vocabulary\ncontains substantial semantic information and has impact on the performance. It is notable that the BBPE, w/o D, WBL\nleads to signiﬁcant performance drop. This is because the additional character \"##\" introduced make the sentence longer\nand the introduced characters are not informative for distinguishing leading and trailing subwords as expected.\nTable 4: Comparison with BBPE without using the technique in Section 3.2.3\nModel En Ar De Es Fr Ru Th Avg. V ocab\nNEZHA (BBPE) 83.21 73.95 79.80 81.50 80.78 77.09 71.54 78.3 100k\nNEZHA (BBPE, w/o D, WWB) 82.48 72.51 78.96 80.10 80.32 77.27 72.63 77.75 100k\nNEZHA (BBPE, w/o D, WBL) 79.56 65.07 77.69 78.04 76.39 72.79 69.90 74.21 100k\nNEZHA (BBPE, w/o D, STE) 82.93 72.95 79.13 80.54 80.44 76.83 73.95 77.94 100k\n4.4.3 Experiment with More Languages (Chinese, Korean and Japanese)\nWe employ wikipedia datasets of Chinese, Korean and Japanese together with the 11 languages mentioned before to and\nalso use the upsampling technique to make the dataset more balanced. Since there are no Korean and Japanese XNLI\ntasks, we only add Chinses (Zh) XNLI tasks in the downstream tasks evaluation. The result is as shown in Figure 5.\nFrom Table 2, we observe that the improvements of NEZHA (BBPE) over others on De, Th and Zh are still notable.\nBut we observe that NEZHA (BBPE) trained on 14 languages has notable performance drop on Ar, Ru compared with\nNEZHA (BBPE) trained on 11 languages as shown in Table 1. This is because the added three languages (Chinese,\nKorean and Japanese) have less sharing with Ar and Ru but consume several tokens in the vocabulary and this makes\nthe tokens in Ar and Ru less.\nTable 5: Results on XNLI (14 Pre-trained Languages)\nModel En Ar De Es Fr Ru Th Zh Avg. V ocab\nBERT (google) 82.4 72.0 76.1 78.4 76.9 74.5 67.1 76.6 75.5 120k\nNEZHA 82.78 75.9 79.1 81.1 79.3 76.3 68.8 76.8 77.5 120k\nNEZHA (BPE) 80.7 70.4 77.5 78.3 78.6 72.9 67.5 77.2 75.4 100k\nNEZHA (BBPE) 82.4 72.1 79.6 80.2 79.3 75.1 70.2 77.8 77.1 100k\nWe also compare the BBPE vocabulary on 14 languages with Google multilingual vocabulary. Figure 8 shows the\nlanguage-wise comparison between Google multilingual vocabulary and our BBPE vocabulary. Each bar in the ﬁgure\nshows the relative difference of the tokens in the corresponding language (i.e., (A - B) / A, where A (B resp.) is\nthe number of tokens in the language in BBPE (Google multilingual resp.) vocabulary). We observe that BBPE\nvocabulary has more tokens in Spanish, Arabic, Portuguese, Malay and Thai and less tokens in other languages\ncompared with Google multilingual vocabulary. This implies that our BBPE vocabulary removes redundant tokens\n(esp., rare words/subwords) in languages like English, Russian, French etc. (esp. Japanese, Korean and Chinese which\ncontains many rare characters/words) and reserves the tokens for low-resource languages like Arabic, Malay and Thai.\n5 Conclusion\nIn this technical report, we deployed the byte-level byte pair encoding (BBPE) in vocabulary building for the multilingual\npre-trained language models. We detail several important techniques in developing byte-level vocabulary and provides\ninsights on its practice. Our experiment shows that our method signiﬁcantly outperforms several baselines which trains\nthe models in text-level only (including Google’s multilingual BERT, vanilla NEZHA and NEZHA with text-level BPE)\nand veriﬁed the effect of byte-level techniques that we deploy.\nReferences\n[1] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. OpenAI Blog, 2019.\n[2] TB Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell,\net al. Language models are few-shot learners. arxiv 2020. arXiv preprint arXiv:2005.14165, 4.\n12\nTECHNICAL REPORT - JUNE 4, 2021\n[3] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. 2019.\n[4] Changhan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-level subwords. arXiv\npreprint arXiv:1909.03341, 2019.\n[5] Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao\nChen, and Qun Liu. Nezha: Neural contextualized representation for chinese language understanding. arXiv\npreprint arXiv:1909.00204, 2019.\n[6] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227–2237, 2018.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186, 2019.\n[8] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian,\nand Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223,\n2019.\n[9] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. Ernie 2.0: A continual\npre-training framework for language understanding. arXiv preprint arXiv:1907.12412, 2019.\n[10] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced language\nrepresentation with informative entities. arXiv preprint arXiv:1905.07129, 2019.\n[11] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.\n[12] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A\nlite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\n[13] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as\ndiscriminators rather than generators. In International Conference on Learning Representations, 2019.\n[14] Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Advances in Neural\nInformation Processing Systems, pages 7057–7067, 2019.\n[15] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Proceedings of the 2013\nConference on Empirical Methods in Natural Language Processing, pages 1700–1709, 2013.\n[16] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align\nand translate. In 3rd International Conference on Learning Representations, ICLR 2015, 2015.\n[17] Minh-Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba. Addressing the rare\nword problem in neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume\n1: Long Papers), pages 11–19, 2015.\n[18] Sebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary\nfor neural machine translation. In 53rd Annual Meeting of the Association for Computational Linguistics and the\n7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language\nProcessing, ACL-IJCNLP 2015, pages 1–10. Association for Computational Linguistics (ACL), 2015.\n[19] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling\nwith deeper self-attention. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages\n3159–3166, 2019.\n[20] Philip Gage. A new algorithm for data compression. In The C Users Journal, 1994.\n[21] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units.\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1715–1725, 2016.\n[22] Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword\ncandidates. arXiv preprint arXiv:1804.10959, 2018.\n[23] Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, pages 66–71, Brussels, Belgium, November 2018. Association for\nComputational Linguistics.\n13\nTECHNICAL REPORT - JUNE 4, 2021\n[24] Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag Subramanya. Multilingual language processing from bytes.\nIn Proceedings of NAACL-HLT, pages 1296–1306, 2016.\n[25] Marta Ruiz Costa-Jussà, Carlos Escolano Peinado, and José Adrián Rodríguez Fonollosa. Byte-based neural\nmachine translation. In Proceedings of the First Workshop on Subword and Character Level Models in NLP ,\npages 154–158. Association for Computational Linguistics, 2017.\n[26] Tom Kenter, Llion Jones, and Daniel Hewlett. Byte-level machine reading across morphologically varied languages.\nIn AAAI, 2018.\n[27] Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, and William Chan. Bytes are all you need: End-to-end multilingual\nspeech recognition and synthesis with bytes. In ICASSP 2019-2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 5621–5625. IEEE, 2019.\n[28] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in TensorFlow. arXiv\npreprint arXiv:1802.05799, 2018.\n[29] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Gins-\nburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint\narXiv:1710.03740, 2017.\n14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8454841375350952
    },
    {
      "name": "Byte",
      "score": 0.7220650315284729
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.7064337134361267
    },
    {
      "name": "Natural language processing",
      "score": 0.6748625040054321
    },
    {
      "name": "Language model",
      "score": 0.6555509567260742
    },
    {
      "name": "Vocabulary",
      "score": 0.6341484785079956
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5883861780166626
    },
    {
      "name": "Natural language understanding",
      "score": 0.4835231304168701
    },
    {
      "name": "Machine translation",
      "score": 0.43433523178100586
    },
    {
      "name": "Natural language",
      "score": 0.4213302433490753
    },
    {
      "name": "Linguistics",
      "score": 0.18541425466537476
    },
    {
      "name": "Machine learning",
      "score": 0.15382811427116394
    },
    {
      "name": "Programming language",
      "score": 0.15236437320709229
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}