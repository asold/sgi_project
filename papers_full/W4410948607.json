{
  "title": "From Conventional Methods to Large Language Models: A Systematic Review of Techniques in Mobile App Review Analysis",
  "url": "https://openalex.org/W4410948607",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5081204889",
      "name": "Nimasha Arambepola",
      "affiliations": [
        "University of Kelaniya"
      ]
    },
    {
      "id": "https://openalex.org/A5117172155",
      "name": "Waruni Wimalasena",
      "affiliations": [
        "University of Kelaniya"
      ]
    },
    {
      "id": "https://openalex.org/A5083699400",
      "name": "Lankeshwara Munasinghe",
      "affiliations": [
        "Robert Gordon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4313528596",
    "https://openalex.org/W2911025356",
    "https://openalex.org/W4394995597",
    "https://openalex.org/W3216278056",
    "https://openalex.org/W4403786608",
    "https://openalex.org/W4396813716",
    "https://openalex.org/W4296087229",
    "https://openalex.org/W4311808576",
    "https://openalex.org/W2112143630",
    "https://openalex.org/W3162893202",
    "https://openalex.org/W4386161683",
    "https://openalex.org/W2601548810",
    "https://openalex.org/W4312371564",
    "https://openalex.org/W4318562101",
    "https://openalex.org/W4366828587",
    "https://openalex.org/W4384827897",
    "https://openalex.org/W4399702760",
    "https://openalex.org/W2547513165",
    "https://openalex.org/W2249123738",
    "https://openalex.org/W2794720137",
    "https://openalex.org/W3159506990",
    "https://openalex.org/W2549738163",
    "https://openalex.org/W4318003013",
    "https://openalex.org/W2074975950",
    "https://openalex.org/W4304587410",
    "https://openalex.org/W3011717062",
    "https://openalex.org/W4391883788",
    "https://openalex.org/W6873178854",
    "https://openalex.org/W2754222522",
    "https://openalex.org/W3201649100",
    "https://openalex.org/W2895905752",
    "https://openalex.org/W4394769228",
    "https://openalex.org/W2797779452",
    "https://openalex.org/W2362569215",
    "https://openalex.org/W4200184183",
    "https://openalex.org/W2766358679",
    "https://openalex.org/W4386730372",
    "https://openalex.org/W4289596333",
    "https://openalex.org/W4391183982",
    "https://openalex.org/W3020829086",
    "https://openalex.org/W2122839539",
    "https://openalex.org/W2613577477",
    "https://openalex.org/W4391071102",
    "https://openalex.org/W4392822596",
    "https://openalex.org/W4210607630",
    "https://openalex.org/W4403746906",
    "https://openalex.org/W3089199879",
    "https://openalex.org/W2800889653",
    "https://openalex.org/W4399539690",
    "https://openalex.org/W2389246322",
    "https://openalex.org/W3013341859",
    "https://openalex.org/W4387561129",
    "https://openalex.org/W3154560989",
    "https://openalex.org/W3157275317",
    "https://openalex.org/W2513402967",
    "https://openalex.org/W4401042286",
    "https://openalex.org/W4405396166"
  ],
  "abstract": "Aim/Purpose: This paper focuses on app review analysis techniques, driven by the rapid advancement of the mobile app market and NLP techniques in optimizing mobile app user experiences. Background: Owing to technological advancements, app review analysis has rapidly evolved. This study examines both conventional and emerging techniques, including current advancements such as large language models (LLMs) in app review analysis. It provides an overview of the various methods used across different categories of app review analysis, comparing effective strategies for identifying user concerns and enhancing app functionality. Methodology: A systematic review was utilized based on two major standard guidelines, PRISMA and Kitchenham’s guidelines, for the period of 2014 to 2024. After defining the review protocol, papers were identified through keyword-based searches on six major online databases: Scopus, Web of Science, IEEE Xplore, ACM Digital Library, Science Direct, and Springer. Following screening and excluding papers based on defined quality criteria, 53 papers were considered for this study. The use of PRISMA ensures a transparent and reproducible review process, while Kitchenham’s guidelines provide a structured and rigorous approach for evaluating and synthesizing the literature. Contribution: This review study aims to evaluate the current state of knowledge on app review analysis techniques to improve mobile app user experiences. This study categorized the existing state-of-the-art papers into eight different categories, such as sentiment analysis, review classification, summarization, and prioritization, and examined challenges related to app review analysis. Furthermore, the study emphasizes the potential of LLMs for optimizing and automating app review analysis and provides future directions to address gaps in user-centric app development. Findings: Among the eight main categories defined in app review analysis, sentiment analysis is the most prevalent, followed by review classification and information extraction. Most studies use a combination of these categories to achieve a comprehensive goal. Prioritization techniques such as risk matrices, thumbs-up count-based approach, and anomaly detection are widely used to identify emerging issues. Extracting meaningful information and evaluating the proposed approach are the most common challenges identified. Novel LLMs, like Chat-GPT, significantly enhance review analysis by automating the process, improving feature extraction, and enabling context-aware review classification. Recommendations for Practitioners: The combination of conventional approaches and novel LLM-based methods can enhance both the efficiency and accuracy in identifying and addressing critical issues raised through mobile app user reviews. It effectively prioritizes user concerns by leveraging the strengths of both traditional preprocessing techniques and advanced LLMs. Recommendation for Researchers: Researchers are encouraged to explore the integration of emerging technologies like LLMs to enhance the of app review analysis, particularly in feature-specific sentiment analysis. Impact on Society: The results of this study contribute to enhancing the mobile app user experience through effective app review analysis, which improves user satisfaction and supports user-centered app development. This ultimately leads to a better mobile app ecosystem, benefiting both users and developers. Future Research: In the future, this research can be extended in multiple directions. Researchers can address the existing research gaps that LLMs have yet to address, particularly in prioritizing user concerns. Additionally, there is potential for further research on tool implementations focusing on identifying persistent issues through time series analysis by considering the app version and date of the app reviews. Moreover, there is a need to develop comprehensive frameworks that are more generalizable across different apps and categories, with a focus on identifying user concerns related to specific features.",
  "full_text": " \nVolume 20, 2025 \nAccepting Editor Ahmad Samed Al-Adwan│Received: November 20, 2024│ Revised: March 5, March 27, \nApril 14, 2025 │ Accepted: April 15, 2025.  \nCite as: Arambepola, N., Munasinghe, L., & Wimalasena, W. (2025). From conventional methods to large lan-\nguage models: A systematic review of techniques in mobile app review analysis. Interdisciplinary Journal of Infor-\nmation, Knowledge, and Management, 20, Article 16. https://doi.org/10.28945/5491  \n(CC BY-NC 4.0) This article is licensed to you under a Creative Commons Attribution-NonCommercial 4.0 International \nLicense. When you copy and redistribute this paper in full or in part, you need to provide proper attribution to it to ensure \nthat others can later locate this work (and to ensure that others do not accuse you of plagiarism). You may (and we encour-\nage you to) adapt, remix, transform, and build upon the material for any non-commercial purposes. This license does not \npermit you to use this material for commercial purposes. \nFROM CONVENTIONAL METHODS TO LARGE LANGUAGE \nMODELS: A SYSTEMATIC REVIEW OF TECHNIQUES IN \nMOBILE APP REVIEW ANALYSIS \nNimasha Arambepola* Software Engineering Teaching Unit, \nFaculty of Science, University of \nKelaniya, Sri Lanka \nnimasha@kln.ac.lk    \nLankeshwara Munasinghe School of Computing, Engineering, and \nTechnology, Robert Gordon University, \nAberdeen, Scotland, United Kingdom \nl.munasinghe@rgu.ac.uk  \nWaruni Wimalasena Software Engineering Teaching Unit, \nFaculty of Science, University of \nKelaniya, Sri Lanka \nrawlw241@kln.ac.lk \n* Corresponding author \nABSTRACT  \nAim/Purpose This paper focuses on app review analysis techniques, driven by the rapid advance-\nment of the mobile app market and NLP techniques in optimizing mobile app \nuser experiences. \nBackground Owing to technological advancements, app review analysis has rapidly evolved. \nThis study examines both conventional and emerging techniques, including cur-\nrent advancements such as large language models (LLMs) in app review analysis. It \nprovides an overview of the various methods used across different categories of \napp review analysis, comparing effective strategies for identifying user concerns \nand enhancing app functionality. \nMethodology A systematic review was utilized based on two major standard guidelines, PRISMA \nand Kitchenham’s guidelines, for the period of 2014 to 2024. After defining the re-\nview protocol, papers were identified through keyword-based searches on six ma-\njor online databases: Scopus, Web of Science, IEEE Xplore, ACM Digital Library, \nScience Direct, and Springer. Following screening and excluding papers based on \ndefined quality criteria, 53 papers were considered for this study. The use of \nLarge Language Models  \n2 \nPRISMA ensures a transparent and reproducible review process, while Kitchen-\nham’s guidelines provide a structured and rigorous approach for evaluating and \nsynthesizing the literature. \nContribution This review study aims to evaluate the current state of knowledge on app review \nanalysis techniques to improve mobile app user experiences. This study catego-\nrized the existing state-of-the-art papers into eight different categories, such as \nsentiment analysis, review classification, summarization, and prioritization, and ex-\namined challenges related to app review analysis. Furthermore, the study empha-\nsizes the potential of LLMs for optimizing and automating app review analysis and \nprovides future directions to address gaps in user-centric app development. \nFindings Among the eight main categories defined in app review analysis, sentiment analysis \nis the most prevalent, followed by review classification and information extraction. \nMost studies use a combination of these categories to achieve a comprehensive \ngoal. Prioritization techniques such as risk matrices, thumbs-up count-based ap-\nproach, and anomaly detection are widely used to identify emerging issues. Ex-\ntracting meaningful information and evaluating the proposed approach are the \nmost common challenges identified. Novel LLMs, like Chat-GPT, significantly en-\nhance review analysis by automating the process, improving feature extraction, and \nenabling context-aware review classification. \nRecommendations  \nfor Practitioners \nThe combination of conventional approaches and novel LLM-based methods can \nenhance both the efficiency and accuracy in identifying and addressing critical is-\nsues raised through mobile app user reviews. It effectively prioritizes user concerns \nby leveraging the strengths of both traditional preprocessing techniques and ad-\nvanced LLMs. \nRecommendations  \nfor Researchers  \nResearchers are encouraged to explore the integration of emerging technologies \nlike LLMs to enhance the of app review analysis, particularly in feature-specific \nsentiment analysis.  \nImpact on Society The results of this study contribute to enhancing the mobile app user experience \nthrough effective app review analysis, which improves user satisfaction and sup-\nports user-centered app development. This ultimately leads to a better mobile app \necosystem, benefiting both users and developers. \nFuture Research In the future, this research can be extended in multiple directions. Researchers can \naddress the existing research gaps that LLMs have yet to address, particularly in \nprioritizing user concerns. Additionally, there is potential for further research on \ntool implementations focusing on identifying persistent issues through time series \nanalysis by considering the app version and date of the app reviews. Moreover, \nthere is a need to develop comprehensive frameworks that are more generalizable \nacross different apps and categories, with a focus on identifying user concerns re-\nlated to specific features. \nKeywords app user reviews, text analysis, LLM, user experience, systematic review  \nINTRODUCTION  \nOwing to the widespread adoption of smartphones and the increasing dependence on mobile appli-\ncations, the mobile app market has seen significant expansion throughout the past decade. Especially \nduring and after the COVID-19 pandemic, people established and maintained their daily activities via \nArambepola, Munasinghe, & Wimalasena \n3 \nmobile applications (Chemnad et al., 2022). For example, several new mobile apps have been intro-\nduced to the app market, mainly in app categories such as education, social media, lifestyle, shopping, \nand entertainment. This expansion motivates continuous and rapid app enhancement to retain and \nattract users. In this competitive app market, app user reviews are a crucial feedback mechanism that \nprovides direct insights into users’ experiences, preferences, expectations, and difficulties while using \nthe app.  \nApp review analysis involves extracting meaningful information from user feedback to identify com-\nmon issues, desired features, and overall user satisfaction. This process is essential for prioritizing \nuser requirements to make decisions for the next release plan to optimize the user experience (UX). \nApp review analysis is carried out in different categories/types, including sentiment analysis, review \nclassification, and review prioritization (Sultana & Sarker, 2018; Villarroel et al., 2016). However, \nmanual analysis is impractical for millions of reviews. Therefore, it is necessary to employ techniques \nsuch as machine learning (ML) and natural language processing (NLP).  \nDespite ML and NLP, large language models (LLMs) have emerged as powerful tools in recent years, \nand their popularity has grown exponentially. The ability of LLMs for sentiment analysis has been \nemerging as a promising area across various industries, including e-commerce, social media monitor-\ning, healthcare, and finance, to gain insights about customer opinions, brand perception, market \ntrends, and public sentiment, enabling data-driven decision-making and enhanced customer experi-\nences (Upadhye, 2024). For example, ChatGPT, a state-of-the-art LLM, has shown its potential to \nconduct advanced aspect-based analysis of hotel reviews to understand the areas lacking customer \nsatisfaction (Jeong & Lee, 2024). With their content extraction and prompt generation capabilities, \nLLMs have also shown their capabilities in app testing by helping detect unusual input detection (Z. \nLiu et al., 2024). In addition to these capabilities, LLMs have proven instrumental in analyzing macro \nand micro-level customer behavior trends over time to measure the impact of negative reviews on \nconsumer behavior (Z. Wang et al., 2024). The growth of LLMs’ capabilities also expands into aca-\ndemic research areas, especially in the systematic literature review (SLR) process, which starts by au-\ntomating the screening process of SLR (Dennstädt et al., 2024). Then, it expanded into automating \nthe initial search, screening, summarization, and analysis phases in the manual SLR (Sami et al., \n2024). These approaches prove the capability of LLMs to evolve the manual analyzing processes into \na much more efficient and effective automated process. \nThe use of LLMs has also brought about notable advancements in the efficiency of app review analy-\nsis. With the popularity of LLMs such as Chat-GPT, Gemini, and LLaMA, the capability of automat-\ning review analysis, information extraction, and response generation has been explored in a new as-\npect of this area. These models are designed to capture subtle linguistic nuances, enabling them to \nprovide more accurate and comprehensive analyses (Roumeliotis et al., 2024). It has been widely \nspread across various aspects, including improving the app review process and e-commerce plat-\nforms (Azov et al., 2024) and hotel services (Jeong & Lee, 2024) by analyzing customer comments, \nwhich are much more relatable to app reviews. LLMs’ improvements offer enhanced capabilities for \nreview classification, sentiment analysis, and feature extraction (Assi et al., 2024; Zhang et al., 2024). \nThey can operate in complex scenarios, such as extracting user sentiments and app features with var-\nied levels of contextual understanding (Roumeliotis et al., 2024; Shah et al., 2024). For instance, \nChatGPT and GPT-4 models have effectively analyzed feedback to identify user preferences and \ncomplaints (Roumeliotis et al., 2024; Shah et al., 2024). Furthermore, approaches such as LLM-Cure \nutilize LLMs in multiple phases, like categorizing feedback, identifying underperforming features, and \nsuggesting improvements by referencing highly rated features from competitors (Assi et al., 2024). \nThis proves that LLMs demonstrate the versatile applications of these models in app review analysis, \nallowing stakeholders to pinpoint issues, prioritize user concerns, and align development strategies to \noptimize user experience effectively (Assi et al., 2024; Zhao et al., 2024). \nDespite their benefits, LLMs still have many limitations. Challenges related to data privacy, contex-\ntual understanding, and handling ambiguous feedback remain areas of ongoing research (Zhao et al., \nLarge Language Models  \n4 \n2024). Additionally, while LLMs perform well in conventional tasks such as sentiment classification \nand fundamental text analysis, they may struggle with more complex tasks that require deep contex-\ntual or structured sentiment information (Morbidoni, 2023; Zhang et al., 2024). For instance, com-\nplex tasks like quadruple extraction, which require specialized prompt templates and methods to lev-\nerage few-shot examples, test the capability boundaries of LLMs (de Lima et al., 2023b). Comparative \nevaluations against state-of-the-art models on public datasets have shown both the potential and limi-\ntations of LLMs in handling such complex extractions (Xu et al., 2023). Future advancements in \nLLM training, domain-specific fine-tuning (Roumeliotis et al., 2024), and combining LLMs with re-\ntrieval-augmented generation (Azov et al., 2024) are expected to address these challenges. Such im-\nprovements will enhance the ability of app developers and researchers to leverage LLMs for continu-\nous, scalable, and accurate app review analysis and feature enhancement. Continued research refining \nLLMs’ contextual comprehension and domain adaptability will further improve their application in \napp review analysis, ultimately focusing on more user-centered, innovative, and competitive mobile \napp experiences. Despite the advancement, some methods, such as review labelling and topic evalua-\ntion, still require manual effort. Even though generative artificial intelligence (AI) is capable of auto-\nmating text labelling, it still relies on manually labelled datasets to improve precision and relevance \n(Chou & Cho, 2023; Kozlowski et al., 2024). Therefore, it remains a challenge to conduct a fully au-\ntomatic review analysis.  \nReview analysis is a well-established yet continuously evolving research field. Researchers in this field \nutilize different and combined techniques to develop frameworks that enhance accuracy and effi-\nciency in review analysis. Consequently, despite the advancements in user review analysis, several \nchallenges still exist. Therefore, it is beneficial to identify different challenges and limitations with dif-\nferent techniques and identify methods to overcome them for future research studies. Thus, this sys-\ntematic review provides a comprehensive overview of state-of-the-art techniques in app review analy-\nsis and challenges for app review analysis in software engineering by examining previous research \nstudies published between 2014 and 2024 across six major academic databases. While previous re-\nview studies have addressed app review analysis (Dąbrowski et al., 2022; Genc-Nayebi & Abran, \n2017), this study identifies explicitly gaps in prioritization methods for user concerns, examines chal-\nlenges in app review analysis, and explores potential strategies to overcome them.  \nThere are several notable gaps in the existing systematic reviews on app review analysis. A significant \ngap is the lack of focus on prioritization methods for user concerns. While many reviews emphasize \nsentiment analysis and classification (Dąbrowski et al., 2022; Genc-Nayebi & Abran, 2017), they ne-\nglect how to align user feedback with development priorities. This limits developers’ guidance on de-\ncision-making. Moreover, there is an insufficient exploration of emerging techniques, such as LLMs. \nAdditionally, despite the growing need for responsive and agile development, cross-domain applica-\ntions and real-time analysis are often neglected. This study seeks to address these gaps by reviewing \napp review prioritization techniques, LLM integration, and cross-domain application review analysis. \nTherefore, the objectives of this systematic review are to categorize types of app review analysis, \nidentify various techniques, including emerging LLM-based methods, examine prioritization ap-\nproaches, and analyze the challenges and solutions in app review analysis. \nThe remainder of this paper is organized as follows. The next section discusses the review process \nand methodology used for the analysis in this study. Then, the results of the findings are presented \nand discussed. The paper concludes with a discussion of future research directions. \nRELATED WORKS  \nOver the past decade, user reviews have become more complex due to the increasing diversity of app \nusers and their evolving requirements. Mobile app review analysis aims to enhance UX by under-\nstanding user feedback. However, due to many lengthy, non-informative, erroneous user reviews, it is \nArambepola, Munasinghe, & Wimalasena \n5 \ndifficult for users and developers to read every review to understand user concerns related to a par-\nticular app. Even though an app rating system is available to express the overall user opinion, dispari-\nties exist between user ratings and review comments. Consequently, a sentiment rating approach has \nbeen proposed to provide summarised feedback, providing users with a clearer understanding of the \napplication beyond the star rating (Rodrigues et al., 2017; Yu et al., 2017). \nCHALLENGES IN APP REVIEW ANALYSIS  \nThere are common challenges for app review analysis due to inconsistencies in app user reviews. \nAmong them, many existing studies highlight the need for effective preprocessing techniques due to \nthe difficulties associated with extracting meaningful information from app reviews. Early works in \nthis field typically focused on basic text preprocessing techniques. For instance, tokenization, stop \nword removal, stemming, and lemmatization are the most common initial preprocessing steps in text \nanalysis (Genc-Nayebi & Abran, 2017). These methods laid the foundation for app review analysis, \nbut they were limited in their ability to handle the complexities of user feedback. For example, to-\nkenization and stemming frequently ignore the context in which words are used, potentially resulting \nin the loss of important meaning. Specifically, this affects the reviews that contain domain-specific \nterms (Gao et al., 2022). Additionally, non-informative reviews can be categorized as meaningful and \nmeaningless reviews. While meaningless, non-informative reviews are not helpful, meaningful non-\ninformative reviews can be useful for initial filtering and primary analyses. For example, a review \nsuch as “Really nice app” is helpful for sentiment analysis to determine a positive or negative opin-\nion, but it lacks details in identifying specific aspects of the app that are appreciated or need improve-\nment. Thus, this is non-informative for extracting meaningful insights. Filtering and extracting only \nmeaningful sentences by setting a threshold for review length can overcome the issue of having a \nsheer volume of non-informative reviews in the review analysis. Furthermore, custom stop word re-\nmoval has been widely used, as certain words are meaningless for identifying prominent topics or \nthemes from app user reviews (Arambepola et al., 2024; Gao et al., 2022). Despite this, many studies \nstill overlook the importance of handling mixed or neutral sentiment reviews, which often lack clear \npolarity but may contain significant information about app features that need improvement. \n APPLICATIONS OF APP REVIEW ANALYSIS  \nApp review analyses have been conducted for various purposes, with some studies explicitly focusing \non particular app categories, such as health and fitness (Ahn & Park, 2023; Haggag et al., 2022). The \nfindings from these analyses are integral at various stages of the software development life cycle, \nfrom requirement gathering to app maintenance (Al-Subaihin et al., 2021; Dąbrowski et al., 2022, \n2023). Consequently, researchers have conducted app review analysis to identify the supporting soft-\nware engineering activities and investigate user reviews related to specific aspects of apps. For exam-\nple, usability and user experience identification through app reviews is widely adopted (Lim et al., \n2021; W. Nakamura et al., 2022), particularly emphasizing user interface improvements (Q. Chen et \nal., 2021). Moreover, apps that satisfy users in some countries may not meet users’ expectations in \nother countries due to economic disparities and different user expectations (Srisopha et al., 2020). \nThus, country-specific feature requests are essential for customizing mobile apps based on the prefer-\nences of user demographics. \nFurthermore, app review analysis is crucial for market research for app development, as it allows \ncomparing competitive mobile apps in app stores (Li et al., 2017). In there, feature-oriented senti-\nment analysis is vital for understanding which features contribute positively and negatively to user \nexperience (Luiz et al., 2018). This informative classification also supports decision-making for up-\ncoming app release updates and patches. Different tools and frameworks have been proposed to as-\nsist in analyzing app reviews. For instance, AR-Miner is a tool that is used to gather feedback from \nusers (N. Chen et al., 2014), SUR miner permits sentiment analysis together with topic modelling (Di \nSorbo et al., 2016), while MApp-IDEA is an involved review analytics and data visualization platform \nLarge Language Models  \n6 \n(Gao et al., 2018). These advancements show significant progress in app review analysis. However, \nhandling many heterogeneous reviews characterized by context-richness still needs to be solved. \nSTATE -OF-THE -ART  TECHNIQUES  IN APP REVIEW ANALYSIS  \nRecent studies underscore the significance of large language models (LLMs) in user review analysis \nacross various domains. Automated feature-level sentiment analysis in app reviews has proven essen-\ntial, with state-of-the-art LLMs, such as GPT-4, surpassing rule-based methods in extracting feature-\nspecific sentiments and generating valuable insights from minimal labeled data (Shah et al., 2024). \nSimilarly, LLMs play a crucial role in e-commerce by enhancing customer sentiment analysis through \ncomparisons of fine-tuned and pre-trained models, contributing to a deeper understanding of user \nsatisfaction and improving customer experience(Roumeliotis et al., 2024). These findings affirm the \npotential of LLMs in automated review analysis, extending their utility beyond conventional senti-\nment classification. \nRecent research has focused on LLMs for more advanced and automated app review analyses. The \nemergence of LLM-based tools, such as LLM-Cure, has improved more targeted and effective fea-\nture enhancement suggestions by identifying underperforming app features and comparing them with \nhighly rated features from competing applications (Assi et al., 2024). This tool emphasizes the poten-\ntial of LLMs in augmenting app review analysis, automating feature extraction, and generating in-\nformed recommendations for product enhancement. Another significant contribution is the applica-\ntion of LLMs for structured user review analysis, which employs retrieval-augmented generation \n(RAG) for more accurate data processing (Azov et al., 2024). New systems like SCRABLE have been \ndesigned to self-optimize their prompts and assess response quality using an LLM-based judging \nmechanism. The efficacy of such systems is evidenced by their ability to produce high-quality re-\nsponses that exceed baseline results by substantial margins. This reinforces the importance of com-\nbining advanced LLM capabilities with adaptive and context-aware mechanisms to enhance the accu-\nracy and efficiency of app review analysis (Azov et al., 2024). Moreover, integrating LLMs into the \napp review analysis process supports a more holistic understanding of user feedback. Unlike tradi-\ntional methods that rely on primary sentiment classification, LLMs facilitate aspect-based sentiment \nanalysis. They can process complex quadruple extractions using specialized prompt templates for as-\npect, category, opinion, and sentiment extraction to identify opinions related to specific app features \n(Xu et al., 2023). Recent research has also introduced a dynamic prompt generation technique to ex-\ntract specific application characteristics from user reviews and classify risks by severity, from negligi-\nble to critical. This approach enables the automatic construction of a standardized risk matrix, with \nevidence showing that the Open Pre-trained Transformers (OPT) model competes well with proprie-\ntary models like GPT-3.5 (de Lima et al., 2023b). \nAdopting LLMs has transformed app review analysis, shifting it from conventional sentiment classifi-\ncation to more sophisticated, context-rich evaluations that can inform actionable strategies. The fu-\nture of this field will likely see further integration of LLMs with user-contributed documents and \nreal-time data processing to create dynamic, continuously learning systems that better cater to user \nand developer needs. This evolution aligns with the broader trend of AI-driven decision-making in \nsoftware development. This has the potential to position LLMs as crucial tools in extracting, inter-\npreting, and acting on user feedback to drive competitive advantage and user satisfaction. \nETH ICAL CONSIDERATION S AND LIMITATIONS  \nWhile LLMs offer significant potential for app review analysis, their deployment raises several ethical \nand practical concerns. A major limitation is the representativeness of the datasets used to train these \nmodels, which may not fully capture the diversity of user experiences, thereby affecting their generali-\nzability across different application domains (Roumeliotis et al., 2024). Additionally, privacy and data \nsecurity concerns remain paramount, as the automated processing of user reviews involves handling \nsensitive information. Addressing these challenges necessitates the implementation of robust ethical \nArambepola, Munasinghe, & Wimalasena \n7 \nframeworks and regulatory guidelines to ensure transparency and accountability in LLM-based analy-\nsis (Zhao et al., 2024). \nMoreover, the responsible use of sentiment analysis systems requires careful consideration of poten-\ntial biases, societal impacts, and unintended consequences. Ethical guidelines and stakeholder engage-\nment mechanisms are essential to mitigate risks associated with automated decision-making and en-\nsure the fair and responsible deployment of AI-driven review analysis tools (Upadhye, 2024). Fur-\nthermore, the evolving nature of LLM ecosystems calls for continuous dialogue between developers, \npolicymakers, and researchers to establish best practices for responsible AI development, particularly \nin relation to security, privacy, and user trust (Zhao et al., 2024). By incorporating these ethical con-\nsiderations, future research can contribute to developing more transparent, accountable, and socially \nresponsible LLM-based review analysis systems. \nRECENT RESEARCH TRENDS AND GAPS \nDespite advancements in app review analysis, several critical gaps remain unaddressed. One signifi-\ncant limitation is the lack of emphasis on prioritization strategies for user concerns. Existing studies \nprimarily focus on sentiment analysis and classification (Dąbrowski et al., 2022; Genc-Nayebi & \nAbran, 2017) yet fail to establish methodologies for aligning user feedback with development priori-\nties. This shortcoming restricts developers’ ability to make informed decisions based on user needs. \nAdditionally, there is limited exploration of emerging approaches, particularly the integration of \nLLMs, in refining app review analysis (Noei & Lyons, 2019). Furthermore, despite the growing de-\nmand for agile and responsive development practices, cross-domain applications and real-time analyt-\nics remain underexplored. Addressing these gaps is essential for enhancing the effectiveness of app \nreview analysis. This study seeks to bridge these deficiencies by investigating prioritization frame-\nworks, LLM-driven review synthesis, and methodologies for analyzing cross-domain app reviews in \nreal time. \nMETHODOLOGY  \nA systematic review collects and synthesizes findings to address specific questions within a given field \nor subject. Several standard guidelines for performing systematic literature reviews vary depending on \nthe area or domains involved. For instance, commonly used guidelines include Kitchenham’s guide-\nlines (Kitchenham & Charters, 2007) and the Preferred Reporting Items for Systematic Reviews and \nMeta-Analyses (PRISMA) (Page et al., 2021). Although PRISMA primarily applies to healthcare, so-\ncial science, and educational research, it is also popular in computer science research (Boaye Belle & \nZhao, 2022). In contrast, Kitchenham’s guidelines provide comprehensive instructions for systematic \nliterature reviews in Software Engineering. Therefore, this study adopts a combined approach, utiliz-\ning both guidelines for this systematic literature review. \nThe study was conducted in three stages: planning, conducting, and reporting the review. The most \ncrucial stage, planning, involved formulating the research questions and developing and evaluating \nthe review protocol. The review protocol consists of the following components: search strategy (key-\nwords and databases), study inclusion and exclusion criteria, and data extraction approach (summa-\nrizing and analyzing data). The review protocol was assessed using a set of pre-selected studies perti-\nnent to the research area. Subsequently, the review was conducted according to the finalized review \nprotocol. The main stages are discussed in the following sections. \nFORM ULATIN G RESEARCH QUESTIONS  \nFormulating the research question is a critical stage in a systematic review as it sets the direction for \nthe study. Therefore, it is essential to establish clear and concise research questions that align with the \noverall research objective. Thus, the formulated research questions are as follows: \nRQ1: What are the main categories of app review analysis in Software Engineering?  \nLarge Language Models  \n8 \nRQ2: Which techniques are employed to perform app review analyses?  \nRQ3: What are the techniques used for prioritization of user concerns?  \nRQ4: What challenges are encountered in app review analysis? \nRQ5: How have LLMs evolved for the app review analysis process? \nA systematic literature review followed the developed review protocol to address these research ques-\ntions. \nLITERATURE SEARCH  AND SELECTION  \nThe PRISMA method was followed to select literature for the review, with publications from 2014 to \n2024 being considered. Initially, papers were identified through keyword-based searches on six major \nonline databases: Scopus, Web of Science, IEEE Xplore, ACM Digital Library, Science Direct, and \nSpringer. The key terms used for the search were ‘app review analysis,’ ‘app review mining,’ ‘analyz-\ning app reviews,’ and ‘mining app reviews.’ As shown in Figure 1, a total of 221 research papers were \nidentified from the initial search. According to the diagram, after removing 56 duplicate research arti-\ncles, 165 articles were screened according to the inclusion and exclusion criteria stated in Table 1. As \nthe research scope was narrowed, most of the identified research articles were excluded due to their \nlack of relevance to the study. For instance, 116 publications were excluded based on the inclusion \nand exclusion criteria. \n \n \nFigure 1. PRISMA diagram of study selection \n \n \n\nArambepola, Munasinghe, & Wimalasena \n9 \nConsequently, backward and forward snowballing was performed, starting with the 12 most relevant \nresearch papers, which led to the identification of 19 further relevant studies for the literature review. \nThe articles for the literature review were then finalized, with additional exclusions based on quality \nassessment criteria. The quality of the primary studies was evaluated using the checklist shown in Ta-\nble 2. Among the questions in the quality checklist, only papers that adhered to at least four ques-\ntions were considered for the review. Ultimately, a total of 53 papers were considered for the review.  \nTable 1. Study selection criteria (inclusion and exclusion criteria) \nNumber Inclusion criteria \n1 Studies published as conference papers, journal articles, or book chapters as full \npapers. \n2 Studies related to app review analysis that support software engineering activities, \neither directly or indirectly. \nNumber Exclusion criteria \n1 Papers not written in English. \n2 Studies not published as full papers. \n3 Studies not related to the specified research questions. \n \nTable 2. Quality checklist (Kitchenham & Charters, 2007) \nNumber Question \n1 Are the objectives clearly defined? \n2 How clear and transparent is the method of evaluative assessment? \nNumber Exclusion criteria \n3 How effectively was data gathering conducted? \n4 How clearly has the approach to and description of analysis been presented? \n5 How clear are the links between data, interpretation, and conclusions? \n6 How adequately has the research process been documented? \nDATA EXTRACTION AND SYNTH ESIS \nThe data extraction approach was established during the review protocol development stage. The \nmajority of the selected research papers have used a mix of qualitative and quantitative methods. \nData from the selected papers was extracted using an electronic spreadsheet across ten specified \nfields, focusing on addressing the formulated research questions. The extracted data was organized in \nthe spreadsheet for easy identification, aggregation, and comparison (Dąbrowski et al., 2022). Some \nof these fields pertain to the characteristics of the publications. All ten fields are as follows: \n• Title \n• Author(s) \n• Year of publication  \n• Type of publication: Journal, conference, or book chapter  \n• Analysis type/objective: Review classification, summarizing, prioritization, information ex-\ntraction, information retrieval, visualization  \n• Data and data sources: App store(s), number of app categories, number of reviews  \n• Analysis techniques  \n• Research outcome/result: Tool or framework developed  \n• Evaluation: procedure, metrics, and criteria, result  \n• Limitations and challenges of the study \nLarge Language Models  \n10 \nAfter organizing the data in the spreadsheet, classification and clustering methods were utilized to \nidentify various review analysis types/categories and techniques. Additionally, previous literature re-\nlated to prioritizing user concerns from app reviews was separately examined to identify specific \ntechniques used, challenges, and limitations to address one of the objectives of this review. Finally, \nLLM-based research studies were critically analyzed to answer and address the last research question. \nRESULTS  \nFindings and their significance for each research question are presented in this section. Despite the \nextensive research in opinion mining and review analysis, this study focuses explicitly on app review \nanalysis conducted using automated and/or semi-automated techniques over the past decade, exclud-\ning manual approaches. The selection criteria were designed to identify papers most relevant to tech-\nniques for prioritizing user concerns, resulting in the inclusion of 53 articles for review. Publications \nfrom 2014 to May 2024 were considered, and the distribution of studies per year is illustrated in Fig-\nure 2.  \n \nFigure 2. Number of publications per year \nR Q1: CATEGORIES OF APP REVIEW ANALYSIS  \nApp review analysis has been conducted with various objectives and purposes. In this review, we \nhave identified eight distinct categories of app review analysis, as illustrated in Figure 3. It is im-\nportant to note that it is rarely seen for a study to employ only a single category of analysis. Further-\nmore, researchers have proposed and evaluated various tools and frameworks to automate app re-\nview analysis. Table 3 summarizes the tools that have been proposed by previous researchers to auto-\nmate app review analysis, and the number of app reviews used to evaluate the tool.  \n \n0\n2\n4\n6\n8\n10\n12\n14\n16\n2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024\nNumber of Publications\nYear\nArambepola, Munasinghe, & Wimalasena \n11 \n \nFigure 3. Review analysis categories and their occurrences in the studied literature \nTable 3. Tools and frameworks for app review analysis \n0 2 4 6 8 10 12 14 16\nPrioratizing\nVizualization\nSummarization\nSentiment analysis\nClustering\nClassification\nInformation retreival\nInformation extraction\nNumber of studies\nReview analysis category\nTool \nFrame-\nwork \nPurpose \nConsidered app \nstores/app categories \nand number of apps \nfor evaluation \nNumber of \nreviews Year Reference \nAR-Miner Information extraction, \nclustering, prioritization \n& visualization \nPlay Store, 4 apps - 2014 (N. Chen et \nal., 2014) \nMERIT Summarizing & visuali-\nzation  \nPlay Store (4 apps from \n4 categories), App \nStore (2 apps from 2 \ncategories) \n164,026 2015 (Gao et al., \n2022) \nPAID Information extraction, \nclustering, prioritization \n& visualization \nPlay Store (35 apps \nfrom 10 categories) \n2,089,737 2015 (Gao et al., \n2015) \nMARK Information extraction, \nclassification & visuali-\nzation \nPlay Store & App \nStore, 95 apps \n- 2015 (Phong  et al., \n2015) \nSURF Summarizing  17 apps - 2016 (Di Sorbo et \nal., 2016) \nURR Classification & infor-\nmation retrieval \n39 apps - 2017 (Ciurumelea \net al., 2017) \n IDEA Information extraction, \nprioritization & visuali-\nzation \nPlay Store (60 apps \nfrom 20 categories) \n(from the literature) \n164,026 2018 (Gao et al., \n2018) \nMApp \nIDEA \nInformation extraction, \nprioritization & visuali-\nzation \nPlay Store (60 apps \nfrom 20 categories) \n5 million 2023 (de Lima et \nal., 2023a) \nLarge Language Models  \n12 \n \nRQ3:  TECHNIQUES USED FOR PRIORITIZING USER CONCERNS  \nThis section addresses RQ3 (What techniques are used for prioritizing user concerns?). The primary \nobjective and significant focus of this literature review is to identify techniques employed to prioritize \nuser concerns. Two main types of prioritizations are identified: review prioritization and user require-\nment prioritization (Malgaonkar et al., 2022). Table 4 summarizes the prioritization techniques used \nby previous researchers. \nTable 4. Summary of techniques used for prioritizing user concerns in app reviews \nCategory Techniques Purpose References \nReview \nPrioritization \nGrouping-Based Ranking Categorizing reviews into \npriority levels \n(Gao et al., \n2015)  \nRegression Techniques \n(Time Series, Average \nRatings) \nPredicting review \nimportance over time \n(N. Chen et al., \n2014) \n \nTOUR (Topic & \nSentiment Analysis of User \nReviews) \nDynamically prioritizing \nemerging issues based on \napp versions \n(T. Yang et al., \n2021) \nUser \nRequirement \nPrioritization \nAnomaly Detection Identifying unusual trends \nin reviews \n(Gao et al., \n2022) \n \nRisk Matrices (Clustering \n& Graph Theory) \nRanking concerns based on \nseverity \n(de Lima et al., \n2023a) \n Thumbs-Up Count-Based \nApproach \nPrioritizing reviews based \non user upvotes \n(Arambepola et \nal., 2024) \nLLM-Based \nApproaches \nOPT Model for Risk \nMatrices \nAutomating risk severity \nclassification of user con-\ncerns \n(de Lima et al., \n2023b) \nTool \nFrame-\nwork \nPurpose \nConsidered app \nstores/app categories \nand number of apps \nfor evaluation \nNumber of \nreviews Year Reference \nTOUR Information extraction, \nsummarization, & pri-\noritization \n   (T. Yang et \nal., 2021) \nUX MAP-\nPER \nSummarizing Play Store & App Store - 2024 (W. T. Naka-\nmura et al., \n2024) \nLLM-Cure Classification, prioriti-\nzation, information ex-\ntraction, & information \nretrieval \n70 popular Android \napps. \n1,056,739 2024 (Assi et al., \n2024) \nSCRABLE Information extraction, \n& information retrieval \n9 real-world customers 49 2024 (Azov et al., \n2024) \nLLM-based \nRisk Matrix \nInformation extraction, \nclassification, & priori-\ntization \n8 Popular apps (eBay, \nEvernote, Facebook, \nNetflix, Photo editor, \nSpotify, Twitter, \nWhatsapp )  \n363843 2023 (de Lima et \nal., 2023b) \nArambepola, Munasinghe, & Wimalasena \n13 \n \nRQ4:  CH ALLENGES IN APP REVIEW ANALYSIS \nThe summary of the critical challenges and common strategies used to mitigate them is shown in Ta-\nble 5. \nTable 5. Challenges in app review analysis \nChallenge Strategies to overcome \nManual labelling Multiple researchers are often involved in labeling, and statistical tests such \nas Cohen’s Kappa are employed to assess inter-rater agreement and reduce \nbias (W. Nakamura et al., 2022). \nExtracting meaningful \ninformative reviews \nGet the feedback from external professional app developers (Di Sorbo et \nal., 2016; Malgaonkar et al., 2022). \nEvaluating the proposed \napproach and validating \nthe results \n Use official app change logs (Gao et al., 2022; C. Yang et al., 2021) and \napp descriptions (Y. Liu et al., 2018). \nGeneralizing the \nproposed framework \nValidate with diverse app categories from different app stores. \nTopic interpretation Phase extraction over keyword extraction (Gao et al., 2015, 2022), mapping \nwith the existing UX factors (Arambepola et al., 2024). \nLLM context \nlimitations \n \nThe batch-and-match method. This approach incrementally extracts the \ntop k features from a large corpus of user reviews, ensuring efficient \nprocessing despite LLM context constraints (Assi et al., 2024). \n \nCost and availability of \nhuman evaluations \nAn automated system that can simulate human judgment in evaluating cus-\ntomer feedback, enabling real-time review assessments and fostering con-\ntinuous service improvements (Azov et al., 2024). \nDiverse descriptions and \na large review volume \nfor risk assessment \nUse of automatic machine learning-based methods for extracting risks from \nreviews and classifying their priority (de Lima et al., 2023b).  \nDISCUSSION \nThe results presented in the previous section provide valuable insights into the mobile app user re-\nview analysis. This section presents the findings in the context of existing literature, highlighting their \nbroader implications.       \nR Q1: CATEGORIES OF APP REVIEW ANALYSIS  \nThis section addresses RQ1 (What are the main categories of app review analysis in Software Engi-\nneering?). App review analysis has been conducted with various objectives and purposes. In this re-\nview, we have identified eight distinct categories of app review analysis, as illustrated in Figure 3. \nMost review analyses incorporate multiple categories to achieve their objectives (N. Chen et al., 2014; \nGao et al., 2015; Phong et al., 2015). For example, AR-Miner is a computational framework consist-\ning of components for extracting informative user reviews, clustering, prioritizing them using a re-\nview ranking schema, and visualization (N. Chen et al., 2014). Based on the prevalence in the selected \nliterature, sentiment analysis is the most frequently used category, while review classification, infor-\nmation extraction, and information extraction are also widely employed in numerous studies. Fur-\nthermore, researchers have proposed and evaluated various tools and frameworks to automate app \nreview analysis (Table 3). These tools support the different categories mentioned above to achieve \ntheir primary objectives. Reviews extracted from multiple apps across various app stores have been \nLarge Language Models  \n14 \nutilized to validate the applicability, generalizability, and effectiveness of the proposed tools and \nframeworks. \nAmong the notable advancements in app review analysis, tools that involve LLM techniques such as \nLLM-Cure and SCRABLE (Assi et al., 2024; Azov et al., 2024) are crucial. LLM-Cure excels in accu-\nrately assigning features to user reviews and offers developers targeted suggestions for improving app \nfeatures based on specific complaints by considering the positive reviews in competitive apps. SCRA-\nBLE focuses on customer review response generation by taking insights from real-world customers \nto craft meaningful and contextually relevant responses (Azov et al., 2024). These tools show the \npower of integrating LLMs in app review analysis. It enhances the overall quality of feedback inter-\npretation and response generation by exploring the capabilities of LLMs, focusing on information \nretrieval and extraction at a more advanced level.  \nRQ2:  TECHNIQUES USED IN APP REVIEW ANALYSIS  \nAnswering RQ2 (Which techniques are employed for app review analyses?), even though manual \nanalysis has been used in various domains, it is not feasible to carry out large-scale app review analy-\nsis due to its time-consuming nature, potential for bias, and human error. Consequently, this study \nexcludes research papers that rely solely on manual analysis. Machine Learning (ML) and Natural \nLanguage Processing (NLP) have been the most widely used techniques for automating app review \nanalysis. Additionally, various statistical methods are used to summarize and validate the proposed \ntools and approaches for app review analysis. Moreover, statistical techniques are also utilized for \nsampling selection and visualizing results through box plots and charts (W. Nakamura et al., 2022; \nNoei & Lyons, 2019). \nML techniques were primarily employed for the classification and clustering of app reviews. For in-\nstance, supervised ML techniques such as Naive Bayes, Support Vector Machine, and Random For-\nest are commonly used for binary and multiclass classification of reviews into categories such as ‘bug \nreports,’ ‘end-user requests,’ ‘user experience,’ and ‘ratings’ (Maalej et al., 2016; L. Wang et al., 2020). \nHowever, these techniques require labelled data as ground truth, which poses a limitation. Since user \nreviews are textual data, most studies utilize unsupervised ML techniques (de Lima et al., 2023a; Mal-\ngaonkar et al., 2022) or NLP techniques for review analysis. For example, various NLP techniques \nare applied for information extraction, summarization, and sentiment analysis. Among the NLP \nmethods, topic modelling is extensively used, with Latent Dirichlet Allocation (LDA) being the most \nprevalent technique, and some studies adopting modified LDA approaches (Gao et al., 2018). Simi-\nlarly, sentiment analysis has been implemented with various modifications (W. Nakamura et al., \n2022). In addition, word embedding was utilized to identify synonyms for topic keywords in review \nsummarizing and prioritization (Arambepola et al., 2024; Samy et al., 2021). Moreover, most research \nintegrates multiple techniques to achieve its objectives in app review analysis. For example, sentiment \nanalysis and LDA are commonly combined (Araujo et al., 2022).  \nRecent advancements have notably utilized LLMs such as Chat-GPT, Gemini, and LLaMA to en-\nhance app review analysis (Gunathilaka & De Silva, 2022). These models have improved review clas-\nsification, sentiment analysis, and feature extraction (Gunathilaka & De Silva, 2022; Roumeliotis et \nal., 2024). Additionally, these models can be further optimized by incorporating hyperparameters \nsuch as sequence length (maximum length of input tokens processed at once), temperature, and mar-\ngin values (Assi et al., 2024). Also, recent developments in app review analysis leverage LLMs and \nRAG techniques to enhance the feature extraction process. One approach focuses on competitive \nuser review analysis for feature enhancement, utilizing advanced LLMs, specifically the OPT model \n(de Lima et al., 2023b). This model’s capabilities allow for a deeper understanding of user feedback \nby analyzing the nuances in user reviews, leading to more targeted feature improvements for applica-\ntions. The integration of RAG techniques with LLMs enables researchers and developers to extract \nrelevant features and generate insightful recommendations for enhancing app functionalities (Azov et \nal., 2024).  \nArambepola, Munasinghe, & Wimalasena \n15 \nRQ3:  TECHNIQUES USED FOR PRIORITIZING  USER CONCERNS  \nThis section addresses RQ3 (What techniques are used for prioritizing user concerns?). The primary \nobjective and significant focus of this literature review is to identify techniques employed to prioritize \nuser concerns. Two main types of prioritizations are identified: review prioritization and user require-\nment prioritization (Malgaonkar et al., 2022). Reviews are prioritized based on predefined criteria or \nmetrics in review prioritization. In contrast, user requirement prioritization gives a list of prioritized \nuser concerns and feedback by covering the overall user review dataset. Therefore, prioritizing the \nuser requirement is worth identifying the features that need further improvements. Prioritization \nserves various objectives, including identifying emerging issues, optimizing release planning, and fa-\ncilitating prompt feedback by minimizing the time between issue identification and resolution (de \nLima et al., 2023b; Gao et al., 2015, 2018, 2022; Malgaonkar et al., 2022). Therefore, user requirement \nprioritization is particularly critical for enhancing the release planning of future app versions. Widely \nused prioritization techniques include anomaly detection methods (Gao et al., 2022), risk matrices \ncombining clustering and graph theory approaches (de Lima et al., 2023a), thumbs-up count-based \napproach (Arambepola et al., 2024), grouping-based ranking methods (Gao et al., 2015), and regres-\nsion techniques involving time series matrices and average ratings (N. Chen et al., 2014). Addition-\nally, TOpic and sentiment analysis of User Reviews (TOUR) is a tool capable of dynamically identify-\ning and prioritizing emerging issues based on the app version. The prioritization is based on the \nprobability distribution of the reviews under the topic (C. Yang et al., 2021). \nRecent advancements in user requirement prioritization have evolved by integrating LLMs, such as \nthe OPT model, to automate risk matrix construction. Traditionally, constructing a risk matrix was \nmanual and time-consuming, requiring stakeholders to sift through large volumes of reviews with \nvaried descriptions. LLMs, specifically the OPT model, have automated this process by extracting \nrelevant features and bugs from app reviews, classifying them by risk severity, and generating dy-\nnamic, customized risk matrices (de Lima et al., 2023b). \nRQ4:  CHALLENGES IN APP REVIEW ANALYSIS  \nAddressing RQ4 (What challenges are encountered in app review analysis?), data preparation (re-\nview/data preprocessing), framework/tool evaluation, and validation are the key stages where chal-\nlenges arise. The summary of the critical challenges and common strategies used to mitigate them is \nshown in Table 5. For instance, manual labelling is inherently challenging due to biases and time con-\nstraints. Additionally, authors who are not professional app developers in academic settings may need \nhelp accurately categorizing reviews as informative or non-informative or assigning them to specific \ncategories. This potential discrepancy can impact research findings. To address this, researchers often \nuse multiple professional app developers as external validators to ensure the accuracy of categoriza-\ntion (Di Sorbo et al., 2016; Malgaonkar et al., 2022). \nFurthermore, using official app changelogs as ground truth data is an effective strategy (Gao et al., \n2022; T. Yang et al., 2021). App descriptions are also used as an evaluation approach in app review \nsummarization (Y. Liu et al., 2018). Given the diversity of mobile apps available, generalizing app re-\nview analysis approaches across different applications can be particularly challenging. Researchers ad-\ndress this by leveraging data from multiple app stores and diverse app categories to validate their ap-\nproaches (Al-Subaihin et al., 2021; Mcilroy et al., 2017). Moreover, after identifying topics by analyz-\ning app reviews using methods like LDA, topic interpretation is challenging and requires manual ef-\nfort (Arambepola et al., 2024). System Usability Scale (SUS) is also an established method for evaluat-\ning newly proposed frameworks or tools’ usability, reliability, and efficiency (Hirave et al., 2019). \nThere, the tool’s usability is assessed through a test of the tool followed by a set of SUS questions on \na five-point Likert scale of ‘Strong Agreement’ to ‘Strong Disagreement’.  \nAdditionally, several emerging challenges have surfaced in app review analysis. One significant chal-\nlenge is the limitations of LLMs in processing extensive contexts, which can hinder scalability. The \nbatch-and-match method has been introduced to address this challenge, allowing for the incremental \nLarge Language Models  \n16 \nextraction of the top k features from a large corpus of user reviews, ensuring efficient processing de-\nspite LLM context constraints (Assi et al., 2024). Furthermore, the cost and availability of human \nevaluations can pose obstacles due to their limited availability and expense. To mitigate this, an auto-\nmated system called LLM-as-a-Judge has been proposed, which simulates human judgment in evalu-\nating customer feedback, enabling real-time review assessments and fostering continuous service im-\nprovements (Azov et al., 2024). Lastly, the diverse descriptions and large volume of reviews compli-\ncate risk assessment, necessitating the use of automatic machine learning-based methods to extract \nrisks from reviews and classify their priority. These methods can effectively address the challenges \nposed by varied descriptions and substantial review volumes, enhancing the overall reliability of app \nreview analysis (de Lima et al., 2023b). \nRQ5:  UTILIZING LLM S IN APP REVIEW ANALYSIS  \nThis section addresses RQ5 (How LLMs have evolved for the app review analysis process?). LLMs \nhave evolved significantly in the app review analysis process, enhancing various app review categories \nsuch as classification, sentiment analysis, information extraction, and information retrieval.  \nInitially, LLMs were used primarily for review classification and sentiment analysis, enabling auto-\nmated systems to categorize feedback based on user sentiment and identify key concerns regarding \nkey features (Morbidoni, 2023; Roumeliotis et al., 2024; Shah et al., 2024). State-of-the-art LLMs like \nGPT-4 and ChatGPT have demonstrated their ability to analyse app reviews regarding these classifi-\ncation and sentiment analysis processes (Roumeliotis et al., 2024; Shah et al., 2024). Over time, these \nLLMs have become more sophisticated with tools like LLMs-Cure, offering enhanced capabilities for \nanalyzing feedback to identify underperforming features and suggest improvements based on user \npreferences (Assi et al., 2024), by showing the improvement in not only review classification and sen-\ntiment analysis but also information extraction and retrieval.  \nRecent advancements have utilized LLMs, specifically the OPT model, to automatically construct risk \nmatrices from app reviews by extracting features and bugs mentioned in the reviews. This approach \nincorporates dynamic, automatic prompt generation to extract specific application characteristics, and \nit evaluates prompts that classify risks by severity, from negligible to critical. This facilitates standard-\nized, automated risk assessment and matrix construction. Experimental results indicate that OPT \ncompetes well with proprietary models like GPT-3.5 in risk matrix generation, providing a significant \nstep forward in software product maintenance and evolution (de Lima et al., 2023b). Further research \nhas developed specialized prompt templates to enable ChatGPT’s effectiveness in complex tasks, \nsuch as quadruple extraction, by employing a selection method on few-shot examples to fully lever-\nage its in-context learning ability. Comparative evaluations against state-of-the-art models on public \ndatasets reveal the capability boundaries of ChatGPT in such complex extraction tasks (Xu et al., \n2023). \nChallenges persist in tasks requiring complex information extraction, such as quadruple extraction \n(Xu et al., 2023). However, advancements like specialized prompt templates and few-shot learning \nare pushing the boundaries of LLM capabilities by improving prioritization, classifications, and infor-\nmation extraction in much more complex scenarios rather than identifying key features and senti-\nments (de Lima et al., 2023b). Overall, LLMs have become invaluable tools in app review analysis. \nWith ongoing research, their ability to understand and process feedback will continue to improve, \ndriving future user-centred and competitive app development. In the future, LLMs can play a trans-\nformative role by enabling more comprehensive, context-aware analysis through deep semantic un-\nderstanding and automated extraction of app features. These advanced models, such as LLM-Cure, \ncan enhance precision in feature-specific sentiment analysis and provide actionable insights that sur-\npass traditional methods in areas such as information retrieval. Integrating LLMs into app review \nanalysis also opens new opportunities for adaptive prompt engineering and RAG, which would facili-\ntate real-time updates and targeted feature optimization with more user-centric application develop-\nment.  \nArambepola, Munasinghe, & Wimalasena \n17 \nThe findings of this study extend beyond mobile app review analysis to broader domains such as cus-\ntomer feedback management, requirement engineering, UX research, and risk assessment. Auto-\nmated techniques like sentiment analysis, topic modeling, and LLM-driven tools can be applied to \nvarious tasks. For example, help businesses to analyze user feedback in e-commerce (N. Chen et al., \n2014; Guzman & Maalej, 2014), enhance software development by identifying key requirements \n(Maalej et al., 2016; L. Wang et al., 2020), and improve UX in chatbots and smart devices (Gunathil-\naka & De Silva, 2022). Additionally, prioritization frameworks used in app reviews can support risk \nassessment in FinTech and healthcare, ensuring compliance and service improvements (de Lima et \nal., 2023a; T. Yang et al., 2021). Future research can explore multi-modal data sources like voice and \nvideo reviews to enhance automated feedback analysis. Our findings align with prior studies high-\nlighting sentiment analysis, topic modeling, and machine learning as dominant techniques in app re-\nview analysis. However, unlike earlier research, we emphasize the increasing role of LLMs in auto-\nmating feature extraction, sentiment classification, and prioritization, particularly with models like \nOPT and ChatGPT. Additionally, while prior studies relied heavily on predefined heuristics and sta-\ntistical techniques, our review highlights emerging approaches integrating LLMs and RAG techniques \nfor dynamic, real-time prioritization. Furthermore, we identify challenges specific to LLM-based anal-\nysis, such as context limitations and evaluation constraints, which were less explored in previous liter-\nature. \nWhile this systematic review covers a broad and concise range of techniques, several limitations exist. \nFirst, the review is limited to studies published within the last decade (2014–2024). While this time \nframe captures recent advancements in the field, it may not fully account for longer-term trends or \nfoundational research that preceded this period. Second, the inclusion of only English-language pub-\nlications may result in the exclusion of important studies published in other languages, limiting the \ndiversity of techniques considered. Additionally, while grey literature was consulted to inform the sta-\ntistical and contextual analysis, it was not included in the review due to concerns regarding the uncer-\ntainty and validity of such sources. However, this may limit the incorporation of the most recent de-\nvelopments or cutting-edge techniques that have not yet been formally published in peer-reviewed \njournals in this highly dynamic research field. Future research could refine LLM processing under re-\nsource constraints. One potential approach is the batch-and-match method, which incrementally ex-\ntracts the top k features from extensive user reviews, optimizing processing within LLM context lim-\nits (Assi et al., 2024). Also, addressing ethical concerns in automated decision-making is crucial. Fu-\nture studies should explore human-judgment-simulating systems for real-time customer feedback \nanalysis, ensuring fairness and transparency (Azov et al., 2024). Moreover, automated machine learn-\ning methods for extracting and prioritizing risks from reviews can improve service quality while miti-\ngating classification biases. Aligning these techniques with regulatory and ethical standards is essential \nfor responsible AI deployment (de Lima et al., 2023b). \nCONCLUSION \nThis systematic review emphasizes the importance of app review analysis in Software Engineering, \nparticularly for enhancing mobile app user experiences. The review examined studies published be-\ntween 2014 and 2024 to identify critical categories, techniques, challenges, and strategies to overcome \nthese challenges in app review analysis, including in novel LLM-based approaches. The results found \nthat sentiment analysis, review classification, and information extraction are significant categories, \nwith ML and NLP being the most commonly used techniques in app review analysis. Prioritizing user \nconcerns is a crucial aspect of app review analysis. It has been achieved through anomaly detection, \nrisk matrices, thumbs-up count-based approaches, and regression techniques. These methods help to \noptimize the release plan by identifying emerging issues from the user reviews. Difficulties in data \npreparation, biases in manual labelling for supervised machine learning, interpreting topics or themes \nidentified from app reviews, and generalizing findings across diverse apps are the main challenges in \nreview analysis. Moreover, LLM-based approaches, including ChatGPT and OPT, are emerging as \nLarge Language Models  \n18 \nvaluable tools for extracting meaningful insights from user reviews. In conclusion, systematic app re-\nview analysis is vital for improving app quality and user satisfaction by providing an optimized UX. \nIn the future, app review analysis can be extended in multiple directions, such as incorporating large \nlanguage models for enhanced sentiment analysis and topic modelling, conducting longitudinal stud-\nies to track and address persistent issues over time, and focusing on identifying ongoing user con-\ncerns to provide actionable insights for continuous app improvement. \nREFERENCES \nAhn, H., & Park, E. (2023). Motivations for user satisfaction of mobile fitness applications: An analysis of user \nexperience based on online review comments. Humanities and Social Sciences Communications, 10, Article 3. \nhttps://doi.org/10.1057/s41599-022-01452-6  \nAl-Subaihin, A. A., Sarro, F., Black, S., Capra, L., & Harman, M. (2021). App store effects on software engi-\nneering practices. IEEE Transactions on Software Engineering, 47(2), 300–319. \nhttps://doi.org/10.1109/TSE.2019.2891715  \nArambepola, N., Munasinghe, L., & Warnajith, N. (2024). Factors influencing mobile app user experience: An \nanalysis of education app user reviews. Proceedings of the 4th International Conference on Advanced Research in Com-\nputing, Belihuloya, Sri Lanka, 223-228. https://doi.org/10.1109/icarc61713.2024.10499727  \nAraujo, A. F., Gôlo, M. P. S., & Marcacini, R. M. (2022). Opinion mining for app reviews: A n analysis of tex-\ntual representation and predictive models. Automated Software Engineering, 29, Article 5. \nhttps://doi.org/10.1007/s10515-021-00301-1  \nAssi, M., Hassan, S., & Zou, Y. (2024). LLM-Cure: LLM-based competitor user review analysis for feature enhancement. \nPsyArXiv. https://doi.org/10.48550/arXiv.2409.15724  \nAzov, G., Pelc, T., Alon, A. F., & Kamhi, G. (2024). Self-improving customer review response generation based on LLMs. \nPsyArXiv. http://arxiv.org/abs/2405.03845  \nBoaye Belle, A., & Zhao, Y. (2022). Evidence-based software engineering: A checklist-based approach to assess \nthe abstracts of reviews self-identifying as systematic reviews. Applied Sciences, 12(18), 9017. \nhttps://doi.org/10.3390/app12189017 \nChemnad, K., Alshakhsi, S., Almourad, M. B., Altuwairiqi, M., Phalp, K., & Ali, R. (2022). Smartphone u sage \nbefore and during COVID-19: A comparative study based on objective recording of usage data. Informatics, \n9(4), 98. https://doi.org/10.3390/informatics9040098  \nChen, N., Lin, J., Hoi, S. C. H., Xiao, X., & Zhang, B. (2014). AR-miner: Mining informative reviews for devel-\nopers from mobile app marketplace. Proceedings of the 36th International Conference on Software Engineering (pp. \n767–778). Association for Computing Machinery. https://doi.org/10.1145/2568225.2568263  \nChen, Q., Chen, C., Hassan, S., Xing, Z., Xia, X., & Hassan, A. E. (2021). How should I improve the UI of my \napp? A study of user reviews of popular apps in the Google Play. ACM Transactions on Software Engineering \nand Methodology, 30(3), Article 37. https://doi.org/10.1145/3447808  \nChou, H.-M., & Cho, T.-L. (2023). Utilizing text mining for labeling training models from futures corpus in \ngenerative AI. Applied Sciences, 13(17), 9622. https://doi.org/10.3390/app13179622  \nCiurumelea, A., Schaufelbühl, A., Panichella, S., & Gall, H. C. (2017). Analyzing reviews and code of mobile \napps for better release planning. Proceedings of the IEEE 24th International Conference on Software Analysis, Evolu-\ntion and Reengineering, Klagenfurt, Austria, 91–102. https://doi.org/10.1109/SANER.2017.7884612  \nDąbrowski, J., Letier, E., Perini, A., & Susi, A. (2022). Mining user feedback for software engineering: Use \ncases and reference architecture. Proceedings of the IEEE International Conference on Requirements Engineering, \nMelbourne, Australia, 114–126. https://doi.org/10.1109/RE54965.2022.00017  \nDąbrowski, J., Letier, E., Perini, A., & Susi, A. (2023). Mining and searching app reviews for requirements engi-\nneering: Evaluation and replication studies. Information Systems, 114, 102181. \nhttps://doi.org/10.1016/j.is.2023.102181 \nArambepola, Munasinghe, & Wimalasena \n19 \nde Lima, V. M. A., Barbosa, J. R., & Marcacini, R. M. (2023a). Issue detection and prioritization based on app reviews. \nhttps://doi.org/10.21203/rs.3.rs-2838568/v1  \nde Lima, V. M. A., Barbosa, J. R., & Marcacini, R. M. (2023b). Learning risk factors from app reviews: A large language \nmodel approach for risk matrix construction. https://doi.org/10.21203/rs.3.rs-3182322/v1  \nDennstädt, F., Zink, J., Putora, P. M., Hastings, J., & Cihoric, N. (2024). Title and abstract screening for litera-\nture reviews using large language models: An exploratory study in the biomedical domain. Systematic Re-\nviews, 13, Article 158. https://doi.org/10.1186/s13643-024-02575-4  \nDi Sorbo, A., Panichella, S., Alexandru, C. V., Shimagaki, J., Visaggio, C. A., Canfora, G., & Gall, H. C. (2016). \nWhat would users change in my app? summarizing app reviews for recommending software changes. Pro-\nceedings of the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering (pp. 499–510). \nAssociation for Computing Machinery. https://doi.org/10.1145/2950290.2950299  \nGao, C., Wang, B., He, P., Zhu, J., Zhou, Y., & Lyu, M. R. (2015, November). PAID: Prioritizing app issues for \ndevelopers by tracking user reviews over versions. Proceedings of the IEEE 26th International Symposium on Soft-\nware Reliability Engineering, Gaithersbury, MD, USA, 35–45. https://doi.org/10.1109/ISSRE.2015.7381797 \nGao, C., Zeng, J., Lyu, M. R., & King, I. (2018). Online app review analysis for identifying emerging issues. Pro-\nceedings of the 40th International Conference on Software Engineering (pp. 48–58). Association for Computing Ma-\nchinery. https://doi.org/10.1145/3180155.3180218  \nGao, C., Zeng, J., Wen, Z., Lo, D., Xia, X., King, I., & Lyu, M. R. (2022). Emerging app issue identification via \nonline joint sentiment-topic tracing. IEEE Transactions on Software Engineering, 48(8), 3025–3043. \nhttps://doi.org/10.1109/tse.2021.3076179 \nGenc-Nayebi, N., & Abran, A. (2017). A systematic literature review: Opinion mining studies from mobile app \nstore user reviews. Journal of Systems and Software, 125, 207–219. https://doi.org/10.1016/j.jss.2016.11.027 \nGunathilaka, S., & De Silva, N. (2022, November). Aspect-based sentiment analysis on mobile application re-\nviews. Proceedings of the 22nd International Conference on Advances in ICT for Emerging Regions, Colombo, Sri Lanka, \n183–188. https://doi.org/10.1109/ICTer58063.2022.10024070  \nGuzman, E., & Maalej, W. (2014, August 1). How do users like this feature? A fine grained sentiment analysis \nof app reviews. IEEE 22nd International Requirements Engineering Conference (RE),  Karlskrona, Sweden, 2014, \npp. 153-162, https://doi.org/10.1109/RE.2014.6912257 \nHaggag, O., Grundy, J., Abdelrazek, M., & Haggag, S. (2022). A large scale analysis of mHealth app user re-\nviews. Empirical Software Engineering, 27, Article 196. https://doi.org/10.1007/s10664-022-10222-6  \nHirave, T., Malgaonkar, S., Alwash, M., Cheriyan, J., & Surve, S. (2019, December). Analysis and prioritization \nof app reviews. Proceedings of the International Conference on Advances in Computing, Communication and Control, \nMumbai, India, 1–8. https://doi.org/10.1109/ICAC347590.2019.9036801  \nJeong, N., & Lee, J. (2024). An aspect-based review analysis using ChatGPT for the exploration of hotel service \nfailures. Sustainability, 16(4), 1640. https://doi.org/10.3390/su16041640  \nKitchenham, B., & Charters, S. (2007). Guidelines for performing systematic literature reviews in software engineering. Tech-\nnical Report EBSE 2007-001. https://legacyfileshare.elsevier.com/promis_misc/525444system-\naticreviewsguide.pdf  \nKozlowski, D., Pradier, C., & Benz, P. (2024). Generative AI for automatic topic labelling. PsyArXiv. \nhttps://arxiv.org/abs/2408.07003  \nLi, Y., Jia, B., Guo, Y., & Chen, X. (2017). Mining user reviews for mobile app comparisons. Proceedings of the \nACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 1(3), 1–15. https://doi.org/10.1145/3130935  \nLim, Z. Y., Ong, L. Y., & Leow, M. C. (2021). A review on clustering techniques: Creating better user experi-\nence for online roadshow. Future Internet, 13(9). https://doi.org/10.3390/fi13090233  \nLiu, Y., Liu, L., Liu, H., & Wang, X. (2018). Analyzing reviews guided by App descriptions for the software de-\nvelopment and evolution. In Journal of Software: Evolution and Process (Vol. 30, Issue 12). John Wiley and \nSons. https://doi.org/10.1002/smr.2112  \nLarge Language Models  \n20 \nLiu, Z., Chen, C., Wang, J., Chen, M., Wu, B., Tian, Z., Huang, Y., Hu, J., & Wang, Q. (2024). Testing the lim-\nits: Unusual text inputs generation for mobile app crash detection with large language model. Proceedings - \nInternational Conference on Software Engineering, 1685–1696. https://doi.org/10.1145/3597503.3639118 \nLuiz, W., Viegas, F., Alencar, R., Mourão, F., Salles, T., Carvalho, D., Gonçalves, M. A., & Rocha, L. (2018 , \nApril). A feature-oriented sentiment rating for mobile app reviews. Proceedings of the 2018 World Wide Web \nConference, Lyon, France, 1909–1918. https://doi.org/10.1145/3178876.3186168  \nMaalej, W., Kurtanović, Z., Nabil, H., & Stanik, C. (2016). On the automatic classification of app reviews. Re-\nquirements Engineering, 21(3), 311–331. https://doi.org/10.1007/s00766-016-0251-9  \nMalgaonkar, S., Licorish, S. A., & Savarimuthu, B. T. R. (2022). Prioritizing user concerns in app reviews –  A \nstudy of requests for new features, enhancements and bug fixes. Information and Software Technology , 144, \n106798. https://doi.org/10.1016/j.infsof.2021.106798  \nMcilroy, S., Shang, W., Ali, N., & Hassan, A. E. (2017). User reviews of top mobile apps in Apple and Google \napp stores. Communications of the ACM, 60(11), 62–67. https://doi.org/10.1145/3141771  \nMorbidoni, C. (2023). Poster: LLMs for online customer reviews analysis: Oracles or tools? Experiments with \nGPT 3.5. Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter (Article 38). Association for \nComputing Machinery. https://doi.org/10.1145/3605390.3610810  \nNakamura, W., Oliveira, E., de Oliveira, E., Redmiles, D., & Conte, T. (2022). What factors affect the UX in \nmobile apps? A systematic mapping study on the analysis of app store reviews. Journal of Systems and Soft-\nware, 193, 111462. https://doi.org/10.1016/j.jss.2022.111462  \nNakamura, W. T., de Oliveira, E. C., de Oliveira, E., & Conte, T. (2024). UX-MAPPER: A user eXperience \nmethod to analyze app store reviews. Proceedings of the XXII Brazilian Symposium on Human Factors in Compu-\nting Systems. Association for Computing Machinery. https://doi.org/10.1145/3638067.3638109  \nNoei, E., & Lyons, K. (2019). A survey of utilizing user-reviews posted on Google play store. Proceedings of the \n29th Annual International Conference on Computer Science and Software Engineering (pp. 54–63). IBM. \nPage, M. J., McKenzie, J. E., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., Shamseer, L., Tetz-\nlaff, J. M., Akl, E. A., Brennan, S. E., Chou, R., Glanville, J., Grimshaw, J. M., Hróbjartsson, A., Lalu, M. \nM., Li, T., Loder, E. W., Mayo-Wilson, E., McDonald, S., … Moher, D. (2021). The PRISMA 2020 state-\nment: An updated guideline for reporting systematic reviews. BMJ, 372(71). \nhttps://doi.org/10.1136/bmj.n71  \nPhong, M. V., Nguyen, T. T., Pham, H. V., & Nguyen, T. T. (2015). Mining user opinions in mobile app re-\nviews: A keyword-based approach (T). Proceedings of the 30th IEEE/ACM International Conference on Auto-\nmated Software Engineering, Lincoln, NE, USA, 749–759. https://doi.org/10.1109/ASE.2015.85  \nRodrigues, P., Silva, I., Barbosa, G., Coutinho, F., & Mourão, F. (2017). Beyond the stars: Towards a novel sen-\ntiment rating to evaluate applications in web stores of mobile apps. Proceedings of the 26th International Confer-\nence on World Wide Web Companion (pp. 109–117). International World Wide Web Conferences Steering \nCommittee. https://doi.org/10.1145/3041021.3054139  \nRoumeliotis, K. I., Tselikas, N. D., & Nasiopoulos, D. K. (2024). LLMs in e-commerce: A comparative analysis \nof GPT and LLaMA models in product review evaluation. Natural Language Processing Journal, 6, 100056. \nhttps://doi.org/10.1016/j.nlp.2024.100056  \nSami, A. M., Rasheed, Z., Kemell, K.-K., Waseem, M., Kilamo, T., Saari, M., Duc, A. N., Systä, K., & Abra-\nhamsson, P. (2024). System for systematic literature review using multiple AI agents: Concept and an empirical evaluation . \nPsyArXiv. http://arxiv.org/abs/2403.08399 \nSamy, H., Helmy, A., & Ramadan, N. (2021). Aspect-based sentiment analysis of mobile apps reviews using \nclass association rules and LDA. Proceedings of the Tenth International Conference on Intelligent Computing and Infor-\nmation Systems, Cairo, Egypt, 183–189. https://doi.org/10.1109/ICICIS52592.2021.9694242  \nShah, F. A., Sabir, A., & Sharma, R. (2024). A fine-grained sentiment analysis of app reviews using large language models: \nAn evaluation study. PsyArXiv. http://arxiv.org/abs/2409.07162  \nSrisopha, K., Phonsom, C., Li, M., Link, D., & Boehm, B. (2020). On building an automatic identification of \ncountry-specific feature requests in mobile app reviews: Possibilities and challenges. Proceedings of the \nArambepola, Munasinghe, & Wimalasena \n21 \nIEEE/ACM 42nd International Conference on Software Engineering Workshops (pp. 494–498). Association for \nComputing Machinery. https://doi.org/10.1145/3387940.3391492  \nSultana, R., & Sarker, S. (2018). App review mining and summarization. International Journal of Computer Applica-\ntions, 179(38), 45–52. https://doi.org/10.5120/ijca2018916918  \nUpadhye, A. (2024). Sentiment analysis using large language models: Methodologies, applications, and chal-\nlenges. International Journal of Computer Applications, 186(20), 30–34. https://doi.org/10.5120/ijca2024923625 \nVillarroel, L., Bavota, G., Russo, B., Oliveto, R., & Di Penta, M. (2016). Release planning of mobile apps based \non user reviews. Proceedings of the 38th International Conference on Software Engineering (pp. 14–24). Association \nfor Computing Machinery. https://doi.org/10.1145/2884781.2884818  \nWang, L., Nakagawa, H., & Tsuchiya, T. (2020). Opinion Analysis and Organization of Mobile Application \nUser Reviews. REFSQ Workshops. \nWang, Z., Zhu, Y., & Zhang, Q. (2024). LLM for sentiment analysis in e-commerce: A deep dive into customer \nfeedback. Applied Science and Engineering Journal for Advanced Research, 3(4), 8–13. https://doi.org/10.5281/ze-\nnodo.12730477 \nXu, X., Zhang, J.-D., Xiao, R., & Xiong, L. (2023). The limits of ChatGPT in extracting aspect-category-opinion-sentiment \nquadruples: A comparative analysis. PsyArXiv. https://doi.org/10.48550/arXiv.2310.06502 \nYang, C., Wu, L., Yu, C., & Zhou, Y. (2021). A phrase-level user requests mining approach in mobile applica-\ntion reviews: Concept, framework, and operation. Information, 12(5), 177. \nhttps://doi.org/10.3390/info12050177  \nYang, T., Gao, C., Zang, J., Lo, D., & Lyu, M. (2021). TOUR: Dynamic topic and sentiment analysis of user \nreviews for assisting app release. Companion Proceedings of the Web Conference (pp. 708–712). Association for \nComputing Machinery. https://doi.org/10.1145/3442442.3458612  \nYu, D., Mu, Y., & Jin, Y. (2017). Rating prediction using review texts with underlying sentiments. Information \nProcessing Letters, 117, 10–18. https://doi.org/10.1016/j.ipl.2016.08.002 \nZhang, W., Deng, Y., Liu, B., Pan, S. J., & Bing, L. (2024). Sentiment analysis in the era of large language mod-\nels: A reality check. Findings of the Association for Computational Linguistics (pp. 3881–3906). Association for \nComputational Linguistics. https://doi.org/10.18653/v1/2024.findings-naacl.246 \nZhao, Y., Hou, X., Wang, S., & Wang, H. (2024). LLM app store analysis: A vision and roadmap. ACM Transac-\ntions on Software Engineering and Methodology. https://doi.org/10.1145/3708530 \n \nAUTHORS \nNimasha Arambepola is a Probationary Lecturer at the Software Engi-\nneering Teaching Unit, Faculty of Science, University of Kelaniya, Sri \nLanka. She earned her BSc (Hons) in Software Engineering with First \nClass Honours in 2020 from the same university, and is currently pursu-\ning an MPhil in Software Engineering. Her research interests include \nSoftware Engineering, Human-Computer Interaction (HCI), User Factor \nAnalysis, Information Extraction, and Natural Language Processing. \n \n \n\nLarge Language Models  \n22 \nDr Lankeshwara Munasinghe is a Lecturer in Cyber Security at the \nSchool of Computing, Engineering, and Technology, Robert Gordon \nUniversity, with a strong background in cybersecurity, AI security, and \nresearch software engineering. His research focuses on cybersecurity \nknowledge graphs, AI-driven security solutions, and social network secu-\nrity, particularly in misinformation spread and anomaly detection. Dr. \nMunasinghe holds a PhD in Informatics from The Graduate University \nfor Advanced Studies (SOKENDAI), Japan, and a BSc (Hons) in Statis-\ntics & Computing from the University of Kelaniya. His professional expe-\nrience includes roles as a Senior Lecturer at the University of Kelaniya, a \nVisiting Researcher at Hokkaido University, and a Senior Staff Engineer \nat Motorola Solutions, where he contributed to human activity recognition systems for security appli-\ncations. \n \n \nR. A. W. L. Wimalasena is a Temporary Demonstrator at the Software \nEngineering Teaching Unit, University of Kelaniya, Sri Lanka. She holds \na First Class BSc (Hons) degree in Software Engineering from the same \nuniversity. Her research interests focus on Artificial Intelligence, particu-\nlarly Deep Learning and Generative AI, and sustainable web development \ntechnologies. She has contributed to academic publications and remains \nactively engaged in research within her areas of expertise. \n \n",
  "topic": "Mobile apps",
  "concepts": [
    {
      "name": "Mobile apps",
      "score": 0.602643609046936
    },
    {
      "name": "Computer science",
      "score": 0.548126220703125
    },
    {
      "name": "Natural language processing",
      "score": 0.32463401556015015
    },
    {
      "name": "World Wide Web",
      "score": 0.2378183901309967
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I142651801",
      "name": "University of Kelaniya",
      "country": "LK"
    },
    {
      "id": "https://openalex.org/I522815984",
      "name": "Robert Gordon University",
      "country": "GB"
    }
  ],
  "cited_by": 1
}