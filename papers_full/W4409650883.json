{
  "title": "Adversarial Attacks on Agentic AI Systems: Mechanisms, Impacts, and Defense Strategies",
  "url": "https://openalex.org/W4409650883",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5111027262",
      "name": "Pradipta Kishore Chakrabarty",
      "affiliations": [
        "University of Richmond"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4404088011",
    "https://openalex.org/W4408042000",
    "https://openalex.org/W4408670415",
    "https://openalex.org/W2969804742",
    "https://openalex.org/W4388516011"
  ],
  "abstract": null,
  "full_text": "International Journal of \nScience and Research (IJSR)\n \nISSN: 2319\n-\n7064\n \nImpact Factor 2024: 7.101\n \nVolume 14 Issue 4, April 2025\n \nFully Refereed | Open Access | Double Blind Peer Reviewed Journal\n \nwww.ijsr.net\n \nAdversarial Attacks on Agentic AI Systems: \nMechanisms, Impacts, and Defense Strategies\n \n \nPradipta Kishore Chakrabarty\n \n \nRichmond, VA, USA\n \n \n \nAbstract:\n \nThis study delves into the growing threat of adversarial attacks on agentic AI systems, highlighting their unique vulnerabili\nties \nowing to their complexity and expanded access privileges. Through theoretical and experimental analyses, it categorizes the a\nt\ntack vectors \nspecific to these systems and evaluates their impacts. This study identifies novel attack surfaces beyond traditional AI vuln\nerabilities, \nparticularly in systems with database access or critical decision\n-\nmaking capabilities \n[1]\n. This study proposes a multilayered defense \nframework to mitigate these threats, contributing significantly to agentic AI security. These insights are crucial for develo\nping secure and \ntrustworthy autonomous AI systems for rapidly evolving landscapes.\n \n \nKeywords:\n \nAgentic AI, \nAgentic AI Security, \nAdversarial Attacks, \nAdversarial Threats, Autonomous\n \nSystems, AI Security, Defense \nStrategies, Threat Modeling, Adversarial Machine Learning, Cybersecurity, Attack Mitigation, AI Vulnerabilities, Prompt Injec\ntion, Agent \nManipulation\n, Multilayered defense strategies\n  \n \n \n1.\n \nIntroduction\n \n \nThe Rise and Significance of Agentic AI Systems\n \nAgentic AI systems represent a significant advancement in \nartificial intelligence, distinguished by their autonomy, \ndecision\n-\nmaking capabilities, and capacity to interact with \nvarious systems on behalf of users. These systems are being \nincreasingly impleme\nnted across multiple domains, including \ncybersecurity, healthcare, finance, and critical infrastructure. \nUnlike traditional AI systems, agentic AI possesses the \nauthority to make decisions and take actions with minimal \nhuman intervention, thereby significa\nntly enhancing both \ntheir utility and potential security implications.\n[2]\n \n \n \nThe Evolving Landscape of Adversarial Attacks\n \nAdversarial attacks have long presented significant \nchallenges to AI systems by utilizing techniques that examine \nneural network parameters to identify input modifications that \ncan change outcomes. However, the emergence of agentic AI \nhas introduced new at\ntack vectors and vulnerabilities that go \nbeyond traditional adversarial machine\n-\nlearning threats. \nThese attacks can target various stages of the AI lifecycle, \nfrom training to inference, with diverse objectives, such as \nintegrity violations, availability d\nisruptions, and privacy \nbreaches.\n \n \nUnique Vulnerabilities of Agentic AI Systems\n \nAI systems with agentic properties present notable security \nchallenges because of their advanced functionalities and \naccess levels. The intricate nature of these systems, along \nwith their capacity to handle and evaluate large volumes of \ndata, increases the\n \nchance of data leaks or breaches, whether \nunintentional or caused by malicious interference. As AI \nagents gain independence, their ability to circumvent or \nexploit security protocols has become a growing concern.\n \n \n2.\n \nResearch Objectives and Questions\n \n \nThis study seeks to explore several critical questions \nconcerning adversarial attacks on agentic AI systems. \n \n1)\n \nWhat types of adversarial attacks can malicious actors \nsuccessfully execute against these systems?\n \n2)\n \nWhat knowledge and resources do attackers possess to \ncarry out such attacks?\n \n3)\n \nHow effective might these attacks be and what potential \nimpacts could they have?\n \n4)\n \nWhich defense strategies can effectively mitigate these \nattacks?\n \n \n3.\n \nSignificance of the Research\n \n \nIt is important to understand how adversarial attacks work, \ntheir effects, and defend against them in AI systems. This \nresearch will help in the new field of AI security, offering \nuseful information for developers, organizations, and \npolicymakers who want \nto use AI systems safely and \nresponsibly.\n \n \n4.\n \nMethodology\n \n \nResearch Design\n \nOur research employed a comprehensive methodology that \nintegrates theoretical analysis, experimental evaluation, and \ncase studies to examine adversarial attacks on agentic AI \nsystems. This multifaceted approach facilitates a \nthorough \ninvestigation of attack mechanisms, their impact, and \npotential defense strategies.\n \n \nTaxonomy Development\n \nWe adapted NIST taxonomy for adversarial machine learning \n[3]\n  \nto specifically address agentic AI systems. Our taxonomy \ncategorizes attacks based on the following criteria. \n \n•\n \nStages of learning (training, inference)\n \n•\n \nAttacker goals and objectives (integrity violation, \navailability breakdown, privacy \ncompromise)\n \n•\n \nAttacker capabilities and knowledge (model control, data \ncontrol, and query access). \n \n \nThis classification framework, as illustrated in the NIST \nreport \n[3]\n, offers a structured approach for understanding the \nlandscape of adversarial attacks on agentic AI systems.\n \n \n \nPaper ID: SR25417074844\nDOI: https://dx.doi.org/10.21275/SR25417074844\n1367 \nInternational Journal of \nScience and Research (IJSR)\n \nISSN: 2319\n-\n7064\n \nImpact Factor 2024: 7.101\n \nVolume 14 Issue 4, April 2025\n \nFully Refereed | Open Access | Double Blind Peer Reviewed Journal\n \nwww.ijsr.net\n \n5.\n \nExperimental Setup\n \n \nOur experimental setup involved testing various attack \nvectors against representative agentic AI systems, including\n \n•\n \nEvasion attacks designed to manipulate agent inputs and \nbypass security mechanisms\n \n•\n \nPoisoning attacks targeting training data or models\n \n•\n \nPrivacy attacks aimed at extracting sensitive information\n \n•\n \nAgent\n-\nspecific attacks focusing on goal manipulation \nand prompt injection\n \n \nImpact Assessment Framework\n \nWe developed metrics to measure the success rates of \ndifferent attacks and evaluate their operational impacts, \nsecurity breaches, and potential ripple effects on dependent \nsystems. This framework allows a comprehensive assessment \nof the severity and scope \nof adversarial attacks on agentic AI \nsystems.\n \n \nDefense Strategy Evaluation\n \nWe evaluated various defense strategies using a framework \nthat assessed prevention, detection, and mitigation \napproaches. This includes incorporating elements of the \nMAESTRO framework \nfor agentic AI threat modeling\n6\n \nand \nevaluating different defense approaches, such as enhanced \nthreat detection, automated incident management, and \nproactive defense with predictive capabilities.\n \n \nData Collection and Analysis\n \nOur research collected data from experimental results and \ncase studies using rigorous analytical \ntechniques to interpret \nthe findings and draw meaningful conclusions about the \nsecurity landscape of agentic AI systems.\n \n \nTaxonomy of Adversarial Attacks on Agentic AI Systems\n \nOur research broadens the NIST taxonomy of attacks on \npredictive AI systems to include distinct features of agentic \nAI. This taxonomy classifies attacks based on the attacker's \nobjectives (availability, integrity, privacy) as well as the \ncapabilities and k\nnowledge required \n[3]\n. This thorough \nclassification illustrates how traditional attack vectors are \nintensified in agentic systems owing to their autonomous \nnature and enhanced access capability.\n \n \nUnique Vulnerabilities of Agentic AI Systems\n \nThe analysis revealed several critical vulnerabilities that are \nspecific to agentic AI systems.\n \n•\n \nUnauthorized data retrieval due to agents' access to \ndatabase systems\n \n•\n \nExploitation of system vulnerabilities through \nautonomous decision\n-\nmaking\n \n•\n \nMisuse of personal or confidential data through access \npatterns\n \n•\n \nIncreased risk of data leaks through adversarial \nmanipulation \n[1]\n \n \nThe complexity of agentic AI systems, combined with their \nability to process and analyze large volumes of data, \nsignificantly increases the attack surface compared with \ntraditional AI systems.\n \n \nMechanisms of Adversarial Attacks\n \nWe found several ways in which attackers target agentic AI \nsystems.\n \n \nEvasion Attacks\n \nEvasion attacks on agentic AI systems involve manipulation \nof inputs to induce erroneous decisions or actions. These \nattacks are particularly effective against agentic systems \nowing to their autonomous decision\n-\nmaking capabilities \n[3]\n. \nBoth black\n-\nbox and white\n-\nbox attack scenarios pose \nsignificant risks and require varying levels of attacker \nknowledge.\n \n \nPoisoning Attacks\n \nPoisoning attacks are directed at the training process or at data \nfrom agentic AI systems. These attacks encompass data \npoisoning, wherein adversaries manipulate the training data, \nand model poisoning, where the integrity of the model itself \nis compromised\n \n[3]\n. Backdoor attacks constitute a particularly \nconcerning variant because they enable attackers to embed \nhidden functionalities that can be activated under specific \nconditions.\n \n \nPrivacy Attacks\n \nPrivacy attacks are designed to extract sensitive information \nfrom agentic artificial intelligence (AI) systems. These \nattacks include model extraction, data reconstruction, and \nmembership inference \n[3]\n. The access privileges inherent to \nagentic systems render these attacks particularly concerning \nbecause they have the potential to expose sensitive user data \nor system information.\n \n \nAgent\n-\nSpecific Attacks\n \nOur research delineates attack vectors specific to agentic AI, \nincluding\n \n•\n \nGoal manipulation strategies that subvert the agent's \nintended objectives\n \n•\n \nPrompt injection techniques that exploit the agent's \ninterpretation of instructions\n \n•\n \nMethods for manipulating the agent's decision\n-\nmaking \nprocesses\n \n \nImpacts of Adversarial Attacks\n \nSuccessful adversarial attacks on agentic AI systems have far\n-\nreaching consequences.\n \n \nOperational Impacts\n \nAdversarial actions can impair the performance of agents, \ndisrupt their functionality, or undermine their goals and \nobjectives. This issue is particularly critical in domains in \nwhich agentic AI systems are responsible for making \nautonomous decisions.\n \n \nSecurity Impacts\n \nUnauthorized access to data, system breaches, and privilege \nescalation constitute significant security threats associated \nwith successful attacks on agentic AI systems \n[1]\n. These risks \nare exacerbated by the agents' access to sensitive systems and \ndata.\n \n \nTrust Impacts\n \nSuccessful attacks can undermine user confidence, raise \nethical concerns, and cause reputational harm. The \nPaper ID: SR25417074844\nDOI: https://dx.doi.org/10.21275/SR25417074844\n1368 \nInternational Journal of \nScience and Research (IJSR)\n \nISSN: 2319\n-\n7064\n \nImpact Factor 2024: 7.101\n \nVolume 14 Issue 4, April 2025\n \nFully Refereed | Open Access | Double Blind Peer Reviewed Journal\n \nwww.ijsr.net\n \nautonomous nature of agentic AI systems makes trust a \ncritical factor in their adoption and utilization.\n \n \nDefense Strategies and Their Effectiveness\n \nOur research evaluated several defense strategies for \nprotecting agentic AI systems.\n \n \nEnhanced Threat Detection and Response\n \nAI\n-\ndriven security platforms are capable of monitoring \nnetwork traffic and identifying anomalous patterns even in the \nabsence of established attack signatures \n[4]\n. These systems \ncan autonomously initiate responses by isolating \ncompromised devices or by obstructing suspicious traffic.\n \n \nAutomated Incident Management\n \nAutomated incident management systems have the potential \nto significantly enhance response times by coordinating \nworkflows across diverse security tools and employing \nmachine learning to ascertain appropriate responses \n[5]\n. This \nmethodology is particularly advantageous for addressing \nintricate attack vectors targeting agentic AI systems.\n \n \nProactive Defense with Predictive Capabilities\n \nProactive defense strategies employ artificial intelligence to \nanticipate potential cyberattacks by analyzing historical data, \nthreat intelligence, and real\n-\ntime activities. This predictive \ncapability allows organizations to fortify their defenses in \nadvan\nce, offering a significant advantage against \nsophisticated adversarial threats.\n \n \nContinuous Vulnerability Scanning\n \nContinuous monitoring and real\n-\ntime vulnerability \nmanagement are crucial to sustain a robust security posture in \nagentic AI systems. Automated vulnerability scanning \nthroughout the network facilitates the identification of weak \npoints and prioritization of\n \npatches based on risk assessment.\n \n \nCase Studies and Implementation Challenges\n \nOur research encompasses several case studies that illustrate \nsuccessful adversarial attacks on agentic AI systems along \nwith the implementation of defense strategies. These cases \nunderscore the efficacy of various attack vectors and the \ninherent challenge\ns in defending against them in real\n-\nworld \nscenarios.\n \n \nImplications for Development and Deployment\n \nThe findings of this study have significant implications for the \ndevelopment and deployment of agentic AI systems.\n \n•\n \nSecurity considerations must be integrated throughout \nthe AI development lifecycle\n \n•\n \nOrganizations must implement robust monitoring and \ncontrol mechanisms\n \n•\n \nPolicy and regulatory frameworks should address the \nunique security challenges posed by agentic AI systems\n \n \n6.\n \nConclusion\n \n \nSummary of Key Findings\n \nOur research indicates that agentic AI systems encounter \ndistinct and substantial security challenges owing to their \nautonomous nature and enhanced access capability. The \nmechanisms of adversarial attacks on these systems surpass \nthose of traditional AI vulnerabilities, necessitating \nspecialized defense strategies tailored to their specific \ncharacteristics.\n \n \nCritical Challenges and Considerations\n \nThe autonomous decision\n-\nmaking capabilities of agentic AI \nsystems pose unique security challenges that \nfundamentally \ndiffer from those of traditional AI systems. These challenges \nare exacerbated by the systems' access to sensitive data and \ncritical infrastructure, thereby increasing the potential impact \nof successful attacks.\n \n \nRecommendations for Security Framework Development\n \nWe advocate the creation of specialized security frameworks \ntailored to agentic AI systems to address their distinct \nvulnerabilities and potential attack vectors. Such frameworks \nshould integrate advanced threat detection with predictive \ncapabilities, auto\nmated incident management, and continuous \nvulnerability scanning.\n \n \n7.\n \nLimitations and Future Research Directions\n \n \nAlthough this research provides valuable insights into the \nsecurity landscape of agentic AI systems, several areas \nwarrant further investigation.\n \n•\n \nDevelopment of standardized security benchmarks for \nagentic AI systems\n \n•\n \nInvestigation of novel defense techniques specifically \ndesigned for autonomous agents\n \n•\n \nExploration of the ethical and societal implications of \nadversarial attacks on agentic AI\n \n•\n \nExamination of the evolving attack landscape as agentic \nAI technology advances\n \n \nAs agentic AI systems continue to advance and become \nincreasingly integrated into critical infrastructure and \ndecision\n-\nmaking processes, it is imperative to comprehend \nand address their security vulnerabilities to ensure safe and \nresponsible deployment.\n \n \nReferences\n \n \n[1]\n \nR. Khan, S. Sarkar, S. Mahata, and E. Jose, “Security \nThreats in Agentic AI System.” Oct. 16, 2024. doi: \n10.48550/arxiv.2410.14728.\n \n[2]\n \nN. Kshetri\n, “Transforming Cybersecurity with Agentic \nAi to Combat Emerging Cyber Threats.” elsevier bv, \nJan. 01, 2025. doi: 10.2139/ssrn.5159598.\n \n[3]\n \nVassilev, “Adversarial Machine Learning,” national \ninstitute of standards technology, Jan. 2025. doi: \n10.6028/nist.ai.100\n-\n2e2025.\n \n[4]\n \nS. Xu, Y. Qian, and R. Q. Hu, “Data\n-\nDriven Edge \nIntelligence for Robust Network Anomaly Detection,” \nIEEE Transactions on Network Science and \nEngineering\n, vol. 7, no. 3, pp. 1481\n–\n1492, Jul. 2020, \ndoi: 10.1109/tnse.2019.2936466.\n \n[5]\n \nR. Sinha, T. M. M. Victor, and K. Singla, “Artificial \nIntelligence and Machine Learning for Cybersecurity \nApplications and Challenges,” igi global, 2023, pp. \n109\n–\n146. doi: 10.4018/978\n-\n1\n-\n6684\n-\n9317\n-\n5.ch007.\n \n \n \nPaper ID: SR25417074844\nDOI: https://dx.doi.org/10.21275/SR25417074844\n1369 ",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.7668219804763794
    },
    {
      "name": "Computer science",
      "score": 0.4447399973869324
    },
    {
      "name": "Cognitive science",
      "score": 0.4376324415206909
    },
    {
      "name": "Computer security",
      "score": 0.4376254081726074
    },
    {
      "name": "Psychology",
      "score": 0.2933048605918884
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2619420289993286
    }
  ],
  "institutions": [],
  "cited_by": 1
}