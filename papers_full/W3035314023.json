{
  "title": "On the Computational Power of Transformers and Its Implications in Sequence Modeling",
  "url": "https://openalex.org/W3035314023",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2887141342",
      "name": "Satwik Bhattamishra",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2989693568",
      "name": "Arkil Patel",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2130066203",
      "name": "Navin Goyal",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2995273672",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2914557243",
    "https://openalex.org/W1581242383",
    "https://openalex.org/W1496937667",
    "https://openalex.org/W2072160811",
    "https://openalex.org/W2964189376",
    "https://openalex.org/W2963059228",
    "https://openalex.org/W2949292434",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3034979923",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2983902802",
    "https://openalex.org/W2916877561",
    "https://openalex.org/W3034801370",
    "https://openalex.org/W2962749806",
    "https://openalex.org/W3033357972",
    "https://openalex.org/W2951977278",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3036369012",
    "https://openalex.org/W2963706817",
    "https://openalex.org/W3204406378",
    "https://openalex.org/W1995341919",
    "https://openalex.org/W2963753324"
  ],
  "abstract": "Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their power and inherent limitations is still nascent. In particular, the roles of various components in Transformers such as positional encodings, attention heads, residual connections, and feedforward networks, are not clear. In this paper, we take a step towards answering these questions. We analyze the computational power as captured by Turing-completeness. We first provide an alternate and simpler proof to show that vanilla Transformers are Turing-complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing-complete. We further analyze the necessity of each component for the Turing-completeness of the network; interestingly, we find that a particular type of residual connection is necessary. We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks.",
  "full_text": "Proceedings of the 24th Conference on Computational Natural Language Learning, pages 455–475\nOnline, November 19-20, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n455\nOn the Computational Power of Transformers and its\nImplications in Sequence Modeling\nSatwik Bhattamishra Arkil Patel Navin Goyal\nMicrosoft Research India\n{t-satbh,t-arkpat,navingo}@microsoft.com\nAbstract\nTransformers are being used extensively\nacross several sequence modeling tasks. Sig-\nniﬁcant research effort has been devoted to\nexperimentally probe the inner workings of\nTransformers. However, our conceptual and\ntheoretical understanding of their power and\ninherent limitations is still nascent. In partic-\nular, the roles of various components in Trans-\nformers such as positional encodings, atten-\ntion heads, residual connections, and feedfor-\nward networks, are not clear. In this paper,\nwe take a step towards answering these ques-\ntions. We analyze the computational power\nas captured by Turing-completeness. We ﬁrst\nprovide an alternate and simpler proof to show\nthat vanilla Transformers are Turing-complete\nand then we prove that Transformers with only\npositional masking and without any positional\nencoding are also Turing-complete. We further\nanalyze the necessity of each component for\nthe Turing-completeness of the network; inter-\nestingly, we ﬁnd that a particular type of resid-\nual connection is necessary. We demonstrate\nthe practical implications of our results via\nexperiments on machine translation and syn-\nthetic tasks.\n1 Introduction\nTransformer (Vaswani et al., 2017) is a recent self-\nattention based sequence-to-sequence architecture\nwhich has led to state of the art results across vari-\nous NLP tasks including machine translation (Ott\net al., 2018), language modeling (Radford et al.,\n2018) and question answering (Devlin et al., 2019).\nAlthough a number of variants of Transformers\nhave been proposed, the original architecture still\nunderlies these variants.\nWhile the training and generalization of machine\nlearning models such as Transformers are the cen-\ntral goals in their analysis, an essential prerequisite\nto this end is characterization of the computational\nPOS(1) POS(2) POS(3)\n(a) (b)\nFigure 1: (a) Self-Attention Network with positional\nencoding, (b) Self-Attention Network with positional\nmasking without any positional encoding\npower of the model: training a model for a certain\ntask cannot succeed if the model is computation-\nally incapable of carrying out the task. While the\ncomputational capabilities of recurrent networks\n(RNNs) have been studied for decades (Kolen and\nKremer, 2001; Siegelmann, 2012), for Transform-\ners we are still in the early stages.\nThe celebrated work of Siegelmann and Sontag\n(1992) showed, assuming arbitrary precision, that\nRNNs are Turing-complete, meaning that they are\ncapable of carrying out any algorithmic task for-\nmalized by Turing machines. Recently, P´erez et al.\n(2019) have shown that vanilla Transformers with\nhard-attention can also simulate Turing machines\ngiven arbitrary precision. However, in contrast to\nRNNs, Transformers consist of several components\nand it is unclear which components are necessary\nfor its Turing-completeness and thereby crucial to\nits computational expressiveness.\nThe role of various components of the Trans-\nformer in its efﬁcacy is an important question for\nfurther improvements. Since the Transformer does\nnot process the input sequentially, it requires some\nform of positional information. Various positional\nencoding schemes have been proposed to capture\norder information (Shaw et al., 2018; Dai et al.,\n2019; Huang et al., 2018). At the same time, on\n456\nmachine translation, Yang et al. (2019) showed that\nthe performance of Transformers with only posi-\ntional masking (Shen et al., 2018) is comparable to\nthat with positional encodings. In case of positional\nmasking (Fig. 1), as opposed to explicit encodings,\nthe model is only allowed to attend over preceding\ninputs and no additional positional encoding vec-\ntor is combined with the input vector. Tsai et al.\n(2019) raised the question of whether explicit en-\ncoding is necessary if positional masking is used.\nAdditionally, since P ´erez et al. (2019)’s Turing-\ncompleteness proof relied heavily on residual con-\nnections, they asked whether these connections are\nessential for Turing-completeness. In this paper,\nwe take a step towards answering such questions.\nBelow, we list the main contributions of the paper,\n•We provide an alternate and arguably simpler\nproof to show that Transformers are Turing-\ncomplete by directly relating them to RNNs.\n•More importantly, we prove that Transform-\ners with positional masking and without posi-\ntional encoding are also Turing-complete.\n•We analyze the necessity of various compo-\nnents such as self-attention blocks, residual\nconnections and feedforward networks for\nTuring-completeness. Figure 2 provides an\noverview.\n•We explore implications of our results on ma-\nchine translation and synthetic tasks.1\n2 Related Work\nComputational Power of neural networks has\nbeen studied since the foundational paper Mc-\nCulloch and Pitts (1943); in particular, among\nsequence-to-sequence models, this aspect of RNNs\nhas long been studied (Kolen and Kremer, 2001).\nThe seminal work by Siegelmann and Sontag\n(1992) showed that RNNs can simulate a Turing\nmachine by using unbounded precision. Chen et al.\n(2018) showed that RNNs with ReLU activations\nare also Turing-complete. Many recent works have\nexplored the computational power of RNNs in prac-\ntical settings. Several works (Merrill et al., 2020),\n(Weiss et al., 2018) recently studied the ability of\nRNNs to recognize counter-like languages. The ca-\npability of RNNs to recognize strings of balanced\n1We have made our source code available at\nhttps://github.com/satwik77/Transformer-Computation-\nAnalysis.\nparantheses has also been studied (Sennhauser and\nBerwick, 2018; Skachkova et al., 2018). However,\nsuch analysis on Transformers has been scarce.\nTheoretical work on Transformerswas initiated\nby P´erez et al. (2019) who formalized the notion of\nTransformers and showed that it can simulate a Tur-\ning machine given arbitrary precision. Concurrent\nto our work, there have been several efforts to un-\nderstand self-attention based models (Levine et al.,\n2020; Kim et al., 2020). Hron et al. (2020) show\nthat Transformers behave as Gaussian processes\nwhen the number of heads tend to inﬁnity. Hahn\n(2020) showed some limitations of Transformer\nencoders in modeling regular and context-free lan-\nguages. It has been recently shown that Transform-\ners are universal approximators of sequence-to-\nsequence functions given arbitrary precision (Yun\net al., 2020). However, these are not applicable 2\nto the complete Transformer architecture. With a\ngoal similar to ours, Tsai et al. (2019) attempted\nto study the attention mechanism via a kernel for-\nmulation. However, a systematic study of various\ncomponents of Transformers has not been done.\n3 Deﬁnitions and Preliminaries\nAll the numbers used in our computations will be\nfrom the set of rational numbers denoted Q. For\na sequence X = ( x1,..., xn), we set Xj :=\n(x1,..., xj) for 1 ≤ j ≤ n. We will work\nwith an alphabet Σ of size m, with special sym-\nbols # and $ signifying the beginning and end of\nthe input sequence, respectively. The symbols are\nmapped to vectors via a given ‘base’ embedding\nfb : Σ →Qdb, where db is the dimension of the\nembedding. E.g., this embedding could be the one\nused for processing the symbols by the RNN.\nWe set fb(#) = 0db and fb($) = 0db. Posi-\ntional encoding is a function pos : N →Qdb. To-\ngether, these provide embedding for a symbol sat\nposition i given by f(fb(s),pos(i)), often taken\nto be simply fb(s) + pos(i). Vector JsK ∈Qm\ndenotes one-hot encoding of a symbol s∈Σ.\n3.1 RNNs\nWe follow Siegelmann and Sontag (1992) in\nour deﬁnition of RNNs. To feed the sequences\n2Hahn (2020) and Yun et al. (2020) study encoder-only seq-\nto-seq models with ﬁxed length outputs in which the computa-\ntion halts as soon as the last symbol of the input is processed.\nOur work is about the full Transformer (encoder and decoder)\nwhich is a seq-to-seq model with variable length sequence\noutput in which the decoder starts operating sequentially after\nthe encoder.\n457\ns1s2 ...s n ∈ Σ∗ to the RNN, these are con-\nverted to the vectors x1,x2,..., xn where xi =\nfb(si). The RNN is given by the recurrence ht =\ng(Whht−1 + Wxxt + b), where t ≥1, function\ng(·) is a multilayer feedforward network (FFN)\nwith activation σ, bias vector b ∈Qdh, matrices\nWh ∈Qdh×dh and Wx ∈Qdh×db, and ht ∈Qdh\nis the hidden state with given initial hidden state\nh0; dh is the hidden state dimension.\nAfter the last symbol sn has been fed, we con-\ntinue to feed the RNN with the terminal symbol\nfb($) until it halts. This allows the RNN to carry\nout computation after having read the input.\nA class of seq-to-seq neural networks is Turing-\ncomplete if the class of languages recognized by\nthe networks is exactly the class of languages rec-\nognized by Turing machines.\nTheorem 3.1. (Siegelmann and Sontag, 1992) Any\nseq-to-seq function Σ∗ → Σ∗ computable by a\nTuring machine can also be computed by an RNN.\nFor details please see section B.1 in appendix.\n3.2 Transformer Architecture\nVanilla Transformer. We describe the original\nTransformer architecture with positional encoding\n(Vaswani et al., 2017) as formalized by P´erez et al.\n(2019), with some modiﬁcations. All vectors in\nthis subsection are from Qd.\nThe transformer, denoted Trans, is a seq-to-seq\narchitecture. Its input consists of (i) a sequence\nX = ( x1,..., xn) of vectors, (ii) a seed vector\ny0. The output is a sequence Y = ( y1,..., yr)\nof vectors. The sequence X is obtained from\nthe sequence (s1,...,s n) ∈ Σn of symbols by\nusing the embedding mentioned earlier: xi =\nf(fb(si),pos(i)).\nThe transformer consists of composition oftrans-\nformer encoder and transformer decoder. For the\nfeedforward networks in the transformer layers we\nuse the activation as in Siegelmann and Sontag\n(1992), namely the saturated linear activation func-\ntion σ(x) which takes value 0 for x <0, value x\nfor 0 <x< 1 and value 1 for x> 1. This activa-\ntion can be easily replaced by the standard ReLU\nactivation via σ(x) = ReLU(x) −ReLU(x−1).\nSelf-attention. The self-attention mechanism\ntakes as input (i) a query vector q, (ii) a sequence\nof key vectors K = ( k1,..., kn), and (iii) a se-\nquence of value vectors V = (v1,..., vn). The\nq-attention over Kand V, denoted Att(q,K,V),\nis a vectora= α1v1+α2v2+···+αnvn, where (i)\n(α1,...,α n) = ρ(fatt(q,k1),...,f att(q,kn)).\n(ii) The normalization function ρ: Qn →Qn\n≥0 is\nhardmax: for x= (x1,...,x n) ∈Qn, if the maxi-\nmum value occurs rtimes among x1,...,x n, then\nhardmax(x)i := 1 /r if xi is a maximum value\nand hardmax(x)i := 0 otherwise. In practice, the\nsoftmax is often used but its output values are in\ngeneral not rational.\n(iii) For vanilla transformers, the scoring function\nfatt used is a combination of multiplicative atten-\ntion (Vaswani et al., 2017) and a non-linear func-\ntion: fatt(q,ki) = −\n⏐⏐⟨q,ki⟩\n⏐⏐. This was also used\nby P´erez et al. (2019).\nTransformer encoder. A single-layer encoder\nis a function Enc(X; θ), with input X =\n(x1,..., xn) a sequence of vectors in Qd, and\nparameters θ. The output is another sequence\nZ = (z1,..., zn) of vectors in Qd. The parame-\nters θspecify functions Q(·),K(·),V (·), and O(·),\nall of type Qd →Qd. The functions Q(·),K(·),\nand V(·) are linear transformations and O(·) an\nFFN. For 1 ≤i≤n, the output of the self-attention\nblock is produced by\nai = Att(Q(xi),K(X),V (X)) + xi (1)\nThis operation is also referred to as the encoder-\nencoder attention block. The output Zis computed\nby zi = O(ai) + ai for 1 ≤ i ≤ n. The ad-\ndition operations +xi and +ai are the residual\nconnections. The complete L-layer transformer en-\ncoder TEnc(L)(X; θ) = (Ke,Ve) has the same\ninput X = ( x1,..., xn) as the single-layer en-\ncoder. In contrast, its output Ke = (ke\n1,..., ke\nn)\nand Ve = ( ve\n1,... ve\nn) contains two sequences.\nTEnc(L) is obtained by composition of Lsingle-\nlayer encoders: let X(0) := X, and for 0 ≤ℓ ≤\nL−1, let X(ℓ+1) = Enc( X(ℓ); θℓ) and ﬁnally,\nKe = K(L)(X(L)), Ve = V(L)(X(L)).\nTransformer decoder. The input to a single-\nlayer decoder is (i) (Ke,Ve) output by the en-\ncoder, and (ii) sequence Y = (y1,..., yk) of vec-\ntors for k ≥1. The output is another sequence\nZ= (z1,..., zk).\nSimilar to the single-layer encoder, a single-\nlayer decoder is parameterized by functions\nQ(·),K(·),V (·) and O(·) and is deﬁned by\npt = Att( Q(yt),K(Yt),V (Yt)) + yt, (2)\nat = Att( pt,Ke,Ve) + pt, (3)\nzt = O(at) + at,\nwhere 1 ≤t ≤k. The operation in (2) will be\n458\nreferred to as the decoder-decoder attention block\nand the operation in (3) as the decoder-encoder\nattention block. In (2), positional masking is ap-\nplied to prevent the network from attending over\nsymbols which are ahead of them.\nAn L-layer Transformer decoder\nTDecL((Ke,Ve),Y; θ) = z is obtained by\nrepeated application of L single-layer decoders\neach with its own parameters, and a transformation\nfunction F : Qd →Qd applied to the last vector in\nthe sequence of vectors output by the ﬁnal decoder.\nFormally, for0 ≤ℓ≤L−1 and Y0 := Y we have\nYℓ+1 = Dec((Ke,Ve),Yℓ; θℓ), z = F(yL\nk).\nNote that while the output of a single-layer decoder\nis a sequence of vectors, the output of an L-layer\nTransformer decoder is a single vector.\nThe complete Transformer. The output\nTrans(X,y0) = Y is computed by the recur-\nrence ˜yt+1 = TDec(TEnc(X),(y0,y1,..., yt)),\nfor 0 ≤t ≤r−1. We get yt+1 by adding posi-\ntional encoding: yt+1 = ˜yt+1 + pos(t+ 1).\nDirectional Transformer. We denote the Trans-\nformer with only positional masking and no posi-\ntional encodings as Directional Transformer and\nuse them interchangeably. In this case, we use stan-\ndard multiplicative attention as the scoring function\nin our construction, i.e, fatt(q,ki) = ⟨q,ki⟩. The\ngeneral architecture is the same as for the vanilla\ncase; the differences due to positional masking are\nthe following.\nThere are no positional encodings. So the\ninput vectors xi only involve fb(si). Simi-\nlarly, yt = ˜yt. In (1), Att(·) is replaced\nby Att(Q(xi),K(Xi),V (Xi)) where Xi :=\n(x1,..., xi) for 1 ≤ i ≤ n. Similarly, in (3),\nAtt(·) is replaced by Att(pt,Ke\nt,Ve\nt ).\nRemark 1. Our deﬁnitions deviate slightly\nfrom practice, hard-attention being the main one\nsince hardmax keeps the values rational whereas\nsoftmax takes the values to irrational space. Previ-\nous studies have shown that soft-attention behaves\nlike hard-attention in practice and Hahn (2020) dis-\ncusses its practical relevance.\nRemark 2. Transformer Networks with positional\nencodings are not necessarily equivalent in terms\nof their computational expressiveness (Yun et al.,\n2020) to those with only positional masking when\nconsidering the encoder only model (as used in\nBERT and GPT-2). Our results in Section 4.1 show\ntheir equivalence in terms of expressiveness for the\ncomplete seq-to-seq architecture.\n4 Primary Results\n4.1 Turing-Completeness Results\nIn light of Theorem 3.1, to prove that Transformers\nare Turing-complete, it sufﬁces to show that they\ncan simulate RNNs. We say that a Transformer\nsimulates an RNN (as deﬁned in Sec. 3.1) if on\nevery input s ∈Σ∗, at each step t, the vector yt\ncontains the hidden state ht as a subvector, i.e.\nyt = [ht,·], and halts at the same step as the RNN.\nTheorem 4.1. The class of Transformers with po-\nsitional encodings is Turing-complete.\nProof Sketch. The input s0,...,s n ∈Σ∗ is pro-\nvided to the transformer as the sequence of vectors\nx0,..., xn, where xi = [ 0dh,fb(si),0dh,i, 1],\nwhich has as sub-vector the given base embedding\nfb(si) and the positional encoding i, along with\nextra coordinates set to constant values and will be\nused later.\nThe basic observation behind our construction of\nthe simulating Transformer is that the transformer\ndecoder can naturally implement the recurrence\noperations of the type used by RNNs. To this end,\nthe FFN Odec(·) of the decoder, which plays the\nsame role as the FFN component of the RNN, needs\nsequential access to the input in the same way as\nRNN. But the Transformer receives the whole input\nat the same time. We utilize positional encoding\nalong with the attention mechanism to isolate xt\nat time tand feed it to Odec(·), thereby simulating\nthe RNN.\nAs stated earlier, we append the inputs1,...,s n\nof the RNN with $’s until it halts. Since the Trans-\nformer takes its input all at once, appending by $’s\nis not possible (in particular, we do not know how\nlong the computation would take). Instead, we ap-\npend the input with a single $. After encountering\na $ once, the Transformer will feed (encoding of)\n$ to Odec(·) in subsequent steps until termination.\nHere we conﬁne our discussion to the case t≤n;\nthe t>n case is slightly different but simpler.\nThe construction is straightforward: it has only\none head, one encoder layer and one decoder layer;\nmoreover, the attention mechanisms in the encoder\nand the decoder-decoder attention block of the de-\ncoder are trivial as described below.\nThe encoder attention layer does trivial com-\nputation in that it merely computes the iden-\ntity function: zi = xi, which can be easily\nachieved, e.g. by using the residual connec-\ntion and setting the value vectors to 0. The ﬁ-\n459\nEncoder-Encoder \nAttention Head\nFeed Forward \nNetwork\nInput\nEmbedding\nDecoder-Decoder\nAttention Head\nDecoder-Encoder\nAttention Head\nFeed Forward \nNetwork\nOutput\nEmbedding\nPositional \nEncoding\nPositional \nEncoding\nFigure 2: Transformer network with various com-\nponents highlighted. The components marked red\nare essential for the Turing-completeness whereas for\nthe pairs of blocks and residual connections marked\ngreen, either one of the component is enough. The\ndashed residual connection is not necessary for Turing-\ncompleteness of the network.\nnal K(1)(·) and V(1)(·) functions bring (Ke,Ve)\ninto useful forms by appropriate linear transfor-\nmations: ki = [ 0db,0db,0db,−1,i] and vi =\n[0db,fb(si),0db,0,0]. Thus, the key vectors only\nencode the positional information and the value\nvectors only encode the input symbols.\nThe output sequence of the decoder isy1,y2,... .\nOur construction will ensure, by induction on t,\nthat yt contains the hidden states ht of the RNN\nas a sub-vector along with positional information:\nyt = [ht,0db,0db,t+ 1,1]. This is easy to arrange\nfor t= 0, and assuming it fortwe prove it fort+1.\nAs for the encoder, the decoder-decoder attention\nblock acts as the identity: pt = yt. Now, using the\nlast but one coordinate in yt representing the time\nt+ 1, the attention mechanism Att(pt,Ke,Ve)\ncan retrieve the embedding of the t-th input sym-\nbol xt. This is possible because in the key vector\nki mentioned above, almost all coordinates other\nthan the one representing the position iare set to\n0, allowing the mechanism to only focus on the\npositional information and not be distracted by the\nother contents of pt = yt: the scoring function has\nvalue fatt(pt,ki) = −|⟨pt,ki⟩|= −|i−(t+ 1)|.\nFor a given t, it is maximized at i = t+ 1 for\nt<n and at i= nfor t≥n. This use of scoring\nfunction is similar to P´erez et al. (2019).\nAt this point, Odec(·) has at its disposal the hid-\nden state ht (coming from yt via pt and the resid-\nual connection) and the input symbol xt (coming\nvia the attention mechanism and the residual con-\nnection). Hence O(·) can act just like the FFN\n(Lemma C.4) underlying the RNN to computeht+1\nand thus yt+1, proving the induction hypothesis.\nThe complete construction can be found in Sec. C.2\nin the appendix.\nTheorem 4.2. The class of Transformers with posi-\ntional masking and no explicit positional encodings\nis Turing-complete.\nProof Sketch. As before, by Theorem 3.1 it suf-\nﬁces to show that Transformers can simulate RNNs.\nThe input s0,...,s n is provided to the transformer\nas the sequence of vectors x0,..., xn, where\nxi = [ 0dh,0dh,fb(si),JsiK,0,0m,0m,0m]. The\ngeneral goal for the directional case is similar to\nthe vanilla case, namely we would like the FFN\nOdec(·) of the decoder to directly simulate the com-\nputation in the underlying RNN. In the vanilla case,\npositional encoding and the attention mechanism\nhelped us feed input xt at the t-th iteration of the\ndecoder to Odec(·). However, we no longer have\nexplicit positional information in the input xt such\nas a coordinate with value t. The key insight is that\nwe do not need the positional information explic-\nitly to recover xt at step t: in our construction, the\nattention mechanism with masking will recover xt\nin an indirect manner even though it’s not able to\n“zero in” on the t-th position.\nLet us ﬁrst explain this without details of the con-\nstruction. We maintain in vector ωt ∈Qm, with a\ncoordinate each for symbols in Σ, the fraction of\ntimes the symbol has occurred up to stept. Now, at\na step t≤n, for the difference ωt −ωt−1 (which\nis part of the query vector), it can be shown easily\nthat only the coordinate corresponding to st is pos-\nitive. Thus after applying the linearized sigmoid\nσ(ωt −ωt−1), we can isolate the coordinate corre-\nsponding to st. Now using this query vector, the\n(hard) attention mechanism will be able to retrieve\nthe value vectors for all indices jsuch that sj = st\nand output their average. Crucially, the value vec-\ntor for an index jis essentially xj which depends\nonly on sj. Thus, all these vectors are equal to xt,\nand so is their average. This recovers xt, which\n460\ncan now be fed to Odec(·), simulating the RNN.\nWe now outline the construction and relate it\nto the above discussion. As before, for simplic-\nity we restrict to the case t ≤ n. We use only\none head, one layer encoder and two layer decoder.\nThe encoder, as in the vanilla case, does very lit-\ntle other than pass information along. The vec-\ntors in (Ke,Ve) are obtained by the trivial atten-\ntion mechanism followed by simple linear transfor-\nmations: ke\ni = [0dh,0dh,0db,JsiK,0,0m,0m,0m]\nand ve\ni = [0dh,0dh,fb(si),0m,0,0m,JsiK,0m].\nOur construction ensures that at step twe have\nyt = [ ht−1,0dh,0db,0m, 1\n2t ,0m,0m,ωt−1]. As\nbefore, the proof is by induction on t.\nIn the ﬁrst layer of decoder, the decoder-\ndecoder attention block is trivial: p(1)\nt =\nyt. In the decoder-encoder attention block,\nwe give equal attention to all the t + 1 val-\nues, which along with Oenc(·), leads to z(1)\nt =\n[ht−1, 0dh, 0db,δt, 1\n2t+1 ,0m,0m, ωt], where\nessentially δt = σ(ωt −ωt−1), except with a\nchange for the last coordinate due to special status\nof the last symbol $ in the processing of RNN.\nIn the second layer, the decoder-decoder atten-\ntion block is again trivial with p(2)\nt = z(1)\nt . We\nremark that in this construction, the scoring func-\ntion is the standard multiplicative attention 3. Now\n⟨p(2)\nt ,ke\nj⟩= ⟨δt,JsjK⟩= δt,j, which is positive if\nand only if sj = st, as mentioned earlier. Thus\nattention weights in Att(p(2)\nt ,Ke\nt,Ve\nt ) satisfy\nhardmax(⟨p(2)\nt ,ke\n1⟩,..., ⟨p(2)\nt ,ke\nt⟩) = 1\nλt (I(s0 =\nst),I(s1 = st),..., I(st = st)), where λt is a nor-\nmalization constant and I(·) is the indicator. See\nLemma D.3 for more details.\nAt this point, Odec(·) has at its disposal the hid-\nden state ht (coming from z(1)\nt via p(2)\nt and the\nresidual connection) and the input symbol xt (com-\ning via the attention mechanism and the residual\nconnection). Hence Odec(·) can act just like the\nFFN underlying the RNN to compute ht+1 and\nthus yt+1, proving the induction hypothesis.\nThe complete construction can be found in\nSec. D in the Appendix.\nIn practice, Yang et al. (2019) found that for\nNMT, Transformers with only positional masking\nachieve comparable performance compared to the\nones with positional encodings. Similar evidence\n3Note that it is closer to practice than the scoring function\n−|⟨q, k⟩|used in P´erez et al. (2019) and Theorem 4.1\nwas found by Tsai et al. (2019). Our proof for di-\nrectional transformers entails that there is no loss of\norder information if positional information is only\nprovided in the form of masking. However, we do\nnot recommend using masking as a replacement\nfor explicit encodings. The computational equiva-\nlence of encoding and masking given by our results\nimplies that any differences in their performance\nmust come from differences in learning dynamics.\n4.2 Analysis of Components\nThe results for various components follow from\nour construction in Theorem 4.1. Note that in both\nthe encoder and decoder attention blocks, we need\nto compute the identity function. We can nullify\nthe role of the attention heads by setting the value\nvectors to zero and making use of only the resid-\nual connections to implement the identity function.\nThus, even if we remove those attention heads, the\nmodel is still Turing-complete. On the other hand,\nwe can remove the residual connections around\nthe attention blocks and make use of the attention\nheads to implement the identity function by us-\ning positional encodings. Hence, either the atten-\ntion head or the residual connection is sufﬁcient to\nachieve Turing-completeness. A similar argument\ncan be made for the FFN in the encoder layer: ei-\nther the residual connection or the FFN is sufﬁcient\nfor Turing-completeness. For the decoder-encoder\nattention head, since it is the only way for the de-\ncoder to obtain information about the input, it is\nnecessary for the completeness. The FFN is the\nonly component that can perform computations\nbased on the input and the computations performed\nearlier via recurrence and hence, the model is not\nTuring-complete without it. Figure 2 summarizes\nthe role of different components with respect to the\ncomputational expressiveness of the network.\nProposition 4.3. The class of Transformers with-\nout residual connection around the decoder-\nencoder attention block is not Turing-complete.\nProof Sketch. We conﬁne our discussion to single-\nlayer decoder; the case of multilayer decoder is\nsimilar. Without the residual connection, the\ndecoder-encoder attention block produces at =\nAtt(pt,Ke,Ve) = ∑n\ni=1 αive\ni for some αi’s\nsuch that ∑n\ni αi = 1. Note that, without residual\nconnection at can take on at most 2n −1 values.\nThis is because by the deﬁnition of hard attention\nthe vector (α1,...,α n) is characterized by the set\nof zero coordinates and there are at most 2n −1\n461\nsuch sets (all coordinates cannot be zero). This\nrestriction on the number of values on at holds re-\ngardless of the value of pt. If the task requires the\nnetwork to produce values of at that come from\na set with size at least 2n, then the network will\nnot be able to perform the task. Here’s an exam-\nple task: given a number ∆ ∈(0,1), the network\nmust produce numbers 0,∆,2∆,...,k ∆, where k\nis the maximum integer such that k∆ ≤1. If the\nnetwork receives a single input ∆, then it is easy to\nsee that the vector at will be a constant (ve\n1) at any\nstep and hence the output of the network will also\nbe constant at all steps. Thus, the model cannot\nperform such a task. If the input is combined with\nn−1 auxiliary symbols (such as # and $), then in\nthe network, each at takes on at most 2n−1 values.\nHence, the model will be incapable of performing\nthe task if ∆ < 1/2n. Such a limitation does not\nexist with a residual connection since the vector\nat = ∑n\ni=1 αive\ni +pt can take arbitrary number of\nvalues depending on its prior computations in pt.\nFor further details, see Sec. C.1 in the Appendix.\nDiscussion. It is perhaps surprising that residual\nconnection, originally proposed to assist in the\nlearning ability of very deep networks, plays a vi-\ntal role in the computational expressiveness of the\nnetwork. Without it, the model is limited in its capa-\nbility to make decisions based on predictions in the\nprevious steps. We explore practical implications\nof this result in section 5.\n5 Experiments\nIn this section, we explore the practical implica-\ntions of our results. Our experiments are geared\ntowards answering the following questions:\nQ1. Are there any practical implications of the lim-\nitation of Transformers without decoder-encoder\nresidual connections? What tasks can they do or\nnot do compared to vanilla Transformers?\nQ2. Is there any additional beneﬁt of using posi-\ntional masking as opposed to absolute positional\nencoding (Vaswani et al., 2017)?\nAlthough we showed that Transformers without\ndecoder-encoder residual connection are not Turing\ncomplete, it does not imply that they are incapable\nof performing all the tasks. Our results suggest that\nthey are limited in their capability to make infer-\nences based on their previous computations, which\nis required for tasks such as counting and language\nmodeling. However, it can be shown that the model\nis capable of performing tasks which rely only on\ninformation provided at a given step such as copy-\ning and mapping. For such tasks, given positional\ninformation at a particular step, the model can look\nup the corresponding input and map it via the FFN.\nWe evaluate these hypotheses via our experiments.\nModel Copy Task Counting\nVanilla Transformers 100.0 100.0\n- Dec-Enc Residual 99.7 0.0\n- Dec-Dec Residual 99.7 99.8\nTable 1: BLEU scores ( ↑) for copy and counting task.\nPlease see Section 5 for details\nFor our experiments on synthetic data, we con-\nsider two tasks, namely thecopy task and the count-\ning task. For the copy task, the goal of a model is\nto reproduce the input sequence. We sample sen-\ntences of lengths between 5-12 words from Penn\nTreebank and create a train-test split of 40k-1k\nwith all sentences belonging to the same range of\nlength. In the counting task, we create a very sim-\nple dataset where the model is given one number\nbetween 0 and 100 as input and its goal is to predict\nthe next ﬁve numbers. Since only a single input\nis provided to the encoder, it is necessary for the\ndecoder to be able to make inferences based on its\nprevious predictions to perform this task. The ben-\neﬁt of conducting these experiments on synthetic\ndata is that they isolate the phenomena we wish to\nevaluate. For both these tasks, we compare vanilla\nTransformer with the one without decoder-encoder\nresidual connection. As a baseline we also con-\nsider the model without decoder-decoder residual\nconnection, since according to our results, that con-\nnection does not inﬂuence the computational power\nof the model. We implement a single layer encoder-\ndecoder network with only a single attention head\nin each block.\nWe then assess the inﬂuence of the limitation\non Machine Translation which requires a model\nto do a combination of both mapping and infer-\nring from computations in previous timesteps. We\nevaluate the models on IWSLT’14 German-English\ndataset and IWSLT’15 English-Vietnamese dataset.\nWe again compare vanilla Transformer with the\nones without decoder-encoder and decoder-decoder\nresidual connection. While tuning the models, we\nvary the number of layers from 1 to 4, the learn-\ning rate, warmup steps and the number of heads.\nSpeciﬁcations of the models, experimental setup,\ndatasets and sample outputs can be found in Sec. E\n462\nModel De-En En-Vi\nVanilla Transformers 32.9 28.8\n- Dec-Enc Residual 24.1 21.8\n- Dec-Dec Residual 30.6 27.2\nTable 2: BLEU scores ( ↑) for translation task. Please\nsee Section 5 for details.\nin the Appendix.\nResults on the effect of residual connections on\nsynthetic tasks can be found in Table 1. As per\nour hypothesis, all the variants are able to perfectly\nperform the copy task. For the counting task, the\none without decoder-encoder residual connection is\nincapable of performing it. However, the other two\nincluding the one without decoder-decoder resid-\nual connection are able to accomplish the task by\nlearning to make decisions based on their prior pre-\ndictions. Table 3 provides some illustrative sample\noutputs of the models. For the MT task, results can\nbe found in Table 2. While the drop from removing\ndecoder-encoder residual connection is signiﬁcant,\nit is still able to perform reasonably well since the\ntask can be largely fulﬁlled by mapping different\nwords from one sentence to another.\nFor positional masking, our proof technique sug-\ngests that due to lack of positional encodings, the\nmodel must come up with its own mechanism to\nmake order related decisions. Our hypothesis is\nthat, if it is able to develop such a mechanism,\nit should be able to generalize to higher lengths\nand not overﬁt on the data it is provided. To eval-\nuate this claim, we simply extend the copy task\nupto higher lengths. The training set remains the\nsame as before, containing sentences of length 5-12\nwords. We create 5 different validation sets each\ncontaining 1k sentences each. The ﬁrst set con-\ntains sentences within the same length as seen in\ntraining (5-12 words), the second set contains sen-\ntences of length 13-15 words while the third, fourth\nand ﬁfth sets contain sentences of lengths 15-20,\n21-25 and 26-30 words respectively. We consider\ntwo models, one which is provided absolute po-\nsitional encodings and one where only positional\nmasking is applied. Figure 3 shows the perfor-\nmance of these models across various lengths. The\nmodel with positional masking clearly generalizes\nup to higher lengths although its performance too\ndegrades at extreme lengths. We found that the\nmodel with absolute positional encodings during\ntraining overﬁts on the fact that the 13th token is\nalways the terminal symbol. Hence, when evalu-\nFigure 3: Performance of the two models on the copy\ntask across varying lengths of test inputs. DiSAN refers\nto Transformer with only positional masking. SAN\nrefers to vanilla Transformers.\nated on higher lengths it never produces a sentence\nof length greater than 12. Other encoding schemes\nsuch as relative positional encodings (Shaw et al.,\n2018; Dai et al., 2019) can generalize better, since\nthey are inherently designed to address this particu-\nlar issue. However, our goal is not to propose mask-\ning as a replacement of positional encodings, rather\nit is to determine whether the mechanism that the\nmodel develops during training is helpful in gen-\neralizing to higher lengths. Note that, positional\nmasking was not devised by keeping generalization\nor any other beneﬁt in mind. Our claim is only\nthat, the use of masking does not limit the model’s\nexpressiveness and it may beneﬁt in other ways,\nbut during practice one should explore each of the\nmechanisms and even a combination of both. Yang\net al. (2019) showed that a combination of both\nmasking and encodings is better able to learn order\ninformation as compared to explicit encodings.\nSOURCE – 42\nREFERENCE – 43 44 45 46 47\nVANILLA TRANSFORMER – 43 44 45 46 47\n- DEC-ENC RESIDUAL – 27 27 27 27 27\n- DEC-DEC RESIDUAL – 43 44 45 46 47\nTable 3: Sample outputs by the models on the counting\ntask. Without the residual connection around Decoder-\nEncoder block, the model is incapable of predicting\nmore than one distinct output.\n6 Discussion and Final Remarks\nWe showed that the class of languages recognized\nby Transformers and RNNs are exactly the same.\nThis implies that the difference in performance of\nboth the networks across different tasks can be at-\ntributed only to their learning abilities. In contrast\nto RNNs, Transformers are composed of multiple\ncomponents which are not essential for their com-\n463\nputational expressiveness. However, in practice\nthey may play a crucial role. Recently, V oita et al.\n(2019) showed that the decoder-decoder attention\nheads in the lower layers of the decoder do play a\nsigniﬁcant role in the NMT task and suggest that\nthey may be helping in language modeling. This in-\ndicates that components which are not essential for\nthe computational power may play a vital role in\nimproving the learning and generalization ability.\nTake-Home Messages. We showed that the or-\nder information can be provided either in the form\nof explicit encodings or masking without affect-\ning computational power of Transformers. The\ndecoder-encoder attention block plays a necessary\nrole in conditioning the computation on the input\nsequence while the residual connection around it is\nnecessary to keep track of previous computations.\nThe feedforward network in the decoder is the only\ncomponent capable of performing computations\nbased on the input and prior computations. Our\nexperimental results show that removing compo-\nnents essential for computational power inhibit the\nmodel’s ability to perform certain tasks. At the\nsame time, the components which do not play a\nrole in the computational power may be vital to the\nlearning ability of the network.\nAlthough our proofs rely on arbitrary preci-\nsion, which is common practice while studying the\ncomputational power of neural networks in theory\n(Siegelmann and Sontag, 1992; P´erez et al., 2019;\nHahn, 2020; Yun et al., 2020), implementations in\npractice work over ﬁxed precision settings. How-\never, our construction provides a starting point to\nanalyze Transformers under ﬁnite precision. Since\nRNNs can recognize all regular languages in ﬁnite\nprecision (Korsky and Berwick, 2019), it follows\nfrom our construction that Transformer can also\nrecognize a large class of regular languages in ﬁ-\nnite precision. At the same time, it does not imply\nthat it can recognize all regular languages given\nthe limitation due to the precision required to en-\ncode positional information. We leave the study of\nTransformers in ﬁnite precision for future work.\nAcknowledgements\nWe thank the anonymous reviewers for their con-\nstructive comments and suggestions. We would\nalso like to thank our colleagues at Microsoft Re-\nsearch and Michael Hahn for their valuable feed-\nback and helpful discussions.\nReferences\nYining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan\nMay, and Kevin Knight. 2018. Recurrent neural\nnetworks as weighted language recognizers. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 2261–2271, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMichael Hahn. 2020. Theoretical limitations of self-\nattention in neural sequence models. Transactions\nof the Association for Computational Linguistics ,\n8:156–171.\nJiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and\nRoman Novak. 2020. Inﬁnite attention: Nngp and\nntk for deep attention networks. arXiv preprint\narXiv:2006.10540.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Noam Shazeer, Curtis Hawthorne, An-\ndrew M. Dai, Matthew D. Hoffman, and Douglas\nEck. 2018. An improved relative self-attention\nmechanism for transformer with application to mu-\nsic generation. ArXiv, abs/1809.04281.\nHyunjik Kim, George Papamakarios, and Andriy Mnih.\n2020. The lipschitz constant of self-attention. arXiv\npreprint arXiv:2006.04710.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander Rush. 2017. OpenNMT: Open-\nsource toolkit for neural machine translation. In\nProceedings of ACL 2017, System Demonstrations ,\npages 67–72, Vancouver, Canada. Association for\nComputational Linguistics.\nJohn F Kolen and Stefan C Kremer. 2001. A ﬁeld\nguide to dynamical recurrent networks. John Wiley\n& Sons.\nSamuel A Korsky and Robert C Berwick. 2019. On\nthe computational power of rnns. arXiv preprint\narXiv:1906.06349.\n464\nYoav Levine, Noam Wies, Or Sharir, Hoﬁt Bata, and\nAmnon Shashua. 2020. Limits to depth efﬁciencies\nof self-attention. arXiv preprint arXiv:2006.12467.\nMinh-Thang Luong and Christopher D Manning. 2015.\nStanford neural machine translation systems for spo-\nken language domains. In Proceedings of the In-\nternational Workshop on Spoken Language Transla-\ntion, pages 76–79.\nWarren S McCulloch and Walter Pitts. 1943. A logical\ncalculus of the ideas immanent in nervous activity.\nThe bulletin of mathematical biophysics , 5(4):115–\n133.\nWilliam Merrill, Gail Weiss, Yoav Goldberg, Roy\nSchwartz, Noah A. Smith, and Eran Yahav. 2020. A\nformal hierarchy of RNN architectures. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 443–459, On-\nline. Association for Computational Linguistics.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proceedings of the Third Conference on\nMachine Translation: Research Papers , pages 1–9,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJorge P ´erez, Javier Marinkovi ´c, and Pablo Barcel ´o.\n2019. On the turing completeness of modern neural\nnetwork architectures. In International Conference\non Learning Representations.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlexander Rush. 2018. The annotated transformer.\nIn Proceedings of Workshop for NLP Open Source\nSoftware (NLP-OSS), pages 52–60, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nLuzi Sennhauser and Robert Berwick. 2018. Evaluat-\ning the ability of LSTMs to learn context-free gram-\nmars. In Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 115–124, Brussels, Bel-\ngium. Association for Computational Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nShirui Pan, and Chengqi Zhang. 2018. Disan: Di-\nrectional self-attention network for rnn/cnn-free lan-\nguage understanding. In Thirty-Second AAAI Con-\nference on Artiﬁcial Intelligence.\nHava T Siegelmann. 2012. Neural networks and ana-\nlog computation: beyond the Turing limit . Springer\nScience & Business Media.\nHava T Siegelmann and Eduardo D Sontag. 1992. On\nthe computational power of neural nets. In Proceed-\nings of the ﬁfth annual workshop on Computational\nlearning theory, pages 440–449. ACM.\nNatalia Skachkova, Thomas Trost, and Dietrich\nKlakow. 2018. Closing brackets with recurrent neu-\nral networks. In Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpret-\ning Neural Networks for NLP, pages 232–239, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nYao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada,\nLouis-Philippe Morency, and Ruslan Salakhutdinov.\n2019. Transformer dissection: An uniﬁed under-\nstanding for transformer’s attention via the lens of\nkernel. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4344–4353, Hong Kong, China. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nGail Weiss, Yoav Goldberg, and Eran Yahav. 2018.\nOn the practical computational power of ﬁnite pre-\ncision RNNs for language recognition. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 740–745, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nBaosong Yang, Longyue Wang, Derek F. Wong,\nLidia S. Chao, and Zhaopeng Tu. 2019. Assessing\nthe ability of self-attention networks to learn word\norder. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 3635–3644, Florence, Italy. Association\nfor Computational Linguistics.\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh\nRawat, Sashank Reddi, and Sanjiv Kumar. 2020.\nAre transformers universal approximators of\nsequence-to-sequence functions? In International\nConference on Learning Representations.\n465\nA Roadmap\nWe begin with various deﬁnitions and results. We\ndeﬁne simulation of Turing machines by RNNs\nand state the Turing-completeness result for RNNs.\nWe deﬁne vanilla and directional Transformers and\nwhat it means for Transformers to simulate RNNs.\nMany of the deﬁnitions from the main paper are\nreproduced here, but in more detail. In Sec. C.1\nwe discuss the effect of removing a residual con-\nnection on computational power of Transformers.\nSec. C.2 contains the proof of Turing completeness\nof vanilla Transformers and Sec. D the correspond-\ning proof for directional Transformers. Finally,\nSec. 5 has further details of experiments.\nB Deﬁnitions\nDenote the set {1,2,...,n }by [n]. Functions de-\nﬁned for scalars are extended to vectors in the nat-\nural way: for a function F deﬁned on a set A,\nfor a sequence (a1,...,a n) of elements in A, we\nset F(a1,...,a n) := ( F(a1),...,F (an)). Indi-\ncator I(P) is 1, if predicate P is true and is 0\notherwise. For a sequence X = ( xn′,..., xn)\nfor some n′≥0, we set Xj := (xn′,..., xj) for\nj ∈{n′,i+1,...,n }. We will work with an alpha-\nbet Σ = {β1,...,β m}, with β1 = # and βm = $.\nThe special symbols # and $ correspond to the\nbeginning and end of the input sequence, resp. For\na vector v, by 0v we mean the all-0 vector of the\nsame dimension as v. Let ¯t:= min{t,n}\nB.1 RNNs and Turing-completeness\nHere we summarize, somewhat informally, the\nTuring-completeness result for RNNs due to\n(Siegelmann and Sontag, 1992). We recall basic\nnotions from computability theory. In the main\npaper, for simplicity we stated the results for to-\ntal recursive functions φ : {0,1}∗→{0,1}∗, i.e.\na function that is deﬁned on every s ∈ {0,1}∗\nand whose values can be computed by a Turing\nmachine. While total recursive functions form a\nsatisfactory formalization of seq-to-seq tasks, here\nwe state the more general result for partial recur-\nsive functions. Let φ: {0,1}∗→{0,1}∗be partial\nrecursive. A partial recursive function is one that\nneed not be deﬁned for every s ∈ {0,1}∗, and\nthere exists a Turing Machine Mwith the follow-\ning property. The input sis initially written on the\ntape of the Turing Machine Mand the output φ(s)\nis the content of the tape upon acceptance which\nis indicated by halting in a designated accept state.\nOn sfor which φis undeﬁned, Mdoes not halt.\nWe now specify how Turing machineMis simu-\nlated by RNN R(M). In the RNNs in (Siegelmann\nand Sontag, 1992) the hidden state ht has the form\nht = [ qt,Ψ1,Ψ2],\nwhere qt = [ q1,...,q s] denotes the state of M\none-hot form. Numbers Ψ1,Ψ2 ∈Q, called stacks,\nstore the contents of the tape in a certain Cantor\nset like encoding (which is similar to, but slightly\nmore involved, than binary representation) at each\nstep. The simulating RNN R(M), gets as input\nencodings of s1s2...sn in the ﬁrst nsteps, and from\nthen on receives the vector 0 as input in each step.\nIf φis deﬁned on s, then Mhalts and accepts with\nthe output φ(s) the content of the tape. In this\ncase, R(M) enters a special accept state, and Ψ1\nencodes φ(s) and Ψ2 = 0. If Mdoes not halt then\nR(M) also does not enter the accept state.\nSiegelmann and Sontag (1992) further show that\nfrom R(M) one can further explicitly produce the\nφ(s) as its output. In the present paper, we will\nnot deal with explicit production of the output but\nrather work with the deﬁnition of simulation in the\nprevious paragraph. This is for simplicity of expo-\nsition, and the main ideas are already contained in\nour results. If the Turing machine computes φ(s)\nin time T(s), the simulation takes O(|s|) time to\nencode the input sequence sand 4T(s) to compute\nφ(s).\nTheorem B.1 ((Siegelmann and Sontag, 1992)) .\nGiven any partial recursive function φ: {0,1}∗→\n{0,1}∗computed by Turing machine Mφ, there\nexists a simulating RNN R(Mφ).\nIn view of the above theorem, for establishing\nTuring-completeness of Transformers, it sufﬁces to\nshow that RNNs can be simulated by Transform-\ners. Thus, in the sequel we will only talk about\nsimulating RNNs.\nB.2 Vanilla Transformer Architecture\nHere we describe the original transformer archi-\ntecture due to (Vaswani et al., 2017) as formalized\nby (P ´erez et al., 2019). While our notation and\ndeﬁnitions largely follow (P´erez et al., 2019), they\nare not identical. The transformer here makes use\nof positional encoding; later we will discuss the\ntransformer variant using directional attention but\nwithout using positional encoding.\n466\nThe transformer, denoted Trans, is a sequence-\nto-sequence architecture. Its input consists of\n(i) a sequence X = ( x1,..., xn) of vectors in\nQd, (ii) a seed vector y0 ∈ Qd. The output is\na sequence Y = ( y1,..., yr) of vectors in Qd.\nThe sequence X is obtained from the sequence\n(s0,...,s n) ∈Σn+1 of symbols by using the em-\nbedding mentioned earlier: xi = f(fb(si),pos(i))\nfor 0 ≤i ≤n. The transformer consists of com-\nposition of transformer encoder and a transformer\ndecoder. The transformer encoder is obtained by\ncomposing one or more single-layer encoders and\nsimilarly the transformer decoder is obtained by\ncomposing one or more single-layer decoders. For\nthe feed-forward networks in the transformer layers\nwe use the activation as in (Siegelmann and Son-\ntag, 1992), namely the saturated linear activation\nfunction:\nσ(x) =\n\n\n\n0 if x< 0,\nx if 0 ≤x≤1,\n1 if x> 1.\n(4)\nAs mentioned in the main paper, we can eas-\nily work with the standard ReLU activation via\nσ(x) = ReLU(x)−ReLU(x−1). In the following,\nafter deﬁning these components, we will put them\ntogether to specify the full transformer architecture.\nBut we begin with self-attention mechanism which\nis the central feature of the transformer.\nSelf-attention. The self-attention mechanism\ntakes as input (i) a query vector q, (ii) a sequence\nof key vectors K = ( k1,..., kn), and (iii) a se-\nquence of value vectors V = ( v1,..., vn). All\nvectors are in Qd.\nThe q-attention over keys Kand values V, de-\nnoted by Att(q,K,V), is a vector agiven by\n(α1,...,α n) = ρ(fatt(q,k1),...,f att(q,kn)),\na = α1v1 + α2v2 + ··· + αnvn.\nThe above deﬁnition uses two functions ρand\nfatt which we now describe. For the normal-\nization function ρ : Qn → Qn\n≥0 we will use\nhardmax: for x= (x1,...,x n) ∈Qn, if the maxi-\nmum value occurs rtimes among x1,...,x n, then\nhardmax(x)i := 1 /r if xi is a maximum value\nand hardmax(x)i := 0 otherwise. In practice, the\nsoftmax is often used but its output values are in\ngeneral not rational. The names soft-attention and\nhard-attention are used for the attention mecha-\nnism depending on which normalization function\nis used.\nFor the Turing-completeness proof of vanilla\ntransformers, the scoring function fatt used\nis a combination of multiplicative attention\n(Vaswani et al., 2017) and a non-linear function:\nfatt(q,ki) = −\n⏐⏐⟨q,ki⟩\n⏐⏐. For directional trans-\nformers, the standard multiplicative attention is\nused, that is, fatt(q,ki) = ⟨q,ki⟩.\nTransformer encoder. A single-layer encoder\nis a function Enc(X; θ), where θis the parame-\nter vector and the input X = ( x1,..., xn) is a\nsequence of vector in Qd. The output is another\nsequence Z= (z1,..., zn) of vectors in Qd. The\nparameters θ specify functions Q(·),K(·),V (·),\nand O(·), all of type Qd →Qd. The functions\nQ(·),K(·),and V(·) are usually linear transforma-\ntions and this will be the case in our constructions:\nQ(xi) = xT\ni WQ,\nK(xi) = xT\ni WK,\nV(xi) = xT\ni WV,\nwhere WQ,WK,WV ∈Qd×d. The function O(·)\nis a feed-forward network. The single-layer en-\ncoder is then deﬁned by\nai = Att( Q(xi),K(X),V (X)) + xi, (5)\nzi = O(ai) + ai.\nThe addition operations +xi and +ai are the resid-\nual connections. The operation in (5) is called the\nencoder-encoder attention block.\nThe complete L-layer transformer encoder\nTEnc(L)(X; θ) has the same input X =\n(x1,..., xn) as the single-layer encoder. By\ncontrast, its output consists of two sequences\n(Ke,Ve), each a sequence of n vectors in Qd.\nThe encoder TEnc(L)(·) is obtained by repeated\napplication of single-layer encoders, each with its\nown parameters; and at the end, two trasformation\nfunctions KL(·) and VL(·) are applied to the se-\nquence of output vectors at the last layer. Functions\nK(L)(·) and V(L)(·) are linear transformations in\nour constructions. Formally, for 1 ≤ℓ ≤L−1\nand X1 := X, we have\nXℓ+1 = Enc( Xℓ; θℓ),\nKe = K(L)(XL),\nVe = V(L)(XL).\n467\nThe output of the L-layer Transformer encoder\n(Ke,Ve) = TEnc (L)(X) is fed to the Trans-\nformer decoder which we describe next.\nTransformer decoder. The input to a single-\nlayer decoder is (i) (Ke,Ve), the sequences of\nkey and value vectors output by the encoder, and\n(ii) a sequence Y = (y1,..., yk) of vectors in Qd.\nThe output is another sequence Z= (z1,..., zk)\nof vectors in Qd.\nSimilar to the single-layer encoder, a single-\nlayer decoder is parameterized by functions\nQ(·),K(·),V (·) and O(·) and is deﬁned by\npt = Att( Q(yt),K(Yt),V (Yt)) + yt, (6)\nat = Att( pt,Ke,Ve) + pt, (7)\nzt = O(at) + at.\nThe operation in (6) will be referred to as the\ndecoder-decoder attention block and the opera-\ntion in (7) as the decoder-encoder attention block.\nIn the decoder-decoder attention block, positional\nmasking is applied to prevent the network from\nattending over symbols which are ahead of them.\nAn L-layer Transformer decoder is obtained by\nrepeated application of L single-layer decoders\neach with its own parameters and a transformation\nfunction F : Qd →Qd applied to the last vector in\nthe sequence of vectors output by the ﬁnal decoder.\nFormally, for 1 ≤ℓ≤L−1 and Y1 = Y we have\nYℓ+1 = Dec((Ke,Ve),Yℓ; θℓ),\nz = F(yL\nt ).\nWe use z = TDec L((Ke,Ve),Y; θ) to denote\nan L-layer Transformer decoder. Note that while\nthe output of a single-layer decoder is a sequence\nof vectors, the output of an L-layer Transformer\ndecoder is a single vector.\nThe complete Transformer. A Transformer net-\nwork receives an input sequence X, a seed vector\ny0, and r ∈N. For t≥0 its output is a sequence\nY = (y1,..., yr) deﬁned by\n˜yt+1 = TDec\n(\nTEnc(X),(y0,y1,..., yt)\n)\n.\nWe get yt+1 by adding positional encoding:\nyt+1 = ˜yt+1 + pos(t+ 1). We denote the com-\nplete Transformer by Trans(X,y0) = Y. The\nTransformer “halts” whenyT ∈H, where H is a\nprespeciﬁed halting set.\nSimulation of RNNs by Transformers. We say\nthat a Transformer simulates an RNN (as deﬁned in\nSec. B.1) if on input s∈Σ∗, at each step t, the vec-\ntor yt contains the hidden state ht as a subvector:\nyt = [ht,·], and halts at the same step as RNN.\nC Results on Vanilla Transformers\nC.1 Residual Connections\nProposition C.1. The Transformer without resid-\nual connection around the Decoder-Encoder Atten-\ntion block in the Decoder is not Turing Complete\nProof. Recall that the vectors at is produced from\nthe Encoder-Decoder Attention block in the follow-\ning way,\nat = Att(pt,Ke,Ve) + pt\nThe result follows from the observation\nthat without the residual connections, at =\nAtt(pt,Ke,Ve), which leads to at = ∑n\ni=1 αive\ni\nfor some αis such that ∑n\ni αi = 1. Since ve\ni is pro-\nduced from the encoder, the vector at will have no\ninformation about its previous hidden state values.\nSince the previous hidden state information was\ncomputed and stored in pt, without the residual\nconnection, the information in at depends solely\non the output of the encoder.\nOne could argue that since the attention weights\nαis depend on the query vector pt, it could still use\nit gain the necessary information from the vectors\nve\nis. However, note that by deﬁnition of hard atten-\ntion, the attention weights αi in at = ∑n\ni=1 αive\ni\ncan either be zero or some nonzero value depending\non the attention logits. Since the attention weights\nαi are such that ∑n\ni αi = 1 and all the nonzero\nweights are equal to each other. Thus given the con-\nstraints there are2n−1 ways to attend overninputs\nexcluding the case where no input is attended over.\nHence, the network without decoder-encoder resid-\nual connection withninputs can have at most2n−1\ndistinct at values. This implies that the model will\nbe unable to perform a task that takes ninputs and\nhas to produce more than 2n −1 outputs. Note\nthat, such a limitation will not exist with a residual\nconnection since the vector at = Σn\ni=1αive\ni + pt\ncan take arbitrary number of values depending on\nits prior computations in pt.\nAs an example to illustrate the limitation, con-\nsider the following simple problem, given a value\n∆, where 0 ≤∆ ≤1, the network must produce\n468\nthe values 0,∆,2∆,...,k ∆, where kis the max-\nimum integer such that k∆ ≤1. If the network\nreceives a single input ∆, the encoder will produce\nonly one particular output vector and regardless of\nwhat the value of the query vector pt is, the vector\nat will be constant at every timestep. Since at is\nfed to feedforward network which maps it to zt,\nthe output of the decoder will remain the same at\nevery timestep and it cannot produce distinct val-\nues. If the input is combined with n−1 auxiliary\nsymbols (such as # and $), then the network can\nonly produce 2n−1 outputs. Hence, the model will\nbe incapable of performing the task if ∆ <1/2n .\nThus the model cannot perform the task deﬁned\nabove which RNNs and Vanilla Transformers can\neasily do with a simple counting mechanism via\ntheir recurrent connection.\nFor the case of multilayer decoder, consider\nany L layer decoder model. If the residual con-\nnection is removed, the output of decoder-encoder\nattention block at each layer isa(ℓ)\nt = ∑n\ni=1 α(ℓ)\ni ve\ni\nfor 1 ≤ℓ ≤L. Observe, that since output of the\ndecoder-encoder attention block in the last (L-th)\nlayer of the decoder isa(L)\nt = ∑n\ni=1 α(L)\ni ve\ni. Since\nthe output of the Llayer decoder will be a feedfor-\nward network over a(L)\nt , the computation reduces\nto the single layer decoder case. Hence, similar to\nthe single layer case, if the task requires the net-\nwork to produce values of at that come from a set\nwith size at least 2n, then the network will not be\nable to perform the task.\nThis implies that the model without decoder-\nencoder residual connection is limited in its capa-\nbility to perform tasks which requires it to make\ninferences based on previously generated outputs.\nC.2 Simulation of RNNs by Transformers\nwith positional encoding\nTheorem C.2. RNNs can be simulated by vanilla\nTransformers and hence the class of vanilla Trans-\nformers is Turing-complete.\nProof. The construction of the simulating trans-\nformer is simple: it uses a single head and both\nthe encoder and decoder have one layer. Moreover,\nthe encoder does very little and most of the action\nhappens in the decoder. The main task for the sim-\nulation is to design the input embedding (building\non the given base embedding fb), the feedforward\nnetwork O(·) and the matrices corresponding to\nfunctions Q(·),K(·),V (·).\nInput embedding. The input embedding is ob-\ntained by summing the symbol and positional en-\ncodings which we next describe. These encodings\nhave dimension d= 2dh+ db+ 2, where dh is the\ndimension of the hidden state of the RNN and db\nis the dimension of the given encoding fb of the\ninput symbols. We will use the symbol encoding\nfsymb : Σ →Qd which is essentially the same as\nfb except that the dimension is now larger:\nfsymb(s) = [0dh,fe(s); 0dh,0,0].\nThe positional encoding pos : N →Qd is simply\npos(i) = [0dh,0db,0dh,i, 1].\nTogether, these deﬁne the combined embedding f\nfor a given input sequence s0s1 ···sn ∈Σ∗by\nf(si) = fsymb(si)+pos(i) = [0dh,fb(si),0dh,i, 1].\nThe vectors v∈Qd used in the computation of our\ntransformer are of the form\nv= [h1,s; h2,x1,x2],\nwhere h1,h2 ∈Qdh,s ∈Qde, and x1,x2 ∈Q.\nThe coordinates corresponding to the hi’s are re-\nserved for computation related to hidden states of\nthe RNN, the coordinates corresponding to sare\nreserved for base embeddings, and those for x1\nand x2 are reserved for scalar values related to\npositional operations. The ﬁrst two blocks, corre-\nsponding to h1 and sare reserved for computation\nof the RNN.\nDuring the computation of the Transformer, the\nunderlying RNN will get the input s¯t at step tfor\nt= 0,1,... , where recall that ¯t= min{t,n}.This\nsequence leads to the RNN getting the embedding\nof the input sequence s0,...,s n in the ﬁrst n+ 1\nsteps followed by the embedding of the symbol $\nfor the subsequent steps, which is in accordance\nwith the requirements of (Siegelmann and Sontag,\n1992). Similar to (P´erez et al., 2019) we use the fol-\nlowing scoring function in the attention mechanism\nin our construction,\nfatt(qi,kj) = −|⟨qi,kj⟩| (8)\nConstruction of TEnc. As previously men-\ntioned, our transformer encoder has only one layer,\nand the computation in the encoder is very simple:\nthe attention mechanism is not utilized, only the\nresidual connections are. This is done by setting\n469\nthe matrix for V(·) to the all-zeros matrix, and the\nfeedforward networks to always output 0. The ap-\nplication of appropriately chosen linear transforma-\ntions for the ﬁnal K(·) and V(·) give the following\nlemma about the output of the encoder.\nLemma C.3. There exists a single layer encoder\ndenoted by TEnc that takes as input the sequence\n(x1,..., xn,$) and generates the tuple (Ke,Ve)\nwhere Ke = (k1,..., kn) and Ve = (v1,..., vn)\nsuch that,\nki = [ 0h,0s; 0h,−1,i],\nvi = [ 0h,si; 0h,0,0].\nConstruction of TDec. As in the construction\nof TEnc, our TDec has only one layer. Also\nlike TEnc, the decoder-decoder attention block\njust computes the identity: we set V(1)(·) = 0\nidentically, and use the residual connection so that\npt = yt.\nFor t ≥0, at the t-th step we denote the input\nto the decoder as yt = ˜yt + pos(t). Let h0 =\n0h and ˜y0 = 0. We will show by induction that at\nthe t-th timestep we have\nyt = [ht,0s; 0h,t + 1,1]. (9)\nBy construction, this is true for t= 0:\ny0 = [0h,0s; 0h,1,1].\nAssuming that it holds for t, we show it for t+ 1.\nBy Lemma C.5\nAtt(pt,Ke,Ve) = [0h,vt+1; 0h,0,0]. (10)\nLemma C.5 basically shows how we retrieve the\ninput st+1 at the relevant step for further computa-\ntion in the decoder. It follows that\nat = Att(pt,Ke,Ve) + pt\n= [ht,st+1,0h,t + 1,1].\nIn the ﬁnal block of the decoder, the computation\nfor RNN takes place:\nLemma C.4. There exists a function O(·) deﬁned\nby feed-forward network such that,\nO(at) = [(ht+1 −ht),−st+1,0h,−(t+ 1),−1],\nwhere Wh,Wx and bdenote the parameters of the\nRNN under consideration.\nThis leads to\nzt = O(at) + at = [ht+1,0s; 0h,0,0].\nWe choose the function F for our decoder\nto be the identity function, therefore ˜yt+1 =\n[ht+1,0s; 0h,0,0], which means yt+1 = ˜yt+1 +\npos(i+ 1) = [ht+1,0s; 0h,t + 2,1], proving our\ninduction hypothesis.\nC.3 Technical Lemmas\nProof of Lemma C.3. We construct a single-layer en-\ncoder achieving the desired Ke and Ve. We make\nuse of the residual connections and via trivial self-\nattention we get that zi = xi. More speciﬁcally\nfor i∈[n] we have\nV(1)(xi) = 0,\nai = 0 + xi,\nO(ai) = 0,\nzi = 0 + ai = xi.\nV(1)(xi) = 0 can be achieved by setting the\nweight matrix as the all-0 matrix. Recall that xi is\ndeﬁned as\nxi = [ 0h,si,\n0h,i, 1 ] .\nWe then apply linear transformations in\nK(zi) = ziWk and V(zi) = ziWv, where\nWT\nk =\n\n\n0 0 ··· 0 0\n... ... ... ...\n0 0 ··· 0 0\n0 0 ··· 0 1\n0 0 ··· −1 0\n\n\n,\nand Wk ∈Qd×d, and similarly one can obtain vi\nby setting the submatrix of Wv ∈Qd×d formed\nby the ﬁrst d−2 rows and columns to the identity\nmatrix, and the rest of the entries to zeros.\nLemma C.5. Let qt ∈Qd be a query vector such\nthat q = [ ·,..., ·,t + 1,1] where t ∈N and ‘·’\ndenotes an arbitrary value. Then we have\nAtt(qt,Ke,Ve) = [0h,st+1,0h,0,0]. (11)\n470\nProof. Recall that pt = yt = [ ht,0,..., 0,t +\n1,1] and ki = [0,0,..., 0,−1,i] and hence\n⟨pt,ki⟩= i−(t+ 1),\nfatt(pt,ki) = −|i−(t+ 1)|.\nThus, for i∈[n], the scoring functionfatt(pt,ki)\nhas the maximum value 0 at index i = t+ 1 if\nt<n ; for t≥n, the maximum value t+ 1 −nis\nachieved for i= n. Therefore\nAtt(pt,Ke,Ve) = st+1.\nProof of Lemma C.4. Recall that\nat = [ ht,st+1,\n0h,t + 1,1 ]\nNetwork O(at) is of the form\nO(at) = W2σ(W1at + b1),\nwhere Wi ∈Qd×d and b∈Qd and\nW1 =\ndh de dh 2\ndh\nde\ndh\n2\n\n\nWh Wx 0 0\n0 I 0 0\nI 0 0 0\n0 0 0 I\n\n\nand b1 = [bh,0s,0h,0,0]. Hence\nσ(W1at + b1) = [σ(Whht + Wxst+1 + b),\nst+1,ht,t + 1,1]\nNext we deﬁne W2 by\nW2 =\ndh de dh 2\ndh\nde\ndh\n2\n\n\nI 0 −I 0\n0 −I 0 0\n0 0 0 0\n0 0 0 −I\n\n.\nThis leads to\nO(at) = W2σ(W1at + b1)\n= [σ(Whht + Wxst+1 + b) −ht,−st+1,\n0h,−(t+ 1),−1],\nwhich is what we wanted to prove.\nD Completeness of Directional\nTransformers\nThere are a few changes in the architecture of the\nTransformer to obtain directional Transformer. The\nﬁrst change is that there are no positional encodings\nand thus the input vector xi only consists of si.\nSimilarly, there are no positional encodings in the\ndecoder inputs and hence yt = ˜yt. The vector ˜yis\nthe output representation produced at the previous\nstep and the ﬁrst input vector to the decoder˜y0 = 0.\nInstead of using positional encodings, we apply\npositional masking to the inputs and outputs of the\nencoder.\nThus the encoder-encoder attention in (5) is re-\ndeﬁned as\na(ℓ+1)\ni = Att(Q(z(ℓ)\ni ),K(Z(ℓ)\ni ),V (Z(ℓ)\ni )) +z(ℓ)\ni ,\nwhere Z(0) = X. Similarly the decoder-encoder\nattention in (7) is redeﬁned by\na(ℓ)\nt = Att(p(ℓ)\nt ,Ke\nt,Ve\nt ) + p(ℓ)\nt ,\nwhere ℓ in a(ℓ)\nt denotes the layer ℓ and we use\nv(ℓ,b) to denote any intermediate vector being used\nin ℓ-th layer and b-th block in cases where the same\nsymbol is used in multiple blocks in the same layer.\nTheorem D.1. RNNs can be simulated by vanilla\nTransformers and hence the class of vanilla Trans-\nformers is Turing-complete.\nProof. The Transformer network in this case will\nbe more complex than the construction for the\nvanilla case. The encoder remains very similar,\nbut the decoder is different and has two layers.\nEmbedding. We will construct our Transformer\nto simulate an RNN of the form given in the deﬁni-\ntion with the recurrence\nht = g(Whht−1 + Wxxt + b).\nThe vectors used in the Transformer layers are of\ndimension d= 2dh + de + 4|Σ|+ 1. Where dh is\nthe dimension of the hidden state of the RNN and\nde is the dimension of the input embedding.\nAll vector v∈Qd used during the computation\nof the network are of the form\nv= [h1,h2,s1,Js1K,x1,Js2KJs3K,Js4K]\nwhere hi ∈ Qdh,s ∈ Qde and xi ∈ Q. These\nblocks reserved for different types of objects. The\n471\nvectors his are reserved for computation related to\nhidden states of RNNs, sis are reserved for input\nembeddings and xis are reserved for scalar values\nrelated to positional operations.\nGiven an input sequence s0s1s2 ···sn ∈ Σ∗\nwhere s0 = # and sn = $, we use an embedding\nfunction f : Σ →Qd deﬁned as\nf(si) = xi = [ 0h,0h,si,\nJsiK,0,0ω,0ω,0ω ]\nUnlike (P´erez et al., 2019), we use the dot prod-\nuct as our scoring function as used in Vaswani et al.\n(2017) in the attention mechanism in our construc-\ntion,\nfatt(qi,kj) = ⟨qi,kj⟩.\nFor the computation of the Transformer, we also\nuse a vector sequence in Q|Σ|deﬁned by\nωt = 1\nt+ 1\nt∑\nj=0\nJstK,\nwhere 0 ≤ t ≤ n. The vector ωt =\n(ωt,1,..., ωt,|Σ|) contains the proportion of each\ninput symbol till step t for 0 ≤ t ≤ n. Set\nω−1 = 0. From the deﬁntion of ωt, it follows\nthat at any step 1 ≤k≤|Σ|we have\nωt,k = φt,k\nt+ 1, (12)\nwhere φt,k denotes the number of times the k-th\nsymbol βk in Σ has appeared till the t-th step.\nNote that ωt,0 = 1\nt+1 since the ﬁrst coordinate\ncorresponds to the proportion of the start symbol\n# which appears only once at t = 0 . Similarly,\nωt,|Σ| = 0 for 0 ≤t < nand ωt,|Σ| = 1/(t+ 1)\nfor t≥n, since the end symbol $ doesn’t appear\ntill the end of the input and it appears only once at\nt= n.\nWe deﬁne two more sequences of vectors inQ|Σ|\nfor 0 ≤t≤n:\n∆t = σ(ωt −ωt−1),\nδt = (∆t,1,..., ∆t,|Σ|−1,1/2t+1).\nHere ∆t denotes the difference in the proportion of\nsymbols between the t-th and (t−1)-th steps, with\nthe applicatin of sigmoid activation. In vector δt,\nthe last coordinate of ∆t has been replaced with\n1/2t+1. The last coordinate in ωt indicates the pro-\nportion of the terminal symbol $ and hence the last\nvalue in ∆t denotes the change in proportion of $.\nWe set the last coordinate in δt to an exponentially\ndecreasing sequence so that afternsteps we always\nhave a nonzero score for the terminal symbol and it\nis taken as input in the underlying RNN. Different\nand perhaps simpler choices for the last coordinate\nof δt may be possible. Note that 0 ≤∆t,k ≤1 and\n0 ≤δt,k ≤1 for 0 ≤t≤nand 1 ≤k≤|Σ|.\nConstruction of TEnc. The input to the network\nDTransM is the sequence (s0,s1,...,s n−1,sn)\nwhere s0 = # and sn = $ . Our en-\ncoder is a simple single layer network such that\nTEnc(x0,x1,..., xn) = (Ke,Ve) where Ke =\n(ke\n0,..., ke\nn) and Ve = (ve\n0,..., ve\nn) such that,\nke\ni = [ 0h,0h,0s,\nJsiK,0,0ω,0ω,0ω ], (13)\nve\ni = [ 0h,0h,si,\n0ω,0,0ω,JsiK,0ω ].\nSimilar to our construction of the encoder for\nvanilla transformer (Lemma C.3), the above Ke\nand Ve can be obtained by making the output of\nAtt(·) = 0 by choosing the V(·) to always evalu-\nate to 0 and similarly for O(·), and using residual\nconnections. Then one can produce Ke and Ve\nvia simple linear transformations using K(·) and\nV(·).\nConstruction of TDec. At the t-th step we de-\nnote the input to the decoder as yt = ˜yt, where\n0 ≤t≤r, where ris the step where the decoder\nhalts. Let h−1 = 0h and h0 = 0h. We will prove\nby induction on tthat for 0 ≤t≤rwe have\nyt = [ ht−1,0h,0s,\n0ω, 1\n2t ,0ω,0ω,ωt−1 ]. (14)\nThis is true for t= 0 by the choice of seed vector:\ny0 = [ 0h,0h,0s,\n0ω,1,0ω,0ω,0ω ].\nAssuming the truth of (14) for t, we show it for\nt+ 1.\nLayer 1. Similar to the construction in\nLemma C.3, in the decoder-decoder attention\nblock we set V(1)(·) = 0d and use the residual\nconnections to set p(1)\nt = yt. At the t-th step in the\ndecoder-encoder attention block of layer 1 we have\nAtt(p(1)\nt ,Ke¯t,Ve¯t ) =\n¯t∑\nj=0\nˆα(1,2)\nt,j ve\nj,\n472\nwhere\n(ˆα(2,2)\nt,1 ,..., ˆα(2,2)\nt,¯t )\n= hardmax\n(\n⟨p(1)\nt ,ke\n1⟩,..., ⟨p(1)\nt ,ke¯t⟩\n)\n= hardmax(0,..., 0)\n=\n( 1\n¯t+ 1,..., 1\n¯t+ 1\n)\n.\nTherefore\n∑¯t\nj=0 ˆα(1,2)\nt,j ve\nj = [ 0h,0h,s0:t,\n0ω,0,0ω,ω¯t,0ω ]\nwhere\ns0:t = 1\n¯t+ 1\n¯t∑\nj=0\nsj.\nThus,\na(1)\nt = Att(p(1)\nt ,Ke¯t,Ve¯t ) + p(1)\nt\n= [ht−1,0h,s0:t,0ω, 1\n2t,0ω,ω¯t,ωt−1].\nIn Lemma D.2 we construct feed-forward network\nO(1)(·) such that\nO(1)(a(1)\nt ) = [0h,0h,−s0:t,δ¯t,−1\n2t + 1\n2t+1 ,\n0ω,−ω¯t,−ωt−1 + ω¯t].\nHence\nz(1)\nt = O(1)(a(1)\nt ) + a(1)\nt (15)\n= [ht−1,0h,0s,δ¯t, 1\n2t+1 ,0ω,0ω,ω¯t].\nLayer 2. In the ﬁrst block of layer 2, we set the\nvalue transformation function to identically zero\nsimilar to Lemma C.3, i.e.V(2)(·) = 0 which leads\nto the output of Att(·) to be 0 and then using the\nresidual connection we get p(2)\nt = z(1)\nt . It follows\nby Lemma D.3 that\nAtt(p(2)\nt ,Ke¯t,Ve¯t )\n= [0h,0h,s¯t,0ω,0,0ω,JstK,0ω].\nThus,\na(2)\nt = Att(p(2)\nt ,Ke¯t,Ve¯t ) + p(2)\nt\n= [ht−1,0h,s¯t,δ¯t, 1\n2t+1 ,0ω,JstK,ω¯t].\nIn the ﬁnal block of the decoder in the second\nlayer, the computation for RNN takes place. In\nLemma D.4 below we construct the feed-forward\nnetwork O(2)(·) such that\nO(2)(a(2)\nt ) = [σ(Whht−1 + Wxs¯t + b) −ht−1\n0h,−s¯t,−δt,0,0ω,−JstK,0ω]\nand hence\nz(2)\nt =O(2)(a(2)\nt ) + a(2)\nt\n=[σ(Whht−1 + Wxs¯t + b),0h,0s,\n0ω, 1\n2t+1 ,0ω,0ω,ω¯t],\nwhich gives\nyt+1 = [ ht,0h,0s,\n0ω, 1\n2t+1 ,0ω,0ω,ω¯t ],\nproving the induction hypothesis (14) fort+1, and\ncompleting the simulation of RNN.\nD.1 Technical Lemmas\nLemma D.2. There exists a function O(1)(.) de-\nﬁned by feed-forward network such that,\nO(1)(a(1)\nt ) = [0h,0h,−s0:t,δt,\n−1\n2t + 1\n2t+1 ,0ω, −ωt,−ωt−1 + ωt]\nProof. We deﬁne the feed-forward networkO(1)(.)\nsuch that\nO(1)(a(1)\nt ) = [0h,0h,−s0:t,δt −ωt,\n−1\n2t + 1\n2t+1 , 0ω, 0ω, −ωt−1 + ωt]\nwhere\nδt = (∆t,1,..., ∆t,n−1,1/2t+1), 0 ≤δt ≤1\nRecall that,\na(1)\nt = [ ht−1,0h,s0:t,\nωt, 1\n2t ,0ω,0ω,ωt−1 ]\nWe deﬁne the feed-forward network O(at) as\nfollows,\nO(1)(at) = W2σ(W1a(1)\nt + b1)\n473\nwhere Wi ∈Qd×d and b1 ∈Qd. Deﬁne W1 as\n2dh de dω 1 dω dω dω\n2dh\nde\ndω −1\n1\n1\ndω\ndω\ndω\n\n\n0 0 0 0 0 0 0\n0 I 0 0 0 0 0\n0 0 0 0 0 I −I\n0 0 0 1\n2 0 0 0\n0 0 0 1\n2 0 0 0\n0 0 I 0 0 0 0\n0 0 0 0 0 I 0\n0 0 0 0 0 0 I\n\n\nand b1 = 0, then\nσ(W1a(1)\nt + b1) = [0h,0h,s0:t,∆t, 1\n2t+1 ,\nωt, ωt−1, ωt−1]\nWe deﬁne W2 as\n2dh de dω−1 2 dω dω dω\n2dh\nde\ndω −1\n1\n1\ndω\ndω\ndω\n\n\n0 0 0 0 0 0 0\n0 −I 0 0 0 0 0\n0 0 I 0 0 0 0\n0 0 0 1,0 0 0 0\n0 0 0 −2,1 0 0 0\n0 0 I 0 0 0 0\n0 0 0 0 0 −I 0\n0 0 0 0 0 I −I\n\n\nThis leads to\nO(1)(a(1)\nt ) = [0h,0h,s0:t,δt,\n−1\n2t + 1\n2t+1 , 0ω, −ωt, −ωt−1 + ωt]\nwhich is what we wanted to prove.\nLemma D.3. Let p(2)\nt ∈Qdbe a query vector such\nthat\np(2)\nt = [ ·, ·, ·,\nδt, ·,·,·,· ]\nwhere t ≥0 and ‘·’ denotes an arbitrary value.\nThen we have\nAtt(p(2)\nt ,Ke¯t,Ve¯t ) = [ 0h,0h,s¯t,\n0ω,0,0ω,JstK,0ω ].\n(16)\nProof. Let\n(ˆα(2,2)\nt,1 ,..., ˆα(2,2)\nt,¯t )\n= hardmax\n(\n⟨p(2)\nt ,ke\n1⟩,..., ⟨p(2)\nt ,ke¯t⟩\n)\nbe the vector of normalized attention scores in the\ndecoder-encoder attention block of layer 2 at time\nt. Then\nAtt(p(2)\nt ,Ke¯t,Ve¯t ) =\n¯t∑\nj=0\nˆα(2,2)\nt,j ve\nj.\nWe claim that\nClaim 1. For t≥0 we have\n(ˆα(2,2)\nt,1 ,..., ˆα(2,2)\nt,¯t )\n= 1\nλ¯t\n(\nI(s0 = st),I(s1 = st),..., I(s¯t = st)\n)\n,\nwhere λt is a normalization factor given by λt =∑n−1\nj=0 I(sj = st).\nWe now prove the lemma assuming the claim\nabove. Denote the L.H.S. in (16) by γt. Note that\nif sj = st, then ve\nj = γt. Now we have\n¯t∑\nj=0\nˆα(2,2)\nt,j ve\nj = 1\nλt\n¯t∑\nj=0\nI(sj = st) ve\nj\n= 1\nλt\n\n\n¯t∑\nj=0\nI(sj = st)\n\nγt\n= γt,\ncompleting the proof of the lemma modulo the\nproof of the claim, which we prove next.\nProof. (of Claim 1) For 0 < t≤n, the vector\nωt −ωt−1 has the form\n(( 1\nt+ 1 −1\nt\n)\n,...,\n(φt,k\nt+ 1 −φt−1,k\nt\n)\n,..., 0\n)\n.\nIf st = βk, then\n(ωt − ωt−1)k (17)\n=\n(φt,k\nt+ 1 −φt−1,k\nt\n)\n(18)\n=\n(φt−1,k + 1\nt+ 1 −φt−1,k\nt\n)\n(19)\n= t−φt−1,k\nt(t+ 1) (20)\n≥ 1\nt(t+ 1). (21)\nThe last inequality used our assumption that s0 =\n# and that # does not occur at any later time and\n474\ntherefore φt−1,j <t. On the other hand, if st ̸= βk,\nthen\n(ωt −ωt−1)k =\n(φt,k\nt+ 1 −φt−1,k\nt\n)\n=\n(φt−1,k\nt+ 1 −φt−1,k\nt\n)\n= −φt−1,j\nt(t+ 1) (22)\n≤ 0.\nThis leads to,\n(ωt −ωt−1)k >0 if st = βk,\n(ωt −ωt−1)k ≤0 otherwise.\nIn words, the change in the proportion of a symbol\nis positive from step t−1 to tif and only if it is\nthe input symbol at the t-th step. For 0 ≤t ≤n\nand 1 ≤k≤|Σ|, this leads to\n∆t,k = σ(ωt −ωt−1)k >0 if st = βk,\n∆t,k = σ(ωt −ωt−1)k = 0 otherwise,\nFor t>n ,\n∆t = 0.\nRecall that p(2)\nt = z(1)\nt which comes from (15),\nand ke\nj is deﬁned in (13). We reproduce these for\nconvenience:\np(2)\nt = [ ht−1, 0h, 0s,\nδ¯t, 1\n2t+1 ,0ω,0ω, ω¯t ],\nke\nj = [ 0h,0h,0s,\nJsjK,0,0ω,0ω,0ω ].\nIt now follows that for 0 <t<n , if 0 ≤j ≤tis\nsuch that sj ̸= st, then\n⟨p(2)\nt ,ke\nj⟩= ⟨δt,JsjK⟩= δt,i = 0.\nAnd for 0 < t < n, if 0 ≤j ≤t is such that\nsj = st = βi, then\n⟨p(2)\nt ,ke\nj⟩= ⟨δt,JsjK⟩= δt,i (23)\n= t−φt−1,j\nt(t+ 1) ≥ 1\nt(t+ 1). (24)\nThus, for 0 ≤ t < n , in the vector(\n⟨p(2)\nt ,ke\n0⟩,..., ⟨p(2)\nt ,ke\nt⟩\n)\n, the largest coordi-\nnates are the ones indexed by j with sj = st\nand they all equal t−φt−1,i\nt(t+1) . All other coordi-\nnates are 0. For t ≥n, only the last coordinate\n⟨p(2)\nt ,ke\nn⟩= ⟨δt,J$K⟩= 1\n2t+1 is non-zero. Now\nthe claim follows immediately by the deﬁnition of\nhardmax.\nLemma D.4. There exists a function O(2)(.) de-\nﬁned by feed-forward network such that, for t≥0,\nO(2)(a(2)\nt ) = [σ(Whht−1 + Wxs¯t + b) −ht−1,\n0h,−s¯t,−δt,0,0ω,−JstK,0ω]\nwhere Wh,Wx and bdenote the parameters of the\nRNN under consideration.\nProof. Proof is very similar to proof of lemma C.4.\nE Details of Experiments\nIn this section, we describe the speciﬁcs of our\nexperimental setup. This includes details about the\ndataset, models, setup and some sample outputs.\nE.1 Impact of Residual Connections\nThe models under consideration are the vanilla\nTransformer, the one without decoder-encoder\nresidual connection and the one without decoder-\ndecoder residual connection. For the synthetic\ntasks, we implement a single layer encoder-decoder\nnetwork with only a single attention head in each\nblock. Our implementation of the Transformer is\nadapted from the implementation of (Rush, 2018).\nTable 4 provides some illustrative sample outputs\nof the models for the copy task.\nSOURCE &\nREFERENCE\n– there was no problem at all says douglas ford chief\nexecutive ofﬁcer of the futures exchange\nDIRECTIONAL\nTRANS -\nFORMER\n– there was no problem at all says douglas ford chief\nexecutive ofﬁcer of the futures exchange\nVANILLA\nTRANS -\nFORMER\n– there was no problem at all says douglas ford chief\nexecutive ofﬁcer\nTable 4: Sample outputs by the models on the copy task\non length 16. With absolute positional encodings the\nmodel overﬁts on terminal symbol at position 13 and\ngenerates sequence of length 12.\nFor the machine translation task, we use Open-\nNMT (Klein et al., 2017) for our implementation.\nFor preprocessing the German-English dataset we\nused the script from fairseq. The dataset contains\nabout 153k training sentences, 7k development sen-\ntences and 7k test sentences. The hyperparame-\nters to train the vanilla Transformer were obtained\nfrom fairseq’s guidelines. We tuned the parameters\non the validation set for the two baseline model.\nTo preprocess the English-Vietnamese dataset, we\nfollow Luong and Manning (2015). The dataset\ncontains about 133k training sentences. We use\n475\nthe tst2012 dataset containing 1.5k sentences for\nvalidation and tst2013 containing 1.3k sentences\nas test set. We use noam optimizer in all our ex-\nperiments. While tuning the network, we vary the\nnumber of layer from 1 to 4, the learning rate, the\nnumber of heads, the warmup steps, embedding\nsize and feedforward embedding size.\nE.2 Masking and Encodings\nOur implementation for directional transformer is\nbased on (Yang et al., 2019) but we use only unidi-\nrectional masking as opposed to bidirectional used\nin their setup. While tuning the models, we vary\nthe layers from 1 to 4, the learning rate, warmup\nsteps and the number of heads.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.717400848865509
    },
    {
      "name": "Computer science",
      "score": 0.7056818008422852
    },
    {
      "name": "Turing",
      "score": 0.6654013991355896
    },
    {
      "name": "Turing machine",
      "score": 0.5485650897026062
    },
    {
      "name": "Theoretical computer science",
      "score": 0.44953447580337524
    },
    {
      "name": "Residual",
      "score": 0.4225127696990967
    },
    {
      "name": "Feed forward",
      "score": 0.42046666145324707
    },
    {
      "name": "Algorithm",
      "score": 0.4048530161380768
    },
    {
      "name": "Artificial intelligence",
      "score": 0.368211567401886
    },
    {
      "name": "Computation",
      "score": 0.18300983309745789
    },
    {
      "name": "Programming language",
      "score": 0.1402975618839264
    },
    {
      "name": "Control engineering",
      "score": 0.1129327118396759
    },
    {
      "name": "Electrical engineering",
      "score": 0.10320639610290527
    },
    {
      "name": "Voltage",
      "score": 0.10255801677703857
    },
    {
      "name": "Engineering",
      "score": 0.08734166622161865
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 7
}