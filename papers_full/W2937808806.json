{
  "title": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models",
  "url": "https://openalex.org/W2937808806",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A120212391",
      "name": "Marjan Ghazvininejad",
      "affiliations": [
        "Meta (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2250897584",
      "name": "Omer Levy",
      "affiliations": [
        "Meta (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2356008774",
      "name": "Yinhan Liu",
      "affiliations": [
        "Meta (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A334758317",
      "name": "Luke Zettlemoyer",
      "affiliations": [
        "Meta (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2922349260",
    "https://openalex.org/W2963112338",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2964089333",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2913250058",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2892213699",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2920538220",
    "https://openalex.org/W2914120296"
  ],
  "abstract": "Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 6112–6121,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n6112\nMask-Predict: Parallel Decoding of\nConditional Masked Language Models\nMarjan Ghazvininejad∗ Omer Levy∗ Yinhan Liu∗ Luke Zettlemoyer\nFacebook AI Research\nSeattle, W A\nAbstract\nMost machine translation systems generate\ntext autoregressively from left to right. We,\ninstead, use a masked language modeling ob-\njective to train a model to predict any subset\nof the target words, conditioned on both the\ninput text and a partially masked target trans-\nlation. This approach allows for efﬁcient it-\nerative decoding, where we ﬁrst predict all\nof the target words non-autoregressively, and\nthen repeatedly mask out and regenerate the\nsubset of words that the model is least conﬁ-\ndent about. By applying this strategy for a con-\nstant number of iterations, our model improves\nstate-of-the-art performance levels for non-\nautoregressive and parallel decoding transla-\ntion models by over 4 BLEU on average. It is\nalso able to reach within about 1 BLEU point\nof a typical left-to-right transformer model,\nwhile decoding signiﬁcantly faster.1\n1 Introduction\nMost machine translation systems use sequen-\ntial decoding strategies where words are predicted\none-by-one. In this paper, we present a model and\na parallel decoding algorithm which, for a rela-\ntively small sacriﬁce in performance, can be used\nto generate translations in a constant number of\ndecoding iterations.\nWe introduce conditional masked language\nmodels (CMLMs), which are encoder-decoder ar-\nchitectures trained with a masked language model\nobjective (Devlin et al., 2018; Lample and Con-\nneau, 2019). This change allows the model to\nlearn to predict, in parallel, any arbitrary subset\nof masked words in the target translation. We use\ntransformer CMLMs, where the decoder’s self at-\ntention (Vaswani et al., 2017) can attend to the\n∗ ∗Equal contribution, sorted alphabetically.\n1Our code is publicly available at:\nhttps://github.com/facebookresearch/Mask-Predict\nentire sequence (left and right context) to pre-\ndict each masked word. We train with a simple\nmasking scheme where the number of masked tar-\nget tokens is distributed uniformly, presenting the\nmodel with both easy (single mask) and difﬁcult\n(completely masked) examples. Unlike recently\nproposed insertion models (Gu et al., 2019; Stern\net al., 2019), which treat each token as a separate\ntraining instance, CMLMs can train from the en-\ntire sequence in parallel, resulting in much faster\ntraining.\nWe also introduce a new decoding algorithm,\nmask-predict, which uses the order-agnostic na-\nture of CMLMs to support highly parallel decod-\ning. Mask-predict repeatedly masks out and re-\npredicts the subset of words in the current trans-\nlation that the model is least conﬁdent about,\nin contrast to recent parallel decoding translation\napproaches that repeatedly predict the entire se-\nquence (Lee et al., 2018). Decoding starts with a\ncompletely masked target text, to predict all of the\nwords in parallel, and ends after a constant num-\nber of mask-predict cycles. This overall strategy\nallows the model to repeatedly reconsider word\nchoices within a rich bi-directional context and, as\nwe will show, produce high-quality translations in\njust a few cycles.\nExperiments on benchmark machine translation\ndatasets show the strengths of mask-predict de-\ncoding for transformer CMLMs. With just 4 it-\nerations, BLEU scores already surpass the perfor-\nmance of the best non-autoregressive and parallel\ndecoding models.2\nWith 10 iterations, the approach outper-\nforms the current state-of-the-art parallel decod-\n2We use the term “parallel decoding” to refer to the family\nof approaches that can generate the entire target sequence in\nparallel. These are often referred to as “non-autoregressive”\napproaches, but both iterative reﬁnement (Lee et al., 2018)\nand our mask-predict approach condition on the model’s past\npredictions.\n6113\ning model (Lee et al., 2018) by gaps of 4-5 BLEU\npoints on the WMT’14 English-German transla-\ntion benchmark, and up to 3 BLEU points on\nWMT’16 English-Romanian, but with the same\nmodel complexity and decoding speed. When\ncompared to standard autoregressive transformer\nmodels, CMLMs with mask-predict offer a trade-\noff between speed and performance, trading up\nto 2 BLEU points in translation quality for a 3x\nspeed-up during decoding.\n2 Conditional Masked Language Models\nA conditional masked language model (CMLM)\npredicts a set of target tokensYmask given a source\ntext X and part of the target text Yobs. It makes\nthe strong assumption that the tokens Ymask are\nconditionally independent of each other (given X\nand Yobs), and predicts the individual probabilities\nP(y|X,Yobs) for each y∈Ymask. Since the num-\nber of tokens in Ymask is given in advance, the\nmodel is also implicitly conditioning on the length\nof the target sequence N = |Ymask|+ |Yobs|.\n2.1 Architecture\nWe adopt the standard encoder-decoder trans-\nformer for machine translation (Vaswani et al.,\n2017): a source-language encoder that does self-\nattention, and a target-language decoder that has\none set of attention heads over the encoder’s out-\nput and another set for the target language (self-\nattention). In terms of parameters, our architec-\nture is identical to the standard one. We deviate\nfrom the standard decoder by removing the self-\nattention mask that prevents left-to-right decoders\nfrom attending on future tokens. In other words,\nour decoder is bi-directional, in the sense that it\ncan use both left and right contexts to predict each\ntoken.\n2.2 Training Objective\nDuring training, we randomly selectYmask among\nthe target tokens. We ﬁrst sample the number\nof masked tokens from a uniform distribution be-\ntween one and the sequence’s length, and then ran-\ndomly choose that number of tokens. Following\nDevlin et al. (2018), we replace the inputs of the\ntokens Ymask with a special MASK token.\nWe optimize the CMLM for cross-entropy loss\nover every token in Ymask. This can be done in\nparallel, since the model assumes that the tokens in\nYmask are conditionally independent of each other.\nWhile the architecture can technically make pre-\ndictions over all target-language tokens (including\nYobs), we only compute the loss for the tokens in\nYmask.\n2.3 Predicting Target Sequence Length\nIn traditional left-to-right machine translation,\nwhere the target sequence is predicted token by\ntoken, it is natural to determine the length of the\nsequence dynamically by simply predicting a spe-\ncial EOS (end of sentence) token. However, for\nCMLMs to predict the entire sequence in paral-\nlel, they must know its length in advance. This\nproblem was recognized by prior work in non-\nautoregressive translation, where the length is pre-\ndicted with a fertility model (Gu et al., 2018) or by\npooling the encoder’s outputs into a length classi-\nﬁer (Lee et al., 2018).\nWe follow Devlin et al. (2018) and add a special\nLENGTH token to the encoder, akin to the CLS to-\nken in BERT. The model is trained to predict the\nlength of the target sequence N as the LENGTH\ntoken’s output, similar to predicting another token\nfrom a different vocabulary, and its loss is added to\nthe cross-entropy loss from the target sequence.\n3 Decoding with Mask-Predict\nWe introduce the mask-predict algorithm, which\ndecodes an entire sequence in parallel within a\nconstant number of cycles. At each iteration, the\nalgorithm selects a subset of tokens to mask, and\nthen predicts them (in parallel) using an underly-\ning CMLM. Masking the tokens where the model\nhas doubts while conditioning on previous high-\nconﬁdence predictions lets the model re-predict\nthe more challenging cases, but with more infor-\nmation. At the same time, the ability to make large\nparallel changes at each step allows mask-predict\nto converge on a high quality output sequence in a\nsub-linear number of decoding iterations.\n3.1 Formal Description\nGiven the target sequence’s length N (see Sec-\ntion 3.3), we deﬁne two variables: the target se-\nquence (y1,...,y N ) and the probability of each\ntoken (p1,...,p N ). The algorithm runs for a pre-\ndetermined number of iterations T, which is either\na constant or a simple function of N. At each iter-\nation, we perform a mask operation, followed by\npredict.\n6114\nsrc Der Abzug der franzsischen Kampftruppen wurde am 20. November abgeschlossen .\nt= 0 The departure of the French combat completed completed on 20 November .\nt= 1 The departure of French combat troops was completed on 20 November .\nt= 2 The withdrawal of French combat troops was completed on November 20th .\nFigure 1: An example from the WMT’14 DE-EN validation set that illustrates how mask-predict generates text. At\neach iteration, the highlighted tokens are masked and repredicted, conditioned on the other tokens in the sequence.\nMask For the ﬁrst iteration ( t = 0), we mask\nall the tokens. For later iterations, we mask the n\ntokens with the lowest probability scores:\nY(t)\nmask = arg min\ni\n(pi,n)\nY(t)\nobs = Y \\Y(t)\nmask\nThe number of masked tokens n is a function of\nthe iteration t; speciﬁcally, we use linear decay\nn= N·T−t\nT , where T is the total number of itera-\ntions. For example, if T = 10, we will mask 90%\nof the tokens at t= 1, 80% at t= 2, and so forth.\nPredict After masking, the CMLM predicts the\nmasked tokens Y(t)\nmask, conditioned on the source\ntext X and the unmasked target tokens Y(t)\nobs. We\nselect the prediction with the highest probability\nfor each masked token yi ∈Y(t)\nmask and update its\nprobability score accordingly:\ny(t)\ni = arg max\nw\nP(yi = w|X,Y (t)\nobs)\np(t)\ni = max\nw\nP(yi = w|X,Y (t)\nobs)\nThe values and the probabilities of unmasked to-\nkens Y(t)\nobs remain unchanged:\ny(t)\ni = y(t−1)\ni\np(t)\ni = p(t−1)\ni\nWe tried updating or decaying these probabilities\nin preliminary experiments, but found that this\nheuristic works well despite the fact that some\nprobabilities are stale.\n3.2 Example\nFigure 1 illustrates how mask-predict can generate\na good translation in just three iterations.\nIn the ﬁrst iteration ( t = 0), the entire target\nsequence is masked ( Y(0)\nmask = Y and Y(0)\nobs = ∅),\nand is thus generated by the CMLM in a purely\nnon-autoregressive process:\nP(Y(0)\nmask|X,Y (0)\nobs ) =P(Y|X)\nThis produces an ungrammatical translation with\nrepetitions (“completed completed”), which is\ntypical of non-autoregressive models due to the\nmulti-modality problem (Gu et al., 2018).\nIn the second iteration ( t = 1), we select 8 of\nthe 12 tokens generated in the previous step; these\ntoken were predicted with the lowest probabilities\nat t = 0. We mask them and repredict with the\nCMLM, while conditioning on the 4 unmasked\ntokens Y(1)\nobs = {“The”,“20”,“November”,“.”}.\nThis results in a more grammatical and accurate\ntranslation. Our analysis shows that this second it-\neration removes most repetitions, perhaps because\nconditioning on even a little bit of the target se-\nquence is enough to collapse the multi-modal tar-\nget distribution into a single output (Section 5.1).\nIn the last iteration ( t = 2), we select the 4\nof the 12 tokens that had the lowest probabilities.\nTwo of those tokens were predicted at the ﬁrst step\n(t = 0), and not repredicted at the second step\n(t= 1). It is quite common for earlier predictions\nto be masked at later iterations because they were\npredicted with less information and thus tend to\nhave lower probabilities. Now that the model is\nconditioning on 8 tokens, it is able to produce an\nmore ﬂuent translation; “withdrawal” is a better\nﬁt for describing troop movement, and “November\n20th” is a more common date format in English.\n3.3 Deciding Target Sequence Length\nWhen generating, we ﬁrst compute the CMLM’s\nencoder, and then use the LENGTH token’s en-\ncoding to predict a distribution over the target se-\nquence’s length (see Section 2.3). Since much of\nthe CMLM’s computation can be batched, we se-\nlect the top ℓ length candidates with the highest\nprobabilities, and decode the same example with\ndifferent lengths in parallel. We then select the se-\nquence with the highest average log-probability as\nour result:\n1\nN\n∑\nlog p(T)\ni\n6115\nOur analysis reveals that translating multiple can-\ndidate sequences of different lengths can improve\nperformance (see Section 5.3).\n4 Experiments\nWe evaluate CMLMs with mask-predict decoding\non standard machine translation benchmarks. We\nﬁnd that our approach signiﬁcantly outperforms\nprior parallel decoding machine translation meth-\nods and even approaches the performance of stan-\ndard autoregressive models (Section 4.2), while\ndecoding signiﬁcantly faster (Section 4.3).\n4.1 Experimental Setup\nTranslation Benchmarks We evaluate on three\nstandard datasets, WMT’14 EN-DE (4.5M sen-\ntence pairs), WMT’16 EN-RO (610k pairs) and\nWMT’17 EN-ZH (20M pairs) in both directions.\nThe datasets are tokenized into subword units us-\ning BPE (Sennrich et al., 2016). We use the same\npreprocessed data as Vaswani et al. (2017) and Wu\net al. (2019) for WMT’14 EN-DE and WMT’17\nEN-ZH respectively, and use the data from Lee\net al. (2018) for WMT’16 EN-RO. We evaluate\nperformance with BLEU (Papineni et al., 2002)\nfor all language pairs, except from EN to ZH,\nwhere we use SacreBLEU (Post, 2018).3\nHyperparameters We follow most of the stan-\ndard hyperparameters for transformers in the base\nconﬁguration (Vaswani et al., 2017): 6 layers per\nstack, 8 attention heads per layer, 512 model di-\nmensions, 2048 hidden dimensions. We also ex-\nperiment with 512 hidden dimensions, for com-\nparison with previous parallel decoding models\n(Gu et al., 2018; Lee et al., 2018). We follow\nthe weight initialization scheme from BERT (De-\nvlin et al., 2018), which samples weights from\nN(0,0.02), initializes biases to zero, and sets\nlayer normalization parameters to β = 0,γ = 1.\nFor regularization, we use 0.3 dropout, 0.01 L2\nweight decay, and smoothed cross validation loss\nwith ε = 0 .1. We train batches of 128k to-\nkens using Adam (Kingma and Ba, 2015) with\nβ = (0.9,0.999) and ε = 10−6. The learning\nrate warms up to a peak of 5 ·10−4 within 10,000\nsteps, and then decays with the inverse square-\nroot schedule. We trained all models for 300k\nsteps, measured the validation loss at the end of\neach epoch, and averaged the 5 best checkpoints\n3SacreBLEU hash: BLEU+case.mixed+lang.en-zh\n+numrefs.1+smooth.exp+test.wmt17+tok.zh+version.1.3.7\nto create the ﬁnal model. During decoding, we\nuse a beam size of b = 5 for autoregressive de-\ncoding, and similarly use ℓ= 5length candidates\nfor mask-predict decoding. We trained with mixed\nprecision ﬂoating point arithmetic on two DGX-\n1 machines, each with eight 16GB Nvidia V100\nGPUs interconnected by Inﬁniband (Micikevicius\net al., 2018).\nModel Distillation Following previous work on\nnon-autoregressive and insertion-based machine\ntranslation (Gu et al., 2018; Lee et al., 2018; Stern\net al., 2019), we train CMLMs on translations\nproduced by a standard left-to-right transformer\nmodel (large for EN-DE and EN-ZH, base for EN-\nRO). For a fair comparison, we also train standard\nleft-to-right base transformers on translations pro-\nduced by large transformer models for EN-DE and\nEN-ZH, in addition to the standard baselines. We\nanalyze the impact of distillation in Section 5.4.\n4.2 Translation Quality\nWe compare our approach to three other parallel\ndecoding translation methods: the fertility-based\nsequence-to-sequence model of Gu et al. (2018),\nthe CTC-loss transformer of Libovick ´y and Helcl\n(2018), and the iterative reﬁnement approach of\nLee et al. (2018). The ﬁrst two methods are purely\nnon-autoregressive, while the iterative reﬁnement\napproach is only non-autoregressive in the ﬁrst de-\ncoding iteration, similar to our approach. In terms\nof speed, each mask-predict iteration is virtually\nequivalent to a reﬁnement iteration.\nTable 1 shows that among the parallel decoding\nmethods, our approach yields the highest BLEU\nscores by a considerable margin. When control-\nling for the number of parameters (i.e. considering\nonly the smaller CMLM conﬁguration), CMLMs\nscore roughly 4 BLEU points higher than the pre-\nvious state of the art on WMT’14 EN-DE, in both\ndirections. Another striking result is that a CMLM\nwith only 4 mask-predict iterations yields higher\nscores than 10 iterations of the iterative reﬁnement\nmodel; in fact, only 3 mask-predict iterations are\nnecessary for achieving a new state of the art on\nboth directions of WMT’14 EN-DE (not shown).\nThe translations produced by CMLMs with\nmask-predict also score competitively when com-\npared to strong transformer-based autoregressive\nmodels. In all 4 benchmarks, our base CMLM\nreaches within 0.5-1.2 BLEU points from a well-\ntuned base transformer, a relative decrease of less\n6116\nModel Dimensions Iterations WMT’14 WMT’16\n(Model/Hidden) EN-DE DE-EN EN-RO RO-EN\nNAT w/ Fertility (Gu et al., 2018) 512/512 1 19.17 23.20 29.79 31.44\nCTC Loss (Libovick´y and Helcl, 2018) 512/4096 1 17.68 19.80 19.93 24.71\nIterative Reﬁnement (Lee et al., 2018) 512/512 1 13.91 16.77 24.45 25.73\n512/512 10 21.61 25.48 29.32 30.19\n(Dynamic #Iterations) 512/512 ? 21.54 25.43 29.66 30.30\nSmall CMLM with Mask-Predict 512/512 1 15.06 19.26 20.12 20.36\n512/512 4 24.17 28.55 30.00 30.43\n512/512 10 25.51 29.47 31.65 32.27\nBase CMLM with Mask-Predict 512/2048 1 18.05 21.83 27.32 28.20\n512/2048 4 25.94 29.90 32.53 33.23\n512/2048 10 27.03 30.53 33.08 33.31\nBase Transformer (Vaswani et al., 2017) 512/2048 N 27.30 — — — — — —\nBase Transformer (Our Implementation) 512/2048 N 27.74 31.09 34.28 33.99\nBase Transformer (+Distillation) 512/2048 N 27.86 31.07 — — — —\nLarge Transformer (Vaswani et al., 2017) 1024/4096 N 28.40 — — — — — —\nLarge Transformer (Our Implementation) 1024/4096 N 28.60 31.71 — — — —\nTable 1: The performance (BLEU) of CMLMs with mask-predict, compared to other parallel decoding machine\ntranslation methods. The standard (sequential) transformer is shown for reference. Bold numbers indicate state-\nof-the-art performance among parallel decoding methods.\nModel Dimensions Iterations WMT’17\n(Model/Hidden) EN-ZH ZH-EN\nBase CMLM with Mask-Predict 512/2048 1 24.23 13.64\n512/2048 4 32.63 21.90\n512/2048 10 33.19 23.21\nBase Transformer (Our Implementation) 512/2048 N 34.31 23.74\nBase Transformer (+Distillation) 512/2048 N 34.44 23.99\nLarge Transformer (Our Implementation) 1024/4096 N 35.01 24.65\nTable 2: The performance (BLEU) of CMLMs with mask-predict, compared to the standard (sequential) trans-\nformer on WMT’ 17 EN-ZH.\nthan 4% in translation quality. In many scenarios,\nthis is an acceptable price to pay for a signiﬁcant\nspeedup from parallel decoding.\nTable 2 shows that these trends also hold for\nEnglish-Chinese translation, in both directions,\ndespite major linguistic differences between the\ntwo languages.\n4.3 Decoding Speed\nBecause CMLMs can predict the entire sequence\nin parallel, mask-predict can translate an entire\nsequence in a constant number of decoding iter-\nations. Does this appealing theoretical property\ntranslate into a wall-time speed-up in practice? By\ncomparing the actual decoding times, we show\nthat, for some sacriﬁce in performance, our paral-\nlel method can translate much faster than standard\nsequential transformers.\nSetup As the baseline system, we use the base\ntransformer with beam search ( b = 5) to trans-\nlate WMT’14 EN-DE; we also use greedy search\n(b = 1) as a faster but less accurate baseline. For\nCMLMs, we vary the number of mask-predict it-\nerations ( T = 4,..., 10) and length candidates\n(ℓ= 1,2,3). For both models, we decode batches\nof 10 sentences. 4 For each decoding run, we\nmeasure the performance (BLEU) and wall time\n(seconds) from when the model and data have\nbeen loaded until the last example has been trans-\nlated, and calculate the relative decoding speed-up\n(CMLM time / baseline time) to assess the speed-\nperformance trade-off.\nThe implementation of both the baseline trans-\nformer and our CMLM is based on fairseq\n(Gehring et al., 2017), which efﬁciently decodes\nleft-to-right transformers by caching the state.\nCaching reduces the baseline’s decoding speed\nfrom 210 seconds to 128.5; CMLMs do not use\ncached decoding. All experiments used exactly\nthe same machine and the same single GPU.\n4The batch size was chosen arbitrarily; mask-predict can\nscale up to much larger batch sizes.\n6117\nFigure 2: The trade-off between speed-up and translation quality of a base CMLM with mask-predict, compared\nto the standard sequentially-decoded base transformer on the WMT’14 EN-DE test set, with beam sizes b = 1\n(orange triangle) and b= 5(red triangle). Each blue circle represents a mask-predict decoding run with a different\nnumber of iterations (T = 4,..., 10) and length candidates (ℓ= 1,2,3).\nResults Figure 2 shows the speed-performance\ntrade-off. We see that mask-predict is versatile;\non one hand, we can translate over 3 times faster\nthan the baseline at a cost of 2 BLEU points\n(T = 4, ℓ= 2), or alternatively retain a high qual-\nity of 27.03 BLEU while gaining a 30% speed-up\n(T = 4, ℓ = 2). Surprisingly, this latter conﬁg-\nuration outperforms an autoregressive transformer\nwith greedy decoding (b = 1) in both quality and\nspeed. We also observe that more balanced con-\nﬁgurations (e.g. T = 8, ℓ = 2) yield similar per-\nformance to the single-beam autoregressive trans-\nformer, but decode much faster.\n5 Analysis\nTo complement the quantitative results in Sec-\ntion 4, we present qualitative analysis that pro-\nvides some intuition as to why our approach works\nand where future work could potentially improve\nit.\n5.1 Why Are Multiple Iterations Necessary?\nVarious non-autoregressive translation models, in-\ncluding our own CMLM, make the strong as-\nsumption that the individual token predictions are\nconditionally independent of each other. Such a\nmodel might consider two or more possible trans-\nlations, A and B, but because there is no coordi-\nnation mechanism between the token predictions,\nit could predict one token from A and another to-\nken from B. This problem, known as the multi-\nmodality problem (Gu et al., 2018), often man-\nifest as token repetitions in the output when the\nmodel has multiple hypotheses that predict the\nsame word wwith high conﬁdence, but at different\npositions.\nWe hypothesize that multiple mask-predict it-\nerations alleviate the multi-modality problem by\nallowing the model to condition on parts of the in-\nput, thus collapsing the multi-modal distribution\ninto a sharper uni-modal distribution. To test our\n6118\nIterations WMT’14 EN-DE WMT’16 EN-RO\nBLEU Reps BLEU Reps\nT = 1 18.05 16.72% 27.32 9.34%\nT = 2 22.91 5.40% 31.08 2.82%\nT = 3 24.99 2.03% 32.19 1.26%\nT = 4 25.94 1.07% 32.53 0.87%\nT = 5 26.30 0.72% 32.62 0.61%\nTable 3: The performance (BLEU) and percentage of\nrepeating tokens when decoding with a different num-\nber of mask-predict iterations (T).\nhypothesis, we measure the percentage of repet-\nitive tokens produced by each iteration of mask-\npredict as a proxy metric for the multi-modality\nproblem.\nTable 3 shows that, indeed, the proportion of\nrepetitive tokens drops drastically during the ﬁrst\n2-3 iterations. This ﬁnding suggests that the ﬁrst\nfew iterations are critical for converging into a\nuni-modal distribution. The decrease in repeti-\ntions also correlates with the steep rise in trans-\nlation quality (BLEU), supporting the conjecture\nof Gu et al. (2018) that multi-modality is a major\nroadblock for purely non-autoregressive machine\ntranslation.\n5.2 Do Longer Sequences Need More\nIterations?\nA potential concern with using a constant amount\nof decoding iterations is that it may be effective for\nshort sequences (where the number of iterationsT\nis closer to the output’s lengthN), but insufﬁcient\nfor longer sequences. To determine whether this\nis the case, we use compare-mt (Neubig et al.,\n2019) to bucket the evaluation data by target sen-\ntence length and compute the performance with\ndifferent values of T.\nTable 4 shows that increasing the number of de-\ncoding iterations ( T) appears to mainly improve\nthe performance on longer sequences. Having said\nthat, the performance differences across length\nbuckets are not very large, and it seems that even 4\nmask-predict iterations are enough to produce de-\ncent translations for long sequences (40 ≤N).\n5.3 Do More Length Candidates Help?\nTraditional autoregressive models can dynami-\ncally decide the length of the target sequence by\ngenerating a special END token when they are\ndone, but that is not true for models that de-\ncode multiple tokens in parallel, such as CMLMs.\nTo address this problem, our model predicts the\nT = 4 T = 10 T = N\n1 ≤ N <10 21.8 22.4 22.4\n10 ≤ N <20 24.6 25.9 26.0\n20 ≤ N <30 24.9 26.7 27.1\n30 ≤ N <40 24.9 26.7 27.6\n40 ≤ N 25.0 27.5 28.1\nTable 4: The performance (BLEU) of base CMLM\nwith different amounts of mask-predict iterations ( T)\non WMT’14 EN-DE, bucketed by target sequence\nlength (N). Decoding with ℓ= 1length candidates.\nLength WMT’14 EN-DE WMT’16 EN-RO\nCandidates BLEU LP BLEU LP\nℓ = 1 26.56 16.1% 32.75 13.8%\nℓ = 2 27.03 30.6% 33.06 26.1%\nℓ = 3 27.09 43.1% 33.11 39.6%\nℓ = 4 27.09 53.1% 32.13 49.2%\nℓ = 5 27.03 62.2% 33.08 57.5%\nℓ = 6 26.91 69.5% 32.91 64.3%\nℓ = 7 26.71 75.5% 32.75 70.4%\nℓ = 8 26.59 80.3% 32.50 74.6%\nℓ = 9 26.42 83.8% 32.09 78.3%\nGold 27.27 — 33.20 —\nTable 5: The performance (BLEU) of base CMLM\nwith 10 mask-predict iterations (T = 10), varied by the\nnumber of length candidates (ℓ), compared to decoding\nwith the reference target length (Gold). Length preci-\nsion (LP) is the percentage of examples that contain the\ncorrect length as one of their candidates.\nlength of the target sequence (Section 2.3) and de-\ncodes multiple length candidates in parallel (Sec-\ntion 3.3). We compare our model’s performance\nwith a varying number of length candidates to its\nperformance when conditioned on the reference\n(gold) target length in order to determine how ac-\ncurate it is at predicting the correct length and\nassess the relative contribution of decoding with\nmultiple length candidates.\nTable 5 shows that having multiple candidates\ncan increase performance almost as much as con-\nditioning on the gold length. Surprisingly, adding\ntoo many candidates can even degrade perfor-\nmance. We suspect that because CMLMs are im-\nplicitly conditioned on the target length, producing\na translation that is too short (i.e. high precision,\nlow recall) will have a high average log probabil-\nity. In preliminary experiments, we tried to ad-\ndress this issue by weighting the different candi-\ndates according to the model’s length prediction,\nbut this approach gave too much weight to the top\ncandidate and resulted in lower performance.\n6119\nIterations WMT’14 EN-DE WMT’16 EN-RO\nRaw Dist Raw Dist\nT = 1 10.64 18.05 21.22 27.32\nT = 4 22.25 25.94 31.40 32.53\nT = 10 24.61 27.03 32.86 33.08\nTable 6: The performance (BLEU) of base CMLM,\ntrained with either raw data (Raw) or knowledge dis-\ntillation from an autoregressive model (Dist).\n5.4 Is Model Distillation Necessary?\nPrevious work on non-autoregressive and\ninsertion-based machine translation reported that\nit was necessary to train their models on text\ngenerated by an autoregressive teacher model,\na process known as distillation. To determine\nCMLM’s dependence on this process, we train\na models on both raw and distilled data, and\ncompare their performance.\nTable 6 shows that in every case, training with\nmodel distillation substantially outperforms train-\ning on raw data. The gaps are especially large\nwhen decoding with a single iteration (purely non-\nautoregressive). Overall, it appears as though\nCMLMs are heavily dependent on model distilla-\ntion.\nOn the English-Romanian benchmark, the dif-\nferences are much smaller, and after 10 iterations\nthe raw-data model can perform comparably with\nthe distilled model. A possible explanation is that\nour teacher model was weaker for this dataset due\nto insufﬁcient hyperparameter tuning. Alterna-\ntively, it could also be the case that the English-\nGerman dataset is much noisier than the English-\nRomanian one, and that the teacher model essen-\ntially cleans the training data. Unfortunately, we\ndo not have enough evidence to support or refute\neither hypothesis at this time.\n6 Related Work\nTraining Masked Language Models with\nTranslation Data Recent work by Lample and\nConneau (2019) shows that training a masked\nlanguage model on sentence-pair translation data,\nas a pre-training step, can improve performance\non cross-lingual tasks, including autoregressive\nmachine translation. Our training scheme builds\non their work, with the following differences: we\nuse separate model parameters for source and\ntarget texts (encoder and decoder), and we also\nuse a different masking scheme. Speciﬁcally, we\nmask a varying percentage of tokens, only from\nthe target, and do not replace input tokens with\nnoise. Most importantly, the goal of our work is\ndifferent; we do not use CMLMs for pre-training,\nbut to directly generate text with mask-predict\ndecoding.\nConcurrently with our work, Song et al. (2019)\nextend the approach of Lample and Conneau\n(2019) by using separate encoder and decoder pa-\nrameters (as in our model) and pre-training them\njointly in an autoregressive version of masked\nlanguage modeling, although with monolingual\ndata. While this work demonstrates that pre-\ntraining CMLMs can improve autoregressive ma-\nchine translation, it does not try to leverage the\nparallel and bi-directional nature of CMLMs to\ngenerate text in a non-left-to-right manner.\nGenerating from Masked Language Models\nOne such approach for generating text from a\nmasked language model casts BERT (Devlin et al.,\n2018), a non-conditional masked language model,\nas a Markov random ﬁeld (Wang and Cho, 2019).\nBy masking a sequence of length N and then\niteratively sampling a single token at each time\nfrom the model (either sequentially or in arbitrary\norder), one can produce grammatical examples.\nWhile this sampling process has a theoretical jus-\ntiﬁcation, it also requires N forward passes of the\nmodel; mask-predict decoding, on the other hand,\ncan produce text in a constant number of itera-\ntions.\nParallel Decoding for Machine Translation\nThere have been several advances in parallel\ndecoding machine translation by training non-\nautoregressive models. Gu et al. (2018) introduce\na transformer-based approach with explicit word\nfertility, and identify the multi-modality problem.\nLibovick´y and Helcl (2018) approach the multi-\nmodality problem by collapsing repetitions with\nthe Connectionist Temporal Classiﬁcation training\nobjective (Graves et al., 2006). Perhaps most simi-\nlar to our work is the iterative reﬁnement approach\nof Lee et al. (2018), in which the model corrects\nthe original non-autoregressive prediction by pass-\ning it multiple times through a denoising autoen-\ncoder. A major difference is that Lee et al. (2018)\ntrain their noisy autoencoder to deal with corrupt\ninputs by applying stochastic corruption heuristics\non the training data, while we simply mask a ran-\ndom number of input tokens. We also show that\n6120\nour approach outperforms all of these models by\nwide margins.\nArbitrary Order Language Generation Fi-\nnally, recent work has developed insertion-based\ntransformers for arbitrary, but ﬁxed, word order\ngeneration (Gu et al., 2019; Stern et al., 2019).\nWhile they do not decode in a constant number of\niterations, Stern et al. (2019) show strong results in\nlogarithmic time. Both models treat each token in-\nsertion as a separate training example, which can-\nnot be computed in parallel with every other in-\nsertion in the same sequence. This makes training\nsigniﬁcantly more expensive that standard trans-\nformers (which use causal attention masking) and\nour CMLMs (which can predict all of the masked\ntokens in parallel).\n7 Conclusion\nThis work introduces conditional masked lan-\nguage models and a novel mask-predict decod-\ning algorithm that leverages their parallelism to\ngenerate text in a constant number of decoding\niterations. We show that, in the context of ma-\nchine translation, our approach substantially out-\nperforms previous parallel decoding methods, and\ncan approach the performance of sequential au-\ntoregressive models while decoding much faster.\nWhile there are still open problems, such as the\nneed to condition on the target’s length and the\ndependence on knowledge distillation, our re-\nsults provide a signiﬁcant step forward in non-\nautoregressive and parallel decoding approaches\nto machine translation. In a broader sense, this pa-\nper shows that masked language models are useful\nnot only for representing text, but also for gener-\nating text efﬁciently.\nAcknowledgements\nWe thank Abdelrahman Mohamed for sharing his\nexpertise on non-autoregressive models, and our\ncolleagues at FAIR for valuable feedback.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional\nSequence to Sequence Learning. ArXiv e-prints.\nAlex Graves, Santiago Fern ´andez, Faustino Gomez,\nand J ¨urgen Schmidhuber. 2006. Connectionist\ntemporal classiﬁcation: Labelling unsegmented se-\nquence data with recurrent neural networks. In Pro-\nceedings of the 23rd international conference on\nMachine learning, pages 369–376. ACM.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor OK\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. In ICLR.\nJiatao Gu, Qi Liu, and Kyunghyun Cho. 2019.\nInsertion-based decoding with automatically\ninferred generation order. arXiv preprint\narXiv:1902.01370.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference for Learning Representations.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative reﬁnement. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1173–\n1182, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nJindˇrich Libovick ´y and Jind ˇrich Helcl. 2018. End-to-\nend non-autoregressive neural machine translation\nwith connectionist temporal classiﬁcation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 3016–\n3021, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev,\nGanesh Venkatesh, and Hao Wu. 2018. Mixed preci-\nsion training. In International Conference on Learn-\ning Representations.\nGraham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel,\nDanish Pruthi, and Xinyi Wang. 2019. compare-mt:\nA tool for holistic comparison of language gener-\nation systems. In Meeting of the North American\nChapter of the Association for Computational Lin-\nguistics (NAACL) Demo Track, Minneapolis, USA.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\n6121\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to sequence\npre-training for language generation. arXiv preprint\narXiv:1905.02450.\nMitchell Stern, William Chan, Jamie Kiros, and Jakob\nUszkoreit. 2019. Insertion transformer: Flexible se-\nquence generation via insertion operations. arXiv\npreprint arXiv:1902.03249.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nAlex Wang and Kyunghyun Cho. 2019. Bert has\na mouth, and it must speak: Bert as a markov\nrandom ﬁeld language model. arXiv preprint\narXiv:1902.04094.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. 2019. Pay less attention\nwith lightweight and dynamic convolutions. Inter-\nnational Conference on Learning Representations.",
  "topic": "Decoding methods",
  "concepts": [
    {
      "name": "Decoding methods",
      "score": 0.8839695453643799
    },
    {
      "name": "BLEU",
      "score": 0.8068603277206421
    },
    {
      "name": "Computer science",
      "score": 0.775790810585022
    },
    {
      "name": "Machine translation",
      "score": 0.7375751733779907
    },
    {
      "name": "Transformer",
      "score": 0.6678547859191895
    },
    {
      "name": "Autoregressive model",
      "score": 0.5863831639289856
    },
    {
      "name": "Language model",
      "score": 0.5585402846336365
    },
    {
      "name": "Translation (biology)",
      "score": 0.5503631234169006
    },
    {
      "name": "Speech recognition",
      "score": 0.529204249382019
    },
    {
      "name": "Point (geometry)",
      "score": 0.5029887557029724
    },
    {
      "name": "Constant (computer programming)",
      "score": 0.4821452796459198
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47783803939819336
    },
    {
      "name": "Natural language processing",
      "score": 0.4473266005516052
    },
    {
      "name": "Algorithm",
      "score": 0.39828166365623474
    },
    {
      "name": "Arithmetic",
      "score": 0.3463256061077118
    },
    {
      "name": "Mathematics",
      "score": 0.15592318773269653
    },
    {
      "name": "Statistics",
      "score": 0.08677473664283752
    },
    {
      "name": "Programming language",
      "score": 0.06477752327919006
    },
    {
      "name": "Voltage",
      "score": 0.05517569184303284
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ]
}