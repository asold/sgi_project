{
    "title": "Personalized Query Rewriting in Conversational AI Agents",
    "url": "https://openalex.org/W3106451700",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Roshan-Ghias, Alireza",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224573820",
            "name": "Mathialagan, Clint Solomon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224573819",
            "name": "Ponnusamy, Pragaash",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2949514118",
            "name": "Mathias Lambert",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202080235",
            "name": "Guo, Chenlei",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2921671634",
        "https://openalex.org/W3105804405",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2963386218",
        "https://openalex.org/W2889012072",
        "https://openalex.org/W1564094940",
        "https://openalex.org/W2586847566",
        "https://openalex.org/W2810084418",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2340995799",
        "https://openalex.org/W2922386288",
        "https://openalex.org/W2745673470",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W2890394457",
        "https://openalex.org/W2963491014",
        "https://openalex.org/W2963825865",
        "https://openalex.org/W2979478117"
    ],
    "abstract": "Spoken language understanding (SLU) systems in conversational AI agents often experience errors in the form of misrecognitions by automatic speech recognition (ASR) or semantic gaps in natural language understanding (NLU). These errors easily translate to user frustrations, particularly so in recurrent events e.g. regularly toggling an appliance, calling a frequent contact, etc. In this work, we propose a query rewriting approach by leveraging users' historically successful interactions as a form of memory. We present a neural retrieval model and a pointer-generator network with hierarchical attention and show that they perform significantly better at the query rewriting task with the aforementioned user memories than without. We also highlight how our approach with the proposed models leverages the structural and semantic diversity in ASR's output towards recovering users' intents.",
    "full_text": "PERSONALIZED QUERY REWRITING IN CONVERSATIONAL AI AGENTS\nAlireza Roshan-Ghias, Clint Solomon Mathialagan, Pragaash Ponnusamy, Lambert Mathias, Chenlei Guo\nAmazon Alexa, Seattle, W A, USA\nABSTRACT\nSpoken language understanding (SLU) systems in conversa-\ntional AI agents often experience errors in the form of mis-\nrecognitions by automatic speech recognition (ASR) or se-\nmantic gaps in natural language understanding (NLU). These\nerrors easily translate to user frustrations, particularly so in re-\ncurrent events e.g. regularly toggling an appliance, calling a\nfrequent contact, etc. In this work, we propose a query rewrit-\ning approach by leveraging users’ historically successful in-\nteractions as a form of memory. We present a neural retrieval\nmodel and a pointer-generator network with hierarchical at-\ntention and show that they perform signiﬁcantly better at the\nquery rewriting task with the aforementioned user memories\nthan without. We also highlight how our approach with the\nproposed models leverages the structural and semantic diver-\nsity in ASR’s output towards recovering users’ intents.\nIndex Terms— spoken language understanding (SLU),\nquery rewriting, memory, pointer networks, neural retrieval.\n1. INTRODUCTION\nAdvances in SLU technologies have fueled the growth of con-\nversational AI agents such as Amazon Alexa, Google Home\nand Siri, where voice is the primary input modality. Deal-\ning with ASR and NLU errors in these systems is crucial for\nproviding a frustration-free customer experience.\nThere has been a large body of work surrounding the de-\ntection and correction of ASR errors, which include either us-\ning contextual vocabulary during decoding [1–3], augment-\ning speech recognition training datasets with implicit tran-\nscripts [4], or rectifying historically prevalent errors via lattice\nrescoring [5]. All of these works focus on improving the ASR\noutput. However, downstream components such as NLU, the\ndialog state manager, and skills can very well introduce er-\nrors, which are typically solved independently of each other.\nMoreover, some errors may be only recoverable in the context\nof the whole conversation. A holistic approach that bridges\nthe learning across all components can decrease user friction,\nregardless of the source of error.\nQuery rewriting has been successfully applied in search\napplications to increase recall, and/or correct misspelling by\nthe users [6,7]. Similarly, we propose resolving errors in SLU\nsystems using query rewriting where the rewrite occurs on the\nFig. 1. The query rewriting (QR) engine in context of the\nend-to-end SLU service.\nutterance output of a high quality ASR system before being\nfed downstream into the NLU component. This reformula-\ntion strategy allows us to deal with errors across the entire\npipeline in a single generalized framework. Similar to ad-\nvances in dialog-based tasks that beneﬁt from being grounded\nby dialogue contexts [8], knowledge bases [9, 10], and users’\npersona [11,12], we hypothesize that the query rewriting task\nshould also be grounded on users’ interaction histories.\nFigure 1 shows how the query rewriting engine ﬁts into\nAlexa’s architecture as a whole. To exemplify this, consider\nwhen the user says ”laundry room on”, but ASR wrongly tran-\nscribes it as ”launch room on”. By inspecting the user’s his-\ntory, we would know that the user never had asuccessful utter-\nance referencing ”launch room”. As such, we propose rewrit-\ning it as ”laundry room on”. Thereafter, both the original and\nthe rewrite go through the NLU system in parallel before be-\ning arbitrated by a merger in rendering the ﬁnal decision that\nindeed, the user actually meant to turn on the laundry room.\nIn this paper, we propose a personalized query rewriting\napproach by leveraging user memories to reduce errors in a\nlarge-scale SLU system. Our main contributions are:\n1. We present a novel approach for query rewriting that\nleverages user-speciﬁc memories aggregated based on\ntheir successful historical interactions.\n2. We compare and contrast two neural architectures - a\narXiv:2011.04748v1  [cs.AI]  9 Nov 2020\nFig. 2. Retrieval model with user memory. Blue and green\nrectangles denote encoded vectors and attention, accordingly.\nretrieval based approach as well as a generative ap-\nproach on our task.\n3. We demonstrate the impact of leveraging ASR uncer-\ntainty in query rewriting.\n2. DATA\nIn this work, we sample a subset of anonymized user inter-\nactions with Alexa (complying with user instructions on data\nretention) for our experiments, focusing speciﬁcally on smart\nhome, where there is a need to understand the set of person-\nalized appliances each user has.\nOur data consists of two parts: User memories and\nrephrase pairs. For user memories, we mine and aggre-\ngate queries over 4 weeks of the successful1 dialogues. For\nrephrase pairs, we mine pairs of smart home utterances across\nAlexa users in which the ﬁrst utterance 2 is defective, i.e.\nresulted in an Alexa error, and the second consecutive suc-\ncessful utterance3. In order to minimize overlap, and make\nthe comparisons fair, the rephrase data is collected for the\nweek after the user memory is aggregated. Since some users\nmight change their mind in the second turn, or they might\nissue a new command, the second utterance may not seman-\ntically match a memory utterance. We consider these cases\nas ”non-rewritable”. A semantic match is deﬁned on the in-\ntent and slot output of the NLU system. This allows us to\ndeclare, for example, ”turn on the light” and ”can you turn\non light” as a semantic match. We opted to not remove the\nnon-rewritable pairs from our training data, since we want to\nlearn to not rewrite these cases. Note that we do not need any\n”annotations” to establish our training and testing dataset.\nWe use top 5 ASR n-best for the ﬁrst utterance in the\nrephrase pair to incorporate ASR uncertainty. We ﬁnally\n1Deﬁned as an interaction that Alexa executed for the user and the user\ndid not initiate a follow-up intent in that domain for a speciﬁed period of\ntime. For example, the air conditioner was set to a setting, or a device was\nturned off, and the user did not change them for a given period of time\n2We use the n-best output from the ASR system, and so refer to this as\nASR n-best\n3We refer to this as a rephrase\njoined these two datasets ( ∼1M data points), and split them\nby randomly assigning 80% of users to training, and the rest\nto the testing set.\n3. MODELS\n3.1. Baseline models\n3.1.1. Pointer Network without User Memory\nIn order to evaluate the impact of having user memories on the\nperformance of query rewriting, we ﬁrst establish a baseline\nwithout memory. This approach is akin to learning to ”re-\nrank” the ASR n-best using the rephrase dataset. The model\nis similar to the pointer network model (Figure 3) without the\nmemory related components. For more details, we refer the\nreader to Section 3.2.\n3.1.2. Retrieval Baseline with User Memory\nWe use a point-wise one-way attention for the retrieval model\n[13]. Figure 2 shows the architecture of the model. We en-\ncode each ASR n-best and memory using bi-LSTM, then use\nthe encoded memory to attend on the ASR n-best, and cal-\nculate ASR n-best context accordingly. We then concatenate\nthe encoded memory and the ASR n-best context, and pass it\nthrough two dense layers followed by a sigmoid. The label is\n1, if the rewrite matches the rephrase. The model is trained\nusing cross-entropy loss. Each memory (represented as an\nutterance) is encoded independently, and the highest scoring\nmemory is retrieved as the ﬁnal query rewrite.\nTo encode the tokens at the word level, we sum up the\nassociated subword embeddings. Subwords are determined\nusing byte-pair encoding [14]. The subword embeddings are\nlearnt as part of the training process.\n3.2. Pointer Network with User Memory\nWe extend the pointer networks architecture [8, 15] by im-\nplementing 1) hierarchical attention on both ASR n-best and\nuser memory, and 2) multi-task learning to both generate an\nutterance and predict whether it is ”rewritable” or not (Fig-\nure 3). We ﬁrst encode all utterances using a bidirectional\nLSTM [16]. Similar to the retrieval model, word embedding\nis the sum of each word’s subword embeddings. At each de-\ncoding step, the ﬁnal word probability distribution is calcu-\nlated as a mix of vocabulary probability distributions from\nuser memory and ASR n-best, each corresponding to the sum\nof hierarchical attention per word [8]. After calculating the ﬁ-\nnal vocabulary distribution, we pick the word with maximum\nprobability and pass it back to the decoder as the input for the\nnext decoding step. In order to predict whether to rewrite or\nnot, we use the ASR n-best and user memory contexts to pre-\ndict whether we should rewrite or not at every timestep. To\ntrain the model, we combine the cross-entropy loss of pointer\nFig. 3. Memory-based pointer network model. Blue, red and green rectangles denote word-level encoder outputs, context\nvectors, and attention weights, respectively. Frequency of each user memory is concatenated with the word embedding before\npassing through the encoder. Note that the rewritable ﬂag is either 1 or 0 for the entirety of the sequence.\nnetwork part and the binary loss for the rewritable prediction\npart. We determine the mixing ratio using hyperparameter\noptimization. We also mask the loss for the non-rewritable\nutterances for the pointer network part.\n4. RESULTS\n4.1. Evaluation Criteria\nIn order to evaluate the results, we compare the predicted\nrewrite and the actual rephrase for a given ASR n-best. In\nthe retrieval model, the highest point probability for each\nuser memory is considered as the ”rewrite probability”. In\nthe pointer network model, we have two arrays of probabil-\nities corresponding to each timestep’s word probability and\nrewritability probability. We calculate the geometric mean of\neach array, and again, take the geometric mean of the those\nmeans to represent the ”rewrite probability”. This way we\nare combining the conﬁdence in the rewrite and rewritability\nprediction, as estimated by the model.\nWe deﬁne true positive (TP) as when the rewrite matches\nthe rephrase. False positive (FP) is when rewrite is not equal\nto the rephrase in both rewritable and non-rewritable cases.\nFalse negative (FN) is deﬁned only for rewritable utterances\nthat the model doesn’t rewrite (due to low conﬁdence). True\nnegative (TN) is deﬁned when the utterance is non-rewritable\nand we don’t rewrite it.We also measure the performance of\neach approach in cases where the intent of the rephrase and\nrewrite are different.\n4.2. Experiments\n4.2.1. Training Setup\nAll models are implemented in PyTorch [17]. Sub-word em-\nbedding dimensions in all models were set to 100, and were\nrandomly initialized and updated during the training. We set\nthe size of encoder and decoder LSTM hidden layers to 100.\nSince we are concatenating frequency of occurrence with\nword embeddings, we have two different encoders for n-best\nand user memory. We used Adam optimizer with batch sizes\nof 512 and 256 for the retrieval and pointer network models,\nrespectively, with default learning rates. We use greedy de-\ncoding for the pointer network model. All models are trained\non a single GPU p3.2x large AWS instance.\n4.2.2. Results\nFor the retrieval model, we tried three different encoders: 1)\nAverage of all word embeddings, 2) passing each utterance\nthrough bi-directional LSTM, and taking the average of en-\ncoder outputs, and 3) self-attention on the bi-LSTM encoder\noutputs [18]. We also experimented with both multiplicative\nand additive attentions [19]. We found additive attention to\nbe marginally better.\nFor the pointer network model, we experimented with\ndifferent attentions (additive vs. multiplicative), and slight\nchanges to the architecture. Important factors were frequency\nof user memory, and how context was deﬁned. We found that\nif we only use the decoder output to attend to memories, the\nresults are signiﬁcantly weaker. The best results were ob-\nModel N-Best Memory AUC PR Recall\nPointer Network 5 No 0.225 N/A\nRetrieval 1 Yes 0.813 19.7\nPointer Network 1 Yes 0.821 28.4\nRetrieval 5 Yes 0.848 36.6\nPointer Network 5 Yes 0.856 42.0\nTable 1. Comparing the three models based on area under precision-recall curve (PR-AUC) and recall at precision 90%.\ntained when decoder output was combined with ASR n-best\ncontext to attend on user memories. Other alternatives such\nas attending to the memory ﬁrst, and using the context to\nattend to ASR n-best proved less effective. This can be due to\nthe fact that number of memory utterances are usually higher\nthan ASR n-best, so it is easier to get the correct context from\nASR n-best.\nAs expected, the pointer network model without memory\nperforms poorly (Table 1). The model is not even able to\nachieve precision of 90%, since it produces many false posi-\ntives. This was expected, since without memory, it is difﬁcult\nto know if the user meant ”bathroom”, or ”bedroom”, for ex-\nample. Only when we look at the user memory and we see\nthat the user only has turned on and off bedroom, then we can\nrewrite to ”turn on bedroom”. Both retrieval and pointer net-\nwork models perform signiﬁcantly better with memory, with\nthe pointer network model having an edge, especially at hav-\ning a higher recall when we ﬁx the precision to 90%. We\nalso observed that the pointer network model is 5 times less\nlikely to get the intent wrong. This shows that the pointer\nnetwork model is learning to effectively construct the right\nutterance. For example, it learns to deal with patterns like\n”turn on no turn off the fan” by taking the latter intent from\nthe utterance. Another example of how the pointer network\nmodel excels is when the user memory only contains ”turn\non bedroom” intent, for example, and what the user want is\nto ”turn off bedroom”. The retrieval model can mistakenly\nretrieve the opposite intent, but the pointer network model is\nable to ”compose” the right intent by copying ”turn off” from\nthe ASR n-best, and the device name from the memory.\nIt is also clear that adding ASR uncertainty (in the form\nof n-best) helps with the performance for both retrieval and\npointer network models. This is not a surprise, since it pro-\nvides more signals for the model to learn from. There are\neven cases (like the example in Figure 3) that what the user\nmeant is already included in the n-best (n >1).\nAlso, note that since our test data is not annotated, there\nis a minority of cases where the rephrase (i.e. second turn)\npresents a different intention compared to the ASR n-best (i.e.\nﬁrst turn). For example, we might have ”turn on tv” as the\nﬁrst turn, and per deﬁnition, it didn’t succeed, and in the sec-\nond turn, the user changes his/her mind and says ”turn on\nthe lamp”. This results in not knowing the upper-bound of\nprecision-recall for the model. However, since we are com-\nparing the methods on the same metric and on the same test\nset, this noise is cancelled out.\n5. CONCLUSION\nIn this paper, we proposed a personalized query rewriting ap-\nproach using user-speciﬁc memories to reduce frictions asso-\nciated with Alexa interactions. This is achieved by condition-\ning the rewrite on both the ASR n-best and the user’s mem-\nories. We have shown that the pointer-generator network ar-\nchitecture was able to outperform the retrieval-based baseline\nmodel. We note that this is particularly interesting especially\nsince the generative model takes on a more complex approach\nto query rewriting than its retrieval counterpart. Here, the gen-\nerative model would need to learn an implicit language model\nto reconstitute the utterance exactly as the rephrase, while the\nretrieval model focuses squarely on structural and semantic\nmatching against the user memory entries.\nBy taking on a more complex task, the pointer network\nalso learns to get the intent right, far better than the retrieval\nmodel. The pointer network also has the capability to rewrite\nonly a part of the input query. Consider an ASR output of\n”set the landry room to white”. The pointer model can learn\nto rewrite this to ”set the laundry room to white” even if the\nuser memory contains ”set the laundry room to yellow”. In\ncontrast, a retrieval model relies on having the exact rephrase\nin the user memory. While the pointer network is end-to-end\nand works with all the available memory simultaneously, the\nretrieval model only works with one memory at a time, and\nhas a separate ranking logic. However, the pointer network\nhas a higher latency compared to the retrieval model.\nThe proposed framework is easily extensible to other\nsources of knowledge and memory such as users’ current\nconversation history. This ﬂexibility allows for more com-\nplex conversation-level query rewriting. As a future work,\nwe plan on incorporating dialogue context similar to the ar-\nchitectures proposed in [20]. Finally, our proposed method\nalso takes full advantage of ASR n-best without invoking any\nadditional overhead on downstream pipelines. In the future,\nwe will explore using the ASR lattice output directly, as it\nprovides a richer and more diverse hypothesis space.\n6. REFERENCES\n[1] Anirudh Raju, Behnam Hedayatnia, Linda Liu, Ankur\nGandhe, Chandra Khatri, Angeliki Metallinou, Anu\nVenkatesh, and Ariya Rastrow, “Contextual Language\nModel Adaptation for Conversational Agents,” in Inter-\nspeech, 2018.\n[2] Ian Williams, Anjuli Kannan, Petar Aleksic, David Ry-\nbach, and Tara N. Sainath, “Contextual speech recog-\nnition in end-to-end neural network systems using beam\nsearch,” in Interspeech, 2018, pp. 2227–2231.\n[3] Golan Pundak, Tara N. Sainath, Rohit Prabhavalkar, An-\njuli Kannan, and Ding Zhao, “Deep Context: End-to-\nend Contextual Speech Recognition,” in 2018 IEEE\nSpoken Language Technology Workshop, SLT 2018 -\nProceedings, 2019, pp. 418–425.\n[4] Milad Shokouhi, Umut Ozertem, and Nick Craswell,\n“Did you say U2 or Youtube? Inferring Implicit Tran-\nscripts from V oice Search Logs,” in World Wide Web\nConference, 2016.\n[5] Prashanth Gurunath Shivakumar, Haoqi Li, Kevin\nKnight, and Panayiotis Georgiou, “Learning from past\nmistakes: Improving automatic speech recognition out-\nput via noisy-clean phrase context modeling,” APSIPA\nTransactions on Signal and Information Processing, vol.\n8, 2019.\n[6] Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo\nMendoza, “Query Recommendation Using Query Logs\nin Search Engines,” in International Conference on Ex-\ntending Database Technology, 2005, pp. 588–596.\n[7] Mostafa Dehghani, Sascha Rothe, Enrique Alfonseca,\nand Pascal Fleury, “Learning to attend, copy, and gener-\nate for session-based query suggestion,” in Proceedings\nof the 2017 ACM on Conference on Information and\nKnowledge Management. ACM, 2017, pp. 1747–1756.\n[8] Semih Yavuz, Abhinav Rastogi, Guan-lin Chao, and\nDilek Hakkani-T¨ur, “DEEPCOPY: Grounded Response\nGeneration with Hierarchical Pointer Networks,” in\nNIPS Conversational AI workshop, 2018.\n[9] Marjan Ghazvininejad, Chris Brockett, Ming-Wei\nChang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and\nMichel Galley, “A Knowledge-Grounded Neural Con-\nversation Model,” in AAAI, 2017, pp. 5110–5117.\n[10] Eric Mihail, Laksmi Krishnan, Francois Charette, and\nChristopher D. Manning, “Key-Value Retrieval Net-\nworks for Task-Oriented Dialogue,” in Annual SIGdial\nMeeting on Discourse and Dialogue, 2017.\n[11] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston, “Personaliz-\ning Dialogue Agents: I have a dog, do you have pets\ntoo?,” in ACL, 2018, pp. 2204–2213.\n[12] Pierre-Emmanuel Mazar ´e, Samuel Humeau, Martin\nRaison, and Antoine Bordes, “Training Millions of\nPersonalized Dialogue Agents,” in EMNLP, 2018, pp.\n2775–2779.\n[13] Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang,\nQingyao Ai, Hamed Zamani, Chen Wu, W Bruce Croft,\nand Xueqi Cheng, “A Deep Look into neural ranking\nmodels for information retrieval,” Information Process-\ning and Management, 2019.\n[14] Rico Sennrich, Barry Haddow, and Alexandra Birch,\n“Neural Machine Translation of Rare Words with Sub-\nword Units,” in ACL, 2016, pp. 1715–1725.\n[15] Abigail See, Peter J. Liu, and Christopher D. Man-\nning, “Get To The Point: Summarization with Pointer-\nGenerator Networks,” in ACL, 2017.\n[16] Sepp Hochreiter and J ¨urgen Schmidhuber, “Long short-\nterm memory,”Neural Comput., vol. 9, no. 8, pp. 1735–\n1780, Nov. 1997.\n[17] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin,\nAlban Desmaison, Luca Antiga, and Adam Lerer, “Au-\ntomatic differentiation in PyTorch,” in NIPS Autodiff\nWorkshop, 2017.\n[18] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Ben-\ngio, “A Structured Self-attentive Sentence Embedding,”\nin ICLR, 2017, pp. 1–15.\n[19] Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Ben-\ngio, “Neural Machine Translation by Jointly learning to\nAlign and Translate,” in ICLR, 2015.\n[20] Pushpendre Rastogi, Arpit Gupta, Tongfei Chen, and\nLambert Mathias, “Scaling multi-domain dialogue state\ntracking via query reformulation,” inProceedings of the\n2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics. 2019, Asso-\nciation for Computational Linguistics."
}