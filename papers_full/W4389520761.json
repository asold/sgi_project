{
  "title": "Not all quantifiers are equal: Probing Transformer-based language models’ understanding of generalised quantifiers",
  "url": "https://openalex.org/W4389520761",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5051705594",
      "name": "Tharindu Madusanka",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5024303085",
      "name": "Iqra Zahid",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100348668",
      "name": "Hao Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5033570514",
      "name": "Ian Pratt‐Hartmann",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5050960711",
      "name": "Riza Batista-Navarro",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2160024011",
    "https://openalex.org/W2104116194",
    "https://openalex.org/W4283793025",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W643745644",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3207580712",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W2412947200",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4237280301",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2984373328",
    "https://openalex.org/W4230346027",
    "https://openalex.org/W4386566774",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4386566726",
    "https://openalex.org/W3139496152",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W2475732641",
    "https://openalex.org/W4385573566",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2080042577",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2954301182",
    "https://openalex.org/W2921935588",
    "https://openalex.org/W2160581883",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2139461341",
    "https://openalex.org/W2036361251",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287887938",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W1840435438"
  ],
  "abstract": "How do different generalised quantifiers affect the behaviour of transformer-based language models (TLMs)? The recent popularity of TLMs and the central role generalised quantifiers have traditionally played in linguistics and logic bring this question into particular focus. The current research investigating this subject has not utilised a task defined purely in a logical sense, and thus, has not captured the underlying logical significance of generalised quantifiers. Consequently, they have not answered the aforementioned question faithfully or adequately. Therefore, we investigate how different generalised quantifiers affect TLMs by employing a textual entailment problem defined in a purely logical sense, namely, model-checking with natural language. Our approach permits the automatic construction of datasets with respect to which we can assess the ability of TLMs to learn the meanings of generalised quantifiers. Our investigation reveals that TLMs generally can comprehend the logical semantics of the most common generalised quantifiers, but that distinct quantifiers influence TLMs in varying ways.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8680–8692\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nNot all quantifiers are equal: Probing transformer-based language models’\nunderstanding of generalised quantifiers\nTharindu Madusanka and Iqra Zahid and Hao Li\nIan Pratt-Hartmann and Riza Batista-Navarro\nDepartment of Computer Science\nThe University of Manchester\nAbstract\nHow do different generalised quantifiers af-\nfect the behaviour of transformer-based lan-\nguage models (TLMs)? The recent popularity\nof TLMs and the central role generalised quan-\ntifiers have traditionally played in linguistics\nand logic bring this question into particular fo-\ncus. The current research investigating this\nsubject has not utilised a task defined purely in\na logical sense, and thus, has not captured the\nunderlying logical significance of generalised\nquantifiers. Consequently, they have not an-\nswered the aforementioned question faithfully\nor adequately. Therefore, we investigate how\ndifferent generalised quantifiers affect TLMs\nby employing a textual entailment problem de-\nfined in a purely logical sense, namely, model-\nchecking with natural language. Our approach\npermits the automatic construction of datasets\nwith respect to which we can assess the abil-\nity of TLMs to learn the meanings of gener-\nalised quantifiers. Our investigation reveals\nthat TLMs generally can comprehend the logi-\ncal semantics of the most common generalised\nquantifiers, but that distinct quantifiers influ-\nence TLMs in varying ways.\n1 Introduction\nGeneralised quantifiers have been a topic of much\ninterest for more than a century in logic and linguis-\ntics (Frege, 1882; Westerståhl, 1987; Gabbay et al.,\n1989; Mostowski, 1957). By capturing the interplay\nbetween quantity and cardinality, they provide a\nuseful lens through which to understand human lan-\nguage and cognition (Troiani et al., 2009; Szymanik\nand Zajenkowski, 2010a). Since transformer-based\nlanguage models (TLMs) strive to stimulate human-\nlike language understanding (Vaswani et al., 2017;\nDevlin et al., 2018; Raffel et al., 2019; Ouyang\net al., 2022; Chowdhery et al., 2022), it is essential\nto determine the extent to which they can com-\nprehend generalised quantifiers. Assessing the\ndepth of understanding that TLMs possess for any\ngiven concept is best achieved by evaluating their\nproficiency in applying it. In the case of gen-\neralised quantifiers, the most suitable evaluation\ntask is textual entailment. This is particularly rele-\nvant because altering quantifiers can fundamentally\nchange the logical inferences derived from a given\ntext, reinforcing the integral role that quantifiers\noccupy within the scope of the textual entailment\ntask.\nWhen discussing entailment, it is vital to ac-\nknowledge two distinct strands of research in the\nliterature. The first strand incorporates background\nknowledge and common sense into entailment, im-\nbuing it with a probabilistic character (Bowman\net al., 2015; Williams et al., 2018; Wang et al.,\n2019). The second strand examines textual en-\ntailment in a purely logical sense, eliminating the\ninfluence of background knowledge and common\nsense (Richardson and Sabharwal, 2021; Schlegel\net al., 2022; Madusanka et al., 2023). While the\nfirst form of entailment proves beneficial for a mul-\ntitude of practical applications, it is not ideal in\nan investigation centred on the impact of linguistic\nproperties with logical significance, such as gen-\neralised quantifiers and negation. The empirical\nevaluation of linguistic constructs under this kind\nof entailment gets compromised due to its intri-\ncate association with other concepts. Consequently,\nit is challenging to differentiate the performance\nvariation due to linguistic properties from those\nattributable to concepts like common sense and\nbackground knowledge. However, prior literature\nhas only investigated generalised quantifiers in the\ncontext of entailment that incorporates background\nknowledge and common sense (Cui et al., 2022;\nApidianaki and Garí Soler, 2021) and naturally\nsuffers from the same predicament. The second\nstrand of textual entailment by defining entailment\nin a purely logical sense circumvents the afore-\nmentioned shortcoming. Consequently, it offers a\nconducive environment for conducting evaluations\n8680\nFigure 1: An instance of the model-checking with natural language problem, the sentence “At least 3 musicians are\nguitarists” is True according to the structure since the set musicians X = {Roger,Solomon,Ava,Aria }are also\nguitarists and |X|≥ 3. However, the sentence “ All bee-keepers are scientists” are False as the set of bee-keepers\n{Talia,Solomon }are not scientists\ncentred around linguistic constructs.\nThe logical problem that is most suited to study\nthe influence of language constructs of logical sig-\nnificance is that of model-checking: given a for-\nmula ϕ and a structure A, determine whether ϕ\nis true in A (A |= ϕ). In the context of nat-\nural language, we are interested in a variant of\nthe model-checking problem where the structure\nand the formula are translated into natural lan-\nguage. An instance of model-checking with nat-\nural language problem is depicted in Figure 1.\nFrom a complexity-theoretic point-of-view, model-\nchecking in most formal languages is, compara-\ntively speaking, straightforward. Indeed the model-\nchecking problem with a fixed number of free vari-\nables and a finite structure is in PTIME . This is\nin contrast to other logical problems, such as satis-\nfiability, whose problems for various fragments\nof logic can belong to different computational\ncomplexity classes (Pratt-Hartmann, 2004; Pratt-\nHartmann and Third, 2006). Yet, solving instances\nof the model-checking problem with natural lan-\nguage requires a comprehensive understanding of\nthe logical semantics of the expressions involved.\nThus, it provides an ideal test environment to faith-\nfully evaluate the extent to which generalised quan-\ntifiers affect transformer-based language models.\nIn this study, we embark on an in-depth inves-\ntigation into TLMs’ understanding of generalised\nquantifiers utilising the model checking problem,\njuxtaposing this with cognitive science research\non quantifier verification tasks (Szymanik and Za-\njenkowski, 2010a,b; McMillan et al., 2005). A\ncritical part of our exploration involves the evalua-\ntion of pre-trained models prior to any fine-tuning.\nThus, allowing us to discern whether any differ-\nences identified are intrinsic to the models them-\nselves or introduced through the process of fine-\ntuning. Additionally, we consider the complexi-\nties arising from the integration of Boolean con-\njunctions and negation with generalised quantifiers.\nThis aspect of our study sheds light on the intri-\ncate dynamics that exist between these linguistic\nelements and the challenges they pose to TLMs.\nThis comprehensive analysis paves the way for a\nmore nuanced understanding of how TLMs handle\nintricate linguistic constructs such as generalised\nquantifiers.\nThe key contributions of the present research\ncan be summarised as follows: (1) To the best\nof our knowledge, this study represents the first\nexploration into the effects of generalised quanti-\nfiers within a logical entailment context; (2) We\nanalyse the effect on TLMs when quantifiers are\npaired with diverse logical constructs like nega-\ntion and Boolean conjunctions; (3) We compare\nand contrast the behaviour of TLMs with quanti-\nfiers with that of quantifier verification experiments\ndone with human beings; and (4) We delve into how\nwell TLMs comprehend generalised quantifiers in\na zero-shot context employing prompt engineering\napproaches such as chain-of-thought-prompting\n(Wei et al., 2022b) and provide comparisons be-\ntween pre-trained and fine-tuned models.\n2 Related Work\nOur work follows the literature on probing how dif-\nferent linguistic properties affect the behaviour of\nneural approaches such as transformer-based lan-\nguage models (Madusanka et al., 2023; Clark et al.,\n2021; Buijtelaar and Pezzelle, 2023; Jawahar et al.,\n2019; Ettinger, 2020). Specifically, our investiga-\ntion is closely related to the literature whose lin-\nguistic properties of interest are generalised quanti-\nfiers (Cui et al., 2022; Apidianaki and Garí Soler,\n2021). Our exploration differentiates from prior\nresearch in two key ways. First, we explore gener-\nalised quantifiers employing a task that is defined\npurely in a logical sense. Thus, we provide a more\nfaithful investigation into how TLMs comprehend\n8681\nGQ Logical Denotation\nAll {(X,Y ) |X ⊆Y ⊆A}\nSome {(X,Y ) |X∩Y ̸= ∅andX,Y ⊆A}\nAt least K {(X,Y ) ||X∩Y|≥ KandX,Y ⊆A}\nAt most K {(X,Y ) ||X∩Y|≤ KandX,Y ⊆A}\nLess than K {(X,Y ) ||X∩Y|<KandX,Y ⊆A}\nMore than K {(X,Y ) ||X∩Y|>KandX,Y ⊆A}\nK {(X,Y ) ||X∩Y|= KandX,Y ⊆A}\nMost {(X,Y ) ||X∩Y|> 1\n2 |X−Y|andX,Y ⊆A}\nFew {(X,Y ) ||X∩Y|< 1\n2 |X−Y|andX,Y ⊆A}\nTable 1: The generalised quantifiers (GQ) we used in our experimental setup, along with their logical denotation\ndefined on some structure A.\ngeneralised quantifiers. Second, our research also\nintegrates a comprehensive analysis of how the in-\nteraction of negations and Boolean conjunctions\nwith quantifiers influences TLMs’ performance in\na simple entailment task. We follow the logical\ndenotations introduced in logical studies to for-\nmalised generalised quantifiers when formulating\nour task (Westerståhl, 1987; Mostowski, 1957; Gab-\nbay et al., 1989; Fuhrken, 1970; Peters and West-\nerståhl, 2006) and draw parallels with cognitive\nscience work on quantifier verification in our exper-\nimental setup (Szymanik and Zajenkowski, 2010b;\nMcMillan et al., 2005; Szymanik et al., 2016).\nOur evaluation scheme for evaluating TLMs in\na zero-shot setting builds upon prior literature on\nprompt engineering (Brown et al., 2020; Kojima\net al., 2022; Wei et al., 2022b). However, ours is\nthe first literature evaluating TLMs on the model-\nchecking problem in zero-shot settings.\n3 Methodology\n3.1 Language Fragments and Generalised\nQuantifiers\nWe define a language fragment to be a set of sen-\ntence forms equipped with semantics translating\nthose sentences to some formal system such as first-\norder logic (Pratt-Hartmann, 2004) and perhaps\nthe simplest way to define a language fragment is\nvia a finite set of sentence templates. A sentence\ntemplate is a sentence in which certain open-class\nwords have been replaced by schematic variables.\nFor example, “All As are Bs” is a sentence tem-\nplate where A and B substitute ordinary nouns\n(e.g., artist, musician beekeeper, ...), and by substi-\ntuting Aand Bwith such nouns, we can formulate\nsentences such as “All musicians are artists”. Due\nto the formal structure that exists in language frag-\nments, a set of sentence templates is a natural way\nof representing them. For example the Aristotelian\nsyllogism (Smith et al., 1989) can be defined using\nthe following set of templates,\nAll As are Bs Some As are As\nNo Ais a B Some Bare not Bs\nIn this work of literature, we employ a slightly\nextended version of the Aristotelian syllogistic to\nallow negations at the subject, (e.g., Some non-\nmusicians are beekeepers) and generalised quanti-\nfiers when generating sentences.\nGeneralised quantifiers define the semantics of\nsentences that include them in terms of relations\nbetween subsets of the structure (Szymanik et al.,\n2013). Consider for example “All musicians are\nartists”. The determiner phrase “All” in this sen-\ntence specifies a relation between the set of musi-\ncians and the set of artists, namely that the former\nis a subset of the latter. More generally, “All” in a\nstructure A expresses the binary quantifier:\n{(X,Y ) |X ⊆Y ⊆A}\nThis idea can be generalised to accommodate\nother quantifiers. Consider the sentence “At least\nKmusicians are artists” where K ∈N. The phrase\n“At least K” likewise expresses a relation between\nthe set of musicians and the set of artists, namely\nthe cardinality of their intersection is at least K,\nthat is, “At least K”in A expresses the binary quan-\ntifier:\n{(X,Y ) ||X∩Y|≥ KandX,Y ⊆A}\nIn our scholarly inquiry, we examine logical\nquantifiers such as “All”, numerical quantifiers\nsuch as “At least K” and propositional quantifiers\nsuch as “Most”. The quantifiers employed, and\n8682\ntheir logical denotation on structure A are depicted\nin Table 1. We utilise these generalised quantifiers\nwhen defining language fragments for sentence gen-\neration. Let TQ be the sentence template which\ndefines the language fragment corresponding to the\nquantifier Q. For example, consider the quanti-\nfier “All”, the corresponding template TAll takes\nthe form of “All (non-)As are (not) Bs” where A\nand Bare replaced by ordinary nouns. Appendix\nA depicts the sentence templates used to define\nlanguage fragments for each of the quantifiers.\n3.2 Data Construction\nAlgorithm 1 Data Construction - Model checking\nwith Generalised Quantifiers\nInput : The Quantifier Q and corresponding\nsentence template TQ, a natural language template\nMto convert the structure to natural language, the\nvocabulary of proper nouns Dand ordinary nouns\nP, minimum and maximum number of domain\nelements dmin and dmax, minimum and maximum\nnumber of predicates pmin and pmax\nOutput : model checking dataset D\n1: D←{}\n2: repeat\n3: D,P ←sample from vocabularies D and\nP such that dmin ≤|D|≤ dmax, pmin ≤\n|P|≤ pmax\n4: A,B ←sample two predicates from P\n5: A,B ←negate A,B with pneg\n6: s ←substitute predicates A and B for\nschematic variables in the template TQ\n7: ℓ←sample from {True,False }\n8: repeat\n9: A ←generate structure randomly using\n(D,P )\n10: ℓ←MODEL CHECKER (A,(Q,A,B))\n11: until ℓ= ℓ‘\n12: M ←translates A to natural language using\nthe template M\n13: D←D∪{ M,s,ℓ }\n14: until stop condition is met\nWe develop a data construction algorithm (Algo-\nrithm 1) to construct a balanced dataset free from\neasily exploitable trivial linguistic patterns. The al-\ngorithm constructs a set of triplets (M,s,ℓ ), where\nM is the natural language translation of the struc-\nture, sis a sentence of the relevant fragmentTQ and\nℓis a label (True/False) specifying whether sis\ntrue in M. To construct (M,s,ℓ ), apart from TQ,\nthe algorithm takes the vocabularies D, P also as\ninputs. The vocabulary Dcomprises proper nouns\nemployed to characterise domain elements, while\nthe vocabulary P comprises ordinary nouns that\ncharacterise predicates. We draw a random sample\nof elements Dand P, from vocabularies Dand P\nto construct the structure. Two random nouns are\nsampled from P, each is then negated with proba-\nbility pneg, and these are finally substituted for the\ntwo schematic variables in the template TQ to form\nthe sentence s.\nGiven (probably negated) words A, B, a gener-\nalised quantifier Qand a structure A, the model-\nchecker determines A |= s, where s is the sen-\ntence formed by substituting A, B for schematic\nvariables in the templates TQ. This involves\nfirst determining the extensions of A and B\nin A and then applying the meaning of gener-\nalised quantifier Q to these sets. Consider the\nexample put forth in the section 1, (Q,A,B)\ncorresponding to the sentence “All bee-keepers\nare scientists” is (All,beekeepers,scientist ).\nthe model-checker determines the extensions of\nbeekeepers and scientistsin the corresponding\nstructure A to be {Talia,Ava,Solomon }and\n{Hailee,Ava,Tony }, respectively. The quanti-\nfier Alldictates that in order for the sentence to be\nTrue, the former needs to be a subset of the lat-\nter. However, the set {Talia,Ava,Solomon }̸⊆\n{Hailee,Ava,Tony }, thus, the model-checker as-\nsigns False as the validity label ℓ.\nThis setup with relative ease can be extended\nwhen Boolean conjunctions are introduced to\nthe sentences. Consider, for instance, a sen-\ntence pair s1 and s2, formed using the predi-\ncates (A1,B1) and (A2,B2), respectively, for\nsome quantifier Q, merged using Boolean con-\njunction ⊙∈{∧ ,∨}. To adapt to this scenario,\nthe algorithm can be augmented by effecting a\nsimple modification to step 10, transforming it\ninto ℓ ←MODEL CHECKER (A,(Q,A1,B1)) ⊙\nMODEL CHECKER (A,(Q,A2,B2)).\n3.3 Prompts for Zero-shot Evaluation\nGiven that transformer-based language models un-\ndergo pre-training through a certain form of lan-\nguage modelling objective, the most common ap-\nproach to evaluate these models in the zero-shot\nsetting is by employing prompt engineering (Brown\net al., 2020). Consequently, we formulate prompts\n8683\nfollowing a template-based strategy, utilising the\nconstructed tuples (M,s,ℓ ). We adopt two distinct\ntypes of templates. The first adheres to a more\ntraditional form of prompting, which we refer to\nas standard prompting. The second type of tem-\nplate is based on the concept of chain-of-thought\nprompting (Wei et al., 2022b). Chain-of-thought\nprompting is a technique in which an example prob-\nlem instance, accompanied by an explanation of\nthe underlying thought process, is used to guide the\nmodel towards generating more precise responses.\nWe depict the exact templates in Appendix A.\n4 Experimental Setup\n4.1 Transformer-based language models\nTo explore the transformer-based language mod-\nels’ ability to comprehend different generalised\nquantifiers, we employ a set of TLMs that have a\nproven track record in textual entailment problems,\nnamely, T5, Flan-T5, DeBERTa, LLaMA and GPT.\nT5 Following the prior work on textual entail-\nment defined purely in a logical sense (Richardson\nand Sabharwal, 2021; Tafjord et al., 2021; Madu-\nsanka et al., 2023), we utilise the T5 model in our\nexperimental setup as one of the baseline models.\nThe T5 model (Raffel et al., 2019) employs a uni-\nfied text-to-text format where all inputs and out-\nputs are textual strings. We fine-tune the T5-large\nmodel with 770M parameters to perform the model-\nchecking task.\nFlan-T5 Fine-tuned Language Net (Chung et al.,\n2022), also known as Flan, is based on instruc-\ntion fine-tuning (Wei et al., 2022a) with the ob-\njective of making the transformer model gener-\nalise better to unseen tasks. The Flan-T5 model,\nconsidered to be an improvement to T5, applies\ninstruction fine-tuning on the T5-model family.\nThus, we primarily centred our experimental setup\naround the Flan-T5 model. We fine-tune the\nFlan-T5-large model with 770M parameters\nand utilise Flan-T5-base with 220M parameters,\nFlan-T5-large, Flan-T5-xl with 3B parameters\nand Flan-T5-xxl with 11B parameters in the zero-\nshot setting.\nDeBERTa-v3 Due to the recent success of the\nDeBERTa-v3 model (He et al., 2021) in solving nat-\nural language inference tasks, we utilise it as a base-\nline model. The DeBERTa architecture improves\nupon the BERT and Roberta models using a disen-\ntangled attention mechanism and enhanced mask\ndecoder. DeBerta-v3 further improves the architec-\nture by utilising an ELECTRA-style pre-training\nwith Gradient Disentangled Embedding Sharing.\nWe fine-tune the DeBERTa-v3-large model with\naround 304M parameters.\nChatGPT Due to the recent success of ChatGPT\nin solving many natural language tasks in a zero-\nshot setting (Bang et al., 2023), we employ it in a\nsimilar context. Similar to InstructGPT (Ouyang\net al., 2022), ChatGPT is trained to follow human\ninstructions but follows a slightly different data\ncollection approach.\nLLaMA Considering that Flan-T5 and ChatGPT\nare trained to follow instructions, we decided to\nuse a TLM which has not been explicitly trained to\nfollow instructions as one of our baselines. Thus,\nwe employ LLaMA-30B model in zero-shot settings.\nThe LLaMA is said to outperform GPT-3 in most\nbaselines and achieve comparable performance\nwith respect to state-of-the-art TLMs (Touvron\net al., 2023).\n4.2 Dataset and Evaluation\nTo fine-tune and evaluate TLMs, we construct\ntrain and test sets with 72K and 36K unique prob-\nlem instances with 8K and 4K data points for\neach generalised quantifier1. We arbitrarily select\n[dmin,dmax] = [8,14] and [pmin,pmax] = [5,10]\nwhen constructing problem instances. We con-\nstruct a balanced dataset, and thus, we use accu-\nracy as the main metric but chose to depict the\noverall precision, recall and f1-score to provide a\nmore detailed analysis. We deem this setup an-\nswers the question, “How do different quantifiers\naffect the behaviour of TLMs?” . As the prob-\nlem instances contain negations, our experiment\nwill also provide insight on the effect of negation\nwhen intertwined with quantifiers on TLMs’ under-\nstanding of language. We construct separate train\nand test sets with 72K and 36K problem instances\nwith sentences containing Boolean conjunctions to\nanswer the question, “How do Boolean conjunc-\ntions affect the behaviour of TLMs when coupled\nwith different quantifiers?”. By evaluating these\nfine-tuned models against problem instances with\nhigher Kvalues in the numerical quantifiers than\nthat of the train set, we ask the question “Do TLMs\nlearn to comprehend the logical semantics of gen-\neralised quantifiers?”. To answer the questions,\n1The dataset and code available at https://github.com/\niTharindu/generalised-quantifiers-model-checking\n8684\n“How do pre-trained TLMs comprehend different\nquantifiers?” and “Do they have any biases when\nperforming a simple entailment task?” , we eval-\nuate TLMs in a zero-shot setting. We found that\nthe problem instances with [dmin,dmax] = [8,14]\nand [pmin,pmax] = [5,10] are challenging for\nTLMs in zero-shot settings. Consequently, the use\nof the same test set did not yield any meaning-\nful insights. Thus, we formulate a much simple\nproblem instance with [dmin,dmax] = [3,6] and\n[pmin,pmax] = [2,4]. A more detailed descrip-\ntion of the dataset and fine-tuning is provided in\nAppendix B.\n5 Results and Discussion\nThe ability of transformer-based language mod-\nels to solve instances of the model-checking prob-\nlem is differentially influenced by various gen-\neralised quantifiers. As demonstrated in Table\n2, TLMs appear to encounter the most difficulty\nwith proportional quantifiers such as “Most” and\n“Few”. Interestingly, this empirical observation\naligns with cognitive science research, which also\nhighlights the complexities faced by humans in in-\nterpreting proportional quantifiers (Szymanik and\nZajenkowski, 2010a; McMillan et al., 2005; Troiani\net al., 2009). In addition, the performance related\nto the quantifier “K” is notably lower in compar-\nison to other numerical quantifiers. A sentence\nincorporating the quantifier “K” is probabilistically\nmore likely to be False given a random structure.\nTherefore, in a balanced dataset, the determination\nof the truth value of a sentence containing the quan-\ntifier “K” necessitates a more detailed examination\ncompared to other numerical quantifiers consid-\nered in this study. However, as illustrated in Figure\n2, given an adequate number of training steps, all\nTLMs attain satisfactory performance levels across\nall quantifiers. Moreover, the newer TLM models,\nsuch as DeBERTa-v3 and Flan-T5, exhibit a faster\nconvergence rate compared to T5.\nAs expressed by their precision and recall values,\nTLMs often predict True for quantifiers such as\n“All” and “Less than K”, but often predict False\nwith respect to quantifiers like “Some” and “More\nthan K”. We attribute this to be an overcorrection in-\ntroduced during fine-tuning. Consider the sentence\nwith the quantifier “Less than K”: “Less than K\nartists are engineers”. This sentence is more likely\nto be False in the context of the real world for K\nvalues we consider in this study (8 ≤K ≤14)\nGQ ac pr re f1\nAll 91.4 88.6 95.3 91.8\nSome 92.2 96.8 88.0 92.2\nAt least K 94.2 97.7 90.4 93.9\nAt most K 96.6 96.5 96.6 96.5\nLess than K 95.1 93.4 97.2 95.3\nMore than K 94.9 96.9 93.0 94.9\nK 90.8 86.7 96.2 91.2\nMost 90.4 92.9 87.3 90.0\nFew 91.1 93.1 89.2 91.1\nTable 2: The test scores for the Flan-T5-large model\nacross various generalised quantifiers. The abbrevia-\ntions ac, pr, re and f1 denote accuracy, precision, recall\nand F1 score values.\nsince there are more than 14 artists who are en-\ngineers in the world. This proposition remains\ntrue even when negations are introduced to the sen-\ntences. Thus, we speculate that TLMs overcorrect\nduring fine-tuning and predict True or False ac-\ncordingly.\nTLMs show evidence of learning to under-\nstand the logical semantics of generalised quan-\ntifiers. As illustrated in Figure 3, when tested with\na dataset containing higher K values than that of\nthe train set, the accuracy of TLMs only decreases\nslightly for all numerical quantifiers. Therefore,\nwe posit that TLMs possess the capacity to learn\nthe logical semantics associated with generalised\nquantifiers. Our conclusions regarding generalisa-\ntion bear resemblances to prior work conducted\non model-checking with natural language (Madu-\nsanka et al., 2023). Their research also supports the\npremise that TLMs are capable of comprehending\nthe logical semantics of natural language. Addition-\nally, we highlight the contrast between the demon-\nstrated ability of TLMs to generalise in the context\nof model-checking problems, and their apparent\nlack of such generalisation when solving satisfia-\nbility problems (Schlegel et al., 2022; Richardson\nand Sabharwal, 2021). We hypothesise that this\ndistinction is due to the different complexity levels\nassociated with these two types of problems and\nthe necessity to understand complex inference rules\nwhen solving satisfiability problems.\nThe Boolean conjunctions have a significant\neffect, while negation has much less effect on\nfine-tuned TLMs when coupled with generalised\nquantifiers. As demonstrated in Table 3, it is ap-\nparent that fine-tuned TLMs possess the capacity to\n8685\nFigure 2: The rate of convergence of (a) Flan-T5-large and (b) DeBERTa-v3-large and (c) T5-large models\nbreak down based on different quantifiers\nac pr re f1ANDORANDORANDOR ANDORAll 91.0 83.7 89.0 80.9 93.8 89.5 91.3 84.9Some 83.4 92.3 89.0 96.4 75.9 88.2 81.9 92.1At least K90.9 86.9 93.3 84.7 88.8 89.4 91.0 87.0At most K86.4 93.2 95.4 95.1 76.5 91.0 84.9 93.0K 87.5 74.2 88.9 69.7 85.7 86.3 87.3 77.1Less than K88.5 90.6 92.3 90.3 84.0 91.0 87.9 90.7More than K92.5 88.5 93.2 85.4 91.6 92.9 92.4 89.0Most 82.2 81.3 82.6 80.7 83.0 83.0 82.8 81.8Few 82.2 81.0 83.4 81.8 80.1 79.0 81.7 80.4\nTable 3: The test accuracy values for the\nFlan-T5-large model across various generalised\nquantifiers, broken down based on the Boolean conjunc-\ntion (AND, OR) in the sentence. The abbreviations ac,\npr, re and f1 denote accuracy, precision, recall and F1\nscore values.\ncomprehend negations. In contrast, as depicted in\nFigure 4 (c), prior to the fine-tuning process, nega-\ntions exert a considerable influence. Consequently,\nwe propose that fine-tuning plays a significant role\nin enhancing the ability of TLMs to understand\nnegations. The inclusion of Boolean conjunctions\nsignificantly reduces the accuracy for all quanti-\nfiers. Noticeably, quantifiers for whom TLMs tend\nto predict True often tend to have higher accuracy\nin the context of the OR operation compared to\nAND and vice-versa. In our findings, we discov-\nered that quantifiers for which TLMs frequently\npredict the label as True also display elevated re-\ncall values forORoperations and diminished recall\nfor AND operations. Conversely, quantifiers that\nTLMs often predict as False exhibit higher preci-\nsion for AND operations and lower precision for\nORoperations.\nThe number of parameters, training pro-\ncess and type of prompting can influence\nthe TLMs’ performance when solving model-\nchecking problem instances in zero-shot settings.\nThe performance for Flan-T5 models exhibits the\npower law relationship with the number of param-\neters, as illustrated in Figure 4 (a). This empirical\nfinding is consistent with prior research analysing\nlanguage models’ performance variation with fac-\nFigure 3: The accuracy value when tested against prob-\nlem instances with sentences containing higher Kval-\nues than that of the train set. The results are broken\ndown based on the numerical quantifier.\ntors such as the number of parameters, dataset\nsize and computational resources (Kaplan et al.,\n2020). However, upon breaking down the per-\nformance metrics based on the quantifiers, the re-\nsulting graph (Figure 4 (b)) is observed to be less\nuniform compared to the representation of overall\nperformance.We attribute this behaviour to the in-\nherent probabilistic aspect of the predictions formu-\nlated by TLMs since language models are trained\nto find the most probable next word given a set of\nwords. This probabilistic nature of language mod-\nels can lead to inaccurate predictions, especially in\na logical context.\nThe Flan-T5 model with fewer parameters out-\nperformed the ChatGPT and LLaMA models in\na zero-shot setting, as depicted in Table 5. This\nphenomenon is unsurprising since Flan-based mod-\nels are very effective in tasks naturally verbalised\nas instructions due to their employment of instruc-\ntion fine-tuning (Wei et al., 2022a). Upon con-\ntrasting the efficiency of ChatGPT and Flan-T5\nmodels in the context of standard and chain-of-\nthought prompting techniques, it is observed that\nthe discrepancy in accuracy metrics across these\ntwo distinct prompting methodologies is not sub-\nstantial. However, the LLaMA model generated\nboth True and False when generating the label\nwhen standard prompting is used, failing to follow\nthe instruction properly. We attribute this failure in\n8686\nFigure 4: The (a) overall test loss and (b) test loss break down based on quantifier and (c) accuracy values breakdown\nbased on the availability of negations for the Flan-T5 model with a different number of parameters in zero-shot\nsettings, the number of parameters variates from 220M to 11B.\nGQ s0n0 s0n1 s1n0 s1n1\nAll 91.6 94.7 89.0 89.6\nSome 95.3 91.8 91.9 89.7\nAt least K 94.7 94.6 93.9 94.6\nAt most K 96.2 97.0 96.5 96.6\nLess than K 96.1 95.5 95.7 95.0\nMore than K 95.6 95.2 95.4 94.9\nK 90.9 91.8 91.5 89.7\nMost 93.6 92.4 88.1 88.6\nFew 91.7 92.5 87.4 89.3\nTable 4: The test accuracy values for the\nFlan-T5-large model across various generalised\nquantifiers breakdown based on the negations in\nthe sentence. The s,n denote subject and predicate\nnominative, 1,0 denotes having or not having a\nnegation at s,n. For example s0n1 denote no negation\nat subject and negation at predicate nominative.\nthe LLaMA model to its training process, which,\nunlike the other two models, is not trained to fol-\nlow instructions. When subjected to the chain-of-\nthought prompting approach, the LLaMA model\ndisplayed more consistency, generating either a\nTrue or False label. Thus, we infer that the in-\nclusion of examples assists the LLaMA model in\ngenerating more concise outputs.\nAccuracy values for TLMs in zero-shot set-\ntings vary drastically with different quantifiers.\nAs depicted in Table 5, TLMs struggle with numer-\nical quantifiers whose cardinality of intersection\nhas an upper bound, such as “At most K” and “Less\nthan K”. This diminished performance can be at-\ntributed primarily to the TLMs’ tendency to predict\nthe label False more frequently in sentences incor-\nporating these specific quantifiers. We hypothesise\nthis phenomenon is due to two factors. First, the\nbackground knowledge already embedded in TLMs\nfrom pretraining. As indicated previously, the sen-\ntences containing the above quantifiers coupled\nwith a low Kvalue are often False in a real-world\nChatGPT Flan-T5 LLaMA\nst ch st ch ch\nAll 57.8 59.3 59.5 65.6 50.5\nSome 77.7 80.0 79.4 79.1 58.0\nAt least K 84.1 80.0 87.1 87.1 67.3\nAt most K 42.9 42.2 50.6 46.9 45.3\nLess than K 48.5 44.4 63.0 64.6 49.2\nMore than K 82.6 75.9 86.5 88.2 53.2\nK 68.1 70.1 76.6 70.5 53.4\nMost 54.6 65.4 65.7 69.1 57.9\nFew 44.4 43.2 57.6 55.9 46.4\nOverall 62.3 62.3 69.6 69.9 53.5\nTable 5: The test accuracy values for the ChatGPT,\nFlan-T5-xxl and LLaMA-30B model in zero shot set-\ntings, stdenotes standard-prompting approach while ch\ndenotes the chain-of-thought prompting approach.\nscenario. Second, the prior cognitive research on\nquantifiers has demonstrated that quantifiers with a\ndownward monotone, such as “Few” or “Less than\nK”, present more processing challenges for humans\ncompared to those with an upward monotone, such\nas “Most” and “More than K” (Geurts and van der\nSlik, 2005; Zeijlstra, 2020; Agmon et al., 2019).\nSince TLMs are trained on human-generated data,\nit is highly likely that these models have incorpo-\nrated this cognitive trait into their understanding\nof language, which, in turn, affects their responses.\nMoreover, a deeper analysis of the answers gener-\nated through chain-of-thought prompting revealed\nthat even when the predicted label is correct, the\noverall answer is often incoherent. This coherence\ndeficit in TLMs, coupled with their difficulties in\nhandling certain quantifiers, suggests that these\nmodels are yet to achieve proficiency in learning\neven the simplest inferential rules.\n6 Conclusion\nWe investigated how generalised quantifiers affect\nthe behaviour of transformer-based language mod-\n8687\nels by employing the problem of model-checking.\nWe found that different generalised quantifiers af-\nfect TLMs in varying ways when solving model-\nchecking problems in both fine-tuned and zero-shot\nsettings. Based on empirical findings on generalisa-\ntion, we posited TLMs can learn to understand the\nlogical semantics of generalised quantifiers. More-\nover, our experimental setup in the zero-shot setting\ndemonstrated that a multitude of factors, such as\nthe training process, size of the models and type\nof prompts, can affect the ability of TLMs to solve\na simple entailment task. Thus, a compelling av-\nenue for future research is to probe how varying\nfactors affect transformer-based language models\nwhen solving a more complex entailment task, like\ndetermining satisfiability.\n7 Limitations\nDue to the empirical nature of this study, it suffers\nfrom an inductive dilemma on three fronts. One,\nin the front of transformers, the second related to\ngeneralised quantifiers and the third in relation to\nprompts we explored in zero-shot settings. We\nexplored several transformer-based language mod-\nels that are in line with prior literature and probe\nhow different generalised quantifiers affect their be-\nhaviour. Nonetheless, due to the empirical nature\nof this investigation, it is plausible that some TLM\narchitectures could deviate from the behavioural\nnorms discussed in this paper when interacting with\ngeneralised quantifiers. A similar limitation applies\nto the range of generalised quantifiers examined,\nas the ones employed in our study do not represent\nthe entire spectrum of generalised quantifiers. In\nzero-shot settings, this limitation further extends to\nthe prompt templates we employed. We consider\ntwo types of prompt templates, but there is a multi-\ntude of alternative ways prompts can be formulated\nby using the (M,s,ℓ ) triplets.\nReferences\nGalit Agmon, Yonatan Loewenstein, and Yosef Grodzin-\nsky. 2019. Measuring the cognitive cost of down-\nward monotonicity by controlling for negative polar-\nity. Glossa: a journal of general linguistics, 4(1).\nMarianna Apidianaki and Aina Garí Soler. 2021. ALL\ndolphins are intelligent and SOME are friendly: Prob-\ning BERT for nouns’ semantic properties and their\nprototypicality. In Proceedings of the Fourth Black-\nboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP, pages 79–94, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nLars Buijtelaar and Sandro Pezzelle. 2023. A psycholin-\nguistic analysis of BERT’s representations of com-\npounds. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 2230–2241, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2021.\nTransformers as soft reasoners over language. In\nProceedings of the Twenty-Ninth International Joint\nConference on Artificial Intelligence, IJCAI’20.\nRuixiang Cui, Daniel Hershcovich, and Anders Søgaard.\n2022. Generalized quantifiers as a source of error in\nmultilingual NLU benchmarks. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4875–4893,\nSeattle, United States. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\n8688\nAllyson Ettinger. 2020. What BERT Is Not: Lessons\nfrom a New Suite of Psycholinguistic Diagnostics for\nLanguage Models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nGottlob Frege. 1882. Über die wissenschaftliche berech-\ntigung einer begriffsschrift.\nG. Fuhrken. 1970. Per lindström. first order predicate\nlogic with generalized quantifiers. theoria, vol. 32\n(1966), pp. 186–195. The Journal of Symbolic Logic,\n34(4):650–650.\nD Gabbay, F Guenthner, and Dag Westerståhl.\n1989. Quantifiers in formal and natural languages.\nSpringer.\nBart Geurts and Frans van der Slik. 2005. Mono-\ntonicity and Processing Load. Journal of Semantics,\n22(1):97–117.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651–3657, Florence, Italy. Association for\nComputational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems.\nTharindu Madusanka, Riza Batista-navarro, and Ian\nPratt-hartmann. 2023. Identifying the limits of trans-\nformers when performing model-checking with natu-\nral language. In Proceedings of the 17th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 3539–3550, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nCorey T McMillan, Robin Clark, Peachie Moore, Chris-\ntian Devita, and Murray Grossman. 2005. Neural\nbasis for generalized quantifier comprehension. Neu-\nropsychologia, 43(12):1729–1737.\nAndrzej Mostowski. 1957. On a generalization of quan-\ntifiers. Fundamenta mathematicae, 44(2).\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nStanley Peters and Dag Westerståhl. 2006. Quantifiers\nin language and logic. OUP Oxford.\nIan Pratt-Hartmann. 2004. Fragments of language.\nJournal of Logic, Language and Information ,\n13(2):207–223.\nIan Pratt-Hartmann and Allan Third. 2006. More frag-\nments of language. Notre Dame Journal of Formal\nLogic, 47(2):151–177.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nKyle Richardson and Ashish Sabharwal. 2021. Pushing\nthe limits of rule reasoning in transformers through\nnatural language satisfiability. In AAAI Conference\non Artificial Intelligence.\nViktor Schlegel, Kamen Pavlov, and Ian Pratt-Hartmann.\n2022. Can transformers reason in fragments of natu-\nral language? In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 11184–11199, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nRobin Smith et al. 1989. Aristotle’s Prior Analytics.\nHackett Publishing.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nJakub Szymanik, Shane Steinert-Threlkeld, Marcin Za-\njenkowski, and Thomas F Icard III. 2013. Automata\nand complexity in multiple-quantifier sentence verifi-\ncation. Logic and Interactive RAtionality Yearbook\n2012, page 133.\nJakub Szymanik and Marcin Zajenkowski. 2010a. Com-\nprehension of simple quantifiers: Empirical evalua-\ntion of a computational model. Cognitive Science,\n34(3):521–532.\nJakub Szymanik and Marcin Zajenkowski. 2010b.\nQuantifiers and working memory. In Logic, Lan-\nguage and Meaning: 17th Amsterdam Colloquium,\nAmsterdam, The Netherlands, December 16-18, 2009,\nRevised Selected Papers, pages 456–464. Springer.\nJakub Szymanik et al. 2016. Quantifiers and cognition:\nLogical and computational perspectives, volume 96.\nSpringer.\n8689\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 3621–3634, Online.\nAssociation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nVanessa Troiani, Jonathan E Peelle, Robin Clark, and\nMurray Grossman. 2009. Is it logical to count on\nquantifiers? dissociable neural networks underlying\nnumerical and logical quantifiers. Neuropsychologia,\n47(1):104–111.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022a. Finetuned language\nmodels are zero-shot learners. In International Con-\nference on Learning Representations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nDag Westerståhl. 1987. Branching generalized quanti-\nfiers and natural language. Generalized quantifiers:\nLinguistic and logical approaches, pages 269–298.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nHedde Zeijlstra. 2020. 426Negative Quantifiers. In The\nOxford Handbook of Negation . Oxford University\nPress.\nA Appendix: Templates\nA.1 Sentence Templates\nWhen constructing sentences, as mentioned in the\nmethodology section, we employ sentence tem-\nplates. Let Qbe a generalised quantifier, andTQ be\nthe sentence template for the corresponding quanti-\nfier Q. Then TQ take the general form,\nQ(non-) As are (not) Bs\nThe inclusion of “non/not” is determined by the\navailability of the negations and A,B are ordinary\nnouns. Consider the quantifier “At most K” for\nsome natural number K. Then the correspond-\ning sentence template takes the form “At most K\n(non-) As are (not) Bs”. Table 6 depicts sentence\ntemplates corresponding to quantifiers considered\nin this study.\nA.2 Prompt templates for Zero-shot settings\nWe employ the tuples (M,s,ℓ ) to delineate\nprompts for the language modelling objective,\nproviding a framework for evaluating the effec-\ntiveness of TLMs in zero-shot settings. As\nmentioned in the methodology section, we ex-\nplored two types of prompts. One, we informally\ncalled standard prompts and the other is based on\nchain-of-thought-prompting. The standard prompt-\ning is conceptualised by the following template,\nQ: Given the following scenario, M. Is the\nsentence s Trueor False according to the\nscenario?\nA:\nThe chain-of-thought prompting employs an ex-\nample problem instance with an explanation of the\nthought process, thereby facilitating a more pre-\ncise response from TLMs. If we let (Me,se,ℓe)\nrepresent this example problem instance and Eelu-\ncidate the thought process, the chain of thought\nprompting can then be defined using the template,\nQ: Given the following scenario, Me. Is the\nsentence se True or False according to the\nscenario?\nA: ℓe. E\nQ: Given the following scenario, M. Is the\nsentence s Trueor False according to the\nscenario?\nA:\n8690\nGQ Sentece Template\nAll All (non-) As are (not) Bs\nSome Some (non-) As are (not) Bs\nAt least K At least K(non-) As are (not) Bs\nAt most K At most K(non-) As are (not) Bs\nLess than K Less than K(non-) As are (not) Bs\nMore than K More than K(non-) As are (not) Bs\nK K(non-) As are (not) Bs\nMost Most (non-) As are (not) Bs\nFew Few (non-) As are (not) Bs\nTable 6: The generalised quantifiers (GQ) we used in our experimental setup, along with their sentence templates\nGQ maximum minimum mean\nAll 161 47 90.1\nSome 165 48 89.7\nAt least K 160 47 92.4\nAt most K 162 48 92.2\nLess than K 163 49 92.4\nMore than K 168 49 92.1\nK 164 46 89.9\nMost 162 45 89.8\nFew 158 47 89.6\nTable 7: minimum, maximum and mean number of\nwords (tokens) in problem instances ( M + s) when\nseperated by SPACE\nB Appendix: Dataset and Training\nDetails\nB.1 Dataset details\nWe utilised nine quantifiers when constructing sen-\ntences. We construct train and test sets with 72K\nand 36K unique data points with 8K and 4K data\npoints for each quantifier for fine-tuning and evalu-\nating TLMs. As the vocabulary, We employed the\nRichardson and Sabharwal (2021) vocabulary of\nnouns, which contains a list of professions (e.g.,\n\"artist\", \"doctor\"), that we extended by adding\nmore professions. The dataset contains an equal\nnumber of True and False problem instances\nfor each generalised quantifier. When construct-\ning the dataset for fine-tuning TLMs, we select\n[dmin,dmax] = [8,14] and [pmin,pmax] = [5,10].\nFor sentences with numerical quantifiers, we se-\nlect a K values randomly from the range [1,|D|],\nwhere |D|denotes the number of domain elements\nselected when formulating the problem instance.\nThe minimum, maximum and mean number of to-\nkens for problem instances of each quantifier is\ndepicted in Table 7. To evaluate TLMs’ behaviour\nwith boolean conjunctions, we also constructed\nseparate train and test sets with 72K and 36K data\npoints. The dataset contains an equal number of\nproblem instances for each conjunction, and quan-\ntifier pair. Moreover, since the intention was to\ncompare the effects of generalised quantifiers on\ntransformer-based language models, so we decided\nto use the simplest form of language templates, i.e.\nsyllogistic.\nWe also emphasise the rationality behind the iter-\native approach we used in constructing the data. An\nalternative way of constructing problem instances\nis to derive the label ℓ using the model-checker\ninstead of iteratively creating structures to match\na pre-defined label and a sentence. However, this\nalternative approach can induce easily exploitable\npatterns. Consider the quantifier “K”, “All” and\n“Some”. For a random structure, quantifiers “K”\nand “All” are more likely False, while the quanti-\nfier “Some” is probabilistically True.\nB.2 Fine-tuning Details\nFormally, we define the task as a binary classifica-\ntion problem where the objective of the transformer-\nbased language model is to predict the label ℓ\n(True or False) given the natural language in-\nterpretation of the structure M and the sentence\ns as the inputs. We select and fine-tune three\nTLMs, namely T5, Flan-T5, and DeBERTa-v3,\nall of which have previously demonstrated their\nefficiency and reliability in resolving textual en-\ntailment tasks. According to prior literature, the\nperformance of TLMs mostly depends on the pre-\ntrained data, and size of the models rather than\nthe architectural choice (Raffel et al., 2019; Ka-\nplan et al., 2020). Moreover, the accuracy values\nyielded for all TLMs are similar. Thus, we expect\n8691\na similar behaviour for other TLM architectures as\nwell. Since the TLMs achieve satisfactory accuracy\nand since the central research interest is to analyse\nthe behaviour of TLMs rather than identifying the\nbest-performing TLM, we do not perform any hy-\nperparameter tuning. Moreover, exploring several\ndifferent TLMs and performing hyperparameter\ntuning leaves a higher carbon footprint (Strubell\net al., 2019). Due to the nature of the research ques-\ntion, we consider such an exploration unnecessary.\nLoss function and optimizer We fine-tune each\nTLM to predict the label ℓ given the (M,s) by\nreducing the binary cross entropy loss over the\ntarget using the ADAM (Kingma and Ba, 2015)\noptimizer.\nBatch size Utilising gradient checkpointing for\nmemory-efficient fine-tuning, we set the batch size\nto 36.\nNumber of epochs We fine-tune each TLM for\n4 epochs, resulting in 8000 steps.\nmaximum token length We set the maximum\ntoken length to 512 since the maximum problem\nlength is much lower than that, thus, we do not\ntruncate the inputs.\nlearning rate We set the learninig rate of 1 ×\n10−5\nWe utilise Huggingface (Wolf et al., 2019) im-\nplementation when experimenting with the TLM\nmodels we consider in this study.\n8692",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6757562160491943
    },
    {
      "name": "Logical consequence",
      "score": 0.5240216851234436
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48886555433273315
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.47053417563438416
    },
    {
      "name": "Natural language processing",
      "score": 0.45077845454216003
    },
    {
      "name": "Linguistics",
      "score": 0.3478126525878906
    },
    {
      "name": "Programming language",
      "score": 0.2992055416107178
    },
    {
      "name": "Philosophy",
      "score": 0.09519311785697937
    }
  ]
}