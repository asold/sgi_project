{
  "title": "Big Data Language Model of Contemporary Polish",
  "url": "https://openalex.org/W2757554342",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A4208803968",
      "name": "Krzysztof Wo≈Çk",
      "affiliations": [
        "Polish-Japanese Academy of Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5009169567",
      "name": "Agnieszka Wo≈Çk",
      "affiliations": [
        "Polish-Japanese Academy of Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1787426632",
      "name": "Krzysztof Marasek",
      "affiliations": [
        "Polish-Japanese Academy of Information Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6676373471",
    "https://openalex.org/W2122429665",
    "https://openalex.org/W127782651",
    "https://openalex.org/W2150417504",
    "https://openalex.org/W2145833060",
    "https://openalex.org/W7044042833",
    "https://openalex.org/W6679904954",
    "https://openalex.org/W2251994258",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2130450156",
    "https://openalex.org/W2079735306",
    "https://openalex.org/W6636811518",
    "https://openalex.org/W1742951243",
    "https://openalex.org/W2119202242",
    "https://openalex.org/W46893310",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2115081467",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2013540053",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2012557190",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2270190199",
    "https://openalex.org/W2329003026",
    "https://openalex.org/W2133473895",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2916285486",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W3204668696"
  ],
  "abstract": "Based on big data training we provide 5-gram language models of contemporary Polish which are based on the Common Crawl corpus (which is a compilation of more than 9,000,000,000 pages from across the web) and other resources.We prove that our model is better than the Google WEB1T n-gram counts and assures better quality in terms of perplexity and machine translation.The model includes lower-counting entries and also de-duplication in order to lessen boilerplate.We also provide POS tagged version of raw corpus and raw corpus itself.We also provide dictionary of contemporary Polish.By maintaining singletons, Kneser-Ney smoothing in SRILM toolkit was used in order to construct big data language models.In this research, it is detailed exactly how the corpus was obtained and pre-processed, with a prominence on issues which surface when working with information on this scale.We train the language model and finally present advances of BLEU score in MT and perplexity values, through the utilization of our model.",
  "full_text": "Big Data Language Model of Contemporary Polish \nKrzysztof Wo≈Çk  \nPolish-Japanese Academy of \nInformation Technology,  \nul. Koszykowa 86, 02-008 \nWarszawa, Poland  \nEmail: kwolk@pja.edu.pl  \nAgnieszka Wo≈Çk  \nPolish-Japanese Academy of \nInformation Technology,  \nul. Koszykowa 86, 02-008 \nWarszawa, Poland  \nEmail: awolk@pja.edu.pl  \n \nKrzysztof Marasek  \nPolish-Japanese Academy of \nInformation Technology,  \nul. Koszykowa 86, 02-008 \nWarszawa, Poland  \nEmail: kmarasek@pja.edu.pl  \n \nAbstract  - Based on big data training we provide 5-gram \nlanguage models of contemporary Polish which are based on the \nCommon Crawl corpus (which is a compilation of more than \n9,000,000,000 pages from across the web) and other resources. We \nprove that our model is better than the Google WEB1T n-gram \ncounts and assures better quality in terms of perplexity and \nmachine translation. The model includes lower-counting entries \nand also de-duplication in order to lessen boilerplate. We also \nprovide POS tagged version of raw corpus and raw corpus itself. \nWe also provide dictionary of contemporary Polish. By \nmaintaining singletons, Kneser-Ney smoothing in SRILM toolkit \nwas used in order to construct big data language models. In this \nresearch, it is detailed exactly how the corpus was obtained and \npre -processed, with a prominence on issues which surface when \nworking with information on this scale. We train the language \nmodel and finally present advances of BLEU score in MT and \nperplexity values, through the utilization of our model. \nI.  I NTRODUCTION  \nThere are a large number of language processing tasks \navailable that make web-scale corpora attractive and needed due \nin most, to the vast amount of information which exists in \ndifferent languages. Language modelling is of great \nsignificance, where web-scale models for language have \ndemonstrated their ability to enhance automated speech \nrecognition performance and machine translation quality [1, 2, \n3]. There are also other NLP tasks that depend greatly on \nlanguage modelling e.g. language quantification. [4] \nContained within, are language models trained on the \nCommon Crawl corpus and n-gram counts. Google has \ndischarged n-gram counts which have been trained on \n1,000,000,000,000 tokens of text [5]. N-grams which were \npresent on fewer than forty occasions were pruned, and wo rds \nwhich were present fewer than two hundred times were replaced  \nwith the unknown word. The counts are not suitable for judging \na language model with the Kneser-Net smoothing algorithm due \nto this pruning as the algorithm needs unpruned counts, \nalthough pruning will happen on the last model anyway. \nThere is another challenge with the Google n-gram counts \nthat are available publicly, [5] and this due to the fact that  the \ntraining information was not de-duplicated, meaning that \nboilerplate, like copyright notices have got excessively high \ncounts [6]. Despite Google sharing a version [7], in limited \ncontext [6], that has been de-duplicated, this was never \nofficially released to the public [8]. Before adding up the n-\ngrams, the training data was de-duplicated. There is a web \nservice which is provided by Microsoft [9], you can query it f or \nlanguage model probabilities. However, this is limited to \nEnglish language only, whereas our model preparation \nmethodology is compatible with more languages outside of \nEnglish. Additionally, there was an experiment conducted on \nthe re-ranking of machine translated Polish, due to the num ber \nof queries from the output, the service crashed on seve ral \noccasions, even with client-side caching. Utilization of the \nservice from Microsoft, throughout machine translation \ndecoding, would mean there is a requirement for a lower late ncy \nand there would be a greater volume of queries.  \n Summing up in our research we show how to build a \ncontemporary language model from big data amounts of texts \nfor any language supported in Common Crawl project (based on \nPolish). We compare its quality to Google WEB1T model and  \nto set of freely available Polish corpora found in the web.  We \nevaluate quality of our approach by measuring perplexity and \nshowing higher quality of machine translation systems tha t use \nour model. Lastly, we share publicly results of our work as plain \ntext data, trained 1- ,2 - ,4 - and 5-gram language model, RNN \nbased language model and dictionary sorted by most frequent \nunigrams together with dictionary cleaned from numbers, \nnames and less likely words. The data publicly available \n(https://goo.gl/hO1hTz). \nII .  P REPARATION OF THE D ATA  \nA crawl of the web which is in the available in the public \ndomain is the CommonCrawl project. It contains petabytes of \ndata collected over the last 7 years. It contains raw we b page \ndata, extracted metadata and text extractions.  \nThe data is accessible as text only files as well as ra w HTML. \nThe text only files contain all the RSS and HTML files  that the \ntags were stripped from. The text is converted to UTF-8 and the  \nHTML is in the original encoding. There is a distinct be nefit to \nbe gained when using the HTML files because the structure of \nthe document can be used to choose paragraphs, and can tell \nac tual content from boilerplate. Parsing vast amounts of  HTML \nneeds a lot of normalization step and it is non-trivial . \nProceedings of the Federated Conference on\nComputer Science and Information Systems pp. 389‚Äì395\nDOI: 10.15439/2017F432\nISSN 2300-5963 ACSIS, V ol. 11\nIEEE Catalog Number: CFP1785N-AR T c‚Éù 2017, PTI 389\nThroughout this work, the focus is on dealing with the text-only  \nfiles that were downloaded and processed on a small cluster \nlocally. The benefits of structured text are unable to can cel out \nthe additional computing power that is needed for the \nprocessing. \nThere were many problems that needed to be solved as pre-\nprocessing step. First of all, the selection of data onl y in a \nspecific language. CommonCrawl also has some mistakes wit h \nencoding when parsing to UTF-8 which resulted with spelling \nerrors. What is more, some texts are repeated many tim es e.g. \ncopyright, comment, data, etc. Many text structures were  \nungrammatical or contained strange insertions. There were a lso \nsome language specific difficulties that must have been \naddressed as well for each language separately. In addition, data \ncontained both samples of spoken texts like dialogs or wr itten \narticles and literature. The text domain also was not de fined. \nA.  Differences between Polish and English languages  \nIn general, Polish and English differ in syntax and gra mmar. \nEnglish is a positional language, which means that the sy ntactic \norder (the order of words in a sentence) plays a very im portant \nrole, particularly due to the limited inflection of words (e. g., \nlack of declension endings). Sometimes, the position of a word \nin a sentence is the only indicator of the sentence‚Äôs meaning. In \na Polish sentence, a thought can be expressed using several  \ndifferent word orderings, which is not possible in Engli sh. For \nexample, the sentence ‚Äú I bought myself a new car. ‚Äù can be \nwritten in Polish as ‚ÄúKupi≈Çem sobie nowy samocho\nÃÅ\nd. ‚Äù, or \n‚Äú Nowy samoch o\nÃÅ\nd sobie kupi≈Çem.‚Äù, or ‚ÄúSobie kupi≈Çem nowy \nsamoch o\nÃÅ\nd. ‚Äù, or ‚Äú Samoch o\nÃÅ\nd nowy sobie kupi≈Çem. ‚Äù The only \nexception is when the subject and the object are in the  same \nclause and the context is the only indication which is th e object \nand which is subject. For example, ‚ÄúMysz li≈ºe ko≈õƒá. (A m ouse \nis licking a bone.)‚Äù and  ‚ÄúKo≈õƒá li≈ºe mysz. (A bone is licking a \nmouse).‚Äù.  \nDifferences in potential sentence word order make the \ntranslation process more complex, especially when using a  \nphrase-model with no additional lexical information [10]. I n \naddition, in Polish it is not necessary to use the opera tor, \nbecause the Polish form of a verb always contains info rmation \nabout the subject of a sentence. For example, the sent ence ‚ÄúOn \njutro jedzie na wakacje.‚Äù is equivalent to the Polish ‚ÄúJutr o jedzie \nna wakacje.‚Äù and would be translated  as ‚ÄúHe is going on \nvacation tomorrow.‚Äù. [11]  \nIn the Polish language, the plural formation is not made  by \nadding the letter ‚Äús‚Äù as a suffix to a word, but rather ea ch word \nhas its own plural variant (e.g., ‚Äúpies - psy‚Äù, ‚Äúartysta - arty≈õci‚Äù, \netc.). Addition ally, prefixes before nouns like ‚Äúa‚Äù, ‚Äúan‚Äù, ‚Äúthe‚Äù, \ndo not exist in Polish (e.g., ‚Äúa cat - kot‚Äù, ‚Äúan apple - jab≈Çko‚Äù, \netc.) [10]. \nThe Polish language has only three tenses (present, past, a nd \nfuture). However, it must be noted that the only indicatio n \nwhether an action has ended is an aspect. For example, \n‚ÄúRobi≈Çem pranie.‚Äù Would be translated as ‚ÄúI have been doing  \nlaundry‚Äù, but ‚ÄúZrobi≈Çem pranie.‚Äù as ‚ÄúI have done laundry‚Äù,  or \n‚Äúp≈Çakaƒá - wyp≈Çakaƒá‚Äù as ‚Äúcry - cry out‚Äù [10].  \nThe gender of a noun in English does not have any effect on \nthe form of a verb, but it does in Polish. For examp le, ‚ÄúZrobi≈Ç \nto. ‚Äì  He has done it.‚Äù, ‚ÄúZrobi≈Ça to. ‚Äì  She has done it.‚Äù, \n‚Äúlekarz/lekarka - doctor‚Äù, ‚Äúucze≈Ñ/uczennica = student‚Äù, etc. [10]  \nBecause of this complexity, progress in the development of \nSMT systems for West-Slavic languages has been substanti ally \nslower than for other languages. On the other hand, excell ent \ntranslation systems have been developed for many popular \nlanguages. \nB.  Spoken vs written language  \nThe differences between speech and text within the conte xt \nof the literature should also be clarified. Chong [11] poi nted out \nthat writing and speech differ considerably in both functi on and \nstyle. Writing tends towards greater precision and detail, whil st \nspeech is often punctuated with repetition and includes pros ody, \nwhich writing does not possess, to further convey intent and  \ntone beyond the meaning of the words themselves. \nAccording to William Bright [12], spoken language consists  \nof two basic units: Phonemes, units of sound, (that are \nthemselves meaningless) are combined into morphemes, which \nare meaningful (e.g., the phonemes /b/, /i/, and /t/ form the  word \n‚Äúbit‚Äù). Contrary alphabetic scripts work in similar way. In a  \ndifferent type of script, the basic unit corresponds to a spoken \nsyllable. In logographic script (e.g., Chinese), each charac ter \ncorresponds to an entire morpheme, which is usually a word \n[12]. \nIt is possible to convey the same messages in either s peech or \nwriting, but spoken language typically conveys more explicit \ninformation than writing. The spoken and written forms of a \ngiven language tend to correspond to one or more levels and \nmay influence each other (e.g., ‚Äúthrough‚Äù is spoken as ‚Äúth ru‚Äù).  \nIn addition, writing can be perceived as colder, or more \nimpersonal, than speech. Spoken languages have dialects \nvarying across geographical areas and social groups. \nCommunication may be formal or casual. In literate soc ieties, \nwriting may be associated with a formal style and speec h with \na more casual style. Using speech requires simplification, a s the \naverage adult can read around 300 words per minute, but the \nsame person would be able to follow only 150-200 spoken \nwords in the same amount of time [13]. That is why speech is \nusually clearer and more constrained. \nThe punctuation and layout of written text do not have any \nspoken equivalent. But it must be noted that some forms of  \nwritten language (e.g., instant messages or emails) are cl oser to \nspoken language. On the other hand, spoken language tends to \nbe rich in repetition, incomplete sentences, correct ions, and \ninterruptions [14]. \n390 PROCEEDINGS OF THE FEDCSIS. PRAGUE, 2017\nWhen using written texts, it is not possible to receive \nimmediate feedback from the readers. Therefore, it is not \npossible to rely on context to clarify things. There is mor e need \nto explain things clearly and unambiguously than in speech, \nwhich is usually a dynamic interaction between two or more \npeople. Context, situation, and shared knowledge play a maj or \nrole in their communication. It allows us to leave infor mation \neither unsaid or indirectly implied [14]. \nC.  Main types of errors found in textual data  \nAnother problem was that the data contained many errors. \nThis data set had spelling errors that artificially increased the \ndictionary size and made the statistics unreliable. Some  of them \nwere casual errors and most of them were because of wrong t ext \nencoding conversion. We extracted randomly 10,000 segments \nof text from different (also) random parts of the Comm onCrawl \ncorpus. Then, a dictionary consisting of 92,135 unique words \nforms was created from TED 2013 (iwslt.org) data. The \nintersection of those two dictionaries resulted in infor mation \nthat that about 12% of the whole test set were spelling error s. \nWhat was found to be more problematic was that there were \nsentences with odd nesting, such as: \nPart A, Part A, Part B, Part B., e.g.: \n‚ÄúAle bƒôdƒô stara≈Ç siƒô udowodniƒá, ≈ºe mimo z≈Ço≈ºono≈õci, Ale \nbƒôdƒô stara≈Ç siƒô udowodniƒá, ≈ºe mimo z≈Ço≈ºono≈õci, istniejƒÖ pewne  \nrzeczy pomagajƒÖce w zrozumieniu. IstniejƒÖ pewne rzeczy \npomagajƒÖce w zrozumieniu.‚Äù  \nSome parts (words, full phrases, or even entire sentenc es) \nwere duplicated. Furthermore, there are segments containing  \nrepetitions of whole sentences inside one segment. For  instance: \nSentence A. Sentence A., e.g.: \n‚ÄúZakumulujƒÖ siƒô u tych najbardziej pijanych i skƒÖpych. \nZak umulujƒÖ siƒô u tych najbardziej pijanych i skƒÖpych.‚Äù  \nor: Part A, Part B, Part B, Part C, e.g.: \n‚ÄùMatka mo≈ºe siƒô ponownie rozmna≈ºaƒá, ale jak wysokƒÖ cenƒô \np≈Çaci, przez akumulacjƒô toksyn w swoim organizmie - przez \nakumulacjƒô toksyn w swoim organizmie - ≈õmierƒá pi erwszego \nm≈Çodego.‚Äù  \nThe analysis identified that 4% of test data contained such \nmistakes. \nIn addition, there were numerous untranslated English \nnames, words, and phrases mixed into the Polish texts. Ther e \nare also some words that originate from other languages (e. g., \nGerman and French). \nD.  Language Detection  \nThe initial stage in the data acquisition pipeline is to s eparate \nthe information by language. We looked at the option of \ndetecting the main language automatically for each page, \nhowever, we discovered the mixed language occurs frequently \nwithin one page, and is relatively common. We implemented \npython tool that worked in 3 phases. Firstly, we used Python \nLangDetect [15] library to discover entire pages that seemed to \nbe in Polish language. In the second phase, we used plWor dnet \n[16] in order to compare vocabulary of extracted articles with \nPolish vocabulary. We removed articles that contained less than \n30% of Polish words. What is more before using the plWordnet  \nthe aspell tool was used in order to correct spelling errors th at \ncould be corrected automatically. In the last step, we divid ed \ntext into sentences using automatic tool implemented within \n[17] research. When data was divided into sentences each \nsentence was checked by calculating its probability in Googl e \nWEB1T language model. We removed 20% of less likely \nsentences. This assured removal of grammatically incorrec t \nsentences or sentences in different languages while maint aining \ndata that included additional Polish data not calculated in \nGoogle WEB1T. \nBy facilitating this technique, we were able to gather 278GB \nof clean textual data in UTF-8 encoding, that was sentence \nspited. The text contained 1,962,047,863 sentences in total. \nE.  Deduplication and normalization  \nBecause the CommonCrawl consists of web pages there are \nmany fragments which are not content, but are artefacts of auto-\npage generation, copyright notices are just one example,  it is \nessential to remove such data because it would alter wrong ly the \nstatistical model. It must also be noted that some texts  are \nrepeated over the internet many time e.g. press informati on. To \nlessen the volume of boilerplate, before further proc essing, we \ntook out any lines which were duplicated. For the purpose of \ndeduplication, we implemented a python tool. The comparison \nwas done at the level of sentences. The following Table I \ncontains details about quantity of data before and after \ndeduplication. \nT ABLE I.  \n D EDUPLICATION R ESULTS  \n Size in GB  Number of \nsentences  \nNumber of \nunique words  \nBefore  296,1  1,962,047,863  87,543,726  \nAfter  94 ,8  920,517,413  87,543,726  \n \nThe step of de-duplication takes out around 75% of the Polish \ndata. This is on par with the reductions reported by Bergsma et \nal. [18].  \nAs well as de-duplicating the information, data was restrict ed \nto printable UTF-8 characters, we replaced all email addresse s \nwith the identical address, and removed the left-over HTM L \ntags. Prior to the creation of the language models, punctuat ion \nwas normalized utilizing the script which was supplied by the \nWorkshop on Statistical Machine Translation [19], by using th e \nMoses tokenizer [20] it was tokenized, and then the Moses true \ncaser was applied. \nKRZYSZTOF WO≈ÅK ET AL.: BIG DA T A LANGUAGE MODEL OF CONTEMPORAR Y POLISH 391\nIII .  E V ALUATION  \nIn order to measure the performance of new language model \nwe used the perplexity measure. Perplexity, developed for \ninformation theory, is a performance measurement for a \nlanguage model. Specifically, it is the reciprocal of the a verage \nprobability that the LM assigns to each word in a data se t. Thus, \nwhen perplexity is minimized, the probability of the LM‚Äôs \nprediction of an unknown test set is maximized. [21, 22, 23, 24] \nTo be more precise, we chose 3 different test sets a cor pus of \nTED lectures from IWSLT\n1\n conference, European Medicines \nAgency Leaflets (EMEA)\n2\n corpus and OpenSubtitles\n3\n corpus. \nFrom all 3 corpora, we randomly selected 1,000 sentences fo r \nthe evaluation with perplexity. The details of used cor pora are \nshown in Table II: \nT ABLE II. \n T EST C ORPORA S PECIFICATION  \n Number of \nsentences  \nNumber of PL \nwords  \nNumber of EN  \nwords  \nTED  210,549  218,426  104,177  \nEMEA  1,046,764  148,230  109,361  \nOPEN 33,570,553 1,519,948 758,238 \n \nSecondly, using the same data sets, we trained 3 statisti cal \nmachine translation models using Moses SMT toolkit. The \ntranslation took place from English to Polish. Translatio n \nsystems were enriched with prepared language models and \nevaluated with BLEU metric. \nBLEU was developed on a premise like that used for speech \nrecognition, described in Papineni et al. [25] as: ‚ÄúThe closer a \nmachine translation is to a professional human transla tion, the \nbetter it is.‚Äù Hence, the BLEU metric is designed to mea sure \nhow close SMT output is to that of human reference \ntranslations. It is important to note that translations, SMT or \nhuman, may differ significantly in word usage, word order, an d \nphrase length [25]. To address these complexities, BLEU \nattempts to match phrases of variable length between SMT \noutput and the reference translations. Weighted match av erages \nar e used to determine the translation score [26]. Several \nvariations of the BLEU metric exist. The basic metric  requires \ncalculation of a brevity penalty PB as follows: \n \nùëÉ\nùêµ\n= {\n1, ùëê > ùëü\nùëí\n·à∫‡¨µ‚àí\nùëü\nùëê\n‚ÅÑ ·àª\n, ùëê ‚â§ ùëü\n \n \nwhere r is the length of the reference corpus, and candida te \n(reference) translation length is given by c [27]. The basic \nBLEU metric is then determined as shown in [26]: \n                                                                   \n1 iwslt.org \n2 opus.lingfil.uu.se \nùêµùêøùê∏ùëà = ùëÉ\nùêµ\nexp‚Å° ·à∫‚àë ùë§\nùëõ\nlog ùëù\nùëõ\n·àª\nùëÅ\nùëõ=‡¨¥\n \nwhere w\nn\n are, positive weights summing to one, and the n-\ngram precision p\nn\n is calculated using n-grams with a maximum \nlength of N. There are several other important features  of \nBLEU. Word and phrase positions in the text are not evaluat ed \nby this metric. To prevent SMT systems from artificially \ninflating their scores by overuse of words known with high \nconfidence, each candidate word is constrained by the word \ncount of the corresponding reference translation. The \ngeometric mean of individual sentence scores, by consider ing \nthe brevity penalty, is then calculated for the entire  corpus [26].  \n The baseline results of SMT systems for each corpus are  \nshown in Table III. \nT ABLE I II . \n T EST C ORPORA S PECIFICATION  \nCorpus Name Baseline system score (BLEU) \nTED  17,42  \nEMEA  36,74  \nOPEN 58,52 \n \nFor language model training we used SRILM toolkit [28]. \nThe fundamental challenge that language models handle is \nsparse data. It is possible that some possible translations  were \nnot present in the training data but occur in real life. Ther e are \nsome methods in SRILM, such as add-one smoothing, deleted \nestimation, and Good-Turing smoothing, that cope with this \nproblem [23]. \nInterpolation and back-off are other methods of solving t he \nsparse data problem in n-gram LMs. Interpolation is define d as \na combination of various n-gram models with different orde rs. \nBack-off is responsible for choosing the highest-order n -gram \nmodel for predicted words from its history. It can also res tore \nlower-order n-gram models that have shorter histories. Th ere \nare many methods that determine the back-off costs and ada pt \nn-gram models. The most popular method is known as Kneser-\nNey smoothing. It analyses the diversity of predicted wor ds and \ntakes their histories into account [20]. We used this smoothin g \nmethod and trained 5-gram language models. \nFor machine translation, we used the Experiment \nManagement System [20] from the open source Moses SMT \ntoolkit to conduct the experiments. Binarization of 5-gram \nlanguage model was accomplished in our resulting systems \nusing the KenLM Modeling Toolkit and language modelling \nitself, as mentioned, with SRILM [28] with an interpolate d \nversion of Kneser-Key discounting (interpolate ‚Äì  unk ‚Äì\nkndiscount) that was used in our baseline systems. Word and \nphrase alignment was performed using SyMGIZA++ [29] \n3 opensubtitles.org  \n392 PROCEEDINGS OF THE FEDCSIS. PRAGUE, 2017\ninstead of standard The OOV‚Äôs were handled by using \nUnsupervised Transliteration Model [30]. \nSumming up in this research we used big data CommonCrawl \nbased corpus (COMMON), Google Corpus (WEB1T) and \ncorpus gathered from available resources and crawled sources  \n(OTHER). All but WEB1T that was already trained by Google \nin 5-gram order. The details about those corpora and numbe r of \nngrams are showed in following Table IV. \nT ABLE IV. \n N UMBER OF N- GRAMS IN L ANGUAGE M ODELS  \n COMMON  WEB1T  OTHER  \n1-grams  102,742,823  9,749,397  18,953,166  \n2-grams  1,227,434,111  72,096,704  248,705,481  \n3-grams 1,208,818,561 128,491,454 350,220,758 \n4-grams 1,513,980,357 128,789,635 468,203,863 \n5-grams 1,433,864,427 113,097,133 431,451,627 \nIV .  E XPERIMENTS  \nThe new data were: \n‚Ä¢  A Polish ‚Äì  English dictionary (bilingual \nparallel) \n‚Ä¢  Additional (newer) TED Talks data sets not \nincluded in the original train data (we crawled \nbilingual data and created a corpus from it) \n(bilingual parallel) \n‚Ä¢  E-books \n‚Ä¢  Subtitles for movies and TV series  \n‚Ä¢  Parliament and senate proceedings  \n‚Ä¢  Wikipedia Comparable Corpus (bilingual \nparallel) \n‚Ä¢  Euronews Comparable Corpus (bilingual \nparallel) \n‚Ä¢  Repository of PJIIT‚Äôs diplomas  \n‚Ä¢  Many PL monolingual data web crawled \nfrom main web portals like blogs, chip.pl, \nFocus news archive, interia.pl, wp.pl, onet.pl, \nmoney.pl, Usenet, Termedia, Wordpress web \npages, Wprost news archive, Wyborcza news \narchive, Newsweek news archive, etc. \n‚ÄúOther‚Äù in the table below stands for many very small mode ls \nmerged together. EMEA are texts from the European Medicine s \nAgency, KDE4 is a localization file of that GUI, ECB stan ds for \nEuropean Central Bank corpus, OpenSubtitles [31] are movies \nand TV series subtitles, EUNEWS is a web crawl of the \neuronews.com web page and EUBOOKSHOP comes from \nbookshop.europa.eu. Lastly bilingual TEDDL is additional \nTED data. \nT ABLE V. \n C RAWLED CORPORA SPECIFICATION  \nData set  Dictionary  Sentences  \nEMEA  148,230  1,046,764 \nKDE4  131,477  185,282 \nECB 62,147 73,198 \nOpenSubtitles 2,446,006 33,570,553 \nEBOOKS 1,283,060 17,256,305 \nEUNEWS 33.591 43,534 \nNEWS COMM 85,380 1,209,608 \nEUBOOKSHOP 599,405 593,818  \nUN TEXTS 606,989 5,312,280 \nDICTIONARY 92,121 n/a \nOTHER 51,056 61,384 \nWIKIPEDIA 887,999 172,663 \nWEB \nPORTALS \n4,797,497 26,578,683 \nBLOGS 1,645,106 2,735,568 \nUSENET 1,583,413 3,768,719 \nDIPLOMAS 490.616 666,576 \nTEDDL 129,436 54,142 \n \nData perplexity was examined by experiments with the TED \nlectures, OPEN and EMEA corpora. Perplexities for the test  sets \nare shown in Table VI. The perplexity (PPL) values are with \nKneser-Ney smoothing of the data. \nT ABLE VI . \n P ERPLEXITY - BASED LANGUAGE MODEL EVALUATION  \nCORPUS  MODEL  PERPLEXITY \n(PPL)  \nTED  Common Crawl  1471 \nTED  WEB1T  1523 \nTED OTHER 1628 \nOPEN Common Crawl 480  \nOPEN WEB1T 671  \nOPEN OTHER 823  \nEMEA Common Crawl 1163 \nEMEA WEB1T 1253 \nEMEA OTHER 1417 \n \nThe following Table VII provides results of our language \nmodel evaluation using SMT systems. We trained 3 baseline \nsystems (Baseline BLEU) and then augmented them with our \nCommonCrawl-based language model (Augmented BLEU). \nThe same was done using WEB1T and OTHER language \nmodels. The translation was conducted into Polish directio n. \nThe Delta column contains difference between baseline an d \naugmented systems. It must be noted that we did not conduct \nany in-domain adaptation of language models. \n \nKRZYSZTOF WO≈ÅK ET AL.: BIG DA T A LANGUAGE MODEL OF CONTEMPORAR Y POLISH 393\nT ABLE VII. \n SMT - BASED LANGUAGE MODEL EVALUATION  \nCORPUS  LANGUAGE \nMODEL  \nBaseline \nBLEU   \nAugumented \nBLEU  \nDelta \nTED  Common \nCrawl  \n17.42 18.33 0.91 \nTED  WEB1T  17.42 17.97 0.55 \nTED OTHER 17.42 17.76 0.34 \nOPEN Common \nCrawl \n58.52 59.23 0.71 \nOPEN WEB1T 58.52 59.01 0.49 \nOPEN OTHER 58.52 58.79 0.27 \nEMEA Common \nCrawl \n36.74 38.34 1.6 \nEMEA WEB1T 36.74 37.93 1.19 \nEMEA OTHER 36.74 37.26 0.52 \n \nV .  R ESULTS AND C ONCLUSIONS  \nSumming up, we successfully released n-gram counts and \nlanguage models built using big data textual corpora which \novercomes limitations of other smaller, publicly available \nresources. In addition, we were able to show that after some  \nbasic pre-processing of the data we were able to obtain BLEU  \nand perplexity results that outperform state- of -the-art language \nmodels like WEB1T and other smaller corpora even after \nmerging them together. We proved that improvements in \nperplexity and also in machine translation lead to better \nlanguage knowledge utilisation. The results of our work are free \nand publicly available . The resources we share are the raw data \nafter pre-processing, raw data tagged with POS using Morfeusz \ntagger [32], trained 5-gram language model with pruned 20% of \nless likely n-grams, dictionary with count of most frequent \nwords in Polish based on CommonCrawl corpus and lastly a \nsimilar dictionary without counts but manually cleaned from  \nnoisy data by native Polish translators. \nVI .  A CKNOWLEDGEMENTS  \nWork financed as part of the investment in the CLARIN-PL \nresearch infrastructure funded by the Polish Ministry of Science \nand Higher Education and was backed by the PJATK legal \nresources. \nVII.  R EFERENCES  \n[1] Brants, T., Popat, A. C., Xu, P., Och, F. J., & Dean, J. (20 07). \nLarge language models in machine translation. In \nProceedings of the Joint Conference on Empirical Methods \nin Natural Language Processing and Computational Natural \nLanguage Learning. \n[2] Guthrie, D., & Hepple, M. (2010, October). Storing the we b \nin memory: Space efficient language models with constant \ntime retrieval. In Proceedings of the 2010 Conference on \nEmpirical Methods in Natural Language Processing (pp. \n262 -272). Association for Computational Linguistics. \n[3] Chelba, C., & Schalkwyk, J. (2013). Empirical exploration of \nlanguage modeling for the google. com query stream as \napplied to mobile voice search. In Mobile Speech and \nAdvanced Natural Language Solutions (pp. 197-229). \nSpringer New York, DOI: 10.1007/978-1-4614-6018-3_8 \n[4] Lenko-Szymanska, A., (2016). A corpus-based analysis of th e \ndevelopment of phraseological competence in EFL learners \nusing the CollGram profile. Paper presented at the 7 th \nConference of the Formulaic Language Research Network \n(FLaRN), Vilnius, 28- 30  June. \n[5] Brants, T., & Franz, A. (2006). Web 1T 5-gram corpus version \n1.1. Google Inc. \n[6] Lin, D., Church, K., Ji, H., Sekine, S., Yarowsky, D. , Bergsma, \nS., Patil, K., Pitler, E., Lathbury, R., Rao, V., Dalwani, K. , & \nNarsale, S. (2010). Final report of the 2009 JHU CLSP \nworkshop. \n[7] Bergsma, S., Pitler, E., & Lin, D. (2010, July). Creating  robust \nsupervised classifiers via web-scale N-gram data. In \nProceedings of the 48th Annual Meeting of the Association \nfor Computational Linguistics (pp. 865-874). Association fo r \nComputational Linguistics. \n[8] Lin, D., (2013). Personal communication, October \n[9] Wang, K., Thrasher, C., Viegas, E., Li, X., & Hsu, B. J.  P. \n(2010, June). An overview of Microsoft Web N-gram corpus \nand applications. In Proceedings of the NAACL HLT 2010 \nDemonstration Session (pp. 45-48). Association for \nComputational Linguistics. \n[10] Swan, O. E. (2003). Polish Grammar in a Nutshell. \nUniversity of Pittsburgh.  \n[11] Choong, C., & Power, M. S. The Difference between Written \nand Spoken English. Assignment Unit, 1. \n[12] Daniels, P. T., & Bright, W. (1996). The world's writing  \nsystems. Oxford University Press on Demand. \n[13] Coleman, J. (2014). A speech is not an essay. Harvard \nBusiness Review. \n[14] Ager, S. (2013). Differences between writing and speech, \nOmniglot ‚Äî the online encyclopedia of writing systems and \nlanguages. \n[15] Language detection library ported from Google's language-\ndetection. https://pypi.python.org/pypi/langdetect? \n[16] Maziarz, M., Piasecki, M., & Szpakowicz, S. (2012). \nApproaching plWordNet 2.0. In Proceedings of 6th \nInternational Global Wordnet Conference, The Global \nWordNet Association (pp. 189-196). \n[17] Wo≈Çk, K., Marasek, K. (2014) Polish ‚Äì  English Speech \nStatistical Machine Translation Systems for the IWSLT \n2014, Proceedings of the 11th International Workshop on \nSpoken Language Translation, Tahoe Lake, USA, p. 143-\n149  \n[18] Bergsma, S., Pitler, E., & Lin, D. (2010, July). Creating \nrobust supervised classifiers via web-scale N-gram data. In \nProceedings of the 48th Annual Meeting of the Association \nfor Computational Linguistics (pp. 865-874). Association fo r \nComputational Linguistics. \n[19] Bojar, O., Buck, C., Callison-Burch, C., Federmann, C., \nHaddow, B., Koehn, P., Monz, C., Post, M., Soricut, R. & \nSpecia, L. (2013). Findings of the 2013 Workshop on \nStatistical Machine Translation. In Proceedings of the Eighth \n394 PROCEEDINGS OF THE FEDCSIS. PRAGUE, 2017\nWorkshop  on  Statistical  Machine  Translation,  pages  1‚Äì44,  Sofia,\nBulgaria. Association for Computational Linguistics\n[20] Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M.,\nBertoldi, N., ... & Dyer, C. (2007, June). Moses: Open source tool kit\nfor statistical machine translation. In Proceedings of the 45th  annual\nmeeting of the ACL on interactive poster and demonstration sessi ons\n(pp. 177-180). Association for Computational Linguistics.\n[21] Chen,  S.  F.,  &  Goodman,  J.  (1996,  June).  An  empirical  study  of\nsmoothing techniques for language modeling. In Proceedings of the\n34th  annual meeting  on Association  for Computational  Linguistics\n(pp.  310-318).  Association  for  Computational  Linguistics,  DOI:\n10.3115/981863.981904\n[22] Perplexity  [Online].  Hidden  Markov  Model  Toolkit  website.\nCambridge University Engineering Dept. Available: http://www1.ics i.\nberkeley.edu/Speech/docs/HTKBook3.2/node188_mn.html,  retrieved\non November 29, 2015.\n[23] Koehn, P., (2010) Moses, statistical machine translation syste m, user\nmanual and code guide.\n[24] Jurafsky, D., [Online] Language modeling: Introduction to n-grams\n[Online].  Stanford  University.  Available:  https://web.stanford.\nedu/class/cs124/lec/languagemodeling.pdf, retrieved on November 29,\n2015.\n[25] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002, July). BLEU:\na  method  for  automatic  evaluation  of  machine  translation.  In\nProceedings  of  the  40th  annual  meeting  on  association  for\ncomputational  linguistics  (pp.  311-318).  Association  for\nComputational Linguistics.\n[26] Axelrod, A. (2006). Factored language models for statistical machine\ntranslation. DOI 10.1007/s10590-010-9082-5\n[27] Graves,  A.,  &  Schmidhuber,  J.  (2005).  Framewise  phoneme\nclassification  with  bidirectional  LSTM  and  other  neural  netw ork\narchitectures. Neural Networks, 18(5), 602-610.\n[28] Stolcke,  A.  (2002,  September).  SRILM-an  extensible  language\nmodeling toolkit. In Interspeech (Vol. 2002, p. 2002).\n[29] Junczys-Dowmunt, M., & Sza≈Ç, A. (2012). Symgiza++: symmetrized\nword alignment models for statistical machine translation. In  Security\nand Intelligent Information  Systems (pp. 379-390). Springer Berlin\nHeidelberg, DOI: 10.1007/978-3-642-25261-7_30\n[30] Durrani,  N.,  Sajjad,  H.,  Hoang,  H.,  &  Koehn,  P.  (2014,  April).\nIntegrating  an  Unsupervised  Transliteration  Model  into  Statist ical\nMachine  Translation.  In  EACL  (Vol.  14,  pp.  148-153),  DOI:\n10.3115/v1/E14-4029\n[31] Wo≈Çk,  K.,  &  Marasek,  K.  (2014).  Real-time  statistical  s peech\ntranslation.  In  New  Perspectives  in  Information  Systems  and\nTechnologies,  Volume  1  (pp.  107-113).  Springer  International\nPublishing, DOI: 10.1007/978-3-319-05951-8_11\n[32] Morfeusz  Tagger,  Available:  http://sgjp.pl/morfeusz/morfeusz .html,\nretrieved on March 23, 2017\nKRZYSZTOF WO≈ÅK ET AL.: BIG DA T A LANGUAGE MODEL OF CONTEMPORAR Y POLISH 395",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5991496443748474
    },
    {
      "name": "Big data",
      "score": 0.5946478247642517
    },
    {
      "name": "Data modeling",
      "score": 0.4456751346588135
    },
    {
      "name": "Natural language processing",
      "score": 0.4008331894874573
    },
    {
      "name": "Linguistics",
      "score": 0.3275109827518463
    },
    {
      "name": "Data science",
      "score": 0.32045963406562805
    },
    {
      "name": "Database",
      "score": 0.1778460144996643
    },
    {
      "name": "Data mining",
      "score": 0.10518360137939453
    },
    {
      "name": "Philosophy",
      "score": 0.09578284621238708
    }
  ]
}