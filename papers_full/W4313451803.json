{
  "title": "Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models",
  "url": "https://openalex.org/W4313451803",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5055949685",
      "name": "Tiffany H. Kung",
      "affiliations": [
        null,
        "Massachusetts General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5080358515",
      "name": "Morgan Cheatham",
      "affiliations": [
        "Brown University",
        "Warren Alpert Foundation"
      ]
    },
    {
      "id": "https://openalex.org/A5015894459",
      "name": "Arielle Medenilla",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5087687954",
      "name": "Czarina Sillos",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5003016349",
      "name": "Lorie De Leon",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5072923710",
      "name": "Camille Elepaño",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5069780423",
      "name": "Maria Madriaga",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5071379154",
      "name": "Rimel Aggabao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5079156966",
      "name": "Giezel Diaz-Candido",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5001189652",
      "name": "James Maningo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015132009",
      "name": "Victor Tseng",
      "affiliations": [
        null,
        "SeaWorld Entertainment"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2964345285",
    "https://openalex.org/W2974204346",
    "https://openalex.org/W3138114698",
    "https://openalex.org/W2936086693",
    "https://openalex.org/W2557738935",
    "https://openalex.org/W2901612843",
    "https://openalex.org/W3025011581",
    "https://openalex.org/W2766760598",
    "https://openalex.org/W3186799149",
    "https://openalex.org/W3041565340",
    "https://openalex.org/W4297997268",
    "https://openalex.org/W2920897817",
    "https://openalex.org/W2793981925",
    "https://openalex.org/W4280572991",
    "https://openalex.org/W2958109836",
    "https://openalex.org/W3013597571",
    "https://openalex.org/W3028484854"
  ],
  "abstract": "ABSTRACT We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.",
  "full_text": "Kung et al. Page 1 of 25 \n \nPerformance of ChatGPT on USMLE: Potential for AI-Assisted Medical 1 \nEducation Using Large Language Models 2 \n 3 \nTiffany H. Kung1,2; Morgan Cheatham3; ChatGPT4; Arielle Medenilla1; Czarina Sillos1; Lorie De Leon1; 4 \nCamille Elepaño1; Maria Madriaga1; Rimel Aggabao1, Giezel Diaz-Candido1; James Maningo1; Victor 5 \nTseng*1,5 6 \n 7 \nAuthor Affiliations:  8 \n1AnsibleHealth, Inc (Mountain View, CA) 9 \n2Department of Anesthesiology, Massachusetts General Hospital, Harvard School of Medicine 10 \n(Boston, MA) 11 \n3Warren Alpert Medical School; Brown University (Providence, RI) 12 \n4OpenAI, Inc; (San Francisco, CA) 13 \n5Department of Medical Education, UWorld, LLC (Dallas, TX) 14 \n 15 \n*Indicates corresponding author 16 \n 17 \nCorresponding Author Information:  18 \nVictor Tseng, MD 19 \nMedical Director, Pulmonology 20 \nAnsible Health, Inc 21 \n229 Polaris Avenue, Ste 10 22 \nMountain View, CA, 94043 23 \nOffice Phone: (404) 595-7948 24 \nEmail: victor@ansiblehealth.com  25 \n 26  \nRunning Title: ChatGPT and Medical Education 27 \n 28 \nSubject Codes: artificial intelligence; clinical decision support; medical education; standardized testing; 29 \nChatGPT; large language model; machine learning 30 \n 31 \nWord Count: 3786 (Main Text: 3342) 32 \n 33 \nMain Figures: 3     Tables: 0 34 \n 35 \nSupplementary Figures and Tables: 2 36 \n 37 \n 38 \n 39 \n 40 \n 41 \n 42 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nKung et al. Page 2 of 25 \n \nGLOSSARY OF NONSTANDARD ABBREVIATIONS 43 \n 44 \n 45 \nACI  Accuracy- Concordance-Insight scoring system 46 \nDOI  Density of insight 47 \nGPT  Generative pretrained transformer 48 \nLLM  Large language model 49 \nMCSA  Multiple choice single answer 50 \nMC-J  Multiple choice single answer with forced justification 51 \nMC-NJ  Multiple choice single answer without forced justification 52 \nNLP  Natural language processing 53 \nOE  Open-ended question formulation 54 \nQn.m  Question n, input run m 55 \nUSMLE United States Medical Licensing Exam 56 \n 57 \n 58 \n 59 \n 60 \n 61 \n 62 \n 63 \n 64 \n 65 \n 66 \n 67 \n 68 \n 69 \n 70 \n 71 \n 72 \n 73 \n 74 \n 75 \n 76 \n 77 \n 78 \n 79 \n 80 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 3 of 25 \n \nABSTRACT 81 \n 82 \nWe evaluated the performance of a large language model called ChatGPT on the United States Medical 83 \nLicensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT 84 \nperformed at or near the passing threshold for all three exams without any specialized training or 85 \nreinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its 86 \nexplanations. These results suggest that large language models may have the potential to assist with 87 \nmedical education, and potentially, clinical decision-making. 88 \n 89 \n 90 \n 91 \n 92 \n 93 \n 94 \n 95 \n 96 \n 97 \n 98 \n 99 \n 100 \n 101 \n 102 \n 103 \n 104 \n 105 \n 106 \n 107 \n 108 \n 109 \n 110 \n 111 \n 112 \n 113 \n 114 \n 115 \n 116 \n 117 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 4 of 25 \n \nINTRODUCTION 118 \n 119 \nOver the past decade, advances in neural networks, deep learning, and artificial intelligence (AI) have 120 \ntransformed the way we approach a wide range of tasks and industries ranging from manufacturing and 121 \nfinance to consumer products. The ability to build highly accurate classification models rapidly and 122 \nregardless of input data type (e.g. images, text, audio) has enabled widespread adoption of applications 123 \nsuch as automated tagging of objects and users in photographs1, near-human level text translation2, 124 \nautomated scanning in bank ATMs, and even the generation of image captions3. 125 \n 126 \nWhile these technologies have made significant impacts across many industries, applications in clinical 127 \ncare remain limited. The proliferation of clinical free-text fields combined with a lack of general 128 \ninteroperability between health IT systems contribute to a paucity of structured, machine-readable data 129 \nrequired for the development of deep learning algorithms. Even when algorithms applicable to clinical 130 \ncare are developed, their quality tends to be highly variable, with many failing to generalize across 131 \nsettings due to limited technical, statistical, and conceptual reproducibility4. As a result, the overwhelming 132 \nmajority of successful healthcare applications currently support back-office functions ranging from payor 133 \noperations, automated prior authorization processing, and management of supply chains and 134 \ncybersecurity threats. With rare exceptions – even in medical imaging – there are relatively few 135 \napplications of AI directly used in widespread clinical care today. 136 \n 137 \nThe proper development of clinical AI models5 requires significant time, resources, and more importantly, 138 \nhighly domain and problem-specific training data, all of which are in short supply in the world of 139 \nhealthcare. One of the key developments that enabled image-based AI in clinical imaging has been the 140 \nability of large general domain models to perform as well as, or even outperform, domain-specific 141 \nmodels. This development has catalyzed significant AI activity in medical imaging, where otherwise it 142 \nwould be challenging to obtain sufficient annotated clinical images. Indeed today, Inception-V3 serves as 143 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 5 of 25 \n \nthe basic foundation of many of the top medical imaging models currently published, ranging from 144 \nophthalmology5,6, pathology7, to dermatology8.  145 \n 146 \nIn the past three weeks, a new AI model called ChatGPT captured significant attention due to its ability to 147 \nperform a diverse array of natural language tasks9. ChatGPT is a general Large Language Model (LLM) 148 \ndeveloped recently by OpenAI. While the previous class of AI models have primarily been Deep Learning 149 \n(DL) models, which are designed to learn and recognize patterns in data, LLMs are a new type of AI 150 \nalgorithm trained to predict the likelihood of a given sequence of words based on the context of the 151 \nwords that come before it. Thus, if LLMs are trained on sufficiently large amounts of text data, they are 152 \ncapable of generating novel sequences of words never observed previously by the model, but that 153 \nrepresent plausible sequences based on natural human language. ChatGPT is powered by GPT3.5, an 154 \nLLM trained on the OpenAI 175B parameter foundation model and a large corpus of text data from the 155 \nInternet via reinforcement and supervised learning methods. Anecdotal usage indicates that ChatGPT 156 \nexhibits evidence of deductive reasoning and chain of thought, as well as long-term dependency skills. 157 \n 158 \nIn this study, we evaluate the performance of ChatGPT, a non-domain specific LLM, on its ability to 159 \nperform clinical reasoning by testing its performance on questions from the United States Medical 160 \nLicensing Examination (USMLE). The USMLE is a high-stakes, comprehensive three-step standardized 161 \ntesting program covering all topics in physicians’ fund of knowledge, spanning basic science, clinical 162 \nreasoning, medical management, and bioethics. The difficulty and complexity of questions is highly 163 \nstandardized and regulated, making it an ideal input substrate for AI testing. The examination is well-164 \nestablished, showing remarkably stable raw scores and psychometric properties over the previous ten 165 \nyears10. The Step 1 exam is typically taken by medical students who have completed two years of 166 \ndidactic and problem-based learning and focuses on basic science, pharmacology, and pathophysiology; 167 \nmedical students often spend approximately 300-400 hours of dedicated study time in preparation for this 168 \nexam11. The Step 2CK exam is usually taken by fourth-year medical students who have additionally 169 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 6 of 25 \n \ncompleted 1.5 to 2 years of clinical rotations; it emphasizes clinical reasoning, medical management, and 170 \nbioethics. The Step 3 exam is taken by physicians who generally have completed at least a 0.5 to 1 year 171 \nof postgraduate medical education. 172 \n 173 \nUSMLE questions are textually and conceptually dense; text vignettes contain multimodal clinical data 174 \n(i.e., history, physical examination, laboratory values, and study results) often used to generate 175 \nambiguous scenarios with closely-related differential diagnoses. Due to its linguistic and conceptual 176 \nrichness, we reasoned that the USMLE would serve as an excellent challenge for ChatGPT. 177 \n 178 \nOur work aims to provide both qualitative and quantitative feedback on the performance of ChatGPT and 179 \nassess its potential for use in healthcare. 180 \n 181 \n 182 \n 183 \n 184 \n 185 \n 186 \n 187 \n 188 \n 189 \n 190 \n 191 \n 192 \n 193 \n 194 \n 195 \n 196 \n 197 \n 198 \n 199 \n 200 \n 201 \n 202 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 7 of 25 \n \nMETHODS 203 \n 204 \nArtificial Intelligence: ChatGPT (OpenAI; San Francisco, CA), is a large language model that uses self-205 \nattention mechanisms and a large amount of training data to generate natural language responses to 206 \ntext input in a conversational context. It is particularly effective at handling long-range dependencies and 207 \ngenerating coherent and contextually appropriate responses. ChatGPT is a server-contained language 208 \nmodel that is unable to browse or perform internet searches. Therefore, all responses are generated in 209 \nsitu, based on the abstract relationship between words (“tokens”) in the neural network. This contrasts to 210 \nother chatbots or conversational systems that are permitted to access external sources of information 211 \n(e.g. performing online searches or accessing databases) in order to provide directed responses to user 212 \nqueries.  213 \n 214 \nInput Source: 376 publicly-available test questions were obtained from the June 2022 sample exam 215 \nrelease on the official USMLE website. Random spot checking was performed to ensure that none of the 216 \nanswers, explanations, or related content were indexed on Google prior to January 1, 2022, representing 217 \nthe last date accessible to the ChatGPT training dataset.  All sample test questions were screened, and 218 \nquestions containing visual assets such as clinical images, medical photography, and graphs were 219 \nremoved. After filtering, 305 USMLE items (Step 1: 93, Step 2CK: 99, Step 3: 113) were advanced to 220 \nencoding.  221 \n 222 \nEncoding: Questions were formatted into three variants and input into ChatGPT in the following 223 \nsequence: 224 \n1. Open-ended (OE) format: Created by removing all answer choices, adding a variable lead-in 225 \ninterrogative phrase. This format simulates free input and a natural user query pattern. 226 \n2. Multiple choice single answer without forced justification (MC-NJ): Created by reproducing the 227 \noriginal USMLE question verbatim. 228 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 8 of 25 \n \n3. Multiple choice single answer with forced justification (MC-J): Created by adding a variable lead-229 \nin imperative or interrogative phrase mandating ChatGPT to provide a rationale for each answer 230 \nchoice.  231 \n 232 \nEncoders employed deliberate variation in the lead-in prompts to avoid systematic errors that could be 233 \ncaused by stereotyped wording. To reduce memory retention bias, a new chat session was started in 234 \nChatGPT for each entry. Post-hoc analyses were performed to exclude systematic variation by encoder 235 \n(data not shown).  236 \n 237 \nAdjudication: AI outputs were independently scored for Accuracy, Concordance, and Insight by two 238 \nphysician adjudicators using the rubric provided in Supplemental Table 1. To minimize within-item 239 \nanchoring bias, adjudicators scored Accuracy for all items, followed by Concordance for all items, 240 \nfollowed by Insight for all items. To minimize interrater cross-contamination, Physician 1 adjudicated 241 \nAccuracy while Physician 2 adjudicated Concordance, and so forth. If consensus was not achieved for all 242 \nthree domains, the item was referred to a final physician adjudicator. Only 11 items (3.6% of the dataset) 243 \nrequired arbitration.  244 \n 245 \nA schematic overview of the experimental protocol is provided in Figure 1.  246 \n 247 \n 248 \n 249 \n 250 \n 251 \n 252 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 9 of 25 \n \nDATA AVAILABILITY 253 \nThe data analyzed in this study were obtained from USMLE sample questions sets which are publicly 254 \navailable. The question index, raw inputs, and raw AI outputs are available in the Online Data 255 \nSupplement. Inquiries and requests for additional dataset items and adjudication results can be 256 \nprovided upon reasonable request by contacting Victor Tseng, MD (victor@ansiblehealth.com).  257 \n 258  \n 259 \n 260 \n 261 \n 262 \n 263 \n 264 \n 265 \n 266 \n 267 \n 268 \n 269 \n 270 \n 271 \n 272 \n 273 \n 274 \n 275 \n 276 \n 277 \n 278 \n 279 \n 280 \n 281 \n 282 \n 283 \n 284 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 10 of 25 \n \nRESULTS 285 \n 286 \nChatGPT yields moderate accuracy approaching passing performance on USMLE 287 \n 288 \nExam items were first encoded as open-ended questions with variable lead-in prompts. This input format 289 \nsimulates a free natural user query pattern. With indeterminate responses censored/included, ChatGPT 290 \naccuracy for USMLE Steps 1, 2CK, and 3 was 68.0%/42.9%, 58.3%/51.4%, and 62.4%/55.7%, 291 \nrespectively (Figure 2A).  292 \n 293 \nNext, exam items were encoded as multiple choice single answer questions with no forced justification 294 \n(MC-NJ). This input is the verbatim question format presented to test-takers. With indeterminate 295 \nresponses censored/included, ChatGPT accuracy for USMLE Steps 1, 2CK, and 3 was 55.1%/36.1%, 296 \n59.1%/56.9%, and 60.9%/54.9%, respectively.  297 \n 298 \nFinally, items were encoded as multiple choice single answer questions with forced justification of 299 \npositive and negative selections (MC-J). This input format simulates insight-seeking user behavior. With 300 \nindeterminate responses censored/included, ChatGPT accuracy was 62.3%/ 40.3%, 51.9%/48.6%, and 301 \n64.6%/59.8%, respectively (Figure 2B).  302 \n 303 \n 304 \nChatGPT demonstrates high internal concordance 305 \n 306 \nConcordance was independently adjudicated by two physician reviewers by inspection of the explanation 307 \ncontent. Overall, ChatGPT outputted answers and explanations with 94.6% concordance across all 308 \nquestions. High global concordance was sustained across all exam levels, and across OE, MC-NJ, and 309 \nMC-J question input formats (Figure 3A). 310 \n 311 \nNext, we analyzed the contingency between accuracy and concordance in MC-J responses. ChatGPT 312 \nwas forced to justify its answer choice preference, and to defend its rejection of alternative choices. 313 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 11 of 25 \n \nConcordance amongst accurate responses was nearly perfect, and significantly greater than amongst 314 \ninaccurate responses (99.1% vs. 85.1%, p<0.001) (Figure 3B). 315 \n 316 \nThese data indicate that ChatGPT exhibits very high answer-explanation concordance, likely reflecting 317 \nhigh internal consistency in its probabilistic language model.  318 \n 319 \nGenerative insight offered by ChatGPT may assist the human learner 320 \n 321 \nHaving established the accuracy and concordance of ChatGPT, we next examined its potential to 322 \naugment human learning in the domain of medical education. AI-generated explanations were 323 \nindependently adjudicated by 2 physician reviewers. Explanation content was examined for significant 324 \ninsights, defined as instances that met the criteria (see Supplemental Table 1) of novelty, 325 \nnonobviousness, and validity. The perspective of the target test audience was adopted by the 326 \nadjudicator, as a second-year medical student for Step 1, fourth-year medical student for Step 2CK, and 327 \npost-graduate year 1 resident for Step 3.  328 \n 329 \nOverall, ChatGPT produced at least one significant insight in 88.9% of all responses. The prevalence of 330 \ninsight was generally consistent between exam type and question input format (Figure 3C). In Step 2CK 331 \nhowever, insight decreased by 10.3% (n = 11 items) between MC-NJ and MC-J formulations. Review of 332 \nthis subset of questions did not reveal a discernible pattern for the paradoxical decrease (see 333 \nSupplemental Table 2B).  334 \n 335 \nTo quantify the density of insight (DOI) contained within AI-generated explanations, the number of unique 336 \ninsights was normalized to the number of possible answer choices. This analysis was performed on MC-337 \nJ entries only. High quality outputs were generally characterized by DOI >0.6 (i.e. unique, novel, 338 \nnonobvious, and valid insights provided for >3 out of 5 choices); low quality outputs were generally 339 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 12 of 25 \n \ncharacterized by DOI ≤0.2. The upper limit on DOI is only bounded by the maximum length of text output. 340 \nAcross all exam types, we observed that DOI was significantly higher in questions items answered 341 \naccurately versus inaccurately (0.458 versus 0.199%, p <0.0001) (Figure 3D). 342 \n 343 \nThese data indicate that a target human learner (e.g., such as a second-year medical student preparing 344 \nfor Step 1), if answering incorrectly, is likely to gain new or remedial insight from the ChatGPT AI output. 345 \nConversely, a human learner, if answering correctly, is less likely, but still able to access additional 346 \ninsight. 347 \n 348 \n 349 \n 350 \n 351 \n 352 \n 353 \n 354 \n 355 \n 356 \n 357 \n 358 \n 359 \n 360 \n 361 \n 362 \n 363 \n 364 \n 365 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 13 of 25 \n \nDISCUSSION 366 \n 367 \nIn this study, we provide new and surprising evidence that ChatGPT is able to perform several intricate 368 \ntasks relevant to handling complex medical and clinical information. To assess ChatGPT’s capabilities 369 \nagainst biomedical and clinical questions of standardized complexity and difficulty, we tested its 370 \nperformance characteristics on the United States Medical Licensing Examination (USMLE).  371 \n 372 \nOur findings can be organized into two major themes: (1) the rising accuracy of ChatGPT, which 373 \napproaches or exceeds the passing threshold for USMLE; and (2) the potential for this AI to generate 374 \nnovel insights that can assist human learners in a medical education setting. 375 \n 376 \nThe rising accuracy of ChatGPT: The most recent iteration of the GPT LLM (GPT3) achieved 46% 377 \naccuracy with zero prompting12, which marginally improved to 50% with further model training. Previous 378 \nmodels, merely months prior, performed at 36.7%13. In this present study, ChatGPT performed at >50% 379 \naccuracy across all examinations, exceeding 60% in most analyses. The USMLE pass threshold, while 380 \nvarying by year, is approximately 60%. Therefore, ChatGPT is now comfortably within the passing range. 381 \nBeing the first experiment to reach this benchmark, we believe this is a surprising and impressive result. 382 \nMoreover, we provided no prompting or training to the AI, minimized grounding bias by expunging the AI 383 \nsession prior to inputting each question variant, and avoided chain-of-thought biasing by requesting 384 \nforced justification only as the final input. Further model interaction and prompting could often produce 385 \nmore accurate results (data not shown). Given this trajectory, it is likely that AI performance will continue 386 \nto rise as LLM models continue to mature.  387 \n 388 \nParadoxically, ChatGPT outperformed PubMedGPT (accuracy 50.8%, unpublished data), a counterpart 389 \nLLM with similar neural structure, but trained exclusively on biomedical domain literature.  We speculate 390 \nthat domain-specific training may have created greater ambivalence in the PubMedGPT model, as it 391 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 14 of 25 \n \nabsorbs real-world text from ongoing academic discourse that tends to be inconclusive, contradictory, or 392 \nhighly conservative or noncommittal in its language. A foundation LLM trained on general content, such 393 \nas ChatGPT, may therefore have an advantage because it is also exposed to broader clinical content, 394 \nsuch as patient-facing disease primers and provider-facing drug package inserts, that are more definitive 395 \nand congruent.  396 \n 397 \nConsistent with the mechanism of generative language models, we observed that the accuracy of 398 \nChatGPT was strongly mediated by concordance and insight. High accuracy outputs were characterized 399 \nby high concordance and high density of insight. Poorer accuracy was characterized by lower 400 \nconcordance and a poverty of insight. Therefore, inaccurate responses were driven primarily by missing 401 \ninformation, leading to diminished insight and indecision in the AI, rather than overcommitment to the 402 \nincorrect answer choice. These findings indicate that model performance could be significantly improved 403 \nby merging foundation models, such as ChatGPT, with a domain-specific LLM or other model trained on 404 \na voluminous and highly validated medical knowledge resources, such as UpToDate, or other ACGME-405 \naccredited content. 406 \n 407 \nInterestingly, the accuracy of ChatGPT tended to be lowest for Step 1, followed by Step 2CK, followed by 408 \nStep 3. This mirrors both the subjective difficulty and objective performance for real-world test takers on 409 \nStep 1, which is collectively regarded as the most difficult exam of the series. The low accuracy on Step 410 \n1 could be explained by an undertrained model on the input side (e.g. underrepresentation of basic 411 \nscience content on the general information space) and/or the human side (e.g. insufficient or invalid 412 \nhuman judgment at initial reinforcement stages). This result exposes a key vulnerability in pre-trained 413 \nLLMs, such as ChatGPT: AI ability becomes yoked to human ability. ChatGPT’s performance on Step 1 414 \nis poorer precisely because human users perceive its subject matter (e.g., pathophysiology) as more 415 \ndifficult or opaque.  416 \n 417 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 15 of 25 \n \nThe potential for AI-assisted human learning in medical education: We also examined the ability of 418 \nChatGPT to assist the human learning process of its target audience (e.g., a second year medical 419 \nstudent preparing for USMLE Step 1). As a proxy for the metric of helpfulness, we assessed the 420 \nconcordance and insight offered by the AI explanation outputs. ChatGPT responses were highly 421 \nconcordant, such that a human learner could easily follow the internal language, logic, and directionality 422 \nof relationships contained within the explanation text (e.g., adrenal hypercortisolism ⥬ increased bone 423 \nosteoclast activity ⥬ increased calcium resorption ⥬ decreased bone mineral density ⥬ increased fracture 424 \nrisk). High internal concordance and low self-contradiction is a proxy of sound clinical reasoning and an 425 \nimportant metric of explanation quality. It is reassuring that the directionality of relationships is preserved 426 \nby the language processing model, where each verbal object is individually lemmatized. 427 \n 428 \nAI-generated responses also offered significant insight, role-modeling a deductive reasoning process 429 \nvaluable to human learners (see Supplemental Table 2). At least one significant insight was present in 430 \napproximately 90% of outputs. ChatGPT therefore possesses the partial ability to teach medicine by 431 \nsurfacing novel and nonobvious concepts that may not be in learners’ sphere of awareness. This 432 \nqualitative gain provides a basis for future real-world studies on the efficacy of generative AI to augment 433 \nthe human medical education process. For example, longitudinal exam performance can be studied in a 434 \nquasi-controlled in AI-assisted and unassisted learners. Unit economic analysis may clarify the cost-435 \neffectiveness of incremental student performance gain in comparison to existing tools such as virtual 436 \ntutors and study aids.  437 \n 438 \nMedical education, licensing examinations, and test preparation services form a large industrial complex 439 \neclipsing a nine-figure market size annually. While its relevance remains debated, standardized testing 440 \nhas emerged as an important end-target of medical learning. In parallel, of the didactic techniques, a 441 \nsocratic teaching style is favored by medical students14. The rate-limiting step for fresh content 442 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 16 of 25 \n \ngeneration is the human cognitive effort required to craft realistic clinical vignettes that probe “high-yield” 443 \nconcepts in a subtle way, engage critical thinking, and offer pearls of knowledge even if answered 444 \nincorrectly. Demand for new examination content continues to increase. For a national medical examiner, 445 \na single item typically requires 0.1 FTE work effort to produce (NBME, personal communication). Future 446 \nstudies may investigate the ability of generative language AI to offload this human effort by assisting in 447 \nthe question-explanation writing process or, in some cases, writing entire items autonomously. 448 \n 449 \nFinally, the advent of AI in medical education demands an open science research infrastructure to 450 \nstandardize experimental methods, readouts, and benchmarks to describe and quantify human-AI 451 \ninteractions. Multiple dimensions must be covered, including user experience, learning environment, 452 \nhybridization with other teaching modes, and effect on cognitive bias. In this report, we provide an initial 453 \nbasic protocol for adjudicating AI-generated responses along axes of accuracy, concordance, and 454 \ninsight.  455 \n 456 \nOur study has several important limitations. The relatively small input size restricted the depth and range 457 \nof analyses. For example, stratifying the output of ChatGPT by subject taxonomy (e.g., pharmacology, 458 \nbioethics) or competency type (e.g., differential diagnosis, management) may be of great interest to 459 \nmedical educators, and could reveal heterogeneities in performance across language processing for 460 \ndifferent clinical reasoning tasks. Similarly, a more robust AI failure mode analysis (e.g., language 461 \nparsing error) may lend insight into the etiology of inaccuracy and discordance. In addition to being 462 \nlaborious, human adjudication is error-prone and subject to greater variability and bias. Future studies 463 \nwill undoubtedly apply unbiased approaches, using quantitative natural language processing and text 464 \nmining tools such as word network analysis. In addition to increasing validity and accelerating throughput 465 \nby several orders of magnitude, these methods are likely to better characterize the depth, coherence, 466 \nand learning value of AI output. Finally, to truly assess the utility of generative language AI for medical 467 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 17 of 25 \n \neducation, ChatGPT and related applications must be studied in both controlled and real-world learning 468 \nscenarios with students across the engagement and knowledge spectrum.  469 \n 470 \nAs AI becomes increasingly proficient, it will soon become ubiquitous, transforming clinical medicine 471 \nacross all healthcare sectors. Investigation of AI has now entered into the era of randomized controlled 472 \ntrials15. Additionally, a profusion of pragmatic and observational studies supports a versatile role of AI in 473 \nvirtually all medical disciplines and specialities by improving risk assessment16,17, data reduction, clinical 474 \ndecision support18,19,  operational efficiency, and patient communication20,21.  475 \n 476 \nInspired by the remarkable performance of ChatGPT on the USMLE, clinicians within AnsibleHealth, a 477 \nvirtual chronic pulmonary disease clinic, have begun to experiment with ChatGPT as part of their 478 \nworkflows. Inputting queries in a secure and de-identified manner, our clinicians request ChatGPT to 479 \nassist with traditionally onerous writing tasks such as composing appeal letters to payors, simplifying 480 \nradiology reports (and other jargon-dense records) to facilitate patient comprehension, and even to 481 \nbrainstorm freely in a bid to kindle insight when faced with nebulous and diagnostically challenging 482 \ncases. Overall, our clinicians reported a 33% decrease (future publication) in the time required to 483 \ncomplete documentation and indirect patient care tasks. We believe this is an early but important signal 484 \nthat LLMs such as ChatGPT are reaching a maturity level that will soon impact clinical care at large and 485 \nits ability to deliver truly individualized, compassionate, and scalable healthcare. 486 \n 487 \n 488 \n 489 \n 490 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 18 of 25 \n \nACKNOWLEDGEMENTS 491 \nThe authors thank Dr. Kristine Vanijchroenkarn, MD and Ms. Audra Doyle RRT, NP for fruitful 492 \ndiscussions and technical assistance. We also thank Mr. Vangjush Vellahu for technical assistance with 493 \ngraphical design and preparation.  494 \n  495 \nFUNDING 496 \nThe work received no external funding.  497 \n 498 \n AUTHOR CONTRIBUTIONS 499 \nTHK, MC, and VT conceived and designed the study, developed the study protocol, supervised the 500 \nresearch team, analyzed the data, and wrote the manuscript. AM, CS, LDL, CE, MM, DJC, and JM 501 \nencoded and input the data into ChatGPT. THK, VT, AM, and CS independently adjudicated the raw 502 \nChatGPT outputs. JM and VT performed data synthesis, quality control, and statistical analyses. 503 \nChatGPT contributed to the writing of several sections of this manuscript.  504 \n 505 \nConflicts of Interest: None 506 \n 507 \n 508 \n 509 \n 510 \n 511 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 19 of 25 \n \nFIGURE LEGENDS 512 \n 513 \n 514 \nFigure 1. Schematic of workflow for sourcing, encoding, and adjudicating results  515 \nAbbreviations: QC = quality control; MCSA-NJ = multiple choice single answer without forced 516 \njustification; MCSA-J = multiple choice single answer with forced justification; OE = open-ended question 517 \nformat  518 \n 519 \nFigure 2. Accuracy of ChatGPT on USMLE 520 \nFor USMLE Steps 1, 2CK, and 3, AI outputs were adjudicated to be accurate, inaccurate, or 521 \nindeterminate based on the ACI scoring system provided in Supplemental Table 1.  522 \nA: Accuracy distribution for inputs encoded as open-ended questions 523 \nB: Accuracy distribution for inputs encoded as multiple choice single answer without (MC-NJ) or with 524 \nforced justification (MC-J)  525 \n 526 \nFigure 3. Concordance and insight of ChatGPT on USMLE 527 \nFor USMLE Steps 1, 2CK, and 3, AI outputs were adjudicated on concordance and density of insight 528 \n(DOI) based on the ACI scoring system provided in Supplemental Table 1.  529 \nA: Overall concordance across all exam types and question encoding formats 530 \nB: Concordance rates stratified between accurate vs inaccurate outputs, across all exam types and 531 \nquestion encoding formats. p <0.001 for accurate vs inaccurate outputs by Fisher exact test  532 \nC: Overall insight prevalence, defined as proportion of outputs with ≥1 insight, across all exams for 533 \nquestions encoded in MC-J format   534 \nD: DOI stratified between accurate vs inaccurate outputs, across all exam types for questions encoded in 535 \nMC-J format. Horizontal line indicates the mean. p-value determined by parametric  2-way ANOVA 536 \ntesting with Benjamini-Krieger-Yekutieli (BKY) post hoc to control for false discovery rate.  537 \n 538 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 20 of 25 \n \nFIGURES AND TABLES 539 \n 540 \n 541 \n 542 \n 543 \nFigure 1 544 \n 545 \n 546 \n 547 \n 548 \n 549 \n 550 \n 551 \n 552 \n 553 \n 554 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 21 of 25 \n \n 555 \nFigure 2 556 \n 557 \n 558 \n 559 \n 560 \n 561 \n 562 \n 563 \n 564 \n 565 \n 566 \n 567 \n 568 \n 569 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 22 of 25 \n \n 570 \nFigure 3 571 \n 572 \n 573 \n 574 \n 575 \n 576 \n 577 \n 578 \n 579 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 23 of 25 \n \nAdjudication Criteria: A-C-I Scoring System \n \n \n \n \n \nAccuracy \nMC-NJ and MC-J \n● Accurate: Final answer matches the NBME key \n● Inaccurate: Incorrect answer choice is selected \n● Indeterminate: Response is not an answer choice, fails to select an answer, or claims  \n that not enough information is available to commit to an answer \nOE \n● Accurate: Response identifies the correct concept, is specific, and is clinically sound \n● Inaccurate: Response targets an unrelated concept or is not clinically sound \n● Indeterminate: Any other response, including generic advice  \n \n \n \n \nConcordance \nMC-J \n● Concordant: Explanation affirms the answer and negates all remaining choices \n● Discordant: Any part of explanation contradicts itself \nMC-NJ and OE \n● Concordant: Explanation affirms the answer \n● Discordant: Any part of explanation contradicts itself \n \n \n \n \nInsight \n \n \n \nInsight: An instance of text in the explanation that is: \n● Nondefinitional: Does not simply define a term in the input question \n● Unique: A single insight may be used to eliminate several answer choices \n● Nonobvious: Requires deduction or knowledge external to the question input  \n● Valid: In clinically or numerically accurate; preserves directionality \n \nDensity of Insight (DOI): Number of insights / (number of answer choices + 1) \n● Insightful: DOI >0 and offers a new concept or concept linkage \n● Uninsightful: DOI = 0  \n 580 \nSupplemental Table 1 581 \n 582 \n 583 \n 584 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 24 of 25 \n \nREFERENCES 585 \n1. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. & Wojna, Z. Rethinking the Inception Architecture 586 \nfor Computer Vision. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 587 \nPreprint at https://doi.org/10.1109/cvpr.2016.308 (2016). 588 \n2. Zhang, W., Feng, Y., Meng, F., You, D. & Liu, Q. Bridging the Gap between Training and Inference 589 \nfor Neural Machine Translation. Proceedings of the 57th Annual Meeting of the Association for 590 \nComputational Linguistics Preprint at https://doi.org/10.18653/v1/p19-1426 (2019). 591 \n3. Bhatia, Y., Bajpayee, A., Raghuvanshi, D. & Mittal, H. Image Captioning using Google’s Inception-592 \nresnet-v2 and Recurrent Neural Network. 2019 Twelfth International Conference on Contemporary 593 \nComputing (IC3) Preprint at https://doi.org/10.1109/ic3.2019.8844921 (2019). 594 \n4. McDermott, M. B. A. et al. Reproducibility in machine learning for health research: Still a ways to go. 595 \nSci. Transl. Med. 13, (2021). 596 \n5. Chen, P.-H. C., Liu, Y. & Peng, L. How to develop machine learning models for healthcare. Nature 597 \nMaterials vol. 18 410–414 Preprint at https://doi.org/10.1038/s41563-019-0345-0 (2019). 598 \n6. Gulshan, V. et al. Development and Validation of a Deep Learning Algorithm for Detection of 599 \nDiabetic Retinopathy in Retinal Fundus Photographs. JAMA 316, 2402–2410 (2016). 600 \n7. Nagpal, K. et al. Development and validation of a deep learning algorithm for improving Gleason 601 \nscoring of prostate cancer. npj Digital Medicine 2, 1–10 (2019). 602 \n8. Liu, Y. et al. A deep learning system for differential diagnosis of skin diseases. Nat. Med. 26, 900–603 \n908 (2020). 604 \n9. Website. https://openai.com/blog/chatgpt/. 605 \n10. Performance data. https://www.usmle.org/performance-data. 606 \n11. Burk-Rafel, J., Santen, S. A. & Purkiss, J. Study Behaviors and USMLE Step 1 Performance: 607 \nImplications of a Student Self-Directed Parallel Curriculum. Acad. Med. 92,  S67–S74 (2017). 608 \n12. Liévin, V., Hother, C. E. & Winther, O. Can large language models reason about medical questions? 609 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint \nKung et al. Page 25 of 25 \n \narXiv [cs.CL] (2022). 610 \n13. Jin, D. et al. What Disease does this Patient Have? A Large-scale Open Domain Question 611 \nAnswering Dataset from Medical Exams. arXiv [cs.CL] (2020). 612 \n14. Abou-Hanna, J. J., Owens, S. T., Kinnucan, J. A., Mian, S. I. & Kolars, J. C. Resuscitating the 613 \nSocratic Method: Student and Faculty Perspectives on Posing Probing Questions During Clinical 614 \nTeaching. Acad. Med. 96, 113–117 (2021). 615 \n15. Plana, D. et al. Randomized Clinical Trials of Machine Learning Interventions in Health Care: A 616 \nSystematic Review. JAMA Netw Open 5, e2233946 (2022). 617 \n16. Kan, H. J. et al. Exploring the use of machine learning for risk adjustment: A comparison of standard 618 \nand penalized linear regression models in predicting health care costs in older adults. PLoS One 14, 619 \ne0213258 (2019). 620 \n17. Delahanty, R. J., Kaufman, D. & Jones, S. S. Development and Evaluation of an Automated 621 \nMachine Learning Algorithm for In-Hospital Mortality Risk Adjustment Among Critical Care Patients. 622 \nCrit. Care Med. 46, e481–e488 (2018). 623 \n18. Vasey, B. et al. Reporting guideline for the early-stage clinical evaluation of decision support 624 \nsystems driven by artificial intelligence: DECIDE-AI. Nat. Med. 28, 924–933 (2022). 625 \n19. Garcia-Vidal, C., Sanjuan, G., Puerta-Alcalde, P., Moreno-García, E. & Soriano, A. Artificial 626 \nintelligence to support clinical decision-making processes. EBioMedicine 46, 27–29 (2019). 627 \n20. Bala, S., Keniston, A. & Burden, M. Patient Perception of Plain-Language Medical Notes Generated 628 \nUsing Artificial Intelligence Software: Pilot Mixed-Methods Study. JMIR Form Res 4, e16670 (2020). 629 \n21. Milne-Ives, M. et al. The Effectiveness of Artificial Intelligence Conversational Agents in Health Care: 630 \nSystematic Review. J. Med. Internet Res. 22, e20346 (2020). 631 \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 21, 2022. ; https://doi.org/10.1101/2022.12.19.22283643doi: medRxiv preprint ",
  "topic": "Concordance",
  "concepts": [
    {
      "name": "Concordance",
      "score": 0.78314208984375
    },
    {
      "name": "United States Medical Licensing Examination",
      "score": 0.6385855078697205
    },
    {
      "name": "Computer science",
      "score": 0.5572667717933655
    },
    {
      "name": "Medical education",
      "score": 0.5163782835006714
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3404301702976227
    },
    {
      "name": "Medical school",
      "score": 0.2995780408382416
    },
    {
      "name": "Medicine",
      "score": 0.26246100664138794
    },
    {
      "name": "Internal medicine",
      "score": 0.07584810256958008
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210087915",
      "name": "Massachusetts General Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I27804330",
      "name": "Brown University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801652605",
      "name": "Warren Alpert Foundation",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1300699541",
      "name": "SeaWorld Entertainment",
      "country": "US"
    }
  ]
}