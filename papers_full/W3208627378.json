{
  "title": "IIP-Transformer: Intra-Inter-Part Transformer for Skeleton-Based Action Recognition",
  "url": "https://openalex.org/W3208627378",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2420607892",
      "name": "Wang Qingtian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2354817381",
      "name": "Peng, Jianlin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286939683",
      "name": "Shi, Shuze",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A421053996",
      "name": "Liu Ting-xi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2716153820",
      "name": "He, Jiabin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745990577",
      "name": "Weng Renliang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963032654",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W3174836262",
    "https://openalex.org/W2099634219",
    "https://openalex.org/W2284126738",
    "https://openalex.org/W2997769980",
    "https://openalex.org/W3108496296",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2056898157",
    "https://openalex.org/W2593146028",
    "https://openalex.org/W2963757256",
    "https://openalex.org/W2970982695",
    "https://openalex.org/W2510185399",
    "https://openalex.org/W3106615203",
    "https://openalex.org/W2948058585",
    "https://openalex.org/W3185273257",
    "https://openalex.org/W2892065377",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W3092754310",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962730651",
    "https://openalex.org/W2745471877",
    "https://openalex.org/W3156509901",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963369114",
    "https://openalex.org/W2606294640",
    "https://openalex.org/W4310978364",
    "https://openalex.org/W2964134613",
    "https://openalex.org/W4287111248",
    "https://openalex.org/W2989577264",
    "https://openalex.org/W3093411241",
    "https://openalex.org/W3113067059",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W1744759976",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W3035050855",
    "https://openalex.org/W2948246283",
    "https://openalex.org/W3191441354",
    "https://openalex.org/W3035225512",
    "https://openalex.org/W2963465695",
    "https://openalex.org/W2944006115"
  ],
  "abstract": "Recently, Transformer-based networks have shown great promise on skeleton-based action recognition tasks. The ability to capture global and local dependencies is the key to success while it also brings quadratic computation and memory cost. Another problem is that previous studies mainly focus on the relationships among individual joints, which often suffers from the noisy skeleton joints introduced by the noisy inputs of sensors or inaccurate estimations. To address the above issues, we propose a novel Transformer-based network (IIP-Transformer). Instead of exploiting interactions among individual joints, our IIP-Transformer incorporates body joints and parts interactions simultaneously and thus can capture both joint-level (intra-part) and part-level (inter-part) dependencies efficiently and effectively. From the data aspect, we introduce a part-level skeleton data encoding that significantly reduces the computational complexity and is more robust to joint-level skeleton noise. Besides, a new part-level data augmentation is proposed to improve the performance of the model. On two large-scale datasets, NTU-RGB+D 60 and NTU RGB+D 120, the proposed IIP-Transformer achieves the-state-of-art performance with more than 8x less computational complexity than DSTA-Net, which is the SOTA Transformer-based method.",
  "full_text": "IIP-Transformer: Intra-Inter-Part Transformer for Skeleton-Based Action\nRecognition\nQingtian Wang1, Jianlin Peng1,3, Shuze Shi1,2, Tingxi Liu1, Jiabin He1, Renliang Weng1\n1Algorithm Research, Aibee Inc\n2Beijing Jiaotong University\n3Nanjing University of Aeronautics and Astronautics\n{qtwang,jlpeng,tliu,jhe,rlweng}@aibee.com,19120306@bjtu.edu.cn\nAbstract\nRecently, Transformer-based networks have shown great\npromise on skeleton-based action recognition tasks. The\nability to capture global and local dependencies is the key\nto success while it also brings quadratic computation and\nmemory cost. Another problem is that previous studies\nmainly focus on the relationships among individual joints,\nwhich often suffers from the noisy skeleton joints introduced\nby the noisy inputs of sensors or inaccurate estimations. To\naddress the above issues, we propose a novel Transformer-\nbased network (IIP-Transformer). Instead of exploiting in-\nteractions among individual joints, our IIP-Transformer in-\ncorporates body joints and parts interactions simultane-\nously and thus can capture both joint-level (intra-part) and\npart-level (inter-part) dependencies efï¬ciently and effec-\ntively. From the data aspect, we introduce a part-level skele-\nton data encoding that signiï¬cantly reduces the computa-\ntional complexity and is more robust to joint-level skeleton\nnoise. Besides, a new part-level data augmentation is pro-\nposed to improve the performance of the model. On two\nlarge-scale datasets, NTU-RGB+D 60 and NTU RGB+D\n120, the proposed IIP-Transformer achieves the-state-of-art\nperformance with more than 8Ã—less computational com-\nplexity than DSTA-Net, which is the SOTA Transformer-\nbased method.\n1. Introduction\nHuman action recognition has been studied during the\npast decades and achieves promising progress in many\napplications ranging from human-computer interaction to\nvideo retrieval [4, 12, 28, 29]. Recently, skeleton-based\nrepresentation has received increasing attention due to\nits compactness of depicting dynamic changes in human\nbody movements [14]. Advanced pose estimation algo-\nrithms [2] and advances in the somatosensory cameras such\nInter-Part\nIntra-Part\nPart division \nFigure 1. Illustration of our main idea. The body joints are divided\ninto 5 parts. The Inter-Part branch is used to explore relationships\nbetween parts and the Intra-Part branch aims to capture dependen-\ncies between joints in the same part.\nas Kinect [38] and RealSense [15] make it possible to obtain\nbody keypoints accurately and quickly at a low cost. In ad-\ndition, skeleton-based representation is more robust to vari-\nations of illumination and background noises in contrast to\nRGB representation. These merits attract researchers to de-\nvelop various methods to exploit skeleton for action recog-\nnition.\nPrevious skeleton-based action recognition methods uti-\nlize graph topologies or manually designed rules to trans-\nform the raw skeleton sequence into a grid-shape structure\nsuch as pseudo-image [10, 17, 20] or graph, and then feed it\ninto RNNs, CNNs, or GCNs [36] to extract features. How-\never, there is no guarantee that the hand-crafted structure is\nthe best choice of capturing joints relationships, which lim-\nits the generalizability and performance of previous works.\nRecently, Transformer-based methods [22, 27] have been\nproposed, relying on the multi-head self-attention mecha-\nnism which adaptively explores the potential dependencies\nbetween skeleton joints. Speciï¬cally, Shi et al. [27] treat\neach individual joint as a token while the calculation of self-\nattention grows quadratically to the number of tokens, thus\nintroduces a huge amount of calculations. This work also\n1\narXiv:2110.13385v1  [cs.CV]  26 Oct 2021\nsuffers from the noisy skeleton joints collected by sensors\nor inaccurate estimations.\nTo solve these problems, we introduce the concept of\nbody parts into transformer network. Most of actions such\nas standing up or dancing are performed by co-movement\nof body parts. Actually, body parts can be considered as\nthe minimum units of action execution, which means these\nactions can be identiï¬ed only by the movement of body\nparts. Different from other complex partition strategies, we\nsimply aggregate body joints into several parts according\nto human body topologies and encourage the model to ex-\nploit the complicated interactions. Speciï¬cally, we divide v\nbody joints into pparts and encode each part into a token,\nwhich reduces the spatial self-attention computation cost by\nv2/p2 times. Another advantage of our proposed partition\nencoding is that it enables the model to take sparser frames\nas temporal inputs, which brings in additional computation\nreduction. To encourage the model to reason globally in-\nstead of relying on a particular part, we propose a new data\naugmentation method named Part-Mask which masks out a\npart randomly during training. This new strategy makes the\nmodel more robust across challenging cases.\nSince the body joints are divided into parts, the joint-\nlevel information may be lost. For some ï¬ne-grained ac-\ntions, e.g. clapping or writing, it is necessary to capture\nthe interactions between body joints additionally. We pro-\npose the novel Intra-Inter-Part Transformer network (IIP-\nTransformer) to tackle this issue and make three main\nimprovements comparing with standard Transformer net-\nworks. First, Intra-Inter-Part self-attention mechanism is\nproposed to simultaneously capture intra-part features and\ninter-part features without increasing much computational\ncomplexity, as depicted in Figure 1. Second, inspired by\nBERT [9], we introduce a learnable class-token instead of\npooling all features extracted by backbone. Last, instead\nof using two individual transformers to model spatial and\ntemporal dependencies, we propose a new spatial-temporal\ntransformer that reduces model size while increases the\ngeneralization of the model. The code and models will\nbe made publicly available at https://github.com/\nqtwang0035/IIP-Transformer.\nOverall, our contributions can be summarized as follows:\nâ€¢ We introduce the concept of body parts into\ntransformer-based skeleton action recogni-\ntion. Our proposed partition encoding sig-\nniï¬cantly reduces self-attention computa-\ntional complexity and is relatively insensi-\ntive to joint noise.\nâ€¢ We propose IIP-Transformer, a novel\nspatial-temporal transformer network that\ncaptures intra-part and inter-part relations\nsimultaneously.\nâ€¢ Extensive experiments on two large-scale\nskeleton action datasets, e.g. NTU RGB+D\n60 & 120, show that our proposed IIP-\nTransformer achieves state-of-the-art per-\nformance with 2 âˆ¼36Ã—less computational\ncost.\n2. Related Work\nTransformer. In recent years, with the development of\nNatural Language Processing (NLP) tasks, the Transformer\nstructure [34] has been proposed to replace the traditional\nNLP network structures, e.g., RNNs. Different with RNN\narchitectures, the encoder and decoder of transformers com-\npletely rely on the self-attention mechanism, which can\neffectively solve the problems of long-sequence modeling\nand parallel processing. Because of these characteristics\nof the self-attention mechanism, the Transformer networks\nhave been also applied in various computer vision tasks\n[1, 3, 11, 22] and achieve superior results comparing with\nthe CNN models.\nSkeleton-based Action Recognition. Skeleton-based ac-\ntion recognition has received increasing attentions due to\nits compactness comparing with the RGB-based representa-\ntions. Previous data-driven methods rely on manual designs\nof traversal rules to transform the raw skeleton data into\na meaningful form such as a point-sequence or a pseudo-\nimage, so that they can be fed into the deep networks such\nas RNNs or CNNs for feature extraction [8, 16, 17, 35]. In-\nspired by the booming graph-based methods, Yanet al. [36]\nintroduce GCN into the skeleton-based action recognition\ntask, and propose the ST-GCN to model the spatial conï¬gu-\nrations and temporal dynamics of skeletons simultaneously.\nThe GCN-based methods [7, 26] use the topological struc-\nture of the human skeleton to aggregate features of related\nskeleton nodes and time series. Therefore, the GCN-based\nmethods show better performance than previous methods.\nInstead of formulating the skeleton data into the images or\ngraphs, Transformer-based methods directly model the de-\npendencies of joints with pure attention blocks. Plizzari et\nal. [22] propose a method that introduces Transformer in\nskeleton activity recognition and combine it with GCN. Shi\net al. [27] employ a solely transformer network to exploit\nrelations between joints. Our proposed method utilizes part-\nlevel input, thus could efï¬ciently capture both joint-level\nand part-level relations.\nPart-based Methods. Part-based methods are designed\nto extract features of body parts individually since human\nbody is a natural topology with ï¬ve major parts. Thakkar\net al. [33] divide graph into several sub-graphs with shared\nnodes. They employ GCN operation within sub-graphs of\nbody parts and then propagate information between sub-\ngraphs via the shared nodes. Huang et al. [13] propose an\n2\nS-IIPA\nT-IIPAPatch  Embedding\nPosition Encoding\nC\nFFN\nFC\nğ‘¿ğ’‘\nIntra-Inter-Part Transformer\nÃ—L\nğğ«ğğğ¢ğœğ­ğ¢ğ¨ğ§\nclass token\nSkeleton data\nPart division ST-Transformer Layer\nNÃ—ğ¶! class token\nFigure 2. The overall architecture of the proposed pipeline which is composed of Partition Encoding and IIP-Transformer.\nautomatic partition strategy and utilize a part-based GCN to\nexplore discriminative features from joints and body parts.\nSong et al. [31] propose part-attention mechanism to dis-\ncover the most informative parts. All these part-based meth-\nods employ complex strategies to propagate information in-\ndividually or fuse information from all parts, while our work\nfocuses on simultaneously capturing discriminative features\nfrom intra and inter parts with less computational cost.\n3. Methods\n3.1. Overall Architecture\nFigure 2 shows the overall architecture of our pipeline,\nwhich mainly contains two sections, the partition encoding\nand the IIP-Transformer network. For an input skeleton se-\nquence with V joints, F frames and C channels, we ï¬rst\ndivide V joints into P parts, and then a patch embedding\nlayer is used to project these part data into N tokens with\ndimension Cp, where N = P Ã—F. Before being fed into\nIIP-Transformer, a class-token will be concatenated with to-\nkens above, resulting in N + 1tokens. There are Llayers\nstacked in IIP-Transformer in total and each layer is com-\nposed of a spatial IIPA module (S-IIPA) that models the\nspatial relations between parts in the same frame, a tem-\nporal IIPA module (T-IIPA) that captures the temporal re-\nlations of parts among different frames and a feed-forward\nnetwork. Each layer of IIP-Transformer maintains the same\nnumber of tokens, and thus we get N + 1tokens as ï¬nal\noutput features. We feed the feature corresponding to the\nclass-token into a fully-connected layer to obtain the clas-\nsiï¬cation scores. The implementation details will be intro-\nduced in the following sections.\n3.2. Partition Encoding\nIn order to reduce the computation complexity of self-\nattention in spatial dimension, we propose a strategy to en-\ncode the joints intoP parts (5 parts in our case speciï¬cally).\nThe partition encoding procedure is illustrated in Figure 3.\nFirst, the raw skeleton sequence X âˆˆRCÃ—FÃ—V is fed into\nfeature extraction layers fJ(Â·) to obtain a deeper feature\nğ‘Ã—ğ¶!\nğ‘¿\nReshape\nPatch  Embedding\nğ¶\"Ã—FÃ—ğ‘€Ã—ğ‘ƒ\nğ¶Ã—ğ¹Ã—ğ‘‰\nğ‘Ã—ğ¶#\nğ‘¿ğ’‘\nFeature Extraction\nPart Division ğ¶\"Ã—ğ¹Ã—ğ‘‰\nFigure 3. Illustration of the Partition Encoding Procedure.\nXJ âˆˆ RCoÃ—FÃ—V , where Co denotes the channel of fea-\ntures after feature extraction and fJ(Â·) is implemented by\ntwo convolution layers with BatchNorm and ReLU func-\ntion. Then P individual body parts are obtained by selecting\ncorresponding joints:\nXJ â†’[x1,x2,Â·Â·Â· ,xP ],xi âˆˆRCoÃ—FÃ—M (1)\nwhere M denotes the max number of joints in each part.\nThe parts with less than M joints will be padded with zero.\nSubsequently, we concatenate P parts and permute the di-\nmensions:\nËœXJ = Concat(xi|i= 1,2,Â·Â·Â· ,P) (2)\nËœXJ âˆˆRCoÃ—FÃ—MÃ—P â†’RNÃ—Cm (3)\nwhere N = P Ã—F and Cm = Co Ã—M. Finally, ËœXJ is fed\ninto patch embedding layer fP (Â·) to aggregate information\nof the internal skeleton joints of a part and we get the ï¬nal\n3\npartition encoding XP âˆˆRNÃ—CP , where CP is the chan-\nnels of the partition encoding and fP (Â·) is implemented by\nlinear layers with ReLU function.\nBy aggregating the information of the internal skeleton\njoints of the parts, an informative encoding is extracted. It\ndrives the model to concentrate on body parts instead of\njoints, and thus reduce the inï¬‚uence of individual noisy\njoints. Besides, experiments show that the partition encod-\ning enables model to take sparser temporal inputs.\n3.3. Intra-Inter-Part Transformer\nThe backbone of IIP-Transformer is constructed by\nstacking L layers of Spatial-Temporal Transformer Layer\nwhich is composed of S-IIPA, T-IIPA and feed-forward net-\nwork. The core of S-IIPA and T-IIPA is Intra-Inter-Part\nSelf-Attention mechanism. In this section, the IIPA mech-\nanism and Spatial-Temporal Transformer will be brieï¬‚y in-\ntroduced.\nmulti-head self- attention\nğ’‡ğ’Šğ’ğ’•ğ’“ğ’‚(Â·)\nğ‘¿ğ’ğ’–ğ’•\nV\nK\nQ\nğ‘¿ğ’Šğ’\nFigure 4. Illustration of the IIPA mechanism. The original multi-\nhead self attention is used to explore relations between tokens\nwhile the function fintra is used to capture internal relations of\na token.\nIntra-Inter-Part Self-Attention. Basic transformer uti-\nlizes self-attention mechanism to model the relationships\nbetween input tokens. But in part-level cases, each token\nrepresents a collection of joints in the same part. Therefore,\nthe relationships of joints in the same part have not been\nfully exploited and can not be propagated effectively. We\npresent a new self-attention mechanism named Intra-Inter-\nPart self-attention (IIPA) to simultaneously capture relation-\nships inside and between tokens. As shown in Figure 4,\nour proposed IIPA mainly consists of a standard multi-head\nself-attention and an intra-part branch.\nGiven the input feature Xin, we ï¬rst compute the query\nQ, key K and value V using three linear projection layers.\nThe information ï¬‚ow across tokens is achieved by the stan-\ndard multi-head self-attention:\nXinter = MHSA (Q,K,V ) (4)\nThe intra-part branch fintra(Â·) is implemented by linear\nlayers. It takes value V as input and extracts features in-\nside tokens, termed joint-level feature.\nXintra = fintra (V) (5)\nFinally, we fuse the multi-head self-attention output Xinter\nwith the intra-part branch output Xintra, so that features\nextracted by IIPA carry both joint-level and part-level infor-\nmation.\nXout = Xinter + Xintra (6)\nComparing with standard self-attention mechanism, our\nproposed IIPA introduces no more than 1/4 calculations, but\nachieves obvious improvement in ï¬ne-grained actions, as\nshown in ablation study.\n(A)\nSelf-AttentionAdd&NormFFNAdd&NormT-IIPAAdd&NormFFNAdd&Norm\nS-IIPAAdd&Norm\n(ğ‘·ğ‘­+ğŸ)Ã—ğ‘ª (ğ‘·ğ‘­+ğŸ)Ã—ğ‘ª\n(ğ‘·ğ‘­+ğŸ)Ã—ğ‘ª\n(B)\nFigure 5. Two Transformer structures. (A) Flatten the spatial-\ntemporal data into a single sequence and use an standard trans-\nformer. (B) Explore spatial and temporal relations with S-IIPA\nand T-IIPA respectively and fuse them by a feed-forward layer.\nSpatial-Temporal Transformer Layer. In previous\nskeleton-based action recognition transformers [22, 25], all\nfeatures extracted by backbone is average-pooled to obtain\nthe ï¬nal feature for classiï¬cation. Inspired by BERT [9],\nclass-token is introduced into our model. In BERT, the in-\nput X1d âˆˆR(N+1)Ã—C is a single dimension sequence with\nN tokens and a class-token. The single dimension struc-\nture naturally ï¬ts the self-attention mechanism. But for the\nskeleton sequence which has two dimensions, there are two\nproblems to solve: one is how to exploit spatial and tem-\nporal dimension with self-attention mechanism, another is\nhow to deal with class-token. The ï¬rst method is ï¬‚attening\nthe raw skeleton sequenceX2d âˆˆRPÃ—FÃ—C into a single di-\nmension sequence Xflatten âˆˆRPF Ã—C and concatenating it\nwith a class-token to obtain Xin âˆˆR(PF +1)Ã—C, where P\ndenotes the number of parts and F denotes the number of\nframes. The output Xout âˆˆR(PF +1)Ã—C is calculated by\na standard transformer and the corresponding structure is\nshown in Figure 5-(A):\nXout = softmax\n(QKT\nâˆš\nC\n)\nV (7)\n4\nSelf-Attentionğ‘°ğ‘°ğ‘·ğ‘¨\nC\nxğ’„ğ’ğ’”ğ’’ K V\nğ‘¿ğ’‡ğ’Šğ’ğ’‚ğ’_ğ’ğ’–ğ’•ğ’”\nğ‘¿ğ’ğ’–ğ’•ğ’”xğ’„ğ’ğ’”ğ’ reshape\nQ\nğ‘½ğ’”ğ‘¸ğ’”\nreshape\nğ‘²ğ’”\nğ‘¿ğ’Šğ’\nxğ’„ğ’ğ’”ğ’Œ xğ’„ğ’ğ’”ğ’—\nxğ’„ğ’ğ’”\nFigure 6. Illustration of S-IIPA. The symbol C stands for concate-\nnate operation. T-IIPA has the same structure as S-IIPA except for\nreshape size of K, Q, V.\nHowever, it is unreasonable to treat spatial and temporal\nfeatures equivalently since the semantic information they\ncontain are totally different. Besides, the computational\ncomplexity of calculating the attention map is quadratic to\nsequence length, therefore ï¬‚attening the skeleton sequence\ninto a long single dimension sequence will introduce a large\namount of calculations.\nInstead, the spatial and temporal dimension are treated as\ntwo different dimensions in Spatial-Temporal Transformer\nlayer and processed by S-IIPA and T-IIPA respectively, as\nshown in Figure 5-(B). The input of S-IIPA and T-IIPA is\nthe same as above, a ï¬‚attened sequence with a class-token.\nS-IIPA is adopted to explore the dependencies of parts in\na single frame while T-IIPA is employed to propagate in-\nformation for each part across different frames. Letâ€™s take\nS-IIPA for an example, we ï¬rst project the part-level se-\nquence Xin âˆˆ R(PF +1)Ã—C with three linear projections\nlayers fq(Â·),fk(Â·) and fv(Â·):\nQcls,Q = fq (Xin) (8)\nKcls,K = fk (Xin) (9)\nVcls,V = fv (Xin) (10)\nwhere Qcls,Kcls,Vcls âˆˆR1Ã—C and Q,K,V âˆˆRPF Ã—C.\nDue to the existence of class-token, we employ two\nbranches, the self-attention branch and IIPA branch, to\nmodel class-token and other tokens separately, as shown in\nFigure 6. The self-attention branch is similar to the above\nmethod, and thus the output Xo\ncls âˆˆR1Ã—C could obtain in-\nformation from all parts at all frames.\nKc = Concat(Kcls,K) (11)\nVc = Concat(Vcls,V ) (12)\nXo\ncls = softmax\n(\nQcls (Kc)T\nâˆš\nC\n)\nVc (13)\nIn the IIPA branch, we reshape K,Q,V from single\ndimension into temporal Ã—spatial dimension and repeat\nKcls,Qcls,Vcls in the temporal dimension, then concate-\nnate them in the spatial dimension.\nQ,K,V âˆˆRPF Ã—C â†’Qs,Ks,V s âˆˆRFÃ—PÃ—C (14)\nKcls,Vcls âˆˆR1Ã—C â†’RFÃ—1Ã—C (15)\nKs = Concat(Kcls, Ks) (16)\nVs = Concat(Vcls, Vs) (17)\nBy following the preceding steps, we get Ks,V s âˆˆ\nRFÃ—(P+1)Ã—C , (P + 1)means appending a class-token in\nevery frame. Then Qs,Ks,V s are fed into IIPA to calculate\nthe output frame by frame, and each frame uses a unique at-\ntention map:\nXf = softmax\nï£«\nï£¬ï£­\nQs\nf\n(\nKs\nf\n)T\nâˆš\nC\nï£¶\nï£·ï£¸Vs\nf , f= 1,2 Â·Â·Â· ,F\n(18)\nwhere Xf âˆˆ RPÃ—C is the output for frame f, Qs\nf âˆˆ\nRPÃ—C; Ks\nf , Vs\nf âˆˆ R(P+1)Ã—C. Finally, we concatenate\nthe outputs of all frames and reshape it back to single di-\nmension:\nXs\nout = Concat({Xf |f = 1,2,Â·Â·Â· ,F}) (19)\nXs\nout âˆˆRFÃ—PÃ—C â†’RFP Ã—C (20)\nXs\nfinal out = Concat(Xo\ncls,Xs\nout) (21)\nwhere Xs\nfinal out âˆˆ R(FP +1)Ã—C is the ï¬nal output of\nS-IIPA with the same dimension size as the input se-\nquence. The only difference in T-IIPA is that Qt âˆˆ\nRPÃ—FÃ—C; Kt,V t âˆˆ RPÃ—(F+1)Ã—C, corresponding to\nQs,Ks,V s, thus the output is calculated part by part.\nComparing with other spatial-temporal transformer\nmethods, e.g. DSTA [27], which uses two complete trans-\nformers to model space and time, we remove the feed-\nforward layer in the spatial transformer structure and com-\nbine S-IIPA, T-IIPA and feed-forward layer as a new spatial-\ntemporal transformer structure. This structure performs\nspatial and temporal feature extraction in one stage so that\nthe same-order features can be fully exploited. Meanwhile,\nit also reduces a large amount of parameters in feed-forward\nlayer and improves the generalizability of the model.\n5\n3.4. Data Augmentation\nRao et al. [23] propose several joint-level skeleton data\naugmentation methods, such as Rotation, GaussianNoise,\nGaussianBlur, JointMask, to improve the generalizability\nof models. However, since the part-level encoding is more\nrobust to joint noises, these methods do not work well on\npart-level skeleton data, except for Rotation. Therefore we\npropose a new data augmentation method called PartMask,\nto encourage the model to reason globally instead of relying\non a particular part. The Rotation and PartMask methods\nare deï¬ned as follows.\nRotation. Most public skeleton-based datasets are captured\nin speciï¬c view points. We obtain plentiful data through\nRotation transformation which simulates viewpoint change\nof the camera.\nBased on Eulerâ€™s rotation theorem, any 3D rotation can\nbe disassembled into a composition of rotations about three\naxes [35]. The basic rotation matrices are represented as\nbelow:\nRx(Î±) =\nï£®\nï£°\n1 0 0\n0 cos Î± âˆ’sin Î±\n0 sin Î± cos Î±\nï£¹\nï£» (22)\nRy(Î²) =\nï£®\nï£°\ncos Î² 0 sin Î²\n0 1 0\nâˆ’sin Î² 0 cos Î²\nï£¹\nï£» (23)\nRz(Î³) =\nï£®\nï£°\ncos Î³ âˆ’sin Î³ 0\nsin Î³ cos Î³ 0\n0 0 1\nï£¹\nï£» (24)\nR= Rz(Î³)Ry(Î²)Rx(Î±) (25)\nwhere Rx(Î±),Ry(Î²),RZ(Î³) denote the rotation matrices\nof x,y,z axis with angle Î±,Î²,Î³ respectively, and Ris the\ngeneral rotation matrix that will be applied to original coor-\ndinates of the skeleton sequence. In this work, the rotation\nangles are randomly sampled from [âˆ’Ï€/10,Ï€/10].\nPartMask. As an effective augmentation strategy to reduce\nreliance on speciï¬c regions, the mask strategy has been\nwidely used in data augmentation. But simply employing\na random zero-mask to a number of body joints in skele-\nton frames before partition encoding, which is similar to\njoint-level noises, does not work well due to the anti-noise\nability of partition encoding. On the other hand, capturing\nthe global information instead of focusing on a particular\npart will beneï¬t the action classiï¬cation tasks. Therefore we\nemploy a part-level mask to encourage the model to reason\nglobally. Speciï¬cally, we randomly select a certain body\npart p from [1,2,Â·Â·Â· ,P], and apply zero-mask to it in all\nframes:\nXJ = XJ âŠ™mask\n= [x1,x2,Â·Â·Â· ,xP ] âŠ™[1,1,Â·Â·Â· ,0\np\n,Â·Â·Â· ,1] (26)\nwhere XJ denotes the intermediate result of partitioning,\nas discussed in Section 3.2, and mask is an all-ones vector\nexcept for the position of the selected part p.\n4. Experiments\n4.1. DataSets\nNTU RGB+D.NTU RGB+D [24] is a widely used large-\nscale human skeleton-based action recognition dataset,\nwhich contains 56,880 skeletal action sequences. These\naction sequences were performed by 40 volunteers and di-\nvided into 60 categories. Each action sequence is completed\nby one or two subjects and is captured by three Microsoft\nKinect-V2 cameras from different views simultaneously.\nThe benchmark evaluations include Cross-Subject (X-Sub)\nand Cross-View (X-View). In the Cross-Subject, training\ndata comes from 20 subjects, and testing data comes from\nthe other 20 subjects. In the Cross-View, training data\ncomes from camera views 2 and 3, and testing data comes\nfrom camera view 1. Note that there are 302 wrong samples\nthat need to be ignored.\nNTU RGB+D 120.NTU RGB+D 120 [18] is currently the\nlargest human skeleton-based action recognition dataset. It\nis an extension of the NTU RGB+D dataset, with 113,945\naction sequences and 120 action classes in total. These ac-\ntion sequences were performed by 106 volunteers, captured\nwith three cameras views, and contains 32 setups, each of\nwhich represents a different location and background. The\nbenchmark evaluations include Cross-Subject (X-Sub) and\nCross-Setup (X-Setup). In the Cross-Subject, training data\ncomes from 53 subjects, and testing data comes from the\nother 53 subjects. In the Cross-Setup, training data comes\nfrom samples with even setup IDs, and testing data comes\nfrom samples with odd setup IDs. In this dataset, 532 bad\nsamples should be ignored.\n4.2. Implementation Details\nAll experiments are conducted on 8 GTX 1080Ti GPUs.\nOur model is trained using SGD optimizer with momentum\n0.9 and weight decay 0.0002. The training epoch is set to\n300. Learning rate is set to 0.1 and decays with a cosine\nscheduler. The batch size is 64 and each sample contains 32\nframes. The number of Spatial-Temporal Transformer layer\nis set to 4.\n4.3. Ablation Study\nIn this section, we investigate the effectiveness of the\nproposed components of the IIP-Transformer. All experi-\nments are conducted on NTU RGB+D 60 with joint stream\nif no special instruction.\nEffect of IIPA.The comparison between IIPA and stan-\ndard Self-Attention in Table 1 shows that there are 1.8%\n6\nMethods X-Sub X-View\nStandard Self-Attention 85.3 91.2\nIIPA (ours) 87.1 93.2\nTable 1. Comparison between IIPA and standard Self-Attention.\nFigure 7. The accuracy comparison on actions with different mo-\ntion intensity. The categories above the red horizontal line are\nï¬ne-grained action categories.\nand 2% improvements in X-Sub and X-View respectively.\nWe pick out some actions and visualize the changes of ac-\ncuracy between IIPA and standard Self-Attention in Figure\nFigure 7. It demonstrates that the IIPA achieves remarkable\nimprovements on ï¬ne-grained actions (e.g., type on a key-\nboard, clapping, reading etc.), while the accuracy of drastic\nactions (e.g., throw, pushing, pickup etc.) is about the same,\nwhich indicates that our proposed IIPA can exploit the joint-\nlevel information of body parts more effectively.\nEffect of Class-Token.To explore the effect of class-token,\nwe replace it by simply performing global average pooling\nto the output features from last layer [22, 27]. As shown in\nTable 2, by introducing the class-token, the performance of\nX-Sub and X-View improve by 1.3% and 1.7% respectively.\nMethods X-Sub X-View\nNon-CLS 85.8 91.5\nCLS (ours) 87.1 93.2\nTable 2. Ablation study on class-token. Non-CLS denotes the\nprevious global average pooling method.\nEffect of Partition Encoding.To explore the effect of Par-\ntition Encoding, we remove the Partition Encoding module\nfrom the proposed pipeline, which increases the number of\ntokens by 5 times (from 5 parts to 25 joints) and conduct\nexperiments with different frame numbers. Comparing with\nMethods Frames X-Sub FLOPs\n32 84.1 44.6G\nNon-PE 64 85.2 97.2G\n128 86.0 198.7G\n32 87.1 7.2G\nPE 64 86.9 19.8G\n128 87.0 45.5G\nTable 3. Ablation study of the Partition Encoding.\nFigure 8. Ablation study over different methods with different in-\nput frames on NTU RGB+D 60 (X-Sub). The results are obtained\nafter multi-stream fusion which will be introduced in Section 4.4.\nMethods X-Sub X-View\nOrigin 87.1 93.2\nGaussian-Noise 87.1 93.1\nGaussian-Blur 86.5 92.8\nJoint-Mask 87.2 92.9\nRotation 87.9 93.9\nPart-Mask 88.4 94.0\nRotation+Part-Mask 88.9 94.2\nTable 4. Ablation study of data augmentation on NTU RGB+D\n60.\nNon-Partition Encoding (Non-PE), Partition Encoding (PE)\nimproves accuracy by 1.1% with much less computational\ncost, as shown in Table 3. Besides, PE enables the model\nto take sparser frames as temporal inputs, while the accu-\nracy of Non-PE drops by 1.9% when reducing the number\nof frames from 128 to 32, as shown in Table 3. In addition,\nwe compare the accuracy of methods [7,21,22,27] with dif-\nferent number of input frames, as shown in Figure 8. Our\nIIP-Transformer achieves comparable results with only 32\nframes.\nEffect of Data Augmentation.To evaluate the impact of\nthe proposed data augmentation strategies, we conduct ex-\nperiments with different data augmentation (see Table 4).\nApplying part-level data augmentation strategies( e.g., Ro-\ntation and Part-Mask) on X-Sub improves the accuracy\n7\nMethods X-Sub X-View X-Sub 120 X-Set 120 Param FLOPs\nST-LSTM [19] 69.2 77.7 55.7 57.9 - -\nHCN [17] 86.5 91.1 - - - -\nST-GCN [36] 81.5 88.3 - - 3.1M 16.3G\n2s-AGCN [26] 88.5 95.1 82.9 84.9 6.9M 37.3G\nAGC-LSTM [30] 89.2 95.0 - - 22.9M -\nPL-GCN [13] 89.2 95.0 - - 20.7M -\nDGNN [25] 89.9 96.1 - - 26.2M -\nShift-GCN [7] 90.7 96.5 85.9 87.6 2.8M 10.0G\nDC-GCN+ADG [6] 90.8 96.6 86.5 88.1 4.9M 25.7G\nPA-ResGCN-B19 [31] 90.9 96.0 87.3 88.3 3.6M 18.5G\nDynamic GCN [37] 91.5 96.0 87.3 88.6 14.4M -\nMS-G3D [21] 91.5 96.2 86.9 88.4 2.8M 48.8G\nMST-GCN [5] 91.5 96.6 87.5 88.8 12.0M -\nEfï¬cientGCN-B4 [32] 91.7 95.7 88.3 89.1 2.0M 15.2G\nST-TR [22] 89.9 96.1 82.7 84.7 12.1M 259.4G\nDSTA [27] 91.5 96.4 86.6 89.0 4.1M 64.7G\nIIP-Transformer 92.3 96.4 88.4 89.7 2.9M 7.2G\nTable 5. Comparison of top-1 accuracy (%), model size and computational complexity over different methods on the NTU RGB+D 60/120\ndatasets.\nby 0.8% and 1.3% respectively, and the best results are\nachieved when combining these two strategies. While\nthe effect of joint-level data augmentation ( e.g., Gaussian-\nNoise, Gaussian-Blur and Joint-Mask) is marginal. Intu-\nitively, the reason that Part-Mask strategy can effectively\nimprove the accuracy of the model is that it encourages the\nmodel to reason globally and reduce the dependency on any\nparticular part.\n4.4. Comparison with State of the Arts\nSimilar to most SOTA methods, we follow the same\nmulti-stream fusion strategies proposed in [7] for fair com-\nparison. We train four models with different modalities,\ne.g., joint, bone, joint motion, and bone motion respectively,\nthen average the softmax outputs from multiple streams to\nobtain the ï¬nal scores during inference. The comparison\nresults are shown in Table 5.\nAccuracy Comparison.First we compare our results with\nGCN-based methods [6,7,21,25,26,31,32,36,37]. Our pro-\nposed method outperforms the best GCN method by 0.6%\non X-sub of NTU RGB+D 60 and 0.6% on X-Set of NTU\nRGB+D 120 respectively. In terms of Transformer-based\nmethods [22, 27], our method also surpasses them on most\nof the metrics by a signiï¬cant margin.\nComplexity Comparison. We evaluate the computational\ncomplexity with FLOPs, e.g. the number of ï¬‚oating-point\nmultiplication-adds, and measure the model size with the\namount of parameters. The computational complexity of\nour proposed IIP-Transformer is 2.2 times less than ST-\nGCN [36] and 6.7 times less than MS-G3D [21], while the\nmodel sizes are similar. Comparing with the Transformer-\nbased methods, IIP-Transformer achieves superior results\nwith only a fraction of computational complexity and model\nsize.\n5. Conclusion\nIn this work, we propose a novel intra-inter-part trans-\nformer network (IIP-Transformer) for skeleton-based ac-\ntion recognition. It effectively captures inter-part and intra-\npart dependencies. Thanks to the part-level encoding and\nspatial-temporal separation, our method enjoys high efï¬-\nciency. Besides, a part-level data augmentation namedPart-\nMask is proposed to encourage the model focus on global\nparts. On two large scale datasets, NTU RGB+D 60 & 120,\nthe proposed IIP-Transformer notably exceeds the current\nstate-of-the-art methods with 2 âˆ¼36Ã—less computational\ncost.\nReferences\n[1] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V Le. Attention augmented convolutional net-\nworks. In IEEE International Conference on Computer Vi-\nsion (ICCV), pages 3285â€“3294, 2019. 2\n[2] Z. Cao, G. Hidalgo, T. Simon, S. E. Wei, and Y . Sheikh.\nOpenpose: Realtime multi-person 2d pose estimation using\npart afï¬nity ï¬elds. IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI), 43(1):172â€“186, 2018. 1\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Andrea Vedaldi,\n8\nHorst Bischof, Thomas Brox, and Jan-Michael Frahm, edi-\ntors, Computer Vision - ECCV 2020 - 16th European Confer-\nence, Glasgow, UK, August 23-28, 2020, Proceedings, Part I,\nvolume 12346 of Lecture Notes in Computer Science, pages\n213â€“229. Springer, 2020. 2\n[4] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In 2017\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 4724â€“4733, July 2017. 1\n[5] Zhan Chen, Sicheng Li, Bing Yang, Qinghan Li, and Hong\nLiu. Multi-scale spatial temporal graph convolutional net-\nwork for skeleton-based action recognition. In Proceed-\nings of the AAAI Conference on Artiï¬cial Intelligence, pages\n1113â€“1122, 2021. 8\n[6] Ke Cheng, Yifan Zhang, Congqi Cao, Lei Shi, Jian Cheng,\nand Hanqing Lu. Decoupling gcn with dropgraph module\nfor skeleton-based action recognition. In European Confer-\nence on Computer Vision (ECCV), pages 536â€“553. Springer,\n2020. 8\n[7] Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian\nCheng, and Hanqing Lu. Skeleton-based action recognition\nwith shift graph convolutional network. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n180â€“189, June 2020. 2, 7, 8\n[8] Guilhem Ch Â´eron, Ivan Laptev, and Cordelia Schmid. P-cnn:\nPose-based cnn features for action recognition. In IEEE In-\nternational Conference on Computer Vision (ICCV) , pages\n3218â€“3226, 2015. 2\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert:pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\nConference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language\nTechnologies(NAACL-HLT), pages 4171â€“4186, 2019. 2, 4\n[10] Zewei Ding, Pichao Wang, Philip O Ogunbona, and Wanqing\nLi. Investigation of different skeleton features for cnn-based\n3d action recognition. In IEEE International Conference on\nMultimedia & Expo Workshops (ICMEW) , pages 617â€“622.\nIEEE, 2017. 1\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. InInternational Con-\nference on Learning Representations (ICLR), 2020. 2\n[12] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\n2019 IEEE/CVF International Conference on Computer Vi-\nsion, ICCV 2019, Seoul, Korea (South), October 27 - Novem-\nber 2, 2019, pages 6201â€“6210. IEEE, 2019. 1\n[13] Linjiang Huang, Yan Huang, Wanli Ouyang, and Liang\nWang. Part-level graph convolutional network for skeleton-\nbased action recognition. In Proceedings of the AAAI Con-\nference on Artiï¬cial Intelligence , volume 34, pages 11045â€“\n11052, 2020. 2, 8\n[14] G. Johansson. Visual perception of biological motion and\na model for its analysis. Perception & Psychophysics, 14,\n1973. 1\n[15] L. Keselman, J. I. Woodï¬ll, A. Grunnet-Jepsen, and A.\nBhowmik. Intel(r) realsense(tm) stereoscopic depth cameras.\nIn IEEE Conference on Computer Vision and Pattern Recog-\nnition Workshops (CVPRW), pages 1267â€“1276, 2017. 1\n[16] Guy Lev, Gil Sadeh, Benjamin Klein, and Lior Wolf. Rnn\nï¬sher vectors for action recognition and image annotation.\nIn European Conference on Computer Vision (ECCV), pages\n833â€“850. Springer, 2016. 2\n[17] Chao Li, Qiaoyong Zhong, Di Xie, and Shiliang Pu. Co-\noccurrence feature learning from skeleton data for action\nrecognition and detection with hierarchical aggregation.\npages 786â€“792, 2018. 1, 2, 8\n[18] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,\nLing-Yu Duan, and Alex C. Kot. Ntu rgb+d 120: A large-\nscale benchmark for 3d human activity understanding. IEEE\nTransactions on Pattern Analysis and Machine Intelligence\n(TPAMI), 42(10):2684â€“2701, 2020. 6\n[19] Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang.\nSpatio-temporal lstm with trust gates for 3d human action\nrecognition. In European Conference on Computer Vision\n(ECCV), pages 816â€“833. Springer, 2016. 8\n[20] Mengyuan Liu, Hong Liu, and Chen Chen. Enhanced skele-\nton visualization for view invariant human action recogni-\ntion. Pattern Recognition, 68:346â€“362, 2017. 1\n[21] Ziyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang,\nand Wanli Ouyang. Disentangling and unifying graph con-\nvolutions for skeleton-based action recognition. In IEEE\nconference on Computer Vision and Pattern Recognition\n(CVPR), pages 143â€“152, 2020. 7, 8\n[22] Chiara Plizzari, Marco Cannici, and Matteo Matteucci.\nSkeleton-based action recognition via spatial and temporal\ntransformer networks. Computer Vision and Image Under-\nstanding, 208:103219, 2021. 1, 2, 4, 7, 8\n[23] Haocong Rao, Shihao Xu, Xiping Hu, Jun Cheng, and Bin\nHu. Augmented skeleton based contrastive action learning\nwith momentum lstm for unsupervised action recognition.\nInformation Sciences, 569:90â€“109, 2021. 6\n[24] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\nNtu rgb+d: A large scale dataset for 3d human activity anal-\nysis. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1010â€“1019, June 2016. 6\n[25] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.\nSkeleton-based action recognition with directed graph neu-\nral networks. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 7912â€“7921, 2019. 4, 8\n[26] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Two-\nstream adaptive graph convolutional networks for skeleton-\nbased action recognition. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 12026â€“\n12035, 2019. 2, 8\n[27] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Decou-\npled spatial-temporal attention network for skeleton-based\naction-gesture recognition. In Asian Conference on Com-\nputer Vision (ACCV), volume 12626, pages 38â€“53, 2020. 1,\n2, 5, 7, 8\n[28] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Action\nrecognition via pose-based graph convolutional networks\n9\nwith intermediate dense supervision. Pattern Recognition,\npage 108170, 2021. 1\n[29] L. Shi, Y . Zhang, J. Hu, J. Cheng, and H. Lu. Gesture recog-\nnition using spatiotemporal deformable convolutional repre-\nsentation. In 2019 IEEE International Conference on Image\nProcessing (ICIP), pages 1900â€“1904, 2019. 1\n[30] Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, and\nTieniu Tan. An attention enhanced graph convolutional lstm\nnetwork for skeleton-based action recognition. In IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 1227â€“1236, 2019. 8\n[31] Yi-Fan Song, Zhang Zhang, Caifeng Shan, and Liang Wang.\nStronger, faster and more explainable: A graph convolutional\nbaseline for skeleton-based action recognition. In Proceed-\nings of the 28th ACM International Conference on Multime-\ndia, pages 1625â€“1633, 2020. 3, 8\n[32] Yi-Fan Song, Zhang Zhang, Caifeng Shan, and Liang Wang.\nConstructing stronger and faster baselines for skeleton-based\naction recognition. arXiv preprint arXiv:2106.15125, 2021.\n8\n[33] Kalpit C. Thakkar and P. J. Narayanan. Part-based graph\nconvolutional network for action recognition. In British Ma-\nchine Vision Conference (BMVC), page 270, 2018. 2\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998â€“6008, 2017. 2\n[35] Hongsong Wang and Liang Wang. Modeling temporal\ndynamics and spatial conï¬gurations of actions using two-\nstream recurrent neural networks. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , pages\n499â€“508, 2017. 2, 6\n[36] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-\nral graph convolutional networks for skeleton-based action\nrecognition. In Sheila A. McIlraith and Kilian Q. Wein-\nberger, editors, Proceedings of the AAAI Conference on Ar-\ntiï¬cial Intelligence, pages 7444â€“7452. AAAI Press, 2018. 1,\n2, 8\n[37] Fanfan Ye, Shiliang Pu, Qiaoyong Zhong, Chao Li, Di Xie,\nand Huiming Tang. Dynamic GCN: context-enriched topol-\nogy learning for skeleton-based action recognition. CoRR,\nabs/2007.14690, 2020. 8\n[38] Zhengyou Zhang. Microsoft kinect sensor and its effect.\nIEEE MultiMedia, 19(2):4â€“10, 2012. 1\n10",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7800540924072266
    },
    {
      "name": "Computer science",
      "score": 0.6947202682495117
    },
    {
      "name": "Computation",
      "score": 0.5397428274154663
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4580402076244354
    },
    {
      "name": "Computational complexity theory",
      "score": 0.44138211011886597
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32288658618927
    },
    {
      "name": "Algorithm",
      "score": 0.31525859236717224
    },
    {
      "name": "Engineering",
      "score": 0.1497710645198822
    },
    {
      "name": "Voltage",
      "score": 0.10080301761627197
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}