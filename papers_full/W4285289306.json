{
  "title": "AraT5: Text-to-Text Transformers for Arabic Language Generation",
  "url": "https://openalex.org/W4285289306",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2620602306",
      "name": "El Moatez Billah Nagoudi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4319245227",
      "name": "AbdelRahim Elmadany",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A3119026467",
      "name": "Muhammad Abdul-Mageed",
      "affiliations": [
        "University of British Columbia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3102483398",
    "https://openalex.org/W2527845440",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W1485311222",
    "https://openalex.org/W4288112344",
    "https://openalex.org/W3133440961",
    "https://openalex.org/W3119989665",
    "https://openalex.org/W3041866211",
    "https://openalex.org/W3094431412",
    "https://openalex.org/W2970513828",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2573197213",
    "https://openalex.org/W2953830716",
    "https://openalex.org/W3204130541",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W137989762",
    "https://openalex.org/W630532510",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2806962830",
    "https://openalex.org/W3104196571",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2791757336",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2991784022",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W2251224833",
    "https://openalex.org/W3088728183",
    "https://openalex.org/W179850243",
    "https://openalex.org/W2917668649",
    "https://openalex.org/W2396324390",
    "https://openalex.org/W2251654371",
    "https://openalex.org/W2970814728",
    "https://openalex.org/W2572474373",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3168030157",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W4297663785",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3127274523",
    "https://openalex.org/W3153540814",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W3120115961",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W3098302716",
    "https://openalex.org/W2250792670",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W3118036215",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3025939269",
    "https://openalex.org/W3165821503",
    "https://openalex.org/W2160802179",
    "https://openalex.org/W2970960342",
    "https://openalex.org/W3137010024",
    "https://openalex.org/W3134155512"
  ],
  "abstract": "Transfer learning with a unified Transformer framework (T5) that converts all language problems into a text-to-text format was recently proposed as a simple and effective transfer learning approach. Although a multilingual version of the T5 model (mT5) was also introduced, it is not clear how well it can fare on non-English tasks involving diverse data. To investigate this question, we apply mT5 on a language with a wide variety of dialects–Arabic. For evaluation, we introduce a novel benchmark for ARabic language GENeration (ARGEN), covering seven important tasks. For model comparison, we pre-train three powerful Arabic T5-style models and evaluate them on ARGEN. Although pre-trained with ~49 less data, our new models perform significantly better than mT5 on all ARGEN tasks (in 52 out of 59 test sets) and set several new SOTAs. Our models also establish new SOTA on the recently-proposed, large Arabic language understanding evaluation benchmark ARLUE (Abdul-Mageed et al., 2021). Our new models are publicly available. We also link to ARGEN datasets through our repository: https://github.com/UBC-NLP/araT5.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 628 - 647\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nAraT5: Text-to-Text Transformers for Arabic Language Generation\nEl Moatez Billah Nagoudi⋆ AbdelRahim Elmadany⋆ Muhammad Abdul-Mageed⋆\nDeep Learning and Natural Language Processing Group\nThe University of British Columbia\n{moatez.nagoudi,a.elmadany,muhammad.mageed}@ubc.ca\nAbstract\nTransfer learning with a uniﬁed Transformer frame-\nwork (T5) that converts all language problems into\na text-to-text format was recently proposed as a\nsimple and effective transfer learning approach.\nAlthough a multilingual version of the T5 model\n(mT5) was also introduced, it is not clear how well\nit can fare on non-English tasks involving diverse\ndata. To investigate this question, we apply mT5 on\na language with a wide variety of dialects–Arabic.\nFor evaluation, we introduce a novel benchmark\nfor ARabic language GENeration (ARGEN),\ncovering seven important tasks. For model\ncomparison, we pre-train three powerful Arabic\nT5-style models and evaluate them on ARGEN.\nAlthough pre-trained with ∼49% less data, our\nnew models perform signiﬁcantly better than mT5\non all ARGEN tasks (in 52 out of 59 test sets) and\nset several new SOTAs. Our models also establish\nnew SOTA on the recently-proposed, large Arabic\nlanguage understanding evaluation benchmark\nARLUE (Abdul-Mageed et al., 2021). Our models\nare publicly available. We also link to individual\nARGEN datasets through our public repository.1\n1 Introduction\nDue to their remarkable ability to transfer knowl-\nedge from unlabeled data to downstream tasks,\npre-trained Transformer-based language models\nhave emerged as important components of mod-\nern natural language processing (NLP) systems. In\nparticular, the uniﬁed framework that converts all\ntext-based language problems into a text-to-text for-\nmat presented through the T5 model (Raffel et al.,\n2019) is attractive. In addition to its simplicity,\nthis approach is effective since it allows knowledge\ntransfer from high-resource to low-resource tasks\n1https://github.com/UBC-NLP/araT5\n⋆ All authors contributed equally.\nFigure 1: Our AraT5 encoder-decoder model and prompt\nsamples from four investigated tasks, namely: title generation,\nmachine translation, question generation, and paraphrasing.\nwithout the need for changing model architecture.\nUnlike models such as BERT (Devlin et al., 2019),\nwhich are based on encoders only, the T5 model\nis an encoder-decoder that can naturally be em-\nployed for natural language generation. Although\nthe T5 model, originally pre-trained for English,\nwas recently extended to the multilingual setting as\nmT5 (Xue et al., 2020), it is not clear how suited\nit is to individual languages (and varieties of these\nlanguages). In addition, systematic issues have\nbeen discovered in multilingual corpora on which\nlanguage models have been trained (Kreutzer et al.,\n2021). In absence of comparisons with monolin-\ngual pre-trained language models that serve differ-\nent non-English contexts, it remains unknown how\nmultilingual models really fare against language-\nspeciﬁc models.\nIn this work, we offer the ﬁrst comparison of the\nmT5 model to similar encoder-decoder models ded-\nicated to Arabic. We choose Arabic as our context\ndue to its large set of diverse varieties as well as its\nwide use on social media. Our work aims at uncov-\nering the extent to which mT5 can serve Arabic’s\ndifferent varieties. Our work also meets an existing\nneed for pre-trained Transformer-based sequence-\nto-sequence models. In other words, while sev-\neral BERT-based models have been pre-trained for\nArabic (Antoun et al., 2020; Abdul-Mageed et al.,\n628\n2021; Inoue et al., 2021), no such attempts have\nbeen made to create sequence-to-sequence models\nthat we know of. Another motivation for our work\nis absence of an evaluation benchmark for Arabic\nlanguage generation tasks. Apart from machine\ntranslation where researchers are starting to pro-\npose benchmarks such as AraBench (Sajjad et al.,\n2020), there are no benchmarks that can be used\nto methodically measure Arabic natural language\ngeneration performance.\nOur main contributions are as follows: (1) We\nintroduce three powerful variants of the text-to-text\ntransformer (T5) model dedicated to Modern Stan-\ndard Arabic (MSA) and a diverse set of Arabic\ndialects. We include in our vocabulary 11 lan-\nguages other than Arabic (e.g., English, French,\nGerman, Russian), which also allows us to evaluate\nour models under zero-shot pre-training conditions\ninvolving these languages. (2) We propose a novel\nuniﬁed benchmark for ARabic natural language\nGEeneration (ARGEN) composed of seven tasks:\nmachine translation, code-switched text translation,\nsummarization, news title generation, question gen-\neration, paraphrasing, and transliteration. ARGEN\nis collected from a total of 19 datasets, including\n9 new datasets proposed in this work. (3) To show\nthe utility of our new models, we evaluate them on\nARGEN under both full and zero-shot pre-training\nconditions. Our models set new SOTA on the ma-\njority of datasets in all seven tasks. (4) Although\nthe main focus of our work is language generation,\nwe also show the effectiveness of our models on\nArabic language understanding by ﬁne-tuning our\nnew models on a large, recently proposed Arabic\nlanguage understanding benchmark. Again, our\nmodels establish new SOTA on the majority of lan-\nguage understanding tasks.\nThe rest of the paper is organized as follows:\nSection 2 describes our Arabic pre-tained models.\nIn Section 3, we introduce ARGEN, our new natu-\nral language generation benchmark. We evaluate\nour models on ARGEN in Section 4. Section 5 is\nan analysis and discussion of our results. In Sec-\ntion 6, we provide an overview of related work. We\nconclude in Section 7. We now introduce our new\npre-trained models.\n2 Our Models\n2.1 Pre-Training Data\nMSA Data. We use 70GB of MSA text\n(7.1B tokens) from the following sources:\nAraNews (Nagoudi et al., 2020), El-Khair El-Khair\n(2016), Gigaword,2, OSCAR (Suárez et al., 2019),\nOSIAN (Zeroual et al., 2019), Wikipedia Arabic,\nand Hindawi Books.3\nTwitter Data. We randomly sample 1.5B Arabic\ntweets (178GB) from a large in-house dataset of\n∼10B tweets. We use string matching to only\ninclude tweets with at least 3 Arabic words, regard-\nless whether the tweet has non-Arabic string or\nnot.\nOur combined MSA and Twitter data make up\n29B tokens, and hence is ∼49% less than Arabic\ntokens on which mT5 is pre-trained ( 57B Arabic\ntokens). More information about our pre-training\ndata is in Table 1.\nMSA Vs. Dialect Distribution. In order to ana-\nlyze MSA-dialect distribution in our Twitter data,\nwe run the binary (MSA-dialect) classiﬁer intro-\nduced in Abdul-Mageed et al. (2020b) on a random\nsample of 100M tweets. We ﬁnd the data to in-\nvolve 28.39% predicted dialect tweets and 71.61%\npredicted MSA. We also acquire country-level di-\nalect labels using an in-house strong classiﬁer on\nthe dialectal portion of the data (i.e., ∼28.39 mil-\nlions tweets), ﬁnding dialectal tweets to be truly\ngeographically diverse as shown in Figure 2.\nFigure 2: Country-level distribution in the dialectal por-\ntion of our data.\nNaturally-Occurring Code-Switching. Using\n1M random tweets from our data, we perform an\nanalysis of code-switching. For this, we employ\nsimple string matching to identify Arabic and run\nthe CLD3 language ID tool 4 on the non-Arabic\nstring sequences. We ﬁnd the data to have 4.14%\nnon-Arabic. These turn out to be almost always\nnatural code-switching involving many foreign lan-\nguages (e.g., English, French, Korean, etc.).\n2https://catalog.ldc.upenn.edu/LDC2009T30.\n3https://www.hindawi.org/books.\n4https://github.com/google/cld3\n629\nSource Size Tokens\nAraNews 8.6GB 847 .8M\nBooks 650 MB 72.5M\nEl-Khair 16GB 1.6B\nGigawords 10GB 1.1B\nOSIAN 2.8GB 292 .6M\nOSCAR-MSA 31GB 3.4B\nOSCAR-Egyptian 32MB 3.8M\nWiki 1.4GB 156 .5M\nMSA-Total 70GB 7.1B\nTwitter (1.5B) 178 GB 21.9B\nALL 248 GB 29.0B\nTable 1: The MSA and Twitter resources used to pre-\ntrain AraT5MSA, AraT5TW, and AraT5.\n2.2 Pre-Processing and Vocabulary\nWe remove diacritics and replace URLs and user\nmentions with <URL> and <USER>. We also clean\nthe data by removing HTML tags, elongation, and\nthe hash signs. Further, we reduce repetitive char-\nacters, emojis, and emoticons to one. To create\nour language model vocabulary, we use Sentence-\nPiece (Kudo, 2018) to encode text as WordPiece\ntokens (Sennrich et al., 2016) with 110K Word-\nPieces. To allow for further pre-training (and/or\nﬁne-tuning) on additional languages, we extract our\nvocabulary as follows: 70M MSA sentences, 200M\nArabic twitter data, 15M sentences from Wikipedia\nEnglish, and 5M sentences from the Wikipedia of\n10 other languages (Bulgarian, French, German,\nGreek, Italian, Portuguese, Russian, Spanish, Turk-\nish, Czech).5 In § 3.1.2, we describe parallel data\nfrom four of these languages on which we ﬁne-tune\nour models for X→Arabic MT. Our respective re-\nsults (reported in Table 4.2) demonstrate the utility\nof including foreign vocabulary in our models.\n2.3 AraT5\nModel Architecture. We leverage our unlabeled\nMSA and Twitter data described in § 2.1 to pre-\ntrain three models: AraT5MSA on MSA data,\nAraT5TW on twitter data, and AraT5 on both\nMSA and twitter data using the T5 Base encoder-\ndecoder architecture (Raffel et al., 2019). Each\nof the encoder and decoder components is similar\nin size and conﬁguration to BERTBase (Devlin et al.,\n2019), with 12 layers each with 12 attention heads,\nand 768 hidden units. In total, this results in a\nmodel with ∼220 million parameters.6 Objective.\nRaffel et al. (2019) pre-train T5 Base using a self-\n5The MSA and twitter data are extracted from our training\ndata presented in Section 2.1.\n6The output dimensionality is dff = 3, 072 and inner di-\nmensionality of dkv = 64.\nsupervised (denoising) objective. The main idea is\nto feed the model with masked (corrupted) versions\nof the original sentence, and train it to reconstruct\nthe original sequence. Inspired by BERT’s objec-\ntive (Devlin et al., 2019), the denoising objective\n(Raffel et al., 2019) works by randomly sampling\nand dropping out 15% of tokens in the input se-\nquence. All consecutive spans of dropped-out to-\nkens are then replaced by a single sentinel token.\nPre-Training. For all three of our pre-trained mod-\nels, we use a learning rate of 0.01, a batch size of\n128 sequences, and a maximum sequence length\nof 512, except for AraT5TW where the maximum\nsequence is 128.7 We pre-train each model for 1M\nsteps. Pre-training of each model took ∼80 days\non one Google Cloud TPU with8 cores (v3.8) from\nTensorFlow Research Cloud (TFRC).8 We now in-\ntroduce our language generation and understating\nbenchmarks.\n3 ARGEN\nIn order to evaluate our pre-trained language mod-\nels, we introduce our new benchmark for Ara-\nbic language generation evaluation ARGEN. It in-\ncludes 19 different datasets with 59 test splits and\ncovers seven tasks: machine translation (MT), code-\nswitched translation (CST), text summarization\n(TS), news title generation (NGT), question gen-\neration (QG), transliteration (TR), and paraphras-\ning (PPH). As such, ARGEN has wide-coverage\nboth in terms of the number of tasks and datasets.\nIt is also linguistically diverse as it covers both\nMSA and various Arabic dialects, in addition to\nArabizi (romanized Arabic in the TS task) and code-\nswitching (in the CST task). We now describe each\ncomponent of ARGEN.\n3.1 Machine Translation\nTo design the MT component of ARGEN,\nARGENMT, we consolidate 7 unique datasets with\n46 different test splits. The datasets come from\nboth MSA and Arabic dialects, and range between\n600-138K sentences (details in Table C.2 in Ap-\npendix). We introduce each dataset brieﬂy here.\n3.1.1 Arabic →English\n(1) United Nations Parallel Corpus. Ziemski\net al. (2016) introduce this parallel corpus of man-\n7We choose the same maximum sequence used in MAR-\nBERT (Abdul-Mageed et al., 2021), the most powerful model\ntrained on Arabic twitter to date (Farha and Magdy, 2021).\n8https://www.tensorﬂow.org/tfrc.\n630\nually translated UN documents covering the six\nofﬁcial UN languages (i.e., Arabic, Chinese, En-\nglish, French, Russian, and Spanish). The corpus\nconsists of development and test sets only, each of\nwhich comprise 4, 000 sentences that are one-to-\none alignments across all ofﬁcial languages.\n(2) IWSLT Corpus. Several Arabic-to-English\nparallel datasets were released during IWSLT eval-\nuation campaigns (Federico et al., 2012; Cettolo\net al., 2013, 2014, 2016). The datasets are mainly\nextracted from transcriptions of TED talks between\n2010 and 2016, and the QCRI Educational Domain\nCorpus (QED 2016) (Abdelali et al., 2014).\nAraBench Datasets. Sajjad et al. (2020) introduce\nAraBench, an evaluation suite for MSA and di-\nalectal Arabic to English MT consisting of ﬁve\npublicly available datasets: (3) ADPT: Arabic-\nDialect/English Parallel Text (Zbib et al., 2012),\n(4) MADAR: Multi-Arabic Dialect Applications\nand Resources dataset (Bouamor et al., 2018), (5)\nQAraC: Qatari-English speech corpus (Elmahdy\net al., 2014), and (6) Bible: The English Bible\ntranslated into MSA, Moroccan, and Tunisian Ara-\nbic dialects.9 For all these datasets, we use the\nsame splits as Sajjad et al. (2020) in our experi-\nments.\n3.1.2 X →Arabic\nTo investigate ability of our models to generate Ara-\nbic starting from foreign languages in our vocab-\nulary, we create an X→Arabic benchmark of four\nlanguages (English, French, German, and Russian)\nby extracting parallel data from OPUS (Tiedemann,\n2012). For each language, we pick 1M sentences\nfor training and 5K sentences for each of devel-\nopment and test splits. This gives us our seventh\nARGENMT dataset, which we call (7) OPUS-X-\nAra.\n3.2 Code-Switched Translation\nThere is rising interest in translating code-switched\ndata (Nagoudi et al., 2021). Our purpose here is\nto translate Arabic text involving code-switching\nfrom a foreign language into (i) that foreign lan-\nguage as well as into (ii) MSA. Hence we create\nARGENCST, our code-switched translation bench-\nmark component, using four sub-test sets. Two of\nthese are natural and two are synthetic, as follows:\nNatural Code-Switched Data. We create two\nhuman written (natural) code-switched parallel\n9The United Bible Societies https://www.bible.com.\ndatasets: (1) ALG-CST. This is collected from\nAlgerian Twitter and consists of code-switched\nArabic-French posts. We translate these manu-\nally into monolingual French. (2) JOR-CST. This\nis collected from Jordanian Twitter and consists\nof code-switched Arabic-English posts, which we\nmanually translate into monolingual English. Each\nof ALG-CST and JOR-CST comprises 300 tweets\n(total=600). Human translation is performed by\none native speaker from each dialect with semi-\nnative English/French ﬂuency.\nSynthetic Code-Switched Data. We use the multi-\nlingual sequence-to-sequence model mBART (Liu\net al., 2020) to create synthetic code-switched data\nfollowing Jawahar et al. (2021). We exploit the\nUN multi-parallel data (Ziemski et al., 2016) using\nthe Arabic-English and Arabic-French test splits\n(4, 000 sentences each, described in § 3.1) to gen-\nerate our two code-switched test sets (3) MSA-EN\nand (4) MSA-FR. In each case, we use mBART to\ntranslate ∼30% random Arabic n-grams into the\ntarget language (i.e., English or French).\n3.3 Text Summarization\nTo build our text summarization benchmark com-\nponent, ARGENTS, we use the following:\nEssex Arabic Summaries Corpus (EASC).\nEASC (El-Haj et al., 2010) contains 153 Arabic\nWikipedia and newspaper articles, each with 5\nhuman-generated extractive summaries (total=765\nsummaries). The summaries are crowdsourced via\nMechanical Turk.10\nWikiLingua. An abstractive summarization\ndataset in 18 languages, including Arabic\n(Faisal Ladhak and McKeown, 2020). It contains\narticles and their summaries from WikiHow.11 The\nArabic part includes summaries for 29.2K articles,\nwhich we split into 80% Train (23.4K), 10% Dev\n(2.9K), and 10% Test (2.9K).\n3.4 News Title Generation\nThe purpose of the news title generation (NTG)\ntask is to produce proper news article titles (Liang\net al., 2020). We introduce NTG as a new task\nfor Arabic language generation. Given an article,\na title generation model needs to output a short\ngrammatical sequence of words suited to the arti-\ncle content. For this, we introduce ARGENNTG, a\nnovel NTG dataset exploiting 120K articles along\n10http://www.mturk.com/\n11http://www.wikihow.com\n631\nwith their titles extracted from AraNews (Nagoudi\net al., 2020).12 We only include titles with at least\nthree words in this dataset. We split ARGENNTG\ndata into 80% Train ( 93.3K), 10% Dev ( 11.7K),\nand 10% Test (11.7K). Details about ARGENNTG\nare in Table C.1 (Appendix). A sample of a news\narticle from our Test split and example titles gener-\nated by our models are in Table D.5 (Appendix).\n3.5 Question Generation\nIn the question generation (QG) task, a question\nis produced for a passage (Gehrmann et al., 2021).\nGiven the absence of an Arabic QG dataset, we\ncreate a new Arabic QG dataset (ARGENQG) us-\ning a publicly available Arabic question answering\n(QA) resource. We follow Kriangchaivech and\nWangperawong (2019) who train a model to gen-\nerate simple questions relevant to passages and\nanswers extracted from SQuAD (Rajpurkar et al.,\n2016). In our case, we build ARGENQG by extract-\ning 96K (passage, answer, and question) triplets\nfrom (1) The Arabic QA dataset ARCD (Mozan-\nnar et al., 2019), and (2) three multi-lingual QA\ndatasets: XTREME benchmark (Hu et al., 2020),\nMLQA (Lewis et al., 2019), XQuAD (Artetxe et al.,\n2020), and TyDi QA (Artetxe et al., 2020).\n3.6 Paraphrasing\nThe main goal of this task is to produce for a given\nArabic sentence a paraphrase with the same mean-\ning. In order to build our paraphrasing benchmark\ncomponent (ARGENPPH), we use the following\nthree datasets:\nAraPara. We introduce AraPara, a new multi-\ndomain Arabic paraphrasing dataset we create us-\ning English-Arabic parallel OPUS data (Tiede-\nmann, 2012). AraPara covers several domains such\nas news, religion, politics, movies, and technol-\nogy. To create a high quality machine generated\nparaphrase dataset, we follow four careful steps\ninvolving human validation (more details are of-\nfered in Appendix C.1). AraPara consists of 122K\nparaphrase pairs. We only use AraPara for model\ndevelopment, and hence we split it into116K Train\nand 6K Dev.\nArabic SemEval Paraphrasing (ASEP). We also\ncreate a new Arabic paraphrasing dataset using\nthree existing Arabic semantic similarity datasets\nreleased during SemEval 2017 (Cer et al., 2017).\n12We ensure no overlap exists between ARGENTG and the\nAraNews data we use to pre-train our language models (de-\nscribed in § 2.3).\nThese are MSR-Paraphrase ( 510 pairs), MSR-\nVideo (368 pairs), and SMTeuroparl ( 203 pairs).\nThe pairs are labeled with a similarity score on\na scale from 0 to 5. For our purpose, we only\nkeep sentence pairs with a semantic similarity score\n≥3.5 which gives us 603 pairs. We merge and\nshufﬂe all three ASEP datasets for our use.\nArabic Paraphrasing Benchmark (APB). APB\nis created by Alian et al. (2019). It consists of\n1, 010 Arabic sentence pairs that are collected from\ndifferent Arabic books. Paraphrasing was per-\nformed manually using six transformation proce-\ndures (i.e., addition, deletion, expansion, permuta-\ntion, reduction, and replacement).\n3.7 Transliteration.\nTransliteration involves mapping a text written\nwith orthographic symbols in a given script into\nanother (Beesley, 1998). We use the BOLT\nEgyptian Arabic SMS/Chat and Transliteration\ndataset (Song et al., 2014), 13 a collection of\nnaturally-occurring chat and short messages (SMS)\nfrom Egyptian native speakers. The messages\n(sources) were natively written in either romanized\nArabizi or Egyptian Arabic orthography. The target\nis the Egyptian transliteration of these message.14\nFor experiments, we use the same split proposed\nby Shazal et al. (2020) (58.9K for Train and 5.4K\nfor Dev and Test each). We refer to this dataset as\nARGENTR.\n4 Evaluation on ARGEN\nBaselines and Procedure. For all tasks, we com-\npare our models to models ﬁne-tuned with mT5 us-\ning the same training data. In addition, for MT, we\ncompare to a vanilla sequence-to-sequence (S2S)\nTransformer (Vaswani et al., 2017) trained from\nscratch as implemented in Fairseq (Ott et al., 2019).\nFor all models and baselines, across all tasks, we\nidentify the best model on the respective Dev data\nand blind-test it on Test data. As a rule, we report\non both Dev and Test sets. All our Dev results are\nin Section C.2 in the Appendix.\n4.1 Machine Translation.\nWe train two S2S Transformers models on 2M\n(S2S2M) and 10M (S2S10M) MSA-English paral-\nlel sentences extracted from OPUS. We take these\n13https://catalog.ldc.upenn.edu/LDC2017T07\n14Some transliteration sequences involve code mixing be-\ntween Egyptian Arabic and English.\n632\nDataset Test Split S2S2M S2S10M mT5 AraT5Tw AraT5MSA AraT5 SOTA\nADPT† Lev 4.30 6 .20 8 .33 8.32 8.52 8.42 10.80\nEgy 5.21 8 .9 12 .57 11.25 12 .38 12.92 14.00\nBible I Tun. 4.12 4 .44 8 .08 5.86 8.52 7.94 7.00\nMor. 2.60 2 .80 7 .21 4.69 7.83 6.82 4.20\nMADAR I†\nEgy. 17.25 17 .71 24 .44 21.75 24.98 24.66 28.90\nQat. 15.98 17 .92 23 .72 22.23 24.00 23.92 27.60\nLeb. 12.15 10 .14 14 .61 12.25 14.92 14.18 17.00\nTun. 8.49 8 .57 10 .12 9.09 10.18 9.60 11.40\nMor. 11.07 11 .83 16 .61 12.37 16.99 16.82 14.70\nDIA\nMADAR II†\nEgy-Alex. 19.01 19 .74 29 .34 24.79 29.87 29.02 28.90\nEgy-Asw. 16.37 16 .95 23 .01 19.52 23.41 22.06 26.30\nSud-Kha. 24.97 25 .65 30 .87 28.13 31.39 30.65 36.70\nYem-San. 19.62 20 .35 24 .87 23.19 26.10 25.73 29.90\nOma-Mus. 29.12 30 .66 33 .74 32.15 34.62 34.18 39.50\nKSA-Riy. 26.14 26 .66 33 .54 30.81 33.86 33.59 40.70\nKSA-Jed. 16.08 17 .21 23.57 20.91 23 .45 23 .11 27.40\nIraq-Bag. 15.98 19 .09 22 .92 20.84 23 .24 22.52 28.30\nIraq-Bas. 16.46 17 .12 22.94 20.47 22 .61 22 .00 27.70\nIraq-Mos. 18.25 19 .14 23 .69 21.95 24.41 23.12 30.00\nPal-Jer. 15.18 16 .06 24 .61 20.91 24.95 24.45 27.00\nJor-Amm. 18.68 18 .86 26 .45 22.92 26.78 25.26 30.00\nJor-Salt. 17.14 17 .78 26 .04 23.05 26.56 26.05 29.60\nSyr-Dam. 13.63 14 .83 21 .93 18.55 22.54 21.80 25.90\nSyr-Alep. 14.16 15 .27 22 .39 19.55 22 .91 23.26 26.40\nAlg-Alg. 13.94 14 .24 16 .97 14.26 17.46 16.62 17.30\nLyb-Trip. 14.49 15 .44 20 .17 17.56 20.31 19.85 22.80\nLyb-Beng. 19.02 19 .32 25 .50 23.39 25 .46 25.54 28.40\nTun-Saf 7.89 8 .57 9 .26 8.15 9.94 9.60 10.80\nMor-Fes 15.09 15 .59 22 .81 17.33 23.33 21.97 20.90\nQAraC† Qatar 10.33 10 .47 11.84 11.11 11 .42 10 .57 11.90\nAverage DIA 14.75 15 .58 20 .66 18.28 21.02 20.49 23.49\nBible II† Test 1 10.44 10 .86 15 .58 13.04 16.38 15.71 17.00\nTest 2 5.55 6 .20 12 .14 9.27 12.53 11.64 12.80\nMSA\nMADAR I† MSA 10.33 10 .47 11.84 11.11 11 .42 10 .57 11.90\nIWSLT‡\nTED10 24.12 25 .13 28 .02 27.35 28.64 28.32 28.00\nTED11 23.96 25 .01 28 .89 28.03 29.93 27.34 32.80\nTED12 28.34 28 .98 33 .77 32.74 35.07 34.238 36.50\nTED13 24.19 25 .02 27 .12 27.52 27.95 27.52 37.40\nTED14 25.64 26 .48 29 .85 28.64 30.94 30.06 31.70\nTED15 27.68 28 .73 29 .39 28.2 30 .37 30.45 34.10\nTED16 25.71 25 .77 28 .39 27.03 29.37 29.18 31.80\nQED16 19.44 19 .90 21.09 18.55 20 .98 19 .11 28.10\nUN†† AR-EN 52.54 53 .12 52 .38 51.48 53 .29 52.96 56.90\nAverage MSA 23.54 24 .19 27 .03 25.43 27.77 26.98 30.63\nAverage All 19.14 19 .89 23 .84 21.85 24.39 23.74 27.06\nTable 2: English to Arabic results in BLEU using ARGENMT datasets. Baseline I : Sequence-to-Sequence Trans-\nformer models trained from scratch on 2M and 10M parallel sentences. Baseline II : mT5 (Xue et al., 2020).\nOur models : ArT5Tweet, ArT5MSA, ArT5. SOTA : † Sajjad et al. (2020) trained on ∼42M sentences, ‡ Durrani\net al. (2017) trained on ∼59M sentences, †† Junczys-Dowmunt et al. (2016) trained on ∼12M sentences.\n633\ntwo models as ourbaseline I. We also ﬁne-tune our\nthree models as well as mT5 on the same OPUS2M\nMSA-English parallel sentences used for baseline I.\nFine-tuned mT5 is our second baseline baseline II.\nArabic →English. Results of ARGENMT are re-\nported in Table 2. Results show that our models\nachieve best BLEU score in 37 out of the 42 tests\nsplits. AraT5 MSA acquires best results in 32 of\nthese test splits, outperforming all the baselines\n(S2S2M), (S2S10M), and mT5 with + 5.25, +4.99,\nand +0.45 BLEU points. These results are strik-\ning since our language models are pre-trained on\nArabic data only (although they include English vo-\ncabulary and marginal amounts of code-switching;\nsee § 2.1). In other words, even under this arguably\nzero-shot setting,15 the models perform very well.\nIn addition, our AraT5 model outperforms even\nthe S2S model trained with 5X more data. For\ncompleteness, we also provide the current SOTA\non each of our datasets. We do not compare our\nresults to SOTA since these are acquired by models\nﬁne-tuned on much larger datasets than ours. For\nexample, Sajjad et al. (2020) exploit ∼42M par-\nralel sentences to train their models. To limit GPU\nneeds during our experiments, especially given the\ntime-consuming ﬁne-tuning process typical of T5\nmodels, we do not ﬁne-tune the models on the full\namounts of available parallel data. However, in the\nfuture we plan to compare our models under the\nfull data setting.\nX →Arabic. Our language models are not pre-\ntrained on foreign data, but we include vocabulary\nfrom 11 foreign languages. Our X →Arabic exper-\niments here are hence zero-shot (from the perspec-\ntive of pre-training). Table 4.2 shows the results\nof AraT5MSA and mT5 on OPUS-X-Ara.16 We ob-\nserve that our model outperforms mT5 in the four X\n→Arabic sub-tasks with an average of +1.12 and\n+0.86 BLEU points on Dev and Test, respectively.\n4.2 Code-Switched Translation.\nFor this task, we test on the two natural code-\nswitched translation (CST) test sets that we manu-\nally created, ALG-FR→FR and JOR-EN→EN. We\nalso evaluate on our two synthetic CST datasets,\nMSA-EN and MSA-FR, one time with EN/FR as\ntarget (e.g., MSA-EN→EN) and another with MSA\nas target (e.g., MSA-EN →MSA). We ﬁne-tune\n15At best, this can be viewed as few-shot pre-training.\n16To limit GPU time, we ﬁne-tune only AraT5MSA model\non the X →Arabic direction since it performed best on\nArabic→English section above.\nour three pre-trained models as well as mT5 on\nthe OPUS-X-Ara segments involving English and\nFrench (each with 1M parallel sentences, described\nin § 3.1.2), in both directions. Since these MT\nmodels are only ﬁne-tuned on parallel monolin-\ngual data, we refer to these experiments as zero-\nshot. We test these models on both our natural and\nsynthetic code-switched data (described in § 3.2).\nWe report results in Table 3. Our models achieve\nbest results in one out of the two natural test sets\n(with + 4.36 BLEU points on ALG-FR) and all\nfour synthetic test sets (e.g., + 4.55 BLEU points\non MSA-EN→MSA). These results clearly show\nour models’ remarkable language generation abil-\nity especially in the Arabic direction.\nDataset Split mT5 AraT5 Tw AraT5MSA AraT5\nNaturalALG-FR→FR 23.83 28.19 26.27 26 .17\nJOR-EN→EN 23.06 21.60 21 .58 20 .45\nSynthetic\nMSA-FR→FR 12.76 10.57 13.78 13.25\nMSA-EN→EN 11.06 8 .99 11.53 11.42\nMSA-FR→MSA12.93 12.14 14.39 13.92\nMSA-EN→MSA19.82 18.43 23 .89 24.37\nTable 3: Performance of our models on ARGENCS.\n4.3 Text Summarization\nFor the two ARGENST datasets, we ﬁne-tune and\nidentify the best model on the Train and Dev\nsplits of WikiLingua (Faisal Ladhak and McKeown,\n2020) and test on all EASC and the Test of Wik-\niLingua. We report different ROUGE scores (Lin,\n2004) in Table 5. As the Table shows, AraT5Tw ac-\nquires best results on WikiLingua data, while mT5\noutperforms us on EASC (we hypothesize since\nEASC is older data that is likely part of the mC4\non which mT5 was pre-trained). On both datasets,\nwe establish new SOTA (both with our pre-trained\nmodels and mT5).\n4.4 News Title and Question Generation\nFor both tasks, we ﬁne-tune all our models on the\nTrain splits of ARGENNTG and ARGENQG, respec-\ntively. As Table 6 shows,all our models outperform\nmT5 on each of the two tasks. AraT5 MSA excels\nwith 20.61% BLEU on ARGENNTG and AraT5 is\nat 16.99% on ARGENQG.\n4.5 Paraphrasing and Transliteration\nFor the paraphrasing task, we ﬁne-tune and vali-\ndate on our new AraPra dataset and blind-test on\nboth APB and ASEP datasets (described in§ 3.6).\n634\nDataset DEV TEST\nmT5 AraT5MSA mT5 AraT5MSA\nEN→AR 13.60 15.72 17.80 18.58\nDE→AR 12.88 13.74 11.92 12.80\nFR→AR 17.52 17.96 18.61 18.99\nRU→AR 26.78 27.87 26.63 28.01\nAverage 17.70 18.82 18.74 19.60\nTable 4: Performance of MT models on OPUS-X-Ara.\nDataset Metric mT5 AraT5Tw AraT5MSA AraT5\nEASC\nRouge162.98 60.74 59 .54 54 .61\nRouge251.93 48.89 47 .37 43 .58\nRougeL62.98 60.73 59 .55 54 .55\nWikiLin.\nRouge1 71.63 74.61 72.64 73 .48\nRouge2 63.60 67.00 64.21 65 .09\nRougeL71.56 74.52 72.57 73 .37\nTable 5: Performance of summarization models on Test.\nWe consider mT5 as SOTA for WikiLin, and Alami\net al. (2021) (ROUGE1=59.17) for EASC.\nAs Table 6 shows, AraT5MSA is best on APB (17.52\nBLEU) and ASEP ( 19.38 BLEU). For translit-\neration, we ﬁne-tune our models on the Train\nsplit of ARGEN TR. As Table 6 shows, each of\nAraT5MSA and AraT5 outperform mT5. Notably,\nAraT5MSA is at 65.88 BLEU, outperforming previ-\nous SOTA (Shazal et al., 2020) by7.1 points.\nDataset mT5 AraT5 Tw AraT5MSA AraT5\nARGENNTG 19.49 20 .00 20.61 20.51\nARGENQG 15.29 12 .06 14 .18 16.99\nARGENTR 60.81 59 .55 65.88 62.51\nARGENPPHI 19.32 18 .17 19.38 19.03\nARGENPPHII 19.25 17 .34 19.43 18.42\nTable 6: Performance of our models on title, question\ngeneration, transliteration, and paraphrasing tasks in\nBLEU. ARGENPPH I and II : results on ASEP and\nAPB paraphrase datasets, respectively. We consider\nmT5 as SOTA for NTG, QG, and PPH ARGENNTG,\nARGENQG, and ARGENPPH. For ARGENTR, SOTA\nis Shazal et al. (2020) (BLEU=65.88).\n4.6 Evaluation on Arabic NLU\nWe also evaluate our new pre-trained models on\nthe recently proposed Arabic language understand-\ning and evaluation benchmark, ARLUE (Abdul-\nMageed et al., 2021) that involves six cluster tasks\n(i.e., sentiment analysis, social meaning, topic\nclassiﬁcation, dialect identiﬁcation, named entity\nrecognition, and question answering). Our mod-\nels establish new SOTA on the benchmark with an\nARLUE score of 77.52 vs. the previous SOTA of\n76.53, reported by ARLUE authors. We provide\nresults of this set of experiments in Appendix B.\n5 Analysis and Discussion\n5.1 Multilingual vs. Dedicated Models.\nOur results conﬁrm the utility of dedicated lan-\nguage models as compared to multilingual models\nsuch as mT5 (101+ languages). Our AraT5 model\noutperforms mT5, even though it is pre-trained\nwith 49% less data (see § 2.1). One reason might\nbe that massively multilingual models are more\nprone to suffering from capacity issues. Data qual-\nity is another challenge for multilingual models.\nAs pointed out earlier, Kreutzer et al. (2021) ﬁnd\nsystematic issues with data representing several\nlanguages (including Arabic) in the mC4 dataset\non which mT5 is pre-trained. We perform a data\nquality study conﬁrming the ﬁndings of Kreutzer\net al. (2021). We also ﬁnd Arabic mC4 data to be\nless geographically diverse than our Twitter pre-\ntraining data (described in § 2.1). Our mC4 data\nstudy is in Appendix A.\nCode-Switching. We also study code-switching\nin both our Twitter dataset and the Arabic part of\nmC4. We ﬁnd that while our Twitter data involves\nnatural code-switching (∼4% of sequences), code-\nswitching in Arabic mC4 is very rare. This explains\nthe strong performance of our AraT5 Tw model\non the natural code-switched translation data on\nFrench. We conjecture that mT5 good performance\non English code-switched data is due to it being\npre-trained on very large amounts of English rather\nthan natural code-switching.\n5.2 Effect of Sample Length on MT.\nWe were inquisitive how MT models ﬁne-tuning\nour pre-trained language models compare to mT5\nunder different length conditions. For this, we\n(1) merge all MSA and dialectal Test datasets in\nour Arabic→English experiments to form a single\ndataset that we then (2) split into three bins/Test\nsets based on sentence length as shown in Table D.1.\nAs the Table shows, our AraT5 MSA outperform\nmT5 in all but one condition (where our model\nacquires marginally less performance). We also\nperformed similar evaluation on the merged Dev\nsets of all MSA and dialectal Arabic MT datasets\nin the Arabic→English direction. We do not show\nrelated results here, but we note our AraT5 MSA\noutperforms mT5 on all conditions.\n635\n(1) Source:J’aime une vidéo Episode 1 -\u0010è\tQK\n\tQªË@ ú\n\u0010æJ.\u001c\n\t\u001d4 :ALG-FR\nTarget:FR :J’ aime une vidéo Episode 1 - ma chère belle-mère 4\nmT5 J’ aime unev-Chèrenièce4.\nAraT5Tw J’aime une vidéo Episode 1 - ma chèretante4.\nAraT5MSA J’aime une vidéo 1 - Ma chèresœur4.\nAraT5 J’aime une vidéo 1 - Ma chèrebébé\n(2) Source:\u0010é\u0010®J\n\u0010®k \r\u001dAK. Zú\næ\u0011 XAëðcomfort zoneÈ@ ú\n\t¯ ÕËAªË@\u0010éÊ¢\u001d. :JOR-EN\nTarget:EN :The world champion in the comfort zone and this is really miserable\nmT5 the worldworldchampion in comfort zone, and that’s really abadthing.\nAraT5Tw the worldheroin comfort zone and it’s really amiserablething.\nAraT5MSAworld champion in comfort zone, and that’s really abadthing.\nAraT5 the world’sthe world’sheroin the comfort zone, and it’s a reallybadthing.\nTable 7: CS sentences with their English/French trans-\nlations using our Models and mT5. Data samples are\nextracted from the Dev datasets. Green refers to good\ntranslation. Red refers to problematic translation.\n5.3 Qualitative Analysis.\nWe also perform qualitative analyses of the outputs\nof several of our models, including as to length\nof MT source data (Appendix D). In particular,\nour analyses are for the following tasks: machine\ntranslation, code-switched translation, paraphras-\ning, transliteration, and news title generation. MT\nModel. Table D.2 (Appendix) shows three exam-\nples of Arabic→English MT models. Sentence (1)\nis in MSA source, sentence (2) is in Levantine Ara-\nbic source, and sentence (3) is in Egyptian source.\nIn all three examples, one or more of our models\ngenerate(s) more ﬂuent translations than mT5. This\nincludes ability of our models to translate dialectal\nsentences where mT5 seems to struggle (e.g., mT5\nis not able to translate the equivalents of “drive\"\nfrom Egyptian Arabic).\nCode-Switched Translation Model. Table 7\nshows two code-switched examples from\nARGENCS. Sentence (1) is Algerian dialect at\nsource translated into French, while sentence (2) is\nJordanian dialect translated into English. In both\ncases, our models not only handle the dialects but\nalso their use in code-switched contexts better than\nmT5.\nParaphrasing, Transliteration, and Title Gen-\neration. Each of Tables D.3, D.4, and D.5 (Ap-\npendix D) shows two output samples from our\nparaphrasing, transliteration, and title generation\nmodels, respectively. In each case, the samples\nare high-quality, informative, and ﬂuent. Our para-\nphrase samples also tightly capture the meaning of\nthe source sentences.\n6 Related Work\nMultilingual LMs. mBERT is the multilingual\nversion of BERT (Devlin et al., 2019), which is an\nencoder model with bidirectional representations\nfrom Transformers trained with a denoising ob-\njective. mBERT is trained on Wikipedia for 104\nlanguages, including Arabic. XLM-R (Conneau\net al., 2020) is also a Transformer-based multilin-\ngual masked language model pre-trained on more\nthan 2TB of CommonCrawl (CC) data in 100 lan-\nguages, including Arabic (2.9B tokens). XLM-R\nmodel uses the same masking objective as BERT,\nbut not the next sentence prediction. mT5 (Xue\net al., 2020) is the multilingual version of Text-\nto-Text Transfer Transformer model (T5) (Raffel\net al., 2019). T5 is an encoder-decoder Transformer\nsimilar in conﬁguration and size to a BERT Base.\nIt is trained on mC4, which is ∼26.76TB for 101\nlanguages generated from 71 CC dumps.\nArabic LMs. AraBERT (Antoun et al., 2020) is\nan Arabic pre-trained language model based on the\nBERTBase architecture with 24GB of MSA data.\nARBERT and MARBERT (Abdul-Mageed et al.,\n2021) are two BERT-based models, with the ﬁrst\nfocused on MSA (61GB) and the second on both\nMSA and dialects (128GB). MARBERT achieves\nSOTA on most Arabic NLU tasks. QARiB (Abde-\nlali et al., 2021) is similarly a BERT-based model\ncovering both MSA and dialects. CamelBERT (In-\noue et al., 2021) is also a BERT-based model pre-\ntrained with MSA, dialectal, and classical Arabic.\n7 Conclusion\nWe introduced three powerful Arabic-speciﬁc text-\nto-text Transformer models trained on large MSA\nand/or Arabic dialectal data. We also introduced\nARGEN, a uniﬁed benchmark for Arabic Natu-\nral Language generation evaluation composed of\nseven tasks collected from a total of 19 datasets.\nOur models outperform mT5 on all ARGEN tasks\n(52 out of 59 test sets, i.e., 88.14%). This is true\neven for MT involving four foreign languages from\nwhich the models have seen marginal or no pre-\ntraining data (i.e., zero- and few-shot pre-training).\nOur models also set new SOTA on the large Ara-\nbic language understanding evaluation benchmark\nARLUE. Our models involve vocabulary from11\nlanguages other than Arabic, and hence can easily\nbe further pre-trained/ﬁne-tuned in these languages.\nOur models are publicly available, and ARGEN\ndatasets are accessible from our repository.\n636\nAcknowledgements\nWe gratefully acknowledge support from the Nat-\nural Sciences and Engineering Research Council\nof Canada (NSERC; RGPIN-2018-04267), the So-\ncial Sciences and Humanities Research Council\nof Canada (SSHRC; 435-2018-0576; 895-2020-\n1004), Canadian Foundation for Innovation (CFI;\n37771), Compute Canada (CC), 17, UBC ARC-\nSockeye,18 and Advanced Micro Devices, Inc.\n(AMD). We thank the Google TFRC program for\nproviding us with free TPU access. 19 Any opin-\nions, conclusions or recommendations expressed in\nthis material are those of the author(s) and do not\nnecessarily reﬂect the views of NSERC, SSHRC,\nCFI, CC, ARC-Sockeye, AMD, or Google. We\nthank Bashar Talafha for help with code-switching\ndata preparation.\nEthics Statement\nEnergy efﬁciency. Our models, similar to many\ndeep learning language models, take signiﬁcant\npre-training time and are not energy efﬁcient. We\nacknowledge this important issue and believe work\non creating energy efﬁcient models should receive\nscholarly attention.\nData. Our pre-training datasets are collected from\nthe public domain and cover diverse communities.\nAs we have demonstrated, our resulting models\nare better equipped to power applications involving\nseveral varieties of Arabic as well as code-switched\nlanguage use involving Arabic. From this perspec-\ntive, we hope they add to ongoing efforts in the\ncommunity to design models that are fairer and\nmore representative.\nARGEN Benchmark Release. We design AR-\nGEN using both existing datasets and new datasets\nthat we create for this work. In our accompanying\nGitHub repository, we link to all existing publicly\navailable components of the benchmark with stan-\ndard splits from source as well as components that\ncan be acquired from data organizations. In ad-\ndition, we released all the new datasets we have\ndeveloped. While we have prioritized standardiz-\ning evaluation on as many uniﬁed and consolidated\ndatasets and tasks as possible, we also report per-\nformance on individual test sets so as to enable the\ncommunity to replicate our work even on particular\nparts or tasks of ARGEN if they so wish.\n17https://www.computecanada.ca\n18https://arc.ubc.ca/ubc-arc-sockeye\n19https://sites.research.google/trc/about/\nAraT5 Models Release. All our pre-trained mod-\nels are publicly available for non-malicious use.\nWe acknowledge our models may still be misused\nin real world. However, we hope the models will\nbe deployed in domains such as education, disaster\nmanagement, health, recreation, travel, etc. in so-\ncially beneﬁcial ways. These meaningful potential\nuse cases are behind our decision to release the\nmodels.\nReferences\nMourad Abbas, Kamel Smaïli, and Daoud Berkani.\n2011. Evaluation of topic identiﬁcation methods on\narabic corpora. JDIM, 9(5):185–192.\nAhmed Abdelali, Francisco Guzman, Hassan Sajjad,\nand Stephan V ogel. 2014. The amara corpus: Build-\ning parallel language resources for the educational\ndomain. In LREC, volume 14, pages 1044–1054.\nAhmed Abdelali, Sabit Hassan, Hamdy Mubarak, Ka-\nreem Darwish, and Younes Samih. 2021. Pre-\ntraining bert on arabic tweets: Practical considera-\ntions. arXiv preprint arXiv:2102.10684.\nAhmed Abdelali, Hamdy Mubarak, Younes Samih,\nSabit Hassan, and Kareem Darwish. 2020. Arabic\nDialect Identiﬁcation in the Wild. arXiv preprint\narXiv:2005.06557.\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2021. ARBERT\n& MARBERT: Deep Bidirectional Transformers for\nArabic. In Proceedings of the ACL-IJCNLP 2021\nMain Conference . Association for Computational\nLinguistics.\nMuhammad Abdul-Mageed, Chiyu Zhang, Houda\nBouamor, and Nizar Habash. 2020a. NADI 2020:\nThe First Nuanced Arabic Dialect Identiﬁcation\nShared Task. In Proceedings of the Fourth Arabic\nNatural Language Processing Workshop.\nMuhammad Abdul-Mageed, Chiyu Zhang, Abdel-\nRahim Elmadany, and Lyle Ungar. 2020b. Toward\nmicro-dialect identiﬁcation in diaglossic and code-\nswitched environments. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5855–5876, On-\nline. Association for Computational Linguistics.\nNabil Alami, Mohammed Meknassi, Noureddine En-\nnahnahi, Yassine El Adlouni, and Ouafae Ammor.\n2021. Unsupervised neural networks for automatic\narabic text summarization using document cluster-\ning and topic modeling. Expert Systems with Appli-\ncations, 172:114652.\nMarwah Alian, Arafat Awajan, Ahmad Al-Hasan, and\nRaeda Akuzhia. 2019. Towards building arabic para-\nphrasing benchmark. In Proceedings of the Sec-\n637\nond International conference on Data Science E-\nlearning and Information Systems (DATA’ 2019) ,\npages 1–5.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nArabert: Transformer-based model for arabic lan-\nguage understanding. In Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637.\nKenneth Beesley. 1998. Romanization, transcription\nand transliteration. Retrieved June, 19:2006.\nHouda Bouamor, Nizar Habash, Mohammad Salameh,\nWajdi Zaghouani, Owen Rambow, Dana Abdul-\nrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani,\nAlexander Erdmann, et al. 2018. The madar arabic\ndialect corpus and lexicon. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC-2018).\nHouda Bouamor, Sabit Hassan, and Nizar Habash.\n2019. The madar shared task on arabic ﬁne-\ngrained dialect identiﬁcation. In Proceedings of the\nFourth Arabic Natural Language Processing Work-\nshop (WANLP19), Florence, Italy.\nRich Caruana. 1997. Multitask learning. Machine\nlearning, 28(1):41–75.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055.\nMauro Cettolo, Niehues Jan, Stüker Sebastian, Luisa\nBentivogli, Roldano Cattoni, and Marcello Federico.\n2016. The iwslt 2016 evaluation campaign. In In-\nternational Workshop on Spoken Language Transla-\ntion.\nMauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa\nBentivogli, and Marcello Federico. 2013. Report on\nthe 10th iwslt evaluation campaign. In Proceedings\nof the International Workshop on Spoken Language\nTranslation, Heidelberg, Germany.\nMauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa\nBentivogli, and Marcello Federico. 2014. Report\non the 11th iwslt evaluation campaign, iwslt 2014.\nIn Proceedings of the International Workshop on\nSpoken Language Translation, Hanoi, Vietnam, vol-\nume 57.\nAmina Chouigui, Oussama Ben Khiroun, and Bilel\nElayeb. 2017. Ant corpus: an arabic news text col-\nlection for textual classiﬁcation. In 2017 IEEE/ACS\n14th International Conference on Computer Systems\nand Applications (AICCSA), pages 135–142. IEEE.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186.\nNadir Durrani, Fahim Dalvi, Hassan Sajjad, and\nStephan V ogel. 2017. Qcri machine translation sys-\ntems for iwslt 16. arXiv preprint arXiv:1701.03924.\nMahmoud El-Haj, Udo Kruschwitz, and Chris Fox.\n2010. Using mechanical turk to create a corpus of\narabic summaries.\nIbrahim Abu El-Khair. 2016. 1.5 billion words arabic\ncorpus. arXiv preprint arXiv:1611.04033.\nMohamed Elmahdy, Mark Hasegawa-Johnson, and\nEiman Mustafawi. 2014. Development of a TV\nbroadcasts speech recognition system for qatari Ara-\nbic. In Proceedings of the Ninth International\nConference on Language Resources and Evalua-\ntion (LREC’14), pages 3057–3061, Reykjavik, Ice-\nland. European Language Resources Association\n(ELRA).\nClaire Cardie Faisal Ladhak, Esin Durmus and Kath-\nleen McKeown. 2020. Wikilingua: A new bench-\nmark dataset for multilingual abstractive summariza-\ntion. In Findings of EMNLP , 2020.\nIbrahim Abu Farha and Walid Magdy. 2020. From Ara-\nbic Sentiment Analysis to Sarcasm Detection: The\nArSarcasm Dataset. In Proceedings of the 4th Work-\nshop on Open-Source Arabic Corpora and Process-\ning Tools, with a Shared Task on Offensive Language\nDetection, pages 32–39.\nIbrahim Abu Farha and Walid Magdy. 2021. Bench-\nmarking transformer-based language models for ara-\nbic sentiment and sarcasm detection. In Proceed-\nings of the Sixth Arabic Natural Language Process-\ning Workshop, pages 21–31.\nMarcello Federico, Mauro Cettolo, Luisa Ben-\ntivogli, Paul Michael, and Stüker Sebastian. 2012.\nOverview of the iwslt 2012 evaluation campaign.\nIn IWSLT-International Workshop on Spoken Lan-\nguage Translation, pages 12–33.\nSebastian Gehrmann, Tosin Adewumi, Karmanya Ag-\ngarwal, Pawan Sasanka Ammanamanchi, Aremu\nAnuoluwapo, Antoine Bosselut, Khyathi Raghavi\n638\nChandu, Miruna Clinciu, Dipanjan Das, Kaustubh D\nDhole, et al. 2021. The gem benchmark: Natu-\nral language generation, its evaluation and metrics.\narXiv preprint arXiv:2102.01672.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning , volume 119 of\nProceedings of Machine Learning Research , pages\n4411–4421. PMLR.\nGo Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda\nBouamor, and Nizar Habash. 2021. The interplay\nof variant, size, and task type in Arabic pre-trained\nlanguage models. In Proceedings of the Sixth Ara-\nbic Natural Language Processing Workshop , Kyiv,\nUkraine (Online). Association for Computational\nLinguistics.\nGanesh Jawahar, El Moatez Billah Nagoudi, Muham-\nmad Abdul-Mageed, and Laks VS Lakshmanan.\n2021. Exploring text-to-text transformers for en-\nglish to hinglish machine translation with synthetic\ncode-mixing. NAACL 2021, page 36.\nMarcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu\nHoang. 2016. Is neural machine translation ready\nfor deployment? a case study on 30 translation di-\nrections. arXiv preprint arXiv:1610.01108.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wa-\nhab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Al-\nlahsera Tapo, Nishant Subramani, Artem Sokolov,\nClaytone Sikasote, Monang Setyawan, Supheak-\nmungkol Sarin, Sokhar Samb, Benoît Sagot, Clara\nRivera, Annette Rios, Isabel Papadimitriou, Sa-\nlomey Osei, Pedro Ortiz Suárez, Iroro Orife, Kelechi\nOgueji, Andre Niyongabo Rubungo, Toan Q.\nNguyen, Mathias Müller, André Müller, Sham-\nsuddeen Hassan Muhammad, Nanda Muhammad,\nAyanda Mnyakeni, Jamshidbek Mirzakhalov, Tapi-\nwanashe Matangira, Colin Leong, Nze Lawson,\nSneha Kudugunta, Yacine Jernite, Mathias Jenny,\nOrhan Firat, Bonaventure F. P. Dossou, Sakhile\nDlamini, Nisansa de Silva, Sakine Çabuk Ballı,\nStella Biderman, Alessia Battisti, Ahmed Baruwa,\nAnkur Bapna, Pallavi Baljekar, Israel Abebe Azime,\nAyodele Awokoya, Duygu Ataman, Orevaoghene\nAhia, Oghenefego Ahia, Sweta Agrawal, and Mofe-\ntoluwa Adeyemi. 2021. Quality at a glance: An\naudit of web-crawled multilingual datasets. arXiv\npreprint arXiv:2103.12028.\nKettip Kriangchaivech and Artit Wangperawong. 2019.\nQuestion generation by transformers. arXiv preprint\narXiv:1909.05017.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. arXiv preprint arXiv:1804.10959.\nPatrick Lewis, Barlas O ˘guz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2019. Mlqa: Eval-\nuating cross-lingual extractive question answering.\narXiv preprint arXiv:1910.07475.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-\nfei Guo, Weizhen Qi, Ming Gong, Linjun Shou,\nDaxin Jiang, Guihong Cao, et al. 2020. Xglue: A\nnew benchmark datasetfor cross-lingual pre-training,\nunderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008–6018.\nChin-Yew Lin. 2004. Rouge: A package for auto-\nmatic evaluation of summaries. Text Summarization\nBranches Out.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nMichael McCandless. 2010. Accuracy and perfor-\nmance of google’s compact language detector. Blog\npost.\nHussein Mozannar, Karl El Hajal, Elie Maamary, and\nHazem Hajj. 2019. Neural arabic question answer-\ning. arXiv preprint arXiv:1906.05394.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany,\nand Muhammad Abdul-Mageed. 2021. Investigat-\ning code-mixed Modern Standard Arabic-Egyptian\nto English machine translation. In Proceedings of\nthe Fifth Workshop on Computational Approaches\nto Linguistic Code-Switching, pages 56–64, Online.\nAssociation for Computational Linguistics.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany,\nMuhammad Abdul-Mageed, Tariq Alhindi, and\nHasan Cavusoglu. 2020. Machine generation and\ndetection of arabic manipulated and fake news. In\nProceedings of the Fifth Arabic Natural Language\nProcessing Workshop, pages 69–84.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensi-\nble toolkit for sequence modeling. arXiv preprint\narXiv:1904.01038.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nSebastian Ruder. 2017. An overview of multi-task\nlearning in deep neural networks. arXiv preprint\narXiv:1706.05098.\n639\nMotaz K Saad and Wesam M Ashour. 2010. Osac:\nOpen source arabic corpora. Osac: Open source\narabic corpora, 10.\nHassan Sajjad, Ahmed Abdelali, Nadir Durrani, and\nFahim Dalvi. 2020. Arabench: Benchmarking di-\nalectal arabic-english machine translation. In Pro-\nceedings of the 28th International Conference on\nComputational Linguistics, pages 5094–5107.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nAli Shazal, Aiza Usman, and Nizar Habash. 2020. A\nuniﬁed model for arabizi detection and translitera-\ntion using sequence-to-sequence models. In Pro-\nceedings of the Fifth Arabic Natural Language Pro-\ncessing Workshop, pages 167–177.\nZhiyi Song, Stephanie M Strassel, Haejoong Lee,\nKevin Walker, Jonathan Wright, Jennifer Garland,\nDana Fore, Brian Gainor, Preston Cabe, Thomas\nThomas, et al. 2014. Collecting natural sms and chat\nconversations in multiple languages: The bolt phase\n2 corpus. In LREC, pages 1699–1704. Citeseer.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous pipeline for process-\ning huge corpora on medium to low resource infras-\ntructures. In 7th Workshop on the Challenges in the\nManagement of Large Corpora (CMLC-7). Leibniz-\nInstitut für Deutsche Sprache.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. pages 2214–2218.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy\nGuo, Jax Law, Noah Constant, Gustavo Hernan-\ndez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan\nSung, et al. 2019. Multilingual universal sen-\ntence encoder for semantic retrieval. arXiv preprint\narXiv:1907.04307.\nOmar F Zaidan and Chris Callison-Burch. 2014. Ara-\nbic dialect identiﬁcation. Computational Linguis-\ntics, 40(1):171–202.\nRabih Zbib, Erika Malchiodi, Jacob Devlin, David Stal-\nlard, Spyros Matsoukas, Richard Schwartz, John\nMakhoul, Omar Zaidan, and Chris Callison-Burch.\n2012. Machine translation of arabic dialects. In Pro-\nceedings of the 2012 conference of the north ameri-\ncan chapter of the association for computational lin-\nguistics: Human language technologies , pages 49–\n59.\nImad Zeroual, Dirk Goldhahn, Thomas Eckart, and Ab-\ndelhak Lakhouaja. 2019. Osian: Open source inter-\nnational arabic news corpus-preparation and integra-\ntion into the clarin-infrastructure. In Proceedings\nof the Fourth Arabic Natural Language Processing\nWorkshop, pages 175–182.\nMichał Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The united nations parallel cor-\npus v1. 0. In Proceedings of the Tenth International\nConference on Language Resources and Evaluation\n(LREC’16), pages 3530–3534.\n640\nAppendices\nA A Study of Arabic mC4 Data Quality\nXue et al. (2020) train mT5 on the mC4 dataset.\nThey report 57B Arabic tokens (almost double our\ntoken size) from 53M webpages, making 1.66% of\nall mT5 data. For our analysis, we randomly sam-\nple 1M paragraphs from the Arabic part of mC4.\nWe use paragraphs rather than whole documents for\na more ﬁne-grained analysis that is more compara-\nble to our own data (especially in the case of Twit-\nter). We ﬁrst perform language identiﬁcation using\nCLD3 (McCandless, 2010) on the data. We ﬁnd\na sizable amount of the data (i.e., 13.59%) to be\nnon-Arabic (mostly English or French). We man-\nually inspect ∼100 random samples of the data\npredicted as non-Arabic. We ﬁnd these are mostly\neither non-linguistic content (e.g., java-script or\nHTML code) or non-Arabic text. The non-Arabic\ntext is sometimes foreign language advertising or\neven full translation of the Arabic text in some\ncases. In many cases, non-Arabic is also boilerplate\ntext such as that in web fora. Also, no samples of\nthe non-Arabic included real code-switching.\nWe also run an in-house MSA-dialect classiﬁer\non the same 1M data sample. The classiﬁer pre-\ndicts an overriding majority of the data (99.83%)\nas MSA. We again manually inspect ∼100 sam-\nples from the small fraction predicted as dialects\n(i.e., 0.17%). While we ﬁnd some of these to be ac-\ntual dialectal text (usually short belonging to either\nEgyptian or Saudi dialects) from web fora, in the\nmajority of cases the text is simply names of soap\noperas or advertisements. Our own pre-training\ndata in the case of Twitter, in comparison, involve\nmuch more dialectal content (28.39% as listed in\n§ 2.1).\nB Evaluation on Arabic NLU\nB.1 ARLUE Benchmark\nRecently, Abdul-Mageed et al. (2021) introduced\nARLUE, a natural language understanding bench-\nmark for Arabic. ARLUE is composed of 42 pub-\nlicly available datasets, making it the largest and\nmost diverse Arabic NLP benchmark. ARLUE\nis arranged into the six cluster tasks of sentiment\nanalysis (SA), social meaning (SM), topic classi-\nﬁcation (TC), dialect identiﬁcation (DI), named\nentity recognition (NER), and question answering\n(QA). We methodically evaluate each cluster task,\nultimately reporting a single ARLUE score follow-\ning Abdul-Mageed et al. (2021). Table B.1, shows\na summary of the ARLUE benchmark. We brieﬂy\ndescribe ARLUE tasks next.\nARLUESenti. To construct this task cluster Abdul-\nMageed et al. (2021) merged 17 MSA and DA\npublicly available datasets.\nARLUESM. ARLUESM refers to eight social mean-\ning datasets covering prediction of age, dangerous\nspeech, emotion, gender, hate speech, irony, offen-\nsive language, and sarcasm. used in this benchmark.\nWe will follow Abdul-Mageed et al. (2021) in not\nmerging the social meaning datasets, but rather re-\nport performance on each individual dataset as well\nas average performance across all tasks as part of\nan overall ARLUE score.\nARLUETopic. This benchmark component is a con-\ncatenation 20 of three topic classiﬁcation datasets:\nArabic News Text (ANT) (Chouigui et al., 2017),\nKhaleej (Abbas et al., 2011), and OSAC (Saad and\nAshour, 2010).\nARLUEDia. Five datasets are used for dialect clas-\nsiﬁcation. These are AOC Zaidan and Callison-\nBurch (2014), ArSarcasm Dia (Farha and Magdy,\n2020), MADAR (sub-task 2) (Bouamor et al.,\n2019), NADI-2020 (Abdul-Mageed et al., 2020a),\nand QADI (Abdelali et al., 2020).\nARLUEDia involve three categories, namely,\nARLUEDia-B for MSA-dialect classiﬁcation ( bi-\nnary). ARLUEDia-R, and ARLUEDia-C for the\nregion and country level classiﬁcation into four\nclasses (region), and 21 classes (country) respec-\ntively.\nARLUEQA. Four Arabic and multilingual QA\ndatasets are concatenated to build ARLUE QA:\nARCD (Mozannar et al., 2019) MLQA (Lewis\net al., 2019), XQuAD (Artetxe et al., 2020), and\nTyDi QA (Artetxe et al., 2020).21\nB.2 ARLUE Evaluation\nBaselines. For comparison, we ﬁne-tune a number\nof models on the same training data as our new\nmodels. These include the multilingual sequence-\nto-sequence model mT5 (Xue et al., 2020), and\nthe powerful Arabic-speciﬁc BERT-based model\nMARBERT (Abdul-Mageed et al., 2021). We note\n20We note that the classes were straightforwardly merged\nwithout modifying any class labels.\n21All corresponding splits from the different QA datasets\nare merged.\n641\nthat MARBERT achieves the SOTA22 across the\nmajority of 6 cluster tasks of ARLUE, with the\nhighest ARLUE score.\nSettings and Evaluation. We evaluate our models\non the language understanding benchmark, AR-\nLUE, under two settings: (i) single task learning\nand (ii) multi-task learning. We present results\non all the task clusters included in ARLUE ex-\ncept for NER which is a token-level task that is\nnot straightforward with the text-to-text set up we\nadopt. Table B.2 shows our evaluation results using\nthe relevant metric for each task.\nAbdul-Mageed et al. (2021) introduced ARLUE\nscore, a metric used to score pre-trained language\nmodel performance on multiple datasets. AR-\nLUE score is a simply macro-average of the dif-\nferent scores across all task clusters, where each\ntask is weighted equally following (Wang et al.,\n2018). We compute the ARLUE score (i.e., overall\nmacro-average) for each of our three models (i.e.,\nAraT5MSA, AraT5Tw, and AraT5) and the baseline\n(mT5).\nDataset #Datasets Task TRAIN DEV TEST\nARLUESenti 17 SA 190.9K 6.5K 44.2K\nARLUESM 8 SM 1.51M 162.5K 166.1K\nARLUETopic 5 TC 47.5K 5.9K 5.9K\nARLUEDia-B 2 DI 94.9K 10.8K 12.9K\nARLUEDia-R 2 DI 38.5K 4.5K 5.3K\nARLUEDia-C 3 DI 711.9K 31.5K 52.1K\nARLUEQA‡ 4 QA 101.6K 517 7.45K\nTable B.1: ARLUE categories across the different data splits.\n‡ Number of question-answer pairs (Abdul-Mageed et al.,\n2021).\nDataset SOTA mT5 AraT5TweetAraT5MSA AraT5\nARLUESenti⋆ 93.30/94.00 92.46/93.50 92.79/93.5093.44/94.0093.30/94.00\nARLUESM† 81.60/76.34 80.26/73.59 80.41/75.0881.97/76.6081.09/75.99\nARLUETopic90.07/91.54 91.92/93.36 90.86/92.0892.32/ 93.3092.32/93.66\nARLUEDia-B88.47/87.87 86.48/85.72 87.72/87.0688.51/87.9088.01/87.41\nARLUEDia-R90.04/89.67 88.30/87.93 90.12/89.6591.17/90.80 91.13/90.87\nARLUEDia-C47.49/38.53 45.94/38.14 53.34/42.02 52.65/42.42 53.64/43.18\nARLUEQA‡ 40.47/62.0936.92/56.17 30.42/49.57 39.47/60.51 39.80/60.93\nAverage75.92/77.15 74.61/75.49 75.09/75.5677.08/77.93 77.04/78.01\nARLUEScore 76.53 75 .05 75 .33 77 .50 77.52\nTable B.2: Performance of our models on ARLUE\nTEST datasets (Acc / F 1). ⋆ Metric for ARLUE Senti\nis Acc/ F 1PN. ‡ Metric for ARLUE QA is Exact\nMatch (EM) / F 1.† ARLUESM results is the average\nscore across the social meaning tasks. SOTA: MAR-\nBERT (Abdul-Mageed et al., 2021).\nSingle Task. We ﬁne-tune our three models and\n22MARBERT outperform both multilingual encoder-only\nTransformers mBERT, XLM-RBase, XLM-RLarge, and Arabic-\nspeciﬁc BERT-based AraBERT (Antoun et al., 2020), AR-\nBERT (Abdul-Mageed et al., 2021).\nmT5 individually on each of the six tasks of AR-\nLUE. We typically (i.e., in all our experiments)\nidentify the best checkpoint for each model on\nthe development set, and report its performance\non both development and test data. As Table B.2\nshows, our AraT5 model achieves the highest AR-\nLUE score (77.52), followed by AraT5MSA (77.50)\nand AraT5TW (75.33). We note that all our models\noutperform mT5 and the MARBERT (SOTA) by\n∼+2.74 and ∼+1 ARLUE score points, respec-\ntively.\nDataset S/M mT5 AraT5Tw AraT5MSA AraT5\nARLUEDia-B S 86.48/85.72 87.72/87.06 88.51/87.9088.01/87.41\nM 86.30/85.54 87.77/87.20 87.93/87.36 88.02/87.40\nARLUEDia-R S 88.30/87.93 90.12/89.65 91.17/90.80 91.13/90.87\nM 89.01/88.15 91.53/91.17 91.42/91.15 91.51/91.24\nARLUEDia-C S 45.94/38.14 53.34/42.02 52.65/42.42 53.64/43.18\nM 45.86 / 38.12 53.42 / 40.86 53.34 / 43.0353.70/43.37\nTable B.3: Performance of our models on ARLUE Di-\nalects Test datasets on single and multi tasks setting\n(Acc / F 1). We copied single tasks results from Ta-\nble B.2 in this table for comparison.\nDataset S/M mT5 AraT5Tw AraT5MSA AraT5\nAge S 60.86/61.05 62.29/62.48 63.26/63.41 63.50/63.66\nM 61.37/61.47 63.92/64.10 63.84/38.41 63.82/63.93\nDangerousS 81.75/64.52 77.68/63.52 82.50/66.93 75.41/62.41\nM 79.03/66.4684.92/68.73 84.46/71.62 77.53/66.53\nEmotionS 72.90/71.34 73.65/72.19 74.92/73.30 76.51/75.24\nM 70.88/68.87 72.79/71.24 74.39/73.08 74.28/72.57\nGender S 72.05/71.83 72.27/72.06 73.83/73.56 73.38/73.24\nM 72.72/72.42 74.58/74.39 74.33/74.23 74.65/74.52\nHate S 95.70/78.96 96.45/81.75 96.95/84.88 96.55/83.33\nM 95.75/79.29 97.00/82.73 96.40/82.07 96.15/80.39\nIrony S 82.61/82.40 82.48/82.25 83.23/83.05 82.98/82.80\nM 80.99/80.78 82.86/82.65 82.86/82.66 82.36/82.21\nOffensiveS 91.35/85.93 94.40/90.96 94.15/91.10 93.80/90.11\nM 90.30/85.15 93.70/90.41 94.10/90.83 94.05/90.85\nSarcasmS 84.83/72.66 84.08/75.42 86.92/76.53 86.59/77.13\nM 84.64/74.06 85.55/75.25 86.26/77.06 86.26/76.63\nARLUESM S 80.26/73.59 80.41/75.08 81.97/76.60 81.09/75.99\nM 79.46/73.56 81.92/76.19 82.08/73.75 81.14/75.95\nTable B.4: Performance of our models on ARLUE so-\ncial meaning (SM) Test datasets on single- and multi-\ntasks setting (Acc / F1). S: Single Task. M:Multi-task.\nMultitask. We also investigate multitask learning\n(Caruana, 1997; Ruder, 2017) with our AraT5 mod-\nels. This approach consists of training the model on\nmultiple tasks simultaneously (i.e., the model and\nits parameters are shared across all tasks) in order\nto eventually improve performance on each indi-\nvidual task. In our case, we ﬁne-tune our models\non many tasks at the same time using: (i) The three\ndialect datasets: ARLUE Dia-B, ARLUEDia-R, and\nARLUEDia-C and (ii) the social meaning datasets\n642\nof ARLUESM. Table B.3 and Table B.4 show the\nresults of multi-task experiments for dialect set-\ntings and social meaning, respectively. Our results\nshow that multi-task training outperforms single\ntask models in the majority of the dialects experi-\nments (n=7 out of 9 experiments, 77.78% of the\ntasks) and half of the social meaning tasks (n=18\nout of 36 experiments, 50% of the tasks). These\nresults are promising, and hence we plan to fur-\nther investigate multi-task learning with our new\nmodels in the future.\nC ARGEN\nC.1 Arabic Paraphrase Data\nAraPara. is a new multi-domain Arabic paraphras-\ning dataset we create using English-Arabic parallel\nOPUS data (Tiedemann, 2012). To ensure high-\nquality, we follow four careful steps: (1) We pick\n1 million English-Arabic parallel sentences from\nOPUS (Tiedemann, 2012) covering the different\ndomains. (2) We translate the English sentences\nusing a high-quality in-house English→Arabic MT\nmodel. (3) We run the multi-lingual semantic simi-\nlarity model from Yang et al. (2019) on the Arabic\nmachine translated sentences and the human trans-\nlation (i.e., original Arabic sentences from OPUS),\nkeeping only sentences with an arbitrary semantic\nsimilarity score between 0.70 and 0.99. This al-\nlows us to ﬁlter out identical sentence pairs (i.e.,\nsimilarity score = 1) and those that are not good\ntranslations (i.e., those with a semantic similarity\nscore < 0.70). (4) In order to maximize syntactic\nand lexical diversity of the pairs of paraphrased sen-\ntences, we perform an analysis based on word over-\nlap between the semantically similar pair sentences\n(i.e., the output of the previous step). We then\nperform a manual analysis of the data, identify-\ning sentences with unigram token overlap between\n35% and 70% as sufﬁciently distinct paraphrase\npairs. This gives us 122K paraphrase pairs. We\nsplit these sentence pairs into 116K for training\nand 6K for validation.\nC.2 Evaluation on DEV\nIn this section we describe the ARGENMT datasets\nsplits and report the evaluation results in valida-\ntion datasets. Details about ARGENNTG are in Ta-\nble C.1 and ARGENMT datasets splits are shown in\nTable C.2. Moreover, The evaluation on validation\ndatasets for ARGENTS are described in Table C.3\nand C.4, respectively. Finally, Table C.5 shows\nSplit Article/Title Avg article len Avg title len\nTRAIN 93.3K 256.46 10 .06\nDEV 11.7K 253.11 10 .03\nTEST 11.7K 260.32 10 .03\nTotal 116.6K 256.63 10 .04\nTable C.1: Main characteristics of ARGEN NTG data splits.\nFor each split, we provide the number of article-title pairs and\nthe average length of the articles and titles.\nthe validation results of ARGENNTG, ARGENQG,\nARGENTR, and ARGENPHP datasets.\nD Qualitative Analysis of Models\nIn this section, we explore ability of our models\nto generate MSA and dialectal Arabic under vari-\nous conditions. We now overview various types of\nanalyses in this regard. While samples presented\nhere are handpicked, we note that they are mostly\nrepresentative of outputs from our models since we\nmainly chose them to demonstrate different linguis-\ntic attributes that we believed would be relevant to\nthe analysis.\nEffect of Sample Length on MT.We were inquis-\nitive how MT models ﬁne-tuning our pre-trained\nlanguage models compare to mT5 under different\nlength conditions. For this, we (1) merge all MSA\nand dialectal Test datasets in our Arabic→English\nexperiments to form a single dataset that we then\n(2) split into three bins/Test sets based on sentence\nlength as shown in Table D.1. As the Table shows,\nour AraT5MSA outperform mT5 in all but one con-\ndition (where our model acquires marginally less\nperformance). We also performed similar evalu-\nation on the merged Dev sets of all MSA and di-\nalectal Arabic MT datasets in the Arabic→English\ndirection. We do not show related results here, but\nwe note our AraT5 MSA outperforms mT5 on all\nconditions.\nMT Model Output. Table D.2 shows three exam-\nples of Arabic→English MT models. Sentence (1)\nis in MSA source, sentence (2) is in Levantine Ara-\nbic source, and sentence (3) is in Egyptian source.\nIn all three examples, on or more of our models\ngenerate(s) more ﬂuent translations than mT5. This\nincludes ability of our models to translate dialectal\nsentences where mT5 seems to struggle (e.g., mT5\nis not able to translate the equivalents of “drive\"\nfrom Egyptian Arabic).\nCode-Switched Translation Model Output. Ta-\nble 7 shows two code-switched examples from\nARGENCS. Sentence (1) is Algerian dialect at\nsource translated into French, while sentence (2)\n643\nVarieties Dataset Region Country-Level City-Level DEV TEST\nADPT Zbib et al. (2012) Levantine - - - 138K\nNile Egypt - - 38K\nBible I Maghrebi Tunisia - - 600\nMorocco - - 600\nDIA\nMADAR I Bouamor et al. (2018)\nNile\nEgypt Cairo - 6.5k\nEgypt Alexandria - 2k\nEgypt Aswan - 2k\nSudan Khartoum - 2k\nGulf\nQatar Doha - 6.5k\nYemen Sana’a - 2k\nOman Muscat - 2k\nKSA Riyadh - 2k\nJedd Muscat - 2k\nIraq Baghdad - 2k\nIraq Basra - 2k\nIraq Mosu - 2k\nLeventian\nLebanon Beirut - 6.5k\nPalestine Jerusalem - 2k\nJordan Amman - 2k\nJordan Salt. - 2k\nSyria damascus - 2k\nSyria Alep - 2k\nMaghrebi\nAlgeria Alger - 2k\nLybia Trip - 2k\nLybia Beng - 2k\nTunisia Tunis - 6.5k\nTunisia Safax - 2k\nMorocco Fes - 6.5k\nMorocco Rabat - 2k\nMSA\nBible II - - - - 600\n- - - - 600\nMADAR II Bouamor et al. (2018) - - - - 6.5k\nIWSLT TED15 Cettolo et al. (2016) - - - - 1.1k\nIWSLT TED16 / Cettolo et al. (2016) - - - - 1.1k\nIWSLT QED16 (Cettolo et al., 2016) - - - - 550\nUN Ziemski et al. (2016) - - - 4k 4k\nOPUS-X-Ara - - - 5k 5k\nTable C.2: Arabic to English datasets included in ARGEN MT. MADAR I: corpus consists of 2k sentences (Test)\nof 21 city-level dialects each. MADAR II: 12k sentences (5.5k for Dev, and 6.5k for Test sets) each of ﬁve other\ncity-level dialects and MSA. Bible I: 600 sentences each as Dev and Test sets for Moroccan, Tunisian, and MSA.\nBible II: Two Dev and Test splits (600 sentences each) are used for Bible MSA.\n644\nDataset Test Split S2S 2M S2S10M mT5 AraT5 Tw AraT5MSA AraT5 SOTA\nADPT† Lev 4.90 7 .50 10 .12 10.53 9.33 9 .53 11.00\nEgy 5.04 9 .21 11 .63 10 .68 11 .33 11.87 13.40\nBible I† Tun. 4.44 4 .80 6 .98 4 .63 7.48 6.50 7.20\nMor. 3.22 3 .47 7.65 5.98 8 .25 7 .83 4.10\nDA MADAR I†\nEgy. 17.1 17 .71 24 .07 21 .68 24.75 24.29 27.1\nQat. 16.52 17 .92 23 .45 22 .32 23.98 23.58 28.10\nLeb. 9.61 12 .93 18 .19 16 .06 18.64 16.82 21.80\nTun. 9.06 9 .30 10 .62 9 .23 10.97 10.25 12.10\nMor. 8.46 8 .40 11 .83 8 .39 12.09 11.26 10.00\nQAraC† − 10.31 10 .46 11 .87 10 .73 11.30 10.64 11.70\nMSA\nBible II† Test 1 11.43 11 .33 15 .68 13 .13 16.43 15.89 16.60\nTest 2 5.88 6 .41 12 .76 9 .69 13.53 11.96 12.9\nMADAR I† MSA 40.75 41 .84 39 .11 38 .06 39.92 39.25 45.8\nIWSLT‡ QED16 28.39 29 .04 29 .18 28 .59 30.19 29.97 −\nUN†† Ar-En 51.54 51 .97 50 .84 50 .14 52.11 51.54 −\nAverage 14.67 15 .66 18 .50 16 .94 18.90 18.31 17.06\nTable C.3: ARGEN MT datasets on Dev splits. S2S: Sequence-to-sequence Transformer models trained from\nscratch without use of a language model. SOTA: †(Sajjad et al., 2020), ‡(Durrani et al., 2017), ††(Junczys-\nDowmunt et al., 2016).\nDataset Metric mT5 AraT5Tweet AraT5MSA AraT5\nWikiLin.\nRouge171.03 74.20 72.64 73 .87\nRouge262.87 66.37 64.24 65 .76\nRougeL70.99 74.14 72.55 73 .79\nTable C.4: Performance of our models on document\nsummarization Dev splits.\nDataset mT5 AraT5 Tweet AraT5MSA AraT5\nARGENNTG 19.22 19 .38 20.19 20.01\nARGENQG 13.95 11 .25 12 .96 15.36\nARGENTR 64.81 62 .95 69.30 65.54\nARGENPHP 30.70 31 .54 33.15 32.36\nTable C.5: Performance of our models on title, question\ngeneration, transliteration, and paraphrasing DEV split\nbased on Bleu score.\nJordanian dialect translated into English. In both\ncases, our models not only handle the dialects but\nalso their use in code-switched contexts better than\nmT5.\nParaphrasing, Transliteration, and Title Gen-\neration Output. Tables D.3, D.4, and D.5 each\nshows two output samples from our paraphrasing,\ntransliteration, and title generation models, respec-\ntively. In each case, the samples are high-quality,\ninformative, and ﬂuent. Our paraphrase samples\nalso tightly capture the meaning of the source sen-\ntences.\nDataset mT5 AraT5Tweet AraT5MSA AraT5\nAll Length\nMSA 28.38 27 .03 29.16 28.65\nDA 20.19 17 .73 20.54 20.10\nAll 21.14 18 .83 21.55 21.09\nSequence length<10\nMSA 35.73 35 .50 36.96 36.44\nDA 20.81 18 .73 21.29 20.68\nAll 21.70 19 .75 22.23 21.65\n20≤Sequence length≤10\nMSA 26.18 24 .31 26.90 26.24\nDA 19.74 16 .30 19.78 19.56\nAll 21.03 17 .94 21.22 20.91\n20<Sequence length\nMSA 19.50 16.91 19.28 19.45\nDA 13.51 11 .52 13.69 13.44\nAll 15.20 13 .05 15.26 15.13\nTable D.1: Sequence length based results on\nARGENMT Test datasets.\n645\n(1) Source:é\tKñÒÊK\nð hAK\nQË@ ú\n\t¯ I. J\nËA\rB@ @ñ\t\u001cK\n ÕË \táK\n\tYË@ ½\rJËð\r@ \tá\u001e\nK. \u0011\u001c\nªË@\u0010éQ\t¯ ú\n\t¯ AJ\n\t¯@Qk.ñ\tJ\u0011KB\r @ \u0011HAm\u001a'.\r@ i. ëAJ.Ó Yg\r@ð Q\t®ÊË\u0010èQ\u001e\nJ.ºË@ ©\u0010JÖÏ@ Yg\r@ \tà\r@ \tàñ\t¯Qª\u0010K Éë:MSA\n\u0010èQÖÏ@ \u0010HA\u0010KAJ.\tJË@ \u0010@Pð\r@ ú\n\t¯ é\tKñ\u0010¯ð\tY\u0010JK\nð PA¢Ó\rB@ Aî\u0010DÊ\u0010® ú\n\u0010æË@ PAm.k\rB@ ú\n\t¯\nTarget: EN:Do you know that one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those\nwho have not forgotten the old ways, who still feel their past in the wind, touch it in stones polished by rain, taste it in the bitter leaves of plants.\nmT5 you know, oneof the greatenjoymentsof travel and one of the pleasure ofsstatisticsresearch is theopportunityto live among those who\nhave not forgotten old methods, who still feel their past inwind, touch the rain-saving stonesand taste it in the snail of plants.\nAraT5Tw\nyou know, oneof the big pleasures of travel and one of thephysicalresearchapproaches is a livingchanceamong those who have not\nforgettedold methods, who still feel their past in the wind, touch it inthe stones that rainedand taste it in thefreshplant leaves .\nAraT5MSA\nDo you know thatone of the greatpleasuresof travel and one of the joys ofethnographicresearch is theopportunityto live among those who have\nnot forgotten the ancient methods, who still feel their past in the wind, touch it inrain-puriﬁedstones and taste it in the bitter leaves of plants?\nAraT5 you know, oneof the greatbeneﬁtsof travel and one of thephysiologyresearch is the opportunity to live among those who have not\nforgotten the old methods, who still feel their past in the wind, they feel their past in thestones that are reﬁned by rain, and they taste itin the leaf.\n(2) Source: ?\u0010é¢\u001d\nQ\tmÌ'Aê«\u0010èPA\u0011@\r \táîD\nÊ« ¡j\u0010JK. . ø\nñ\u0011ÊË ñÊg ð\u0010K\n@P Ñª¢ÓúÎ« \u0011\u0010\u001c\t®Ô« :LEV\nTarget: EN:I’m looking for a nice, quiet grill-type restaurant. would you point them out on this map?\nmT5 You ﬁnda nice and sweetcookingrestaurant with a map sign?\nAraT5Tw a snack ona nice and sweet sweat restaurantsnack, you put on them a map sign?\nAraT5MSA You’relookingat a nice and sweetsnackrestaurantwith a sign on the map?\nAraT5 looking for a nice and sweet restaurantto eat ,put a sign on themforthe map?\n(3) Source . \tá\u001e\nÒJ\nË@ ¨PA\u0011Ë@ I. \tJk. úÎ« \u0010ñ\u001d\n Yg@ñË@\tà@ \t­Ê\u0010J\tm× Cª\t¯ èX:EGY\nTarget: EN:It’s really different driving on the right side of the street.\nmT5 that’sreally different that onewalkson the right side of the street.\nAraT5Tw that’sreally different that onedrivesbythe right side of the street.\nAraT5MSA That’sreally different that onerunson the right side of the street.\nAraT5 That’sreally different that onedriveson the right side of the street.\nTable D.2: MSA and DIA sentences with their English translations using our Models and mT5. Data samples are\nextracted from the Dev datasets. Green refers to good translation. Red refers to problematic translation.\n646\n(1) Source Arabic sent: \u0010H@YëAªÖÏ@ è \tYë iJ\n \tñ\u0010K ð ¡J\n\u001c.\u0010K , Bð \r@\nTarget Paraphrase: . \u0010H@YëAªÖÏ@ è \tYë iJ\n \tñ\u0010K ð ¡J\n\u001c.\u0010K I. m.\u001a'\n , \u0010éK\n@YJ.Ë@ ú\n\t¯ .\nmT5 . \u0010H@YëAªÖÏ@ iJ\n \tñ\u0010Kð ¡J\n\u001c.\u0010K , Bð\r@\nAraT5Tw . AêjJ\n \tñ\u0010Kð \u0010H@YëAªÖÏ@ ¡J\n\u001c.\u0010K , Bð \r@\nAraT5MSA . \u0010H@YëAªÖÏ@ è \tYë iJ\n \tñ\u0010K ð ¡J\n\u001c.\u0010K , Bð\r@\nAraT5 . AêjJ\n \tñ\u0010Kð \u0010H@YëAªÖÏ@ ¡J\n\u001c.\u0010K Õç \u0010' , Bð \r@\n(2) Source Arabic sent: . j. \u0010JË@ \u0010éÒî\u0010DK. é\u0010J\tK@X@\r \u0010IÖ\u0010ß AÓ @ \tX@\r Ð@Y«B\r @ \u0010éK. ñ\u0010®« ék. @ñK\n Y\u0010¯\nTarget Paraphrase: . Ð@Y«B\r @ \u0010éK. ñ\u0010®ªË ø\n Xñ\u0010K \tà\r@ ðYªË@ \u0010èY«AÓð j. \u0010JË@ Ñî \u0010DË \táºÖß \nmT5 . m.\u001a\u0010' \u0010éÖß \nQk. ¼A\tJë \tà\r@ ú«X@ @ \tX@\r Ð@Y«B\r @ \u0010éK. ñ\u0010®« ék. @ñK\n \tà\r@ \táºÖß \nð\nAraT5Tw . j. \u0010JËAK. \táK\nX\r@ @ \tX@\r Ð@Y«B\r AK. éJ\nÊ« Õºm\u001a'\n \tà\r@ \táºÖß \nAraT5MSA . j. \u0010JË@ \u0010éÒî\u0010DK. \táK\nX\r@ @ \tX@\r Ð@Y«B\r AK. I. \u0010¯AªK\n Y\u0010¯ð\nAraT5 . j. \u0010JËAK. \táK\nX\r@ @ \tX@\r Ð@Y«B\r AK. éJ\nÊ« ÕºmÌ'@ \tPñm.\u001a'\nð\nTable D.3: Paraphrasing examples extracted from Dev data splits.\n(1) Source Arabizi: Tab matsha3’ali 5edmt el iphone men V odafone\nTarget Egy: V odafone \táÓ iphone È@ \u0010éÓY \tg úÎ \tª \u0011\u0010\u001d AÓ I . J\n£\nmT5 V odafone \táÓ \tàñ \t®K\n\rB@ \u0010éÓY \tg ú\nÎ \tª \u0011\u0010\u001d AÓ I . £\nAraT5 Tw V odafone \táÓ \tàñ \t®K\n\rB@ \u0010éÓY \tg ú\nÎ \tª \u0011\u0010\u001d AÓ I . J\n£\nAraT5 MSA\n\tàñ \t¯@Xñ \t¯ \táÓ \tàñ \t®K\n\rB@ \u0010éÓY \tg ú\nÎ \tª \u0011\u0010\u001d AÓ I . £\nAraT5 \tàñ \t¯@Xñ \t¯ \táÓ \tàñ \t®K\n\rB@ \u0010éÓY \tg ú\nÎ \tª \u0011\u0010\u001d AÓ I . £\n(2) Source Arabizi: 3amla eiih enty weih 2a5bar el 5otoba 3la 7eseek?\nTarget Egy: ? ½k úÎ« \u0010éK. ñ¢ \tmÌ'@ PAJ . \tk@ éK \n@\rð ú \n\u0010æ \tK@ éK \n@\r \u0010éÊÓA«\nmT5 ? ½K. Ak úÎ« \u0010éK. ñ¢ \tmÌ'@ PAJ . \tk@ éJ \n \t¯ ú\n\u0010æ \tK@ éK \n@\r \u0010éÊÓA«\nAraT5 Tw ? ½J.k úÎ« \u0010éK. ñ¢ \tmÌ'@ PAJ . \tk@ éK \n@\rð ú \n\u0010æ \tK@ éK \n@\r \u0010éÊÓA«\nAraT5 MSA ? ½K. Ak úÎ« \u0010éK. ñ¢ \tmÌ'@ PAJ . \tk@ éK \n@\rð ú \n\u0010æ \tK@ éK \n@\r \u0010éÊÓA«\nAraT5 ? ½J.m\u001a'. úÎ« \u0010éK. ñ¢ \tmÌ'@ PAJ . \tk@ éK \n@\rð ú \n\u0010æ \tK@ éK \n@\r \u0010éÊÓA«\nTable D.4: Transliteration examples extracted frm from Dev data splits.\n(1) Document:\nPAÒ\u0011J\u0010\u001cB@ QK\n \tPð \táÔgQË@YJ.« ú\n\tæ\tªË@YJ.« Q\u0011KYÓ Pñ\u0010J»X éA\rKQK. èQmÌ'@ \u0010@ñB@ð\u0010£A\tJÒÊË ú\n×ñ\u0010®Ë@ Êj. ÖÏ@ PY@ : ÐñJ\nË@ \tà@XñË@\u0010é\u0010J \tjÖÏ@ \u0010HAêm.Ì'@ P@Q\u0010®Ë@ ék. ðð QÔgB@ QjJ.Ë@ éK\nBñK. ÕæQ\u0010®Ó èQK\n \tQm.\u001a'. \u0010éK\nXA\u0010J\u0010¯B@ ÕËAªË@ I. Ê\u0010¯ é» Qå\u0011 ÉÔ« J\n \tkQ\u0010K ZA\tªËA\rK. @P@Q\u0010¯.... úÍAmÌ'@ ñJ\n\tKñK\n \táÓ 13 t\u001a'\nPA\u0010JK. Y\u0010®ª\tK@ ø\tYË@ é«AÒ\u0010Jk. @ ú\t¯ P@Q\u0010®Ë@ Êj. ÖÏ@ \tY\tm\u001a\u0010'@ \u0011IJ\nk è\tYJ\n\t®\tJ\u0010K \u0010èPðQå\t\u001d.\nGold Title: ÕËAªË@ I. Ê\u0010¯ \u0010é» Qå\u0011 ÉÔ« J\n \tkQ\u0010K ZA\u0010®ËAK. P@Q\u0010¯ PY@ ..\u0010èQmÌ'@ \u0010@ñ\rB@ ú\n×ñ\u0010®Ë@ Êj. ÖÏ@\nmT5: \u0010éK\nXA\u0010J\u0010¯B@ ÕËAªË@ I. Ê\u0010¯ \u0010é» Qå\u0011 ÉÔ« J\n \tkQ\u0010K ZA\tªËA\rK. P@Q\u0010¯\nAraT5Tweet: èQK\n \tQm.\u001a'. \u0010éK\nXA\u0010J\u0010¯B@ ÕËAªË@ I. Ê\u0010¯ \u0010é» Qå\u0011 ÉÔ« J\n \tkQ\u0010K ù\n \tªÊK\n PAÒ\u0011J\u0010\u001cB@ QK\n \tPð\nAraT5MSA: \u0010éK\nXA\u0010J\u0010¯B@ ÕËAªË@ I. Ê\u0010¯ \u0010é» Qå\u0011 J\n \tkQ\u0010K ZA\tªË@\r\nAraT5: \u0010éK\nXA\u0010J\u0010¯B@ ÕËAªË@ I. Ê\u0010¯ \u0010é» Qå\u0011 ÉÔ« J\n \tkQ\u0010K ZA\tªË@\r\n(2) Document:\n\u0010éJ\n¢\t®\tJË@ \u0010HAK.ñ\u0010®ªË@ \táÓ % 25 ñm\u001a\t' \táÓ ZA\t®«@\r úÎ« \u0010IÊk èXCK. \tà@\r ,\u0010éªÒm.Ì'@ ÐñJ\nË@ \tQ\u001e\nÖ\tßðX l\u001a\u0010'A\t¯ ú\n»Q\u0010\u001eË@ \u0010é\u0010¯A¢Ë@ QK\n \tPð ÈA\u0010¯\n..... \tàñK\n \tQ\t®Ê\u0010K \u0010é¢m× ©Ó \u0010éÊK.A\u0010®Ó ú\n\t¯ \tQ\u001e\nÖ\tßðX ÈA\u0010¯ð . AK\nñ\tJ ¡\t®\tJË@ \táÓ \tá£ \tá\u001e\nK\nCÓ 3 ñm\u001a\t' ÈXAªK\n AÖß. , \tà@QK\n@\r úÎ« \u0010èYj\u0010JÖÏ@ \u0010HAK\nBñË@ Aî\u0010D \tQ\t¯ ú\n\u0010æË@\nGold Title: \tà@QK\n@\r úÎ« \u0010éJ\n¢\t®\tJË@ \u0010HAK.ñ\u0010®ªË@ \táÓ % 25\u0010éJ.\t\u001cK. AJ\n» Q\u0010K ZA\t®«@\r : ú\n»Q\u0010K QK\n \tPð\nmT5: \tà@QK\n@\r úÎ« \u0010éJ\n¢\t®\tJË@ \u0010HAK.ñ\u0010®ªË@ \táÓ % 25 ù\n \t®ª\u0010K AJ\n» Q\u0010K\nAraT5Tweet: \tà@QK\n@\r úÎ« % 25\u0010éJ.\t\u001cK. \u0010éJ\n¢\t®\tJË@ \u0010HAK.ñ\u0010®ªË@ \táÓ ù\n \t®ª\u0010K AJ\n» Q\u0010K\nAraT5MSA: \tà@QK\n@\r úÎ« \u0010éJ\nºK\nQÓ\rB@ \u0010éJ\n¢\t®\tJË@ \u0010HAK.ñ\u0010®ªË@ \táÓ % 25\táÓ ZA\t®«@\r úÎ« Ém\u001a\u0010' AJ\n» Q\u0010K\nAraT5: \tà@QK\n@\r úÎ« \u0010éJ\nºK\nQÓ\rB@ \u0010HAK.ñ\u0010®ªË@ \táÓ % 25 ZA\t®«@\r úÎ« Ém\u001a\u0010' AJ\n» Q\u0010K\nTable D.5: Title generation samples from Dev set using our Models.\n647",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8223623037338257
    },
    {
      "name": "Transformer",
      "score": 0.7825050354003906
    },
    {
      "name": "Arabic",
      "score": 0.7153911590576172
    },
    {
      "name": "Natural language processing",
      "score": 0.6884305477142334
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6532688140869141
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6475555896759033
    },
    {
      "name": "Language model",
      "score": 0.642642080783844
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4879429042339325
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.47517579793930054
    },
    {
      "name": "Test set",
      "score": 0.4436289072036743
    },
    {
      "name": "Programming language",
      "score": 0.15514495968818665
    },
    {
      "name": "Linguistics",
      "score": 0.14091446995735168
    },
    {
      "name": "Engineering",
      "score": 0.08194026350975037
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141945490",
      "name": "University of British Columbia",
      "country": "CA"
    }
  ]
}