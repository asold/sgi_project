{
  "title": "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors",
  "url": "https://openalex.org/W3170470779",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3143894319",
      "name": "Zeyu Yun",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2185822796",
      "name": "Yubei Chen",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley",
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A3167108774",
      "name": "Bruno Olshausen",
      "affiliations": [
        "University of California, Berkeley",
        "Berkeley College"
      ]
    },
    {
      "id": "https://openalex.org/A2053214915",
      "name": "Yann LeCun",
      "affiliations": [
        "Meta (Israel)",
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2100556411",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2979463038",
    "https://openalex.org/W2963703618",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2140610559",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3132401450",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2963639656",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these 'black boxes' as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at: https://github.com/zeyuyun1/TransformerVis.",
  "full_text": "Proceedings of Deep Learning Inside Out (DeeLIO):\nThe 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,pages 1–10\nOnline, June 10, 2021. ©2021 Association for Computational Linguistics\n1\nTransformer visualization via dictionary learning:\ncontextualized embedding as a linear superposition of transformer factors\nZeyu Yun∗2 Yubei Chen∗ 1,2 Bruno A Olshausen2,4 Yann LeCun1,3\n1 Facebook AI Research\n2 Berkeley AI Research (BAIR), UC Berkeley\n3 New Y ork University\n4 Redwood Center for Theoretical Neuroscience, UC Berkeley\nAbstract\nTransformer networks have revolutionized\nNLP representation learning since they were\nintroduced. Though a great effort has been\nmade to explain the representation in trans-\nformers, it is widely recognized that our un-\nderstanding is not sufﬁcient. One important\nreason is that there lack enough visualiza-\ntion tools for detailed analysis. In this pa-\nper, we propose to use dictionary learning\nto open up these ‘black boxes’ as linear su-\nperpositions of transformer factors. Through\nvisualization, we demonstrate the hierarchi-\ncal semantic structures captured by the trans-\nformer factors, e.g., word-level polysemy dis-\nambiguation, sentence-level pattern formation,\nand long-range dependency. While some\nof these patterns conﬁrm the conventional\nprior linguistic knowledge, the rest are rela-\ntively unexpected, which may provide new in-\nsights. We hope this visualization tool can\nbring further knowledge and a better under-\nstanding of how transformer networks work.\nThe code is available athttps://github.\ncom/zeyuyun1/TransformerVis.\n1 Introduction\nThough the transformer networks (V aswani et al.,\n2017; Devlin et al., 2018) have achieved great suc-\ncess, our understanding of how they work is still\nfairly limited. This has triggered increasing efforts\nto visualize and analyze these “black boxes”. Be-\nsides a direct visualization of the attention weights,\nmost of the current efforts to interpret transformer\nmodels involve “probing tasks”. They are achieved\nby attaching a light-weighted auxiliary classiﬁer at\nthe output of the target transformer layer. Then\nonly the auxiliary classiﬁer is trained for well-\nknown NLP tasks like part-of-speech (POS) Tag-\nging, Named-entity recognition (NER) Tagging,\n∗ equal contribution. Correspondence to: Zeyu\nYu n <chobitstian@berkeley.edu>, Y ubei Chen\n<yubeic@{fb.com, berkeley.edu}>\nSyntactic Dependency, etc. Tenney et al. (2019)\nand Liu et al. (2019) show transformer models\nhave excellent performance in those probing tasks.\nThese results indicate that transformer models have\nlearned the language representation related to the\nprobing tasks. Though the probing tasks are great\ntools for interpreting language models, their lim-\nitation is explained in Rogers et al. (2020). We\nsummarize the limitation into three major points:\n• Most probing tasks, like POS and NER tag-\nging, are too simple. A model that performs\nwell in those probing tasks does not reﬂect the\nmodel’s true capacity.\n• Probing tasks can only verify whether a cer-\ntain prior structure is learned in a language\nmodel. They can not reveal the structures be-\nyond our prior knowledge.\n• It’s hard to locate where exactly the related\nlinguistic representation is learned in the trans-\nformer.\nEfforts are made to remove those limitations and\nmake probing tasks more diverse. For instance,\nHewitt and Manning (2019) proposes “structural\nprobe”, which is a much more intricate probing\ntask. Jiang et al.(2020) proposes to generate spe-\nciﬁc probing tasks automatically. Non-probing\nmethods are also explored to relieve the last two\nlimitations. For example, Reif et al. (2019) visu-\nalizes embedding from BERT using UMAP and\nshows that the embeddings of the same word un-\nder different contexts are separated into different\nclusters. Ethayarajh (2019) analyzes the similarity\nbetween embeddings of the same word in different\ncontexts. Both of these works show transformers\nprovide a context-speciﬁc representation.\nFaruqui et al.(2015); Arora et al.(2018); Zhang\net al. (2019) demonstrate how to use dictionary\nlearning to explain, improve, and visualize the un-\ncontextualized word embedding representations. In\n2\nthis work, we propose to use dictionary learning\nto alleviate the limitations of the other transformer\ninterpretation techniques. Our results show that dic-\ntionary learning provides a powerful visualization\ntool, leading to some surprising new knowledge.\n2 Method\nHypothesis: contextualized word embedding as\na sparse linear superposition of transformer\nfactors.\nIt is shown that word embedding vectors\ncan be factorized into a sparse linear combination\nof word factors (Arora et al., 2018; Zhang et al.,\n2019), which correspond to elementary semantic\nmeanings. An example is:\napple =0.09“dessert”+0 .11“organism”+0 .16\n“fruit”+0 .22“mobile&IT”+0 .42“other”.\nWe view the latent representation of words in a\ntransformer as contextualized word embedding.\nSimilarly, we hypothesize that a contextualized\nword embedding vector can also be factorized as\na sparse linear superposition of a set of elemen-\ntary elements, which we calltransformer factors.\nThe exact deﬁnition will be presented later in this\nsection.\nFigure 1: Building block (layer) of transformer\nDue to the skip connections in each of the trans-\nformer blocks, we hypothesize that the representa-\ntion in any layer would be a superposition of the hi-\nerarchical representations in all of the lower layers.\nAs a result, the output of a particular transformer\nblock would be the sum of all of the modiﬁcations\nalong the way. Indeed, we verify this intuition\nwith the experiments. Based on the above observa-\ntion, we propose to learn a single dictionary for the\ncontextualized word vectors from different layers’\noutput.\nTo learn a dictionary of transformer factors\nwith non-negative sparse coding.\nGiven a set of tokenized text sequences, we col-\nlect the contextualized embedding of every word\nusing a transformer model. We deﬁne the set of\nall word embedding vectors fromlth layer of trans-\nformer model as X(l). Furthermore, we collect\nthe embeddings across all layers into a single set\nX = X(1) ∪X(2) ∪···∪ X(L).\nBy our hypothesis, we assume each embedding\nvector x ∈ X is a sparse linear superposition of\ntransformer factors:\nx =Φ α+ϵ, s.t. α⪰ 0, (1)\nwhere Φ ∈ IRd×m is a dictionary matrix with\ncolumns Φ:,c , α ∈ IRm is a sparse vector of coef-\nﬁcients to be inferred andϵis a vector containing\nindependent Gaussian noise samples, which are as-\nsumed to be small relative tox. Typically m>d\nso that the representation is overcomplete. This\ninverse problem can be efﬁciently solved by FIST A\nalgorithm (Beck and Teboulle, 2009). The dictio-\nnary matrix Φcan be learned in an iterative fashion\nby using non-negative sparse coding, which we\nleave to the appendix sectionC. Each columnΦ:,c\nof Φis a transformer factorand its corresponding\nsparse coefﬁcient αc is its activation level.\nVisualization by top activation and LIME inter-\npretation. An important empirical method to visu-\nalize a feature in deep learning is to use the input\nsamples, which trigger the top activation of the fea-\nture (Zeiler and Fergus, 2014). We adopt this con-\nvention. As a starting point, we try to visualize each\nof the dimensions of a particular layer,X(l). Un-\nfortunately, the hidden dimensions of transformers\nare not semantically meaningful, which is similar\nto the uncontextualized word embeddings (Zhang\net al., 2019).\nInstead, we can try to visualize the transformer\nfactors. For a transformer factor Φ:,c and for a\nlayer-l, we denote the 1000 contextualized word\nvectors with the largest sparse coefﬁcientsα(l)\nc as\nX(l)\nc ⊂ X(l), which correspond to 1000 differ-\nent sequences. For example, Figure 3 shows the\ntop 5 words that activated transformer factor-17\nΦ:,17 at layer-0, layer-2, and layer-6 respectively.\nSince a contextualized word vector is generally af-\nfected by many tokens in the sequence, we can use\nLIME (Ribeiro et al., 2016) to assign a weight to\neach token in the sequence to identify their relative\n3\nimportance to αc. The detailed method is left to\nSection 3.\nTo determine low-, mid-, and high-level trans-\nformer factors with importance score. As we\nbuild a single dictionary for all of the transformer\nlayers, the semantic meaning of the transformer fac-\ntors has different levels. While some of the factors\nappear in lower layers and continue to be used in\nthe later stages, the rest of the factors may only be\nactivated in the higher layers of the transformer net-\nwork. A central question in representation learning\nis: “where does the network learn certain informa-\ntion?” To answer this question, we can compute\nan “importance score” for each transformer factor\nΦ:,c at layer-l as I(l)\nc . I(l)\nc is the average of the\nlargest 1000 sparse coefﬁcientsα(l)\nc ’s, which cor-\nrespond to X(l)\nc . We plot the importance scores\nfor each transformer factor as a curve is shown in\nFigure 2. We then use these importance score (IS)\ncurves to identify which layer a transformer factor\nemerges. Figure 2a shows an IS curve peak in the\nearlier layer. The corresponding transformer factor\nemerges in the earlier stage, which may capture\nlower-level semantic meanings. In contrast, Fig-\nure 2b shows a peak in the higher layers, which\nindicates the transformer factor emerges much later\nand may correspond to mid- or high-level seman-\ntic structures. More subtleties are involved when\ndistinguishing between mid-level and high-level\nfactors, which will be discussed later.\nAn important characteristic is that the IS curve\nfor each transformer factor is relatively smooth.\nThis indicates if a vital feature is learned in the\nbeginning layers, it won’t disappear in later stages.\nInstead, it will be carried all the way to the end\nwith gradually decayed weight since many more\nfeatures would join along the way. Similarly, ab-\nstract information learned in higher layers is slowly\ndeveloped from the early layers. Figure 3 and 5\nconﬁrm this idea, which will be explained in the\nnext section.\n3 Experiments and Discoveries\nWe use a 12-layer pre-trained BERT model (Pre;\nDevlin et al., 2018) and freeze the weights. Since\nwe learn a single dictionary of transformer factors\nfor all of the layers in the transformer, we show that\nthese transformer factors correspond to different\nlevels of semantic or syntactic patterns. The pat-\nterns can be roughly divided into three categories:\n(a)\n (b)\nFigure 2: Importance score (IS) across all layers for\ntwo different transformer factors. (a) This ﬁgure shows\na typical IS curve of a transformer factor correspond-\ning to low-level information. (b) This ﬁgure shows a\ntypical IS curve of a transformer factor corresponds to\nmid-level information.\nword-level disambiguation, sentence-level pattern\nformation, and long-range dependency. In the fol-\nlowing, we provide detailed visualization for each\npattern category. Due to the space limit, only a\nsmall amount of the factors are demonstrated in the\npaper. To alleviate the “cherry-picking” bias, we\nalso build a website for the interested readers to\nplay with these results.\nLow-level: word-level polysemy disambigua-\ntion. While the input embedding of a token con-\ntains polysemy, we ﬁnd transformer factors with\nearly IS curve peaks usually correspond to a spe-\nciﬁc word-level meaning. By visualizing the top\nactivation sequences, we can see how word-level\ndisambiguation is gradually developed in a trans-\nformer.\nWe show how the disambiguation effect devel-\nops progressively through each layer in Figure3.\nIn Figure 3, the top 5 activated words and their\ncontexts for transformer factor Φ:,30 in different\nlayers are listed. The top activated words in layer\n0 contain the word “left” varying senses, which is\nbeing mostly disambiguated in layer 2 albeit not\ncompletely. In layer 4, the word “left” is fully\ndisambiguated since the top-activated word con-\ntains only “left” with the word sense “leaving, exit-\ning.” We also show more examples of those types\nof transformer factors in Table1: for each trans-\nformer factor, we list out the top 3 activated words\nand their contexts in layer 4. As shown in the table,\nnearly all top-activated words are disambiguated\ninto a single sense.\nFurther, we can quantify the quality of the disam-\nbiguation ability of the transformer model. In the\nexample above, since the top 1000 activated words\n4\n(a) layer 0\n (b) layer 2\n (c) layer 6\nFigure 3: Visualization of a low-level transformer factor, Φ:,30 at different layers. (a), (b) and (c) are the top-\nactivated words and contexts forΦ:,30 in layer-0, 2 and 4 respectively. We can see that at layer-0, this transformer\nfactor corresponds to word vectors that encode the word “left” with different senses. In layer-2, a majority of the\ntop activated words “left” correspond to a single sense, \"leaving, exiting.\" In layer 4, all of the top-activated words\n“left” have corresponded to the same sense, \"leaving, exiting.\" Due to space limitations, we invite the readers to\nuse our website to see more of those disambiguation effects.\nTop 3 activated words and their contexts Explanation\nΦ:,2\n• that snare shot sounded like somebody’ d kicked open the door to your\nmind\".\n• i became very frustrated with that and ﬁnally made up mymind to start\ngetting back into things.\"\n• when evita asked for more time so she could make up hermind, the crowd\ndemanded,\" ¡ ahora, evita,<\n• Word “mind”\n• Noun\n• Deﬁnition: the element of a\nperson that enables them to be\naware of the world and their ex-\nperiences.\nΦ:,16\n•nington joined the ﬁve members xero and the band was renamed to linkin\npark.\n• times about his feelings about gordon, and the price family even sat away\nfrom park’ s supporters during the trial itself.\n• on 25 january 2010, the morning ofpark’ s 66th birthday, he was found\nhanged and unconscious in his\n• Word “park”\n• Noun\n• Deﬁnition: a common ﬁrst and\nlast name\nΦ:,30\n• saying that he has left the outsiders, kovu asks simba to let him join his\npride\n• eventually, all boycott’ s employeesleft, forcing him to run the estate without\nhelp.\n• the story concerned the attempts of a scientist to photograph the soul as it\nleft the body.\n• Word “left\"\n• V erb\n• Deﬁnition: leaving, exiting\nΦ:,33\n• forced to visit the sarajevo television station at night and to ﬁlm with as\nlittle light as possible to avoid the attention of snipers and bombers.\n• by the modest, cream@-@ colored attire in the airy,light@-@ ﬁlled clip.\n• the man asked her to help him carry the case to his car, alight@-@ brown\nvolkswagen beetle.\n• Word “light”\n• Noun\n• Deﬁnition: the natural agent\nthat stimulates sight and makes\nthings visible\nTable 1: Several examples of low-level transformer factors. Their top-activated words in layer 4 are markedblue,\nand the corresponding contexts are shown as examples for each transformer factor. As shown in the table, nearly\nall of the top-activated words are disambiguated into a single sense. Please note the last example ofΦ\n:,33 is a rare\nexception, the reader may check the appendix to see a more complete list. More examples, top-activated words\nand contexts are provided in Appendix.\nand contexts are “left” with only the word sense\n“leave, exiting”, we can assume “left” when used\nas a verb, triggers higher activation inΦ:,30 than\n“left” used as other sense of speech. We can verify\nthis hypothesis using a human-annotated corpus:\nBrown corpus (Francis and Kucera, 1979). In this\ncorpus, each word is annotated with its correspond-\ning part-of-speech. We collect all the sentences\ncontains the word “left” annotated as a verb in one\nset and sentences contains “left” annotated as other\npart-of-speech. As shown in Figure4a, in layer 0,\nthe average activation ofΦ:,30 for the word “left”\nmarked as a verb is no different from “left” as other\nsenses. However, at layer 2, “left” marked as a\nverb triggers a higher activation ofΦ:,30. In layer\n4, this difference further increases, indicating dis-\nambiguation develops progressively across layers.\nIn fact, we plot the activation of “left” marked as\nverb and the activation of other “left” in Figure4b.\nIn layer 4, they are nearly linearly separable by this\n5\n(a)\n (b)\nFigure 4: (a) Average activation ofΦ:,30 for word vector “left” across different layers. (b) Instead of averaging, we\nplot the activation of all “left” with different contexts in layer-0, 2, and 4. Random noise is added to the y-axis to\nprevent overplotting. The activation ofΦ:,30 for two different word senses of “left” is blended together in layer-0.\nThey disentangle to a great extent in layer-2and nearly separable in layer-4by this single dimension.\n(a) layer 4\n (b) layer 6\n (c) layer 8\nFigure 5: Visualization of a mid-level transformer factor. (a), (b), (c) are the top 5 activated words and contexts\nfor this transformer factor in layer-4, 6, and 8 respectively. Again, the position of the word vector is markedblue.\nPlease notice that sometimes only a part of a word is marked blue. This is due to that BERT uses word-piece\ntokenizer instead of whole word tokenizer. This transformer factor corresponds to the pattern of “consecutive\nadjective”. As shown in the ﬁgure, this feature starts to develop at layer-4and fully develops at layer-8.\nPrecision\n(%)\nRecall\n(%)\nF1 score\n(%)\nAverage perceptron POS\ntagger 92.7 95.5 94.1\nFinetuned BERT base\nmodel for POS task 97.5 95.2 96.3\nLogistic regression clas-\nsiﬁer with activation of\nΦ:,30 at layer 4\n97.2 95.8 96.5\nTable 2: Evaluation of binary POS tagging task: predict\nwhether or not “left” in a given context is a verb.\nsingle feature. Since each word “left” corresponds\nto an activation value, we can perform a logistic\nregression classiﬁcation to differentiate those two\ntypes of “left”. From the result shown in Figure4a,\nit is pretty fascinating to see that the disambigua-\ntion ability of justΦ:,30 is better than the other two\nclassiﬁers trained with supervised data. This result\nconﬁrms that disambiguation is indeed done in the\nearly part of pre-trained transformer model and we\nare able to detect it via dictionary learning.\nMid level: sentence-level pattern formation.We\nﬁnd most of the transformer factors, with an IS\ncurve peak after layer 6, capture mid-level or high-\nlevel semantic meanings. In particular, the mid-\nlevel ones correspond to semantic patterns like\nphrases and sentences pattern.\nWe ﬁrst show two detailed examples of mid-level\ntransformer factors. Figure5 shows a transformer\nfactor that detects the pattern of consecutive usage\nof adjectives. This pattern starts to emerge at layer\n4, develops at layer 6, and becomes quite reliable at\nlayer 8. Figure6 shows a transformer factor, which\ncorresponds to a pretty unexpected pattern: “unit\nexchange”, e.g., 56 inches (140 cm). Although this\nexact pattern only starts to appear at layer 8, the\nsub-structures that make this pattern, e.g., paren-\nthesis and numbers, appear to trigger this factor in\nlayers 4 and 6. Thus this transformer factor is also\n6\n(a) layer 4\n (b) layer 6\n (c) layer 8\nFigure 6: Another example of a mid-level transformer factor visualized at layer-4, 6, and 8. The pattern that cor-\nresponds to this transformer factor is “unit exchange”. Such a pattern is somewhat unexpected based on linguistic\nprior knowledge.\n2 example words and their contexts with high activation Patterns L4\n(%)\nL6\n(%)\nL8\n(%)\nL10\n(%)\nΦ:,13\n• the steel pipeline was about 20 ° f(- 7° c) degrees.\n• hand( 56 to 64 inches( 140 to 160cm)) war horse is that\nit was a\nUnit exchange with paren-\ntheses 0 0 64.5 95.5\nΦ:,42\n• he died at the hospice of lancaster county from heart\n• holly’ s drummer carl bunch suffered frostbite to his\ntoes( while aboard theailments on 23 june 2007.\nSomething unfortunate\nhappened 94.0 100 100 100\nΦ:,50\n• hurricane pack 1 was a revamped version of story mode;\n• in 1998, the categories were retitled best short form\nmusic video, and best\nDoing something again,\nor making something new\nagain\n74.5 100 100 100\nΦ:,86\n• he ﬁnished the 2005 –06 season with 21 appearances\nand seven goals.\n• of an offensive game, ﬁnishing off the 2001 –02 season\nwith 58 points in the 47 games\nConsecutive years, used\nin foodball season nam-\ning\n0 100 85.0 95.5\nΦ:,102\n• the most prominent of which was bishop abelmuzorewa’\ns united african national council\n• ralambo’ s father, andriamanelo, had established rules of\nsuccession by\nAfrican names 99.0 100 100 100\nΦ:,125\n• music writer jeff weiss of pitchfork describes the\" endur-\ning image\"\n• club reviewer erik adams wrote that the episode was a\nperfect mix\nDescribing someone in a\nparaphrasing style. Name,\nCareer\n15.5 99.0 100 98.5\nΦ:,184\n• the world wide fund for nature(wwf) announced in 2010\nthat a biodiversity study from\n• fm) was halted by the federal communications commis-\nsion( fcc) due to a complaint that the company buying\nInstitution with abbrevia-\ntion 0 15.5 39.0 63.0\nΦ:,193\n• 74, 22@,@ 500 vietnamese during 1979 – 92, over\n2@,@ 500 bosnian\n•, the russo@-@ turkish war of 1877– 88 and the ﬁrst\nbalkan war in 1913.\nTime span in years 97.0 95.5 96.5 95.5\nΦ:,195\n•s, hares, badgers, foxes,weasels, ground squirrels, mice,\nhamsters\n•-@ watching, boxing, chess, cycling,drama, languages,\ngeography, jazz and other music\nConsecutive of noun\n(Enumerating) 8.0 98.5 100 100\nΦ:,225\n• technologist at the united states marine hospital in key\nwest, ﬂorida who developed a morbid obsession for\n• 00°,11”, w, near smithvalley, nevada.\nPlaces in US, follow-\nings the convention “city,\nstate\"\n51.5 91.5 91.0 77.5\nTable 3: A list of typical mid-level transformer factors. The top-activation words and their context sequences for\neach transformer factor at layer-8are shown in the second column. We summarize the patterns of each transformer\nfactor in the third column. The last 4 columns are the percentage of the top 200 activated words and sequences that\ncontain the summarized patterns in layer-4,6,8, and 10 respectively.\ngradually developed through several layers.\nWhile some mid-level transformer factors verify\ncommon semantic or syntactic patterns, there are\nalso many surprising mid-level transformer factors.\nWe list a few in Table3 with quantitative analysis.\nFor each listed transformer factor, we analyze the\ntop 200 activating words and their contexts in each\nlayer. We record the percentage of those words and\ncontexts that correspond to the factors’ semantic\npattern in Table3. From the table, we see that large\n7\nAdversarial Text Explaination α35\n(o) album as \"full of exhilarating, ecstatic,thrilling, fun and\nsometimes downright silly songs\"\nThe original top-activated word and its\ncontext sentence for transformer factor\nΦ:,35 (not an adversarial text)\n9.5\n(a) album as \"full of delightful, lively,exciting, interesting\nand sometimes downright silly songs\"\nReplace the adjectives in sentence (o)\nwith different adjectives. 9.2\n(b) album as \"full of unfortunate, heartbroken,annoying, bor-\ning and sometimes downright silly songs\"\nReplace the adjectives in sentence (o)\nwith negative adjectives. 8.2\n(c) album as \"full of [UNK], [UNK],thrilling, [UNK] and\nsometimes downright silly songs\"\nMask the adjectives in sentence (o)\nwith unknown tokens. 5.3\n(d) album as \"full ofthrilling and sometimes downright silly\nsongs\"\nRemove the ﬁrst three adjectives in sen-\ntence (o). 7.8\n(e) album as \"full of natural, smooth, rock, electronic and\nsometimes downright silly songs\"\nReplace the adjectives in sentence (o)\nwith neutral adjectives. 6.2\n(f) each participant starts the battlewith one balloon. these\ncan be re@-@ inﬂated up to four Use a random sentence. 0.0\n(g) The book is described as \"innovative,beautiful and bril-\nliant\". It receive the highest opinion from James Wood\nWe create this sentence that contain the\npattern of consecutive adjective. 7.9\nTable 4: We construct adversarial texts similar but different to the pattern “Consecutive adjective”. The last column\nshows the activation ofΦ:,35,o r α(8)\n35 , w.r.t. the blue-marked word in layer 8.\npercentages of top-activated words and contexts\ndo corresponds to the pattern we describe. It also\nshows most of these mid-level patterns start to de-\nvelop at layer 4 or 6. More detailed examples are\nprovided in the appendix sectionF. Though it’s still\nmysterious why the transformer network develops\nrepresentations for these surprising patterns, we\nbelieve such a direct visualization can provide ad-\nditional insights, which complements the “probing\ntasks”.\nTo further conﬁrm a transformer factor does\ncorrespond to a speciﬁc pattern, we can use con-\nstructed example words and context to probe their\nactivation. In Table 4, we construct several text\nsequences that are similar to the patterns corre-\nsponding to a particular transformer factor but with\nsubtle differences. The result conﬁrms that the con-\ntext that strictly follows the pattern represented by\nthat transformer factor triggers a high activation.\nOn the other hand, the closer the adversarial exam-\nple to this pattern, the higher activation it receives\nat this transformer factor.\nHigh-level: long-range dependency. High-level\ntransformer factors correspond to those linguistic\npatterns that span an extended range in the text.\nSince the IS curves of mid-level and high-level\ntransformer factors are similar, it is difﬁcult to dis-\ntinguish those transformer factors based on their\nIS cures. Thus, we have to manually examine the\ntop-activation words and contexts for each trans-\nformer factor to differentiate between mid-level\nand high-level transformer factors. To ease the\nprocess, we choose to use the black-box interpreta-\ntion algorithm LIME (Ribeiro et al., 2016) to iden-\ntify the contribution of each token in a sequence.\nThere also exist interpretation tools that speciﬁcally\nleverage the transformer architecture (Chefer et al.,\n2021, 2020). In the future, one could adapt those\ninterpretation tools, which may potentially provide\nbetter visualization.\nGiven a sequences ∈ S, we can treatα(l)\nc,i, the\nactivation of Φ:,c in layer-l at location i, as a scalar\nfunction of s, f(l)\nc,i (s). Assume a sequence s trig-\ngers a high activationα(l)\nc,i, i.e. f(l)\nc,i (s) is large. We\nwant to know how much each token (or equivalently\neach position) in s contributes to f(l)\nc,i (s).T o d o\nso, we generated a sequence setS(s), where each\ns′ ∈S (s) is the same ass except for that several\nrandom positions ins′are masked by [‘UNK’] (the\nunknown token). Then we learns a linear model\ngw(s′)with weights w ∈ RT to approximate f(s′),\nwhere T is the length of sentences. This can be\nsolved as a ridge regression:\nmin\nw∈RT\nL(f,w, S(s))+ σ∥w∥2\n2.\nThe learned weights w can serve as a saliency\nmap that reﬂects the “contribution” of each token\nin the sequence s. Like in Figure 7, the color re-\nﬂects the weightsw at each position. Red means\nthe given position has positive weight and green\nmeans negative weight. The magnitude of weight\nis represented by the intensity. The redder a token\nis, the more it contributions to the activation of\nthe transformer factor. We leave more implementa-\ntion and mathematical formulation details of LIME\nalgorithm in the appendix.\n8\nWe provide detailed visualization for two differ-\nent transformer factors that show long-range depen-\ndency in Figure7, 8. Since visualization of high-\nlevel information requires more extended context,\nwe only offer the top two activated words and their\ncontexts for each such transformer factor. Many\nmore will be provided in the appendix sectionG.\nWe name the pattern for transformer factorΦ:,297\nin Figure 7 as “repetitive pattern detector”. All top\nactivated contexts for Φ:,297 contain an obvious\nrepetitive structure. Speciﬁcally, the text snippet\n“can’t get you out of my head\" appears twice in the\nﬁrst example, and the text snippet “xxx class pas-\nsenger, star alliance” appears three times in the sec-\nond example. Compared to the patterns we found\nin the mid-level [6], the high-level patterns like\n“repetitive pattern detector” are much more abstract.\nIn some sense, the transformer detects if there are\ntwo (or multiple) almost identical embedding vec-\ntors at layer-10without caring what they are. Such\nbehavior might be highly related to the concept\nproposed in the capsule networks (Sabour et al.,\n2017; Hinton, 2021). To further understand this be-\nhavior and study how the self-attention mechanism\nhelps model the relationships between the features\noutlines an interesting future research direction.\nFigure 8 shown another high-level factor, which\ndetects text snippets related to “the beginning of\na biography”. The necessary components, day of\nbirth as month and four-digit years, ﬁrst name and\nlast name, familial relation, and career, are all mid-\nlevel information. In Figure8, we see that all the\ninformation relates to biography has a high weight\nin the saliency map. Thus, they are all together\ncombined to detect the high-level pattern.\nFigure 7: Two examples of the high activated words\nand their contexts for transformer factor Φ:,297.W e\nalso provide the saliency map of the tokens generated\nusing LIME. This transformer factor corresponds to the\nconcept: “repetitive pattern detector”. In other words,\nrepetitive text sequences will trigger high activation of\nΦ\n:,297.\nFigure 8: Visualization ofΦ:,322. This transformer fac-\ntor corresponds to the concept: “some born in some\nyear” in biography. All of the high-activation contexts\ncontain the beginning of a biography. As shown in the\nﬁgure, the attributes of someone, name, age, career, and\nfamilial relation all have high saliency weights.\n4 Discussion\nDictionary learning has been successfully used to\nvisualize the classical word embeddings ( Arora\net al., 2018; Zhang et al., 2019). In this paper,\nwe propose to use this simple method to visual-\nize the representation learned in transformer net-\nworks to supplement the implicit “probing-tasks”\nmethods. Our results show that the learned trans-\nformer factors are relatively reliable and can even\nprovide many surprising insights into the linguis-\ntic structures. This simple tool can open up the\ntransformer networks and show the hierarchical\nsemantic or syntactic representation learned at dif-\nferent stages. In short, we ﬁnd word-level disam-\nbiguation, sentence-level pattern formation, and\nlong-range dependency. The idea of a neural net-\nwork learns low-level features in early layers, and\nabstract concepts in the later stages are very simi-\nlar to the visualization in CNN (Zeiler and Fergus,\n2014). Dictionary learning can be a convenient\ntool to help visualize a broad category of neural\nnetworks with skip connections, like ResNet (He\net al., 2016), ViT models (Dosovitskiy et al., 2020),\netc. For more interested readers, we provide an\ninteractive website1 for the readers to gain some\nfurther insights.\nAcknowledgements\nWe thank our reviewers for their detailed and in-\nsightful comments. We also thank Y uhao Zhang\nfor his suggestions during the preparation of this\npaper.\nReferences\nPretrained bert base model (12 layers). https:\n//huggingface.co/bert-base-uncased,\n1https://transformervis.github.io/transformervis/\n9\nlast accessed on 03/11/2021.\nSanjeev Arora, Y uanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2018. Linear algebraic struc-\nture of word senses, with applications to polysemy.\nTransactions of the Association for Computational\nLinguistics, 6:483–495.\nAmir Beck and Marc Teboulle. 2009. A fast itera-\ntive shrinkage-thresholding algorithm for linear in-\nverse problems. SIAM journal on imaging sciences,\n2(1):183–202.\nHila Chefer, Shir Gur, and Lior Wolf. 2020. Trans-\nformer interpretability beyond attention visualiza-\ntion. CoRR, abs/2012.09838.\nHila Chefer, Shir Gur, and Lior Wolf. 2021. Generic\nattention-model explainability for interpreting bi-\nmodal and encoder-decoder transformers . CoRR,\nabs/2103.15679.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929.\nJohn Duchi, Elad Hazan, and Y oram Singer. 2011.\nAdaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine\nLearning Research, 12(Jul):2121–2159.\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? comparing the geom-\netry of bert, elmo, and GPT -2 embeddings. InPro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing, EMNLP-IJCNLP, pages 55–65. Associ-\nation for Computational Linguistics.\nManaal Faruqui, Y ulia Tsvetkov, Dani Y ogatama, Chris\nDyer, and Noah A. Smith. 2015. Sparse overcom-\nplete word vector representations. In Proceedings\nof the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International\nJoint Conference on Natural Language Processing\n(V olume 1: Long Papers). Association for Computa-\ntional Linguistics.\nW . N. Francis and H. Kucera. 1979. Brown corpus\nmanual. Technical report, Department of Linguis-\ntics, Brown University, Providence, Rhode Island,\nUS.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, V olume 1 (Long and Short Papers).\nGeoffrey Hinton. 2021. How to represent part-whole\nhierarchies in a neural network. arXiv preprint\narXiv:2102.12627.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know. Trans. Assoc. Comput. Linguistics,\n8:423–438.\nNelson F. Liu, Matt Gardner, Y onatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, V olume 1 (Long and Short Pa-\npers). Association for Computational Linguistics.\nEmily Reif, Ann Y uan, Martin Wattenberg, Fernanda B.\nViégas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nBERT. In Advances in Neural Information Process-\ning Systems 32: Annual Conference on Neural Infor-\nmation Processing Systems, (NeurIPS), pages 8592–\n8600.\nMarco Túlio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \"why should I trust you?\": Ex-\nplaining the predictions of any classiﬁer . CoRR,\nabs/1602.04938.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow BERT works. Trans. Assoc. Comput. Linguis-\ntics, 8:842–866.\nSara Sabour, Nicholas Frosst, and Geoffrey E Hinton.\n2017. Dynamic routing between capsules. arXiv\npreprint arXiv:1710.09829.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin V an Durme, Samuel R Bowman, Dipan-\njan Das, et al. 2019. What do you learn from\ncontext? probing for sentence structure in con-\ntextualized word representations. arXiv preprint\narXiv:1905.06316.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nMatthew D Zeiler and Rob Fergus. 2014. Visualizing\nand understanding convolutional networks. InEuro-\npean conference on computer vision, pages 818–833.\nSpringer.\n10\nJuexiao Zhang, Y ubei Chen, Brian Cheung, and\nBruno A Olshausen. 2019. Word embedding vi-\nsualization via dictionary learning. arXiv preprint\narXiv:1910.03833.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7648837566375732
    },
    {
      "name": "Visualization",
      "score": 0.7415637969970703
    },
    {
      "name": "Computer science",
      "score": 0.7234058380126953
    },
    {
      "name": "Sentence",
      "score": 0.5611032247543335
    },
    {
      "name": "Natural language processing",
      "score": 0.5580026507377625
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4721291661262512
    },
    {
      "name": "Embedding",
      "score": 0.438912957906723
    },
    {
      "name": "Engineering",
      "score": 0.13581043481826782
    },
    {
      "name": "Voltage",
      "score": 0.09633207321166992
    },
    {
      "name": "Electrical engineering",
      "score": 0.0935567319393158
    }
  ]
}