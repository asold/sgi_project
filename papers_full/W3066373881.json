{
  "title": "Very Deep Transformers for Neural Machine Translation",
  "url": "https://openalex.org/W3066373881",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1971393576",
      "name": "Liu XiaoDong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2491181426",
      "name": "Duh, Kevin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112714395",
      "name": "Liu Li-yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119363152",
      "name": "Gao Jian-feng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2133512280",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2952509486",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W2964046515",
    "https://openalex.org/W2970295111",
    "https://openalex.org/W2798761464",
    "https://openalex.org/W2890964657",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W3017003177",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2994689640",
    "https://openalex.org/W2970157301",
    "https://openalex.org/W2144600658",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W2963823140",
    "https://openalex.org/W1026270304",
    "https://openalex.org/W3016635207",
    "https://openalex.org/W2964093309",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2922349260",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W3000127803",
    "https://openalex.org/W3015777882"
  ],
  "abstract": "We explore the application of very deep Transformer models for Neural Machine Translation (NMT). Using a simple yet effective initialization technique that stabilizes training, we show that it is feasible to build standard Transformer-based models with up to 60 encoder layers and 12 decoder layers. These deep models outperform their baseline 6-layer counterparts by as much as 2.5 BLEU, and achieve new state-of-the-art benchmark results on WMT14 English-French (43.8 BLEU and 46.4 BLEU with back-translation) and WMT14 English-German (30.1 BLEU).The code and trained models will be publicly available at: https://github.com/namisan/exdeep-nmt.",
  "full_text": "Very Deep Transformers for Neural Machine Translation\nXiaodong Liu†, Kevin Duh‡, Liyuan Liu§and Jianfeng Gao†\n†Microsoft Research ‡Johns Hopkins University\n§University of Illinois at Urbana-Champaign\n{xiaodl,jfgao}@microsoft.com\nkevinduh@cs.jhu.edu, ll2@illinois.edu\nAbstract\nWe explore the application of very deep Trans-\nformer models for Neural Machine Translation\n(NMT). Using a simple yet effective initializa-\ntion technique that stabilizes training, we show\nthat it is feasible to build standard Transformer-\nbased models with up to 60 encoder layers\nand 12 decoder layers. These deep models\noutperform their baseline 6-layer counterparts\nby as much as 2.5 BLEU, and achieve new\nstate-of-the-art benchmark results on WMT14\nEnglish-French (43.8 BLEU and 46.4 BLEU\nwith back-translation) and WMT14 English-\nGerman (30.1 BLEU). To facilitate further re-\nsearch in Very Deep Transformers for NMT,\nwe release the code and models: https://\ngithub.com/namisan/exdeep-nmt.\n1 Introduction\nThe capacity of a neural network inﬂuences its\nability to model complex functions. In particular, it\nhas been argued that deeper models are conducive\nto more expressive features (Bengio, 2009). Very\ndeep neural network models have proved successful\nin computer vision (He et al., 2016; Srivastava et al.,\n2015) and text classiﬁcation (Conneau et al., 2017;\nMinaee et al., 2020). In neural machine translation\n(NMT), however, current state-of-the-art models\nsuch as the Transformer typically employ only 6-\n12 layers (Bawden et al., 2019; Junczys-Dowmunt,\n2019; Ng et al., 2019).\nPrevious work has shown that it is difﬁcult to\ntrain deep Transformers, such as those over 12 lay-\ners (Bapna et al., 2018). This is due to optimization\nchallenges: the variance of the output at each layer\ncompounds as they get deeper, leading to unstable\ngradients and ultimately diverged training runs.\nIn this empirical study, we re-investigate whether\ndeeper Transformer models are useful for NMT. We\napply a recent initialization technique called AD-\nMIN (Liu et al., 2020a), which remedies the vari-\nFigure 1: Transformer model\nance problem. This enables us train Transformers\nthat are signiﬁcantly deeper, e.g. with 60 encoder\nlayers and 12 decoder layers.1\nIn contrast to previous research, we show that it\nis indeed feasible to train the standard2 Transformer\n(Vaswani et al., 2017) with many layers. These\ndeep models signiﬁcantly outperform their 6-layer\nbaseline, with up to 2.5 BLEU improvement. Fur-\nther, they obtain state-of-the-art on the WMT’14\nEN-FR and WMT’14 EN-DE benchmarks.\n2 Background\nWe focus on the Transformer model (Vaswani\net al., 2017), shown in Figure 1. The encoder\nconsists of N layers/blocks of attention + feed-\nforward components. The decoder consists of M\nlayers/blocks of masked-attention, attention, and\nfeed-forward components. To illustrate, the in-\n1We choose to focus on this layer size since it results in the\nmaximum model size that can ﬁt within a single GPU system.\nThe purpose of this study is to show that it is feasible for most\nresearchers to experiment with very deep models; access to\nmassive GPU budgets is not a requirement.\n2Note there are architectural variants that enable deeper\nmodels (Wang et al., 2019; Nguyen and Salazar, 2019), dis-\ncussed in Sec 2. We focus on the standard architecture here.\narXiv:2008.07772v2  [cs.CL]  14 Oct 2020\nput tensor xi−1 at the encoder is ﬁrst transformed\nby a multi-head attention mechanism to generate\nthe tensor fATT (xi−1). This result is added back\nwith xi−1 as a residual connection, then layer-\nnormalization (fLN(·)) is applied to generate the\noutput: xi = fLN(xi−1 + fATT (xi−1)). Continu-\ning onto the next component, xi is passed through\na feed-forward network fFF (·), and is again added\nand layer-normalized to generate the output tensor:\nxi+1 = fLN(xi + fFF (xi)). Abstractly, the out-\nput tensor at each Add+Norm component in the\nTransformer (Figure 1) can be expressed as:\nxi = fLN(xi−1 + fi(xi−1)) (1)\nwhere fi represents a attention, masked-attention,\nor feed-forward subnetwork. This process repeats\n2×N times for a N-layer encoder and 3×M times\nfor a M-layer decoder. The ﬁnal output of the\ndecoder is passed through a softmax layer which\npredicts the probabilities of output words, and the\nentire network is optimized via back-propagation.\nOptimization difﬁculty has been attributed to\nvanishing gradient, despite layer normalization (Xu\net al., 2019) providing some mitigation. The lack\nof gradient ﬂow between the decoder and the lower\nlayers of the encoder is especially problematic;\nthis can be addressed with short-cut connections\n(Bapna et al., 2018; He et al., 2018). An orthogo-\nnal solution is to swap the positions of layerwise\nnormalization fLN and subnetworks fi within each\nblock (Nguyen and Salazar, 2019; Domhan, 2018;\nChen et al., 2018) by: xi = fi(xi−1 + fLN(xi−1))\nThis is known as pre-LN (contrasted with post-LN\nin Eq. 1), and has been effective in training net-\nworks up to 30 layers (Wang et al., 2019).3\nHowever, it has been shown that post-LN, if\ntrained well, can outperform pre-LN (Liu et al.,\n2020a). Ideally, we hope to train a standard Trans-\nformer without additional architecture modiﬁca-\ntions. In this sense, our motivation is similar to that\nof Wu et al. (2019b), which grows the depth of a\nstandard Transformer in a stage-wise fashion.\n3 Initialization Technique\nThe initialization technique ADMIN (Liu et al.,\n2020a) we will apply here reformulates Eq. 1 as:\nxi = fLN(xi−1 ·ωi + fi(xi−1)) (2)\nwhere ωi is a constant vector that is element-wise\nmultiplied to xi−1 in order to balance the contri-\nbution against fi(xi−1). The observation is that in\n3The 96-layer GPT-3 (Brown et al., 2020) uses pre-LN.\naddition to vanishing gradients, the unequal mag-\nnitudes in the two terms xi−1 and fi(xi−1) is the\nmain cause of instability in training. Refer to (Liu\net al., 2020a) for theoretical details.4\nADMIN initialization involves two phases: At\nthe Proﬁling phase, we randomly initialize the\nmodel parameters using default initialization, set\nωi = 1, and perform one step forward pass in or-\nder to compute the output variance of the residual\nbranch V ar[f(xi−1)] at each layer.5 In the Train-\ning phase, we ﬁx ωi =\n√∑\nj<i V ar[f(xj−1)],\nand then train the model using standard back-\npropagation. After training ﬁnishes, ωi can be\nfolded back into the model parameters to recover\nthe standard Transformer architecture. This simple\ninitialization method is effective in ensuring that\ntraining does not diverge, even in deep networks.\n4 Experiments\nExperiments are conducted on standard WMT’14\nEnglish-French (FR) and English-German (DE)\nbenchmarks. For FR, we mimic the setup6 of (Ott\net al., 2018), with 36M training sentences and 40k\nsubword vocabulary. We use the provided ’valid’\nﬁle for development and newstest14 for test. For\nDE, we mimic the setup7 of (So et al., 2019), with\n4.5M training sentences, 32K subword vocabulary,\nnewstest2013 for dev, and newstest2014 for test.\nWe adopt the hyper-parameters of the\nTransformer-based model (Vaswani et al., 2017)\nas implemented in FAIRSEQ (Ott et al., 2019),\ni.e. 512-dim word embedding, 2048 feed-forward\nmodel size, and 8 heads, but vary the number of\nlayers. RAdam (Liu et al., 2019) is our optimizer.8\nMain Result: Our goal is to explore whether\nvery deep Transformers are feasible and effective.\nWe compare: (a) 6L-6L: a baseline Transformer\nBase with 6 layer encoder and 6 layer decoder, vs.\n(b) 60L-12L: A deep transformer with 60 encoder\n4Note that paper presents results of 18-layer Transform-\ners on the WMT’14 En-De, which we also use here. Our\ncontribution is a more comprehensive evaluation.\n5We estimate the variance with one batch of 8k tokens.\n6https://github.com/pytorch/fairseq/\nblob/master/examples/translation/\nprepare-WMT’14en2fr.sh\n7https://github.com/tensorflow/\ntensor2tensor/blob/master/tensor2tensor/\ndata_generators/translate_ende.py\n8For FR, #warmup steps is 8000, max #epochs is 50, and\npeak learning rate is 0.0007. For DE, #warmup steps is 4000,\nmax #epochs is 50, and learning rate is 0.001. Max #tokens in\neach batch is set to 3584 following (Ott et al., 2019).\nWMT’14 English-French (FR) WMT’14 English-German (DE)\nModel #param T↓ M↑ BLEU↑ ∆ #param T↓ M↑ BLEU↑ ∆\n6L-6L Default 67M 42.2 60.5 41.3 - 61M 54.4 46.6 27.6 -\n6L-6L ADMIN 67M 41.8 60.7 41.5 0.2 61M 54.1 46.7 27.7 0.1\n60L-12L Default 262M diverge 256M diverge\n60L-12L ADMIN 262M 40.3 62.4 43.8 2.5 256M 51.8 48.3 30.1 2.5\nTable 1: Test results on WMT’14 benchmarks, in terms of TER ( T↓), METEOR ( M↑), and BLEU. ∆ shows\ndifference in BLEU score against baseline 6L-6L Default. Best results are boldfaced. 60L-12L ADMIN outper-\nforms 6L-6L in all metrics with statistical signiﬁcance ( p <0.05). Following convention, BLEU is computed by\nmulti-bleu.perl via the standardized tokenization of the publicly-accessible dataset.\nBLEU via multi-bleu.perl FR DE\n60L-12L ADMIN 43.8 30.1\n(Wu et al., 2019b) 43.3 29.9\n(Wang et al., 2019) - 29.6\n(Wu et al., 2019a) 43.2 29.7\n(Ott et al., 2018) 43.2 29.3\n(Vaswani et al., 2017) 41.8 28.4\n(So et al., 2019) 41.3 29.8\n(Gehring et al., 2017) 40.5 25.2\nBLEU via sacreBLEU.py FR DE\n60L-12L ADMIN 41.8 29.5\n(Ott et al., 2018) 41.4 28.6\n(So et al., 2019) n/a 29.2\nTable 2: State-of-the-Art on WMT’14 EN-FR/EN-DE\nlayers and 12 decoder layers.9 For each architec-\nture, we train with either default initialization (Glo-\nrot and Bengio, 2010) or ADMIN initialization.\nThe results in terms of BLEU (Papineni et al.,\n2002), TER (Snover et al., 2006), and METEOR\n(Lavie and Agarwal, 2007) are reported in Table 1.\nSimilar to previous work (Bapna et al., 2018), we\nobserve that deep 60L-12L Default diverges during\ntraining. But the same deep model with ADMIN\nsuccessfully trains and impressively achieves 2.5\nBLEU improvement over the baseline 6L-6L De-\nfault in both datasets. The improvements are also\nseen in terms of other metrics: in EN-FR, 60L-12L\nADMIN outperforms the 6L-6L models in TER\n(40.3 vs 42.2) and in METEOR (62.4 vs 60.5). All\nresults are statistically signiﬁcant (p <0.05) with\na 1000-sample bootstrap test (Clark et al., 2011).\nThese results indicate that it is feasible to\n9We use “(N)L-(M)L\" to denote that a model has N en-\ncoder layers and M decoder layers. N & M are chosen based\non GPU (16G) memory constraint. For reproducibility and\nsimplicity, we focused on models that ﬁt easily on a single\nGPU system. Taking FR as an example, it takes 2.5 days to\ntrain 60L-12L using one DGX-2 (16 V100’s), 2 days to train\na 6L-6L using 4 V100’s.\n(a) Train set perplexity: Default vs ADMIN\n(b) Dev set perplexity: different ADMIN models\nFigure 2: Learning curve\ntrain standard (post-LN) Transformers that are\nvery deep. 10 These models achieve state-of-\nthe-art results in both datasets. The top re-\nsults in the literature are compared in Ta-\nble 2. 11 We list BLEU scores computed with\nmulti-bleu.perl on the tokenization of\nthe downloaded data (commonly done in previ-\n10Note: the pre-LN version does train successively on 60L-\n12L and achieves 29.3 BLEU in DE & 43.2 in FR. It is better\nthan 6L-6L but worse than 60L-12L ADMIN.\n11The table does not include systems that use extra data.\nModel BLEU a b c d e f g\na:6L-6L 41.5 - - - - - -\nb:12L-12L 42.6 + - - - - -\nc:24L-12L 43.3 + + = - = =\nd:48L-12L 43.6 + + = = = +\ne:60L-12L 43.8 + + + = = +\nf:36L-36L 43.7 + + = = = +\ng:12L-60L 43.1 + + = - - -\nTable 3: BLEU comparison of different encoder and de-\ncoder layers (using ADMIN initialization, on WMT’14\nEN-FR). In the matrix, each element (i,j) indicates if\nthe model in row i signiﬁcantly outperforms the model\nin column j (+), under-performs j (-), or has no statisti-\ncally signiﬁcant difference (=).\nous work), and with sacrebleu.py (version:\ntok.13a+version.1.2.10). which allows for a safer\ntoken-agnostic evaluation (Post, 2018).\nLearning Curve: We would like to understand\nwhy 60L-12L ADMIN is doing better from the opti-\nmization perspective. Figure 2 (a) plots the learning\ncurve comparing ADMIN to Default initialization.\nWe see that Default has difﬁculty decreasing the\ntraining perplexity; its gradients hit NaN, and the\nresulting model is not better than a random model.\nIn Figure 2 (b), we see that larger models (60L-12L,\n36L-36L) are able obtain lower dev perplexities\nthan 6L-6L, implying that the increased capacity\ndoes lead to better generalization.\nFine-grained error analysis: We are also inter-\nested in understanding how BLEU improvements\nare reﬂected in terms of more nuanced measures.\nFor example, do the deeper models particularly im-\nprove translation of low frequency words? Do they\nwork better for long sentences? The answer is that\nthe deeper models appear to provide improvements\ngenerally across the board (Figure 3).12\nAblation Studies: We experimented with differ-\nent number of encoder and decoder layers, given\nthe constraint of a 16GB GPU. Table 3 shows the\npairwise comparison of models. We observe that\n60L-12L, 48L-12L, and 36L-36L are statistically\ntied for best BLEU performance. It appears that\ndeeper encoders are more worthwhile than deeper\ndecoders, when comparing 60L-12L to 12L-60L,\ndespite the latter having more parameters.13\n12Computed by compare-mt (Neubig et al., 2019).\n13Recall from Figure 1 that each encoder layer has 2 subnet-\nwork components and each decoder layer has 3 components.\n(a) Word accuracy according to frequency in the training data\n(b) BLEU scores according to sentence length.\nFigure 3: Fine-grained Error Analysis: note the deep\nmodel performs better across the board, indicating that\nit helps translation in general.\nWe also experiment with wider networks, start-\ning with a 6L-6L Transformer-Big (1024-dim word\nembedding, 4096 feed-forward size, 16 heads) and\ndoubling its layers to 12L-12L. The BLEU score\non EN-FR improved from 43.2 to 43.6 (statistically\nsigniﬁcant, p < 0.05). A 24L-12L Transformer\nwith BERT-Base like settings (768-dim word em-\nbedding, 3072 feed-forward size, 12 heads) obtain\n44.0 BLEU score on WMT’14 EN-FR. This shows\nthat increased depth also helps models that are al-\nready relatively wide.\nBack-translation We investigate whether deeper\nmodels also beneﬁt when trained on the large but\npotentially noisy data such as back-translation. We\nfollow the back-translation settings of (Edunov\net al., 2018) and generated additional 21.8M trans-\nlation pairs for EN-FR. The hyperparameters are\nthe same as the one without back-translation as\nintroduced in (Edunov et al., 2018), except for an\nup-sampling rate 1 for EN-FR.\nTable 4 compares the ADMIN 60L-12L and\nADMIN 36L-12L-768D model 14 with the default\n14It is BERT-base setting with 768-dim word embedding,\nbig transformer architecture (6L-6L) which obtains\nstates-of-the-art results (Edunov et al., 2018). We\nsee that with back-translation, both ADMIN 60L-\n12L + BT and ADMIN 36L-12L-768D still signiﬁ-\ncantly outperforms its baseline ADMIN 60L-12L.\nFurthermore, ADMIN 36L-12L-768D achieves\nnew state-of-the-art benchmark results on WMT’14\nEnglish-French (46.4 BLEU and 44.4 sacreBLEU\n15).\nBLEU via multi-bleu.perl FR\n36L-12L-768D ADMIN + BT 46.4\n60L-12L ADMIN + BT 46.0\nBT (Edunov et al., 2018) 45.6\n60L-12L ADMIN 43.8\nBLEU via sacreBLEU.py FR\n36L-12L-768D ADMIN + BT 44.4\n60L-12L ADMIN + BT 44.1\n60L-12L ADMIN 41.8\nBT (Edunov et al., 2018) -\nTable 4: Back-translation results on WMT’14 EN-FR.\n5 Conclusion\nWe show that it is feasible to train Transformers at\na depth that was previously believed to be difﬁcult.\nUsing ADMIN initialization, we build Transformer-\nbased models of 60 encoder layers and 12 decoder\nlayers. On WMT’14 EN-FR and WMT’14 EN-EN,\nthese deep models outperform the conventional 6-\nlayer Transformers by up to 2.5 BLEU, and obtain\nstate-of-the-art results.\nWe believe that the ability to train very deep\nmodels may open up new avenues of research in\nNMT, including: (a) Training on extremely large\nbut noisy data, e.g. back-translation (Edunov et al.,\n2018) and adversarial training (Cheng et al., 2019;\nLiu et al., 2020b), to see if it can be exploited by\nthe larger model capacity. (b) Analyzing the inter-\nnal representations, to see if deeper networks can\nindeed extract higher-level features in syntax and\nsemantics (Belinkov and Glass, 2019). (c) Com-\npressing the very deep model via e.g. knowledge\ndistillation (Kim and Rush, 2016), to study the\ntrade-offs between size and translation quality. (d)\nAnalyzing how deep models work (Allen-Zhu and\nLi, 2020) in theory.\n3072 feed-froward size and 12 heads.\n15BLEU+case.mixed+lang.en-\nfr+numrefs.1+smooth.exp+test.wmt14+tok.13a+version.1.2.10\nAcknowledgments\nWe thank Hao Cheng, Akiko Eriguchi, Hany Has-\nsan Awadalla and Zeyuan Allen-Zhu for valuable\ndiscussions.\nReferences\nZeyuan Allen-Zhu and Yuanzhi Li. 2020. Backward\nfeature correction: How deep learning performs\ndeep learning. arXiv preprint arXiv:2001.04413.\nAnkur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao,\nand Yonghui Wu. 2018. Training deeper neural ma-\nchine translation models with transparent attention.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3028–3033.\nRachel Bawden, Nikolay Bogoychev, Ulrich Germann,\nRoman Grundkiewicz, Faheem Kirefu, Antonio Va-\nlerio Miceli Barone, and Alexandra Birch. 2019.\nThe university of edinburgh submissions to the\nwmt19 news translation task. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1), pages 103–115, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nYoshua Bengio. 2009. Learning Deep Architectures\nfor AI, volume Foundations and Trends in Machine\nLearning. NOW Publishers.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin John-\nson, George Foster, Llion Jones, Mike Schuster,\nNoam Shazeer, Niki Parmar, Ashish Vaswani, Jakob\nUszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui\nWu, and Macduff Hughes. 2018. The best of both\nworlds: Combining recent advances in neural ma-\nchine translation. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nYong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\nRobust neural machine translation with doubly ad-\nversarial inputs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4324–4333.\nJonathan H Clark, Chris Dyer, Alon Lavie, and Noah A\nSmith. 2011. Better hypothesis testing for statistical\nmachine translation: Controlling for optimizer insta-\nbility. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 176–181.\nAlexis Conneau, Holger Schwenk, Loïc Barrault, and\nYann Lecun. 2017. Very deep convolutional net-\nworks for text classiﬁcation. In Proceedings of the\n15th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Volume 1,\nLong Papers, pages 1107–1116, Valencia, Spain. As-\nsociation for Computational Linguistics.\nTobias Domhan. 2018. How much attention do you\nneed? a granular analysis of neural machine trans-\nlation architectures. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1799–\n1808, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 489–500.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning - Volume 70 , ICML’17, page 1243–1252.\nJMLR.org.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth interna-\ntional conference on artiﬁcial intelligence and statis-\ntics, pages 249–256.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nTianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo\nChen, and Tie-Yan Liu. 2018. Layer-wise coordi-\nnation between encoder and decoder for neural ma-\nchine translation. In Advances in Neural Informa-\ntion Processing Systems, pages 7944–7954.\nMarcin Junczys-Dowmunt. 2019. Microsoft translator\nat wmt 2019: Towards large-scale document-level\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1), pages 225–233, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1317–1327, Austin,\nTexas. Association for Computational Linguistics.\nA. Lavie and A. Agarwal. 2007. METEOR: An auto-\nmatic metric for mt evaluation with high levels of\ncorrelation with human judgments. In Workshop on\nStatistical Machine Translation.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2019. On the variance of the adaptive learning rate\nand beyond. arXiv preprint arXiv:1908.03265.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu\nChen, and Jiawei Han. 2020a. Understanding the\ndifﬁculty of training transformers. arXiv preprint\narXiv:2004.08249.\nXiaodong Liu, Hao Cheng, Pengcheng He, Weizhu\nChen, Yu Wang, Hoifung Poon, and Jianfeng Gao.\n2020b. Adversarial training for large neural lan-\nguage models. arXiv preprint arXiv:2004.08994.\nShervin Minaee, Nal Kalchbrenner, Erik Cambria,\nNarjes Nikzad, Meysam Chenaghlu, and Jianfeng\nGao. 2020. Deep learning based text classiﬁca-\ntion: a comprehensive review. arXiv preprint\narXiv:2004.03705.\nGraham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel,\nDanish Pruthi, and Xinyi Wang. 2019. compare-mt:\nA tool for holistic comparison of language genera-\ntion systems. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Asso-\nciation for Computational Linguistics (Demonstra-\ntions), pages 35–41, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2019. Facebook\nfair’s wmt19 news translation task submission. In\nProceedings of the Fourth Conference on Machine\nTranslation (Volume 2: Shared Task Papers, Day\n1), pages 314–319, Florence, Italy. Association for\nComputational Linguistics.\nToan Q. Nguyen and Julian Salazar. 2019. Transform-\ners without tears: Improving the normalization of\nself-attention. In Proc. of the International Work-\nshop on Spoken Language Translation (IWSLT).\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensi-\nble toolkit for sequence modeling. arXiv preprint\narXiv:1904.01038.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proceedings of the Third Conference on\nMachine Translation: Research Papers , pages 1–9,\nBrussels, Belgium. Association for Computational\nLinguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: A method for automatic\nevaluation of machine translation. In ACL.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nM. Snover, B. Dorr, R. Schwartz, L. Micciulla, and\nJ. Makhoul. 2006. A study of translation edit rate\nwith targeted human annotation. In AMTA.\nDavid So, Quoc Le, and Chen Liang. 2019. The\nevolved transformer. In Proceedings of the 36th In-\nternational Conference on Machine Learning , vol-\nume 97 of Proceedings of Machine Learning Re-\nsearch, pages 5877–5886, Long Beach, California,\nUSA. PMLR.\nRupesh K Srivastava, Klaus Greff, and Jürgen Schmid-\nhuber. 2015. Training very deep networks. In\nAdvances in neural information processing systems ,\npages 2377–2385.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810–1822, Florence, Italy. Associa-\ntion for Computational Linguistics.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N.\nDauphin, and Michael Auli. 2019a. Pay less atten-\ntion with lightweight and dynamic convolutions. In\n7th International Conference on Learning Represen-\ntations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019.\nLijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei\nGao, Tao Qin, Jianhuang Lai, and Tie-Yan Liu.\n2019b. Depth growing for neural machine trans-\nlation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 5558–5563, Florence, Italy. Association\nfor Computational Linguistics.\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang\nZhao, and Junyang Lin. 2019. Understanding and\nimproving layer normalization. In Advances in Neu-\nral Information Processing Systems 32, pages 4381–\n4391. Curran Associates, Inc.",
  "topic": "BLEU",
  "concepts": [
    {
      "name": "BLEU",
      "score": 0.9000567197799683
    },
    {
      "name": "Machine translation",
      "score": 0.8755000829696655
    },
    {
      "name": "Initialization",
      "score": 0.8597360849380493
    },
    {
      "name": "Computer science",
      "score": 0.8352500796318054
    },
    {
      "name": "Transformer",
      "score": 0.8338697552680969
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6138572692871094
    },
    {
      "name": "Encoder",
      "score": 0.611829400062561
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5740046501159668
    },
    {
      "name": "Natural language processing",
      "score": 0.4675259590148926
    },
    {
      "name": "Evaluation of machine translation",
      "score": 0.4534991383552551
    },
    {
      "name": "Translation (biology)",
      "score": 0.41484329104423523
    },
    {
      "name": "Speech recognition",
      "score": 0.3463931679725647
    },
    {
      "name": "Example-based machine translation",
      "score": 0.21858644485473633
    },
    {
      "name": "Voltage",
      "score": 0.11438536643981934
    },
    {
      "name": "Programming language",
      "score": 0.10253956913948059
    },
    {
      "name": "Machine translation software usability",
      "score": 0.07101798057556152
    },
    {
      "name": "Engineering",
      "score": 0.05987197160720825
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 71
}