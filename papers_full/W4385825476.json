{
  "title": "ViGT: proposal-free video grounding with a learnable token in the transformer",
  "url": "https://openalex.org/W4385825476",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2002222866",
      "name": "Li Kun",
      "affiliations": [
        "Ministry of Education of the People's Republic of China",
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2113595753",
      "name": "Guo Dan",
      "affiliations": [
        "Ministry of Education of the People's Republic of China",
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2080387516",
      "name": "Wang Meng",
      "affiliations": [
        "Hefei University of Technology",
        "Ministry of Education of the People's Republic of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288751939",
    "https://openalex.org/W2974467376",
    "https://openalex.org/W2964089981",
    "https://openalex.org/W2963393391",
    "https://openalex.org/W3034743747",
    "https://openalex.org/W2997429269",
    "https://openalex.org/W3176201273",
    "https://openalex.org/W2507009361",
    "https://openalex.org/W2964214371",
    "https://openalex.org/W2755876276",
    "https://openalex.org/W3096935578",
    "https://openalex.org/W4304098887",
    "https://openalex.org/W4283516491",
    "https://openalex.org/W2788334925",
    "https://openalex.org/W2597812494",
    "https://openalex.org/W2965778995",
    "https://openalex.org/W3035339529",
    "https://openalex.org/W2970898753",
    "https://openalex.org/W2963017553",
    "https://openalex.org/W2798354744",
    "https://openalex.org/W2897628926",
    "https://openalex.org/W2890502146",
    "https://openalex.org/W2998495542",
    "https://openalex.org/W3175082063",
    "https://openalex.org/W2969291041",
    "https://openalex.org/W3180476551",
    "https://openalex.org/W2962869524",
    "https://openalex.org/W6841268915",
    "https://openalex.org/W3107593835",
    "https://openalex.org/W2894280539",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W3035640828",
    "https://openalex.org/W3120889656",
    "https://openalex.org/W3104893896",
    "https://openalex.org/W2964232540",
    "https://openalex.org/W2963095467",
    "https://openalex.org/W3174490084",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W4214490042",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2798858969",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W2963916161",
    "https://openalex.org/W4214663214",
    "https://openalex.org/W2952132648",
    "https://openalex.org/W2948958195",
    "https://openalex.org/W4284696747",
    "https://openalex.org/W4284693480",
    "https://openalex.org/W4304142044",
    "https://openalex.org/W2939519298",
    "https://openalex.org/W2970401629",
    "https://openalex.org/W3174364033",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3101429639"
  ],
  "abstract": null,
  "full_text": "SCIENCE CHINA\nInformation Sciences\n. RESEARCH PAPER .\nViGT: Proposal-free Video Grounding with\nLearnable Token in Transformer\nKun LI1,2, Dan GUO1,2,3,4* & Meng WANG1,2,3,4*\n1School of Computer Science and Information Engineering, Hefei University of Technology, Hefei 230601, China;\n2Key Laboratory of Knowledge Engineering with Big Data, Ministry of Education, Hefei 230601, China;\n3Intelligent Interconnected Systems Laboratory of Anhui Province, Hefei 230601, China;\n4Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei 230088, China\nAbstract The video grounding (VG) task aims to locate the queried action or event in an untrimmed\nvideo based on rich linguistic descriptions. Existing proposal-free methods are trapped in complex interac-\ntion between video and query, overemphasizing cross-modal feature fusion and feature correlation for VG.\nIn this paper, we propose a novel boundary regression paradigm that performs regression token learning\nin a transformer. Particularly, we present a simple but effective proposal-free framework, namely Video\nGrounding Transformer (ViGT), which predicts the temporal boundary using a learnable regression token\nrather than multi-modal or cross-modal features. In ViGT, the benefits of a learnable token are manifested\nas follows. (1) The token is unrelated to the video or the query and avoids data bias toward the original video\nand query. (2) The token simultaneously performs global context aggregation from video and query features.\nFirst, we employed a sharing feature encoder to project both video and query into a joint feature space\nbefore performing cross-modal co-attention ( i.e., video-to-query attention and query-to-video attention) to\nhighlight discriminative features in each modality. Furthermore, we concatenated a learnable regression to-\nken [REG] with the video and query features as the input of a vision-language transformer. Finally, we\nutilized the token [REG] to predict the target moment and visual features to constrain the foreground and\nbackground probabilities at each timestamp. The proposed ViGT performed well on three public datasets:\nANet Captions, TACoS and YouCookII. Extensive ablation studies and qualitative analysis further validated\nthe interpretability of ViGT.\nKeywords video grounding, temporal sentence grounding, boundary regression, token learning, proposal-\nfree.\nCitation\n1 Introduction\nRecently, Video grounding (VG), which is a challenging task in the computer vision community [9, 42],\nhas received increasing attention [14,22,52,56,58]. The associated tasks include action classification [44],\naction localization [3, 37], etc. Unlike these action-relevant tasks, which recognize or locate the action\nin videos automatically or with the predefined action labels, the VG task in our work requires first\nunderstanding a linguistic query sentence and then locating the queried visual content in the video.\nTherefore, the VG task has also attracted considerable interest due to its importance in the field of\nvision-language understanding, such as video captioning [7], video question answering [23], and cross-\nmodal understanding [15, 16, 19, 33]. The conventional paradigm of VG [14, 30, 52] encodes video and\nquery features, employs different methods of multi-modal interaction or multi-modal fusion to obtain\ncross-modal features, and finally predicts the target video segment. As illustrated in Figure 1, existing\nmethods can be classified into proposal-based methods and proposal-free methods.\nIn proposal-based methods , numerous candidate proposals are generated to cover the target mo-\nment. The researchers then apply the confidence scores of these dense candidate proposals to choose\nthe final proposals. In the early stage, VG was implemented using clip-query matching [1, 14, 26, 27]. It\n* Corresponding author (email: guodan@hfut.edu.cn, eric.mengwang@gmail.com)\narXiv:2308.06009v1  [cs.CV]  11 Aug 2023\nSci China Inf Sci 2\n(a) Pipeline of proposal-basedmethods.\n(b) Pipeline of proposal-freemethods.\n(c) Pipeline of our proposed ViGT.\nVideo EncoderQuery EncoderMulti-modal FusionProposalsgen & rankâ€¦0.2 0.7Score: 0.81 0.3 0.5Score: 0.92 \n0.40.5Score: 0.34 â€¦\nâ€¦\n0.2 0.6Score: 0.75\nRQ\nQQ\n0.30.6\n0.20.6Learnable regression token\nVideoQuery\nBoundaryregression\nBoundaryregression\nVideo EncoderQuery EncoderMulti-modal FusionVideoQuery\nVideo EncoderQuery EncoderVisual-TextualTransformer[REG]VideoQuery\nFigure 1 Pipeline of the video grounding task. Previous works can be divided into two pipelines: (a) proposal-based methods\n(e.g., [14,51]) and (b) proposal-free methods ( e.g., [22,30,52]). In contrast to them, the (c) proposed ViGT performs the boundary\nregression with a learnable token in transformer architecture.\nis assumed that cropped video segments can cover the target segment by applying a multi-scale sliding\nwindow on the videos. Under this assumption, temporal Intersection over Union, tIoU [14] is constantly\ncalculated for binary action or event classification evaluation. However, this assumption is not always\nvalid, particularly when the target segments vary over a wide range. To generate high quality proposals,\nChen et al. [6] and Wang et al. [43] produced proposals with multiple times scales at each time by fol-\nlowing action detection [3]. Zhang et al. [58] enumerated all the candidate segments using a 2D temporal\nmap. Furthermore, Liu et al. [24] proposed a novel bi-affine network to predict all possible candidates.\nProposal-free methods directly predict the target moment without any candidate proposals, avoiding\nthe heavy calculation of candidate proposal generation and ranking. Yuan et al. [52] and Mun et al. [30]\npredicted the segment boundaries using query-to-video attention. Rodriguez et al. [35] and Zhang et\nal. [56] predicted the probability scores of start and end at each timestamp and then chose the position\nwith the highest ones as the target moment. Chen et al. [7] performed both sequence-wise and channel-\nwise modality interactions. Li et al. [22] realized the multi-scale 2D correlation map in a proposal-free\nmanner.\nEven though previous works have made considerable advances in VG, they have some weaknesses. Typi-\ncally, proposal-based methodsare trapped in numerous candidate proposal generation and human-heuristic\nproposal ranking rules, making the model overfit to the distribution of training data. Additionally, the\naccuracy of proposal ranking is easily influenced by the candidate proposals and the post-processing\nefficiency. Current proposal-free methodslargely depend on cross-modal fusion or interaction methods,\nwhich are always solved in two modes: 1) first one-side intra-modal correlation and then fusion, or 2)\nmapping a query to a semantic vector and then correlating it with visual features. Proposal-free methods\ngets rid of redundant candidate proposals to reduce the costly computational overhead; however, they\nare easily influenced by the fusion or correlation effect of video and query.\nIn this work, we propose a simple but effective transformer-based framework for exploring a proposal-\nfree regression using a learnable token. Our method is implemented in an end-to-end manner. We named\nthe proposed method with a learnable Token in the transformer for VG as ViGT. The pipeline of ViGT\nis depicted in Figures 1 and 2. Unlike current proposal-based methods, the proposed ViGT no longer\nrequires candidate proposals but predicts the target moment from a learnable token. Furthermore, the\nvisual-linguistic correlation is addressed by imposing a learnable token on the relation learning of video\nand query rather than multi-modal or cross-modal features.\nThe overview of ViGT is depicted in Figure 3. First, we encode encode the video and query features\nusing a video-language encoder. Next, the learnable regression token is concatenated with video and\nquery features. It is then encoded by a video-language transformer. Finally, the regression token is used\nto predict the target moment. We attribute the success of the learnable token in our work to two aspects\nas follows: (1) The token is unrelated to the video or the query. It is randomly initialized and continually\nupdated during model optimization. It prevents data bias toward the original video and query. (2) The\nSci China Inf Sci 3\nVideoQueryCross-modal interaction [11, 54] TokenDifferent Transformer usages\nTransformer\nÃ˜Ã˜Boundary regressionÃ˜Fore-/Back-ground classification\nTransformer\nÃ˜Ã˜Fore-/Back-ground classification\nÃ˜Ã˜Boundary regression\nQueryVideo ViGT: Learnable token\nFigure 2 Different transformer-based architectures for video grounding. In comparison to current works [10, 57], the proposed\nViGT has a different usage. We investigated a learnable token rather than multi-modal features for boundary regression.\ntoken simultaneously interacts with all video and query features and retains informative cues in a global\nviewpoint of contextual aggregation.\nThe main contributions of our method are summarized as follows:\nâ€¢ We consider the concept of a learnable token for VG. We propose a simple but effective proposal-free\ntransformer-based framework that implicitly captures the relationship between the video and query.\nâ€¢ The proposed method is implemented in a proposal-free manner, and it predicts the target moment\nfrom a learnable token rather than multi-modal or cross-modal features, as in previous works.\nâ€¢ Extensive experiments are conducted on three benchmark datasets and demonstrate the efficacy of\nthe proposed method. Ablation studies and qualitative visualizations also confirm the contribution of\neach component.\n2 Related Work\nThis section reviews the two VG pipelines: proposal-based and proposal-free methods. Our method falls\nunder the category of proposal-free methods. Furthermore, we also review the related transformer-based\nwork for video understanding because our method is a transformer-based network.\n2.1 Video Grounding\nThe VG task evolved from temporal action detection, in which the action instance is located in the video,\nand its action category is identified. Later, the action detection task was extend to VG with a language\nsentence by the researchers. The VG task is also well known as temporal sentence grounding [24, 51],\nvideo moment retrieval [25, 55], and temporal activity localization via language [49] , etc. We grouped\nthe existing works into proposal-based and proposal-free methods.\n2.1.1 Proposal-based methods\nTo accurately locate the target action moment in videos, a simple solution is to generate a large num-\nber of potential proposals of activity instances and then select the best matching proposal as the final\nprediction. Early works [1, 14, 26, 27] were always performed in a propose-and-rank manner. They used\na multi-scale sliding window to generate proposal candidates and then measured the semantic distance\nto select the best one. However, the sliding-window-based methods suffer from heavy calculations and\nhuge memory costs. To address these issues, some works [6, 48] proposed to generate proposals that\nare directly conditioned on the sentence query. This proposal generation strategy was more adaptable\nthan the sliding window strategy. Particularly, Xu et al. [48] proposed a query-guided segment proposal\nnetwork that leverages video and query correlation to generate temporal attention weights for proposal\ngeneration. Chen et al. [6] proposed a semantic activity proposal network that generated proposals based\non a temporally semantic correlation score between video and semantic entities in the query. However,\nthe quality of generated proposals typically depended on the multi-modal interaction in the proposal\ngeneration module. Motivated by the proposal generation methods SSD for object detection [28] and\nSST for action detection [3], several similar works were proposed for VG [6, 43, 51]. Based on SST,\nTGN [6] extended a cross-modal interaction before proposal generation for VG. Inspired by the SSD\nmodel, SCDM [51] utilized a stacked convolutional block to generate dense proposals through iterative\nSci China Inf Sci 4\ninteraction. Specifically, multi-scale proposals were generated by using the stacked temporal convolu-\ntional block, which can cover the varied-length target actions. Additionally, unlike the aforementioned\nmethods, Zhang et al. [58] proposed a 2D temporal map to enumerate all proposals, while Zeng et al. [53]\naddressed the imbalance problem of temporal boundaries distribution of ground-truth through a dense\nregression head. In summary, these proposal-based methods have made considerable progress through\nefficient proposal generation. However, the proposal-based methods can not get rid of the post-processing\nof proposal ranking, which typically reduces grounding efficiency.\n2.1.2 Proposal-free methods\nAs opposed to proposal-based methods, proposal-free methods do not require the generation and ranking\nof candidate proposals but directly predict the start and end timestamps. Proposal-free approaches are\nmore efficient and can be easily applied to videos of varying lengths. On the one hand, some approaches\npredict the probability scores of starting and ending timestamps along the timeline, and choose the\nhighest one as the target moment, such as TMLGA [35], VSLNet [56], DORi [36]. On the other hand,\nsome approaches focus on moment regression, namely directly outputting the final momentâ€™s starting and\nending times, such as ABLR [52], LGI [30], CPNet [22], PMI [7], and HVTG [8]. For the score probability-\nbased approaches, TMLGA [35] and VSLNet [56] projected the query onto the video and then used multi-\nmodal features to predict the probabilities of starting and ending at each timestamp. However, following\nthe moment interval labels, the annotation value of the score curve is 1 only at the starting/ending\npositions; otherwise, 0. In other words, the supervision information provided along the timeline is limited\nand insufficient. This makes it difficult for the model to predict exact score probabilities of starting\nand ending along the timeline. Furthermore, visual reasoning was considered in this task. DORi [36]\ndesigned a language-conditioned spatial graph to model the fine-grained relationship between the visual\nelements of objects and subjects in the videos for visual reasoning. The object information indeed\nhelps in building fine-grained video-text interaction modeling, but the fine-grained feature extraction\nat the object-level requires additional computation and memory. In terms of regression approaches,\nLGI [30] developed a local-global video-text interaction network that performed the interactions between\nvideo and multiple phrases of query in a local-to-global manner for boundary regression. Nonetheless,\nthe language query is divided into multiple phrases, which may limit the comprehension of the query.\nPMI [7] presented a pairwise interaction network that models the modality interaction at the pair of\nsequence-level and channel-level semantics. HVTG [8] used a hierarchical graph composed of object-object\nand object-sentence subgraphs to model fine-grained visual-textual interactions for VG. Both the above\nDORi [36] and HVTG [8] took fine-grained object information into account. DORi [36] predicted the\nscore probabilities while HVTG [8] implemented the moment regression. These methods heavily rely on\nobject detection and have complex calculations, especially in complex scenes. By further observation, in\ncurrent proposal-free methods, a core technique is the attention mechanism, e.g., cross-attention [35,52],\nco-attention [22, 56], self-attention [30], transformer [57], and spatial-temporal graph [36] that are all\nexplored to address this task. The purpose of attention is to enhance visual or textual features through\nvarious interaction mechanisms. Furthermore, there are some new technique attempts,e.g., reinforcement\nlearning-based method to design the reward policy for this tasks [18,46], contrastive learning mechanism\nwith the structured causal model to learn more representative features [31].\n2.2 Transformer for Video Understanding\nThe booming technique transformer is rapidly developed in the field of vision and language, such as fa-\nmous pre-training architectures DETR [4], ViT [13], and VL-BERT [38]. These pre-training architectures\nare typically based on large-scale datasets, and have a large number of parameters and calculation compu-\ntations. Typically, for visual and video understanding tasks, there are TransVG for visual grounding [11],\nViViT for action classification [2] and DETR [4] for moment detection [21], etc. As for the VG task,\nZhang et al. [57] proposed a multi-stage aggregated transformer network (MATN), which utilized the ar-\nchitecture VL-BERT [38] to enhance cross-modal features, and Chen et al. [10] proposed a multi-modal\nlearning framework named DRFT that leveraged a co-attentional transformer and contrastive learning to\nlearn more informative cross-modal features. In the both two studies, the methodological contributions\nwere made to enhance query-video interaction. In other words, current transformer-based research is\nprimarily concerned with how to improve the cross-modal representation ability. The learning of cross-\nmodal correlation improves performance by enhancing visual and textual features. To summarize, both\nSci China Inf Sci 5\nâ€¦\nQuery: The man puts noodles on it and spins them.\nVideo:\nLocalization HeadÃ˜Boundary RegressionVideo-LanguageEncoderVideo-Language Transformer\nÃ˜Fore-/back-ground Classificationğ“›ğ’“ğ’†ğ’ˆ=ğ€ğ“›ğ‘º%ğ‘³ğŸ(ğ’ƒ,ğ’ƒ')+ğœ·ğ“›ğ‘®ğ‘°ğ‘¶ğ‘¼(ğ’ƒ,ğ’ƒ')\nğ“›ğ’„ğ’ğ’”=ğœ¶ğ“›ğ‘¿ğ‘¬(ğ’‚,ğ’‚-)\nâ€¦\nL+T\n[REG] ğ’‡ğ’“\n1,       2,        â€¦,      T\nFFN & Add\nâ€¦â€¦\nparameter sharing\nğ…ğ…ğğœğ¥ğ¬\nğ…ğ…ğğ«ğğ ğ“›ğ’“ğ’†ğ’ˆ\nğ“›ğ’„ğ’ğ’”\nğ‘¸âˆˆğ‘¹ğ‘³Ã—ğ’…ğ’’\nğ‘½âˆˆğ‘¹ğ‘»Ã—ğ’…ğ’—\n[REG] ğ’‡ğ’“5\n[0.4, 0.2]coordinates ğ’ƒ\nÃ—ğ‘µğ’\nprobabilities  ğ’‚\nÃ—ğ‘µğ’„ğ’ğ’ğ’—\nM-H Self-Att& Add\nPositional EncodingLayer Norm1D Conv. & AddLayer Norm\nFFN & AddLayer Norm\nÃ—ğ‘µğ’„ğ’ğ’ğ’—\nM-H Self-Att& Add\nPositional EncodingLayer Norm1D Conv. & AddLayer Norm\nFFN & AddLayer Norm\nGloVe EmbeddingC3D/I3DFeature\nLayer NormMulti-Head Self-Att& AddLayer Norm\nâ€¦ 0123LL+1L+2L+3L+T\nVideo-to-Query AttentionQuery-to-Video Attention\nFigure 3 Overall framework of ViGT for video grounding. â€œM-H Self-Attâ€ in video-language encoder denotes Multi-Head Self-\nAttention operator. First, we use a sharing multi-modal feature encoder to map video and query features into a joint feature\nspace. Subsequently, we investigate a co-attention mechanism ( i.e., query-to-video and video-to-query attentions) to correlate\nthese two features. Next, the correlated query and video features are concatenated with a [REG] token and fed to a Video-\nLanguage Transformer. Finally, merely the [REG] token is used to predict the target boundary (defined as a boundary head\nFFNreg). Simultaneously, the video features are used to constrain foreground or background along the timeline of video (defined as\nclassification head FFNcls). In other words, both the two heads are used during training, while we just use the boundary regression\nhead FFNreg for prediction.\nMATN and DRFT predict the target segment using cross-modal features. From the pipeline perspec-\ntive, MATN [57] belongs to the proposal-based method, whereas DRFT [10] belongs to the proposal-free\nmethod. In contrast to these works, in this work, we propose an end-to-end proposal-free boundary\nregression paradigm using a learnable regression tokenfor VG (described in Section 3). The main idea\nis that we use a learnable token in transformer architecture to detect and retain the relational contexts\nbetween the video and query. When compared to multi-modal features for boundary regression in cur-\nrent attention-based and transformer-based modes, the learnable token regression paradigm is a simple\ninnovative attempt at VG.\n3 Our Approach\nIn this work, we propose a proposal-free video grounding framework through a learnable regression token\nin an end-to-end manner. We deem the video grounding task as a boundary regression problem. A model\n(denoted as F) is required to locate the starting and ending timestamps of the target action or event\nsegment described by language query Q in video V :\n< Ë†ts, Ë†te >= F(V, Q,Î˜), (1)\nwhere Ë†ts and Ë†te denote the starting and ending timestamps respectively, and Î˜ is the parameter collection\nof model F.\nThe overview of our proposal-free video grounding with learnable token method ( ViGT) is shown in\nFigure 3. The ViGT is inspired by the famous transformer architecture. For this task, we elaborate the\nproposed method as below: 1) we employ a sharing feature encoder to map both video and query sequence\ninto a joint feature space, 2) then we perform cross-modal co-attention to highlight the discriminative\nfeatures in each modality, 3) more importantly, we concatenate a learnable regression token [REG] with\nboth query and video features as the input of vision-language transformer, and 4) finally, we utilize the\nregression token [REG] to predict the boundary. Besides, we further use the video features to constrain\nthe probabilities of foreground and background along the timeline of video, where the foreground denotes\nthe occurrence of the queried action or event in the video.\n3.1 Video-Language Encoder\nAs a preliminary operation, we extract original query features Q âˆˆ RLÃ—dq by GloVe embedding [32] and\noriginal video features V âˆˆ RTÃ—dv by using the pre-trained C3D [40] or I3D [5]. Then, we perform a\nvideo-language encoder to further encode the intra- and inter-modal relationships in and between the\nvideo and query.\nSci China Inf Sci 6\n3.1.1 Feature Encoding\nIn this study, we use the transformer-encoder in QANet [50] as a Feature Encoder (FE) layer to embed\nboth the query and video sequences. The encoder consists of a stacked convolutional block for local context\nlearning, multi-head self-attention ( MSA) for long-dependence learning, positional encoding ( PE) [41]\nfor position modeling, and a feedforward layer ( FFN) for feature projection, respectively. Besides, layer\nnorm ( LN) is also applied. Let the input feature sequence as X = {x1, x2, x3, . . . , xN } âˆˆRNÃ—d, the\nfeature encoder is formulated as follow:\nË†X = Feature Encoder(X) â‡”\nï£±\nï£´ï£´ï£´ï£´ï£²\nï£´ï£´ï£´ï£´ï£³\nX\nâ€²\n= PE(X) + X;\nÂ¯X = Conv1D(LN(X\nâ€²\n), K1D, Nconv) + X\nâ€²\n;\nËœX = MSA(LN( Â¯X))+ Â¯X;\nË†X = FFN(LN( ËœX)) + ËœX,\n(2)\nwhere Conv1D(Â·, K1D, Nconv) implements one-dimensional convolution with kernel size ofK1D and layer\nnumber of Nconv.\nGiven the original features of video V ={v1, v2, v3, . . ., vT } âˆˆRTÃ—dv and query Q={q1, q2, q3,. . . , qL}âˆˆ\nRLÃ—dq , we project them into the same dimension d:\n( Ë†Q = Feature Encoder(QWq) âˆˆ RLÃ—d;\nË†V = Feature Encoder(V Wv) âˆˆ RTÃ—d,\n(3)\nwhere Wq âˆˆ RdqÃ—d and Wv âˆˆ RdvÃ—d are two learnable parameters for linear projection. Please note that\nthe parameters of this Feature Encoder (FE) layer are shared between video and query. Up to now,\nwe obtain new encoded features Ë†V âˆˆ RTÃ—d and Ë†Q âˆˆ RLÃ—d.\n3.1.2 Cross-Modal Co-Attention\nThe above feature encoder learns intra-modal relationship in each modality. Here, we use aCross-Modal\nCo-Attention (CMCA) to capture the inter-modal correlation between video and query. The CMCA\nis implemented with a vanilla transformer [41]. Taking video-to-query attention as an example, we target\nto discover responsive visual features Ë†V under the guidance of query Ë†Q, and aggregate them onto the\ntextual dimension. Thus, Qâˆ— is a new textual feature with video aggregation formulated as below:\nQâˆ— = CMCA( Ë†Q, Ë†V ) â‡”\n(\nQ\nâ€²â€²\n= LN\n\u0000\nMSA(Q= Ë†Q; K,V = Ë†V )+ Ë†Q\n\u0001\n;\nQâˆ— = LN\n\u0000\nFFN(Q\nâ€²â€²\n) + Q\nâ€²â€² \u0001\n,\n(4)\nwhere Q, K, and V are three factors in the MSA operation.\nSimilarly, for query-to-video attention, taking video Ë†V as a query, we discover the responsive query\nfeatures in Ë†Q and aggregate them onto the visual dimension. Thus, a new video feature with query\naggregation is calculated as follows:\nV âˆ— = CMCA( Ë†V , Ë†Q) â‡”\n(\nV\nâ€²â€²\n= LN\n\u0000\nMSA(Q= Ë†V ; K,V = Ë†Q)+ Ë†V\n\u0001\n;\nV âˆ— = LN\n\u0000\nFFN(V\nâ€²â€²\n) + V\nâ€²â€² \u0001\n.\n(5)\n3.2 Video-Language Transformer\nAfter the above-mentioned feature encoding, the enhanced feature representation is more discriminative.\nHere, we add a learnable token [REG] and apply a video-language transformer to model the relationships\namong the token, query and video features. Specifically, the video-language transformer consists of Nl\ntransformer encoder blocks with multi-head self-attention ( MSA), feedforward layer (FFN), layernorm\n(LN), and residual connections. Let the regression token [REG] be fr âˆˆ Rd, we concatenate it with\nSci China Inf Sci 7\nthe video feature V âˆ— = [ vâˆ—\n1, Â·Â·Â· , vâˆ—\nT ] and query feature Qâˆ— = [ qâˆ—\n1, Â·Â·Â· , qâˆ—\nL]. The transformer block is\nformulated as follows: ï£±\nï£´ï£´ï£²\nï£´ï£´ï£³\nz0 = [fr; qâˆ—\n1, Â·Â·Â· , qâˆ—\nL; vâˆ—\n1, Â·Â·Â· , vâˆ—\nT ]+fpos;\nzâ€²n = MSA(LN(znâˆ’1)) + znâˆ’1;\nzn = FFN(LN(zâ€²n)) + zâ€²n,\n(6)\nwhere n âˆˆ [1, Nl], fpos âˆˆR(L+T+1)Ã—d denotes the initialized position embedding [41], and we add it to the\ninput feature sequence for temporal modeling. The regression token [REG] is randomly initialized and\noptimized with the full model.\n3.3 Localization Head\nIn this work, we consider two types of localization head for video grounding, i.e., a boundary regression\nhead and a fore-/back-ground classification head. For boundary regression, we define a feedforward\nunit as FFNreg, which consists of three fully-connected layers and a sigmoid activation. Through the\nregression head FFNreg, we obtain the predicted target moment. Specifically, we input the token [REG]\nË†fr into FFNreg, where Ë†fr âˆˆ Rd is the output of video-language transformer as shown in Figure 3. The\npredicted target moment b is calculated by:\nb = FFNreg( Ë†fr), (7)\nwhere b âˆˆ [0, 1]2 denotes the normalized center and width coordinates of the target moment.\nTo achieve the semantic alignment between video and query, we design an additional head, i.e., fore-\n/back-ground classification headdenoted as FFNcls. We assign 1 to foreground (i.e., target segment)\nand 0 to background, and then predict a confidence score a âˆˆ RT along the timeline of video. In other\nwords, a is used to judge each visual feature whether belong to foreground or not (background). Here, we\ndesign a feedforward unit consisting of a layer fully-connected layer and a sigmoid activation as FFNcls.\na = FFNcls( Ë†V âˆ—), (8)\nwhere Ë†V âˆ— âˆˆ RTÃ—d denotes the output visual features from the video-language transformer as shown in\nFigure 3.\n3.4 Training\nAccording to the localization head, we design a multi-task loss. For the boundary regression head, we\nset two objective terms. The first term is Smooth-l1 loss, which is widely applied to estimate the target\nmoment in previous works [22, 52]. The second term is Generalized IoU (GIoU) loss originated from\nobject detection [34]. We apply it in this field to supervise the target boundary. The total regression loss\nLreg is formulated as follows:\nLreg = Î»Lsâˆ’l1 (Î¦(b), Î¦(Ë†b)) + Î²Lgiou(b,Ë†b), (9)\nwhere Î» and Î² are trade-off hyper-parameters, Ë†b âˆˆ [0, 1]2 is the normalized center and width coordinates of\ntarget moment in ground-truth, and function Î¦( Â·) converts the target moment from the format <center,\nwidth> to format <start, end>. For the GIoU term, we calculate it with 1D temporal boundary <center,\nwidth> by replacing 2D bounding box<center x, center y, width, height> in the IoU calculation of object\ndetection [34].\nIn addition, we define a classification loss Lcls to supervise the confidence score a in the fore-/back-\nground classification head. The loss Lcls is implemented as follows:\nLcls = Î±LXE (a, Ë†a), (10)\nLXE (a, Ë†a) = 1\nN\nX\ni\nâˆ’[Ë†a Â· log(a) + (1âˆ’ Ë†a) Â· log(1 âˆ’ a)], (11)\nwhere Î± is a balance hyper-parameter, a denotes the predicted probability of each visual feature belong\nto foreground, and Ë†a is the ground-truth. To summarize, the multi-task loss is formulated as in Eq. 12.\nPlease note that with the two localization heads, Lreg and Lcls are both used during training. For target\nboundary prediction, we only use the boundary regression head for prediction.\nL = Lreg + Lcls. (12)\nSci China Inf Sci 8\nTable 1 Performance comparison on the ANet-Captions dataset with C3D features. â€œCore Techâ€ records the methodological\ntechnique. â€œTransformer with MLMâ€ denotes using Masked Language Modeling (MLM) in the transformer training. DRFT â€ \ndenotes the DRFT model trained with rich features of RGB, depth and optical flow data.\nMethod Venue Core Tech IoU@0.3â†‘ IoU@0.5â†‘ IoU@0.7â†‘ mIoUâ†‘\nMCN [1] ICCVâ€™17 Sliding window 39.35 21.36 6.43 15.83\nCTRL [14] ICCVâ€™17 Sliding window 47.43 29.01 10.34 20.54\nTGN [6] SIGIRâ€™18 Interaction 45.51 28.47 â€“ â€“\nSCDM [51] NIPSâ€™19 Interaction 54.80 36.75 19.86 â€“\nCMIN [59] SIGIRâ€™20 Attention 63.61 43.40 23.88 â€“\nCBP [43] AAAIâ€™20 Interaction 54.30 35.76 17.80 36.85\n2D-TAN [58] AAAIâ€™20 2D Temporal Map 59.45 44.05 27.38 â€“\nDRN [53] CVPRâ€™20 Dense regression â€“ 45.45 24.36 â€“\nMATN [57] CVPRâ€™21 Transformer â€“ 46.26 28.82 â€“\nMATNâ€¡ [57] CVPRâ€™21 Transformer with MLM â€“ 48.02 31.78 â€“\nProposal-based methodsCBLN [24] CVPRâ€™21 Multi-interaction 66.34 48.12 27.60 â€“\nMI-PUL [12] TIPâ€™22 2D Temporal Map 60.15 46.35 28.13 â€“\nMGPN [39] SIGIRâ€™22 2D Temporal Map â€“ 47.92 30.47 â€“\nHCLNet [54] ACM MMâ€™22 Contrastive learning 63.05 45.82 26.79 44.89\nMethod Venue Core Tech IoU@0.3â†‘ IoU@0.5â†‘ IoU@0.7â†‘ mIoUâ†‘\nABLR [52] AAAIâ€™19 Attention 55.67 36.79 â€“ 36.99\nDEBUG [29] EMNLPâ€™19 Attention 55.91 39.72 â€“ 39.51\nTripNet [17] BMVCâ€™20 Reinforcement Learning 48.42 32.19 13.93 â€“\nTMLGA [35] WACVâ€™20 Attention â€“ 33.04 19.26 37.78\nLGI [30] CVPRâ€™20 Attention 58.52 41.51 23.07 41.13\nPMI [7] ECCVâ€™20 Interaction 59.69 39.37 19.27 â€“\nHVTG [8] ECCVâ€™20 Visual-textual Graph 57.60 40.15 18.27 â€“\nVSLNet [56] ACLâ€™20 Attention 63.16 43.22 26.16 43.19\nDORi [36] WACVâ€™21 Spatial-temporal Graph 57.89 41.49 26.41 42.78\nBPNet [47] AAAIâ€™21 2D Temporal Map 58.98 42.07 24.69 42.11\nCPNet [22] AAAIâ€™21 Attention â€“ 40.56 21.63 40.65\nDRFT [10] NeurIPSâ€™21 Transformer 60.25 42.37 25.23 43.18\nDRFTâ€  [10] NeurIPSâ€™21 Transformer 62.91 45.72 27.79 45.86\nIVG [31] CVPRâ€™21 Contrastive Learning 63.22 43.84 27.10 44.21\nProposal-free methods\nOurs â€“ Transformer 64.59 46.71 26.90 45.71\n4 Experiments\nIn this section, we make a comparison with previous works. Ablation studies are conducted to investigate\nthe effectiveness of our method. We also show qualitative results to demonstrate the interpretability of\nour method.\n4.1 Experimental setup\nDatasets and Evaluation. We experiment on three public benchmark datasets, and introduce datasets\nin detail as follows.\nâ€¢ ActivityNet Captions (ANet-Captions) [20] consists of 20K videos that are collected from\nYouTube. The average duration of video is about 2 minutes, and each video are annotated with 3.65\nqueries averagely. Following [56], the â€œtrainâ€ set is used for training, â€œval 1â€ set for validation, and\nâ€œval 2â€ set for test. Specifically, the dataset is split into the train, validation and test sets of 37,421,\n17,505, and 17,031 query-clip pairs. We notice that this dataset is released for the dense video captioning\ntask [20,45] including numerous segment-captioning pairs, and then is extended for video grounding.\nâ€¢ TACoS [14] consists of more challenging cooking videos. There are 127 average 4.79 minutes-long\nvideos and each video has around 178 queries. Compared with the ANet-Captions dataset, the TACoS\nhas denser queries on each video. This dataset is split int 10,146, 4,589, and 4,083 query-clip pairs for\ntrain, valid, and test sets, respectively.\nâ€¢ YouCookII [60] consists of 2,000 untrimmed long videos that are collected from YouTube. Videos\nin this dataset are collected in the cooking scenarios as in the TACoS dataset too. The average duration\nSci China Inf Sci 9\nTable 2 Performance comparison on the TACoS dataset with C3D features.â‹† denotes using I3D features provided by VSLNet [56],\nand â‹„ denotes using C3D features provided by 2D-TAN [58].\nMethod Venue Core Tech IoU@0.1â†‘ IoU@0.3â†‘ IoU@0.5â†‘ mIoUâ†‘\nMCN [1] ICCVâ€™17 Sliding window â€“ â€“ 5.58 â€“\nCTRL [14] ICCVâ€™17 Sliding window 24.32 18.32 13.30 â€“\nTGN [6] SIGIRâ€™18 Interaction 41.87 21.77 18.90 â€“\nSCDM [51] NIPSâ€™19 Interaction â€“ 26.11 â€“ â€“\nCMIN [59] SIGIRâ€™20 Attention 32.48 24.64 18.05 â€“\nCBP [43] AAAIâ€™20 Interaction â€“ 27.31 24.79 21.59\n2D-TANâ‹„ [58] AAAIâ€™20 2D Temporal Map 47.59 37.29 25.23 â€“\nDRN [53] CVPRâ€™20 Dense regression â€“ â€“ 23.17 â€“\nProposal-based methods\nMATN [57] CVPRâ€™21 Transformer â€“ 45.64 34.79 â€“\nMATNâ€¡ [57] CVPRâ€™21 Transformer with MLM â€“ 48.79 37.57 â€“\nCBLN [24] CVPRâ€™21 Multi-interaction 49.16 38.98 27.65 â€“\nMI-PUL [12] TIPâ€™22 2D Temporal Map 52.05 41.03 31.47 â€“\nMGPN [39] SIGIRâ€™22 2D Temporal Map â€“ 48.81 36.74 â€“\nHCLNet [54] ACM MMâ€™22 Contrastive learning 62.03 50.04 37.89 34.80\nMethod Venue Core Tech IoU@0.1â†‘ IoU@0.3â†‘ IoU@0.5â†‘ mIoUâ†‘\nABLR [52] AAAIâ€™19 Attention â€“ 19.50 â€“ 13.40\nTripNet [17] CVPRWâ€™19 Reinforcement Learning â€“ 23.95 19.17 â€“\nSM-RL [46] CVPRâ€™19 Reinforcement Learning 26.51 20.25 15.95 â€“\nDEBUG [29] EMNLPâ€™19 Attention 41.15 23.45 â€“ 16.03\nVSLNetâ‹† [56] ACLâ€™20 Attention â€“ 29.61 24.27 24.11\nDORi [36] WACVâ€™21 Spatial-temporal Graph â€“ 28.69 24.91 26.42\nBPNet [47] AAAIâ€™21 2D Temporal Map â€“ 25.96 20.96 19.53\nCPNetâ‹„ [22] AAAIâ€™21 Attention â€“ 42.61 28.29 28.69\nIVG [31] CVPRâ€™21 Contrastive Learning 49.36 38.84 29.07 â€“\nOursâ‹† â€“ Transformer 57.56 45.34 31.77 30.82\nProposal-free methods\nOursâ‹„ â€“ Transformer 55.59 45.99 32.32 30.97\nof video is 5.26 minutes and each video has 7.73 queries on average.\nEvaluation Metrics. Following the protocol [14, 30], we adopt two metrics â€œR@n, IoU@mâ€ to\nrepresent the ratio of queried moments having IoU (Interaction-Over-Union) larger than threshold m in\nthe top-n predicted ones:\nR@n, IoU@m = 1\nNq\nNqX\ni=1\nr(n, m, qi), (13)\nwhere Nq denotes the query number andqi denotes the i-th query. The rule is that r(n, m, qi)=1 if the IoU\nvalue of qi with ground-truth is larger than m, otherwise r(n, m, qi)=0. â€œmIoUâ€ represents the average\nIoU for all the queries. Since our ViGT is a proposal-free method, we report the experimental results at\nn=1, where m âˆˆ {0.5, 0.7} on the ActivityNet Captions and YouCookII datasets and m âˆˆ {0.3, 0.5} [56]\non the TACoS dataset. â€œR@n, IoU@mâ€ is abbreviated to â€œIoU@mâ€ in experimental results.\nImplementation Details. We conduct the same experiment setting on above three datasets. We use\na pre-trained 3D network (i.e., C3D [40] or I3D [5]) to extract original video features. The clip number of\neach video is set to 128. Each word in the query sentence are embedded into a 300-dim vector by GloVe\nembedding [32]. The batch size is set to 100. For the 1D convolutional operation in Feature Encoder, the\nkernel size K1D and layer number Nconv are set to 7 and 4, respectively. The number of transformer block\nis set to Nl = 6. The hyper-parameter Î», Î²and Î± in Eqs. 9-10 are set to 0.5, 1.0 and 2.0, respectively.\nThe parameter of the model is optimized with Adam optimizer with learning rate of 1 Ã—10âˆ’4. We set\nthe dropout ratio to 0.1 for multi-head self-attention in the video-language transformer. To facilitate the\nimplementation of the proposed model, the ground-truth values of starting and ending timestamps are\nnormalized to [0, 1].\n4.2 Main Comparison\nTo validate the effectiveness of the proposed ViGT, we compare it with the state-of-the-art methods on\nthe aforementioned three datasets, involving: 1) Proposal-based methods: CTRL [14], MCN [1], TGN [6],\nSci China Inf Sci 10\nTable 3 Performance comparison on the YouCookII dataset.\nMethod Venue Core Tech IoU@0.5â†‘ IoU@0.7â†‘ mIoUâ†‘\nRandom WACVâ€™20 â€“ 1.72 0.60 -\nTMLGA [35] WACVâ€™20 Attention 20.65 10.94 23.07\nOurs - Transformer 27.18 12.17 27.61\nFeature Encoder\nQuery\nFeature Encoder\nVideo\n12L â€¦â€¦\nVideo-to-Query AttVideo-to-Query Att\nVideo-Language Transformer\nâ€¦\nTemporal Attentive \nPooling FFNreg\nğ“›ğ’“ğ’†ğ’ˆ\n[0.4, 0.2]\n12L â€¦â€¦\nVideo-Language Transformer\nFFNreg\nğ“›ğ’“ğ’†ğ’ˆ\n[0.4, 0.2]\n(a) ViGT w/o token (b) ViGT w/o FE & CMCA 0\nCoordinateğ’ƒ\nCoordinateğ’ƒ\nGloVe EmbeddingFeature Encoder\nQueryVideo\nGloVe EmbeddingFeature Encoder\nFeature EncoderFeature Encoder\n12L â€¦â€¦\nVideo-Language Transformer\nFFNreg\nğ“›ğ’“ğ’†ğ’ˆ\n[0.4, 0.2]\n(c) ViGT w/o CMCA 0\nCoordinateğ’ƒ\nQueryVideo\nGloVe EmbeddingFeature Encoder\nFFNcls\nâ€¦\nğ“›ğ’„ğ’ğ’”\nğ’‚\nFFNcls\nâ€¦\nğ“›ğ’„ğ’ğ’”\nğ’‚\n12L â€¦â€¦\nL+2L+T L+1\nVideo-to-Query AttVideo-to-Query Att\nVideo-Language Transformer\nFFNreg\nğ“›ğ’“ğ’†ğ’ˆ\n[0.4, 0.2]\n(d) ViGT w/o FE 0\nCoordinateğ’ƒ\nQueryVideo\nGloVe EmbeddingFeature Encoder\nFFNcls\nâ€¦\nğ“›ğ’„ğ’ğ’”\nğ’‚\n[REG]\n[REG]\n[REG]\nL+2L+T L+1\nL+2L+T L+1\nL+2L+T L+1\nFigure 4 Various architectures of â€œ ViGTâ€ in ablation experiments. â€œ ViGT w/o tokenâ€ is used to verify the effectiveness of\nregression token [REG]. â€œ ViGT w/o FE&CMCAâ€ denotes the removal of both FE (Feature Encoder) and CMCA (Cross-Modal\nCo-Attention) layers in video-language encoder. â€œ ViGT w/o CMCAâ€ and â€œ ViGT w/o FEâ€ are used to verify the effects of FE\nand CMCA layers separately.\nCMIN [59], SCDM [51], CBP [43], 2D-TAN [58], DRN [53], MATN [57] CBLN [24], MI-PUL [12],\nMGPN [39], HCLNet [54], and 2) Proposal-free methods: ABLR [52], TripNet [17], SM-RL [46], DE-\nBUG [29], TMLGA [35], LGI [30], PMI [7], HVTG [8], VSLNet [56], BPNet [47], CPNet [22], DORi [36],\nDRFT [10], IVG [31]. It is well known that proposal-based methods achieve promising performance, but\nare accompanied by numerous proposal candidates and expensive post-processing. Proposal-free methods\nimplemented in an end-to-end manner always retain a few hyperparameters, which can save computation\ncost.\nTable 4 Ablation studies of token, FE (Feature Encoder)\nand CMCA (Cross-Modal Co-Attention) layers on the ANet-Captions dataset.\ntoken FE CMCA IoU@0.3â†‘ IoU@0.5â†‘ IoU@0.7â†‘ mIoUâ†‘\n- âœ“ âœ“ 64.29 44.72 24.78 45.04\nâœ“ - - 63.60 44.84 24.68 44.56\nâœ“ âœ“ - 64.32 46.05 25.67 45.23\nâœ“ - âœ“ 63.67 45.83 26.28 45.11\nâœ“ âœ“ âœ“ 64.59 46.71 26.90 45.71\nObserving Tables 1âˆ¼3, ViGT achieves competitive performances. On the large-scale dataset ANet-\nCaptions [20], it performs the best mIoU with 45.71 on proposal-free methods including DRFT except\nDRFTâ€ , where DRFTâ€  introduce extra data, i.e., depth and optical flow features. Even this, ViGT sur-\npasses DRFTâ€  at IoU@0.5. On the TACoS dataset,ViGT achieves the best among proposal-free methods\nand outperforms most proposal-based methods except for MATN â€¡ [57], MGPN [39], and HCLNet [54],\nwhere MATNâ€¡ is a transformer-based model with MLM training technique (masked language modeling)\nto extract candidate proposals, MGPN utilizes more complicated coarse- and fine- grained features to\ngenerate candidate proposals, and HCLNet use hierarchical contrastive learning to align video and query.\nOur approach is a simple and light end-to-end transformer model without MLM training; it achieves\ncomparable performance with MATN ( e.g., 45.99 v.s. 45.64). Turning to the challenging proposal-free\ncomparison on the TACoS dataset again, ViGT has more significant improvements than existing mod-\nels including CPNet, e.g., lift 28.69 to 30.97 with 8.0% improvement on mIoU, and lift 28.29 to 32.32\n14.3% improvement on IoU@0.5. On the YouCookII dataset, our proposed method achieves the best\nperformance with â€œIoU@0.7â€ of 12.17 and â€œmIoUâ€ of 27.61. These results demonstrate the effectiveness\nof our method for video grounding. Its superiority provides researchers with the token learning insight\nto regard regression models in this field.\nSci China Inf Sci 11\nTable 5 Ablation studies of the feature dimension d in Video-Language Transformer on the ANet-Captions dataset.\nMethod IoU@0.3â†‘ IoU@0.5â†‘ IoU@0.7â†‘ mIoUâ†‘\nd=128 63.84 45.04 24.36 44.62\nd=256 64.50 45.44 25.84 45.20\nd=512 64.59 46.71 26.90 45.71\nTable 6 Ablation studies of the layer number Nl in Video-Language Transformer on the ANet-Captions dataset.\nMethod IoU@0.3â†‘ IoU@0.5â†‘ IoU@0.7â†‘ mIoUâ†‘\nNl=1 65.35 44.87 22.44 44.62\nNl=2 64.80 45.58 25.21 45.03\nNl=4 64.32 46.08 25.77 45.27\nNl=6 64.59 46.71 26.90 45.71\nNl=8 63.58 45.53 25.76 44.92\n4.3 Ablation Study\n1300\n1400\n1500\n1600\n1700\n1800\n[0.1, 0.2)[0.2, 0.3)[0.3, 0.4)[0.4, 0.5)[0.5, 0.6)[0.6, 0.7)[0.7, 0.8)[0.8, 0.9)[0.9, 1.0]\nViGT w/o token ViGT\nIoU threshold\nNumber of results\nFigure 5 The statistics of predicted results across different IoU\nranges on the ANet-Captions dataset. Compared with â€œ ViGT\nw/o tokenâ€, â€œ ViGTâ€ predicts high-quality segments, especially\nat higher IoU ranges.\nIn this subsection, we conduct adequate ablation\nstudies to validate the effectiveness of ViGT, in-\nvolving: 1) learnable token [REG], 2) FE and\nCMCA layers in video-language encoder, 3) pa-\nrameters Nl and d of video-language transformer,\n4) parameter sharing in FE, 5) the order of FE\nand CMCA layers, 6) two objective losses Lreg\nand Lcls, and 7) inference speed. The variants of\nthe proposed model used in these ablation exper-\niments are shown in Figure 4.\n4.3.1 Learnable token\nTo verify the effectiveness of token [REG], we\nremove it and feed the transformed feature\nË†V âˆ—âˆˆRTÃ—d into an attentive-regression module (widely used in recent proposal-free methods [22, 30])\nfor boundary prediction. This ablation model is named â€œ ViGT w/o tokenâ€, and its architecture is\nshown in Figure 4 (a). Observing the first row of Table 4, regression without token learning leads to\nobvious performance degradation at IoU@0.5/0.7. This meets our intention of using a token to learn\nmore informative relations between video and query sequences.\nIn addition, we perform statistics on the prediction results across different IoU ranges. As shown in\nFigure 5, we illustrate the IoU distribution of all the prediction results on the ANet-Captions dataset.\nFrom Figure 5, we can obverse that â€œ ViGTâ€ predicts more high-quality segments than â€œ ViGT w/o\ntokenâ€, especially at the range of [0.8, 0.9). The higher the IoU, the more accurate the prediction result.\nThe results reflect that the regression with token learning is more efficient than the multi-modal feature\nregression.\n4.3.2 FE and CMCA layers\nAfter that, we test the effect of feature encoder. In the setting of â€œ ViGT w/o FE&CMCAâ€, we input\noriginal video and query features into the Video-Language Transformer. We also test â€œViGT w FEâ€ and\nâ€œViGT w CMCAâ€. From Tables 4 and 1, we observe that â€œViGT w/o FE&CMCAâ€ surpasses most\nproposal-free methods. By adding FE, the â€œViGT w FEâ€ shows a significant improvement on metrics\nIoU@0.3 and IoU@0.5, which means that the FE layer benefits the accuracy improvement of massive\nprediction results with low intersection values. The â€œ ViGT w CMCAâ€ yields a significant boost on\nmetric IoU@0.7 with high intersection values; it means that CMCA can enhance the correlation of\ncrucial visual and textual cues. Anyway, the full model â€œ ViGT w FE&CMCAâ€ performs the best.\nSci China Inf Sci 12\nTable 7 Ablation studies of parameter sharing in FE (Feature Encoder) layer on the ANet-Captions dataset. ViGT-unshare\nrepresents that the weight of feature encoder is unshared.\nSetting IoU=0.3 â†‘ IoU=0.5 â†‘ IoU=0.7 â†‘ mIoU â†‘\nViGT-unshare 64.57 44.96 24.49 45.17\nViGT-share (Ours) 64.59 46.71 26.90 45.71\nTable 8 Ablation studies of the order of FE (Feature Encoder) and CMCA (Cross-Modal Co-Attention) layers on the ANet-\nCaptions dataset.\nSetting IoU=0.3 â†‘ IoU=0.5 â†‘ IoU=0.7 â†‘ mIoU â†‘\nViGT (CMCA â†’ FE) 61.13 40.83 20.96 42.33\nViGT (FE â†’ CMCA) 64.59 46.71 26.90 45.71\nTable 9 Ablation studies of different losses on ANet-Captions dataset.\nLsâˆ’l1 Lgiou Lcls IoU@0.3â†‘ IoU@0.5â†‘ IoU@0.7â†‘ mIoUâ†‘\nâœ“ - - 57.07 37.53 17.44 37.84\n- âœ“ - 59.66 32.04 9.45 36.70\nâœ“ - âœ“ 57.53 39.53 21.48 39.89\n- âœ“ âœ“ 65.89 45.59 24.45 45.47\nâœ“ âœ“ - 63.15 47.47 24.57 43.15\nâœ“ âœ“ âœ“ 64.59 46.71 26.90 45.71\nTable 10 Computation comparison of inference speed and required GPU memory on the ANet-Captions dataset.\nModel Batch size Speed â†“ Memory â†“ IoU@0.5â†‘ mIoUâ†‘\n2D-TAN [58] 100 0.0732s 8401M 44.51 -\nViGT (Ours) 100 0.0018s 3755M 46.71 45.71\n4.3.3 Parameters of Video-Language Transformer\nWe test the layer number of transformer Nl âˆˆ {1, 2, 4, 6, 8}. As shown in Table 6, the best performance\nis obtained with Nl = 6. We set it as the optimal setting. For the feature dimension d, as shown in\nTable 6, the ViGT gets the best results while d =512.\n4.3.4 Parameter sharing in FE layer\nSince the task targets to predict the coordinates of the queried action in the video, the model is required\nto understand the cross-modal correlation between video and query. In the feature encoding (FE) stage,\nthe purpose of sharing weights is to map the features of video and query into the same feature space,\nand then facilitate the subsequent cross-modal correlation. If without the parameter sharing strategy,\nthe feature encoding of each modality can be taken as independent feature embedding. Actually, the\nquality of original features is relatively good, and the independent feature refinement just has a slight\neffect on performance improvement. We conducted the experiment that the model without share weights\nstrategy, and the results are listed in Table 7. We can see that sharing weights improves the performance\non IoU=0.7 by a large margin (i.e., from 24.49 to 26.90). In other words, the parameter sharing strategy\nis used as a data preparation process for the subsequent cross-modal correlation. These results validate\nthe effectiveness of parameter sharing in the feature encoder layer.\n4.3.5 Order of FE and CMCA layers\nTo verify the effectiveness of the order of FE and CMCA layers, we conducted the experiment of the\nmodel that first conducts CMCA and then implements FE. The experimental results are listed in Table 8.\nWe can see that if first conducts CMCA and then implements FE, the performance of the model will\ndrop by a large margin ( e.g., mIoU from 45.71 to 42.33, and IoU=0.7 from 26.90 to 20.96). As stated\nin the above discussion, the parameter sharing strategy benefits the subsequent cross-modal co-attention\nby first mapping them into a same feature space. Thus, we first use the FE to enhance the features of\nvideo and query through parameter sharing and then implement the CMCA.\nSci China Inf Sci 13\nQuery 1: He hits the balls one last time.\n55.1s 87.7s\n67.7s\n57.1s\n89.5s\n89.5s\n35.5s 113.7s\n77.7s\n26.6s 123.9s\n1.8s\nQuery 2: He is holding an archery set and talking.\nGT\nViGT\nViGT w/o token\nGT\nViGT\nViGT w/o token\n30.9s 61.4s\n42.5s 67.8s\n31.6s\nQuery 3: Then she ties the shoe on the right.\n67.8s\nGT\nViGT\nViGT w/o token\n38.5s 76.4s\n77.7s\n22.8s 79.2s\n5.3s\nQuery 4: He starts to paint the board.\nGT\nViGT\nViGT w/o token\nFigure 6 Qualitative results of â€œ ViGTâ€ and â€œ ViGT w/o tokenâ€ on the ANet-Captions dataset. Compared with with â€œ ViGT\nw/o tokenâ€, the proposed â€œ ViGTâ€ locates the target segment more accurately in different scenarios.\nQuery 5: A athlete practices his long jump on a track and field course with some light jumps.\n23.1s0.0s\n22.9s\n0.0s\nlayer\nQuery 6: He puts noodles into the pot.\n75.9s41.0s\n47.0s 75.9s\nlayer\n0.0 1.0\nQuery 7: He is using a rag to clean a sink.\n70.1s15.8s\n14.4s 64.7s\nlayer\nâ€œlong jumpâ€\n â€œlong jumpâ€\n â€œlong jumpâ€\nâ€œlight jumpâ€\n0.0 1.0\nGT\nViGT\n0.0 1.0\nGT\nViGT\nâ€œlight jumpâ€\n â€œlong jumpâ€\nTokenâ€™s attention \non video\nQuery-to-video \nattention \nInput query \nand video\nGT\nViGTGrounding results\nTokenâ€™s attention \non video\nQuery-to-video \nattention \nInput query\nand video\nGrounding results\nTokenâ€™s attention \non video\nQuery-to-video \nattention \nInput query \nand video\nGrounding results\nFigure 7 Visualization of [REG] tokenâ€™s attention on the video in video-language transformer and query-to-video attention in\nCMCA. Different from query-to-video attention of CMCA attending almost frames evenly, the video-language transformer performs\ndifferent visual preference and progressively attends to more discriminative frames.\n4.3.6 Objective lossesLreg and Lcls\nAt first, we discuss the regression objective Lreg, which consists of two terms Lsâˆ’l1 and Lgiou. As shown\nin rows 1-2 of Table 9, the proposed model with either single Lsâˆ’l1 or Lgiou can achieve unsatisfactory\nperformances. While injecting the classification loss Lcls, under Lsâˆ’l1&Lcls and Lgiou&Lcls, the perfor-\nmances improve stably as shown in rows 3-4 of Table 9. Obviously, Lcls has a significant impact. Besides,\nplease notice that Lgiou is a new term originated from object detection. In the video grounding task, the\nmodel with only Lgiou performs the worst, such as 9.45 at IoU@0.7, but the combinations of Lgiou with\nSci China Inf Sci 14\nHe puts noodles\ninto the pot.\nA athlete practices his long \njump on a track and field \ncourse with some light \njumps.\nQuery 5: A athlete practices his long jump on a track and field course with some light jumps.\nQuery 6: He puts noodles into the pot.\nlight\nA\nfiled jumps\nputs\nGT\n22.9s\n0.0s\n41.0s\n47.0s\n75.9s\n75.9s\nViGT 0.0s\n23.1s\nTokenâ€™s attention on\nquery and video \nCMCA attention\nInput query \nand video\nGrounding results\nlayer\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\nlayer\nGT\nViGT\n0.0 1.0\n0.0 1.0\nTokenâ€™s attention on\nquery and video \nCMCA attention\nInput query \nand video\nGrounding results\nFigure 8 Visualization of [REG] tokenâ€™s attention on both video and query in video-language transformer and CMCA modules.\nIn this figure, we display the attention distribution in the term of attention curves. For Queries 5 and 6, the CMCA and video-\nlanguage transformer exhibit opposite properties, where the CMCA benefits for discovering query cues and the video-language\ntransformer benefits for discovering video cues. In summary, ViGT progressively searches relevant visual and textual cues, and\npredicts the target moments accurately.\nany others all achieve promising performances, such as Lsâˆ’l1&Lgiou, Lgiou&Lcls, or Lsâˆ’l1&Lgiou&Lcls.\nLgiou is an effective auxiliary loss for this task. As a conclusion, ViGT with the all the loss terms reaches\na large margin improvement, e.g., raising the mIoU up to 45.71.\n4.3.7 Inference speed of the proposed model\nWe compare our method with a typical proposal-based method 2D-TAN [58]. In Table 10, the speed\nrepresents the average inference time per query, and the memory denotes the GPU memory consumption\nof each model. Compared with the 2D-TAN, the ViGT consumes less GPU memory, i.e., 3755M vs.\n8401M. In addition, the proposed ViGT outperforms 2D-TAN by a large margin in the term of inference\nspeed, i.e., 0.0018s vs. 0.0732s per query. The faster inference speed and lower memory consumption\ndemonstrate the outstanding superiority of ViGT without extra proposal post-processing.\n4.4 Qualitative Results\nTo demonstrate the effectiveness of interpretability ofViGT, we display some video samples in Figures 6,\n7, and 8. At first, as shown in Figure 6, we display four qualitative results from the ANet-Captions dataset.\nFor â€œQuery 1: He hits the balls one last time.â€ and â€œ Query 3: Then she ties the shoe on the right.â€, both\nâ€œViGT w/o tokenâ€ and ViGT locate the target moment, but ViGT predicts more accurate boundaries\nthan â€œViGT w/o tokenâ€. Compared with the ground-truth and ViGT, â€œViGT w/o tokenâ€ predicts\nmuch more narrowing boundaries in Queries 1&3. For â€œ Query 2: He is holding an archery set and\ntalking.â€ and â€œ Query 4: He starts to paint the board.â€, obviously, the â€œ ViGT w/o tokenâ€ predicts the\ninexact moments, while ViGT precisely locate the action in the videos. From the above qualitative\nresults, the token can effectively aggregate global semantics of video and query for video grounding.\nIn Figure 7, we illustrate the [REG] tokenâ€™s attention on the visual sequence step by step. The upper\nsix rows display the tokenâ€™s attention maps on the video from the layer-0 to layer-5 of the video-language\ntransformer, and the bottom row reflects the query-to-video attention map of CMCA. Taking Queries\n5âˆ¼7 as examples, either for the videos in Queries 5âˆ¼6 having continuous changing frames or the video\nin Query 7 having similar frames, the query-to-video pays attention to almost frames evenly. This is\nSci China Inf Sci 15\nnot in line with the queried moment. By comparison, the tokenâ€™s attention shows significant disparity,\nwhich searches the potential segments in the six layers of transformer progressively and finally predicts\nthe target segment accurately.\nTo further display the tokenâ€™s effectiveness, we show the textual attention curves of CMCA and video-\nlanguage transformer in Figure 8. Observing the CMCA and the video-language transformer, the con-\ntributive textual attention is conducted by CMCA. For the video-language transformer, it no longer pays\nattention to any word after the 3- th layer of Query 5 and the 2- th layer of Query 6. The CMCA and\nvideo-language transformer perform oppositely, where the CMCA module benefits for discovering query\ncues and the video-language transformer modules benefits for discovering video cues. To further validate\nthis, we also show the visual attention curves on video in Figure 8. Taking Query 5 as an example, it is\na challenging video, in which visual appearances of â€œ light jumpâ€ and â€œ long jumpâ€ are easily confusing.\nThe textual attention of CMCA is useful, but the visual attention of CMCA is useless as it attends\nalmost frames eventually. By comparison with the CMCA, the visual attention of transformer is much\nmore distinctive and useful. Specifically, the video-language transformer firstly attends to the segments\ncovering â€œslight jumpâ€ at {0, 1}-th layers. Then, it attends to the segments covering â€œ long jumpâ€ at\n{2, 3, 4}-th layers. Eventually, ViGT locates the correct segment of â€œ slight jumpsâ€ at the last layer.\n5 Conclusion\nIn this paper, we investigate a learnable regression token for video grounding and propose a novel\ntransformer-based proposal-free framework namedViGT. Token learning is a new methodological paradigm\nfor video grounding that predicts the target moment using a learnable token rather than multi-modal\nfeatures or cross-modal features, as in previous works. Extensive experiments demonstrate that the to-\nken can learn informative semantics from videos and queries. The visualization results also show the\ninterpretability of our method.\nAcknowledgements This work was supported by the National Natural Science Foundation of China (72188101, 62020106007,\nand 62272144, and U20A20183), and the Major Project of Anhui Province (202203a05020011).\nReferences\n1 Anne Hendricks L, Wang O, Shechtman E, et al. Localizing moments in video with natural language. In: Proceedings of the\nIEEE International Conference on Computer Vision. 2017, 5803â€“5812\n2 Arnab A, Dehghani M, Heigold G, et al. Vivit: A video vision transformer. In: Proceedings of the IEEE International\nConference on Computer Vision. 2021, 6836â€“6846\n3 Buch S, Escorcia V, Shen C, et al. Sst: Single-stream temporal action proposals. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. 2017, 2911â€“2920\n4 Carion N, Massa F, Synnaeve G, et al. End-to-end object detection with transformers. In: Proceedings of the European\nConference on Computer Vision. 2020, 213â€“229\n5 Carreira J and Zisserman A. Quo vadis, action recognition? a new model and the kinetics dataset. In: proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition. 2017, 6299â€“6308\n6 Chen J, Chen X, Ma L, et al. Temporally grounding natural sentence in video. In: Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing. 2018, 162â€“171\n7 Chen S, Jiang W, Liu W, et al. Learning modality interaction for temporal sentence localization and event captioning in\nvideos. In: Proceedings of the European Conference on Computer Vision. 2020, 333â€“351\n8 Chen S and Jiang Y G. Hierarchical visual-textual graph for temporal activity localization via language. In: Proceedings of\nthe European Conference on Computer Vision. 2020, 601â€“618\n9 Chen Y, Hao C, Yang Z X, et al. Fast target-aware learning for few-shot video object segmentation. Science China Information\nSciences, 2022. 65:1â€“16\n10 Chen Y W, Tsai Y H, and Yang M H. End-to-end multi-modal video temporal grounding. Advances in Neural Information\nProcessing Systems, 2021. 34\n11 Deng J, Yang Z, Chen T, et al. Transvg: End-to-end visual grounding with transformers. In: Proceedings of the IEEE\nInternational Conference on Computer Vision. 2021, 1769â€“1779\n12 Ding X, Wang N, Zhang S, et al. Exploring language hierarchy for video grounding. IEEE Transactions on Image Processing,\n2022. 31:4693â€“4706\n13 Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale.\nIn: International Conference on Learning Representations. 2020\n14 Gao J, Sun C, Yang Z, et al. Tall: Temporal activity localization via language query. In: Proceedings of the IEEE International\nConference on Computer Vision. 2017, 5267â€“5275\n15 Guo D, Wang S, Tian Q, et al. Dense temporal convolution network for sign language translation. In: IJCAI. 2019, 744â€“750\n16 Guo D, Zhou W, Li H, et al. Hierarchical lstm for sign language translation. In: Proceedings of the AAAI Conference on\nArtificial Intelligence. 2018, volume 32\n17 Hahn M, Kadav A, Rehg J M, et al. Tripping through time: Efficient localization of activities in videos. In: Proceedings of\nthe British Machine Vision Conference. 2020\n18 He D, Zhao X, Huang J, et al. Read, watch, and move: Reinforcement learning for temporally grounding natural language\ndescriptions in videos. In: Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 8393â€“8400\nSci China Inf Sci 16\n19 Ji Z, Chen K, He Y, et al. Heterogeneous memory enhanced graph reasoning network for cross-modal retrieval. Science China\nInformation Sciences, 2022. 65:1â€“13\n20 Krishna R, Hata K, Ren F, et al. Dense-captioning events in videos. In: Proceedings of the IEEE International Conference\non Computer Vision. 2017, 706â€“715\n21 Lei J, Berg T L, and Bansal M. Detecting moments and highlights in videos via natural language queries. Advances in Neural\nInformation Processing Systems, 2021. 34:11846â€“11858\n22 Li K, Guo D, and Wang M. Proposal-free video grounding with contextual pyramid network. In: Proceedings of the AAAI\nConference on Artificial Intelligence. 2021, 1902â€“1910\n23 Li Y, Wang X, Xiao J, et al. Equivariant and invariant grounding for video question answering. In: Proceedings of the 30th\nACM International Conference on Multimedia. 2022, 4714â€“4722\n24 Liu D, Qu X, Dong J, et al. Context-aware biaffine localizing network for temporal sentence grounding. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition. 2021, 11235â€“11244\n25 Liu M, Nie L, Wang Y, et al. A survey on video moment localization. ACM Computing Surveys, 2023. 55:1â€“37\n26 Liu M, Wang X, Nie L, et al. Attentive moment retrieval in videos. In: The 41st international ACM SIGIR Conference on\nResearch & Development in Information Retrieval. 2018, 15â€“24\n27 Liu M, Wang X, Nie L, et al. Cross-modal moment localization in videos. In: Proceedings of the 26th ACM international\nconference on Multimedia. 2018, 843â€“851\n28 Liu W, Anguelov D, Erhan D, et al. Ssd: Single shot multibox detector. In: Proceedings of the European Conference on\nComputer Vision. 2016, 21â€“37\n29 Lu C, Chen L, Tan C, et al. Debug: A dense bottom-up grounding approach for natural language video localization. In:\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing. 2019, 5144â€“5153\n30 Mun J, Cho M, and Han B. Local-global video-text interactions for temporal grounding. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. 2020, 10810â€“10819\n31 Nan G, Qiao R, Xiao Y, et al. Interventional video grounding with dual contrastive learning. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. 2021, 2765â€“2775\n32 Pennington J, Socher R, and Manning C D. Glove: Global vectors for word representation. In: Proceedings of the 2014\nConference on Empirical Methods in Natural Language Processing. 2014, 1532â€“1543\n33 Qu W, Wang D, Feng S, et al. A novel cross-modal hashing algorithm based on multimodal deep learning. Science China\nInformation Sciences, 2017. 60:1â€“14\n34 Rezatofighi H, Tsoi N, Gwak J, et al. Generalized intersection over union: A metric and a loss for bounding box regression.\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019, 658â€“666\n35 Rodriguez C, Marrese-Taylor E, Saleh F S, et al. Proposal-free temporal moment localization of a natural-language query in\nvideo using guided attention. In: The IEEE Winter Conference on Applications of Computer Vision. 2020, 2464â€“2473\n36 Rodriguez-Opazo C, Marrese-Taylor E, Fernando B, et al. Dori: discovering object relationships for moment localization of\na natural language query in a video. In: Proceedings of the IEEE Winter Conference on Applications of Computer Vision.\n2021, 1079â€“1088\n37 Shou Z, Wang D, and Chang S F. Temporal action localization in untrimmed videos via multi-stage cnns. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition. 2016, 1049â€“1058\n38 Su W, Zhu X, Cao Y, et al. Vl-bert: Pre-training of generic visual-linguistic representations. In: International Conference\non Learning Representations. 2019\n39 Sun X, Wang X, Gao J, et al. You need to read again: Multi-granularity perception network for moment retrieval in videos.\nIn: Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval.\n2022, 1022â€“1032\n40 Tran D, Bourdev L, Fergus R, et al. Learning spatiotemporal features with 3d convolutional networks. In: Proceedings of\nthe IEEE International Conference on Computer Vision. 2015, 4489â€“4497\n41 Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. In: Advances in Neural Information Processing Systems.\n2017, 5998â€“6008\n42 Wang H, Wu Y, Li M, et al. Survey on rain removal from videos or a single image. Science China Information Sciences, 2022.\n65:1â€“23\n43 Wang J, Ma L, and Jiang W. Temporally grounding language queries in videos by contextual boundary-aware prediction. In:\nProceedings of the AAAI Conference on Artificial Intelligence. 2020, 12168â€“12175\n44 Wang L, Xiong Y, Wang Z, et al. Temporal segment networks: Towards good practices for deep action recognition. In:\nProceedings of the European Conference on Computer Vision. 2016, 20â€“36\n45 Wang T, Zhang R, Lu Z, et al. End-to-end dense video captioning with parallel decoding. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. 2021, 6847â€“6857\n46 Wang W, Huang Y, and Wang L. Language-driven temporal activity localization: A semantic matching reinforcement learning\nmodel. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019, 334â€“343\n47 Xiao S, Chen L, Zhang S, et al. Boundary proposal network for two-stage natural language video localization. In: Proceedings\nof the AAAI Conference on Artificial Intelligence. 2021, 2986â€“2994\n48 Xu H, He K, Plummer B A, et al. Multilevel language and vision integration for text-to-clip retrieval. In: Proceedings of the\nAAAI Conference on Artificial Intelligence. 2019, 9062â€“9069\n49 Yang Y, Li Z, and Zeng G. A survey of temporal activity localization via language in untrimmed videos. In: 2020 International\nConference on Culture-oriented Science & Technology. 2020, 596â€“601\n50 Yu A W, Dohan D, Luong M T, et al. Qanet: Combining local convolution with global self-attention for reading comprehension.\nIn: International Conference on Learning Representations. 2018\n51 Yuan Y, Ma L, Wang J, et al. Semantic conditioned dynamic modulation for temporal sentence grounding in videos. In:\nAdvances in Neural Information Processing Systems. 2019, 536â€“546\n52 Yuan Y, Mei T, and Zhu W. To find where you talk: Temporal sentence localization in video with attention based location\nregression. In: Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 9159â€“9166\n53 Zeng R, Xu H, Huang W, et al. Dense regression network for video grounding. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. 2020, 10287â€“10296\n54 Zhang B, Yang C, Jiang B, et al. Video moment retrieval with hierarchical contrastive learning. In: Proceedings of the 30th\nACM International Conference on Multimedia. 2022, 346â€“355\nSci China Inf Sci 17\n55 Zhang D, Dai X, Wang X, et al. Man: Moment alignment network for natural language moment retrieval via iterative graph\nadjustment. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019, 1247â€“1257\n56 Zhang H, Sun A, Jing W, et al. Span-based localizing network for natural language video localization. In: Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics. 2020, 6543â€“6554\n57 Zhang M, Yang Y, Chen X, et al. Multi-stage aggregated transformer network for temporal language localization in videos.\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2021, 12669â€“12678\n58 Zhang S, Peng H, Fu J, et al. Learning 2d temporal adjacent networks for moment localization with natural language. In:\nProceedings of the AAAI Conference on Artificial Intelligence. 2020, 12870â€“12877\n59 Zhang Z, Lin Z, Zhao Z, et al. Cross-modal interaction networks for query-based moment retrieval in videos. In: Proceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2019, 655â€“664\n60 Zhou L, Xu C, and Corso J J. Towards automatic learning of procedures from web instructional videos. In: Proceedings of\nthe AAAI Conference on Artificial Intelligence. 2018, 7590â€“7598",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7832756042480469
    },
    {
      "name": "Security token",
      "score": 0.7675431966781616
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4746154844760895
    },
    {
      "name": "Encoder",
      "score": 0.46174055337905884
    },
    {
      "name": "Discriminative model",
      "score": 0.433559387922287
    },
    {
      "name": "Speech recognition",
      "score": 0.33102554082870483
    },
    {
      "name": "Computer network",
      "score": 0.1130455732345581
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16365422",
      "name": "Hefei University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I1327237609",
      "name": "Ministry of Education of the People's Republic of China",
      "country": "CN"
    }
  ]
}