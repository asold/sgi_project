{
  "title": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models",
  "url": "https://openalex.org/W4377987838",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4223995596",
      "name": "AlKhamissi, Badr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3180312236",
      "name": "Verma Siddharth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2032794384",
      "name": "Yu Ping",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222932458",
      "name": "Jin, Zhijing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223995598",
      "name": "Celikyilmaz, Asli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222817077",
      "name": "Diab, Mona",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3207166518",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4311992335",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4287393336",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3175591618",
    "https://openalex.org/W4223974161"
  ],
  "abstract": "In this paper, we conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model's performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.",
  "full_text": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting\nfor Reasoning Skills of Large Language Models\nBadr AlKhamissi Siddharth Verma Ping Yu Zhijing Jin\nAsli Celikyilmaz Mona Diab\nMeta AI\nAbstract\nIn this paper, we conduct a thorough investiga-\ntion into the reasoning capabilities of Large\nLanguage Models (LLMs), focusing specif-\nically on the Open Pretrained Transformers\n(OPT) models as a representative of such mod-\nels. Our study entails finetuning three differ-\nent sizes of OPT on a carefully curated rea-\nsoning corpus, resulting in two sets of fine-\ntuned models: OPT- R, finetuned without ex-\nplanations, and OPT-RE, finetuned with expla-\nnations. We then evaluate all models on 57\nout-of-domain tasks drawn from the S UPER-\nNATURALINSTRUCTIONS benchmark, cover-\ning 26 distinct reasoning skills, utilizing three\nprompting techniques. Through a comprehen-\nsive grid of 27 configurations and 6,156 test\nevaluations, we investigate the dimensions of\nfinetuning, prompting, and scale to understand\nthe role of explanations on different reasoning\nskills. Our findings reveal that having expla-\nnations in the fewshot exemplar has no signifi-\ncant impact on the model’s performance when\nthe model is finetuned, while positively affect-\ning the non-finetuned counterpart. Moreover,\nwe observe a slight yet consistent increase in\nclassification accuracy as we incorporate ex-\nplanations during prompting and finetuning, re-\nspectively. Finally, we offer insights on which\nskills benefit the most from incorporating ex-\nplanations during finetuning and prompting,\nsuch as Numerical (+20.4%) and Analogical\n(+13.9%) reasoning, as well as skills that ex-\nhibit negligible or negative effects.\n1 Introduction\nRecently, there has been a surge in the release of\nLarge Language Models (LLMs) by both indus-\ntrial and academic institutions. These models vary\nfrom open-source releases such as OPT (Zhang\net al., 2022) and LLAMA (Touvron et al., 2023) to\nclosed-source ones like GPT-3 (Brown et al., 2020)\nand PALM (Chowdhery et al., 2022). In addition,\nresearchers have developed models that are fine-\ntuned on top of these foundational models to better\nScale \nFinetuning \nPrompting \n1.3B \n6.7B \n13B \nOPT OPT -R OPT -RE \nZeroshot \nFewshot \nFewshot-E \nFigure 1: Three-Dimensional Grid of Fine-Tuning,\nPrompting, and Scale. Each dimension is represented\nas an axis, with three levels for each of finetuning,\nprompting, and scale plotted on each axis. The resulting\ngrid consists of 27 different combinations evaluated on\nvarious reasoning tasks. It should be noted that there is\na hidden dimension, the scoring function, comprising\nfour components. This results in a comprehensive total\nof 6,156 evaluations.\nfollow instructions, such as OPT-IML (Iyer et al.,\n2022) and Alpaca (Taori et al., 2023). Despite\nthe remarkable progress in LLMs’ performance\nin Natural Language Processing (NLP) tasks, rea-\nsoning remains a challenging area. For example,\nprior work have shown that LLMs struggle with\ncommonsense reasoning (West et al., 2022) and\narithmetic reasoning (Hendrycks et al., 2021) to\nname a few.\nRecent efforts have attempted to improve the\nreasoning performance of LLMs by decomposing\nanswers into step-by-step reasoning chains using in-\ncontext learning (Wei et al., 2022b; Kojima et al.,\n2022) or during finetuning (Chung et al., 2022;\nWei et al., 2021a). While these approaches have\nshown some improvement on benchmarks such as\nGSM8K (Cobbe et al., 2021), it is not clear how\nthose explanations affect finetuning, prompting, or\narXiv:2305.12001v2  [cs.CL]  24 Oct 2023\n{Task Definition} \nProvide your answer followed by a brief reasoning.\n{In-Context Examples}\nInput: {input}\nOptions: {options}\nOutput: The answer is {answer} because {explanation}\nFigure 2: Template used during both training and infer-\nence. The model is tasked with predicting the answer\nfollowed by the explanation.\ntheir combination. Concurrent work has investi-\ngated the generalization capability of such models\nto reasoning skills beyond those encountered dur-\ning finetuning (Yu et al., 2022), but a comprehen-\nsive evaluation of the role of explanation during\nfinetuning and prompting with respect to reasoning\nskills is still lacking.\nIn this paper, we aim to address this gap. We in-\nvestigate OPT (Zhang et al., 2022) as a representa-\ntive of such models and utilize it as our base model.\nThrough finetuning OPT on a collection of care-\nfully curated open-source reasoning datasets that\ncome with explanations for each instance, we eval-\nuate its performance on 57 tasks drawn from the\nSUPER-NATURALINSTRUCTIONS benchmark (Wang\net al., 2022), covering 26 different reasoning skills.\nOur experiments are structured around three key\ndimensions: finetuning, prompting, and scale, each\nof which is comprised of three distinct components\n(See Figure 1). Finetuning: (1) a (vanilla) un-\nfinetuned OPT model; (2) A finetuned OPT model\nwithout explanations (OPT-R); and, (3) A finetuned\nOPT model with explanations (OPT-RE). Prompt-\ning: (1) zero-shot prompting; (2) Fewshot prompt-\ning without explanations; and, (3) Fewshot prompt-\ning with explanations. Finally, Scale: (1) 1.3B; (2)\n6.7B; and, (3) 13B. Accordingly, we create grid\nof 27 different components, providing a detailed\nanalysis measuring the impact of explanations dur-\ning finetuning and inference across different model\nscales.\nOur findings reveals that finetuning on reason-\ning datasets leads to statistically significant im-\nprovements in seven reasoning skills, including\nNumerical, Analogical and Reasoning on Objects,\nwith Physical, Counting and Textual Entailment\nshowing a significant effect only for the OPT-RE\nmodel, across both fewshot prompting conditions\nand model sizes, as compared to the vanilla OPT\nmodel (see Table 2). However, we also find that\nthis approach significantly hinders the performance\nof three other reasoning skills (see Table 3). We\nalso investigate the impact of incorporating expla-\nnations during fewshot prompting and find that it\ndoes not have a significant impact on the perfor-\nmance of the finetuned models, as measured by the\nvariance in the difference between both prompting\nmethods across reasoning skills for each model.\nHowever, we notice that it has a more noticeable ef-\nfect on the performance of the vanilla OPT model,\nas shown in Table 5. Additionally, we observe\na consistent increase in the average performance\nacross all tasks from Fewshot to Fewshot-E, as well\nas from OPT to OPT-R to OPT-RE models, indi-\ncating that explanations do have a small effect on\nperformance during both finetuning and prompting.\nFinally, Table 4 presents a summary of the results,\nindicating which reasoning skills demonstrate im-\nprovement due to the incorporation of explanations\nduring either finetuning or prompting, which skills\nshow a negative effect, and which skills have negli-\ngible effects regarding explanations.\n2 OPT-R: Finetuning on Reasoning Skills\n2.1 Reasoning Datasets with Explanations\nFigure 3: Number of samples in each dataset of the\ntraining corpus. Y-axis in log scale.\nThe finetuning corpus utilized to refine OPT is\ncomposed of various reasoning datasets, each of\nwhich includes a corresponding explanation or ra-\ntionale for the answer. These rationales may con-\nsist of a sequence of smaller steps (i.e. chain-of-\nthought) or a free-form text that elucidates the rea-\nsoning behind the answer. As shown in Figure 2,\nwe employ a uniform template for all tasks during\nthe training process. The input to the model begins\nwith a task definition, followed by an instruction to\nprovide an answer followed by a brief reasoning.\nNext, we extract two random in-context examples\nuniformly from the training set that remain con-\nstant throughout training for each instance. The\ninput for the current training instance is then pre-\nsented in a format specific to each task. The options\nfor the answer are then included in the input, but\nnot in the in-context examples (see Appendix A\nfor further details on task-specific definitions and\noptions). The options are pre-shuffled for each\ntraining instance. The model is finally provided\nwith the answer prefix, \"Output: The answer\nis\", and is tasked to predict the answer, followed\nby an explanation if OPT- RE is being finetuned.\nSimilarly, the in-context examples only comprise\nan explanation when training OPT-RE.\nBelow is a brief description of each dataset used\nduring finetuning. See Figure 3 for the relative size\nof each dataset.\nAQUA-RAT The Algebra Question Answering\nwith Rationales dataset (Ling et al., 2017) render-\ning the task of solving algebraic word problems\nmore feasible by dividing the problem into a series\nof smaller steps. They create a100k-sample dataset\nthat contains questions, answers and rationales in\nnatural language and human-readable mathemati-\ncal expressions that can be used to derive the final\nanswer.\nCoQA The Conversational Question Answering\ndataset Reddy et al. (2019). It consists of 127k\nquestions and answers, compiled from 8k conversa-\ntions about passages from seven different domains.\nGiven a passage that contains a conversation, the\nmodel is tasked with answering a question by high-\nlighting the corresponding evidence from the pas-\nsage.\nCoS-E The Common Sense Explanations dataset\nRajani et al. (2019) to induce language models with\ncommonsense reasoning. In this dataset, the model\nis given a question and a set of choices and is tasked\nwith selecting one of the provided choices along\nwith providing an explanation in natural language\nas to why that choice is correct.\nECQA The Explanations for Commonsense\nQuestion Answering dataset Aggarwal et al. (2021).\nIt is similar to CoS-E since it requires the model to\nchoose one of the provided options to answer the\ngiven question, and also provide an explanation.\nESNLI The Stanford Natural Language Infer-\nence dataset with Explanations Camburu et al.\n(2018) to train models to provide interpretable and\nrobust explanations for their decisions. The authors\nextend the SNLI dataset (Bowman et al., 2015) with\nhuman-annotated explanations. Similar to any NLI\ntask, the model is given a premise and hypothesis\nand the task is to determine whether the hypothe-\nsis sentence entails, contradicts, or is neutral with\nrespect to the given premise.\nGSM8K The Grade School Math dataset Cobbe\net al. (2021) to train models to better perform multi-\nstep mathematical reasoning. It consists of 8.5k\nlinguistically diverse grade school math word prob-\nlems. Therefore, the task for the model is to answer\nthe question by performing a series of arithmetic\noperations to obtain a final answer, while explain-\ning it’s reasoning steps.\nProofWriter The ProofWriter dataset Tafjord\net al. (2021) to generate both the implications of\na theory from the RuleTaker dataset (Clark et al.,\n2020) and the natural language proofs that support\nthem. Specifically, given a sequence of facts and\nrules, the model is tasked with answering a ques-\ntion using “Yes”, “No”, or “Unknown” and provide\nthe reasoning path by referring to the provided facts\nand rules. We consider the open-world assumption\nsubset of RuleTaker with questions that requires\nreasoning up to a depth of 5.\nStrategyQA The Strategy Question Answering\ndataset Geva et al. (2021) to improve multi-hop rea-\nsoning for questions where the required reasoning\nsteps are implicit in the question. Therefore, the\ntask of the model is to answer the question using\n“Yes” or “No” then provide a strategy that explains\nthe answer by decomposing it into a number of\nsteps.\n2.2 Finetuning Procedures\nOPT The Open Pretrained Transformers (OPT)\nmodels are a suite of decoder-only pre-trained trans-\nformers ranging from 125M to 175B parameters\nreleased by Zhang et al. (2022). In this work, we\nuse three OPT models with sizes of 1.3B, 6.7B\nand 13B. The details of each model architecture,\npre-training corpus and training configuration (e.g.\nweight initialization, optimizer, tokenizer, hyperpa-\nrameters, etc.) can be found in Zhang et al. (2022).\nReasoning Skill Task IDs\nAbductive Reasoning task854\nAnalogical Reasoning task1287, task1288\nArgument Reasoning task514\nCausal Reasoning task1393\nCommonsense Reasoning task279, task156, task295\nCommonsense Reasoning →Numerical Commonsense ... task1403\nCommonsense Reasoning →Physical Reasoning task084\nCommonsense Reasoning →Social Situations task580, task937, task1606\nCommonsense Reasoning →Spatial Reasoning task082, task083\nDeductive Reasoning task221, task1568, task220\nEthics task667, task724, task723\nGrammatical Reasoning task1712, task052, task1559\nLogical Reasoning task717, task211, task268\nLogical Reasoning →Reasoning with Symbols task923, task935\nMathematics → Counting task523, task155\nMultihop Reasoning task1297, task056\nNumerical Reasoning task621, task1333\nReasoning on Objects task1583, task1584\nReasoning on Social Interactions task609, task881, task875\nReasoning on Strings task1189\nRelational Reasoning task1380, task472, task1505\nScientific Reasoning task1431, task228, task714\nTemporal Reasoning task018, task1549, task383\nTextual Entailment task738, task890, task463\nTextual Entailment → Analogical Reasoning task1347\nTextual Entailment → Deductive Reasoning task1612, task534, task1366\nTable 1: Evaluation tasks from SUP-NATINST (Wang et al., 2022) used for each reasoning skill.\nImplementation Details To finetune the selected\nmodels, we utilized the metaseq1 implementation\nsince it enables higher training efficiency compared\nto other codebases (Zhang et al., 2022). Each\nmodel is finetuned twice for 10 epochs, once with\nexplanations and once without (i.e. OPT- RE vs\nOPT-R, respectively). Models are evaluated at\nthe end of each epoch on a chosen set of S UPER-\nNATURALINSTRUCTIONS validation tasks, and the\ncheckpoint with the best performance is selected\nfor evaluation on the testing tasks. The loss is\ncalculated only on the tokens the model is tasked\nto predict during inference, and not the full input,\nwhat is referred to as label-loss in (Iyer et al., 2022).\nThe samples across all datasets are shuffled during\ntraining. Further, the model is provided with two\nin-context examples during finetuning in addition\nto the task definition to match inference time fol-\nlowing (Wang et al., 2022).\n1https://github.com/facebookresearch/metaseq\n3 Evaluating the Models\n3.1 S UPER-NATURALINSTRUCTIONS Tasks\nIn this study, we focus on a subset of the S UPER-\nNATURALINSTRUCTIONS benchmark version 2.62\n(SUP-NATINST for short) proposed by Wang et al.\n(2022), which comprises 1,616 varied NLP tasks\nand includes meta-labels for each task, such as\ntask type, domain and more importantly for this\nwork: the underlying reasoning skills. Specifically,\nwe select a subset of tasks that satisfy two key\ncriteria: (i) the task focuses on a single reasoning\nskill, enabling us to evaluate a specific atomic skill,\nand (ii) the task can be tested using classification\nmode, as detailed in Section 3.2. Note that there is\nno data contamination between finetuning data and\nthe evaluation benchmark.\n2We downloaded the data from https://github.com/\nallenai/natural-instructions/tree/v2.6.\nFigure 4: Results achieved across all tasks as a function of the three primary dimensions analyzed in this study:\nFinetuning, Prompting and Scale.\nBenchmark Splits Following the task selection\nprocess, we apply a random sampling technique to\nensure diversity within the testing set. Specifically,\nwe select a maximum of three tasks from each rea-\nsoning skill, and allocate any remaining tasks to\nthe validation set. Notably, this approach enables\nus to obtain a representative sample of the selected\nreasoning skills for testing, while also ensuring\nthat our model’s performance is not influenced by\na particular subset of tasks. Table 1 shows the com-\nplete list of tasks used for evaluating our finetuned\nmodels for each reasoning skill.\n3.2 Evaluation Setup\nEarlier, we mentioned that we selected 57 tasks\nspanning 26 reasoning skills from SUP-NATINST to\nevaluate our finetuned models. To meet our criteria,\nas detailed in Section 3.1, each task had to fulfill\ntwo conditions. The second condition required\nthat the task can be considered a classification task.\nThat means there is a discrete set of candidates\n(one of which is correct) and thereby treating it as\na classification problem where the highest-scoring\ncandidate is considered the answer. To ensure this,\nwe utilized a straightforward heuristic: we only\nsampled tasks that had no more than 10 possible\ncandidate answers.\nClassification Method To determine the correct\nanswer, we conduct a forward pass for each poten-\ntial candidate answer and utilize a scoring function\nto measure the likelihood that the candidate tokens\nfollows the input, similar to Brown et al. (2020).\nThis process is repeated four times using distinct\nscoring functions, as detailed in the subsequent\nparagraph. The highest accuracy score from the\nfour scoring functions is considered as result of the\ntask.\nScoring Functions This is considered the fourth\ndimension of this work since we evaluate each task\nusing four different scoring functions and take the\nmaximum accuracy as the result. The four scoring\nfunctions used are as follows: (1) mean, which\ninvolves computing the average of the log probabil-\nities of candidate tokens, also referred to as token\nscore. (2) unconditional-norm, which computes\nthe difference between the sum of token scores of\nthe candidate when unconditioned by any previous\ntokens and the sum of candidate token scores when\nconditioned by previous input. (3) suffix, which\ncomputes the sum of the conditioned candidate’s to-\nken scores alone. Finally, (4) sum, which involves\ncalculating the sum of all the token scores passed\nto the model. The reason we employed different\nfunctions is that we observed significant gains in\nperformance when using one scoring function over\nthe other for specific tasks. Therefore, in order\nto ensure fairness across all tasks, we selected the\nhighest accuracy over all scoring functions for each\ntask.\n4 Results & Findings\nIn this section, we present the results and findings\nof our experiments. First, we illustrate in Figure\n4 the outcome of our evaluation on the effective-\nness of finetuned models as compared to the vanilla\nOPT model, across three different scales when us-\ning both fewshot prompting with and without ex-\nplanations. Furthermore, we observe a monotonic\nincrease in the performance of each model as we\nincrease the scale under those two prompting condi-\ntion, which indicates a positive correlation between\nthe model’s capacity and its overall performance.\nHowever, we note that this trend does not apply\nto the zeroshot prompting method, since we are\ntesting out-of-distribution tasks and that the fine-\ntuned models were trained with fewshot exemplars\nin their context. This leads us to focus only on\nthe fewshot prompting methods, with and with-\nout explanations, for the remaining of our evalu-\nations. Specifically, we investigate the impact of\nfinetuning the OPT models on reasoning datasets,\nas compared to the vanilla OPT model, and explore\nthe effect of explanations during finetuning and\nprompting, both in terms of the reasoning skill.\n4.1 Model Performance for Reasoning Skills\nThe results reported in this and the following sec-\ntion are the classification accuracy of each reason-\ning skill across different conditions, such as model\nsizes and fewshot prompting methods. Table 2\nshows the reasoning skills where either OPT- RE\nor OPT-R are significantly better than the vanilla\nOPT model, as measured by Welch’s t-test, where\np < 0.05. Conversely, Table 3 show the reason-\ning skills where the vanilla OPT model performs\nsignificantly better than either of its finetuned coun-\nterparts.\nSkill OPT OPT- R OPT-RE\nNumerical 44.8 65.2* 64.7*\nAnalogical 49.0 62.9* 60.8*\nCounting 19.8 13.1 31.3*\nPhysical 38.2 37.8 49.1*\nEntailment 42.6 47.2 51.6*\nSocial Int 34.1 43.0* 40.1\nObjects 54.3 62.6* 59.9*\nTable 2: Performance as a function of the reasoning\nskills where OPT-RE or OPT-R performs significantly\nbetter than the OPT model as measured by Welch’s t-test\n(p <0.05) denoted by the * symbol. The performance is\nmeasured across Fewshot and Fewshot-E prompting, the\nthree different scales and tasks under the corresponding\nreasoning skill. Best result indicated in bold.\nThe results reveal that the finetuned variants of\nthe OPT model demonstrate a significant improve-\nment on seven distinct reasoning skills, with par-\nticular emphasis on the Numerical and Analogical\nreasoning tasks. Specifically, for the Mathematical\nSkill OPT OPT- R OPT-RE\nArgument 57.9 46.1− 48.7−\nTE - Deductive 36.0 29.0− 29.4−\nCommonsense 33.4 29.7 28.8 −\nTable 3: Performance as a function of the reasoning\nskill where OPT performs significantly better than either\nOPT-R or OPT-RE as measured by Welch’s t-test (p <\n0.05) denoted by the − symbol. The performance is\nmeasured across Fewshot and Fewshot-E prompting, the\nthree different scales and tasks under the corresponding\nreasoning skill. TE is Textual Entailment.\nCounting skill, the OPT-RE variant outperforms\nboth the OPT- R and OPT models, underscoring\nthe criticality of incorporating explanations during\nthe finetuning process for mathematical datasets.\nLikewise, the Physical Reasoning tasks exhibit a\nsimilar trend. On the other hand, we can see that\nfor the Argument, Deductive Textual Entailment\nand Commonsense skills the non-finetuned version\noutperforms considerably.\n4.2 Fine-Grained Skill Analysis\nTable 4 shows the classification accuracy results\nobtained from the three models, in relation to the\nreasoning skill and few-shot prompting method\nused. The best accuracy value for each reasoning\nskill is indicated in bold, and the cells are shaded\nwith colors ranging from green to white to indicate\ntheir position in the accuracy spectrum of each rea-\nsoning skill. The skills with similar performance\nacross different models are assigned a lighter shade\nof green, indicating that their color spectrum ends\nearlier than that of other skills where the difference\nin performance between models is more significant.\nThe table is divided into four blocks to distinguish\neffects of finetuning and prompting methods on\nreasoning skills: the first block showcases skills\nwhere the finetuned (OPT- RE and OPT-R) mod-\nels outperform the vanilla OPT model, the second\nblock highlights skills where OPT- RE has better\naccuracy than other models therefore illustrating\nthe importance of finetuning on explanations on\nthose skills. The third block displays skills where\nOPT outperforms other models showing that fine-\ntuning actually hurts performance in this case, and\nthe fourth block identifies skills where the choice\nof model or prompting method has little impact on\nthe overall performance.\nOPT OPT- R OPT-RE\nSkill Fewshot Fewshot-E Fewshot Fewshot-E Fewshot Fewshot-E\nNumerical 39.9 49.7 65.1 65.3 64.7 64.8\nAnalogical 51.9 46.2 63.3 62.5 60.7 60.9\nObjects 53.5 55.1 61.4 63.8 60.0 59.7\nSocial Interactions 33.6 34.7 43.8 42.3 40.2 40.0\nTextual Entailment 43.3 42.0 47.1 47.3 51.9 51.2\nGrammatical 54.4 55.1 61.2 60.0 62.0 63.1\nMultihop 36.6 31.7 38.9 39.9 39.5 37.0\nSymbols 44.2 47.2 51.7 51.8 51.9 52.4\nSpatial 44.1 47.1 49.8 51.8 49.6 49.2\nSocial Situations 46.3 46.6 53.2 53.2 51.9 52.3\nCounting 19.6 20.0 13.5 12.7 29.8 32.9\nPhysical 35.8 40.6 36.9 38.8 48.1 50.0\nLogical 31.7 33.4 33.7 34.1 36.9 38.4\nTemporal 50.7 49.7 43.4 46.5 48.5 38.5\nArgument 55.8 60.1 46.3 45.9 48.6 48.8\nTE - Deductive 33.7 38.3 27.9 30.1 29.0 29.9\nRelational 47.4 51.1 47.6 47.9 44.8 44.6\nCommonsense 35.0 31.8 29.8 29.5 28.5 29.2\nTE - Analogical 16.3 18.7 18.6 20.7 18.7 18.1\nAbductive 33.9 36.1 36.9 34.4 34.2 35.3\nEthics 26.8 25.8 26.5 25.9 26.2 27.6\nDeductive 39.4 40.4 39.4 40.4 40.0 41.1\nCausal 50.2 50.6 49.1 48.9 50.1 50.5\nScientific 23.4 23.3 24.3 24.5 25.0 24.5\nNumerical Commonsense 59.5 59.2 59.0 59.0 59.2 59.4\nStrings 60.7 60.7 61.1 61.2 60.7 60.7\nTable 4: Classification accuracy results achieved by different models as a function of the reasoning skill and few-shot\nprompting method employed. The best accuracy obtained for each reasoning skill is highlighted in bold. The cells\nare shaded with colors ranging from green to white to indicate their position in the accuracy spectrum. Reasoning\nskills with smaller variance in achieved results are assigned a lighter shade of green to convey the extent of similarity\nbetween models. The first block highlights skills where the finetuned models perform notably better than the vanilla\nOPT. The second block emphasizes the skills where OPT-RE outperforms other models. In contrast, the third block\nshowcases the skills where OPT outperforms the other models. Lastly, the fourth block identifies skills where the\nchoice of model or prompting method has little impact on the overall performance.\nExplanations’ Effect One of the central ques-\ntions that we sought to investigate in this study\nis the extent to which explanations play a role in\nimproving the reasoning capabilities of OPT mod-\nels during finetuning and prompting. The results\npresented in Table 5 suggest that the presence or\nabsence of explanations in the fewshot examples\nemployed for prompting does not significantly im-\npact the performance of the model when the model\nis finetuned on reasoning datasets. Concretely, in\nTable 5, we present the variance of the absolute\naccuracy difference for each model across reason-\ning skills by excluding the Temporal skill, which\nwas identified as an outlier. Specifically, we com-\npute the difference between the two corresponding\ncolumns for each model in Table 4. These values\nprovide insights into the impact of including ex-\nplanations during prompting on the performance\nof the models. Our findings reveal that the differ-\nence is negligible for OPT-R and OPT-RE models,\nsuggesting that the choice of prompting method\ndoes not significantly affect the model’s accuracy.\nHowever, for the vanilla OPT model, the differ-\nence is more substantial, emphasizing the impor-\ntance of employing explanations during fewshot\nprompting. However, the mean performance of\neach model across the distinct fewshot prompting\nmethods demonstrates a slight yet consistent in-\ncrease in classification accuracy, from Fewshot to\nFewshot-E (incorporating explanations), as well as\nfrom OPT to OPT-R to OPT-RE models showing\nthat explanations do have a small effect on perfor-\nmance during both finetuning and prompting.\nModel Std(|F-FE|) Avg(F) Avg(FE)\nOPT 2.31 40.68 41.82\nOPT-R 0.84 43.44 43.68\nOPT-RE 0.78 44.49 44.86\nTable 5: The first column shows the variance of the\nabsolute difference in accuracy for each model across\ndifferent reasoning skills, when using Fewshot (F) and\nFewshot-E (FE) prompting methods. The second and\nthird columns show the average performance of each\nmodel across each prompting method. Results are ob-\ntained after dropping the outlier Temporal skill.\n5 Related Work\nReasoning LLMs LLMs have made significant\nadvancements in the field of NLP and related ar-\neas (Brown et al., 2020; Chowdhery et al., 2022;\nChung et al., 2022), especially with the advent of\nthe pre-train, prompt, and predict paradigm (Liu\net al., 2021). This paradigm has enabled these\nmodels to solve a multitude of tasks through in-\ncontext fewshot or zeroshot learning using instruc-\ntions (Wei et al., 2021b; Iyer et al., 2022). However,\ntheir reasoning abilities have been a subject of de-\nbate in recent literature (Huang and Chang, 2022;\nAlKhamissi et al., 2022). Several studies suggest\nthat increasing the size of an LM trained through\nthe same next-token prediction method can lead to\nthe emergence of complex behaviors (Wei et al.,\n2022a), including reasoning. For instance, some re-\nsearch has demonstrated that sufficiently large LMs\ncan use chain-of-thought prompting (Wei et al.,\n2022b) to simulate human-like reasoning. Other\nstudies have shown that the addition of a simple\nprompt, such as \"Let’s think step-by-step\" (Ko-\njima et al., 2022) can elicit reasoning abilities in\nLLMs by generating explicit reasoning steps be-\nfore decoding the final answer. However, some\nresearchers contend that emulating the human rea-\nsoning thought process is distinct from claiming\nthat the model can truly reason (Wei et al., 2022b).\nFinetuned LLMs Concurrent studies have fine-\ntuned LLMs to follow instructions to improve their\ngeneralization ability to unseen tasks through zero\nand fewshot learning (Iyer et al., 2022; Chung et al.,\n2022). However, our approach differs in that we\nonly finetune on a selected number of open-source\ndatasets that provide explanations for each instance.\nThis enables us to focus on the importance of expla-\nnations during finetuning in the context of reason-\ning skills. While concurrent works, such as (Iyer\net al., 2022; Wang et al., 2022), have experimented\nwith different prompting methods during finetuning\nand inference, our study focuses primarily on eval-\nuating the reasoning ability of the finetuned models\nacross a set of reasoning skills. Other concurrent\nstudies have explored the impact of finetuning on a\nset of held-out reasoning tasks (Yu et al., 2022), but\ntheir evaluation approach, which involves generat-\ning answers, may be influenced by various factors\nsuch as decoding strategy, decoding parameters,\nand prompt templates. In contrast, we adopt a rank\nclassification approach similar to (Brown et al.,\n2020), which better captures the reasoning perfor-\nmance of the model being evaluated, in addition to\ncovering a larger number of reasoning skills and\ntasks.\n6 Conclusion\nIn this study, we investigated the impact of incorpo-\nrating explanations during finetuning and prompt-\ning on three different sizes of the OPT model.\nThrough a systematic and comprehensive evalu-\nation process that considered three key dimensions,\nwe found that while explanations did provide a\nsmall improvement in performance, the effect was\nnot significant when incorporated in the in-context\ndemonstrations during inference for the finetuned\nmodels. Additionally, our results showed that both\nfinetuned models exhibited significant improve-\nments in reasoning skills such as Numerical, Ana-\nlogical and Reasoning on Objects. Moreover, we\ndemonstrated that skills such as Physical, Count-\ning, and Textual Entailmentbenefited from incorpo-\nrating explanations during the finetuning process.\nOverall, our findings provide insights into the im-\npact of incorporating explanations on the reason-\ning capabilities of LLMs and offer guidance on\nwhich reasoning skills would benefit most from\nthe inclusion and exclusion of explanations during\nfinetuning and prompting.\nLimitations\nWhile our study provides valuable insights into\nthe impact of finetuning on reasoning performance\nand the role of explanations during finetuning and\nprompting with respect to various reasoning skills,\nthere are several limitations to our work. Firstly,\nwe only consider a single LLM, OPT, as our base\nmodel. Our results may not generalize to other\nLLMs with different architectures or pretraining\nobjectives. Secondly, we only use a limited set of\nreasoning datasets for finetuning due to the limited\navailability of open-source datasets with explana-\ntions. However, it is possible that our findings\nmay not hold for models finetuned on larger closed\ndatasets as usually seen in real-world scenarios.\nThirdly, our experiments only cover a limited range\nof model sizes due to limitations in computational\nbudget, therefore it is possible that our findings may\nnot hold for much larger models. Finally, we only\nconsider finetuning using fewshot prompting condi-\ntions in our experiments, and it is possible that our\nfindings may not hold for models finetuned with-\nout in-context exemplars. Overall, while our study\nprovides valuable insights into the impact of fine-\ntuning and explanations on reasoning performance,\nfurther research is needed to investigate these fac-\ntors across a broader range of models, datasets, and\nfinetuning strategies.\nEthics Statement\nThis work is based on analyzing and evaluating\nthe performance of LLMs on reasoning tasks using\nexisting public datasets. No personally identifiable\ninformation or sensitive data was collected or used\nin this research. We acknowledge the potential\nrisks of developing LLMs, including their potential\nimpact on spreading misinformation, generating\nunwanted content and the exacerbation of existing\nbiases in datasets. Our work aims to contribute to\nimproving the transparency and understanding of\nhow LLMs can be optimized for specific reasoning\nskills. We hope our findings will inspire further\nresearch on developing ethical and responsible ap-\nproaches for developing and deploying LLMs.\nReferences\nShourya Aggarwal, Divyanshu Mandowara, Vishwa-\njeet Agrawal, Dinesh Khandelwal, Parag Singla, and\nDinesh Garg. 2021. Explanations for Common-\nsenseQA: New Dataset and Models. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3050–3065, Online.\nAssociation for Computational Linguistics.\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz,\nMona T. Diab, and Marjan Ghazvininejad. 2022.\nA review on language models as knowledge bases.\nArXiv, abs/2204.06031.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAssociation for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\nral language inference with natural language expla-\nnations. In S. Bengio, H. Wallach, H. Larochelle,\nK. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\nitors, Advances in Neural Information Processing\nSystems 31, pages 9539–9549. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.\nTransformers as soft reasoners over language. In\nIJCAI.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nJacob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training verifiers to solve\nmath word problems. ArXiv, abs/2110.14168.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346–\n361.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\nwards reasoning in large language models: A survey.\nArXiv, abs/2212.10403.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Dániel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022. Opt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 158–167, Vancouver,\nCanada. Association for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55:1 – 35.\nNazneen Rajani, Bryan McCann, Caiming Xiong, and\nRichard Socher. 2019. Explain yourself! leveraging\nlanguage models for commonsense reasoning. In\nACL.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019. CoQA: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics, 7:249–266.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 3621–3634, Online.\nAssociation for Computational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur’elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran,\nAnjana Arunkumar, David Stap, Eshaan Pathak,\nGiannis Karamanolakis, Haizhi Lai, Ishan Puro-\nhit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,\nKrima Doshi, Kuntal Kumar Pal, Maitreya Patel,\nMehrad Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,\nRavsehaj Singh Puri, Rushang Karia, Savan Doshi,\nShailaja Keyur Sampat, Siddhartha Mishra, Sujan\nReddy A, Sumanta Patro, Tanay Dixit, and Xudong\nShen. 2022. Super-NaturalInstructions: Generaliza-\ntion via declarative instructions on 1600+ NLP tasks.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5085–5109, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021a. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021b. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus.\n2022a. Emergent abilities of large language models.\nArXiv, abs/2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D.\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models.\nPing Yu, Tianlu Wang, O. Yu. Golovneva, Badr\nAlKhamissi, Siddharth Verma, Zhijing Jin, Gargi\nGhosh, Mona Diab, and Asli Celikyilmaz. 2022.\nAlert: Adapting language models to reasoning tasks.\nArXiv, abs/2212.08286.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nDataset Task Definition Options\nAQuA\nYou are given an algebraic word question. Questions in this task often\nrequires executing a series of arithmetic operations to obtain a final answer.\nYou are also given 5 answer options (associated with ’A’, ’B, ’C’, ’D’, ’E’).\nDo not generate anything else apart from one of the following characters:\n\"A\", \"B\", \"C\", \"D\", \"E\" and the corresponding explanation.\n-A\n-B\n-C\n-D\n-E\nCoQA You are given a passage that contains a conversation and a question. The\ntask is to answer the question and provide an explanation that highlights\nthe corresponding evidence in the passage.\nFree-form text\nCoS-E You are given a passage that contains a sentence and a question. The task\nis to answer the question by selecting one of the provided choices.\nSelect one of\nthe provided choices\nECQA You are given a question that requires commonsense reasoning. The task\nis to answer the question by selecting one of the provided choices.\nSelect one of\nthe provided choices\nESNLI\nYou will be presented with a premise and a hypothesis sentence. The\ntask is to determine whether the hypothesis sentence entails (implies),\ncontradicts (opposes), or is neutral with respect to the given premise\nsentence. Please answer with \"Contradiction\", \"Neutral\",or \"Entailment\".\n-Contradiction\n-Neutral\n-Entailment\nGSM8K You will be presented with a passage that contains a grade school math\nword problem. The task is to answer the question by performing a series\nof arithmetic operations to obtain a final answer.\nNumber\nProofWriter You are given a sequence of facts and rules followed by a question. The\ntask is to answer the question using \"Yes\", \"No\" or \"Unknown\".\n-Yes\n-No\n-Unknown\nStrategyQA\nYou are given a sentence and a question. The required reasoning steps are\nimplicit in the question. The task is to answer the question using \"Yes\" or\n\"No\" then provide a strategy that explains the answer by decomposing it\ninto a number of steps.\n-Yes\n-No\nTable 6: Task definition and options used for each of the finetuning reasoning datasets.\nA Finetuning Task Definition and Options\nTable 6 shows the task definition and options pro-\nvided as input to the template shown in Figure 2\nduring finetuning the OPT models on the reasoning\ndatasets.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8539986610412598
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5377929210662842
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4607366919517517
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3391309082508087
    },
    {
      "name": "Cognitive science",
      "score": 0.33269309997558594
    },
    {
      "name": "Psychology",
      "score": 0.12106442451477051
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}