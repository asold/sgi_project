{
  "title": "Multi-Task Learning for Knowledge Graph Completion with Pre-trained Language Models",
  "url": "https://openalex.org/W3117339789",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2113904279",
      "name": "Bosung Kim",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A2150347667",
      "name": "Taesuk Hong",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2105058880",
      "name": "Youngjoong Ko",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A2171777892",
      "name": "Jungyun Seo",
      "affiliations": [
        "Sogang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963432357",
    "https://openalex.org/W2250184916",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2908230750",
    "https://openalex.org/W2949434543",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2432356473",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2774837955",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2741075451",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2728059831",
    "https://openalex.org/W2499696929",
    "https://openalex.org/W2963842982",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2964116313",
    "https://openalex.org/W2909137510",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2556468274"
  ],
  "abstract": "As research on utilizing human knowledge in natural language processing has attracted considerable attention in recent years, knowledge graph (KG) completion has come into the spotlight. Recently, a new knowledge graph completion method using a pre-trained language model, such as KG-BERT, is presented and showed high performance. However, its scores in ranking metrics such as Hits@k are still behind state-of-the-art models. We claim that there are two main reasons: 1) failure in sufficiently learning relational information in knowledge graphs, and 2) difficulty in picking out the correct answer from lexically similar candidates. In this paper, we propose an effective multi-task learning method to overcome the limitations of previous works. By combining relation prediction and relevance ranking tasks with our target link prediction, the proposed model can learn more relational properties in KGs and properly perform even when lexical similarity occurs. Experimental results show that we not only largely improve the ranking performances compared to KG-BERT but also achieve the state-of-the-art performances in Mean Rank and Hits@10 on the WN18RR dataset.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 1737–1743\nBarcelona, Spain (Online), December 8-13, 2020\n1737\nMulti-Task Learning for Knowledge Graph Completion\nwith Pre-trained Language Models\nBosung Kim1, Taesuk Hong2, Youngjoong Ko1, Jungyun Seo2\n1Sungkyunkwan University, Suwon, Gyeonggi-do, Korea\n2Sogang University, Seoul, Korea\n{bosungkim17,lino.taesuk}@gmail.com\nyjko@skku.edu, seojy@sogang.ac.kr\nAbstract\nAs research on utilizing human knowledge in natural language processing has attracted consid-\nerable attention in recent years, knowledge graph (KG) completion has come into the spotlight.\nRecently, a new knowledge graph completion method using a pre-trained language model, such\nas KG-BERT, was presented and showed high performance. However, its scores in ranking met-\nrics such as Hits@k are still behind state-of-the-art models. We claim that there are two main\nreasons: 1) failure in sufﬁciently learning relational information in knowledge graphs, and 2)\ndifﬁculty in picking out the correct answer from lexically similar candidates. In this paper, we\npropose an effective multi-task learning method to overcome the limitations of previous works.\nBy combining relation prediction and relevance ranking tasks with our target link prediction, the\nproposed model can learn more relational properties in KGs and properly perform even when\nlexical similarity occurs. Experimental results show that we not only largely improve the ranking\nperformances compared to KG-BERT but also achieve the state-of-the-art performances in Mean\nRank and Hits@10 on the WN18RR dataset.\n1 Introduction\nA Knowledge Graph (KG) is a graph-structured knowledge base, where real-world knowledge is rep-\nresented in the form of triple (h,r,t): (head entity, relation, tail entity) which means h and t have a\nrelationship r. Entities and the relation in a triple are denoted as nodes and an edge of the graph, re-\nspectively. In recent years, Natural Language Processing (NLP) has beneﬁted from utilizing KGs in\nvarious applications such as language modeling (Peters et al., 2019; Liu et al., 2019a), question answer-\ning (Zhang et al., 2019; Huang et al., 2019), and machine reading (Yang and Mitchell, 2017). Since\nthere has been an increasing demand for high-quality knowledge, the reliability of KG has also become\nimportant. Therefore, knowledge graph completion (a.k.a. link prediction), which identiﬁes whether the\ntriple in KG is valid or not, has been actively investigated.\nSeveral studies on the knowledge graph completion have been conducted (Bordes et al., 2013; Trouil-\nlon et al., 2016; Sun et al., 2019; Dettmers et al., 2018). They presented methods to model the connec-\ntivity patterns between entities in KG, and score functions to deﬁne the validity of the triple. However,\nthese methods only consider graph structure and relational information depending on existing KG. Thus,\nthey cannot predict well on triples that contain less frequent entities. Recently, addressing the sparseness\nproblem of previous models, Yao et al. (2019) proposed a method called KG-BERT for knowledge graph\ncompletion, using entity descriptions and pre-trained language models. Even though KG-BERT signiﬁ-\ncantly improved mean ranks using preliminary linguistic information from BERT (Devlin et al., 2018),\nthe results in other ranking metrics such as MRR and Hit@k are still behind the state-of-the-art models.\nWe claim that there are two major reasons for this problem. First, KG-BERT misses lots of relation\ninformation in KGs. While previous state-of-the-art methods aimed to model relational properties in\ngraphs, KG-BERT only uses binary cross entropy loss to predict valid or invalid triples for the link\nprediction task. Next, KG-BERT has difﬁculty in picking out the answer entity between lexically similar\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n1738\nFigure 1: Architecture of the proposed multi-task learning method for knowledge graph completion.\ncandidates. For example, given head entity and relation as (take a breather, derivationally related for,\n) and the correct tail entity as “breathing time” , KG-BERT predicts “snorkel breather” and “breath”\nas top scores because of the lexical similarity by “breath”. This problem leads to lower performance in\nMRR and Hits@k.\nIn this paper, we propose an effective multi-task learning method to overcome these problems. We\ndevise a multi-task framework by adding two tasks (relation prediction and relevance ranking) to link\nprediction, our target task. In the relation prediction, the model is trained to predict the relationship\nbetween given two entities, which helps the model learn more relational properties. In the relevance\nranking, the model is trained by the margin ranking loss to make a gap between the valid triple and\nlexically similar candidates. We evaluate the proposed method on two popular datasets WN18RR and\nFB15k-237, and experimental results show that our method could improve ranking performance by a\nlarge margin compared to KG-BERT. Notably, our method achieves state-of-the-art performances in\nMean Rank and Hits@10 on the WN18RR dataset.\n2 Proposed Method\nIn this section, we propose a multi-task learning for knowledge graph completion. As shown in Figure 1,\nwe follow a multi-task learning framework in MT-DNN (Liu et al., 2019b), and use the pre-trained BERT\nmodel as a shared layer. We combine three tasks: link prediction, relation prediction, and relevance\nranking. Each task has a classiﬁcation layer W ∈RK×H where K is the number of labels and H is the\nhidden size of BERT. Following Devlin et al. (2018), every input sequence has a [CLS] token at the\nhead of sentence, and [SEP] token is used as a separator.\nLink Prediction (LP): We deﬁne link prediction as same as KG-BERT (Yao et al., 2019), and this\nis our main target task. Given a training set S, the input xis a text sequence of (h,r,t). Each entity is\nrepresented as entity name and description, e.g., for triple (plant tissue, hypernym, plant structure), the\ninput sequence is as follows:\n[CLS] plant tissue, the tissue of a plant [SEP] hypernym [SEP] plant structure, any part of\na plant or fungus [SEP]\nThe model is trained to predict whether a given triple(h,r,t) is valid or not, and invalid triples are made\nby replacing head or tail entity with one of random entities. Let C be the ﬁnal hidden vector of [CLS]\ntoken, WLP ∈R2×H be a classiﬁcation layer for link prediction, and S′be a invalid triple set, then\nf(x) =softmax(CWT\nLP ) = [ˆy0,ˆy1], LLP = −\n∑\nx∈{S∪S′}\nylog ˆy1 + (1−y) logˆy0 (1)\n1739\n# of entities # of relations train validation test\nWN18RR 40,943 11 86,835 3,034 3,134\nFB15k-237 14,541 237 272,115 17,535 20,466\nTable 1: Statistics of datasets.\nwhere f(x) is the ﬁnal output of the model and y ∈ {0,1}is a label. Let the output of CWT\nLP be\n[s0,s1] ∈R2, then s1 is used as the ﬁnal ranking score in evaluation.\nRelation Prediction (RP): The model learns to classify the relation of two entities. The input is head\nand tail entity sequences, e.g., “[CLS] plant tissue, the tissue of a plant [SEP] plant structure, any part\nof a plant or fungus [SEP]”, then the model trains to predict the relation hypernym. The classiﬁcation\nlayer for relation prediction is WRP ∈RR×H where Ris the number of relations, and we minimize a\ncross-entropy loss.\ng(x) =softmax(CWT\nRP ), LRP = −\n∑\nx∈S\nylog g(x) (2)\nwhere g(x) is the output of the model and y∈RR is a class indicator.\nRelevance Ranking (RR): The objective of relevance ranking is to make valid triples keep higher\nscores than invalid triples. We use a margin ranking loss to provide a bigger gap between valid and\ninvalid triples. The input is the same as link prediction, and the classiﬁcation layer for relevance ranking\nis WRR ∈R1×H.\nh(x) =sigmoid(CWT\nRR), LRR =\n∑\nx∈S,x′∈S′\nmax{0,h(x′) −h(x) +λ} (3)\nwhere h(x) is the output of the model and λis a margin.\nIn the training time, we use mini-batch based stochastic gradient descent. We ﬁrst compose mini-\nbatches for each task, DLP ,DRP , and DRR, then combine all data D = DLP ∪DRP ∪DRR. At each\ntraining step, the mini-batch is randomly selected from D, and then the task corresponding to the batch\nis trained sequentially.\n3 Experiments\nDatasets We evaluated the proposed multi-task learning method on two benchmark datasets WN18RR\n(Dettmers et al., 2018) and FB15k-237 (Toutanova and Chen, 2015). Each dataset consists of a set of\ntriples in the form of (h,r,t). WN18RR is a subset of WordNet, which is a lexical database of English.\nThus, entities in WN18RR are words or short phrases, and there exists 11 relations between two words,\nsuch as hypernym and similar to. FB15k-237 is a subset of Freebase (Bollacker et al., 2008), a large-\nscale graph database including general human knowledge. FB15k-237 has more general entities, such as\nLincoln and Monaco, and relations are longer and more complex than WN18RR. We used the same entity\ndescriptions with Yao et al. (2019): synset deﬁnitions from WordNet for WN18RR and descriptions from\nXie et al. (2016) for FB15k-237. Table 1 summarizes our datasets.\nBaselines We mainly compare our method with KG-BERT (Yao et al., 2019), and also provide a\ncomparison with several outstanding models: TransE (Bordes et al., 2013), DistMult (Yang et al., 2014),\nComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), and RotatE (Sun et al., 2019).\nExperimental Settings We used pre-trained BERT-base as a shared layer and ﬁne-tuned over the\nmulti-task setup for 3 epochs. We used mini-batch size of 32 and Adam optimizer (Kingma and Ba,\n2014) with learning rate 2e-5. In relevance ranking, we set the margin λ on the validation set, and it\nshowed best results when λ= 0.1 .\nEvaluation Settings We evaluate our method on the link prediction, where the model predicts the\nhead entity given ( ,r,t) and tail entity given (h,r, ). To compare prior work, we follow the evaluation\nprotocol and ﬁltered setting in Bordes et al. (2013). Let E be a entity set and T be a set of all triples\nin train, valid, and test. Then, the set of test candidates U for predicting hin a given triple (h,r,t) is\n1740\nWN18RR FB15k-237\nMR MRR Hits@1 Hits@3 Hits@10 MR MRR Hits@1 Hits@3 Hits@10\nKG-BERT (Yao et al., 2019) 97 - - - 52.4 153 - - - 42.0\nKG-BERT (our results) 108 21.9 9.5 24.3 49.7 145 23.7 14.4 26.0 42.7\nLP + RP 112 30.2 17.7 35.3 56.0 138 26.2 16.9 28.9 44.7\nLP + RR 97 27.7 13.0 34.1 57.6 143 24.7 15.4 27.2 43.4\nLP + RP + RR 89 33.1 20.3 38.3 59.7 132 26.7 17.2 29.8 45.8\nTable 2: Link prediction results on WN18RR and FB15k-237. The second row shows our results of\nKG-BERT under the same implementation and hyperparameter settings as the original work.\nKG-BERT Ours\nrank entity score rank entity score\n1 snorkel breather 4.128 1 breathing time4.541\n2 breath 4.119 2 rest 4.442\n3 artiﬁcial respiration 4.118 3 relaxer 4.359\n4 respirator 4.114 4 time out 4.333\n5 relaxer 4.106 5 respirator 4.271\n6 take a breath 3.991 6 breath 4.195\n... ...\n22 breathing time 3.804\nTable 3: Example of results for the triple (take\na breather, derivationally related for, breathing\ntime) in WN18RR. Given(take a breather, deriva-\ntionally related for, ), the answer is breathing\ntime.\nWN18RR FB15k-237\nMR Hits@10 MR Hits@10\nTransE 2365 50.5 223 47.4\nDistMult 3704 47.7 411 41.9\nComplEx 3921 48.3 508 43.4\nConvE 5277 48.0 246 49.1\nRotatE 3340 57.1 177 53.3\nKG-BERT 97 52.4 153 42.0\nOurs 89 59.7 132 45.8\nTable 4: Comparison with previous state-of-the-\nart models. Results are taken from Yao et al.\n(2019).\nU = (h, r, t)∪{(h′,r,t)|h′∈E ∧(h′,r,t) /∈T}and U for predicting tis U = (h, r, t)∪{(h,r,t′)|t′∈\nE∧(h,r,t′) /∈T}. After the model computes scores of all candidate triples, they are sorted in descending\norder. The performances are evaluated in Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits@1,\n3, 10.\n3.1 Main Results\nTable 2 demonstrates how the proposed method improves performance over the baseline model on the\nlink prediction. The results show that multi-task learning with two tasks (LP + RP) and (LP + RR) could\nimprove over the baseline by a large margin maintaining low MR scores. When the model is trained on\nthree tasks (LP + RP + RR), we gain signiﬁcant improvements, especially in Hits@1 and Hits@3 with\n10.8 and 14.0, respectively. Table 3 shows an example of results in WN18RR. We observe that our model\ncan choose the correct answer “breathing time” as the ﬁrst ranking among lexically similar words, while\nthe KG-BERT predicts “snorkel breather” and “breath” in top ranks. More examples are presented in\nAppendix A.\nIn the FB15k-237 benchmark, the task becomes more challenging as the number of relations increases\nup to 237, whereas the WN18RR contains only 11 relations. Thus, joint training with Relation Prediction\n(RP) was more effective on the FB15k-237, and this is shown as results that the model outperformed the\nbaseline by 7, 2.5, 2.5, 2.9, and 2 absolute scores on MR, MRR, Hits@1, Hits@3, and Hits@10, respec-\ntively. When the Relevance Ranking (RR) task is added, and the model is trained with three different\ntasks, it achieves further improvements in all metrics with 13, 3, 2.8, 3.8, and 3.1 points, respectively.\nA Comparison with previous models is presented in Table 4. Our model achieved state-of-the-art\nperformances in MR and hits@10 on the WN18RR. In the FB15k-237 dataset, the performance of our\nmodel is lower than that of several models in Hits@10. Since FB15k-237 has more relations and a more\ncomplex graph structure than WN18RR, we conjecture that pre-trained language models cannot capture\nthe complex structural information in knowledge graphs. Despite that, we achieved the best MR score\non FB15k-237.\n1741\n4 Related Work\nA common approach for the knowledge graph completion is learning vector embeddings of the entities\nand the relationships in KG (Bordes et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Sun et al., 2019;\nDettmers et al., 2018). The most widely used method is TransE (Bordes et al., 2013), which models the\nrelationships as translations in low-dimensional vector space. Dettmers et al. (2018) and Nguyen et al.\n(2018) proposed the embedding models using a convolutional neural network. Recent research has shown\nthat the relation in complex vector space can infer the connectivity patterns: symmetry/antisymmetry,\ninversion, and composition (Sun et al., 2019). On the one hand, Yao et al. (2019) proposed KG-BERT\nthat uses pre-trained language models (PLM) with entity descriptions. It can capture the contextualized\nmeaning of entities and signiﬁcantly improve mean ranks with rich linguistic information from PLM.\nMulti-task learning has gained popularity over a decade in natural language processing (Collobert and\nWeston, 2008; Luong et al., 2015; Hashimoto et al., 2017; Liu et al., 2019b) of various tasks. It aims to\nregularize deep learning models from overﬁtting by sharing parameters of different tasks while jointly\ntraining them. With the advent of powerful PLMs such as BERT (Devlin et al., 2018) and XLNet (Yang\net al., 2019), a multi-task learning scheme is applied by sharing pre-trained parameters of these models\nwhen training different tasks simultaneously.\n5 Conclusion and Future Work\nWe propose an effective multi-task learning method for knowledge graph completion by combining re-\nlation prediction and relevance ranking tasks with link prediction. Experimental results demonstrate that\nour method outperforms previous strong baselines, and we largely improve MRR and Hits@k compared\nto the previous KG-BERT model.\nIn the future, we plan to investigate how to combine pre-trained language models and graph embed-\nding methods to fully utilize the prior linguistic information of pre-trained models and graph structural\ninformation.\nAcknowledgements\nThis work was supported by Institute for Information & communications Technology Planning & Eval-\nuation(IITP) grant funded by the Korea government(MSIT) (No. 2020-0-00368, A Neural-Symbolic\nModel for Knowledge Acquisition and Inference Techniques).\nReferences\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively\ncreated graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD Interna-\ntional Conference on Management of Data, SIGMOD ’08, page 1247–1250, New York, NY , USA. Association\nfor Computing Machinery.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translat-\ning embeddings for modeling multi-relational data. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani,\nand K. Q. Weinberger, editors,Advances in Neural Information Processing Systems 26, pages 2787–2795. Cur-\nran Associates, Inc.\nRonan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural language processing: Deep neural\nnetworks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning ,\nICML ’08, page 160–167, New York, NY , USA. Association for Computing Machinery.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2d knowledge\ngraph embeddings. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nKazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A joint many-task model:\nGrowing a neural network for multiple NLP tasks. In Proceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1923–1933, Copenhagen, Denmark, September. Association for\nComputational Linguistics.\n1742\nXiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. 2019. Knowledge graph embedding based question\nanswering. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining ,\nWSDM ’19, page 105–113, New York, NY , USA. Association for Computing Machinery.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. cite\narxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference for Learning\nRepresentations, San Diego, 2015.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2019a. K-bert: Enabling\nlanguage representation with knowledge graph. arXiv preprint arXiv:1909.07606.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019b. Multi-task deep neural networks for nat-\nural language understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 4487–4496, Florence, Italy, July. Association for Computational Linguistics.\nMinh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2015. Multi-task sequence to\nsequence learning. arXiv preprint arXiv:1511.06114.\nDai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, and Dinh Phung. 2018. A novel embedding model for\nknowledge base completion based on convolutional neural network. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 2 (Short Papers), pages 327–333, New Orleans, Louisiana, June. Association for Computational\nLinguistics.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith.\n2019. Knowledge enhanced contextual word representations. In Proceedings of the 2019 Conference on Empir-\nical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 43–54, Hong Kong, China, November. Association for Computational\nLinguistics.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. Rotate: Knowledge graph embedding by\nrelational rotation in complex space. arXiv preprint arXiv:1902.10197.\nKristina Toutanova and Danqi Chen. 2015. Observed versus latent features for knowledge base and text inference.\nIn Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality , pages\n57–66, Beijing, China, July. Association for Computational Linguistics.\nTh´eo Trouillon, Johannes Welbl, Sebastian Riedel, ´Eric Gaussier, and Guillaume Bouchard. 2016. Complex\nembeddings for simple link prediction. International Conference on Machine Learning (ICML).\nRuobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. 2016. Representation learning of knowledge\ngraphs with entity descriptions. In Thirtieth AAAI Conference on Artiﬁcial Intelligence.\nBishan Yang and Tom Mitchell. 2017. Leveraging knowledge bases in LSTMs for improving machine reading.\nIn Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1436–1446, Vancouver, Canada, July. Association for Computational Linguistics.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2014. Embedding entities and relations for\nlearning and inference in knowledge bases. arXiv preprint arXiv:1412.6575.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xl-\nnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information\nProcessing Systems 32, pages 5753–5763. Curran Associates, Inc.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg-bert: Bert for knowledge graph completion.arXiv preprint\narXiv:1909.03193.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced lan-\nguage representation with informative entities. InProceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 1441–1451, Florence, Italy, July. Association for Computational Linguistics.\n1743\nAppendix A Examples of the results in Link Prediction\nE1. Given(take a breather, derivationally related form,), the answer isbreathing time\nTransE RotatE KG-BERT Ours\nrank entity score rank entity score rank entity score rank entity score\n1 take a breather 5.9748 1 respiratory 0.0338 1 snorkel breather 4.1283 1 breathing time4.5419\n2 rest 4.0482 2 respirator -0.2770 2 breath 4.1192 2 rest 4.4420\n3 pause 1.2906 3 caesura -0.4172 3 artiﬁcial respiration 4.11813 relaxer 4.3598\n4 rest 0.3586 4 blackout -0.4752 4 respirator 4.1147 4 time out 4.3331\n5 respire 0.2834 5 intake -0.6387 5 relaxer 4.1060 5 respirator 4.2710\n6 rest 0.2108 6 time out -0.7105 6 take a breath 3.9910 6 breath 4.1959\n... ... ... ...\n9 breathing time 11 breathing time 22 breathing time\nE2. Given(, hypernym, piece of music), the answer isandante\nTransE RotatE KG-BERT Ours\nrank entity score rank entity score rank entity score rank entity score\n1 piece of music 2.8572 1 piece of music 1.4983 1 composition 4.1524 1 sonata 3.9856\n2 melodize 0.9326 2 andante 0.6013 2 piece of music 4.1427 2 piece of music 3.9836\n3 soloist 0.8523 3 music 0.3708 3 ﬁnale 4.1155 3 andante 3.9605\n4 realize 0.7959 4 tune -0.0011 4 theme 4.1149 4 harmonization 3.9135\n5 write 0.7664 5 serenade -0.1049 5 andante 4.1021 5 composition 3.9127\n6 score 0.6901 6 tucket -0.1136 6 recapitulation 4.0845 6 ﬁnale 3.7482\n7 andante 0.6199 7 strain -0.1219 7 sonata 4.0736 7 fragment 3.7108\nE3. Given(systems software, hypernym,), the answer isprogramme\nTransE RotatE KG-BERT Ours\nrank entity score rank entity score rank entity score rank entity score\n1 systems software 2.85721 systems software -9.82551 systems software 4.1226 1 programme 4.1257\n2 inﬂuence -9.7353 2 location -10.0784 2 programme 4.1201 2 utility program 4.0341\n3 concert -9.7715 3 horse -10.1251 3 applications programme 4.11463 programme 4.0108\n4 tap -9.7720 4 learned profession -10.13454 utility program 4.1045 4 software system 3.9995\n5 fellow traveller -9.77675 chemistry -10.1458 5 compiling program 4.10245 programming 3.9647\n6 landing -9.7976 6 ofﬁciate -10.1596 6 object-oriented\nprogramming language4.0856 6 systems software 3.8621\n... ...\n8235programme 16452programme\nTable 5: Examples of results in Link Prediction.\nFor the example 1, the entitybreathing timeappears only once in the training set. Thus, the methods using\nonly graph structure information, such as TransE and RotatE, cannot predict well on the given triple. Our\nmodel provides the correct answer, while KG-BERT predicts snorkel breather and breath as top scores\ndue to the lexical similarity by breath. In example 2, the entity piece of music has lots of relationships\nwith other entities; thus, most models show low performance on that example. Lastly, the example 3\nshows that how the pre-trained language model (PLM) improves Mean Rank signiﬁcantly. KG-BERT\nand our model give a high score for the answerprogramme using preliminary linguistic information from\nPLM, but the results of TransE and RotatE are extremely low.\nAppendix B Computing Infrastructure\nWe ran all experiments on a single NVIDIA Titan RTX (24GB) with CUDA 10.1 version.\nAppendix C Implementation\nThe source code of the paper is available at https://github.com/bosung/MTL-KGC.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7812455296516418
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.632373034954071
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6247470378875732
    },
    {
      "name": "Learning to rank",
      "score": 0.624125063419342
    },
    {
      "name": "Knowledge graph",
      "score": 0.6093177795410156
    },
    {
      "name": "Machine learning",
      "score": 0.5468133091926575
    },
    {
      "name": "Natural language processing",
      "score": 0.5427011847496033
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5218052864074707
    },
    {
      "name": "Relevance (law)",
      "score": 0.5193070769309998
    },
    {
      "name": "Task (project management)",
      "score": 0.5142495036125183
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.5117859244346619
    },
    {
      "name": "Graph",
      "score": 0.5092977285385132
    },
    {
      "name": "Relation (database)",
      "score": 0.48529714345932007
    },
    {
      "name": "Information retrieval",
      "score": 0.34805262088775635
    },
    {
      "name": "Data mining",
      "score": 0.18955186009407043
    },
    {
      "name": "Theoretical computer science",
      "score": 0.13052299618721008
    },
    {
      "name": "Mathematics",
      "score": 0.07533702254295349
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I848706",
      "name": "Sungkyunkwan University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I148751991",
      "name": "Sogang University",
      "country": "KR"
    }
  ]
}