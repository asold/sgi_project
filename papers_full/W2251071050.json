{
  "title": "Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models",
  "url": "https://openalex.org/W2251071050",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A2139710560",
      "name": "Michael Auli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104437897",
      "name": "Jian-Feng Gao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2164732909",
    "https://openalex.org/W2250993494",
    "https://openalex.org/W2146574666",
    "https://openalex.org/W2252102852",
    "https://openalex.org/W2251912507",
    "https://openalex.org/W2095755718",
    "https://openalex.org/W2163548102",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W2251098065",
    "https://openalex.org/W2250379827",
    "https://openalex.org/W2296386938",
    "https://openalex.org/W2100183594",
    "https://openalex.org/W2143564602",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2250445771",
    "https://openalex.org/W932413789",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2096557251",
    "https://openalex.org/W2100238596",
    "https://openalex.org/W1575384945",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2250610505",
    "https://openalex.org/W2766736793",
    "https://openalex.org/W1595109586",
    "https://openalex.org/W2250489405",
    "https://openalex.org/W2058695628",
    "https://openalex.org/W2124807415"
  ],
  "abstract": "Neural network language models are often trained by optimizing likelihood, but we would prefer to optimize for a task specific metric, such as BLEU in machine translation. We show how a recurrent neural network language model can be optimized towards an expected BLEU loss instead of the usual cross-entropy criterion. Furthermore, we tackle the issue of directly integrating a recurrent network into firstpass decoding under an efficient approximation. Our best results improve a phrasebased statistical machine translation system trained on WMT 2012 French-English data by up to 2.0 BLEU, and the expected BLEU objective improves over a crossentropy trained model by up to 0.6 BLEU in a single reference setup.",
  "full_text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 136–142,\nBaltimore, Maryland, USA, June 23-25 2014.c⃝2014 Association for Computational Linguistics\nDecoder Integration and Expected BLEU Training\nfor Recurrent Neural Network Language Models\nMichael Auli\nMicrosoft Research\nRedmond, W A, USA\nmichael.auli@microsoft.com\nJianfeng Gao\nMicrosoft Research\nRedmond, W A, USA\njfgao@microsoft.com\nAbstract\nNeural network language models are often\ntrained by optimizing likelihood, but we\nwould prefer to optimize for a task speciﬁc\nmetric, such as BLEU in machine trans-\nlation. We show how a recurrent neural\nnetwork language model can be optimized\ntowards an expected BLEU loss instead\nof the usual cross-entropy criterion. Fur-\nthermore, we tackle the issue of directly\nintegrating a recurrent network into ﬁrst-\npass decoding under an efﬁcient approxi-\nmation. Our best results improve a phrase-\nbased statistical machine translation sys-\ntem trained on WMT 2012 French-English\ndata by up to 2.0 BLEU, and the expected\nBLEU objective improves over a cross-\nentropy trained model by up to 0.6 BLEU\nin a single reference setup.\n1 Introduction\nNeural network-based language and translation\nmodels have achieved impressive accuracy im-\nprovements on statistical machine translation tasks\n(Allauzen et al., 2011; Le et al., 2012b; Schwenk\net al., 2012; Vaswani et al., 2013; Gao et al., 2014).\nIn this paper we focus on recurrent neural network\narchitectures which have recently advanced the\nstate of the art in language modeling (Mikolov et\nal., 2010; Mikolov et al., 2011; Sundermeyer et al.,\n2013) with several subsequent applications in ma-\nchine translation (Auli et al., 2013; Kalchbrenner\nand Blunsom, 2013; Hu et al., 2014). Recurrent\nmodels have the potential to capture long-span de-\npendencies since their predictions are based on an\nunbounded historyof previous words (§2).\nIn practice, neural network models for machine\ntranslation are usually trained by maximizing the\nlikelihood of the training data, either via a cross-\nentropy objective (Mikolov et al., 2010; Schwenk\net al., 2012) or more recently, noise-contrastive es-\ntimation (Vaswani et al., 2013). However, it is\nwidely appreciated that directly optimizing for a\ntask-speciﬁc metric often leads to better perfor-\nmance (Goodman, 1996; Och, 2003; Auli and\nLopez, 2011). The expected BLEU objective pro-\nvides an efﬁcient way of achieving this for ma-\nchine translation (Rosti et al., 2010; Rosti et al.,\n2011; He and Deng, 2012; Gao and He, 2013;\nGao et al., 2014) instead of solely relying on tra-\nditional optimizers such as Minimum Error Rate\nTraining (MERT) that only adjust the weighting\nof entire component models within the log-linear\nframework of machine translation (§3).\nMost previous work on neural networks for ma-\nchine translation is based on a rescoring setup\n(Arisoy et al., 2012; Mikolov, 2012; Le et al.,\n2012a; Auli et al., 2013), thereby side stepping\nthe algorithmic and engineering challenges of di-\nrect decoder-integration. One recent exception is\nVaswani et al. (2013) who demonstrated that feed-\nforward network-based language models are more\naccurate in ﬁrst-pass decoding than in rescoring.\nDecoder integration has the advantage for the neu-\nral network to directly inﬂuence search, unlike\nrescoring which is restricted to an n-best list or lat-\ntice. Decoding with feed-forward architectures is\nstraightforward, since predictions are based on a\nﬁxed size input, similar to n-gram language mod-\nels. However, for recurrent networks we have to\ndeal with the unbounded history, which breaks the\nusual dynamic programming assumptions for efﬁ-\ncient search. We show how a simple but effective\napproximation can side step this issue and we em-\npirically demonstrate its effectiveness (§4).\nWe test the expected BLEU objective by train-\ning a recurrent neural network language model\nand obtain substantial improvements. We also ﬁnd\nthat our efﬁcient approximation for decoder inte-\ngration is very accurate, clearly outperforming a\nrescoring setup (§5).\n136\nwt\nht-1\nht\nyt\nV\nW\nU\n0\n0\n1\n0\n0\n0\nFigure 1: Structure of the recurrent neural network\nlanguage model.\n2 Recurrent Neural Network LMs\nOur model has a similar structure to the recurrent\nneural network language model of Mikolov et al.\n(2010) which is factored into an input layer, a hid-\nden layer with recurrent connections, and an out-\nput layer (Figure 1). The input layer encodes the\nword at position t as a 1-of-N vector wt. The out-\nput layer yt represents scores over possible next\nwords; both the input and output layers are of size\n|V|, the size of the vocabulary. The hidden layer\nstate ht encodes the history of all words observed\nin the sequence up to time step t. The state of\nthe hidden layer is determined by the input layer\nand the hidden layer conﬁguration of the previous\ntime step ht−1. The weights of the connections\nbetween the layers are summarized in a number\nof matrices: U represents weights from the in-\nput layer to the hidden layer, and W represents\nconnections from the previous hidden layer to the\ncurrent hidden layer. Matrix V contains weights\nbetween the current hidden layer and the output\nlayer. The activations of the hidden and output\nlayers are computed by:\nht = tanh(Uwt + Wht−1)\nyt = tanh(Vht)\nDifferent to previous work (Mikolov et al., 2010),\nwe do not use the softmax activation function to\noutput a probability over the next word, but in-\nstead just compute a single unnormalized score.\nThis is computationally more efﬁcient than sum-\nming over all possible outputs such as required\nfor the cross-entropy error function (Bengio et al.,\n2003; Mikolov et al., 2010; Schwenk et al., 2012).\nTraining is based on the back propagation through\ntime algorithm, which unrolls the network and\nthen computes error gradients over multiple time\nsteps (Rumelhart et al., 1986); we use the expected\nBLEU loss (§3) to obtain the error with respect to\nthe output activations. After training, the output\nlayer represents scores s(wt+1|w1 ...w t,ht) for\nthe next word given the previoustinput words and\nthe current hidden layer conﬁguration ht.\n3 Expected BLEU Training\nWe integrate the recurrent neural network lan-\nguage model as an additional feature into the stan-\ndard log-linear framework of translation (Och,\n2003). Formally, our phrase-based model is pa-\nrameterized by M parameters Λ where each λm ∈\nΛ, m = 1 ...M is the weight of an associated\nfeature hm(f,e). Function h(f,e) maps foreign\nsentences f and English sentences eto the vector\nh1(f,e) ... (f,e), and the model chooses transla-\ntions according to the following decision rule:\nˆe= arg max\ne∈E(f)\nΛTh(f,e)\nWe summarize the weights of the recurrent neural\nnetwork language model as θ= {U,W,V}and\nadd the model as an additional feature to the log-\nlinear translation model using the simpliﬁed nota-\ntion sθ(wt) = s(wt|w1 ...w t−1,ht−1):\nhM+1(e) = sθ(e) =\n|e|∑\nt=1\nlog sθ(wt) (1)\nwhich computes a sentence-level language model\nscore as the sum of individual word scores. The\ntranslation model is parameterized by Λ and θ\nwhich are learned as follows (Gao et al., 2014):\n1. We generate an n-best list for each foreign\nsentence in the training data with the baseline\ntranslation system given Λ where λM+1 = 0\nusing the settings described in §5. The n-best\nlists serve as an approximation to E(f) used\nin the next step for expected BLEU training\nof the recurrent neural network model (§3.1).\n2. Next, we ﬁx Λ, set λM+1 = 1 and opti-\nmize θ with respect to the loss function on\nthe training data using stochastic gradient de-\nscent (SGD).1\n1We tuned λM+1 on the development set but found that\nλM+1 = 1resulted in faster training and equal accuracy.\n137\n3. We ﬁx θ and re-optimize Λ in the presence\nof the recurrent neural network model using\nMinimum Error Rate Training (Och, 2003)\non the development set (§5).\n3.1 Expected BLEU Objective\nFormally, we deﬁne our loss function l(θ) as\nthe negative expected BLEU score, denoted as\nxBLEU(θ) for a given foreign sentence f:\nl(θ) = −xBLEU(θ)\n=\n∑\ne∈E(f)\npΛ,θ(e|f)sBLEU(e,e(i)) (2)\nwhere sBLEU (e,e(i)) is a smoothed sentence-\nlevel BLEU score with respect to the reference\ntranslation e(i), and E(f) is the generation set\ngiven by an n-best list. 2 We use a sentence-level\nBLEU approximation similar to He and Deng\n(2012).3 The normalized probability pΛ,θ(e|f) of\na particular translation egiven f is deﬁned as:\npΛ,θ(e|f) = exp{γΛTh(f,e)}∑\ne′∈E(f) exp{γΛTh(f,e′)} (3)\nwhere ΛTh(f,e) includes the recurrent neural net-\nwork hM+1(e), and γ ∈[0,inf) is a scaling factor\nthat ﬂattens the distribution for γ <1 and sharp-\nens it for γ >1 (Tromble et al., 2008).4\nNext, we deﬁne the gradient of the expected\nBLEU loss functionl(θ) using the observation that\nthe loss does not explicitly depend on θ:\n∂l(θ)\n∂θ =\n∑\ne\n|e|∑\nt=1\n∂l(θ)\n∂sθ(wt)\n∂sθ(wt)\n∂θ\n=\n∑\ne\n|e|∑\nt=1\n−δwt\n∂sθ(wt)\n∂θ\nwhere δwt is the error termfor English word wt.5\nThe error term indicates how the loss changes with\nthe translation probability which we derive next.6\n2Our deﬁnitions do not take into account multiple derivations\nfor the same translation because our n-best lists contain only\nunique entries which we obtain by choosing the highest scor-\ning translation among string identical candidates.\n3In early experiments we found that the BLEU+1 approxi-\nmation used by Liang et al. (2006) and Nakov et. al (2012)\nworked equally well in our setting.\n4The γ parameter is only used during expected BLEU training\nbut not for subsequent MERT tuning.\n5A sentence may contain the same word multiple times and\nwe compute the error term for each occurrence separately\nsince the error depends on the individual history.\n6We omit the gradient of the recurrent neural network score\n∂sθ(wt)\n∂θ since it follows the standard form (Mikolov, 2012).\n3.2 Derivation of the Error Termδwt\nWe rewrite the loss function (2) using (3) and sep-\narate it into two terms G(θ) and Z(θ) as follows:\nl(θ) = −xBLEU(θ) = −G(θ)\nZ(θ) (4)\n= −\n∑\ne∈E(f) exp{γΛTh(f,e)}sBLEU(e,e(i))\n∑\ne∈E(f) exp{γΛTh(f,e)}\nNext, we apply the quotient rule of differentiation:\nδwt = ∂xBLEU(θ)\n∂sθ(wt) = ∂(G(θ)/Z(θ))\n∂sθ(wt)\n= 1\nZ(θ)\n( ∂G(θ)\n∂sθ(wt) − ∂Z(θ)\n∂sθ(wt)xBLEU(θ)\n)\nUsing the observation that θis only relevant to the\nrecurrent neural network hM+1(e) (1) we have\n∂γΛTh(f,e)\n∂sθ(wt) = γλM+1\n∂hM+1(e)\n∂sθ(wt) = γλM+1\nsθ(wt)\nwhich together with the chain rule, (3) and (4) al-\nlows us to rewrite δwt as follows:\nδwt = 1\nZ(θ)\n∑\ne∈E(f),\ns.t.wt∈e\n(∂exp{γΛTh(f,e)}\n∂sθ(wt) U(θ,e)\n)\n=\n∑\ne∈E(f),\ns.t.wt∈e\n(\npΛ,θ(e|f)U(θ,e)λM+1\nγ\nsθ(wt)\n)\nwhere U(θ,e) = sBLEU(e,ei) −xBLEU(θ).\n4 Decoder Integration\nDirectly integrating our recurrent neural network\nlanguage model into ﬁrst-pass decoding enables us\nto search a much larger space than would be pos-\nsible in rescoring.\nTypically, phrase-based decoders maintain a set\nof states representing partial and complete transla-\ntion hypothesis that are scored by a set of features.\nMost features are local, meaning that all required\ninformation for them to assign a score is available\nwithin the state. One exception is the n-gram lan-\nguage model which requires the preceding n−1\nwords as well. In order to accommodate this fea-\nture, each state usually keeps these words as con-\ntext. Unfortunately, a recurrent neural network\nmakes even weaker independence assumptions so\n138\nthat it depends on the entire left preﬁx of a sen-\ntence. Furthermore, the weaker independence as-\nsumptions also dramatically reduce the effective-\nness of dynamic programming by allowing much\nfewer states to be recombined.7\nTo solve this problem, we follow previous work\non lattice rescoring with recurrent networks that\nmaintained the usual n-gram context but kept a\nbeam of hidden layer conﬁgurations at each state\n(Auli et al., 2013). In fact, to make decoding as\nefﬁcient as possible, we only keep the single best\nscoring hidden layer conﬁguration. This approx-\nimation has been effective for lattice rescoring,\nsince the translations represented by each state are\nin fact very similar: They share both the same\nsource words as well as the same n-gram context\nwhich is likely to result in similar recurrent his-\ntories that can be safely pruned. As future cost\nestimate we score each phrase in isolation, reset-\nting the hidden layer at the beginning of a phrase.\nWhile simple, we found our estimate to be more\naccurate than no future cost at all.\n5 Experiments\nBaseline. We use a phrase-based system simi-\nlar to Moses (Koehn et al., 2007) based on a set\nof common features including maximum likeli-\nhood estimates pML(e|f) and pML(f|e), lexically\nweighted estimates pLW(e|f) and pLW(f|e),\nword and phrase-penalties, a hierarchical reorder-\ning model (Galley and Manning, 2008), a linear\ndistortion feature, and a modiﬁed Kneser-Ney lan-\nguage model trained on the target-side of the paral-\nlel data. Log-linear weights are tuned with MERT.\nEvaluation. We use training and test data from\nthe WMT 2012 campaign and report results on\nFrench-English and German-English. Transla-\ntion models are estimated on 102M words of par-\nallel data for French-English, and 99M words\nfor German-English; about 6.5M words for each\nlanguage pair are newswire, the remainder are\nparliamentary proceedings. We evaluate on six\nnewswire domain test sets from 2008 to 2013 con-\ntaining between 2034 to 3003 sentences. Log-\nlinear weights are estimated on the 2009 data set\ncomprising 2525 sentences. We evaluate accuracy\nin terms of BLEU with a single reference.\nRescoring Setup. For rescoring we use ei-\n7Recombination only retains the highest scoring state if there\nare multiple identical states, that is, they cover the same\nsource span, the same translation phrase and contexts.\nther lattices or the unique 100-best output of\nthe phrase-based decoder and re-estimate the log-\nlinear weights by running a further iteration of\nMERT on the n-best list of the development set,\naugmented by scores corresponding to the neural\nnetwork models. At test time we rescore n-best\nlists with the new weights.\nNeural Network Training. All neural network\nmodels are trained on the news portion of the\nparallel data, corresponding to 136K sentences,\nwhich we found to be most useful in initial exper-\niments. As training data we use unique 100-best\nlists generated by the baseline system. We use the\nsame data both for training the phrase-based sys-\ntem as well as the language model but ﬁnd that\nthe resulting bias did not hurt end-to-end accu-\nracy (Yu et al., 2013). The vocabulary consists of\nwords that occur in at least two different sentences,\nwhich is 31K words for both language pairs. We\ntuned the learning rate µ of our mini-batch SGD\ntrainer as well as the probability scaling parameter\nγ(3) on a held-out set and found simple settings of\nµ= 0.1 and γ = 1 to be good choices. To prevent\nover-ﬁtting, we experimented with L2 regulariza-\ntion, but found no accuracy improvements, prob-\nably because SGD regularizes enough. We evalu-\nate performance on a held-out set during training\nand stop whenever the objective changes less than\n0.0003. The hidden layer uses 100 neurons unless\notherwise stated.\n5.1 Decoder Integration\nWe compare the effect of direct decoder integra-\ntion to rescoring with both lattices and n-best lists\nwhen the model is trained with a cross-entropy ob-\njective (Mikolov et al., 2010). The results (Ta-\nble 1 and Table 2) show that direct integration im-\nproves accuracy across all six test sets on both lan-\nguage pairs. For French-English we improve over\nn-best rescoring by up to 1.1 BLEU and by up to\n0.5 BLEU for German-English. We improve over\nlattice rescoring by up to 0.4 BLEU on French-\nEnglish and by up to 0.3 BLEU on German-\nEnglish. Compared to the baseline, we achieve\nimprovements of up to 2.0 BLEU for French-\nEnglish and up to 1.3 BLEU for German-English.\nThe average improvement across all test sets is\n1.5 BLEU for French-English and 1.0 BLEU for\nGerman-English compared to the baseline.\n139\ndev 2008 2010 syscomb2010 2011 2012 2013 AllTest\nBaseline 24.11 20.73 24.68 24.59 25.62 24.85 25.54 24.53\nRNN n-best rescore 24.83 21.41 25.17 25.06 26.53 25.74 26.31 25.25\nRNN lattice rescore 24.91 21.73 25.56 25.43 27.04 26.43 26.75 25.72\nRNN decode 25.14 22.03 25.86 25.74 27.32 26.86 27.15 26.06\nTable 1: French-English accuracy of decoder integration of a recurrent neural network language model\n(RNN decode) compared to n-best and lattice rescoring as well as the output of a phrase-based system\nusing an n-gram model (Baseline); Alltest is the corpus-weighted average BLEU across all test sets.\ndev 2008 2010 syscomb2010 2011 2012 2013 AllTest\nBaseline 19.35 19.96 20.87 20.66 19.60 19.80 22.48 20.58\nRNN n-best rescore 20.17 20.29 21.35 21.27 20.51 20.54 23.03 21.21\nRNN lattice rescore 20.24 20.38 21.55 21.43 20.77 20.63 23.23 21.38\nRNN decode 20.13 20.51 21.79 21.71 20.91 20.93 23.53 21.61\nTable 2: German-English results of direct decoder integration (cf. Table 1).\ndev 2008 2010 syscomb2010 2011 2012 2013 AllTest\nBaseline 24.11 20.73 24.68 24.59 25.62 24.85 25.54 24.53\nCE RNN 24.80 21.15 25.14 25.06 26.45 25.83 26.69 25.29\n+ xBLEU RNN 25.11 21.74 25.52 25.42 27.06 26.42 26.72 25.71\nTable 3: French-English accuracy of a decoder integrated cross-entropy recurrent neural network model\n(CE RNN) and a combination with an expected BLEU trained model (xBLEU RNN). Results are not\ncomparable to Table 1 since a smaller hidden layer was used to keep training times manageable (§5.2).\n5.2 Expected BLEU Training\nTraining with the expected BLEU loss is compu-\ntationally more expensive than with cross-entropy\nsince each training example is an n-best list in-\nstead of a single sentence. This increases the num-\nber of words to be processed from 3.5M to 340M.\nTo keep training times manageable, we reduce the\nhidden layer size to 30 neurons, thereby greatly\nincreasing speed. Despite slower training, the ac-\ntual scoring at test time of expected BLEU mod-\nels is about 5 times faster than for cross-entropy\nmodels since we do not need to normalize the out-\nput layer anymore. The results (Table 3) show\nimprovements of up to 0.6 BLEU when combin-\ning a cross-entropy model with an expected BLEU\nvariant. Average gains across all test sets are 0.4\nBLEU, demonstrating that the gains from the ex-\npected BLEU loss are additive.\n6 Conclusion and Future Work\nWe introduce an empirically effective approxima-\ntion to integrate a recurrent neural network model\ninto ﬁrst pass decoding, thereby extending pre-\nvious work on decoding with feed-forward neu-\nral networks (Vaswani et al., 2013). Our best re-\nsult improves the output of a phrase-based decoder\nby up to 2.0 BLEU on French-English translation,\noutperforming n-best rescoring by up to 1.1 BLEU\nand lattice rescoring by up to 0.4 BLEU. Directly\noptimizing a recurrent neural network language\nmodel towards an expected BLEU loss proves ef-\nfective, improving a cross-entropy trained variant\nby up 0.6 BLEU. Despite higher training complex-\nity, our expected BLEU trained model has ﬁve\ntimes faster runtime than a cross-entropy model\nsince it does not require normalization.\nIn future work, we would like to scale up to\nlarger data sets and more complex models through\nparallelization. We would also like to experiment\nwith more elaborate future cost estimates, such as\nthe average score assigned to all occurrences of a\nphrase in a large corpus.\n7 Acknowledgments\nWe thank Michel Galley, Arul Menezes, Chris\nQuirk and Geoffrey Zweig for helpful discussions\nrelated to this work as well as the four anonymous\nreviewers for their comments.\n140\nReferences\nAlexandre Allauzen, H ´el`ene Bonneau-Maynard, Hai-\nSon Le, Aur ´elien Max, Guillaume Wisniewski,\nFranc ¸ois Yvon, Gilles Adda, Josep Maria Crego,\nAdrien Lardilleux, Thomas Lavergne, and Artem\nSokolov. 2011. LIMSI @ WMT11. In Proc. of\nWMT, pages 309–315, Edinburgh, Scotland, July.\nAssociation for Computational Linguistics.\nEbru Arisoy, Tara N. Sainath, Brian Kingsbury, and\nBhuvana Ramabhadran. 2012. Deep Neural Net-\nwork Language Models. In NAACL-HLT Work-\nshop on the Future of Language Modeling for HLT,\npages 20–28, Stroudsburg, PA, USA. Association\nfor Computational Linguistics.\nMichael Auli and Adam Lopez. 2011. Training a\nLog-Linear Parser with Loss Functions via Softmax-\nMargin. In Proc. of EMNLP, pages 333–343. Asso-\nciation for Computational Linguistics, July.\nMichael Auli, Michel Galley, Chris Quirk, and Geof-\nfrey Zweig. 2013. Joint Language and Translation\nModeling with Recurrent Neural Networks. InProc.\nof EMNLP, October.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A Neural Probabilistic Lan-\nguage Model. Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nMichel Galley and Christopher D. Manning. 2008. A\nSimple and Effective Hierarchical Phrase Reorder-\ning Model. In Proc. of EMNLP, pages 848–856.\nJianfeng Gao and Xiaodong He. 2013. Training MRF-\nBased Phrase Translation Models using Gradient\nAscent. In Proc. of NAACL-HLT, pages 450–459.\nAssociation for Computational Linguistics, June.\nJianfeng Gao, Xiaodong He, Scott Wen tau Yih, and\nLi Deng. 2014. Learning Continuous Phrase Rep-\nresentations for Translation Modeling. In Proc.\nof ACL. Association for Computational Linguistics,\nJune.\nJoshua Goodman. 1996. Parsing Algorithms and Met-\nrics. In Proc. of ACL, pages 177–183, Santa Cruz,\nCA, USA, June.\nXiaodong He and Li Deng. 2012. Maximum Expected\nBLEU Training of Phrase and Lexicon Translation\nModels. In Proc. of ACL, pages 8–14. Association\nfor Computational Linguistics, July.\nYuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.\n2014. Minimum Translation Modeling with Recur-\nrent Neural Networks. In Proc. of EACL. Associa-\ntion for Computational Linguistics, April.\nNal Kalchbrenner and Phil Blunsom. 2013. Re-\ncurrent Continuous Translation Models. In Proc.\nof EMNLP, pages 1700–1709, Seattle, Washington,\nUSA, October. Association for Computational Lin-\nguistics.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nSource Toolkit for Statistical Machine Translation.\nIn Proc. of ACL Demo and Poster Sessions, pages\n177–180, Prague, Czech Republic, Jun.\nHai-Son Le, Alexandre Allauzen, and Franc ¸ois Yvon.\n2012a. Continuous Space Translation Models with\nNeural Networks. In Proc. of HLT-NAACL, pages\n39–48, Montr ´eal, Canada. Association for Compu-\ntational Linguistics.\nHai-Son Le, Thomas Lavergne, Alexandre Al-\nlauzen, Marianna Apidianaki, Li Gong, Aur ´elien\nMax, Artem Sokolov, Guillaume Wisniewski, and\nFranc ¸ois Yvon. 2012b. LIMSI @ WMT12. InProc.\nof WMT, pages 330–337, Montr ´eal, Canada, June.\nAssociation for Computational Linguistics.\nPercy Liang, Alexandre Bouchard-C ˆot´e, Ben Taskar,\nand Dan Klein. 2006. An end-to-end discriminative\napproach to machine translation. In Proc. of ACL-\nCOLING, pages 761–768, Jul.\nTom´aˇs Mikolov, Karaﬁ ´at Martin, Luk ´aˇs Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2010. Recur-\nrent Neural Network based Language Model. In\nProc. of INTERSPEECH, pages 1045–1048.\nTom´aˇs Mikolov, Anoop Deoras, Daniel Povey, Luk ´aˇs\nBurget, and Jan ˇCernock´y. 2011. Strategies for\nTraining Large Scale Neural Network Language\nModels. In Proc. of ASRU, pages 196–201.\nTom´aˇs Mikolov. 2012. Statistical Language Models\nbased on Neural Networks. Ph.D. thesis, Brno Uni-\nversity of Technology.\nPreslav Nakov, Francisco Guzman, and Stephan V o-\ngel. 2012. Optimizing for Sentence-Level BLEU+1\nYields Short Translations. In Proc. of COLING. As-\nsociation for Computational Linguistics.\nFranz Josef Och. 2003. Minimum Error Rate Training\nin Statistical Machine Translation. In Proc. of ACL,\npages 160–167, Sapporo, Japan, July.\nAntti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,\nand Richard Schwartz. 2010. BBN System De-\nscription for WMT10 System Combination Task.\nIn Proc. of WMT, pages 321–326. Association for\nComputational Linguistics, July.\nAntti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,\nand Richard Schwartz. 2011. Expected BLEU\nTraining for Graphs: BBN System Description for\nWMT11 System Combination Task. In Proc. of\nWMT, pages 159–165. Association for Computa-\ntional Linguistics, July.\nDavid E. Rumelhart, Geoffrey E. Hinton, and Ronald J.\nWilliams. 1986. Learning Internal Representations\nby Error Propagation. In Symposium on Parallel and\nDistributed Processing.\n141\nHolger Schwenk, Anthony Rousseau, and Mohammed\nAttik. 2012. Large, Pruned or Continuous Space\nLanguage Models on a GPU for Statistical Machine\nTranslation. In NAACL-HLT Workshop on the Fu-\nture of Language Modeling for HLT, pages 11–19.\nAssociation for Computational Linguistics.\nMartin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,\nBen Freiberg, Ralf Schl ¨uter, and Hermann Ney.\n2013. Comparison of Feedforward and Recurrent\nNeural Network Language Models. In IEEE Inter-\nnational Conference on Acoustics, Speech, and Sig-\nnal Processing, pages 8430–8434, May.\nRoy W. Tromble, Shankar Kumar, Franz Och, and\nWolfgang Macherey. 2008. Lattice Minimum\nBayes-Risk Decoding for Statistical Machine Trans-\nlation. In Proc. of EMNLP, pages 620–629. Associ-\nation for Computational Linguistics, October.\nAshish Vaswani, Yinggong Zhao, Victoria Fossum, and\nDavid Chiang. 2013. Decoding with Large-scale\nNeural Language Models improves Translation. In\nProc. of EMNLP. Association for Computational\nLinguistics, October.\nHeng Yu, Liang Huang, Haitao Mi, and Kai Zhao.\n2013. Max-Violation Perceptron and Forced Decod-\ning for Scalable MT Training. In Proc. of EMNLP,\npages 1112–1123. Association for Computational\nLinguistics, October.\n142",
  "topic": "BLEU",
  "concepts": [
    {
      "name": "BLEU",
      "score": 0.9575185775756836
    },
    {
      "name": "Computer science",
      "score": 0.87455153465271
    },
    {
      "name": "Machine translation",
      "score": 0.8160388469696045
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6929380893707275
    },
    {
      "name": "Decoding methods",
      "score": 0.6820329427719116
    },
    {
      "name": "Language model",
      "score": 0.6356077194213867
    },
    {
      "name": "Artificial neural network",
      "score": 0.5651566982269287
    },
    {
      "name": "Task (project management)",
      "score": 0.5394161343574524
    },
    {
      "name": "Metric (unit)",
      "score": 0.5346789360046387
    },
    {
      "name": "Natural language processing",
      "score": 0.529310405254364
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4828925132751465
    },
    {
      "name": "Cross entropy",
      "score": 0.45951950550079346
    },
    {
      "name": "Speech recognition",
      "score": 0.4450153112411499
    },
    {
      "name": "Principle of maximum entropy",
      "score": 0.3964768052101135
    },
    {
      "name": "Machine learning",
      "score": 0.39001381397247314
    },
    {
      "name": "Algorithm",
      "score": 0.1400248110294342
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}