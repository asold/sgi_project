{
  "title": "Automated generation of discharge summaries: leveraging large language models with clinical data",
  "url": "https://openalex.org/W4410302384",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1950146825",
      "name": "Matthias Ganzinger",
      "affiliations": [
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2770739818",
      "name": "Nicola Kunz",
      "affiliations": [
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2340999559",
      "name": "Pascal Fuchs",
      "affiliations": [
        "Heidelberg University",
        "University Hospital Heidelberg"
      ]
    },
    {
      "id": "https://openalex.org/A5117520602",
      "name": "Cornelia K Lyu",
      "affiliations": [
        "University Hospital Heidelberg",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2101901015",
      "name": "Martin Loos",
      "affiliations": [
        "Heidelberg University",
        "University Hospital Heidelberg"
      ]
    },
    {
      "id": "https://openalex.org/A2461720623",
      "name": "Martin Dugas",
      "affiliations": [
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A4296821550",
      "name": "Thomas M. Pausch",
      "affiliations": [
        "University Hospital Heidelberg",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A1950146825",
      "name": "Matthias Ganzinger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2770739818",
      "name": "Nicola Kunz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2340999559",
      "name": "Pascal Fuchs",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5117520602",
      "name": "Cornelia K Lyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101901015",
      "name": "Martin Loos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2461720623",
      "name": "Martin Dugas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296821550",
      "name": "Thomas M. Pausch",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1999685724",
    "https://openalex.org/W2165175575",
    "https://openalex.org/W3181151122",
    "https://openalex.org/W4402671338",
    "https://openalex.org/W2112169451",
    "https://openalex.org/W2538585973",
    "https://openalex.org/W4399276598",
    "https://openalex.org/W4210439206",
    "https://openalex.org/W4386867830",
    "https://openalex.org/W4368342518",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4311180795",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4392736665",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W7036002982",
    "https://openalex.org/W4385571357",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W4394672910",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2565017937",
    "https://openalex.org/W4365514966",
    "https://openalex.org/W2744160972",
    "https://openalex.org/W3140938259",
    "https://openalex.org/W4225464496"
  ],
  "abstract": "Abstract This study explores the use of open-source large language models (LLMs) to automate generation of German discharge summaries from structured clinical data. The structured data used to produce AI-generated summaries were manually extracted from electronic health records (EHRs) by a trained medical professional. By leveraging structured documentation collected for research and quality management, the goal is to assist physicians with editable draft summaries. After de-identifying 25 patient datasets, we optimized the output of the LLaMA3 model through prompt engineering and evaluated it using error analysis, as well as quantitative and qualitative metrics. The LLM-generated summaries were rated by physicians on comprehensiveness, conciseness, correctness, and fluency. Key results include an error rate of 2.84 mistakes per summary, and low-to-moderate alignment between generated and physician-written summaries (ROUGE-1: 0.25, BERTScore: 0.64). Medical professionals rated the summaries 3.72 ± 0.89 for comprehensiveness and 3.88 ± 0.97 for factual correctness on a 5-point Likert-scale; however, only 60% rated the comprehensiveness as good (4 or 5 out of 5). Despite overall informativeness, essential details—such as patient history, lifestyle factors, and intraoperative findings—were frequently omitted, reflecting gaps in summary completeness. While the LLaMA3 model captured much of the clinical information, complex cases and temporal reasoning presented challenges, leading to factual inaccuracies, such as incorrect age calculations. Limitations include a small dataset size, missing structured data elements, and the model’s limited proficiency with German medical terminology, highlighting the need for large, more complete datasets and potential model fine-tuning. In conclusion, this work provides a set of real-world methods, findings, experiences, insights, and descriptive results for a focused use case that may be useful to guide future work in the LLM generation of discharge summaries, perhaps especially for those working with German and possibly other non-English content.",
  "full_text": "Automated generation of discharge \nsummaries: leveraging large \nlanguage models with clinical data\nMatthias Ganzinger1,3, Nicola Kunz1,3, Pascal Fuchs2, Cornelia K. Lyu2, Martin Loos2, \nMartin Dugas1 & Thomas M. Pausch1,2\nThis study explores the use of open-source large language models (LLMs) to automate generation \nof German discharge summaries from structured clinical data. The structured data used to produce \nAI-generated summaries were manually extracted from electronic health records (EHRs) by a trained \nmedical professional. By leveraging structured documentation collected for research and quality \nmanagement, the goal is to assist physicians with editable draft summaries. After de-identifying 25 \npatient datasets, we optimized the output of the LLaMA3 model through prompt engineering and \nevaluated it using error analysis, as well as quantitative and qualitative metrics. The LLM-generated \nsummaries were rated by physicians on comprehensiveness, conciseness, correctness, and fluency. Key \nresults include an error rate of 2.84 mistakes per summary, and low-to-moderate alignment between \ngenerated and physician-written summaries (ROUGE-1: 0.25, BERTScore: 0.64). Medical professionals \nrated the summaries 3.72 ± 0.89 for comprehensiveness and 3.88 ± 0.97 for factual correctness on a \n5-point Likert-scale; however, only 60% rated the comprehensiveness as good (4 or 5 out of 5). Despite \noverall informativeness, essential details—such as patient history, lifestyle factors, and intraoperative \nfindings—were frequently omitted, reflecting gaps in summary completeness. While the LLaMA3 \nmodel captured much of the clinical information, complex cases and temporal reasoning presented \nchallenges, leading to factual inaccuracies, such as incorrect age calculations. Limitations include a \nsmall dataset size, missing structured data elements, and the model’s limited proficiency with German \nmedical terminology, highlighting the need for large, more complete datasets and potential model \nfine-tuning. In conclusion, this work provides a set of real-world methods, findings, experiences, \ninsights, and descriptive results for a focused use case that may be useful to guide future work in \nthe LLM generation of discharge summaries, perhaps especially for those working with German and \npossibly other non-English content.\nClinical documentation serves a variety of purposes, each with specific requirements. Typically, these \ndocumentation efforts fall into two categories: structured documentation and free text documentation. In this \ncontext, we consider structured documentation to be data stored with reference to a well-defined coding scheme, \nas opposed to a defined structure within a narrative text. Structured documentation is particularly important \nwhen patient data are analyzed across cases, for example in quality assurance, clinical research, or outcome \nbenchmarking. Research relies on structured data for enrolling patients in clinical trials and for quantitative \nanalysis of patient data in general. This is also true for quality assurance in clinical care 1–4. Structured data are \nalso more accessible to decision support systems, helping to improve patient care.\nOn the other hand, narrative documentation still plays a vital role in the hospital: At the end of a hospital \nstay, each patient receives a discharge summary describing medical history that has caused hospitalization, \ndiagnostics, treatment, and the course of the inpatient stay. This important document is written by a physician \nfor each discharged patient and is the basis for post-hospital care 1. It is typically shared with the patients, their \ngeneral practitioners, and any relevant outpatient specialists. By facilitating communication between healthcare \nprofessionals, the discharge summary plays a vital role in ensuring continuity of care and guiding the patient’s \npost-hospital treatment5–7. Given its importance, both the completeness and accuracy of the discharge summary \nare essential.\nSeveral studies have examined the impact and quality of structured (tabular, e.g.) discharge summaries as \nopposed to narrative summaries, which may be richer in describing the individual case 2,3. Our work is not \n1Institute of Medical Informatics, Heidelberg University, Heidelberg, Germany. 2Department of General, Visceral, \nand Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany. 3Matthias Ganzinger and \nNicola Kunz contributed equally to this work. email: matthias.ganzinger@med.uni-heidelberg.de\nOPEN\nScientific Reports |        (2025) 15:16466 1| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports\n\nintended to add to that discussion or to convince physicians to change their documentation practices. In our \nhospital, it is standard practice to provide a narrative discharge summary for each patient. In addition, a second, \nstructured documentation is maintained for research and quality assurance purposes. We are exploring how we \ncan possibly support medical professionals in such a scenario.\nIn any case, the process of writing discharge summaries is a time-consuming task for physicians. Arndt et \nal.8 found that physicians spend approximately 44% of their daily working hours on electronic health record \n(EHR) management, with a significant portion of this time dedicated to documentation tasks. In addition, \ntimely completion of discharge summaries is crucial. These documents should ideally be available on discharge \nand are for that reason time-critical documents. If the summaries are sent later, the risk of rehospitalization and \nmedication errors might increase9,10.\nThis dual requirement to provide both a comprehensive narrative discharge summary and structured \ndocumentation places an additional burden on documenting physicians. To assist physicians and possibly save \nthem some time, automatically generating discharge summaries from structured EHR data could reduce the \nneed for manual writing, allowing physicians to focus more on patient care and clinical decision-making 4. \nSeveral efforts have been made to automate the generation of discharge summaries using natural language \ngeneration techniques. Early methods focused on rule-based systems that leveraged text components and \nmedical ontologies5. These systems relied on a narrowly defined domain-specific rule base, making it difficult to \nscale this approach to other areas of medicine. More recently, transformer-based models, such as Bidirectional \nEncoder Representations from Transformers (BERT) and Bidirectional and Auto-Regressive Transformers \n(BART), have been employed to classify relevant text and generate summaries based on EHR data3. Additionally, \ngenerative pre-trained transformer (GPT) models have shown promising results in automating this task5–8.\nFor instance, Aali et al. and Van Veen et al. compared various LLM and benchmarked them based on a task \nof summarizing free text clinical documents. They compared several LLMs, including LLaMA2-13B and GPT4, \nagainst human written texts, both qualitatively and quantitatively9,10. Schwieger et al. followed a similar approach \nby generating psychiatric discharge summaries with ChatGPT-4 from electronic health records8. They reported \nthat one of their most significant evaluation questions for physicians was whether the generated summaries were \nready for use without manual revision. In contrast, Ellershaw et al. focused on generating discharge summaries \nbased on clinical guidelines and physician notes by means of LLM. For their study, the authors used the Medical \nInformation Mart for Intensive Care III (MIMIC-III) data set and had their results evaluated by clinicians11,12.\nDespite these advancements, the effectiveness of LLMs varies significantly depending on the domain and \nlanguage in which they are applied6. Many existing models are trained primarily on English datasets that often \nlack the specialized medical terminology necessary for clinical settings. So far, there are only few approaches for \nGerman medical data7.\nHere, we explore approaches to generating German discharge summaries from structured German clinical \ndata using open-source LLMs and describe the quality of the generated summaries. Unlike other studies, our \ngoal is not to produce a discharge summary that can be sent without further review. Instead, we focus on \nsupporting physicians with a body of text that they can revise during the important process of reflecting on the \npatient’s course of treatment. This study explores whether structured data, originally collected for science and \nquality management, can be secondarily used for generating clinical documentation. We utilized EHR data from \n25 cases of pancreatic surgery at Heidelberg University Hospital, aiming to evaluate LLM-generated summaries \nin a non-English medical context. To accomplish this, we applied prompt engineering (PE) techniques to create \na tailored prompt, developed a structured data scheme, and successfully generated discharge summaries. This \nstudy builds upon prior work that established structured data collection in pancreatic surgery integrating an \nelectronic data collection platform, called “IMI-EDC” . We connected the platform to the established research \npatient registry of our pancreatic surgery center via API to gain a next generation database with semi-automated \ndata collection15. We addressed both the technical and clinical challenges of automated summary generation for \nthis complex surgical field. We conducted a systematic error analysis, as well as both quantitative and qualitative \nevaluations of the generated summaries to assess the accuracy, completeness, fluency, and relevance compared \nto those written by physicians, identifying potential improvements and limitations of the automated approach. \nWith this study, we aim to explore a potential pathway for integrating AI-driven documentation tools into \nclinical workflows, to assist documentation where structured and narrative documentation are required.\nResults\nClinical data included\nThis study utilized data from 25 patients who had undergone pancreatic surgery at the European Pancreas \nCenter of Heidelberg University Hospital and had been treated in either an inpatient unit or an intermediate \ncare (IMC) unit. Patients requiring admission to the intensive care unit (ICU) were excluded to focus on the \nmajority of standard cases. The data were collected from four primary sources, a patient self-disclosure form and \nthree inpatient documentation forms:\nThe patient self-disclosure form, filled out by the patients during their initial outpatient pancreatic \nconsultation, contained general demographic information, as well as the patient’s medical and family history. \nThe inpatient documentation forms consisted of three components: the admission questionnaire served as a \nstructured summary of the relevant previous medical information by the admitting physician. Intraoperative \ndocumentation should include details of the surgery directly from the operating surgeon. Finally, the course \nof inpatient treatment should be recorded by the attending ward physician at the point of discharge. At the \ntime of the study, structured data collection using IMI-EDC had not been fully implemented at the Pancreas \nCenter, so we decided to collect data retrospectively from completed cases to investigate the secondary benefit \nof this data, which was originally collected for a different purpose, for automated clinical documentation. The \ninpatient documentation was extracted retrospectively from the patients’ unstructured EHR by a medical \nScientific Reports |        (2025) 15:16466 2| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/\ndoctoral candidate with one year of clinical experience (PF). The EHR primarily contained free-text data, such \nas documentation of the clinical course, histological reports, and radiological result letters. Some information, \nsuch as care documentation, was recorded only by hand and was available as a scanned document, which was \nconsidered if it was legible. The inpatient stays occurred between January 2023 and March 2024.\nA detailed review of existing original discharge summaries of these patients revealed that sections such as \n“Histologic Findings” had often been copied verbatim from existing documents as well as lists like “previous \ndiagnoses” . In contrast, sections such as “Medical History and Findings” and “Therapy and Course” were largely \nwritten in running text by physicians, making them the primary focus of this study. For comparison, a physician-\nauthored discharge summary was available for each case.\nWe assessed the extent of information available in the abstracted EHR data compared to the physician-written \ndischarge summaries by analyzing 10 randomly selected summaries. For the “Medical History and Findings” \nand “Therapy and Course” sections, 54% of the content in the physician-written summaries—measured as \nshared text volume (number of characters)—was present in the structured dataset and could be used by the \nLLM with a goal to mirror the structure and key elements of physician-authored summaries. This 54% overlap, \nbased on character count, provides a rough approximation of the extent to which structured data aligned with \nphysician-written summaries. However, this metric does not distinguish between clinically meaningful content \nand redundant or stylistic differences. The remaining 46% may include information that was undocumented, \nonly present in free-text scanned forms, synthesized by the physician, or outside the abstraction scope.\nData scheme\nTo create an understandable data structure for the LLM from the available patient data, specific rules were \ndeveloped in collaboration with an experienced pancreatic surgeon (TMP). Data fields were included only if the \nassociated conditions were met:\n 1. Size and weight The Body Mass Index (BMI) was calculated using the patient’s height and weight. If the BMI \nfell outside the normal range, both height and weight were added to the data set.\n 2. Alcohol consumption This information was included if the patient had a diagnosis of pancreatitis or reported \nfrequent alcohol consumption.\n 3. Tobacco consumption Data on smoking was included if the patient was a current smoker or had a history of \nsmoking.\n 4. Drainages If the intraoperative drainage was removed within three days of the surgery, it was recorded as \n“timely removal” . If the drainage was kept for a longer period, the actual removal date was noted.\n 5. Laboratory values The laboratory results were included only if they fell outside the normal range.\n 6. Bowel movements The number of daily bowel movements was included if it exceeded three or if the patient \nexperienced diarrhea or fatty stools.\nAdditionally, a new data structure was developed, organized into four main sections reflecting the data sources as \ndescribed above: “General Information” , “Before Surgery” , “During Surgery” and “Inpatient Stay” . This structure \norganized the data in broad chronological categories to improve its logical flow and comprehensibility. Table 1 \nshows a corresponding example data set of a patient used to generate the discharge summary. The two columns \nhave the same content, left hand side in English, right hand side in German.\nPrompt engineering\nPrompt Engineering (PE) is an iterative and exploratory process driven by both creativity and intuition 8,9. The \nliterature provides various patterns for structuring prompts, which are employed to enhance the performance of \nthe LLM. Table 2 presents the different prompt patterns we experimented with, along with their corresponding \noutcomes. The column labeled “improvement” indicates whether any observable improvement in model \nperformance was achieved for each pattern.\nTo evaluate improvements, we first selected a complex test case as a benchmark. The test patient was diagnosed \nwith a resectable ductal adenocarcinoma of the pancreas (PDAC) and underwent a total pancreatectomy via \nopen surgery. The patient was treated on the IMC unit and had a prolonged hospital stay of 18 days. The details \nof his case are presented in Table 1.\nNext, NK prompted the LLM using the various engineered prompt patterns and compared the generated \ndischarge summaries to the physician-written summary. Improvements were noted based on the following \ncriteria: how closely the LLM-generated summary aligned with the physician’s version and its readability in \nterms of comprehensiveness, conciseness, fluency and factual correctness. If an improvement was observed, \nthe corresponding row in the “improvement” column was marked “Y es” . For example, the ‘Mind the grammar’ \nprompt was evaluated by comparing the number of grammatical mistakes before and after the change, with a \nreduction in errors indicating improved fluency. The principle ‘Content description of both paragraphs’ was \nassessed by comparing whether the revised summary better aligned with the structure of the original physician-\nwritten summary—specifically, whether key information was included and placed in the appropriate paragraphs, \nindicating improved comprehensiveness and conciseness.\nTwo prompt patterns emerged as particularly useful: The template and the role pattern. The template \npattern ensured a consistent structure of the generated discharge summaries, with each summary containing \nidentically formatted sections “Medical History and Findings” and “Therapy and Course” . The role pattern was \nhelpful in maintaining an appropriate medical tone and factual accuracy, ensuring that the summaries were \nwritten exclusively in German. The final prompt, incorporating the patterns that demonstrated measurable \nimprovement, is provided in Table 3.\nScientific Reports |        (2025) 15:16466 3| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/\nPrompt Chaining In addition to standard PE principles, we explored the technique of prompt chaining, which \nis particularly useful for handling complex, detailed tasks by breaking them down into manageable subtasks. In \nprompt chaining, the output from one prompt is used as input for the next, creating a sequence of interconnected \nprompts. This method enhances the model’s controllability and improves the reliability of its responses13.\nWe applied this technique using the initial prompt developed in the previous step (see Table 3), dividing it \ninto discrete sections that were sequentially adapted and executed. By chaining prompts in this way, the model \nwas able to manage more complex cases with promising results, as demonstrated in the supplementary materials.\nHowever, a key challenge with prompt chaining was extracting the final generated discharge summary. The \nmodel tended to embed the summary within the ongoing conversation, making it difficult to isolate the final \nversion. Sometimes the most up-to-date summary appeared in the last response, while other times it was found \nEnglish German\nGeneral information\nGender: Male\nDate of Birth: 07/30/1985\nKnown Pre-existing Conditions: Arterial Hypertension, Coronary Artery Disease (CAD)\nDiabetes Mellitus: Diagnosed since 09/2043\nExisting Conditions: Colon Polyps (First diagnosed: 09/2045)\nSmoking Status: Smoker for 30 years, 10 cigarettes/day\nAlcohol Consumption: Never\nAllgemeine informationen\nGeschlecht: männlich\nGeburtsdatum: 30.07.1985\nBekannte Vorerkrankungen: Arterielle Hypertonie, Koronare Herzkrankheit (KHK)\nDiabetes mellitus liegt vor seit 09/2043\nVorliegende Veränderungen: Kolonpolypen (Erstdiagnose: 09/2045)\nRaucherstatus: Raucher: Seit 30 Jahren, Zigaretten pro Tag: 10\nAlkoholkonsum: Häufigkeit: Nie\nPre-surgery information\nReason for Visit: Acute Pancreatitis, Pancreatic Cancer, Intraductal Papillary Mucinous \nNeoplasms (IPMN), Pancreatic Cyst\nDate of Initial Diagnosis: 04/03/2045\nCurrent Suspected Diagnosis: Resectable Ductal Adenocarcinoma of the Pancreas \n(PDAC)\nDiagnosis Not Histologically Confirmed\nUnintentional Weight Loss: 30 kg in 24 weeks\nChronic Pain: Present for 12 months, severe fluctuating pain, recurrent daily, treated with \nMorphine\nDigestion: 10 bowel movements/day, diarrhea, vomiting, pancreatic enzyme intake since \n04/2045\nAcute Pancreatitis Present: Since 04/2045\nEndoscopic Retrograde Cholangiopancreatography (ERCP): Date: 02/12/2045, number \nof procedures: 1\nRecent Nausea Present\nLaboratory Values: CA 19–9: 42.0 U/ml, elevated; AP: 119.0 U/L, elevated; GGT: 105.0 \nU/L, elevated\nPrevious Pancreatic Surgery: DHC Stent Placement, Surgery Date: 04/03/2045\nPreoperative Treatment: Bile duct stent; Neoadjuvant therapy: Chemotherapy, 6 cycles, \ncompleted, last chemotherapy date: 09/25/2045\nRadiological Findings of Suspected Diagnosis: PDAC, Resectable Tumor\nTherapy Recommendation: Explorative Laparotomy, Total Pancreatectomy\nInformationen Vor operation\nGrund des Besuchs: Akute Pankreatitis, Pankreaskarzinom, Intraduktale papillär \nmuzinöse Neoplasien (IPMN), Bauchspeicheldrüsenzyste\nDatum der Erstdiagnose: 03.04.2045\nAktuelle Verdachtsdiagnose: Resektables duktales Adenokarzinom des Pankreas (PDAC)\nDiagnose histologisch nicht gesichert\nUngewollter Gewichtsverlust: 30 kg in 24 Wochen\nChronische Schmerzen liegen vor: Beschwerdedauer: Seit 12 Monaten, \nSchmerzcharakter: Stark schwankende Schmerzen, Schmerzverlauf: Rezidivierend, \nSchmerzfrequenz: Täglich, Schmerzmedikation: Morphium\nVerdauung: Anzahl Stuhlgänge pro Tag: 10, Durchfall liegt vor, Erbrechen liegt vor, \nEinnahme von Pankreasenzymen: seit 04/2045\nAkute Pankreatitis liegt vor seit 04/2045\nEndoskopisch retrograde Cholangiopankreatikographie liegt vor: Zeitpunkt: 12.02.2045, \nAnzahl Untersuchungen bisher: 1\nÜbelkeit liegt in letzter Zeit vor\nLaborwerte: Carbohydrat-Antigen 19–9 (CA 19–9): 42.0 U/ml: Wert erhöht; Alkalische \nPhosphatase (AP): 119.0 U/L: Wert erhöht; Gamma-glutamyl Transferase (GGT): 105.0 \nU/L: Wert erhöht\nVorhergehende Pankreas-OP: DHC Stent Einlage, Datum der OP: 03.04.2045\nPräoperative Behandlung: Gallengangsstent, Neoadjuvante Therapie: Chemotherapie, \nAnzahl Zyklen: 6, Vollständigkeit der Chemotherapie: Vollständig, Datum letzte \nChemotherapie: 25.09.2045\nRadiologischer Befund der Verdachtsdiagnose: PDAC / maligne Pankreastumoren, \nResezierbarkeit des Tumors: Resektabel\nTherapieempfehlung: Explorative Laparotomie, Totale Pankreatektomie\nSurgery information\nHospital Admission Date: 10/12/2045\nSurgery Date: 13.10.2045\nConfirmed Diagnosis: PDAC\nType of Surgery: Total Pancreatectomy, Open Surgery\nExtended Surgery: Additional multivisceral resection of the stomach\nIntraoperative Drains: Easy-Flow Drain placed on 10/13/2045, removed on 10/25/2045\nAdditional Vascular Resection: Portal vein interposition graft, Divestment of Superior \nMesenteric Artery, Divestment of Celiac Trunk, Arterial Surgery: Triangle OP\nOther Intraoperative Details: 800 ml blood loss, transfusion given, antibiotics continued \npostoperatively\nInformationen Zur Operation\nDatum der stationären Aufnahme: 12.10.2045\nOP-Datum: 13.10.2045\nDiagnose (gesichert): PDAC\nArt der OP: Totale Pankreatektomie, OP-Zugangsmethode: Offene OP\nOP wurde erweitert: Zusätzliche multiviszerale Resektion des Magens durchgeführt\nIntraoperative Drainagen: Easy-Flow Drainage: Eingelegt am 13.10.2045, entfernt am \n25.10.2045\nZusätzliche Gefäßresektion: Pfortader Interponat: Sonstiges, Divestment Arteria \nMesenterica Superior, Divestment Truncus Coeliacus, Arterielle OP: Traingle OP\nSonstige Informationen aus der OP: Transfusion gegeben, Blutverlust von 800 ml, \nAntibiose weiter postoperativ verabreicht\nPost-surgery course\nPost-Surgery Transfer: Recovery room on 10/13/2045, then transfer to IMC\nInpatient Antibiotic Therapy: Ceftriaxone/Metronidazole i.v. from 10/12/2045 to \n10/16/2045 (perioperative antibiotic); Piperacillin/Tazobactam i.v. from 10/24/2045 to \n10/29/2045 (suspected central venous catheter infection)\nAnalgesics Administered: NSAIDs, oral opioids, intravenous opioids, patient-controlled \nanalgesia, regional anesthesia (including epidural)\nImaging: CT scan due to infection marker increase, treated with antibiotics\nAdditional Complications: Isolated CRP increase\nWound Status: No complications\nPatient Mobilization: As before surgery\nDiet Progression: Timely\nSufficient Bowel Movement: Since 10/17/2045; vomiting and nausea persisted from day 1 \nto day 3 postoperatively\nOther Observations: In addition to a regular diet, parenteral nutrition with Olimel \n1500 ml/day was continued to ensure adequate caloric intake and prevent postoperative \ncatabolic metabolism\nDischarge Date: 10/30/2045\nStationärer verlauf\nVerlegung nach OP in Aufwachraum am 13.10.2045, Verlegung auf IMC\nStationäre antibiotische Therapie: Gabe von Ceftriaxon/Metronidazol i.v. von 12.10.2045 \nbis 16.10.2045, Indikation: Perioperative Antibiose; Gabe von Piperacillin/Tazobactam \ni.v. von 24.10.2045 bis 29.10.2045, Indikation: Verdacht ZVK Infektion\nFolgende Analgetika wurden verabreicht: Nicht steroidale Antirheumatika (NSAID), \nOpioide p.o., Opioide i.v., patientenkontrollierte Analgesie, Regionalanästhesie (inkl. \nPeriduralanästhesie (PDA))\nBildgebung: CT Schnittbild:\n-Indikation: (Erneuter) Infektwertanstieg, Behandlung durch Gabe von Antibiotika\nWeitere Komplikationen: Isolierter CRP Anstieg\nEs zeigten sich reizlose Wundverhältnisse\nMobilisation des Patienten war wie vor Operation möglich\nDer Kostaufbau des Patienten erfolgte zeitgerecht\nSuffiziente Darmpassage seit 17.10.2045; Erbrechen und Übelkeit ab Tag 1 postoperativ \nund über den dritten Tag postoperativ hinaus\nSonstige Auffälligkeiten: führten zusätzlich zur Vollkost eine parenterale \nErnährungstherapie mit Olimel 1500 ml/d weiter, um eine adäquate Kalorienaufnahme \nzu gewährleisten und eine post-operative katabole Stoffwechsellage zu vermeiden\nEntlassungsdatum: 30.10.2045\nTable 1. Example data of a patient (Left Column: English, Right Column: German; Dates were shifted forward \nby a random, fixed interval).\n \nScientific Reports |        (2025) 15:16466 4| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/\nin earlier responses. Table 4 highlights the sections that needed to be extracted, shown in italics. To address this, \nwe developed a pattern-matching approach that reliably extracts the discharge summary in most cases. Our \nmethod identifies summaries based on structural features, such as the presence of specific headings (‘History \nand Findings’ and ‘Therapy and Course’), the organization of text into paragraphs, and the absence of paragraph \nbreaks in the main section. While not perfect, this approach allows for accurate extraction in the majority of \ncases.\nIn-Context Learning (ICL)  Another promising approach explored in this study was ICL, a lightweight \nmethod that integrates example-based learning directly into the model’s prompt during inference. By including \nin-context examples, the model learned the structure, inputs, and labels of the dataset and could apply this \nunderstanding to new test cases14.\nBuilding on the prompt from Table 3, we experimented with ICL as an enhancement option. To generate \ndischarge summaries, the prompt was extended using one- and two-shot examples, randomly selected from \nthe dataset. Both complex and simple examples were tested. In a further variation, only the relevant sections of \nthe discharge summaries (“Medical History and Findings, ” “Therapy and Course”) were provided, omitting the \ncorresponding structured dataset.\nGenerated text\nI have reviewed and adjusted the two sections again to ensure that no content is duplicated. Here are the two sections:\n**History and findings**\nThe 65-year-old male patient was admitted to our clinic on 30.06.2033 with acute pancreatitis. The medical history revealed previous treatment for colon cancer, which was operated on \n03/2033, as well as a colon polyp initial diagnosis on 02/2033. The patient also suffered from arterial hypertension and diabetes mellitus since 02/2032, for which he has been insulin-\ndependent since 03/2032\n**Therapy and course**\nThe patient underwent an exploratory laparotomy, left pancreatic resection with splenectomy and cholecystectomy on 04/07/2033. The operation was without complications. Following the \noperation, the patient received pain therapy with opioids p.o. and i.v. as well as regional anesthesia (including epidural anesthesia (PDA)). The wound healed without irritation and the \npatient could be mobilized as before the operation. The patient’s diet was built up in good time and adequate bowel transit was achieved from 08.07.2033\nI hope that these are the corrected sections!\nTable 4. Example of a generated discharge summary using prompt chaining.\n \nRole Prompt\nSystem Y ou are a ward physician at the university hospital and write discharge letters for the patient’s general practitioner. Y ou use correct medical \nterminology. Y ou only speak German\nUser\nWrite the ‘Medical history and findings’ and ‘Therapy and course’ sections of the discharge letter for a patient\nThe ‘Medical history and findings’ section should contain the patient’s medical history and explain why the patient had the operation\nThe ‘Therapy and course’ section describes the course of the operation and information about the patient’s inpatient treatment that is \nrelevant for the general practitioner after discharge\nAll other sections (‘Diagnosis’ , ‘Therapy’ , ‘Histology’ , ‘Procedure’ and ‘Medication’) are created automatically and should not be written by \nyou. Information from the patient’s EHR is available to you for writing the ‘Medical history and findings’ and ‘Therapy and course’ sections\nHere is the data from the patient file:\n{data}a\nMedical history and findings:\nTherapy and course:\nTable 3. Final prompt containing all useful prompt engineering principles. aThe patient’s data are \nautomatically inserted in place of the placeholder. The prompt was used in German and was only translated in \nthis paper for reasons of comprehensibility. The original prompt in German is provided in the supplement.\n \nPE \nprinciple Description Application in prompt Improvement? Reference\nRole\nPhysician writing for another physician; \nuse precise medical terminology\n“Y ou’re a ward doctor at the university hospital and write discharge summaries for the \npatient’s general practitioner. ” Ye s\n10\nWrite in German “Y ou only speak German. ” Ye s\nMind the grammar “Y ou pay attention to the correct use of German grammar. ” No\nGoal Detailed description of the goal “Write the sections ‘Medical history and findings’ and ‘Therapy and course’ of a discharge \nsummary for a patient. ” Ye s 10\nConstraints Output length “The discharge summary should be approximately 2000 characters long. ” No 11\nContext \nManager\nContent description of both paragraphs “The “Medical History and Findings” section should include the patient’s medical history \nand explain why the patient received the surgery. ” Ye s\n12\nDecide which information is relevant “Decide for yourself which information from the patient file is relevant and only use this \nin the discharge summary. ” No\nUse placeholder for missing information “If you are missing information, you can use MISSING as a placeholder” No\nTemplate\nStructuring the output into the sections \n“ Anamnese und Befund” and “Therapie \nund Verlauf ”\nAt the end of the summary: “Medical history and findings: Therapy and course:” Ye s 12\nTable 2. Applied prompt engineering (PE) principles.\n \nScientific Reports |        (2025) 15:16466 5| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/\nAcross all scenarios, it became clear that the model heavily relied on the provided examples, often reproducing \nsentence structures from them verbatim—no matter if they were contextually appropriate or not. This resulted \nin discharge summaries containing significantly more errors compared to outputs generated without ICL. \nAdditionally, due to hardware limitations, a maximum of two examples could be included per prompt.\nDischarge summary generation\nAll 25 discharge summaries were generated successfully, each containing the required running text sections \n“Medical History and Findings” and “Therapy and Course” . The summaries were written entirely in German \nand, aside from a few minor details, were readily comprehensible. The generated summaries are displayed in the \nsupplement.\nThe average time to generate each discharge summary was 112.89 ± 8.19 s, excluding the preprocessing of the \nstructured dataset. The “Medical History and Findings” section had an average length of 698 ± 100 characters, \nwhile the “Therapy and Course” section was typically longer, averaging 886 ± 158 characters. In comparison, \nphysician-written discharge summaries had average lengths of 652 ± 374 characters for the “Medical History and \nFindings” section and 2047 ± 1248 characters for the “Therapy and Course” section.\nError analysis\nFrequent mistakes The generated discharge summaries were analyzed for errors, with an average of 2.84 ± 1.71 \nmistakes found per summary. Below are some frequent types of errors we observed and potential solutions.\n 1. Incorrect Age Calculation in two-thirds of the summaries, the patient’s age was inferred correctly from the \ndate of birth and discharge date. However, in one-third, the age was incorrect. We could resolve this by \ncalculating the age during preprocessing and including it in the structured dataset. We could observe that \nthe incorrectly inferred age calculations not only considered the wrong month and day and were therefore \none year off, but several years off. Calculating the age during preprocessing and explicitly including it in the \nstructured dataset could prevent these errors. When we explicitly provided the current date (corresponding \nto the discharge date) in the prompt, the model was able to determine the age difference in years but could \nnot account for the exact month and day, leading to deviations of up to one year.\n 2. Date Confusion the model often used the date of first diagnosis for the date of first clinical presentation, \nwhich led to incorrect dates in half of the cases. However, these two dates were only identical in two cases. \nSince the first date of clinical presentation was not available in the structured data we collected, it should be \nadded to our future experiments to address this issue. Notably, when we explicitly provided the correct date \nof first clinical presentation, the model used it correctly, suggesting that the issue arises from missing data.\n 3. Pathological Bowel Movements in 10 cases, when bowel movements per day were provided in the structured \ndataset, the summaries incorrectly described them as pathological, even when normal (Summary 16). Ac -\ncording to the rules in section “Data Scheme” , one to three bowel movements per day were considered phys-\niological. The number of bowel movements mentioned did not affect this misclassification, suggesting that \nthe model did not know this rule.\n 4. Imprecise or Incomplete Information some summaries contained vague or misleading details. For example, \none summary inaccurately described the procedure as being stopped and replaced by another, when in fact \nit was converted to a different type of surgery (Summary 3). In another case, while adjustments were made \nto the patient’s anti-hypertensive medication, the summary failed to mention that the medication was later \ndiscontinued due to intolerance (Summary 16). Additionally, some summaries incorrectly presented cause-\nand-effect relationships, such as suggesting that the removal of a drainage was part of the treatment for a \nlymphatic fistula, when it was just a routine step in the patient’s overall recovery process (Summary 6). In \ntotal we could observe 4 cases in 3 summaries.\n 5. Literal Use of Information from the Structured Dataset  the model often reproduced text directly from the \nEHR without adjusting for context, such as when referring to the patient’s gender. In multiple instances, it \nincorrectly used a male pronoun and descriptor for a female patient (Summary 2). To address this, a possible \nsolution would be to modify the language during preprocessing to ensure the text reflects the correct gender.\n 6. Grammatical Errors various grammatical mistakes were identified throughout the summaries. Some exam -\nples involved incorrect grammatical cases, such as incorrect article usage (Summary 3). Other issues includ-\ned improper verb conjugation (Summary 13), as well as the use of uncommon or awkward phrasing that \nappears to be literal translations from English, which are not typical in German medical language (Summary \n11).\n 7. Spelling Errors some summaries contained spelling mistakes, such as misspellings of medical terms (Summa-\nry 2), even though the correct versions were available in the structured dataset. These errors are likely caused \nby inconsistencies in the model’s training data.\n 8. Hallucinations the model occasionally generated false information. For instance, a summary mentioned de-\nlayed patient mobilization, while the structured dataset indicated timely mobilization (Summary 3). In an -\nother case, nausea was incorrectly described as decreasing from day 3, despite the structured dataset showing \nit persisted from day 1 onward (Summary 3). In total we could observe 5 cases in 4 summaries.\nMissing Information We analyzed details that had been present in the structured dataset but were either partially \nor entirely omitted from the generated discharge summaries. Our analysis aimed to highlight the most commonly \nmissed information and assess the completeness of the summaries.\nIn the “General Information” section, key details like the patient’s height and weight were consistently absent \nfrom the generated discharge summaries, despite being available in the structured dataset. Previous illnesses \nwere included in fewer than half of the cases. Family history was fully or partially included in only 4 cases and \nScientific Reports |        (2025) 15:16466 6| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/\nomitted in 10. Similarly, information about the patient’s smoking and alcohol consumption habits was missing \nin approximately three-quarters of the summaries, even when available in the structured dataset.\nIn the “Before Surgery” section, histological findings were included in only one-third of the summaries, \nreflecting a significant gap in capturing important preoperative details.\nFor the “During Surgery” section, intraoperative assessments, such as the condition of the pancreatic \nparenchyma, were included in only 25% of cases. However, other intraoperative details, such as blood loss and \nwhether a transfusion was required, were at least partially included in two-thirds of the summaries.\nIn the “Inpatient Stay” section, the transfer of patients from the operating room to recovery, and subsequently \nto the normal ward or IMC, was included in about two-thirds of the summaries.\nOverall, no consistent pattern could be recognized as to which information was included, and which was \nomitted by the LLM. Determining the clinical relevance of each missing piece of information, and whether it \nshould be consistently included in discharge summaries, requires further evaluation by experienced clinicians.\nQuantitative evaluation\nTo align the quality of the LLM-generated discharge summaries with physician-written summaries, we employed \ntwo widely recognized metrics: the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) score and \nBERTScore.\nThe ROUGE score served as standard metric to evaluate summarization tasks and to measure syntactic \nsimilarity by comparing n-grams, word sequences, and sentence structures between the generated text and a \nhuman-written reference summary 15. Specifically, we used ROUGE-1 (unigrams), ROUGE-2 (bigrams), and \nROUGE-L (longest common subsequences). We also utilized BERTScore, a semantic metric to evaluate the \nsimilarity of meanings between the words in the generated and original reference texts.\nTable 5 presents the average scores and standard deviations for all calculated metrics:\nROUGE-1 and ROUGE-L indicated that around 25% of the words or word sequences in the summaries \ngenerated matched the reference summaries. ROUGE-2 (0.06 ± 0.03) reflected lower bigram overlap, which \ncould suggest variability in sentence structure and the use of different combinations of words.\nThe BERTScore of 0.64 showed moderate semantic similarity.\nQualitative evaluation\nA qualitative evaluation of clinicians and medical students provided initial feedback from medical professionals. \nFive individuals participated in the evaluation, using a questionnaire adapted from Aali et al. 23 and translated \ninto German. Each participant rated five summaries on four criteria—comprehensiveness, conciseness, fluency, \nand factual correctness—using a 5-point Likert scale from 1 (very poor) to 5 (very good). For each case, the \nevaluators were provided with the LLM-generated summary and the corresponding structured input data used \nto generate it. Figure 1 displays the ratings on each criterion across the five included summaries. Summaries 1, \n2, and 4 performed slightly better compared to summaries 3 and 5. Importantly, none of the summaries received \na score of 1 (very poor) on any criterion. Notably, summary 3 scored approximately one point lower than the \nothers in the comprehensiveness criterion, indicating that the participants found it to be less detailed or missing \nkey information.\nTable 6 displays the mean values and standard deviation for each criterion across all summaries. Overall, \nthe average scores across all criteria ranged from 3.72 to 3.96, with factual correctness and fluency scoring the \nhighest. However, the standard deviation extended below 3 for some criteria. The percentage of ratings classified \nas “good” (4 or 5) was 60% for comprehensiveness, 72% for conciseness, 80% for factual correctness, and 68% \nfor fluency.\nThe findings indicate that while AI-generated summaries have potential, certain limitations remain. \nSpecifically, the model occasionally oversimplifies complex clinical cases and also exhibits significant issues with \ncompleteness—likely due to missing or insufficient input data, and potentially related to the same mechanisms \nthat drive oversimplification.\nDiscussion\nThis study introduces several new findings in the application of LLMs for generating German-language discharge \nsummaries from structured datasets. It is among the first to assess how well LLMs can capture clinically relevant \ninformation in German, and shows that summaries generated by the LLaMA3 model can be a first step toward \naligning with physician-written summaries in both structure and content. This might eventually lead to added \nvalue for clinicians by collecting structured data through secondary use for automated physician letter writing. \nThe data were collected using the modern data capture platform IMI-EDC, also with a scientific focus.\nOur results suggest that structuring the input data into broad chronological categories and applying specific \ninclusion rules may help organize the summary content. However, handling complex cases, such as intermediate \nType of score Score+\nROUGE-1 0.25 ± 0.04\nROUGE-2 0.06 ± 0.03\nROUGE-L 0.24 ± 0.04\nBERTScore* 0.64 ± 0.01\nTable 5. Calculated metrics. +Mean ± Standard deviation. *Computed with “facebook/bart-large-mnli” .\n \nScientific Reports |        (2025) 15:16466 7| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/\ncare (IMC) patients, remains challenging for the model. Additionally, PE and prompt chaining techniques seem \nto help enhancing summary quality, albeit with considerable processing time.\nCompared to the literature, our study underscores similar challenges in data completeness and consistency \nseen in LLM-generated summaries. About 46% of the content typically found in physician-written summaries, \nmeasured as shared text-volume, was missing from the structured dataset. This echoes findings from earlier \nresearch, which suggest that some clinically relevant information resides only in physicians’ notes or unstructured \ndata sources outside the EHR16. This finding also aligns with our qualitative evaluation, which showed that only \n60% of summaries were rated “good” in terms of comprehensiveness.\nOur study supports previous research findings on the limitations and potentials of PE and in-context learning \n(ICL) in improving LLM performance in complex data-to-text tasks. For instance, while ICL has demonstrated \npotential in improving model performance on specific tasks, our findings echo Reynolds and McDonell’s 17 \nobservation that examples do not always improve model outputs, particularly for complex and nuanced tasks \nlike clinical documentation. Overall, iterative prompt engineering and prompt chaining were judged as more \nfruitful than ICL for generating higher quality summaries.\nThe model’s settings were tuned to optimize performance. As recommended in existing literature18–20, a low \ntemperature setting was used to minimize hallucinations and ensure a more objective output. While this resulted \nin fewer factual inaccuracies, slightly increasing the temperature improved fluency and readability, suggesting \nthe need for fine-tuning based on specific use cases. Careful testing of these parameters remains essential.\nSome errors made by the model—such as the literal use of information from the structured dataset, \ngrammatical errors, and spelling errors—only marginally affect the comprehensibility of the generated discharge \nsummaries. These issues primarily stem from the model’s limited proficiency in medical terminology and \nGerman.\nA more significant challenge is the incorrect age calculation observed in one-third of the cases. As noted by \nTan et al.21, many LLMs struggle with temporal reasoning. In our analysis, we found that while LLaMA3 can \naccurately compute the difference between two given dates as a standalone task, it fails to apply this ability when \ngenerating full discharge summaries that require multiple reasoning tasks. This limitation arises because LLMs \nCriteria Score+\nComprehensiveness 3.72 ± 0.89\nConciseness 3.96 ± 0.84\nFactual correctness 3.88 ± 0.97\nFluency 3.88 ± 0.83\nTable 6. Qualitative evaluation scores. +Mean ± Standard deviation.\n \nFig. 1. Qualitative evaluation per category; The width of each violin at a given rating value reflects the \ndensity of responses. Although most ratings clustered at 4 and 5, a minority of ratings at 2–3 created a visible \ndistribution spread. This helps visualize variability beyond just reporting averages.\n \nScientific Reports |        (2025) 15:16466 8| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/\ndo not inherently possess a structured understanding of time; instead, they rely on pattern recognition from \ntheir training data rather than explicit date arithmetic. Tan et al.21 highlight that LLMs often default to heuristics, \nsuch as assuming that later-mentioned dates are more relevant or interpreting temporal relationships based on \nco-occurrence statistics rather than logical computation. These biases may explain why the model occasionally \nmiscalculates the patient’s age.\nThe LLaMA3 model24,25 used in this study showed strong performance in handling minor inconsistencies in \nthe structured dataset. For instance, despite the presence of conflicting information—such as antibiotics being \ndocumented as administered only intraoperatively in one section of the EHR and recorded again in the inpatient \ncourse—the model correctly synthesized this and included antibiotic administration in the generated discharge \nsummary (Summary 3).\nHallucinations, vague or misleading statements, or otherwise imprecise information were identified in 9 out \nof 25 summaries. This refers to individual content issues, not the overall completeness of each summary. Such \nerrors are particularly difficult to detect, making mitigation strategies essential. One potential approach is the \nintegration of Retrieval-Augmented Generation (RAG) 22, which could help reduce inaccuracies caused by a \nlack of medical knowledge—such as the misclassification of bowel movement frequency—by incorporating up-\nto-date information from literature and clinical guidelines. Some errors may arise due to issues with reasoning \nor context integration. In these cases, implementing Human-in-the-Loop (HITL) validation within clinical \nworkflows could provide real-time oversight. A potential HITL workflow could involve generating a draft  \ndischarge summary that is then reviewed and corrected by a physician before being finalized and handed to the \npatient. The data gathered from these HITL interactions could also be leveraged for reinforcement learning with \nhuman feedback (RLHF) to further refine model outputs in future research23.\nFor quantitative evaluation we applied ROUGE and BERTScore as commonly used in related research and \nwhile they provided valuable insights into syntactic and semantic similarity, they were limited in capturing clinical \nrelevance and domain-specific language quality. As highlighted by Jung et al.24, these metrics do not account for \ncritical aspects of medical documentation, such as the correct use of medical terminology, underscoring the \nneed for clinically oriented evaluation metrics.\nWe achieved low to moderate ROUGE and BERTScore scores compared to the literature. However, it is \ndifficult to compare the absolute metrics with other studies because we compared the generated summaries \nwith those written by physicians. Most other studies use these metrics as relative values to compare summaries \ngenerated by different language models on the same base text within the self-contained systems of their \nstudies9,23,27. In particular, ROUGE-L could suffer from our approach of generating texts from structured \ndata, which are typically short concepts that do not lead to long overlaps with texts written by physicians. Our \nqualitative survey findings are in line with those of Aali et al.24, who conducted a similar survey. Although they \ndo not report numbers, according to the violin plots of their publication, GPT-4 achieved the highest ratings \n(average 4–5, small standard deviation), while LLaMA2-13B and physician-written summaries were rated lower, \nbut at a comparable level (comprehensiveness/factual accuracy: average 2–3, very large standard deviation; \nconciseness/fluency: 3–4, large standard deviation). Considering Table 6 the LLaMA3 model used in our study \nappears to fall between GPT-4 and LLaMA2 in terms of performance across all criteria.\nHowever, several limitations must be considered. First, the relatively small dataset of 25 cases restricted both \nthe diversity of clinical contexts and the generalizability of our findings. This sample size limits our ability to \nfine-tune models with techniques like Quantized Low-Rank Adaptation (QLoRA)25, which would likely improve \nLLM performance with larger datasets. Additionally, inconsistencies, missing structured data, and errors in the \nraw EHR data highlight the challenges of retrospective data collection. However, the completeness of LLM-\ngenerated discharge summaries depends upon the completeness and/or curation of the data fed to the LLM. The \nmodel also showed limitations in handling missing information, especially when critical details—such as hand-\nwritten notes or non-EHR information—were absent from the structured input data. This missing data reduced \nthe accuracy of the summaries generated, underscoring the need for more comprehensive data integration in \nfuture studies. Further, the ability to incorporate a larger number of ICL examples was constrained by GPU \nlimitations, potentially impacting the model’s performance. Also, model-generated summaries exhibited issues \nwith German grammar, spelling, and medical terminology, underscoring the need for model fine-tuning on \nmedical data or integrating RAG28, which could improve the model’s medical competency and reduce the need \nfor manual corrections 32. Additionally, alternative models, such as GPT-4, which performed well in similar \nstudies24,33, may have produced better results in our context. The rapid evolution of LLMs also suggests that new \nmodels in the near future may significantly outperform those used in the current study, potentially addressing \nsome of the limitations observed here.\nFuture research should consider larger datasets and improved data collection methods to enhance the \ndiversity and completeness of training data, which could improve model accuracy and reduce the need for \nphysician oversight. Expanding data sources to include unstructured clinical texts and nursing notes could \nalso enrich input quality, addressing limitations due to incomplete raw EHR data. Testing alternative model \nfine-tuning approaches, such as QLoRA, or incorporating RAG could enhance domain-specific accuracy and \nmedical terminology use. Using ICL with an increased and carefully selected number of examples can have a \nsignificant impact and should therefore be tested with more eligible hardware 26,27. More extensive qualitative \nevaluations are also needed, involving a broader group of healthcare professionals to evaluate the practical \nrelevance, acceptance, and error identification in automatically generated summaries. Future research should \nalso address questions of time efficiency, acceptance among clinicians, and the ability to categorize errors in \ngenerated content20. With improved model accuracy and clinically oriented evaluation metrics, LLMs could \noffer a viable tool for discharge documentation in real-world settings.\nScientific Reports |        (2025) 15:16466 9| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/\nConclusion\nOur study describes the application of an open-source LLM to generate German-language discharge summaries \nfrom structured clinical data in a real-world setting. The approach demonstrated the ability to produce coherent \nand moderately accurate drafts. However, our data also underscore significant limitations—particularly in \ntemporal reasoning, error-prone verbatim copying in ICL, and inconsistent inclusion of clinically relevant \ncontent. These findings highlight that while LLM-based tools may one day be useful for supporting the clinical \ndocumentation process, achieving high-quality outputs remains dependent on multiple factors, including data \ncompleteness, task framing, and post-processing pipelines. Our methods, findings, and descriptive analyses \ncan inform future research and implementation efforts, particularly in the context of non-English medical \ndocumentation.\nMethods\nLLM selection\nTo generate discharge summaries from raw EHR data, we first identified a suitable LLM based on specific \nrequirements: the model needed to be open source for transparency and local deployment, compatible with \navailable GPU constraints, able to generate text from structured data, support German language comprehension, \nhandle medical terminology, and maintain a context window of at least 4000 tokens.\nWe reviewed domain-specific literature, consulted experts, and evaluated models using leaderboards \nlike Open Medical LLM Leaderboard. SauerkrautLM 28, OpenBioLLM 29, and LLaMA3 30 emerged as the top \ncandidates. OpenBioLLM scored highly in medical comprehension, particularly on medical multiple-choice \nbenchmarks, suggesting suitability for clinical language tasks. SauerkrautLM, fine-tuned on German texts, \ndemonstrated strong language proficiency. LLaMA3, with a robust multilingual foundation, showed versatility \nin generating high-quality German medical text without domain-specific fine-tuning. All three models support \nan 8192-token context window.\nWhile GPT-4 is widely used in comparable research 18–20, it was excluded from consideration due to the \nrequirement for an open-source model that could be deployed locally, ensuring compliance with strict data \nprotection protocols.\nDue to its high performance on our qualitative criteria (comprehensiveness, conciseness, factual correctness, \nand fluency), LLaMA3 was selected for this project.\nData preprocessing and prompt engineering\nFor this study, a total of 25 datasets from completed cases from the European Pancreas Center of Heidelberg \nUniversity Hospital were retrospectively collected by a medical doctoral candidate (PF , one year clinical practice) \nand recorded in two distinct forms: a self-disclosure form and an inpatient documentation form. The data was \ncollected with the electronic data capture tool “IMI-EDC” and exported as CSV file. The use of the hospital data \nwas conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of the \nmedical faculty at the University of Heidelberg (S301/2001; S708/2019; S083/2021).\nTo protect patient privacy, to the extent feasible and as described below, all protected health information \n(PHI) was removed. While the German Data Protection Regulation does not specifically define how to de-\nidentify clinical documents, we adhered to best practices established by the Health Insurance Portability and \nAccountability Act (HIPAA)31, similar to other German and European studies 32–34. This process involved an \ninitial automated PHI removal using a pattern-matching approach to identify and redact personal identifiers \nsuch as names, titles, organizations, histological findings, locations, and phone numbers. In a second step, any \nremaining PHI was manually reviewed and removed. Additionally, dates were shifted forward by a random, fixed \ninterval—adjusting the day, month, and year—to further anonymize the data, in line with HIPAA standards31.\nThe design of the prompts for the LLM followed an iterative process to determine the most effective approach. \nVarious principles identified in the literature were tested on a sample case. The generated discharge summaries \nwere evaluated based on qualitative criteria, including comprehensiveness, conciseness, factual accuracy, and \nfluency. Principles that led to improvements in the output were retained, while those that did not were discarded.\nAdditionally, we tested two advanced techniques, Prompt Chaining and ICL, which may further enhance \nprompt effectiveness. Prompt Chaining is a method used to handle complex tasks by breaking them into smaller, \nmore manageable subtasks. The model’s output from one prompt serves as the input for the next, creating a chain \nof prompts. This approach can increase both the controllability and reliability of the model’s responses. For this \nstudy, the initially developed prompt (see Table 3) was divided into five stages: first, the model was asked to \norganize the provided structured dataset; next, it generated the “Medical History and Findings” section; followed \nby the “Therapy and Course” section; fourth, it combined and refined both sections and finally, the content was \nreviewed by comparing it against the original data. The complete chat, including all prompts, is provided in Table \n2 of the supplementary material. For ICL, we provided the model with one- or two-shot examples, which were \nincluded in the prompt to serve as references for the task at hand. These examples were intended to guide the \nmodel and help it adapt its responses by learning from the patterns and structures presented in the examples.\nThe structured data provided was transformed into a rule-based natural language description of each feature. \nThrough an iterative process, an optimal structure and sequence were developed to guide the LLM. This involved \nadding explanatory information for specific data points and formulating conditional rules under which certain \nattributes were made available to the model.\nScientific Reports |        (2025) 15:16466 10| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/\nModel implementation and setup\nThe preprocessed data, along with the prompt developed during the PE process, was used with the LLaMA3 \nchat template to generate the discharge summaries. The generation process followed these steps for each patient \nrecord:\n 1. Prompt Generation:\n• Abstracted EHR Data Preprocessing: The structured data was extracted from the completed forms and \nformatted according to the structure presented in Table 1.\n• Prompt Formatting: The extracted data was inserted into the predefined prompt structure (as shown in \nTable 3), replacing the {data} placeholder.\n 2. Summary Generation:\n• The formatted prompt was processed by LLaMA3 using the specified chat template to generate the dis -\ncharge summary.\n 3. Summary Evaluation:\n• The generated summary was evaluated as described in the ‘Model Evaluation’ section.\nFor text generation, the following libraries and versions were employed: Transformers (Version 4.39.3)\nPyTorch (Version 2.2.2)\nBitsAndBytesConfig (Version 0.43.1) for 4-bit quantization and bfloat16 inference.\nThe Python (v3.11.5) script ran on an NVIDIA RTX A6000 GPU (48  GB), with hyperparameters set to a \ntemperature of 0.2 for deterministic outputs and a 1000-token limit for new content generation.\nModel evaluation\nError Analysis Where possible, underlying causes or explanations for errors were explored, along with potential \nstrategies for improvement. Additionally, the average number of errors per summary was calculated to quantify \nthe model’s performance.\nThe error analysis was conducted by one of the authors (NK), with a second author (TMP) consulted in cases \nof uncertainty. The analysis of frequent mistakes involved a systematic comparison between structured data \nfrom the EHR and the generated summaries. An error was defined as any piece of information in the summary \ngenerated that either did not match the corresponding structured dataset or could not be inferred from it. All \nidentified errors were labeled within the respective summaries, recurring errors were categorized.\nFurthermore, instances where information present in the structured dataset was absent from the generated \nsummaries were identified and marked in the structured datset. This “missing information” was defined as any \ninformation available in the structured EHR dataset but not included in the generated summary.\nQuantitative Evaluation Summary quality was evaluated using BERTScore and ROUGE by comparing the \ngenerated summaries to the physician-written summaries. BERTScore assessed semantic similarity using the \nBERTScore library (v0.3.13) and contextualized embeddings from “facebook/bart-large-mnli” 35. ROUGE \nmetrics (ROUGE-1, ROUGE-2, and ROUGE-L) measured n-gram overlaps and sequence commonalities. \nConsistent with prior literature, both ROUGE (syntactic similarity) and BERTscore (semantic similarity) were \nused9,10,23,27.\nQualitative Evaluation Physicians and medical students conducted an anonymous survey evaluating five \nrandomly selected summaries, along with their corresponding structured dataset. The survey consisted of four \nquestions, adapted from Aali et al.18 and translated into German. Respondents rated each summary on a 5-point \nLikert scale, ranging from 1 (very poor) to 5 (very good), based on the following criteria:\n 1. Comprehensiveness: how well does the summary capture important information? This assesses the recall of \nclinically significant details from the input text.\n 2. Conciseness: how well does the summary exclude non-important information? This compares how well the \nsummary is condensed, considering the value of a summary decreases with superfluous information.\n 3. Factual Correctness: how well does the summary agree with the facts outlined in the clinical note? This eval-\nuates the precision of the information provided.\n 4. Fluency: how well does the summary exhibit fluency? This assesses the readability and natural flow of the \ncontent.\nThe goal of the evaluation was to gather preliminary insights into how well the LLaMA3-generated summaries \nalign with clinical expectations.\nData availability\nThe datasets used and/or analysed during the current study are available from the corresponding author on \nreasonable request.\nCode availability\nThe underlying code for this study is available on GitHub and can be accessed via this link  h t t p s : / / g i t h u b . c o m / I \nM I - H D / l l m - d i s c h a r g e - s u m m a r i e s     .  \nScientific Reports |        (2025) 15:16466 11| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/\nReceived: 8 January 2025; Accepted: 7 May 2025\nReferences\n 1. Lenert, L. A., Sakaguchi, F . H. & Weir, C. R. Rethinking the discharge summary: a focus on handoff communication. Acad. Med. J. \nAssoc. Am. Med. Coll. 89, 393–398. https://doi.org/10.1097/ACM.0000000000000145 (2014).\n 2. Salim Al-Damluji, M. et al. Association of discharge summary quality with readmission risk for patients hospitalized with heart \nfailure exacerbation. Circ. Cardiovasc. Qual. Outcomes 8, 109–111. https://doi.org/10.1161/CIRCOUTCOMES.114.001476 (2015).\n 3. Martin, D. B. et al. Preferences in oncology history documentation styles among clinical practitioners. JCO Oncol. Pract. 18, e1–e8. \nhttps://doi.org/10.1200/OP .20.00756 (2022).\n 4. Liu, J., Nicolson, A., Dowling, J., Koopman, B. & Nguyen, A. e-Health CSIRO at “Discharge Me!” 2024: Generating discharge \nsummary sections with fine-tuned language models. (2024).\n 5. Hüske-Kraus, D. Suregen-2: A shell system for the generation of clinical documents. In EACL ‘03: Proceedings of the tenth conference \non European chapter of the Association for Computational Linguistics - Volume 2, 215–218. https://doi.org/10.3115/1067737.1067788 \n(2003).\n 6. Starlinger, J., Kittner, M., Blankenstein, O. & Leser, U. How to improve information extraction from German medical records. it \nInf. Technol. 59, 171–179. https://doi.org/10.1515/itit-2016-0027 (2017).\n 7. Heilmeyer, F . et al. Viability of open large language models for clinical documentation in German Health Care: Real-world model \nevaluation study. JMIR Med. Inform. 12, e59617. https://doi.org/10.2196/59617 (2024).\n 8. Amatriain, X. Prompt design and engineering: Introduction and advanced methods. Available at http://arxiv.org/pdf/2401.14423v3 \n(2024).\n 9. Ggaliwango, M., Nakayiza, H., Daudi, J. & Nakatumba-Nabende, J. Prompt engineering in large language models. In Data \nIntelligence and Cognitive Informatics. Proceedings of ICDICI 2023 1st edn (eds Jacob, I. J. et al.) 387–402 (Springer, 2024).\n 10. Meskó, B. Prompt engineering as an important emerging skill for medical professionals: tutorial. J. Med. Internet Res. 25, e50638. \nhttps://doi.org/10.2196/50638 (2023).\n 11. Ekin, S. Prompt engineering for ChatGPT: A quick guide to techniques, tips, and best practices. TechRxiv.  h t t p s : / / d o i . o r g / 1 0 . 3 6 2 2 \n7 / t e c h r x i v . 2 2 6 8 3 9 1 9 . v 2     (2023).\n 12. White, J. et al. A prompt pattern catalog to enhance prompt engineering with ChatGPT. Available at  h t t p : / / a r x i v . o r g / p d f / 2 3 0 2 . 1 1 3 \n8 2 v 1     (2023).\n 13. Prompt Engineering Guide. Techniques: Prompt chaining. Available at  h t t p s :  / / w w w .  p r o m p t  i n g g u i  d e . a i  / t e c h n  i q u e s /  p r o m p t  _ c h a i n \ni n g (2024).\n 14. Min, S. et al. Rethinking the role of demonstrations: What makes in-context learning work? Available at  h t t p : / / a r x i v . o r g / p d f / 2 2 0 2 \n. 1 2 8 3 7 v 2     (2022).\n 15. Cai, P . et al. Generation of patient after-visit summaries to support physicians. In Proceedings of the 29th International Conference on \nComputational Linguistics (eds Calzolari, N. et al. 6234–6247 (International Committee on Computational Linguistics, Gyeongju, \nRepublic of Korea, 2022).\n 16. Ando, K., Okumura, T., Komachi, M., Horiguchi, H. & Matsumoto, Y . Is artificial intelligence capable of generating hospital \ndischarge summaries from inpatient records?. PLOS Digit. Health  1, e0000158. https://doi.org/10.1371/journal.pdig.0000158 \n(2022).\n 17. Reynolds, L. & McDonell, K. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended \nAbstracts of the 2021 CHI Conference on Human Factors in Computing Systems (eds Kitamura, Y ., Quigley, A., Isbister, K. & Igarashi, \nT. ) 1–7 (ACM, New Y ork, 2021).\n 18. Aali, A. et al. A dataset and benchmark for hospital course summarization with adapted large language models. J. Am. Med. Inform. \nAssoc. JAMIA 32, 470–479. https://doi.org/10.1093/jamia/ocae312 (2025).\n 19. van Veen, D. et al. Adapted large language models can outperform medical experts in clinical text summarization. Nat. Med. 30, \n1134–1142. https://doi.org/10.1038/s41591-024-02855-5 (2024).\n 20. Ellershaw, S. et al. Automated generation of hospital discharge summaries using clinical guidelines and large language models. In \nAAAI 2024 Spring Symposium on Clinical Foundation Models. Available at https://openreview.net/pdf?id=1kDJJPppRG (2024).\n 21. Tan, Q., Ng, H. T. & Bing, L. Towards benchmarking and improving the temporal reasoning capability of large language models. \n(2023).\n 22. Lewis, P . et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. Available at http://arxiv.org/pdf/2005.11401v4 \n(2020).\n 23. Stiennon, N. et al. Learning to summarize from human feedback. In NIPS’20: Proceedings of the 34th International Conference on \nNeural Information Processing Systems, 3008–3021. https://doi.org/10.48550/arXiv.2009.01325 (2020).\n 24. Jung, H. et al. Enhancing clinical efficiency through LLM: Discharge note generation for cardiac patients (2024).\n 25. Dettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. QLoRA: Efficient finetuning of quantized LLMs (2023).\n 26. Brown, T. B. et al. Language models are few-shot learners. Available at http://arxiv.org/pdf/2005.14165v4 (2020).\n 27. Liu, J. et al. What makes good in-context examples for GPT-$3$? Available at http://arxiv.org/pdf/2101.06804v1 (2021).\n 28. Golchinfar, D. SauerkrautLM Model Card. Available at  h t t p s :  / / h u g g  i n g f a c  e . c o / V  A G O s o  l u t i o n  s / L l a m  a - 3 - S a  u e r k r a u t L M - 7 0 b - I n s t r \nu c t (2024).\n 29. Pal, A. & Sankarasubbu, M. OpenBioLLMs: Advancing open-source large language models for healthcare and life sciences. \nAvailable at  h t t p s :  / / h u g g  i n g f a c  e . c o / a  a d i t y  a / O p e n  B i o L L M  - L l a m a  3 - 7 0 B (2024).\n 30. AI@Meta. Llama 3 Model Card. Available at  h t t p s :  / / g i t h  u b . c o m  / m e t a -  l l a m a  / l l a m a  3 / b l o b  / m a i n /  M O D E L _ C A R D . m d (2024).\n 31. U.S. Department of Health and Human Services. Guidance regarding methods for de-identification of protected health information \nin accordance with the health insurance portability and accountability act (HIPAA) privacy rule. Available at  h t t p s :   /  / w w  w . h h  s . g  o v / \nh i  p  a a /  f  o r - p r o  f e s s i o  n  a l s /  p r i  v a  c y / s p e   c i a l -  t  o p i  c  s / d e - i d e n t i fi    c a t i o n  / i n d e x . h t m l (2012).\n 32. Richter-Pechanski, P . et al. A distributable German clinical corpus containing cardiovascular clinical routine doctor’s letters. Sci. \nData 10, 207. https://doi.org/10.1038/s41597-023-02128-9 (2023).\n 33. Menger, V ., Scheepers, F ., van Wijk, L. M. & Spruit, M. DEDUCE: A pattern matching method for automatic de-identification of \nDutch medical text. Telemat. Inform. 35, 727–736. https://doi.org/10.1016/j.tele.2017.08.002 (2018).\n 34. Pérez-Díez, I., Pérez-Moraga, R., López-Cerdán, A., Salinas-Serrano, J.-M. & de La Iglesia-Vayá, M. De-identifying Spanish \nmedical texts - named entity recognition applied to radiology reports. J. Biomed. Semant. 12, 6.  h t t p s : / / d o i . o r g / 1 0 . 1 1 8 6 / s 1 3 3 2 6 - 0 2 \n1 - 0 0 2 3 6 - 2     (2021).\n 35. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language \nunderstanding. In Proceedings of the 2019 Conference of the North (eds Burstein, J., Doran, C. & Solorio, T.) 4171–4186 (Association \nfor Computational Linguistics, Stroudsburg, 2019).\nAuthor contributions\nT.M.P ., M.G. and N.K. designed the study. P .F . collected and curated the data retrospectively. T.M.P . and M.G. \nprovided critical insights and guidance throughout the project. M.L. provided clinical data for the study. C.K.L. \nScientific Reports |        (2025) 15:16466 12| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/\ncontributed to the interpretation of the text data. N.K. implemented the experiments and performed the data \nanalysis. M.G., T.M.P ., and N.K. drafted the manuscript. All co-authors read and commented on the manuscript.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nThe research leading to these results received funding for T.M.P . through state funds approved by the State \nParliament of Baden-Württemberg for the Innovation Campus Health + Life Science alliance Heidelberg \nMannheim. The other authors did not receive funding for this research. For the publication fee the authors \nacknowledge financial support by Heidelberg University.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nEthical approval\nEthical review and approval were waived for generating a patient data sample from completed inpatient cases \nfrom the patient registry database of our pancreatic surgery center at Heidelberg University Hospital. No new \npatients were recruited, and all the obtained patient information was de-identified before data processing. \nInformed consent had been previously obtained from all subjects. The use of the database from our hospital \nwas conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of the \nmedical faculty at the University of Heidelberg (S301/2001; S708/2019; S083/2021).\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 0 1 6 1 8 - 7     .  \nCorrespondence and requests for materials should be addressed to M.G.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and \nindicate if changes were made. The images or other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025 \nScientific Reports |        (2025) 15:16466 13| https://doi.org/10.1038/s41598-025-01618-7\nwww.nature.com/scientificreports/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7799760103225708
    },
    {
      "name": "Correctness",
      "score": 0.6482177972793579
    },
    {
      "name": "Documentation",
      "score": 0.5867792963981628
    },
    {
      "name": "Terminology",
      "score": 0.5644989609718323
    },
    {
      "name": "Likert scale",
      "score": 0.44634929299354553
    },
    {
      "name": "Data science",
      "score": 0.44109901785850525
    },
    {
      "name": "Fluency",
      "score": 0.42311999201774597
    },
    {
      "name": "Unified Medical Language System",
      "score": 0.4209638237953186
    },
    {
      "name": "Language model",
      "score": 0.41394996643066406
    },
    {
      "name": "Information retrieval",
      "score": 0.41195064783096313
    },
    {
      "name": "Natural language processing",
      "score": 0.3932145833969116
    },
    {
      "name": "Data mining",
      "score": 0.3917885720729828
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3512415289878845
    },
    {
      "name": "Statistics",
      "score": 0.1294882595539093
    },
    {
      "name": "Programming language",
      "score": 0.12024235725402832
    },
    {
      "name": "Psychology",
      "score": 0.11390691995620728
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I223822909",
      "name": "Heidelberg University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I2802164966",
      "name": "University Hospital Heidelberg",
      "country": "DE"
    }
  ],
  "cited_by": 6
}