{
  "title": "FeedFormer: Revisiting Transformer Decoder for Efficient Semantic Segmentation",
  "url": "https://openalex.org/W4382449692",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2158358957",
      "name": "Jae Hun Shim",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2161691247",
      "name": "Hyunwoo Yu",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2774427172",
      "name": "Kyeongbo Kong",
      "affiliations": [
        "Pukyong National University"
      ]
    },
    {
      "id": "https://openalex.org/A2641478003",
      "name": "Suk Ju Kang",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2158358957",
      "name": "Jae Hun Shim",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2161691247",
      "name": "Hyunwoo Yu",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2774427172",
      "name": "Kyeongbo Kong",
      "affiliations": [
        "Pukyong National University"
      ]
    },
    {
      "id": "https://openalex.org/A2641478003",
      "name": "Suk Ju Kang",
      "affiliations": [
        "Sogang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W3217112505",
    "https://openalex.org/W3180659539",
    "https://openalex.org/W3020904829",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W3157274912",
    "https://openalex.org/W2150066425",
    "https://openalex.org/W2902930830",
    "https://openalex.org/W2910554758",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2924984885",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W6795103355",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W6811432336",
    "https://openalex.org/W2886934227",
    "https://openalex.org/W6805224724",
    "https://openalex.org/W2976122994",
    "https://openalex.org/W6730342312",
    "https://openalex.org/W6741437728",
    "https://openalex.org/W2950045474",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W4365446402",
    "https://openalex.org/W4312815172",
    "https://openalex.org/W2963419596",
    "https://openalex.org/W4385815585",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W3115781494",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W3217727157",
    "https://openalex.org/W4312820606",
    "https://openalex.org/W3213165621",
    "https://openalex.org/W4312986923",
    "https://openalex.org/W4390189979",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963136578",
    "https://openalex.org/W4287198652",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W3040304705",
    "https://openalex.org/W3034438741",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "With the success of Vision Transformer (ViT) in image classification, its variants have yielded great success in many downstream vision tasks. Among those, the semantic segmentation task has also benefited greatly from the advance of ViT variants. However, most studies of the transformer for semantic segmentation only focus on designing efficient transformer encoders, rarely giving attention to designing the decoder. Several studies make attempts in using the transformer decoder as the segmentation decoder with class-wise learnable query. Instead, we aim to directly use the encoder features as the queries. This paper proposes the Feature Enhancing Decoder transFormer (FeedFormer) that enhances structural information using the transformer decoder. Our goal is to decode the high-level encoder features using the lowest-level encoder feature. We do this by formulating high-level features as queries, and the lowest-level feature as the key and value. This enhances the high-level features by collecting the structural information from the lowest-level feature. Additionally, we use a simple reformation trick of pushing the encoder blocks to take the place of the existing self-attention module of the decoder to improve efficiency. We show the superiority of our decoder with various light-weight transformer-based decoders on popular semantic segmentation datasets. Despite the minute computation, our model has achieved state-of-the-art performance in the performance computation trade-off. Our model FeedFormer-B0 surpasses SegFormer-B0 with 1.8% higher mIoU and 7.1% less computation on ADE20K, and 1.7% higher mIoU and 14.4% less computation on Cityscapes, respectively. Code will be released at: https://github.com/jhshim1995/FeedFormer.",
  "full_text": "FeedFormer: Revisiting Transformer Decoder for Efficient Semantic Segmentation\nJae-hun Shim1*, Hyunwoo Yu1*, Kyeongbo Kong2*, Suk-Ju Kang1†\n1 Department of Electronic Engineering, Sogang University, Seoul, 04017, Republic of Korea\n2 Department of Media School, Pukyong National University, Busan, 48547, Republic of Korea\njhshim1995@sogang.ac.kr, hyunwoo137@sogang.ac.kr, kbkong@pknu.ac.kr, sjkang@sogang.ac.kr\nAbstract\nWith the success of Vision Transformer (ViT) in image classi-\nfication, its variants have yielded great success in many down-\nstream vision tasks. Among those, the semantic segmentation\ntask has also benefited greatly from the advance of ViT vari-\nants. However, most studies of the transformer for semantic\nsegmentation only focus on designing efficient transformer\nencoders, rarely giving attention to designing the decoder.\nSeveral studies make attempts in using the transformer de-\ncoder as the segmentation decoder with class-wise learnable\nquery. Instead, we aim to directly use the encoder features\nas the queries. This paper proposes the Feature Enhancing\nDecoder transFormer (FeedFormer) that enhances structural\ninformation using the transformer decoder. Our goal is to de-\ncode the high-level encoder features using the lowest-level\nencoder feature. We do this by formulating high-level features\nas queries, and the lowest-level feature as the key and value.\nThis enhances the high-level features by collecting the struc-\ntural information from the lowest-level feature. Additionally,\nwe use a simple reformation trick of pushing the encoder\nblocks to take the place of the existing self-attention mod-\nule of the decoder to improve efficiency. We show the supe-\nriority of our decoder with various light-weight transformer-\nbased decoders on popular semantic segmentation datasets.\nDespite the minute computation, our model has achieved\nstate-of-the-art performance in the performance computation\ntrade-off. Our model FeedFormer-B0 surpasses SegFormer-\nB0 with 1.8% higher mIoU and 7.1% less computation on\nADE20K, and 1.7% higher mIoU and 14.4% less compu-\ntation on Cityscapes, respectively. Code will be released at:\nhttps://github.com/jhshim1995/FeedFormer.\nIntroduction\nSemantic segmentation is one of the most fundamental com-\nputer vision tasks, and it is used extensively in many real-\nworld applications, such as autonomous driving (Cordts\net al. 2016; Geiger, Lenz, and Urtasun 2012) and medical di-\nagnoses (Ma et al. 2021). However, the accurate pixel-wise\nprediction of the whole image is very challenging because it\nrequires considerations for both global and local relations.\n*These authors contributed equally.\n†Corresponding author.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nRecently, this difficulty has been largely alleviated with\nthe emergence of transformers. Following the significant im-\nprovement by using the transformer architectures in natural\nlanguage processing (NLP), (Dosovitskiy et al. 2020) first\nintroduced Vision Transformer (ViT) which utilizes the pure\ntransformer encoder for vision tasks. Since then, many stud-\nies have been conducted to extend ViT to the field of the\nsemantic segmentation (Xie et al. 2021; Zheng et al. 2021;\nLiu et al. 2021; Wang et al. 2021; Yu and Wu 2021; Yang\net al. 2022; Jain et al. 2021). In these studies, most methods\nfocus only on improving the efficiency of the transformer\nencoder, but only take a simple design or borrow from the\nexisting design for the decoder architecture. For example,\nSegFormer in Figure 1 (a) shows impressive performance\nby designing an efficient transformer encoder with a sim-\nple ALL-MLP decoder. However, it lacks consideration for\ndesigning the segmentation decoder optimized for the trans-\nformer encoder structure.\nOther studies include utilizing the transformer decoder\nstructure in vision tasks. DEtection TRansformer (Carion\net al. 2020) was the first to bring the transformer encoder-\ndecoder structure to detection and segmentation tasks. In-\nspired by DETR, several works expand on DETR to employ\nthe transformer decoder in the task of semantic segmenta-\ntion (Strudel et al. 2021; Bousselham et al. 2021; Cheng,\nSchwing, and Kirillov 2021; Cheng et al. 2022). The main\ndifference between the feature decoder and the learnable\nobject query decoder in Figure 1 (a) and (b), is the use\nof learnable query. General transformer decoders use class-\nwise learnable queries to extract feature information related\nto each class. Therefore, they either only use the highest-\nlevel feature or need an additional pixel decoder to extract\nthe multi-scale features. Moreover, to utilize the multi-scale\nfeatures, the class-wise queries have to be decoded cumula-\ntively for each multi-scale feature. Therefore, the feature has\nto pass through the transformer decoder repetitively, which\ncauses large computational complexity.Our strategy is to di-\nrectly use features as queries instead of class-wise learnable\nqueries. In other words, we replace an ALL-MLP feature de-\ncoder with the transformer decoder, as shown in Figure 1\n(c). Then, which level of feature should be used as query\nand key?\nIn the semantic segmentation task, consideration of low-\nlevel feature is very important because the spatial detail is\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n2263\nFigure 1: Comparison between the proposed and previous architectures. (a) Feature decoder (SegFormer) decodes multi-scale\nfeatures from the encoder using only MLPs. (b) Architectures like Mask2Former take a learnable token as the query to use\nit for the mask prediction. The repetition of the transformer decoder and an additional pixel decoder is required to consider\nmulti-scale features. (c) Our method, feature query decoder (FeedFormer), is the combination of the two methods. Our method\n(c) uses the feature query instead of the learnable query as in (b) to decode higher-level features with respect to the lowest-level\nfeature which contains structural information.\nlost as the network progresses down to high-level features.\nTherefore, how to pass on the low-level detail to the high-\nlevel semantic feature is of great importance. To cope with\nthe challenge, many studies have adopted various techniques\nto restore lost details in the latter stages of the network (Ron-\nneberger, Fischer, and Brox 2015; Chen et al. 2018; Wang\net al. 2020). Their precise methods all differ, but they all\nhave a unifying goal of figuring out how to add the low-level\nrepresentation to the high-level features.\nThis paper proposes the Feature Enhancing Decoder\ntransFormer (FeedFormer), which improves structure infor-\nmation of the high-level features with the transformer de-\ncoder architecture for segmentation prediction.Our key idea\nis to decode the high-level encoder features using the lowest-\nlevel encoder feature. To do this, as shown in Figure 1 (c),\nwe replace an ALL-MLP decoder to the transformer de-\ncoder, which considers the lowest-level feature as key and\nvalue and the higher features as queries. Intuitively, this is\nreasonable because using the key information, the decoder\ncollects the value information most relevant to the query\nand adds the relevant information back to the query. After\ndecoding each feature with respect to the lowest-level fea-\nture, the lowest-level feature and all decoded features are\nconcatenated through a prediction head to obtain a segmen-\ntation map.\nFortunately, the standard transformer decoder starts with\na self-attention module, which has an overlapping role with\nthe encoder’s self-attention block. Therefore, to improve ef-\nficiency, we introduce a novel reformation trick, which is to\nsimply push the self-attention block of the encoder into the\ndecoder to take the place of the existing self-attention mod-\nule as shown in Figure 2 (d). In addition, since our Feed-\nFormer uses the transformer decoder only once, the addi-\ntional computation imposed by the decoder is very small.\nWe demonstrate the advantages of our architecture in\nterms of model size, computation, and performance on two\npublicly available datasets. Our contributions are summa-\nrized as follows:\n• We propose a novel and efficient transformer decoder\narchitecture for semantic segmentation, which uses fea-\ntures as queries instead of class-wise learnable queries. In\nmore detail, our FeedFormer decodes high-level features\n(queries) with respect to the lowest-level feature (key),\nwhich serves to enhance the structural information ab-\nsent in the high-level features.\n• To improve efficiency, we rephrase a part of the trans-\nformer encoder as the transformer decoder and push the\nself-attention block of the encoder into the decoder. We\nalso exploit the transformer decoder only once for each\nfeature, which strengthens the efficiency of our model.\n• Our methodology can be easily applied to various\ntransformer-based encoders such as Mix Transformer\n(MiT) (Xie et al. 2021) and Lite Vision Transformer\n(LVT) (Yang et al. 2022) with a simple modification.\n• The superior performance of our FeedFormer is demon-\nstrated on widely used semantic segmentation datasets\nin light-weight and advanced settings. Our light-weight\nmodel, FeedFormer-LVT, achieves 41.0% mIoU on\nADE20K and 78.6% mIoU on Cityscapes val, respec-\ntively with only 4.6M parameters.\n2264\nFigure 2: The proposed FeedFormer framework (a) A conceptual diagram. Among the features F1, F2, F3, and F4 extracted\nfrom the encoder, only F1 is used as the key and value. F2, F3, and F4 are used as feature queries in the transformer decoder.\nThrough this, F2, F3, and F4 are decoded by extracting the structural information related to F1. (b) A simplified version of (a).\n(c) Final form of our FeedFormer architecture after applying the rearrangement trick. (d) The rearrangement process in which\nthe self-attention layer of the transformer decoder is replaced by the encoder stage block.\nRelated Work\nTransformer Encoder Architecture\nSince the success of transformer (Vaswani et al. 2017) in\nNLP, the scope of transformer applications expanded to var-\nious vision tasks. ViT (Dosovitskiy et al. 2020) first proved\nthe effectiveness of using transformer in vision task by ap-\nplying it in the classification task. ViT splits an image into\na sequence of patches, which is then linearly embedded as a\ntoken for the transformer block. SETR (Zheng et al. 2021)\nused vision transformer for the segmentation task by adopt-\ning ViT as the segmentation encoder. PVT (Wang et al.\n2021) and Swin Transformer (Liu et al. 2021) tackled the\nproblem of ViT of only being able to process single-scale\nfeatures and proposed hierarchical vision transformers with\nfour-stage design along with down-sampling features to be\napplied to dense prediction tasks. LVT (Yang et al. 2022)\ndived further into increasing the efficiency of ViT. Recently,\nSegFormer (Xie et al. 2021) proposed a very simple and ef-\nficient joint structure of a Mix Transformer encoder and an\nALL-MLP decoder.\nTransformer Decoder Architecture\nRecent works adopted the transformer decoder to enhance\nthe performance of semantic segmentation. DETR (Carion\net al. 2020) was the first to apply a pure segmentation de-\ncoder for the segmentation task. DETR used the learned set\nof object queries to reason about the relations of the object\nand the context to produce a parallel set of predictions. Seg-\nmenter (Strudel et al. 2021) used a pure transformer encoder-\ndecoder for semantic segmentation. SenFormer (Boussel-\nham et al. 2021), MaskFormer (Cheng, Schwing, and Kir-\nillov 2021), and Mask2Former (Cheng et al. 2022) used the\ntransformer decoder by learning query sets, that can refine\nmulti-scales features extracted from the backbone by stack-\ning transformer decoders. However all these methodologies\nuse randomly initialized learnable query, requiring an addi-\ntional pixel-decoder to extract multi-scale features inputted\nto the transformer decoder. Our model differs in that multi-\nscales features are directly used from the encoder for the\ntransformer decoder by formulating the detailed highest-\nlevel feature as the feature query.\n2265\nProposed FeedFormer Architecture\nIn this section, we present the proposed FeedFormer, the\nsimple transformer encoder-decoder structure for semantic\nsegmentation. Overall conceptual diagram is illustrated in\nFigure 2 (a). We first extract four multi-scale features from\nthe hierarchical vision transformer encoder. Then, of the\nfour multi-scale features, we use the latter three features,\nF2, F3, and F4 as queries, and the lowest-level feature, F1,\nas the key and value of the transformer decoder. This is\nthe main difference from the existing transformer decoder,\nwhich uses learnable class-wise queries. In the decoder,\npooling is performed onF1 to match the spatial dimension of\nthe queries. The output decoded features, D2, D3, and D4,\nare up-sampled to match the spatial dimension of F1. The\nthree output decoded features, together withF1, are concate-\nnated and processed through the MLP layer to produce the\nfinal prediction. Our FeedFormer can also be depicted as in\nFigure 2 (b). Next, let us take a closer look at the inside of\nour FeedFormer decoder.\nBuilding Blocks of FeedFormer\nThe transformer conducts attention mechanisms for the em-\nbedded patches. Here, we briefly explain the patch embed-\nding layer and the attention block.\nPatch Embedding Layer For the case of most hierarchi-\ncal transformers, when an image withH ×W ×3 is inputted\nto the network, output features of each stage take the shape\nof Fi ∈ R(Hi×Wi×Ci). Each stage feature Fi is then divided\ninto H×W\n2i+1×2i+1 number of patches, which is linearly pro-\njected to produce embedded patches with H\n2i+1 × W\n2i+1 × Ci\nsize. Linearly embedded patches serve as an input to the\nmulti-head attention (MHA) layer in the attention block.\nAttention Block In the attention block of Figure 3, the\nMHA layer with query, key, and value, each denoted as Q,\nK and V, is computed as follows:\nMHA(Q, K, V) =Softmax (QKT)V. (1)\nThis MHA layer and the feed forward layer (FFL) are se-\nquentially connected, with layer norms (LNs) applied before\nevery block and residual connections after every block:\naj−1 = MHA(LN(kvj−1, qj−1)) +qj−1, (2)\nqj = FFL(LN(aj−1)) +aj−1, (3)\nwhere qj−1 is the embedded patches of query, kvj−1 is the\nembedded patches of key, value, and j ∈ {1, ..., Li}. Li is\nthe number of repeated attention blocks.\nThe main difference between a self-attention layer and a\ncross-attention layer is how to configure the key, query, and\nvalue. The self-attention layer uses the same feature as the\nkey, query, and value, whereas the cross-attention layer uses\none feature as the query and another feature as the key and\nvalue. In our FeedFormer, the cross-attention layer regards\nthe lowest-level feature as the key and value and high-level\nfeatures as the queries. Through the attention mechanism,\nit collects the detail information highly relevant to the query\nfeature from the lowest-level feature and adds it back to each\nFigure 3: Attention block in a general transformer architec-\nture. kvj−1 denotes the feature for key and value, and qj−1\ndenotes the query feature.\nhigh-level feature. This process has the effect of improving\nthe detail of the high-level features regardless of the feature\nscales.\nReforming FeedFormer for Efficient Architecture\nIntuitively, the feature queries of the transformer decoder are\nsequentially connected. Therefore, as in Steps 1 and 2 in Fig-\nure 2 (d), Fi+1 can be represented as a function ofFi, where\nStagei denotes each encoder stage, and PatchEmbed de-\nnotes the Patch Embedding layer:\nFi+1 = Stagei+1(PatchEmbed (Fi)). (4)\nFortunately, the typical transformer decoder starts with a\nself-attention module, which has an overlapping role with\nthe encoder’s self-attention block. Therefore, to enhance ef-\nficiency, we simply push the self-attention block of the en-\ncoder into the decoder to take the place of the existing self-\nattention module (Step 3). As a result, the query of the trans-\nformer decoder is changed to the patch embedded feature of\nthe previous stage (Step 4). This process is equally applied\nto all three decoder stages. Our final architecture results in\nFigure 2 (c), where only the first stage is used as the encoder,\nand the other stages are incorporated into the decoder. Note\nthat the feature queries are changed by the encoder’s previ-\nous stage features. To the best of our knowledge, we are the\nfirst that the part of the encoders has been reinterpreted as\ndecoders to make an efficient structure.\nIn general, the learnable query-based transformer de-\ncoders repeatedly perform decoding to utilize multi-scale\nfeatures. In contrast, our FeedFormer uses rich features\nof encoders as queries, thus, structural information can be\ntransferred well by only decoding each feature once.\nExperiments\nExperimental Settings\nDatasets We conducted experiments on two publicly\navailable datasets, ADE20K (Zhou et al. 2017) and\nCityscapes (Cordts et al. 2016). ADE20K is a challenging\nscene parsing dataset covering 150 fine-grained semantic\nconcepts. It consists of a training set of 20,210 images, a\nvalidation set of 2,000 images, and a testing set of 3,352 im-\nages. Cityscapes is an urban driving scene dataset for seman-\ntic segmentation consisting of 5,000 finely annotated with 19\ncategories. It contains 5,000 high-resolution images divided\ninto a training set of 2,975 images, a validation set of 500\nimages and a testing set of 1,525 images.\n2266\nMethod Encoder Params (M) ADE20K Cityscapes\nGFLOPs mIoU GFLOPs mIoU\nLight-weight\nFCN (Long, Shelhamer, and Darrell 2015) MobileNetV2 9.8 39.6 19.7 317.1 61.5\nPSPNet (Zhao et al. 2017) MobileNetV2 13.7 52.9 29.6 423.4 70.2\nDeepLabV3+ (Chen et al. 2018) MobileNetV2 15.4 69.4 34.0 555.4 75.2\nSwiftNetRN (Orsic et al. 2019) RseNet18 11.8 - - 104.0 75.5\nSemantic FPN (Li et al. 2021) ConvMLP-S 12.8 33.8 35.8 - -\nSegFormer-B0 (Xie et al. 2021) MiT-B0 3.8 8.4 37.4 125.5 76.2\nFeedFormer-B0 (Ours) MiT-B0 4.5 7.8 39.2 107.4 77.9\nSegFormer-LVT (Yang et al. 2022) LVT 3.9 10.6 39.3 140.9 77.6\nFeedFormer-LVT (Ours) LVT 4.6 10.0 41.0 124.6 78.6\nAdvanced\nCCNet (Huang et al. 2019) ResNet-101 68.9 278.4 43.7 2224.8 79.5\nDeepLabV3+ (Chen et al. 2018) ResNet101 62.7 255.1 44.1 2032.3 80.9\nOCRNet (Yuan, Chen, and Wang 2020) HRNet-W48 70.5 164.8 45.6 1296.8 81.1\nAuto-DeepLab (Liu et al. 2019) Auto-DeepLab-L 44.4 - - 695.0 80.3\nSwin UperNet-T (Liu et al. 2021) Swin-T 60.0 236.0 44.4 - -\nSenFormer (Bousselham et al. 2021) Swin-T 144.0 179.0 46.0 - -\nSeg-S-Mask/16 (Strudel et al. 2021) ViT-S 22.0 - 45.4 - -\nSegFormer-B2 (Xie et al. 2021) MiT-B2 27.5 62.4 46.5 717.1 81.0\nMaskFormer (Cheng, Schwing, and Kirillov 2021) Swin-T 42.0 55.0 46.7 - -\nMask2Former (Cheng et al. 2022) Swin-T 47.0 74.0 47.7 - 82.1\nFeedFormer-B2 (Ours) MiT-B2 29.1 42.7 48.0 522.7 81.5\nTable 1: Performance comparison with the state-of-the-art methods on ADE20K val and Cityscapes val. Compared to other\nmethods, our model displays superior performance compared to models with similar or larger computational complexity.\nMethod Params (M) GFLOPs mIoU\nSegmenter-B0 4.4 - 32.5\nFeedFormer-B0 (Ours) 4.5 7.8 39.2\nMask2Former-B0 23.0 51.8 41.1\nFeedFormer-B2 (Ours) 29.4 42.7 48.0\nTable 2: Performance comparison with learnable object\nquery decoders. B0 and B2 each denote MiT-B0, MiT-B2.\nMethod ADE20K CityScapes\nmIoU mBA mIoU mBA\nSegFormer-B0 37.4 31.2 76.2 53.6\nFeedFormer-B0 (Ours) 39.2 32.0 77.9 54.2\nSegFormer-B2 46.5 35.4 81.0 56.0\nFeedFormer-B2 (Ours) 48.0 35.8 81.5 56.1\nTable 3: Comparison of mBA on SegFormer and Feed-\nFormer. The result shows consistent improvement in both\nsegmentation mIoU and boundary mBA.\nImplementation Details We used Mix Transformer\n(MiT) (Xie et al. 2021) and Lite Vision Transformer (LVT)\n(Yang et al. 2022) as our encoder backbone. Our model with\nMiT-B0, MiT-B2, and LVT encoder backbone were each\nnamed FeedFormer-B0, FeedFormer-B2, and FeedFormer-\nLVT. For our light-weight variants FeedFormer-B0, and\nFeedFormer-LVT, we used an embedding dimension of 128\nfor the final MLP before the prediction head, and 768 for\nFeedFormer-B2. For ADE20K, we keep the input resolution\nof the features when inputted to the decoder. For Cityscapes,\nwe downscale all the features into half the input dimension\nbefore inputting them into the decoder to compensate for\nthe high computational cost imposed by the high resolution\nof the image.\nDecoder Params (M) GFLOPs mIoU\nSum Head 3.5 5.6 36.5\nConcat Head 3.6 7.6 37.2\nCross-attn Head 4.4 7.3 37.2\nOurs 4.5 7.8 39.2\nTable 4: Performance comparison with different decoder\nheads.\nF1 F2 F3 F4 Params (M) mIoU\nK, V - - - 4.5 39.2\n- K, V - - 4.4 38.8\n- - K, V - 4.1 38.1\nTable 5: Performance comparison with different level fea-\ntures as the key and value.K, Veach indicates key, value of\nthe transformer decoder.\nTraining and Evaluation Settings We used the default\nsetting based on the public codebase mmsegmentation1. We\nused 4 RTX 3090 GPUs for all training throughout the ex-\nperiments. We used encoders pre-trained on ImageNet-1K\ndataset. During the training, we applied data augmentation\nusing random resize from 0.5 to 2.0 ratios, random hori-\nzontal flipping, and random cropping to 512×512 pixel res-\nolution for ADE20k and 1024×1024 pixel resolution for\nCityscapes. We trained the models using AdamW optimizer\nfor 160K iterations. We set the batch size to 16 for ADE20K\nand 8 for Cityscapes. We set the learning rate to an initial\nvalue of 6e-5, and then, used a polynomial learning rate de-\ncay schedule with factor 1.0 by default. For evaluation, we\nuse ADE20K validation set and Cityscapes validation set to\n1https://github.com/open-mmlab/mmsegmentation\n2267\nFigure 4: Qualitative results on ADE20K and Cityscapes.\nCompared to SegFormer, our FeedFormer shows strength in\naccurately segmenting complex details.\ntest the performance of our model. For ADE20K, we re-\nscaled the short side of the image to 512 and kept the as-\npect ratio. For Cityscapes, we applied the sliding window\ntest by cropping 1024 × 1024 windows. We reported all\nour main semantic segmentation results in mean Intersection\nover Union (mIoU) under the single scale inference setting.\nComparison to State-of-The-Art Methods\nWe compared our results with the existing methods on\nADE20K and Cityscapes datasets. Table 1 presents our re-\nsults including parameter size, Floating Point Operations\n(FLOPs), and mIoU for ADE20K and Cityscapes.\nLight-weight Models In the top part of Table 1, we re-\nported the performance of the light-weight models. As\nshown in the table, our light-weight model FeedFormer-\nB0 yielded 39.2% mIoU with only 4.5M parameters and\n7.8 GFLOPs for ADE20K. Compared to SegFormer-B0,\nFeedFormer-B0 shows 1.8% higher mIoU and 7.1% less\ncomputation. For Cityscapes, the performance gap in-\ncreases even more, achieving 77.9% mIoU with only 107.4\nGFLOPs. This is 1.7% increase in mIoU and 14.4% de-\ncrease in computation compared to SegFormer-B0. By us-\ning LVT as the backbone, we were able to boost the per-\nformance further, outperforming all models compared in the\ntable. Our FeedFormer-LVT showed exceeding performance\nwith 41.0% and 78.6% mIoU in ADE20K and Cityscapes\nrespectively, with only 4.6M parameters.\nAdvanced Models In the bottom part, we show the re-\nsults of the larger model, FeedFormer-B2. The results of\nFeedFormer-B2 showed outstanding results even in a rel-\natively larger model. FeedFormer-B2 yielded 48.0% mIoU\nwith only 29.1M parameters and 42.7 GFLOPs on ADE20K,\nwhich is 31.2% less computation and 1.5% better mIoU\ncompared to SegFormer-B2. In particular, when compared\nFigure 5: Visualization of ADE20K prediction results with\nand without the presence of transformer decoder; w/o de-\ncoding: 37.1% mIoU and w/ decoding: 38.2% mIoU.\nto the mask2former, which uses the powerful Swin-T back-\nbone, the number of parameters decreased by 38%, and the\ncomputation decreased by 42%, with 0.3% higher mIoU. On\nCityscapes, our FeedFormer-B2 yielded 81.5% mIoU with\nonly 522.7 GFLOPs. This is 0.5% higher mIoU and 27%\nless computation compared to SegFormer-B2, proving the\nsuperiority of our model on all two datasets.\nQualitative Results\nFigure 4 shows the qualitative results between our Feed-\nFormer and SegFormer on ADE20K and Cityscapes. As our\nmethod has the benefit of considering structural informa-\ntion when decoding multi-scale features, our model clearly\nshows strength in segmenting complex regions with details.\nAblation Studies\nEffectiveness of Feature Decoding Figure 5 visualizes\nthe effect of feature decoding with ADE20K prediction re-\nsults. You can see that without the decoding process, inac-\ncurate segmentation occurs in the boundaries of complex\nobjects. Figure 6 further visualizes the effect of our detail\nenhancement decoder. In the figure, F1, F2, F3, F4 denote\nthe four multi-scale features, outputted from the encoder,\nand D2, D3, D4 denote the decoded features with respect to\nF1. The figure shows that the decoded features highlight the\nboundary details of each multi-scale feature, which are lost\nbefore the decoding.\nEffectiveness of using Feature Queries Next, to ver-\nify the effectiveness of feature decoding, we compared\nour FeedFormer with learnable query-based transformer de-\ncoder architecture, Segmenter (Strudel et al. 2021) and\nMask2Former (Cheng et al. 2022). For a fair comparison of\nthe decoders, we used MiT-B0 as the encoder for all exper-\niments. As shown in Table 2, FeedFormer-B0 outperforms\n2268\nFigure 6: Visualization of feature detail enhancement of the proposed method. F2, F3, and F4 denote the query features of the\ndecoder, F1 the key, value feature, and D2, D3, and D4 denote the output of each cross-attention block. Note that boundary\ndetails from F1 are added to F2, F3, and F4 to produce output features D2, D3, and D4.\nSegmenter-B0, because only the highest-level feature is used\nfrom the hierarchical transformer encoder. Mask2Former-\nB0 shows a slight increase in performance compared to\nFeedFormer-B0. However, its size and computation are\ncomparable to our larger model FeedFormer-B2, which has\n6.9% higher mIoU on ADE20K. This shows that our model\nis more effective in decoding multi-scale features compared\nto transformer decoders that use learnable query.\nBoundary Detail Recovery To show that using the\nlowest-level feature as the key and value leads to the preser-\nvation of boundary details, we evaluated our FeedFormer\nand SegFormer using mean Boundary Accuracy (mBA) pro-\nposed by (Cheng et al. 2020). As shown in Table 3, our\nFeedFormer produces segmentation predictions with strong\nboundaries, which result in the final segmentation perfor-\nmance increase.\nEffectiveness of Decoding Head In this experiment, we\nverified the effect of our decoding head by comparing\nsummation head (Sum Head), concatenation head (Concat\nHead), and cross-attention head (Cross-attn Head) to our\nFeedFormer head. Using the three techniques, the features\nwere decoded with respect to the lowest-level featureF1. For\nthe Sum Head and the Concat Head, MLPs of embedding\ndimension 256 were each applied to the feature beforehand,\nand Cross-Attn head only used cross-attention in decoding\nthe feature. Table 4 shows that our FeedFormer head has\nachieved the best performance with similar computational\ncomplexity. The result also shows that the sole use of cross-\nattention does not bring performance improvements. This is\nbecause the meta-architecture of transformer is essential for\nthe performance, which is also discussed in the recent study\n(Yu et al. 2022). Our result shows that the same belief ap-\nplies to the design of transformer decoder.\nWhich key, value do we choose? In Table 5, To see which\nfeature serves the best as the key and value for the decoder,\nwe experimented by changing the key and value of our de-\ncoder from F1 to F3. The results show that using the lowest-\nlevel F1 as the key and value performs the best out of the\nthree features. From this observation, we used the lowest-\nlevel feature F1 as the key and value of the decoder.\nMethod Params (M) GFLOPs mIoU Time (ms)\nPSPNet 13.7 423.4 70.2 85.5\nDeepLabV3 15.4 125.5 75.2 116.3\nBiSeNet 49.0 98.3 74.8 28.6\nSTDCNet 16.1 54.9 77.0 33.7\nSegFormer 3.8 125.5 76.2 91.2\nFeedFormer 4.5 107.4 77.9 79.5\nTable 6: Inference time comparisons with real-time models.\nLimitations Our proposed method encounters limitations\nwhen it comes to inference time. In Table 6, we represent\nthe inference time comparisons of various models including\nsome famous real-time segmentation models on Cityscapes\ndataset. We tested inference time of a single image of\n2048×1024 resolution using a single RTX3090 GPU un-\nder the mmsegmentation benchmark without any additional\naccelerating techniques. The results show that our Feed-\nFormer is slow compared to the CNN-based real-time mod-\nels, BiSeNet (Yu et al. 2018) and STDCNet (Fan et al. 2021).\nOur model was not able to overcome the speed lag from the\nlack of optimization support of the transformer components,\nwhich is a general problem in many transformer-based mod-\nels. We leave such limitations to future works.\nConclusion\nIn this paper, we present FeedFormer, a simple, pure yet\npowerful decoder architecture that enhances the structural\ninformation of the high-level features using the lowest-level\nfeature. We perform this by building a transformer decoder,\nwhich takes the lowest-level feature as the key and value,\nand high-level features as the queries. To make our architec-\nture more efficient, we proposed a simple reformation trick\nthat pushes the encoder blocks to replace the existing self-\nattention module of the decoder. We show the superiority of\nour decoder with various light-weight transformer-based en-\ncoders on popular semantic segmentation datasets. For a fu-\nture work, we plan to conduct experiments on various cross-\nattention blocks of the transformer decoder to verify our fea-\nture decoding methodology.\n2269\nAcknowledgments\nThis work was supported by the National Research Foun-\ndation of Korea (NRF) grant funded by the Korea gov-\nernment (MSIT) (No. 2021R1A2C1004208), the National\nRD Program through the National Research Founda-\ntion of Korea(NRF) funded by Ministry of Science and\nICT(2021M3H2A1038042) and the Samsung Electronics\nCo., Ltd(IO201218-08232-01).\nReferences\nBousselham, W.; Thibault, G.; Pagano, L.; Machireddy, A.;\nGray, J.; Chang, Y . H.; and Song, X. 2021. Efficient self-\nensemble framework for semantic segmentation. arXiv\npreprint arXiv:2111.13280.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European conference on computer vi-\nsion, 213–229. Springer.\nChen, L.-C.; Zhu, Y .; Papandreou, G.; Schroff, F.; and\nAdam, H. 2018. Encoder-decoder with atrous separable con-\nvolution for semantic image segmentation. In Proceedings\nof the European conference on computer vision (ECCV) ,\n801–818.\nCheng, B.; Misra, I.; Schwing, A. G.; Kirillov, A.; and Gird-\nhar, R. 2022. Masked-attention mask transformer for univer-\nsal image segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n1290–1299.\nCheng, B.; Schwing, A.; and Kirillov, A. 2021. Per-pixel\nclassification is not all you need for semantic segmenta-\ntion. Advances in Neural Information Processing Systems,\n34: 17864–17875.\nCheng, H. K.; Chung, J.; Tai, Y .-W.; and Tang, C.-K. 2020.\nCascadepsp: Toward class-agnostic and very high-resolution\nsegmentation via global and local refinement. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 8890–8899.\nCordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler,\nM.; Benenson, R.; Franke, U.; Roth, S.; and Schiele, B.\n2016. The cityscapes dataset for semantic urban scene un-\nderstanding. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 3213–3223.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFan, M.; Lai, S.; Huang, J.; Wei, X.; Chai, Z.; Luo, J.; and\nWei, X. 2021. Rethinking BiSeNet for real-time semantic\nsegmentation. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 9716–9725.\nGeiger, A.; Lenz, P.; and Urtasun, R. 2012. Are we ready\nfor autonomous driving? the kitti vision benchmark suite.\nIn 2012 IEEE conference on computer vision and pattern\nrecognition, 3354–3361. IEEE.\nHuang, Z.; Wang, X.; Huang, L.; Huang, C.; Wei, Y .; and\nLiu, W. 2019. Ccnet: Criss-cross attention for semantic seg-\nmentation. In Proceedings of the IEEE/CVF international\nconference on computer vision, 603–612.\nJain, J.; Singh, A.; Orlov, N.; Huang, Z.; Li, J.; Wal-\nton, S.; and Shi, H. 2021. Semask: Semantically masked\ntransformers for semantic segmentation. arXiv preprint\narXiv:2112.12782.\nLi, J.; Hassani, A.; Walton, S.; and Shi, H. 2021. Convmlp:\nHierarchical convolutional mlps for vision. arXiv preprint\narXiv:2109.04454.\nLiu, C.; Chen, L.-C.; Schroff, F.; Adam, H.; Hua, W.; Yuille,\nA. L.; and Fei-Fei, L. 2019. Auto-deeplab: Hierarchical neu-\nral architecture search for semantic image segmentation. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, 82–92.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 10012–10022.\nLong, J.; Shelhamer, E.; and Darrell, T. 2015. Fully convo-\nlutional networks for semantic segmentation. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, 3431–3440.\nMa, J.; Wang, Y .; An, X.; Ge, C.; Yu, Z.; Chen, J.; Zhu, Q.;\nDong, G.; He, J.; He, Z.; et al. 2021. Toward data-efficient\nlearning: A benchmark for COVID-19 CT lung and infection\nsegmentation. Medical physics, 48(3): 1197–1210.\nOrsic, M.; Kreso, I.; Bevandic, P.; and Segvic, S. 2019. In\ndefense of pre-trained imagenet architectures for real-time\nsemantic segmentation of road-driving images. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 12607–12616.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Con-\nvolutional networks for biomedical image segmentation. In\nInternational Conference on Medical image computing and\ncomputer-assisted intervention, 234–241. Springer.\nStrudel, R.; Garcia, R.; Laptev, I.; and Schmid, C. 2021.\nSegmenter: Transformer for semantic segmentation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 7262–7272.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, J.; Sun, K.; Cheng, T.; Jiang, B.; Deng, C.; Zhao,\nY .; Liu, D.; Mu, Y .; Tan, M.; Wang, X.; et al. 2020. Deep\nhigh-resolution representation learning for visual recogni-\ntion. IEEE transactions on pattern analysis and machine\nintelligence, 43(10): 3349–3364.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. InProceedings of the IEEE/CVF International\nConference on Computer Vision, 568–578.\n2270\nXie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;\nand Luo, P. 2021. SegFormer: Simple and efficient design\nfor semantic segmentation with transformers. Advances in\nNeural Information Processing Systems, 34: 12077–12090.\nYang, C.; Wang, Y .; Zhang, J.; Zhang, H.; Wei, Z.; Lin, Z.;\nand Yuille, A. 2022. Lite vision transformer with enhanced\nself-attention. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 11998–12008.\nYu, C.; Wang, J.; Peng, C.; Gao, C.; Yu, G.; and Sang, N.\n2018. Bisenet: Bilateral segmentation network for real-time\nsemantic segmentation. In Proceedings of the European\nconference on computer vision (ECCV), 325–341.\nYu, H.; and Wu, J. 2021. A unified pruning framework for\nvision transformers. arXiv preprint arXiv:2111.15127.\nYu, W.; Luo, M.; Zhou, P.; Si, C.; Zhou, Y .; Wang, X.; Feng,\nJ.; and Yan, S. 2022. Metaformer is actually what you need\nfor vision. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 10819–10829.\nYuan, Y .; Chen, X.; and Wang, J. 2020. Object-contextual\nrepresentations for semantic segmentation. In European\nconference on computer vision, 173–190. Springer.\nZhao, H.; Shi, J.; Qi, X.; Wang, X.; and Jia, J. 2017. Pyra-\nmid scene parsing network. InProceedings of the IEEE con-\nference on computer vision and pattern recognition, 2881–\n2890.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition ,\n6881–6890.\nZhou, B.; Zhao, H.; Puig, X.; Fidler, S.; Barriuso, A.; and\nTorralba, A. 2017. Scene parsing through ade20k dataset. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 633–641.\n2271",
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.7946312427520752
    },
    {
      "name": "Computer science",
      "score": 0.7332894206047058
    },
    {
      "name": "Transformer",
      "score": 0.6958491206169128
    },
    {
      "name": "Segmentation",
      "score": 0.6909406185150146
    },
    {
      "name": "Computation",
      "score": 0.6356309652328491
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49522629380226135
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.413211464881897
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36562037467956543
    },
    {
      "name": "Computer vision",
      "score": 0.3636731505393982
    },
    {
      "name": "Algorithm",
      "score": 0.16370561718940735
    },
    {
      "name": "Engineering",
      "score": 0.12288492918014526
    },
    {
      "name": "Voltage",
      "score": 0.07232159376144409
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}