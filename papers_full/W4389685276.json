{
  "title": "Unbiased organism-agnostic and highly sensitive signal peptide predictor with deep protein language model",
  "url": "https://openalex.org/W4389685276",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5017524939",
      "name": "Junbo Shen",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5069586601",
      "name": "Qinze Yu",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5017603767",
      "name": "Shenyang Chen",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5101846980",
      "name": "Qingxiong Tan",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5101650532",
      "name": "Jingchen Li",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5100345753",
      "name": "Yu Li",
      "affiliations": [
        "City University of Hong Kong, Shenzhen Research Institute",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2767638256",
    "https://openalex.org/W2038605297",
    "https://openalex.org/W2066636585",
    "https://openalex.org/W2042096080",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W2149241518",
    "https://openalex.org/W2094201044",
    "https://openalex.org/W2912990441",
    "https://openalex.org/W4205192056",
    "https://openalex.org/W2131778683",
    "https://openalex.org/W2006201402",
    "https://openalex.org/W1935158889",
    "https://openalex.org/W2583479953",
    "https://openalex.org/W2952200190",
    "https://openalex.org/W2773939681",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W6791955017",
    "https://openalex.org/W3144239152",
    "https://openalex.org/W1972340877",
    "https://openalex.org/W6682137061",
    "https://openalex.org/W4286500588",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W2977368063",
    "https://openalex.org/W4214943162",
    "https://openalex.org/W2613156048",
    "https://openalex.org/W4309506674",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W3136918052",
    "https://openalex.org/W2107158607",
    "https://openalex.org/W6676179870",
    "https://openalex.org/W2047663485",
    "https://openalex.org/W3121116873",
    "https://openalex.org/W2156125289",
    "https://openalex.org/W3097720621",
    "https://openalex.org/W2272625812",
    "https://openalex.org/W3113410518",
    "https://openalex.org/W3117246385",
    "https://openalex.org/W3146280455",
    "https://openalex.org/W2767039451",
    "https://openalex.org/W2951912016",
    "https://openalex.org/W2949342052",
    "https://openalex.org/W2557595285",
    "https://openalex.org/W2917828716",
    "https://openalex.org/W2972411752",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W3194955130",
    "https://openalex.org/W4281790889",
    "https://openalex.org/W2102245393",
    "https://openalex.org/W6887746721",
    "https://openalex.org/W6945036920",
    "https://openalex.org/W2937378039",
    "https://openalex.org/W2609575245",
    "https://openalex.org/W2776525063",
    "https://openalex.org/W4234609530",
    "https://openalex.org/W2091359755",
    "https://openalex.org/W2028927099",
    "https://openalex.org/W2807900833",
    "https://openalex.org/W4306360519",
    "https://openalex.org/W2069790849",
    "https://openalex.org/W4233120011",
    "https://openalex.org/W2982552606",
    "https://openalex.org/W2074216064",
    "https://openalex.org/W4206950245",
    "https://openalex.org/W2951527505",
    "https://openalex.org/W2900834943",
    "https://openalex.org/W2161746138",
    "https://openalex.org/W3032993244",
    "https://openalex.org/W4225125006",
    "https://openalex.org/W2963691377",
    "https://openalex.org/W3128120098",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W2164025376",
    "https://openalex.org/W4307483975",
    "https://openalex.org/W4253308184",
    "https://openalex.org/W2970941190",
    "https://openalex.org/W2782893969",
    "https://openalex.org/W2885949257",
    "https://openalex.org/W2766352633"
  ],
  "abstract": null,
  "full_text": "Unbiased organism-agnostic and highly sensitive signal peptide\npredictor with deep protein language model\nJunbo Shen*1,4, Qinze Yu∗1, Shenyang Chen∗1,2,5, Qingxiong Tan1, Jingchen Li1, and Yu Li\n†1,2,3,6,7,8\n1Department of Computer Science and Engineering, CUHK, Hong Kong SAR, China\n2The CUHK Shenzhen Research Institute, Hi-Tech Park, Nanshan, Shenzhen, 518057, China\n3Shanghai Artificial Intelligence Laboratory, Shanghai, China\n4Department of Computer Science and Engineering, Washington University, St. Louis, MO 63130, United States\n5Georgia Institute of Technology, Atlanta, GA 30332, United States\n6Institute for Medical Engineering and Science, Massachusetts Institute of Technology, Cambridge, MA, USA\n7Wyss Institute for Biologically Inspired Engineering, Harvard University, Boston, MA, USA\n8Broad Institute of MIT and Harvard, Cambridge, MA, USA\nAbstract\nSignal peptide (SP) is a short peptide located in the N-terminus of proteins. It is essential to target and trans-\nfer transmembrane and secreted proteins to correct positions. Compared with traditional experimental methods\nto identify signal peptides, computational methods are faster and more efficient, which are more practical for an-\nalyzing thousands or even millions of protein sequences, especially for metagenomic data. Computational tools\nare recently proposed to classify signal peptides and predict cleavage site positions. However, most of them dis-\nregard the extreme data imbalance problem in these tasks. In addition, almost all these methods rely on additional\ngroup information of proteins to boost their performances, which, however, may not always be available. To ad-\ndress these issues, we present Unbiased Organism-agnostic Signal Peptide Network (USPNet), a signal peptide\nclassification and cleavage site prediction deep learning method that takes advantage of protein language models.\nWe propose to apply label distribution-aware margin loss to handle data imbalance problems and use evolution-\nary information of protein to enrich representation and overcome species information dependence. Extensive\nexperimental results demonstrate that our proposed method significantly outperforms all the previous methods\non classification performance by 10% on multiple criteria. Additional studies on the simulated proteome data\nand organism-agnostic experiments further indicate that our model is a more universal and robust tool without\ndependency on additional group information of proteins. Building on this, we design a whole signal-peptides-\ndiscovering pipeline to explore unprecedented signal peptides from metagenomic data. The proposed method\nis highly sensitive and reveals 347 predictions to be the candidate novel SPs, with the lowest sequence identity\nbetween our candidate peptides and the closest signal peptide in the training dataset at only 13%. Interestingly,\nthe TM-scores between candidates and SPs in the training set are mostly above 0.8. The further analysis of the\nexperimentally verified novel SPs provides evidence that although USPNet does not rely on any structure infor-\nmation as input, it detects SPs based on evolutionary and structural information instead of sequence-similarity.\nThe results showcase that USPNet has learned SP structure with just raw amino acid sequences and the large\nprotein language model, and thus enables the discovery of novel SPs that are distant from existing knowledge\neffectively.\n*Equal first authorship.\n†Corresponding Author. Email: liyu@cse.cuhk.edu.hk\n1\narXiv:2312.08987v1  [cs.AI]  14 Dec 2023\n1 Introduction\nA signal peptide (SP) is a short amino acid sequence working as a specific targeting signal to guide and transfer\nproteins into secretory pathways [1]. It has a three-domain structure: Positively charged N-region, hydrophobic\nH-region, and uncharged C-region [2]. The SPs function as specific segments to guide proteins to reach correct\npositions and then be cleaved by cleavage sites nearby their C region. Thus, the identification of signal peptides is\nvital for studying destinations and functions of proteins [3, 4, 5, 6].\nAs comprehensive experimental identification of SPs can be time- and resources-consuming, many compu-\ntational tools have been proposed to classify signal peptides and predict cleavage sites. The first attempt was\na formulating rule proposed in 1983 [7]. V on Heijne first applied a statistical method to unveil patterns nearby\ncleavage sites of signal peptides based on only 78 Eukaryotic proteins [7]. Furthermore, generative models, such\nas the hidden Markov model (HMM), were proposed to facilitate the recognition of signal peptides. These models\nfocused on analyzing these three functional regions (N-region, C-region, and H-region) in detail and were built by\ncapturing the relationships between different regions of signal peptide [8, 9, 10, 11, 12]. Different from generative\nmodels, some homology-based methods were proposed [13]. The predictions of these methods are based on the\nsimilarities between sequences in the existing knowledge base and the input sequences. In addition, they can\nachieve similar prediction performance as generative models.\nRecently, supervised models have made great progress in signal peptide recognition. The query sequences\nare encoded into embedding vectors and then fed into models to directly compute probabilities for each signal\npeptide type. Among these methods, machine-learning-based models play an important role in their remarkable\nperformances [14]. DeepSig applied deep convolutional neural networks (DCNNs) architecture to the recogni-\ntion of signal peptides and the prediction of cleavage site positions [15]. Furthermore, SignalP5.0 came up and\nbenchmarked all the previously proposed methods [16], and SignalP6.0 [17] is able to predict all 5 types of signal\npeptides that the previous model failed to detect. These methods achieved advanced performance in tasks, but\nmost of them suffered from extreme class imbalance and therefore performed poorly on data from minor classes\n[18, 19, 20]. In addition, these methods often depend heavily on additional information about groups of organisms\nto boost their performances. However, it is impractical to obtain sufficient group information from metagenomic\ndata in reality [21, 22]. A robust tool should only require amino acid sequences to yield accurate prediction results.\nIn signal peptide classification, the key problems to solve are the imbalance of training data and object de-\npendence on group information. Rao et al. [23] introduced a transformer-based language model, ESM-1b, which\ndemonstrated that information learned from large-scale protein sequences alone could implicitly encode func-\ntional and structural information and benefits various downstream tasks, such as secondary structure prediction\nand contact prediction, outperforming specific-data-trained model by a large margin. Rao et al. [24] also found\nthat integrating multiple sequence alignments (MSA) into the model, referred to as the MSA transformer, led to\nmore excellent performance. These protein language models [25, 26] gain improvement on problems with limited\nannotated data. Inspired by that, we thus propose the unbiased organism-agnostic signal peptide predictor (USP-\nNet) based on a BiLSTM [27] framework and protein language models to classify signal peptides and predict their\ncleavage site positions. We leverage an advanced MSA-based protein language model to enrich the representa-\ntions to aid in encoding group information of sequences. Moreover, we combine class-balance loss with Label\ndistribution-aware margin (LDAM) loss [28] as the loss function of USPNet to improve generalization. Our model\nis end-to-end, which takes just raw amino acids as inputs. It is efficient to perform the classification of all five\ntypes of signal peptides and the non-signal-peptide type protein. We compare our model with several task-related\ndeep learning models on the re-classified SignalP5.0 benchmark set. Notably, USPNet achieves more than 10%\nimprovement of MCC on multiple classes compared to the previous state-of-the-art methods. Besides, our model\nsignificantly outperforms SignalP6.0 on the recall rate of cleavage site prediction. On our curated domain-shift\nindependent set, USPNet also performs better than other models, showing the generalization of our method on\nthe classification of signal peptides. To further showcase the potency of USPNet, we collect proteome-wide data\nfrom Escherichia coli (strain K12) as well as other 7 organisms. When applying USPNet to detect signal peptides\nfrom proteome-wide data, it retrieves nearly all of them, which is among the best of all models being tested. In\naddition, we thoroughly assessed the group information dependency by conducting an organism-agnostic exper-\niment which removes group information in the input. USPNet’s well-trained encoders empowered with MSA\ninformation as well as protein language models capture rich evolution and functional information, allowing the\nmodel to remain robust despite the absence of group information. Such highly sensitive signal peptide prediction\ncapability enables novel SPs mining from large metagenomic resources. We thus build a complete pipeline from\nhandling metagenomic data to making novel signal peptide detection. We collect swine gut metagenomics data\nfrom multiple resources to carry out the case study and finally screen out 347 peptides from millions of sequences\nas the candidates that have low sequence identity with existing SPs and are likely to be novel signal peptides.\n2\nThe result indicates that USPNet is able to provide evolutionary and structural information of SPs and effectively\ndiscovered candidate signal peptides that are distant from existing knowledge.\nThe main contributions of our work are summarized as follows:\n• We introduce Unbiased Organism-agnostic Signal Peptide Network (USPNet), which is able to predict all\n5 known types of signal peptides. Extensive experiments demonstrate that the proposed method achieves\nstate-of-the-art performance over other signal peptide predictors on signal peptide classification. We ap-\nply USPNet to the independent set and proteome-wide studies. The model reaches 10% improvement on\nmultiple criteria compared with previous methods and keeps its performance above 90%.\n• We provide two versions of USPNet for usage. One constructs multi-sequence alignment (MSA) and uses\nan MSA transformer to generate embeddings to enrich our representations. The other utilizes evolutionary\nscale modeling (ESM) embeddings [23], which we named USPNet-fast. The first version of USPNet has\nbetter prediction ability, and USPNet-fast can make inferences 20 times faster. We facilitate users so that\nthey can choose the tool based on the application scenarios.\n• We resolve the extreme imbalance problem in signal peptide prediction. Considering that previous al-\ngorithms train models mainly based on cross-entropy loss, we propose to apply label-distribution-aware-\nmargin loss (LDAM) to improve the generalization of less frequent classes [28]. We present a modified loss\nfunction by combining class-balance loss with LDAM loss to make USPNet learn useful information from\nsmall classes.\n• We build a whole pipeline to detect signal peptides from original metagenomics data. We reveal 347 predic-\ntions to be the candidate novel SPs, with the lowest sequence identity between our candidate peptides and\nthe closest signal peptide in the training dataset at only 13%. Notably, the TM-scores between candidates\nand SPs in the training set are mostly above 0.8. We also retrieve all 4 experimentally verified signal pep-\ntides of our study genomes supported by literature and do not exist in the training dataset. Results show that\nUSPNet learns evolutionary and structural information without additional inputs from the protein folding\nmodel and hence could discover peptides that have low sequence identity but high structural similarity at an\nideal speed.\n2 Result\n2.1 USPNet is a pipeline for predicting signal peptides from metagenomics data\nAs shown in Figure 1.a, the pipeline can predict signal peptides from metagenomics data, and even discover\nnovel SP candidates. The basic architecture of our method is the Bi-LSTM [27, 29] with the self-attention mecha-\nnism [30], and we leverage protein language model-based encoder to enrich representations (Figure 1.b). USPNet\ntakes amino acid sequences as input and simultaneously predicts signal peptide types and the corresponding cleav-\nage sites. Considering the length of signal peptides in proteins N-terminus is usually between 5 and 30, we set 70\nas the cut-off to the length of proteins, which means that each input sequence contains at most 70 amino acids.\nThe sequence then goes to the feature extraction module, employed with an embedding layer. As the number of\ncommon residue types is 20, it converts the input sequence into an L×20-dimensional matrix, where L is the length\nof the sequence. Particularly, we add an L×4-dimensional vector at the head of the generated embedding to store\nthe group information (Eukaryotes, Gram-positive, Gram-negative bacteria, and Archaea). Then, the generated\nembedding is fed into our BiLSTM part. It consists of a Bi-LSTM layer with self-attention and a CNN, which\nsimultaneously extracts forward and backward directions of long-distance dependencies and global/local features\nof sequences. Following the Bi-LSTM module, we develop an MLP-based module to predict cleavage sites and\nSP types separately. To integrate more information in classifying signal peptides, we include MSA embeddings.\nTo be specific, we first generate Multi-sequence alignment (MSA) for each sequence and then input MSAs to\nthe pre-trained MSA Transformer model [24] to obtain embeddings from its final layer. And to bend over back-\nward to resolve the data imbalance, our loss function is designed by combining class-balance loss with Label\ndistribution-aware margin (LDAM) loss [28]. Detailed information is introduced in the Methods part.\nBesides the above-mentioned method, we also introduce another version of USPNet, called USPNet-fast,\nwhich replaces MSA embeddings with ESM-1b embeddings [23], and keeps other modules unchanged (Figure\n1.c). USPNet-fast is able to predict signal peptide and cleavage sites faster and without much performance degen-\neration.\n3\nIntegrated\nFeature \nM\nA\nP\nT\nL\nF\nQ\n…\nI\n···\nProtein \nSequences\nEmbedding \nLayer\nBiLSTM Block\nQuery Weights\nCNN Block\nCleavage Site Prediction\nEncoder\nlayers\nmean\nTransformer\nFeatures \nSignal Peptide Type Prediction\nProtein Language \nModel\nM\nD\nD\nI\nS\nG\nR\n…\nL\n…\nImbalanced Data \nDistribution\nEvolutionary \nDatabase\nLDAM Loss Function\nFeature Extraction Module BiLSTM-attention Module\nMSA\nAdded \nembedding\nBiLstm \nembedding\nCNN \nembedding\nConcatenated\nembedding\nAdded \nembedding\nPrediction\nHeads\n I\n I\n I\n I\n I\n I\n I\n…\nI\n S\n S\n S\n S\n S\n S\n S\n…\nS\n…\nPrediction\nHeads\nProtein Language ModelPrediction Module\na\nb\nMulti-head \nattention\nOptimize\nM\nA\nP\nT\n...\nY\nL\nG\nProtein sequences\nProtein Language\nModel\n(MSA-transformer)\nBi-LSTM Module\nSec/SPI\nSec/SPII\nSec/SPIII\nTat/SPI\nTat/SPII\nNO-SP\nPredicting\nC\nC\nT\nG\n...\nA\nC\nG\nMetagenomics data\nG\nG\nA\nG\n...\nC\nC\nA\n...\nORF \nPrediction\nM\nK\nK\nI\n...\nA\nS\nA\n... Filtering\nPart of Novel Signal Peptide Candidates\n(Most candidates have low sequence identity \nbut high structural similarity with existing SPs)\nUSPNet\nEncoder\nlayers\nmean\nTransformer\nFeatures \nProtein Language \nModel\nEvolutionary \nDatabase\nESM-1b (No MSA)\nUSPNet-fast\n(More than 20 times faster)\nProtein Language\nModel\n(MSA-transformer)\nChange to \nProtein Language\nModel\n(ESM-1b)\nc\nProtein ID (NCBI) Predicted SP Count\nWP_008666378.1 MNKKFLSAILFGALMVSSTGT 179\nWP_008661887.1 MNKKFLSVILFSALMVGTAGT 102\nABG29383.1 MKKTLVSALTTALVVGAASTTFA 61\nWP_009145071.1 MKKSLVLAMAMALGVTASA 49\nWP_005825009.1 MNLNFRRTALLVGICSAVSLTYTPQLFA 32\nWP_006847594.1 MRRLTLLLVSLVLLSIQSVLA 32\nWP_006846852.1 MKHLKQFLLMAMLLFTLAPLQVSA 30\nWP_006848777.1 MKKIAILSLSLTLAAAAQA 28\nWP_006846306.1 MYQQLKRASMALTLSSVCFLAFA 27\nWP_005844474.1 MIKKLYLPLVALLVLALSS 26\nEvidence\nECO:\n0000256 \nAutomatic \nassertion \naccording to \nsequence \nanalysis. \nSAM:\nSignalP\nFigure 1: USPNet workflow for predicting signal peptide (SP) and cleavage site. a. The pipeline of USPNet for\nsignal peptide discovery from metagenomics data. USPNet takes raw amino acid sequences as input and uses BiL-\nSTM module as well as Protein Language Model to get concatenated embeddings as input for prediction. Here we\nlist the top 10 most frequently occurring peptides that are automatically annotated as SPs by the UniProt database\nout of 347 candidates. b. Detailed architecture of USPNet. The training data is imbalanced. The protein sequences\ngo through the feature extraction module and are then passed to the BiLSTM module, which includes a Bi-LSTM\nlayer with self-attention and a CNN for extracting long-distance dependencies and features of the sequences. For\nSP type prediction, USPNet incorporates MSA embeddings generated by a pre-trained MSA Transformer model.\nSubsequent MLP-based modules predict cleavage sites and signal peptide types. Label Distribution-Aware Margin\n(LDAM) loss is employed in training to address data imbalance. c. USPNet-fast replaces the MSA-transformer\nwith ESM-1b, which does not require MSA, and thus enables a much faster inference speed.\n2.2 USPNet outperforms the previous methods on the benchmark dataset\nSignal peptide type prediction\nUSPNet is able to predict the type and cleavage site of a signal peptide at the same time. To fair analyze\nperformance, we train and test our model on the re-classified, extended, and homology-reduced datasets derived\nfrom the data published with SignalP5.0 and SignalP6.0[16, 17]. The combination of training and benchmark\ndata is identical to the homology partitioned SignalP6.0 dataset. For signal peptide type prediction, the training\ndata, with benchmark data excluded from the dataset, contains 13679 sequences and can be considered into six\nparts according to six different kinds of labels. However, the data is extremely imbalanced. As shown in Table\n1, the sequences with major-class labels are ten times more than those with minor-class labels. Therefore, to\nmitigate this bias and achieve a fair assessment of performance, we focus on the Matthews Correlation Coefficient\n(MCC) over different divisions of sequences that belong to the less frequent classes. We use two versions of MCC\nas measurements, namely MCC1, which contrasts specific signal peptides against TM/Globular (NO-SP) type\nproteins, and MCC2, which additionally includes all remaining sequences in the negative set.\n4\na c\nd\nSignalP6.0\nDEEPSIG\nLipoP\nPRED-LIPO\nPRED-SIGNAL\nPRED-TAT\nTOPCONS2\nTATFIND\nTatLipo\nUSPNet-fast\nUSPNet\nArchaea Sec/SPI\nEukarya Sec/SPI\nNegative Sec/SPI\nPositive Sec/SPI\nArchaea Sec/SPII\nNegative Sec/SPII\nPositive Sec/SPII\nArchaea Tat/SPI\nNegative Tat/SPI\nPositive Tat/SPI\nArchaea Tat/SPII\nNegative Tat/SPII\nPositive Tat/SPII\nMCC1 MCC2\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nArchaea Sec/SPI\nEukarya Sec/SPI\nNegative Sec/SPI\nPositive Sec/SPI\nArchaea Sec/SPII\nNegative Sec/SPII\nPositive Sec/SPII\nArchaea Tat/SPI\nNegative Tat/SPI\nPositive Tat/SPI\nPrecision Recall\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nUSPNet\nSignalP6.0Signal\nP5.0\n(re-trained)\nUSPNet\nUSPNet\nUSPNet-fast\nSignal\nP6.0\nUSPNet\nUSPNet-fast\nSignal\nP6.0\nSignalP6.0Signal\nP5.0\n(re-trained)\n0.4 0.6 0.8 1.0\nMCC1, Archaea\nArchaea Sec/SPI (61.0%)\nArchaea Sec/SPII (15.3%) Archaea Tat/SPI (15.3%)\nArchaea Tat/SPII (8.5%)\nMCC1, Negative\nNegative Sec/SPI (16.3%)\nNegative Sec/SPII (68.7%)\nNegative Tat/SPI (13.6%)\nNegative Tat/SPII (1.3%)\nMCC1, Positive\nPositive Sec/SPI (9.6%)\nPositive Sec/SPII (76.9%)\nPositive Tat/SPI (11.5%)\nPositive Tat/SPII (1.9%)\n11-20\n21-30\n31-40\n41-50 >50\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0\n100\n200\n300\n400\nBenchmark set (SP)\nLength\nMCC\nCounts\nRunning time comparison\nUSPNet Inference\nGenerating MSA\nUSPNet-fast Inference\nUSPNet\nUSPNet-fast\ne\nCorrect Incorrect\nTat/SPI Postive and Negative\nBenchmark set\n10 10\n2\n10\n3\n10\n4\n100\n101\n102\n103\n104\n105\n106\nNumber of sequences\nTime (s)\nCorrect Incorrect0\n30\n60\n90\n120\n150Neff\nIndependent t-test p = 0.002\n0\n30\n60\n90\n120\n150Neff\nIndependent t-test p = 0.021\n30% 40% 50% 60% 70% 80% 90%\n0.2\n0.4\n0.6\n0.8\n1.0\nSimilarity against training data\nMCC\nBenchmark set\nb\nFigure 2: USPNet demonstrates robust performance across different signal peptide types and organism\ngroups. a . Length distributions with the performance of signal peptides in the benchmark set. Most SPs are\nbelow 50 AAs in length, and all models have degradation in performance when SPs are longer than 30 AAs.\nb. The performance under different sequence identities between the benchmark set and training set. c. Signal\npeptide prediction running time comparison between USPNet and USPNet-fast. The MSA generation step is time-\nconsuming, accounting for roughly 95% of the total processing time.d. Radar charts of USPNet and other models\non different perspectives of performance. We compare MCC1 of different models from the viewpoint of organism\ngroups with proportional information of each SP type. USPNet behaves as the most powerful signal peptide\npredictor in every organism group. e. Comparison between USPNet, USPNet-fast, SignalP6.0, and SignalP5.0\non both signal peptide type prediction and signal peptide cleavage site prediction. And Neff scores of MSA on\nTat/SPI of Gram+ and Gram-, and benchmark set. (Independent t-test: two independent samples t-test).\nWe compare signal peptide classification performance over different organism groups of several known signal\npeptide prediction models on the benchmark dataset. In total, 16 methods are selected for comparison; among\nthem, SignalP6.0 and SignalP5.0-retrained are trained on the same training dataset as ours. Results for other\nmethods are obtained directly from their publicly available web servers, which leads to potential performance\noverestimation due to the lack of homology partitioning. As the task at hand involves multi-class classification,\n5\nTable 1: Statistics for the composition of the full dataset adopted in this study, with the benchmark dataset numbers\nin parentheses. SP denotes Sec/SPI, L denotes Sec/SPII, P denotes Sec/SPIII, T denotes Tat/SPI, TL denotes\nTat/SPII, and N/C denotes TM/Globular(NO-SP) here.\nDataset Organism SP L P T TL N/C Total\nSP type prediction\nEukaryotes 2040 (146) - - - - 14356 (5581) 16396 (5727)\nGram-positive 142 (15) 516 (120) 4 (0) 39 (18) 8 (3) 226 (81) 935 (237)\nGram-negative 356 (61) 1087 (257) 56 (0) 313 (51) 19 (5) 933 (133) 2764 (507)\nArchaea 44 (36) 12 (9) 10 (0) 13 (9) 6 (5) 110 (81) 195 (140)\nwe dissect the results based on the types of signal peptides. For the data with Sec/SPI labels, our model outper-\nforms other methods on nearly all the organisms and metrics. The only exception is PRED-SIGNAL, specifically\ndesigned to detect only Sec/SPI SPs, which marginally outperforms us on MCC1 of Archaea. Nonetheless, our\nmodel still demonstrates superior performance on MCC2 (Figure 2.d and Supplementary Figure 3). When con-\nsidering Sec/SPII SP classification, only 5 methods demonstrate capability, with corresponding data available for\nthree organism groups. USPNet unequivocally has the best performance across all the metrics. For the MCC1 of\nArchaea, we exceed others by at least 6%. Moreover, even our USPNet-fast does better than all other competitors.\nWhen looking at the performance from the perspectives of SP length and sequence identity, we find that\ndetection performances for short SPs are good, and all methods have degradation when SPs are longer than 30 AAs\n(Figure 2.a). And when sequence identities are lower than 40%, USPNet has minimal performance degradation\n(Figure 2.b), showing better generalization to proteins distant from the training data. It is obvious that USPNet\nhas made an impressive promotion in the prediction performances, especially over the minor classes.\nWe also conduct a head-to-head comparison of MCC1 and MCC2 between USPNet and SignalP6.0, as shown\nin Figure 2.e. USPNet achieves significant improvement on most SP types. Especially for data with Sec/SPII\nlabels, we observe an increase of 10.0% across all 4 groups. And on Tat/SPII, the class with the least data, USPNet\nis able to retrieve nearly all SPs. Despite SignalP6.0 demonstrating commendable performance, it falls short when\ncompared with USPNet. On Tat/SPI of Gram-positive and Gram-negative, however, USPNet is behind USPNet-\nfast and SignalP6.0. This can primarily be attributed to the subpar quality of multiple sequence alignments (MSA)\nin incorrect predictions. We calculate the number of effective sequences (Neff) of correct predictions and incorrect\npredictions used by the MSA-transformer in our method(Figure 2.e). Specifically, for Tat/SPI, there exists a\nsignificant Neff gap between correct and incorrect predictions: the median of the former is around 90, while\nthe latter is only 4. Across the entire benchmark set, the MSA quality of correctly predicted SPs is also better,\nsuggesting that high-quality MSA generation could boost the performance of USPNet. Overall, the results on\nthe benchmark set accord with our assumption that the margin-based loss formulation allows the classification\nboundary of rare classes to be extended further and avoid overfitting in some ways. With better generalization,\nUSPNet is an unbiased multi-class SP predictor that is able to predict all 5 kinds of SPs. Besides the benchmark\nset, we also conduct 5-fold cross-validation on the full training and benchmark sets (Supplementary Table 5-9).\nSince homology partitioning is inoperative in cross-validation, USPNet showcased more impressive performance,\nespecially for the minor classes such as Tat/SPII and Sec/SPIII signal peptides.\nSignal peptide cleavage site prediction\nGenerally, the key focus of USPNet is to produce a predictor to classify various signal peptide types with a\nbetter generalization of different classes. However, before categorizing SPs, finding out the precise cleavage site\nof a protein is also an essential step in a real-world application. Here in USPNet, we utilize the attention weights\nof the context attention matrix to acquire information related to per amino acid, subsequently informing cleavage\nsite decisions. For comparison, we employ both precision and recall rates to evaluate USPNet’s performances on\ncleavage site prediction against SignalP6.0 and SignalP5.0-retrained, and the result can be observed from Figure\n2.e. We notice that the overall performances on cleavage site predictions of USPNet are comparable to SignalP6.0.\nIn especial, our model significantly outperforms the other 2 models on recall, which indicates that we are better\nat finding as many signal peptides in organisms as possible, especially those belonging to elusive minor classes.\nHowever, our model’s precision is slightly lower than that of SignalP6.0 in certain classes, potentially due to the\namplified weight allotted to minor classes, leading to a potential increase in false positives. Nevertheless, users\ncan adjust weights according to their specific needs, enhancing USPNet’s effectiveness and suitability for their\napplication scenarios.\n6\nUSPNet versus USPNet-fast\nIn the study, we introduce two versions of USPNet for use. They are USPNet and USPNet-fast. The rationale\nbehind developing USPNet-fast is to provide a quicker prediction option, recognizing that the multiple sequence\nalignment (MSA) steps can be particularly time-consuming. Inspired by this need, we incorporate ESM-1b to\nreplace MSA-transformer and expedite prediction while retaining other modules (Figure 1.c). ESM-1b encodes\nproperties of protein sequences across different scales. As confirmed in previous research, the model can learn\nstructures of amino acids, protein sequences, and evolutionary homology solely from sequence data, without\nadditional biological signals [23]. To measure the degree of speed increase, we randomly select some sequences to\ntest the prediction time of both USPNet and USPNet-fast (Figure 2.c). It shows the average inference time for the\ntwo methods is roughly the same. However, generating MSA is a demanding task, accounting for approximately\n95% of the total processing time. USPNet-fast, by avoiding the need for MSA files and working directly with\namino acid sequences, is significantly faster. We evaluate the performance of USPNet-fast on the benchmark\nset and compare it with USPNet (Figure 2.e). On most categories of signal peptides, USPNet-fast is slightly\nbehind USPNet, but the gap is modest. Notably, for Tat/SPI of both the Gram-positive and Gram-negative groups,\nUSPNet-fast outperforms the standard USPNet by approximately 2% and 7%, respectively. Hence, when the\nMSA quality is not satisfying, applying USPNet-fast instead of USPNet is preferable. Generally, the fast version\noutperforms most previous methods (Figure 2.d) and offers substantial speed benefits.\n2.3 Protein language models and LDAM loss improve the generalization of USPNet\nIn this part, we will discuss the performances of models with different loss functions and embeddings to look\ndeeply into the reasons why USPNet is universal. We will compare LDAM loss with other commonly-used loss\nfunctions and analyze the performance differences between models with and without MSA and ESM embeddings.\nNotably, we employ overall MCC, Kappa, and balanced accuracy for evaluation instead of using MCC over\ndifferent classes and organism groups. Because performance across minor classes for some models is remarkably\nsimilar, and these metrics can make overall performance more straightforward and intuitive to display differences.\nBefore delving into the experiment results, to verify the efficiency of protein language models sequence em-\nbeddings, we perform visual processing of the embedding vectors on our benchmark data. We extract embeddings\nfrom 4 models and apply UMAP [31] for data dimensionality reduction. UMAP uncovers similarity patterns in\ndata points with multiple features, allowing us to observe the convergence of data points with high similarity\ninto clusters when the dimensionality-reduced results are projected onto a two-dimensional plane. For USPNet,\nwe extract embeddings from the last embedding layer, the result is shown in Figure 3.a. We can observe some\nobviously differentiated clusters, and each cluster mainly consists of one type of data, which indicates USPNet\nnicely mines the functional features inside the peptide sequences and is efficient in classifying different types of\nsignal peptides. For the two protein language models applied in our study to build USPNet and USPNet-fast,\nwe take their output directly for visualization. As these two language models are trained on large-scale protein\ndata via self-supervised learning, they have learned the structural and functional information of universal protein\nsequences. And results show that the models also encode the functional information of signal peptides. At last, to\nevaluate the help of protein language models from another perspective, we train a model that removes the embed-\ndings of protein language models and remains all other modules, which we call Bi-LSTM. Although Bi-LSTM is\ntrained on the same data as USPNet, it cannot splendidly discover the pattern of signal peptides and therefore does\nnot encode much valuable information into its embeddings. We then evaluate the efficiency of different modules\nin USPNet by concrete measurable metrics.\nWe first trained the USPNet model with three loss functions: cross-entropy function, focal loss, and LDAM\nloss [32]. The other two introduced here are for better comparability to display the advantages of LDAM loss. The\nhyperparameters of loss functions are chosen manually according to multiple runs. In Figure 3.b, by analyzing\nthe results of experiments, it is clear that LDAM loss allows for maximum gains in improving the accuracy of\npredictions. With the same model architecture and ESM embedding, the LDAM ESM model trained based on\nLDAM loss reaches the MCC of 0.93, which is 1.8% higher than the Focal loss and 2.6% higher than cross-\nentropy loss, respectively. In addition, we try to apply the deferred re-balancing training procedure proposed with\nthe LDAM Loss (LDAM-RW). However, when training the signal peptide predictor, the LDAM-RW algorithm\ncannot help gain benefits. Instead, we propose to directly apply the combination of LDAM Loss and class-balanced\nloss[33], called CB-LDAM loss. Except for baseline loss functions, we used focal loss and cross-entropy loss with\nreweighting for comparison. We manually give different weights according to the frequency of different signal\npeptide labels to enhance generalization. And these manual reweighting factors are acquired based on the results of\nmultiple runs. Results show that class-balanced reweighting from the start will help models make further progress.\n7\nUSPNet embeddings projection MSA embeddings projection\nNO_SP\nSP\nLIPO\nTAT\nTATLIPO\nPILIN\nESM-1b embeddings projection Bi-LSTM embeddings projection\nROC Curves for USPNet ROC Curves for Bi-LSTM\nPrecision\nPR curves for USPNet\nRecall\nPR curves for Bi-LSTM\na\nb c\n0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\n0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSec/SPI (AUC = 0.04)\nSec/SPII (AUC = 0.98)\nTat/SPI (AUC = 0.94)\nTat/SPII (AUC = 0.67)\nRecall\nFalse Positive Rate\nMCC Kappa Balanced Accuracy0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nPerformance\nPerformance of Models Trained with Different Loss Functions and Embeddings\nLDAM_MSA\nLDAM_RW_ESM\nLDAM_RW\nLDAM_ESM\nLDAM\nFocal_RW_ESM\nFocal_RW\nFocal_ESM\nFocal\nCE_RW_ESM\nCE_RW\nCE_ESM\nCE\nWith all group information\n Without group information\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nMCC\nPerformance on benchmark dataset with/without group information\nArchaea Sec/SPIArchaea Sec/SPIIArchaea\n Tat/SPI\nArchaea\n Tat/SPII\nEukaryotes Sec/SPINegative Sec/SPINegative Sec/SPIINegative\n Tat/SPI\nNegative\n Tat/SPII\nPositive Sec/SPIPositive Sec/SPIIPositive \nTat/SPI\nPositive \nTat/SPII\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0MCC\nPerformance on benchmark dataset without group information\nUSPNet\nSignalP6.0\nSignalP5.0\nSec/SPI (AUC = 0.82)\nSec/SPII (AUC = 0.97)\nTat/SPI (AUC = 0.94)\nTat/SPII (AUC = 0.86)\nSec/SPI (AUC = 0.51)\nSec/SPII (AUC = 0.99)\nTat/SPI (AUC = 0.99)\nTat/SPII (AUC = 0.99)\nSec/SPI (AUC = 0.93)\nSec/SPII (AUC = 0.99)\nTat/SPI (AUC = 0.97)\nTat/SPII (AUC = 0.99)\nd\nFigure 3: Embedding and ablation study performance analysis of USPNet compared to alternative models.\na. 2-D UMAP projection of different embeddings on benchmark data. USPNet embeddings show the same types\nof peptides cluster together. Bi-LSTM embeddings are the last layer output of the Bi-LSTM model. Even if it\nis trained on SP data, we cannot see clear clusters of different SP types. Both protein language models (MSA-\ntransformer and ESM-1b) form clusters between different SP types, indicating that they encode some functional\ninformation related to signal peptides. b. Ablation study performance of USPNet: MCC, Kappa, and Balanced\nAccuracy of models trained with different loss functions and embeddings. c. ROC curves and Precision-Recall\ncurves of USPNet and Bi-LSTM on different types of signal peptides prediction. Our model is stable across all 4\nkinds of SPs, while Bi-LSTM suffers from the data imbalance problem. d. Comparison of USPNet’s organism-\nagnostic performance against SignalP6.0 and SignalP5.0 on the benchmark dataset, when organism group infor-\nmation is missing, showcasing comparable performance to SignalP6.0 without obvious performance degradation\nand significant outperformance compared to SignalP5.0 across all categories.\nFinally, to evaluate the functionality of protein language model embeddings, we add one, two, and three linear\nlayers, respectively, to integrate information from original ESM or MSA embeddings. The results of experiments\nsuggest that we get the best results using two layers. And the reason for the decline in performance with more\nlayers may be overfitting. It is clear that the model can acquire continuous improvement by integrating information\nfrom state-of-the-art embeddings. Compared with the ESM embedding, the MSA embedding further improves\nthe overall performance of the model. Besides, we also remove the MSA or ESM embeddings to evaluate the\nperformance of Bi-LSTM. Specifically, we take AUCROC and AUCPR to make the comparison (Figure 3.c).\n8\nUSPNet behaves steadily on all types of signal peptides and keeps the value of AUROC above 0.9 and AUPRC\nabove 0.8. Interestingly, the lowest performance is seen in class Sec/SPI, which indicates USPNet’s excellent\nperformance for minor classes. But when we look into the Bi-LSTM, it shows that the performance is extremely\npoor on type Sec/SPI. One reason is that we set the weight of the minor classes at a high level, making the model\ninclined to predict the signal peptides as minor classes. However, if we do not set the weights, the model can\nsuffer from data imbalance problem [29], compromising its ability to identify these minor classes. In summary,\nthe protein language models boost the generalization, while our loss function strategically shifts the model’s focus\ntowards minor classes. Working in unison, these factors enable USPNet to classify signal peptides precisely and\nresolve the data imbalance.\n2.4 USPNet is robust against the organism-agnostic experiments\nThe previous results indicate USPNet is able to make predictions that outperform other previous models in\nclassifying signal peptide types, especially in the minor classes. To further tap the group-information-independent\ncapacity of USPNet, in this part, we carry out organism-agnostic classification.\nMost previous proposed signal peptide prediction tools, including SignalP5.0 [16] and TargetP2.0 [29] rely on\nextra group information of sequences to enrich embeddings and boost their performance. Typically, this involves\nincorporating a one-hot embedding vector with a specific number of dimensions corresponding to group origins\ninto the model. However, in the context of metagenomic research and various other applications, obtaining group\ninformation for all sequences is not practical. It is thus essential to develop methods capable of accurately pre-\ndicting signal peptide types using solely amino acid sequences. In the following experiment, we still utilize the\nbenchmark dataset, but remove the organism group information. Specifically, group information is missing in the\ninput for USPNet. We also include the retrained SignalP5.0 for comparison since SignalP6.0 does not rely on\ngroup information [17]. For SignalP5.0, we input randomized group information instead because it does not sup-\nport blank input of group information. The performance is assessed using the Matthews Correlation Coefficient\n(MCC2).\nAs depicted in Figure 3.d, remarkably, USPNet’s performance, in general, remains high and exhibits no signif-\nicant degradation in signal peptide detection without group information. A comparison of the overall performance\n(multi-class MCC) between the three models reveals that USPNet still behaves the best when missing group in-\nformation. This demonstrates the resilience and adaptability of USPNet in scenarios where group information is\nunavailable.\nConversely, SignalP5.0 experiences a substantial decline across all categories when group information is ab-\nsent. Specifically, SignalP5.0 displays a notable decrease of 0.155 in its multi-class MCC when without group\ninformation. The signal peptides of minor classes concentrate on Archaea, Gram-Negative, and Gram-Positive\ngroups, which suffer the most when group information is missing.\nOne reason for the robust performance of USPNet is that protein language models [23, 24] applied in USPNet\nare able to encode protein sequences into a high-level feature space and represent biological variation. The two\nmodels take advantage of enormous protein data and learn structural/functional properties as well as evolutionary\ninformation. The representations will possibly substitute for the formerly mentioned group embedding. With\nour manipulation that concatenates MSA/Evolutionary Scale Modeling (ESM) embeddings with the Bi-LSTM\nembeddings, USPNet is turned into an organism-agnostic model to handle the universal database reliably. What’s\nmore, USPNet has the potential to serve as an end-to-end model to directly get results based on residue-level\nsequences from unknown origins, which can be beneficial for typical cases of metagenomic data.\n2.5 USPNet has impressive generalization ability and works well on domain-shifted data\nPerformance of USPNet on the rigorous non-retrospective independent test set\nUSPNet performs well on the benchmark set and does not rely on group information. However, sound perfor-\nmance may be expected on account that it is done in the same domain as the training set. In order to verify our\nmodel’s robustness in a domain-shift dataset, we first curate an independent test set, called SP22, with stringent\ncriteria to ensure its differentiation from our training dataset (40% sequence similarity, published after November\n2020, i.e., the date of benchmark dataset collection, and so on, see Methods). SP22 comprises 43 protein sequences\nfrom 39 species, which are largely absent from the training set, with each sequence containing a signal peptide.\nAmong them, 31 are from Eukaryotes group, 6 are from Gram-negative group, and 6 are from Gram-positive\ngroup.\n9\nSec/SPI Sec/SPII Tat/SPI\nPredicted Label\nSec/SPISec/SPIITat/SPI\nTrue Label\n123\n98.4%\n0\n0.0%\n0\n0.0%\n2\n1.6%\n6\n100.0%\n0\n0.0%\n0\n0.0%\n0\n0.0%\n10\n100.0%\nEscherichia coli (strain K12)\n84%\n5%\n10%\nNO_SP/NO_SP\nNO_SP/SP\nSP/SP correct type\nSP/SP incorrect type\nSP/NO_SP\na b\nc d\nEukaryotes Gram-Negative Gram-Positive4\n5\n6\n~\n26\n27\n28\nNumber of correct predictions\nPerformance of different models on SP22 dataset\nSignalP5.0\nSignalP6.0\nUSPNet\n0 1000 2000 3000 4000 5000\nProtein Sequence Length\nUSPNet\nSignalP6.0\nSP22 Dataset Prediction Results\nCorrectly Identified Incorrectly Identified\nPrediction results of complete proteome-wide data of \nEscherichia coli (strain K12) (totally 4403 sequences)\nO05512\nP0AD\n59\nP7720\n2\nP0AFM2\nP28307\nP0ADA1\nP0AET2\nO34313\nP08331\nP69739\nP69741\nP0A8X2\nP23857\nP76172\nP36560\nP77754\nP0ABK9\nO31803\nP0A901\nP46922\nP0AFL3\nP3868\n3\nP5450\n7\nP0702\n4\nP0AF\nK9\nP31133\nP2384\n3\nP7734\n8\nP5417\n8\nP0AG82\nP9674\n0\nP0A8\n55\nP3984\n8\nP3738\n7\nP9452\n2\nP0AB\nL3\nP0AF\nH8\nP2382\n7\nP0AA\nA9\nP3732\n9\nP3767\n6\nQ5SK82\nP0292\n4\nP6131\n6\nP0697\n1\nP3665\n5\nP0A9\n15\nP2506\n2\nP3984\n4\nP7721\n4\nP0AE\nG4\nP1940\n5\nP1940\n6\nP0481\n6\nP0AD\n96\nP3795\n7\nQ79F14P0C0V\n0\nP3909\n9\nP3982\n4 17054Q 03103P P3312\n9\nQ021\n13\nP7634\n1\nP7634\n4\nQ021\n14\nP0710\n2\nP0A9\n21\nP3918\n7\nP0495\n7\nP7604\n5\nP4105\n2\nP0AA\nX8\nP3917\n6\nP1992\n6\nP0AGD1\nP1731\n5\nP2571\n8\nO31737\nP1303\n6\nP0293\n0\nP6453\n4\nP0AD\nB1\nP0A\nEE5\nO31526\nP391\n16\nP0AE\nS9\nP3321\n9\nP0294\n3\nP1038\n4\nO05391\nP0AD\nV1\nP0292\n5\nQ9RUE8\nP1042\n3\nP5448\n4\nP0A9\n35\nP0612\n9\nP9672\n9\nP1010\n0\nP13482\nP69776\nP16869\nP0A940\nP31554\nP0AB24\nP0AGC3\nP04189\nP54423\nP0AG78\nP16700\nP0AEG6\nP0C0T5\nP08506\nP0AEB2\nP08750\nP35150\nP0AFI5\nP0AEU7\nQ47706\nP4978\n2\nP2133\n8\nP6968\n1P0A9\n10\nP2422\n8P0AB\nZ6P1895\n6P3702\n8O34385\nP3271\n7P0AE\nX9P3932\n5P7652\n0P0AE\n22P7726\n9P3917\n2P4552\n3\nP0955\n1\nP0AE\nU0\nP3085\n9\nP3086\n0\nP0AE\nM9\nP3790\n2\nP0AE\nQ3\nP5432\n7\nQ9RU24\nQ8NNK2P0080\n5\nP421\n11\nP9435\n6\nP0939\n4\nP4688\n3\nP3764\n8\nO07532P5442\n1\nO07921O34669\n19180P\n08193P\nP4225\n1\nQ5SME\n3\nP0AD\nU5\nP1047\n5\nQ8U0A4\nP0081\n1\nP2384\n7\nP0AD\n70\nP3393\n7\nP3758\n0\nP5494\n1\nP3313\n6\nP4012\n0\nP0916\n9\nP3421\n0\nP0699\n6\nP7774\n7\nP0293\n1\nP0A9\n08P7677\n3\nP3155\n0\nP7634\n2P3169\n7Q58232\nP2664\n8P3664\n9D4GPK6\nP3322\n5P4692\n3P1877\n5P0A9\n17O34344\nP0AD\nE4\nTree scale: 1\nPrediction results\nMiss by both models\nMiss by SignalP6.0\nHit by both models\nOrganism group\nGram-negative bacteria\nGram-positive bacteria\nArchaea\nOrganism\nEscherichia coli\nBacillus subtilis\nHaloferax volcanii\nDeinococcus radiodurans\nCorynebacterium glutamicum\nMethanocaldococcus jannaschii\nPyrococcus furiosus\nThermus thermophilus\nLength of SPs\n0-10\n11-20\n21-30\n31-40\n41-50\n>50\ne\nFigure 4: Performance of USPNet on domain-shift data. a . The rigorous non-retrospective SP22 indepen-\ndent dataset performances of different models on sequences from Eukaryotes, Gram-Negative, and Gram-Positive\ngroups. b. Scatter plot of the prediction of USPNet and SignalP6.0 on the SP22 dataset. USPNet can retrieve the\nsignal peptides in both very short and long proteins. c. Escherichia coli (strain K12) proteome-wide prediction\nperformance of USPNet. It accurately classifies almost all proteins. d. The UniProt labels and USPNet prediction\nresults’ matching conditions on the complete proteome data of Escherichia coli (strain K12). e. Cladogram of\nspecies tree of all the 193 reference proteomes with experimentally verified SPs. The annotation rings from inner\nto outer are: 1) prediction results of USPNet and SignalP6.0; 2) organism group of proteins; 3) detailed organism\nof proteins; 4) length of signal peptides.\n10\nWe analyze the performances of USPNet as well as SignalP6.0 and SignalP5.0 on the SP22 dataset to see if our\nmodel works well on domain-shift data. We collect the results of SignalP6.0 and SignalP5.0 by feeding sequences\ninto their web servers. USPNet correctly identifies 28 Eukaryotes SP sequences, which is better than SignalP6.0\nand SignalP5.0, as Figure 4.a shows. For groups of Gram-negative and Gram-positive, USPNet correctly identifies\nall 6 Gram-negative SP sequences and 6 Gram-positive SP sequences, while both SignalP6.0 and SignalP5.0\nmispredict one SP sequence for each organism group. We then analyze the misclassified protein sequence made\nby the two models (Figure 4.b) and find that USPNet can correctly predict the SP type of two particularly long\nprotein sequences, one with a length of 5206 in the Gram-negative group and one with a length of 2178 in the\nGram-positive group, while other models can not. Additionally, USPNet can classify the shortest sequence in\nthe SP22 dataset, with a length of 36 from the Eukaryotes group. The results demonstrate that USPNet is able\nto handle proteins with various lengths, while previous methods have exhibited limitations in this aspect. The\nperformance on the SP22 further unveils the advantages of USPNet in learning features of data from various\nsources with minor-class labels and the ability to make unbiased predictions.\nUSPNet retrieves most SPs on proteome-wide data\nBesides the independent test, we want to evaluate if USPNet is effective with the case study for species.\nConsequently, we examine the proteome-wide prediction performance of USPNet in Escherichia coli (strain K12)\nas well as other 7 organisms and compare it with the benchmark model, SignalP6.0. The data we selected comprise\nthe entire set of proteins that can be expressed by an organism. To ensure accuracy in subsequent classification,\nwe demand high-resolution cleavage site prediction. Despite the variability in protein lengths, our restricted input\nmakes USPNet focus on analyzing the N-terminus of a sequence and therefore alleviates the influence of long\nproteins. Moreover, the attention mechanism enables the holistic analysis of the input sequences and thus makes\nUSPNet performs meticulous detection.\nWe further assess the proteome-wide performance using the well-annotated proteome of Escherichia coli\n(strain K12) (Figure 4.c). Out of the 141 experimentally verified SPs, 125 are Sec/SPI, 6 are Sec/SPII, and 10 are\nTat/SPI SPs. USPNet successfully detects all 141 experimentally verified SPs. Out of them, USPNet accurately\npredicts 123 out of 125 SPs as Sec/SPI type and precisely predicts all 16 SPs from Sec/SPII and Tat/SPI type. As\none of the most advanced methods, SignalP6.0 also predicts all SPs from strain K12.\nBesides the experimentally verified signal peptides, we also apply USPNet to the complete proteome-wide\ndata of Escherichia coli (strain K12). Specifically, we use the reference proteome with Proteome ID UP000000625\n[34], which includes the complete genome sequence of Escherichia coli K-12, as the dataset for our study. The\ndataset contains 4403 protein sequences. Among them, 3907 are annotated as NO-SP and 496 are annotated as SP\nby UniProt. USPNet predicts 3734 proteins as NO-SP and 669 as SP. The detailed matching information is shown\nin Figure 4.d, before ‘/’ is UniProt’s annotation, and after ‘/’ is USPNet’s prediction. ‘SP/SP correct type’ means\nthe type of predicted SP is the same as the annotation, while ‘SP/SP incorrect type’ means the predicted SP type\nand annotation are different. Notably, most of the prediction results are matched with the annotations, and only\naround 6% are inconsistent. However, it is difficult to verify the correctness of the result because some annotations\nin UniProt are automatic annotations provided by existing prediction tools. In other words, some of them are not\nexperimentally verified. Nonetheless, USPNet’s genome-scale predictions yield estimates of plausible results.\nFor other reference proteomes from 7 different organisms, the UniProt database reports 52 experimentally\nverified SPs. USPNet shows a strong performance that seeks out 51 experimentally verified SPs, with only one\nmiss prediction in Bacillus subtilis. However, in comparison, SignalP6.0 makes 3 false predictions for experi-\nmentally verified SPs from Bacillus subtilis. We then reconstructed the phylogeny of all proteins and obtained\nthe unrooted tree, which is annotated with the prediction results (Figure 4.e). The incorrect predictions made by\nboth models are all proteins from Gram-positive bacteria. The result shows the effectiveness of USPNet in find-\ning out proteome-wide signal peptides across different organisms with greater accuracy than previous methods.\nThe near-perfect performance demonstrates USPNet can potentially handle the annotation of proteomes with high\nconfidence when there is no experimental validation.\n11\na\nORF 40,391,320Swine gut meragenome collection 5\nc_Signal peptides 6,407,575c_Signal peptides 3,881,170c_Signal peptides 347\nGenome 14,027\nORF prediction\nMetaproteome\nFiltering\nRedundant\nFiltering\n0.14%\n68.98%\n30.88%\nArchaea\nGram-negative\nGram-positive\n30.82%\n68.07%\n0.93%\nSP\nLIPO\nTAT\nTATLIPO\nPILIN\nRMSD: 1.072\nprotein pLDDT: 0.533\npeptide pLDDT: 0.834\nRMSD: 1.218\nprotein pLDDT: 0.792\npeptide pLDDT: 0.820\nRMSD: 0.184\nprotein pLDDT: 0.814\npeptide pLDDT: 0.830\nRMSD: 1.143\nprotein pLDDT: 0.790\npeptide pLDDT: 0.820\nRMSD: 3.561\nprotein pLDDT: 0.800\npeptide pLDDT: 0.781\nRMSD: 0.238\nprotein pLDDT: 0.772\npeptide pLDDT: 0.834\nb\nc\nd\n0\n1\n2\n3\n4bits\nN\n1M\n2\nS\nR\nN\nK\n3\nQ\nT\nI\nL\nN\nR\nK\n4\nN\nF\nL\nI\nK\n5\nF\nI\nK\nL\n6\nA\nK\nI\nL\n7\nA\nV\nK\nF\nS\nI\nL\n8\nF\nS\nI\nA\nL\n9\nV\nM\nI\nA\nL\n10\nI\nS\nG\nF\nV\nL\nA\n11\nS\nG\nM\nI\nV\nL\nA\n12\nC\nS\nG\nM\nV\nA\nL\n13\nF\nS\nV\nI\nG\nM\nL\nA\n14\nI\nG\nT\nM\nS\nV\nL\nA\n15\nI\nT\nS\nF\nM\nV\nA\nL\n16\nM\nI\nT\nF\nG\nV\nS\nL\nA\n17\nF\nM\nG\nT\nV\nS\nL\nA\n18\nM\nT\nF\nV\nL\nS\nA\n19\nF\nV\nT\nG\nL\nS\nA\n20\nF\nT\nG\nV\nS\nL\nA\n21\nS\nV\nT\nG\nL\nA\n22\nT\nG\nF\nV\nL\nS\nA\n23\nM\nG\nT\nS\nL\nA\n24\nG\nF\nV\nL\nS\nA\n25\nQ\nT\nP\nG\nL\nS\nA\n26\nM\nT\nF\nL\nS\nA\nC\n0\n1\n2\n3\n4bits\nN\n1M\n2K\n3\nT\nQ\nYK\n4\nS\nH\nLI\n5\nI\nA\nW\nT\nL\n6\nI\nV\nP\nA\nL\n7\nS\nG\nW\nT\nL\nA\n8\nW\nS\nG\nAL\n9\nV\nL\nG\nA\n10\nT\nG\nA\nL\n11\nT\nG\nAL\n12\nT\nP\nG\nV\nA\nL\n13\nW\nG\nA\nL\n14\nV\nLA\n15\nF\nQ\nLA\n16\nT\nS\nG\nF\nQ\nP\nA\n17\nQ\nF\nS\nPA\n18\nI\nS\nM\nA\n19\nT\nP\nL\nS\nM\nA\n20\nS\nK\nM\nA\n21\nA\n22\n23\n24\n25\n26C\nUSPNet-fast\nPrediction\nMax Seq-Sim: 0.818\nRMSD: 2.262\ntrain_sp ID: Q540U1 \n pLDDT: 0.829\ncandidate ID: WP_022315869.1\n pLDDT: 0.716\nMin Seq-Sim: 0.130\nRMSD: 0.277\ntrain_sp ID: P38571 \n pLDDT: 0.762\ncandidate ID: WP_022366228.1\n pLDDT: 0.862\nRandom Seq-Sim: 0.500\nRMSD: 0.212\ntrain_sp ID: P83508\n pLDDT: 0.864\ncandidate ID: WP_005840282.1\n pLDDT: 0.821\ne\n0.1 0.3 0.5 0.7 0.9\n0.2\n0.4\n0.6\n0.8\n1.0\nMax Seq-Sim against train signal peptides\nTM-score\nFigure 5: The exploration of metagenomics data for signal peptide discovery. a . Schematic representation\nof the USPNet pipeline for novel signal peptide discovery in metagenomics data. 347 novel SP candidates were\nidentified. b. Pie charts depicting the distribution of 3,881,170 protein sequences predicted by USPNet that\ncontain signal peptides, categorized by predicted SP types (left) and organism groups (right).c. Sequence logo for\n347 novel signal peptide candidates (top), as well as 16 artificial novel SPs (bottom). Two sets of peptides exhibit\na similar residue distribution. d. Visualization of the alignment between select proteins and their predicted signal\npeptides, accompanied by RMSD and pLDDT values; the majority exhibit alignment with the predicted novel\nSPs. e. Distribution of the highest sequence and structure similarity between 347 candidates and 4 experimentally\nverified SPs mentioned in our study to that in the training dataset. The sequence identity hit a low of only 13%,\nbut the structural similarity is strikingly high, with TM-scores mostly above 0.8. And 3 pairs of sequences, one\nwith maximum sequence similarity, one with minimum sequence similarity, and a random one, are sampled and\naligned. They have analogous structures.\n12\n2.6 USPNet enables the discovery of novel SPs based on structural information with\nextremely low sequence similarity\nThe benchmarking exhibits that USPNet enables high-resolution signal peptide prediction, and drastically im-\nproves performance on the minor classes compared with previous methods. Besides, the proteome-wide research\ndemonstrates our method effectively detects the SPs from the proteomes of various organisms. Moreover, in the\norganism-agnostic experiment, the performance of USPNet seems to be group-information-independent. All the\nabove studies provide evidence that USPNet has the potential to discover SPs from multiple sources of data. One\nof the most preliminary data for research is metagenomics. However, previous works have not shown that they can\nefficiently detect SPs or even novel SPs from original metagenomics data. At the same time, no standard pipeline\nis available for signal peptide discovery from metagenomics cohorts. Therefore, we would like to evaluate if UP-\nSNet is applicable in real-world metagenomics research and explore the pattern of USPNet in recognizing SPs. In\nthis section, we build a complete pipeline from collecting metagenomic data to make novel signal peptide detec-\ntion. The pipeline is shown in Figure 5.a. We studied the swine microbiome, which has a complex community\nstructure. And its constituent microbes, especially those in the gut, could employ many functional proteins like\nAMPs [35] to help compete for resources or stabilize the community structure. Many of these proteins are likely\ndependent on signal peptides for their functionality, providing an adequate resource for finding SPs.\nHere we collect the swine gut metagenomic data from five projects and resources. In total, we have 14027\ngenomes, which are used to predict ORF sequences. To generate high-quality protein sequences, we first inte-\ngrate fastp [36] as a quality controlling tool and utilize PLASS [37] to assemble nucleotide sequences to protein\nsequences, which leads to 40,391,320 sequences. Then, considering the amount of data as well as the time cost,\nwe apply USPNet-fast to recognize signal peptides on all sequences. As a result, 6,407,575 are predicted as signal\npeptides (c Signal peptides). The total running time of USPNet-fast for completing all predictions is 211,932\nseconds. To avoid some potential false positives, we remove the genomes that are without organism group infor-\nmation, and the known signal peptides are also removed. Then, we get 3,881,170 sequences. The distribution of\n3,881,170 protein sequences according to predicted SP types and group information is shown in Figure 5.b. The\nmajority of sequences come from gram-negative bacteria. Sec/SPII type SPs are predicted to be the most frequent\nin both Gram-negative and Gram-positive bacteria and second frequent in Archaea. Sec/SPI type SPs are predicted\nto be the most frequent in Archaea.\nGiven the known effect of signal peptides in directing the transfer of synthesized proteins to the secretory\npathway, we further select sequences that are likely expressed to proteins/peptides. We accomplish this by identi-\nfying the same peptides in swine gut metaproteomics data, and eventually identify 347 sequences. Besides novel\npeptides, our method also retrieves all the 4 experimentally verified signal peptides that exist in swine gut data\nbut do not exist in the previously mentioned datasets (Table 2). The evidence shows their exhaustive functions\nin controlling protein secretion and translocation, which belong to different families and play a part in different\norganisms. Therefore, our comprehensive filtering guarantees all 347 peptides are novel in comparison with ex-\nperimentally verified SPs. We compare them with 16 artificially designed and synthesized novel SPs, which have\nvarious compositions in the h-region and are validated to improve the secretion [38], by logo plot (Figure 5.c).\nNotably, our candidate peptides show similar motifs to these novel signal peptides: the second and third places\nare dominated by lysine. For the rest of the positions, alanine and leucine take first place. The similarity between\nthese sequences indicates they may have homologies. We also applied USPNet on these 16 novel SPs. It precisely\nclassifies all their types and cleavage sites. To look for further evidence that our discovered peptides are probably\nSPs, we search in the UniProtKB [39]. It shows that 1,715 proteins contain these peptides with a confident label\nECO:0000256 (automatic annotation), demonstrating that they are automatically classified as signal peptides by\nthe database with high confidence, 10 most frequently occurring sequences of 347 predicted novel signal peptides\nare provided in Table 2. The most often seen sequence is WP 008666378.1 (NCBI ID), which appeared 179\ntimes, about one-tenth of all proteins. In total, we have 281 peptides that appeared in the 1715 sequences. We then\nchoose some of these proteins and predict their 3-D structure using Alphafold2 [40]. Thereafter, we align them\nwith their corresponding predicted signal peptides (Figure 5.d). Prominently, the N-terminus sequences of these\nproteins are mostly well-aligned with our predicted novel SPs. Meanwhile, the structural connection between\nsuch sequences and their subsequent sequences does not seem very strong. It signifies that these N-terminus se-\nquences are more likely to be deleted or replaced when functioning, which coincides with how signal peptides\nwork. Finally, our analysis reveals that the maximum sequence identity between our candidate peptides and the\nclosest signal peptides (SPs) in the training dataset is merely 81.8%. The majority exhibits less than 60% similar-\nity, with the lowest similarity being only 13%. However, their structures have a remarkable likeness, the average\nTM-score is above 0.8. We further perform structural alignment on 3 pairs of sampled sequences with the highest\nsimilarity value, the lowest similarity value, and the random one (Figure 5.e). These results further underscore the\n13\nTable 2: Top 10 most frequently occurring predicted signal peptide sequences in 1715 proteins (Novel SP can-\ndidates), and some experimentally verified SPs that we figure out from swine gut metagenomics data. (Seq-sim:\nhighest sequence similarity value in the reconstructed training set, TM-score: highest structure similarity (TM-\nscore) value in the reconstructed training set)\nNovel SP candidates\nprotein ID (NCBI) Predicted SP Count Evidence\nWP 008666378.1 MNKKFLSAILFGALMVSSTGT 179\nECO:0000256\nAutomatic assertion\naccording to\nsequence analysis.\nSAM:SignalP\nWP 008661887.1 MNKKFLSVILFSALMVGTAGT 102\nABG29383.1 MKKTLVSALTTALVVGAASTTFA 61\nWP 009145071.1 MKKSLVLAMAMALGVTASA 49\nWP 005825009.1 MNLNFRRTALLVGICSA VSLTYTPQLFA 32\nWP 006847594.1 MRRLTLLLVSLVLLSIQSVLA 32\nWP 006846852.1 MKHLKQFLLMAMLLFTLAPLQVSA 30\nWP 006848777.1 MKKIAILSLSLTLAAAAQA 28\nWP 006846306.1 MYQQLKRASMALTLSSVCFLAFA 27\nWP 005844474.1 MIKKLYLPLV ALLVLALSS 26\nExperimentally Verified\nEntry (UniProtKB/Swiss-Prot ID) Predicted SP Evidence Seq-sim TM-score\nP02768 MKWVTFISLLFLFSSAYS [41] 0.222 0.990\nP28800 MALLWGLLALILSCLSSLCSAQ [42] 0.35 0.902\nQ2UVX4 MKPTSGPSLLLLLLASLPMALG [43] 0.381 0.709\nQ3MHN5 MKRILVFLLA V AFVHA [44] 0.125 0.961\nstructural homology between our candidate peptides and specific signal peptides. Our method takes raw amino\nacid sequences as input and does not rely on structural information. However, the discovered candidates seem to\nhave been screened out based on structural similarities. To further test our hypothesis, next, we look into the 4\nverified SPs. Our original training set contains SPs exhibiting over 60% similarity to these verified SPs. To ensure\nUSPNet detects SPs without depending on sequence similarity, we exclude SPs from the training set with more\nthan 40% sequence identity to 4 verified SPs and retrain our model. This time, our model is still able to detect all\n4 SPs, which on average, have quite high structural similarities to the training set (Table 2). The literature-based\nevaluation and database search results show that our USPNet is scalable to large amounts of data. Although it is a\nsequence-based method, it can discover peptides that have low sequence identity but high structure resemblance,\nwhich shows that our method has learned to represent protein folding information without any related inputs at a\nrelatively ideal speed. It may potentially help to screen signal peptides from metagenomics data and discover SPs\nthat differ significantly from what is currently known.\n3 Discussion\nWe develop USPNet as a deep-learning method focusing on resolving the existing long-tail data distribution\nproblem and group-information dependency in signal peptide prediction. To solve the data imbalance, we first\ninclude LDAM, which is orthogonal to most deep-learning-based signal peptide predictors. As the single loss\nfunction has limited effects Based and considering the feature of LDAM, we propose to combine class-balanced\nloss and LDAM loss together in our method, which we named CB-LDAM loss to enhance the generalization\nof boundaries of minor classes and major classes. Furthermore, the developed protein language models incor-\nporate rich evolutionary and structural information and have shown their excellent abilities to improve plenty of\ndownstream tasks. Therefore, we propose to introduce MSA transformer and ESM embeddings to achieve an\nend-to-end organism-agnostic model. Finally, we present a new model architecture. The attention-based BiLSTM\nmodel allows for maximizing the possibilities to integrate and extract relationships among different positions of\ninputting sequences and helps boost performance.\nAlthough comprehensive experiments have verified that the integration of all these techniques can achieve\nbetter performance in signal peptide prediction compared with state-of-the-art methods, our result on recognizing\nSec/SPI signal peptide of Archaea is not that impressive. We suppose the reason for this phenomenon is our loss\nfunction adjusts the weight of different types of SPs, and hence makes USPNet tend to classify peptides into minor\nclasses. During real-life usage, to make the model more focused, we recommend that users can tune the weights\nof different types according to their demand.\nAs previously highlighted, we develop a complete pipeline to discover novel signal peptides from metage-\n14\nnomics data. The reason we choose USPNet-fast for the study is the multi-sequence alignment can be time-\nconsuming, making it impossible to finish the large-scale screening in a reasonable time. The 347 candidate novel\nSPs are available as an open science resource. Potentially, new signal peptides may be used to improve the se-\ncretion efficiency of heterologous proteins. We provide a paradigm for the discovery process, and it is easy to\nreproduce. Besides swine gut data we used in our study, other resources like the human gut and cardiac tissue are\nalso signal peptide reservoirs. We expect our pipeline can give new insights into the related research.\nApart from signal peptide prediction, data imbalance, and object—dependence is very common in biological\nprediction problems. Our model-developing strategy can be transferred to more general fields of bioinformatics\n[45, 46, 47, 48, 46, 49]. In summary, USPNet represents a widely applicable framework for predicting signal\npeptides or even protein sequences. Considering the fact that it can be integrated with other tools in a pipeline, we\nbelieve USPNet will be helpful to investigate a wide range of signal peptide problems.\n4 Methods\n4.1 Data collection\nWe collected, in total, four kinds of datasets in our study, including benchmark data, independent dataset,\nproteome data, and metagenomic data.\nBenchmark data\nThe training and benchmark test set is the same as introduced in SignalP6.0 [17], which reclassified some\nSP types for data published with SignalP5.0 [16]. Further, it added some new SPs from UniProt20 [50] and\nProsite21 [51], and new soluble and transmembrane proteins from UniProt and TOPDB22 [52]. Then, part of the\ndata in the new dataset was removed with the homology partitioning methodology introduced by G ´ıslason et al.\n[53]. In total, the training data contains 13679 sequences, and the benchmark test data contains 6611 sequences.\nThe original training dataset comprises proteins from four organism groups: Eukaryotes, Gram-positive, Gram-\nnegative bacteria, and Archaea. To verify the effects of better generalization on minor classes, we carry out\nfive separate SP types: Sec/SPI, Sec/SPII, Tat/SPI, Tat/SPII, and Sec/SPIII SPs. Other proteins are accordingly\nconsidered as TM/Globular (NO-SP) type, as shown in Table 1. The dataset with 6 separate labels has a long-\ntailed label distribution, which means that the NO-SP type is superior in numbers among different labels. Notably,\nthere are three minority class SPs: Tat/SPI contains 365 data points, and even worse, only 33 data are labeled as\nTat/SPII signal peptides, and 70 are labeled as Sec/SPIII signal peptides. In both training and benchmark sets, the\nlength of the protein sequences can be varied. However, the lengths of signal peptides usually fall in 10-50 AAs\n(Figure 2). We thus set a cut-off at 70 AAs to highlight signal peptides and avoid the influence of the long protein\nsequences.\nIndependent test set\nThe independent test dataset we curated is named SP22. It collected verified SP proteins from the Swiss-\nProt database [50], which was released after November 2020, i.e., the date of SignalP 6.0 dataset collection.\nSpecifically, it is generated in the following steps: (1) we remove proteins before November 2020, and proteins\ncomposed of less than 30 amino acids; (2) we select those from eukaryotes, Gram-positive and Gram-negative\nbacteria; (3) we select proteins containing signal peptides with a confident label ECO:0000269 (experimental\nannotation) and ECO:0000305 (manually curated annotation) from the Swiss-Prot database (released on 2022\n04). Furthermore, to better ensure the independence of the SP22 dataset, CD-HIT [54] is applied to remove\nredundant proteins sharing more than 40% similarity with proteins in SiganlP6.0 dataset, and internal redundancy\nof SP22 is cut off to 80%. In total, SP22 has 43 protein sequences from 39 species that seldom appeared in the\ntraining set, and all of the sequences have signal peptides. Among them, 31 are from Eukaryotes group, 6 are from\nGram-negative group, and 6 are from Gram-positive group.\nProteome-wide data\nWe collect reference proteome data from UniProt database. For Escherichia coli (strain K12), UniProt reports\n496 proteins with signal peptides, 141 of which are experimentally verified and 355 predicted in various ways. Ex-\nperimentally verified SPs are collected as signal peptides with a confident label ECO:0000269 and ECO:0000305\n15\nfrom the Swiss-Prot database (released on 2022 04) for all proteomes. We also study other 7 reference pro-\nteomes available in UniProt, including Bacillus subtilis, Corynebacterium glutamicum, Deinococcus radiodurans,\nHaloferax volcanii, Methanocaldococcus jannaschii, Pyrococcus furiosus, and Thermus thermophilus.\nMetagenomics data\nWe collect metagenomes of swine gut from five resources: PRJEB38078 [55], PRJNA561470 [56], PR-\nJNA647157 [57], CNP0000824 [58], and Global Microbiome Conservancy (GMBC) [59]. They are used for\npredicting sORF sequences. And to ensure that our sORFs are indeed expressed, we used a metaproteome dataset\nfrom PRIDE project PXD006224 [60]. We select sequences that are identical to our predicted signal peptides.\n4.2 Establishment of USPNet\nConsidering the length of signal peptides in protein N-terminus, we set a cut-off as 70 to the input sequence\nlength of protein L, which means that each input sequence contains at most 70 amino acids. Considering the\nnumber of different residue types is 20, we employ an embedding layer to extract features and therefore convert\ninput sequences into the L×20-dimensional matrix. At the same time, we use one-hot encoding to express 4 kinds\nof group information (Eukaryotes, Gram-positive, Gram-negative bacteria, and Archaea) of the input sequence\nusing 4 binary numbers. Then, we replicate them, converting input auxiliary sequences into the L×4-dimensional\nmatrix. If there is no provided group information, all entries are designated as 0.\nFor USPNet, the model architecture comprises two key components: the BiLSTM-attention model and the\nfeature extraction module to predict both signal peptide type and cleavage sites.\nBiLSTM module\nWe first describe the BiLSTM module. It gets the L×20-dimensional embedding vectors of protein sequences\nand feeds them into one fully connected layer with 252 hidden units. The output of the fully connected layer\nhas two usages. First, it is passed to two CNN layers to generate L×512-dimensional representations for future\nuse. The CNN can capture the important motifs and aggregate both useful local and global information across the\nentire input sequence. Second, the output is concatenated with the auxiliary matrix to form the L×256-dimensional\nmatrix as the input to the BiLSTM layer. It is with 128 hidden units to extract long-distance dependencies of\nsequences over forward and backward directions. In this way, we integrate information from input sequences at a\nhigher level for later prediction by concatenating the outputs of two directions, which can be written as:\nht = [− →h t; ← −h t]\n= BiLST M\n\u0010\nexi\n, − →h t−1, ← −h t+1, θCls\n\u0011\n,\n(1)\nwhere − →h t and ← −h t denote hidden states at time point t over forward and backward directions, respectively.exi\nde-\nnotes the embedded representation of the input sequence element xi. θCls denotes the parameters of the BiLSTM\nlayer. The BiLSTM layer takes the embedded input exi\n, the forward hidden state − →h t−1, the backward hidden\nstate ← −h t+1, and the parameters θCls as input and outputs the concatenated hidden states ht. The BiLSTM layer\noutputs 256-dimensional vectors.\nTo better aggregate information from different feature subspaces, we develop a multi-head attention mecha-\nnism in the BiLSTM module, with the scaled dot-product attention in each head:\nAttention(Q, K, V) =softmax(QKT\n√dk\n)V, (2)\nwhere 1√dk\nis the scaling factor, and dk is the dimension of the queries and keys. We conduct Equation (2) in each\nhead and concatenate the multiple single-headed outputs together:\nMultiHead(Q, K, V) = Concat ( head 1, . . . ,head h) WO,\nheadi = Attention\n\u0010\nQWQ\ni , KWK\ni , V WV\ni\n\u0011 (3)\nWhere the projections are parameter matrices WQ\ni ∈ Rdmodel×dk , WK\ni ∈ Rdmodel×dk , WV\ni ∈ Rdmodel×dv ;\nQ is the output from the previous BiLSTM layer; both K and V are the same as the input to the BiLSTM layer.\nAnd h represents the number of heads. We employ h = 2to form the parallel attention layers. For each head, we\n16\nuse dmodel = 256, dk = dv = dmodel/h = 128. Within the multi-head attention module, we employ a residual\nconnection step to stabilize training and help mitigate the possible vanishing gradient problem. The output of\nthe multi-head attention module is further concatenated with its input, and then element-wise added with the\nrepresentations of previous CNN layers. After that, the vector is finally fed into the second BiLSTM layer with\n256 hidden units.\nTo predict the cleavage site, we build a three-layer MLP on top of the second BiLSTM layer. The output\nof BiLSTM is hence fed into three fully connected layers to produce the L×11-dimensional matrix. The 11-\ndimensional vectors correspond to SP region labels, with 10 SP region labels for each amino acid in protein\nsequences and 1 annotation for vacancies of input sequences with lengths less than 70. The output matrix is used\nto directly predict the presence of cleavage sites across all positions of amino acids of input protein sequences.\nFurther, the output of the second BiLSTM layer is also used to predict signal peptide types. The output\nvector is reshaped and first goes through one fully connected layer to reduce its dimensionality and result in a\n512-dimensional vector. Meanwhile, we include MSA embeddings here to integrate more information. The pre-\ntrained MSA Transformer model [24] generates 768-dimensional embeddings, and we use fully connected layers\nto aggregate high-level information about these embeddings as a 64-dimensional vector. The processed results are\nconcatenated with the vectors from previous outputs of the fully connected layer. The concatenated results hence\nhave 512+64 channels and are summarized by a fully connected layer with 256 hidden units. Finally, the outputs\nare forwarded to a normalized linear projection layer to make the prediction.\nLoss function designed for the data imbalance problem\nMost existing knowledge bases related to signal peptides suffer from extreme data imbalance. The number of\nthe minority classes of signal peptide sequences is usually much smaller than that of non-signal-peptide sequences.\nThis scenario may lead to poor generalization in low-resource data types. Existing techniques, such as cost-\nsensitive re-sampling and re-weighting, can effectively cope with challenges brought by data imbalance. How-\never, for the signal peptide prediction, these methods are case-sensitive and prone to overfitting. Post-correction\nmethods are also proposed to handle data imbalance, but generalization improvement cannot be ensured. In-\nspired by the vanilla empirical risk minimization (ERM) algorithm, we introduce LDAM loss and combine it with\nreweighting to solve the issue.\nLDAM loss focuses on correcting the cross-entropy function by introducing the margin item △y to improve\nthe generalization of classes. It suggests that a suitable margin should achieve good trade-offs between the gener-\nalization of major classes and minor classes. The class-dependent margin for multi classes is verified to have the\nform:\n△yj = C\nn1/4\nj\n, (4)\nwhere nj is the sample size of the j-th class, C is a hyper-parameter to be tuned.\nHowever, the margin item has changed the amplitude of logits of the correct category and influenced gener-\nalization, which adds up to the difficulties of representation learning and makes the model sensitive to parameter\nsettings. Here, we introduce agent vectors for each class by applying a normalized linear layer in the final layer of\nthe classifier [61]. We normalize both the inputs and the weights of the linear projection layer to clamp the inner\nproduct of feature vectors and weight vectors into [-1, 1]. The experiments prove that the normalization can not\nonly improve the robustness of the model but also accelerate the convergence during the training process.\nEmpirically, softmax cross-entropy loss with scaling factor is widely used in reinforcement learning and rel-\nevant fields [62]. On the one hand, if the scaling factor is too large, class intervals will become close to 0 and\ntherefore influence generalization. On the other hand, a small scaling factor will lead to a deviation from the\nobjective function. Here in our method, a scaling factor is used as a hyper-parameter to ensure the final logits\nlocate in a reasonable scope:\nL(x, y) =−log\n\n e\n(zy−△y)\ns\ne\n(zy−△y)\ns + P\ni̸=y e\nzi\ns\n\n, (5)\nwhere\n△j = C\nn1/4\nj\nfor j∈ {1, ..., K}, (6)\n17\nzy denotes the logit score of the ground truth label, zi denotes the logit score of other labels, and s represents the\nscaling factor. Both zy and zi are normalized as agent vectors.\nThe final objective function optimizes signal peptide prediction and cleavage site prediction jointly:\nLs = − 1\nN\nNX\nj=1\nKX\ny=1\nL(xj, yj), (7)\nLc = − 1\nN ∗ L\nN∗LX\nj=1\nK′\nX\ny=1\nL(xj, yj), (8)\nLuspnet = Ls + τLc, (9)\nwhere τ = 1. Ls denotes the objective function for signal peptide prediction, where the number of classes\nK = 6. And Lc denotes the objective function for cleavage site prediction, where the number of region labels\nK′ = 11. N is the number of input protein sequences, and L is the maximum length of the input sequence set as\n70.\nProtein language models to enrich representations\nCompared with other protein language models, the MSA transformer [24] makes the best of the powerful\nmodel architecture and large evolutionary database. In addition, self-supervised learning is good at encoding\nproperties of protein sequences in many different scales, which contributes to the high performance of the model.\nIt is verified that without biological signals other than sequences, the model can still learn the structures of amino\nacids, protein sequences, and evolutionary homology [23]. Furthermore, the generalization of the model trained\nover multi-families is better than the effects based on a single family. Accordingly, we believe that by adding the\nrepresentation of the MSA transformer, the similarity of sequences within the single group can be captured, and\nthe USPNet will become more powerful to differentiate sequences from different group organisms.\nTo apply MSA transformer in our method, we first generated Multi-sequence alignment (MSA) for each se-\nquence by searching updated UniClust30[63] with HHblits[64]. And diversity maximizing subsampling strategy[24],\na greedy strategy that starts from the reference and adds the sequence with the highest average hamming distance\nto the current set of sequences, is used to reduce the number of aligned sequences to 128 for full MSAs with\nthe size larger than 128. Then we input MSAs to the pre-trained MSA Transformer model [24] to generate 768-\ndimensional embeddings from the final layer of the model.\nThe other version of our model, USPNet-fast replaces MSA embeddings with ESM-1b embeddings [23].\nESM-1b takes single sequences as input to generate embeddings, so it enables USPNet-fast to predict signal\npeptide and cleavage sites faster and without much performance degeneration.\nTraining details\nIn USPNet, we apply the Adam optimizer [65] with the initial learning rate of 2×10−3 and a weight decay of\n1×10−3 [32]. The total number of epochs is set to be 300, with the early stopping strategy to get the best model\nduring training. All the experiments run on four V100 GPU cards with 32GB memory. USPNet does not rely\non PSSM and HMM profiles to enrich the embeddings or enhance performances. It is more straightforward to\nmake predictions without evolutionary profile-based features, therefore ensuring a shorter processing time. For\nSignalP6.0 and SignalP5.0, we used the hyperparameters they provided in the code repositories to train the models.\n4.3 Performance evaluation\nFor the taken metrics in the experiments, we summarize them into two components: Metrics used for evalua-\ntion of classification performance and cleavage site (CS) prediction performance. For classification, we used the\nMatthews correlation coefficient (MCC) as a measurement. We considered true/false positives/negatives by labels\nof sequences. MCC can be written as:\nMCC = TP × TN − FP × FNp\n(TP + FP)× (TP + FN)× (TN + FP)× (TN + FN)\n, (10)\n18\nwhere TP denotes the number of true positives, TN denotes the number of true negatives, FP denotes the\nnumber of false positives, and FN denotes the number of false negatives. Here, we include both MCC1 and MCC2\nto evaluate the performance and hence compute twice. For MCC1, we only take globular and/or transmembrane\nproteins as negative set and proteins of relevant signal peptide type as positive set. And then, for MCC2, all the\nrest sequences are added to the negative set.\nWhile in the ablation study, we not only measure classification performance by MCC over different organism\ngroups but also apply overall MCC, Kappa, and balanced accuracy for evaluation. This is because in the ablation\nstudy part, some models’ performances in minor classes are pretty close, and the overall performances will be more\nstraightforward and intuitive to display differences. Kappa is broadly used in consistency tests and measurements\nof multi-class classification. And considering the extreme imbalance in datasets, accuracy cannot demonstrate the\nactual performance of models. Therefore, we take balanced accuracy instead. Balanced accuracy normalizes true\npositive and true negative predictions over the total number of samples:\nBalanced accuracy = T P R+ T NR\n2 , (11)\nwhere TPR denotes the true positive rate and TNR denotes the true negative rate.\nIn the CS prediction part, we take precision and recall as measurements. Precision represents the fraction of\ncorrect CS predictions over the total number of predicted CSs, and recall represents the fraction of correct CS\npredictions over the number of ground truth labels of CSs. Notably, we do not tolerate prediction deviations; in\nother words, only exact site predictions will be considered correct.\nTo estimate the quality of multiple sequence alignment, the number of effective sequences (Neff) is applied. It\ncan be calculated as the following function:\nNeff =\nNX\ni=1\n1\nweight i\n, (12)\nwhere N is the number of sequences in an MSA, and weightij is the sequence identity between any two homolo-\ngous sequences i and j in the MSA.\n4.4 3-D structure prediction\nConsidering the computational resource, we apply the ColabFold [66] version of AlphaFold2 to perform the\n3-D structure prediction in Figure 5.d and e. Specifically, we set the number of synchronously running models\nas 1, and use the amber relaxation to refine the prediction. Other settings remain default. For the visualization\nof the structure and the following alignment operation, we utilize Pymol [67]. And because the training set has\na large number of sequences, to save our time, the structure inferences of the training set and 347 candidates\nwere performed by ESMFold [68], and the TM-scores are calculated by TM-align [69] (the subsequent 3 pairs of\nsamples structure alignment was still conducted by ColabFold).\n5 Data availability\nAll the datasets we used are listed in the Method part and are publicly available. All other relevant data\nsupporting the key findings of this study, such as the results of the metagenomics study, are available within the\narticle and the Supplementary Information files or from the corresponding author upon reasonable request. Source\ndata are provided in this paper.\n6 Code availability\nThe open source codes of USPNet can be found at https://github.com/ml4bio/USPNet, and the experiments\nconducted to produce the main results of this article are also stored in this repository.\n19\nReferences\n[1] von Heijne, G. Life and death of a signal peptide. Nature 396, 111–113 (1998).\n[2] Heijne, G. V . The signal peptide. The Journal of Membrane Biology 115, 195–201 (1990).\n[3] Bradshaw, N., Neher, S. B., Booth, D. S. & Walter, P. Signal sequences activate the catalytic switch of srp\nrna. science 323, 127–130 (2009).\n[4] Craig, L., Forest, K. T. & Maier, B. Type iv pili: dynamics, biophysics and functional consequences. Nature\nreviews microbiology 17, 429–440 (2019).\n[5] Duan, G.-F. et al. Signal peptide represses gluk1 surface and synaptic trafficking through binding to amino-\nterminal domain. Nature Communications 9, 4879 (2018).\n[6] Jiang, F. et al. N-terminal signal peptides facilitate the engineering of pvc complex as a potent protein\ndelivery system. Science Advances 8, eabm2343 (2022).\n[7] Heijne, G. V . Patterns of amino acids near signal-sequence cleavage sites.European Journal of Biochemistry\n133 (1983).\n[8] Bendtsen, J. D., Nielsen, H., V on Heijne, G. & Brunak, S. Improved prediction of signal peptides: Signalp\n3.0. Journal of molecular biology 340, 783–795 (2004).\n[9] Reynolds, S. M., K ¨all, L., Riffle, M. E., Bilmes, J. A. & Noble, W. S. Transmembrane topology and signal\npeptide prediction using dynamic bayesian networks. PLoS computational biology 4, e1000213 (2008).\n[10] Ehsan, A., Mahmood, K., Khan, Y . D., Khan, S. A. & Chou, K.-C. A novel modeling in mathematical\nbiology for classification of signal peptides. Scientific reports 8, 1039 (2018).\n[11] Janda, C. Y . et al. Recognition of a signal peptide by the signal recognition particle. Nature 465, 507–510\n(2010).\n[12] Madani, A. et al. Large language models generate functional protein sequences across diverse families.\nNature Biotechnology 1–8 (2023).\n[13] Frank, K. & Sippl, M. J. High-performance signal peptide prediction based on sequence alignment tech-\nniques. Bioinformatics 24, 2172–2176 (2008).\n[14] Petersen, T. N., Brunak, S., V on Heijne, G. & Nielsen, H. Signalp 4.0: discriminating signal peptides from\ntransmembrane regions. Nature methods 8, 785–786 (2011).\n[15] Savojardo, C., Martelli, P. L., Fariselli, P. & Casadio, R. Deepsig: deep learning improves signal peptide\ndetection in proteins. Bioinformatics 10 (2017).\n[16] Armenteros, J. J. A. et al. Signalp 5.0 improves signal peptide predictions using deep neural networks.\nNature biotechnology 37, 420–423 (2019).\n[17] Teufel, F. et al. Signalp 6.0 predicts all five types of signal peptides using protein language models. Nature\nbiotechnology 1–3 (2022).\n[18] Juncker, A. S. et al. Prediction of lipoprotein signal peptides in gram-negative bacteria. Protein Science 12,\n1652–1662 (2003).\n[19] Bagos, P. G., Tsirigos, K. D., Liakopoulos, T. D. & Hamodrakas, S. J. Prediction of lipoprotein signal\npeptides in gram-positive bacteria with a hidden markov model. Journal of proteome research7, 5082–5093\n(2008).\n[20] Bendtsen, J. D., Nielsen, H., Widdick, D., Palmer, T. & Brunak, S. Prediction of twin-arginine signal\npeptides. BMC bioinformatics 6, 1–9 (2005).\n[21] Pasolli, E. et al. Accessible, curated metagenomic data through experimenthub. Nature methods 14, 1023–\n1024 (2017).\n20\n[22] Sczyrba, A. et al. Critical assessment of metagenome interpretation—a benchmark of metagenomics soft-\nware. Nature methods 14, 1063–1071 (2017).\n[23] Rives, A. et al. Biological structure and function emerge from scaling unsupervised learning to 250 million\nprotein sequences. Proceedings of the National Academy of Sciences 118 (2021).\n[24] Rao, R. M. et al. Msa transformer. In Meila, M. & Zhang, T. (eds.) Proceedings of the 38th Interna-\ntional Conference on Machine Learning, vol. 139 ofProceedings of Machine Learning Research, 8844–8856\n(PMLR, 2021).\n[25] Biswas, S., Khimulya, G., Alley, E. C., Esvelt, K. M. & Church, G. M. Low-n protein engineering with\ndata-efficient deep learning. Nature methods 18, 389–396 (2021).\n[26] Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. & Church, G. M. Unified rational protein engineering\nwith sequence-based deep representation learning. Nature methods 16, 1315–1322 (2019).\n[27] Thireou, T. & Reczko, M. Bidirectional long short-term memory networks for predicting the subcellular\nlocalization of eukaryotic proteins. IEEE/ACM transactions on computational biology and bioinformatics4,\n441–446 (2007).\n[28] Cao, K., Wei, C., Gaidon, A., Arechiga, N. & Ma, T. Learning imbalanced datasets with label-distribution-\naware margin loss. arXiv preprint arXiv:1906.07413 (2019).\n[29] Armenteros, J. J. A. et al. Detecting sequence signals in targeting peptides using deep learning. Life science\nalliance 2 (2019).\n[30] Mnih, V ., Heess, N., Graves, A.et al. Recurrent models of visual attention. Advances in neural information\nprocessing systems 27 (2014).\n[31] McInnes, L., Healy, J. & Melville, J. Umap: Uniform manifold approximation and projection for dimension\nreduction. arXiv preprint arXiv:1802.03426 (2018).\n[32] Lin, T.-Y ., Goyal, P., Girshick, R., He, K. & Doll´ar, P. Focal loss for dense object detection. In Proceedings\nof the IEEE international conference on computer vision, 2980–2988 (2017).\n[33] Cui, Y ., Jia, M., Lin, T.-Y ., Song, Y . & Belongie, S. Class-balanced loss based on effective number of\nsamples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 9268–\n9277 (2019).\n[34] Blattner, F. R. et al. The complete genome sequence of escherichia coli k-12. science 277, 1453–1462\n(1997).\n[35] Ma, Y . et al. Identification of antimicrobial peptides from the human gut microbiome using deep learning.\nNature Biotechnology 40, 921–931 (2022).\n[36] Chen, S., Zhou, Y ., Chen, Y . & Gu, J. fastp: an ultra-fast all-in-one fastq preprocessor. Bioinformatics 34,\ni884–i890 (2018).\n[37] Steinegger, M., Mirdita, M. & S ¨oding, J. Protein-level assembly increases protein sequence recovery from\nmetagenomic samples manyfold. Nature methods 16, 603–606 (2019).\n[38] Han, S. et al. Novel signal peptides improve the secretion of recombinant staphylococcus aureus alpha\ntoxinh35l in escherichia coli. Amb Express 7, 1–14 (2017).\n[39] Consortium, T. U. UniProt: the Universal Protein Knowledgebase in 2023. Nucleic Acids Research 51,\nD523–D531 (2022).\n[40] Jumper, J. et al. Highly accurate protein structure prediction with alphafold. Nature 596, 583–589 (2021).\n[41] Patterson, J. E. & Geller, D. M. Bovine microsomal albumin: amino terminal sequence of bovine proalbumin.\nBiochemical and Biophysical Research Communications 74, 1220–1226 (1977).\n[42] Christensen, S. & Sottrup-Jensen, L. Bovine α2-antiplasmin n-terminal and reactive site sequence. FEBS\nletters 312, 100–104 (1992).\n21\n[43] Baldo, A. et al. The adipsin-acylation stimulating protein system and regulation of intracellular triglyceride\nsynthesis. The Journal of clinical investigation 92, 1543–1547 (1993).\n[44] Nykjaer, A. et al. An endocytic pathway essential for renal uptake and activation of the steroid 25-(oh)\nvitamin d3. Cell 96, 507–515 (1999).\n[45] Li, Y . et al. Deepre: sequence-based enzyme ec number prediction by deep learning. Bioinformatics 34,\n760–769 (2018).\n[46] Yu, Q., Dong, Z., Fan, X., Zong, L. & Li, Y . Hmd-amp: Protein language-powered hierarchical multi-label\ndeep forest for annotating antimicrobial peptides. arXiv preprint arXiv:2111.06023 (2021).\n[47] Lam, J. H. et al. A deep learning framework to predict binding preference of rna constituents on protein\nsurface. Nature communications 10, 1–13 (2019).\n[48] Wei, J., Chen, S., Zong, L., Gao, X. & Li, Y . Protein-rna interaction prediction with deep learning: Structure\nmatters. arXiv preprint arXiv:2107.12243 (2021).\n[49] Li, Y . et al. Hmd-arg: hierarchical multi-task deep learning for annotating antibiotic resistance genes. Mi-\ncrobiome 9, 1–12 (2021).\n[50] Consortium, U. Uniprot: a worldwide hub of protein knowledge. Nucleic acids research 47, D506–D515\n(2019).\n[51] Sigrist, C. J. et al. New and continuing developments at prosite. Nucleic acids research 41, D344–D347\n(2012).\n[52] Dobson, L., Lango, T., Rem ´enyi, I. & Tusn ´ady, G. E. Expediting topology data gathering for the topdb\ndatabase. Nucleic acids research 43, D283–D289 (2015).\n[53] G ´ıslason, M. H., Nielsen, H., Armenteros, J. J. A. & Johansen, A. R. Prediction of gpi-anchored proteins\nwith pointer neural networks. Current Research in Biotechnology 3, 6–13 (2021).\n[54] Li, W. & Godzik, A. Cd-hit: a fast program for clustering and comparing large sets of pro-\ntein or nucleotide sequences. Bioinformatics 22, 1658–1659 (2006). URL https://doi.org/\n10.1093/bioinformatics/btl158. https://academic.oup.com/bioinformatics/\narticle-pdf/22/13/1658/484588/btl158.pdf.\n[55] Youngblut, N. D. et al. Large-scale metagenome assembly reveals novel animal-associated microbial\ngenomes, biosynthetic gene clusters, and other genetic diversity. Msystems 5, e01045–20 (2020).\n[56] Looft, T., Bayles, D., Alt, D. & Stanton, T. Complete genome sequence of coriobacteriaceae strain 68-1-3, a\nnovel mucus-degrading isolate from the swine intestinal tract.Genome announcements 3, e01143–15 (2015).\n[57] Zhou, S. et al. Characterization of metagenome-assembled genomes and carbohydrate-degrading genes in\nthe gut microbiota of tibetan pig. Frontiers in Microbiology11, 595066 (2020).\n[58] Chen, C. et al. Prevotella copri increases fat accumulation in pigs fed with formula diets. Microbiome 9,\n1–21 (2021).\n[59] Groussin, M. et al. Elevated rates of horizontal gene transfer in the industrialized human microbiome. Cell\n184, 2053–2067 (2021).\n[60] Tilocca, B. et al. Dietary changes in nutritional studies shape the structural and functional composition of\nthe pigs’ fecal microbiome—from days to weeks. Microbiome 5, 1–15 (2017).\n[61] Wang, F., Xiang, X., Cheng, J. & Yuille, A. L. Normface: L2 hypersphere embedding for face verification.\nIn Proceedings of the 25th ACM international conference on Multimedia, 1041–1049 (2017).\n[62] Liu, J., Krishnamachari, B., Zhou, S. & Niu, Z. Deepnap: Data-driven base station sleeping operations\nthrough deep reinforcement learning. IEEE Internet of Things Journal 5, 4273–4282 (2018).\n[63] Mirdita, M. et al. Uniclust databases of clustered and deeply annotated protein sequences and alignments.\nNucleic acids research 45, D170–D176 (2017).\n22\n[64] Steinegger, M. et al. Hh-suite3 for fast remote homology detection and deep protein annotation. BMC\nbioinformatics 20, 1–15 (2019).\n[65] Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980\n(2014).\n[66] Mirdita, M. et al. Colabfold: making protein folding accessible to all. Nature methods 19, 679–682 (2022).\n[67] DeLano, W. L. et al. Pymol: An open-source molecular graphics tool. CCP4 Newsl. Protein Crystallogr 40,\n82–92 (2002).\n[68] Lin, Z. et al. Language models of protein sequences at the scale of evolution enable accurate structure\nprediction. BioRxiv 2022, 500902 (2022).\n[69] Zhang, Y . & Skolnick, J. Tm-align: a protein structure alignment algorithm based on the tm-score. Nucleic\nacids research 33, 2302–2309 (2005).\n23",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6329570412635803
    },
    {
      "name": "Signal peptide",
      "score": 0.6310877799987793
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5544759035110474
    },
    {
      "name": "Metagenomics",
      "score": 0.5540704131126404
    },
    {
      "name": "Pipeline (software)",
      "score": 0.5104362368583679
    },
    {
      "name": "Computational biology",
      "score": 0.4966610074043274
    },
    {
      "name": "Organism",
      "score": 0.48021969199180603
    },
    {
      "name": "Training set",
      "score": 0.45183998346328735
    },
    {
      "name": "Machine learning",
      "score": 0.4401856064796448
    },
    {
      "name": "Deep learning",
      "score": 0.42978551983833313
    },
    {
      "name": "Natural language processing",
      "score": 0.3493484556674957
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3430820405483246
    },
    {
      "name": "Peptide sequence",
      "score": 0.3376122713088989
    },
    {
      "name": "Biology",
      "score": 0.24906909465789795
    },
    {
      "name": "Biochemistry",
      "score": 0.1783943772315979
    },
    {
      "name": "Gene",
      "score": 0.1486477255821228
    },
    {
      "name": "Genetics",
      "score": 0.10371392965316772
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4210105229",
      "name": "City University of Hong Kong, Shenzhen Research Institute",
      "country": "CN"
    }
  ]
}