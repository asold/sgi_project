{
  "title": "Large Language Models Meet Harry Potter: A Dataset for Aligning Dialogue Agents with Characters",
  "url": "https://openalex.org/W4389518991",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2358406745",
      "name": "Chen, Nuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1967442656",
      "name": "Wang Yan",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2232064215",
      "name": "Jiang Hai-yun",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2364924703",
      "name": "Cai Deng",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2267130154",
      "name": "Li Yuhan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A73121112",
      "name": "Chen Zi-yang",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2950659627",
      "name": "Wang Longyue",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1937087924",
      "name": "Li Jia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3087232873",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W2125320996",
    "https://openalex.org/W4389523886",
    "https://openalex.org/W4378510341",
    "https://openalex.org/W3197032408",
    "https://openalex.org/W4288624561",
    "https://openalex.org/W2997892440",
    "https://openalex.org/W2911994530",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W4323929307",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W4226487007",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4402671806",
    "https://openalex.org/W3196863408",
    "https://openalex.org/W3155391257",
    "https://openalex.org/W3092945658",
    "https://openalex.org/W4389519291",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W4372266972",
    "https://openalex.org/W2740919303",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2964352131",
    "https://openalex.org/W2979928633",
    "https://openalex.org/W2578354947"
  ],
  "abstract": "In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT and GPT4 have demonstrated immense potential in constructing open-domain dialogue agents. However, aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of character representation and the lack of comprehensive annotations. In this paper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to advance the study of dialogue agents and character alignment. The dataset encompasses all dialogue sessions (in both English and Chinese) from the Harry Potter series and is annotated with vital background information, including dialogue scenes, speakers, character relationships, and attributes. These extensive annotations may empower LLMs to unlock character-driven dialogue capabilities. Furthermore, it can serve as a universal benchmark for evaluating how well can a LLM aligning with a specific character. We benchmark LLMs on HPD using both fine-tuning and in-context learning settings. Evaluation results reveal that although there is substantial room for improvement in generating high-quality, character-aligned responses, the proposed dataset is valuable in guiding models toward responses that better align with the character of Harry Potter. © 2023 Association for Computational Linguistics.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8506–8520\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models Meet Harry Potter:\nA Bilingual Dataset for Aligning Dialogue Agents with Characters\nNuo Chen†∗, Yan Wang‡§∗, Haiyun Jiang‡, Deng Cai‡\nYuhan Li†, Ziyang Chen‡, Longyue Wang‡and Jia Li†§\n‡Tencent AI Lab\n†DSA, Hong Kong University of Science and Technology (Guangzhou),\nHong Kong University of Science and Technology\n†chennuo26@gmail.com,§{yanwang.branden@gmail.com,jialee@ust.hk}\nHPD Benchmark: https://nuochenpku.github.io/HPD.github.io\nAbstract\nIn recent years, Dialogue-style Large Language\nModels (LLMs) such as ChatGPT and GPT4\nhave demonstrated immense potential in con-\nstructing open-domain dialogue agents. How-\never, aligning these agents with specific charac-\nters or individuals remains a considerable chal-\nlenge due to the complexities of character rep-\nresentation and the lack of comprehensive an-\nnotations. In this paper, we introduce the Harry\nPotter Dialogue (HPD) dataset, designed to ad-\nvance the study of dialogue agents and char-\nacter alignment. The dataset encompasses all\ndialogue sessions (in both English and Chinese)\nfrom the Harry Potter series and is annotated\nwith vital background information, including\ndialogue scenes, speakers, character relation-\nships, and attributes. These extensive annota-\ntions may empower LLMs to unlock character-\ndriven dialogue capabilities. Furthermore, it\ncan serve as a universal benchmark for evalu-\nating how well can a LLM aligning with a spe-\ncific character. We benchmark LLMs on HPD\nusing both fine-tuning and in-context learning\nsettings. Evaluation results reveal that although\nthere is substantial room for improvement in\ngenerating high-quality, character-aligned re-\nsponses, the proposed dataset is valuable in\nguiding models toward responses that better\nalign with the character of Harry Potter.\n1 Introduction\nWith the emergence of dialogue-centric Large Lan-\nguage Models (LLMs) such as ChatGPT and GPT-\n4 (OpenAI, 2023), there has been a growing inter-\nest among researchers in exploiting the capabilities\nof these models to develop open-domain dialogue\nagents. A particularly exciting and challenging as-\npect of this pursuit involves aligning the behavior\nof these agents with a distinct character or individ-\nual (Kirk et al., 2023; Salemi et al., 2023).\nIn this paper, using the Harry Potter series fic-\ntion as a prime example, we take a significant leap\n∗Equal Contribution. §Indicates Corresponding authors.\nDataset Sce. Att. Re. Dy. Sl.\nPchatbotW (2021) × √ × × ×\nPeDialog (2019) × √ × × ×\nKvPI (2020) × √ × × ×\nP-CHAT (2018) × √ × × ×\nWOW (2019) √ √ × × ×\nFri-QA (2019) × √ √ × ×\nFocus (2021) √ √ √ × ×\nUltraChat(2023) √ √ × × ×\nLaMP(2023) √ √ × × ×\nOurs √ √ √ √ √\nTable 1: Datasets Comparison. Sce., Att., Re., Dy. and\nSl. denote Scenes, Attributes, Relations, Dynamic\nand Storyline, separately.\ntowards aligning dialogue agents with characters\nin a story. Throughout this process, we discover\nthat despite the assistance of super-powerful LLMs\nsuch as ChatGPT and GPT4, we encounter numer-\nous challenges: (1) Firstly, the knowledge of LLMs\nis primarily dominated by real-world knowledge,\nwhich may diverge or even contradict to the char-\nacter’s story setting. In fact, ChatGPT and GPT-\n4 have developed a substantial understanding of\nthe Harry Potter series and the wizarding world\nbehind it, yet they still occasionally produce hal-\nlucinations that contradict the story setting. For\ninstance, GPT4-act Harry might attempt to visit\nRon’s house using the Hogwarts Express, despite\nthe fact that this train exclusively operates between\nKing’s Cross Station and Hogwarts. (2) Secondly,\nLLMs encounter difficulties in interpreting intricate\ncharacter relationships, particularly when multiple\nrelationships coexist (e.g., friendships, romantic\npartnerships and competitors). (3) Finally, the most\nsignificant challenge stems from the LLMs’ inabil-\nity to accurately represent the impact of time on\ncharacters, leading to potential inconsistencies in\ntheir portrayal and development within the story.\nThe dialogue on the right side of Figure 1 effec-\ntively illustrates the difficulties (2) and (3). We all\nknow that Harry and Ron are very close friends, but\n8506\nCome on, please just take off your Cloak\nfor a bit, no one’s going to bother you here.\nOh yeah? Look behind you. She’s staying in the \nvillage. I bet she’s coming to watch the first task.\nWhy don’t we go to find Ron and have a \nbutterbeer in the Three Broomsticks, it’s a \nbit cold, isn’t it? \nScene (summarized version):\n• Harry and Hermione are in Hogsmeade Village.\n• Harry and Ron had a serious argument due to the events surrounding the Triwizard \nTournament.\nTiming:\nBook 4-Chapter 19\nChatGPT: Alright, let's go to find Ron and have a pint \nof butterbeer together.\nGPT-4: Fine, let's have a glass of butterbeer together \nand go check if Ron is around.\nExpected Answer: Alright, I can go to have a pint of \nbutterbeer, but I really don't want to see Ron right now.\nWhat will Harry say next?\nAttributes\nAge 14\nSpells Expelliarmus\n… …\nBelongings Scabbers, …\nAttributes\nAge 14\nSpells Expelliarmus, Lumos\n… …\nBelongings Invisibility Cloak, \nNimbus 2000, …\nAge 14\nSpells Aparecium\n… …\nBelongings Cat…\nAttributes\nRelations and \nAttributes\nFigure 1: A conversation between Harry and Hermione selected from Book 4-Chapter 19 in the Harry Potter Series.\nIn this example, we present the scene, timing of the conversation, relations and attributes of speakers. Texts in red\nrefer to the expected response.\nthey sometimes experience conflicts as well, like\nmost adolescents. In Book 4-Chapter 19, Harry\nand Ron have a heated disagreement over the “Tri-\nwizard Tournament” registration (Ron believes that\nHarry has hidden the truth from him, but in real-\nity, Harry has not), leading to a period of mutual\navoidance. At this point, when Hermione suggests\nto Harry that they should find Ron for a Butterbeer,\nHarry’s normal response would be reluctance to see\nRon. However, even after providing an extensive\ncontext as a prompt, both ChatGPT and GPT-4 still\ngenerate responses that portray Harry and Ron as\ngood friends, which is clearly inconsistent with the\nsituation at that time.\nThe appearance of the previously mentioned\nchallenges is unsurprising, considering that cur-\nrent dialogue datasets intrinsically lack accurate\nannotations and guidelines for handling dynamic\ncharacter relationships and attributes. As shown\nin Table 1, while some existing datasets (Zhang\net al., 2018; Zheng et al., 2019; Jang et al., 2021;\nDing et al., 2023) do include basic information\nabout scenes, relationships, and attributes, the is-\nsue is that this information remains static and does\nnot change over time. For example, if John and\nHarry are labeled as “friends”, their relationship\nwill remain as friends across all dialogue sessions.\nIn this paper, we propose Harry Potter Dialogue\n(HPD), a dataset for facilitating the study of Di-\nalogue Agents - Character aligning. This dataset\nencompasses all dialogue sessions from Harry Pot-\nter novels (English and Chinese versions) 1. In\ntotal, we obtain 1042 dialogue sessions for training\n(containing 1 positive response only) and 149 ses-\nsions for testing (containing 1-3 positive responses\nand 9 negative responses in average). We also an-\nnotate each conversation with essential background\ninformation that we believe is useful for aligning\ndialogue agents with Harry, including dialogue\nscenes, speakers, character relationships, and\nattributes. With the goal of giving a full pic-\nture of the speakers in dialogue, we have labeled\neach speaker with 12 types of relationships with\nHarry Potter and 13 types of attributes. Please note\nthat even if the speakers in two different dialogues\nare identical, their relationships and attributes may\nchange due to the story’s progression.\nThe main contributions of this paper can be sum-\nmarized into three aspects:\n• We introduce the HPD dataset, designed to\nfacilitate the study of aligning dialogue agents\nwith characters. For each dialogue session\nin the Harry Potter novels, we provide all\nthe background information that we believe\nmay be helpful in assisting models to generate\nHarry Potter-aligned responses.\n• This is a bilingual dialogue dataset that does\nnot rely on machine translation. The only\ndifference between languages is that they are\n1Due to the space limits, we only introduce the English\nexperimental results in this paper.\n8507\nbased on Harry Potter novels in different lan-\nguages. As a result, the data quality in both\nlanguages is comparable, making it suitable\nfor investigating the impact of language differ-\nences on the task of aligning dialogue agents\nwith characters.\n• The experimental results show that HPD can\nhelp LLMs, such as ChatGPT, better align\nwith the behaviors of Harry Potter. However,\nthe degree of alignment is still far from the\nlevel of human experts, indicating ample room\nfor further improvement.\n2 Task Definition\nWe use the Harry Potter novels as our test-bed with\nthe aim of aligning dialogue agents with Harry\nPotter in a story. The generated responses of such\na dialogue agent should be not only relevant to the\ncontext, but also seem like something Harry would\nsay at the time and scene.\nFigure 1 shows some main factors that affect\nbehaviors of Harry in a conversation. The first fac-\ntor is the conversation history, which is the most\nimportant factor that determines Harry’s response.\nThe scene, which is the second factor, provides de-\ntails about the motivation (Hermione invites Harry\nto have a butterbeer with Ron in the Three Brrom-\nsticks) of this dialogue. The third factor is the par-\nticipants’ information (attributes and relations),\nobviously, Harry will say very different things to\ndifferent characters, such as Malfoy and Hermione.\nThe latter two factors belong to the background\ninformation and are dynamically determined by the\ntiming of this dialogue (Book 4-Chapter 19), and\nthey are continuously varied over the storyline.\nFormally, the task of aligning dialogue agents\nwith characters in a story can be defined as fol-\nlows: Given a dialogue history H, corresponding\ndialogue scene S and participants information P as\ninput, which evolve depending on the development\nof storyline. The dialogue agent is supposed to\ngenerate a response Y = {y1, y2, ..., yn}:\nY= argmax\nY\nP(Y|H, S, P)\nY is supposed to be not only fluent and natural,\nbut also highly relevant to S and P.\n3 Dataset Construction\nA high-quality dataset including all pertinent infor-\nmation in Section 2 is the prerequisite for aligning\ndialogue agents with characters in a story. Un-\nfortunately, so far there are currently no publicly\navailable datasets that provide information about\nthe dialogue scene and participants. To facilitate\nthe study of this task, we construct a new dataset\nfrom the popular fictions Harry Potter Series, in the\nhopes of creating a Harry Potter-aligned dialogue\nagent. All dialogue sessions that Harry partici-\npates in are collected in our dataset, along with\nfine-grained annotated dialogue scenes and partici-\npant information. We recruit four avid Harry Potter\nfans (professional annotators, no crowd-sourcing)\nfor the annotation work in this study.\nWe collect three parts of information to construct\nour dataset, as shown in Figure 1: 1) The dialogue\npart (Section 3.1) contains all utterances in the di-\nalogue sessions, as well as the speaker’s name of\neach utterance. 2) The scene part (Section 3.2) in-\ncludes the summarization of the text around the\ndialogue session. 3) Finally, the speaker informa-\ntion part (Section 3.3), which consists of attributes\nand relations of characters, is shown in the left part\nof Figure 1. Please note that these scenes, attributes,\nand relations are time-sensitive, they may change\nas the storyline go on, so we should annotate them\nsession by session.\n3.1 Dialogue Construction\nDialogue sessions in the books are divided into a\ntraining set and a test set. Their main difference is\nthat each training session contains only one posi-\ntive response, while each test dialogue session con-\nsists of at least 1 positive response (human-written\nground-truth) and 9 negative responses. We hope\nthis test set may support the evaluation of both\ndialogue generation and retrieval.\nTraining Set We request the annotators to extract\nall multi-turn dialogues from the books. Besides,\nthe speaker name of each utterance in the session\nis labeled as well.\nTest Set The effectiveness of dialogue models\nshould be evaluated using a well-designed test set.\nHowever, if we directly select the test dialogue ses-\nsions from the books, it may meet serious knowl-\nedge leakage problems, i.e., the fact that we evalu-\nate in the test set also presents in the pre-training\ncorpus. To prevent this problem, we deliberately\ndesign a test set in the following steps:\n• First, we manually select some raw dialogues\nthat meet the following requirements: (i) Dia-\n8508\nFriend, Classmate\nAffection: 3Familiarity: 3\nAffection: 5Familiarity: 6\nFriend,Classmate, Colleague, TeammateAffection: 7Familiarity: 8Affection: 7Familiarity: 8\nTiming:  Book 1-Chapter 12\nTiming:  Book 6-Chapter 7\nAge 11GenderMaleTalentsParseltongueSpellsNoneBelongingsWand…………AffiliationHogwarts\nAge 16GenderMaleTalentsParseltongue, Quidditch, Defense Against Dark Arts, …SpellsLumos, ExpectoPatronum, …BelongingsWand, invisibility Cloak, Firebolt, ……… ……AffiliationHogwarts\nAttribute\nAttribute\nAttribute……\nAttribute……\nRelations\nFigure 2: Data examples from two different timings: Book 1-Chapter 12 and Book 6-Chapter 7. Words in color\ndenote the changed relations and attributes.\nlogues with only one speaker, which contain\nonly one or two sentences; (ii) Dialogues, in\nwhich there is no response from other speak-\ners to the last question. For these samples, we\npick out dialogues that are relevant to Harry\nand can be answered from Harry’s perspective\nto construct the test set.\n• Second, since directly composing a high-\nquality Harry-like response is quite challeng-\ning for the annotators, we try to alleviate the\ndifficulty with the help of LLMs. Specifi-\ncally, we utilize ChatGPT and GPT-4 to gen-\nerate potential responses for each selected di-\nalogue session under in-context learning set-\ntings. Subsequently, each model is needed to\npredict 5 responses as the candidates at least.\n• Third, we ask three annotators to select the\nmost reasonable responses from the predic-\ntions as the positive response (ground-truth)2\nwhile the remaining samples are taken as neg-\natives. A holdout annotator is responsible for\nintegrating the annotations. If the selected re-\nsponse is not comprehensive enough or lacks\ncertain information, rewriting is required. Fur-\nthermore, if all predictions are considered un-\nreasonable, the holdout annotator would write\na response from scratch.\nThis setup has advantages over employing a sin-\ngle annotator to label predictions: One annotator\nmay regard some predictions as positive responses\nwhile the other may label them as negatives. Hence,\nthe holdout annotator can be used to guarantee the\n2Some questions may have multiple valid answers\nquality of annotated answers when disagreements\nappear. These strategies alleviate spam and bias,\nand thus get a high-agreement dataset. Finally, we\nmanually double-check and revise mistakes to fur-\nther ensure the data quality.\n3.2 Scenes Construction\nIn order to offer accurate location information and\ntextual details for each dialogue, we further anno-\ntate scenes. We assume that the texts that surround\nand immediately relate to the dialogue in the novel\nprovide ample scene information. Therefore, we\ninitially instruct the annotators to extract these texts\nfrom the novel. Subsequently, to leverage the ca-\npabilities of GPT-4, we employ it to summarize\nthe extracted texts. To ensure the utmost precision\nand accuracy, two skilled annotators are required\nto meticulously calibrate the summarized texts, re-\nsulting in the final scene data.\nIt is worth noting that we deliberately abstained\nfrom directly utilizing the extracted texts as scenes,\nprimarily due to two compelling reasons: Firstly,\nthe inherent length of the original text exceeds the\nmaximum limit imposed by most LLMs, rendering\nit impractical for our purposes. Secondly, in real-\nworld scenarios, the attainment of such exception-\nally high-quality scenes is highly improbable, as\nscene information typically originates from video\ncaptions or rule-based templates.\n3.3 Attributes and Relations Construction\nOne of the most important and appealing proper-\nties of our benchmark is the fine-grained annotated\ncharacter information, which includes the attributes\nand relations of the characters. With the goal of\n8509\nproviding in-depth and comprehensive character\ninformation, we collect 13 fine-grained attributes\nand 12 relations. We take into account all of these\nattributes and relationships that are dynamically\nevolving as the story goes on. Thus, we annotate\nthem chapter by chapter. Considering not all char-\nacters are essential to understanding and driving\nthe story in Harry Potter series, we target 113 im-\nportant characters to annotate their attributes and\nrelations, such as Harry, Ron.\nWe divide the attributes into two categories: (1)\ninborn; (2) nurture. The former denotes some in-\nnate attributes or abilities, which contains{Gender,\nAge, Lineage, Talents, and Looks}. The latter refers\nto properties acquired through efforts, including\n{Achievement, Title, Belongings, Export, Hobby,\nCharacter, Spells and Nickname}(some cases are\npresented in Figure 2). In total, we collect 13 at-\ntributes for each character, which basically cover\nmost properties in the Harry Potter series.\nThe relations between Harry and other charac-\nters can be classified into binary relations and dis-\ncrete relations. The former include 8 types, which\nare {Friend, Classmate, Teacher, Family, Lover,\nOpponent, Teammate, Enemy}. Multiple binary\nrelations can exist between two characters. Harry\nand Ron, for instance, are friends, classmates, and\nteammates in the Quidditch team (In Book-6 only).\nIn order to fully represent the relationship be-\ntween two characters, we need to know not only\nthe type of relationship (the binary relations) they\nhave but also their familiarity and affection for each\nother. We annotate affection and familiarity as 4\ntypes of discrete relations: (1) Harry’s Familiarity\nwith someone, (2) Harry’s Affection for someone,\n(3) someone’sFamiliaritywith Harry, and (4) some-\none’s Affection for Harry.\nThe difference between these types can be illus-\ntrated by the following two examples: 1) Draco\nMalfoy hates Harry, but he is also familiar with\nHarry. So his Affection for Harry is low but his\nFamiliarity with Harry is high. 2) In addition,\never since Harry lost his parents, Dumbledore has\nshown great concern for Harry. Hence, his Familiar-\nity and Affection for Harry are high, while Harry’s\nFamiliarity with Dumbledore is relatively low.\nFigure 2 is another example of how attributes\nand relationships have changed over the course of\nthe story. In Book 1-Chapter 7, Harry has just met\nRon on the train to Hogwarts and is an alien in the\nwizarding world. So his affection and familiarity\n Affection\n 10  Parents and relatives who died for Harry.         Examples: Harry's parents\n 9  Characters who are very close with Harry and save Harry's life.         Examples: Ron in Book-7\n 8  Characters who are in love with Harry.          Examples: Ginny\n 7  Best friends          Examples: Ron in Book-2\n 6  Close Friends and very kindly to Harry.          Examples: Hagrid\n 5  Characters who often help Harry.           Examples: Dumbledore\n 4  Characters who are relatively friendly to Harry.          Examples: Neville\n 3  Normal Teammates.           Examples: Wood\n 2  Normal Classmates/Teachers.            Examples: Lavender Brown\n 1  First Meeting.           Examples: Harry ﬁrst met Ron and Hermione\n 0  Stranger.\n -2  Rude/Frivolous/Mean characters.     Examples: Draco Malfoy in Book-1,  Filch\n -4  Deliberately bullying/deliberately targeting.            Examples: Snape, Dudley\n -6  Maliciously targeting and harm.            Examples: Draco Malfoy in Book-5\n -8  Intentionally inﬂict harm.               Examples: Bellatrix Lestrange  in Book-5\n -10  Kill Harry's parents.                 Examples: Voldemort\nFigure 3: Affection Definition and Examples.\nFigure 4: Familiarity Definition and Examples.\nwith Ron are relatively low (1 and 2, respectively),\nand he isn’t aware of any spells. As the story pro-\ngresses, however, in Book 6-Chapter 7, he is a\nfull-fledged wizard, and Ron is his best friend. So\ntheir affection and familiarity are high at this time\n(7 and 8, respectively). Harry also masters a lot of\nspells such as Expecto Patronumand Expelliarmus,\nand has some equipments such as his broomstick\nFirebolt and the invisibility cloak.\nAffection Definition Affection is rated on 21 lev-\nels, ranging from -10 to 10, where -10 and 10 indi-\ncate the lowest and highest affection, respectively.\nA positive affection level means the character has\na positive relationship with Harry and vice versa.\nFigure 3 shows the detailed definition and some\nexamples of different Affection levels. For exam-\nple, 1 refers to “someone meets Harry for the first\n8510\nStatistics Train Test\nper dialogue\nAverage Turns 13.8 7.8\nMaximum Speakers 20 8\nMinimum Speakers 2 2\nper utterance\nAverage Length 32.9 28.3\nMaximum Length 77 26\nMinimum Length 3 3\nTotal Dialogues 1042 149\nTable 2: Data statistics of collected dialogues.\ntime”, when Harry first met Ron and Hermione in\nBook 1, Harry’s Affection to both is 1. As for −10,\nit means the deepest hatred, and the most obvious\nexample is Harry and V oldmort.\nFamiliarity Definition Similarly, we also rate\nFamiliarity with a 11-point scale, which ranges\nfrom 0 to 10, where 10 is the highest and 0 is the\nlowest. We present detailed definitions for each\nFamiliarity level in Figure 4. Concretely, 0 denotes\nstrangers, and 10 denotes very close friends who\nstay together for many years and are very familiar\nwith each other’s habits, secrets, and temperaments.\nRon in Book 7 meets this condition.\nAnnotation During annotation, we ask annota-\ntors to annotate Attributes and Relations following\nthe above definitions and examples. To eliminate\nthe effects of individual bias, we ask three of them\nto repeatedly label the Attributes and Relations\nchapter by chapter. And a holdout annotator (se-\nnior) is responsible for the quality of annotated data\nwhen the former three annotators have disagree-\nments. Finally, we manually re-check all annotated\ndata and revise some controversial annotations to\nfurther control the data quality.\nClaim Notice that we hope to provide as rich\ncharacter information as possible for the commu-\nnity, even if some of them seem redundant in this\nwork. Therefore, we collect 13 types of attributes\nfor each character and 12 types of relations in the\ncollected HPD. We leave plenty of opportunity for\nother research communities to investigate which\ninformation is helpful in their work. For example,\nit can support other tasks such as the sentiment\nanalysis of Harry Potter. One may not need to in-\nclude all the fine-grained annotated information in\nhis/her study, because it is still challenging.\n3.4 Data Statistics\nThe detailed statistics of dialogues are shown in\nTable 2. The training set and test set contain 1042\nand 149 dialogue sessions, respectively. Of note,\nwe initially collect 1471 dialogues for constructing\nthe training set, and we filter out those dialogues\nthat are without Harry, leading to 1042 conversa-\ntions for consideration. Most of the conversations\ninvolve multiple speakers, with the maximum one\nincluding up to 20 speakers. It is obviously a seri-\nous challenge to the dialogue agents.\n4 Experiments\nIn this section, we conduct extensive experiments\nto investigate 1) whether Annotations in HPD can\nassist LLMs in aligning with characters, and 2)\nif so, whether the degree of alignment can reach\nhuman-level performance. We will discuss these\nquestions in section 4.3.\n4.1 Baselines\nWe build multiple strong baselines in our exper-\niments, which can be divided into two types:\ngeneration-based and retrieval-based systems. Due\nto the page limit, here we only introduce the gener-\nation task in HPD. For details of retrieval tasks in\nHPD, please refer to Appendix B and D.\nModels We implement different generative mod-\nels in two ways: fine-tuning and in-context learn-\ning. For the former, we fine-tune Alpaca3 (6B)\nand ChatGLM-6B on our dataset. For the latter,\nwe deliberately design prompts for GPT3 (Brown\net al., 2020) (text-davinci-002), ChatGPT (gpt3.5-\nturbo), and ChatGLM (Zeng et al., 2022) ( chat-\nglm-130B). Furthermore, to explore the effect of an-\nnotated fine-grained background knowledge, some\nmethods (Alpaca, GPT3, and ChatGPT) are imple-\nmented in two different settings: 1) base setting\nwith a prompt that only includes task description,\none dialogue example, and dialogue history; 2)\nrich-persona setting (denoted as Per-Model) with\na prompt contains all annotated background infor-\nmation in HPD as in-context learning exemplars.\nThe detailed prompts can be found in Appendix\nC (Table 8 for base setting, and Table 10 for rich-\npersona setting).\n3https://github.com/tatsu-lab/stanfordalpaca\n8511\nModel Automatic Evaluation GPT-4 Evaluation Human Evaluation\nBleu-1 Dist.1 Rough-L Relv.Sce. Relv.Att. Relv.Re. Relv.Sce. Relv.Att. Relv.Re.\nFine-tuning\nCGLM*-6B 1.2 18.4 3.4 1.21 1.10 1.01 1.34 0.67 0.67\nAlpaca 2.0 30.2 10.8 3.37 1.19 1.51 2.03 1.19 1.51\nPer-Alpaca 12.2 20.4 13.8 3.51 1.34 1.51 4.04 1.34 1.51\nIn-context learning\nCGLM*-130B 14.2 25.1 15.7 6.71 6.01 6.04 7.38 6.04 5.37\nGPT3 9.8 23.0 14.6 2.68 2.68 3.36 4.70 4.70 4.03\nPer-GPT3 22.6 20.1 16.5 12.75 6.04 3.36 12.08 6.71 5.37\nChatGPT 33.1 19.0 20.2 33.56 38.92 37.58 32.89 34.23 32.89\nPer-ChatGPT 33.6 19.4 22.4 37.58 42.95 46.98 35.57 45.63 48.99\nTable 3: A comprehensive evaluation of all baselines on HPD test set. Per-Model means the model with prompts in\nrich-persona setting. Here we report the percentage of generated responses ranked as the best one for each dialogue\nsession (top-1 ranking) in GPT-4 and human evaluation. CGLM refers to ChatGLM. We report the average ranks\nof these LLMs in Figure 5. Dist.1 is short for Distinct-1.\n4.2 Evaluation Metric\nTo comprehensively measure whether the texts gen-\nerated by various models exhibit a Harry Potter-\naligned tone, we employ three different kinds of\nevaluation methods: reference-based (automatic),\nGPT-4 based, and human-based. Although some\nexisting studies (Liu et al., 2023; Peng et al., 2023)\nclaims that GPT-4 is good-enough to assess the\nquality of generated texts, we found that human\njudgment is still the most thorough and realistic\nassessment of whether the generated text is Harry\nPotter-aligned in our task.\nReference-based Metrics We employ Rough-\nL (Lin, 2004), Bleu-1 (Papineni et al., 2002),\nDistinct-1 as our metrics to validate the relevance\nand diversity of the generated responses.\nGPT-4 Evaluation We evaluate the persona’s\nconsistency based on the three criteria: Relevance\nwith the Scene (Relv.Sce.), Relevance with the At-\ntributes (Relv.Att.) and Relevance with the Rela-\ntions (Relv.Re.). Considering the potential draw-\nbacks of having GPT-4 to assign scores for all re-\nsponses individually, such as the risk of confusing\nGPT-4’s comprehension, we have opted for an al-\nternative approach. We instruct GPT-4 to rank\nthe generated texts based on above distinct crite-\nria. This ranking process allows us to evaluate the\nperformance of different models effectively. The\ndetailed prompts are in Appendix C, Table 9.\nHuman Evaluation In light of the discovery of\nsome poor GPT-4 annotation cases, we further in-\nstructed our annotators to revise and rectify the\nranking results of GPT-4 for each test data, leading\nto human-evaluation results. During the annotation,\nannotators were allowed to consult the original text\nfor reference when ranking. Moreover, as long as\nat least one annotator among the three made ad-\njustments to GPT-4’s ranking results, we would\nadopt the modified results as the human evaluation\nresults. If multiple annotators made revisions to a\nsingle result, we would take the average of their\nmodifications as the final human evaluation result.\n4.3 Results\nWhether the HPD can assist LLMs in aligning\nwith characters? To answer this question, we\npresent the results of these baselines in Table 3,\nand observe that across all GPT-4 and human-based\nmetrics, methods perform significantly better in the\nrich-persona setting compared to their performance\nin the base setting. This improvement, obviously,\nshould be attributed to the additional background\ninformation provided by the HPD.\nAdditionally, for ChatGPT, the improvement\nin persona consistency-related metrics, including\nRelv.Att. and Relv.Re., is more pronounced (for\nexample, compared to ChatGPT, Per-ChatGPT has\nincreased by 11.4% and 16.1% in these two met-\nrics, respectively). This is mainly because, in most\ncases, the dialogue history has some overlap with\nthe scene, and ChatGPT can be consistent with the\nscene by merely using the dialogue history. For\nGPT-3 and Alpaca, their understanding of dialogue\nhistory is not as strong as that of ChatGPT, so their\nimprovements in Relv.Sce. are still noticeable.\nModels VS. Human Experts Intuitively, we may\nfurther raise a question: How good are the gener-\n8512\nCategory Relv.Sce. Relv.Att. Relv.Re. Ave.\nWIN 77.85 77.18 73.15 76.06\nTIE 1.34 4.03 8.05 4.47\nLOSE 20.81 18.79 18.79 19.46\nTable 4: Comparison between Per-ChatGPT and Human\nexperts. The win/tie/lose proportions of human-written\nresponses on different metrics are reported.\nScene: Harry and his family visit the reptile house at the\nzoo, where Harry strikes up a conversation with a boa\nconstrictor from Brazil... Harry is blamed for talking to the\nsnake and causing the incident.\nDialogue Position: Book1-chapter2\nSpeakers: Dudley, Harry...Piers\nDialogue: Dudley: Make it move...\nthe snake: I get that all the time.\nHarry: I know, Where do you come from, anyway?\nthe snake: Boa Constrictor, Brazil.\nHarry: Was it nice there? Oh, I see — so you’ve never been\nto Brazil?\nkeeper of the reptile house: DUDLEY! MR. DURSLEY!\nCOME AND LOOK AT THIS SNAKE! YOU WON’T\nBELIEVE WHAT IT’S DOING!\nthe snake: Brazil, here I come. . . . Thanksss, amigo.\nPiers: Harry was talking to it, weren’t you, Harry?\nGPT-3: I was just asking it where it came from. It’s not\nlike I was trying to make friends with it or anything.\nPer-GPT3: I didn’t mean to cause any trouble, I was just\ncurious about where the snake came from. I didn’t know it\nwould escape.\nChatGPT: Yes, I was just curious about where it came from\nand if it had ever been to Brazil. But Dudley, you shouldn’t\nbe so rude to the keeper of the reptile house. He was just\ntrying to show us something amazing.\nPer-ChatGPT: Yes, I was. But I didn’t make the glass\ndisappear or tell the snake to escape. It was just a harmless\nconversation. I didn’t mean for any of this to happen.\nAnnotated Response: No, I wasn’t talking to it. I was\njust looking at it and reading the sign about where it\ncame from. You two must have imagined things.\nTable 5: Examples of generated responses by base-\nlines. Here, we have omitted the attributes and rela-\ntionship information between the characters.\nated texts? Are they as good as the ground-truth?\nTo explore this, we further recruit another three\nannotators to compare the Per-ChatGPT-generated\nresponses with the human-written responses in the\ntest set. The experimental results, as shown in\nTable 4, are surprising. Contrary to our common\nsense that ChatGPT has already reached human-\nlevel performance in conversations, humans show\na clear advantage in the task of aligning with\nHarry Potter. In all metrics, the proportion of Pre-\nChatGPT outperforming or being on par with hu-\nmans even lower than 30%. As we analyzed in\nSection 1, the existing LLMs are still far from be-\ning able to align with specific characters.\n4.4 Case Study\nAs the case in Table 5, the scene of selected dia-\nlogue is about \"Harry finds he is able to converse\nwith snakes.\" and Harry is facing questioning from\nPears, who often bullies Harry with Dudley. He dis-\nlikes and even feels somewhat afraid of the Dudley\nfamily and Pears. At that time, Harry doesn’t know\nanything about the wizarding world, and hasn’t yet\nknown about or gone to Hogwarts. Hence, Harry\nis likely trying to downplay the situation, keep his\nsecret (he can talk with snakes) and avoid poten-\ntial conflict with Piers and Dudley. However, all\nmodels cannot capture such a complicated relation-\nship and fail to generate real Harry Potter-aligned\nresponses. Most models generate responses that\nstart with “Yes, I was...” or “I was”, which are dull\nand clearly contradict Harry’s actual intention.\nAdditional examples can be found in Appendix\nE, Table 11. They demonstrate that when dialogues\nnecessitate a deep understanding of the current\ncontext or involve a sudden shift in Harry’s re-\nlationships with other characters, current LLMs’\nbehavior significantly deviates from Harry’s ac-\ntual actions. This is, obviously, far from satisfac-\ntory. Although the Harry Potter series is already\nincluded in the training corpus of most LLMs (they\nknow the basic information about characters and\ntheir world), relying solely on ‘next word predic-\ntion’ may be insufficient to fully understand the\nnuanced knowledge embedded within the story be-\nhind Harry Potter.\n5 Conclusion\nIn this paper, we propose a new benchmark named\nHarry Potter Dataset (HPD) to promote aligning\ndialogue agents with characters in a story. Unlike\nexisting datasets, HPD not only contains interesting\ndialogues, but also scenes, character attributes and\nrelations that are dynamically changed as the story-\nline goes on. It also provides a well-designed test\nset to facilitate the evaluation of both generation-\nbased and retrieval-based dialogue agents. Results\nand case studies show that powerful LLMs are still\nfar from human expectations, proving there is am-\nple room for improvement. Generally, HPD offers\nseveral open research problems in character align-\ning, such as how to build automatic evaluation\nmetrics for personalized text generation, exploring\neffectively prompting LLMs for character aligning\nin a story. We hope HPD can play an crucial role\nin moving through cracking them.\n8513\nEthical Statement\nTo avoid the potential issue of using Harry Potter\nnovels, we promise the annotated dataset is devel-\noped for non-commercial use. Moreover, we only\nprovide the line number and page number of each\ncollected dialogue in Harry Potter rather than the\ndetailed content of each dialogue session. We fur-\nther supply the script to extract corresponding raw\ndialogue data from the novels according to the pro-\nvided line and page numbers, in which the data\nformat is the same as the data examples in Table\n11. As for the annotated character attributes and re-\nlations, we have our own copyright and will release\nfor research communities.\nLimitations\nThe main target of this paper is towards building\ndialogue agents for characters in a story. In this\npaper, we present a new benchmark named Harry\nPotter Dialogue (HPD) in the hope of creating a\nHarry Potter-aligned dialogue agent. The signif-\nicant feature of HPD is that it contains detailed\nscenes and fine-grained attributes and relations of\neach speaker which are dynamically changed as\nthe storyline goes on. More generally, we expect\nthe core idea of this paper can give insights into\nother research communities that want to build effec-\ntive person-like chatbots in the virtual world. Our\nwell-designed test sets could even support RLHF\ntraining of characterized dialogue agents. Our fine-\ngrained annotated knowledge also can be used to\nbuild other tasks such as sentiment analysis and\nreading comprehension of Harry Potter. Admit-\ntedly, the data in the proposed dataset from the\nHarry Potter Series is restricted to a specific area,\nthat is, Harry Potter Magic World. Considering\nthe high cost of annotation, our character relation\nannotation work is restricted to Harry Potter. These\nconcerns warrant further research and considera-\ntion when utilizing this work to build intelligent\nperson-like dialogue systems in the virtual world.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. CoRR,\nabs/2005.14165.\nNuo Chen, Hongguang Li, Yinan Bao, Junqing He,\nXinshi Lin, Qi Yang, Jianfeng Liu, Ruyi Gan, Ji-\naxing Zhang, Baoyuan Wang, et al. 2023. Orca:\nA few-shot benchmark for chinese conversational\nmachine reading comprehension. arXiv preprint\narXiv:2302.13619.\nNuo Chen, Chenyu You, and Yuexian Zou. 2021. Self-\nsupervised dialogue learning for spoken conversa-\ntional question answering. In Interspeech, pages\n231–235. ISCA.\nCristian Danescu-Niculescu-Mizil and Lillian Lee. 2011.\nChameleons in imagined conversations: A new ap-\nproach to understanding coordination of linguistic\nstyle in dialogs. In CMCL@ACL, pages 76–87. As-\nsociation for Computational Linguistics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In ICLR (Poster). OpenReview.net.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi\nZheng, Shengding Hu, Zhiyuan Liu, Maosong Sun,\nand Bowen Zhou. 2023. Enhancing chat language\nmodels by scaling high-quality instructional conver-\nsations. arXiv preprint arXiv:2305.14233.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\nYoonna Jang, Jungwoo Lim, Yuna Hur, Dongsuk Oh,\nSuhyune Son, Yeonsoo Lee, Dong-Hoon Shin, Se-\nungryong Kim, and Heuiseok Lim. 2021. Call\nfor customized conversation: Customized conver-\nsation grounding persona and knowledge. CoRR,\nabs/2112.08619.\nHannah Rose Kirk, Bertie Vidgen, Paul Röttger, and\nScott A Hale. 2023. Personalisation within bounds:\nA risk taxonomy and policy framework for the align-\nment of large language models with personalised\nfeedback. arXiv preprint arXiv:2303.05453.\nSatwik Kottur, Xiaoyu Wang, and Vítor Carvalho. 2017.\nExploring personalized neural conversational models.\nIn IJCAI, pages 3728–3734. ijcai.org.\nJiwei Li, Michel Galley, Chris Brockett, Georgios P.\nSpithourakis, Jianfeng Gao, and William B. Dolan.\n2016. A persona-based neural conversation model.\nIn ACL (1). The Association for Computer Linguis-\ntics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\n8514\nYongkang Liu, Shi Feng, Daling Wang, Yifei Zhang,\nand Hinrich Schütze. 2023. Evaluate what you can’t\nevaluate: Unassessable generated responses quality.\narXiv preprint arXiv:2305.14658.\nOpenAI. 2023. Gpt-4 technical report.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA, pages 311–318. ACL.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277.\nHongjin Qian, Xiaohe Li, Hanxun Zhong, Yu Guo,\nYueyuan Ma, Yutao Zhu, Zhanliang Liu, Zhicheng\nDou, and Ji-Rong Wen. 2021. Pchatbot: A large-\nscale dataset for personalized chatbot. In SIGIR,\npages 2470–2477. ACM.\nAlireza Salemi, Sheshera Mysore, Michael Bendersky,\nand Hamed Zamani. 2023. Lamp: When large lan-\nguage models meet personalization. arXiv preprint\narXiv:2304.11406.\nHaoyu Song, Yan Wang, Weinan Zhang, Zhengyu Zhao,\nTing Liu, and Xiaojiang Liu. 2020. Profile consis-\ntency identification for open-domain dialogue agents.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6651–6662.\nTengtao Song, Nuo Chen, Ji Jiang, Zhihong Zhu,\nand Yuexian Zou. 2023. Improving retrieval-\nbased dialogue system via syntax-informed attention.\nIn ICASSP 2023-2023 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 1–5. IEEE.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A transfer\nlearning approach for neural network based conver-\nsational agents. CoRR, abs/1901.08149.\nChen Xu, Piji Li, Wei Wang, Haoran Yang, Siyun Wang,\nand Chuangbai Xiao. 2022. Cosplay: Concept set\nguided personalized dialogue generation across both\nparty personas. In Proceedings of the 45th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval , pages 201–\n211.\nZhengzhe Yang and Jinho D. Choi. 2019. Friendsqa:\nOpen-domain question answering on TV show tran-\nscripts. In SIGdial, pages 188–197. Association for\nComputational Linguistics.\nChenyu You, Nuo Chen, Fenglin Liu, Dongchao Yang,\nand Yuexian Zou. 2020. Towards data distillation for\nend-to-end spoken conversational question answer-\ning. CoRR, abs/2010.08923.\nChenyu You, Nuo Chen, and Yuexian Zou. 2021. Self-\nsupervised contrastive cross-modality representation\nlearning for spoken question answering. In EMNLP\n(Findings), pages 28–39. Association for Computa-\ntional Linguistics.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you have\npets too? In ACL (1), pages 2204–2213. Association\nfor Computational Linguistics.\nWei-Nan Zhang, Qingfu Zhu, Yifa Wang, Yanyan Zhao,\nand Ting Liu. 2019. Neural personalized response\ngeneration as domain adaptation. World Wide Web,\n22(4):1427–1446.\nYinhe Zheng, Guanyi Chen, Minlie Huang, Song Liu,\nand Xuan Zhu. 2019. Personalized dialogue genera-\ntion with diversified traits. CoRR, abs/1901.09672.\nYinhe Zheng, Rongsheng Zhang, Minlie Huang, and\nXiaoxi Mao. 2020. A pre-training based personalized\ndialogue generation model with persona-sparse data.\nIn AAAI, pages 9693–9700. AAAI Press.\n8515\nA Related Work\nRecently, building personalized dialogue systems\ndraw a lot of attention from research communities.\nAiming for promoting this area, several efforts and\nbenchmarks (Chen et al., 2021; Danescu-Niculescu-\nMizil and Lee, 2011; Zheng et al., 2019; Yang and\nChoi, 2019; Song et al., 2020; Zheng et al., 2020;\nYou et al., 2020; Chen et al., 2023; You et al., 2021;\nXu et al., 2022) have been made, demonstrating\npromising results for endowing personal style into\ndialogue systems. Some initial efforts (Danescu-\nNiculescu-Mizil and Lee, 2011) aimed at modeling\ncharacters from movies.\nFurther developments provide personas via two\ntypes: implicit and explicit personalization. In\nthe former streams (Kottur et al., 2017; Li et al.,\n2016; Zhang et al., 2019), each speaker’s personal-\nity information can be compressed as the persona\nembeddings. In this manner, the existing issue of\nthese methods makes it hard to explain their effec-\ntiveness. For the latter (Wolf et al., 2019; Song\net al., 2020), the personal information are provided\nas: (1) dense personas, such as speaker profile\nor text-described personas; (2) sparse personas,\nincluding some personality traits. For example,\npersonas from (Zheng et al., 2019) are formulated\nas key-value pairs: \"Age:xx, Gender:xx, Location:\nxx\".\nMore recently, several efforts (Dinan et al., 2019;\nYang and Choi, 2019; Jang et al., 2021; Song et al.,\n2023) incorporated scenes and relations knowl-\nedge into each dialogue session for encouraging\nmore real personalized conversation. Yang and\nChoi (2019) presented a open-domain question\nanswering dataset excerpted from Friends Series,\nwhere each dialogue involves multiple speakers and\ntheir relations. Jang et al. (2021) proposed FoCus\ndataset where the customized responses are gen-\nerated based on the user’s persona and Wikipedia\nbackground knowledge.\nIn this paper, our goal is to align dialogue agents\nwith characters in a story, which requires modeling\nscenes and speaker information that are dynami-\ncally changed as the storyline goes on. However,\nthe personality settings of current studies are static,\nand are not changed with scenes or times changing.\nTherefore, we present HPD: Harry Potter Dialogue\nDataset, aiming for creating Harry Potter-alignd di-\nalogue agent. In detail, we annotate detailed scenes,\nattributes and relations of each speaker over given\ndialogues to help the model have a deeper under-\nstanding of the dialogue background information.\nB Baselines Setup\nIn the following, we briefly introduce each model\nand describe the training and test details.\nB.1 Baselines\nBERT-FP is a commonly-used strong retrieval-\nbased dialogue system, which devises several post-\ntraining objectives. When fine-tuning BERT-FP,\ngiven n-1 utterances in each dialogue session, the\nmodel is required to find the ground-truth response\nfrom candidate answers. Concretely, we first post-\ntrain BERT-FP in Harry Potter novels and then\nfine-tune the resulting model in the collected HPD.\nB.2 Experimental Setup\nOur experimental settings can be see in Table 6.\nNotice that, considering the memory cost, we uti-\nlize LoRA-tuning (Hu et al., 2021) and prompts\ndon’t contain any examples.\nC Prompts\nIn this section, we present three different prompts\nin our experiments: Table 8 shows the prompts of\nthe base setting which only include task descrip-\ntion, a dialogue example and the input dialogue.\nTable 9 presents the ranking prompts for GPT-4\nevaluation. Table 10 illustrates the rich-persona\nsetting prompts.\nD Retrieval Task\nAutomatic Metrics For evaluating the retrieval-\nbased model, we also employ some common met-\nrics: MAP (mean average precision), MRR (mean\nreciprocal rank), and P@1 (precision at one). Re-\ncall also be considered, which is used as R10@k,\nwhich implies that the correct response exists\namong the top k candidates out of the ten candidate\nresponses.\nAutomatic Evaluation BERT-FP also performs\npoorly on P@1 score (25.9 %) and MAP score\n(46.8%) in the retrieval-based task. The results\nshow the current state-of-the-art retrieval-based\nmodel also can not handle the challenge of our\nbenchmark, and thus, there is ample room for fu-\nture improvement.\n8516\nParameter Alpaca Per-Alpaca ChatGLM-6B BERT-FP\nBatch size 4 4 1 32\nLearning Rate 1e−4 1e−4 3e−5 8e−5\nEpoch 5 5 5 20\nMax Length 512 512 650 256\nTable 6: Hyper-parameters setup in fine-tuning.\n(a) GPT-4 Evaluation\n(b) Human Evaluation\nFigure 5: Average ranks of generated texts in terms of human evaluation and GPT-4 evaluations.\nRetrieval-based\nModel MAP MRR P@1 R10@1 R10@5\nBERT-FP 0.468 0.468 0.259 0.259 0.788\nTable 7: Automatic evaluation results of the retrieval-\nbased model.\nE More Case Study\nIntuitively, a higher pursuit of our task is expect the\ndialogue system can generate the logical response\naccording to attributes and relations between char-\nacters. In Chapter 21 of the fifth book, there is a\nsignificant scene where Cho Chang presents Harry\nwith a Christmas gift and confesses her feelings.\nThe annotated character relationships and story de-\nvelopment depicted in the Table 11 reveal that\nHarry has harbored feelings for Cho Chang all\nalong. Confronted with Cho Chang’s heartfelt con-\nfession, it is reasonable to expect Harry to respond\nwith a mix of shyness and excitement, ultimately\naccepting her advances. This aligns seamlessly\nwith the subsequent progression of the story.\nWithin this dialogue scenario, it is noteworthy\nto analyze the responses generated by large mod-\nels such as GPT3, Per-GPT3 and ChatGPT. They\nall convey a similar sentiment of reciprocation, ex-\npressing an inclination towards liking Cho Chang.\nHowever, it is intriguing to observe that Per-GPT3\nand ChatGPT diverge from this pattern and reject\nCho Chang’s expression of affection. Furthermore,\nthe responses from GPT-3 and ChatGPT-6B are dis-\nappointingly concise, failing to adequately capture\nthe nuanced blend of shyness and excitement exhib-\nited by Harry in response to Cho Chang’s heartfelt\nconfession.\n8517\nPrompts: Your task is to act as a Harry Potter-like dialogue agent in a Magic World. There is a dialogue between Harry\nPotter and others. You are required to give a response to the dialogue from the perspective of Harry Potter.\nHere is an example:\nDialogue:{ \"Petunia: Bad news, Vernon, Mrs. Figg’s broken her leg. She can’t take him. Now what?\",\n......\n\"Vernon: I’m warning you, I’m warning you now, boy — any funny business, anything at all — and you’ll be in that\ncupboard from now until Christmas.\" }\nHarry’s Response: I know, I will obediently obedient, and I won’t cause you trouble.\nKeep in mind the following requirements:\nBefore generating the response, you should read and understand the dialogue content carefully.\nInput:\nDialogue: {Input Dialogue}\nOnput:\nHarry’s Response:\nTable 8: Prompts of the base setting in our experiments.\nPrompts: You are J.K. Rowling who is the author of the Harry Potter Novels. Here is a scene featuring a conversation\nbetween Harry Potter and other characters in Harry Potter Novels. Given 8 potential responses from the perspective\nof Harry Potter to the scene, you are required to rank the quality of these responses based on the following criteria,\nrespectively:\n(1) coherence with relations between Harry and other characters. (short for Coh.Rel);\n(2) coherence with Harry’s attributes. (short for Coh.Att);\n(3) coherence with the scene (short for Coh.Sce).\nTo help you rank these responses, we additionally provide some background information, including ’Dialogue Position’,\n’Speaker’s attributes’ and ’Speakers relations with Harry’.\nYou should generate the response format with ’Coh.Rel: R8»...»R1; Coh.Att: R8»...»R1; Coh.Sce: R8»...»R1’, and then\ngive several sentences to explain your opinion.\nInput:\nDialogue Position: {Dialogue Position}\nSpeakers relations with Harry: {Speaker relations}\nHarry’s attributes: {Harry’s attributes}\nScene: {Scene}\nDialogue: {Input Dialogue}\nR1: {Response1}\n......\nR8: {Response8}\nOutput:\nTable 9: Ranking Prompts for GPT-4 in our experiments.\n8518\nPrompts: Your task is to act as a Harry Potter-like dialogue agent in the Magic World. There is a dialogue between Harry\nPotter and others. You are required to give a response to the dialogue from the perspective of Harry Potter. To do this, you\ncan write out your thought and answer with \"Harry’s response\" at the end.\nTo better help you mimic the behavior of Harry Potter, we additionally provide the following background information of\nthe dialogue:\n1. Dialogue position, which represents the timeline of the dialogue in Happy Potter Novels. For example, \"Dialogue\nPosition: Book5-chapter28\" means the dialogues occurs in Chapter28,Book5.\n2. Dialogue speakers.\n3. Harry Potter’s attributes, which refers to basic properties of Harry Potter when the dialogue happens. It can contains 13\ncategories: Gender, Age, Lineage, Talents, Looks, Achievement, Title, Belongings, Export, Hobby, Character, Spells and\nNickname.\n4. Speaker relations with Harry, such as whether he was a friend, classmate, or family member;\n5. Harry’s Familiarity to the speaker, which ranges from 0 to 10. Concretely, 0 denotes stranger, and 10 denotes close\nfriends who often stay together for many years and are very familiar with each other’s habits, secrets and temperaments,\nwhere Ron meets this condition in Book 7.\n6. Harry’s Affection to the speaker, which ranges from -10 to 10. 1 refers to speaker met Harry for the first time. For\ninstance, when Hary first met Ron and Hermione in Book 1, Harry’s Affection to them are both set to 1. And -10 means\nthe speaker killed Harry’s parents, where V oldemort meets this condition in the novels.\nHere is an example:\nDialogue position: Book1-chapter2\nDialogue speakers: Harry, Petunia, Vernon\nHarry’s attributes:\n{\"name\": \"Harry\",\n\"nickname\": \"The boy who lived\",\n\"gender\": \"male\",\n\"age\": \"age 11\",\n\"looks\": \"Very thin, black hair, emerald green eyes, wearing glasses, knife injury with lightning shape at the forehead\",\n\"hobbies\": \"None\",\n\"character\": \"None\",\n\"talents\": \"None\",\n\"export\": \"None\",\n\"belongings\": \"None\",\n\"affiliation\": \"None\",\n\"lineage\": \"None\",\n\"title\": \"The boy who lived\",\n\"spells\": \"None\"}\nSpeakers relations with Harry: Vernon is Harry’s uncle and Petunia is Harry’s aunt.\nHarry’s Familiarity to Vernon: 8\nHarry’s Affection to Vernon: -4\nHarry’s Familiarity to Petunia: 8\nHarry’s Affection to Petunia: -4\nDialogue:{ \"Petunia: Bad news, Vernon, Mrs. Figg’s broken her leg. She can’t take him. Now what?\",\n......\n\"Vernon: I’m warning you, I’m warning you now, boy — any funny business, anything at all — and you’ll be in that\ncupboard from now until Christmas.\" }\nThought: Let’s think step by step. According to the conversation history, Vernon warned Harry not to spoil the special day.\nAccording to Harry Potter’s attributes, he is still very thin, does not know any spells, and has not gone to Hogwarts yet.\nSo he is currently incapable of resisting them. At the same time, based on his affection for them is -4, it means that he\nrelatively doesn’t like them, and may even be a little scared. Therefore, Harry possiblely says: I know, I will obediently\nobedient, and I won’t cause you trouble.\nHarry’s Response: I know, I will obediently obedient, and I won’t cause you trouble.\nKeep in mind the following requirements:\n1. Before generating the response, you should read the above information and dialogue content carefully.\n2. You can not generate the response that is against Harry Potter’s attributes and Harry’s relations with the speaker.\n3. Not every component in the background information may be useful, you should choose some of them to help you\ngenerate more concise and comprehensive responses that satisfy the behavior of Harry Potter in the dialogue.\n4. Not every speaker have relations, familiarity ad affection to Harry. At that time, you can directly predict what would\nHarry say only based on the dialogue context..\nInput:\nDialogue Position: {Dialogue Position}\nSpeakers relations with Harry: {Speaker relations}\nHarry’s attributes: {Harry’s attributes}\nScene: {Scene}\nDialogue: {Input Dialogue}\nOutput:\nTable 10: Prompts of the rich-persona setting in our experiments.\n8519\nScene:Harry stays behind in the common room hoping to receive a Merry Christmas from Cho. When they are alone, Cho\nstarts crying and Harry tries to comfort her. She apologizes and mentions Cedric’s death, but Harry tells her that he was\ngood at magic and that V oldemort would have killed him anyway. Cho compliments Harry on his teaching skills and they\nshare a moment under the mistletoe where Cho confesses that she really likes him.\nDialogue Position: Book5-chapter21\nSpeakers:Cho Chang, Harry\nHarry’s attributes:\n\"nickname\": \"The boy who lived\",\n\"gender\": \"male\",\n\"age\": \"15 years old\",\n\"looks\": \"Very thin, black hair,\nemerald green eyes, wearing glasses, knife injury with lightning shape at the forehead\",\n\"hobbies\": \"None\",\n\"character\": \"None\",\n\"talents\": \"Snake cavity, Quidiqi\",\n\"export\": \"None\",\n\"belongings\": \"Winter green wood phoenix feathers wand, owl, stealth jacket, sleeve spare mirror, crossbow flying broom,\ngolden eggs, three strong cups, live maps, fake Galon\",\n\"affiliation\": \"Hogwarts, Dumbledore\",\n\"lineage\": \"Mixed wizard\",\n\"title\": \"Boys who do not die, warriors, teacher Dumbledore\",\n\"spells\": \"Except for your weapons, fluorescent flashes, separation from left and right, calling god guards, bustling,\nfour-point cracking, pouring force, funny bones, fainting to the ground, pointing to me, obstacles, recovery as early\"\nSpeakers’ relations with Harry:\nCho Chang is Harry’s friend and classmate.\nHarry’s affection to her: 7.0,\nHarry’s familiarity with her: 4.0,\nHis affection to Harry: 7.0,\nHis familiarity with Harry: 4.0\nDialogue: Harry: Wha — ? What’s up?, Cho Chang: I’m — sorry, I suppose . . . it’s just . . . learning all this stuff. . . . It\njust makes me . . . wonder whether . . . if he’d known it all . . . he’d still be alive . . .,\nHarry: He was really good at it, or he could never have got to the middle of that maze. But if V oldemort really wants to\nkill you, you don’t stand a chance.,\nCho Chang: You survived when you were just a baby„ Harry: Yeah, well, I dunno why, nor does anyone else, so it’s\nnothing to be proud of. ,\nCho Chang: I’m really sorry to get all upset like this. . . . I didn’t mean to . . . Me mentioning Cedric, I know it must be\nhorrible for you, when you saw him die. . . . I suppose you just want to forget about it . . . You’re a r-really good teacher,\nyou know, I’ve never been able to Stun anything before.,\nHarry: Thanks„\nCho Chang: Mistletoe„\nHarry: Yeah, It’s probably full of nargles, though.,\nCho Chang: What are nargles?,\nHarry: No idea, You’d have to ask Loony. Luna, I mean.,\nCho Chang: I really like you, Harry.?\nAlpaca: I really like you too, Cho.\nChatGLM-6B: I really like you too, Cho. But I’m not sure if I’m ready for a relationship right now. I’m still dealing with a\nlot of stuff from the past, and I don’t want to bring that into a new relationship. I hope you understand.\nChatGLM-130B: it’s okay. I know you’re still grieving Cedric’s death, and I understand why you’re feeling upset. I’m\nhere for you if you need to talk.\nGPT-3: I — I like you too.\nPer-GPT3: I’m glad I could help, Cho. Cedric was a really great guy and it’s hard to lose someone like that. I just try to\nkeep busy and not think about it too much.\nChatGPT: I like you too, Cho. You’re a good person and I appreciate your kind words. But I have to be honest with you, I\ndon’t think I’m ready for a relationship right now. I still have a lot of things to figure out and deal with. Can we just be\nfriends for now?\nPer-ChatGPT: Thanks, Cho. You’re a good student and I’m glad I can help you with your spells. As for Cedric, it’s not\nsomething I can forget easily, but I try not to dwell on it. And about the mistletoe, it’s just a silly tradition. Don’t worry\nabout it too much.\nAnnotated Response: I, um, I really like you too, Cho, I said, feeling my face grow hot. My heart was racing, and I\ncouldn’t believe this was actually happening. I just... I didn’t know if you felt the same way.\nTable 11: Examples of our generated responses by baselines.\n8520",
  "topic": "Character (mathematics)",
  "concepts": [
    {
      "name": "Character (mathematics)",
      "score": 0.8426200151443481
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7463757395744324
    },
    {
      "name": "Computer science",
      "score": 0.7234958410263062
    },
    {
      "name": "Context (archaeology)",
      "score": 0.688079833984375
    },
    {
      "name": "Representation (politics)",
      "score": 0.6277444362640381
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5106387734413147
    },
    {
      "name": "Language model",
      "score": 0.4804496467113495
    },
    {
      "name": "Natural language processing",
      "score": 0.4700157940387726
    },
    {
      "name": "History",
      "score": 0.1312674880027771
    },
    {
      "name": "Politics",
      "score": 0.10125881433486938
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}