{
  "title": "SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks",
  "url": "https://openalex.org/W3036737467",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221478547",
      "name": "Fuchs, Fabian B.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226660942",
      "name": "Worrall, Daniel E.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118204256",
      "name": "Fischer Volker",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221603798",
      "name": "Welling, Max",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2788775653",
    "https://openalex.org/W2998658430",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2793179281",
    "https://openalex.org/W2963057320",
    "https://openalex.org/W2970661753",
    "https://openalex.org/W2948196881",
    "https://openalex.org/W2970663744",
    "https://openalex.org/W2963370555",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W2995360807",
    "https://openalex.org/W2982319484",
    "https://openalex.org/W3034459762",
    "https://openalex.org/W2798270772",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2080635178",
    "https://openalex.org/W2979240068",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W2963711743",
    "https://openalex.org/W2963386218",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2913668534",
    "https://openalex.org/W2963464736",
    "https://openalex.org/W2797461639",
    "https://openalex.org/W3102380997",
    "https://openalex.org/W2970754912",
    "https://openalex.org/W2981440248",
    "https://openalex.org/W3138516063",
    "https://openalex.org/W2964213081",
    "https://openalex.org/W2576915720",
    "https://openalex.org/W2082841340",
    "https://openalex.org/W2948107928",
    "https://openalex.org/W2963461379",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3035684957",
    "https://openalex.org/W2963158438",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3088547251",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3035702705"
  ],
  "abstract": "We introduce the SE(3)-Transformer, a variant of the self-attention module for 3D point clouds and graphs, which is equivariant under continuous 3D roto-translations. Equivariance is important to ensure stable and predictable performance in the presence of nuisance transformations of the data input. A positive corollary of equivariance is increased weight-tying within the model. The SE(3)-Transformer leverages the benefits of self-attention to operate on large point clouds and graphs with varying number of points, while guaranteeing SE(3)-equivariance for robustness. We evaluate our model on a toy N-body particle simulation dataset, showcasing the robustness of the predictions under rotations of the input. We further achieve competitive performance on two real-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms a strong, non-equivariant attention baseline and an equivariant model without attention.",
  "full_text": "SE(3)-Transformers: 3D Roto-Translation\nEquivariant Attention Networks\nFabian B. Fuchs∗†\nBosch Center for Artiﬁcial Intelligence\nA2I Lab, Oxford University\nfabian@robots.ox.ac.uk\nDaniel E. Worrall*\nAmsterdam Machine Learning Lab, Philips Lab\nUniversity of Amsterdam\nd.e.worrall@uva.nl\nVolker Fischer\nBosch Center for Artiﬁcial Intelligence\nvolker.fischer@de.bosch.com\nMax Welling\nAmsterdam Machine Learning Lab\nUniversity of Amsterdam\nm.welling@uva.nl\nAbstract\nWe introduce the SE(3)-Transformer, a variant of the self-attention module for\n3D point clouds and graphs, which is equivariant under continuous 3D roto-\ntranslations. Equivariance is important to ensure stable and predictable perfor-\nmance in the presence of nuisance transformations of the data input. A positive\ncorollary of equivariance is increased weight-tying within the model. The SE(3)-\nTransformer leverages the beneﬁts of self-attention to operate on large point clouds\nand graphs with varying number of points, while guaranteeing SE(3)-equivariance\nfor robustness. We evaluate our model on a toy N-body particle simulation dataset,\nshowcasing the robustness of the predictions under rotations of the input. We fur-\nther achieve competitive performance on two real-world datasets, ScanObjectNN\nand QM9. In all cases, our model outperforms a strong, non-equivariant attention\nbaseline and an equivariant model without attention.\n1 Introduction\nSelf-attention mechanisms [31] have enjoyed a sharp rise in popularity in recent years. Their relative\nimplementational simplicity coupled with high efﬁcacy on a wide range of tasks such as language\nmodeling [31], image recognition [ 18], or graph-based problems [ 32], make them an attractive\ncomponent to use. However, their generality of application means that for speciﬁc tasks, knowledge\nof existing underlying structure is unused. In this paper, we propose the SE(3)-Transformer shown in\nFig. 1, a self-attention mechanism speciﬁcally for 3D point cloud and graph data, which adheres to\nequivariance constraints, improving robustness to nuisance transformations and general performance.\nPoint cloud data is ubiquitous across many ﬁelds, presenting itself in diverse forms such as 3D\nobject scans [29], 3D molecular structures [21], or N-body particle simulations [14]. Finding neural\nstructures which can adapt to the varying number of points in an input, while respecting the irregular\nsampling of point positions, is challenging. Furthermore, an important property is that these structures\nshould be invariant to global changes in overall input pose; that is, 3D translations and rotations of\nthe input point cloud should not affect the output. In this paper, we ﬁnd that the explicit imposition\nof equivariance constraints on the self-attention mechanism addresses these challenges. The SE(3)-\nTransformer uses the self-attention mechanism as a data-dependent ﬁlter particularly suited for sparse,\nnon-voxelised point cloud data, while respecting and leveraging the symmetries of the task at hand.\n∗equal contribution\n†work done while at the Bosch Center for Artiﬁcial Intelligence\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2006.10503v3  [cs.LG]  24 Nov 2020\nmap pool\nlearned per \npoint features equivariant\ninvariant\nA B\nFigure 1: A) Each layer of the SE(3)-Transformer maps from a point cloud to a point cloud (or graph\nto graph) while guaranteeing equivariance. For classiﬁcation, this is followed by an invariant pooling\nlayer and an MLP. B) In each layer, for each node, attention is performed. Here, the red node attends\nto its neighbours. Attention weights (indicated by line thickness) are invariant w.r.t. input rotation.\nSelf-attention itself is a pseudo-linear map between sets of points. It can be seen to consist of\ntwo components: input-dependent attention weights and an embedding of the input, called a value\nembedding. In Fig. 1, we show an example of a molecular graph, where attached to every atom we\nsee a value embedding vector and where the attention weights are represented as edges, with width\ncorresponding to the attention weight magnitude. In the SE(3)-Transformer, we explicitly design the\nattention weights to be invariant to global pose. Furthermore, we design the value embedding to be\nequivariant to global pose. Equivariance generalises the translational weight-tying of convolutions. It\nensures that transformations of a layer’s input manifest as equivalent transformations of the output.\nSE(3)-equivariance in particular is the generalisation of translational weight-tying in 2D known from\nconventional convolutions to roto-translations in 3D. This restricts the space of learnable functions to\na subspace which adheres to the symmetries of the task and thus reduces the number of learnable\nparameters. Meanwhile, it provides us with a richer form of invariance, since relative positional\ninformation between features in the input is preserved.\nThe works closest related to ours are tensor ﬁeld networks (TFN) [28] and their voxelised equivalent,\n3D steerable CNNs [37]. These provide frameworks for building SE(3)-equivariant convolutional\nnetworks operating on point clouds. Employing self-attention instead of convolutions has several\nadvantages. (1) It allows a natural handling of edge features extending TFNs to the graph setting.\n(2) This is one of the ﬁrst examples of a nonlinear equivariant layer. In Section 3.2, we show our\nproposed approach relieves the strong angular constraints on the ﬁlter compared to TFNs, therefore\nadding representational capacity. This constraint has been pointed out in the equivariance literature to\nlimit performance severely [36]. Furthermore, we provide a more efﬁcient implementation, mainly\ndue to a GPU accelerated version of the spherical harmonics. The TFN baselines in our experiments\nleverage this and use signiﬁcantly scaled up architectures compared to the ones used in [28].\nOur contributions are the following:\n• We introduce a novel self-attention mechanism, guaranteeably invariant to global rotations\nand translations of its input. It is also equivariant to permutations of the input point labels.\n• We show that the SE(3)-Transformer resolves an issue with concurrent SE(3)-equivariant\nneural networks, which suffer from angularly constrained ﬁlters.\n• We introduce a Pytorch implementation of spherical harmonics, which is 10x faster than\nScipy on CPU and 100 −1000×faster on GPU. This directly addresses a bottleneck of\nTFNs [28]. E.g., for a ScanObjectNN model, we achieve ≈22×speed up of the forward\npass compared to a network built with SH from the lielearn library (see Appendix C).\n• Code available at https://github.com/FabianFuchsML/se3-transformer-public\n2 Background And Related Work\nIn this section we introduce the relevant background materials on self-attention, graph neural networks,\nand equivariance. We are concerned with point cloud based machine learning tasks, such as object\nclassiﬁcation or segmentation. In such a task, we are given a point cloud as input, represented as a\ncollection of ncoordinate vectors xi ∈R3 with optional per-point features fi ∈Rd.\n2.1 The Attention Mechanism\nThe standard attention mechanism [31] can be thought of as consisting of three terms: a set of\nquery vectors qi ∈Rp for i= 1,...,m , a set of key vectors kj ∈Rp for j = 1,...,n , and a set of\n2\nvalue vectors vj ∈Rr for j = 1,...,n , where rand pare the dimensions of the low dimensional\nembeddings. We commonly interpret the key kj and the value vj as being ‘attached’ to the same\npoint j. For a given query qi, the attention mechanism can be written as\nAttn (qi,{kj},{vj}) =\nn∑\nj=1\nαijvj, α ij = exp(q⊤\ni kj)∑n\nj′=1 exp(q⊤\ni kj′ ) (1)\nwhere we used a softmax as a nonlinearity acting on the weights. In general, the number of query\nvectors does not have to equal the number of input points [16]. In the case of self-attention the query,\nkey, and value vectors are embeddings of the input features, so\nq = hQ(f), k = hK(f), v = hV(f), (2)\nwhere {hQ,hK,hV}are, in the most general case, neural networks [30]. For us, query qiis associated\nwith a point iin the input, which has a geometric location xi. Thus if we have npoints, we have n\npossible queries. For query qi, we say that node iattends to all other nodes j ̸= i.\nMotivated by a successes across a wide range of tasks in deep learning such as language modeling\n[31], image recognition [18], graph-based problems [32], and relational reasoning [30, 9], a recent\nstream of work has applied forms of self-attention algorithms to point cloud data [44, 42, 16]. One\nsuch example is the Set Transformer [16]. When applied to object classiﬁcation on ModelNet40 [ 41],\nthe input to the Set Transformer are the cartesian coordinates of the points. Each layer embeds this\npositional information further while dynamically querying information from other points. The ﬁnal\nper-point embeddings are downsampled and used for object classiﬁcation.\nPermutation equivariance A key property of self-attention is permutation equivariance. Permuta-\ntions of point labels 1,...,n lead to permutations of the self-attention output. This guarantees the\nattention output does not depend arbitrarily on input point ordering. Wagstaff et al. [33] recently\nshowed that this mechanism can theoretically approximate all permutation equivariant functions. The\nSE(3)-transformer is a special case of this attention mechanism, inheriting permutation equivariance.\nHowever, it limits the space of learnable functions to rotation and translation equivariant ones.\n2.2 Graph Neural Networks\nAttention scales quadratically with point cloud size, so it is useful to introduce neighbourhoods:\ninstead of each point attending to all other points, it only attends to its nearest neighbours. Sets with\nneighbourhoods are naturally represented as graphs. Attention has previously been introduced on\ngraphs under the names of intra-, self-, vertex-, or graph-attention [17, 31, 32, 12, 26]. These methods\nwere uniﬁed by Wang et al. [34] with the non-local neural network. This has the simple form\nyi = 1\nC({fj ∈Ni})\n∑\nj∈Ni\nw(fi,fj)h(fj) (3)\nwhere w and hare neural networks and Cnormalises the sum as a function of all features in the\nneighbourhood Ni. This has a similar structure to attention, and indeed we can see it as performing\nattention per neighbourhood. While non-local modules do not explicitly incorporate edge-features, it\nis possible to add them, as done in Veliˇckovi´c et al. [32] and Hoshen [12].\n2.3 Equivariance\nGiven a set of transformations Tg : V→V for g ∈G, where Gis an abstract group, a function\nφ: V→Y is called equivariant if for every gthere exists a transformation Sg : Y→Y such that\nSg[φ(v)] = φ(Tg[v]) for all g∈G,v ∈V. (4)\nThe indices gcan be considered as parameters describing the transformation. Given a pair (Tg,Sg),\nwe can solve for the family of equivariant functions φsatisfying Equation 4. Furthermore, if (Tg,Sg)\nare linear and the map φis also linear, then a very rich and developed theory already exists for ﬁnding\nφ[6]. In the equivariance literature, deep networks are built from interleaved linear maps φand\nequivariant nonlinearities. In the case of 3D roto-translations it has already been shown that a suitable\nstructure for φis a tensor ﬁeld network [28], explained below. Note that Romero et al. [24] recently\nintroduced a 2D roto-translationally equivariant attention module for pixel-based image data.\nGroup Representations In general, the transformations (Tg,Sg) are called group representations.\nFormally, a group representation ρ : G →GL(N) is a map from a group Gto the set of N ×N\n3\ninvertible matrices GL(N). Critically ρis a group homomorphism; that is, it satisﬁes the following\nproperty ρ(g1g2) = ρ(g1)ρ(g2) for all g1,g2 ∈G. Speciﬁcally for 3D rotations G = SO(3), we\nhave a few interesting properties: 1) its representations are orthogonal matrices, 2) all representations\ncan be decomposed as\nρ(g) = Q⊤\n[⨁\nℓ\nDℓ(g)\n]\nQ, (5)\nwhere Q is an orthogonal, N ×N, change-of-basis matrix [ 5]; each Dℓ for ℓ = 0 ,1,2,... is a\n(2ℓ+1) ×(2ℓ+1) matrix known as a Wigner-D matrix3; and the ⨁is the direct sum or concatenation\nof matrices along the diagonal. The Wigner-D matrices are irreducible representations of SO(3)—\nthink of them as the ‘smallest’ representations possible. Vectors transforming according toDℓ (i.e.\nwe set Q = I), are called type-ℓvectors. Type-0 vectors are invariant under rotations and type-1\nvectors rotate according to 3D rotation matrices. Note, type-ℓvectors have length 2ℓ+ 1. They can\nbe stacked, forming a feature vector f transforming according to Eq. (5).\nTensor Field Networks Tensor ﬁeld networks (TFN) [28] are neural networks, which map point\nclouds to point clouds under the constraint of SE(3)-equivariance, the group of 3D rotations and\ntranslations. For point clouds, the input is a vector ﬁeld f : R3 →Rd of the form\nf(x) =\nN∑\nj=1\nfjδ(x −xj), (6)\nwhere δis the Dirac delta function, {xj}are the 3D point coordinates and {fj}are point features,\nrepresenting such quantities as atomic number or point identity. For equivariance to be satisﬁed, the\nfeatures of a TFN transform under Eq. (5), where Q = I. Each fj is a concatenation of vectors of\ndifferent types, where a subvector of type-ℓis written fℓ\nj. A TFN layer computes the convolution of a\ncontinuous-in-space, learnable weight kernel Wℓk : R3 →R(2ℓ+1)×(2k+1) from type-kfeatures to\ntype-ℓfeatures. The type-ℓoutput of the TFN layer at position xi is\nfℓ\nout,i =\n∑\nk≥0\n∫\nWℓk(x′−xi)fk\nin(x′) dx′\n  \nk→ℓconvolution\n=\n∑\nk≥0\nn∑\nj=1\nWℓk(xj −xi)fk\nin,j,\n  \nnode j →node imessage\n(7)\nWe can also include a sum over input channels, but we omit it here. Weiler et al. [37], Thomas\net al. [28] and Kondor [15] showed that the kernel Wℓk lies in the span of an equivariant basis\n{Wℓk\nJ }k+ℓ\nJ=|k−ℓ|. The kernel is a linear combination of these basis kernels, where the Jth coefﬁcient is\na learnable function ϕℓk\nJ : R≥0 →R of the radius ∥x∥. Mathematically this is\nWℓk(x) =\nk+ℓ∑\nJ=|k−ℓ|\nϕℓk\nJ (∥x∥)Wℓk\nJ (x), where Wℓk\nJ (x) =\nJ∑\nm=−J\nYJm(x/∥x∥)Qℓk\nJm. (8)\nEach basis kernel Wℓk\nJ : R3 →R(2ℓ+1)×(2k+1) is formed by taking a linear combination of Clebsch-\nGordan matrices Qℓk\nJm of shape (2ℓ+ 1) ×(2k+ 1), where the J,mth linear combination coefﬁcient\nis the mth dimension of the Jth spherical harmonic YJ : R3 →R2J+1. Each basis kernel Wℓk\nJ\ncompletely constrains the form of the learned kernel in the angular direction, leaving the only\nlearnable degree of freedom in the radial direction. Note that Wℓk\nJ (0) ̸= 0 only when k = ℓand\nJ = 0, which reduces the kernel to a scalar wmultiplied by the identity, Wℓℓ = wℓℓI, referred to as\nself-interaction [28]. As such we can rewrite the TFN layer as\nfℓ\nout,i = wℓℓfℓ\nin,i  \nself-interaction\n+\n∑\nk≥0\nn∑\nj̸=i\nWℓk(xj −xi)fk\nin,j, (9)\nEq. (7) and Eq. (9) present the convolution in message-passing form, where messages are aggregated\nfrom all nodes and feature types. They are also a form of nonlocal graph operation as in Eq. (3),\nwhere the weights are functions on edges and the features {fi}are node features. We will later see\nhow our proposed attention layer uniﬁes aspects of convolutions and graph neural networks.\n3The ‘D’ stands forDarstellung, German for representation\n4\nStep 4: Compute attention and aggregate\nStep 3: Propagate queries, keys, and values to edges\nStep 2: Get SO(3)-equivariant weight matricesStep 1: Get nearest neighbours and relative positions\nSpherical\nHarmonics\nMatrix W consists of blocks mapping between degrees\nRadial Neural \nNetwork\nClebsch-\nGordon Coeff.\nFigure 2: Updating the node features using our equivariant attention mechanism in four steps. A\nmore detailed description, especially of step 2, is provided in the Appendix. Steps 3 and 4 visualise a\ngraph network perspective: features are passed from nodes to edges to compute keys, queries and\nvalues, which depend both on features and relative positions in a rotation-equivariant manner.\n3 Method\nHere, we present the SE(3)-Transformer. The layer can be broken down into a procedure of steps as\nshown in Fig. 2, which we describe in the following section. These are the construction of a graph\nfrom a point cloud, the construction of equivariant edge functions on the graph, how to propagate\nSE(3)-equivariant messages on the graph, and how to aggregate them. We also introduce an alternative\nfor the self-interaction layer, which we call attentive self-interaction.\n3.1 Neighbourhoods\nGiven a point cloud {(xi,fi)}, we ﬁrst introduce a collection of neighbourhoods Ni ⊆{1,...,N },\none centered on each point i. These neighbourhoods are computed either via the nearest-neighbours\nmethods or may already be deﬁned. For instance, molecular structures have neighbourhoods deﬁned\nby their bonding structure. Neighbourhoods reduce the computational complexity of the attention\nmechanism from quadratic in the number of points to linear. The introduction of neighbourhoods\nconverts our point cloud into a graph. This step is shown as Step 1 of Fig. 2.\n3.2 The SE(3)-Transformer\nThe SE(3)-Transformer itself consists of three components. These are 1) edge-wise attention weights\nαij, constructed to be SE(3)-invariant on each edgeij, 2) edge-wise SE(3)-equivariant value messages,\npropagating information between nodes, as found in the TFN convolution of Eq. (7), and 3) a\nlinear/attentive self-interaction layer. Attention is performed on a per-neighbourhood basis as follows:\nfℓ\nout,i = Wℓℓ\nVfℓ\nin,i  \n3⃝self-interaction\n+\n∑\nk≥0\n∑\nj∈Ni\\i\nαij\n1⃝attention\nWℓk\nV (xj −xi)fk\nin,j  \n2⃝value message\n. (10)\nThese components are visualised in Fig. 2. If we remove the attention weights then we have a\ntensor ﬁeld convolution, and if we instead remove the dependence of WV on (xj −xi), we have a\nconventional attention mechanism. Provided that the attention weights αij are invariant, Eq. (10) is\nequivariant to SE(3)-transformations. This is because it is just a linear combination of equivariant\nvalue messages. Invariant attention weights can be achieved with a dot-product attention structure\nshown in Eq. (11). This mechanism consists of a normalised inner product between a query vector qi\n5\nat node iand a set of key vectors {kij}j∈Ni along each edge ijin the neighbourhood Ni where\nαij = exp(q⊤\ni kij)∑\nj′∈Ni\\iexp(q⊤\ni kij′ ), qi =\n⨁\nℓ≥0\n∑\nk≥0\nWℓk\nQfk\nin,i, kij =\n⨁\nℓ≥0\n∑\nk≥0\nWℓk\nK(xj −xi)fk\nin,j. (11)\n⨁is the direct sum, i.e. vector concatenation in this instance. The linear embedding matrices Wℓk\nQ\nand Wℓk\nK(xj −xi) are of TFN type (c.f. Eq. (8)). The attention weights αij are invariant for the\nfollowing reason. If the input features {fin,j}are SO(3)-equivariant, then the query qi and key\nvectors {kij}are also SE(3)-equivariant, since the linear embedding matrices are of TFN type.\nThe inner product of SO(3)-equivariant vectors, transforming under the same representation Sg is\ninvariant, since if q ↦→Sgq and k ↦→Sgk, then q⊤S⊤\ng Sgk = q⊤k, because of the orthonormality of\nrepresentations of SO(3), mentioned in the background section. We follow the common practice from\nthe self-attention literature [31, 16], and chosen a softmax nonlinearity to normalise the attention\nweights to unity, but in general any nonlinear function could be used.\nAside: Angular Modulation The attention weights add extra degrees of freedom to the TFN\nkernel in the angular direction. This is seen when Eq. (10) is viewed as a convolution with a data-\ndependent kernel αijWℓk\nV (x). In the literature, SO(3) equivariant kernels are decomposed as a sum\nof products of learnable radial functions ϕℓk\nJ (∥x∥) and non-learnable angular kernels Wℓk\nJ (x/∥x∥)\n(c.f. Eq. (8)). The ﬁxed angular dependence of Wℓk\nJ (x/∥x∥) is a strange artifact of the equivariance\ncondition in noncommutative algebras and while necessary to guarantee equivariance, it is seen as\noverconstraining the expressiveness of the kernels. Interestingly, the attention weights αij introduce\na means to modulate the angular proﬁle of Wℓk\nJ (x/∥x∥), while maintaining equivariance.\nChannels, Self-interaction Layers, and Non-Linearities Analogous to conventional neural net-\nworks, the SE(3)-Transformer can straightforwardly be extended to multiple channels per repre-\nsentation degree ℓ, so far omitted for brevity. This sets the stage for self-interaction layers. The\nattention layer (c.f. Fig. 2 and circles 1 and 2 of Eq. (10)) aggregates information over nodes and input\nrepresentation degrees k. In contrast, the self-interaction layer (c.f. circle 3 of Eq. (10)) exchanges\ninformation solely between features of the same degree and within one node—much akin to 1x1\nconvolutions in CNNs. Self-interaction is an elegant form of learnable skip connection, transporting\ninformation from query point iin layer Lto query point iin layer L+ 1. This is crucial since, in\nthe SE(3)-Transformer, points do not attend to themselves. In our experiments, we use two different\ntypes of self-interaction layer: (1) linear and (2) attentive, both of the form\nfℓ\nout,i,c′ =\n∑\nc\nwℓℓ\ni,c′cfℓ\nin,i,c. (12)\nLinear: Following Schütt et al. [25], output channels are a learned linear combination of input\nchannels using one set of weights wℓℓ\ni,c′c = wℓℓ\nc′c per representation degree, shared across all points.\nAs proposed in Thomas et al. [28], this is followed by a norm-based non-linearity.\nAttentive: We propose an extension of linear self-interaction, attentive self-interaction, combining\nself-interaction and nonlinearity. We replace the learned scalar weights wℓℓ\nc′c with attention weights\noutput from an MLP, shown in Eq. (13) (⨁means concatenation.). These weights are SE(3)-invariant\ndue to the invariance of inner products of features, transforming under the same representation.\nwℓℓ\ni,c′c = MLP\n\n⨁\nc,c′\nfℓ⊤\nin,i,c′ fℓ\nin,i,c\n\n (13)\n3.3 Node and Edge Features\nPoint cloud data often has information attached to points (node-features) and connections between\npoints (edge-features), which we would both like to pass as inputs into the ﬁrst layer of the network.\nNode information can directly be incorporated via the tensorsfj in Eqs. (6) and (10). For incorporating\nedge information, note that fj is part of multiple neighbourhoods. One can replace fj with fij in\nEq. (10). Now, fij can carry different information depending on which neighbourhood Ni we are\ncurrently performing attention over. In other words, fij can carry information both about node jbut\nalso about edge ij. Alternatively, if the edge information is scalar, it can be incorporated into the\nweight matrices WV and WK as an input to the radial network (see step 2 in Fig. 2).\n6\nTable 1: Predicting future locations and velocities in an electron-proton simulation.\nLinear DeepSet [46] Tensor Field [28] Set Transformer [16]SE(3)-Transformer\nMSEx 0.0691 0 .0639 0 .0151 0 .0139 0.0076\nPosition std - 0.0086 0 .0011 0 .0004 0 .0002\n∆EQ - 0.038 1 .9·10−7 0.167 3 .2·10−7\nMSEv 0.261 0 .246 0 .125 0 .101 0.075\nVelocity std - 0.017 0 .002 0 .004 0 .001\n∆EQ - 1.11 5 .0·10−7 0.37 6 .3·10−7\n4 Experiments\nWe test the efﬁcacy of the SE(3)-Transformer on three datasets, each testing different aspects of the\nmodel. The N-body problem is an equivariant task: rotation of the input should result in rotated\npredictions of locations and velocities of the particles. Next, we evaluate on a real-world object\nclassiﬁcation task. Here, the network is confronted with large point clouds of noisy data with\nsymmetry only around the gravitational axis. Finally, we test the SE(3)-Transformer on a molecular\nproperty regression task, which shines light on its ability to incorporate rich graph structures. We\ncompare to publicly available, state-of-the-art results as well as a set of our own baselines. Speciﬁcally,\nwe compare to the Set-Transformer [ 16], a non-equivariant attention model, and Tensor Field\nNetworks [28], which is similar to SE(3)-Transformer but does not leverage attention.\nSimilar to [27, 39], we measure the exactness of equivariance by applying uniformly sampled SO(3)-\ntransformations to input and output. The distance between the two, averaged over samples, yields the\nequivariance error. Note that, unlike in Sosnovik et al. [27], the error is not squared:\n∆EQ = ∥LsΦ(f) −ΦLs(f)∥2 /∥LsΦ(f)∥2 (14)\n4.1 N-Body Simulations\nIn this experiment, we use an adaptation of the dataset from Kipf et al. [14]. Five particles each carry\neither a positive or a negative charge and exert repulsive or attractive forces on each other. The input\nto the network is the position of a particle in a speciﬁc time step, its velocity, and its charge. The task\nof the algorithm is then to predict the relative location and velocity 500 time steps into the future.\nWe deliberately formulated this as a regression problem to avoid the need to predict multiple time\nsteps iteratively. Even though it certainly is an interesting direction for future research to combine\nequivariant attention with, e.g., an LSTM, our goal here was to test our core contribution and compare\nit to related models. This task sets itself apart from the other two experiments by not being invariant\nbut equivariant: When the input is rotated or translated, the output changes respectively (see Fig. 3).\nWe trained an SE(3)-Transformer with 4 equivariant layers, each followed by an attentive self-\ninteraction layer (details are provided in the Appendix). Table 1 shows quantitative results. Our\nmodel outperforms both an attention-based, but not rotation-equivariant approach (Set Transformer)\nand a equivariant approach which does not levarage attention (Tensor Field). The equivariance error\nshows that our approach is indeed fully rotation equivariant up to the precision of the computations.\ninput\nlabel\nSet Transf.\noriginal\nrotated\n(a) Set Transformer\ninput\nlabel\nSE(3) Transf.\noriginal\nrotated (b) SE(3)-Transformer\nFigure 3: A model based on conventional self-attention (left) and our rotation-equivariant version\n(right) predict future locations and velocities in a 5-body problem. The respective left-hand plots show\ninput locations at time step t= 0, ground truth locations at t= 500, and the respective predictions.\nThe right-hand plots show predicted locations and velocities for rotations of the input in steps of 10\ndegrees. The dashed curves show the predicted locations of a perfectly equivariant model.\n7\n0 30 60 90 120 150 180\nmaximum rotation\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8accuracy\n SE(3)-Transformer +z\nSE(3)-Transformer\nTensor Field\nSetTransformer\nDeepSet\n(a) Training without data augmentation.\n0 30 60 90 120 150 180\nmaximum rotation\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8accuracy\n SE(3)-Transformer +z\nSE(3)-Transformer\nTensor Field\nSetTransformer\nDeepSet (b) Training with data augmentation.\nFigure 4: ScanObjectNN: x-axis shows data augmentation on the test set. The x-value corresponds\nto the maximum rotation around a random axis in the x-y-plane. If both training and test set are not\nrotated (x= 0 in a), breaking the symmetry of the SE(3)-Transformer by providing the z-component\nof the coordinates as an additional, scalar input improves the performance signiﬁcantly. Interestingly,\nthe model learns to ignore the additional, symmetry-breaking input when the training set presents a\nrotation-invariant problem (strongly overlapping dark red circles and dark purple triangles in b).\nTable 2: Classiﬁcation accuracy on the ’object only’ category of the ScanObjectNN dataset4. The\nperformance of the SE(3)-Transformer is averaged over 5 runs (standard deviation 0.7%).\nTensor FieldDeepSetSE(3)-Transf.3DmFVSet TransformerPointNetSpiderCNNTensor Field +zPointNet++SE(3)-Transf.+zPointCNNDGCNNPointGLR\nNo. Points 128 1024 128 1024 1024 1024 1024 128 1024 128 1024 1024 1024\nAccuracy 63.1% 71.4%72.8% 73.8% 74.1% 79.2% 79.5% 81.0% 84.3%85.0%85.5% 86.2% 87.2%\n4.2 Real-World Object Classiﬁcation on ScanObjectNN\nObject classiﬁcation from point clouds is a well-studied task. Interestingly, the vast majority of current\nneural network methods work on scalar coordinates without incorporating vector speciﬁc inductive\nbiases. Some recent works explore rotation invariant point cloud classiﬁcation [45, 47, 4, 22]. Our\nmethod sets itself apart by using roto-translation equivariant layers acting directly on the point cloud\nwithout prior projection onto a sphere [22, 45, 7]. This allows for weight-tying and increased sample\nefﬁciency while transporting information about local and global orientation through the network -\nanalogous to the translation equivariance of CNNs on 2D images. To test our method, we choose\nScanObjectNN, a recently introduced dataset for real-world object classiﬁcation. The benchmark\nprovides point clouds of 2902 objects across 15 different categories. We only use the coordinates of\nthe points as input and object categories as training labels. We train an SE(3)-Transformer with 4\nequivariant layers with linear self-interaction followed by max-pooling and an MLP. Interestingly,\nthe task is not fully rotation invariant, in a statistical sense, as the objects are aligned with respect to\nthe gravity axis. This results in a performance loss when deploying a fully SO(3) invariant model\n(see Fig. 4a). In other words: when looking at a new object, it helps to know where ‘up’ is. We\ncreate an SO(2) invariant version of our algorithm by additionally feeding the z-component as an\ntype-0 ﬁeld and the x, yposition as an additional type-1 ﬁeld (see Appendix). We dub this model\nSE(3)-Transformer +z. This way, the model can ‘learn’ which symmetries to adhere to by suppressing\nand promoting different inputs (compare Fig. 4a and Fig. 4b). In Table 2, we compare our model to\nthe current state-of-the-art in object classiﬁcation4. Despite the dataset not playing to the strengths\nof our model (full SE(3)-invariance) and a much lower number of input points, the performance is\ncompetitive with models speciﬁcally designed for object classiﬁcation - PointGLR [23], for instance,\nis pre-trained on the larger ModelNet40 dataset [41]. For a discussion of performance vs. number of\ninput points used, see Appendix D.1.2.\n4At time of submission, PointGLR was a recently published preprint [23]. The performance of the following\nmodels was taken from the ofﬁcial benchmark of the dataset as of June 4th, 2020 ( https://hkust-vgd.\ngithub.io/benchmark/): 3DmFV [3], PointNet [19], SpiderCNN [43], PointNet++ [20], DGCN [35].\n8\n4.3 QM9 Table 3: QM9 Mean Absolute Error. Top: Non-equivariant models.\nBottom: Equivariant models. SE(3)-Tr. is averaged over 5 runs.\nTASK α ∆ε εHOMO εLUMO µ C ν\nUNITS bohr3 meV meV meV D cal/mol K\nWaveScatt [11] .160 118 85 76 .340 .049NMP [10] .092 69 43 38 .030 .040SchNet [25] .235 63 41 34 .033 .033\nCormorant [1] .085 61 34 38 .038 .026LieConv(T3) [8] .084 49 30 25 .032 .038TFN [28] .223 58 40 38 .064 .101SE(3)-Transformer.142±.002 53.0±0.3 35.0±.9 33.0±.7 .051±.001 .054±.002\nThe QM9 regression dataset [21]\nis a publicly available chemical\nproperty prediction task. There\nare 134k molecules with up to 29\natoms per molecule. Atoms are\nrepresented as a 5 dimensional\none-hot node embeddings in a\nmolecular graph connected by\n4 different chemical bond types\n(more details in Appendix). ‘Po-\nsitions’ of each atom are provided. We show results on the test set of Anderson et al. [1] for 6\nregression tasks in Table 3. Lower is better. The table is split into non-equivariant (top) and equivari-\nant models (bottom). Our nearest models are Cormorant and TFN (own implementation). We see that\nwhile not state-of-the-art, we offer competitive performance, especially against Cormorant and TFN,\nwhich transform under irreducible representations of SE(3) (like us), unlike LieConv(T3), using a\nleft-regular representation of SE(3), which may explain its success.\n5 Conclusion\nWe have presented an attention-based neural architecture designed speciﬁcally for point cloud data.\nThis architecture is guaranteed to be robust to rotations and translations of the input, obviating the need\nfor training time data augmentation and ensuring stability to arbitrary choices of coordinate frame.\nThe use of self-attention allows for anisotropic, data-adaptive ﬁlters, while the use of neighbourhoods\nenables scalability to large point clouds. We have also introduced the interpretation of the attention\nmechanism as a data-dependent nonlinearity, adding to the list of equivariant nonlinearties which we\ncan use in equivariant networks. Furthermore, we provide code for a speed up of spherical harmonics\ncomputation of up to 3 orders of magnitudes. This speed-up allowed us to train signiﬁcantly larger\nversions of both the SE(3)-Transformer and the Tensor Field network [28] and to apply these models\nto real-world datasets.\nOur experiments showed that adding attention to a roto-translation-equivariant model consistently led\nto higher accuracy and increased training stability. Speciﬁcally for large neighbourhoods, attention\nproved essential for model convergence. On the other hand, compared to convential attention, adding\nthe equivariance constraints also increases performance in all of our experiments while at the same\ntime providing a mathematical guarantee for robustness with respect to rotations of the input data.\nBroader Impact\nThe main contribution of the paper is a mathematically motivated attention mechanism which can be\nused for deep learning on point cloud based problems. We do not see a direct potential of negative\nimpact to the society. However, we would like to stress that this type of algorithm is inherently suited\nfor classiﬁcation and regression problems on molecules. The SE(3)-Transformer therefore lends itself\nto application in drug research. One concrete application we are currently investigating is to use\nthe algorithm for early-stage suitability classiﬁcation of molecules for inhibiting the reproductive\ncycle of the coronavirus. While research of this sort always requires intensive testing in wet labs,\ncomputer algorithms can be and are being used to ﬁlter out particularly promising compounds from\nlarge databases of millions of molecules.\nAcknowledgements and Funding Disclosure\nWe would like to express our gratitude to the Bosch Center for Artiﬁcial Intelligence and Konincklijke\nPhilips N.V . for funding our work and contributing to open research by publishing our paper. Fabian\nFuchs worked on this project while on a research sabbatical at the Bosch Center for Artiﬁcial\nIntelligence. His PhD is funded by Kellogg College Oxford and the EPSRC AIMS Centre for\nDoctoral Training at Oxford University.\n9\nReferences\n[1] Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural\nnetworks. In Advances in Neural Information Processing Systems (NeurIPS). 2019.\n[2] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv Preprint,\n2016.\n[3] Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fischer. 3dmfv: Three-dimensional\npoint cloud classiﬁcation in realtime using convolutional neural networks. IEEE Robotics and\nAutomation Letters, 2018.\n[4] Chao Chen, Guanbin Li, Ruijia Xu, Tianshui Chen, Meng Wang, and Liang Lin. ClusterNet:\nDeep Hierarchical Cluster Network with Rigorously Rotation-Invariant Representation for Point\nCloud Analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019.\n[5] Gregory S Chirikjian, Alexander B Kyatkin, and AC Buckingham. Engineering applications of\nnoncommutative harmonic analysis: with emphasis on rotation and motion groups. Appl. Mech.\nRev., 54(6):B97–B98, 2001.\n[6] Taco S. Cohen and Max Welling. Steerable cnns. International Conference on Learning\nRepresentations (ICLR), 2017.\n[7] Taco S. Cohen, Mario Geiger, Jonas Koehler, and Max Welling. Spherical cnns. InInternational\nConference on Learning Representations (ICLR), 2018.\n[8] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Wilson. Generalizing convolutional\nneural networks for equivariance to lie groups on arbitrary continuous data. Proceedings of the\nInternational Conference on Machine Learning, ICML, 2020.\n[9] Fabian B. Fuchs, Adam R. Kosiorek, Li Sun, Oiwi Parker Jones, and Ingmar Posner. End-to-end\nrecurrent multi-object tracking and prediction with relational reasoning. arXiv preprint, 2020.\n[10] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, and Oriol Vinyals George E. Dahl.\nNeural message passing for quantum chemistry. Proceedings of the International Conference\non Machine Learning, ICML, 2020.\n[11] Matthew J. Hirn, Stéphane Mallat, and Nicolas Poilvert. Wavelet scattering regression of\nquantum chemical energies. Multiscale Model. Simul., 15(2):827–863, 2017.\n[12] Yedid Hoshen. Vain: Attentional multi-agent predictive modeling. Advances in Neural Informa-\ntion Processing Systems (NeurIPS), 2017.\n[13] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-\ntional Conference on Learning Representations, ICLR, 2015.\n[14] Thomas N. Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard S. Zemel. Neural\nrelational inference for interacting systems. In Proceedings of the International Conference on\nMachine Learning, ICML, 2018.\n[15] Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning\natomic potentials. arXiv preprint, 2018.\n[16] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh.\nSet transformer: A framework for attention-based permutation-invariant neural networks. In\nProceedings of the International Conference on Machine Learning, ICML, 2019.\n[17] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou,\nand Yoshua Bengio. A structured self-attentive sentence embedding. International Conference\non Learning Representations (ICLR), 2017.\n[18] Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and\nJon Shlens. Stand-alone self-attention in vision models. In Advances in Neural Information\nProcessing System (NeurIPS), 2019.\n10\n[19] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point\nsets for 3d classiﬁcation and segmentation. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017.\n[20] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature\nlearning on point sets in a metric space. Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n[21] Raghunathan Ramakrishnan, Pavlo Dral, Matthias Rupp, and Anatole von Lilienfeld. Quantum\nchemistry structures and properties of 134 kilo molecules. Scientiﬁc Data, 1, 08 2014.\n[22] Yongming Rao, Jiwen Lu, and Jie Zhou. Spherical fractal convolutional neural networks for\npoint cloud recognition. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2019.\n[23] Yongming Rao, Jiwen Lu, and Jie Zhou. Global-local bidirectional reasoning for unsupervised\nrepresentation learning of 3d point clouds. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2020.\n[24] David W. Romero, Erik J. Bekkers, Jakub M. Tomczak, and Mark Hoogendoorn. Attentive\ngroup equivariant convolutional networks. Proceedings of the International Conference on\nMachine Learning (ICML), 2020.\n[25] K. T. Schütt, P.-J. Kindermans, H. E. Sauceda, S. Chmiela1, A. Tkatchenko, and K.-R. Müller.\nSchnet: A continuous-ﬁlter convolutional neural network for modeling quantum interactions.\nAdvances in Neural Information Processing Systems (NeurIPS), 2017.\n[26] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position represen-\ntations. Annual Conference of the North American Chapter of the Association for Computational\nLinguistics (NAACL-HLT), 2018.\n[27] Ivan Sosnovik, Michał Szmaja, and Arnold Smeulders. Scale-equivariant steerable networks.\nInternational Conference on Learning Representations (ICLR), 2020.\n[28] Nathaniel Thomas, Tess Smidt, Steven M. Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and\nPatrick Riley. Tensor ﬁeld networks: Rotation- and translation-equivariant neural networks for\n3d point clouds. ArXiv Preprint, 2018.\n[29] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit\nYeung. Revisiting point cloud classiﬁcation: A new benchmark dataset and classiﬁcation model\non real-world data. In International Conference on Computer Vision (ICCV), 2019.\n[30] Sjoerd van Steenkiste, Michael Chang, Klaus Greff, and Jürgen Schmidhuber. Relational\nneural expectation maximization: Unsupervised discovery of objects and their interactions.\nInternational Conference on Learning Representations (ICLR), 2018.\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information\nProcessing Systems (NeurIPS), 2017.\n[32] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua\nBengio. Graph attention networks. International Conference on Learning Representations\n(ICLR), 2018.\n[33] Edward Wagstaff, Fabian B. Fuchs, Martin Engelcke, Ingmar Posner, and Michael A. Osborne.\nOn the limitations of representing functions on sets. International Conference on Machine\nLearning (ICML), 2019.\n[34] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n[35] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M.\nSolomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics\n(TOG), 2019.\n11\n[36] Maurice Weiler and Gabriele Cesa. General E(2)-Equivariant Steerable CNNs. In Conference\non Neural Information Processing Systems (NeurIPS), 2019.\n[37] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3d steerable\ncnns: Learning rotationally equivariant features in volumetric data. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2018.\n[38] Marysia Winkels and Taco S. Cohen. 3d g-cnns for pulmonary nodule detection.1st Conference\non Medical Imaging with Deep Learning (MIDL), 2018.\n[39] Daniel E. Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. In Advances\nin Neural Information Processing Systems (NeurIPS), 2019.\n[40] Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow.\nHarmonic networks: Deep translation and rotation equivariance. IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2017.\n[41] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and\nJianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2015.\n[42] Saining Xie, Sainan Liu, and Zeyu Chen Zhuowen Tu. Attentional shapecontextnet for point\ncloud recognition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2018.\n[43] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep learning on\npoint sets with parameterized convolutional ﬁlters. European Conference on Computer Vision\n(ECCV), 2018.\n[44] Jiancheng Yang, Qiang Zhang, and Bingbing Ni. Modeling point clouds with self-attention\nand gumbel subset sampling. IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2019.\n[45] Yang You, Yujing Lou, Qi Liu, Yu-Wing Tai, Lizhuang Ma, Cewu Lu, and Weiming Wang. Point-\nwise Rotation-Invariant Network with Adaptive Sampling and 3D Spherical V oxel Convolution.\nIn AAAI Conference on Artiﬁcial Intelligence, 2019.\n[46] Manzil Zaheer, Satwik Kottur, Siamak Ravanbhakhsh, Barnabás Póczos, Ruslan Salakhutdinov,\nand Alexander Smola. Deep Sets. In Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n[47] Zhiyuan Zhang, Binh-Son Hua, David W. Rosen, and Sai-Kit Yeung. Rotation invariant\nconvolutions for 3d point clouds deep learning. InInternational Conference on 3D Vision (3DV),\n2019.\n12\nA Group Theory and Tensor Field Networks\nGroups A group is an abstract mathematical concept. Formally a group (G,◦) consists of a set G\nand a binary composition operator ◦: G×G→G(typically we just use the symbol Gto refer to\nthe group). All groups must adhere to the following 4 axioms\n• Closure: g◦h∈Gfor all g,h ∈G\n• Associativity: f ◦(g◦h) = (f ◦g) ◦h= f ◦g◦hfor all f,g,h ∈G\n• Identity: There exists an element e∈Gsuch that e◦g= g◦e= gfor all g∈G\n• Inverses: For each g∈Gthere exists a g−1 ∈Gsuch that g−1 ◦g= g◦g−1 = e\nIn practice, we omit writing the binary composition operator ◦, so would write ghinstead of g◦h.\nGroups can be ﬁnite or inﬁnite, countable or uncountable, compact or non-compact. Note that they\nare not necessarily commutative; that is, gh̸= hgin general.\nActions/Transformations Groups are useful concepts, because they allow us to describe the\nstructure of transformations, also sometimes called actions. A transformation (operator) Tg : X→X\nis an injective map from a space into itself. It is parameterised by an element g of a group G.\nTransformations obey two laws:\n• Closure: Tg ◦Th is a valid transformation for all g,h ∈G\n• Identity: There exists at least one element e∈Gsuch that Te[x] = x for all x ∈X,\nwhere ◦denotes composition of transformations. For the expression Tg[x], we say that Tg acts on x.\nIt can also be shown that transformations are associative under composition. To codify the structure\nof a transformation, we note that due to closure we can always write\nTg ◦Th = Tgh, (15)\nIf for any x,y ∈X we can always ﬁnd a group element g, such that Tg[x] = y, then we call Xa\nhomogeneous space. Homogeneous spaces are important concepts, because to each pair of points\nx,y we can always associate at least one group element.\nEquivariance and Intertwiners As written in the main body of the text, equivariance is a property\nof functions f : X →Y. Just to recap, given a set of transformations Tg : X →Xfor g ∈G,\nwhere Gis an abstract group, a function f : X→Y is called equivariant if for every gthere exists a\ntransformation Sg : Y→Y such that\nSg[f(x)] = f(Tg[x]) for all g∈G,x ∈X. (16)\nIf f is linear and equivariant, then it is called an intertwiner. Two important questions arise: 1) How\ndo we choose Sg? 2) once we have (Tg,Sg), how do we solve for f? To answer these questions, we\nneed to understand what kinds of Sg are possible. For this, we review representations.\nRepresentations A group representation ρ : G →GL(N) is a map from a group Gto the set\nof N ×N invertible matrices GL(N). Critically ρis a group homomorphism; that is, it satisﬁes\nthe following property ρ(g1g2) = ρ(g1)ρ(g2) for all g1,g2 ∈G. Representations can be used as\ntransformation operators, acting on N-dimensional vectors x ∈RN. For instance, for the group of\n3D rotations, known as SO(3), we have that 3D rotation matrices, ρ(g) = Rg act on (i.e., rotate) 3D\nvectors, as\nTg[x] = ρ(g)x = Rgx, for all x ∈X,g ∈G. (17)\nHowever, there are many more representations of SO(3) than just the 3D rotation matrices. Among\nrepresentations, two representations ρand ρ′of the same dimensionality are said to be equivalent if\nthey can be connected by a similarity transformation\nρ′(g) = Q−1ρ(g)Q, for all g∈G. (18)\nWe also say that a representation is reducible if is can be written as\nρ(g) = Q−1(ρ1(g) ⊕ρ2(g))Q = Q−1\n[\nρ1(g)\nρ2(g)\n]\nQ, for all g∈G. (19)\nIf the representations ρ1 and ρ2 are not reducible, then they are called irreducible representations\nof G, or irreps. In a sense, they are the atoms among representations, out of which all other\nrepresentations can be constructed. Note that each irrep acts on a separate subspace, mapping\nvectors from that space back into it. We say that subspace Xℓ ∈X is invariant under irrep ρℓ, if\n{ρℓ(g)x |x ∈Xℓ,g ∈G}⊆X ℓ.\n13\nRepresentation theory of SO(3) As it turns out, all linear representations of compact groups 5\n(such as SO(3)) can be decomposed into a direct sum of irreps, as\nρ(g) = Q⊤\n[⨁\nJ\nDJ(g)\n]\nQ, (20)\nwhere Q is an orthogonal, N ×N, change-of-basis matrix [ 5]; and each DJ for J = 0 ,1,2,...\nis a (2J + 1) ×(2J + 1) matrix known as a Wigner-D matrix. The Wigner-D matrices are the\nirreducible representations of SO(3). We also mentioned that vectors transforming according to DJ\n(i.e. we set Q = I), are called type-J vectors. Type-0 vectors are invariant under rotations and type-1\nvectors rotate according to 3D rotation matrices. Note, type- J vectors have length 2J + 1. In the\nprevious paragraph we mentioned that irreps act on orthogonal subspaces X0,X1,.... The orthogonal\nsubspaces corresponding to the Wigner-D matrices are the space of spherical harmonics.\nThe Spherical Harmonics The spherical harmonics YJ : S2 →C2J+1 for J ≥0 are square-\nintegrable complex-valued functions on the sphere S2. They have the satisfying property that they\nare rotated directly by the Wigner-D matrices as\nYJ(R−1\ng x) = D∗\nJ(g)YJ(x), x ∈S2,g ∈G, (21)\nwhere DJ is the Jth Wigner-D matrix and D∗\nJ is its complex conjugate. They form an orthonormal\nbasis for (the Hilbert space of) square-integrable functions on the sphere L2(S2), with inner product\ngiven as\n⟨f,h⟩S2 =\n∫\nS2\nf(x)h∗(x) dx. (22)\nSo ⟨YJm,YJ′m′ ⟩S2 = δJJ′ δmm′ , where YJm is the mth element of YJ. We can express any function\nin L2(S2) as a linear combination of spherical harmonics, where\nf(x) =\n∑\nJ≥0\nf⊤\nJYJ(x), x ∈S2, (23)\nwhere each fJ is a vector of coefﬁcients of length 2J + 1. And in the opposite direction, we can\nretrieve the coefﬁcients as\nfJ =\n∫\nS2\nf(x)Y∗\nJ(x) dx (24)\nfollowing from the orthonormality of the spherical harmonics. This is in fact a Fourier transform on\nthe sphere and the the vectors fJ can be considered Fourier coefﬁcients. Critically, we can represent\nrotated functions as\nf(R−1\ng x) =\n∑\nJ≥0\nf⊤\nJD∗\nJ(g)YJ(x), x ∈S2,g ∈G. (25)\nThe Clebsch-Gordan Decomposition In the main text we introduced the Clebsch-Gordan coef-\nﬁcients. These are used in the construction of the equivariant kernels. They arise in the situation\nwhere we have a tensor product of Wigner-D matrices, which as we will see is part of the equiv-\nariance constraint on the form of the equivariant kernels. In representation theory a tensor product\nof representations is also a representation, but since it is not an easy object to work with, we seek\nto decompose it into a direct sum of irreps, which are easier. This decomposition is of the form of\nEq. (20), written\nDk(g) ⊗Dℓ(g) = Qℓk⊤\n\n\nk+ℓ⨁\nJ=|k−ℓ|\nDJ(g)\n\nQℓk. (26)\nIn this speciﬁc instance, the change of basis matrices Qℓk are given the special name of the Clebsch-\nGordan coefﬁcients. These can be found in many mathematical physics libraries.\n5Over a ﬁeld of characteristic zero.\n14\nTensor Field Layers In Tensor Field Networks [ 28] and 3D Steerable CNNs [ 37], the authors\nsolve for the intertwiners between SO(3) equivariant point clouds. Here we run through the derivation\nagain in our own notation.\nWe begin with a point cloudf(x) = ∑N\nj=1 fjδ(x −xj), where fj is an equivariant point feature. Let’s\nsay that fj is a type-kfeature, which we write as fk\nj to remind ourselves of the fact. Now say we\nperform a convolution ∗with kernel Wℓk : R3 →R(2ℓ+1)×(2k+1), which maps from type-kfeatures\nto type-ℓfeatures. Then\nfℓ\nout,i = [Wℓk ∗fk\nin](x) (27)\n=\n∫\nR3\nWℓk(x′−xi)fk\nin(x′) dx′ (28)\n=\n∫\nR3\nWℓk(x′−xi)\nN∑\nj=1\nfk\nin,jδ(x′−xj) dx′ (29)\n=\nN∑\nj=1\n∫\nR3\nWℓk(x′−xi)fk\nin,jδ(x′−xj) dx′ change of variables x′′= x′−xj (30)\n=\nN∑\nj=1\n∫\nR3\nWℓk(x′′+ xj −xi)fk\nin,jδ(x′′) dx′′ sifting theorem (31)\n=\nN∑\nj=1\nWℓk(xj −xi)fk\nin,j. (32)\nNow let’s apply the equivariance condition to this expression, then\nDℓ(g)fℓ\nout,i =\nN∑\nj=1\nWℓk(R−1\ng (xj −xi))Dk(g)fk\nin,j (33)\n=⇒ fℓ\nout,i =\nN∑\nj=1\nDℓ(g)−1Wℓk(R−1\ng (xj −xi))Dk(g)fk\nin,j (34)\nNow we notice that this expression should also be equal to Eq. (32), which is the convolution with an\nunrotated point cloud. Thus we end up at\nWℓk(R−1\ng x) = Dℓ(g)Wℓk(x)Dk(g)−1, (35)\nwhich is sometimes refered to as the kernel constraint. To solve the kernel constraint, we notice that\nit is a linear equation and that we can rearrange it as\nvec(Wℓk(R−1\ng x)) = (Dk(g) ⊗Dℓ(g))vec(Wℓk(x)) (36)\nwhere we used the identity vec(AXB) = (B⊤⊗A)vec(X) and the fact that the Wigner-D matrices\nare orthogonal. Using the Clebsch-Gordan decomposition we rewrite this as\nvec(Wℓk(R−1\ng x)) = Qℓk⊤\n\n\nk+ℓ⨁\nJ=|k−ℓ|\nDJ(g)\n\nQℓkvec(Wℓk(R−1\ng x)). (37)\nLastly, we can left multiply both sides by Qℓk and denote ηℓk(x) ≜ Qℓkvec(Wℓk(x)), noting the the\nClebsch-Gordan matrices are orthogonal. At the same time we\nηℓk(R−1\ng x) =\n\n\nk+ℓ⨁\nJ=|k−ℓ|\nDJ(g)\n\nηℓk(x). (38)\nThus we have that ηℓk\nJ (R−1\ng x) the Jth subvector of ηℓk(R−1\ng x) is subject to the constraint\nηℓk\nJ (R−1\ng x) = DJ(g)ηℓk\nJ (x), (39)\n15\nwhich is exactly the transformation law for the spherical harmonics from Eq. (21)! Thus one way\nhow Wℓk(x) can be constructed is\nvec\n(\nWℓk(x)\n)\n= Qℓk⊤\nk+ℓ⨁\nJ=|k−ℓ|\nYJ(x). (40)\nB Recipe for Building an Equivariant Weight Matrix\nOne of the core operations in the SE(3)-Transformer is multiplying a feature vectorf, which transforms\naccording to SO(3), with a matrix W while preserving equivariance:\nSg[W ∗f](x) = [W ∗Tg[f]](x), (41)\nwhere Tg[f](x) = ρin(g)f(R−1\ng x) and Sg[f](x) = ρout(g)f(R−1\ng x). Here, as in the previous section\nwe showed how such a matrix W could be constructed when mapping between features of type-k\nand type-ℓ, where ρin(g) is a block diagonal matrix of type-kWigner-D matrices and similarly ρin(g)\nis made of type-ℓWigner-D matrices. W is dependent on the relative position x and underlies the\nlinear equivariance constraints, but is also has learnable components, which we did not show in the\nprevious section. In this section, we show how such a matrix is constructed in practice.\nPreviously we showed that\nvec\n(\nWℓk(x)\n)\n= Qℓk⊤\nk+ℓ⨁\nJ=|k−ℓ|\nYJ(x), (42)\nwhich is an equivariant mapping between vectors of types kand ℓ. In practice, we have multiple\ninput vectors {fk\nc}c of type-k and multiple output vectors of type- ℓ. For simplicity, however, we\nignore this and pretend we only have a single input and single output. Note that Wℓk has no learnable\ncomponents. Note that the kernel constraint only acts in the angular direction, but not in the radial\ndirection, so we can introduce scalar radial functions ϕℓk\nJ : R≥0 →R (one for each J), such that\nvec\n(\nWℓk(x)\n)\n= Qℓk⊤\nk+ℓ⨁\nJ=|k−ℓ|\nϕℓk\nJ (∥x∥)YJ(x), (43)\nThe radial functions ϕℓk\nJ (∥x∥) act as an independent, learnable scalar factor for each degree J. The\nvectorised matrix has dimensionality (2ℓ+ 1)(2k+ 1). We can unvectorise the above yielding\nWℓk(x) = unvec\n\nQℓk⊤\nk+ℓ⨁\nJ=|k−ℓ|\nϕℓk\nJ (∥x∥)YJ(x)\n\n (44)\n=\nk+ℓ∑\nJ=|k−ℓ|\nϕℓk\nJ (∥x∥)unvec\n(\nQℓk⊤\nJ YJ(x)\n)\n(45)\nwhere Qℓk\nJ is a (2ℓ+ 1)(2k+ 1) ×(2J+ 1) slice from Qℓk, corresponding to spherical harmonic\nYJ.As we showed in the main text, we can also rewrite the unvectorised Clebsch-Gordan–spherical\nharmonic matrix-vector product as\nunvec\n(\nQℓk⊤\nJ YJ(x)\n)\n=\nJ∑\nm=−J\nQℓk⊤\nJm YJm(x). (46)\nIn contrast to Weiler et al.[37], we do not voxelise space and thereforex will be different for each pair\nof points in each point cloud. However, the same YJ (x) will be used multiple times in the network\nand even multiple times in the same layer. Hence, precomputing them at the beginning of each\nforward pass for the entire network can signiﬁcantly speed up the computation. The Clebsch-Gordan\ncoefﬁcients do not depend on the relative positions and can therefore be precomputed once and stored\non disk. Multiple libraries exist which approximate those coefﬁcients numerically.\n16\n100 101 102 103 104 105 106\nNum. elements\n10 2\n10 1\n100\n101\nSE(3)-transformer / lie_learn time\nSpeed comparison: CPU\nJ=0\nJ=1\nJ=2\nJ=3\nJ=4\nJ=5\nJ=6\nJ=7\nJ=8\nJ=9\n(a) Speed comparison on the CPU.\n100 101 102 103 104 105 106\nNum. elements\n10 3\n10 2\n10 1\n100\nSE(3)-transformer / lie_learn time\nSpeed comparison: GPU\nJ=0\nJ=1\nJ=2\nJ=3\nJ=4\nJ=5\nJ=6\nJ=7\nJ=8\nJ=9 (b) Speed comparison on the GPU.\nFigure 5: Spherical harmonics computation of our own implementation compared to the lie-learn\nlibrary. We found that speeding up the computation of spherical harmonics is critical to scale up both\nTensor Field Networks [28] and SE(3)-Transformers to solve real-world machine learning tasks.\nC Accelerated Computation of Spherical Harmonics\nThe spherical harmonics (SH) typically have to be computed on the ﬂy for point cloud methods\nbased on irreducible computations, a bottleneck of TFNs [28]. Thomas et al. [28] ameliorate this by\nrestricting the maximum type of feature to type-2, trading expressivity for speed. Weiler et al. [37]\ncircumvent this challenge by voxelising the input, allowing them to pre-compute spherical harmonics\nfor ﬁxed relative positions. This is at the cost of detail as well as exact rotation and translation\nequivariance.\nThe number of spherical harmonic lookups in a network based on irreducible representations can\nquickly become large (number of layers ×number of points ×number of neighbours ×number of\nchannels ×number of degrees needed). This motivates parallelised computation on the GPU - a\nfeature not supported by common libraries. To that end, we wrote our own spherical harmonics library\nin Pytorch, which can generate spherical harmonics on the GPU. We found this critical to being able\nto run the SE(3)-Transformer and Tensor Field network baselines in a reasonable time. This library\nis accurate to within machine precision against the scipy counterpart scipy.special.sph_harm\nand is signiﬁcantly faster. E.g., for a ScanObjectNN model, we achieve ∼22×speed up of the\nforward pass compared to a network built with SH from the lielearn library. A speed comparison\nisolating the computation of the spherical harmonics is shown in Fig. 5. Code is available at https:\n//github.com/FabianFuchsML/se3-transformer-public. In the following, we outline our\nmethod to generate them.\nThe tesseral/real spherical harmonics are given as\nYJm(θ,φ) =\n√\n2J+ 1\n4π\n(J−m)!\n(J+ m)!P|m|\nJ (cos θ) ·\n{sin(|m|φ) m< 0,\n1 m= 0,\ncos(mφ) m> 0,\n(47)\nwhere P|m|\nJ is the associated Legendre polynomial (ALP), θ∈[0,2π) is azimuth, and φ∈[0,π] is a\npolar angle. The term P|m|\nJ is by far the most expensive component to compute and can be computed\nrecursively. To speed up the computation, we use dynamic programming storing intermediate results\nin a memoization.\nWe make use of the following recursion relations in the computation of the ALPs:\nP|J|\nJ (x) = (−1)|J|·(1 −x2)|J|/2 ·(2|J|−1)!! boundary: J = m (48)\nP−m\nJ (x) = (−1)J(ℓ−m)!\n(ℓ+ m)!Pm\nJ (x) negate m (49)\nP|m|\nJ (x) = 2J−1\nJ−|m|xPm\nJ−1(x) + I[J−|m|>1]J+ |m|−1\nJ−|m| Pm\nJ−2(x) recurse (50)\nwhere the semifactorial is deﬁned as x!! = x(x−2)(x−4) ···, and I is the indicator function. These\nrelations are helpful because they deﬁne a recursion.\n17\n0\n1\n2\n3\n4\n40 1 32-1-2-3-4m\nJ\nFigure 6: Subproblem graph for the computatin of the associated Legendre polynomials. To compute\nP−1\n3 (x), we compute P1\n3 (x), for which we need P1\n2 (x) and P1\n1 (x). We store each intermediate\ncomputation, speeding up average computation time by a factor of ∼10 on CPU.\nTo understand how we recurse, we consider an example. Fig. 6 shows the space of J and m. The\nblack vertices represent a particular ALP, for instance, we have highlightedP−1\n3 (x). When m< 0,\nwe can use Eq. (49) to compute P−1\n3 (x) from P1\n3 (x). We can then use Eq. (50) to compute P1\n3 (x)\nfrom P1\n2 (x) and P1\n1 (x). P1\n2 (x) can also be computed from Eq. (50) and the boundary value P1\n1 (x)\ncan be computed directly using Eq. (48). Crucially, all intermediate ALPs are stored for reuse. Say\nwe wanted to compute P−1\n4 (x), then we could use Eq. (49) to ﬁnd it from P−1\n4 (x), which can be\nrecursed from the stored values P1\n3 (x) and P1\n2 (x), without needing to recurse down to the boundary.\nD Experimental Details\nD.1 ScanObjectNN\nD.1.1 SE(3)-Transformer and Tensor Field Network\nA particularity of object classiﬁcation from point clouds is the large number of points the algorithm\nneeds to handle. We use up to 200 points out of the available 2024 points per sample and create\nneighbourhoods with up to 40 nearest neighbours. It is worth pointing out that especially in this\nsetting, adding self-attention (i.e. when comparing the SE(3) Transformer to Tensor Field Networks)\nsigniﬁcantly increased the stability. As a result, whenever we swapped out the attention mechanism\nfor a convolution to retrieve the Tensor Field network baseline, we had to decrease the model size to\nobtain stable training. However, we would like to stress that all the Tensor Field networks we trained\nwere signiﬁcantly bigger than in the original paper [28], mostly enabled by the faster computation of\nthe spherical harmonics.\nFor the ablation study in Fig. 4, we trained networks with 4 hidden equivariant layers with 5 channels\neach, and up to representation degree 2. This results in a hidden feature size per point of\n5 ·\n2∑\nℓ=0\n(2ℓ+ 1) = 45 (51)\nWe used 200 points of the point cloud and neighbourhood size 40. For the Tensor Field network\nbaseline, in order to achieve stable training, we used a smaller model with 3 instead of 5 channels,\n100 input points and neighbourhood size 10, but with representation degrees up to 3.\nWe used 1 head per attention mechanism yielding one attention weight for each pair of points but\nacross all channels and degrees (for an implementation of multi-head attention, see Appendix D.3).\nFor the query embedding, we used the identity matrix. For the key embedding, we used a square\nequivariant matrix preserving the number of degrees and channels per degree.\nFor the quantitative comparison to the start-of-the-art in Table 2, we used 128 input points and\nneighbourhood size 10 for both the Tensor Field network baseline and the SE(3)-Transformer. We\nused farthest point sampling with a random starting point to retrieve the 128 points from the overall\n18\npoint cloud. We used degrees up to 3 and 5 channels per degree, which we again had to reduce to 3\nchannels for the Tensor Field network to obtain stable training. We used a norm based non-linearity\nfor the Tensor Field network (as in [ 28]) and no extra non-linearity (beyond the softmax in the\nself-attention algorithm) for the SE(3) Transformer.\nFor all experiments, the ﬁnal layer of the equivariant encoder maps to 64 channels of degree 0\nrepresentations. This yields a 64-dimensional SE(3) invariant representation per point. Next, we pool\nover the point dimension followed by an MLP with one hidden layer of dimension 64, a ReLU and a\n15 dimensional output with a cross entropy loss. We trained for 60000 steps with batch size 10. We\nused the Adam optimizer [13] with a start learning of 1e-2 and a reduction of the learning rate by\n70% every 15000 steps. Training took up to 2 days on a system with 4 CPU cores, 30 GB of RAM,\nand an NVIDIA GeForce GTX 1080 Ti GPU.\nThe input to the Tensorﬁeld network and the Se(3) Transformer are relative x-y-z positions of each\npoint w.r.t. their neighbours. To guarantee equivariance, these inputs are provided as ﬁelds of degree 1.\nFor the ‘+z‘ versions, however, we deliberately break the SE(3) equivariance by providing additional\nand relative z-position as two additional scalar ﬁelds (i.e. degree 0), as well as relative x-y positions\nas a degree 1 ﬁeld (where the z-component is set to 0).\nD.1.2 Number of Input Points\nLimiting the input to 128/200 points in our experiments on ScanObjectNN was not primarily due to\ncomputational limitations: we conducted experiments with up to 2048 points, but without performance\nimprovements. We suspect this is due to the global pooling. Examining cascaded pooling via attention\nis a future research direction. Interestingly, when limiting other methods to using 128 points, the SE(3)-\nTransformer outperforms the baselines (PointCNN: 80.3 ±0.8%, PointGLR: 81.5 ±1.0%, DGCNN:\n82.2 ±0.8%, ours: 85.0 ±0.7%). It is worth noting that these methods were explicitly designed\nfor the well-studied task of point classiﬁcation whereas the SE(3)-Transformer was applied as is.\nCombining different elements from the current state-of-the-art methods with the geometrical inductive\nbias of the SE(3)-Transformer could potentially yield additional performance gains, especially with\nrespect to leveraging inputs with more points. It is also worth noting that the SE(3)-Transformer was\nremarkably stable with respect to lowering the number of input points in an ablation study (16 points:\n79.2%, 32 points: 81.4%, 64 points: 82.5%, 128 points: 85.0%, 256 points: 82.6%).\nD.1.3 Sample Complexity\nEquivariance is known to often lead to smaller sample complexity, meaning that less training data\nis needed (Fig. 10 in Worrall et al. [40], Fig. 4 in Winkels and Cohen [38], Fig. 4 in Weiler et al.\n[37]). We conducted experiments with different amounts of training samples Nsamples from the\nScanObjectNN dataset. The results showed that for all Nsamples, the SE(3)-Transformer outperformed\nthe Set Transformer, a non-equivariant network based on attention. The performance delta was also\nslightly higher for the smallest Nsamples we tested (3.1% of the samples available in the training split\nof ScanObjectNN) than when using all the data indicating that the SE(3)-Transformer performs\nparticularly well on small amounts of training data. However, performance differences can be due to\nmultiple reasons. Especially for small datasets, such as ScanObjectNN, where the performance does\nnot saturate with respect to amount of data available, it is difﬁcult to draw conclusions about sample\ncomplexity of one model versus another. In summary, we found that our experimental results are in\nline with the claim that equivariance decreases sample complexity in this speciﬁc case but do not\ngive deﬁnitive support.\nD.1.4 Baselines\nDeepSet Baseline We originally replicated the implementation proposed in [46] for their object\nclassiﬁcation experiment on ModelNet40 [ 41]. However, most likely due to the relatively small\nnumber of objects in the ScanObjectNN dataset, we found that reducing the model size helped the\nperformance signiﬁcantly. The reported model had 128 units per hidden layer (instead of 256) and no\ndropout but the same number of layers and type of non-linearity as in [46].\nSet Transformer Baseline We used the same architecture as [ 16] in their object classiﬁcation\nexperiment on ModelNet40 [41] with an ISAB (induced set attention block)-based encoder followed\nby PMA (pooling by multihead attention) and an MLP.\n19\nD.2 Relational Inference\nFollowing Kipf et al. [14], we simulated trajectories for 5 charged, interacting particles. Instead of a\n2d simulation setup, we considered a 3d setup. Positive and negative charges were drawn as Bernoulli\ntrials (p= 0.5). We used the provided code base https://github.com/ethanfetaya/nri with\nthe following modiﬁcations: While we randomly sampled initial positions inside a [−5,5]3 box, we\nremoved the bounding-boxes during the simulation. We generated 5k simulation samples for training\nand 1k for testing. Instead of phrasing it as a time-series task, we posed it as a regression task: The\ninput data is positions and velocities at a random time step as well as the signs of the charges. The\nlabels (which the algorithm is learning to predict) are the positions and velocities 500 simulation time\nsteps into the future.\nTraining Details We trained each model for 100,000 steps with batch size 128 using an Adam\noptimizer [13]. We used a ﬁxed learning rate throughout training and conducted a separate hyper\nparameter search for each model to ﬁnd a suitable learning rate.\nSE(3)-Transformer Architecture We trained an SE(3)-Transformer with 4 equivariant layers,\nwhere the hidden layers had representation degrees {0,1,2,3}and 3 channels per degree. The input\nis handled as two type-1 ﬁelds (for positions and velocities) and one type-0 ﬁeld (for charges). The\nlearning rate was set to 3e-3. Each layer included attentive self-interaction.\nWe used 1 head per attention mechanism yielding one attention weight for each pair of points but\nacross all channels and degrees (for an implementation of multi-head attention, see Appendix D.3).\nFor the query embedding, we used the identity matrix. For the key embedding, we used a square\nequivariant matrix preserving the number of degrees and channels per degree.\nBaseline Architectures All our baselines fulﬁll permutation invariance (ordering of input points),\nbut only the Tensor Field network and the linear baseline are SE(3) equivariant. For theTensor Field\nNetwork[28] baseline, we used the same hyper parameters as for the SE(3) Transformer but with a\nlinear self-interaction and an additional norm-based nonlinearity in each layer as in Thomas et al.\n[28]. For the DeepSet[46] baseline, we used 3 fully connected layers, a pooling layer, and two more\nfully connected layers with 64 units each. All fully connected layers act pointwise. The pooling\nlayer uses max pooling to aggregate information from all points, but concatenates this with a skip\nconnection for each point. Each hidden layer was followed by a LeakyReLU. The learning rate was\nset to 1e-3. For the Set Transformer[16], we used 4 self-attention blocks with 64 hidden units and 4\nheads each. For each point this was then followed by a fully connected layer (64 units), a LeakyReLU\nand another fully connected layer. The learning rate was set to 3e-4.\nFor the linear baseline, we simply propagated the particles linearly according to the simulation\nhyperparamaters. The linear baseline can be seen as removing the interactions between particles from\nthe prediction. Any performance improvement beyond the linear baseline can therefore be interpreted\nas an indication for relational reasoning being performed.\nD.3 QM9\nThe QM9 regression dataset [21] is a publicly available chemical property prediction task consisting\nof 134k small drug-like organic molecules with up to 29 atoms per molecule. There are 5 atomic\nspecies (Hydrogen, Carbon, Oxygen, Nitrogen, and Flourine) in a molecular graph connected by\nchemical bonds of 4 types (single, double, triple, and aromatic bonds). ‘Positions’ of each atom,\nmeasured in ångtröms, are provided. We used the exact same train/validation/test splits as Anderson\net al. [1] of sizes 100k/18k/13k.\nThe architecture we used is shown in Table 4. It consists of 7 multihead attention layers interspersed\nwith norm nonlinearities, followed by a TFN layer, max pooling, and two linear layers separated by a\nReLU. For each attention layer, shown in Fig. 7, we embed the input to half the number of feature\nchannels before applying multiheaded attention [31]. Multiheaded attention is a variation of attention,\nwhere we partition the queries, keys, and values into H attention heads. So if our embeddings have\ndimensionality (4,16) (denoting 4 feature types with 16 channels each) and we use H = 8 attention\nheads, then we partition the embeddings to shape (4,2). We then combine each of the 8 sets of shape\n(4,2) queries, keys, and values individually and then concatenate the results into a single vector of the\noriginal shape (4,16). The keys and queries are edge embeddings, and thus the embedding matrices\nare of TFN type (c.f. Eq. (8)). For TFN type layers, the radial functions are learnable maps. For these\nwe used neural networks with architecture shown in Table 5.\n20\nFor the norm nonlinearities [40], we use\nNorm ReLU(fℓ) = ReLU(LN\n(\n∥fℓ∥\n)\n) · fℓ\n∥fℓ∥\n, where ∥fℓ∥=\n√\nℓ∑\nm=−ℓ\n(fℓm)2, (52)\nwhere LN is layer norm [2] applied across all features within a feature type. For the TFN baseline,\nwe used the exact same architecture but we replaced each of the multiheaded attention blocks with a\nTFN layer with the same output shape.\nThe input to the network is a sparse molecular graph, with edges represented by the molecular bonds.\nThe node embeedings are a 6 dimensional vector composed of a 5 dimensional one-hot embedding of\nthe 5 atomic species and a 1 dimension integer node embedding for number of protons per atom. The\nedges embeddings are a 5 dimensional vector consisting of a 4 dimensional one-hot embedding of\nbond type and a positive scalar for the Euclidean distance between the two atoms at the ends of the\nbond. For each regression target, we normalised the values by mean and dividing by the standard\ndeviation of the training set.\nWe trained for 50 epochs using Adam [ 13] at initial learning rate 1e-3 and a single-cycle cosine\nrate-decay to learning rate 1e-4. The batch size was 32, but for the TFN baseline we used batch\nsize 16, to ﬁt the model in memory. We show results on the 6 regression tasks not requiring\nthermochemical energy subtraction in Table 3. As is common practice, we optimised architectures\nand hyperparameters on εHOMO and retrained each network on the other tasks. Training took about\n2.5 days on an NVIDIA GeForce GTX 1080 Ti GPU with 4 CPU cores and 15 GB of RAM.\nTable 4: QM9 Network architecture: dout is the number of feature types of degrees 0,1,...,d out −1 at\nthe output of the corresponding layer and Cis the number of multiplicities/channels per feature type.\nFor the norm nonlinearity we use ReLUs, preceded by equivariant layer norm [37] with learnable\nafﬁne transform.\nNO. REPEATS LAYER TYPE dout C\n1x Input 1 6\n1x Attention: 8 heads 4 16\nNorm Nonlinearity 4 16\n6x Attention: 8 heads 4 16\nNorm Nonlinearity 4 16\n1x TFN layer 1 128\n1x Max pool 1 128\n1x Linear 1 128\n1x ReLU 1 128\n1x Linear 1 1\nTable 5: QM9 Radial Function Architecture. Cis the number of output channels at each layer. Layer\nnorm [2] is computed per pair of input and output feature types, which have Cin and Cout channels\neach.\nLAYER TYPE C\nInput 6\nLinear 32\nLayer Norm 32\nReLU 32\nLinear 32\nLayer Norm 32\nReLU 32\nLinear dout ∗Cin ∗Cout\n21\nInput: (din,Cin)\nV: (dout,C/2) K: (din,C/2) Q: (din,C/2)\nAttention: (dout,C/2), H heads\nLinear Projection: (dout,C)\nAttention Block\nFigure 7: Attention block for the QM9 dataset. Each component is listed with a tuple of numbers\nrepresenting the output feature types and multiplicities, so (4,32) means feature types 0,1,2,3 (with\ndimensionalities 1,3,5,7), with 32 channels per type.\nD.4 General Remark\nAcross experiments on different datasets with the SE(3)-Transformer, we made the observation that\nthe number of representation degrees have a signiﬁcant but saturating impact on performance. A\nbig improvement was observed when switching from degrees {0,1}to {0,1,2}. Adding type-3\nlatent representations gave small improvements, further representation degrees did not change the\nperformance of the model. However, higher representation degrees have a signiﬁcant impact on\nmemory usage and computation time. We therefore recommend representation degrees up to 2, when\ncomputation time and memory usage is a concern, and 3 otherwise.\n22",
  "topic": "Equivariant map",
  "concepts": [
    {
      "name": "Equivariant map",
      "score": 0.7522050142288208
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6107287406921387
    },
    {
      "name": "Corollary",
      "score": 0.598081111907959
    },
    {
      "name": "Transformer",
      "score": 0.5942859649658203
    },
    {
      "name": "Computer science",
      "score": 0.480782687664032
    },
    {
      "name": "Tying",
      "score": 0.47151386737823486
    },
    {
      "name": "Point cloud",
      "score": 0.45711708068847656
    },
    {
      "name": "Mathematics",
      "score": 0.3438321352005005
    },
    {
      "name": "Control theory (sociology)",
      "score": 0.3380325436592102
    },
    {
      "name": "Algorithm",
      "score": 0.32907161116600037
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2991511821746826
    },
    {
      "name": "Pure mathematics",
      "score": 0.22861582040786743
    },
    {
      "name": "Engineering",
      "score": 0.18462663888931274
    },
    {
      "name": "Electrical engineering",
      "score": 0.11402556300163269
    },
    {
      "name": "Control (management)",
      "score": 0.07566505670547485
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I4210145457",
      "name": "Robert Bosch (Taiwan)",
      "country": "TW"
    }
  ]
}