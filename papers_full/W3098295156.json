{
  "title": "PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation",
  "url": "https://openalex.org/W3098295156",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2283225712",
      "name": "Xinyu Hua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099626634",
      "name": "Lu Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4250641076",
    "https://openalex.org/W2949940827",
    "https://openalex.org/W2963332597",
    "https://openalex.org/W2044599851",
    "https://openalex.org/W2962735233",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2949413855",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2167984972",
    "https://openalex.org/W2133512280",
    "https://openalex.org/W3114869109",
    "https://openalex.org/W2962717182",
    "https://openalex.org/W1497300277",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2540413092",
    "https://openalex.org/W2916772188",
    "https://openalex.org/W2970383515",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2121678312",
    "https://openalex.org/W3039805635",
    "https://openalex.org/W2945735543",
    "https://openalex.org/W2931212643",
    "https://openalex.org/W2166957049",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W1948566616",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3034531294",
    "https://openalex.org/W3158986179",
    "https://openalex.org/W2970038984",
    "https://openalex.org/W4297779694",
    "https://openalex.org/W2106766220",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2739046565",
    "https://openalex.org/W8032586",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2953147883",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2963352809",
    "https://openalex.org/W2964029788",
    "https://openalex.org/W2885421725",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3034961030",
    "https://openalex.org/W2429300145",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2970530566",
    "https://openalex.org/W2752047430",
    "https://openalex.org/W2139079654",
    "https://openalex.org/W2935206035",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2962805889",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2947898088",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2251293245",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2085116970",
    "https://openalex.org/W2889009749",
    "https://openalex.org/W2962801572",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2963434219"
  ],
  "abstract": "Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often “rambling” without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 781–793,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n781\nPAIR: Planning and Iterative Reﬁnement in Pre-trained Transformers\nfor Long Text Generation\nXinyu Hua\nKhoury College of Computer Sciences\nNortheastern University\nBoston, MA\nhua.x@northeastern.edu\nLu Wang\nComputer Science and Engineering\nUniversity of Michigan\nAnn Arbor, MI\nwangluxy@umich.edu\nAbstract\nPre-trained Transformers have enabled im-\npressive breakthroughs in generating long\nand ﬂuent text, yet their outputs are often\n“rambling” without coherently arranged con-\ntent. In this work, we present a novel\ncontent-controlled text generation framework,\nPAIR, with planning and iterative reﬁnement,\nwhich is built upon a large model, BART.\nWe ﬁrst adapt the BERT model to automat-\nically construct the content plans, consist-\ning of keyphrase assignments and their corre-\nsponding sentence-level positions. The BART\nmodel is employed for generation without\nmodifying its structure. We then propose\na reﬁnement algorithm to gradually enhance\nthe generation quality within the sequence-to-\nsequence framework. Evaluation with auto-\nmatic metrics shows that adding planning con-\nsistently improves the generation quality on\nthree distinct domains, with an average of\n20 BLEU points and 12 METEOR points im-\nprovements. In addition, human judges rate\nour system outputs to be more relevant and co-\nherent than comparisons without planning.\n1 Introduction\nLarge pre-trained language models are the cor-\nnerstone of many state-of-the-art models in vari-\nous natural language understanding and generation\ntasks (Devlin et al., 2019; Liu et al., 2019; Lewis\net al., 2020), yet they are far from perfect. In gener-\nation tasks, although models like GPT-2 (Radford\net al., 2019) are able to produce plausible text, their\nspontaneous nature limits their utility in actual ap-\nplications, e.g., users cannot specify what contents\nto include, and in what order.\nTo make large models more useful in practice,\nand to improve their generation quality, we believe\nit is critical to inform them of when to say what ,\nwhich is addressed as content planning in tradi-\ntional generation systems (Duboue and McKeown,\nContent Plan (output by planning model):(1)acommunist3▷beginwith8▷coherentideology15▷[SEN]21\n(2)[SEN]4\n(3)noevidence2▷anycoherent8▷heldbeliefs12▷anytopic15▷[SEN]18\nPrompt: CMV . Donald Trump is a communist.\nTemplate:(1)__0__1__2a communist __5__6__7begin with __10\n__11__12__13__14coherent ideology__17__18__19__20\n(2)__0__1__2__3\n(3)__0__1no evidence __4__5__6__7any coherent __10\n__11held beliefs __14 any topic __17\nDraft (initial generation):(1)Well call him a communist, you must begin with that Donald Trump has some kind of coherent ideology to begin with.\n(2)Which is unlikely.(3)There is no evidence to suggest Donald Trump has any coherent or commonly held beliefs on any topic.Refined(final generation):(1)Tocall him a communist, you must begin with that hehas some kind of coherent ideology in the first place.(2)He does not.(3)There is no evidence whatsoever that Trump has any coherent, commonly held beliefs on any topic.\nI:Template construction\nIII: Refinement II: Generation \nwith content plan\nFigure 1: An argument generation example using Red-\ndit ChangeMyView. [Top] Partial output by our planner\nwith keyphrase assignment and positions (in subscripts)\nfor each sentence, segmented by special token [SEN],\nfrom which a template is constructed. [Bottom] A draft\nis ﬁrst produced and then reﬁned, with updated words\nhighlighted in italics.\n2001; Stent et al., 2004). Specially designed con-\ntrol codes and auxiliary planning modules have\nbeen integrated into neural models (Keskar et al.,\n2019; Moryossef et al., 2019; Hua and Wang,\n2019), yet those solutions require model architec-\nture modiﬁcation or retraining, making text genera-\ntion with large models a very costly endeavor.\nTo this end, this work aims to bring new in-\nsights into how to effectively incorporate content\nplans into large models to generate more rele-\n782\nvant and coherent text . We ﬁrst study a plan-\nning model trained from BERT (Devlin et al.,\n2019) to produce the initial content plan, which\nassigns keyphrases to different sentences and pre-\ndicts their positions. Next, we propose a content-\ncontrolled text generation framework, built upon\nthe pre-trained sequence-to-sequence (seq2seq)\nTransformer model BART (Lewis et al., 2020). As\nshown in Figure 1, our generation model takes in a\ncontent plan consisting of keyphrase assignments\nand their corresponding positions for each sentence.\nThe plan is encoded as a template, with [MASK]\ntokens added at positions where no content is spec-\niﬁed. Our model then outputs a ﬂuent and coherent\nmulti-sentence text (draft) to reﬂect the plan. This\nis done by ﬁne-tuning BART without modifying its\narchitecture.\nFurthermore, we present an iterative reﬁnement\nalgorithm to improve the generation in multiple\npasses, within the seq2seq framework. At each\niteration, tokens with low generation conﬁdence are\nreplaced with [MASK] to compose a new template,\nfrom which a new output is produced. Unlike prior\nreﬁnement algorithms that only permit editing in\nplace, our solution offers more ﬂexibility. Figure 1\nexempliﬁes the reﬁnement outcome.\nWe call our system PAIR (Planning And Itera-\ntive Reﬁnement).1 It is experimented on three dis-\ntinct domains: counter-argument generation with\nReddit ChangeMyView data, opinion article writ-\ning with the New York Times (NYT) corpus2 (Sand-\nhaus, 2008), and news report production on NYT.\nAutomatic evaluation with BLEU, ROUGE, and\nMETEOR shows that, by informing the generation\nmodel with sentence-level content plans, our model\nsigniﬁcantly outperforms a BART model ﬁne-tuned\nwith the same set of keyphrases as input (§5.1). Hu-\nman judges also rate our system outputs as more\nrelevant and coherent ( §5.2). Additionally, our\niterative reﬁnement strategy consistently improves\nthe generation quality according to both automatic\nscores and human evaluation. Finally, our model\nachieves better content control by reﬂecting the\nspeciﬁed keyphrases in the content plan, whose\noutputs are preferred by human to another version\nwith weaker control.\nTo summarize, our major contributions include:\n•We propose a novel content planner built upon\n1Code and data are available at: http://xinyuhua.\ngithub.io/Resources/emnlp20/\n2https://catalog.ldc.upenn.edu/\nLDC2008T19\nBERT to facilitate long-form text generation.\n•We present a novel template mask-and-ﬁll\nmethod to incorporate content planning into gener-\nation models based on BART.\n•We devise an iterative reﬁnement algorithm\nthat works within the seq2seq framework to ﬂexibly\nimprove the generation quality.\n2 Related Work\nContent Planning as a Generation Component.\nDespite the impressive progress made in many gen-\neration tasks, neural systems are known to pro-\nduce low-quality content (Wiseman et al., 2017;\nRohrbach et al., 2018), often with low relevance (Li\net al., 2016) and poor discourse structure (Zhao\net al., 2017; Xu et al., 2020). Consequently, plan-\nning modules are designed and added into neural\nsystems to enhance content relevance (Wiseman\net al., 2018; Moryossef et al., 2019; Yao et al.,\n2019; Hua and Wang, 2019). However, it is still\nan open question to include content plans in large\nmodels, given the additional and expensive model\nretraining required. This work innovates by adding\ncontent plans as masked templates and designing\nreﬁnement strategy to further boost generation per-\nformance, without architectural change.\nControlled Text Generation. Our work is also in\nline with the study of controllability of neural text\ngeneration models. This includes manipulating the\nsyntax (Duˇsek and Jurˇc´ıˇcek, 2016; Goyal and Dur-\nrett, 2020) and semantics (Wen et al., 2015; Chen\net al., 2019) of the output. Speciﬁc applications\nencourage the model to cover a given topic (Wang\net al., 2017; See et al., 2019), mention speciﬁed\nentities (Fan et al., 2018), or display a certain at-\ntribute (Hu et al., 2017; Luo et al., 2019; Balakr-\nishnan et al., 2019). However, most existing work\nrelies on model engineering, limiting the general-\nizability to new domains and adaptability to large\npre-trained Transformers. One exception is the\nPlug and Play model (Dathathri et al., 2020), which\ndirectly modiﬁes the key and value states of GPT-\n2 (Radford et al., 2019). However, since the signal\nis derived from the whole generated text, it is too\ncoarse to provide precise sentence-level content\ncontrol. Here, we instead gain ﬁne-grained con-\ntrollability through keyphrase assignment and posi-\ntioning per sentence, which can be adapted to any\noff-the-shelf pre-trained Transformer generators.\nIterative Reﬁnement has been studied in machine\ntranslation (Lee et al., 2018; Freitag et al., 2019;\n783\nMansimov et al., 2019; Kasai et al., 2020) to grad-\nually improve translation quality. Reﬁnement is\nalso used with masked language models to im-\nprove ﬂuency of non-autoregressive generation out-\nputs (Ghazvininejad et al., 2019; Lawrence et al.,\n2019). Our work uses BART (Lewis et al., 2020),\na state-of-the-art seq2seq model that offers better\ngeneralizability and stronger capacity for long text\ngeneration. Our proposed strategy substantially dif-\nfers from prior solutions that rely on in-place word\nsubstitutions (Novak et al., 2016; Xia et al., 2017;\nWeston et al., 2018), as we leverage the seq2seq\narchitecture to offer more ﬂexible edits.\n3 Content-controlled Text Generation\nwith PAIR\nTask Description. Our input consists of (1) a\nsentence-level prompt x, such as a news headline,\nor a proposition in an argument, and (2) a set of\nkeyphrases m that are relevant to the prompt. The\nsystem aims to generate y that contains multiple\nsentences, as in a news report or an argument, by\nreﬂecting the keyphrases in a coherent way.\nIn this section, we ﬁrst introduce content plan-\nning built upon BERT, that assigns keyphrases into\nsentences and predicts their positions (§3.1). Then\nwe propose a seq2seq generation framework with\nBART ﬁne-tuning that includes a given content\nplan derived from keyphrases m (§3.2). Finally,\n§3.3 discusses improving generation quality by\niteratively masking the less conﬁdent predictions\nand regenerating within our framework.\n3.1 Content Planning with BERT\nOur content planner is trained from BERT to as-\nsign keyphrases to different sentences and predict\ntheir corresponding positions. As shown in Fig-\nure 2, the concatenation of promptx and unordered\nkeyphrases m is encoded with bidirectional self-\nattentions. Keyphrase assignments are produced au-\ntoregressively as a sequence of tokens m′= {wj},\nwith their positions in the sentence s = {sj}pre-\ndicted as a sequence tagging task.\nWe choose BERT because it has been shown\nto be effective at both language modeling and se-\nquence tagging. Moreover, we leverage its segment\nembedding to distinguish the input and output se-\nquences. Speciﬁcally, we reuse its pre-trained lan-\nguage model output layer for keyphrase assignment.\nWe further design a separate keyphrase positioning\nlayer to predict token position sj as the relative\n w1        w2       w3   \nKP-1\nLanguage model output layer\n[SEP] [BOK]\nBidirectional self-attention\nBERT\nPrompt\nx\nKeyphrase set\nm\nw2\nw1\n w3\n w4\ns1\n s2\n s3\n s4\nKP-2\nPosition prediction layer\nSegment type: 1\nSegment type: 2\nCausal attention\nFigure 2: Content planning with BERT. We use bidi-\nrectional self-attentions for input encoding, and ap-\nply causal self-attentions for keyphrase assignment and\nposition prediction. The input ( x, m) and output\nkeyphrase assignments ( m′) are distinguished by dif-\nferent segment embeddings.\ndistance from each sentence’s beginning:\np(sj|w≤j) =softmax(HLWs) (1)\nwhere HL is the last layer hidden states of\nthe Transformer, and Ws are the newly added\nkeyphrase positioning parameters learned during\nBERT ﬁne-tuning. The range of allowed positions\nis from 0 to 127.\nNoticeably, as our prediction is done autoregres-\nsively, attentions should only consider the gener-\nated tokens, but not the future tokens. However,\nBERT relies on bidirectional self-attentions to at-\ntend to both left and right. To resolve this discrep-\nancy, we apply causal attention masks (Dong et al.,\n2019) over m′to disallow attending to the future\n(gray arrows in Figure 2).\nTraining the Planner. We extract keyphrases and\nacquire their ground-truth positions from human-\nwritten references, and ﬁne-tune BERT with cross-\nentropy losses for both assignment and positioning,\nwith a scaling factor 0.1 over the positioning loss.\nInference. A [BOK] token signals the beginning\nof keyphrase assignment generation. We employ a\ngreedy decoding algorithm, and limit the output vo-\ncabulary to tokens in m and ensure each keyphrase\nis generated at most once. To allow sentence-level\ncontent planning, a special [SEN] token is gener-\nated to represent the sentence boundary, with its\npredicted position indicating the length. The plan-\nning process terminates when [EOS] is produced.\n784\nPrompt!\n Keyphrase Assignment\"′\n Masked Template\n$(&'()\n Draft: *(&)\nDecoder\nEncoder\n$(&)\nmaskupdate\na communist begin with coherent ideology[SEN][…]\n[M][M][M]a communist3[M][M][M]begin with8[M][M][M][M][M][M]coherent ideology15 […]\nWell call him a communist3 , you must begin with8Donald Trump has some kind of coherent ideology15 […]\nInitial template construction\nGeneration with content plan\n$(+)\n*(()\npositions\nFigure 3: Our content-controlled text generation framework, PAIR, which is built on BART. Decoding is executed\niteratively. At each iteration, the encoder consumes the input prompt x, the keyphrase assignments m′, as well as\na partially masked template (t(r−1) for the r-th iteration, [M] for masks). The autoregressive decoder produces a\ncomplete sequence y(r), a subset of which is further masked, to serve as the next iteration’s templatet(r).\n3.2 Adding Content Plan with a Template\nMask-and-Fill Procedure\nGiven a content planning model, we invoke it to out-\nput keyphrase assignments to different sentences\n(m′), their corresponding positions s, along with\neach sentence’s length (based on the prediction of\n[SEN]). We ﬁrst employ a post-processing step to\nconvert between different tokenizers, and correct\nerroneous position predictions that violate the as-\nsignment ordering or break the consecutivity of the\nphrase (Appendix A). We then convert the plan into\na template t(0) as follows: For each sentence, the\nassigned keyphrases are placed at their predicted\npositions, and empty slots are ﬁlled with [MASK]\nsymbols. Figure 3 illustrates the template construc-\ntion process and our seq2seq generation model. In\nAppendix B, we show statistics on the constructed\ntemplates.\nThe input prompt x, keyphrase assignments m′,\nand template t(0) are concatenated as the input to\nthe encoder. The decoder then generates an out-\nput y(1) according to the model’s estimation of\np(y(1)|x,m′,t(0)). y(1) is treated as a draft, to be\nfurther reﬁned as described in the next section.\nOur method is substantially different from prior\nwork that uses constrained decoding to enforce\nwords to appear at speciﬁc positions (Hokamp and\nLiu, 2017; Post and Vilar, 2018; Hu et al., 2019),\nwhich is highly biased by the surrounding few\nwords and suffers from disﬂuency. Since BART is\ntrained to denoise the masked input with contextual\nunderstanding, it naturally beneﬁts our method.\nDecoding. We employ the nucleus sampling strat-\negy (Holtzman et al., 2019), which is shown to\nyield superior output quality in long text generation.\nIn addition to the standard top- k sampling from\ntokens with the highest probabilities, nucleus sam-\npling further limits possible choices based on a cu-\nmulative probability threshold (set to 0.9 in all ex-\nperiments below). We also require the keyphrases\nto be generated at or nearby their predicted po-\nsitions. Concretely, for positions that match any\nkeyphrase token, we force the decoder to copy the\nkeyphrase unless it has already been generated in\nthe previous ﬁve tokens. We sample three times\nto choose the one with the lowest perplexity, as\nestimated by GPT-2base (Radford et al., 2019).\n3.3 Iterative Reﬁnement\nOutputs generated in a single pass may suffer\nfrom incorrectness and incoherence (see Figure 1),\ntherefore we propose an iterative reﬁnement pro-\ncedure to improve the quality. In each pass, to-\nkens with low generation conﬁdence are masked\n(Algorithm 1). This is inspired by iterative de-\ncoding designed for inference acceleration in\nnon-autoregressive generation (Lee et al., 2018;\nLawrence et al., 2019), though their reﬁnement\nmostly focuses on word substitution and lacks the\nﬂexibility for other operations. Moreover, our goal\nis to improve ﬂuency while ensuring the generation\nof given keyphrases.\nAt each iteration, the n least conﬁdent tokens\nare replaced with [MASK]. Similar as the mask-\npredict algorithm (Ghazvininejad et al., 2019), we\ngradually reduce the number of masks. In our ex-\nperiments, each sample is reﬁned for 5 iterations,\nwith ndecaying linearly from 80% of |y(r)|to 0.\nTraining the Generator. Our training scheme is\nsimilar to masked language model pre-training.\nGiven the training corpus D= {(xi,m′\ni,yi)}, we\nconsider two approaches that add noise to the tar-\nget yi by randomly masking a subset of (1) any\ntokens, or (2) tokens that are not within the span\n785\nAlgorithm 1: Iteratively reﬁnement via\ntemplate mask-and-ﬁll. The sample with\nthe lowest perplexity (thus with better ﬂu-\nency) is selected for each iteration.\nData: prompt x, keyphrase assignments m′,\nkeyphrase positions s, Rreﬁnement\niterations, ρnucleus sampling runs\nResult: ﬁnal output y(R)\nConstruct template t(0) based on m′and s ;\nfor r= 1to Rdo\nRun encoder over x ⊕m′⊕t(r−1) ;\nY← ∅ ;\nfor i= 1to ρdo\nRun nucleus sampling to generate yi\nwith keyphrase position\nenforcement;\nAppend yi to Y;\ny(r) ←argminyi∈YGPT2-PPL(yi);\nn←|y(r)|×(1 −r/R);\nMask ntokens with the lowest\nprobabilities to create new template\nt(r);\nof any keyphrase. The latter is better aligned with\nour decoding objective, since keyphrases are never\nmasked. We concatenate xi, m′\ni, and the corrupted\ntarget ˜yi as input, and ﬁne-tine BART to recon-\nstruct the original yi with a cross-entropy loss.\n4 Experiment Setups\n4.1 Tasks and Datasets\nWe evaluate our generation and planning models\non datasets from three distinct domains for multi-\nparagraph-level text generation: (1) argument gen-\neration (ARGGEN) (Hua et al., 2019), to produce a\ncounter-argument to refute a given proposition; (2)\nwriting opinionated articles (OPINION ), e.g., edito-\nrials and op-eds, to show idea exchange on a given\nsubject; and (3) composing news reports (NEWS )\nto describe events. The three domains are selected\nwith diverse levels of subjectivity and various com-\nmunicative goals (persuading vs. informing), with\nstatistics shown in Table 1.\nTask 1: Argument Generation. We ﬁrst eval-\nuate our models on persuasive argument gener-\nation, based on a dataset collected from Red-\ndit r/ChangeMyView (CMV) in our prior\nwork (Hua et al., 2019). This dataset contains\npairs of original post (OP) statement on a contro-\n# Sample |Prompt||Target|# KP KP Cov.\nARGGEN 56,504 19.4 116.6 20.6 30.5%\nOPINION 104,610 6.1 205.6 19.0 26.0%\nNEWS 239,959 7.0 282.7 30.3 32.6%\nTable 1: Statistics of the three datasets. We report av-\nerage lengths of the prompt and the target generation,\nnumber of unique keyphrases (# KP) used in the input,\nand the percentage of content words in target covered\nby the keyphrases (KP Cov.).\nversial issue about politics and ﬁltered high-quality\ncounter-arguments, covering 14,833 threads from\n2013 to 2018. We use the OP title, which con-\ntains a proposition (e.g. the minimum wage should\nbe abolished), to form the input prompt x. In our\nprior work, only the ﬁrst paragraphs of high-quality\ncounter-arguments are used for generation. Here\nwe consider generating the full post, which is sig-\nniﬁcantly longer. Keyphrases are identiﬁed as noun\nphrases and verb phrases that contain at least one\ntopic signature word (Lin and Hovy, 2000), which\nis determined by a log-likelihood ratio test that in-\ndicates word salience. Following our prior work,\nwe expand the set of topic signatures with their syn-\nonyms, hyponyms, hypernyms, and antonyms ac-\ncording to WordNet (Miller, 1994). The keyphrases\nlonger than 10 tokens are further discarded.\nTask 2: Opinion Article Generation. We collect\nopinion articles from the New York Times (NYT)\ncorpus (Sandhaus, 2008). An article is selected if\nits taxonomies label has a preﬁx ofTop/Opinion.\nWe eliminate articles with an empty headline or less\nthan three sentences. Keyphrases are extracted in\na similar manner as done in argument generation.\nSamples without any keyphrase are removed. The\narticle headline is treated as the input, and our tar-\nget is to construct the full article. Table 1 shows that\nopinion samples have shorter input than arguments,\nand the keyphrase set also covers fewer content\nwords in the target outputs, requiring the model to\ngeneralize well to capture the unseen tokens.\nTask 3: News Report Generation. Simi-\nlarly, we collect and process news reports from\nNYT, ﬁltering by taxonomy labels starting with\n“Top/News”, removing articles that have no con-\ntent word overlap with the headline, and ones\nwith material-types labeled as one of “statis-\ntics”, “list”, “correction”, “biography”, or “review.”\nNews reports describe events and facts, and in this\ndomain we aim to study and emphasize the impor-\n786\nARGGEN OPINION NEWS\nB-4 R-L MTR Len. B-4 R-L MTR Len. B-4 R-L MTR Len.\nSEQ2SEQ 0.76 13.80 9.36 97 1.42 15.97 10.97 156 1.11 15.60 10.10 242\nKPS EQ2SEQ 6.78 19.43 15.98 97 11.38 22.75 18.38 164 11.61 21.05 18.61 286\nPAIRlight 26.38 47.97 31.64 119 16.27 33.30 24.32 210 28.03 43.39 27.70 272\nPAIRlight w/o reﬁne 25.17 46.84 31.31 120 15.45 32.35 24.11 214 27.32 43.08 27.35 278\nPAIRfull 36.09 56.86 33.30 102 23.12 40.53 24.73 167 34.37 51.10 29.50 259\nPAIRfull w/o reﬁne 34.09 55.42 32.74 101 22.17 39.71 24.65 169 33.48 50.27 29.26 260\nTable 2: Key results on argument generation, opinion article writing, and news report generation. BLEU-4 (B-\n4), ROUGE-L (R-L), METEOR (MTR), and average output lengths are reported (for references, the lengths are\n100, 166, and 250, respectively). PAIRlight, using keyphrase assignments only, consistently outperforms baselines;\nadding keyphrase positions, PAIR full further boosts scores. Improvements by our models over baselines are all\nsigniﬁcant (p< 0.0001, approximate randomization test). Iterative reﬁnement helps on both setups.\ntance of faithfully reﬂecting content plans during\ngeneration and reﬁnement.\nData Split and Preprocessing. For argument gen-\neration, we split the data into 75%, 12.5%, and\n12.5% for training, validation, and test sets. To\navoid test set contamination, the split is conducted\non thread level. For opinion and news generation,\nwe reserve the most recent 5k articles for testing,\nanother 5k for validation, and the rest (23k for news\nand 10k for opinion) are used for training. We ap-\nply the BPE tokenization (Sennrich et al., 2016)\nfor the generation model as BART does, and use\nWordPiece (Wu et al., 2016) for BERT-based plan-\nner. To ﬁt the data into our GPUs, we truncate the\ntarget size to 140 tokens for argument, sizes of 243\nand 335 are applied for opinion and news, for both\ntraining and inference.\n4.2 Implementation Details\nOur code is written in PyTorch (Paszke et al.,\n2019). For ﬁne-tuning, we adopt the standard\nlinear warmup and inverse square root decaying\nscheme for learning rates, with a maximum value\nof 5 ×10−5. Adam (Kingma and Ba, 2014) is used\nas the optimizer, with a batch size of 10 for reﬁne-\nment and 20 for content planning, and a maximum\ngradient clipped at 1.0. All hyperparameters are\ntuned on validation set, with early stopping used to\navoid overﬁtting. More details are in Appendix A.\n4.3 Baselines and Comparisons\nWe consider two baselines, both are ﬁne-tuned\nfrom BART as in our models: (1) SEQ2SEQ di-\nrectly generates the target from the prompt; (2)\nKPS EQ2SEQ encodes the concatenation of the\nprompt and the unordered keyphrase set. To study\nif using only sentence-level keyphrase assignments\nhelps, we include a model variant (PAIRlight) by\nremoving keyphrase position information (s) from\nthe input of our generator and using an initial tem-\nplate with all [MASK] symbols. Our model with\nfull plans is denoted as PAIRfull. We ﬁrst re-\nport generation results using ground-truth content\nplans constructed from human-written text, and\nalso show the end-to-end results with predicted\ncontent plans by our planner.\n5 Results\n5.1 Automatic Evaluation\nWe report scores with BLEU (Papineni et al.,\n2002), which is based on n-gram precision (up\nto 4-grams); ROUGE-L (Lin, 2004), measuring\nrecall of the longest common subsequences; and\nMETEOR (Lavie and Agarwal, 2007), which ac-\ncounts for paraphrase. For our models PAIRfull and\nPAIRlight, we evaluate both the ﬁrst draft and the ﬁ-\nnal output after reﬁnement. Table 2 lists the results\nwhen ground-truth content plans are applied.\nFirst, our content-controlled generation model\nwith planning consistently outperforms compar-\nisons and other model variants on all datasets ,\nwith or without iterative reﬁnement. Among our\nmodel variants, PAIRfull that has access to full con-\ntent plans obtains signiﬁcantly better scores than\nPAIRlight that only includes keyphrase assignments\nbut not their positions. Lengths of PAIRfull’s out-\nputs are also closer to those of human references.\nBoth imply the beneﬁt of keyphrase positioning.\nTable 2 also shows thatthe iterative reﬁnement\nstrategy can steadily boost performance on both\nof our setups. By inspecting the performance of\nreﬁnement in different iterations (Figure 4), we\nobserve that both BLEU and ROUGE-L scores\ngradually increase while perplexity lowers as the\n787\n1 2 3 4 520\n25\n30\n35\nBLEU-4\n1 2 3 4 5\n40\n45\n50\n55\nROUGE-L\n1 2 3 4 5\n20\n30\n40\nPerplexity\nArgGen Opinion News\nFigure 4: Results on iterative reﬁnement with ﬁve it-\nerations. Both BLEU and ROUGE-L scores steadily\nincrease, with perplexity lowers in later iterations.\n5\n10\n15BLEU-4\n6.8\n12.0 12.4 13.2\nArgGen\n5\n10\n15\n11.4\n9.0\n10.7 11.2\nOpinion\n5\n10\n11.6 11.8 12.0 12.5\nNews\n10\n20\n30METEOR\n16.0\n24.2 23.7 23.8\n10\n20 18.4\n20.6 19.6 19.7\n10\n20 18.6\n20.9 20.5 20.5\nKPSeq2seq PAIRlight w/o refine PAIRfull w/o refine PAIRfull\nFigure 5: End-to-end generation results with automati-\ncally predicted content plans. Our models outperform\nKPS EQ2SEQ in both metrics, except for BLEU-4 on\nopinion articles where results are comparable.\nreﬁnement progresses. This indicates that iterative\npost-editing improves both content and ﬂuency.\nResults with Predicted Content Plans. We fur-\nther report results by using content plans predicted\nby our BERT-based planner. Figure 5 compares\nPAIRfull and PAIRlight with KPS EQ2SEQ . Our\nmodels yield better METEOR scores on all three\ndomains. That said, the improvement from pre-\ndicted plans is not as pronounced as that from\nground-truth plans. Upon inspection, we ﬁnd that\nour planner often falls short of accurately posi-\ntioning the given keyphrases, leading to degraded\ngeneration performance. This points to a potential\ndirection for future work where better positioning\nmodel should be developed.\n5.2 Human Evaluation\nWe hire four proﬁcient English speakers 3 to rate\nthree aspects of the generated arguments on a scale\nof 1 (worst) to 5 (best): ﬂuency, coherence—if\nthe information organization is natural and logi-\ncal, and relevance—if the topic is related to the\nprompt and whether the stance is correct. 50 sam-\nples are randomly selected, with system outputs\nby KPS EQ2SEQ , PAIRfull and PAIRlight shown to\nhuman judges in random order. The evaluation\n3They are all US-based college students. Each of them is\npaid $15 hourly for the task.\nARGGEN Fluency Coherence Relevance\nKPS EQ2SEQ 4.63 3.28 2.79\nPAIRlight 4.75 3.97 ∗ 3.85∗\nPAIRfull 4.46 3.76 ∗ 3.79∗\nTable 3: Human evaluation for argument generation on\nﬂuency, coherence, and relevance, with 5 as the best.\nThe Krippendorff’s αare 0.28, 0.30, and 0.37, respec-\ntively. Our model outputs are signiﬁcantly more coher-\nent and relevant than KPS EQ2SEQ (∗: p < 0.0001),\nwith comparable ﬂuency.\nPrompt (News): 4 Arrested in Theft of Baby Jesus Figurines\nPAIRfull: Four New Jersey teenagers arrested yesterday\nwere accused of stealing more than25 plastic baby Jesus ﬁg-\nurines from a church before they burn in a bonﬁre, the po-\nlice said. The police in Sayreville, N.J., arrested Michael\nPayne, 18, and T.J. Jones, 18, of Union City; Nicholas M.\nHess, 18, of Matawan; and Jason L. O’Neill, 18, of Port New\nYork, N.J., and charged them with burglary. Their vandals\nremoved more than 100 ﬁgurines of Jesus from a cemetery\noutside St. Stanislaus Kostka Church in Sayreville, the\npolice said. Detectives said the four had taken their ﬂash-\nlights to the cemetery and jotted down the license plates\nof several cars in the vicinity [. . . ]\nPrompt (Opinion): Drowning in a Sea of College Debt\nPAIRfull: Once again, the appalling problem of increasing\ndebt in ﬁnancing higher education that has taken place is\nmore intolerable, particularly because of the arrogance of\nour colleges and universities. The burden ofsaddling college\nstudents with debt in the middle of theirteenage years, when\nthey were in debt, is essential for a good education . Our\neducational system is designed to allow kids to develop the\nskills necessary, but it does not create optimal conditions\nfor mature students who know they will not be able [. . . ]\nTable 4: Sample outputs in the news and opinion do-\nmain. Keyphrases assigned to different sentences are\nin boldface and color-coded.\nguideline is in the supplementary material.\nTable 3 shows that both of our models are\nrated with better coherence and relevance than\nKPS EQ2SEQ which uses the same but unordered\nkeyphrases as input. Interestingly, outputs by\nPAIRlight are regarded as more ﬂuent and coherent,\nthough the difference is not signiﬁcant. However,\ndiscourse analysis in §6 reveals that clauses pro-\nduced by PAIRlight are more locally related, com-\npared to PAIRfull, which can be perceived as easier\nto read. In addition to the sample argument in Fig-\nure 1, Table 4 shows PAIRfull’s output in the news\nand opinion domains. More samples by different\nsystems are in the supplementary material.\nEffect of Reﬁnement and Keyphrase Enforce-\n788\nment. We further askwhether human judges prefer\nthe reﬁned text and whether enforcing keyphrases\nto be generated yields noticeable content improve-\nment. In a second study, we present the same 50\nprompts from the previous evaluation on argument\ngeneration, and an additional 50 samples for opin-\nion article writing to the same group of human\njudge. For each sample, PAIR full’s outputs with\nand without reﬁnement are shown in random or-\nder. Judges indicate their preference based on the\noverall quality. The same procedure is conducted\nto compare with a version where we do not enforce\nkeyphrases to be copied at their predicted positions\nduring decoding. Table 5 demonstrates that the re-\nﬁned text is preferred in more than half of the cases,\nfor both domains. Enforcing keyphrase generation\nbased on their positions is also more favorable than\nnot enforcing such constraint.\nPAIRfull w/o reﬁne PAIRfull w/o enforce\nARGGEN 52.7% 33.3% 45.3% 40.0%\nOPINION 52.7% 30.7% 50.0% 29.3%\nTable 5: Percentages of samples preferred by human\njudges before and after reﬁnement [Left]; with and\nwithout enforcing keyphrases to appear at the predicted\npositions [Right]. Ties are omitted.\nWhat is updated during iterative reﬁnement?\nSince reﬁnement yields better text, we compare\ngenerations before and after the reﬁnement. First,\nwe ﬁnd that masks are regularly put on “functional”\nwords and phrases. For example, stopwords and\npunctuation along with their bigrams are often\nswapped out, with new words ﬁlled in to improve\nﬂuency. Moreover, about 85% of the reﬁnement op-\nerations result in new content being generated. This\nincludes changing prepositions and paraphrasing,\ne.g., replacing “a research fellow” with “a gradu-\nate student.” On both news and opinion domains,\nnumerical and temporal expressions are often incor-\nrectly substituted, suggesting that better fact control\nneeds to be designed to maintain factuality.\n6 Further Discussions on Discourse\nPrior work’s evaluation mainly focuses on ﬂuency\nand content relevance, and largely ignores the dis-\ncourse structure exposed by the generated text.\nHowever, unnatural discourse and lack of focus\nare indeed perceived as major problems of long-\nform neural generations, as identiﬁed by human ex-\n0\n5\n10\n15RST Tree Depth\nArgGen\nKPSeq2seq PAIRlight PAIRfull Human\n5\n10\n15\nOpinion\n5\n10\n15\nNews\nFigure 6: Distributions of RST tree depth. PAIRfull bet-\nter resembles the patterns in human-written texts.\nperts.4 Here, we aim to investigate whether content-\ncontrolled generation with ground-truth content\nplans resembles human-written text by studying\ndiscourse phenomena.\nAre PAIR generations similar to human-\nwritten text in discourse structure? We uti-\nlize DPLP (Ji and Eisenstein, 2014), an off-the-\nshelf Rhetorical Structure Theory (RST) discourse\nparser. DPLP converts a given text into a binary\ntree, with elementary discourse units (EDUs, usu-\nally clauses) as nucleus and satellite nodes. For\ninstance, a relation NS-elaboration indicates\nthe second node as a satellite (S) elaborating on the\nﬁrst nucleus ( N) node. DPLP achieves F1 scores\nof 81.6 for EDU detection and 71.0 for relation\nprediction on news articles from the annotated RST\nDiscourse Treebank (Carlson et al., 2001). We\nrun this trained model on our data for both human\nreferences and model generations.\nFirst, we analyze the depth of RST parse trees,\nwhich exhibits whether the text is more locally or\nglobally connected. For all trees, we truncate at a\nmaximum number of EDUs based on the 90 per-\ncentile of EDU count for human references. Distri-\nbutions of tree depth are displayed in Figure 6. As\ncan be seen, generations by PAIRfull show similar\npatterns to human-written arguments and articles.\nWe also ﬁnd that trees by PAIR light tend to have\na more “linear” structure, highlighting the domi-\nnance of local relations between adjacent EDUs,\ncompared with PAIRfull which uses knowledge of\nkeyphrases positions. This implies that content po-\nsitioning helps with structure at a more global level.\nWe further look into the ratios of NS, NN, SN re-\nlations, and observe that most model outputs have\nsimilar trends as human-written texts, except for\nKPS EQ2SEQ which has more SN relations, e.g., it\nproduces twice as many SNs than others on argu-\nments.\n4https://www.economist.com/open-future/2019/10/01/\nhow-to-respond-to-climate-change-if-you-are-an-algorithm\n789\nif\nbecausegiven lest until lest thoughhoweverwhetherdespitein fact nor yet\nfor example\nfurtherin additioninsteadnot onlythat is\n100\n101\n102\n103 Argument\ncorrect\nOpinion\nwrong\nNews\nFigure 7: Discourse markers that are correctly and incorrectly (shaded) generated by PAIRfull, compared to aligned\nsentences in human references. Discourse markers are grouped (from left to right) into senses of C ONTINGENCY\n(higher marker generation accuracy observed), COMPARISON , and EXPANSION . y-axis: # of generated sentences\nwith the corresponding marker.\nCan PAIR correctly generate discourse mark-\ners? Since discourse markers are crucial for coher-\nence (Grote and Stede, 1998; Callaway, 2003) and\nhave received dedicated research efforts in rule-\nbased systems (Reed et al., 2018; Balakrishnan\net al., 2019), we examine if PAIRfull can properly\ngenerate them. For each sample, we construct sen-\ntence pairs based on content word overlaps between\nsystem generation and human reference. We manu-\nally select a set of unambiguous discourse markers\nfrom Appendix A of the Penn Discourse Treebank\nmanual (Prasad et al., 2008). When a marker is\npresent in the ﬁrst three words in a reference sen-\ntence, we check if the corresponding system output\ndoes the same.\nFigure 7 displays the numbers of generated sen-\ntences with markers produced as the same in human\nreferences (correct) or not (wrong). The markers\nare grouped into three senses: CONTINGENCY ,\nCOMPARISON , and EXPANSION . The charts indi-\ncates that PAIRfull does better at reproducing mark-\ners for CONTINGENCY , followed by COMPARISON\nand EXPANSION . Manual inspections show that\ncertain missed cases are in fact plausible replace-\nments, such as using at the same time for\nin addition, or also for further, while in\nother cases the markers tend to be omitted. Overall,\nwe believe that content control alone is still insuf-\nﬁcient to capture discourse relations, motivating\nfuture work on discourse planning.\n7 Ethics Statement\nWe recognize that the proposed system can gener-\nate fabricated and inaccurate information due to\nthe systematic biases introduced during model pre-\ntraining based on web corpora. We urge the users\nto cautiously examine the ethical implications of\nthe generated output in real world applications.\n8 Conclusion\nWe present a novel content-controlled generation\nframework that adds content planning to large pre-\ntrained Transformers without modifying model ar-\nchitecture. A BERT-based planning model is ﬁrst\ndesigned to assign and position keyphrases into dif-\nferent sentences. We then investigate an iterative\nreﬁnement algorithm that works with the sequence-\nto-sequence models to improve generation quality\nwith ﬂexible editing. Both automatic evaluation\nand human judgments show that our model with\nplanning and reﬁnement enhances the relevance\nand coherence of the generated content.\nAcknowledgements\nThis research is supported in part by National Sci-\nence Foundation through Grant IIS-1813341 and\nNvidia GPU gifts. We thank three anonymous re-\nviewers for their constructive suggestions on many\naspects of this work.\nReferences\nAnusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani,\nMichael White, and Rajen Subba. 2019. Con-\nstrained decoding for neural NLG from composi-\ntional representations in task-oriented dialogue. In\nProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 831–\n844, Florence, Italy. Association for Computational\nLinguistics.\nCharles B. Callaway. 2003. Integrating discourse mark-\ners into a pipelined natural language generation ar-\nchitecture. In Proceedings of the 41st Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 264–271, Sapporo, Japan. Association\nfor Computational Linguistics.\n790\nLynn Carlson, Daniel Marcu, and Mary Ellen\nOkurovsky. 2001. Building a discourse-tagged cor-\npus in the framework of Rhetorical Structure Theory.\nIn Proceedings of the Second SIGdial Workshop on\nDiscourse and Dialogue.\nMingda Chen, Qingming Tang, Sam Wiseman, and\nKevin Gimpel. 2019. A multi-task approach for dis-\nentangling syntax and semantics in sentence repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 2453–2464, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language mod-\nels: A simple approach to controlled text generation.\nIn International Conference on Learning Represen-\ntations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems, pages 13042–13054.\nPablo A. Duboue and Kathleen R. McKeown. 2001.\nEmpirically estimating order constraints for content\nplanning in generation. In Proceedings of the 39th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 172–179, Toulouse, France.\nAssociation for Computational Linguistics.\nOndˇrej Duˇsek and Filip Jur ˇc´ıˇcek. 2016. Sequence-to-\nsequence generation for spoken dialogue via deep\nsyntax trees and strings. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n45–51, Berlin, Germany. Association for Computa-\ntional Linguistics.\nW A Falcon. 2019. Pytorch lightning. GitHub. Note:\nhttps://github. com/williamFalcon/pytorch-lightning\nCited by, 3.\nAngela Fan, David Grangier, and Michael Auli. 2018.\nControllable abstractive summarization. In Proceed-\nings of the 2nd Workshop on Neural Machine Trans-\nlation and Generation , pages 45–54, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nMarkus Freitag, Isaac Caswell, and Scott Roy. 2019.\nAPE at scale and its implications on MT evaluation\nbiases. In Proceedings of the Fourth Conference on\nMachine Translation (Volume 1: Research Papers) ,\npages 34–44, Florence, Italy. Association for Com-\nputational Linguistics.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 6112–\n6121, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTanya Goyal and Greg Durrett. 2020. Neural syntactic\npreordering for controlled paraphrase generation. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 238–\n252, Online. Association for Computational Linguis-\ntics.\nBrigitte Grote and Manfred Stede. 1998. Discourse\nmarker choice in sentence planning. In Natural Lan-\nguage Generation.\nChris Hokamp and Qun Liu. 2017. Lexically con-\nstrained decoding for sequence generation using grid\nbeam search. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1535–1546,\nVancouver, Canada. Association for Computational\nLinguistics.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations.\nJ. Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick\nXia, Tongfei Chen, Matt Post, and Benjamin\nVan Durme. 2019. Improved lexically constrained\ndecoding for translation and monolingual rewriting.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 839–850,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P Xing. 2017. Toward\ncontrolled generation of text. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 1587–1596. JMLR. org.\nXinyu Hua, Zhe Hu, and Lu Wang. 2019. Argument\ngeneration with retrieval, planning, and realization.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n2661–2672, Florence, Italy. Association for Compu-\ntational Linguistics.\n791\nXinyu Hua and Lu Wang. 2019. Sentence-level content\nplanning and style speciﬁcation for neural text gen-\neration. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n591–602, Hong Kong, China. Association for Com-\nputational Linguistics.\nYangfeng Ji and Jacob Eisenstein. 2014. Representa-\ntion learning for text-level discourse parsing. InPro-\nceedings of the 52nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 13–24, Baltimore, Maryland. Associ-\nation for Computational Linguistics.\nJungo Kasai, James Cross, Marjan Ghazvininejad, and\nJiatao Gu. 2020. Non-autoregressive machine trans-\nlation with disentangled context transformer. In\nProc. of ICML.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization.\nAlon Lavie and Abhaya Agarwal. 2007. METEOR: An\nautomatic metric for MT evaluation with high levels\nof correlation with human judgments. In Proceed-\nings of the Second Workshop on Statistical Machine\nTranslation, pages 228–231, Prague, Czech Repub-\nlic. Association for Computational Linguistics.\nCarolin Lawrence, Bhushan Kotnis, and Mathias\nNiepert. 2019. Attending to future tokens for bidi-\nrectional sequence generation. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1–10, Hong Kong, China.\nAssociation for Computational Linguistics.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative reﬁnement. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1173–\n1182, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nChin-Yew Lin and Eduard Hovy. 2000. The automated\nacquisition of topic signatures for text summariza-\ntion. In COLING 2000 Volume 1: The 18th Interna-\ntional Conference on Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nFuli Luo, Damai Dai, Pengcheng Yang, Tianyu Liu,\nBaobao Chang, Zhifang Sui, and Xu Sun. 2019.\nLearning to control the ﬁne-grained sentiment for\nstory ending generation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 6020–6026, Florence, Italy.\nAssociation for Computational Linguistics.\nElman Mansimov, Alex Wang, Sean Welleck, and\nKyunghyun Cho. 2019. A generalized framework of\nsequence generation with application to undirected\nsequence models. arXiv preprint arXiv:1905.12790.\nGeorge A. Miller. 1994. WordNet: A lexical database\nfor English. In Human Language Technology: Pro-\nceedings of a Workshop held at Plainsboro, New Jer-\nsey, March 8-11, 1994.\nAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.\nStep-by-step: Separating planning from realization\nin neural data-to-text generation. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers), pages 2267–2277, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nRoman Novak, Michael Auli, and David Grangier.\n2016. Iterative reﬁnement for machine translation.\narXiv preprint arXiv:1610.06602.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\n792\nhigh-performance deep learning library. In Ad-\nvances in Neural Information Processing Systems ,\npages 8024–8035.\nMatt Post and David Vilar. 2018. Fast lexically con-\nstrained decoding with dynamic beam allocation for\nneural machine translation. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 1314–1324, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nRashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-\nsakaki, Livio Robaldo, Aravind K Joshi, and Bon-\nnie L Webber. 2008. The penn discourse treebank\n2.0. In LREC. Citeseer.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nLena Reed, Shereen Oraby, and Marilyn Walker. 2018.\nCan neural generators for dialogue learn sentence\nplanning and discourse structuring? In Proceed-\nings of the 11th International Conference on Natu-\nral Language Generation , pages 284–295, Tilburg\nUniversity, The Netherlands. Association for Com-\nputational Linguistics.\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,\nTrevor Darrell, and Kate Saenko. 2018. Object hal-\nlucination in image captioning. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4035–4045, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nEvan Sandhaus. 2008. The new york times annotated\ncorpus. Linguistic Data Consortium, Philadelphia ,\n6(12):e26752.\nAbigail See, Stephen Roller, Douwe Kiela, and Ja-\nson Weston. 2019. What makes a good conver-\nsation? how controllable attributes affect human\njudgments. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 1702–1723, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nAmanda Stent, Rashmi Prasad, and Marilyn Walker.\n2004. Trainable sentence planning for complex in-\nformation presentations in spoken dialog systems.\nIn Proceedings of the 42nd Annual Meeting of\nthe Association for Computational Linguistics (ACL-\n04), pages 79–86, Barcelona, Spain.\nDi Wang, Nebojsa Jojic, Chris Brockett, and Eric Ny-\nberg. 2017. Steering output style and topic in neu-\nral response generation. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2140–2150, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nTsung-Hsien Wen, Milica Ga ˇsi´c, Nikola Mrk ˇsi´c, Pei-\nHao Su, David Vandyke, and Steve Young. 2015.\nSemantically conditioned LSTM-based natural lan-\nguage generation for spoken dialogue systems. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing , pages\n1711–1721, Lisbon, Portugal. Association for Com-\nputational Linguistics.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and reﬁne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92, Brussels, Belgium. Association for\nComputational Linguistics.\nSam Wiseman, Stuart Shieber, and Alexander Rush.\n2017. Challenges in data-to-document generation.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2253–2263, Copenhagen, Denmark. Association for\nComputational Linguistics.\nSam Wiseman, Stuart Shieber, and Alexander Rush.\n2018. Learning neural templates for text genera-\ntion. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3174–3187, Brussels, Belgium. Association\nfor Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,\nNenghai Yu, and Tie-Yan Liu. 2017. Deliberation\nnetworks: Sequence generation beyond one-pass de-\ncoding. In Advances in Neural Information Process-\ning Systems, pages 1784–1794.\nJiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu.\n2020. Discourse-aware neural extractive text sum-\nmarization. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\n793\npages 5021–5031, Online. Association for Computa-\ntional Linguistics.\nLili Yao, Nanyun Peng, Ralph Weischedel, Kevin\nKnight, Dongyan Zhao, and Rui Yan. 2019. Plan-\nand-write: Towards better automatic storytelling. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 33, pages 7378–7385.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi.\n2017. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 654–664, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nA Reproducibility\nComputing Infrastructure. Our model is built\nupon the PyTorch transformers-2.6.0\nlibrary by Wolf et al. (2019), with\nPytorch-Lightning-0.7.3 (Falcon,\n2019) for training routines. To improve training\nefﬁciency, we adopt mixed-precision ﬂoating\npoint (FP16) computation using the O2 option of\nNVIDIA apex5. For both training and decoding,\nwe utilize the Titan RTX GPU card with 24 GB\nmemory.\nModel Sizes. Our generation model has the same\narchitecture as BART (Lewis et al., 2020) with\n406M parameters. The content planner is built on\ntop of BERTbase, which has 110M parameters.\nRunning Time. Training the generation model\ntakes 2.5 hours for argument, 5 hours for opinion,\nand 24 hours for news. The content planning model\nconverges in 2.5-4 hours for three domains.\nDecoding Settings. At inference time, we set\nk= 50, temperature=1.0, and p= 0.9 for nucleus\nsampling. The relatively large k value is deter-\nmined based on a pilot study, where we ﬁnd that\nthe reﬁnement lacks diversity if k is set to small\nvalues. Moreover, since the Transformer states\nneed to be cached during autoregressive decoding\nand we perform three complete nucleus sampling\nruns in each reﬁnement iteration, the GPU memory\nconsumption is substantially increased. We there-\nfore limit the maximum generation steps to 140 for\nargument, 243 and 335 for opinion and news.\nAuto-Correction for Content Plan. When the\ncontent plan is predicted by the planner, the follow-\ning post-processing steps are employed prior to the\n5https://github.com/NVIDIA/apex\nARGGEN OPINION NEWS\nsys ref sys ref sys ref\n# tokens 133.3 130.2 228.5 246.3 424.5 435.5\n# sentences 8.6 5.6 11.1 8.2 19.2 13.5\n# KP per sent. 2.96 3.77 2.22 2.49 3.40 3.24\nKP distance 2.61 2.95 5.70 6.02 3.76 5.08\nTable 6: Statistics on generated templates by our con-\ntent planner. Tokens are measured in units of Word-\nPiece (Sennrich et al., 2016). KP distance denotes the\naverage number of tokens between two keyphrases that\nare in the same sentence. Both system output (sys) and\nhuman reference (ref) are reported.\nmasked template construction: (1) For a predicted\nkeyphrase, its token positions are adjusted to a con-\nsecutive segment, so that the phrase is kept intact in\nthe template. (2) If the predicted positions are not\nmonotonic to the assignment ordering, they will\nbe rearranged. For instance, if the assignment con-\ntains KP1 ⊿KP2, but position of KP2 is not strictly\nlarger than that of KP1, we instead place KP2 im-\nmediately after KP1 in the template. (3) Finally,\nsince the planner and generator have different sub-\nword vocabularies, it is necessary to detokenize the\npredicted keyphrase assignment, and re-tokenize\nwith the BPE vocabulary of the generator.\nB Template Construction Statistics\nWe characterize the content planning results in Ta-\nble 6. Speciﬁcally, we show the statistics on the\nautomatically created templates based on the plan-\nner’s output. As we can see, our system predicted\ntemplates approach human reference in terms of\nlength, per sentence keyphrase count, and the av-\nerage keyphrase spacing. Sentence segmentation\noccurs more often in our templates than the refer-\nence text, likely due to the frequent generation of\n[SEN] tokens.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.802351176738739
    },
    {
      "name": "Transformer",
      "score": 0.755119800567627
    },
    {
      "name": "Text generation",
      "score": 0.642494261264801
    },
    {
      "name": "Sentence",
      "score": 0.5801731944084167
    },
    {
      "name": "Iterative refinement",
      "score": 0.5773855447769165
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5085464715957642
    },
    {
      "name": "Natural language processing",
      "score": 0.5071020126342773
    },
    {
      "name": "Sequence (biology)",
      "score": 0.502579927444458
    },
    {
      "name": "Algorithm",
      "score": 0.376598060131073
    },
    {
      "name": "Voltage",
      "score": 0.1104271411895752
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12912129",
      "name": "Northeastern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    }
  ],
  "cited_by": 43
}