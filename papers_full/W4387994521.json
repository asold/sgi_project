{
    "title": "Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages",
    "url": "https://openalex.org/W4387994521",
    "year": 2023,
    "authors": [
        {
            "id": null,
            "name": "Khraisha, Qusai",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Put, Sophie",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Kappenberg, Johanna",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Warraitch, Azza",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Hadfield, Kristin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2889414415",
        "https://openalex.org/W2070285512",
        "https://openalex.org/W4367368990",
        "https://openalex.org/W2864040241",
        "https://openalex.org/W3044451687",
        "https://openalex.org/W4282973240",
        "https://openalex.org/W4313014664",
        "https://openalex.org/W2053301680",
        "https://openalex.org/W4284887585",
        "https://openalex.org/W2999536597",
        "https://openalex.org/W2214143284",
        "https://openalex.org/W2994661761",
        "https://openalex.org/W2805303998",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4323347737",
        "https://openalex.org/W4385297391",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W3164471952",
        "https://openalex.org/W4367701145",
        "https://openalex.org/W4366269319",
        "https://openalex.org/W3033479980",
        "https://openalex.org/W4383722515",
        "https://openalex.org/W2053154970",
        "https://openalex.org/W4365451283",
        "https://openalex.org/W4385778721",
        "https://openalex.org/W2593758073",
        "https://openalex.org/W4384641573",
        "https://openalex.org/W4213127247",
        "https://openalex.org/W4383554209",
        "https://openalex.org/W2913850779",
        "https://openalex.org/W2969341057",
        "https://openalex.org/W4384389814",
        "https://openalex.org/W4294214983",
        "https://openalex.org/W2964179635",
        "https://openalex.org/W3027335960",
        "https://openalex.org/W3003735349",
        "https://openalex.org/W4315641584"
    ],
    "abstract": "Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts - screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prompts, GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key studies using highly reliable prompts improved its performance even more. Our findings indicate that, currently, substantial caution should be used if LLMs are being used to conduct systematic reviews, but suggest that, for certain systematic review tasks delivered under reliable prompts, LLMs can rival human performance.",
    "full_text": "CAN LARGE LANGUAGE MODELS REPLACE HUMANS IN THE\nSYSTEMATIC REVIEW PROCESS ? E VALUATING GPT-4’ S\nEFFICACY IN SCREENING AND EXTRACTING DATA FROM\nPEER -REVIEWED AND GREY LITERATURE IN MULTIPLE\nLANGUAGES\nA PREPRINT\nQusai Khraisha\nTrinity Centre for Global Health\nSchool of Psychology\nTrinity College Dublin\nIreland\nkhraishq@tcd.ie\nSophie Put\nDepartment of Education\nUniversity of York\nUK\nJohanna Kappenberg\nSchool of Psychology\nTrinity College Dublin\nIreland\nAzza Warraitch\nTrinity Centre for Global Health\nSchool of Psychology\nTrinity College Dublin\nIreland\nKristin Hadfield\nTrinity Centre for Global Health\nSchool of Psychology\nTrinity College Dublin\nIreland\nOctober 30, 2023\nABSTRACT\nSystematic reviews are vital for guiding practice, research, and policy, yet they are often slow and\nlabour-intensive. Large language models (LLMs) could offer a way to speed up and automate\nsystematic reviews, but their performance in such tasks has not been comprehensively evaluated\nagainst humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study\nevaluates GPT-4’s capability in title/abstract screening, full-text review, and data extraction across\nvarious literature types and languages using a ‘human-out-of-the-loop’ approach. Although GPT-4\nhad accuracy on par with human performance in most tasks, results were skewed by chance agreement\nand dataset imbalance. After adjusting for these, there was a moderate level of performance for data\nextraction, and – barring studies that used highly reliable prompts – screening performance levelled\nat none to moderate for different stages and languages. When screening full-text literature using\nhighly reliable prompts, GPT-4’s performance was ‘almost perfect.’ Penalising GPT-4 for missing\nkey studies using highly reliable prompts improved its performance even more. Our findings indicate\nthat, currently, substantial caution should be used if LLMs are being used to conduct systematic\nreviews, but suggest that, for certain systematic review tasks delivered under reliable prompts, LLMs\ncan rival human performance.\nKeywords Systematic reviews, Large language models, LLMs, GPT, Artificial intelligence, AI, Natural Language\nProcessing, NLP, Machine learning\n1 Introduction\nSystematic reviews play a crucial role in advancing practice, research, and policy (Aromataris et al., 2015). However,\nthe current approach to systematic reviews is laborious and can be slow to the point that the resulting synthesis of\narXiv:2310.17526v2  [cs.CL]  27 Oct 2023\nCan large language models replace humans in the systematic review process? A PREPRINT\nknowledge may no longer be up to date when it is completed (Borah et al., 2017; Michelson and Reuter, 2019). The\nexplosion of scientific literature, coupled with the complexity and specificity of many research questions, further adds to\nthese challenges (Fiorini et al., 2018). Artificial intelligence (AI) has emerged as a potential solution to these challenges,\nwith recent studies and evaluations suggesting its capability to enhance the quality and efficiency of systematic reviews\n(Blaizot et al., 2022; Dijk et al., 2023; Kebede et al., 2023; Mahuli et al., 2023; Moreno-Garcia et al., 2023; Nugroho et\nal., 2023; Santos et al., 2023).\nSome examples of AI tools that have been used in systematic reviews include Rayyan and Abstracker, which help\nwith screening titles and abstracts (Giummarra et al., 2020; Rogers et al., 2020), trialStreamer, which helps with\ndata extraction from full-text articles (Marshall et al., 2020), and RobotReviewer, which helps with assessing study\nquality and bias (Goldkuhle et al., 2018). The major shortcoming of these tools is that their performance significantly\ndeteriorates without looping in a human in the decision-making process, as shown by Blaizot and colleagues’ (2022)\nmeta-analysis. One possible reason for these limitations is that they split text into fixed segments, which has been argued\nto hinder their ability to understand context, especially in longer texts (Guo et al., 2023). Newer large language models\n(LLMs) based on the transformer technology (Vaswani et al., 2017), such as Generative Pre-Trained Transformer (GPT)\nand Bidirectional Encoder Representations from Transformers (BERT), may overcome this problem since they capture\nmore contextual information. This was demonstrated in a recent study, which highlighted GPT’s superior performance\non systematic review tasks compared to older AI methods (Syriani et al., 2023).\nThe question of whether AI tools can match or surpass human performance in conducting systematic reviews carries\nprofound implications for the future of scientific research. It holds the potential to radically transform knowledge\nsynthesis, turning systematic reviews from static literature summaries into dynamic, continually updated resources –\npotentially altering the very way we approach science. Given these significant implications, it is crucial to acknowledge\nthe current uncertain state of AI in this domain, especially regarding the most substantial LLM model, GPT-4. This\nmodel has been reported to significantly surpass all other LLMs, including previous versions of GPT, in various\nnatural language processing tasks across both English and other languages (OpenAI, 2023). Yet, as of now, nothing is\ndocumented about GPT-4’s performance in conducting systematic reviews.\nResearch on using other LLMs in systematic reviews (mostly earlier versions of GPT) is not as comprehensive or\nsystematic as it could be, with much of the work containing contaminated datasets and inadequate metrics. No study,\nfor instance, has tested grey literature and non-English literature, which can constitute a large proportion of the evidence\nbase for some topics (Lawrence et al., 2014). Most studies focused on narrow aspects of systematic reviews, such\nas Boolean queries (S. Wang et al., 2023) or only evaluating performance on titles and abstracts screening (Alshami\net al., 2023; Guo et al., 2023; Syriani et al., 2023). Some have methodological shortcomings, such as Mahuli et al.,\n(2023), who did not provide an objective evaluation of GPT’s performance, or Alshami et al., (2023), who did not\ntest GPT’s autonomous performance, instead relying on a ‘human-in-the-loop’ approach. A few may have included\ncontaminated data, such as Guo et al., (2023) and Syriani et al., (2023), who used datasets for systematic reviews that\npublished their results before or in 2021, when GPT was trained, which may bias the results in favour of GPT. Other\nstudies, such as Guo et al., (2023), did not consider imbalance nor incorporate chance agreement in their interpretation\nof the results, thereby potentially inflating GPT’s accuracy. Syriani et al., (2023) addressed this issue but focused\non investigating GPT’s performance against other AI tools, not human reviewers. Our pre-registered study is the\nfirst to evaluate GPT-4’s autonomous performance across several systematic review processes, including title/abstract\nscreening, full-text screening, and data extraction. It is also the first to test an LLM model in reviewing grey literature\nand literature in other languages.\n2 Methods\nThis study assessed GPT-4’s performance in screening and extracting data from documents for an ongoing systematic\nreview on parenting in protracted refugee situations. We used the ChatGPT interface to access the GPT-4 model between\nMay and September 2023. We tested documents that were reviewed using four inclusion/exclusion criteria: containing\nempirical data, parenting behaviour, refugee status, and protracted refugee situation. Links to our GPT-4 prompts and\noutputs, as well as the R code used for analysis, are on the Open Science Foundation (OSF) page (link). We registered\nthis study protocol on the OSF, while the details of the review can be found on both OSF (link) and PROSPERO\n(Anonymised). We screened 300 titles/abstracts and 150 full-texts, as well as extracted data from 30 documents (see\nFigures 1 and 2). Sample size was largely based on studies reviewed by at least two humans for screening, which was\nthe case for English language documents at all stages and those written in other languages in the title/abstract stage.\nHowever, due to time and resource constraints, we only used one human reviewer for non-English studies at the full-text\nlevel, and for data extraction from all studies. While we ensured a mix of decisions in terms of a random selection of\nrelevant and irrelevant studies written in English to gain deeper insight into inclusion performance, this was generally\nnot possible for non-English studies due to the large number of excluded studies.\n2\nCan large language models replace humans in the systematic review process? A PREPRINT\n2.1 Prompt engineering approach\nWe experimented with GPT-4 prompt formats for title/abstract screening. Initially, we tested citations alone, but GPT-4\nappeared to focus mainly on titles, and its accuracy decreased with an increased volume of citations. To address this,\nwe presented complete abstracts in our prompts. Our next challenge was finding that GPT-4 struggled with complex\nqueries and large data volumes, prompting us to present inclusion/exclusion criteria separately and reduce the number\nof titles/abstracts in each message. This approach seemed to reduce hallucinations, a frequently observed phenomenon\nwhere an LLM confidently produces an inaccurate output (Beutel et al., 2023), but reduced consistency upon retesting.\nPresenting criteria and text within the same message and lowering the number of studies based on the criterion’s\ncomplexity improved consistency without increasing hallucinations.\nWe assessed test-retest reliability using 10 studies on each of our four criteria. Each criterion corresponds to a prompt.\nWe tested the four prompts five times and generated five decisions per criterion per study. We then recorded each\ntime GPT-4 output was inconsistent from the original answer. These scores were used to assess the impact of prompt\nreliability on accuracy scores.\n2.2 Screening and extraction\nEach abstract batch (a maximum of three abstracts for the protracted concept – given that it contained a longer prompt –\nand five abstracts for other concepts) underwent four chat tests based on inclusion/exclusion criteria. We tested multiple\nabstracts within the same query to increase efficiency in screening. We proceeded to the next criterion in another chat if\nGPT-4 responded ‘yes/maybe’. A ‘no’ meant exclusion from further review. If an abstract was excluded for not meeting\na criterion, we removed it and introduced a new one for subsequent tests, meaning that the number of abstracts remained\nthe same every time a criterion was tested. We set a five-message limit for titles/abstracts and three for full texts to\navoid GPT-4’s inaccuracies arising from too much information. Lengthy abstracts in grey literature were divided for\nseparate evaluations until GPT-4 finalised a decision. During grey literature screening, we modified the query to fit\nsource formats, switching “titles/abstracts” to “websites/reports” and “texts.”\nFor full text, we adjusted our queries, using “Include” instead of “Yes/maybe” and “Exclude” for “No”. Due to character\nlimits, we segmented texts. To reduce contamination of responses with each other, every time a text snippet was sent to\nGPT-4, it was accompanied by the specific inclusion/exclusion criterion it was being tested against together within the\nsame prompt. If GPT-4 said “Include” for a segment, we viewed that criterion as met and began a new chat with the\nnext criterion from the first segment. An “Exclude” led us to present subsequent segments. If GPT-4 never indicated\n3\nCan large language models replace humans in the systematic review process? A PREPRINT\n“Include” by the last segment, the study was marked as excluded, and no further criteria were tested. If humans had\nalready excluded a study, GPT-4 began screening with the humans’ exclusion reason. If GPT-4 shared the decision\nmade by humans, no further tests were needed. Otherwise, all remaining criteria were checked until we received an\nexclusion decision. As a result, 72% of the English peer-reviewed literature needed testing on the ‘parenting’ and\n‘protracted refugee situations’ prompt, compared to 44% and 18% for grey and non-English literature, respectively.\nExtracted information mainly revolved around characteristics of parents (e.g., number of parents, gender, and education\ndistribution), characteristics of children (e.g., number of children, gender, and education distribution), study design (by\nduration, data collection method and group allocation), sampling technique, and instruments used to measure parenting\n(e.g., instrument type and name, target respondent and the main focus of the instrument). Only text from the methods\nand results sections of the chosen papers were inputted into GPT-4. Similar to the above, we segmented the text into\npieces. Only the first response to a prompt was deemed final, except if the response was incomplete, then the subsequent\nresponses were also recorded as part of the answer.\n2.3 Analysis metrics\nWe applied four metrics: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). TP\nand TN are the cases where GPT-4 agreed with human reviewers, meaning it made correct decisions. FP and FN are\nthe cases where GPT-4 disagreed with human reviewers, meaning it made wrong decisions. Ratios derived from these\nmetrics provide insight into the imbalance of the dataset by dividing true cases against false cases. A meta-analysis\nindicated that systematic reviews’ datasets have between 40% to less than 1% of relevant studies, with an average of\n3%. This suggests that imbalance scores of 3% are typical, while a score of 40% or less than 1% is ‘somewhat typical,’\nand a score of above 40% is ‘atypical’ (Sampson et al., 2011).\nImbalance = TP + FP\nTN + FN (1)\nFor GPT-4 performance, we calculated sensitivity, specificity and accuracy, which are commonly used metrics (e.g.,\nFrömke et al., 2022; Patel et al., 2021). Sensitivity (also known as recall) shows how well GPT-4 identified positive\ncases by taking (TP) and dividing them by the sum of TP and FN. Specificity indicates how well GPT-4 identified\nnegative cases by taking TN and dividing them by the sum of TN and FP. Accuracy, which evaluates the overall\n4\nCan large language models replace humans in the systematic review process? A PREPRINT\ncorrectness of GPT-4, was calculated by adding TP and TN and dividing by the total number of cases. There is no\nconsensus on how to interpret these scores, as they largely depend on the context (e.g., Shreffler and Huecker, 2023).\nPrevious studies have reported that human error rates in systematic review screening range from 5% to 20%, implying\nthat a score of 100% is ‘superior’ to humans, while an accuracy score of 80% to 95% for GPT-4 could be regarded as\n‘on par’ (Wang et al., 2020). In the worst possible documented prediction, human error rates reached up to 40% (Wang\net al., 2020). This suggests that accuracy scores between 60% and 80% could be regarded as ‘near-par,’ and scores\nbelow 60% could be regarded as ‘subpar’ to humans.\nSensitivity = TP\nTP + FN (2)\nSpecificity = TN\nTN + FP (3)\nAccuracy = TP + TN\nTP + TN + FP + FN (4)\nCohen’s Kappa (κ) was calculated to compare the actual and expected agreement between GPT-4 and human reviewers\n(Cohen, 1960). This is important for systematic reviews, where inclusion is rare, and exclusion is common, which can\nmake humans and GPT-4 seem more agreeable than they are. The actual agreement (Po) is the proportion of cases\nhumans and GPT-4 gave the same rating, positive or negative. The expected agreement (Pe) is the probability that\nhumans and GPT-4 gave the same rating by chance, based on their rating frequencies. We subtracted the expected\nagreement from the actual agreement and divided it by the maximum possible agreement (1 minus the expected\nagreement). This gives a score between -1 and 1. We followed what McHugh (2012) suggested for the classification of\nagreement scores, which are more stringent on interpreting values than those suggested by Cohen (1960): values .0 –\n.20 as no agreement, .21 – .40 as minimal, .41 – .59 as weak, .60 –.79 as moderate, and 0.80 – .90 as almost perfect\nagreement.\nCohen’s kappa has been shown to produce a false agreement rate in imbalanced datasets, so we used PABAK (prevalence-\nadjusted bias-adjusted kappa) to account kappa for the effects of prevalence and bias in the data set (Byrt et al., 1993).\nPABAK corrects for this by accounting for the distribution of the categories in the denominator and by adjusting the\ncounts of agreements and disagreements in the formula. We also used weighted kappa to better capture the severity of\nfalse rejections by GPT-4, which are the most serious errors it can make, because this would exclude a study which\nmeets the inclusion criteria. Weighted kappa (ωκ) assigns higher weights to greater disagreements, with 0 for complete\nagreement. There is no standard test that can combine weights with PABAK, so we could not account for the effects of\ndata imbalance in the weighted Cohen’s kappa scores. Sampson et al. (2011) found that the median search precision\nfor systematic reviews is around 3%; that is, there are about 30 times more excluded studies than included ones. This\nsuggests a weight of 30 for false rejections in the calculation of weighted kappa.\nκ = 1 − Pe\nPo − Pe (5)\nωκ = 1− 1 − Poω\n1 − Peω (6)\nPABAK = 1 − 0.5\n2Po − 1 (7)\n3 Results\nIn this study, we evaluated GPT-4’s performance in screening and extracting peer-reviewed, grey (non-peer reviewed),\nand non-English literature. We first report on the reliability of answers when using the same prompt. This means that\nGPT-4 gave the same answer every time, without any variation. For instance, if GPT-4 gave the same answer 10 times\nin a row for the same text and prompt, it scored 100%, but if it gave the same initial answer only 7 times out of 10, it\nscored 70%. GPT-4 performed best when assessing empirical data and refugees (100% reliability) and struggled with\nthe concepts of parenting behaviour (50% reliability) and protracted refugee situations (70% reliability). With these\nfindings in mind, we created a sub-literature sample that included only prompts relating to refugee status and empirical\ndata called the ‘high-reliability prompt group,’ given that these prompts had the highest reliability scores. Ultimately,\nthis sub-sample included 23 studies: a third were English peer-reviewed studies, a third were English grey literature\nstudies, and another third were non-English studies.\n5\nCan large language models replace humans in the systematic review process? A PREPRINT\nWe subsequently looked at the balance of data, which is the ratio of relevant to irrelevant studies, for each literature\ntype, language, and stage (for the extraction stage, this means the presence or absence of data). Peer-reviewed studies in\nEnglish were fully balanced, as intended by our design, except in the extraction stage (.03; that is, 1 included for every\n30 excluded). Unlike non-English studies (.05), the grey literature was fully balanced at the title/abstract stage, but\nthen both grey literature and non-English studies were skewed towards irrelevance in the full-text screening (.11 and\n.09, respectively) and extraction stages (.24 and .20, respectively). It is important to note that while balance aids in\nunderstanding all aspects of performance, it does not reflect the inherent imbalances in real-world datasets of systematic\nreview. Based on Sampson et al., (2011) finding that there are typically about 30 times more excluded studies than\nincluded studies, our most imbalanced datasets were also the most consistent with other systematic reviews.\nAcross all stages and categories, the specificity was on par with human performance (>.80, except for English peer-\nreviewed full-text screening), indicating a robust ability of GPT-4 to correctly identify irrelevant studies (Table 1). This\nwas especially true in literature containing non-English studies (>.90). Sensitivity, indicating how effectively GPT-4\nidentified relevant studies, was highest in the extraction stage for both peer-reviewed (English: .75) and grey literature\n(.65), although note that for non-English studies the sensitivity was only .36 for data extraction. For non-English studies,\nperfect sensitivity was achieved during the full-text stage. Accuracy was somewhat higher in the data extraction stage\nthan in the title/abstract screening phase, ranging between near-par and on par with human performance, except for the\nEnglish peer-reviewed literature.\nThe balanced dataset of English peer-reviewed literature had lower accuracy (title/abstract: .67, full-text: .69) than the\nmore unbalanced non-English literature dataset (title/abstract: .88, full-text: .96). In extraction, which was the only\ntime the dataset of English peer-reviewed literature was imbalanced, accuracy was the highest (.84). This suggests\nthat GPT-4’s high accuracy might be due to chance. Supporting this notion, the associated adjusted kappa scores were\nlow, ranging from ‘none’ to ‘moderate’ as categorised by McHugh (2012). An outlier in these scores was the ‘almost\nperfect’ agreement seen in the highly-reliable prompt group, which exclusively featured responses from highly reliable\nprompts (.91). When we weighted kappa to emphasise false rejections, in a way penalising GPT-4 for missing key\nstudies, scores for the highly reliable prompt group improved even more (.97).\n4 Discussion\nWe found mixed results on the efficacy of GPT-4 as compared to human reviewers across various systematic review\ntasks, languages, and literature types. GPT-4’s accuracy was influenced by chance agreement and dataset imbalance, and\nwhen these factors were considered, GPT-4 often substantially underperformed humans. Yet, under specific conditions –\nnamely, when given entirely highly reliable prompts in full-text screening – GPT-4 demonstrated an ‘almost perfect’\nperformance on par with humans. While our findings indicate the need for caution in assuming uniform proficiency\nacross tasks, they also suggest that, under certain conditions, LLMs have the potential to revolutionise how we synthesise\nknowledge.\nOur results for sensitivity, and specificity are consistent with previous studies. For instance, Guo et al. (2023) and\nAlshami et al. (2023) found that specificity was the strongest metric for title/abstract screening, as we did. They achieved\na specificity score close to ours (90% and 93%, respectively, vs our 92%). Their sensitivity score was slightly higher\nthan ours (76% and 84%, respectively, vs our 67%; although note that Alshami et al., 2023 used a human-in-the-loop\nmethod). Our accuracy score for the peer-reviewed literature (67%) was lower than in previous work, possibly because\nour study was the only one that artificially balanced its dataset. Unlike the others (Alshami et al., 2023; Guo et al.,\n2023), we took this step because skewed datasets can make accuracy metrics unreliable, as later indicated by our low\nadjusted kappa. Such a limitation will not be easily detected by anecdotal tests (Mahuli et al., 2023; Qureshi et al.,\n2023) and may mislead general users who may assume that GPT is performing well when it is not. A possible reason\nfor this misconception is that GPT typically generates text that resembles human writing in terms of quality, style, and\ncontent (Jakesch et al., 2023), thus misleading users to think that GPT has human-level abilities based on its human-like\noutputs.\nThis study was the first to report on the novel application of an LLM in conducting full-text screening and extraction.\nAutomation techniques have been hailed as a way of increasing reproducibility (Ivimey-Cook et al., 2023). However,\nprevious endeavours employing AI tools for these tasks pinpointed several challenges. These included the need for\ncontinuous human intervention in full-text screening (Beller et al., 2018), the necessity for extensive pre-screening of\ntraining datasets (Halamoda-Kenzaoui et al., 2022), and the incapability to either review full-texts (Clark et al., 2020;\nNye et al., 2018) or to extract data deviating from a predetermined structure (Summerscales et al., 2011; Wallace et al.,\n2016). Our findings indicate that GPT-4 has limited potential in full-text screening and data extraction, with moderate\nperformance in non-English and grey literature, and a very poor ability with English peer-reviewed texts. Its training\non publicly available data, which might lean more towards grey literature and non-English sources, could explain this\n6\nCan large language models replace humans in the systematic review process? A PREPRINT\nTable 1: Performance Evaluation of GPT-4 versus Human Reviewers in Screening and Extraction\nBalance Sensitivity Specificity Accuracy Cohen Kappa ∗ Weighted Kappa Adjusted Kappa ∗∗\nTitle and abstract screening\nEnglish peer-reviewed 1 .42 .92 .67 .34 .23 .34\nEnglish grey 1 .48 .84 .66 .32 .24 .32\nOther languages .05 .50 .89 .88 .21 .40 .75\nFull text screening\nEnglish peer-reviewed .92 .38 .69 .54 .07 .05 .08\nEnglish grey .11 .60 .80 .78 .24 .44 .55\nOther languages .09 1 .95 .96 -.10 -.11 .64\nHigh-reliability prompt group\nHigh-reliability prompt group .05 .36 .94 .85 .65 .97 .91\nData extraction\nEnglish peer-reviewed .03 .75 .84 .82 .54 .63 .63\nEnglish grey .24 .65 .85 .81 .45 .53 .62\nOther languages .20 .36 .94 .85 .35 .29 .69\n* The inter-rater reliability between human reviewers for the full-text data was a Cohen Kappa coefficient of .77. However, the review is not yet completed, so the\nfinal value may vary slightly. The inter-rater reliability between human reviewers was calculated using data mostly from the peer-reviewed literature, but it also\nincludes a small portion of grey literature and non-English studies.\n** The human reviewers achieved an adjusted Cohen Kappa of .89 for the same literature sample Cohen Kappa was calculated for above.\nNote. We used PABAK (Prevalence-Adjusted Bias-Adjusted Kappa) to adjust Kappa for the effects of prevalence and bias in the data set (Byrt et al., 1993) and a\nweight of 30 for false rejections in the calculation of weighted Kappa based on previous studies which have shown that the median search precision for systematic\nreviews is around 3% (Sampson et al., 2011).\n7\nCan large language models replace humans in the systematic review process? A PREPRINT\ndifference. However, we should note that the English peer-reviewed literature data had a very unusual balance of studies,\nunlike the other databases, which are closer in their compositions to other systematic reviews, which suggests caution\nagainst assuming generalisability. Lastly, GPT-4 performance was strongly influenced by prompt reliability, which\ncould itself be affected by word count and prompt complexity. Longer prompts, like those for parenting behaviour and\nprotracted refugee situations (around 400 words and 1600 words, respectively; the other two prompts were less than 300\nwords), may have lost important context due to their size. We also observe that the ‘parenting behaviour’ prompt might\nhave been most challenging for GPT-4 because the prompt was more open than specific (to capture non-traditional\nways of parenting), unlike all other ones, which contained exact definitions.\nThere are three main strengths and weaknesses to this study. First, despite our comprehensive approach covering\nvarious literature types and our efforts to achieve balance, the challenges in balancing non-English texts and our very\nattempts at balancing could have both introduced biases. Second, we used various metrics to measure GPT-4’s accuracy,\nconsistency, and agreement with human reviewers, but our relatively small sample size of evaluated papers might limit\nthe generalisability of GPT-4’s findings for other systematic reviews. Third, we registered and documented our research\nprotocol and analysis plan to ensure its validity and replicability but might have benefited from also creating a detailed\nplan for prompt engineering in advance.\n5 Conclusion\nOver a hundred years ago, audiences worldwide were captivated by a horse named Hans, who, with a confident tap of\nhis hoof, appeared to solve mathematical problems. They believed Hans possessed an extraordinary cognitive ability,\nalmost human-like in nature. Yet, beneath this illusion lay a simpler truth: Hans was reading his handler. He picked up\non the faintest of cues – a twitch of a muscle, a barely noticeable nod, or even an unconscious sigh of anticipation. This\nphenomenon mirrors the workings of LLMs like GPT-4. While not deciphering human cues per se, they are heavily\ninfluenced by the prompts they receive, much like Hans needed clear guidance from his handler. But there is a key\ndistinction: whereas Hans required human guidance for his output, GPT-4 needs clear human input and generates\noutputs autonomously. Our study underscores this, showing that when given a reliable prompt, GPT-4’s screening\nperformance rises to be almost perfect. In harnessing this potential, LLMs might pave the way for a transformative era\nin systematic reviews.\nReferences\nAlshami, A., Elsayed, M., Ali, E., Eltoukhy, A. E. E., and Zayed, T. (2023). Harnessing the power of chatgpt\nfor automating systematic review process: Methodology, case study, limitations, and future directions. Systems,\n11(7):351.\nAromataris, E., Fernandez, R., Godfrey, C. M., Holly, C., Khalil, H., and Tungpunkom, P. (2015). Summarizing\nsystematic reviews: Methodological development, conduct and reporting of an umbrella review approach. JBI\nEvidence Implementation, 13(3):132.\nBeller, E., Clark, J., Tsafnat, G., Adams, C., Diehl, H., Lund, H., Ouzzani, M., Thayer, K., Thomas, J., Turner, T., Xia,\nJ., Robinson, K., Glasziou, P., Adams, C., Ahtirschi, O., Beller, E., Clark, J., Christensen, R., Diehl, H., and et al.\n(2018). Making progress with the automation of systematic reviews: Principles of the international collaboration for\nthe automation of systematic reviews (icasr). Systematic Reviews, 7(1):77.\nBeutel, G., Geerits, E., and Kielstein, J. T. (2023). Artificial hallucination: Gpt on lsd? Critical Care, 27(1):148.\nBlaizot, A., Veettil, S. K., Saidoung, P., Moreno-Garcia, C. F., Wiratunga, N., Aceves-Martins, M., Lai, N. M., and\nChaiyakunapruk, N. (2022). Using artificial intelligence methods for systematic review in health sciences: A\nsystematic review. Research Synthesis Methods, 13(3):353–362.\nBorah, R., Brown, A. W., Capers, P. L., and Kaiser, K. A. (2017). Analysis of the time and workers needed to conduct\nsystematic reviews of medical interventions using data from the prospero registry. BMJ Open, 7(2):e012545.\nByrt, T., Bishop, J., and Carlin, J. B. (1993). Bias, prevalence and kappa. Journal of Clinical Epidemiology, 46(5):423–\n429.\nClark, J., Glasziou, P., Del Mar, C., Bannach-Brown, A., Stehlik, P., and Scott, A. M. (2020). A full systematic review\nwas completed in 2 weeks using automation tools: A case study. Journal of Clinical Epidemiology, 121:81–90.\nCohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement,\n20(1):37–46.\nFiorini, N., Canese, K., Starchenko, G., Kireev, E., Kim, W., Miller, V ., Osipov, M., Kholodov, M., Ismagilov, R., Mohan,\nS., Ostell, J., and Lu, Z. (2018). Best match: New relevance search for pubmed. PLOS Biology, 16(8):e2005343.\n8\nCan large language models replace humans in the systematic review process? A PREPRINT\nFr\n\"omke, C., Kirstein, M., and Zapf, A. (2022). A semiparametric approach for meta-analysis of diagnostic accuracy\nstudies with multiple cut-offs. Research Synthesis Methods, 13(5):612–621.\nGiummarra, M. J., Lau, G., Grant, G., and Gabbe, B. J. (2020). A systematic review of the association between fault or\nblame-related attributions and procedures after transport injury and health and work-related outcomes. Accident;\nAnalysis and Prevention, 135:105333.\nGoldkuhle, M., Dimaki, M., Gartlehner, G., Monsef, I., Dahm, P., Glossmann, J.-P., Engert, A., von Tresckow, B., and\nSkoetz, N. (2018). Nivolumab for adults with hodgkin’s lymphoma (a rapid review using the software robotreviewer).\nThe Cochrane Database of Systematic Reviews, 7(7):CD012556.\nGuo, E., Gupta, M., Deng, J., Park, Y .-J., Paget, M., and Naugler, C. (2023). Automated paper screening for clinical\nreviews using large language models. arXiv:2305.00844.\nHalamoda-Kenzaoui, B., Rolland, E., Piovesan, J., Puertas Gallardo, A., and Bremer-Hoffmann, S. (2022). Toxic effects\nof nanomaterials for health applications: How automation can support a systematic review of the literature? Journal\nof Applied Toxicology, 42(1):41–51.\nIvimey-Cook, E. R., Noble, D. W. A., Nakagawa, S., Lajeunesse, M. J., and Pick, J. L. (2023). Advice for improving\nthe reproducibility of data extraction in meta-analysis. Research Synthesis Methods.\nJakesch, M., Hancock, J. T., and Naaman, M. (2023). Human heuristics for ai-generated language are flawed.\nProceedings of the National Academy of Sciences, 120(11):e2208839120.\nKebede, M. M., Le Cornet, C., and Fortner, R. T. (2023). In-depth evaluation of machine learning methods for\nsemi-automating article screening in a systematic review of mechanistic literature. Research Synthesis Methods,\n14(2):156–172.\nLawrence, A., Houghton, J., Thomas, J., and Weldon, P. (2014). Where is the evidence? realising the value of grey\nliterature for public policy and practice, a discussion paper.\nMahuli, S. A., Rai, A., Mahuli, A. V ., and Kumar, A. (2023). Application chatgpt in conducting systematic reviews and\nmeta-analyses. British Dental Journal, 235(2):Article 2.\nMarshall, I. J., Nye, B., Kuiper, J., Noel-Storr, A., Marshall, R., Maclean, R., Soboczenski, F., Nenkova, A., Thomas, J.,\nand Wallace, B. C. (2020). Trialstreamer: A living, automatically updated database of clinical trial reports. Journal\nof the American Medical Informatics Association, 27(12):1903–1912.\nMcHugh, M. L. (2012). Interrater reliability: The kappa statistic. Biochemia Medica, 22(3):276–282.\nMichelson, M. and Reuter, K. (2019). The significant cost of systematic reviews and meta-analyses: A call for\ngreater involvement of machine learning to assess the promise of clinical trials. Contemporary Clinical Trials\nCommunications, 16:100443.\nMoreno-Garcia, C. F., Jayne, C., Elyan, E., and Aceves-Martins, M. (2023). A novel application of machine learning\nand zero-shot classification methods for automated abstract screening in systematic reviews. Decision Analytics\nJournal, 6:100162.\nNugroho, P. A., Anna, N. E. V ., and Ismail, N. (2023). The shift in research trends related to artificial intelligence in\nlibrary repositories during the coronavirus pandemic. Library Hi Tech, ahead-of-print.\nNye, B., Li, J. J., Patel, R., Yang, Y ., Marshall, I., Nenkova, A., and Wallace, B. (2018). A corpus with multi-\nlevel annotations of patients, interventions and outcomes to support language processing for medical literature.\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 197–207.\nOpenAI (2023). Gpt-4 technical report. arXiv:2303.08774.\nPatel, A., Cooper, N., Freeman, S., and Sutton, A. (2021). Graphical enhancements to summary receiver operating\ncharacteristic plots to facilitate the analysis and reporting of meta-analysis of diagnostic test accuracy data. Research\nSynthesis Methods, 12(1):34–44.\nQureshi, R., Shaughnessy, D., Gill, K. A. R., Robinson, K. A., Li, T., and Agai, E. (2023). Are chatgpt and large\nlanguage models “the answer” to bringing us closer to systematic review automation? Systematic Reviews, 12(1):72.\nRogers, C. R., Matthews, P., Xu, L., Boucher, K., Riley, C., Huntington, M., Le Duc, N., Okuyemi, K. S., and Foster,\nM. J. (2020). Interventions for increasing colorectal cancer screening uptake among african-american men: A\nsystematic review and meta-analysis. PloS One, 15(9):e0238354.\nSantos, Á. O. d., da Silva, E. S., Couto, L. M., Reis, G. V . L., and Belo, V . S. (2023). The use of artificial intelligence for\nautomating or semi-automating biomedical literature analyses: A scoping review. Journal of Biomedical Informatics,\n142:104389.\n9\nCan large language models replace humans in the systematic review process? A PREPRINT\nShreffler, J. and Huecker, M. R. (2023). Diagnostic testing accuracy: Sensitivity, specificity, predictive values and\nlikelihood ratios. In StatPearls. StatPearls Publishing.\nSummerscales, R. L., Argamon, S., Bai, S., Hupert, J., and Schwartz, A. (2011). Automatic summarization of results\nfrom clinical trials. In 2011 IEEE International Conference on Bioinformatics and Biomedicine, pages 372–377.\nSyriani, E., David, I., and Kumar, G. (2023). Assessing the ability of chatgpt to screen articles for systematic reviews.\narXiv:2307.06464.\nvan Dijk, S. H. B., Brusse-Keizer, M. G. J., Bucsán, C. C., van der Palen, J., Doggen, C. J. M., and Lenferink, A. (2023).\nArtificial intelligence in systematic reviews: Promising when appropriately used. BMJ Open, 13(7):e072254.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017).\nAttention is all you need.\nWallace, B. C., Kuiper, J., and Sharma, A. (2016). Extracting pico sentences from clinical trial reports using supervised\ndistant supervision.\nWang, S., Scells, H., Koopman, B., and Zuccon, G. (2023). Can chatgpt write a good boolean query for systematic\nreview literature search? arXiv:2302.03495.\nWang, Z., Nayfeh, T., Tetzlaff, J., O’Blenis, P., and Murad, M. H. (2020). Error rates of human reviewers during\nabstract screening in systematic reviews. PLoS ONE, 15(1):e0227742.\n10"
}