{
  "title": "SEFormer: Structure Embedding Transformer for 3D Object Detection",
  "url": "https://openalex.org/W4382465451",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5059135535",
      "name": "Xiaoyu Feng",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5069395028",
      "name": "Heming Du",
      "affiliations": [
        "Australian National University"
      ]
    },
    {
      "id": "https://openalex.org/A5002207978",
      "name": "Hehe Fan",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A5013973037",
      "name": "Yueqi Duan",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5045721867",
      "name": "Yongpan Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4283819626",
    "https://openalex.org/W4226098442",
    "https://openalex.org/W3098325765",
    "https://openalex.org/W3097055324",
    "https://openalex.org/W4283807197",
    "https://openalex.org/W4221003815",
    "https://openalex.org/W3168718178",
    "https://openalex.org/W4200629701",
    "https://openalex.org/W6677174981",
    "https://openalex.org/W6793697131",
    "https://openalex.org/W4225357271",
    "https://openalex.org/W3033591146",
    "https://openalex.org/W4283810247",
    "https://openalex.org/W6809950564",
    "https://openalex.org/W2905076052",
    "https://openalex.org/W3190215433",
    "https://openalex.org/W3191320620",
    "https://openalex.org/W6792134461",
    "https://openalex.org/W3196633915",
    "https://openalex.org/W6801065500",
    "https://openalex.org/W6787910462",
    "https://openalex.org/W6761849836",
    "https://openalex.org/W6745673378",
    "https://openalex.org/W2950642167",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3196219612",
    "https://openalex.org/W2997188997",
    "https://openalex.org/W2903870443",
    "https://openalex.org/W2341497066",
    "https://openalex.org/W2749581528",
    "https://openalex.org/W3173857035",
    "https://openalex.org/W6780673685",
    "https://openalex.org/W3129242782",
    "https://openalex.org/W3194643899",
    "https://openalex.org/W3198869840",
    "https://openalex.org/W4200632008",
    "https://openalex.org/W6755285678",
    "https://openalex.org/W6773990164",
    "https://openalex.org/W6765771010",
    "https://openalex.org/W6779712747",
    "https://openalex.org/W6788305448",
    "https://openalex.org/W3153352467",
    "https://openalex.org/W4223930281",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W4313136902",
    "https://openalex.org/W3175563878",
    "https://openalex.org/W3035172746",
    "https://openalex.org/W3006754479",
    "https://openalex.org/W2963587345",
    "https://openalex.org/W4312437143",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W3167732492",
    "https://openalex.org/W2988715931",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3166470370",
    "https://openalex.org/W4288049158",
    "https://openalex.org/W4312307873",
    "https://openalex.org/W3107212734",
    "https://openalex.org/W2897529137",
    "https://openalex.org/W4214704706",
    "https://openalex.org/W2964062501",
    "https://openalex.org/W3166089996",
    "https://openalex.org/W3034314779",
    "https://openalex.org/W4287022186",
    "https://openalex.org/W3118341329",
    "https://openalex.org/W3036853234",
    "https://openalex.org/W2963516811",
    "https://openalex.org/W3206826736",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3167095230",
    "https://openalex.org/W2981949127",
    "https://openalex.org/W3201719054",
    "https://openalex.org/W2968296999",
    "https://openalex.org/W4312546175",
    "https://openalex.org/W3172752666",
    "https://openalex.org/W4308537955",
    "https://openalex.org/W4214777292",
    "https://openalex.org/W3205844608",
    "https://openalex.org/W4286974534",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2949708697",
    "https://openalex.org/W4310078553"
  ],
  "abstract": "Effectively preserving and encoding structure features from objects in irregular and sparse LiDAR points is a crucial challenge to 3D object detection on the point cloud. Recently, Transformer has demonstrated promising performance on many 2D and even 3D vision tasks. Compared with the fixed and rigid convolution kernels, the self-attention mechanism in Transformer can adaptively exclude the unrelated or noisy points and is thus suitable for preserving the local spatial structure in the irregular LiDAR point cloud. However, Transformer only performs a simple sum on the point features, based on the self-attention mechanism, and all the points share the same transformation for value. A such isotropic operation cannot capture the direction-distance-oriented local structure, which is essential for 3D object detection. In this work, we propose a Structure-Embedding transFormer (SEFormer), which can not only preserve the local structure as a traditional Transformer but also have the ability to encode the local structure. Compared to the self-attention mechanism in traditional Transformer, SEFormer learns different feature transformations for value points based on the relative directions and distances to the query point. Then we propose a SEFormer-based network for high-performance 3D object detection. Extensive experiments show that the proposed architecture can achieve SOTA results on the Waymo Open Dataset, one of the most significant 3D detection benchmarks for autonomous driving. Specifically, SEFormer achieves 79.02% mAP, which is 1.2% higher than existing works. https://github.com/tdzdog/SEFormer.",
  "full_text": "SEFormer: Structure Embedding Transformer for 3D Object Detection\nXiaoyu Feng1, Heming Du2, Hehe Fan3, Yueqi Duan1, Yongpan Liu1\n1Tsinghua University,\n2Australian National University,\n3National University of Singapore\nfeng-xy18@mails.tsinghua.edu.cn\nAbstract\nEffectively preserving and encoding structure features from\nobjects in irregular and sparse LiDAR points is a crucial chal-\nlenge to 3D object detection on the point cloud. Recently,\nTransformer has demonstrated promising performance on\nmany 2D and even 3D vision tasks. Compared with the fixed\nand rigid convolution kernels, the self-attention mechanism\nin Transformer can adaptively exclude the unrelated or noisy\npoints and is thus suitable for preserving the local spatial\nstructure in the irregular LiDAR point cloud. However, Trans-\nformer only performs a simple sum on the point features,\nbased on the self-attention mechanism, and all the points\nshare the same transformation for value. A such isotropic\noperation cannot capture the direction-distance-oriented lo-\ncal structure, which is essential for 3D object detection. In\nthis work, we propose a Structure-Embedding transFormer\n(SEFormer), which can not only preserve the local structure\nas a traditional Transformer but also have the ability to en-\ncode the local structure. Compared to the self-attention mech-\nanism in traditional Transformer, SEFormer learns different\nfeature transformations for value points based on the relative\ndirections and distances to the query point. Then we pro-\npose a SEFormer-based network for high-performance 3D\nobject detection. Extensive experiments show that the pro-\nposed architecture can achieve SOTA results on the Waymo\nOpen Dataset, one of the most significant 3D detection\nbenchmarks for autonomous driving. Specifically, SEFormer\nachieves 79.02% mAP, which is 1.2% higher than existing\nworks. https://github.com/tdzdog/SEFormer.\nIntroduction\nPoint cloud-based 3D object detection has attracted more\nand more attention with the development of autonomous\ndriving and robotics. Due to the lack of texture and color in-\nformation in the point cloud, 3D object detection highly de-\npends on the structure information of local areas. However,\nunlike the grid-arranged 2D images, the sparse and irregular\nnature of LiDAR point clouds makes the local structure of-\nten incomplete and noisy. Hence, how to effectively extract\nthe essential structure feature still needs to be solved.\nInspired by the success of 2D object detection (Ren et al.\n2015; Wu et al. 2021b,a; Shrivastava, Gupta, and Girshick\n2016; Sun et al. 2021b; Chi, Wei, and Hu 2020; Dong et al.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nTransformation for value\n(a) Vanilla Transformer (b) SEFormer\nUnrelated Points Related Points\nFigure 1: Motivation illustration: (a) The self-attention\nmechanism in Transformer can adaptively exclude unrelated\nor noisy points. However, the vanilla Transformer shares\nthe same feature transformation for all value points. Such\nisotropic operation ignores the local structure information\nbetween the key points and the query point. (b) While in\nSEFormer, we propose to learn different feature transforma-\ntions for value points based on their relative directions and\ndistances to the query point. Hence the local structure can be\nencoded in the Transformer output. For example, SEFormer\ncan differentiate a car’s front and tail points.\n2022), convolution rapidly becomes the mainstream oper-\nator in 3D object detection. Traditional convolution-based\n3D object detection can be divided into two main trends:\npoint (Qi et al. 2017a,b; Shi, Wang, and Li 2019; Qi et al.\n2018; Yang et al. 2020; Chen et al. 2022; He et al. 2022b)\nand voxel-based solutions (Yan, Mao, and Li 2018; Shi et al.\n2020; Deng et al. 2020; Zheng et al. 2021; Lang et al. 2019;\nShi et al. 2022; Li et al. 2021b,a; Xu, Zhong, and Neumann\n2022).\nHowever, convolution is designed with fixed and rigid\nkernel sizes and treats all neighboring points equally.\nTherefore, it inevitably contains unrelated or noisy points\nfrom other objects or backgrounds. Recently, Transformer\n(Vaswani et al. 2017) has shown its effectiveness in 3D vi-\nsion tasks such as classification, segmentation and object\ndetection (Zheng et al. 2022; Zhao et al. 2021; Wang et al.\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n632\n2021; Cao et al. 2022). Compared with convolution, the self-\nattention mechanism in Transformer can adaptively exclude\nnoisy or irrelevant points from other objects. We refer to the\nsuch ability of Transformer as structure preserving. How-\never, the vanilla Transformer shares the same feature trans-\nformation for points value. Such isotropic operation ignores\nthe local structure information in the spatial relationships,\ne.g., directions and distances, from the center point to its\nneighbors. As illustrated in Fig. 1 (a), the points share the\nsame transformation. If we swap the positions of points, the\nTransformer output remains the same. It challenges recog-\nnizing the object’s direction, which is vital to 3D object de-\ntection.\nIn this work, we are motivated by convolution, learning\na set of different kernels to embed point features from dif-\nferent distances and directions. Hence, we design a novel\nStructure-Embedding transFormer (SEFormer), which can\nencode the direction-distance-oriented local into its out-\nput. Compared with the vanilla Transformer, the proposed\nSEFormer learns different transformations for value points\nfrom different directions and distances. Hence the change in\nlocal spatial structure can be encoded in the output features.\nWe refer to SEFormer’s such ability asstructure encoding.\nAs shown in Fig. 1(b), the points are embedded with differ-\nent transformations (differently colored arrows). Once the\npoints are swapped, the correspondence between the points\nand transformations is changed. Hence, the position change\ncan be encoded in the SEFormer output and provide a clue to\nrecognize the object directions accurately. As a Transformer,\nSEFormer can also adaptively preserve the local structure.\nWith the additional structure encoding ability, SEFormer\ncan extract better local structure information.\nBased on the proposed SEFormer unit, we propose a\nmulti-scale SEFormer network to extract local structure de-\nscriptions for 3D object detection. Precisely, we extract\npoint- and object-level structure features. Based on the\nmulti-scale features extracted by stacked convolution layers,\nmultiple parallel SEFormer blocks, each containing stacked\nSEFormer units with different neighbor search radii, are uti-\nlized to extract richer structure features around each sam-\npled embedding point. Each embedding point provides a\npoint-level structure description of its surrounding local\narea. Then these point-level embedding features are sent to a\nSEFormer-based detection head. The network first predicts\nmultiple potential region proposals. Based on stacked SE-\nFormer layers, each proposal integrates its nearby point em-\nbedding and outputs an object-level feature embedding. The\nfinal bounding boxes are generated based on such object-\nlevel embedding. Our main contributions are threefold:\n• We propose SEFormer, a new Structure-Embedding\ntransFormer to capture the local point structure. SE-\nFormer can not only preserve the local structure as tra-\nditional Transformer but also have additional ability to\nencode the local direction-distance-oriented structure.\n• Based on the proposed SEFormer unit, we design a new\nmulti-scale 3D object detection framework. With multi-\nple SEFormer blocks, we extract point- and object-level\nstructure features for more accurate detection.\n• Extensive experiments prove the advantages of our\nSEFormer. On the Waymo Open dataset, we achieve\n79.02% mAP, which is 1.2% higher than existing works.\nRelated Work\n3D Object Detection on Point Cloud.3D object detection\nmethods on point clouds has made a giant leap recently. Ac-\ncording to different input representations, recent research\ncan be categorized into two families: point- and voxel-based.\n(1) Point-based Object Detection. Many works (Qi et al.\n2019; Xie et al. 2021; Chen et al. 2022) propose to di-\nrectly process raw point cloud data by adopting point-based\nbackbones, such as PointNet (Qi et al. 2017a) and Point-\nNet++ (Qi et al. 2017b). To process a mass of LiDAR points\nin outdoor environments, i.e., KITTI (Geiger et al. 2013)\nand Waymo (Sun et al. 2020), previous point-based ap-\nproaches (Qi et al. 2018; Shi, Wang, and Li 2019; Yang\net al. 2019) usually downsample the input point cloud and\ndisturbed by the information loss. (2) Voxel-based Object\nDetection. V oxel-based works (Yan, Mao, and Li 2018; Shi\net al. 2020; Deng et al. 2020; Xu, Zhong, and Neumann\n2022) transform raw point cloud into compact voxel-grid\nrepresentation and utilize efficient 3D sparse convolution\noperator so high-resolution voxelization can be adopted dur-\ning computation. In this work, we mainly refer to voxel-\nbased ones when talking about convolution-based works.\nTransformer for 3D Object Detection.Many researchers\nare motivated by the recent success of Transformer in point\ncloud processing (Guo et al. 2021; Fan, Yang, and Kankan-\nhalli 2021; Zhao et al. 2021; Fan, Yang, and Kankanhalli\n2022) and have tried to introduce Transformer into 3D ob-\nject detection. Pointformer (Pan et al. 2021) follows their\nparadigm and designs three Transformers to extract fea-\ntures from different scales. V oxel Transformer (Mao et al.\n2021b) combines Transformer with voxel representation and\nachieves much higher precision. Fan et al. (2022) propose a\nsingle-stride Transformer that improves the detection preci-\nsion on small objects such as pedestrians and cyclists. How-\never, the vanilla Transformer adopted in such works lacks\nthe ability of local structure encoding. Hence we propose\nSEFormer to better extract local structure.\nMethod\nIn this section, we first introduce the proposed SEFormer\nunit, including its motivation and corresponding architec-\nture. Then, we present the proposed detection architecture\nin the following sections.\nStructure Embedding Transformer (SEFormer)\nStructure Preserving& Encoding. Before introducing SE-\nFormer, we will first state our primary motivation, achieving\nsimultaneous structure-preserving and encoding. Such mo-\ntivation comes from one critical insight we have on two ex-\nisting operators, convolution and Transformer.\nConvolution is the most famous operator in computer vi-\nsion tasks because convolution’s locality and spatial invari-\nance adapt well to the inductive bias in images. While we\npropose another essential advantage of convolution is that it\n633\nFirst-stage Proposal\nProposal \nBased \nEmbedding \nPoint \nSampling\nSEFormer Based ROI Head\nSEFormer Based Spatial Structure Module \nF\n(1)\nF\n(2)\nF\n(3)\nF\n(4)\nE\n(final)\nSEFormer\nEmpty\nEmpty\nEmpty\nSR0\nSR 7\nSR3\nSR2\nSR1\nSampling + \nInterpolation\nReg.\nCls.\nE\n(init)\nSEFormerSEFormerSEFormer\nGIGI\nGI\nC\nE\n(1)\nSEFormerSEFormerSEFormer\nGIGI\nGI\nC\nE\n(3)\nSEFormerSEFormerSEFormer\nGIGI\nGI\nC\nProposal Sub RegionsSplit\nOE\n(init)\nOE\n(final)\nSEFormer SEFormer Block E Point-level \nEmbedding GI Grid Interpolation SR Sub Region OE Object-level \nEmbedding F Multi-scale Features C Cat.\nFigure 2: Overview of the proposed multi-scale SEFormer network. Stacked convolution layers are first used to extract multi-\nscale voxel features. Then a SEFormer-based spatial structure module aggregates the multi-scale features into several point-\nlevel embedding features (yellow cube). A SEFormer-based detection head further integrates the point-level embedding with\npredicted region proposals to generate the object-level embedding features (red cube) for final bounding boxes prediction.\ncan encode the structural information of data. To illustrate\nsuch a point, we first formulate convolution as follows:\nf\n′\np =\nX\nδ\nwδ(p) · fp+δ(p) (1)\nHere f, f\n′\nrepresents the input and output feature of a con-\nvolution layer at center position p, while δ denotes the rel-\native position between the neighboring points and the cen-\nter point. We decompose convolution as a two-step operator,\ntransformation and aggregation. Each point will be multi-\nplied during transformation by its corresponding kernel wδ.\nThen these points will be summed with a fixed aggregation\ncoefficient α = 1. The kernels are differently learned in\nconvolution based on their directions and distances to the\nkernel center. Hence convolution can encode the local spa-\ntial structure into the output. However, in convolution, all\nneighboring points are equally (α= 1) treated during aggre-\ngation. The mainstream convolution operator adopts a static\nand rigid kernel, but the LiDAR point cloud is often irreg-\nular and incomplete. Hence convolution inevitably includes\nirrelevant or noisy points in the output feature.\nCompared with convolution, the self-attention mechanism\nin Transformer provides a more effective method to pre-\nserve the irregular objects’ shapes and boundaries in Point\nCloud. For a point cloud with N points, p = [p1, . . . , pN ],\nTransformer computes the response of each point as:\nf\n′\np =\nX\nδ\nαδ(p) · Wvfp+δ(p) (2)\nHere αδ represents the self-attention coefficients among\npoints in the neighboring area while Wv means the value\ntransformation. We can still decompose Eq. 2 into a trans-\nformation process with a transformation matrix Wv and\nan aggregation process with attention coefficients αδ. The\ncoefficient αij between point pi and pj can be calculated\nas αij = exp(eij)\nPN\nk=1 exp(eik) . Here eij = (Wqfi)(Wkfj)T\n√c is\nthe scaled dot-produce attention between pi and pj and\nWq, Wk represent the transformation matrix for query and\nkey. Compared with the static α = 1 in convolution, the\nself-attention coefficients allow the Transformer to adap-\ntively choose the points for aggregation and exclude the in-\nfluence of unrelated points. We call Transformer’s such abil-\nity as structure preserving. However, according to Eq. 2,\nthe same transformation for value is shared among all the\npoints in Transformer. It means that the Transformer misses\nthe structure encodingability, which convolution has.\nGiven the above discussion, we can find that convolu-\ntion can encode data structure while Transformer can well\npreserve the structure. Hence, the straightforward idea is\nto design a new operator with both convolution and Trans-\nformer advantages. Hence we propose a new Transformer,\nSEFormer, which can be formulated as follows:\nf\n′\np =\nX\nδ\nαδ(p) · Wv\nδ(p)fp+δ(p) (3)\nIf we compare Eq. 3 with Eq. 2, we can find the most dif-\nference between SEFormer and vanilla Transformer is that\ndifferent transformations for value points are learned based\non the relative positions between points.\nArchitecture of SEFormer. Fig. 3 provides a compari-\nson between the vanilla Transformer and the proposed SE-\n634\ndd\nd\nd\n0\n1\n2 3\n4 5 6\n7 8\nQuery Point Real Points Sampled Nearest Key Point\nValue\nKey\nQuery\n0 1 2 8\n0 1 2 8\n0\n(a) Attention of Vanilla Transformer\ndd\nd\nd\n1 2 3\n4 0 5\n6 7 8\nQuery Point Real Points  (for interpolation) Interpolated Key Point\nValue\nKey\nQuery\n0 1 2 8\n0 1 2 8\n0\n(b) Attention of SEFormer\nFigure 3: Architecture comparison of Transformer and SE-\nFormer. (a) In the vanilla Transformer, nine nearest points\n(8 key points + 1 center query point) are sampled, and the\npoints share the same transformation for value. (b) While in\nSEFormer, the key points are generated with gird interpola-\ntion, and different points have their transformation forvalue.\nFormer. Given the irregularity of point cloud, we follow the\nparadigm of Point Transformer (Zhao et al. 2021) to first\nsample the neighboring points around each query point inde-\npendently before imported into the Transformer. Unlike the\ncommonly used sampling methods like K Nearest Neigh-\nbor (KNN) or Ball Query (BQ), we choose to adopt a grid\ninterpolation to generate the key points. As shown in Fig.\n3(b), around the red query point, several (8 in Fig. 3(b)) grid-\narranged virtual points are generated. The distance between\ntwo grids is a predefined d. Then the virtual points are in-\nterpolated with their nearest neighboring points. Compared\nwith traditional sampling like KNN, the advantage of grid\nsampling is that it can forcibly sample points from different\ndirections. As shown in Fig. 3(a), KNN greedily samples the\nnearest points. The sampled points show a strong bias from\nthe right corner points. While grid interpolation can avoid\nsuch a problem and provide a better description of the local\nstructure. However, grid interpolation uses a fixed grid dis-\ntance. Hence we adopt a multi-radii strategy in implementa-\ntion to increase the flexibility of sampling.\nDuring calculation, in Fig. 3 (a), all the anchor points\nshare the same transformation for key and value. While in\nFig. 3 (b), SEFormer constructs a memory pool containing\nmultiple transformation matrices (W v) for value. The in-\nterpolated key points will be transformed by different Wv\nbased on their relative grid coordinate to the query point.\nFor example, the top left key in Fig. (b) is transformed by\nthe red Wv while the green mWv transforms the correct\nkey. Hence SEFormer can have the structure encoding abil-\nity missed in the vanilla Transformer.\nSEFormer Based 3D Object Detection\nThe whole detection framework is shown in Fig. 2. We\nfirst construct a 3D CNN backbone for multi-scale voxel\nfeatures extraction and initial proposals generation. Then a\nmulti-scale SEFormer network is applied to extract richer\nlocal structure features from the voxel features. It con-\ntains a SEFormer-based spatial structure module for point-\nlevel structure features and a SEFormer-based ROI head for\nobject-level structure features.\n3D CNN Backbone.The CNN backbone transforms the in-\nput into multiple voxel features with 1×, 2×, 4× and 8×\ndownsampling sizes. After the feature extraction, the8× 3D\nfeature volume will be compressed along the Z-axis and\nconverted into a 2D BEV feature map. Then a center-based\napproach (Yin, Zhou, and Krahenbuhl 2021) is applied to\npredict first-stage proposals based on the BEV feature map.\nSEFormer Based Spatial Structure module.Then the pro-\nposed spatial structure module aggregates the multi-scale\nfeatures [F(1), F(2), F(3), F(4)] into several point-level em-\nbedding E. Starting from Einit, we first integrate the finest-\ngrained features F(1). Each embedding point’s correspond-\ning key points are interpolated from F(1). We use m dif-\nferent grid distance d to generate sets of multi-scale key\nfeatures as F(1)\n1 , F(1)\n2 , . . . ,F(1)\nm . Such a multi-radii strategy\ncan better handle the sparse and irregular point distribution\nin LiDAR. Then m parallel SEFormer blocks which contain\nmultiple SEFormer units are applied and result in m new\nembedding E(1)\n1 , E(1)\n2 , . . . ,E(1)\nm . In the end of the block,\nE(1)\n1 , E(1)\n2 , . . . ,E(1)\nm are concatenated and transformed into\nembedding E(1) with a vanilla Transformer. Then E(1) re-\npeats the above process and aggregates [F(2), F(3), F(4)]\ninto the final embedding Efinal . Compared with the origi-\nnal voxel featuresF, the embeddingEfinal aggregated con-\ntains a richer structural description of the local neighboring\narea.\nSEFormer Based ROI Head.Based on the point-level em-\nbeddings Efinal , the proposed head aggregates it into sev-\neral object-level embeddings to generate final proposals.\nSpecifically, we divide each proposal from the first-stage\ncenter-based head into multiple cubic sub-regions and inter-\npolate each sub-region with surrounding point-level embed-\nding features. Due to the sparsity of the point cloud, some\nsub-regions are often empty. Traditional works simply sum\nthe features from the non-empty parts. However, the car side\naway from the LiDAR source is often sparse. Hence the rel-\native positions of the empty sub-regions can provide a use-\nful object-level structure feature for direction recognition. In\ncontrast, the proposed SEFormer can utilize such informa-\ntion by embedding both the full and empty sub-regions. As\nshown in part III of Fig. 2, a SEFormer block takes in both\nthe empty and non-empty sub-regions and integrates their\nfeatures into a proposal embedding OEfinal . The stronger\nstructure embedding ability of SEFormer can provide a bet-\nter description of the object-level structure and then gener-\nates more accurate 3D proposals.\n635\nMethods LEVEL 1 (IoU=0.7) LEVEL 2 (IoU=0.7) LEVEL 1 3D mAP/mAPH by Distance\n3D mAP/mAPH 3D mAP/mAPH 0-30m 30-50m 50m-Inf\nPointPillars (Lang et al. 2019)* 56.62/- -/- 81.01/- 51.75/- 27.94/-\nMVF (Zhou et al. 2020) 62.93/- -/- 86.30/- 60.02/- 36.02/-\nAFDet (Ge et al. 2020) 63.69/- -/- 87.38/- 62.19/- 29.27/-\nPillar-OD (Wang et al. 2020) 69.8/- -/- 88.5/- 66.5/- 42.9/-\nCVCNet (Chen et al. 2020) 65.2/- -/- 86.80/- 62.19/- 29.27/-\nSVGA-Net (He et al. 2022b) 73.45/- 66.65/- 92.53 69.44 42.08\nV oTr-SSD (Mao et al. 2021b) 68.99/68.39 60.22/59.69 88.18/87.62 66.73/66.05 42.08/41.38\nPV-RCNN (Shi et al. 2020) 70.3/69.7 65.4/64.80 91.9/91.3 69.2/68.5 42.2/41.3\nV oTr-TSD (Mao et al. 2021b) 74.95/74.25 65.91/65.29 92.28/91.73 73.36/72.56 51.09/50.01\nRSN (Sun et al. 2021a) † 75.1/74.6 66.0/65.5 91.8/91.4 73.5/73.1 53.1/52.5\nV oxel RCNN (Deng et al. 2020) 75.59/- 66.59/- 92.49/- 74.09/- 53.15/-\nSCIR-Net(He et al. 2022c) 75.63/- 66.73/- 92.55/- 72.42/- -/-\nLiDAR RCNN(Li, Wang, and Wang 2021)† 76.0/75.5 68.3/67.9 92.1/91.6 74.6/74.1 54.5/53.4\nSST TS (Fan et al. 2022) † 76.22/75.79 68.04/67.64 -/- -/- -/-\nCT3D (Sheng et al. 2021) 76.30/- 69.04/- 92.51/- 75.07/- 55.36/-\nPyramid RCNN(Mao et al. 2021a) 76.30/75.68 67.23/66.68 92.67/92.20 74.91/74.21 54.54/53.45\nCenterPoint(Yin, Zhou, and Krahenbuhl 2021) † 76.7/68.8 76.2/68.3 -/- -/- -/-\nPDV (Hu, Kuai, and Waslander 2022) 76.85/76.33 69.30/68.81 93.13/92.71 75.49/74.91 54.75/53.90\nV oxel-to-Point(Li et al. 2021b) 77.24/- 69.77/- 93.23/- 76.21/- 55.79/-\nPV-RCNN++(Shi et al. 2022) 77.32/- 68.62/- -/- -/- -/-\nV oxSet (He et al. 2022a) 77.82/- 70.21/- 92.78/- 77.21/- 54.41/-\nOurs 79.02/78.52 70.31/69.85 93.10/92.66 78.07/77.54 57.60/56.87\nTable 1: Performance comparison on the Waymo Open Dataset with 202 validation sequences for the 3D vehicle detection.\nOnly one frame is used for training and testing. * is re-implemented by (Zhou et al. 2020). 20% training data are used for most\nmethods. While † denotes methods that use the whole 100% training dataset.\nExperiment\nIn this work, we mainly evaluate the proposed SEFormer on\nWaymo. Because its large data scale can provide much more\nconvincing evaluation than other benchmarks. We will first\nintroduce Waymo and describe the details of our implemen-\ntation. Then we will compare with state-of-the-art works on\nWaymo Open and provide an ablation analysis for the pro-\nposed method.\nImplementation Details\nWaymo Open Dataset.The Waymo dataset contains 1000\nLiDAR sequences in total. These sequences are further split\ninto 798 training sequences (including around 158k LiDAR\nsamples) and 202 validation sequences (including around\n40k LiDAR samples). Waymo provides object annotations\nin the entire 360 ◦ field. Its Official evaluation metrics in-\nclude the standard 3D mean Average Precision (mAP) and\nmAP weighted by heading accuracy (mAPH). In this work,\nwe present such two metrics mainly from two aspects: dif-\nficulty levels and object distance. For the first way, the\nground-truth boxes are divided into two groups: LEVEL\n1\n(number of points is more than five) and LEVEL 2 (the box\ncontains at least one point). For the second way, apart from\nthe overall mAP, we will also show respective mAP for ob-\njects located in 0 − 30m, 30 − 50m, and > 50m.\nNetwork Architecture.First, the points within the range of\n[−75.2, 75.2]m, [−75.2, 75.2]m, and [−2, 4]m for the X,\nY , and Z-axis are extracted. Then they are voxelized with\na (0.05m, 0.05m, 0.1m) step. Our first-stage convolution\nbackbone and the BEV neck follow the same architecture\nin (Yan, Mao, and Li 2018). The 3D backbone transforms\nthe input into 1×, 2×, 4× and 8× downsampled voxel vol-\numes with 16, 32, 64, 64 dimensions respectively. In the SE-\nFormer based spatial structure module, 4096 query points\nare selected for each scene. In the SEFormer head, each pro-\nposal is divided into 6 × 6 × 6 sub-regions.\nTraining and Inference. We use 4 RTX 3090 GPUs to\ntrain the entire network with batch size 8. We keep most\ntraining and inference hyper-parameters same with existing\nworks(Mao et al. 2021a; Shi et al. 2022; Deng et al. 2020;\nMao et al. 2021b; Sheng et al. 2021) for a fair comparison.\nWe adopt AdamW optimizer and one-cycle policy(Smith\nand Topin 2019) with division factor 10 and momentum\nranges from 0.95 to 0.85 to train the model. The learning\nrate is initialized with 0.003. The training time is 40 epochs.\nGiven the large scale of Waymo dataset, we uniformly use\n20% training samples for training but use the whole valida-\ntion set for evaluation.\nDetection Results on Waymo Detection Dataset\nFig. 4 illustrates a qualitative presentation of our detection\nresults on Waymo dataset. Table 1 and Table 2 show the 3D\nand BEV vehicle precision comparison with SOTA works\non the Waymo Open Dataset. 0.7 IoU threshold is adopted\nfor both evaluations. We mainly compare the one-frame de-\ntection results here.\nIn Table 1, it can be found further improvement of current\n3D object detection has become more and more difficult.\nHowever, our SEFormer can still achieve a significant im-\nprovement compared with prior works. For the commonly\n636\nDiff. Methods Overall 0-30m 30-50m 50m-Inf\nLV 1\nPointPillars(2019)* 75.57 92.10 74.06 55.47\nMVF(2020) 80.40 93.59 79.21 63.09\nPillar-OD (2020) 87.11 95.78 84.87 72.12\nPV-RCNN (2020) 82.96 97.35 82.99 64.97\nSVGA-Net (2022b) 83.52 97.60 83.14 64.52\nV oxel RCNN (2020) 88.19 97.62 87.34 77.70\nSCIR-Net (2022c) 88.45 97.71 88.41 -\nLiDAR RCNN (2021) 90.1 97.0 89.5 78.9\nV oxel-to-Point(2021b) 88.93 98.05 88.25 79.19\nV oxSet(2022a) 90.31 96.11 88.12 77.98\nOurs 91.73 98.13 91.23 82.12\nLV 2\nPV-RCNN (2020) 77.45 94.64 80.39 55.39\nSVGA-Net (2022b) 80.97 95.54 81.58 60.18\nV oxel RCNN (2020) 81.07 96.99 81.37 63.26\nSCIR-Net (2022c) 81.65 96.88 81.34 -\nLiDAR RCNN (2021) 81.7 94.3 82.3 65.8\nV oxel-to-Point(2021b) 82.18 97.48 82.51 64.86\nV oxSet(2022a) 80.56 96.79 80.44 62.37\nOurs 85.18 97.55 85.99 69.48\nTable 2: Comparison of BEV vehicle detection on the WOD\nwith 202 validation sequences. * is re-implemented by\n(Zhou et al. 2020). We only use 20% training data.\nused LEVEL 1 mAP/mAPH, we achieve 79.02%/78.52%,\nwhich exceeds state-of-the-art works by 1.2% for the\nLEVEL 1 mAP. For the difficult LEVEL 2 result, we can\nstill get the SOTA results and achieve 70.31%/69.85% for\nmAP/mAPH. Such results demonstrate the effectiveness of\nthe proposed SEFormer. To evaluate the influence of ob-\nject distance, we also provide the range-based LEVEL\n1\nmAP/mAPH. Although we show lower precision on near\nobjects (< 30m), 93.10% mAP vs 93.23% mAP, our im-\nprovement for distant objects is much more significant. The\nimprovement for 30−50m and 50m−Inf targets achieves\n0.86% and 3.19% mAP respectively. In most cases, the dis-\ntant objects are often sparse and only show part of the outline\nof the objects, which makes extracting useful structure in-\nformation much more difficult. While SEFormer’s structure\npreserving and encoding ability alleviates such problem.\nIn Table 2, SEFormer also outperforms prior works\non BEV precision. 91.73% LEVEL\n1 and 85.18%\nLEVEL 2 BEV mAP are achieved. It can be found that\nthe LEVEL 2 improvement is higher. Compared with\nLEVEL 1, LEVEL 2 contains objects that have very few\npoints. Hence such BEV results also support the above claim\nthat SEFormer has more advantages for sparse objects.\nAblation Study\nComparison between vanilla Transformer and SE-\nFormer. In this work, we propose a new Transformer, SE-\nFormer, to encode the local spatial structure. Hence we\ncompare the performance of the vanilla Transformer and\nthe proposed SEFormer in Table 3. The T and S represent\nvanilla Transformer and out SEFormer respectively. In this\nwork, we propose a SEFormer based spatial structure mod-\nule (SSM) and a SEFormer based head to extract point-\nand object-level structure features. Hence we use Trans-\nformer based SSM and head as the baseline. It can be found\nSSM Head LV 1 (IoU=0.7) LV 2 (IoU=0.7)\n3D mAP/mAPH 3D mAP/mAPH\nT T 76.10/75.61 68.24/67.78\nS T 77.54/77.05 68.82/68.38\nS S 79.02/78.52 70.31/69.85\nTable 3: Comparison between vanilla Transformer and SE-\nFormer. Here SSM and Head respectively denote the spatial\nstructure module and the detection head while T and S rep-\nresent vanilla Transformer and SEFormer respectively.\nBlock Num (m) LV 1 (IoU=0.7) LV 2 (IoU=0.7)\n3D mAP/mAPH 3D mAP/mAPH\n1 78.76/78.25 70.08/69.62\n2 79.02/78.52 70.31/69.85\n3 78.81/78.32 70.00/69.55\nTable 4: Effects of the number of parallel SEFormer blocks.\nvanilla Transformer only achieves 76.10%/75.61% and\n68.24%/67.78% for LEVEL 1 and LEVEL 2 mAP/mAPH.\nReplacing Transformer in SSM with SEFormer improves\nthe performance to77.54%/77.05% LEVEL 1 mAP/mAPH\nand 68.82%/68.38% LEVEL 2 mAP/mAPH. If we further\nreplace the Transformer in the head with SEFormer, we can\nfurther get 1.48% LEVEL 1 mAP and 1.49% LEVEL 2\nmAP improvement respectively. The results illustrate that\nthe proposed SEFormer has a better ability to capture the\nstructural features of local areas than Transformer. In most\nTransformer based works, relative position encoding is of-\nten used as a method to introduce the relative spatial rela-\ntionship. Hence we use relative position encoding in both\nthe Transformer based baseline and our SEFormer for a\nfair comparison. Hence the improvement of SEFormer over\nTransformer shows that simple relative position encoding\ncannot fully encode the structure information.\nEffect of the number of parallel SEFormer blocks.As\nnoted in Section , multiple parallel SEFormer blocks with\ndifferent search radii are established. Hence, we investigate\nthe effects of the number of parallel SEFormer blocks in Ta-\nble 4. In implementation, the parallel SEFormer blocks have\ngradually doubled search radii. While we set the initial radii\nas 0.4/0.8/1.2/2.4 for the multi-scale features. According to\nthe results, it can be found that 2 parallel SEFormer blocks\nachieve the best performance. Increasing or decreasing the\nblock number causes about 0.2% LEVEL\n1 mAP reduction.\nEffect of the number of heads.Multi-head Transformer\noften has better performance than single-head Transformer.\nHence, we provide an investigation of the effects of head\nnumber in Table 5. It can be found that single-head SE-\nFormer achieves 78.87%/78.37% and 70.14%/69.69% for\nLEVEL\n1 and LEVEL 2 mAP/mAPH respectively. Adopt-\ning double-head SEFormer can reach 79.02%/78.52%\nLEVEL 1 and 70.31%/69.85% LEVEL 2 mAP/mAPH.\nBut the results reduce if we further increase the head num-\nber. Hence we choose head number h = 2in this work.\nEffect of multi-scale features.Table 6 demonstrates the ef-\nfects of using multi-scale features. Only using the single-\nscale feature (conv1) only achieves 78.54% and 69.94%\n637\nSEFormer\n Vanilla Transformer\nFigure 4: Qualitative visualization on WOD. The green boxes denote the groundtruth.\nHead Num (h) LV 1 (IoU=0.7) LV 2 (IoU=0.7)\n3D mAP/mAPH 3D mAP/mAPH\n1 78.87/78.37 70.14/69.69\n2 79.02/78.52 70.31/69.85\n4 79.00/78.51 70.30/69.85\nTable 5: Effects of the head number in SEFormer.\nconv1 conv2 conv3 conv4 LV 1 (IoU=0.7) LV 2 (IoU=0.7)\n3D mAP/mAPH 3D mAP/mAPH\n✓ 78.54/78.04 69.94/69.49\n✓ ✓ 78.81/78.32 70.16/69.71\n✓ ✓ ✓ 79.02/78.52 70.31/69.85\n✓ ✓ ✓ ✓ 78.77/78.29 69.96/69.5 2\nTable 6: Effects of multi-scale features.\nLEVEL 1 and LEVEL 2 mAP. Introducing more features of\ndifferent scales gradually improves the performance while\nusing features of all 4 scales reduces the precision to some\nextent. Please see our supplementary material for more re-\nsults.\nEffect of grid interpolation.Table 7 illustrates the effects\nof grid interpolation. For the control group, grid interpo-\nlation is replaced with random sampling within a radius.\nPlease see our supplementary material for more results and\ndiscussions.\nStructure of the spatial structure module.In the spatial\nE\n(init) Block2\nBlock1\nBlock2\nBlock1\nBlock2\nBlock1\nBlock2\nBlock1 E\n(final)(c)\nE\n(init)\nE\n(final)\nBlock2Block1 Block2Block1 Block2Block1 Block2Block1(b)\nE\n(init)\nE\n(final)\nBlock2\nBlock1\n(a) Block2\nBlock1\nBlock2\nBlock1\nBlock2\nBlock1\nFigure 5: Illustration of (a) fully parallel (b) fully chained\nand (c) half parallel half chained spatial structure module.\nLV 1 (IoU=0.7) LV 2 (IoU=0.7)\n3D mAP/mAPH 3D mAP/mAPH\nw/o GI 78.63/78.14 69.83/69.38\nw/ GI 79.02/78.52 70.31/69.85\nTable 7: Effects of grid interpolation.\nSSM Structure LEVEL 1 (IoU=0.7) LEVEL 2 (IoU=0.7)\n3D mAP/mAPH 3D mAP/mAPH\n(a) 78.70/78.23 70.04/69.56\n(b) 78.86/78.39 70.12/69.68\n(c) 79.02/78.52 70.31/69.85\nTable 8: Comparison among different SSM structures.\nstructure module, we aggregate the multi-scale features one\nby one. While multiple SEFormer blocks with different radii\nare adopted to extract structure information from one fea-\nture. To show the effects of such strategy, we design three\ndifferent structures for the spatial structure module, fully\nparallel, full chained, and half parallel half chained. Fig. 5\nillustrates the difference among such three structures. Half\nparallel half chained denotes the structure used in this work.\nTheir effects on model performance are shown in Table 8. It\ncan be found that the half parallel half chained structure has\nbetter results than others.\nConclusion\nThis work proposes a new Transformer, SEFormer. In\nvanilla Transformer, all the value points share the same\ntransformation. Hence it lacks the ability to encode the\ndistance-direction-oriented local spatial structure. To solve\nsuch problem, SEFormer learns different transforms for\nvalue points based on their relative directions and distances\nto the center query point. Based on the proposed SEFormer,\nwe establish a new 3D detection network including a SE-\nFormer based spatial structure module to extract point-level\nstructure information and a SEFormer based head to capture\nobject-level structure features. Compared with state-of-the-\nart solutions, our SEFormer achieves higher detection preci-\nsion on the Waymo Open dataset.\n638\nAcknowledgements\nThis work was partly supported by the National Key Re-\nsearch and Development Program of China under Grant\n2021YFB3200903, in part by the National Natural Science\nFoundation of China under Grant 61934005, in part by the\nNatural Science Foundation of Beijing Municipality under\nGrant L211005.\nReferences\nCao, X.; Yuan, P.; Feng, B.; and Niu, K. 2022. CF-DETR:\nCoarse-to-Fine Transformers for End-to-End Object Detec-\ntion. Proceedings of the AAAI Conference on Artificial In-\ntelligence, 36(1): 185–193.\nChen, C.; Chen, Z.; Zhang, J.; and Tao, D. 2022. Sasa:\nSemantics-augmented set abstraction for point-based 3d ob-\nject detection. In AAAI Conference on Artificial Intelligence,\nvolume 1.\nChen, Q.; Sun, L.; Cheung, E.; and Yuille, A. L. 2020. Every\nview counts: Cross-view consistency in 3d object detection\nwith hybrid-cylindrical-spherical voxelization. Advances in\nNeural Information Processing Systems.\nChi, C.; Wei, F.; and Hu, H. 2020. Relationnet++: Bridging\nvisual representations for object detection via transformer\ndecoder. Advances in Neural Information Processing Sys-\ntems, 33: 13564–13574.\nDeng, J.; Shi, S.; Li, P.; Zhou, W.; Zhang, Y .; and\nLi, H. 2020. V oxel R-CNN: Towards High Perfor-\nmance V oxel-based 3D Object Detection. arXiv preprint\narXiv:2012.15712.\nDong, J.; Huang, Y .; Zhang, S.; Chen, S.; and Zheng, N.\n2022. Construct Effective Geometry Aware Feature Pyramid\nNetwork for Multi-Scale Object Detection. Proceedings of\nthe AAAI Conference on Artificial Intelligence, 36(1).\nFan, H.; Yang, Y .; and Kankanhalli, M. 2022. Point spatio-\ntemporal transformer networks for point cloud video mod-\neling. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 45(2): 2181–2192.\nFan, H.; Yang, Y .; and Kankanhalli, M. S. 2021. Point\n4D Transformer Networks for Spatio-Temporal Modeling\nin Point Cloud Videos. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n14204–14213.\nFan, L.; Pang, Z.; Zhang, T.; Wang, Y .-X.; Zhao, H.; Wang,\nF.; Wang, N.; and Zhang, Z. 2022. Embracing single stride\n3d object detector with sparse transformer. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 8458–8468.\nGe, R.; Ding, Z.; Hu, Y .; Wang, Y .; Chen, S.; Huang, L.;\nand Li, Y . 2020. Afdet: Anchor free one stage 3d object\ndetection. arXiv preprint arXiv:2006.12671.\nGeiger, A.; Lenz, P.; Stiller, C.; and Urtasun, R. 2013. Vision\nmeets robotics: The kitti dataset. The International Journal\nof Robotics Research, 32(11): 1231–1237.\nGuo, M.; Cai, J.; Liu, Z.; Mu, T.; Martin, R. R.; and Hu, S.\n2021. PCT: Point cloud transformer. Comput. Vis. Media,\n7(2): 187–199.\nHe, C.; Li, R.; Li, S.; and Zhang, L. 2022a. V oxel Set Trans-\nformer: A Set-to-Set Approach to 3D Object Detection from\nPoint Clouds. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 8417–8427.\nHe, Q.; Wang, Z.; Zeng, H.; Zeng, Y .; and Liu, Y . 2022b.\nSvga-net: Sparse voxel-graph attention network for 3d ob-\nject detection from point clouds. InProceedings of the AAAI\nConference on Artificial Intelligence, volume 36, 870–878.\nHe, Q.; Zeng, H.; Zeng, Y .; and Liu, Y . 2022c. SCIR-Net:\nStructured Color Image Representation Based 3D Object\nDetection Network from Point Clouds. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 36,\n4486–4494.\nHu, J. S.; Kuai, T.; and Waslander, S. L. 2022. Point density-\naware voxels for lidar 3d object detection. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 8469–8478.\nLang, A. H.; V ora, S.; Caesar, H.; Zhou, L.; Yang, J.; and\nBeijbom, O. 2019. Pointpillars: Fast encoders for object de-\ntection from point clouds. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n12697–12705.\nLi, J.; Dai, H.; Shao, L.; and Ding, Y . 2021a. Anchor-free\n3d single stage detector with mask-guided attention for point\ncloud. In Proceedings of the 29th ACM International Con-\nference on Multimedia, 553–562.\nLi, J.; Dai, H.; Shao, L.; and Ding, Y . 2021b. From voxel\nto point: Iou-guided 3d object detection for point cloud with\nvoxel-to-point decoder. In Proceedings of the 29th ACM In-\nternational Conference on Multimedia, 4622–4631.\nLi, Z.; Wang, F.; and Wang, N. 2021. LiDAR R-CNN: An\nEfficient and Universal 3D Object Detector. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 7546–7555.\nMao, J.; Niu, M.; Bai, H.; Liang, X.; Xu, H.; and Xu, C.\n2021a. Pyramid r-cnn: Towards better performance and\nadaptability for 3d object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n2723–2732.\nMao, J.; Xue, Y .; Niu, M.; Bai, H.; Feng, J.; Liang, X.; Xu,\nH.; and Xu, C. 2021b. V oxel Transformer for 3D Object\nDetection. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 3164–3173.\nPan, X.; Xia, Z.; Song, S.; Li, L. E.; and Huang, G. 2021.\n3d object detection with pointformer. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 7463–7472.\nQi, C. R.; Litany, O.; He, K.; and Guibas, L. J. 2019. Deep\nhough voting for 3d object detection in point clouds. In\nproceedings of the IEEE/CVF International Conference on\nComputer Vision, 9277–9286.\nQi, C. R.; Liu, W.; Wu, C.; Su, H.; and Guibas, L. J. 2018.\nFrustum pointnets for 3d object detection from rgb-d data.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 918–927.\n639\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017a. Pointnet:\nDeep learning on point sets for 3d classification and segmen-\ntation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 652–660.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017b. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. arXiv preprint arXiv:1706.02413.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn:\nTowards real-time object detection with region proposal net-\nworks. Advances in neural information processing systems,\n28.\nSheng, H.; Cai, S.; Liu, Y .; Deng, B.; Huang, J.; Hua, X.-\nS.; and Zhao, M.-J. 2021. Improving 3D Object Detec-\ntion with Channel-wise Transformer. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n2743–2752.\nShi, S.; Guo, C.; Jiang, L.; Wang, Z.; Shi, J.; Wang, X.; and\nLi, H. 2020. Pv-rcnn: Point-voxel feature set abstraction\nfor 3d object detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n10529–10538.\nShi, S.; Jiang, L.; Deng, J.; Wang, Z.; Guo, C.; Shi, J.; Wang,\nX.; and Li, H. 2022. PV-RCNN++: Point-voxel feature set\nabstraction with local vector representation for 3D object de-\ntection. arXiv preprint arXiv:2102.00463.\nShi, S.; Wang, X.; and Li, H. 2019. Pointrcnn: 3d object\nproposal generation and detection from point cloud. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 770–779.\nShrivastava, A.; Gupta, A.; and Girshick, R. 2016. Train-\ning region-based object detectors with online hard example\nmining. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 761–769.\nSmith, L. N.; and Topin, N. 2019. Super-convergence: Very\nfast training of neural networks using large learning rates.\nIn Artificial intelligence and machine learning for multi-\ndomain operations applications, volume 11006, 1100612.\nInternational Society for Optics and Photonics.\nSun, P.; Kretzschmar, H.; Dotiwalla, X.; Chouard, A.; Pat-\nnaik, V .; Tsui, P.; Guo, J.; Zhou, Y .; Chai, Y .; Caine, B.;\net al. 2020. Scalability in perception for autonomous driv-\ning: Waymo open dataset. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n2446–2454.\nSun, P.; Wang, W.; Chai, Y .; Elsayed, G.; Bewley, A.; Zhang,\nX.; Sminchisescu, C.; and Anguelov, D. 2021a. Rsn: Range\nsparse net for efficient, accurate lidar 3d object detection.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 5725–5734.\nSun, P.; Zhang, R.; Jiang, Y .; Kong, T.; Xu, C.; Zhan, W.;\nTomizuka, M.; Li, L.; Yuan, Z.; Wang, C.; et al. 2021b.\nSparse r-cnn: End-to-end object detection with learnable\nproposals. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 14454–14463.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, Y .; Fathi, A.; Kundu, A.; Ross, D. A.; Pantofaru,\nC.; Funkhouser, T.; and Solomon, J. 2020. Pillar-based\nobject detection for autonomous driving. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XXII 16, 18–34.\nSpringer.\nWang, Y .; Zhang, X.; Yang, T.; and Sun, J. 2021. Anchor\nDETR: Query Design for Transformer-Based Object Detec-\ntion. arXiv preprint arXiv:2109.07107.\nWu, A.; Han, Y .; Zhu, L.; and Yang, Y . 2021a. Instance-\ninvariant domain adaptive object detection via progressive\ndisentanglement. IEEE Transactions on Pattern Analysis\nand Machine Intelligence.\nWu, A.; Liu, R.; Han, Y .; Zhu, L.; and Yang, Y . 2021b.\nVector-decomposed disentanglement for domain-invariant\nobject detection. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 9342–9351.\nXie, Q.; Lai, Y .-K.; Wu, J.; Wang, Z.; Lu, D.; Wei, M.; and\nWang, J. 2021. VENet: V oting Enhancement Network for\n3D Object Detection. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, 3712–3721.\nXu, Q.; Zhong, Y .; and Neumann, U. 2022. Behind the\ncurtain: Learning occluded shapes for 3D object detection.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 36, 2893–2901.\nYan, Y .; Mao, Y .; and Li, B. 2018. Second: Sparsely embed-\nded convolutional detection. Sensors, 18(10): 3337.\nYang, Z.; Sun, Y .; Liu, S.; and Jia, J. 2020. 3dssd: Point-\nbased 3d single stage object detector. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 11040–11048.\nYang, Z.; Sun, Y .; Liu, S.; Shen, X.; and Jia, J. 2019.\nStd: Sparse-to-dense 3d object detector for point cloud. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 1951–1960.\nYin, T.; Zhou, X.; and Krahenbuhl, P. 2021. Center-\nbased 3d object detection and tracking. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 11784–11793.\nZhao, H.; Jiang, L.; Jia, J.; Torr, P. H.; and Koltun, V . 2021.\nPoint transformer. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, 16259–16268.\nZheng, W.; Tang, W.; Jiang, L.; and Fu, C.-W. 2021. SE-\nSSD: Self-Ensembling Single-Stage Object Detector From\nPoint Cloud. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 14494–14503.\nZheng, Y .; Duan, Y .; Lu, J.; Zhou, J.; and Tian, Q. 2022. Hy-\nperDet3D: Learning a Scene-conditioned 3D Object Detec-\ntor. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 5585–5594.\nZhou, Y .; Sun, P.; Zhang, Y .; Anguelov, D.; Gao, J.; Ouyang,\nT.; Guo, J.; Ngiam, J.; and Vasudevan, V . 2020. End-to-\nend multi-view fusion for 3d object detection in lidar point\nclouds. In Conference on Robot Learning, 923–932. PMLR.\n640",
  "topic": "Point cloud",
  "concepts": [
    {
      "name": "Point cloud",
      "score": 0.7051717042922974
    },
    {
      "name": "Computer science",
      "score": 0.6886240243911743
    },
    {
      "name": "Embedding",
      "score": 0.6437622308731079
    },
    {
      "name": "Transformer",
      "score": 0.6244964003562927
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5766791105270386
    },
    {
      "name": "Object detection",
      "score": 0.5218122005462646
    },
    {
      "name": "Lidar",
      "score": 0.49184584617614746
    },
    {
      "name": "Computer vision",
      "score": 0.4908038079738617
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38170087337493896
    },
    {
      "name": "Engineering",
      "score": 0.08745273947715759
    },
    {
      "name": "Geography",
      "score": 0.0787932276725769
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Remote sensing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I118347636",
      "name": "Australian National University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ]
}