{
  "title": "Traces of Memorisation in Large Language Models for Code",
  "url": "https://openalex.org/W4394744221",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4377300147",
      "name": "Ali Al-Kaswan",
      "affiliations": [
        "Delft University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2521711437",
      "name": "Maliheh Izadi",
      "affiliations": [
        "Delft University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2155442793",
      "name": "Arie van Deursen",
      "affiliations": [
        "Delft University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4376606797",
    "https://openalex.org/W4385270018",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W4283172211",
    "https://openalex.org/W4288057780",
    "https://openalex.org/W6839352123",
    "https://openalex.org/W4391725330",
    "https://openalex.org/W3035261884",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2965527189",
    "https://openalex.org/W3011159643",
    "https://openalex.org/W3138815606",
    "https://openalex.org/W4385573569",
    "https://openalex.org/W4385734176",
    "https://openalex.org/W4220672926",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4385573947",
    "https://openalex.org/W4319988693",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W3210860486",
    "https://openalex.org/W4281483318",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4283364113",
    "https://openalex.org/W3106051020"
  ],
  "abstract": "Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentially extractable we were able to extract 47% from a CodeGen-Mono-16B code completion model. We also observe that models memorise more, as their parameter count grows, and that their pre-training data are also vulnerable to attack. We also find that data carriers are memorised at a higher rate than regular code or documentation and that different model architectures memorise different samples. Data leakage has severe outcomes, so we urge the research community to further investigate the extent of this phenomenon using a wider range of models and extraction techniques in order to build safeguards to mitigate this issue.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8041644096374512
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.582298994064331
    },
    {
      "name": "Construct (python library)",
      "score": 0.5197712779045105
    },
    {
      "name": "Code (set theory)",
      "score": 0.4795779585838318
    },
    {
      "name": "Software",
      "score": 0.4776507616043091
    },
    {
      "name": "Natural language",
      "score": 0.4481557309627533
    },
    {
      "name": "Source code",
      "score": 0.43622493743896484
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42270150780677795
    },
    {
      "name": "Data modeling",
      "score": 0.4114794135093689
    },
    {
      "name": "Natural language processing",
      "score": 0.3704484701156616
    },
    {
      "name": "Database",
      "score": 0.24408894777297974
    },
    {
      "name": "Programming language",
      "score": 0.2094055414199829
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98358874",
      "name": "Delft University of Technology",
      "country": "NL"
    }
  ],
  "cited_by": 13
}