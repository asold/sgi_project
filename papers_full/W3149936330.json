{
    "title": "TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking",
    "url": "https://openalex.org/W3149936330",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2111477885",
            "name": "Chu Peng",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2103935630",
            "name": "Wang Jian-g",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2749626812",
            "name": "You, Quanzeng",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2747133689",
            "name": "Ling, Haibin",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A2352283912",
            "name": "Liu Zicheng",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2967086232",
        "https://openalex.org/W2604679602",
        "https://openalex.org/W2920942303",
        "https://openalex.org/W2124781496",
        "https://openalex.org/W2252355370",
        "https://openalex.org/W2766984662",
        "https://openalex.org/W3035442500",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2739374836",
        "https://openalex.org/W2986732333",
        "https://openalex.org/W2963481014",
        "https://openalex.org/W3175455177",
        "https://openalex.org/W2964321699",
        "https://openalex.org/W1921895346",
        "https://openalex.org/W6775253321",
        "https://openalex.org/W2520234541",
        "https://openalex.org/W2964019074",
        "https://openalex.org/W2168356304",
        "https://openalex.org/W4205537101",
        "https://openalex.org/W6779650541",
        "https://openalex.org/W2122469558",
        "https://openalex.org/W2897582990",
        "https://openalex.org/W2237765446",
        "https://openalex.org/W2519362791",
        "https://openalex.org/W1521019969",
        "https://openalex.org/W3094000868",
        "https://openalex.org/W3034512672",
        "https://openalex.org/W4383755718",
        "https://openalex.org/W3035410385",
        "https://openalex.org/W3118212025",
        "https://openalex.org/W2596030096",
        "https://openalex.org/W3119686997",
        "https://openalex.org/W6696672603",
        "https://openalex.org/W2963063317",
        "https://openalex.org/W2343841668",
        "https://openalex.org/W2963121817",
        "https://openalex.org/W3035727180",
        "https://openalex.org/W3096068180",
        "https://openalex.org/W2016135469",
        "https://openalex.org/W639708223",
        "https://openalex.org/W2511791013",
        "https://openalex.org/W2579024533",
        "https://openalex.org/W6784419737",
        "https://openalex.org/W6750697433",
        "https://openalex.org/W2018776447",
        "https://openalex.org/W6788023325",
        "https://openalex.org/W2962803115",
        "https://openalex.org/W2007352603",
        "https://openalex.org/W2739491435",
        "https://openalex.org/W4214516362",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3176403636",
        "https://openalex.org/W1576709274",
        "https://openalex.org/W3207452968",
        "https://openalex.org/W2603203130",
        "https://openalex.org/W2225887246",
        "https://openalex.org/W3034679090",
        "https://openalex.org/W3113093028",
        "https://openalex.org/W2474389331",
        "https://openalex.org/W3097237405",
        "https://openalex.org/W1528063097",
        "https://openalex.org/W2111644456",
        "https://openalex.org/W3027919498",
        "https://openalex.org/W3084173793",
        "https://openalex.org/W2963049565",
        "https://openalex.org/W3095753995",
        "https://openalex.org/W2895150009",
        "https://openalex.org/W3190647944",
        "https://openalex.org/W3099598823",
        "https://openalex.org/W2339473870",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4295331127",
        "https://openalex.org/W3012922853",
        "https://openalex.org/W3093245983",
        "https://openalex.org/W3035308063",
        "https://openalex.org/W3036415994",
        "https://openalex.org/W3115390238",
        "https://openalex.org/W2508815980",
        "https://openalex.org/W2291627510",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2798542761",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W3104218139",
        "https://openalex.org/W3021922267",
        "https://openalex.org/W3084263909",
        "https://openalex.org/W2981393651",
        "https://openalex.org/W4308351635"
    ],
    "abstract": "Tracking multiple objects in videos relies on modeling the spatial-temporal interactions of the objects. In this paper, we propose a solution named TransMOT, which leverages powerful graph transformers to efficiently model the spatial and temporal interactions among the objects. TransMOT effectively models the interactions of a large number of objects by arranging the trajectories of the tracked objects as a set of sparse weighted graphs, and constructing a spatial graph transformer encoder layer, a temporal transformer encoder layer, and a spatial graph transformer decoder layer based on the graphs. TransMOT is not only more computationally efficient than the traditional Transformer, but it also achieves better tracking accuracy. To further improve the tracking speed and accuracy, we propose a cascade association framework to handle low-score detections and long-term occlusions that require large computational resources to model in TransMOT. The proposed method is evaluated on multiple benchmark datasets including MOT15, MOT16, MOT17, and MOT20, and it achieves state-of-the-art performance on all the datasets.",
    "full_text": "TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking\nPeng Chu1 Jiang Wang1 Quanzeng You1 Haibin Ling2\nZicheng Liu1\n1Microsoft 2Stony Brook University\n{pengchu, jiangwang, quanzeng.you, zliu}@microsoft.com, hling@cs.stonybrook.edu\nAbstract\nTracking multiple objects in videos relies on modeling\nthe spatial-temporal interactions of the objects. In this pa-\nper, we propose a solution named TransMOT, which lever-\nages powerful graph transformers to efﬁciently model the\nspatial and temporal interactions among the objects. Trans-\nMOT effectively models the interactions of a large number\nof objects by arranging the trajectories of the tracked ob-\njects as a set of sparse weighted graphs, and constructing a\nspatial graph transformer encoder layer, a temporal trans-\nformer encoder layer, and a spatial graph transformer de-\ncoder layer based on the graphs. TransMOT is not only\nmore computationally efﬁcient than the traditional Trans-\nformer, but it also achieves better tracking accuracy. To\nfurther improve the tracking speed and accuracy, we pro-\npose a cascade association framework to handle low-score\ndetections and long-term occlusions that require large com-\nputational resources to model in TransMOT. The proposed\nmethod is evaluated on multiple benchmark datasets includ-\ning MOT15, MOT16, MOT17, and MOT20, and it achieves\nstate-of-the-art performance on all the datasets.\n1. Introduction\nRobust tracking of multiple objects in video is criti-\ncal for many real-world applications, ranging from vision-\nbased surveillance to autonomous driving vehicles. Most of\nthe recent state-of-the-art Multiple Object Tracking (MOT)\nmethods use the tracking-by-detection strategy, where tar-\nget candidates proposed by an object detector on each\nframe are associated and connected to form target trajec-\ntories [3, 15, 21, 33, 37, 49, 52]. There are two core tasks in\nthis framework: accurate object detection and robust target\nassociation. In this paper, we focus on building models for\nrobust target association, where successfully modeling the\ntemporal history and appearance of the targets, as well as\ntheir spatial-temporal relationships plays an important role.\nTraditional models of spatial-temporal relationships usu-\nally rely on manually designed association rules, such as\nsocial interaction models or spatial exclusion models [28].\nThe recent advances in deep learning inspire us to explore\nto learn the spatial-temporal relationships using deep learn-\ning. In particular, the success of Transformer suggests a\nnew paradigm of modeling temporal dependencies through\nthe powerful self-attention mechanism. Recent studies\nin [30, 44] have proved the feasibility of directly modeling\nspatial-temporal relationships with transformers. However,\nthe tracking performance of transformer-based tracker is not\nstate-of-the-art for several reasons. First, a video contains a\nlarge number of objects. Modeling the spatial temporal rela-\ntionships of these objects with a general Transformer is in-\neffective, because it does not take the spatial-temporal struc-\nture of the objects into consideration. Second, it requires\na lot of computation resources and data to learn a trans-\nformer to model long term temporal dependencies. Third,\nDETR-based object detector used in these works is still not\nthe state-of-the-art for MOT.\nIn this paper, we propose a novel spatial-temporal graph\nTransformer for MOT (TransMOT) to resolve all these is-\nsues. In TransMOT, the trajectories of all the tracked tar-\ngets are arranged as a series of sparse weighted graphs that\nare constructed using spatial relationships of the targets.\nBased on these sparse graphs, TransMOT builds a spatial\ngraph transformer encoder layer, a temporal transformer\nencoder layer, and a spatial transformer decoder layer to\nmodel the spatial temporal relationships of the objects. It\nis more computationally efﬁcient during training and infer-\nence because of the sparsity of the weighted graph repre-\nsentation. It is also a more effective model than regular\ntransformer because it exploits the structure of the objects.\nWe also propose a cascade association framework to han-\ndle low-score detections and long-term occlusions. By in-\ncorporating TransMOT into the cascade association frame-\nwork, we do not need to learn to associate a large number\nof low-score detections or model long-term temporal rela-\ntionships. TransMOT can also be combined with different\nobject detectors or visual feature extraction sub-networks to\nform a uniﬁed end-to-end solution so that we can exploit the\narXiv:2104.00194v2  [cs.CV]  3 Apr 2021\nSpatial Graph \nTransformer Encoder \nLayer\nTemporal \nTransformer Encoder \nLayer\nSpatial Graph \nTransformer Decoder \nLayer\nSpatial-Temporal Graph Transformer Encoder\nTransMOT\nSpatial Graph Transformer Decoder\nEmbedding Embedding\nVirtual \nSink\nVirtual \nSource\nOcclusion/Entering \nHandling\nExtended \nAssignment Matrix\nTracked Targets Graph Series Candidates Graph\ntt-1t-2t-3t-4\nFigure 1. Overview of the proposed TransMOT pipeline for online MOT. The trajectories graph seriesΞt−1 till frame t −1 and detection\ncandidates graph Θt at frame t serve as the source and target inputs, respectively, to the spatial-temporal graph transformer.\nstate-of-the-art object detectors for MOT. Extensive experi-\nments on MOT15, MOT16, MOT17, and MOT20 challenge\ndatasets demonstrate that the proposed approach achieves\nthe best overall performance and establishes new state-of-\nthe-art in comparison with other published works.\nIn summary, we make following contributions:\n• We propose a spatial-temporal graph Transformer\n(TransMOT) for effective modeling of the spatial-\ntemporal relationship of the objects for MOT.\n• We design a cascade association framework that\ncan improve TransMOT and other transformer-based\ntrackers by handling low-score detections and long-\nterm occlusion.\n2. Related Works\nMost of the recent Multiple Object Tracking (MOT)\ntrackers are based on the tracking-by-detection framework.\nTracking-by-detection framework generates tracklets by as-\nsociating object detections in all the frames using match-\ning algorithms such as Hungarian algorithm [6, 16, 20],\nnetwork ﬂow [14, 56, 57], and multiple hypotheses track-\ning [10, 22]. Many works solve the association problem by\nbuilding graphs of object detections across all the frames,\nsuch as multi-cuts [21, 46] and lifting edges [47]. However,\nthese methods need to perform computationally expensive\nglobal optimization on large graphs, which limits their ap-\nplication to online tracking.\nRecently, deep learning-based association algorithm is\ngaining popularity in MOT [62]. In [34] and [32], recur-\nrent neural networks (RNN) is explored to solve the associ-\nation problem using only the motion information. In [11], a\npower iteration layer is introduced in the rank-1 tensor ap-\nproximation framework [43] to solve the multi-dimension\nassignment in MOT. [4] and [61] combine object detection\nand target association that directly predict target locations in\nthe current frame. In [53], a differentiable MOT loss is pro-\nposed to learn deep Hungarian Net for association. In [8],\ngraph convolutional neural network is adopted as a neural\nsolver for MOT, where a dense graph connecting every pair\nof nodes in different frames is constructed to infer the asso-\nciation. The proposed TransMOT also constructs a spatial\ngraph for the objects within the same frame, but it exploits\nthe Transformer networking architecture to jointly learn the\nspatial and temporal relationship of the tracklets and candi-\ndates for efﬁcient association.\nTransformer has achieved great success in various com-\nputer vision tasks, such as detection [9] and segmenta-\ntion [26]. In [55], Transformer is adopted for trajectory pre-\ndiction. The studies in [30,44] are the pioneer investigations\nin applying Transformer in MOT. Both methods use DETR\nfor detection and feature extraction, and model the spatial-\ntemporal relationship of the tracklets and detections using\nTransformer. The proposed TransMOT framework utilizes\nspatial graph transformer to model spatial relationship of\nthe tracklets and detections, and it factorizes the spatial and\ntemporal transformer encoder for model efﬁcient modeling.\n3. Overview\nWe aim at joint detection and tracking multiple objects in\nvideos in an online fashion. Fig. 1 illustrates our framework\nbuilt upon tracking-by-detection framework. The frame-\nwork maintains a set of Nt−1 tracklets, each of which rep-\nresents a tracked object. Each tracklet Lt−1\ni maintains a\nset of states, such as its past locations\n{\nˆxI\nt′\n}t−1\nt′=t−T and\nappearance features\n{ˆfI\nt′\n}t−1\nt′=t−T on the previous T image\nframes. Given a new image frame It, the online tracking\nalgorithm eliminates the tracklets whose tracked object ex-\nits the scene, determines whether any tracked objects are\noccluded, computes new locations for the existing tracklets\nˆXt =\n{\nˆxt\ni\n}Nt\ni=1, and generates new tracklets for new objects\nthat enter the scene.\nAs shown in Fig. 1, our framework contains two major\nparts: the detection and feature extraction sub-networks,\nand spatial temporal graph transformer association sub-\nnetwork. At each frame, the detection and feature extrac-\ntion sub-networks generate Mt candidate object detection\nproposals Ot =\n{\not\nj\n}Mt\nj=1, as well as visual features for\neach proposal. The spatial-temporal graph transformer ﬁnds\nthe best candidate proposal for each tracklet and models the\nspecial events, such as entering, exiting, or occlusion.\nFor each tracklet Lt−1\ni , the best matching is ob-\ntained through selecting the oj\nt maximizing the afﬁnity\nφ(Lt−1\ni ,ot\nj), where φ(·) is a scoring function that computes\nthe afﬁnity of the tracklet state and the candidate. Taking all\ntracklets into consideration, the problem can be formulated\nas a constrained optimization problem as\nmax\nAt=(at\nij)\nNt−1∑\ni=1\nMt∑\nj=1\nat\nijφ(Lt−1\ni ,ot\nj), (1)\ns.t.\n\n\n\n∑\ni at\nij = 1, ∀i= 1,...,N t−1∑\nj at\nij = 1, ∀j = 1,...,M t\nat\nij ∈{0,1}, ∀i= 1,...,N t−1; j = 1,...,M t\n(2)\nwhere At = (at\nij) indicates the association between track-\nlets Lt−1 = {Lt−1\ni }Nt\ni=1 and detected candidates Ot. Eq. 2\nis used to enforce the assignment constraints.\nIn order to more effectively model the spatial-temporal\nrelationship between all the tracklets and candidates, the\nproposed framework rewrites Eq. 1 and Eq. 2 into a single\nfunction At = Φ(Lt−1,Ot), where Lt−1 and Ot consist of\nall the tracklets and candidates, respectively.\nTo model the spatial-temporal object correlation, we\nbuild a weighted spatial graph Θt for the proposals at the\ncurrent frame, and a set of weighted spatial graphs Ξt−1 =\n{ξt−T ,ξ2,...,ξ t−1}of the tracked objects at the previ-\nous T frames. The spatial-temporal graph neural network\nutilizes these graphs to build an efﬁcient spatial-temporal\ngraph transformer that models the relationship between the\ntracked objects and newly generated proposals. It gener-\nates an assignment matrix ¯At to track the objects and model\nthe special events, such as entering, exiting, or occlusion,\nas shown in Fig. 1. The assignment matrix is used to up-\ndate the tracked target while the special events are handled\nby the post-processing module, which will be explained in\nSec. 4.4.\nThe details of the spatial-temporal graph Transformer\nwill be explained in Sec. 4.1 and Sec. 4.2. The two types\nof training losses to train TransMOT will be elaborated in\nSec. 4.3.\n4. TransMOT\nSpatial-temporal graph Transformer for MOT (Trans-\nMOT) uses the graphs Ξt−1 and Θt to learn a mapping\nΦ(·) that models the spatial-temporal correlations, and gen-\nerates an assignment/mapping matrix ¯At. It contains three\nparts: a spatial graph transformer encoder layer, a temporal\ntransformer encoder layer, and a spatial graph transformer\ndecoder layer. We propose graph multi-head attention to\nAdd & Norm\nFeed Forward\nAdd & Norm\nGraph Conv\nLinear\n Linear\nCollect Feat.\nCollect Feat.\nGraph Multi-Head \nAttention\n  \n   \nFigure 2. The spatial graph transformer encoder layer.\nmodel spatial relationship of the tracklets and candidates\nusing the self-attention mechanism. It is crucial for both\nthe spatial graph transformer encoder layer and the spatial\ngraph transformer decoder layer.\n4.1. Spatial-Temporal Graph Transformer Encoder\nThe spatial-temporal graph encoder consists of a spatial-\ntemporal graph transformer encoder layer to model the spa-\ntial correlation among tracklets, and a temporal transformer\nencoder layer to further fuse and encode the spatial and tem-\nporal information of the tracklets. We ﬁnd that by factoring\nthe transformer into spatial and temporal transformers, it\nmakes the model both more accurate and computationally\nefﬁcient.\n4.1.1 Spatial Graph Transformer Encoder Layer\nThe input of spatial-temporal graph encoder layer is the\nstates of the tracklets for the past T frames. The track-\nlet state features are arranged using a sequence of track-\nlet graphs Ξt−1 = {ξt−T ,ξt−T+1,...,ξ t−1} , where\nξt−1 = G({xt−1\ni },Et−1\nX ,wt−1\nX ) is the spatial graph1 of the\ntracklets at frame t−1. At frame t−1, the graph node\nxt−1\ni represents the status of i-th tracklet at this frame, two\nnodes are connected by an edge in Et−1\nX if their corre-\nsponding bounding boxes have IoU larger than 0, and the\nedge weight in wt−1\nX is set to the IoU. The weight matrix\nwt−1\nX ∈RNt−1×Nt−1 is a sparse matrix, whose (i,j) entry\nis the weight of the edge connecting nodeiand node j, or 0\nif they are not connected.\nThe node features for the tracklets are ﬁrst embedded\nthrough a source embedding layer (a linear layer) indepen-\ndently for each node. All the node features are arranged into\na feature tensor Fs ∈RNt−1×T×D, where Dis the dimen-\nsion of the source embedding layer. It is passed into the spa-\ntial graph transformer encoder layer together with the graph\n1We use G(·) to denote a graph.\nseries as shown in Fig. 2. Inside the layer, a multi-head\ngraph attention module is utilized to generate self-attention\nfor the input graph series. This module takes feature tensor\nFs and the graph weights wt−1\nX to generate self-attention\nweights for the i-th head:\nFAW\ni = softmax\n[\nϕ(Fs,WQ\ni ,WK\ni ) ◦wt−1\nX\n]\n, (3)\nwhere ϕ(·) is the regular scaled dot-product to obtain atten-\ntion weights as in [48], and ◦is the element-wise product.\nIt can be understood as computing the spatial graph self-\nattention for each timestamp independently.\nThe multi-head graph attention utilizes the graph weights\nwt−1\nX to generate non-zero attention weights only for the\ntracklets that have spatial interactions, because the tracklets\nthat are far way from each other usually have very little in-\nteraction in practice. By focusing its attention on a much\nsmaller subset, the spatial graph transformer encoder layer\nmodels the interactions more effectively, and runs faster\nduring training and inference.\nWe also apply graph convolution instead of the linear\nlayer to aggregate information from neighboring nodes. Af-\nter the graph convolution layer, the node features are col-\nlected to form a value tensor FV\ni . Combined with the at-\ntention weights in Eq. 3, the graph multi-head attention\nweighted feature tensor can be written as\nFen\natt = Concate({FAW\ni ⊗FV\ni }) ⊗WO,\nwhere {·}iterates and aggregates the outputs from all the\nattention heads, ⊗is the tensor mode product2\nThe attention weighted feature tensor is projected\nthrough a linear feed forward and a normalization layer to\nget the ﬁnal output of the spatial graph transformer encoder\nlayer.\n4.1.2 Temporal Transformer Encoder Layer\nThe features of the tracklets are further encoded by a tem-\nporal transformer encoder layer. The temporal transformer\nencoder layer transposes the ﬁrst two dimension of the out-\nput tensor from the spatial graph transformer encoder, re-\nsulting in a tensor Fen\ntmp ∈ RT×Nt−1×D. The temporal\ntransformer encoder layer employs a standard Transformer\nencoder layer over the temporal dimension for each track-\nlets independently. It calculates the self-attention weights\nalong the temporal dimension, and computes the temporal\nattention-weighted feature tensor for the tracklets.\nThe output of the temporal transformer encoder layer is\nthe ﬁnal output of the spatial-temporal graph transformer\nencoder Fen\nout.\nGraph Muti-\nHead Attention\nAdd & Norm\nAdd Virtual \nSource\nMuti-Head Cross \nAttention\nDuplicate\nAdd \nVirtual \nSink\nSpatial Graph \nTransformer Decoder\nFigure 3. Illustration of the spatial graph transformer decoder.\n4.2. Spatial Graph Transformer Decoder\nThe spatial graph transformer decoder produces ex-\ntended assignment matrix ¯At from the candidate graph\nΘt = G({ot\nj},Et\nO,wt\nO) and the output of the spatial-\ntemporal transformer encoder Fen\nout. The candidate graph\nis constructed similarly to the tracklet graphs in Sec. 4.1.\nEach node ot\nj represents a candidate in frame t. Two nodes\nare connected only if their bounding box’s IoU is larger than\nzero, and the weight of the edge is set to the IoU. Besides the\nnodes representing the real candidates, a virtual sink node\nis added to the graph. The virtual sink node is responsible\nfor exiting or occlusion events of any tracklet in the current\nframe. In particular, a node with a set of learnable embed-\nding fsnk ∈RD is added to the Θt. The virtual sink node\nis connected to all the other nodes with weight 0.5.\nSimilar as the encoder in Sec. 4.1, the candidate node\nfeatures of input graph are embedded and collected. The\nfsnk is appended to the embedded feature set such that\nFde\ntgt ∈ R(Mt+1)×1×D. The spatial graph decoder ﬁrst\nuses graph multi-head attention to encode the node features\nthat is similar to the one Sec. 4.1, shown in Fig. 3. We\ndenote the attention weighted candidate node features as\nFde\natt ∈R(Mt+1)×1×D.\nFor the tracklet embedding Fen\nout generated by the\nspatial-temporal graph transformer encoder, we add a vir-\ntual source to handle the candidates that initiate a new\ntracklet in the current frame t to form an extended track-\nlet embedding Fen′\nout ∈RT×(Nt−1+1)×D. The embedding\nof the virtual source is a learnable parameter. Note that\nwe only add one virtual source node compared to multi-\n2It performs matrix product of each slice of right and left tensors along\nthe dimension sharing the same length.\nple virtual source nodes in Transform-based MOT track-\ners, because we ﬁnd adding one virtual source node yields\ncomparable performance as adding multiple virtual source\nnodes while achieving better computational efﬁciency.Fde\natt\nis duplicated Nt−1 + 1 times such that Fde\natt → Fde′\natt ∈\nR(Mt+1)×(Nt−1+1)×D. Multi-head cross attention is cal-\nculated for Fde′\natt and Fen′\nout to generate unnormalized atten-\ntion weights. The output is passed through a feed forward\nlayer and a normalization layer to generate the output ten-\nsor R(Mt+1)×(Nt−1+1)×D that corresponds to the matching\nbetween the tracklets and the candidates.\nThe output of the spatial graph decoder can be passed\nthrough a linear layer and a Softmax layer to generate the\nassignment matrix ¯At ∈R(Mt+1)×(Nt−1+1).\n4.3. Training\nThe TransMOT is trained end-to-end with the guidance\nof the groundtruth extended assignment matrix. The con-\nstraints in Eq. 2 need to be relaxed to allow efﬁcient opti-\nmization. We relax the constraints so that a detection candi-\ndate is always associated with a tracklet or a virtual source,\nwhile a tracklet can be associated with multiple candidates.\nIn this way, Eq. 2 can be relaxed as:\ns.t.\nNt−1+1∑\nj\n¯at\nij = 1,i ∈[1,Mt],¯at\nij ∈{0,1}.\nAs a result, a row of the assignment matrix can be treated as\na probability distribution over a total ofNt−1 +1 categories,\nand we use the cross-entropy loss to optimize the network.\nIn each training iteration, a continuous sequence ofT+1\nframes are randomly sampled from the training set. The\nbounding boxes and their corresponding IDs are collected\nfrom each frame. The groundtruth bounding boxes are\nthen replaced by the bounding boxes generated from the\nobject detector by matching their IoUs. In this way, the\nTransMOT will be more robust to detection noise. For\nall bounding boxes in a frame, their IDs are remapped to\n{0,1,··· ,Nt−1}indicating whether they are matched to a\ntracklet or a virtual source.\nFor the rows that correspond to actual tracklets, a cross-\nentropy loss is utilized, as mentioned above. The last row\nof ¯At represents the virtual sink, and it may be matched to\nmultiple tracklets. Thus, a multi-label soft margin loss is\nemployed to optimize this part separately.\nIn summary, the overall training loss can be written as\nL= − 1\nMt\nMt∑\nm=1\nymlog(¯am)\n+ λ\nNt−1\nNt−1∑\nn=1\nysnk\nn log\n( 1\n1 + e−a′n\n)\n+ λ\nNt−1\nNt−1∑\nn=1\n(1 −ysnk\nn )log\n( e−a′\nn\n1 + e−a′n\n)\n,\nTracked \nTargets\nMatching \nwith Motion\n TransMOT\nExpire \nTargets\nDuplicate \nDetection \nRemoving\nLong-term \nOcclusion \nHandling\nAssociated Targets\nDetection\nLow Score \nDetection\nFigure 4. Illustration of the cascade association framework based\ntracking system.\nwhere ym and ysnk\nn are IDs of the detection candidates and\nthe virtual sink respectively, ¯am is the row element of ¯At,\n¯aMt+1 = {a′\nn}, and λis a weighting coefﬁcient.\n4.4. Cascade Association Framework\nAlthough the spatial-temporal graph transformer net-\nwork can effectively model the spatial-temporal relation-\nship between tracklets and object candidates, we can\nachieve better inference speed and tracking accuracy by in-\ncorporating it in a three-stage cascade association frame-\nwork. The illustration of the cascade association framework\nis shown in Fig. 4.\nThe ﬁrst stage matches and ﬁlters the low conﬁdence\ncandidate boxes using motion information. In particular,\na Kalman Filter predicts the bounding boxes of the robustly\ntracked tracklets in the current frames. These tracklets must\nbe successfully and continuously associated on the past Kr\nframes. The IoU between the predicted bounding boxes and\nthe candidate boxes are utilized as the association score. We\nmatch the predicted boxes and the candidate boxes with\nHungarian algorithm. Only the matched pairs with IoU\nlarger than τM are associated. The rest of the candidate\nboxes whose conﬁdence scores are lower than a threshold\nis ﬁltered. This stage is mainly a speed optimization, be-\ncause it removes the candidate boxes and tracklets that can\nbe easily matched or ﬁltered, and leaves the more challeng-\ning tracklets and candidate boxes for TransMOT for further\nassociation.\nIn the second stage, TransMOT calculates the extended\nassignment matrix for the rest of the tracklets and candi-\ndates. The upper left part of the extended assignment matrix\n¯At denotes as ˆA ∈RMt×Nt−1 determines the matching of\nactual tracklets and candidate boxes. Since the elements of\nˆA∈[0,1] is a soft assignment, we apply bipartite matching\nalgorithm to generate the actual matching. Similarly, only\nthe pairs with assignment scores larger than a threshold will\nbe matched at this stage. After the matching, the unresolved\ncandidate boxes will be matched again to recover occlusion\nor initiate new targets in the third stage.\nThe third stage is the Long-Term Occlusion and Dupli-\ncated detection handling module in Fig. 4. Since Trans-\nMOT only models the tracklets of the previous T frames,\nMethod IDF1 MOTA MT ML ↓ FP↓ FN↓ IDS↓\nDMT [23] 49.2 44.5 34.7% 22.1% 8,088 25,335 684\nTubeTK [35] 53.1 58.4 39.3% 18.0% 5,756 18,961 854\nCDADDAL [3] 54.1 51.3 36.3% 22.2% 7,110 22,271 544\nTRID [29] 61.0 55.7 40.6% 25.8% 6,273 20,611 351\nRAR15 [16] 61.3 56.5 45.1% 14.6% 9,386 16,921 428\nGSDT [50] 64.6 60.7 47.0% 10.5% 7,334 16,358 477\nFair [59] 64.7 60.6 47.6% 11.0% 7,854 15,785 591\nTransMOT 66.0 57.0 64.5% 17.8% 12,454 13,725 244\nTable 1. Tracking Performance on the MOT15 benchmark test set\nprivate detection track. Best in bold.\nMethod IDF1 MOTA MT ML ↓ FP↓ FN↓ IDS↓\nIoU [7] 46.9 57.1 23.6% 32.9% 5,702 70,278 2,167\nCTracker [36] 57.2 67.6 32.9% 23.1% 8,934 48,305 1,897\nLMCNN [2] 61.2 67.4 38.2% 19.2% 10,109 48,435 931\nDeepSort [51] 62.2 61.4 32.8% 18.2% 12,852 56,668 781\nFUFET [41] 68.6 76.5 52.8% 12.3% 12,878 28,982 1,026\nLMP [47] 70.1 71.0 46.9% 21.9% 7,880 44,564 434\nCSTrack [25] 73.3 75.6 42.8% 16.5% 9,646 33,777 1,121\nTransMOT 76.8 76.7 56.5% 19.2% 14,999 26,967 517\nTable 2. Tracking Performance on the MOT16 benchmark test set\nprivate detection track. Best in bold.\nthe tracklets that are occluded in the previous T frames are\nnot matched. For these tracklets, we store their visual fea-\ntures and the bounding box coordinates at the latest frame\nwhen they were visible, and use them to calculate the as-\nsociation cost of a tracklet and a candidate detection. The\nassociation cost is deﬁned as the addition of the Euclidean\ndistance of the visual features and the normalized top dis-\ntance of the occluded tracklets and candidate boxes.\nDtop =\n\n(\nui + wi\n2 −uj −wj\n2 ,vi −vj\n)\n/\nhi,\nwhere [u,v] indicates the left upper corner of the bounding\nbox and [w,h] indicates its size3.\nDuplicate detection handling removes the unmatched de-\ntection candidates that might be duplicate of the detections\nthat are already matched to a tracklet. In this step, the un-\nassociated candidates are matched to all the associated can-\ndidates. The association cost for this step is the bound-\ning box intersection between the associated box and un-\nassociated box over the area of the un-associated candi-\ndates. This cost is chosen so that a candidate box that is\na sub-box of a matched candidate is removed, because it is\nvery likely that this box is a duplicate detection. All the\nmatched candidate detection at this step will be removed\ndirectly.\nFinally, each of the remaining candidates is initialized as\na new tracklet, and the unresolved tracklets that have not\nbeen updated for more than Kp frames are removed. The\ntracklets that are not updated for less than Kp frames are\nset to “occluded” state.\n3The notation w should not be confused with the weight w in Sec. 4.1.\n5. Experiments\nWe conduct extensive experiments on four standard\nMOT challenge datasets for pedestrian tracking: MOT15\n[24], MOT16, MOT17, and MOT20 [31]. The proposed\nTransMOT based tracking framework is evaluated on both\npublic and private detection tracks.\n5.1. Experiment Setting and Implementation De-\ntails\nThe proposed approach is implemented in PyTorch, and\nthe training and inference are performed on a machine with\na 10 cores CPU@3.60GHz and an Nvidia Tesla V100 GPU.\nWe set the number of frames for trackletsT = 5, the feature\nembedding dimension D= 1024, and the number of heads\nfor all the multi-headed attention in spatial and temporal\ntransformers to 8. For graph multi-head attention module, a\nsingle layer of ChebConv from [13] with neighboring dis-\ntance of 2 is adopted. The node features for an object at a\nframe are the concatenation of its visual features and nor-\nmalized bounding box coordinates. During training, we use\nvanilla SGD with an initial learning rate of 0.0015. For\nall the experiments in Sec. 5.2, we use the training dataset\nfrom [27] to train our TransMOT model. During inference,\nτM is set to 0.75 for selecting conﬁdent associations. Kr\nand Kp are set to 15 and 50 respectively.\nWe trained a YOLOv5 [1] detector model with 407 lay-\ners for 300 epochs on the combination of CrowdHuman\ndataset [42] and the training sets of MOT17/MOT20. The\nSiamFC network [40] pretrained on the ILSVRC15 dataset\nis adopted as our visual feature extraction sub-network. The\nmaximum input image dimension of the tracking pipeline is\nset to 1920. The detector runs at 15.4 fps on our machine,\nwhile the TransMOT and visual feature extraction sub-\nnetwork run at 24.5fps. The whole tracking pipeline runs at\n9.6 fps. We also experimented with using TransTrack [44]\nas our detection and feature extraction sub-network, as well\nas other visual features. These comparisons will be com-\npared in the MOT16/17 and ablation parts of Sec. 5.2.\nTo evaluate the performance of the proposed method,\nthe standard ID score metrics [39] and CLEAR MOT met-\nrics [5] are reported. ID score metrics calculate the tra-\njectory level ID precision (IDP), ID recall (IDR), and the\nIDF1 scores. CLEAR MOT metrics include multiple object\ntracking precision (MOTP) and multiple object tracking ac-\ncuracy (MOTA) that combine false positives (FP), false neg-\natives (FN) and the identity switches (IDS). The percentage\nof mostly tracked targets (MT) and the percentage of mostly\nlost targets (ML) are also reported.\n5.2. Evaluation Results\nMOT15. MOT15 [24] contains 22 different indoor and out-\ndoor scenes for pedestrian tracking. The 22 sequences are\ncollected from several public and private datasets, and they\nMethod IDF1 MOTA MT ML ↓ FP↓ FN↓ IDS↓\nPublic Detection\nTrctrD [53] 53.8 53.7 19.4% 36.6% 11,731 247,447 1,947\nTracktor [4] 55.1 56.3 21.1% 35.3% 8,866 235,449 1,987\nCTTrack [61] 59.6 61.5 26.4% 31.9% 14,076 200,672 2,583\nTrackFormer [30] 59.8 61.8 35.4% 21.1% 35,226 177,270 2,982\nMPNTrack [8] 61.7 58.8 28.8% 33.5% 17,413 213,594 1,185\nLifT [19] 65.6 60.5 27.0% 33.6% 14,966 206,619 1,189\nMAT [18] 69.2 67.1 38.9% 26.4% 22,756 161,547 1,279\nTransMOT-P 72.2 68.7 33.5% 31.0% 8,078 167,602 1,014\nPrivate Detection\nDAN [45] 49.5 52.4 21.4% 30.7% 25,423 234,592 8,431\nTransTrack [44] 56.9 65.8 32.2% 21.8% 24,000 163,683 5,355\nTubeTK [35] 58.6 63.0 31.2% 19.9% 27,060 177,483 5,727\nCTTrack [61] 64.7 67.8 34.6% 24.6% 18,498 160,332 6,102\nFair [59] 72.3 73.7 43.2% 17.3% 27,507 117,477 8,073\nTransMOT-D 66.9 68.8 35.8% 31.7% 26,670 147,690 1,797\nTransMOT 75.1 76.7 51.0% 16.4% 36,231 93,150 2,346\nTable 3. Tracking Performance on the MOT17 benchmark test set.\nBest in bold.\nMethod IDF1 MOTA MT ML ↓ FP↓ FN↓ IDS↓\nPublic Det.\nSORT* [6] 45.1 42.7 16.7% 26.2% 27,521 264,694 4,470\nTracktor [4] 52.7 52.6 29.4% 26.7% 6,930 236,680 1,648\nMPNTrack [8] 59.1 57.6 38.2% 22.5% 16,953 201,384 1,210\nLPCMOT [12] 62.5 56.3 34.1% 25.2% 11,726 213,056 1,562\nTransMOT-P 74.3 73.1 54.3% 14.6% 12,366 125,665 1,042\nPrivate Det.\nMLT [58] 54.6 48.9 30.9% 22.1% 45,660 216,803 2,187\nGSDT [50] 67.5 67.1 53.1% 13.2% 31,913 135,409 3,131\nFair [59] 67.3 61.8 68.8% 7.6% 103,440 88,901 5,243\nCSTrack [25] 68.6 66.6 50.4% 15.5% 25,404 144,358 3,196\nTransMOT 75.2 77.5 70.7% 9.1% 34,201 80,788 1,615\nTable 4. Tracking Performance on the MOT20 benchmark test set.\nBest in bold. Method marked with * in public detection track does\nnot use public detection ﬁltering mechanism. It might achieve bet-\nter tracking accuracy if the mechanism is employed.\nwere recorded with different camera motion, camera an-\ngles and imaging conditions. The dataset is equally split for\ntraining and testing. We report the quantitative results of the\nproposed method on the private detection track in Tab. 1,\nand the visualizations of the tracking results on selected\nvideos are shown in Fig. 5. TransMOT achieves state-of-\nthe-art performance in metrics IDF1, MT, FN, and IDS. The\nrelatively lower MOTA score on this dataset is caused by the\nhigh FP rate, because not all the objects are exhaustively an-\nnotated for some testing sequences.\nMOT16/17. MOT16 and MOT17 [31] contain the same\n14 videos for pedestrians tracking. MOT17 has more accu-\nrate ground truth annotations compared to MOT16 dataset.\nMOT17 also evaluates the effect of object detection qual-\nity on trackers, by providing three pretrained object detec-\ntors using DPM [17], Faster-RCNN [38] and SDP [54]. We\nreport the performance and comparisons with the state-of-\nthe-art methods on the private detection track of MOT16\nin Tab. 2. Our approach outperforms all other published\ntrackers using the private detector in both IDF1 and MOTA\nmetrics.\nIn MOT17, for a more complete comparison, we conﬁg-\nure TransMOT as two additional settings: TransMOT-P and\nTransMOT-D. TransMOT-P uses the public detection re-\nsults, and it follows the ﬁltering mechanism adopted by [4]\nand [61]. A new trajectory is initialized only if its bounding\nbox at the current frame overlaps a public detection with\nIoU larger than 0.5. We compare TransMOT-P with other\ntrackers adopting the same ﬁltering mechanism on the pub-\nlic detection track of MOT17 in Tab. 3. Compared with\nregular Transformer-based tracker [30], TransMOT outper-\nforms it in IDF1, MOTA, and IDS by a large margin. Trans-\nMOT also achieves the best IDF1 and MOTA scores among\nall published trackers, which demonstrates the robustness of\nTransMOT against detection quality variations.\nTransMOT-D adopts the DETR framework as detection\nand visual feature extraction sub-networks. TransMOT-D\ntakes the detection outputs of pretrained TransTrack [44]\nand their Transformer embedding as visual features. For a\nfair comparison, the pretrained model of TransTrack is not\nﬁne-tuned in TransMOT-D. We compare TransMOT-D and\nTransMOT with state-of-the-art trackers on MOT17 private\ndetection track in Tab. 3. The performance of TransMOT-D\nis better than TransTrack [44] by 10.0% and 3.0% in IDF1\nand MOTA metrics respectively. This shows that our Trans-\nMOT framework can better model the spatial-temporal rela-\ntionship of the tracklets and detections than standard Trans-\nformer. TransMOT-D achieves the lowest IDS among all\nthe methods, because it generates fewer tracklets than other\nmethods. Our TransMOT framework achieves the best\nIDF1 and MOTA metrics. It is also ranked as the top tracker\nin terms of IDF1 and FN on the private detector track of\nMOT17 challenge leaderboard when the paper is submitted.\nMOT20. MOT20 consists of eight sequences for pedestrian\ntracking. MOT20 video sequences are more challenging\nbecause they have much higher object density, e.g. 170.9\nvs 31.8 in the test set. We report the experimental results\nof the proposed TransMOT and the comparison with the\nother methods in Tab. 4. Our method establishes state-of-\nthe-art in most metrics among all submissions. In partic-\nular, TransMOT achieves 6.6% higher IDF1 than CStrack,\nwhich ranked second in the leaderboard. In public detec-\ntion setting, TransMOT-P also demonstrates its robustness\nto detection noise. Compared with the private detector set-\nting, the MOTA decreases 4.4%, but IDF1 score only drops\n0.9%. The experiments on both public and private detec-\ntion settings demonstrate that the capability of TransMOT\nof modeling a large number of tracklets and detections in\ncrowd scenes as shown in Fig. 5.\n5.3. Ablation\nWe study the signiﬁcance of different modules in the pro-\nposed method through ablation study as shown in Tab. 5.\nThe ablations are conducted on the MOT17 training set. To\navoid overﬁtting, the object detector used in the ablation\nMOT15: Venice-1 MOT15: A VG-TownCentre MOT16-03 MOT16-07\nMOT17-08 MOT17-14 MOT20-04 MOT20-06\nFigure 5. Results visualization of selected sequences in MOT15, MOT16, MOT17, and MOT20.\nstudy is trained on only the CrowdHuman dataset.\nWe ﬁrst evaluate the settings that remove the Match\nwith Motion module or the Long-Term Occlusion Handling\nmodule from the cascade association framework, noted as\nTransMOT−MwM and TransMOT−LTOH in Tab. 5. The\nMOTA score decreases signiﬁcantly without MwM because\nthe TransMOT cannot handle low score detection candi-\ndates well. It also leads to a slower speed, because Trans-\nMOT needs to handle more tracklets and detection can-\ndidates that were handled by MwM. Dropping the LTOH\nmodule results in more fragmented tracklets because the\ntracklets that are occluded for more than T frames are not\nlinked in this case. It dramatically increases the number of\ntracklets, reduces the IDF1 score, and increases the infer-\nence speed.\nThe major hyper-parameters of the tracker are also stud-\nied. We test TransMOT running at a reduced input image\nresolution at maximum width of 1280 (TransMOT@1280\nin Tab. 5) (The default setting uses a maximum width of\n1920). We ﬁnd that lower resolution input causes the tracker\nto miss small targets at the far end of the scene. The choice\nof the temporal history length T is investigated by choos-\ning T = 1, T = 10, and T = 20 (T is set to 5 in default\nsetting). Compared with T = 1 where no temporal history\nis included, T > 1 can improve the association accuracy.\nHowever, since increasing T also adds more tracklets for\nassociation, it increases the complexity of the association\ntask, and makes the learning harder under a limited number\nof training data for MOT. In addition, a lot of the tracklets\nthat can be matched by increasing T are already handled by\nthe long-term occlusion handling module. Thus increasing\nLbeyond 5 has no performance gain and makes inference\nslower. T = 5 is used in all the other experiments.\nFinally, in addition to SiamFC and DETR features, we\nevaluate other shallow and deep visual features, including\ncolor histogram and ReID feature DGNet [60]. Beneﬁt-\ning from the fully trainable Transformer, even using sim-\nConﬁguration IDF1 MOTA FPS\nTransMOT−MwM 76.0 73.6 9.2\nTransMOT−LTOH 76.5 74.6 7.8\nTransMOT@1280 75.3 72.9 13.8\nTransMOT@T = 1 77.8 74.7 9.8\nTransMOT@T = 10 78.0 74.8 9.3\nTransMOT@T = 20 77.8 74.7 8.5\nTransMOT+DGNet 77.5 74.7 6.0\nTransMOT+Histogram 77.5 74.9 12.3\nTransMOT+SiamcFC(Ours) 78.1 74.8 9.6\nTable 5. Ablations on the MOT17 benchmark training set. FPS\nindicates the inference speed of the full tracker that includes the\ndetection and feature sub-net.\nple color histogram features, TransMOT can achieve simi-\nlar performance with the one using deep ReID features and\nruns at a much faster inference speed. On the other hand,\nSiamcFC features perform better than both color histogram\nand ReID features, because it is trained on a large scale\nvideo dataset.\n6. Conclusion\nWe proposed a novel Spatial-Temporal Graph Trans-\nformer for multi-object tracking (TransMOT) with Trans-\nformers. By formulating the tracklets and candidate detec-\ntions as a series of weighted graphs, the spatial and temporal\nrelationships of the tracklets and candidates are explicitly\nmodeled and leveraged. The proposed TransMOT not only\nachieves higher tracking accuracy, but also is more compu-\ntationally efﬁcient than the transitional Transformer-based\nmethods. Additionally, we developed the cascade associa-\ntion framework to further optimize the speed and accuracy\nof TransMOT by ﬁltering low score candidates, recovering\nthe tracklets that are occluded for a long time, and remove\nduplicate detections. Experiments on MOT15, MOT16,\nMOT17, and MOT20 challenge datasets show that the pro-\nposed approach achieves state-of-the-art performance on all\nthe benchmark datasets.\nReferences\n[1] Yolov5. https://github.com/ultralytics/\nyolov5. 6\n[2] Maryam Babaee, Zimu Li, and Gerhard Rigoll. A dual cnn–\nrnn for multiple people tracking. Neurocomputing, 368:69–\n83, 2019. 6\n[3] Seung-Hwan Bae and Kuk-Jin Yoon. Conﬁdence-based data\nassociation and discriminative deep appearance learning for\nrobust online multi-object tracking. TPAMI, 2018. 1, 6\n[4] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe.\nTracking without bells and whistles. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 941–951, 2019. 2, 7\n[5] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple\nobject tracking performance: the clear mot metrics. JIVP,\n2008. 6\n[6] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and\nBen Upcroft. Simple online and realtime tracking. In ICIP,\n2016. 2, 7\n[7] Erik Bochinski, V olker Eiselein, and Thomas Sikora. High-\nspeed tracking-by-detection without using image informa-\ntion. In AVSS, 2017. 6\n[8] Guillem Bras ´o and Laura Leal-Taix ´e. Learning a neu-\nral solver for multiple object tracking. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6247–6257, 2020. 2, 7\n[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision , pages 213–229. Springer, 2020.\n2\n[10] Jiahui Chen, Hao Sheng, Yang Zhang, and Zhang Xiong. En-\nhancing detection model for multiple hypothesis tracking. In\nCVPRw, 2017. 2\n[11] Peng Chu and Haibin Ling. Famnet: Joint learning of fea-\nture, afﬁnity and multi-dimensional assignment for online\nmultiple object tracking. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 6172–\n6181, 2019. 2\n[12] Peng Dai, Renliang Weng, Wongun Choi, Changshui Zhang,\nZhangping He, and Wei Ding. Learning a proposal classiﬁer\nfor multiple object tracking. In CVPR, 2021. 7\n[13] Micha ¨el Defferrard, Xavier Bresson, and Pierre Van-\ndergheynst. Convolutional neural networks on graphs with\nfast localized spectral ﬁltering. In NIPS, 2016. 6\n[14] Afshin Dehghan, Yicong Tian, Philip HS Torr, and Mubarak\nShah. Target identity-aware network ﬂow for online multiple\ntarget tracking. In CVPR, 2015. 2\n[15] Lo ¨ıc Fagot-Bouquet, Romaric Audigier, Yoann Dhome, and\nFr´ed´eric Lerasle. Improving multi-frame data association\nwith sparse representations for robust near-online multi-\nobject tracking. In ECCV, 2016. 1\n[16] Kuan Fang, Yu Xiang, Xiaocheng Li, and Silvio Savarese.\nRecurrent autoregressive networks for online multi-object\ntracking. In WACV, 2018. 2, 6\n[17] Pedro F Felzenszwalb, Ross B Girshick, David McAllester,\nand Deva Ramanan. Object detection with discriminatively\ntrained part-based models. TPAMI, 2010. 7\n[18] Shoudong Han, Piao Huang, Hongwei Wang, En Yu,\nDonghaisheng Liu, Xiaofeng Pan, and Jun Zhao. Mat:\nMotion-aware multi-object tracking. arXiv preprint\narXiv:2009.04794, 2020. 7\n[19] Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn,\nand Paul Swoboda. Lifted disjoint paths with application\nin multiple object tracking. In International Conference on\nMachine Learning, pages 4364–4375. PMLR, 2020. 7\n[20] Chang Huang, Bo Wu, and Ramakant Nevatia. Robust object\ntracking by hierarchical association of detection responses.\nIn ECCV, 2008. 2\n[21] Margret Keuper, Siyu Tang, Bjorn Andres, Thomas Brox,\nand Bernt Schiele. Motion segmentation & multiple object\ntracking by correlation co-clustering. TPAMI, 2018. 1, 2\n[22] Chanho Kim, Fuxin Li, Arridhana Ciptadi, and James M\nRehg. Multiple hypothesis tracking revisited. InICCV, 2015.\n2\n[23] Han-Ul Kim and Chang-Su Kim. Cdt: Cooperative detection\nand tracking for tracing multiple objects in video sequences.\nIn European Conference on Computer Vision , pages 851–\n867. Springer, 2016. 6\n[24] Laura Leal-Taix ´e, Anton Milan, Ian Reid, Stefan Roth, and\nKonrad Schindler. MOTChallenge 2015: Towards a bench-\nmark for multi-target tracking. arXiv:1504.01942, 2015. 6\n[25] Chao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li,\nXiyong Ye, and Jianxiao Zou. Rethinking the competition\nbetween detection and reid in multi-object tracking. arXiv\npreprint arXiv:2010.12138, 2020. 6, 7\n[26] Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Yuwen\nXiong, Rui Hu, and Raquel Urtasun. Polytransform: Deep\npolygon transformer for instance segmentation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9131–9140, 2020. 2\n[27] Weiyao Lin, Huabin Liu, Shizhan Liu, Yuxi Li, Guo-Jun Qi,\nRui Qian, Tao Wang, Nicu Sebe, Ning Xu, Hongkai Xiong,\net al. Human in events: A large-scale benchmark for human-\ncentric video analysis in complex events. arXiv preprint\narXiv:2005.04490, 2020. 6\n[28] Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang,\nWei Liu, and Tae-Kyun Kim. Multiple object tracking: A\nliterature review. Artiﬁcial Intelligence, page 103448, 2020.\n1\n[29] Santiago Manen, Michael Gygli, Dengxin Dai, and Luc\nVan Gool. Pathtrack: Fast trajectory annotation with path\nsupervision. In Proceedings of the IEEE International Con-\nference on Computer Vision, pages 290–299, 2017. 6\n[30] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and\nChristoph Feichtenhofer. Trackformer: Multi-object track-\ning with transformers. arXiv preprint arXiv:2101.02702 ,\n2021. 1, 2, 7\n[31] A. Milan, L. Leal-Taix ´e, I. Reid, S. Roth, and K.\nSchindler. MOT16: A benchmark for multi-object tracking.\narXiv:1603.00831, 2016. 6, 7\n[32] Anton Milan, Seyed Hamid Rezatoﬁghi, Anthony R Dick,\nIan D Reid, and Konrad Schindler. Online multi-target track-\ning using recurrent neural networks. In AAAI, 2017. 2\n[33] Anton Milan, Konrad Schindler, and Stefan Roth. Multi-\ntarget tracking by discrete-continuous energy minimization.\nTPAMI, 2016. 1\n[34] Peter Ondruska and Ingmar Posner. Deep tracking: Seeing\nbeyond seeing using recurrent neural networks. In AAAI,\n2016. 2\n[35] Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, and Cewu Lu.\nTubetk: Adopting tubes to track multi-object in a one-step\ntraining model. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , June\n2020. 6, 7\n[36] Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu,\nYabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue\nHuang, and Yanwei Fu. Chained-tracker: Chaining paired at-\ntentive regression results for end-to-end joint multiple-object\ndetection and tracking. In European Conference on Com-\nputer Vision, pages 145–161. Springer, 2020. 6\n[37] Hamed Pirsiavash, Deva Ramanan, and Charless C Fowlkes.\nGlobally-optimal greedy algorithms for tracking a variable\nnumber of objects. In CVPR, 2011. 1\n[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: towards real-time object detection with region\nproposal networks. In NIPS, 2015. 7\n[39] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,\nand Carlo Tomasi. Performance measures and a data set for\nmulti-target, multi-camera tracking. In European conference\non computer vision, pages 17–35. Springer, 2016. 6\n[40] Amir Sadeghian, Alexandre Alahi, and Silvio Savarese.\nTracking the untrackable: Learning to track multiple cues\nwith long-term dependencies. arXiv:1701.01909, 2017. 6\n[41] Chaobing Shan, Chunbo Wei, Bing Deng, Jianqiang Huang,\nXian-Sheng Hua, Xiaoliang Cheng, and Kewei Liang.\nFgagt: Flow-guided adaptive graph tracking. arXiv preprint\narXiv:2010.09015, 2020. 6\n[42] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu,\nXiangyu Zhang, and Jian Sun. Crowdhuman: A bench-\nmark for detecting human in a crowd. arXiv preprint\narXiv:1805.00123, 2018. 6\n[43] Xinchu Shi, Haibin Ling, Junliang Xing, and Weiming Hu.\nMulti-target tracking by rank-1 tensor approximation. In\nCVPR, 2013. 2\n[44] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao,\nXinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, and\nPing Luo. Transtrack: Multiple-object tracking with trans-\nformer. arXiv preprint arXiv:2012.15460, 2020. 1, 2, 6, 7\n[45] ShiJie Sun, Naveed Akhtar, HuanSheng Song, Ajmal Mian,\nand Mubarak Shah. Deep afﬁnity network for multiple object\ntracking. IEEE transactions on pattern analysis and machine\nintelligence, 43(1):104–119, 2019. 7\n[46] Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, and Bernt\nSchiele. Multi-person tracking by multicut and deep match-\ning. In ECCVw, pages 100–111. Springer, 2016. 2\n[47] Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, and Bernt\nSchiele. Multiple people tracking by lifted multicut and per-\nson re-identiﬁcation. In CVPR, 2017. 2, 6\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017. 4\n[49] Xinchao Wang, Engin T ¨uretken, Francois Fleuret, and Pascal\nFua. Tracking interacting objects using intertwined ﬂows.\nTPAMI, 2016. 1\n[50] Yongxin Wang, Xinshuo Weng, and Kris Kitani. Joint detec-\ntion and multi-object tracking with graph neural networks.\narXiv preprint arXiv:2006.13164, 2020. 6, 7\n[51] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple\nonline and realtime tracking with a deep association metric.\nIn 2017 IEEE international conference on image processing\n(ICIP), pages 3645–3649. IEEE, 2017. 6\n[52] Yu Xiang, Alexandre Alahi, and Silvio Savarese. Learning\nto track: Online multi-object tracking by decision making.\nIn ICCV, 2015. 1\n[53] Yihong Xu, Aljosa Osep, Yutong Ban, Radu Horaud, Laura\nLeal-Taix´e, and Xavier Alameda-Pineda. How to train your\ndeep multi-object tracker. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 6787–6796, 2020. 2, 7\n[54] Fan Yang, Wongun Choi, and Yuanqing Lin. Exploit all the\nlayers: Fast and accurate cnn object detector with scale de-\npendent pooling and cascaded rejection classiﬁers. In CVPR,\n2016. 7\n[55] Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, and Shuai Yi.\nSpatio-temporal graph transformer networks for pedestrian\ntrajectory prediction. In European Conference on Computer\nVision, pages 507–523. Springer, 2020. 2\n[56] Amir Roshan Zamir, Afshin Dehghan, and Mubarak Shah.\nGmcp-tracker: Global multi-object tracking using general-\nized minimum clique graphs. In ECCV. 2012. 2\n[57] Li Zhang, Yuan Li, and Ramakant Nevatia. Global data as-\nsociation for multi-object tracking using network ﬂows. In\nCVPR, 2008. 2\n[58] Yang Zhang, Hao Sheng, Yubin Wu, Shuai Wang, Wei Ke,\nand Zhang Xiong. Multiplex labeling graph for near-online\ntracking in crowded scenes.IEEE Internet of Things Journal,\n7(9):7892–7902, 2020. 7\n[59] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\nand Wenyu Liu. Fairmot: On the fairness of detection and\nre-identiﬁcation in multiple object tracking. arXiv e-prints,\npages arXiv–2004, 2020. 6, 7\n[60] Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng,\nYi Yang, and Jan Kautz. Joint discriminative and genera-\ntive learning for person re-identiﬁcation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2138–2147, 2019. 8\n[61] Xingyi Zhou, Vladlen Koltun, and Philipp Kr ¨ahenb¨uhl.\nTracking objects as points. InEuropean Conference on Com-\nputer Vision, pages 474–490. Springer, 2020. 2, 7\n[62] Ji Zhu, Hua Yang, Nian Liu, Minyoung Kim, Wenjun Zhang,\nand Ming-Hsuan Yang. Online multi-object tracking with\ndual matching attention networks. In ECCV, 2018. 2"
}