{
  "title": "Health system-scale language models are all-purpose prediction engines",
  "url": "https://openalex.org/W4379769651",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4287909325",
      "name": "Jiang, Lavender Yao",
      "affiliations": [
        "New York University",
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Liu, Xujin Chris",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Nejatian, Nima Pour",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4286943956",
      "name": "Nasir-Moin, Mustafa",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A2095169902",
      "name": "Wang Duo",
      "affiliations": [
        "Predictive Science (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4225881272",
      "name": "Abidin, Anas",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3036935244",
      "name": "Eaton Kevin",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Riina, Howard Antony",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Laufer, Ilya",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Punjabi, Paawan",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Miceli, Madeline",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Kim, Nora C.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Orillac, Cordelia",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Schnurman, Zane",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Livia, Christopher",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4309292908",
      "name": "Weiss, Hannah",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4309292909",
      "name": "Kurland, David",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Neifert, Sean",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Dastagirzada, Yosef",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A2613621714",
      "name": "Kondziolka, Douglas",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4309292907",
      "name": "Cheung, Alexander T. M.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4221608591",
      "name": "Yang, Grace",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2104073621",
      "name": "Cao Ming",
      "affiliations": [
        "New York University",
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4307323907",
      "name": "Flores, Mona",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4378230282",
      "name": "Costa, Anthony B",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3187378300",
      "name": "Aphinyanaphongs, Yindalon",
      "affiliations": [
        "NYU Langone Health",
        "Predictive Science (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2753220617",
      "name": "Cho, Kyunghyun",
      "affiliations": [
        "New York University",
        "Canadian Institute for Advanced Research",
        "Courant Institute of Mathematical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4384614816",
      "name": "Oermann, Eric Karl",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4287909325",
      "name": "Jiang, Lavender Yao",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Liu, Xujin Chris",
      "affiliations": [
        "NYU Langone Health",
        "Tân Tạo University"
      ]
    },
    {
      "id": null,
      "name": "Nejatian, Nima Pour",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4286943956",
      "name": "Nasir-Moin, Mustafa",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A2095169902",
      "name": "Wang Duo",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4225881272",
      "name": "Abidin, Anas",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3036935244",
      "name": "Eaton Kevin",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Riina, Howard Antony",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Laufer, Ilya",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Punjabi, Paawan",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Miceli, Madeline",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Kim, Nora C.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Orillac, Cordelia",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Schnurman, Zane",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Livia, Christopher",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4309292908",
      "name": "Weiss, Hannah",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4309292909",
      "name": "Kurland, David",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Neifert, Sean",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": null,
      "name": "Dastagirzada, Yosef",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A2613621714",
      "name": "Kondziolka, Douglas",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4309292907",
      "name": "Cheung, Alexander T. M.",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A4221608591",
      "name": "Yang, Grace",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2104073621",
      "name": "Cao Ming",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4307323907",
      "name": "Flores, Mona",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4378230282",
      "name": "Costa, Anthony B",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3187378300",
      "name": "Aphinyanaphongs, Yindalon",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A2753220617",
      "name": "Cho, Kyunghyun",
      "affiliations": [
        "Canadian Institute for Advanced Research",
        "Courant Institute of Mathematical Sciences",
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4384614816",
      "name": "Oermann, Eric Karl",
      "affiliations": [
        "New York University",
        "NYU Langone Health"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3119527628",
    "https://openalex.org/W3136933888",
    "https://openalex.org/W2982580298",
    "https://openalex.org/W3130937151",
    "https://openalex.org/W2136065216",
    "https://openalex.org/W7075659476",
    "https://openalex.org/W2060123467",
    "https://openalex.org/W1970598617",
    "https://openalex.org/W2964696298",
    "https://openalex.org/W2923106228",
    "https://openalex.org/W2911462778",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W4247943214",
    "https://openalex.org/W4293242440",
    "https://openalex.org/W2000445173",
    "https://openalex.org/W2526656344",
    "https://openalex.org/W3157643309",
    "https://openalex.org/W4309289584",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W2005430761",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2129997990",
    "https://openalex.org/W2165037395",
    "https://openalex.org/W4245267204",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W3172260675",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W6846896813",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2883265831",
    "https://openalex.org/W2107978811",
    "https://openalex.org/W2062989416"
  ],
  "abstract": "Abstract Physicians make critical time-constrained decisions every day. Clinical predictive models can help physicians and administrators make decisions by forecasting clinical and operational events. Existing structured data-based clinical predictive models have limited use in everyday practice owing to complexity in data processing, as well as model development and deployment 1–3 . Here we show that unstructured clinical notes from the electronic health record can enable the training of clinical language models, which can be used as all-purpose clinical predictive engines with low-resistance development and deployment. Our approach leverages recent advances in natural language processing 4,5 to train a large language model for medical language (NYUTron) and subsequently fine-tune it across a wide range of clinical and operational predictive tasks. We evaluated our approach within our health system for five such tasks: 30-day all-cause readmission prediction, in-hospital mortality prediction, comorbidity index prediction, length of stay prediction, and insurance denial prediction. We show that NYUTron has an area under the curve (AUC) of 78.7–94.9%, with an improvement of 5.36–14.7% in the AUC compared with traditional models. We additionally demonstrate the benefits of pretraining with clinical text, the potential for increasing generalizability to different sites through fine-tuning and the full deployment of our system in a prospective, single-arm trial. These results show the potential for using clinical language models in medicine to read alongside physicians and provide guidance at the point of care.",
  "full_text": "Nature | Vol 619 | 13 July 2023 | 357\nArticle\nHealth system-scale language models are \nall-purpose prediction engines\nLavender Yao Jiang1,2, Xujin Chris Liu1,3, Nima Pour Nejatian4, Mustafa Nasir-Moin1, \nDuo Wang5, Anas Abidin4, Kevin Eaton6, Howard Antony Riina1, Ilya Laufer1, Paawan Punjabi6, \nMadeline Miceli6, Nora C. Kim1, Cordelia Orillac1, Zane Schnurman1, Christopher Livia1, \nHannah Weiss1, David Kurland1, Sean Neifert1, Yosef Dastagirzada1, Douglas Kondziolka1, \nAlexander T . M. Cheung1, Grace Yang1,2, Ming Cao1,2, Mona Flores4, Anthony B. Costa4, \nYindalon Aphinyanaphongs5,7, Kyunghyun Cho2,8,9,10 & Eric Karl Oermann1,2,11 ✉\nPhysicians make critical time-constrained decisions every day. Clinical predictive \nmodels can help physicians and administrators make decisions by forecasting clinical \nand operational events. Existing structured data-based clinical predictive models \nhave limited use in everyday practice owing to complexity in data processing, as  \nwell as model development and deployment1–3. Here we show that unstructured \nclinical notes from the electronic health record can enable the training of clinical \nlanguage models, which can be used as all-purpose clinical predictive engines  \nwith low-resistance development and deployment. Our approach leverages recent \nadvances in natural language processing\n4,5 to train a large language model for medical \nlanguage (NYUTron) and subsequently fine-tune it across a wide range of clinical and \noperational predictive tasks. We evaluated our approach within our health system  \nfor five such tasks: 30-day all-cause readmission prediction, in-hospital mortality \nprediction, comorbidity index prediction, length of stay prediction, and insurance \ndenial prediction. We show that NYUTron has an area under the curve (AUC) of  \n78.7–94.9%, with an improvement of 5.36–14.7% in the AUC compared with traditional \nmodels. We additionally demonstrate the benefits of pretraining with clinical text,  \nthe potential for increasing generalizability to different sites through fine-tuning and \nthe full deployment of our system in a prospective, single-arm trial. These results \nshow the potential for using clinical language models in medicine to read alongside \nphysicians and provide guidance at the point of care.\nPhysicians make difficult decisions every day requiring the integra-\ntion of a tremendous amount of information. The information needed \nto make these medical decisions is scattered across various records, \nfor example, a patient’s medical history and laboratory and imaging \nreports. When physicians perform their work, however, all of this infor-\nmation is ultimately integrated into the notes written by physicians to \ndocument and summarize patient care.\nClinical predictive models are frequently derived from rules that have \nexisted for decades6–9, as well as from machine learning methods10–12, \nwith most relying on structured inputs pulled from the electronic health \nrecord (EHR) or direct clinician inputs. This reliance on structured \ninputs introduces complexity in data processing, as well as in model \ndevelopment and deployment, which in part is responsible for the \noverwhelming majority of medical predictive algorithms being trained, \ntested and published, yet never deployed to assess their impact on \nreal-world clinical care. This is frequently referred to as the ‘last-mile \nproblem’ (refs. 1–3).\nOne of the most exciting recent developments in modern artificial \nintelligence (AI) research is large language models (LLMs). These mas-\nsive neural networks (with millions or even billions of parameters) have \nbeen shown to obtain impactful results on a wide range of problems that \nrely on the reading and interpretation of human language. Several styles \nof LLMs have been developed over the past few years, broadly ranging \nfrom encoder models (such as BERT4) to decoder models (such as GPT3; \nref. 5). We theorized that LLMs could potentially solve the last-mile \nproblem in medical predictive analytics by simply reading the notes \nwritten by physicians, thereby immediately accessing a comprehensive \ndescription of a patient’s medical state to provide decision support at \nthe point of care across a wide range of clinical and operational tasks.\nHere we present our results from developing, evaluating, deploying \nand prospectively assessing NYUTron, an LLM-based system that can \nintegrate in real time with clinical workflows centred around writing \nnotes and placing electronic orders. Our approach relies on the fact that \nall clinically useful data and medical professionals’ decision-making \nhttps://doi.org/10.1038/s41586-023-06160-y\nReceived: 14 October 2022\nAccepted: 2 May 2023\nPublished online: 7 June 2023\nOpen access\n Check for updates\n1Department of Neurosurgery, NYU Langone Health, New York, NY, USA. 2Center for Data Science, New York University, New York, NY, USA. 3Electrical and Computer Engineering, Tandon \nSchool of Engineering, New York, NY, USA. 4NVIDIA, Santa Clara, CA, USA. 5Predictive Analytics Unit, NYU Langone Health, New York, NY, USA. 6Department of Internal Medicine, NYU Langone \nHealth, New York, NY, USA. 7Department of Population Health, NYU Langone Health, New York, NY, USA. 8Prescient Design, Genentech, New York, NY, USA. 9Courant Institute of Mathematical \nSciences, New York University, New York, NY, USA. 10Canadian Institute for Advanced Research, Toronto, Ontario, Canada. 11Department of Radiology, NYU Langone Health, New York, NY, USA. \n✉e-mail: eric.oermann@nyulangone.org\n358 | Nature | Vol 619 | 13 July 2023\nArticle\nprocesses can be found as structured or unstructured text in the EHR \n(for example, as notes, laboratory results and reports on studies). Our \napproach leverages recent advances in natural language processing \nthat suggest that sufficiently scaled, self-supervised LLMs can out -\nperform strongly supervised approaches on non-medical predictive \ntasks4,5,13. We investigate our hypothesis in the NYU Langone Health \nSystem (‘NYU Langone’), a large multi-borough hospital system with \na diverse patient population in New York, with 4 urban hospitals and \n350 outpatient sites. We assess NYUTron on a battery of five tasks, \nincluding three clinical and two operational tasks (30-day all-cause \nreadmission prediction, in-hospital mortality prediction, comorbidity \nindex prediction, length of stay (LOS) prediction and insurance denial \nprediction) and provide a detailed analysis of our 30-day readmission \ntask to look at questions of data efficiency, generalizability, deployabil-\nity and potential clinical impact. By rethinking all of medical predictive \nanalytics (see Supplementary Information section 1.1 for previous \nworks) as a natural language processing problem, we show that it is \npossible to use LLMs as universal prediction engines for a wide range \nof medical predictive tasks.\nLanguage model-based clinical prediction\nOur language model-based approach has four steps: data collection, \npretraining, fine-tuning and deployment. In the first step (Fig. 1a), we \ncollected a vast set of unlabelled clinical notes and five task-specific \nlabelled clinical notes from the NYU Langone EHR. Unlike other studies, \nour datasets come from the entire hospital system with a diverse patient \npopulation from different clinical departments. Our large unlabelled \ndataset, ‘NYU Notes’ , comprises 7.25 million clinical notes (for exam-\nple, radiographic reads, history and physicals) from 387,144 patients \nacross four hospitals, resulting in a 4.1 billion-word corpus curated \nfrom January 2011 to May 2020. Each one of our labelled fine-tuning \nsets contains 1–10 years of inpatient clinical notes (55,791–413,845 \npatients, 51–87 million words) with task-specific labels (2–4 classes). \nSee Extended Data Table 1 for dataset statistics.\nIn the second and third steps (Fig. 1b,c), we pretrained and fine-tuned \nan LLM for each downstream task using a bidirectional encoder model \nknown as BERT (Bidirectional Encoder Representation with Trans -\nformer) and a masked language modelling (MLM) objective on the \nNYU Notes dataset11 until the validation loss plateaued. The MLM objec-\ntive randomly masks words or subwords in clinical notes and trains \nthe language model to fill in the masked word correctly. Next, using \nthe fine-tuning dataset, we fine-tuned the pretrained model (termed \n‘NYUTron’) to predict the task label using the relationships learned in \npretraining with clinical notes.\nIn the fourth step (Fig.  1d), we deployed our best model to a \nhigh-performance inference engine, NYUTriton, that interfaces with \nthe NYU Langone EHR. Deployment enabled real-time LLM-guided \ninference at the point of care. In a single-arm, non-interventional, \nprospective trial, we validated NYUTron’s performance on 30-day \nreadmission prediction in a real-world environment and assessed its \npotential clinical impacts.\nOverall performance on five tasks\nT o assess the breadth of NYUTron’s applicability, we evaluated \nNYUTron’s performance on five tasks retrospectively. We trained \nwith the full dataset and evaluated performance with two test sets:  \n(1) a random test set (clinical notes sampled from the same time as the \ntraining data) and (2) a temporal test set (clinical notes sampled from \nthe future of the training data). The temporal test set more closely \nresembles the deployment scenario, in which inference data come \nfrom the future of the training data. Our battery of tasks consisted \nof three clinical tasks and two operational tasks, as shown in Fig. 2a. \nWe compared NYUTron against structured baselines, which forward \nstructured features used by traditional clinical predictive models into \nan extreme gradient-boosted tree14 model.\nNYUTron is capable of being extended to multiple clinical and \noperational tasks. Figure  2b and Fig.  2c show that, on prediction \ntasks (in-hospital mortality, readmission, LOS and insurance denial), \nNYUTron had an area under the curve (AUC) of 78.7–94.9%, with an \nimprovement of 5.36–14.7% in AUC from traditional clinical predictive \nmodels. On the comorbidity index imputation task, NYUTron had a \nmedian AUC of 89.4% ± 0.275%. We first present our results across four \nof the tasks and conclude with a focused look at readmission prediction \nthat addresses questions of data efficiency, model generalizability and \ndeployment in a real-world environment.\nNYUTron is capable of predicting risk of in-hospital mortality on \nadmission and imputing a comorbidity index. The task of in-hospital \nmortality prediction was to estimate (at admission) the likelihood of a \npatient’s death during the present inpatient encounter. Figure 2b shows \nthat, for in-hospital mortality prediction, NYUTron had a median AUC \nof 94.9% ± 0.168%, with a 7.43% improvement from its structured base-\nline based on Simplified Acute Physiology Score (SAPS2)15 and Acute \nPhysiology and Chronic Health Evaluation (APACHE2)16 features such \nas age and mean heart rate. The task of comorbidity index imputation \nwas to predict (at admission) the Charlson comorbidity index (CCI)17 \nwith no available structured features for chronic diseases. We framed \nthis as a data imputation problem, as 22% of our dataset lacked CCI \nscores and this was a known area for documentation improvement \na  Data collection\nLangone EHR NYU Notes\nNYU /f_ine-tuning\nTask-speci/f_ic\nlabels\nb  Pretraining\nLanguage model\nMasked language modelling\nFill in [MASK]:\nA 39-year-old [MASK] was brought in by\nambulance.\nc  Fine-tuning\nPretrained model\nFine-tuning\n( 0.6 , 0.4 )\n(  )\nPredicted\np(label)\nGround\ntruth Loss\nd  Deployment\nFine-tuned model Inference engine\nEmail alertHospital EHR\nPhysician\nWeight update\nClinical\nnotes\nPatient\nNYU Notes\nClinical\nnotes\nClinical\nnotes  ,\nTask-speci/f_ic\nlabels(  ) ,\nNYU /f_ine-tuning\nClinical\nnotes\nClinical\nnotes\nFig. 1 | Overview of the language model-based approach for clinical \nprediction.  a, We queried the NYU Langone EHR for two types of datasets.  \nThe pretraining dataset, NYU Notes, contains 10 years of inpatient clinical \nnotes (387,144 patients, 4.1 billion words). There are five fine-tuning datasets. \nEach contains 1–10 years of inpatient clinical notes (55,791–413,845 patients, \n51–87 million words) with task-specific labels (2–4 classes). b , We pretrained a \n109 million-parameter BERT-like LLM, termed NYUTron, on the entire EHR \nusing an MLM task to create a pretrained model for medical language \ncontained within the EHR. c , We subsequently fine-tuned the pretrained model \non specific tasks (for example, 30-day all-cause readmission prediction) and \nvalidated it on held-out retrospective data. d , Lastly, the fine-tuned model was \ncompressed into an accelerated format and loaded into an inference engine, \nwhich interfaces with the NYU Langone EHR to read discharge notes when they \nare signed by treating physicians.\nNature | Vol 619 | 13 July 2023 | 359\n(see Supplementary Information section 2.3 for more context). We \ndiscretized the index into four bins according to the original paper’s \ngrades of severity (0, none; 1–2, mild; 3–4, moderate; ≥5, severe).  \nFigure 2b shows that, on comorbidity imputation, NYUTron had a \nmedian AUC of 89.4% ± 0.275% and 88% precision when identifying \npatients whose CCI score was 0.\nNYUTron can also be used for operational endpoints and to predict \nin-patient LOS and insurance claim denial on admission. The task of \nLOS prediction was to predict (at admission) the likely range of days \na patient would stay in the hospital. We discretized LOS into four bins \n(0–25% quantile, 25–50% quantile, 50–75% quantile, >75% quantile). \nFigure  2c shows that, for LOS prediction, NYUTron had a median \none-versus-rest (OVR) AUC of 78.7% ± 0.179%, with a 12.3% improvement \nfrom the structured baseline, which used an available subset of ‘Lisbon \nPortugal’ features18. The task of insurance claim denial prediction was \nto predict (at admission) whether the insurance claims submitted for \nan encounter would be accepted or initially denied. Figure 2c shows \nthat, for insurance denial prediction, NYUTron had a median AUC of \n87.2% ± 0.246%, with a 14.7% improvement from the structured baseline, \nwhich used an available subset of ‘claim form’ features19 such as age \nand insurance provider. NYUTron is also capable of predicting differ-\nent types of denials from both admission notes and discharge notes \nwith similar performance (Supplementary Information section 2.2).\nDetailed analysis on readmission\nT o better understand NYUTron’s performance, we carried out a \ndetailed analysis of 30-day all-cause readmission prediction. The task \nof readmission prediction is to predict (at discharge) the likelihood of \na patient coming back to the hospital within 30 days and is a well-studied \nproblem in the medical informatics literature (see Supplementary \nInformation section 2.1 for more details on the readmission predic-\ntion task). Figure 2b shows that, for 30-day all-cause readmission pre-\ndiction, NYUTron had a median AUC of 79.87% ± 0.168%, with a 5.36% \nimprovement from its structured baseline, which used LACE20 features \n(a mnemonic for LOS, acuity of admission, Charlson comorbidity index \nand number of emergency department visits in the past 6 months). \nWe performed five additional evaluations in both retrospective and \nprospective settings: (1) a human comparison with six attending phy-\nsicians for prediction of readmission for 20 patient cases sampled \nfrom a random split, (2) a study of NYUTron’s scaling properties with \nrespect to data in which NYUTron and other models were compared \nusing a different number of fine-tuned data points, (3) an assessment \nof NYUTron’s cross-site generalizability using pretraining, fine-tuning \nand test data from different locations, (4) a prospective, single-arm, \nnon-interventional study to evaluate NYUTron’s deployability and  \n(5) a qualitative evaluation by a physician panel of NYUTron’s prospec-\ntive performance to assess clinical impacts.\nRetrospective study of readmission\nOn small samples, NYUTron was competitive with a small group of phy-\nsicians at predicting 30-day readmission. We tested a group of six physi-\ncians at different levels of seniority against NYUTron in a head-to-head \ncomparison to establish a baseline difficulty for predicting 30-day \nall-cause readmission at the time of discharge. Discharge summaries \n(n = 20, including 11 positive cases and 9 negative cases) were sam-\npled from a random split and uploaded to an online evaluation plat-\nform. Median physician performance was worse than that of NYUTron \n(Fig. 3a). For physicians and NYUTron, the median false positive rate \n(FPR) was 11.11%, whereas the median true positive rate (TPR) was 50% \nfor physicians compared with 81.82% for NYUTron. Physicians had a \nmedian F1 score of 62.8% and substantial variance of 22.2% compared \nwith NYUTron, which had a median F1 score of 77.8%.\nThe random split does not resemble the deployment scenario, in \nwhich the test data come from the future of the training data. We there-\nfore created a temporal split to simulate deployment and observed a \nmeaningful difference in test statistics compared with the random \nsplit (the random test AUC was 84.13%, whereas the temporal test AUC \nwas 80.2%), confirming the importance of this second testing phase \n(further comparison in Extended Data Fig. 1).\nNYUTron is competitive with traditional models and other LLMs. \nWe evaluated the effectiveness of NYUTron by comparing its test \nperformance on the temporal split against that of a traditional model \na\nClinical\ntask\nOperational\ntask\nPhysician\nAdmin\nIn-hospital mortality prediction\nHow likely is the patient to die in the hospital before discharge?\nBinned comorbidity index imputation\nWithout structured ICDS, how sick/chronically ill is the patient?\n30-day all-cause readmission prediction\nHow likely is the patient to come back within 30 days of discharge?\nBinned LOS prediction\nHow long will the patient stay in the hospital?\nInsurance denial prediction\nHow likely is the patient's insurance claim to be denied?\nb\nPredicted CCI\nTrue CCI\n0.88 0.10 0.01 0.01\n0.33 0.56 0.07 0.04\n0.18 0.31 0.08\n0.11 0.23 0.14 0.52\nIn-hospital\nmortality\nReadmission\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nTemporal (OVR) AUC\nComorbidity imputation\nInsurance denial LOS\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nTemporal (OVR) AUC\nClinical notes + NYUTron\nStructured features baseline\nc\nClinical notes + NYUTron\nStructured features baseline\n0.44 \n0 1–2 3–4 ≥5\n0\n1–2\n3–4\n≥5\nFig. 2 | Overall temporal test performance across five tasks. a, The five  \ntasks include three clinical tasks and two operational tasks. b , On readmission \nprediction, NYUTron had a median AUC of 79.9% ± 0.168% with a 5.36% \nimprovement. On in-hospital mortality prediction, NYUTron had a median  \nAUC of 94.9% ± 0.168% with a 7.43% improvement. On comorbidity index \nimputation, NYUTron had an OVR median AUC of 89.4% ± 0.275%. A confusion \nmatrix is shown on the right. c , On binned LOS prediction, NYUTron had a \nmedian AUC of 78.7% ± 0.179% with a 12.3% improvement from the structured \nbaseline. On insurance denial prediction, NYUTron had a median AUC of \n87.2% ± 0.246% with a 14.7% improvement. For b,c, the height of the error bar is \nthe median AUC and the half-width of the error bar is 1 s.d. The grey points are \nindividual data points from n = 5 experiments using distinct random seeds.\n360 | Nature | Vol 619 | 13 July 2023\nArticle\nand four different types of LLMs. NYUTron had the highest AUC \nwhen fine-tuned with the full dataset (Fig. 3b), with a median AUC of \n79.87% ± 0.17%, which was similar to the clinical+web-wiki+bio AUC \nof 80.14% ± 0.26%. Compared with LLMs pretrained with non-clinical \ntext (web-wiki+bio and web-wiki), NYUTron’s median AUC was 2.37% to \n3.23% higher. Compared with the traditional model that uses structured \nfeatures (lace+xgb), NYUTron had a 5.36% higher AUC. Compared with \na model using traditional natural language processing (NLP) embed-\nding (tf-idf+xgb), NYUTron had a 12.8% higher median AUC (Extended \nData Fig. 2a).\nAn LLM trained on unstructured clinical notes better scales with data \nthan traditional structured models. Compared with lace+xgb, NYUTron \nbenefits from an increasing amount of labelled examples and achieved \na better AUC when fine-tuned with the full dataset. Figure 3b shows \nthat lace+xgb (dashed yellow line) and NYUTron (solid green line) had \nsimilar AUCs at 100 and 1,000 examples. However, NYUTron’s AUC con-\nsistently improved with more examples whereas lace+xgb’s AUC started \nto plateau (from 100 to 1,000 examples, NYUTron’s AUC increased by \n7.27% whereas that of lace+xgb increased by 3.98%; from 10,000 to \n392,336 examples, NYUTron’s AUC increased by 2.15% whereas that \nof lace+xgb increased by 0.63%). With the full fine-tuning dataset, \nNYUTron had a 7.04% higher AUC than lace+xgb.\nPretraining on a large amount of unlabelled clinical notes contrib-\nutes to performance. Compared with the randomly initialized LLM \n(random-init), NYUTron learns to generalize better from fewer exam-\nples. Figure 3b shows that, whereas NYUTron needed 10,000 examples \nto achieve an AUC of around 75%, random-init needed 100,000 exam-\nples. We also observed a similar trend in another clinical prediction \ntask: NYUTron performed better than the random-init model (36.83% \nhigher F1 score) and the non-clinically pretrained models (2.06% to \n3.73% higher F1 score) on the clinical named entity recognition (NER) \ntask from the 2012 i2b2 challenge (Extended Data Fig. 2b).\nIt is beneficial to match the domain of the pretraining corpus and \nthe domain of the fine-tuning corpus. Figure 3b shows three pieces \nof evidence: LLMs pretrained on non-clinical text (web-wiki and \nweb-wiki+bio) had similar performance as random-init. A separate \nLLM, web-wiki+bio+clinical, had similar performance as NYUTron. \nThird, compared with LLMs pretrained on non-clinical text (web-wiki \nand web-wiki+bio), clinically pretrained LLMs (NYUTron and \nweb-wiki+bio+clinical) learned to generalize better from fewer exam-\nples. See Extended Data Fig. 3 for comparison of the pretraining corpus.\nHaving a close domain match during pretraining is particularly \nbeneficial in the low-data setting during fine-tuning. We compared \ntwo language models that were pretrained on clinical text from \ndifferent hospital systems, NYUTron (NYU Langone Health) and \nweb-wiki+bio+clinical (University of Florida). Figure 3b shows that, \nat 1,000 examples, NYUTron (the in-domain model) had a higher AUC \nfor NYU Langone readmission prediction than web-wiki+bio+clinical \n(the out-of-domain model). Notably, NYUTron’s advantage disappeared \nas the number of fine-tuning examples increased, suggesting that suf-\nficient in-domain fine-tuning can adapt models that were pretrained \nout of domain.\nClinical language models show generalizability to different sites \nthrough local fine-tuning. T o investigate the robustness of NYUTron \nacross clinical environments, we chose two hospitals that are geo -\ngraphically separated within the NYU Langone Health System. For \nbrevity, we refer to Tisch Hospital in Manhattan as ‘Manhattan’ , NYU \nLangone Hospital–Brooklyn as ‘Brooklyn’ and all four hospitals within \nthe NYU Langone Health System (Manhattan, Brooklyn, NYU Langone \nOrthopedic Hospital and NYU Langone Hospital–Long Island) as ‘all \nsites’ . We considered three LLMs pretrained on different sites: the first \none was pretrained in Manhattan, the second one was pretrained in \nBrooklyn and the third one was pretrained on all sites. For each of the \npretrained LLMs, we fine-tuned the LLM with a readmission dataset \nfrom either Manhattan or Brooklyn. Finally, we asked the fine-tuned \nLLM to predict readmission on the basis of discharge notes from either \nManhattan or Brooklyn. Figure 3c,d shows that the LLM pretrained on \nall sites had the best performance on both ‘test Manhattan’ and ‘test \nBrooklyn’ . For all the LLMs, fine-tuning with the local dataset (‘fine-tune \nManhattan/Brooklyn’) led to a higher test AUC at the test site (‘test \n0 0.2 0.4 0.6 0.8 1.0\nFPR\n0\n0.2\n0.4\n0.6\n0.8\n1.0TPR\nPhysician\nNYUtron\nMedian physician\nNYUtron-AUC\n102\nNumber of /f_ine-tuning examples\n0.60\n0.65\n0.70\n0.75\n0.80Temporal test AUROC\nFirst time for NYUTron to reach ~75% AUC\nFirst time for random-init to reach ~75% AUC\nab\nc Test Manhattan Test Brooklynd\n103 104 105\nManhattan\n80.70% ± 0.32%81.73% ± 0.26%\n78.6% ± 0.60%80.91% ± 0.34%\nBrooklyn\nManhattan\nBrooklyn\n80.99% ± 0.17%81.61% ± 0.32%All sites\nFine-tune\nPretrain\nManhattan Brooklyn\nManhattan\nBrooklyn\nAll sites\nFine-tune\nPretrain\n80.55% ± 0.29%78.64% ± 0.27%\n79.65% ± 0.69%78.74% ± 0.56%\n81.24% ± 0.55%79.14% ± 0.92%\nNYUTron (ours, clinical)\nweb-wiki+bio+clinical\nweb-wiki+bio\nweb-wiki\nrandom-init\nlace+xgb\nData type\nUnstructured\nStructured\nFig. 3 | Retrospective study of NYUTron’s readmission prediction.  a, On 20 \ncases sampled from a random split, we compared NYUTron’s TPR and FPR with \nthose for six physicians. NYUTron (orange triangles) had a higher TPR and the \nsame FPR when compared with the median physician performance (green \ncircles). The error band for AUC ranges from the minimum to maximum, and \nthe orange crosses indicate TPR and FPR using all possible thresholds. We \nchose NYUTron’s threshold on the basis of validation data. b , Comparison of \nthe temporal test AUCs of different pretrained LLMs with an increasing number \nof fine-tuning examples. For simplicity, we omit the variance and only plot the \nmedian performance of five trials. Differences in median performance with \n100 and 1,000 examples are less notable because AUCs with sparse fine-tuning \nexamples have high variance (at 100 examples, we had 4.26% to 9.56% variance; \nat 1,000 examples, we had 0.44% to 9.46% variance). AUC variance decreases \nwith more fine-tuning examples. The horizontal dashed line at 0.75 corresponds \nto the threshold for performance. See alternative presentations in Extended \nData Fig. 7. c,d, Temporal test performance of NYUTron using pretraining, \nfine-tuning and test data from different sites. For both the Manhattan and \nBrooklyn tests, the column corresponding to local fine-tuning shows better \nperformance than that with external fine-tuning. Each entry in c ,d is presented \nas the mean ± 1 s.d. for n = 5 experiments using distinct random seeds.\nNature | Vol 619 | 13 July 2023 | 361\nManhattan/Brooklyn’) compared with fine-tuning at another site \n(‘fine-tune Brooklyn/Manhattan’). Therefore, pretraining with data \nfrom all sites and local fine-tuning is the best way to optimize perfor-\nmance. We performed additional analyses that showed that NYUTron is \nable to generalize to a different health system through local fine-tuning \n(Supplementary Information section 4.1 and Extended Data Fig. 4) and \ncompared the robustness of NYUTron and lace+xgb with respect to \ntraining sites (Supplementary Information section 4.2). We also found \nthat NYUTron is sensitive to notes from different clinical departments \nand patients with different demographics and that its performance \nfluctuates over months (Extended Data Figs. 5 and 6). The causes of \nthe discrepancies can be very complex (discussed in Supplementary \nInformation section 4.3) and will be studied in future work.\nProspective study of readmission\nT o assess NYUTron’s performance outside the development environ-\nment, we selected a model on the basis of the retrospective trial results \nand ran a prospective trial from January to April 2022. During this time \nperiod, we deployed NYUTron in an accelerated format and loaded it \ninto an inference engine, which interfaces with the EHR, to read dis-\ncharge notes as they were signed by treating physicians. In this period, \nthere were 29,286 discharged encounters, with 3,271 patients (11.17%) \nreturning within 30 days. NYUTron predicted 2,692 of the 3,271 read-\nmissions (82.30% recall) with 20.58% precision. Figure 4a shows that \nNYUTron had an AUC of 78.70%.\nT o gauge the potential clinical impact, a group of six physicians per-\nformed a qualitative evaluation of 100 randomly sampled readmitted \ncases that were captured by NYUTron following the trial’s conclusion. \nPhysician review suggested that some true positive predictions by \nNYUTron are clinically meaningful, preventable readmissions. Overall, \nreadmitted patients who were predicted to be readmitted were 6.02 \ntimes more likely to die in hospital and stay 2.93 days longer (P < 10−4). \nAs shown in Fig. 4b, 61% of the predicted case were unplanned, and \nthe mean predicted probabilities for these unplanned readmissions \nwere lower than those for planned readmissions (31.9% ± 31.1% versus \n82.1% ± 27.3%; P < 10−4). Among the unplanned readmissions, 19.67% \nof patients experienced an adverse event or death on readmission, \nwith 50% of these events considered preventable by the physician \npanel. From a financial standpoint, 81.9% of the unplanned readmis-\nsions would be penalized according to Centers for Medicare and \nMedicaid Services (CMS) guidelines. Among the penalizable cases, \n54% were considered preventable. Notably, 3 of the 27 preventable \nreadmissions had Clostridioides difficile enterocolitis, a contagious, \nhealthcare-associated bacterial infection that causes 1 in 11 people \nover age 65 to die within 1 month21.\nDiscussion\nWe present our work in developing, training, validating and deploying \nNYUTron, a health system-scale LLM designed and validated for clinical \nuse. We demonstrate NYUTron’s performance on three clinical tasks \n(in-patient mortality prediction, comorbidity index prediction and \nreadmission prediction) and two operational tasks (insurance claim \ndenial prediction and inpatient LOS prediction). We also performed \na detailed analysis of readmission prediction owing to its clinical and \noperational importance and its well-documented history in the medical \ninformatics literature. We view the flexibility of our approach in using \nan encoder architecture (BERT), which relies on only unstructured text \ninputs to generate a single prediction, as being a virtue, and we antici-\npate many future tasks built on this fundamental paradigm to assist with \nmultiple aspects of patient care and automating hospital operations.\nAn ethical consideration in deployment is that physicians and \nadministrators could over-rely on NYUTron’s predictions owing to \nits seamless integration with existing medical workflows, thereby \nleading to undesirable outcomes. Further research is needed to opti-\nmize human–AI interactions, as well as development of standardized \nassessments for sources of bias or other unexpected failure points. \nOngoing work from our group around measuring the similarity between \nlanguage models’ sensitivity patterns and those of physicians through \ntoken-level perturbations of the clinical notes22 is one among many  \nsuch efforts.\nLarge, generative LLMs also present a unique opportunity for inte-\ngration into medical workflows; however, they are highly depend -\nent on user inputs and prompting 23 and are not as easily adapted \nfor automation of basic clinical and operational tasks. The seamless \nintegration into existing medical informatics workflows is a virtue of \nour approach, and we hope that this work presents itself as a flexible \nsolution to the last-mile problem—any structured data algorithm can \nbe reconceptualized and rapidly prototyped within this framework. As \npart of monitoring the impact of such a system on physician behaviour \nand on patients, there should be a level of continuous supervision to \ncapture human–machine interactions, as well as mitigate the risk of \nmodel drift over time. We discuss our implementation of such a system \nin Supplementary Information section 5.\nOur approach of using a smaller (<1 billion parameters) encoder lan-\nguage model trained on highly tailored data represents a marked depar-\nture from the current trend in language model research that focuses \non massive (>1 billion parameters), generative models pretrained on \nlarge, non-specific datasets. Nonetheless, even relatively small LLMs, \na\nb Predicted\nreadmission\nPredicted,\nunplanned\nreadmission\nPredicted,\nunplanned,\npreventable\nreadmission\nPredicted,\nunplanned,\npenalizable\nreadmission\n0 0.2 0.4 0.6 0.8 1.0\nFPR\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nTPR\nFig. 4 | Prospective study of NYUTron’s predictive performance. a, NYUTron \nhad an AUC of 78.70% in a prospective, single-arm, non-interventional trial  \nwith recall of 82.3% and precision of 20.6%. b , A panel of six physicians reviewed \nNYUTron’s results for potential clinical impact. Of 100 readmissions that  \nwere successfully identified by NYUTron, 61% were unplanned readmissions, \n50% would have resulted in a penalty under CMS guidelines and 27% were \npreventable at the time of discharge according to the consensus opinion of the \nmulti-specialty panel of physicians who reviewed cases from the prospective \ntrial. See Supplementary Information section 2.1 for a discussion of the \nreadmission label and the practical significance of the observed performance.\n362 | Nature | Vol 619 | 13 July 2023\nArticle\nsuch as the ones used in this study, require a substantial amount of \ncompute time for pretraining. Our pretraining used 24 NVIDIA A100 \nGPUs with 40 GB of VRAM for 3 weeks, and our fine-tuning used 8 A100 \nGPUs for 6 hours per run. This amount of computation is not commonly \naccessible to research groups, although we note that it is less than \nthat in similar LLM projects routinely pursued by industry research \ngroups and that our results indicate that massive pretraining may not \nbe necessary to obtain highly performant models. Our results show \nthat high-quality datasets for fine-tuning are more valuable than pre-\ntraining, and, on the basis of our experimental results, we recommend \nthat users locally fine-tune an externally pretrained language model \nwhen computational ability is limited. Regarding the choice for the \nexternally pretrained model, we further recommend using a model \npretrained with a large amount of in-domain clinical text, although we \nnote that large, out-of-domain models can be highly performant, par-\nticularly when combined with in-domain fine-tuning. Work with larger \ndecoder-based architectures has also demonstrated a benefit with \nfine-tuning on medical data or prompt tuning with chain of thought, \ninstructions and related techniques24,25, which further emphasizes the \nnecessity of accounting for the domain shift from general to medical \ntext for LLM work in the medical sciences. Although we have not com-\npared these approaches directly (which would require more medical \ntext or fusion with general-domain text for training a compute-optimal \nmodel26), we believe that this could be an interesting future direction \nfor research and that, in the end, approaches combining these different \napproaches to language modelling may prove to be complementary \ndepending on the use case.\nThe ultimate validation of our approach must come from randomized \ncontrolled trials of interventions tied to individual task predictions to \nassess their clinical impact and from user feedback as we continue to \nintegrate NYUTron into health systems. As we plan this within our own \nhealth system, we recommend the consideration of different levels of \nintervention depending on the predicted risk of patients for each task. \nFor instance, for a patient at low risk for 30-day readmission, follow-up \ncalls could be scheduled; for a high-risk patient, care should be taken \nto limit premature discharge. All interventions should be decided on \nwith physician supervision, although many of the operational uses can \nprobably be fully automated.\nIt is a long-standing dream for physicians to have AI assistants observ-\ning care along with them and chiming in with predictions and advice. T o \ntake a step towards this futuristic vision, we trained an LLM, NYUTron, \non the entire EHR of a large healthcare system to read physician notes \nand make several of these predictions across a wide range of clinical and \noperational tasks. We deployed NYUTron in a live healthcare environ-\nment and demonstrate its efficacy at predicting 30-day readmission \nwhile being integrated seamlessly into clinical workflows. We believe \nthat this work opens the door to translating the progress in modern \nnatural language processing and deep learning to improving the \nquality and affordability of healthcare, and we are excited to see what  \ncomes next.\nOnline content\nAny methods, additional references, Nature Portfolio reporting summa-\nries, source data, extended data, supplementary information, acknowl-\nedgements, peer review information; details of author contributions \nand competing interests; and statements of data and code availability \nare available at https://doi.org/10.1038/s41586-023-06160-y.\n1. Roberts, M. et al. Common pitfalls and recommendations for using machine learning to \ndetect and prognosticate for COVID-19 using chest radiographs and CT scans. Nat. Mach. \nIntel. 3, 199–217 (2021).\n2. Kelly, C. J., Karthikesalingam, A., Suleyman, M., Corrado, G. & King, D. Key challenges for \ndelivering clinical impact with artificial intelligence. BMC Med. 17, 195 (2019).\n3. Gaube, S. et al. Do as AI say: susceptibility in deployment of clinical decision-aids. NPJ \nDigit. Med. 4, 31 (2021).\n4. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training of deep bidirectional \ntransformers for language understanding. in Proc. 2019 NAACL: Human Language \nTechnologies (eds Burstein, J., Doran, C. & Solorio, T.) 4171–4186 (Association for \nComputational Linguistics, 2019).\n5. Brown, T. B. et al. Language models are few-shot learners. in Proc. NeurIPS (eds Wallach, \nH. et al.) 1877–1901 (Neural Information Processing Systems, 2020).\n6. Gage, B. F. et al. Selecting patients with atrial fibrillation for anticoagulation: stroke risk \nstratification in patients taking aspirin. Circulation 110, 2287–2292 (2004).\n7. Child, C. G. & Turcotte, J. G. Surgery and portal hypertension. Major Prob. Clin. Surg. 1, \n1–85 (1964).\n8. Pugh, R. N. H., Murray-Lyon, I. M., Dawson, J. L., Pietroni, M. C. & Williams, R. Transection \nof the oesophagus for bleeding oesophageal varices. Br. J. Surg. 60, 646–649 (2005).\n9. Wells, P. et al. Accuracy of clinical assessment of deep-vein thrombosis. Lancet 345, \n1326–1330 (1995).\n10. Tomašev, N. et al. A clinically applicable approach to continuous prediction of future \nacute kidney injury. Nature 572, 116–119 (2019).\n11. Wu, N. et al. Deep neural networks improve radiologists’ performance in breast cancer \nscreening. IEEE TMI 39, 1184–1194 (2020).\n12. Liang, H. et al. Evaluation and accurate diagnoses of pediatric diseases using artificial \nintelligence. Nat. Med. 25, 433–438 (2019).\n13. Kaplan, J. et al. Scaling laws for neural language models. Preprint at https://doi.org/ \n10.48550/arXiv.2001.08361 (2020).\n14. Chen, T. & Guestrin, C. XGBoost: a scalable tree boosting system. in Proc. 2016 SIGKDD \n785–794 (Association for Computing Machinery, 2016).\n15. Le Gall, J.-R. A. New simplified acute physiology score (SAPS II) based on a European/\nNorth American multicenter study. J. Am. Med. Assoc. 270, 2957–2963 (1993).\n16. Knaus, W. A., Draper, E. A., Wagner, D. P. & Zimmerman, J. E. APACHE II: a severity of \ndisease classification system. Crit. Care Med. 13, 818–829 (1985).\n17. Charlson, M. E., Pompei, P., Ales, K. L. & MacKenzie, C. R. A new method of classifying \nprognostic comorbidity in longitudinal studies: development and validation. J. Chron. \nDis. 40, 373–383 (1987).\n18. Caetano, N., Laureano, R. M. S. & Cortez, P. A data-driven approach to predict  \nhospital length of stay—a Portuguese case study. in Proc. 2014 ICEIS (eds Hammoudi,  \nS., Maciaszek, L. & Cordeiro, J.) 407–414 (SCITEPRESS Digital Library, 2014).\n19. Johnson, M., Albizri, A. & Harfouche, A. Responsible artificial intelligence in healthcare: \npredicting and preventing insurance claim denials for economic and social wellbeing.  \nInf. Syst. Front. https://doi.org/10.1007/s10796-021-10137-5 (2021).\n20. van Walraven, C., Wong, J. & Forster, A. J. LACE+ index: extension of a validated index to \npredict early death or urgent readmission after hospital discharge using administrative \ndata. Open Med. 6, 80–90 (2012).\n21. Center for Disease Control. What is C. diff? https://www.cdc.gov/cdiff/what-is.html (2022).\n22. Yang, G. et al. Language model classifier aligns better with physician word sensitivity \nthan XGBoost on readmission prediction. Preprint at https://doi.org/10.48550/\narXiv.2211.07047 (2022).\n23. Perez, E., Kiela, D. & Cho, K. True few-shot learning with language models. in Proc. \nNeurIPS (eds Ranzato, M. et al.) 11054–11070 (Neural Information Processing Systems, \n2021).\n24. Singhal, K. et al. Large language models encode clinical knowledge. Preprint at https://\ndoi.org/10.48550/arXiv.2212.13138 (2022).\n25. Bolton, E. et al. PubMedGPT 2.7B. Technical report. Stanford University Center for Research \non Foundation Models https://crfm.stanford.edu/2022/12/15/pubmedgpt.html (2022).\n26. Hoffmann, J. et al. An empirical analysis of compute-optimal large language model \ntraining. in Proc. NeurIPS (eds Koyejo, S. et al.) 30016–30030 (Neural Information \nProcessing Systems, 2022).\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution \n4.0 International License, which permits use, sharing, adaptation, distribution \nand reproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, \nand indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your \nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, \nvisit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023, corrected publication 2023\nMethods\nPretraining datasets\nNYU Notes. We created this dataset of unlabelled clinical notes directly \nfrom the NYU Langone EHR. The dataset contains 387,144 patients, \n7,247,694 notes and 4,112,249,482 words in total. We built NYU Notes as \nfollows: we wrote structured query language (SQL) scripts to query the \nNYU Langone EHR. We first prototyped the queries with an interactive \nweb-based editor (Cloudera Hue) and then download the query results \nas comma-separated files (CSVs) to NYU Langone’s high-performance \ncomputing cluster. We included notes signed by medical profession-\nals (physicians, residents, physician assistants, nurse practitioners \nand fellows) at Tisch Hospital, NYU Langone Hospital–Brooklyn, NYU \nLangone Hospital–Long Island and NYU Langone Orthopedic Hospital \nfrom 2011 to 2020 (inclusive). We excluded any notes that were derived \nfrom billing, labelled as invalid or empty. We split the notes into three \nsets, training, validation and test sets, with a ratio of 949:50:1. Lastly, we \nmasked tokens with 15% probability to create masked text and labels.\nNYU Notes–Manhattan. We created this dataset of unlabelled clinical \nnotes as the subset of NYU Notes that were written in Tisch Hospital \nin Manhattan. The dataset contains 256,217 patients, 4,342,602 notes \nand 2,381,466,993 words in total.\nNYU Notes–Brooklyn. We created this dataset of unlabelled clinical \nnotes as the subset of NYU Notes that were written in NYU Langone \nHealth–Brooklyn. The dataset contains 104,521 patients, 1,337,352 \nnotes and 1,102,078,012 words in total.\nFine-tuning datasets\nNYU Readmission. We created this dataset of labelled discharge notes \n(with binary labels for readmission) from the NYU Langone EHR. Most \nof the notes from this dataset are a subset of NYU Notes, with additional \ndischarge notes from 2021 for the temporal test. The dataset contains \n413,845 patients, 506,740 notes and 487,395,462 words in total. We \nbuilt this dataset as follows: for each encounter that ended between \nJanuary 2011 and November 2021, we included its discharge note \nwith a binary label for 30-day all-cause readmission. We assigned the  \n‘readmitted’ label if the patient had an admission note within 30 days \nof being discharged. T o focus on modelling acute care readmission, we \nexcluded discharge notes from the rehabilitation, dialysis and palliative \ncare departments because these were not acute care admissions. We \nsplit the dataset into four sets: training, validation, test and temporal \ntest sets. The first three sets were notes from January 2011 to May 2021, \nwith a ratio of 8:1:1. The temporal test set included notes from June to \nDecember 2021. See Extended Data Fig. 8a for a visualization of the \nfour-way split.\nNYU Readmission–Manhattan. We created this dataset of unlabelled \nclinical notes as the subset of notes in the NYU Readmission dataset \nthat were written in Tisch Hospital in Manhattan. The dataset contains \n240,824 patients, 296,519 notes and 253,622,053 words.\nNYU Readmission–Brooklyn. We created this dataset of unlabelled \nclinical notes as the subset of clinical notes from the NYU Readmis-\nsion dataset that were written in NYU Langone Health–Brooklyn. The \ndataset contains 94,653 patients, 113,275 notes and 142,767,957 words.\nNYU Mortality. We created this dataset of history and physical (H&P) \nnotes with binary labels for in-hospital mortality from the NYU Langone \nEHR. Most of the notes from this dataset are a subset of NYU Notes, \nwith additional H&P notes from 2021 for the temporal test. The data-\nset contains 371,922 patients, 469,162 notes and 484,467,141 words in \ntotal. We built this dataset as follows: for each encounter that ended \nbetween January 2011 and November 2021, we included its H&P note \nwith a binary label for in-hospital mortality. We assigned the positive \nlabel if the patient’s discharge disposition was ‘expired’ . We split the \ndataset into four sets: training, validation, test and temporal test sets. \nThe first three sets were notes from January 2011 to May 2021, with a \nratio of 8:1:1, and the temporal test set included notes from June to \nDecember 2021.\nNYU Binned Comorbidity. We created this dataset of H&P notes with \nfive class labels for hospital LOS from the NYU Langone EHR. Most \nof the notes from this dataset were a subset of NYU Notes, with addi-\ntional H&P notes from 2021 for the temporal test. The dataset contains \n327,039 patients, 403,579 notes and 422,485,417 words in total. The \ndataset contains fewer labelled encounters than the NYU Mortality \nand NYU Binned LOS datasets because 22% of the encounters had no \nInternational Classification of Diseases (ICD) codes to calculate the \nCCI score. This missingness motivated our task of predicting binned \nCCI score with a lack of structured ICD codes. We built this dataset \nas follows: for each encounter that ended between January 2011 \nand November 2021, we included its H&P note with a five-class label \nfor binned CCI score. T o generate the labels, we first calculated the  \ncomorbidity index using the ICD codes and the scoring function in ref. 27.  \nWe then discretized the scores into five classes: we assigned label 0 for a \ncomorbidity index below the 50% quantile (0 days), label 1 for a comor-\nbidity index between the 50% and 75% quantile (1–2 days), label 2 for a \ncomorbidity index between the 75% and 90% quantile (3–4 days), label \n3 for a comorbidity index between the 90% and 99% quantile (4–7 days)  \nand label 4 for a comorbidity index above the 99% quantile (>7 days). We \nsplit the dataset into four sets: training, validation, test and temporal \ntest sets. The first three sets were notes from January 2011 to May 2021, \nwith a ratio of 8:1:1, and the temporal test set included notes from June \nto December 2021.\nNYU Binned LOS. We created this dataset of H&P notes with quantile \nlabels for hospital LOS from the NYU Langone EHR. Most of the notes \nfrom this dataset were a subset of NYU Notes, with additional H&P \nnotes from 2021 for the temporal test. The dataset contains 371,922 \npatients, 469,162 notes and 484,467,141 words in total. We built this \ndataset as follows: for each encounter that ended between January 2011 \nand November 2021, we included its H&P note with a binary label and a \nquantile label for LOS. For the quantile label, we assigned label 0 for an \nLOS below the 25% quantile (0–2 days), label 1 for an LOS between the \n25% and 50% quantile (3 days), label 2 for an LOS between the 50% and \n75% quantile (4–5 days) and label 3 for an LOS above the 75% quantile  \n(>5 days). We split the dataset into four sets: training, validation, test \nand temporal test sets. The first three sets were notes from January \n2011 to May 2021, with a ratio of 8:1:1, and the temporal test set included \nnotes from June to December 2021.\nNYU Insurance Denial. We created this dataset of H&P notes with \nbinary labels for whether the patient’s insurance claim was initially \nrejected or directly approved. The dataset contains 54,563 patients, \n55,791 notes and 51,270,256 words in total. We built this dataset as fol-\nlows: for each encounter that occurred between May 1, 2021, and April \n30, 2022, we included its H&P note with a binary label for insurance  \ndenial. We assigned a positive label if the patient’s insurance claim \nstatus was ‘final, adverse determination’ (claim was rejected by insur-\nance and was again rejected following appeal) or ‘final, favorable de-\ntermination’ (claim was rejected by insurance and approved following \nappeal). We split the dataset into four sets: training, validation, test and \ntemporal test sets. The first three sets were notes from May 1, 2021, to \nFebruary 30, 2022, with a ratio of 18:1:1. The temporal test set included \nnotes from March 1 to April 30, 2022.\nNYU Insurance Denial–Discharge Notes. We created this dataset of \ndischarge notes with binary labels for whether the patient’s insurance \nArticle\nclaim was initially rejected or directly approved. The dataset contains \n54,563 patients, 55,791 notes and 49,405,133 words in total. We built this \ndataset as follows: for each encounter that occurred between May 1, \n2021, and April 30, 2022, we included its discharge note with a binary \nlabel for insurance denial. The label assignment and four-way split were \nthe same as in the NYU Insurance Denial dataset.\nNYU Insurance Eventual Denial, H&P . This dataset contained the \nsame notes as the NYU Insurance Denial dataset, but the labels were \ndifferent. The binary label indicated whether the patient’s insurance \nclaim was eventually rejected (even after appeal) or was eventually \napproved (direct approval or approval after appeal).\nNYU Insurance Eventual Denial–Discharge. This dataset contained \nthe same notes as the NYU Insurance Denial–Discharge Notes dataset, \nbut the labels were different. The binary label indicated whether the \npatient’s insurance claim was eventually rejected (even after appeal) \nor was eventually approved (direct approval or approval after appeal).\ni2b2-2012 NER. This is an open dataset released by the Harvard Medi-\ncal School as part of an annual clinical NLP challenge28. This dataset is \na well-known benchmark in the clinical NLP community. The task is to \nidentify and classify clinical concepts (for example, treatments), clinical \ndepartments (for example, surgery), occurrences of events (for exam-\nple, admission) and evidentials (for example, the patient complained) \nfrom de-identified clinical notes from Beth Israel Medical Center in \nBoston. The dataset contains no more than 310 patients, 310 notes and \n636,000 words. We downloaded the dataset as a compressed tar.gz \nfile from the n2c2 data portal after our use application was approved.\nMIMIC-III Readmission. This is an open dataset for an intensive care \nunit (ICU) EHR released by MIT and Boston Beth Israel Medical Center29.  \nWe collected a set of 52,726 discharge notes and created a 30-day \nall-cause readmission label by checking whether there was any subse-\nquent encounter within 30 days. The readmission rate was 6%. We split \nthe data into training, validation and test sets in a 8:1:1 ratio.\nDeployment dataset\nNYU Readmission–Deployment. This dataset consists of discharge \nnotes with binary labels for readmission from our deployment engine \nand the NYU Langone EHR. From January to April 2022, every time a dis-\ncharge note was signed by a physician, the note was sent to our custom \ninference engine for NYUTron’s prediction. The paired discharge note \nand prediction were recorded in a database. The database contained \n27,376 patients, 29,287 notes and 34,669,963 words by the end of the \nstudy period.\nStructured datasets\nNYU Readmission–LACE. We created this dataset of structured LACE30 \nfeatures with binary labels for readmission for comparison against the \nunstructured models. The dataset contains structured features for \nall encounters in the NYU readmission dataset. LACE is a traditional \nclinical prediction rule for readmission with four features: LOS, acuity \nof readmission, Charlson comorbidity index, and number of recent \nemergency department visits in the past 6 months. We built the dataset \nas follows: for every encounter in the NYU Readmission dataset, we \ncollected data on the four LACE features from the NYU Langone EHR. \nLOS was the difference (in days) between the discharge date and the \nadmission date. Acuity of readmission was a binary feature indicating \nwhether the patient was admitted to the emergency department. The \ncomorbidity index was calculated with the ICD-9 or ICD-10 codes for \nchronic diseases, on the basis of the mapping algorithm in ref. 31 and the \nscoring function in ref. 27. The number of emergency department visits \nwas calculated from the patient’s encounter history up to 6 months \nbefore the admission date.\nNYU Readmission–LACE, Manhattan. We created this dataset of struc-\ntured LACE features from the subset of notes from the NYU Readmis-\nsion–LACE dataset that were written in Tisch Hospital in Manhattan.\nNYU Readmission–LACE, Brooklyn. We created this dataset of struc-\ntured LACE features from the subset of notes from the NYU Readmis-\nsion–LACE dataset that were written in NYU Langone Health–Brooklyn.\nNYU Mortality–SAPS2 + APACHE2. We created this dataset of struc-\ntured SAPS2 + APACHE2 features with binary labels for in-hospital mor-\ntality to compare against the unstructured data. The dataset contains a \nsubset of structured SAPS2 + APACHE2 features for all encounters in the \nNYU Mortality dataset. SAPS2 + APACHE2 features are a subset of the \nfeatures used in the SAPS2 model15 and the APACHE2 model16 for ICU \nmortality prediction. We selected the subset of features that were avail-\nable in the NYU Langone EHR. We included the following 12 features: \nage (numerical), mean heart rate (numerical), systolic blood pressure \n(numerical), atrial temperature (numerical), blood urea nitrogen con-\ncentration (numerical), sodium concentration (numerical), potassium \nconcentration (numerical), bilirubin concentration (numerical), white \nblood cell count (numerical), pH (numerical), creatine concentration \n(numerical) and haematocrit (numerical). We additionally included \ndepartment specialty (categorical). We excluded the following features \nowing to their unavailability: PaO2/FiO2 (ratio of arterial oxygen partial \npressure to fractional inspired oxygen), whether the patient was on \nmechanical ventilation or continuous positive airway pressure (CPAP), \nbicarbonate concentration, urine output, Glasgow Coma Scale score, \npresence of metastatic cancer or haematological malignancy or AIDS, \nand whether the admission was scheduled.\nNYU Binned LOS–Lisbon Portugal. We created this dataset of struc-\ntured ‘Lisbon Portugal’ features with binary labels for in-hospital mor-\ntality to compare against the unstructured data model. The dataset \ncontains a subset of the features used in the Lisbon Portugal dataset18 \n(which is widely used in the LOS prediction literature) for all encounters \nin the NYU Binned LOS dataset. We selected a subset of 12 features that \nwere available in the NYU Langone EHR: gender (categorical), age as \nmeasured by the difference in years between the birth date and the \nadmission date (numerical), highest level of education (categorical), \ncountry (categorical), postal code as address (categorical), marital \nstatus (categorical), admission type (categorical), admission service \ntype (categorical), provider ID (categorical), department specialty \n(categorical), procedure name (categorical) and number of previous \nadmissions (numerical). We left out diagnosis because it is not always \navailable at the time of writing H&P notes. We excluded the following \nthree features owing to difficulty in finding them in the NYU Langone \nEHR: homogeneous group diagnosis code, great diagnostic category \nand treatment.\nNYU Insurance Denial–Claim Forms. We created this structured \ndataset based on the NYU Insurance Denial dataset for comparison \nagainst the unstructured data model. The dataset contains structured \nfeatures for all encounters in the NYU Insurance Denial dataset and \nhas the same splits as the NYU Insurance Denial dataset. Selection of \nstructured features was based on the features in ref. 19, which built a \nmodel that predicts insurance claim denial from demographic and \ncare-related features found in the claim form. We found eight avail-\nable features in the NYU Langone EHR: patient name (categorical), age \n(numerical), gender (categorical), postal code as a generalization of \naddress (categorical), insurance brand (categorical), first insurance \nplan name (categorical), provider ID (categorical) and provider type \n(categorical). We additionally added four features based on the clini-\ncian’s inputs: second insurance plan code (categorical), a binary flag for \nsurgical cases (categorical), a binary flag for emergency department \ncases (categorical) and a binary flag for Medicare fee-for-service users \n(categorical). We left out six features in ref. 19 owing to difficulty in \nsearching for them: the patient’s relationship to the insured person, \nnetwork type, whether the claim was a resubmission, diagnosis pointer, \ncharge of service and prior authorization number.\nPreprocessing\nPretraining datasets (NYU Notes, NYU Notes–Manhattan, NYU  \nNotes–Brooklyn). Using these datasets, we trained an uncased BERT \nwordpiece tokenizer with a vocabulary size of 50,000 tokens, a maxi-\nmum sequence length of 512 tokens and special tokens [SEP], [PAD], \n[UNK], [MASK] and [CLS]. Because most of the clinical notes had more \nthan 512 tokens, we split each long note into non-overlapping chunks \nthat were under the maximum sequence length. Specifically, we split \neach note into sentences using natural language toolkit (nltk)32 and  \ntokenized each sentence. For sentences that were longer than 512 to-\nkens, we truncated them. Next, for all tokenized sentences in the same \nnote, we concatenated them into groups such that each group had \nexactly the maximum sequence length. We discarded any remaining \ngroup (with a length strictly less than the maximum) of a long note.\nFine-tuning datasets (NYU Readmission, NYU Readmission–  \nManhattan, NYU Readmission–Brooklyn, NYU Mortality, NYU  \nBinned LOS, NYU Insurance Denial, NYU Binned Comorbidity). Using \nthe tokenizer trained with NYU Notes, we first tokenized the discharge \nnote. We truncated notes that exceeded the maximum sequence length \nof 512 tokens. We leave it for the future to design a language model that \nefficiently reads longer clinical notes (see Extended Data Fig. 8b for the \nimpact of note length on language model performance).\ni2b2-2012 NER. We first decompressed the tar.gz files into folders \nof xml files. We then converted the xml files to brat format. Next, we \nconverted the brat files to bio files. Finally, we wrote a custom Hugging-\nFace33 data loader to convert the folder of bio files into a HuggingFace \ndataset. Our code for preprocessing is available at GitHub.\nDeployment datasets. We first cleaned the notes by stripping out \nhtml artifacts. We then tokenized the discharge note using NYUTron’s \ntokenizer. We truncated notes that exceeded the maximum sequence \nlength of 512 tokens.\nStructured dataset (NYU Readmission–LACE, NYU Mortality–  \nSAPS2 + APACHE2, NYU Binned LOS–Lisbon Portugal, NYU Insurance  \nDenial–Claim Forms). When there was a missing numerical feature \n(for example, the average heart rate was NaN), we filled in the feature \nas the average feature across the training set. For missing categorical \nfeatures (for example, the admitting department was ‘unspecified’), \nwe left them as category ‘none’ .\nPretraining\nWe pretrained a 109 million-parameter BERT model using preproc-\nessed NYU Notes and the MLM objective for 3 weeks (96 epochs) on \n24 NVIDIA A100 GPUs distributed over three compute nodes until the \nvalidation loss started to plateau. The model has 12 hidden layers with \ndimension 768, with 12 attention heads per layer. We used a per-device \ntraining batch size of 64 and saved every 2,000 steps. We used the Zero \nRedundancy AdamW optimizer (an improvement over the Adam opti-\nmizer) with a constant learning rate of 5 × 10−5, FP16 mixed precision \nand stage 2 parallelization34–36.\nFine-tuning\nNYUTron + discharge notes for readmission prediction. We replaced \nthe trained MLM classifier with a randomly initialized linear classifier \nafter the last hidden layer of the pretrained BERT model. We fine-tuned \nthe model end to end using the training set of the NYU Readmission \ndataset for ten epochs, evaluating the validation AUC every half epoch \nand stopping early with a patience of five. We used the following hyper-\nparameters from manual tuning based on the validation AUC: a learning \nrate of 2 × 10−5, a weight decay of 0.01 and a per-device batch size of 4. \nWe optimized the cross-entropy loss using the AdamW optimizer. While \nvarying the size of the dataset (N ∈ {102, 103, 104, 105, 3.92336 × 105}), we \nfine-tuned the pretrained model using subsamples of the NYU Read-\nmission dataset and evaluated their AUC on the temporal test set. For \neach size of subsample, we ran five experiments with distinct random \nseeds (0, 13, 24, 36, 42). For comparison, we looked at the median AUC \nand the standard deviation of the five experiments.\nNYUTron + H&P notes for in-hospital mortality prediction. We  \nreplaced the trained MLM classifier with a randomly initialized linear \nclassifier after the last hidden layer of the pretrained BERT model. \nWe fine-tuned the model end to end using the training set of the NYU \nMortality dataset for ten epochs, evaluating the validation AUC every \nhalf epoch and stopping early with a patience of 5. We used the fol-\nlowing hyperparameters from manual tuning based on the validation \nAUC: a learning rate of 2 × 10−5, a weight decay of 0.01 and a per-device \nbatch size of 4. We optimized the cross-entropy loss using the AdamW \noptimizer. Using the full dataset, we fine-tuned the pretrained model \nusing subsamples of the NYU Mortality dataset and evaluated their \nAUC on the temporal test set. For each size of subsample, we ran five \nexperiments with distinct random seeds (0, 13, 24, 36, 42). For com-\nparison, we looked at the median AUC and the standard deviation of \nthe five experiments.\nNYUTron + H&P notes for binned comorbidity prediction. We  \nreplaced the trained MLM classifier with a randomly initialized linear \nclassifier after the last hidden layer of the pretrained BERT model. \nWe fine-tuned the model end to end using the training set of the NYU \nBinned Comorbidity dataset for ten epochs, evaluating the validation \nOVR AUC every half epoch and stopping early with a patience of 5. We \nused the following hyperparameters from manual tuning based on the \nvalidation OVR AUC: a learning rate of 2 × 10−5, a weight decay of 0.01 \nand a per-device batch size of 4. We optimized the cross-entropy loss \nusing the AdamW optimizer. Using the full dataset, we fine-tuned the \npretrained model with subsamples of the NYU Binned Comorbidity \ndataset and evaluated their OVR AUC on the temporal test set. For each \nsize of subsample, we ran five experiments with distinct random seeds \n(0, 13, 24, 36, 42). For comparison, we looked at the median OVR AUC \nand the standard deviation of the five experiments.\nNYUTron + H&P notes for binned LOS prediction. We replaced the \ntrained MLM classifier with a randomly initialized linear classifier after \nthe last hidden layer of the pretrained BERT model. We fine-tuned the \nmodel end to end using the training set of the NYU Binned LOS dataset \nfor ten epochs, evaluating the validation AUC every half epoch and \nstopping early with a patience of 5. We used the following hyperparam-\neters from manual tuning based on the validation OVR AUC: a learning \nrate of 2 × 10−5, a weight decay of 0.01 and a per-device batch size of 4.  \nWe optimized the cross-entropy loss using the AdamW optimizer. Us-\ning the full dataset, we fine-tuned the pretrained model with subsam-\nples of the NYU Binned LOS dataset and evaluated their AUC on the \ntemporal test set. For each size of subsample, we ran five experiments \nwith distinct random seeds (0, 13, 24, 36, 42). For inference, we com-\nbined the last two classes, label 3 (90–99% quantile) and label 4 (>99%  \nquantile) because label 4 was very sparse. For comparison, we \nlooked at the median OVR AUC and the standard deviation of the five  \nexperiments.\nNYUTron + H&P notes for insurance denial prediction. We replaced \nthe trained MLM classifier with a randomly initialized linear classifier \nafter the last hidden layer of the pretrained BERT model. We fine-tuned \nthe model end to end using the training set of the NYU Insurance \nArticle\nDenial dataset for ten epochs, evaluating the validation AUC every half  \nepoch and stopping early with a patience of 5. We used the follow -\ning hyperparameters from manual tuning based on the validation \nAUC: a learning rate of 2 × 10−5, a weight decay of 0.01 and a per-device \nbatch size of 4. We optimized the cross-entropy loss using the AdamW  \noptimizer. Using the full dataset, we fine-tuned the pretrained model \nusing subsamples of the NYU Insurance Denial dataset and evaluated \ntheir AUC on the temporal test set. For each size of subsample, we ran \nfive experiments with distinct random seeds (0, 13, 24, 36, 42). For \ncomparison, we looked at the median AUC and the standard deviation \nof the five experiments.\nNYUTron + clinical notes for NER. We performed the fine-tuning \nexperiments as follows. For each LLM in Extended Data Table 2, we \ninitialized a HuggingFace token classification model with the LLM as \nthe pretrained checkpoint. We fine-tuned the model using i2b2-2012 \nNER for ten epochs using the AdamW optimizer with a learning rate of \n2 × 10−5, a weight decay of 0.01 and a batch size of 4, evaluating every \n50 steps and stopping early on the basis of area under the receiver \noperating characteristic (AUROC) with a patience of 1. This took 20 to \n40 min on one node of four NVIDIA 17-GB V100 GPUs. We performed \nfine-tuning five times with random seeds 0, 13, 24, 36 and 42 and  \nrecorded the average and standard deviation of the micro-averaged F1 \nscore (excluding the label for non-entity, ‘O’).\nNYUTron + MIMIC-III readmission. We performed the fine-tuning \nexperiments as follows: For both NYUTron and BioClinicalBert, we \ninitialized a HuggingFace token classification model with the LLM as \nthe pretrained checkpoint. We fine-tuned the model using MIMIC-III \nReadmission for ten epoch using the AdamW optimizer with a learning \nrate of 2 × 10−5, a weight decay of 0.01 and a batch size of 16, evaluating \nevery half epoch. We performed fine-tuning five times with random \nseeds 0, 13, 24, 36 and 42.\nDeployment\nThe fine-tuned model was converted to a high-performance format \n(Onnx or T ensorRT) and loaded into our deployment platform, an \nNVIDIA Triton inference engine that interfaces with the NYU Langone \nEHR through the HLA7 Fast Health Interoperability Resources (FHIR)37 \ninterface. For our consideration of performance, security, reliability \nand interpretability, see Supplementary Information section 5.\nOur deployment platform consisted of a modified version of NVIDIA’s \nTriton Inference Server that we named NYUTriton (pronounced ‘nutri-\ntion’ because it is good for the health system). NVIDIA Triton supports \nGPU-, x86- and ARM CPU-based inferencing and several key features, \nincluding dynamic batching, concurrent execution, a highly flexible \nmodel specification interface, and the ability to support a wide range \nof deep learning frameworks and accelerated model formats for maxi-\nmum throughput. We modified NVIDIA Triton to interface seamlessly \nwith HuggingFace-formatted language models so as to provide a uni-\nform and highly flexible crossover point between our development \nand production pipelines. Trained models were saved in a standard \nHuggingFace-style format and converted into Onnx and then T ensorRT \nto obtain sub-millisecond-scale inference results. NYUTriton is hosted \non a dedicated inference server that consists of an AMD Threadripper \n3960X (24 cores, 3.8 GHz), two RTX 3090 GPUs and 128 GB of DDR5 \nsystem memory purchased from Lambda Labs.\nFollowing the signing of discharge summaries in Epic, the HL7 FHIR \ninterface connects with NYUTriton and sends a JavaScript Object Nota-\ntion ( JSON) payload consisting of the discharge summary and metadata \nspecifying the underlying readmission model and sender. NYUTri-\nton preprocesses the text, runs an inference job with the accelerated \nNYUTron readmission model and returns the model’s inference result to \na secondary orchestration server, which writes the result to a database \nand generates an email to the signing physician.\nStructured baselines\nThe structured baselines were (1) SAPS2/APACHE2 features + XGBoost \nfor in-hospital mortality prediction, (2) LACE features + XGBoost for \nreadmission prediction, (3) Lisbon Portugal features + XGBoost for \nbinned LOS prediction and (4) claim form features + XGBoost for insur-\nance denial prediction.\nFor all structured baselines, we used the xgboost library to train an \nextreme gradient-boosted tree classifier with a binary logistic loss (mul-\nticlass softmax loss for more than two classes). We used scikit-learn’s \nrandomized search to search hyperparameters among minimum_child_\nweight from {1, 5, 10}, gamma from {0.5, 1, 1.5, 2, 5}, subsample from {0.6, \n0.8, 1}, col_sample_bytree from {0.6, 0.8, 1.0}, max_depth from {3, 4, 5}, \nlearning_rates from {0.001, 0.01, 0.1, 0.5} and n_estimators from {10, \n100, 1000} for 100 iterations based on AUROC score (ovr-auroc score \nfor multiple classes) from threefold cross-validation38. We ran each \nexperiment five times with distinct random seeds (0, 13, 24, 36, 42). For \nmortality, binned comorbidity, binned LOS and insurance denial, we \nran the experiment with the full dataset. For readmission, we trained \nthe model using subsamples (N ∈ {102, 103, 104, 105, 3.92336 × 105}) of \nthe NYU Readmission–LACE dataset.\nMetrics\nWe evaluated the five tasks (in-hospital mortality prediction, binned \ncomorbidity index prediction, 30-day all-cause readmission prediction, \nbinned LOS prediction and insurance denial prediction) with AUC for \nbinary classes and OVR AUROC for multiple classes. AUROC is the area \nunder the two-dimensional curve consisting of tuples of the form (TPR, \nFPR) resulting from different decision thresholds.\nWe additionally evaluated readmission prediction with the following \nmetrics: TPR, FPR, precision, recall and F1 score, all of which have a range \nof [0, 1]. We evaluated NER using a micro-averaged NER F1 score. The \nNER F1 score is similar to the normal F1 score except that the non-entity \nlabel ‘O’ is excluded for calculation.\nBaseline algorithms for retrospective study\nWe compared NYUTron against physicians. We worked with six physi-\ncians with different levels of seniority: three attending physicians and \nthree residents. The physicians were asked to review discharge sum-\nmaries and predict whether the described patient would come back \nto the hospital within 30 days.\nWe compared NYUTron against four other LLMs and two machine \nlearning models. ‘random-init’ is a BERT-base uncased model with \nrandomly initialized parameters. ‘web-wiki’ is a BERT-base uncased \nmodel that is pretrained using web text (from the BookCorpus data-\nset39) and Wikipedia articles (from the English Wikipedia dataset40). \n‘web-wiki+bio’ is a BERT model pretrained using web text, Wikipedia \narticles, PubMed abstracts41 and PubMed Central (PMC) full articles42. \n‘web-wiki+bio+clinical’ , or gatortron-og43, is a Megatron-BERT44 model \npretrained using web text, Wikipedia articles, PubMed abstracts, PMC \nfull articles, MIMIC-III notes and de-identified clinical notes from Univer-\nsity of Florida Health. ‘lace+xgb’ reads structured LACE features (from \na traditional clinical prediction rule) with an extreme gradient-boosted \ntree model14. ‘tf-idf+xgb’ reads corpus-level bag-of-words features with \nan extreme gradient-boosted tree model. For detailed statistics and \nexamples of the pretraining corpora, see Extended Data Table 2 and \nExtended Data Fig. 3.\nComparison with physicians\nWe randomly sampled 20 discharge notes from the random test set \nand asked six doctors with different seniority to predict whether the \npatient would come back within 30 days. The six physicians included \nthree attending neurosurgeons, two neurosurgery residents and one \nICU resident.\nWe used REDCap to perform the survey and gave physicians unlim-\nited time. The survey was structured as follows: for each case, we \nasked “Will this person be admitted within 30 days?” , followed by the \ndischarge summary. The physician could choose to answer “yes” or \n“no” . If the patient came back within 30 days, we had three follow-up \nquestions to assess the characteristics of the subsequent readmission. \nFirst, we asked “Is this readmission related to the prior discharge?” , fol-\nlowed by the H&P note of the subsequent readmission. The physician \ncould answer “yes” , “no” , “partial” or “does not meet Medicare criteria \nfor 30-day readmission” . The second follow-up question was “Is this \nreadmission preventable?” , to which the physician could answer “yes” , \n“no” or “partial” . The third follow-up question, “ Any comments?” , had a \nfree-text response where the physician could explain why the readmis-\nsion was partially related to the prior discharge or why the readmission \nwas partially preventable.\nT o collect NYUTron’s predictions, we used the text classification pipe-\nline from HuggingFace to perform inference on the 20 discharge notes. \nFor each discharge note, the pipeline output a predicted probability for \nreadmission. We converted this predicted probability to a binary label \nwith a threshold of 0.07 (a predicted probability no less than 0.07 was \nconverted to a positive label). We chose 0.07 as the decision bound-\nary because it was the minimum threshold that gave us above 80% \nvalidation recall among the thresholds {0.01 × n : n ∈ {1, ..., 90} (the \n80% criterion was chosen on the basis of clinical applicability). See \nExtended Data Fig. 8c for NYUTron’s calibration curve.\nComparison with other language models\nDischarge notes + other LLMs for readmission prediction. The \ndataset, hyperparameters, and evaluation and software libraries for \nfine-tuning other LLMs were the same as when fine-tuning NYUTron. \nThe pretrained LLMs were constructed as follows: random-init is \na BERT-base uncased model with reset parameters. web-wiki is a \nBERT-base uncased model. web-wiki+bio is a dmis-lab/biobert-base \ncased v1.2 model. web-wiki+bio+clinical was Gatortron-og downloaded \nfrom NVIDIA NGC and converted to a HuggingFace checkpoint using \nconvert megatron bert checkpoint.\nClinical notes + other LLMs for NER. The dataset, hyperparameters, \nand evaluation and software libraries for fine-tuning other LLMs were \nthe same as for fine-tuning NYUTron. The pretrained LLMs were the \nsame as the baseline LLMs for predicting readmission from discharge \nnotes.\nComparison with machine learning models\nLACE features + XGBoost for readmission prediction. Using the NYU \nReadmission–LACE dataset, we used the xgboost library to train an \nextreme gradient-boosted tree classifier with binary logistic loss with \nhyperparameter search. We used scikit-learn’s randomized search to \nsearch among minimum_child_weight from {1, 5, 10}, gamma from {0.5, \n1, 1.5, 2, 5}, subsample from {0.6, 0.8, 1}, col_sample_bytree from {0.6, \n0.8, 1.0}, max_depth from {3, 4, 5}, learning_rates from {0.001, 0.01, \n0.1, 0.5} and n_estimators from {10, 100, 1000} for 100 iterations on \nthe basis of AUROC score on the validation set37. We trained the model \nusing subsamples (N ∈ {102, 103, 104, 105, 3.92336 × 105}) of the NYU \nReadmission–LACE dataset and evaluated their AUROC on the tempo-\nral test set. For each size of subsample, we ran five experiments with \ndistinct random seeds (0, 13, 24, 36, 42). For comparison, we looked at \nthe median AUROC and the standard deviation of the five experiments.\nXGBoost + TF-IDF for readmission prediction. We transformed the text \nfrom the NYU Readmission dataset into tf-idf (term frequency–inverse  \ndocument frequency) embeddings and used an xgboost classifier with \nbinary logistic loss to predict readmission. We used raytune45 to search \nhyperparameters, including max_tf-idf features from {512, 5000}, max_\ndepth from a quantized random integer of 3 to 16 with an interval of 4, \nlearning_rate from a log uniform distribution from 10−2 to 10−1, gamma \nfrom a quantized uniform distribution from 0 to 12 with an interval \nof 4, minimum_child_weight from a quantized uniform distribution \nfrom 0 to 8 with an interval of 4, reg lambda from a quantized uniform \ndistribution from 0 to 10 with an interval of 2, colsample_bytree from \na uniform distribution from 0.7 to 1, scale pos weight from a quantized \nuniform distribution from 0 to 50 with an interval of 10 and n_estimator \nfrom a quantized integer distribution from 50 to 300 with an interval \nof 50. We trained the model using subsamples (N ∈ {102, 103, 104, 105, \n3.92336 × 105}) of the NYU Readmission dataset and evaluated their \nAUROC on the temporal test set. For each size of subsample, we ran \nfive experiments with distinct random seeds (0, 13, 24, 36, 42). For \ncomparison, we looked at the median AUROC and the standard devia-\ntion of the five experiments.\nComparison of multi-site pretraining and fine-tuning\nWe compared NYUTron with its four variants (pretrained and fine-tuned \nusing data from different sites): (1) NYU Notes–Manhattan + NYU Read-\nmission–Manhattan, (2) NYU Notes–Manhattan + NYU Readmission–\nBrooklyn, (3) NYU Notes–Brooklyn + NYU Readmission–Brooklyn and \n(4) NYU Notes–Brooklyn + NYU Readmission–Manhattan. The hyperpa-\nrameters and evaluation and software libraries for fine-tuning NYUTron \nvariants were the same as for fine-tuning NYUTron.\nAnalysis of prospective performance\nOn the basis of the temporal test performance in the retrospective \nstudy, we selected a fine-tuned model with a decision threshold of 0.07 \nfor use in the prospective trial.\nComparison of mortality rate and LOS. T o assess the condition of \nthe readmitted patients who were correctly predicted (n = 3,298), we \ncompared their in-hospital mortality rate and length of hospitaliza-\ntion with that of patients who were admitted in the same period. We \ncollected data on patients who were admitted from February to May \n2022 (n = 30,548) and compared their in-hospital mortality rate and \nLOS with that of the readmitted patients caught by NYUTron from \nJanuary to April 2022. We used two-sided Welch’s t tests (with the null \nhypothesis that the two groups had the same average) to assess the \nstatistical significance of our comparison46.\nAssessing NYUTron’s clinical impacts with physician review. We \nperformed a post hoc analysis of readmitted patients in the prospective \ncohort to better understand model performance in a real-world envi-\nronment and in anticipation of creating targeted interventions based \non model outputs. One hundred readmitted patients were sampled \nfrom the five largest departments at NYU Langone by patient volume: \ninternal medicine, pediatrics, general surgery, obstetrics and gynaecol-\nogy, and haematology and oncology. Each department contributed 20 \ncases, with 10 cases having the highest predicted probabilities in that \ndepartment and 10 cases having the lowest predicted probabilities. \nAll cases had their encounter IDs logged for their index discharge and \nreadmission on a secure online platform. A standardized questionnaire \nwas constructed for manual review asking whether the readmission \nwas planned, whether the readmission met CMS criteria for a penalized \n30-day readmission, whether the readmission was preventable, wheth-\ner an adverse event occurred on readmission, whether any adverse \nevents were preventable and whether the reviewing physicians had any \ncomments on the case. A team of ten physicians from internal medicine \nand neurosurgery were randomly assigned cases to be reviewed in \npairs, with any disagreement between the reviewers adjudicated by \na third physician reviewer. T o determine whether a readmission was \npreventable, the reviewer looked at the discharge note of the inference \nencounter and the H&P note of the readmission encounter.\nEthical approval\nOur research was approved by the NYU Langone institutional review board \nas ‘s21-01189 NYUtron’ , and the methods were carried out in accordance \nwith the institutional review board’s relevant guidelines and regulations.\nArticle\nReporting summary\nFurther information on research design is available in the Nature Port-\nfolio Reporting Summary linked to this article.\nData availability\nThe clinical data used for the pretraining, fine-tuning, validation and \ntest sets were collected from the NYU Langone Health System EHR \nmaintained by the NYULH Datacore team. T ext data were stripped of \nrich-text features and directly included in the dataset ‘as is’ and were \naugmented with structured features where noted. These data consist \nof the production medical records of NYU Langone and cannot be made \npublicly available. Researchers may obtain a limited de-identified data-\nset (or a test subset) from NYU Langone Health System by reasonable \nrequest and subject to local and national ethical approvals. We also \nused publicly available i2b2-2012 (https://portal.dbmi.hms.harvard.\nedu/projects/n2c2-nlp/) and MIMIC-III (https://physionet.org/content/\nmimiciii/1.4/) datasets.\nCode availability\nWe used sql and Python 3.8.13 to collect data from the NYU Langone \nEHR. We used REDCap 12.4.31 to collect physician responses. This work \nused several open-source libraries, including HuggingFace Transform-\ners 4.19.2, Datasets 2.2.2, Evaluate 0.1.1, wandb 0.12.17, matplotlib \n3.5.2, seaborn 0.12.2, pandas 1.4.2, ray 2.0.0, sklearn 1.1.1, deepspeed \n0.8.0+384f17b, NVIDIA Apex, XGBoost 1.6.1 and nltk 3.6.3. Our experi-\nmental framework involved the use of these libraries and, in some cases, \nmodification of them. We will release code to replicate the pretraining, \nfine-tuning and testing of the models described in this paper at the \ntime of publication (code for experiments available at https://github.\ncom/nyuolab/NYUTron, preprocessing code for i2b2-2012 available \nat https://github.com/nyuolab/i2b2_2012_preprocessing). We include \ndetailed methods and implementation steps in the Methods and Sup-\nplementary Information to allow for independent replication.\n \n27. Charlson, M. Charlson comorbidity index (CCI). MD+CALC https://www.mdcalc.com/\ncalc/3917/charlson-comorbidity-index-cci (2022).\n28. Sun, W., Rumshisky, A., & Uzuner, O. Annotating temporal information in clinical \nnarratives. J. Biomed. Inform. 46, 5–12 (2013).\n29. Johnson, A. E. W. et al. MIMIC-III, a freely accessible critical care database. Sci. Data 3, \n160035 (2016).\n30. van Walraven, C. et al. Derivation and validation of an index to predict early death or \nunplanned readmission after discharge from hospital to the community. Can. Med. \nAssoc. J. 182, 551–557 (2010).\n31. Sundararajan, V. et al. New ICD-10 version of the Charlson comorbidity index predicted \nin-hospital mortality. J. Clin. Epidemiol. 57, 1288–1294 (2004).\n32. Bird, S. & Loper, E. NLTK: The Natural Language Toolkit. in Proc. 2004 ACL Interactive \nPoster and Demonstration Sessions 214–217 (Association for Computational Linguistics, \n2004).\n33. Wolf, T. et al. Transformers: state-of-the-art natural language processing. in Proc. 2020 \nEMNLP (eds Webber, B., Cohn, T., He, Y. & Liu, Y.) 38–45 (Association for Computational \nLinguistics, 2020).\n34. Rajbhandari, S., Rasley, J., Ruwase, O. & He, Y. ZeRO: memory optimizations. Toward \ntraining trillion parameter models. in Proc. Int. Conf. High Performance Computing, \nNetworking, Storage and Analysis 1–16 (IEEE Press, 2020).\n35. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. ICLR https://openreview.net/ \nforum?id=Bkg6RiCqY7 (2019).\n36. Kingma, D. P. & Ba, J. Adam: a method for stochastic optimization. ICLR https://arxiv.org/\nabs/1412.6980 (2017).\n37. Ayaz, M., Pasha, M. F., Alzahrani, M. Y., Budiarto, R. & Stiawan, D. The Fast Health \nInteroperability Resources (FHIR) standard: systematic literature review of implementations, \napplications, challenges and opportunities. JMIR Med. Inform. 9, 21929 (2021).\n38. Pedregosa, F. et al. Scikit-Learn: machine learning in Python. J. Mach. Learn. Res. 12, \n2825–2830 (2011).\n39. Zhu, Y. et al. Aligning books and movies: towards story-like visual explanations by watching \nmovies and reading books. in Proc. 2015 ICCV (ed. O’Conner, L.) 19–27 (IEEE, 2015).\n40. Wikimedia Foundation. Wikimedia downloads. https://dumps.wikimedia.org/  \n(2021).\n41. NCBI Literature Resources. Download PubMed data. https://pubmed.ncbi.nlm.nih.gov/\ndownload/ (2022).\n42. National Library of Medicine. PubMed Central: PMC article datasets. https://www.ncbi.\nnlm.nih.gov/pmc/tools/textmining/ (2022).\n43. Yang, X. et al. A large language model for electronic health records. NPJ Digit. Med. 5, 194 \n(2022).\n44. Shoeybi, M. et al. Megatron-LM: training multi-billion parameter language models  \nusing model parallelism. Preprint at https://doi.org/10.48550/arXiv.1909.08053  \n(2020).\n45. Liaw, R. et al. Tune: a research platform for distributed model selection and training. \nPreprint at https://doi.org/10.48550/arXiv.1807.05118 (2018).\n46. Welch, B. L. The generalization of Student’s problem when several different population \nvariances are involved. Biometrika 34, 28–35 (1947).\nAcknowledgements E.K.O. is supported by the National Cancer Institute’s Early Surgeon \nScientist Program (3P30CA016087-41S1) and the W.M. Keck Foundation. We would like to \nacknowledge J. Golfinos, whose vision and support made this project possible. We also  \nwould like to acknowledge our collaborators M. Costantino and K. Yie from the NYU Langone \nHigh-Performance Computing (HPC) team; without their tireless assistance in building and \nmaintaining our GPU cluster, none of this research would have been possible. We would also \nlike to thank D. Bar-Sagi and N. Mherabi, whose support for this research has made everything \npossible. We would like to thank B. Guzman from the NYU Langone Predictive Analytics Unit \nand V.J. Major from the NYU Grossman School of Medicine for their help with learning the SQL \ndata structures used as part of this work. We would like to thank Y.(R.) Pang for reviewing and \nediting the initial manuscript. We would like to thank X. Yang from University of Florida for \nhelping us with preprocessing and evaluating the i2b2 dataset. We thank S. Ciprut for helping \nwith the REDCap survey and research administration for our team. We thank C. Fernandez- \nGranda, J. Kempe, V. Dhar, N. Wu, M. Barot, A. Chen, K. Link and F. Kwon for their valuable \ndiscussions.\nAuthor contributions E.K.O. conceptualized and supervised the project. L.Y.J. collected data \n(except the NYU Insurance Denial and MIMIC-III Readmission datasets) and performed \nexperiments. L.Y.J. and X.C.L. prepared the figures. X.C.L., N.P.N., M.N.-M. and K.C. debugged \nand tested the model and the pretraining and fine-tuning software. E.K.O. designed the \nNYUTriton deployment platform, and E.K.O., A.A. and D.W. built the system and integrated it \nwith the EHR. K.E., E.K.O., D.W. and Y.A. collected and processed the NYU Insurance Denial \ndataset. H.A.R., I.L., P.P., K.E., M.M., N.C.K., C.O., Z.S., C.L., H.W., D.K., S.N., Y.D., D.K. and A.T.M.C. \nparticipated in the human experiments, review of cases, and providing user feedback and \ntesting. G.Y. and M.C. provided the scripts for tf-idf+xgb and built the MIMIC-III Readmission \ndataset. M.F., A.B.C., Y.A. and K.C. provided guidance and feedback throughout the project. \nL.Y.J., K.C. and E.K.O. wrote the initial draft. L.Y.J., E.K.O., K.C., M.N.-M., G.Y. and M.C. formatted \nthe final submission. All authors edited and revised the manuscript.\nCompeting interests E.K.O. reports consulting with Sofinnova and Google, income from Merck \n& Co. and Mirati Therapeutics, and equity in Artisight. N.P.N., M.F. and A.B.C. are employed by \nNVIDIA. D.K. reports consulting with Elekta. K.C. is employed by Prescient Design, a Genentech \naccelerator, a subsidiary of Roche. There are no other potential conflicts of interest. The work \npresented herein was performed exclusively within the NYU Langone Health System.\nAdditional information\nSupplementary information The online version contains supplementary material available at \nhttps://doi.org/10.1038/s41586-023-06160-y.\nCorrespondence and requests for materials should be addressed to Eric Karl Oermann.\nPeer review information Nature thanks Ziad Obermeyer and the other, anonymous, reviewer(s) \nfor their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\nExtended Data Fig. 1 | Difference between random test and temporal test.  \na, AUC curve for the random test shows better performance than the temporal \ntest. The random-test AUC is 84.13%, compared to the temporal-test AUC of \n80.2%. The difference highlights the importance of creating a test set to reflect \nthe problem setup. In the case of readmission prediction, the deployment set \nalways comes from the future of the training set. Thus we use the temporal test \nAUC for model selection. b , Comparison of random-test AUC and temporal-test \nAUC as the number of training examples increases shows that temporal-testing \nis important to estimate deployment performance. Here we show that \nsampling a temporally split out dataset seems “harder” than a randomly \nsampled test dataset because all tested LLMs and lace+xgb perform worse on \nthe temporal test (notes from the future) than the random test (notes from the \nsame time as the training data). The colored lines on the left (random test AUCs) \nare generally higher than the colored lines on the right (temporal test AUCs). \nWe conclude that this is an important distinction that temporally sampled \nheld-out test sets give a more realistic estimate of model performance. \nInterestingly, the language models seem to be more sensitive to this \nphenomenon than the lace+xgb model.\nArticle\nExtended Data Fig. 2 | Benchmarking NYUTron against a traditional NLP \nmodel and other language models on a different clinical prediction task \n(clinical concept extraction).  We observe a similar trend as readmission \nprediction: (a) shows that NYUTron has better performance than tf-idf under \ndifferent data availability settings and (b) shows that clinically pretrained \nlanguage models have better performance than non-clinically pretrained \nlanguage models. This corroborates our findings that health-system scale \nlanguage models are general purpose clinical prediction engines and that a \ndomain match between pretraining and finetuning corpus contributes to task \nperformance. a, Comparison of temporal test AUCs between NYUTron and a \ntraditional NLP model (tf-idf+xgb). NYUTron has a higher median AUC than \ntf-idf+xgb for all tested number of finetuning examples. The black vertical line \nindicates standard deviation over 5 trials of different random seeds (0, 13, 24, \n36, 42). b, Comparison of LLMs’ finetuning performances on the NER task. On \nthe i2b2-2012 clinical concept extraction task, the LLMs that are pretrained \nwith clinical corpora (NYUTron, web-wiki+bio+clinical) have a higher average \nf1 score than LLMs that are not pretrained with clinical corpora (web-wiki+bio, \nweb-wiki, random-init). Specifically, NYUTron and web-wiki+bio+clinical \nperform better than the randomly initialized model (36.64% higher median \nseqeval f1 score) and non-clinically pretrained models (2.01%–3.48% higher \nmedian seqeval f1 score). Note that the height of each bar is the average f1 score \nand the half length of each black vertical line indicates the standard deviation \nover 5 trials of different random seeds (0, 13, 24, 36, 42).\nExtended Data Fig. 3 | Examples of pretraining corpora. We include here \nsome examples from the utilized pretraining corpora to help contextualize our \nwork. Examples from three types of pretrain corpus: (1) web-wiki (online books \nfrom bookcorpus and encyclopedia articles from English Wikipedia), (2) bio \n(abstracts of academic papers from Pubmed Abstracts and full articles from \nPubmed Central), and (3) clinical (NYU Notes, NYU Readmission from Langone \nEHR and clinical notes from University of Florida Health).\nArticle\nExtended Data Fig. 4 | Comparison of NYUTron’s and BioClinicalBERT’s \nperformance on MIMIC-III Readmission.  To test how much finetuning \nNYUTron needs to generalize to another health system, we finetune NYUTron \nand BioClinicalBERT (which has the same number of parameters and \narchitecture as NYUTron, but pretrained on MIMIC notes, bookcorpus, \npubmed and wikipedia articles) using different subsamples of MIMIC-III \nreadmission dataset. The dataset contains 52,726 de-identified ICU discharge \nnotes from Boston Beth Israel Hospital with 8:1:1 train-val-test split. At 100 \nsamples, the AUC is similar. At 1000 samples, NYUTron has a 3.58% higher \nmedian AUC than BioClinicalBERT (57.22% v.s. 53.64%). At 10,000 samples, \nNYUTron has a 6.42% higher median AUC than BioClinicalBERT (65.56% v.s. \n59.14%). Using the full dataset (42,180 samples), NYUTron has a 3.8% higher \nmedian AUC than BioClinicalBERT (67.04% v.s. 63.24%). Given that NYUTron \nwas pretrained on identified all-department notes from NYU Langone and \nfinetuned on de-identified ICU-specific notes from Beth-Israel, this result \nshows that NYUTron is able to generalize to a very different health environment \nthrough local finetuning. The height of the bar indicates the median \nperformance of 5 experiments using distinct random seeds (0, 13, 24, 36, 42) \nand the error bar indicates the min-max range.\nExtended Data Fig. 5 | Bias analysis stratifying NYUTron’s performance by \nclinical departments and months.  a, A stratified analysis of NYUTron’s \ntemporal test performance by clinical department and oncological \nsubspecialty. NYUTron performs best in the Neurology Department (AUC \n90.12%), and performs worst in the Internal Medicine Department (AUC 67.95% \nfor non-oncology specialty and AUC 63.77% for oncology specialty), with a \ndifference of about 20% AUC. This significant variance across clinical \ndepartments suggests that a more fine-grained analysis may lead to \nperformance benefits. We annotate the number of examples (N) and the \nreadmission rate (p) for each department. b , NYUTron’s performance displays \nminor fluctuations over months. We plot the average monthly test AUC of \nNYUTron from January 2013 to December 2021 to look for underlying monthly \ntrends or cycles and to test the hypothesis that performance would be worst in \nJuly when new physicians start their training with a different writing style than \nphysicians already in practice (dashed red line indicating the monthly AUC of \nJuly). The height of the bar indicates average monthly performance across the  \n9 years and the vertical bar indicates the standard deviation. We annotate the \nnumber of examples (N) and the readmission rate (p) for each month. We note \nthat July has the second lowest monthly AUC and the highest variance. We \nspeculate (and need more years of data to verify) that clinical notes written by \nnew physicians are associated with the temporal shift across the months and \nthe drop in performance in July. Average AUCs from the quarters January to \nMarch, April to June, and July to September are increasing, which may coincide \nwith residents’ rotation schedule across different clinical departments. We \nleave further investigation of this cyclical performance to future work.\nArticle\nExtended Data Fig. 6 | Bias analysis stratifying NYUTron’s performance by \nage groups and major racial groups. As part of an analysis of model \nperformance by two possible sources of bias, age and race, we perform \nstratified analyses of NYUTron’s performance. We annotate the number of \nexamples (N) and the readmission rate (p) for each evaluation. a , We stratify the \ntemporal test based on nine bins of ages (0 to 90 years with bins of 10 year \nintervals). NYUTron performs best for patients who are 10 to 40 years old, and \nhas declining performance by decile over the age of 40 years with the worst \nperformance in the 80–90 years of age group. We observe that this isn’t an \neffect of sample size, the single largest sample is age 80–90, but likely reflects \ncomplexity and comorbidity burdens being disproportionately higher with \nadvanced age. b, To test for potential dependencies and bias by race, we first \nidentify the five most frequent races in the dataset (White, Other Race, Black, \nChinese, Indian), then stratify the evaluation results by race. NYUTron \nperforms best on Chinese patients and worst on Black patients with a mild \nvariation in AUC across both groups.\nExtended Data Fig. 7 | Detailed statistics of the comparison between \nlanguage models and lace+xgb.  a, A box plot with individual data points. For \neach model, 5 experiments were run using random seeds 0, 13, 24, 36, 42. The \ncenterline of the box plot indicates the median. The upper line of the box \nindicates the first quantile. The lower line of the plot indicates the last quantile. \nThe whisker extends to 1.5 times the interquartile length and the diamonds \nindicate outliers. b , A bar plot that shows the mean and standard deviation. The \nheight of the bar indicates the mean across 5 experiments and the length of the \nblack vertical line indicates the standard deviation.\nArticle\nExtended Data Fig. 8 | Additional information about readmission \nprediction.  a, Visualization of readmission data split timelines. We \nvisualize the random split, temporal split, and deployment split on a \ntimeline to indicate this decision for model evaluation. The random  \nsplit starts from January 2013 and ends in May 2021 (inclusive), which is \nfurther split into a 80% train set, 10% validation set and a 10% test set.  \nThe temporal split (temporal test) starts from June 2021 and ends in \nDecember 2021, a time period from which no training samples were \nsampled from. The deployment data is necessarily sampled from the \nfuture as it is accrued prospectively as part of our single arm, non-\ninterventional clinical trial. b , NYUTron’s performance increases with \nmore complete input notes. To attempt to estimate performance as a \nfunction of sequence length we sampled a subset of “long notes” from \nthe temporal test set. Each note in this subset has no less than 400 \nwords, or approximately 512 tokens. We truncated these long notes to \n100, 200, 300 and 400 words while keeping their readmission labels \nfixed in order to demonstrate the incremental gain in performance as  \nwe capture proportionally more information from each of these “long \nnotes”. The dashed line is the AUC of all notes. This figure shows that \nprocessing more words from the possible input leads to a better \nevaluation performance and confirms that there is a clear potential  \nfor improving performance by increasing maximum sequence length. \nc,d NYUTron’s calibration curve for temporal test (c, number of \nevaluation examples is N = 53,916) and prospective deployment  \n(d, number of evaluation examples is N = 29,286). As a reference, the \norange line is the calibration curve of an ideally calibrated classifier. \nThe blue line is NYUTron’s calibration curve. Currently we do not \nperform any additional calibration and choose the decision threshold \nbased on the precision and recall on the temporal validation set. The \npredicted probability is normalized by the largest predicted probability. \nOverall the model is well calibrated to the 30-day readmission task.\nExtended Data Table 1 | Detailed statistics of datasets\nWe built a comprehensive pretraining dataset (NYU Notes) with two site-specific variants (NYU Notes - Manhattan/Brooklyn) as discussed in the Methods section. For readmission prediction,  \nwe also built a finetuning dataset (NYU Readmission) with two site-specific variants (NYU Readmission Manhattan/Brooklyn), one structured-data variant (NYU Readmission - LACE),  \nand a deployment test set (NYU Readmission - Deployment) that was sampled in real-time as part of our prospective trial. To test the breadth of NYUTron’s applicability, we added 4 tasks  \n(NYU Mortality, NYU Binned LOS, NYU Comorbidity, NYU Insurance denial) with their respective structured-data variant (NYU Mortality - SAPS2+APACHE2, NYU Binned LOS - Lisbon Portugal, \nNYU Insurance Denial - Claim forms). NYU Comorbidity has no structured-data variant because the task is to impute comorbidity index with the lack of structured icd codes. Finally, we have  \na Named Entity Recognition (NER) dataset for testing how well NYUTron generalizes to different clinical predictive tasks using non-NYU data.\nArticle\nExtended Data Table 2 | Sizes and pretrain corpora for LLMs\nWe test 6 types of LLMs with different model sizes and different pretraining corpora. We list out the various corporate here as well as model parameter counts to facilitate ease of comparison. \nWe also note that one key distinction between web-wiki+bio+clinical and NYUTron, clinical is that the former was stripped of identifying information while the latter was not.\n\n\n",
  "topic": "Generalizability theory",
  "concepts": [
    {
      "name": "Generalizability theory",
      "score": 0.8298549652099609
    },
    {
      "name": "Software deployment",
      "score": 0.7250336408615112
    },
    {
      "name": "Computer science",
      "score": 0.6956379413604736
    },
    {
      "name": "Machine learning",
      "score": 0.5552404522895813
    },
    {
      "name": "Predictive modelling",
      "score": 0.5143100023269653
    },
    {
      "name": "Artificial intelligence",
      "score": 0.490026593208313
    },
    {
      "name": "Clinical decision support system",
      "score": 0.46437445282936096
    },
    {
      "name": "Language model",
      "score": 0.43003514409065247
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4155416786670685
    },
    {
      "name": "Health care",
      "score": 0.41089802980422974
    },
    {
      "name": "Data science",
      "score": 0.36859145760536194
    },
    {
      "name": "Decision support system",
      "score": 0.27173250913619995
    },
    {
      "name": "Psychology",
      "score": 0.14130446314811707
    },
    {
      "name": "Software engineering",
      "score": 0.106342613697052
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210086933",
      "name": "NYU Langone Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210127875",
      "name": "Nvidia (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210159872",
      "name": "Predictive Science (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 368
}