{
  "title": "Knowledge Injection into Dialogue Generation via Language Models",
  "url": "https://openalex.org/W3144328175",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5016343106",
      "name": "Yi-Lin Tuan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100323842",
      "name": "Wei Wei",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100702488",
      "name": "William Yang Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2982394217",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2807873315",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2970252402",
    "https://openalex.org/W2964352131",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W2735642330",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W2024472792",
    "https://openalex.org/W2963248296",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W2890969459",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1965555277",
    "https://openalex.org/W2963475460",
    "https://openalex.org/W2997892440",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2971199636",
    "https://openalex.org/W1608475977",
    "https://openalex.org/W2971236040",
    "https://openalex.org/W2962717182",
    "https://openalex.org/W2141708418",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2963790827",
    "https://openalex.org/W2963248455",
    "https://openalex.org/W2950457956",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2914204778",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2913443447",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2093976733",
    "https://openalex.org/W2963186354",
    "https://openalex.org/W2963085895",
    "https://openalex.org/W2586847566",
    "https://openalex.org/W2963559013",
    "https://openalex.org/W2145688713",
    "https://openalex.org/W2581637843",
    "https://openalex.org/W2734443755",
    "https://openalex.org/W2756487349",
    "https://openalex.org/W3103100151",
    "https://openalex.org/W1964475109"
  ],
  "abstract": "Dialogue generation has been successfully learned from scratch by neural networks, but tends to produce the same general response, e.g., \"what are you talking about?\", in many conversations. To reduce this homogeneity, external knowledge such as the speaker's profile and domain knowledge is applied as an additional condition to diversify a model's output. The required knowledge to develop an effective conversation, however, is not always available, which is different from prior work's assumption that a model always has acquired sufficient knowledge before chatting. This problem can be detrimental when applying a dialogue model like this chatting online with unconstrained people and topics, because the model does not have the needed knowledge. To address this problem, we propose InjK, which is a two-stage approach to inject knowledge into a dialogue generation model. First, we train a large-scale language model and query it as textual knowledge. Second, we frame a dialogue generation model to sequentially generate textual knowledge and a corresponding response. Empirically, when a dialogue generation model can only access limited knowledge, our method outperforms prior work by producing more coherent and informative responses.",
  "full_text": "Knowledge Injection into Dialogue Generation\nvia Language Models\nYi-Lin Tuan1, Wei Wei2, William Yang Wang1\n1University of California, Santa Barbara, USA\n2Google Research, Mountain View, USA\n{ytuan, william}@cs.ucsb.edu, wewei@google.com\nAbstract\nDialogue generation has been successfully\nlearned from scratch by neural networks, but\ntends to produce the same general response,\ne.g., “what are you talking about?”, in many\nconversations. To reduce this homogeneity, ex-\nternal knowledge such as the speaker’s pro-\nﬁle and domain knowledge is applied as an\nadditional condition to diversify a model’s\noutput. The required knowledge to develop\nan effective conversation, however, is not al-\nways available, which is different from prior\nwork’s assumption that a model always has\nacquired sufﬁcient knowledge before chatting.\nThis problem can be detrimental when apply-\ning a dialogue model like this chatting online\nwith unconstrained people and topics, because\nthe model does not have the needed knowl-\nedge. To address this problem, we propose\nInjK, which is a two-stage approach to Inj ect\nKnowledge into a dialogue generation model.\nFirst, we train a large-scale language model\nand query it as textual knowledge. Second, we\nframe a dialogue generation model to sequen-\ntially generate textual knowledge and a corre-\nsponding response. Empirically, when a dia-\nlogue generation model can only access lim-\nited knowledge, our method outperforms prior\nwork by producing more coherent and informa-\ntive responses.\n1 Introduction\nResearch in dialogue generation aims to develop\nmachines that can vividly converse with humans.\nOne predominant method to solve this task is learn-\ning a neural network from large-scale real con-\nversations (Vinyals and Le, 2015). However, this\napproach creates the problem that a generated re-\nsponse tends to be general (e.g., “I don’t know.”\nand “What are you talking about?” are responses\nacceptable for many cases) (Li et al., 2016a). One\nreason is that there are diverse valid responses to\nPerplexity\nA: Hi, I live in wine country, wrote a book at home, 17 with blue hair. you?\nB: I am a single mom with two kids and my job is an accountant.\nA: Thank you for sharing. You play with your children?\nUnavailable\nKnowledge\nno, but I like blue hair.\nI play with my son. I don't have\nany children but I love him.I play with my\ntwo boys. We\ngo camping\nin my honda\ncivic.\n0% 70% 100%\nFigure 1: An example of a knowledge-grounded di-\nalogue generation model test with insufﬁcient knowl-\nedge, where underlines indicate knowledge related\nwords. The model generates lower quality responses\nand rises perplexity when less knowledge is available.\nthe same dialogue, a method without proper de-\nsign can only learn one vague response instead of\nmemorizing multiple possibilities.\nTo generate more informative responses, recent\nworks have collected data with external knowl-\nedge for reference, such as Persona-Chat (Zhang\net al., 2018a; Dinan et al., 2020). It provides tex-\ntual proﬁles of the speakers involving in dialogues,\ne.g., “Speaker1’s favorite sport is ultimate frisbee.”\nThese proﬁles are often taken as the extra condi-\ntions for both training and testing the neural models.\nIn this way, we can prevent vague responses but\nadd speciﬁed information to them.\nHowever, in a real conversation, we do not al-\nways have sufﬁcient knowledge of a discussed topic\nas those artiﬁcially designed datasets. For instance,\nwe do not know a stranger’s background in advance,\nnor do we know a song’s production process for\nit is often not publicly available. Given that the\naccessible knowledge is actually limited, we con-\nducted an experiment simulating this insufﬁciency\nin Figure 1. The generated response is deviated\nfrom the conversation, resulting in much higher\narXiv:2004.14614v2  [cs.CL]  5 Apr 2021\nperplexity. This inspires us to tackle the problem\nof insufﬁcient knowledge in dialogue generation.\nTo solve knowledge insufﬁciency, we propose\nInjK, a uniﬁed method that does not require exter-\nnal knowledge in datasets. This approach formu-\nlates dialogue generation as a two-stage process.\nFirst, given an ongoing conversation, we conjec-\nture possible knowledge in a learned domain that\ncan be used for this dialogue. Second, the con-\njectured knowledge is used with the conversation\nto predict the response. More technically, the ap-\nproach disentangles semantics information from\nthe input message, maps it to a knowledge domain,\nthen takes the mapped one as the knowledge for\nresponse generation. This process can model the\nstochastic causality from unobserved knowledge to\nresponse, thus improving the contained information\nin responses.\nWe analyze the performance of state-of-the-art\nand our proposed approach on this newly proposed\ntask by how informative the generated responses\nare when limited knowledge for reference. We\nshow that prior methods will collapse when not\nenough amounts of knowledge can be referred to,\nwhile InjK performs well less regarding to how\nmuch knowledge is given. The results demonstrate\nthat considering the knowledge inefﬁciency in train-\ning methods for dialogue generation can improve\nuser experience in real-scenarios, in which little\nknowledge can be obtained beforehand.\nOur contributions are:\n• Discussion and simulation of knowledge in-\nsufﬁciency in dialogue generation.\n• Proposal of a stochastic causal model to learn\nlatent knowledge variable.\n• Evaluation of knowledge insufﬁciency in in-\nference time, showing that our proposed\nmodel achieves overall the best on two\ndatasets.\n2 Related Work\nAfter the success of learning neural conversation\nmodels (Vinyals and Le, 2015; Serban et al., 2016),\npeople have explored to generate more informative\nresponses.\nEarly approaches focused on learning more di-\nverse responses by introducing latent variables. (Li\net al., 2016a; Shao et al., 2017) re-ranked possi-\nble responses according to the mutual informa-\ntion between predictions and inputs. (Bowman\nx z \ny \nx \ny \nx z \ny \n(1) (2) (3) \nZ \nFigure 2: The graphical models of (1) standard di-\nalogue generation (2) knowledge-grounded dialogue\ngeneration (3) our proposed approach, where x repre-\nsents the dialogue history, yrepresents the response, z\nrepresents the paired knowledge in (2), which is not al-\nways accessible, or the inferred knowledge in (3), and\nZ represents the knowledge domain.\net al., 2016; Serban et al., 2017; Zhao et al., 2017;\nGao et al., 2019) matched the posteriors and pri-\nors of dialogue generation as variational autoen-\ncoders (Kingma and Welling, 2014) to reduce lost\ninformation in context. (Li et al., 2017; Xu et al.,\n2017; Zhang et al., 2018b; Xu et al., 2018; Tuan\nand Lee, 2019) utilized adversarial learning (Good-\nfellow et al., 2014) that was able to learn less\nvague distribution, thus providing more diverse\nresults. These approaches target to uncover under-\nlying knowledge in conversations, but not injecting\nnew information.\nBeyond modifying models, recent predomi-\nnant approaches were annotating conversations\nwith speakers’ proﬁles (Li et al., 2016b; Zhang\net al., 2018a; Shum et al., 2019; Zheng et al.,\n2020; Dinan et al., 2020) and topic-related knowl-\nedge (Ghazvininejad et al., 2018; Dinan et al., 2018;\nZhou et al., 2018; Moon et al., 2019; Raghu et al.,\n2019; Tuan et al., 2019). By increasing information\nin context, the responses were more informative\nthan only revising models. Since this advantage\nneeded intensive context-knowledge-response pairs\nin data, (Zhao et al., 2020; Li et al., 2020) devised\nmethods to reduce these required annotations in\ntraining set. However, the lack of knowledge in\ninference time is still an issue. This neglect can\ncause severe distribution shift problem when releas-\ning those dialogue models for applications.\n3 Preliminaries\n3.1 Dialogue Generation\nGiven a dialogue history x, which consists of mul-\ntiple previous turns in a conversation, the task di-\nalogue generation aims to predict the next turn y,\nwhich is a sequence of tokens. When modeling\ndialogue generation by neural networks, a stan-\ndard method maximizes the log-likelihood of gen-\nerating yby the loss function L = log P(y|x) =\nΣtlog P(yt|x,y<t) (Vinyals and Le, 2015).\nSome dialogue generation models assume that an\nexternal knowledge base is available. For each con-\nversation pair (x,y), a paired knowledge sample is\neither retrieved from the knowledge base or anno-\ntated. Here we focus on examples of textual knowl-\nedge and denote a sample knowledge sentence as\nz.1 Given this paired z, knowledge-grounded dia-\nlogue models are generally optimized by the loss\nfunction L= log P(y|x,z) (Ghazvininejad et al.,\n2018) that gains beneﬁts from zto restrict the vari-\nance of y.\nTheir graphical models are plotted in Fig-\nure 2(1)(2), where the standard model only learns\nthe causal relation from xto ybut the knowledge-\ngrounded model attributes yto xand z.\n3.2 Knowledge Insufﬁciency\nIn a real conversation, we may not have the required\nknowledge to develop an engaging response. For\nexample, if there is a conversation about “Game of\nThrones”, but we have not seen “Game of Thrones”\nand do not know what it is, we are less likely to\nengage in this conversation. We call this the insufﬁ-\ncient knowledge problem. With this lack, a person\nmight just copy a familiar term in the conversa-\ntion and said “What is the board game?” (however,\n“Game of Thrones” is a TV series), thus deviating\nfrom the conversation.\nConversations with insufﬁcient knowledge are\ncommon in natural discussion, where an interlocu-\ntor might not have adequate knowledge or expe-\nrience regarding the topic at hand and need to\nproduce exchangeable information relying on our\ncognitive system (Pask, 1976; Hammersley, 2003;\nTurnbull, 2003). We describe this scenario in Fig-\nure 2(3), where Z imitates the cognitive system\nto produce information z with the x’s assistance,\nwhile the model attributes yto xand the guessed z\nwithout access to external annotations.\n4 Knowledge-injected Dialogue Model\nWe propose a knowledge-injected dialogue model\n(InjK) that instead of “grounding on” external\nknowledge (Ghazvininejad et al., 2018; Zhou et al.,\n2018; Zhao et al., 2020; Li et al., 2020) “injects”\nknowledge into a model, so that this model can be\ntest with limited accessible knowledge.\n1Other forms of knowledge, e.g., knowledge graph, is left\nfor future work and can also be framed as sentences using\ntemplates.\nKnowledge Generation Model Knowledge LM\n(                         )\nResponse Generation Model\nx1 x2 xM......\nz1 z2 zK......\ny1 y2 yN......\nx1 x2 xM......\nz=z1z2...zK\nprobability of z\ndialogue history\nestimated\nknowledge\ngenerated response\nused to guide z\nFigure 3: The proposed method is composed of three\nmodules. First the dialogue history x = x1x2...xM is\nfed into knowledge generate model. The model then\ninfers an estimated knowledge ˆz = ˆz1 ˆz2...ˆzK. Finally,\nˆzand xare concatenated and fed into the response gen-\neration model to produce the response ˆy = ˆy1 ˆy2...ˆyN.\nDuring training, the estimated knowledge ˆz is fed into\nthe knowledge LM to maximize its probability.\n4.1 Motivation\nWe observe that when a talker is having a conversa-\ntion, she carries assumptions about the world that is\nsomewhat independent from what she heard in this\nconversation. The talker then develops a response\nby combining her received messages in this dia-\nlogue and her realization of the world. For example\nin a conversation, someone said “I once road on\nThe Royal Blue train from New York to D.C”, the\ntalker may have her “assumed knowledge” (may\nnot be the truth) that “People can see beautiful\nviews from the windows of a train”, therefore she\nsaid “Oh that sounds really nice. I bet there was a\nlot of scenery and blue skies.”\nIn this illustrative example, we can summarize\nthe relationships among “assumed knowledge”z,\ndialogue history x, and response y as the follow-\ning three points that were said to build causal-\nity (Hlav´aˇckov´a-Schindler et al., 2007) from xto y\nand zto y.\n• yfollows after xand z(time-ordering)\n• yrelates to xand z(covariation)\n• after removing x, ystill relates to z(xand z\nare confounding variables to y)\nMotivated by this observation, we cast dialogue\ngeneration as a stochastic causal model (Pearl,\n1987) and take a talker’s assumed knowledge as an\nunobserved variable zthat meets the above three\nconditions. We deﬁne the causality among x,y,zin\nequation 1, where Z is an abstract set of knowledge.\nP(y|x) = Ez∈ZP(y|x,z)P(z|x) (1)\n4.2 Model Details\nThe framework of our proposed InjK is depicted\nin Figure 3 and has three modules: a knowledge\nlanguage model (LM) θ, a knowledge generation\nmodel σ, and a response generation modelφ, where\nθ, σ, φare their parameters. We denote a textual\nknowledge as z= {zt}|z|\n1 ∈Z, where each zt is a\ntoken and |z|is the length of this knowledge.\nTest Phase. We ﬁrst feed a sequence xinto the\nknowledge generation model σand sample an es-\ntimated knowledge sequence ˆzfrom a probability\ndistribution Pσ(ˆz|x) that is parameterized by σ.\nNext, we take x and ˆz as the inputs to predict a\nresponse ˆyfrom distribution Pφ(ˆy|x,ˆz) using the\nresponse generation model. Note that we predict\nˆz and ˆy token-by-token until the eos-of-sentence\nsymbol is generated. We formulate this genera-\ntion process in test phase as equation 2, where the\nsymbol ∼means sampled from a distribution.\nˆz∼ΠtPσ(ˆzt|x,ˆz<t)\nˆy∼ΠtPφ(ˆyt|x,ˆz,ˆy<t) (2)\nTraining Phase. Similar to Test Phase, we sam-\nple ˆzfrom Pσ(ˆz|x), but use x, ˆz, and the previous\ntokens in ground-truth response y<t to predict to-\nken yt on t-th step. We then reﬁne equation 1 and\ndeﬁne the general objective as\nL(φ,σ) = Ez∈ZPσ(z|x)ΠtPφ(yt|x,z,y <t) (3)\nNext, we relax the constraint z ∈ Z by regu-\nlarizing a deduced z lying in Pθ(Z), which is\nparamerized by a knowledge LM θthat simulates\nthe distribution of possible knowledge. Particu-\nlarly, we train a knowledge LM by maximizing\nlog-likelihood on a collection of textual knowledge\nthat depends on used dataset. We then adopt the\nmode-seeking direction of Kullback-Leibler diver-\ngence(KLD) (Kullback and Leibler, 1951; Agarwal\net al., 2019) and formulate the loss functions as\nGeneration Models:\nL(φ,σ) = −log Pφ(y|x,z)Pσ(z|x)\n+ DKL(Pσ(z|x)||Pθ(z)) (4)\nKnowledge LM:\nL(θ) = −Σ|z|\nt=1 log Pθ(zt|z<t) (5)\nDetailed derivations are presented in Supplemen-\ntary Material.\nAlgorithm 1: INJK OPTIMIZATION\nData: dialogues D, textual knowledge Z\nInput: initialize knowledge LM (θ),\nknowledge generation model σ,\nresponse generation model φ\nInput: hyperparamters α, β, γ\nfor each pretraining iteration do\nsample a mini-batch zfrom Z\ncompute ∇θ as Eq. 9\nupdate θwith learning rate rθ by\nθ←φ+ r∇θ\nfor each training iteration do\nsample a mini-batch (x,y) from D\nsample ˆzfrom Pσ(z|x)\ncompute ∇φ as Eq. 9\ncompute ∇σ as Eq. 8\nupdate φand σwith learning rate rby\nφ←φ+ r∇φ\nσ←σ+ r∇σ\n4.3 Optimization Challenges\nWe propose two further steps to avoid the in-\ntractability of (a) DKL and (b) Pσ(z|x) in equa-\ntion 4.\nDKL When computing the KLD term in equa-\ntion 4, we have to multiply the probabilities of each\ntoken in z, which will grow exponentially w.r.t|z|\nand make this KLD term intractable. In practice,\nwe substitute the KLD with an upper bound in\nequation 6 and prove this bound in Supplementary\nMaterial.\nDKL(Pσ(z|x)||Pθ(z))\n≤ΣtDKL(Pσ(zt|x,z<t)||Pθ(zt|z<t))\n(6)\nPσ(z|x) We apply policy gradient (Sutton and\nBarto, 2018; Ranzato et al., 2016) to prevent from\nthe non-differentiable z in Pφ(y|x,z)Pσ(z|x),\nsince zis a sequence of discrete variables sampled\nfrom distribution Pσ(z|x). We cast this problem as\nreinforcement learning by framing the Pφ(y|x,z)\nas the reward after taking a sequence of actions\n{zt}|z|\n1 that are selected from the policy Pσ(z|x).\nTo reduce the high variance of policy gradient up-\ndate, we subtract the expected reward of taking\naction zt by a baseline b. The gradient of σfrom\nPφ(y|x,z) can thus be formulated as\n∇σ = Σ|z|\nt=1(Pφ(y|x,z) −b)∇σlog Pσ(zt|x,z<t)\n(7)\nOverall, we approximate the gradient of σ by\ncombining equation 6 and equation 7 as\n∇σ =\nΣ|z|\nt=1[β(Pφ(y|x,z) −b)∇σlog Pσ(zt|x,z<t)\n−γ∇σDKL(Pσ(zt|x,z<t)||Pθ(zt|z<t))]\n(8)\nwhile we compute the gradients of θand φby max-\nimizing log-likelihood as\n∇θ = Σ|z|\nt=1∇θlog Pθ(zt|z<t)\n∇φ = αΣ|y|\nt=1∇φlog Pφ(yt|x,z,y <t)\n(9)\nwhere α, β, γ are coefﬁcients to tune the impor-\ntance of each term. Empirically, the gradients work\nwell when α, βare set to 1, and γ ∈[0.1,1]. The\nparameters θ, φ, σ are then updated by gradient\nascent.\nThe learning algorithm is summarized in Algo-\nrithm 1.\n5 Experiments\nWe evaluate the knowledge insufﬁciency problem\nin test phase on dialogues that should be informa-\ntive but lack knowledge for reference.\n5.1 Datasets\nTo simulate usual scenarios that people are unfamil-\niar with a discussed topic, we design two datasets\nthat respectively does introduction and role-playing\ngame tasks, but take speakers’ proﬁles as the knowl-\nedge.\nConversations talking about proﬁles. We for-\nmulate Personachat (Zhang et al., 2018a; Dinan\net al., 2020) as two-speaker dialogues with a textual\ndescription of whom is to respond. The description\nis composed of multiple sentences about personali-\nties. When collecting the dialogues, the speakers\nwere asked to get to know each other following\ntheir assigned proﬁles. The resulting amounts are\n131438 / 7801 / 7504 for train/valid/test sets.\nConversations acting as proﬁles. We frame\nLIGHT (Urbanek et al., 2019) as two-speaker di-\nalogues with textual descriptions of characters in\nan adventure game. When collecting the dialogues,\nthe speaker was asked to act as the assigned pro-\nﬁles to converse with other characters. The re-\nsulting amounts are 102109 / 6123 / 12272 for\ntrain/valid/test sets.\nWe simulate the cases that the required knowl-\nedge is not accessible by masking the proﬁles. The\nmasked datasets are then challenging for that we\nneed to learn the underlying knowledge of a re-\nsponse without annotations but infer an engaging\nresponse that (a) directly talks about the knowledge\nand (b) implicitly acts following the knowledge.\n5.2 Baselines\nWe modiﬁed state-of-the-art dialogue models for\nthis task as baselines. For fairness, we imple-\nmented them based on the same transformer ar-\nchitecture. MLE (Vinyals and Le, 2015), stands\nfor maximum likelihood estimation, that trains a\nneural conversation model with only dialogue his-\ntory as inputs, and then predicts the response. The\nloss function is L = −log P(y|x). CV AE(Zhao\net al., 2017; Serban et al., 2017; Gao et al., 2019),\nstands for conditional variational autoencoder, that\nﬁrst predicts a discrete latent variable z′, which\nis trained as the posterior of the dialogue his-\ntory and the response. Then the method uses\nz′ and dialogue history to predict the response.\nThe loss function is L = −log P(y|x,z′) +\nDKL(P(z′|x)||P(z′|x,y)). KGuide (Zhao et al.,\n2017) that trains a model to predict the paired\nknowledge ˜zand then takes the prediction and dia-\nlogue history as inputs to generate the response.\nThe loss function is L = −log P(y|z,x) −\nlog P(˜z|x). KGround (Wolf et al., 2019) that\ntrains a model by concatenating the paired knowl-\nedge ˜zand the dialogue history, and takes them as\ninputs to generate the response. The loss function\nis L = −log P(y|˜z,x). We prevent some recent\nwork such as (Zhao et al., 2020; Li et al., 2020)\nfrom comparison since they require the presence\nof a large knowledge base in test time, which is not\nfair to compete in our setup.\n5.3 Implementation Details\nWe implemented the models based on trans-\nformer (Vaswani et al., 2017; Liu et al., 2018)\nand started the training from the GPT model\n(110M) (Radford et al., 2018) pretrained on\nBooksCorpus (Zhu et al., 2015).2 We append to the\ninputs with speaker embeddings, which indicate\na token is of whom. Besides log-likelihood loss,\nthe last hidden state is passed to a fully-connected\nlayer to classify if a response is the ground-truth\n2We implement the code with (Wolf et al., 2020)\nand refer to https://github.com/huggingface/\ntransfer-learning-conv-ai .\n0 5 10 15 20\nlength of textual knowledge\n19.2\n19.4\n19.6\n19.8\n20.0\n20.2\n20.4ppl\nPersonachat\n0 5 10 15 20\nlength of textual knowledge\n24\n25ppl\nLIGHT\n0 5 10 15 20\nlength of textual knowledge\n61\n62\n63\n64\n65\n66hits@1\nPersonachat\n0 5 10 15 20\nlength of textual knowledge\n58\n59\n60\n61\n62\n63\n64\n65hits@1\nLIGHT\nMLE\nCVAE\nInjK\n(a) Comparison to models trained without paired knowledge.\n0 5 10 15 20\nlength of textual knowledge\n18\n19\n20\n21\n22\n23\n24\n25ppl\nPersonachat\n0 5 10 15 20\nlength of textual knowledge\n23\n24\n25\n26ppl\nLIGHT\n0 5 10 15 20\nlength of textual knowledge\n52\n54\n56\n58\n60\n62\n64\n66\n68hits@1\nPersonachat\n0 5 10 15 20\nlength of textual knowledge\n54\n56\n58\n60\n62\n64\n66hits@1\nLIGHT\nKGuide\nKGround\nInjK\n(b) Comparison to models trained with paired knowledge.\nFigure 4: Evaluation of insufﬁcient knowledge (length of textual knowledge) by perplexity (ppl) and hits@1 on\nPersonachat and LIGHT datasets.\nPersonachat LIGHT\nModel PPL Hits@1 BLEU-1 BLEU-2 Average PPL Hits@1 BLEU-1 BLEU-2 Average\nMLE 19.9 62.5 11.13 4.75 0.8657 24.5 61.9 8.84 2.90 0.8566\nCV AE 20.0 64.1 11.19 4.87 0.8644 23.9 62.9 8.97 2.89 0.8622\nKGuide 20.7 63.8 11.02 4.87 0.8646 23.9 63.5 9.06 2.98 0.8639\nKGround 20.0 59.4 10.61 4.89 0.8608 23.3 62.4 8.72 2.88 0.8641\nInjK 19.5 64.9 11.38 4.92 0.8658 23.2 65.0 9.42 3.12 0.8657\nTable 1: Automatic evaluation on Personachat and LIGHT with limited textual knowledge (length=10).\ncompared with randomly sampled candidates. The\nmodel is trained on one Titan RTX with a batch size\n4, a maximum input length 512, and a maximum\noutput length 20. The α,β,γ in Algorithm 1 are\nset as 1 but γ for LIGHT is set as 0.1. The learn-\ning rate is 0.0000625 with linearly decay to 0 in 3\nepochs and the optimizer is AdamW (Loshchilov\nand Hutter, 2019).\n5.4 Results\nWe automatically evaluate the models by perplexity\n(PPL) of the ground-truth responses and Hits@1\namong 20 candidate responses following (Zhang\net al., 2018a; Urbanek et al., 2019). PPL is the\nlower the better while Hits@1 is the higher the bet-\nter. We also provide BLEU and average embedding\ncosine similarity 3 in Table 1, where InjK performs\nbetter or comparable to the baselines with limited\nknowledge available.\nIn Figure 4, we simulate the knowledge insuf-\nﬁciency problem by testing a dialogue generation\nmodel with limited textual knowledge. The x-axis\nin the plots is speciﬁcally the length of a randomly\nselected sub-sequence in the paired knowledge.\n3https://github.com/Maluuba/nlg-eval\nAs shown in Figure 4, most PPL curves de-\ncline and most Hit@1 curves rise along with the\namount of accessible knowledge. This validate\nour assumption that z is a confounding variable\nto y in Section 4, thus enabling our further ex-\nperiments. When comparing InjK with MLE and\nCV AE that also do not trained with paired knowl-\nedge, as presented in sub-ﬁgures 4(1)(2), we ob-\nserve that InjK outperforms MLE and CV AE along\ndifferent extents of knowledge insufﬁciency. This\ndemonstrates that InjK can learn how to more efﬁ-\nciently utilize knowledge even when no annotations\nare available for training. When comparing InjK\nwith KGuide and KGround that are trained with\nannotations, as seen in sub-ﬁgures 4(3)(4), we ob-\nserve that InjK outperforms KGuide and KGround\nwhen test with limited obtainable knowledge. This\nshows that KGuide and KGround can suffer from\nsevere dataset shift when they are trained with an-\nnotations but test with limited amounts (we will\nfurther analyze this shift in Section 6.3). This com-\nparison also indicates that InjK can perform well\nregardless of knowledge insufﬁciency.\nInput Response\nA: wow, you need a hobby to get away, like jujitsu or running.\nB: i like running, but i have no time.\nA: well i sure can understand that. running does relieve stress\nthough\nB: it does, i like to wear my favorite color blue when i do run.\nA: when you have the time where do you run?\nMLE: i run to the gym in my spare time.\nCV AE: i like torun in the woods.\nKGuide: anywhere but the park.\nKGround: the farm i grew up on is a farm, but i like to\nrun in the ﬁelds.\nInjK: i run to the park near my house.\nInput Response\nA: i mean that’s where i live now how about you\nB: big city now. helps with my career in singing\nA: so what do you see any music you sing\nMLE: i like rock music and i sing in front of a bunch\nof people.\nCV AE:rock, pop, rap, alternative. i also like to hike.\nKGuide: i like to sing and sing on the weekends.\nKGround: mostly horror movies. i watch a lot of them.\nInjK: i sing in a church choir.\nTable 2: Examples of generated responses with limited textual knowledge (length=10) of Personchat. Bold texts\nindicate the related knowledge parts in the inputs and responses.\nCoh Info\nwin tie κ win tie κ\nInjK vs MLE 46.0 17.1 .59 38.6 23.6 .52\nInjK vs CV AE 45.4 16.1 .49 34.0 27.3 .53\nTable 3: Human evaluation of coherence (Coh) and in-\nformativeness (Info) on Personachat without parallel\nproﬁles. Note that Info is designed to be independent\nfrom Coh by only considering responses but no inputs,\nso Coh and Info should be used together to access if a\nresponse gives needed information.\n5.5 Human Evaluation\nWe conducted human evaluation mainly on Per-\nsonachat and focused on models trained with-\nout knowledge. We adopted softmax sampling\nwith temperature set to 0.7 to generate the re-\nsponses (Wolf et al., 2019) with no knowledge\nby each model and randomly selected 200 history-\nresponse pairs for evaluation. Next, we invited 3 na-\ntive speakers with over 95% approval rate on Ama-\nzon Mechanical Turk (MTurk) (Buhrmester et al.,\n2016) to judge the models’ performance. They\nwere asked to compare two generated responses\naccording to coherence (Coh) and informativeness\n(Info). The instruction is given as, (a) Coh: the\nsentence is coherent to the semantics of the con-\nversation history, and is a proper response to the\nconversation history; (b) Info: the sentence pro-\nvides some information about the speakers, scenes,\nor background. The sentence should not be a gen-\neral question nor be meaningless, such as “I don’t\nknow”. They were asked to choose which response\nis better on each criterion or the responses are tied.\nThe human evaluation results are listed in Ta-\n0 5 10 15 20 25 30\nlength of textual knowledge\n61\n62\n63\n64\n65\n66\n67hits@1\nPersonachat\nInjK\n-pg\n-kld\n0 5 10 15 20 25 30\nlength of textual knowledge\n58\n59\n60\n61\n62\n63\n64\n65hits@1\nLIGHT\nInjK\n-pg\n-kld\nFigure 5: The ablation study of InjK. -pg and -kld are\nthe results when the policy gradient and KLD terms\ndrop off from equation 7.\nble 3, where the annotators’ agreementsκare mea-\nsured by Fleiss’ kappa (Fleiss, 1971). According\nto an interpretation of Fleiss’ kappa (Landis and\nKoch, 1977), the annotators’ agreements are moder-\nate. The results show that InjK has better coherence\nthan both MLE and CV AE, but their contained in-\nformation is about the same level when testing with-\nout paired knowledge. This may because that with-\nout the guidance of knowledge, MLE and CV AE\ngenerate some randomized semantics that annota-\ntors may take the semantics to be informative, but\nthe responses can actually distract users from the\nconversation.\n6 Analysis\nWe study generated examples in subsection 6.1,\nthe necessity of policy gradient and KLD terms in\nequation 7 in 6.2, and our proposed knowledge gap\nin subsection 6.3.\n6.1 Qualitative Analysis\nSome examples are presented in Table 2, where\nMLE and CV AE generate acceptable response\nbut slightly deviate from the focus, KGuide and\nKGround generate diverse ngrams but less likely\nto maintain the ﬂow, InjK shows its possibility to\nexchange precise information in conversations.\n6.2 Ablation Study\nWe respectively drop the policy gradient (pg) and\nKLD terms off from equation 7 in InjK and present\nthe results in Figure 5. As can be seen, the impacts\nof deleting pg and KLD terms vary on Personachat\nand LIGHT, but including both terms generally\nachieves the best performance.\n6.3 Knowledge Gaps Analysis\nTo analyze the slope of curves in Figure 4, we de-\nﬁne knowledge gap as the variance of PPL w.r.t.\nknowledge amounts to measure the importance of\nthe presence of sufﬁcient external knowledge to a\ndialogue model. Formally, we formulate the knowl-\nedge gap of a dialogue model as the standard de-\nviation of PPL over tests with different lengths of\ntextual knowledge. Moreover, we deﬁne the aver-\nage knowledge gap in training phase of a dataset as\n1\nnum(k)Σkstdevi({PPL(Pφ(i) (y|x,zk))}i)\n(10)\nwhere φ(i) is the parameters of i-th model, stdevi\nmeans standard deviation over i, and kdenotes the\nlength of textual knowledge. This training phase\nknowledge gap is the average value over kof the\nPPL standard deviation across models and aims to\nquantify the importance of using knowledge for\ntraining.\nWe deﬁne the knowledge gap in testing phase of\na dataset as\n1\nnum(i)Σistdevk({PPL(Pφ(i) (y|x,zk))}k)\n(11)\nwhich means the average value over models of the\nPPL standard deviation across length of textual\nknowledge. This value targets to quantify the im-\nportance of using knowledge in test time.\nKnowledge gaps in training v.s test phases. As\nshown in Figure 6(1), the average knowledge gaps\nin test phase are comparable to the ones in train-\ning phase. This indicates that test with sufﬁcient\nknowledge is as inﬂuential as training, whereas\nprior work only focus on the problem in training\ntime.\nPersonachat LIGHT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2stdev(PPL)\ntrain\ntest\nPersonachat LIGHT\n0.0\n0.5\n1.0\n1.5\n2.0stdev(PPL)\nMLE\nCVAE\nKGuide\nKGround\nInjK\nFigure 6: Comparison of (1) the knowledge gaps in\ntraining and test phase, as well as (2) the knowledge\ngaps of different models, which are deﬁned as equa-\ntions 10 and 11, for each dataset.\nKnowledge gaps of different models. We draw\nthe knowledge gaps of different models in Fig-\nure 6(2). As can be seen, KGuide and KGround\nhighly depend on the amounts of knowledge. This\nshows they fail to effectively utilize the dialogue\nhistory and unlikely generate knowledge as a cog-\nnitive process. MLE, CV AE, and InjK, with nar-\nrower knowledge gaps, can optimize the usage of\ndialogue history. Among them, InjK generally has\nthe smallest knowledge gap and performs better\nin Figure 4, showing that considering knowledge\ninsufﬁciency in both training and test is more likely\nto embed cognition in a dialogue model.\n7 Conclusion\nWe formulate the insufﬁcient knowledge problem\nin a real conversation where speakers cannot ac-\ncess needed knowledge to respond. To deal with\nthis problem, we propose InjK that injects knowl-\nedge into a dialogue model by regularizing a latent\nvariable being knowledgeable. Empirically, we an-\nalyze the impact of knowledge insufﬁciency in both\ntraining and testing phases. The results show that\nInjK outperforms state-of-the-art baselines when\nlimited knowledge is obtainable in applications.\n8 Ethical Considerations\nThis work trains the models on crowd-source\ndatasets from prior work, thus may resulting in\nsome toxic knowledge in generated responses, such\nas bias and misinformation.\nReferences\nRishabh Agarwal, Chen Liang, Dale Schuurmans, and\nMohammad Norouzi. 2019. Learning to generalize\nfrom sparse and underspeciﬁed rewards. In Interna-\ntional Conference on Machine Learning, pages 130–\n140. PMLR.\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Con-\nference on Computational Natural Language Learn-\ning, pages 10–21, Berlin, Germany. Association for\nComputational Linguistics.\nMichael Buhrmester, Tracy Kwang, and Samuel D\nGosling. 2016. Amazon’s mechanical turk: A new\nsource of inexpensive, yet high-quality data?\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\nAlexander Miller, Kurt Shuster, Jack Urbanek,\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\nLowe, et al. 2020. The second conversational in-\ntelligence challenge (convai2). In The NeurIPS’18\nCompetition, pages 187–208. Springer.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In International Conference on Learning\nRepresentations.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin ,\n76(5):378.\nJun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, Guodong\nZhou, and Shuming Shi. 2019. A discrete cvae for\nresponse generation on short-text conversation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1898–\n1908.\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei\nChang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and\nMichel Galley. 2018. A knowledge-grounded neural\nconversation model. In Thirty-Second AAAI Confer-\nence on Artiﬁcial Intelligence.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Advances in neural information\nprocessing systems, pages 2672–2680.\nClive WJ Granger. 2004. Time series analysis, cointe-\ngration, and applications. American Economic Re-\nview, 94(3):421–425.\nMartyn Hammersley. 2003. Conversation analysis and\ndiscourse analysis: methods or paradigms? Dis-\ncourse & Society, 14(6):751–781.\nKaterina Hlav ´aˇckov´a-Schindler, Milan Palu ˇs, Martin\nVejmelka, and Joydeep Bhattacharya. 2007. Causal-\nity detection based on information-theoretic ap-\nproaches in time series analysis. Physics Reports ,\n441(1):1–46.\nDiederik P Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. stat, 1050:1.\nSolomon Kullback and Richard A Leibler. 1951. On\ninformation and sufﬁciency. The annals of mathe-\nmatical statistics, 22(1):79–86.\nJ Richard Landis and Gary G Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nbiometrics, pages 159–174.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016a. A diversity-promoting objec-\ntive function for neural conversation models. InPro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n110–119.\nJiwei Li, Michel Galley, Chris Brockett, Georgios Sp-\nithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A\npersona-based neural conversation model. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 994–1003.\nJiwei Li, Will Monroe, Tianlin Shi, S ˙ebastien Jean,\nAlan Ritter, and Dan Jurafsky. 2017. Adversarial\nlearning for neural dialogue generation. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 2157–2169.\nLinxiao Li, Can Xu, Wei Wu, Yufan Zhao, Xueliang\nZhao, and Chongyang Tao. 2020. Zero-resource\nknowledge-grounded dialogue generation. pre-\nproceedings of NeurIPS.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summariz-\ning long sequences. In International Conference on\nLearning Representations.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-\njen Subba. 2019. Opendialkg: Explainable conver-\nsational reasoning with attention-based walks over\nknowledge graphs. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 845–854.\nGordon Pask. 1976. Conversation theory. Applications\nin Education and Epistemology.\nJudea Pearl. 1987. Evidential reasoning using stochas-\ntic simulation of causal models. Artiﬁcial Intelli-\ngence, 32(2):245–257.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nDinesh Raghu, Nikhil Gupta, et al. 2019. Disentan-\ngling language and knowledge in task-oriented di-\nalogs. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n1239–1255.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2016. Sequence level train-\ning with recurrent neural networks. International\nConference on Learning Representations.\nIulian V Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron Courville, and Joelle Pineau. 2016. Building\nend-to-end dialogue systems using generative hier-\narchical neural network models. In Thirtieth AAAI\nConference on Artiﬁcial Intelligence.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron Courville, and\nYoshua Bengio. 2017. A hierarchical latent variable\nencoder-decoder model for generating dialogues. In\nThirty-First AAAI Conference on Artiﬁcial Intelli-\ngence.\nYuanlong Shao, Stephan Gouws, Denny Britz, Anna\nGoldie, Brian Strope, and Ray Kurzweil. 2017. Gen-\nerating high-quality and informative conversation re-\nsponses with sequence-to-sequence models. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing , pages 2210–\n2219.\nMichael Shum, Stephan Zheng, Wojciech Kry ´sci´nski,\nCaiming Xiong, and Richard Socher. 2019. Sketch-\nﬁll-ar: A persona-grounded chit-chat generation\nframework. arXiv preprint arXiv:1910.13008.\nRichard S Sutton and Andrew G Barto. 2018. Rein-\nforcement learning: An introduction. MIT press.\nYi-Lin Tuan, Yun-Nung Chen, and Hung-yi Lee.\n2019. Dykgchat: Benchmarking dialogue genera-\ntion grounding on dynamic knowledge graphs. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1855–\n1865.\nYi-Lin Tuan and Hung-Yi Lee. 2019. Improv-\ning conditional sequence generative adversarial net-\nworks by stepwise evaluation. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing ,\n27(4):788–798.\nWilliam Turnbull. 2003. Language in action: Psycho-\nlogical models of conversation. Psychology Press.\nJack Urbanek, Angela Fan, Siddharth Karamcheti,\nSaachi Jain, Samuel Humeau, Emily Dinan, Tim\nRockt¨aschel, Douwe Kiela, Arthur Szlam, and Ja-\nson Weston. 2019. Learning to speak and act in\na fantasy text adventure game. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 673–683.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nOriol Vinyals and Quoc Le. 2015. A neural conver-\nsational model. ICML Deep Learning Workshop,\n2015.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A trans-\nfer learning approach for neural network based con-\nversational agents. NeurIPS 2018 CAI Workshop.\nJingjing Xu, Xuancheng Ren, Junyang Lin, and\nXu Sun. 2018. Diversity-promoting gan: A cross-\nentropy based generative adversarial network for di-\nversiﬁed text generation. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3940–3949.\nZhen Xu, Bingquan Liu, Baoxun Wang, Cheng-Jie Sun,\nXiaolong Wang, Zhuoran Wang, and Chao Qi. 2017.\nNeural response generation via gan with an approx-\nimate embedding layer. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 617–626.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018a. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 2204–\n2213.\nYizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,\nXiujun Li, Chris Brockett, and Bill Dolan. 2018b.\nGenerating informative and diverse conversational\nresponses via adversarial information maximization.\nIn Advances in Neural Information Processing Sys-\ntems, pages 1810–1820.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi.\n2017. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 654–664.\nXueliang Zhao, Wei Wu, Chongyang Tao, Can Xu,\nDongyan Zhao, and Rui Yan. 2020. Low-resource\nknowledge-grounded dialogue generation. In Inter-\nnational Conference on Learning Representations.\nYinhe Zheng, Rongsheng Zhang, Xiaoxi Mao, and\nMinlie Huang. 2020. A pre-training based personal-\nized dialogue generation model with persona-sparse\ndata. AAAI.\nHao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,\nJingfang Xu, and Xiaoyan Zhu. 2018. Com-\nmonsense knowledge aware conversation generation\nwith graph attention. In Proceedings of the 27th\nInternational Joint Conference on Artiﬁcial Intelli-\ngence, pages 4623–4629.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 19–\n27.\nA Derivation of KLD upper bound\nProof.\nDKL(Pσ(z|x)||Pθ(z))\n= Σz∈ZPσ(z|x) log Pσ(z|x)\nPθ(z)\n= Σz∈ZPσ(z|x)[log Pσ(z|x) −log Pθ(z)]\n= Σ\nz∈Z\nPσ(z|x)[Σ\nk\nlog Pσ(zk|x,z1:k−1) −Σ\nk\nlog Pθ(zk|z1:k−1)]\n= Σz∈ZΣkPσ(z|x) log Pσ(zk|x,z1:k−1)\nPθ(zk|z1:k−1)\n≤ΣzΣkPσ(zk|x,z1:k−1) log Pσ(zk|x,z1:k−1)\nPθ(zk|z1:k−1)\n= ΣkDKL(Pσ(zk|x,z1:k−1)||Pθ(zk|z1:k−1))\n(12)\nThe last inequality holds since Pσ(z|x) =∏\nkPσ(zk|x,z1:k−1),\nPσ(z|x) ≤Pσ(zk|x,z1:k−1), for any k (13)\nB Derivation of InjK\nRecall that in Method Section in the main content,\nwe deﬁne notations x for dialogue history, y for\nresponse, and zfor the knowledge. To clarify more,\nwe deﬁne ˜yas the ground-truth response, ˆyas the\ngenerated response, ˜z as the labeled knowledge,\nand ˆzas the estimated knowledge. When using y\nor z, we do not specify its status but represent it as\nthe variable for description. Moreover, we denote\nthe knowledge domain as Z and assume all z∈Z.\nBorrowing the concept of Causal-\nity (Hlav ´aˇckov´a-Schindler et al., 2007), we\ncan suppose z is a confounding variable besides\nx of y since it contains extra information for y\nthat xdoes not have. However, when having no\nknowledge during training, z is an unobserved\nvariable that needs us to discover. As the concept\nof Causality, variable zis a cause to yother than x\nif and only if P(y|x,z) > P(y|x). This means z\ngives extra, useful information to infer y. We can\nrewrite the inequality in a more realistic scenario\nas the following.\nLemma 1. given (x,˜y,˜z) ∼PD, where PD is the\nreal data distribution, and ˜zcontains information\nuseful to ˜yand different from x.\nmax\nφ\nPφ(˜y|x) <max\nθ\nPθ(˜y|x,˜z) (14)\nwhere φ is the parameters of the generative\nmodel that only takes xas input, and θis the ones\nthat also takes ˜zas input.\nWe further deﬁne a residual functionϵas follows\nto quantify the difference between maxφPφ(˜y|x)\nand maxθPθ(˜y|x,z). This function is continuous\nwith respect to xand z. We note that here we use z\nbut not ˜zbecause we will ﬁnd out how to learn the\nzto maximize the residual. Also, in the following,\nwe will keep usingyin replacement of ˜yto simplify\nthe notations.\nDeﬁnition 1. Deﬁne ϵ(z), for (x,y) ∼PD and\nz ∈ Z, is the residual of log Pθ∗(y|x,z) −\nlog Pφ∗(y|x), where θ∗is the optimal parameters\nof P(y|x,z) and φ∗is the optimal parameters of\nP(y|x). The lower bound of ϵ(z) can be derived\nas follows.\nϵ(z) ≥log Pφ∗(x,y,z )\nPD(x,y)PD(z) (15)\nThe derivation is as follows. Note that the\nprior distributions P(x), P(y), P(z) are avail-\nable from the dataset. Therefore we label them\nwith subscript D and we do not need to model\nthese priors. In addition, we assume that P(x)\nand P(z) are identically independent distributed,\ni.e., P(x,z) = P(x)P(z), which holds in some\nreal-world cases and also satisﬁes the deﬁnition in\nGranger causality (Granger, 2004), which indicates\nthat zshould contain some unique information that\ndoes not exist in other variables.\nϵ(z) := log Pθ∗(y|x,z) −log Pφ∗(y|x)\n≥log Pφ∗(y|x,z) −log Pφ∗(y|x)\n= log Pφ∗(x,y,z )\nPφ∗(x,z)\nPφ∗(x)\nPφ∗(x,y)\n≥log Pφ∗(x,y,z )PD(x)\nPD(x)PD(z)PD(x,y)\n= log Pφ(x,y,z )\nPD(x,y)PD(z)\n(16)\nTo ﬁnd out the latent z that contributes most\nto the generation of y, our aim is to sample a\nˆz ∼ pσ(z|x) which makes pφ∗(y|x,ˆz) as close\nas possible to pθ∗(y|x,˜z), that is to maximize the\nresidual ϵ(ˆz).\nTheorem 1. The target to ﬁnd ˆz that makes\nlog Pφ(y|x,ˆz) as close as possible to the upper\nbound log Pθ∗(y|x,˜z) is formulated as:\nmin\nσ\nlog Pθ∗(y|x,˜z) −log Pφ(y|x,ˆz) (17)\nThe tighter lower bound is:\narg max\nσ\nlog Pφ(x,y, ˆz), subject to min\nσ\nI(x; ˆz)\n(18)\nNote that we suppose a φthat is able to utilize z.\nProof. because ˜zis the optimal zand ˆzis an arbi-\ntrary z, the inequalities hold\npφ(y|x) <pφ(y|x,ˆz) ≤pθ∗(y|x,˜z) (19)\nTherefore, to optimize the model with respect\nto ˆzis the same as to minimize the difference be-\ntween residuals ϵ(ˆz) and ϵ(˜z), since ˆzis based on\nφ and does not change the lower bound of ϵ in\nDeﬁnition 1.\narg min\nσ\nlog Pθ∗(y|x,˜z) −log Pφ(y|x,ˆz)\n= arg min\nσ\nϵ(˜z) −ϵ(ˆz)\n= arg max\nσ\nϵ(ˆz)\n(20)\nWe are not able to compute the gradient of σ of\nmaximizing ϵ(ˆz) since the process ˆz∼Pσ(z|x) is\nnot differentiable. Therefore, we optimize its lower\nbound, at least to give the ϵ(ˆz) a tighter constraint.\narg max\nσ\nlog Pφ(x,y, ˆz)\nsubject to Pφ(x,ˆz) = PD(x)PD(ˆz)\n(21)\nBecause logarithm an increasing function, we\nchoose to optimize its variable:\n∇σPφ(x,y, ˆz) ≈ 1\nm\nm∑\ni=1\npφ(yi|xi,ˆzi)∇σlog pσ(ˆzi|xi)\n(22)\nwhere mis the batch size.\nThe constraint Pφ(x,z) = PD(x)PD(z) can be\nrewritten as minimizing the mutual information of\nx and z variables I(X; Z) = Ep(x,z) log p(x,z)\np(x)p(z) =\nEp(x)DKL(p(z|x)||p(z)).\nC Computational Costs\nMethod Training (hrs)\nKGround ∼6\nMLE ∼5\nCV AE ∼8\nInjK ∼13\nTable 4: The required training times of the methods on\nPersonachat.\nSince CV AE samples one more token and InjK\nsamples a sequence of tokens for computing KL\ndivergence, CV AE consumes higher computational\ncosts than MLE and InjK requires even more. To\nreduce this training cost is a worth discussing future\nwork.\nD Validation Results\nMethod Hits@1 PPL\n(Persona-Chat, w/ paired knowledge)\nKGround∗ 78.96 15.34\nMLE 63.81 18.88\nCV AE 65.15 18.20\nInjK 68.12 17.94\n(LIGHT, w/ paired knowledge)\nKGround∗ 69.31 22.05\nMLE 54.16 23.67\nCV AE 54.91 23.39\nInjK 60.25 22.92\nTable 5: The results on validation set.",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8302494883537292
    },
    {
      "name": "Conversation",
      "score": 0.6142846345901489
    },
    {
      "name": "Domain knowledge",
      "score": 0.6060700416564941
    },
    {
      "name": "Language model",
      "score": 0.5666661262512207
    },
    {
      "name": "Scratch",
      "score": 0.4907423257827759
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4596298635005951
    },
    {
      "name": "Frame (networking)",
      "score": 0.4574853479862213
    },
    {
      "name": "Natural language processing",
      "score": 0.38665375113487244
    },
    {
      "name": "Linguistics",
      "score": 0.14090141654014587
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": []
}