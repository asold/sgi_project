{
  "title": "LogFiT: Log Anomaly Detection Using Fine-Tuned Language Models",
  "url": "https://openalex.org/W4391235307",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4328093882",
      "name": "Crispin Almodovar",
      "affiliations": [
        "Central Queensland University"
      ]
    },
    {
      "id": "https://openalex.org/A2021096729",
      "name": "Fariza Sabrina",
      "affiliations": [
        "Central Queensland University"
      ]
    },
    {
      "id": "https://openalex.org/A2780609653",
      "name": "Sarvnaz Karimi",
      "affiliations": [
        "Data61",
        "Commonwealth Scientific and Industrial Research Organisation"
      ]
    },
    {
      "id": "https://openalex.org/A2081522951",
      "name": "Salahuddin Azad",
      "affiliations": [
        "Central Queensland University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6929356459",
    "https://openalex.org/W3183619936",
    "https://openalex.org/W4225014872",
    "https://openalex.org/W2888115557",
    "https://openalex.org/W2767094836",
    "https://openalex.org/W6790849562",
    "https://openalex.org/W3127712067",
    "https://openalex.org/W2947815220",
    "https://openalex.org/W3095840026",
    "https://openalex.org/W3167623949",
    "https://openalex.org/W6688220550",
    "https://openalex.org/W3198845989",
    "https://openalex.org/W4205965165",
    "https://openalex.org/W6801354200",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6989671925",
    "https://openalex.org/W6793953445",
    "https://openalex.org/W2962703433",
    "https://openalex.org/W4225697716",
    "https://openalex.org/W2986944522",
    "https://openalex.org/W3007878096",
    "https://openalex.org/W4324007233",
    "https://openalex.org/W2965838158",
    "https://openalex.org/W4385605314",
    "https://openalex.org/W6763121668",
    "https://openalex.org/W6786301958",
    "https://openalex.org/W6796768152",
    "https://openalex.org/W6787995345",
    "https://openalex.org/W3040266635",
    "https://openalex.org/W6758101687",
    "https://openalex.org/W3194035167",
    "https://openalex.org/W6763229968",
    "https://openalex.org/W6757883768",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6776048684",
    "https://openalex.org/W2963587345",
    "https://openalex.org/W6750253784",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W6810879928",
    "https://openalex.org/W2039157918",
    "https://openalex.org/W2107263349",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W4391235307",
    "https://openalex.org/W1953585660",
    "https://openalex.org/W6748816842",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W3135550350",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2910068345",
    "https://openalex.org/W3169622372",
    "https://openalex.org/W4221149036",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W3195577433"
  ],
  "abstract": "System logs are a valuable source of information for monitoring and maintaining the security and stability of computer systems. Techniques based on Deep Learning and Natural Language Processing have demonstrated effectiveness in detecting abnormal behaviour from these system logs. However, existing anomaly detection approaches have limitations in terms of flexibility and practicality. Techniques that rely on log templates such as DeepLog and LogBERT fail to capture semantic information and are unable to handle variability in log content. On the other hand, classification-based approaches such as LogSy, LogRobust and HitAnomaly require time-consuming data labelling for supervised training. In this paper, a novel log anomaly detection model named LogFiT is proposed. The LogFiT model doesn't make use of a vocabulary of log templates and it doesn't require any labeled data as the model only requires self-supervised training. The LogFiT model uses a pretrained Bidirectional Encoder Representations from Transformers (BERT)-based language model fine-tuned to recognise the linguistic patterns of the normal log data. The LogFiT model is trained using masked sentence prediction on the normal log data only. Consequently, when presented with the new log data, the model's top-k token prediction accuracy serves as a threshold for determining whether the new log data deviates from the normal log data. Experimental results show that LogFiT's F1 score exceeds that of baselines on the HDFS, BGL, and Thunderbird datasets. Critically, when variability is introduced in the log data during evaluation, LogFiT retains its effectiveness compared to that of baselines.",
  "full_text": "1\nLogFiT: Log Anomaly Detection using Fine-Tuned\nLanguage Models\nCrispin Almodovar1 Fariza Sabrina1 Sarvnaz Karimi2 Salahuddin Azad1\n1Central Queensland University, Rockhampton, Australia\n2CSIRO Data61, Sydney, Australia\ncrispin.almodovar@cqumail.com\n{f.sabrina,s.azad}@cqu.edu.au\n{sarvnaz.karimi}@csiro.au\nAbstract—System logs are a valuable source of information\nfor monitoring and maintaining the security and stability of\ncomputer systems. Techniques based on Deep Learning and\nNatural Language Processing have demonstrated effectiveness in\ndetecting abnormal behaviour from these system logs. However,\nexisting anomaly detection approaches have limitations in terms\nof flexibility and practicality. Techniques that rely on log\ntemplates such as DeepLog and LogBERT fail to capture\nsemantic information and are unable to handle variability in\nlog content. On the other hand, classification-based approaches\nsuch as LogSy, LogRobust and HitAnomaly require time-\nconsuming data labelling for supervised training. In this paper,\na novel log anomaly detection model named LogFiT is proposed.\nThe LogFiT model doesn’t make use of a vocabulary of\nlog templates and it doesn’t require any labeled data as\nthe model only requires self-supervised training. The LogFiT\nmodel uses a pretrained Bidirectional Encoder Representations\nfrom Transformers (BERT)-based language model fine-tuned to\nrecognise the linguistic patterns of the normal log data. The\nLogFiT model is trained using masked sentence prediction on\nthe normal log data only. Consequently, when presented with\nthe new log data, the model’s top- k token prediction accuracy\nserves as a threshold for determining whether the new log data\ndeviates from the normal log data. Experimental results show\nthat LogFiT’s F1 score exceeds that of baselines on the HDFS,\nBGL, and Thunderbird datasets. Critically, when variability is\nintroduced in the log data during evaluation, LogFiT retains its\neffectiveness compared to that of baselines.\nIndex Terms—Service monitoring, fault management, log\nanomaly detection, deep learning, natural language processing,\nlanguage modeling.\nI. I NTRODUCTION\nAnnually, cybercrime results in billions of dollars of losses\nfor businesses [1]–[3]. Log anomaly detection is an active\narea of research owing to its relevance to the problem of\nensuring the security and reliability of organisations’ digital\ninfrastructure. The large amounts of log data generated by\ncomputer systems provide valuable information about the\nsystems’ real-time operation. However, human operators are\nincreasingly unable to cope with the volume and velocity\nof log data generated by the systems being monitored.\nConsequently, Machine Learning (ML) and Deep Learning\n(DL)-based solutions have been proposed to automatically\ndetect anomalies from system log data, thereby reducing the\nburden on human operators [4]–[6].\nSystem logs are produced by the logging instructions that\nsoftware engineers insert in a computer program’s source code\nto communicate the program’s run-time state. The system logs\nthus produced consist of ordered sequences of log sentences\nthat assert the occurrence of certain events in the system\n[7]. Typically, log sentences are grouped according to some\ncriteria, such as time window (e.g., 60 second intervals)\nor unique identifier (e.g., HDFS block ID). In this study,\nsuch grouping of log sentences is referred to as a ”log\nparagraph”. The idea behind log anomaly detection is to\nlearn a model of the normal behaviour of a computer system\nbased on the sequence of log sentences that it generates\nduring normal operations. If there is a significant deviation\nin the new observed behaviour from the learned normal\nbehaviour, the deviation can be regarded as an anomaly. For\ninstance, a normal sequence of log sentences may consist of\none or more ”File opened successfully” entries followed by\n“File write operation completed” and/or “File read operation\ncompleted” entries. However, a sequence of log sentences\nthat only contains ”File opened successfully” entries without\ncorresponding ”read” or ”write” entries can be considered an\nanomaly.\nThere are several considerations in the study of log anomaly\ndetection. The first consideration is the overall Machine\nLearning approach to be used for anomaly detection. In\nliterature, approaches based on Deep Learning have been\nproven to be more effective than traditional Machine Learning\nbased approaches such as Principal Component Analysis\n(PCA), Support Vector Machines (SVM) and Isolation\nForest [6], [8], [9].\nA further consideration is whether to use supervised\nor unsupervised Machine Learning. Due to the high cost\nof preparing labelled data, supervised methods such as\nLogSy[10], LogRobust [11] and HitAnomaly [12] have limited\nutility in production settings. As a result, unsupervised\ntechniques dominate; these techniques assume a zero-positive\ntraining scenario, where normal log data is the only data\navailable for training [4]. The study in [13] identified\ntwo categories for unsupervised models for log anomaly\nThis article has been accepted for publication in IEEE Transactions on Network and Service Management. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSM.2024.3358730\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2\ndetection: forecasting-based model which learns by predicting\nthe next log sentence based on immediately preceding log\nsentences; and reconstruction-based model which learns by\nreconstructing sequences of log sentences that have been\nintentionally corrupted.\nAnother consideration is the use of a log parser and log\ntemplates to generate representations of log sentences. The\nquality of the representation of log sequences is a critical\nfactor that impacts the effectiveness of log anomaly detection\nmodels, especially when the evolution of log content is\ntaken into account [14], [15]. As shown in Figure 1, log\nparsers [8], [9] extract string templates from the log data\nto build a vocabulary of all known log templates (steps 1\nand 2). Subsequently, all input log sentences are mapped to\nan entry in the log template vocabulary (step 3). Thus a\nsequence of log sentences is represented by a sequence of\nlog template IDs. This approach has been shown to negatively\naffect model effectiveness because of its semantically deficient\nrepresentation of log sentences. Furthermore, the unseen input\nlog sentences are expected to match a log template, which\nmakes this approach incapable of handling variability in log\nsentences over time [10], [16], [17].\nAn important consideration is the choice of model\narchitecture. DL-based approaches have taken inspiration\nfrom the field of Natural Language Processing (NLP) to\nprocess log data that are in the form of natural language\nsentences. Literature reveals that state-of-the-art research in\nthis area employs the Long Short-Term Memory (LSTM)\narchitecture, as exemplified by the DeepLog model [8], and\nTransformers, represented by the LogBERT model [9]. The\nproblems identified with LSTMs are (1) Inability to cater to\nlonger sequences owing to their sequential nature; and, (2)\nDeficient capacity to learn complex and ambiguous contextual\nrelationships in the input data [9]. Being an LSTM-based\narchitecture, DeepLog inherits the limitations of LSTM. On\nthe other hand, Bidirectional Encoder Representations from\nTransformers (BERT) [18] has several advantages over LSTM.\nIt inherits Transformer’s [19] self-attention mechanism which\nhelps it avoid local bias, and its use of bidirectional context\nallows it to capture complex and ambiguous contextual\nrelationships. Although LogBERT inherits the advantages of\nBERT, its dependency on log parsing and log templates makes\nit less adaptable to changes in log structures. Moreover,\nLogBERT trains a BERT model customised to a specific\ndataset from scratch, which limits its ability to generalise\nto other datasets. Furthermore, LogBERT does not benefit\nfrom prior semantic knowledge already learned by a pretrained\nBERT model.\nTo address the limitations of existing approaches, this work\nintroduces the following contributions:\n• We propose LogFiT, a log anomaly detection model\nimplemented as a Python package. LogFiT leverages a\nfine-tuned, pretrained BERT-based model to semantically\nanalyse logs without requiring an intermediate parsing\nstep. During inference, top- k prediction accuracy is used\nfor anomaly identification, with heuristics to set the\nthreshold automatically.\n• The model employs a novel masked sentence\nprediction training objective to enhance the contextual\nunderstanding of log sentences and their constituent\ntokens, thus improving anomaly detection performance.\n• LogFiT works directly on raw logs, eliminating the need\nfor a separate log parsing step to extract log templates.\nLogFiT uses the pretrained model’s extensive vocabulary\nof sub-word tokens to adapt to diverse log content.\n• LogFiT offers a provision of using one of two base\nmodels - RoBERTa and Longformer. The former allows\nfaster training, while the latter is capable of handling\nlonger log sequences.\n• Heuristics are used for base model selection and\nhyperparameter tuning, enhancing ease of use.\n• LogFiT can also serve as a semantic embedding tool,\ngenerating log representations that can be used for other\ntasks outside of anomaly detection.\n• LogFiT is designed for easy integration into existing NLP\ntool sets and the larger system observability ecosystem.\n• We compare LogFiT against established methods,\nDeepLog and LogBERT, to demonstrate its effectiveness.\nThe present paper differs from the authors’ earlier\nconference paper [20], outlined as follows. Firstly, the\nrobustness of LogFiT against gradual changes to the lexical\ncontent of log data has been demonstrated. Secondly, the\ncentroid minimisation training objective that was used in\nthe earlier method has been removed, and the model’s\neffectiveness is not degraded by its removal. Furthermore, the\npresent paper leverages k-fold cross-validation to improve the\nreliability of the experiments, and includes experiments that\nexplore the effect of varying the top- k and top- k accuracy\nthreshold values, and different time windows.\nThe rest of the paper is organised as follows. Section II\ndiscusses the related works on Deep Learning and natural\nlanguage-based approaches for anomaly detection using log\ndata and the pros and cons of those approaches. Section III\nillustrates the proposed LogFiT model in detail. Section IV\ndescribes the datasets, experimental setup and implementation\ndetails. Section V analyses the experimental results. Finally,\nSection V concludes the paper with future directions.\nII. R ELATED WORK\nConsiderable research has been done on Deep Learning\nbased supervised and unsupervised anomaly detection. Many\nof the approaches [8], [9], [11], [12] require log parsing,\nwhile recent studies [11], [16] suggested that log parsing can\nreduce accuracy. Graph-based anomaly detection [21]–[26] is\nan emerging topic, drawing interest from researchers.\nA. Supervised vs. Unsupervised Methods\n1) Unsupervised Methods: DeepLog [8] pioneered the\napplication of Deep Learning and Natural Language\nProcessing to the log anomaly detection problem domain. By\nutilising a forecasting-based approach based on the patterns\nof past log sentences, it improved upon previous methods,\nsuch as Principal Component Analysis (PCA), Support Vector\nMachines (SVM) and Isolation Forest. However, its use\nThis article has been accepted for publication in IEEE Transactions on Network and Service Management. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSM.2024.3358730\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3\nof LSTM and log template indexes negatively affects its\ncapability to handle complex semantic relationships between\nlog sentences. LogAnomaly [27] built upon the principles\nset by DeepLog by introducing the encoding of semantic\ninformation. LogAnomaly used the template2vec method to\nencode the semantic content of log data, enhancing the model’s\nability to detect anomalies. However, LogAnomaly’s use of\nLSTM hampers its ability to process longer sequences and\ncomplex log relationships. LogBERT [9] adopted the BERT\narchitecture to learn more nuanced patterns in the log data,\nwhich allowed it to perform better than previous methods.\nHowever, LogBERT’s use of log parsing and log templates\nmakes it less adaptable to changes in the lexical structure of\nlog data.\n2) Supervised Methods: LogRobust [11] and HitAnomaly\n[12] integrate semantic vectorisation, bidirectional LSTM\nwith attention, and transformer architecture, offering more\naccurate anomaly classification. However, the supervised\nnature limits their generalisability and scalability, demanding\na significant effort in labelling data. The requirement for\nlabelled data makes them less flexible in scenarios where\nobtaining labelled anomalies is challenging. Additionally,\ndomain-specific embedding might limit their applicability\nacross various domains. Furthermore, their reliance on specific\nlog parsers like the Drain parser may also limit flexibility.\nLogSy [10] introduced flexible preprocessing through its\nuse of a Transformer architecture. LogSy also introduced a\nspherical loss function, which was later adopted by LogBERT.\nHowever, LogSy’s classification-based approach can create\nchallenges in scenarios where labelled data is not readily\navailable. The method’s requirement for abnormal log lines\nimported from an auxiliary data source creates an external\ndependency, which hinders its usability in settings where\nauxiliary data sources are not available.\nB. Parsing-based vs. Parsing-free Methods\n1) Parsing-based log anomaly detection: As mentioned\nearlier, DeepLog [8] and LogBERT’s [9] dependency on\nstandardized templates limits their adaptability to new or rare\nlog sentence structures. Also, the use of a log parsing tool\nmight affect its ability to capture the nuanced differences in log\nmessages. Similarly, LogRobust [11] and HitAnomaly’s [12]\ndependency on log parsing tools limits flexibility in capturing\nnuanced differences.\n2) Parsing-free log anomaly detection: As mentioned\nearlier, while LogSy’s [10] use of flexible pre-processing and\nTransformer architecture makes it robust against the evolution\nof log data, its classification-based approach poses a challenge\nwhere labelled data is not readily available. LAnoBERT\n[28] utilises BERT’s natural language understanding to detect\nanomalies without relying on conventional parsing. This\napproach allows it to handle variability in formats. However,\nsimilar to the LogBERT method, this approach trains a BERT\nmodel from scratch. This limits its reuse for other datasets\nand schemas and foregoes the benefits of a pretrained model’s\nprior semantic knowledge.\nFigure 1. Log parsing applied to HDFS log data.\nC. Graph-based approaches\nLog analysis for the intrusion detection use case\nprimarily use graph-based models, requiring substantial feature\nengineering and domain knowledge to identify entities and\ntheir interactions [21]–[26]. The input to graph-based models\nis formatted as information triples, such as (user1, authenticate,\ncomputer2). In contrast, LogFiT operates on well-formed\nsentences, leveraging its transformer architecture to understand\nboth structural and semantic properties of log data [29], [30].\nRecent studies indicate that Transformer models can learn\nrepresentations comparable to graph-based approaches [31],\n[32]. Therefore in future, we aim to adapt LogFiT for intrusion\ndetection scenarios, for example by converting intrusion\ndetection log data into processable sentence structures.\nD. Log anomaly detection workflow\nThe log anomaly detection workflow involves four steps\nas identified in prior studies [5], [33]–[35]: pre-processing\nfor data quality, vectorisation for Machine Learning, model\ndevelopment and evaluation, and final operationalisation\nin production. Current research does not recommend log\nparsing for vectorisation due to accuracy issues [11], [16],\nand recommends NLP-based architectures like LSTM and\nTransformers for optimal performance [8]–[10], [16].\nE. Pretrained Language Models\nIn recent research, there has been a growing interest in the\nuse of pretrained language models (LMs) such as BERT [18]\nto improve anomaly detection in system logs. Studies in [15],\n[16] demonstrated that pretrained LMs can offer significant\nadvantages over word embeddings, that were used in the\nLogRobust model [11]. According to these studies, pretrained\nLMs capture contextual information at the level of the\nwhole log sentence, whereas word embeddings only provide\nrepresentations for individual words in a single sentence.\nFurthermore, pretrained LMs are capable of handling out-of-\nvocabulary words, unlike static word embeddings. Existing\nresearch suggests that pretrained LMs such as BERT can learn\nboth syntactic and semantic information, which can improve\nthe effectiveness of Natural Language Processing tasks [29],\n[30], [36], [37]. The BERT model is a Transformer encoder\nmodel that has the capabilities of an auto-encoder. The BERT\nThis article has been accepted for publication in IEEE Transactions on Network and Service Management. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSM.2024.3358730\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4\nTable I\nCOMPARATIVE ANALYSIS OF LOG ANOMALY DETECTION METHODS\nMethod Key Technical Aspects Notable Limitations\nDeepLog [8] LSTM, log templates, forecasting-based, unsupervised Struggles with long sequences and variability,\nlimited semantic ability\nLogAnomaly [27] LSTM, template2vec semantic encoding, forecasting-based, unsupervised Struggles with long sequences\nLogBERT [9] BERT, log templates, reconstruction-based, spherical loss, unsupervised Struggles with variability, limited semantic ability,\nreduced adaptability, miss benefits of transfer learning\nLogRobust [11] Transformer & LSTM, semantic vectorisation, classification-based Requires labeled data, less generalisable\nLogSy [10] Transformer, semantic vectorisation, spherical loss, classification-based Dependent on external source of abnormal logs\nLAnoBERT [16] BERT, semantic vectorisation, unsupervised Trains from scratch, miss benefits of transfer learning\nGraph-based Models [21]–[26] Entity interaction analysis, graph-structures Requires considerable domain knowledge\nLogFiT (ours) Transformer (any encoder model), LM fine-tuning, semantic understanding, Sub-optimal detection throughput\nreconstruction-based, unsupervised\nmodel’s encoder capability allows it to generate semantic\nvectors that are sensitive to the full context of the input log\nsequence, and to reconstruct log sequences that have been\ncorrupted [18]. Therefore, in this study, a pretrained LM is\nincorporated into the LogFiT model to leverage its ability to\nunderstand the sequential properties and linguistic structure of\nsystem logs.\nIn Table I, various log anomaly detection methods are\nsummarised and compared, highlighting their key technical\naspects and limitations. Table I presents the landscape of\nexisting approaches to highlight the research gap addressed\nby the proposed LogFiT method. LogFiT distinguishes itself\nfrom other methods by leveraging transfer learning and LM\nfine-tuning through masked sentence prediction, enabling it to\nadapt more robustly to evolving log data, unlike other methods\nthat require log templates, LSTM, labelled data, from-scratch\ntraining, or extensive domain knowledge. Furthermore, it is\nimportant to note that LogFiT is not limited to using RoBERTa\nor Longformer. It is straightforward to swap out LogFiT’s\nunderlying model and replace it with other Transformer\nencoder models available on the Hugging Face model hub.\nIII. L OGFIT\nThe proposed LogFiT approach takes advantage of\nadvancements in Deep Learning and NLP for system log\nanomaly detection. It employs pretrained foundation models\n[38], specifically fine-tuning RoBERTa [39] or Longformer\n[40] to learn the linguistic and sequential properties of normal\nlog data. Longformer is selected for its support of sequences\nexceeding 512 tokens, overcoming limitations in BERT [18]\nand RoBERTa. LogFiT incorporates a heuristic to choose\nbetween RoBERTa and Longformer based on log sequence\nlengths: RoBERTa for sequences up to 512 tokens, and\nLongformer for longer log sequences.\nLogFiT utilises a self-supervised training strategy, focusing\nexclusively on normal log data to learn its linguistic and\nsequential patterns. The model aims to predict masked\ntokens in log sentences, employing cross-entropy loss to\noptimise its predictions. This loss function is logarithmic,\npenalising incorrect predictions more severely than correct\nones. LogFiT’s semantic vectors, stored in the [CLS] token, are\nsuitable for downstream tasks like clustering and visualisation.\nAs shown in Figure 2, the LogFiT architecture and workflow\ninvolve: (1) preprocessing raw log lines; (2) and (3) fine-\ntuning a pretrained RoBERTa/Longformer model on normal\nFigure 2. Logical architecture of the LogFiT log anomaly detection approach.\nlogs using masked sentence prediction and cross-entropy loss;\nand, (4) detecting anomalies in new log data. Additionally,\nFigure 3 shows LogFiT can be integrated into a system\nobservability platform (such as the Elasticsearch stack) and\nconfigured for multiple log streams. That is, (1) LogFiT\npreprocesses centralised log data for training and inference;\n(2) The model trains on normal historical logs; (3) Optimal\nthreshold is determined; (4) During inference, anomalies are\ndetected using the model and threshold; and, (5) Alerts appear\non the platform’s UI for operator action.\nIn its early version [20], LogFiT incorporated centroid loss,\ninspired by the LogBERT [9] method. Subsequent research\nindicated that centroid loss is unnecessary, thus the present\nversion of LogFiT does not incorporate it.\nThis article has been accepted for publication in IEEE Transactions on Network and Service Management. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSM.2024.3358730\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n5\nFigure 3. LogFiT integrated into existing system observability platform.\nA. Framework\nInput Representation. The LogFiT model utilises normal\nlog data for training, which is converted to semantic vectors\nbefore being passed to the model. In contrast to both the\nDeepLog and LogBERT methods, the LogFiT model does\nnot require the input log data to first be converted to log\ntemplates. Rather, log data is directly tokenised using the\npretrained RoBERTa or Longformer model’s default tokeniser\ncomponent. LogFiT takes inspiration from recent models [15],\n[16] that forego the log template extraction step and directly\nconvert the log data into semantic vectors using a pretrained\nLM. These models however treat the pretrained language\nmodel as a static semantic embedding generator, whereas\nLogFiT fine tunes it to the log anomaly task.\nTransformer Architecture. LogFiT inherits from the\ninnovations introduced by the BERT-based language models.\nBERT’s ability to accurately reconstruct corrupted input logs\ncan be used as a threshold-based anomaly detection method.\nThus by inheriting from BERT, LogFiT includes both the\nvectoriser and log anomaly detection component in a single\npackage, allowing for end-to-end training that does not require\nintermediate log template extraction or vectorisation steps. As\nmentioned earlier, LogFiT makes use of the RoBERTa model\nto process log datasets containing up to 512 tokens per sample,\nand the Longformer model for datasets where the token count\nper sample exceeds 512. The Longformer model overcomes\nBERT’s limitation on the number of tokens, which is due\nto the quadratic computational complexity of BERT’s self-\nattention mechanism, by introducing a sliding window strategy\nthat effectively reduces attention computation to a linear time\n[40]. The LogFiT model consists of 12 stacked Transformer\nencoders, 12 attention heads per layer, 768-dimension vectors,\nand a maximum possible sequence length of 4096 tokens – as\nillustrated in Figure 2.\nHeuristic. The LogFiT tool also includes a heuristic\nto automatically select between RoBERta and Longformer\nbased on the 0.8-quantile length (in words) of the training\nlog samples, found during preprocessing. LogFiT selects\nRoBERTa for datasets with log samples containing no more\nthan 512 tokens. For log datasets with samples that exceed 512\ntokens, LogFiT switches to Longformer to take advantage of\nits ability to handle longer log samples via its use of local and\nglobal attention.\nFine-tuning. To adapt the RoBERTa or Longformer model\nto the log anomaly detection task, LogFiT utilises fine-\ntuning methods based on super-convergence techniques [41]\nimplemented in the FastAI/ULMFiT framework [42], [43].\nResearch has demonstrated that fine-tuning yields significant\nperformance improvements (an average gain of 2%) [44] and\nthat gradual unfreezing [42], [43] counters the negative effects\nof weight dissipation during fine-tuning of pretrained language\nmodels [45].\nB. Training Objective\nAs mentioned previously, the LogFiT model is trained in a\nself-supervised manner using masked sentence prediction. This\ntraining objective is a modified version of the self-supervised\nmasked language modelling (MLM) training objective used\nto pretrain BERT-based language models [18]. In LogFiT a\nvariable ratio (default 0.5) of the sentences that constitute a\nlog paragraph is randomly chosen for masking, unlike BERT\nwhich randomly chooses a set ratio (0.15) of all the tokens\nthat make up a log paragraph is randomly chosen for masking.\nSubsequently, LogFiT masks a variable ratio (default 0.8) of\nthe tokens that make up each log sentence. Afterwards, the\nmodel is tasked with predicting what the masked tokens were.\nThe intuition behind this training objective is to force the\nmodel to learn not just the contextual relationships between\nthe tokens that make up a log sentence, but also the contextual\nrelationships between the log sentences to accurately predict\nthe masked tokens. This enables the model to develop an\nunderstanding of the language rules used by the normal system\nlogs. As a result, it can differentiate normal log data from\nanomaly log data.\nTo satisfy the masked token prediction training objective,\nthe model minimises the cross-entropy loss between its\nmasked token predictions and the actual tokens. The\ncomputation of the cross-entropy loss for a mini-batch of log\ndata is shown in Equation 1. The LogFiT model minimises\nthe training loss using the Adam optimiser, initialised using\ndefault values from the FastAI [43] Deep Learning library:\nmomentum = 0.9, sqr momentum = 0.99, epsilon = 1e − 5,\nand weight decay = 0.01.\nLoss = −1\nb\nbX\nj=1\nmX\ni=1\nyj\nmaski\nlog(pj\nmaski\n), (1)\nwhere b is size of the the mini-batch, m is the number of\nmasked tokens, y and p are the true and predicted values,\nrespectively.\nThis article has been accepted for publication in IEEE Transactions on Network and Service Management. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSM.2024.3358730\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n6\nC. Anomaly Detection\nThe LogFiT model, which is exclusively trained on normal\ndata, can then be used to detect abnormal log data. During the\ninference stage, log paragraphs are processed in the same way\nas during training. To determine whether a log paragraph is\nanomalous, LogFiT uses a technique adopted from LogBERT.\nThe trained model’s top- k accuracy in correctly predicting the\nmasked tokens is used as an anomaly score. If the model’s\ntop-k predictions for a masked token contain the correct token,\nthe model’s prediction is considered correct. A log paragraph\nis considered normal if the model’s accuracy in correctly\npredicting the masked tokens is above some threshold. If\nthe model’s accuracy falls below the threshold, the log\nparagraph is deemed an anomaly. LogFiT includes a heuristic\nto determine the optimal threshold during hyperparameter\ntuning, as described in the Experimental Setup section.\nIV. E XPERIMENTS\nIn this section, the datasets, experimental setup, and\nimplementation details are described. Subsequently, the results\nof running the experiments are evaluated.\nA. Experimental Setup\nDatasets. The LogFiT model is trained and evaluated\nusing three public datasets: HDFS [46], BGL [47] and\nThunderbird [47], as used by baseline models [8], [9].\nWhile these datasets are partially labelled, LogFiT uses the\nlabels solely for model validation. In real-world applications,\nlogs are often unlabeled. HDFS logs were generated by\nthe Hadoop Distributed File System and contain both\nnormal and anomalous events, manually tagged by experts.\nAnomalies in this dataset pertain to abnormal file system\noperations. The dataset comprises 11,175,629 log sentences,\nwith 284,818 identified as anomalies. BGL logs come from\nthe Blue Gene/L supercomputer at Lawrence Livermore\nNational Laboratory. It has 4,747,963 log sentences, with\n348,460 categorised as anomalies. Thunderbird logs were\nproduced by the Thunderbird supercomputer system at Sandia\nNational Laboratories. The full dataset contains 211,212,192\nlog sentences; this study considers the first 20,000,000, of\nwhich 758,562 are anomalies.\nLog paragraphs. The HDFS log sentences are chunked into\nlog paragraphs using the HDFS block ID, which represents a\nsession in HDFS. The BGL and Thunderbird datasets do not\nhave a natural grouping field, so the log sentences are grouped\nusing time windows of 10, 30 and 60 seconds. A shorter\ntime window facilitates timely feedback for system operators.\nTable II shows some statistics about the HDFS, BGL, and\nThunderbird datasets.\nK-fold Cross-validation. In the experiments, k-fold cross-\nvalidation was used, specifically using a five-fold approach.\nFor each dataset, a total of 25,000 normal and 2,000 anomaly\nlog paragraphs were allocated for this process. The normal\nlogs were used exclusively for training, while the anomaly\nlogs were reserved for hyperparameter tuning and model\nevaluation. During each five-fold iteration, 5,000 logs sampled\nfrom the 20,000 training split were utilised for training. For\nTable II\nPER-PARAGRAPH WORD AND SENTENCE STATISTICS FOR THE DATASETS .\nDataset Avg Word Count Avg Sentence Count #Unique Words\nHDFS 176.04 18.63 146\nBGL 128.66 15.73 6,046\nThunderbird 1445.70 126.63 15,557\nhyperparameter tuning, 1,000 normal logs (from the training\nsplit) were used, supplemented with 1,000 anomaly logs. The\nfinal model evaluation exclusively used the 5,000 logs from\nthe test split, supplemented with 1,000 anomaly logs.\nLog Content Variability. To test the models’ ability to\nhandle variation in the syntactic structure of the log data,\nthe evaluation set is dynamically modified during model\nevaluation on the BGL dataset, so that the top 10% most\ncommonly occurring verbs are replaced with their WordNet\n[48] lemmas.\nBaselines. The performance of the LogFiT model is\ncompared against two key baselines. DeepLog [8] and\nLogBERT [9]. DeepLog utilised an LSTM-based architecture\nand a forecasting-based approach for anomaly detection by\npredicting the next log template based on its preceding ones.\nThe model deems a log sequence normal if the correct template\nfalls within its top- k predictions. The results are produced\nusing the logdeep library 1, and it should be noted that the\noriginal DeepLog performance metrics are not reproducible\nwith this implementation. LogBERT, on the other hand,\nemploys a BERT-based architecture and a reconstruction-based\napproach. It learns normal log patterns through masked log key\nprediction and centroid distance minimisation. Anomalies are\nidentified by predicting masked log keys and calculating an\nanomaly score based on top- k accuracy and centroid distance.\nIf either metric exceeds a certain threshold, the sequence is\nclassified as anomalous. Results are produced from publicly\navailable LogBERT source code, and it is noted that the\noriginal evaluations are also not reproducible.\nImplementation Details. LogFiT was implemented using\nPython and leveraged several well-known libraries to\naccelerate the development and evaluation of the model, such\nas Pytorch, FastAI and Hugging Face. The details of the\nimplementation can be found in the pre-print version of this\npaper [49]. The source code implementing the LogFiT model,\ndatasets and model checkpoints will be made available online.\nEvaluation Metrics. To evaluate the effectiveness of the\nmodels, the experiments use the following metrics:\n• Precision (P) measures the proportion of correctly\nidentified anomaly samples (TP ), out of all the anomalies\ndetected by the model, and is calculated as P = TP /\n(TP+FP).\n• Recall (R) measures the proportion of correctly identified\nanomaly samples ( TP ) out of all real anomalies and is\ncalculated as R = TP / (TP+FN) .\n• F1 Score (F1) is the harmonic mean of the Precision and\nRecall and is calculated as F1 = 2 * (P*R)/(P+R) .\n1Included in the LogBERT source code distribution.\nThis article has been accepted for publication in IEEE Transactions on Network and Service Management. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSM.2024.3358730\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n7\n• Specificity (S) measures the proportion of correctly\nidentified normal samples ( TN ) out of all real normal\nsamples and is calculated as S = TN/(TN+FP) .\nIn real-world deployment scenarios, having a predictive\nmodel with high Specificity is more advantageous since it\nminimises the chances of producing false positives or false\nalarms. Furthermore, in [4] it was noted that a high Specificity\ncan help mitigate the impact of having an imbalanced class\ndistribution on the model’s overall performance.\nHyperparameter tuning. During the hyperparameter\ntuning step, LogFiT iterates through top- k values in the range:\n5, 9, and 12. Subsequently, for the top- k accuracy threshold,\nLogFiT iterates through values based on the top-1 token\nprediction accuracy of the model during training. For example,\nif the model’s top-1 token prediction accuracy during training\nwas 0.9, the range of values for the top- k accuracy threshold\nsearch is derived by computing 3 evenly spaced numbers in\nthe range [ (0.9 − 0.1), 0.9]. This computation is facilitated\nby the linspace function from the NumPy library, which\nyields the values 0.8, 0.85, and 0.9.\nObservations. In our experiments, we observed that the\nimplementations of LogBERT and DeepLog included the test\nset during the log parsing step (which builds the vocabulary\nof log templates), thus artificially avoiding out-of-vocabulary\nissues. The implementations also filtered out log instances\nwith fewer than 10 log template IDs, which avoided a failure\ncondition when the input sequence length is 1. We modified\nthe implementations to align with LogFiT’s setup.\nWe note that k-fold cross-validation may lead the models\nto peek into the future, by processing log samples that only\noccur at later times leading to the model’s effectiveness\nimprovement. This has been shown in the past in time-based\ndatasets [50].\nV. E XPERIMENTAL RESULTS\nLog Anomaly Detection Effectiveness. Table III, Table IV\nand Table V show the results of running anomaly detection\ninference using LogFiT, as compared to the results from\nrunning DeepLog and LogBERT using the available source\ncode implementation. The results show that LogFiT’s F1\nscores exceed that of LogBERT and DeepLog on all three\ndatasets, while LogFiT’s specificity exceeds that of the\nbaseline models on the HDFS and BGL datasets and is very\nclose to LogBERT’s on the Thunderbird dataset. The DeepLog\nand LogBERT models were trained and evaluated using the\nsource code implementation mentioned earlier.\nAnalysis. The LogFiT results indicate that fine-tuning\npretrained RoBERTa or Longformer models with a novel\nmasked sentence prediction training objective is effective\nfor adapting these language models to the task of detecting\nanomalies in system logs. This training approach enhances\nLogFiT’s contextual understanding of log data. LogFiT’s\nextensive sub-word token vocabulary and its lack of need for\nlog parsing allow it to easily adapt to diverse log content.\nAdditionally, we corrected implementation issues in DeepLog\nand LogBERT, which may have influenced these methods’\nperformance. It is emphasised that while LogFiT improves\nTable III\nANOMALY DETECTION PRECISION (P), R ECALL (R), F1 SCORE (F) AND\nSPECIFICITY (S) OF DIFFERENT METHODS ON THE HDFS DATASET.\nLOGFIT VALUES ARE AVERAGED FROM 5-FOLD CROSS -VALIDATION .\nMethod P R F1 S\nDeepLog 100.0 60.90 75.70 100.0\nLogBERT 24.02 82.80 37.24 47.62\nLogFiT (ours) 99.78 90.70 95.02 99.96\nTable IV\nANOMALY DETECTION PRECISION (P), R ECALL (R), F1 SCORE (F) AND\nSPECIFICITY (S) OF DIFFERENT METHODS ON THE BGL DATASET.\nLOGFIT VALUES ARE AVERAGED FROM 5-FOLD CROSS -VALIDATION AND\nTIME WINDOWS OF 10S, 30 S, AND 60S.\nMethod P R F1 S\nDeepLog 90.2 70.68 79.25 98.32\nLogBERT 88.92 88.35 88.63 97.59\nLogFiT (ours) 94.39 85.80 89.89 98.98\nupon baseline methods on standard metrics, the LogFiT\nmethod’s true effectiveness is evident in scenarios where\nthe textual content of log data changes due to log schema\nevolution. LogFiT’s superior adaptability and robustness in\nhandling dynamic changes in log data - achieved through\nLM fine-tuning - underscores its significant contribution to\nthe domain of log anomaly detection.\nAnomaly Detection Throughput. Table VI presents the\nthroughput rates (in samples per second) for DeepLog,\nLogBERT and LogFiT on the HDFS, BGL and Thunderbird\ndatasets. DeepLog excels in throughput on the HDFS\ndataset, owing to this dataset’s shorter sequence lengths.\nHowever, its LSTM architecture limits its efficiency on the\nBGL and Thunderbird datasets, where sequence lengths are\nlonger. LogBERT achieves superior throughput on these latter\ndatasets, benefiting from its parallel token processing due\nto its use of a transformer architecture. LogFiT lags in\nthroughput primarily due to its larger vocabulary size of\n50K, which dictates the size of its embedding layer. Unlike\nDeepLog and LogBERT, whose vocabularies are based on the\nnumber of unique log templates, LogFiT’s vocabulary is more\nextensive - which allows it to handle variability in log content.\nAdditionally, LogFiT accommodates up to 4096 tokens, as\nopposed to the 512-token limit in DeepLog and LogBERT.\nFinally, LogFiT’s lower throughput is also influenced by its\ngeneration of detailed metrics and artifacts at inference time.\nIn contrast, DeepLog and LogBERT’s outputs are simple\nstatements printed out to the terminal. It is important to clarify\nthat the primary objective of our research was not centred\non throughput optimization. Throughput enhancements for\nLogFiT is a subject we explore in the future.\nCentroid Distance Minimisation. The LogFiT model\nwas extended to include a centroid distance minimisation\nobjective (as used in LogBERT) alongside the standard\nmasked token prediction. The loss function, as shown in\nEquation 2, was thus a combination of the cross-entropy loss\nfrom Equation 1 and a new term representing the centroid\ndistance weighted by a hyperparameter cw (set to 0.25). The\nThis article has been accepted for publication in IEEE Transactions on Network and Service Management. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSM.2024.3358730\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8\nTable V\nANOMALY DETECTION PRECISION (P), R ECALL (R), F1 SCORE (F), AND\nSPECIFICITY (S) OF DIFFERENT METHODS ON THE THUNDERBIRD\nDATASET. LOGFIT VALUES ARE AVERAGED FROM 5-FOLD\nCROSS -VALIDATION AND TIME WINDOWS OF 10S, 30 S, AND 60S.\nMethod P R F1 S\nDeepLog 65.05 99.40 78.64 89.30\nLogBERT 91.75 95.70 93.69 98.28\nLogFiT (ours) 89.72 98.60 93.94 97.74\nTable VI\nANOMALY DETECTION THROUGHPUT (IN SAMPLES PER SECOND ) OF\nDIFFERENT METHODS ON THE HDFS, BGL AND THUNDERBIRD\nDATASETS . BGL AND THUNDERBIRD VALUES ARE AVERAGED ACROSS\nTIME WINDOWS OF 10S, 30 S, AND 60S. BOLD VALUES REPRESENT THE\nHIGHEST IN A COLUMN .\nMethod HDFS BGL Thunderbird\nDeepLog 1100.91 25.43 26.56\nLogBERT 497.92 183.08 127.84\nLogFiT (ours) 13.28 9.68 4.09\ncentroid distance is essentially the proximity of each log\nparagraph’s [CLS] vector to a mean vector calculated from\nnormal logs. A specific q-quantile (with q between 0.65 to 0.9)\ncentroid distance was also calculated to serve as a threshold\nduring inference. However, experimental results indicated\nthat the incorporation of centroid distance did not enhance\nthe effectiveness of the model in distinguishing normal\nfrom anomalous log entries. Despite these modifications, the\nextended model’s performance remained comparable to the\noriginal LogFiT model. Furthermore, the semantic vectors\nof the training set did not form distinct clusters, after\ndimensionality reduction via the UMAP algorithm [51], as\nshown in Figure 4. Consequently, we concluded that centroid\ndistance minimisation does not offer an advantage and can be\nomitted from the model.\nLoss = Losscross−entropy + cw ∗ 1\nb\nbX\nj=1\n(CVj − centroid)2.\n(2)\nVariability in input data. In practical applications, it is\nexpected that the content of the log sentences changes over\ntime. This can be because the programmers may change\nsome words in the log sentences, or introduce misspellings.\nThe LogFiT model contains built-in support for log sentence\nvariability due to its large vocabulary of sub-word tokens\n(around 50K). In contrast, the DeepLog and LogBERT models\nwould fail if they encounter variations in log sentences that\ncannot be mapped to their list of known log templates. To test\nthe LogFiT model’s ability to handle log sentence variability,\nthe evaluation set is dynamically modified during inference so\nthat the top 10 occurring action words (that can be mapped\nto synonyms in WordNet) are replaced with their WordNet\nlemmas. Table VII shows the results of feeding the modified\nBGL evaluation set to the trained LogFiT, DeepLog and\nLogBERT models. The results indicate that the LogFiT model\nFigure 4. UMAP plot of Thunderbird semantic vectors, where the blue,\npink, and yellow colours of the points represent training samples, normal\npredictions, and anomaly predictions respectively.\nTable VII\nANOMALY DETECTION PRECISION (P), R ECALL (R), F1 SCORE (F) AND\nSPECIFICITY (S) OF DIFFERENT METHODS ON THE BGL DATASET. THE\nEVALUATION DATA WAS MODIFIED TO REPLACE THE TOP 10 VERBS WITH\nTHEIR WORD NET LEMMAS .\nMethod P R F1 S\nDeepLog 40.10 79.79 53.38 68.06\nLogBERT 28.82 94.92 44.22 37.18\nLogFiT (ours) 95.249 84.20 89.38 99.16\nis robust to changes in the log sentences, as the reduction in\nF1 is around 2% (from 91.22 to 89.38). However, the drop in\nF1 performance for LogBERT is large, from 88.63 to 44.22.\nSimilarly for DeepLog, F1 dropped from 79.25 to 53.38.\nVI. C ONCLUSION\nThe paper has introduced LogFiT, a novel log anomaly\ndetection model that leverages the general linguistic\nknowledge of a pretrained BERT-based LM by adapting\nit to learn the linguistic patterns of normal system logs.\nLogFiT is trained using a novel self-supervised masked\nsentence prediction objective, using only normal log data.\nThis approach enables LogFiT to recognise the linguistic\nstructure of normal system logs only, thus anomalies can\nbe flagged when the model fails to predict the correct log\nsentences for new log data. Critically, LogFiT can handle\nvariability in the content of system logs because of its use\nof a BERT-based LM. The performance of LogFiT on the\nHDFS, BGL, and Thunderbird datasets has been evaluated\nand it has been that LogFiT’s F1 score outperformed that of\nthe baseline models. Moreover, LogFiT’s specificity exceeded\nthat of the baselines on the HDFS and BGL datasets and\nwas comparable to LogBERT on the Thunderbird dataset.\nIn addition, LogFiT demonstrated superior effectiveness over\nLogBERT in experiments that tested for variations in the\ncontent of input log paragraphs, which is attributed to its\nability to handle out-of-vocabulary tokens. LogFiT integrates\nwith the popular Hugging Face ecosystem, making it easy\nto adapt in future work. Overall, LogFiT presents a flexible\napproach to detecting abnormal behaviour in computer systems\nthrough language model adaptation and fine-tuning.\nThis article has been accepted for publication in IEEE Transactions on Network and Service Management. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSM.2024.3358730\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n9\nA. Future Work\nWhile the LogFiT model is intended to be used as a\nthreshold-based anomaly detector trained in a self-supervised\nmanner, it can easily be converted to a classifier. If at\nsome point after the model is deployed, operators can\ncollect and label anomaly log samples, the model can be\nconverted to a classifier by replacing its language modelling\nhead with a classification head. Additionally, the LogFiT\nLM can be pretrained on diverse log datasets, allowing\nit to be used as the foundation for downstream NLP\nand log anomaly detection tasks. Furthermore, LogFiT’s\nsuitability for the intrusion detection use case can be\nconsidered in future studies. Lastly, ongoing research to\naddress LogFiT’s sub-optimal throughput performance focuses\non efficient training and deployment strategies. These include\nthe use of LoRA adapters, quantisation, and optimised\nmodel serving environments. These initiatives are aimed at\nimproving LogFiT’s operational effectiveness in real-world\nscenarios, balancing its throughput with its anomaly detection\ncapabilities.\nREFERENCES\n[1] RiskIQ, The evil internet minute 2019 , 2019. [Online].\nAvailable: https : / / www . riskiq . com / resources /\ninfographic/evil-internet-minute-2019.\n[2] Australia Department of Home Affairs, Australia’s\ncyber security strategy 2020, 2020. [Online]. Available:\nhttps://www.homeaffairs.gov.au/cyber-security-subsite/\nfiles/cyber-security-strategy-2020.pdf.\n[3] International Business Machines, Cost of a data breach\nreport 2022 , 2022. [Online]. Available: https://www.\nibm.com/security/data-breach.\n[4] V . H. Le and H. Zhang, Log-based Anomaly Detection\nwith Deep Learning: How Far Are We? , 1. Association\nfor Computing Machinery, 2022, vol. 1, ISBN :\n9781450392211. DOI : 10 . 1145 / 3510003 . 3510155.\narXiv: 2202 . 04301. [Online]. Available: http : / / arxiv.\norg/abs/2202.04301.\n[5] S. He, P. He, Z. Chen, T. Yang, Y . Su, and M. R. Lyu,\n“A survey on automated log analysis for reliability\nengineering”, ACM computing surveys (CSUR), vol. 54,\nno. 6, pp. 1–37, 2021.\n[6] Z. Chen, J. Liu, W. Gu, Y . Su, and M. R. Lyu,\n“Experience Report: Deep Learning-based System Log\nAnalysis for Anomaly Detection”, Jul. 2021. arXiv:\n2107.05908. [Online]. Available: http://arxiv.org/abs/\n2107.05908.\n[7] P. He, Z. Chen, S. He, and M. R. Lyu, “Characterizing\nthe natural language descriptions in software logging\nstatements”, in Proceedings of the 33rd ACM/IEEE\nInternational Conference on Automated Software\nEngineering, 2018, pp. 178–189.\n[8] M. Du, F. Li, G. Zheng, and V . Srikumar, “DeepLog:\nAnomaly detection and diagnosis from system logs\nthrough deep learning”, in Proceedings of the ACM\nConference on Computer and Communications Security,\nNew York, NY , USA: ACM, 2017, pp. 1285–1298,\nISBN : 9781450349468. DOI : 10 . 1145 / 3133956 .\n3134015.\n[9] H. Guo, S. Yuan, and X. Wu, “LogBERT: Log Anomaly\nDetection via BERT”, Proceedings of the International\nJoint Conference on Neural Networks , vol. 2021-July,\nMar. 2021. DOI : 10.1109/IJCNN52387.2021.9534113.\narXiv: 2103.04475. [Online]. Available: https://arxiv.\norg/abs/2103.04475v1.\n[10] S. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso,\nand O. Kao, “Self-attentive classification-based\nanomaly detection in unstructured logs”, in\nProceedings - IEEE International Conference on\nData Mining, ICDM , vol. 2020-Novem, Institute\nof Electrical and Electronics Engineers Inc., Nov.\n2020, pp. 1196–1201, ISBN : 9781728183169. DOI :\n10.1109/ICDM50108.2020.00148. arXiv: 2008.09340.\n[11] X. Zhang, Y . Xu, Q. Lin, et al. , “Robust log-\nbased anomaly detection on unstable log data”, in\nESEC/FSE 2019 - Proceedings of the 2019 27th\nACM Joint Meeting European Software Engineering\nConference and Symposium on the Foundations of\nSoftware Engineering , vol. 19, 2019, pp. 807–817,\nISBN : 9781450355728. DOI : 10 . 1145 / 3338906 .\n3338931. [Online]. Available: https://doi.org/10.1145/\n3338906.3338931.\n[12] S. Huang, Y . Liu, C. Fung, et al. , “Hitanomaly:\nHierarchical transformers for anomaly detection in\nsystem log”, IEEE Transactions on Network and Service\nManagement, vol. 17, no. 4, pp. 2064–2076, 2020. DOI :\n10.1109/TNSM.2020.3034647.\n[13] L. P. Yuan, P. Liu, and S. Zhu, “Recompose Event\nSequences vs. Predict Next Events: A Novel Anomaly\nDetection Approach for Discrete Event Logs”, in ASIA\nCCS 2021 - Proc. 2021 ACM Asia Conf. Comput.\nCommun. Secur. , vol. 1, Association for Computing\nMachinery, 2021, pp. 336–348, ISBN : 9781450382878.\nDOI : 10.1145/3433210.3453098.\n[14] D. Hendrycks, X. Liu, E. Wallace, A. Dziedzic,\nR. Krishnan, and D. Song, “Pretrained Transformers\nImprove Out-of-Distribution Robustness”, Association\nfor Computational Linguistics (ACL), Apr. 2020,\npp. 2744–2751. DOI : 10.18653/v1/2020.acl-main.244.\narXiv: 2004.06100. [Online]. Available: https://arxiv.\norg/abs/2004.06100v2.\n[15] H. Ott, J. Bogatinovski, A. Acker, S. Nedelkoski,\nand O. Kao, “Robust and transferable anomaly\ndetection in log data using pre-trained language\nmodels”, in 2021 IEEE/ACM International Workshop\non Cloud Intelligence (CloudIntelligence) , IEEE, 2021,\npp. 19–24.\n[16] V .-H. Le and H. Zhang, “Log-based anomaly detection\nwithout log parsing”, in 2021 36th IEEE/ACM\nInternational Conference on Automated Software\nEngineering (ASE), IEEE, 2021, pp. 492–504.\n[17] T. Wittkopp, A. Acker, S. Nedelkoski, et al., “A2Log:\nAttentive Augmented Log Anomaly Detection”, HICSS\n2022 : Hawaii International Conference on System\nThis article has been accepted for publication in IEEE Transactions on Network and Service Management. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSM.2024.3358730\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n10\nSciences, Sep. 2021. arXiv: 2109 . 09537. [Online].\nAvailable: http://arxiv.org/abs/2109.09537.\n[18] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova,\n“BERT: Pre-training of deep bidirectional transformers\nfor language understanding”, in NAACL HLT 2019\n- 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies - Proceedings of the\nConference, vol. 1, Association for Computational\nLinguistics (ACL), Oct. 2019, pp. 4171–4186,\nISBN : 9781950737130. arXiv: 1810 . 04805. [Online].\nAvailable: https://arxiv.org/abs/1810.04805v2.\n[19] A. Vaswani, N. Shazeer, N. Parmar, et al. , “Attention\nis all you need”, Advances in Neural Information\nProcessing Systems , vol. 2017-Decem, no. Nips,\npp. 5999–6009, 2017, ISSN : 10495258. arXiv: 1706 .\n03762.\n[20] C. Almodovar, F. Sabrina, S. Karimi, and S. Azad, “Can\nlanguage models help in system security? investigating\nlog anomaly detection using BERT”, in Proceedings\nof the The 20th Annual Workshop of the Australasian\nLanguage Technology Association, Adelaide, Australia:\nAustralasian Language Technology Association, Dec.\n2022, pp. 139–147. [Online]. Available: https : / /\naclanthology.org/2022.alta-1.19.\n[21] A. Alsaheel, Y . Nan, S. Ma, et al. , “ATLAS:\nA sequence-based learning approach for attack\ninvestigation”, in 30th USENIX Security Symposium\n(USENIX Security 21) , USENIX Association, Aug.\n2021, pp. 3005–3022, ISBN : 978-1-939133-24-3.\n[Online]. Available: https://www.usenix.org/conference/\nusenixsecurity21/presentation/alsaheel.\n[22] S. M. Milajerdi, R. Gjomemo, B. Eshete, R. Sekar, and\nV . Venkatakrishnan, “Holmes: Real-time apt detection\nthrough correlation of suspicious information flows”, in\n2019 IEEE Symposium on Security and Privacy (SP) ,\n2019, pp. 1137–1152. DOI : 10.1109/SP.2019.00026.\n[23] I. J. King and H. H. Huang, “Euler: Detecting network\nlateral movement via scalable temporal link prediction”,\nACM Transactions on Privacy and Security , 2023.\n[24] F. Liu, Y . Wen, D. Zhang, X. Jiang, X. Xing, and\nD. Meng, “Log2vec: A heterogeneous graph embedding\nbased approach for detecting cyber threats within\nenterprise”, in Proceedings of the 2019 ACM SIGSAC\nconference on computer and communications security ,\n2019, pp. 1777–1794.\n[25] R. Yang, S. Ma, H. Xu, X. Zhang, and Y . Chen,\n“Uiscope: Accurate, instrumentation-free, and visible\nattack investigation for gui applications.”, in NDSS,\n2020.\n[26] C. Fu, Q. Li, and K. Xu, “Detecting unknown encrypted\nmalicious traffic in real time via flow interaction graph\nanalysis”, arXiv preprint arXiv:2301.13686 , 2023.\n[27] W. Meng, Y . Liu, Y . Zhu, et al. , “Loganomaly:\nUnsupervised detection of sequential and quantitative\nanomalies in unstructured logs”, in IJCAI International\nJoint Conference on Artificial Intelligence , vol. 2019-\nAugus, 2019, pp. 4739–4745, ISBN : 9780999241141.\nDOI : 10.24963/ijcai.2019/658.\n[28] Y . Lee, J. Kim, and P. Kang, “LAnoBERT : System Log\nAnomaly Detection based on BERT Masked Language\nModel”, Nov. 2021. arXiv: 2111 . 09564. [Online].\nAvailable: http://arxiv.org/abs/2111.09564.\n[29] G. Jawahar, B. Sagot, and D. Seddah, “What does\nBERT learn about the structure of language?”, in ACL\n2019 - 57th Annu. Meet. Assoc. Comput. Linguist.\nProc. Conf., Association for Computational Linguistics\n(ACL), 2020, pp. 3651–3657, ISBN : 9781950737482.\nDOI : 10.18653/v1/p19-1356. [Online]. Available: https:\n//aclanthology.org/P19-1356.\n[30] D. Yenicelik, F. Schmidt, and Y . Kilcher, “How does\nBERT capture semantics? A closer look at polysemous\nwords”, Association for Computational Linguistics\n(ACL), Nov. 2020, pp. 156–162. DOI : 10 . 18653 / v1 /\n2020 . blackboxnlp - 1 . 15. [Online]. Available: https : / /\naclanthology.org/2020.blackboxnlp-1.15.\n[31] C. Ying, T. Cai, S. Luo, et al., Do transformers really\nperform bad for graph representation? , 2021. arXiv:\n2106.05234 [cs.LG].\n[32] V . P. Dwivedi and X. Bresson, A generalization of\ntransformer networks to graphs , 2021. arXiv: 2012 .\n09699 [cs.LG].\n[33] G. Pang, C. Shen, L. Cao, and A. V . D. Hengel,\n“Deep learning for anomaly detection: A review”, ACM\nComput. Surv., vol. 54, no. 2, Mar. 2021, ISSN : 0360-\n0300. DOI : 10.1145/3439950. [Online]. Available: https:\n//doi.org/10.1145/3439950.\n[34] R. Chalapathy and S. Chawla, “Deep Learning for\nAnomaly Detection: A Survey”, 2019. arXiv: 1901 .\n03407. [Online]. Available: http://arxiv.org/abs/1901.\n03407.\n[35] N. Zhao, H. Wang, Z. Li, et al. , “An empirical\ninvestigation of practical log anomaly detection\nfor online service systems”, in ESEC/FSE 2021 -\nProceedings of the 29th ACM Joint Meeting European\nSoftware Engineering Conference and Symposium on\nthe Foundations of Software Engineering, vol. 21, 2021,\npp. 1404–1415, ISBN : 9781450385626. DOI : 10.1145/\n3468264.3473933. [Online]. Available: https://doi.org/\n10.1145/3468264.3473933.\n[36] Y . Lin, Y . C. Tan, and R. Frank, “Open Sesame:\nGetting inside BERT’s Linguistic Knowledge”, 2019,\npp. 241–253. DOI : 10 . 18653 / v1 / w19 - 4825. arXiv:\n1906.01698. [Online]. Available: https://github.com/.\n[37] Y . Goldberg, “Assessing BERT’s Syntactic Abilities”,\n2019. arXiv: 1901.05287. [Online]. Available: http://\narxiv.org/abs/1901.05287.\n[38] R. Bommasani, D. A. Hudson, E. Adeli, et al., “On the\nOpportunities and Risks of Foundation Models”, Aug.\n2021. arXiv: 2108.07258. [Online]. Available: http://\narxiv.org/abs/2108.07258.\n[39] Y . Liu, M. Ott, N. Goyal, et al., “RoBERTa: A Robustly\nOptimized BERT Pretraining Approach”, 2019. arXiv:\n1907.11692. [Online]. Available: http://arxiv.org/abs/\n1907.11692.\nThis article has been accepted for publication in IEEE Transactions on Network and Service Management. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSM.2024.3358730\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n11\n[40] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer:\nThe Long-Document Transformer”, 2020. arXiv: 2004.\n05150. [Online]. Available: http://arxiv.org/abs/2004.\n05150.\n[41] L. N. Smith and N. Topin, “Super-convergence: very\nfast training of neural networks using large learning\nrates”, 2019, p. 36, ISBN : 9781510626775. DOI : 10 .\n1117/12.2520589. arXiv: 1708.07120.\n[42] J. Howard and S. Ruder, “Universal language model\nfine-tuning for text classification”, in Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\nMelbourne, Australia: Association for Computational\nLinguistics, Jul. 2018, pp. 328–339. DOI : 10 . 18653 /\nv1/P18-1031. [Online]. Available: https://aclanthology.\norg/P18-1031.\n[43] J. Howard and S. Gugger, “Fastai: A layered api\nfor deep learning”, Information (Switzerland) , vol. 11,\nno. 2, pp. 1–26, 2020, ISSN : 20782489. DOI : 10.3390/\ninfo11020108. arXiv: 2002.04688.\n[44] S. Gururangan, A. Marasovi ´c, S. Swayamdipta, et al. ,\n“Don’t Stop Pretraining: Adapt Language Models to\nDomains and Tasks”, 2020, pp. 8342–8360. DOI : 10.\n18653/v1/2020.acl-main.740. arXiv: 2004.10964.\n[45] A. Kumar, A. Raghunathan, R. Jones, T. Ma, and P.\nLiang, “Fine-Tuning can Distort Pretrained Features and\nUnderperform Out-of-Distribution”, Feb. 2022. arXiv:\n2202.10054. [Online]. Available: http://arxiv.org/abs/\n2202.10054.\n[46] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I.\nJordan, “Detecting large-scale system problems by\nmining console logs”, in ICML 2010 - Proceedings,\n27th International Conference on Machine Learning ,\n2010, pp. 37–44, ISBN : 9781605589077.\n[47] A. Oliner and J. Stearley, “What supercomputers say:\nA study of five system logs”, in Proceedings of the\nInternational Conference on Dependable Systems and\nNetworks, 2007, pp. 575–584, ISBN : 0769528554. DOI :\n10.1109/DSN.2007.103.\n[48] G. A. Miller, “Wordnet: A lexical database for english”,\nCommunications of the ACM, vol. 38, no. 11, pp. 39–41,\n1995.\n[49] C. Almodovar, F. Sabrina, S. Karimi, and S. Azad,\n“LogFiT: Log Anomaly Detection using Fine-Tuned\nLanguage Models”, Mar. 2023. DOI : 10.36227/techrxiv.\n22290982.v1. [Online]. Available: https://www.techrxiv.\norg/articles/preprint/LogFiT Log Anomaly Detection\nusing Fine-Tuned Language Models/22290982.\n[50] S. Karimi, J. Yin, and J. Baum, “Evaluation methods for\nstatistically dependent text”, Computational Linguistics,\nvol. 41, no. 3, 2015.\n[51] L. McInnes, J. Healy, and J. Melville, “Umap: Uniform\nmanifold approximation and projection for dimension\nreduction”, arXiv preprint arXiv:1802.03426 , 2018.\nThis article has been accepted for publication in IEEE Transactions on Network and Service Management. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSM.2024.3358730\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7660103440284729
    },
    {
      "name": "Anomaly detection",
      "score": 0.5924952626228333
    },
    {
      "name": "Real-time computing",
      "score": 0.32459136843681335
    },
    {
      "name": "Data mining",
      "score": 0.2809227705001831
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74899385",
      "name": "Central Queensland University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I1292875679",
      "name": "Commonwealth Scientific and Industrial Research Organisation",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I42894916",
      "name": "Data61",
      "country": "AU"
    }
  ],
  "cited_by": 40
}