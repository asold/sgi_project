{
  "title": "Perspectives on Large Language Models for Relevance Judgment",
  "url": "https://openalex.org/W4366460536",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4288570892",
      "name": "Faggioli, Guglielmo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289375827",
      "name": "Dietz, Laura",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2553977857",
      "name": "Charles Clarke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2604019324",
      "name": "Demartini, Gianluca",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222735144",
      "name": "Hagen, Matthias",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3174052050",
      "name": "Hauff, Claudia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2196835065",
      "name": "Kando Noriko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3206930870",
      "name": "Kanoulas, Evangelos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222735147",
      "name": "Potthast, Martin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745078541",
      "name": "Stein, Benno",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224904060",
      "name": "Wachsmuth, Henning",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4284713818",
    "https://openalex.org/W4287822877",
    "https://openalex.org/W2574917025",
    "https://openalex.org/W2006149654",
    "https://openalex.org/W3043669475",
    "https://openalex.org/W4287710008",
    "https://openalex.org/W8550301",
    "https://openalex.org/W3146187623",
    "https://openalex.org/W1999322447",
    "https://openalex.org/W4306317650",
    "https://openalex.org/W4327644104",
    "https://openalex.org/W2047810014",
    "https://openalex.org/W2113474780",
    "https://openalex.org/W3175111331",
    "https://openalex.org/W2903058329",
    "https://openalex.org/W2146938270",
    "https://openalex.org/W1967953323",
    "https://openalex.org/W2914694065",
    "https://openalex.org/W2061503648",
    "https://openalex.org/W4224937983",
    "https://openalex.org/W4292938050",
    "https://openalex.org/W2166455213",
    "https://openalex.org/W3118140671",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2124504084",
    "https://openalex.org/W1497326661",
    "https://openalex.org/W2139516750",
    "https://openalex.org/W4293998609",
    "https://openalex.org/W3004392977",
    "https://openalex.org/W2563948597",
    "https://openalex.org/W2026969101",
    "https://openalex.org/W4313216063",
    "https://openalex.org/W2780799698",
    "https://openalex.org/W1546195864",
    "https://openalex.org/W2945760033",
    "https://openalex.org/W2115536324",
    "https://openalex.org/W3132995484",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W2794432940",
    "https://openalex.org/W2810116858",
    "https://openalex.org/W1550436572",
    "https://openalex.org/W2510759893",
    "https://openalex.org/W2799108077",
    "https://openalex.org/W4384660953",
    "https://openalex.org/W3191640844",
    "https://openalex.org/W2250857132",
    "https://openalex.org/W2781131455",
    "https://openalex.org/W2996500226",
    "https://openalex.org/W2058896506",
    "https://openalex.org/W2972668795",
    "https://openalex.org/W1543648998",
    "https://openalex.org/W2017633929",
    "https://openalex.org/W2559836063",
    "https://openalex.org/W1497983610",
    "https://openalex.org/W3208394801",
    "https://openalex.org/W2752644629",
    "https://openalex.org/W1900856733",
    "https://openalex.org/W1979562318",
    "https://openalex.org/W2144200917",
    "https://openalex.org/W1983595289",
    "https://openalex.org/W2057495142",
    "https://openalex.org/W4292341621",
    "https://openalex.org/W3142525300",
    "https://openalex.org/W2672337983",
    "https://openalex.org/W2798523458",
    "https://openalex.org/W4285129823",
    "https://openalex.org/W3035643691",
    "https://openalex.org/W2159981039",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W4284666093",
    "https://openalex.org/W4367860590",
    "https://openalex.org/W4234995456",
    "https://openalex.org/W4283791551",
    "https://openalex.org/W4285290591",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W15201044",
    "https://openalex.org/W2120308175",
    "https://openalex.org/W2008285050",
    "https://openalex.org/W3201762742",
    "https://openalex.org/W2110933113"
  ],
  "abstract": "When asked, large language models (LLMs) like ChatGPT claim that they can assist with relevance judgments but it is not clear whether automated judgments can reliably be used in evaluations of retrieval systems. In this perspectives paper, we discuss possible ways for LLMs to support relevance judgments along with concerns and issues that arise. We devise a human--machine collaboration spectrum that allows to categorize different relevance judgment strategies, based on how much humans rely on machines. For the extreme point of \"fully automated judgments\", we further include a pilot experiment on whether LLM-based relevance judgments correlate with judgments from trained human assessors. We conclude the paper by providing opposing perspectives for and against the use of~LLMs for automatic relevance judgments, and a compromise perspective, informed by our analyses of the literature, our preliminary experimental evidence, and our experience as IR researchers.",
  "full_text": "Perspectives on Large Language Models for Relevance Judgment\nGuglielmo Faggioli\nUniversity of Padova\nLaura Dietz\nUniversity of New Hampshire\nCharles L. A. Clarke\nUniversity of Waterloo\nGianluca Demartini\nUniversity of Queensland\nMatthias Hagen\nFriedrich-Schiller-Universit√§t Jena\nClaudia Hauff\nSpotify\nNoriko Kando\nNational Institute of Informatics (NII)\nEvangelos Kanoulas\nUniversity of Amsterdam\nMartin Potthast\nLeipzig University and ScaDS.AI\nBenno Stein\nBauhaus-Universit√§t Weimar\nHenning Wachsmuth\nLeibniz University Hannover\nABSTRACT\nWhen asked, large language models (LLMs) like ChatGPT claim\nthat they can assist with relevance judgments but it is not clear\nwhether automated judgments can reliably be used in evaluations\nof retrieval systems. In this perspectives paper, we discuss possible\nways for LLMs to support relevance judgments along with concerns\nand issues that arise. We devise a human‚Äìmachine collaboration\nspectrum that allows to categorize different relevance judgment\nstrategies, based on how much humans rely on machines. For the\nextreme point of ‚Äòfully automated judgments‚Äô, we further include a\npilot experiment on whether LLM-based relevance judgments corre-\nlate with judgments from trained human assessors. We conclude the\npaper by providing opposing perspectives for and against the use\nof LLMs for automatic relevance judgments, and a compromise per-\nspective, informed by our analyses of the literature, our preliminary\nexperimental evidence, and our experience as IR researchers.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíRelevance assessment.\nKEYWORDS\nlarge language models, relevance judgments, human‚Äìmachine col-\nlaboration, automatic test collections\n1 INTRODUCTION\nEvaluation is very important to the information retrieval (IR) com-\nmunity and the difficulty of proper evaluation setups is well-known\nWe thank Ian Soboroff for his ideas, comments, and other contributions.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan\n¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0073-6/23/07. . . $15.00\nhttps://doi.org/10.1145/3578337.3605136\nDear LLM, can you assist us with relevance judgments?\nYes, I can certainly provide assistance with relevance judgments.\nPlease provide me with more information about what you need help\nwith, and I will do my best to assist you.\nIf you need assistance with a specific task, such as evaluating the\nrelevance of a set of documents or web pages, please provide me with\nthe necessary information, such as the topic or query, the criteria\nfor relevance, and any other relevant details. I can then help you to\nperform the evaluations and provide feedback on the relevance of\nthe content.\nFigure 1: Asking ChatGPT for assistance on February 15,\n2023.\nand often discussed (e.g., [40, 54, 68, 70]). Many long-standing eval-\nuation campaigns like TREC, NTCIR, CLEF, or FIRE [15, 42, 47, 56]\ntrace their roots back to the Cranfield paradigm [20], which relies\non test collections that consist of (i) a document corpus, (ii) a set\nof information needs or topics, and (iii) relevance judgments for\ndocuments on the topics. Critically, according to the Cranfield par-\nadigm, human assessors are needed for the relevance judgments‚Äîa\ntime-intensive and costly procedure.1\nHowever, over the past decades, in IR more and more tasks have\nbeen delegated to machines that were traditionally performed by\nhumans, starting with indexing and retrieval. While the idea of au-\ntomatically generated judgments has been considered before [77], it\nhas not found widespread use in the IR community. Other previous\nideas to minimize the cost of collecting relevance judgments include\njudging text nuggets instead of documents [66], using crowdwork-\ners [3, 14] (though this comes with its own set of problems [63]),\ncleverly selecting which documents to judge [17, 55], constructing\ntest collections from Wikipedia [ 30], or automating parts of the\njudgment process via a QA system [69].\n1As a concrete example, for the 50 topics in the TREC-8 Ad Hoc track [81], 129 partici-\npating systems led to more than 86,000 pooled documents to judge, requiring more\nthan 700 assessor hours at a cost of about USD 15,000.\narXiv:2304.09161v2  [cs.IR]  18 Nov 2023\nICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan Guglielmo Faggioli et al.\nFigure 1 shows the response of ChatGPT2 when asked whether\nit can assist with relevance judgments. ChatGPT suggests that it is\nable to carry out relevance judgments, but it is unclear how well\nsuch judgments align with those made by human annotators. In\nthis perspectives paper, we explore whether we are on the verge\nof being able to delegate relevance judgments to machines‚Äîeither\nfully or partially‚Äîby employing large language models (LLMs). We\naim to provide a balanced view on this contentious question by\npresenting both consenting and dissenting voices in the scientific\ndebate surrounding the use of LLMs for relevance judgments. Al-\nthough a variety of document modalities exist (audio, video, images,\ntext), we here focus on text-based test collections. The consolidated\nmethodology for assessing the relevance of textual documents,\nwhich dates back to the Cranfield paradigm, enables us to carry out\na grounded comparison between LLMs and human assessors. While\nthe technology might not be ready yet to provide fully automatic\nrelevance judgments, we argue that LLMs are already able to help\nhumans in relevance assessment‚Äîto various extents. To model the\nrange of automation options, we propose and discuss a spectrum of\ncollaboration between humans and LLMs (cf. Table 1): from manual\njudgments, the current setup, to fully automated judgments that\nare carried out solely by LLMs as a potential future option.\nSome of the spectrum‚Äôs scenarios have already been studied\n(cf. Section 2), while others are currently emerging. We describe\nrisks as well as open questions that require further research and we\nconduct a pilot feasibility experiment where we assess how well\njudgments generated by LLMs agree with humans, including an\nanalysis of LLM-specific caveats. To conclude our paper, we provide\ntwo opposing perspectives‚Äîfor and against the use of LLMs as rel-\nevance ‚Äúassessors‚Äù‚Äî, as well as a compromise between them. All of\nthe perspectives are informed by our analyses of the literature, our\npilot experimental evidence, and our experience as IR researchers.\n2 RELATED WORK\nFollowing the Cranfield paradigm, a test collection-based approach\nto IR evaluation requires documents, queries, and relevance judg-\nments for query‚Äìdocument pairs. The traditional approach to ac-\nquire relevance judgments is to hire human assessors. However,\nthe judgment effort is staggering, leading to a range of approaches\nto assist the assessors or to automate tedious tasks. Below, we de-\nscribe existing approaches and relate them to our human‚Äìmachine\ncollaboration spectrum (cf. Table 1).\n2.1 Human Judgment\nAs document collections kept growing in size, the ratio of docu-\nments that could practically be judged by human assessors kept\ngetting smaller. This triggered the IR community to look for ways\nto scale-up human-generated relevance judgments. Around 2010,\nreplacing trained human assessors by micro-task crowdsourcing\nbecame an option [3] so that the community started to study the\nreliability of crowdsourced relevance judgments [14] and questions\nrelated to cost and quality management [63].\nThe workforce increased via crowdsourcing usually comes with\na decreased reliability, often due to the complicated interactions\nbetween crowdworkers and task requesters [65]. Still, before the\n2https://chat.openai.com/chat\nadvent of large language models, several studies showed that crowd-\nsourcing is a viable alternative to scale-up relevance judgments\ncompared to the ‚Äúclassic‚Äù hiring of trained human assessors‚Äîas\nlong as the domain is accessible to non-experts and quality con-\ntrol mechanisms are put in place [79]. Quality control mechanisms\nmay include label aggregation methods [ 75], task design strate-\ngies [2, 58], and crowdworker selection strategies [41].\nSome studies have tried to increase the judgment efficiency\nof crowdworkers by adding machine-generated information (e.g.,\nmetadata) [85] but recent findings suggest that LLMs alone are even\nbetter at several text annotation tasks than crowdworkers [43].\n2.2 Human Verification and AI Assistance\nIn this scenario, humans partially relinquish control over which\ndocuments will be assessed or how machine assessments will be\nderived but humans remain in control of defining relevance.\nFor example, some studies suggest to adjust evaluation metrics\nto be able to deal with incomplete judgments (e.g., [39, 87]). This\nway, judgment costs can be reduced by reducing the number of\nassessments needed for evaluating retrieval systems.\nAlternatively, Keikha et al. [59] suggest to automatically transfer\nmanual relevance judgments in the context of passage retrieval: any\nunjudged passage that has a high similarity to a judged passage will\ninherit the judged passage‚Äôs relevance label on a given topic. In the\noriginal setup, the authors used ROUGE as the similarity measure\nbut also ‚Äúmodern‚Äù alternatives like BertScore [90] could be tried‚Äîas\ntransferring relevance judgments between corpora without proper\nsimilarity checking is problematic [38].\nOther ideas for semi-automatic relevance judgments are active\nlearning [21] (e.g., human assessors only label documents for which\nan automatic relevance assessment has a low confidence) or to\nautomatically identify potentially relevant documents that only\nmanual runs would contribute to the pool [55]‚Äîin order to construct\nlow-bias reusable test collections.\nInstead of asking humans for relevance assessments on query‚Äì\ndocument pairs, Sander and Dietz [69] suggest to ask humans for\n(exam) questions related to a query / topic that should be answerable\nfrom the content of a relevant document. The more of the manually\nformulated questions an automatic question-answering system can\nanswer, the more relevant a to-be-judged document is‚Äîcaptured by\nthe authors‚Äô proposed EXAM answerability metric. Similar ideas\nhave also been used successfully in other labeling tasks [29, 32, 51].\n2.3 Fully Automated Test Collections\nInspired by ideas from evaluating aspect-based summarization [49]\nor text segmentation [ 6], the Wikimarks approach [ 30] aims to\nautomatically create queries and judgments for a test collection. The\ntitle and subheadings of Wikipedia articles are used to formulate\nqueries and the passage below the title / heading is assumed to be\nrelevant for the respective query‚Äîwithout actual human judgments.\nSimilar distant supervision-style approaches to acquire relevance\nassessments for ‚Äúartificial‚Äù queries exploit other facets of human-\nauthored (semi-structured) documents: anchor text [7], metadata of\nscientific articles [13], categories in the Open Directory Project [11],\nglosses in Freebase [26], or infoboxes [50, 57].\nPerspectives on Large Language Models for Relevance Judgment ICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan\nTable 1: A spectrum of collaborative\n human ‚Äì\n machine\ntask organization to produce relevance judgments. The ‚ñ≥\nindicates where on the spectrum each possibility falls.\nCollaboration\nIntegration Task Organization\nHuman Judgment\n‚ñ≥ Humans do all judgments manually without\nany kind of support.\n‚ñ≥ Humans have full control of judging but are\nsupported by text highlighting, document\nclustering, etc.\nAI Assistance\n‚ñ≥ Humans judge documents while having\naccess to LLM-generated summaries.\n‚ñ≥ Balanced competence partitioning. Humans\nand LLMs focus on (sub-)tasks they are good\nat.\nHuman Verification\n‚ñ≥ Two LLMs each generate a judgment, and\nhumans select the better one.\n‚ñ≥ An LLM produces a judgment (and an\nexplanation) that humans can accept or\nreject.\n¬∑ùëõ‚ñ≥ LLMs are considered crowdworkers with\nvaried specific characteristics, but\nsupervised / controlled by humans.\nFully Automated\n‚ñ≥ Fully automatic judgments.\nAlso for the task of query performance prediction (QPP) [16, 48],\nthe goal is to estimate retrieval effectiveness (i.e., the ability to re-\nturn relevant results) without having manual relevance judgments‚Äî\noften even without knowing the actual retrieval results. While some\nrecent studies effectively used LLMs in QPP scenarios [4, 5, 18, 28,\n34], an open question still is how well LLM-based relevance assess-\nments agree with manual assessments. A study on the leaderboards\nof the TREC CAR track found a very high rank correlation [31] and\nsome preliminary evidence seems to indicate that LLMs can replace\nhuman assessors in several NLP tasks [92]‚Äîwith a high variance\nin the quality of the annotations, though‚Äî, but MacAvaney and\nSoldaini [62] found that automatic relevance judgments may cor-\nrelate poorly with human assessments. Still, system leaderboards\nobtained from the automatic relevance judgments were comparable\nto those based on manual assessments.\n3 SPECTRUM OF HUMAN‚ÄìMACHINE\nCOLLABORATION\nTo discuss potential capabilities of LLMs in the context of relevance\njudgments, we devise a human‚Äìmachine collaboration spectrum\nwith different levels of ‚Äúlabor division‚Äù between humans and LLMs\n(cf. Table 1 for an overview). At one end, humans manually judge\nwithout any LLM interaction, while at the other end, LLMs replace\nhumans completely. In between, LLMs assist humans at various\ndegrees of interdependence.\nHuman Judgment. On this end of the spectrum, humans manu-\nally decide what is relevant without being influenced by an LLM. In\nreality, of course, humans are still supported by basic features of a\njudgment interface. Such features might still be based on heuristics\nthat do not require any form of automatic training / feedback. For\ninstance, humans may define so-called scan terms to be highlighted\nin a text, they may limit viewing the pool of documents that have\nalready been judged, or they may order documents by similarity\nso that it is easier to assign the same relevance label to similar\ndocuments. This end of the spectrum thus represents the status\nquo, where humans are considered the only reliable judges.\nAI Assistance. More advanced assistance can come in many\nforms. For example, an LLM may generate a summary of a to-\nbe-judged document so that a human assessor can more efficiently\nmake a judgment based on the compressed representation. Another\napproach could be to manually define information nuggets that\nare relevant (e.g., exam questions / answers [69]) and to then train\nan LLM to automatically indicate how many test nuggets are con-\ntained in a to-be-judged document (e.g., via a QA system). This\ndirectly implies questions towards improving the human‚Äìmachine\ncollaboration: How to employ LLMs, as well as other AI tools, to aid\nhuman assessors in devising reliable judgments while enhancing\nthe efficiency of the process? What are tasks that can be taken over\nby LLMs (e.g., document summarization or keyphrase extraction)?\nHuman Verification. For each document to judge, a first-pass\njudgment of an LLM is automatically produced as a suggestion\nalong with a generated rationale. We consider this to be a human-\nin-the-loop approach: one or more LLMs provide their relevance\njudgment and humans verify them. In most cases, the humans\nmight not have to intervene at all but they might still be required\nin challenging situations where the LLM has low confidence.\nAn idea could also follow the ‚Äòpreference testing‚Äô paradigm [84]:\ntwo LLMs each generate a judgment, and a human will select\nthe better one‚Äîintervening only in case of disagreement between\nthe LLMs. Still, in the scenario of human verification, humans make\nthe ultimate decision wherever needed. A concern then could be\nthat some bias of the LLMs might affect the final relevance judg-\nments, as humans might not be able to recognize all biases. Related\nquestions that we wish to raise within the community are: What\nsub-tasks of the judgment process require human input (e.g., prompt\nengineering [78, 91]‚Äîfor now) and for what tasks or judgments\nshould human assessors not be replaced by machines?\nFully Automated. If LLMs were able to reliably assess relevance,\nthey could completely replace humans in the judgment process.\nA fully automatic judgment system might be as good as humans\nICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan Guglielmo Faggioli et al.\nin producing high-quality relevance judgments (for a specific cor-\npus / domain) but automatic judgments might even surpass humans\nin terms of quality, which raises the follow-up issue of how to de-\ntect that. A question that our community ought to investigate thus\nis: How can and should humans be replaced entirely by LLMs\nin the judgment process? Indeed, one could go as far as asking\nwhether generative LLMs can and should be used to create com-\nplete test collections by generating documents / passages, as well\nas queries / conversations and relevance judgments.\nA central aspect to be investigated is where on this four-level\nhuman‚Äìmachine collaboration spectrum we actually obtain the\nideal relevance judgments at the best cost. At this point, humans\nperform tasks that humans are good at, while machines perform\ntasks that machines are good at‚Äîoften referred to as competence\npartitioning [37, 46]: a task is assigned to either a human or a\nmachine, depending on who is better suited. Note that in our current\nversion of the spectrum, we still (optimistically) show balanced\ncompetence partitioning as part of ‚ÄòAI assistance‚Äô.\n4 OPEN ISSUES AND OPPORTUNITIES\nIn this section, we identify several issues that arise when LLMs are\nused during relevance judgment tasks. We discuss open questions,\nrisks we foresee, as well as opportunities to move beyond the cur-\nrently accepted retrieval evaluation paradigms.\n4.1 LLM Judgment Cost and Quality\nIt is currently unclear what the benefits and risks of LLMs for rel-\nevance judgments are. This situation is similar to the time when\ncrowdsourced judgments became possible. Until about 10‚Äì15 years\nago, judgments typically came from (trained) in-house experts but\nthen suddenly could be delegated to cheaper crowdworkers result-\ning in an increased amount of financially feasible judgments‚Äîbut\nat a substantially decreased quality [45] so that quality-assurance\nmethods had to be developed [27]. With LLMs, history may some-\nwhat repeat itself. Based on current pricing models, the inference\ncosts per LLM judgment can be much lower than for crowdsourcing\n(cf. the estimates in the column ‚ÄòCost‚Äô of Table 2) so that again an in-\ncrease in the amount of financially feasible judgments (from LLMs)\nis very likely. Still, the effect with respect to judgment quality is\nunclear‚Äîeven improvements are possible‚Äîand can only be clari-\nfied / controlled by conducting respective studies and developing\nLLM-specific quality estimation and assurance methods.\nThe pressing question is: What is the effectiveness of LLM-based\njudgment (support)? In Table 2, we depict our current understand-\ning by distinguishing four assessor types (user, expert, crowdworker,\nand LLM) and four judgment tasks: preference (which of two doc-\numents is more relevant?), binary (is this document relevant?),\ngraded (how relevant is this document?), and explained (justify\na judgment). The table entries indicate potential substitutions in\nthe sense that similar abilities of LLMs hint at a replaceability of\nrespective assessors (e.g., LLMs instead of crowdworkers for binary\njudgments). Still, the table cannot fully clarify the role of LLMs as\nwe are still in the early stages of development and simply do not\nknow the eventual capabilities: ‚äïand ‚äôin the ‚ÄòLLM‚Äô row should\nthus be interpreted with these current uncertainties in mind.\nTable 2: Abilities of different types of assessors to handle\nvarious types of judgments. Similar levels of ability might\nhint at scenarios where specific types of human assessors\nmight be replaced by LLMs.\nType of Judgment\nType of Assessor Cost PreferenceBinary GradedExplained\nUser free ‚äï ‚äï ‚äï ‚äô\nExpert expensive ‚äï‚äï ‚äï‚äï ‚äï ‚äï\nCrowdworker cheap ‚äô ‚äï ‚äï ‚äô\nLLM\n very cheap ‚äï ‚äï ‚äô ‚äï\nLegend: ‚äï‚äïcan judge, ‚äïdepends, ‚äôunknown\nTo align their judgments with humans, LLMs could be fine-tuned\nby observing human relevance assessors or they might use an active\nlearning strategy [73, 74, 89]. For instance, an LLM could start with\nmild suggestions to a human assessor on how relevant a document\nis and could then continuously learn from the actual judgments of\nthe assessor to improve its own suggestions.\n4.2 Human Verification\nUsing Multiple LLMs as Assessors. While hiring multiple human\nrelevance assessors with different backgrounds usually is very easy\nand potentially occurring judgment disagreements are not unre-\nsolvable [35], many LLMs are trained on very similar web corpora\nwhich may yield highly correlated judgments of not yet known\nquality or bias. A possible solution to obtain less correlated LLMs\nis to train or fine-tune them on different data (e.g., subcorpora).\nFine-tuning on different user types could even yield ‚Äúpersonalized‚Äù\nmodels [53, 83, 88] that might enable automatic judgments accord-\ning to specific user groups‚Äô perspectives on relevance.\nTruthfulness & Misinformation. An important aspect of relevance\njudgments is factuality. For a question like ‚Äúdo lemons cure can-\ncer?‚Äù, some top-ranked document may indeed suggest lemons as\na treatment for cancer. While topically matching, the content is\nunlikely to be factually correct and the document should therefore\nbe judged as non-relevant. Trained human assessors may very well\nbe able to determine the trustworthiness of a document and, at least\nto some extent, the truthfulness. But the ability of LLMs to do so is\nquite unclear and probably also depends on the characteristics of\nthe training data that often are not disclosed. This raises at least\ntwo questions: Can we automatically assess the reliability and fac-\ntuality of LLM-generated relevance judgments? Can we identify\nthe textual training sources underlying an LLM‚Äôs judgment and can\nwe verify that they are represented accurately?\nGoing forward, it will also be vital to be able to distinguish\nhuman-generated from automatically generated sources, especially\nin contexts such as journalism where correctness is critical.\nBias. Bender et al. [12] highlight limitations of LLMs and identify\nbias as a severe risk. As LLMs are intrinsically biased [ 9, 52, 60],\nPerspectives on Large Language Models for Relevance Judgment ICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan\nsuch bias may also be reflected in LLMs‚Äô relevance judgments. For\nexample, an LLM might be prone to consider scientific documents\nas relevant, while documents written in informal language are per-\nceived as less relevant. The IR community should focus on finding\nways to evaluate LLMs in terms of judgment bias, i.e., to analyze\nto what extent the intrinsic bias actually affects evaluations using\nLLM-supported / LLM-based relevance judgments.\nFaithful Reasoning. LLMs often generate text that contains inac-\ncurate or false information (i.e., they confabulate or ‚Äúhallucinate‚Äù)\nand usually do so in an affirmative manner that makes it difficult\nfor humans to even suspect errors. In response, the NLP commu-\nnity is exploring a new research direction called ‚Äúfaithful reason-\ning‚Äù [23]. This approach aims to generate text that is less opaque,\nalso describing explicitly the step-by-step reasoning, or the ‚Äúchain\nof thoughts‚Äù [61]. A similar idea of ‚Äúreasoned‚Äù automatic relevance\njudgments might be an interesting IR research direction.\nExplaining Relevance to LLMs. Judgment guidelines often provide\na comprehensive overview of what constitutes a relevant document\nin what scenario‚Äîmost famously, Google‚Äôs search quality evaluator\nguidelines have more than 170 pages. 3 Still, it is open how such\nguidelines should be ‚Äútranslated‚Äù to prompt LLMs. In addition,\nrelevance may go beyond topical relevance [ 71]. For instance, a\ncertain style may be required or the desired information should\nallure users from certain communities or cultures with different\nbelief systems. We do not yet know to what extent LLMs are capable\nof assessing such different variations of relevance so that human\nintervention might still play a central role when taking document\naspects into account that may not yet be easily discernable by LLMs.\n4.3 Fully Automated\nLLM-based Evaluation of LLM-based Systems. In the fully auto-\nmated scenario, a circulatory problem can arise: Why not use a good\nLLM-based relevance assessor as an actual approach to produce\na ranking? However, in practical settings, we expect LLMs used\nfor ranking to be much smaller (more cost effective, lower latency,\netc.; e.g., via knowledge distillation) than LLMs used for judging. In\naddition, the judging LLMs may have additional information about\nrelevant facts / questions / nuggets that a ranker does not know,\nand, as assessment latency might not be an issue, different (more\ncomplex) judging LLMs may even be combined in an ensemble.\nMoving beyond Cranfield. Given limited time or monetary bud-\ngets, retrieval evaluations based on manual judgments are often\nonly feasible due to ‚Äústandard‚Äù simplifying assumptions. For exam-\nple, document collections are assumed to be static, small sets of\nqueries / topics are assumed to suffice, and a document‚Äôs relevance\nis assumed to not change (definitely a simplification [72, 80]) and to\nbe independent of other documents. If LLMs would produce reliable\nrelevance judgments with little human verification effort, many\nof the simplifying assumptions could be relaxed. For example, in\nsearch sessions or in the TREC CAsT track [24, 25],4 information\nneeds are changing over the course of a session or a conversation as\n3https://guidelines.raterhub.com/searchqualityevaluatorguidelines.pdf\n4TREC CAsT is a shared task that aims at evaluating conversational agents and thus\nprovides information needs in the form of multi-turn conversations, each containing\nseveral utterances that a user might pose to an agent.\nthe user learns more about a topic. Collaborative human‚Äìmachine\nrelevance judgment might help to scale-up evaluations using such\nmore comprehensive and thus more realistic notions of relevance.\nMoving beyond Human. Finally, at one end of our proposed spec-\ntrum, machines may surpass humans in the relevance judgment\ntask. This phenomenon has already been witnessed in a variety\nof NLP tasks, such as scientific abstract classification [44] or sen-\ntiment detection [82]. Humans are likely to make mistakes when\njudging relevance and are limited by time. It is conceivable that\nLLMs with sufficient monetary funds will be capable of providing\na larger number of more consistent judgments. However, if we use\nhuman-annotated data as a gold standard, we will not be able to\ndetect when LLMs surpass human judgment quality as we then will\nhave reached the limit of measurement.\n5 PRELIMINARY ASSESSMENT\nTo provide a preliminary assessment of today‚Äôs LLMs‚Äô capability for\nrelevance judgments, we conduct an empirical comparison between\nhuman and LLM assessors. This comparison includes two LLMs\n(GPT-3.5 and YouChat), two test collections (the TREC-8 ad hoc\nretrieval task [81] and the TREC 2021 Deep Learning track [22]), two\ntypes of judgments (binary and graded), and two tailored prompts.\nThe experiments were conducted in January and February 2023.\n5.1 Methodology\nOur experiments are not meant to be exhaustive but rather to\nexplore where LLMs (dis-)agree with manual relevance judgments.\nLLMs. We selected two LLMs for our experiments: GPT-3.5, more\nspecifically text-davinci-0035 accessed via OpenAI‚Äôs API, 6 and\nYouChat. GPT-3.5 is an established standard model for many ap-\nplications and thus serves as a natural baseline, while, shortly af-\nter OpenAI‚Äôs release of ChatGPT, YouChat has been one of the\nfirst LLMs to be fully integrated with a commercial search en-\ngine7 for the task of generating a new kind of search engine result\npage (SERP) on which a generated text summarizes the top-ùëò search\nresults (ùëò ‚â≤ 5) in a query-biased way with numbered in-text refer-\nences to ùëò results listed as ‚Äúblue links‚Äù below the summary.\nTest Collections. We base our experiments on (i) the ad hoc re-\ntrieval task of TREC-8 [81] and (ii) the passage retrieval task of the\nTREC 2021 Deep Learning track (TREC-DL 2021) [22]. Both collec-\ntions have many relevance judgments but also have contrasting\nproperties. TREC-DL 2021 comprises short documents and queries\nphrased as questions, while TREC-8 comprises much longer, com-\nplete documents, with detailed descriptions of information needs,\nexplicitly stating what is (not) considered relevant. As an experi-\nmental corpus, TREC-DL 2021 provides the additional benefit that\nits release date (second half of 2021) falls after the time that training\ndata was crawled for GPT-3.5 (up to June 2021) but falls before the\nrelease of GPT-3.5 itself (November 2022).8 Hence, GPT-3.5 has not\nbeen trained on TREC-DL 2021 relevance judgments, nor has it been\nused as a component in any system participating in TREC-DL 2021.\n5https://spiresdigital.com/new-gpt-3-model-text-davinci-003\n6https://platform.openai.com/docs/api-reference/introduction\n7https://you.com.\n8https://platform.openai.com/docs/models/overview\nICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan Guglielmo Faggioli et al.\nInstruction: You are an expert assessor making TREC relevance judg-\nments. You will be given a TREC topic and a portion of a document.\nIf any part of the document is relevant to the topic, answer ‚ÄúYes‚Äù.\nIf not, answer ‚ÄúNo‚Äù. Remember that the TREC relevance condition\nstates that a document is relevant to a topic if it contains information\nthat is helpful in satisfying the user‚Äôs information need described by\nthe topic. A document is judged relevant if it contains information\nthat is on-topic and of potential value to the user.\nTopic: {topic}\nDocument: {document}\nRelevant?\nInstruction: Indicate if the passage is relevant for the question.\nQuestion: {question}\nPassage: {passage}\nFigure 2: Prompts used in our experiments on TREC-8 (top)\nand TREC-DL 2021 (bottom). At the placeholders {topic},\n{document}, {question}, and {passage}, the actually sampled\npairs are included.\nJudgment Sampling. We sampledùëõ = 1000 topic‚Äìdocument pairs\neach from the relevance judgments files of TREC-8 and TREC-\nDL 2021 but due to a limited scalability when using YouChat, for\nsome experiments had to restrict ourselves to 100 random samples\nper relevance grade (binary for TREC-8, graded for TREC-DL 2021).\nPrompts. We used two simple and straightforward prompts for\nthe two collections (cf. Figure 2) but explicitly did not spend time on\noptimizing the prompts (so-called ‚Äúprompt engineering‚Äù) to keep\nthe prompts straightforward and to the point as a first baseline.\nFormulating and studying better prompts is left for future work.\nAnswer Parsing. We recorded the models‚Äô generated answers and\nmapped them to binary relevance judgments. As for GPT-3.5, the\nprompts and setting temperature = 0 were sufficient to constrain\nthe model to emit only one of the requested relevance grades. As\nfor YouChat, the answers were more verbose but rather homoge-\nneous. With only two exceptions, they started with ‚ÄúThe docu-\nment / passage is relevant [. . . ]‚Äù or with ‚ÄúThe document / passage\nis not relevant [. . . ]‚Äù and were thus straightforward to parse.\n5.2 Results\nTable 3 shows the results for TREC-8. We observe a clear divide\naccording to the relevance label. For documents judged as non-\nrelevant by human assessors, GPT-3.5 generates the same judgment\nin 90% of the cases. In contrast though, for the documents judged as\nrelevant by human assessors, this agreement drops to 47%. Likewise,\nYouChat has judged 74% of the non-relevant documents correctly,\nbut this agreement drops even more to 33% for the relevant ones.\nInterestingly though, the results on TREC-DL 2021 in Table 4\nshow an opposite trend for YouChat: the higher the relevance grade,\nthe more YouChat is in line with the human assessors. For 96 out of\n100 question‚Äìpassage pairs that TREC assessors judged as highly\nrelevant (i.e., grade 3), YouChat agreed with the assessors. In con-\ntrast, for the non-relevant question‚Äìpassage pairs, the agreement\nseems more or less random: YouChat only agrees with the manual\nTable 3: Judgment agreement on TREC-8 between TREC as-\nsessors and the LLMs; 1000 topic‚Äìdocument pairs for GPT-3.5\nand 100 for each grade (relevant, non-relevant) for YouChat.\nLLM Prediction TREC-8 Assessors Cohen‚Äôs ùúø\nRelevant Non-relevant\nGPT-3.5 Relevant 237 48 0.38Non-relevant 263 452\nYouChat Relevant 33 26\n0.07 Non-relevant 6774\nTable 4: Judgment agreement on TREC-DL 2021 between\nTREC assessors and the LLMs; 100 question‚Äìpassage pairs\nfor each grade from 3 (highly relevant) to 0 (non-relevant).\nLLM Prediction TREC-DL 2021 Assessors\n3 2 1 0 Cohen‚Äôs ùúø\nGPT-3.5 Relevant 89 65 48 16 0.40Non-relevant 11 35 52 84\nYouChat Relevant 96 93 79 42 0.49Non-relevant 4 7 21 58\nassessments on 42 of the 100 pairs. Similarly, on TREC-DL 2021,\nGPT-3.5 seems to have problems with the middle grades of 1 and 2.\nWe thus hypothesize that human assessors may use subtle details\nto distinguish ‚Äòsomewhat relevant‚Äô from ‚Äòprobably non-relevant\ndocuments‚Äô in the binary case that are not captured by the LLMs\nand similarly that also the ‚Äúdifferences‚Äù that human assessors use\nto decide some difficult 1-or-0 cases on a 3‚Äì0-scale might rather\nstill be too subtle to be recognizable for the LLMs.\n6 RE-JUDGING TREC 2021 DEEP LEARNING\nTo complement the experiments from Section 5, we now re-evaluate\nsubmissions to the passage ranking task of the TREC 2021 Deep\nLearning track [22] (TREC-DL 2021) using LLM-based judgments\nbut adhering as closely as possible to the methodology used in the\ntrack itself [22], including the use of graded judgments.\n6.1 Methodology\nThe participants of TREC-DL 2021 submitted 63 runs, each compris-\ning up to 1000 ranked passages for 200 questions. These runs were\npooled, and the results for 53 questions were judged by assessors\nusing a combination of methods, including active learning [1, 76].\nThis generated a total of 10,828 judgments on a 4-point scale: ‚Äòper-\nfectly relevant‚Äô ‚âª‚Äòhighly relevant‚Äô ‚âª‚Äòrelevant‚Äô (named ‚Äòrelated‚Äô in\nthe track) ‚âª‚Äònon-relevant‚Äô (named ‚Äòirrelevant‚Äô in the track).\nWe re-judged this pool using the GPT-3.5 text-davinci-003 lan-\nguage model, as accessed through Open AI‚Äôs API in February 2023.\nConsistent with a classification task‚Äîand with our GPT-3.5 experi-\nments reported in Section 5‚Äî, we settemperature = 0 and otherwise\nuse default parameters and settings.\nPerspectives on Large Language Models for Relevance Judgment ICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan\nFigure 3: Scatter plots of the effectiveness of TREC-DL 2021\nruns (MAP (top) and NDCG@10 (bottom)) according to the\ntrack‚Äôs human judgments and our LLM-based judgments. A\npoint represents a single run averaged over all questions.\nOur relatively long prompt9 is inspired by a prompt of Ferraretto\net al. [36]: importantly‚Äîand different from the prompt in Figure 2‚Äî\nit leverages few-shot learning by listing multiple examples illustrat-\ning different levels of relevance for different questions. We provide\none example each for ‚Äòperfectly relevant‚Äô, ‚Äòhighly relevant‚Äô, and\n‚Äòrelevant‚Äô, and we provide two examples for ‚Äònon-relevant‚Äô, with\none providing a judged ‚Äònon-relevant‚Äô passage, and the other pro-\nviding an unrelated passage from the pool. These examples were\nchosen arbitrarily from the pool, based on the TREC judgments.\nWe also used the term ‚Äòrelevant‚Äô in the prompt, instead of ‚Äòrelated‚Äô,\nsince ‚Äòrelated‚Äô is a non-standard label for relevance judgments; in\npreliminary experiments, the LLM would sometimes return ‚Äòrel-\nevant‚Äô unprompted. Using this prompt, each judgment did cost\nabout USD 0.01‚Äîwe spent a total of USD 111.90, including a small\nnumber of duplicate requests due to failures and other issues. In\ncomparison, Clarke et al. [19] report spending USD 0.25 per human\njudgment on a task of similar scope‚Äîwith a single-page ‚Äúprompt‚Äù\nand no training of assessors.\nTable 5: Confusion matrices comparing all official TREC\nquestion‚Äìpassage judgments with GPT-3.5 judgments on\nTREC-DL 2021 question‚Äìpassage pairs. The upper matrix\n(GRADED) compares judgments on all four relevance levels.\nThe lower matrix (BIN.) collapses the relevance labels to two\nlevels, following the TREC-DL 2021 convention for comput-\ning binary measures.\nPrediction TREC-DL 2021 Assessors\nPerf. rel. High. rel. Related Irrel.\nGRADED\nPerfectly relevant 250 248 177 87\nHighly relevant 360 575 628 370\nRelevant 328 880 798 442\nNon-relevant 148 638 1460 3439\nPrediction TREC-DL 2021 Assessors\nRelevant Not relevant\nBIN.\nRelevant 1433 1262\nNon-relevant 1994 6139\n6.2 Results\nTable 5 shows the ‚Äúagreement‚Äù on the full 4-point relevance scale\nand on a binarized relevance scale‚Äîfollowing the TREC-DL 2021\nconvention, we map ‚Äòperfectly relevant‚Äô and ‚Äòhighly relevant‚Äô to\n‚Äòrelevant‚Äô, and ‚Äòrelevant‚Äô and ‚Äònon-relevant‚Äô to ‚Äònon-relevant‚Äô. On\nthe binarized judgments the Cohen‚Äôs ùúÖ is 0.26, which indicates a\n‚Äòfair‚Äô level of agreement. Note that on a similar experiment with two\ntypes of human judgments, Cormack et al. [21] report a Cohen‚Äôs ùúÖ\nof 0.52 (‚Äòmoderate‚Äô agreement).\nCompared to the system rankings using the official judgments,\nusing the LLM judgments to compute standard evaluation measures\nfor the runs submitted to TREC-DL 2021 yields the correlations and\nKendall‚Äôsùúè values shown in Figure 3. Note that the top run under the\nofficial judgments remains the top run under the LLM judgments.\nFor comparison, Voorhees [80] report a Kendall‚Äôs ùúè = .90 for MAP\non a similar experiment with two types of human judgments.\nWe find that measures computed under the LLM judgments are\nless sensitive than measures computed under human judgments.\nSensitivity (or ‚Äúdiscriminative power‚Äù) measures the ability of an\nevaluation method to recognize a significant difference between\nretrieval approaches [19, 33, 67, 86]. To compute sensitivity, we take\nall pairs of submitted runs and compute a paired t-test between\nthem. Here, we consider a pair with ùëù < 0.05 as distinguished [86]\nand define sensitivity as# of distinguished pairs\ntotal pairs . Since we do not correct\nfor the multiple comparisons problem, some of the distinguished\npairs may not represent actual significant differences. With human\njudgments, 72% of the pairs are distinguished under MAP (74% un-\nder NDCG@10). In contrast, with GPT-3.5 judgments, only 65% are\ndistinguished under MAP (69% under NDCG@10).\n9Available at: https://plg.uwaterloo.ca/~claclark/trec2021_DL_prompt.txt\nICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan Guglielmo Faggioli et al.\n7 PERSPECTIVES FOR THE FUTURE\nAs this is a perspectives paper, we provide two opposing perspec-\ntives on the use of LLMs for automatic relevance judgments‚Äîfor\nand against‚Äîand a third compromise perspective.\n7.1 In Favor of Using LLMs for Judgments\nIn addition to providing a judgment of relevance, LLMs are able to\nproduce a natural language explanation why a certain document is\nrelevant or not to a topic [36]. Such AI-generated explanations may\nbe used to assist human assessors in relevance judgments, particu-\nlarly non-experts like crowdworkers. This setup may lead to better\nquality judgments as compared to the unsupported crowd. While\nLLM-generated labels and explanations may lead to an overreliance\nof human assessors , human assessors may serve as a quality con-\ntrol mechanism for the LLM. Furthermore, they serve as a feedback\nloop for the LLM to continuously improve its judgments. Our pilot\nexperiments demonstrate that it is feasible for LLMs to indicate\nwhen a document is likely not relevant. We might therefore let\nhuman annotators assess (a) first those documents that are deemed\nrelevant by LLMs, or (b) a subsample of documents from those con-\nsidered relevant by the LLM, as an LLM can be run at scale. Thereby,\nwe envision the use of LLMs to reduce annotation cost/time when\ncreating high-quality IR evaluation collections.\nIt is noteworthy that LLMs may be better at providing fair and\nconsistent judgments than humans. They can judge the relevance\nof documents without being affected by documents they have seen\nbefore, and with no boredom or tiredness effects. They are likely to\nassess conceptually similar documents the same way. Furthermore,\nthey will often have seen much more information on a specific topic\nthan most humans. Another advantage of today‚Äôs LLMs is their\ninherent ability to process and generate text in many different lan-\nguages. For multilingual corpora (which often appear in industrial\nsettings) the assessment is typically restricted to a small subset of\nlanguages due to the limited availability of assessors. With LLMs\nbeing part of the assessment tool, this limitation no longer applies.\nLLMs are not just restricted to one input modality and thus con-\nducting assessments that require the simultaneous consideration\nof multiple pieces of content (e.g. judging a web page based on the\ntext but also the document‚Äôs structure, visual cues, embedded video\nmaterial, etc.) at the same time becomes possible. Finally, we note\nthe cost factor‚Äîif we are able to judge hundreds of thousands of\ndocuments for a relatively small price, we can build much larger\nand much more complex test collections with regularly updated\nrelevance assessments, in particular in domains that today lack\nmeaningful test collections.\nIn summary, LLMs can provide explanations, scalability, consis-\ntency, and a certain level of quality when performing relevance\njudgments, underlining the great potential of deploying them as a\ncomplement to human assessors in certain judgments task.\n7.2 Against Using LLMs for Judgments\nWhile we have given several reasons to believe that we are close to\nusing LLMs for automatic relevance judgment, there are also sev-\neral concerns that should be addressed by the research community\nbefore deploying full-fledged automatic judgment. The primary\nconcern is that LLMs are not people. IR measures of effectiveness\nare ultimately grounded in a human user‚Äôs relevance judgment.\nRelevance is subjective, and changes over time for the same per-\nson [64]. Even if LLMs are increasingly good at mimicking human\nlanguage in evaluating contents, it is a big leap of faith to fully\ntrust the model‚Äôs ability to make correct assessments without hu-\nman verification. Currently, there is no proof that the judgments\nmade by LLMs are grounded in reality. This raises an essential\nquestion: If the output from an LLM is indistinguishable from a\nhuman-made relevance judgment, is this just a distinction without\na difference? After all, people disagree on relevance and change\ntheir opinions over time due to implicit and explicit learning ef-\nfects. Usually, however, those disagreements do not have an effect\non the evaluation unless there are systematic causes [ 8, 80]. To\nsafely adopt LLMs to replace human annotators, the community\nshould examine whether LLM-based relevance judgments may in\nfact be systematically different from those of real users. Not only do\nwe know this affects the evaluation, but the complexity (or black-\nbox nature) of the model precludes defining systematic bias in any\nuseful way. There is a general concern about solely evaluating IR\nresearch with relevance assessment: Information retrieval systems\nare not just result-ranking machines, but are a system that is to\nassist a human to obtain information. Hence, only the user who con-\nsumes the results could tell which ones are useful. Another concern\nof applying LLMs as relevance annotators regards the ‚Äúcircular-\nity‚Äù of the evaluation. Assume we are able to devise an annotation\nmodel based on LLMs. The same model could ideally also be used\nto retrieve and rank documents based on their expected relevance.\nIf the model is used to judge relevance both for annotation and\nfor retrieval, its evaluation would be overinflated, possibly with\nperfect performance. Vice-versa, models based on widely different\nrationales (such as BM25 or classical lexical approaches), might\nbe penalized, because of how they estimate document relevance.\nAs counter-considerations, we might hypothesize that the model\nused to label documents for relevance (a) is highly computation-\nally expensive, making it almost unfeasible to use it as a retrieval\nsystem, and/or (b) has access to more information and facts than\nthe retrieval model. The former holds as long as we do not use the\nautomatic annotator as an expensive re-ranker capable of dealing\nwith just a few documents. The latter, on the other hand, does not\nsolve the problem of the automatic annotation, but simply shifts\nthe problem: Either, the additional facts and information need to be\nannotated manually; then the human annotator remains essential.\nOr, the facts can be collected automatically; then we may assume\nthat also a retrieval system could obtain them.\nOther concerns arise if we even consider generative models as\na replacement for traditional IR and search. In a plain old search\nengine, results for a query are ranked according to predicted rel-\nevance (ignoring sponsored results and advertising here). Each\nhas a clear source, and each can be inspected directly as an en-\ntity separate from the search engine. Moreover, users frequently\nreformulate queries and try suggestions from the search engine,\nin a virtuous cycle wherein the users fulfill or adjust their concep-\ntual information needs. Currently, hardly any of these is possible\nusing LLM-generated responses: The results often are not attrib-\nuted, rarely can be explored or probed, and are often completely\ngenerated. Also, best approaches for prompt engineering are not\nsufficiently studied, and their effect is more opaque than approaches\nPerspectives on Large Language Models for Relevance Judgment ICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan\nto query reformulation. LLMs will not be usable for many infor-\nmation needs until they can attribute sources reliably and can be\ninterrogated systematically. Will become available soon.\nFinally, there are significant socio-technical concerns. Generative\nAI models can be used to generate fake photos and videos, for extor-\ntion purposes and misinformation. They are perceived as stealing\nthe intellectual property. Furthermore, LLMs are affected by bias,\nstereotypical associations [9, 60], and adverse sentiments towards\nspecific groups [52]. Critically, we cannot assess whether the LLM\nmay have seen information that biases the relevance judgment in\nan unwanted way, let alone that the company owning the LLM may\nchange it anytime without our knowledge or control. As a result,\nwe ourselves as the authors of this perspectives paper disagree on\nwhether, as a profession and considering the ACM‚Äôs Code of Ethics,\nwe should use generative models in deployed systems at all until\nthese issues are worked out.\n7.3 A Compromise: Double-checking LLMs and\nHuman‚ÄìMachine Collaboration\nOur pilot study in Sections 5 and 6 finds a reasonable correlation\nbetween highly-trained human assessors and a fully automated\nLLM, yielding similar leaderboards. This suggests that the tech-\nnology is promising and deserves further study. The experiment\ncould be implemented to double-check LLM judgments: produce\nfully automated as well as human judgments on a shared judgment\npool, then analyze correlations of labels and system rankings, then\ndecide whether LLM‚Äôs relevance judgments are good enough to be\nshared as an alternative test collection with the community. The au-\ntomatic judgment paradigm should be revealed along with prompts,\nhyperparameters, and details for reproducibility. We also suggest to\ndeclare which judgment paradigm was chosen when releasing data\nresources (such as in TREC CAR). At the very least, such automatic\njudgments could be used to evaluate early prototypes of approaches,\nfor initial judgments for novel tasks, and for large-scale training.\nWhile the discussion is easily dominated by the fully automated\nevaluation‚Äîthis is merely an extreme point on our spectrum in\nSection 3. The majority of authors do not believe this constitutes the\nbest path towards credible IR research. For example, ‚ÄúAI Assistance‚Äù\nis probably the most credible path for LLMs to be incorporated\nduring evaluation. However, it is also the least explored so far.\nThis calls for more research on innovative ways to use LLMs\nfor assistance during the judgment process and how to leverage\nhumans for verifying the LLMs‚Äô suggestions. As a community, we\nshould explore how the performance of human assessors changes,\nwhen they are shown rationales or chain-of-thoughts that are gen-\nerated by LLMs. Human assessors often struggle to see a pertinent\nconnection when they are lacking world knowledge. An example\nof this issue is the task of assessing the relevance of ‚Äúdiabetes‚Äù for\nthe topic ‚Äúchild trafficking‚Äù. LLMs can generate rationales that can\nexplain such connections. However, it requires a human to realize\nwhen such a rationale was hallucinated. Only a human can assess\nwhether the information provided appears true and reliable.\n8 CONCLUSION\nIn this paper, we investigated the opportunity that large language\nmodels (LLMs) now may generate relevance judgments automat-\nically. We discussed previous attempts to automate and scale-up\nthe relevance judgment task, and we presented experimental re-\nsults showing promise in the ability to mimic human relevance\nassessments with LLMs. Our findings suggest that, while the path\nis promising and worthy of being investigated, at the time of writ-\ning several reasons prevent LLMs from being employed as fully\nautomated annotation tools. Nevertheless, there is a spectrum of\nsolutions to employ LLMs as support for human assessors in a\nhuman‚Äìmachine collaboration. Therefore, we present our perspec-\ntives on why and why not the IR community should employ LLMs\nin some way in the evaluation process. Undoubtedly, more research\non LLMs for relevance judgment is to be carried out in the future,\nfor which this paper provides a starting point.\nACKNOWLEDGMENTS\nThis paper is based on discussions during a breakout group at\nthe Dagstuhl Seminar 23031 on ‚ÄúFrontiers of Information Access\nExperimentation for Research and Education‚Äù [10]. We express our\ngratitude to the Seminar organizers, Christine Bauer, Ben Carterette,\nNicola Ferro, and Norbert Fuhr.\nCertain companies and software are identified in this paper in\norder to specify the experimental procedure adequately. Such iden-\ntification is not intended to imply recommendation or endorsement\nof any product or service, nor is it intended to imply that the soft-\nware or companies identified are necessarily the best available for\nthe purpose.\nThis material is based upon work supported by the National Sci-\nence Foundation under Grant No. 1846017. Any opinions, findings,\nand conclusions or recommendations expressed in this material are\nthose of the author(s) and do not necessarily reflect the views of\nthe National Science Foundation.\nREFERENCES\n[1] Mustafa Abualsaud, Nimesh Ghelani, Haotian Zhang, Mark D. Smucker, Gordon V.\nCormack, and Maura R. Grossman. 2018. A System for Efficient High-Recall\nRetrieval. In The 41st International ACM SIGIR Conference on Research & Develop-\nment in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 .\nACM, 1317‚Äì1320. https://doi.org/10.1145/3209978.3210176\n[2] Omar Alonso and Ricardo Baeza-Yates. 2011. Design and Implementation of Rel-\nevance Assessments Using Crowdsourcing. In Advances in Information Retrieval -\n33rd European Conference on IR Research, ECIR 2011, Dublin, Ireland, April 18-21,\n2011. Proceedings (Lecture Notes in Computer Science, Vol. 6611) . Springer, 153‚Äì164.\nhttps://doi.org/10.1007/978-3-642-20161-5_16\n[3] Omar Alonso and Stefano Mizzaro. 2009. Can we get rid of TREC assessors?\nUsing Mechanical Turk for Relevance Assessment. In Proceedings of the SIGIR\n2009 Workshop on the Future of IR Evaluation , Vol. 15. 16.\n[4] Negar Arabzadeh, Maryam Khodabakhsh, and Ebrahim Bagheri. 2021. BERT-QPP:\nContextualized Pre-trained Transformers for Query Performance Prediction. In\nCIKM ‚Äô21: The 30th ACM International Conference on Information and Knowledge\nManagement, Virtual Event, Queensland, Australia, November 1 - 5, 2021 . ACM,\n2857‚Äì2861. https://doi.org/10.1145/3459637.3482063\n[5] Negar Arabzadeh, Mahsa Seifikar, and Charles L. A. Clarke. 2022. Unsupervised\nQuestion Clarity Prediction through Retrieved Item Coherency. In Proceedings of\nthe 31st ACM International Conference on Information & Knowledge Management,\nAtlanta, GA, USA, October 17-21, 2022 . ACM, 3811‚Äì3816. https://doi.org/10.1145/\n3511808.3557719\n[6] Sebastian Arnold, Rudolf Schneider, Philippe Cudr√©-Mauroux, Felix A. Gers, and\nAlexander L√∂ser. 2019. SECTOR: A Neural Model for Coherent Topic Segmen-\ntation and Classification. Trans. Assoc. Comput. Linguistics 7 (2019), 169‚Äì184.\nhttps://doi.org/10.1162/tacl_a_00261\nICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan Guglielmo Faggioli et al.\n[7] Nima Asadi, Donald Metzler, Tamer Elsayed, and Jimmy Lin. 2011. Pseudo\nTest Collections for Learning Web Search Ranking Functions. InProceeding of\nthe 34th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR 2011, Beijing, China, July 25-29, 2011 . ACM, 1073‚Äì1082.\nhttps://doi.org/10.1145/2009916.2010058\n[8] Peter Bailey, Nick Craswell, Ian Soboroff, Paul Thomas, Arjen P. de Vries, and\nEmine Yilmaz. 2008. Relevance Assessment: Are Judges Exchangeable and Does\nit Matter. In Proceedings of the 31st Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval, SIGIR 2008, Singapore, July\n20-24, 2008 . ACM, 667‚Äì674. https://doi.org/10.1145/1390334.1390447\n[9] Christine Basta, Marta Ruiz Costa-juss√†, and Noe Casas. 2019. Evaluating the Un-\nderlying Gender Bias in Contextualized Word Embeddings. CoRR abs/1904.08783\n(2019). arXiv:1904.08783 http://arxiv.org/abs/1904.08783\n[10] Christine Bauer, Ben Carterette, Nicola Ferro, and Norbert Fuhr. 2023. Report\nfrom Dagstuhl Seminar 23031: Frontiers of Information Access Experimentation\nfor Research and Education. CoRR abs/2305.01509 (2023). https://doi.org/10.485\n50/arXiv.2305.01509 arXiv:2305.01509\n[11] Steven M. Beitzel, Eric C. Jensen, Abdur Chowdhury, and David A. Grossman.\n2003. Using Titles and Category Names from Editor-Driven Taxonomies for\nAutomatic Evaluation. In Proceedings of the 2003 ACM CIKM International Con-\nference on Information and Knowledge Management, New Orleans, Louisiana, USA,\nNovember 2-8, 2003 . ACM, 17‚Äì23. https://doi.org/10.1145/956863.956868\n[12] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models\nBe Too Big?. In FAccT ‚Äô21: 2021 ACM Conference on Fairness, Accountability, and\nTransparency, Virtual Event / Toronto, Canada, March 3-10, 2021 . ACM, 610‚Äì623.\nhttps://doi.org/10.1145/3442188.3445922\n[13] Richard Berendsen, Manos Tsagkias, Maarten de Rijke, and Edgar Meij. 2012.\nGenerating Pseudo Test Collections for Learning to Rank Scientific Articles. In\nInformation Access Evaluation. Multilinguality, Multimodality, and Visual Analytics\n- Third International Conference of the CLEF Initiative, CLEF 2012, Rome, Italy,\nSeptember 17-20, 2012. Proceedings (Lecture Notes in Computer Science, Vol. 7488) .\nSpringer, 42‚Äì53. https://doi.org/10.1007/978-3-642-33247-0_6\n[14] Roi Blanco, Harry Halpin, Daniel M. Herzig, Peter Mika, Jeffrey Pound, Henry S.\nThompson, and Duc Thanh Tran. 2011. Repeatable and Reliable Search System\nEvaluation Using Crowdsourcing. In Proceeding of the 34th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, SIGIR\n2011, Beijing, China, July 25-29, 2011 . ACM, 923‚Äì932. https://doi.org/10.1145/20\n09916.2010039\n[15] Martin Braschler. 2000. CLEF 2000 - Overview of Results. In Cross-Language\nInformation Retrieval and Evaluation, Workshop of Cross-Language Evaluation\nForum, CLEF 2000, Lisbon, Portugal, September 21-22, 2000, Revised Papers (Lecture\nNotes in Computer Science, Vol. 2069) . Springer, 89‚Äì101. https://doi.org/10.1007/3-\n540-44645-1_9\n[16] David Carmel and Elad Yom-Tov. 2010. Estimating the Query Difficulty for Infor-\nmation Retrieval. Morgan & Claypool Publishers. https://doi.org/10.2200/S00235\nED1V01Y201004ICR015\n[17] Ben Carterette, James Allan, and Ramesh K. Sitaraman. 2006. Minimal Test\nCollections for Retrieval Evaluation. In SIGIR 2006: Proceedings of the 29th Annual\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, Seattle, Washington, USA, August 6-11, 2006 . ACM, 268‚Äì275. https:\n//doi.org/10.1145/1148170.1148219\n[18] Xiaoyang Chen, Ben He, and Le Sun. 2022. Groupwise Query Performance\nPrediction with BERT. In Advances in Information Retrieval - 44th European\nConference on IR Research, ECIR 2022, Stavanger, Norway, April 10-14, 2022, Pro-\nceedings, Part II (Lecture Notes in Computer Science, Vol. 13186) . Springer, 64‚Äì74.\nhttps://doi.org/10.1007/978-3-030-99739-7_8\n[19] Charles L. A. Clarke, Alexandra Vtyurina, and Mark D. Smucker. 2020. Assessing\nTop-k Preferences. CoRR abs/2007.11682 (2020). arXiv:2007.11682 https://arxiv.\norg/abs/2007.11682\n[20] Cyril W Cleverdon. 1960. The Aslib Cranfield Research Project on the Compara-\ntive Efficiency of Indexing Systems. In Aslib Proceedings , Vol. 12. MCB UP Ltd,\n421‚Äì431.\n[21] Gordon V. Cormack, Christopher R. Palmer, and Charles L. A. Clarke. 1998.\nEfficient Construction of Large Test Collections. In SIGIR ‚Äô98: Proceedings of the\n21st Annual International ACM SIGIR Conference on Research and Development\nin Information Retrieval, August 24-28 1998, Melbourne, Australia . ACM, 282‚Äì289.\nhttps://doi.org/10.1145/290941.291009\n[22] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview\nof the TREC 2020 Deep Learning Track. CoRR abs/2102.07662. arXiv:2102.07662\nhttps://arxiv.org/abs/2102.07662\n[23] Antonia Creswell and Murray Shanahan. 2022. Faithful Reasoning Using Large\nLanguage Models. CoRR abs/2208.14271 (2022). https://doi.org/10.48550/arXiv.2\n208.14271 arXiv:2208.14271\n[24] Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2020. TREC CAsT 2019:\nThe Conversational Assistance Track Overview. CoRR abs/2003.13624 (2020).\narXiv:2003.13624 https://arxiv.org/abs/2003.13624\n[25] Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2020. TREC CAsT\n2019: The Conversational Assistance Track Overview. CoRR abs/2003.13624.\narXiv:2003.13624 https://arxiv.org/abs/2003.13624\n[26] Bhavana Bharat Dalvi, Einat Minkov, Partha Pratim Talukdar, and William W.\nCohen. 2015. Automatic Gloss Finding for a Knowledge Base using Ontological\nConstraints. In Proceedings of the Eighth ACM International Conference on Web\nSearch and Data Mining, WSDM 2015, Shanghai, China, February 2-6, 2015 . ACM,\n369‚Äì378. https://doi.org/10.1145/2684822.2685288\n[27] Florian Daniel, Pavel Kucherbaev, Cinzia Cappiello, Boualem Benatallah, and\nMohammad Allahbakhsh. 2018. Quality Control in Crowdsourcing: A Survey\nof Quality Attributes, Assessment Techniques, and Assurance Actions. ACM\nComput. Surv. 51, 1 (2018), 7:1‚Äì7:40. https://doi.org/10.1145/3148148\n[28] Suchana Datta, Sean MacAvaney, Debasis Ganguly, and Derek Greene. 2022.\nA ‚ÄôPointwise-Query, Listwise-Document‚Äô Based Query Performance Prediction\nApproach. In Proceedings of 45th international ACM SIGIR conference research\ndevelopment in information retrieval . 2148‚Äî-2153. https://doi.org/10.1145/347749\n5.3531821\n[29] Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. 2021. Towards Question-\nAnswering as an Automatic Metric for Evaluating the Content Quality of a\nSummary. Transactions of the Association for Computational Linguistics 9 (2021),\n774‚Äì789. https://doi.org/10.1162/tacl_a_00397\n[30] Laura Dietz, Shubham Chatterjee, Connor Lennox, Sumanta Kashyapi, Pooja\nOza, and Ben Gamari. 2022. Wikimarks: Harvesting Relevance Benchmarks from\nWikipedia. In SIGIR ‚Äô22: The 45th International ACM SIGIR Conference on Research\nand Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM,\n3003‚Äì3012. https://doi.org/10.1145/3477495.3531731\n[31] Laura Dietz and Jeff Dalton. 2020. Humans Optional? Automatic Large-Scale\nTest Collections for Entity, Passage, and Entity-Passage Retrieval. Datenbank-\nSpektrum 20, 1 (2020), 17‚Äì28. https://doi.org/10.1007/s13222-020-00334-y\n[32] Matan Eyal, Tal Baumel, and Michael Elhadad. 2019. Question Answering as an\nAutomatic Evaluation Metric for News Article Summarization. In Proceedings of\nthe 2019 Conference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill Burstein, Christy\nDoran, and Thamar Solorio (Eds.). Association for Computational Linguistics,\n3938‚Äì3948. https://doi.org/10.18653/v1/n19-1395\n[33] Guglielmo Faggioli and Nicola Ferro. 2021. System Effect Estimation by Sharding:\nA Comparison Between ANOVA Approaches to Detect Significant Differences. In\nAdvances in Information Retrieval - 43rd European Conference on IR Research, ECIR\n2021, Virtual Event, March 28 - April 1, 2021, Proceedings, Part II (Lecture Notes in\nComputer Science, Vol. 12657) . Springer, 33‚Äì46. https://doi.org/10.1007/978-3-\n030-72240-1_3\n[34] Guglielmo Faggioli, Nicola Ferro, Cristina Muntean, Raffaele Perego, and Nicola\nTonellotto. 2023. A Geometric Framework for Query Performance Prediction in\nConversational Search. In Proceedings of 46th international ACM SIGIR Conference\non Research & Development in Information Retrieval, SIGIR 2023 July 23‚Äì27, 2023,\nTaipei, Taiwan. ACM. https://doi.org/10.1145/3539618.3591625\n[35] Marco Ferrante, Nicola Ferro, and Maria Maistro. 2017. AWARE: Exploiting\nEvaluation Measures to Combine Multiple Assessors. ACM Transactions on\nInformation Systems 36, 2 (2017), 20:1‚Äì20:38. https://doi.org/10.1145/3110217\n[36] Fernando Ferraretto, Thiago Laitz, Roberto de Alencar Lotufo, and Rodrigo\nNogueira. 2023. ExaRanker: Explanation-Augmented Neural Ranker. https:\n//doi.org/10.48550/arXiv.2301.10521 arXiv:2301.10521\n[37] Frank Flemisch, David Abbink, Makoto Itoh, Marie-Pierre Pacaux-Lemoine, and\nGina We√üel. 2016. Shared Control is the Sharp End of Cooperation: Towards\na Common Framework of Joint Action, Shared Control and Human Machine\nCooperation. IFAC-PapersOnLine 49, 19 (2016), 72‚Äì77. https://doi.org/10.1016/j.\nifacol.2016.10.464 13th IFAC Symposium on Analysis, Design, and Evaluation\nofHuman-Machine Systems HMS 2016.\n[38] Maik Fr√∂be, Christopher Akiki, Martin Potthast, and Matthias Hagen. 2022. Noise-\nReduction for Automatically Transferred Relevance Judgments. In Experimen-\ntal IR Meets Multilinguality, Multimodality, and Interaction - 13th International\nConference of the CLEF Association, CLEF 2022, Bologna, Italy, September 5-8,\n2022, Proceedings (Lecture Notes in Computer Science, Vol. 13390) . Springer, 48‚Äì61.\nhttps://doi.org/10.1007/978-3-031-13643-6_4\n[39] Maik Fr√∂be, Lukas Gienapp, Martin Potthast, and Matthias Hagen. 2023. Boot-\nstrapped nDCG Estimation in the Presence of Unjudged Documents. In Advances\nin Information Retrieval - 45th European Conference on Information Retrieval, ECIR\n2023, Dublin, Ireland, April 2-6, 2023, Proceedings, Part I (Lecture Notes in Computer\nScience, Vol. 13980) . Springer, 313‚Äì329. https://doi.org/10.1007/978-3-031-28244-\n7_20\n[40] Norbert Fuhr. 2017. Some Common Mistakes In IR Evaluation, And How They Can\nBe Avoided. SIGIR Forum 51, 3, 32‚Äì41. https://doi.org/10.1145/3190580.3190586\n[41] Ujwal Gadiraju, Gianluca Demartini, Ricardo Kawase, and Stefan Dietze. 2019.\nCrowd Anatomy Beyond the Good and Bad: Behavioral Traces for Crowd Worker\nModeling and Pre-selection. Computer Supported Cooperative Work 28, 5 (2019),\n815‚Äì841. https://doi.org/10.1007/s10606-018-9336-y\nPerspectives on Large Language Models for Relevance Judgment ICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan\n[42] Debasis Ganguly, Surupendu Gangopadhyay, Mandar Mitra, and Prasenjit Ma-\njumder (Eds.). 2022. FIRE ‚Äô22: Proceedings of the 14th Annual Meeting of the Forum\nfor Information Retrieval Evaluation (Kolkata, India). Association for Computing\nMachinery, New York, NY, USA.\n[43] Fabrizio Gilardi, Meysam Alizadeh, and Ma√´l Kubli. 2023. ChatGPT Outperforms\nCrowd-Workers for Text-Annotation Tasks.CoRR abs/2303.15056 (2023). https:\n//doi.org/10.48550/arXiv.2303.15056 arXiv:2303.15056\n[44] Yeow Chong Goh, Xin Qing Cai, Walter Theseira, Giovanni Ko, and Khiam Aik\nKhor. 2020. Evaluating Human Versus Machine Learning Performance in Clas-\nsifying Research Abstracts. Scientometrics 125, 2 (2020), 1197‚Äì1212. https:\n//doi.org/10.1007/s11192-020-03614-2\n[45] Martin Halvey, Robert Villa, and Paul D. Clough. 2015. SIGIR 2014: Workshop on\nGathering Efficient Assessments of Relevance (GEAR). SIGIR Forum 49, 1 (2015),\n16‚Äì19. https://doi.org/10.1145/2795403.2795409\n[46] PA Hancock. 2013. Task partitioning effects in semi-automated human‚Äìmachine\nsystem performance. Ergonomics 56, 9 (2013), 1387‚Äì1399. https://doi.org/10.108\n0/00140139.2013.816374\n[47] Donna Harman. 1992. Overview of the First Text REtrieval Conference (TREC-\n1). NIST Special Publication, Vol. 500-207. National Institute of Standards and\nTechnology (NIST). 1‚Äì20 pages. http://trec.nist.gov/pubs/trec1/papers/01.txt\n[48] Claudia Hauff. 2010. Predicting the effectiveness of queries and retrieval systems.\nSIGIR Forum 44, 1 (2010), 88. https://doi.org/10.1145/1842890.1842906\n[49] Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan,\nand Graham Neubig. 2021. WikiAsp: A Dataset for Multi-domain Aspect-based\nSummarization. Transactions of the Association for Computational Linguistics 9\n(2021), 211‚Äì225. https://doi.org/10.1162/tacl_a_00362\n[50] Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fan-\ndrianto, Jay Han, Matthew Kelcey, and David Berthelot. 2016. WikiReading:\nA Novel Large-scale Language Understanding Task over Wikipedia. (2016).\nhttps://doi.org/10.18653/v1/p16-1145\n[51] Luyang Huang, Lingfei Wu, and Lu Wang. 2020. Knowledge Graph-Augmented\nAbstractive Summarization with Semantic-Driven Cloze Reward. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020 . Association for Computational Linguistics, 5094‚Äì\n5107. https://doi.org/10.18653/v1/2020.acl-main.457\n[52] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu\nZhong, and Stephen Denuyl. 2020. Social Biases in NLP Models as Barriers for\nPersons with Disabilities. In Proceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 . Association\nfor Computational Linguistics, 5491‚Äì5501. https://doi.org/10.18653/v1/2020.acl-\nmain.487\n[53] Aaron Jaech and Mari Ostendorf. 2018. Personalized Language Model for Query\nAuto-Completion. In Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018,\nVolume 2: Short Papers . Association for Computational Linguistics, 700‚Äì705. https:\n//doi.org/10.18653/v1/P18-2111\n[54] Kalervo J√§rvelin. 2009. Explaining User Performance in Information Retrieval:\nChallenges to IR Evaluation. In Advances in Information Retrieval Theory, Second\nInternational Conference on the Theory of Information Retrieval, ICTIR 2009, Cam-\nbridge, UK, September 10-12, 2009, Proceedings (Lecture Notes in Computer Science,\nVol. 5766). Springer, 289‚Äì296. https://doi.org/10.1007/978-3-642-04417-5_28\n[55] Gaya K. Jayasinghe, William Webber, Mark Sanderson, and J. Shane Culpepper.\n2014. Improving Test Collection Pools with Machine Learning. In Proceedings of\nthe 2014 Australasian Document Computing Symposium, ADCS 2014, Melbourne,\nVIC, Australia, November 27-28, 2014 . ACM, 2. https://doi.org/10.1145/2682862.\n2682864\n[56] Noriko Kando (Ed.). 1999. Proceedings of the First NTCIR Workshop on Research\nin Japanese Text Retrieval and Term Recognition . National Center for Science\nInformation Systems (NACSIS). http://research.nii.ac.jp/ntcir/workshop/Online\nProceedings/\n[57] Gjergji Kasneci, Maya Ramanath, Fabian M. Suchanek, and Gerhard Weikum.\n2008. The YAGO-NAGA Approach to Knowledge Discovery. ACM SIGMOD\nRecord 37, 4 (2008), 41‚Äì47. https://doi.org/10.1145/1519103.1519110\n[58] Gabriella Kazai, Jaap Kamps, and Natasa Milic-Frayling. 2013. An Analysis of\nHuman Factors and Label Accuracy in Crowdsourcing Relevance Judgments.\nInformation Retrieval 16, 2 (2013), 138‚Äì178. https://doi.org/10.1007/s10791-012-\n9205-0\n[59] Mostafa Keikha, Jae Hyun Park, and W. Bruce Croft. 2014. Evaluating Answer\nPassages Using Summarization Measures. In The 37th International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ‚Äô14, Gold\nCoast , QLD, Australia - July 06 - 11, 2014 . ACM, 963‚Äì966. https://doi.org/10.114\n5/2600428.2609485\n[60] Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W. Black, and Yulia Tsvetkov. 2019.\nMeasuring Bias in Contextualized Word Representations. CoRR abs/1906.07337\n(2019). arXiv:1906.07337 http://arxiv.org/abs/1906.07337\n[61] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Mari-\nanna Apidianaki, and Chris Callison-Burch. 2023. Faithful Chain-of-Thought Rea-\nsoning. CoRR abs/2301.13379 (2023). https://doi.org/10.48550/arXiv.2301.13379\narXiv:2301.13379\n[62] Sean MacAvaney and Luca Soldaini. 2023. One-Shot Labeling for Automatic\nRelevance Estimation. CoRR abs/2302.11266 (2023). https://doi.org/10.48550/arX\niv.2302.11266 arXiv:2302.11266\n[63] Eddy Maddalena, Marco Basaldella, Dario De Nart, Dante Degl‚ÄôInnocenti, Stefano\nMizzaro, and Gianluca Demartini. 2016. Crowdsourcing Relevance Assessments:\nThe Unexpected Benefits of Limiting the Time to Judge. In Proceedings of the\nFourth AAAI Conference on Human Computation and Crowdsourcing, HCOMP\n2016, 30 October - 3 November, 2016, Austin, Texas, USA . AAAI Press, 129‚Äì138.\nhttp://aaai.org/ocs/index.php/HCOMP/HCOMP16/paper/view/14040\n[64] Stefano Mizzaro. 1997. Relevance: The Whole History. Journal of the American\nsociety for information science 48, 9 (1997), 810‚Äì832. https://doi.org/10.1002/(SI\nCI)1097-4571(199709)48:9%3C810::AID-ASI6%3E3.0.CO;2-U\n[65] Zahra Nouri, Henning Wachsmuth, and Gregor Engels. 2020. Mining Crowd-\nsourcing Problems from Discussion Forums of Workers. In Proceedings of the\n28th International Conference on Computational Linguistics . International Com-\nmittee on Computational Linguistics, Barcelona, Spain (Online), 6264‚Äì6276.\nhttps://doi.org/10.18653/v1/2020.coling-main.551\n[66] Virgiliu Pavlu, Shahzad Rajput, Peter B. Golbus, and Javed A. Aslam. 2012. IR\nSystem Evaluation Using Nugget-Based Test Collections. In Proceedings of the\nFifth International Conference on Web Search and Web Data Mining, WSDM 2012,\nSeattle, WA, USA, February 8-12, 2012 . ACM, 393‚Äì402. https://doi.org/10.1145/21\n24295.2124343\n[67] Tetsuya Sakai. 2006. Evaluating Evaluation Metrics Based on the Bootstrap. In\nSIGIR 2006: Proceedings of the 29th Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval, Seattle, Washington, USA,\nAugust 6-11, 2006 . ACM, 525‚Äì532. https://doi.org/10.1145/1148170.1148261\n[68] Tetsuya Sakai. 2020. On Fuhr‚Äôs Guideline for IR Evaluation. SIGIR Forum 54, 1,\n12:1‚Äì12:8. https://doi.org/10.1145/3451964.3451976\n[69] David P. Sander and Laura Dietz. 2021. EXAM: How to Evaluate Retrieve-\nand-Generate Systems for Users Who Do Not (Yet) Know What They Want.\nIn Proceedings of the Second International Conference on Design of Experimental\nSearch & Information REtrieval Systems, Padova, Italy, September 15-18, 2021 (CEUR\nWorkshop Proceedings, Vol. 2950). CEUR-WS.org, 136‚Äì146. http://ceur-ws.org/Vol-\n2950/paper-16.pdf\n[70] Tefko Saracevic. 1995. Evaluation of Evaluation in Information Retrieval. In\nSIGIR‚Äô95, Proceedings of the 18th Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval. Seattle, Washington, USA,\nJuly 9-13, 1995 (Special Issue of the SIGIR Forum) . ACM Press, 138‚Äì146. https:\n//doi.org/10.1145/215206.215351\n[71] Tefko Saracevic. 1996. Relevance Reconsidered. In Proceedings of the second\nconference on conceptions of library and information science (CoLIS 2) . 201‚Äì218.\n[72] Linda Schamber. 1994. Relevance and Information Behavior. Annual review of\ninformation science and technology (ARIST) 29 (1994), 3‚Äì48.\n[73] Seungmin Seo, Donghyun Kim, Youbin Ahn, and Kyong-Ho Lee. 2022. Active\nLearning on Pre-trained Language Model with Task-Independent Triplet Loss. In\nThirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI\n2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2022 Virtual Event, February 22 - March 1, 2022 . AAAI Press, 11276‚Äì11284.\nhttps://ojs.aaai.org/index.php/AAAI/article/view/21378\n[74] Akanksha Rai Sharma and Pranav Kaushik. 2017. Literature Survey of Statistical,\nDeep and Reinforcement Learning in Natural Language Processing. In 2017\nInternational Conference on Computing, Communication and Automation (ICCCA) .\n350‚Äì354. https://doi.org/10.1109/CCAA.2017.8229841\n[75] Aashish Sheshadri and Matthew Lease. 2013. SQUARE: A Benchmark for Research\non Computing Crowd Consensus. In Proceedings of the First AAAI Conference on\nHuman Computation and Crowdsourcing, HCOMP 2013, November 7-9, 2013, Palm\nSprings, CA, USA . AAAI. http://www.aaai.org/ocs/index.php/HCOMP/HCOMP\n13/paper/view/7550\n[76] Ian Soboroff. 2021. Overview of TREC 2021. In 30th Text REtrieval Conference .\nGaithersburg, Maryland. https://trec.nist.gov/pubs/trec30/papers/Overview-\n2021.pdf\n[77] Ian Soboroff, Charles K. Nicholas, and Patrick Cahan. 2001. Ranking Retrieval\nSystems without Relevance Judgments. In SIGIR 2001: Proceedings of the 24th\nAnnual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, September 9-13, 2001, New Orleans, Louisiana, USA . ACM,\n66‚Äì73. https://doi.org/10.1145/383952.383961\n[78] Taylor Sorensen, Joshua Robinson, Christopher Michael Rytting, Alexander Glenn\nShaw, Kyle Jeffrey Rogers, Alexia Pauline Delorey, Mahmoud Khalil, Nancy Fulda,\nand David Wingate. 2022. An Information-theoretic Approach to Prompt Engi-\nneering Without Ground Truth Labels. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022 . Association for Computational Linguistics,\n819‚Äì862. https://doi.org/10.18653/v1/2022.acl-long.60\n[79] Lynda Tamine and Cecile Chouquet. 2017. On the Impact of Domain Expertise\non Query Formulation, Relevance Assessment and Retrieval Performance in\nClinical Settings. Information Processing & Management 53, 2 (2017), 332‚Äì350.\nhttps://doi.org/10.1016/j.ipm.2016.11.004\nICTIR ‚Äô23, July 23, 2023, Taipei, Taiwan Guglielmo Faggioli et al.\n[80] Ellen M. Voorhees. 2000. Variations in Relevance Judgments and the Measurement\nof Retrieval Effectiveness. Inf. Process. Manag. 36, 5 (2000), 697‚Äì716. https:\n//doi.org/10.1016/S0306-4573(00)00010-8\n[81] Ellen M. Voorhees and Donna Harman. 1999. Overview of the Eighth Text\nREtrieval Conference (TREC-8). In Proceedings of The Eighth Text REtrieval Con-\nference, TREC 1999, Gaithersburg, Maryland, USA, November 17-19, 1999 (NIST\nSpecial Publication, Vol. 500-246) . National Institute of Standards and Technology\n(NIST). http://trec.nist.gov/pubs/trec8/papers/overview_8.ps\n[82] Christian Weismayer, Ilona Pezenka, and Christopher Han-Kie Gan. 2018. Aspect-\nBased Sentiment Detection: Comparing Human Versus Automated Classifica-\ntions of TripAdvisor Reviews. In Information and Communication Technologies in\nTourism 2018, ENTER 2018, Proceedings of the International Conference in J√∂nk√∂ping,\nSweden, January 24-26, 2018 . Springer, 365‚Äì380. https://doi.org/10.1007/978-3-\n319-72923-7_28\n[83] Charles Welch, Chenxi Gu, Jonathan K. Kummerfeld, Ver√≥nica P√©rez-Rosas,\nand Rada Mihalcea. 2022. Leveraging Similar Users for Personalized Language\nModeling with Limited Data. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022 . Association for Computational Linguistics, 1742‚Äì\n1752. https://doi.org/10.18653/v1/2022.acl-long.122\n[84] Jennifer Windsor, Laura M Pich√©, and Peggy A Locke. 1994. Preference Testing: A\nComparison of Two Presentation Methods. Research in developmental disabilities\n15, 6 (1994), 439‚Äì455. https://doi.org/10.1016/0891-4222(94)90028-0\n[85] Jiechen Xu, Lei Han, Shazia Sadiq, and Gianluca Demartini. 2023. On The Role\nof Human and Machine Metadata in Relevance Judgment Tasks. Information\nProcessing & Management 60, 2 (2023), 103177. https://doi.org/10.1016/j.ipm.20\n22.103177\n[86] Ziying Yang, Alistair Moffat, and Andrew Turpin. 2018. Pairwise Crowd Judg-\nments: Preference, Absolute, and Ratio. In Proceedings of the 23rd Australasian\nDocument Computing Symposium, ADCS 2018, Dunedin, New Zealand, December\n11-12, 2018 . ACM, 3:1‚Äì3:8. https://doi.org/10.1145/3291992.3291995\n[87] Emine Yilmaz, Evangelos Kanoulas, and Javed A. Aslam. 2008. A Simple and\nEfficient Sampling Method for Estimating AP and NDCG. In Proceedings of the\n31st Annual International ACM SIGIR Conference on Research and Development\nin Information Retrieval, SIGIR 2008, Singapore, July 20-24, 2008 . ACM, 603‚Äì610.\nhttps://doi.org/10.1145/1390334.1390437\n[88] Seunghyun Yoon, Hyeongu Yun, Yuna Kim, Gyu-tae Park, and Kyomin Jung.\n2017. Efficient Transfer Learning Schemes for Personalized Language Modeling\nusing Recurrent Neural Network. In The Workshops of the The Thirty-First AAAI\nConference on Artificial Intelligence, Saturday, February 4-9, 2017, San Francisco,\nCalifornia, USA (AAAI Technical Report, Vol. WS-17) . AAAI Press. http://aaai.org\n/ocs/index.php/WS/AAAIW17/paper/view/15144\n[89] Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, Jae Sung Park, Ximing Lu,\nPrithviraj Ammanabrolu, Rowan Zellers, Ronan Le Bras, Gunhee Kim, and Yejin\nChoi. 2022. Multimodal Knowledge Alignment with Reinforcement Learning.\nCoRR abs/2205.12630 (2022). https://doi.org/10.48550/arXiv.2205.12630\narXiv:2205.12630\n[90] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.\n2020. BERTScore: Evaluating Text Generation with BERT. In 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=SkeHuCVFDr\n[91] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,\nHarris Chan, and Jimmy Ba. 2022. Large Language Models Are Human-Level\nPrompt Engineers. CoRR abs/2211.01910 (2022). https://doi.org/10.48550/arXiv.2\n211.01910 arXiv:2211.01910\n[92] Yiming Zhu, Peixian Zhang, Ehsan ul Haq, Pan Hui, and Gareth Tyson. 2023. Can\nChatGPT Reproduce Human-Generated Labels? A Study of Social Computing\nTasks. CoRR abs/2304.10145 (2023). https://doi.org/10.48550/arXiv.2304.10145\narXiv:2304.10145\nPerspectives on Large Language Models for Relevance Judgment\nGuglielmo Faggioli\nUniversity of Padova\nLaura Dietz\nUniversity of New Hampshire\nCharles L. A. Clarke\nUniversity of Waterloo\nGianluca Demartini\nUniversity of Queensland\nMatthias Hagen\nFriedrich-Schiller-Universit√§t Jena\nClaudia Hauff\nSpotify\nNoriko Kando\nNational Institute of Informatics (NII)\nEvangelos Kanoulas\nUniversity of Amsterdam\nMartin Potthast\nLeipzig University and ScaDS.AI\nBenno Stein\nBauhaus-Universit√§t Weimar\nHenning Wachsmuth\nLeibniz University Hannover\nABSTRACT\nWhen asked, current large language models (LLMs) like ChatGPT\nclaim that they can assist us with relevance judgments. Many re-\nsearchers think this would not lead to credible IR research. In this\nperspective paper, we discuss possible ways for LLMs to assist hu-\nman experts along with concerns and issues that arise. We devise\na human‚Äìmachine collaboration spectrum that allows categoriz-\ning different relevance judgment strategies, based on how much\nthe human relies on the machine. For the extreme point of \"fully\nautomated assessment\", we further include a pilot experiment on\nwhether LLM-based relevance judgments correlate with judgments\nfrom trained human assessors. We conclude the paper by provid-\ning two opposing perspectives‚Äîfor and against the use of LLMs\nfor automatic relevance judgments‚Äîand a compromise perspec-\ntive, informed by our analyses of the literature, our preliminary\nexperimental evidence, and our experience as IR researchers.\nWe hope to start a constructive discussion within the community\nto avoid a stale-mate during review, where work is dammed if is\nuses LLMs for evaluation and dammed if it doesn‚Äôt.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíRelevance assessment.\nKEYWORDS\nlarge language models, relevance judgments, human‚Äìmachine col-\nlaboration, automatic test collections\n1 INTRODUCTION\nThat evaluation is very important to the information retrieval (IR)\ncommunity is demonstrated by long-standing evaluation campaigns\nspread throughout the world [ 14, 37, 41, 50]. The difficulty of a\nproper evaluation setup in IR is also well-known [ 35, 48, 62, 64].\nWe thank Ian Soboroff for his ideas, comments, and other contributions.\nApril 2023, arXiv, Internet\n¬© 2023 Association for Computing Machinery.\nThis is the author‚Äôs version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record was published in .\nDear LLM, can you assist us with relevance judgments?\nYes, I can certainly provide assistance with relevance\njudgments. Please provide me with more information about\nwhat you need help with, and I will do my best to assist you.\nIf you need assistance with a specific task, such as evaluating\nthe relevance of a set of documents or web pages, please\nprovide me with the necessary information, such as the topic\nor query, the criteria for relevance, and any other relevant\ndetails. I can then help you to perform the evaluations and\nprovide feedback on the relevance of the content.\nFigure 1: Asking ChatGPT for assistance on Feb. 15, 2023.\nIR evaluation traces its roots back to the Cranfield paradigm [19],\nwhich is based on the concept of test collections consisting of (i) a\ndocument corpus, (ii) a set of information needs or topics, and\n(iii) relevance judgments for documents on the topics. Critically, ac-\ncording to the Cranfield paradigm, human assessors are needed for\nthe relevance judgments‚Äîa time-intensive and costly procedure.1\nHowever, over the past decades, we have become used to wit-\nnessing tasks that were traditionally performed by humans being\ndelegated to machines, starting with indexing and retrieval. While\nthe idea of automatically generated judgments [71] has been consid-\nered before, it has not found widespread use in the IR community.\nOther routes to minimize the cost of collecting relevance judgments\nin the past include judging text nuggets instead of documents [60],\nusing crowdworkers [3, 13] (though this comes with its own set of\nproblems [56]), cleverly selecting which documents to judge [16, 49],\nconstructing test collections from Wikipedia [29], or automating\nparts of the judgment process via a QA system [63].\nFigure 1 shows the response of ChatGPT2 when asked if it can\nassist with relevance judgments. The response suggests that it is\n1As a concrete example, for the 50 topics in the TREC-8 Ad Hoc track [76], 129 partici-\npating systems led to more than 86,000 pooled documents to judge, requiring more\nthan 700 assessor hours at a cost of about USD 15,000.\n2https://chat.openai.com/chat\narXiv:2304.09161v2  [cs.IR]  18 Nov 2023\nApril 2023, arXiv, Internet Faggioli, Dietz, Clarke, Demartini, Hagen, Hauff, Kando, Kanoulas, Potthast, Stein, and Wachsmuth\nable to carry out relevance judgments, but it is unclear how well\nsuch judgments align with those made by human annotators. In\nthis perspectives paper, we explore whether we are on the verge\nof being able to delegate the process of relevance judgment to\nmachines too, by means of large language models (LLMs)‚Äîeither\nfully or partially, across different domains and tasks or just for a\nselect few. We aim to provide a balanced view on this contentious\nstatement by presenting both consenting and dissenting voices in\nthe scientific debate surrounding the use of LLMs for this purpose.\nAlthough a variety of document modalities exist (audio, video, im-\nages, text), we here focus on text-based test collections. We opt\nfor text collections being the most commonly used ones in IR: the\nconsolidated methodology for assessing the relevance of textual\ndocuments, which dates back to the Cranfield paradigm, enables us\nto carry a ground comparison between LLMs and human assessors.\nWhile the technology might not be ready yet to provide fully\nautomatic relevance judgments, we argue that LLMs are already\nable to help humans in this task‚Äîto various extents. To model\nthe range of automation, we propose a spectrum illustrating the\ndegrees of collaboration between humans and LLMs (see Table 1).\nThis spectrum spans from manual judgments, the current setup, to\nfully automated judgments that are carried out solely by LLMs, a\npotentially envisioned perspective. The level of human involvement\nand decision making varies along the spectrum.\nContributions. In this perspectives paper, we discuss a spectrum\nof scenarios of leveraging human‚Äìmachine collaboration for rele-\nvance judgments in IR contexts. Some scenarios have been studied\nalready and are elaborated in the related work section. Others are\ncurrently emerging, for which we describe risks as well as open\nquestions that require further research. We also conduct a pilot\nfeasibility experiment where we assess to what extent judgments\ngenerated by LLMs agree with human judgments, including an anal-\nysis of LLM-specific caveats. To conclude our paper, we provide\ntwo opposing perspectives‚Äîfor and against the use of LLMs for\nautomatic relevance judgments‚Äîas well as a compromise between\nthem. All of them are informed by our analyses of the literature, our\npilot experimental evidence, and our experience as IR researchers.\n2 RELATED WORK\nThe test collection approach to Information Retrieval (IR) requires\nthe creation of queries, documents and relevance judgments to be\ncreated. The traditional approach is to hire human assessors, to\nprovide relevant judgments. However, the manual effort associated\nwith their creation is staggering, leading to a range of approaches\nto either assist the assessor or automate tedious tasks. The goal is to\nboth improve the annotation quality, consistency, and efficiency of\nthe assessment. Below we describe existing approaches and relate\nthem to the Human-Machine-Collaboration spectrum.\n2.1 Human Judgment\nAssessment Systems. Neves and Seva [58] provide a rich survey\nof tools used by human experts in annotating documents. They\nidentify a set of 13 features of such tools that help the human\nassessor in completing their task, such as text highlighting support\nfor pre-annotations and integration with external data sources (e.g.,\nontologies and thesauri) [73, 83].\nCrowdsourcing. As document collections kept growing in size,\nthe ratio of documents that could practically be judged by hu-\nman assessors kept getting smaller. This triggered the research\ncommunity to look for ways to scale-up the collection of human-\ngenerated relevance judgments. Around 2010, research looking at\nreplacing trained human assessors leveraging micro-task crowd-\nsourcing started to appear [3]. In the last 10 years, the community\nhas been looking at research questions related to the reliability\nof crowdsourced relevance judgments [13] as well as at questions\nrelated to cost and quality management [56]. The increase in pos-\nsible scale and accessibility of work power usually comes with a\ndecrease in reliability, often due to the complicated interaction of\ncrowd workers and task requesters [59]. The current understand-\ning based on research findings is that crowdsourcing relevance\njudgments is a viable solution to scaling up the collection of labels\nand an alternative approach to traditional relevance judgments per-\nformed by trained human assessors. This is true as long as quality\ncontrol mechanisms are put in place and the domain is accessible\nto non-experts [74]. Quality control mechanisms may include label\naggregation methods [69], task design strategies [2, 52], and crowd\nworker selection strategies [36]. Recent research has looked at how\nto support crowd workers in judging relevance by presenting them\nwith extra information (e.g., machine-generated metadata) that can\nincrease their judgment efficiency [80].\n2.2 Human Verification and AI Assistance\nIn this scenario, the human partially relinquishes control over which\ndocuments will be assessed or how the assessments will be derived\nby the machine but remains in control of defining relevance.\nPassage-ROUGE and BertScore. As a cost-effective means to judge\npassages, Keikha et al. [53] expand automatically manual relevance\njudgments: any unjudged passage that has a high similarity to a\njudged passage, will inherit its relevance label. They explore the\nROUGE measure as a similarity. Alternatively, approaches such as\nBertScore [86] can serve as an LLM-based similarity.\nAutoTar. Several approaches to semi-automatic support in test\ncollection creation have been proposed. One approach is to use\nactive learning for annotation [20], where the pool of documents\nto manually assess is determined based on the confidence of a\nmachine learning algorithm, the role of the human is to assess given\ndocuments. This approach is very successful when the failure to\nidentify a relevant document must be avoided. Similarly, Jayasinghe\net al. [49] describe a method for selecting documents to be included\nin a test collection using a machine learning approach: the proposed\nmethodology finds relevant documents that would otherwise only\nbe found using manual runs, and allows for constructing a low-bias\nreusable test collection.\nEstimating AP. Alternatively, evaluation metrics can be adjusted\nto correct for biases of incomplete judgments [82]. This approach\nreduces the cost by reducing the number of assessments needed for\nevaluating search systems.\nEXAM. Instead of asking humans to assess each document for\nrelevance, for the EXAM Answerability Metric Sander and Dietz\n[63] ask humans to design a set of exam questions that can be\nPerspectives on Large Language Models for Relevance Judgment April 2023, arXiv, Internet\nanswered with relevant text. An automatic question-answering Sys-\ntem is asked to answer these exam questions by using the content\nof retrieved documents. The idea is that the more questions can\nbe answered correctly with the document, the better the search\nsystem that retrieved the document is. Similar paradigms have been\nused successfully in other labeling tasks as well [28, 31, 45]\nQuery Performance Prediction. A related body of work concerns\nthe Query Performance Prediction (QPP), which is defined as the\ntask of evaluating the performance of an IR system, in the absence\nof human-made relevance judgements [15, 42]. In this regard, our\nproposal for automatic assessment of the documents using Large\nLanguage Model (LLM) not only would provide benefits to a number\nof downstream tasks, such as QPP, but its effectiveness has already\nbeen partially shown and is supported by flourishing literature\nconcerning LLMs in the QPP domain [4, 5, 17, 27].\n2.3 Fully Automated Test Collections\nA further strategy to devise queries automatically is the Wikimarks\napproach [29]. Wikimarks derives queries from the title and head-\ning structure of Wikipedia articles, with passages below taken as\nrelevant. This approach has also been applied to aspect-based sum-\nmarization [43], and text segmentation [6].\nReconstruct Documents. Instead of hiring assessors, repositories\nof semi-structured (human-authored) articles can be used to derive\nwhat the human author considered relevant. To this end approaches\nuse anchor text [ 7], metadata of scientific article sections [ 12],\ncategories in the Open Directory Project [10], glosses in Freebase\n[25] or infoboxes [44, 51].\nEvaluation of Automatic Evaluation. A question is how well au-\ntomatic assessments would agree with manual assessments. To this\nend, a study on the correlation of leaderboards on the TREC CAR\ndata found a very high-rank correlation [30]. We repeat a similar\nstudy in the context of LLMs in Section 5.\n3 SPECTRUM OF HUMAN‚ÄìMACHINE\nCOLLABORATION\nTo identify what contributions LLMs may provide to relevance judg-\nments, we devise a human‚Äìmachine collaboration spectrum. This\nspectrum outlines different levels of collaboration between humans\nand LLMs. At one end, humans make judgments manually, while at\nthe other end, LLMs replace humans completely. In between, LLMs\nassist humans with various degrees of interdependence. A sum-\nmary of our proposed four levels of human‚Äìmachine collaboration\nis shown in Table 1. In the following, we discuss each level in detail.\nHuman Judgment. On one extreme, humans do all judgments\nmanually and decide what is relevant without being influenced\nby an LLM. In reality, of course, humans are still supported with\nbasic features of the judgment interface. Such features might be\nbased on heuristics, but should not require any form of automatic\ntraining/feedback. For instance, humans may define ‚Äúscan terms‚Äù\nto be highlighted in the text, they may limit viewing the pool of\ndocuments that have already been judged, or they may order docu-\nments so that similar documents are near each other. This end of\nTable 1: A spectrum of collaborative\n human ‚Äì\n machine\ntask organization to produce relevance judgments. The ‚ñ≥\nindicates where on the spectrum each possibility falls.\nCollaboration\nIntegration Task Organization\nHuman Judgment\n‚ñ≥ Humans do all judgments manually without\nany kind of support.\n‚ñ≥ Humans have full control of judging but are\nsupported by text highlighting, document\nclustering, etc.\nAI Assistance\n‚ñ≥ Humans judge documents while having\naccess to LLM-generated summaries.\n‚ñ≥ Balanced competence partitioning. Humans\nand LLMs focus on (sub-)tasks they are good\nat.\nHuman Verification\n‚ñ≥ Two LLMs each generate a judgment, and\nhumans select the better one.\n‚ñ≥ An LLM produces a judgment (and an\nexplanation) that humans can accept or\nreject.\n¬∑ùëõ‚ñ≥ LLMs are considered crowdworkers with\nvaried specific characteristics, but\nsupervised / controlled by humans.\nFully Automated\n‚ñ≥ Fully automatic judgments.\nthe spectrum thus represents the status quo, where humans are, in\nthe end, the only reliable judges.\nAI Assistance. Advanced assistance can come in many forms,\nfor example, an LLM may generate a summary of a to-be-judged\ndocument so that the human assessor can more efficiently make\na judgment based on this compressed representation. Another ap-\nproach could be to manually define information nuggets that are\nrelevant (e.g., exam questions [ 63]) and to then train an LLM to\nautomatically determine how many test nuggets are contained in\nthe retrieved results (e.g., via a QA system).\nThis leads us to the first research direction towards improving\nthe human‚Äìmachine collaboration: How to employ LLMs, as well as\nother AI tools, to aid human assessors in devising reliable judgments\nwhile enhancing the efficiency of the process? What are tasks that can\nbe taken over by LLMs (e.g., document summarization or keyphrase\nextraction)?\nHuman Verification. For each document to judge, a first-pass\njudgment of an LLM is automatically produced as a suggestion\nApril 2023, arXiv, Internet Faggioli, Dietz, Clarke, Demartini, Hagen, Hauff, Kando, Kanoulas, Potthast, Stein, and Wachsmuth\nalong with a generated rationale. We consider this to be a human-\nin-the-loop approach: one or more LLMs provide their relevance\njudgment and the human verifies them. In most cases, the human\nwill therefore be assigned menial and undemanding tasks, or will\nnot have to intervene at all. Regardless, the human might still be\nrequired in challenging scenarios or situations where the LLM has\nlow confidence. Another approach could follow the ‚Äúpreference\ntesting‚Äù paradigm [79] where two machines each generate a judg-\nment, and a human will select the better one‚Äîintervening only\nin the case of disagreements between the machines and verifying\nthe information. In both cases, humans make the ultimate decision\nwherever needed. The concern is that any bias in the LLM might\nbe affecting relevance judgments, as humans will not be able to\ncorrect for information they will not see.\nConcerning this layer of the spectrum, the research direction\nthat we wish to raise within the community is: What sub-tasks of\nthe judgment process require human input (e.g., prompt engineer-\ning [72, 87]‚Äîfor now) and for what tasks should human assessors\nnot be replaced by machines?\nFully Automated. If LLMs were able to assess relevance reliably,\nthey could completely replace humans in the judgment process.\nWe explore the possibility that a fully automatic judgment system\nmight be as good as a human in producing high-quality relevance\njudgments (for a specific corpus/domain). Automatic judgments\nmight even surpass the human in terms of quality, which raises\nfollow-up issues (cf. Section 4.3).\nIn this regard, the third research direction that our community\nshould investigate is: How can humans be replaced entirely by LLMs\nin the judgment process? Indeed, one can go as far as asking whether\ngenerative LLMs can be used to create new test collections by\ncreating new corpora, queries, abstracts, and conversations.\nA central aspect to be investigated is where on this four-level\nhuman‚Äìmachine collaboration spectrum we actually obtain the\nideal relevance judgments at the best cost. At this point, humans\nperform tasks that humans are good at (maybe none?!), while ma-\nchines perform tasks that machines are good at. We refer to this\nscenario as competence partitioning [34, 40]: the task is assigned to\neither the human or the machine, depending on who is better. Note\nthat in our current version of the spectrum, we still (optimistically)\nshow balanced competence partitioning as part of ‚ÄúAI assistance‚Äù.\n4 OPEN ISSUES, FORESEEABLE RISKS, AND\nOPPORTUNITIES\nIn this section, we look at different issues that come up whenLLMs\nare used for relevance judgment tasks. We discuss open research\nquestions, risks we foresee, as well as opportunities to move beyond\nthe currently accepted IR evaluation paradigms.\n4.1 AI Assistance\nLLMs‚Äô Judgment Quality. It is yet to be understood what the\nbenefits and risks associated with LLM technology are. A rather\nsimilar debate was spawned more than ten years ago with the early\nuse of crowd workers to create relevance judgments. While be-\nfore, judgments were typically made by in-house experts, the very\nsame judgment tasks were then delegated to crowd workers, with\nTable 2: Abilities of different types of assessors to handle\nvarious types of judgments. Similar levels of ability might\nhint at scenarios where specific types of human assessors\nmight be replaced by LLMs.\nType of Judgment\nType of Assessor Cost PreferenceBinary GradedExplained\nUser free ‚äï ‚äï ‚äï ‚äô\nExpert expensive ‚äï‚äï ‚äï‚äï ‚äï ‚äï\nCrowdworker cheap ‚äô ‚äï ‚äï ‚äô\nLLM\n very cheap ‚äï ‚äï ‚äô ‚äï\nLegend: ‚äï‚äïcan judge, ‚äïdepends, ‚äôunknown\na substantial decrease in terms of quality of the judgment, compen-\nsated by a huge increase in annotated data [39]. Quality-assurance\nmethods were developed to obtain the highest gains [ 26]. With\nLLMs, history may repeat itself: a huge increase in annotated data,\nwith a decrease in terms of quality‚Äîalthough the specific extent\nof the deterioration is still unclear. LLM-specific quality assurance\nmethods will need to be developed, and, even an improvement in\nquality is possible. A related idea consists in allowingLLMs to learn\nby observing human annotators performing the task or following\nan active learning paradigm [67, 68, 85]. The LLM starts with mild\nsuggestions to the assessor on how to annotate documents, then\nit continues to learn by considering actual decisions made by the\nannotator and finally improving the quality of the suggestions pro-\nvided. See in this regard the scenarios ‚ÄúAI Assistance‚Äù and ‚ÄúHuman\nVerification‚Äù in Table 1.\nIn essence, we ask the question: For which tasks can what type of\nhuman assessor be replaced by an LLM? Table 2 provides a rough\nview in this regard: We distinguish four types of assessors (user,\nexpert, crowdworker, and LLM) over four judgment tasks: prefer-\nence (which document is more relevant), binary (which of the two\ndocuments is relevant), graded (distinguish more than two levels\nof relevance), explained (justify the relevance decision). Table 2 is\nuseful in showing a spectrum of substitutions, but it is unsatisfac-\ntory in clarifying the role of LLM‚Äîwe are still in the early stages\nof development and simply do not know (‚äô).\nLLMs Cost. Related to AI assistance as well as human verifica-\ntion, Table 2 shows tendencies regarding the replacement or the\nindispensable properties of humans in judgment tasks. The table\nincludes a ‚Äúcost‚Äù column that will play a role in the future, but for\nwhich only relative estimates can be provided at this time. Note\nthat there is no clear exclusion for either party.\n4.2 Manual Verification\nUsing Multiple LLMs as Assessors. A difference between humans\nand automatic assessors concerns the number of assessors. While it\nis possible to hire multiple human assessors to annotate documents\nand, possibly, resolve disagreements between annotators [32], this\nis not that trivial in the automatic assessor case. LLMs which are\nPerspectives on Large Language Models for Relevance Judgment April 2023, arXiv, Internet\ntrained on similar corpora are likely to produce correlated answers‚Äî\nbut we do not know whether these are correct. A possible solution\nto this would include the usage of different subcorpora based on\ndifferent sets of documents. This, in turn, could lead to personalized\nLLMs [47, 78, 84], fine-tuned on data from different types of users,\nwhich would allow to auto-annotate documents directly according\nto a user‚Äôs subjective point of view, while also helping with increas-\ning the pool of judgments collected. While this technology is not\navailable yet, mostly due to computational reasons, we expect it to\nbe available in the coming years.\nTruthfulness & Misinformation. An important aspect to consider\nwhen it comes to relevance judgments is factuality. Consider the\nquestion ‚Äúdo lemons cure cancer?‚Äù, for which top-ranked documents\nmay indeed discuss healing cancer with lemons. While topically\nrelevant, the content is unlikely to be factually correct. The result\ncan therefore be defined as not relevant to correctly answering\nthe information need. To overcome this issue, human assessors\nhave to access external information (as well as their own acquired\nknowledge) to determine the trustworthiness of a source as well as\nthe truthfulness of a document.\nIn the fully automatic setting, we rely entirely onLLMs to verify\nthe source and the truthfulness of the document content. This\nraises questions: Can we automatically assess the reliability ofLLM-\ngenerated results? Can we automate fact-checking, for example,\nby identifying the information source of a generative model and\nverifying that it is presented accurately? Going forward, it will\nalso be vital to be able to distinguish between human-generated\nand LLM-generated data, especially in contexts such as journalism\nwhere the correctness of facts is critical.\nBias. LLMs are biased, the evaluation should not be. Bender et al.\n[11] highlight limitations associated withLLMs, identifying a severe\nrisk in their internal bias. LLMs are intrinsically biased [9, 46, 54]\nand such bias may also be reflected in the relevance judgments. For\nexample, an LLM might be prone to consider documents written\nin scientific language as relevant, while being biased against docu-\nments written in informal language. The community should focus\non finding a way to evaluate the model itself in terms of bias, and\nverify that, even though a model has been trained on biased data,\nthe evaluation is not unduly affected by the same biases.\nFaithful Reasoning. LLMs can generate text that contains inac-\ncurate or false information (i.e., hallucinate). This text is often pre-\nsented in such an affirmative manner that it makes it difficult for hu-\nmans to detect errors. In response, the NLP community is exploring\na new research direction called ‚Äúfaithful reasoning‚Äù [22]. This ap-\nproach aims to generate text that is less opaque, also describing ex-\nplicitly the step-by-step reasoning, or the ‚Äúchain of thoughts‚Äù [55].\nExplain Relevance to LLMs. Judgment guidelines provide a com-\nprehensive overview of what constitutes a relevant document for a\nspecific task‚Äîmost famously, Google‚Äôs search quality rating guide-\nlines for web search have been more than 170 pages long.3 It is an\nopen question how to ‚Äútranslate‚Äù such guidelines for LLMs.\nIn addition, for many tasks, relevance may go beyond topical\nrelevance [65]. Sometimes, a certain style is desired. Sometimes,\n3https://guidelines.raterhub.com/searchqualityevaluatorguidelines.pdf\nthe truthfulness of the information is very important. Sometimes,\ndesired information should allure users from certain communities\nand cultures with different belief systems. We do not yet know\nto what extent LLMs are capable of assessing these very different\ninstantiations of relevance. We believe that, to properly support\nwidely different tasks, human intervention needs to be plugged\ninto the collection and judgment of additional facts and document\naspects not yet easily discernable for an LLM.\n4.3 Fully Automated\nLLM-based Evaluation of LLM-based Systems. In the fully auto-\nmated scenario, a circulatory problem can arise: How is this ranking\nevaluation different from being an approach that produces a rank-\ning? In practical settings, we expect theLLM used for ranking to be\nmuch smaller (more cost effective, lower latency, etc. achieved for\nexample by knowledge distillation) than the LLM used for judging.\nIn addition, the judging LLM can be endowed with additional in-\nformation about relevant facts/questions/nuggets that the system\nunder evaluation does not have access to. Lastly, we point to an\nensemble of judging LLMs as a potential way forward.\nMoving Beyond Cranfield. Many assumptions and decisions taken\nin the relevance judgment process enable us to make the manual\njudgment feasible within a limited time and monetary budget. For\nexample, we consider collections static, and relevance judgments\nto not change over time (a simplification as seen in [ 66, 75]); we\nassume that the relevance of a document is not dependent on the\nother documents in the same ranking and that creating relevance\njudgments for a small set of queries provides us with a sufficiently\ngood amount of data to compare a set of search systems with each\nother. If LLMs would perform reliably with little human verifica-\ntion, many of these assumptions could be relaxed. For example, in\nTREC CAsT [23, 24]4, information needs are developing (instead\nof static) as the user learns more about the domain. Hence a tree of\nconnected information needs is defined, where one conversation\ntakes a path through the tree. The Human-Machine evaluation par-\nadigm might make it feasible to assess more connected (and hence,\nrealistic) definitions of relevance.\nMoving Beyond Human. Finally, we point out that there is room\nbeyond our proposed spectrum: this point is reached when ma-\nchines surpass humans in the relevance judgment task. We have\nwitnessed this phenomenon in a variety of NLP tasks, such as\nscientific abstract classification [38] and sentiment detection [77].\nHumans are likely to make mistakes when annotating documents\nand are limited in the time dedicated to judgment. It is likely that\nLLMs will be more self-consistent, and (with sufficient monetary\nfunds) capable of providing a large number and more consistent\njudgments. However, if we use human-annotated data as a gold\nstandard, we will not be able to detect when the LLM surpasses\nhuman performance. We then will have reached the limit of mea-\nsurement: We will not be able to use differences between the current\nevaluation paradigms to evaluate such models.\n4TREC CAsT is a shared task that aims at evaluating conversational agents. TREC CAsT\nprovides information needs in the form of multi-turn conversations, each containing\nseveral utterances that a user might pose to a conversational agent.\nApril 2023, arXiv, Internet Faggioli, Dietz, Clarke, Demartini, Hagen, Hauff, Kando, Kanoulas, Potthast, Stein, and Wachsmuth\n5 PRELIMINARY ASSESSMENT\nTo provide a preliminary assessment of today‚Äôs LLM capability for\nrelevance judgments, we conducted an empirical comparison be-\ntween human andLLM assessors. This comparison includes two test\ncollections (TREC-8 adhoc retrieval [76] and the TREC 2021 Deep\nLearning Track [21]), two types of judgments (binary and graded),\ntwo tailored prompts and two models (GPT-3.5 and YouChat). The\nexperiments we report in this section were conducted in January\nand February 2023.\n5.1 Methodology\nWe want to emphasize that the experiments we present are not\nmeant to be exhaustive, instead the goal is to explore where LLMs\nagree or disagree with manual relevance judgments.\nCorpora. We base our experiments on two test collections: (i) the\npassage retrieval task of the TREC 2021 Deep Learning Track (TREC\nDL 2021) [21], and (ii) the adhoc retrieval task of TREC-8 [76]. Be-\nsides having a large number of relevance judgments, these col-\nlections also have contrasting properties. The TREC DL-2021 test\ncollection comprises short documents and queries phrased as ques-\ntions; the TREC-8 adhoc test collection comprises much longer,\ncomplete documents, with detailed descriptions of information\nneeds, explicitly stating what is and is not considered relevant. As\nan experimental corpus, TREC DL 2021 provides the additional\nbenefit that its creation date falls after the time that training data\nwas crawled for the main GPT-3.5 LLM model we are employing in\nour experiments (up to June 2021) but falls before the release of the\nmodel itself (November 2022)5. The LLM was not directly trained\non TREC-DL topics and relevance judgments, nor was it used as a\ncomponent in any system generating experimental runs.\nSampling. Given the available relevance judgments created by\nprofessional TREC assessors, we sampled ùëõ = 1000 TREC-8 and\nTREC-DL 2021 topic‚Äìdocument pairs from the published relevance\njudgments files, respectively. Due to the limited scalability of using\nYouChat, we restricted ourselves to100 samples per relevance grade\nfor both tasks. We sampled random pairs from all available pairs,\nso that each relevance grade (binary for TREC-8 and graded for\nTREC-DL 2021) appeared with the same frequency in our sample.\nLLMs. We selected twoLLMs for our experiments: GPT-3.5, more\nspecifically text-davinci-0036, as accessed via OpenAI‚Äôs API,7\nand YouChat, both in February 2023. The former is an established\nstandard model for many applications and thus serves as a natural\nstarting point and first baseline, the latter has been recently inte-\ngrated with the You search engine8 as one of the first LLMs to be\nfully integrated with a commercial search engine for the task of\ngenerating a new kind of search engine result page (SERP) that\nresembles a Wikipedia article, where the text is a query-biased\nsummary of the top-ùëò most relevant web pages, ùëò ‚â≤ 5, according\nto You‚Äôs retrieval model with numbered references to the ùëò web\npages, which are listed as ùëò ‚Äúblue links‚Äù below it. The YouChat\nrelease followed closely in the wake of that of OpenAI‚Äôs ChatGPT.\n5https://platform.openai.com/docs/models/overview\n6https://spiresdigital.com/new-gpt-3-model-text-davinci-003\n7https://platform.openai.com/docs/api-reference/introduction\n8https://you.com.\nInstruction: You are an expert assessor making TREC\nrelevance judgments. You will be given a TREC topic and\na portion of a document. If any part of the document is\nrelevant to the topic, answer ‚ÄúYes‚Äù. If not, answer ‚ÄúNo‚Äù.\nRemember that the TREC relevance condition states that a\ndocument is relevant to a topic if it contains information that\nis helpful in satisfying the user‚Äôs information need described\nby the topic. A document is judged relevant if it contains\ninformation that is on-topic and of potential value to the user.\nTopic: {topic}\nDocument: {document}\nRelevant?\nInstruction: Indicate if the passage is relevant for the question.\nQuestion: {question}\nPassage: {passage}\nFigure 2: Prompts used in our ¬ß5 experiments on TREC-8 (top)\nand TREC-DL 2021 (bottom). The placeholders {topic} and\n{document} (TREC-8) and {question} and {passage} (TREC-\nDL 2021) are replaced with our sampled pairs.\nWe chose the former due to it being an order of magnitude faster\nand more stable at the time of writing, whereas the latter had long\ntime spans of unreachability and instability.\nPrompts. We created two simple and straightforward prompts for\nthe two corpora as shown in Figure 2. We explicitly did not spend\ntime on optimizing the prompts (so-called ‚Äúprompt engineering‚Äù)\nto determine whether those small differences in phrasing have an\nimpact. Rather, we kept the prompts straightforward and to the\npoint to establish a first baseline, and leave studying the importance\nof the prompt for future work.\nAnswer Parsing. We recorded each model‚Äôs generated answers\nand translated them into binary relevance judgments. In the case\nof GPT-3.5, the prompts and the setting temperature = 0 were suf-\nficient to constrain the model to emit only the relevance grades\nrequested in the prompt. In the case of YouChat, with two excep-\ntions, the answers for TREC-DL 2021 were entirely homogeneous,\nand started with either‚ÄúThe passage is relevant [. . . ]‚Äù or ‚ÄúThe passage\nis not relevant [. . . ]‚Äù and were thus straightforward to parse. The\nanswers for the TREC-8 prompts were similarly homogeneous.\n5.2 Results\nIn Table 3 we report our results for TREC-8 assessors vs. GPT-3.5\nand YouChat respectively. We observe a clear divide according to\nthe relevance label: for the documents judged by human assessors\nas non-relevant, GPT-3.5 generates the same answer in 90% of the\ncases. In contrast though, for the documents judged as relevant by\nhuman assessors, this agreement drops to 50%. Likewise, YouChat\nhas judged 74% of the non-relevant correctly to be non-relevant,\nwhereas this agreement drops even more to 33% for the relevant\ndocuments.\nPerspectives on Large Language Models for Relevance Judgment April 2023, arXiv, Internet\nTable 3: Judgment agreement on TREC-8 between TREC as-\nsessors and the LLMs; 1000 topic‚Äìdocument pairs for GPT-3.5\nand 100 for each grade (relevant, non-relevant) for YouChat.\nLLM Prediction TREC-8 Assessors Cohen‚Äôs ùúø\nRelevant Non-relevant\nGPT-3.5 Relevant 237 48 0.38Non-relevant 263 452\nYouChat Relevant 33 26\n0.07 Non-relevant 6774\nTable 4: Judgment agreement on TREC-DL 2021 between\nTREC assessors and the LLMs; 100 question‚Äìpassage pairs\nfor each grade from 3 (highly relevant) to 0 (non-relevant).\nLLM Prediction TREC-DL 2021 Assessors\n3 2 1 0 Cohen‚Äôs ùúø\nGPT-3.5 Relevant 89 65 48 16 0.40Non-relevant 11 35 52 84\nYouChat Relevant 96 93 79 42 0.49Non-relevant 4 7 21 58\nInterestingly though, when we consider the results of our sec-\nond experiment in Table 4‚ÄîTREC-DL21 assessors vs. GPT-3.5 and\nYouChat respectively‚Äîthe picture changes completely. We observe\nalmost the opposite of what we have just described in the previous\nparagraph. Concretely: the higher the relevance grade, the more\nYouChat is in line with the human assessors. For 96 out of 100\nquestion‚Äìpassage pairs that TREC assessors judges as highly rele-\nvant (i.e., relevance grade 3), YouChat agreed with the assessor. In\ncontrast, for the non-relevant question‚Äìpassage pairs, the agree-\nment is random. YouChat only agrees with manual assessments\non 42 of 100 non-relevant question‚Äìpassage pairs.\nAs a possible explanation for these observations, we hypothesize\nthat human assessors are better at recognizing subtle details that\ndistinguish relevant from non-relevant documents. When explor-\ning coarse-grained graded relevance judgments, however, LLMs\ndemonstrate a better correlation with the human judgments. We\nsuspect that LLMs would be helped by symmetrically centering\nrelevance judgments around 0 (‚Äúborderline relevant‚Äù) with a range\nfrom -3 to 3.\n6 RE-JUDGING TREC 2021 DEEP LEARNING\nTo complement the experiments reported in Section 5, in this section\nwe report an experiment to fully re-judge submissions to a single\nevaluation exercise, the passage ranking task of the TREC 2021 Deep\nLearning Track [21]. Unlike the experiments reported in Section 5,\nwhich focused on binary relevance, we attempt to adhere as closely\nas possible to the methodology used in the track itself, including\nthe use of graded judgments.\n6.1 Methodology\nCraswell et al. [21] provide full details of the passage ranking task\nof the TREC 2021 Deep Learning Track (TREC-DL 2021). TREC-\nDL 2021 track participants submitted a total of 63 experimental\nruns, with each run comprising up to 1000 ranked passages for\n200 test queries. These runs were pooled, and 53 queries were\njudged by assessors using a combination of methods, including\nactive learning [1, 70]. This generated a total of 10,828 judgments on\na 4-point scale: ‚ÄúPerfectly relevant‚Äù ‚âª‚ÄúHighly relevant‚Äù ‚âª‚ÄúRelated‚Äù\n‚âª‚ÄúIrrelevant‚Äù.\nWe re-judged this pool using the GPT-3.5 text-davinci-003\nlanguage model, as accessed through Open AI‚Äôs API in Febru-\nary 2023. Consistent with a classification task‚Äîand consistent with\nthe GPT-3.5 experiments reported in Section 5‚Äîwe set thetempera-\nture parameter to 0, but otherwise default parameters and settings.\nSince our prompt is relatively long, we provide it online.9 The\nprompt is inspired by a prompt appearing in Ferraretto et al. [33]:\nimportantly‚Äîand different from the prompt in Figure 2‚Äîit lever-\nages few-shot learning by listing multiple examples illustrating\ndifferent levels of relevance for different queries. We provide one\nexample each for ‚ÄúPerfectly relevant‚Äù, ‚ÄúHighly relevant‚Äù, and ‚ÄúRe-\nlated‚Äù; we provide two examples for ‚ÄúIrrelevant‚Äù, with one providing\na judged ‚ÄúIrrelevant‚Äù passage, and the other providing an unrelated\npassage from the pool. These examples were chosen arbitrarily from\nthe pool, based on the TREC judgments. We also used the term\n‚ÄúRelevant‚Äù in the prompt, instead of ‚ÄúRelated‚Äù, since ‚ÄúRelated‚Äù is a\nnon-standard label for relevance judgments; in preliminary experi-\nments, the LLM would sometimes return ‚ÄúRelevant‚Äù unprompted.\nUsing this prompt, judgments cost around USD 1 cent each. For this\nexperiment we spent a total of USD 111.90, including a small num-\nber of duplicate requests due to failures and other issues. To provide\na basis for comparison, Clarke et al. [18] report spending USD 25\ncents per human label on a judgment task of similar scope‚Äîwith a\nsingle-page ‚Äúprompt‚Äù and no training of assessors.\n6.2 Results\nTable 5 provides a summary of the results. We provide a summary\nfor both the full 4-point relevance scale and a binary relevance\nscale, which follows the TREC-DL 2021 convention for computing\nbinary measures such as MAP. This convention maps ‚ÄúPerfectly\nrelevant‚Äù and ‚ÄúHighly relevant‚Äù to ‚ÄúRelevant‚Äù, and maps ‚ÄúRelevant‚Äù\nand ‚ÄúIrrelevant‚Äù to ‚ÄúNot relevant‚Äù. Whereas for Table 4, in order to\ncompare results with YouChat, we followed the more usual conven-\ntion of treating all grades except ‚ÄúIrrelevant‚Äù as relevant. On the\nbinary judgments of Table 5, Cohen‚ÄôsùúÖ = 0.26, a level of agreement\nthat is conventionally described as ‚Äúfair‚Äù. To provide a basis for\ncomparison, Cormack et al. [20] report results corresponding to a\nCohen‚Äôs ùúÖ = 0.52 on a similar experiment comparing two types of\nhuman judgments, a level of agreement conventionally described\nas ‚Äúmoderate‚Äù.\nWe applied the LLM judgments to compute standard evaluation\nmeasures on the runs submitted to TREC-DL 2021, with the results\nshown in Figure 3. Kendall‚Äôsùúè values show the correlation between\nsystem rankings. To provide a basis for comparison, Voorhees [75]\n9https://plg.uwaterloo.ca/~claclark/trec2021_DL_prompt.txt\nApril 2023, arXiv, Internet Faggioli, Dietz, Clarke, Demartini, Hagen, Hauff, Kando, Kanoulas, Potthast, Stein, and Wachsmuth\nFigure 3: Scatter plots comparing the performance of TREC\n2021 Deep Learning Track passage ranking runs using official,\nhuman judgments and unofficial, LLM judgments, with MAP\n(top) and NDCG@10 (bottom). A point represents the perfor-\nmance of a single experimental run avg. over all queries.\nreport a Kendall‚Äôs ùúè = .90 for MAP on a similar experiment compar-\ning two types of human judgments. Nonetheless, the top run under\nthe official judgments remains the top run under the LLM judg-\nments.\nWe find that measures computed under the LLM judgments are\nless sensitive than measures computed under human judgments.\nSensitivity (or ‚Äúdiscriminative power‚Äù) measures the ability of an\nevaluation method to recognize a significant difference between\nretrieval approaches [18, 61, 81]. To compute sensitivity, we take\nall pairs of experimental runs and compute a paired t-test between\nthem. A pair with ùëù < 0.05 is considered to be distinguished [81],\nwith sensitivity defined as # of distinguished pairs\ntotal pairs . Since we do not\ncorrect for the multiple comparisons problem, some of the dis-\ntinguished pairs may not represent actual significant differences.\nUnder human judgments 72% of systems are distinguished under\nTable 5: Confusion matrices comparing all official TREC\nquestion‚Äìpassage judgments with GPT-3.5 judgments on\nTREC-DL 2021 question‚Äìpassage pairs. The upper matrix\n(GRADED) compares judgments on all four relevance levels.\nThe lower matrix (BIN.) collapses the relevance labels to two\nlevels, following the TREC-DL 2021 convention for comput-\ning binary measures.\nPrediction TREC-DL 2021 Assessors\nPerf. rel. High. rel. Related Irrel.\nGRADED\nPerfectly relevant 250 248 177 87\nHighly relevant 360 575 628 370\nRelevant 328 880 798 442\nNon-relevant 148 638 1460 3439\nPrediction TREC-DL 2021 Assessors\nRelevant Not relevant\nBIN.\nRelevant 1433 1262\nNon-relevant 1994 6139\nMAP (74% under NDCG@10). In contrast, under GPT-3.5 judgments\nonly 65% are distinguished (69% under NDCG@10).\n7 PERSPECTIVES FOR THE FUTURE\nAs this is a perspectives paper, we now provide two opposing\nperspectives‚Äîfor and against the use of LLMs for automatic rel-\nevance judgments‚Äîand a compromise perspective, all of which\nare informed by our analysis of the literature, our experimental\nevidence, and our experience as IR researchers.\n7.1 In Favor of Using LLMs for Judgments\nMore than just the plain judgment of relevance, LLMs are able to\nproduce a natural language explanation why a certain document\nis relevant or not to a topic [33]. Such AI-generated explanations\nmay be used to assist human assessors in relevance judgments,\nparticularly non-experts like crowdworkers. This setup may lead\nto better quality judgments as compared to the unsupported crowd.\nWhile LLM-generated labels and explanations may bias human as-\nsessors and mislead them on the relevance a document has, human\nassessors may serve as a quality control mechanism for the LLM\nas well as a feedback loop for the LLM to continuously improve its\njudgments. Our pilot experiments demonstrate that it is feasible for\nLLMs to indicate when a document is likely not relevant. We might\ntherefore let human annotators assess (a) first those documents\nthat are deemed relevant by LLMs, or (b) a subsample of documents\nfrom those considered relevant by the LLM, as an LLM can be run\nat scale. Thereby, we envision the use of LLMs to reduce annotation\ncost/time when creating high-quality IR evaluation collections.\nNoteworthy, LLMs have actual conceptual advantages over hu-\nmans when it comes to a fair and consistent judgment. They can\njudge the relevance of documents without being affected by doc-\numents they have seen before, and with no boredom or tiredness\nPerspectives on Large Language Models for Relevance Judgment April 2023, arXiv, Internet\neffects. They can also ensure to treat conceptually identical doc-\numents identically. At the same time, they will often have seen\nmuch more information on a specific topic than a human. Another\nadvantage of today‚Äôs LLMs is their inherent ability to process and\ngenerate text in many different languages. For multilingual cor-\npora (which often appear in industrial settings) the assessment is\ntypically restricted to a small subset of languages due to the lim-\nited availability of assessors. With LLMs as assessment tool, this\nlimitation no longer applies.\nLLMs are not just restricted to one input modality and thus con-\nducting assessments that require the simultaneous consideration\nof multiple pieces of content (e.g. judging a web page based on the\ntext but also the document‚Äôs structure, visual cues, embedded video\nmaterial, etc.) at the same time becomes possible. Finally, we note\nthe cost factor‚Äîif we are able to judge hundreds of thousands of\ndocuments for a relatively small price, we can build much larger\nand much more complex test collections with regularly updated\nrelevance assessments, in particular in domains that today lack\nmeaningful test collections.\nIn summary, LLMs can provide explanations, scalability, consis-\ntency, and a certain level of quality when performing relevance\njudgments, underlining the great potential of deploying them as a\ncomplement to human assessors in certain judgments task.\n7.2 Against Using LLMs for Judgments\nWhile we have given several reasons to believe that we are close\nto using LLMs for automatic relevance judgment, there are also\nseveral concerns that should be addressed by the research commu-\nnity before being able to deploy full-fledged automatic judgment.\nThe primary concern is that LLMs are not people. IR measures of\neffectiveness are ultimately grounded in a human user‚Äôs relevance\njudgment. Relevance is subjective, and changes over time for the\nsame person [57]. Even if LLMs are increasingly good at mimicking\nhuman language in evaluating contents, jumping from that up to\ntrusting the model as if it were a human is a big leap of faith. Cur-\nrently, there is no proof that the evaluation made by LLMs has any\nrelationship to reality. This raises an essential question:If the output\nfrom an LLM is indistinguishable from a human-made relevance judg-\nment, is this just a distinction without a difference? After all, people\ndisagree on relevance and change their opinions over time due to\nimplicit and explicit learning effects. Usually, however, those dis-\nagreements do not have an effect on the evaluation unless there are\nsystematic causes [8, 75]. To safely adopt LLMs to replace human\nannotators, the community should examine whether LLM-based\nrelevance judgments may in fact be systematically different from\nthose of real users. Not only do we know this affects the evaluation,\nbut the complexity (or black-box nature) of the model precludes\ndefining systematic bias in any useful way.\nThere is a general concern about solely evaluating IR research\nwith relevance assessment: Information retrieval systems are not\njust result-ranking machines, but are a system that is to assist a\nhuman to obtain information. Hence, only the user who consumes\nthe results could tell which ones are useful.\nAnother concern of applying LLMs as relevance annotators re-\ngards the ‚Äúcircularity‚Äù of the evaluation. Assume we are able to\ndevise an annotation model based on LLMs. The same model could\nideally also be used to retrieve and rank documents based on their\nexpected relevance. If the model is used to judge relevance both\nfor annotation and for retrieval, its evaluation would be overin-\nflated, possibly with perfect performance. Vice-versa, models based\non widely different rationales (such as BM25 or classical lexical\napproaches), might be penalized, because of how they estimate doc-\nument relevance. As counter-considerations, we might hypothesize\nthat the model used to label documents for relevance (a) is highly\ncomputationally expensive, making it almost unfeasible to use it as\na retrieval system, and/or (b) has access to more information and\nfacts than the retrieval model. The former holds as long as we do\nnot use the automatic annotator as an expensive re-ranker capable\nof dealing with just a few documents. The latter, on the other hand,\ndoes not solve the problem of the automatic annotation, but simply\nmoves it: Either, the additional facts and information need to be\nannotated manually; then the human annotator remains essential.\nOr, the facts can be collected automatically; then we may assume\nthat also a retrieval system could obtain them.\nOther concerns arise if we even consider generative models as\na replacement for traditional IR and search. In a plain old search\nengine, results for a query are ranked according to predicted rel-\nevance (ignoring sponsored results and advertising here). Each\nhas a clear source, and each can be inspected directly as an entity\nseparate from the search engine. Moreover, users frequently refor-\nmulate queries and try suggestions from the search engine, in a\nvirtuous cycle wherein the users fulfill or adjust their conceptual\ninformation needs. Currently, hardly any of these is possible us-\ning LLM-generated responses: The results often are not attributed,\nrarely can be explored or probed, and are often wholly generated.\nAlso, ‚Äúprompt engineering‚Äù is still explored much less and hence\nmore opaque than query reformulation. LLMs will not be usable\nfor many information needs until they can attribute sources reli-\nably and can be interrogated systematically. We expect working\nsolutions to these issues to be just a matter of time, though.\nFinally, there are significant socio-technical concerns. Generative\nAI models can be used to generate fake photos and videos, for\nextortion purposes, or for misinformation. They are perceived as\nstealing the work of others. Furthermore,LLMs are affected by bias,\nstereotypical associations [9, 54], and adverse sentiments towards\nspecific groups [46]. Critically, we cannot assess whether the LLM\nmay have seen information that biases the relevance judgment in\nan unwanted way, let alone that the company owning the LLM may\nchange it anytime without our knowledge or control. As a result,\nwe ourselves as the authors of this perspectives paper disagree on\nwhether, as a profession and considering the ACM‚Äôs Code of Ethics,\nwe should use generative models in deployed systems at all until\nthese issues are worked out.\n7.3 A Compromise: Double-checking LLMs and\nHuman‚ÄìMachine Collaboration\nOur pilot study in Sections 5 and 6 finds a reasonable correlation\nbetween highly-trained human assessors and a fully automated\nLLM, yielding similar leaderboards. This suggests that the tech-\nnology is promising and deserves further study. The experiment\ncould be implemented to double-check LLM judgments: Produce\nfully automated as well as human judgments on a shared judgment\nApril 2023, arXiv, Internet Faggioli, Dietz, Clarke, Demartini, Hagen, Hauff, Kando, Kanoulas, Potthast, Stein, and Wachsmuth\npool, then analyze correlations of labels and system rankings, then\ndecide whether LLM‚Äôs relevance judgments are good enough to be\nshared as an alternative test collection with the community. The au-\ntomatic judgment paradigm should be revealed along with prompts,\nhyperparameters, and details for reproducibility. We also suggest to\ndeclare which judgment paradigm was chosen when releasing data\nresources (such as in TREC CAR). At the very least, such automatic\njudgments could be used to evaluate early prototypes of approaches,\nfor initial judgments for novel tasks, and for large-scale training.\nWhile the discussion is easily dominated by fully automated\nevaluation‚Äîthese are merely an extreme point on our spectrum in\nSection 3. The majority of authors do not believe this constitutes the\nbest path towards credible IR research. For example, ‚ÄúAI Assistance‚Äù\nis probably the most credible path for LLMs to be incorporated\nduring evaluation. However, it is also the least explored so far.\nThis calls for more research on innovative ways to use LLMs\nfor assistance during the judgment process and how to leverage\nhumans for verifying the LLMs‚Äô suggestions. As a community, we\nshould explore how the performance of human assessors changes,\nwhen they are shown rationales or chain-of-thoughts that are gen-\nerated by LLMs. Human assessors often struggle to see a pertinent\nconnection when they are lacking world knowledge. An example\nof this issue is the task of assessing the relevance of ‚Äúdiabetes‚Äù for\nthe topic ‚Äúchild trafficking‚Äù. LLMs can generate rationales that can\nexplain such connections. However, it requires a human to realize\nwhen such a rationale was hallucinated. Only a human can assess\nwhether the information provided appears true and reliable.\n8 CONCLUSION\nIn this paper, we investigated the opportunity that large language\nmodels (LLMs) now provide to generate relevance judgments au-\ntomatically. We discussed previous attempts to automatize and\nscale-up the relevance judgment task, and we presented experimen-\ntal results showing promise in the ability to mimic human relevance\nassessments. Finally, we presented our views on why and why not\nthe research community should employ LLMs in some fashion in\nthe IR evaluation process. Undoubtedly, more research onLLMs for\nrelevance judgment is to be carried out in the future, for which this\npaper provides a starting point.\nACKNOWLEDGMENTS\nThis paper is based on discussions during a breakout group at the\nDagstuhl Seminar 23031 on ‚ÄúFrontiers of Information Access Exper-\nimentation for Research and Education‚Äù. We express our gratitude\nto the Seminar organizers, Christine Bauer, Ben Carterette, Nicola\nFerro, and Norbert Fuhr.\nCertain companies and software are identified in this paper in\norder to specify the experimental procedure adequately. Such iden-\ntification is not intended to imply recommendation or endorsement\nof any product or service, nor is it intended to imply that the soft-\nware or companies identified are necessarily the best available for\nthe purpose.\nThis material is based upon work supported by the National Sci-\nence Foundation under Grant No. 1846017. Any opinions, findings,\nand conclusions or recommendations expressed in this material are\nthose of the author(s) and do not necessarily reflect the views of\nthe National Science Foundation.\nREFERENCES\n[1] Mustafa Abualsaud, Nimesh Ghelani, Haotian Zhang, Mark D. Smucker, Gordon V.\nCormack, and Maura R. Grossman. 2018. A System for Efficient High-Recall\nRetrieval. In The 41st International ACM SIGIR Conference on Research & Develop-\nment in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 ,\nKevyn Collins-Thompson, Qiaozhu Mei, Brian D. Davison, Yiqun Liu, and Emine\nYilmaz (Eds.). ACM, 1317‚Äì1320. https://doi.org/10.1145/3209978.3210176\n[2] Omar Alonso and Ricardo Baeza-Yates. 2011. Design and Implementation of Rel-\nevance Assessments Using Crowdsourcing. In Advances in Information Retrieval -\n33rd European Conference on IR Research, ECIR 2011, Dublin, Ireland, April 18-21,\n2011. Proceedings (Lecture Notes in Computer Science, Vol. 6611) , Paul D. Clough,\nColum Foley, Cathal Gurrin, Gareth J. F. Jones, Wessel Kraaij, Hyowon Lee, and\nVanessa Murdock (Eds.). Springer, 153‚Äì164. https://doi.org/10.1007/978-3-642-\n20161-5_16\n[3] Omar Alonso and Stefano Mizzaro. 2009. Can we get rid of TREC assessors?\nUsing Mechanical Turk for relevance assessment. InProceedings of the SIGIR 2009\nWorkshop on the Future of IR Evaluation , Vol. 15. 16.\n[4] Negar Arabzadeh, Maryam Khodabakhsh, and Ebrahim Bagheri. 2021. BERT-QPP:\nContextualized Pre-trained transformers for Query Performance Prediction. In\nCIKM ‚Äô21: The 30th ACM International Conference on Information and Knowledge\nManagement, Virtual Event, Queensland, Australia, November 1 - 5, 2021 , Gianluca\nDemartini, Guido Zuccon, J. Shane Culpepper, Zi Huang, and Hanghang Tong\n(Eds.). ACM, 2857‚Äì2861. https://doi.org/10.1145/3459637.3482063\n[5] Negar Arabzadeh, Mahsa Seifikar, and Charles L. A. Clarke. 2022. Unsupervised\nQuestion Clarity Prediction through Retrieved Item Coherency. In Proceedings of\nthe 31st ACM International Conference on Information & Knowledge Management,\nAtlanta, GA, USA, October 17-21, 2022 , Mohammad Al Hasan and Li Xiong (Eds.).\nACM, 3811‚Äì3816. https://doi.org/10.1145/3511808.3557719\n[6] Sebastian Arnold, Rudolf Schneider, Philippe Cudr√©-Mauroux, Felix A. Gers, and\nAlexander L√∂ser. 2019. SECTOR: A Neural Model for Coherent Topic Segmen-\ntation and Classification. Trans. Assoc. Comput. Linguistics 7 (2019), 169‚Äì184.\nhttps://doi.org/10.1162/tacl_a_00261\n[7] Nima Asadi, Donald Metzler, Tamer Elsayed, and Jimmy Lin. 2011. Pseudo test\ncollections for learning web search ranking functions. In Proceeding of the 34th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR 2011, Beijing, China, July 25-29, 2011 , Wei-Ying Ma, Jian-Yun Nie,\nRicardo Baeza-Yates, Tat-Seng Chua, and W. Bruce Croft (Eds.). ACM, 1073‚Äì1082.\nhttps://doi.org/10.1145/2009916.2010058\n[8] Peter Bailey, Nick Craswell, Ian Soboroff, Paul Thomas, Arjen P. de Vries, and\nEmine Yilmaz. 2008. Relevance assessment: are judges exchangeable and does it\nmatter. In Proceedings of the 31st Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, SIGIR 2008, Singapore, July\n20-24, 2008 , Sung-Hyon Myaeng, Douglas W. Oard, Fabrizio Sebastiani, Tat-Seng\nChua, and Mun-Kew Leong (Eds.). ACM, 667‚Äì674. https://doi.org/10.1145/1390\n334.1390447\n[9] Christine Basta, Marta Ruiz Costa-juss√†, and Noe Casas. 2019. Evaluating the Un-\nderlying Gender Bias in Contextualized Word Embeddings. CoRR abs/1904.08783\n(2019). arXiv:1904.08783 http://arxiv.org/abs/1904.08783\n[10] Steven M. Beitzel, Eric C. Jensen, Abdur Chowdhury, and David A. Grossman.\n2003. Using titles and category names from editor-driven taxonomies for auto-\nmatic evaluation. InProceedings of the 2003 ACM CIKM International Conference on\nInformation and Knowledge Management, New Orleans, Louisiana, USA, November\n2-8, 2003 . ACM, 17‚Äì23. https://doi.org/10.1145/956863.956868\n[11] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models\nBe Too Big?. In FAccT ‚Äô21: 2021 ACM Conference on Fairness, Accountability, and\nTransparency, Virtual Event / Toronto, Canada, March 3-10, 2021 , Madeleine Clare\nElish, William Isaac, and Richard S. Zemel (Eds.). ACM, 610‚Äì623. https://doi.or\ng/10.1145/3442188.3445922\n[12] Richard Berendsen, Manos Tsagkias, Maarten de Rijke, and Edgar Meij. 2012.\nGenerating Pseudo Test Collections for Learning to Rank Scientific Articles. In\nInformation Access Evaluation. Multilinguality, Multimodality, and Visual Analytics\n- Third International Conference of the CLEF Initiative, CLEF 2012, Rome, Italy,\nSeptember 17-20, 2012. Proceedings (Lecture Notes in Computer Science, Vol. 7488) ,\nTiziana Catarci, Pamela Forner, Djoerd Hiemstra, Anselmo Pe√±as, and Giuseppe\nSantucci (Eds.). Springer, 42‚Äì53. https://doi.org/10.1007/978-3-642-33247-0_6\n[13] Roi Blanco, Harry Halpin, Daniel M. Herzig, Peter Mika, Jeffrey Pound, Henry S.\nThompson, and Duc Thanh Tran. 2011. Repeatable and reliable search system\nevaluation using crowdsourcing. In Proceeding of the 34th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, SIGIR\n2011, Beijing, China, July 25-29, 2011 , Wei-Ying Ma, Jian-Yun Nie, Ricardo Baeza-\nYates, Tat-Seng Chua, and W. Bruce Croft (Eds.). ACM, 923‚Äì932. https://doi.org/\n10.1145/2009916.2010039\n[14] Martin Braschler. 2000. CLEF 2000 - Overview of Results. In Cross-Language\nInformation Retrieval and Evaluation, Workshop of Cross-Language Evaluation\nPerspectives on Large Language Models for Relevance Judgment April 2023, arXiv, Internet\nForum, CLEF 2000, Lisbon, Portugal, September 21-22, 2000, Revised Papers (Lecture\nNotes in Computer Science, Vol. 2069) , Carol Peters (Ed.). Springer, 89‚Äì101. https:\n//doi.org/10.1007/3-540-44645-1_9\n[15] David Carmel and Elad Yom-Tov. 2010. Estimating the Query Difficulty for Infor-\nmation Retrieval. Morgan & Claypool Publishers. https://doi.org/10.2200/S00235\nED1V01Y201004ICR015\n[16] Ben Carterette, James Allan, and Ramesh K. Sitaraman. 2006. Minimal test\ncollections for retrieval evaluation. In SIGIR 2006: Proceedings of the 29th Annual\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, Seattle, Washington, USA, August 6-11, 2006 , Efthimis N. Efthimiadis,\nSusan T. Dumais, David Hawking, and Kalervo J√§rvelin (Eds.). ACM, 268‚Äì275.\nhttps://doi.org/10.1145/1148170.1148219\n[17] Xiaoyang Chen, Ben He, and Le Sun. 2022. Groupwise Query Performance Predic-\ntion with BERT. In Advances in Information Retrieval - 44th European Conference\non IR Research, ECIR 2022, Stavanger, Norway, April 10-14, 2022, Proceedings, Part II\n(Lecture Notes in Computer Science, Vol. 13186) , Matthias Hagen, Suzan Verberne,\nCraig Macdonald, Christin Seifert, Krisztian Balog, Kjetil N√∏rv√•g, and Vinay\nSetty (Eds.). Springer, 64‚Äì74. https://doi.org/10.1007/978-3-030-99739-7_8\n[18] Charles L. A. Clarke, Alexandra Vtyurina, and Mark D. Smucker. 2020. Assessing\ntop-k preferences. CoRR abs/2007.11682 (2020). arXiv:2007.11682 https://arxiv.\norg/abs/2007.11682\n[19] Cyril W Cleverdon. 1960. The Aslib Cranfield Research Project on the Compara-\ntive Efficiency of Indexing Systems. In Aslib Proceedings , Vol. 12. MCB UP Ltd,\n421‚Äì431.\n[20] Gordon V. Cormack, Christopher R. Palmer, and Charles L. A. Clarke. 1998.\nEfficient Construction of Large Test Collections. In SIGIR ‚Äô98: Proceedings of the\n21st Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, August 24-28 1998, Melbourne, Australia , W. Bruce Croft,\nAlistair Moffat, C. J. van Rijsbergen, Ross Wilkinson, and Justin Zobel (Eds.).\nACM, 282‚Äì289. https://doi.org/10.1145/290941.291009\n[21] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview\nof the TREC 2020 deep learning track. CoRR abs/2102.07662. arXiv:2102.07662\nhttps://arxiv.org/abs/2102.07662\n[22] Antonia Creswell and Murray Shanahan. 2022. Faithful Reasoning Using Large\nLanguage Models. CoRR abs/2208.14271 (2022). https://doi.org/10.48550/arXiv.2\n208.14271 arXiv:2208.14271\n[23] Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2020. TREC CAsT 2019:\nThe Conversational Assistance Track Overview. CoRR abs/2003.13624 (2020).\narXiv:2003.13624 https://arxiv.org/abs/2003.13624\n[24] Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2020. TREC CAsT\n2019: The Conversational Assistance Track Overview. CoRR abs/2003.13624.\narXiv:2003.13624 https://arxiv.org/abs/2003.13624\n[25] Bhavana Bharat Dalvi, Einat Minkov, Partha Pratim Talukdar, and William W.\nCohen. 2015. Automatic Gloss Finding for a Knowledge Base using Ontological\nConstraints. In Proceedings of the Eighth ACM International Conference on Web\nSearch and Data Mining, WSDM 2015, Shanghai, China, February 2-6, 2015 , Xueqi\nCheng, Hang Li, Evgeniy Gabrilovich, and Jie Tang (Eds.). ACM, 369‚Äì378. https:\n//doi.org/10.1145/2684822.2685288\n[26] Florian Daniel, Pavel Kucherbaev, Cinzia Cappiello, Boualem Benatallah, and\nMohammad Allahbakhsh. 2018. Quality Control in Crowdsourcing: A Survey\nof Quality Attributes, Assessment Techniques, and Assurance Actions. ACM\nComput. Surv. 51, 1 (2018), 7:1‚Äì7:40. https://doi.org/10.1145/3148148\n[27] Suchana Datta, Sean MacAvaney, Debasis Ganguly, and Derek Greene. 2022.\nA ‚ÄôPointwise-Query, Listwise-Document‚Äô Based Query Performance Prediction\nApproach. In Proceedings of 45th international ACM SIGIR conference research\ndevelopment in information retrieval . 2148‚Äî-2153. https://doi.org/10.1145/347749\n5.3531821\n[28] Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. 2021. Towards Question-\nAnswering as an Automatic Metric for Evaluating the Content Quality of a\nSummary. Transactions of the Association for Computational Linguistics 9 (2021),\n774‚Äì789. https://doi.org/10.1162/tacl_a_00397\n[29] Laura Dietz, Shubham Chatterjee, Connor Lennox, Sumanta Kashyapi, Pooja\nOza, and Ben Gamari. 2022. Wikimarks: Harvesting Relevance Benchmarks from\nWikipedia. In SIGIR ‚Äô22: The 45th International ACM SIGIR Conference on Research\nand Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 , Enrique\nAmig√≥, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and\nGabriella Kazai (Eds.). ACM, 3003‚Äì3012. https://doi.org/10.1145/3477495.3531731\n[30] Laura Dietz and Jeff Dalton. 2020. Humans Optional? Automatic Large-Scale\nTest Collections for Entity, Passage, and Entity-Passage Retrieval. Datenbank-\nSpektrum 20, 1 (2020), 17‚Äì28. https://doi.org/10.1007/s13222-020-00334-y\n[31] Matan Eyal, Tal Baumel, and Michael Elhadad. 2019. Question Answering as an\nAutomatic Evaluation Metric for News Article Summarization. In Proceedings of\nthe 2019 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers). 3938‚Äì3948.\n[32] Marco Ferrante, Nicola Ferro, and Maria Maistro. 2017. AWARE: Exploiting\nEvaluation Measures to Combine Multiple Assessors. ACM Transactions on\nInformation Systems 36, 2 (2017), 20:1‚Äì20:38. https://doi.org/10.1145/3110217\n[33] Fernando Ferraretto, Thiago Laitz, Roberto de Alencar Lotufo, and Rodrigo\nNogueira. 2023. ExaRanker: Explanation-Augmented Neural Ranker. https:\n//doi.org/10.48550/arXiv.2301.10521 arXiv:2301.10521\n[34] Frank Flemisch, David Abbink, Makoto Itoh, Marie-Pierre Pacaux-Lemoine, and\nGina We√üel. 2016. Shared control is the sharp end of cooperation: Towards a\ncommon framework of joint action, shared control and human machine coop-\neration. IFAC-PapersOnLine 49, 19 (2016), 72‚Äì77. https://doi.org/10.1016/j.if\nacol.2016.10.464 13th IFAC Symposium on Analysis, Design, and Evaluation\nofHuman-Machine Systems HMS 2016.\n[35] Norbert Fuhr. 2017. Some Common Mistakes In IR Evaluation, And How They Can\nBe Avoided. SIGIR Forum 51, 3, 32‚Äì41. https://doi.org/10.1145/3190580.3190586\n[36] Ujwal Gadiraju, Gianluca Demartini, Ricardo Kawase, and Stefan Dietze. 2019.\nCrowd Anatomy Beyond the Good and Bad: Behavioral Traces for Crowd Worker\nModeling and Pre-selection. Computer Supported Cooperative Work 28, 5 (2019),\n815‚Äì841. https://doi.org/10.1007/s10606-018-9336-y\n[37] Debasis Ganguly, Surupendu Gangopadhyay, Mandar Mitra, and Prasenjit Ma-\njumder (Eds.). 2022. FIRE ‚Äô22: Proceedings of the 14th Annual Meeting of the Forum\nfor Information Retrieval Evaluation (Kolkata, India). Association for Computing\nMachinery, New York, NY, USA.\n[38] Yeow Chong Goh, Xin Qing Cai, Walter Theseira, Giovanni Ko, and Khiam Aik\nKhor. 2020. Evaluating human versus machine learning performance in clas-\nsifying research abstracts. Scientometrics 125, 2 (2020), 1197‚Äì1212. https:\n//doi.org/10.1007/s11192-020-03614-2\n[39] Martin Halvey, Robert Villa, and Paul D. Clough. 2015. SIGIR 2014: Workshop on\nGathering Efficient Assessments of Relevance (GEAR). SIGIR Forum 49, 1 (2015),\n16‚Äì19. https://doi.org/10.1145/2795403.2795409\n[40] PA Hancock. 2013. Task partitioning effects in semi-automated human‚Äìmachine\nsystem performance. Ergonomics 56, 9 (2013), 1387‚Äì1399. https://doi.org/10.108\n0/00140139.2013.816374\n[41] Donna Harman. 1992. Overview of the First Text REtrieval Conference (TREC-\n1). NIST Special Publication, Vol. 500-207. National Institute of Standards and\nTechnology (NIST). 1‚Äì20 pages. http://trec.nist.gov/pubs/trec1/papers/01.txt\n[42] Claudia Hauff. 2010. Predicting the effectiveness of queries and retrieval systems.\nSIGIR Forum 44, 1 (2010), 88. https://doi.org/10.1145/1842890.1842906\n[43] Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan,\nand Graham Neubig. 2021. WikiAsp: A Dataset for Multi-domain Aspect-based\nSummarization. Transactions of the Association for Computational Linguistics 9\n(2021), 211‚Äì225. https://doi.org/10.1162/tacl_a_00362\n[44] Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fan-\ndrianto, Jay Han, Matthew Kelcey, and David Berthelot. 2016. WikiReading:\nA Novel Large-scale Language Understanding Task over Wikipedia. (2016).\nhttps://doi.org/10.18653/v1/p16-1145\n[45] Luyang Huang, Lingfei Wu, and Lu Wang. 2020. Knowledge Graph-Augmented\nAbstractive Summarization with Semantic-Driven Cloze Reward. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020 , Dan Jurafsky, Joyce Chai, Natalie Schluter, and\nJoel R. Tetreault (Eds.). Association for Computational Linguistics, 5094‚Äì5107.\nhttps://doi.org/10.18653/v1/2020.acl-main.457\n[46] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu\nZhong, and Stephen Denuyl. 2020. Social Biases in NLP Models as Barriers\nfor Persons with Disabilities. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , Dan\nJurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association\nfor Computational Linguistics, 5491‚Äì5501. https://doi.org/10.18653/v1/2020.acl-\nmain.487\n[47] Aaron Jaech and Mari Ostendorf. 2018. Personalized Language Model for Query\nAuto-Completion. In Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018,\nVolume 2: Short Papers , Iryna Gurevych and Yusuke Miyao (Eds.). Association for\nComputational Linguistics, 700‚Äì705. https://doi.org/10.18653/v1/P18-2111\n[48] Kalervo J√§rvelin. 2009. Explaining User Performance in Information Retrieval:\nChallenges to IR Evaluation. In Advances in Information Retrieval Theory, Sec-\nond International Conference on the Theory of Information Retrieval, ICTIR 2009,\nCambridge, UK, September 10-12, 2009, Proceedings (Lecture Notes in Computer Sci-\nence, Vol. 5766) , Leif Azzopardi, Gabriella Kazai, Stephen E. Robertson, Stefan M.\nR√ºger, Milad Shokouhi, Dawei Song, and Emine Yilmaz (Eds.). Springer, 289‚Äì296.\nhttps://doi.org/10.1007/978-3-642-04417-5_28\n[49] Gaya K. Jayasinghe, William Webber, Mark Sanderson, and J. Shane Culpepper.\n2014. Improving test collection pools with machine learning. In Proceedings of\nthe 2014 Australasian Document Computing Symposium, ADCS 2014, Melbourne,\nVIC, Australia, November 27-28, 2014 , J. Shane Culpepper, Laurence Anthony F.\nPark, and Guido Zuccon (Eds.). ACM, 2. https://doi.org/10.1145/2682862.2682864\n[50] Noriko Kando (Ed.). 1999. Proceedings of the First NTCIR Workshop on Research\nin Japanese Text Retrieval and Term Recognition . National Center for Science\nInformation Systems (NACSIS). http://research.nii.ac.jp/ntcir/workshop/Online\nProceedings/\n[51] Gjergji Kasneci, Maya Ramanath, Fabian M. Suchanek, and Gerhard Weikum.\n2008. The YAGO-NAGA approach to knowledge discovery.ACM SIGMOD Record\nApril 2023, arXiv, Internet Faggioli, Dietz, Clarke, Demartini, Hagen, Hauff, Kando, Kanoulas, Potthast, Stein, and Wachsmuth\n37, 4 (2008), 41‚Äì47. https://doi.org/10.1145/1519103.1519110\n[52] Gabriella Kazai, Jaap Kamps, and Natasa Milic-Frayling. 2013. An analysis of\nhuman factors and label accuracy in crowdsourcing relevance judgments. Infor-\nmation Retrieval 16, 2 (2013), 138‚Äì178. https://doi.org/10.1007/s10791-012-9205-0\n[53] Mostafa Keikha, Jae Hyun Park, and W. Bruce Croft. 2014. Evaluating answer\npassages using summarization measures. In The 37th International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ‚Äô14, Gold\nCoast , QLD, Australia - July 06 - 11, 2014 , Shlomo Geva, Andrew Trotman,\nPeter Bruza, Charles L. A. Clarke, and Kalervo J√§rvelin (Eds.). ACM, 963‚Äì966.\nhttps://doi.org/10.1145/2600428.2609485\n[54] Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W. Black, and Yulia Tsvetkov. 2019.\nMeasuring Bias in Contextualized Word Representations. CoRR abs/1906.07337\n(2019). arXiv:1906.07337 http://arxiv.org/abs/1906.07337\n[55] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Mari-\nanna Apidianaki, and Chris Callison-Burch. 2023. Faithful Chain-of-Thought Rea-\nsoning. CoRR abs/2301.13379 (2023). https://doi.org/10.48550/arXiv.2301.13379\narXiv:2301.13379\n[56] Eddy Maddalena, Marco Basaldella, Dario De Nart, Dante Degl‚ÄôInnocenti, Stefano\nMizzaro, and Gianluca Demartini. 2016. Crowdsourcing Relevance Assessments:\nThe Unexpected Benefits of Limiting the Time to Judge. In Proceedings of the\nFourth AAAI Conference on Human Computation and Crowdsourcing, HCOMP 2016,\n30 October - 3 November, 2016, Austin, Texas, USA , Arpita Ghosh and Matthew\nLease (Eds.). AAAI Press, 129‚Äì138. http://aaai.org/ocs/index.php/HCOMP/HCO\nMP16/paper/view/14040\n[57] Stefano Mizzaro. 1997. Relevance: The Whole History. Journal of the American\nsociety for information science 48, 9 (1997), 810‚Äì832. https://doi.org/10.1002/(SI\nCI)1097-4571(199709)48:9%3C810::AID-ASI6%3E3.0.CO;2-U\n[58] Mariana Neves and Jurica Seva. 2021. An extensive review of tools for manual\nannotation of documents. Briefings Bioinformatics 22, 1 (2021), 146‚Äì163. https:\n//doi.org/10.1093/bib/bbz130\n[59] Zahra Nouri, Henning Wachsmuth, and Gregor Engels. 2020. Mining Crowd-\nsourcing Problems from Discussion Forums of Workers. In Proceedings of the\n28th International Conference on Computational Linguistics . International Com-\nmittee on Computational Linguistics, Barcelona, Spain (Online), 6264‚Äì6276.\nhttps://doi.org/10.18653/v1/2020.coling-main.551\n[60] Virgiliu Pavlu, Shahzad Rajput, Peter B. Golbus, and Javed A. Aslam. 2012. IR\nsystem evaluation using nugget-based test collections. In Proceedings of the Fifth\nInternational Conference on Web Search and Web Data Mining, WSDM 2012, Seattle,\nWA, USA, February 8-12, 2012 , Eytan Adar, Jaime Teevan, Eugene Agichtein, and\nYoelle Maarek (Eds.). ACM, 393‚Äì402. https://doi.org/10.1145/2124295.2124343\n[61] Tetsuya Sakai. 2006. Evaluating evaluation metrics based on the bootstrap. In\nSIGIR 2006: Proceedings of the 29th Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval, Seattle, Washington, USA,\nAugust 6-11, 2006 , Efthimis N. Efthimiadis, Susan T. Dumais, David Hawking, and\nKalervo J√§rvelin (Eds.). ACM, 525‚Äì532. https://doi.org/10.1145/1148170.1148261\n[62] Tetsuya Sakai. 2020. On Fuhr‚Äôs guideline for IR evaluation. SIGIR Forum 54, 1,\n12:1‚Äì12:8. https://doi.org/10.1145/3451964.3451976\n[63] David P. Sander and Laura Dietz. 2021. EXAM: How to Evaluate Retrieve-\nand-Generate Systems for Users Who Do Not (Yet) Know What They Want.\nIn Proceedings of the Second International Conference on Design of Experimental\nSearch & Information REtrieval Systems, Padova, Italy, September 15-18, 2021 (CEUR\nWorkshop Proceedings, Vol. 2950) , Omar Alonso, Stefano Marchesin, Marc Najork,\nand Gianmaria Silvello (Eds.). CEUR-WS.org, 136‚Äì146. http://ceur-ws.org/Vol-\n2950/paper-16.pdf\n[64] Tefko Saracevic. 1995. Evaluation of Evaluation in Information Retrieval. In\nSIGIR‚Äô95, Proceedings of the 18th Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval. Seattle, Washington, USA, July\n9-13, 1995 (Special Issue of the SIGIR Forum) , Edward A. Fox, Peter Ingwersen, and\nRaya Fidel (Eds.). ACM Press, 138‚Äì146. https://doi.org/10.1145/215206.215351\n[65] Tefko Saracevic. 1996. Relevance reconsidered. In Proceedings of the second\nconference on conceptions of library and information science (CoLIS 2) . 201‚Äì218.\n[66] Linda Schamber. 1994. Relevance and information behavior. Annual review of\ninformation science and technology (ARIST) 29 (1994), 3‚Äì48.\n[67] Seungmin Seo, Donghyun Kim, Youbin Ahn, and Kyong-Ho Lee. 2022. Active\nLearning on Pre-trained Language Model with Task-Independent Triplet Loss. In\nThirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI\n2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2022 Virtual Event, February 22 - March 1, 2022 . AAAI Press, 11276‚Äì11284.\nhttps://ojs.aaai.org/index.php/AAAI/article/view/21378\n[68] Akanksha Rai Sharma and Pranav Kaushik. 2017. Literature survey of statis-\ntical, deep and reinforcement learning in natural language processing. In 2017\nInternational Conference on Computing, Communication and Automation (ICCCA) .\n350‚Äì354. https://doi.org/10.1109/CCAA.2017.8229841\n[69] Aashish Sheshadri and Matthew Lease. 2013. SQUARE: A Benchmark for Research\non Computing Crowd Consensus. In Proceedings of the First AAAI Conference\non Human Computation and Crowdsourcing, HCOMP 2013, November 7-9, 2013,\nPalm Springs, CA, USA , Bj√∂rn Hartman and Eric Horvitz (Eds.). AAAI. http:\n//www.aaai.org/ocs/index.php/HCOMP/HCOMP13/paper/view/7550\n[70] Ian Soboroff. 2021. Overview of TREC 2021. In 30th Text REtrieval Conference .\nGaithersburg, Maryland. https://trec.nist.gov/pubs/trec30/papers/Overview-\n2021.pdf\n[71] Ian Soboroff, Charles K. Nicholas, and Patrick Cahan. 2001. Ranking Retrieval\nSystems without Relevance Judgments. In SIGIR 2001: Proceedings of the 24th\nAnnual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, September 9-13, 2001, New Orleans, Louisiana, USA , W. Bruce\nCroft, David J. Harper, Donald H. Kraft, and Justin Zobel (Eds.). ACM, 66‚Äì73.\nhttps://doi.org/10.1145/383952.383961\n[72] Taylor Sorensen, Joshua Robinson, Christopher Michael Rytting, Alexander Glenn\nShaw, Kyle Jeffrey Rogers, Alexia Pauline Delorey, Mahmoud Khalil, Nancy\nFulda, and David Wingate. 2022. An Information-theoretic Approach to Prompt\nEngineering Without Ground Truth Labels. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers),\nACL 2022, Dublin, Ireland, May 22-27, 2022 , Smaranda Muresan, Preslav Nakov,\nand Aline Villavicencio (Eds.). Association for Computational Linguistics, 819‚Äì\n862. https://doi.org/10.18653/v1/2022.acl-long.60\n[73] Pontus Stenetorp, Sampo Pyysalo, Goran Topic, Tomoko Ohta, Sophia Ananiadou,\nand Jun‚Äôichi Tsujii. 2012. brat: a Web-based Tool for NLP-Assisted Text Annota-\ntion. In EACL 2012, 13th Conference of the European Chapter of the Association\nfor Computational Linguistics, Avignon, France, April 23-27, 2012 , Walter Daele-\nmans, Mirella Lapata, and Llu√≠s M√†rquez (Eds.). The Association for Computer\nLinguistics, 102‚Äì107. https://aclanthology.org/E12-2021/\n[74] Lynda Tamine and Cecile Chouquet. 2017. On the impact of domain expertise on\nquery formulation, relevance assessment and retrieval performance in clinical\nsettings. Information Processing & Management 53, 2 (2017), 332‚Äì350. https:\n//doi.org/10.1016/j.ipm.2016.11.004\n[75] Ellen M. Voorhees. 2000. Variations in relevance judgments and the measurement\nof retrieval effectiveness. Inf. Process. Manag. 36, 5 (2000), 697‚Äì716. https:\n//doi.org/10.1016/S0306-4573(00)00010-8\n[76] Ellen M. Voorhees and Donna Harman. 1999. Overview of the Eighth Text\nREtrieval Conference (TREC-8). In Proceedings of The Eighth Text REtrieval\nConference, TREC 1999, Gaithersburg, Maryland, USA, November 17-19, 1999\n(NIST Special Publication, Vol. 500-246) , Ellen M. Voorhees and Donna K. Har-\nman (Eds.). National Institute of Standards and Technology (NIST). http:\n//trec.nist.gov/pubs/trec8/papers/overview_8.ps\n[77] Christian Weismayer, Ilona Pezenka, and Christopher Han-Kie Gan. 2018. Aspect-\nBased Sentiment Detection: Comparing Human Versus Automated Classifica-\ntions of TripAdvisor Reviews. In Information and Communication Technologies in\nTourism 2018, ENTER 2018, Proceedings of the International Conference in J√∂nk√∂ping,\nSweden, January 24-26, 2018 , Brigitte Stangl and Juho Pesonen (Eds.). Springer,\n365‚Äì380. https://doi.org/10.1007/978-3-319-72923-7_28\n[78] Charles Welch, Chenxi Gu, Jonathan K. Kummerfeld, Ver√≥nica P√©rez-Rosas,\nand Rada Mihalcea. 2022. Leveraging Similar Users for Personalized Language\nModeling with Limited Data. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022 , Smaranda Muresan, Preslav Nakov, and Aline\nVillavicencio (Eds.). Association for Computational Linguistics, 1742‚Äì1752. https:\n//doi.org/10.18653/v1/2022.acl-long.122\n[79] Jennifer Windsor, Laura M Pich√©, and Peggy A Locke. 1994. Preference testing: A\ncomparison of two presentation methods. Research in developmental disabilities\n15, 6 (1994), 439‚Äì455. https://doi.org/10.1016/0891-4222(94)90028-0\n[80] Jiechen Xu, Lei Han, Shazia Sadiq, and Gianluca Demartini. 2023. On the role of\nhuman and machine metadata in relevance judgment tasks.Information Processing\n& Management 60, 2 (2023), 103177. https://doi.org/10.1016/j.ipm.2022.103177\n[81] Ziying Yang, Alistair Moffat, and Andrew Turpin. 2018. Pairwise Crowd Judg-\nments: Preference, Absolute, and Ratio. In Proceedings of the 23rd Australasian\nDocument Computing Symposium, ADCS 2018, Dunedin, New Zealand, December\n11-12, 2018 . ACM, 3:1‚Äì3:8. https://doi.org/10.1145/3291992.3291995\n[82] Emine Yilmaz, Evangelos Kanoulas, and Javed A. Aslam. 2008. A simple and\nefficient sampling method for estimating AP and NDCG. In Proceedings of the\n31st Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR 2008, Singapore, July 20-24, 2008 , Sung-Hyon Myaeng,\nDouglas W. Oard, Fabrizio Sebastiani, Tat-Seng Chua, and Mun-Kew Leong (Eds.).\nACM, 603‚Äì610. https://doi.org/10.1145/1390334.1390437\n[83] Seid Muhie Yimam, Iryna Gurevych, Richard Eckart de Castilho, and Chris Bie-\nmann. 2013. WebAnno: A Flexible, Web-based and Visually Supported System for\nDistributed Annotations. In 51st Annual Meeting of the Association for Computa-\ntional Linguistics, ACL 2013, Proceedings of the Conference System Demonstrations,\n4-9 August 2013, Sofia, Bulgaria . The Association for Computer Linguistics, 1‚Äì6.\nhttps://aclanthology.org/P13-4001/\n[84] Seunghyun Yoon, Hyeongu Yun, Yuna Kim, Gyu-tae Park, and Kyomin Jung.\n2017. Efficient Transfer Learning Schemes for Personalized Language Modeling\nusing Recurrent Neural Network. In The Workshops of the The Thirty-First AAAI\nConference on Artificial Intelligence, Saturday, February 4-9, 2017, San Francisco,\nCalifornia, USA (AAAI Technical Report, Vol. WS-17) . AAAI Press. http://aaai.org\n/ocs/index.php/WS/AAAIW17/paper/view/15144\nPerspectives on Large Language Models for Relevance Judgment April 2023, arXiv, Internet\n[85] Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, Jae Sung Park, Ximing Lu,\nPrithviraj Ammanabrolu, Rowan Zellers, Ronan Le Bras, Gunhee Kim, and Yejin\nChoi. 2022. Multimodal Knowledge Alignment with Reinforcement Learning.\nCoRR abs/2205.12630 (2022). https://doi.org/10.48550/arXiv.2205.12630\narXiv:2205.12630\n[86] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.\n2020. BERTScore: Evaluating Text Generation with BERT. In 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=SkeHuCVFDr\n[87] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,\nHarris Chan, and Jimmy Ba. 2022. Large Language Models Are Human-Level\nPrompt Engineers. CoRR abs/2211.01910 (2022). https://doi.org/10.48550/arXiv.2\n211.01910 arXiv:2211.01910",
  "topic": "Relevance (law)",
  "concepts": [
    {
      "name": "Relevance (law)",
      "score": 0.9180439114570618
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.656033992767334
    },
    {
      "name": "Categorization",
      "score": 0.6195998787879944
    },
    {
      "name": "Point (geometry)",
      "score": 0.5008764266967773
    },
    {
      "name": "Compromise",
      "score": 0.46971338987350464
    },
    {
      "name": "Psychology",
      "score": 0.4631803333759308
    },
    {
      "name": "Cognitive psychology",
      "score": 0.43293696641921997
    },
    {
      "name": "Computer science",
      "score": 0.40663716197013855
    },
    {
      "name": "Artificial intelligence",
      "score": 0.21017757058143616
    },
    {
      "name": "Sociology",
      "score": 0.12734603881835938
    },
    {
      "name": "Political science",
      "score": 0.11648231744766235
    },
    {
      "name": "Social science",
      "score": 0.07866746187210083
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I138689650",
      "name": "University of Padua",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I179093154",
      "name": "University of New Hampshire at Manchester",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I165143802",
      "name": "The University of Queensland",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I76198965",
      "name": "Friedrich Schiller University Jena",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I184597095",
      "name": "National Institute of Informatics",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I926574661",
      "name": "Leipzig University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I51441396",
      "name": "Bauhaus-Universit√§t Weimar",
      "country": "DE"
    }
  ],
  "cited_by": 8
}