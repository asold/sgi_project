{
  "title": "Evolving Game Skill-Depth using General Video Game AI Agents",
  "url": "https://openalex.org/W2950893049",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100456636",
      "name": "Jialin Liu",
      "affiliations": [
        "University of Essex"
      ]
    },
    {
      "id": "https://openalex.org/A5077267552",
      "name": "Julian Togelius",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5058274276",
      "name": "Diego Pérez-Liébana",
      "affiliations": [
        "University of Essex"
      ]
    },
    {
      "id": "https://openalex.org/A5062380176",
      "name": "Simon M. Lucas",
      "affiliations": [
        "University of Essex"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2160237541",
    "https://openalex.org/W2170567160",
    "https://openalex.org/W1526339308",
    "https://openalex.org/W2782805716",
    "https://openalex.org/W2168115594",
    "https://openalex.org/W2463247845",
    "https://openalex.org/W2964262220",
    "https://openalex.org/W2963708837",
    "https://openalex.org/W2051967127",
    "https://openalex.org/W788042400",
    "https://openalex.org/W1765127455",
    "https://openalex.org/W2111149155",
    "https://openalex.org/W2516092374",
    "https://openalex.org/W1605527696",
    "https://openalex.org/W2144230296",
    "https://openalex.org/W2139165539"
  ],
  "abstract": "Most games have, or can be generalised to have, a number of parameters that may be varied in order to provide instances of games that lead to very different player experiences. The space of possible parameter settings can be seen as a search space, and we can therefore use a Random Mutation Hill Climbing algorithm or other search methods to find the parameter settings that induce the best games. One of the hardest parts of this approach is defining a suitable fitness function. In this paper we explore the possibility of using one of a growing set of General Video Game AI agents to perform automatic play-testing. This enables a very general approach to game evaluation based on estimating the skill-depth of a game. Agent-based play-testing is computationally expensive, so we compare two simple but efficient optimisation algorithms: the Random Mutation Hill-Climber and the Multi-Armed Bandit Random Mutation Hill-Climber. For the test game we use a space-battle game in order to provide a suitable balance between simulation speed and potential skill-depth. Results show that both algorithms are able to rapidly evolve game versions with significant skill-depth, but that choosing a suitable resampling number is essential in order to combat the effects of noise.",
  "full_text": "Evolving Game Skill-Depth using General Video\nGame AI Agents\nJialin Liu\nUniversity of Essex\nColchester, UK\njialin.liu@essex.ac.uk\nJulian Togelius\nNew York University\nNew York City, US\njulian.togelius@nyu.edu\nDiego P ´erez-Li´ebana\nUniversity of Essex\nColchester, UK\ndperez@essex.ac.uk\nSimon M. Lucas\nUniversity of Essex\nColchester, UK\nsml@essex.ac.uk\nAbstract—Most games have, or can be generalised to have, a\nnumber of parameters that may be varied in order to provide\ninstances of games that lead to very different player experiences.\nThe space of possible parameter settings can be seen as a\nsearch space, and we can therefore use a Random Mutation\nHill Climbing algorithm or other search methods to ﬁnd the\nparameter settings that induce the best games. One of the hardest\nparts of this approach is deﬁning a suitable ﬁtness function. In\nthis paper we explore the possibility of using one of a growing\nset of General Video Game AI agents to perform automatic play-\ntesting. This enables a very general approach to game evaluation\nbased on estimating the skill-depth of a game. Agent-based play-\ntesting is computationally expensive, so we compare two simple\nbut efﬁcient optimisation algorithms: the Random Mutation Hill-\nClimber and the Multi-Armed Bandit Random Mutation Hill-\nClimber. For the test game we use a space-battle game in order\nto provide a suitable balance between simulation speed and\npotential skill-depth. Results show that both algorithms are able\nto rapidly evolve game versions with signiﬁcant skill-depth, but\nthat choosing a suitable resampling number is essential in order\nto combat the effects of noise.\nIndex Terms—Automatic game design, game tuning, optimisa-\ntion, RMHC, GVG-AI\nI. I NTRODUCTION\nDesigning games is an interesting and challenging discipline\ntraditionally demanding creativity and insight into the types of\nexperience which will cause players to enjoy the game or at\nleast play it and replay it. There have been various attempts\nto automate or part-automate the game generation process,\nas this is an interesting challenge for AI and computational\ncreativity [1], [2], [3]. So far the quality of the generated games\n(with some exceptions) do not challenge the skill of human\ngame designers. This is because the generation of complete\ngames is a more challenging task than the more constrained\ntask of generating game content such as levels or maps. Many\nvideo games require content to be produced for them, and\nrecent years have seen a surge in AI-based procedural content\ngeneration [4].\nThere is another aspect of AI-assisted game design which\nwe believe is hugely under-explored: automatic game tuning.\nThis involves taking an existing game (either human-designed\nor auto-generated) and performing a comprehensive explo-\nration of the parameter space to ﬁnd the most interesting game\ninstances.\nRecent work has demonstrated the potential of this ap-\nproach, automatically generating distinct and novel variants\nof the minimalist mobile game Flappy Bird [5]. That work\ninvolved using a very simple agent to play through each\ngenerated game instance. Noise was added to the selected\nactions, and a game variant was deemed to have an appropriate\nlevel of difﬁculty if a speciﬁed number of players achieved a\ndesired score. For Flappy Bird it is straightforward to design\nan AI agent capable of near-optimal play. Adding noise to the\nselected actions of this player can be used to provide a less\nthan perfect agent that better represents human reactions. An\nevolutionary algorithm was used to search for game variants\nthat were as far apart from each other in parameter space as\npossible but were still playable.\nHowever, for more complex games it is harder to provide\na good AI agent, and writing a new game playing agent for\neach new game would make the process more time consuming.\nFurthermore, a single hand-crafted agent may be blind to novel\naspects of evolved game-play elements that the designer of the\nAI agent had not considered. This could severely inhibit the\nutility of the approach. In this work we mitigate these concerns\nby tapping in to an ever-growing pool of agents designed for\nthe General Video Game AI (GVG-AI) competition1. The idea\nis that using a rich set of general agents will provide the basis\nfor a robust evaluation process with a higher likelihood of\nﬁnding skill-depth wherever it may lie in the chosen search\nspace of possible games. In this paper we use one of the\nsample GVG-AI agents, varying it by changing the rollout\nbudget. This was done by making the game implement a\nstandard GVG-AI game interface, so that any GVG-AI agent\ncan be used with very little effort, allowing the full set of\nagents to be used in future experiments.\nLiu et al. [6] introduced a two-player space-battle game,\nderived from the original Spacewar, and performed a study on\ndifferent parameter settings to bring out some strengths and\nweaknesses of the various algorithms under test. A key ﬁnding\nis that the rankings of the algorithms depend very much on\nthe details of the game. A mutation of one parameter may lead\nto a totally different ranking of algorithms. If the game using\nonly a single parameter setting is tested, the conclusions could\nbe less robust and misleading.\n1http://www.gvgai.net/\n978-1-5090-4601-0/17/$31.00 c⃝2017 IEEE\narXiv:1703.06275v1  [cs.AI]  18 Mar 2017\nIn this paper, we adapt the space-battle game introduced by\nLiu et al. [6] to the GVG-AI framework, then uses the Random\nMutation Hill Climber (RMHC) and Multi-Armed Bandit\nRMHC (MABRMHC) to evolve game parameters to provide\nsome game instances that lead to high winning rates for GVG-\nAI sample MCTS agents. This is used as an approximate\nmeasure of skill-depth, the idea being that the smarter MCTS\nagents should beat unintelligent agents, or that MCTS agents\nwith a high rollout budget should beat those with a low rollout\nbudget.\nThe paper is structured as follows: Section II provides a\nbrief review of the related work on automatic game design,\nSection III describes the game engine, Section IV introduces\nthe two optimisation algorithms used in this paper, Section V\npresents the experimental results, ﬁnally Section VI concludes\nand discusses the potential directions in the future.\nII. A UTOMATIC GAME DESIGN AND DEPTH ESTIMATION\nAttempts to automatically design complete games go back\nto Barney Pell, who generated rules for chess-like games [7].\nIt did not however become an active research topic until the\nlate 2000’s.\nTogelius et al. [8] evolved racing tracks in a car racing\ngame using a simple multi-objective evolutionary algorithm\ncalled Cascading Elitism . The ﬁtness functions attempted to\ncapture various aspects of player experience, using a neural\nnetwork model of the player. This can be seen as an early\nform of experience-driven procedural content generation [9],\nwhere game content is generated through search in content\nspace using evolutionary computation or some other form\nof stochastic optimisation. Similar methods have since been\nused to generate many types of game content, such as particle\nsystems for weapons in a space shooter [10], platform game\nlevels [11] or puzzles [12]. In most of these cases, the ﬁtness\nfunctions measure some aspect of problem difﬁculty, with the\nassumption that good game content should not make the game\ntoo hard nor too easy.\nWhile the research discussed above focuses on generating\ncontent for an existing game, there have been several attempts\nto use the search-based methods to generate new games by\nsearching though spaces of game rules. Togelius and Schmid-\nhuber [1] used a simple hill-climber to generate single-player\nPac-Man-like games given a restricted rule search space.\nThe ﬁtness function was based on learnability of the game,\noperationalised as the capacity of another machine learning\nalgorithm to learn to play the game.\nThis approach was taken further by Cook et al. [13], [3],\nwho used search-based methods to design rulesets, maps and\nobject layouts in tandem for producing simple arcade games\nvia a system called ANGELINA. Further iterations of this\nsystem include the automatic selection of media sources, such\nas images and resources, giving this work a unique ﬂavour.\nIn a similar vein, Browne and Maire [2] developed a system\nfor automatic generation of board games; they also used\nevolutionary algorithms, and a complex ﬁtness function based\non data gathered from dozens of humans playing different\nboard games. Browne’s work is perhaps the only to result in a\ngame of sufﬁcient quality to be sold as a stand-alone product;\nthis is partly a result of working in a constrained space of\nsimple board games.\nA very different approach to game generation was taken by\nNelson and Mateas [14], who use reasoning methods to create\nWario Ware-style minigames out of verb-noun relations and\ncommon minigame design patterns. Conceptnet and Wordnet\nwere used to ﬁnd suitable roles for game objects.\nQuite recently, some authors have used search-based meth-\nods to optimise the parameters of a single game, while keeping\nboth game rules and other parts of the game content constant.\nIn the introduction we discussed the work of Isaksen et al.\non generating playable Flappy Bird variants [5]. Similarly,\nPowley et al. [15] optimise the parameters of an abstract\ntouch-based mobile game, showing that parameter changes to a\nsingle ruleset can give rise to what feels and plays like different\ngames.\nOne of the more important properties of a game can be said\nto be its skill depth , often just called depth. This property is\nuniversally considered desirable by game designers, yet it is\nhard to deﬁne properly; some of the deﬁnitions build on the\nidea of a skill chain, where deeper games simply have more\nthings that can be learned [16]. Various attempts have been\nmade to algorithmically estimate depth and use it as a ﬁtness\nfunction; some of the research discussed above can be said to\nembody an implicit notion of depth in their ﬁtness functions.\nRelative Algorithm Performance Proﬁles (RAPP) is a more\nexplicit attempt at game depth estimation; the basic idea is that\nin a deeper game, a better player get relatively better result\nthan a poorer player. Therefore, we can use game-playing\nagents of different strengths to play the same game, and the\nbigger the difference in outcome the greater the depth [17].\nIn this paper we use a form of RAPP to try to estimate\nthe depth of variants of a simple two-player game. Using this\nmeasure as a ﬁtness function, we optimise the parameters of\nthis game to try to ﬁnd deeper game variants, using two types\nof Random-Mutation Hill-Climber. The current work differs\nfrom the work discussed above both in the type of game used\n(two-player physics-based game), the search space (a multi-\ndimensional discrete space) and the optimisation method. In\nparticular, compared to previous work by Isaksen et al, the\ncurrent paper investigates a more complex game and uses a\nsigniﬁcantly more advanced agent, and also optimizes for skill-\ndepth rather than difﬁculty. This work is, as far as we know, the\nﬁrst attempt to optimize skill-depth that has had good results.\nIII. F RAMEWORK\nWe adapt the two-player space-battle game introduced by\nLiu et al. [6] to the GVG-AI framework, then use RMHC and\nMABRMHC to evolve game parameters to provide some game\ninstances that lead to high winning rate for GVG-AI sample\nMCTS agents. The main difference in the modiﬁed space-\nbattle game used in this work is the introduction of weapon\nsystem. Each ship has the choice to ﬁre a missile after its\ncooldown period has ﬁnished. From now on, we use the term\n“game” to refer to a game instance, i.e. a speciﬁc conﬁguration\nof game parameters.\na) Spaceship: Each player/agent controllers a spaceship\nwhich has a maximal speed, vs units distance per game\ntick, and slows down over time. At each game tick, the\nplayer can choose to do nothing or to make an action among\n{RotateClockwise, RotateAnticlockwise, Thrust, Shoot }. A\nmissile is launched while the Shoot action is chosen and its\ncooldown period is ﬁnished, otherwise, no action will be taken\n(like do nothing). The spaceship is affected by a random recoil\nforce when launching a missile.\nb) Missile: A missile has a maximal speed, vm units\ndistance per game tick, and vanishes into nothing after 30\ngame tick. It never damages its mother ship.\nEvery spaceship has a radius of 20 pixels and every missile\nhas a radius of 4 pixels in a layout of size 640*480.\nc) Score: Every time a player hits its opponent, it obtains\n100 points (reward). Every time a player launches a missile,\nit is penalized by c points (cost). Given a game state s, the\nplayer i ∈{1, 2}has a score calculated by:\nscore(i) = 100 ×nbk(i) −c ×nbm(i), (1)\nwhere nbk(i) is the number of lives subtracted from the op-\nponent and nbm(i) indicates the number of launched missiles\nby player i ∈{1, 2}.\nd) End condition: A game ends after 500 game ticks. A\nplayer wins the game if it has higher score than its opponent\nafter 500 game ticks, and it’s a loss of the other player. If both\nplayers have the same score, it’s a draw.\ne) Parameter space: The parameters to be optimised are\ndetailed in Table I. There are in total 14,400 possible games in\nthe 5-dimensional search space. Fig. 1 illustrates brieﬂy how\nthe game changes by varying only the cooldown time for ﬁring\nmissiles.\nTABLE I\nGAME PARAMETERS . ONLY THE FIRST 5 PARAMETERS ARE OPTIMISED IN\nTHE PRIMARY EXPERIMENTS . THE LAST ONE (SHIP RADIUS ) IS TAKEN\nINTO ACCOUNT IN SECTION V-C.\nParameter Notation Legal values Dimension\nMaximal ship speed vs 4, 6, 8, 10 4\nThrust speed vt 1, 2, 3, 4, 5 5\nMaximal missile speed vm 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 10\nCooldown time d 1, 2, 3, 4, 5, 6, 7, 8, 9 9\nMissile cost c 0, 1, 5, 10, 20, 50, 75, 100 8\nShip radius sr 10, 20, 30, 40, 50 5\nThe game is stochastic but fully observable. Each game\nstarts with the agents in the symmetric positions. The two\nagents make simultaneous moves and in a fair situation. Thus,\nchanging the player id does not change the situation of any\nplayer.\nIV. O PTIMISERS\nWe compare a Random Mutation Hill-Climber to an Multi-\nArmed Bandit Random Mutation Hill-Climber in evolving\ninstances for space-battle game described previously. This\nsection is organised as follows. Section IV-A brieﬂy recalls\nFig. 1. .Space-battle game with high (left) and low (right) missile cooldown\ntime while ﬁxing the other game parameters. It is more difﬁcult to approach\nto RAS in the latter case.\nthe Random Mutation Hill-Climber. Section IV-B presents the\nMulti-Armed Bandit Random Mutation Hill-Climber and its\nselection and mutation rules.\nA. Random Mutation Hill-Climber\nThe Random Mutation Hill-Climber (RMHC) is a simple\nbut efﬁcient derivative-free optimisation method mostly used\nin discrete domains [18], [19]. The pseudo-code of RMHC\nis given in Algorithm 1. At each generation, an offspring is\ngenerated based on the only best-so-far genome (parent) by\nmutating exactly one uniformly randomly chosen gene. The\nbest-so-far genome is updated if the offspring’s ﬁtness value\nis better or equivalent to the best-so-far.\nB. Multi-Armed Bandit Random Mutation Hill-Climber\nMulti-Armed Bandit Random Mutation Hill-Climber\n(MABRMHC), derived from the 2-armed bandit-based RMHC\n[20], [21], uses both UCB-style selection and mutation rules.\nMABRMHC selects the coordinate (bandit) with the maximal\nurgency (Equation 2) to mutate, then mutates the parameter\nin dimension d to the value (arm) which leads to the maximal\nreward (Equation 3).\nAlgorithm 1 Random Mutation Hill-Climber (RMHC).\nRequire: X: search space\nRequire: D = |X|: problem dimension (genome length)\nRequire: f : X↦→ [0, 1]: ﬁtness function\n1: Randomly initialise a genome x ∈X\n2: bestFitSoFar ←0\n3: M ←0 ⊿ Counter for the latest best-so-far genome\n4: N ←0 ⊿ Total evaluation count so far\n5: while time not elapsed do\n6: Uniformly randomly select d ∈{1, . . . , D}\n7: y ←new genome by uniformly randomly mutating\nthe dth gene of x\n8: Fitx ←fitness(x)\n9: Fity ←fitness(y)\n10: averageFitness x ←bestFitSoFar ×M+Fitx\nM+1\n11: N ←N + 2 ⊿ Update evaluation count\n12: if Fity ≥averageFitness x then\n13: x ←y ⊿ Replace the best-so-far genome\n14: bestFitSoFar ←Fity\n15: M ←1\n16: else\n17: bestFitSoFar ←averageFitness x\n18: M ←M + 1\n19: end if\n20: end while\n21: return x\nFor any multi-armed bandit d ∈{1, 2, . . . , D}, its urgencyd\nis deﬁned as\nurgencyd =\nmin\n1≤j≤Dim(d)\n\n∆d(j) +\n√\n2 log(∑Dim(d)\nk=1 Nd(k))\nNd\n+ ω\n\n,\n(2)\nwhere Nd(k) is the number of times the kth value is selected\nwhen the dth coordinate is selected; Nd is the number of\ntimes the dth coordinate is selected to mutate, thus Nd =∑Dim(d)\nk=1 Nd(k); ∆d(k) is the maximal difference between\nthe ﬁtness values if the value k is mutated to when the dth\ndimension is selected, i.e., the changing of ﬁtness value; ω\ndenotes a uniformly distributed value between 0 and 1e−6\nwhich is used to randomly break ties. Once the coordinate to\nmutate (eg. d∗) is selected, the index of the value to mutated\nto is determined by\nk∗ = argmax\n1≤k≤Dim(d∗)\n(\n¯∆d∗ (k) +\n√\n2 log(Nd∗ )\nNd∗ (k) + ω\n)\n, (3)\nwhere ¯∆d∗ (k) denotes the average changing of ﬁtness value if\nthe value k is mutated to when the dimension d∗ is selected.\nThe pseudo-code of MABRMHC is given in Algorithm 2.\nIn this work, we model each of the game parameter to\noptimise as a bandit, and the legal values for the parameter\nas the arms of this bandit. The search space is folded in the\nsense that it takes far less computational cost to mutate and\nAlgorithm 2 Multi-Armed Bandit Random Mutation Hill-\nClimber (MABRMHC). Dim(d) returns the number of possi-\nble values in dimension d. ω denotes a uniformly distributed\nvalue between 0 and 1e−6 which is used to randomly break\nties.\nRequire: X: search space\nRequire: D = |X|: problem dimension (genome length)\nRequire: f : X↦→ [0, 1]: ﬁtness function\n1: Randomly initialise a genome x ∈X\n2: bestFitSoFar ←0\n3: M ←0 ⊿ Counter for the latest best-so-far genome\n4: N ←0 ⊿ Total evaluation count so far\n5: for d ∈{1, . . . , D}do\n6: Nd = 0\n7: for k ∈{1, . . . , Dim(d)}do\n8: Nd(k) = 0, ∆d(k) = 0, ¯∆d(k) = 0\n9: end for\n10: end for\n11: while time not elapsed do\n12: d∗ = argmax\n1≤d≤D\n(\nmin\n1≤j≤Dim(d)\n∆d(j) +\n√\n2 log(∑Dim(d)\nk=1 Nd(k))\nNd\n+ ω\n)\n⊿ Select the coordinate to mutate (Equation 2)\n13: k∗ = argmax\n1≤k≤Dim(d∗)\n(\n¯∆d∗ (k) +\n√\n2 log(Nd∗ )\nNd∗ (k) + ω\n)\n⊿\nSelect the index of value to take (Equation 3)\n14: y ←after mutating the element d∗ of x to the k∗ legal\nvalue\n15: Fitx ←fitness(x)\n16: Fity ←fitness(y)\n17: averageFitness ←bestFitSoFar ×M+Fitx\nM+1\n18: N ←N + 2 ⊿ Update the counter\n19: ∆ = Fity −averageFitness\n20: Update ∆d∗ (k∗) and ¯∆d∗ (k∗) ⊿ Update the statistic\n21: Nd(k) ←Nd(k) + 1, Nd ←Nd + 1 ⊿ Update the\ncounters\n22: if ∆ ≥0 then\n23: x ←y ⊿ Replace the best-so-far genome\n24: bestFitSoFar ←Fity\n25: M ←1\n26: else\n27: bestFitSoFar ←averageFitness\n28: M ←M + 1\n29: end if\n30: end while\n31: return x\nevaluate every legal value of each parameter once than to\nevaluate mutate and evaluate every legal game instance once.\nV. E XPERIMENTAL RESULTS\nWe ﬁrstly use the sample agent using a two-player Open-\nLoop Monte-Carlo Tree Search algorithm provided by the\nGVG-AI framework, which uses the difference of scores (Eq.\n1) of both players as its heuristic (denoted as OLMCTS), as\nplayer 1. No modiﬁcation or tuning has been performed on\nthis sample agent. We implement a consistently rotate-and-\nshoot agent (denoted as RAS) as the player 2. More precisely,\nthe RAS is a deterministic agent and, by Eq. 1, the OLM-\nCTS aims at maximising (100 ×nbk(1) −c ×nbm(1)) −\n(100 ×nbk(2) −c ×nbm(2)), where nbk(1) and nbk(2) are\nthe numbers of lives subtracted from the RAS and OLMCTS,\nrespectively; nbm(1) and nbm(2) indicates the number of\nlaunched missiles by OLMCTS and RAS, respectively. Again,\nthis heuristic is already deﬁned in the sample agent, not by\nus. Basically, a human player could probably choose to play\nthe game in a passive way by avoiding the missiles and not\nﬁring at all, and ﬁnally win the game.\nThe landscape of winning rate of OLMCTS against RAS is\nstudied in Section V-A. Section V-B presents the performance\nof RMHC and MABRMHC with different resampling numbers\nto generate games in the parameter space detailed previously\n(Section III-0e) and Section V-C presents their performances\nin a 5 times larger parameter space.\nA. Winning rate distribution\nWe use a OLMCTS agent as the player 1 and a RAS agent\nas the player 2. At each game tick, 10ms is allocated to each\nof the agents to decide an action. The average number of\niterations performed by OLMCTS is 350. The time to return\nan action for RAS is negligible.\nThe average winning rates over 11 and 69 repeated trials\nof all the 14,400 legal game instances played by OLMCTS\nagainst RAS are shown in Fig. 2. The winning rate over 69\ntrials of each games instance varies between 20% and 100%.\nAmong all the legal game instances, the OLMCTS does not\nachieve a 100% winning rate in more than 5,000 games.\nFig. 3 demonstrates how the winning rate varies along with\nthe changing of each parameter. The maximal ship speed and\nthe thrust speed have negligible inﬂuence on the OLMCTS’s\naverage winning rate. Higher the maximal missile speed is\nor shorter the cooldown time is, higher the average winning\nrate is. But still, the average winning rate remains above 87%.\nThe most important factor is the cost of ﬁring a missile. It is\nnot surprising, since the RAS ﬁres successively missiles and\nthe number of missiles it ﬁres during each game is constant\ndepending on the cooldown time. the OLMCTS only ﬁres\nwhile necessary or it is likely to slash its opponent.\nB. Evolving games by RMHC and MABRMHC using different\nresampling numbers\nWe use the same agents as described in Section V-A. RMHC\n(Algorithm 1) and MABRMHC (Algorithm 2) are applied to\nFig. 2. Empirical winning rates for OLMCTS sorted in increasing order, over\n11 trials (left) and 69 trials (right), of all the 14,400 legal game instances\nplayed by OLMCTS against RAS. The standard error is shown by the shaded\nboundary.\noptimise the parameters of the space-battle game, aiming at\nmaximising the win probability for the OLMCTS against the\nRAS. Since the true win probability is unknown, we need\nto deﬁne the ﬁtness of a game using some winning rate by\nrepeating the same game several times, i.e., resampling the\ngame. We deﬁne the ﬁtness value of a game g as the winning\nrate over r repeated games, i.e.,\nfitness(g) = 1\nr\nr∑\ni=1\nGameV alue(g). (4)\nThe value of game g is deﬁned as\nGameV alue(g) =\n\n\n\n1, if OLMCTS wins\n0, if RAS wins\n0.5, otherwise (a draw).\nA call to fitness(·) is actually based on independent r\nrealizations of the same game. Due to the internal stochastic\neffects in the game, each realization may return a different\nFig. 3. Empirical winning rates over 69 trials of all the 14,400 legal game instances played by OLMCTS against RAS, classiﬁed by the maximal ship\nspeed, the thrust speed, the maximal missile speed, the cooldown time and the cost of ﬁring a missile, respectively. The standard error is shown by the shaded\nboundary.\ngame value. We aim at maximising the ﬁtness f in this work.\nThe empirical winning rates shown in Fig. 2 are two example\nfitness(·) with r = 11 (left) and r = 69 (right). The strength\nof noise decreases while repeating the same game more times,\ni.e., increasing r.\nA recent work applied the RMHC and a two-armed bandit-\nbased RMHC with resamplings to a noisy variant of the One-\nMax problem, and showed both theoretically and practically\nthe importance of choosing a suitable resampling number to\naccelerate the convergence to the optimum [22], [20], [21]. As\nthe space-battle game introduced previously is stochastic and\nthe agents can be stochastic as well, it is not trivial to model\nthe noise or provide mathematically any optimal resampling\nnumber. Therefore, in this work, some resampling numbers are\narbitrarily chosen and compared to give a primary idea about\nthe necessary number of resamplings.\nFigs. 5 and 4 illustrate the overall performance of RMHC\nand MABRMHC using different resampling numbers over\n1,000 optimisation trials with random starting parameters. A\nnumber of 5,000 game evaluations is allocated as optimisation\nbudget in each trial. In other words, given a resampling number\nr, the fitness(·) (Eq. 4) is called at most 5, 000/r times.\nRMHC and MABRMHC using smaller resampling number\nachieve a faster move towards to the neighborhood of the\noptimum at the beginning of optimisation, however, they do\nnot converge to the optimum along with time; despite the\nslow speed at the beginning, RMHC and MABRMHC using\nlarger resampling number ﬁnally succeed in converging to the\noptimum in the limited budget. A dynamic resampling number\nwhich smoothly increases with the number of generations will\nbe favourable.\nUsing smaller budget, MABRMHC reaches the neighbor-\nhood of the optimum faster than RMHC. While the current\nbest-so-far ﬁtness is near the optimal ﬁtness value, it’s not\nsurprising to see the jagged curves (Fig. 4, right) while the\ngame evaluation consumed is moderate. The drop to the valley\ndues to the exploration of MABRMHC, then it manages\nto return to the previous optimum found or possibly ﬁnd\nanother optimum. Along with the increment of budget, i.e.,\ngame evaluations, the quality of best-so-far games found by\nMABRMHC remains stable.\nC. Evolving games in a larger search space\nAll the 5 parameters considered previously are used for\nevolving the game rules. In this section, we expand the pa-\nrameter space by taking into account a parameter for graphical\nobject: the radius of ship. The legal values for ship’s radius are\n10, 20, 30, 40 and 50. Thus, the search space is 6-dimensional\nand the total number of possible games is increase to 72,000\n(5 times larger).\nInstead of an intelligent agent and a deterministic agent, we\nplay the same OLMCTS agent (with 350 iterations), which\nhas been used previously in Section V-A and Section V-B,\nagainst two of its instances: a OLMCTS with 700 iterations\nand a OLMCTS with 175 iterations, denoted as OLMCTS700\nand OLMCTS175 respectively. The same optimisation process\nusing RMHC and MABRMHC is repeated separately, using\n1,000 game evaluation. The resampling numbers used are 5\nand 50, the ones which have achieved either fastest conver-\ngence at the beginning or provides the best recommendation\nFig. 4. Average ﬁtness value (left) with respect to the evaluation number over 1,000 optimisation trials by RMHC. The average winning rate of recommended\ngame instances at each generation are shown on the right. The standard error is shown by the shaded boundary.\nFig. 5. Average ﬁtness value (left) respected to the evaluation number over 1,000 optimisation trials by MABRMHC. The average winning rate of recommended\ngame instances at each generation are shown on the right. The standard error is shown by the shaded boundary.\nat the end of optimisation (after 1,000 game evaluations),\nrespectively. We aim at verifying if the same algorithms still\nperform well in a larger parameter space and with smaller\noptimisation budget.\nFig. 6 shows the average ﬁtness value respected to the num-\nber of game evaluations over 11 optimisation trials. Resam-\npling 50 times (black curves in Fig. 6) the same game instance\nguarantee a more accurate winning rate, while resampling 5\ntimes (red curves in Fig. 6) seems to converge faster.\nTo validate the quality of recommendations, we play each\nrecommended game instance, optimised by playing OLMCTS\nagainst OLMCTS175, 100 times using the OLMCTS175 and\na random agent, which uniformly randomly returns a legal\naction. The idea is to verify that the game instances optimised\nfor OLMCTS, are still playable and beneﬁcial for OLMCTS\ninstance with small number of iterations. The statistic is\nsummarised in Table II. The game instances recommended\nby RMHC and MABRMHC after optimising for OLMCTS\nwith more iterations are still beneﬁcial for the OLMCTS with\nless iterations. The game is still very difﬁcult for the random\nagent.\nTABLE II\nAVERAGE WINNING RATE (%) OVER 11 RECOMMENDATIONS AFTER\nOPTIMISATION USING 1,000 GAME EVALUATIONS , WITH DIFFERENT\nRESAMPLING NUMBERS . EACH GAME HAS BEEN REPEATED 100 TIMES .\nAlgorithm 5 samples 50 samples\nRMHC 86.00 91.8182\nMABRMHC 81.23 80.9545\nD. But what are the evolved games actually like?\nTo understand the results of the optimisation process, we\nvisually inspected a random sample of games that had been\nfound to have high ﬁtness in the optimisation process, and\ncompared these with several games that had low ﬁtness.\nFig. 6. Average ﬁtness value respected to the evaluation number over 11 optimisation trials by RMHC (left) and MABRMHC (right) using different resampling\nnumbers. The games are played by OLMCTS with 350 iterations against OLMCTS with 700 iterations (top) or OLMCTS against OLMCTS with 175 iterations\n(bottom). The standard error is shown by the shaded boundary.\nWe can discern some patterns in the high-ﬁtness games. One\nof them is to simply have a very high cost for ﬁring missiles.\nThis is somewhat disappointing, as it means that the OLMCTS\nagent will score higher simply by staying far away from the\nRAS agent. The latter will quickly reach large negative scores.\nA more interesting pattern was to have low missile costs,\nslow missiles, fast turning speed and fast thrusters. This\nresulted in a behaviour where the OLMCTS agent coasts\naround the screen in a mostly straight line, most of the time\nout of reach of the RAS agent’s missiles. When it gets close\nto the RAS agent, the OLMCTS turns to intercept and salvo\nof missiles (which typically all hit), and then ﬂies past.\nIn contrast, several of the low-ﬁtness games have low\nmissile costs and low cool-down times, so that the RAS\nagent effectively surrounds itself with a wall of missiles. The\nOLMCTS agent will occasionally attack, but typically loses\nmore score from getting hit and than it gains from hitting the\nRAS agent. An example of this can be seen in Fig. 3.\nIt appears from this that the high-ﬁtness games, at least\nthose that do not have excessive missile costs, are indeed\ndeeper games in that skilful play is possible.\nVI. C ONCLUSION AND FURTHER WORK\nThe work described in this paper makes several contribu-\ntions in different directions. Our main aim in this work is to\nprovide an automatic game tuning method using simple but\nefﬁcient black-box noisy optimisation algorithms, which can\nserve as a base-level game generator and part on an AI-assisted\ngame design tool, assisting a human game designer with tuning\nthe game for depth. The baseline game generator can also\nhelp with suggesting game variants that a human designer can\nbuild on. Conversely, instead of initialising the optimisation\nwith randomly generated parameters in the search space (as\nwhat we have done in this paper), human game designers can\nprovide a set of possibly good initial parameters with their\nknowledge and experiences.\nThe game instance evolving provides a method for auto-\nmatic game parameter tuning or for automatically designing\nnew games or levels by deﬁning different ﬁtness function used\nby optimisation algorithms. Even a simple algorithm such as\nRMHC may be used to automate game tuning. The application\nof other optimisation algorithms is straightforward.\nThe two tested optimisation algorithms achieve fast con-\nvergence towards the optimum even with a small resampling\nnumber when optimising for the OLMCTS against the RAS.\nUsing dynamic non-adaptive or adaptive resampling numbers\nincreasing with the generation number, such as the resampling\nrules discussed in [23], to take the strength of both small and\nbig numbers of resamplings will be favourable.\nThough the primary application of MABRMHC to the\nspace-battle game shows its strength, there is still more to ex-\nplore. For instance, the selection between parent and offspring\nis still achieved by resampling each of them several times\nand comparing their average noise ﬁtness value. However,\nthe classic bandit algorithm stores the average reward and\nthe times that each sampled candidate has been re-evaluated,\nwhich is also a form of resampling. We are not making\nuse of this information while making the choice between\nthe parent and offspring at each generation. Using a better\nrecommendation policy (such as UCB or most visited) seems\nlike a fruitful avenue of future work. Another potential issue\nis the dependencies between the parameters to be optimised in\nsome games or other real world problems. A N-Tuple Bandit\nEvolutionary Algorithm [24] is proposed to handle such case.\nThe study of the winning rate distribution and landscape\nover game instances helps us understand more about the\ngame difﬁculty. Another possible future work is the study of\nﬁtness distance correlation across parameters. Isaksen et al. [5]\nused Euclidean distance for measuring distance between game\ninstances of Flappy Bird and discovered that such a simple\nmeasure can be misleading, since the difference between\ngame instances does not always reﬂect the difference between\ntheir parameter values. We observe the same situation when\nanalysing the landscape of ﬁtness value by the possible values\nof individual game parameter (Fig. 3).\nThough we focus on a discrete domain in this work, it’s\nobviously applicable to optimise game parameters in con-\ntinuous domains, either by applying continuous black-box\nnoisy optimisation algorithms or by discretising the continuous\nparameter space to discrete values. Evolving parameters for\nsome other games, such as the games in GVG-AI framework,\nis another interesting extension of this work.\nThe approach is currently constrained by the limited intelli-\ngence of the GVG-AI agent we used, the proof of which is that\non many instances of the game a reasonable human player is\nable to defeat both rotate-and-shoot (RAS) and the OLMCTS\nplayers. This problem will be overcome over time as the set\nof available GVG-AI agents grows.\nREFERENCES\n[1] J. Togelius and J. Schmidhuber, “An experiment in automatic game\ndesign.” in Proceedings of the 2008 IEEE Conference on Computational\nIntelligence and Games , 2008, pp. 111–118.\n[2] C. Browne and F. Maire, “Evolutionary game design,” IEEE Transac-\ntions on Computational Intelligence and AI in Games , vol. 2, no. 1, pp.\n1–16, 2010.\n[3] M. Cook, S. Colton, and A. Pease, “Aesthetic considerations for au-\ntomated platformer design.” in The Eighth Annual AAAI Conference\non Artiﬁcial Intelligence and Interactive Digital Entertainment (AIIDE) ,\n2012.\n[4] N. Shaker, J. Togelius, and M. J. Nelson, “Procedural content generation\nin games: A textbook and an overview of current research,” Procedural\nContent Generation in Games: A Textbook and an Overview of Current\nResearch, 2016.\n[5] A. Isaksen, D. Gopstein, J. Togelius, and A. Nealen, “Discovering unique\ngame variants,” inComputational Creativity and Games Workshop at the\n2015 International Conference on Computational Creativity , 2015.\n[6] J. Liu, D. P ´erez-Li´ebana, and S. M. Lucas, “Rolling horizon coevo-\nlutionary planning for two-player video games,” in Proceedings of\nthe IEEE Computer Science and Electronic Engineering Conference\n(CEEC), 2016.\n[7] B. Pell, “Metagame in symmetric chess-like games,” 1992.\n[8] J. Togelius, R. De Nardi, and S. M. Lucas, “Towards automatic person-\nalised content creation for racing games,” in 2007 IEEE Symposium on\nComputational Intelligence and Games . IEEE, 2007, pp. 252–259.\n[9] J. Togelius, G. N. Yannakakis, K. O. Stanley, and C. Browne, “Search-\nbased procedural content generation: A taxonomy and survey,” IEEE\nTransactions on Computational Intelligence and AI in Games , vol. 3,\nno. 3, pp. 172–186, 2011.\n[10] E. J. Hastings, R. K. Guha, and K. O. Stanley, “Automatic content\ngeneration in the galactic arms race video game,” IEEE Transactions\non Computational Intelligence and AI in Games , vol. 1, no. 4, pp. 245–\n263, 2009.\n[11] N. Sorenson and P. Pasquier, “Towards a generic framework for au-\ntomated video game level creation,” in European Conference on the\nApplications of Evolutionary Computation . Springer, 2010, pp. 131–\n140.\n[12] D. Ashlock, “Automatic generation of game elements via evolution,”\nin Proceedings of the 2010 IEEE Conference on Computational Intelli-\ngence and Games . IEEE, 2010, pp. 289–296.\n[13] M. Cook and S. Colton, “Multi-faceted evolution of simple arcade\ngames.” in Proceedings of the 2011 IEEE Conference on Computational\nIntelligence and Games , 2011, pp. 289–296.\n[14] M. J. Nelson and M. Mateas, “Towards automated game design,” in\nCongress of the Italian Association for Artiﬁcial Intelligence . Springer,\n2007, pp. 626–637.\n[15] E. J. Powley, S. Gaudl, S. Colton, M. J. Nelson, R. Saunders, and\nM. Cook, “Automated tweaking of levels for casual creation of mobile\ngames,” 2016.\n[16] F. Lantz, A. Isaksen, A. Jaffe, A. Nealen, and J. Togelius, “Depth in\nstrategic games,” in under review, 2017.\n[17] T. S. Nielsen, G. A. Barros, J. Togelius, and M. J. Nelson, “General\nvideo game evaluation using relative algorithm performance proﬁles,” in\nEuropean Conference on the Applications of Evolutionary Computation .\nSpringer, 2015, pp. 369–380.\n[18] S. M. Lucas and T. J. Reynolds, “Learning DFA: Evolution versus\nEvidence Driven State Merging,” in Evolutionary Computation, 2003.\nCEC’03. The 2003 Congress on , vol. 1. IEEE, 2003, pp. 351–358.\n[19] ——, “Learning deterministic ﬁnite automata with a smart state labeling\nevolutionary algorithm,” Pattern Analysis and Machine Intelligence,\nIEEE Transactions on , vol. 27, no. 7, pp. 1063–1074, 2005.\n[20] J. Liu, D. Pe ´rez-Liebana, and S. M. Lucas, “Bandit-based random\nmutation hill-climbing,” arXiv preprint arXiv:1606.06041 , 2016.\n[21] D. P.-L. Jialin Liu and S. M. Lucas, “Bandit-based random mutation\nhill-climbing,” in Evolutionary Computation, 2017. CEC’17. The 2017\nCongress on. IEEE, 2017.\n[22] J. Liu, M. Fairbank, D. P ´erez-Li´ebana, and S. M. Lucas, “Optimal resam-\npling for the noisy onemax problem,” arXiv preprint arXiv:1607.06641,\n2016.\n[23] J. Liu, “Portfolio methods in uncertain contexts,” Ph.D. dissertation,\nUniversit´e Paris-Saclay, 2015.\n[24] K. Kunanusont, R. D. Gaina, J. Liu, D. Perez-Liebana, and S. M.\nLucas, “The n-tuple bandit evolutionary algorithm for automatic game\nimprovement,” in Evolutionary Computation, 2017. CEC’17. The 2017\nCongress on. IEEE, 2017.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6745288372039795
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5657076239585876
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49552953243255615
    },
    {
      "name": "Video game",
      "score": 0.4933396875858307
    },
    {
      "name": "Fitness function",
      "score": 0.4715564250946045
    },
    {
      "name": "Mathematical optimization",
      "score": 0.4189798831939697
    },
    {
      "name": "Stochastic game",
      "score": 0.4109615087509155
    },
    {
      "name": "Machine learning",
      "score": 0.39241254329681396
    },
    {
      "name": "Genetic algorithm",
      "score": 0.29844605922698975
    },
    {
      "name": "Mathematics",
      "score": 0.22523164749145508
    },
    {
      "name": "Mathematical economics",
      "score": 0.11047953367233276
    },
    {
      "name": "Multimedia",
      "score": 0.08923318982124329
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I110002522",
      "name": "University of Essex",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 1
}