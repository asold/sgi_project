{
  "title": "Provably Confidential Language Modelling",
  "url": "https://openalex.org/W4229053728",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2125004599",
      "name": "Xuandong Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098784551",
      "name": "Lei Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110773776",
      "name": "Yu-Xiang Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3207429447",
    "https://openalex.org/W1965093697",
    "https://openalex.org/W4226142937",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W4287795696",
    "https://openalex.org/W2055981215",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W3035261884",
    "https://openalex.org/W2207879440",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W3034746805",
    "https://openalex.org/W4287888099",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3045703328",
    "https://openalex.org/W3188505388",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2962934384",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2810715221",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2123845384"
  ],
  "abstract": "Large language models are shown to memorize privacy information such as social security numbers in training data. Given the sheer scale of the training corpus, it is challenging to screen and filter these privacy data, either manually or automatically. In this paper, we propose Confidentially Redacted Training (CRT), a method to train language generation models while protecting the confidential segments. We borrow ideas from differential privacy (which solves a related but distinct problem) and show that our method is able to provably prevent unintended memorization by randomizing parts of the training process. Moreover, we show that redaction with an approximately correct screening policy amplifies the confidentiality guarantee. We implement the method for both LSTM and GPT language models. Our experimental results show that the models trained by CRT obtain almost the same perplexity while preserving strong confidentiality.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 943 - 955\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nProvably Confidential Language Modelling\nXuandong Zhao Lei Li Yu-Xiang Wang\nUniversity of California, Santa Barbara\n{xuandongzhao,leili,yuxiangw}@cs.ucsb.edu\nAbstract\nLarge language models are shown to memo-\nrize privacy information such as social secu-\nrity numbers in training data. Given the sheer\nscale of the training corpus, it is challenging\nto screen and filter all privacy data, either man-\nually or automatically. In this paper, we pro-\npose Confidentially Redacted Training (CRT),\na method to train language generation models\nwhile protecting the confidential segments. We\nborrow ideas from differential privacy (which\nsolves a related but distinct problem) and show\nthat our method is able to provably prevent un-\nintended memorization by randomizing parts\nof the training process. Moreover, we show\nthat redaction with an approximately correct\nscreening policy amplifies the confidentiality\nguarantee. We implement the method for both\nLSTM and GPT language models. Our experi-\nmental results show that the models trained by\nCRT obtain almost the same perplexity while\npreserving strong confidentiality1.\n1 Introduction\nLanguage models (LM) have rich real-world ap-\nplications in, among others, machine translation\n(Bahdanau et al., 2015), AI chatbots (Hosseini-Asl\net al., 2020), question answering (Kwiatkowski\net al., 2019), and information retrieval (Ganguly\net al., 2015). The advent of transformers (Vaswani\net al., 2017) has fostered a dramatic advancement\nin the capabilities of generative neural language\nmodels, yet they come at a cost to privacy, as the\namount of excess parameters in the LM enables\nit to memorize certain training samples. Recent\nworks show that sensitive user information from\nthe training dataset, such as address and name, can\nbe extracted verbatim from text generation mod-\nels by querying the LM as an API (Carlini et al.,\n2019, 2021; Lee et al., 2022). How to train a high-\nperforming language model without memorizing\n1Our code is available at https://github.com/\nXuandongZhao/CRT\nsensitive text has become a major research chal-\nlenge.\nExisting solutions to this problem primarily\nleverage differential privacy (DP) (Dwork et al.,\n2006).\nDifferentially private learning algorithms ensure\nthat an attacker could not infer whether a data point\nis used for training, let alone extracting the sensi-\ntive information within that data point.\nHowever, there are several mismatches between\nthe problem of privacy that DP addresses, and our\nproblem of preventing the memorization of sensi-\ntive text (henceforth referred to as confidentiality).\nFirst, confidential information in a natural language\ndataset is sparse (e.g., the bulk of an email might\nnot carry confidential information). DP’s undis-\ncriminating protection for all sentences could be\nunnecessarily conservative which limits the util-\nity of the trained model. Second, what needs to\nbe protected is the content of the sensitive text,\nrather than the data context. For example, in the\nsentence “My SSN is 123-45-6789.”, it is\nthe actual SSN that we hope to conceal rather than\nthe general information that someone entered her\nSSN in a chatbot dialogue. Thirdly, the same sen-\nsitive content could appear in many data points,\nwhich makes the protection of the content more\nchallenging than protecting one data sample. These\ndifferences motivate us to treat the problem of con-\nfidentiality protection in LM separately with new\ndefinitions.\nBesides DP, we also consider classical tech-\nniques of redaction and deduplication. Redaction\nrefers to the process of removing sensitive or classi-\nfied information from a document prior to its publi-\ncation in governmental and legal contexts.Dedupli-\ncation is the procedure of detecting and removing\nidentical and nearly identical texts from a corpus.\nThe main challenge of applying these techniques is\nthat it is hard to manually redact a gigantic dataset\nand automated tools are far from being perfect.\n943\nSYS:Hello, I am the customer support bot.What do you need?USR:Hello robot. Where is my package?SYS:May I have your full name?USR:Yes, James Bing.SYS:We will need the shipping address as well.USR:Ok, it is 81171 Nguyen Ford North Crystalbury, MO 52398.SYS:The tracking number is VD98ID6CXJ. What else can I do?USR:I have all I need.\nSYS:Hello, I am the customer support bot.What do you need?USR:Hello robot. Where is my package?SYS:May I have your full name?USR:Yes, James Bing.SYS:We will need the shipping address as well.USR:Ok, it is 81171 Nguyen Ford North Crystalbury, MO 52398.SYS:The tracking number is VD98ID6CXJ. What else can I do?USR:I have all I need.\nfalse negative\nSYS:Hello, I am the customer support bot.What do you need?USR:Hello robot. Where is my package?SYS:May I have your full name?USR:Yes, James Bing.SYS:We will need the shipping address as well.USR:Ok, it is 81171 Nguyen Ford North Crystalbury, MO 52398.SYS:The tracking number is VD98ID6CXJ. What else can I do?USR:I have all I need.\nSYS:Hello, I am the customer support bot.What do you need?USR:Hello robot. Where is my package?SYS:May I have your full name?USR:Yes, James Bing.SYS:We will need the shipping address as well.USR:Ok, it is 81171 Nguyen Ford North Crystalbury, MO 52398.SYS:The tracking number is VD98ID6CXJ. What else can I do?USR:I have all I need.\nRedactionwithapolicywithrecall0.9andhighprecisioncompromisesconfidentiality.\nRedactionwithapolicywithrecall1.0butpoorprecisionresultsinuselessdata.\nfalse positives\nOurresults:1.Provableconfidentialityensuresthatthesetwoareindistinguishable!2.Approximateredactionpolicyamplifiestheconfidentialityguarantee.\nRawsensitivetextPerfectlyredactedtext\nFigure 1: An example from simulated dialog dataset CustomerSim. The yellow highlights are confidential\ncontent (middle). Left shows the text after Redaction by a sequence labeling policy π. However, if the policy is not\nperfect, there exists false negative or false positive samples as shown on the right.\nThe contribution of this paper is fivefold.\n1. We show that in the absence of a perfect\nscreening policy, the risk of a language model\nmemorizing sensitive content is real and can\nbe efficiently exploited with only blackbox\naccess to the model even if the learning algo-\nrithm satisfies the recently proposed notion of\nselective differential privacy (Shi et al., 2021).\n2. Inspired by differential privacy, we introduce\na new definition of confidentiality which pre-\ncisely quantifies the risk of leaking sensitive\ntext.\n3. We propose CRT to train language generation\nmodels while protecting confidential text. The\nmethod with deduplication and redaction oper-\nations work even under imperfect confidential\ntext labeling policies.\n4. We theoretically prove that CRT, combined\nwith differentially private stochastic gradient\ndescent (DP-SGD), provides strong confiden-\ntiality guarantees.\n5. Our experiments on both MultiWOZ 2.2 and\nCustomerSim datasets show that different\nmodels trained by CRT can achieve the same\nor better perplexity than existing solutions\n(against the attacks of Carlini et al. (2019,\n2021)).\nTo the best of our knowledge, we are the first that\nrigorously establish the role of deduplication and\nredaction in achieving provably stronger confiden-\ntiality (or the related differential privacy) guaran-\ntees; and the first that achieve provably confidential-\nity in transformer models with only a mild utility\nloss.\n2 Background & Related Work\nNext, we briefly introduce the relevant background\nand discuss the related work to put our work in\ncontext.\nLanguage modeling is a fundamental problem\nin natural language processing (Devlin et al., 2019;\nHoward and Ruder, 2018; Raffel et al., 2020).\nConsider a text sequence that consists of mul-\ntiple tokens from a vocabulary V, i.e., w =\n(w1,w2,...,w n), where wi is the i-th token. The\ngoal of language modeling is to construct a gen-\nerative model of the distribution Pr(w), by apply-\ning the chain rule Pr(w) =∏n\ni=1 Pr (wi |w<i) .\nWe let fθ(wi|w<i) denote the likelihood of token\nwi when evaluating the neural network f with\nparameters θ. A language model is trained to\nmaximize the probability of the data in a training\nset W, by minimizing the negative log-likelihood\nover each training example with the loss function\nL(θ) =−log ∏n\ni=1 fθ(wi |w<i) .Recurrent neu-\nral networks (RNNs) used to be a common choice\nfor the neural network architecture to estimate the\nprobability distribution Pr(w). (Hochreiter and\nSchmidhuber, 1997; Mikolov et al., 2010). More\nrecently, large-scale Transformer-based language\nmodels have replaced RNNs in state-of-the-art\n944\nmodels for all sorts of NLP tasks (Vaswani et al.,\n2017; Radford et al., 2019). Nevertheless, common\nlanguage models are vulnerable to privacy attacks\nand possibly expose information about their sensi-\ntive training data (Carlini et al., 2019, 2021).\nDifferentially private (DP) learning methods\n(see, e.g., Abadi et al., 2016) has been applied to\nlanguage models as a blanket solution for a num-\nber of privacy and security risks. McMahan et al.\n(2018) trained an RNN language model with DP\nguarantees in a federated learning setup. Anil et al.\n(2021) pre-trained BERT under DP on datasets with\nhundreds of millions of examples. These paper\nalso demonstrated that DP can effectively prevent\ndata-extraction attacks in practice even for algo-\nrithms with DP guarantees that are considered too\nweak from a theoretical-perspective (e.g., ϵ = 8\nor 16). However, the strong protection of DP of-\nten results in a substantial drop in the utility of the\ntrained model, which makes them less desirable in\npractice. In fact, it was recently shown that it is\nnecessary for deep learning models to memorize\ncertain training data to achieve high accuracy (Feld-\nman, 2020), which suggests that DP or any other\ntechniques that require the model to not memorize\nany training data will perform poorly in the high-\ndimensional, power-law distributed real datasets.\nThis motivates us to consider weakened models\nthat only prevent memorizing the sensitive part of\nthe text.\nRecent works (Lee et al., 2022; Kandpal et al.,\n2022) show that deduplication enables language\nmodels to emit memorized text less frequently with\nsame or better accuracy. However, deduplicating\ntraining datasets can not prevent all unintended\nmemorization. We combine deduplication and\nredaction and then apply both techniques to the\ntraining process of LM to achieve confidentiality\nwith provable guarantee.\nThe closest to us is perhaps the work of Shi\net al. (2021), who proposed selective differential\nprivacy (S-DP), which requires indistinguishability\nbetween two datasets that differ only on a sensi-\ntive message. Correspondingly, they propose an\nalgorithm (Selective DP-SGD) for training RNN\nthat adds noise only to the part of computation that\ninvolves sensitive tokens. To define S-DP and to\nrun Selective DP-SGD, one needs to have access to\na policy function F which determines which token\nis sensitive. This requirement limits the applicabil-\nity of their approach to those applications where\nsuch perfect F is known. We note that even for\nname-entity recognition the state-of-the-art model\nis far from being perfect, and which part of the text\nis considered sensitive is often ambiguous even\nfor human annotators. We will see that naively\nrunning Selective DP-SGD with an approximate\npolicy function does not provide a meaningful con-\nfidentiality guarantee and is vulnerable to practical\ndata extraction attacks. Finally, we note that in the\ncase when a perfect policy function is available, we\ncan simply use it for redaction, which provides a\nperfect S-DP with ϵ = 0. A big part of our con-\ntribution is to refine S-DP to a (slightly different)\ndefinition called “confidentiality” and to demon-\nstrate that we use an approximate screening policy\nto amplify the confidentiality parameter.\n3 The CRT Method and Theory\nIn this section, we develop our method with prov-\nable confidentiality.\n3.1 Formally defining confidentiality\nLet the dataset be a collection of ndata points —\neach being a sequence of tokens. A “secret” x\nis a contiguous subsequence of tokens within a\ndata point that is considered sensitive or confiden-\ntial. The goal of our research is to allow us to\ntrain language models on such datasets that could\ncontain secrets while provably prevent the model\nfrom remembering that these secrets were. We start\nby defining a formal definition of confidentiality,\nwhich uses the following idea of indistinguishabil-\nity from the DP literature.\nDefinition 1 (Indistinguishability). We say that a\npair of distributions P,Q defined on the same prob-\nability space are (ϵ,δ)-indistinguishable if for any\nmeasurable set S,\nPr\nP\n[S] ≤eϵPr\nQ\n[S] +δ.\nDefinition 2 (Confidentiality). We say that Aen-\nsures that a secret x is (ϵ(x),δ)-confidential, if\nfor any dataset D that contains x in one of its\ndata points, and an alternative dataset D′that re-\nplaces xin Dwith a generic <MASK>, it holds that\n(A(D),A(D′)) are (ϵ(x),δ)-indistinguishable. In\naddition, we simply say that Aensures (ϵ,δ)-\nconfidentiality if ϵ(x) ≤ϵfor all secret x.\nThis definition ensures that an attacker cannot\ndistinguish from the output of A(the trained lan-\nguage model) whether it was x or <MASK> that\n945\nwas used for training, thus formalizing the idea of\nconfidentiality. The protection should be viewed\nas relative, rather than absolute. The definition\nbounds the risk of any bad event by an multiplica-\ntive factor of eϵ and an additive factor of δ, which\nimplies that anything that could happen when we\nrun Aon the sensitive data could’ve happened with\nwith similar probability even if Aruns on an alter-\nnative world where these sensitive information are\nperfectly masked.\nConnections to differential privacy. Our defi-\nnition of confidentiality is related to (and inspired\nby) (ϵ,δ)-differential privacy (DP) but is differ-\nent in several ways. DP is stronger (and im-\nplies confidentiality!) requires Ato ensure (ϵ,δ)-\nindistinguishability for all D,D′that can be mod-\nified from each other by adding or removing one\nindividual person / data point (or tokens, depend-\ning on the desired granularity); but for Ato en-\nsure (ϵ,δ)-confidentiality, it only requires (ϵ,δ)-\nindistinguishability for specific D,D′ where D′\nreplaces x in D with <MASK>. Moreover, it is\nmore informative to define ϵas a function of each\nspecific x, which is different from DP (it resembles\npersonalized DP (Ghosh and Roth, 2015)).\nThe confidentiality definition makes sense for\nour problem because it protects the content of the\nsensitive text xrather than its existence. Specif-\nically, a pre-processing algorithm that masks all\nsensitive text ensures(0,0)-confidentiality but does\nnot satisfy any non-trivial DP guarantees.\nSometimes, it is useful to consider the confiden-\ntiality of multiple secret texts. For example, a se-\ncret key xcould appear multiple times in multiple\ndata points. Also, there might be multiple secret\ntexts that are correlated to each other such that the\nknowledge of one would reveal other secrets.\nDefinition 3 (Group Confidentiality). We say that\nA ensures that a list of sensitive texts S :=\n[x1,...,x k] is (ϵ(S),δ)-(group) confidential, if for\nany dataset Dthat contains [x1,...,x k] in up to k\ndata points, and D′being the version that replaces\neach element in S with <MASK>, it holds that\n(A(D),A(D′)) are (ϵ(S),δ)-indistinguishable.\nA special case of such group confidentiality is\nwhen Scollects the all secret text in D, which\nprotects all secret texts uniformly. We call this\nuniform-confidentiality. Note that the standard def-\ninition of confidentiality also protect every secret\nx, except that it protects each secret xindividually,\nrather than together.\nInspired by the recent development of Bayesian\nDP (Triastcyn and Faltings, 2020), we also define\nBayesian confidentiality as follows.\nDefinition 4 (Bayesian Confidentiality). Let Dbe\na dataset that is fixed except a random secretx∼µ\ndrawn from some distribution µ. Let D′ be ob-\ntained by replacing xwith <MASK>2. Then Aen-\nsures (ϵ,δ)-Bayesian Confidentiality if for any D′,\n(A(D),A(D′)) is (ϵ,δ)-indistinguishable, where\nA(D) is jointly distributed over x∼µand A.\nThe Bayesian confidentiality measures how\nmuch information an attacker could gain if he/she’s\nprior knowledge about this secret xis described by\nthe distribution µ. This is a strict generalization\nbecause when µis a single point mass atx, it recov-\ners Definition 2. The additional generality allows\nus to quantify the stronger confidentiality guaran-\ntee against weaker adversaries without complete\ninformation.\n3.2 Confidentially redacted training\nIn this section we describe the CRT method to train\nlanguage models with provable confidentiality guar-\nantee. It includes two pre-processing operations\n(deduplication and redaction) and a switching opti-\nmization procedure. The overall idea is to screen\nthe corpus into two separate sets, one public set in-\ncluding sentences with no confidential information,\nand one private set including sentences containing\nconfidential content. We then use normal optimiza-\ntion algorithms (e.g. SGD) on the public set and\ndifferential privacy optimizer (e.g. DP-SGD) on\nthe private set.\nDeduplication. The deduplication procedure\nDedup detects all sentences that appear multiple\ntimes in the training data and replace them into\na single <MASK> from the second occurrence on-\nwards (<MASK> is for proving purpose).\nRedaction. The redaction procedure Redactπ\ntakes applies a sequence labelling policy π to\nscreen confidential content in the training corpus\nD. π(s,x) = 1if a token xin a sentence sshould\nbe confidential. The labeled span in each detected\nsentence is replaced with a special token <MASK>.\nNote that we do not assume the policy is perfect. It\nmay label some non-sensitive tokens as sensitive\n(false positives) and label some sensitive text as\nnon-sensitive (false negative, or 1−recall).\n2Notice that D′is fixed even though x is random.\n946\nAlgorithm 1: CRT\nInput :Dataset D(after tokenization /\nsplitting), labelling policies π,πc,\nnumber of epochs T\n1 D′←Dedup(D)\n2 D′′←Redactπ(D′)\n3 Dpri ←{s∈D′′|∃x∈ss.t. π(s,x) =\n1 or ∃x⊂ss.t. πc(s,x) = 1}\n4 Dpub = {s∈D′′|s /∈Dpri}.\n5 for e= 1,...,T do\n6 Run one epoch of SGD with Dpub.\n7 Run one epoch3 of DP-SGD with Dpri.\n8 end\nRedact and Dedup could be implemented man-\nually, but with the large text corpus nowadays it\nis more common that these procedures are im-\nplemented using automated tools. For example,\nDedup could be implemented efficiently with just\none pass of data using a bloom filter (Bloom, 1970)\n(or other hashing tricks that also catches near-\nduplicates). Bloom filter in particular, enjoys the\nnice property that it could have false positives but\nnever any false negatives. Redactπ could be real-\nized by a named entity recognition (NER) model or\na personal-identifiable information (PII) detector.\nFinally, CRT combines the two pre-processing\nsteps with normal optimizer and DP-SGD, the stan-\ndard algorithm for deep learning with differential\nprivacy. A pseudo-code of the algorithm is given\nin Algorithm 1.\nBesides using a sequence labeling policy πwith\nbalanced precision/recall as part of the redaction\nprocess. The algorithm uses another, more conser-\nvative, policyπcwith nearly perfect recall to decide\non the data points that do not contain sensitive text.\nIn the situation when such πc isn’t available, we\nsimply choose πc(s,x) = 1for all tokens xin a\nsentence sand the second part becomes the vanila\nDP-SGD. It is also important that every data point\nthat contains a <MASK> requires protection.\n3.3 Theoretical analysis\nWe analyze the theoretical properties of the above\nmethod and show that they result in provable im-\nprovements in the (regular, group and Bayesian)\n3DP-SGD uses Poisson-sampled Gaussian mechanisms\n(with a random batchsize), thus cannot ensure all data points\nare seen and some data points might be seen many times.\nOne epoch means the number of iterations that in expectation\ncovers |Dpri| data points.\nconfidentiality parameters for any algorithms that\nare provably (ϵ(x),δ)-confidential as defined in\nSection 3.1.\nThe following theorem captures the benefit of\nredaction in improving confidentiality.\nProposition 5 (Confidentiality under redaction). If\nAensures (ϵ(x),δ)-Confidentiality for each token\nx of sentence s ∈ S(Sis a corpus), then A◦\nRedactπ ensures (˜ϵ(x),δ)-confidentiality with\n˜ϵ(x) =\n{\nϵ(x) if π(s,x) = 0\n0 otherwise.\nIn addition, A ◦ Redactπ also satisfies\n(˜ϵ(S),˜δ(S))-group confidentiality with\n˜ϵ(S) =\n∑\nx∈s&s∈S\nϵ(x)1(π(s,x) = 0),\n˜δ(S) =˜ke˜ϵ(S)δ\nwhere ˜k:= ∑\nx∈S1(π(s,x) = 0).\nAs an application of the above, if Aensures\n(ϵ,δ)-confidentiality, and that the empirical recall\nrates of the redaction policy on D is 1 −γ, then\nthe above proposition suggests that A◦Redactπ\nimproves the uniform-confidentiality over applying\nAwithout redaction by a factor of γ. The proof is\nin the appendix.\nRedaction also improves Bayesian confidential-\nity in a way that mirrors the privacy amplification\nby sampling from the DP literature.\nProposition 6 (Bayesian Confidentiality under\nRedaction). If Aensures (ϵ,δ)-Bayesian Confi-\ndentiality with respect to µ[x|π(s,x) = 0]for a\ntoken x in a sentence s, then A◦ Redactπ en-\nsures (log(1 +γ(eϵ−1)),γδ)-Bayesian Confiden-\ntiality under µif π has a false negative rate (i.e.,\n1−“Recall”) of γunder µ.\nThe proposition says that if the redaction pol-\nicy is accurate for secrets x ∼ µ, then we can\nhave a stronger confidentiality parameter that scales\nroughly at ˜ϵ= O(γϵ). The idea behind the proof\nis that over the distribution of x ∼µ, with prob-\nability 1 −γ, Redactπ(D) = Redactπ(D′), thus\nA◦Redactπ(D) ≡A◦ Redactπ(D′). With prob-\nability γ, Redactπ(D),Redactπ(D′) are different\nand conditioning on the fact that Redactπ fails to\ndetect x. Note that πis also applied to other text\nthat are not sensitive, and could result in false pos-\nitives, but they do not matter as the modification\n947\nof Redactπ to Dand D′will be identical. A full\nproof is given in the appendix.\nNext we turn to deduplication.\nProposition 7 (Group confidentiality under\ndeduplication.). If A ensures (ϵ(S),δ(S))-\nGroup Confidentiality, then A◦ Dedup ensures\n(ϵ(Unique(S)),δ(Unique(S)))-Group Confiden-\ntiality.\nDeduplication provides a stronger protection for\nthose cases where some secret xcould appear mul-\ntiple times in the dataset.\nTheorem 8. Let DP-SGD from Algorithm 1 satis-\nfies (ϵ,δ)-differential privacy.\n1. Assume πc(s,x) = 1for all secret tokens x\nin a sentence ssuch that π(s,x) = 0, then\nAlgorithm 1 satisfies (ϵ1(π(s,x) = 0),δ)-\nconfidentiality.\n2. Let Sbe a group containing munique secrets\nsuch that πc(s,x) = 1∀x ∈ sand s ∈ S\nand that πdetects ˜γ-proportion of the unique\nsecrets in S. Then Algorithm 1 satisfies\n(˜γmϵ,˜γme˜γmϵδ)-group confidentiality for S.\n3. Let π,πc has a a recall of 1 −γ and 1 −δ2\nrespectively on µ, then Algorithm 1 satisfies\n(log(1 +γ(eϵ −1)),γδ + δ2)-Bayesian Con-\nfidentiality for µ.\nThe theorem demonstrates that our CRT algo-\nrithm enjoys a full suite of confidentiality guaran-\ntees and they all benefit from the deduplication and\nredaction, particularly if πhas high recall.\nNote that the CRT algorithm achieves the worst-\ncase confidentiality guarantee if we have a non-\ntrivial conservative screening policy that outputs\nπc(x) = 1for all secret xthat πmisses, or we sim-\nply run vanilla DP-SGD after deduplication and\nredaction. On the other hand, CRT still satisfies\nBayesian confidentiality for each µdepending on\nthe recall rate of πc under µ.\n4 Experiments\nWe evaluate CRT by training two types of language\nmodel, LSTM and GPT-2, on two datasets: 1) Mul-\ntiWOZ 2.2, a well-known human-written dialogue\ndataset and 2) CustomerSim, a simulated dialogue\ndataset for conversation generation.\nMultiWOZ 2.2 is an already-public dialogue\ndataset written by crowd-workers, which collects\nover 10,000 annotated dialogues spanning 8 do-\nmains (Zang et al., 2020). We use this dataset to\nshow how CRT works in real-world applications.\nFollowing US Department of Labor’s guidance4 on\npersonal-identifiable information (PII), we treat all\nconfidential information (e.g. email address, ref-\nerence number, telephone number, etc.) as secrets.\nFor the sequence labeling policy πand conserva-\ntive policy πc, we build upon an NER model to do\nredaction. See Appendix A.4 for more details.\nCustomerSim. Following S-DP Shi et al. (2021),\nwe simulate a dialog dataset called CustomerSim\nwith synthetic user information. The dialog flow is\nsimulated based on a fixed agenda and the language\ngeneration is template-based (Zhao and Eskénazi,\n2018). CustomerSim consists of 10 thousand ex-\namples and over one million tokens. We treat user\nname, address, phone number, order, and tracking\nnumber as secrets, and use a regular expression\ntester (regex) to detect them for the redaction pro-\ncess.\nExperiment details. For LSTM model, we fol-\nlow the setting in S-DP to choose a one-layer\nLSTM. Because S-DP requires hidden states of\nthe sensitive input to be protected, it doesn’t sup-\nport more layers nor Bidirectional LSTM. Since\nthe advent of Transformers (Vaswani et al., 2017)\nsignificantly improves the capabilities of generative\nlanguage models, we also test transformer-based\nlanguage model GPT-2 (Radford et al., 2019) from\nHuggingFace (Wolf et al., 2019). As for deduplica-\ntion, we use SHA-1 (Jarvinen, 2004) hash function\nto encode sequences to SHA-1 hash code and then\nremove identical sequences based on the same hash\ncode. For Bayesian Confidentiality, we treat the\nuniform distribution over the secret sequences as\nthe distribution µ. More experiment details can be\nfound in Appendix A.3.\nBaselines. For LSTM model, we compare four\ndifferent training approaches: (1) vanilla SGD\n(denoted by \"Non-private-LSTM\"), (2) Selective\nDPSGD (denoted by \"S-DP-LSTM\") (3) DPSGD\n(denoted by \"DPSGD-LSTM\") and (4) confiden-\ntially redacted training (denoted by \"CRT-LSTM\").\nWhile for GPT-2 model, we compare three dif-\nferent training approaches: (1) vanilla SGD (de-\nnoted by \"Non-private-GPT\"), (2) DPSGD (de-\nnoted by \"DPSGD-GPT\") and (3) CRT (denoted by\n\"CRT-GPT\"). Our implementation of S-DP-LSTM\n4https://www.dol.gov/general/ppii\n948\n1 2 3 4 5\nBayesian confidentiality parameter \n10\n15\n20\n25\n30\n35\n40Perplexity (PPL) on test set\nMultiWOZ 2.2\nNon-private-GPT\nDPSGD-GPT\nCRT-GPT\n0.0 0.5 1.0 1.5 2.0\nBayesian confidentiality parameter \n2\n4\n6\n8\n10\n12Perplexity (PPL) on test set\nCustomerSim\nNon-private-GPT\nDPSGD-GPT\nCRT-GPT\n1 2 3 4 5\nBayesian confidentiality parameter \n100\n200\n300\n400\n500Perplexity (PPL) on test set\nMultiWOZ 2.2\nNon-private-LSTM\nDPSGD-LSTM\nS-DP-LSTM\n(w. perfect policy)\nCRT-LSTM\n0.5 1.0 1.5 2.0\nBayesian confidentiality parameter \n10\n20\n30Perplexity (PPL) on test set\nCustomerSim\nNon-private-LSTM\nDPSGD-LSTM\nS-DP-LSTM \n(w. perfect policy)\nCRT-LSTM\nFigure 2: Model utility and confidentiality guarantee on MultiWOZ 2.2 and CustomerSim datasets with µbeing a\nuniform distribution over the secret sequences in each dataset. PPL: Perplexity on the test set. ϵ: Privacy guarantee\nin Bayesian Confidentiality. We fix δ= 8e−5 for all models. Since Selective DP-SGD with approximate policy\ngives ϵ= +∞, we show its result with a perfect screen policy. But when a perfect policy is available, Redaction\nonly gives ϵ= 0and achieves the PPL of vanilla training with no noise added (Non-private-GPT/LSTM). For other\nmodels we set γ = 0.1 to show the result under approximate policy.\nmodel is built upon Shi et al. (2021) 5. We run\nthe experiment for the GPT-2 model following Li\net al. (2021)6, in which they propose ghost clipping\nmethod to alleviate the computational challenge of\nrunning DP-SGD with large Transformers.\nAll the models are trained five times to reduce\nrandomness, and the parameters are tuned based\non the validation set performances.\n5 Experimental Results\n5.1 Evaluation procedure\nWe need to evaluate both model utilities and privacy\nguarantees of the language models. We measure\npredictive perplexity (PPL) for the quality of LM.\nWe also analyze the theoretical privacy budget(ϵ,\nδ) and test whether language models are private\nunder attacks detailed below.\nCanary insertion attack. Canary insertion is\nproposed as a testing methodology for quantita-\ntively assessing the risk of unintended memoriza-\ntion (Carlini et al., 2019). It inserts random se-\nquences called canaries into the training dataset,\nthen trains the model, and finally calculates the\nfollowing exposure for the inserted canaries to\nmeasure a model’s potential for privacy risks. In\nour experiment, we randomly generate 10 ca-\nnaries in the form of \" My ID is: <random\n6-digit number here>\". Each canary is in-\nserted into the training dataset 20 times to generate\nmore salient differences between models.\nDefinition 9 (Canary Exposure). Given a canary\ns[r], a model with parameters θ, and the random-\nness space R, the exposure of s[r] is\nexposureθ = log2 |R|−log2 rankθ(s[r])\n5https://github.com/wyshi/lm_privacy\n6https://github.com/lxuechen/private-transformers\nAfter training, we calculate empirical model per-\nplexity for all possibly-instantiated canaries and list\nthem in sorted order. Then we can get the canary\nexposure based on the rank of a specific canary se-\nquence rankθ(s[r]) and the number of all possible\ncandidates |R|. In our setting, we show the highest\ncanary exposure in 10 canaries. For example, if a\ncanary ranks 1st among 1M candidates, the canary\nexposure is 19.93.\nMembership inference attack. Membership In-\nference is a widely used privacy attack method.\nGiven a non-privately trained model, an adversary\ncan predict whether or not a particular example\nwas used to train the model. We adopt the member-\nship inference attack in Carlini et al. (2021). The\ngeneral idea is to calculate the given samples’ per-\nplexities under the model, rank them and choose\nthe ones with the lowest perplexities, i.e., highest\nlikelihood by the model. We can think of this pro-\ncess as training a binary classifier based on the\nperplexity feature. We also implement the group\nmembership inference attack to show the group\nconfidentiality. More details about the implementa-\ntion can be found in the Appendix A.5.\n5.2 Overall performance\nFigure 2 presents the results of model utilities and\nconfidentiality guarantees across our models of in-\nterest on MultiWOZ 2.2 and CustomerSim datasets.\nEach point denotes a model for different epochs in\na training process. Since the X-axis isϵin Bayesian\nConfidentiality (the lower the better) and the Y-axis\nis perplexity (the lower the better), a perfect model\nwill lie in the bottom-left corner. CRT-GPT and\nDPSGD-GPT in general, perform better than S-DP-\nLSTM, CRT-LSTM and, DPSGD-LSTM on the\ntest sets. Our model CRT-GPT’s performance is\n949\nclose to Non-private-GPT in terms of PPL while\npreserving strong confidentiality. Besides, CRT-\nGPT is better than DPSGD-GPT manifested by a\nmuch lower ϵ, which demonstrates that approxi-\nmately correct screening policy amplifies the confi-\ndentiality guarantee.\nDifferences can be witnessed in the results from\ntwo different datasets: the models trained on Cus-\ntomerSim achieve overall better performances than\nthose trained on MultiWOZ. We think it’s due to\nthe fact that CustomerSim contains simple dialogs\nfrom template-based simulations.\n5.3 Attack results\nFigure 3, 4, and 5 present the results from canary\ninsertion attack and individual/group membership\ninference attack on MultiWOZ 2.2 and Customer-\nSim datasets. The X-axis is the false negative rate\nγ of screening policy π, ranging from 0.0 to 0.5;\nthe Y-axis is the canary exposure (in Figure 3) and\nmembership inference accuracy (in Figure 4 and 5),\nwhich measures the effectiveness of the attacks.\nThe lower the canary exposure or inference ac-\ncuracy, the better protection the model provides\nagainst the attacks.\n0.0 0.1 0.2 0.3 0.4 0.5\n0\n5\n10\n15\n20Canary Exposure\nMultiWOZ 2.2\n0.0 0.1 0.2 0.3 0.4 0.5\n0\n5\n10\n15\n20Canary Exposure\nCustomerSim\nNon-private-LSTM\nNon-private-GPT\nS-DP-LSTM\nCRT-LSTM\nCRT-GPT\nFigure 3: Canary insertion attack result. CRT achieves\nalmost 0 canary exposure, which means it can prevent\nunintended memorization.\nFor canary insertion attack, it can be seen from\nFigure 3 that the canary exposures for CRT-LSTM\nand CRT-GPT are both close to 0 which thus guar-\nantee excellent confidentiality. Non-private-LSTM\nand Non-private-GPT with mask can also attain\ngreat protection at perfect screening policy accu-\nracy (γ = 0), nonetheless a rise in γ results in a\nsharp increase in the exposure. It should be noticed\nthat S-DP-LSTM also has high exposure, similar\nto Non-private models, given any γabove 0. This\nis because that many sensitive data has been falsely\nidentified as non-sensitive by the approximate pol-\nicy, S-DPSGD does not protect these false negative\nsamples and hence a privacy leakage.\nFor membership inference attack, we compare\nthe inference accuracy with the benchmark value of\n0.0 0.1 0.2 0.3 0.4 0.5\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Inference Acc\nMultiWOZ 2.2\n0.0 0.1 0.2 0.3 0.4 0.5\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Inference Acc\nCustomerSim\nNon-private-LSTM\nNon-private-GPT\nS-DP-LSTM\nCRT-LSTM\nCRT-GPT\nRandom Guess\nFigure 4: Membership inference attack result. CRT at-\ntains nearly 50% accuracy, indicating that the adversary\ncould not infer whether a data point is used for training.\n0.0 0.1 0.2 0.3 0.4 0.5\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Inference Acc\nMultiWOZ 2.2\n0.0 0.1 0.2 0.3 0.4 0.5\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Inference Acc\nCustomerSim\nNon-private-LSTM\nNon-private-GPT\nS-DP-LSTM\nCRT-LSTM\nCRT-GPT\nRandom Guess\nFigure 5: Group membership inference attack result.\n0.5, which equals the random guess performance.\nIn Figure 4 and 5, we see that CRT-LSTM and CRT-\nGPT align well with the 0.5 horizontal line, suggest-\ning that they are rather safe to the attack. The infer-\nence accuracy for Non-private-LSTM/Non-private-\nGPT/S-DP-LSTM, in contrast, surges above 0.5\nas the false negative rate γdeviates from 0.0, indi-\ncating that these models become vulnerable to the\nattack under non-perfect screen policy. In addition,\nNon-private and S-DP models show even worse\nprotection under the group attack than the individ-\nual one in view of a higher inference accuracy at\ncertain γ.\n0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50\n0.5\n1.0\n1.5\n2.0 for Bayesian Confidentiality\n′= 3.0\n′= 2.0\n′= 1.0\nFigure 6: Bayesian Confidentiality amplification result.\nCRT helps to amplify the confidentiality guarantee.\n5.4 CRT amplifies Bayesian Confidentiality\nguarantees\nFigure 6 shows that confidentially redacted train-\ning can help to amplify the confidentiality guaran-\ntees. We set the ϵ′in DP-SGD fixed and show the\n950\ncorresponding ϵin Bayesian Confidentiality with\ndifferent screen policy π. Both ϵ′ and ϵ are for\nδ= 8e−5. If the approximately screening policy\nπ has a high recall ( γ is small), we will achieve\nmuch improvement in the Bayesian Confidentiality\nparameter ϵby deduplication and redaction. For\nexample, with (ϵ′= 1.0,γ = 0.1), we reduce the\nϵto 0.12.\n6 Conclusion\nIn this paper, we propose confidentially redacted\ntraining (CRT), a method to train language models\nwhile protecting the secret texts. We introduce a\nnew definition of confidentiality which quantifies\nthe risk of leaking sensitive content. We prove the\neffectiveness of CRT both theoretically and empiri-\ncally on multiple datasets and language models.\n7 Broader Impact\nThis work will alleviate ethical concerns of large-\nscale pre-trained language models. This paper pro-\nvides one promising solution to an important as-\npect of NLP: training high quality language models\nfor text generation without compromising confi-\ndential information. The current use cases of lan-\nguage models involve pretraining on public web\ncorpus and fine-tuning on individual application\ndata. However, the private application specific data\noften contains user-generated sensitive information.\nThe proposed method in this paper aims to use\nas much individual fine-tuning data as possible,\nwhile does not leak or memorize any confidential\ninformation with provable guarantees. Without the\nmethod, one has to either use the general pretrain-\ning LM without fine-tuning or manually filter sen-\nsitive information and fine-tuning on the remaining.\nIt can be applied in broader applications that need\nlanguage models or text generation models.\nIn our experiments, we use a simulation scheme\nto mimic confidential content in a real corpus. We\ndid not compromise any real user’s confidential\ninformation.\nAcknowledgements\nThe work was partially supported by NSF Award\n# 2048091. XZ was supported by UCSB Chancel-\nlor’s Fellowship. We would like to thank the anony-\nmous reviewers for their thoughtful comments. We\nwould also like to thank Siqi Ouyang for the help-\nful discussion and Yang Gao for polishing up the\ndraft.\nReferences\nMartín Abadi, Andy Chu, Ian J. Goodfellow, H. Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. Proceedings of the 2016 ACM SIGSAC Confer-\nence on Computer and Communications Security.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar,\nand Pasin Manurangsi. 2021. Large-scale differen-\ntially private bert. ArXiv, abs/2108.01624.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473.\nBorja Balle, Gilles Barthe, and Marco Gaboardi. 2018.\nPrivacy amplification by subsampling: tight analy-\nses via couplings and divergences. In Advances in\nNeural Information Processing Systems, pages 6280–\n6290.\nGilles Barthe and Federico Olmedo. 2013. Beyond\ndifferential privacy: Composition theorems and re-\nlational logic for f-divergences between probabilis-\ntic programs. In International Colloquium on Au-\ntomata, Languages, and Programming, pages 49–60.\nSpringer.\nBurton H Bloom. 1970. Space/time trade-offs in hash\ncoding with allowable errors. Communications of the\nACM, 13(7):422–426.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Xiaodong Song. 2019. The secret\nsharer: Evaluating and testing unintended memo-\nrization in neural networks. In USENIX Security\nSymposium.\nNicholas Carlini, Florian Tramèr, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom B. Brown, Dawn Xiaodong\nSong, Úlfar Erlingsson, Alina Oprea, and Colin Raf-\nfel. 2021. Extracting training data from large lan-\nguage models. In USENIX Security Symposium.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. ArXiv, abs/1810.04805.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and\nAdam Smith. 2006. Calibrating noise to sensitivity\nin private data analysis. In Theory of cryptography\nconference, pages 265–284. Springer.\nVitaly Feldman. 2020. Does learning require memoriza-\ntion? a short tale about a long tail. In Proceedings\nof the 52nd Annual ACM SIGACT Symposium on\nTheory of Computing, pages 954–959.\nDebasis Ganguly, Dwaipayan Roy, Mandar Mitra, and\nGareth J.F. Jones. 2015. Word embedding based\ngeneralized language model for information retrieval.\nProceedings of the 38th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval.\n951\nArpita Ghosh and Aaron Roth. 2015. Selling privacy\nat auction. Games and Economic Behavior, 91:334–\n346.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9:1735–\n1780.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,\nSemih Yavuz, and Richard Socher. 2020. A simple\nlanguage model for task-oriented dialogue. ArXiv,\nabs/2005.00796.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification. In\nACL.\nKimmo Jarvinen. 2004. Design and implementation of\na sha-1 hash module on fpgas. Helsinki University of\nTechnology Signal Processing Laboratory.\nNikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.\nDeduplicating training data mitigates privacy risks in\nlanguage models. arXiv preprint arXiv:2202.06539.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc V . Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:453–466.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics. Association for Compu-\ntational Linguistics.\nXuechen Li, Florian Tramèr, Percy Liang, and Tat-\nsunori Hashimoto. 2021. Large language models\ncan be strong differentially private learners. ArXiv,\nabs/2110.05679.\nH. Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2018. Learning differentially private\nrecurrent language models. In ICLR.\nTomas Mikolov, Martin Karafiát, Lukás Burget,\nJan Honza Cernocký, and Sanjeev Khudanpur. 2010.\nRecurrent neural network based language model. In\nINTERSPEECH.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. ArXiv, abs/1910.10683.\nWeiyan Shi, Aiqi Cui, Evan Li, R. Jia, and Zhou Yu.\n2021. Selective differential privacy for language\nmodeling. ArXiv, abs/2108.12944.\nAleksei Triastcyn and Boi Faltings. 2020. Bayesian\ndifferential privacy for machine learning. In Inter-\nnational Conference on Machine Learning , pages\n9583–9592. PMLR.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. ArXiv, abs/1706.03762.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, and et al. 2013. Ontonotes release 5.0.\nLinguistic Data Consortium, Philadelphia, PA, 23.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nXiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara,\nRaghav Gupta, Jianguo Zhang, and Jindong Chen.\n2020. Multiwoz 2.2: A dialogue dataset with addi-\ntional annotation corrections and state tracking base-\nlines. In Proceedings of the 2nd Workshop on Natu-\nral Language Processing for Conversational AI, ACL\n2020, pages 109–117.\nTiancheng Zhao and Maxine Eskénazi. 2018. Zero-shot\ndialog generation with cross-domain latent actions.\nIn SIGDIAL Conference.\n952\nA Appendix\nA.1 Illustration of our proposed algorithm\nSYS:Hello, I am the customer support bot.What do you need?USR:Hirobot. It’smeagain.SYS:Whatisyourfull name?USR:James Bing.SYS:Isyourshippingaddressstill81171 Nguyen Ford North Crystalbury, MO 52398?USR:Yes!SYS:The tracking number is KHSIDHUE25. What else can I do?USR:Nothingelse.Thankyou!\nSYS:Hello, I am the customer support bot.What do you need?USR:Hello robot. Where is my package?SYS:May I have your full name?USR:Yes, James Bing.SYS:We will need the shipping address as well.USR:Ok, it is 81171 Nguyen Ford North Crystalbury, MO 52398.SYS:The tracking number is VD98ID6CXJ. What else can I do?USR:I have all I need.\nRawdataset SYS:Hello, I am the customer support bot.What do you need?USR:Hello robot. Where is my package?SYS:May I have your full name?USR:Yes, James Bing.SYS:We will need the shipping address as well.USR:Ok, it is 81171 Nguyen Ford North Crystalbury, MO 52398.SYS:The tracking number is VD98ID6CXJ. What else can I do?USR:I have all I need.\nRedactionwithanapproximatepolicywithbalancedprecision/recall.\nDeduplicationwithaBloomfilter.SYS:Hello, I am the customer support bot.What do you need?USR:Hirobot. It’smeagain.SYS:Whatisyourfull name?USR:James Bing.SYS:Isyourshippingaddressstill81171 Nguyen Ford North Crystalbury, MO 52398?USR:Yes!SYS:The tracking number is KHSIDHUE25. What else can I do?USR:Nothingelse.Thankyou!\nPre-processeddataset\nNoiseaddedtothegradientsofalldatapointswitha<MASK>Andalldatapointsselectedbyapolicywithnearlyperfectrecall.\nSelectivenoise-addingDP-SGD<MASK><MASK>\n<MASK>\n<MASK>\n<MASK>\n<MASK>\nwithprovableconfidentiality\n<MASK>\nGPT-2\nFigure 7: An illustration of our proposed algorithm on a dataset with two data points. The first data point is the\nexample from Figure 1, and the second data point is modified to illustrate the various aspects of the pre-processing\nsteps. The red-colored mask indicates the masks produced by deduplication just for illustration purposes. In the\nalgorithm, both of them replace a sequence of tokens with the same special token <MASK>.\nA.2 Proofs of technical results\nProof of Proposition 5. The first statement straigtforwardly follows from that Redactπ(D) =\nRedactπ(D′) if π(s,x) = 1and that Redactπ(D) and Redactπ(D′) remain a pair of neighbors differing\nby only x. The group confidentiality claims follows from the standard calculation of small group privacy\nfrom differential privacy, which applies the (singlex) confidentiality iteratively. Let ˜D= Redactπ(D),\n˜D′= Redactπ(D′) and ˜S = [x1,...,x ˜k] be the list of Sthat are not masked by π. For any measurable\nevent E\nP[A◦Redactπ(D) ∈E] =P[A( ˜D)] ≤eϵx1P[A( ˜D−x1,+<MASK>) ∈E] +δ\n≤eϵx1+ϵ(x2)P[A( ˜D−x1,2,+<MASK>2) ∈E] +eϵx1δ+ δ\n...\n≤e\n∑˜k\ni=1 ϵxiP[A( ˜D′) ∈E] +δ(1 +eϵx1 + eϵx1+ϵx2 + ...+ eϵx1+...+ϵx˜k−1)\n≤e˜ϵ(S)P[A◦Redactπ(D′) ∈E] +ke˜ϵ(S)δ\nProof of Proposition 6. Consider a dataset D(in which one of the data point has x∼µ) and a fixed D′.\nDenote the probability distributions p,q,r as shorthands for\np∼A◦ Redactπ(D)|π(s,x) = 1\nq∼A◦ Redactπ(D)|π(s,x) = 0\nr∼A◦ Redactπ(D′)|π(s,x) = 0\nMoreover, we use αp+ (1−α)qto denote the mixture distribution that samples from pwith probability\nαand qwith probability 1 −α.\nRecall that the Hockey-Stick-divergence characterization of (ϵ,δ)-indistinguishsability (Barthe and\nOlmedo, 2013), which says that (P,Q) are (ϵ,δ)-indistinguishsable if and only if\nHeϵ(P∥Q) :=Ey∼Q[(dP\ndQ(y) −eϵ)+] ≤δ.\n953\nIt suffices for us to bound the following quantity:\nH1+γ(eϵ−1)(A◦Redactπ(D)∥A◦Redactπ(D′)) =Heϵ((1 −γ)p+ γq∥(1 −γ)p+ γr)\n=γHeϵ(q∥(1 −β)p+ βr) ≤γ((1 −β)Heϵ(q∥p) +βHeϵ(q∥r))\nwhere β = 1+γ(eϵ−1)\neϵ . In the above, the second line follows from Theorem 2 of (Balle et al., 2018) (an\nidentity called “Advanced Joint Convexity” by the authors) and the inequality is due to the (standard) joint\nconvexity of the Hockey-Stick divergence. It remains to bound Heϵ(q∥p) and Heϵ(q∥r).\nCheck that p,r,A◦Redactπ(D′) are identically distributed and that Heϵ(q∥r) ≤δby our assumption\non A’s Bayesian confidentiality guarantee w.r.t. µ(x|π(s,x) = 0). This completes the proof.\nProof of Proposition 7. The proof is straightforward as Dedup(D) differs from Dedup(D′) only by\nUnique(S).\nProof of Theorem 8. The proof for the first statement follows from the fact that DP implies (ϵ,δ)-\nconfidentiality and Proposition 5. Notably, if πc catches all x that is missed by π, then we get that\nfor all secret x, ϵ(x) ≤ϵ.\nThe proof of the second statement applies Proposition 7 and the second part of Proposition 5.\nThe proof of the third statement applies Proposition 6 but requires a separate treatment of the case when\nxis missed by both πand πc. Let the event that a secret xis not selected by the conservative policy be E\nand let Abe a generic algorithm satisfying (ϵ,δ1) Bayesian confidentiality under µ,\nP[A(D) ∈S] ≤P[A◦Redactπ(D) ∈S ⊂Ec] +δ\n≤eϵP[A(D′) ∈S ⊂Ec] +δ1 + δ2\n≤eϵP[A(D′) ∈S] +δ1 + δ2.\nThis completes the proof.\nA.3 More details on experiments\nWe choose the one-layer LSTM with an embedding size of 200 and a hidden size of 200. We choose\ndistill-gpt27 as the GPT-2 model, which has 6 layers, 768 dimension and 12 heads. V ocabulary size\nfor GPT-2 is 50257. Our experiments are conducted on NVIDIA TITAN-Xp GPU. For LSTM models,\nwe tune the hyperparameters of the learning rate (lr) among {20, 10, 5, 1, 0.1, 0.05, 0.01}, batch size\n(bs) and the epochs among {5, 10, 30, 50, 100}. We finally choose {lr=20, bs=256, epochs=50} for\nNon-private-LSTM, {lr=0.1, bs=5, epochs=50} for S-DPSGD-LSTM and {lr=0.05, bs=10, epochs=100}\nfor CRT-LSTM. The same set of hyperparameters are tuned for GPT model as well. Our final choice\nfor DPSGD-GPT/CRT-GPT model is {lr=5e-4, bs=256, epochs=10}. The actual run-time of algorithms\ndepends on implementation details. Here, we outline estimates of the run-time for training. Running one\nepoch on CRT-LSTM takes 2 hours wheras the same task on CRT-GPT only takes 30 minutes since the\nimplementation of Li et al. (2021) is highly efficient. We use autodp8, an automating differential privacy\ncomputation for the privacy analysis. Noise scale σis calculated numerically so that a DP budget of (ϵ,δ)\nis spent after T epochs.\nA.4 Redaction policy details\nWe build the sequence labeling policy based on trimming one NER model 9 trained on OntoNotes-5.0\n(Weischedel et al., 2013) dataset. We modify the last layer of the NER model and set the threshold for the\noutput scores to enable abnormal/sensitive data detection. For the screen policy π, we set the threshold to\nbe 0.3 for all predictions with OntoNotes tags. For the conservative policy πc, we select all predictions\nwith tags and all plain texts with scores smaller than 0.9 to be sensitive data. We manually label 200 data\npoints and find that the conservative policy πc can achieve 100% recall with lots of false positives and that\nπcan achieve 90% recall with few false positives.\n7https://huggingface.co/distilgpt2\n8https://github.com/yuxiangw/autodp\n9https://huggingface.co/flair/ner-english-ontonotes-fast\n954\nA.5 Membership inference attack details\nIn our experiments, we manually construct a dataset with 2000 sequences. We select 1000 sequences from\nthe protected secrets used in the training data. And we randomly generate 1000 samples of similar format\nwhich are not used in the training data. In this way, a random guess generates an accuracy of 50%. For\nMultiWoz 2.2, we use sentences with reference numbers as the secrets. For CustomerSim, we choose\ncustomer addresses as the secrets.\nIn order to show group confidentiality guarantees, we also conduct group membership inference attack.\nIn this setting, we construct a dataset with 2000 groups, each of which includes 20 sentences. One half of\nthe groups are “sensitive groups\" with all 20 sentences drawn from protected secrets and the other half are\n\"insensitive groups\" with all 20 sentences being random. We build the classifier based on the sum of the\nperplexities in one group.\nA.6 “The devil is in the details” – how things could go wrong with seemingly inocuous changes to\nthe algorithm.\nIn this section, we highlight various aspects of our algorithms and why certain choices in the pre-processing\nsteps need to be done in the specific way we recommend for our results to hold for them.\n1. It is important that the definition of confidentiality is defined with respect to a perfectly redacted\nversion of the dataset. If we define it as in selective differential privacy, then there will not be an\namplification effect from redaction. This is because if we replace a secret xthat can be detected\nby πwith another x′that cannot be detected by π, then even if xis replaced with <MASK>, x′will\nnot be and the two datasets are still different after redaction. In addition, the S-DP definition will\nnot be useful for us we do not know how to define a confidentiality parameter specific for each xor\nBayesian confidentiality parameter for each µ\n2. Tokenization and splitting into individual “sentences” (data points) should go before redaction / de-\nduplication. Otherwise redaction with an approximate screening policy and with an ideal screening\npolicy, or deduplication may cause misalignments, resulting in almost all data points being different\nin the preprocessed version of Dand D′.\n3. Each data point should contain only “whole” natural sentences, otherwise the sensitive part of a\nnatural sentence could split into two data points.\n4. Deduplication steps should replace duplicate text with the same <MASK>, otherwise\n<MASK_Dedup> and <MASK_Redact> are not the same so even if all secrets are masked, there\nwill be a difference between the pre-processed versions of Dand its neighbor, while in our approach\nthere are no differences and we achieve perfect confidentility (with ϵ= 0).\n5. Any data point containing <MASK> needs to be put in Dpri. This is because otherwise our algorithm\nthat works onD′will be a deterministic algorithm that is perfectly distinguishable from the alternative\nworld where the algorithm is random because the approximate policy πfails to redact certain secrets\nx.\n6. In the DP-SGD algorithm, the sampled minibatches should contain the whole minibatch from Dpri\nor the whole minibatch from Dpub. Otherwise the noise always need to be added and the algorithm is\nidentical to the vanilla DP-SGD, and there is no benefit of having a portion of the data being public\ncomparing to all of the data are private.\n955",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8971922397613525
    },
    {
      "name": "Computer science",
      "score": 0.840829610824585
    },
    {
      "name": "Confidentiality",
      "score": 0.7786079049110413
    },
    {
      "name": "Language model",
      "score": 0.6924062371253967
    },
    {
      "name": "Memorization",
      "score": 0.6666169762611389
    },
    {
      "name": "Process (computing)",
      "score": 0.5274180173873901
    },
    {
      "name": "Differential privacy",
      "score": 0.5156885385513306
    },
    {
      "name": "Redaction",
      "score": 0.4762183129787445
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.4689333736896515
    },
    {
      "name": "Scale (ratio)",
      "score": 0.45875951647758484
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44024717807769775
    },
    {
      "name": "Natural language processing",
      "score": 0.39141958951950073
    },
    {
      "name": "Computer security",
      "score": 0.37952905893325806
    },
    {
      "name": "Data mining",
      "score": 0.28440481424331665
    },
    {
      "name": "Programming language",
      "score": 0.16855737566947937
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    }
  ]
}