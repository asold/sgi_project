{
  "title": "An End-to-End Air Writing Recognition Method Based on Transformer",
  "url": "https://openalex.org/W4387350588",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5091703741",
      "name": "Xuhang Tan",
      "affiliations": [
        "Waseda University"
      ]
    },
    {
      "id": "https://openalex.org/A5051076104",
      "name": "J. Tong",
      "affiliations": [
        "Waseda University"
      ]
    },
    {
      "id": "https://openalex.org/A5005634226",
      "name": "Takafumi Matsumaru",
      "affiliations": [
        "Waseda University"
      ]
    },
    {
      "id": "https://openalex.org/A5029157597",
      "name": "Vibekananda Dutta",
      "affiliations": [
        "Warsaw University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5032967539",
      "name": "Xin He",
      "affiliations": [
        "Waseda University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2080116634",
    "https://openalex.org/W3139330439",
    "https://openalex.org/W3047741592",
    "https://openalex.org/W4321607994",
    "https://openalex.org/W3004690717",
    "https://openalex.org/W2033154814",
    "https://openalex.org/W2614376571",
    "https://openalex.org/W1997086460",
    "https://openalex.org/W2065434270",
    "https://openalex.org/W2342966107",
    "https://openalex.org/W2129114382",
    "https://openalex.org/W3210206767",
    "https://openalex.org/W6637319697",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W1923211482",
    "https://openalex.org/W2156279557",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W6729983426",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2972369255",
    "https://openalex.org/W3197522112",
    "https://openalex.org/W3009828227",
    "https://openalex.org/W6849704378",
    "https://openalex.org/W2089513492",
    "https://openalex.org/W4312375927",
    "https://openalex.org/W3086649583",
    "https://openalex.org/W3136789123",
    "https://openalex.org/W3032325918",
    "https://openalex.org/W2343424255",
    "https://openalex.org/W6810263219",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963066927",
    "https://openalex.org/W3113709932",
    "https://openalex.org/W2989930793",
    "https://openalex.org/W2102593897",
    "https://openalex.org/W2948805184",
    "https://openalex.org/W2094536769",
    "https://openalex.org/W4205326599",
    "https://openalex.org/W2788919350",
    "https://openalex.org/W2090203103",
    "https://openalex.org/W2921232547",
    "https://openalex.org/W3119429060",
    "https://openalex.org/W2999040843",
    "https://openalex.org/W2091017714",
    "https://openalex.org/W2766703716",
    "https://openalex.org/W2178158312",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2954996726",
    "https://openalex.org/W4220710236",
    "https://openalex.org/W4321021111",
    "https://openalex.org/W3099884890",
    "https://openalex.org/W4319777935",
    "https://openalex.org/W1673518641",
    "https://openalex.org/W4288335398",
    "https://openalex.org/W2949846184",
    "https://openalex.org/W1918878574",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "The air-writing recognition task entails the computer&#x2019;s ability to directly recognize and interpret user input generated by finger movements in the air. This form of interaction between humans and computers is considered natural, cost-effective, and immersive within the domain of human-computer interaction (HCI). While conventional air-writing recognition has primarily focused on recognizing individual characters, a recent advancement in 2022 introduced the concept of writing in the air (WiTA) to address continuous air-writing tasks. In this context, we assert that the Transformer-based approach can offer improved performance for the WiTA task. To solve the WiTA task, this study formulated an end-to-end air-writing recognition method called TR-AWR, which leverages the Transformer model. Our proposed method adopts a holistic approach by utilizing video frame sequences as input and generating letter sequences as outputs. To enhance the performance of the WiTA task, our method combines the vision transformer model with the traditional transformer model, while introducing data augmentation techniques for the first time. Our approach achieves a character error rate (CER) of 29.86&#x0025; and a decoding frames per second (D-fps) value of 194.67 fps. Notably, our method outperforms the baseline models in terms of recognition accuracy while maintaining a certain level of real-time performance. The contributions of this paper are as follows: Firstly, this study is the first to incorporate the Transformer method into continuous air-writing recognition research, thereby reducing overall complexity and attaining improved results. Additionally, we adopt an end-to-end approach that streamlines the entire recognition process. Lastly, we propose specific data augmentation guidelines tailored explicitly for the WiTA task. In summary, our study presents a promising direction for effectively addressing the WiTA task and holds potential for further advancements in this domain.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2017.DOI\nAn End-to-End Air Writing Recognition Method\nBased on Transformer\nXUHANG TAN1, JICHENG TONG1, Takafumi Matsumaru1, (Senior Member, IEEE), Vibekananda\nDutta2, (Member, IEEE), XIN HE1, (Student Member, IEEE)\n1Graduate School of Information, Production and Systems, Waseda University, 2-7 Hibikino, Wakamatsu-ku, Kitakyushu, Fukuoka 808-0135\n2Institute of Micromechanics and Photonics, Faculty of Mechatronics, Warsaw University of Technology, 00-661 Warsaw, Poland\nCorresponding author: Tan Xuhang (e-mail: tanxuhang2021ips@akane.waseda.jp)\nThis study is partially supported by JSPS KAKENHI Grant Number JP22K04034 and Waseda University Grant for Special Research\nProject Numbers 2023C-496, 2022C-183 and 2021C-589, for which we would like to express our sincere gratitude.\nABSTRACT The air-writing recognition task entails the computer’s ability to directly recognize and interpret\nuser input generated by finger movements in the air. This form of interaction between humans and computers\nis considered natural, cost-effective, and immersive within the domain of human-computer interaction (HCI).\nWhile conventional air-writing recognition has primarily focused on recognizing individual characters, a\nrecent advancement in 2022 introduced the concept of writing in the air (WiTA) to address continuous air-\nwriting tasks. In this context, we assert that the Transformer-based approach can offer improved performance\nfor the WiTA task.\nTo solve the WiTA task, this study formulated an end-to-end air-writing recognition method called TR-\nAWR, which leverages the Transformer model. Our proposed method adopts a holistic approach by utilizing\nvideo frame sequences as input and generating letter sequences as outputs. To enhance the performance\nof the WiTA task, our method combines the vision transformer model with the traditional transformer\nmodel, while introducing data augmentation techniques for the first time. Our approach achieves a character\nerror rate (CER) of 29.86 % and a decoding frames per second (D-fps) value of 194.67 fps. Notably, our\nmethod outperforms the baseline models in terms of recognition accuracy while maintaining a certain level\nof real-time performance. The contributions of this paper are as follows: Firstly, this study is the first\nto incorporate the Transformer method into continuous air-writing recognition research, thereby reducing\noverall complexity and attaining improved results. Additionally, we adopt an end-to-end approach that\nstreamlines the entire recognition process. Lastly, we propose specific data augmentation guidelines tailored\nexplicitly for the WiTA task. In summary, our study presents a promising direction for effectively addressing\nthe WiTA task and holds potential for further advancements in this domain.\nINDEX TERMS Air writing recognition, Transformer model, Human-computer interaction (HCI)\nI. INTRODUCTION\nA. AIR WRITING AND ITS APPLICATION\nW\nITH the rapid development of computers, human-\ncomputer interaction (HCI) technology is constantly\nupdating and has the trend of transforming traditional human-\ncomputer interaction to natural human-computer interaction.\nTraditional human-computer interaction generally refers to\npeople issuing commands to machines in different ways,\nincluding touch, speech, typing, and more, using devices such\nas mice, keyboards, microphones, and touchscreens [1]. After\nreceiving the information, the machine provides feedback to\nthe user’s commands, including performing specific actions\nor providing appropriate information.\nTraditional human-computer interaction technology mainly\nrelies on hardware devices, which limits the flexibility of\nHCI, increases the cost, and requires specialized learning,\nit is not a natural way to interact and cannot meet people’s\nincreasing needs. Natural HCI is more closely integrated\nwith people’s daily experiences than traditional HCI. In\ndaily human-to-human communication, people communicate\nwith other humans through voice, gestures, etc., and gather\ninformation around them through their senses such as sight\nand touch. The core of natural human-computer interaction is\nto allow people to interact with computers in the same way\nthat they interact with each other in their daily lives. With the\nrapid development of Sensor Systems, various new intelligent\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ndevices have been developed, such as virtual reality (VR)\nheadsets and augmented reality (AR) glasses [2]. The rapid\nspread of these devices has directly led to a rapid increase\nin the demand for natural human-computer interaction to\nreplace traditional human-computer interaction.\nThe hand is the most flexible part of the human body\nand can convey information more quickly and conveniently\nduring the interaction process with computers or other intelli-\ngent devices. Therefore, hand gesture recognition technology\nhas gradually become a popular human-computer interaction\nmethod. On the other hand, text is one of the most important\nmedia for modern human communication [3]. Compared with\nother forms of information carriers, text can carry a wide va-\nriety of information and transmit it accurately and efficiently\nto other devices. Air writing is a human-computer interaction\nmethod that combines gesture interaction with text. As a\nnatural method of human-computer interaction, it holds great\npotential.\nAir-writing refers to a computer directly recognizing the\nactions of a person’s fingers in the air [4]. Compared with\ntraditional human-computer interaction methods, air-writing\nrecognition technology allows users to perform six degrees\nof freedom of movement and write in the air in a natural,\nunconstrained gesture, thereby providing a more intuitive,\nconvenient, and comfortable HCI method [5]. Unconstrained\nmovements also represent the ability to carry more informa-\ntion, with a learning cost far lower than that of a keyboard,\nwhich can help people such as the disabled, the elderly, and\nchildren to interact with computers more conveniently [6].\nAs a science-fiction-like interaction method, air-writing\nhas broad applications in smart homes, education, virtual real-\nity (VR), and other fields. In the future, it could be integrated\ninto all aspects of human life by facilitating the following\npractical scenarios:\n(a) In a smart home system, when a user wants to search\nfor a movie on a smart TV, instead of using a remote control\nto enter and select each letter, the user writes the English ab-\nbreviation of the movie in the air, and the signal is transmitted\nto the TV via a camera, completing the typing and search;\n(b) In a preschool education scenario, children can learn\ndifferent vocabulary through air writing technology, they can\nalso play the game Write and Guess with the teacher using\nair-writing, and the system judges correctness by, combining\neducation with entertainment;\n(c) Air writing recognition can also be used in situations\nwhere direct text input is not convenient, such as when driv-\ning, where it allows the driver to reply to simple messages\nin a noisy environment, and in situations where small-screen\nelectronic devices are used for input (e.g., smart watches), it\nallows the user to easily enter text messages. Figure 1 shows\na simple example diagram of Air-writing recognition.\nB. WRITING IN THE AIR (WITA) TASK\nDepending on the environment, the device, or the method\nof data input, Air-writing recognition can occur in various\nforms, such as recognizing individual characters as men-\nFIGURE 1. An example of Air-writing, the volunteers is writing \"Much\".\ntioned earlier. However, we aim to solve a different type of\ntask called the writing in the Air (WiTA) task [7], which has\nthe following features:\n(a) Continuous air-writing recognition: Unlike previous\ntasks, in the WiTA task, users can write multiple characters\ncontinuously without waiting for the computer to recognize\nthe previous character. This can greatly increase recognition\nefficiency.\n(b) Use of visual information: Visual sensors are the most\ncommon type of sensors, and methods based on visual infor-\nmation can be cost-effective and easily applied in real-time\nsituations. Technological developments in computer vision\nand deep learning provide even better solutions. For freehand\nair-writing recognition, visual-based methods are the current\ntrend, far outmatching sensor-based methods. Visual sensors\nare ubiquitous and can be found on devices such as laptops\nand smartphones, making them simple and convenient data\ninput devices. In practical environments, visual sensors can\nbe deployed more easily and have the advantages of low cost,\nlow learning cost, and non-contact measurement. Further-\nmore, deep learning models based on visual information have\nshown excellent results in recent years and can be applied\nto many related tasks [8]. Therefore, this study used visual\ndevices as data input.\n(c) Max identification unit is word: In the WiTA task, the\nmaximum recognition unit is a word or sequence of charac-\nters. The evaluation criteria for the WiTA task differed, allow-\ning for the recognition of more comprehensive information\n[9]. This sequence can be meaningful words or meaningless\nletter sequences; therefore, for our method, it is not possible\nto segment an entire sentence. The minimum format for the\ninput and output was a letter sequence.\nIt is worth mentioning that in this task, we did not extend\nthe method to real-time air-writing recognition, but ensured\na certain level of real-time performance by calculating the\ndecoding frames per second (D-fps). This reflects the speed\nof the algorithm to a certain extent.\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nC. RESEARCH CHALLENGES OF CONTINUOUS AIR\nWRITING RECOGNITION\nIn previous methods for solving air-writing recognition prob-\nlems, the unit of recognition was the individual characters. In\ncontrast to the recognition of individual characters, continu-\nous air-writing recognition has other unique characteristics,\nthat pose significant challenges in the process of solving air-\nwriting recognition problems. These challenges include the\nfollowing.\n(a) Poor data readability. For non-visual data, the sensor\ncollects acceleration signals, angular velocity signals, or other\nnon-textual signals, which are not spatial trajectories. It is\ndifficult to distinguish the specific content of handwriting\nwith the naked eye. Fig.2 shows a handwritten trajectory\ndiagram, which makes it difficult to distinguish the original\nwritten content. This causes difficulties in data cleaning, sam-\nple segmentation, sample labeling, and other post-processing\noperations. For visual devices, if non-trajectory data such as\nspatial coordinates are obtained, this problem will also exist.\nThere are two main reasons for such issues in air-writing\nrecognition.\nFirst, information about the lifting and landing of the pen\nwas missing. For sensors based on non-visual information,\ncharacters on the signal waveform are indistinguishable, and\nthe connecting strokes are merged with the characters, mak-\ning it difficult to perform character-level segmentation on\ncontinuous handwritten strings in the post-processing stage.\nIn trajectory recognition methods, the connecting parts be-\ntween letters can also affect the recognition results, making it\nsometimes difficult to distinguish whether a certain trajectory\nis part of a letter or a connecting stroke.\nSecondly, there is interference from similar strokes. Some\ncharacters with similar strokes are prone to misidentification,\nsuch as \"a\" and \"d\", \"i\" and \"l\". For non-visual information,\nthe difference in acceleration or angular velocity signals be-\ntween writing different letters may not be significant. For tra-\njectory recognition, the trajectories between different letters\ncan also be easily confused, posing a challenge for air-writing\nrecognition.\nFIGURE 2. The Air-writing track sometimes is hard to recognize (newly\ncreated using the figure in [9] as material).\n(b) Differences in user habits. Different people always\nhave different writing habits, including stroke order, the range\nof hand movements, the strength and speed of writing, etc.\n[10]. These differences are ultimately reflected in the dif-\nferences in amplitude, frequency, and trend of the sensor\nsignal waveform or different letter-writing methods, which\nstill exist in samples of the same category, bringing certain\nchallenges to model recognition. Fig.3 shows the different\nways of writing the letters E, F, X, and N.\nFIGURE 3. Different people have different habits for writing the same\nletters (newly created using the figure in [5] as material).\n(c) Overlapping of handwriting space. In daily life,\nwhen people write on paper, they refer to what they have\nwritten before to arrange letters in spatial order [11]. How-\never, the characteristic of air writing is that people cannot\nknow the previous writing content. Therefore, for trajectory\nrecognition-based methods, once there are many characters\nto be detected, the pattern of each character will stack up,\naffecting the pattern recognition result. Figure 4 shows the\nstacking of strokes that occurs with continuous writing.\nFIGURE 4. Stroke stacking occurs during continuous writing (newly\ncreated using the figure in [5] as material).\nD. PAPER CONTRIBUTIONS\nThis article proposes an end-to-end air writing recognition\nmethod based on transformer models (TR-AWR), which has\nthe following contributions:\n(a) We introduce an end-to-end framework that directly\ntakes video frame sequences as input and delivers character\nsequences as output, significantly reducing the complexity of\nthe problem and avoiding information loss caused by inter-\nmediate processing steps. By training on a large benchmark\ndataset, we can mitigate some traditional problems of air\nwriting to some extent.\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n(b) To solve the WiTA task, we employed the transformer\nmodel for the first time, which demonstrates significant con-\ntributions in other sequence-to-sequence studies. The experi-\nmental results exhibit, the proposed method outperforms the\nbaseline models in the WiTA task.\n(c) We propose data augmentation principles to prevent\nincorrect methods from affecting the final results. By using\nappropriate data augmentation methods, we aim to reduce the\nimpact of irrelevant features on the results and increase the\nsize of the dataset, which allows us to improve the accuracy\nof the method further.\nE. PAPER OUTLINE\nThis study has five main parts. Section I introduces the back-\nground of the problem, past solutions, technical difficulties,\nand contributions. Section II introduces the proposed method,\nbaseline model, and contributions. Section III describes the\nimplementation of the TR-AWR method. Section IV intro-\nduces the dataset used in the study and the specific steps of the\nexperiments, and analyzes the experimental results. Section\nV presents the conclusions and discusses the possibility of\nfurther deepening this study.\nII. RELATED WORK\nA. AIR-WRITING RECOGNITION BASED ON NON-VISUAL\nSENSORS\nNon-visual-based air-writing recognition mainly refers to the\nuse of wearable devices to obtain motion information from\nuser actions, such as inertia (2021) [12], acceleration (2011)\n[13], electromyography (EMG) signals (2023) [14], and WiFi\nsignals (2020) [15]. C. Amma et al. developed an air-writing\nrecognition system based on wearable gloves and inertial\nsensors [16], which used the signal from the inertial sensor\nas the input and used Hidden Markov Models (HMMs) to\nclassify 26 English letters, achieving a recognition rate of\n81.9%, C. Duffner et al. further integrated this method into\nsmartphones(2014) [17], making it convenient for signature\nrecognition and handwriting authentication.\nA. Akl et al. proposed a motion sensor-based system, using\nsensors worn on the fingertips to record information such as\nvelocity and acceleration to recognize hand gestures (2011)\n[18]. Y. Fang et al. proposed a WiFi signal-based method,\nusing WiFi signal disturbances caused by hand movements\nfor gesture recognition (2020) [15]. A. Tripathi et al. proposed\na wearable data glove that can capture Surface Electromyog-\nraphy signals (MSE) from the user’s body to achieve a series\nof human-computer interaction functions [14], including air-\nwriting recognition. These comprehensive and diverse meth-\nods of air-writing recognition utilize analyzable data gener-\nated by human movement from multiple perspectives.\nB. AIR-WRITING RECOGNITION BASED ON VISUAL\nSENSORS\nWith the development of computer vision technology, meth-\nods for air-writing recognition based on visual devices have\ngradually become more diverse and mature [19]. These meth-\nods primarily use visual devices to capture user-related in-\nformation. The most common approach in this category is to\nobtain a user’s hand or pen movement trajectory. The earliest\nattempt in this field was made in 1995 by S. Nabeshima\net al., who proposed the MEMO-PEN system (1995) [20],\nwhich is a device equipped with a camera, pressure sensor,\nand microcomputer. The camera can capture each frame of\nthe written image, and a pressure sensor is used to determine\nwhether the pen is lifted. The trajectory of the pen tip is\nobtained via image processing. N. Xu et al. (2015) proposed\na novel 3D interaction method that can recognize airborne\nhandwritten Chinese characters using Leap Motion to accu-\nrately capture fingertip motion trajectories [21], achieving a\nsingle-character recognition rate of 90.6 %.\nFor air-writing English word recognition, J. Gan et al. built\nan air-writing recognition system based on attention models,\nproviding sufficient samples for training robust classifiers.\nThey proposed a novel and effective neural network archi-\ntecture based on an attention mechanism to recognize words\n(2017) [22], ultimately achieving a recognition accuracy of\n97.74%. S. Mohammadi et al. implemented a real-time air-\nborne system based on Kinect, using K-means clustering to\ndetect fingertips and eliminate depth image noise (2019) [23].\nThey proposed a slope-change detection method to describe\ncharacter features. C. Lee et al. proposed a visual fingertip\nhandwriting recognition system, that uses motion and skin\ncolor detection to complete gesture segmentation. They then\ndetect fingertips based on finger contour features and pro-\nposed an improved deep neural network to recognize hand-\nwritten characters (2010) [24]. In addition to trajectory recog-\nnition, M. S. Alam et al. used a depth camera to recognize\nair-writing by tracking the three-dimensional data changes of\nfingertip positions in space (2020) [25], A. Rahman et al.\nproposed a bone recognition-based method that uses hand\nkey points and joint angles as input data and an RNN-LSTM\nstructure to recognize handwritten characters (2021) [26].\nCurrently, air-writing recognition based on visual devices\nis a popular research topic that is closely integrated with the\ndevelopment of computer vision technology and has achieved\nincreasingly better results.\nC. PREVIOUS WORK ON CONTINUOUS AIR-WRITING\nRECOGNITION\nAs explained earlier, the WiTA task is a newly proposed\nproblem in 2022. The only previous work on this problem\nwas the baseline model presented in our article.\nIn 2022, a team of Korean researchers proposed an un-\nconstrained air-writing recognition method using Spatio-\nTemporal Convolution as a baseline for this study [9]. This\npaper presents an end-to-end approach that directly inputs\nvideo frame data to output a sequence of letters. The au-\nthors designed a Spatio-Temporal Convolution deep learning\nnetwork and proposed a judgment criterion based on the\nCharacter Error Rate (CER) to adapt to the serialized output,\ncovering both English and Korean Air-writing recognition.\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFor English Air-writing recognition, the authors proposed\nthree models: mixed 3D-2D reversed MC (ST-rMC), residual\n3D convolutions (ST-R3D), and 2D convolutions followed\nby 1D convolutions (ST-R(2+1)D). The baseline model is\nthe first to adopt an end-to-end approach to solve the Air-\nwriting recognition problem and propose an evaluation cri-\nterion based on Character Error Rate (CER). However, the\nresults of the three proposed models showed that the lowest\nCER data were still over 30 %, indicating that there is signifi-\ncant room for improvement in accuracy. Therefore, this study\naims to inherit the end-to-end approach and use different\ndeep-learning methods to achieve better results.\nIII. PROPOSED METHOD\nWe propose a Transformer-based End-to-End air writing\nrecognition method (TR-AWR), which is a concise method\nconsisting of two main parts. The first part is data augmenta-\ntion, and the second part is a deep learning network based on\nthe transformer. It directly takes the video frame sequence as\ninput and the character sequence as the output and calculates\nthe CER value between the output and true sequences. Fig.\n5 Shows the overall structure of the TR-AWR method, In\nthe TR-AWR solution, the training part contains three main\ncomponents, (1) data augmentation, which we only use in\nthe training set, (2) Transformer-based deep learning model,\nand (3) is the calculation of Connectionist Temporal Classifi-\ncation (CTC) Loss. In the testing part, we used the trained\ntransformer-based deep learning model to translate words\nwritten in the air.\nA. DATA AUGMENTATION\nData Augmentation is the process of generating more repre-\nsentations from original data without substantially increasing\nthe amount of data. The goal is to improve both the quantity\nand quality of the original data in order [27], to approximate\nthe value of having more data. It is important to note that some\ngeneral principles should be followed when using data aug-\nmentation because incorrect methods can negatively impact\nthe final results. First, data augmentation should not generate\nincorrect data, such as by changing the labels of the original\ndata or altering too many elements in the data. Second, the\ndata should not focus on irrelevant features [28].\nBased on the characteristics of air writing recognition, we\npropose the following principles for data augmentation for\nthis task:\n(a) Retaining important parts: The information is mainly\ncarried by hand movements in the frame sequence of air-\nwriting videos. This can be divided into two parts: the hand\nmovements in a specific image, and the overall movement\ndirection of the hand in a sequence of images. Regardless\nof the data augmentation method used, both parts of the\ninformation must be preserved.\n(b) Using sequence as the basic unit: Because the min-\nimum data unit in this study is a sequence of video frames,\ndata augmentation should be performed using sequences as\nthe basic unit. The same data augmentation method should\nbe applied to each image within a sequence, whereas dif-\nferent data augmentation methods should be used randomly\nbetween different image sequences. This is done to ensure\nthat the machine learning model focuses on hand movements\nwithin a single video frame sequence, rather than identifying\ndifferent data augmentation methods as features.\nGiven the two principles of data augmentation mentioned\nabove, the data augmentation methods chosen in this study\ndiffer from the traditional approaches. First, mirror augmen-\ntation was not used because the direction of hand movement\ncarries a crucial meaning in air writing videos. Mirroring\nall video frames alters the hand trajectory between different\nframes, effectively changing the actual air writing content.\nIf the ground truth remains the same, incorrect data will\nbe introduced, thus affecting recognition accuracy. Similarly,\nmethods such as frame sampling, random frame flipping,\ntime warping, and interpolation were not utilized [29]. An-\nother commonly used data augmentation method for videos\nis data mixing. However, because it is difficult to visually\ndetermine the font of the air writing, it is impossible to define\nthe textual content written before and after a certain frame.\nConsequently, data augmentation through label mixing based\non the video content cannot be achieved.\nNext, we introduce the selected data augmentation methods\nin this experiment and describe how these methods reduce the\ninfluence of irrelevant features on recognition:\n(a) Rotation: The images were appropriately rotated to\nsimulate the different postures commonly used by individuals\nduring air writing. Notably, in general, in air-writing practice\napplications, people interact with computers while sitting or\nstanding, maintaining a vertical relationship with the ground.\nTherefore, we limited the angle range of rotation to ensure\ndata plausibility. This data augmentation method enhances\nthe recognition accuracy of the algorithm for air writing\ncontent with different arm angles and postures, reducing the\nimpact of irrelevant features and improving the robustness of\nthe model.\n(b) Shift: A portion of the image was cropped and shifted,\nand the remaining parts were reassembled to maintain the size\nof the image. It is worth noting that because the individuals\nin the source data are generally positioned in the middle of\nthe image, the range of cropping can be limited to preserve\nthe integrity of the hand. Through this data augmentation\nmethod, the algorithm can better recognize the content written\nby volunteers in different positions within the frame, directing\nits attention to hand movements and reducing the influence of\nirrelevant features.\n(c) HSV color change: The HSV color model is a color\nspace created based on the intuitive characteristics of colors,\nand is also known as the hexagonal cone model [30]. The\nparameters in this model are the hue (H), saturation (S),\nand value (V). The HSV color space is more accurate for\nperceiving colors than the traditional RGB color space. The\nform of the image was altered by randomly changing these\nthree parameters. Through this data augmentation method,\nthe attention of the algorithm can be focused more on the\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 5. Overall structure of TR-AWR method, which includes (a) train part and (b) test part. The train part(a) includes (1) Data augmentation, (2)\nTransformer-based deep learning model, and (3) CTC loss function. As for the test part(b), it only involves the trained Transformer-based deep learning\nmodel.\nhand’s movement, reducing the impact of different colors and\nlighting conditions on the results [31].\n(d) Blur: Image blurring is a common data augmenta-\ntion method. Proper blurring can effectively disregard high-\nfrequency patterns and simulate the effects of different cam-\neras in real environments, thereby enhancing the robustness\nof the model.\n(e) Noise: Gaussian noise refers to a type of noise whose\nprobability density function follows a Gaussian distribu-\ntion (i.e., normal distribution). By adding Gaussian noise to\nthe image, deep neural networks can ignore certain high-\nfrequency patterns that are not useful [32].\nData augmentation methods used in this study can be di-\nvided into two categories. First, all data will be randomly\nmirrored or shifted, with each sequence receiving exactly the\nsame processing, and the ratio of the processed data to the\noriginal data is 1:1. Second, all the data will be randomly\nadded with blur, noise, or HSV color changes, and each\nsequence of images will receive the same processing, with the\nratio of the processed data to the original data being 1:1:1. In\ntotal, the data augmentation process results in three times the\namount of data.\nIn conclusion, we believe that the aforementioned methods\ncan help increase the dataset while preserving essential parts\nof the images, thereby enhancing the robustness of the model.\nThese methods were carefully selected, following the two\nprinciples of data augmentation for this task, to ensure that\ndata augmentation did not have negative effects on the exper-\niment. Figure 6 shows five methods of data augmentation.\nFIGURE 6. Five data augmentation methods (The images are taken from\nWiTA dataset [9]).\nB. TRANSFORMER-BASED DEEP LEARNING MODEL\nThis study utilizes a neural network model based on trans-\nformer architecture, consisting of an image transformer for\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nextracting visual features and a traditional text transformer for\nlanguage modeling. The structure of the article is based on a\nbasic transformer encoder-decoder structure. The encoder is\ndesigned to extract visual features from image patches, and\nthe decoder generates character sequences based on the visual\nfeatures and previous predictions. The deep learning model\nmainly includes three steps: Figure 7 shows the specific\nstructure of the transformer-based deep learning model in this\nstudy.\n1) Resize\nThe WiTA task is a task that converts image sequences\nto character sequences. For the encoder part of the model,\nwe referred to the ViT model’s solution [33]. Because the\ntransformer encoder cannot process image data, we input the\nentire image sequence and applied a fixed size to each image.\nFinally, the image is converted into a vector for input. The\ndimensions of the vector are related to the size of the image\ndata, and data that is too large can cause calculation diffi-\nculties. Thus, the image must be resized. In this experiment,\nwe used the bilinear method to resize a 224 ×224 image in a\nsequence into a 56 ×56 image and finally output a 56 ×56×N\nvideo frame sequence, where N is the number of images in\nthe sequence [34].\n2) Data Vectorization and Positional Embedding\nIn addition to word embedding, the transformer also needs to\nuse positional embedding to represent the position of words\nin a sentence. Because the Transformer does not adopt the\nstructure of the RNN and uses global information, it cannot\nutilize word order information, which is essential for NLP.\nTherefore, the Transformer uses positional embedding to save\nthe relative or absolute positions of the words in the sequence.\nPrior to this process, the data must be vectorized.\nAny image can be represented as a 56 ×56×3 vector be-\ncause each image contains three channels. For a sequence, it\ncan be represented as 56 ×56×3N, and the flattening process\nflattens the image length and height. Finally, for each image,\nwe obtained a 3136 ×3N vector.\nFor the traditional ViT model, large images were cut into\ndifferent patches. The position of each patch in the original\nimage will be used for positional encoding. In this study, we\ndirectly use the Visual Transformer’s positional embedding\n[33]. However, we did not choose to cut the image but directly\nused the position of each image in the sequence for positional\nencoding, and the specific algorithm for positional encoding\nis consistent with ViT. Because its dimensionality is the same\nas that of patch embedding, the final vector length does not\nchange.\n3) Transformer Encoder and Decoder\nIn this study, we used an original transformer decoder [35].\nThe standard Transformer decoder also has a set of similar\nlayers, with a structure similar to the layers in the encoder,\nexcept that the decoder inserts \"encoder-decoder attention\"\nbetween the multi-head self-attention and the feedforward\nnetwork to allocate different attention to the encoder’s output.\nIn the encoder-decoder attention module, three input matrices\nare accepted: query (Q), key (K), and value (V) [35]. The\nkey (K) and value (V) are derived from the encoder output,\nwhereas the query is derived from the decoder input. These\nmatrices are different representations embedded in the input\nthrough the dense or linear layers. Attention scores are evalu-\nated by the dot product of the encoder-decoder hidden states\n(in the encoder-decoder attention mechanism), as shown in\nequation1 [35].\nAttention(Q, K, V ) = softmax\n\u0012QKT\n√dk\n\u0013\nV (1)\nIn addition, the decoder uses attention masks in self-\nattention to prevent access to more information during train-\ning than during inference. Given that the decoder’s output is\nshifted right by one position from the input, the attention mask\nensures that the output at position i can only attend to the input\nup to position i in the output. We stacked three encoder layers\nsequentially, followed by three decoder layers. The earliest\nversion of the transformer model used six encoder and six\ndecoder layers. Owing to the limited training set in this study,\ntoo many layers may cause overfitting. Therefore, the depth\nof the deep learning model is reduced [36]. Finally, during the\ntesting process, we use a linear layer with softmax activation\nto output the character sequence. For the training process, we\nneed the softmax matrix to calculate the CTC loss. Figure.8\nshows the specific structure of the Transformer encoder and\ndecoder parts in this study.\nC. FEATURES OF PROPOSED METHOD\nIn this study, we propose a robust and concise end-to-end\nmethod for air-writing recognition. To overcome the difficul-\nties of traditional air writing recognition and achieve a better\nCER than the baseline model, our TR-AWR method has two\nimportant features. The first is that our approach is based on\nadopting end-to-end thought, and the other important feature\nis that we use transformer models for recognition.\n1) End-to-End method\nTraditional machine learning models are often composed of\nmultiple independent modules, and visual-based methods for\nfreehand air writing recognition also adopt this approach.\nAfter obtaining visual information, many methods have been\nused to process this information. For example, trajectory\nimages are obtained by tracking, and these images are used\nas input data to convert freehand air writing recognition into\na handwriting recognition problem. Alternatively, hand key-\npoint coordinates and joint angles were extracted, or fingertip\ncoordinates were obtained using a depth sensor, and these\nnumbers or vectors were used as data input for air-writing\nrecognition. Table 1 describes the intermediate steps of some\nmethods.\nOne common feature of these methods lies in their adoption\nof certain steps, such as hand keypoint recognition or fingertip\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 7. Specific structure of Transformer-based deep learning model in this study, which contains three main parts, a block: (Resize), b block: (Data\nVectorization and Positional Embedding), c block: (Transformer Encoder and Decoder).\nFIGURE 8. Specific structure of the Transformer encoder and decoder\nparts (newly created inspired from the figure in [35]). This method uses a\nthree-layer Encoder and three-layer Decoder structure.\ntracking, which, however, leads to an increased chance of\nerror in themselves, and increases the complexity of data\ncollection and processing [37]. The difficulties mentioned in\nthe previous section I-c also affect the recognition results.\nGiven these problems, we believe that an end-to-end ap-\nproach should be used to solve the problem of air writing\nrecognition. That is, the video sequence is taken as the input,\nthe letter sequence or number sequence carried by the video is\ntaken as the output, and the many difficulties mentioned in the\nprevious section I-c are overcome by increasing the original\ndata and reasonable data augmentation methods. Compared\nwith non-end-to-end methods, this approach can avoid errors\nor mistakes caused by intermediate steps [38], complete re-\ntention of feature information in the image, and reduce the\ncomplexity of data collection and processing by including\nmore different features.\nTABLE 1. Intermediate steps for traditional air-wiring recognition\nmethods\nMethods Intermediate steps Recognition steps\nFingertip\ntracking\nrecognition\n[23]\nUse fingertip-tracking algo-\nrithms to obtain trajectory\ngraphs\nUsing a handwritten digit\ndataset (MNIST) as a training\nset to identify the content of\nthe trajectory graph\nDepth-\nCamera\n[25]\nCollects fingertip\ncoordinates (XYZ) via\ndepth sensors\nair-writing recognition based\non coordinate data using CNN\nnetworks\nHand\nSkeleton\nRecognition\n[26]\nObtaining hand key point\ncoordinates and joint angles\nusing a deep learning frame-\nwork\nAir writing recognition using\nthe RNN-LSTM model based\non hand key points and joint\nangles\n2) Transformer model\nBased on the baseline model, we further analyzed the air\nwriting recognition task from the end-to-end perspective. This\nis a sequence-to-sequence task that, inputs a video frame\nsequence and outputs a character sequence. The Transformer\nmodel is an excellent solution to this type kind of problem. In\nrecent years, the transformer model has been widely applied\nin computer vision tasks and has shown excellent results,\nsuch as in Optical Character Recognition (OCR) task [39].\nApart from OCR tasks, transformer models have also shown\nexcellent results in video understanding tasks [40].\nAlthough convolution-based methods have long held a\ndominant position, they also have limitations. In comparison\nto the model described in the baseline, we believe that the\ntransformer model possesses several characteristics that allow\nfor better recognition results in WiTA tasks. We believe that\nthe following four reasons explain the transformer model’s\nability to achieve better results in the WiTA task.\n(a) Self-attention mechanism: The Transformer model is\nbased on a self-attention mechanism, that effectively captures\nlong-range dependencies in input sequences. In air-writing\nrecognition tasks, the input sequences often contain longer\nwriting segments [41]. The self-attention mechanism excels\nin capturing the correlated information within these segments,\nwhereas the limited receptive field of smaller convolutional\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\noperators constrains their ability to model long-range depen-\ndencies [42]. The self-attention mechanism in the transformer\nexpands the receptive field, thereby enhancing video recogni-\ntion performance.\n(b) Parallel computing capability: The self-attention\nmechanism in the transformer enables direct interaction be-\ntween representations at each input position and all other\npositions, thereby facilitating highly parallelized computation\n[43]. Traditional recurrent neural network (RNN) models\nmay encounter computational efficiency limitations when\nhandling lengthy sequences in air-writing. The parallel com-\nputing capability of the Transformer model makes it more\nsuitable for processing long sequence data.\n(c) Multi-head attention mechanism: The Transformer\nmodel incorporates a multi-head attention mechanism, allow-\ning simultaneous focus on different feature subspaces at dis-\ntinct positions. This empowers the model to capture various\ntypes of information within the input sequences [44]. In air-\nwriting recognition tasks, input sequences may encompass\ndiverse stroke information and shape features. By leveraging\nthe multi-head attention mechanism, the transformer model\ncan effectively utilize this range of information for feature\nextraction and representation learning.\n(d) Encoder-decoder architecture: The Transformer\nmodel adopts an encoder-decoder structure, which has\ndemonstrated outstanding performance in sequence-to-\nsequence tasks. In Air Writing recognition tasks, the encoder\nmaps input sequences to high-dimensional representations,\nwhile the decoder generates corresponding recognition re-\nsults based on these representations [45]. This encoder-\ndecoder architecture enhances the model’s generalization\nability and recognition accuracy.\nIn conclusion, the transformer model’s self-attention mech-\nanism, parallel computing capability, multi-head attention\nmechanism, and encoder-decoder architecture contribute to\nits strong performance in Air Writing recognition tasks. Thus,\nthis study applies the transformer model to air-writing recog-\nnition. Notably, the transformer model relies on a substantial\namount of data. Consequently, this study proposes data aug-\nmentation criteria specifically tailored to air-writing recogni-\ntion tasks and significantly increases the dataset using data\naugmentation methods.\nD. CONNECTIONIST TEMPORAL CLASSIFICATION (CTC)\nLOSS FUNCTION\nIn this task, the input data is a sequence of images, and the\noutput data is a string sequence. The smallest recognition unit\nis a word. Therefore, the air-writing recognition task in this\nstudy faced the same problem as traditional Optical Character\nRecognition (OCR) and machine translation tasks. It is dif-\nficult to align the input and output texts at the word level,\nand it is challenging to align them during pre-processing.\nHowever, if the model is trained without alignment, it will\nbe difficult to converge owing to differences in character\ndistances. Therefore, this study uses Connectionist Temporal\nClassification (CTC) [46] as the loss function.\nFirst, we encode the text into a sequence of separate letters.\nWe use the special character ’ ∼’ to distinguish two identical\ncharacters that appear continuously in English words. For\nexample, the “apple” becomes (a, p, ∼, p, l, e), and \"success\"\nbecomes (s, u, c, ∼, c, e, s, ∼, s).\nNext, we defined the input video frame sequence as X =\n[x1, x2, ..., XT], where Xi represents a certain image frame in\nthe sequence. For such a video frame sequence X, there exists\na corresponding character sequence Y = [Y1, Y2, ..., YU],\nwhere Yi represents a specific letter in the sequence. CTC\nprovides a solution to this problem by providing the output\ndistribution of all possible Y for a given input sequence X.\nBased on this distribution, we can output the most likely result\nor give the probability of a certain output.\nFor the loss calculation, given input sequence X, we hope\nto maximize the posterior probability of Y. It should be differ-\nentiable such that we can gradient-descent optimization can\nbe performed. The maximization of Y’s posterior probability\ncan be expressed in the following equation:\nY ∗ = arg maxp(Y | X). (2)\nAfter applying the CTC algorithm, the loss function for a\ngiven video sequence X and its corresponding true label Y in\ntraining set S can be expressed in the following equation:\nLctc = −\nX\n(X,Y )∈S\nln p(Y | X). (3)\nE. SOLUTION TO RESEARCH CHALLENGES OF\nCONTINUOUS AIR WRITING RECOGNITION\nBuilding upon the above-mentioned ideas, we believe that\nour method can solve some of the general challenges in\ncontinuous air writing recognition:\n(a) To solve the issue of poor data readability, we used\nthe most intuitive and common input format of video data,\nmaking it easy to collect, annotate, and clean data. This\nallowed us to quickly and conveniently increase the dataset\nsize, avoiding the problem of poor data readability associated\nwith sensor data acquisition.\n(b) To solve the problem of user habit differences, we ex-\npanded the dataset to some extent to alleviate these issues. The\ndataset included air-writing videos from over 100 individu-\nals, each with their own writing habits. Sufficient samples\nfor similar strokes were provided by selecting an adequate\nvocabulary, and data augmentation was used to expand the\nsample size further.\n(c) To solve the problem of spatial stacking, our approach\nrecognizes the entire movement process instead of only the\nwriting part. Unlike common methods that track fingertip\ntrajectories and perform image recognition, our method rec-\nognizes the movement itself, which changes over time and is\nnot stacked with previous movements.\n(d) Compared to the results of the baseline model, our\ntransformer-based method can achieve better recognition re-\nsults. Table 2 presents a comprehensive comparative analysis\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 2. Comparison of traditional method, baseline method, and\nTR-AWR (our)\nItems Traditional method Baseline method TR-AWR\nmethod\nintermediate\nsteps\nYes No No\nRecognized\ndata\nCoordinate/Key\nPoints...\nFrame Sequence Frame\nSequence\nRecognition\nformat\nSingle character Character Sequence Character\nSequence\nCriteria for\njudging\nSingle character ac-\ncuracy\nCER CER\nDeep learning\nmodels\nLstm/RNN... Spatio-Temporal\nConvolution\nTransformer\nData Augmen-\ntation\nYes/No No Yes\nof the key factors in the traditional air writing recognition\nmethods, the baseline method [9], and TR-AWR (our).\nIV. EXPERIMENT\nA. DATASET\nThe dataset used in this study is the WiTA dataset [9], which\nconsists of five sub-datasets in two languages (Korean and\nEnglish). Only the English portion was used in this experi-\nment, which included 10620 video sequences from 122 par-\nticipants. The data were sourced from an RGB camera with a\nframe rate of 29 fps, and all video frames were converted to\n224x224 pixel images. The data were primarily divided into\ntwo parts: English Lexical, which retrieved the top 6,000 most\nfrequent words from the Google 1B dataset [47], accounting\nfor 86.3 % of the dataset, and English Non-Lexical, account-\ning for 13.7 % of the dataset. The dataset authors randomly\ngenerated non-lexical words by sampling from 26 letters. The\nlengths of the non-lexical words range from three to seven.\nFigure 9 presents an example of the dataset, containing both\nlexical and non-lexical categories.\nFIGURE 9. Examples of the air-writing text.\nB. PARAMETER SETTING\nIn order to ensure rigorous training and evaluation of the mod-\nels, the dataset was partitioned into three distinct groups: a\ntraining dataset comprising 80 % of the total data, a validation\ndataset consisting of 10%, and a test dataset encompassing the\nremaining 10 %. To conduct our experiments, we employed\nfour NVIDIA RTX 2080ti GPUs with 11GB memory each,\nmaintaining the same training environment as specified in the\nbaseline model [9]. The comparison methods utilized in this\nstudy strictly adhered to the optimal settings outlined in the\nbaseline paper [9]. In the case of our TR-AWR method, we\nimplemented a learning rate warm-up scheme, initializing the\nlearning rate at 1e-3. For the 6-layered Transformer models, a\nbatch size of 128 was employed, whereas a batch size of 1 was\nused for measuring D-fps. We incorporated early stopping as\na means of model selection, employing a stopping condition\nduring the training procedure. It is noteworthy that all models\nachieved convergence within 175 training epochs.\nC. COMPARISON METHODS\nAs stated in Section II-C, Korean researchers proposed an\nunconstrained air-writing recognition method using Spatio-\nTemporal Convolution, their solution is the comparison meth-\nods in this article. For the English Air writing recognition, we\nhave three comparison methods, details of the three methods\nare as follows.\n(a) Comparison Method 1 : ST-rMC (2022) [9] means\nmixed reverse 3D-2D convolutions, this model contains seven\nmain layers, the video sequence first enters the Stem Block,\nthen passes through two 2D Conv Blocks, after which it\npasses through the ST-pooling layer, after which it bridges the\ntwo 3D Conv Blocks, and finally passes through the Spatial\npooling layer.\n(b) Comparison Method 2 : ST-R3D (2022) [9] means\nresidual 3D convolutions, his model contains seven main\nlayers, the video sequence first enters the Stem Block, then\npasses through two 3D Conv Blocks, after which it passes\nthrough the identity layer, after which it bridges the two 3D\nConv Blocks, and finally passes through the Spatial pooling\nlayer.\n(c) Comparison Method 3 : ST-R(2+1)D (2022) [9] means\n2D convolutions followed by 1D convolutions, this model\ncontains seven main layers, the video sequence first enters the\nStem Block, then passes through two (2+1)D Conv Blocks,\nafter which it passes through the identity layer, after which\nit bridges the two (2+1)D Conv Blocks, and finally passes\nthrough the Spatial pooling layer.\nD. EVALUATION CRITERIA\nAs the input of this experiment was a sequence of video\nframes and the output was a sequence of characters, we chose\nthe Character Error Rate (CER) as the accuracy measurement\ninstead of the accuracy of individual characters. The CER was\ncalculated as follows:\nCER = MCD(S, P)/length(P) (4)\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nIn the above equation, MCD (S, P) is the minimum char-\nacter distance (Levenshtein measure [48]) between decoded\nphrase S and ground-truth phrase P, and length (P) is the\nnumber of characters in P. The Levenshtein distance was\ncalculated as shown as follows.\nlev(a, b) =\n\n\n\n|a| if |b| = 0,\n|b| if |a| = 0,\nlev(tail(a), tail(b)) if a[0] =b[0]\n1 + min\n\n\n\nlev(tail(a), b)\nlev(a, tail(b)) otherwise\nlev(tail(a), tail(b))\n(5)\nThe Levenshtein measure, also known as the edit distance,\nis a metric used to measure the difference or similarity be-\ntween two sequences. It calculates the minimum number\nof edit operations (insertions, deletions, and substitutions)\nrequired to transform one sequence into another [49]. The\nLevenshtein distance can be applied in various fields such\nas natural language processing, spell checking, and speech\nrecognition [50]. It provides a universal method for measuring\nthe similarity or difference between sequences, allowing for\ncomparing and matching different texts or sequences. In this\nstudy, both the proposed method and the baseline models use\nthe Levenshtein measure to calculate the Character Error Rate\n(CER). The CER value between the sequence and ground\ntruth is computed for each generated sequence. The overall\nCER for each group is then calculated by taking the average\nof all the sequence results. The CER results for the lexical and\nN-lexical groups are weighted averages based on the number\nof sequences, yielding the CER values for each method.\nTo ensure the real-time operation of decoders, we also\ncalculated the Average Decoding Frames per second (D-fps)\nperformance metric. The D-fps was calculated by averaging\nthe total number of frames decoded in one second. This metric\nmay vary depending on the device used; however, as long as\nthe output results are not too low, it can be ensured that the\nmodel has a certain real-time performance. The D-fps was\ncalculated as shown as follows.\nD − fps = Decoding frames Number\nTime(s) (6)\nE. EXPERIMENTAL RESULTS AND ANALYSIS\nAccording to Table 3, our method (TR-AWR) achieved CER\nvalues of 28.65 and 37.51 in the lexical and N-Lexical groups,\nrespectively. The overall CER value was 29.86, which was\nsignificantly lower than that of the ST-rMC model (CER:\n95.79) and ST-R (2+1)D model (CER: 91.74) in the base-\nline model. Compared to the best-performing method, ST-\nR3D, in the baseline model, our comprehensive error rate\nwas reduced by approximately 3 percentage points. Addi-\ntionally, our method achieved better experimental results in\nboth lexical and N-Lexical groups, particularly in the lexical\nclassification task.\nRegarding real-time performance, we compared our\nmethod with three baseline models using the same machine.\nAlthough our D-fps data are lower than the three baseline\nmodels, the result of 194.67 still ensures a reasonable recog-\nnition speed. Overall, the proposed method outperformed the\nbaseline models in terms of accuracy and maintained a certain\nlevel of real-time capability.\nTABLE 3. Results of the test dataset\nModel CER(%) D-fps(fps)\nLexical N-Lexical Overall\nTR-AWR (our) 28.65 37.51 29.86 194.67\nST-rMC [9] 95.64 96.77 95.79 460.24\nST-R3D [9] 31.30 39.48 32.41 282.36\nST-R(2+1)D [9] 91.43 93.68 91.74 267.95\nFigure 10 shows three examples of correct recognition in\nthe results, the first image sequence is the letter \"c\" in the\nsequence \"child\", the second image sequence is the letter \"jet\"\nin the sequence \"map\", and the third image sequence is the\nletter \"m\" in the sequence \"map\". The first image sequence\nis the letter \"c\" in the sequence \"child\" and the second image\nsequence is the letter \"j\" in the sequence \"jet\". The third image\nsequence is the letter \"m\" in the sequence \"map\".\nFIGURE 10. True case of air-writing recognition by three examples. The\npictures on the upper row are the sequence of images corresponding to a\nparticular letter in the action of drawing a word. The text on the middle\nrow shows the actual correct character and the character output by the\nproposed method on the lower row, which is correct.\nFigure 11 shows two examples of recognition errors. The\nfirst video clip is for the letter \"i\" in the sequence \"smiles\", but\nthe result is \"l\", and the second video clip is for the letter \"c\"\nin the sequence \"economy\", but the result is \"e\". The second\nvideo clip is the letter \"c\" of the letter sequence \"economy\",\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nbut the result is the letter \"e\". The reason for this result\nmay be related to the difficulty in recognizing continuous air\nwriting that we analyzed in section I-C, where the two falsely\nrecognized letters are very similar, and the links between the\nletters also interfere with the accuracy of the recognition.\nFIGURE 11. False case of air-writing recognition by two examples. The\npictures on the upper row are the sequence of images corresponding to a\nparticular letter in the action of drawing a word. The text on the middle\nrow shows the actual correct character and the character output by the\nproposed method on the lower row, which is false.\nF. METHOD LIMITATIONS\nThe TR-AWR method proposed in this study represents a\nfurther exploration of the WiTA task; therefore, it still has\nseveral limitations:\n(a) Recognition unit and accuracy: In this study, the\nminimum recognition unit was a word, rather than a com-\nplete sentence. If this method is to be applied in practical\nenvironments, further adjustments must be made to ensure\ncoherence between words. In addition, the accuracy of the\ncurrent method still requires improvement.\n(b) Real-time recognition: In this task, we did not extend\nthe method to real-time air writing; instead, we ensured a\ncertain level of real-time capability by calculating the de-\ncoding frames per second (D-fps). Transformer models, as\nthey typically require global attention computations over the\nentire input sequence, may introduce latency when processing\nlonger input sequences. This may result in a delay in display-\ning recognition results within certain time windows during the\nuser’s writing, thus reducing the user’s interactive experience.\n(c) Computational complexity: The Transformer model\nmay require more computational resources and memory when\nprocessing longer input sequences [51]. Particularly in the\ncase of continuous writing processes, if the real-time execu-\ntion of large-scale transformer models is necessary, it may\nimpose increased demands on computing devices. This can\npose limitations in embedded devices or resource-constrained\nenvironments.\nV. CONCLUDING REMARKS\nA. CONCLUSION\nThis study presents a novel end-to-end air-writing recog-\nnition method based on the transformer model, that effec-\ntively addresses the WiTA task, which entails continuous air-\nwriting recognition. By leveraging the transformer model,\nwe construct an end-to-end Air-Writing model that success-\nfully transforms sequences of Air-Writing video frames into\ncorresponding character sequences. Our model employs an\nattention mechanism within an encoder-decoder framework,\nallowing it to automatically recognize handwritten charac-\nters by learning the mapping relationship between frame\nsequences and characters. Evaluation of the model’s per-\nformance is based on the character error rate (CER), while\nensuring specific real-time performance by calculating D-\nfps. This study achieved a CER value of 29.86 %, currently\nstanding as the best-reported data. Furthermore, the algorithm\ndemonstrates a D-fps result of 186.75 fps, guaranteeing its\nreal-time performance. Comparative experiments with sim-\nilar end-to-end methods confirm that our model achieves\nsuperior CER results while maintaining a specific level of\nreal-time performance, thereby enhancing overall accuracy.\nThe contributions of this article lie in two significant con-\ntributions. Firstly, we introduce the Transformer model to\nthe WiTA task, surpassing the performance of the baseline\nmethod. Secondly, we pioneer the use of data augmentation\ntechniques specifically tailored for the WiTA task, along with\nthe proposal of criteria for effective data augmentation. Col-\nlectively, our study provides valuable insights and guidance\nfor other researchers aiming to address the WiTA task.\nB. FUTURE WORK\nThe Transformer model is a concise and efficient deep learn-\ning model, and the size of the dataset is an important factor\naffecting its performance. In future work, the most important\naspect will be to establish a larger and more diverse dataset\nin multiple dimensions. We focus on improving the model’s\nperformance in handling different writing styles, languages,\nand environments to enhance its practicality [52]. For ex-\nample, we will collect more sample data for different age\ngroups, education levels, and professions and analyze and\npre-process the data to improve the model’s performance. In\naddition, we will attempt to apply this model to other fields\nsuch as numerals, graphics, and Chinese characters, thereby\nexpanding its application scope. With further development of\ncomputer vision methods, more advanced computer vision\nmethods should also be applied to this problem to further\nimprove the model’s accuracy and real-time performance.\nREFERENCES\n[1] B. R. Gaines and M. L. G. Shaw, “From timesharing to the sixth generation:\nThe development of human-computer interaction. part I,” International\nJournal of Man-Machine Studies, vol. 24, no. 1, pp. 1–27, 1986.\n[2] M. B. Rosson and J. M. Carroll, Usability engineering: Scenario-based\ndevelopment of Human Computer Interaction. San Francisco, CA: Morgan\nKaufmann, 2009.\n[3] J. Katona, “A review of Human–Computer Interaction and virtual reality\nresearch fields in cognitive info-communications,” Applied Sciences, vol.\n11, no. 6, p. 2646, 2021.\n[4] F. Gurcan, N. E. Cagiltay, and K. Cagiltay, “Mapping human–computer\ninteraction research themes and trends from its existence to today: A\ntopic modeling-based review of past 60 years,” International Journal of\nHuman–Computer Interaction, vol. 37, no. 3, pp. 267–280, 2020.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[5] M. Chen, G. AlRegib, and B.-H. Juang, “Air-writing recognition—part\nI: Modeling and recognition of characters, words, and connecting mo-\ntions,” IEEE Transactions on Human-Machine Systems, vol. 46, no. 3, pp.\n403–413, Nov. 2015.\n[6] W. Roldan et al., “Opportunities and challenges in involving\nusers in project-based HCI education,” Proceedings of the 2020\nCHI Conference on Human Factors in Computing Systems, 2020.\ndoi:10.1145/3313831.3376530\n[7] Y. Song, D. Demirdjian, and R. Davis, “Continuous body and hand gesture\nrecognition for natural human-computer interaction,” ACM Transactions\non Interactive Intelligent Systems, vol. 2, no. 1, pp. 1–28, Mar. 2012.\n[8] D. D. Mohan, B. Jawade, S. Setlur, and V. Govindaraju, “Deep Metric\nLearning for Computer Vision: A brief overview,” Handbook of Statistics,\npp. 59–79, Feb. 2018.\n[9] U.-H. Kim, Y. Hwang, S.-K. Lee, and J.-H. Kim, “Writing in the air: Un-\nconstrained text recognition from finger movement using spatio-temporal\nconvolution,” IEEE Transactions on Artificial Intelligence, pp. 1–13, Oct.\n2022.\n[10] L. G. Hafemann, R. Sabourin, and L. S. Oliveira, “Learning features\nfor offline handwritten signature verification using deep convolutional\nneural networks,” Pattern Recognition, vol. 70, pp. 163–176, May 2017.\ndoi:10.1016/j.patcog.2017.05.012\n[11] D. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber, “Con-\nvolutional neural network committees for handwritten character classifica-\ntion,” 2011 International Conference on Document Analysis and Recogni-\ntion, Feb. 2011. doi:10.1109/icdar.2011.229\n[12] S. Xu, Y. Xue, X. Zhang, and L. Jin, “A novel unsupervised domain\nadaptation method for inertia-trajectory translation of in-air handwriting,”\nPattern Recognition, vol. 116, p. 107939, Aug. 2021.\n[13] S. Agrawal et al., “Using mobile phones to write in Air,” Proceedings of the\n9th international conference on Mobile systems, applications, and services,\n2011. doi:10.1145/1999995.1999998\n[14] A. Tripathi, A. P. Prathosh, S. P. Muthukrishnan, and L. Kumar,\n“SurfMyoAiR: A surface electromyography-based framework for Airwrit-\ning Recognition,” IEEE Transactions on Instrumentation and Measure-\nment, vol. 72, pp. 1–12, Feb. 2023.\n[15] Y. Fang, Y. Xu, H. Li, X. He, and L. Kang, “Writing in the air: Recognize\nletters using Deep Learning through WIFI Signals,” 2020 6th International\nConference on Big Data Computing and Communications (BIGCOM), Jul.\n2020.\n[16] C. Amma, M. Georgi, and T. Schultz, “Airwriting: Hands-free mobile text\ninput by spotting and continuous recognition of 3D-space handwriting\nwith inertial sensors,” 2012 16th International Symposium on Wearable\nComputers, Jun. 2012.\n[17] S. Duffner, S. Berlemont, G. Lefebvre, and C. Garcia, “3D gesture clas-\nsification with Convolutional Neural Networks,” 2014 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), May\n2014.\n[18] A. Akl, C. Feng, and S. Valaee, “A novel accelerometer-based gesture\nrecognition system,” IEEE Transactions on Signal Processing, vol. 59, no.\n12, pp. 6197–6205, Aug. 2011.\n[19] M. Chen, G. AlRegib, and B.-H. Juang, “Air-writing recognition—part\nII: Detection and recognition of writing activity in continuous stream of\nMotion Data,” IEEE Transactions on Human-Machine Systems, vol. 46,\nno. 3, pp. 436–444, Jan. 2016. doi:10.1109/thms.2015.2492599\n[20] S. Nabeshima, S. Yamamoto, K. Agusa, and T. Taguchi, “Memo-pen,”\nConference companion on Human factors in computing systems - CHI ’95,\nMay 1995.\n[21] N. Xu, W. Wang, and X. Qu, “Recognition of in-air handwritten Chinese\ncharacter based on Leap Motion Controller,” Lecture Notes in Computer\nScience, pp. 160–168, Jan. 2015.\n[22] J. Gan and W. Wang, “In-air handwritten English word recognition using\nattention recurrent translator,” Neural Computing and Applications, vol.\n31, no. 7, pp. 3155–3172, 2017.\n[23] S. Mohammadi and R. Maleki, “Real-time Kinect-based air-writing system\nwith a novel analytical classifier,” International Journal on Document\nAnalysis and Recognition (IJDAR), vol. 22, no. 2, pp. 113–125, 2019.\n[24] C.-C. Lee, C.-Y. Shih, and B.-S. Jeng, “Fingertip-writing alphanumeric\ncharacter recognition for vision-based Human Computer Interaction,” 2010\nInternational Conference on Broadband, Wireless Computing, Communi-\ncation and Applications, December 2010.\n[25] M. S. Alam, K.-C. Kwon, M. A. Alam, M. Y. Abbass, S. M. Imtiaz,\nand N. Kim, “Trajectory-based air-writing recognition using Deep Neural\nNetwork and depth sensor,” Sensors, vol. 20, no. 2, p. 376, 2020.\n[26] A. Rahman, P. Roy, and U. Pal, “Air writing: Recognizing multi-digit\nnumeral string traced in air using RNN-LSTM architecture,” SN Computer\nScience, vol. 2, no. 1, 2021.\n[27] C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmentation\nfor Deep Learning,” Journal of Big Data, vol. 6, no. 1, 2019.\n[28] J. Wei and K. Zou, “Eda: Easy data augmentation techniques for boosting\nperformance on text classification tasks,” Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), May 2019. doi:10.18653/v1/d19-1670\n[29] N. Cauli and D. Reforgiato Recupero, “Survey on videos data augmentation\nfor Deep Learning Models,” Future Internet, vol. 14, no. 3, p. 93, 2022.\ndoi:10.3390/fi14030093\n[30] Sural, S., Gang Qian and Pramanik, S. (1n.d.) “Segmentation and histogram\ngeneration using the HSV color space for image retrieval,” Proceedings.\nInternational Conference on Image Processing [Preprint]. Available at:\nhttps://doi.org/10.1109/icip.2002.1040019.\n[31] H.-T. Duong and V. T. Hoang, “Data augmentation based on color\nfeatures for limited training texture classification,” 2019 4th In-\nternational Conference on Information Technology (InCIT), 2019.\ndoi:10.1109/incit.2019.8911934\n[32] R. G. Lopes, D. Yin, B. Poole, J. Gilmer, and E. D. Cubuk, “Improving ro-\nbustness without sacrificing accuracy with patch gaussian augmentation,”\narXiv.org, https://arxiv.org/abs/1906.02611 (accessed Jun. 29, 2023).\n[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T.\nUnterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\nand N. Houlsby, “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” arXiv.org, 03-Jun-2021. [Online]. Available:\nhttps://arxiv.org/abs/2010.11929. [Accessed: 12-Mar-2023].\n[34] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell, “Compact bilinear pool-\ning,” 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2016. doi:10.1109/cvpr.2016.41\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L.\nKaiser, and I. Polosukhin, “Attention is all you need,” arXiv.org, 06-Dec-\n2017. [Online]. Available: https://arxiv.org/abs/1706.03762. [Accessed:\n30-Apr-2023].\n[36] W. Wang and Z. Tu, “Rethinking the value of transformer components,”\nProceedings of the 28th International Conference on Computational Lin-\nguistics, Oct. 2020. doi:10.18653/v1/2020.coling-main.529\n[37] C. Coleman, D. Narayanan, D. Kang, and T. Zhao, Dawnbench:\nAn end-to-end Deep learning benchmark and competition,\nhttps://dawn.cs.stanford.edu/benchmark/papers/nips17-dawnbench.pdf\n(accessed Jun. 28, 2023).\n[38] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, “An end-to-end deep\nlearning architecture for Graph Classification,” Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 32, no. 1, 2018.\n[39] D. C. Bui, D. Truong, N. D. Vo, and K. Nguyen, “MC-OCR Challenge\n2021: Deep Learning Approach for Vietnamese receipts OCR,” 2021 RIVF\nInternational Conference on Computing and Communication Technologies\n(RIVF), 2021.\n[40] J. Selva, A. S. Johansen, S. Escalera, K. Nasrollahi, T. B. Moeslund, and A.\nClapés, “Video transformers: A survey,” arXiv.org, 13-Feb-2023. [Online].\nAvailable: https://arxiv.org/abs/2201.05991. [Accessed: 02-Apr-2023].\n[41] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative po-\nsition representations,” Proceedings of the 2018 Conference of the North\nAmerican , Sep. 2018. doi:10.18653/v1/n18-2074\n[42] W. Luo, Y. Li, R. Urtasun, and R. Zemel, “Understanding the effective\nreceptive field in deep convolutional neural networks: Proceedings of the\n30th International Conference on Neural Information Processing Systems,”\nGuide Proceedings, https://dl.acm.org/doi/10.5555/3157382.3157645 (ac-\ncessed Jul. 14, 2023).\n[43] C. Liang, M. Xu, and X.-L. Zhang, “Transformer-based end-to-end speech\nrecognition with residual gaussian-based self-attention,” Interspeech 2021,\n2021. doi:10.21437/interspeech.2021-427\n[44] M. India, P. Safari, and J. Hernando, “Self multi-head attention for speaker\nrecognition,” Interspeech 2019, 2019. doi:10.21437/interspeech.2019-\n2616\n[45] W. Roldan et al., “Opportunities and challenges in involving\nusers in project-based HCI education,” Proceedings of the 2020\nCHI Conference on Human Factors in Computing Systems, 2020.\ndoi:10.1145/3313831.3376530\n[46] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, “Connectionist\nTemporal Classification,” Proceedings of the 23rd international conference\non Machine learning - ICML ’06, 2006.\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[47] C. Chelba et al., “One Billion word benchmark for measuring\nprogress in Statistical language modeling,” Interspeech 2014, 2014.\ndoi:10.21437/interspeech.2014-564\n[48] Yujian, L. and Bo, L. (2007) “A normalized Levenshtein distance metric,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 29(6),\npp. 1091–1095. Available at: https://doi.org/10.1109/tpami.2007.1078.\n[49] D. Li, C. R. Opazo, X. Yu, and H. Li, “Word-level deep sign language\nrecognition from video: A new large-scale dataset and methods compari-\nson,” 2020 IEEE Winter Conference on Applications of Computer Vision\n(WACV), Jul. 2020. doi:10.1109/wacv45572.2020.9093512\n[50] R. Haldar and D. Mukhopadhyay, “Levenshtein distance technique\nin dictionary lookup methods: An improved approach,” arXiv.org,\nhttps://arxiv.org/abs/1101.1232 (accessed Jun. 29, 2023).\n[51] Z. Zhang, Z. Gong, and Q. Hong, “A survey on: Application of trans-\nformer in computer vision,” The Proceedings of The 8th International\nConference on Intelligent Systems and Image Processing 2021, Aug. 2021.\ndoi:10.12792/icisip2021.006\n[52] V. Carbune et al., “Fast multi-language LSTM-based online handwriting\nrecognition,” International Journal on Document Analysis and Recognition\n(IJDAR), vol. 23, no. 2, pp. 89–102, 2020. doi:10.1007/s10032-020-00350-\n4\nXUHANG TANwas born in Heilongjiang, China,\nin 2000. He received his bachelor’s degree in\n2021. He is currently pursuing a master’s degree\nin Information, Production, and Systems Engineer-\ning from Waseda University, Fukuoka, Japan. His\nmain research interest includes computer vision,\ndeep learning, and human-computer interaction\nJICHENG TONG was born in Inner Mongolia,\nChina, in 1998. He received his bachelor’s degree\nin 2021. He is currently pursuing a master’s degree\nin Information, Production, and Systems Engineer-\ning from Waseda University, Fukuoka, Japan. His\nmain research interest includes data mining, deep\nlearning, and human-computer interaction\nTAKAFUMI MATSUMARU (Senior Member,\nIEEE) received the B.S., M.S., and Ph.D. de-\ngrees in mechanical engineering from Waseda Uni-\nversity, Tokyo, Japan, in 1985, 1987, and 1998,\nrespectively. He worked for Toshiba Corpora-\ntion, Kawasaki, Japan, and Shizuoka University,\nHamamatsu, Japan. He is currently a Professor at\nWaseda University as the head of Bio-Robotics\nand Human-Mechatronics Laboratory (BRHM lab,\nhttp://www.waseda.jp/sem-matsumaru/). Visiting\nProfessor of Warsaw University of Technology (2023). Author of \"Seitai-\nkinou Kougaku (Biological Function Engineering)\" (Tokyo Denki University\nPress, ISBN 9784501417505). His research interest includes bio-robotics and\nhuman-mechatronics, especially in human-robot interaction.\nVIBEKANANDA DUTTA received the M.Sc. de-\ngree in computer science, with specialization in ar-\ntificial intelligence, from the Central University of\nRajasthan, Rajasthan, India, in 2012, and the Ph.D.\ndegree in automation and robotics from the War-\nsaw University of Technology, Warsaw, Poland, in\n2019. His researches focus on the human–robot\ninteraction, machine vision, and 3D imaging.\nXIN HE received the master’s degree from Grad-\nuate School of Information, Production and Sys-\ntem, Waseda University in 2021. Now is working\ntoward the Ph.D. degree with Graduate School of\nInformation, Production and System, Waseda Uni-\nversity.His research interests include deformable\nobjects manipulation, object shape modeling, point\ncloud data process and human gesture recognition.\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321807\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "End-to-end principle",
  "concepts": [
    {
      "name": "End-to-end principle",
      "score": 0.7266931533813477
    },
    {
      "name": "Computer science",
      "score": 0.6070920825004578
    },
    {
      "name": "Transformer",
      "score": 0.5844743251800537
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2721654176712036
    },
    {
      "name": "Electrical engineering",
      "score": 0.23674243688583374
    },
    {
      "name": "Engineering",
      "score": 0.1509248912334442
    },
    {
      "name": "Voltage",
      "score": 0.10045543313026428
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150744194",
      "name": "Waseda University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I108403487",
      "name": "Warsaw University of Technology",
      "country": "PL"
    }
  ]
}