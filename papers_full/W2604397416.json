{
    "title": "Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding",
    "url": "https://openalex.org/W2604397416",
    "year": 2017,
    "authors": [
        {
            "id": "https://openalex.org/A5001666093",
            "name": "Will Monroe",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A5041689299",
            "name": "Robert D. Hawkins",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A5001961716",
            "name": "Noah D. Goodman",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A5042601761",
            "name": "Christopher Potts",
            "affiliations": [
                "Stanford University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2115811569",
        "https://openalex.org/W2110930288",
        "https://openalex.org/W1993979041",
        "https://openalex.org/W2525032226",
        "https://openalex.org/W2002206192",
        "https://openalex.org/W2049991344",
        "https://openalex.org/W2095285839",
        "https://openalex.org/W2029138396",
        "https://openalex.org/W1993800891",
        "https://openalex.org/W1982249382",
        "https://openalex.org/W224572144",
        "https://openalex.org/W1970139752",
        "https://openalex.org/W2000512760",
        "https://openalex.org/W2142801638",
        "https://openalex.org/W2123713131",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W1895577753",
        "https://openalex.org/W6908809",
        "https://openalex.org/W2046515274",
        "https://openalex.org/W1996430422",
        "https://openalex.org/W2116492379",
        "https://openalex.org/W2963738237",
        "https://openalex.org/W2962719075",
        "https://openalex.org/W2964183327",
        "https://openalex.org/W1560265695",
        "https://openalex.org/W2118508845",
        "https://openalex.org/W2373419227",
        "https://openalex.org/W2949888546",
        "https://openalex.org/W2240667216",
        "https://openalex.org/W2252100656",
        "https://openalex.org/W2524363614",
        "https://openalex.org/W2293700449",
        "https://openalex.org/W2264742718",
        "https://openalex.org/W1533917153",
        "https://openalex.org/W2116561061",
        "https://openalex.org/W122142318",
        "https://openalex.org/W2250281182",
        "https://openalex.org/W2038721957",
        "https://openalex.org/W4231575501"
    ],
    "abstract": "We present a model of pragmatic referring expression interpretation in a grounded communication task (identifying colors from descriptions) that draws upon predictions from two recurrent neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning framework. Experiments show that this combined pragmatic model interprets color descriptions more accurately than the classifiers from which it is built, and that much of this improvement results from combining the speaker and listener perspectives. We observe that pragmatic reasoning helps primarily in the hardest cases: when the model must distinguish very similar colors, or when few utterances adequately express the target color. Our findings make use of a newly-collected corpus of human utterances in color reference games, which exhibit a variety of pragmatic behaviors. We also show that the embedded speaker model reproduces many of these pragmatic behaviors.",
    "full_text": "Colors in Context: A Pragmatic Neural Model for\nGrounded Language Understanding\nWill Monroe,1 Robert X.D. Hawkins,2 Noah D. Goodman,1,2 and Christopher Potts3\nDepartments of 1Computer Science, 2Psychology, and 3Linguistics\nStanford University, Stanford, CA 94305\nwmonroe4@cs.stanford.edu, {rxdh, ngoodman, cgpotts}@stanford.edu\nAbstract\nWe present a model of pragmatic referring\nexpression interpretation in a grounded com-\nmunication task (identifying colors from de-\nscriptions) that draws upon predictions from\ntwo recurrent neural network classiﬁers, a\nspeaker and a listener, uniﬁed by a recur-\nsive pragmatic reasoning framework. Exper-\niments show that this combined pragmatic\nmodel interprets color descriptions more ac-\ncurately than the classiﬁers from which it is\nbuilt, and that much of this improvement re-\nsults from combining the speaker and listener\nperspectives. We observe that pragmatic rea-\nsoning helps primarily in the hardest cases:\nwhen the model must distinguish very simi-\nlar colors, or when few utterances adequately\nexpress the target color. Our ﬁndings make\nuse of a newly-collected corpus of human ut-\nterances in color reference games, which ex-\nhibit a variety of pragmatic behaviors. We also\nshow that the embedded speaker model repro-\nduces many of these pragmatic behaviors.\n1 Introduction\nHuman communication is situated. In using lan-\nguage, we are sensitive to context and our interlocu-\ntors’ expectations, both when choosing our utter-\nances (as speakers) and when interpreting the utter-\nances we hear (as listeners). Visual referring tasks\nexercise this complex process of grounding, in the\nenvironment and in our mental models of each other,\nand thus provide a valuable test-bed for computa-\ntional models of production and comprehension.\nTable 1 illustrates the situated nature of reference\nunderstanding with descriptions of colors from a\nContext Utterance\n1. xxxx xxxx xxxx darker blue\n2. xxxx xxxx xxxx Purple\n3. xxxx xxxx xxxx blue\n4. xxxx xxxx xxxx blue\nTable 1: Examples of color reference in context, taken\nfrom our corpus. The target color is boxed. The speaker’s\ndescription is shaped not only by this target, but also by\nthe other context colors and their relationships.\ntask-oriented dialogue corpus we introduce in this\npaper. In these dialogues, the speaker is trying to\nidentify their (privately assigned) target color for the\nlistener. In context 1, the comparativedarker implic-\nitly refers to both the target (boxed) and one of the\nother colors. In contexts 2 and 3, the target color\nis the same, but the distractors led the speaker to\nchoose different basic color terms. In context 4,\nblue is a pragmatic choice even though two colors\nare shades of blue, because the interlocutors assume\nabout each other that they ﬁnd the target color a\nmore prototypical representative of blue and would\nprefer other descriptions (teal, cyan) for the middle\ncolor. The fact that blue appears in three of these\nfour cases highlights the ﬂexibility and context de-\npendence of color descriptions.\nIn this paper, we present a scalable, learned model\nof pragmatic language understanding. The model is\nbuilt around a version of the Rational Speech Acts\n(RSA) model (Frank and Goodman, 2012; Good-\nman and Frank, 2016), in which agents reason recur-\n325\nTransactions of the Association for Computational Linguistics, vol. 5, pp. 325–338, 2017. Action Editor: Luke Zettlemoyer.\nSubmission batch: 2/2017; Published 9/2017.\nc⃝2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\nsively about each other’s expectations and intentions\nto communicate more effectively than literal seman-\ntic agents could. In most work on RSA, the literal\nsemantic agents use ﬁxed message sets and stipu-\nlated grammars, which is a barrier to experiments in\nlinguistically complex domains. In our formulation,\nthe literal semantic agents are recurrent neural net-\nworks (RNNs) that produce and interpret color de-\nscriptions in context. These models are learned from\ndata and scale easily to large datasets containing di-\nverse utterances. The RSA recursion is then deﬁned\nin terms of these base agents: the pragmatic speaker\nproduces utterances based on a literal RNN listener\n(Andreas and Klein, 2016), and the pragmatic lis-\ntener interprets utterances based on the pragmatic\nspeaker’s behavior.\nWe focus on accuracy in a listener task (i.e., at\nlanguage understanding). However, our most suc-\ncessful model integrates speaker and listener per-\nspectives, combining predictions made by a sys-\ntem trained to understand color descriptions and one\ntrained to produce them.\nWe evaluate this model with a new, psycholin-\nguistically motivated corpus of real-time, dyadic ref-\nerence games in which the referents are patches of\ncolor. Our task is fundamentally the same as that\nof Baumgaertner et al. (2012), but the corpus we re-\nlease is larger by several orders of magnitude, con-\nsisting of 948 complete games with 53,365 utter-\nances produced by human participants paired into\ndyads on the web. The linguistic behavior of the\nplayers exhibits many of the intricacies of language\nin general, including not just the context dependence\nand cognitive complexity discussed above, but also\ncompositionality, vagueness, and ambiguity. While\nmany previous data sets feature descriptions of in-\ndividual colors (Cook et al., 2005; Munroe, 2010;\nKawakami et al., 2016), situating colors in a com-\nmunicative context elicits greater variety in language\nuse, including negations, comparatives, superlatives,\nmetaphor, and shared associations.\nExperiments on the data in our corpus show that\nthis combined pragmatic model improves accuracy\nin interpreting human-produced descriptions over\nthe basic RNN listener alone. We ﬁnd that the\nlargest improvement over the single RNN comes\nfrom blending it with an RNN trained to perform\nthe speaker task, despite the fact that a model based\nFigure 1: Example trial in corpus collection task, from\nspeaker’s perspective. The target color (boxed) was pre-\nsented among two distractors on a neutral background.\nonly on this speaker RNN performs poorly on its\nown. Pragmatic reasoning on top of the listener\nRNN alone also yields improvements, which more-\nover come primarily in the hardest cases: 1) contexts\nwith colors that are very similar, thus requiring the\ninterpretation of descriptions that convey ﬁne dis-\ntinctions; and 2) target colors that most referring ex-\npressions fail to identify, whether due to a lack of ad-\nequate descriptive terms or a consistent bias against\nthe color in the RNN listener.\n2 Task and data collection\nWe evaluate our agents on a task of language un-\nderstanding in a dyadic reference game (Rosen-\nberg and Cohen, 1964; Krauss and Weinheimer,\n1964; Paetzel et al., 2014). Unlike traditional natu-\nral language processing tasks, in which participants\nprovide impartial judgements of language in iso-\nlation, reference games embed language use in a\ngoal-oriented communicative context (Clark, 1996;\nTanenhaus and Brown-Schmidt, 2008). Since they\noffer the simplest experimental setup where many\npragmatic and discourse-level phenomena emerge,\nthese games have been used widely in cognitive sci-\nence to study topics like common ground and con-\nventionalization (Clark and Wilkes-Gibbs, 1986),\nreferential domains (Brown-Schmidt and Tanen-\nhaus, 2008), perspective-taking (Hanna et al., 2003),\nand overinformativeness (Koolen et al., 2011).\nTo obtain a corpus of natural color reference data\nacross varying contexts, we recruited 967 unique\nparticipants from Amazon Mechanical Turk to play\n1,059 games of 50 rounds each, using the open-\n326\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\nsource framework of Hawkins (2015). Participants\nwere sorted into dyads, randomly assigned the role\nof speaker or listener, and placed in a game envi-\nronment containing a chat box and an array of three\ncolor patches (Figure 1). On each round, one of the\nthree colors was chosen to be the target and high-\nlighted for the speaker. They were instructed to com-\nmunicate this information to the listener, who could\nthen click on one of the colors to advance to the next\ntrial. Both participants were free to use the chat box\nat any point.\nTo ensure a range of difﬁculty, we randomly in-\nterspersed an equal number of trials from three dif-\nferent conditions: 1) close, where colors were all\nwithin a distance of θfrom one another but still per-\nceptible,1 2) split, where one distractor was within\na distance of θof the target, but the other distractor\nwas farther than θ, and 3) far, where all colors were\nfarther than θfrom one another. Colors were rejec-\ntion sampled uniformly from RGB (red, green, blue)\nspace to meet these constraints.\nAfter excluding extremely long messages, 2 in-\ncomplete games, and games whose participants self-\nreported confusion about the instructions or non-\nnative English proﬁciency, we were left with a\ncorpus of 53,365 speaker utterances across 46,994\nrounds in 948 games. The three conditions are\nequally represented, with 15,519 close trials, 15,693\nsplit trials, and 15,782 far trials. Participants were\nallowed to play more than once, but the modal num-\nber of games played per participant was one (75%).\nThe modal number of messages sent per round was\nalso one (90%). We release the ﬁltered corpus\nwe used throughout our analyses alongside the raw,\npre-ﬁlter data collected from these experiments (see\nFootnote 11).\n3 Behavioral results\nOur corpus was developed not only to facilitate the\ndevelopment of models for grounded language un-\n1We used the most recent CIEDE standard to measure color\ndifferences, which is calibrated to human vision (Sharma et al.,\n2005). All distances were constrained to be larger than a lower\nbound of ϵ = 5 to ensure perceptible differences, and we used\na threshold value of θ= 20 to create conditions.\n2Speciﬁcally, we set a length criterion at 4σ of the mean\nnumber of words per message (about 14 words, in our case), ex-\ncluding 627 utterances. These often included meta-commentary\nabout the game rather than color terms.\nderstanding, but also to provide a richer picture\nof human pragmatic communication. The collec-\ntion effort was thus structured like a large-scale be-\nhavioral experiment, closely following experimen-\ntal designs like those of Clark and Wilkes-Gibbs\n(1986). This paves the way to assessing our model\nnot solely based on the listener’s classiﬁcation accu-\nracy, but also in terms of how qualitative features of\nthe speaker’s production compare to that of our hu-\nman participants. Thus, the current section brieﬂy\nreviews some novel ﬁndings from the human corpus\nthat we use to inform our model assessment.\n3.1 Listener behavior\nSince color reference is a difﬁcult task even for hu-\nmans, we compared listener accuracy across condi-\ntions to calibrate our expectations about model per-\nformance. While participants’ accuracy was close to\nceiling (97%) on the far condition, they made sig-\nniﬁcantly more errors on the split (90%) and close\n(83%) conditions (see Figure 4).\n3.2 Speaker behavior\nFor ease of comparison to computational results, we\nfocus on ﬁve metrics capturing different aspects of\npragmatic behavior displayed by both human and ar-\ntiﬁcial speakers in our task (Table 2). In all cases,\nwe report test statistics from a mixed-effects regres-\nsion including condition as a ﬁxed effect and game\nID as a random effect; except where noted, all test\nstatistics reported correspond to p-values < 10−4\nand have been omitted for readability.\nWords and characters We expect human speak-\ners to be more verbose in split and close contexts\nthan far contexts; the shortest, simplest color terms\nfor the target may also apply to one or both dis-\ntractors, thus incentivizing the speaker to use more\nlengthy descriptions to fully distinguish it. Indeed,\neven if they know enough simple color terms to dis-\ntinguish all the colors lexically, they might be unsure\ntheir listeners will and so resort to modiﬁers any-\nway. To assess this hypothesis, we counted the av-\nerage number of words and characters per message.\nCompared to the baseline far context, participants\nused signiﬁcantly more words both in the split con-\ntext (t = 45.85) and the close context (t = 73.06).\nSimilar results hold for the character metric.\n327\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\nhuman S0 S1\nfar split close far split close far split close\n# Chars 7.8 12.3 14.9 9.0 12.8 16.6 9.0 12.8 16.4\n# Words 1.7 2.7 3.3 2.0 2.8 3.7 2.0 2.8 3.7\n% Comparatives 1.7 14.2 12.8 3.6 8.8 13.1 4.2 9.0 13.7\n% High Speciﬁcity 7.0 7.6 7.4 6.4 8.4 7.6 6.8 7.9 7.5\n% Negatives 2.8 10.0 12.9 4.8 8.9 13.3 4.4 8.5 14.1\n% Superlatives 2.2 6.1 16.7 4.7 9.7 17.2 4.8 10.3 16.6\nTable 2: Corpus statistics and statistics of samples from artiﬁcial speakers (rates per utterance).S0: RNN speaker; S1:\npragmatic speaker derived from RNN listener (see Section 4.3). The human and artiﬁcial speakers show many of the\nsame correlations between language use and context type.\nComparatives and superlatives As noted in Sec-\ntion 1, comparative morphology implicitly encodes\na dependence on the context; a speaker who refers\nto the target color as the darker blue is presuppos-\ning that there is another (lighter) blue in the con-\ntext. Similarly, superlatives like the bluest one or\nthe lightest one presuppose that all the colors can be\ncompared along a speciﬁc semantic dimension. We\nthus expect to see this morphology more often where\ntwo or more of the colors are comparable in this way.\nTo test this, we used the Stanford CoreNLP part-of-\nspeech tagger (Toutanova et al., 2003) to mark the\npresence or absence of comparatives (JJR or RBR)\nand superlatives (JJS or RBS) for each message.\nWe found two related patterns across conditions.\nFirst, participants were signiﬁcantly more likely to\nuse both comparatives ( z = 37 .39) and superla-\ntives ( z = 31 .32) when one or more distractors\nwere close to the target. Second, we found evidence\nof an asymmetry in the use of these constructions\nacross the split and close contexts. Comparatives\nwere used signiﬁcantly more often in the split con-\ntext (z = 4.4), where only one distractor was close\nto the target, while superlatives were much more\nlikely to be used in theclose condition (z= 32.72).3\nNegatives In our referential contexts, negation is\nlikely to play a role similar to that of comparatives:\na phrase like not the red or blue one singles out the\nthird color, and blue but not bright blue achieves a\nmore nuanced kind of comparison. Thus, as with\n3We used Helmert coding to test these speciﬁc patterns: the\nﬁrst regression coefﬁcient compares the ‘far’ condition to the\nmean of the other two conditions, and the second regression co-\nefﬁcient compares the ‘split’ condition to the ‘close’ condition.\ncomparatives, we expect negation to be more likely\nwhere one or more distractors are close to the tar-\nget. To test this, we counted occurrences of the\nstring ‘not’ (by far the most frequent negation in the\ncorpus). Compared to the baseline far context, we\nfound that participants were more likely to use neg-\native constructions when one ( z = 27 .36) or both\n(z= 34.32) distractors were close to the target.\nWordNet speciﬁcity We expect speakers to prefer\nbasic color terms wherever they sufﬁce to achieve\nthe communicative goal, since such terms are most\nlikely to succeed with the widest range of listeners.\nThus, a speaker might choose blue even for a clear\nperiwinkle color. However, as the colors get closer\ntogether, the basic terms become too ambiguous,\nand thus the risk of speciﬁc terms becomes worth-\nwhile (though lengthy descriptions might be a safer\nstrategy, as discussed above). To evaluate this idea,\nwe use WordNet (Fellbaum, 1998) to derive a speci-\nﬁcity hierarchy for color terms, and we hypothesized\nthat split or close conditions will tend to lead speak-\ners to go lower in this hierarchy.\nFor each message, we transformed adjectives into\ntheir closest noun forms (e.g. ‘reddish’ →‘red’),\nﬁltered to include only nouns with ‘color’ in their\nhypernym paths, calculated the depth of the hyper-\nnym path of each color word, and took the maxi-\nmum depth occurring in a message. For instance, the\nmessage “deep magenta, purple with some pink” re-\nceived a score of 9. It has three color terms: “purple”\nand “pink,” which have the basic-level depth of 7,\nand “magenta,” which is a highly speciﬁc color term\nwith a depth of 9. Finally, because there weren’t\nmeaningful differences between words at depths of\n328\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\nxx xx xx\nblue 1 1 0\nteal 0 1 0\ndull 1 0 1\n(a) The lexicon Ldeﬁnes\nutterances’ truth values.\nOur neural listener skips L\nand models l0’s probability\ndistributions directly.\nxx xx xx\nblue 50 50 0\nteal 0 100 0\ndull 50 0 50\n(b) The literal listener l0\nchooses colors compatible\nwith the literal semantics\nof the utterance; other than\nthat, it guesses randomly.\nxx xx xx\nblue 50 33 0\nteal 0 67 0\ndull 50 0 100\n(c) The pragmatic speaker\ns1 soft-maximizes the in-\nformativity of its utter-\nances. (For simplicity, α=\n1 and κ(u) = 0.)\nxx xx xx\nblue 60 40 0\nteal 0 100 0\ndull 33 0 67\n(d) The pragmatic listener\nl2 uses Bayes’ rule to in-\nfer the target using the\nspeaker’s utterance as evi-\ndence.\nFigure 2: The basic RSA model applied to a reference task (literal semantics and alternative utterances simpliﬁed for\ndemonstration). (b)-(d) show conditional probabilities (%).\n8 (“rose”, “teal”) and 9 (“tan,” “taupe”), we con-\nducted our analyses on a binary variable thresholded\nto distinguish “high speciﬁcity” messages with a\ndepth greater than 7. We found a small but reli-\nable increase in the likelihood of “high speciﬁcity”\nmessages from human speakers in the split (z =\n2.84,p = 0 .005) and close (z = 2 .33,p = 0 .02)\ncontexts, compared to the baseline far context.\n4 Models\nWe ﬁrst deﬁne the basic RSA model as applied to\nthe color reference games introduced in Section 2;\nan example is shown in Figure 2.\nListener-based listener The starting point of\nRSA is a model of a literal listener:\nl0(t|u,L) ∝L(u,t)P(t) (1)\nwhere tis a color in the context setC, uis a message\ndrawn from a set of possible utterances U, P is a\nprior over colors, and L(u,t) is a semantic interpre-\ntation function that takes the value 1 if uis true of t,\nelse 0. Figure 2a shows the values of Ldeﬁned for a\nvery simple context in which U = {blue,teal,dull},\nand C = {xx , xx , xx }; Figure 2b shows the cor-\nresponding literal listener l0 if the prior P over col-\nors is ﬂat. (In our scalable extension, we will substi-\ntute a neural network model for l0, bypassing Land\nallowing for non-binary semantic judgments.)\nRSA postulates a model of a pragmatic speaker\n(Figure 2c) that behaves according to a distribution\nthat soft-maximizes a utility function rewarding in-\nformativity and penalizing cost:\ns1(u|t,L) ∝eαlog(l0(t|u,L))−κ(u) (2)\nHere, κis a real-valued cost function on utterances,\nand α∈[0,∞) is an inverse temperature parameter\ngoverning the “rationality” of the speaker model. A\nlarge αmeans the pragmatic speaker is expected to\nchoose the most informative utterance (minus cost)\nconsistently; a smallαmeans the speaker is modeled\nas choosing suboptimal utterances frequently.\nFinally, apragmatic listener (Figure 2d) interprets\nutterances by reasoning about the behavior of the\npragmatic speaker:\nl2(t|u,L) ∝s1(u|t,L)P(t) (3)\nThe αparameter of the speaker indirectly affects\nthe listener’s interpretations: the more reliably the\nspeaker chooses the optimal utterance for a referent,\nthe more the listener will take deviations from the\noptimum as a signal to choose a different referent.\nThe most important feature of this model is that\nthe pragmatic listener l2 reasons not about the se-\nmantic interpretation function Ldirectly, but rather\nabout a speaker who reasons about a listener who\nreasons about Ldirectly. The back-and-forth na-\nture of this interpretive process mirrors that of con-\nversational implicature (Grice, 1975) and reﬂects\nmore general ideas from Bayesian cognitive mod-\neling (Tenenbaum et al., 2011). The model and its\nvariants have been shown to capture a wide range of\npragmatic phenomena in a cognitively realistic man-\nner (Goodman and Stuhlm ¨uller, 2013; Smith et al.,\n2013; Kao et al., 2014; Bergen et al., 2016), and\nthe central Bayesian calculation has proven useful\nin a variety of communicative domains (Tellex et al.,\n2014; V ogel et al., 2013).\n329\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\nu1 u2 u3\n(µ,Σ) c1 c2 c3\n• • •\nc3\nEmbedding\nLSTM\nSoftmax\n(a) The L0 agent processes tokens ui of a color descrip-\ntion u sequentially. The ﬁnal representation is trans-\nformed into a Gaussian distribution in color space, which\nis used to score the context colors c1 ...c 3.\nc1 c2 ct\nh h; ⟨s⟩ h; u1 h; u2\nu1 u2 ⟨/s⟩\nLSTM\nFully connected\nSoftmax\n(b) The S0 agent processes the target color ct in context\nand produces tokens ui of a color description sequen-\ntially. Each step in production is conditioned by the con-\ntext representation hand the previous word produced.\nFigure 3: The neural base speaker and listener agents.\nSpeaker-based listener The deﬁnitions of s1 (2)\nand l2 (3) give a general method of deriving a\nspeaker from a listener and vice versa. This sug-\ngests an alternative formulation of a pragmatic lis-\ntener, starting from a literal speaker:\ns0(u|t,L) ∝L(u,t)e−κ(u) (4)\nl1(t|u,L) ∝s0(u|t,L)P(t) (5)\nHere, it is the speaker that reasons about the seman-\ntics, while the listener reasons about this speaker.\nBoth of these versions of RSA pose problems with\nscalability, stemming from the set of messages U\nand the interpretation function L. In most versions\nof RSA, these are speciﬁed by hand (but see Mon-\nroe and Potts 2015). This presents a serious practi-\ncal obstacle to applying RSA to large data sets con-\ntaining realistic utterances. The set U also raises a\nmore fundamental issue: if this set is not ﬁnite (as\none would expect from a compositional grammar),\nthen in general there is no exact way to normalize\nthe s1 scores, since the denominator must sum over\nall messages. The same problem applies to s0, un-\nless Lfactorizes in an unrealistically clean way.\nOver the next few subsections, we overcome these\nobstacles by replacingl0 and s0 with RNN-based lis-\ntener agents, denoted with capital letters: L0, S0.\nWe use theS0 agent both as a base model for a prag-\nmatic listener analogous to l1 in (5) and to acquire\nsample utterances for approximating the normaliza-\ntion required in deﬁning the s1 agent in (2).\n4.1 Base listener\nOur base listener agent L0 (Figure 3a) is an LSTM\nencoder model that predicts a Gaussian distribution\nover colors in a transformed representation space.\nThe input words are embedded in a 100-dimensional\nvector space. Word embeddings are initialized to\nrandom normally-distributed vectors ( µ = 0 , σ =\n0.01) and trained. The sequence of word vectors is\nused as input to an LSTM with 100-dimensional hid-\nden state, and a linear transformation is applied to\nthe output representation to produce the parameters\nµand Σ of a quadratic form4\nscore(f) = −(f −µ)TΣ(f −µ)\nwhere f is a vector representation of a color. Each\ncolor is represented in its simplest form as a three-\ndimensional vector in RGB space. These RGB vec-\ntors are then Fourier-transformed as in Monroe et al.\n(2016) to obtain the representation f.\nThe values of score(f) for each of the K con-\ntext colors are normalized in log space to produce a\nprobability distribution over the context colors. We\ndenote this distribution by L0(t |u,C; θ), where θ\n4The quadratic form is not guaranteed to be negative deﬁnite\nand thus deﬁne a Gaussian; however, it is for >95% of inputs.\nThe distribution over context colors is well-deﬁned regardless.\n330\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\nrepresents the vector of parameters that deﬁne the\ntrained model.\n4.2 Base speaker\nWe also employ an LSTM-based speaker model\nS0(u |t,C; φ). This speaker serves two purposes:\n1) it is used to deﬁne a pragmatic listener akin to l1\nin (5), and 2) it provides samples of alternative ut-\nterances for each context, to avoid enumerating the\nintractably large space of possible utterances.\nThe speaker model consists of an LSTM con-\ntext encoder and an LSTM description decoder (Fig-\nure 3b). In this model, the colors of the context\nci ∈C are transformed into Fourier representation\nspace, and the sequence of color representations is\npassed through an LSTM with 100-dimensional hid-\nden state. The context is reordered to place the tar-\nget color last, minimizing the length of dependence\nbetween the most important input color and the out-\nput (Sutskever et al., 2014) and eliminating the need\nto represent the index of the target separately. The\nﬁnal cell state of this recurrent neural network is\nconcatenated with a 100-dimensional embedding for\nthe previous token output at each step of decoding.\nThe resulting vector is input along with the previous\ncell state to the LSTM cell, and an afﬁne transfor-\nmation and softmax function are applied to the out-\nput to produce a probability distribution predicting\nthe following token of the description. The model is\nsubstantively similar to well-known models for im-\nage caption generation (Karpathy and Fei-Fei, 2015;\nVinyals et al., 2015), which use the output of a con-\nvolutional neural network as the representation of an\ninput image and provide this representation to the\nRNN as an initial state or ﬁrst word (we represent\nthe context using a second RNN and concatenate the\ncontext representation onto each input word vector).\n4.3 Pragmatic agents\nUsing the above base agents, we deﬁne a pragmatic\nspeaker S1 and a pragmatic listener L2:\nS1(u|t,C; θ) = L0(t|u,C; θ)α\n∑\nu′ L0(t|u′,C; θ)α (6)\nL2(t|u,C; θ) = S1(u|t,C; θ)∑\nt′ S1(u|t′,C; θ) (7)\nThese deﬁnitions mirror those in (2) and (3) above,\nwith Lreplaced by the learned weights θ.\nJust as in (2), the denominator in (6) should con-\nsist of a sum over the entire set of potential utter-\nances, which is exponentially large in the maximum\nutterance length and might not even be ﬁnite. As\nmentioned in Section 4.2, we limit this search by\ntaking msamples from S0(u |i,C; φ) for each tar-\nget index i, adding the actual utterance from the test-\ning example, and taking the resulting multiset as the\nuniverse of possible utterances, weighted towards\nfrequently-sampled utterances. 5 Taking a number\nof samples from S0 for each referent in the context\ngives the pragmatic listener a variety of informative\nalternative utterances to consider when interpreting\nthe true input description. We have found thatmcan\nbe small; in our experiments, it is set to 8.\nTo reduce the noise resulting from the stochasti-\ncally chosen alternative utterance sets, we also per-\nform this alternative-set sampling ntimes and aver-\nage the resulting probabilities in the ﬁnal L2 output.\nWe again choose n = 8 as a satisfactory compro-\nmise between effectiveness and computation time.\nBlending with a speaker-based agent A second\npragmatic listener L1 can be formed in a similar\nway, analogous to l1 in (5):\nL1(t|u,C; φ) = S0(u|t,C; φ)∑\nt′ S0(u|t′,C; φ) (8)\nWe expect L1 to be less accurate than L0 or L2,\nbecause it is performing a listener task using only the\noutputs of a model trained for a speaker task. How-\never, this difference in training objective can also\ngive the model strengths that complement those of\nthe two listener-based agents. One might also ex-\npect a realistic model of human language interpreta-\ntion to lie somewhere between the “reﬂex” interpre-\ntations of the neural base listener and the “reasoned”\ninterpretations of one of the pragmatic models. This\nhas an intuitive justiﬁcation in people’s uncertainty\nabout whether their interlocutors are speaking prag-\nmatically: “should I read more into that statement,\nor take it at face value?” We therefore also eval-\nuate models deﬁned as a weighted average of L0\n5An alternative would be to enforce uniqueness within the\nalternative set, keeping it a true set as in the basic RSA formula-\ntion; this could be done with rejection sampling or beam search\nfor the highest-scoring speaker utterances. We found that doing\nso with rejection sampling hurt model performance somewhat,\nso we did not pursue the more complex beam search approach.\n331\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\nand each of L1 and L2, as well as an “ensemble”\nmodel that combines all of these agents. Specif-\nically, we consider the following blends of neural\nbase models and pragmatic models, with Li abbre-\nviating Li(t|u,C; θ,φ) for convenience:\nLa ∝L0βa ·L1−βa\n1 (9)\nLb ∝L0βb ·L1−βb\n2 (10)\nLe ∝Laγ ·L1−γ\nb (11)\nThe hyperparameters in the exponents allow tuning\nthe blend of each pair of models—e.g., overriding\nthe neural model with the pragmatic reasoning in\nLb. The value of the weights βa, βb, and γ can be\nany real number; however, we ﬁnd that good val-\nues of these weights lie in the range [−1,1]. As an\nexample, setting βb = 0 makes the blended model\nLb equivalent to the pragmatic model L2; βb = 1\nignores the pragmatic reasoning and uses the base\nmodel L0’s outputs; and βb = −1 “subtracts” the\nbase model from the pragmatic model (in log prob-\nability space) to yield a “hyperpragmatic” model.\n4.4 Training\nWe split our corpus into approximately equal\ntrain/dev/test sets (15,665 train trials, 15,670 dev,\n15,659 test), ensuring that trials from the same dyad\nare present in only one split. We preprocess the\ndata by 1) lowercasing; 2) tokenizing by splitting\noff punctuation as well as the endings -er, -est, and\n-ish;6 and 3) replacing tokens that appear once or\nnot at all in the training split7 with <unk>. We also\nremove listener utterances and concatenate speaker\nutterances on the same context. We leave handling\nof interactive dialogue to future work (Section 8).\nWe use ADADELTA (Zeiler, 2012) and Adam\n(Kingma and Ba, 2014), adaptive variants of\nstochastic gradient descent (SGD), to train listener\nand speaker models. The choice of optimization al-\ngorithm and learning rate for each model were tuned\nwith grid search on a held-out tuning set consist-\ning of 3,500 contexts. 8 We also use a ﬁne-grained\n6We only apply this heuristic ending segmentation for the\nlistener; the speaker is trained to produce words with these end-\nings unsegmented, to avoid segmentation inconsistencies when\npassing speaker samples as alternative utterances to the listener.\n71.13% of training tokens, 1.99% of dev/test.\n8For L0: ADADELTA, learning rateη= 0.2; for S0: Adam,\nlearning rate α= 0.004.\ngrid search on this tuning set to determine the values\nof the pragmatic reasoning parameters α, β, and γ.\nIn our ﬁnal ensemble Le, we use α = 0 .544, base\nweights βa = 0 .492 and βb = −0.15, and a ﬁnal\nblending weight γ = 0.491. It is noteworthy that the\noptimal value ofβbfrom grid search isnegative. The\neffect of this is to amplify the difference between\nL0 and L2: the listener-based pragmatic model, evi-\ndently, is not quite pragmatic enough.\n5 Model results\n5.1 Speaker behavior\nTo compare human behavior with the behavior of\nour embedded speaker models, we performed the\nsame behavorial analysis done in Section 3.2. Re-\nsults from this analysis are included alongside the\nhuman results in Table 2. Our pragmatic speaker\nmodel S1 did not differ qualitatively from our base\nspeaker S0 on any of the metrics, so we only sum-\nmarize results for humans and the pragmatic model.\nWords and characters We found human speak-\ners to be more verbose when colors were closer\ntogether, in both number of words and number of\ncharacters. As Table 2 shows, our S1 agent shows\nthe same increase in utterance length in the split\n(t = 18 .07) and close (t = 35 .77) contexts com-\npared to the far contexts.\nComparatives and superlatives Humans used\nmore comparatives and superlatives when colors\nwere closer together; however, comparatives were\npreferred in the split contexts, superlatives in the\nclose contexts. Our pragmatic speaker shows the\nﬁrst of these two patterns, producing more compar-\natives (z = 14 .45) and superlatives ( z = 16 ) in\nthe split or close conditions than in the baseline far\ncondition. It does not, however, capture the peak in\ncomparative use in thesplit condition. This suggests\nthat our model is simulating the human strategy at\nsome level, but that more subtle patterns require fur-\nther attention.\nNegations Humans used more negations when the\ncolors were closer together. Our pragmatic speaker’s\nuse of negation shows the same relationship to the\ncontext (z= 8.55 and z= 16.61, respectively).\n332\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\nmodel accuracy (%) perplexity\nL0 83.30 1.73\nL1 = L(S0) 80.51 1.59\nL2 = L(S(L0)) 83.95 1.51\nLa = L0 ·L1 84.72 1.47\nLb = L0 ·L2 83.98 1.50\nLe = La ·Lb 84.84 1.45\nhuman 90.40\nL0 85.08 1.62\nLe 86.98 1.39\nhuman 91.08\nTable 3: Accuracy and perplexity of the base and prag-\nmatic listeners and various blends (weighted averages,\ndenoted A·B). Top: dev set; bottom: test set.\nWordNet speciﬁcity Humans used more “high\nspeciﬁcity” words (by WordNet hypernymy depth)\nwhen the colors were closer together. Our pragmatic\nspeaker showed a similar effect ( z = 2 .65,p =\n0.008 and z= 2.1,p = 0.036, respectively).\n5.2 Listener accuracy\nTable 3 shows the accuracy and perplexity of the\nbase listener L0, the pragmatic listeners L1 and L2,\nand the blended models La, Lb, and Le at resolving\nthe human-written color references. Accuracy dif-\nferences are signiﬁcant 9 for all pairs except L2/Lb\nand La/Le. As we expected, the speaker-based L1\nalone performs the worst of all the models. How-\never, blending it with L0 doesn’t drag down L0’s\nperformance but rather produces a considerable im-\nprovement compared to both of the original mod-\nels, consistent with our expectation that the listener-\nbased and speaker-based models have complemen-\ntary strengths.\nWe observe that L2 signiﬁcantly outperforms its\nown base model L0, showing that pragmatic rea-\nsoning on its own contributes positively. Blending\nthe pragmatic models with the base listener also im-\nproves over both individually, although not signiﬁ-\ncantly in the case of Lb over L2. Finally, the most\neffective listener combines both pragmatic models\nwith the base listener. Plotting the number of ex-\n9p< 0.012, approximate permutation test (Pad´o, 2006) with\nBonferroni correction, 10,000 samples.\n70\n80\n90\n100\nclose split far\ncondition\n% correct\nagent\nliteral (Lₒ)\npragmatic (Lₑ)\nhuman\n0\n2\n4\n6\nclose split far\ncondition\n% of trials changed, Lₒ → Lₑ\nchange\ndeclined\nimproved\nFigure 4: Human and model reference game performance\n(top) and fraction of examples improved and declined\nfrom L0 to Le (bottom) on the dev set, by condition.\namples changed by condition on the dev set (Fig-\nure 4) reveals that the primary gain from including\nthe pragmatic models is in the close and split condi-\ntions, when the model has to distinguish highly sim-\nilar colors and often cannot rely only on basic color\nterms. On the test set, the ﬁnal ensemble improves\nsigniﬁcantly10 over the base model on both metrics.\n6 Model analysis\nExamining the full probability tables for various dev\nset examples offers insight into the value of each\nmodel in isolation and how they complement each\nother when blended together. In particular, we see\nthat the listener-based ( L2) and speaker-based ( L1)\npragmatic listeners each overcome a different kind\nof “blind spot” in the neural base listener’s under-\nstanding ability.\nFirst, we inspect examples in which L2 is supe-\nrior to L0. In most of these examples, the alternative\nutterances sampled from S0 for one of the referents\ni fail to identify their intended referent to L0. The\npragmatic listener interprets this to mean that refer-\nent iis inherently difﬁcult to refer to, and it compen-\nsates by increasing referent i’s probability.\nThis is beneﬁcial when i is the true target. The\n10p< 0.001, approximate permutation test, 10,000 samples.\n333\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\nL0 xxxx xxxx xxxx\nblue 9 91 <1\ntrue blue 11 89 <1\nlight blue <1 >99 <1\nbrightest <1 >99 <1\nbright blue <1 >99 <1\nred <1 1 99\npurple <1 2 98\nS1 xxxx xxxx xxxx\nblue 41 19 <1\ntrue blue 47 19 <1\nlight blue 5 20 <1\nbrightest <1 20 <1\nbright blue 2 20 <1\nred 1 2 50\npurple 5 1 50\nL2 xxxx xxxx xxxx\nblue 68 32 <1\nS0 5.71 7.63 0.01\nL1 43 57 <1\nLa 50 50 <1\nLb 68 32 <1\nLe 59 41 <1\nL0 xxxx xxxx xxxx\ndrab green not the bluer one <1 <1 >99\ngray 96 4 <1\nblue dull green 24 76 <1\nblue <1 >99 <1\nbluish <1 >99 <1\ngreen 4 1 95\nyellow <1 <1 >99\nS1 xxxx xxxx xxxx\ndrab green not the bluer one 1 <1 34\ngray 58 5 <1\nblue dull green 27 28 <1\nblue 2 32 <1\nbluish 1 32 <1\ngreen 10 3 33\nyellow <1 <1 34\nL2 xxxx xxxx xxxx\ndrab green not the bluer one 5 <1 95\nS0 (×10−9) 5.85 0.38 <0.01\nL1 94 6 <1\nLa 92 6 2\nLb 8 1 91\nLe 63 6 32\nFigure 5: Conditional probabilities (%) of all agents for two dev set examples. The target color is boxed, and the\nhuman utterances (blue, drab green not the bluer one) are bolded. Boxed cells for alternative utterances indicate the\nintended target; largest probabilities are inbold. S0 probabilities (italics) are normalized across all utterances. Sample\nsizes are reduced to save space; here, m= 2 and n= 1 (see Section 4.3).\nleft column of Figure 5 shows one such example: a\ncontext consisting of a somewhat prototypical blue,\na bright cyan, and a purple-tinged brown, with the\nutterance blue. The base listener interprets this as\nreferring to the cyan with 91% probability, perhaps\ndue to the extreme saturation of the cyan maximally\nactivating certain parts of the neural network. How-\never, when the pragmatic model takes samples from\nS0 to probe the space of alternative utterances, it\nbecomes apparent that indicating the more ordinary\nblue to the listener is difﬁcult: for the utterances\nchosen by S0 intending this referent (true blue, light\nblue), the listener also chooses the cyan with >89%\nconﬁdence.\nPragmatic reasoning overcomes this difﬁculty.\nOnly two utterances in the alternative set (the ac-\ntual utterance blue and the sampled alternative true\nblue) result in any appreciable probability mass on\nthe true target, so the pragmatic listener’s model of\nthe speaker predicts that the speaker would usually\nchoose one of these two utterances for the prototyp-\nical blue. However, if the target were the cyan, the\nspeaker would have many good options. Therefore,\nthe fact that the speaker chose blue is interpreted as\nevidence for the true target. This mirrors the back-\nand-forth reasoning behind the deﬁnition of conver-\n334\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\n0\n20\n40\n60\n80\n100Saturation\ndistractor 1distractor 1distractor 2distractor 2 targettarget\n0 60 120 180 240 300\nHue\nFigure 6: L0’s log marginal probability density, marginal-\nizing over V (value) in HSV space, of color conditioned\non the utterance drab green not the bluer one . White\nregions have higher probability. Labeled colors are the\nthree colors from the right column of Figure 5.\nsational implicature (Grice, 1975).\nThis reasoning can be harmful when i is one of\nthe distractors: the pragmatic listener is then in dan-\nger of overweighting the distractor and incorrectly\nchoosing it. This is a likely reason for the small per-\nformance difference between L0 and L2. Still, the\nfact that L2 is more accurate overall, in addition to\nthe negative value of βb discovered in grid search,\nsuggests that the pragmatic reasoning provides value\non its own.\nHowever, the ﬁnal performance improves greatly\nwhen we incorporate both listener-based and\nspeaker-based agents. To explain this improvement,\nwe examine examples in which both listener-based\nagents L0 and L2 give the wrong answer but are\noverridden by the speaker-based L1 to produce the\ncorrect referent. The discrepancy between the two\nkinds of models in many of these examples can be\nexplained by the fact that the speaker takes the con-\ntext as input, while the listener does not. The listener\nis thus asked to predict a region of color space from\nthe utterance a priori, while the speaker can take into\naccount relationships between the context colors in\nscoring utterances.\nThe right column of Figure 5 shows an example of\nthis. The context contains a grayish green (the tar-\nget), a grayish blue-green (“distractor 1”), and a yel-\nlowish green (“distractor 2”). The utterance from the\nhuman speaker is drab green not the bluer one, pre-\nsumably intending drab to exclude the brighter yel-\nlowish green. However, the L0 listener must choose\na region of color space to predict based on the utter-\nance alone, without seeing the other context colors.\nFigure 6 shows a visualization of the listener’s\nprediction. The ﬁgure is a heatmap of the proba-\nbility density output by the listener, as a function of\nhue and saturation in HSV (hue, saturation, value)\nspace. We use HSV here, rather than the RGB\ncoordinate system used by the model, because the\nsemantic constraints are more clearly expressed in\nterms of hue and saturation components: the color\nshould be drab (low-saturation) and green (near 120\non the hue spectrum) but not blue (near 240 in hue).\nThe utterance does not constrain the value (roughly,\nbrightness–darkness) component, so we sum over\nthis component to summarize the 3-dimensional dis-\ntribution in 2 dimensions.\nThe L0 model correctly interprets all of these\nconstraints: it gives higher probability to low-\nsaturation colors and greens, while avoiding bluer\ncolors. However, the result is a probability distri-\nbution nearly centered at distractor 2, the brighter\ngreen. In fact, if we were not comparing it to the\nother colors in the context, distractor 2 would be a\nvery good example of a drab green that is not bluish.\nThe speaker S0, however, produces utterances\nconditioned on the context; it has successfully\nlearned that drab would be more likely as a descrip-\ntion of the grayish green than as a description of the\nyellowish one in this context. The speaker-based\nlistener L1 therefore predicts the true target, with\ngreater conﬁdence than L0 or L2. This prediction\nresults in the blends La and Le preferring the true\ntarget, allowing the speaker’s perspective to override\nthe listener’s.\n7 Related work\nPrior work combining machine learning with prob-\nabilistic pragmatic reasoning models has largely fo-\ncused on the speaker side, i.e., generation. Golland\net al. (2010) develop a pragmatic speaker model,\nS(L0), that reasons about log-linear listeners trained\non human utterances containing spatial references\nin virtual-world environments. Tellex et al. (2014)\napply a similar technique, under the name inverse\nsemantics, to create a robot that can informatively\nask humans for assistance in accomplishing tasks.\nMeo et al. (2014) evaluate a model of color descrip-\ntion generation (McMahan and Stone, 2015) on the\n335\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\ncolor reference data of Baumgaertner et al. (2012)\nby creating an L(S0) listener. Monroe and Potts\n(2015) implement an end-to-end trained S(L(S0))\nmodel for referring expression generation in a ref-\nerence game task. Many of these models require\nenumerating the set of possible utterances for each\ncontext, which is infeasible when utterances are as\nvaried as those in our dataset.\nThe closest work to ours that we are aware of is\nthat of Andreas and Klein (2016), who also combine\nneural speaker and listener models in a reference\ngame setting. They propose a pragmatic speaker,\nS(L0), sampling from a neural S0 model to limit\nthe search space and regularize the model toward\nhuman-like utterances. We show these techniques\nhelp in listener (understanding) tasks as well. Ap-\nproaching pragmatics from the listener side requires\neither inverting the pragmatic reasoning (i.e., deriv-\ning a listener from a speaker), or adding another\nstep of recursive reasoning, yielding a two-level de-\nrived pragmatic modelL(S(L0)). We show both ap-\nproaches contribute to an effective listener.\n8 Conclusion\nIn this paper, we present a newly-collected corpus\nof color descriptions from reference games, and we\nshow that a pragmatic reasoning agent incorporating\nneural listener and speaker models interprets color\ndescriptions in context better than the listener alone.\nThe separation of referent and utterance represen-\ntation in our base speaker and listener models in\nprinciple allows easy substitution of referents other\nthan colors (for example, images), although the per-\nformance of the listener agents could be limited by\nthe representation of utterance semantics as a Gaus-\nsian distribution in referent representation space.\nOur pragmatic agents also rely on the ability to enu-\nmerate the set of possible referents. Avoiding this\nenumeration, as would be necessary in tasks with in-\ntractably large referent spaces, is a challenging the-\noretical problem for RSA-like models.\nAnother important next step is to pursue multi-\nturn dialogue. As noted in Section 2, both par-\nticipants in our reference game task could use the\nchat window at any point, and more than half of\ndyads had at least one two-way interaction. Dia-\nlogue agents are more challenging to model than\nisolated speakers and listeners, requiring long-term\nplanning, remembering previous utterances, and (for\nthe listener) deciding when to ask for clariﬁcation or\ncommit to a referent (Lewis, 1979; Brown and Yule,\n1983; Clark, 1996; Roberts, 1996). We release our\ndataset11 with the expectation that others may ﬁnd\ninterest in these challenges as well.\nAcknowledgments\nWe thank Kai Sheng Tai and Ashwin Paranjape for\nhelpful feedback. This material is based in part upon\nwork supported by the Stanford Data Science Initia-\ntive and by the NSF under Grant No. BCS-1456077.\nRXDH was supported by the Stanford Graduate Fel-\nlowship and the NSF Graduate Research Fellowship\nunder Grant No. DGE-114747. NDG was supported\nby the Alfred P. Sloan Foundation Fellowship and\nDARPA under Agreement No. FA8750-14-2-0009.\nAny opinions, ﬁndings, and conclusions or recom-\nmendations expressed in this material are those of\nthe authors and do not necessarily reﬂect the views\nof the NSF, DARPA, or the Sloan Foundation.\nReferences\nJacob Andreas and Dan Klein. 2016. Reasoning about\npragmatics with neural listeners and speakers. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods on Natural Language Processing (EMNLP), pages\n1173–1182.\nBert Baumgaertner, Raquel Fernandez, and Matthew\nStone. 2012. Towards a ﬂexible semantics: Colour\nterms in collaborative reference tasks. In Proceedings\nof the First Joint Conference on Lexical and Computa-\ntional Semantics (*SEM), pages 80–84.\nLeon Bergen, Roger Levy, and Noah D. Goodman. 2016.\nPragmatic reasoning through semantic inference. Se-\nmantics and Pragmatics, 9(20).\nGillian Brown and George Yule. 1983. Discourse Anal-\nysis. Cambridge University Press.\nSarah Brown-Schmidt and Michael K. Tanenhaus. 2008.\nReal-time investigation of referential domains in un-\nscripted conversation: A targeted language game ap-\nproach. Cognitive Science, 32(4):643–684.\nHerbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-\nferring as a collaborative process. Cognition, 22(1):1–\n39.\n11https://cocolab.stanford.edu/datasets/\ncolors.html\n336\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\nHerbert H. Clark. 1996. Using Language. Cambridge\nUniversity Press.\nRichard S. Cook, Paul Kay, and Terry Regier. 2005. The\nWorld Color Survey database. Handbook of Catego-\nrization in Cognitive Science, pages 223–241.\nChristiane Fellbaum. 1998. WordNet: An Electronic\nLexical Database. MIT Press.\nMichael C. Frank and Noah D. Goodman. 2012. Predict-\ning pragmatic reasoning in language games. Science,\n336(6084):998.\nDave Golland, Percy Liang, and Dan Klein. 2010. A\ngame-theoretic approach to generating spatial descrip-\ntions. In Proceedings of the 2010 Conference on\nEmpirical Methods on Natural Language Processing\n(EMNLP), pages 410–419.\nNoah D. Goodman and Michael C. Frank. 2016. Prag-\nmatic language interpretation as probabilistic infer-\nence. Trends in Cognitive Sciences, 20(11):818–829.\nNoah D. Goodman and Andreas Stuhlm ¨uller. 2013.\nKnowledge and implicature: Modeling language un-\nderstanding as social cognition. Topics in Cognitive\nScience, 5(1):173–184.\nH. Paul Grice. 1975. Logic and conversation. In Peter\nCole and Jerry Morgan, editors, Syntax and Seman-\ntics, V olume 3: Speech Acts, pages 43–58. Academic\nPress.\nJoy E. Hanna, Michael K. Tanenhaus, and John C.\nTrueswell. 2003. The effects of common ground and\nperspective on domains of referential interpretation.\nJournal of Memory and Language, 49(1):43–61.\nRobert X. D. Hawkins. 2015. Conducting real-time mul-\ntiplayer experiments on the web. Behavior Research\nMethods, 47(4):966–976.\nJustine T. Kao, Jean Y . Wu, Leon Bergen, and Noah D.\nGoodman. 2014. Nonliteral understanding of num-\nber words. Proceedings of the National Academy of\nSciences, 111(33):12002–12007.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In Proceedings of the 2015 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR),\npages 3128–3137.\nKazuya Kawakami, Chris Dyer, Bryan Routledge, and\nNoah A. Smith. 2016. Character sequence models\nfor colorful words. In Proceedings of the 2016 Con-\nference on Empirical Methods on Natural Language\nProcessing (EMNLP), pages 1949–1954.\nDiederik P. Kingma and Jimmy Lei Ba. 2014. Adam:\nA method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nRuud Koolen, Albert Gatt, Martijn Goudbeek, and Emiel\nKrahmer. 2011. Factors causing overspeciﬁca-\ntion in deﬁnite descriptions. Journal of Pragmatics ,\n43(13):3231–3250.\nRobert M. Krauss and Sidney Weinheimer. 1964.\nChanges in reference phrases as a function of fre-\nquency of usage in social interaction: A preliminary\nstudy. Psychonomic Science, 1(1–12):113–114.\nDavid Lewis. 1979. Scorekeeping in a language game.\nJournal of Philosophical Logic, 8(1):339–359.\nBrian McMahan and Matthew Stone. 2015. A Bayesian\nmodel of grounded color semantics. Transactions of\nthe Association for Computational Linguistics, 3:103–\n115.\nTimothy Meo, Brian McMahan, and Matthew Stone.\n2014. Generating and resolving vague color refer-\nences. In Proceedings of the 18th Workshop on the Se-\nmantics and Pragmatics of Dialogue (SemDial), pages\n107–115.\nWill Monroe and Christopher Potts. 2015. Learning in\nthe Rational Speech Acts model. In Proceedings of the\n20th Amsterdam Colloquium, pages 1–12.\nWill Monroe, Noah D. Goodman, and Christopher Potts.\n2016. Learning to generate compositional color de-\nscriptions. In Proceedings of the 2016 Conference on\nEmpirical Methods on Natural Language Processing\n(EMNLP), pages 2243–2248.\nRandall Munroe. 2010. Color survey results. Online at\nhttp://blog.xkcd.com/2010/05/03/color-survey-results.\nSebastian Pad ´o, 2006. User’s Guide to sigf:\nSigniﬁcance Testing by Approximate Randomisa-\ntion. http://www.nlpado.de/˜sebastian/\nsoftware/sigf.shtml.\nMaike Paetzel, David Nicolas Racca, and David De-\nVault. 2014. A multimodal corpus of rapid dia-\nlogue games. In Proceedings of the Ninth Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC), pages 4189–4195.\nCraige Roberts. 1996. Information structure in dis-\ncourse: Towards an integrated formal theory of prag-\nmatics. Working Papers in Linguistics—Ohio State\nUniversity Department of Linguistics, pages 91–136.\nSeymour Rosenberg and Bertram D. Cohen. 1964.\nSpeakers’ and listeners’ processes in a word commu-\nnication task. Science, 145(3637):1201–1203.\nGaurav Sharma, Wencheng Wu, and Edul N. Dalal. 2005.\nThe CIEDE2000 color-difference formula: Implemen-\ntation notes, supplementary test data, and mathemat-\nical observations. Color Research & Application ,\n30(1):21–30.\nNathaniel J. Smith, Noah D. Goodman, and Michael C.\nFrank. 2013. Learning and using language via recur-\nsive pragmatic reasoning about other agents. In Ad-\nvances in Neural Information Processing Systems 26\n(NIPS 2013), pages 3039–3047.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014. Se-\nquence to sequence learning with neural networks. In\n337\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025\nAdvances in Neural Information Processing Systems\n27 (NIPS 2014), pages 3104–3112.\nMichael K. Tanenhaus and Sarah Brown-Schmidt. 2008.\nLanguage processing in the natural world. Philosoph-\nical Transactions of the Royal Society of London B:\nBiological Sciences, 363(1493):1105–1122.\nStefanie Tellex, Ross A. Knepper, Adrian Li, Daniela\nRus, and Nicholas Roy. 2014. Asking for help using\ninverse semantics. In Robotics: Science and Systems.\nJoshua B. Tenenbaum, Charles Kemp, Thomas L. Grif-\nﬁths, and Noah D. Goodman. 2011. How to grow a\nmind: Statistics, structure, and abstraction. Science,\n331(6022):1279–1285.\nKristina Toutanova, Dan Klein, Christopher D. Manning,\nand Yoram Singer. 2003. Feature-rich part-of-speech\ntagging with a cyclic dependency network. In Pro-\nceedings of the Human Language Technology Confer-\nence/North American Chapter of the Association for\nComputational Linguistics (HLT-NAACL), pages 173–\n180.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Du-\nmitru Erhan. 2015. Show and tell: A neural image\ncaption generator. In Proceedings of the 2015 IEEE\nConference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 3156–3164.\nAdam V ogel, Christopher Potts, and Dan Jurafsky.\n2013. Implicatures and nested beliefs in approximate\nDecentralized-POMDPs. In Proceedings of the 51st\nAnnual Meeting of the Association for Computational\nLinguistics (ACL), pages 74–80.\nMatthew D. Zeiler. 2012. ADADELTA: An\nadaptive learning rate method. arXiv preprint\narXiv:1212.5701.\n338\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00064 by guest on 05 November 2025"
}