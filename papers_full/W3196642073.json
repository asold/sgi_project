{
  "title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners",
  "url": "https://openalex.org/W3196642073",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A693219661",
      "name": "Zhang Ningyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4307961427",
      "name": "Li, Luoqiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2010438866",
      "name": "Chen Xiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2372640170",
      "name": "Deng, Shumin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2351355285",
      "name": "Bi, Zhen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3210421445",
      "name": "Tan, Chuanqi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097369839",
      "name": "Huang, Fei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2048668577",
      "name": "Chen, Huajun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3195893957",
    "https://openalex.org/W3131755153",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W2970959783",
    "https://openalex.org/W3113529090",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3115718313",
    "https://openalex.org/W3012584427",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2995322030",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3188542058",
    "https://openalex.org/W3157374291",
    "https://openalex.org/W3099910226",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3100124407",
    "https://openalex.org/W2279376656",
    "https://openalex.org/W3137573489",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W3113747735",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W3165416482",
    "https://openalex.org/W3034891697",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3106031848",
    "https://openalex.org/W3152497014",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W3132736064",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3198724266",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W3192405822",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W3038105747",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3135934234",
    "https://openalex.org/W3194836374",
    "https://openalex.org/W2964316912"
  ],
  "abstract": "Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners without any prompt engineering. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance. Code is available in https://github.com/zjunlp/DART.",
  "full_text": "Published as a conference paper at ICLR 2022\nDIFFERENTIABLE PROMPT MAKES PRE-TRAINED\nLANGUAGE MODELS BETTER FEW-SHOT LEARNERS\nNingyu Zhang1,2,3‚àó Luoqiu Li1,3‚àó Xiang Chen1,3 Shumin Deng1,3 Zhen Bi2,3\nChuanqi Tan5 Fei Huang5 Huajun Chen1,3,4‚Ä†\n1College of Computer Science and Technology, Zhejiang University\n2School of Software Technology, Zhejiang University\n3Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies\n4Hangzhou Innovation Center, Zhejiang University\n5Alibaba Group\n{zhangningyu,3160102409,xiang chen,231sm,bizhen zju}@zju.edu.cn,\n{chuanqi.tcq,songfang.hsf,f.huang}@alibaba-inc.com\nABSTRACT\nLarge-scale pre-trained language models have contributed signiÔ¨Åcantly to natural\nlanguage processing by demonstrating remarkable abilities as few-shot learners.\nHowever, their effectiveness depends mainly on scaling the model parameters and\nprompt design, hindering their implementation in most real-world applications.\nThis study proposes a novel pluggable, extensible, and efÔ¨Åcient approach named\nDifferentiAble pRompT (DART), which can convert small language models into\nbetter few-shot learners. The main principle behind this approach involves refor-\nmulating potential natural language processing tasks into the task of a pre-trained\nlanguage model and differentially optimizing the prompt template as well as the\ntarget label with backpropagation. Furthermore, the proposed approach can be: (i)\nPlugged to any pre-trained language models; (ii) Extended to widespread classiÔ¨Å-\ncation tasks. A comprehensive evaluation of standard NLP tasks demonstrates that\nthe proposed approach achieves a better few-shot performance1.\n1 I NTRODUCTION\nThe pre-train‚ÄîÔ¨Åne-tune paradigm has become the de facto standard for natural language processing\n(NLP), and has achieved excellent results in several benchmarks (Devlin et al., 2019; Liu et al., 2019;\nLewis et al., 2020; Dong et al., 2019; Bao et al., 2020a). The success of these pioneers seems to\nsuggest that large-scale pre-trained models are always nothing short of a panacea for boosting machine\nintelligence. However, supervised Ô¨Åne-tuning is still prone to labeled data in practice and faces\nunignorable challenges owing to the variations of domains, language, and tasks. These drawbacks\nlead to the research of an important technique, few-shot learning, which can signiÔ¨Åcantly improve the\nlearning capabilities of machine intelligence and practical adaptive applications by accessing only a\nsmall number of labeled examples.\nThe GPT-3 model, introduced by Brown et al. (2020), exhibits impressive few-shot learning capabili-\nties. Given a natural language prompt and 16 labeled samples as demonstrations in the contextual\ninput, GPT-3 achieves 80% of the SOTA results. However, GPT-3 is a fully dense transformer model\nwith 175B parameters, which makes it challenging to deploy in most real-world applications.\nRecently, an emerging Ô¨Åne-tuning methodology has arisen to equip smaller language models (LMs)\nwith few-shot capabilities: adapting the pre-trained LM directly as a predictor through completion\nof a cloze task (Schick & Sch¬®utze (2021; 2020); Gao et al. (2020); Liu et al. (2021c)), which treats\nthe downstream task as a (masked) language modeling problem. These prompts can be used in Ô¨Åne-\ntuning to provide the classiÔ¨Åer with additional task information, especially in the low-data regime.\n‚àóEqual contribution and shared co-Ô¨Årst authorship.\n‚Ä†Corresponding author.\n1Code is available in https://github.com/zjunlp/DART.\n1\narXiv:2108.13161v7  [cs.CL]  4 May 2022\nPublished as a conference paper at ICLR 2022\n[CLS] The drama [MASK] nothing. [SEP] [CLS] The drama [MASK] nothing. [SEP][MASK] [CLS]\nMLM \nHead\nCLS \nHead\n...\ndiscloses\nis\n...\nVocabulary ùí±\nlabel: positive\nlabel: negative\nLabel ùí¥\nMLM Pre-training Conventional Fine-tuning\nDifferentiable Prompt Fine-tuning (Our Approach DART)\nThe drama discloses nothing. [ùëá!] [ùëá\"] [ùëá#] [MASK]\ne(The) e(drama) e(discloses) e(nothing) e([SEP]) \nh([ùëá!]) h([ùëá\"]) h([ùëá#]) e([MASK]).\ne(The) e([MASK]) e(discloses) e(nothing) e([SEP]) \nh([ùëá!]) h([ùëá\"]) h([ùëá#]) e([Y]).\nDifferentiable\nTemplate Tokens\nToken Embeddings\nPre-trained Language Model\nOutput Logits\nToken Embeddings\nClass Discrimination Object Fluency Constraint Object\n‚ãØ ùëå! ‚ãØ ùëå$ ‚ãØ\nBCE\ndrama report\n‚ãØ\n‚ãØ\n‚ãØ\nFigure 1: The architecture of DifferentiAble pRompT (DART) model comparing with MLM pre-\ntraining and conventional Ô¨Åne-tuning, where Ti and Yi are unused or special tokens in the vocabulary.\nWe leverage a few parameters within the language model as the template and label tokens and\noptimize them via backpropagation without introducing additional parameters apart from the model.\nNotably, Scao & Rush (2021) observe that prompting can often compensate for hundreds of data\npoints on average across multiple classiÔ¨Åcation tasks. However, determining the appropriate prompts\nrequires domain expertise, and handcrafting a high-performing prompt often requires impractically\nlarge validation sets (Perez et al. (2021)). Recent studies (Lu et al. (2021); Zhao et al. (2021)) have\nreported that the manual prompt format can be sub-optimal, which would result in the accuracy\nvarying from random guess performance to near the state-of-the-art. Therefore, previous approaches\nhave attempted to search for discrete prompt tokens automatically. However, it is non-trivial for\nwidespread classiÔ¨Åcation tasks to obtain an optimized prompt template and target label token. For\nexample, speciÔ¨Åc classiÔ¨Åcation tasks such as relation extraction with the label of alternate name and\ncountry o f birth cannot specify a single label token in the vocabulary.\nIn this paper, we propose a novel DifferentiAble pRompT (DART) Ô¨Åne-tuning approach, which\nis model-agnostic, parameter-efÔ¨Åcient. As illustrated in Figure 1, the key idea is to leverage a\nfew parameters (unused tokens) in the language model, which serve as the template and label\ntokens, and to optimize them in the continuous space using backpropagation. Subsequently, we\nintroduce differentiable prompt learning to obtain optimized prompt templates as well as labels. Since\nÔ¨Åne-tuning with limited samples can be affected by instability (Dodge et al. (2020); Zhang et al.\n(2021)), we propose an optimization algorithm to jointly learning templates as well as labels. We\nfurther introduce an auxiliary Ô¨Çuency constraint object to ensure the association among the prompt\nembeddings.\nWe conduct extensive experiments on 15 NLP datasets. With only a few training samples across all\nthe tasks, our approach (DART) can obtain a better performance. Notably, absolute performance\nimprovement of up to 23.28%, over the conventional Ô¨Åne-tuning, is obtained on average in the setting\nof K = 8 (and 1.55% for fully supervised settings) on relation extraction datasets with complex label\nsemantics. Our approach can be applied to real-world classiÔ¨Åcation tasks without the high cost of\ncollecting and annotating a large amount of data. The main contributions of this study are as follows:\n‚Ä¢ We propose a new simple framework for few-shot learning, which is pluggable, extensible,\nand efÔ¨Åcient. To the best of our knowledge, optimizing label tokens in continuous space is\nalso a new branch of research that has not been explored in language model prompting.\n2\nPublished as a conference paper at ICLR 2022\n‚Ä¢ A systematic evaluation of 15 NLP tasks shows that the simple-yet-effective method con-\ntributes towards improvements across all these tasks. Remarkably, given only 8 labeled\nsamples per class, our proposed approach can achieve 90% performance of the SOTA results\n(full dataset).\n2 R ELATED WORK\nLanguage Model Prompting. The language model prompting has emerged with the introduction\nof GPT-3 (Brown et al. (2020)), which demonstrates excellent few-shot performance (Liu et al.\n(2021b)). However, GPT-3 is not designed for Ô¨Åne-tuning; it mainly relies on the handcraft prompt\n(in-context learning (Liu et al. (2021a); Zhao et al. (2021); Ding et al. (2021); Min et al. (2021))). Thus,\nrecent studies (Qin & Eisner (2021); Hambardzumyan et al. (2021); Chen et al. (2021)) conducted\nin this Ô¨Åeld have been focused on automatically searching the prompts. Schick & Sch ¬®utze (2021;\n2020) propose the PET, which reformulates the NLP tasks as cloze-style questions and performs\ngradient-based Ô¨Åne-tuning. Tam et al. (2021) improve the PET with a denser supervision object\nduring Ô¨Åne-tuning. Shin et al. (2020) propose the AUTOPROMPT to create prompts for a diverse set\nof tasks based on a gradient-guided search. Han et al. (2021) propose an approach called PTR, which\nleverages logic rules to construct prompts with sub-prompts for many-class text classiÔ¨Åcation. Wang\net al. (2021) reformulate potential NLP task into an entailment one, and then Ô¨Åne-tune the model with\nfew-shot samples. Hu et al. (2021) propose an approach to incorporate external knowledge graph into\nthe verbalizer with calibration. Additionally, Gao et al. (2020) present LM-BFF‚Äîbetter few-shot\nÔ¨Åne-tuning of language models, which leverages T5 (Raffel et al. (2020)) to generate templates and\nsearch label tokens in the vocabulary. However, the utilization of the generative model and the label\nsearch with validation is computation-intensive. Moreover, the prompt search over discrete space is\nsub-optimal due to the continuous nature of neural networks.\nTo overcome these limitations, Liu et al. (2021c) propose P-tuning, which employs trainable continu-\nous prompt embeddings learned by an LSTM. Zhong et al. (2021) propose an effective continuous\nmethod called OPTIPROMPT to optimize prompts for factual probing. Liu et al. (2021c) propose\npreÔ¨Åx-tuning, which keeps language model parameters frozen but optimizes a small continuous task-\nspeciÔ¨Åc vector for natural language generation tasks. Lester et al. (2021) propose a mechanism for\nlearning ‚Äúsoft prompts‚Äù to condition frozen language models to perform downstream tasks. However,\nthese approaches still have to optimize the external parameters (e.g., LSTM in P-tuning) and are\nprone to complex label space.\nConversely, this study aims to develop a novel few-shot learning framework based on pre-trained\nlanguage models which can reduce the prompt engineering (including templates and labels) and\nexternal parameter optimization. Furthermore, the proposed approach only leverages the noninvasive\nmodiÔ¨Åcation of the model, which can be plugged into any pre-trained language model and extended\nto the widespread classiÔ¨Åcation task.\nFew-shot Learning. Few-shot learning can signiÔ¨Åcantly improve the learning capabilities for\nmachine intelligence and practical adaptive applications by accessing only a small number of labeled\nexamples (Zhang et al. (2020)). The proposed approach corresponds to the other few-shot NLP\nmethods, including: (1) Meta-learning (Yu et al. (2018); Bao et al. (2020b); Bansal et al. (2020);\nDeng et al. (2020b;a); Yu et al. (2020)), in which the quantities of the auxiliary tasks are optimized.\n(2) Intermediate training (Phang et al. (2018); Yin et al. (2020)), which supplements the pre-trained\nLMs with further training on the data-rich supervised tasks. (3) Semi-supervised learning (Miyato\net al. (2017); Xie et al. (2020)), which leverages unlabeled samples. The proposed approach focuses\non a more realistic few-shot setting (the number of labeled instances per class can be any variable).\n3 B ACKGROUND\nLet Xin = {x1,x2,..., xL}be a sentence, where xi is the ith token in the input sentence and L is the\nnumber of tokens. SpeciÔ¨Åcally, Xin is converted to a Ô¨Åxed token sequence ÀúXin and then mapped to\na sequence of hidden vectors {hk ‚ààRd}. Given the input sequence, ÀúXin = [CLS]Xin[SEP], the\nconventional Ô¨Åne-tuning approaches leverage a generic head layer over [CLS] embeddings (e.g., an\nMLP layer) to predict an output class. For the prompt-based method, a task-speciÔ¨Åc pattern string\n3\nPublished as a conference paper at ICLR 2022\n(template T ) is designed to coax the model into producing a textual output corresponding to a given\nclass (label token M (Y ))‚Äîwe refer to these two things together as a prompt. SpeciÔ¨Åcally, Xprompt\ncontaining one [MASK] token is directly tasked with the MLM input as:\nXprompt = [CLS]Xin [SEP]T [SEP] (1)\nWhen the prompt is fed into the MLM, the model can obtain the probability distribution\np([MASK]|(Xprompt) of the candidate class, y ‚ààY as:\np(y|Xprompt) = ‚àë\nw‚ààVy\np([MASK] = w|Xprompt) (2)\nwhere w represents the wth label token of class y.\n4 O UR APPROACH\n4.1 M OTIVATION\nIt can be observed from the previous empirical Ô¨Åndings (Gao et al. (2020); Scao & Rush (2021))\nthat an optimal prompt is necessary for the improvement of the pre-trained language models for\nthe few-shot learners. Since templates with discrete tokens may be sub-optimal and are insufÔ¨Åcient\nto represent a speciÔ¨Åc class 2, this study proposes DifferentiAble pRompT, referred to as DART,\nwhich can reduce the requirement of prompt engineering in order to improve the applicability of the\nproposed method in various domains.\n4.2 D IFFERENTIABLE TEMPLATE OPTIMIZATION\nSince the language tokens are discrete variables, Ô¨Ånding the optimal prompts with token searching\nis non-trivial and may easily fall into the local minima. To overcome these limitations, we utilize\npseudo tokens to construct templates and then optimize them with backpropagation. SpeciÔ¨Åcally,\ngiven the template,T = {[T0:i],[MASK],[Ti+1: j]}, which varies from the traditional discrete prompts,\nsatisfying [Ti] ‚ààV and map T into:\n{w([T0:i]),w([MASK]),w([Ti+1:m])} (3)\nDART considers [Ti] as pseudo tokens and maps the template as follows:\n{h0,..., hi,w([MASK]),hi+1,..., hm} (4)\nwhere hi(0 ‚â§i ‚â§j) are trainable parameters. Differentiable template optimization can obtain\nexpressive templates beyond the original vocabularyV . Lastly, the templates, hi, are differentially\noptimized by:\nÀÜh0:m = argmin\nh\nL(Xprompt,y) (5)\nNote that the values of the prompt embeddings, hi, must be co-dependent with each other rather\nthan independent. Unlike P-tuning (Liu et al. (2021c)), which utilizes a bidirectional LSTM, DART\nleverages an auxiliary Ô¨Çuency constraint objective to associate the prompt embeddings with each\nother, thus stimulating the model to focus on context representation learning.\n4.3 D IFFERENTIABLE LABEL OPTIMIZATION\nPrompt-based Ô¨Åne-tuning requires Ô¨Ålling in one word, and the masked word prediction is mapped\nto a verbalizer, which produces a class (i.e., ‚ÄùYes‚Äù: True. ‚ÄùNo‚Äù: False). For each class c ‚ààY , the\n2It is non-trivial to evaluate all options of templates and label tokens.\n4\nPublished as a conference paper at ICLR 2022\nprevious approaches such as LM-BFF (Gao et al. (2020)) estimate the conditional likelihood of the\ninitial L on a pruned set V c ‚äÇV of the top k vocabulary words.\nHowever, the brute-forcing label searching: (1) is computationally intensive and tedious because\nthe Ddev is generally very large, requiring multiple rounds of evaluation. (2) has poor scalability\nwith an increase in the class numbers (many classiÔ¨Åcation datasets have more than 100 classes), the\nnumber of searches may be kC (C represents the total number of classes), which is exponential and\nthus intractable. Additionally, the labels of classes contain rich, complex semantic knowledge, and\none discrete token may be insufÔ¨Åcient to represent this information.\nSpeciÔ¨Åcally, with the labels, Y = {Y1,Y2,.., Ym}, different from the previous approach which converts\nthe class type Yi into a variable number of label tokens {...,v1,..,vk,...}, DART maps the Yj to a\ncontinuous vocabulary space as follows:\nM (Yj) ={hm+ j}, (6)\nwhere m is the number of trainable embedding in template. To avoid optimizing any external\nparameters, {h1,..., hm,.., hm+n}is replaced with unused tokens (e.g., [unused1] or special tokens in\nvocabulary) in V to generate V ‚Ä≤, as shown in Figure 1.\n4.4 T RAINING OBJECTIVES\nSince the pseudo tokens in the prompt template must be co-dependent with each other, we introduce\nan auxiliary Ô¨Çuency constraint training without optimizing any other parameters inspired by Liu et al.\n(2021c); Tam et al. (2021). Overall, there are two objectives: the class discrimination objectiveLC\nand the Ô¨Çuency constraint objective LF .\nClass Discrimination Object The class discrimination objective is the main objective that aims to\nclassify the sentences. As shown in Figure 1, given (Xin,T ), we can generate Xprompt as:\nLC = CE(g(y|Xprompt)). (7)\nwhere CE is the cross-entropy loss function, LC represents the class discrimination loss.\nFluency Constraint Object To ensure the association among the template tokens and to maintain\nthe ability of language understanding inherited from the PLMs, we leverage a Ô¨Çuency constraint\nobject with the MLM. As shown in Figure 1, one token in the input sentence is randomly masked,\nand the masked language prediction is conducted. x and x‚Ä≤are the original and masked sequences,\nrespectively. Let xm be the target token that has been masked out in x‚Ä≤, and g(xm|x‚Ä≤,y) is maximized\nas follows3:\nh(xm|x‚Ä≤,y) = exp([[f (x‚Ä≤,y)]]xm )\n‚àë\nv‚Ä≤‚ààV ‚Ä≤\nexp([[f (x‚Ä≤,y)]]v‚Ä≤) (8)\nLF = ‚àë\nm‚ààM\nBCE(h(xm|x‚Ä≤,y)). (9)\nBy optimizing LF , the language model can obtain a better contextual representation with a rich\nassociation among the template tokens. We have the following training object:\nL = LC +ŒªLF , (10)\nwhere Œª is the hyper-parameter. Lastly, we introduce the overall optimization procedure of DART.\nTo mitigate the instability of the few-shot Ô¨Åne-tuning, we jointly optimize templates and labels. Note\nthat our approach can reuse the same transformer architecture (rather than additional LSTM) so that\nit enjoys the beauty of simplicity for prompt-tuning.\n3We use the golden label y rather than the [MASK] in the input of the Ô¨Çuency constraint object.\n5\nPublished as a conference paper at ICLR 2022\nModel SST-2 (acc) MR(acc) CR(acc) Subj(acc) TREC(acc)\nMajority‚Ä† 50.9 50.0 50.0 50.0 18.8\nPrompt-based zero-shot‚Ä° 83.6 80.8 79.5 51.4 32.0\n‚ÄúGPT-3‚Äù in-context learning 84.8 (1.3) 80.5 (1.7) 87.4 (0.8) 53.6 (1.0) 26.2 (2.4)\nFine-tuning 81.4 (3.8) 76.9 (5.9) 75.8 (3.2) 90.8 (1.8) 88.8 (2.1)\nLM-BFF 92.3 (1.0) 85.5 (2.8) 89.0 (1.4) 91.2 (1.1) 88.2 (2.0)\nP-Tuning 92.2 (0.4) 86.7 (1.2) 91.8 (1.1) 90.3 (2.2) 86.3 (4.5)\nDART 93.5 (0.5) 88.2 (1.0) 91.8 (0.5) 90.7 (1.4) 87.1(3.8)\nFine-tuning (full)‚Ä† 95.0 90.8 89.4 97.0 97.4\nModel MNLI (acc) SNLI(acc) QNLI(acc) MRPC(F1) QQP(F1)\nMajority‚Ä† 32.7 33.8 49.5 81.2 0.0\nPrompt-based zero-shot‚Ä° 50.8 49.5 50.8 61.9 49.7\n‚ÄúGPT-3‚Äù in-context learning 52.0 (0.7) 47.1 (0.6) 53.8 (0.4) 45.7 (6.0) 36.1 (5.2)\nFine-tuning 45.8 (6.4) 48.4 (4.8) 60.2 (6.5) 76.6 (2.5) 60.7 (4.3)\nLM-BFF 68.3 (2.5) 77.1 (2.1) 68.3 (7.4) 76.2 (2.3) 67.0 (3.0)\nP-Tuning 61.5 (2.1) 72.3 (3.0) 64.3 (2.8) 74.5 (7.6) 65.6 (3.0)\nDART 67.5 (2.6) 75.8 (1.6) 66.7 (3.7) 78.3 (4.5) 67.8 (3.2)\nFine-tuning (full)‚Ä† 89.8 92.6 93.3 91.4 81.7\nTable 1: Our main results with RoBERTa-large. ‚Ä†: the full training set is used. ‚Ä°: no training\nexamples are used. Otherwise, we use K = 16 (# examples per class). We report mean (and standard\ndeviation) performance over 5 different splits. Majority: majority class ‚ÄúGPT-3‚Äù in-context learning:\nusing the in-context learning proposed in with RoBERTa-large (no parameter updates); LM-BFF: we\nreport the performance in Gao et al. (2020). full: Ô¨Åne-tuning using full training set.\n5 E XPERIMENTS\nIn this section, we detail the comprehensive experimental results conducted on classiÔ¨Åcation tasks.\nThe promising results demonstrate that our proposed DART substantially outperforms the conven-\ntional Ô¨Åne-tuning method, thus, making pre-trained language models better few-shot learners.\n5.1 D ATASET STATISTICS\nWe conduct a comprehensive study across 15 NLP tasks, which covers sentiment analysis, natural\nlanguage inference, paraphrases, sentence similarity, relation extraction, and event extraction (We\nonly report event argument extraction performance). The evaluation consisted of 10 popular sentence\nclassiÔ¨Åcation datasets (SST-2, MR, CR, Subj, TREC, MNLI, SNLI, QNLI, MRPC, QQP).To further\nevaluate the effectiveness of the proposed approach with complex label space, we conduct experiments\non the relation extraction and event extraction datasets, including SemEval-2010 Task 8 (Hendrickx\net al., 2010), TACRED-Revisit (Alt et al. (2020)), Wiki804 (Han et al., 2019), ChemProt (Kringelum\net al., 2016), and ACE-20055.\n5.2 S ETTINGS\nThe proposed model is implemented using Pytorch (Paszke et al. (2019)). Our experiments are\nconducted with the same setting following LM-BFF ( Gao et al. (2020)), which measures the average\nperformance with a Ô¨Åxed set of seeds, Sseed, across Ô¨Åve different sampled Dtrain for each task. We\nutilize a grid search over multiple hyperparameters and select the best result as measured on Ddev for\neach set {Ds\ntrain,Ddev},s ‚ààSseed. We employ AdamW as the optimizer. We conduct experiments with\na RoBERTa-large (Liu et al. (2019)) on classiÔ¨Åcation tasks for a fair comparison with LM-BFF. We\nleverage an uncased BERT-large (Devlin et al. (2019)) for relation extraction datasets, except that we\nuse SCIBERT (Beltagy et al. (2019)) for the ChemProt dataset. We follow Soares et al. (2019) and\nuse special entity markers uniformly to highlight the entity mentions for relation extraction.\n4https://github.com/thunlp/OpenNRE/\n5https://catalog.ldc.upenn.edu/LDC2006T06\n6\nPublished as a conference paper at ICLR 2022\nDataset Model K = 8 K = 16 K = 32 Full\nSemEval\nFine-tuning 26.3 43.8 64.2 87.8\nLM-BFF 43.2 62.0 72.9 88.0\nDART 51.8 (+25.5) 67.2 (+23.4) 77.3 (+13.1) 89.1 (+1.3)\nTACRED-Revisit\nFine-tuning 7.4 15.5 25.8 75.0\nLM-BFF 21.0 23.7 27.1 76.4\nDART 25.8 (+18.4) 30.1 (+14.6) 31.8 (+6.0) 77.8 (+2.8)\nWiKi80\nFine-tuning 46.3 60.3 70.0 87.5\nLM-BFF 66.5 73.5 78.1 86.2\nDART 68.5 (+22.2) 75.2 (+14.9) 79.4 (+9.4) 88.1 (+0.6)\nChemProt\nFine-tuning 30.2 41.5 52.5 79.5\nLM-BFF 55.0 56.1 60.0 79.1\nDART 57.2 (+27.0) 60.8 (+19.3) 63.1 (+10.6) 81.0 (+1.5)\nTable 2: Results on RE dataset WiKi80 (accuracy), while other datasets (micro F 1). We use\nK = 8,16,32 (# examples per class). Full represents the full training set is used.\nMethod K=8 K=16 K=32 Full\nConventional FT 26.3 43.8 64.2 87.8\nDART 51.8 67.2 77.3 89.1\n-Ô¨Çuency constraint object 50.3 (-1.5) 66.1 (-1.1) 76.0 (-1.3) 88.2 (-0.9)\n-differentiable template 49.8 (-2.0) 66.3 (-0.9) 76.2 (-1.1) 88.4 (-0.7)\n-differentiable label 47.5 (-4.3) 62.5 (-4.7) 73.7 (-0.6) 87.8 (-1.3)\nTable 3: Ablation of DART with different components on SemEval. (FT= Fine tuning)\n5.3 M AIN RESULTS\nAs shown in Table 1, we observe that our approach obtains better performance than conventional\nÔ¨Åne-tuning and achieves comparable results with LM-BFF. Note that DART can reduce the prompt\nengineering without external models (e.g., T5 in LM-BFF) to generate templates that are readily\neasy to adapt to other datasets. DART can obtain 11.3% improvement with only 16 training samples\nper class on the MR dataset, comparable with LM-BFF, which leverages T5 to generate appropriate\nprompts. These results indicate that DART can better stimulate potential ability and makes the pre-\ntrained language model a better few-shot learner. We also notice that DART yields better performance\nthan P-tuning, which indicates that label optimization is beneÔ¨Åcial.\nFor the classiÔ¨Åcation tasks with the complex label space, as shown in Table 2 and Figure 2(a), we\nobserve that DART outperforms the conventional Ô¨Åne-tuning approach as well as LM-BFF with\na large margin on relation extraction and event extraction datasets in both the few-shot and fully\nsupervised settings. The proposed approach achieves an improvement of 2.8% of the absolute\nperformance on the TACRED-Revisit dataset with full supervision and yields 18.4% gains with only\n8 training samples per class. These Ô¨Åndings also indicate that more relevant templates and labels can\nbe determined without expert intervention, making it possible to generalize the proposed approach to\nother domains. We attribute the signiÔ¨Åcant improvements to the fact that, unlike the GLUE datasets\ncontaining small categories, in relation extraction and event extraction tasks, the datasets consist of a\nlarge number of classes with complex label space, making it more challenging to obtain suitable label\ntokens. Furthermore, we notice that the improvement decays slowly when K becomes larger (i.e.,\nfrom 8 to 32). Our approach is a simple yet effective Ô¨Åne-tuning paradigm that can reduce prompt\nengineering within the complex label space, thus, making it possible to be an appropriate plug-in for\nsome SOTA models.\n5.4 A BLATION STUDY\nWe conduct an ablation study to validate the effectiveness of the components in the proposed approach.\nWe observe that DART exhibits a performance decay in the absence of any one of the modules, i.e.,\n7\nPublished as a conference paper at ICLR 2022\n4 8 16 32 full\nK\n30\n40\n50\n60\n70Micro F1 (%)\nDART\nLM-BFF\nBERT\n(a) Event extraction results on ACE-2005.\n8 16 32 full\nK\n40\n60\n80Micro F1 (%)\nDART(BERT)\nDART(GPT-2-medium)\nFT(GPT-2-medium)\nFT(BERT) (b) BERT-large & GPT-2-medium results on Se-\nmEval.\nFigure 2: (a) Few-shot results using the ACE-2005. We used K = 4, 8, 16, and 32 (# examples per\nclass) with BERT. (FT= Fine-tuning) (b) BERT-large vs. GPT-2-medium results for the SemEval.\nMoreover, for lower K, our method consistently outperforms conventional Ô¨Åne-tuning.\nÔ¨Çuency constraint object, differentiable template, or differentiable label, demonstrating that all the\nmodules are advantageous. Furthermore, we notice that differentiable label optimization is more\nsensitive to performance and is highly beneÔ¨Åcial for DART, especially for low-resource settings.\nSince the proposed approach is the Ô¨Årst approach that utilizes the differentiable label optimization,\nthese Ô¨Åndings illustrate that a suitable label token is important.\n5.5 A NALYSIS AND DISCUSSION\nCAN DART B E APPLIED TO OTHER PRE-TRAINED LMS?\nTo evaluate whether the proposed approach can be applied to other LMs, we conduct experiments\nusing GPT-2-medium6 . From Figure 2(b), we observe that DART with GPT-2-medium yields better\nperformance than the conventional Ô¨Åne-tuning approach. Furthermore, we notice that DART with\nGPT-2-medium can achieve performance on par with BERT-large, as observed by Liu et al. (2021c),\nindicating that the potential of GPT-style architectures for natural language understanding has been\nunderestimated.\nWHY DO DIFFERENTIABLE PROMPTS YIELD BETTER PERFORMANCE ?\nTo further analyze why our differentiable prompts method yields better performance compared with\nprompts with Ô¨Åxed templates and label tokens, we visualize the representation of masked tokens in\nthe CR dataset during different training steps (from left to right) as shown in Figure 3 (Ô¨Åxed) and\n4 (differentiable), respectively. While both methods learn separable hidden states, differentiable\nprompts‚Äô representation is relatively more compact while the representation generated from Ô¨Åxed\nprompts is more scattered. This observation of differentiable prompts generating more discriminative\nrepresentations than the Ô¨Åxed prompts method is supported by an indicator RD, the ratio between\naverage intra-class and average inter-class distance. We believe the main reason behind its better\nperformance lies in the more discriminative representation of the differentiable method. More details\ncan be found in Appendix A.6.\nWHAT EXACTLY IS OPTIMIZED PROMPT ?\nSince prompt templates and label tokens in the proposed approach are mapped as{h1,..., hm,.., hm+n},\nwe further analyze what exactly optimized label learned. We conduct a nearest-neighbor vocabulary\nembedding search to project the Top-3 optimized pseudo-label tokens in V to a readable natural\n6We do not utilize the Ô¨Çuency constraint object in GPT-2-medium since the model is not pre-trained with\nMLM objective.\n8\nPublished as a conference paper at ICLR 2022\n50\n 0 50\n60\n40\n20\n0\n20\n40\n60\n80\n50\n 0 50\n100\n75\n50\n25\n0\n25\n50\n75\n100\n50\n 0 50\n100\n75\n50\n25\n0\n25\n50\n75\n100\n50\n 0 50\n100\n75\n50\n25\n0\n25\n50\n75\n100\nFigure 3: Visualization of masked tokens‚Äô representation in different training steps (with training 10,\n30, 50, 70 steps from left to right) with Ô¨Åxed prompts.\n40\n 20\n 0 20\n90\n80\n70\n60\n50\n40\n30\n20\n50\n 0 50 100\n80\n60\n40\n20\n0\n20\n40\n60\n80\n50\n 0 50 100\n60\n40\n20\n0\n20\n40\n60\n80\n50\n 0 50 100\n60\n40\n20\n0\n20\n40\n60\n80\nFigure 4: Visualization of masked tokens‚Äô representation in different training steps (with training 10,\n30, 50, 70 steps from left to right) with differentiable prompts.\nlanguage.We use t-SNE (Van der Maaten & Hinton (2008)) with normalization to visualize labels on\nWiki80 dataset. For example, ‚Äúmilitary branch‚Äù refers to as red ‚ãÜ in Figure 5 represents the relation\ntype, which is learned by optimizing the pseudo label in the continuous space, and the ‚Äúvolunteered ‚Äù,\n‚Äúcorporal ‚Äù and ‚Äúbuddies‚Äù, refers to as ‚Ä¢are the tokens closest to the label. This Ô¨Ånding indicates that\nthe differentiable method generates better semantic representation.\nDART V.S. C ONVENTIONAL FINE -TUNING\nFigure 5: A 3D visualization of several\nlabel representations learned in DART.\nThe ability of DART to perform few-shot learning can be\nattributed to the label and being a true language under-\nstanding task, that once the model is capable of perform-\ning it correctly, it can easily apply this knowledge to other\ntasks that are framed as such. SuperÔ¨Åcially, (i) DART does\nnot optimize any new parameters; however, conventional\nÔ¨Åne-tuning should learn an explicit classiÔ¨Åer head over\n[CLS] embeddings, which may fail in the low-data regime.\n(ii) DART has the same task setting as large-scale language\nmodel pre-training.\n6 C ONCLUSION AND FUTURE WORK\nThis paper presents DART, a simple yet effective Ô¨Åne-\ntuning approach that improves the fast-shot learning pre-\ntrained language model. The proposed approach can pro-\nduce satisfactory improvements in the few-shot scenarios when compared to the conventional Ô¨Åne-\ntuning approaches. The proposed method is also pluggable for other language models (e.g., BART)\nand can be extended to other tasks, such as intent detection and sentiment analysis. Intuitively, the\nresults obtained in this study can be used to stimulate future research directions in the few-shot or\nlifelong learning for NLP.\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGMENTS\nWe want to express gratitude to the anonymous reviewers for their hard work and kind comments.\nThis work is funded by National Key R&D Program of China (Funding No.SQ2018YFC000004),\nNSFCU19B2027/NSFC91846204, Zhejiang Provincial Natural Science Foundation of China (No.\nLGG22F030011), Ningbo Natural Science Foundation (2021J190), and Yongjiang Talent Introduction\nProgramme (2021A-156-G).\nREPRODUCIBILITY STATEMENT\nOur code is available in https://github.com/zjunlp/DART for reproducibility. Hyper-\nparameters are provided in the Appendix A.1.\nREFERENCES\nChristoph Alt, Aleksandra Gabryszak, and Leonhard Hennig. TACRED revisited: A thorough\nevaluation of the TACRED relation extraction task. In Dan Jurafsky, Joyce Chai, Natalie Schluter,\nand Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 1558‚Äì1569. Association for\nComputational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.142. URL https://doi.\norg/10.18653/v1/2020.acl-main.142.\nTrapit Bansal, Rishikesh Jha, and Andrew McCallum. Learning to few-shot learn across diverse\nnatural language classiÔ¨Åcation tasks. In Donia Scott, N ¬¥uria Bel, and Chengqing Zong (eds.),\nProceedings of the 28th International Conference on Computational Linguistics, COLING 2020,\nBarcelona, Spain (Online), December 8-13, 2020, pp. 5108‚Äì5123. International Committee on\nComputational Linguistics, 2020. doi: 10.18653/v1/2020.coling-main.448. URL https://doi.\norg/10.18653/v1/2020.coling-main.448.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao,\nSonghao Piao, Ming Zhou, and Hsiao-Wuen Hon. Unilmv2: Pseudo-masked language models\nfor uniÔ¨Åed language model pre-training. In Proceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of\nMachine Learning Research, pp. 642‚Äì652. PMLR, 2020a. URL http://proceedings.mlr.\npress/v119/bao20a.html.\nYujia Bao, Menghua Wu, Shiyu Chang, and Regina Barzilay. Few-shot text classiÔ¨Åcation with\ndistributional signatures. In 8th International Conference on Learning Representations, ICLR\n2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020b. URL https://\nopenreview.net/forum?id=H1emfT4twB.\nIz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientiÔ¨Åc text. In\nKentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.),Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019,\npp. 3613‚Äì3618. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1371.\nURL https://doi.org/10.18653/v1/D19-1371.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot\nlearners. In Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,\nand Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n10\nPublished as a conference paper at ICLR 2022\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo\nSi, and Huajun Chen. Knowledge-aware prompt-tuning with synergistic optimization for relation\nextraction. arXiv preprint arXiv:2104.07650, 2021.\nShumin Deng, Ningyu Zhang, Jiaojian Kang, Yichi Zhang, Wei Zhang, and Huajun Chen. Meta-\nlearning with dynamic-memory-based prototypical network for few-shot event detection. In James\nCaverlee, Xia (Ben) Hu, Mounia Lalmas, and Wei Wang (eds.),WSDM ‚Äô20: The Thirteenth ACM\nInternational Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020,\npp. 151‚Äì159. ACM, 2020a. doi: 10.1145/3336191.3371796. URL https://doi.org/10.\n1145/3336191.3371796.\nShumin Deng, Ningyu Zhang, Zhanlin Sun, Jiaoyan Chen, and Huajun Chen. When low resource\nNLP meets unsupervised language model: Meta-pretraining then meta-learning for few-shot text\nclassiÔ¨Åcation (student abstract). In The Thirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Applications of ArtiÔ¨Åcial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational Advances in ArtiÔ¨Åcial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, pp. 13773‚Äì13774. AAAI Press, 2020b. URL\nhttps://aaai.org/ojs/index.php/AAAI/article/view/7158.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and\nThamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pp. 4171‚Äì\n4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL\nhttps://doi.org/10.18653/v1/n19-1423.\nNing Ding, Yulin Chen, Xu Han, Guangwei Xu, Pengjun Xie, Hai-Tao Zheng, Zhiyuan Liu,\nJuanzi Li, and Hong-Gee Kim. Prompt-learning for Ô¨Åne-grained entity typing. arXiv preprint\narXiv:2108.10604, 2021.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah A. Smith.\nFine-tuning pretrained language models: Weight initializations, data orders, and early stopping.\nCoRR, abs/2002.06305, 2020. URL https://arxiv.org/abs/2002.06305.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. UniÔ¨Åed language model pre-training for natural language\nunderstanding and generation. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-\nimer, Florence d‚ÄôAlch ¬¥e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu-\nral Information Processing Systems 32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pp.\n13042‚Äì13054, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/\nc20bb2d9a50d5ac1f713f8b34d9aac5a-Abstract.html.\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\nlearners. CoRR, abs/2012.15723, 2020. URL https://arxiv.org/abs/2012.15723.\nKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: word-level adversarial\nreprogramming. CoRR, abs/2101.00121, 2021. URL https://arxiv.org/abs/2101.\n00121.\nXu Han, Tianyu Gao, Yuan Yao, Deming Ye, Zhiyuan Liu, and Maosong Sun. Opennre: An open\nand extensible toolkit for neural relation extraction. In Sebastian Pad ¬¥o and Ruihong Huang (eds.),\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019 - System Demonstrations, pp. 169‚Äì174. Association for\nComputational Linguistics, 2019. doi: 10.18653/v1/D19-3029. URL https://doi.org/10.\n18653/v1/D19-3029.\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. PTR: prompt tuning with rules\nfor text classiÔ¨Åcation. CoRR, abs/2105.11259, 2021. URL https://arxiv.org/abs/2105.\n11259.\n11\nPublished as a conference paper at ICLR 2022\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid ¬¥O S¬¥eaghdha, Sebastian\nPad¬¥o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. Semeval-2010 task 8: Multi-\nway classiÔ¨Åcation of semantic relations between pairs of nominals. In Katrin Erk and Carlo Strappa-\nrava (eds.), Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval@ACL\n2010, Uppsala University, Uppsala, Sweden, July 15-16, 2010, pp. 33‚Äì38. The Association for Com-\nputer Linguistics, 2010. URL https://www.aclweb.org/anthology/S10-1006/.\nShengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Juanzi Li, and Maosong Sun. Knowledge-\nable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classiÔ¨Åcation. CoRR,\nabs/2108.02035, 2021. URL https://arxiv.org/abs/2108.02035.\nJens Kringelum, Sonny Kim Kj√¶rulff, S√∏ren Brunak, Ole Lund, Tudor I. Oprea, and Olivier\nTaboureau. Chemprot-3.0: a global chemical biology diseases mapping. Database J. Biol.\nDatabases Curation, 2016, 2016. doi: 10.1093/database/bav123. URL https://doi.org/\n10.1093/database/bav123.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efÔ¨Åcient prompt\ntuning. CoRR, abs/2104.08691, 2021. URL https://arxiv.org/abs/2104.08691.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehension. In Dan Jurafsky, Joyce Chai,\nNatalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 7871‚Äì7880.\nAssociation for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.703. URL\nhttps://doi.org/10.18653/v1/2020.acl-main.703.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What\nmakes good in-context examples for gpt-3? CoRR, abs/2101.06804, 2021a. URL https:\n//arxiv.org/abs/2101.06804.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language\nprocessing. CoRR, abs/2107.13586, 2021b. URL https://arxiv.org/abs/2107.13586.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\nunderstands, too. CoRR, abs/2103.10385, 2021c. URL https://arxiv.org/abs/2103.\n10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically\nordered prompts and where to Ô¨Ånd them: Overcoming few-shot prompt order sensitivity. CoRR,\nabs/2104.08786, 2021. URL https://arxiv.org/abs/2104.08786.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language\nmodel prompting for few-shot text classiÔ¨Åcation. CoRR, abs/2108.04106, 2021. URL https:\n//arxiv.org/abs/2108.04106.\nTakeru Miyato, Andrew M. Dai, and Ian J. Goodfellow. Adversarial training methods for semi-\nsupervised text classiÔ¨Åcation. In 5th International Conference on Learning Representations, ICLR\n2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\nURL https://openreview.net/forum?id=r1X3g2_xl.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nK¬®opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\nhigh-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina\nBeygelzimer, Florence d‚ÄôAlch¬¥e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in\n12\nPublished as a conference paper at ICLR 2022\nNeural Information Processing Systems 32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pp.\n8024‚Äì8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/\nbdbca288fee7f92f2bfa9f7012727740-Abstract.html.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models.\narXiv preprint arXiv:2105.11447, 2021.\nJason Phang, Thibault F¬¥evry, and Samuel R. Bowman. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. CoRR, abs/1811.01088, 2018. URL http://arxiv.\norg/abs/1811.01088.\nGuanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\nCoRR, abs/2104.06599, 2021. URL https://arxiv.org/abs/2104.06599.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniÔ¨Åed text-to-\ntext transformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67, 2020. URL http://jmlr.org/\npapers/v21/20-074.html.\nTeven Le Scao and Alexander M. Rush. How many data points is a prompt worth? CoRR,\nabs/2103.08493, 2021. URL https://arxiv.org/abs/2103.08493.\nTimo Schick and Hinrich Sch¬®utze. It‚Äôs not just size that matters: Small language models are also\nfew-shot learners. CoRR, abs/2009.07118, 2020. URL https://arxiv.org/abs/2009.\n07118.\nTimo Schick and Hinrich Sch¬®utze. Exploiting cloze-questions for few-shot text classiÔ¨Åcation and\nnatural language inference. In Paola Merlo, J¬®org Tiedemann, and Reut Tsarfaty (eds.),Proceedings\nof the 16th Conference of the European Chapter of the Association for Computational Linguistics:\nMain Volume, EACL 2021, Online, April 19 - 23, 2021, pp. 255‚Äì269. Association for Computational\nLinguistics, 2021. URL https://www.aclweb.org/anthology/2021.eacl-main.\n20/.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric Wallace, and Sameer Singh. Autoprompt:\nEliciting knowledge from language models with automatically generated prompts. In Bonnie\nWebber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20,\n2020, pp. 4222‚Äì4235. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.\nemnlp-main.346. URL https://doi.org/10.18653/v1/2020.emnlp-main.346.\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the\nblanks: Distributional similarity for relation learning. In Anna Korhonen, David R. Traum, and\nLlu¬¥ƒ±s M`arquez (eds.), Proceedings of the 57th Conference of the Association for Computational\nLinguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pp.\n2895‚Äì2905. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1279. URL\nhttps://doi.org/10.18653/v1/p19-1279.\nDerek Tam, Rakesh R. Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel. Improving and\nsimplifying pattern exploiting training. CoRR, abs/2103.11955, 2021. URL https://arxiv.\norg/abs/2103.11955.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\nSinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma. Entailment as few-shot learner.\nCoRR, abs/2104.14690, 2021. URL https://arxiv.org/abs/2104.14690.\nQizhe Xie, Zihang Dai, Eduard H. Hovy, Thang Luong, and Quoc Le. Unsupervised data aug-\nmentation for consistency training. In Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell,\nMaria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/\n2020/hash/44feb0096faa8326192570788b38c1d1-Abstract.html.\n13\nPublished as a conference paper at ICLR 2022\nWenpeng Yin, Nazneen Fatema Rajani, Dragomir R. Radev, Richard Socher, and Caiming Xiong.\nUniversal natural language processing with limited annotations: Try few-shot textual entailment\nas a start. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,\nOnline, November 16-20, 2020, pp. 8229‚Äì8239. Association for Computational Linguistics, 2020.\ndoi: 10.18653/v1/2020.emnlp-main.660. URL https://doi.org/10.18653/v1/2020.\nemnlp-main.660.\nHaiyang Yu, Ningyu Zhang, Shumin Deng, Hongbin Ye, Wei Zhang, and Huajun Chen. Bridging text\nand knowledge with multi-prototype embedding for few-shot relational triple extraction. In Donia\nScott, N¬¥uria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on\nComputational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pp.\n6399‚Äì6410. International Committee on Computational Linguistics, 2020. doi: 10.18653/v1/2020.\ncoling-main.563. URL https://doi.org/10.18653/v1/2020.coling-main.563.\nMo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Yu Cheng, Gerald Tesauro, Haoyu\nWang, and Bowen Zhou. Diverse few-shot text classiÔ¨Åcation with multiple metrics. In Marilyn A.\nWalker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pp.\n1206‚Äì1215. Association for Computational Linguistics, 2018. doi: 10.18653/v1/n18-1109. URL\nhttps://doi.org/10.18653/v1/n18-1109.\nNingyu Zhang, Shumin Deng, Zhanlin Sun, Jiaoyan Chen, Wei Zhang, and Huajun Chen. Relation\nadversarial network for low resource knowledge graph completion. In Yennun Huang, Irwin\nKing, Tie-Yan Liu, and Maarten van Steen (eds.),WWW ‚Äô20: The Web Conference 2020, Taipei,\nTaiwan, April 20-24, 2020, pp. 1‚Äì12. ACM / IW3C2, 2020. doi: 10.1145/3366423.3380089. URL\nhttps://doi.org/10.1145/3366423.3380089.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. Revisiting few-\nsample {bert}Ô¨Åne-tuning. In International Conference on Learning Representations, 2021. URL\nhttps://openreview.net/forum?id=cO1IH43yUF.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving\nfew-shot performance of language models. CoRR, abs/2102.09690, 2021. URL https://\narxiv.org/abs/2102.09690.\nZexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is[mask]: Learning vs. learning to\nrecall. In North American Association for Computational Linguistics (NAACL), 2021.\nA A PPENDIX\nOur code is available in the supplementary materials for reproducibility. This section contains details\nabout the training procedures and hyperparameters for each of the datasets. We utilize Pytorch (Paszke\net al., 2019) to conduct experiments with 1 Nvidia 3090 GPUs. All optimizations are performed with\nthe AdamW optimizer with a linear warmup of learning rate over the Ô¨Årst 10% of gradient updates\nto a maximum value, then linear decay over the remainder of the training. Gradients are clipped if\ntheir norm exceeds 1.0, and weight decay on all non-bias parameters is set to 0.01. Early stopping is\nadopted to reduce over-Ô¨Åtting on the training set.\nWe follow LM-BFF (Gao et al., 2020) to measure the average performance of models trained on 5\ndifferent randomly sampledDtrain and Ddev splits, and perform grid search for optimal hyper-parameter\ncombinations on each split, including learning-rate, weight decay, and batch size.\nFor P-tuning (Liu et al., 2021c), due to the limit of search space, we do not set anchor tokens in\nprompt tokens.\nFor DART, we adopt joint optimization to acquire optimal prompts and Ô¨Åne-tune over global parame-\nters. Note that we use base prompts as templates of pseudo tokens to accelerate convergence.\n14\nPublished as a conference paper at ICLR 2022\nTo compare fairly, we use RoBERTa-large (Liu et al., 2019) as pre-trained model for both DART and\nP-tuning framework, following LM-BFF (Gao et al., 2020). We also adopt the best discrete prompts\ntogether with label words in LM-BFF as base prompt settings for each framework, as stated below.\nA.1 H YPER -PARAMETER SEARCH SPACE OF OUR METHOD IN GRID SEARCH\nSST-2, MR, CR, Subj, TREC, QNLI, MRPC, QQP\nThe hyper-parameter search space is (the optimal set of parameters may vary across different tasks\nand data splits):\n‚Ä¢ learning rate [1e-5, 5e-5, 1e-4, 2e-4]\n‚Ä¢ weight decay [0.0, 0.01, 0.05, 0.10]\n‚Ä¢ number epochs [20,30]\n‚Ä¢ batch size: [4, 8, 16, 24, 32]\n‚Ä¢ max seq length: 128\n‚Ä¢ gradient accumulation steps: [ 1, 2]\nMNLI, SNLI\nThe hyper-parameter search space is (the optimal set of parameters may vary across different tasks\nand data splits):\n‚Ä¢ learning rate [1e-5, 5e-5, 1e-4, 2e-4]\n‚Ä¢ weight decay [0.0, 0.01, 0.05, 0.10]\n‚Ä¢ number epochs [30,40]\n‚Ä¢ batch size: [4, 8, 16]\n‚Ä¢ max seq length: 256\n‚Ä¢ gradient accumulation steps: [1, 2]\nTACRED-Revisit, WiKi80, SemEval\nThe hyper-parameter search space are:\n‚Ä¢ learning rate [3e-5,5e-5,1e-5,5e-6]\n‚Ä¢ number epochs [20,30]\n‚Ä¢ batch size: 48\n‚Ä¢ max seq length: 128\n‚Ä¢ gradient accumulation steps: 2\nChemProt\nThe hyper-parameter search space are:\n‚Ä¢ learning rate [3e-5,5e-5,1e-5,5e-6]\n‚Ä¢ number epochs [20,30]\n‚Ä¢ batch size: 48\n‚Ä¢ max seq length: 256\n‚Ä¢ gradient accumulation steps: 4\nDialogRE\nThe hyper-parameter search space is (the optimal set of parameters may vary across different tasks\nand data splits):\n‚Ä¢ learning rate [1e-5, 5e-5, 1e-4, 2e-4]\n15\nPublished as a conference paper at ICLR 2022\n‚Ä¢ weight decay [0.0, 0.10]\n‚Ä¢ number epochs [20,30,40]\n‚Ä¢ batch size: [4, 8]\n‚Ä¢ max seq length: 256\n‚Ä¢ gradient accumulation steps: [1, 2]\nA.2 B ASE PROMPT AND LABEL WORDS\nSST-2, MR, CR\n‚Ä¢ prompt template(length = 3) [‚Äùtext‚Äù, ‚Äùit‚Äù, ‚Äùwas‚Äù, ‚Äù<mask>‚Äù, ‚Äù.‚Äù]\n‚Ä¢ label words {‚Äù0‚Äù: ‚Äùterrible‚Äù, ‚Äù1‚Äù: ‚Äùgreat‚Äù}\nSubj\n‚Ä¢ prompt template(length = 3) [‚Äùtext‚Äù, ‚ÄùThis‚Äù, ‚Äùis‚Äù, ‚Äù<mask>‚Äù, ‚Äù.‚Äù]\n‚Ä¢ label words {‚Äù0‚Äù: ‚Äùincorrect‚Äù, ‚Äù1‚Äù: ‚Äùcorrect‚Äù}\nTREC\n‚Ä¢ prompt template(length = 1) [‚Äù<mask>‚Äù, ‚Äù:‚Äù, ‚Äùtext‚Äù]\n‚Ä¢ label words {‚Äù0‚Äù: ‚ÄùDescription‚Äù, ‚Äù1‚Äù:‚ÄùEntity‚Äù,‚Äù2: ‚ÄùExpression‚Äù,‚Äù3‚Äù: ‚ÄùHuman‚Äù,‚Äù4‚Äù: ‚ÄùLoca-\ntion‚Äù,‚Äù5‚Äù:‚ÄùNumber‚Äù}\nMNLI, SNLI\n‚Ä¢ prompt template(length = 2) [‚Äùtexta‚Äù, ‚Äù?‚Äù, ‚Äù<mask>‚Äù, ‚Äù,‚Äù, ‚Äùtextb‚Äù]\n‚Ä¢ label words {‚Äùcontradiction‚Äù: ‚ÄùNo‚Äù,‚Äùentailment‚Äù: ‚ÄùYes‚Äù, ‚Äùneutral‚Äù: ‚ÄùMaybe‚Äù}\nQNLI\n‚Ä¢ prompt template(length = 2) [‚Äùtexta‚Äù, ‚Äù?‚Äù, ‚Äù<mask>‚Äù, ‚Äù,‚Äù, ‚Äùtextb‚Äù]\n‚Ä¢ label words {‚Äùnot entailment‚Äù: ‚ÄùNo‚Äù,‚Äùentailment‚Äù: ‚ÄùYes‚Äù}\nMRPC, QQP\n‚Ä¢ prompt template(length = 2) [‚Äùtexta‚Äù, ‚Äù?‚Äù, ‚Äù<mask>‚Äù, ‚Äù,‚Äù, ‚Äùtextb‚Äù]\n‚Ä¢ label words {‚Äù0‚Äù: ‚ÄùNo‚Äù, ‚Äù1‚Äù: ‚ÄùYes‚Äù}\nTACRED-Revisit, WiKi80, SemEval,DialogRE\n‚Ä¢ prompt template(length = 3) [‚Äùtext‚Äù, Entity1, ‚Äùis‚Äù, ‚Äùthe‚Äù, ‚Äù<mask>‚Äù, ‚Äùof‚Äù, Entity2]\n‚Ä¢ label words {‚Äùcountry of origin‚Äù, ‚Äùparticipating team‚Äù, ‚Äùparticipant of‚Äù,...}\nA.3 T EMPLATE LENGTH ANALYSIS\nModel Accuracy\nDART (length = 2) 92.6 (0.6)\nDART(length = 3) 93.5 (0.5)\nDART (length = 5) 91.2 (1.1)\nDART (length = 10) 90.6 (0.5)\nFine-tuning 81.4 (3.8)\nTable 4: Few-shot performance on SST-2 task using templates with different length.\n16\nPublished as a conference paper at ICLR 2022\nWe deÔ¨Åne the length of a template as the number of tokens except for input sentence and <mask>\ntoken, and apply DART on templates with different length. The performance of a speciÔ¨Åc template\nlength l is derived by summarizing the averaging accuracy on each few-shot data splits, using template\nT = t1,t2,..., tl. From the Table 4, we observe that for the SST-2 task, the model whose template\nlength is three yield best performance; however, the overall impact of template length is rather\ninsigniÔ¨Åcant as models with different template length obtain relatively similar performance.\nA.4 P ERFORMANCE ON FULL TRAINING SET\nModel SST-2 (acc) MR (acc) CR (acc) Subj (acc) TREC (acc)\nFine-tuning 95.0 90.8 89.4 97.0 97.4\nLM-BFF 94.9 91.9 92.4 96.9 97.3\nDART 94.6 91.3 93.8 96.6 95.6\nModel MNLI (acc) SNLI (acc) QNLI (acc) MRPC (F1) QQP (F1)\nFine-tuning 89.8 92.6 93.3 91.4 81.7\nLM-BFF 89.6 90.3 92.8 91.7 86.4\nDART 87.3 89.5 92.3 90.4 89.5\nTable 5: Full training set results with RoBERTa-large. Fine-tuning: we reported same results as Gao\net al. (2020). LM-BFF: we trained LM-BFF model (without demonstration) on full-training set.\nWe conduct experiments and report the performance of DART with full-sized training data of GLUE\ntasks. From Table 5, we notice that DART obtain better or comparable results compared with the\nstandard Ô¨Åne-tuning and LM-BFF, indicating that prompt-based tuning methods beneÔ¨Åt less from\nfull-sized data.\nA.5 P ERFORMANCE WITH CONSTRAINED LABEL TOKENS\nWe conduct a nearest neighbor vocabulary embedding search to project the best optimized differen-\ntialble label token to a readable natural token. Those tokens are chosen based on cosine-similarity\nbetween all tokens‚Äô embedding and the optimized differentialble label token of DART. We list them\nin descending order with similarity scores (i.e., the token ‚Äògreat‚Äò is chosen as its cosine-similarity\nscore with trained positive label embedding of DART is the highest among all tokens, and the token\n‚Äòterrible‚Äò is the most similar token with the trained negative label embedding; the other tokens are\nselected and listed in descending order with similarity scores). From Table 6, we observe that the\nperformance of Ô¨Åxed prompt models is related to the similarity score of the chosen label token and that\nthe DART model learns more semantic representation for label tokens, thus, yield best performance.\nLabel tokens Accuracy\ndifferentiable token (DART) 91.8 (0.5)\ngreat/terrible 91.5 (0.3)\nfantastic/awful 91.0 (0.6)\namazing/horrible 90.2 (0.8)\ngood/bad 89.6 (0.5)\nTable 6: Few-shot performance on CR task using constrained label tokens with DART.\n17\nPublished as a conference paper at ICLR 2022\n0 10 20 30 40 50 60 70\nTraining steps\n0.5\n0.6\n0.7\n0.8Intra-class distance / inter-class distance\nFixed\nDifferentiable\nFigure 6: The RD ratio curve on dev set of CR task of Ô¨Åxed prompt and differentiable prompt during\ntraining.\nA.6 M ORE EXPERIMENTS\nWe numeralize our observation on representation of masked token with a ratio between the average\nintra-class distance and average inter-class distance of hidden state vectors as RD =\n¬ØDintra\n¬ØDinter\n, where:\n¬ØDintra = 1\nC\nC\n‚àë\nc=1\n¬ØDintra(c) = 1\nC\nC\n‚àë\nc=1\n1\nNc\nNc\n‚àë\ni=1\nNc\n‚àë\nj=1\ndistance(Hc[i],Hc[ j]);\n¬ØDinter = 1\nC(C ‚àí1)\nC\n‚àë\nc1=1\n‚àë\nc2Ã∏=c1\n¬ØDinter(c1,c2) = 1\nC(C ‚àí1)\nC\n‚àë\nc1=1\n‚àë\nc2Ã∏=c1\nNc1\n‚àë\ni=1\nNc2\n‚àë\nj=1\ndistance(Hc1 [i],Hc2 [ j]);\n(11)\nwhere distance is the euclidean metric between two vectors, and Hc[i] means the hidden state\nrepresentation of masked token of i-th sample from class c. For discriminative representation, its\naverage intra-class distance is low as data points within the same class tend to gather together, and its\naverage inter-class distance is high as data points from different classes are separated, so its RD ratio\nshould be close to 0.\nAs is shown in Figure 6, the RD ratio of the differentiable method grows lower than that of the Ô¨Åxed\nlabel method, which shows the hidden state representation trained in the differentiable method has\nbetter linear separability.\nNote that in a masked language model, a linear transformation is performed on the hidden state\nrepresentations, with a linear decoder sharing weights with the model‚Äôs word embeddings serving\nas the Ô¨Ånal token classiÔ¨Åer. Hence it is evident that better linear separability of the representations\nleads to better performance. In our case, the differentiable method yields better performance due to\nits better linear separability.\nA.7 L IMITATIONS\nOur work may fail when the distribution of the task corpus varies from that of the pre-training corpus.\nFor example, a general pre-trained language model may be Ô¨Åne-tuned with more training instances\nin a speciÔ¨Åc domain (e.g., medical domain). This issue can be addressed by intermediate training\n(Phang et al., 2018; Yin et al., 2020; Zhao et al., 2021), and will be analyzed in the future work.\nBesides, our work also shows an instability associated with hyper-parameters which is also observed\nby Dodge et al. (2020); Zhang et al. (2021); Perez et al. (2021) as volatility of few-shot learning\n18\nPublished as a conference paper at ICLR 2022\nin NLP. Overall, however, we believe our work will inspire future work to few-shot settings with\nmore practical applications to low-data settings, e.g., that involve low-resource languages or expert\nannotation.\nA.8 B ROADER IMPACT\nThe pre-train-Ô¨Åne-tune approach has become the standard for natural language processing (NLP).\nHowever, supervised Ô¨Åne-tuning is still practically affected by labeled data. This study proposes a\nnovel pluggable, extensible, and efÔ¨Åcient approach named DifferntiAble pRompT (DART), which\ncan convert small language models into better few-shot learners. We believe that our study makes\na signiÔ¨Åcant contribution to the literature because determining the appropriate prompts requires\ndomain expertise, and handcrafting a high-performing prompt often requires impractically large\nvalidation sets, and these issues have been overcome with the use of the proposed method, which\nis model-agnostic, parameter-efÔ¨Åcient. We experimentally veriÔ¨Åed our proposed approach on 13\nstandard NLP tasks, and it was seen to outperform several standard NLP platforms.\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8526649475097656
    },
    {
      "name": "Task (project management)",
      "score": 0.6867084503173828
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5981810688972473
    },
    {
      "name": "Natural language processing",
      "score": 0.5784593224525452
    },
    {
      "name": "Code (set theory)",
      "score": 0.550329864025116
    },
    {
      "name": "Language model",
      "score": 0.5498349070549011
    },
    {
      "name": "Natural language",
      "score": 0.5029775500297546
    },
    {
      "name": "Shot (pellet)",
      "score": 0.47909167408943176
    },
    {
      "name": "Natural language understanding",
      "score": 0.4356027841567993
    },
    {
      "name": "Differentiable function",
      "score": 0.411583811044693
    },
    {
      "name": "Machine learning",
      "score": 0.33332559466362
    },
    {
      "name": "Programming language",
      "score": 0.29296261072158813
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": []
}