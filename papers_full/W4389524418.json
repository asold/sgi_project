{
  "title": "Adaptive Gating in Mixture-of-Experts based Language Models",
  "url": "https://openalex.org/W4389524418",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2140154394",
      "name": "Jiamin Li",
      "affiliations": [
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2076969185",
      "name": "Qiang Su",
      "affiliations": [
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2101017707",
      "name": "Yitao Yang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2108587029",
      "name": "Yimin Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104339954",
      "name": "Wang Cong",
      "affiliations": [
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2022457978",
      "name": "Hong Xu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287124167",
    "https://openalex.org/W4311252752",
    "https://openalex.org/W2970295111",
    "https://openalex.org/W4389519535",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3205972749",
    "https://openalex.org/W4280504625",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3199518308",
    "https://openalex.org/W3142849873",
    "https://openalex.org/W4310510250",
    "https://openalex.org/W4221156865",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W3175781807",
    "https://openalex.org/W4289828103",
    "https://openalex.org/W4389520274",
    "https://openalex.org/W4308760184",
    "https://openalex.org/W4323322833",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4226079124",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3170796112",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4226515448",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W4386076670",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4372349677"
  ],
  "abstract": "Large language models have demonstrated exceptional language understanding capabilities in many NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE models adopt a fixed gating network where each token is computed by the same number of experts. This contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. Adaptive gating preserves sparsity while improving training efficiency. We further draw upon curriculum learning to better align the order of training samples and maximize the training time savings. Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality. Moreover, we conduct a comprehensive analysis of the gating decisions and present our insights on which tokens are inherently difficult to process, depending on the specific language task.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3577–3587\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAdaptive Gating in Mixture-of-Experts based Language Models\nJiamin Li1, Qiang Su1, Yitao Yang2, Yimin Jiang, Cong Wang1, Hong Xu2\n1City University of Hong Kong, 2The Chinese University of Hong Kong\njiamin.li@my.cityu.edu.hk, qiang.su@my.cityu.edu.hk\nytyang@cse.cuhk.edu.hk, jymthu@gmail.com\ncongwang@cityu.edu.hk, hongxu@cuhk.edu.hk\nAbstract\nLarge language models, such as OpenAI’s Chat-\nGPT, have demonstrated exceptional language\nunderstanding capabilities in various NLP tasks.\nSparsely activated mixture-of-experts (MoE)\nhas emerged as a promising solution for scaling\nmodels while maintaining a constant number of\ncomputational operations. Existing MoE model\nadopts a ﬁxed gating network where each token\nis computed by the same number of experts.\nHowever, this approach contradicts our intu-\nition that the tokens in each sequence vary in\nterms of their linguistic complexity and, conse-\nquently, require different computational costs.\nLittle is discussed in prior research on the trade-\noff between computation per token and model\nperformance. This paper introduces adaptive\ngating in MoE, a ﬂexible training strategy that\nallows tokens to be processed by a variable\nnumber of experts based on expert probabil-\nity distribution. The proposed framework pre-\nserves sparsity while improving training efﬁ-\nciency. Additionally, curriculum learning is\nleveraged to further reduce training time. Ex-\ntensive experiments on diverse NLP tasks show\nthat adaptive gating reduces at most 22.5%\ntraining time while maintaining inference qual-\nity. Moreover, we conduct a comprehensive\nanalysis of the routing decisions and present\nour insights when adaptive gating is used.\n1 Introduction\nThe ﬁeld of natural language processing (NLP)\nhas undergone a remarkable revolution driven by\nthe rapid advancements in language models (Cha;\nTouvron et al., 2023; Bar; pal). They exhibit so-\ncalled “emergent” capabilities for a wide variety\nof applications (Wei et al., 2022). However, as\ndemands for these applications continue to grow,\nscalability of these models poses an increasingly\nchallenging hurdle due to constraints in compu-\ntational resources, memory capacity, interconnect\nbandwidth, etc. (Pope et al., 2023).\nSparsely-activated mixture-of-experts (MoE) is\na promising paradigm to address the scalability\nissue while maintaining a constant number of com-\nputation FLOPs (Lepikhin et al., 2020; Fedus\net al., 2021). MoE utilizes an ensemble of ex-\nperts to collectively tackle the learning task. Each\ninput activates a subset of experts, resulting in\na dynamically-changing and sparse computation\ngraph. This method effectively distributes the com-\nputation among experts, increases model capacity\nand improves training efﬁciency (Du et al., 2022;\nRajbhandari et al., 2022). Very recently, there has\nbeen quite some prior work on improving the per-\nformance of Transformers using MoE (Rajbhandari\net al., 2022; Zoph et al., 2022; Chen et al., 2023a;\nGale et al., 2022).\nDespite MoE’s beneﬁt in scalability, it suffers\nfrom suboptimal training efﬁciency. In particular,\nwe focus on the gating mechanism that selects the\nexperts for each token in this work. Existing MoE\nmodels adopt a ﬁxed top-2 gating in training while\nemploying top-1 gating during inference for shorter\nresponse times. Top-2 gating entails twice the com-\nputational cost per token and doubles the data trans-\nfer size of all-to-all operations compared to top-1.\nYet, it remains unclear whether top-2 gating actu-\nally leads to performance gains that could justify\nthe additional overheads. Therefore, a compre-\nhensive analysis of the trade-off between training\nefﬁciency and model performance is increasingly\ncrucial. More practically, how to construct an MoE\nlanguage model that effectively balances training\nefﬁciency and performance, is of great interest and\nimminent value.\nTowards this end, we present our ﬁrst attempt to\nempirically characterize and improve the efﬁciency\nof the gating mechanism in MoE. We observe that\nacross various models and tasks, a large number\nof tokens display simple linguistic characteristics\nor a single dominant feature, which allows them to\nbe effectively processed using just the top-1 expert.\n3577\nThis observation suggests that the current top-2 gat-\ning strategy incurs unnecessary computation costs\nfor a signiﬁcant number of tokens.\nMotivated by this insight, we further introduce\nadaptive gating in MoE that enables tokens to be\nprocessed by a ﬂexible number of experts depend-\ning on the gating decision. Our approach, in con-\ntrast to conventional MoE models, preserves the\nsparsity of MoE models while enhancing ﬂexibil-\nity in token handling. We incorporate a threshold\nwithin the gating network to conduct adaptive token\nrouting based on the distribution of expert proba-\nbilities. With adaptive gating, the majority of to-\nkens use simple top-1 gating; top-2 gating is selec-\ntively applied only when necessary and beneﬁcial,\nthus signiﬁcantly reducing the computation cost.\nHowever, the training efﬁciency cannot achieve the\nsame improvement as the computation cost due to\nthe fact that tokens with top-2 gating always incur a\nlonger training step, thus becoming the bottleneck.\nTherefore, to enhance training efﬁciency even fur-\nther, we leverage the idea of curriculum learning\nby strategically adjusting the order of training data\nsamples.\nWe conduct extensive experiments on six NLP\ntasks with different encoder and decoder models.\nThe results show that our approach can effectively\nreduce the end-to-end training time by at most\n22.5%, while achieving comparable inference qual-\nity with top-2 gating MoE models. Moreover, we\nshow that the tokens routed to two experts are cou-\npled with the nature of each NLP task. For sen-\ntiment analysis, those are the tokens expressing\nneutral opinions; translation task pays attention\nto sentences with complex structure; Question and\nAnswer connects key words in question and context\nand assign both with top-2 gating; summarization\nputs more effort in understanding the pronouns and\nﬁnding tokens expressing central idea; top-2 rout-\ning decision changes along with the token to gen-\nerated in text completion task and conversational\ntokens in dialogue response task use top-2 experts\nfrequently. Empirically, we ﬁnd that a small thresh-\nold value (i.e. 0.1, 0.2) in adaptive gating can lead\nto a similar performance as top-2 gating.\nOur contributions are as follows:\n• We propose adaptive gating in the MoE train-\ning scheme, which enables tokens to be pro-\ncessed by a ﬂexible number of experts.\n• We leverage curriculum learning to alleviate\nthe training bottleneck caused by varying exe-\ncution times of tokens.\n• We conduct extensive experiments on various\nNLP tasks and datasets and present a thorough\nanalysis of the gating decision of the tokens\nto prove the effectiveness and efﬁciency of\nadaptive gating.\n2 Background\n2.1 Mixture-of-Experts\nMixture-of-Experts (MoE) has been adopted in var-\nious deep neural network models (Shen et al., 2023;\nChen et al., 2023b) and has shown great promise\nin enhancing the performance of language mod-\nels. For example, GShard (Lepikhin et al., 2020)\nand Switch Transformer (Fedus et al., 2021) effec-\ntively scale Transformer-based language models\nwith MoE layers.\nIn particular, these models typically employ an\nMoE layer to substitute the feed-forward network\n(FFN) layer. The MoE layer comprises multiple\nFFNs, each acting as an expert, along with a gating\nnetwork. Each expert i is a fully-connected two-\nlayer network utilizing ReLU activation and with\nits own set of parameters. For a given token x, the\noutput of an expert can be deﬁned as:\nFFN i(x) =ReLU(x ·Wi\n0) ·Wi\n1, (1)\nwhere Wi\n0 and Wi\n1 are the trainable weights of the\ntwo linear layers in expert i.\nThe gating network takes in the embedding vec-\ntor of each token x and multiplies them with its\ntrainable matrix WG. The gate value for a speciﬁc\ntoken can be determined through:\nR = softmax(x ·WG). (2)\nThis softmax activation R indicates the weight of\neach expert in processing the token. The gating\nnetwork then dispatches this token to top-k experts\nwith k highest activations. The ﬁnal output of the\nMoE layer is:\ny =\n∑\ni∈E\nRi ·FFN i(x), (3)\nthat is, the weighted sum of outputs from selected\nexpert(s) E ⊂ {FFN 1, FFN2...FFN N }. The\nsparse nature of MoE improves the model scaling\nin size without increasing the training cost.\nRelated work. Several prior works have explored\nthe efﬁcient use of gating or expert selection in\nMoE. Aoki et al., 2022; Zhou et al., 2022; Haz-\nimeh et al., 2021; Ma et al., 2018 propose different\n3578\napproaches to encourage expert specialization. Dai\net al., 2022 adopt a pre-deﬁned expert assignment\nfor each input categories. Roller et al., 2021; Zuo\net al., 2021 propose to remove gating networks.\nZhou et al., 2022 present a novel selection mecha-\nnism where experts selects token instead of token\nselecting experts. Hazimeh et al., 2021 introduce\nmultiple routing policies to enhance specialization\nin multi-task scenario. Roller et al., 2021 use deter-\nministic hashing, while Zuo et al., 2021 use stochas-\ntic routing. However, it could lead to inconsistent\ninference results. Therefore, they employ a regular-\nized loss to penalize the discrepancy of expert se-\nlection. All existing work adopts a ﬁxed and equal\ncomputation capacity for each token and expert,\nwhile we look into the trade-off between compu-\ntation costs and model performance with adaptive\ngating.\n3 Design\nWe now discuss the design of adaptive gating in\nMoE for training.\n3.1 Adaptive Gating in MoE\nObservation. We ﬁrst present our empirical ﬁnd-\nings from experiments with classical MoE mod-\nels. Speciﬁcally, we extract the softmax activations\nand analyze the probability distribution of expert\nselection for each token in the gating network. Fig-\nures 1 depict the normalized activation values of\nfour sampled tokens across 16 experts. We see that\nfor tokens 1 and 4, their activations of the top-1 and\ntop-2 expert are very close as shown in Figures 1a\nand 1d, while for tokens 2 and 3 a signiﬁcant bias\ntowards the top-1 expert exists as in Figures 1b and\n1c. We ﬁnd that these signiﬁcantly-biased distribu-\ntion accounts for at least 55% of all the tokens in\nour evaluation.\nAdaptive gating. Previous work has demonstrated\nthat MoE experts specialize in different linguistic\naspects. Building upon our empirical ﬁndings, one\ncan see that many tokens can be effectively handled\nby a single expert during the training stage. To\ncontrol the number of experts handling each token,\nwe introduce a threshold parameter, denoted as T.\nIf the activation value difference between the top-1\nexpert, denoted as i, and the top-2 expert, denoted\nas j, is within the threshold T, we consider the\ntoken as requiring both expert i and expert j for\nprocessing. Otherwise, we route the token only to\nthe top-1 expert.\n0 10\nExpert\n0.00\n0.05\n0.10\n0.15\n0.20Norm. Prob.\n(a) Token 1\n0 10\nExpert\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25Norm. Prob. (b) Token 2\n0 10\nExpert\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25Norm. Prob.\n(c) Token 3\n0 10\nExpert\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25Norm. Prob. (d) Token 4\nFigure 1: Normalized expert probability computed by top-2\ngating network from four sampled tokens. Here we use the\nSentiment analysis task list in Table 2.\nGate Norm. Computation Norm. MoE Layer Running Time\nTop-1 0.5 0.67Adaptive (80% Top-1) 0.6x 0.76xAdaptive (50% Top-1) 0.75x 0.92xAdaptive (20% Top-1) 0.9x 0.97x\nTable 1: We compare the computation savings and running\ntime reduction of the MoE layer of varying degrees of top-1\ngating against top-2 gating. The MoE layer running time is\nmeasured on our testbed Section 4.3. Tokens are randomly\nselected from the data batch. Here we also use the Sentiment\nanalysis task list in Table 2. We show the results averaged\nfrom 40 runs.\nLoad balancing loss. Adaptive gating uses a ﬂexi-\nble number of experts to process each token. This\nﬂexibility, however, adds extra difﬁculty to the load\nbalancing problem in training which aims to evenly\ndistribute tokens among all experts. As it is still im-\nportant to prevent the gating network from overly\nconcentrating on a very small number of experts, in\nadaptive gating, we impose the soft load balancing\nconstraints on the top-1 gating decisions, while al-\nlowing top-2 gating decisions to be trained without\nany soft constraints. That is, the loss of each MoE\nlayer i becomes:\nLi = Ei\n∑\ne∈E\nf1\ne pe, (4)\nwhere f1\ne is the fraction of tokens dispatched to\nexpert e among those processed by top-1 gating; pe\nis the average gating probability to experte over all\ntokens in the current batch, and Ei is the number\nof experts at layer i just as in classical MoE (Fedus\net al., 2021).\n3.2 Batching\nChallenge. While adaptive gating provides effec-\ntive computational savings, Transformer MoE’s\nmodel architecture poses a signiﬁcant challenge to\ntraining efﬁciency. Speciﬁcally, there is a mismatch\n3579\nTask Dataset Model Architecture\nSentiment analysis SST-2 (Socher et al., 2013) BERT-Base (Devlin et al., 2018) 12-layer encoderTranslation WMT19 (De->En) (Foundation) FSMT (Ng et al., 2020) 6-layer encoder, 6-layer decoderQuestion and Answer SQuAD (Rajpurkar et al., 2016) BERT-Base (Devlin et al., 2018) 12-layer encoderSummarization CNN/Daily Mail (Hermann et al., 2015; See et al., 2017) BART-Large (Lewis et al., 2019) 12-layer encoder, 12-layer decoderText generation wikitext (Merity et al., 2016) GPT-2 (Radford et al., 2019) 24-layer decoderDialogue response SODA (Kim et al., 2022) DialoGPT-medium (Zhang et al., 2020) 24-layer decoder\nTable 2: Overall performance of adaptive MoE and compared baselines in different NLP tasks. All the models converge to the\nsame loss value.\nin the data processing granularity between the MoE\nexperts and the Attention layer. The MoE experts\noperate on individual tokens, while the Attention\nlayer requires input in the form of a complete sen-\ntence. As a result, although the processing time\nfor a large portion of tokens is reduced by half in\nthe MoE layer, we still need to wait until the re-\nmaining tokens (in the same data batch) complete\ntheir top-2 processing. Consequently, training step\ntime cannot enjoy the same reduction as in compu-\ntation. Table 1 shows the computation reduction\nas well as empirical MoE layer running time, both\nnormalized to conventional top-2 gating. We use\nPyTorch Proﬁler to obtain the computation time of\nMoE layer. For simplicity, here we force a ﬁxed\npercentage of tokens to be routed to only top-1 ex-\npert and measure the running time. The reduction\nin running time is clearly much smaller than the\ncomputation savings.\nCurriculum learning. In adaptive gating, we\npropose to incorporate the concept of curriculum\nlearning to address the aforementioned training ef-\nﬁciency challenge. Curriculum learning (Bengio\net al., 2009), as the name implies, is a paradigm\nwhere training examples are presented to a model\nin increasing order of complexity. It aims to en-\nhance the learning efﬁciency and generalization\nperformance of models. By carefully designing\nthe curriculum, the model is exposed to easier ex-\namples at the initial stages, allowing it to build a\nsolid foundation before tackling more challenging\nconcepts. This gradual learning process has shown\npromising results in NLP (Wang et al., 2021).\nAdjust training data order. Our intuition is that\nthe number of experts required by each token can\nbe an indicator of the token complexity. We can\ntherefore reorder the training data in a way that\nprioritizes simpler sequences during model train-\ning. Additionally, we can group together training\ndata with similar complexity levels to minimize the\nbottleneck effect caused by difﬁcult tokens in need\nof top-2 experts.\nTo quantify the complexity of a training sample\nd, we deﬁne a complexity vector C:\nCd = [rd\n0, rd\n1, ...rd\nL], (5)\nwhere L is the number of MoE layers in the model,\nand ri represents the ratio of tokens processed by\ntop-2 experts to the sequence length (i.e., the total\nnumber of tokens in data sample d) in layer i.\nTo determine the order of the training data, we\nidentify the data sample with the fewest tokens pro-\ncessed by top-2 experts, and calculate the cosine\nsimilarity using complexity vectors of the remain-\ning data samples. Training data is then reordered\nbased on this similarity value, starting from the\nmost similar ones. This approach allows the model\nto gradually learn from simpler sequences and pro-\ngressively handle more complex sequences.\n4 Evaluation\nWe evaluate adaptive gating in MoE on six NLP\ntasks using various encoder and decoder models.\nWe then analyze the gating decision to better un-\nderstand the effectiveness of adaptive gating.\n4.1 Tasks and Models\nTable 2 summarizes the details.\n4.2 Baselines\nWe use the Transformer models from HuggingFace\nand convert the FFN layers to MoE layers (Komat-\nsuzaki et al., 2022). We compare adaptive gating’s\ntraining efﬁciency with the following three base-\nlines and then evaluate the inference performance\nwith top-1 gating MoE.\nDense models. Transformer with no MoE layers.\nTop-2 gating MoE. MoE models with top-2 gat-\ning (Lepikhin et al., 2020; Hazimeh et al., 2021)\nfor training.\nTop-1 gating MoE (Switch Transformer).Switch\nTransformer (Fedus et al., 2021; Kim et al., 2021;\nXue et al., 2022) uses top-1 gating to mitigate train-\ning instabilities.\n3580\nTask Scheme Norm. Training Time Computation FLOPs Inference Performance\nSentiment analysis\nDense 0.88x 2.18G 0.912\nTop-2 Gating 1x 3.28G 0.918\nTop-1 Gating 0.99x 2.18G 0.902\n(Accuracy) Adaptive Gating 0.77x 2.30G 0.919\nEn->De translation\nDense 0.87x 10.6G 40.9\nTop-2 Gating 1x 15.9G 41.1\nTop-1 Gating 1.04x 10.6G 39.5\n(BLEU Score) Adaptive Gating 0.79x 11.5G 41.1\nQuestion and Answer\nDense 0.84x 2.18G 75.7\nTop-2 Gating 1x 3.27G 77.6\nTop-1 Gating 1.07x 2.18G 75.5\n(F1 Score) Adaptive Gating 0.86x 2.36G 77.4\nSummarization\nDense 0.89x 79G 42.3\nTop-2 Gating 1x 119G 43.4\nTop-1 Gating 1.06x 79G 40.8\n(ROUGE-1) Adaptive Gating 0.86x 87G 43.3\nText completion\nDense 0.84x 3.4T 16.3\nTop-2 Gating 1x 4.9T 17.8\nTop-1 Gating 1.14x 3.4T 16.5\n(Perplexity) Adaptive Gating 0.89x 3.73T 17.5\nDialogue response\nDense 0.82x 3.4T 12.5\nTop-2 Gating 1x 4.9T 13.4\nTop-1 Gating 0.93x 3.4T 12.6\n(Perplexity) Adaptive Gating 0.82x 3.76T 13.3\nTable 3: Overall performance of adaptive gating and compared baselines in different NLP tasks. We normalize the training time\nwith reference to the performance of top-2 gating MoE. All the schemes in the same task converge to the same loss.\n4.3 Training Conﬁgurations\nWe use 8 A100 GPUs, each with 40 GB memory.\nData and expert parallel is used for distributed train-\ning. We distribute the experts evenly among all\nthe GPUs. In terms of hyperparameters and model\narchitecture, we adopt the default conﬁgurations es-\ntablished in the existing models (Wolf et al., 2020;\nKwon and Chung, 2023).\nModel architecture. BERT-Base has 12 attention\nheads per layer. The hidden size is 768 and the\nintermediate dimension is 3072. The Transformer\nmodel has 16 attention heads. The hidden size is\n1024 and the intermediate dimension in encoder\nand decoder layers are 8192 and 4096, respectively.\nBART-Large has 16 attention heads. The hidden\nsize is 1024 and the intermediate dimension is 4096.\nGPT-2 and DialoGPT-medium have 16 attention\nheads. The hidden size is 1024 and the intermediate\ndimension is 4096.\nHyperparameters. BERT-Base has a batch size\nof 24 and the learning rate is 0.00003. The maxi-\nmum number of tokens for the translation model is\n4096 with a learning rate of 0.0005. The maximum\nnumber of tokens allowed for BART-Large is set\nto 4096. The learning rate is 0.00001. The batch\nsize of GPT-2 is 8 with a learning rate of 0.00015.\nFor DialoGPT-medium, the batch size and learning\nrate are 64 and 0.0001.\nMoE conﬁgurations. The parameter size of the\nFFN in each model is the same in Baseline and\nMoE models and we set the number of FFNs (i.e.\nexperts) to 16 for all evaluated tasks. The coef-\nﬁcient of the load balancing loss is 0.01. No ca-\npacity constraints are enabled so no tokens would\nbe dropped. The expert parameters are randomly\ninitialized. We normalize the expert probability in\nadaptive gating and set the threshold T to 0.1.\n4.4 Overall Performance\nWe present the overall training and inference perfor-\nmance in Table 3. Overall, adaptive gating achieves\ncomparable performance to the baselines while sig-\nniﬁcantly reducing the training time even compared\nto top-1 gating. This is because though top-1 gat-\ning maximizes the computation saving, it makes\ntraining more difﬁcult to converge to the same loss\nvalue, eventually leading to slightly longer training\ntime compared to top-2 gating in 4 out of 6 tasks\nwe run. An in-depth analysis of how adaptive gat-\ning works in connection to each task is presented\nin Section 4.5.\nSentiment analysis. Adaptive gating in MoE out-\nperforms both Dense models and top-2 gating MoE\n3581\n0 1 2 3 4 5 6 7 8 9 1011\nLayer\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25Top-2 (%)\n(a) Sentiment analysis\n0 1 2 3 4 5 6 7 8 9 1011\nLayer\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25Top-2 (%)\n (b) En->De Translation\n0 1 2 3 4 5 6 7 8 9 1011\nLayer\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25Top-2 (%)\n (c) Question and Answer\n0 2 4 6 8 10121416182022\nLayer\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35Top-2 (%)\n(d) Summarization\n0 2 4 6 8 10121416182022\nLayer\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35Top-2 (%)\n (e) Text generation\n0 2 4 6 8 10121416182022\nLayer\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35Top-2 (%)\n (f) Dialogue response\nFigure 2: Percentage of tokens computed by top-2 experts over all the tokens in each layer when using adaptive gating in MoE.\nin all metrics. While the average computation\nFLOPs per token is higher with adaptive gating\ncompared to top-1 gating MoE, which represents\nthe minimum possible FLOPs in the MoE struc-\nture, adaptive gating requires less training time and\nachieves superior accuracy during the inference\nstage. This is consistent across all the tasks. No-\ntably, only 11.3% of the tokens in our evaluation\nreceive two experts, which is the lowest among all\ntasks. Compared to top-2 gating, adaptive gating fo-\ncuses on assigning more experts to tokens that rep-\nresent neutral opinions, allowing for a more com-\nprehensive decision-making process. Conversely,\ntokens expressing little or obvious sentiment are\ngiven less attention without degrading accuracy.\nTranslation. Adaptive gating delivers the same per-\nformance with top-2 gating while reducing training\ntime and FLOPs per token by 25.6% and 38.2%,\nrespectively. Notably, we observe that the gating\nnetwork in adaptive gating exhibits a particular fo-\ncus on the complexity of sentence structures. Even\ntokens that appear linguistically simple can involve\ntwo experts when they appear in sentences with\nintricate structures and grammar. Overall, 25.6%\nof all trained tokens are routed to two experts.\nQuestion and Answer. The training time with\nadaptive gating is 85.7% that of top-2 gating. Al-\nthough its inference performance is slightly lower,\nit still outperforms top-1 gating. Through our ex-\nperiments (refer to Section 4.6), we discover that\nadaptive gating achieves the best results when the\nthreshold is set to 0.2 for Question and Answer.\nThe gating decision is inﬂuenced by both the con-\ntext and the speciﬁc question being asked. For this\ntask 16.4% tokens receive top-2 processing.\nSummarization. Summarization is the most chal-\nlenging task in our evaluation, as it involves pro-\ncessing long and information-rich articles. Adap-\ntive gating takes 11.8% less training time than top-2\ngating. However, its inference performance slightly\nlags behind. Particularly, in adaptive gating tokens\nselected for top-2 experts exhibit signiﬁcant vari-\nations across different layers. We provide a more\ndetailed analysis of this observation in Section 4.5.\nText completion. We use a GPT-like decoder-\nonly architecture for this task. Adaptive gating\nachieves similar performance as top-2 gating and\nDense models while outperforming top-1 gating.\nWhen compared to top-2 gating, only 21.8% to-\nkens rely on two experts, resulting in a reduction\nof 23.8% in average computation FLOPs per token.\nThe selection of tokens utilizing two experts varies\nconsiderably due to the diverse nature of the input.\nDialogue response. Dialogue response requires\nmore nuanced processing compared to simple text\ngeneration, as it involves generating responses in a\ntargeted role based on narrative input and dialogue\nhistory. The sparsity introduced by MoE is advan-\ntageous for this task. All three MoE approaches\noutperform the Dense model. Among all the tasks\nevaluated, dialogue response exhibits the highest\npercentage, 23.4% of tokens routed to two experts,\nindicating the higher utilization of the top-2 gating\nmechanism among all the tasks. Upon evaluating\nthe tokens, we observe that this task can be viewed\nas a combination of all the other evaluated tasks.\n4.5 Analysis and Insights\nWhile it is intuitive to understand that some minor\ntokens (e.g., “a”, “the”, “is”) only need top-1 expert\nto process, this does not fully explain how and\nwhy adaptive gating works in different NLP tasks.\nThus we analyze how the tokens are processed\nin training with adaptive gating, and make quite\na few interesting observations that can help better\nanswer this question. In a broader sense, we believe\n3582\nour insights are also instrumental towards building\nbetter language models.\nNote that when BPE tokenizer is used, we ag-\ngregate the result by mapping the tokens to the\nnatural language word and perform analysis on the\naggregated statistics.\nSentiment analysis. Sentiment analysis exhibits\nthe lowest percentage of top-2 gating among all\ntasks, and the percentage is stable across layers\n(Figure 2a). The top-2 gating mechanism focuses\non two main types of input here. First, it frequently\nselects tokens that express a more neutral opin-\nion since they are more difﬁcult to classify (Ta-\nble 4). Second, tokens associated with sarcastic\nstatements, double negatives, or conﬂicting opin-\nions are also commonly routed to two experts.\nAdaptive gating effectively identiﬁes these tokens\nearly on in the model as they are relatively easy to\nextract, which explains the stable percentage across\nlayers. A special case is when the input does not\nexplicitly convey any sentiment. Adaptive gating\ntends to initially route all tokens to either the top-1\nor top-2 experts and gradually narrows down to\nmore informative tokens. A typical instance of this\nis “as a dentist’s waiting room.”\nTranslation. We focus on English-to-German\ntranslation only. We examine the top-2 gating re-\nsults based on our understanding of the source sen-\ntences. The distribution of the top-2 gating per-\ncentages varies between the encoder and decoder\nlayers, exhibiting a gradual decrease in the encoder\nlayers and an increase in the decoder layers (Fig-\nure 2b). From sampled tokens and the adjusted data\norder in adaptive gating, we observe that tokens re-\nquiring two experts are usually within the same\nsentence. This observation leads us to infer that\nthe complexity of sentence structure inﬂuences the\ngating results. In Table 4, we present one sentence\ncontaining multiple clauses that are frequently pro-\ncessed by the top-2 experts.\nQuestion and Answer. The percentage of top-2 to-\nkens in question and answer tasks ﬂuctuates across\nlayers (Figure 2c). First, adaptive gating pays ex-\ntra attention to the question itself. Words listed\nin Table 4 are some common examples. These\ntokens often either specify the scope of the ques-\ntion or pose constraints to the answers. Second, in\nthe context side, tokens routed to two experts are\nclosely related to the question in the input as well.\nFor example, asking a question about numbers and\ncomputations would result in top-2 gating on the\nnumbers and the objects those numbers refer to.\nSummarization. In summarization, the percent-\nage of tokens using two experts decreases in both\nencoder and decoder layers (Figure 2d). Based on\nour analysis of sampled tokens, we identify two\npatterns for tokens that are likely to be routed to\ntop-2 experts. First, tokens with multiple meanings\nthat rely on both themselves and the surrounding\ncontext for their ultimate interpretation. They are\noften routed to two experts in the shallow layers.\nSecond, pronoun tokens, as understanding their ref-\nerents is crucial for accurate summarization, use\ntwo experts in the deeper layers. This pattern is\nparticularly prevalent in this task. Additionally, cer-\ntain key tokens (e.g. “in conclusion”, “however”,\n“in all”) that indicate the beginning the central idea\nor the main opinion of the context are often sent to\ntwo experts together with the following tokens.\nText completion. Text completion differs from\nthe previous tasks as it is a decoder-only and auto-\nregressive task. The gating results in text comple-\ntion are inﬂuenced by the current prediction being\ngenerated. The focus of tokens changes dynam-\nically based on the current prediction. It is chal-\nlenging to identify speciﬁc types of tokens that con-\nsistently receive two experts. When predicting a\npronoun, for example, the focus shifts to the names\nof individuals. Similar patterns can be observed\nfor numbers and dates. Additionally, we ﬁnd that\nthe percentage of tokens routed to two experts is\nlinked to the length of the current sequence. Longer\nsequences have a higher percentage of top-2 gating.\nDialogue response. Dialogue response, compared\nto text completion, requires more understanding of\nthe narrative input and the dialogue history. We ﬁnd\nthat lots of effort are put into processing dialogue\nhistory. First, one key distinction is that tokens\nwith a conversational meaning occur much more\nfrequently. These words lack informative content\nbut serve to express human-like sentiments, such\nas gratitude and politeness. We infer that routing\nthese tokens for two experts indicates that there is\na difference between the conversational usage and\nwritten text and it is also critical to learn where and\nwhen these words should be used. Second, given\nthe nature of the dialogue, many conversations are\nbased on underlying assumptions and conditions.\nRelated tokens are usually processed with two to-\nkens to improve the understanding of the context.\nFor instance, the dialogue example provided in Ta-\nble 4 is built on top of a scenario assuming that\n3583\nTask Top-2 gating tokens\nSentiment analysis realistic, thoroughly, handsome but unfulﬁlling, simply, is not the worst movie of the\nyear, generic\nTranslation I believe that anyone who has had the opportunity to visit Algeria during recent months\nor years can make a better assessment of what this terrible outbreak of terrorism means\nto the Algerian people and, indeed, I believe that it would be to our credit if we dealt\nwith this issue in an urgent debate.\nQuestion and Answer Which entity, who else, after what, Up until, who was blamed, in terms of, after,\nWho’s death caused this protest?\nSummarization Japanese actress Rinko Kikuchi walks Anjali Rao through the streets of Tokyo. She\nstunned global cinema audiences with her controversial and Oscar-nominated perfor-\nmance as a lonely deaf girl in the ﬁlm “Babel”. Rinko Kikuchi is one of Japan’s hottest\nyoung actresses and models, recently working with Karl Lagerfeld as the new face of\nChannel. Despite her success , she remains an unconventional ﬁgure in Japan, at odds\nwith the traditional demure image of the Japanese woman and forging a career on her\nown terms...\nText completion Harris announced he would be stepping down as rabbi in 2011, and the synagogue hired\nBoris Dolin as his successor . Born and raised in Oregon, Dolin had worked at Temple\nBeth Israel as a teacher and youth group adviser from 1999 to 2001.\nDialogue response exactly, deﬁnitely, hmm, um, well, I guess, [Narrative] Johnathan plans to tell his parents\nthat he is gay . He feels anxious because he doesn’t know they will react . He is worried\nthat they will be disappointed or even angry with him .\nTable 4: Examples of tokens using top-2 experts in different tasks. Underlined tokens use top-2 gating in a sequence.\n“Johnathan tells his parents that he is gay” and asks\nthe model to answer questions with this condition.\n4.6 Ablation Study\nThreshold T in adaptive gating. We now con-\nduct an ablation study on the threshold T intro-\nduced in adaptive gating. Increasing the threshold\nvalue results in a less sparse model, where more\ntokens are assigned to the top-2 gating mechanism,\nsubsequently increasing the computational FLOPs.\nTable 5 shows the inference performance of dif-\nferent tasks when the threshold is increased from\n0.05 to 0.5. When using a small threshold of 0.05,\nboth the training time and inference performance\nclosely resemble those of top-1 gating MoE. On\nthe other hand, setting the threshold to 0.4 does\nnot always lead to the same performance as top-2\ngating. Together with Table 3, we discover that\nthreshold values of 0.1 and 0.2 often strike a favor-\nable balance between training time and inference\nperformance.\nCurriculum learning. Essentially, we disable the\ndata order adjustment before each epoch and use\nthe random data loader to feed the training set. We\npresent the performance degradation compared to\nthe full-set adaptive gating in Table 6. Since it is\nhighly possible that there is at least one token that\nare routed to top-2 experts, the step time of each\nTask Norm. Training Time Inference Performance\n0.05 0.2 0.3 0.4 0.05 0.2 0.3 0.4\nSentiment analysis 1.02x 0.77x 0.92x 1.01x 0.9120.9180.917 0.918Translation 0.88x 0.83x 0.83x 0.88x 40.241.1 40.8 41.1Question and Answer 0.92x 0.87x 0.93x 0.96x 74.377.6 77.6 77.6Summarization 0.98x 1.02x 1.05x 1.04x 40.8 42.343.1 43.1Text generation 0.95x 0.93x 0.99x 1.01x 16.6 17.217.4 17.4Dialogue response 0.93x 0.91x 1.01x 1.01x 12.2 12.8 13.213.4\nTable 5: Overall performance when the threshold T changes.\nTraining time is normalized with reference to top-2 gating\nMoE. We highlight the best one with the least training time.\nTask Training Time Inﬂation Inference Performance\nSentiment analysis 22% +0.00\nTranslation 14% -0.14\nQuestion and Answer 9% -0.21\nSummarization 14% -0.14\nText completion 12% -0.01\nDialogue response 11% -0.19\nTable 6: Overall performance comparison of adaptive gating\nwhen data batch is not adjusted.\niteration cannot achieve the same level of reduc-\ntion as the computation FLOPs. Consequently, the\nend-to-end training time is signiﬁcantly inﬂated,\nwith an average increase of 13.7%. Additionally,\nthe idea of the curriculum also contributes to the\nimprovement in inference performance. The max-\nimum drop is 0.21 in Question and Answer task\nwhen the data is fed and trained in a random man-\nner.\n3584\n5 Limitation\nChoice of k. Adaptive gating in MoE currently is\nlimited to top-k gating, where k can be either 1 or\n2. This is built on the common practice in extensive\nprior work that top-2 gating shows a promissing\nresut in MoE. Further evaluation is necessary to\nvalidate the performance of a wider range of k val-\nues. Our experiments were conducted on a diverse\nset of NLP tasks and datasets, but it is essential to\nnote that the effectiveness and efﬁciency of adap-\ntive MoE may vary depending on the speciﬁc task\ncharacteristics. Different tasks may exhibit distinct\npatterns and complexities, which can impact the\nperformance and generalizability of the proposed\napproach. Further investigation and evaluation on\na wider range of tasks would provide a more com-\nprehensive understanding of the limitations and\napplicability of adaptive MoE.\n6 Conclusion\nThis paper demonstrates the effectiveness and ﬂexi-\nbility of adaptive gating in MoE models for a wide\nrange of natural language processing tasks. By dy-\nnamically adjusting the number of experts based on\ntoken characteristics, we achieve improved training\nefﬁciency without compromising inference perfor-\nmance. Additionally, the integration of curricu-\nlum learning allows us to tackle the challenge of\nvarying execution times, thereby reducing training\ncosts. Our research sheds light on the trade-off be-\ntween training efﬁciency and model performance in\nsparse and dynamic MoE networks, offering valu-\nable insights for the development of more scalable\nand adaptable language models.\nAcknowledgement\nWe thank the anonymous EMNLP’23 reviewers\nand Area Chairs for their constructive and valu-\nable comments. This work was supported in\npart by funding from the Research Grants Coun-\ncil of Hong Kong (N_CityU139/21, C2004-21GF,\nR1012-21, R6021-20F, GRF 11209520, and CRF\nC7004-22G).\nEthics Statement\nThere is no ethic problem in this work.\nReferences\nAI ACROSS GOOGLE: PaLM 2. https://ai.\ngoogle/discover/palm2/.\nChatGPT: Optimizing Language Models for Dialogue.\nhttps://openai.com/blog/chatgpt/.\nGoogle Bard. https://bard.google.com/.\nRaquel Aoki, Frederick Tung, and Gabriel L Oliveira.\n2022. Heterogeneous multi-task learning with ex-\npert diversity. IEEE/ACM Transactions on Computa-\ntional Biology and Bioinformatics, 19(6):3093–3102.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProc. ICML.\nTianlong Chen, Zhenyu Zhang, AJAY KUMAR\nJAISW AL, Shiwei Liu, and Zhangyang Wang. 2023a.\nSparse MoE as the New Dropout: Scaling Dense and\nSelf-Slimmable Transformers. In Proc. ICLR.\nZitian Chen, Yikang Shen, Mingyu Ding, Zhenfang\nChen, Hengshuang Zhao, Erik G Learned-Miller, and\nChuang Gan. 2023b. Mod-squad: Designing mix-\ntures of experts as modular multi-task learners. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 11828–\n11837.\nYong Dai, Duyu Tang, Liangxin Liu, Minghuan Tan,\nCong Zhou, Jingquan Wang, Zhangyin Feng, Fan\nZhang, Xueyu Hu, and Shuming Shi. 2022. One\nmodel, multiple modalities: A sparsely activated ap-\nproach for text, sound, image, video and code. arXiv\npreprint arXiv:2205.06126.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, Barret\nZoph, Liam Fedus, Maarten P Bosma, Zongwei Zhou,\nTao Wang, Emma Wang, Kellie Webster, Marie Pel-\nlat, Kevin Robinson, Kathleen Meier-Hellstern, Toju\nDuke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui\nWu, Zhifeng Chen, and Claire Cui. 2022. GLaM:\nEfﬁcient scaling of language models with mixture-\nof-experts. In Proceedings of Machine Learning Re-\nsearch.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv\npreprint arXiv:2101.03961.\nWikimedia Foundation. Acl 2019 fourth conference on\nmachine translation (wmt19), shared task: Machine\ntranslation of news.\nTrevor Gale, Deepak Narayanan, Cliff Young, and\nMatei Zaharia. 2022. Megablocks: Efﬁcient sparse\ntraining with mixture-of-experts. arXiv preprint\narXiv:2211.15841.\n3585\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdh-\nery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul\nMazumder, Lichan Hong, and Ed Chi. 2021. Dselect-\nk: Differentiable selection in the mixture of experts\nwith applications to multi-task learning. Advances in\nNeural Information Processing Systems, 34.\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In NIPS.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West,\nXiming Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras,\nMalihe Alikhani, Gunhee Kim, Maarten Sap, and\nYejin Choi. 2022. Soda: Million-scale dialogue dis-\ntillation with social commonsense contextualization.\nArXiv, abs/2212.10465.\nYoung Jin Kim, Ammar Ahmad Awan, Alexandre\nMuzio, Andres Felipe Cruz Salinas, Liyang Lu,\nAmr Hendy, Samyam Rajbhandari, Yuxiong He, and\nHany Hassan Awadalla. 2021. Scalable and efﬁ-\ncient moe training for multitask multilingual models.\narXiv preprint arXiv:2109.10465.\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp,\nCarlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie,\nYi Tay, Mostafa Dehghani, and Neil Houlsby.\n2022. Sparse upcycling: Training mixture-of-\nexperts from dense checkpoints. arXiv preprint\narXiv:2212.05055.\nYoohwan Kwon and Soo-Whan Chung. 2023. Mole:\nMixture of language experts for multi-lingual auto-\nmatic speech recognition. In ICASSP.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. arXiv preprint\narXiv:2006.16668.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan\nHong, and Ed H Chi. 2018. Modeling task relation-\nships in multi-task learning with multi-gate mixture-\nof-experts. In Proceedings of the 24th ACM SIGKDD\ninternational conference on knowledge discovery &\ndata mining.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2020. Facebook\nfair’s wmt19 news translation task submission. In\nProc. of WMT.\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery,\nJacob Devlin, James Bradbury, Jonathan Heek, Kefan\nXiao, Shivani Agrawal, and Jeff Dean. 2023. Efﬁ-\nciently scaling transformer inference. Proc .MLSys.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Min-\njia Zhang, Reza Yazdani Aminabadi, Ammar Ah-\nmad Awan, Jeff Rasley, and Yuxiong He. 2022.\nDeepSpeed-MoE: Advancing Mixture-of-Experts In-\nference and Training to Power Next-Generation AI\nScale. arXiv preprint arXiv:2201.05596.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions for\nMachine Comprehension of Text.\nStephen Roller, Sainbayar Sukhbaatar, Jason Weston,\net al. 2021. Hash layers for large sparse models.\nAdvances in Neural Information Processing Systems,\n34.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers).\nSheng Shen, Zhewei Yao, Chunyuan Li, Trevor Dar-\nrell, Kurt Keutzer, and Yuxiong He. 2023. Scaling\nvision-language models with sparse mixture of ex-\nperts. arXiv preprint arXiv:2303.07226.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proc. EMNLP.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and efﬁ-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nXin Wang, Yudong Chen, and Wenwu Zhu. 2021. A\nsurvey on curriculum learning. IEEE Transactions\non Pattern Analysis and Machine Intelligence.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\n3586\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proc .EMNLP.\nFuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou,\nand Yang You. 2022. One student knows all ex-\nperts know: From sparse to dense. arXiv preprint\narXiv:2201.10890.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. In ACL, system demonstration.\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping\nHuang, Vincent Zhao, Andrew M Dai, Quoc V Le,\nJames Laudon, et al. 2022. Mixture-of-experts with\nexpert choice routing. Advances in Neural Informa-\ntion Processing Systems.\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yan-\nping Huang, Jeff Dean, Noam Shazeer, and William\nFedus. 2022. Designing Effective Sparse Expert\nModels. arXiv preprint arXiv:2202.08906.\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim,\nHany Hassan, Ruofei Zhang, Tuo Zhao, and Jian-\nfeng Gao. 2021. Taming sparsely activated trans-\nformer with stochastic experts. arXiv preprint\narXiv:2110.04260.\n3587",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.804590106010437
    },
    {
      "name": "Inference",
      "score": 0.6468397974967957
    },
    {
      "name": "Gating",
      "score": 0.6362125277519226
    },
    {
      "name": "Language model",
      "score": 0.6328855156898499
    },
    {
      "name": "Security token",
      "score": 0.5842820405960083
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5603457093238831
    },
    {
      "name": "Machine learning",
      "score": 0.5415485501289368
    },
    {
      "name": "Intuition",
      "score": 0.4879262149333954
    },
    {
      "name": "Computation",
      "score": 0.4423356056213379
    },
    {
      "name": "Process (computing)",
      "score": 0.4198063611984253
    },
    {
      "name": "Task (project management)",
      "score": 0.4149775803089142
    },
    {
      "name": "Algorithm",
      "score": 0.13939017057418823
    },
    {
      "name": "Engineering",
      "score": 0.08824673295021057
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Physiology",
      "score": 0.0
    }
  ]
}