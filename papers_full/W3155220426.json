{
  "title": "K-PLUG: Knowledge-injected Pre-trained Language Model for Natural Language Understanding and Generation in E-Commerce",
  "url": "https://openalex.org/W3155220426",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2015885439",
      "name": "Song Xu",
      "affiliations": [
        "JDSU (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2099815881",
      "name": "Haoran Li",
      "affiliations": [
        "JDSU (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1986837181",
      "name": "Peng Yuan",
      "affiliations": [
        "JDSU (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2101011829",
      "name": "Yujia Wang",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2109328759",
      "name": "Youzheng Wu",
      "affiliations": [
        "JDSU (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2122755126",
      "name": "Xiaodong He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2054286775",
      "name": "Ying Liu",
      "affiliations": [
        "Renmin University of China",
        "JDSU (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2110030736",
      "name": "Bowen Zhou",
      "affiliations": [
        "JDSU (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3105111366",
    "https://openalex.org/W2997326549",
    "https://openalex.org/W3104609290",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2951865668",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3101913037",
    "https://openalex.org/W2998733554",
    "https://openalex.org/W2858627132",
    "https://openalex.org/W2890721473",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W3031414376",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W3098003395",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2964309167",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2773765985",
    "https://openalex.org/W3017231514",
    "https://openalex.org/W3037151198",
    "https://openalex.org/W2903268415",
    "https://openalex.org/W2150824314",
    "https://openalex.org/W2970648534",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3087568487",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1524730382",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3035419191",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2808413133",
    "https://openalex.org/W3034682120",
    "https://openalex.org/W3100439847",
    "https://openalex.org/W2949446780",
    "https://openalex.org/W3032187523",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2891416139",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2998559045",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W3115729981",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2983102021",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2952813980",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2798456655",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2938830017"
  ],
  "abstract": "Existing pre-trained language models (PLMs) have demonstrated the effectiveness of self-supervised learning for a broad range of natural language processing (NLP) tasks. However, most of them are not explicitly aware of domain-specific knowledge, which is essential for downstream tasks in many domains, such as tasks in e-commerce scenarios. In this paper, we propose K-PLUG, a knowledge-injected pre-trained language model based on the encoder-decoder transformer that can be transferred to both natural language understanding and generation tasks. Specifically, we propose five knowledge-aware self-supervised pre-training objectives to formulate the learning of domain-specific knowledge, including e-commerce domain-specific knowledge-bases, aspects of product entities, categories of product entities, and unique selling propositions of product entities. We verify our method in a diverse range of e-commerce scenarios that require domain-specific knowledge, including product knowledge base completion, abstractive product summarization, and multi-turn dialogue. K-PLUG significantly outperforms baselines across the board, which demonstrates that the proposed method effectively learns a diverse set of domain-specific knowledge for both language understanding and generation tasks. Our code is available.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1–17\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n1\nK-PLUG: Knowledge-injected Pre-trained Language Model for Natural\nLanguage Understanding and Generation in E-Commerce\nSong Xu∗1, Haoran Li∗1, Peng Yuan1, Yujia Wang2, Youzheng Wu1,\nXiaodong He1, Ying Liu3, Bowen Zhou1\n1 JD AI Research\n2 University of California, Berkeley\n3 Renmin University of China\n{xusong28, lihaoran24, yuanpeng29}@jd.com\nAbstract\nExisting pre-trained language models (PLMs)\nhave demonstrated the effectiveness of self-\nsupervised learning for a broad range of nat-\nural language processing (NLP) tasks. How-\never, most of them are not explicitly aware\nof domain-speciﬁc knowledge, which is essen-\ntial for downstream tasks in many domains,\nsuch as tasks in e-commerce scenarios. In\nthis paper, we propose K-PLUG, a knowledge-\ninjected pre-trained language model based on\nthe encoder-decoder transformer that can be\ntransferred to both natural language under-\nstanding and generation tasks. We verify\nour method in a diverse range of e-commerce\nscenarios that require domain-speciﬁc knowl-\nedge. Speciﬁcally, we propose ﬁve knowledge-\naware self-supervised pre-training objectives\nto formulate the learning of domain-speciﬁc\nknowledge, including e-commerce domain-\nspeciﬁc knowledge-bases, aspects of product\nentities, categories of product entities, and\nunique selling propositions of product entities.\nK-PLUG achieves new state-of-the-art results\non a suite of domain-speciﬁc NLP tasks, in-\ncluding product knowledge base completion,\nabstractive product summarization, and multi-\nturn dialogue, signiﬁcantly outperforms base-\nlines across the board, which demonstrates\nthat the proposed method effectively learns a\ndiverse set of domain-speciﬁc knowledge for\nboth language understanding and generation\ntasks. Our code is available at https://\ngithub.com/xu-song/k-plug.\n1 Introduction\nPre-trained language models (PLMs), such as\nELMo (Peters et al., 2018), GPT (Radford et al.,\n2018), BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019), and XLNet (Yang et al., 2019), have\nmade remarkable breakthroughs in many natural\n∗Equal contribution.\nlanguage understanding (NLU) tasks, including\ntext classiﬁcation, reading comprehension, and nat-\nural language inference. These models are trained\non large-scale text corpora with self-supervision\nbased on either bi-directional or auto-regressive\npre-training. Equally promising performances\nhave been achieved in natural language genera-\ntion (NLG) tasks, such as machine translation and\ntext summarization, by MASS (Song et al., 2019),\nUniLM (Dong et al., 2019), BART (Lewis et al.,\n2020), T5 (Raffel et al., 2020), PEGASUS (Zhang\net al., 2020), and ProphetNet (Qi et al., 2020).\nIn contrast, these approaches adopt Transformer-\nbased sequence-to-sequence models to jointly pre-\ntrain for both the encoder and the decoder.\nWhile these PLMs can learn rich semantic pat-\nterns from raw text data and thereby enhance down-\nstream NLP applications, many of them do not\nexplicitly model domain-speciﬁc knowledge. As a\nresult, they may not be as sufﬁcient for capturing\nhuman-curated or domain-speciﬁc knowledge that\nis necessary for tasks in a certain domain, such as\ntasks in e-commerce scenarios. In order to over-\ncome this limitation, several recent studies have\nproposed to enrich PLMs with explicit knowledge,\nincluding knowledge base (KB) (Zhang et al., 2019;\nPeters et al., 2019; Xiong et al., 2020; Wang et al.,\n2019, 2020), lexical relation (Lauscher et al., 2019;\nWang et al., 2020), word sense (Levine et al., 2020),\npart-of-speech tag (Ke et al., 2020), and sentiment\npolarity (Ke et al., 2020; Tian et al., 2020). How-\never, these methods only integrate knowledge into\nthe encoder, and the decoding process in many\nNLG tasks beneﬁts little from these knowledge.\nTo mitigate this problem, we propose a\nKnowledge-injected Pre-trained Language model\nthat is suitable for both Natural Language\nUnderstanding and Generation ( K-PLUG). Dif-\nferent from existing knowledge-injected PLMs,\n2\nK-PLUG integrates knowledge into pre-training\nfor both the encoder and the decoder, and thus\nK-PLUG can be adopted to both downstream\nknowledge-driven NLU and NLG tasks. We verify\nthe performance of the proposed method in various\ne-commerce scenarios. In the proposed K-PLUG,\nwe formulate the learning of four types of domain-\nspeciﬁc knowledge: e-commerce domain-speciﬁc\nknowledge-bases, aspects of product entities, cate-\ngories of product entities, and unique selling propo-\nsitions (USPs) (Reeves, 1961) of product entities.\nSpeciﬁcally, e-commerce KB stores standardized\nproduct attribute information, product aspects are\nfeatures that play a crucial role in understanding\nproduct information, product categories are the\nbackbones for constructing taxonomies for orga-\nnization, and USPs are the essence of what differ-\nentiates a product from its competitors. K-PLUG\nlearns these types of knowledge into a uniﬁed PLM,\nenhancing performances for various language un-\nderstanding and generation tasks.\nTo effectively learn these four types of valu-\nable domain-speciﬁc knowledge in K-PLUG,\nwe proposed ﬁve new pre-training objectives:\nknowledge-aware masked language model\n(KMLM), knowledge-aware masked sequence-\nto-sequence (KMS2S), product entity aspect\nboundary detection (PEABD), product entity\ncategory classiﬁcation (PECC), and product\nentity aspect summary generation (PEASG).\nAmong these objectives, KMLM and KMS2S\nlearn to predict the masked single and multiple\ntokens, respectively, that are associated with\ndomain-speciﬁc knowledge rather than general\ninformation; PEABD determines the boundaries\nbetween descriptions of different product aspects\ngiven full product information; PECC identiﬁes\nthe product category that each product belongs\nto; and PEASG generates a summary for each\nindividual product aspect based on the entire\nproduct description.\nAfter pre-training K-PLUG, we ﬁne-tune it\non three domain-speciﬁc NLP tasks, namely, e-\ncommerce knowledge base completion, abstractive\nproduct summarization, and multi-turn dialogue.\nThe results show that K-PLUG signiﬁcantly outper-\nforms comparative models on all these tasks.\nOur main contributions are as follows:\n• We present K-PLUG that learns domain-\nspeciﬁc knowledge for both the encoder and\nthe decoder in a pre-training language model\nframework, which beneﬁts both NLG and\nNLU tasks.\n• We formulate the learning of four types\nof knowledge in e-commerce scenarios: e-\ncommerce knowledge-bases, aspects of prod-\nuct entities, categories of product entities,\nand unique selling propositions of product\nentities, which provide critical information\nfor many applications in the domain of e-\ncommerce. Speciﬁcally, ﬁve self-supervised\nobjectives are proposed to learn these four\ntypes of knowledge into a uniﬁed PLM.\n• Our proposed model exhibits clear effective-\nness in many downstream tasks in the e-\ncommerce scenario, including e-commerce\nKB completion, abstractive product summa-\nrization, and multi-turn dialogue.\n2 Related Work\n2.1 PLMs in General\nUnsupervised pre-training language model has\nbeen successfully applied to many NLP tasks.\nELMo (Peters et al., 2018) learns the contex-\ntual representations based on a bidirectional LM.\nGPT (Radford et al., 2018) predicts tokens based\non the context on the left-hand side. BERT (Devlin\net al., 2019) adopts a bi-directional LM to predict\nthe masked tokens. XLNet (Yang et al., 2019) pre-\ndicts masked tokens in a permuted order through\nan autoregressive method. MASS (Song et al.,\n2019) pre-trains the sequence-to-sequence LM to\nrecover a span of masked tokens. UniLM (Dong\net al., 2019) combines bidirectional, unidirectional,\nand sequence-to-sequence LMs. T5 (Raffel et al.,\n2020) and BART (Lewis et al., 2020) present de-\nnoising sequence-to-sequence pre-training. PE-\nGASUS (Zhang et al., 2020) pre-trains with gap-\nsentence generation objective. While human-\ncurated or domain-speciﬁc knowledge is essen-\ntial for downstream knowledge-driven tasks, these\nmethods do not explicitly consider external knowl-\nedge like our proposed K-PLUG.\n2.2 Injecting Knowledge into PLMs\nRecent work investigates how to incorporate knowl-\nedge into PLMs for NLU. ERNIE (Sun et al.,\n2019) enhances language representation with the\nentity/phrase-level masking. ERNIE (Zhang et al.,\n2019) identiﬁes and links entity mentions in texts\n3\nEncoderDecoder[CLS]water… losscoolingfast\ndrawercrisperwithwindowvent\n[BOS]\n[EOS]\nEncoder[CLS]water…\n[Refrigerator]\nlosscoolingfast\nEncoder[CLS]waterloss… coolingfast\n0100…\nDecoderEncoder[CLS]waterloss\ncooling\n…\nfast\n[MASK][MASK]\nEncoder[CLS]waterlosscooling…\nfast\n[MASK]\nCategory:RefrigeratorKnowledge-bases:\nAspect1:Summary:crisper drawer with ventwindowDescription:the intelligent multi-channelairoutlet helpsto reduce water loss…Aspect2:Summary: high quality condenserDescription: fast cooling effectand low consumptionwith invertercompressor, high quality copper aluminumcondenser…\n(b) Pre-training objective: KMLM (c) Pre-training objective: KMS2S (d) Pre-training objective: PEABD \n(e) Pre-training objective: PECC(f) Pre-training objective: PEASG(a) Pre-training data\nTypeofmotorinverterMaterialcopper aluminum\nFigure 1: Our pre-training data consists of 25 million textual product descriptions depicting multiple product\naspects. We deﬁne knowledge as e-commerce knowledge-bases, aspects of product entities , categories of prod-\nuct entities, and :::::::::::::::::::::::::::::::::::::unique selling propositions of product entities. Pre-training objectives include knowledge-aware\nmasked language model (KMLM), knowledge-aware masked sequence-to-sequence (KMS2S), product entity as-\npect boundary detection (PEABD), product entity category classiﬁcation (PECC), and product entity aspect sum-\nmary generation (PEASG).\nto their corresponding entities in KB. Similar to\nERNIE (Zhang et al., 2019), KnowBERT (Peters\net al., 2019) injects KBs into PLM. Xiong et al.\n(2020) leverages an entity replacement pre-training\nobjective to learn better representations for entities.\nKEPLER (Wang et al., 2019) adopts the knowledge\nembedding objective in the pre-training. Besides,\nSKEP (Tian et al., 2020), SenseBERT (Levine\net al., 2020), SentiLARE (Ke et al., 2020), and\nK-ADAPTER (Wang et al., 2020) propose to inte-\ngrate sentiment knowledge, word sense, sentiment\npolarity, and lexical relation into PLM, respectively.\nHowever, most of these studies are focused on in-\ntegrating knowledge for language understanding\ntask, work of utilizing domain-speciﬁc knowledge\nfor pre-training for language generation tasks are\nlimited. Inspired by these work, we construct K-\nPLUG that learns domain-speciﬁc knowledge into\na PLM for both NLU and NLG tasks.\n3 Knowledge-injected Pre-training\nIn this section, we explain the data used to pre-\ntrain K-PLUG, its model architecture, and our pre-\ntraining objectives.\n3.1 Data Preparation\nWe collect the pre-training data from a mainstream\nChinese e-commerce platform1, which contains ap-\nproximately 25 million textual product descriptions\nand covers 40 product categories. With an average\nlength of 405 tokens, these product descriptions\n1https://www.jd.com/\nconstitute a corpus with a size of 10B Chinese\ncharacters. Each product description consists of in-\nformation on 10.7 product aspects on average, and\neach product aspect is accompanied with a sum-\nmary highlighting its prominent features, as shown\nin Figure 1(a). Additionally, the e-commerce KB\nand USPs (further explained below) used in our pre-\ntraining data are as speciﬁed by the e-commerce\nplatform and its online stores.\n3.2 Model Architecture\nK-PLUG adopts the standard sequence-to-\nsequence Transformer architecture (Vaswani\net al., 2017), consisting of a 6-layer encoder\nand a 6-layer decoder as Song et al. (2019). We\nset the size of hidden vectors as 768, and the\nnumber of self-attention heads as 12. We adopt\nGELU activation (Hendrycks and Gimpel, 2016)\nas in GPT (Radford et al., 2018). We use Adam\noptimizer (Kingma and Ba, 2015) with a learning\nrate of 5e-4, β1 = 0.9, β2= 0.98, L2 weight decay\nof 0.01, learning rate warm-up over the ﬁrst\n10,000 steps and linear decay of the learning rate.\nThe dropout probability is 0.1. The maximum\nsequence length is set to 512 tokens. Pre-training\nwas performed with 4 Telsa V100 GPUs. The\npre-training is done within 10 epochs, which takes\naround 10 days, and the ﬁne-tuning takes up to 1\nday. We use the beam search with a beam size of 5\nfor inference for the NLG tasks.\n4\n3.3 Knowledge Formulation and Pre-training\nObjectives\nWe formulate the learning of four types of knowl-\nedge in a uniﬁed PLM: e-commerce KB, aspects\nof product entities, categories of product enti-\nties, and USPs of product entities. Speciﬁcally,\ne-commerce KB stores standardized product at-\ntribute information, e.g., (Material: Cotton) and\n(Collar Type: Pointed Collar). It provides details\nabout the products (Logan IV et al., 2017). As-\npects of product entities are features of a prod-\nuct, such as the sound quality of a stereo speaker,\netc. (Li et al., 2020a). Categories of product en-\ntities such as Clothing and Food are widely used\nby e-commerce platforms to organize their prod-\nucts so to present structured offerings to their cus-\ntomers (Luo et al., 2020; Dong et al., 2020) USPs\nof product entities are the essence of what differ-\nentiates a product from its competitors (Reeves,\n1961). For example, a stereo speaker’s USP ex-\nhibiting its supreme sound quality could be “crystal\nclear stereo sound”. An effective USP immediately\nmotivates the purchasing behavior of potential buy-\ners.\nWe propose and evaluate ﬁve novel self-\nsupervised pre-training objectives to learn the\nabove-mentioned four types of knowledge in the\nK-PLUG model (see Figure 1).\nKnowledge-aware Masked Language Model\n(KMLM)\nInspired by BERT (Devlin et al., 2019), we adopt\nthe masked language model (MLM) to train the\nTransformer encoder as one of our pre-training\nobjectives, which learns to predict the masked to-\nkens in the source sequence (e.g., “The company\nis [MASK] at the foot of a hill.”). Similar to\nBERT, we mask 15% of all tokens in a text se-\nquence; 80% of the masked tokens are replaced\nwith the [MASK] token, 10% with a random to-\nken, and 10% left unchanged. Particularly, given\nan original text sequence x = (x1,...,x m,...,x M )\nwith M tokens, a masked sequence is produced\nby masking xm through one of the three ways ex-\nplained above, e.g., replacing xm with [MASK] to\ncreate ˜x = (x1,..., [MASK],..., xM). MLM aims\nto model the conditional likelihood P(xm|˜x), and\nthe loss function is:\nLMLM = logP(xm|˜x) (1)\nThe major difference from BERT is that our\nKMLM prioritizes knowledge tokens, which con-\ntain knowledge regarding product attributes and\nUSPs, when selecting positions to mask and, in the\ncase that the knowledge tokens make up less than\n15% of all tokens, randomly picks non-knowledge\ntokens to complete the masking.\nKnowledge-aware Masked Sequence-to-\nSequence (KMS2S)\nK-PLUG inherits the strong ability of language\ngeneration from the masked sequence-to-sequence\n(MS2S) objective. The encoder takes a sentence\nwith a masked fragment (several consecutive to-\nkens) as the input, and the decoder predicts this\nmasked fragment conditioned on the encoder repre-\nsentations (e.g., “The company [MASK] [MASK]\n[MASK] the foot of a hill.”).\nGiven a text sequence x =\n(x1,...,x u,...,x v,...,x M ), a masked sequence\n˜x = ( x1,..., [MASK],..., [MASK],..., xM) is\nproduced by replacing the span xu:v, ranging from\nxu to xv, with the [MASK] token. MS2S aims to\nmodel P(xu:v|˜x), which can be further factorized\ninto a product P(xu:v|˜x) = ∏v\nt=u P(xt|˜x)\naccording to the chain rule. The loss function is:\nLMS 2S =\nv∑\nt=u\nlog P(xt|˜x) (2)\nWe set the length of the masked span as 30% of\nthe length of the original text sequence. Similar\nto KMLM, KMS2S prioritizes the masking of text\nspans that cover knowledge tokens.\nProduct Entity Aspect Boundary Detection\n(PEABD)\nA product description usually contains multiple\nproduct entity aspects. Existing work (Li et al.,\n2020a) proves that product aspects inﬂuence the\nquality of product summaries from the views of im-\nportance, non-redundancy, and readability, which\nare not directly taken into account in language mod-\neling. In order to train a model that understands\nproduct aspects, we leverage the PEABD objective\nto detect boundaries between the product entity\naspects. It is essentially a sequence labeling task\nbased on the representations of K-PLUG’s top en-\ncoder layer.\nGiven a text sequence x = (x1,...,x M ), the\nencoder of K-PLUG outputs a sequence h =\n(h1,...,h M ), which is fed into a softmax layer, and\ngenerates a probability sequence y. The loss func-\ntion is:\nLPEABD = −\n∑\nt\nˆyt log yt (3)\n5\nwhere y ∈{[0,1]}are the ground-truth labels for\nthe aspect boundary detection task.\nProduct Entity Category Classiﬁcation\n(PECC)\nProduct entity categories are the backbones for\nconstructing taxonomies (Luo et al., 2020; Dong\net al., 2020). Each product description document\ncorresponds to one of the 40 categories included in\nour corpus, such as Clothing, Bags, Home Appli-\nances, Shoes, Foods, etc. Identifying the product\nentity categories accurately is the prerequisite for\ncreating an output that is consistent with the input.\nGiven a text sequence x = (x1,...,x M ), a soft-\nmax layer outputs the classiﬁcation score, y, based\non the representation of the encoder classiﬁcation\ntoken, [CLS]. The loss function maximizes the\nmodel’s probability of outputting the true product\nentity category as follows:\nLPECC = −ˆylog y (4)\nwhere ˆyis the ground-truth product category.\nProduct Entity Aspect Summary Generation\n(PEASG)\nInspired by PEGASUS (Zhang et al., 2020),\nwhich proves that using a pre-training objective\nthat more closely resembles the downstream task\nleads to better and faster ﬁne-tuning performance,\nwe propose a PEASG objective to generate a sum-\nmary from the description of a product entity as-\npect. Unlike extracted gap-sentences generation\nin PEGASUS, our method constructs a more real-\nistic summary generation task because the aspect\nsummary naturally exists in our pre-training data.\nGiven an aspect description sequence x =\n(x1,...,x M ), and an aspect summary sequence\ny = (y1,...,y T ), PEASG aims to model the condi-\ntional likelihood P(y|x). The loss function is:\nLPEASG =\n∑\nt\nlog P(yt|x,y<t) (5)\nOverall, the pre-training loss is the sum of above-\nmentioned loss functions:\nLK−PLUG =LMLM + LMS 2S + LPEABD\n+ LPECC + LPEASG (6)\n4 Experiments and Results\n4.1 Pre-trained Model Variants\nTo evaluate the effectiveness of pre-training with\ndomain-speciﬁc data and with domain-speciﬁc\nknowledge separately, we implement pre-training\nexperiments with two model variants: C-PLUG\nand E-PLUG, whose conﬁgurations are the same\nas that of K-PLUG.\n• C-PLUG is a pre-trained language model\nwith the original objectives of MLM and\nMS2S, trained with a general pre-training cor-\npus, CLUE (Xu et al., 2020a), which contains\n30GB of raw text with around 8B Chinese\nwords.\n• E-PLUG is a pre-trained language model\nwith the original objectives of MLM and\nMS2S, trained with our collected e-commerce\ndomain-speciﬁc corpus.\n4.2 Downstream Tasks\nWe ﬁne-tune K-PLUG on three downstream tasks:\ne-commerce KB completion, abstractive product\nsummarization, and multi-turn dialogue. The e-\ncommerce KB completion task involves the predic-\ntion of product attributes and values given product\ninformation. The abstractive product summariza-\ntion task requires the model to generate a product\nsummary from textual product description. The\nmulti-turn dialogue task aims to output the response\nby utilizing a multi-turn dialogue context. The\ndomain-speciﬁc knowledge we deﬁned in this pa-\nper is essential for these tasks.\n4.2.1 E-commerce KB Completion\nTask Deﬁnition. E-commerce KB provides abun-\ndant product information that is in the form of\n(product entity, product attribute, attribute value),\nsuch as (pid#133443, Material, Copper Aluminum).\nFor the E-commerce KB completion task, the input\nis a textual product description for a given product,\nand the output is the product attribute values.\nDataset. We conduct experiments on the dataset\nof MEPA VE (Zhu et al., 2020b). This dataset is\ncollected from a major Chinese e-commerce plat-\nform, which consists of 87,194 instances annotated\nwith the position of attribute values mentioned in\nthe product descriptions. There are totally 26 types\nof product attributes such as Material, Collar Type,\nColor, etc. The training, validation, and testing sets\ncontain 71,194/8,000/8,000 instances, respectively.\nModel. We consider the e-commerce KB com-\npletion task as a sequence labeling task that tags\nthe input word sequence x = (x1,...,x N ) with\nthe label sequence y = (y1,...,y N ) in the BIO\nformat. For example, for the input sentence “ A\n6\nModel P R F1\nLSTM 79.68 86.43 82.92\nScalingUp 65.48 93.78 77.12\nBERT 78.27 88.62 83.12\nJA VE 80.27 89.82 84.78\nM-JA VE 83.49 90.94 87.17\nC-PLUG 89.79 96.47 93.02\nE-PLUG 89.91 96.75 93.20\nK-PLUG 93.58 97.92 95.97\nTable 1: Experimental results with the F1 score for the\ne-commerce KB completion task. The results in the\nﬁrst block are taken from Zhu et al. (2020b).\nbright yellow collar”, the corresponding labels for\n“bright” and “yellow” are Color-B and Color-I, re-\nspectively, and O for the other tokens. For an input\nsequence, K-PLUG outputs an encoding represen-\ntation sequence, and a linear classiﬁcation layer\nwith the softmax predicts the label for each input\ntoken based on the encoding representation.\nBaselines.\n• ScalingUp (Xu et al., 2019) adopts BiLSTM,\nCRF, and attention mechanism to extract at-\ntributes.\n• JA VE(Zhu et al., 2020b) is a joint attribute\nand value extraction model based on a pre-\ntrained BERT.\n• M-JA VE(Zhu et al., 2020b) is a multimodal\nJA VE model, which additionally utilizes prod-\nuct image information.\nResult. Table 1 shows the experimental re-\nsults. We observe that our K-PLUG performs better\nthan baselines. C-PLUG achieves signiﬁcantly bet-\nter performance than BERT, which indicates that\nMS2S can also beneﬁt the NLU task. E-PLUG\noutperforms C-PLUG, showing that training with\ndomain-speciﬁc corpus is helpful. K-PLUG fur-\nther exhibits a 2.51% improvement compared with\nE-PLUG. In short, we can conclude that the im-\nprovement is due to both the domain-speciﬁc pre-\ntraining data and knowledge-injected pre-training\nobjectives.\n4.2.2 Abstractive Product Summarization\nTask Deﬁnition. Abstractive product summariza-\ntion task aims to capture the most attractive in-\nformation of a product that resonates with poten-\ntial purchasers. Similar to the text summarization\ntask (Rush et al., 2015; Nallapati et al., 2016; Li\net al., 2018b; Zhang et al., 2018a; Li et al., 2020b;\nXu et al., 2020b), the input for this task is a textual\nproduct description, and the output is a condensed\nproduct summary.\nDataset. We perform experiments on the CEP-\nSUM dataset (Li et al., 2020a), which contains 1.4\nmillion instances collected from a major Chinese\ne-commerce platform, covering three categories of\nproduct: Home Appliances, Clothing, and Cases\n& Bags. Each instance in the dataset is a (prod-\nuct information, product summary) pair, and the\nproduct information contains an image, a title, and\nother product descriptions. In our work, we do\nnot consider the visual information of products.\nNotice that the task of abstractive product summa-\nrization and product entity aspect summary gener-\nation (PEASG) are partly different. The abstrac-\ntive product summarization task aims to generate\na complete and cohesive product summary given\na detailed product description. Given a product\naspect description, PEASG aims to produce an as-\npect summary that basically consists of condensed\nUSPs. In addition, for abstractive product sum-\nmarization task, the average length of the product\nsummaries is 79, while the lengths of the product\naspect summaries are less than 10 in general.\nModel. Abstractive product summarization task\nis an NLG task that takes the product description\nas the input and product summary as the output.\nBaselines.\n• LexRank (Erkan and Radev, 2004) is a graph-\nbased extractive summarization method.\n• Seq2seq (Bahdanau et al., 2015) is a standard\nseq2seq model with an attention mechanism.\n• Pointer-Generator (PG) (See et al., 2017) is\na seq2seq model with a copying mechanism.\n• Aspect MMPG (Li et al., 2020a) is the-state-\nof-the-art method for abstractive product sum-\nmarization, taking both textual and visual\nproduct information as the input.\nResult. Table 2 shows the experimental results,\nincluding ROUGE-1 (RG-1), ROUGE-2 (RG-2),\nand ROUGE-L (RG-L) F1 scores (Lin and Hovy,\n2003). K-PLUG clearly performs better than other\ntext-based methods. E-commerce knowledge plays\na signiﬁcant role in the abstractive product sum-\nmarization task, and domain-speciﬁc pre-training\ndata and knowledge-injected pre-training objec-\ntives both enhance the model. K-PLUG achieves\n7\nModel Home Applications Clothing Cases&Bags\nRG-1 RG-2 RG-L RG-1 RG-2 RG-L RG-1 RG-2 RG-L\nLexRank 24.06 10.01 18.19 26.87 9.01 17.76 27.09 9.87 18.03\nSeq2seq 21.57 7.18 17.61 23.05 6.84 16.82 23.18 6.94 17.29\nMASS 28.19 8.02 18.73 26.73 8.03 17.72 27.19 9.03 18.17\nPG 31.11 10.93 21.11 29.11 9.24 19.92 31.31 10.27 21.79\nAspect MMPG* 34.36 12.52 22.35 31.93 11.09 21.54 33.78 12.51 22.43\nC-PLUG 32.75 11.62 21.76 31.73 10.86 20.37 32.04 10.75 21.85\nE-PLUG 33.11 12.07 22.01 32.61 11.03 20.98 32.37 11.14 21.98\nK-PLUG 33.56 12.50 22.15 33.00 11.24 21.43 33.87 11.83 22.35\nTable 2: Experimental results with the ROUGE score for the abstractive product summarization task. The results\nin bold are the best performances among the models taking only texts as the input, and * denotes the model taking\nboth product images and texts as the input. The results in the ﬁrst and second blocks are taken from Li et al.\n(2020a).\nKB Aspect\nWin/Lose/Tie Kappa Win/Lose/Tie Kappa\n32.67/11.00/56.33 0.515 32.67/12.00/55.33 0.441\nCategory USPs\nWin/Lose/Tie Kappa Win/Lose/Tie Kappa\n25.33/7.00/67.67 0.612 28.67/9.33/62.00 0.428\nTable 3: Human evaluation results (%). “Win” denotes\nthat the generated summary of K-PLUG is better than\nE-PLUG.\ncomparable results with the multimodal model, As-\npect MMPG. The work of Li et al. (2020a) suggests\nthat product images are essential for this task, and\nwe will advance K-PLUG with multimodal infor-\nmation (Li et al., 2018a; Zhu et al., 2018, 2020a)\nin the future.\nHuman Evaluation. To help understand\nwhether the knowledge has been learned during\nthe pre-training, we conduct a knowledge-oriented\nhuman evaluation on 100 samples from the test set\nof CEPSUM dataset. Three experienced annotators\nare involved to determine whether K-PLUG out-\nperforms E-PLUG with respect to (1) KB: whether\nthe model provides details about product attributes,\n(2) Aspect: whether the model mentions distinc-\ntive product aspects, (3) Category: whether the\nmodel describes the correct product category, and\n(4) USPs: whether the model generate proper USPs.\nThe results are shown in Table 3. We can con-\nclude that K-PLUG can learn the knowledge better\nthan E-PLUG (p-value <0.01 for t-test). Kappa\nvalues (Fleiss, 1971) conﬁrm the consistency for\ndifferent annotators.\n4.2.3 Multi-Turn Dialogue\nTask Deﬁnition. The multi-turn dialogue task aims\nto output a response based on the multi-turn dia-\nlogue context (Shum et al., 2018). The input for\nthis task is the dialogue context consisting of pre-\nvious question answering, and the output is the\nresponse to the last question.\nDataset. We conduct experiments on two\ndatasets of JDDC (Chen et al., 2020) and\nECD (Zhang et al., 2018b). JDDC is collected\nfrom the conversations between users and customer\nservice staffs from a popular e-commerce website\nin China and contains 289 different intents, which\nare the goals of a dialogue, such as updating ad-\ndresses, inquiring prices, etc, from after-sales as-\nsistance. There are 1,024,196 multi-turn sessions\nand 20,451,337 utterances in total. The average\nnumber of turns for each session is 20, and the av-\nerage tokens per utterance is about 7.4. After pre-\nprocessing, the training, validation, and testing sets\ninclude 1,522,859/5,000/5,000 (dialogue context,\nresponse) pairs, respectively. ECD is collected\nfrom another popular e-commerce website in China\nand covers over 5 types of conversations based on\n20 commodities. Additionally, for each ground-\ntruth response, negative responses are provided for\ndiscriminative learning. The training, validation,\nand testing sets include 1,000,000/10,000/10,000\n(dialogue context, response) pairs, respectively.\nModel. We test with two types of K-PLUG:\nretrieval-based K-PLUG on the ECD dataset and\ngenerative-based K-PLUG on the JDDC dataset.\nFor the retrieval-based approach, we concatenate\nthe dialogue context and use [SEP] token to sepa-\nrate context and response. The [CLS] representa-\ntion is fed into the output layer for classiﬁcation.\n8\nModel RG-L BLEU\nBM25 19.47 9.94\nBERT 19.90 10.27\nSeq2Seq 22.17 14.15\nPG 23.62 14.27\nC-PLUG 25.47 16.75\nE-PLUG 25.93 17.12\nK-PLUG 26.60 17.80\nTable 4: Experimental results for the multi-turn conver-\nsation task on the JDDC dataset. The results in the ﬁrst\nblock are taken from Chen et al. (2020).\nThe generative-based approach is a sequence-to-\nsequence model, which is the same as the model\nadopted in the abstractive product summarization\ntask.\nBaselines. The baselines also include both\nthe retrieval-based (BM25, CNN, BiLSTM, and\nBERT) and generative-based approaches. Other\nbaselines are as follows.\n• SMN (Wu et al., 2017) matches a response\nwith each utterance in the context.\n• DUA (Zhang et al., 2018b) is a deep utterance\naggregation model based on the ﬁne-grained\ncontext representations.\n• DAM (Zhou et al., 2018) matches a response\nwith the context based using dependency in-\nformation based on self-attention and cross-\nattention.\n• IoI (Tao et al., 2019) is a deep matching model\nby stacking multiple interactions blocks be-\ntween utterance and response.\n• MSN (Yuan et al., 2019) selects relevant con-\ntext and generates better context representa-\ntions with the selected context.\nResult. Table 4 and 5 show the experimental\nresults on the JDDC and ECD datasets, respec-\ntively. We report ROUGE-L (RG-L) F1, BLEU,\nand recall at position k in n candidates (Rn@k).\nWe can observe that, both on the retrieval-based\nand generative-based tasks, K-PLUG achieves new\nstate-of-the-art results, and e-commerce knowledge\npresents consistent improvements. K-PLUG is evi-\ndently superior to BERT, possibly due to BERT’s\nlack of domain-speciﬁc knowledge for pre-training\nwith the general MLM objective.\nHuman Evaluation. We further perform a hu-\nman evaluation on the JDDC dataset. We randomly\nModel R10@1 R10@2 R10@5\nCNN 32.8 51.5 79.2\nBiLSTM 35.5 52.5 82.5\nSMN 45.3 65.4 88.6\nDUA 50.1 70.0 92.1\nDAM 52.6 72.7 93.3\nIoI-local 56.3 76.8 95.0\nMSN 60.6 77.0 93.7\nBERT 54.3 73.4 94.3\nC-PLUG 62.7 76.8 95.0\nE-PLUG 65.8 80.1 95.6\nK-PLUG 73.5 82.9 96.4\nTable 5: Experimental results for the multi-turn conver-\nsation task on the ECD dataset. The results in the ﬁrst\nblock are taken from Zhang et al. (2018b).\nRelevance Readability\nWin/Lose/Tie Kappa Win/Lose/Tie Kappa\n29.00/21.00/50.00 0.428 7.00/2.00/91.00 0.479\nTable 6: Human evaluation results (%). “Win” denotes\nthat the generated response of K-PLUG is better than\nE-PLUG.\nchoose 100 samples from the test set, and three\nannotators are involved to determine whether K-\nPLUG outperforms E-PLUG with respect to (1)\nrelevance between the response and the contexts\nand (2) readability of the response. The results are\nshown in Table 6. We can see that the percentage of\n“Win”, which denotes that the results of K-PLUG\nis better than E-PLUG, is signiﬁcantly larger than\n“Lose” (p-value<0.01 for t-test).\n4.3 Ablation Studies\nTo better understand our model, we perform abla-\ntion experiments to study the effects of different\npre-training objectives.\nResult. The ablation results are shown in Ta-\nble 7. We can conclude that the lack of any pre-\ntraining objective hurts performance across all the\ntasks. KMS2S is the most effective objective for\nthe abstractive product summarization and genera-\ntive conversation tasks since this objective is highly\nclose to the essence of NLG. Product-aspect-related\nobjectives, i.e., PEABD and PEASG, contribute\nmuch to the abstractive product summarization task,\nwhich proves that this task requires comprehen-\nsively understanding the product description from\nthe view of product aspects, going beyond individ-\nual tokens.\n9\nModel\nKB Abstractive Product Summarization Multi-Turn\nCompletion Home Applications Clothing Cases&Bags Conversation\nF1 RG-1 RG-2 RG-1 RG-2 RG-1 RG-2 RG-L BLEU\nK-PLUG 95.97 33.56 12.50 33.00 11.24 33.87 11.83 26.60 17.80\n-KMLM 95.88 33.52 12.43 32.87 11.20 33.75 11.70 26.43 17.62\n-KMS2S 95.76 33.13 12.14 32.12 10.97 33.74 11.43 25.82 16.97\n-PEABD 95.89 33.26 12.30 32.96 11.14 33.69 11.17 26.07 17.58\n-PECC 95.59 33.24 12.17 32.25 11.12 33.59 11.18 26.02 17.16\n-PEASG 95.48 33.39 12.36 32.57 11.16 33.78 11.45 26.12 17.38\nTable 7: Experimental results for ablation studies.\n5 Conclusion\nWe present a knowledge-injected pre-trained model\n(K-PLUG) that is a powerful domain-speciﬁc lan-\nguage model trained on a large-scale e-commerce\ncorpus designed to capture e-commerce knowl-\nedge, including e-commerce KB, product aspects,\nproduct categories, and USPs. The pre-training\nframework combines masked language model and\nmasked seq2seq with novel objectives formulated\nas product aspect boundary detection, product as-\npect summary generation, and product category\nclassiﬁcation tasks. Our proposed model demon-\nstrates strong performances on both natural lan-\nguage understanding and generation downstream\ntasks, including e-commerce KB completion, ab-\nstractive product summarization, and multi-turn\ndialogue.\nAcknowledgements\nThis work was supported by the National\nKey R&D Program of China under Grant No.\n2020AAA0108600.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, (ICLR).\nMeng Chen, Ruixue Liu, Lei Shen, Shaozu Yuan,\nJingyan Zhou, Youzheng Wu, Xiaodong He, and\nBowen Zhou. 2020. The JDDC corpus: A large-\nscale multi-turn Chinese dialogue dataset for E-\ncommerce customer service. In Proceedings of the\n12th Language Resources and Evaluation Confer-\nence, pages 459–466.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (NAACL), pages 4171–4186.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems 32: Annual Conference on\nNeural Information Processing Systems, (NeurIPS) ,\npages 13042–13054.\nXin Luna Dong, Xiang He, Andrey Kan, Xian Li, Yan\nLiang, Jun Ma, Yifan Ethan Xu, Chenwei Zhang,\nTong Zhao, Gabriel Blanco Saldana, et al. 2020.\nAutoKnow: Self-driving knowledge collection for\nproducts of thousands of types. In Proceedings of\nthe 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining (KDD), pages\n2724–2734.\nGünes Erkan and Dragomir R Radev. 2004. LexRank:\nGraph-based lexical centrality as salience in text\nsummarization. Journal of Artiﬁcial Intelligence Re-\nsearch (JAIR), 22:457–479.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin ,\n76(5):378.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nPei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, and Min-\nlie Huang. 2020. SentiLARE: Sentiment-aware lan-\nguage representation learning with linguistic knowl-\nedge. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 6975–6988.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\n(ICLR).\nAnne Lauscher, Ivan Vuli´c, Edoardo Maria Ponti, Anna\nKorhonen, and Goran Glavaš. 2019. Specializing\nunsupervised pretraining models for word-level se-\nmantic similarity. arXiv preprint arXiv:1909.02339.\n10\nYoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan\nPadnos, Or Sharir, Shai Shalev-Shwartz, Amnon\nShashua, and Yoav Shoham. 2020. SenseBERT:\nDriving some sense into BERT. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics (ACL), pages 4656–4667.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL), pages 7871–7880.\nHaoran Li, Peng Yuan, Song Xu, Youzheng Wu, Xi-\naodong He, and Bowen Zhou. 2020a. Aspect-aware\nmultimodal summarization for chinese e-commerce\nproducts. In Proceedings of the Thirty-Forth AAAI\nConference on Artiﬁcial Intelligence (AAAI) , pages\n8188–8195.\nHaoran Li, Junnan Zhu, Tianshang Liu, Jiajun Zhang,\nChengqing Zong, et al. 2018a. Multi-modal sen-\ntence summarization with modality attention and im-\nage ﬁltering. In Proceedings of the Twenty-Seventh\nInternational Joint Conference on Artiﬁcial Intelli-\ngence (IJCAI-18), pages 4152–4158.\nHaoran Li, Junnan Zhu, Jiajun Zhang, and Chengqing\nZong. 2018b. Ensure the correctness of the sum-\nmary: Incorporate entailment knowledge into ab-\nstractive sentence summarization. In Proceedings\nof the 27th International Conference on Computa-\ntional Linguistics, pages 1430–1441, Santa Fe, New\nMexico, USA. Association for Computational Lin-\nguistics.\nHaoran Li, Junnan Zhu, Jiajun Zhang, Chengqing\nZong, and Xiaodong He. 2020b. Keywords-guided\nabstractive sentence summarization. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence ,\npages 8196–8203.\nChin-Yew Lin and Eduard Hovy. 2003. Auto-\nmatic evaluation of summaries using n-gram co-\noccurrence statistics. In Proceedings of the 2003 Hu-\nman Language Technology Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics (NAACL), pages 150–157.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nRobert L Logan IV , Samuel Humeau, and Sameer\nSingh. 2017. Multimodal attribute extraction. arXiv\npreprint arXiv:1711.11118.\nXusheng Luo, Luxin Liu, Yonghua Yang, Le Bo, Yuan-\npeng Cao, Jinghang Wu, Qiang Li, Keping Yang,\nand Kenny Q Zhu. 2020. AliCoCo: Alibaba e-\ncommerce cognitive concept net. In Proceedings of\nthe 2020 ACM SIGMOD International Conference\non Management of Data (SIGMOD) , pages 313–\n327.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gulçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL), pages 2227–2237.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43–54.\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu,\nNan Duan, Jiusheng Chen, Ruofei Zhang, and Ming\nZhou. 2020. ProphetNet: Predicting future n-gram\nfor sequence-to-SequencePre-training. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 2401–2410.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research ,\n21:1–67.\nRosser Reeves. 1961. Reality in advertising. Knopf.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 379–389, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL), pages 1073–1083.\nHeung-Yeung Shum, Xiao-dong He, and Di Li. 2018.\nFrom Eliza to Xiaoice: challenges and opportunities\nwith social chatbots. Frontiers of Information Tech-\nnology & Electronic Engineering, 19:10–26.\n11\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: masked sequence to se-\nquence pre-training for language generation. In Pro-\nceedings of the 36th International Conference on\nMachine Learning (ICML), pages 5926–5936.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. ERNIE: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nChongyang Tao, Wei Wu, Can Xu, Wenpeng Hu,\nDongyan Zhao, and Rui Yan. 2019. One time of\ninteraction may not be enough: Go deep with an\ninteraction-over-interaction network for response se-\nlection in dialogues. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1–11.\nHao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He,\nHua Wu, Haifeng Wang, and Feng Wu. 2020. SKEP:\nSentiment knowledge enhanced pre-training for sen-\ntiment analysis. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL), pages 4067–4076.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems (NeurIPS) , pages\n5998–6008.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Cuihong Cao, Daxin Jiang, Ming\nZhou, et al. 2020. K-Adapter: Infusing knowl-\nedge into pre-trained models with adapters. arXiv\npreprint arXiv:2002.01808.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan\nLiu, Juanzi Li, and Jian Tang. 2019. KEPLER: A\nuniﬁed model for knowledge embedding and pre-\ntrained language representation. arXiv preprint\narXiv:1911.06136.\nYu Wu, Wei Wu, Chen Xing, Ming Zhou, and Zhou-\njun Li. 2017. Sequential matching network: A\nnew architecture for multi-turn response selection\nin retrieval-based chatbots. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL), pages 496–505.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In 8th International Conference on Learn-\ning Representations (ICLR).\nHuimin Xu, Wenting Wang, Xin Mao, Xinyu Jiang,\nand Man Lan. 2019. Scaling up open tagging from\ntens to thousands: Comprehension empowered at-\ntribute value extraction from product title. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL) , pages\n5214–5223.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie\nCao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu,\nCong Yu, Yin Tian, Qianqian Dong, Weitang Liu,\nBo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao\nWang, Weijian Xie, Yanting Li, Yina Patterson,\nZuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua\nLiu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui\nZhang, Zhengliang Yang, Kyle Richardson, and\nZhenzhong Lan. 2020a. CLUE: A Chinese language\nunderstanding evaluation benchmark. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 4762–4772.\nSong Xu, Haoran Li, Peng Yuan, Youzheng Wu, Xi-\naodong He, and Bowen Zhou. 2020b. Self-attention\nguided copy mechanism for abstractive summariza-\ntion. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 1355–1362, Online.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNET: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n(NeurIPS), pages 5753–5763.\nChunyuan Yuan, Wei Zhou, Mingming Li, Shangwen\nLv, Fuqing Zhu, Jizhong Han, and Songlin Hu. 2019.\nMulti-hop selector network for multi-turn response\nselection in retrieval-based chatbots. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 111–120.\nJiajun Zhang, Yang Zhao, Haoran Li, and Chengqing\nZong. 2018a. Attention with sparsity regularization\nfor neural machine translation and summarization.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, 27(3):507–518.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J Liu. 2020. PEGASUS: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn Proceedings of the 37th International Conference\non Machine Learning (ICML).\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), pages 1441–1451.\nZhuosheng Zhang, Jiangtong Li, Pengfei Zhu, Hai\nZhao, and Gongshen Liu. 2018b. Modeling multi-\nturn conversation with deep utterance aggregation.\nIn Proceedings of the 27th International Conference\non Computational Linguistics (COLING) , pages\n3740–3752.\nXiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying\nChen, Wayne Xin Zhao, Dianhai Yu, and Hua Wu.\n12\n2018. Multi-turn response selection for chatbots\nwith deep attention matching network. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1118–1127.\nJunnan Zhu, Haoran Li, Tianshang Liu, Yu Zhou, Ji-\najun Zhang, and Chengqing Zong. 2018. MSMO:\nMultimodal summarization with multimodal output.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 4154–4164, Brussels, Belgium. Association\nfor Computational Linguistics.\nJunnan Zhu, Yu Zhou, Jiajun Zhang, Haoran Li,\nChengqing Zong, and Changliang Li. 2020a. Multi-\nmodal summarization with guidance of multimodal\nreference. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, pages 9749–9756.\nTiangang Zhu, Yue Wang, Haoran Li, Youzheng Wu,\nXiaodong He, and Bowen Zhou. 2020b. Multimodal\njoint attribute prediction and value extraction for e-\ncommerce product. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nA Appendix\nA.1 Case studies\nWe present some examples from the test set of each\ntask, with comparisons of the ground-truth result\nand the outputs produced by the models of E-PLUG\nand K-PLUG.\n13\nGround-truth ECCO 防滑简约筒[短靴]靴筒高度\n(ECCO’s non-slip simple [ankle boots]shaft height)\nE-PLUG ECCO 防滑简约筒短靴鞋跟高度\n(ECCO’s non-slip simple [ankle boots]heel height)\nK-PLUG ECCO 防滑简约筒[短靴]靴筒高度\n(ECCO’s non-slip simple [ankle boots]shaft height)\nGround-truth a21 [运动风]风格[撞色]图案 风衣\n(A21’s [sports]style windbreaker jacket with [contrasting color]design style)\nE-PLUG a21 [运动风]风格 [撞色风]风格 衣\n(A21’s sports [windbreaker]style jacket with [contrasting color style]style\nK-PLUG a21 [运动风]风格[撞色]图案 风衣\n(A21’s sports [windbreaker]style jacket with [contrasting color]design style)\nGround-truth 配合[微弹]弹性的[棉质]材质 面料手感柔软顺滑\n(made from [low-strech]elasticity [cotton fabric]material for a silky smooth touch)\nE-PLUG 配合[微弹]裤型 的[棉质]材质 面料手感柔软顺滑\n(made from [low-strech]pants ﬁt [cotton fabric]material for a silky smooth touch)\nK-PLUG 配合[微弹]弹性的[棉质]材质 面料手感柔软顺滑\n(made from [low-strech]elasticity [cotton fabric]material for a silky smooth touch)\nTable 8: Case study for the e-commerce KB complete task. The product attribute and the corresponding attribute\nvalue is presented as [attribute value]product attribute. The K-PLUG model accurately complete the e-commerce KB,\nwhile the E-PLUG model sometimes fails. The translation texts are given below the original examples.\n14\nInput 帅康大吸力智能触控欧式抽油烟机，不锈钢材质，品质大于所见，多重贴心，310pa大\nDescription 风压，17立方大吸力，不碰头设计，53db静音，动力强，拢烟面积大，只留美味，不\n留烟味. . .\n(Sacon’s European style kitchen range hood. Extra powerful ventilation. Intelligent touch\ncontrol. Stainless steel. Superior quality with extra reliability. 310 Pa air pressure. 17 m3\nsuction. Protection against head bumping. Limited noise of 53 dB. Highly powered. Captures\nlarge volume of fumes. Leaves no cooking smells ...)\nGround-truth 想要清新的厨房环境，那就试试这款:::::大吸力油烟机吧！310pa大风压与17立方米的大吸\n力的完美搭配，能够瞬间吸附烹饪过程中产生的油烟，让您清新烹饪，不做“黄脸婆”！\n(This kitchen range hood with:::::::::::::::::::::extra suction and ventilation is ideal for a clean and fresh kitchen.\nPerfectly combining 310pa of air pressure and 17m3 of suction power, it ﬁlters out grease before\nyou realize it. Never have to worry again about keeping your loved ones from cooking fumes. )\nE-PLUG 这款帅康油烟机，采用优质的不锈钢材质，精工细作，打造出简约大气的外观，尽显出\n欧式的厨房魅力，具有强劲的:::::大吸力，快速吸走烹饪产生的油烟，智能的触控面板设计，\n使操作更加方便。\n(This kitchen range hood from Sacon is made of stainless steel with cutting edge technology. Its\ndesign adds contemporary style to your European style kitchen. It features :::::::::::::extra suction and\n::::::::ventilation for immediate ﬁltering of fumes and grease. Its intelligent touch control panel makes\nit extremely easy to use. )\nK-PLUG 这款不锈钢油烟机拥有17立方:::::大吸力，快速清除油烟，给您一个清新的厨房环境。310pa\n大风压，保持排烟畅通，防止油烟倒灌。低至53分贝的运行声音，让您告别厨房的嘈杂。\n(This stainless steel kitchen range hood features:::::::::::::::::::::extra suction and ventilation of 17 m3 for\nimmediate ﬁltering of grease and fumes, leaving your kitchen fresh. Its 310 pa air pressure\nensures clear ﬁltering without backﬂow. Its low working noise of 53 dB keeps your kitchen nice\nand quiet.)\nInput 康宝8升四季控温型，经济适用，家用厨房卫生间淋浴洗澡强排式燃气热水器，厨房洗菜，\nDescription 冬天洗漱再也不冻手，实打实的配置，高纯度无氧铜水箱，LED数字大屏，封闭式燃烧\n系统，防患于未然，控温清晰可见，多重安全防护，无氧铜水箱，让每次沐浴都是全新\n享受...\n(Canbo’s affordable 8L thermal control natural gas residential kitchen and bathroom water heater.\nProvides warm water for doing dishes and cleaning up in winter. High-quality accessories. High\nconcentration oxygen-free copper tank. LED digital screen. Closed boiling system for safety. Easy\ntemperature control. Multiple security checks. Oxygen-free copper tank. Lets you enjoy your\nshowers ... )\nGround-truth 采用高纯度无氧铜水箱，增大了加热面积，高效速热且温度持久稳定。:::::::::::::封闭式燃烧技术，\n提升了热效率，节能又高效。拥有控温功能，搭配::::::::::LED数字大屏，让你能够根据需求来\n随意调节水温。\n(High concentration oxygen-free tank with added heating area boils water quickly with stability.\n::::::::::::::::::Closed boiling technique increases efﬁciency and saves energy. Thermal control and:::::::::LED digital\n:::::screen for adjustable water temperature anytime. )\nE-PLUG 这款燃气热水器容量大，可以满足一年四季的用水需求。它采用::::::::::::封闭式燃烧技术，出热水的\n速度很快，沐浴的时候不用着急的等待，而且它在低压状态下也能正常运行，老小区用户\n也适用。\n(This gas water heater has a large volume and provides warm water year round. It adopts the:::::closed\n:::::::::::::boiling technique for faster warming up and shorter wait time in shower. It works in low pressure\ncondition and is suitable for older neighborhoods. )\nK-PLUG 高配::::::::::LED数字大屏，控温清晰可见，温度随时掌控于心。:::::::::::::封闭式燃烧技术，快速度燃烧，\n加热效率更高。高纯度无氧铜水箱，加热速度更快。\n(Equipped with :::::::::::::LED digital screen for clear thermal control and temperature adjustment. :::::Closed\n:::::::::::::boiling technique speeds up warming and increases efﬁciency. High concentration oxygen-free\ncopper tank for faster warming up.)\nTable 9: Case study for the abstractive product summarization task ( Home Applications category). The\nK-PLUG model generates summaries describing more information about e-commerce knowledge bases and\n:::::::::::::::::::::::::::::::::::::unique selling propositions of product entities.\n15\nInput 劲霸男士t恤，夏季新品，撞色时尚，舒适，花卉印花，短袖，黑色，商务休闲，圆领，\nDescription 夏季，时尚都市，短袖t恤，悬垂感和耐穿性好，时尚圆领，简约大气，多色可选，莫代尔\n棉针织面料，柔滑触感，清凉舒爽，花卉印花结合，珠片绣花，时尚大方...\n(K-Boxing’s men’s t-Shirt. This summer’s new arrival. Contrasting colors. Soft and comfortable.\nFlower print. Short-sleeve. Black. Business casual. Crewneck. Summer. Urban style. Short-sleeve\nt-shirt. Naturally draping with high durability. Stylish crewneck. Basic and causal. Multiple\ncolor chioces. Modal knit fabric. Soft and elastic. Loose ﬁt. Flower print. Glitter embroidery.\nStylish and casual ....)\nReference 选用莫代尔棉针织面料，:::::::手感柔滑，上身:::::::清凉舒爽，悬垂感好，耐穿性强。胸前撞色花卉\n印花结合::::::::珠片绣花，时尚又大方。经典圆领设计，自然突显:::::::型男风范。\n(Made from modal knit fabric. ::::::::::::::::::Incredibly soft and elastic. Features a ::::::::::::slightly looser ﬁt. Natural\ndraping with high durability. Decorated with ﬂower print and :::::::::::::glitter embroidery for a stylish and\ncasual look. Finished with a classy crewneck,::::::::::::::::::::::::::::::appealing to the modern sense of fashion)\nE-PLUG 圆领的领口更好的将颈部曲线所勾勒而出。水洗磨白的颜色平添了几分帅气与随性。让你\n和夏季的风一起随行。让休闲舒适的田园感油然而生。\n(The crewneck design helps in elongating the neck. The light wash color gives an attractive yet\neffortless look. It dresses you up in a country asethetic style, comfy and relaxed just like the summer\nbreeze. )\nK-PLUG 采用莫代尔棉针织面料，:::::::柔滑触感，:::::::清凉舒爽，花卉印花结合:::::::珠片绣花，时尚大方，简约\n圆领设计，轻松:::::::::::修饰颈部线条。\n(Made from modal knit fabric. :::::::::::::::::::Incredibly soft and stretchy. :::::::::::::Slightly looser ﬁt. Flower print and\n:::::::::::::glitter embroidery. Stylish and casual. The basic crewneck design easily helps in ::::::::::::::elongating the neck. )\nInput 吉普，羽绒服，男，中长款，90绒，冬季外套，新品，连帽，加绒，加厚，保暖羽绒外套，\nDescription 黑色，白鸭绒，时尚都市，型男都这么穿，融合艺术细节，创造76年传奇，潮流趋势必备，\n温暖新升级...\n(Jeep’s men’s down jacket. Mid-thigh length. Filled with 90% down. Winter jacket. This winter’s\nnew arrival. Hoodedd. The down ﬁll provides extra warmth. Warm down jacket. Black. White\nduck down. Urban style. Built for a perfect look. Designed with artistic details. Creating a legend\nfor 76 years. A must-have to keep up with current fashion trends. Keeps you warmer than ever ... )\nReference 采用聚酯纤维面料，::::::::手感柔软，轻盈且透气性较好，穿在身上干爽舒适。内部以白鸭绒进行\n填充，充绒量较高，::::::::柔软蓬松，更有:::::::加厚修身的版型设计，保暖效果较好，为您抵御户外严\n寒。\n(Made from polyester. ::::::Feel soft, lightweight, and breathable. Keeps you dry and comfortable.\nFilled primarily with white duck down, ::::::::::ﬂuffy and light. Features a :::::::::::thick yet slim-ﬁt design. Keeps you\nwarm in cold climates. )\nE-PLUG 这款羽绒服采用中长款的版型设计，修饰你的身材线条，而且还不乏:::::::优雅稳重气质。连帽的，\n加持增添青春学院风气息。衣上字母印花的点缀，俏皮又减龄。\n(This down jacket features a mid-thigh length, keeping a stylish silhouette and giving you an:::::elegant\n::::::::::::and mature look. The hood and letter print on the jacket make you look younger. )\nK-PLUG 采用聚酯纤维面料制成，:::::::手感柔软，:::::::亲肤透气。内部以白鸭绒填充，蓬松度高，:::::::轻盈温暖，\n更有连帽设计，可以锁住人体的热量，为您抵御户外寒冷天气，带来舒适的穿着体验。\n(Made from polyester. ::::::::::::::Soft and breathable. Filled primarily with white duck down, ﬂuffy and light.\n::::::::::::::::Lightweight but warm. Features a hooded design. Locks in the heat and keeps out the cold. Comfortable\nto wear. )\nTable 10: Case study for the abstractive product summarization task ( Clothing category). The K-\nPLUG model generates summaries describing more information about e-commerce knowledge bases and\n:::::::::::::::::::::::::::::::::::::unique selling propositions of product entities.\n16\nInput 菲安妮，秋季新款，斜跨包，女，印花磁扣，小方包，时尚单肩包，精致长款肩带，匀\nDescription 整车线工艺，高档ykk拉链，手感丝滑柔软且不脱色，优选进口水珠纹pvc，logo印花与\n包身融为一体，手感柔软舒适，防水耐磨，皮质肩带轻便减压，长度可调节，单肩/斜挎\n更舒心，平整均匀的车缝线技术\n(Fion. This fall’s new arrival. Corss body bag for women. Flower print magnetic snap closure.\nSquare-shaped. Stylish tote bag. Well made long bag straps. Finished with ﬂat lock stiching.\nZippers produced by YKK. Flat and smooth surface. Anti-fading coloring. Made from imported\nPVC coated fabric. Logo print ﬁts nicely. Soft and comfortable. Water-proof and durable.\nLightweight leather shoulder strap. Adjustable length. 2 ways to carry. Finished with ﬂat and even\nﬂatlock stiching.)\nReference 采用进口水珠纹pvc面料制作，:::::::::::手感柔软舒适，:::::::::::防水耐磨耐用。品牌定制拉链，平滑顺畅不\n卡链。长款肩带，:::::::::长度可调节，皮质:::::::轻便减压，单肩斜挎随心。\n(Made from imported PVC coated fabric.::::::::::::::::Soft and comfortable. :::::::::::::::::Water-proof and durable. Specially\nmade smoothly meshing zippers. Long shoulder strap with ::::::::::::adjustable length, made with leather for\n:::::::::::::::::::::::::light weight and reduced pressure. 2 ways to wear: cross body orwith the top handles. )\nE-PLUG 这款单肩包采用了优质的pvc材料制作，它表面具有细腻的纹理，而它的包身上还具有精美\n的印花装饰，整体看上去非常优雅，而其内部空间也很大，所以带着它外出非常的方便。\n(This tote bag is made friom imported high-qiuality PVC coated fabric. It has delicate texture and\nﬂower print decorations, giving it an elegant look. It can hold many items, making it ideal for every\nday use.)\nK-PLUG 优选进口水珠纹pvc，logo印花与包身融为一体，:::::::::::手感柔软舒适，:::::::防水耐磨，皮质肩带:::::轻便减\n::压，:::::::::长度可调节，单肩斜挎更舒心，平整均匀的车缝线技术，细节之处彰显品质。\n(Made from imported PVC coated fabric. Logo print ﬁts nicely. :::::::::::::::Soft and comfortable. :::::::::::Water-proof and:\n::::::durable. Leather shoulder strap for ::::::::::::::::::::::::light weight and reduced pressure with:::::::::::::adjustable length. 2 ways\nto wear: cross body or with the top handles. Flat and even ﬂatlocking ﬁnish shows its high quality. )\nInput 美旅铝框拉杆箱，男女行李箱，20英寸，登机箱，万向轮，艺术旅行，坚韧抗压，打破时空\nDescription 的边界，突破风格的局限，灵活流畅，双排滑轮，内外兼修干湿分离，防护升级，包角设计\n，三重防爆锁扣，美观更牢固，复古亦时尚，且具有良好的密封性\n(AmericanTourister’s aluminum frame suitcase. For men and women. 20 Inch. Carry-on luggage.\nSpinner wheels. Essential for the creative journey. Strong and durable. Breaks the limits of time\nand style. Stable and effortless steering thanks to the doubled wheel design. Includes dividers for easy\norganization of packed items. Extra stability with corner protection. Triple locks for added security.\nStylish design without compromising on durability. Classic yet fashionable. Closes and seals nicely. )\nReference 包角设计，显得更加简洁利落，而且又能有效[防撞耐摔]。铝合金框架结构，坚实牢固，具有\n:::::::::::良好的密封性。:::::::::::三重防爆锁扣更加稳固，让你出门安心无忧。内外间修，:::::::干湿分离，合理收纳\n更加有序。\n(Conrer protection gives and basic and simple look with :::::::::::::::::::::added stability and durability. Aluminum alloy\nframe makes it strong and:::::::::::::::close and seal nicely. :::::::::Triple locks for added stability and security. Includes\n::::::dividers for easy and organized packing.)\nE-PLUG 这款拉杆箱选用干净的黑色调，适合多种场合携带。精选材质，具有细腻的纹理质感，:::::::经久耐用。\n内部大空间处理，可以放置更多物品。\n(This suitcase in black is suitable for various occasions. It is made from high-quality materials with\ndelicate texture and [increased durability]. This large suitcase is built to provide room for extra items.)\nK-PLUG 采用铝合金框架结构，坚韧抗压，具有:::::::::::良好的密封性，配备:::::::::三重防爆锁扣，美观更牢固，搭载内\n置式:::::::八轮设计，灵活流畅，搭配::::::::::::tsa海关密码锁，有效保护您的物品安全。\n(Made with aluminum alloy frame for added stability.:::::::::::::::::Closes and seals nicely. ::::::::Triple-lock design to\nprovide extra security. Stable and effortless steering thanks to the 8-wheel design. ::::::::::::::TAS-approved locks\nprotect your items and cause no damage during security check.)\nTable 11: Case study for the abstractive product summarization task ( Bags&Cases category). The K-\nPLUG model generates summaries describing more information about e-commerce knowledge bases and\n:::::::::::::::::::::::::::::::::::::unique selling propositions of product entities.\n17\nQ1 是纸皮核桃吗\n(Is it paper-peel walnut?)\nA1 是的哦\n(Yes, it is.)\nQ2 发的是特级吗\n(Is it of premium quality?)\nA2 什么意思\n(What do you mean?)\nQ3 核桃不是有大小吗我要送人去买10斤人在吗\n(Aren’t these walnuts sized differently? I’m getting 5 kilos. Are you still here?)\nE-PLUG 这是纸皮核桃手捏就开哦很好剥的呢\n(This is paper-peel walnut, which you can peel easily with bare hands.)\nK-PLUG 一般直径在3-5cm\n(They mostly have a diameter between 3-5 cm.)\nQ1 这两个有什么区别\n(What’s the difference between these two?)\nA1 您好上面的纯棉的尺寸1520\n(Hi, the cotton one above has a size of 1520.)\nQ2 这三种就是大小不一样啊\n(These three only differ in size?)\nE-PLUG 是的哦亲以上是3种干巾的区别哦用途都一样呢\n(Sure. That’s exactly the difference between these three wiping products. They are made\nfor the same purpose.)\nK-PLUG 中间的无纺布的尺寸1820 下面的天然纤维尺寸2020 加大加厚款\n(The middle one made of nonwoven fabric has a size of 1820, and the bottom one made\nof natural ﬁber, with a size of 2020, is extra large and thick.)\nQ1 这款茶刀包邮吗。\n(Do you offer free shipping for this tea knife?)\nA1 包邮的哦。\n(Yes we do.)\nQ2 多少钱啊\n(How much is it?)\nE-PLUG 主要是我们30 箱之下就是这个价格了呢\n(It’s because we have this set price for all orders below 30 boxes.)\nK-PLUG 单把茶刀48哦\n(It’s 48 yuan for a single tea knife.)\nTable 12: Case study for the multi-turn dialogue task on the ECD dataset. The K-PLUG model produces more\naccurate responses for the questions related to e-commerce knowledge bases.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8397101163864136
    },
    {
      "name": "Automatic summarization",
      "score": 0.6437878012657166
    },
    {
      "name": "Domain knowledge",
      "score": 0.6115186214447021
    },
    {
      "name": "Natural language processing",
      "score": 0.5735783576965332
    },
    {
      "name": "Natural language understanding",
      "score": 0.5681495666503906
    },
    {
      "name": "Knowledge base",
      "score": 0.5418269634246826
    },
    {
      "name": "Transformer",
      "score": 0.5250822305679321
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5113098621368408
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5056233406066895
    },
    {
      "name": "Natural language",
      "score": 0.4864813983440399
    },
    {
      "name": "Language model",
      "score": 0.43028831481933594
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4112422466278076
    },
    {
      "name": "Programming language",
      "score": 0.0926789939403534
    },
    {
      "name": "Engineering",
      "score": 0.06663116812705994
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}