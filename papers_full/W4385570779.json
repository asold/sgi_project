{
    "title": "MUSTIE: Multimodal Structural Transformer for Web Information Extraction",
    "url": "https://openalex.org/W4385570779",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2127284166",
            "name": "Qifan Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2124326937",
            "name": "Jingang Wang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2099425862",
            "name": "Xiaojun Quan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2323056039",
            "name": "Fuli Feng",
            "affiliations": [
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2607121186",
            "name": "Zenglin Xu",
            "affiliations": [
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2561852407",
            "name": "Shaoliang Nie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2166097142",
            "name": "Sinong Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A296516693",
            "name": "Madian Khabsa",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2971547511",
            "name": "Hamed Firooz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109573734",
            "name": "Dong-fang Liu",
            "affiliations": [
                "Rochester Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3102205863",
        "https://openalex.org/W4221167659",
        "https://openalex.org/W2922714365",
        "https://openalex.org/W3035131649",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2118090838",
        "https://openalex.org/W2944898795",
        "https://openalex.org/W2080132606",
        "https://openalex.org/W2024091454",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W3121976951",
        "https://openalex.org/W4224919569",
        "https://openalex.org/W2955858358",
        "https://openalex.org/W3105238007",
        "https://openalex.org/W2400661088",
        "https://openalex.org/W4286902133",
        "https://openalex.org/W3101427524",
        "https://openalex.org/W4288024261",
        "https://openalex.org/W3105188155",
        "https://openalex.org/W2962982640",
        "https://openalex.org/W2604259521",
        "https://openalex.org/W3035275890",
        "https://openalex.org/W2517903345",
        "https://openalex.org/W1976022204",
        "https://openalex.org/W3099461227",
        "https://openalex.org/W3169766753",
        "https://openalex.org/W3119466332",
        "https://openalex.org/W3173306993",
        "https://openalex.org/W3034864438",
        "https://openalex.org/W2898700358",
        "https://openalex.org/W4285241172",
        "https://openalex.org/W2108745425",
        "https://openalex.org/W3176664887",
        "https://openalex.org/W2161861392",
        "https://openalex.org/W3102567691",
        "https://openalex.org/W2962785754",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W1795600436",
        "https://openalex.org/W2970467549",
        "https://openalex.org/W2134150392",
        "https://openalex.org/W3037361235",
        "https://openalex.org/W3034300118",
        "https://openalex.org/W2146266087",
        "https://openalex.org/W2891117443",
        "https://openalex.org/W2951865668",
        "https://openalex.org/W4385574309",
        "https://openalex.org/W3035140194",
        "https://openalex.org/W2982150889",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3176851559",
        "https://openalex.org/W2765619745",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W3085139254",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3104953317",
        "https://openalex.org/W2154444297",
        "https://openalex.org/W4307593450",
        "https://openalex.org/W2890166583",
        "https://openalex.org/W3205981739"
    ],
    "abstract": "Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Zenglin Xu, Shaoliang Nie, Sinong Wang, Madian Khabsa, Hamed Firooz, Dongfang Liu. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2405–2420\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nMUSTIE: Multimodal Structural Transformer for\nWeb Information Extraction\nQifan Wang1, Jingang Wang2∗, Xiaojun Quan3, Fuli Feng4, Zenglin Xu5,\nShaoliang Nie1, Sinong Wang1, Madian Khabsa1, Hamed Firooz1 and Dongfang Liu6*\n1Meta AI 2Meituan Lab 3Sun Yat-sen University\n4University of Science and Technology of China\n5Peng Cheng Lab 6Rochester Institute of Technology\nwqfcr@fb.com\nAbstract\nThe task of web information extraction is to ex-\ntract target fields of an object from web pages,\nsuch as extracting the name, genre and actor\nfrom a movie page. Recent sequential model-\ning approaches have achieved state-of-the-art\nresults on web information extraction. How-\never, most of these methods only focus on ex-\ntracting information from textual sources while\nignoring the rich information from other modal-\nities such as image and web layout. In this\nwork, we propose a novel MUltimodal Struc-\ntural Transformer (MUST) that incorporates\nmultiple modalities for web information ex-\ntraction. Concretely, we develop a structural\nencoder that jointly encodes the multimodal\ninformation based on the HTML structure of\nthe web layout, where high-level DOM nodes,\nlow-level text, and image tokens are introduced\nto represent the entire page. Structural atten-\ntion patterns are designed to learn effective\ncross-modal embeddings for all DOM nodes\nand low-level tokens. An extensive set of ex-\nperiments has been conducted on WebSRC and\nCommon Crawl benchmarks. Experimental re-\nsults demonstrate the superior performance of\nMUST over several state-of-the-art baselines.\n1 Introduction\nThe world wide web has grown explosively in the\npast decades, with millions of new web pages be-\ning created everyday. Web pages and documents\nhave been widely used and become a powerful re-\nsource for humans to obtain information. For exam-\nple, Figure 1 shows a movie page from the IMDB\nwebsite, which contains structured movie infor-\nmation including movie name, description, genre,\netc. This information is essential to facilitate new\nexperiences in applications like web search and\nretrieval (Crescenzi and Mecca, 2004; Yan et al.,\n2009). There has been an enduring demand for au-\ntomatic information extraction from unstructured\n∗Corresponding authors.\nFigure 1: An example of a movie page from the IMDB\nwebsite. The extractions of movie name, description,\ngenre, duration, director, actor and release date are high-\nlighted with colored bounding boxes on the web page.\nor semi-structured web pages to create structured\nknowledge bases (Chang et al., 2006; Hao et al.,\n2011). Therefore, it is an important research prob-\nlem to extract structured information from web\npages (Carlson and Schafer, 2008).\nWeb information extraction (Manabe and Tajima,\n2015; Wu et al., 2018) poses a lot of challenges to\nresearchers in both academia and industry, due to\nthe unstructured nature and the diverse layout pat-\nterns of the web documents (Xiong et al., 2019;\nLockard et al., 2019). Moreover, web data often\ncontains multiple modalities such as texts, tables,\nand images. A substantial amount of research\n(Katti et al., 2018; Zhang et al., 2021) has been\nproposed for automatic web information extraction,\nincluding early works of template-based extrac-\ntion (Dalvi et al., 2011). However, these methods\nclearly do not scale up to billions of websites. Deep\nlearning models (Gogar et al., 2016; Zhou et al.,\n2021) attempt to use supervisions from markup\npages (Tempelmeier et al., 2018) to build different\nextractors for different fields.\nWith the recent development of natural language\nprocessing (Vaswani et al., 2017), language models\nhave been successfully applied to web informa-\n2405\ntion extraction. These methods first convert the\nweb document to a text sequence by concatenat-\ning all the text nodes (Gupta et al., 2020) or to a\nconnected graph by using the rendered page (Qian\net al., 2019), and then adopt sequential modeling\nsuch as LSTM (Lin et al., 2020) or attention net-\nworks (Hwang et al., 2021) to extract the target\nfields from the web. More recently, several mul-\ntimodal language models (Dong et al., 2020; Xu\net al., 2020) have been proposed to extract web\ninformation from both textual and visual signals.\nDespite achieving promising results on web infor-\nmation extraction, there are several major limita-\ntions for existing natural language models. First,\nthey encode each modality of the web document in-\ndependently with an individual encoder, which fails\nto capture the connections among different modali-\nties, resulting in a less effective web representation.\nSecond, they do not fully encode the semi-structure\nHTML layout, which carries important knowledge\nabout the correlations between different fields. For\nexample, in Figure 1, the DOM nodes correspond-\ning to the movie ‘name’ usually appear directly\nafter the image node in the HTML, while the ‘re-\nlease date’ and ‘duration’ nodes are often siblings.\nTherefore, encoding the structural HTML would\nbenefit the information extraction. Third, the texts\nand images from individual modalities are simply\nconcatenated, making existing Transformer models\nincapable of handling large web documents.\nTo address these challenges, in this work, we\npropose a novel MUltimodal Structural Trans-\nformer (namely MUST), which incorporates multi-\nple modalities for web information extraction. In\nparticular, we design a multimodal encoder with a\nstructural attention mechanism to jointly encode all\nthe DOM nodes from multiple modalities, and learn\nthe cross-modal embeddings for them. Intuitively,\nMUST leverages the web layout structure that nat-\nurally connects DOM nodes from all modalities for\nmore effective attention weight computation. The\ninformation of the target fields is then extracted\nfrom the learned node embeddings. We conduct\nevaluations of our model on WebSRC and Com-\nmon Crawl benchmarks, and show the superior\nperformance of MUST over several state-of-the-art\nmethods. The experimental results also demon-\nstrate the effectiveness of the structural attention\nin modeling web documents with multimodal data.\nThe main contributions are summarized as follows:\n• We propose a unified Multimodal Structural\nTransformer for web information extraction,\nwhich effectively models the multimodal data\nwith the HTML layout and jointly extracts the\ninformation for the target fields.\n• We design a structural attention mechanism to\ncapture the correlation among different modal-\nities of the web document for learning effec-\ntive cross-modal embeddings.\n• We conduct an extensive set of experiments\non two benchmarks and demonstrate the ef-\nfectiveness of the proposed approach.\n2 Related Work\nWeb Information Extraction Early works in\nweb information extraction are wrapper induction\nmethods (Kim and Shim, 2011; Lockard et al.,\n2018), which construct templates by learning the\ndesired patterns from the web documents. Several\ndeep learning methods (Sleiman and Corchuelo,\n2013; Wang et al., 2019) are proposed to extract\nor classify a text node to a set of fields using its\ntextual and visual features, e.g., classify whether a\ntext node is the ‘name’ field.\nWith the recent advancement in natural language\nprocessing (NLP) (Devlin et al., 2019), an increas-\ning number of language models (Appalaraju et al.,\n2021; Wang et al., 2020a; Yang et al., 2022; Zhao\net al., 2022) have been developed for web infor-\nmation extraction. These methods can be further\ndivided into three main groups. The first group con-\ntains the sequential modeling approaches (Herzig\net al., 2020; Majumder et al., 2020), which con-\nstruct a text sequence by concatenating all the text\nnodes from the web and performing the extraction.\nForm2Seq (Aggarwal et al., 2020) designs a seq-to-\nseq model with an RNN. WebFormer (Wang et al.,\n2022a) merges all the text nodes from the HTML\nand trains a model with hierarchical attention. The\nsecond group includes the graph learning models\n(Qian et al., 2019; Lockard et al., 2020), which treat\nthe web document as a graph connecting multiple\nrendered components and directly learn the web\nrepresentation on the graph. FormNet (Lee et al.,\n2022) generates a structure-aware graph from the\nrendered web document and uses the graph con-\nvolutional network (GCN) for obtaining the node\nembeddings. The third group consists of the multi-\nmodal methods (Gong et al., 2017; Liu et al., 2019;\nWang et al., 2020b; Li et al., 2021), which learn\nto extract field information from both textual and\n2406\nFigure 2: Overview of MUST model. The embedding layer generates the embeddings for all the input DOM nodes,\ntexts and images. The MUST encoder constructs structural attention to jointly encode the entire web and capture the\ninformation among different modalities. The extraction layer outputs the final extractions of the text field.\nvisual clues on the web. LayoutLMv2 (Xu et al.,\n2021) adopts a two-stream multimodal Transformer\nencoder to model the interaction among text and\nimage.\nStructure and Efficient Transformers Our\nwork is also related to those Transformer models\n(Tay et al., 2022; Rae et al., 2020; Wang et al.,\n2022b) that focus on efficiently encoding structure\nand large sequences. ETC (Ainslie et al., 2020)\nand Longformer (Beltagy et al., 2020) describe a\nmethod to use a global memory with a relative\nattention pattern (Shaw et al., 2018, 2019) to rep-\nresent the structure text input. Transformer XL\n(Dai et al., 2019) develops an approach to encode\nlong text sequences beyond a fixed size. HIBERT\n(Zhang et al., 2019) uses hierarchical attention on\nthe equally divided input blocks. Random sparse\nattention is utilized in BigBird (Zaheer et al., 2020)\nto reduce the quadratic computations to linear time.\nThese methods achieve promising results in deal-\ning with structure and large input. However, they\ncannot be directly applied to encode HTML layout\nwith multiple modalities.\n3 Multimodal Structural Transformer\n3.1 Problem Setting\nIn this section, we formally define the problem of\nweb information extraction. A web document can\nbe essentially represented as a HTML DOM tree\nH. It usually contains information from multiple\nmodalities, such as texts and images, which are\nnaturally the leaf nodes in the DOM tree (see Figure\n2). In order to encode the target field, we create\na special DOM node ‘Field’ under the root of the\nDOM tree, with a leaf node representing the text\nfield attached to it. Similarly, for ‘<img>’ DOM\nnodes, we apply Optical Character Recognition\n(OCR) to obtain the texts from the image and add\nthese OCR nodes under the image node. We denote\nthe leaf nodes as C= (C1,C2,...,C n), where Ci\nrepresents the i-thleaf node in the DOM tree. For\neach leaf node, it is either a text sequence or an\nimage, i.e., Ci = (wi\n1,...,w i\nni), where wi\nj is the\nj-thword or image token in Ci.\nThe goal of web information extraction is that\ngiven a target field T, extract its corresponding\ninformation from the web document. For exam-\nple, for the text field ‘Director’, we aim to obtain\n‘Steven Spielberg’. And for the target field ‘Name’,\n‘Jurassic Park’ would be the correct extraction.\n3.2 Overview\nThe overall model architecture of MUST is shown\nin Figure 2, which consists of three key compo-\nnents, the embedding layer, the MUST encoder\nand the extraction layer. The embedding layer ini-\ntializes the embeddings of both the text and image\ntokens (referred to as TI tokens in the rest of the\npaper), as well as the DOM nodes. The MUST en-\ncoder jointly encodes the multimodal information\n2407\nfrom the DOM tree with structural attention pat-\nterns to capture the correlations among DOM nodes\nand text/image tokens. The extraction layer extracts\nthe answer from the embedding of the ‘Field’ with\na Transformer decoder.\nThere are several advantages to our modeling.\n(1) The multimodal information on the web is\njointly encoded through a unified structural en-\ncoder, where the information from different modal-\nities effectively communicates with each other. (2)\nWe directly encode the HTML DOM tree instead\nof sequentializing the document (Chen et al., 2021;\nWang et al., 2022a) which does not fully capture the\nstructure information, or generating a graph from\nthe web (Qian et al., 2019; Lee et al., 2022) which\nrequires careful design of the nodes and edges. (3)\nOur model does not concatenate all the inputs, al-\nlowing it to scale to large documents.\n3.3 Embedding Layer\nExisting multimodal approaches (Xiong et al.,\n2019; Li et al., 2021) encode textual and visual fea-\ntures separately with individual encoders. Different\nfrom previous works, we jointly encode texts and\nimages together with the DOM tree from the web\ndocument in a multimodal structural Transformer.\nIn the embedding layer, we initialize the embed-\ndings for all DOM nodes and TI tokens with a d-\ndimensional vector. The embedding of each DOM\nnode can be viewed as a summarization of the sub-\ntree under it. For example, in Figure 2, the DOM\nnode ‘<head>’ represents the whole web document\nand can be used for document-level classification.\nThe ‘<img>’ DOM node essentially contains all the\ninformation about that image. For a DOM node, its\nembedding is constructed by adding a node embed-\nding, a type embedding and a tag embedding. For a\nTI token, it is constructed by a word/patch embed-\nding and a type embedding. The word embedding\n(Zou et al., 2013) is widely used in language mod-\nels. The patch embedding is obtained by a linear\nprojection of the visual feature from ResNet101\n(He et al., 2016). The type embedding is used to\nindicate the type of the token, i.e., DOM node, text\nor image. The tag embedding represents the HTML\ntag of the DOM node such as ‘<div>’ and ‘<img>’.\nAll these embeddings are trainable.\n3.4 MUST Encoder\nThe MUST encoder contains a stack of Lidentical\nlayers, which connects the DOM nodes, texts and\nimages from multiple modalities with a structural\nattention mechanism, and learns cross-modal con-\ntextual representations of the web document and\nfield. In each encoder layer, there are four different\nattention patterns. First, structural attention among\nDOM nodes, which transfers the knowledge across\nthe DOM tree. Second, bottom up attention from\ntext/image token to DOM node. Third, top down\nattention that passes the information from DOM\nnodes to the text/image token. Fourth, local atten-\ntion that learns contextual embeddings from other\nTI tokens in the same leaf node.\nDOM-to-DOM Attention The DOM-to-DOM\nattention is designed to propagate the information\nfrom one DOM node to another, which essentially\ncalculates the attention weights among the DOM\nnodes. We utilize the connections in the DOM\ntree H to compute the DOM-to-DOM attention,\ni.e., we allow each DOM node to attend to a set\nof DOM nodes in the DOM tree, including itself,\nits parent, children and siblings. For instance, the\nDOM node ‘<img>’ will attend to (besides itself)\nthe parent node ‘<div>’, the children ‘<alt>’ and\ntwo ‘<OCR>’ nodes, and the sibling node ‘<div>’.\nFormally, given the DOM nodes embedding XD,\nthe DOM-to-DOM attention is defined as:\neNNij = xDi WNN\nQ (xDj WNN\nK +tNNij )T/√d\nαNN\nij =\nexp(eNN\nij )∑\nℓ∈S(xD\ni ) exp(eNN\niℓ ), forxj ∈S(xD\ni )\nwhere S(xD\ni ) denotes the set of DOM nodes that\nxD\ni can attend to. WNN\nQ and WNN\nK are learnable\nweight matrices, and tNN\nij are learnable vectors\nrepresenting the connection type between the two\nnodes, i.e. self, parent, child or sibling. dis the\nembedding dimension.\nBottom-Up Attention There are several choices\nfor designing the Bottom-Up attention. For ex-\nample, allowing full attention from TI tokens to\na DOM node. However, the computation grows\nlinearly with the total number of the TI tokens,\nwhich is costly for large web documents. There-\nfore, in the Bottom-Up attention, we only enable\nattention from TI tokens to the DOM node they\nbelong to. Note that for Bottom-Up attention, only\nleaf nodes are involved. For instance, in Figure 2,\nthe ‘<h1>’ DOM node only directly receives infor-\nmation from the text tokens within it, i.e., ‘Jurassic’\nand ‘Park’. The information contained in other TI\ntokens will be propagated to the ‘<h1>’ DOM node\n2408\nthrough DOM-to-DOM attention. Denote the TI to-\nken embeddings as XTI , the restricted Bottom-Up\nattention for a leaf node Ci is defined as:\neBUij = xDi WBU\nQ (xTIj WBU\nK )T/√d\nαBU\nij =\nexp(eBU\nij )∑\nℓ∈Ci exp(eBU\niℓ ), forj∈Ci\nwhere WBU\nQ and WBU\nK are weight matrices in\nBottom-Up attention.\nTop-Down Attention In Top-Down attention,\neach TI token directly connects with every DOM\nnode, absorbing the high-level representation from\nthese DOM nodes. For example in Figure 2, the\ntext token ‘Jurassic’ from leaf node ‘<h1>’ attends\nto all DOM nodes in the DOM tree. The defini-\ntion of the Top-Down attention is similar to the\nabove Bottom-Up attention except that each TI to-\nken attends to all DOM nodes. Full details are in\nAppendix A.\nLocal Attention The local attention is the tradi-\ntional attention mechanism used in various existing\nTransformer models (Devlin et al., 2019; Dosovit-\nskiy et al., 2021), which learns contextual token\nembeddings from the input sequence. Again, in\nour design, we only restrict local attention between\ntwo TI tokens from the same leaf DOM node to\nfurther reduce the computational cost.\nThe final representation of the DOM nodes and\nTI tokens can be achieved by merging the above\nstructural attention patterns. The output embed-\ndings for DOM nodes and TI tokens ZD,ZTI are\ncalculated as follows:\nzDi = ∑\nj∈S(xD\ni ) αDDij xDj WD\nV +∑\nℓ∈Ci αBU\niℓ xTI\nℓ WTI\nV\nzTIi = ∑\nℓ∈Ci αLA\niℓ xTI\nℓ WTI\nV +∑\nj αTDij xDj WD\nV\nwhere all the attention weights αij are described\nabove. WD\nV and WTI\nV are the learnable matrices to\ncompute the values for DOM nodes and TI tokens\nrespectively. Intuitively, these structure attention\npatterns effectively connect the DOM nodes and\nTI tokens on the web from different modalities,\nenabling efficient interactions across the DOM tree.\n3.5 Extraction Layer\nThe extraction layer of MUST outputs the final an-\nswer for the target field from the web document.\nWe use a Transformer decoder (Vaswani et al.,\n2017) on the output embeddings of the DOM node\n‘Field’ to generate the extraction word by word:\n¯wt = arg max\nwt\n(softmax(WdeXt\nde))\nwhere Xt\nde is the decoder output at word position\nt. Wde is the output matrix which projects the final\nembedding to the logits of vocabulary size. A copy\nmechanism (Zhao et al., 2018) is employed into the\ndecoder to allow both copying words from the text\nnodes, and generating words from a predefined vo-\ncabulary during decoding. To further improve the\nembedding learning, we supplement two auxiliary\ntasks as shown in Figure 2. (1) extracting the text\nspans from the text nodes via sequential tagging\n(Xu et al., 2019; Chen et al., 2021). (2) classifying\nthe web document using the embedding from the\n‘<head>’ node. The total loss is defined as:\nL= LD + αLSeq + βLCls\nwhere α and β are hyper-parameters to balance\namong different losses.\n4 Experiments\n4.1 Datasets\nWe evaluate our method on two multimodal bench-\nmarks, WebSRC (Chen et al., 2021) and Common\nCrawl (Wang et al., 2022a; Li et al., 2022).\nWebSRC1 is designed for structural reading com-\nprehension and information extraction on the web.\nIt contains 6.5K web pages with their HTML\nsources and images from 10 domains, e.g. “Jobs”,\n“Books”, “Autos”, etc. We use the KV-type pages in\nour experiment, resulting in a subset of 3214 pages\nwith 71 unique fields. These pages are all single\nobject pages containing multiple key-value pairs,\ne.g. (“genre”, “Science Fiction”). The keys are\nused as the fields, while the values are the answers\nto be extracted from the web page.\nCommon Crawl2 is commonly used in various\nweb information extraction tasks. It contains more\nthan 3 billion web pages from various domains,\nand we choose three domains Movies, Events and\nProducts in the experiments. We further select\nweb pages with schema.org annotations 3, which\ncontain the full markup information about the ob-\nject and are used as the ground-truth labels. The\n1https://x-lance.github.io/WebSRC/\n2https://commoncrawl.org/the-data/\n3https://schema.org/\n2409\nModels WebSRC Common Crawl\nMovies Events Products\nEM F1 EM F1 EM F1 EM F1\nGraphIE (Qian et al., 2019)66.34±0.27 73.15±0.2281.85±0.21 86.01±0.1979.11±0.16 83.86±0.1773.94±0.24 77.62±0.19\nFreeDOM (Lin et al., 2020)68.24±0.35 74.64±0.2981.64±0.35 86.28±0.1779.52±0.29 84.98±0.1674.83±0.31 78.29±0.22\nSimpDOM (Zhou et al., 2021)70.18±0.24 76.35±0.1482.87±0.25 87.66±0.1281.47±0.26 86.05±0.1475.21±0.23 78.40±0.25\nV-PLM (Chen et al., 2021)73.25±0.23 76.20±0.2183.04±0.25 88.53±0.1482.29±0.15 87.34±0.1677.18±0.13 81.05±0.24\nWebFormer (Wang et al., 2022a)73.57±0.18 80.04±0.3185.81±0.1190.75±0.2685.36±0.26 90.41±0.1380.24±0.1783.85±0.21\nMarkupLM (Li et al., 2022)74.43±0.2380.52±0.2285.33±0.15 89.84±0.1685.93±0.3091.12±0.2578.67±0.29 82.28±0.26\nMUST 75.68±0.18 81.13±0.2487.79±0.24 92.34±0.1887.67±0.20 93.37±0.2382.30±0.19 85.41±0.24\nTable 1: Performance comparison results with standard deviation on all datasets. Results are statistically significant\nwith p-value <0.001.\nfields are {“Name”, “Description”, “Genre”, “Du-\nration”, “Director”, “Actor”, “Published Date”}\nfor Movies, {“Name”, “Description”, “Date”, “Lo-\ncation”} for Events and {“Name”, “Description”,\n“Brand”, “Price”, “Color”} for Product pages. We\ndownsample the web pages by allowing at most 2k\npages per website to balance the data. More details\nare provided in Appendix B.\n4.2 Baselines\nOur model is compared with six state-of-the-art\nweb information extraction methods.\nGraphIE (Qian et al., 2019) propagates infor-\nmation between connected nodes through graph\nconvolutions.\nFreeDOM (Lin et al., 2020) proposes a two-\nstage neural network to extract the information\nfrom text nodes.\nSimpDOM (Zhou et al., 2021) treats the problem\nas a DOM node tagging task and uses a LSTM to\njointly encode XPath with the text features.\nV-PLM (Chen et al., 2021) models the HTML,\ntext and visual signal together by concatenating\ntheir embeddings with individual encoders.\nWebFormer (Wang et al., 2022a) concatenates\nthe HTML and the text sequence and builds a se-\nquential tagging model.\nMarkupLM (Li et al., 2022) designs a multi-\nmodal pre-training model with text, layout, and\nimage, and fine-tunes it for information extraction.\n4.3 Settings\nWe implement MUST using Tensorflow and trained\non a 32 core TPU v3 configuration. During train-\ning, we use the gradient descent algorithm with\nAdam optimizer. During inference, we conduct\nbeam search with beam width 6. The details of\nall hyper-parameters are reported in Appendix C.\nFollowing previous works (Li et al., 2022), we use\nExact Match (EM) and F1 as the evaluation metrics.\nWe repeat each experiment 10 times and report the\nmetrics based on the average over these runs.\n5 Results\n5.1 Main Results\nMUST outperforms the state-of-the-art web in-\nformation extraction methods on all datasets.\nWe report the performance comparison result on all\ndatasets in Table 1. It is not surprising to see that\nthe node-level extraction methods FreeDOM and\nGraphIE do not perform well, as they only extract\nthe text from each text node independently or with\nlocal information based on the text features. Simp-\nDOM uses a LSTM to jointly encode the XPath in-\nformation with the text feature, and thus boosts the\nperformance. V-PLM, WebFormer and MarkupLM\nachieve even stronger results compared to these\nmethods due to the explicit modeling of the HTML.\nNevertheless, it can be seen that MUST achieves\nthe best performance over all the compared meth-\nods on all datasets. For example, the EM score\nof MUST increases over 2.57% and 4.61% com-\npared with WebFormer and MarkupLM on Prod-\nucts. The reason is that these sequential modeling\nand multimodal methods separately encode HTML,\ntext and image with individual encoders, and con-\ncatenate them into a single sequence for learning\ntheir embedding. In contrast, MUST jointly en-\ncodes the multimodal information from the web in\na structural manner, which effectively transfers the\nknowledge among different modalities, leading to\nbetter cross-modal embeddings. We also report a\nfield level results of MUST on the Products data\nin Table 2. We can see that MUST achieves higher\nperformance on ‘Name’ and ‘Brand’ compared to\nthe fields ‘Price’ and ‘Description’. More detailed\nanalysis is provided in Appendix ??.\n2410\nName Desc Brand Price Color\nEM 87.34 79.57 86.36 77.15 82.68\nF1 92.27 83.78 88.72 79.37 84.46\nTable 2: Field level results of MUST on Products.\nModels WebSRC Common CrawlMoviesEventsProducts\nGraphIE (Qian et al., 2019)62.29 74.37 73.21 63.64FreeDOM (Lin et al., 2020)63.54 74.68 74.72 64.34SimpDOM (Zhou et al., 2021)63.98 75.54 75.37 64.46V-PLM (Chen et al., 2021)67.46 80.37 80.14 72.57WebFormer (Wang et al., 2022a)70.58 82.35 82.59 74.68MarkupLM (Li et al., 2022)71.73 84.36 84.92 78.16\nMUST 73.42 84.81 85.31 77.87\nTable 3: Low-resource performance comparison results\n(F1 scores) on all datasets.\n5.2 Results on Low-resource Scenario\nMUST performs reasonably well in low-\nresource scenarios. We further evaluate the per-\nformance of MUST and all other baselines in a\nlow-resource setting. Specifically, we randomly\nsample 20% and 10% training data from WebSRC\nand Common Crawl respectively and retrain the\nmodels. The F1 scores are reported in Table 3.\nThere are several observation from these results.\nFirst, it is clear that all methods suffer from large\nperformance drop. However, the performance gap\nbetween the low-resource and full-resource sce-\nnarios is relatively small for those methods that\nencode the HTML information, e.g., V-PLM, Web-\nFormer, MarkupLM and MUST. Our hypothesis is\nthat in the low-resource training, the HTML lay-\nout provides additional knowledge beyond the text\nfor information extraction, which is particularly\nimportance under low-resource settings. Second,\nMUST still outperforms the baselines in most cases.\nWe also observe that MarkupLM achieves even\nstronger result than MUST on Products. We be-\nlieve this is due to their large pretraining on web\ndocuments, which learns certain common knowl-\nedge in the HTML.\n6 Analysis and Discussion\n6.1 Importance of Different Modalities\nHTML layout plays an important role for web\ninformation extraction, while OCR texts and vi-\nsual information from the web images are also\nvaluable sources that boost the extraction per-\nformance. To understand the impact of different\nmodalities from the web document, i.e., HTML\nlayout, OCR texts and visual signals, we conduct\nan ablation study by removing each modality from\nF1\n70\n75\n80\n85\n90\n95\nWebSRC Movies Events Products\nw/o OCR text w/o visual w/o HTML All\nFigure 3: Importance of different modalities.\nF1\n70\n75\n80\n85\n90\n95\nName Description Brand Price Color\nw/o OCR text w/o visual w/o HTML All\nFigure 4: Field level importance of different modalities.\nour model. Concretely, removing HTML layout\nmeans we do not leverage the DOM tree in MUST,\nbut just concatenate the text and image tokens from\nall leaf nodes. Removing OCR texts or visual sig-\nnals means delete the corresponding DOM nodes\nin the DOM tree during encoding. The results of F1\nscores on all datasets are illustrated in Figure 3. It is\nclear that HTML layout plays a crucial role for the\ninformation extraction task on all datasets, which\nis consistent with our expectation. Moreover, both\nthe OCR text and visual information help improve\nthe extraction performances.\n6.2 Field Level Importance of Different\nModalities\nEach modality has different impacts on differ-\nent fields. While the visual signal is very useful\nfor ‘Color’ extraction, OCR text benefits the ex-\ntraction of both ‘Price’ and ‘Brand’. To further\nanalyze the impact of different modalities on differ-\nent fields, we conduct another field level ablation\nstudy on the Products data. The experimental set-\ntings are the same as in the above experiment, and\nwe remove each modality at a time. The results of\nfield level F1 scores are shown in Figure 4. We ob-\nserve that HTML layout still plays an essential role\nacross all fields. It can be seen from the results that\nthe visual signal does not help too much on ‘Name’\nand ‘Description’ extraction, but clearly improves\nthe performance on ‘Color’ extraction. The reason\nis that many product images carry the information\nabout the product color, and therefore can be use-\nful when extracting the product ‘Color’. We also\n2411\nF1\n70\n75\n80\n85\n90\n95\nWebSRC Movies Events Products\nw/o Node-to-Node w/o Bottom-Up w/o Top-Down All\nFigure 5: Importance of different attention patterns.\nobserve that the OCR text boosts the extraction of\n‘Brand’, as it is often the case that product ‘Brand’\nis mentioned in the product image. We provide\nmore case studies in Appendix ??.\n6.3 Impact of Different Attention Patterns\nEvery attention pattern has a positive impact on\nthe model performance, while MUST with all\nstructural attention patterns achieves the best\nperformance. In this ablation study, we evalu-\nate the impact of different attention patterns on the\nmodel performance by eliminating each attention at\na time. Concretely, we train three additional mod-\nels without the three attentions respectively, i.e.,\nDOM-to-DOM, Bottom-UP and Top-Down atten-\ntion. Note that we always keep the Local attention\nas it is the fundamental component of Transformer\nmodels. The F1 scores of these three models to-\ngether with the original MUST on all datasets are\nshown in Figure 5. First, we observe clear model\nperformance drop without the Bottom-Up attention\non all datasets. This is because the Bottom-Up\nattention is used to transfer knowledge from leaf\nnodes (containing text and image information) to\nDOM nodes, which is important for learning effec-\ntive contextual embeddings for DOM nodes. We\nalso observe some performance drop, around 1 to 2\npercent in terms of F1 score, when eliminating one\nof the other two attention patterns. This observation\nvalidates that the structural attention mechanism\nis crucial for modeling the multimodal web docu-\nments and extracting the information from them.\nNevertheless, it is clear that MUST with all atten-\ntion patterns achieves the best performance.\n6.4 Performance-Scale Trade-off\nMUST with a 12-layer encoder and a 4-layer\ndecoder achieves good performance-scale trade-\noff. We conduct a performance-scale study on\ndifferent MUST configurations. In particular, the\nMUST-base model uses a 12-layer encoder with\nMUST # Parameters WebSRC Movies Events Products\nEncoder-2L 46M 78.59 89.92 91.46 83.32\nEncoder-6L 88M 79.88 90.73 92.25 84.10\nEncoder-12L 152M 81.13 92.34 93.37 85.41\nEncoder-24L 269M 82.38 93.46 94.87 87.09\nDecoder-2L 131M 80.25 91.68 92.43 84.78\nDecoder-4L 152M 81.13 92.34 93.37 85.41\nDecoder-12L 235M 81.26 92.41 93.70 85.83\nTable 4: Model performance (F1) over different encoder\nand decoder configurations.\na 4-layer decoder. We evaluate the model perfor-\nmance with a different number of encoder layers\nin {2L, 6L, 12L, 24L}, and decoder layers in {2L,\n4L, 12L}. The F1 scores of different models are\nreported in Table 4. It is not surprising to see that\nEncoder-24L and Decoder-12L obtain the best per-\nformances, which is expected. On the other hand,\nlarger models usually require both longer training\nand inference time. Our MUST model with a 12-\nlayer encoder and a 4-layer decoder performs rea-\nsonably well on all datasets, which achieves good\nperformance-scale trade-off.\nMovies\n91.0\n91.5\n92.0\n92.5\n0 2 4 6 8 10\nα β\nFigure 6: Impact of multi-task learning.\n6.5 Impact of Multi-task Learning\nBoth text span extraction and web document\nclassification help improve the model perfor-\nmance. To understand the impact of the auxiliary\ntasks, we evaluate the model performance by vary-\ning the hyper-parameters α and β from {0, 0.1,\n0.5, 0.8, 2, 10}. Note that we modify one hyper-\nparameter by fixing the other one to the optimal\nvalue (see Appendix C). The model performances\nwith different hyper-parameter values are shown in\nFigure 6. It is clear that both tasks lift the model\nperformance (0 value of αor β means removing\nthat task). However, the text span extraction task\nplays a more important role compared to the web\nclassification task.\n7 Conclusions\nThis paper presents a novel Multimodal Structural\nTransformer (MUST) for web information extrac-\ntion. A structural encoder is developed and used to\n2412\njointly encode the multimodal information associ-\nated with the HTML layout, where high-level DOM\nnodes, and low-level text and image tokens are in-\ntroduced to represent the entire web. Structural\nattention patterns are designed to learn effective\ncross-modal embeddings for all DOM nodes and\ntext/image tokens. Experimental results on Web-\nSRC and Common Crawl benchmarks demonstrate\nthe effectiveness of the proposed approach.\nLimitations\nThere are two limitations of the current MUST\nmodel. First, although pre-trained language mod-\nels can potentially boost the performance in web\ninformation extraction, pre-train a MUST on web\ndocuments has its unique challenges. There are\nseveral possibilities for our future exploration. For\nexample, we plan to pretrain a MUST model by in-\ncorporating HTML-specific tasks, such as masking\nDOM nodes and predicting the relations between\nDOM nodes. Second, our model focuses on web\npages with single-object, where each target field\nonly has exactly one answer. For a multi-object\npage, e.g. a movie listing page, there are different\nmovie names corresponding to different movies on\nthe page. However, methods like repeated patterns\n(Adelfio and Samet, 2013) can be applied.\nReferences\nMarco D. Adelfio and Hanan Samet. 2013. Schema\nextraction for tabular data on the web. Proc. VLDB\nEndow., 6(6):421–432.\nMilan Aggarwal, Hiresh Gupta, Mausoom Sarkar, and\nBalaji Krishnamurthy. 2020. Form2seq : A frame-\nwork for higher-order form structure extraction. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 3830–\n3840. Association for Computational Linguistics.\nJoshua Ainslie, Santiago Ontañón, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: encoding long and structured inputs in\ntransformers. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020,\npages 268–284. Association for Computational Lin-\nguistics.\nSrikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota,\nYusheng Xie, and R. Manmatha. 2021. Docformer:\nEnd-to-end transformer for document understanding.\nIn 2021 IEEE/CVF International Conference on Com-\nputer Vision, ICCV 2021, Montreal, QC, Canada,\nOctober 10-17, 2021, pages 973–983. IEEE.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. CoRR,\nabs/2004.05150.\nAndrew Carlson and Charles Schafer. 2008. Bootstrap-\nping information extraction from semi-structured\nweb pages. In Machine Learning and Knowl-\nedge Discovery in Databases, European Conference,\nECML/PKDD 2008, Antwerp, Belgium, September\n15-19, 2008, Proceedings, Part I , volume 5211 of\nLecture Notes in Computer Science, pages 195–210.\nSpringer.\nChia-Hui Chang, Mohammed Kayed, Moheb R. Girgis,\nand Khaled F. Shaalan. 2006. A survey of web infor-\nmation extraction systems. IEEE Trans. Knowl. Data\nEng., 18(10):1411–1428.\nXingyu Chen, Zihan Zhao, Lu Chen, Jiabao Ji, Danyang\nZhang, Ao Luo, Yuxuan Xiong, and Kai Yu. 2021.\nWebsrc: A dataset for web-based structural reading\ncomprehension. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 7-11 November, 2021 , pages\n4173–4185. Association for Computational Linguis-\ntics.\nValter Crescenzi and Giansalvatore Mecca. 2004. Auto-\nmatic information extraction from large websites. J.\nACM, 51(5):731–779.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a fixed-length context. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n2978–2988. Association for Computational Linguis-\ntics.\nNilesh N. Dalvi, Ravi Kumar, and Mohamed A. Soli-\nman. 2011. Automatic wrappers for large scale web\nextraction. Proc. VLDB Endow., 4(4):219–230.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nXin Luna Dong, Hannaneh Hajishirzi, Colin Lockard,\nand Prashant Shiralkar. 2020. Multi-modal informa-\ntion extraction from text, semi-structured, and tabular\ndata on the web. In Proceedings of the 58th Annual\n2413\nMeeting of the Association for Computational Lin-\nguistics: Tutorial Abstracts, ACL 2020, Online, July\n5, 2020, pages 23–26. Association for Computational\nLinguistics.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nTomas Gogar, Ondrej Hubácek, and Jan Sedivý. 2016.\nDeep neural networks for web page information ex-\ntraction. In Artificial Intelligence Applications and\nInnovations - 12th IFIP WG 12.5 International Con-\nference and Workshops, AIAI 2016, Thessaloniki,\nGreece, September 16-18, 2016, Proceedings , vol-\nume 475 of IFIP Advances in Information and Com-\nmunication Technology, pages 154–163. Springer.\nDihong Gong, Daisy Zhe Wang, and Yang Peng. 2017.\nMultimodal learning for web information extraction.\nIn Proceedings of the 2017 ACM on Multimedia Con-\nference, MM 2017, Mountain View, CA, USA, Octo-\nber 23-27, 2017, pages 288–296. ACM.\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek\nSrikumar. 2020. INFOTABS: inference on tables as\nsemi-structured data. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, ACL 2020, Online, July 5-10, 2020,\npages 2309–2324. Association for Computational\nLinguistics.\nQiang Hao, Rui Cai, Yanwei Pang, and Lei Zhang. 2011.\nFrom one tree to a forest: a unified solution for struc-\ntured web data extraction. In Proceeding of the 34th\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval, SIGIR\n2011, Beijing, China, July 25-29, 2011, pages 775–\n784. ACM.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2016, Las Vegas,\nNV , USA, June 27-30, 2016, pages 770–778. IEEE\nComputer Society.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Martin Eisen-\nschlos. 2020. Tapas: Weakly supervised table parsing\nvia pre-training. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2020, Online, July 5-10, 2020, pages\n4320–4333. Association for Computational Linguis-\ntics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, So-\nhee Yang, and Minjoon Seo. 2021. Spatial depen-\ndency parsing for semi-structured document informa-\ntion extraction. In Findings of the Association for\nComputational Linguistics: ACL/IJCNLP 2021, On-\nline Event, August 1-6, 2021, volume ACL/IJCNLP\n2021 of Findings of ACL, pages 330–343. Associa-\ntion for Computational Linguistics.\nAnoop R. Katti, Christian Reisswig, Cordula Guder,\nSebastian Brarda, Steffen Bickel, Johannes Höhne,\nand Jean Baptiste Faddoul. 2018. Chargrid: Towards\nunderstanding 2d documents. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October 31\n- November 4, 2018, pages 4459–4469. Association\nfor Computational Linguistics.\nChulyun Kim and Kyuseok Shim. 2011. TEXT: auto-\nmatic template extraction from heterogeneous web\npages. IEEE Trans. Knowl. Data Eng., 23(4):612–\n626.\nChen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vin-\ncent Perot, Guolong Su, Nan Hua, Joshua Ainslie,\nRenshen Wang, Yasuhisa Fujii, and Tomas Pfister.\n2022. Formnet: Structural encoding beyond sequen-\ntial modeling in form document information extrac-\ntion. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 3735–3754. Association for\nComputational Linguistics.\nJunlong Li, Yiheng Xu, Lei Cui, and Furu Wei. 2022.\nMarkuplm: Pre-training of text and markup language\nfor visually rich document understanding. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 6078–6087. Association for Computa-\ntional Linguistics.\nYulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin,\nChengquan Zhang, Yan Liu, Kun Yao, Junyu Han,\nJingtuo Liu, and Errui Ding. 2021. Structext: Struc-\ntured text understanding with multi-modal transform-\ners. In MM ’21: ACM Multimedia Conference, Vir-\ntual Event, China, October 20 - 24, 2021 , pages\n1912–1920. ACM.\nBill Yuchen Lin, Ying Sheng, Nguyen V o, and Sandeep\nTata. 2020. Freedom: A transferable neural archi-\ntecture for structured information extraction on web\ndocuments. In KDD ’20: The 26th ACM SIGKDD\nConference on Knowledge Discovery and Data Min-\ning, Virtual Event, CA, USA, August 23-27, 2020 ,\npages 1092–1102. ACM.\nXiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha\nZhao. 2019. Graph convolution for multimodal in-\nformation extraction from visually rich documents.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-\n7, 2019, Volume 2 (Industry Papers), pages 32–39.\nAssociation for Computational Linguistics.\n2414\nColin Lockard, Xin Luna Dong, Prashant Shiralkar, and\nArash Einolghozati. 2018. CERES: distantly super-\nvised relation extraction from the semi-structured\nweb. Proc. VLDB Endow., 11(10):1084–1096.\nColin Lockard, Prashant Shiralkar, and Xin Luna Dong.\n2019. Openceres: When open information extraction\nmeets the semi-structured web. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), pages 3047–3056. Associa-\ntion for Computational Linguistics.\nColin Lockard, Prashant Shiralkar, Xin Luna Dong, and\nHannaneh Hajishirzi. 2020. Zeroshotceres: Zero-\nshot relation extraction from semi-structured web-\npages. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020 , pages 8105–8117.\nAssociation for Computational Linguistics.\nBodhisattwa Prasad Majumder, Navneet Potti, Sandeep\nTata, James Bradley Wendt, Qi Zhao, and Marc Na-\njork. 2020. Representation learning for information\nextraction from form-like documents. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020, pages 6495–6504. Association for\nComputational Linguistics.\nTomohiro Manabe and Keishi Tajima. 2015. Extracting\nlogical hierarchical structure of HTML documents\nbased on headings. Proc. VLDB Endow., 8(12):1606–\n1617.\nYujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, and\nRegina Barzilay. 2019. Graphie: A graph-based\nframework for information extraction. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019,\nVolume 1 (Long and Short Papers), pages 751–761.\nAssociation for Computational Linguistics.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,\nChloe Hillier, and Timothy P. Lillicrap. 2020. Com-\npressive transformers for long-range sequence mod-\nelling. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nPeter Shaw, Philip Massey, Angelica Chen, Francesco\nPiccinno, and Yasemin Altun. 2019. Generating log-\nical forms from graph representations of text and\nentities. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 95–106. Association for\nComputational Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT, New Orleans, Louisiana, USA, June\n1-6, 2018, Volume 2 (Short Papers), pages 464–468.\nAssociation for Computational Linguistics.\nHassan A. Sleiman and Rafael Corchuelo. 2013. A\nsurvey on region extractors from web documents.\nIEEE Trans. Knowl. Data Eng., 25(9):1960–1981.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. 2022. Efficient transformers: A survey. ACM\nComput. Surv.\nNicolas Tempelmeier, Elena Demidova, and Stefan Di-\netze. 2018. Inferring missing categorical information\nin noisy and sparse web markup. In Proceedings of\nthe 2018 World Wide Web Conference on World Wide\nWeb, WWW 2018, Lyon, France, April 23-27, 2018,\npages 1297–1306. ACM.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nQifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xi-\naojun Quan, and Dongfang Liu. 2022a. Webformer:\nThe web-page transformer for structure information\nextraction. In WWW ’22: The ACM Web Confer-\nence 2022, Virtual Event, Lyon, France, April 25 - 29,\n2022, pages 3124–3133. ACM.\nQifan Wang, Bhargav Kanagal, Vijay Garg, and\nD. Sivakumar. 2019. Constructing a comprehensive\nevents database from the web. In Proceedings of the\n28th ACM International Conference on Information\nand Knowledge Management, CIKM 2019, Beijing,\nChina, November 3-7, 2019, pages 229–238. ACM.\nQifan Wang, Li Yang, Bhargav Kanagal, Sumit Sanghai,\nD. Sivakumar, Bin Shu, Zac Yu, and Jon Elsas. 2020a.\nLearning to extract attribute value from product via\nquestion answering: A multi-task approach. In Pro-\nceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery amp; Data Min-\ning, KDD ’20, page 47–55, New York, NY , USA.\nAssociation for Computing Machinery.\nQifan Wang, Li Yang, Jingang Wang, Jitin Krishnan,\nBo Dai, Sinong Wang, Zenglin Xu, Madian Khabsa,\nand Hao Ma. 2022b. SMARTA VE: Structured mul-\ntimodal transformer for product attribute value ex-\ntraction. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2022, pages 263–276,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nYansen Wang, Zhen Fan, and Carolyn Penstein Rosé.\n2020b. Incorporating multimodal information in\nopen-domain web keyphrase extraction. In Proceed-\nings of the 2020 Conference on Empirical Methods in\n2415\nNatural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 1790–1800. Associa-\ntion for Computational Linguistics.\nSen Wu, Luke Hsiao, Xiao Cheng, Braden Hancock,\nTheodoros Rekatsinas, Philip Alexander Levis, and\nChristopher Ré. 2018. Fonduer: Knowledge base\nconstruction from richly formatted data. In Proceed-\nings of the 2018 International Conference on Man-\nagement of Data, SIGMOD Conference 2018, Hous-\nton, TX, USA, June 10-15, 2018, pages 1301–1316.\nACM.\nLee Xiong, Chuan Hu, Chenyan Xiong, Daniel Cam-\npos, and Arnold Overwijk. 2019. Open domain\nweb keyphrase extraction beyond language modeling.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 5174–5183.\nAssociation for Computational Linguistics.\nHuimin Xu, Wenting Wang, Xin Mao, Xinyu Jiang, and\nMan Lan. 2019. Scaling up open tagging from tens\nto thousands: Comprehension empowered attribute\nvalue extraction from product title. In Proceedings\nof the 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n5214–5223. Association for Computational Linguis-\ntics.\nYang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei,\nGuoxin Wang, Yijuan Lu, Dinei A. F. Florêncio, Cha\nZhang, Wanxiang Che, Min Zhang, and Lidong Zhou.\n2021. Layoutlmv2: Multi-modal pre-training for\nvisually-rich document understanding. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 2579–2591. Associa-\ntion for Computational Linguistics.\nYiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu\nWei, and Ming Zhou. 2020. Layoutlm: Pre-training\nof text and layout for document image understanding.\nIn KDD ’20: The 26th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, Virtual\nEvent, CA, USA, August 23-27, 2020 , pages 1192–\n1200. ACM.\nYulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu\nYang, and Mitsuru Ishizuka. 2009. Unsupervised\nrelation extraction by mining wikipedia texts using\ninformation from the web. In ACL 2009, Proceedings\nof the 47th Annual Meeting of the Association for\nComputational Linguistics and the 4th International\nJoint Conference on Natural Language Processing of\nthe AFNLP , 2-7 August 2009, Singapore, pages 1021–\n1029. The Association for Computer Linguistics.\nLi Yang, Qifan Wang, Zac Yu, Anand Kulkarni, Sumit\nSanghai, Bin Shu, Jon Elsas, and Bhargav Kanagal.\n2022. Mave: A product dataset for multi-source\nattribute value extraction. In Proceedings of the Fif-\nteenth ACM International Conference on Web Search\nand Data Mining, WSDM ’22, page 1256–1265, New\nYork, NY , USA. Association for Computing Machin-\nery.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntañón, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big bird: Trans-\nformers for longer sequences. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nKai Zhang, Yuan Yao, Ruobing Xie, Xu Han, Zhiyuan\nLiu, Fen Lin, Leyu Lin, and Maosong Sun. 2021.\nOpen hierarchical relation extraction. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 5682–5693. Associa-\ntion for Computational Linguistics.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. HI-\nBERT: document level pre-training of hierarchical\nbidirectional transformers for document summariza-\ntion. In Proceedings of the 57th Conference of the As-\nsociation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1:\nLong Papers, pages 5059–5069. Association for Com-\nputational Linguistics.\nYao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa\nKe. 2018. Paragraph-level neural question gener-\nation with maxout pointer and gated self-attention\nnetworks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 3901–3910. Association for Computational\nLinguistics.\nZihan Zhao, Lu Chen, Ruisheng Cao, Hongshen Xu,\nXingyu Chen, and Kai Yu. 2022. TIE: topological\ninformation enhanced structural reading comprehen-\nsion on web pages. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2022.\nYichao Zhou, Ying Sheng, Nguyen V o, Nick Edmonds,\nand Sandeep Tata. 2021. Simplified DOM trees for\ntransferable attribute extraction from the web. CoRR,\nabs/2101.02415.\nWill Y . Zou, Richard Socher, Daniel M. Cer, and\nChristopher D. Manning. 2013. Bilingual word em-\nbeddings for phrase-based machine translation. In\nProceedings of the 2013 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2013, 18-21 October 2013, Grand Hyatt Seattle, Seat-\ntle, Washington, USA, A meeting of SIGDAT, a Spe-\ncial Interest Group of the ACL , pages 1393–1398.\nACL.\n2416\nA More Technical Details\nWe provide more technical details on our MUST in\nthis section.\nMUST Encoder As mentioned in the main paper,\nthe MUST encoder is a stack of Lidentical layers:\nXl = MUST(Xl−1), 1 ≤l≤L\nwhere X0 is the input embedding for the first layer,\nwhich is obtained from the embedding layer. Each\nencoder layer contains a structural attention layer\nfollowed by a standard feed forward network:\nZk = StrAtt(Xk−1), Xk = FFN(Zk)\nThe StrAtt layer uses the structural attention mech-\nanism described in the main paper. We supplement\nthe full details of the Top-Down attention and the\nLocal attention.\nTop-Down Attention The Top-Down attention\nis defined as:\neTD\nij =\nxTI\ni WTD\nQ (xD\nj WTD\nK )T\n√\nd\nαTD\nij =\nexp(eTD\nij )∑\nℓ exp(eTD\niℓ )\nLocal Attention The Local attention is defined\nas:\neLA\nij =\nxTI\ni WLA\nQ (xTI\nj WLA\nK )T\n√\nd\nαLA\nij =\nexp(eLA\nij )∑\nℓ∈Ci exp(eLA\niℓ ), forj ∈Ci\nB Dataset\nB.1 Data Processing\nThe WebSRC dataset contains three types of web\npages, i.e. KV (key-value), Comparison and Table.\nAs stated in the main paper, we only use the KV\ntype pages in our experiments. The reason is that\nboth Comparison and Table web pages are more\nsuitable for multi-object extraction, where those\nobjects’ information are described in a table or list\nand can be obtained directly with repeated pattern\nor table extraction techniques (Wang et al., 2019).\nFor the KV pages, the key-value pairs only contain\nvalue text without any span information in the text\nsequence of the web page. Therefore, we need to\nlabel the span of the value in the text sequence,\nFigure 7: Example of schema.org annotations of an\nevent page, including name, description, date and loca-\ntion.\nsince the sequential tagging task in MUST requires\ntoken level spans during training.\nThe Common Crawl dataset contains a huge\namount of web pages with schema.org annotations,\nwhich are used as the supervision in various infor-\nmation extraction tasks. An example of schema.org\nEvent annotations is shown in Figure 7. It contains\nthe annotation type “https://schema.org/Event”, as\nwell as the annotations for all the event fields in-\ncluding name, description, date and location. In\nour experiments, we work on three big domains\n- Movies, Events and Products. We further filter\nthese pages by restricting to English and single\nobject pages (have one single schema.org type an-\nnotation). We also label the span corresponding to\nthe field in the text sequence.\nThe process of labeling spans is straightforward\nas follows:\n• Use white-space to tokenize the text on the\nweb into unigrams. For example, ‘This is\na very long paragraph about HelloKitty’ is\ntokenized to [‘This’, ‘is’, ‘a’, ‘very’, ‘long’,\n‘paragraph’, ‘about’, ‘HelloKitty’]. In this\nstep, all punctuations are removed.\n• Use white-space to tokenize the answer into\nunigrams. For example, ‘very long’ is tok-\nenized to [‘very’, ‘long’].\n• Search and match the answer unigrams in the\ntext unigrams.\n• Map the unigram span of the answer to char-\nacter bytes span.\n2417\nData Splits WebSRC Common Crawl\nMoviesEventsProducts\nTrain 2,572 45,58661,512 84,937\nDev/Test 321 5,698 7,689 10,617\nTotal 3,214 56,98276,890106,171\nTraining Time (15 epoch)11m 2h 45m3h 38m4h 42m\nTable 5: Statistics of the datasets with the training time.\nThere are 3.87% examples in the Common Crawl\ndataset, whose answer text can not be matched by\nthis procedure. We simply exclude these examples\nin our experiments. Moreover, we also found there\nare roughly 21.54% examples where the answer\nhas multiple occurrences in the text.\nB.2 Statistics\nThe statistics of the datasets with training time are\nshown in Table 5.\nB.3 Baseline Discussion\nWe want to provide some clarification on the results\nof the two baselines, WebFormer and MarkupLM,\nin Table 1. First, for both methods, we directly run\ntheir codes to obtain the results. The code/model of\nMarkupLM is publicly available. For WebFormer,\nwe obtain the original code and model from its\nauthors. Second, our results are consistent with\nMarkupLM on WebSRC (last row in their Table\n1). Here we use stronger baseline MarkupLM-large\nfor comparison. Third, for CommonCrawl, we re-\nprocess the data by removing non-matched ground-\ntruth (as discussed above), resulting in slightly less\ndata (in our Table 5) compared to the data used\nin WebFormer (in their Table 1). This is the main\nreason why the reported numbers of WebFormer in\nthis work are even higher than the original results.\nC Implementation Details\nFor data pre-processing, we use open-source\nLXML library4 to process each page for obtaining\nthe DOM tree structures. For all these baselines, we\nuse the same English uncased WordPiece vocabu-\nlary as in BERT. The word embedding is initialized\nwith the pretrained BERT-base. The encoder pa-\nrameters used in MUST are 12 layers, 768 hidden\nsize, 3072 hidden units (for FFN). The maximum\ntext sequence length is set to 2048. The decoder\nparameters used in MUST are 4 layers, 768 hid-\nden size, 3072 hidden units, max output sequence\nlength is 128. During training, we use the gradient\n4https://lxml.de/\nParameter Value\nencoder layers 12\nencoder heads 12\nencoder hiden size 768\nencoder hidden units 3,072\nmax input sequence length 2,048\ndecoder layer 4\ndecoder heads 6\ndecoder hiden size 768\ndecoder hidden units 3,072\nmax output sequence length 128\nbeam width 6\nbatch size 64\ntraining epochs 15\noptimizer Adam\nlearning rate schedule linear decay\nlearning rate 2e−5\nlearning rate warmup steps 5,000\nvocab BERT-base\nvocab size 30,522\nα 0.8\nβ 0.5\nTable 6: Model Hyper-parameters details.\ndescent algorithm with Adam optimizer. The ini-\ntial learning rate is set to 2e−5. The batch size for\neach update is set as 64 and the model is trained for\nup to 15 epochs. The dropout probability for the\nattention layer is set to 0.1. The model parameters\nare provided in Table 6.\n2418\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLeft blank.\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□\u0017 B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. Left blank.\nC □\u0013 Did you run computational experiments?\nLeft blank.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n2419\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n2420"
}