{
    "title": "A Comparison of Character Neural Language Model and Bootstrapping for Language Identification in Multilingual Noisy Texts",
    "url": "https://openalex.org/W2805953373",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A5008737006",
            "name": "Wafia Adouane",
            "affiliations": [
                null,
                "University of Gothenburg"
            ]
        },
        {
            "id": "https://openalex.org/A5014291440",
            "name": "Simon Dobnik",
            "affiliations": [
                null,
                "University of Gothenburg"
            ]
        },
        {
            "id": "https://openalex.org/A5070681215",
            "name": "Jean-Philippe Bernardy",
            "affiliations": [
                null,
                "University of Gothenburg"
            ]
        },
        {
            "id": "https://openalex.org/A5110302916",
            "name": "Nasredine Semmar",
            "affiliations": [
                "CEA LIST",
                "Commissariat à l'Énergie Atomique et aux Énergies Alternatives"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2130903752",
        "https://openalex.org/W2739602391",
        "https://openalex.org/W2165698076",
        "https://openalex.org/W162697342",
        "https://openalex.org/W2149933564",
        "https://openalex.org/W4233576700",
        "https://openalex.org/W2786351641",
        "https://openalex.org/W3032726",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W1445897916",
        "https://openalex.org/W2251801815",
        "https://openalex.org/W2152463966",
        "https://openalex.org/W4254816979",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2097089247",
        "https://openalex.org/W2250548009",
        "https://openalex.org/W2949369097",
        "https://openalex.org/W2557418006",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W2154455818",
        "https://openalex.org/W1579429723",
        "https://openalex.org/W4299518610",
        "https://openalex.org/W2963012544"
    ],
    "abstract": "International audience",
    "full_text": "Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 22–31\nNew Orleans, Louisiana, June 6, 2018.c⃝2018 Association for Computational Linguistics\nA Comparison of Character Neural Language Model and Bootstrapping\nfor Language Identiﬁcation in Multilingual Noisy Texts\nWaﬁa Adouane1, Simon Dobnik1, Jean-Philippe Bernardy1, and Nasredine Semmar2\n1Department of Philosophy, Linguistics and Theory of Science (FLoV),\nCentre for Linguistic Theory and Studies in Probability (CLASP), University of Gothenburg\n2CEA, LIST, Vision and Content Engineering Laboratory Gif-sur-Yvette, France\n{wafia.adouane,simon.dobnik,jean-philippe.bernardy}@gu.se\nnasredine.semmar@cea.fr\nAbstract\nThis paper seeks to examine the effect of in-\ncluding background knowledge in the form of\ncharacter pre-trained neural language model\n(LM), and data bootstrapping to overcome the\nproblem of unbalanced limited resources. As a\ntest, we explore the task of language identiﬁca-\ntion in mixed-language short non-edited texts\nwith an under-resourced language, namely the\ncase of Algerian Arabic for which both la-\nbelled and unlabelled data are limited. We\ncompare the performance of two traditional\nmachine learning methods and a deep neural\nnetworks (DNNs) model. The results show\nthat overall DNNs perform better on labelled\ndata for the majority categories and struggle\nwith the minority ones. While the effect of\nthe untokenised and unlabelled data encoded\nas LM differs for each category, bootstrap-\nping, however, improves the performance of\nall systems and all categories. These methods\nare language independent and could be gener-\nalised to other under-resourced languages for\nwhich a small labelled data and a larger unla-\nbelled data are available.\n1 Introduction\nMost Natural Language Processing (NLP) tools\nare generally designed to deal with monolingual\ntexts with more or less standardised spelling.\nHowever, users in social media, especially in\nmultilingual societies, generate multilingual non-\nedited material where at least two languages or\nlanguage varieties are used. This phenomenon is\nlinguistically referred to as language (code) mix-\ning where code-switching and borrowing, among\nothers, are the most studied phenomena. Poplack\nand Meechan (1998) deﬁned borrowing as a mor-\nphological or a phonological adaptation of a word\nfrom one language to another and code-switching\nas the use of a foreign word, as it is in its origi-\nnal language, to express something in another lan-\nguage. However, the literature does not make it\nclear whether the use of different script is counted\nas borrowing, or code-switching or something\nelse. For instance, there is no linguistic well-\nmotivated theory about how to classify languages\nwritten in other scripts, like French written in Ara-\nbic script which is frequently the case in North\nAfrica. This theoretical gap could be explained\nby the fact that this fairly recent phenomenon has\nemerged with the widespread of the new tech-\nnologies. In this paper, we consider both code-\nswitching and borrowing and refer to them collec-\ntively as language mixing. Our motivation in do-\ning so is to offer to sociolinguists a linguistically\ninformative tool to analyse and study the language\ncontact behaviour in the included languages.\nThe task of identifying languages in mixed-\nlanguage texts is a useful pre-processing tool\nwhere sequences belonging to different lan-\nguages/varieties are identiﬁed. They are then pro-\ncessed by further language/variety-speciﬁc tools\nand models. This task itself has neither been well\nstudied for situations when many languages are\nmixed nor has it been explored as a main or an aux-\niliary task in multi-task learning (see Section 2).\n1.1 Related Work\nThere has been some interesting work in detecting\ncode mixing for a couple of languages/language\nvarieties, mostly using traditional sequence la-\nbelling algorithms like Conditional Random Field\n(CRF), Hidden Markov Model (HMM), linear ker-\nnel Support Vector Machines (SVMs) and a com-\nbination of different methods and linguistic re-\nsources (Elfardy and Diab, 2012; Elfardy et al.,\n2013; Barman et al., 2014b,a; Diab et al., 2016;\nSamih and Maier, 2016; Adouane and Dobnik,\n2017). Prior work that is most closely related\nto our work using neural networks and related\nlanguages, Samih et al. (2016) used supervised\n22\ndeep neural networks (LSTM) and a CRF classi-\nﬁer on the top of it to detect code-switching, us-\ning small datasets of tweets, between Egyptian\nArabic and MSA and between Spanish and En-\nglish using pre-trained word embeddings trained\non larger datasets. However, in their annotation\nthey combined ambiguous words, which are words\nthat could be of either languages depending on the\ncontext, in one category called ’ambiguous’ and\nignored words from minority languages. More-\nover, the system was evaluated on a dataset with\nno instances of neither ’ambiguous’ nor ’mixed-\nlanguage’ words, basically distinguishing between\nMSA and Egyptian Arabic words in addition to\nNamed Entities and other non-linguistic tokens\nlike punctuation, etc.\nSimilar to our work, Kocmi and Bojar (2017)\nproposed a supervised bidirectional LSTM model.\nHowever, the data used to train the model was cre-\nated by mixing edited texts, at a line level, in 131\nlanguages written in different scripts to create a\nmultilingual data, making it a very different task\nfrom the one investigated here. We use non-edited\ntexts, a realistic data as generated by users reﬂect-\ning the real use of the included languages which\nare all written in the same Arabic script. Our texts\nare shorter and the size of the dataset is smaller,\ntherefore, our task is more challenging.\nBy comparison to our work, most of the litera-\nture focuses on detecting code-switching points in\na text, either at a token level or at a phrase level\nor even beyond a sentence boundaries, we distin-\nguish between borrowing and code-switching at\na word level by assigning all borrowed words to\na separate variety (BOR). Most importantly, our\nmain focus is to investigate ways to inject ex-\ntra knowledge to take advantage of the unlabelled\ndata.\n1.2 Linguistic Situation in Algeria\nThe linguistic landscape in Algeria consists of\nseveral languages which are used in different\nsocial and geographic contexts to different de-\ngrees (Adouane et al., 2016a): local Arabic va-\nrieties (ALG), Modern Standard Arabic (MSA)\nwhich is the only standardised Arabic variety,\nBerber which is an Afro-Asiatic language different\nfrom Arabic and widely spoken in North Africa,\nand other non-Arabic languages such as French,\nEnglish, Spanish, Turkish, etc. A typical text con-\nsists of a mixture of these languages, and this mix-\nture is often referred to, somewhat mistakenly as\nAlgerian Arabic. In this paper, we use the term Al-\ngerian language to refer to a mixture of languages\nand language varieties spoken in Algeria, and the\nterm Algerian variety (ALG) to refer to the local\nvariety of Arabic, which is used alongside other\nlanguages such as, for example Berber (BER).\nThis work seeks to identify the language or lan-\nguage variety of each word within an Algerian lan-\nguage text. Algerian language is characterised by\nnon-standardised spelling and spelling variations\nbased on the phonetic transcription of many local\nvariants. For instance, the Algerian sentence in\n(1), which is user generated, is a mixture of 3 lan-\nguages (Arabic, French and Berber) and 2 Arabic\nvarieties (MSA and ALG). Each word is coloured\nby its language in d., b. is an IPA transcription\nand c. is the human English translation. To il-\nlustrate the difﬁculty of the problem, we addition-\nally show the (incorrect) translation proposed by\nGoogle translate e., where words in black are ad-\nditional words not appearing in the original sen-\ntence.\n(1) a. /charbc/char40/char50/charf1/chard3 /char48/char2e/char41/char4a/char2e/charcb /char51/charba/char83 /charf0/char10/chare9/char10/charaf/char41/chara2/charcb/char40 /charc9/char67/charfa/char0a/charce/char4b/char2e/charf1/char10/char4a/charca/char4a/char0a/char83\nb. [muræk ælbæb sekkær wu æt Qaqæ èæl\nsi:ltupli:]\nc. Please open the window and close the\ndoor behind you\nd. French Algerian Berber MSA Berber\nMSA Algerian\ne. SELTOPLEY POWER SOLUTION\nAND SUGAR FOR MORAK PAPER\nAll the words in different languages are normally\nwritten in the Arabic script, which causes high de-\ngree of lexical ambiguity and therefore even if we\nhad dictionaries (only available for MSA) it would\nbe hard to disambiguate word senses this way. In\n(1), the ALG word /charc9/char67open means solution in\nMSA, the Berber word /char10/chare9/char10/charaf/char41/chara2/charcb/char40window which is\nadapted to the MSA morphology by adding the\nMSA deﬁnite article /charc8/char40(case of borrowing) means\nenergy/capacity in MSA. The Berber word /char51/charba/char83\n23\nclose means sugar / sweeten / liquor / get drunkin\nMSA.\nMoreover, the rich morphology of Arabic is\nchallenging because it is a fusional language\nwhere sufﬁxes and other morphemes are added to\nthe base word, and a single morpheme denotes\nmultiple aspects and features. Algerian Arabic\nshares many linguistic features with MSA, but it\ndiffers from it mainly phonologically, morpholog-\nically and lexically. For instance, a verb in the ﬁrst\nperson singular in ALG is the same as the ﬁrst per-\nson plural in MSA. The absence of a morphologi-\ncal/syntactic analyser for ALG makes it challeng-\ning to correctly analyse an ALG text mixed with\nother languages and varieties.\nExcept for MSA, Arabic varieties are neither\nwell-documented nor well-studied, and they are\nclassiﬁed as under-resourced languages. Further-\nmore, social media are the only source of written\ntexts for Algerian Arabic. The work in NLP on Al-\ngerian Arabic and other Arabic varieties also suf-\nfers severely from the lack of labelled (and even\nunlabelled) data that would allow any kind of su-\npervised training. Another challenge is that we\nhave to deal with all the complications present\nin social media domain, namely the use of short\ntexts, spelling and word segmentation errors, etc.\nin addition to the non-standard orthography used\nin informal Arabic varieties. We see the task of\nidentiﬁcation of the variety of each word in a text\na necessary ﬁrst step towards developing more\nsophisticated NLP tools for this Arabic variety\nwhich is itself a mixture of other languages and\nvarieties.\nIn this paper we explore two avenues for im-\nproving the state of the art in variety identiﬁ-\ncation for Algerian Arabic. First, we measure\nthe ability of recurrent neural networks to iden-\ntify language mixing using only a limited train-\ning corpus. Second, we explore to what extent\nadding background knowledge in the form of pre-\ntrained character-based language model and boot-\nstrapping can be effective in dealing with under-\nresourced languages in the domain of language\nidentiﬁcation in mixed-language texts for which\nneither large labelled nor unlabelled datasets ex-\nist.\nThe paper is organized as follows: in Section\n2, we give a brief overview of methods for lever-\ning learning from limited datasets. In Section 3,\nwe describe the data. In Section 4, we present the\narchitecture of our learning conﬁgurations which\ninclude both traditional approaches and deep neu-\nral networks and explain the training methods used\non the labelled data, experiments and results. In\nSection 5, we experiment with these models when\nadding background knowledge and report the re-\nsults.\n2 Leveraging Limited Datasets\nDeep learning has become the leading approach\nto solving linguistic tasks. However deep neural\nnetworks (DNNs) used in a supervised and un-\nsupervised learning scenario usually require large\ndatasets in order for the trained models to per-\nform well. For example, Zhang et al. (2015) es-\ntimates that the size of the training dataset for\ncharacter-level DNNs for text classiﬁcation task\nshould range from hundreds of thousands to sev-\neral million of examples.\nThe limits imposed by the lack of labelled\ndatasets have been countered by combining\nstructural learning and semi-supervised learn-\ning (Ando and Zhang, 2005). Contrary to the su-\npervised approach where a labelled dataset is used\nto train a model, in structural learning, the learner\nﬁrst learns underlying structures from either la-\nbelled or unlabelled data. If the model is trained\non labelled data, it should be possible to reuse\nthe knowledge encoded in the relations of the pre-\ndictive features in this auxiliary task, if properly\ntrained, to solve other related tasks. If the model\nis trained on unlabelled data, the model captures\nthe underlying structures of words or characters in\na language as a language model (LM), i.e., model\nthe probabilistic distribution of words and charac-\nters of a text.\nSuch pre-trained LM should be useful for var-\nious supervised tasks assuming that linguistic\nstructures are predictive of the labels used in these\ntasks. Approaches like this are known as trans-\nfer learning or multi-task learning (MTL) and are\nclassiﬁed as a semi-supervised approaches (with\nno bootstrapping) (Zhou et al., 2004). There is an\nincreasing interest in evaluating different frame-\nworks (Ando and Zhang, 2005; Pan and Yang,\n2010) and comparing neural network models (Cho\net al., 2014; Yosinski et al., 2014). Some studies\nhave shown that MTL is useful for certain tasks\n(Sutton et al., 2007) while others reported that it is\nnot always effective (Alonso and Plank, 2017).\nBootstrapping (Nigam et al., 2000) is a gen-\n24\neral and commonly used method of countering the\nlimits of labelled datasets for learning. It is a\nsemi-supervised method where a well-performing\nmodel is used to automatically label new data\nwhich is subsequently used as a training data for\nanother model. This helps to enhance supervised\nlearning. However, this is also not always effec-\ntive. For example, Pierce and Cardie (2001) and\nAndo and Zhang (2005) show that bootstrapping\ndegraded the performance of some classiﬁers.\n3 Data\nIn this section, we describe the datasets that we use\nfor training and testing our models. We use two\ndatasets: small dataset, annotated with language\nlabels, and a larger dataset lacking such annota-\ntion.\n3.1 Labelled data\nWe use the human labelled corpus described by\nAdouane and Dobnik (2017) where each word is\ntagged with one of the following labels: ALG (Al-\ngerian), BER (Berber), BOR (Borrowing), ENG\n(English), FRC (French), MSA (Modern Stan-\ndard Arabic), NER (Named Entity), SND (inter-\njections/sounds) and DIG (digits). The annotators\nhave access to the full context for each word. To\nthe best of our knowledge, this corpus is the only\navailable labelled dataset for code-switching and\nborrowing in Algerian Arabic, written in Arabic\nscript, and in fact also one of the very few available\ndatasets for this particular language variety over-\nall. Because of the limited annotation resources\nthe corpus is small, containing only 10,590 sam-\nples (each sample is a short text, for example one\npost in a social media platform). In total, the data\ncontains 215,875 tokens distributed unbalancely\nas follows: 55.10% ALG (representing the major-\nity category with 118,960 words), 38.04% MSA\n(82,121 words), 2.80% FRC (6,049 words), 1.87%\nBOR (4,044 words), 1.05% NER (2,283 words),\n0.64% DIG (1,392 numbers), 0.32% SND (691 to-\nkens), 0.10% ENG (236 words), and 0.04% BER\n(99 words).\n3.2 Unlabelled data\nUnfortunately, there is no existing user-generated\nunlabelled textual corpus for ALG. Therefore, we\nalso collected, automatically and manually, new\ncontent from social media in Algerian Arabic\nwhich include social networking sites, blogs, mi-\ncroblogs, forums, community media sites and user\nreviews.1\nThe new raw corpus contains mainly short non-\nedited texts which require further processing be-\nfore useful information can be extracted from\nthem. We cleaned and pre-processed the cor-\npus following the pre-processing and normalisa-\ntion methods described by Adouane and Dobnik\n(2017). The data pre-processing and normalisa-\ntion is based on applying certain linguistic rules,\nincluding: 1. Removal of non-linguistic words\nlike punctuation and emoticons (indeed emoticons\nand inconsistent punctuation are abundant in so-\ncial media texts.) 2. Reducing all adjacent re-\npeated letters to maximum two occurrences of let-\nters, based on the principle that MSA allows no\nmore than two adjacent occurrences of the same\nletter. 3. Removal of diacritics representing short\nvowels, because these are rarely used; 4. Removal\nall duplicated instances of texts; 5. Removal of\ntexts not mainly written in Arabic script 6. Nor-\nmalisation all remaining characters to the Arabic\nscript. Indeed, some users use related scripts like\nPersian, Pashto or Urdu characters, either because\nof their keyboard layout or to express some sounds\nwhich do not exist in the Arabic alphabet, e.g. /p/,\n/v/ and /g/.\nAdditionally, we feed each document, as a\nwhole, to a language identiﬁcation system that dis-\ntinguishes between the most popular Arabic vari-\neties (Adouane et al., 2016b) including MSA; Mo-\nroccan (MOR); Tunisian (TUN); Egyptian (EGY);\nLevantine (LEV); Iraqi (IRQ) and Gulf (GUF)\nArabic. We retain only those predicted to be Al-\ngerian language, so that we can focus on language\nidentiﬁcation within Algerian Arabic, at the word\nlevel.\nTable 1 gives some statistics about the labelled\nand unlabelled datasets. Texts refer to short texts\nfrom social media, words to linguistic words ex-\ncluding punctuation and other tokens, and types\nto sets of words or unique words. We notice that\n82.52% of the words occur less than 10 times in\nboth datasets. This is due to the high variation\nof spelling and misspellings which are common in\nthese kinds of texts.\n1We have a documented permission from the own-\ners/users of the used social media platforms to use their tex-\ntual contributions for research.\n25\nDataset #Texts #Words #Types\nLabelled 10,590 213,792 57,054\nUnlabelled 189,479 3,270,996 290,629\nTable 1: Information about datasets.\n4 Using Labelled Data\n4.1 Systems and Models\nWe frame the task as a sequence labelling prob-\nlem, namely to assign each word in a sequence the\nlabel of the language that the word has in that con-\ntext. We use three different approaches: two ex-\nisting sequence labelling systems – (i) an HHM-\nbased sequence labeller (Adouane and Dobnik,\n2017); (ii) a classiﬁcation-based system with vari-\nous back-off strategies from (Adouane and Dob-\nnik, 2017) which previously performed best on\nthis task, henceforth called the state-of-the-art sys-\ntem; and (iii) a new system using deep neural net-\nworks (DNNs).\n4.1.1 HMM system\nThe HMM system is a classical probabilistic se-\nquence labelling system based on Hidden Markov\nModel where the probability of a label is estimated\nbased on the history of the observations, previous\nwords and previous labels. In order to optimise the\nprobabilities and ﬁnd the best sequence of labels\nbased on a sequence of words, the Viterbi algo-\nrithm is used. For words that have not been seen in\nthe training data, an constant low probability com-\nputed from the training data is assigned.\n4.1.2 State-of-the-art system\nThe best-so-far performing system for identifying\nlanguage mixing in Algerian texts is described by\nAdouane and Dobnik (2017). The system is a\nclassiﬁer-based model that predicts the language\nor variety of each word in the input text with var-\nious back-off-strategies: trigram and bigram clas-\nsiﬁcation, lexicon lookup from fairly large man-\nually compiled and curated lexicons, manually-\ndeﬁned rules capturing linguistic knowledge based\non word afﬁxes, word length and character combi-\nnations, and ﬁnally the most frequent class (uni-\ngram).\n4.1.3 DNN model\nRecurrent Neural Networks (RNNs) (Elman,\n1990) have been used extensively in sequence pre-\ndiction. The most popular RNN variants are the\nLong Short-Term Memory (LSTMs) (Hochreiter\nand Schmidhuber, 1997) and the Gated Recurrent\nUnit (GRUs) (Cho et al., 2014).\nOur neural networks consists of four layers: one\nembedding layer, two recurrent layers, and a dense\nlayer with softmax activation. All our models are\noptimized using the Adam optimizer, built using\nthe Keras library (Chollet, 2015), and run using\na TensorFlow backend. A summary of the model\narchitecture is shown in Figure 1. (This variant\nis composed of only the uncoloured (white) parts\nof the ﬁgure; the coloured parts are added in the\nmodel described in section 5). The DNN is pro-\nvided the input character by character. We opt for\ncharacter-based input rather than word-based in-\nput for two reasons. First, we expect that the inter-\nnal structure of words (phonemes and morphemes)\nis predictive of a particular variety. This way we\nhope to capture contexts within words and across\nwords. Second, we do not have to worry about the\nsize of the vocabulary, which we would if we were\nto use word embeddings.\nThis language-identiﬁcation model is trained\nend-to-end. Because of the nature of RNNs, the\nnetwork will assign one language variant per in-\nput symbol, and thus per character — even though\nthe tags are logically associated word-by-word. To\ndeal with this mismatch, when training we tag\neach character of a word and the space which fol-\nlows it with the variant of the word. When eval-\nuating the model, we use the tag associated with\nthe space, so that all the word has been fed to the\nmodel before a prediction is made.\nWe have trained models with various values\nfor the hyper-parameters: number of layers, num-\nber of epochs, memory size, drop-out rate and\nthe batch size, but report detailed results for the\nmodel with the best behaviour. We experimented\nwith both GRU and LSTM RNNs and found that\nthe GRU performs better than LSTM on our task\nwhich is in line with the results of the previous\ncomparisons but on different tasks (Chung et al.,\n2014). We also found out that our best systems are\noptimised with the architecture shown in Figure 1\nwith a memory size of 200, batch size of 512 and\nnumber of epochs of 25. Increasing or decreasing\nthese values caused the overall accuracy to drop.\nUsing drop-out improved the performance of the\nsystems (overall accuracy > 90%) over not using\nit ( < 70%). The best results are obtained using\ndrop-out rate of 0.2 for the recurrent layers. We\n26\nrefer to this model as DNN in the following.\nEmbedding\nLanguage model (2 × RNN)\nTagging model (2 × RNN)\nDense layer\nSoftmax\ninput characters in [0..38]\ncharacter representation in R40\nlanguage model state in R200\ntagging model state in R200\ntag score in R10\ntag prediction in [0,1]10\nDense layer\nSoftmax\ncharacter representation in R40\nnext character prediction in [0,1]39\nFigure 1: DNN architecture.\n4.2 Results\nTo ensure a fair comparison, all the models have\nbeen evaluated under the same conditions. We\nuse 10-fold cross validation on all of them and\nreport their performance measured as the aver-\nage accuracy. Table 2 shows the results. Note\nthat for the DNN we only report the results of\nthe (best-performing) GRU models. As a base-\nline we take the most frequent category in the\nlabelled data. State-of-the-art (2) outperforms\nslightly HMM (1). DNN (3) outperforms slightly\nthe State-of-the-art (2). All the systems perform\nbetter than the baseline.\nModel Accuracy (%)\n1 HMM 89.29\n2 State-of-the-art 89.83\n3 DNN 90.53\n4 Baseline 55.10\nTable 2: Performance of the models on labelled data.\nFigure 2 shows the performance of each model\nper category reported as average F-score. Over-\nall the models perform better on the majority cat-\negories such as ALG (Algerian) and MSA (Mod-\nern Standard Arabic), and non linguistic categories\nlike DIG (digits) and SND (sounds) because their\npatterns are more or less regular and language in-\ndependent. The State-of-the-art system achieves\nthe best performances for all categories except for\nALG where it is slightly outperformed by DNN,\nALG BER BOR DIG ENG FRC MSA NER SND\n0\n20\n40\n60\n80\n100\nHMM State-of-the-art DNN\nFigure 2: Models’ average F-score per category.\naverage F-score of 91.45 and 92.22 respectively.\nA possible explanation for this is that the State-\nof-the-art system is more robust because it in-\nvolves several strategies of classiﬁcation. DNN\nperformed better than HMM in all cases except for\nENG (English) and SND. Both DNN and HMM\nstruggle with minority categories like ENG, BOR\n(borrowing), BER (Berber), NER (Named Enti-\nties), and FRC (French). Note that in this exper-\niment we only used the smaller labelled dataset.\nIn the following section, we explore ways to take\nadvantage of the additional relatively large unla-\nbelled dataset in order to improve the performance\nof the systems.\n5 Using Data Augmentation With\nBackground Knowledge\n5.1 Training Methods\nIn this section, we examine which data augmen-\ntation method performed on the unlabelled cor-\npus can best enhance the performance of our three\nmodels. We experiment with data bootstrapping,\npre-training a language model, and the combina-\ntion of both methods. In each case, we are pro-\nviding some form of background knowledge com-\npared to the task described in Section 4.\n5.1.1 Bootstrapping\nFor bootstrapping, we use the State-of-the-art sys-\ntem (Section 4.1.2) to label the unlabelled data\nwithout additional checking of the quality of an-\nnotation and then use this bootstrapped data in fur-\nther training. We re-run the experiments described\nin Section 4 using the bootstrapped data as the\n27\ntraining data. We refer to the systems as HMM\nbootstrapped, State-of-the-art bootstrapped , and\nDNN bootstrapped respectively.\n5.1.2 Language Model\nAnother way to take advantage of the unlabelled\ndata is to train a language model (LM) on the\nwhole data and use the internal state of the LM\nas input to the tagger, rather than using the raw\ntextual input. To this end, we modify the struc-\nture of our DNN as indicated by the blue-coloured\nparts in Figure 1. Namely, we add two language-\nmodelling RNN layers between the embedding\nand the tagging layers. They are followed by a\ndense layer with softmax activation, which pre-\ndicts the next character in the input.\nWith this setup, we train the language-\nmodelling layers on the unlabelled corpus, as a\ngenerative language model on the unlabelled data\nset. Thus, the output of these layers contains the\ninformation necessary to predict the next charac-\nter given the previous sequence of characters. The\nlanguage model is trained on 80% of the unla-\nbelled data and evaluated on the remaining 20%.\nThe rest of the network is then trained as in the\nprevious case (Section 4.1.3). We stress that, in\nthis instance, only the last two layers are trained\non the language-identiﬁcation task. We refer to\nthis model as DNN with LM.\n25 50 75 100 125 150\n1.68\n1.7\n1.73\n1.75\n1.78\n1.8\n1.83\nvalidation loss training loss\nFigure 3: Language model loss through training epochs\nYou may notice in Figure 3 that the model is\nstill improving (at 150 epochs), albeit slowly, even\nafter exhausting our computational budget. Never-\ntheless, the model appears to be working well as a\ntext generator. For instance, we took sentence (1)\nas a seed and obtained sentences that are gram-\nmatically and structurally acceptable, even if they\nare semantically meaningless and reproduce the\nmany spelling variants found in the original cor-\npus. Here are two examples:\n1. /charfa/char0a\n/char09/char47/char40/char50 /charfa/char0a/char10/chare6/char09/char6b /char41/char4b/char0a/chare9/char3c/charcb/char40 /charf0 /char41/charee/char44/char0a/charca/charab/char10/char48/charf1/chard6/char09/chardf/char11/char80/char50/char59/char10/charae/char09/char4b /char41/chard3 /char41/char09/char4b/char40 /chard1/charea/chard6/charcf/char40\n/charc8/char41/char6d/charcc/char27/char40/char09/chare1/chard3 /chard5/charce/charaa/char10/char4a/char09/char4b/char11/char80/char50/char59/char10/charae/char09/char4b /char41/chard3 /char41/char09/char4b/char40 /charbd/charcb/charf1/char10/charae/char09/char4b /char41/chard3 /chare9/char3c/charcb/char40 /charf0 /charbc/char41/charaa/chard3\n2. /char09/charac/char51/charaa/char4b/char0a/charf0 /charf0/char43/char4a/char2e/char10/char1c/char4b/char0a/char11/char81/char09/charaf/char51/charaa/char4b/char0a/char41/chard3 /charbc/char50/char41/char4a/char2e/char4b/char0a/chare9/char3c/charcb/char40 /charc8/charf1/char10/charae/char09/char4b /char41/chard3 /chare9/char3c/charcb/char40 /charf0\n/char41/char09/char4a/char4b/char0a/char59/charee/char45/char0a/chare9/char3c/charcb/char40 /charc8/charf1/char10/charae/char4b/char0a/char49/char2e/char6d/char1a/char27/char0a/char11/char81/char1c/char2e/char6d/char1a/char27/char0a/char41/chard3 /charfa/char0a/charcd/char40 /char40/char51/chard6/charcf/char40 /chare8/char40/char50/char09/chare1/char4b/char0a/charf0\n5.1.3 Language Model and Bootstrapping\nWe retrain the DNN model using the pre-trained\nLM and the bootstrapped data in order to optimise\nthe use of the unlabelled data. We refer to this\nmodel as DNN bootstrapped and LM.\n5.2 Results\nWe evaluate all the models under the same condi-\ntions as in Section 4, using 10-fold cross valida-\ntion we report the average accuracy over the folds.\nThe evaluation set in the bootstrapping models in\neach fold is only taken from the labelled data while\nthe training part consists of a combined 9-folds\nfrom the labelled data and the entire bootstrapped\ndata. In other words, the entire bootstrapped data\nis added to the training data at each time. In\nthe case of DNNs, we found again that GRUs\nperform signiﬁcantly better than LSTM, and that\nbootstrapped models are optimised with drop-out\nrate of 0.2 whereas models with language model\nperform better with drop-out rate of 0.1. The ob-\ntained results are reported as the average accuracy\nin Table 3. For the DNN, we only report the results\nof the (best-performing) GRU models.\nModel Accuracy (%)\n1 HMM bootstrapped 93.97\n2 State-of-the-art bootstrapped 95.42\n3 DNN bootstrapped 93.31\n4 DNN with LM 90.31\n5 DNN bootstrapped and LM 90.19\nTable 3: Performance of the models with background\nknowledge.\nThe best performance overall is achieved by the\nbootstrapped state-of-of-the-art model (2). HMM\nbootstrapped (1) performs slightly better than\nthe DNN bootstrapped (3). Bootstrapping helps\nthe State-of-the-art system and HMM more than\nDNN. This is due to the training nature of the\nDNN which is based on capturing frequent reg-\nular patterns, hence adding the bootstrapped data\nmeans introducing even more irregular patterns.\n28\nCompared to the results in Section 4.2, the DNN\nbootstrapped (3) outperforms all the models with\nthe labelled data: (1), (2) and (3) in Table 2. The\nbootstrapping method thus improves the perfor-\nmance of all conﬁgurations, whether they are us-\ning DNNs or not. The reported beneﬁts of boot-\nstrapping are contrary to the previous observations\nwhere bootstrapping did not help (Section 2).\nHowever, the use of the language model (4) de-\ncreases slightly (−0.22%) the performance of the\nDNN compared to its performance with the la-\nbelled data (3) in Table 2. The use of the bootstrap-\nping and the language model (5) leads to no signif-\nicant difference in performance in respect to (4).\nOverall, it appears that the usage of the language\nmodel has no strong effect. This could be caused\nby the noise in the data, and adding more unla-\nbelled data makes it hard for the language model\nto learn all the data irregularities. Maybe the sys-\ntem requires more training data.\nALG BER BOR DIG ENG FRC MSA NER SND\n0\n20\n40\n60\n80\n100\nHMM bootstrapped\nState-of-the-art bootstrapped\nDNN with LM\nDNN bootstrapped\nDNN bootstrapped and LM\nFigure 4: Models’ average F-score per category.\nFigure 4 sums up the performance of each\nmodel per category reported as the average F-\nscore. The ﬁrst thing to notice is that bootstrap-\nping improves the performance of all systems, and\nthe best performance is achieved with the State-of-\nthe-art. This could be explained by ‘the more data,\nthe better performance’. HMM bootstrapped out-\nperforms the DNN bootstrapped except for FRC\nand BER. Adding language model to the DNN\ncauses the overall accuracy to drop compared to\nthe DNN bootstrapped. Nevertheless, compared\nto the results in Figure 2, language model be-\nhaves differently with each category. For instance,\nit boosts the performance of the DNN on ENG,\nand the performance on BOR, BER, FRC over\nHMM. Whereas combining language model and\nbootstrapped data performs the worst except for\nBER, ENG and NER. The effect of combining\nbootstrapping and language model is better for mi-\nnority categories: BER, ENG and NER.\nError analysis of the confusion matrices shows\nthat all the systems are confused, chieﬂy between\nALG and MSA, BOR and ALG, FRC and ALG.\nThe confusions are caused mainly by the lexical\nambiguity between these categories, given that we\nidentify the language of each word in its context.\n6 Conclusions\nWe have examined the automatic classiﬁcation of\nlanguage identiﬁcation in mixed-language texts\non limited datasets of Algerian Arabic, in par-\nticular a small unbalanced labelled dataset and\na slightly larger unlabelled dataset. We tested\nwhether the inclusion of a pre-trained LM on\nthe unlabelled dataset and bootstrapping the un-\nlabelled dataset can leverage the performance of\nthe systems. Overall when using only the small\nlabelled data, DNNs outperformed the HMM and\nthe State-of-the-art system. However, DNNs per-\nformed better on the majority categories and strug-\ngled with the minority ones in comparison to the\nState-of-the-art system. Bootstrapping improved\nthe performance of all models, both DNNs and not\nDNNs for all categories.\nAdding a background knowledge in the form of\na pre-trained LM to DNNs had a different effect\nper category. While it boosted the performance\nof the minority categories, its effect on the major-\nity ones was not clear. Despite the generative be-\nhaviour of the LM, tested in Section 5.1.2, which\nshowed that LM did learn the underlying struc-\ntures of the unlabelled data, the effect of the en-\ncoded knowledge maybe was not suitable for our\nmain task. This could be also caused by the high\nnoise level in the data, even though deep learning\nis generally thought to handle noise well.\nIn our future work, we will focus on exploring\n(i) different DNN conﬁgurations to investigate the\nbest ways of injecting background knowledge as\nwell as (ii) different data pre-processing methods\nto normalise spelling and remove misspellings for\nMSA, and deal with word segmentation errors.\n29\nAcknowledgement\nThe research reported in this paper was supported\nby a grant from the Swedish Research Council\n(VR project 2014-39) for the establishment of the\nCentre for Linguistic Theory and Studies in Prob-\nability (CLASP) at the University of Gothenburg.\nReferences\nWaﬁa Adouane and Simon Dobnik. 2017. Proceed-\nings of the Third Arabic Natural Language Process-\ning Workshop, chapter Identiﬁcation of Languages\nin Algerian Arabic Multilingual Documents. Asso-\nciation for Computational Linguistics.\nWaﬁa Adouane, Nasredine Semmar, Richard Johans-\nson, and Victoria Bobicev. 2016a. Automatic De-\ntection of Arabicized Berber and Arabic Varieties.\npages 63–72. The COLING 2016 Organizing Com-\nmittee.\nWaﬁa Adouane, Nasredine Semmar, Richard Johans-\nson, and Victoria Bobicev. 2016b. Automatic Detec-\ntion of Arabicized Berber and Arabic Varieties. In\nProceedings of the Third Workshop on NLP for Simi-\nlar Languages, Varieties and Dialects, pages 63–72.\nH´ector Mart `ınez Alonso and Barbara Plank. 2017.\nWhen is Multitask Learning Effective? Seman-\ntic Sequence Prediction under Varying Data Condi-\ntions. In EACL (long).\nRie Kubota Ando and Tong Zhang. 2005. A Frame-\nwork for Learning Predictive Structures from Multi-\nple Tasks and Unlabeled Data. The Journal of Ma-\nchine Learning Research, 6:1817–1853.\nUtsab Barman, Amitava Das, Joachim Wagner, and\nJennifer Foster. 2014a. Code-mixing: A challenge\nfor Language Identiﬁcation in the Language of So-\ncial Media. In In Proceedings of the First Workshop\non Computational Approaches to Code-Switching.\nUtsab Barman, Joachim Wagner, Grzegorz Chrupala,\nand Jennifer Foster. 2014b. Dcu-uvt: Word-Level\nLanguage Classiﬁcation with Code-Mixed Data.\nKyunghyun Cho, Bart Van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learn-\ning Phrase Representations Using RNN Encoder-\nDecoder for Statistical Machine Translation. arXiv\npreprint arXiv:1406.1078.\nFranc ¸ois Chollet. 2015. Keras.https://github.\ncom/fchollet/keras.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical Evaluation\nof Gated Recurrent Neural Networks on Sequence\nModeling. arXiv:1412.3555.\nMona Diab, Pascale Fung, Mahmoud Ghoneim, Julia\nHirschberg, and Thamar Solorio. 2016. Proceed-\nings of the Second Workshop on Computational Ap-\nproaches to Code Switching. Association for Com-\nputational Linguistics, Austin, Texas.\nHeba Elfardy, Mohamed Al-Badrashiny, and Mona\nDiab. 2013. Code Switch Point Detection in Arabic.\nIn Proceedings of the 18th International Conference\non Application of Natural Language to Information\nSystems (NLDB2013).\nHeba Elfardy and Mona Diab. 2012. Token Level Iden-\ntiﬁcation of Linguistic Code Switching. In COL-\nING, pages 287–296.\nJeffrey L. Elman. 1990. Finding Structure in Time.\nCognitive Science, 14(2):179–211.\nSepp Hochreiter and Juergen Schmidhuber. 1997.\nLong Short-Term Memory. Neural Computation ,\n9(8):1735–1780.\nTom Kocmi and Ondˇrej Bojar. 2017. Lanidenn: Multi-\nlingual Language Identiﬁcation on Text Stream. In\nProceedings of the 15th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Volume 1, Long Papers , pages 927–936.\nAssociation for Computational Linguistics.\nKamal Nigam, Andrew Kachites McCallum, Sebastian\nThrun, and Tom Mitchell. 2000. Text Classiﬁcation\nfrom Labeled and Unlabeled Documents Using EM.\nMachine Learning, Special issue on Information Re-\ntrieval, pages 103–134.\nSinno Jialin Pan and Qiang Yang. 2010. A Survey on\nTransfer Learning. IEEE Transactions on Knowl-\nedge and Data Engineering, 22(10).\nDavid Pierce and Claire Cardie. 2001. Limitations\nof Co-training for Natural Language Learning from\nLarge Datasets. In Proceedings of the 2001 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nShana Poplack and Marjory Meechan. 1998. How Lan-\nguages Fit Together in Codemixing. The Interna-\ntional Journal of Bilingualism, 2(2):127–138.\nYounes Samih, Suraj Maharjan, Mohammed Attia,\nLaura Kallmeyer, and Thamar Solorio. 2016. Multi-\nlingual code-switching identiﬁcation via lstm recur-\nrent neural networks. In Proceedings of the Second\nWorkshop on Computational Approaches to Code\nSwitching, pages 50–59.\nYounes Samih and Wolfgang Maier. 2016. Detecting\nCode-Switching in Moroccan Arabic. In Proceed-\nings of SocialNLP @ IJCAI-2016.\nCharles Sutton, Andrew McCallum, and Khashayar\nRohanimanesh. 2007. Dynamic Conditional Ran-\ndom Fields: Factorized Probabilistic Models for La-\nbeling and Segmenting Sequence Data. Journal of\nMachine Learning Research, 8(Mar):693–723.\n30\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How Transferable are Features in\nDeep Neural Networks? In Advances in Neural In-\nformation Processing Systems 27 (NIPS ’14), NIPS\nFoundation.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level Convolutional Networks for Text\nClassiﬁcation. Advances in Neural Information\nProcessing Systems 28 (NIPS 2015).\nDengyong Zhou, Olivier Bousquet, Thomas Navin\nLal, Jason Weston, and Bernhard Sch ¨olkopf. 2004.\nLearning with Local and Global Consistency . In\nNIPS 2003.\n31"
}