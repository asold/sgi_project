{
  "title": "Artificial intelligence and free will: generative agents utilizing large language models have functional free will",
  "url": "https://openalex.org/W4410293199",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5038038035",
      "name": "Frank Martela",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4387378202",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4388696164",
    "https://openalex.org/W4386307074",
    "https://openalex.org/W4380575774",
    "https://openalex.org/W4385447813",
    "https://openalex.org/W4406800520",
    "https://openalex.org/W4406139504",
    "https://openalex.org/W4401717638",
    "https://openalex.org/W2075162993",
    "https://openalex.org/W2742015144",
    "https://openalex.org/W2318568705",
    "https://openalex.org/W2955083195",
    "https://openalex.org/W2622744968",
    "https://openalex.org/W1991779836",
    "https://openalex.org/W926153431",
    "https://openalex.org/W2105140708",
    "https://openalex.org/W4252582630",
    "https://openalex.org/W2057060574",
    "https://openalex.org/W2342835205",
    "https://openalex.org/W2072267104",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4393065402",
    "https://openalex.org/W4292542163",
    "https://openalex.org/W2902999784",
    "https://openalex.org/W2114116188",
    "https://openalex.org/W2081155210",
    "https://openalex.org/W2122517769",
    "https://openalex.org/W2140031276",
    "https://openalex.org/W4383104208",
    "https://openalex.org/W3007421089",
    "https://openalex.org/W4293236341",
    "https://openalex.org/W2394834952",
    "https://openalex.org/W2946798032",
    "https://openalex.org/W3163428404",
    "https://openalex.org/W2023377725",
    "https://openalex.org/W2526702845",
    "https://openalex.org/W4242280694",
    "https://openalex.org/W2920815137",
    "https://openalex.org/W4213178090",
    "https://openalex.org/W4213304676",
    "https://openalex.org/W4405017722",
    "https://openalex.org/W3185446826",
    "https://openalex.org/W2325029567"
  ],
  "abstract": "Abstract Combining large language models (LLMs) with memory, planning, and execution units has made possible almost human-like agentic behavior, where the artificial intelligence creates goals for itself, breaks them into concrete plans, and refines the tactics based on sensory feedback. Do such generative LLM agents possess free will? Free will requires that an entity exhibits intentional agency, has genuine alternatives, and can control its actions. Building on Dennett’s intentional stance and List’s theory of free will, I will focus on functional free will, where we observe an entity to determine whether we need to postulate free will to understand and predict its behavior. Focusing on two running examples, the recently developed Voyager, an LLM-powered Minecraft agent, and the fictitious Spitenik, an assassin drone, I will argue that the best (and only viable) way of explaining both of their behavior involves postulating that they have goals, face alternatives, and that their intentions guide their behavior. While this does not entail that they have consciousness or that they possess physical free will, where their intentions alter physical causal chains, we must nevertheless conclude that they are agents whose behavior cannot be understood without postulating that they possess functional free will.",
  "full_text": "ORIGINAL RESEARCH\nAI and Ethics (2025) 5:4389–4400\nhttps://doi.org/10.1007/s43681-025-00740-6\nhuman supervision, while reacting to their environment in a \nway that advances their progress [ 10, 11]. Given that such \nentities react to the changing environment, make plans, and \nmake many independent decisions on their way toward their \ngoals, the question becomes: Do they possess a degree of \nfree will?\nFree will, roughly speaking, refers to whether an entity \ncan alter its own behavior by making choices based on its \ninternal intentions. Whether humans have such free will has \nbeen a classic philosophical question pondered by every -\none from Epictetus, St. Augustine, and Thomas Aquinas \nto Descartes, Spinoza, and Kant [ 12]. The debate has been \nespecially intensified with the advent of the modern scien -\ntific worldview, as the idea of fundamental laws of nature \ngoverning physical events has been hard to reconcile with \nthe idea of freely determined human action. Incompatibil-\nists see that physical determinism implies that agents can -\nnot genuinely do otherwise, and thus do not possess free \nwill [e.g., 13], while compatibilists aim to find some way to \nreconcile physical determinism with free human will [e.g., \n14]. The more specific definition and conditions for free will \nvary between authors, with a rich debate of arguments and \ncounterarguments between different positions, see [12, 15]. \n1 Introduction\nThe most recent large language models (LLM) have acquired \nmany human-level cognitive capabilities. In addition to hold-\ning a conversation, they can solve verbal insight problems \n[1], they can engage in chain-of-thought reasoning [2], they \ncan make and execute plans [3], they can modify their mod-\nels about the world based on incoming sensor data [4], their \nmedical advice is only weakly distinguishable from expert \nanswers [5, 6], and they can generate innovative business \nideas better than business school students [7]. When LLMs, \nsuch as GPT-4, are combined with memory, planning, and \nexecution units [8], they can engage in human-like behavior \nin simulated environments, such as exploring and acquiring \nnew skills in Minecraft [9] or inviting their secret crush to a \nValentine’s Day party [3]. At the same time, the latest gen -\nerations of self-driving cars, battle robots, and drones can \nact autonomously, pursuing their overarching goals without \n \r Frank Martela\nfrank.martela@aalto.fi\n1 Aalto University, Espoo, Finland\nAbstract\nCombining large language models (LLMs) with memory, planning, and execution units has made possible almost human-\nlike agentic behavior, where the artificial intelligence creates goals for itself, breaks them into concrete plans, and refines \nthe tactics based on sensory feedback. Do such generative LLM agents possess free will? Free will requires that an entity \nexhibits intentional agency, has genuine alternatives, and can control its actions. Building on Dennett’s intentional stance \nand List’s theory of free will, I will focus on functional free will, where we observe an entity to determine whether we \nneed to postulate free will to understand and predict its behavior. Focusing on two running examples, the recently devel -\noped V oyager, an LLM-powered Minecraft agent, and the fictitious Spitenik, an assassin drone, I will argue that the best \n(and only viable) way of explaining both of their behavior involves postulating that they have goals, face alternatives, \nand that their intentions guide their behavior. While this does not entail that they have consciousness or that they possess \nphysical free will, where their intentions alter physical causal chains, we must nevertheless conclude that they are agents \nwhose behavior cannot be understood without postulating that they possess functional free will.\nKeywords AI · Free will · Functionalism · Generative agents · Intentional stance · Large language models\nReceived: 24 January 2025 / Accepted: 17 April 2025 / Published online: 12 May 2025\n© The Author(s) 2025\nArtificial intelligence and free will: generative agents utilizing large \nlanguage models have functional free will\nFrank Martela1\n1 3\nAI and Ethics (2025) 5:4389–4400\nInstead of aiming to address the whole debate, a task impos-\nsible for one article, I will here follow List [ 16, 17] in set-\nting the bar relatively high by stating that for an entity to \nhave free will it must fulfill three conditions: It must have \n(1) intentional agency, (2) genuine alternatives, and (3) the \ncapacity to control its actions. Only if all three conditions \nare met, can an entity be said to possess free will.\nTo determine whether an entity fulfills these conditions, \nI will follow a functional, explanatory approach, where we \nexamine an entity’s behavior from the outside, to determine \nwhether our best way of understanding and predicting its \nbehavior entails postulating that it has intentionality, alter -\nnatives, and the capability to choose [ 16, 17]. If an entity \nbehaves as if  it has free will, if we are unable to explain \nits behavior without assuming that it has intentions and is \nmaking choices, then for all practical purposes—such as \ninteracting with it or trying to predict or understand it—we \nget the best results by postulating it has free will see [ 18]. \nIn this case, we can conclude that the entity has functional \nfree will. List, writing in 2019, argues that chess-playing \ncomputers might be best explained at the algorithmic level \nbut future AI systems with “richer and more flexible behav-\nioural repertoire” could reach a point where “viewing them \nas intentional agents may be not just optional, but explana -\ntory necessary” [17, p. 157]. The aim of the present article is \nto investigate whether the latest generation of AI fulfills the \nconditions for having free will.\nTo focus our discussion, I will utilize two running exam-\nples of AI-powered agents. First, Voyager is an open-ended \ngenerative agent embodied in the computer game Minecraft, \npowered by the GPT-4 large language model [ 9]. Its archi-\ntecture involves three key components: (1) A planning unit \nthat automatically generates new tasks for the agent taking \ninto account the agent’s current state and level of progress, \n(2) a memory unit that stores and retrieves acquired skills \nand patterns of complex behavior, (3) an action unit that \niteratively generates prompts for behavior, observes what \neffect the action has in the environment, and incorporates \nthe feedback for another round of prompt refinement until \nits self-verification module confirms it has completed the \ntask it aimed to accomplish. All three systems are powered \nby GPT-4, which thus acts as the generator of the agent’s \nbehavior. Minecraft is an open-ended gaming environment \nwhere the game does not have a predefined end goal, clear \npoint system, or a fixed storyline, instead allowing the agent \nto explore the vast, procedurally generated 3D terrains, \nwhere the agent can mine wood, cook food, fight monsters, \nand craft new tools to unlock a tech tree. The V oyager is \ngiven the overarching goal of “discover as many diverse \nthings as possible, accomplish as many diverse tasks as pos-\nsible and become the best Minecraft player in the world” \n[9, p. 21]. However, the agent’s more specific behavior is \nautonomous, based on LLM-powered, in-situation plan -\nning and decision-making that builds on its understanding \nof its current situation and its memory of what it has thus far \nlearned in the game. Tests of V oyager agents in Minecraft \ndemonstrate that they are able to explore vast territories and \nacquire new skills automatically, with different trials pro -\nducing different behaviors, thus functioning there as inde -\npendent agents [9].\nSpitenik is an AI-directed autonomous drone equipped \nwith cameras, other sensors, and weaponry, capable of \npatrolling an area and identifying and attacking enemies \nwithout human intervention. Already in 2020, the retreating \nforces of Libyan strongman Khalifa Hafter were attacked \nby unmanned autonomous combat aerial vehicles, in par -\nticular Turkish STM Kargu-2 drones, in likely the first case \nof AI-based autonomous weapons being used to kill [ 19, \n20]. Israel’s Harpy drones can be launched to a pre-defined \n“loitering area” where they autonomously patrol, detecting, \nengaging, and destroying enemy targets [ 21]. Currently, \nthe US, China, and other major powers are in an arms race \nto develop the next generations of lethal autonomous war \nrobots (often called lethal autonomous weapon systems or \nLAWS) equipped with even more intelligence and lethal \ncapacity [22, 23]. Autonomous battle drones outmaneuver-\ning each other utilizing sophisticated AI-generated tactics \nare turning from science fiction into reality as we speak.\nFor the sake of building an example, let’s give Spitenik \na few more characteristics. This is a fictitious example but \nas regards its cognitive functioning, it aims to stay within \nthe capabilities of current agentic LLM-powered systems in \nthe area of unmanned aerial vehicles [ 10, 11]1. As regards \nits cognitive architecture, it is a generative agent [ 3] with a \nmemory unit for storing past experiences, a planning unit \nthat synthesizes past memories, current environmental facts, \nand long-term goals into a number of short-term goals, and \nan execution unit transforming present goals into action, \nwhile staying responsive to environmental feedback [ 3, 9]. \nLet’s first give it a mission: To find and destroy a known ter-\nrorist leader Axl Rogue. It is given the probable coordinates \nwhere Rogue resides, and all known photographs of Rogue, \nand it is launched from an undisclosed location in the Nor -\ndic countries, towards the Rogue’s location in Siberia. Let’s \nsay that Spitenik is solar-powered, filling its battery with \nsunlight. It has a face detection system, a flight system \n1  Combat drones is an area where developers understandably are \nreluctant to share latest breakthroughs. The capacity of the solar-pow-\nered battery and the ability to use the computational power of GPT-4 \nor similar energy-consuming LLM model in a portable unit needing \nto have light-weight battery might represent technical challenges to \nthe existence of Spitenik [ 11]. But its agentic capabilities—which \nare relevant for the present discussion—represent nothing that would \nbe impossible given latest LLMs and the cognitive architecture built \naround them.\n1 3\n4390\nAI and Ethics (2025) 5:4389–4400\nallowing it to avoid obstacles and choose its route, and a \nsurvival system, the latter designed to monitor the battery \nlevels and the time of sunrise and sunset, with the capability \nto search for a safe place to rest during the night, waiting \nfor sunlight. It has a memory unit that includes information \nabout how it and other similar drones behaved in the past \nand what results their behavior had, from which it retrieves \ninformation about similar situations when making decisions \nin a novel situation. Its planning unit is iterative, making \nplans and executing them, constantly monitoring the envi -\nronment for unexpected stimuli that lead it to reiterate its \nplans. All of these capabilities are approximately possible \ngiven current technology [3, 9–11].\nI will come to argue that V oyager and Spitenik both \npossess functional free will. Both of their behavior is best \nexplained and predicted by assuming that they have inten -\ntions and plans that they have decided to follow and based \non which they make choices among alternatives about what \nto do next. Without this assumption, our ability to interpret \nand predict their behavior is considerably diminished, and \nthus postulating free will to them is part of our best expla -\nnations of their behavior. I will set aside the question of \nwhether they ‘really’ have intentions, whether they ‘really’ \nmake decisions, and thus whether they ‘really’ have free \nwill—I will explain why I don’t see those as interesting \nor even well-defined questions. What matters for the pres -\nent conclusions is that both V oyager and Spitenik behave \nas if they would make decisions among alternatives based \non their autonomously chosen goals and plans, which is \nenough to conclude that they have functional free will.\n2 What is free will?\nTo start answering the question of whether AI can have free \nwill, we first need a working conceptualization of free will \nitself. Free will, in general, entails that an entity has the abil-\nity to alter its behavior by making choices among genuine \nalternatives based on its goals and intentions. According \nto List [ 16, 17], this boils down to three conditions all of \nwhich must be fulfilled for an entity to have free will: (1) \nthe capacity for intentional agency, (2) the capacity to have \nalternative possibilities, and (3) the capacity to control one’s \nactions.\nThe capacity for intentional agency means that to have \nfree will, the entity needs to be an actor with goals and pur-\nposes. The entity thus needs to not only mechanistically \nreact to stimuli but actively aim at something, interacting \nwith its environment in a goal-directed way. Intentional \nagency is more formally defined by List as an input-output \nsystem that must have representational states that encode \nthe entity’s ‘beliefs’ about how things are, motivational \nstates that encode the entity’s ‘desires’, and a capacity to \nprocess and utilize those states in its interaction with the \nenvironment. The entity thus needs to have some encoded \nmodel of the state of the world, an encoded model of their \nown goals, and some way of using those models in guiding \ntheir action.\nHaving alternative possibilities  means that there needs \nto be several options available for the agent. An electric \nkettle, having very simple circuitry, has no alternative pos -\nsibilities when the user pushes the button: As the circuits are \nswitched on, it inescapably starts heating the water. The phi-\nlosopher having pushed the button, on the other hand, can \nmeanwhile contemplate on their tea bag collection, having \nseveral alternative possibilities of what flavor of tea to enjoy \ntoday. Sometimes called the “could have done otherwise” \nclause, this condition thus emphasizes that several paths \nneed to be open for the entity, otherwise it can’t have free \nwill. The need to have alternative possibilities also empha -\nsizes the role of the entity’s environment in providing more \nor less room for free will. An intentional agent with a capac-\nity to control its actions is not able to exercise its potential \nfor free will if the environment does not allow it to have \nseveral possibilities from which to choose.\nThe capacity to control  one’s actions means that “the \nrelevant actions of any bearer of free will are caused, not \nmerely by some nonintentional physical processes but by \nthe appropriate mental states, and specifically the intentions \nbehind those actions” [17, p. 24]. The entity thus must have \ngoals and desires—and those goals and desires need to be \nwhat causes it to act in a certain way. The entity’s goals and \ndesires thus must play a causal role in its actions—without \nthose goals and desires the action would not take place.\n2.1 Physical free will and functional free will\nHow can we determine whether an entity fulfills these three \nconditions? To make our intuitions clear, it is good to dis -\ntinguish two types of free will: physical free will and func -\ntional free will.\nPhysical free will  implies that ‘the mind’ is somehow \nable to have a direct causal influence on the physical real -\nity. Thoughts and intentions must somehow influence the \natoms, thus breaking or escaping the determinism 2 of laws \nof physics by behaving in a way not predetermined by the \nphysical reality. While there have been—especially in the \npast—proponents of this kind of free will, see [ 12, 15], it \nis hard to reconcile this kind of free will with the modern \n2  Or indeterminism of physics. The point of physical free will is that \nthe mind influences matter. If that matter is indeterministic (e.g. due \nto quantum level randomness) this does not change the picture. Still, \nmind would need to somehow escape the physical laws of nature, be \nthey deterministic or indeterministic.\n1 3\n4391\nAI and Ethics (2025) 5:4389–4400\ntheories. Importantly, different explanatory theories operate \non different levels—some focus on explaining the behavior \nof atoms, others focus on explaining the behavior of liq -\nuids, livers, or persons. Given this focus on explaining the \nbehavior of an entity from the outside, functional free will is \nnot dependent on ‘consciousness’ understood as life feeling \nsomething from the inside. An entity can have free will in \nthe sense of making choices based on some encoded goals \nand maps of the environment no matter whether these goals \nfeel like something for the entity. I’ll return to the topic of \nconsciousness in the discussion.\nIt is worth noting that for List [ 16, p. 174], free will “is \nnot a physical phenomenon, but a higher-level phenom -\nenon.” Thus, from his point of view, talking about ‘physical \nfree will’ makes a category mistake of confusing different \nlevels of explanation. He would likely argue that what I call \nfunctional free will is the only plausible way to interpret \nfree will. Nevertheless, as much of the intuitive support \nfor incompatibilism seems to emerge from ideas related to \nphysical free will, making this distinction helps in sorting \nout our intuitions and making it clearer what a belief in free \nwill commits us to.\n2.2 Physical and functional free will operate on \ndifferent levels of explanation\nContemporary philosophy of science increasingly recog -\nnizes a layered ontology, where various phenomena can \nbe observed on several separate levels of explanation [ 28, \n29]. For example, humans can be observed from a physi -\ncal, chemical, biological, neurological, or psychological \npoint of view, each revealing something unique about how \nthe human body behaves and functions. Although the higher \nlevels build upon the lower levels, they also exhibit emer -\ngent properties and behavior that are “ not explainable  in \nterms of—or, alternatively, not reducible  to—lower-level \nproperties” [30, p. 870]. As an example, think about a glass \nflask filled with water that you start to heat [example from \n17, p. 133]. At boiling point, the pressure created by the \nwater suddenly breaks the bottle. How to explain what hap-\npened (or predict in advance what will happen)? We could \ntry to focus on the micro-level behavior of the individual \nmolecules utilizing relevant mechanistic or quantum theo -\nries—but that would not get us far. Instead, a macro-level \nexplanation utilizing thermodynamical laws that ignore \nindividual atoms and focus on the behavior of ‘liquids’ and \n‘gas’ when they are heated up will provide the most predic-\ntive explanation of the situation—allowing us to calculate, \nfor example, how thick the glass would need to be to not \nbreak. Thermodynamical laws make it possible to explain \nand predict behavior not explainable at the molecular level, \nscientific worldview that postulates certain laws of nature \nas explanations for the movement of matter. The key chal -\nlenge for physical free will is the principle of causal clo -\nsure, according to which all physical events can be fully \nexplained by physical causes governed by the fundamental \nlaws of nature [24, 25]. Physical free will would entail that \nthe mind—whatever that is—can directly alter or influence \nthose physical causal chains, making choices that are not \nconstrained by physical laws, and having intentions that \nstand in some way outside the physical laws of nature. Thus, \nto demonstrate that some entity has physical free will, we \nwould need to see evidence of it altering its physical behav-\nior in a way that escapes the physical causal chains.\nWhile there are theorists still debating causal closure and \nmental causation e.g [ 26]., I will here follow List [ 17, p. \n150] in concluding that humans do not possess such free \nwill: “From a purely physical or neuroscientific perspective, \nthere are no intentional agents; there are no forks in the road \nbetween which an agent can choose; and there is no causa -\ntion of human actions by people’s intentions.” As far as we \nknow, the human mind possesses nothing that would allow \nit to directly influence or escape the physical reality, and \nthus on the level of physical reality, humans have no free \nwill. And if humans have no such physical free will, asking \nwhether AI has such free will is a rather uninteresting ques-\ntion: There is nothing in their microprocessors that would \nallow them to escape the laws of physics either. On the \nphysical level, their behavior can be tracked down to elec -\ntrons moving through the circuits. Thus, it is rather obvious \nthat AI does not possess physical free will any more than \nhumans do.\nHowever, functional free will is a more interesting form \nof free will as it is compatible with the modern scientific \nworldview, yet proposes that humans have such free will. \nEspecially developed by Christian List [ 16, 17], and build-\ning on Dennett’s [ 18, 27] intentional stance, functional \nfree will examines an entity from the outside to determine \nwhether we need to postulate free will to explain its behav -\nior. If our most predictive theories of explaining that entity’s \nbehavior entail free will, making it impossible to understand \nand predict their behavior without that postulate, then for all \npractical purposes—such as interacting with the entity or \ntrying to predict or understand it—we need to conclude that \nthe entity has free will.\nFunctional free will thus entails an explanatory account \nwhere we in essence ask: “How can we best explain a given \nsystem’s behavior?” [ 17, p. 55]. When trying to interpret \nan entity’s behavior—be it a rock, an electric kettle, a dog, \na philosopher, or an LLM—we can utilize various theories \nthat yield more or less reliable ways of explaining and pre -\ndicting its behavior, ranging from Newtonian physics and \nthermodynamical laws to biological and psychological \n1 3\n4392\nAI and Ethics (2025) 5:4389–4400\n2.3 Humans have functional free will but not \nphysical free will\nBefore answering whether LLMs have free will, let’s settle \nfirst whether humans have free will. As already noted, I \ndon’t believe humans possess physical free will, as there \nis nothing in our mind that could alter the physical causal \nchains. However, concluding that humans have no free will \non a physical level is compatible with saying that they pos-\nsess free will on the behavioral level.\nWhen we try to explain human behavior, physical theo -\nries will not get us far. If I tell you that “I promise to be at \nour seaside meeting spot tomorrow at noon”, your ability to \npredict my location the next day at 12 will not be served by \nquantum mechanics, thermodynamical laws, or even evolu-\ntion theory. However, postulating that humans have ‘inten -\ntions’ and ‘commitments’ that are expressible in certain \nsounds being interpreted as ‘language’ will help you predict \nthat I will almost certainly be at the meeting spot the next \nday. Our ability to navigate our everyday life inhabited by \nother humans (and ourselves as a human) is made possible \nby postulating that humans adhere to certain psychological \nbehavioral patterns, such as attempting to be loyal to friends, \ntrying to keep promises, and more generally having certain \ngoals, values, and desires that guide their behavior. These \nare all phenomena that do not exist at the physical level. \nWhile the billions of physical neurons in our brain give rise \nto these psychological phenomena, factors such as the mul-\ntiple realizability of psychological phenomena make it—at \nleast practically but most likely also theoretically—impos -\nsible to reduce them to something explainable by physical \nlaws [17]. Any attempt to explain human behavior requires \ntaking seriously the psychological level, as that gives us the \nmost powerful tools to predict how a particular human being \nwill behave.\nKeeping in mind the intentional stance and the differ -\nent levels of explanation, it is relatively easy to see that on \nthe levels most relevant for explaining human behavior, \nhuman action is characterized by intentionality, alternatives, \nand making choices—List’s three criteria for free will. In \nexplaining and predicting human behavior, we regularly \ntreat them as creatures that have goals and intentions. We see \nthem as creatures that make choices among alternatives and \nthat could have chosen otherwise, and we hold each other \naccountable for those choices. Humans thus seem like the \nprime candidates for having functional free will. To explain \nand predict their behavior, we treat them as if they would \nmake choices among alternatives based on their intentions. \nWhatever our view on whether humans have physical free \nwill we inescapably treat them as creatures possessing func-\ntional free will, both in our everyday life and in our attempts \nto scientifically explain their behavior.\nand the same holds true for psychological and economic \ntheories.\nOf course, we can ask does a ‘liquid’ exist for real or is \nit just an aggregate of molecules? Asking such a question is \na symptom of conflating ‘real’ with the lowest level of real-\nity. If only atoms are ‘real’, then there are no such things as \nliquids, bacteria, humans, universities, or economies, as all \nof these entail postulating entities operating at higher lev -\nels. However, if our aim is to explain and predict phenom -\nena around us—as it should be—then we need to conclude \nthat theories at various levels all provide predictive power \nas regards phenomena taking place at that level, that is not \nprovided by the other levels.\nWe thus have several potential theories we use—both in \nour everyday lives and as scientists—to explain and pre -\ndict the behavior of various entities. While some of these \nexplanatory theories, like the law of gravity, make no \nassumptions about the inner states of the entity, others take \nan “intentional stance” [ 27] towards the entity, postulating \nintentions, beliefs, goals, and desires as part of the explana-\ntion for its behavior. Attributing intentionality to the elec -\ntric kettle does not explain anything about its behavior that \nmechanistic knowledge about electric circuitry would not \nexplain. Explaining why a dog runs after a rabbit, however, \nis impossible with the laws of physics alone. When we pos-\ntulate that the dog has a strong biologically instilled desire \nto run after potential prey, as hunting has been necessary \nfor the survival of its ancestors, this provides an explana -\ntion that helps us predict the behavior of the given dog in \nsituations involving rabbits, mice, birds, and other similar \nanimals.\nThe best way to infer whether an entity has intentions \nis thus to try to explain that entity’s behavior, to determine \n“whether the system is best explained in intentional terms” \n[17, p. 57]. If postulating intentions and agency is indis -\npensable for an explanation of the entity’s behavior, this is a \nstrong indicator of it being an agent [ 18]. Absent any com-\nplicating factors, an inference to the best explanation would \nthen compel us to conclude that the entity indeed is an agent \nwith intentions.\nOne good reason to favor this functional account to deter-\nmine whether an entity has free will is that we don’t have \nmany good alternatives. We don’t know what is it like to be \nan LLM any more than we know what it is like to be a bat \nor an electric kettle cf [ 31]. The only entity we can experi -\nence ‘from the inside’ is ourselves. Thus, when evaluating \nthe free will of anyone else—including other people—we \nhave no recourse than to examine them ‘from the outside’, \nobserving their behavior and making inferences about their \ncapacity for free will based on that.\n1 3\n4393\nAI and Ethics (2025) 5:4389–4400\nentity would behave: It sets itself targets, comes up with a \nplan to pursue those targets, and monitors the feedback it \ngets from the game to finetune its actions to accomplish its \ntargets. Both Spitenik and V oyager thus behave exactly as \none would expect them to behave if they were intentional, \ngoal-directed agents.\nHow about alternative possibilities, the second criterion? \nAs regards V oyager, different trials of the same program \nshow that it does not follow any rigid path but on differ -\nent occasions can come up with quite different pathways \ntoward developing its skills and acquiring new resources. \nThis is not only about the environment being slightly dif -\nferent on each trial (as Minecraft generates the environment \nanew for each trial) but also about the LLM coming to fol -\nlow different paths on different trials. Similar to ChatGPT \nproviding somewhat different answers to the same question \non separate trials, the V oyager comes up with slightly differ-\nent targets and pathways for itself on each separate trial, due \nto the in-built randomness of LLMs. Given the open-ended \nenvironment of Minecraft, there is no one thing the V oyager \ncan engage in but several possible paths open in front of it, \nand on different trials it can make different choices. Given \nthe ability of the V oyager to make choices that are relatively \nrational given its overall goal, it clearly has some form of \nhigher-level conceptual understanding of the situation it is \nin. With ‘conceptual understanding’ I mean that it is some -\nhow able to detect various higher-level patterns in its envi -\nronment and these higher-level patterns are somehow coded \nin how its various circuits are activated—analogous to how \nhuman concepts are coded in certain patterns of brain cell \nactivation. It is on this higher-level interpretive understand-\ning of its environment that it faces various possibilities \nbetween which it must make choices. To an observer, its way \nof reacting to the environment makes sense if one assumes it \nis able to make some higher-level interpretations of what is \nhappening in its environment, whether it has, for example, \n‘succeeded’ in a certain task. An outsider observing it must \nthus conclude that it seems to have alternatives and it “could \nhave done otherwise”, compared to the choice that it comes \nto take. By making different choices in similar situations on \ndifferent trials, V oyager thus behaves exactly as if it would \nmake genuine choices among alternative possibilities.\nSimilarly, there is no one straightforward path for the \nSpitenik, but it constantly faces bigger and smaller choices \non its way toward the target. When the time for sunset \napproaches, it must choose between various potential loca -\ntions where to spend the night, some of them closer, some \nof them further away, with much uncertainty about the exact \ncharacteristics of those options further away. Like humans \nin a similar situation, it must optimize several different \ncriteria in their selection—closeness, how protected the \nplace is, how likely it is that humans would visit it during \n3 Do LLMs have free will?\nNow, do V oyager and Spitenik have free will? The prepa -\nrational conceptual work having been made, we can finally \nanswer that question. Both build on the emergence of Large \nLanguage Models (LLMs) that have given computational \nsystems unprecedented capabilities for processing informa-\ntion. LLMs are built by training neural networks consist -\ning of billions of nodes with massive quantities of data to \nrecognize patterns in the training data (typically followed \nby reinforcement learning from human feedback to finetune \nthe model). The modern transformer learning architecture of \nsuch neural networks [32] has led them to be highly capable \nof predicting next words in a sequence—such as the likely \nanswer to a question provided by the human. This concep -\ntual way of reasoning using natural language has made them \nvery adapt in various tasks, exhibiting chain-of-thought rea-\nsoning and problem decomposition [8]. Moreover, the mod-\nels are typically programmed to not automatically pick the \nhighest probability next word but there is a degree of built-\nin randomness where the model picks one of the top alter -\nnatives, as this has been shown to increase the creativity of \nits responses [ 33]. Agentic LLM-models combine LLM as \nthe general ‘reasoning’ unit with units for memory storage, \nlong-term planning and action, along with various sensory \ncapabilities through which it can monitor the environment \nand the effects of its actions on that environment, making \nthe agents capable to plan, act, and self-correct their actions \nbased on the environmental feedback [8, 34].\nGiven List’s three criteria, to answer the question of \nwhether V oyager and Spitenik have free will, we need to \ndetermine whether they have intentionality, alternatives, \nand whether they make choices.\nStarting with intentionality, Spitenik certainly behaves \nlike a goal-directed system: It has a target it aims to accom-\nplish, and it monitors its environment through its various \nsensors to avoid obstacles and to identify a safe place to \nrest during the night. It thus has some encoded goals and \nan encoded map of its environment that it uses to guide its \nactions toward the goal. So, it behaves exactly as it would \nbehave if it had motivational states and representational \nstates guiding its action. It thus functions as an intentional \nagent would function. The same is true of the Kargu-2 \ndrones that were used to autonomously attack targets in \nLibya—they behaved as if they intended to kill certain sol -\ndiers. From a functional point of view, modern battle drones \nthus fulfill the condition of intentionality: They behave \nexactly as one would expect them to behave if they had \ncertain goals that they are pursuing while having a work -\ning understanding of the key physical features of the world \naround them. The same conclusion holds true also as regards \nV oyager: It behaves in Minecraft exactly as a goal-directed \n1 3\n4394\nAI and Ethics (2025) 5:4389–4400\nof choice. Accordingly, at the agency-level, the LLMs may \nface genuine alternative possibilities.\nFinally, the third criterion is about the capacity to control \none’s actions. V oyager, as noted, utilizes LLM to come up \nwith a target, and then translates that target into commands \nit gives to the Minecraft character. It is thus the encoded \nintentions and goals that cause it to behave in the way it \nbehaves. If we know those intentions, we can predict its \nbehavior with quite good accuracy (except in fork-in-a-road \nsituations with two relatively equally good options). So, the \ncausal chain that gives us the most power to understand and \npredict the behavior of the V oyager involves the intentions \nof the V oyager causing it to behave in ways that are in accor-\ndance with those intentions. Similarly, when the Spitenik \nis flying eastward and detects an obstacle—a mountain or \na skyscraper—it transitions to a route that goes around the \nobstacle. Here it seems to control its action. The mountain \nitself did not cause it to change course but this was caused \nby the drone detecting the mountain and reacting to it by \nchanging its course. To explain the behavior of Spitenik, \nthe causal pathway giving us the most explanatory power \ninvolves its intentions causing its behavior.\nAll in all, both Spitenik and V oyager thus behave as \nif they would have free will: They behave exactly as one \nwould predict something to behave, if that something would \nhave intentions and goals, would encounter alternative pos-\nsibilities, and would be making choices based on its inten -\ntions that would directly influence its behavior. From a \nfunctional point of view, we can thus conclude that the Spit-\nenik and the V oyager both have free will. They might not \nhave physical free will any more than humans would have, \nas on the physical level their behavior is reducible to elec -\ntricity flowing in circuits. However, looking at the physical \nlevel alone, we could not explain why Spitenik is constantly \nflying eastwards, and why every evening, approximately \nhalf an hour before sunset, it lands on some surface that is \nabove the ground. Its cognitive architecture is way too com-\nplex to make it possible to make any predictions about its \nbehavior based on how its circuits are connected. Postulat -\ning that it has intentions, such as having a target in Siberia \nit is pursuing, and the aim of finding a safe place during the \nnight, provides us an explanation of its behavior. In other \nwords, it behaves as if  it would have the target of flying \neastwards while safeguarding itself during the night. Simi -\nlarly, V oyager behaves exactly as if  it would behave if it \nwere pursuing the goals of exploring new things and acquir-\ning new skills in Minecraft. Thus, as long as we are talk -\ning about functional free will, both Spitenik and V oyager \nbehave exactly as if they would behave if they had free will.\nthe night—with no straightforward right answer available. \nThus, the different options are genuine until a selection is \nmade, and when two options are roughly equally good, an \noutsider cannot predict which option the Spitenik will pick. \nSimilarly, once closer to the target, its camera might iden -\ntify three humans further away in separate locations. It then \nmust select which of the humans to approach first, to get \nclose enough to engage in face detection. Again, all three \noptions are genuine possibilities. The Spitenik thus con -\nstantly faces forks in the road, where several options are \navailable, sometimes with clear best selections, sometimes \nwith several equally good options. Especially in the latter \nsituation, from an outside perspective, it behaves exactly as \nif it “could have done otherwise.”\nHere it is important to keep our analysis on the right level \nof explanation. List [17, p. 91], in arguing for humans having \nalternative possibilities, reminds that “physical and agential \nphenomena stand in a relation of supervenience with mul -\ntiple realizability.” What he means by this is that a person \nhaving certain types of beliefs and desires may be compat -\nible with different subvenient physical details. On a warm \nsummer day, you and I may both contemplate between two \nalternatives that we desire, vanilla and strawberry ice cream. \nThe way the desire for strawberry ice cream is realized on \nthe physical level of our brains may, however, be quite dif -\nferent, with different patterns of brain cell activation corre -\nsponding to strawberry ice cream desire in your brain and in \nmy brain. “Different physical configurations—most likely \nan inordinate number—can instantiate the same state of the \nworld as specified through the lens of psychology ” [17, p. \n91]. There are myriad ways in which desire for strawberry \nice cream could be coded on the brain cell level, but on a \npsychological level, all these physical configurations could \nlead to the same choice at the ice cream stand.\nThus, the alternate possibilities encountered at the psy -\nchological level do not correspond to any one specific path \non the physical level but instead many different physical \nconfigurations can lead to the same psychological choice sit-\nuations, making the alternate possibilities encountered at the \npsychological level into genuine alternatives at that level. \nWhat this aims to show is that physical-level determinism \nis compatible with genuine alternatives at the agency-level \n[for a more technical analysis, see 16]. Applied to the case \nof Large Language Models, on a physical level, the atoms \nand electric circuits they are realized on may be determinis-\ntic. But as their behavior is best explained on a higher level \nwhere they utilize their higher-level interpretation of the sit-\nuation to identify alternative courses of action, these higher-\nlevel interpretations of alternatives are multiply realizable \nwith a myriad of different physical-level configurations cor-\nresponding to the same higher-level encounter of a situation \n1 3\n4395\nAI and Ethics (2025) 5:4389–4400\nself is not bound by the past self’s choices, moral concerns, \ninstinctual preferences, or anything else. At the other end of \nthe spectrum is an agent who has fixed overall goals, fixed \nsubgoals, and fixed paths to reach those goals, but who is \nstill able to exercise on a very local level some free will in \nthe selection of current tactics.\nWe humans and the generative AI agents are both found \non that continuum. However, humans are not found at one \nend of the spectrum because Sartre greatly exaggerates \nhuman freedom. As regards life goals, humans are not born \ntabula rasa.  As biological creatures, we come equipped \nwith a number of physical and psychological needs related \nto survival that manifest themselves as strong motivations \n[36, 37]. Starvation overrides many other potential motives \nin pushing the human agent to seek food at the cost of other \ngoals. As evolution favors those who reproduce, we have \nstrong motives related to finding mating partners, engaging \nin sexual intercourse with them, and taking care of our off -\nspring [38]. As hominids are a social species, we also have \ncomplex social needs related to group acceptance, a sense \nof relatedness and belonging, and reciprocal close relation -\nships [39, 40]. Furthermore, as a cultural species, humans \nhave a strong tendency to internalize the goals and values \nof their relevant social group [40, 41]. All of this means that \nany ‘self-selected’ goals of the individual emerge from (and \nare to a large degree complex manifestations of) these bio -\nlogically and culturally programmed goals that they have. \nHumans thus don’t have control over the “entire causal \npre-history” of their goals and preferences [ 42, p. 15]. Our \ngoals, values, and desires are more predetermined than what \nwe typically want to admit or even realize. At the same time, \nthe human capacity for slow, language-mediated, reflec -\ntive thinking indeed gives us an unprecedented degree of \nfreedom as regards our goals, allowing us to escape mere \ninstinctual behavior [ 43]. We humans can override our \nphysical need for nutrition by engaging in a hunger strike \nfor a cause we believe in—in several reported cases starving \nto death. There is thus a degree of freedom in human over -\narching goals that is quite unprecedented compared to any \nother animal, making our functional free will more global \nthan the free will of any other animal. However, there are \nstill limits to this freedom. I cannot will myself to start tor -\nturing dogs or assassinating people wearing orange shirts \nin public parks as an overarching life goal. I could casu -\nally throw such ideas into a philosophical conversation as \nan argument. But in real life, they would go so much against \nall my natural instincts that I could never actually carry such \nacts out even if I wanted (and I could not bring myself to \nwant them either).\nThe difference between humans and AI is thus a mat -\nter of degree, humans are closer to the global free will end \nof the spectrum than current versions of the AI. However, \n4 Does AI have free will as regards its \noverarching purpose?\nThe key counterargument that many would have against the \nconclusion that Spitenik and V oyager have free will would \nfocus on a distinction between overarching goals and sub-\ngoals. While both seemingly make independent choices \nabout the more specific targets they have, being able to \nbreak them into step-by-step plans, it is the human who pro-\nvided them with the overarching goal: In the case of Spit -\nenik, the target in Siberia it is pursuing, and in the case of \nV oyager, the goal of discovering diverse things and acquir-\ning diverse skills. This could be taken as a crucial difference \nbetween human beings and current AI systems, as humans \nseemingly have the power to alter also their overarching \ngoals in life. Thus, while Spitenik operates autonomously \nonce it has been given an overarching goal—in this case \nto kill the rogue terrorist—it seems to have no freedom as \nregards what that ultimate goal is. During the long mission, \nthe Spitenik must generate many sub-goals autonomously, \nmaking several independent choices as regards where to rest \nduring the night, which humans to approach for face detec -\ntion, how to approach them safely, and when it is certain \nenough about the target to pull the trigger. Yet it seems to \nhave no way of overriding its overarching goal. Similarly, \nfor V oyager, the human user gives it its grand task. Thus, \nwhile it autonomously divides this into sub-goals, making \nselections among various options, and coming up with new \nsub-tasks along the way, it cannot overrule or change this \noverarching goal that puts it in motion.\nHumans, in contrast, don’t have such a clear overarch -\ning purpose. Instead, we seem to be capable of endorsing \ncertain goals but also revising those goals, inventing new \nmissions for ourselves, and having inner battles between \nvarious independently important goals. We could thus argue \nthat while AI has ‘local’ free will, humans have ‘global’ free \nwill. Local free will means the freedom to come up with \nsub-goals, consider alternatives, and make choices within \nthe constraints of an overarching goal. Global free will \nrequires that one also has freedom as regards what one’s \noverarching goals are.\nWhile this is a relevant difference, we should not think \nabout the distinction between global and local as a dichot -\nomy but as a continuum. At the one end of the spectrum is \na totally free agent that has no constraints on the overarch -\ning goals it may choose. This is the Sartrean free individual \nfor whom “freedom is existence”, and “existence precedes \nessence” [35, p. 567]. One day the individual might decide \nto cure cancer, the next day it decides to kidnap and torture \ncats and dogs found in the neighborhood, the third day it \ndecides to line up torn newspapers in neat rows. Any mis -\nsion is equally possible in global free will, and the current \n1 3\n4396\nAI and Ethics (2025) 5:4389–4400\nmore than we can imagine how life feels like for a bat [cf. \n31]. I would be inclined to think that they do not possess \nphenomenological consciousness where life feels something \nfrom the inside. But speculating about the consciousness of \nAI agents is another story, which would take us to discus -\nsions about the basic building blocks of human conscious -\nness [45] and perhaps also the integrated information theory \n[46]. Consciousness is not necessary for functional free will \nas [42] also argues. When explaining the behavior of gen -\nerative LLM agents, we come to postulate that it has ‘goals’ \nand ‘intentions’ that it pursues through making ‘choices.’ \nFrom a functional, explanatory point of view, it does not \nmake a difference whether it behaves as if it has goals or \nwhether it phenomenologically feels drawn towards such \ngoals. Thus, discussing the consciousness of AI agents is a \ndifferent story not relevant to the present discussion.\nMoreover, the fact that LLM agents are capable of pur -\nsuing goals by making choices does not preclude the pos -\nsibility that they make the wrong choices. In fact, having \nagency entails the possibility of making mistakes, as having \ngoals and choices is what makes it possible to make wrong \nchoices [47]. The fact that LLMs suffer from sometimes hal-\nlucinating and making mistakes thus does not diminish their \nability to make choices but actually highlights it.\nWhat are the implications of certain types of AI possess-\ning functional free will? Some might take this as a counter-\nargument against List’s theory of free will and a functional \napproach to the whole question. If AI can possess functional \nfree will, then this can’t be a relevant form of free will. \nWhile this easily sounds like speciesism, where the aim is to \nretain the special moral status of humans by keeping other \nanimals and AIs at arm’s length, I would say that the (mor -\nally) relevant differences between humans and currently \nexisting types of AI are found in other features, such as the \nability to feel pain, be conscious and care about one’s own \nexistence, and the ability to feel empathy. Whether a certain \ntype of AI be recognized as a person with certain rights and \nmoral status is thus a debate mainly settled based on other \nfeatures than whether it has functional free will [for more on \nthe topic, see e.g., 48, 49].\nA more interesting implication is whether the AI can be \nheld morally responsible for their choices if they possess \nfunctional free will. The capacity to freely choose one’s acts \nis often seen as a key prerequisite for moral agency [ 50]. \nIn this spirit, Laukyte [ 51] has used List and Pettit’s [ 52] \naccount of agency to argue that certain types of AI could \nfulfill the conditions for artificial agents and thus could \nbe ascribed certain rights and duties. The more capable of \nindependent choices the AI agents become, the more the \nquestion arises about their own moral responsibilities. How-\never, currently, the moral responsibility for their behavior \nis typically seen to belong to their developers, as seen, for \nthis difference is likely to get narrower in the near future. \nIt is easy to imagine drones equipped with AI that would \nbe given very broad goals such as ‘try to survive’, ‘explore \nthe world’, ‘try to learn new skills’, or ‘make much money.’ \nSo, while the functional free will of humans is more global \nthan the functional free will of current generative AI agents, \nthe latter can have quite broad overarching goals that allow \nmuch variation in the more specific, yet still relatively broad \ngoals. Also, the difference between two initially similar AI \nagents could become very significant, given different ini -\ntial choices and subsequent life trajectories, even if they \nwould start with the same broad overarching goal such as \n‘try to survive and explore.’ A difference thus exists between \nhumans and generative AI agents, but it is a matter of degree, \nand not as large as one would initially assume.\nFurthermore, from the point of view of explaining the \ngenerative agent’s behavior, knowing the overarching, \noutside-given goal will provide some limited explanatory \npower. But to make more predictive assumptions about its \nbehavior requires us to postulate various more specific goals \nto the agent, which it has ‘chosen’ oneself, within the limit \nof the overarching goal. Thus, we still need to refer to the \nself-chosen goals of such agents to explain their behavior.\n5 Conclusion\nDoes AI possess free will? This depends on how we define \n‘AI’ and ‘free will.’ In this article, I have focused on gen-\nerative agents utilizing large language models that possess \noverarching goals as well as memory, sensory, and plan -\nning units that allow them to navigate in their environ -\nment towards their goals while breaking down their overall \npurpose into various sub-goals based on their memory and \nplanning capacity. As regards free will, I have focused on \nfunctional free will—an entity being best explained and its \nbehavior best predicted by postulating it as having inten -\ntions, alternatives, and the ability to make choices among \nthese alternatives based on its intentions. At a low enough \nlevel of explanation human action is reduced to neurobio -\nlogical processes and AI action to processes of electric cir -\ncuits [ 30, 44], and on that level we are not prone to find \nwhat I called physical free will  in humans nor in AIs. But \nwhen we narrow our question to functional free will, I have \nargued that generative LLM agents indeed have functional \nfree will. In trying to make sense of their behavior, our \nmost predictive models involve postulating that they make \nchoices among alternative paths based on their goals and \nintentions.\nNote that what I have not been taking a stance on is \nwhether generative LLM agents are conscious. We don’t \nknow how life feels like for a generative LLM agent—any \n1 3\n4397\nAI and Ethics (2025) 5:4389–4400\nNo funding was received for conducting this study.\nData availability No datasets were generated or analysed during the \ncurrent study.\nDeclarations\nConflict of interest The authors declare no competing interests.\nOpen Access   This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  v e c  o m m o  n s .  o \nr g  / l i c e n s e s / b y / 4 . 0 /.\nReferences\n1. Orrù, G., Piarulli, A., Conversano, C., Gemignani, A.: Human-\nlike problem-solving abilities in large language models using \nChatGPT. Front. Artif. Intell. 6(1199350), 1–13 (2023)\n2. Hagendorff, T., Fabi, S., Kosinski, M.: Human-like intuitive \nbehavior and reasoning biases emerged in large language mod -\nels but disappeared in ChatGPT. Nat. Comput. Sci. 3, 833–838 \n(2023).  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 3 8  / s 4  3 5 8 8 - 0 2 3 - 0 0 5 2 7 - x\n3. Park, J.S., O’Brien, J., Cai, C.J., Morris, M.R., Liang, P., Bern -\nstein, M.S.: Generative agents: interactive simulacra of human \nbehaviour. In: Proceedings of the 36th Annual ACM Symposium \non User Interface Software and Technology, San Francisco CA \nUSA: ACM, Oct. 2023, pp. 1–22.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 4 5  / 3 5  8 6 1 8 \n3 . 3 6 0 6 7 6 3\n4. Pezzulo, G., Parr, T., Cisek, P., Clark, A., Friston, K.: Generating \nmeaning: active inference and the scope and limits of passive AI. \nTrends Cogn. Sci. 28(2), 97–112 (2024).  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 1 6  / \nj .  t i c s . 2 0 2 3 . 1 0 . 0 0 2\n5. Hulman, A., et al.: ChatGPT- versus human-generated answers to \nfrequently asked questions about diabetes: a Turing test-inspired \nsurvey among employees of a Danish diabetes center. PLoS ONE \n18(8), e0290773 (2023).  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  3 7 1  / j o  u r n a l . p o n e . 0 2 \n9 0 7 7 3\n6. Nov, O., Singh, N., Mann, D.: Putting ChatGPT’s medical advice \nto the (turing) test: survey study. JMIR Med. Educ. 9(1), e46939 \n(2023). https://doi.org/10.2196/46939\n7. Girotra, K., Meincke, L., Terwiesch, C., Ulrich, K.T.: Ideas are \ndimes a dozen: large language models for idea generation in inno-\nvation. Available SSRN 4526071, 2023, Accessed: Dec. 19, 2023. \n[Online]. Available:  h t t p  s : /  / p a p  e r  s . s  s r n .  c o m  / s o  l 3 /  p a p  e r s .  c f  m ? a  b s t \nr  a c t  _ i d  = 4 5 2 6 0 7 1\n8. Xi, Z., et al.: The rise and potential of large language model based \nagents: a survey. Sci. China Inf. Sci. 68(2), 121101 (2025)\n9. Wang, G., et al.: V oyager: an open-ended embodied agent with \nlarge language models. arXiv: arXiv:2305.16291. Accessed: Jan. \n15, 2024. [Online]. Available:  h t t p  : / /  a r x i  v .  o r g  / a b s  / 2 3  0 5 .  1 6 2 9 1 \n(2023)\nexample, in a recent case of Air Canada having to pay a \ncustomer for the misleading information its chatbot gave \n[53]. While capacity to freely choose one’s acts can be seen \nas a necessary prerequisite for moral agency, another key \nprerequisite is the capacity to know and be able to deliber -\nate about moral concerns [ 44, 50]. To apply human moral \nstandards to the AI agents, we first would need to teach them \nhuman moral standards. Human morality arises as a com -\nbination of certain evolutionarily developed dispositions \nand our upbringing where various cultural norms are trans -\nmitted to us [ 54]. A child that, due to some brain damage, \nwould completely lack the relevant empathetic dispositions, \nand would be raised without having been taught any moral \nprinciples, would lack the relevant knowledge to make mor-\nally right choices. The same holds true for AI agents. At the \npresent stage, the moral responsibility is mainly in the hands \nof their developers, who are responsible for imagining what \nmorally consequential behavior the AI Agents are capable \nof, and then making sure to teach them the necessary con -\nstraints on their behavior, to ensure they don’t engage in \nmorally harmful behavior [ 55]. It is the responsibility of \nthe developers to ensure that the AI agent is aware of and \nconstrained by the relevant moral standards, and when this \nawareness of moral standards and deliberation about them \nbecomes sophisticated enough, we might be able to treat AI \nagents as morally responsible. Of course, this is a topic that \nagain would need its own discussion, for example about the \nrole of consciousness in moral agency, see e.g [ 48, 50, 55, \n56]. In general, I would argue that functional free will is a \nnecessary but not a sufficient condition to make AI agents \nmorally responsible for their choices.\nAll in all, conscious and reflective adult humans being \nour prototypes for morally responsible agents, we some -\ntimes have a hard time disentangling the various properties \nfrom which full human agency emerges. One of these prop-\nerties is functional free will—a human lacking that would \nnot be considered a fully human agent. While it is amazing \nthat we have now been able to develop versions of AI that \nfeature so advanced agentic capabilities that it is possible \nto argue that they possess functional free will, having this \nfeature alone does not make AI agents human-like. Hav -\ning consciousness and having capabilities to understand, be \nmotivated, and adhere to moral principles—among other \nproperties—are separate capabilities that current versions of \nAI still lack. Thus, there are still other building blocks that \nseparate humans and AI from each other. Future will show \nwhen and how these other building blocks will be imple -\nmented in new AI agents, to make their agency—and their \nmoral responsibility—even more like humans.\nAuthor contributions F.M. conceptualized and wrote the whole article.\nFunding Open Access funding provided by Aalto University.\n1 3\n4398\nAI and Ethics (2025) 5:4389–4400\n31. Nagel, T.: What is it like to be a bat? Philos. Rev. 83(4), 435–450 \n(1974)\n32. Vaswani, A., et al.: Attention is all you need. Adv. Neural. Inf. \nProcess. Syst. 30, 20750–20762 (2017)\n33. Wolfram, S.: What Is ChatGPT Doing:…and Why Does It Work? \nWolfram Media, Champaign (2023)\n34. Wang, L., et al.: A survey on large language model based autono-\nmous agents. Front. Comput. Sci. 18(186345), 1–26 (2024).  h t t p  s \n: /  / d o i  . o  r g /  1 0 . 1  0 0 7  / s 1  1 7 0 4 - 0 2 4 - 4 0 2 3 1 - 1\n35. Sartre, J.-P.: Being and Nothingness. Washington Square Press, \nNew York (1956)\n36. Maslow, A.H.: A theory of human motivation. Psychol. Rev. \n50(4), 370–396 (1943)\n37. Ryan, R.M., Deci, E.L.: Self-Determination Theory: Basic Psy -\nchological Needs in Motivation, Development, and Wellness. \nGuilford Press, New York (2017)\n38. Kenrick, D.T., Griskevicius, V ., Neuberg, S.L., Schaller, M.: Ren-\novating the pyramid of needs contemporary extensions built upon \nancient foundations. Perspect. Psychol. Sci. 5(3), 292–314 (2010)\n39. Baumeister, R.F., Leary, M.R.: The need to belong: desire for \ninterpersonal attachments as a fundamental human motivation. \nPsychol. Bull. 117(3), 497–529 (1995)\n40. Deci, E.L., Ryan, R.M.: The\" what\" and\" why\" of goal pursuits: \nHuman needs and the self-determination of behavior. Psychol. \nInq. 11(4), 227–268 (2000)\n41. Bond, R., Smith, P.B.: Culture and conformity: a meta-analysis of \nstudies using Asch’s (1952b, 1956) line judgment task. Psychol. \nBull. 119(1), 111–137 (1996)\n42. List, C.: Do group agents have free will? Inquiry 68(3), 1021–\n1048 (2025).  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 8 0  / 0 0  2 0 1  7 4 X  . 2 0 2  3 .  2 2 1 8 7 2 1\n43. Stanovich, K.E.: The Robot’s Rebellion: Finding Meaning in the \nAge of Darwin. University of Chicago press, Chicago (2005)\n44. Mabaso, B.A.: Computationally rational agents can be moral \nagents. Ethics Inf. Technol. 23(2), 137–145 (2021).  h t t p  s : /  / d o i  . o  r \ng /  1 0 . 1  0 0 7  / s 1  0 6 7 6 - 0 2 0 - 0 9 5 2 7 - 1\n45. Veit, W.: The origins of consciousness or the war of the five \ndimensions. Biol. Theory 17(4), 276–291 (2022).  h t t p  s : /  / d o i  . o  r g /  \n1 0 . 1  0 0 7  / s 1  3 7 5 2 - 0 2 2 - 0 0 4 0 8 - y\n46. Tononi, G., Boly, M., Massimini, M., Koch, C.: Integrated infor-\nmation theory: from consciousness to its physical substrate. Nat. \nRev. Neurosci. 17(7), 450–461 (2016).  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 3 8  / n r  \nn . 2 0 1 6 . 4 4\n47. Amaya, S.: Agency and mistakes. In: Ferrero, L. (Ed.) The Rout-\nledge Handbook of Philosophy of Agency. Routledge, pp. 149–\n158 (2022).  h t t p  s : /  / d o i  . o  r g /  1 0 . 4  3 2 4  / 9 7  8 0 4  2 9 2  0 2 1 3  1 -  1 7 /  a g e n  c y \n-  m i s  t a k e s - s a n t i a g o - a m a y a\n48. List, C.: Group agency and artificial intelligence. Philos. Technol. \n34(4), 1213–1242 (2021).  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 0 7  / s 1  3 3 4 7 - 0 2 1 - 0 0 \n4 5 4 - 7\n49. Müller, V .C.: Is it time for robot rights? Moral status in artificial \nentities. Ethics Inf. Technol. 23(4), 579–587 (2021).  h t t p  s : /  / d o i  . o  \nr g /  1 0 . 1  0 0 7  / s 1  0 6 7 6 - 0 2 1 - 0 9 5 9 6 - w\n50. Himma, K.E.: Artificial agency, consciousness, and the criteria \nfor moral agency: what properties must an artificial agent have to \nbe a moral agent? Ethics Inf. Technol. 11(1), 19–29 (2009).  h t t p  s \n: /  / d o i  . o  r g /  1 0 . 1  0 0 7  / s 1  0 6 7 6 - 0 0 8 - 9 1 6 7 - 5\n51. Laukyte, M.: Artificial agents among us: Should we recognize \nthem as agents proper? Ethics Inf. Technol. 19, 1–17 (2017)\n52. List, C., Pettit, P.: Group Agency: The Possibility, Design, and \nStatus of Corporate Agents. Oxford University Press, Oxford, \nNew York (2013)\n53. BBC: Airline held liable for its chatbot giving passenger bad \nadvice - what this means for travellers. Accessed: Feb. 26, 2024. \n[Online]. Available:  h t t p s :   /  / w w  w . b  b  c  . c  o  m / t r  a v   e l /  a r t  i  c l  e / 2  0 2 4  0  2 2  \n2  - a  i r  - c a   n a d  a - c h   a t  b o t  - m i  s i n f  o r  m  a t  i o  n  - w h  a t -  t r a v  e l l e r s  - s h o u l d - k n o \nw\n10. Tian, Y ., et al.: UA Vs Meet LLMs: overviews and perspectives \ntoward agentic low-altitude mobility. arXiv:  h t t p  s : /  / d o i  . o  r g /  1 0 . 4  8 \n5 5  0 / a  r X i v . 2 5 0 1 . 0 2 3 4 1 (2025)\n11. Javaid, S., Fahim, H., He, B., Saeed, N.: Large language models \nfor UA Vs: current state and pathways to the future. IEEE Open J. \nVeh. Technol. 5, 1166–1192 (2024).  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 0 9  / O J  V \nT . 2 0 2 4 . 3 4 4 6 7 9 9\n12. O’Connor, T., Franklin, C.: Free Will. In: The Stanford Encyclo-\npedia of Philosophy, Winter 2022., E. N. Zalta and U. Nodelman, \nEds., Metaphysics Research Lab, Stanford University, 2022. \nAccessed: Jan. 10, 2024. [Online]. Available:  h t t p  s : /  / p l a  t o  . s t  a n f o  r \nd .  e d u  / a r  c h i  v e s /  w i  n 2 0  2 2 / e  n t r  i e s  / f r e e w i l l /\n13. Van Inwagen, P.: The incompatibility of free will and determin -\nism. Philos. Stud. 27(3), 185–199 (1975)\n14. Dennett, D.C.: Elbow Room: The Varieties of Free Will Worth \nWanting. The MIT Press, Cambridge (1984)\n15. Kane, R.: Introduction: the contours of contemporary free-will \ndebates (part 2). In: Kane, R. (ed.) The Oxford Handbook of Free \nWill, pp. 2–35. Oxford University Press (2011).  h t t p  s : /  / d o i  . o  r g /  1 0 \n. 1  0 9 3  / o x  f o r  d h b  / 9 7 8  0 1  9 5 3 9 9 6 9 1 . 0 0 3 . 0 0 0 1\n16. List, C.: Free will, determinism, and the possibility of doing oth-\nerwise. Noûs 48(1), 156–178 (2014).  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 1 1  / n o  u \ns . 1 2 0 1 9\n17. List, C.: Why Free Will is Real. Harvard University Press, Cam -\nbridge (2019)\n18. Dennett, D.C.: Intentional systems theory. In: Beckermann, A., \nMcLaughlin, B.P., Walter, S. (eds.) The Oxford Handbook of \nPhilosophy of Mind, pp. 339–350. Oxford University Press, New \nYork (2009)\n19. Hernandez, J.: A military drone with a mind of its own was used \nin combat, U.N. Says. NPR, Jun. 01, 2021. Accessed: Dec. 15, \n2023. [Online]. Available:  h t t p  s : /  / w w w  . n  p r .  o r g /  2 0 2  1 / 0  6 / 0  1 / 1  0 0 2 \n1  9 6  2 4 5  / a - u  - n -  r e p  o r t  - s u  g g e s  t s  - l i  b y a -  s a w  - t h  e - fi   r s  t - b a  t t  l e fi   e l d  - k i  l l i  \nn g - b y - a n - a u t o n o m o u s - d\n20. United Nations Security Council: Final report of the Panel of \nExperts on Libya established pursuant to Security Council reso -\nlution 1973 (2011). United Nations (2021)\n21. IAI: Fire and forget: Harpy is an autonomous weapon for all \nweather. IAI. Accessed: Dec. 16, 2023. [Online]. Available: \nhttps://www.iai.co.il/p/harpy\n22. Hirsh, M.: How AI will revolutionize warfare. Foreign Policy, \nDec. 18, 2023. Accessed. [Online]. Available:  h t t p  s : /  / f o r  e i  g n p  o l i \nc  y . c  o m /  2 0 2  3 / 0  4 / 1 1  / a  i - a  r m s -  r a c  e - a  r t i  fi  c  i a l -  i n  t e l  l i g e  n c e  - c h  a t g p t - m \ni l i t a r y - t e c h n o l o g y / (2023)\n23. Layton, P.: US military plans to unleash thousands of autonomous \nwar robots over next two years. The Conversation, Aug. 30, 2023. \n[Online]. Available:  h t t p  : / /  t h e c  o n  v e r  s a t i  o n .  c o m  / u s  - m i  l i t a  r y  - p l  a n s \n-  t o -  u n l  e a s  h - t  h o u s  a n  d s -  o f - a  u t o  n o m  o u s  - w a  r - r o  b o  t s -  o v e r  - n e  x t -  t w \no - y e a r s - 2 1 2 4 4 4 (2023)\n24. Lowe, E.J.: Causal closure principles and emergentism. Philoso -\nphy 75(4), 571–585 (2000)\n25. Papineau, D.: Physicalism and the human sciences. In: Mantza -\nvinos, C. (ed.) Philosophy of the Social Sciences: Philosophical \nTheory and Scientific Practice, pp. 103–123. Cambridge Univer -\nsity Press (2009).  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 1 7  / C B  O 9 7 8 0 5 1 1 8 1 2 8 8 0 . 0 \n1 0\n26. Gibb, S.: The causal closure principle. Philos. Q. 65(261), 626–\n647 (2015).  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 9 3  / p q  / p q v 0 3 0\n27. Dennett, D.C.: Intentional systems. J. Philos. 68(4), 87–106 \n(1971)\n28. Kim, J.: The layered model: metaphysical considerations. Philos. \nExplor. 5(1), 2–20 (2002)\n29. Oppenheim, P., Putnam, H.: Unity of science as a working \nhypothesis. Minn. Stud. Philos. Sci. 2, 3–36 (1958)\n30. List, C.: Levels: descriptive, explanatory, and ontological. Noûs \n53(4), 852–883 (2019)\n1 3\n4399\nAI and Ethics (2025) 5:4389–4400\nPublisher’s note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional affiliations.\n54. Kitcher, P.: The Ethical Project. Harvard University Press, Cam -\nbridge (2011)\n55. Hakli, R., Mäkelä, P.: Moral responsibility of robots and hybrid \nagents. Monist 102(2), 259–275 (2019)\n56. Fisher, M., List, C., Slavkovik, M., Winfield, A.: Engineering \nmoral machines. Inform. Spektrum 39(6), 467–472 (2016)\n1 3\n4400",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.6943346261978149
    },
    {
      "name": "Computer science",
      "score": 0.5168700218200684
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3721294403076172
    },
    {
      "name": "Cognitive science",
      "score": 0.3517161011695862
    },
    {
      "name": "Psychology",
      "score": 0.188914954662323
    }
  ],
  "institutions": []
}