{
    "title": "SIGNFORMER: DeepVision Transformer for Sign Language Recognition",
    "url": "https://openalex.org/W4313855689",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4316649220",
            "name": "Deep R. Kothadiya",
            "affiliations": [
                "Charotar University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2158671929",
            "name": "Chintan M Bhatt",
            "affiliations": [
                "Pandit Deendayal Petroleum University"
            ]
        },
        {
            "id": "https://openalex.org/A2249215634",
            "name": "Tanzila Saba",
            "affiliations": [
                "Prince Sultan University"
            ]
        },
        {
            "id": "https://openalex.org/A2305127621",
            "name": "Amjad Rehman",
            "affiliations": [
                "Prince Sultan University"
            ]
        },
        {
            "id": "https://openalex.org/A2804438199",
            "name": "Saeed Ali Bahaj",
            "affiliations": [
                "Prince Sattam Bin Abdulaziz University",
                "University of Business and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2015058588",
        "https://openalex.org/W2062136142",
        "https://openalex.org/W4281710609",
        "https://openalex.org/W2737899662",
        "https://openalex.org/W4224054210",
        "https://openalex.org/W2899058768",
        "https://openalex.org/W3182659197",
        "https://openalex.org/W4225160998",
        "https://openalex.org/W4285685564",
        "https://openalex.org/W3028504508",
        "https://openalex.org/W3129986654",
        "https://openalex.org/W6676925370",
        "https://openalex.org/W3184215204",
        "https://openalex.org/W3198787543",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W6810023147",
        "https://openalex.org/W4297851380",
        "https://openalex.org/W2970146327",
        "https://openalex.org/W1506441995",
        "https://openalex.org/W1548002489",
        "https://openalex.org/W581810680",
        "https://openalex.org/W2113819043",
        "https://openalex.org/W2500095286",
        "https://openalex.org/W3187112921"
    ],
    "abstract": "Sign language is the most common form of communication for the hearing impaired. To bridge the communication gap with such impaired people, a normal people should be able to recognize the signs. Therefore, it is necessary to introduce a sign language recognition system to assist such impaired people. This paper proposes the Transformer Encoder as a useful tool for sign language recognition. For the recognition of static Indian signs, the authors have implemented a vision transformer. To recognize static Indian sign language, proposed methodology archives noticeable performance over other state-of-the-art convolution architecture. The suggested methodology divides the sign into a series of positional embedding patches, which are then sent to a transformer block with four self-attention layers and a multilayer perceptron network. Experimental results show satisfactory identification of gestures under various augmentation methods. Moreover, the proposed approach only requires a very small number of training epochs to achieve 99.29 percent accuracy.",
    "full_text": "Received 23 October 2022, accepted 10 December 2022, date of publication 9 January 2023, date of current version 18 January 2023.\nDigital Object Identifier 10.1 109/ACCESS.2022.3231 130\nSIGNFORMER: DeepVision Transformer\nfor Sign Language Recognition\nDEEP R. KOTHADIYA\n 1, CHINTAN M. BHATT\n2, TANZILA SABA\n 3, (Senior Member, IEEE),\nAMJAD REHMAN\n 3, (Senior Member, IEEE), AND SAEED ALI BAHAJ4\n1U & P U Patel Department of Computer Engineering, Chandubhai S. Patel Institute of Technology (CSPIT), Faculty of Technology (FTE), Charotar University\nof Science and Technology (CHARUSAT), Changa 388421, India\n2Department of Computer Science and Engineering, School of Engineering and Technology, Pandit Deendayal Energy University, Gandhinagar,\nGujarat 382007, India\n3Artiﬁcial Intelligence and Data Analytics Lab (AIDA), CCIS, Prince Sultan University, Riyadh 11586, Saudi Arabia\n4MIS Department, College of Business Administration, Prince Sattam bin Abdulaziz University, Al-Kharj 11942, Saudi Arabia\nCorresponding authors: Deep R. Kothadiya (deepkothadiya.ce@charusat.ac.in) and Tanzila Saba (tsaba@psu.edu.sa)\nThis work was supported by the Artiﬁcial Intelligence and Data Analytics Lab (AIDA). College of Computer and Information Sciences\n(CCIS), Prince Sultan University, Riyadh, Saudi Arabia. The authors would also like to acknowledge the support of Prince Sultan\nUniversity for paying the Article Processing Charges (APC) of this publication.\nABSTRACT Sign language is the most common form of communication for the hearing impaired. To bridge\nthe communication gap with such impaired people, a normal people should be able to recognize the signs.\nTherefore, it is necessary to introduce a sign language recognition system to assist such impaired people. This\npaper proposes the Transformer Encoder as a useful tool for sign language recognition. For the recognition\nof static Indian signs, the authors have implemented a vision transformer. To recognize static Indian sign\nlanguage, proposed methodology archives noticeable performance over other state-of-the-art convolution\narchitecture. The suggested methodology divides the sign into a series of positional embedding patches,\nwhich are then sent to a transformer block with four self-attention layers and a multilayer perceptron\nnetwork. Experimental results show satisfactory identiﬁcation of gestures under various augmentation\nmethods. Moreover, the proposed approach only requires a very small number of training epochs to achieve\n99.29 percent accuracy.\nINDEX TERMSTransformer, deep learning, sign language, multi head attention, healthcare, mental health.\nI. INTRODUCTION\nA communication medium consists of hand gestures and\nthe most structured and organized language to effectively\ncommunicate for impaired people. Sign language is a col-\nlection of various gesture-generation techniques. Sign lan-\nguage is a more effective method of communication than\nleap moment identiﬁcation or writing a message. Sign lan-\nguage is vast and consists entirely of gestures to properly\ncomprehend messages. Sign language is not just a gesture\nusing ﬁngers and palms; it involves visual cues through the\neyes, face, mouth, eyebrows, etc. Additional components, like\nfacial expressions, involve expressing the complex meaning.\nNormal verbal language is much more creative and cultivated\nthan normal verbal language. The artistic spirit of life is given\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Chuan Li.\nby the hand moment, body, and facial expression. Although\nthe sign language can be simple and professional, it can also\nbe an animated way to communicate, even though the sign\nlanguage is very formal.\nSign language recognition is an area of research that\ninvolves pattern matching, deep learning, computer vision,\nnatural language processing, and a design module or algo-\nrithm to identify sign language. It can be extended further to\nhuman-computer interaction without a voice interface. This\nsystem belongs to multidisciplinary content and the approach\ncan be considered as a part of the Sign Language System.\nThere are around 300 sign languages used around the\nworld [1]. The numbers don’t have some level of conﬁdence\nbecause day by day some countries immerge with their own\nsign language. American, British, and Chinese sign languages\nare the most widely used worldwide [1]. There is a very huge\ndiversiﬁcation in sign language, which varies from region\n4730 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 11, 2023\nD. R. Kothadiya et al.: SIGNFORMER: DeepVision Transformer for Sign Language Recognition\nFIGURE 1. Formal classification of sign language.\nto region. It involves several local or conventional subsets of\nthe language. The diversity may lead to different ﬂows of ges-\nture singing, expression, jargon and the formation of gestures.\nAs certain English words are spoken differently in different\nparts of one country, different accents and distinct dialects\nmay be present in sign language. Indian sign language is the\nmost widely used language in the South Asian regions [2].\nSign language is classiﬁed based on static and dynamic\nor involving manual and non-manual body parts. This clas-\nsiﬁcation can be helpful to researchers and designers of sign\nlanguage recognition systems. We must combine both sign\ncomponents to design a robust or real-time sign language\nrecognition system. As shown in ﬁgure 1, sign language can\nbe divided into two basic categories: static and dynamic.\nStatic signs can be generated using one hand or two hands,\nwhile dynamic signs are further divided into two sub-\ncategories as isolated and continuous. To include emotional\nsubstances, dynamic signs can be further divided by the\ninvolvement of non-manual body parts. Non-manual body\nparts can be eyes, head movements, leaps, and eyebrows [3].\nThe main contributions of the work as, i) Proposed\nTransformer-based DeepVisionTransformer to recognize\nIndian sign language. ii) Evaluate the proposed model with a\nvery small number of learning cycles (5 epochs), which is tiny\ncompared to other state-of-the-art sign recognition models\nThe rest of the article organized as section II contains liter-\nature study relevant to sign language recognition. Section III\nincludes a detailed description of Vision Transformer and\nVision Transformer for image recognition. Section IV con-\ntains details of the proposed methodology. Section V contains\ndetails of the experiment and result. Section VI present dis-\ncussion and conclusion of proposed work.\nII. RELATED WORK\nRokade [4] proposed a methodology for automatically recog-\nnising ﬁngerspelling in the Indian sign language. Input the\nsign image ﬁrst to perform segmentation based on skin colour\nto detect the sign’s shape. The detected area is converted\ninto a binary image. Furthermore, a Euclidian distant trans-\nformation is applied to the binary image. After the feature\nextraction using Hu’s moment, classiﬁcation is done with\nANN and SVM. The accuracy was 94.37% with SVM and\n92.12% with ANN over 13 features [4]. The author has found\ngood accuracy with ANN even with a smaller number of\nfeatures set. The author has used a black background image of\nthe letter (26class) with dimensions of 320*240.Video-based\nIndian signs are used to recognize them by the proposed sys-\ntem. Katoch et al. [5] present a technique that uses the ‘‘Bag\nof Visual Words’’ model (BOVW) to recognize Indian sign\nlanguage letters and digits. The proposed methodology uses\nsegmentation based on skin color and background subtrac-\ntion. The authors used histogram-based sign mapping. At the\nend, CNN and SVM were used for classiﬁcation. The author\nhas also developed a GUI to make access easier. The author\nhas used a custom dataset of more than 36,000 images to rec-\nognize Indian sign language. Over the dataset mask generates\nbinary and canny edges to extract the feature with SURF. The\nproposed methodology with SVM and CNN found 99.17%\nand 99.64% accuracy, respectively [5]. Shenoy et al. [6] pro-\nposed a static hand pose for Indian sign language recognition.\nVideo frames are captured from a smartphone and transmitted\nto a server for processing. The author has used skin color\nsegmentation for hand detection and tracking. Feature extrac-\ntion uses a grid base technique to represent hand gestures in\na feature vector. The author has used KNN for static hand\npose (alphabet and number) classiﬁcation, while HMM was\nused to classify other gestures of Indian sign language with\nan accuracy of 99.7% and 97.23%, respectively [6]. The\nauthor used a custom dataset of over 24624 images for the\nexperiment. De Coster et al. [7] proposed a sign language\nrecognition methodology over the Flemish Sign Language\ncorpus. The author has used OpenPose feature extraction\nand end-to-end learning with CNN, and applied a multi-head\nattention approach to isolated sign recognition. Over the class\nof 100 signs, 74.7% accuracy has been obtained as a state-of-\nthe-art result over the Flemish Sign Language Corpus. The\nauthor introduces the Multimodal Transformer Network with\nPose LSTM and Pose Transformer, especially self-attention\nfor sign language recognition [7]. Mannan et al. [8] proposed\nHypertuned DeepCNN for American Static sign language,\nauthor has used data augmentation to create more number\nof learning data sample, as deep learning model accuracy\nwill increase with more samples for the training process.\nThe proposed architecture follows conventional CNN with\ntuned hyper that parameters able to achieve 99.67% accu-\nracy with 20 epochs. Zakariah et al. [9] proposed CNN\nbased architecture for Arabic letter sign recognition. The\nauthors generated 160000 images from 32000 images with\ndata augmentation, which helps to consider different bright-\nness and angular scenario. EfﬁcientNetB4 method used for\nsimulation. The authors also modiﬁed existing EfﬁcientNet\nby adding one fully connected dense layer Author used a\nstandard ArSL2018 dataset with 32 classes and get 95% of\naccuracy in 30 epochs. Kamruzzaman [10] proposed CNN\nbased method for Sign language detection. Authors have pro-\nposed ResNet50 and MobileNetV2 based methodology for\nVOLUME 11, 2023 4731\nD. R. Kothadiya et al.: SIGNFORMER: DeepVision Transformer for Sign Language Recognition\nTABLE 1. Comparative analysis of training phase prospective with\nstate-of-the-art results in static sign recognition.\nArabic sign language recognition. ResNet50 and Mobile-\nNetV2 simulated separately on 32 classes of ArSL2018\ndataset. A combination of ResNet50 and MobileNetV2 can\nable achieve 98.2% accuracy with 10 epochs. Rathi et al. [11]\nproposed deep learning based sign language recognition\nmodel. Authors have used 2-level ResNet50 architecture to\nrecognize sign language, authors have used 36 classes of\nthe American Sign Language dataset form Massey univer-\nsity [12] having approx. 70 RGB images. Proposed 2 level\nResNet50 methodology archive 99.03% accuracy. S. Jiang,\net al. proposed skeleton aware multi-model for sign language\nrecognition [13]. The authors used hand detectors with a pose\nestimator to extract hand key points. Methodology introduces\nthe sign language GraphCN(SL-GCN). As a result, proposed\nmethodology archive 98.425 of accuracy over RGB images.\nRoman Tongi, introduced a transfer learning based method-\nology for sign language recognition [14]. The methodol-\nogy proposes how transfer learning can be applied to SLR\nusing inﬂated 3D convolution neural network. American Sign\nLanguage (MS-SAL) and German Sign Language Dataset\n(SIGNUM) were used for simulation, and archive compat-\nible result. In 2017, Google Brain researchers proposed an\nencoder and decoder network architecture based on attention\nmechanisms. The author used this transformer architecture\nto translate the language. For a model architecture that com-\npletely relies on an attention mechanism, foregoing recur-\nrence, to identify global dependencies between input and\noutput. Experiment with 8 GPUs (P100). We obtain state-of-\nthe-art results for English to French translations [15].\nBy following outstanding performance on a language task,\nthe transformer opens up a new dimension for computer\nvision problems. Use a transformer for image classiﬁcation.\nAttention can be implemented in conjunction with a con-\nvolution network in computer vision. The proposed method\ntakes an image as input and does not extract any features.\nInstead, convert the image into patches, and the sequence of\npatches serves as an input matrix to the transformer’s encoder\nlayer. Further classiﬁcation to be done with the MLP. The\nauthors have introduced three variants of ViT, as Base, Large,\nand Huge, having 12, 24, and 32 layers, respectively [16].\nTable 1 shows the comparative analytics of static sign lan-\nguage detection with state-of-the-art deep learning models,\nwith parameter details and the number of classes.\nIII. MATERIALS AND METHODS\nThe proposed architecture used a transformer-based method\nfor static Indian sign recognition, multihead self-attention\nis proposed in the encoder phase of the transformer, the\ndetail study of vision transformer and multi-head attention as\nfollow.\nA. VISION TRANSFORMER\nThe emergence of Vision Transformer (ViT) strongly com-\npetes with the CNN, the state-of-the-art of computer vision,\nso commonly utilized in several image recognition tasks.\nThe ViT models surpass the convolutional neural networks\n(CNNs) in terms of computational capabilities, efﬁciency,\nand accuracy [17]. In the ﬁeld of natural language processing,\ntransformer architectures have state-of-the-art performance\nstandards. Only a few use cases are included in the ﬁeld\nof computer vision because attention is used in association\nwith convolutional networks (CNNs) or can be used as a\nsubstitute for certain convolution features while preserving\ntheir original composition. The transformer encoder detaches\nthese dependencies of the CNN, and the standard transformer\narchitecture can be directly applied to the sequences of image\npatches, and it works surprisingly well and accurately over\nimage classiﬁcation tasks.\nInitially, the transformer was introduced for language pro-\ncessing tasks. The trans-former used attention mechanisms\nrather than convolution layers. The design of a trans-former\nconsists of an encoder and a decoder. Both of them involve\nself-attention and feed-forward mechanism. The transformer\ncan be applied to various computer vision tasks by per-\nformance capabilities. The transformer performs better than\nother convolution methodologies in various computer vision\ntask [18]. The special use of a transformer in a computer\nvision task is known as a vision transformer (ViT). ViTs\n(different variants of ViT) achieve promising and remarkable\nresults in computer vision tasks. ViT has two major beneﬁts:\n1) self-attention mechanism, where the model reads a long\nrange of input seeds (tokens) in a universal context. 2) The\nability to train on large tasks.\nFigure 2 depicts the ViT design, in which ﬁrst images\nare converted into patches in accordance with the model\ndesign. Then patches directly feed into the linear projection\nlayer. In the second stage, the patch embedding process is\nperformed. The class token has been added to the sequence\nof the embedded patches. Thus, the size of patches increases\nby one. Embedded patches are also added with positional\nembedding to the memory positional sequence of patches.\nFinally, patch embedding and positional encoding with a\n4732 VOLUME 11, 2023\nD. R. Kothadiya et al.: SIGNFORMER: DeepVision Transformer for Sign Language Recognition\nFIGURE 2. Visual transformer process sequence for image classification\nin an encoder.\nclass token are fed to the encoder layer as the ﬁrst trans-\nformer layer. Encoding is the most important component\nof a transformer, especially in ViT. It contains two major\ncomponents, Multi Head Self-Attention (MHSA) and Multi-\nLayer Perceptron (MLP). The embedded input is normalized\nthrough the normalization layer. A normalized value is used\nto obtain Query (q), Key (k), and value (v) as a matrix,\nas shown in equation (1). The MHSA module executes the\nfollowing equation to achieve attention operation inside the\nencoding [19]. Finally, the attention layer’s output is fed to\nthe feed forward layer, which generates the encoder’s ﬁnal\noutput.\nAttention(q, k, v) =softmax(q ∗kkT /\n√\ndk ) ∗v. (1)\nB. VIT FOR IMAGE CLASSIFICATION\nThe vision, to vision transformer for image recognition task\nwas introduced in ‘‘An Image is Worth 16 ×16 Words:\nTransformers for Image Recognition at Scale’’ by Alexey\nDosovitskiy, Lucas Beyer, Alexander Kolesnikov et al. and\nsuccessfully trained on ImageNet, attaining good results\ncompared to the convolution network [15]. The transformer\nencoder receives an input image, generates ﬁxed size non-\noverlapped patches from these images, and then linearly\nembeds the sequence of patches. The class/token is embed-\nded to represent an entire image, which can be needed at\nthe classiﬁcation phase. The author also adds the resulting\nvector’s absolute position embedding and set sequence to the\npure transformer encoder. The original image resolution and\npatch resolution used in training and ﬁne tuning are reﬂected\nat every transformer layer checkpoint; however, any pre-train\nmodel can be used for the same. The author does this to\nimprove the accuracy and predictive power of the transformer,\nbecause each head has its own way of internal representa-\ntion and computation of input, each head can manage to\nunderstand the relationship between patches in sequence\n(i.e., collective, shared knowledge). Any relationship\ninformation among the patches missed by one head is highly\nlikely to be covered by another head [20].\nIV. PROPOSED ARCHITECTURE DESIGN\nThe proposed multihead self-attention based architecture\ncontains three major parts: patch embedding, feature extrac-\ntion and a classiﬁcation head. Stacked encoders are a core\npart of the simulation because of the feature extraction.\nAlgorithm 1 represent pseudo code for proposed\nmethodology.\nAlgorithm 1Sign Recognition\nInput: RBG sign image\nOutput: Sign value of image\n1: procedure data_input(img)\n2: x_train, x_test, y_test, y_train ←split(img, lable, split_ratio)\n3: return x_train, x_test, y_test, y_train\n4: end procedure\n5: procedure DATA_AUGMENTATION(img)\n6: img ←IMG.NORMALIZATION()\n7: img ←IMG.RESIZE(72, 72)\n8: img ←IMG.RENDOMFLIP(Horizontal)\n9: img ←IMG.RENDOMROTATION(0.02)\n10: return img\n11: end procedure\n12: procedure PATCHES(img,patch_size)\n13: patches ←IMG.RESHAPE(img, extract_patch, batch_size, num_patch)\n14: return patches\n15: end procedure\n16: procedure PATCHENCODER(patches)\n17: patches ←IMG.RESHAPE(img, extract_patch, batch_size, num_patch)\n18: position ←IMG.RANGE(0, num_patch, delta)\n19: encoded ←SELF.PROJECTION(patch)+SELF.POSITION_EMBEDDING(position)\n20: return encoded\n21: end procedure\n22: for each_img do\n23: inputs ←DATA_INPUT(img)\n24: augmented ←DATA_AUGMENTATION(inputs)\n25: patches ←PATCHES(augmented, patch_size)\n26: encoded_patches ←PATCHENCODER(patches)\n27: features ←TRANSFORMER(encoded_patches)\n28: end for\n29: classiﬁcation ←MLP(features)\n30: return class_sign\nA. PATCH EMBEDDING\nInitially 2D images convert into 1D sequences of embedding\ntokens. Images (Xi) are reshaped as, X ∈R(H∗W ∗C) where\nc is the number of channels as 3, and (H, W) consider a\nweight and height as (72 ×72). These images are converted\ninto the sequence of ﬂattened 1D patches with the shape of\n(N, P2 ∗C), where N is the total number of patches and (P, P)\nis the dimension of the patch. A positional embedding tensor\n(Epos) with shape of (N, D), learns 1D positional information\nof each patch and generates the spatial representation of the\npatches.\nB. MULTIHEAD SELF-ATTENTION\nThe multihead mechanism learns embedding vectors with\ndifferent aspect. The multihead attention (MHA) is included\nin each of the 8 layered stacked transformers. The hidden\nstate divided into n=4 heads to generate n feature tensor.\nEach self-attention head has three trainable matrices (q, k, v),\nrepresented in equation 2. Every (four) heads have an atten-\ntion tensor that can calculate as equation 1. softmax operation\nVOLUME 11, 2023 4733\nD. R. Kothadiya et al.: SIGNFORMER: DeepVision Transformer for Sign Language Recognition\nFIGURE 3. Proposed Transformer encoder architecture for static Indian\nsign language recognition.\ngives the attention score for every attention head. Farther self-\nattention matrices can be calculated as dot products of A and v\n(equation 3) [16], and concatenated features of the tensor can\nbe generated with equation 4 [16].\n[q, k, v] =zUqkv (2)\nSA(z) =Av (3)\nMSA(z) =[SA1(z);SA2(z);SA3(z);. . . ..;SAn(z)]Umsa (4)\nFIGURE 4. MLP classifier with four hidden layers.\nC. MLP CLASSIFICATION\nAn output tensor of multihead is added to the residual connec-\ntion, which is projected by point wise feed-forward network\nwith two linear layers with ReLU activation in between. Each\nlayer uses different weights (W1, W2) and bias (b1, b2) as\nshown in equation 5.\nFFNN(x) =ReLU(W1x +b1)W2 +b2 (5)\nProposed methodology uses MLP [21] classiﬁer with four\nhidden layers of vary in size to perform classiﬁcation over\nMultihead transformer network as shown in ﬁgure 4. classi-\nﬁcation network is deﬁned as xn ∗a1n ∗a2n ∗a3n ∗a4n ∗yn\nwere xn represent input feature vector, a(x)n represent neu-\nrons in respective hidden layers, yn represent output class\nprediction. Proposed methodology uses MLP classiﬁer based\non capabilities like i) ability to learn in complex and non-\nlinear networks ii) Generalization ability can be improved\niii) MLP learns independently from input variable size [22].\nFarther Adam Deep learning optimizer has been used, which\ninherit the feature of RMSprop and AdaGard [23]. Parameter\nof classiﬁer were set to improve model performance.\nThe authors have proposed a transformer-based encoder\nmodel to recognize static Indian sign language from an\nimage-based dataset. Initially, the dataset was divided into\ntrain validation splits with a 0.2 splitting factor, and the\nread resized image was 72 ×72. Resize image converted to\nthe same size as the non-overlapped patches. The proposed\nmethodology creates 144 patch form input images, as shown\nin ﬁgure 3. The sequential embedding layer creates a patch\nsequence, which is further combined with the positional vec-\ntor. The output of the positional embedding layer is fed to\nmulti-head attention. The proposed architecture uses six self-\nattention layers, and an appropriate tensor is managed at the\npositional embedding layer. The classiﬁcation has been per-\nformed in the MLP head as a single layer of ﬁne-tuned time.\nThe proposed methodology can archive validation accuracy\nof 99.29% with only ﬁve epochs. A small amount of training\n4734 VOLUME 11, 2023\nD. R. Kothadiya et al.: SIGNFORMER: DeepVision Transformer for Sign Language Recognition\nTABLE 2. Specification of dataset used in simulation.\ncan result in a high recognition rate. As per table 1, the\nproposed methodology can recognize static signs with little\ntraining and high accuracy.\nV. EXPERIMENT SETUP\nA. DATASET\nThe dataset used in the simulation is prepared from col-\nlection of publicly available Indian sign language dataset\n(static) [24], [25], which includes gesture of numbers (0–9)\nand alphabet. Dataset consist of RGB images of total\n36 classes with more than 1000 images per class to improve\ndata generalization augmentation has been done. Table 2\nshows the characteristics of dataset used in this experiment.\nB. DATA AUGMENTATION\nData augmentation is mainly used for sample balancing and\nimproving training sample variability. Data augmentation is\nalso signiﬁcant for transformer based framework because the\nhuge amount of data is essential for model training. Using\ndifferent augmentation techniques like ﬂipping, cropping,\nrotation, etc., authors simulate horizontal ﬂipping, colour\nspace transformation, random zoom with 0.2 weight and\nheight and slight rotation from −1 to −10 to train the model\non more generalized data. Figure 5 shows an overview of data\nset after augmentation.\nC. IMPLEMENTATION DETAILS\nThe authors have worked on a standard Static Indian Sign\nLanguage dataset [24], [25]. During this study, the authors\nimplemented a modiﬁed transformer using TensorFlow-\nKeras. The proposed methodology has achieved 99.29%\naccuracy over the 36 image-based Indian sign language\nclasses. The proposed methodology uses a static Indian\nsign language dataset [22] of images with a size of\n480 ×320 pixels with 3 (RGB) channels. The dataset is split\ninto 2 parts (train-test) with a 0.2 splinting rate as for training\nand testing. Initially 72 ×72 images are converted in patch\nsize of 6 ×6, total 144 patches as (( image_size/patch_size)2)\nwill be created for every image. The position embedding\ntensor of the patches will be used as the encoder input.\nFarther tensor will pass through two normalized layers with\nactivation (ReLU) and MLP classiﬁcation head. The perfor-\nmance of the proposed methodology was evaluated by three\ndifferent experiments. The parameters used in training are\nTABLE 3. Specification of dataset used in simulation.\nﬁne-tuned, like the optimizer number of layers and activation.\nPrecision [26], recall [27] and f1-score [28] were calculated\nas equations 6, 7 and 8, respectively, where TP and FP are the\nnumber of true and false positives, respectively\nPrecision =TP/(TP +FP) (6)\nRecall =TP/(TP +FN) (7)\nF1 −Score =2 ∗(Precision ∗Recall/(Precision+Recall))\n(8)\nThe proposed methodology has achieved signiﬁcant accuracy\nusing a smaller number of attention layers in the encoding\ncomponent of the transformer as well as a very small number\nof learning cycles. All the results were taken on a personal\ncomputer with an Intel Core i7 and 16 GB of RAM only.\nJupyter Lab is used to implement the proposed methodology.\nFigure 8 represents a heat-map for the class wise recognition\nof static sign language.\nD. RESULT OF PROPOSED METHOD\nIn this simulation, we evaluated different combinations of\nstate-of-the-art classiﬁcation networks with different classi-\nﬁers. Furthermore, the Author has also simulated proposed\nmethodology with different classiﬁers and train test split\nratio.\n1) EXPERIMENT WITH DIFFERENT BACKGROUND\nTable3 shows result comparison of The Indian sign language\ndataset with and without static background, in both cases\ntraining-testing parameters will be constant as ﬁve epochs\nand 80-20 train-test splitting ratio has been taken for simula-\ntion. Data augmentation was performed in both the scenario.\nThe outcome of this experiment to the analysis of the pro-\nposed methodology can recognize sign gestures over various\nbackgrounds (vary from sign to sign). The different appear-\nances of the background do not depend on the environmental\nfactor and resolution of camera.\n2) EXPERIMENT WITH DIFFERENT TRAIN–TEST SPLIT RATIO\nIn this experiment, the we simulated the proposed method-\nology’s performance over different train-test split ratios as\n80-20, 70-30, 60-40 shown in table 4. The reduction in the\ntraining dataset slightly effect on the recognition rate.\n3) EXPERIMENT WITH DIFFERENT SELF-ATTENTION HEAD\nAuthor also experiment with different numbers of self-\nattention head, as ﬁgure 6 shows that classiﬁcation accuracy\ndiffers with the number of attention head. This study aims\nVOLUME 11, 2023 4735\nD. R. Kothadiya et al.: SIGNFORMER: DeepVision Transformer for Sign Language Recognition\nFIGURE 5. Sample of augmented data with horizontal flip, rotation and colour space transformation.\nTABLE 4. Specification of dataset used in simulation.\nFIGURE 6. Accuracy comparison with multiple self-attention head.\nto know how classiﬁcation accuracy depend on the number\nof self-attention head. It also observes that from one head to\n2 head, there is no major change in accuracy, whereas 2 to\n4 head shows exponentially changes in accuracy.\n4) EXPERIMENT WITH DIFFERENT CLASSIFIER\nFigure 7 present the comparative analytics of different\nstate-of-the-art classiﬁers over the Indian sign language\ndataset [24]. VGG16, VGG19, Inception V3 and ResNet-50\nwere taken for convolution with different classiﬁers.\nE. PERFORMANCE MEASURES\nSeveral standard metrics for performance evaluation like\naccuracy, classiﬁcation error and precision have been con-\nsidered for model computation performance measurement.\nAccuracy can be considered an indicator of the model’s\nTABLE 5. Performance analysis of the proposed architecture on other\nstatic sign language dataset.\nperformance across all classes. The precision can be calcu-\nlated as the ratio of the total number of positive samples\nidentify correctly over the total number of samples classiﬁed\nas positive. Classiﬁcation errors can be deﬁned as missing of\nclassiﬁcation accuracy and error in the classiﬁcation instant.\nFigure 8 shows the classiﬁcation results of all 36 classes\nas a part of class wise performance analytics. Heat map\nrepresent erroneously classify signs. These three performance\nmetrics are used to better understand the model perfor-\nmance with the existing model to identify the signiﬁcant\nperformance of Transformer for sign language recognition.\nFigure 9 shows the result comparative analysis of Indian Sign\nLanguage Dataset with augmentation and MLP classiﬁer\nwith ReLU activation over other state-of-the-art deep learning\nmethodology of gesture recognition. Augmented ISL datasets\nhave been tested over CNN(core), Fast-R-CNN and Adaptive\nCNN. Five training cycles (epochs) have been taken as static\nparameters for comparisons, ﬁgure 9 also represent graphical\nrepresentation of the classiﬁcation report over tested deep\nlearning models.\nF. DISCUSSION\nIn this article sign language recognition is considered for\nstatic Indian sign language. The proposed methodology\nmay assist impaired people in communicating with other\n4736 VOLUME 11, 2023\nD. R. Kothadiya et al.: SIGNFORMER: DeepVision Transformer for Sign Language Recognition\nFIGURE 7. Different classifier comparative analysis of state-of the-art image based classification network. (RF=Random Forest, KNN= K-Nearest\nNeighbour, SVM= Support Vector Machine, LR= Logistic Regression.)\nFIGURE 8. Heat map graph for static Indian sign recognition.\nnormal people. The dataset used in the study is a static ISL\ndataset having signs of digit and alphabet in the context of\nthe Indian community. The proposed methodology is able to\nachieve very good accuracy as 99.29% with a higher number\nclass as 36 and very small number of training cycles as\nﬁve epochs. There is no need for data pre-processing while\nworking with the transformer encoder. Multihead attention\nFIGURE 9. Performance analysis of existing model on static Indian sign\nlanguage (ISL) dataset.\nhelps the model to improve the performance compared to\nother standard transformer model. Table 5 shows the per-\nformance of the proposed methodology over other standard\nstatic sign language datasets, such as American Sign Lan-\nguage (ASL) with and without augmentation and Bangle\nSign Language. The proposed methodology also performs\neffectively on other standard datasets with the same number\nof training cycles as ﬁve epochs.\nVI. CONCLUSION\nSign language recognition systems have lots of potential\napplications in the ﬁeld of human-computer interaction.\nA vision-based static sign gesture recognition system is\nessential to reduce the communication gap between normal\npeople and visually impaired people. The proposed method-\nology presents transformer-based sign language recogni-\ntion for static signs. A multi-head attention-based encoding\nVOLUME 11, 2023 4737\nD. R. Kothadiya et al.: SIGNFORMER: DeepVision Transformer for Sign Language Recognition\nframework can achieve good accuracy with a very small\nnumber of training layers and epochs. A framework with a\ntiny training process can also ﬁnd good accuracy over a large\nset of classes. The multi-head encoding framework in the\nTransformer broke up the recognition rate of gesture and sign\nlanguage recognition as a part of human-computer interac-\ntion applications. The Proposed methodology can also detect\nimages with augmentation like different angular position and\ndifferent brightness levels. Multihead based transformers are\nsuccessful for static sign recognition, for more advancement\nproposed model can be modiﬁed for isolated and continuous\nsign language detection. Father transformer based methodol-\nogy can proceed to identiﬁed sign gesture form isolated video\nof sign language, and farther extended to recognize continues\nsign language.\nACKNOWLEDGMENT\nThe authors would like to acknowledge the support of Prince\nSultan University for paying the APC of this publication.\nREFERENCES\n[1] W. Bright, ‘‘Ethnologue: Languages of the world ed. by Barbara F. Grimes,\nand: Index to the tenth edition of ethnologue: Languages of the world ed.\nby Barbara F. Grimes,’’ Language, vol. 62, no. 3, p. 698, 1986.\n[2] I. Meir, ‘‘Review of Zeshan (2000): Sign language in Indo-Pakistan:\nA description of a signed language,’’ Sign Lang. Linguistics, vol. 3, no. 2,\npp. 263–267, Dec. 2000, doi: 10.1075/sll.3.2.10mei.\n[3] D. Kothadiya, C. Bhatt, K. Sapariya, K. Patel, A.-B. Gil-González, and\nJ. M. Corchado, ‘‘Deepsign: Sign language detection and recognition\nusing deep learning,’’ Electronics, vol. 11, no. 11, p. 1780, Jun. 2022.\n[4] Y. Rokade, ‘‘Indian sign language recognition system,’’ Int. J. Eng. Tech-\nnol., vol. 9, no. 3, pp. 189–196, 2017.\n[5] S. Katoch, V. Singh, and U. S. Tiwary, ‘‘Indian sign language recognition\nsystem using SURF with SVM and CNN,’’ Array, vol. 14, Jul. 2022,\nArt. no. 100141.\n[6] K. Shenoy, T. Dastane, V. Rao, and D. Vyavaharkar, ‘‘Real-time Indian sign\nlanguage (ISL) recognition,’’ in Proc. 9th Int. Conf. Comput., Commun.\nNetw. Technol. (ICCCNT), Jul. 2018, pp. 1–9.\n[7] M. De Coster, M. Van Herreweghe, and J. Dambre, ‘‘Isolated sign recog-\nnition from RGB video using pose ﬂow and self-attention,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW),\nJun. 2021, pp. 3441–3450.\n[8] A. Mannan, A. Abbasi, A. R. Javed, A. Ahsan, T. R. Gadekallu, and\nQ. Xin, ‘‘Hypertuned deep convolutional neural network for sign language\nrecognition,’’Comput. Intell. Neurosci., vol. 2022, pp. 1–10, Apr. 2022.\n[9] M. Zakariah, Y. A. Alotaibi, D. Koundal, Y. Guo, and M. M. Elahi,\n‘‘Sign language recognition for Arabic alphabets using transfer learning\ntechnique,’’Comput. Intell. Neurosci., vol. 2022, pp. 1–15, Apr. 2022.\n[10] M. M. Kamruzzaman, ‘‘Arabic sign language recognition and generating\nArabic speech using convolutional neural network,’’ Wireless Commun.\nMobile Comput., vol. 2020, pp. 1–9, May 2020.\n[11] P. Rathi, R. K. Gupta, S. Agarwal, and A. Shukla, ‘‘Sign language recog-\nnition using ResNet50 deep neural network architecture,’’ in Proc. 5th Int.\nConf. Next Gener. Comput. Technol. (NGCT), 2020.\n[12] A. L. C. Barczak, N. H. Reyes, M. Abastillas, A. Piccio, and T. Susnjak,\n‘‘A new 2D static hand gesture colour image dataset for ASL gestures,’’\nHandle Proxy, vol. 15, pp. 12–20, Jan. 1970. Accessed: Oct. 18, 2022.\n[Online]. Available: http://hdl.handle.net/10179/4514\n[13] S. Jiang, B. Sun, L. Wang, Y. Bai, K. Li, and Y. Fu, ‘‘Skeleton aware multi-\nmodal sign language recognition,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. Workshops (CVPRW), Jun. 2021, pp. 3413–3423.\n[14] M. Varsha and C. S. Nair, ‘‘Indian sign language gesture recognition using\ndeep convolutional neural network,’’ in Proc. 8th Int. Conf. Smart Comput.\nCommun. (ICSCC), Jul. 2021, pp. 193–197.\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nI. Polosukhin, and L. Kaiser, ‘‘Attention is all you need,’’ in Proc. 31st Int.\nConf. Neural Inf. Process. Syst., 2017, pp. 6000–6010.\n[16] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16 ×16 words: Trans-\nformers for image recognition at scale,’’ in Proc. Int. Conf. Learn. Repre-\nsent., 2021, pp. 1–22.\n[17] G. Boesch. (Aug. 23, 2022). Vision Transformers (ViT) in Image\nRecognition—2022 Guide. Accessed: Oct. 18, 2022. [Online]. Available:\nhttps://viso.ai/deep-learning/vision-transformer\n[18] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,\nC. Xu, Y. Xu, Z. Yang, Y. Zhang, and D. Tao, ‘‘A survey on vision\ntransformer,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 1,\npp. 87–110, Jan. 2023.\n[19] K. Islam, ‘‘Recent advances in vision transformer: A survey and outlook\nof recent work,’’ Aug. 2022, arXiv:2203.01536. Accessed: Oct. 18, 2022.\n[20] R. Anand. (Jul. 2, 2021). Vision Transformers Explained. Accessed:\nOct. 18, 2022. [Online]. Available: https://blog.paperspace.com/vision-\ntransformers\n[21] B. Krose and P. V. D. Smagt, An Introduction to Neural Networks.\nPrinceton, NJ, USA: Citeseer, 1993.\n[22] H. K. Gajera, D. R. Nayak, and M. A. Zaveri, ‘‘A comprehensive analysis\nof dermoscopy images for melanoma detection via deep CNN features,’’\nBiomed. Signal Process. Control, vol. 79, Jan. 2023, Art. no. 104186.\n[23] J. Singh and R. Banerjee, ‘‘A study on single and multi-layer perceptron\nneural network,’’ in Proc. 3rd Int. Conf. Comput. Methodol. Commun.\n(ICCMC), Mar. 2019, pp. 35–40.\n[24] P. Arikeri. (Jun. 4, 2021). Indian sign language (ISL). Kaggle.\nAccessed: Oct. 18, 2022. [Online]. Available: https://www.kaggle.\ncom/datasets/prathumarikeri/indian-sign-language-isl\n[25] D. R. Kothadiya. Deepkothadiya/STATIC_ISL: Static Indian sign language\ndataset having sign of digit and alphabet. GitHub. Accessed: Oct. 19, 2022.\n[Online]. Available: https://github.com/DeepKothadiya/Static_ISL\n[26] J. Huang, W. Zhou, H. Li, and W. Li, ‘‘Sign language recognition using 3D\nconvolutional neural networks,’’ in Proc. IEEE Int. Conf. Multimedia Expo\n(ICME), Jun. 2015, pp. 1–6.\n[27] J. Huang, W. Zhou, H. Li, and W. Li, ‘‘Sign language recognition using\nreal-sense,’’ in Proc. IEEE China Summit Int. Conf. Signal Inf. Process.\n(ChinaSIP), Jul. 2015, pp. 166–170.\n[28] L. Pigou, S. Dieleman, P.-J. Kindermans, and B. Schrauwen, ‘‘Sign lan-\nguage recognition using convolutional neural networks,’’ in Proc. Eur.\nConf. Comput. Vis., 2015, pp. 572–578.\n[29] GrassKnot. Grassknoted/unvoiced: American sign language to speech\napplication. GitHub. Accessed: Oct. 18, 2022. [Online]. Available:\nhttps://github.com/grassknoted/Unvoiced\n[30] A. Thakur. (May 1, 2019). American sign language dataset. Kaggle.\nAccessed: Oct. 19, 2022. [Online]. Available: https://www.kaggle.com/\ndatasets/ayuraj/american-sign-language-dataset\n[31] S. M. Rayeed. (Aug. 8, 2021). Bangla sign language dataset. Kaggle.\nAccessed: Oct. 18, 2022. [Online]. Available: https://www.kaggle.com/\ndatasets/rayeed045/bangla-sign-language-dataset\nDEEP R. KOTHADIYA received the bachelor’s\nand master’s degrees in computer science and\nengineering from Gujarat Technological Uni-\nversity. He is currently pursuing the Ph.D.\ndegree with the Charotar University of Sci-\nence and Technology (CHARUSAT). He is also\nworking as an Assistant Professor with the\nU & P U Patel Department of Computer Engineer-\ning, Chandubhai S. Patel Institute of Technology,\nCHARUSAT. He is also a Research Scholar with\nCHARUSAT. He has already published three research papers, including one\nSCI indexed paper. He is also a Technical Reviewer of International Journal\nof Computing and Digital Systems (Scopus).\n4738 VOLUME 11, 2023\nD. R. Kothadiya et al.: SIGNFORMER: DeepVision Transformer for Sign Language Recognition\nCHINTAN M. BHATT worked as an Assis-\ntant Professor with the CE Department, CSPIT,\nCHARUSAT, for 11 years. He is currently working\nas an Assistant Professor with the Department of\nComputer Science and Engineering (CSE), School\nof Technology, Pandit Deendayal Energy Univer-\nsity (PDEU). He is the author or coauthor of more\nthan 80 publications in the areas of computer\nvision, the Internet of Things, and fog computing.\nHe was involved in successful organization of few\nspecial issues in SCI/Scopus journals. He has won several awards, including\nthe CSI Award and the Best Paper Award for his CSI articles and conference\npublications.\nTANZILA SABA (Senior Member, IEEE) received the Ph.D. degree in\ndocument information security and management from the Faculty of Com-\nputing, University Teknologi Malaysia (UTM), Malaysia, in 2012. She is\ncurrently working as an Associate Professor with the College of Com-\nputer and Information Sciences, Prince Sultan University (PSU), Riyadh,\nSaudi Arabia. She has published more than 100 publications in high ranked\njournals. Her research interests include bioinformatics, data mining, and\nclassiﬁcation. She won the Best Student Award at the Faculty of Computing,\nUTM, in 2012. She was awarded the Best Research of the Year Award at\nPSU, from 2013 to 2016. Due to her excellent research achievement, she\nis included in Marquis Who’s Who (S & T) 2012. She is also an editor of\nseveral reputed journals and on panel of TPC of international conferences.\nAMJAD REHMAN (Senior Member, IEEE)\nreceived the Ph.D. degree (Hons.) from the Fac-\nulty of Computing, Universiti Teknologi Malaysia,\nin 2010, with a specialization in forensic docu-\nments analysis and security. He was a Postdoc-\ntoral Researcher with the Faculty of Computing,\nUniversiti Teknologi Malaysia, in 2011. He is\ncurrently a Senior Researcher with the Artiﬁcial\nIntelligence and Data Analytics Laboratory, CCIS,\nPrince Sultan University, Riyadh, Saudi Arabia.\nHe is also a PI in several funded projects and also completed projects funded\nfrom MOHE Malaysia, Saudi Arabia. He is the author of more than 200 ISI\njournal articles and conferences. His research interests include data mining,\nhealth informatics, and pattern recognition. He received the Rector Award\nfor the 2010 Best Student in the university.\nSAEED ALI BAHAJ received the Ph.D. degree\nfrom Pune University, India, in 2006. He is\ncurrently an Associate Professor with the Depart-\nment of Computer Engineering, Hadramout Uni-\nversity, and also an Associate Professor with\nPrince Sattam bin Abdulaziz University. His\nresearch interests include artiﬁcial intelligence,\ninformation management, forecasting, informa-\ntion engineering, big data, and information\nsecurity.\nVOLUME 11, 2023 4739"
}