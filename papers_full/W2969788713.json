{
    "title": "Latent Relation Language Models",
    "url": "https://openalex.org/W2969788713",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2300956062",
            "name": "Hayashi Hiroaki",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Hu, Zecong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746807616",
            "name": "Xiong, Chenyan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2138493030",
            "name": "Neubig, Graham",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W179875071",
        "https://openalex.org/W2963824800",
        "https://openalex.org/W2963676655",
        "https://openalex.org/W2798935874",
        "https://openalex.org/W2963735467",
        "https://openalex.org/W2963537482",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2321470647",
        "https://openalex.org/W2009169514",
        "https://openalex.org/W1521413921",
        "https://openalex.org/W2136297100",
        "https://openalex.org/W2084531783",
        "https://openalex.org/W2057653135",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W1956559956",
        "https://openalex.org/W2963223306",
        "https://openalex.org/W2963324947",
        "https://openalex.org/W2413436069",
        "https://openalex.org/W2080133951",
        "https://openalex.org/W2621404689",
        "https://openalex.org/W2526471240",
        "https://openalex.org/W2963451457",
        "https://openalex.org/W2962708992",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2897767292",
        "https://openalex.org/W2964165364",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2963541420",
        "https://openalex.org/W2963415248",
        "https://openalex.org/W2963290255",
        "https://openalex.org/W2963091658",
        "https://openalex.org/W2964325845",
        "https://openalex.org/W2803267010",
        "https://openalex.org/W2951048068",
        "https://openalex.org/W2157812664",
        "https://openalex.org/W2094728533",
        "https://openalex.org/W2476140796",
        "https://openalex.org/W2561658355",
        "https://openalex.org/W2612773933",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2889009749",
        "https://openalex.org/W2003170434",
        "https://openalex.org/W2107598941",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963831883",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2154764394",
        "https://openalex.org/W2041258965",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2141608913",
        "https://openalex.org/W2962832505",
        "https://openalex.org/W2086699924",
        "https://openalex.org/W2893600504",
        "https://openalex.org/W2493916176"
    ],
    "abstract": "In this paper, we propose Latent Relation Language Models (LRLMs), a class of language models that parameterizes the joint distribution over the words in a document and the entities that occur therein via knowledge graph relations. This model has a number of attractive properties: it not only improves language modeling performance, but is also able to annotate the posterior probability of entity spans for a given text through relations. Experiments demonstrate empirical improvements over both a word-based baseline language model and a previous approach that incorporates knowledge graph information. Qualitative analysis further demonstrates the proposed model's ability to learn to predict appropriate relations in context.",
    "full_text": "Latent Relation Language Models\nHiroaki Hayashi†∗, Zecong Hu†∗, Chenyan Xiong‡, Graham Neubig†\n†Carnegie Mellon University, ‡Microsoft Research AI\nAbstract\nIn this paper, we propose Latent Relation Lan-\nguage Models (LRLMs), a class of language\nmodels that parameterizes the joint distribu-\ntion over the words in a document and the en-\ntities that occur therein via knowledge graph\nrelations. This model has a number of at-\ntractive properties: it not only improves lan-\nguage modeling performance, but is also able\nto annotate the posterior probability of entity\nspans for a given text through relations. Exper-\niments demonstrate empirical improvements\nover both a word-based baseline language\nmodel and a previous approach that incorpo-\nrates knowledge graph information. Quali-\ntative analysis further demonstrates the pro-\nposed model’s ability to learn to predict appro-\npriate relations in context.\n1 Introduction\nLanguage models (LMs) calculate the probability\nP(X) of textual data X, and are a core model\nclass of interest to NLP. LMs are used as testbeds\nfor evaluation of generative models of text, and\nhave applications such as rescoring of upstream\nlanguage generation inputs (Sundermeyer et al.,\n2012), grammatical error correction (Felice et al.,\n2014), or pre-training of sentence representations\n(Dai and Le, 2015; Peters et al., 2018). State-of-\nthe-art LMs uses neural networks to calculate this\nprobability (Bengio et al., 2003; Mikolov et al.,\n2010; Merity et al., 2017b; Yang et al., 2018).\nWithin X, there exist a wide variety of words\nto be modeled, from closed-class function words,\nto common nouns or verbs, to named entities and\nnumbers (Zipf, 1949). Notably, words on the\nrarer end of this spectrum are often more seman-\ntically or topically important (as evidenced by the\nsuccess of heuristics such as TF-IDF (Salton and\nMcGill, 1986), which up-weight words with low\n∗Equal contribution.\n<nationality>\n<position held> \nlawyer \n(“ attorney”, ...) \n  American \npr esident of the United States \nT opic:  Barack Obama \n Knowledge Graph \n  Article \n<occupation>\nBarack  Hussein  Obama  II  (...;   born  August  4,  1961)  is  an \nAmerican [nationality]   attorney [occupation]   and  polit­ \nician [occupation]   wh o  served  as  the  44th  president  of  the \nUnited States [position held]  from 2009 to 2017. ... \npolitician <occupation>\n... \nFigure 1: Overview of our task of language model-\ning conditioned on structured knowledge. For a given\ntopic, we want to learn an LM that leverages the knowl-\nedge graph through relations when modeling the text.\nfrequency). Previous work has noted that while\nneural LMs greatly out-perform alternatives such\nas n-gram models on frequent words, they often\nunder-perform on these rare words due to their\nlimited parameter budget, which puts them at a\ndisadvantage compared to non-parametric models\nlike standard n-grams (Neubig and Dyer, 2016).\nWays to mitigate this bottleneck have been pro-\nposed in the context ofconditional LMs, which in-\nstead model the conditional probability P(X|C),\nwhere C is some context given to the model. For\ninstance in sequence transduction tasks, there are\nmechanisms to copy from the source sequence (Gu\net al., 2016) or use word or phrase dictionar-\nies (Arthur et al., 2016; Tang et al., 2016) to im-\nprove modeling of low-frequency words. Per-\nhaps more interesting from an LM perspective\nare methods explicitly conditioned on informa-\ntion from structured knowledge sources such as\nknowledge graphs (Angeli et al., 2010; Ahn et al.,\n2016; Parvez et al., 2018; Wang et al., 2018), ta-\nbles (Barzilay and Lapata, 2005; Lebret et al.,\n2016), or grammars (Konstas and Lapata, 2013).\nThese methods are analogous to human language\n1https://www.wikidata.org/wiki/Q892.\narXiv:1908.07690v1  [cs.CL]  21 Aug 2019\nproduction, where the underlying knowledge or\nintent is converted into linguistic realizations.\nIn this work, we propose Latent Relation Lan-\nguage Models (LRLMs), a class of conditional\nLMs that take relational information between en-\ntities in a knowledge graph as context. Speciﬁ-\ncally, our model is able to generate words either\nfrom a ﬁxed word vocabulary, or through spans\ndeﬁned according to their relations with a topic en-\ntity of interest, as shown in Figure 1. The choice of\nwhich method of generation to use is deﬁned as a\nlatent variable sequence Z. We use Latent Predic-\ntor Networks (LPNs; Ling et al. (2016)) to jointly\nlearn P(X,Z |C), thus tractably marginalizing\nover all the possible spans. Compared to other\nmethods that condition LMs on knowledge graphs\n(KGs; Ahn et al. (2016); Wang et al. (2018)) ,\nthe span-based generation from the KGs alleviates\nproblems of malformed or incomplete mentions.\nMoreover, the posterior probabilities ofZcan also\nbe considered as entity links, which are of inter-\nest in their own right in the information extraction\nﬁeld (Ceccarelli et al., 2013; Piccinno and Ferrag-\nina, 2014; Ganea and Hofmann, 2017).\nWe apply the model on Wikipedia articles (X),\nwith the help of relational information ( C) such\nas Wikidata (Vrande ˇci´c and Krötzsch, 2014) or\nFreebase (Bollacker et al., 2008) regarding each\narticle topic. Empirical results on open vocabu-\nlary language modeling show that the proposed\nmodel out-performs previous approaches on the\nsame task, demonstrating that LRLMs provide an\neffective way to condition on this context. We also\ndemonstrate the merit of explicitly modeling latent\nrelations by examining the posterior probabilities\nover the chosen relations Z, which are in concert\nwith human intuitions about how relations are be-\ning expressed in the text.\n2 Language Modeling Conditioned on\nStructured Knowledge\nFirst, we deﬁne the task of open-vocabulary lan-\nguage modeling conditioned on structured data.\n2.1 Task Deﬁnition\nConsider a directed and labeled knowledge\ngraph (KG) G = (V,E) consisting of a set of\nnodes V = {v1,...,v |V|}and a set of relation\nedges E = {ei:⟨si,ωi,oi⟩| si,oi ∈V, ωi ∈R}.\nRelation ei contains si, ωi, and oi as the sub-\nject, relation type, and object. R is the set of\nall relation types. Each node vi ∈ V repre-\nsents either an entity or an attribute, and is as-\nsociated with a set of surface forms A(vi) =\n{ai,1,...,a i,|A(vi)|}that can be used to refer to\nvi. For instance, the subject “ Barack Obama ”2\nis connected to both “ politician” and “ lawyer”\nwith the relation <occupation>, and the object\nentity “politician”3 has “political figure”\nand “polit.” as additional aliases. Notably sur-\nface forms of many objects in the KG can be mul-\ntiple words, and thus it is necessary to have ma-\nchinery to deal with this fact.\nGiven this KG, we further deﬁne a topic entity\nsabout which we would like to generate an expla-\nnation. Our conditional language modeling prob-\nlem is then deﬁned as the problem of modeling the\nconditional probability of text X: P(X|G,s). In\nparticular, we consider a subgraph G′ = (V′,E′)\nof the original KG G by extracting nodes and\nedges directly related to the topic entity s:\nV′: {s}∪{oi |⟨s,∗,oi⟩∈ E},\nE′: {ei:⟨s,ωi,oi⟩|⟨ s,ωi,oi⟩∈ E}.\n2.2 Why Condition on Knowledge Graphs?\nKGs provide two important beneﬁts for neural\nLMs. First, they have high coverage of rare words,\nwhich addresses lack of textual supervision for\npredicting these words. More importantly, KGs\nhave the potential to help LMs generate factu-\nally consistent text by providing factually consis-\ntent associations between entities. Normal LMs\nwould have to rely on supervision purely from tex-\ntual data, which may not provide a learning signal\nstrong enough to accurately generate these facts.\nFor instance, results from Radford et al. (2019)\nshow that even with a very large model trained on\nmassive amounts of data, samples can be factually\nincorrect, although being ﬂuent and coherent.\n3 Latent Relation Language Models\nNext we describe our proposed framework of La-\ntent Relation Language Models (LRLMs).\n3.1 Motivation\nThe goal of the conditional language model-\ning task is to model the conditional probability\nP(X|G′,s), assuming the presence of a KG sub-\ngraph G′ = (V′,E′) related to a topic entity s.\n2https://www.wikidata.org/wiki/Q76.\n3https://www.wikidata.org/wiki/Q82955.\nRelation\nWord\n… …\nBarack Hussein Obama II born August 4 , 1961\n<birth name> <birth date>\n<given name> <family name>\nModel\nGenerated\nText\nborn<s> …Barack Hussein Obama II August 4 , 1961 …\n1x 2x 3x 4x 8x 9x 10x 11x 12x\n1σ 3σ 4σ… …\nChosen Span\nPossible Span\nFigure 2: While generating, our model switches between the two sources, namely “Relation” and “Word”. Nodes\nrepresent hidden states up to each token, and edges represent possible span matches,i.e., choice of latent variables.\nIn this example, we show one choice of latent variables with solid lines, and other options as dashed lines. We also\nshow an “annotation” of the token sequence by the spans and sources we choose.\nSpeciﬁcally, we can choose edges from E′ and\ncopy the corresponding object nodes from V′.\nHowever, it is insufﬁcient to model this probabil-\nity using only G′and s as conditions, because it\nis unknown to us which text spans are matched\nto which relations, and simple text matching al-\ngorithms would yield many false positives.4\nTo circumvent this lack of relation annotation,\nwe treat such text spans as latent variables. For-\nmally, let X = {xi}N\ni=1 be the sequence of N\ntokens, and Z = {(πt,σt,ρt)}T\nt=1 a sequence of\nlatent variables describing text span matches:\n•The source variable πt ∈{REL ,WORD }denotes\nthe generation source of the span xσt.\n•The span variable σt = (ℓt,rt) speciﬁes a token\nsubsequence xσt = {xi}rt\ni=ℓt.\n•The relation variable ρt = (et,at) describes the\nmatching relation and surface form of the span\nxσt, and is only used when πt = REL .\nFor Zto be a valid sequence of latent variables,\nthe following conditions must be satisﬁed:\n•The span latent variables {σt}T\nt=1 form a seg-\nmentation of X, i.e., ℓt = rt−1 + 1 for t =\n2,...,T . This also implies T ≤N.\n•If πt = WORD , then ℓt = rt.\n•If πt = REL , then ρt = (et,at) where et =\n⟨s,ωt,ot⟩should satisfy et ∈E′, at ∈A(ot),\nand xσt = at, i.e., ρt must correspond to a valid\nsurface form of an object that is related to the\ntopic entity sand matches the text span.\n4For example, “ New York City ” has an alias “ New\nYork”, which matches “New York” (state) and parts of “New\nYork City Council”.\nLet Zbe the set of all valid latent variable se-\nquences. We can now model the conditional prob-\nability by marginalizing over Z:\nP(X|G′,s) =\n∑\nZ∈Z\nP(X,Z |G′,s). (1)\nWe will show in section 3.3 that this marginaliza-\ntion is tractable. For sake of brevity, unless noted\notherwise, we drop G′and sfrom the conditions\nin the following sections.\n3.2 Deﬁnition\nGiven the latent variable sequence Z, we follow\nLing et al. (2016) in factoring the joint probability:\nP(X,Z) =\nT∏\nt=1\nP(πt,σt,ρt,xσt |x<ℓt)\n=\nT∏\nt=1\nP(πt|x<ℓt)P(σt,xσt,ρt|πt,x<ℓt),\nhere x<i is the sequence of ﬁrst i−1 tokens in X.\nFigure 2 shows an example of generation accord-\ning to this factorization, and Algorithm 1 precisely\ndeﬁnes the process of generating at time step t.\n3.3 Training\nDuring training, we marginalize over Zaccording\nto Equation 1. Since the probability at time step t\nis independent of previous latent variable choices,\nthe marginalization is tractable using the forward-\nbackward algorithm (Baum et al., 1970).\nDeﬁne the forward probability αi as the\nmarginal probability of the sequence up to the i-\nth token, computed as follows:\nαi =\n∑\n(π,σ:(ℓ,r),ρ)∈τi\nαℓP(π,σ,x σ,ρ |x<ℓ),\nAlgorithm 1 Generative Process of LRLM\nInput previous span σt−1 = (ℓt−1,rt−1), previously generated tokens x<rt−1 .\nOutput source πt, span σt = (ℓt,rt), relation ρt = (et,at), and token subsequence xσt .\n1: ℓt ←rt−1 + 1 ⊿Update the beginning of span. :1\n2: ˆπt ∼P(πt|x<ℓt ) ⊿Choose whether to generate a word or relation. :2\n3: if ˆπt = WORD then ⊿Generating a word. :3\n4: P(σt,xσt ,ρt|πt = WORD ,x<ℓt ) := P(xℓt |x<ℓt ) ⊿Simplify the probability. :4\n5: ˆxℓt ∼P(xℓt |x<ℓt ) ⊿Choose a word from model vocabulary. :5\n6: if ˆxℓt = <UNK> then\n7: ˆxℓt ∼CHAR MODEL ⊿Generate a word using a character model. :7\n8: else if ˆxℓt = <EOS> then\n9: End generation.\n10: end if\n11: else if ˆπt = REL then ⊿Generating a relation. :11\n12: P(σt,xσt ,ρt|πt = REL ,x<ℓt ) := P(et|x<ℓt )P(at|et,x<ℓt ) ⊿Factor the probability. :12\n13: ˆet ∼P(et|x<ℓt ) ⊿Choose a relation. :13\n14: ˆat ∼P(at|ˆet,x<ℓt ) ⊿Choose a surface form from the selected relation. :14\n15: ˆxσt ←ˆat ⊿Generate a phrase. :15\n16: end if\nwhere τi is the set of valid latent variable tuples\n(π,σ:(ℓ,r),ρ) such that r= i, i.e., all valid spans\nending at the i-th token. The marginal probability\nwe optimize for is then αN. The backward prob-\nability βi which is required for gradient computa-\ntion can be similarly calculated.\n3.4 Parameterization\nWe use neural networks to parameterize all prob-\nability distributions mentioned above. Decisions\nfor time step tare based on a D-dimensional hid-\nden state hℓt. This hidden state can be generated\nby any neural sequence model, and we experiment\nwith multiple models in experiments to demon-\nstrate the generality of our approach.\n3.4.1 Source Selection\nSource selection is done using a simple linear\nmodel followed by a softmax function applied to\nthe latest word-level hidden state hℓt:\nP(πt|x<ℓt) = softmax(Wπhℓt + bπ).\nWπ ∈R2×D,bπ ∈R2 are trainable parameters.\n3.4.2 Word Generation\nLike conventional word-level neural language\nmodels, we have the option to generate the next\ntoken from a ﬁxed vocabulary. This option is used\nto generate any word that isn’t an object partici-\npating in a relation. The probability is:\nP(xℓt |x<ℓt) = softmax(Linearw(hℓt)),\nwhere we deﬁne Linear(h) as a linear transform\nwith a bottleneck of dimension K into a vector\nover vocabulary size L:\nLinear(h) =W1(W2h + b2) +b1,\nwhere W1 ∈RL×K, b1 ∈RL, W2 ∈RK×D,\nb2 ∈RD are trainable parameters. Empirically\nwe found this low-rank version to out-perform a\nfull linear transform.\nGenerating unknown words As our task is\nopen-vocabulary language modeling, we must be\nable to generate words even if they are out of\nvocabulary. Following Chung et al. (2017) and\nLuong and Manning (2016), we do so by hav-\ning a character-level LM “spell-out” any unknown\nwords. If the unknown word is x= c1 ...c |c|with\n|c|characters:\nP(x|x<ℓt) =P(<UNK> |x<ℓt)P(c1 ...c |c|; θchar),\nwhere θchar are the parameters of the character\nLM. We pre-train this model on the set of all the\nunique words in the training set and ﬁx its param-\neters while training LRLM.\n3.4.3 Relation Generation\nThe goal of relation generation is to ﬁnd the most\nsuitable span that can be copied into the text. As\nLine 12 of Algorithm 1 depicts, this probability\nis factorized into two steps: relation selection and\nsurface form selection.\nRelation selection We utilize pretrained KG\nembeddings5 for entities and relation types. For\na relation ei: ⟨s,ωi,oi⟩, we concatenate KG em-\nbeddings for ωi and oi to obtain the relation em-\nbedding ei.6 We then compute the probability of\n5Speciﬁcally, from OpenKE (Han et al., 2018).\n6We train embeddings for each relation type not covered\nby pre-trained embeddings, and an UNK embedding for at-\ntributes and entities not covered by pre-trained embeddings.\nDataset Doc V ocab Rel/Ent Tok/Doc Ment/Doc\nWikiFacts 7856 40.0k 82.71 157.25 9.64\nWikiText-S 27685 71.1k 11.38 295.75 11.20\nWikiText-F 27685 264k 11.38 3559.91 73.01\nTable 1: Training set statistics for all dataset variations:\nnumber of training documents, vocabulary size, rela-\ntions per head entity, tokens per document, and entity\nmentions per document.\nselecting each relation as:\nP(ei|x<ℓt) = softmax(e⊤\ni Linearo(hℓt)).\nSurface form selection We featurize surface\nforms via fastText (Bojanowski et al., 2017) em-\nbeddings pre-trained on the training corpus, and\ncalculate probability of surface form ak as:\nP(ak|ei,x<ℓt) = softmax(f⊤\nak(Wahℓt + ba)),\nwhere fak is the embedding forak and Wa, ba are\ntrainable parameters.\n4 Datasets\nWe use two datasets with different characteristics\nfor experiments; statistics are shown in Table 1.\n4.1 WikiFacts\nWikiFacts7 (Ahn et al., 2016) is a collection of\nWikipedia articles restricted to /film/actor\ndomain entities in Freebase (Bollacker et al.,\n2008). Each example consists of the ﬁrst section\nof the original article. Since ofﬁcial splits for eval-\nuation are not provided, we follow previous work\nand performed a random split of 80/10/10%.\nIn addition to Freebase, this dataset expands the\nset of relations by including topic entities from\nother articles linked to the page to be generated.\nSince these (gold) entities will not be available if\nwe attempt to generate new articles, we remove\nthem from the dataset for our main experiments8.\nFinally, we note that this dataset does not in-\nclude aliases for entities, i.e., |A(o)|= 1 for all\nobjects o. Hence, the surface form selection mod-\nule acts as oracle, where it always assigns a prob-\nability of 1 to the correct surface form.\n7https://bitbucket.org/skaasj/\nwikifact_filmactor\n8For consistency with prior work, we also report results\nwith them in Appendix C.\n4.2 WikiText\nWhile WikiFacts has been used in previous work\non LMs using structured data (Ahn et al., 2016),\nthe domain is limited (ﬁlm actors). To investi-\ngate the capability of knowledge-infused LMs in\nan open-domain setting with a wide variety of rela-\ntions, we build a large-scale open-domain dataset\nfrom the existing WikiText-103 dataset (Merity\net al., 2017b) by associating articles with enti-\nties in Wikidata (Vrande ˇci´c and Krötzsch, 2014).\nWe employ the same data splits from the origi-\nnal dataset. In the following paragraphs, we dis-\ncuss how we bridge KGs and the articles from\nWikiText-103 (more details in Appendix A).\nConstructing subgraphs for articles As dis-\ncussed in Section 2, we take the original KG and\nextract a relevant subgraph G′ for each article.\nWhile there are many options on how to extract\nthis subgraph, we choose the subgraph G′consist-\ning of direct neighbors of the topic entity for each\narticle. This forms a star-shaped subgraph, with\nthe topic entity as the central node, connected by\nthe related entities and attributes. We found on av-\nerage 3.1 surface forms for each entity.\nLinking mentions with the KG For each ob-\nject in G′, we search for occurrences of all surface\nforms in the article while allowing token overlaps\namong them. Note that, similarly to distant super-\nvision for relation extraction (Mintz et al., 2009),\nthis process can produce false positive relation\nmentions because of simple string-based match-\ning. We rely on our model’s ability to ignore such\nmentions by learning to assign high probabilities\nonly on the correct mentions.\nWe name the dataset obtained through this pro-\ncess WikiText-F (Full). We also create WikiText-S\n(Short) by truncating after the ﬁrst sections of each\nexample in WikiText-F. This dataset is similar to\nWikiFacts in terms of article length, and allows\nperformance comparisons among the two datasets.\n5 Experiments\nAs previously noted, we evaluate our models on\nopen-vocabulary language modeling and report\ntoken-level perplexity. This provides more real-\nistic perplexity measures of text than in closed\nsetting by considering OOV words. Speciﬁcally,\nwe use pre-trained character-level LMs from Sec-\ntion 3.4.2 for each dataset to discount the probabil-\nity of an unknown word based on its spelling. Un-\nlike UPP (Ueberla, 1994), which also adjusts the\nperplexity of OOV words but are limited within\ncorpus, discounting based on spelling enables\ntruly open-vocabulary evaluation. This is done for\nall tested models, both proposed and baselines.\n5.1 Model Conﬁguration\nFor WikiFacts, we use a ﬁxed word vocabulary\nsize of 40,000 following previous work. For\nWikiText-derived datasets, we include all words\nwith frequencies no less than 3 in our dataset fol-\nlowing Merity et al. (2017b). We use adaptive\nembeddings and softmax to handle large vocabu-\nlary (Baevski and Auli, 2019; Grave et al., 2017).\nTo calculate the hidden state hx<i, we test\ntwo varieties of neural sequence models: stan-\ndard LSTMs (Hochreiter and Schmidhuber, 1997),\nand the state-of-the-art Transformer-XL (Dai\net al., 2019). We implement all models in Py-\nTorch (Paszke et al., 2017). Training details and\nhyperparameters are summarized in Appendix B.\n5.2 Baselines\nWe compare LRLM against two baselines:\nVanilla language model (Vanilla LM) This is\na simpliﬁcation of LRLM removing the relation\ngeneration module, analogous to standard LSTM\nor Transformer-XL language models from previ-\nous work (Merity et al., 2017a; Dai et al., 2019).\nNeural Knowledge Language Model (NKLM)\nSimilar to LRLM, the Neural Knowledge Lan-\nguage Model (NKLM; Ahn et al. (2016)) also has\nthe ability to copy from a given set of KG triples,\nbut differs from LRLM in several ways:\n1. LRLM marginalizes over all derivations of a se-\nquence, which allows processing of overlapped\ntokens among spans, while NKLM makes all\ndecisions in a hard fashion and cannot handle\nsuch overlapped tokens.9\n2. LRLM allows generation at span-level ( i.e.\ncan predict multi-word entities at once), while\nNKLM predicts one word at a time and the\nmodel needs to repeatedly predict the right re-\nlation until copying of an object is done.\nThe original NKLM does not differentiate be-\ntween aliases, so we perform the same surface\nform selection as LRLM for fair comparison.\n9We perform additional data preprocessing on WikiText\nfor NKLM, detailed in Appendix D.\n6 Results and Analysis\n6.1 Main Results\nPerplexities over the datasets are shown in Ta-\nble 2. We observe that for both sequence models,\nLRLM out-performs the baselines on all datasets\n(although on the one case of LSTM+WikiText-S\nthe improvement was not statistically signiﬁcant).\nParticularly on the two WikiText-derived datasets,\nour model shows signiﬁcant improvements over\nthe baselines by leveraging KGs in comparison to\nthe vanilla LM, while NKLM has difﬁculty utiliz-\ning the KGs to achieve better perplexity, and in\nsome cases results in worse perplexities than the\nvanilla LM. Note that these results are on open-\nvocabulary modeling, and results and analyses on\nthe closed vocabulary setting can be found in Ap-\npendix C. We also report UPP values (Ueberla,\n1994) in Appendix E.\n6.2 Generated Samples\nTo illustrate the behavior of the learned models,\nwe take the three models trained on WikiText-S\nand draw 10 samples while conditioning onG′and\ns = “Sonic the Hedgehog”, and show the sample\nwith lowest perplexity in Figure 3. Highlighted\nterms with different colors represent two types of\nmentions generated from the relation predictor:\nfull and partial. A full mention is an identical copy\nof an entity surface form, while a partial mention\nis an incomplete subphrase of an entity surface\nform. NKLM’s word-by-word generation scheme\nresults in partial mention being generated, while\nLRLM does not due to span-level copying from\nKGs. A perfect model should not generate partial\nmentions as it leads to possibly corrupted phrases,\nand should generate the same set of full mentions\nas the gold mentions.\nAlthough NKLM generates more mentions, it\nsuffers from generating partial mentions because\nit 1) is unaware of the length of entities, and 2) re-\nquires making copy decisions as many times as the\nnumber of tokens in a phrase. As a result, we often\nobserve NKLM switching entities or surface forms\nhalfway through, ending mentions early, and re-\npeating the same entity. In contrast, LRLM, by\ndesign, only generates full mentions.\nWe quantitatively show this in Table 3 by count-\ning the average number of partial and full men-\ntions in samples. We take 10 samples from 10 ran-\ndom development set articles. Next, we performed\na precursory manual annotation of “valid” men-\nBase model Dataset Dev Test\nVanilla LM NKLM LRLM Vanilla LM NKLM LRLM\nLSTM\nWikiFacts 219.11 93.09 89.55∗ 208.44 87.88 82.89∗\nWikiText-S 68.37 46.16 45.84 86.12 55.98 55.38\nWikiText-F 45.13 44.46 42.18∗ 49.47 48.54 45.70∗\nTransformer-XL\nWikiFacts 170.40 98.98 83.19∗∗ 162.65 92.92 76.46∗∗\nWikiText-S 42.63 43.05 37.75∗∗ 52.96 52.51 44.98∗∗\nWikiText-F 30.14 32.19 29.56∗∗ 33.01 35.27 32.20∗∗\nTable 2: Perplexity values of different models on open vocabulary language modeling, lower is better. Best results\nare in bold. Asterisk symbols represent statistical signiﬁcance according to Wilcoxon signed-rank test (Dror et al.,\n2018) against the better model among NKLM and Vanilla LM, with p< 0.05 (∗) and p< 0.01 (∗∗), respectively.\ns\t= Sonic\tthe\tHedgehog\t(1991\tvideo\tgame) \nSonic\tthe\tHedgehog [TITLE] \t Sonic\tthe\tHedgehog [TITLE] \t Sonic\tthe\tHedgehog [TITLE] \t(\t Jordon \tVS\t.\t スト\nリートファイター \t,\t Picardi \tSutorīto\tFaitā\t Jobs \tSutorīto\tFaitā\t)\tis\t A [CERO\trating] \tplatform\tvideo \ngame [instance\tof] \tdeveloped\tby\t Sonic\tT eam [developer] \tand\tpublished\tby\t Sega [publisher] \t.\tIn\t2008\t,\tthe \ngame [instance\tof] \twas\treleased\tfor\t Sega [platform] \t 3 [platform] \tand\t 1320 \tplatforms\tin\tMarch\t2009\t, \nreplacing\tthe\toriginal\t game [instance\tof] \t Sony [platform] \t Drive [platform] \tcounterpart\t Sonic\tthe \nHedgehog [characters] \tfor\tthe\t Android [platform]\t GameCube [platform] \t.\tIt\twas\tre\t-\treleased\ton\tMarch\t12\t, \n2010\t,\tin\t... \nNKLM: \nWarm-up:<s>\t=\tSonic\tthe\tHedgehog\t(\t1991\tvideo\tgame\t)\t=\t\nSonic\tthe\tHedgehog [TITLE] \t(\talso\tknown\tas\tSonic\tthe\tHedgehog\t3\tand\t Sonic [series] \tthe\tHedgehog\t2\t)\tis\ta \n1986\trole\t-\tplaying\tvideo\tgame\tdeveloped\tby\t Sonic\tT eam [developer] \tand\tpublished\tby\tSony\tComputer \nEntertainment\t(\t SEGA [publisher] \t)\tfor\tthe\t PlayStation\t3 [platform] \t(\t Xbox\t360 [platform] \t)\t.\tIt\twas \ndeveloped\tand\tpublished\tby\t Sega [publisher] \tin\t1997\tfor\tthe\tW ii\t,\tand\twas\tported\tas\ta\tthird\tinstallment\tin \nthe\t Sonic\tthe\tHedgehog [series] \tseries\tand\treleased\tin\tJapan\tin\t1996\t.\tOn\tthe\t... \nLRLM: \n(ω,\to)\t=\t{\n\t\t(<TITLE>,\tSonic\tthe\tHedgehog\t(1991\tvideo\tgame)),\n\t\t(<instance of>,\t\tvideo\tgame),\n\t\t(<CERO rating>,\t\tA),\n\t\t(<developer>,\t\t\t\t\t\tSonic\tTeam),\n\t\t(<publisher>,\t\t\t\t\t\tSega),\n\t\t(<platform>,\t\t\t\t\t\t\t\tSega\tMega\tDrive),\n\t\t(<platform>,\t\t\t\t\t\t\t\tWii),\n\t\t(<platform>,\t\t\t\t\t\t\t\tNintendo\tGameCube),\n\t\t(<platform>,\t\t\t\t\t\t\t\tXbox\t360),\n\t\t(<platform>,\t\t\t\t\t\t\t\tPlaystation\t3),\n\t\t(<platform>,\t\t\t\t\t\t\t\tAndroid),\n\t\t(<characters>,\t\t\t\tSonic\tthe\tHedgehog),\n\t\t(<series>,\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tSonic\tthe\tHedgehog\t(video\tgame\tseries)),\n\t\t...\n}\nSonic\tthe\tHedgehog\tis\tan\taction\taction\t-\tadventure\tvideo\tgame\tpublished\tby\tSonic\tof\tprogrammers\tfor\tthe \n1999\tNintendo\tGameCube\t's\tSNES\tvideo\tgame\tSonic\tthe\tHedgehog\t2\t.\tIt\twas\treleased\tfor\tthe\tNintendo\tDS \non\tSeptember\t16\t,\t1994\tin\tNorth\tAmerica\tand\tEurope\tin\tthe\tlatter\tpart\tof\tthe\toriginal\tHalo\t2\t.\tIt\twas\t played \nin\ta\tpost\t-\tapocalyptic\tfantasy\tfantasy\tuniverse\t,\tby\tNintendo\tComputer\tEntertainment\ton\tMarch\t6\t,\t1999 \nunder\tthe\t... \nVanilla\nLM: \nFigure 3: Samples from the three models for a topic entity “ Sonic the Hedgehog (1991 video game) ” with the\ncorresponding subgraph on the right. Square brackets denote the relation type of copied objects. Highlighted\nspans in light green represent objects that are copied in full, whereas those in dark red represent partially copied\nobjects. Underlined tokens are unknown words sampled from character model.\nPartial Full Valid Invalid\nNKLM 16.9 7.81 6.37 1.44\nLRLM − 6.32 5.63 0.69\nGold − 9.00 9.00 0.00\nTable 3: Average number of partially generated, fully\ngenerated, and valid mentions over 100 samples from\nthe development set or gold human-generated article.\ntions, which we deemed as semantically correct\nbased on the sentential context. NKLM generates\nmore invalid mentions than LRLM, most of which\nare false positives and repetitions of the same en-\ntity. LRLM has almost no repetitions, but some-\ntimes incorrectly predicts the article’s theme.10\n6.3 Posterior Probability of Spans\nOne of the advantages of our model is its capa-\nbility to calculate the posterior probability of a re-\nlation generating a span in an existing text. We\ncalculate the joint probability of a span and the\n10For example, generating an article about a TV episode\nfor a topic entity of a song.\nsurrounding text11 by marginalizing over the latent\nvariable Zfor both sides of context, and normalize\nover all possible spans:\nP(X,Z) =αi ·P(Z|x<ℓi) ·βi\nP(Z|X) =P(X,Z) /\n∑\nZ∈Z\nP(X,Z)\nwhere βi is the backward probability calcu-\nlated reversely following Section 3.3. Table 4\nshows spans with the posterior probability of var-\nious relation types from an article about “ Sorry\n(Madonna song) ”. The model demonstrates the\nability to relate the entity “Madonna” to the topic\nbased on context. We also observe a general\ntrend that the model prefers generating multi-word\nspans through relations rather than word by word\nfrom vocabulary. However, when generating com-\nmon phrases (e.g., “the United States”), our model\noften favors word-based generation even if an al-\nternative relation-based prediction is possible.\n11We consider the text segment in the batch where the span\nbelongs to as the surrounding text.\nTitle: Sorry (Madonna Song)\n... song by American singer Madonna from her tenth ...\nRelations:\n<performer> 0.9697\n<lyrics by> 0.0289\nword 0.0014\n... written and produced by Madonna and Stuart Price , ...\nRelations:\n<performer> 0.1545\n<lyrics by> 0.7693\nword 0.0762\n... continuation from the “ Hung Up ” music video . ...\nRelations: <follows> 1.0000\nword 0.0000\n... . However , in the United States , the song did ...\nRelations:\n<origin> 0.0000\nword →<origin> 0.0003\nword 0.9997\nTable 4: Posterior probability of spans (highlighted) in\ncontexts. word represents word-based generation. The\nsecond relation in the last example means generation of\n“the” using word, followed by relation-based genera-\ntion of “United States” using the <origin> relation.\n12 16 22 27 37 49 70 114 2041284\nBinned Number of Relations\n5.5\n5.0\n4.5\n4.0\n3.5\nLog-probability\nLRLM\nNKLM\nLM\nFigure 4: Word-average log-probabilities on develop-\nment set of WikiFacts grouped by average relations per\narticle. LRLM shows a larger gain over the baselines\nas the number of relations increases.\n6.4 Effect of Subgraph Size\nFinally, we measure the performance of models\nwith respect to the richness of resource available\nfor conditioning. We group WikiFacts articles into\n10 bins by the number of relations available, and\nplot binned word-average log-probabilities in Fig-\nure 4. While all models have slightly higher log-\nprobabilities as the number of relations increase,\nLRLM achieves the largest gain. We believe this is\ndue to marginalization over the latent variables in\nLRLM helping better disambiguate between many\ncandidates, while NKLM struggles to predict the\nright relations and surface form lengths as the\nnumber of candidates increases.\n7 Related Work\nA variety of entity-aware LMs exist, condition-\ning on a variety of information sources such as\nexpert coreference annotations (Ji et al., 2017;\nClark et al., 2018; Yang et al., 2017), entity an-\nnotations (Logan et al., 2019), deﬁnitions (Bah-\ndanau et al., 2017), or keywords (Kiddon et al.,\n2016; Parvez et al., 2018). As mentioned above,\nNKLM (Ahn et al., 2016) is the most relevant pre-\nvious work that uses relational information. Our\nproposed LRLM formulation is more successful\nat lowering perplexity and also allows calculating\nposterior probabilities of relations.\nIncorporating KGs for natural language gener-\nation (NLG) has a long history (Goldberg et al.,\n1994; Reiter et al., 2005; Chen and Mooney,\n2008). With the recent advancement of neural\nsequence modeling, prevalent approaches for lan-\nguage generation from KGs employ sequence-to-\nsequence models (Sutskever et al., 2014) with spe-\ncial attention mechanisms tailored for input struc-\ntures such as graphs (Wang et al., 2018) or ta-\nbles (Liu et al., 2018; Perez-Beltrachini and Lap-\nata, 2018). Unlike our focus, however, this class of\nresearch focuses on learning discriminative mod-\nels that do not explicitly generate the referent en-\ntity as latent variables, like we do in Section 6.3.\nWhile not directly related to our core task, there\nhave been a number of other methods for incorpo-\nrating latent variables into NLG problems. Latent\nstructure has included predicting latent sequences\nof topics (Wiseman et al., 2018), chunking of\nword sequences into n-grams (Buckman and Neu-\nbig, 2018), deciding between input sources (Ling\net al., 2016; Gu et al., 2016), predicting latent con-\ntinuous vectors (Bowman et al., 2016), generat-\ning compressed summary tokens (Miao and Blun-\nsom, 2016), or inducing syntactic and semantic\ntrees (Yogatama et al., 2016; Yin et al., 2018). Our\nwork borrows heavily from Ling et al. (2016), who\nselect from multiple sources for source code gen-\neration. We use a similar method for selecting la-\ntent sources for Wikipedia article language mod-\neling with a repository of KG triples.\n8 Conclusion\nIn this work, we propose Latent Relation Lan-\nguage Models, a class of conditional language\nmodels conditioned on knowledge graphs. Our\ngenerative framework models text as a sequence\nof spans, some of which are generated as enti-\nties included in the knowledge graph. Marginal-\nization over latent variables allows the model to\nnot only out-perform previous work in conditional\nlanguage modeling tasks, but also score spans with\ntheir posterior relation probability.\nAcknowledgements\nThis research was supported in part by Funai\nFoundation for Information Technology and Ama-\nzon. The authors would also like to thank Qian\nWang for helping designing the model ﬁgure and\nthe members of the NeuLab for helpful discussion.\nReferences\nSungjin Ahn, Heeyoul Choi, Tanel Pärnamaa, and\nYoshua Bengio. 2016. A neural knowledge lan-\nguage model. CoRR, arXiv:1608.00318.\nGabor Angeli, Percy Liang, and Dan Klein. 2010. A\nsimple domain-independent probabilistic approach\nto generation. In Proceedings of the 2010 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 502–512. Association for Com-\nputational Linguistics.\nPhilip Arthur, Graham Neubig, and Satoshi Nakamura.\n2016. Incorporating discrete translation lexicons\ninto neural machine translation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1557–1567. Asso-\nciation for Computational Linguistics.\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\nInternational Conference on Learning Representa-\ntions.\nDzmitry Bahdanau, Tom Bosc, Stanisław Jastrz˛ ebski,\nEdward Grefenstette, Pascal Vincent, and Yoshua\nBengio. 2017. Learning to compute word embed-\ndings on the ﬂy. CoRR, arXiv:1706.00286.\nRegina Barzilay and Mirella Lapata. 2005. Collective\ncontent selection for concept-to-text generation. In\nProceedings of Human Language Technology Con-\nference and Conference on Empirical Methods in\nNatural Language Processing, pages 331–338. As-\nsociation for Computational Linguistics.\nLeonard E. Baum, Ted Petrie, George Soules, and Nor-\nman Weiss. 1970. A maximization technique occur-\nring in the statistical analysis of probabilistic func-\ntions of markov chains. The Annals of Mathematical\nStatistics, 41(1):164–171.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch, 3(Feb):1137–1155.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: A collab-\noratively created graph database for structuring hu-\nman knowledge. In Proceedings of the 2008 ACM\nSIGMOD International Conference on Management\nof Data, SIGMOD ’08, pages 1247–1250. Associa-\ntion for Computing Machinery.\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Con-\nference on Computational Natural Language Learn-\ning, pages 10–21, Berlin, Germany. Association for\nComputational Linguistics.\nJacob Buckman and Graham Neubig. 2018. Neural lat-\ntice language models. Transactions of the Associa-\ntion for Computational Linguistics, 6:529–541.\nDiego Ceccarelli, Claudio Lucchese, Salvatore Or-\nlando, Raffaele Perego, and Salvatore Trani. 2013.\nLearning relatedness measures for entity linking. In\nProceedings of the 22nd ACM International con-\nference on Information & Knowledge Management,\npages 139–148. Association for Computing Machin-\nery.\nDavid L. Chen and Raymond J. Mooney. 2008. Learn-\ning to sportscast: A test of grounded language ac-\nquisition. In Proceedings of the 25th International\nConference on Machine Learning , pages 128–135.\nAssociation for Computing Machinery.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2017. Hierarchical multiscale recurrent neural net-\nworks. In International Conference on Learning\nRepresentations.\nElizabeth Clark, Yangfeng Ji, and Noah A. Smith.\n2018. Neural text generation in stories using en-\ntity representations as context. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 2250–2260. Association for Computa-\ntional Linguistics.\nAndrew M. Dai and Quoc V . Le. 2015. Semi-\nsupervised sequence learning. In Advances in Neu-\nral Information Processing Systems 28, pages 3079–\n3087.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nRotem Dror, Gili Baumer, Segev Shlomov, and Roi Re-\nichart. 2018. The hitchhiker’s guide to testing statis-\ntical signiﬁcance in natural language processing. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1383–1392, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nMariano Felice, Zheng Yuan, Øistein E. Andersen, He-\nlen Yannakoudakis, and Ekaterina Kochmar. 2014.\nGrammatical error correction using hybrid systems\nand type ﬁltering. In Proceedings of the Eigh-\nteenth Conference on Computational Natural Lan-\nguage Learning: Shared Task , pages 15–24, Bal-\ntimore, Maryland. Association for Computational\nLinguistics.\nOctavian-Eugen Ganea and Thomas Hofmann. 2017.\nDeep joint entity disambiguation with local neural\nattention. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 2619–2629, Copenhagen, Denmark. As-\nsociation for Computational Linguistics.\nEli Goldberg, Norbert Driedger, and Richard I Kit-\ntredge. 1994. Using natural-language processing to\nproduce weather forecasts. IEEE Expert, 9(2):45–\n53.\nÉdouard Grave, Armand Joulin, Moustapha Cissé,\nDavid Grangier, and Hervé Jégou. 2017. Efﬁcient\nsoftmax approximation for GPUs. In Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine\nLearning Research, pages 1302–1310, International\nConvention Centre, Sydney, Australia. PMLR.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. In Proceedings of\nthe 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1631–1640, Berlin, Germany. Association for\nComputational Linguistics.\nXu Han, Shulin Cao, Xin Lv, Yankai Lin, Zhiyuan Liu,\nMaosong Sun, and Juanzi Li. 2018. OpenKE: An\nopen toolkit for knowledge embedding. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing: System Demon-\nstrations, pages 139–144. Association for Compu-\ntational Linguistics.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nYangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin\nChoi, and Noah A. Smith. 2017. Dynamic entity\nrepresentations in neural language models. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1830–\n1839, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nChloé Kiddon, Luke Zettlemoyer, and Yejin Choi.\n2016. Globally coherent text generation with neu-\nral checklist models. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 329–339. Association for\nComputational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nIoannis Konstas and Mirella Lapata. 2013. A global\nmodel for concept-to-text generation. Journal of Ar-\ntiﬁcial Intelligence Research, 48:305–346.\nRémi Lebret, David Grangier, and Michael Auli. 2016.\nNeural text generation from structured data with ap-\nplication to the biography domain. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 1203–1213.\nAssociation for Computational Linguistics.\nWang Ling, Phil Blunsom, Edward Grefenstette,\nKarl Moritz Hermann, Tomáš Ko ˇciský, Fumin\nWang, and Andrew Senior. 2016. Latent predictor\nnetworks for code generation. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n599–609. Association for Computational Linguis-\ntics.\nTianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,\nand Zhifang Sui. 2018. Table-to-text generation by\nstructure-aware seq2seq learning. Proceedings of\nthe Thirty-Second AAAI Conference on Artiﬁcial In-\ntelligence.\nRobert Logan, Nelson F. Liu, Matthew E. Peters, Matt\nGardner, and Sameer Singh. 2019. Barack’s wife\nHillary: Using knowledge graphs for fact-aware lan-\nguage modeling. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5962–5971, Florence, Italy. Associa-\ntion for Computational Linguistics.\nMinh-Thang Luong and Christopher D. Manning.\n2016. Achieving open vocabulary neural machine\ntranslation with hybrid word-character models. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 1054–1063. Association for\nComputational Linguistics.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017a. Regularizing and optimizing LSTM\nlanguage models. CoRR, arXiv:1708.02182.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017b. Pointer sentinel mixture\nmodels. In International Conference on Learning\nRepresentations.\nYishu Miao and Phil Blunsom. 2016. Language as a\nlatent variable: Discrete generative models for sen-\ntence compression. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 319–328, Austin, Texas. Associa-\ntion for Computational Linguistics.\nTomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nMike Mintz, Steven Bills, Rion Snow, and Daniel Ju-\nrafsky. 2009. Distant supervision for relation ex-\ntraction without labeled data. In Proceedings of\nthe Joint Conference of the 47th Annual Meeting of\nthe ACL and the 4th International Joint Conference\non Natural Language Processing of the AFNLP ,\npages 1003–1011, Suntec, Singapore. Association\nfor Computational Linguistics.\nGraham Neubig and Chris Dyer. 2016. Generalizing\nand hybridizing count-based and neural language\nmodels. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1163–1172, Austin, Texas. Association\nfor Computational Linguistics.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2018. Building language\nmodels for text with named entities. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2373–2383, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nAdam Paszke, Sam Gross, Soumith Chintala, Gre-\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\ning Lin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in PyTorch.\nLaura Perez-Beltrachini and Mirella Lapata. 2018.\nBootstrapping generators from noisy data. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long Papers), pages 1516–1527. Association\nfor Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237. Association for Computational Linguistics.\nFrancesco Piccinno and Paolo Ferragina. 2014. From\nTagME to W AT: a new entity annotator. InProceed-\nings of the First International Workshop on Entity\nRecognition & Disambiguation, pages 55–62. Asso-\nciation for Computing Machinery.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learners.\nPreprint.\nEhud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,\nand Ian Davy. 2005. Choosing words in computer-\ngenerated weather forecasts. Artiﬁcial Intelligence,\n167(1-2):137–169.\nGerard Salton and Michael J. McGill. 1986. Intro-\nduction to Modern Information Retrieval. McGraw-\nHill, Inc., New York, NY , USA.\nMartin Sundermeyer, Ralf Schlüter, and Hermann Ney.\n2012. LSTM neural networks for language model-\ning. In Thirteenth Annual Conference of the Inter-\nnational Speech Communication Association.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in Neural Information Process-\ning Systems 27, pages 3104–3112.\nYaohua Tang, Fandong Meng, Zhengdong Lu, Hang\nLi, and Philip LH Yu. 2016. Neural machine\ntranslation with external phrase memory. CoRR,\narXiv:1606.01792.\nJoerg Ueberla. 1994. Analysing a simple language\nmodel·some general conclusions for language mod-\nels for speech recognition. Computer Speech & Lan-\nguage, 8(2):153–176.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commu-\nnications of the ACM, 57(10):78–85.\nQingyun Wang, Xiaoman Pan, Lifu Huang, Boliang\nZhang, Zhiying Jiang, Heng Ji, and Kevin Knight.\n2018. Describing a knowledge base. In Proceed-\nings of the 11th International Conference on Natu-\nral Language Generation, pages 10–21. Association\nfor Computational Linguistics.\nRonald J. Williams and Jing Peng. 1990. An efﬁcient\ngradient-based algorithm for on-line training of re-\ncurrent network trajectories. Neural Computation,\n2(4):490–501.\nSam Wiseman, Stuart Shieber, and Alexander Rush.\n2018. Learning neural templates for text genera-\ntion. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 3174–3187, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2018. Breaking the softmax\nbottleneck: A high-rank RNN language model. In\nInternational Conference on Learning Representa-\ntions.\nZichao Yang, Phil Blunsom, Chris Dyer, and Wang\nLing. 2017. Reference-aware language models. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n1850–1859. Association for Computational Linguis-\ntics.\nPengcheng Yin, Chunting Zhou, Junxian He, and Gra-\nham Neubig. 2018. StructV AE: Tree-structured la-\ntent variable models for semi-supervised semantic\nparsing. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 754–765. Associa-\ntion for Computational Linguistics.\nDani Yogatama, Phil Blunsom, Chris Dyer, Edward\nGrefenstette, and Wang Ling. 2016. Learning to\ncompose words into sentences with reinforcement\nlearning. In International Conference on Learning\nRepresentations.\nGeorge Kingsley Zipf. 1949. Human behavior and the\nprinciple of least effort: An introduction to human\neoclogy. Addison-Wesley Press.\nA Article Collection\nWe collect seed Wikipedia articles from the raw\nrelease of WikiText-103 (Merity et al., 2017b),\nwhere raw vocabulary was preserved. Mini-\nmal preprocessing was performed by the dataset\nproviders.12 The dataset provides an open domain,\nquality-assured set of Wikipedia articles veriﬁed\nby editors. We take the dataset and split each set\nback into per-article texts with simple regular ex-\npression rules for detecting titles. Then we query\nthe Wikipedia API to identify the Wikidata en-\ntity13 for each article. During this process, we dis-\ncarded some articles in the training set where the\nAPI failed to return Wikidata IDs, which was due\nto their deletion or title renames since the release\nof original dataset in 2016. For development and\ntest set, we manually matched the few missed arti-\ncles to recover all the articles.\nB Training Details and Hyperparameters\nTraining Details All models are trained using\nAdam (Kingma and Ba, 2015). Models equipped\nwith Transformer-XL are trained with the same\nschedule as the original paper; the learning rate\nis linearly increased over the ﬁrst 6000 gradient\nsteps up to 0.00025, and reduced according to co-\nsine annealing. Models with LSTM are trained\nwith the initial learning rate set to 0.001. Valida-\ntion is performed on the development set after ev-\nery epoch, and when validation loss does not im-\nprove, learning rate is multiplied by 0.9 and the\nmodel and optimizer parameters are reset to the\nprevious checkpoint. For all experiments, we use\ntruncated backpropagation through time (Williams\nand Peng, 1990) with the truncation window size\nbeing 150.\nHyperparameters We list the common model\nhyperparameters for the model in Table 5. While\nwe use the same Transformer-XL hyperparameters\nacross datasets, we apply different sets of LSTM\nhyperparameters on WikiFacts, WikiText-S, and\nWikiText-F for better performance. See Section 5\nfor more details on the vocabulary size. We take\npre-trained KG embeddings from OpenKE (Han\net al., 2018), with dimensions of 50 and 100 for\n12Data can be found at https://s3.amazonaws.\ncom/research.metamind.io/wikitext/\nwikitext-103-raw-v1.zip .\n13We used a Wikidata dump as of 2018/09/20.\nCommon hyperparameters\nLearning rate decay rate 0.9\nBatch size 60\nBPTT window size 150\nEntity embedding size 50/100/100\nfastText embedding size 300\nTransformer-XL hyperparameters\nLearning rate 0.00025\nWarm up steps 6000\nAttention dropout rate 0\nDropout rate 0.1\nEmbedding size 410\nFC layer hidden unit size 2100\nMemory size 150\nNumber of layers 16\nNumber of heads 10\nPer-head attention dimension 41\nLSTM hyperparameters\nLearning rate 0.001\nDropout rate 0.5 / 0.5 / 0.1\nEmbedding size 400 / 400 / 512\nHidden unit size 1000 / 1000 / 1024\nLinear hidden unit size 1000 / 1000 / 500\nNumber of layers 2/2/4\nLRLM-speciﬁc hyperparameters\nRelation linear hidden unit size 1000 / 1000 / 800\nNKLM-speciﬁc hyperparameters\nMax position count 20\nPosition embedding size 40 / 40 / 50\nTable 5: Model and training hyperparameters that are\ncommon across the models. Slash-delimited values\nrepresent different hyperparameters used in WikiFacts,\nWikiText-S, WikiText-F, respectively.\nWikiFacts and WikiText respectively.14\nC Utilization of Extra Entities\nAdding extra entities to WikiFacts increased the\naverage number of relations per article from 82.71\nto 89.28, and mentions from 9.64 to 16.97. On\naverage, each added entity matches 1.12 spans.\nTable 6 compares results under different set-\ntings. The inclusion of extra entities signiﬁcantly\nimproves results for both models. This is due to\nthe fact that these entities are extracted from hy-\nperlinks within text, so 1) they are mostly rare\nwords; 2) the model can easily learn that all such\nentities must be included in the text at some point.\nD Data Preprocessing for NKLM\nWikiFacts The provided WikiFacts dataset con-\ntains KG subgraphs and text annotated with non-\n14Ahn et al. (2016) uses 100-d KG embeddings, but there\nwere no publicly available embeddings in that dimension.\nDataset Dev Test\nVanilla LM NKLM LRLM Vanilla LM NKLM LRLM\nWikiFacts 217.19 95.68 94.64 207.54 90.44 87.73\n+ Entity 217.19 59.84 54.60 207.54 57.14 51.34\n+ Oracle char model 88.03 38.54 34.73 84.56 37.23 33.02\n(Ahn et al., 2016) 82.4 41.4 – 86.4 43.6 –\nTable 6: Perplexity values of models on WikiFacts, lower is better. “ + Entity” means trained with extra entities;\n“+ Oracle char model” means treating the character model as oracle, i.e., treating spell-out probabilities of OOV\nwords as 1. Best results are in bold. Note that our results are not directly comparable with reported results by Ahn\net al. (2016) due to different dataset splits being used.\nBase model Dataset Dev Test\nVanilla LM NKLM LRLM Vanilla LM NKLM LRLM\nLSTM\nWikiFacts 156.29 74.04 71.20∗ 148.05 70.08 66.09∗\nWikiText-S 65.42 49.95 44.44∗∗ 80.69 60.96 52.81∗∗\nWikiText-F 43.59 42.99 40.88∗∗ 47.14 46.37 43.72∗∗\nTransformer-XL\nWikiFacts 121.55 78.72 66.14∗∗ 115.53 74.09 60.96∗∗\nWikiText-S 40.79 41.59 37.75∗∗ 49.62 49.92 42.76∗∗\nWikiText-F 29.11 32.19 28.59∗∗ 31.45 33.69 30.75∗∗\nTable 7: UPP of different models, lower is better. Best results are in bold. Asterisk symbols represent statistical\nsigniﬁcance according to Wilcoxon signed-rank test (Dror et al., 2018) against the better model among NKLM\nand Vanilla LM, with p< 0.05 (∗) and p< 0.01 (∗∗), respectively.\noverlapping matched spans. Copying positions 15\nare sequentially assigned within a matched span.\nOne caveat is that the dataset includes relations\ncontaining Freebase Compound Value Type(CVT)\nas entities. These types are used to encapsulate a\nstructured representation with multiple ﬁelds. We\nremoved all relations where the subject entity is\na CVT. For relations where the object entity is a\nCVT, we substitute it with multiple relations us-\ning the ﬁeld types and values of the CVT. Without\nCVT-based relations, each article has on average\n37.67 relations.\nWikiText The WikiText-derived dataset is con-\nstructed using the methods described in Section 4.\nThe methods can potentially match overlapping\nspans, which cannot be handled by NKLM.\nThus, we prune the set of matching spans for\neach article so that no two spans overlap. Prun-\ning is done by iterating over the spans in a pre-\ndeﬁned order, and greedily selecting spans that do\nnot overlap with previously selected spans. The\nspans are ordered by the following criteria:\n•In descending order of span length. (Prefer\nlonger spans)\n15Copying position of a word is the 0-based word index\ninto the matching entity surface form, which indicates the po-\nsition to copy from.\n•In ascending order of span starting index. (Pre-\nfer spans appearing earlier)\n•Order spans that match entity canonical forms\n(the ﬁrst surface form in list) in front. (Prefer\nspans matching canonical forms)\n•Ties are broken by relation type ID and index of\nmatched surface form.\nWhile NKLM supports partial and arbitrary-\norder entity matches by specifying copying posi-\ntions,16 we do not perform this kind of matching\nas it greatly increases the complexity of the match-\ning algorithm, and could produce more false pos-\nitives. We sequentially assign copying positions\nwithin matched spans as in WikiFacts.\nE Comparison of Models using UPP\nWe show the main results evaluated according to\nUPP (Ueberla, 1994) in Table 7. This adjusted per-\nplexity measure penalizes unknown word proba-\nbilities by a constant value of1/|Vout|, where Vout\nis the set of OOV words in a corpus.\n16For example, the entity “ Barack Hussein Obama ” can\nmatch the text “Obama Barack” with copying positions 2\nand 0."
}