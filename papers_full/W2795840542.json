{
  "title": "End-to-End Dense Video Captioning with Masked Transformer",
  "url": "https://openalex.org/W2795840542",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A19601303",
      "name": "Zhou, Luowei",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2362876091",
      "name": "Zhou, Yingbo",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4224634901",
      "name": "Corso, Jason J.",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3176399732",
      "name": "Socher, Richard",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2751801181",
      "name": "Xiong, Caiming",
      "affiliations": [
        "Salesforce (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1957740064",
    "https://openalex.org/W2099614498",
    "https://openalex.org/W2394849137",
    "https://openalex.org/W2136985729",
    "https://openalex.org/W2519328139",
    "https://openalex.org/W2953022248",
    "https://openalex.org/W2610163825",
    "https://openalex.org/W2950477994",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2951912364",
    "https://openalex.org/W2951183276",
    "https://openalex.org/W2765277449",
    "https://openalex.org/W2185243164",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W411141150",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2953229046",
    "https://openalex.org/W2964241990",
    "https://openalex.org/W2950019618",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2950307714",
    "https://openalex.org/W2486996822",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2951548327",
    "https://openalex.org/W2402902088",
    "https://openalex.org/W2784025607",
    "https://openalex.org/W1995820507",
    "https://openalex.org/W2953106684",
    "https://openalex.org/W2558834163",
    "https://openalex.org/W2951159095",
    "https://openalex.org/W2560313346",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2950304420",
    "https://openalex.org/W2755876276",
    "https://openalex.org/W1927052826",
    "https://openalex.org/W2558401060"
  ],
  "abstract": "Dense video captioning aims to generate text descriptions for all events in an untrimmed video. This involves both detecting and describing events. Therefore, all previous methods on dense video captioning tackle this problem by building two models, i.e. an event proposal and a captioning model, for these two sub-problems. The models are either trained separately or in alternation. This prevents direct influence of the language description to the event proposal, which is important for generating accurate descriptions. To address this problem, we propose an end-to-end transformer model for dense video captioning. The encoder encodes the video into appropriate representations. The proposal decoder decodes from the encoding with different anchors to form video event proposals. The captioning decoder employs a masking network to restrict its attention to the proposal event over the encoding feature. This masking network converts the event proposal to a differentiable mask, which ensures the consistency between the proposal and captioning during training. In addition, our model employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements. We demonstrate the effectiveness of this end-to-end model on ActivityNet Captions and YouCookII datasets, where we achieved 10.12 and 6.58 METEOR score, respectively.",
  "full_text": "End-to-End Dense Video Captioning with Masked Transformer\nLuowei Zhou∗\nUniversity of Michigan\nluozhou@umich.edu\nYingbo Zhou∗\nSalesforce Research\nyingbo.zhou@salesforce.com\nJason J. Corso\nUniversity of Michigan\njjcorso@eecs.umich.edu\nRichard Socher\nSalesforce Research\nrichard@socher.org\nCaiming Xiong†\nSalesforce Research\ncxiong@salesforce.com\nAbstract\nDense video captioning aims to generate text descrip-\ntions for all events in an untrimmed video. This involves\nboth detecting and describing events. Therefore, all previ-\nous methods on dense video captioning tackle this problem\nby building two models, i.e. an event proposal and a cap-\ntioning model, for these two sub-problems. The models are\neither trained separately or in alternation. This prevents di-\nrect inﬂuence of the language description to the event pro-\nposal, which is important for generating accurate descrip-\ntions. To address this problem, we propose an end-to-end\ntransformer model for dense video captioning. The encoder\nencodes the video into appropriate representations. The\nproposal decoder decodes from the encoding with different\nanchors to form video event proposals. The captioning de-\ncoder employs a masking network to restrict its attention to\nthe proposal event over the encoding feature. This mask-\ning network converts the event proposal to a differentiable\nmask, which ensures the consistency between the proposal\nand captioning during training. In addition, our model em-\nploys a self-attention mechanism, which enables the use of\nefﬁcient non-recurrent structure during encoding and leads\nto performance improvements. We demonstrate the effec-\ntiveness of this end-to-end model on ActivityNet Captions\nand YouCookII datasets, where we achieved 10.12 and 6.58\nMETEOR score, respectively.\n1. Introduction\nVideo has become an important source for humans to\nlearn and acquire knowledge ( e.g. video lectures, making\nsandwiches [21], changing tires [1]). Video content con-\nsumes high cognitive bandwidth, and thus is slow for hu-\nmans to digest. Although the visual signal itself can some-\n∗Equal contribution\n†Corresponding author\nCNN CNN CNNӗ\nSelf-Attention Layer\nFeed \nForward\nFeed \nForward\nFeed \nForward\nTemporal Convolutional Networks\n{ < s 1 , e 1 , p 1 >, < s 2 , e 2 , p 2 >, ···}\nx 2\nEmb ӗ\nSelf-Attention \nLayer\nMulti-head \nAttention \nposition \nencoding\nposition \nencoding\nFeed \nForward\n Softmax\nX\nMasking\n2 x\nw 2\nj\n(shifted right)\nw 2\nj + 1\nVideo \nEncoder \nStack\nProposal \nDecoder\nCaption \nDecoder\nt\n Linear\nFigure 1. Dense video captioning is to localize (temporal) events\nfrom a video, which are then described with natural language sen-\ntences. We leverage temporal convolutional networks and self-\nattention mechanisms for precise event proposal generation and\ncaptioning.\ntimes disambiguate certain semantics, one way to make\nvideo content more easily and rapidly understood by hu-\nmans is to compress it in a way that retains the seman-\ntics. This is particularly important given the massive\namount of video being produced everyday. Video sum-\nmarization [42] is one way of doing this, but it loses the\nlanguage components of the video, which are particularly\nimportant in instructional videos. Dense video caption-\ning [20]—describing events in the video with descriptive\nnatural language—is another way of achieving this com-\npression while retaining the language components.\nDense video captioning can be decomposed into two\nparts: event detection and event description. Existing meth-\nods tackle these two sub-problems using event proposal and\ncaptioning modules, and exploit two ways to combine them\nfor dense video captioning. One way is to train the two\n1\narXiv:1804.00819v1  [cs.CV]  3 Apr 2018\nmodules independently and generate descriptions for the\nbest event proposals with the best captioning model [13].\nThe other way is to alternate training [20] between the\ntwo modules, i.e., alternate between i) training the pro-\nposal module only and ii) training the captioning module on\nthe positive event proposals while ﬁne-tuning the proposal\nmodule. However, in either case, the language information\ncannot have direct impacts on the event proposal.\nIntuitively, the video event segments and language are\nclosely related and the language information should be able\nto help localize events in the video. To this end, we pro-\npose an encoder-decoder based end-to-end model for doing\ndense video captioning (see Fig. 1). The encoder encodes\nthe video frames (features) into the proper representation.\nThe proposal decoder then decodes this representation with\ndifferent anchors to form event proposals, i.e., start and end\ntime of the event, and a conﬁdence score. The captioning\ndecoder then decodes the proposal speciﬁc representation\nusing a masking network, which converts the event proposal\ninto a differentiable mask. This continuous mask enables\nboth the proposal and captioning decoder to be trained con-\nsistently, i.e. the proposal module now learns to adjust its\nprediction based on the quality of the generated caption. In\nother words, the language information from caption now is\nable to guide the visual model to generate more plausible\nproposals. In contrast to the existing methods where the\nproposal module solves a class-agnostic binary classiﬁca-\ntion problem regardless the details in the video content, our\nmodel enforces the consistency between the content in the\nproposed video segment and the semantic information in the\nlanguage description.\nAnother challenge for dense video captioning, and more\nbroadly for sequence modeling tasks, is the need to learn a\nrepresentation that is capable of capturing long term depen-\ndencies. Recurrent Neural Networks (RNN) are possible\nsolutions to this problem, however, learning such represen-\ntation is still difﬁcult [24]. Self-attention [22, 25, 30] al-\nlows for an attention mechanism within a module and is a\npotential way to learn this long-range dependence. In self-\nattention the higher layer in the same module is able to at-\ntend to all states below it. This made the length of the paths\nof states from the higher layer to all states in the lower layer\nto be one, and thus facilitates more effective learning. The\nshorter path length facilitates learning these dependencies\nbecause larger gradients can now pass to all states. Trans-\nformer [30] implements a fast self-attention mechanism and\nhas demonstrated its effectiveness in machine translation.\nUnlike traditional sequential models, transformer does not\nrequire unrolling across time, and therefore trains and tests\nmuch faster as compared to RNN based models. We employ\ntransformer in both the encoder and decoder of our model.\nOur main contributions are twofold. First, we propose\nan end-to-end model for doing dense video captioning. A\ndifferentiable masking scheme is proposed to ensure the\nconsistency between proposal and captioning module dur-\ning training. Second, we employ self-attention: a scheme\nthat facilitates the learning of long-range dependencies to\ndo dense video captioning. To the best of our knowledge,\nour model is the ﬁrst one that does not use a RNN-based\nmodel for doing dense video captioning. In addition, we\nachieve competitive results on ActivityNet Captions [20]\nand YouCookII [43] datasets.\n2. Related Work\nImage and Video Captioning.In contrast to earlier video\ncaptioning papers, which are based on models like hidden\nMarkov models and ontologies [40, 7], recent work on cap-\ntioning is dominated by deep neural network-based meth-\nods [33, 35, 38, 44, 37, 27]. Generally, they use Convolu-\ntional Neural Networks (CNNs) [29, 16] for encoding video\nframes, followed by a recurrent language decoder, e.g.,\nLong Short-Term Memory [18]. They vary mainly based\non frame encoding, e.g., via mean-pooling [32, 11], recur-\nrent nets [8, 31], and attention mechanisms [36, 23, 11].\nThe attention mechanism was initially proposed for ma-\nchine translation [3] and has achieved top performance in\nvarious language generation tasks, either as temporal atten-\ntion [36], semantic attention [11] or both [23]. Our work\nfalls into the ﬁrst of the three types. In addition to using\ncross-module attention, we apply self-attention [30] within\neach module.\nTemporal Action Proposals. Temporal action proposals\n(TAP) aim to temporally localize action-agnostic propos-\nals in a long untrimmed video. Existing methods formu-\nlate TAP as a binary classiﬁcation problem and differ in\nhow the proposals are proposed and discriminated from the\nbackground. Shuo et al. [28] propose and classify pro-\nposal candidates directly over video frames in a sliding win-\ndow fashion, which is computationally expensive. More re-\ncently, inspired by the anchoring mechanism from object\ndetection [26], two types of methods have been proposed—\nexplicit anchoring [12, 43] and implicit anchoring [9, 5].\nIn the former case, each anchor is an encoding of the vi-\nsual features between the anchor temporal boundaries and\nis classﬁed as action or background. In implicit anchor-\ning, recurrent networks encode the video sequence and, at\neach anchor center, multiple anchors with various sizes are\nproposed based on the same visual feature. So far, ex-\nplicit anchoring methods accompanied with location regres-\nsion yield better performance [12]. Our proposal module is\nbased upon Zhou et al. [43], which is designed to detect long\ncomplicated events rather than actions. We further improve\nthe framework with a temporal convolutional proposal net-\nwork and self-attention based context encoding.\nDense Video Captioning. The video paragraph caption-\ning method proposed by Yu et al. [41] generates sentence\ndescriptions for temporally localized video events. How-\never, the temporal locations of each event are provided be-\nforehand. Das et al. [7] generates dense captions over the\nentire video using sparse object stitching, but their work re-\nlies on a top-down ontology for the actual description and\nis not data-driven like the recent captioning methods. The\nmost similar work to ours is Krishna et al. [20] who intro-\nduce a dense video captioning model that learns to propose\nthe event locations and caption each event with a sentence.\nHowever, they combine the proposal and the captioning\nmodules through co-training and are not able to take ad-\nvantage of language to beneﬁt the event proposal [17]. To\nthis end, we propose an end-to-end framework for doing\ndense video captioning that is able to produce proposal and\ndescription simultaneously. Also, our work directly incor-\nporates the semantics from captions to the proposal module.\n3. Preliminary\nIn this section we introduce some background on Trans-\nformer [30], which is the building block for our model. We\nstart by introducing the scaled dot-product attention, which\nis the foundation of transformer. Given a query qi ∈Rd\nfrom all T′ queries, a set of keys kt ∈ Rd and values\nvt ∈ Rd where t = 1 ,2,...,T , the scaled dot-product\nattention outputs a weighted sum of values vt, where the\nweights are determined by the dot-products of query qand\nkeys kt. In practice, we pack kt and vt into matricies\nK = (k1,...,k T) and V = (v1,...,v T), respectively. The\nattention output on query qis:\nA(qi,K,V ) = V\nexp\n{\nKTqi/\n√\nd\n}\n∑T\nt=1 exp{kT\nt qi/\n√\nd}\n(1)\nThe multi-head attention consists of H paralleled scaled\ndot-product attention layers called “head”, where each\n“head” is an independent dot-product attention. The atten-\ntion output from multi-head attention is as below:\nMA(qi,K,V ) = WO\n\n\nhead1\n···\nheadH\n\n (2)\nheadj = A(Wq\njqi,WK\nj K,WV\nj V) (3)\nwhere Wq\nj,WK\nj ,WV\nj ∈R\nd\nH ×d are the independent head\nprojection matrices, j = 1,2,...,H , and WO ∈Rd×d.\nThis formulation of attention is quite general, for exam-\nple when the query is the hidden states from the decoder,\nand both the keys and values are all the encoder hidden\nstates, it represents the common cross-module attention.\nSelf-attention [30] is another case of multi-head attention\nwhere the queries, keys and values are all from the same\nhidden layer (see also in Fig. 2).\nEmbӗ\nSelf-Attention \nLayer\nFeed \nForward\nEmb ӗ\nSelf-Attention \nLayer\nMulti-head \nAttention \nFeed \nForward\n Softmax\nK V Q\nEncoder\nDecoder\nposition \nencoding\nposition \nencodingInputs Outputs\n(shifted right)\n Linear\nFigure 2. Transformer with 1-layer encoder and 1-layer decoder.\nNow we are ready to introduce Transformer model,\nwhich is an encoder-decoder based model that is origi-\nnally proposed for machine translation [30]. The building\nblock for Transformer is multi-head attention and a point-\nwise feed-forward layer. The pointwise feed-forward layer\ntakes the input from multi-head attention layer, and fur-\nther transforms it through two linear projections with ReLU\nactivation. The feed-forward layer can also be viewed as\ntwo convolution layers with kernel size one. The encoder\nand decoder of Transformer is composed by multiple such\nbuilding blocks, and they have the same number of layers.\nThe decoder from each layer takes input from the encoder\nof the same layer as well as the lower layer decoder out-\nput. Self-attention is applied to both encoder and decoder.\nCross-module attention between encoder and decoder is\nalso applied. Note that the self-attention layer in the de-\ncoder can only attend to the current and previous positions\nto preserve the auto-regressive property. Residual connec-\ntion [16] is applied to all input and output layers. Addition-\nally, layer normalization [2] (LayerNorm) is applied to all\nlayers. Fig. 2 shows a one layered transformer.\n4. End-to-End Dense Video Captioning\nOur end-to-end model is composed of three parts: a\nvideo encoder, a proposal decoder, and a captioning de-\ncoder that contains a mask prediction network to generate\ntext description from a given proposal. The video encoder\nis composed of multiple self-attention layers. The proposal\ndecoder takes the visual features from the encoder and out-\nputs event proposals. The mask prediction network takes\nthe proposal output and generates a differentiable mask for\na certain event proposal. To make the decoder caption the\ncurrent proposal, we then apply this mask by element-wise\nmultiplication between it, the input visual embedding and\nall outputs from proposal encoder. In the following sec-\ntions, we illustrate each component of our model in detail.\n4.1. Video Encoder\nEach frame xtof the videoX = {x1,...,x T}is ﬁrst en-\ncoded to a continuous representation F0 = {f0\n1 ,...,f 0\nT}.\nIt is then fed forward toLencoding layers, where each layer\nlearns a representation Fl+1 = V(Fl) by taking input from\nprevious layer l,\nV(Fl) = Ψ(PF(Γ(Fl)),Γ(Fl)) (4)\nΓ(Fl) =\n\n\nΨ(MA(fl\n1,Fl,Fl),fl\n1)⊤\n···\nΨ(MA(fl\nT,Fl,Fl),fl\nT)⊤\n\n\n⊤\n(5)\nΨ(α,β) = LayerNorm(α+ β) (6)\nPF(γ) = Ml\n2 max(0,Ml\n1γ+ bl\n1) + bl\n2 (7)\nwhere Ψ(·) represents the function that performs layer nor-\nmalization on the residual output, PF (·) denotes the 2-\nlayered feed-forward neural network with ReLU nonlinear-\nity for the ﬁrst layer, Ml\n1, Ml\n2 are the weights for the feed-\nforward layers, and bl\n1, bl\n2 are the biases. Notice the self-\nattention used in eq. 5. At each time step t, fl\nt is given as\nthe query to the attention layer and the output is the weight\nsum of fl\nt, t = 1,2,...,T , which encodes not only the in-\nformation regarding the current time step, but also all other\ntime steps. Therefore, each time step of the output from the\nself-attention is able to encode all context information. In\naddition, it is easy to see that the length of the path between\ntime steps is only one. In contrast to recurrent models, this\nmakes the gradient update independent with respect to their\nposition in time, and thus makes learning potential depen-\ndencies amongst distant frames easier.\n4.2. Proposal Decoder\nOur event proposal decoder is based on ProcNets [43],\nfor its state-of-the-art performance on long dense event pro-\nposals. We adopt the same anchor-offset mechanism as in\nProcNets and design a set of N explicit anchors for event\nproposals. Each anchor-based proposal is represented by an\nevent proposal score Pe ∈[0,1] and two offsets: center θc\nand length θl. The associated anchor has length la and cen-\nter ca. The proposal boundaries (Sp, Ep) are determined by\nthe anchor locations and offsets:\ncp = ca + θcla lp = laexp{θl},\nSp = cp −lp/2 Ep = cp + lp/2. (8)\nThese proposal outputs are obtained from temporal convo-\nlution (i.e. 1-D convolutions) applied on the last layer out-\nput of the visual encoder. The score indicates the likelihood\nfor a proposal to be an event. The offsets are used to ad-\njust the proposed segment boundaries from the associated\nanchor locations. We made following changes to ProcNets:\n•The sequential prediction module in ProcNets is re-\nmoved, as the event segments in a video are not closely\ncoupled and the number of events is small in general.\n•Use input from a multi-head self-attention layer in-\nstead of a bidirectional LSTM (Bi-LSTM) layer [15].\n•Use multi-layer temporal convolutions to generate the\nproposal score and offsets. The temporal convolutional\nnetwork contain three 1-D conv. layers, with batch\nnormalization [19]. We use ReLU activation for hid-\nden layers.\n•In our model, the conv. stride depends on kernel size\n(⌈kernelsize\ns ⌉) versus always 1 in ProcNets1.\nWe encode the video context by a self-attention layer\nas it has potential to learn better context representation.\nChanging stride size based on kernel size reduces the num-\nber of longer proposals so that the training samples is more\nbalanced, because a larger kernel size makes it easier to get\ngood overlap with ground truth. It also speeds up training\nas the number of long proposals is reduced.\n4.3. Captioning Decoder\nMasked Transformer. The captioning decoder takes in-\nput from both the visual encoder and the proposal decoder.\nGiven a proposal tuple (Pe,Sp,Ep) and visual representa-\ntions {F1,...,F L}, the L-layered captioning decoder gen-\nerates the t-th word by doing the following\nYl+1\n≤t = C(Yl\n≤t) = Ψ(PF(Φ(Yl\n≤t)),Φ(Yl\n≤t)) (9)\nΦ(Yl\n≤t) =\n\n\nΨ(MA(Ω(Yl\n≤t)1, ˆFl, ˆFl),Ω(Yl\n≤t)1)\n···\nΨ(MA(Ω(Yl\n≤t)t, ˆFl, ˆFl),Ω(Yl\n≤t)t)\n\n (10)\nΩ(Yl\n≤t) =\n\n\nΨ(MA(yl\n1,Y l,Y l),yl\n1)⊤\n···\nΨ(MA(yl\nt,Y l,Y l),yl\nt)⊤\n\n (11)\nˆFl = fM(Sp,Ep) ⊙Fl (12)\np(wt+1|X,Y L\n≤t) = softmax(WVyL\nt+1) (13)\nwhere y0\ni represents word vector,Yl\n≤t = {yl\n1,...,y l\nt}, wt+1\ndenotes the probability of each word in the vocabulary for\ntime t+1, WV ∈Rν×ddenotes the word embedding matrix\nwith vocabulary size ν, and ⊙indicates elementwise mul-\ntiplication. C(·) denotes the decoder representation, i.e. the\noutput from feed-forward layer in Fig. 1. Φ(·) denotes the\ncross module attention that use the current decoder states to\nattend to encoder states (i.e. multi-head attention in Fig. 1).\nΩ(·) represents the self-attention in decoder. Notice that the\nsubscript ≤trestricts the attention only on the already gen-\nerated words. fM : R2 ↦→[0,1]T is a masking function that\noutput values (near) zero when outside the predicted starting\nand ending locations, and (near) one otherwise. With this\n1s is a scalar that affects the convolution stride for different kernel size\nfunction, the receptive region of the model is restricted to\nthe current segment so that the visual representation focuses\non describing the current event. Note that during decoding,\nthe encoder performs the forward propagation again so that\nthe representation of each encoder layer contains only the\ninformation for the current proposal (see eq. 12). This is\ndifferent from simply multiplying the mask with the exist-\ning representation from the encoder during proposal pre-\ndiction, since the representation of the latter still contains\ninformation that is outside the proposal region. The repre-\nsentation from the L-th layer of captioning decoder is then\nused for predicting the next word for the current proposal\nusing a linear layer with softmax activation (see eq. 13).\nDifferentiable Proposal Mask. We cannot choose any\narbitrary function for fM as a discrete one would prevent\nus from doing end-to-end training. We therefore propose to\nuse a fully differentiable function to obtain the mask for vi-\nsual events. This function fM maps the predicted proposal\nlocation to a differentiable mask M ∈RT for each time\nstep i∈{1,...,T }.\nfM(Sp,Ep,Sa,Ea,i) = σ(g( (14)\n[ρ(Sp,:),ρ(Ep,:),ρ(Sa,:),ρ(Ee,:),Bin(Sa,Ea,:)]))\nρ(pos,i) =\n{\nsin(pos/10000i/d) iis even\ncos(pos/10000(i−1)/d) otherwise (15)\nBin(Sa,Ea,i) =\n{\n1 if i∈[Sa,Ea]\n0 otherwise (16)\nwhere Sa and Ea are the start and end position of anchor,\n[·] denotes concatenation, g(·) is a continuous function, and\nσ(·) is the logistic sigmoid function. We choose to use a\nmultilayer perceptron to parameterize g. In other words, we\nhave a feed-forward neural network that takes the positional\nencoding from the anchor and predicted boundary positions\nand the corresponding binary mask to predict the continu-\nous mask. We use the same positional encoding strategy as\nin [30].\nDirectly learning the mask would be difﬁcult and unnec-\nessary, since we would already have a reasonable bound-\nary prediction from the proposal module. Therefore, we\nuse a gated formulation that lets the model choose between\nthe learned continuous mask and the discrete mask obtained\nfrom the proposal module. More precisely, the gated mask-\ning function fGM is\nfGM(Sp,Ep,Sa,Ea,i) =\nPeBin(Sp,Ep,i) + (1−Pe)fM(Sp,Ep,Sa,Ea,i) (17)\nSince the proposal score Pe ∈[0,1], it now acts as a gating\nmechanism. This can also be viewed as a modulation be-\ntween the continuous and proposal masks, the continuous\nmask is used as a supplement for the proposal mask in case\nthe conﬁdence is low from the proposal module.\n4.4. Model Learning\nOur model is fully differentiable and can be trained con-\nsistently from end-to-end The event proposal anchors are\nsampled as follows. Anchors that have overlap greater than\n70% with any ground-truth segments are regarded as pos-\nitive samples and ones that have less than 30% overlap\nwith all ground-truth segments are negative. The proposal\nboundaries for positive samples are regressed to the ground-\ntruth boundaries (offsets). We randomly sample U = 10\nanchors from positive and negative anchor pools that corre-\nspond to one ground-truth segment for each mini-batch.\nThe loss for training our model has four parts: the regres-\nsion loss Lr for event boundary prediction, the binary cross\nentropy mask prediction loss Lm, the event classiﬁcation\nloss Le (i.e. prediction Pe), and the captioning model loss\nLc. The ﬁnal loss Lis a combination of these four losses,\nLr = Smoothℓ1(ˆθc,θc) + Smoothℓ1(ˆθl,θl)\nLi\nm = BCE(Bin(Sp,Ep,i),fM(Sp,Ep,Sa,Ea,i))\nLe = BCE( ˆPe,Pe)\nLt\nc = CE( ˆwt,p(wt|X,Y L\n≤t−1))\nL= λ1Lr + λ2\n∑\ni\nLi\nm + λ3Le + λ4\n∑\nt\nLt\nc\nwhere Smoothℓ1 is the smooth ℓ1 loss deﬁned in [14], BCE\ndenotes binary cross entropy, CE represents cross entropy\nloss, ˆθc and ˆθl represent the ground-truth center and length\noffset with respect to the current anchor, ˆPe is the ground-\ntruth label for the proposed event, ˆwt denotes the ground-\ntruth word at time step t, and λ1...4 ∈R+ are the coefﬁ-\ncients that balance the contribution from each loss.\nSimple Single Stage Models. The key for our proposed\nmodel to work is not the single stage learning of a compo-\nsitional loss, but the ability to keep the consistency between\nthe proposal and captioning. For example, we could make\na single-stage trainable model by simply sticking them to-\ngether with multi-task learning. More precisely, we can\nhave the same model but choose a non-differentiable mask-\ning function fM in eq. 12. The same training procedure can\nbe applied for this model (see the following section). Since\nthe masking function would then be non-differentiable, er-\nror from the captioning model cannot be back propagated\nto modify the proposal predictions. However, the caption-\ning decoder is still able to inﬂuence the visual representation\nthat is learned from the visual encoder. This may be unde-\nsirable, as the updates the visual representation may lead to\nworse performance for the proposal decoder. As a baseline,\nwe also test this single-stage model in our experiments.\n5. Implementation Details\nFor the proposal decoder, the temporal convolutional\nnetworks take the last encoding output from video encoder\nas the input. The sizes of the temporal convolution ker-\nnels vary from 1 to 251 and we set the stride factor s to\n50. For our Transformer model, we set the model dimen-\nsion d = 1024 (same as the Bi-LSTM hidden size) and set\nthe hidden size of feed-forward layer to 2048. We set num-\nber of heads (H) to 8. In addition to the residual dropout\nand attention dropout layers in Transformer, we add a 1-D\ndropout layer at the visual input embedding to avoid over-\nﬁtting. We use recurrent dropout proposed in [10] for this\n1-D dropout. Due to space limits, more details are included\nin the supplementary material.\n6. Experiments\n6.1. Datasets\nActivityNet Captions [20] and YouCookII [43] are the\ntwo largest datasets with temporal event segments anno-\ntated and described by natural language sentences. Ac-\ntivityNet Captions contains 20k videos, and on average\neach video has 3.65 events annotated. YouCookII has 2k\nvideos and the average number of segments per video is\n7.70. The train/val/test splits for ActivityNet Captions are\n0.5:0.25:0.25 while for YouCookII are 0.66:0.23:0.1. We\nreport our results from both datasets on the validation sets.\nFor ActivityNet Captions, we also show the testing results\non the evaluation server while the testing set for YouCookII\nis not available.\nData Preprocessing. We down-sample the video every\n0.5s and extract the 1-D appearance and optical ﬂow fea-\ntures per frame, as suggested by Xiong et al. [34]. For\nappearance features, we take the output of the “Flatten-\n673” layer in ResNet-200 [16]; for optical ﬂow features,\nwe extract the optical ﬂow from 5 contiguous frames, en-\ncode with BN-Inception [19] and take output of the “global-\npool” layer. Both networks are pre-trained on the Activi-\ntyNet dataset [6] for the action recognition task. We then\nconcatenate the two feature vector and further encode with\na linear layer. We set the window size T to 480. The in-\nput is zero padded in case the number of sampled frames is\nsmaller than the size of the window. Otherwise, the video\nis truncated to ﬁt the window. Note that we do not ﬁne-tune\nthe visual features for efﬁciency considerations, however,\nallowing ﬁne-tuning may lead to better performance.\n6.2. Baseline and Metrics\nBaselines. Most of the existing methods can only cap-\ntion an entire video or speciﬁed video clip. For example,\nLSTM-YT [32], S2YT [31], TempoAttn [36], H-RNN [41]\nand DEM [20]. The most relevant baseline is TempoAttn,\nwhere the model temporally attends on visual sequence in-\nputs as the input of LSTM language encoder. For a fair\ncomparison, we made the following changes to the origi-\nnal TempoAttn. First, all the methods take the same visual\nfeature input. Second, we add a Bi-LSTM context encoder\nto TempoAttn while our method use self-attention context\nencoder. Third, we apply temporal attention on Bi-LSTM\noutput for all the language decoder layers in TempoAttn\nsince our decoder has attention each layer. We name this\nbaseline Bi-LSTM+TempoAttn. Since zero inputs deteri-\norates Bi-LSTM encoding, we only apply the masking on\nthe output of the LSTM encoder when it is passed to the\ndecoder. We also compare with a a simple single-stage\nMasked Transformer baseline as mentioned in section 4.4,\nwhere the model employs a discrete binary mask.\nFor event proposals, we compare our self-attention\ntransformer-based model with ProcNets and our own base-\nline with Bi-LSTM. For captioning-only models, we use the\nsame baseline as the full dense video captioning but instead,\nreplace the learned proposals with ground-truth proposals.\nResults for other dense captioning methods ( e.g. the best\npublished method DEM [20]) are not available on the val-\nidation set nor is the source code released. So, we com-\npare our methods against those methods that participated\nin CVPR 2017 ActivityNet Video Dense-captioning Chal-\nlenge [13] for test set performance on ActivityNet.\nEvaluation Metrics. For ground-truth segment caption-\ning, we measure the captioning performance with most\ncommonly-used evaluation metrics: BLEU {3,4}and ME-\nTEOR. For dense captioning, the evaluate metric takes both\nproposal accuracy and captioning accuracy into account.\nGiven a tIoU threshold, if the proposal has an overlapping\nlarger than the threshold with any ground-truth segments,\nthe metric score is computed for the generated sentence and\nthe corresponding ground-truth sentence. Otherwise, the\nmetric score is set to 0. The scores are then averaged across\nall the proposals and ﬁnally averaged across all the tIoU\nthresholds–0.3, 0.5, 0.7, 0.9 in this case.\n6.3. Comparison with State-of-the-Art Methods\nWe compare our proposed method with baselines on the\nActivityNet Caption dataset. The validation and testing set\nresults are shown in Tab. 1 and 2, respectively. All our mod-\nels outperform the LSTM-based models by a large margin,\nwhich may be attributed to their better ability of modeling\nlong-range dependencies.\nWe also test the performance of our model on the\nYouCookII dataset, and the result is shown in Tab. 3.\nHere, we see similar trend on performance. Our transformer\nbased model outperforms the LSTM baseline by a signif-\nicant amount. However, the results on learned proposals\nare much worse as compared to the ActivityNet dataset.\nThis is possibly because of small objects, such as utensils\nand ingredients, are hard to detect using global visual fea-\ntures but are crucial for describing a recipe. Hence, one\nfuture extension for our work is to incorporate object detec-\ntors/trackers [39, 40] into the current captioning system.\nTable 1. Captioning results from ActivityNet Caption Dataset with\nlearned event proposals. All results are on the validation set and\nall our models are based on 2-layer Transformer. We report BLEU\n(B) and METEOR (M). All results are on the validation set. Top\nscores are highlighted.\nMethod B@3 B@4 M\nBi-LSTM 2.43 1.01 7.49+TempoAttn\nMasked Transformer 4.47 2.14 9.43\nEnd-to-end Masked Transformer 4.76 2.23 9.56\nTable 2. Dense video captioning challenge leader board results.\nFor results from the same team, we keep the highest one.\nMethod METEOR\nDEM [20] 4.82\nWang et al. 9.12\nJin et al. 9.62\nGuo et al. 9.87\nYao et al.2(Ensemble) 12.84\nOur Method 10.12\nTable 3. Recipe generation benchmark on YouCookII validation\nset. GT proposals indicate the ground-truth segments are given\nduring inference.\nMethod GT Proposals Learned Proposals\nB@4 M B@4 M\nBi-LSTM 0.87 8.15 0.08 4.62+TempoAttn\nOur Method 1.42 11.20 0.30 6.58\nWe show qualitative results in Fig. 3 where the proposed\nmethod generates captions with more relevant semantic in-\nformation. More visualizations are in the supplementary.\n6.4. Model Analysis\nIn this section we perform experiments to analyze the\neffectiveness of our model on different sub-tasks of dense\nvideo captioning.\nVideo Event Proposal. We ﬁrst evaluate the effect of self-\nattention on event proposal, and the results are shown in\nTab. 4. We use standard average recall (AR) metric [9, 13]\ngiven 100 proposals. Bi-LSTM indicates our improved\nProcNets-prop model by using temporal convolutional and\nlarge kernel strides. We use our full model here, where the\ncontext encoder is replaced by our video encoder. We have\nnoticed that the anchor sizes have a large impact on the re-\nsults. So, for fair comparison, we maintain the same an-\nchor sizes across all three methods. Our proposed Bi-LSTM\nmodel gains a 7% relative improvement from the baseline\nresults from the deeper proposal network and more bal-\nanced anchor candidates. Our video encoder further yields\n2This work is unpublished. It employs external data for model training\nand the ﬁnal prediction is obtained from an ensemble of models.\nTable 4. Event proposal results from ActivityNet Captions dataset.\nWe compare our proposed methods with our baseline method\nProcNets-prop on the validation set.\nMethod Average Recall (%)\nProcNets-prop [43] 47.01\nBi-LSTM (ours) 50.65\nSelf-Attn (our) 52.95\nTable 5. Captioning results from ActivityNet Caption Dataset with\nground-truth proposals. All results are on the validation set. Top\ntwo scores are highlighted.\nMethod B@3 B@4 M\nBi-LSTM 4.8 2.1 10.02+TempoAttn\nOur Method\n1-layer 5.80 2.66 10.92\n2-layer 5.69 2.67 11.06\n4-layer 5.70 2.77 11.11\n6-layer 5.66 2.71 11.10\na 4.5% improvement from our recurrent nets-based model.\nWe show the recall curve under high tIoU threshold (0.8)\nin Fig. 4 follow the convention [20]. DAPs [9], is initially\nproposed for short action proposals and adapted later for\nlong event proposal [20]. The proposed models outper-\nforms DAPs-event and ProcNets-prop by signiﬁcant mar-\ngins. Transformer based and Bi-LSTM based models yield\nsimilar recall results given sufﬁcient number of proposals\n(100), while our self-attention encoding model is more ac-\ncurate when the allowed number of proposals is small.\nDense Video Captioning. Next, we look at the dense\nvideo captioning results in an ideal setting: doing the cap-\ntioning based on the ground-truth event segments. This will\ngive us an ideal captioning performance since all event pro-\nposals are accurate. Because we need access to ground-truth\nevent proposal during test time, we report the results on val-\nidation set3 (see Tab. 5). The proposed Masked Transformer\n(section 4.3) outperforms the baseline by a large margin (by\nmore than 1 METEOR point). This directly substantiates\nthe effectiveness of the transformer on both visual and lan-\nguage encoding and multi-head temporal attention. We no-\ntice that as the number of encoder and decoder layers in-\ncreases, the performance gets further boosts by 1.3%-1.7%.\nAs can be noted here, the 2-layer transformer strikes a good\nbalance point between performance and computation, and\nthus we use 2-layer transformer for all our experiments.\nAnalysis on Long Events. As mentioned in section 4.1,\nlearning long-range dependencies should be easier with\nself-attention, since the next layer observes information\nfrom all time steps of the previous layer. To validate this\nhypothesis directly, we test our model against the LSTM\n3The results are overly optimistic, however, it is ﬁne here since we are\ninterested in the best situation performance. The comparison is also fair,\nsince all methods are tuned to optimize the validation set performance.\nGround-truth\nEvent 0: Two teams are playing volleyball in \na indoor court.\nEvent 1: Two teams wearing dark uniforms \nare doing a volleyball competition, then \nappears a team with yellow t-shirts.\nEvent 2: Then, a boy with a red t-shirt \nserves the ball and the teams start to hit and \nrunning to pass the ball, then another team \nwearing green shorts enters the court.\nEvent 3: After, team wearing blue uniform \ncompetes with teams wearing white and red \nuniforms.\nMasked Trans. (ours)\nEvent 0: a large group of people are seen \nstanding around a gymnasium playing a \ngame of volleyball\nEvent 1: the people in black and yellow \nteam scores a goal\nEvent 2: the people continue playing the \ngame back and fourth while the people \nwatch on the sidelines\nEvent 3: the people continue playing the \ngame back and fourth while the camera \ncaptures their movements\nBi-LSTM+TempoAttn\nEvent 0: a large group of people are seen \nstanding around a field playing a game of \nsoccer\nEvent 1: the players are playing the game \nof tug of war\nEvent 2: the people continue playing with \none another and end by walking away\nEvent 3: the people continue playing and \nends with one another and the other  \nGround-truth\nEvent 0: A man is writing something on a \nclipboard.\nEvent 1: A man holds a ball behind his \nhead and spins around several times and \nthrows the ball.\nEvent 2: People use measuring tape to \nmeasure the distance.\nMasked Trans. (ours)\nEvent 0: a man is seen standing in a large \ncircle and leads into a man holding a ball \nand\nEvent 1: the man spins the ball around \nand throws the ball\nEvent 2: the man throws the ball and his \nthrow the distance\nBi-LSTM+TempoAttn\nEvent 0: a man is seen standing on a field \nwith a man standing on a field\nEvent 1: he throws the ball and throws it \nback and forth\nEvent 2: he throws the ball and throws it \nback and forth  \nFigure 3. Qualitative results on ActivityNet Captions. The color bars represent different events. Colored text highlight relevant content to\nthe event. Our model generates more relevant attributes as compared to the baseline.\nFigure 4. Event proposal recall curve under tIoU threshold 0.8 with\naverage 100 proposals per video.\nTable 6. Evaluating only long events from ActivityNet Caption\nDataset. GT proposals indicate the ground-truth segments are\ngiven during inference.\nGT Proposals Learned Proposals\nMethod B@4 M B@4 M\nBi-LSTM 0.84 5.39 0.42 3.99+TempoAttn\nOur Method 1.13 5.90 1.04 5.93\nbaseline on longer event segments (where the events are at\nleast 50s long) from the ActivityNet Caption dataset, where\nlearning the long-range dependencies are crucial for achiev-\ning good performance. It is clear from the result (see Tab.\n6) that our transformer based model performs signiﬁcantly\nbetter than the LSTM baseline. The discrepancy is even\nlarger when the model needs to learn both the proposal\nand captioning, which demonstrate the effectiveness of self-\nattention in facilitate learning long range dependencies.\n7. Conclusion\nWe propose an end-to-end model for dense video cap-\ntioning. The model is composed of an encoder and two\ndecoders. The encoder encodes the input video to proper\nvisual representations. The proposal decoder then decodes\nfrom this representation with different anchors to form\nvideo event proposals. The captioning decoder employs a\ndifferentiable masking network to restrict its attention to\nthe proposal event, ensures the consistency between the\nproposal and captioning during training. In addition, we\npropose to use self-attention for dense video captioning.\nWe achieved signiﬁcant performance improvement on both\nevent proposal and captioning tasks as compared to RNN-\nbased models. We demonstrate the effectiveness of our\nmodels on ActivityNet Captions and YouCookII dataset.\nAcknowledgement. The technical work was performed while Lu-\nowei was an intern at Salesforce Research. This work is also partly\nsupported by ARO W911NF-15-1-0354 and DARPA FA8750-17-\n2-0112. This article solely reﬂects the opinions and conclusions of\nits authors but not the funding agents.\nReferences\n[1] J.-B. Alayrac, P. Bojanowski, N. Agrawal, J. Sivic, I. Laptev,\nand S. Lacoste-Julien. Unsupervised learning from narrated\ninstruction videos. In CVPR, pages 4575–4583, 2016. 1\n[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016. 3\n[3] D. Bahdanau, K. Cho, and Y . Bengio. Neural machine\ntranslation by jointly learning to align and translate. arXiv\npreprint arXiv:1409.0473, 2014. 2\n[4] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled\nsampling for sequence prediction with recurrent neural net-\nworks. In NIPS, pages 1171–1179, 2015. 11\n[5] S. Buch, V . Escorcia, C. Shen, B. Ghanem, and J. C. Niebles.\nSst: Single-stream temporal action proposals. In CVPR,\npages 2911–2920, 2017. 2\n[6] F. Caba Heilbron, V . Escorcia, B. Ghanem, and J. Car-\nlos Niebles. Activitynet: A large-scale video benchmark\nfor human activity understanding. In CVPR, pages 961–970,\n2015. 6\n[7] P. Das, C. Xu, R. F. Doell, and J. J. Corso. A thousand frames\nin just a few words: Lingual description of videos through\nlatent topics and sparse object stitching. In Proceedings of\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2013. 2, 3\n[8] J. Donahue, L. Anne Hendricks, S. Guadarrama,\nM. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-\nrell. Long-term recurrent convolutional networks for visual\nrecognition and description. In CVPR, pages 2625–2634,\n2015. 2\n[9] V . Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem.\nDaps: Deep action proposals for action understanding. In\nECCV, pages 768–784, 2016. 2, 7\n[10] Y . Gal and Z. Ghahramani. A theoretically grounded applica-\ntion of dropout in recurrent neural networks. In NIPS, pages\n1019–1027, 2016. 6\n[11] Z. Gan, C. Gan, X. He, Y . Pu, K. Tran, J. Gao, L. Carin,\nand L. Deng. Semantic compositional networks for visual\ncaptioning. CVPR, 2017. 2\n[12] J. Gao, Z. Yang, C. Sun, K. Chen, and R. Nevatia. Turn\ntap: Temporal unit regression network for temporal action\nproposals. ICCV, 2017. 2\n[13] B. Ghanem, J. C. Niebles, C. Snoek, F. Caba Heil-\nbron, H. Alwassel, R. Khrisna, V . Escorcia, K. Hata, and\nS. Buch. Activitynet challenge 2017 summary. arXiv\npreprint arXiv:1710.08011, 2017. 2, 6, 7\n[14] R. Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 1440–1448,\n2015. 5\n[15] A. Graves and J. Schmidhuber. Framewise phoneme clas-\nsiﬁcation with bidirectional lstm and other neural network\narchitectures. Neural Networks, 18(5):602–610, 2005. 4\n[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, pages 770–778, 2016. 2, 3,\n6\n[17] F. C. Heilbron, W. Barrios, V . Escorcia, and B. Ghanem. Scc:\nSemantic context cascade for efﬁcient action detection. 3\n[18] S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997. 2\n[19] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nICML, pages 448–456, 2015. 4, 6\n[20] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. C. Niebles.\nDense-captioning events in videos. ICCV, 2017. 1, 2, 3, 6, 7\n[21] H. Kuehne, A. Arslan, and T. Serre. The language of actions:\nRecovering the syntax and semantics of goal-directed human\nactivities. In CVPR, pages 780–787, 2014. 1\n[22] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou,\nand Y . Bengio. A structured self-attentive sentence embed-\nding. arXiv preprint arXiv:1703.03130, 2017. 2\n[23] Y . Pan, T. Yao, H. Li, and T. Mei. Video captioning with\ntransferred semantic attributes. CVPR, 2017. 2\n[24] R. Pascanu, T. Mikolov, and Y . Bengio. On the difﬁculty of\ntraining recurrent neural networks. In ICML, pages 1310–\n1318, 2013. 2, 11\n[25] R. Paulus, C. Xiong, and R. Socher. A deep rein-\nforced model for abstractive summarization. arXiv preprint\narXiv:1705.04304, 2017. 2\n[26] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards\nreal-time object detection with region proposal networks. In\nNIPS, pages 91–99, 2015. 2\n[27] S. J. Rennie, E. Marcheret, Y . Mroueh, J. Ross, and V . Goel.\nSelf-critical sequence training for image captioning. arXiv\npreprint arXiv:1612.00563, 2016. 2\n[28] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-\nization in untrimmed videos via multi-stage cnns. In CVPR,\npages 1049–1058, 2016. 2\n[29] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014. 2\n[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. NIPS, 2017. 2, 3, 5\n[31] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney,\nT. Darrell, and K. Saenko. Sequence to sequence-video to\ntext. In ICCV, pages 4534–4542, 2015. 2, 6\n[32] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach,\nR. Mooney, and K. Saenko. Translating videos to natural lan-\nguage using deep recurrent neural networks. arXiv preprint\narXiv:1412.4729, 2014. 2, 6\n[33] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and\ntell: A neural image caption generator. In CVPR, pages\n3156–3164, 2015. 2\n[34] Y . Xiong, L. Wang, Z. Wang, B. Zhang, H. Song, W. Li,\nD. Lin, Y . Qiao, L. Van Gool, and X. Tang. Cuhk & ethz &\nsiat submission to activitynet challenge 2016. arXiv preprint\narXiv:1608.00797, 2016. 6\n[35] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudi-\nnov, R. Zemel, and Y . Bengio. Show, attend and tell: Neural\nimage caption generation with visual attention. In ICML,\npages 2048–2057, 2015. 2\n[36] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle,\nand A. Courville. Describing videos by exploiting temporal\nstructure. In CVPR, 2015. 2, 6\n[37] T. Yao, Y . Pan, Y . Li, Z. Qiu, and T. Mei. Boosting image\ncaptioning with attributes. ICCV, 2017. 2\n[38] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image cap-\ntioning with semantic attention. In CVPR, pages 4651–4659,\n2016. 2\n[39] H. Yu, N. Siddharth, A. Barbu, and J. M. Siskind. A compo-\nsitional framework for grounding language inference, gener-\nation, and acquisition in video. J. Artif. Intell. Res.(JAIR) ,\n52:601–713, 2015. 6\n[40] H. Yu and J. M. Siskind. Grounded language learning from\nvideo described with sentences. In ACL (1), pages 53–63,\n2013. 2, 6\n[41] H. Yu, J. Wang, Z. Huang, Y . Yang, and W. Xu. Video\nparagraph captioning using hierarchical recurrent neural net-\nworks. CVPR, 2016. 2, 6\n[42] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman. Video sum-\nmarization with long short-term memory. In ECCV, pages\n766–782. Springer, 2016. 1\n[43] L. Zhou, C. Xu, and J. J. Corso. Towards automatic learning\nof procedures from web instructional videos. AAAI, 2018. 2,\n4, 6, 7\n[44] L. Zhou, C. Xu, P. Koch, and J. J. Corso. Watch what you\njust said: Image captioning with text-conditional attention.\nIn Proceedings of the on Thematic Workshops of ACM Mul-\ntimedia 2017, pages 305–313. ACM, 2017. 2\n8. Appendix\n8.1. Implementation Details\nThe sizes of the temporal convolution kernels in the proposal\nmodule are 1 2, 3, 4, 5, 7, 9, 11, 15, 21, 29, 41, 57, 71, 111, 161,\n211 and 251. We set the hyper-parameters for End-to-end Masked\nTransformer as follows. The dropout ratio for Transformer is set\nto 0.2 and that for visual input embedding is set to 0.1. We set the\nloss coefﬁcients λ1,λ2,λ3,λ4 to 10,1,1,0.25. For training, we\nuse stochastic gradient descent (SGD) with Nesterov momentum,\nthe learning rate is set between 0.01 and 0.1 depending on the con-\nvergence, and the momentum is set at 0.95. We decay the learning\nrate by half on plateau. We also clip the gradient [24] to have\nglobal ℓ2 norm of 1. For inference, we ﬁrst pick event propos-\nals with prediction score higher than a pre-deﬁned threshold (0.7).\nWe remove proposals that have high overlap (i.e. ≥ 0.9) with each\nother. For each video, we have at least 50, and at most 500 event\nproposals. The descriptions are then generated for each of the pro-\nposal, and we use greedy decoding for text generation with at most\n20 words. We implement the model in PyTorch and train it using\n8 Tesla K80 GPUs with synchronous SGD. The model typically\ntakes a day to converge.\nThe implementation for proposal-only and captioning-only\nmodel is slightly different. We apply Adam for training rather\nthan SGD and set the learning rate to 0.0001. When training the\ncaptioning-only model, we apply scheduled sampling [4]. We set\nthe sampling ratio to 0.05 at the beginning of training, and increase\nit by 0.05 every 5 epoch until it reaches 0.25. Note that applying\nscheduled sampling to End-to-end Masked Transformer yield no\nimprovements and hence disabled. In the proposal-only model,\nwe report the results on a single-layer Transformer with the model\nsize and hidden size to be 512 and 128. The temporal conv. stride\nfactor sis set to 10.\n8.2. Additional Results\nTo see the effectiveness of self-attention, we performed addi-\ntional ablation studies, where we apply self-attention module at the\nencoder or decoder of the LSTM-based baseline. From the result\nit is clear that self-attention have signiﬁcant impact on the perfor-\nmance of the model (see Tab. 7), especially as in the language\ndecoder.\nNote that the performance of captioning models over ground-\ntruth segments vary little from number of layers. We choose to\nuse 2-layer transformer for the rest of the experiments because\n1) the 4 and 6 layer models are more computational expensive;\n2) the learning is more complicated when the learned proposals\nare approximate, and a 2-layer model give us more ﬂexibility for\nhandling this case (see Tab. 7 for results on the 1-layer model).\nSelf-attention facilitates the learning of long-range dependen-\ncies, which should not hurt the performance on modeling relative\nshort-range dependencies. To validate this we tested our model on\nshorter activities, where the activities are at most 15 seconds long.\nThe result is shown in Tab. 8.\n8.3. Additional Qualitative Results\nWe visualize the learned masks in Fig. 5. The ﬁrst two corre-\nspond to the case where the proposal prediction is conﬁdent, i.e.,\nTable 7. Additional ablation experiments on ActivityNet.\nMethod B@3 B@4 M\nSelfAttn + LSTM TempoAttn 2.91 1.35 7.88\nBiLSTM + SelfAttn 4.06 1.92 9.05\nOur Method (1-layer) 4.49 2.10 9.27\nTable 8. Evaluating only short events from ActivityNet.\nGT Proposals Learned Proposals\nMethod B@4 M B@4 M\nBi-LSTM+TempoAttn 0.74 5.29 0.23 4.43\nOur Method 0.87 5.82 0.68 5.06\nproposal scores are high ( > 0.9) and the last two correspond to\nthe case where the prediction is less conﬁdent, i.e., proposal scores\nare low (<0.7). We visualize the cross module attention in Fig. 7.\nFor convenience, we randomly choose one of the attention matri-\nces from the multi-head attention. Also, we notice that attention\nweights from higher-level self-attention layer is tend to be ﬂat-\nter than these from the lower-level layer. Qualitative results for\nYouCookII are shown in Fig. 6. The visual recognition is chal-\nlenging result from the small and ambiguous objects (e.g., black\npepper, lamb).\n(a) High proposal score.\n(b) High proposal score\n(b) Low proposal score\n(b) Low proposal score\nFigure 5. Visualization of differentiable masks and ﬁnal masks under hight (a and b) and low proposal score (c and d). Videos from\nActivityNet Captions validation set.\nGround-truth\nEvent 0: stretch the dough \nEvent 1: cut the dough into squares \nEvent 2: lay pepperoni and cheese on the \ndough and roll into a ball \nEvent 3: put the rolls in a pan \nEvent 4: brush each pizza bite with some \nmelted butter and sprinkle some italian \nseasoning on top\nMasked Trans. (ours)\nEvent 0: knead the dough \nEvent 1: cut the dough into thin slices roll \nEvent 2: cut the meat into thin slices roll \nEvent 3: dip the fish in the batter and place on \nthe \nEvent 4: dip the fish in the batter and coat the \nbatter the batter the batter\nBi-LSTM+TempoAttn\nEvent 0: cut the roll into pieces the edges and \nroll the dough \nEvent 1: cut the salmon into thin slices the \nsheet \nEvent 2: cut the salmon into thin slices the \nsheet \nEvent 3: place the filling on the bread the \nbread \nEvent 4: place the chicken on the pan the grill \nand serve\nGround-truth\nEvent 0: pour some oil into a hot pan \nEvent 1: add chopped onions and carrots to the \npan \nEvent 2: add salt to the pan and mix \nEvent 3: add butter and garlic to the pan and \nmix \nEvent 4: add lamb to the pan and break it up \nEvent 5: add salt black pepper and italian \nseasoning to the meat \n...\nMasked Trans. (ours)\nEvent 0: heat a pan with oil a pan add the pork \nEvent 1: add the onions and garlic to the pan \nstir \nEvent 2: add the onions and carrots to the pan \nstir \nEvent 3: add the vegetables to the pan and stir \nEvent 4: add the pork to the pan stir \nEvent 5: add the vegetables to the pan and stir \n...\nBi-LSTM+TempoAttn\nEvent 0: add oil to a pan heat <unk> <unk> \n<unk> <unk> <unk> \nEvent 1: add the chicken to the pan heat \nEvent 2: add the chicken to the pan heat \nEvent 3: add the chicken to the pan heat \nEvent 4: add the chicken to the pan heat \nEvent 5: add the chicken to the pan and stir \nheat \n...\nFigure 6. Qualitative results on YouCookII videos. We only showed result for the ﬁrst 6 events in the second example.\nFigure 7. Visualization of weights from the cross-module attention\nlayer. X axis represents the generated words at each time step. Y\naxis indicates the sampled frames.",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9757462739944458
    },
    {
      "name": "Computer science",
      "score": 0.8357805013656616
    },
    {
      "name": "Encoder",
      "score": 0.5768110752105713
    },
    {
      "name": "Transformer",
      "score": 0.5701960325241089
    },
    {
      "name": "Decodes",
      "score": 0.5577220320701599
    },
    {
      "name": "Event (particle physics)",
      "score": 0.5367058515548706
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5085002183914185
    },
    {
      "name": "Decoding methods",
      "score": 0.5058414936065674
    },
    {
      "name": "Language model",
      "score": 0.42489245533943176
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40912559628486633
    },
    {
      "name": "Speech recognition",
      "score": 0.40726274251937866
    },
    {
      "name": "Natural language processing",
      "score": 0.39536941051483154
    },
    {
      "name": "Image (mathematics)",
      "score": 0.10035675764083862
    },
    {
      "name": "Algorithm",
      "score": 0.0980939269065857
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}