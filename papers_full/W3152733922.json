{
  "title": "VideoGPT: Video Generation using VQ-VAE and Transformers",
  "url": "https://openalex.org/W3152733922",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4282133907",
      "name": "Yan, Wilson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1766281539",
      "name": "Zhang Yun-zhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743294920",
      "name": "Abbeel, Pieter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287473691",
      "name": "Srinivas, Aravind",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963871073",
    "https://openalex.org/W2963373786",
    "https://openalex.org/W3013229294",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2893749619",
    "https://openalex.org/W2963139417",
    "https://openalex.org/W2976617189",
    "https://openalex.org/W2964245526",
    "https://openalex.org/W2362143032",
    "https://openalex.org/W2963402657",
    "https://openalex.org/W2901599654",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2994903658",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2770201307",
    "https://openalex.org/W2409550820",
    "https://openalex.org/W2766527293",
    "https://openalex.org/W24089286",
    "https://openalex.org/W2796303840",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2963467180",
    "https://openalex.org/W2963293463",
    "https://openalex.org/W2769810959",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2520707650",
    "https://openalex.org/W2971034910",
    "https://openalex.org/W2994586138",
    "https://openalex.org/W3106570356",
    "https://openalex.org/W2894946340",
    "https://openalex.org/W2129069237",
    "https://openalex.org/W3010151642",
    "https://openalex.org/W3031246127",
    "https://openalex.org/W2778792233",
    "https://openalex.org/W2886748926",
    "https://openalex.org/W2194321275",
    "https://openalex.org/W2173520492",
    "https://openalex.org/W2115907784",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2963045453",
    "https://openalex.org/W3021164770",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W2423557781",
    "https://openalex.org/W2765363933",
    "https://openalex.org/W3102720105",
    "https://openalex.org/W2765349170",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2963345606",
    "https://openalex.org/W2140196014",
    "https://openalex.org/W2950946978",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2971074500",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W2948412951",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W2952453038",
    "https://openalex.org/W2963300588",
    "https://openalex.org/W2964122153",
    "https://openalex.org/W2267126114",
    "https://openalex.org/W3133405188",
    "https://openalex.org/W1583912456",
    "https://openalex.org/W2963092440",
    "https://openalex.org/W2963629403",
    "https://openalex.org/W2601686579",
    "https://openalex.org/W3041956526",
    "https://openalex.org/W2116435618",
    "https://openalex.org/W2788033868",
    "https://openalex.org/W3036167779",
    "https://openalex.org/W3036033610",
    "https://openalex.org/W2951682695",
    "https://openalex.org/W2953212265",
    "https://openalex.org/W2975414524",
    "https://openalex.org/W2194775991"
  ],
  "abstract": "We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html",
  "full_text": "VideoGPT: Video Generation using VQ-V AE and Transformers\nWilson Yan* 1 Yunzhi Zhang* 1 Pieter Abbeel1 Aravind Srinivas1\nAbstract\nWe present VideoGPT: a conceptually simple ar-\nchitecture for scaling likelihood based generative\nmodeling to natural videos. VideoGPT uses VQ-\nV AE that learns downsampled discrete latent rep-\nresentations of a raw video by employing 3D con-\nvolutions and axial self-attention. A simple GPT-\nlike architecture is then used to autoregressively\nmodel the discrete latents using spatio-temporal\nposition encodings. Despite the simplicity in for-\nmulation and ease of training, our architecture is\nable to generate samples competitive with state-\nof-the-art GAN models for video generation on\nthe BAIR Robot dataset, and generate high ﬁ-\ndelity natural videos from UCF-101 and Tum-\nbler GIF Dataset (TGIF). We hope our proposed\narchitecture serves as a reproducible reference\nfor a minimalistic implementation of transformer\nbased video generation models. Samples and\ncode are available at https://wilson1yan.\ngithub.io/videogpt/index.html.\n1. Introduction\nDeep generative models of multiple types (Kingma &\nWelling, 2013; Goodfellow et al., 2014; van den Oord et al.,\n2016b; Dinh et al., 2016) have seen incredible progress in\nthe last few years on multiple modalities including natural\nimages (van den Oord et al., 2016c; Zhang et al., 2019;\nBrock et al., 2018; Kingma & Dhariwal, 2018; Ho et al.,\n2019a; Karras et al., 2017; 2019; Van Den Oord et al.,\n2017; Razavi et al., 2019; Vahdat & Kautz, 2020; Ho et al.,\n2020; Chen et al., 2020; Ramesh et al., 2021), audio wave-\nforms conditioned on language features (van den Oord et al.,\n2016a; Oord et al., 2017; Prenger et al., 2019; Bi ´nkowski\net al., 2019), natural language in the form of text (Rad-\nford et al., 2019; Brown et al., 2020), and music genera-\ntion (Dhariwal et al., 2020). These results have been made\npossible thanks to fundamental advances in deep learning\n*Equal contribution 1University of California, Berkeley.\nCorrespondence to: Wilson Yan, Aravind Srinivas <wil-\nson1.yan@berkeley.edu, aravind srinivas@berkeley.edu>.\narchitectures (He et al., 2015; van den Oord et al., 2016b;c;\nVaswani et al., 2017; Zhang et al., 2019; Menick & Kalch-\nbrenner, 2018) as well as the availability of compute re-\nsources (Jouppi et al., 2017; Amodei & Hernandez, 2018)\nthat are more powerful and plentiful than a few years ago.\nWhile there have certainly been impressive efforts to model\nvideos (V ondrick et al., 2016; Kalchbrenner et al., 2016;\nTulyakov et al., 2018; Clark et al., 2019), high-ﬁdelity nat-\nural videos is one notable modality that has not seen the\nsame level of progress in generative modeling as compared\nto images, audio, and text. This is reasonable since the\ncomplexity of natural videos requires modeling correlations\nacross both space and time with much higher input dimen-\nsions. Video modeling is therefore a natural next challenge\nfor current deep generative models. The complexity of the\nproblem also demands more compute resources which can\nalso be deemed as one important reason for the relatively\nslow progress in generative modeling of videos.\nWhy is it useful to build generative models of videos? Con-\nditional and unconditional video generation implicitly ad-\ndresses the problem of video prediction and forecasting.\nVideo prediction (Srivastava et al., 2015; Finn et al., 2016;\nKalchbrenner et al., 2017; Sønderby et al., 2020) can be\nseen as learning a generative model of future frames con-\nditioned on the past frames. Architectures developed for\nvideo generation can be useful in forecasting applications\nfor weather prediction (Sønderby et al., 2020), autonomous\ndriving (for e.g., such as predicting the future in more se-\nmantic and dense abstractions like segmentation masks (Luc\net al., 2017)). Finally, building generative models of the\nworld around us is considered as one way to measure our\nunderstanding of physical common sense and predictive\nintelligence (Lake et al., 2015).\nMultiple classes of generative models have been shown\nto produce strikingly good samples such as autoregres-\nsive models (van den Oord et al., 2016b;c; Parmar et al.,\n2018; Menick & Kalchbrenner, 2018; Radford et al.,\n2019; Chen et al., 2020), generative adversarial networks\n(GANs) (Goodfellow et al., 2014; Radford et al., 2015),\nvariational autoencoders (V AEs) (Kingma & Welling, 2013;\nKingma et al., 2016; Mittal et al., 2017; Marwah et al., 2017;\nVahdat & Kautz, 2020; Child, 2020), Flows (Dinh et al.,\n2014; 2016; Kingma & Dhariwal, 2018; Ho et al., 2019a),\narXiv:2104.10157v2  [cs.CV]  14 Sep 2021\nVideoGPT\nFigure 1.64 × 64 and 128 × 128 video samples generated by VideoGPT\nvector quantized V AE (VQ-V AE) (Van Den Oord et al.,\n2017; Razavi et al., 2019; Ramesh et al., 2021), and lately\ndiffusion and score matching models (Sohl-Dickstein et al.,\n2015; Song & Ermon, 2019; Ho et al., 2020). These different\ngenerative model families have their tradeoffs across various\ndimensions: sampling speed, sample diversity, sample qual-\nity, optimization stability, compute requirements, ease of\nevaluation, and so forth. Excluding score-matching models,\nat a broad level, one can group these models into likelihood-\nbased (PixelCNNs, iGPT, NV AE, VQ-V AE, Glow), and\nadversarial generative models (GANs). The natural ques-\ntion is: What is a good model class to pick for studying and\nscaling video generation?\nFirst, we make a choice between likelihood-based and adver-\nsarial models. Likelihood-based models are convenient to\ntrain since the objective is well understood, easy to optimize\nacross a range of batch sizes, and easy to evaluate. Given\nthat videos already present a hard modeling challenge due to\nthe nature of the data, we believe likelihood-based models\npresent fewer difﬁculties in the optimization and evaluation,\nhence allowing us to focus on the architecture modeling 1.\nNext, among likelihood-based models, we pick autoregres-\nsive models simply because they have worked well on dis-\ncrete data in particular, have shown greater success in terms\nof sample quality (Ramesh et al., 2021), and have well es-\ntablished training recipes and modeling architectures that\ntake advantage of latest innovations in Transformer archi-\ntectures (Vaswani et al., 2017; Child et al., 2019; Ho et al.,\n2019b; Huang et al., 2019).\nFinally, among autoregressive models, we consider the fol-\nlowing question: Is it better to perform autoregressive mod-\neling in a downsampled latent space without spatio-temporal\nredundancies compared to modeling at the atomic level of all\n1It is not the focus of this paper to say likelihood models are\nbetter than GANs for video modeling. This is purely a design\nchoice guided by our inclination to explore likelihood based gener-\native models and non-empirically established beliefs with respect\nto stability of training.\npixels across space and time? Below, we present our reasons\nfor choosing the former: Natural images and videos contain\na lot of spatial and temporal redundancies and hence the\nreason we use image compression tools such as JPEG (Wal-\nlace, 1992) and video codecs such as MPEG (Le Gall, 1991)\neveryday. These redundancies can be removed by learning\na denoised downsampled encoding of the high resolution\ninputs. For example, 4x downsampling across spatial and\ntemporal dimensions results in 64x downsampled resolu-\ntion so that the computation of powerful deep generative\nmodels is spent on these more fewer and useful bits. As\nshown in VQ-V AE (Van Den Oord et al., 2017), even a lossy\ndecoder can transform the latents to generate sufﬁciently\nrealistic samples. This framework has in recent times pro-\nduce high quality text-to-image generation models such as\nDALL-E (Ramesh et al., 2021). Furthermore, modeling in\nthe latent space downsampled across space and time instead\nof the pixel space improves sampling speed and compute\nrequirements due to reduced dimensionality.\nThe above line of reasoning leads us to our proposed model:\nVideoGPT2, a simple video generation architecture that is\na minimal adaptation of VQ-V AE and GPT architectures\nfor videos. VideoGPT employs 3D convolutions and trans-\nposed convolutions (Tran et al., 2015) along with axial at-\ntention (Huang et al., 2019; Ho et al., 2019b) for the autoen-\ncoder in VQ-V AE, learning a downsampled set of discrete\nlatents from raw pixels of the video frames. These latents\nare then modeled using a strong autoregressive prior using\na GPT-like (Radford et al., 2019; Child et al., 2019; Chen\net al., 2020) architecture. The generated latents from the au-\ntoregressive prior are then decoded to videos of the original\nresolution using the decoder of the VQ-V AE.\n2We note that Video Transformers (Weissenborn et al., 2019)\nalso employ generative pre-training for videos using the Subscale\nPixel Networks (SPN) (Menick & Kalchbrenner, 2018) architec-\nture. Despite this, it is fair to use the GPT terminology for our\nmodel because our architecture more closely resembles the vanilla\nTransformer in a manner similar to iGPT (Chen et al., 2020).\nVideoGPT\nTransformer\nTarget\nFlattened sequenceDiscrete Latents\nCodebook\nConv3D \nEncoder\nDiscrete Latents\nConv3D \nDecoder\nFigure 2.We break down the training pipeline into two sequential stages: training VQ-V AE (Left) and training an autoregressive\ntransformer in the latent space (Right). The ﬁrst stage is similar to the original VQ-V AE training procedure. During the second stage,\nVQ-V AE encodes video data to latent sequences as training data for the prior model. For inference, we ﬁrst sample a latent sequence from\nthe prior, and then use VQ-V AE to decode the latent sequence to a video sample.\nOur results are highlighted below:\n1. On the widely benchmarked BAIR Robot Pushing\ndataset (Ebert et al., 2017), VideoGPT can generate realistic\nsamples that are competitive with existing methods such as\nTrIVD-GAN (Luc et al., 2020), achieving an FVD of 103\nwhen benchmarked with real samples, and an FVD* (Razavi\net al., 2019) of 94 when benchmarked with reconstructions.\n2. In addition, VideoGPT is able to generate realistic sam-\nples from complex natural video datasets, such as UCF-101\nand the Tumblr GIF dataset\n3. We present careful ablation studies for the several archi-\ntecture design choices in VideoGPT including the beneﬁt of\naxial attention blocks, the size of the VQ-V AE latent space,\nnumber of codebooks, and the capacity (model size) of the\nautoregressive prior.\n4. VideoGPT can easily be adapted for action conditional\nvideo generation. We present qualitative results on the BAIR\nRobot Pushing dataset and Vizdoom simulator (Kempka\net al., 2016).\n2. Background\n2.1. VQ-V AE\nThe Vector Quantized Variational Autoencoder (VQ-\nV AE) (Van Den Oord et al., 2017) is a model that learns to\ncompress high dimensional data points into a discretized\nlatent space and reconstruct them. The encoder E(x) →h\nﬁrst encodes x into a series of latent vectors h which is\nthen discretized by performing a nearest neighbors lookup\nin a codebook of embeddings C = {ei}K\ni=1 of size K. The\ndecoder D(e) →ˆxthen learns to reconstruct xfrom the\nquantized encodings. The VQ-V AE is trained using the\nfollowing objective:\nL= ∥x−D(e)∥2\n2  \nLrecon\n+ ∥sg[E(x)] −e∥2\n2  \nLcodebook\n+ β∥sg[e] −E(x)∥2\n2  \nLcommit\nwhere sgrefers to a stop-gradient. The objective consists\nof a reconstruction loss Lrecon, a codebook loss Lcodebook,\nand a commitment loss Lcommit. The reconstruction loss\nencourages the VQ-V AE to learn good representations to\naccurately reconstruct data samples. The codebook loss\nbrings codebook embeddings closer to their corresponding\nencoder outputs, and the commitment loss is weighted by\na hyperparameter βand prevents the encoder outputs from\nﬂuctuating between different code vectors.\nAn alternative replacement for the codebook loss described\nin (Van Den Oord et al., 2017) is to use an EMA update\nwhich empirically shows faster training and convergence\nspeed. In this paper, we use the EMA update when training\nthe VQ-V AE.\n2.2. GPT\nGPT and Image-GPT (Chen et al., 2020) are a class of\nautoregressive transformers that have shown tremendous\nsuccess in modelling discrete data such as natural language\nand high dimensional images. These models factorize the\ndata distribution p(x) according to p(x) =∏d\ni=1 p(xi|x<i)\nthrough masked self-attention mechanisms and are opti-\nmized through maximum likelihood. The architectures em-\nploy multi-head self-attention blocks followed by pointwise\nMLP feedforward blocks following the standard design from\n(Vaswani et al., 2017).\n3. VideoGPT\nOur primary contribution is VideoGPT, a new method to\nmodel complex video data in a computationally efﬁcient\nVideoGPT\nI I I I I cl ,d /J t) \" p I' It) ,o O' \nt) 0 0 CJ I I I \nJ j /; b � le; lO I CJ 10 ;tJ /CJ /0 10 10 /t) IO\nI J J d cl tJ 13 0 I) 1 ,e; I/ tJ O; \nFigure 3.Moving MNIST samples conditioned on a single given frame (red).\nmanner. An overview of our method is shown in Fig 2.\nFigure 4.Architecture of the attention residual block in the VQ-\nV AE as a replacement for standard residual blocks.\nLearning Latent CodesIn order to learn a set of discrete\nlatent codes, we ﬁrst train a VQ-V AE on the video data. The\nencoder architecture consists of a series of 3D convolutions\nthat downsample over space-time, followed by attention\nresidual blocks. Each attention residual block is designed as\nshown in Fig 4, where we use LayerNorm (Ba et al., 2016),\nand axial attention layers following (Huang et al., 2019; Ho\net al., 2019b).\nThe architecture for the decoder is the reverse of the encoder,\nwith attention residual blocks followed by a series of 3D\ntransposed convolution that upsample over space-time. The\nposition encodings are learned spatio-temporal embeddings\nthat are shared between all axial attention layers in the\nencoder and decoder.\nLearning a PriorThe second stage of our method is to\nlearn a prior over the VQ-V AE latent codes from the ﬁrst\nstage. We follow the Image-GPT architecture for prior\nnetwork, except that we add dropout layers after the feed-\nforward and attention block layers for regularization.\nAlthough the VQ-V AE is trained unconditionally, we can\ngenerate conditional samples by training a conditional prior.\nWe use two types of conditioning:\n• Cross Attention: For video frame conditioning, we\nﬁrst feed the conditioned frames into a 3D ResNet,\nand then perform cross-attention on the ResNet output\nrepresentation during prior network training.\n• Conditional Norms: Similar to conditioning methods\nused in GANs, we parameterize the gain and bias in\nthe transformer Layer Normalization (Ba et al., 2016)\nlayers as afﬁne functions of the conditional vector.\nThis conditioning method is used for action and class-\nconditioning models.\n4. Experiments\nIn the following section, we evaluate our method and design\nexperiments to answer the following questions:\n• Can we generate high-ﬁdelity samples from complex\nvideo datasets?\n• How do different architecture design choices for VQ-\nV AE and prior network affect performance?\n4.1. Training Details\nAll image data is scaled to [−0.5,0.5] before training. For\nVQ-V AE training, we use random restarts for embeddings,\nand codebook initialization by copying encoder latents as\ndescribed in (Dhariwal et al., 2020). In addition, we found\nVQ-V AE training to be more stable (less codebook collapse)\nwhen using Normalized MSE for the reconstruction loss,\nwhere MSE loss is divided by the variance of the dataset.\nFor all datasets except UCF-101, we train on64×64 videos\nof sequence length 16. For the transformer, we train Sparse\nTransformers (Child et al., 2019) with local and strided\nattention across space-time. Exact architecture details and\nhyperparameters can be found in Appendix A. We achieve\nall results with a maximum of 8 Quadro RTX 6000 GPUs\n(24 GB memory).\n4.2. Moving MNIST\nFor Moving MNIST, VQ-V AE downsamples input videos\nby a factor of 4 over space-time (64x total reduction), and\ncontains two residual layers with no axial-attention. We\nuse a codebook of 512 codes, each 64-dim embeddings.\nTo learn the single-frame conditional prior, we train a con-\nditional transformer with 384 hidden features, 4 heads, 8\nlayers, and a ResNet-18 single frame encoder. Fig 3 shows\nVideoGPT\nseveral different generated trajectories conditioned on a sin-\ngle frame.\nTable 1.FVD on BAIR\nMethod3 FVD (↓)\nSV2P 262.5\nLVT 125.8\nSA VP 116.4\nDVD-GAN-FP 109.8\nVideoGPT (ours) 103.3\nTrIVD-GAN-FP 103.3\nVideo Transformer 94 ±2\n4.3. BAIR Robot Pushing\nFor BAIR, VQ-V AE downsamples the inputs by a factor of\n2x over each of height, width and time dimensions. The\nembedding in the latent space is a 256-dimensional vector,\nwhich is discretized through a codebook with 1024 codes.\nWe use 4 axial-attention residual blocks for the VQ-V AE\nencoder and a prior network with a hidden size of 512 and\n16 layers.\nQuantitatively, Table 13 shows FVD results on BAIR, com-\nparing our method with prior work. Although our method\ndoes not achieve state of the art, it is able to produce very re-\nalistic samples competitive with the best performing GANs.\nQualitatively, Fig 5 shows VQ-V AE reconstructions on\nBAIR. Fig 6 shows samples primed with a single frames.\nWe can see that our method is able to generate realistically\nlooking samples. In addition, we see that VideoGPT is able\nto sample different trajectories from the same initial frame,\nshowing that it is not simply copying the dataset.\n4.4. ViZDoom\nFor ViZDoom, we use the same VQ-V AE and transformer\narchitectures as for the BAIR dataset, with the exception\nthat the transformer is trained without single-frame condi-\ntioning. We collect the training data by training a policy\nin each ViZDoom environment, and collecting rollouts of\nthe ﬁnal trained policies. The total dataset size consists of\n1000 episodes of length 100 trajectories, split into an 8:1:1\ntrain / validataion / test ratio. We experiment on the Health\nGathering Supreme and Battle2 ViZDoom environments,\ntraining both unconditional and action-conditioned priors.\nVideoGPT is able to capture complex 3D camera move-\nments and environment interactions. In addition, action-\nconditioned samples are visually consistent with the input\n3SV2P (Babaeizadeh et al., 2017), SA VP (Lee et al., 2018),\nDVD-GAN-FP (Clark et al., 2019), Video Transformer (Weis-\nsenborn et al., 2019), Latent Video Transformer (LVT) (Rakhimov\net al., 2020), and TrIVD-GAN (Luc et al., 2020) are our baselines\naction sequence and show a diverse range of backgrounds\nand scenarios under different random generations for the\nsame set of actions. Samples can be found in Appendix B.4\nTable 2.IS on UCF-101\nMethod4 IS (↑)\nVGAN 8.31 ±0.09\nTGAN 11.85 ±0.07\nMoCoGAN 12.42 ±0.03\nProgressive VGAN 14.56 ±0.05\nTGAN-F 22.91 ±0.19\nVideoGPT (ours) 24.69 ±0.30\nTGANv2 28.87 ±0.67\nDVD-GAN 32.97 ±1.7\n4.5. UCF-101\nUCF-101 (Soomro et al., 2012) is an action classiﬁcation\ndataset with 13,320 videos from 101 different classes. We\ntrain unconditional VideoGPT models on 16 frame 64 ×64\nand 128 ×128 videos, where the original videos have their\nshorter side scaled to 128 pixels, and then center cropped.\nTable 2 shows results comparing Inception Score5 (IS) (Sal-\nimans et al., 2016) calculations against various baselines.\nUnconditionally generated samples are shown in Figure 7.\nSimilarly observed in (Clark et al., 2019), we notice that that\nVideoGPT easily overﬁts UCF-101 with a train loss of 3.40\nand test loss of 3.12, suggesting that UCF-101 may be too\nsmall a dataset of the relative complexity of the data itself,\nand more exploration would be needed on larger datasets.\n4.6. Tumblr GIF (TGIF)\nTGIF (Li et al., 2016) is a dataset of 103,068 selected GIFs\nfrom Tumblr, totalling around 100,000 hours of video. Fig-\nure 8 shows samples from a trained unconditional VideoGPT\nmodel. We see that the video sample generations are able\nto capture complex interactions, such as camera movement,\nscene changes, and human and object dynamics. Unlike\nUCF-101, VideoGPT did not overﬁt on TGIF with a train\nloss of 2.87 and test loss 2.86.\n4.7. Ablations\nIn this section, we perform ablations on various architectural\ndesign choices for VideoGPT.\n4VGAN (V ondrick et al., 2016), TGAN (Saito et al., 2017),\nMoCoGAN (Tulyakov et al., 2018), Progressive VGAN (Acharya\net al., 2018), TGAN-F (Kahembwe & Ramamoorthy, 2020),\nTGANv2 (Saito & Saito, 2018), DVD-GAN (Clark et al., 2019)\nare our baselines for IS on UCF-101.\n5Inception Score is calculated using the code at https://\ngithub.com/pfnet-research/tgan2\nVideoGPT\nFigure 5.VQ-V AE reconstructions for BAIR Robot Pushing. The original videos are contained in green boxes and reconstructions in\nblue.\nFigure 6.BAIR Robot Pushing samples from a single-frame conditioned VideoGPT model. Frames highlighting in red are conditioning\nframes. Although all videos follow the same starting frame, the samples eventually diverge to varied trajectories.\nAxial-attention in VQ-V AE increases reconstruction\nand generation quality.\nTable 3.Ablation on attention in VQ-V AE. R-FVD is with recon-\nstructed examples\nVQ-V AE Architecture NMSE ( ↓) R-FVD ( ↓)\nNo Attention 0.0041 15.3\nWith Attention 0.0033 14 .9\nWe compare VQ-V AE with and without axial attention\nblocks as shown in Table 3. Empirically, incorporating axial\nattention into the VQ-V AE architecture improves reconstruc-\ntion (NMSE) performance, and has better reconstruction\nFVD. Note that in order to take into account the added pa-\nrameter count from attention layers, we increase the number\nof convolutional residual blocks in the ”No Attention” ver-\nsion for better comparability. Fig 5 shows samples of videos\nreconstructed by VQ-V AE with axial attention module.\nLarger prior network capacity increases performance.\nTable 4.Ablations comparing the number of transformer layers\nTransformer Layers bits/dim FVD ( ↓)\n2 2.84 120 .4 ±6.0\n4 2.52 110 .0 ±2.4\n8 2.39 103 .3 ±2.2\n16 2.05 103 .6 ±2.0\nComputational efﬁciency is a primary advantage of our\nmethod, where we can ﬁrst use the VQ-V AE to downsam-\nple by space-time before learning an autoregressive prior.\nLower resolution latents allow us to train a larger and more\nexpressive prior network to learn complex data distributions\nunder memory constraints. We run an ablation on the prior\nnetwork size which shows that a larger transformer network\nproduces better results. Table 4 shows the results of training\ntransformers of varied number of layers on BAIR. We can\nsee that for BAIR, our method beneﬁts from training larger\nmodels, where the bits per dim shows substantial improve-\nment in increasing layers, and FVD and sample quality show\nincrements in performance up until around 8 layers.\nVideoGPT\nFigure 7.128 × 128 UCF-101 unconditional samples\nFigure 8.64 × 64 TGIF unconditional samples\nA balanced temporal-spatial downsampling in VQ-V AE\nlatent space increase performance.\nTable 5.Ablations comparing different VideoGPT latent sizes on\nBAIR. R-FVD is the FVD of VQ-V AE reconstructions, and FVD*\nis the FVD between samples generated by VideoGPT and samples\nencoded-decoded from VQ-V AE. For each partition below, the to-\ntal number of latents is the same with varying amounts of spatio-\ntemporal downsampling\nLatent Size R-FVD ( ↓) FVD ( ↓) FVD* ( ↓)\n4 ×16 ×16 82 .1 135 .4 ±3.7 81 .8 ±2.3\n16 ×8 ×8 108 .1 166 .9 ±3.1 81 .6 ±2.2\n8 ×16 ×16 49 .9 124 .7 ±2.7 90 .2 ±2.4\n1 ×64 ×64 41 .6 126 .7 ±3.1 98 .1 ±3.6\n4 ×32 ×32 28 .3 104 .6 ±2.7 90 .6 ±2.7\n16 ×16 ×16 32 .8 113 .4 ±2.5 94 .9 ±1.7\n2 ×64 ×64 22 .4 124 .3 ±1.4 104 .4 ±2.5\n8 ×32 ×32 14 .9 103 .6 ±2.0 94 .6 ±1.5\n4 ×64 ×64 15 .7 109 .4 ±2.1 102 .3 ±2.8\n16 ×32 ×32 10 .1 118 .4 ±3.2 113 .8 ±3.3\nA larger downsampling ratio results in a smaller latent code\nsize, which allows us to train larger and more expressive\nprior models. However, limiting the expressivity of the dis-\ncrete latent codes may introduce a bottleneck that results\nin poor VQ-V AE reconstruction and sample quality. Thus,\nVideoGPT presents an inherent trade-off between the size\nof the latents, and the allowed capacity of prior network.\nTable 5 shows FVD results from training VideoGPT on vary-\ning latent sizes for BAIR. We can see that larger latents sizes\nhave better reconstruction quality (lower R-FVD), however,\nthe largest latents 16 ×32 ×32 does not perform the best\nsample-quality-wise due to limited compute constraints on\nprior model size. On the other hand, the smallest set of\nlatents 4 ×16 ×16 and 16 ×8 ×8 have poor reconstruc-\ntion quality and poor samples. There is a sweet-spot in the\ntrade-off at around 8 ×32 ×32 where we observe the best\nsample quality.\nIn addition to looking at the total number of latents, we also\ninvestigate the appropriate downsampling for each latent\nresolution. Each partition in Table 5 shows latent sizes\nwith the same number of total latents, each with different\nspatio-temporal downsampling allocations. Unsurprisingly,\nwe ﬁnd that a balance of downsampling ratio ( 2 ×2 ×2,\ncorresponding to latent size 8 ×32 ×32) between space\nand time is the best, as opposed to downsampling over only\nspace or only time.\nFurther increasing the number of latent codes does not\naffect performance.\nTable 6.Ablations comparing the number of codebook codes\n# of Codes R-FVD ( ↓) FVD ( ↓) bits/dim\n256 18.2 103 .8 ±3.7 1 .55\n1024 14.9 103 .6 ±2.0 2 .05\n4096 11.3 103 .9 ±5.1 2 .60\nVideoGPT\nIn Table 6, we show experimental results for running\nVideoGPT with different number of codes in the codebooks.\nFor all three runs, the VQ-V AE latent code vector has size\n8×32×32. In the case of BAIR, we ﬁnd that reconstruction\nquality improves with increasing the number of codes due\nto better expressivity in the discrete bottleneck. However,\nthey ultimately do not affect sample quality. This may be\ndue to the fact that in the case of BAIR, using 256 codes\nsurpasses a base threshold for generation quality.\nUsing one VQ-V AE codebook instead of multiple im-\nproves performance.\nTable 7.Ablations comparing the number of codebooks\nLatent Size R-FVD ( ↓) FVD ( ↓) bits/dim\n8 ×32 ×32 ×1 14 .9 103 .6 ±2.0 2 .05\n16 ×16 ×16 ×2 17 .2 106 .3 ±1.7 2 .41\n8 ×16 ×16 ×4 17 .7 131 .4 ±2.9 2 .68\n4 ×16 ×16 ×8 23 .1 135 .7 ±3.3 2 .97\nIn our main results, we use one codebook for VQ-V AE.\nIn Table 7, we compare VideoGPT with different number\nof codebooks. Speciﬁcally, multiple codebooks is imple-\nmented by multiplying VQ-V AE’s encode output channel\ndimension by Ctimes, where Cis the number of codebooks.\nThe encoder output is then sliced along channel dimension,\nand each slice is quantized through a separate codebook.\nAs a result, the size of the discrete latents are of dimen-\nsion T ×H ×W ×C, as opposed to T ×H ×W when\nusing a single codebook. Generally, multiple codebooks\nmay be more favorable over increasing the downsampling\nresolution as multiple codebooks allows a combinatorially\nbetter scaling in bottleneck complexity. In our experiments,\nwe increase the number of codebooks, and reduce spatio-\ntemporal resolutions on latent sizes to keep the size of the\nlatent space constant. We see that increasing the number\nof codebooks worsens sample quality performance, and the\nbest results are attained at the highest resolution with one\ncodebook. Nevertheless, incorporating multiple codebooks\nmight shows its advantage when trained with a larger dataset\nor a different VQ-V AE architecture design.\n5. Related Work\nVideo PredictionThe problem of video prediction (Srivas-\ntava et al., 2015) is quite related to video generation in that\nthe latter is one way to solve the former. Plenty of methods\nhave been proposed for video prediction on the BAIR Robot\ndataset (Finn et al., 2016; Ebert et al., 2017; Babaeizadeh\net al., 2017; Denton et al., 2017; Denton & Fergus, 2018;\nLee et al., 2018) where the future frames are predicted given\nthe past frame(s) and (or) action(s) of a robot arm moving\nacross multiple objects thereby benchmarking the ability\nof video models to capture object-robot interaction, object\npermanance, robot arm motion, etc. Translating videos to\nvideos is another paradigm to think about video prediction\nwith a prominent example being vid2vid (Wang et al.,\n2018). The vid2vid framework uses automatically gen-\nerated supervision from more abstract information such as\nsemantic segmentation (Luc et al., 2017) masks, keypoints,\nposes, edge detectors, etc to further condition the GAN\nbased video translation setup.\nVideo GenerationMost modern generative modeling archi-\ntectures allow for easy adaptation of unconditional video\ngeneration to conditional versions through conditional batch-\nnorm (Brock et al., 2018), concatenation (Salimans et al.,\n2017; van den Oord et al., 2016c), etc. Video Pixel Net-\nworks (Kalchbrenner et al., 2017) propose a convolutional\nLSTM based encoding of the past frames to be able to gen-\nerate the next frame pixel by pixel autoregressively with a\nPixelCNN (van den Oord et al., 2016c) decoder. The archi-\ntecture serves both as a video generative as well as predictive\nmodel, optimized through log-likelihood loss at the pixel\nlevel. Subscale Video Transformers (Weissenborn et al.,\n2019) extend the idea of Subscale Pixel Networks (Menick\n& Kalchbrenner, 2018) for video generation at the pixel\nlevel using the subscale ordering across space and time.\nHowever, the sampling time and compute requirements are\nlarge for these models. In the past, video speciﬁc architec-\ntures have been proposed for GAN based video generation\nwith primitive results by (V ondrick et al., 2016). Recently,\nDVD-GAN proposed by (Clark et al., 2019) adopts a Big-\nGAN like architecture for videos with disentangled (axial)\nnon-local (Wang et al., 2017) blocks across space and time.\nThey present a wide range of results, unconditional, past\nframe(s) conditional, and class conditional video generation.\nOther examples of prior work with video generation of\nGANs include (Saito et al., 2017), (Tulyakov et al., 2018),\n(Acharya et al., 2018), (Yushchenko et al., 2019). In addi-\ntion, (Saito & Saito, 2018) and (Kahembwe & Ramamoor-\nthy, 2020) propose more scalable and efﬁcient GAN models\nfor training on less compute. Our approach builds on top\nof VQ-V AE (Van Den Oord et al., 2017) by adapting it for\nvideo generation. A clean architecture with VQ-V AE for\nvideo generation has not been presented yet and we hope\nVideoGPT is useful from that standpoint. While VQ-V AE-\n2 (Razavi et al., 2019) proposes using multi-scale hierarchi-\ncal latents and SNAIL blocks (Chen et al., 2017) (and this\nsetup has been applied to videos in (Walker et al., 2021)),\nthe pipeline is inherently complicated and hard to reproduce.\nFor simplicity, ease of reproduction and presenting the ﬁrst\nVQ-V AE based video generation model with minimal com-\nplexity, we stick with a single scale of discrete latents and\ntransformers for the autoregressive priors, a design choice\nalso adopted in DALL-E (Ramesh et al., 2021).\nVideoGPT\n6. Conclusion\nWe have presented VideoGPT, a new video generation archi-\ntecture adapting VQ-V AE and Transformer models typically\nused for image generation to the domain of videos with mini-\nmal modiﬁcations. We have shown that VideoGPT is able to\nsynthesize videos that are competitive with state-of-the-art\nGAN based video generation models. We have also pre-\nsented ablations on key design choices used in VideoGPT\nwhich we hope is useful for future design of architectures in\nvideo generation.\nAcknowledgement\nThe work was in part supported by NSF NRI Grant\n#2024675 and by Berkeley Deep Drive.\nReferences\nAcharya, D., Huang, Z., Paudel, D. P., and Van Gool, L.\nTowards high resolution video generation with progres-\nsive growing of sliced wasserstein gans. arXiv preprint\narXiv:1810.02419, 2018.\nAmodei, D. and Hernandez, D. Ai and compute. Herun-\ntergeladen von https://blog. openai. com/aiand-compute,\n2018.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016.\nBabaeizadeh, M., Finn, C., Erhan, D., Campbell, R. H., and\nLevine, S. Stochastic variational video prediction. arXiv\npreprint arXiv:1710.11252, 2017.\nBi´nkowski, M., Donahue, J., Dieleman, S., Clark, A., Elsen,\nE., Casagrande, N., Cobo, L. C., and Simonyan, K. High\nﬁdelity speech synthesis with adversarial networks. arXiv\npreprint arXiv:1909.11646, 2019.\nBrock, A., Donahue, J., and Simonyan, K. Large scale gan\ntraining for high ﬁdelity natural image synthesis. arXiv\npreprint arXiv:1809.11096, 2018.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Dhariwal,\nP., Luan, D., and Sutskever, I. Generative pretraining\nfrom pixels. In Proceedings of the 37th International\nConference on Machine Learning, 2020.\nChen, X., Mishra, N., Rohaninejad, M., and Abbeel, P.\nPixelsnail: An improved autoregressive generative model.\narXiv preprint arXiv:1712.09763, 2017.\nChild, R. Very deep vaes generalize autoregressive models\nand can outperform them on images. arXiv preprint\narXiv:2011.10650, 2020.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gen-\nerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019.\nClark, A., Donahue, J., and Simonyan, K. Adversarial video\ngeneration on complex datasets, 2019.\nDenton, E. and Fergus, R. Stochastic video generation with\na learned prior. arXiv preprint arXiv:1802.07687, 2018.\nDenton, E. L. et al. Unsupervised learning of disentan-\ngled representations from video. In Advances in neural\ninformation processing systems, pp. 4414–4423, 2017.\nDhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,\nand Sutskever, I. Jukebox: A generative model for music.\narXiv preprint arXiv:2005.00341, 2020.\nDinh, L., Krueger, D., and Bengio, Y . Nice: Non-linear\nindependent components estimation. arXiv preprint\narXiv:1410.8516, 2014.\nDinh, L., Sohl-Dickstein, J., and Bengio, S. Density estima-\ntion using Real NVP. arXiv preprint arXiv:1605.08803,\n2016.\nEbert, F., Finn, C., Lee, A. X., and Levine, S. Self-\nsupervised visual planning with temporal skip connec-\ntions. arXiv preprint arXiv:1710.05268, 2017.\nFinn, C., Goodfellow, I., and Levine, S. Unsupervised\nlearning for physical interaction through video prediction.\nIn Advances in neural information processing systems,\npp. 64–72, 2016.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY . Generative adversarial nets. In Advances in neural\ninformation processing systems, pp. 2672–2680, 2014.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep resid-\nual learning for image recognition. arXiv preprint\narXiv:1512.03385, 2015.\nHo, J., Chen, X., Srinivas, A., Duan, Y ., and Abbeel, P.\nFlow++: Improving ﬂow-based generative models with\nvariational dequantization and architecture design. arXiv\npreprint arXiv:1902.00275, 2019a.\nHo, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T.\nAxial attention in multidimensional transformers. arXiv\npreprint arXiv:1912.12180, 2019b.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. arXiv preprint arXiv:2006.11239, 2020.\nVideoGPT\nHuang, Z., Wang, X., Huang, L., Huang, C., Wei, Y ., and\nLiu, W. Ccnet: Criss-cross attention for semantic segmen-\ntation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 603–612, 2019.\nJouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,\nG., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,\nA., et al. In-datacenter performance analysis of a tensor\nprocessing unit. In Proceedings of the 44th Annual Inter-\nnational Symposium on Computer Architecture, pp. 1–12,\n2017.\nKahembwe, E. and Ramamoorthy, S. Lower dimensional\nkernels for video discriminators. Neural Networks, 132:\n506–520, 2020.\nKalchbrenner, N., Oord, A. v. d., Simonyan, K., Danihelka,\nI., Vinyals, O., Graves, A., and Kavukcuoglu, K. Video\npixel networks. arXiv preprint arXiv:1610.00527, 2016.\nKalchbrenner, N., Oord, A., Simonyan, K., Danihelka, I.,\nVinyals, O., Graves, A., and Kavukcuoglu, K. Video\npixel networks. In International Conference on Machine\nLearning, pp. 1771–1779. PMLR, 2017.\nKarras, T., Aila, T., Laine, S., and Lehtinen, J. Progres-\nsive growing of gans for improved quality, stability, and\nvariation. arXiv preprint arXiv:1710.10196, 2017.\nKarras, T., Laine, S., and Aila, T. A style-based generator\narchitecture for generative adversarial networks. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pp. 4401–4410, 2019.\nKempka, M., Wydmuch, M., Runc, G., Toczek, J., and\nJa´skowski, W. Vizdoom: A doom-based ai research plat-\nform for visual reinforcement learning. In 2016 IEEE\nConference on Computational Intelligence and Games\n(CIG), pp. 1–8. IEEE, 2016.\nKingma, D. P. and Dhariwal, P. Glow: Generative\nﬂow with invertible 1x1 convolutions. arXiv preprint\narXiv:1807.03039, 2018.\nKingma, D. P. and Welling, M. Auto-encoding variational\nBayes. Proceedings of the 2nd International Conference\non Learning Representations, 2013.\nKingma, D. P., Salimans, T., Jozefowicz, R., Chen, X.,\nSutskever, I., and Welling, M. Improving variational\ninference with inverse autoregressive ﬂow. In Advances\nin Neural Information Processing Systems, 2016.\nLake, B. M., Salakhutdinov, R., and Tenenbaum, J. B.\nHuman-level concept learning through probabilistic pro-\ngram induction. Science, 350(6266):1332–1338, 2015.\nLe Gall, D. Mpeg: A video compression standard for multi-\nmedia applications. Communications of the ACM, 34(4):\n46–58, 1991.\nLee, A. X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., and\nLevine, S. Stochastic adversarial video prediction. arXiv\npreprint arXiv:1804.01523, 2018.\nLi, Y ., Song, Y ., Cao, L., Tetreault, J., Goldberg, L., Jaimes,\nA., and Luo, J. Tgif: A new dataset and benchmark on\nanimated gif description. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npp. 4641–4650, 2016.\nLuc, P., Neverova, N., Couprie, C., Verbeek, J., and Le-\nCun, Y . Predicting deeper into the future of semantic\nsegmentation. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), Oct 2017.\nLuc, P., Clark, A., Dieleman, S., Casas, D. d. L., Doron, Y .,\nCassirer, A., and Simonyan, K. Transformation-based\nadversarial video prediction on large-scale data. arXiv\npreprint arXiv:2003.04035, 2020.\nMarwah, T., Mittal, G., and Balasubramanian, V . N. At-\ntentive semantic video generation using captions. In\nProceedings of the IEEE International Conference on\nComputer Vision, pp. 1426–1434, 2017.\nMenick, J. and Kalchbrenner, N. Generating high ﬁdelity im-\nages with subscale pixel networks and multidimensional\nupscaling. arXiv preprint arXiv:1812.01608, 2018.\nMittal, G., Marwah, T., and Balasubramanian, V . N. Sync-\ndraw: Automatic video generation using deep recurrent\nattentive architectures. In Proceedings of the 25th ACM\ninternational conference on Multimedia, pp. 1096–1104,\n2017.\nOord, A. v. d., Li, Y ., Babuschkin, I., Simonyan, K.,\nVinyals, O., Kavukcuoglu, K., Driessche, G. v. d., Lock-\nhart, E., Cobo, L. C., Stimberg, F., et al. Parallel\nwavenet: Fast high-ﬁdelity speech synthesis. arXiv\npreprint arXiv:1711.10433, 2017.\nParmar, N., Vaswani, A., Uszkoreit, J., Kaiser,Ł., Shazeer,\nN., and Ku, A. Image transformer. arXiv preprint\narXiv:1802.05751, 2018.\nPrenger, R., Valle, R., and Catanzaro, B. Waveglow: A\nﬂow-based generative network for speech synthesis. In\nICASSP 2019-2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp.\n3617–3621. IEEE, 2019.\nRadford, A., Metz, L., and Chintala, S. Unsupervised rep-\nresentation learning with deep convolutional generative\nadversarial networks. arXiv preprint arXiv:1511.06434,\n2015.\nVideoGPT\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. OpenAI Blog, 1(8):9, 2019.\nRakhimov, R., V olkhonskiy, D., Artemov, A., Zorin, D., and\nBurnaev, E. Latent video transformer. arXiv preprint\narXiv:2006.10704, 2020.\nRamesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Rad-\nford, A., Chen, M., and Sutskever, I. Zero-shot text-\nto-image generation. arXiv preprint arXiv:2102.12092,\n2021.\nRazavi, A., van den Oord, A., and Vinyals, O. Generating\ndiverse high-ﬁdelity images with vq-vae-2. In Advances\nin Neural Information Processing Systems, pp. 14866–\n14876, 2019.\nSaito, M. and Saito, S. Tganv2: Efﬁcient training of large\nmodels for video generation with multiple subsampling\nlayers. arXiv preprint arXiv:1811.09245, 2018.\nSaito, M., Matsumoto, E., and Saito, S. Temporal generative\nadversarial nets with singular value clipping. In Proceed-\nings of the IEEE international conference on computer\nvision, pp. 2830–2839, 2017.\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V .,\nRadford, A., and Chen, X. Improved techniques for\ntraining gans. arXiv preprint arXiv:1606.03498, 2016.\nSalimans, T., Karpathy, A., Chen, X., and Kingma, D. P.\nPixelcnn++: Improving the pixelcnn with discretized lo-\ngistic mixture likelihood and other modiﬁcations. arXiv\npreprint arXiv:1701.05517, 2017.\nSohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequilib-\nrium thermodynamics. arXiv preprint arXiv:1503.03585,\n2015.\nSønderby, C. K., Espeholt, L., Heek, J., Dehghani, M.,\nOliver, A., Salimans, T., Agrawal, S., Hickey, J., and\nKalchbrenner, N. Metnet: A neural weather model for pre-\ncipitation forecasting. arXiv preprint arXiv:2003.12140,\n2020.\nSong, Y . and Ermon, S. Generative modeling by estimating\ngradients of the data distribution. In Advances in Neural\nInformation Processing Systems, pp. 11918–11930, 2019.\nSoomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset\nof 101 human actions classes from videos in the wild.\narXiv preprint arXiv:1212.0402, 2012.\nSrivastava, N., Mansimov, E., and Salakhudinov, R. Unsu-\npervised learning of video representations using lstms. In\nInternational conference on machine learning, pp. 843–\n852, 2015.\nTran, D., Bourdev, L., Fergus, R., Torresani, L., and Paluri,\nM. Learning spatiotemporal features with 3d convolu-\ntional networks. In Proceedings of the IEEE international\nconference on computer vision, pp. 4489–4497, 2015.\nTulyakov, S., Liu, M.-Y ., Yang, X., and Kautz, J. Mocogan:\nDecomposing motion and content for video generation. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 1526–1535, 2018.\nVahdat, A. and Kautz, J. Nvae: A deep hierarchical vari-\national autoencoder. arXiv preprint arXiv:2007.03898,\n2020.\nvan den Oord, A., Dieleman, S., Zen, H., Simonyan, K.,\nVinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and\nKavukcuoglu, K. Wavenet: A generative model for raw\naudio. arXiv preprint arXiv:1609.03499, 2016a.\nvan den Oord, A., Kalchbrenner, N., and Kavukcuoglu, K.\nPixel recurrent neural networks.International Conference\non Machine Learning (ICML), 2016b.\nvan den Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt,\nL., Graves, A., and Kavukcuoglu, K. Conditional im-\nage generation with pixelcnn decoders. arXiv preprint\narXiv:1606.05328, 2016c.\nVan Den Oord, A., Vinyals, O., et al. Neural discrete rep-\nresentation learning. In Advances in Neural Information\nProcessing Systems, pp. 6306–6315, 2017.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. arXiv preprint arXiv:1706.03762, 2017.\nV ondrick, C., Pirsiavash, H., and Torralba, A. Generat-\ning videos with scene dynamics. In Advances in neural\ninformation processing systems, pp. 613–621, 2016.\nWalker, J., Razavi, A., and Oord, A. v. d. Predicting video\nwith vqvae. arXiv preprint arXiv:2103.01950, 2021.\nWallace, G. K. The jpeg still picture compression standard.\nIEEE transactions on consumer electronics, 38(1):xviii–\nxxxiv, 1992.\nWang, T.-C., Liu, M.-Y ., Zhu, J.-Y ., Liu, G., Tao, A., Kautz,\nJ., and Catanzaro, B. Video-to-video synthesis. arXiv\npreprint arXiv:1808.06601, 2018.\nWang, X., Girshick, R., Gupta, A., and He, K. Non-local\nneural networks. arXiv preprint arXiv:1711.07971, 2017.\nWeissenborn, D., T ¨ackstr¨om, O., and Uszkoreit, J. Scal-\ning autoregressive video models. arXiv preprint\narXiv:1906.02634, 2019.\nVideoGPT\nYushchenko, V ., Araslanov, N., and Roth, S. Markov de-\ncision process for video generation. In Proceedings of\nthe IEEE International Conference on Computer Vision\nWorkshops, pp. 0–0, 2019.\nZhang, H., Goodfellow, I., Metaxas, D., and Odena, A. Self-\nattention generative adversarial networks. In Interna-\ntional Conference on Machine Learning, pp. 7354–7363.\nPMLR, 2019.\nVideoGPT\nA. Architecture Details and Hyperparameters\nA.1. VQ-V AE Encoder and Decoder\nTable 8.Hyperparameters of VQ-V AE encoder and decoder models for each dataset\nMoving MNIST BAIR / RoboNet / ViZDoom UCF-101 / TGIF\nInput size 16 ×64 ×64 16 ×64 ×64 16 ×64 ×64\nLatent size 4 ×16 ×16 8 ×32 ×32 4 ×32 ×32\nβ(commitment loss coefﬁcient) 0.25 0.25 0.25\nBatch size 32 32 32\nLearning rate 7 ×10−4 7 ×10−4 7 ×10−4\nHidden units 240 240 240\nResidual units 128 128 128\nResidual layers 2 4 4\nUses attention No Yes Yes\nCodebook size 512 1024 1024\nCodebook dimension 64 256 256\nEncoder ﬁlter size 3 3 3\nUpsampling conv ﬁlter size 4 4 4\nTraining steps 20k 100K 100K\nA.2. Prior Networks\nTable 9.Hyperparameters of prior networks for each dataset\nMoving MNIST BAIR / RoboNet ViZDoom UCF-101 / TGIF\nInput size 4 ×16 ×16 8 ×32 ×32 8 ×32 ×32 4 ×32 ×32\nConditional sizes 1 ×64 ×64 3 ×64 ×64, 64 60 (HGS), 315 (Battle2) n/a\nBatch size 32 32 32 32\nLearning rate 3 ×10−4 3 ×10−4 3 ×10−4 3 ×10−4\nV ocabulary size 512 1024 1024 1024\nAttention heads 4 4 4 8\nAttention layers 8 16 16 20\nEmbedding size 192 512 512 1024\nFeedforward hidden size 384 2048 2048 4096\nResnet depth 18 34 n/a n/a\nResnet units 512 512 n/a n/a\nDropout 0.1 0.2 0.2 0.2\nTraining steps 80k 150K 150K 200K / 600K\nVideoGPT\nB. ViZDoom Samples\nFigure 9.Samples for ViZDoom health gathering supreme environment. (Top) shows unconditionally generated samples. (Bottom) shows\nsamples conditioned on the same action sequence (turn right and go straight).\nFigure 10.Samples for ViZDoom battle2 environment. (Top) shows unconditionally generated samples. (Bottom) shows three samples\nconditioned on the same action sequence (moving forward and right).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7856415510177612
    },
    {
      "name": "Transformer",
      "score": 0.7157824635505676
    },
    {
      "name": "Architecture",
      "score": 0.6840344667434692
    },
    {
      "name": "High fidelity",
      "score": 0.612105667591095
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5832098126411438
    },
    {
      "name": "Fidelity",
      "score": 0.5288969874382019
    },
    {
      "name": "Generative grammar",
      "score": 0.4839901924133301
    },
    {
      "name": "Simplicity",
      "score": 0.4674307703971863
    },
    {
      "name": "Generative model",
      "score": 0.43015530705451965
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36731261014938354
    },
    {
      "name": "Computer vision",
      "score": 0.35340380668640137
    },
    {
      "name": "Engineering",
      "score": 0.06520923972129822
    },
    {
      "name": "Telecommunications",
      "score": 0.05516353249549866
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": []
}