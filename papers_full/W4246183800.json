{
  "title": "TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning",
  "url": "https://openalex.org/W4246183800",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2096132549",
      "name": "Kexin Wang",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2132681681",
      "name": "Nils Reimers",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A40109512",
      "name": "Iryna Gurevych",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891177506",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2963508788",
    "https://openalex.org/W2997574889",
    "https://openalex.org/W2605035112",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2159849140",
    "https://openalex.org/W2256784804",
    "https://openalex.org/W3035324702",
    "https://openalex.org/W3154229486",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2572185161",
    "https://openalex.org/W3122838366",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963001778",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963804993",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W3173783447",
    "https://openalex.org/W2892337787",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W2611029872",
    "https://openalex.org/W3005680577"
  ],
  "abstract": "Learning sentence embeddings often requires a large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1% of the performance of in-domain supervised approaches. Further, we show that TSDAE is a strong domain adaptation and pre-training method for sentence embeddings, significantly outperforming other approaches like Masked Language Model. A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TSDAE and other recent approaches on four different datasets from heterogeneous domains.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 671–688\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n671\nTSDAE: Using Transformer-based Sequential Denoising Auto-Encoder\nfor Unsupervised Sentence Embedding Learning\nKexin Wang, Nils Reimers, Iryna Gurevych\nUbiquitous Knowledge Processing Lab (UKP-TUDA)\nDepartment of Computer Science, Technical University of Darmstadt\nwww.ukp.tu-darmstadt.de\nAbstract\nLearning sentence embeddings often requires\na large amount of labeled data. However,\nfor most tasks and domains, labeled data is\nseldom available and creating it is expensive.\nIn this work, we present a new state-of-the-\nart unsupervised method based on pre-trained\nTransformers and Sequential Denoising Auto-\nEncoder (TSDAE) which outperforms previ-\nous approaches by up to 6.4 points. It can\nachieve up to 93.1% of the performance of in-\ndomain supervised approaches. Further, we\nshow that TSDAE is a strong domain adap-\ntation and pre-training method for sentence\nembeddings, signiﬁcantly outperforming other\napproaches like Masked Language Model.1\nA crucial shortcoming of previous studies is\nthe narrow evaluation: Most work mainly eval-\nuates on the single task of Semantic Textual\nSimilarity (STS), which does not require any\ndomain knowledge. It is unclear if these pro-\nposed methods generalize to other domains\nand tasks. We ﬁll this gap and evaluate TS-\nDAE and other recent approaches on four dif-\nferent datasets from heterogeneous domains.\n1 Introduction\nSentence embedding techniques encode sentences\ninto a ﬁxed-sized, dense vector space such that se-\nmantically similar sentences are close. The most\nsuccessful previous approaches like InferSent (Con-\nneau et al., 2017), Universial Sentence Encoder\n(USE) (Cer et al., 2018) and SBERT (Reimers and\nGurevych, 2019) heavily relied on labeled data to\ntrain sentence embedding models. However, for\nmost tasks and domains, labeled data is not avail-\nable and data annotation is expensive. To overcome\nthis limitation, unsupervised approaches have been\nproposed which learn to embed sentences just using\nan unlabeled corpus for training.\n1Code available at: https://github.com/\nUKPLab/sentence-transformers/\nWe propose a new approach: Transformer-based\nSequential Denoising Auto-Encoder (TSDAE). It\nsigniﬁcantly outperforms previous methods via an\nencoder-decoder architecture. During training, TS-\nDAE encodes corrupted sentences into ﬁxed-sized\nvectors and requires the decoder to reconstruct the\noriginal sentences from this sentence embedding.\nFor good reconstruction quality, the semantics must\nbe captured well in the sentence embedding from\nthe encoder. Later, at inference, we only use the\nencoder for creating sentence embeddings.\nA crucial shortcoming of previous unsupervised\napproaches is the evaluation. Often, approaches\nare mainly evaluated on the Semantic Textual Sim-\nilarity (STS) task from SemEval (Li et al., 2020;\nGiorgi et al., 2021; Carlsson et al., 2021; Gao et al.,\n2021). As we argue in Section 4, we perceive this\nas an insufﬁcient evaluation. The STS datasets do\nnot include sentences with domain speciﬁc knowl-\nedge, i.e., it remains unclear how methods will\nperform on more speciﬁc domains. Further, STS\ndatasets have an artiﬁcial score distribution, and\nthe performance on STS datasets does not corre-\nlate with downstream task performances (Reimers\net al., 2016). In conclusion, it remains unclear, how\nwell unsupervised sentence embedding methods\nwill perform on domain speciﬁc tasks.\nTo answer this question, we compare TSDAE\nwith previous unsupervised sentence embedding\napproaches on three different tasks (Information\nRetrieval, Re-Ranking and Paraphrase Identiﬁca-\ntion), for heterogeneous domains and different text\nstyles. We show that TSDAE can outperform other\nstate-of-the-art unsupervised approaches by up to\n6.4 points. TSDAE is able to perform on-par or\neven outperform existent supervised models like\nUSE-large, which had been trained with a lot of\nlabeled data from various datasets.\nFurther, we demonstrate that TSDAE works well\nfor domain adaptation and as a pre-training task.\nWe observe a signiﬁcant performance improvement\n672\ncompared to other pre-training tasks like Masked\nLanguage Model (MLM).\nOur contributions are three-fold:\n• We propose a novel unsupervised method, TS-\nDAE based on denoising auto-encoders. We\nshow that it outperforms the previous best ap-\nproach by up to 6.4 points on diverse datasets.\n• To the best of our knowledge, we are the ﬁrst\nto compare recent unsupervised sentence em-\nbedding methods for various tasks on hetero-\ngeneous domains.\n• TSDAE outperforms other methods including\nMLM by a large margin as a pre-training and\ndomain adaptation method.\n2 Related Work\nSupervised sentence embeddings utilize labels\nfor sentence pairs which provide the information\nabout the relation between the sentences. Since sen-\ntence embeddings are usually applied to measure\nthe similarity of a sentence pair, the most direct\nway is to label this similarity for supervised train-\ning (Henderson et al., 2017). Many studies also\nﬁnd that natural language inference (NLI), ques-\ntion answering and conversational context datasets\ncan successfully be used to train sentence embed-\ndings (Conneau et al., 2017; Cer et al., 2018). The\nrecently proposed Sentence-BERT (Reimers and\nGurevych, 2019) introduced pre-trained Transform-\ners to the ﬁeld of sentence embeddings. Although\nhigh-quality sentence embeddings can be derived\nvia supervised training, the labeling cost is a major\nobstacle for practical usage, especially for special-\nized domains.\nUnsupervised sentence embeddings utilize\nonly an unlabeled corpus during training. Recent\nwork combined pre-trained Transformers with dif-\nferent training objectives to achieve state-of-the-art\nresults on STS tasks. Among them, Contrastive\nTension (CT) (Giorgi et al., 2021) simply views\nthe identical and different sentences as positive and\nnegative examples, resp. and train two independent\nencoders; BERT-ﬂow (Li et al., 2020) trains model\nvia debiasing embedding distribution towards Gaus-\nsian; SimCSE (Gao et al., 2021) is based on con-\ntrastive learning (Hadsell et al., 2006; Chen et al.,\n2020) and views the identical sentences with dif-\nferent dropout mask as the positive examples. For\nmore details, please refer to Section 5. All of them\npooling\nencoder\ndecoder\ntext without noise \ntext with noise\nFigure 1: Architecture of TSDAE.\nrequires only independent sentences. By contrast,\nDeCLUTR (Giorgi et al., 2021) utilizes sentence-\nlevel contexts and requires long documents (2048\ntokens at least) for training. This requirement is\nhardly met for many cases, e.g. tweets or dialogues.\nThus, in this work we only consider methods which\nuses only single sentences during training.\nMost previous work mainly evaluate only on Se-\nmantic Textual Similarity (STS) from the SemEval\nshared tasks. As we show in Section 4, the unsu-\npervised approaches perform much worse than the\nout-of-the-box supervised pre-trained models even\nthough they were not speciﬁcally trained for STS.\nFurther, a good performance on STS does not nec-\nessarily correlate with the performance on down-\nstream tasks (Reimers et al., 2016). It remains un-\nclear how these methods perform on speciﬁc tasks\nand domains. To answer this, we compare three\nrecent powerful unsupervised methods based on\npre-trained Transformers including CT, SimCSE,\nBERT-ﬂow and our proposed TSDAE on different\ntasks of heterogeneous domains.\n3 Sequential Denoising Auto-Encoder\nAlthough Sequential Denoising Auto-Encoder\n(SDAE) (Vincent et al., 2010; Goodfellow et al.,\n2016; Hill et al., 2016) is a popular unsupervised\nmethod in machine learning, how to combine it\nwith pre-trained Transformers for learning sentence\nembeddings remains unclear. In this section, we\nﬁrst introduce the training objective of TSDAE and\nthen give the optimal conﬁguration of TSDAE.\n3.1 Training Objective\nFigure 1 illustrates the architecture of TSDAE. TS-\nDAE train sentence embeddings by adding a certain\ntype of noise (e.g. deleting or swapping words) to\n673\ninput sentences, encoding the damaged sentences\ninto ﬁxed-sized vectors and then reconstructing the\nvectors into the original input. Formally, the train-\ning objective is:\nJSDAE(θ) =Ex∼D[log Pθ(x|˜x)]\n= Ex∼D[\nl∑\nt=1\nlog Pθ(xt|˜x)]\n= Ex∼D[\nl∑\nt=1\nlog exp(hT\nt et)∑N\ni=1 exp(hT\nt ei)\n]\nwhere D is the training corpus, x = x1x2 ···xl\nis the input sentence with ltokens, ˜xis the corre-\nsponding damaged sentence, et is the word embed-\nding of xt, N is the vocabulary size and ht is the\nhidden state at decoding step t.\nAn important difference to original transformer\nencoder-decoder setup presented in Vaswani et al.\n(2017) is the information available to the decoder:\nOur decoder decodes only from a ﬁxed-size sen-\ntence representation produced by the encoder. It\ndoes not have access to all contextualized word em-\nbeddings from the encoder. This modiﬁcation in-\ntroduces a bottleneck, that should force the encoder\nto produce a meaningful sentence representation.\n3.2 TSDAE\nThe model architecture of TSDAE is a modiﬁed\nencoder-decoder Transformer where the key and\nvalue of the cross-attention are both conﬁned to the\nsentence embedding only. Formally, the formula-\ntion of the modiﬁed cross-attention is:\nH(k) = Attention(H(k−1),[sT],[sT])\nAttention(Q,K,V ) = softmax\n(QKT\n√\nd\n)\nV\nwhere H(k) ∈Rt×d is the decoder hidden states\nwithin tdecoding steps at the k-th layer, dis the\nsize of the sentence embedding, [sT] ∈R1×d is\na one-row matrix including the sentence embed-\nding vector and Q, Kand V are the query, key and\nvalue, respectively. By exploring different conﬁgu-\nrations on the STS benchmark dataset (Cer et al.,\n2017), we discover that the best combination is:\n(1) adopting deletion as the input noise and setting\nthe deletion ratio to 0.6, (2) using the output of the\n[CLS] token as ﬁxed-sized sentence representa-\ntion (3) tying the encoder and decoder parameters\nduring training. For the detailed tuning process,\nplease refer to Appendix A.\n4 Evaluation\nPrevious unsupervised sentence embedding learn-\ning approaches (Giorgi et al., 2021; Carlsson et al.,\n2021; Li et al., 2020; Su et al., 2021; Gao et al.,\n2021) primarily evaluated on the task of Semantic\nTextual Similarity (STS) with data from SemEval\nusing Pearson or Spearman’s rank correlation.\nWe ﬁnd the (sole) evaluation on STS problem-\natic. As shown in (Reimers et al., 2016), perfor-\nmance on the STS dataset does not correlate with\ndownstream task performance, i.e. an approach\nworking well on the STS tasks must not be a good\nchoice for downstream tasks. We conﬁrm this with\nour experiments, the performance on the STS tasks\ndoes not correlate with the performance on other\n(real-world) tasks. See Section 6.1 for more details\non this.\nThis has multiple reasons: First, the STS datasets\nconsists of sentences which do not require domain-\nspeciﬁc knowledge, they are primarily from news\nand image captions. It is unclear how approaches\nwill work for domain-speciﬁc tasks. Second, the\nSTS datasets have an artiﬁcial score distribution -\ndissimilar and similar pairs appear roughly equally.\nFor most real-word tasks, there is an extreme skew\nand only a tiny fraction of pairs are considered\nsimilar. Third, to perform well on the STS datasets,\na method must rank dissimilar pairs and similar\npairs equally well. In contrast, most real-world\ntasks, like duplicate questions detection, related\npaper ﬁnding, or paraphrase mining, only require\nto identify the few similar pairs out of a pool of\nmillions of irrelevant combinations.\nA further shortcoming of previous evaluation\nsetups is just testing the case of unsupervised learn-\ning, ignoring labeled data that potentially exists. In\nmany scenarios, some labeled data exists, either di-\nrectly from the speciﬁc task or from other (similar)\ntasks. A good approach should also work if some\nlabeled data is available.\nHence, we propose to evaluate unsupervised sen-\ntence embedding approaches in following three\nsetups:\nUnsupervised Learning: We assume we just\nhave unlabeled sentences from the target task and\ntune our approaches based on these sentences.\nDomain Adaptation: We assume we have unla-\nbeled sentences from the target task and labeled sen-\ntences from NLI (Bowman et al., 2015; Williams\net al., 2018) and STS benchmark (Cer et al., 2017)\ndatasets. We test two setups: 1) Training on\n674\nNLI+STS data, then unsupervised training to the\ntarget domain, 2) Unsupervised training on the tar-\nget domain, then supervised training on NLI + STS.\nPre-Training: We assume we have a larger col-\nlection of unlabeled sentences from the target task\nand a smaller set of labeled sentences from the\ntarget task.\n4.1 Datasets\nWe evaluate these three settings on different tasks\nfrom heterogeneous (specialized) domains. The\ntasks include Re-Ranking (RR), Information Re-\ntrieval (IR) and Paraphrase Identiﬁcation (PI). In\ndetail, the datasets used are as follows2:\nAskUbuntu (RR task) is a collection of user\nposts from the technical forum AskUbuntu (Lei\net al., 2016). Models are required to re-rank 20\ncandidate questions according to the similarity\ngiven an input post. The candidates are obtained\nvia BM25 term-matching (Robertson et al., 1994).\nThe evaluation metric is Mean Average Precision\n(MAP).\nCQADupStack (IR task) is a question retrieval\ndataset of forum posts on various topics from Stack-\nExchange (Hoogeveen et al., 2015). In detail, it\nhas 12 forums including Android, English, gam-\ning, geographic information system, Mathematica,\nphysics, programmers, statistics, Tex, Unix, Web-\nmasters and WordPress. Models are required to\nretrieve duplicate questions from a large candidate\npool. The metric is MAP@100. We train a single\nmodel for all forums.\nTwitterPara (PI task) consists of two simi-\nlar datasets: the Twitter Paraphrase Corpus (PIT-\n2015) (Xu et al., 2015) and the Twitter News URL\nCorpus (noted as TURL) (Lan et al., 2017). The\ndataset consists of pairs of tweets together with a\ncrowd-annotated score if the pair is a paraphrase.\nThe evaluation metric is Average Precision (AP)\nover the gold conﬁdence scores and the similarity\nscores from the models.\nSciDocs (RR task) is a benchmark consisting\nof multiple tasks about scientiﬁc papers (Cohan\net al., 2020). In our experiments, we use the tasks\nof Cite: Given a paper title, identify the titles the\npaper is citing; Co-Cite (CC), Co-Read (CR), and\nCo-View (CV), for which we must ﬁnd papers that\nare frequently co-cited/-read/-viewed for a given\npaper title. For all these tasks, given one query\n2The dataset splits and the evaluation toolkit are available\nat: https://github.com/UKPLab/useb\npaper title, models are required to identify up to 5\nrelevant papers titles from up to 30 candidates. The\nnegative examples were selected randomly. The\nevaluation metric is MAP.\nFor evaluation, sentences are ﬁrst encoded into\nﬁxed-sized vectors and cosine similarity is used\nfor sentence similarity. Since we focus on embed-\ndings for sentences, we just use the titles from the\nAskUbuntu, CQADupStack and SciDocs datasets.\nFor the datasets with sub-datasets or sub-tasks in-\ncluding CQADupStack, TwitterPara and SciDocs,\nthe ﬁnal score is derived by averaging the scores\nfrom each sub-dataset or sub-task.\nFor unsupervised training, we just use the sen-\ntences from the training split without any labels.\nThe statistics for each dataset are shown in Table 1.\n5 Experiments\nIn this section, we compare our proposed TSDAE\nwith other unsupervised counterparts and out-of-\nthe-box supervised pre-trained models on the above\nmentioned tasks. For comparison, we include three\nrecent state-of-the-art unsupervised approaches:\nCT, SimCSE, and BERT-ﬂow. We use the pro-\nposed hyper-parameters from the respective paper.\nWithout other speciﬁcation, BERT-base-uncased3\nis used as the base Transformer model. To elim-\ninate the inﬂuence of randomness, we report the\nscores averaged over 5 random seeds. For other\ndetails, please refer to Appendix B.\n5.1 Baseline Methods\nWe compare the approaches against avg. GloVe\nembeddings (Pennington et al., 2014) and\nSent2Vec (Pagliardini et al., 2018). The former\ngenerates sentence embeddings by averaging word\nembeddings trained on a large corpus from the gen-\neral domain; the latter is also a bag-of-words model\nbut trained on the in-domain unlabeled corpus. The\nunsupervised baseline of BERT-base-uncased with\nmean pooling is also in comparison. We further\ncompare against existent pre-trained models: Uni-\nversial Sentence Embedding (USE) (Yang et al.,\n2020), which was trained on multiple supervised\ndatasets including NLI and community question\nanswering. From the Sentence-Transformers pack-\nage, we use SBERT-base-nli-v2 and SBERT-base-\nnli-stsb-v2: These models were trained on SNLI +\nMultiNLI data using the Multiple-Negative Rank-\ning Loss (MNRL) (Henderson et al., 2017) and the\n3Results for other checkpoints is reported in Appendix C\n675\nDataset Task #queries Avg.\n#relevant\nAvg.\n#candidates\nAvg.\nlength\nSize of\nunsupervised\ntraining set\nSize of\nsupervised\ntraining set\nAskUbuntu RR 200 5.9/5.4 20 9.2 165K 23K\nCQADupStack IR 3K 1.1/1.1 39K 8.6 44K 13K\nSciDocs RR 4K 5 30 12.5 312K 380K\nDataset Task #paraphrase #non-paraphrase Avg.\nlength\nSize of\nunsupervised\ntraining set\nSize of\nsupervised\ntraining set\nTwitterPara PI –/2K –/9K 13.9 53K 23K\nTable 1: Dataset statistics. The slash symbol ‘/’ separates the numbers for development and test. Multiple sub-\ndatasets are included in CQADupStack, SciDocs and TwitterPara. CQADupStack has one sub-dataset for each of\nthe 12 forums. The avg. #relevant, avg. #candidates and avg. length are all general statistics without distinguishing\nthe sub-datasets.\nMean Square Error (MSE) loss on the STS bench-\nmark train set. Further we include BM25 using\nElasticsearch for comparison.\nTo better understand the relative performance\nof these unsupervised methods, we also train\nSBERT models in an in-domain supervised man-\nner and view their scores as the upper bound. For\nAskUbuntu, CQADupStack and SciDocs, where\nthe relevant sentence pairs are labeled, the in-\ndomain SBERT models are trained with MNRL.\nMNRL is a cross-entropy loss with in-batch neg-\natives. For a batch of relevant sentences pairs\n{x(i),y(i)}M\ni=1, MNRL views the labeled pairs as\npositive and the other in-batch combinations as\nnegative. Formally, the training objective for each\nbatch is:\nJMNRL(θ) =\n1\nM\nM∑\ni=1\nlog exp σ(fθ(x(i)),fθ(y(i)))∑M\nj=1 exp σ(fθ(x(i)),fθ(y(j)))\nwhere σ is a certain similarity function for vec-\ntors and fθ is the sentence encoder that embeds\nsentences. For TwitterPara, whose relevant scores\nare labeled, the MSE loss is adopted to train the\nin-domain models.\n5.2 MLM\nMasked-Language-Model (MLM) is a ﬁll-in-the-\nblank task originally introduced by BERT: Words\nare masked from the input and the transformer net-\nwork must predict the missing words. We use the\noriginal setup in Devlin et al. (2019) except the\nnumber of training steps (100K), the batch size\n(8) and the learning rate (5e-5). To derive a sen-\ntence embedding, we perform mean-pooling of the\noutput token embeddings.\n5.3 Contrastive Tension (CT)\nCT (Carlsson et al., 2021) ﬁnetunes pre-trained\nTransformers in a contrastive-learning fashion. For\neach sentence, it construct a binary cross-entropy\nloss by viewing the same sentence as the relevant\nand samples K random sentences as the irrele-\nvant. To make the training process stable, for each\nsentence pair (a,b), CT uses two independent en-\ncoders fθ1 and fθ2 from the same initial parameter\npoint to encode the sentence aand b, respectively.\nFormally, the learning objective is:\nJCT(θ1,θ2) =E(a,b)∼D[ylog σ(fθ1 (a)Tfθ2 (b))\n+ (1−y) log(1−σ(fθ1 (a)Tfθ2 (b))]\nwhere y∈{0,1}represents whether sentence ais\nidentical to sentence band σis the Logistic func-\ntion. Despite its simplicity, CT achieves state-of-\nthe-art unsupervised performance on the Semantic\nTextual Similarity (STS) datasets.\n5.4 SimCSE\nSimilar to CT, SimCSE (Gao et al., 2021) also\nviews the identical sentences as the positive exam-\nples. The main difference is that SimCSE samples\ndifferent dropout masks for the same sentence to\ngenerate a embedding-level positive pair and uses\nin-batch negatives. Thus, this learning objective is\nequivalent to feeding each batch of sentences to the\nshared encoder twice and applying the MNRL-loss.\n5.5 BERT-ﬂow\nInstead of ﬁne-tuning the parameters of the pre-\ntrained Transformers, BERT-ﬂow (Li et al., 2020)\naims at fully exploiting the semantic information\nencoded by these pre-trained models themselves\nvia distribution debiasing. The paper of BERT-ﬂow\n676\nclaims that the BERT word embeddings are highly\nrelevant to the word frequency, which in turn inﬂu-\nences the hidden states via the Masked Language\nModeling (MLM) pre-training. This ﬁnally leads\nto biased sentence embeddings generated by the\npooling over these hidden states. To solve this prob-\nlem, BERT-ﬂow inputs the biased sentence embed-\nding into a trainable ﬂow network fφ (Kingma and\nDhariwal, 2018) for debiasing via ﬁtting a standard\nGaussian distribution, while keeping the parame-\nters of the BERT model unchanged. Formally, the\ntraining objective is:\nJBERT-ﬂow(φ) =Ex∼D[log pU(u)] (1)\n= Eu[log(pZ(f−1\nφ (u))|det\n∂f−1\nφ (u)\n∂u |)] (2)\nwhere u is the biased embedding of sentence x\nand z = f−1\nφ (u) is the debiased sentence embed-\nding which follows a standard Gaussian distribu-\ntion. Equation 2 is derived by applying the change-\nof-variables theorem to Equation 1.\nAs BERT-ﬂow does not update the parameters\nof the underlying Transformer network, we just re-\nports scores for BERT-ﬂow for unsupervised learn-\ning and domain adaptation NLI+STS→target task.\nIt is not suitable for the other evaluation setups we\nused. We re-implemented BERT-ﬂow under the Py-\ntorch framework, which can reproduce the reported\nresults in the original paper.4\n6 Results\nUnsupervised learning: The results in Table 2\nshow that TSDAE can outperform the previous\nbest approach (CT) by up-to 6.4 points (on Sci-\nDocs on average) and 2.6 points on average over\nall tasks. Surprisingly, a simple Masked-Language-\nModeling (MLM) approach with mean pooling,\nwhich performs badly when evaluated on STS data,\nis the second best unsupervised approach, outper-\nforming more recent approaches like CT, SimCSE,\nand BERT-ﬂow on the selected tasks. TSDAE and\nMLM both removes words from the input, forcing\nthe network to produce robust embeddings. In con-\ntrast, the input sentences for CT and SimCSE are\nnot modiﬁed, resulting in less stable embeddings.\nOur experiments also show that out-of-the-box pre-\ntrained models (SBERT-base-nli-stsb-v2 and USE-\nlarge) achieve strong results on our tasks without\n4Code available at: https://github.com/\nUKPLab/pytorch-bertflow\nany domain-speciﬁc ﬁne-tuning, outperforming re-\ncent proposed unsupervised learning approaches.\nDomain Adaptation: For all unsupervised\nmethods, we ﬁnd that ﬁrst training on the target\ndomain, and then training with labeled NLI+STS\nachieves better results than the opposite direction.\nFor all methods, we observe a performance increase\ncompared to only training on the target domain. On\naverage, the performance improves by 1.3 points\nfor TSDAE, 3.0 points for MLM, 0.6 points for\nCT, and 1.8 points for SimCSE. CT and SimCSE\nperform in this setting only slightly better than the\nout-of-the-box model SBERT-base-nli-stsb-v2.\nPre-training: In Figure 2 we compare the pre-\ntraining performance of the tested approaches: We\nﬁrst pre-train on all available unlabeled sentences\nand then perform in-domain supervised training\nwith different labeled training set sizes. Scores\nare reported by evaluation on the development sets.\nTSDAE outperforms MLM by a signiﬁcant mar-\ngin for all datasets except for AskUbuntu. There,\nMLM works slightly better. For the other datasets,\nTSDAE shows a clear out-performance to other\npre-training strategies. The difference is quite con-\nsistent also for larger labeled training sets. We\nconclude, that TSDAE works well as pre-training\nmethod and can signiﬁcantly improve the perfor-\nmance for later supervised training even for larger\ntraining datasets. CT and SimCSE don’t perform\nwell for pre-training, the results are far worse than\nusing TSDAE/MLM or even starting from the pre-\ntrained SBERT-nli-stsb model.\n6.1 Results on STS data\nWe sample sentences from Wikipedia as done\nby Carlsson et al. (2021) and train a BERT-base-\nuncased model on this dataset with the different un-\nsupervised training methods. In Table 3, we show\nthe performance (Spearman’s rank correlation) on\nthe test set of the STS benchmark5 along with the\navg. performance on our four domain-speciﬁc tasks.\nSee Appendix F for results on other STS datasets.\nWe observe quite different behaviour when eval-\nuating on STS data compared to evaluating on our\ndomain speciﬁc tasks. On STS data, CT and Sim-\nCSE perform strongly, outperforming MLM and\nTSDAE by a large margin. However, when ap-\n5In the original paper of BERT-ﬂow, the mean pooling over\nthe ﬁrst and the last layer is used, which causes the discrepancy\non the STS scores. However, for a comparable setting, as the\nchoice of most of the previous work, we only consider the\npooling over the last layer.\n677\nMethod AskU. CQADup. TwitterP. SciDocs Avg.\nSub-task/-dataset TURL PIT Avg. Cite CC CR CV Avg.\nUnsupervised learning based on BERT-base\nTSDAE 59.4† 14.5† 76.8† 69.2 73.0 71.4† 73.9† 75.0† 75.6† 74.0† 55.2†\nMLM 54.3 11.7 71.9 69.7 70.8 71.2 75.8 75.1 76.2 74.6 52.9\nCT 56.3 13.3 74.6 70.4 72.5 63.4 67.1 70.1 69.7 67.6 52.4\nSimCSE 55.9 12.4 74.5 62.5 68.5 62.5 65.1 67.7 67.6 65.7 50.6\nBERT-ﬂow 53.7 9.2 72.8 65.7 69.3 61.3 62.8 66.7 67.1 64.5 49.2\nDomain adaptation: NLI+STS→target task\nTSDAE 58.7 13.6 75.8 66.2 71 69.9† 73.8† 75† 75.7† 73.6† 54.2†\nMLM 54.4 9.7 69.8 68.1 69 67.1 71.8 72.6 72.9 71.1 51.1\nCT 57.9 14.2 75.6 70.6 73.1 62.3 66.2 68.5 68.9 66.5 52.9\nSimCSE 56.6 13.8 73.4 65.9 69.7 61.8 63.7 67.01 66.7 64.8 51.2\nBERT-ﬂow 58.2 13.9 76.5 67.4 72 62.2 64.8 68.1 68 65.8 52.5\nDomain adaptation: target task→NLI+STS\nTSDAE 59.4† 14.4† 75.8 73.1† 74.5† 75.6† 78.6† 78.1† 78.2† 77.6† 56.5†\nMLM 60.6 14.3 75.0 68.6 71.8 74.7 78.2 77.0 77.6 76.9 55.9\nCT 56.4 13.4 75.9 68.9 72.4 66.5 69.6 70.6 72.2 69.7 53.0\nSimCSE 56.2 13.1 75.5 67.3 71.4 65.5 68.5 70.0 71.4 68.9 52.4\nOther previous unsupervised approaches\nBM25 53.4 13.3 71.9 70.5 71.2 58.9 61.3 67.3 66.9 63.6 50.4\nAvg. GloVe 51.0 10.0 70.1 52.1 61.1 58.8 60.6 64.2 65.4 62.2 46.1\nSent2Vec 49.0 3.2 47.5 39.9 43.7 61.6 66.0 66.1 66.7 65.1 40.2\nBERT-base-uncased 48.5 6.5 69.1 61.7 65.4 59.4 65.1 65.4 68.6 64.6 46.3\nOut-of-the-box supervised pre-trained models\nSBERT-base-nli-v2 53.4 11.8 75.4 69.9 72.7 66.8 70.0 70.7 72.8 70.1 52.0\nSBERT-base-nli-stsb-v254.5 12.9 75.9 68.5 72.2 66.2 69.2 69.9 72.3 69.4 52.3\nUSE-large (59.3) (15.9) 77.1 69.8 73.5 67.1 69.5 71.4 72.6 70.2 54.7\nIn-domain supervised training (upper bound)\nSBERT-supervised 63.8 16.3 81.6 75.8 78.7 90.4 91.2 86.2 83.6 87.9 61.6\nTable 2: Evaluation using average precision. Results are averaged over 5 random seeds. The best results excluding\nthe upper bound are bold. USE-large was trained with in-domain training data for AskUbuntu and CQADupStack\n(scores in italic). Our proposed TSDAE signiﬁcantly outperforms other unsupervised and supervised out-of-the-\nbox approaches.†marks the cases where TSDAE outperforms both CT and SimCSE in all 5 runs.\nMethod STSb Speciﬁc Tasks\nUnsupervised method\nTSDAE 66.0 55.2\nMLM 47.3 52.9\nCT 73.9 52.4\nSimCSE 73.8 50.6\nBERT-ﬂow 48.9 49.2\nOut-of-the-box supervised pre-trained models\nSBERT-base-nli-v2 83.9 52.0\nSBERT-base-nli-stsb-v287.3 52.3\nUSE-large 80.9 54.7\nTable 3: Performance (Spearman’s rank correlation) on\nthe STS benchmark test set. Speciﬁc tasks: Average\nperformance from Table 2.\nplied to domain-speciﬁc real-world tasks, TSDAE\nand MLM are outperforming CT and SimCSE. We\nthink these are due to the reasons mentioned in\nSection 4. Overall, we conclude that a strong per-\nformance on STS data is not a good indicator for\ngood performance on domain-speciﬁc tasks.\n7 Analysis\nWe analyze how many training sentences are\nneeded and if relevant content words are identiﬁed.\nFor all the datasets except TwitterPara, the anal-\nysis is carried out on the development set. For\nTwitterPara, the test set is used, as it has no devel-\nopment split released by the original paper. All\nthe hyper-parameters are chosen up-front without\ntuning to a particular dataset.\n7.1 Inﬂuence of Corpus Size\nIn certain domains, getting a sufﬁciently high num-\nber of (unlabeled) sentences can be challenging.\nHence, data efﬁciency and deriving good sentence\nembeddings even with little unlabeled training data\ncan be important.\nIn order to study this, we train the unsupervised\napproaches with different corpus sizes: Between\n128 and 65,536 sentences. For each experiment, we\ntrain a bert-base-uncased model with 10 epochs up\nto 100k training steps. The models are evaluated\n678\n0 1 2 3 4 5 650\n55\n60\n(a) AskUbuntu\n0 1 2 3 4 5 6\n10\n15\nSBERT\nTSDAE + SBERT\nMLM + SBERT\nCT + SBERT\nSimCSE + SBERT\nSBERT-nli-stsb\nTSDAE (b) CQADupStack\n0 1 2 3 4 5 6 7\n65\n70\n75\n(c) TwitterPara\n0 1 2 3 4 5 6\n70\n80\n (d) SciDocs\nFigure 2: Comparison of different pre-training ap-\nproaches (TSDAE/MLM/CT/SimCSE+SBERT) with\nincreasing sizes of labeled training data (in thou-\nsands). SBERT: Training from the standard BERT-\nbase-uncased checkpoint. TSDAE: Unsupervised base-\nline. Larger plots: Appendix E.\nat the end of each epoch and the best score on the\ndevelopment set is reported.\nThe results are shown in Figure 3. We observe\nthat TSDAE is outperforming previous unsuper-\nvised learning methods often with as little as 1000\nunlabeled sentences. With 10K unlabeled sen-\ntences, the downstream performance usually stag-\nnates for all tested unsupervised sentence embed-\nding methods. The only exception where more\ntraining data is helpful is for the CQADupStack\ntask. This is expected, as the CQADupStack con-\nsists of 12 vastly different StackExchange forums,\nhence, requiring more unlabeled data to represent\nall domains well.\nWe conclude that comparatively little unlabeled\ndata of ∼10K sentences is needed to tune pre-\ntrained transformers to a speciﬁc domain.\n7.2 Relevant Content Words\nNot all word types play an equal role in determin-\ning the semantics of a sentence. Often, nouns are\nthe critical content words in a sentence, while e.g.\nprepositions are less important and can be add /\nremoved from a sentences without changing the\ncontent too much.\nIn this section, we investigate which word types\nare the most relevant for the different sentence em-\nbedding methods, i.e., which words (part-of-speech\n0 10 20 30 40 50 60\n45\n50\n55\n(a) AskUbuntu\n0 10 20 30 40 50 60\n5\n10\n15\n (b) CQADupStack\n0 10 20 30 40 50 60\n40\n50\n60\n70\nTSDAE\nMLM\nCT\nSimCSE\nBERT-flow\n(c) TwitterPara\n0 10 20 30 40 50 60\n50\n60\n70\n (d) SciDocs\nFigure 3: The inﬂuence of the number of training\nsentences (in thousands) on the model performance.\nLarger plots: Appendix G.\nIN PRPSYM\nVB-AUXVB-NAUX\nDT CC NN JJ RB CD\nOTHER\nSBERT-sup.\nUSE-large\nSBERT-nli-stsb\nTSDAE\nMLM\nCT\nSimCSE\nBERT-flow\nPrior\n1.8 0.3 0.8 0.6 12.0 0.5 0.3 70.9 8.4 0.5 3.0 1.0\n0.3 0.2 2.0 0.3 6.5 0.1 0.6 75.1 8.8 0.6 3.1 2.4\n0.8 0.4 0.8 0.3 10.4 0.6 0.2 70.8 9.0 1.4 3.7 1.8\n2.2 0.4 1.2 0.7 13.0 0.9 0.6 66.2 9.7 1.1 2.5 1.6\n3.3 0.3 5.7 0.8 11.8 0.6 0.4 66.4 6.4 0.5 2.2 1.6\n1.0 0.3 2.2 0.5 12.6 0.2 0.3 72.0 8.5 0.6 1.0 0.9\n2.0 0.7 1.3 1.0 14.0 0.8 0.3 67.9 8.0 0.5 1.4 2.1\n2.9 0.8 3.7 1.2 10.0 1.2 0.5 68.8 6.4 0.6 1.5 2.3\n10.7 2.5 9.1 3.5 10.6 5.0 1.9 38.8 7.3 1.3 2.4 6.9\n20\n40\n60\nFrequency%\nFigure 4: POS tag for the most relevant content word in\na sentence, i.e. the word that mostly inﬂuences if a sen-\ntence pair is considered as similar. VB-AUX/-NAUX\nrepresents auxiliary/non-auxiliary verbs.\ntags) mainly inﬂuence if a sentence pair is per-\nceived as similar or not. We are especially inter-\nested if we observe differences between in-domain\nsupervised approaches (SBERT-sup.), out-of-the-\nbox pre-trained approaches, and unsupervised ap-\nproaches.\nTo measure this, we select a sentence pair (a,b)\nthat is labeled as relevant and ﬁnd the word that\nmaximally reduces the cosine-similarity score for\nthe pair (a, b):\nˆw=argmaxw\n(\ncossim(a,b)−\nmin(cossim(a\\w,b),cossim(a,b \\w))\n)\namong all words w that appear in either a or b.\nThen, we record the POS tag for ˆwand compute the\ndistribution of POS tags across all sentence pairs.\n679\nCheckpointAskU.CQADup.TwitterP.SciDocsAvg.\nBERT-base59.4/2.214.5/3.4 73.0/2.474.0/2.855.2/2.7\nScratch 56.6/2.6 8.4/4.2 69.8/3.367.2/3.550.5/3.4\nBART-base58.5/1.4 9.5/2.0 60.3/1.562.0/1.747.6/1.7\nT5-base 45.6/1.0 2.2/1.4 48.2/1.530.8/1.131.7/1.3\nTable 4: Test performance/training loss of TSDAE\nmodels starting from different checkpoints. The results\nfor BERT-base are copied from Table 2.\nPOS-tags are determined using CoreNLP (Manning\net al., 2014).\nThe result averaged over the four datasets is\nshown in Figure 4. For the result on each dataset,\nplease refer to Appendix H. Comparing the in-\ndomain supervised model (SBERT-sup.) and the\nprior distribution of the POS tags, we ﬁnd that\nnouns (NN) are by far the most relevant content\nwords in a sentence, while function words such\nas prepositions (IN) and determinators (DT) have\nlittle inﬂuence on the model prediction. Surpris-\ningly, we do not perceive signiﬁcant differences\nbetween all the approaches. This is good news\nfor the unsupervised methods (TSDAE, CT, Sim-\nCSE and BERT-ﬂow) and show that they can learn\nwhich words types are critical in a sentence without\naccess to labeled data. On the down side, unsuper-\nvised approaches might have issues for tasks where\nnouns are not the most critical content words.\n8 Discussion\nWe mainly experiment with pre-trained Trans-\nformer encoders in this work. Besides single en-\ncoders, there are also pre-trained encoder-decoder\nmodels like BART (Lewis et al., 2020) and T5 (Raf-\nfel et al., 2020). However, they are already exten-\nsively pre-trained with variants of auto-encoder\nloss on the general domain and they are suspected\nof overﬁtting the reconstruction behavior. To ver-\nify this idea, we also further train BART-base and\nT5-base models with TSDAE on the 4 domain-\nspeciﬁc datasets. The results are shown in Table 4.\nWe observe that BART and T5 can achieve much\nlower training loss (1.7 and 1.3 on average, resp.)\nthan from scratch (3.4) or BERT (2.7), but they\nachieve rather bad test performance, even worse\nthan from scratch. Compared with training from\nscratch (which is similar to Zhang et al. (2018)),\non the other hand, we ﬁnd starting from BERT\ncan reach to a much better balance point between\nloss ﬁtting and generalization. Thus, we conclude\nthat TSDAE is more suitable to start from single en-\ncoder checkpoints, which can utilize the pre-trained\nknowledge while avoiding overﬁtting.\n9 Conclusion\nIn this work, we propose a new unsupervised sen-\ntence embedding learning method based on pre-\ntrained Transformers and sequential deoising auto-\nencoder (TSDAE). We evaluate TSDAE on other,\nrecent state-of-the-art unsupervised learning on\nfour different tasks from heterogeneous (special-\nized) domains in three different settings: unsu-\npervised learning, domain adaptation, and pre-\ntraining.\nWe observe that TSDAE performs well on the\nselected tasks and for the different settings, signiﬁ-\ncantly outperforming other approaches.\nFurther, we show that the current evaluation\nof unsupervised sentence embedding learning ap-\nproach, which is primarily done on the Semantic\nTextual Similarity (STS) task, is insufﬁcient: A\nstrong performance on STS does not correlate with\na good performance on speciﬁc tasks. Many recent\nunsupervised approaches are not able to outperform\nout-of-the-box pre-trained models on the selected\ntasks.\nAcknowledgments\nThis work has been supported by the German\nResearch Foundation (DFG) as part of the UKP-\nSQuARE project (grant GU 798/29-1), by the Eu-\nropean Regional Development Fund (ERDF) and\nthe Hessian State Chancellery – Hessian Minis-\nter of Digital Strategy and Development under the\npromotional reference 20005482 (TexPrax), and\nby the German Federal Ministry of Education and\nResearch and the Hessian Ministry of Higher Edu-\ncation, Research, Science and the Arts within their\njoint support of the National Research Center for\nApplied Cybersecurity ATHENE.\nReferences\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large an-\nnotated corpus for learning natural language infer-\nence. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2015, Lisbon, Portugal, September 17-21,\n2015, pages 632–642. The Association for Compu-\ntational Linguistics.\nFredrik Carlsson, Magnus Sahlgren, Evangelia\nGogoulou, Amaru Cuba Gyllensten, and Erik Ylipää\nHellqvist. 2021. Semantic re-tuning with contrastive\n680\ntension. In International Conference on Learning\nRepresentations, ICLR 2021, Vienna, Austria, May\n3-7, 2021, pages 1–21. International Conference on\nLearning Representations.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nBrian Strope, and Ray Kurzweil. 2018. Univer-\nsal sentence encoder for english. In Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2018: Sys-\ntem Demonstrations, Brussels, Belgium, October 31\n- November 4, 2018, pages 169–174. Association for\nComputational Linguistics.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey E. Hinton. 2020. A simple framework\nfor contrastive learning of visual representations. In\nProceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 1597–1607. PMLR.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel S. Weld. 2020. SPECTER:\ndocument-level representation learning using\ncitation-informed transformers. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020, pages 2270–2282. Association for\nComputational Linguistics.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670–680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence\nembeddings. arXiv preprint arXiv:2104.08821.\nJohn Giorgi, Osvald Nitski, Bo Wang, and Gary Bader.\n2021. DeCLUTR: Deep contrastive learning for\nunsupervised textual representations. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 879–895,\nOnline. Association for Computational Linguistics.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville.\n2016. Deep Learning. MIT Press. http://www.\ndeeplearningbook.org.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. 2006.\nDimensionality reduction by learning an invariant\nmapping. In 2006 IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR 2006), 17-22 June 2006, New York, NY, USA,\npages 1735–1742. IEEE Computer Society.\nMatthew L. Henderson, Rami Al-Rfou, Brian Strope,\nYun-Hsuan Sung, László Lukács, Ruiqi Guo, Sanjiv\nKumar, Balint Miklos, and Ray Kurzweil. 2017. Ef-\nﬁcient Natural Language Response Suggestion for\nSmart Reply. arXiv preprint arXiv:1705.00652.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nfrom unlabelled data. In NAACL HLT 2016, The\n2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, San Diego California,\nUSA, June 12-17, 2016, pages 1367–1377. The As-\nsociation for Computational Linguistics.\nDoris Hoogeveen, Karin M. Verspoor, and Timothy\nBaldwin. 2015. Cqadupstack: A benchmark data\nset for community question-answering research. In\nProceedings of the 20th Australasian Document\nComputing Symposium, ADCS 2015, Parramatta,\nNSW, Australia, December 8-9, 2015, pages 3:1–3:8.\nACM.\nDiederik P. Kingma and Prafulla Dhariwal. 2018.\nGlow: Generative ﬂow with invertible 1x1 convolu-\ntions. In Advances in Neural Information Process-\ning Systems 31: Annual Conference on Neural Infor-\nmation Processing Systems 2018, NeurIPS 2018, De-\ncember 3-8, 2018, Montréal, Canada, pages 10236–\n10245.\nWuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017.\nA continuously growing dataset of sentential para-\nphrases. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2017, Copenhagen, Denmark, September 9-\n11, 2017, pages 1224–1234. Association for Compu-\ntational Linguistics.\nTao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi S.\nJaakkola, Kateryna Tymoshenko, Alessandro Mos-\nchitti, and Lluís Màrquez. 2016. Semi-supervised\nquestion retrieval with gated convolutions. In\nNAACL HLT 2016, The 2016 Conference of the\n681\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, San Diego California, USA, June 12-17, 2016,\npages 1279–1289. The Association for Computa-\ntional Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 9119–\n9130. Association for Computational Linguistics.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The Stanford CoreNLP natural language pro-\ncessing toolkit. In Proceedings of 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations, pages 55–60, Bal-\ntimore, Maryland. Association for Computational\nLinguistics.\nMatteo Pagliardini, Prakhar Gupta, and Martin Jaggi.\n2018. Unsupervised learning of sentence embed-\ndings using compositional n-gram features. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 528–\n540. Association for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1532–1543.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nNils Reimers, Philip Beyer, and Iryna Gurevych. 2016.\nTask-oriented intrinsic evaluation of semantic tex-\ntual similarity. In COLING 2016, 26th International\nConference on Computational Linguistics, Proceed-\nings of the Conference: Technical Papers, December\n11-16, 2016, Osaka, Japan, pages 87–96. ACL.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nbert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages\n3980–3990. Association for Computational Linguis-\ntics.\nStephen E. Robertson, Steve Walker, Susan Jones,\nMicheline Hancock-Beaulieu, and Mike Gatford.\n1994. Okapi at TREC-3. In Proceedings of\nThe Third Text REtrieval Conference, TREC 1994,\nGaithersburg, Maryland, USA, November 2-4, 1994,\nvolume 500-225 of NIST Special Publication, pages\n109–126. National Institute of Standards and Tech-\nnology (NIST).\nJianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.\n2021. Whitening Sentence Representations for Bet-\nter Semantics and Faster Retrieval. arXiv preprint\narXiv:2103.15316.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie,\nYoshua Bengio, and Pierre-Antoine Manzagol. 2010.\nStacked denoising autoencoders: Learning useful\nrepresentations in a deep network with a local de-\nnoising criterion. J. Mach. Learn. Res., 11:3371–\n3408.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages\n1112–1122. Association for Computational Linguis-\ntics.\nWei Xu, Chris Callison-Burch, and Bill Dolan. 2015.\nSemeval-2015 task 1: Paraphrase and semantic sim-\nilarity in twitter (PIT). In Proceedings of the\n9th International Workshop on Semantic Evalua-\ntion, SemEval@NAACL-HLT 2015, Denver, Col-\norado, USA, June 4-5, 2015, pages 1–11. The As-\nsociation for Computer Linguistics.\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy\nGuo, Jax Law, Noah Constant, Gustavo Hernández\nÁbrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung,\nBrian Strope, and Ray Kurzweil. 2020. Multilingual\nuniversal sentence encoder for semantic retrieval.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics: System\nDemonstrations, ACL 2020, Online, July 5-10, 2020,\npages 87–94. Association for Computational Lin-\nguistics.\n682\nMinghua Zhang, Yunfang Wu, Weikang Li, and Wei Li.\n2018. Learning universal sentence representations\nwith mean-max attention autoencoder. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 4514–4523.\nAssociation for Computational Linguistics.\n683\nA Optimal Conﬁguration of TSDAE\nTo obtain the optimal conﬁguration, we compare TSDAE models trained and evaluated on the general\ndomain without bias towards any speciﬁc domain. The greedy search is applied by sequentially ﬁnding\nthe best (1) noise type and ratio (2) pooling method and (3) weight tying scheme. Similar to the choice of\nCT and BERT-ﬂow, we train the models on the combination of SNLI and MultiNLI without labels and\nevaluate the models on the STS benchmark with the metric of Spearman rank correlation. The maximum\nnumber of training steps is 30K and the models are evaluated every 1.5K training steps, reporting the best\nvalidation performance. Scores are obtained by calculating the average over 5 random seeds.\nWe ﬁrst compare the scores of different noise types, ﬁxing the noise ratio as 0.3 (i.e. 30% tokens are\ninﬂuenced) and the pooling method as CLS pooling. The results are show in Table 5. This indicates\ndeletion is the best noise type. We then tune the noise ratio of the deletion noise and the results are shown\nin Table 6. This indicates 0.6 is the best noise ratio.\nType Delete Swap Mask Replace Add\nScore 78.33 76.85 76.56 74.01 72.65\nTable 5: Results with different noise types\nRatio 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nScore 77.81 77.70 77.75 78.02 78.25 78.77 78.19 77.69 75.67\nTable 6: Results with different noise ratio.\nWe then compare different pooling methods with the best setting so far. The results are shown in\nTable 7. Since there is little difference between CLS and mean pooling and mean pooling loses the\nposition information, the CLS pooling is chosen. Finally, we ﬁnd that tying the encoder and the decoder\ncan further improve the validation score to 79.15.\nMethod CLS Mean Max\nScore 78.77 78.84 78.17\nTable 7: Results with different pooling methods.\nB Experiment Settings\nWe implement TSDAE, CT and BERT-ﬂow based on Pytorch and Huggingface’s Transformers6 (version\nnumber: v3.1.0). For these three unsupervised methods, following the original papers, the number of\ntraining steps is 100K; the batch size is 8; the optimizers are AdamW, RMSProp and AdamW, respectively;\nthe initial learning rates are 3e-5, 1e-5 and 1e-6, resp. The weight decay for BERT-ﬂow is 0.01. The\nlearning rate for CT follows a segmented-constant scheduling scheme: 1e-5 for step 1 to 500; 8e-6 for step\n501 to 1000; 6e-6 for step 1001 to 1500; 4e-6 for step 1501 to 2000; 2e-6 for others. The pooling method\nfor CT and BERT-ﬂow is both mean pooling. Since CT trains two independent encoders and we ﬁnd the\nsecond encoder has better performance, we use the second encoder for evaluation. For SimCSE, since its\nofﬁcial hyper-parameter setting is very different from the other 3 methods, we use the ofﬁcial code7 along\nwith the default hyper-parameters. In detail, its hyper-parameters are: 1 epoch of training, batch size of\n512, AdamW optimizer with learning rate 5e-5 and a linear layer on the CLS token embedding as the\npooling method.\nSince in the real-world scenario where the labeled data is expensive to obtain, applying early-stopping\nwith a in-domain development set is impractical. Thus, in our unsupervised experiments, we do not use\n6https://github.com/huggingface/transformers\n7https://github.com/princeton-nlp/SimCSE\n684\nearly-stopping with in-domain labeled data and indicate a ﬁxed number of training steps 8 mentioned\nabove instead.\nWe use the repository of sentence-transformers 9 (version number: v0.3.8) to train the in-domain\nsupervised models. For them, the number of training epochs is 10; the maximum number of training\nsteps is 20K; the batch size is 64; the similarity function σis set to cosine similarity; early-stopping is\napplied by checking the validation performance. To eliminate the inﬂuence of randomness, we report the\nscores averaged over 5 random seeds for all the in-domain unsupervised and supervised models. All the\npre-trained checkpoints used are listed in Table 8.\nFor BM25, we use the implementation available on Elasticsearch10 with the default settings.\nModel Name URL\nDeCLUTR-base https://huggingface.co/johngiorgi/declutr-base\nELECTRA-base https://huggingface.co/google/electra-base-discriminator\nDistilRoBERTa-base https://huggingface.co/distilroberta-base\nRoBERTa-base https://huggingface.co/roberta-base\nDistilBERT-base https://huggingface.co/distilbert-base-uncased\nBERT-base https://huggingface.co/bert-base-uncased\nSBERT-base-nli-v2 https://huggingface.co/kwang2049/SBERT-base-nli-v2\nSBERT-base-nli-stsb-v2https://huggingface.co/kwang2049/SBERT-base-nli-stsb-v2\nSDRoBERTa-para https://huggingface.co/sentence-transformers/paraphrase-distilroberta-base-v1\nUSE-large https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\nBART-base https://huggingface.co/facebook/bart-base\nT5-base https://huggingface.co/t5-base\nTable 8: Model checkpoints used in this work.\nC Results of Other Checkpoints\nThe results of other checkpoints besides BERT-base-uncased are shown in Table 9. For all the methods,\nbetter results are achieved by using BERT checkpoints, which also makes TSDAE signiﬁcantly outper-\nforms others. We suppose this advantage comes from the additional pre-training task, next sentence\nprediction of the BERT models, which guides the model to learn from sentence-level contexts.\n8For SimCSE, the ofﬁcial code involves early-stopping on the STS-B development set. We do not change this setting for this\nmethod, since STS-B is not an in-domain dataset in our task- and domain-speciﬁc evaluation.\n9https://github.com/UKPLab/sentence-transformers\n10https://www.elastic.co/\n685\nMethod AskU. CQADup. TwitterP. SciDocs Avg.\nELECTRA-base\nTSDAE 56.6 +/- 1.1 8.0 +/- 0.3 69.0 +/- 1.6 66.2 +/- 5.6 49.9 +/- 1.1\nCT 50.3 +/- 0.4 5.0 +/- 0.2 66.5 +/- 0.7 46.1 +/- 0.6 41.6 +/- 0.8\nSimCSE 50.9 +/- 0.3 6.2 +/- 0.1 61.8 +/- 0.6 49.3 +/- 0.3 42.0 +/- 0.1\nBERT-ﬂow 51.3 +/- 0.3 5.2 +/- 0.0 62.4 +/- 0.1 41.2 +/- 0.1 38.4 +/- 3.0\nDistilRoBERTa-base\nTSDAE 58.9 +/- 0.5 12.5 +/- 0.1 68.5 +/- 0.5 59.3 +/- 0.6 49.9 +/- 0.4\nCT 57.9 +/- 0.8 13.8 +/- 0.3 63.6 +/- 1.4 62.7 +/- 0.5 49.8 +/- 0.4\nSimCSE 57.1 +/- 0.2 12.2 +/- 0.1 65.6 +/- 0.8 63.4 +/- 0.4 49.6 +/- 0.2\nBERT-ﬂow 56.0 +/- 0.2 11.1 +/- 0.1 68.5 +/- 0.1 53.0 +/- 0.2 46.9 +/- 0.1\nRoBERTa-base\nTSDAE 58.3 +/- 0.7 12.2 +/- 0.3 70.0 +/- 0.8 61.4 +/- 0.5 50.3 +/- 0.3\nCT 56.7 +/- 0.5 14.2 +/- 0.3 69.4 +/- 1.3 63.1 +/- 0.3 50.5 +/- 0.4\nSimCSE 56.6 +/- 0.5 12.4 +/- 0.2 66.8 +/- 0.7 64.4 +/- 0.3 50.1 +/- 0.2\nBERT-ﬂow 54.5 +/- 0.2 10.5 +/- 0.1 69.0 +/- 0.1 53.5 +/- 0.2 46.6 +/- 0.1\nDistilBERT-base\nTSDAE 59.2 +/- 0.3 14.6 +/- 0.1 73.9 +/- 0.3 72.3 +/- 0.9 54.9 +/- 0.2\nCT 57.7 +/- 0.8 14.0 +/- 0.3 66.4 +/- 0.4 72.2 +/- 0.7 52.3 +/- 0.3\nSimCSE 54.8 +/- 0.7 12.3 +/- 0.1 66.8 +/- 0.6 65.9 +/- 0.1 49.9 +/- 0.3\nBERT-ﬂow 55.0 +/- 0.2 11.0 +/- 0.0 65.9 +/- 0.0 70.5 +/- 0.1 50.5 +/- 0.1\nBERT-base\nTSDAE 59.4 +/- 0.3 14.5 +/- 0.1 73.0 +/- 0.4 74.0 +/- 0.4 55.2 +/- 0.2\nCT 56.3 +/- 0.7 13.3 +/- 0.3 72.5 +/- 0.5 67.6 +/- 0.4 52.4 +/- 0.3\nSimCSE 55.9 +/- 0.8 12.4 +/- 0.0 68.5 +/- 0.0 65.7 +/- 0.0 50.6 +/- 0.2\nBERT-ﬂow 53.7 +/- 0.2 9.2 +/- 0.1 69.3 +/- 0.2 64.5 +/- 0.1 49.2 +/- 0.1\nTable 9: Evaluation of different checkpoints using average precision. ‘+/-’ separates the mean value and standard\ndeviation over scores of 5 random seeds. Best results within each group are underlined and the overall best results\nare bold.\nD Equivalent Labeling Work\nThe goal of unsupervised sentence embedding learning methods is to eliminate the need of labeled training\ndata, which can be expensive in the creation. However, as shown in Section 6, approaches with sufﬁcient\nin-domain labeled data signiﬁcantly outperform unsupervised approaches.\nAs far as we know, previous work did not study the point of intersection between unsupervised and\nsupervised approaches: If you only need few labeled examples to outperform unsupervised approaches,\nannotating those might be the more viable solution.\nTo ﬁnd this intersection point, we train the in-domain supervised SBERT approach with varying size of\nlabeled training data. Results are shown in Figure 5. To estimate the intersection with more precision,\nwe apply binary search. We set the search precision to the standard deviation of the target score over 5\nrandom seeds.\nThe results are shown in Table 10. To match the performance of TSDAE, 140 - 6k annotated examples\nare required. CQADupStack and the TwitterParaphrase corpus, which compromise various domains,\nrequire more labeled data than AskUbuntu (1 domain). Surprisingly, SciDocs, which includes data from all\ntype of scientiﬁc domains, the in-domain supervised approach outperforms unsupervised approaches with\njust 464 labeled examples. This dataset appears to be especially challenging for unsupervised approaches,\nas we observe a large performance gap between in-domain supervised and unsupervised approaches.\nIn an annotation experiment on the Twitter dataset, we measured that annotating 100 Tweet pairs\ntakes about 20 minutes for an (experienced) annotator. Hence, the state-of-the-art unsupervised TSDAE\napproach achieves the same performance as a supervised approach with 0.5 - 20 hours of annotation work\nfor one annotator (2.5h - 100h for 5 crowd annotators).\n686\nAskU. CQADup. TwitterP.SciDocs Avg.\n140 2661 6067 464 2333\nTable 10: Intersection point (number of labeled sentence pairs) between unsupervised TSDAE and in-domain\nsupervised SBERT.\nE Usage for Pre-Training\nThe pre-training performance on AskUbuntu, CQADupStack and TwitterPara is shown in Figure 5.\n0 1 2 3 4 5\nNumber of relevant sentence pairs (thousands)\n48\n50\n52\n54\n56\n58\n60MAP%\nSBERT\nTSDAE + SBERT\nMLM + SBERT\nCT + SBERT\nSimCSE + SBERT\nSBERT-nli-stsb\nTSDAE\n(a) AskUbuntu\n0 1 2 3 4 5 6\nNumber of relevant sentence pairs (thousands)\n8\n10\n12\n14\n16MAP@100%\nSBERT\nTSDAE + SBERT\nMLM + SBERT\nCT + SBERT\nSimCSE + SBERT\nSBERT-nli-stsb\nTSDAE (b) CQADupStack\n0 2 4 6\nNumber of training examples (thousands)\n66\n68\n70\n72\n74\n76\n78AP%\nSBERT\nTSDAE + SBERT\nMLM + SBERT\nCT + SBERT\nSimCSE + SBERT\nSBERT-nli-stsb\nTSDAE\n(c) TwitterPara\n0 1 2 3 4 5 6\nNumber of relevant sentence pairs (thousands)\n65\n70\n75\n80MAP%\nSBERT\nTSDAE + SBERT\nMLM + SBERT\nCT + SBERT\nSimCSE + SBERT\nSBERT-nli-stsb\nTSDAE (d) SciDocs\nFigure 5: The inﬂuence of the number of training sentences on the model performance.\nF Detailed Results of Semantic Textual Similarity\nThe detailed results of STS on each dataset are shown in Table 11 with the evaluation metric of Spearman’s\nrank correlation. Note that the training set of STSb contains subsets of STS12-16, Thus, we do not include\nthe scores of SBERT-base-nli-stsb-v2 on these datasets for reducing misunderstanding.\n687\nMethod STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg.\nUnsupervised method based on BERT-base\nTSDAE 55.2 67.4 62.4 74.3 73.0 66.0 62.3 65.8\nCT 60.0 76.3 68.2 77.3 75.8 73.9 69.4 71.6\nSimCSE 63.6 79.3 69.6 78.2 77.7 73.8 70.1 73.2\nBERT-ﬂow 34.1 60.7 48.8 61.9 64.8 48.9 58.4 53.9\nMLM 30.9 59.9 47.7 60.3 63.7 47.3 58.2 52.6\nOut-of-the-box supervised pre-trained models\nSBERT-base-nli-v2 72.5 84.8 80.2 84.8 80.0 83.9 78.0 80.6\nSBERT-base-nli-stsb-v2– – – – – 87.3 80.4 –\nUSE-large 74.3 71.8 71.4 82.5 77.5 80.9 75.8 76.3\nTable 11: Evaluation on the task of STS using Spearman’s rank correlation.\nG Inﬂuence of Corpus Size\nThe inﬂuence of corpus size for AskUbuntu, CQADupStack and TwitterPara is shown in Figure 6.\n0 20 40 60\nNumber of training sentences (thousands)\n44\n46\n48\n50\n52\n54MAP%\n TSDAE\nMLM\nCT\nSimCSE\nBERT-flow\n(a) AskUbuntu\n0 20 40 60\nNumber of training sentences (thousands)\n4\n6\n8\n10\n12\n14MAP@100%\nTSDAE\nMLM\nCT\nSimCSE\nBERT-flow (b) CQADupStack\n0 10 20 30 40 50\nNumber of training sentences (thousands)\n40\n45\n50\n55\n60\n65\n70\n75AP%\n TSDAE\nMLM\nCT\nSimCSE\nBERT-flow\n(c) TwitterPara\n0 20 40 60\nNumber of training sentences (thousands)\n50\n55\n60\n65\n70MAP%\n TSDAE\nMLM\nCT\nSimCSE\nBERT-flow (d) SciDocs\nFigure 6: The inﬂuence of the number of training sentences on the model performance.\nH Inﬂuence of Different POS Tags\nThe inﬂuence of different POS tags on the output similarity scores for AskUbuntu, CQADupStack and\nTwitterPara is shown in Figure 7.\n688\nIN PRPSYM\nVB-AUXVB-NAUX\nDT CC NN JJ RB CD\nOTHER\nSBERT-sup.\nUSE-large\nSBERT-nli-stsb\nTSDAE\nMLM\nCT\nSimCSE\nBERT-flow\nPrior\n3.1 0.0 0.3 0.6 15.3 0.9 0.1 65.5 7.8 0.6 4.8 1.0\n0.0 0.1 0.7 0.3 8.9 0.1 0.3 68.810.1 0.7 7.1 3.0\n0.8 0.6 0.1 0.0 12.0 1.6 0.1 66.7 9.1 2.0 4.8 2.4\n3.7 0.2 1.3 0.9 15.9 1.1 0.6 57.5 8.6 1.1 7.2 1.9\n2.2 0.1 3.7 0.9 13.4 0.5 0.0 62.2 7.9 0.3 6.6 2.0\n0.7 0.3 2.0 0.4 15.6 0.3 0.1 69.3 7.1 0.8 2.5 1.0\n2.2 0.6 0.7 0.6 15.4 0.3 0.1 67.2 8.2 0.4 2.7 1.7\n2.1 0.8 4.5 1.7 13.0 2.0 0.4 62.6 5.8 0.3 3.7 3.1\n9.7 3.2 8.6 4.0 12.4 4.1 1.5 33.2 6.2 1.4 5.5 10.3\n0\n10\n20\n30\n40\n50\n60\nFrequency%\n(a) AskUbuntu\nIN PRPSYM\nVB-AUXVB-NAUX\nDT CC NN JJ RB CD\nOTHER\nSBERT-sup.\nUSE-large\nSBERT-nli-stsb\nTSDAE\nMLM\nCT\nSimCSE\nBERT-flow\nPrior\n1.5 0.7 1.7 1.5 9.9 0.5 0.7 73.1 6.5 0.9 0.6 2.4\n0.9 0.4 4.8 0.9 8.4 0.3 1.2 67.9 7.6 1.3 0.6 5.7\n0.9 1.0 2.7 1.2 10.6 0.5 0.3 68.3 8.0 1.7 1.2 3.7\n2.3 0.9 2.4 1.2 13.8 1.1 1.1 64.3 8.0 1.3 0.8 2.7\n3.0 0.6 16.0 1.2 11.3 0.5 0.7 58.2 3.9 0.8 0.7 3.3\n1.1 0.5 4.5 0.7 10.8 0.2 0.7 70.9 7.1 1.3 0.5 1.8\n1.8 0.9 1.9 1.1 12.8 0.9 0.7 67.0 7.8 1.2 0.9 3.1\n3.3 1.7 5.7 2.2 9.7 1.1 0.8 63.6 6.0 1.2 0.9 3.8\n10.5 3.4 12.1 5.7 10.6 5.8 2.1 30.8 6.0 1.6 1.2 10.1\n20\n40\n60\nFrequency% (b) CQADupStack\nIN PRPSYM\nVB-AUXVB-NAUX\nDT CC NN JJ RB CD\nOTHER\nSBERT-sup.\nUSE-large\nSBERT-nli-stsb\nTSDAE\nMLM\nCT\nSimCSE\nBERT-flow\nPrior\n1.2 0.4 0.6 0.4 17.4 0.4 0.2 65.1 7.2 0.4 6.6 0.4\n0.0 0.1 0.2 0.1 2.4 0.0 0.0 86.8 5.0 0.1 4.5 1.0\n0.8 0.1 0.0 0.1 12.5 0.2 0.1 69.2 6.1 1.5 8.5 1.2\n0.8 0.2 0.4 0.6 15.0 0.6 0.1 69.5 8.2 1.7 1.7 1.5\n3.2 0.5 1.2 1.2 16.2 1.2 0.2 70.1 3.6 0.7 1.1 0.9\n1.0 0.3 0.3 1.0 18.1 0.0 0.1 70.4 7.2 0.2 1.0 0.6\n1.8 1.4 0.9 2.4 21.0 0.7 0.1 62.1 4.6 0.3 1.9 3.1\n2.3 0.7 1.6 1.0 11.9 0.7 0.2 73.3 4.9 0.7 1.1 1.8\n10.7 2.9 7.7 4.0 12.3 5.6 1.2 39.6 5.9 1.9 2.2 6.0\n0\n20\n40\n60\n80\nFrequency%\n(c) TwitterPara\nIN PRPSYM\nVB-AUXVB-NAUX\nDT CC NN JJ RB CD\nOTHER\nSBERT-sup.\nUSE-large\nSBERT-nli-stsb\nTSDAE\nMLM\nCT\nSimCSE\nBERT-flow\nPrior\n1.5 0.0 0.4 0.0 5.4 0.1 0.2 79.712.2 0.2 0.1 0.2\n0.4 0.0 2.4 0.0 6.2 0.1 0.9 76.912.4 0.2 0.2 0.1\n0.9 0.0 0.3 0.0 6.4 0.0 0.1 78.812.6 0.3 0.4 0.1\n1.9 0.1 0.8 0.1 7.1 0.9 0.5 73.713.9 0.4 0.3 0.3\n4.8 0.0 2.0 0.1 6.2 0.4 0.7 74.910.4 0.2 0.2 0.3\n1.2 0.0 1.8 0.0 6.0 0.2 0.1 77.312.8 0.3 0.2 0.1\n2.2 0.0 1.7 0.1 6.8 1.3 0.2 75.411.4 0.3 0.2 0.3\n3.7 0.1 2.9 0.1 5.6 1.0 0.8 75.9 9.1 0.2 0.3 0.4\n11.9 0.2 7.9 0.3 7.2 4.6 2.9 51.811.0 0.4 0.8 1.0\n0\n20\n40\n60\nFrequency% (d) SciDocs\nFigure 7: The inﬂuence of different POS tags on the output similarity scores.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8203495740890503
    },
    {
      "name": "Transformer",
      "score": 0.6945736408233643
    },
    {
      "name": "Domain adaptation",
      "score": 0.6899316906929016
    },
    {
      "name": "Sentence",
      "score": 0.6733158826828003
    },
    {
      "name": "Embedding",
      "score": 0.6528946161270142
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6452091932296753
    },
    {
      "name": "Encoder",
      "score": 0.5499207973480225
    },
    {
      "name": "Unsupervised learning",
      "score": 0.5185638070106506
    },
    {
      "name": "Machine learning",
      "score": 0.45144742727279663
    },
    {
      "name": "Natural language processing",
      "score": 0.4466094970703125
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4431379437446594
    },
    {
      "name": "Noise reduction",
      "score": 0.4166823625564575
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31512782",
      "name": "Technical University of Darmstadt",
      "country": "DE"
    }
  ]
}