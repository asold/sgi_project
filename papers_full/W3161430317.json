{
  "title": "Transformers for Clinical Coding in Spanish",
  "url": "https://openalex.org/W3161430317",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2125541959",
      "name": "Guillermo López-García",
      "affiliations": [
        "Universidad de Málaga"
      ]
    },
    {
      "id": "https://openalex.org/A2105206877",
      "name": "José M. Jerez",
      "affiliations": [
        "Universidad de Málaga"
      ]
    },
    {
      "id": "https://openalex.org/A785138820",
      "name": "Nuria Ribelles",
      "affiliations": [
        "Instituto de Investigación Biomédica de Málaga"
      ]
    },
    {
      "id": "https://openalex.org/A2131864759",
      "name": "Emilio Alba",
      "affiliations": [
        "Instituto de Investigación Biomédica de Málaga"
      ]
    },
    {
      "id": "https://openalex.org/A2203072278",
      "name": "Francisco J. Veredas",
      "affiliations": [
        "Universidad de Málaga"
      ]
    },
    {
      "id": "https://openalex.org/A2125541959",
      "name": "Guillermo López-García",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105206877",
      "name": "José M. Jerez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A785138820",
      "name": "Nuria Ribelles",
      "affiliations": [
        "Maine Library Association"
      ]
    },
    {
      "id": "https://openalex.org/A2131864759",
      "name": "Emilio Alba",
      "affiliations": [
        "Maine Library Association"
      ]
    },
    {
      "id": "https://openalex.org/A2203072278",
      "name": "Francisco J. Veredas",
      "affiliations": [
        "Maine Library Association"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3097007564",
    "https://openalex.org/W2943001064",
    "https://openalex.org/W2948230824",
    "https://openalex.org/W6759455113",
    "https://openalex.org/W7051469422",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W3116121758",
    "https://openalex.org/W2889764698",
    "https://openalex.org/W6767164110",
    "https://openalex.org/W2902516827",
    "https://openalex.org/W3046500025",
    "https://openalex.org/W3096590546",
    "https://openalex.org/W3097484695",
    "https://openalex.org/W3095525213",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W6769263558",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6703082266",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W2948042116",
    "https://openalex.org/W1870686808",
    "https://openalex.org/W3013462789",
    "https://openalex.org/W1541322638",
    "https://openalex.org/W6736878346",
    "https://openalex.org/W2096664202",
    "https://openalex.org/W2100676408",
    "https://openalex.org/W2886093373",
    "https://openalex.org/W2805432200",
    "https://openalex.org/W2964295521",
    "https://openalex.org/W3028925963",
    "https://openalex.org/W2883849462",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2955483668",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W3095092693",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2336734465",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2968917279",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963126915",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2994689640",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Automatic clinical coding is an essential task in the process of extracting relevant information from unstructured documents contained in electronic health records (EHRs). However, most research in the development of computer-based methods for clinical coding focuses on texts written in English due to the limited availability of medical linguistic resources in languages other than English. With nearly 500 million native speakers, there is a worldwide interest in processing healthcare texts in Spanish. In this study, we systematically analyzed transformer-based models for automatic clinical coding in Spanish. Using a transfer-learning-based approach, the three existing transformer architectures that support the Spanish language, namely, multilingual BERT (mBERT), BETO and XLM-RoBERTa (XLM-R), were first pretrained on a corpus of real-world oncology clinical cases with the goal of adapting transformers to the particularities of Spanish medical texts. The resulting models were fine-tuned on three distinct clinical coding tasks, following a multilabel sentence classification strategy. For each analyzed transformer, the domain-specific version outperformed the original general domain model across those tasks. Moreover, the combination of the developed strategy with an ensemble approach leveraging the predictive capacities of the three distinct transformers yielded the best obtained results, with MAP scores of 0.662, 0.544 and 0.884 on CodiEsp-D, CodiEsp-P and Cantemist-Coding shared tasks, which remarkably improved the previous state-of-the-art performance by 11.6%, 10.3% and 4.4%, respectively. We publicly release the mBERT, BETO and XLMR transformers adapted to the Spanish clinical domain at https://github.com/guilopgar/ClinicalCodingTransformerES, providing the clinical natural language processing community with advanced deep learning methods for performing medical coding and other tasks in the Spanish clinical domain.",
  "full_text": "Received April 28, 2021, accepted May 6, 2021, date of publication May 13, 2021, date of current version May 21, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3080085\nTransformers for Clinical Coding in Spanish\nGUILLERMO LÓPEZ-GARCÍA\n 1, JOSÉ M. JEREZ1, NURIA RIBELLES2,\nEMILIO ALBA2, AND FRANCISCO J. VEREDAS\n1\n1Departamento de Lenguajes y Ciencias de la Computación, Universidad de Málaga, 29071 Málaga, Spain\n2Unidad de Gestión Clínica Intercentros de Oncología, Instituto de Investigación Biomédica de Málaga (IBIMA), Hospitales Universitarios Regional y\nVirgen de la Victoria, 29010 Málaga, Spain\nCorresponding author: Guillermo López-García (guilopgar@uma.es)\nThis work was supported in part by the Ministerio de Economía y Empresa (MINECO), Plan Nacional de I+D+I, under Project\nTIN2017-88728-C2-1-R, in part by the Andalucía TECH, under Project UMA-CEIATECH-01, in part by the Universidad de Málaga\nand the Consorcio de Bibliotecas Universitarias de Andalucía (CBUA), and in part by the Plan Andaluz de Investigación, Desarrollo e\nInnovación (PAIDI), Junta de Andalucía.\nABSTRACT Automatic clinical coding is an essential task in the process of extracting relevant information\nfrom unstructured documents contained in electronic health records (EHRs). However, most research in the\ndevelopment of computer-based methods for clinical coding focuses on texts written in English due to the\nlimited availability of medical linguistic resources in languages other than English. With nearly 500 million\nnative speakers, there is a worldwide interest in processing healthcare texts in Spanish. In this study, we sys-\ntematically analyzed transformer-based models for automatic clinical coding in Spanish. Using a transfer-\nlearning-based approach, the three existing transformer architectures that support the Spanish language,\nnamely, multilingual BERT (mBERT), BETO and XLM-RoBERTa (XLM-R), were ﬁrst pretrained on a\ncorpus of real-world oncology clinical cases with the goal of adapting transformers to the particularities of\nSpanish medical texts. The resulting models were ﬁne-tuned on three distinct clinical coding tasks, following\na multilabel sentence classiﬁcation strategy. For each analyzed transformer, the domain-speciﬁc version out-\nperformed the original general domain model across those tasks. Moreover, the combination of the developed\nstrategy with an ensemble approach leveraging the predictive capacities of the three distinct transformers\nyielded the best obtained results, with MAP scores of 0.662, 0.544 and 0.884 on CodiEsp-D, CodiEsp-P\nand Cantemist-Coding shared tasks, which remarkably improved the previous state-of-the-art performance\nby 11.6%, 10.3% and 4.4%, respectively. We publicly release the mBERT, BETO and XLMR transform-\ners adapted to the Spanish clinical domain at https://github.com/guilopgar/ClinicalCodingTransformerES,\nproviding the clinical natural language processing community with advanced deep learning methods for\nperforming medical coding and other tasks in the Spanish clinical domain.\nINDEX TERMS Clinical coding, deep learning, natural language processing, text classiﬁcation,\ntransformers.\nI. INTRODUCTION\nThe incremental adoption of electronic health records\n(EHRs), as a main component of the information systems\nof hospitals and healthcare services in general, has raised a\nseries of questions for the scientiﬁc community that remain\nopen. One of the main problems derived from the incremen-\ntal use of EHRs is the need to extract the most signiﬁcant\ninformation stored in the system, the effective management\nof which has direct implications in improving patient health\ncare. EHRs contain data whose volume is constantly growing\nand whose nature is heterogeneous and includes personal\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Yiming Tang\n.\ninformation of patients, standardized health codes, medical\nimages, and genetic data stored in a wide variety of formats.\nEHRs also include free-text documents that use domain-\nspeciﬁc natural language (with specialized vocabulary and\nterminology) and store information about clinical notes, dis-\ncharge reports, radiological or pathology reports, anamnesis\nand notes of clinical examinations, medical orders, etc. [1]\nHowever, the unstructured nature of the text these documents\ncontain makes it especially complex to directly extract rele-\nvant information on medical concepts from these documents\nthat can be used systematically.\nStructured clinical information, in the form of clinical data\nencoded according to controlled and standardized vocabu-\nlaries, is a fundamental resource that allows not only the\nVOLUME 9, 2021 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 72387\nG. López-Garcíaet al.: Transformers for Clinical Coding in Spanish\nFIGURE 1. Example of CIE-10-ES hierarchical structure for the ‘‘other and unspecified malignant neoplasm of skin’’\ncategory (C44). (We preserved the descriptions in Spanish as in the original CIE-10-ES edition of the Spanish\nMinisterio de Sanidad.)\nadministrative management of data or the efﬁcient exchange\nof information between institutions but also the development\nof statistical analyses [2]—of retrospective or prospective\nnature—or epidemiological studies, which can be carried\nout based on data from cohorts or patient populations.\nThese analyses can subsequently serve as support in clin-\nical decision-making and in optimizing the management\nof economic-ﬁnancial resources of healthcare services [3].\nAutomatic clinical coding consists of transforming unstruc-\ntured clinical texts written in specialized natural language\nto structured formats that conform to standardized coding\nterminologies using computational methods [2]. Given the\nimportance of storing descriptions written in natural language\nin the different documents involved in medical and health\ncare processes in EHRs, automatic clinical coding is an essen-\ntial task in the process of extracting meaningful information\nfrom patient data, which affects the improvement of many\nclinical and productivity aspects of the health professionals\ninvolved—freeing professional coders from the arduous task\nof manual coding of large amounts of text generated in daily\nclinical practice [4]—as well as the optimization of diagnoses\nand procedures.\nTraditionally, natural language processing (NLP) strategies\nhave been applied to the problem of automatic clinical cod-\ning [2], [5]–[7], although more recent studies focus on the use\nof rule-based approaches, machine learning (ML) strategies,\nand deep learning (DL) models [8]–[12]. However, most of\nthe previous works in the literature focus on texts written in\nEnglish due to the limited availability of annotated corpora\nwith standardized clinical coding labels and additional lin-\nguistic resources in languages other than English.\nThe main goal of this study is to develop clinical coding\nmodels for Spanish medical documents by adapting several\ntransformer-based models to the particularities of the Spanish\nhealthcare domain. For this purpose, we pretrained the mod-\nels by using a private corpus of deidentiﬁed real-world oncol-\nogy clinical texts written in Spanish. The resulting models\nwere further ﬁne-tuned on 3 clinical coding tasks, namely,\nCodiEsp-D, CodiEsp-P and Cantemist-Coding, by using two\npublic annotated clinical Spanish corpora [13]–[16]. In this\nwork, we explored 3 transformers that support the Spanish\nlanguage: the multilingual BERT (mBERT) [17], BETO [18]\nand XLM-RoBERTa (XLM-R) [19]. With the aim of adapting\ntransformers to the particularities exhibited by real-world\nsmall-data clinical coding tasks, in this study, we developed\na multilabel sentence classiﬁcation approach that serves as a\ndata augmentation procedure. Following the proposed strat-\negy, the transformers achieved new state-of-the-art (SOTA)\nperformance in each of the clinical coding tasks explored\nin this work. The 3 transformer-based models adapted\nto the Spanish clinical domain are publicly available at\nhttps://github.com/guilopgar/ClinicalCodingTransformerES.\nII. BACKGROUND\nA. ICD-10 CODING\nThe ICD 1 classiﬁcation system (International Statistical\nClassiﬁcation of Diseases and Related Health Problems)\nestablishes a standardized coding that allows the statistical\nanalysis of mortality and morbidity of patients belonging\nto healthcare services. The ICD-10 edition—whose corre-\nsponding Spanish version is called CIE-10-ES 2 (see Fig. 1)—\nis structured hierarchically into chapters that group codes\nof up to seven characters, thus allowing it to encode over\n70,000 diagnoses and 72,000 procedures. Hence, the length\nof the ICD-10 codes ranges from a minimum of 3 characters\nto a maximum of 7 characters, depending on the degree of\nspeciﬁcity needed for the concept to be encoded (see Fig. 1).\nCoding with ICD-10 in general, as well as with CIE-10-ES\nin particular, presents great difﬁculties, mainly due to the\nenormous sparsity that the distribution of codes presents:\ncertain diseases and procedures are much less frequent than\n1https://www.who.int/classiﬁcations/classiﬁcation-of-diseases\n2https://eciemaps.mscbs.gob.es/ecieMaps/\n72388 VOLUME 9, 2021\nG. López-Garcíaet al.: Transformers for Clinical Coding in Spanish\nothers, which results in corpora annotated with thousands of\nrare codes and a few tens or hundreds of frequent codes. As a\nconsequence, the datasets tend to be very unbalanced, and in\naddition, they tend to present important biases derived from\nthe local factors characteristic of each health service or insti-\ntution. All of this makes it difﬁcult to develop automatic\ncoding algorithms that obtain acceptable levels of efﬁciency.\nFor the particular case of automatic coding of clinical texts\nin Spanish using CIE-10-ES codes, few works have been\npublished. Thus, Blanco et al. [20] explored different DL\nmodels using a multilabel classiﬁcation approach to address\nthe problem of clinical coding on a corpus obtained from\nthe public hospital system of the Basque Country in Spain.\nPérez et al. [21] used an approach based on latent Dirich-\nlet allocation (LDA) to carry out a multilabel classiﬁcation of\nEHRs obtained from the cardiology department of the same\npublic system of hospitals in the Basque Country, obtaining\npositive results with the 124 most frequent CIE-10-ES codes\npresent in the corpus. More recently, Almagro et al.[22] com-\npared algorithms based on binary outputs, groups of subsets\nand extreme classiﬁcation (eXtreme Multilabel Text Classiﬁ-\ncation, or XMTC) to assign CIE-10-ES codes to clinical texts\nfrom hospital discharge reports of the Hospital Universitario\nFundación Alcorcón in Spain, concluding that assembling\nmethods based on weighting each code according to training\nfrequency and performance can achieve better performance\nscores on extreme distributions, such as CIE-10-ES coding.\nB. CLINICAL CODING SHARED TASKS\nIn the last ﬁve years, the CLEF eHealth Lab 3 has organized a\nseries of shared clinical coding tasks on multilingual or non-\nEnglish corpora. In 2020, Task 1 of the CLEF eHealth was\nmatched with the CodiEsp track [16], which stands out for\nbeing the ﬁrst shared task consisting of the automatic coding\nof clinical cases in Spanish. With this aim, the CodiEsp cor-\npus was provided, a synthetic corpus consisting of 1,000 sam-\nples of clinical cases, manually curated by the organizers\nof the task. Within the CodiEsp track, three different shared\nsubtasks were proposed, all of which focused on the CodiEsp\ncorpus: CodiEsp Diagnosis (CodiEsp-D), CodiEsp Procedure\n(CodiEsp-P) and Explainable AI (CodiEsp-X). Thus, given\na clinical free text written in specialized natural Spanish\nlanguage, the tasks CodiEsp-D and CodiEsp-P consisted of\nassigning to the text a list of CIE-10-ES diagnostic and\nprocedural codes, respectively. In contrast, the CodiEsp-X\nsubtask required the prediction of both types of CIE-10-ES\ncodes together with the exact reference to the text segment\nthat served as justiﬁcation for the assignment of such codes—\nthat is, this last subtask became a named-entity-recognition\nand normalization (NER-N) task. Among the methodologies\nused by the participants to solve these subtasks, the best result\nfor CodiEsp-D (with a mean average precision (MAP) value\nof 0.593) was obtained using an ML algorithm, while the\nbest MAP score for CodiEsp-P (0.493) was achieved using\n3https://clefehealth.imag.fr/\nsystems that did not use ML. Finally, it should be noted that\nmost of the models presented to the CodiEsp-X subtask used\nNLP algorithms from outside the scope of ML [16]. As a\nconsequence of the line of work starting from the evaluation\nshared tasks of the CodiEsp track, some studies have emerged\nin recent months that make use of the CodiEsp corpus or of\na speciﬁc clinical corpus in Spanish to address the problem\nof automatic clinical coding, using various methodologies,\namong which, the DL techniques stand out [12].\nIn the same line of work of automatic coding of clinical\ntexts in Spanish, last year, the Cantemist Track for Cancer\nText Mining evaluation campaign was carried out [13], which\nproposed a series of shared tasks whose objective was the\ndevelopment of systems for assigning ICD-O codes (Inter-\nnational Classiﬁcation of Diseases for Oncology) to clinical\noncology texts in Spanish. More speciﬁcally, it was about\nassigning CIE-O-3.1 codes (which is the Spanish equivalent\nof ICD-O, version 3.1, for Neoplasia Morphology) to the texts\nincluded in a gold standard corpus with mentions of tumor\nmorphologies. Among the methodologies used in the Can-\ntemist shared tasks, the best results (with a maximum MAP\nof 0.847) were obtained using language models based on a\ntransformer [23] and bidirectional recurrent neural networks\nwith LSTM units [13]–[15].\nC. ATTENTION MODELS FOR NLP DOWNSTREAM TASKS\nIn recent years, a new family of models has emerged capa-\nble of associating each word with a contextual numerical\nrepresentation considering the speciﬁc context in which the\nword appears within the text. These types of models are\nknown as contextual embeddings. Some of them are based\non semisupervised sequence learning, as is the case with\nELMo [24], ULMFit [25], transformer [23], BERT [17] and,\nmore recently, RoBERTa [26], T5 [27], XLM-R [19] and\nXLNet [28]. While those based on recurrent neural networks,\nsuch as ELMo or UMLFit, present efﬁciency problems due\nto the sequential nature of these networks, the models based\non a transformer architecture, such as BERT, RoBERTa,\nT5, XLM-R and XLNet, focus on the attention mechanisms\nproposed to, among other advantages, increase computing\nefﬁciency through the parallelization of a large part of their\nnetwork architecture. Furthermore, another particularity of\nthese attention models is that they can be pretrained on a\ngeneral domain corpus and later ﬁne-tuned and adapted on\na domain-speciﬁc corpus to solve a particular NLP task. This\ntechnique, known as transfer learning (TL), is commonly\nused to ﬁt DL algorithms to small datasets.\nRecently, we advanced our research line consisting of\nthe application of TL approaches for problem solving in the\nbiomedical ﬁeld [29]–[31]. In this study, we continued the\nsame line of work by applying a TL-based strategy to address\nthe automatic clinical coding problem in Spanish using trans-\nformers. There exist preliminary works in which BERT-based\nmodels have been applied with moderate results to single\nclinical coding tasks in Spanish [14], [15], [32]. However,\nthere is a lack of studies that systematically analyze the\nVOLUME 9, 2021 72389\nG. López-Garcíaet al.: Transformers for Clinical Coding in Spanish\nTABLE 1. Description of the number of clinical codes annotations used in each of the 3 clinical coding tasks. Only the CIE-10-ES diagnosis annotations\nwere considered in the CodiEsp-D task, while the CIE-10-ES procedures annotations were only used in the CodiEsp-P task.\nFIGURE 2. Illustration of the two existing annotation formats using theS1130-14732005000300004-1\nclinical document from the CodiEsp-D development corpus.A Format of the code annotations available for\nthe clinical coding tasks.B Format of the code annotations available for the NER-N tasks.\nperformance of transformers in the Spanish medical domain,\nparticularly for distinct clinical coding problems. Addition-\nally, there is also a lack of publicly available transformer-\nbased models pretrained on clinical corpora in Spanish.\nGiven the signiﬁcant interest that exists both in industry\nand academia to develop automatic systems for extracting\nrelevant clinical information contained in Spanish medical\ndocuments, the availability of transformers adapted to the par-\nticularities of the real-world Spanish clinical domain would\nfacilitate the adoption of these models in downstream medical\nNLP tasks. In this way, the two main contributions of this\nwork are: on the one hand, a systematic analysis of the\nperformance of transformers on three distinct clinical coding\ntasks in Spanish. On the other hand, the release of the ﬁrst\npublicly available transformer-based models adapted to the\nSpanish clinical ﬁeld, providing the medical NLP community\nwith cutting-edge DL methods for performing clinical coding\nand other downstream tasks in the Spanish medical domain.\nIII. MATERIALS AND METHODS\nA. CORPORA\n1) DOMAIN-SPECIFIC PRETRAINING\nWith the aim of adapting the transformer-based models to\nthe particularities of the Spanish clinical domain, we fur-\nther pretrained them using a private corpus of deidentiﬁed\nclinical cases retrieved from the Galén Oncology Information\nSystem [33], [34]. The corpus corresponds to a collection\nof real-world oncology clinical texts written in Spanish by\nphysicians from the oncology departments of the Hospital\nRegional Universitarioand the Hospital Universitario Virgen\nde la Victoriain Málaga, Spain. In total, the corpus comprises\n30.9 K documents, 64.4 M words and 437.6 M characters.\n2) CLINICAL CODING FINE-TUNING\nWe used 2 publicly annotated clinical corpora in Spanish\nto ﬁne-tune the models on 3 clinical coding tasks. Both the\nCodiEsp-D and CodiEsp-P tasks are based on the CodiEsp\ncorpus [16], a collection of 1 K clinical cases annotated with\nboth CIE-10-ES diagnosis and CIE-10-ES procedures codes.\nThe Cantemist corpus [13] comprises 1.3 K clinical cases\nfrom the oncology domain, annotated with CIE-O-3 codes for\nthe Cantemist-Coding task. Each of the two clinical corpora\nwas split into training, development and test subsets. Table 1\nsummarizes the annotation distribution of the two analyzed\ncorpora.\nFor each of the 3 clinical coding tasks addressed in this\nwork, the available annotations contained the assignment of\na set of clinical codes to each document in the corpus (see\nFig. 2A). Additionally, the organizers of both CodiEsp and\nCantemist tracks also proposed two NER-N tasks, namely,\n72390 VOLUME 9, 2021\nG. López-Garcíaet al.: Transformers for Clinical Coding in Spanish\nFIGURE 3. Workflow of the TL approach for automatic clinical coding using transformers.\nCodiEsp-X—which was based on the CodiEsp corpus and\nconsidered the same codes annotations used both in the\nCodiEsp-D and the CodiEsp-P tasks—and Catemist-Norm—\nwhich considered the same documents and codes annotations\nused in the Cantemist-Coding task. In contrast with the anno-\ntation format of the clinical coding tasks, the code annotations\nof the NER-N tasks contained an additional ﬁeld indicating\nthe mention in the text that supported the coding assignment\n(see Fig. 2B).\nB. TRANSFORMER-BASED MODELS\nIn this work, we systematically analyzed the performance of\ndifferent transformer-based models on clinical coding tasks in\nSpanish. For this reason, we explored 3 transformers that sup-\nport the Spanish language, namely, mBERT [17], BETO [18]\nand XLM-R [19]. To the best of our knowledge, these are\nthe only publicly available transformer-based models that\ninclude Spanish among their supported languages.\n• mBERT: the multilingual version of the BERT-Base\nmodel [17] was pretrained on a corpus comprising\nWikipedia texts from 104 languages. 4 The model uses\na multilingual WordPiece [35] vocabulary of ∼110 K\nsubtokens, and the total number of trainable parameters\nis ∼177 M.\n• BETO: The Spanish-BERT model, called BETO, uses\na similar architecture to the BERT-Base model, with a\ntotal of ∼110 M trainable parameters [18]. The pretrain-\ning corpus exclusively contains texts in Spanish, includ-\ning data from Wikipedia and the OPUS Project [36]. The\nmodel uses a Spanish vocabulary of ∼31 K subwords.\n• XLM-R: the multilingual version of the RoBERTa-Base\nmodel [26] was pretrained following a modiﬁed ver-\nsion of the XLM approach [37], using a CommonCrawl\nCorpus in 100 languages [19]. The model uses a large\nmultilingual SentencePiece [38] vocabulary of ∼250 K\nsubtokens, and the total number of trainable parameters\nis ∼278 M.\n4https://github.com/google-research/bert/blob/master/multilingual.md\nC. TRANSFER LEARNING APPROACH FOR\nAUTOMATIC CLINICAL CODING\nThe workﬂow of our TL approach is shown in Fig. 3, and each\nstage of the pipeline is described in the next subsections.\n1) UNSUPERVISED PRETRAINING\nThe transformers analyzed in this study were further pre-\ntrained on a corpus of unlabeled oncology clinical texts.\nSpeciﬁcally, following the two pretraining objectives pro-\nposed by Devlin et al. [17], the two BERT-based models\nexplored in this work, namely, mBERT and BETO, were\noptimized based on the next sentence prediction (NSP) task\nand the masked language model (MLM) pretraining objective\nwith the whole-word masking (WWM) modiﬁcation. Follow-\ning the approach developed by Conneau et al.[19], we used\nthe MLM objective with the dynamic masking modiﬁcation\nto perform the pretraining of the XLM-R model.\n2) SUPERVISED FINE-TUNING\nIn this work, we addressed the clinical coding problem in\nSpanish using transformers. Generally, clinical coding is\naddressed as a multilabel text classiﬁcation task, where for\na collection of documents, a set of standard medical codes\nmust be assigned to each text. However, when applying\ntransformer-based models to address real-world clinical cod-\ning tasks, two main issues arise. First, transformer architec-\ntures can only deal with ﬁxed-length input sequences due to\nthe quadratic time complexity on the input sequence length\nfor the self-attention layers [23]. For instance, the trans-\nformers analyzed in this work were designed to process\nsubword input sequences up to a maximum length of 512.\nThis represents a signiﬁcant constraint when addressing long-\ntext classiﬁcation problems such as the clinical coding tasks\nfaced in this study since numerous clinical cases from the\nCodiEsp and Cantemist corpora have a subtoken sequence\nlength above the maximum size supported by transformers.\nSecond, DL architectures have been shown to be pri-\nmarily effective in contexts where a large set of samples\nis used to train the models, given their high number of\nVOLUME 9, 2021 72391\nG. López-Garcíaet al.: Transformers for Clinical Coding in Spanish\ntrainable parameters. In small-data text classiﬁcation scenar-\nios, such as the clinical coding tasks faced in this work, only\na few hundred (200-1 K) annotated texts are available to train\nthe classiﬁcation systems. Consequently, the transformers\nexplored in this study—with several hundreds of millions of\nweights—would be prone to suffer from critical overﬁtting\nissues if they were ﬁtted on text classiﬁcation tasks using such\na scarce set of samples as the set of documents contained in\nthe CodiEsp and Cantemist corpora.\nWith the aim of adapting transformers to the particularities\nexhibited by real-world clinical coding tasks, based on our\nprevious works [14], [32], we developed a 3-phase multilabel\nsentence classiﬁcation approach that not only addresses the\nﬁxed-length input sequence limitation presented by trans-\nformers but also serves as a data augmentation procedure.\nIn this way, leveraging the information available for the\nNER-N tasks (see Section III-A2), we applied our 3-phase\nstrategy to each clinical coding task—CodiEsp-D, CodiEsp-P\nand Cantemist-Coding—to convert the multilabel long-text\nclassiﬁcation problem into a multilabel sentence classiﬁca-\ntion task. The next paragraphs describe our sentence classiﬁ-\ncation approach for clinical coding.\nPhase 1: Creation of a corpus of annotated sentences.\nFor each document in the clinical coding corpus, the text\nwas ﬁrst split into sentences using the SPACCC Sentence\nSplitter tool. 5 Then, exploiting the additional information\navailable for the corresponding NER-N task—CodiEsp-X in\nthe case of CodiEsp-D and CodiEsp-P tasks and Cantemist-\nNorm in the case of Cantemist-Coding task—, each sentence\nwas exclusively annotated with the clinical codes whose text\nreferences were contained within the limits of the sentence.\nAs an example, Fig. 4 shows the annotated sentences obtained\nfrom the text of the S1130-14732005000300004-1 clinical\ncase (see also Fig. 2A), using the diagnosis code annotations\nfrom the CodiEsp-X task available for the document (see also\nFig. 2B). In this way, a corpus of sentences annotated with\nclinical codes was created. The generated corpus was signif-\nicantly larger than the original clinical coding corpus, given\nthat, instead of considering the whole text of a document as\na single training sample, each sentence obtained from the\ndocument was treated as an individual training instance.\nPhase 2: Fine-tuning on a multilabel sentence clas-\nsiﬁcation task. Using the corpus of labeled sentences,\nwe ﬁne-tuned each transformer model on a multilabel sen-\ntence classiﬁcation problem. To perform the supervised ﬁne-\ntuning of the whole model architecture on a multilabel\nsentence-level task, the output representation encoded by the\nmodel for the initial beginning of sequence(BOS) subtoken\nwas fed into a ﬁnal classiﬁcation layer with C sigmoid units,\nwhere C denotes the number of distinct codes present in\nthe ﬁne-tuning corpus. Thus, given a sentence as input to\nthe model, the generated output vector of length C could be\ninterpreted as the probability of each of the C codes occurring\nwithin the input sentence.\n5https://zenodo.org/record/2586995#.YDatgnWYVhE\nFIGURE 4. Illustration of the sentences annotated with CIE-10-ES\ndiagnosis codes obtained from theS1130-14732005000300004-1\ndocument belonging to the CodiEsp-D development set. The WordPiece\ntokenizer of the BETO model was used to generate the subtoken\nsequence of each sentence split from the text.\nPhase 3: Predicting code probabilities at the document\nlevel. Although each model was ﬁne-tuned on a supervised\nsentence-level task, the predictive performance of the models\nwas evaluated at the document level. Consequently, using a\nmaximum probability criterion, we postprocessed the prob-\nabilities predicted by the models at the sentence level to\nproduce a vector of code probabilities at the document level.\nHence, given a set S containing all sentences obtained from\na single document d as input, the model outputted a |S|×C\nprobability matrix P. Subsequently, the criterion consisted of\nselecting the maximum probability value across each column\nof P, obtaining a ﬁnal vector p of length C, which represented\nthe probability of each code to occur in d.\nAs a result, at prediction time, given a set of D documents\nas input data, our workﬂow for clinical coding using trans-\nformers produced a D ×C matrix of coding probabilities\npredicted by the model at the document level (see Fig. 3).\nD. EXPERIMENTS\nWe implemented our TL approach for clinical coding using\nboth the transformers library developed by the HuggingFace\nteam [39] and the TensorFlow implementation of BERT\ndeveloped by Google. 6 For all transformer-based models\nexamined in this work, we set a maximum input sequence\nlength of 128 subwords. We ﬁxed the same values for most\nof the hyperparameters during the pretraining of the models\n(see Supplementary Table S1 for further details). When ﬁne-\ntuning the models to perform clinical coding, we used the\nRAdam [40] optimizer with a learning rate of 3 ×10−5,\na batch size of 16 and the number of epochs was empiri-\ncally determined on the development set of the corresponding\nclinical coding corpus, with an upper limit of 50 epochs.\n6https://github.com/google-research/bert\n72392 VOLUME 9, 2021\nG. López-Garcíaet al.: Transformers for Clinical Coding in Spanish\nTABLE 2. Models performances on the CodiEsp-D, CodiEsp-P and Cantemist-Coding test sets. The distribution of the MAP values obtained by the\n5 distinct fine-tuned instances of each model is described, by reporting the mean, standard deviation and maximum values. For the maximum values\ncolumn of each task, the best obtained result is bolded, while the second best is underlined.\nAdditionally, when ﬁne-tuning the models on the CodiEsp-\nD task, we enriched the CodiEsp corpus with a set of Spanish\nabstracts annotated with CIE-10-ES diagnosis codes, pro-\nvided by the organizers of the CodiEsp track [16]. Regard-\ning the hardware resources employed, all experiments were\nconducted using a single GeForce GTX 1080 Ti GPU.\nIV. RESULTS\nTable 2 shows the predictive performance of the\n3 transformer-based models on the CodiEsp-D, CodiEsp-P\nand Cantemist-Coding tasks. For each model, we compared\nthe original model pretrained using general domain cor-\npora (see Section III-B) with the Spanish clinical version\nof the model obtained by further pretraining the general-\ndomain version using the oncology clinical cases from the\nGalén corpus (see Section III-C1). For each transformer,\nwe ﬁne-tuned 5 differently randomly initialized instances.\nThe MAP metric—the ofﬁcial evaluation metric of the tasks\n[13], [16]—was employed to evaluate the performance of the\nmodels. With the aim of maximizing the score obtained for\nthe MAP rank-based metric, for each document in the test\nset, we returned all codes considered by the model, sorted\nin descending order according to their predicted probability\nof occurrence. Among all models, BETO-Galén achieved\nthe best performance on the 3 clinical coding tasks, with\naverage MAP values of 0.616, 0.514 and 0.862. The two\nmultilingual transformers adapted to the clinical domain,\nnamely, mBERT-Galén and XLM-R-Galén, achieved almost\nthe same performance across the 3 tasks, with the mBERT-\nGalén model obtaining mean MAP scores of 0.609, 0.495\nand 0.858 and the XLM-R-Galén model achieving aver-\nage MAP values of 0.611, 0.493 and 0.859, respectively.\nCompared with the general-domain transformers, the clinical\nversion of the models improved the performance for clini-\ncal coding in Spanish. In this way, across the 3 tasks, for\neach transformer, the clinical version surpassed the general-\ndomain version of the model in terms of the average MAP\nscores. When comparing the maximum MAP score results\nobtained in this study with the previously reported SOTA\nresults, new SOTA performance was achieved in each of\nthe 3 clinical coding tasks. Thus, for the CodiEsp-D task,\nXLM-R (0.601), mBERT (0.602), XLM-R-Galén (0.615),\nmBERT-Galén (0.616) and BETO-Galén (0.619) exceeded\nthe prior SOTA performance (0.593) reported by the orga-\nnizers of the shared task [16]. In the case of the CodiEsp-\nP task, XLM-R-Galén (0.498), mBERT-Galén (0.508) and\nBETO-Galén (0.52) surpassed the previous SOTA results\n(0.493) [16]. Finally, for the Cantemist-Coding task, again,\nthe transformers adapted to the Spanish clinical domain,\nnamely, mBERT-Galén (0.86), XLM-R-Galén (0.861) and\nBETO-Galén (0.864), outperformed the prior SOTA results\n(0.847) [13].\nA. ENSEMBLE\nAdditionally, we proposed an ensemble approach to com-\nbine the different clinical coding predictions made by\nthe models. Thus, given a set comprising D documents, our\nproposed workﬂow for clinical coding using a transformer-\nbased model A outputted a D ×C probability matrix MA\n(see Fig. 3), with C representing the number of codes con-\nsidered by the model. In fact, as a result of ﬁne-tuning\n5 distinct instances of each model, 5 different probabil-\nity matrices, namely, M1\nA, M2\nA, . . . ,M5\nA, were obtained for\nmodel A. To combine the 5 distinct matrices into a single\nprobability matrix ME\nA , our ensemble approach plainly con-\nsisted of summing the 5 probability matrices obtained for\nmodel A, i.e., ME\nA = ∑5\ni=1 Mi\nA. Moreover, our ensemble\nstrategy could also be applied to combine the coding pre-\ndictions made by any number of distinct models by directly\nsumming all probability matrices obtained from the models.\nFor instance, given ME\nA and ME\nB as the ensemble probabil-\nity matrices of transformers A and B, respectively, a single\nprobability matrix ME\nA+B could be obtained by summing both\nprevious matrices, i.e., ME\nA+B =ME\nA +ME\nB .\nTable 3 describes the performance of our ensemble\napproach applied to combine both the coding predictions\nmade by single models and the predictions made by multiple\ndifferent models. Regarding the performance of the ensemble\napproach applied to single models, the BETO-Galén ensem-\nble obtained the best results on the 3 clinical coding tasks,\nwith MAP values of 0.648, 0.537 and 0.88, respectively.\nIn relation to the ensemble approach applied to multiple\nVOLUME 9, 2021 72393\nG. López-Garcíaet al.: Transformers for Clinical Coding in Spanish\nTABLE 3. Ensemble models performances on the CodiEsp-D, CodiEsp-P\nand Cantemist-Coding test sets, according to the MAP metric. For each\ntask, the best obtained result is bolded, while the second best is\nunderlined.\nmodels, the ensemble combining the predictions of the\n3 transformers adapted to the Spanish clinical domain,\nnamely, mBERT-Galén, BETO-Galén and XLM-R-Galén,\nachieved the best performance among all models analyzed\nin this study, with MAP values of 0.662, 0.544 and 0.884\non the CodiEsp-D, CodiEsp-P and Cantemist-Coding tasks,\nrespectively. In fact, according to the MAP scores, the results\nobtained by mBERT-Galén+BETO-Galén +XLM-R-Galén\nensemble remarkably surpassed the prior SOTA performance\nby 11.6% on the CodiEsp-D task, 10.3% on the CodiEsp-P\ntask, and 4.4% on the Cantemist-Coding task.\nB. ADDITIONAL METRICS\nFinally, although MAP was the ofﬁcial evaluation metric of\nthe tasks, the organizers also evaluated the participating sys-\ntems using additional metrics [13], [16], namely, the microav-\neraged precision, recall and F1-score. For completeness,\nin Supplementary Table S2-3, we also describe the obtained\nresults by the analyzed transformers on the clinical coding\ntasks considering the additional evaluation metrics. Accord-\ning to the F1-score, for each transformer, the clinical version\nsurpassed the general-domain version of the model. In addi-\ntion, the mBERT-Galén +BETO-Galén +XLM-R-Galén\nensemble model improved the prior SOTA performance on\nthe CodiEsp-P task while obtaining comparable results with\nthe SOTA performance on the CodiEsp-D and Cantemist-\nCoding tasks.\nV. DISCUSSION\nA. DOMAIN-SPECIFIC MODELS\nAutomatic clinical coding is a crucial task in the process\nof extracting valuable information from unstructured patient\ndata stored in modern EHR systems. This work systemati-\ncally analyzed the performance achieved by 3 transformer\narchitectures when applied to the problem of clinical coding\nin Spanish. We followed a TL-based strategy to adapt the\ntransformers to the distinctive features of the Spanish medical\ndomain. For this purpose, the models were ﬁrst pretrained\nby using a private corpus of real-world oncology cases in\nSpanish. To evaluate the validity of the proposed approach,\nwe compared the performance obtained by the original gen-\neral domain version of the transformers with the performance\nachieved by the Spanish clinical version of the models. The\nobtained results showed that for each analyzed transformer,\nthe domain-speciﬁc model outperformed the corresponding\ngeneral model across the 3 clinical coding tasks explored in\nthis study. Among the 3 transformer architectures, BETO,\nthe Spanish version of BERT, was the model that beneﬁted\nthe most from the domain adaptation procedure, improving\nits average performance on the CodiEsp-D, CodiEsp-P and\nCantemist-Coding tasks by approximately 5.5%, 12.2% and\n5.4%, respectively (see Table 2), when pretrained on the\nGalén corpus.\nAdditionally, we further examined the MLM loss of the\n3 transformers on a validation corpus of 4.4K deidentiﬁed\ngenetic counseling documents retrieved from the Galén Infor-\nmation System [33], [34]. The MLM loss should be con-\nsidered an intramodel performance evaluation metric, as it\ngreatly depends on the speciﬁc factors of each model, such\nas the vocabulary employed by the tokenizer. In this way,\nthe MLM loss values of the general domain versions of\nmBERT, BETO and XLM-R were 3.331, 2.114 and 3.21,\nrespectively. When further pretrained on the corpus of oncol-\nogy clinical cases, mBERT, BETO and XLM-R reduced their\nMLM loss scores to 1.666, 1.639 and 1.131, respectively.\nThe obtained loss values on the MLM task show that, for\neach analyzed transformer, its clinical language modeling\ncapabilities improved when further pretrained on the corpus\nof real-world oncology cases.\nThus, the results obtained in this work for automatic\nclinical coding in Spanish support the hypothesis assert-\ning that, when adapted to the speciﬁcities of the clinical\ndomain, transformer-based models outperformed their orig-\ninal nonspeciﬁc domain versions on downstream medical\ntasks. Although this hypothesis was already explored in previ-\nous works [41]–[43], its validity has only been demonstrated\nfor clinical NLP tasks in the English language, for which\na considerable amount of standardized and curated medical\nlinguistic resources is publicly available. In contrast, in this\nstudy, we have experimentally shown the effectiveness of\nthe clinical domain adaptation of transformers when applied\nto small-data NLP tasks in a non-English language with\nlimited textual resources. In this way, a Spanish version and\ntwo multilingual versions of transformer-based models have\nbeneﬁted from pretraining their architectures on a real-world\ncorpus of 30.9K oncology texts to further tackle clinical\ncoding downstream tasks in the Spanish language.\nB. FINE-TUNING APPROACH\nOnce the clinical pretraining of the transformers was com-\npleted, the resulting models were ﬁne-tuned following a mul-\ntilabel sequence classiﬁcation approach to perform clinical\ncoding. In this study, we also analyzed the impact of the\ndeveloped ﬁne-tuning approach on the achieved results by\n72394 VOLUME 9, 2021\nG. López-Garcíaet al.: Transformers for Clinical Coding in Spanish\nTABLE 4. Comparison of the clinical coding performance of BETO-Galén following different strategies to perform the fine-tuning of the model: the\ntext-stream approach [32], themultiple-sentences strategy [14] and thesingle-sentence classification approach proposed in this work. For each clinical\ncoding task, we describe the number of fine-tuning instances obtained from the training and development subsets when applying each of the different\nfine-tuning approaches (Sizecolumn), as well as the average subwords sequence length of the obtained fine-tuning instances (Lencolumn). Additionally,\nthe performance of the model in terms of the distribution of the MAP values obtained over 5 distinct executions (Mean ± Std and Max columns), and the\nMAP score (Enscolumn) obtained by the combination of the 5 model instances following the ensemble approach are reported for each task.\ncomparing them with the results obtained when following\nother existing clinical coding strategies that also deal with\nthe input sequence limitation presented by transformers. In a\nrecent preliminary work [32], we designed a fragment-based\nclassiﬁcation approach consisting of ﬁne-tuning the models\non a corpus of text streams annotated with clinical codes. In a\nmore recent preliminary work [14], we modiﬁed the previous\nstrategy by generating a collection of annotated fragments of\ntext comprising a sequence of multiple contiguous sentences.\nFinally, in the current study, we created a corpus of fragments\nexclusively spanning a single sentence (see Section III-C2).\nTable 4 shows the results obtained by the best perform-\ning model, i.e., BETO-Galén (see Section IV), across the\n3 clinical coding tasks analyzed in this work when ﬁne-\ntuned following the previously described approaches. When\ncomparing the text-stream ﬁne-tuning approach [32] with\nthe multiple-sentences strategy [14], the latter allowed the\nBERT-Galén model to obtain better results in each of the\nclinical coding tasks. Although both strategies produced ﬁne-\ntuning corpora with similar sizes, by following the multiple-\nsentences approach, the model was ﬁne-tuned on a corpus\nof annotated fragments of text with full semantic meaning,\nwhich resulted in the superior predictive performance of\nthe transformer in the downstream tasks. Among the three\nstrategies, the single-sentence approach proposed in this work\nyielded a signiﬁcantly superior performance of the BERT-\nGalén model across the 3 tasks. The key aspect of the single-\nsentence strategy is that, in contrast to the multiple-sentences\napproach, each generated text fragment only comprises one\nsentence, hence reducing the potential length of the input\nsequences to the model. As a consequence, the size of the\ngenerated ﬁne-tuning corpus was signiﬁcantly higher than\nthe number of text fragments obtained using the other two\nstrategies. For this reason, our single-sentence developed\napproach not only deals with the input sequence constraint\nof transformer architectures but also works as an effective\ndata augmentation method, which has been shown to boost\nthe performance of transformer-based models when applied\nto small-data NLP problems such as the clinical coding tasks\nin Spanish addressed in this work.\nThis study has several limitations. We mainly focused\non clinical coding, which is a document-level NLP task.\nHowever, the domain-speciﬁc transformers examined in this\nwork could also be applied to word-level NLP problems,\nsuch as NER tasks, for which transformer-based models have\nbeen shown to achieve SOTA performance in the clinical\nNLP domain [41]–[43], mainly for the English language.\nAdditionally, future studies should pay special attention to\nmodel interpretability, as though a few efforts have already\nbeen made in this particular area, most of DL models are\nstill regarded as ‘‘black-boxes’’. In the medical domain, inter-\npretability is mandatory if computer-based methods aim to\nserve as support in clinical decision-making.\nVI. CONCLUSION\nIn this paper, we systematically evaluated the performance\nof 3 transformer-based models to perform automatic clin-\nical coding in Spanish. By means of a TL-based strategy,\nthe models were ﬁrst pretrained on a private corpus of\nreal-world oncology clinical cases with the aim of adapt-\ning transformers to the speciﬁcities of the Spanish medical\ndomain. The resulting models were further ﬁne-tuned fol-\nlowing a multilabel sentence classiﬁcation approach that not\nonly addressed the ﬁxed-length input sequence constraint\npresented by transformers but also effectively served as a data\naugmentation procedure. The combination of the developed\nTL-based strategy with an ensemble approach that leveraged\nthe predictive capabilities of the distinct models yielded the\nbest obtained results, which remarkably surpassed the prior\nSOTA performance by 11.6% on the CodiEsp-D task, 10.3%\non the CodiEsp-P task, and 4.4% on the Cantemist-Coding\ntask. Furthermore, we publicly released the mBERT, BETO\nand XLM-R transformers adapted to the Spanish clinical\ndomain.\nREFERENCES\n[1] L. A. Baumann, J. Baker, and A. G. Elshaug, ‘‘The impact of electronic\nhealth record systems on clinical documentation times: A systematic\nreview,’’Health Policy, vol. 122, no. 8, pp. 827–836, Aug. 2018.\n[2] M. H. Stanﬁll, M. Williams, S. H. Fenton, R. A. Jenders, and W. R. Hersh,\n‘‘A systematic literature review of automated clinical coding and classiﬁ-\ncation systems,’’ J. Amer. Med. Inform. Assoc., vol. 17, no. 6, pp. 646–651,\nNov. 2010.\n[3] S. Campbell and K. Giadresco, ‘‘Computer-assisted clinical coding: A nar-\nrative review of the literature on its beneﬁts, limitations, implementation\nand impact on clinical coding professionals,’’ Health Inf. Manage. J.,\nvol. 49, no. 1, pp. 5–18, Jan. 2020.\n[4] J. Bronnert, ‘‘Preparing for the CAC transition,’’ J. AHIMA, vol. 82, no. 7,\npp. 1–60, Jul. 2011.\nVOLUME 9, 2021 72395\nG. López-Garcíaet al.: Transformers for Clinical Coding in Spanish\n[5] S. V . S. Pakhomov, J. D. Buntrock, and C. G. Chute, ‘‘Automating the\nassignment of diagnosis codes to patient encounters using example-based\nand machine learning techniques,’’ J. Amer. Med. Inform. Assoc., vol. 13,\nno. 5, pp. 516–525, Sep. 2006.\n[6] J. P. Pestian, C. Brew, P. Matykiewicz, D. J. Hovermale, N. Johnson,\nK. B. Cohen, and W. Duch, ‘‘A shared task involving multi-label clas-\nsiﬁcation of clinical free text,’’ in Proc. Biol., Transl., Clin. Lang. Pro-\ncess. Prague, Czech Republic: Association for Computational Linguistics,\nJun. 2007, pp. 97–104.\n[7] A. Perotte, R. Pivovarov, K. Natarajan, N. Weiskopf, F. Wood, and\nN. Elhadad, ‘‘Diagnosis code assignment: Models and evaluation metrics,’’\nJ. Amer. Med. Inform. Assoc., vol. 21, no. 2, pp. 231–237, Mar. 2014.\n[8] M. Hughes, I. Li, S. Kotoulas, and T. Suzumura, ‘‘Medical text classiﬁ-\ncation using convolutional neural networks,’’ Stud Health Technol. Inf.,\nvol. 235, pp. 246–250, May 2017.\n[9] Z. Miftahutdinov and E. Tutubalina, ‘‘Deep learning for ICD coding:\nLooking for medical concepts in clinical documents in English and in\nFrench,’’ in Experimental IR Meets Multilinguality, Multimodality, and\nInteraction. Cham, Switzerland: Springer, 2018, pp. 203–215.\n[10] G. Mujtaba, L. Shuib, N. Idris, W. L. Hoo, R. G. Raj, K. Khowaja,\nK. Shaikh, and H. F. Nweke, ‘‘Clinical text classiﬁcation research trends:\nSystematic literature review and open issues,’’ Expert Syst. Appl., vol. 116,\npp. 494–520, Feb. 2019.\n[11] L. Yao, C. Mao, and Y . Luo, ‘‘Clinical text classiﬁcation with rule-based\nfeatures and knowledge-guided convolutional neural networks,’’ BMC\nMed. Informat. Decis. Making, vol. 19, no. S3, p. 71, Apr. 2019.\n[12] E. Moons, A. Khanna, A. Akkasi, and M.-F. Moens, ‘‘A comparison of\ndeep learning methods for ICD coding of clinical records,’’ NATO Adv.\nSci. Inst. E, Appl. Sci., vol. 10, no. 15, p. 5262, Jul. 2020.\n[13] A. Miranda-Escalada, E. Farré-Maduell, and M. Krallinger, ‘‘Named\nentity recognition, concept normalization and clinical coding: Overview\nof the cantemist track for cancer text mining in Spanish, corpus, guide-\nlines, methods and results,’’ in Proc. Iberian Lang. Eval. Forum, CEUR\nWorkshop (IberLEF), M. Á. G. Cumbreras, J. Gonzalo, E. M. Cámara,\nR. M. Unanue, P. Rosso, S. J. Zafra, J. A. Ortiz-Zambrano, A. Miranda,\nJ. Porta-Zamorano, Y . Guitiérrez, A. Rosá, M. M.-Y . Gómez, and\nM. García-Vega, Eds., Málaga, Spain, 2020, pp. 303–323.\n[14] G. López-García, J. M. Jerez, N. Ribelles, E. Alba, and F. J. Veredas, ‘‘ICB-\nUMA at CANTEMIST 2020: Automatic ICD-O coding in Spanish with\nBERT,’’ inProc. Iberian Lang. Eval. Forum, CEUR Workshop (IberLEF),\nM. Á. G. Cumbreras, J. Gonzalo, E. M. Cámara, R. M. Unanue, P. Rosso,\nS. J. Zafra, J. A. Ortiz-Zambrano, A. Miranda, J. Porta-Zamorano,\nY . Guitiérrez, A. Rosá, M. M.-Y . Gómez, and M. García-Vega, Eds., 2020,\npp. 468–476.\n[15] A. García-Pablos, N. Perez, and M. Cuadros, ‘‘Vicomtech at\nCANTEMIST,’’ in Proc. Iberian Lang. Eval. Forum, CEUR Workshop\n(IberLEF), M. Á. G. Cumbreras, J. Gonzalo, E. M. Cámara,\nR. M. Unanue, P. Rosso, S. J. Zafra, J. A. Ortiz-Zambrano, A. Miranda,\nJ. Porta-Zamorano, Y . Guitiérrez, A. Rosá, M. M.-Y . Gómez, and\nM. García-Vega, Eds., Málaga, Spain, 2020, pp. 489–498.\n[16] A. Miranda-Escalada, A. Gonzalez-Agirre, J. Armengol-Estapé, and\nM. Krallinger, ‘‘Overview of automatic clinical coding: Annotations,\nguidelines, and solutions for non-English clinical cases at codiesp track\nof CLEF eHealth 2020,’’ in Proc. Work. Notes Conf. Labs Eval., CLEF\nForum, CEUR Workshop, 2020, pp. 303–323.\n[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ Oct. 2018,\narXiv:1810.04805. [Online]. Available: http://arxiv.org/abs/1810.04805\n[18] J. Cañete, G. Chaperon, R. Fuentes, J.-H. Ho, H. Kang, and J. Pérez,\n‘‘Spanish pre-trained BERT model and evaluation data,’’ in Proc. Practical\nML Developing Countries Workshop ICLR, 2020, pp. 1–10.\n[19] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,\nF. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov, ‘‘Unsu-\npervised cross-lingual representation learning at scale,’’ Nov. 2019,\narXiv:1911.02116. [Online]. Available: http://arxiv.org/abs/1911.02116\n[20] A. Blanco, A. Casillas, A. Pérez, and A. D. de Ilarraza, ‘‘Multi-label\nclinical document classiﬁcation: Impact of label-density,’’ Expert Syst.\nAppl., vol. 138, Dec. 2019, Art. no. 112835.\n[21] J. Pérez, A. Pérez, A. Casillas, and K. Gojenola, ‘‘Cardiology record multi-\nlabel classiﬁcation using latent Dirichlet allocation,’’ Comput. Methods\nPrograms Biomed., vol. 164, pp. 111–119, Oct. 2018.\n[22] M. Almagro, R. M. Unanue, V . Fresno, and S. Montalvo, ‘‘ICD-10 coding\nof Spanish electronic discharge summaries: An extreme classiﬁcation\nproblem,’’IEEE Access, vol. 8, pp. 100073–100083, 2020.\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. U. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30, I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Red Hook,\nNY , USA: Curran Associates, 2017, pp. 5998–6008.\n[24] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and\nL. Zettlemoyer, ‘‘Deep contextualized word representations,’’ Feb. 2018,\narXiv:1802.05365. [Online]. Available: http://arxiv.org/abs/1802.05365\n[25] J. Howard and S. Ruder, ‘‘Universal language model ﬁne-tuning for\ntext classiﬁcation,’’ Jan. 2018, arXiv:1801.06146. [Online]. Available:\nhttp://arxiv.org/abs/1801.06146\n[26] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized\nBERT pretraining approach,’’ 2019, arXiv:1907.11692. [Online]. Avail-\nable: http://arxiv.org/abs/1907.11692\n[27] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou,\nW. Li, and P. J. Liu, ‘‘Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer,’’ 2020, arXiv:1910.10683. [Online]. Available:\nhttp://arxiv.org/abs/1910.10683\n[28] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le,\n‘‘XLNet: Generalized autoregressive pretraining for language under-\nstanding,’’ 2020, arXiv:1906.08237. [Online]. Available: http://arxiv.\norg/abs/1906.08237\n[29] G. López-García, J. M. Jerez, L. Franco, and F. J. Veredas, ‘‘Transfer\nlearning with convolutional neural networks for cancer survival predic-\ntion using gene-expression data,’’ PLoS ONE, vol. 15, no. 3, Mar. 2020,\nArt. no. e0230536.\n[30] G. López-García, J. M. Jerez, L. Franco, and F. J. Veredas, ‘‘A transfer-\nlearning approach to feature extraction from cancer transcriptomes with\ndeep autoencoders,’’ in Advances in Computational Intelligence, I. Rojas,\nG. Joya, and A. Catala, Eds. Cham, Switzerland: Springer, 2019,\npp. 912–924.\n[31] D. Urda, F. J. Veredas, I. Turias, and L. Franco, ‘‘Addition of pathway-\nbased information to improve predictions in transcriptomics,’’ in Bioinfor-\nmatics and Biomedical Engineering. Cham, Switzerland: Springer, 2019,\npp. 200–208.\n[32] G. López-Garcıa, J. M. Jerez, and F. J. Veredas, ‘‘ICB-UMA at CLEF e-\nhealth 2020 task 1: Automatic ICD-10 coding in Spanish with BERT,’’\nin Proc. Work. Notes CLEF , Conf. Labs Eval. Forum, CEUR Workshop,\nL. Cappellato, C. Eickhoff, N. Ferro, and A. Névéol, 2020, pp. 1–15.\n[33] N. Ribelles, J. M. Jerez, D. Urda, J. L. Subirats, A. Márquez, C. Quero,\nand L. A. Franco, ‘‘Galén: Sistema de información para la gestión y\ncoordinación de procesos en un servicio de oncología,’’ RevistaeSalud,\nvol. 6, no. 21, pp. 1–12, 2010.\n[34] N. Ribelles, J. M. Jerez, P. Rodriguez-Brazzarola, B. Jimenez,\nT. Diaz-Redondo, H. Mesa, A. Marquez, A. Sanchez-Muñoz, B. Pajares,\nF. Carabantes, M. J. Bermejo, E. Villar, M. E. Dominguez-Recio,\nE. Saez, L. Galvez, A. Godoy, L. Franco, S. Ruiz-Medina, I. Lopez,\nand E. Alba, ‘‘Machine learning and natural language processing (NLP)\napproach to predict early progression to ﬁrst-line treatment in real-world\nhormone receptor-positive (HR+)/HER2-negative advanced breast cancer\npatients,’’Eur. J. Cancer, vol. 144, pp. 224–231, Feb. 2021.\n[35] M. Johnson, M. Schuster, Q. V . Le, M. Krikun, Y . Wu, Z. Chen, N. Thorat,\nF. Viégas, M. Wattenberg, G. Corrado, M. Hughes, and J. Dean, ‘‘Google’s\nmultilingual neural machine translation system: Enabling zero-shot trans-\nlation,’’Trans. Assoc. Comput. Linguistics, vol. 5, pp. 339–351, Oct. 2017.\n[36] J. Tiedemann, ‘‘Parallel data, tools and interfaces in OPUS,’’ in Proc. 8th\nInt. Conf. Lang. Resour. Eval. (LREC), 2012, pp. 2214–2218.\n[37] G. Lample and A. Conneau, ‘‘Cross-lingual language model pre-\ntraining,’’ 2019, arXiv:1901.07291. [Online]. Available: http://arxiv\norg/abs/1901.07291\n[38] T. Kudo and J. Richardson, ‘‘SentencePiece: A simple and lan-\nguage independent subword tokenizer and detokenizer for neural\ntext processing,’’ Aug. 2018, arXiv:1808.06226. [Online]. Available:\nhttp://arxiv.org/abs/1808.06226\n[39] T. Wolf et al., ‘‘HuggingFace’s transformers: State-of-the-art natural lan-\nguage processing,’’ Oct. 2019, arXiv:1910.03771. [Online]. Available:\nhttp://arxiv.org/abs/1910.03771\n[40] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, ‘‘On the vari-\nance of the adaptive learning rate and beyond,’’ 2020, arXiv:1908.03265.\n[Online]. Available: http://arxiv.org/abs/1908.03265\n[41] Y . Si, J. Wang, H. Xu, and K. Roberts, ‘‘Enhancing clinical concept\nextraction with contextual embeddings,’’ J. Amer. Med. Inform. Assoc.,\nvol. 26, no. 11, pp. 1297–1304, Nov. 2019.\n72396 VOLUME 9, 2021\nG. López-Garcíaet al.: Transformers for Clinical Coding in Spanish\n[42] E. Alsentzer, J. Murphy, W. Boag, W.-H. Weng, D. Jindi, T. Naumann,\nand M. McDermott, ‘‘Publicly available clinical BERT embeddings,’’ in\nProc. 2nd Clin. Natural Lang. Process. Workshop. Minneapolis, MI, USA:\nAssociation for Computational Linguistics, Jun. 2019, pp. 72–78.\n[43] X. Yang, J. Bian, W. R. Hogan, and Y . Wu, ‘‘Clinical concept extrac-\ntion using transformers,’’ J. Amer. Med. Inform. Assoc., vol. 27, no. 12,\npp. 1935–1942, Dec. 2020.\nGUILLERMO LÓPEZ-GARCÍA was born in\nMálaga, Spain. He received the B.S. degree in\nbioinformatics and the M.S. degree in software\nengineering and artiﬁcial intelligence from the\nUniversity of Málaga (UMA), Spain, in 2018 and\n2019, respectively, where he is currently pursuing\nthe Ph.D. degree in computer technologies.\nSince 2019, he has been working as a\nPredoctoral Researcher with the Computational\nIntelligence and Biomedicine Research Group,\nDepartment of Computer Science and Programming Languages, UMA.\nHis research interests include adaptation and development of deep learning\nmethods to solve complex and heterogeneous biomedical problems, such as\ncancer survival prediction from omics data, methionine oxidation in proteins,\nand information extraction from medical documents.\nJOSÉ M. JEREZ received the M.S. and Ph.D.\ndegrees in computer science from the University\nof Málaga, Spain. He developed his Ph.D. thesis\nin the ﬁeld of computational neuroscience and its\napplication to artiﬁcial intelligence problems. He\nhas directed both research projects with competi-\ntive funding (National Research and Development\nPlan, Junta de Andalucía) and technology transfer\ncontracts with different private companies aimed\nat the application of data mining techniques to\nclassiﬁcation problems in the ﬁeld of biomedicine and engineering. Since\n2011, he has been responsible for the ICB Research Group (Computational\nIntelligence in Biomedicine, Junta de Andalucía), also integrated in the\nBiomedical Research Institute of Málaga (IBIMA). He is the coauthor of over\n50 articles published in international indexed journals. A signiﬁcant number\nof these works have also been published in collaboration with both Spanish\nand foreign research groups in the areas of molecular biology, oncology,\npathology, surgery, and physics. Some of these works have been the result of\nstable collaboration with research groups of international reference, such as\nthe University of Oxford, Georgetown University, University of Trieste, and\nthe University of Córdoba, Argentina.\nNURIA RIBELLES received the M.D. and Ph.D.\ndegrees. She is currently a Medical Oncologist.\nShe is in charge of the management and orga-\nnization of the outpatient facilities and treatment\narea at the Hospital Universitario Virgen de la\nVictoria integrated into Unidad de Gestión Clínica\nOncología Intercentros of Málaga, Spain. At the\nhealthcare level, her research area of interest is\nbreast cancer, and for this reason, most of the\nresearch projects in which she has participated are\nrelated to the molecular biology of breast cancer, as well as to the study\nof recurrence patterns and the development of predictive models of it. Her\nresearch interests are also focused on information technologies. She has\nmore than ten years of experience in the development of a comprehensive\ninformation system designed for the management of both daily care and\nclinical and translational research of cancer patients treated in four hospitals\nin Málaga. Her interest in massive data analysis and real-world data has led\nher to be part of the Executive Committee of the Section for the Evaluation\nof Results and Clinical Practice of the Spanish Society of Medical Oncology.\nEMILIO ALBA was born in Málaga, Spain,\nin 1958. He received the M.D. and Ph.D. degrees.\nHe is currently a Medical Oncologist. He is the\nDirector of the UGCI Medical Oncology with the\nHospitals Regional University and Virgen de la\nVictoria in Málaga, a Professor of oncology with\nthe University of Málaga (UMA), a Coordinator\nof the Scientiﬁc Area of Onco-Hematology-Rare\nDiseases with the Biomedical Research Institute\nof Málaga (IBIMA), the Director of the UMA\nMedical-Sanitary Research Center (CIMES), a Principal Investigator from\nthe CANCER Thematic Area (CIBERONC) 2017CB16/12/00481, breast\ncancer, and the President of the Association for Oncological Research in\nMalaga (AIOM). He is the coauthor of over 180 articles published in interna-\ntional indexed journals. He is the co-inventor of six patents. Since 2003, he\nhas been a member of the Royal Academy of Medicine of Eastern Andalusia.\nFRANCISCO J. VEREDASwas born in Málaga,\nSpain, in 1970. He received the M.S. and Ph.D.\ndegrees in computer science from the University\nof Málaga, Spain, in 1996 and 2004, respectively.\nSince 1998, he has been working as an Associate\nProfessor of computer programming languages\nwith the Department of Computer Science and\nProgramming Languages, University of Málaga.\nSince 2001, he has been developing his research\nwork in the Group of Computational Intelligence\nin Biomedicine, University of Málaga. His research interests include artiﬁ-\ncial intelligence applied to biomedical problems, machine learning and deep\nlearning for biomedical applications, bioinformatics, and natural language\nprocessing applied to electronic health records. His Ph.D. thesis was about\ncorrelated activity in networks of spiking neurons.\nVOLUME 9, 2021 72397",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7795588970184326
    },
    {
      "name": "Computer science",
      "score": 0.7328118085861206
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.6442069411277771
    },
    {
      "name": "Sentence",
      "score": 0.6242752075195312
    },
    {
      "name": "Natural language processing",
      "score": 0.5554043650627136
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5501329898834229
    },
    {
      "name": "Language model",
      "score": 0.46313929557800293
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4473671615123749
    },
    {
      "name": "Transfer of learning",
      "score": 0.4171910285949707
    },
    {
      "name": "Task (project management)",
      "score": 0.10484504699707031
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82767444",
      "name": "Universidad de Málaga",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I4210164724",
      "name": "Instituto de Investigación Biomédica de Málaga",
      "country": "ES"
    }
  ]
}