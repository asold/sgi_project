{
  "title": "Can large language models help augment English psycholinguistic datasets?",
  "url": "https://openalex.org/W4391132527",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4223174035",
      "name": "Trott, Sean",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4321455981",
    "https://openalex.org/W3000950628",
    "https://openalex.org/W2084086631",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2048795844",
    "https://openalex.org/W2434184590",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W6839366901",
    "https://openalex.org/W4306247398",
    "https://openalex.org/W6846260263",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W1997161938",
    "https://openalex.org/W1989847457",
    "https://openalex.org/W1974991592",
    "https://openalex.org/W4324098937",
    "https://openalex.org/W2062337042",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W2159230047",
    "https://openalex.org/W4378782804",
    "https://openalex.org/W6843304315",
    "https://openalex.org/W2887161450",
    "https://openalex.org/W2996070074",
    "https://openalex.org/W4399848789",
    "https://openalex.org/W2735962253",
    "https://openalex.org/W6741039739",
    "https://openalex.org/W6769430610",
    "https://openalex.org/W2963366649",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W3099695344",
    "https://openalex.org/W3212354382",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W6753056052",
    "https://openalex.org/W2199803028",
    "https://openalex.org/W2085876742",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W4362599805",
    "https://openalex.org/W4385571325",
    "https://openalex.org/W2010523604",
    "https://openalex.org/W4317892547",
    "https://openalex.org/W2069997377",
    "https://openalex.org/W4327519588",
    "https://openalex.org/W6851024552",
    "https://openalex.org/W4287854950",
    "https://openalex.org/W2798881773",
    "https://openalex.org/W2052091366",
    "https://openalex.org/W2160654481",
    "https://openalex.org/W1979532929",
    "https://openalex.org/W6645009471",
    "https://openalex.org/W2971448691",
    "https://openalex.org/W3156782505",
    "https://openalex.org/W2982116886",
    "https://openalex.org/W4392935324",
    "https://openalex.org/W4226510082",
    "https://openalex.org/W1535656004",
    "https://openalex.org/W3198103223",
    "https://openalex.org/W4286988498",
    "https://openalex.org/W4307418102",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2108038333",
    "https://openalex.org/W4379347837",
    "https://openalex.org/W6852454441",
    "https://openalex.org/W2179330824",
    "https://openalex.org/W6685952651",
    "https://openalex.org/W4291991132",
    "https://openalex.org/W4385571495",
    "https://openalex.org/W4377098551",
    "https://openalex.org/W3130319171",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W2889870962",
    "https://openalex.org/W6753817867",
    "https://openalex.org/W2954933613",
    "https://openalex.org/W2124006395",
    "https://openalex.org/W2131694082",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2891905723",
    "https://openalex.org/W2108141763",
    "https://openalex.org/W4388443930",
    "https://openalex.org/W3165845326",
    "https://openalex.org/W3177005832",
    "https://openalex.org/W4323653446",
    "https://openalex.org/W6850435779",
    "https://openalex.org/W4383058631",
    "https://openalex.org/W3031473178",
    "https://openalex.org/W2082230970",
    "https://openalex.org/W4308411106",
    "https://openalex.org/W4242397817",
    "https://openalex.org/W4296755650",
    "https://openalex.org/W4366496010",
    "https://openalex.org/W3027764784",
    "https://openalex.org/W2410831681",
    "https://openalex.org/W6714899898",
    "https://openalex.org/W4392773006",
    "https://openalex.org/W4401506443"
  ],
  "abstract": "Abstract Research on language and cognition relies extensively on psycholinguistic datasets or “norms”. These datasets contain judgments of lexical properties like concreteness and age of acquisition, and can be used to norm experimental stimuli, discover empirical relationships in the lexicon, and stress-test computational models. However, collecting human judgments at scale is both time-consuming and expensive. This issue of scale is compounded for multi-dimensional norms and those incorporating context. The current work asks whether large language models (LLMs) can be leveraged to augment the creation of large, psycholinguistic datasets in English. I use GPT-4 to collect multiple kinds of semantic judgments (e.g., word similarity, contextualized sensorimotor associations, iconicity) for English words and compare these judgments against the human “gold standard”. For each dataset, I find that GPT-4’s judgments are positively correlated with human judgments, in some cases rivaling or even exceeding the average inter-annotator agreement displayed by humans. I then identify several ways in which LLM-generated norms differ from human-generated norms systematically. I also perform several “substitution analyses”, which demonstrate that replacing human-generated norms with LLM-generated norms in a statistical model does not change the sign of parameter estimates (though in select cases, there are significant changes to their magnitude ). I conclude by discussing the considerations and limitations associated with LLM-generated norms in general, including concerns of data contamination, the choice of LLM, external validity, construct validity, and data quality. Additionally, all of GPT-4’s judgments (over 30,000 in total) are made available online for further analysis.",
  "full_text": "Vol:.(1234567890)\nBehavior Research Methods (2024) 56:6082–6100\nhttps://doi.org/10.3758/s13428-024-02337-z\nORIGINAL MANUSCRIPT\nCan large language models help augment English psycholinguistic \ndatasets?\nSean Trott1 \nAccepted: 5 January 2024 / Published online: 23 January 2024 \n© The Author(s) 2024\nAbstract\nResearch on language and cognition relies extensively on psycholinguistic datasets or “norms”. These datasets contain \njudgments of lexical properties like concreteness and age of acquisition, and can be used to norm experimental stimuli, \ndiscover empirical relationships in the lexicon, and stress-test computational models. However, collecting human judgments \nat scale is both time-consuming and expensive. This issue of scale is compounded for multi-dimensional norms and those \nincorporating context. The current work asks whether large language models (LLMs) can be leveraged to augment the crea-\ntion of large, psycholinguistic datasets in English. I use GPT-4 to collect multiple kinds of semantic judgments (e.g., word \nsimilarity, contextualized sensorimotor associations, iconicity) for English words and compare these judgments against the \nhuman “gold standard”. For each dataset, I find that GPT-4’s judgments are positively correlated with human judgments, in \nsome cases rivaling or even exceeding the average inter-annotator agreement displayed by humans. I then identify several \nways in which LLM-generated norms differ from human-generated norms systematically. I also perform several “substitu-\ntion analyses”, which demonstrate that replacing human-generated norms with LLM-generated norms in a statistical model \ndoes not change the sign of parameter estimates (though in select cases, there are significant changes to their magnitude). \nI conclude by discussing the considerations and limitations associated with LLM-generated norms in general, including \nconcerns of data contamination, the choice of LLM, external validity, construct validity, and data quality. Additionally, all \nof GPT-4’s judgments (over 30,000 in total) are made available online for further analysis.\nKeywords Dataset · Psycholinguistic resource · Large language models · ChatGPT\nIntroduction\nResearch on language and cognition relies extensively on \nlarge, psycholinguistic datasets—sometimes called “norms”. \nThese datasets contain information about various properties \nof words and sentences, including concreteness (Brysbaert \net al., 2014a, b), sensorimotor associations (Lynott et al., \n2020), affect (Bradley & Lang, 1999), semantic similarity \n(Hill et al., 2015), iconicity (Winter et al., 2023), and more.\nBuilding these datasets is often time-consuming and \nexpensive. One possible solution is to augment the con-\nstruction of psycholinguistic datasets using computational \ntools, such as Large Language Models (LLMs), to reduce \nthis difficulty; this approach is seeing growing popularity in \nrelated fields (Aher et al., 2022; Argyle et al., 2022; Törn-\nberg, 2023; Zhu et al., 2023; Gilardi et al., 2023). However, \nthe empirical question of whether and to what extent LLMs \ncan reliably capture psycholinguistic judgments remains \nunanswered. In this paper, I apply LLMs to several major \npsycholinguistic datasets, quantify their performance, and \ndiscuss the advantages and disadvantages of this approach.\nWhy do psycholinguists need psycholinguistic \nnorms?\nThese norms have multiple uses. First, experimentalists can \nuse this information to normalize (or “norm”) their stimuli. \nFor example, a researcher designing a lexical decision task \nmight ensure that the words in each condition are “matched” \n * Sean Trott \n sttrott@ucsd.edu\n1 Department of Cognitive Science, UC San Diego, 9500 \nGilman Dr., La Jolla, CA 92093-0515, USA\n6083Behavior Research Methods (2024) 56:6082–6100 \nfor properties such as frequency and concreteness to avoid \nintroducing confounds.1\nSecond, these norms are often interesting to researchers \nin their own right. By examining the relationships between \nthese properties, researchers can make inferences about \nthe mechanisms that guide language acquisition, language \nprocessing and production, and even language change. For \nexample, there is now a robust body of evidence (Thompson \net al., 2012; Dingemanse et al., 2015; Winter et al., 2023) \ndemonstrating a relationship between iconicity (the extent to \nwhich a word’s form resembles its meaning; e.g., the word \n“slurp” is often considered iconic) and age of acquisition  \n(the average age at which a word is learned); this evidence—\nenabled by large-scale norms of iconicity and age of acquisi-\ntion—informs theories of word learning (Dingemanse et al., \n2015; Imai & Kita, 2014).\nImportantly, while some of these norms can be esti-\nmated automatically from a large corpus of text (e.g., word \nfrequency), many datasets rely on crowd-sourced human \njudgments. This raises a set of interrelated challenges for \nresearchers interested in either creating or using these \nnorms. In the section below, I describe why scale presents \na central challenge, then introduce two other challenges—\nmulti-dimensional norms and context-dependent mean-\ning—that compound the problem of scale. I then introduce \na potential solution—using large language models (LLMs) \nsuch as GPT-4 to augment the creation of psycholinguistic \nnorms.\nThe challenge of scale\nIt is both time-consuming and costly to collect these norms \nat scale. As a consequence, older datasets have been rela-\ntively small (e.g., fewer than 1000 words). Within recent \ndecades, the development of online crowd-sourcing tools \nlike Amazon Mechanical Turk 2 has enabled larger-scale \ndatasets containing judgments for thousands of words (Win-\nter et al., 2023; Brysbaert et al., 2014a, b).\nHowever, creating these datasets remains a non-trivial \ntask: if the goal is to collect ten judgments each for 40,000 \nwords, a researcher must collect a total of 400,000 judg-\nments. Assuming each judgment takes approximately 5~s \nto make, then even a researcher paying the federal minimum \nwage ($7.25 per hour3) will need to pay at least $4000. And \nas others have noted (Webb & Tangney, 2022; Veselovsky \net al., 2023), many participants on websites like Amazon \nMechanical Turk are unreliable, which necessitates the use \nof rigorous exclusion criteria. Assuming a 25% exclusion \nrate (substantially lower than the rate reported by Webb & \nTangney, 2022), a researcher would need to collect approxi-\nmately 533,333 judgments, for a total cost of at least $5413.\nThis problem of scale is compounded by two sub-prob-\nlems: the desire to collect multi-dimensional norms for \nwords; and second, the fact that certain semantic properties \nvary across contexts.\nSub-problem #1: Multi-dimensional norms. Research-\ners are increasingly interested in multi-dimensional prop-\nerties of words. For example, the Lancaster sensorimotor \nnorms contain judgments about the extent to which six \nsensory modalities and five action effectors are associated \nwith different words (Lynott et al., 2020); these norms, \nalong with others (Binder et al., 2016), offer a degree of \ngranularity and semantic nuance that is much harder to \nachieve with a single dimension. Yet as more dimensions \nof interest are added, the problem of scale compounds. \nReturning to the example above: if a researcher needs ten \njudgments for 11 different dimensions of 40,000 words, \nwhich amounts to 4,400,00 judgments in total.\nSub-problem #2: Context. Words mean different things \nin different contexts (Yee & Thompson-Schill, 2016). \nThis is most obvious in cases of lexical ambiguity (e.g., \n“wooden table” vs. “data table”), but it arguably also \napplies to more subtle contextual variation (e.g., “She \ncut the lemon” vs. “She juggled the lemon”). This poses a \npotential problem for norms of words judged in isolation. \nFor example, the word “table” would likely be judged \nas more concrete in “wooden table ” than “data table”; \nsimilarly, the word “market” has more olfactory associa-\ntions in the expression “fish market” than “stock market” \n(Trott & Bergen, 2022).\nThere have been some efforts to collect semantic norms \nin context (Scott et al., 2019; Trott & Bergen, 2021; Haber & \nPoesio, 2021; Trott & Bergen, 2022). Here, however, scale is \neven more challenging, as the number of contexts in which \na word might appear is potentially infinite. Which contexts \nshould a researcher collect judgments for?\nThis also inherently limits the utility of these datasets \nfor researchers wishing to norm stimuli that consist of \nmore than single words in isolation. For example, perhaps a \nresearcher wishes to measure behavioral or neurophysiologi-\ncal responses to ambiguous words in different contexts (e.g., \n“wooden table\" vs. “data table”). In this case, the researcher \nmay wish to norm their stimuli not only for the concreteness \nof the target word in isolation (e.g., “table”), but also for \nconcreteness of that particular meaning in a given context \n1 Assuming, of course, that the researcher is not primarily interested \nin the effect of these properties on the dependent variable of interest.\n2 https:// www. mturk. com/\n3 Note that this is not an endorsement of paying a very low wage; this \nexample is used simply to estimate a “lower-bound” on the amount a \nresearcher would need to pay.\n6084 Behavior Research Methods (2024) 56:6082–6100\n(e.g., “wooden table”). Yet it is very unlikely that contextu-\nalized norms for these exact sentences would already exist, \nprecisely because there are an infinite number of contexts in \nwhich a given word could occur—thus requiring researchers \nto collect their own norms. This means that even a very large \ndataset of contextualized judgments may have limited direct \npractical application.\nCan LLMs helps scale psycholinguistic norms?\nThe current work investigates a potential solution to this \nproblem: using large language models (LLMs) to augment \npsycholinguistic datasets. If LLMs provide judgments that \nare sufficiently humanlike, then a small dataset of human \nnorms (i.e., a “gold standard”) could be rapidly scaled to \nencompass more words, more semantic dimensions, and \nmore contexts—all at substantially lower cost. If, however, \nLLMs prove unreliable—i.e., their responses diverge too \nmuch from those of humans—then it is important to quantify \nboth the source and extent of this unreliability. This question \nis especially pressing because there is a growing body of \nresearch interested in using LLMs as experimental subjects \nand data annotators (Aher et al., 2022; Argyle et al., 2022; \nTörnberg, 2023; Zhu et al., 2023; Gilardi et al., 2023; Jain \net al., 2023).\nRelated work Scaling psycholinguistic datasets with the aid \nof computational techniques has been a longstanding goal of \nboth psycholinguistics and natural language processing. The \nmajority of these approaches rely on some level on the dis-\ntributional hypothesis, namely that “you shall know a word \nby the company it keeps” (Firth, 1957, pg. 11; Lewis et al., \n2019). If words with similar meanings appear in similar con-\ntexts (Harris, 1954; McDonald & Ramscar, 2001), then the \nmeaning of a word can be inferred in part by observing the \ncontexts in which it occurs.\nWhile early work (Hatzivassiloglou & McKeown, 1997) \nrelied primarily on count-based methods, the development \nof more advanced computational techniques and accessi-\nbility to larger corpora gave researchers additional power \nand flexibility. Algorithms like word2vec (Mikolov et al., \n2013) allow researchers to represent words as vectors (or \n“word embeddings”) of real numbers, which can in turn be \nused in various arithmetic and statistical operations. Most \nrelevantly, these representations can be leveraged to train a \nstatistical model to predict psycholinguistic dimensions of \ninterest, such as concreteness or arousal (Bestgen & Vincze, \n2012). These representations have been leveraged to extend \nnorms across languages (Thompson & Lupyan, 2018) and \nin some cases (Utsumi, 2020), to norms with many semantic \ndimensions (Binder et al., 2016).\nNotably, however, the approaches reviewed above all \nfocus on predicting semantic properties of words in isola-\ntion; this is in large part because of limitations in compu-\ntational techniques at the time the studies were conducted.\nLarge language models: Advances and break ‑\nthroughs Recent advances—including access to more train-\ning data, more sophisticated neural network architectures, \nand more computing resources—have led to remarkable \nimprovements in language models (Ouyang et al., 2022 ; \nKatz et al., 2023). Because of their size (often billions of \nparameters), these systems are sometimes called large lan-\nguage models (LLMs). Contemporary LLMs are artificial \nneural networks, which learn to map some input representa-\ntion to an output by iteratively tuning the weights between \nneurons in different layers of the network. This tuning pro-\ncess is achieved by extensive training. Specifically, LLMs \nare trained using a token prediction paradigm: given a string \nof tokens (e.g., “The cat sat on the ___”), an LLM must pre-\ndict which word is most likely to come next.4 By observing \nmany sentences like this one, an LLM eventually tunes its \nweights to produce more accurate probability distributions \nover the upcoming token (e.g., “mat”).\nImportantly, LLMs trained in this way display behavior \nconsistent with the acquisition of both syntactic and seman-\ntic knowledge (Tenney et al., 2019). Metrics derived from \nLLMs also successfully predict human dependent measures \non a number of psycholinguistic tasks (Michaelov et al., \n2022; Shain, 2019 ), and are sensitive to relevant psycho-\nlinguistic constructs (Trott & Bergen, 2023; Li & Joanisse, \n2021; Jones et al., 2022; Trott et al., 2022). Critically for our \npurposes, LLMs are sensitive to context: that is, an LLM’s \nrepresentation of a given word (e.g., “bank”) differs on the \nbasis of the immediate linguistic context for that word (e.g., \n“financial bank” vs. “river bank”). This makes LLMs poten-\ntially well suited for research questions that ask how context \nmodifies behavioral responses to a given stimulus or prompt.\nLLMs as experimental subjects and data annota‑\ntors Improvements in LLMs have in turn led to an explosion \nof interest in using LLMs in behavioral research, either to \nreplace crowd-sourced workers (Törnberg, 2023; Zhu et al., \n2023; Gilardi et al., 2023) or even as experimental subjects \n(Aher et al., 2022; Argyle et al., 2022; Dillion et al., 2023; \nJain et al., 2023 ; Cai et al., 2023 ; Binz & Schulz, 2023 ; \nCoda-Forno et al., 2023; Hagendorff, 2023; Kosinski, 2023). \n4 This description applies to auto-regressive LLMs specifically, \nwhich are trained to predict the next word. Some LLMs, such as \nBERT (Devlin et  al., 2018), are “bidirectional” in that they can use \ninformation that comes before and after a “masked” word to predict \nthe identity of the word (e.g., “The [MASK] sat on the mat”).\n6085Behavior Research Methods (2024) 56:6082–6100 \nIn the former case, some efforts have focused on tagging \nsocial media data (Törnberg, 2023; Zhu et al., 2023; Gilardi \net al., 2023) with relevant information such as sentiment and \nthe topic being discussed. Other researchers have explored \nthe use of LLMs in fact-checking (Hoes et al., 2023) and \nanalyzing text for offensiveness or sentiment (Rathje et al., \n2023), albeit with mixed results (Ollion et al., 2023). In the \nlatter case, LLMs have been used as subjects for a diverse \narray of experimental tasks, involving decision-making \n(Coda-Forno et al., 2023), Theory of Mind (Trott et al., \n2022; Kosinski, 2023), sound symbolism (Cai et al., 2023), \nmoral evaluation (Dillion et al., 2023), and more. As oth-\ners have noted (Jain et al., 2023; Doerig et al., 2023), one \nbenefit of LLMs is that they allow for in silico experimenta-\ntion, allowing researchers to rapidly develop and test novel \nhypotheses.\nThis surge of interest is exciting; advances in LLMs rep-\nresent a genuine opportunity for the field of cognitive sci-\nence. One of those opportunities could be helping scale psy-\ncholinguistic datasets. Yet to my knowledge, this question \nhas not been investigated in a systematic way. As behavioral \nresearchers, an empirical, rigorous evaluation of how well \nstate-of-the-art LLMs estimate psycholinguistic norms is \ncritical for making informed decisions about whether and to \nwhat extent to incorporate LLMs into our research agenda. \nIf LLMs perform poorly, it is premature to even consider \nincorporating them; if they perform well, this could pave \nthe way for a more thorough research program on whether \nLLMs might serve a useful purpose. Such a research pro-\ngram could investigate not only how LLMs behave but also \nthe internal representations that guide that behavior (Pavlick, \n2023). However, establishing their performance empirically \nis a crucial first step in either direction.\nCurrent work\nThe primary goal of the current work was to investigate the \nviability of using state-of-the-art LLMs to augment psycho-\nlinguistic norms. I selected a state-of-the-art LLM (GPT-\n4) and elicited judgments for a number of psycholinguistic \nnorms; I then quantified the extent to which LLM-generated \nnorms correlated with those produced by humans. Data-\nsets with contextualized judgments (Trott & Bergen, 2022; \nScott et al., 2019; Trott & Bergen, 2020) were prioritized: as \nmentioned in the Introduction, these are intrinsically chal-\nlenging to scale with human participants, so establishing \nthe viability of LLM-generated judgments was of particular \ninterest. Additionally, I selected several datasets containing \njudgment types that were either known to be challenging \nfor language models, e.g., similarity judgments (Hill et al., \n2015), or seemed a priori like psycholinguistic dimensions \nthat should be challenging for an LLM, e.g., iconicity (Win-\nter et al., 2023). The primary analyses for four of the six \ndatasets considered were pre-registered on the Open Science \nFramework (individual links can be found in the Methods  \nsection). Additionally, the LLM-generated norms, along \nwith the code required to reproduce the analyses described \nbelow, can all be found on GitHub (https:// github. com/ seant \nrott/ llm_ norms).\nMethods\nDatasets Six datasets were considered. Three of these data-\nsets involved contextualized judgments. First, the Glasgow \nNorms (Scott et al., 2019) contain judgments about nine \nsemantic dimensions (concreteness, age of acquisition, \nsemantic size, valence, arousal, semantic gender, semantic \ndominance, familiarity, and imageability) for English words; \nI selected the subset of the norms containing contextualized \njudgments for 379 ambiguous words, e.g., “bow (ribbon)” \nvs. “bow (ship)”. Second, the Contextualized Sensorimo-\ntor Norms (Trott & Bergen, 2022) contain judgments about \nthe relative strength for six sensory modalities (e.g., vision, \nhearing, etc.) and five action effectors (e.g., Hand/Arm, \nFoot/Leg, etc.) for 112 English words, in four different sen-\ntential contexts each (for a total of 448 sentences). Third, \nthe RAW-C dataset (Relatedness of Ambiguous Words—in \nContext) contains judgments about the relatedness between \nthe same ambiguous English word in distinct sentential con-\ntexts (e.g., “She liked the marinated lamb” vs. “She liked the \nfriendly lamb”); RAW-C contains a total of 672 sentence \npairs (Trott & Bergen, 2021).\nI also considered three datasets involving judgments of \nindividual words. SimLex999 (Hill et al., 2015) and Sim-\nVerb3500 (Gerz et al., 2016) contain judgments of similar -\nity (as opposed to relatedness) of 999 word pairs and 3500 \nverb pairs, respectively. Finally, a recent dataset of iconicity \njudgments (the extent to which a word’s form resembles its \nmeaning) for 14,776 English words was included (Winter \net al., 2023).\nModel I used GPT-4, a state-of-the-art large language \nmodel. The primary reason for selecting GPT-4 was its \nsuperior performance on a number of natural language \nprocessing benchmarks, as well as more general metrics of \ncapability.5\nThere are two limitations to using GPT-4 as a model: first, \nthe details of model’s architecture and data are still unclear; \nand second, output can be obtained only by generating \n5 See the technical report: https:// arxiv. org/ abs/ 2303. 08774.\n6086 Behavior Research Methods (2024) 56:6082–6100\ntokens (see “Prompting”), as opposed to accessing individual \nlog probabilities—which may underestimate its performance \n(Hu & Levy, 2023). In my view, the superior performance \nof GPT-4 relative to competitors outweighed its downsides, \nparticularly because the emphasis of the current work is on \nestablishing the viability of the method, as opposed to prob-\ning the internal mechanics of the model itself.\nPrompting One benefit of modern LLMs is that they can be \n“prompted” using approaches not unlike giving instructions \nto human participants. I accessed GPT-4 using the OpenAI \nChat Completion API.6 For each item in each dataset, GPT-4 \nwas presented with instructions that matched the original \ninstructions given to human participants as closely as pos-\nsible. The temperature was set to 0, and GPT-4 was allowed \nto generate up to ten tokens in response.\nIn the case of the Glasgow Norms (Scott et al., 2019), the \ninstructions were modified slightly to specify that GPT-4 \nshould respond with a number between 1 and 7 (except for \nArousal, Valence, and Dominance, for which the scales \nranged from 1 to 9). 7 Additionally, for Age of Acquisition, \nthe pre-registered prompt asked GPT-4 to respond with the \nage at which a word was learned, whereas the original Glas-\ngow Norms map ages to a 1–7 scale; I converted GPT-4’s \nraw age responses to a 1–7 scale using the mapping provided \nin the paper’s supplementary materials (Scott et al., 2019).8\nFor all datasets, the instructions, along with the item \nin question, were presented in entirety as a string input to \nGPT-4; this string included a line-separated prompt for \nGPT-4 to indicate its answer (e.g., “Iconicity: ”, or “Relat-\nedness: ”). For datasets involving multiple semantic dimen-\nsions each item, the item was presented multiple times to \nGPT-4 (as independent “trials”) with modified instructions \n(e.g., according to the semantic dimension in question). \nThis approach was chosen to avoid confounding responses \nbetween dimensions or between items.\nThe prompting method (and primary analyses) were pre-\nregistered for: the iconicity norms (https:// osf. io/ wn9pv), \nSimVerb3500 ( https:// osf. io/ dtekj), the contextualized \nsensorimotor norms (https:// osf. io/ 2e3vk), and the Glasgow \nNorms (https:// osf. io/ 3jvg6).\nProcessing The output of the prompting procedure (a .txt  \nfile) was converted to a .csv file with the appropriate col-\numn headers (e.g., “Word”, “Sentence”, “Visual Strength”). \nAdditionally, GPT-4’s response (originally a string, e.g., \n“1”) was converted to a number (e.g., 1). In three cases, no \nnumber could be identified in GPT-4’s response: in each \ncase, GPT-4’s response indicated a refusal to answer the \nquestion. As decided in the pre-registration, those responses \nwere excluded.\nResults\nPrimary analysis: Assessing GPT‑4’s performance The pri-\nmary question was whether GPT-4’s ratings significantly \nco-varied with human ratings. To measure degree-of-fit, \nI calculated the Spearman’s rank correlation coefficient \nbetween GPT-4’s ratings and the human ratings; for data -\nsets containing multiple dimensions (Trott & Bergen, 2022; \nScott et al., 2019), I calculated rho  for each dimension. In \nall cases, rho was positive and significantly above zero (p < \n.001), demonstrating that GPT-4’s rating’s captured relevant \nvariance about the human ratings. Degree-of-fit ranged from \na low of 0.39 (for semantic dominance) to a high of 0.86 \n(for similarity). For contextualized datasets, the highest rho \nachieved was 0.82 (for contextualized relatedness). The full \nset of correlation coefficients can be found in Table 1.\nAnother key question was how GPT-4’s performance com-\npared to human inter-annotator agreement for that dataset. \nThis is important as a baseline: if human agreement is low, \nthen it is unreasonable to expect GPT-4’s performance to \nbe very high. Here, the most comparable measure was \nleave-one-out inter-annotator agreement, 9 which calculates \nthe correlation between each human’s ratings and the mean \nratings of all other human participants (excluding the par -\nticipant in question). This information was available only \nfor select datasets (Trott & Bergen, 2021; Trott & Bergen, \n2022; Hill et al., 2015; Gerz et al., 2016; Winter et al., \n2023); additionally, for the contextualized sensorimotor \nnorms (Trott & Bergen, 2022), the published inter-annota-\ntor agreement measure was calculated aggregating across \nthe entire set of action norms (five in total) and perception \n6 https:// openai. com/ blog/ openai- api\n7 Note that this modification was simply to make it clear to the model \nthat a number was expected; in the case of a human experiment, this \nwould not have been necessary, as the sliding scale would have been \nvisible to human participants.\n8 Both changes were implemented to make the LLM’s goal more \nstraightforward. In the first case, the prompt specified that a num-\nber (as opposed to a verbal description, e.g., “very concrete”) \nwas required. In the case of Age of Acquisition, the original scale \nincluded ranges that I predicted might be unintuitive (e.g., a “7” \nreferred to any age above 13), so this was the pre-registered prompt \nI decided on.\n9 Leave-one-out is more comparable in that GPT-4 can be construed \nas a single “participant”, whose ratings we are comparing to the mean \nratings of all other participants. It is also a more “generous” measure \nto human participants in that it tends to lead to a higher estimate than \nthe average pairwise correlation between human participants. The \nquestion is thus where the quality of GPT-4’s data falls in the distri-\nbution of human participants.\n6087Behavior Research Methods (2024) 56:6082–6100 \nnorms (six in total) but not each individual dimension. \nFor the iconicity norms (Winter et al., 2023), I calculated \nleave-one-out inter-annotator agreement using the full raw \ndata publicly available online, after applying the exclusion \ncriteria that were possible given the data contents (i.e., all \nbut the attention checks).\nFigure  1 below thus compares rho  for GPT-4 to the \naverage inter-annotator agreement among datasets for \nwhich it was available. Notably, in all but one dataset \n(SimVerb3500), GPT-4’s correlation with human ratings \nwas at least as high as average inter-annotator agreement. \nPut another way: GPT-4 was more correlated with the \nTable 1   Spearman’s rank correlation coefficients for each semantic dimension of each dataset\nDataset Dimension Contextualized? Spearman’s rho\nIconicity norms (Winter et al., 2023) Iconicity No 0.59\nSimLex999 (Hill et al., 2015) Similarity No 0.86\nSimVerb3500 (Gerz et al., 2016 Similarity No 0.81\nRAW-C (Trott & Bergen, 2021) Relatedness Yes 0.82\nCS Norms: Perception (Trott & Bergen, 2022) All perception dimensions Yes 0.84\nCS Norms: Perception (Trott & Bergen, 2022) Interoception Yes 0.55\nCS Norms: Perception (Trott & Bergen, 2022) Taste Yes 0.63\nCS Norms: Perception (Trott & Bergen, 2022) Hearing Yes 0.66\nCS Norms: Perception (Trott & Bergen, 2022) Vision Yes 0.66\nCS Norms: Perception (Trott & Bergen, 2022) Olfaction Yes 0.71\nCS Norms: Perception (Trott & Bergen, 2022) Touch Yes 0.75\nCS Norms: Action (Trott & Bergen, 2022) All action dimensions Yes 0.64\nCS Norms: Action (Trott & Bergen, 2022) Head Yes 0.45\nCS Norms: Action (Trott & Bergen, 2022) Mouth/Throat Yes 0.56\nCS Norms: Action (Trott & Bergen, 2022) Foot/Leg Yes 0.56\nCS Norms: Action (Trott & Bergen, 2022) Torso Yes 0.58\nCS Norms: Action (Trott & Bergen, 2022) Hand/Arm Yes 0.64\nGlasgow Norms (Scott et al., 2019) Valence Yes 0.76\nGlasgow Norms (Scott et al., 2019) Arousal Yes 0.66\nGlasgow Norms (Scott et al., 2019) Concreteness Yes 0.81\nGlasgow Norms (Scott et al., 2019) Familiarity Yes 0.71\nGlasgow Norms (Scott et al., 2019) Imageability Yes 0.74\nGlasgow Norms (Scott et al., 2019) Dominance Yes 0.39\nGlasgow Norms (Scott et al., 2019) AoA Yes 0.72\nGlasgow Norms (Scott et al., 2019) Size Yes 0.69\nGlasgow Norms (Scott et al., 2019) Gender Yes 0.47\nFig. 1   Spearman’s rho between GPT-4’s ratings and the human rat-\nings for datasets with human inter-annotator agreement available. \nHuman inter-annotator agreement is visualized in blue. For each data-\nset but SimVerb3500, GPT-4’s ratings were at least as correlated with \nthe gold standard as the average inter-annotator agreement\n6088 Behavior Research Methods (2024) 56:6082–6100\naverage human rating (the population parameter) than the \naverage human was. (Explanations for this phenomenon \nwill be explored in the General discussion.)\nAnother way to assess the validity of these ratings is \nto ask about their relationship to independently collected \njudgments for the same measure. For example, Winter \net al. (2023) compared their iconicity ratings to previously \npublished judgments for a subset of the same words (Perl -\nman et al., 2015), and obtained Pearson’s correlation coef-\nficients between 0.48 (for auditory stimuli) and 0.55 (for \nwritten stimuli). By comparison, the Pearson’s correlation \nbetween GPT-4’s ratings and the Winter et al. (2023) rat-\nings was r  = 0.63. GPT-4’s ratings were also more corre-\nlated with the Perlman et al. (2015) ratings, for both audi-\ntory stimuli (r = 0.53, p < .001) and written stimuli (r =  \n0.58, p < .001). This is further evidence for the reliability \nof the GPT-4 ratings. (Additionally, data contamination \nfrom previously published datasets is unlikely to be the \nexplanation here: see Supplementary Analysis 1 for more \ndetails.)\nFocusing specifically on the Glasgow Norms (Scott et al., \n2019), GPT-4’s performance ranged considerably, from rela-\ntively low correlations for contextualized semantic domi-\nnance (rho = 0.39) to very high correlation for contextual-\nized concreteness (rho = 0.81). The relationship between \nGPT-4’s ratings and human ratings for each dimension of the \nGlasgow Norms is depicted in Fig.  2. Because inter-anno-\ntator agreement ratings were not available for the Glasgow \nNorms, it is more challenging to assess whether this range \nmirrors agreement observed for humans.\nDoes GPT‑4 make systematic errors?  GPT-4’s ratings cor -\nrelate with human ratings, but not perfectly. Is it possible \nto identify systematic sources of divergence, or are GPT-\n4’s errors randomly distributed? I attempted to address \nthis question using available covariates for SimLex999, \nFig. 2   Relationship between GPT-4’s ratings and human ratings for \neach dimension of the Glasgow Norms. Note that the rating scale \nranged between 1 and 7 for six of the nine norms, and between 1 and \n9 for Arousal, Valence, and Dominance. Highest performance was \nachieved for Concreteness (rho  = 0.81), and the lowest correlation \nwas for dominance (rho = 0.39). GPT-4 ratings were significantly and \npositively correlated with human ratings for all dimensions\n6089Behavior Research Methods (2024) 56:6082–6100 \nSimVerb3500, and RAW-C. These analyses were motivated \nby past work (Utsumi, 2020; Dou et al., 2018; Trott & Ber-\ngen, 2023), but also exploratory in nature. In each case, I \nquantified the absolute error between GPT-4’s ratings and \nthe human ratings.\nSimLex999. The SimLex999 dataset (Hill et al., 2015) \ncontains information about both the part-of-speech of \nthe two words being compared as well as their concrete-\nness (binned by quartile). A linear regression predicting \nabsolute error, with both factors as predictors, suggested \nindependent effect of each: word pairs in higher concrete \nquartiles were associated with higher error [β  = 0.15, SE \n= 0.04, p < .001]; additionally, verbs were also associated \nwith higher error than adjectives [β  = 0.55, SE = 0.12, p \n< .001]. The former effect is consistent with convergent \nevidence that distributional information is better at pre-\ndicting semantic properties of abstract words than concrete \nwords (Utsumi, 2020; Kiros et al., 2018). These effects, \nalong with the analysis of errors for SimVerb3500, are \ndisplayed in Fig.  3. Qualitative inspection revealed that \nsome of the highest divergences were for word pairs that \nformed semantic complements in some way, e.g., “wife/\nhusband”, “south/north”, and “groom/bride”.\nSimVerb3500. The SimVerb3500 dataset (Gerz et al., \n2016) is annotated for the type of relation between the verbs \nin question: synonyms, co-hyponyms, hyper/hyponyms, \nantonyms, or none. GPT-4 achieved high correlations over-\nall, but performance was considerably weaker for antonyms \n(see Fig.  3). Weaker performance for antonyms has been \nobserved for other work on estimating similarity using distri-\nbutional information (Dou et al., 2018). Specifically, GPT-4 \ntended to rate antonym pairs as more similar on average than \nhumans did (mean difference = 2.53, SD = 1.64).\nAs with SimLex999, qualitative inspection revealed that \na number of GPT-4’s ratings diverge not only for antonyms, \nbut for other verb pairs that form semantic “complements” \n(e.g., “incline/decline”, “win/defeat”, “reap/sow”, “push/\ntug”, “die/kill”, “multiply/divide”, and “spring/fall”). In \neach of these cases, GPT-4 rated the pair as more similar \nthan humans did.\nRAW-C. GPT-4’s performance on the RAW-C dataset was \nhigher (rho = 0.82) than both human inter-annotator agree-\nment (rho = 0.79) and past language models tested (rho  = \n0.58). However, motivated by past work (Trott & Bergen, \n2021; Trott & Bergen, 2023), I analyzed whether absolute \nerrors were larger for contexts in which the meanings of the \nambiguous word were distinct (e.g., “brain cell” vs. “prison \ncell’) than contexts in which the meanings were the same \n(e.g., “brain cell” vs. “skin cell”). Indeed, a linear regression \ndemonstrated that errors were smaller on average for Same \nSense contexts [β = – 0.53, SE = 0.04, p < .001].\nI then asked whether human relatedness ratings varied \nsignificantly as a function of Same vs. Different Sense, inde-\npendent of the effect of GPT-4’s relatedness ratings. Also \nconsistent with past work (Trott & Bergen, 2023; Trott & \nBergen, 2021), a linear regression predicting human relat-\nedness suggested independent effects of each factor: GPT-4 \nrelatedness [β = 0.95, SE = 0.05, p < .001] and Same Sense \n[β = 0.97, SE = 0.28, p  < .001]. This suggests that GPT-\n4’s ratings fail to fully account for a psychological effect of \nwhether two contexts convey the same or different meanings \n(Trott & Bergen, 2023); however, even within Same Sense \npairs, GPT-4’s judgments significantly predicted human \nrelatedness judgments [β = 0.34, SE = 0.06, p < .001].\nFinally, a linear regression including an interaction \nbetween GPT-4 rating and Same Sense (along with main \neffects of each factor) revealed significant effects of each \nFig. 3    Absolute differences between GPT-4’s similarity ratings and human similarity ratings for SimVerb3500 and SimLex999. For Sim-\nVerb3500, errors were largest for Antonyms; for SimLex999, errors were larger for more concrete word pairs than more abstract word pairs\n6090 Behavior Research Methods (2024) 56:6082–6100\nterm: GPT-4 relatedness [β  = 1.19, SE = 0.06, p  < .001], \nSame Sense [β = 3.48, SE = 0.34, p  < .001], and the inter-\naction between GPT-4 relatedness and Same Sense [β  = \n-0.85, SE = 0.11, p  < .001]. The intercept is – 1.24, i.e., \nthe estimated human relatedness for a different sense pair \nthat received a rating of 0 from GPT-4 would be – 1.24. \nPut together, these effects can be interpreted as follows: for \neach 1-unit increase in GPT-4 relatedness ratings, human \njudgments of relatedness increase by approximately 1.19; \nadditionally, holding GPT-4 judgments of relatedness con-\nstant, human judgments about the relatedness of Same Sense \npairs are 3.48 higher on average; and finally, the interaction \ntempers this same-sense effect by a factor of – .85. More \nconcretely: a GPT-4 rating of 3 for a same sense pair should \nyield a human relatedness judgment of approximately 3.3, \nwhile the same rating for a different sense pair should yield \na human relatedness judgment of approximately 2.33.\nThis result is also consistent with qualitative inspection \nof the top 20 items with the highest error. In each case, \nGPT-4 systematically overestimated relatedness judgments \nfor contexts conveying different meanings (e.g., “red cape” \nvs. “rocky cape”, or “toast the strudel” vs. “toast the host”).\nSubstitution analysis Another way to evaluate the validity \n(and utility) of LLM-generated norms is to ask whether, and \nto what extent, they can be used as substitutes for human-\ngenerated norms in a statistical analysis. That is, if an \nanalysis relied on LLM-generated norms instead of human-\ngenerated norms, how much would the results change? For \nexample, a change in the sign of a coefficient estimate would \nbe evidence that LLM-generated norms might lead to quali-\ntatively different inferences; a small change in the magnitude \nof a coefficient estimate could be concerning, but perhaps \nless so than a change in its sign.\nIconicity. Winter et al. (2023) report the results of an \nanalysis predicting human iconicity ratings as a func-\ntion of multiple predictors: sensory experience (Juhasz \n& Yap, 2013) humor (Engelthaler & Hills, 2018), log \nletter frequency (Dingemanse & Thompson, 2020), con -\ncreteness (Brysbaert et al., 2014a, b), log word frequency \n(Brysbaert & New, 2009), average radius co-occurrence, \nor “ARC” 10 (Shaoul & Westbury, 2010), age of acquisi-\ntion (Kuperman et al., 2012), and part-of-speech. I rep-\nlicated this analysis of human iconicity ratings using the \ndata provided by the authors; 11 as in the original article, \nall predictors were z -scored. Then, I conducted an identi-\ncal analysis using LLM-generated iconicity as the target \nvariable.\nThe key question was whether coefficient estimates for a \nmodel predicting human-generated iconicity ratings would \nbe different in sign or magnitude from those in a model \npredicting LLM-generated iconicity ratings. As depicted in \nFig. 4, none of the coefficient estimates switched their sign  \n(i.e., no predictors had negative coefficients for one measure \nof iconicity, and positive coefficients for the other meas-\nure). Following the authors’ convention, part-of-speech is \nnot included in the figure.\nFig. 4  Coefficient values for statistical models predicting iconicity \n(left) and relatedness values (right). Iconicity and relatedness values \nwere generated using either LLMs (red circles) or humans (blue tri-\nangles). Error bars represent two standard errors. As depicted, none \nof the coefficients switched their direction when LLM-generated \nnorms were substituted for the dependent variable; however, select \npredictors did change magnitude depending on whether the depend-\nent variable relied on LLM-generated or human-generated norms\n10 ARC is a measure of semantic neighborhood density.\n11 https:// osf. io/ qvw6u/\n6091Behavior Research Methods (2024) 56:6082–6100 \nFollowing past work (Clogg et al., 1995; Paternoster \net al., 1998), differences in magnitude were assessed using \na two-sided z-test:12\nUsing a standard significance threshold of p  < .05, three \npredictors were found to have significantly difference coef-\nficient estimates across models: log word frequency (z  = \n4.24, p < .001), age of acquisition (z = 4.67, p < .001), and \nsensory experience (z  = – 2.93, p  < .003). The remaining \nfive predictors had coefficient estimates that were not sig-\nnificantly different (p  > .1) across models. In other words, \nfive of the predictors had stable coefficients regardless of \nwhether they were used to predict human-generated iconicity \nratings or LLM-generated iconicity ratings.\nSensorimotor distance. Following Wingfield & Con-\nnell (2022), Trott & Bergen (2022) used the contextualized \nsensorimotor norms to construct a measure of contextual-\nized sensorimotor distance: the cosine distance between \nthe 11-dimensional sensorimotor norms for each context in \nwhich an ambiguous word appeared. They demonstrated that \nthis measure was predictive of relatedness judgments for \nthose contexts (Trott & Bergen, 2021) above and beyond \nother measures, such as whether or not the contexts corre-\nsponded to the same sense, the kind of ambiguity expressed \n(homonymy vs. polysemy), and the cosine distance between \nBERT’s contextualized embeddings for those words.\nTo replicate this analysis, I first calculated contextual-\nized sensorimotor distance using both the human-generated \nnorms and the LLM-generated norms. 13 These measures \nwere relatively well-correlated (rho = 0.58, p < .001). I then \nbuilt two different regression models predicting human judg-\nments of contextual relatedness. Each model contained the \nfollowing factors: Cosine Distance (measured by BERT), \nSense Boundary (Same vs. Different Sense), Ambiguity \nz = /u1D6FD1 − /u1D6FD2\n/uni221A.s1\nSE 2\n1 − SE 2\n2\nType (Homonymy vs. Polysemy), and an interaction between \nthe latter two factors. The models differed in which meas-\nure of contextualized sensorimotor distance they used (i.e., \nrelying on the human-generated vs. LLM-generated norms).\nFirst, both models achieved comparable fits (R 2\nLLM = \n0.718, R2\nhuman = 0.719). The coefficient for contextualized \nsensorimotor distance was significantly negative for the \nmodel relying on LLM-generated norms [β  = – 2.36, SE = \n0.341, p < .001] and the model relying on human-generated \nnorms [β = – 3.42, SE = 0.483, p < .001]. A z-test compar-\ning these coefficient values was approaching significance (z \n= – 1.8, p  = 0.07); this is consistent with a small but real \ndifference in magnitudes between the estimates, but could \nalso be consistent with sampling error.\nContextual Relatedness. I replicated the analysis above \nfocusing on the contrast between LLM-generated and \nhuman-generated relatedness. Here, I constructed two linear \nregression models with identical predictors (BERT Distance, \nSense Boundary, Ambiguity Type, an interaction between \nSense Boundary and Ambiguity Type, and Sensorimotor \nDistance as measured by humans); the key difference was \nwhether the dependent variable was LLM-generated relat-\nedness or human-generated relatedness, i.e., the original \nRAW-C norms (Trott & Bergen, 2021).\nAs depicted in Fig.  4, none of the coefficients for the \npredictors changed sign across the models. However, a z-test \ndid reveal significant changes in the magnitude of the coef-\nficients for four of the five predictors: Sense Boundary (z = \n5.03, p < .001), Ambiguity Type (z = 7.54, p < .001), BERT \nDistance (z = – 3.42, p < 0.001), and Sensorimotor Distance \n(z = – 2.05, p = .04). Notably, the effect of each predictor \nwas larger when predicting human-generated relatedness; \nin the case of Sense Boundary and Ambiguity Type, this is \nconsistent with past work (Trott & Bergen, 2023) suggesting \nthat human semantic representations are influenced more by \ncategory boundaries (e.g., between distinct meanings of a \nword) than LLM representations.\nGlasgow Norms. For the Glasgow Norms, I asked to what \nextent human-generated norms and LLM-generated norms \nreflected analogous semantic structure, i.e., whether the cor-\nrelations between each of the nine dimensions (for human-\ngenerated norms) could be accurately reconstructed from \nthe LLM-generated norms. The logic behind this approach \nwas similar to representational similarity analysis, or “RSA” \n(Kriegeskorte et al., 2008). First, I constructed a correla-\ntion matrix between all nine dimensions using the human \nnorms (see Fig.  5a). This reveals which psycholinguistic \ndimensions are positively correlated (e.g., imageability and \nconcreteness) and which are negatively correlated (e.g., age \nof acquisition and familiarity), and to what degree. I then \nconstructed an analogous matrix using the LLM-generated \nnorms for each dimension (see Fig. 5b).\n12 The primary reason for running a z-test here, rather than a t test, \nwas based on the precedent from past work (Clogg et al., 1995). In \npractice, the key difference is which sampling distribution the result-\ning test statistic is compared to, i.e., either the standard normal distri-\nbution (in the case of a z-test) or a t-distribution with the appropriate \ndegrees of freedom (in the case of a t test). Given that t-distributions \nhave heavier tails, using a t  test would generally work against find-\ning significant differences in magnitude between models fit using the \nLLM-generated vs. human-generated norms. P values were calculated \nagain using a t test (where degrees of freedom were estimated as the \ndifference between the number of observations and the number of \ncoefficients in the model minus one), and did not differ qualitatively \nfrom the results obtained using a standard normal distribution.\n13 As in the original paper, this measure of contextualized sensori-\nmotor distance included all 11 dimensions (i.e., all the perception \nnorms and all the action norms).\n6092 Behavior Research Methods (2024) 56:6082–6100\nTo test whether these matrices were more similar than \none would expect by chance, I used a Mantel test of matrix \nsimilarity. A Mantel test calculates a correlation coefficient \n(e.g., Pearson’s r) between the off-diagonal cells across two \nmatrices (the diagonals are excluded because they would \nartificially inflate the correlation value). This correlation \ncoefficient is then compared to the distribution of correla-\ntion coefficients that result from randomly permuting one of \nthe matrices and running the same procedure. Using 1000 \nrandom permutations, I found that the human correlation \nmatrix was significantly correlated with the LLM-generated \ncorrelation matrix (r = 0.65, p < .001). That is, in addition to \ncorrelating with the original dimensions (see Fig. 2), the cor-\nrelations between LLM-generated dimensions capture some \nof the structure of the original human dimensions.\nThat said, there were several notable cases in which GPT-\n4’s correlations departed substantially from human corre-\nlations. For example, arousal and valence were positively \ncorrelated in the human norms (r = 0.31), but (weakly) neg-\natively correlated in the LLM-generated norms (r  = – .16). \nIn other cases, the coefficients had the same sign but var -\nied in magnitude: for example, semantic size and semantic \ndominance were very weakly correlated in the human norms \n(r = 0.09, p < .01), whereas this correlation was somewhat \nstronger in the LLM-generated norms (r = 0.38, p < .001).\nGeneral discussion\nThe primary question of the current work was whether \nLLMs could be used to augment the creation of large-scale \npsycholinguistic datasets, particularly those involving con-\ntextualized judgments. Focusing on six datasets (compris -\ning 24 semantic dimensions total), I approached this ques-\ntion in the following way. First, in a series of pre-registered \nanalyses, I found that LLM-generated norms were positively \ncorrelated with human judgments across all 24 dimensions \nof all datasets. Degree-of-fit ranged considerably across \ndimensions (see Table  1); however, where a baseline of \nhuman inter-annotator agreement was available, I found \nthat LLM-generated norms approached—and in five cases \nexceeded—this baseline.\nSecond, for select datasets, I conducted exploratory anal-\nyses investigating where LLM-generated norms diverged \nfrom human judgments. For similarity judgments, diver -\ngences were largest for concrete words (SimLex999) and \nantonyms (SimVerb3500); both findings were consistent \nwith past work (Utsumi, 2020; Dou et al., 2018). For con-\ntextualized relatedness judgments, divergences were most \npronounced at sense boundaries , also consistent with past \nwork (Trott & Bergen, 2023). Finally, I performed a novel \nsubstitution analysis, which asked whether LLM-generated \nnorms could be substituted for human-generated norms as \neither an independent or dependent variable in a statistical \nmodeling framework. In each of the substitutions performed, \nusing LLM-generated norms did not result in changes of \nthe direction (i.e., sign) for any coefficients across models \n(see Fig. 4). However, there were significant changes in the \nmagnitude of coefficients for select predictors, such as age of \nacquisition and log word frequency (for predicting human-\ngenerated vs. LLM-generated iconicity norms).\nAre LLM‑generated norms viable?\nThe question of viability depends on both theoretical and \npractical factors. First, how successfully can GPT-4 repro-\nduce existing human judgments? Second, how easy is GPT-4 \nto use in this way, and how would that compare to collecting \nhuman judgments at scale? Third, how expensive is GPT-4 \nFig. 5   Correlation matrices for the nine Glasgow dimensions using \nhuman-generated norms (a) and LLM-generated norms (b). c The \ndifference between these matrices (GPT-4 correlation – human cor -\nrelation): a positive value means the dimensions were more positively \ncorrelated using GPT-4 norms, whereas a negative value means the \ndimensions were more positively correlated using human norms\n6093Behavior Research Methods (2024) 56:6082–6100 \nto use, and how does that compare to collecting human \njudgments at scale? I consider these factors in the sections \nbelow. Note that this discussion focuses on written English \nstimuli; other issues (e.g., external validity) are explored in \nthe “Limitations” section.\nEmpirical success Overall, these results are promising. \nGPT-4 achieved comparable (or superior) performance \nwith human annotators in five of the six datasets where \ninter-annotator agreement was available. Additionally, the \nfact that LLM-generated norms could be substituted for \nhuman norms in a statistical model without changing the \nsign of any coefficients in the model suggests that these \nnorms could be used to help drive theoretical inferences \nabout how psycholinguistic variables relate to one another. \nFurther, GPT-4 was presented with instructions that were \nidentical (or nearly identical) to those presented to human \nparticipants. Thus, these results may reflect a lower-bound  \non GPT-4’s ability to produce aligned judgments. Better \nresults could be obtained with alternative prompting meth-\nods (Reynolds & McDonell, 2021), well-chosen examples \n(Brown et al., 2020), or other ways of extracting LLM out-\nput (Hu & Levy, 2023).\nOf course, the correlation with human norms was \nfar from perfect, particularly for dimensions such as \nsemantic dominance (though it is unclear what human \ninter-annotator agreement was for these dimensions). \nThis raises an important question about the degree-of-\nfit required to augment datasets with LLM-generated \nnorms: how successful must an LLM be—relative to a \nhuman baseline—to be used either in norming stimuli or \nincreasing the size of a dataset? This question depends \non a researcher’s goals and on the degree of precision \nrequired. Notably, LLM-generated norms performed \nmuch better for some dimensions than others. If this \ngradient in performance is systematic, then perhaps \nresearchers could rely more on LLMs for those specific \ndimensions (e.g., contextualized concreteness), and focus \ntheir energies on collecting human data for other dimen-\nsions (e.g., semantic dominance); again, in each case, \ncomparison to a human baseline would be essential.\nFurther, the error analysis suggests that LLMs perform \nbetter for some kinds of words (e.g., abstract words) than \nothers (e.g., concrete words and antonyms); these findings \nare consistent with past work (Utsumi, 2020; Trott & Ber-\ngen, 2023), and, if they are replicable, suggest another path \nforward—perhaps LLM-generated norms could be relied \non more for certain kinds of items or relations than others. \nFinally, substitution analyses could be performed to quantify \nthe divergence in theoretical inferences one would obtain \nwhen relying on LLM-generated norms in place of human-\ngenerated norms.\nEase of use One benefit of modern foundational LLMs is \nthat users do not need to train their own model; in the case \nof GPT-4, users can access the model and produce output \nusing either a web interface or a Python API. Intuitively, \nthis seems easier to use than older models, though it is more \nchallenging to compare to the ease of collecting judgments \nfrom human participants (e.g., over Amazon Mechanical \nTurk). Relying on a model like GPT-4 likely requires some \nPython programming knowledge, as well as basic famili-\narity with how LLMs work. On the other hand, collecting \njudgments online requires designing a survey interface (e.g., \nusing Qualtrics) and addressing difficult issues like partici-\npant exclusion (Webb & Tangney, 2022). Ultimately, the \nquestion of which source is easier is ripe for empirical inves-\ntigation. Researchers could combine qualitative and quanti-\ntative approaches to conduct a usability study and identify \nsignificant bottlenecks in each approach.\nCost Running these analyses required access to the OpenAI \nAPI. According to OpenAI, 14 GPT-4 costs $0.03/1000 for \nprompt tokens and $0.06/1000 for sampled tokens. Based \non the number of tokens in the instructions and prompts for \neach dimension of each task,15 this results in a total prompt \ncost of $300.31. For token generation, I allowed GPT-4 to \ngenerate up to ten tokens for each judgment; at a rate of \n$0.06/1000 per generated token, this amounted to $19.63. \nAltogether, using GPT-4 to collect the total set of judgments \ncost approximately $319.94. (Of course, it is possible that \nthe cost of GPT-4 will decrease in the future, or that freely \navailable models will become more powerful; both possi-\nbilities would make LLM norms comparative cheaper, and \nthus this estimate should be considered a conservative, pes-\nsimistic one regarding LLM costs. See also Supplementary \nAnalysis 2 for an analysis using a smaller, cheaper model.)\nEstimating a cost for human-generated norms is more \nchallenging, and relies on several assumptions. The minimal \ncost could be estimated assuming zero exclusions, a single \njudgment per word, a payment of federal minimum wage \n($7.25 per hour), and relatively fast time per judgment (e.g., \n5 s): at this rate, 32,000 judgments would take approximately \n44.44 h, which would cost $329.19—about $10 more than \nthe estimate for the LLM-generated norms.\nHowever, this estimate is optimistic. First, a researcher \nwould likely need to exclude at least some judgments. \nRecent work (Webb & Tangney, 2022) estimated an exclu-\nsion rate for Amazon Mechanical Turk as high as over 90%; \n14 https:// help. openai. com/ en/ artic les/ 71279 56- how- much- does- gpt-\n4- cost\n15 The number of tokens was estimated using tiktoken, a Python \nlibrary (https:// github. com/ openai/ tikto ken).\n6094 Behavior Research Methods (2024) 56:6082–6100\nexclusion rates for the original datasets considered here \nranged from 11% (Winter et al., 2023) to 25% (Trott & Ber-\ngen, 2022). Second, the amount of time required to respond \nto an item will depend on the judgment in question; for sin-\ngle words, 5 s is reasonable (e.g., the average response time \nin Winter et al., 2023 was under 5 s), but a longer sentence \nor passage will naturally take longer to read and produce a \njudgment about.\nAdditionally, a single judgment per item is unusual. For \nexample, Winter et al. (2023) and Trott & Bergen (2022) col-\nlected at least ten judgments per item. The number required \nwill depend on the relative precision  of any given human \njudgment. Recall that for a number of datasets, GPT-4’s \ncorrelation with the human mean was higher than the aver -\nage inter-annotator agreement (see Fig. 1). Thus, a relevant \nquestion is how many human judgments are needed for a \ngiven judgment type to attain the same degree of reliability \nas LLM-generated judgments. Future work could address \nthis question empirically: if this ratio is higher than 1 (i.e., a \nsingle human judgment is, on average, less reliable than an \nLLM-generated judgment), then more than a single human \njudgment would be required to attain comparable reliability, \ntherefore raising the cost of human-collected data. If the \nratio is lower than 1 (single human judgments are more reli-\nable than LLM-generated judgments), then LLM-generated \njudgments would not necessarily be a useful or cost-effective \ncontribution. This last question is also interesting from a \ntheoretical perspective, as it connects to the notion of the \n“wisdom of the crowd” (Stroop, 1932). LLMs are trained \non many more word tokens, from more language producers, \nthan any given human observes; for certain tasks, then, it \nis possible that their output represents the average guess of \nmultiple language producers (Dillion et al., 2023).\nUltimately, better characterization of the factors described \nhere—empirical success, ease of use, and financial cost—\nwould allow researchers to make informed cost/benefit anal-\nyses when determining how to create their normed stimuli. \nAdditional questions about viability are explored in the sec-\ntion below.\nLimitations and future work\nThe work described here has various number of limitations, \nwhich also raise questions and interesting directions for \nfuture work.\nLimited generalizability  One key limitation is that most \nlarge language models like GPT-4 are trained primarily on \nwritten English text produced by a relatively biased sub -\nset of English speakers (Bender et al., 2021; Chang & Ber -\ngen, 2023). Because of this, the output produced by most \nLLMs are limited to English; within English-speaking \ncommunities, they also under-represent the perspectives of \ntraditionally marginalized groups (Groenwold et al., 2020). \nFurther, because LLMs are trained on written text, they fail \nto capture important variation in spoken language, and can-\nnot be used to model judgments about signed languages \nat all (Vinson et al., 2008 ). In addition to concerns about \nperpetuating bias or producing toxic speech (Bender et al., \n2021), this raises a concern about the external validity of \nLLM-generated samples.\nOf course, concerns about external validity are not unique \nto LLMs. Experimental samples in psychology and cogni-\ntive science have traditionally over-represented so-called \n“WEIRD” (Western, Educated, Industrialized, Rich, and \nDemocratic) populations (Henrich et al., 2010). Addi-\ntionally, English specifically has often been treated as the \n“default” language of study (Bender, 2009; Anand et al., \n2020; Blasi et al., 2022). In terms of English psycholin-\nguistic norms in particular—the subject of this paper—it \nis unclear to what extent the samples used to generate these \nnorms are representative of the broader English-speaking \npopulation.\nAltogether, this suggests that researchers should apply \nexercise caution when making claims about the generaliz-\nability of findings obtained on LLM-generated samples—\njust as they should for samples obtained from WEIRD \npopulations.\nLimited understanding LLMs lack both embodied and \ninteractional experience, leading many to question whether \nthey exhibit true “understanding” of human language \n(Bender & Koller, 2020; Mollo & Millière, 2023; Mitchell \n& Krakauer, 2023). Additionally, LLMs may lack “common \nsense” knowledge (Forbes et al., 2019), suggesting that cer-\ntain aspects of human knowledge and reasoning cannot be \nlearned from linguistic input alone. However, others have \nargued that LLMs do acquire relevant aspects of linguistic \nmeaning (Piantadosi & Hill, 2022) and even reasoning abil-\nity (Manning, 2022). Empirically, evidence is mixed: LLMs \ndo perform surprisingly well on select tasks requiring lin-\nguistic or even social reasoning (Hu et al., 2022; Trott et al., \n2022), though typically under-perform human benchmarks \n(Jones et al., 2022). As Pavlick (2023) notes, this debate is \nfar from resolved. Ultimately, a resolution will hinge not \nonly on a priori arguments about what could in principle \nbe learned from language, but empirical investigations into \nboth how LLMs behave and which representations guide \nthat behavior.\nIn terms of the current work, one central question is \nwhether and what LLMs understand about the words and \nconstructs for which they are producing norms. Empirically, \nthe results presented here demonstrate that LLMs perform \nwell overall, and further, that performance is better for some \n6095Behavior Research Methods (2024) 56:6082–6100 \nconstructs (e.g., concreteness) than others (e.g., dominance), \nand that their judgments are also dependent on the types \nof words or relations in question (e.g., LLMs perform bet-\nter for synonyms than antonyms). One interpretation of \nthese results is that LLMs thus “understand” these words \nand constructs moderately well, but better in some cases \nthan others. However, because “understanding” remains a \ncontested concept, consensus on that interpretation may be \nunlikely. As Pavlick (2023) notes, addressing this question is \nlikely to require considerable empirical investigation—and \nalso, crucially, a more complete theory of exactly what and \nhow humans understand language. This issue is explored at \ngreater length in the section below entitled “Which types of \njudgments can LLMs make?”\nWhich types of judgments can LLMs make? As noted in the \nIntroduction, there is growing interest in using LLMs to \nmake a variety of judgments about written stimuli (Törn -\nberg, 2023, Zhu et al., 2023; Gilardi et al., 2023), including \njudgments about the morality of different situations (Dillion \net al., 2023). Should LLMs be relied upon more for certain \nkinds of judgments than others?\nIt seems uncontroversial that LLMs could produce reli-\nable judgments about an English word’s part-of-speech; it \nis less clear whether LLMs can (or should) be relied upon \nfor judgments about the ethics or morality of a written \nscenario. This question also connects to the issue of rep-\nresentativeness: according to recent work, LLM-generated \nmoral evaluations correlate well with norms generated by \nEnglish-speaking participants (Dillion et al., 2023), but \nless well with judgments produced by speakers around the \nworld (Ramezani & Xu, 2023); this is not surprising, given \nthat moral judgments vary considerably by culture (Henrich \net al., 2010; Awad et al., 2020). Thus, the issue appears to \nencapsulate both construct validity (whether LLM-generated \nnorms are valid operationalizations of the underlying theo-\nretical construct) and external validity (whether those norms \nreflect the population of interest). Because external validity \nhas already been discussed above, I focus here on construct \nvalidity.\nThere are at least two approaches to answering this ques-\ntion, which relate to different dimensions of construct valid-\nity. One is empirical and atheoretical: LLMs can be relied \nupon to the extent that their judgments correlate with human \njudgments. This echoes the “duck test” position described in \nother work (Trott et al., 2022), i.e., if an LLM produces judg-\nments that correspond to human-generated judgments, then \nthe LLM is a reliable source of those judgments. Depending \non the empirical analysis in question, this could be analo -\ngized to establishing the reliability of a measure (e.g., inter-\nannotator agreement; see Fig. 1) or establishing the predic-\ntive validity of a measure (e.g., its ability to predict outcomes \nof interest; see Fig.  4 and the corresponding substitution \nanalysis). This empirical approach has the advantage of \noffering a specific, measurable criterion for determining \nwhether or not LLM-generated norms are suitable. However, \na disadvantage is that it does not contend with theoretical \nobjections whether an LLM is in principle capable of provid-\ning certain kinds of judgments.\nThe other approach is conceptual and focuses on ques-\ntions of a priori validity: given the limitations of their train-\ning data (e.g., solely linguistic input), then LLMs can per -\nhaps be relied upon for judgments about language, but not \njudgments about the world or human society. This objection \ncould be analogized to the question of face validity: there \nare certain theoretical constructs for which LLM-generated \njudgments simply seem implausible or inherently unreliable \n(e.g., perhaps moral norms). Other constructs, like iconicity, \nare on the margin: judgments about iconicity require knowl-\nedge both about a word’s meaning (which may in part be \ninferable from distributional statistics) and its form (which \nis not explicitly encoded in an LLM); at the same time, there \nis some evidence that LLMs do acquire knowledge about \nthe spelling of their tokens (Kaushal & Mahowald, 2022), \nwhich could form the foundation of knowledge about ico-\nnicity. Overall, this a priori approach would advocate for \nusing LLM-generated norms only when LLMs could be \nconsidered a plausible, reliable source of knowledge about \na domain—independent of their empirical performance. \nThis approach has the advantage of engaging with issues of \ntheoretical plausibility, but is disadvantaged by the fact that \nit is not always clear how to establish and agree upon clear \ncriteria for something like face validity.\nOf course, it is possible the correct approach lies some-\nwhere in the middle. As described in related work (Trott \net al., 2022), the question of whether or not LLM-generated \njudgments exhibit construct validity could be addressed by \ncomparing humans and LLMs at multiple levels of analy -\nsis. Drawing on Marr’s levels of analysis (Marr & Poggio, \n1976), one might differentiate between analogous input/\noutput behaviors (the “computational” level) and analo-\ngous representational mechanisms underlying that behavior \n(the “representational” or “algorithmic” level). The current \nwork focuses on the computational level of analysis, quan-\ntifying the empirical correlation between human-generated \nand LLM-generated judgments. Future work could aim to \ncharacterize the representational analogies or disanalogies, \nusing empirical and theoretical perspectives (Mahowald \net al., 2023; Pavlick, 2023).\nData leakage Models like GPT-4 are proprietary, both in \nterms of their trained parameter values and the details of \ntheir training data. This makes data leakage a cause for \nconcern, i.e., overlap between the training set and the test \nset. Data leakage can lead to overestimates of an LLM’s \nabilities—for example, if GPT-4 was trained on the RAW-C \n6096 Behavior Research Methods (2024) 56:6082–6100\ndataset, it would not be surprising that it could regenerate \nhuman norms with high accuracy. In the current work, I used \nat least one dataset that was released after GPT-4 was trained \n(Winter et al., 2023), which suggests that data leakage could \nnot be a concern for the iconicity norms specifically. Further, \nas illustrated in Supplementary Analysis 1, the fit between \nGPT-4’s iconicity ratings and human iconicity ratings cannot \nbe explained by iconicity correlates or by the presence of \nwords in pre-existing iconicity datasets. Additionally, Sup-\nplementary Analysis 3 provides further evidence against the \npossibility of data contamination, using a recently pioneered \ndetection method (Golchin & Surdeanu, 2023). However, \nfuture work should aim to address this issue by continuing to \nevaluate an LLM’s performance on norms that are unlikely \nto have been observed in its training set.\nChoice of model The current work relied on GPT-4, a state-\nof-the-art LLM released by OpenAI. As noted in the Meth -\nods section, one limitation of GPT-4 (along with other Ope-\nnAI models) is that the details of its architecture or training \ndata have not been made public. Further, after pre-training, \nGPT-4 was trained using “reinforcement learning with \nhuman feedback (RLHF), in which model weights are itera-\ntively updated according to explicit human feedback about \nwhich model outputs are appropriate or inappropriate; the \ndetails of this feedback are also not entirely open. Lack of \nmodel transparency could be a concern for many scientific \nquestions about LLM performance. For example, if one is \ninterested in how exposure specific kinds (or amounts) of \nlinguistic input facilitates performance on a task, then not \nknowing exactly what a model is trained on impedes one’s \nability to make relevant scientific inferences. Similarly, a \nprocess like RLHF introduces features other than pure dis-\ntributional statistics into the training signal; thus, if one’s \nquestion concerns the sufficiency of more classical models \nof learning from statistical distributions alone, then a model \ntrained with RLHF is likely not suitable.\nThe primary research question of the current work was \nwhether and to what extent a state-of-the-art LLM could \nreproduce human psycholinguistic judgments. Answering \nthis question does not hinge critically on a model’s archi-\ntecture or training regime; the input data does matter, but \nonly insofar as data contamination is a concern (see Sup-\nplementary Analysis 1). In contrast, the question does hinge \non operationalizing “state-of-the-art” and “LLM”; I selected \nGPT-4, which has achieved strong performance on a number \nof benchmarks and real-world tasks, and which is considered \nan LLM. Future work would benefit from comparison to \nother models, including smaller GPT models (e.g., GPT-3) \nas well as open-source LLMs.\nFinally, future work interested in the issue of which \nrepresentations mediate input/output behavior observed \nhere may find it useful to make use of open-source models \nwith accessible internal states. The current work relied \non error analyses to make inferences about the processes \ngiving rise to behavior; this is analogous to a dominant \napproach taken in cognitive psychology, in which internal \nstates cannot be directly observed and must be inferred \nfrom behavior. In the case of LLMs, these questions could \nalso be addressed by analyzing internal states directly \n(perhaps more analogous to neuro-imaging approaches in \nhuman psychology) and even intervened upon (i.e., as in \noptogenetics); this last methodology would be most effec-\ntive at establishing causal mechanisms underlying certain \nbehaviors, and is also an approach that is usually unethical \nto implement in humans.\nTowards a theory of prompting Relatedly, it is important \nto note that the current work prompted GPT-4 with the \nsame (or only slightly modified, in some cases) instruc-\ntions given to human participants. This was done to \nestablish the initial viability of LLM-generated norms \nand to avoid the possibility of introducing either type \nI or type II errors by manipulating the prompt. Given \nthat it is not entirely clear which instructional changes \nwould bias LLMs in which direction, this was taken as a \n“neutral” starting point for establishing a research pro-\ngram focusing on interrogating the reliability of LLM-\ngenerated norms.\nHowever, there is some evidence that alternative prompt-\ning approaches (Hu & Levy, 2023; Reynolds & McDonell, \n2021) lead to more accurate results; further, prompts with \nembedded, well-chosen exemplars (e.g., “few-shot”) may \nimprove LLM performance (Brown et al., 2020). It is pos-\nsible that LLMs may generate more reliable norms using \ndifferent instructions than those given to human participants \nand that the current work is in a “local optimum” in terms of \nprompting. Alternatively, other “adversarial” prompts could \nimpair LLM performance, i.e., lead the LLM to produce \nnorms that are decorrelated or negatively correlated with \nhuman norms. Relatedly, alternative “temperature” settings \ncould be used: rather than selecting the most probable token \nin a given context (a temperature of 0), the model could be \nallowed to generate multiple tokens at a higher temperature; \nthis could give a better indication of the underlying prob -\nability distribution and perhaps yield more accurate judg-\nments, e.g., in the cases when the most likely token is only \nslightly more probable than the second most likely token. \nOne open question is whether variance in GPT-4 judgments \nunder higher temperature settings is correlated with variance \nin human judgments for a given item. Future work should \nexplore the parameterization space more thoroughly, ideally \nwith the ultimate aim of identifying a generalizable theory \nof prompting. The methods developed and presented in this \npaper could be used as a framework for evaluating the suc-\ncess of each approach.\n6097Behavior Research Methods (2024) 56:6082–6100 \nAugmentation vs. replacement  Some recent papers have \nasked whether LLMs could be used to replace humans, \nboth as experimental participants and within the labor force \n(Eloundou et al., 2023). Throughout this manuscript, how -\never, I have approached this as a question as augmentation. \nBecause of the limitations described above—questions of \nexternal validity, precision, etc.—it seems premature to \nseek to replace  human participants entirely. Instead, as \nDillion et al. (2023) notes, perhaps LLMs could be strate-\ngically deployed at select stages of the research cycle (e.g., \npilot studies), or used in concert with human participants to \nreduce the cost of norming stimuli. For example, if LLM-\ngenerated judgments are sufficiently reliable, then rather \nthan collecting ten human judgments per word, research-\ners could collect five human judgments and combine these \nwith LLM-generated judgments. This would decrease the \ncosts of data collection and allow researchers to allocate \nexpenses towards other stages of the research cycle.\nCrucially, however, an estimate of the reliability of \nLLM-generated norms depends upon a human “gold \nstandard” with which to evaluate those norms—which is \na key argument for keeping “humans in the loop”. With-\nout such a corrective baseline, our datasets may “drift” \ntowards the statistical biases of LLMs (see Error analy -\nsis), terraforming the conceptual landscape of our sci-\nentific theories. This also reinforces the importance of \nensuring the reliability and generalizability of our human  \nsamples (Henrich et al., 2010), as well as accounting for \nindividual variability or “inter-annotator disagreement” \nin lexical representations: if individual humans (within \nor across populations) cannot agree on a judgment, then \nwhat, exactly, is in a norm?\nConclusion\nPsycholinguists rely on human judgments of lexical proper-\nties to help norm their experimental stimuli and conduct \nlarge-scale statistical analyses of the lexicon (Xu et al., 2020; \nWinter et al., 2023). However, these datasets are challenging \nand time-consuming to construct, particularly for contextu-\nalized judgments. One solution is to augment contextual-\nized datasets with judgments generated by large language \nmodels (LLMs). I empirically investigated the viability of \nthis solution for English datasets; the results suggest that \nin many cases, LLM-generated norms rival the reliability \nof norms generated by individual humans, and can even be \nsubstituted for human norms in statistical models without \nchanging theoretical inferences. However, LLM-generated \nnorms also diverge from human judgments in predictable \nways, introducing statistical biases into their judgments. \nMoving forward, the Psycholinguistics community could \nbenefit from more systematic investigation of the strengths \nand limitations of this approach, ideally keeping humans “in \nthe loop” to avoid systematic drift of our datasets.\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 3758/ s13428- 024- 02337-z.\nAcknowledgements Thank you to members of the Language and Cog-\nnition Lab at UC San Diego for thoughtful advice and comments on \nthis work (Benjamin Bergen, James Michaelov, Cameron Jones, Tyler \nChang, and Samuel Taylor). I am also grateful to Pamela Rivière for her \nhelp constructing the figures in Affinity Designer. Finally, I am grateful \nto Bodo Winter and Gary Lupyan for pointing me to the original data \nfiles for their work on iconicity norms.\nData availability The output of the GPT-4 norming process can be \nfound on GitHub: https:// github. com/ seant rott/ llm_ norms.\nCode availability Code to reproduce the original GPT-4 norms (using \nthe OpenAI API), along with the analyses described and visualizations \nshown in the paper, can also be found on GitHub: https:// github. com/ \nseant rott/ llm_ norms. The prompting method (and primary analyses) \nwere pre-registered for: the iconicity norms (https:// osf. io/ wn9pv), \nSimVerb3500 (https:// osf. io/ dtekj), the contextualized sensorimotor \nnorms (https:// osf. io/ 2e3vk), and the Glasgow Norms (https:// osf. io/ \n3jvg6).\nDeclarations \nEthics approval This research was approved by the Institutional Review \nBoard (IRB) of University of California, San Diego.\nConsent to participate Not applicable.\nConsent for publication Not applicable.\nConflict of interest The author confirms that there are no known con-\nflicts of interest associated with this publication, and there has been no \nsignificant financial support for this work that could have influenced \nits outcome.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nAher, G., Arriaga, R. I., & Kalai, A. T. (2022). Using large language mod-\nels to simulate multiple humans. arXiv preprint arXiv:2208.10264.\nAnand, P., Chung, S., & Wagers, M. (2020). Widening the net: Challenges \nfor gathering linguistic data in the digital age. Response to NSF SBE.\n6098 Behavior Research Methods (2024) 56:6082–6100\nArgyle, L.P., Busby, E.C., Fulda, N., Gubler, J., Rytting, C., & Wingate, \nD. (2022). Out of one, many: Using language models to simulate \nhuman samples. arXiv preprint arXiv:2209.06899.\nAwad, E., Dsouza, S., Shariff, A., Rahwan, I., & Bonnefon, J. F. (2020). \nUniversals and variations in moral decisions made in 42 countries \nby 70,000 participants. Proceedings of the National Academy of \nSciences, 117(5), 2332–2337.\nBender, E. M. (2009, March). Linguistically naïve!= language inde-\npendent: Why NLP needs linguistic typology. In Proceedings of \nthe EACL 2009 Workshop on the Interaction between Linguistics \nand Computational Linguistics: Virtuous, Vicious or Vacuous?  \n(pp. 26–32).\nBender, E. M., & Koller, A. (2020). Climbing towards NLU: On mean-\ning, form, and understanding in the age of data. In Proceedings \nof the 58th annual meeting of the association for computational \nlinguistics (pp. 5185–5198).\nBender, E.M., Gebru, T., McMillan-Major, A., & Shmitchell, S. \n(2021). On the dangers of stochastic parrots: Can language \nmodels be too big? In Proceedings of the 2021 ACM conference \non fairness, accountability, and transparency (pp. 610–623).\nBestgen, Y., & Vincze, N. (2012). Checking and bootstrapping \nlexical norms by means of word similarity indexes. Behavior \nResearch Methods, 44, 998–1006.\nBinder, J. R., Conant, L. L., Humphries, C. J., Fernandino, L., \nSimons, S. B., Aguilar, M., & Desai, R. H. (2016). Toward a \nbrain-based componential semantic representation. Cognitive \nNeuropsychology, 33(3–4), 130–174.\nBinz, M., & Schulz, E. (2023). Using cognitive psychology to under -\nstand GPT-3. Proceedings of the National Academy of Sciences,  \n120(6), e2218523120.\nBlasi, D. E., Henrich, J., Adamou, E., Kemmerer, D., & Majid, A. \n(2022). Over-reliance on English hinders cognitive science. \nTrends in Cognitive Sciences, 26, 1153–1170.\nBradley, M.M., & Lang, P.J. (1999). Affective norms for English \nwords (ANEW): Instruction manual and affective ratings (vol. \n30, no. 1, pp. 25-36). Technical report C-1, the center for \nresearch in psychophysiology, University of Florida.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., \nDhariwal, P., … & Amodei, D. (2020). Language models \nare few-shot learners.  Advances in Neural Information \nProcessing Systems, 33, 1877–1901.\nBrysbaert, M., & New, B. (2009). Moving beyond Kučera and Fran-\ncis: A critical evaluation of current word frequency norms and \nthe introduction of a new and improved word frequency meas-\nure for American English. Behavior Research Methods, 41(4), \n977–990.\nBrysbaert, M., Stevens, M., De Deyne, S., Voorspoels, W., & Storms, \nG. (2014a). Norms of age of acquisition and concreteness for \n30,000 Dutch words. Acta Psychologica, 150, 80–84.\nBrysbaert, M., Warriner, A. B., & Kuperman, V. (2014b). Concrete-\nness ratings for 40 thousand generally known English word lem-\nmas. Behavior Research Methods, 46, 904–911.\nCai, Z.G., Haslett, D.A., Duan, X., Wang, S., & Pickering, M.J. \n(2023). Does ChatGPT resemble humans in language use? arXiv \npreprint arXiv:2303.08014.\nChang, T.A., & Bergen, B.K. (2023). Language model behavior: A \ncomprehensive survey. arXiv preprint arXiv:2303.11504.\nClogg, C. C., Petkova, E., & Haritou, A. (1995). Statistical methods \nfor comparing regression coefficients between models. Ameri-\ncan Journal of Sociology, 100(5), 1261–1293.\nCoda-Forno, J., Witte, K., Jagadish, A.K., Binz, M., Akata, Z., \n& Schulz, E. (2023). Inducing anxiety in large language \nmodels increases exploration and bias. arXiv preprint \narXiv:2304.11111.\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language under -\nstanding. arXiv preprint arXiv:1810.04805.\nDillion, D., Tandon, N., Gu, Y., & Gray, K. (2023). Can AI language \nmodels replace human participants? Trends in Cognitive Sciences., \n27, 597–600.\nDingemanse, M., Blasi, D. E., Lupyan, G., Christiansen, M. H., \n& Monaghan, P. (2015). Arbitrariness, iconicity, and sys -\ntematicity in language. Trends in Cognitive Sciences,  19(10), \n603–615.\nDoerig, A., Sommers, R. P., Seeliger, K., Richards, B., Ismael, J., \nLindsay, G. W., ... & Kietzmann, T. C. (2023). The neurocon-\nnectionist research programme. Nature Reviews Neuroscience,  \n24(7), 431–450.\nDou, Z., Wei, W., & Wan, X. (2018). Improving word embeddings for \nantonym detection using thesauri and sentiwordnet. In Natural \nLanguage Processing and Chinese Computing: 7th CCF Interna-\ntional Conference, NLPCC 2018, Hohhot, China, August 26–30, \n2018, Proceedings, Part II 7 (pp. 67–79). Springer International \nPublishing.\nDingemanse, M., & Thompson, B. (2020). Playful iconicity: Structural \nmarkedness underlies the relation between funniness and iconicity. \nLanguage and Cognition, 12(1), 203–224.\nEloundou, T., Manning, S., Mishkin, P., & Rock, D. (2023). GPTs are \nGPTs: An early look at the labor market impact potential of large \nlanguage models. arXiv preprint arXiv:2303.10130.\nEngelthaler, T., & Hills, T. T. (2018). Humor norms for 4,997 English \nwords. Behavior Research Methods, 50, 1116–1124.\nFirth, J. R. (1957). A synopsis of linguistic theory 1930-1955. In Stud-\nies in Linguistic Analysis (pp. 1–32). Oxford: Philological Soci-\nety. Reprinted in F.R. Palmer (ed.), Selected Papers of J.R. Firth \n1952–1959, London: Longman (1968).\nForbes, M., Holtzman, A., & Choi, Y. (2019). Do neural language \nrepresentations learn physical commonsense? arXiv preprint \narXiv:1908.02899.\nGerz, D., Vulić, I., Hill, F., Reichart, R., & Korhonen, A. (2016). Sim-\nverb-3500: A large-scale evaluation set of verb similarity. arXiv \npreprint arXiv:1608.00869.\nGilardi, F., Alizadeh, M., & Kubli, M. (2023). ChatGPT outper -\nforms crowd-workers for text-annotation tasks. arXiv preprint \narXiv:2303.15056.\nGolchin, S., & Surdeanu, M. (2023). Time travel in LLMs: Tracing \ndata contamination in large language models. arXiv preprint \narXiv:2308.08493.\nGroenwold, S., Ou, L., Parekh, A., Honnavalli, S., Levy, S., Mirza, D., \n& Wang, W. Y. (2020). Investigating African-American Vernacu-\nlar English in transformer-based text generation. arXiv preprint \narXiv:2010.02510.\nHaber, J., & Poesio, M. (2021). Patterns of polysemy and homonymy \nin contextualized language models. In Findings of the Association \nfor Computational Linguistics: EMNLP 2021 (pp. 2663–2676).\nHagendorff, T. (2023). Machine psychology: Investigating emergent \ncapabilities and behavior in large language models using psycho-\nlogical methods. arXiv preprint arXiv:2303.13988.\nHarris, Z. S. (1954). Distributional structure. Word, 10(2–3), 146–162.\nHatzivassiloglou, V., & McKeown, K. (1997). Predicting the semantic \norientation of adjectives. In 35 th annual meeting of the associa-\ntion for computational linguistics and 8th conference of the Euro-\npean chapter of the association for computational linguistics (pp. \n174–181).\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people \nin the world? Behavioral and Brain Sciences, 33(2–3), 61–83.\nHill, F., Reichart, R., & Korhonen, A. (2015). Simlex-999: Evaluating \nsemantic models with (genuine) similarity estimation. Computa-\ntional Linguistics, 41(4), 665–695.\n6099Behavior Research Methods (2024) 56:6082–6100 \nHoes, E., Altay, S., & Bermeo, J. (2023). Leveraging ChatGPT for \nefficient fact-checking. https:// osf. io/ prepr ints/ psyar xiv/ qnjkf\nHu, J., & Levy, R. (2023). Prompt-based methods may underestimate \nlarge language models’ linguistic generalizations. arXiv preprint \narXiv:2305.13264.\nHu, J., Floyd, S., Jouravlev, O., Fedorenko, E., & Gibson, E. (2022). A \nfine-grained comparison of pragmatic language understanding in \nhumans and language models. arXiv preprint arXiv:2212.06801.\nImai, M., & Kita, S. (2014). The sound symbolism bootstrapping \nhypothesis for language acquisition and language evolution. Philo-\nsophical Transactions of the Royal Society B: Biological Sciences, \n369(1651), 20130298.\nJain, S., Vo, V.A., Wehbe, L., & Huth, A. G. (2023). Computational \nlanguage modeling and the promise of in silico experimentation. \nNeurobiology of Language, 1–65.\nJones, C.R., Chang, T.A., Coulson, S., Michaelov, J.A., Trott, S., & \nBergen, B. (2022). Distributional semantics still can’t account for \naffordances. In Proceedings of the Annual Meeting of the Cogni-\ntive Science Society (vol. 44, no. 44).\nJuhasz, B. J., & Yap, M. J. (2013). Sensory experience ratings for over \n5,000 mono-and disyllabic words. Behavior Research Methods,  \n45, 160–168.\nKatz, D.M., Bommarito, M.J., Gao, S., & Arredondo, P. (2023). GPT-4 \npasses the bar exam. Available at SSRN 4389233.\nKaushal, A., & Mahowald, K. (2022). What do tokens know about \ntheir characters and how do they know it?  arXiv preprint \narXiv:2206.02608.\nKiros, J., Chan, W., & Hinton, G. (2018). Illustrative language under-\nstanding: Large-scale visual grounding with image search. In Pro-\nceedings of the 56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) (pp. 922–933).\nKosinski, M. (2023). Theory of mind may have spontaneously emerged in \nlarge language models. arXiv preprint arXiv:2302.02083.\nKriegeskorte, N., Mur, M., & Bandettini, P. A. (2008). Representational \nsimilarity analysis-connecting the branches of systems neuroscience. \nFrontiers in Systems Neuroscience, 2, 4.\nKuperman, V., Stadthagen-Gonzalez, H., & Brysbaert, M. (2012). Age-of-acqui-\nsition ratings for 30,000 English words. Behavior Research Methods, 44, \n978–990.\nLewis, M., Zettersten, M., & Lupyan, G. (2019). Distributional semantics \nas a source of visual knowledge. Proceedings of the National Acad-\nemy of Sciences, 116(39), 19237–19238.\nLi, J., & Joanisse, M. F. (2021). Word senses as clusters of meaning \nmodulations: A computational model of polysemy. Cognitive Sci-\nence, 45(4), e12955.\nLynott, D., Connell, L., Brysbaert, M., Brand, J., & Carney, J. (2020). \nThe Lancaster Sensorimotor Norms: Multidimensional measures of \nperceptual and action strength for 40,000 English words. Behavior \nResearch Methods, 52, 1271–1291.\nMahowald, K., Ivanova, A. A., Blank, I.A., Kanwisher, N., Tenenbaum, \nJ.B., & Fedorenko, E. (2023). Dissociating language and thought \nin large language models: A cognitive perspective. arXiv preprint \narXiv:2301.06627.\nManning, C. D. (2022). Human language understanding & reasoning. Dae-\ndalus, 151(2), 127–138.\nMarr, D., & Poggio, T. (1976). From understanding computation to \nunderstanding neural circuitry. Neuroscience Research Program \nBulletin, 15(3), 470–488.\nMcDonald, S., & Ramscar, M. (2001). Testing the distributional hypoth-\nesis: The influence of context on judgements of semantic similar-\nity. In Proceedings of the Annual Meeting of the Cognitive Science \nSociety (vol. 23, no. 23).\nMichaelov, J. A., Coulson, S., & Bergen, B. K. (2022). So cloze yet so \nfar: N400 amplitude is better predicted by distributional information \nthan human predictability judgements. IEEE Transactions on Cogni-\ntive and Developmental Systems.\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient esti-\nmation of word representations in vector space. arXiv preprint \narXiv:1301.3781.\nMitchell, M., & Krakauer, D. C. (2023). The debate over understanding in \nAI’s large language models. Proceedings of the National Academy \nof Sciences, 120(13), e2215907120.\nMollo, D.C., & Millière, R. (2023). The vector grounding problem. arXiv \npreprint arXiv:2304.01481.\nOllion, E., Shen, R., Macanovic, A., & Chatelain, A. (2023). ChatGPT for \nText Annotation? Mind the Hype!. https:// files. osf. io/ v1/ resou rces/ \nx58kn/ provi ders/ osfst orage/ 651d6 0731b c8650 a79f3 76cf? direct= & \nmode= render\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., \n… Lowe, R. (2022). Training language models to follow instructions \nwith human feedback. Advances in Neural Information Processing \nSystems (vol. 35, pp. 27730–27744).\nPaternoster, R., Brame, R., Mazerolle, P., & Piquero, A. (1998). Using \nthe correct statistical test for the equality of regression coefficients. \nCriminology, 36(4), 859–866.\nPavlick, E. (2023). Symbols and grounding in large language models. \nPhilosophical Transactions of the Royal Society A, 381(2251), \n20220041.\nPerlman, M., Dale, R., & Lupyan, G. (2015). Iconicity can ground the cre-\nation of vocal symbols. Royal Society Open Science, 2(8), 150152.\nPiantadosi, S., & Hill, F. (2022). Meaning without reference in large lan-\nguage models. In: NeurIPS 2022 Workshop on Neuro Causal and \nSymbolic AI (nCSI).\nRamezani, A., & Xu, Y. (2023). Knowledge of cultural moral norms in \nlarge language models. arXiv preprint arXiv:2306.01857.\nRathje, S., Mirea, D. M., Sucholutsky, I., Marjieh, R., Robertson, C., \n& Van Bavel, J. J. (2023). GPT is an effective tool for multilingual \npsychological text analysis. https:// psyar xiv. com/ sekf5? trk= public_ \npost_ resha re- text\nReynolds, L., & McDonell, K. (2021). Prompt programming for large lan-\nguage models: Beyond the few-shot paradigm. In Extended Abstracts \nof the 2021 CHI Conference on Human Factors in Computing Sys-\ntems (pp. 1–7).\nScott, G. G., Keitel, A., Becirspahic, M., Yao, B., & Sereno, S. C. (2019). \nThe Glasgow Norms: Ratings of 5,500 words on nine scales. Behav-\nior Research Methods, 51, 1258–1270.\nShain, C. (2019). A large-scale study of the effects of word frequency \nand predictability in naturalistic reading. In Proceedings of the 2019 \nconference of the North American chapter of the association for \ncomputational linguistics: Human language technologies, Volume \n1 (Long and short papers) (pp. 4086–4094).\nShaoul, C., & Westbury, C. (2010). Exploring lexical co-occurrence space \nusing HiDEx. Behavior Research Methods, 42(2), 393–413.\nStroop, J. R. (1932). Is the judgment of the group better than that of the \naverage member of the group? Journal of Experimental Psychol-\nogy, 15(5), 550.\nTenney, I., Das, D., & Pavlick, E. (2019). BERT rediscovers the classical \nNLP pipeline. arXiv preprint arXiv:1905.05950.\nThompson, B., & Lupyan, G. (2018). Automatic estimation of lexical \nconcreteness in 77 languages. In The 40th annual conference of the \nCognitive Science Society (cogsci 2018) (pp. 1122–1127).\nThompson, R. L., Vinson, D. P., Woll, B., & Vigliocco, G. (2012). The \nroad to language learning is iconic: Evidence from British Sign Lan-\nguage. Psychological Science, 23(12), 1443–1448.\nTörnberg, P. (2023). ChatGPT-4 outperforms experts and crowd workers in \nannotating political twitter messages with zero-shot learning. arXiv pre-\nprint arXiv:2304.06588.\nTrott, S., & Bergen, B. (2021, August). RAW-C: Relatedness of Ambigu-\nous Words in Context (A New Lexical Resource for English). In \n6100 Behavior Research Methods (2024) 56:6082–6100\nProceedings of the 59th Annual Meeting of the Association for \nComputational Linguistics and the 11th International Joint Con-\nference on Natural Language Processing (vol. 1: Long Papers, pp. \n7077–7087).\nTrott, S., & Bergen, B. (2022). Contextualized sensorimotor norms: \nMulti-dimensional measures of sensorimotor strength for ambigu-\nous English words, in context. arXiv preprint arXiv:2203.05648.\nTrott, S., & Bergen, B. (2023). Word meaning is both categorical and \ncontinuous. Psychological Review, 130, 1239–1261.\nTrott, S., Jones, C., Chang, T., Michaelov, J., & Bergen, B. (2023). Do \nlarge language models know what humans know? Cognitive Science, \n47(7), e13309.\nUtsumi, A. (2020). Exploring what is encoded in distributional word \nvectors: A neurobiologically motivated analysis. Cognitive Science, \n44(6), e12844.\nVeselovsky, V., Ribeiro, M. H., & West, R. (2023). Artificial artificial artificial \nintelligence: Crowd workers widely use large language models for text \nproduction tasks. arXiv preprint arXiv:2306.07899.\nVinson, D. P., Cormier, K., Denmark, T., Schembri, A., & Vigliocco, G. \n(2008). The British Sign Language (BSL) norms for age of acquisition, \nfamiliarity, and iconicity. Behavior Research Methods, 40, 1079–1087.\nWebb, M. A., & Tangney, J. P. (2022). Too good to be true: Bots and bad \ndata from Mechanical Turk. Perspectives on Psychological Science. \nhttps:// doi. org/ 10. 1177/ 17456 91622 11200 27\nWingfield, C., & Connell, L. (2022). Sensorimotor distance: A \ngrounded measure of semantic similarity for 800 million concept \npairs. Behavior Research Methods, 55(7), 3416–3432.\nWinter, B., Lupyan, G., Perry, L. K., Dingemanse, M., & Perlman, M. \n(2023). Iconicity ratings for 14,000+ English words. Behavior \nResearch Methods. https:// doi. org/ 10. 3758/ s13428- 023- 02112-6\nXu, Y., Duong, K., Malt, B. C., Jiang, S., & Srinivasan, M. (2020). \nConceptual relations predict colexification across languages. Cog-\nnition, 201, 104280.\nYee, E., & Thompson-Schill, S. L. (2016). Putting concepts into con-\ntext. Psychonomic Bulletin & Review, 23, 1015–1027.\nZhu, Y., Zhang, P., Haq, E. U., Hui, P., & Tyson, G. (2023). Can Chat-\nGPT reproduce human-generated labels? A study of social com-\nputing tasks. arXiv preprint arXiv:2304.10145.\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Concreteness",
  "concepts": [
    {
      "name": "Concreteness",
      "score": 0.8063042163848877
    },
    {
      "name": "Computer science",
      "score": 0.5786048769950867
    },
    {
      "name": "Cognitive psychology",
      "score": 0.5474863052368164
    },
    {
      "name": "Psycholinguistics",
      "score": 0.5265189409255981
    },
    {
      "name": "Lexicon",
      "score": 0.5154086351394653
    },
    {
      "name": "Cognition",
      "score": 0.48146238923072815
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4706551134586334
    },
    {
      "name": "Context (archaeology)",
      "score": 0.46607279777526855
    },
    {
      "name": "Natural language processing",
      "score": 0.4565429985523224
    },
    {
      "name": "Age of Acquisition",
      "score": 0.43006181716918945
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4177926480770111
    },
    {
      "name": "Psychology",
      "score": 0.40727078914642334
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33615121245384216
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ],
  "cited_by": 22
}