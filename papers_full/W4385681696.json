{
  "title": "Redundancy-aware Transformer for Video Question Answering",
  "url": "https://openalex.org/W4385681696",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2627676801",
      "name": "Li Yicong",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2237112808",
      "name": "Yang Xun",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2100208109",
      "name": "Zhang An",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2150322302",
      "name": "Feng Chun",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1945709380",
      "name": "Wang Xiang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A3005059581",
      "name": "Chua, Tat-Seng",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4313071966",
    "https://openalex.org/W3035517717",
    "https://openalex.org/W4293567872",
    "https://openalex.org/W6790241037",
    "https://openalex.org/W2562731582",
    "https://openalex.org/W3205874482",
    "https://openalex.org/W4387968166",
    "https://openalex.org/W4386075974",
    "https://openalex.org/W2998166190",
    "https://openalex.org/W3034730770",
    "https://openalex.org/W4312761939",
    "https://openalex.org/W4323663038",
    "https://openalex.org/W3205572000",
    "https://openalex.org/W3167092180",
    "https://openalex.org/W3206675006",
    "https://openalex.org/W4285600797",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W2963798744",
    "https://openalex.org/W2951323451",
    "https://openalex.org/W3206633059",
    "https://openalex.org/W3204980132",
    "https://openalex.org/W3217054538",
    "https://openalex.org/W3175961224",
    "https://openalex.org/W4200631219",
    "https://openalex.org/W3152619510",
    "https://openalex.org/W3092803144",
    "https://openalex.org/W4205817612",
    "https://openalex.org/W4285428166",
    "https://openalex.org/W2979997102",
    "https://openalex.org/W4385572712",
    "https://openalex.org/W4298613318",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2788527488",
    "https://openalex.org/W2606982687",
    "https://openalex.org/W2955124656",
    "https://openalex.org/W4221146190",
    "https://openalex.org/W4304098887",
    "https://openalex.org/W4312246181",
    "https://openalex.org/W3109142545",
    "https://openalex.org/W3130796238",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W3189890868",
    "https://openalex.org/W4312974690",
    "https://openalex.org/W2096516049",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3132939773",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3195986544",
    "https://openalex.org/W4307123990",
    "https://openalex.org/W2765716052",
    "https://openalex.org/W4226407477"
  ],
  "abstract": "This paper identifies two kinds of redundancy in the current VideoQA\\nparadigm. Specifically, the current video encoders tend to holistically embed\\nall video clues at different granularities in a hierarchical manner, which\\ninevitably introduces \\\\textit{neighboring-frame redundancy} that can overwhelm\\ndetailed visual clues at the object level. Subsequently, prevailing\\nvision-language fusion designs introduce the \\\\textit{cross-modal redundancy} by\\nexhaustively fusing all visual elements with question tokens without explicitly\\ndifferentiating their pairwise vision-language interactions, thus making a\\npernicious impact on the answering.\\n To this end, we propose a novel transformer-based architecture, that aims to\\nmodel VideoQA in a redundancy-aware manner. To address the neighboring-frame\\nredundancy, we introduce a video encoder structure that emphasizes the\\nobject-level change in neighboring frames, while adopting an out-of-neighboring\\nmessage-passing scheme that imposes attention only on distant frames. As for\\nthe cross-modal redundancy, we equip our fusion module with a novel adaptive\\nsampling, which explicitly differentiates the vision-language interactions by\\nidentifying a small subset of visual elements that exclusively support the\\nanswer. Upon these advancements, we find this\\n\\\\underline{R}edundancy-\\\\underline{a}ware trans\\\\underline{former} (RaFormer) can\\nachieve state-of-the-art results on multiple VideoQA benchmarks.\\n",
  "full_text": "Redundancy-aware Transformer for Video Question Answering\nYicong Li1, Xun Yang2âˆ—, An Zhang1, Chun Feng2, Xiang Wang2, Tat-Seng Chua1\n1National University of Singapore, 2University of Science and Technology of China\nliyicong@u.nus.edu,{hfutyangxun,xiangwang1223}@gmail.com\nfengchun3364@mail.ustc.edu.cn,{an_zhang,dcscts}@nus.edu.sg\nABSTRACT\nThis paper identifies two kinds of redundancy in the current VideoQA\nparadigm. Specifically, the current video encoders tend to holisti-\ncally embed all video clues at different granularities in a hierarchical\nmanner, which inevitably introducesneighboring-frame redundancy\nthat can overwhelm detailed visual clues at the object level. Subse-\nquently, prevailing vision-language fusion designs introduce the\ncross-modal redundancy by exhaustively fusing all visual elements\nwith question tokens without explicitly differentiating their pair-\nwise vision-language interactions, thus making a pernicious impact\non the answering.\nTo this end, we propose a novel transformer-based architec-\nture, that aims to model VideoQA in a redundancy-aware man-\nner. To address the neighboring-frame redundancy, we introduce a\nvideo encoder structure that emphasizes the object-level change in\nneighboring frames, while adopting an out-of-neighboring message-\npassing scheme that imposes attention only on distant frames. As\nfor the cross-modal redundancy, we equip our fusion module with a\nnovel adaptive sampling, which explicitly differentiates the vision-\nlanguage interactions by identifying a small subset of visual ele-\nments that exclusively support the answer. Upon these advance-\nments, we find thisRedundancy-aware transformer (RaFormer) can\nachieve state-of-the-art results on multiple VideoQA benchmarks.\nCCS CONCEPTS\nâ€¢ Information systems â†’Question answering; Multimedia\nand multimodal retrieval .\nKEYWORDS\nVideo Question Answering, Video-Language\nACM Reference Format:\nYicong Li, Xun Yang, An Zhang, Chun Feng, Xiang Wang, and Tat-Seng\nChua. 2023. Redundancy-aware Transformer for Video Question Answering.\nIn Proceedings of the 31st ACM International Conference on Multimedia (MM\nâ€™23), October 29-November 3, 2023, Ottawa, ON, Canada. ACM, New York,\nNY, USA, 9 pages. https://doi.org/10.1145/3581783.3612577\nâˆ—Xun Yang is the corresponding author.\nXiang Wang is also affiliated with Institute of Artificial Intelligence, Institute of Datas-\npace, Hefei Comprehensive National Science Center.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0108-5/23/10. . . $15.00\nhttps://doi.org/10.1145/3581783.3612577\nClipVideoor\nFuseFuse\nGraph-based Hierarchy-based\nClip\nWhat does the  lady  do  after stopping her bike ?Multi-modal knowledge\nQ3: Whydid the girl extend her hand to bring the apple to the dog? A. feed the catB. pick up toy      C. want to feed it      Dtouch the dog       E. to caress the goat\n high\nlow\nAfeed the catBpick up toy Cwant to feed it  D touch the dog\nWhydoes the  girl bring apple  to  the  dog  ? \nSelf-rationalized Cross-modal Fuser\nObjectObjectObjectObject\nVideoClipClip\n Video\n(a)\nClipVideoor\nFuseFuse\nGraph-based Hierarchy-based\nClip\nWhat does the  lady  do  after stopping her bike ?Multi-modal knowledge\nQ3: Whydid the girl extend her hand to bring the apple to the dog? A. feed the catB. pick up toy      C. want to feed it      Dtouch the dog       E. to caress the goat\n high\nlow\nAfeed the catBpick up toy Cwant to feed it  D touch the dog\nWhydoes the  girl bring apple  to  the  dog  ? \nSelf-rationalized Cross-modal Fuser\nObjectObjectObjectObject\nVideoClipClip\n Video (b)\nClipVideoor\nFuseFuse\nGraph-based Hierarchy-based\nClip\nWhat does the  lady  do  after stopping her bike ?Multi-modal knowledge\nQ3: Whydid the girl extend her hand to bring the apple to the dog? A. feed the catB. pick up toy      C. want to feed it      Dtouch the dog       E. to caress the goat\n high\nlow\nAfeed the catBpick up toy Cwant to feed it  D touch the dog\nWhydoes the  girl bring apple  to  the  dog  ? \nSelf-rationalized Cross-modal Fuser\nObjectObjectObjectObject\nVideoClipClip\n Video\n(c)\nFigure 1: (a) Conventional multi-hierarchy video encoder. (b)\nOur design philosophy for video encoder. (c) An illustration\nof how a small proportion of interactions responds to an-\nswering, 5 critical interactions are highlighted.\n1 INTRODUCTION\nEver since the inception of intelligent systems, persistent interest\nhas been paid to revealing how machine reflects the physical world.\nIn this regard, Video Question Answering (VideoQA), a task that\nanswers the natural language questions in the context of videos, has\ngained steady progress over the last few years. This advancement\nstems, in part from the recent surge of pre-trained foundation\nmodels [41], and in part from the enthusiasm for designing task-\nspecific architectures [19, 24, 40].\nScrutinizing the recent progress [14, 15, 19, 24, 40] in VideoQA,\nwe systematize a dominant paradigm as a composition of three\nmodules: 1) a video encoder that embeds the cross-frame dynamic\non top of the frame representations, 2) a question encoder that\ncontextualizes the question semantic to acquire text embeddings,\nand 3) aacross-modal fuser that encapsulates the video and question\nembedding to the multi-modal representations, and then makes the\nprediction on top of that. Clearly, a correct prediction requires a\ncareful processing of visual clues and coordinating the interactions\nof vision-language elements to reflect the answer reasoning as a\nmutual agreement between video and question.\narXiv:2308.03267v1  [cs.CV]  7 Aug 2023\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Li et al.\nAlthough existing works have extensively adopted this para-\ndigm, we argue that the potential of current practices is seriously\nundermined due to their inability in handling the substantial re-\ndundancy of VideoQA. Specifically, the redundancy comes from\ntwo aspects:\nâ€¢The prevailing video encoder [4, 15, 41], typically models the\ninput video as a multi-level hierarchy where visual features at\ndifferent granularities are merged together in a bottom-up man-\nner (as shown in fig. 1a). However, videos are temporally re-\ndundant, the neighboring frames are similar in general because\nthey share identical environment or background, despite the\nminor differences in foreground objects. Therefore, recklessly\nmerging all adjacent frames together is neither effective nor nec-\nessary, which introduces the neighboring-frame redundancy that\nwill overwhelms the detail movement of foreground objects in\nadjacent frames. Thus, as shown in fig. 1b, we argue that an\neffective video modeling approach should focus on the object-\nlevel changes in adjacent frames rather than merging all visual\nelements in each frame together. Moreover, frame-level merging\nshould only be imposed on distant frames, where the gradual\nchanges of objects have accumulated into a qualitative change at\nthe frame level.\nâ€¢The existing cross-modal fuser [4, 10, 15, 25] performs exhaus-\ntive feature fusing, that is, merging all information from the video\nand language inputs, without differentiating the pairwise inter-\nactions between them. However, various interactions contribute\ndifferently, and only a small proportion of critical interactions\nare responsive to answering the question, leaving the rest bulk\nas cross-modal redundancy . This redundancy lead to a highly\nsparse target signal in VideoQA, making the answer reasoning\nextremely challenging. Taking fig. 1c as an example, the inter-\nactions between the query words \"girl, apple, and dog\" and the\n\"feeding\" scene in the last two clips contain the causal informa-\ntion that best supports the answer, while interactions involving\nother clips contain only environmental information and should\nbe ruled out to avoid derailing the reasoning process [3, 19, 30].\nTo resolve these limitations, we propose a novel VideoQA frame-\nwork, named Redundancy-aware Transformer (RaFormer) that\nintroduces two fundamental advancements: 1) a novel video en-\ncoder that emphasizes the object-level change within a temporal\nneighborhood by modelling object movement in adjacent frames,\nwhile avoid neighboring-frame redundancy by imposing attention\nonly on distant frames, and 2) an adaptive sampling module that en-\nables the cross-modal fuser to discover the answer critical frames in\nan interaction-aware manner. Specifically, our video encoder aims\nto model the detailed change of objects in adjacent frames, while\nimposing out-of-neighborhood message passing at frame-level to\navoid redundancy. To achieve this, we first introduce a window\ncross-attention that enhances a single frame representation using\nobjects within its temporal window. Then, on top of all enhanced\nframe representations, we adopt leap attention [46] to impose con-\nnection only between two temporal distant frames, which avoid\nthe neighboring-frame redundancy in a dilate manner. Notably, un-\nlike conventional hierarchy-based methods [4, 15, 41] that merge\nvisual elements in different granularities into a single packed rep-\nresentation (see fig. 1a), our video encoder uses a series of object\nenhanced frames to represent a video, which coordinates our fuser\ndesign. In cross-modal fusing, we first adopt a transformer-style\nencoder to exert all pairwise interactions. Then, based on encoderâ€™s\ncross-attention map, we design an adaptive sampling module to\npinpoint a small set of critical interactions, which enables their\ncorresponding frame being collected in an adaptive manner. Upon\nthese collected critical frames, we can naturally infer the answer.\nIntuitively, such a frame down-sampling scheme acts as an infor-\nmation bottleneck [34] that condenses the critical information for\nanswering and makes it easier for a VideoQA model to capture the\nlearning pattern, thus leading to a significant improvement over\nthe original input.\nOur contributions are summarized as follows:\nâ€¢We address the long-ignored redundancy issue in VideoQA, which\nis elicited by the neighboring-frame redundancy in current prac-\ntice of video encoder and the cross-modal redundancy of fuser.\nâ€¢We propose RaFormer, a fully transformer-based VideoQA model\nthat avoids neighboring-frame redundancy by highlighting object-\nlevel change in adjacent frames and the out-of-neighborhood\nmessage passing at frame-level. In addition, it also handles the\ncross-modal redundancy via a novel adaptive sampling module.\nâ€¢We perform extensive experiments on four benchmark datasets\nto demonstrate the effectiveness of RaFormer. The results show\nsignificant improvements over previous arts. (The absolute im-\nprovements w.r.t. accuracy are: NExT-QA [39] +3.5%, CausalVid-\nQA[16] +3.9%, MSVD-QA [42] +2.3%, MSRVTT[42] +2.6%.)\n2 RELATED WORKS\nVideo Question Answering (VideoQA). Video Question Answer-\ning (VideoQA) is a fundamental extension of ImageQA that incorpo-\nrates a temporal component. Typically, previous efforts either lever-\nage frame-level features as video representation or incorporated\nregion features as an additional input, which brings extra benefit\nfor detail-oriented question. Regardless of the input representation,\ncurrent designs tend to merge all visual elements together as the\nvideo-level representation, using some inductive biases. [14] and\n[24] pioneered graph-based structures for video modeling based\non the heterogeneity of input modality, while [41] enabled relation\nreasoning via dynamic object graphs. In recent years, a prevailing\nline of research tend to extract a multi-level hierarchy from the\nvideo sequence. [15] built a bottom-up pathway by assembling in-\nformation from clip-level to frame-level, while subsequent works\nsuch as [4] and [40] extended their approaches with an additional\nstep in the object region. Despite these advances, existing methods\noverlook the redundant nature of VideoQA tasks. In contrast, we\ndeveloped RaFormer to emphasize the two types of redundancy\nand mitigate their negative impact on answer reasoning.\nRedundancy in Video. Video related tasks [5, 6, 11â€“13, 21, 32, 33,\n43â€“45, 48] have seen significant advancements in recent years, pri-\nmarily driven by the adoption of 3D CNNs [8] or transformer archi-\ntecture [17, 35]. While these models have shown promising results\non different video tasks (including video understanding [20, 31, 38]\nand video-language tasks), there is a growing interest in devel-\noping more efficient techniques to maintain or even improve the\nperformance, due to the redundancy nature of video. In video under-\nstanding domain, previous works have explored various approaches\nRedundancy-aware Transformer for Video Question Answering MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\n[22, 36, 46] to reduce the computational cost and redundancy, such\nas using hybrid 2D-3D architectures, group convolution, or select-\ning salient clips. For the video-language tasks, [1] is the first work\nthat emphasizes the â€œsingle-frame redundancy â€ issue in video.\nHowever, its a analytical work that aims to provide suggestion for\na dataset construction view. To the best od our knowledge, none\nof existing work focus on how ruling out redundancy in video can\nbring extra benefit to the VideoQA. Intuitively, a concise represen-\ntaion with limited redundancy makes model easier to capture the\ncausal pattern, thus outperforms the model trained with original\ninputs [34].\nRationale Discovery. The pursuit of explainability has led to re-\ncent developments in deep neural networks (DNN) that aim to\nreveal the intuitive evidence behind their predictions, known as\nrationales. Rationale discovery is a widely adopted practice in the\nnatural language processing (NLP) community [29], as well as in\nthe graph [37] and vision fields [47]. Recently, this trend has also ex-\ntended to the multi-modal community, with ImageQA-based tasks\nthat request additional textual evidence [7, 23] and causalvid that\napplies the concept to videoQA. Despite the progress, existing solu-\ntions often require a rationale finder with significant computation\noverhead [19] or need to be trained in a contrastive manner [18].\nRaFormer, however, can identify rationales (i.e., the critical frames)\nwithout requiring additional training loss.\n3 PRELIMINARIES\nTaking a holistic view of existing methods, we summarize the para-\ndigm of VideoQA and provide a formal definition in this section.\nThroughout the paper, we use upper-cased (e.g., ğ´) and lower-cased\n(e.g., ğ‘) letters to denote a variable and its deterministic value, re-\nspectively.\nModeling. Given the video ğ‘‰ and the question ğ‘„, the VideoQA\nmodel ğº aims to exploit information in both visual and textual\nstreams to yield the predictive answer Ë†ğ´. In leading VideoQA meth-\nods, ğº is a combination of a video encoder ğºğ‘‰, a question encoder\nğºğ‘„ and a fuser ğºğ¹. It aims to encapsulate the visual content and\nlinguistic semantics into multi-modal knowledge and then generate\nthe predictive answer Ë†ğ´. Typically, an entropy-based risk function\nis adopted to abridge the predictive answer Ë†ğ´and the ground-truth\nanswer ğ´:\nmin L((ğºğ‘‰(ğ‘‰),ğºğ‘„(ğ‘„))â—¦ğºğ¹, ğ´). (1)\nData representation. We take a video of ğ‘‡ clips and select the\nframe in the middle of each clip to serve as a representation of\nthe entire video. Each of these frames is presented by extracting a\nframe feature fğ‘¡ through a pretrained image recognition backbone,\nas well as ğ‘† object features oğ‘¡,ğ‘  using a pretrained object detector,\nwhere ğ‘¡ and ğ‘  are the indices for frame and object, respectively. To\nrepresent the text, we use a pretrained language model as question\nencoder, which takes in the question as a sequence ofğ¿tokens, and\nproduces a textual representationqğ‘™ for each token. During training,\nthe visual backbones are fixed while the language backbone is\nfine-tuned end-to-end, following the approach described in [ 41].\nTo create a commonğ‘‘-dimensional space for the representations,\nwe apply three linear mappings to fğ‘¡, oğ‘¡,ğ‘ , and qğ‘™, respectively,\nresulting in F = {fğ‘¡}ğ‘‡\nğ‘¡=1 âˆˆRğ‘‡Ã—ğ‘‘, O =\n\b\noğ‘¡,ğ‘ \n\tğ‘‡,ğ‘†\nğ‘¡=1,ğ‘ =1 âˆˆRğ‘‡Ã—ğ‘†Ã—ğ‘‘, and\nScoring & Sampling\n! \"\n Answer Query\nTrans -Decoder !!\"(#)\nScoringSampling\nRationalizer #!#(%)\n!!\n!\" Ground-Truth%âˆ—\n12345\n3Trans -Encoder !!%(#)\nMulti-Modal Knowledge \nWindow Cross-AttentionLeap Self-Attention\nScoring\nVideo Encoder\nRationalizer #!#(%)\n!!\nTransformer Layer\nAnswer Decoder\nAttentionMap\nCross-modal Fuser\nObject tokenFrame tokenQuestion token\nMLP[CLS] feed the cat  [CLS] pick up toy [CLS] want to feed it [CLS] touch the dog  \nABCDTrans-DecoderMLP\nPrediction\nMulti-Choice QAOpen-Ended QA\nAnswer Set Trans-Decoder!!\"Answer Query!#$\nPrediction\nQuestion\n(b) Multi-grain AggregationMulti-modal representation%\n&o&,( 8'),* 7'),* 6'),* !\"&o+,*&o+,( \"f#\nTransformer Layer...\n...q$\nq%\nq&\nCritical ObjectsCriticalFrame\nCross-Attention\nVideo Encoder\nf&\nCross-attention\nAnswer Decoder\nAtt-Map\nCross-modal Fuser\nObject tokenFrame tokenQuestion token\nQuestionLeapSelf-Attention (Step Size = 2)\nStepSizeE=4\nAdaptiveSamplingWindow Cross-Attention (Window Size = 3)\nFigure 2: Overview of RaFormer. We made the advancement\non two modules: the video encoder and the cross-modal\nfuser. The video encoder incorporates a window-based cross-\nattention that aggregates objects within a temporal window\nto enhance the representation for each frame. On top of that,\nit also adopts a leap self-attention mechanism that avoid\nneighboring-frame redundancy by imposing attention only\nto distant frames. In the Cross-modal Fuser, a cross-attention\ntakes the video and question tokens as input, yielding multi-\nmodal knowledge and a cross-modal attention map. Based\non this attention map, an adaptive sampling module identi-\nfies critical frame representations, which are then fed into a\nanswer decoder for prediction.\nQ = {qğ‘™}ğ¿\nğ‘™=1 âˆˆRğ¿Ã—ğ‘‘, which represent the features of the frame,\nobject, and question, respectively.\n4 METHODOLOGY\nIn the existing literature, the temporal redundancy in video-language\ntasks has been analyzed by [1] from the dataset construction angle.\nHowever, a comprehensive solution for VideoQA has been missing\nuntil now. To fill this gap, we propose a novel VideoQA framework\nnamed Redundancy-aware Transformer (RaFormer). As depicted in\nfig. 2, RaFormer consists of a novel video encoder that incorporates\na window cross-attention and a leap self-attention to mitigate the\nneighboring-frame redundancy. Additionally, it tackles the cross-\nmodal redundancy by introducing an adaptive sampling module\nto cross-modal fuser, which discover the critical frames from a\nvision-language interaction perspective.\n4.1 Video Encoder\nTo encode a video instance, our video encoder aims to generate con-\ntextualized frame representations that focus on the detailed changes\nof objects within neighboring frames while avoiding neighboring-\nframe redundancy. Specifically, unlike previous encoder designs\nthat resort to multi-level hierarchy [ 4, 40, 41], which introduces\nneighboring-frame redundancy that can overwhelm the object de-\ntails, RaFormer focuses on the detail movement of objects within\nneighboring-frame by adopting a window cross-attention. It en-\nhances the representation of center frames by aggregating the ob-\njects in its temporal neighborhood. On top that, we leverage a leap\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Li et al.\nattention [46] to avoid imposing attention on neighboring frames,\nresulting a concise representation for input video.\n4.1.1 Window Cross-Attention. Window Cross-Attention (WCA)\naims to model the detailed movement of objects within a temporal\nwindow ğ‘Š, which helps to enhance a frame representationfğ‘¡ âˆˆRğ‘‘\nby aggregating all objects within its temporal window. Concretely,\nwe first add spatio-temporal position encoding to each object fea-\nture following [41], and then apply an intra-frame self-attention\nto contextualize all objects within a single frame oğ‘¡âˆˆRğ‘†Ã—ğ‘‘. Based\non the output tokens oâ€²ğ‘¡, we flatten the objects within a temporal\nwindow ğ‘Š (denote as oğ‘Š\nğ‘¡ =\n\b\noğ‘¡,ğ‘ \n\tğ‘¡+(ğ‘Šâˆ’1)/2,ğ‘†\nğ‘¡âˆ’(ğ‘Š+1)/2,ğ‘ =1 âˆˆRğ‘ŠÃ—ğ‘†Ã—ğ‘‘) to the\nshape of (ğ‘Šğ‘†)Ã—ğ‘‘ and feed it as query to window cross-attention\ntransformer decoder, along with the corresponding frame embed-\nding fğ‘¡ as key and value:\noâ€²ğ‘¡ = Self-Attention(oğ‘¡)+oğ‘¡, (2)\nfâ€²ğ‘¡ = Cross-Attentionğ‘Š(oğ‘Š\nğ‘¡ )+oğ‘Š\nğ‘¡ , s.t. |ğ‘Š|= ğ‘¤ (3)\nwhere Cross-Attentionğ‘Š denotes window cross-attention with win-\ndow size ğ‘Š. In practice, we apply window cross-attention in multi-\nhead manner, where each head is set to different window size. We\nempirically show that such multi-scale temporal reception fields\nare more flexible to cater different video content, and thus bringing\nin extra benefits.\n4.1.2 Leap Attention. To avoid neighbor-frame redundancy and\npreserve object details obtained in window cross-attention, we re-\nsort to leap attention [46] that computes self-attention on discrete\ntemporal frame pairs. With this dilated strategy, we could build\nconnection only between the distant frames, instead of recklessly\nmerging all neighboring frame together, thus maintaining the de-\ntailed visual clue at object-level. Here, we use ğ¸ to denotes the\ntemporal skipped step size. For example in fig. 2, when ğ¸=2, we get\nattention connection on following frame pairs for a sequence of\nfour frames: (0,2), (1,3). Formally, by applying leap attention on the\nenhanced frame representations fâ€², we acquire the final output of\nvideo encoder fâ€²â€²âˆˆRğ‘‡Ã—ğ‘‘ as:\nfâ€²â€²= Leap-Attentionğ¸(fâ€²)+fâ€², s.t. |ğ¸|= ğ‘’, (4)\nwhere Leap-Attentionğ¸ refers leap attention with step size ofğ¸. For\nsimplicity, we omit the superscript and use f âˆˆRğ‘‡Ã—ğ‘‘ to denote the\noutput of video encoder.\n4.2 Cross-modal Fuser with Adaptive Sampling\nAlthough we have alleviated neighboring-frame redundancy with\nthe video encoder design, cross-modal redundancy can still over-\nwhelm the critical information when fusing frame and question\ntokens. To address that, we introduce Adaptive Sampling (AS), a\nparameter-free module that adaptively down-samples frame tokens\naccording to their interaction activeness with question embedding.\nAs shown in fig. 3, we first use the attention map between frame\nand question tokens to indicate their interaction activeness. Then,\nwe flatten and normalize all ğ‘‡ Ã—ğ¿interactions to get a probability\ndistribution ğ‘ over all interactions. Next, we calculate cumulative\ndistribution function (CDF) based on ğ‘, and then select a subset of\ninteractions using inverse transform sampling. Finally, we softly\ndown-sample the frames by selecting tokens that corresponds to the\nhigh\nlow\n1.ride the bike 2.look the girl 3.push the bike 4.repair road 5.turn around\nWhat does the lady do after stopping her bike?\n<CLS>\n<CLS> ride the bike<CLS>  look the girl<CLS> push the bike <CLS> repair road <CLS> turn around\nÃ—\nÃ—\nAttention Map A\nåœ¨æ­¤å¤„é”®å…¥å…¬å¼ã€‚\nNormalize &Flatten to 1-D\n Accumulative Sum\nCDF\nFlattened interaction index\nUniformly select N samples from y-axis\nCollect sampled interactions\nMap back to 2-D\nCritical Frames f&\nScoring\nSelect tokens\nT x L\nT\nL\nL\nT\nN=4\nNormalize &Flatten to 1-D\n Accumulative Sum\nCDF\nFlattened interaction index\nUniformly select N samples from y-axis\nCollect sampled interactions\nMap back to 2-D\nSelect tokens\nProbability P in size of 1x(TL)\nCritical Frames f&\nL T\nNumber of selected interaction N=4\nL\nT\nCross-attMap z\n1\n2\n3\n45\n)\n(()\nFigure 3: Illustration of Adaptive Sampling process.\nsampled interactions, which adaptively remove redundant frames\nwith the minimal computational overhead.\n4.2.1 Interaction Scoring. To gather the critical frames, a naive\nsolution is to generate an importance vector in size of 1 Ã—ğ‘‡ by\nprepending a âŸ¨CLSâŸ©token to the question, which denotes the contri-\nbution of each of frame tokens, and then we can collect the critical\nframes by selecting from this importance vector. However, the col-\nlection under such a scheme suffers from uni-modal bias [2]. As a\nresult, tokens with similar background can exert a high attention\nscore, and will be more likely to be selected, while ignoring the the\nquestion-critical ones. Instead, we identifies the critical frames via\nthe cross-modal interactions â€“ a mutual agreement of vision and\nlanguage modalities. As shown in the right part of fig. 2, first apply\na cross-attention using encoded framesf as query and and question\nembedding q as key and value, which yields the frame token Â¯f as\nwell as the pre-normalized cross-attention mapz âˆˆRğ‘‡Ã—ğ¿ as output:\nÂ¯f,z = Cross-Attention(f,q)+f. (5)\nBased on the cross-attention mapz, we apply our adaptive sampling.\n4.2.2 Adaptive Frame Sampling. Adaptive Sampling (AS) aims to\nselect ğ‘ interactions from z âˆˆRğ‘‡Ã—ğ¿ (ğ‘ â‰ªğ‘‡ Ã—ğ¿), then we can\ncollect the corresponding 1-D tokens from the 2-D interactions\nview. Notably, by keeping only one frame when selected repetitively,\nwe end up with an adaptive frame collection where only a small\nproportion of Â¯ğ‘“ are selected as critical frames fğ‘ (i.e., |fğ‘|<= ğ‘).\nSpecifically, to select N interactions from z, a naive approach\nis to select top-N with highest score. However, this deterministic\napproach does not perform well, because it discards all interactions\nwith lower scores. Some of these interactions, however, can be\nuseful particularly at the earlier stages when the features are less\ndiscriminative. Therefore, we resort to inverse transform sampling\nto sample interactions in an differentiable manner. As shown in\nfig. 3, we first flatten z into the shape of 1 Ã—(ğ‘‡ğ¿), and then apply a\nsoftmax over all interactions to obtain the significance scores that\ncan be interpreted as probabilities p âˆˆR1Ã—(ğ‘‡ğ¿):\np = Softmax(Flatten(z)). (6)\nRedundancy-aware Transformer for Video Question Answering MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nScoring & Sampling\n! \"\n Answer Query\nTrans -Decoder !!\"(#)\nScoringSampling\nRationalizer #!#(%)\n!!\n!\" Ground-Truth%âˆ—\n12345\n3Trans -Encoder !!%(#)\nMulti-Modal Knowledge \nWindow Cross-AttentionLeap Self-Attention\nScoring\nVideo Encoder\nRationalizer #!#(%)\n!!\nTransformer Layer\nAnswer Decoder\nAttentionMap\nCross-modal Fuser\nObject tokenFrame tokenQuestion token\nMLP[CLS] feed the cat  [CLS] pick up toy [CLS] want to feed it [CLS] touch the dog  \nABCDTrans-DecoderMLP\nPrediction\nMulti-Choice QAOpen-Ended QA\nAnswer Set Trans-Decoder!!\"Answer Query!#$\nPrediction\nFigure 4: Illustration of our decoding process in Multi-choice\nQA (left) and Open-end QA (right).\nThen we calculate the cumulative distribution function (CDF) of p:\nDCFğ‘˜ =\nğ‘˜âˆ‘ï¸\n1\npğ‘˜, s.t. ğ‘˜ = (ğ‘¡âˆ’1)Ã—ğ¿+ğ‘™, (7)\nwhere ğ‘˜ âˆˆ[1,2,...,ğ‘‡ğ¿ ]is the flatten index of the p.\nNext, we obtain the sampling function by taking the inverse of\nthe CDF as:\nğ‘˜ = DCFâˆ’1 (ğ‘›), ğ‘› âˆˆ[0,1]. (8)\nIn other words, the significance scorep is used to calculate the map-\nping function between the indices of the original interactions and\nthe sampled interactions. To obtain ğ‘ samples, we use a fixed sam-\npling strategy by choosing ğ‘›=\n\b 1\n2ğ‘, 3\n2ğ‘,... 2ğ‘âˆ’1\n2ğ‘\n\t\n. Since DCFâˆ’1 âˆˆ\nR, we consider the indices of the tokens with the nearest significant\nscores as the sampling indices.\nAfter acquiring the ğ‘ sampled interactions, we collect their\ncorresponding frame tokens as critical framesfğ‘. It is worth noticing\nthat gathering 1-D tokens from the 2-D interaction view can incur\nrepetition. Thus, we only keep one frame if it is selected multiple\ntimes, which enables an adaptive collection of critical frames.\n4.3 Answer Prediction\nTo generate the predicted answer, we use a transformer-based an-\nswer decoder that receives an answer query to extract the predictive\ninformation from the multi-modal features, and select the the pre-\ndicted answer Ë†ğ´ from the answer candidates. According to task\nsetting, the answer candidates can be provided as |ğ´ğ‘šğ‘|sentences\nthat are tailored for each instance in the case of multi-choice QA, or\nas global answer categories of size |ğ´ğ‘œğ‘’|that are shared among all\ninstances (a.k.a. , Open-ended QA). This lead to the two formations\nof answer query (see in fig. 4).\n4.3.1 Multi-Choice QA. In the Multi-Choice setting (see fig. 4 left),\nthe answer candidates are given as multiple sentences tailored\nfor each instance. To generate a holistic representation for each\ncandidate answer, we prepend a âŸ¨CLSâŸ©token and use the same\nlanguage model in the question encoder to process the each answer\ncandidate in parallel. Then we gather the processed âŸ¨CLSâŸ©tokens\nfrom |ğ´ğ‘šğ‘|answer candidates representations and form the answer\nquery Qğ‘šğ‘ âˆˆ R|ğ´ğ‘šğ‘ |Ã—ğ‘‘. During decoding, we feed the answer\nquery Qğ‘šğ‘ as the query and the concatenation of critical frames and\nquestion (i.e., [fğ‘; q]) as key and value to the transformer decoder,\nand the decoder outputs the embedding hğ‘šğ‘ âˆˆR|ğ´ğ‘šğ‘ |Ã—ğ‘‘ as:\nhğ‘šğ‘ = Transformer-Decoder(Qğ‘šğ‘,[fğ‘; q]) (9)\nTable 1: Dataset statistics. MC: Multi-Choice OE: Open-Ended\nDataset Challenge #QA pair V-Len Q-Len QA\nNExT-QA Causal & Temporal 48K 44s 11.6 MC\nCausal-VidQA Evidence & Commonsense 161K 9s 9.5 MC\nMSVD-QA Description 50K 10s 6.6 OE\nMSRVTT-QA Description 244K 15s 7.4 OE\nwhere [; ]refers to the concatenation operation. Notably, since\nthe correctness of a answer candidate is invariant to its position,\nanswer query is free of position encoding. Finally, we apply a linear\nprojection on hğ‘šğ‘ to obtain the predictive answer Ë†amc âˆˆR|ğ´ğ‘šğ‘ |.\nË†ağ‘šğ‘ = MLP(hğ‘šğ‘) (10)\n4.3.2 Open-ended QA. In the open-ended setting, it is common\npractice [19, 40] to treat each candidate answer in the global answer\nset as a category, which makes the size of the answer set too large\nto be processed as multi-choice. Therefore, we initialize a single\nlearnable embedding, denoted as Qğ‘œğ‘’ âˆˆRğ‘‘, as the answer query\nfor this setting.\nSimilar to eq. (9), we use the transformer decoder to process the\nanswer query Qğ‘œğ‘’ along with the concatenation of critical frames\nand question ([fğ‘; q]) as the key and value. This results in a decoded\nrepresentation â„ğ‘œğ‘’ âˆˆRğ‘‘. Finally, we projectâ„ğ‘œğ‘’ to the answer space\nR|ğ´ğ‘œğ‘’ |to obtain the prediction Ë†ağ‘œğ‘’.\nË†ağ‘œğ‘’ = MLP(â„ğ‘œğ‘’). (11)\nTo align the predictive answer with the gold answer, we establish\nour objective on a cross-entropy loss.\n5 EXPERIMENTS\nIn this section, we show the experimental results that answer the\nfollowing research questions:\nâ€¢RQ1: How effective is RaFormer compared with the State-of-the-\nArt (SoTA) models?\nâ€¢RQ2: How do modules of Raformer and parameter setting affect\nthe performance?\nâ€¢RQ3: What learning pattern does RaFormer capture?\nDatasets: To evaluate the effectiveness of our proposed design, we\nconduct experiments on four benchmark datasets that challenge\nRaFormer from different aspects. Specifically, we use the prevalent\nMulti-Choice setting of NExT-QA [39] and Causal-VidQA [16]\nto test the modelâ€™s temporal reasoning ability with complex causal\nand commonsense relations. As a complement, we also use the\nOpen-Ended setting of MSVD-QA [42] and MSRVTT-QA [42],\nwhich mainly emphasize the description of video objects, activities,\nand their attributes. For detailed statistics of the datasets, please\nrefer to Tab. 1.\nImplementation Details: We followed the standard procedure\nestablished in [18, 19, 41] for sampling videos, where each video\nis represented as a sequence of ğ‘‡ = 16 frames, and each frame is\nencoded using a ViT-L [27] model pre-trained on ImageNet-21k. To\nextract object-level features, we used a Faster-RCNN [28] model pre-\ntrained on the Visual Genome, which detectsğ‘† = 20 objects on each\nframe. For encoding the textual input (questions and answers), we\nused a pre-trained Deberta-base model [9] as the question encoder.\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Li et al.\nTable 2: Accuracy (%) comparison on Causal-VidQA. D: Description, E: Explanation, P: Prediction, C: Counterfactual. *: Repro-\nduced result using official implementation.\nMethods Venue\nCausal-VidQA\nAcc@D Acc@E Acc@P Acc@C Acc@AllQ â†’A Q â†’R Q â†’AR Q â†’A Q â†’R Q â†’AR\nHCRN [15] CVPR20 56.4 61.6 51.7 51.3 32.6 51.6 53.4 32.7 48.1\nHGA [14] AAAI20 65.7 63.5 49.4 50.6 32.2 52.4 55.9 34.3 48.9\nB2A [24] CVPR21 66.2 62.9 49.0 50.2 31.2 53.3 56.3 35.2 49.1\nVGT* [38] ECCV22 70.8 70.3 55.2 56.9 38.4 61.0 59.3 42.0 55.4\nRaFormer (Ours) - 71.8 73.8 58.5 58.5 41.2 67.1 64.1 48.9 58.9\nAbs. Improve - +1.0 +3.5 +2.7 +1.6 +2.8 +6.1 +4.8 +6.9 +3.5\nTable 3: Accuracy (%) comparison on NExT-QA, MSVD-QA,\nand MSRVTT-QA. VIB: Visual Inductive Bias, G,H denotes\ngraph- and hierarchy- based method, respectively. ROI:\nwhether use additional region feature as additional input.\nAcc@C, T, D, denote questions type of Causal, Temporal, and\nDescriptive in NExT-QA, respectively. The best and 2nd best\nresults are highlighted.\nMethods venue NExT-QA MSVD MSRVTTAcc@C Acc@T Acc@D Acc@All\nHCRN [15] CVPR20) 47.1 49.3 54.0 48.9 36.1 35.6\nHGA [14] AAAI20 48.1 49.1 57.8 50.0 34.7 35.5\nHOSTR [4] IJCAI21 - - - - 39.4 35.9\nPGAT [25] MM21 - - - - 39.0 38.1\nMHN [26] IJCAI22 - - - - 40.4 38.6\nIGV [19] CVPR22 48.6 51.7 59.6 51.3 40.8 38.3\nHQGA [40] AAAI22 49.0 52.3 59.4 51.8 41.2 38.6\nEIGV [18] MM22 51.2 51.5 61.0 52.9 43.7 39.3\nVGT [41] ECCV22 51.6 51.9 63.7 53.7 - 39.7\nVGT-PT [41] ECCV22 52.8 54.5 67.3 55.7 - -\nRaFormer - 58.2 57.7 67.8 59.6 46.0 42.3\nAbs.Improve - +5.4 +3.2 +0.5 +3.9 +2.3 +2.6\nDuring training, we optimized the model using an Adam optimizer\nwith a learning rate of 1e-5, and set the hidden dimension to 768.\nFor for all datasets, we set skip step-size ğ¸=4 in leap attention and\nsample ğ‘ = 10 interactions in adaptive sampling.\n5.1 Main Result (RQ1)\nTab. 2 and Tab. 3 show that RaFormerâ€™s performance far exceeds\nexisting SoTA methods. Specifically, our observations are as follows:\nâ€¢QA settings. When comparing RaFormerâ€™s performance across\nfour datasets, we observe that it provides a greater improve-\nment to Multi-Choice QA than to Open-Ended QA. Specifically,\nNExT-QA and Causal-VidQA saw an increase of 3.9% and 3.5%,\nrespectively, while MSVD-QA and MSRVTT only saw an increase\nof 2.6% and 2.3%, respectively, compared to previously reported\nresults. This difference can be explained by two aspects: Firstly,\nMulti-Choice QA datasets tend to have longer videos with more\nobjects, which inevitably introduce more neighboring-frame re-\ndundancy, compared to the Open-ended setting. Luckily, our\nvideo encoder is better suited to handle neighboring-frame re-\ndundancy, thus can bring more favorable improvement to Multi-\nChoice QA. Secondly, Multi-Choice datasets involve complex\nreasoning scenarios with composite question sentences and long\nvideos, leading to more cross-modal redundancy. In contrast,\nOpen-Ended datasets typically feature simpler questions and\nshorter videos. Therefore, our adaptive sampling strategy is par-\nticularly well-suited for this scenario, which achieves a larger\ngain in Multi-Choice QA by wiping out more environmental\nframes.\nâ€¢Question type. Narrowing down the analysis to Multi-Choice\ndatasets (NExT-QA and CausalVid-QA), it is apparent that the\nperformance gain is primarily due to improvement in the com-\nposite question type (including: Acc@C&Acc@T in NExT-QA,\nAcc@E& Acc@P& Acc@C in Causal-VidQA), which involves\ndeeper understanding such as causal relation and counterfactual\nthinking. In contrast, the improvement in the descriptive ques-\ntion type is much more moderate (Acc@D: +0.5% on NExT-QA,\nand +1.0% on CausalVid). This highlights RaFormerâ€™s exceptional\nreasoning ability for complex questions. In particular, there is a\nsignificant improvement in reason-based questions on Causal-\nVidQA (Qâ†’AR: Acc@P +2.8%, Acc@C +6.9%), where the model\nhas to justify its prediction by selecting the correct evidence.\nThis naturally aligns with the mechanism of adaptive sampling,\nmaking RaFormer an ideal solution for this question type.\nâ€¢Benchmark size. Regarding the performance improvement on\nOpen-Ended datasets, it is interesting to note that the gain on\nMSRVTT-QA (+2.6%) is larger than that on MSVD-QA (+2.3%),\ndespite sharing the exactly same question type. We suggest that\nthe difference in dataset volume may have played a role in this\nobservation, since MSVD-QA is five times smaller than MSRVTT-\nQA (cf. Tab. 1). This could limit the reasoning ability of Raformer\non MSVD-QA due to a smaller amount of training data. This\nhighlights the potential of Raformer when trained on larger and\nmore diverse datasets.\n5.2 In-Depth Study (RQ2)\n5.2.1 Ablative Results. We scrutinize some key designs in RaFormer\nby performing model ablation and discussing other implementation\nalternatives. The observations of Tab. 4 are as follows:\nâ€¢In the top section, we study the design of our video encoder. In the\n1st row, we study the effectiveness of window cross-attention by\nreplacing it with a conventional object merging method [40, 41]\n(i.e., apply mean pooling to merge all objects in a single frame,\nthen add the pooled representation to the frame feature), where\nthe performance drop of this alternative (â€œw/o WCAâ€) demon-\nstrates the necessity of capturing the object transition details\nRedundancy-aware Transformer for Video Question Answering MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nTable 4: Ablative study on NExT-QA and MSRVTT-QA.\nVariants NExT-QA MSRVTTAcc@C Acc@T Acc@D Acc@All\nw/o WCA 57.7 57.3 67.9 59.2 41.1\nw/o LA 58.4 57.0 65.7 59.2 41.3\nw/o WCA & LA 57.1 56.3 65.6 58.2 40.8\nw/o AS 57.1 56.6 67.3 58.7 41.4\nHard TopN 56.9 57.6 68.3 59.0 41.7\nâŸ¨CLSâŸ©Sampling 57.0 56.2 65.0 58.1 41.3\nw/o Enc & AS 57.3 55.4 64.1 57.9 40.3\nRaFormer 58.2 57.7 67.8 59.6 42.3\nin adjacent frames. In 2nd row, we verify the efficacy of Leap-\nAttention (LA) in reducing the neighboring-frame redundancy\nby replacing it with a vanilla self-attention. As a result, we also\nwitness a performance drop on this variant (â€œw/o LAâ€), which\ndemonstrates cutting off connections between adjacent frames\nand restricts them to the distant frames can remove the redun-\ndancy from temporal dimension. In the 3rd row, we ablate the\nwhole encoder design by removing both window attention and\nleap attention, the denote this variant as â€œw/o WCA & LAâ€. We\nnotice the performance of this variant drops more significantly,\nwhich means the benefits of WCA and LA focus on different\naspect of video encoding, (WCA for modelling object detail in ad-\njacent frames, LA help to restrict neighboring-frame redundancy)\nand their contribution can be added together.\nâ€¢In the bottom section, some mutants of the adaptive sampling are\ninvestigated. First, we study the impact of our sampling strategy\nby removing the adaptive sampling module (â€œw/o ASâ€), where\nall frames are used for answer prediction without differentiating\ntheir interactions with question. As expected, the accuracy drop\ndemonstrates the negative impact of in redundant interactions in\nanswer reasoning as well as the necessity of critical frame down-\nsampling. Next, we show that collecting critical interactions via\nhard ranking of (i.e., Hard TopN) interaction scores results in sub-\noptimal performance. The reason is that, in hard topN, interaction\nwith a relatively smaller score will not be explored, even if the\nscores itself are far from discriminative at the early stage, thus\nleading to poor optimization. However, RaFormer circumvents\nthis issue by sampling from inversed cumulative distribution\nfunction, which enable a better exploration of the interactions\nin a differentiable manner. As discussed in Sec. 4.2.1, we also\nanalyze the benefits of interaction-aware sampling by comparing\nto an alternative that collects critical frames from a probability\nvector generated âŸ¨CLSâŸ©token. As expected, this variant causes\na sharp performance decline. We empirically find that this is\nbecause of the uni-modal bias issue mentioned in Sec. 4.2.1, i.e., ,\nthe high correlated adjacent frames can induce a selection bias\nthat prompts visually similar tokens as critical frames, instead of\nthe answer-responsive ones.\nâ€¢Finally, we show the effectiveness of our overall design by ab-\nlating both encoder (implemented as â€œw/o WCA & LAâ€) and\nadaptive sampling (implemented as â€œw/o ASâ€). We show this\nvariant, denoted as â€œw/o Enc & LAâ€, performs poorly comparing\nto RaFormer, which testifies that our overall design is reasonable.\n5.2.2 Study of Window Size. In order to investigate the effect of\nwindow cross-attention, the performance of RaFormer is studied\nusing different window sizes ğ‘Š. Specifically, the default setting\nuses 4-head attention, where different heads can have the same\nwindow size or varied ones. As shown in Tab. 5, when setting all\nheadsâ€™ window size to 1, the model delivers the worst accuracy,\nwhich demonstrates the necessity of modeling the objects in the\nneighboring frame. As the window size increases, the model delivers\nbetter results on MSRVTT when the window size is equal to 3,\nwhile NExT-QA reaches its peak at ğ‘Š=5. This discrepancy may\nbe caused by the complexity of the video source. Videos in NExT-\nQA feature relational reasoning among multiple objects, where a\nlarger window size can incorporate more temporal related objects,\nthus benefiting the relational reasoning. In comparison, MSRVTT\ntypically questions the attributes of a single object, making a smaller\nwindow size sufficient to capture the answer information. Finally,\nthe results in last row show that none of the single-size window\nsettings can beat the multi-scale encoding, where four attention\nheads are equipped with different window sizes. This demonstrates\nthat multi-scale window settings are more flexible in catering to\ndifferent video content.\n5.2.3 Study of Sample Size N. To validate how the size of the sam-\npled interaction ğ‘ affect RaFormer, we conduct experiments with\na variation of ğ‘ and show the results in fig. 6. In both datasets, we\nobserve constant performance gains as ğ‘ increases from 1, and the\npeaks are reached at around 10. Then, asğ‘ keeps growing, the accu-\nracies in both datasets drop since more redundancies are introduced.\nIt is worth noticing that, RaFormer can surpass all existing meth-\nods even with ğ‘ equal to 1 (i.e., only one frame is select and used\nto infer the answer), which shed light on the redundancy nature\nof VideoQA task, and the deficiency of currents methods as their\nignorance of this inherent redundancy. Compare to MSRVTT-QA,\nfluctuation on NExT-QA is more drastic when ğ‘ is small, where\nthe altering can cause a 5% difference in accuracy. This is because\nquestions of MSRVTT-QA generally focus on the description of a\nsingle entity, where the crtical frames are more perceivable even\nwith small ğ‘. In contrast, the NExT-QA typically questions the tem-\nporal relation in a composite scenario, where a small ğ‘ is unable\nto cover the golden frames that delivers the answer information,\nthus leading to sub-optimal performance.\n5.2.4 Study of Critical Frames. To grasp the learning insight of\nRaFormer, we analyzed the number of identified critical frames in\nthe original videos and present the statistics in fig. 7. As expected,\nour adaptive sampling can identify a small group of tokens as crit-\nical frames while leaving the rest as redundant. Specifically, we\nobserved that NExT-QA requires more frames to answer questions\nthan MSRVTT-QA. This is due to the relational reasoning nature of\nNExT-QA, which requires clues from multiple entities, and are typi-\ncally associated with more frames. In contrast, MSRVTT-QA mainly\nfocuses on the description of a single object, which can be encoded\nwith fewer frames. Interestingly, although RaFormerâ€™s peak perfor-\nmance on MSRVTT-QA and NExT-QA is achieved with almost the\nsame size of critical interaction group (both around ğ‘=10 cf. fig. 6),\nthe number of critical frames differs significantly between the two\ndatasets. This demonstrates that our frame selection strategy can\nadaptively cater to different VideoQA instances.\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Li et al.\nQ2. What happens to the manafter riding normally for a while at the end? \nQ2. what does the ladydo after finishing dancing at the end? \nQ1. what does the mando right after hittingthe ball? \nQ1. What does the mando after squatting down for a while?\nC.standup E.fallinto waterQ1. what does the man doas thelady approaches at the start? \nA.jumpD.holdher\nB.lookat where it goes\nQ2. how many people are together on the field? \nD.Three\nFigure 5: Case-Study on NExT-QA. Each video comes with two questions that focus on different parts of the video. The blue\nand green windows indicate the selected critical frames. In addition, we also mark the question tokens that correspond to the\nsampled interactions.\nğ‘Š NExT-QA MSRVTT\n1 58.7 41.7\n3 59.2 42.0\n5 59.4 41.8\n7 58.9 41.9\n[1,3,5,7] 59.6 42.3\nTable 5: WCAâ€™s window size W.\n(\") ($)\n(\")\nFigure 6: Hyper-parameter N\nin adaptive sampling.\nNExT MSRVTT\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18acc (%)\ncritical redundancy\nFigure 7: The number of\nthe critical frames and re-\nmoved tokens after adap-\ntive sampling.\n5.3 Qualitative analysis (RQ3)\nIntuitively, the design of adaptive sampling naturally empowers\nRaFormer with visual explainability. Following this line, fig. 5 shows\nsome predictive results on NExT-QA along with the identified criti-\ncal frames. To better understand learning pattern of adaptive sam-\npling, we also mark the question tokens that corresponds to the\nsampled interactions, which can be interpreted as \"critical words\".\nHere, each video instance comes with two questions that inquire\nabout visual clues at different frames. Generally, RaFormer can\nlocate very few indicative elements that involve the interaction of\nhigh activitness. For the critical words, RaFormer tends to imply the\nentity in the video (e.g., â€œman\", â€œlady\") as well as words that serve as\na temporal indicator (e.g., â€œafter\", â€œend\"). As for the identified critical\nframes, we notice that our adaptive sampling tends to select frames\nthat contains the key entities in the question (e.g., â€œman\", â€œlady\").\nSimilarly, it also select frames that corresponds to the temporal in-\ndicator, which benefits critical frames selection by narrowing down\nthe target scope. This is attributed to our cross-modal sampling\nstrategy, where the answer is referred to as a mutual agreement\nbetween critical frames and question words. Apart from that, we\nalso notice that, even for the same video, RaFormer can accredit\ndifferent scenes as the critical frames in response to the question\nclues. Such adaptiveness demonstrates the design philosopy of our\nsampling module, where frames are differentiate base on their inter-\naction activeness with question, making different question target\nat different scene.\n6 CONCLUSIONS\nIn this paper, we pinpoint the redundancy issue in current VideoQA\nparadigm. Specifically, we design a novel video encoder to empha-\nsize the modelling of detailed object movement within the neigh-\nboring frames, while address the neighboring-frame redundancy by\nimposing the leap attention that models the frame-level represen-\ntations in a dilate manner. To tackle the cross-model redundancy\nin prevailing fuser design, we incorporate an adaptive sampling\nstrategy that select a small set of critical frames according to their\ninteractions with question tokens. Extensive experiments on four\nbenchmark datasets have demonstrated the superior of RaFormer.\nWe hope this simple yet effective design can spark more future\nefforts in handling VideoQA redundancy.\n7 ACKNOWLEDGMENTS\nThis work was supported by NExT search center, the National Nat-\nural Science Foundation of China (NSFC) under Grant 9227010114,\nGrant62272435, Grant U22A2094, and the University Synergy Inno-\nvation Program of Anhui Province (GXXT-2022-040).\nRedundancy-aware Transformer for Video Question Answering MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nREFERENCES\n[1] Shyamal Buch, CristÃ³bal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and\nJuan Carlos Niebles. 2022. Revisiting the â€Videoâ€ in Video-Language Understand-\ning. In CVPR. 2917â€“2927.\n[2] RÃ©mi CadÃ¨ne, Corentin Dancette, Hedi Ben-younes, Matthieu Cord, and Devi\nParikh. 2019. RUBi: Reducing Unimodal Biases for Visual Question Answering.\nIn NeurIPS. 839â€“850.\n[3] Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, and Yueting Zhuang.\n2020. Counterfactual Samples Synthesizing for Robust Visual Question Answer-\ning. In CVPR. 10797â€“10806.\n[4] Long Hoang Dang, Thao Minh Le, Vuong Le, and Truyen Tran. 2021. Hierarchical\nObject-oriented Spatio-Temporal Reasoning for Video Question Answering. In\nIJCAI. 636â€“642.\n[5] Jianfeng Dong, Xianke Chen, Minsong Zhang, Xun Yang, Shujie Chen, Xirong Li,\nand Xun Wang. 2022. Partially Relevant Video Retrieval. In Proceedings of the\n30th ACM International Conference on Multimedia . 246â€“257.\n[6] Jianfeng Dong, Xirong Li, Chaoxi Xu, Xun Yang, Gang Yang, Xun Wang, and\nMeng Wang. 2021. Dual encoding for video retrieval by text. IEEE Transactions\non Pattern Analysis and Machine Intelligence 44, 8 (2021), 4065â€“4080.\n[7] Radhika Dua, Sai Srinivas Kancheti, and Vineeth N. Balasubramanian. 2021. Be-\nyond VQA: Generating Multi-Word Answers and Rationales to Visual Questions.\nIn CVPR. 1623â€“1632.\n[8] Michael Figurnov, Maxwell D. Collins, Yukun Zhu, Li Zhang, Jonathan Huang,\nDmitry P. Vetrov, and Ruslan Salakhutdinov. 2017. Spatially Adaptive Computa-\ntion Time for Residual Networks. In CVPR. 1790â€“1799.\n[9] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta:\ndecoding-Enhanced Bert with Disentangled Attention. In ICLR.\n[10] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017.\nTgif-qa: Toward spatio-temporal reasoning in visual question answering. InCVPR.\n2758â€“2766.\n[11] Wei Ji, Yicong Li, Meng Wei, Xindi Shang, Junbin Xiao, Tongwei Ren, and Tat-\nSeng Chua. 2021. Vidvrd 2021: The third grand challenge on video relation\ndetection. In ACM MM workshop . 4779â€“4783.\n[12] Wei Ji, Renjie Liang, Lizi Liao, Hao Fei, and Fuli Feng. 2023. Partial Annotation-\nbased Video Moment Retrieval via Iterative Learning. In ACM MM.\n[13] Wei Ji, Renjie Liang, Zhedong Zheng, Wenqiao Zhang, Shengyu Zhang, Juncheng\nLi, Mengze Li, and Tat-seng Chua. 2023. Are binary annotations sufficient? video\nmoment retrieval via hierarchical uncertainty-based active learning. In CVPR.\n23013â€“23022.\n[14] Pin Jiang and Yahong Han. 2020. Reasoning with Heterogeneous Graph Align-\nment for Video Question Answering. In AAAI. 11109â€“11116.\n[15] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. 2020. Hierarchical\nConditional Relation Networks for Video Question Answering. In CVPR. 9969â€“\n9978.\n[16] Jiangtong Li, Li Niu, and Liqing Zhang. 2022. From Representation to Reason-\ning: Towards both Evidence and Commonsense Reasoning for Video Question-\nAnswering. In CVPR. 21241â€“21250.\n[17] Kun Li, Jiaxiu Li, Dan Guo, Xun Yang, and Meng Wang. 2023. Transformer-\nbased Visual Grounding with Cross-modality Interaction. ACM Transactions on\nMultimedia Computing, Communications and Applications 19, 6 (2023), 1â€“19.\n[18] Yicong Li, Xiang Wang, Junbin Xiao, and Tat-Seng Chua. 2022. Equivariant\nand Invariant Grounding for Video Question Answering. CoRR abs/2207.12783\n(2022).\n[19] Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, and Tat-Seng Chua. 2022. Invariant\nGrounding for Video Question Answering. In CVPR. 2928â€“2937.\n[20] Yicong Li, Xun Yang, Xindi Shang, and Tat-Seng Chua. 2021. Interventional\nVideo Relation Detection. In ACM MM. 4091â€“4099.\n[21] Yicong Li, Xun Yang, Xindi Shang, and Tat-Seng Chua. 2021. Interventional video\nrelation detection. In ACM MM. 4091â€“4099.\n[22] Bowen Pan, Rameswar Panda, Camilo Luciano Fosco, Chung-Ching Lin, Alex J.\nAndonian, Yue Meng, Kate Saenko, Aude Oliva, and RogÃ©rio Feris. 2021. VA-RED2:\nVideo Adaptive Redundancy Reduction. In ICLR.\n[23] Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt\nSchiele, Trevor Darrell, and Marcus Rohrbach. 2018. Multimodal Explanations:\nJustifying Decisions and Pointing to the Evidence. In CVPR. 8779â€“8788.\n[24] Jungin Park, Jiyoung Lee, and Kwanghoon Sohn. 2021. Bridge To Answer:\nStructure-Aware Graph Interaction Network for Video Question Answering.\nIn CVPR. 15526â€“15535.\n[25] Liang Peng, Shuangji Yang, Yi Bin, and Guoqing Wang. 2021. Progressive Graph\nAttention Network for Video Question Answering. In ACM MM.\n[26] Min Peng, Chongyang Wang, Yuan Gao, Yu Shi, and Xiang-Dong Zhou. 2022.\nMultilevel Hierarchical Network with Multiscale Sampling for Video Question\nAnswering. In IJCAI. 1276â€“1282.\n[27] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui\nHsieh. 2021. DynamicViT: Efficient Vision Transformers with Dynamic Token\nSparsification. In NeurIPS, Marcâ€™Aurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan (Eds.). 13937â€“13949.\n[28] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN:\nTowards Real-Time Object Detection with Region Proposal Networks. InNeurIPS.\n91â€“99.\n[29] Marco TÃºlio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I\nTrust You?\": Explaining the Predictions of Any Classifier. InKDD, Balaji Krish-\nnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and\nRajeev Rastogi (Eds.). 1135â€“1144.\n[30] Andrew Slavin Ross, Michael C. Hughes, and Finale Doshi-Velez. 2017. Right\nfor the Right Reasons: Training Differentiable Models by Constraining their\nExplanations. In IJCAI. 2662â€“2670.\n[31] Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang, and Tat-Seng Chua.\n2019. Annotating objects and relations in user-generated videos. In Proceedings\nof the 2019 on International Conference on Multimedia Retrieval . 279â€“287.\n[32] Xindi Shang, Yicong Li, Junbin Xiao, Wei Ji, and Tat-Seng Chua. 2021. Video\nVisual Relation Detection via Iterative Inference. In ACM MM. 3654â€“3663.\n[33] Yi Tan, Yanbin Hao, Xiangnan He, Yinwei Wei, and Xun Yang. 2021. Selective\ndependency aggregation for action classification. In Proceedings of the 29th ACM\nInternational Conference on Multimedia . 592â€“601.\n[34] Naftali Tishby and Noga Zaslavsky. 2015. Deep Learning and the Information\nBottleneck Principle. CoRR abs/1503.02406 (2015).\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NeurIPS. 5998â€“6008.\n[36] Junke Wang, Xitong Yang, Hengduo Li, Li Liu, Zuxuan Wu, and Yu-Gang Jiang.\n[n. d.]. Efficient Video Transformers with Spatial-Temporal Token Selection. In\nECCV.\n[37] Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. 2022.\nDiscovering Invariant Rationales for Graph Neural Networks. In ICLR.\n[38] Junbin Xiao, Xindi Shang, Xun Yang, Sheng Tang, and Tat-Seng Chua. 2020. Visual\nrelation grounding in videos. In Computer Visionâ€“ECCV 2020: 16th European\nConference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part VI 16 . Springer,\n447â€“464.\n[39] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. 2021. NExT-QA:\nNext Phase of Question-Answering to Explaining Temporal Actions. In CVPR.\n9777â€“9786.\n[40] Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei Ji, and Tat-Seng Chua. 2022.\nVideo as Conditional Graph Hierarchy for Multi-Granular Question Answering.\nIn AAAI. 2804â€“2812.\n[41] Junbin Xiao, Pan Zhou, Tat-Seng Chua, and Shuicheng Yan. 2022. Video Graph\nTransformer for Video Question Answering. In ECCV. Springer, 39â€“58.\n[42] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and\nYueting Zhuang. 2017. Video Question Answering via Gradually Refined Atten-\ntion over Appearance and Motion. In ACM MM. 1645â€“1653.\n[43] Xun Yang, Fuli Feng, Wei Ji, Meng Wang, and Tat-Seng Chua. 2021. Deconfounded\nvideo moment retrieval with causal intervention. In Proceedings of the 44th In-\nternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 1â€“10.\n[44] Xun Yang, Xueliang Liu, Meng Jian, Xinjian Gao, and Meng Wang. 2020. Weakly-\nsupervised video object grounding by exploring spatio-temporal contexts. In\nProceedings of the 28th ACM international conference on multimedia . 1939â€“1947.\n[45] Xun Yang, Shanshan Wang, Jian Dong, Jianfeng Dong, Meng Wang, and Tat-Seng\nChua. 2022. Video moment retrieval with cross-modal neural architecture search.\nIEEE Transactions on Image Processing 31 (2022), 1204â€“1216.\n[46] Hao Zhang, Lechao Cheng, Yanbin Hao, and Chong-wah Ngo. 2022. Long-term\nleap attention, short-term periodic shift for video classification. In ACM MM .\n5773â€“5782.\n[47] Quanshi Zhang, Yu Yang, Haotian Ma, and Ying Nian Wu. 2019. Interpreting\nCNNs via Decision Trees. In CVPR. 6261â€“6270.\n[48] Yaoyao Zhong. 2022. Video Question Answering: Datasets, Algorithms and\nChallenges. EMNLP (2022).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7433095574378967
    },
    {
      "name": "Transformer",
      "score": 0.5895536541938782
    },
    {
      "name": "Redundancy (engineering)",
      "score": 0.5257821083068848
    },
    {
      "name": "Question answering",
      "score": 0.5070362091064453
    },
    {
      "name": "Information retrieval",
      "score": 0.3282673954963684
    },
    {
      "name": "Electrical engineering",
      "score": 0.12200358510017395
    },
    {
      "name": "Engineering",
      "score": 0.113528311252594
    },
    {
      "name": "Voltage",
      "score": 0.10155630111694336
    },
    {
      "name": "Operating system",
      "score": 0.09158322215080261
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    }
  ],
  "cited_by": 16
}