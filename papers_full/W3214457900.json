{
  "title": "Chinese WPLC: A Chinese Dataset for Evaluating Pretrained Language Models on Word Prediction Given Long-Range Context",
  "url": "https://openalex.org/W3214457900",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2318485072",
      "name": "Huibin Ge",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2131977889",
      "name": "Chenxi Sun",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2139156831",
      "name": "Deyi Xiong",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3102725307",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2970819455",
    "https://openalex.org/W2963681467",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2963956494",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2760082451",
    "https://openalex.org/W2473344385",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964267515",
    "https://openalex.org/W3187018546",
    "https://openalex.org/W2126209950",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2971871542",
    "https://openalex.org/W2949884065",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W2954089321",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W3107315802",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2962934410"
  ],
  "abstract": "This paper presents a Chinese dataset for evaluating pretrained language models on Word Prediction given Long-term Context (Chinese WPLC). We propose both automatic and manual selection strategies tailored to Chinese to guarantee that target words in passages collected from over 69K novels can only be predicted with long-term context beyond the scope of sentences containing the target words. Dataset analysis reveals that the types of target words range from common nouns to Chinese 4-character idioms. We also observe that linguistic relations between target words and long-range context exhibit diversity, including lexical match, synonym, summary and reasoning. Experiment results show that the Chinese pretrained language model PanGu-ğ›¼ is 45 points behind human in terms of top-1 word prediction accuracy, indicating that Chinese WPLC is a challenging dataset. The dataset is publicly available at https://git.openi.org.cn/PCL-Platform.Intelligence/Chinese_WPLC.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3770â€“3778\nNovember 7â€“11, 2021.câƒ2021 Association for Computational Linguistics\n3770\nChinese WPLC: A Chinese Dataset for Evaluating Pretrained Language\nModels on Word Prediction Given Long-Range Context\nHuibin Geâ€ , Chenxi Sunâ€ , Deyi Xiongâ€ , and Qun LiuÂ§\nâ€  College of Intelligence and Computing, Tianjin University, Tianjin, China\nÂ§ Huawei Noahâ€™s Ark Lab, Hong Kong, China\n{gehuibin,cxsun,dyxiong}@tju.edu.cn\nqun.liu@huawei.com\nAbstract\nThis paper presents a Chinese dataset for eval-\nuating pretrained language models on Word\nPrediction given Long-term Context (Chinese\nWPLC). We propose both automatic and man-\nual selection strategies tailored to Chinese\nto guarantee that target words in passages\ncollected from over 69K novels can only\nbe predicted with long-term context beyond\nthe scope of sentences containing the target\nwords. Dataset analysis reveals that the types\nof target words range from common nouns\nto Chinese 4-character idioms. We also ob-\nserve that linguistic relations between target\nwords and long-range context exhibit diver-\nsity, including lexical match, synonym, sum-\nmary and reasoning. Experiment results show\nthat the Chinese pretrained language model\nPanGu-Î± (Zeng et al., 2021) is 45 points be-\nhind human in terms of top-1 word predic-\ntion accuracy, indicating that Chinese WPLC\nis a challenging dataset. The dataset is pub-\nlicly available at https://git.openi.org.cn/PCL-\nPlatform.Intelligence/Chinese_WPLC.\n1 Introduction\nPredicting a target word from previous context, es-\npecially long-range context, is a long-standing chal-\nlenging problem in natural language processing. A\nvariety of large-scale datasets such as CNN/Daily\nMail (Hermann et al., 2015), Who-did-What (On-\nishi et al., 2016) and CMRC-2017 (Cui et al., 2018)\nhave been developed to examine the capability of\nmachines in word prediction. However, the major-\nity of such datasets have not undergone a thorough\nmanual testing whether a target word can only be\npredicted from long-range dependencies except for\nLAMBADA (Paperno et al., 2016). This dataset\nprovides a benchmark testbed where a target word\ncan be easily predicted with long-range context\nbut cannot with only context words in the sentence\nwhere the target word is located.\nPartially inspired by LAMBADA, we create\nChinese WPLC, a dataset for evaluating power-\nful pretrained language models on word prediction\nwith long-range context. The passages used in\nour dataset are carefully extracted from over 69K\nChinese novels following a procedure mixed with\nautomatic and manual selection. Signiï¬cant dif-\nferences from LAMBDA lie not only in language\n(English vs. Chinese), but also in the following two\naspects:\nâ€¢ LAMBADA ï¬lters out relatively easy pas-\nsages with weak language models, e.g., RNN,\n4-gram and feed-forward neural language\nmodels, which makes it an outdated dataset for\ncurrent state-of-the-art pretrained language\nmodels as target words in many left passages\nmay be easily predicted by large-scale pre-\ntrained models. Additionally, the original raw\ndata used by LAMBADA may potentially ap-\npear in the training set of current pretrained\nmodels (Brown et al., 2020). To tackle the\naforementioned problems, we use two typical\nlarge-scale pretrained models to ï¬lter out pas-\nsages: NEZHA (a masked language model)\nand NEZHA-Gen (a casual language model)\n(Wei et al., 2019).\nâ€¢ In order to take language features and difï¬-\nculty level into account, we use new strategies\nand methods in passage collection, language\nmodel ï¬ltering and crowdsourced passage se-\nlection, which are different from LAMBADA.\nWe carry out an in-depth analysis on the built\ndataset, ï¬nding that the relations between target\nwords and previous context ranges from lexical\nmatch, synonym, summary to commonsense rea-\nsoning. We conduct experiments on the built\ndataset to evaluate a range of state-of-the-art Chi-\nnese pretrained models, including the Chinese pre-\ntrained model PanGu-Î±with up to 200 billion pa-\nrameters (Zeng et al., 2021), which achieves a top-\n1 accuracy of 12.1%, 45.2 points behind human\n3771\nDataset Task Data Collection QA Train Development Test Language\nCNN/Daily Mail Entity Prediction AC ! 380,298/879,450 3,924/6,4835 3,198/53,182 EN\nCBT Entity Prediction AC ! 669,343 8,000 10,000 EN\nLAMBADA Word Prediction RNNF+MC % - 4,869 5,153 EN\nWSC Commonsense Reasoning MC ! - - 285 EN\nWinoGrande Commonsense Reasoning MC ! - - 44,000 EN\nWPLC (ours) Word Prediction PLMF+MC % - 4,827 4,474 ZH\nTable 1: Comparison between our dataset and other datasets. AC: Automatically Chosen. RNNF: Filtering by\nRNN, MC: manual check. PLMF: Filtering by pretrained language models.\nperformance, indicating a large space for further\nresearch.\n2 Related Work\nCNN/Daily Mail (Hermann et al., 2015) uses an\nautomatic method to create a large amount of in-\nstances of replacing entities with placeholders in\nnews. Childrenâ€™s Book Test (CBT) (Felix et al.,\n2016) removes four types of words that are ex-\npected to be predicted by evaluated models and pro-\nvides candidate choices for models. LAMBADA\n(Paperno et al., 2016) masks the last word in a tar-\nget sentence and evaluates the ability of models in\npredicting the masked target words with broader\ncontext beyond target sentences in novels. Wino-\ngrad Schema Challenge (WSC) (Levesque et al.,\n2012) and WinoGrande (Sakaguchi et al., 2020) de-\nï¬nes a word selection task that focuses on solving\ncommonsense problems in the form of coreference\nresolution. Details on the differences of Chinese\nWPLC from previous related datasets are shown in\nTable 1.\nIn Chinese, People Daily (PD) & Childrenâ€™s\nFairy Tale (CFT) (Cui et al., 2016) corpus is the\nï¬rst cloze-style reading comprehension dataset in\nchinese. ChID (Zheng et al., 2019) offers an in-\nteresting task where words to be predicted are all\nidioms. CLUEWSC2020 (Xu et al., 2020), a Chi-\nnese version of WSC dataset, aims to test the abil-\nity of coreference resolution via word prediction.\nSigniï¬cantly different from such Chinese datasets,\nour dataset is speciï¬cally developed for evaluating\nword prediction from long-range context.\n3 Dataset Creation\n3.1 Passage Collection\nTo diversify topics and domains, we collect raw\ndata for the Chinese WPLC from 69,067 crawled\nnovels with different topics (more details are shown\nin Table 2). The half of the crawled novels are\nused for training while the other half is used for\nBook Topics Nums Percentages (%)\nRomance 22,292 32.3\nFantasy 12,190 17.6\nUrban Supernatural 5,277 7.6\nComprehension 4,624 6.7\nRebirth 3,067 4.4\nScience Fiction 3,023 4.4\nHorror Suspense 2,162 3.1\nHistorical Military 1,868 2.7\nDetective Mystery 1,379 2.0\nModern 1,252 1.8\nOthers 12,933 17.4\nTotal 69,067 100\nTable 2: Topic distribution of crawled novels.\nextracting passages to build the development and\ntest set. We automatically extract passages from\nraw data according to the following three rules:\nâ€¢ As raw Chinese texts are not word-segmented,\nwe use three different state-of-the-art Chinese\nword segmenters, PKUSEG (Luo et al., 2019),\nJieba1 and THULAC (Sun et al., 2016) to\nsegment extracted passages. Only passages\nwhere the last word to be predicted can be con-\nsistently identiï¬ed by the three segmenters are\nkept.\nâ€¢ If the last word is a stop word, the penultimate\nword will be considered as the target word as\nstop words are usually easily to be predicted.\nIf the penultimate word is a stop word too,\nsuch passages will be discarded.\nâ€¢ We set the maximum length of a target word to\n4, making the most difï¬cult part of the task be\nto predict a Chinese idiom (four characters).\nâ€¢ The maximum length of passages is limited\nto 400 characters as long passages make word\nprediction more difï¬cult even for humans.\n3.2 Passage Filtering\nSimilar to LAMBADA (Paperno et al., 2016), we\nalso use language models to ï¬lter out passages\n1https://github.com/fxsjy/jieba\n3772\nwhere the target words (the last words) can be eas-\nily predicted by language models. But signiï¬cantly\ndifferent from LAMBADA, we use more power-\nful pretrained language models, instead of conven-\ntional or neural language models trained on rela-\ntively small data, to make our dataset challenging\nfor state-of-the-art pretrained models.\nWe ï¬netune NEZHA and NEZHA-Gen (Wei\net al., 2019) on the training data which contain\n8.7 billion words from 34,534 novels. We use two\nstrategies to ï¬lter passages: (1) predicting the tar-\nget word given a full passage (context + the target\nsentence that contains the target word) and (2) pre-\ndicting the target word only given the target sen-\ntence. Such strategies are not only more rigorous\nthan that used in LAMBADA but also consistent\nwith the succeeding crowdsourcing step. Differ-\nent combinations of the two pretrained models and\nstrategies are used to ï¬lter passages.\nIn LAMBADA, a passage will be ï¬ltered out if\nthe probability of the target word is greater than a\npreset threshold. Predeï¬ning an appropriate thresh-\nold is rather difï¬cult, heavily depend on human ex-\nperience. Thus, we use a different ï¬ltering method:\nany passages where the target word appears in the\nlist of top-5 words predicted by either of the afore-\nmentioned two ï¬ltering strategies are discarded.\nIn addition to this, another difference is that\nwe compute the ratio of the target word probabili-\nties estimated given the full and target sentence by\nNEZHA-Gen as follows:\nRatio(w) = P(w|c,s\\w)\nP(w|s\\w) (1)\nwhere P(w|c,s\\w) is the probability of the target\nword w given the long-range context c plus the\ntarget sentence sexcluding the target wordwwhile\nP(w|s\\w) is the probability of predicting wonly\ngiven s\\w. Higher ratios indicate that the target\nword can be more conï¬dently predicted given the\nlong-range context than the short-term context in\nthe target sentence. Preference is given to passages\nwith a ratio greater than the base e.\n3.3 Crowdsourced Passage Selection\nWe hire over 100 crowdsourced workers to manu-\nally select passages from the left passages after the\nautomatic passage collection and ï¬ltering proce-\ndure. For crowdsourced manual passage selection,\nwe take 3 steps, similar to LAMBADA, where in\nthe ï¬rst two steps crowdsourced workers are asked\nTWL #Passages #Avg tokens #Avg sentences\n1 408/354 117.7/119.7 3.7/4.3\n2 3,904/3,670 130.7/136.1 3.6/4.3\n3 260/236 130.3/137.1 3.7/4.4\n4 255/214 128.2/127.2 3.8/4.0\ntotal 4,827/4,474 129.5/134.5 3.6/4.3\nTable 3: Statistics on the development/test set. TWL:\nthe length of target words.\nAvg O Avg DL Avg DF\ndev/test 1.3/1.3 72.8/74.1 81.5/83.8\nTable 4: Statistics on lexical match on the dev/test set.\nAvg O: the average number of occurrences of target\nwords in passage. Avg DF/DL: the average number\nof tokens between a target word and its ï¬rst/last occur-\nrence in context.\nto guess the missing target word given the entire\npassage excluding the target word.\nIn the third step, three different crowdsourced\nworkers are asked to guess at most 3 target words\nper worker given the short-term context in the target\nsentence. If none of the manually predicted words\nare the target word, the passage is added to Chinese\nWPLC.\nParticularly, in each step, workers are provided\nwith the length of the target word to ease the guess-\ning difï¬culty.\nAt last, we collect 9,301 passages, among which\n4,827 passages from 17,266 novels are used as the\ndevelopment set while the remaining 4,474 pas-\nsages from 17,267 novels are used as the test set.\nTable 3 provides the detailed statistics of the devel-\nopment and test set with respect to the target word\nlength.\n4 Dataset Analysis\n4.1 Target Word Types\nFigure 1 shows the distribution of the types of target\nwords in Chinese WPLC. The majority of target\nwords are common nouns (60.5%), followed by\nverbs (19.9%). Different from LAMBDA, Chinese\nWPLC contain 3.4% Chinese idioms (See the third\nexample in Appendix Table 6). Chinese idioms in-\ncrease the difï¬culty of word prediction for machine\nalthough they are widely used in human-written\nChinese texts.\n4.2 Linguistic Relations between Target\nWords and Long-Range Context\nInspired by Jing et al. (2019) and Paperno et al.\n(2016), we further analyze the linguistic relations\n3773\nCN:60.5%V:19.9%\nJ:6.7%\nPN:6.7%\nI:\n3.4% O:\n2.8%\nFigure 1: Target word type distribution. CN: common\nnouns. V: verbs. J: adjectives. PN: proper nouns. I:\nChinese idioms. O: other.\nbetween target words and long-term context in pas-\nsages. We sample 100 examples from the develop-\nment set and ï¬nd four linguistic relations: lexical\nmatch, synonym, summary, reasoning as shown in\nAppendix Table 6. Lexical match, indicating that\nthe target word has also occurred in context, ac-\ncounts for 64%. However, lexical match does not\nmean that the target word can be easily predicted as\nfurther statistics in Table 4 disclose that the distance\nbetween the target word and its ï¬rst/last apperance\nin context is very long, ranging from over 70 to\n80 tokens. Synonym, suggesting that a word or\nphrase with similar meaning to the target word oc-\ncurrs in context, accounts for 15%. A more difï¬cult\nphenomenon is to summarize the given passage to\npredict the target word, which accounts for 8% of\nthe sampled data. The left samples need to conduct\nreasoning over context while the target word has\nnot been explicitly mentioned in context at all.\n5 Experiments\nWe carried out experiments with a range of state-\nof-the-art pretrained language models on Chi-\nnese WPLC. As BERT-large and the last layer of\nRoBERTa-large are currently not available for Chi-\nnese, results of these two models are not provided.\nTop-1 and Top-3 accuracy are reported.\n5.1 Baseline Models\nIn addition to BERT (Devlin et al., 2019), we also\nevaluated the following pre-trained language mod-\nels on the dataset.\nâ€¢ ALBERT: ALBERT (Lan et al., 2020) is a\nlite BERT with fewer parameters but more\npowerful performance.\nâ€¢ RoBERTa: RoBERTa (Liu et al., 2019) is\na stronger BERT without the next sentence\nprediction loss.\nâ€¢ MacBERT: MacBERT (Cui et al., 2020) is a\nChinese BERT that uses similar words for the\nmasking purpose.\nâ€¢ CPM: CPM(Zhang et al., 2020) is a Chinese\nGPT-2 (Radford et al., 2019) with 2.6 billion\nparameters.\nâ€¢ PanGu-Î±: PanGu-Î± (Zeng et al., 2021) is\na Chinese pre-trained casual language model\nwith up to 200 billion parameters. The version\nthat we used in experiments has 13 billion\nparameters.\n5.2 Experimental Setup\nAll baselines were tested using their default\nhyper-parameters, including BERT 2, ALBERT3,\nRoBERTa2, MacBERT 4, CPM 5 and PanGu- Î±6.\nFor causal language models, beam-search was used\nto generate top-3 words and the number of gener-\nation steps was the length of the target word. For\nmasked language models, we downloaded a whole\nword mask version and selected top-3 words in the\nmasked positions as predicted target words.\n5.3 Human Evaluation\nIn order to assess human performance on Chinese\nWPLC, we hired another 4 crowdsourced workers\nto perform word guessing on 1000 samples ran-\ndomly chosen from the development and test set\n(500 each). Each worker is asked to guess 3 words\nand the ï¬rst word is considered as the most proba-\nble word guessed by worker.\n5.4 Results\nTable 5 presents the results of the models on the\ndevelopment and the test data. Note that the scores\nof NEZHA and NEZHA-Gen are 0 since they are\nused to ï¬lter passages in Section 3.2.\nPretrained Models vs. Human:All state-of-\nthe-art pretrained models perform much worse than\nhuman on this task. PanGu- Î± achieves a top-1\naccuracy of 12.1%, the highest prediction accuracy\namong all pretrained models, which, however, is\n2https://github.com/ymcui/Chinese-BERT-wwm\n3https://github.com/google-research/ALBERT\n4https://github.com/ymcui/MacBERT\n5https://huggingface.co/mymusise\n6https://git.openi.org.cn/PCL-\nPlatform.Intelligence/PanGu-Alpha\n3774\nTop-1 Top-3\nNezha-Gen 0/0 0/0\nNezha 0/0 0/0\nHuman 57.3 66.4\nCasual Language Models\nCPM 0.6/0.5 1.5/1.5\nCPM-kd 1.2/0.9 2.9/2.4\nPanGu-Î± 12.7/12.1 -/-\nMasked Language Models\nBERT-base 7.3/6.3 10.1/8.9\nRoBERTa-base 6.5/5.7 9.8/8.9\nMacBERT-large 6.8/7.5 10.6/10.5\nALBERT-xxlarge 4.5/3.8 6.5/5.4\nTable 5: Top-1 and Top-3 accuracy (%) results of\nmodels and human on the development/test of Chi-\nnese WPLC. CPM-kd: knowledge distillated (Geoffrey\net al., 2015) CPM.\n45.2 points behind human performance (57.3%).\nWe ï¬nd that knowledge distillation helps in CPM-\nlarge achieve a gain of 0.4 to 1.4 percentage points.\nMasked Language Models (MLMs) vs. Ca-\nsual Language Models (CLMs):MLMs (BERT-\nlike) are slightly better than CLMs (next token pre-\ndiction) in Table 5. The reasons may be two-fold.\nFirst, since MLMs are bidirectional, they can use\nextra information after target words, such as stop\nwords and punctuations, to predict target words.\nSecond, we used stronger NEZHA-Gen to ï¬lter out\npassages in dataset creation, which may make the\nremaining passages difï¬cult for other CLMs.\n5.5 Analysis on PanGu-Î±and Human\nPrediction\nWe analyzed 100 randomly sampled passages from\nthe development set to compare PanGu- Î± with\ncrowdsourced workers. One difference between\nhuman and models on word prediction on Chinese\nWPLC is that human workers can use the length\nof a target word as auxiliary information to predict\ntarget word while current models cannot use such\ninformation. We ï¬nd that 14% of predicted words\nby PanGu-Î±are completely correct and 22% are\nalmost correct (See the ï¬rst and second example in\nAppendix Table 7). There are also 11% of exam-\nples where target words predicted by PanGu-Î±are\nsimilar to the ground-truth target words (See the\nthird example in Appendix Table 7).\nWe also analyzed 100 sampled passages with\ncorrect word predictions by human workers and\nPanGu-Î±. We ï¬nd that 75% of these human predic-\ntions are lexical match and 7% are synonym. The\ntype of summary accounts for only 4% of passages\nwhile the left 14% are reasoning. For PanGu- Î±,\n71% of predictions are lexical match followed by\nreasoning which accounts for 23%. There are also\n4% of synonym, followed by summary, which ac-\ncounts for 2%. Lexical match is the easiest type\nfor both human and models. Even the target words\nof reasoning-type word prediction have not been\nexplicitly mentioned in context at all, we ï¬nd that\nboth human and models can do better than they\ndo in the other two types (i.e., synonym and sum-\nmary).\n6 Conclusions\nIn this paper, we have presented the Chinese\nWPLC, a Chinese word prediction dataset created\nfrom over 69K novels to examine the ability of\npretrained language models on long-term context\nmodeling. We employ both automatic and manual\nselection strategies to keep passages where target\nwords can be only predicted from long-term context\nbeyond target sentences and it is difï¬cult for pre-\ntrained language model to predict target words. Ex-\nperiments with a range of state-of-the-art pretrained\nlanguage models and in-depth analyse demonstrate\nthat the created dataset is a very challenging testbed\neven for the very large Chinese pretrained PanGu-\nÎ±, covering a variety of linguistic phenomena (e.g.,\nlexical match, synonym, summary and reasoning).\nAcknowledgements\nThe present research was supported by Huawei.\nWe would like to thank the anonymous reviewers\nfor their insightful comments. The corresponding\nauthor is Deyi Xiong (dyxiong@tju.edu.cn).\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language Models are Few-Shot Learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877â€“1901. Curran Associates,\nInc.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020. Revisiting pre-\ntrained models for Chinese natural language process-\ning. In Findings of the Association for Computa-\n3775\ntional Linguistics: EMNLP 2020, pages 657â€“668,\nOnline. Association for Computational Linguistics.\nYiming Cui, Ting Liu, Zhipeng Chen, Wentao Ma, Shi-\njin Wang, and Guoping Hu. 2018. Dataset for the\nï¬rst evaluation on Chinese machine reading compre-\nhension. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nYiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang,\nand Guoping Hu. 2016. Consensus attention-based\nneural networks for Chinese reading comprehension.\nIn Proceedings of COLING 2016, the 26th Inter-\nnational Conference on Computational Linguistics:\nTechnical Papers, pages 1777â€“1786, Osaka, Japan.\nThe COLING 2016 Organizing Committee.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171â€“4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHill Felix, Bordes Antoine, Chopra Sumit, and Weston\nJason. 2016. The Goldilocks Principle: Reading\nChildrenâ€™s Books with Explicit Memory Representa-\ntions. In 4th International Conference on Learning\nRepresentations, ICLR 2016, San Juan, Puerto Rico,\nMay 2-4, 2016, Conference Track Proceedings.\nHinton Geoffrey, Vinyals Oriol, and Dean Jeffrey. 2015.\nDistilling the Knowledge in a Neural Network. In\nNIPS Deep Learning and Representation Learning\nWorkshop.\nKarl Moritz Hermann, TomÃ¡Å¡ KoË‡cisk`y, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Proceedings of the 28th In-\nternational Conference on Neural Information Pro-\ncessing Systems-Volume 1, pages 1693â€“1701. MIT\nPress.\nYimin Jing, Deyi Xiong, and Zhen Yan. 2019. Bi-\nPaR: A bilingual parallel dataset for multilingual\nand cross-lingual reading comprehension on novels.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2452â€“\n2462, Hong Kong, China. Association for Computa-\ntional Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In Interna-\ntional Conference on Learning Representations.\nHector J. Levesque, Ernest Davis, and Leora Mor-\ngenstern. 2012. The Winograd Schema Challenge.\nIn Proceedings of the Thirteenth International Con-\nference on Principles of Knowledge Representa-\ntion and Reasoning, KRâ€™12, pages 552â€“561. AAAI\nPress.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach.\nRuixuan Luo, Jingjing Xu, Yi Zhang, Xuancheng\nRen, and Xu Sun. 2019. PKUSEG: A Toolkit for\nMulti-Domain Chinese Word Segmentation. CoRR,\nabs/1906.11455.\nTakeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-\npel, and David McAllester. 2016. Who did what:\nA large-scale person-centered cloze dataset. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2230â€“\n2235, Austin, Texas. Association for Computational\nLinguistics.\nDenis Paperno, GermÃ¡n Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, San-\ndro Pezzelle, Marco Baroni, Gemma Boleda, and\nRaquel FernÃ¡ndez. 2016. The LAMBADA dataset:\nWord prediction requiring a broad discourse context.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525â€“1534, Berlin, Germany.\nAssociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners. Ope-\nnAI blog, 1(8):9.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-\nvatula, and Yejin Choi. 2020. WINOGRANDE: an\nadversarial winograd schema challenge at scale. In\nAAAI.\nMaosong Sun, Xinxiong Chen, Kaixu Zhang, Zhipeng\nGuo, and Zhiyuan Liu. 2016. THULAC: An efï¬-\ncient lexical analyzer for chinese.\nJunqiu Wei, Xiaozhe Ren, Xiaoguang Li, Weny-\nong Huang, Yi Liao, Yasheng Wang, Jiashu\nLin, Xin Jiang, Xiao Chen, and Qun Liu. 2019.\nNEZHA: Neural Contextualized Representation for\nChinese Language Understanding. arXiv preprint\narXiv:1909.00204.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie\nCao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu,\nCong Yu, Yin Tian, Qianqian Dong, Weitang Liu,\nBo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao\nWang, Weijian Xie, Yanting Li, Yina Patterson,\nZuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua\nLiu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui\nZhang, Zhengliang Yang, Kyle Richardson, and\n3776\nZhenzhong Lan. 2020. CLUE: A Chinese language\nunderstanding evaluation benchmark. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 4762â€“4772, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang,\nYi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang\nYang, Kaisheng Wang, Xiaoda Zhang, Chen Li,\nZiyan Gong, Yifan Yao, Xinjing Huang, Jun Wang,\nJianfeng Yu, Qi Guo, Yue Yu, Yan Zhang, Jin\nWang, Hengtao Tao, Dasen Yan, Zexuan Yi, Fang\nPeng, Fangqing Jiang, Han Zhang, Lingfeng Deng,\nYehong Zhang, Zhe Lin, Chao Zhang, Shaojie\nZhang, Mingyue Guo, Shanzhi Gu, Gaojun Fan,\nYaowei Wang, Xuefeng Jin, Qun Liu, and Yonghong\nTian. 2021. PanGu- Î±: Large-scale Autoregressive\nPretrained Chinese Language Models with Auto-\nparallel Computation. CoRR, abs/2104.12369.\nZhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian\nGu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe\nJi, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan\nZheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen,\nDaixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie\nHuang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan\nZhu, and Maosong Sun. 2020. CPM: A Large-scale\nGenerative Chinese Pre-trained Language Model.\nChujie Zheng, Minlie Huang, and Aixin Sun. 2019.\nChID: A large-scale Chinese IDiom dataset for cloze\ntest. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 778â€“787, Florence, Italy. Association for\nComputational Linguistics.\n3777\nA Appendix\nRelations Example %\nLexical\nmatch\nPassage:åœ¨ä¸€å°æ—¶çš„æ—¶é—´é‡Œä»–ä¸€ç›´åœ¨ç¡è§‰ã€‚ç§‘ä¼¦å·´çš„å°æœºåœºéå¸¸æ½®æ¹¿ï¼Œé‚£å„¿èšé›†ç€ä¸€ç¾¤ç­‰å€™å»åœ£å…‹é²æ–¯çš„ç»åˆ©ç»´äºš\näººã€‚ä»–ä»¬ä¸ªä¸ªå¸¦ç€å¤§åŒ…å°åŒ…çš„åœ£è¯ç¤¼ç‰©ã€‚ä»–å«çš„é‚£ä½å‡ºç§Ÿè½¦å¸æœºä¸æ‡‚ä¸€å¥è‹±è¯­ï¼Œä½†è¿™æ²¡å…³ç³»ã€‚å†…ç‰¹æŒ‡ç»™ä»–çœ‹æ—…æ¸¸æ‰‹\nå†Œä¸Šçš„â€œçš‡å®«é¥­åº—â€å‡ ä¸ªå­—ï¼Œä»–åä¸Šè¿™è¾†åˆæ—§åˆè„çš„å‡ºç§Ÿè½¦ç¦»å¼€äº†<mask><mask>ã€‚\nHe had been sleeping during this hour. The small airportin Corumba was very humid, and there was a crowd of Bolivians\nwaiting to head for Santa Cruz.They all carry bags of Christmas presents . The taxi driver he called didnâ€™t understand a word\nof English, but it didnâ€™t matter. Nate pointed out the words â€œPalace Hotelâ€ in the travel brochure. He got in this old and dirty\ntaxi and left the <mask>.\nTarget word:æœºåœº/ airport\n64\nSynonym\nPassage:å®ˆæ®¿çš„å¤§å¤ªç›‘åå«è¿‡ä¸šå¤§ï¼Œäººç§°å¤§å…¬å…¬ã€‚å›½è—©ä¸å¤§å…¬å…¬æ‰“å£°æ‹›å‘¼åï¼Œä¾¿ç«¯ååœ¨å…»æ€§æ®¿å€™é©¾ã€‚ä¸€åæ•´æ•´ä¸¤ä¸ª\næ—¶è¾°ï¼Œæ—¶è‡³æ­£åˆï¼Œå°šä¸è§å¬ï¼Œå›½è—©å¿ƒä¸­çŠ¯ç–‘ï¼Œè¯·å¤§å…¬å…¬æ‰“å¬ã€‚ä¸€ä¼šå„¿ï¼Œå¤§å…¬å…¬å‘Šè¯‰ä»–ï¼šâ€œçš‡ä¸Šä»Šå¤©ä¸æ¥äº†ï¼Œæ˜å¤©åœ¨\nå…»å¿ƒæ®¿<mask><mask>ã€‚â€\nThe eunuch who guarded the temple was called Guoyeda, commonly known as â€œthe Grand Eunuchâ€. Having greeted the\nGrand Eunuch, Guofan sat in the Hall of Mental Cultivation waiting for the emperorâ€™s coming. Guofan sat for two hours\nuntil noon, but still didnâ€™t get called. He got bewildered, so asked the Grand Eunuch to inquire about this. After a while,\nthe Grand Eunuch told him: â€œThe emperor is not coming today, but will <mask> you at the Hall of Mental Cultivation to-\nmorrow.â€\nTarget word:å¬è§/ summon\n15\nSummary\nPassage:å¥åº·çš„çº¢è‰²ä¼šè®©ä»–ä»¬çš„æ— é™éæƒ³é€šè¿‡åŠªåŠ›é€æ¸è½¬å˜æˆä¸ºç°å®ï¼Œè€Œé—æ†¾çš„æ˜¯é‚£äº›æ²¡æœ‰è‡ªåˆ¶åŠ›çš„çº¢è‰²å´ç–äºè¡Œ\nåŠ¨ï¼Œå¾ˆå¤šæ¢¦æƒ³æœ€ç»ˆå •è½ä¸ºç©ºæƒ³ã€‚å› æ­¤ï¼Œä¸å…¶è¯´å ‚å‰ç‚å¾·æ˜¯è¥¿ç­ç‰™çš„æœ€åä¸€ä½éª‘å£«ï¼Œè«å¦‚è¯´ä»–æ˜¯è¶…çº§å¯Œäºå¹»æƒ³çš„çº¢è‰²\nä»£è¡¨äººç‰©ã€‚å½“ç„¶ï¼Œå¦‚æœçº¢è‰²ä¸åœåœ°ç©ºæƒ³ï¼Œå†åŠ ä¸Šå¤¸å¤¸å…¶è°ˆï¼Œä¸€ä¸å°å¿ƒï¼Œå˜æˆâ€œ<mask><mask><mask><mask>â€ã€‚\nThe healthy red can make their inï¬nite daydream changing gradually to a reality through efforts. However, it is a pity that\nthose red who have no self-control failed to take actions, and many dreams eventually degenerates into fantasies. Thus, Don\nQuixote is not so much the last knight of Spain as a super fanciful representative of the red. There is no doubt that if the red\ncannot stop indulging in fantasy, even in some magniloquence, it will turn into <mask><mask><mask><mask>â€easily.\nTarget word:çº¸ä¸Šè°ˆå…µ/ an idea on paper\n8\nReasoning\nPassage:å­Ÿé£é…é…¿äº†åŠå¤©ç¡¬æ˜¯æ²¡å«å‡ºçˆ¸å’Œå¦ˆï¼Œè‹è“ä¸ºå­Ÿé£è§£å›´è¯´ï¼šâ€œä»–ç¬¬ä¸€æ¬¡è§ä½ ä»¬ï¼Œä¸€æ—¶åŠä¼šè¿˜ä¸ä¹ æƒ¯ã€‚â€å¥¹å¦ˆå¦ˆ\néå¸¸å®½å®¹åœ°è¯´ï¼šâ€œå°ä¼™å­ç¬¬ä¸€æ¬¡æ€»æ˜¯å¾ˆéš¾è¯´å‡ºå£çš„ï¼Œç»“äº†å©šå°±æ…¢æ…¢ä¹ æƒ¯äº†ã€‚â€å­Ÿé£ä¸€å¬çªƒå–œï¼Œè¿™è¯è¡¨ç¤ºå¥¹å¦ˆå¦ˆå·²ç»é»˜\nè®¸äº†ä»–è¿™ä½<mask><mask>ã€‚\nMeng Fei had been brewing for a long time but did not call out father and mother in the end. Su Lan helped him out and said,\nâ€œItâ€™s the ï¬rst time he has met you, so he doesnâ€™t get quite used to it in such a short time.â€ Her mother said very tolerantly: â€œIt has\nalways been hard for a young man to say this for the ï¬rst time, but youâ€™ll get used to it after you got married.â€ Meng Fei was\nsecretly pleased on hearing that, which indicated that her mother had acquiesced in him as a <mask><mask><mask>.\nTarget word:å¥³å©¿/ son-in-law\n13\nTable 6: Linguistic relations between target words and long-term context. Each \"<mask>\" represents a single\nChinese character.\n3778\nExample\nPassage:åˆšæ‰ä½ ä¹Ÿéƒ½çœ‹åˆ°äº†ï¼Œåœ¨å‘ç—…æ—¶å€™çš„å¥¹ï¼Œå®Œå…¨å°±æŠŠæˆ‘è¿™ä¸ªå“¥å“¥å½“æˆæ¯’è›‡çŒ›å…½ä¸€èˆ¬ï¼Œå¥¹éå¸¸çš„æ’æ–¥æˆ‘ï¼Œä¸æ„¿æ„è§æˆ‘ï¼Œæ‰€ä»¥\nï¼Œæˆ‘çˆ¸å¦ˆå°±å€Ÿç€å…¬å¸äº‹åŠ¡ï¼Œåœ¨é‚£æ®µæ—¶é—´æŠŠæˆ‘è°ƒç¦»å›½å¤–å»å¤„ç†äº‹æƒ…ï¼Œç­‰æˆ‘å›æ¥ï¼Œå¥¹å·²ç»è¢«é€åˆ°äº†. . . . . .é‚£ä¸ªåŒ»é™¢ï¼Œæˆ‘å»çœ‹å¥¹ï¼Œå¥¹\nä¹Ÿä»æ¥éƒ½æ˜¯<mask><mask><mask><mask>. . . . . .\nAs you have seen just now, during the attack, she completely regarded me, his brother, as a venomous serpent and wild beast. She ostracized\nme very much and was reluctant to see me. Therefore, under the guise of company affairs, my parents sent me abroad to deal with the business.\nWhen I came back, she had been sent to...that hospital. I went to see her, but she had always been <mask><mask>......\nPanGu-Î±: ä¸æ„¿æ„è§æˆ‘/ reluctant to see me\nTarget word / Human:é¿è€Œä¸è§/ evading me\nPassage:è€çˆ·å­å¹´è¿ˆäº†,èº«ä½“ä¹Ÿæ²¡ä»¥å‰ç¡¬æœ—,å¯è¿˜æ˜¯ä¸æœè¾“çš„æ€§å­,ä¸è¿‡,æˆ‘å´è¶Šæ¥è¶Šå¯Ÿè§‰,ä»–å¯¹äºè´¢å¯Œ,å·²æ²¡æœ‰å½“å¹´é‚£èˆ¬çƒ­è¡·,é™†æ°æ——\nä¸‹æœ‰å¤šå°‘ä¼ä¸š,æœ‰å¤šå°‘èµ„äº§,äºä»–,ä¹Ÿåªæ˜¯ä¸€çº¸ç¬¦å·,äººè€äº†,æœ€ç›¼æœ›çš„è¿˜æ˜¯ä¸€å®¶å›¢èš!æœ‰æ—¶é—´,ä½ å¤šç»™å®¶æ¡“æ—æ•²ä¾§å‡»ä¸‹,è®©ä»–æ—©ç‚¹å›æ¥,\nä¸ä»…æ˜¯é™†æ°ç­‰ä»–,è¿˜æœ‰<mask><mask><mask>ã€‚\nThe old man is getting older and his body is also not as strong as before. But he still has an unyielding personality. However, I have become\nmore and more aware that he is no longer as enthusiastic about wealth as he used to be in the past. No matter how many enterprises and assets\nare owned by the Luâ€™s group, itâ€™s just a paper of symbols for him. When people are old, what they most look forward to is family reunion!\nWhen you have time, you could insinuate Jiahuan that he should come back early. Not only is Lu waiting for him, but also <mask><mask>.\nPanGu-Î±: ä½ è€çˆ·å­/ your old man\nTarget word / Human:è€çˆ·å­/ old man\nPassage:æœ‰æ—¶å¯¹æ–¹æ­£æ€¥éœ€,åˆä¸è‚¯å¯¹ä½ æ˜è¨€,æˆ–æ•…æ„è¡¨ç¤ºæ— æ­¤æ€¥éœ€,ä½ å¦‚å¾—çŸ¥æƒ…å½¢,æ›´åº”å°½åŠ›å¸®å¿™,å¹¶ä¸”ä¸èƒ½æœ‰ä¸æ¯«å¾—æ„çš„æ ·å­,ä¸€é¢ä½¿\nä»–æ„Ÿè§‰å—ä¹‹æœ‰æ„§,ä¸€é¢åˆä½¿ä»–æœ‰çŸ¥å·±ä¹‹æ„Ÿã€‚å¯¸é‡‘ä¹‹é‡,ä¸€é¥­ä¹‹æ©,å¯ä»¥ä½¿ä»–ç»ˆç”Ÿé“­è®°ã€‚æ—¥åå¦‚æœ‰æ‰€éœ€,ä»–å¿…å¥‹èº«å›¾æŠ¥ã€‚å³ä½¿ä½ æ— æ‰€éœ€,\nä»–ä¸€æœå¦ææ³°æ¥,ä¹Ÿç»ä¸ä¼šå¿˜äº†ä½ è¿™ä¸ª<mask><mask>ï¼\nSometimes one is in desperate need of you, but would not tell you clearly, or deliberately indicate that there is no urgent need. If you know\nthis situation, you should try your best to help, and cannot show any complacency. On the one hand, it would make him shameful for receiving\nit and give him the feeling of having a new conï¬dant on the other hand. The encounter of an inch of gold and the grace of a meal can make him\nremember for life. And if you need help later, he will go out of his way to help you. Even if you donâ€™t need it, after a storm comes a calm, he\nwill not forget you who is his <mask>ï¼\nPanGu-Î±: æœ‹å‹/ friend\nTarget word / Human:çŸ¥å·±/ conï¬dant\nTable 7: Examples with predicted target words from PanGu-Î±and humans.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8263314962387085
    },
    {
      "name": "Natural language processing",
      "score": 0.7282820343971252
    },
    {
      "name": "Context (archaeology)",
      "score": 0.7045926451683044
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6976183652877808
    },
    {
      "name": "Word (group theory)",
      "score": 0.685901403427124
    },
    {
      "name": "Synonym (taxonomy)",
      "score": 0.6140507459640503
    },
    {
      "name": "Chinese language",
      "score": 0.5159093141555786
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.5091533064842224
    },
    {
      "name": "Term (time)",
      "score": 0.5051434636116028
    },
    {
      "name": "Character (mathematics)",
      "score": 0.49449631571769714
    },
    {
      "name": "Noun",
      "score": 0.48158907890319824
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.4758826792240143
    },
    {
      "name": "Chinese characters",
      "score": 0.4282829165458679
    },
    {
      "name": "Language model",
      "score": 0.42571014165878296
    },
    {
      "name": "Scope (computer science)",
      "score": 0.4219006896018982
    },
    {
      "name": "Linguistics",
      "score": 0.3025643229484558
    },
    {
      "name": "History",
      "score": 0.0721023678779602
    },
    {
      "name": "Mathematics",
      "score": 0.07051292061805725
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Genus",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I162868743",
      "name": "Tianjin University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250955327",
      "name": "Huawei Technologies (China)",
      "country": "CN"
    }
  ],
  "cited_by": 1
}