{
  "title": "Progressive Transformer-Based Generation of Radiology Reports",
  "url": "https://openalex.org/W3130502265",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2799155327",
      "name": "Farhad Nooralahzadeh",
      "affiliations": [
        "University Psychiatric Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2771189127",
      "name": "Nicolas Perez Gonzalez",
      "affiliations": [
        "University Psychiatric Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1319012821",
      "name": "Thomas Frauenfelder",
      "affiliations": [
        "University Psychiatric Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2111924892",
      "name": "Koji Fujimoto",
      "affiliations": [
        "Kyoto University"
      ]
    },
    {
      "id": "https://openalex.org/A2094427164",
      "name": "Michael Krauthammer",
      "affiliations": [
        "University Psychiatric Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3037337776",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W2997704374",
    "https://openalex.org/W2963373823",
    "https://openalex.org/W2778310824",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2953022248",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2929309437",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2990515515",
    "https://openalex.org/W2913279579",
    "https://openalex.org/W3098325931",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W2950489286",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W3104371371",
    "https://openalex.org/W3104027471",
    "https://openalex.org/W2116492146",
    "https://openalex.org/W2904183610",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3104609094",
    "https://openalex.org/W2803411968",
    "https://openalex.org/W2964195337",
    "https://openalex.org/W1686810756"
  ],
  "abstract": "Inspired by Curriculum Learning, we propose a consecutive (i.e., image-to-text-to-text) generation framework where we divide the problem of radiology report generation into two steps. Contrary to generating the full radiology report from the image at once, the model generates global concepts from the image in the first step and then reforms them into finer and coherent texts using transformer-based architecture. We follow the transformer-based sequence-to-sequence paradigm at each step. We improve upon the state-of-the-art on two benchmark datasets.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2824–2832\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n2824\nProgressive Transformer-Based Generation of Radiology Reports\nFarhad Nooralahzadeh1, Nicolas Perez Gonzalez1, Thomas Frauenfelder1,\nKoji Fujimoto2†, Michael Krauthammer1\n1University of Zürich and University Hospital of Zürich , 2Kyoto University\n{farhad.nooralahzadeh,nicolas.perez,michael.krauthammer}@uzh.ch\nthomas.frauenfelder@usz.ch,†kfb@kuhp.kyoto-u.ac.jp\nAbstract\nInspired by Curriculum Learning, we propose\na consecutive (i.e., image-to-text-to-text) gen-\neration framework where we divide the prob-\nlem of radiology report generation into two\nsteps. Contrary to generating the full radiology\nreport from the image at once, the model gen-\nerates global concepts from the image in the\nﬁrst step and then reforms them into ﬁner and\ncoherent texts using a transformer architecture.\nWe follow the transformer-based sequence-to-\nsequence paradigm at each step. We improve\nupon the state-of-the-art on two benchmark\ndatasets.\n1 Introduction\nThe analysis of X-rays in medical practice is the\nmost common and important task for radiologists.\nWith years of training, these experts learn to recog-\nnize particular features in the image that are later\ntranslated to a written report in a clinically appropri-\nate manner. This is a labor intensive and time con-\nsuming task, especially difﬁcult for young trainees.\nWith increasing demand on imaging examinations,\nthe burden on radiologists has increased over time,\nrequiring the addition of the technologies to im-\nprove their workﬂow.\nPrevious research on radiology report genera-\ntion has mostly focused on image-to-text gener-\nation tasks. Jing et al. (2018) introduced a co-\nattention mechanism to generate full paragraphs.\nLovelace and Mortazavi (2020) explored report\ngeneration through transformers. More recently,\nZhang et al. (2020) used a preconstructed graph\nembedding module on multiple disease ﬁndings to\nassist the generation of reports. Finally, Chen et al.\n(2020) proposed to generate radiology reports via\nmemory-driven transformer and showed that their\nproposed approach outperforms previous models\nwith respect to both language generation metrics\nand clinical evaluation. These systems have signiﬁ-\ncant potential in many clinical settings, including\nimprovement in workﬂow in radiology, clinical\ndecision support, and large-scale screening using\nX-ray images.\nIn this work, we focus on generating reports\nfrom chest X-ray images innovating with a double\nstaged transformer based architecture. Our contri-\nbutions in this paper can be summarized as follows:\n(i) We propose to produce radiology reports via\na simple but effective progressive text generation\nmodel by incorporating high-level concepts into\nthe generation process 1, (ii) We conduct extensive\nexperiments and the results show that our proposed\nmodels outperforms the baselines and existing mod-\nels, i.e., achieving a substantial +1.23% increase\nin average over all language generation metrics in\nIU X-RAY , and the increase of +3.2% F1 score\nin MIMIC-CXR , against the best baseline R2GEN ,\nand (iii) We perform a qualitative analysis to fur-\nther demonstrate the quality and properties of the\ngenerated reports.\n2 Method\nAn essential challenge in the radiology report gen-\neration is modeling the clinical coherence across\nthe entire report. Contrary to generating the full ra-\ndiology report from the image at once, we propose\na consecutive (i.e., image-to-text-to-text) genera-\ntion framework (inspired by Curriculum Learning\n(Bengio et al., 2009) and the work of Tan et al.\n(2020)). As shown in Figure 1, we divide the prob-\nlem of radiology report generation into two steps.\nIn the ﬁrst step, the model generates global con-\ncepts from the image and then reforms them into\nﬁner and coherent text using a transformer archi-\ntecture. Each step follows the transformer based\nsequence-to-sequence paradigm.\nModel Architecture Instead of generating the\nfull report from an input radiology image, we frame\n1Our code is available at https://github.com/\nuzh-dqbm-cmi/ARGON\n2825\nFigure 1: Overview of our proposed framework\nthe generation process such as: X →C →Y,\nwhere X = {x1,x2,...,x S}, xs ∈ Rd. X is a\nradiology image and xs is a sequence of patch fea-\ntures extracted from visual extractor and dis the\nsize of the feature vectors. C = {c1,c2,...,c T},\nct ∈V, and Y = {y1,y2,...,y T′ }, yt′ ∈V′, are\nthe generated tokens at intermediate and ﬁnal steps,\nrespectively. T and T′are the length of generated\ntokens and V, V′are the vocabulary of all possi-\nble tokens at each step. Our framework can be\npartitioned into three major components such as:\n1) A visual backbone 2) An intermediate encoder-\ndecoder as a visual language model (ViLM) and\n3) A ﬁnal encoder-decoder as a language model\n(LM).\nVisual Backbone Given a set of radiology\nimages (I), the visual backbone extracts the vi-\nsual features X and results in the source sequence\n{x1,x2,...,x s}for the subsequent visual language\nmodel. The visual backbone can be formulated\nbased on pre-trained Convolutional Neural Net-\nworks (CNN), e,g., DenseNet (Huang et al., 2016),\nVGG (Simonyan and Zisserman, 2015) or ResNet\n(He et al., 2016). We ﬁnd DenseNet to be more\neffective in our generation task and therefore use it\nas our based visual feature extractor.\nVisual Language Model (ViLM)We adapt a\nstate-of-the-art image captioning model, Meshed-\nMemory Transformer (M2 TR.), introduced by (Cor-\nnia et al., 2020) for the intermediate step of our\narchitecture. M2 TR. is a transformer (Vaswani\net al., 2017) based model which presents two ad-\njustments that leveraged the performance of the\nmodel: Memory Augmented Encoder and Meshed\nDecoder. Memory Augmented Encoder extends the\nset of keys and values in the encoder with additional\n“slots” to extract a priori information. The priori in-\nformation is not based on the input; it is encoded in\nlearnable vectors, which are concatenated to keys\nand values and can be directly updated via SGD.\nUnlike the original decoder block in transformer,\nwhich only performs a cross-attention between the\nlast encoding layer and the decoding layers, theM2\nTR. presents a meshed connection with all encoding\nlayers. We refer the reader to Cornia et al. (2020)\nfor a detailed description of the Meshed-Memory\nTransformer.\nGiven the visual language model structure, the\nobjective of the intermediate generation phase can\nbe formalized as :\npθ(C |I) =\nT∏\nt=1\npθ(ct |c<t,I)\nwhere Cat the intermediate step is the high-level\ncontext that contains informative and important\ntokens to serve as skeletons for the following en-\nrichment process. To train the ViLM , we maximize\nthe conditional log-likelihood ∑T\nt=1 log pθ(C |I)\non the training data to ﬁnd the optimized θ∗.\nLanguage Model The third component of our\narchitecture is also based on the transformer as a\nsequence-to-sequence model that follows the con-\nditional probability as:\npθ′ (Y|C) =\nT′\n∏\nt′\npθ′ (yt′ |y<t′ |fθ′ (C))\nwhere fθ is an encoder that transforms the input\nsequence (e.g., high-level context) into another rep-\nresentation that are used by the language model\npθ at decoding step. We employed BART (Lewis\net al., 2020) as a pre-trained language model and\nﬁne-tune on our target domain. BART includes\na BERT-like encoder and GPT2-like decoder. It\nhas an autoregressive decoder and can be directly\nﬁne tuned for sequence generation tasks such as\nparaphrasing and summarization. Similar to the\nprevious module, to train the LM, we maximize the\nconditional likelihood ∑T′\nt′ log pθ′ (Y |C) using\n2826\nAlgorithm 1: Training the Progressive\nTransformer-Based Generation of Radiol-\nogy Reports\nInput: Radiology Reports R and Images I,\nPretrained CNNs Model\nDensNet-121, Pretrained LM BART\n1 Extract a high-level context C from\nRadiology Reports R\n2 Fine-tune ViLM and LM independently\nOutput: Fine-tuned ViLM and LM for\nreport generation from Images I in\na progressive manner\nthe training set.\nTraining Algorithm 1 shows the training steps\nof our proposed architecture. We ﬁrst extract a\nhigh-level context C for each report in training\ndataset (see Figure 1). To do so, we employed\nMIRQI tools implemented by Zhang et al. (2020).\nEach training report is processed with disease word\nextraction, negation/uncertainty extraction, and at-\ntributes extraction based on dependency graph pars-\ning. A similar method proposed in NegBio (Peng\net al., 2018) and CheXpert (Irvin et al., 2019) for\nentity extraction and rule based negation detection\nis adopted in MIRQI. Then, we construct indepen-\ndent training data for each stage, i.e., ﬁne-tuning\nof the ViLM and LM. More concretely, given train-\ning pairs (I,C), we ﬁne-tune ViLM. On the other\nhand, the BART is ﬁne-tuned by using training\npairs (C,R) in the LM stage. Having ﬁne-tuned\nthe ViLM and LM, the model ﬁrst generates the\nintermediate context and subsequently generates\nthe full radiology report by adding ﬁner-grained\ndetails at the ﬁnal stage.\n3 Experiments\nDatasets We examine our proposed framework\non two datasets as follows: i) IU X- RAY (Demner-\nFushman et al., 2015), a public radiology dataset\nthat contains 7,470 chest X-ray images and 3,955\nradiology reports, each report is associated with\none frontal view chest X-ray image and optionally\none lateral view image, ii) MIMIC-CXR (Johnson\net al., 2019), a large publicly available database\nof labeled chest radiographs that contains 473,057\nchest X-ray images and 206,563 reports. In order to\ncompare our method with previous works, we use\nthe available split on two datasets (i.e., the IU X-\nRAY and MIMIC-CXR splits available in Chen et al.\n(2020).)2\nEvaluation Metrics The evaluation of the mod-\nels is preformed using general NLG metrics in-\ncluding BLUE (Papineni et al., 2002), METEOR\n(Denkowski and Lavie, 2011) and ROUGE-L (Lin,\n2004). However, to address the shortcoming of the\nconventional NLG metrics in medical abnormality\ndetection (Liu et al., 2019; Lovelace and Mortazavi,\n2020; Chen et al., 2020), we also report clinical efﬁ-\ncacy (CE) metrics that compare CheXpert extracted\nlabels for the generated and reference reports3. To\nalleviate randomness of the scores, the mean of ﬁve\ndifferent runs are reported.\nBaselines We consider the following baselines\nin our evaluation process: (i) TRANSFORMER : The\nvanilla transformer is employed in the ViLM com-\nponent to generate radiology reports in a standard\nmanner, and (ii) M2 TR.: The Meshed-Memory\nTransformer is used in the ViLM component to\ngenerate text without progressive style.\nMoreover, we compare our model with previ-\nous studies reported in Chen et al. (2020), e.g.,\nST (Vinyals et al., 2015), ATT2IN (Rennie et al.,\n2017), ADAATT (Lu et al., 2017), TOPDOWN (Ander-\nson et al., 2018), COATT (Jing et al., 2018), HRGR\n(Li et al., 2018), CMAS -RL (Jing et al., 2019) and\nR2GEN (Chen et al., 2020) (see Section A in ap-\npendix for more detail). For reproducibility, the\nmodel conﬁguration and training are described in\nSection B of the Appendix.\n4 Results and Discussion\nEffect of progressive generation To show the\neffectiveness of our model, we conduct experi-\nments with baseline models, including our pro-\nposed model ( i.e., M2 TR. P ROGRESSIVE ) as re-\nported in Table 1. The results shows that M2 TR.\nprovides better performance than the vanilla trans-\nformer which conﬁrms the validity of incorporating\nmemory matrices in the encoder and meshed con-\nnectivity between encoding and decoding modules.\nOur progressive model consistently outperforms\nthe standard and single-stage ViLMs by a large\nmargin on almost all metrics in both benchmark\ndatasets, which clearly highlights the beneﬁts of\n2https://github.com/cuhksz-nlp/R2Gen\n3https://github.com/MIT-LCP/mimic-cxr/\ntree/master/txt\n2827\nDATA MODEL NLG METRICS CE METRICS\nBL-1 BL-2 BL-3 BL-4 MTR RG-L P R F1\nIU\nX-R AY\nTRANSFORMER 0.388 0.246 0.176 0.133 0.163 0.340 - - -\nM2 TR. 0.475 0.301 0.228 0.180 0.169 0.373 - - -\nM2 TR. P ROGRESSIVE 0.486 0.317 0.232 0.173 0.192 0.390 - - -\nMIMIC\n-CXR\nTRANSFORMER 0.305 0.188 0.126 0.092 0.128 0.264 0.313 0.224 0.261\nM2 TR. 0.361 0.221 0.146 0.101 0.139 0.266 0.324 0.241 0.276\nM2 TR. P ROGRESSIVE 0.378 0.232 0.154 0.107 0.145 0.272 0.240 0.428 0.308\nTable 1: The performance of baseline and our progressive model on the test sets of IU X-R AY and MIMIC-CXR\ndatasets with respect to NLG and CE metrics. BL-n denotes BLEU score using up to n-grams; MTR and RG-L\ndenote METEOR and ROUGE-L, respectively. The performance of all models is averaged from ﬁve runs.\nthe progressive generation strategy. However the\nprecision of the progressive model is lower than\nthe baselines. We observe that the progressive gen-\neration produces long reports mostly by adding the\nabnormality mentions in negation mode (e.g., No\nevidence of pneumonia , There is no pneumotho-\nrax ), therefore it increases the number of false\npositives (FPs) in the CE metrics.\nIn Table 2, we compare our full model (i.e., M2\nTR. P ROGRESSIVE ) with the previous works on the\nsame datasets. In general, memory based trans-\nformer methods offer signiﬁcant improvements\nacross all metrics compared to the recurrent neu-\nral networks (RNNs) based architectures. This is\nillustrated by comparing R2GEN , M2 TR. and our\nfull model with the other techniques (see also Table\n1). Our model achieves competitive results com-\npare to R2GEN , i.e., +1.23% average on all NLG\nmetrics in IU X- RAY, +0.83% and +3.2% average\non all NLG metrics and F1 score, respectively, in\nthe MIMIC-CXR dataset. This indicates the beneﬁts\nof using the M2 TR. together with our progressive\nstrategy in the radiology reports generation task.\nWe hypothesise that the use of MIRQI in the inter-\nmediate context generation provides informative\nand high-quality plans which results in reasonable\ndescriptions for clinical abnormalities in the last\ngeneration stage.\nAnalysis As a qualitative analysis to explain the\neffectiveness of our progressive model, we examine\nsome of the generated reports with their references\nfrom the MIMIC-CXR test dataset (see Figure 2 in\nthe Appendix). We show the text alignments be-\ntween the reference text and generated one with\nthe same colors. It can be seen in the top two ex-\namples the progressive model is able to provide\nreports aligned with the reference texts where the\nbaseline model fails to cover them, e.g., post me-\ndian sternotomy, and mitral valve replacement, The\nmediastinal contours, enlargement of the cardiac\nsilhouette, bilateral pleural effusions and compres-\nsive atelectasis in the top two examples are not\ngenerated by M2 TR.. Although our model shows\nimprovements in the NLG and CE metrics evalu-\nation, it still fails to generate clinically coherent\nand error-free reports. For example, in the third\nexample of Figure 2, the mild pulmonary edema is\nincorrect since the No new parenchymal opacities\nin the reference implies negative pulmonary edema.\nFurthermore, the sentence left plueral effusion in\nthe last example is not consistent with the previous\ntext bilateral pleural effusion. Additionally, the ex-\namples in Figure 2 contain a comparison of study\nagainst to the previous study such as As compared\nto the previous ... and In comparison with the study\n... in the generated reports. This is a little surprising\nsince the model does not have any clue about the\nprevious report of a patient in its design. It can be\nattributed to the fact that these template sentences\nare more frequent in the training set. The examples\nalso show that the progressive model generates a\nmore comprehensive report compare to the base-\nline.It includes occasionally the extra mentions of\nmedical terms compared to the reference text (e.g.,\nThere is no focal consolidation and No evidence\nof pneumonia in examples 1 and 3, respectively),\nwhich result in false-positive mention of observa-\ntions in the CheXpert labeler of the CE metrics.\n5 Conclusion\nWe propose to produce radiology report via a sim-\nple but effective progressive text generation model\nby incorporating high-level concepts into the gen-\neration process. The experimental results show\n2828\nDATA MODEL\nNLG METRICS CE METRICS\nBL-1 BL-2 BL-3 BL-4 MTR RG-L P R F1\nIU\nX-R AY\nST ⊙ 0.216 0.124 0.087 0.066 - 0.306 - - -\nATT 2IN ⊙ 0.224 0.129 0.089 0.068 - 0.308 - - -\nADA ATT ⊙ 0.220 0.127 0.089 0.068 - 0.308 - - -\nCOATT ⊙ 0.455 0.288 0.205 0.154 - 0.369 - - -\nHRGR ⊙ 0.438 0.298 0.208 0.151 - 0.322 - - -\nCMAS -RL ⊙ 0.464 0.301 0.210 0.154 - 0.362 - - -\nR2G EN ⊙ 0.470 0.304 0.219 0.165 0.187 0.371 - - -\nM2 TR. P ROGRESSIVE 0.486 0.317 0.232 0.173 0.192 0.390 - - -\nMIMIC\n-CXR\nST ⊕ 0.299 0.184 0.121 0.084 0.124 0.263 0.249 0.203 0.204\nATT 2IN ⊕ 0.325 0.203 0.136 0.096 0.134 0.276 0.322 0.239 0.249\nADA ATT ⊕ 0.299 0.185 0.124 0.088 0.118 0.266 0.268 0.186 0.181\nTOPDOWN ⊕ 0.317 0.195 0.130 0.092 0.128 0.267 0.320 0.231 0.238\nR2G EN ⊙ 0.353 0.218 0.145 0.103 0.142 0.277 0.333 0.273 0.276\nM2 TR. P ROGRESSIVE 0.378 0.232 0.154 0.107 0.145 0.272 0.240 0.428 0.308\nTable 2: Comparisons of our full model with previous studies on the test sets of IU X-RAY and MIMIC-CXR with\nrespect to language generation (NLG) and clinical efﬁcacy (CE) metrics. ⊙refers to the result that is directly cited\nfrom the original paper and ⊕represents the replicated results reported on Chen et al. (2020).\nthat our proposed model outperforms the baselines\nand a wide range of radiology report generation\nmethods, in terms of language generation and clini-\ncal efﬁcacy metrics. Further, the manual analysis\ndemonstrates the ability of the model to produce\nlong and more clinically coherent reports, however\nthere is still room for improvement.\nAcknowledgements\nThis research has received funding from the Ky-\noto University-University of Zurich Joint Funding\nProgram. The authors would also like to thank the\nanonymous reviewers for their feedback.\nReferences\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-Up and Top-Down Attention\nfor Image Captioning and Visual Question Answer-\ning. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition , pages 6077–\n6086.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th Annual International Con-\nference on Machine Learning , ICML ’09, page\n41–48, New York, NY , USA. Association for Com-\nputing Machinery.\nZhihong Chen, Yan Song, Tsung-Hui Chang, and Xi-\nang Wan. 2020. Generating radiology reports via\nmemory-driven transformer. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1439–1449,\nOnline. Association for Computational Linguistics.\nMarcella Cornia, Matteo Stefanini, Lorenzo Baraldi,\nand Rita Cucchiara. 2020. Meshed-Memory Trans-\nformer for Image Captioning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition.\nDina Demner-Fushman, Marc D Kohli, Marc B Rosen-\nman, Sonya E Shooshan, Laritza Rodriguez, Sameer\nAntani, George R Thoma, and Clement J McDon-\nald. 2015. Preparing a collection of radiology ex-\naminations for distribution and retrieval. Journal\nof the American Medical Informatics Association ,\n23(2):304–310.\nMichael Denkowski and Alon Lavie. 2011. Meteor 1.3:\nAutomatic metric for reliable optimization and eval-\nuation of machine translation systems. In Proceed-\nings of the Sixth Workshop on Statistical Machine\nTranslation, pages 85–91, Edinburgh, Scotland. As-\nsociation for Computational Linguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2016, Las Ve-\ngas, NV , USA, June 27-30, 2016 , pages 770–778.\nIEEE Computer Society.\n2829\nGao Huang, Zhuang Liu, and Kilian Q. Weinberger.\n2016. Densely connected convolutional networks.\nCoRR, abs/1608.06993.\nJeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu,\nSilviana Ciurea-Ilcus, Chris Chute, Henrik Mark-\nlund, Behzad Haghgoo, Robyn Ball, Katie Shpan-\nskaya, et al. 2019. Chexpert: A large chest radio-\ngraph dataset with uncertainty labels and expert com-\nparison. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 33, pages 590–597.\nBaoyu Jing, Zeya Wang, and Eric Xing. 2019. Show,\nDescribe and Conclude: On Exploiting the Structure\nInformation of Chest X-ray Reports. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 6570–6580.\nBaoyu Jing, Pengtao Xie, and Eric Xing. 2018. On the\nautomatic generation of medical imaging reports. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2577–2586, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nAlistair E. W. Johnson, Tom J. Pollard, Seth J.\nBerkowitz, Nathaniel R. Greenbaum, Matthew P.\nLungren, Chih-ying Deng, Roger G. Mark, and\nSteven Horng. 2019. MIMIC-CXR: A large pub-\nlicly available database of labeled chest radiographs.\nCoRR, abs/1901.07042.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nYuan Li, Xiaodan Liang, Zhiting Hu, and Eric P\nXing. 2018. Hybrid Retrieval-Generation Rein-\nforced Agent for Medical Image Report Generation.\nIn Advances in neural information processing sys-\ntems, pages 1530–1540.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nGuanxiong Liu, Tzu-Ming Harry Hsu, Matthew Mc-\nDermott, Willie Boag, Wei-Hung Weng, Peter\nSzolovits, and Marzyeh Ghassemi. 2019. Clinically\naccurate chest x-ray report generation. In Machine\nLearning for Healthcare Conference , pages 249–\n269. PMLR.\nJustin Lovelace and Bobak Mortazavi. 2020. Learning\nto generate clinically coherent chest X-ray reports.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2020 , pages 1235–1243, On-\nline. Association for Computational Linguistics.\nJiasen Lu, Caiming Xiong, Devi Parikh, and Richard\nSocher. 2017. Knowing When to Look: Adaptive\nAttention via A Visual Sentinel for Image Caption-\ning. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 375–\n383.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nYifan Peng, Xiaosong Wang, Le Lu, M. Bagheri,\nR. Summers, and Z. Lu. 2018. Negbio: a high-\nperformance tool for negation and uncertainty detec-\ntion in radiology reports. AMIA Summits on Trans-\nlational Science Proceedings, 2018:188 – 196.\nSteven J Rennie, Etienne Marcheret, Youssef Mroueh,\nJerret Ross, and Vaibhava Goel. 2017. Self-critical\nSequence Training for Image Captioning. In Pro-\nceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, pages 7008–7024.\nKaren Simonyan and Andrew Zisserman. 2015. Very\ndeep convolutional networks for large-scale image\nrecognition. In 3rd International Conference on\nLearning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceed-\nings.\nBowen Tan, Zichao Yang, Maruan AI-Shedivat, Eric P\nXing, and Zhiting Hu. 2020. Progressive generation\nof long text. arXiv preprint arXiv:2006.15720.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and\nDumitru Erhan. 2015. Show and Tell: A Neural Im-\nage Caption Generator. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 3156–3164.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron C. Courville, R. Salakhutdinov, R. Zemel, and\nYoshua Bengio. 2015. Show, attend and tell: Neural\nimage caption generation with visual attention. In\nICML.\nYixiao Zhang, Xiaosong Wang, Ziyue Xu, Qihang Yu,\nAlan Yuille, and Daguang Xu. 2020. When radiol-\nogy report generation meets knowledge graph. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, 34(07):12910–12917.\n2830\nA Previous Models\n• ST (Vinyals et al., 2015): The model is based\non a convolution neural network that encodes\nan image into a compact representation, fol-\nlowed by a recurrent neural network that gen-\nerates a corresponding sentence. The model\nis trained to maximize the likelihood of the\nsentence given the image.\n• ATT2IN (Rennie et al., 2017): The CNN-\nRNN based model which rather than utilizing\na static, spatially pooled representation of the\nimage, it employs the attention model. The\nattention model dynamically re-weight the in-\nput spatial (CNN) features to focus on speciﬁc\nregions of the image at each time step. The\nmodel considers a modiﬁcation of the archi-\ntecture of the attention model for captioning\nin Xu et al. (2015), and input the attention-\nderived image feature only to the cell node of\nthe LSTM.\n• ADAATT (Lu et al., 2017): It is an adaptive\nattention encoder-decoder framework which\nprovides a fallback option to the decoder. At\neach time step, the model decides whether to\nattend to the image (and if so, to which re-\ngions) or to the visual sentinel. The model\ndecides whether to attend to the image and\nwhere, in order to extract meaningful informa-\ntion for sequential word generation.\n• TOPDOWN (Anderson et al., 2018): A com-\nbined bottom-up and top-down visual atten-\ntion mechanism (based on Faster R-CNN).\nThe bottom-up mechanism proposes image re-\ngions, each with an associated feature vector,\nwhile the top-down mechanism determines\nfeature weightings. The model enables atten-\ntion to be calculated more naturally at the level\nof objects and other salient regions.\n• COATT (Jing et al., 2018): A multi-task\nlearning framework which jointly performs\nthe prediction of tags and the generation of\nparagraphs. The model is based on a hierar-\nchical LSTM model and incorporates a co-\nattention mechanism to localize regions con-\ntaining abnormalities and generate narrations\nfor them.\n• HRGR (Li et al., 2018): A Hybrid Retrieval-\nGeneration Reinforced Agent consists of a\nCNN to extract visual features which is then\ntransformed into a context vector by an image\nencoders. Then a sentence decoder (RNNs-\nbased with attention mechanism) recurrently\ngenerates a sequence of hidden states which\nrepresent sentence topics. A retrieval policy\nmodule is employed to decide for each topic\nstate to either automatic generate a sentence,\nor retrieve a speciﬁc template from a template\ndatabase.\n• CMAS -RL (Jing et al., 2019): It is a LSTM\nbased framework for generating chest X-\nray imaging reports by exploiting the struc-\nture information in the reports. It explic-\nitly models the between-section structure by\na two-stage framework, and implicitly cap-\ntured the within-section structure with a Co-\noperative Multi-Agent System (CMAS) com-\nprising three agents: Planner (PL), Abnormal-\nity Writer (AW) and Normality Writer (NW).\nThe entire system was trained with REIN-\nFORCE algorithm.\n• R2GEN (Chen et al., 2020): The model\nuses ResNet as a visual backbone and gen-\nerate radiology reports with memory-driven\nTransformer, where a relational memory is de-\nsigned to record key information of the genera-\ntion process and a memory-driven conditional\nlayer normalization is applied to incorporating\nthe memory into the decoder of Transformer.\nIt obtained the state-of-the-art on two radiol-\nogy report datasets.\nB Implementation detail\nWe adopt the codebase of R2G EN4 to implement\nour proposed model. We use DenseNet121 (Huang\net al., 2016) pre-trained on CheXpert dataset with\n14-class classiﬁcation setting 5, as the visual back-\nbone to extract visual features with the dimension\n1024. For IU X-RAY , the two images are employed\nto guarantee fair comparison with previous works.\nIn ViLM component, we use the M2 TR. (Cor-\nnia et al., 2020) with 8 attention head, memory\nsize equal to 40, and 3 encoder layers and de-\ncoder layers. The model dimension is 512 with\nthe feed forward layers have a dimension of 2048.\nIn LM component, we adapt a pre-trained BART,\n4https://github.com/cuhksz-nlp/R2Gen\n5Available in https://nlp.stanford.edu/\nysmiura/ifcc/chexpert_auc14.dict.gz\n2831\ni.e., bart-base6 for generation of ﬁnal reports.\nThe model is trained with the Adam optimiser with\nbatch size of 16. The learning rates are set to 5e−5\nand 1e−4 for the visual extractor and the remain-\ning parameters, respectively. The maximum length\nin IU X-RAY is set to 60 and in MIMIC-CXR is set\nto 100. Beam search with beam size of 3 and 5 is\nused to decode texts during experiments with IU\nX-RAY and MIMIC-CXR , respectively. The hyper-\nparameters values are obtained by evaluation of\nthe model with the best BLEU-4 score using the\nvalidation set of two benchmark datasets. We train\nthe model using NVIDIA GeForce RTX 2080 Ti\nfor 100 and 30 epochs with early stopping (pa-\ntience=20) on IU X-RAY and MIMIC-CXR , respec-\ntively.\n6Available in https://huggingface.co/\nfacebook/bart-base\n2832\nFigure 2: Illustrations of reports from test dataset as REFERENCE , M2 TR. as a baseline model and M2 TR. P RO-\nGRESSIVE as a proposed model for selected X-ray chest images. Different colors highlight different medical terms\nand the detected abnormalities. The text alignments between the reference text and generated one are highlighted\nwith the same colors. Top two images are positive results, the bottom two ones are partial failure cases.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7288806438446045
    },
    {
      "name": "Computer science",
      "score": 0.6153714060783386
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5148374438285828
    },
    {
      "name": "Curriculum",
      "score": 0.5021436214447021
    },
    {
      "name": "Architecture",
      "score": 0.49169498682022095
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4766860008239746
    },
    {
      "name": "Sequence (biology)",
      "score": 0.41373181343078613
    },
    {
      "name": "Engineering",
      "score": 0.18122681975364685
    },
    {
      "name": "Electrical engineering",
      "score": 0.1651957929134369
    },
    {
      "name": "Cartography",
      "score": 0.09988585114479065
    },
    {
      "name": "History",
      "score": 0.0800984799861908
    },
    {
      "name": "Voltage",
      "score": 0.0772331953048706
    },
    {
      "name": "Psychology",
      "score": 0.07156676054000854
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210100468",
      "name": "University Hospital of Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I202697423",
      "name": "University of Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I22299242",
      "name": "Kyoto University",
      "country": "JP"
    }
  ]
}