{
  "title": "Investigation of Japanese PnG BERT Language Model in Text-to-Speech Synthesis for Pitch Accent Language",
  "url": "https://openalex.org/W4285412695",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2003761728",
      "name": "Yusuke Yasuda",
      "affiliations": [
        "Nagoya University"
      ]
    },
    {
      "id": "https://openalex.org/A1988107001",
      "name": "Tomoki Toda",
      "affiliations": [
        "Nagoya University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964243274",
    "https://openalex.org/W6754925833",
    "https://openalex.org/W2963945466",
    "https://openalex.org/W2964002616",
    "https://openalex.org/W2972831865",
    "https://openalex.org/W2972610613",
    "https://openalex.org/W3197324626",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3097800663",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2600818048",
    "https://openalex.org/W2405684687",
    "https://openalex.org/W3113217815",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6767988751",
    "https://openalex.org/W6745289305",
    "https://openalex.org/W2964034111",
    "https://openalex.org/W6742080785",
    "https://openalex.org/W2886769154",
    "https://openalex.org/W2973217961",
    "https://openalex.org/W2767052532",
    "https://openalex.org/W2963609956",
    "https://openalex.org/W3015338123",
    "https://openalex.org/W30677081",
    "https://openalex.org/W2962780374",
    "https://openalex.org/W1972978214",
    "https://openalex.org/W2791489977",
    "https://openalex.org/W6634201745",
    "https://openalex.org/W3015680182",
    "https://openalex.org/W2973043900",
    "https://openalex.org/W179875071",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W1578102511",
    "https://openalex.org/W6757699683",
    "https://openalex.org/W3016137096",
    "https://openalex.org/W3163339651",
    "https://openalex.org/W3196027980",
    "https://openalex.org/W2990883660",
    "https://openalex.org/W3081488690",
    "https://openalex.org/W3161732385",
    "https://openalex.org/W2889028433",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W3153839229",
    "https://openalex.org/W3162948689",
    "https://openalex.org/W2765486990",
    "https://openalex.org/W2892140764",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2907916773"
  ],
  "abstract": "End-to-end text-to-speech synthesis (TTS) can generate highly natural\\nsynthetic speech from raw text. However, rendering the correct pitch accents is\\nstill a challenging problem for end-to-end TTS. To tackle the challenge of\\nrendering correct pitch accent in Japanese end-to-end TTS, we adopt PnG~BERT, a\\nself-supervised pretrained model in the character and phoneme domain for TTS.\\nWe investigate the effects of features captured by PnG~BERT on Japanese TTS by\\nmodifying the fine-tuning condition to determine the conditions helpful\\ninferring pitch accents. We manipulate content of PnG~BERT features from being\\ntext-oriented to speech-oriented by changing the number of fine-tuned layers\\nduring TTS. In addition, we teach PnG~BERT pitch accent information by\\nfine-tuning with tone prediction as an additional downstream task. Our\\nexperimental results show that the features of PnG~BERT captured by pretraining\\ncontain information helpful inferring pitch accent, and PnG~BERT outperforms\\nbaseline Tacotron on accent correctness in a listening test.\\n",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 16, NO. 6, OCTOBER 2022 1319\nInvestigation of Japanese PnG BERT Language\nModel in Text-to-Speech Synthesis for Pitch\nAccent Language\nYusuke Yasuda , Member, IEEE, and Tomoki Toda , Senior Member, IEEE\nAbstract—End-to-end text-to-speech synthesis (TTS) can gen-\nerate highly natural synthetic speech from raw text. However,\nrendering the correct pitch accents is still a challenging problem\nfor end-to-end TTS. To tackle the challenge of rendering correct\npitch accent in Japanese end-to-end TTS, we adopt PnG BERT,\na self-supervised pretrained model in the character and phoneme\ndomain for TTS. We investigate the effects of features captured by\nPnG BERT on Japanese TTS by modifying the ﬁne-tuning con-\ndition to determine the conditions helpful inferring pitch accents.\nWe manipulate content of PnG BERT features from being text-\noriented to speech-oriented by changing the number of ﬁne-tuned\nlayers during TTS. In addition, we teach PnG BERT pitch accent\ninformation by ﬁne-tuning with tone prediction as an additional\ndownstream task. Our experimental results show that the features\nof PnG BERT captured by pretraining contain information help-\nful inferring pitch accent, and PnG BERT outperforms baseline\nTacotron on accent correctness in a listening test.\nIndex Terms —PnG BERT, text-to-speech, Japanese, pitch\naccent, self-supervised learning.\nI. I NTRODUCTION\nE\nND-TO-END (E2E) text-to-speech synthesis (TTS) can\ngenerate highly natural synthetic speech from raw texts [1],\n[2]. However, rendering correct pitch accents or tones, which are\naccents involving pitch change, remains a challenging problem\nfor E2E-TTS [3]–[5]. The accuracy of rendering pitch accents is\ncrucial in pitch accent languages such as Japanese, because pitch\naccents control the meaning of words. Conventional TTS sys-\ntems resolve accent information with a morphological analyzer-\nbased text front-end by looking it up in a accent dictionary, which\nis normally expensive and requires language-speciﬁc knowledge\nto construct [6]. The characteristics of E2E-TTS to use texts or\nphonemes directly as input enables it to be applied to various data\nand languages without constructing a pronunciation dictionary.\nManuscript received 14 January 2022; revised 24 April 2022 and 22 June\n2022; accepted 2 July 2022. Date of publication 13 July 2022; date of current\nversion 14 October 2022. This work was supported in part by commissioned\nby NEDO under Project JPNP20006. The guest editor coordinating the review\nof this manuscript and approving it for publication was Dr. Hung-yi Lee.\n(Corresponding author: Yusuke Yasuda.)\nThe authors are with the Information Technology Center, Nagoya Univer-\nsity, Nagoya City 4648-601, Japan (e-mail: yasuda.yusuke@g.sp.m.is.nagoya-\nu.ac.jp; tomoki@icts.nagoya-u.ac.jp).\nThis work involved human subjects or animals in its research. The author(s)\nconﬁrm(s) that all human/animal subject research procedures and protocols are\nexempt from review board approval.\nDigital Object Identiﬁer 10.1109/JSTSP.2022.3190672\nOn the other hand, the challenge of rendering a pitch accent in\nan E2E-TTS approach comes from building knowledge corre-\nsponding to accent dictionary implicitly from text and speech\npair.\nTo tackle the challenge of rendering the correct pitch accent in\nJapanese E2E-TTS, two main issues should be addressed. One\nis the low word coverage in the speech corpus. Pitch accents\nin pitch accent languages are mainly determined by words, so\na low word coverage in training speech data can result in the\nmain problem of poor pitch accents for an end-to-end TTS\nmodel. However, even a large-scale speech corpus can never\nmatch a lexicon dictionary in terms of word coverage [7]. The\nother problem is the diversity of characters. The Japanese writing\nsystem contains ideographic characters that represent meaning\nrather than pronunciation. Owing to the diversity of ideographic\ncharacters, Japanese end-to-end TTS from raw texts has been\nimpossible. Therefore, phonemes are used as input in end-to-\nend Japanese TTS instead. This results in loss of word-related\ninformation, leading to incorrect pitch accents [3], [5]. Both\nphonemes and pitch accents are important speech representa-\ntions to render in Japanese TTS, but these two representations\ndepend on different features: rendering phonemes depends on\ncharacters and phonemes, and rendering pitch accents depends\non word and syntactic features. Therefore, a framework to\ncapture both surface form and high-level features is required.\nTo tackle these two main issues, we utilize a self-supervised\npre-trained model using a large-scale text corpus. As a self-\nsupervised learning method, we use PnG BERT [8]. PnG BERT\nis an extension of BERT [9] designed for TTS as a downstream\ntask by capturing both word and phoneme contexts. We adopt\nPnG BERT for Japanese end-to-end TTS because (1) it can\ncapture both word and phoneme contexts; (2) word and phoneme\nalignment can be learned in text domain instead of the speech\ndomain; (3) a downstream task can be performed against fea-\ntures corresponding to a phoneme segment instead of a word\nsegment. Regarding (1), we can expect that features extracted\nfrom PnG BERT are helpful to render correct pitch accents,\nbecause Japanese pitch accents are mainly determined by words.\nMoreover, BERT can not only provide semantic information but\nalso syntactic information [10], [11]. Syntactic information such\nas conjugation type is also helpful inferring the fundamental\nfrequency ( Fo) in Japanese speech [12]. Concerning (2), intro-\nducing multiple soft-attention layers to TTS enables to us align\nmultiple linguistic units to speech [13] and to overcome the\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n1320 IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 16, NO. 6, OCTOBER 2022\nlack of word boundary symbols in the Japanese writing system.\nHowever, it is preferable to align multiple language units in the\ntext domain considering the limited volume of clean speech data.\nOn the other hand, PnG BERT can learn alignment between\nwords and phonemes in self-attention layers [14] by utilizing\nlarge-scale text corpus [8]. This characteristic enables feature\nextraction considering both words and phonemes for TTS. With\n(3), we can introduce tone prediction as a downstream task of\nPnG BERT in addition to TTS straightforwardly. Tone represen-\ntation of Japanese pitch accent is determined in at the syllable\nlevel. Therefore the tone can be predicted in the phoneme part of\nPnG BERT token-by-token. Naturally, we can expect that tone\nprediction is also beneﬁted from word or syntactic information\ncaptured by PnG BERT considering (1). The tone prediction is\nused to explicitly teach PnG BERT accent information in this\nwork.\nIn this study, we investigate the effects of feature contents\ncaptured by PnG BERT on Japanese TTS by modifying the\nﬁne-tuning condition. In our experiments, we manipulate the\ncontent of PnG BERT features from being text-oriented to\nspeech-oriented by changing the number of ﬁne-tuned layers\nby TTS. In addition, we inject pitch accent information itself to\nPnG BERT features by ﬁne-tuning with tone prediction as an\nadditional downstream task.\nThis paper is organized as follows. In Section II, we de-\nscribe backgrounds of BERT, PnG BERT and background of\nJapanese TTS. In Section III we introduce our proposed method\nfor Japanese TTS using PnG BERT. Section IV shows our\nexperimental results. In Section V, we summarize related works.\nFinally, In Section VI, we conclude our ﬁndings.\nII. B ACKGROUND\nA. BERT\nBERT [9] is a self-supervised learning-based language model\nfor general language representation. It has two steps to learn\ntextual representation and task-related features: pre-training and\nﬁne-tuning. BERT uses the masked language model (MLM)\nobjective during pre-training to capture general language repre-\nsentation. The MLM objective optimizes a model by predicting\nmasked tokens given input texts in which some parts of the\ntokens are randomly masked. In addition to MLM, BERT has\nthe next sentence prediction (NSP) objective, which classiﬁes\nwhether two sentences are adjacent or unrelated. During ﬁne-\ntuning, BERT uses a supervised objective to solve a speciﬁc task.\nExamples of a downstream task to ﬁne-tune BERT are natural\nlanguage processing (NLP) tasks such as language understand-\ning and question answering.\nTo use the MLM and NSP objectives, BERT arranges input\ntokens as follows: an input sequence always starts with a CLS\ntoken, which is used for a classiﬁcation task such as NSP. Each\ninput sentence ends with a SEP token, which means the end of\na sentence. As for the masking method, a portion (e.g., 15%) of\nthe input tokens are randomly chosen to be predicted as targets in\nself-supervised learning. Most of the selected tokens (e.g., 80%)\nare masked by being replaced with a MASK token Some tokens\n(e.g., 10%) are replaced with a random token, whereas some\nFig. 1. Mechanism of PnG BERT. P: phoneme, G: grapheme.\ntokens (e.g., 10%) are not replaced. A BERT model is trained to\nrecover the original sentences from the modiﬁed inputs.\nThe architecture of BERT is equivalent to that of the encoder\nof Transformer [14]. It consists of a stack of self-attention\nblocks that are a combination of a multihead self-attention layer\nand a feed-forward network with ReLU activation. Positional\nencoding is used to take the order of tokens into consideration.\nB. Png BERT\nLanguage models for TTS tasks aim to capture pronunciation-\nrelated information, whereas language models for NLP tasks\nfocus on syntactic or semantic information. PnG BERT is a self-\nsupervised learning-based language model that can capture both\ncontextual word and phoneme information in the text domain [8].\nPnG BERT is an extended method of BERT: it uses the same\nTransformer-based architecture and MLM training objective.\nPnG BERT extends BERT in terms of input representation,\nmasking strategy, and ﬁne-tuning to capture pronunciation in-\nformation.\nFig. 1 shows the structure of PnG BERT. It uses graphemes\nand phonemes as input representation instead of words. Here,\nwe use graphemes to refer to as any textual representation\nunit such as characters and subwords. A target downstream\ntask of PnG BERT is TTS. Character and phonemes are com-\nmon input representations for end-to-end TTS. The input se-\nquence for PnG BERT consists of the concatenation of phoneme\nand grapheme sequences of a corresponding text. Each of the\nphoneme and grapheme sequences ends with a SEP token. A\nCLS symbol is prepended to the whole sequence. The CLS token\nYASUDA AND TODA: INVESTIGATION OF JAPANESE PNG BERT LANGUAGE MODEL IN TEXT-TO-SPEECH SYNTHESIS 1321\nis for the sentence classiﬁcation task. Token, word, and segment\npositions are speciﬁed by positional encoding [14], because\nself-attention in Transformer is permutation invariant.\nThe masking strategy of PnG BERT is designed at the word\nlevel. Because the input representations in PnG BERT are\ngraphemes and phonemes, masking at token-level limits con-\ntextual information captured in pretraining up to surface-form\nlevel. The word-level masking enables PnG BERT to learn\nall layers of contextual information: semantic, surface form,\nand pronunciation. In PnG BERT, one of the following mask-\ning strategies is chosen for a random word: both graphemes\nand phonemes are masked; only graphemes are masked; only\nphonemes are masked; both graphemes and phonemes are kept\nintact; both graphemes and phonemes are randomly replaced\nwith other phonemes and graphemes. For evaluation, grapheme-\nto-phoneme (G2P) and phoneme-to-grapheme (P2G) masking\nstrategies may be used in addition to MLM. In the G2P masking,\nall the phoneme segments are masked. In P2G masking, all the\ngrapheme segments are masked.\nA TTS downstream task can be conducted with PnG BERT by\nfeeding its ﬁnal outputs corresponding to the phoneme segment\ninto the TTS decoder. PnG BERT models are jointly ﬁne-tuned\nwith the TTS decoder while freezing its parameters in part of\nits layers to prevent the loss of learned knowledge during pre-\ntraining. PnG BERT is compatible with any TTS decoder.\nC. Japanese TTS\nJapanese is classiﬁed as a pitch accent language in terms of\nprosodic description. Japanese pitch accents are lexical, which\nmeans that accents depend on words or a combination of words\nto form an accentual phrase. The Japanese pitch accent has an\naccentual nucleus position that is speciﬁed by a pitch fall within\nan accent phrase. The accent nucleus position is measured in a\nsyllable unit called mora. Although the pitch accent of individual\nwords can be described in a lexicon, it is affected by adjacent\nwords in an accentual phrase, which is a phenomenon called\naccent sandhi.\nThe Japanese TTS system uses text front-end for word seg-\nmentation and lexicon lookup for pronunciation by using a\nmorphological analyzer. After the lexicon lookup, accent sandhi\nis estimated on the basis of rules [15] or machine learning [16],\n[17]. The Japanese text front-end provides full-context labels\nthat include various type of information helpful to TTS such\nas part of speech. Ref. [18] shows that at least accent nucleus\nposition, or the accentual type, should be provided to TTS to\nrender a natural pitch accent.\nIII. J APANESE TTS USING PNG BERT\nFig. 2 shows our method to conduct Japanese TTS using\nPnG BERT. Its general framework is the same as that of the\nplain PnG BERT: graphemes and phonemes are concatenated\nas one sequence, and the sequence is fed to Transformer layers;\nthe PnG BERT model is pretrained with the MLM objective\nby masking a grapheme and phoneme pair at the word level;\nduring ﬁne-tuning and speech generation, the phoneme part of\nFig. 2. Mechanism of Japanese PnG BERT to perform pre-training and ﬁne-\ntuning with text-to-speech synthesis and tone prediction as downstream task.\nP: phoneme, G: grapheme.\nthe output of the last Transformer layer is used as the input for\na TTS decoder to generate a spectrogram.\nWe introduce two changes to the PnG BERT to conduct\nJapanese TTS. The ﬁrst modiﬁcation is the absence of word-\nlevel alignment information between graphemes and phonemes\nas word positions in the inputs. The Japanese writing system\nlacks clear word boundary symbols. To obtain word boundaries\nof Japanese texts used for pre-training, we use morphological\nanalyzers. Obviously, automatically derived word boundaries\nfrom the morphological analyzer contain errors. We do not\nuse word positions to extract features to avoid dependence on\nerroneous information. Thus, word-level alignment information\nis required only during pre-training to conduct word-level mask-\ning. We expect that removing word positions does not affect\nthe performance of TTS negatively as Ref. [8] suggests. We\nalso expect that masking based on erroneous word boundaries\nderived from the morphological analyzer does not have major\nnegative effects, because it just affects size of word units but does\nnot affect consistency of word alignments between graphemes\nand phonemes. It is known that the consistency of the word-\nlevel masking affects the performance of pre-training and the\ndownstream TTS task in PnG BERT [8]. We will conﬁrm these\nassumptions by measuring the G2P conversion performance of\nPnG BERT.\nThe second modiﬁcation is the introduction of tone prediction\nto TTS as an additional downstream task. Japanese pitch accents\nare an important spoken feature to disambiguate homonyms.\nUnfortunately, accent labels are not available on the scale of\nlarge corpora that are used for pre-training. Therefore, we in-\ncorporate tone prediction as one of the downstream tasks. The\n1322 IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 16, NO. 6, OCTOBER 2022\ntone prediction task is performed against the phoneme part of\nPnG BERT’s output features similar to the TTS downstream\ntask. We use tone labels to represent accent information because\nthey are aligned to phonemes in a syllable unit. Ideally, the\nrelationship between extracted features and pitch accents is to be\nlearned implicitly from the target mel-spectrogram that contains\nFo information. The tone prediction task is to explicitly teach\nPnG BERT pitch accents. We expect that ﬁne-tuning PnG BERT\nwith accent prediction makes it easy for PnG BERT to learn pitch\naccents.\nIV . E XPERIMENTAL EV ALUATIONS\nTo investigate the effects of feature contents captured by\nPnG BERT on Japanese TTS, we pre-trained a Japanese\nPnG BERT model and constructed TTS models under various\nﬁne-tuning conditions.\nA. Pre-Training Condition\nTo construct a PnG BERT model, we used a BERT model\nwith model size, so-called BERT-base [9], which consists of\n12 Transformer layers with 768 hidden size. 1 The model is\ntrained up to 11.63 M steps with 20 batch size and learning\nrate with linear decay started from 5.0 × 10−5 with the Adam\noptimizer with L2 regularization [19]. Note that the original\nPnG BERT uses the SM3 optimizer [20] instead of Adam.\nWe found that with Adam, it was easy to optimize the model,\nwhereas with SM3, it was difﬁcult to ﬁnd the proper hyper\nparameters, although it was memory-efﬁcient.\nWe randomly selected 25% words in a sentence as a target of\nthe MLM objective. For the 25% selected words, we applied the\nfollowing masking strategy: (a) both graphemes and phonemes\nare masked for 48% random words; (b) only the character part\nis masked for 8% random words; (c) only the phoneme part is\nmasked for 8% random words; (d) both characters and phonemes\nare kept intact for 10% random words; (d) both characters\nand phonemes are randomly replaced with other phonemes or\ncharacters for 10% random words. Note that (b) and (c) are not\npresent in the original PnG BERT [8]. In preliminary experiment,\nwe found that these masking strategies were very effective for\nimproving G2P and P2G performances.\nWe followed the input representations of the original\nPnG BERT paper [8]. We used character and phoneme represen-\ntations as input to PnG BERT. The two sequences were appended\nwith the SEP token and concatenated into one sequence. The\nCLS token was prepended, but we did not use the NSP objective.\nAs Japanese text corpora for pre-training, we used Wikipedia 2\nand Aozorabunko. 3 Table I shows statistics of the text corpora\nused in this study. Wikipedia is a collection of online encyclo-\npedia. We used a subset of Japanese Wikipedia that contained\nabout 18.5 M sentences. Aozorabunko is collection of Japanese\n1We also tried the original model size, which consists of 6 Transformer layers\nwith 512 hidden size. We found that ﬁne-tuning of PnG BERT with the original\nmodel size converged faster and its synthetic speech sounded similar quality as\nPnG BERT with BERT-base size.\n2[Online]. Available: https://dumps.wikimedia.org/\n3[Online]. Available: https://github.com/aozorabunko/aozorabunko/\nTABLE I\nSTATISTICS OF JAPANESE TEXT CORPUS USED IN THIS STUDY\nbooks in public the domain. We used subset of Aozorabunko\nthat contained about 4.9 M sentences. Phoneme transcriptions\nwere obtained by morphological analysis using the Kuromoji\nanalyzer4 with Neologd dictionary [21]–[23]. For validation,\nwe used text and phoneme labels from the JSUT corpus [24],\nwhich was also used to ﬁne-tune a TTS model. We used a\nsubset of JSUT corpus called basic5000, which contains\n5,000 sentences that cover the pronunciations of commonly used\nideographic characters.\nB. TTS Systems\nWe constructed ﬁve TTS systems using the same pre-trained\nPnG BERT model on the basis of different ﬁne-tuning condi-\ntions.\n1) PGB0: No ﬁne-tuning\n2) PGB2: Two-layer ﬁne-tuning\n3) PGB4: Four-layer ﬁne-tuning\n4) PGB6: Six-layer ﬁne-tuning\n5) PGB2T: Two-layer ﬁne-tuning with tone prediction in\naddition to TTS\n6) PGB2MC: Two-layer ﬁne-tuning and all graphemes are\nmasked during ﬁne-tuning and prediction.\nThe PGB0 system is equivalent to TTS using ﬁxed features\nextracted from PnG BERT. The PGB2, PGB4, and PGB6 sys-\ntems conducted ﬁne-tuning of the last two, four, and six layers\nwhile freezing the other layers, respectively. The PGB2T uses\nmulti-downstream tasks of both TTS and tone prediction. In\nall systems, only an output from the last layer was used as\ninput feature to TTS. Note that the PGB2T system did not use\ntone labels during prediction. PGB2MC was to distinguish the\ncontribution of the graphemes by masking all graphemes during\nﬁne-tuning and prediction.\nWe constructed another pre-trained BERT model that uses\nonly phonemes during pre-training, ﬁne-tuning and prediction\nas a baseline to clarify the contribution of the graphemes. In this\nsystem, pre-training, ﬁne-tuning, and prediction scheme were\nsame as PnG BERT except that all graphemes were masked out.\nWe trained phoneme BERT up to 1.0 M steps with 80 batch\nsize with linear decay started from 5.0 × 10−5 with the Adam\noptimizer with L2 regularization. We ﬁne-tuned the last two\nlayers of phoneme BERT with TTS task. We referred to this\nsystem as PB2MC.\n7) PB2MC: Phoneme BERT that uses only phonemes as\ninputs during pre-training and ﬁne-tuning. The last two\nlayers of BERT encoder are ﬁne-tuned.\nWe prepared ﬁve baseline systems with different encoder and\ninput representations.\n4[Online]. Available: https://github.com/atilika/kuromoji\nYASUDA AND TODA: INVESTIGATION OF JAPANESE PNG BERT LANGUAGE MODEL IN TEXT-TO-SPEECH SYNTHESIS 1323\n8) PGBN: Non-pretrained PnG BERT as the encoder with\ngrapheme and phoneme labels\n9) TAC: Tacotron2’s encoder with phoneme labels\n10) TACT: Tacotron2’s encoder with phoneme and tone la-\nbels\n11) BTAC2A: Tacotron2’s phoneme encoder and pre-trained\ncharacter BERT encoder. Its two sources are aligned to\ntarget with dual source attention [25].\n12) BTAC2B: Tacotron2’s phoneme encoder and pre-trained\ncharacter BERT encoder. Its two sources are aligned to\ntarget with dual source biattention [26].\nThe PGBN system used the same framework to conduct TTS\nwith PnG BERT except for skipping pre-training. It used the\nsame BERT architecture, input representation, and TTS proce-\ndure as the other ﬁve systems using PnG BERT.\nWe used Tacotron2 [1] as the baseline method, different from\nPnG BERT. We constructed two Tacotron2 models without\npre-training: TAC that uses the phoneme label and TACT that\nuses phoneme and tone labels as inputs. The phoneme and tones\nwere concatenated after being processed by the pre-net layer\nto feed to the encoder in the TACT system [27]. We used the\nsame phoneme and tone labels that were used in the ﬁne-tuning\nof TTS and tone prediction for the PnG BERT-based systems.\nThe encoder of Tacotron2 consists of three convolutional layers\nand a bidirectional LSTM layer. Note that TAC is a baseline\ncomparable to the PnG BERT systems considering that the\nPnG BERT systems did not use tone labels during prediction,\nand TACT worked as the upper bound as it used tone labels\nduring prediction.\nWe also included two Tacotron baseline systems extended\nwith pre-trained character BERT encoder. These systems com-\nbines the character BERT encoder and phoneme encoder by\ndual source attention [25]. This was a common method to\ncombine BERT with Tacotron [13], [28], and we referred to\nthe system as BTAC2A. This method did not consider alignment\nbetween the two sources, i.e. phoneme features and character\nfeatures. The BTAC2B system used dual source biattention to\nlearn alignment between the two sources [26]. With biatten-\ntion, the model learned to align the two sources, and then the\naligned sources are aligned to targets to predict outputs. Guided\nattention loss [29] were used for alignments between BERT\nfeatures and target mel-spectrogram to enforce monotonicity.\nWe used a publicly available pre-trained Japanese BERT model\nthat used characters as input representation and was trained with\nword-level masking.5 We ﬁne-tuned the last two layers of BERT\nencoder.\nAll these systems used the same decoder as that of\nTacotron2 [1]: two-layer LSTM decoder, attention layer with\nforward attention [27], and two-layer pre-net bottleneck. The\nforward attention enabled faster training by enforcing a mono-\ntonic structure of alignment [27]. The decoder pre-net layer\nprevented the decoder from excessively depending on autore-\ngressive feedback by applying dropout [30].\n5BERT-base_mecab-ipadic-char-4k_do-whole-word-mask. [Online]. Avail-\nable: https://github.com/cl-tohoku/bert-japanese\nC. Fine-Tuning Conditions\n1) Fine-Tuning With TTS Task:The TTS models were ﬁne-\ntuned with a batch size of 60 and learning rate of 1.0 × 10−6\nuntil validation loss stopped improving. The PGB0 system was\ntrained up to 488 K steps by using pre-trained PnG BERT\nparameters. The PGB2 system was trained up to 272 K steps\nby warm-starting from parameters of PGB0.T h e PGB4 system\nwas trained up to 90 K steps by warm-starting from parameters\nof PGB2.T h e PGB6 system was trained up to 142 K steps by\nwarm-starting from parameters of PGB4.T h ePGBN system was\ntrained up to 282 K steps from scratch. The PGB2MC system was\nﬁne-tuned up to 72 k steps by warm-starting from parameters\nof PGB2.T h e PB2MC system was ﬁne-tuned up to 156 k steps\nby freezing all layers and then was ﬁne-tuned up to 32 k steps\nby unfreezing the last two layers. The BTAC2A system was\nﬁne-tuned up to 80 k steps by freezing all layers and then was\nﬁne-tuned up to 20 k steps by unfreezing the last two layers. The\nBTAC2B system was ﬁne-tuned up to 64 k steps by freezing all\nlayers and then was ﬁne-tuned up to 26 k steps by unfreezing\nthe last two layers. We used the same optimizer, i.e., Adam with\nL2 regularization, to ﬁne-tune a TTS model [19].\nWe extracted an 80-dimensional mel-spectrogram from a\nwaveform with a 24 k sampling rate as a target acoustic feature.\nWe used Parallel Wave GAN [31] to generate a waveform from\nthe mel-spectrogram.\nWe used JSUT [24] as the speech corpus to ﬁne-tune TTS\nmodels. We used phoneme labels transcribed from speech. 6 We\nsplit the data into train, validation, and test sets with 5,386, 499,\nand 500 sentences, respectively.\n2) Fine-Tuning With TTS and Tone Prediction Task:To cap-\nture accent information, the PnG BERT model was ﬁne-tuned\nby the tone prediction task in the PGB2T system. In this setting,\na PnG BERT model predicts tone labels from the phoneme\nsegment of an input sequence.\nThe PGB2T system was trained up to 180 K steps by warm-\nstarting from parameters of PGB0 with the Adam optimizer\nwith L2 regularization. The ﬁne-tuning of tone prediction was\nperformed at the same time with TTS ﬁne-tuning. We found that\nthe multitask learning of TTS and tone prediction was essential\nto incorporate tone prediction to PnG BERT. Separating the\nﬁne-tuning by TTS and tone prediction tasks in two stages failed\nto learn the alignment between input features from PnG BERT\nand target speech during TTS training.\nWe used the same corpus as that of TTS, namely, JSUT cor-\npus [24] to ﬁne-tune with tone prediction. 7 The data split setting\nfor tone prediction was the same as that of TTS. We used the\nX-JToBI [32] format to represent the tone labels. The tone label\ncontains the following pitch contour patterns: L (neutral low),\nand H (neutral high), %L (neutral low at the start of accentual\nphrase), L% (neutral low at the end of accentual phrase), and\n6kana_level3. [Online]. Available: https://github.com/sarulab-speech/jsut-\nlabel\n7The accent labels. [Online]. Available: https://github.com/sarulab-speech/\njsut-label\n1324 IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 16, NO. 6, OCTOBER 2022\nA (accent nucleus position). The tone labels were aligned to\nphoneme labels.\nD. Evaluation Methods\n1) Pre-Training Evaluation:We evaluated the performance\nof pre-training with three metrics: accuracies of MLM, G2P,\nand P2G as in [8]. The accuracy of MLM was computed on the\nbasis of the MLM objective with random masking, the same\nas in the pre-training. The accuracy of G2P was computed by\nmasking the entire phoneme segment, and the accuracy of P2G\nwas computed by masking the entire grapheme segment.\n2) Objective Evaluations: We measured attention error\nrate (AER) [18], character error rate (CER), tone predic-\ntion accuracy (TA), phrase prediction accuracy (PA), and\naccentual nucleus prediction accuracy (AA) as objective\nmetrics.\nThe AER was used to evaluate how easily the feature from\nPnG BERT was aligned to speech. The AER was calculated on\nthe basis of the number of non-monotonic attention distribu-\ntions that contain discontinuities or overestimation of duration.\nWe counted discontinuities over four encoder time steps and\nduration over 30 decoder time steps as alignment errors.\nThe CER was used to evaluate the phonetic correctness of\nsynthetic speech. The CER refers to the character-level Lev-\nenshtein edit distance between reference texts and hypothesis\ntexts transcribed by automatic speech recognition (ASR). We\nused ESPNet [33] for ASR . 8\nThe TA, PA, and AA were used to evaluate the abundance of\ntone-related information in encoded features. The TA is predic-\ntion accuracy of all tone labels described in Section IV-C2. The\nPA is the prediction accuracy of start and end of accentual phrase\n(%L and L%). The AA is the prediction accuracy of accent\nnucleus (A). They were measured by conducting linear classiﬁ-\ncation against ﬁxed features from the encoder of PnG BERT or\nTacotron. In the case of the BTAC2A system, concatenation of\noutputs from phoneme encoder and BERT encoder were used as\ninputs for the tone classiﬁer. In the case of the BTAC2B system,\nconcatenation of outputs from phoneme encoder and aligned\nBERT encoder by biattention were used as inputs for the tone\nclassiﬁer.\n3) Subjective Evaluations: We conducted a listening test\nto evaluate the synthetic speech. 9 We included the ten sys-\ntems described in Section IV-B except for PGB2MC, PB2MC,\nBTAC2A, and BTAC2B in addition to natural samples ( NAT)\nand analysis-by-synthesis (ABS) in the listening test. Here, ABS\nmeant synthetic samples from neural vocoder given ground truth\nmel-spectrogram. We asked two questions to listeners. The ﬁrst\nquestion was about naturalness in the ﬁve-scale mean opinion\nscore (MOS): very bad, bad, acceptable, good, and very good.\nThe second question was about accent correctness in four-scale\nMOS: totally wrong, somewhat wrong, somewhat correct, and\ntotally correct. Every sample was evaluated ﬁve times. We\n8We used a publicly available ASR model. [Online]. Available: https://zenodo.\norg/record/4304245#.Yac84FORVqs\n9Our audio sample page. [Online]. Available: https://todalab.github.io/\nyasuda-japanese-pngbert-samples/\nFig. 3. History of pre-training accuracy of a PnG BERT model for validation\nset. MLM refers to accuracy under random masking as training time. G2P refers\nto accuracy under masking all phonemes. P2G refers to accuracy under masking\nall graphemes.\nTABLE II\nALIGNMENT ERROR RATE (AER), CHARACTER ERROR RATE (CER), TONE\nPREDICTION ACCURACY (TA), PHRASE PREDICTION ACCURACY (PA), AND\nACCENTUAL NUCLEUS PREDICTION ACCURACY (AA) OF TTS SYSTEMS.P RED.\nDENOTES PREDICTION\ncollected 25,000 evaluations in total from 248 Japanese listeners.\nWe checked the statistical signiﬁcance with the Mann-Whitney\nrank test [34].\nE. Experimental Results\n1) Pre-Training Results: Fig. 3 shows the accuracies of\nMLM, G2P, and P2G in the validation set as the performance\nof pre-training. The accuracy of MLM consistently improved\nduring pre-training up to 70.3% until overﬁt. On the other hand,\nthe accuracies of G2P and P2G ﬂuctuated during pre-training,\nwhich were 45.5% and 23.6% at the model with the highest\nMLM accuracy, respectively. It seemed that the performances\nof G2P and P2G had a trade-off relationship: when one metric\nincreased, the other decreased. The magnitudes of these values\nwere somewhat consistent with [8]. The relatively low perfor-\nmances of G2P and P2G indicated that the features captured\nby PnG BERT were not dominant in surface-form information\nwith our masking strategy of pre-training, which was expected\nto contain syntactic and semantic information [8].\n2) TTS Results: Table II shows AER, CER, TA, PA, and AA\nfrom the TTS systems. The AER and CER showed roughly the\nsame trends: the systems that had high AER also showed high\nYASUDA AND TODA: INVESTIGATION OF JAPANESE PNG BERT LANGUAGE MODEL IN TEXT-TO-SPEECH SYNTHESIS 1325\nCER. When the number of ﬁne-tuning layers in PnG BERT was\nincreased, both AER and CER improved, which indicated that\nfeatures from PnG BERT contained information more aligned\nto speech. The high AER and CER from the PGB0 system\nshowed that the pre-trained PnG BERT without ﬁne-tuning\nprovides abundant textual features and less spoken features, and\nit was consistent with the relatively low G2P accuracy in the\npre-training. The PGBN system showed high AER and CER\nalthough it ﬁne-tuned all layers in PnG BERT. This implied\nthat pre-training was essential for PnG BERT to learn stable\nalignment between phonemes and speech for TTS.\nAs for tone prediction accuracy, pre-trained PnG BERT sys-\ntems ( PGB0-6) showed high accuracies. They showed high\nPA but low AA, which indicated that the high TA values\nmainly came from rich accentual phrase information in the\nencoded features, and accentual nucleus information was not\nsufﬁciently captured in the features. The system with tone pre-\ndiction (PGB2T) had high AA, which indicating that its feature\ncontained both accentual phrase and nucleus information. The\nmasking graphemes ( PGBMC) resulted in degradation of TA\nfrom 73.1% to 63.3%. The degradation was mainly caused by\nsigniﬁcant decrease of PA from 73.5% to 38.4%, suggesting that\nphrase information depends largely on contextual information\nof graphemes rather than phonemes. The low TA and PA of\nthe PnG BERT system without pre-training ( PGBN) indicated\nthat pre-training was essential to capture information related to\naccentual phrase. Tacotron, which did not perform pre-training,\nalso failed to capture sufﬁcient information related to accentual\nphrase, as indicated by its low TA and PA. The high tone\nprediction accuracy of TACT was because tone labels were\ngiven as input. Both PnG BERT systems that use only phonemes\n(PGB2MC and PB2MC) showed low TA compared to the system\nusing both characters and phonemes ( PGB2). The performance\ndrop of TA was mainly caused by degradation of PA. This\nindicated that contextual information encoded in character part\nenriched phrase-related information. TA from these systems\nwere similar values to that of the Tacotron baselines. This\nsuggested that pre-training or ﬁne-tuning with only phonemes\ndid not help to encode tone-related information. Both Tacotron\nsystems extended with character BERT ( BTAC2A and BTAC2B)\nshowed similar values of TA, which were lower compared to TA\nfrom PnG BERT systems. We thought that the no improvement\nof TA from BERT incorporated Tacotron systems were caused by\npoor alignments between phoneme features and BERT features.\nWe found that it was hard to learn alignment between BERT\nfeatures and mel-spectrogram in both BTAC2A and BTAC2B\nsystems, and alignment between BERT features and phoneme\nfeatures in the BTAC2B system even though various techniques\nsuch as forward attention, guided attention, and biattention were\nused to enforce robust alignments. The attention distributions\nabout BERT features were mostly blurred or nonmonotonic.\nThese results were consistent with the existing works [13],\n[28]. We thought learning alignment proprly between phoneme\nfeatures and BERT features was important to predict tones from\nthe contextual features because tone labels were designed in\nphoneme-level, and high TA from PnG BERT systems came\nFig. 4. Results of listening test.\nfrom learning accurate alignments between phonemes and char-\nacters by utilizing large-scale text resources during pre-training.\nFig. 4 shows results of the listening test on (a) naturalness\nand (b) accent correctness. It showed that pre-training was\ncrucial, considering the signiﬁcant improvement of naturalness\nfrom 1.44 ± 0.02 (PGBN)t o 1.95 ± 0.03 (PGB0). Fine-tuning\nwas also essential, considering the signiﬁcant improvement of\nnaturalness from 1.95 ± 0.03 (PGB0)t o 2.64 ± 0.03 (PGB2).\nIncreasing the number of ﬁne-tuned layers slightly improved\nnaturalness from 2.64 ± 0.03 (PGB2)t o 2.77 ± 0.03 (PGB4),\nbut there was no signiﬁcant difference between the four and six\nlayers, indicating that four layers were sufﬁcient for ﬁne-tune.\nThe tone prediction did not help in improving naturalness,\nbecause there was no signiﬁcant difference between PGB2 and\nPGB2T. The PnG BERT-based systems did not match the natu-\nralness of the Tacotron that had MOS of 2.95 ± 0.03 (TAC). We\nfound that samples from the PnG BERT-based systems had lower\nﬁdelity than samples from the Tacotron systems. We consider\nthat the low ﬁdelity was caused by the difﬁculty of optimizing the\nﬁne-tuning of PnG BERT, as indicated by long ﬁne-tuning time\nrequirements and relatively high loss values. The Tacotron with\naccent labels ( TACT) had the highest MOS score of 3.28 ± 0.03\namong TTS systems as expected of upper bound system.\nFor accent correctness, pre-training and ﬁne-tuning were cru-\ncial as well. There was no signiﬁcant difference among systems\n1326 IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 16, NO. 6, OCTOBER 2022\nwith different numbers of ﬁne-tuned layers. The tone prediction\nslightly improved accent correctness from 2.41 ± 0.03 (PGB2)\nto 2.51 ± 0.03 (PGB2T). The pre-trained PnG BERT based sys-\ntems signiﬁcantly outperformed Tacotron, which had MOS of\n1.89 ± 0.03 (TAC), in terms of accent correctness in contrast to\nthe results on naturalness. This suggests that pre-trained features\nfrom PnG BERT were helpful in inferring pitch accents. The\nPnG BERT without pre-training, which showed a low score of\n1.73 ± 0.03 (PGBN), contained graphemes in inputs, so what\nhelped in inferring pitch accents was not surface form informa-\ntion but presumably syntactic and semantic information. Still,\naccents inferred by the pre-trained PnG BERT-based systems\nwere not sufﬁciently accurate compared to accent labels, as\nindicated by their lower scores than TACT, which used accent\nlabels during prediction. The Tacotron with accent labels (TACT)\nshowed relatively high MOS score of 3.04 ± 0.03, which in-\ndicated that Japanese listeners were sensitive to pitch accent.\nWe consider that the word coverage of training speech and\ntone label data used for ﬁne-tuning limited the correctness\nof inferred accents, because the only phase in which accent\nnucleus positions can be learned by PnG BERT was ﬁne-tuning\nin our method. Considering that accent nucleus positions are\nthe spoken features of words, we think that pre-training with\ntone labels or ﬁne-tuning with large-scale speech data would\nbe required to improve the accent correctness of Japanese\nPnG BERT.\nV. R ELATED WORKS\nAn early attempt to utilize an unsupervised textual represen-\ntation for TTS is the vector space model, which is an information\nretrieval method to derive linguistic feature vectors from texts by\nsingular value decomposition [35]. The objective of the vector\nspace model is to improve the language versatility of TTS by\nreplacing text front-end, which is traditionally dependent on\nmany language-speciﬁc features. However, the vector space\nmodel is not helpful for DNN-based TTS [36].\nNeural-network-based pre-training methods replace the vec-\ntor space model by using much larger text resources. The neural-\nnetwork-based pre-training is useful for TTS in ideographic\nlanguages to overcome data sparsity caused by character diver-\nsity. For example, pre-trained character embedding is used for\na front-end model in Maindarin TTS [37]. Pre-trained linguistic\nencoder is used to enable to take ideographic characters directly\nas input in Mandarin TTS by introducing G2P objective [38].\nWord embedding [39], [40] and contextual word embedding\nsuch as BERT [9] are widely used unsupervised word repre-\nsentation learning methods for general purposes. Word embed-\nding is mainly used to improve prosody of synthetic speech in\nTTS [28], [41]–[46]. Moreover, the word embedding is suitable\nfor prosody modeling in variational autoencoder (V AE)-based\nTTS [47]–[49]. Word embedding is also used to improve data\nefﬁciency under small amounts of training speech data [50].\nBERT can not only be used to extract ﬁxed contextual\nword embedding features, but can also be customized to ﬁt the\ndownstream task. There are three strategies to customize a BERT\nmodel: (1) ﬁne-tuning, (2) input representation, and (3) masking\nstrategy. Fine-tuning enables the ﬁtting of features to a down-\nstream task by tuning part of the parameters of the pre-trained\nmodel. It is known that ﬁne-tuning improves the performance of\nBERT in downstream tasks similar to pre-training methods in the\nNLP domain [51]. Recent TTS works using BERT perform ﬁne-\ntuning [8], [42], [46], [48], [49]. Some studies show that the ﬁne-\ntuning of BERT with TTS improves the naturalness of synthetic\nspeech [8], [46], [48]. In this study, we ﬁne-tuned PnG BERT\nwith TTS and tone prediction and conﬁrmed the effectiveness of\nﬁne-tuning.\nThe design of the input representation of BERT for TTS\nincludes words, subwords, characters, phonemes, and speech.\nPre-training in the word domain obtains semantic information,\nand pre-training in the character domain obtains surface infor-\nmation, and pre-training in the subword domain is considered\nbetween them. Words [45], [45] and subwords [42], [46]–[49]\nare commonly used representations for BERT. In [44], characters\nwere used as input to BERT for Mandarin TTS. PnG BERT [8]\ncombines both characters and phonemes to pre-train BERT\nto capture both surface and phoneme information to apply to\nTTS. This idea to combine subwords and phonemes is also\ninvestigated in spoken language understanding [52]. In [53],\nmel-spectrogram was used to pre-train BERT, which is used\nas a pre-net network [30] in TTS.\nThe masking strategy can be designed speciﬁcally for a down-\nstream task. PnG BERT [8] applies word-level masking on both\ncharacter and phoneme pairs. [52] uses “one-mode masking”\nwas used on subwords and phonemes, which is to mask the\nentire subword or phoneme segment equivalent to the G2P or\nP2G task.\nVI. C ONCLUSION\nIn this work, we investigated the effects of features captured\nby PnG BERT on Japanese TTS by modifying the ﬁne-tuning\ncondition to determine the conditions helpful in rendering pitch\naccents. We manipulated the content of PnG BERT features\nfrom being text-oriented to speech-oriented by changing the\nnumber of ﬁne-tuned layers during TTS. In addition, we taught\nPnG BERT pitch accent information by ﬁne-tuning with tone\nprediction as an additional downstream task. Our experiment\nshowed that pre-training and ﬁne-tuning were essential for both\nnaturalness and accent correctness for Japanese TTS. Increas-\ning the number of ﬁne-tuned layers improved alignment and\ncharacter error rates, but did not contribute considerably to\nnaturalness. PnG BERT provided better accent correctness than\nthe Tacotron baseline, although all the PnG BERT-based systems\nwere inferior to Tacotron in terms of naturalness because of\ntheir low ﬁdelity. Fine-tuning with tone prediction considerably\nimproved tone prediction accuracies from captured features, but\nits improvements were limited in the subjective evaluation of\naccent correctness. Overall, the features of PnG BERT captured\nby pre-training contained information helpful in inferring pitch\naccent, and ﬁne-tuning by TTS enriched pitch accent informa-\ntion for the PnG BERT features.\nYASUDA AND TODA: INVESTIGATION OF JAPANESE PNG BERT LANGUAGE MODEL IN TEXT-TO-SPEECH SYNTHESIS 1327\nOur future works include the improvement of the low ﬁdelity\nof generated speech and long ﬁne-tuning time of PnG BERT.\nIt would be interesting to ﬁnd the optimal masking strategy\nto obtain higher ﬁdelity and ﬁne-tuning efﬁciency of Japanese\nPnG BERT by changing the masking strategy to control the\nbalance of surface form, syntactic, and semantic information.\nWe will also work on the further improvement of Japanese\nPnG BERT to infer pitch accents. One interesting way to improve\nthe capture of pitch accent features is pre-training PnG BERT\nwith accent labels instead of ﬁne-tuning by tone prediction. An-\nother interesting way is to ﬁne-tune PnG BERT with large-scale\nspeech data.\nREFERENCES\n[1] J. Shen et al., “Natural TTS synthesis by conditioning WaveNet on MEL\nspectrogram predictions,” in Proc. IEEE Int. Conf. Acoust., Speech Signal\nProcess., 2018, pp. 4779–4783.\n[2] N. Li, S. Liu, Y . Liu, S. Zhao, M. Liu, and M. Zhou, “Close to Human\nquality tts with transformer,” 2018, arXiv:1809.08895v3.\n[3] Y . Yasuda, X. Wang, S. Takaki, and J. Yamagishi, “Investigation of\nenhanced tacotron text-to-speech synthesis systems with self-attention for\npitch accent language,” in Proc. IEEE Int. Conf. Acoust., Speech Signal\nProcess., 2019, pp. 6905–6909.\n[4] B. Li, Y . Zhang, T. Sainath, Y . Wu, and W. Chan, “Bytes are all you\nneed: End-to-end multilingual speech recognition and synthesis with\nbytes,” in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2019,\npp. 5621–5625.\n[5] T. Fujimoto, K. Hashimoto, K. Oura, Y . Nankaku, and K. Tokuda, “Im-\npacts of input linguistic feature representation on Japanese end-to-end\nspeech synthesis,” in Proc. 10th ISCA Workshop Speech Synth., 2019,\npp. 166–171.\n[6] The HTS Working Group, “The Japanese TTS system ‘Open JTalk’,” 2015.\n[Online]. Available: http://open-jtalk.sourceforge.net/\n[7] J. Taylor and K. Richmond, “Analysis of pronunciation learning in end-\nto-end speech synthesis,” in Proc. Interspeech, 2019, pp. 2070–2074.\n[8] Y . Jia, H. Zen, J. Shen, Y . Zhang, and Y . Wu, “PnG BERT: Augmented\nBERT on phonemes and graphemes for neural TTS,” in Proc. Interspeech,\n2021, pp. 151–155.\n[9] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics: Hum. Lang.\nTechnol., J. Burstein, C. Doran, and T. Solorio, Eds., 2019, vol. 1,\npp. 4171–4186.\n[10] G. Jawahar, B. Sagot, and D. Seddah, “What does BERT learn about the\nstructure of language?,” in Proc. 57th Conf. Assoc. Comput. Linguistics,\nL. A. Papers, D. Korhonen, R. Traum, and L. Má rquez, Eds., 2019,\npp. 3651–3657.\n[11] I. Tenney, D. Das, and E. Pavlick, “BERT rediscovers the classical NLP\npipeline,” in Proc. 57th Conf. Assoc. Comput. Linguistics, L. A. Papers,\nD. Korhonen, R. Traum, and L. Má rquez, Eds., 2019, pp. 4593–4601.\n[12] X. Wang, “Fundamental frequency modeling for neural-network-based\nstatistical parametric speech synthesis,” Ph.D. dissertation, Dept. Infor-\nmat., SOKENDAI, Hayama, Japan, 2018.\n[13] M. Aso, S. Takamichi, and H. Saruwatari, “End-to-end text-to-speech\nsynthesis with unaligned multiple language units based on attention,” in\nProc. Interspeech, 2020, pp. 4009–4013.\n[14] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf.\nProcess. Syst., 2017, pp. 6000–6010.\n[15] Y . Sagisaka and H. Sato, “Accentuation rules for Japanese word concate-\nnation,”IEICE Trans. Inf. Syst., vol. 66, no. 7, pp. 849–856, 1983.\n[16] M. SUZUKI et al., “Accent sandhi estimation of Tokyo dialect of Japanese\nusing conditional random ﬁelds,” IEICE Trans. Inf. Syst., vol. 100, no. 4,\npp. 655–661, 2017.\n[17] M. Nobuaki, K. Ryo, H. Keikichi, and W. Michiko, “CRF-based statistical\nlearning of Japanese accent sandhi for developing Japanese text-to-speech\nsynthesis systems,” in Proc. 6th ISCA Speech Synth. Workshop, 2007,\npp. 148–153.\n[18] Y . Yasuda, X. Wang, and J. Yamagishi, “Investigation of learning abilities\non linguistic features in sequence-to-sequence text-to-speech synthesis,”\nComput. Speech Lang., vol. 67, 2021, Art. no. 101183.\n[19] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nCoRR, 2014, arXiv:1412.6980.\n[20] R. V . Anil, T. Gupta Koren, and Y . Singer, “Memory efﬁcient adap-\ntive optimization,” in Proc. Adv. Neural Inf. Process. Syst. 32: Annu.\nConf. Neural Inf. Process. Syst., H. M. H. Wallach, A. Larochelle,\nF. Beygelzimer d’ E. B. Alché-Buc Fox, and R. Garnett, Eds., 2019,\npp. 9746–9755.\n[21] S. Toshinori, “Neologism dictionary based on the language resources\non the web for Mecab,” 2015. [Online]. Available: https://github.com/\nneologd/mecab-ipadic-neologd\n[22] T. H. T. Sato and M. Okumura, “Operation of a word segmentation\ndictionary generation system called NEologd,” (in Japanese) Inf. Pro-\ncess. Soc. Japan, Special Int. Group Natural Lang. Process., 2016,\npp. 1–14.\n[23] T. H. T. Sato and M. Okumura, “Implementation of a word seg-\nmentation dictionary called mecab-ipadic-NEologd and study on how\nto use it effectively for information retrieval,” (in Japanese) in\nProc. 23rd Annu. Meeting Assoc. Natural Lang. Process. , 2017,\npp. NLP2017–B6-1.\n[24] R. Sonobe, S. Takamichi, and H. Saruwatari, “JSUT corpus: Free large-\nscale Japanese speech corpus for end-to-end speech synthesis,” 2017,\narXiv:1711.00354v1.\n[25] B. Zoph and K. Knight, “Multi-source neural translation,” in Proc. NAACL-\nHLT, 2016, pp. 30–34.\n[26] B. McCann and R. Socher, “Learned in translation: Contextualized word\nvectors,” in Proc. Adv. Neural Inf. Process. Syst. 30: Annu. Conf. Neural\nInf. Process. Syst., U. von Garnett, Ed., 2017, pp. 6294–6305.\n[27] J.-X. Zhang, Z.-H. Ling, and L.-R. Dai, “Forward attention in sequence-\nto-sequence acoustic modeling for speech synthesis,” in Proc. Int. Conf.\nAcoust., Speech Signal Process., 2018, pp. 4789–4793.\n[28] T. Hayashi, S. Watanabe, T. Toda, K. Takeda, S. Toshniwal, and K. Livescu,\n“Pretrained text embeddings for enhanced text-to-speech synthesis,” in\nProc. 20th Annu. Conf. Int. Speech Commun. Assoc., G. Kubin and Z.\nKacic, Eds., 2019, pp. 4430–4434.\n[29] H. Tachibana, K. Uenoyama, and S. Aihara, “Efﬁciently trainable text-\nto-speech system based on deep convolutional networks with guided\nattention,” inProc. IEEE Int. Conf. Acoust., Speech Signal Process., 2018,\npp. 4784–4788.\n[30] Y . Wang et al., “Tacotron: Toward end-to-end speech synthesis,” in Proc.\nInterspeech, 2017, pp. 4006–4010.\n[31] R. Yamamoto, E. Song, and J. Kim, “Parallel waveGAN: A fast waveform\ngeneration model based on generative adversarial networks with multires-\nolution spectrogram,” in Proc. IEEE Int. Conf. Acoust., Speech Signal\nProcess., 2020, pp. 6199–6203.\n[32] K. MAEKAWA, “X-JTOBI : An extended J-TOBI for spontaneous\nspeech,” in Proc. 7th ICSLP, 2002, pp. 1545–1548.\n[33] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,”\nin Proc.19th Annu. Conf. Int. Speech Commun. Assoc. , 2018,\npp. 2207–2211.\n[34] H. B. Mann and D. R. Whitney, “On a test of whether one of two random\nvariables is stochastically larger than the other,”Ann. Math. Statist., vol. 18,\nno. 1, pp. 50–60, 1947.\n[35] O. S. Watts, “Unsupervised learning for text-to-speech synthesis,” Ph.D.\ndissertation, Univ. Edinburgh, Edinburgh, Scotland, 2013.\n[36] H. Lu, S. King, and O. Watts, “Combining a vector space representation of\nlinguistic context with a deep neural network for text-to-speech synthesis,”\nin Proc. 8th ISCA Workshop Speech Synth., 2013, pp. 261–265.\n[37] J. Pan et al., “A uniﬁed sequence-to-sequence front-end model for man-\ndarin text-to-speech synthesis,” in Proc. IEEE Int. Conf. Acoust., 2020,\npp. 6689–6693.\n[38] J. Li, Z. Wu, R. Li, P. Zhi, S. Yang, and H. Meng, “Knowledge-based\nlinguistic encoding for end-to-end mandarin text-to-speech synthesis,” in\nProc. Interspeech, 2019, pp. 4494–4498.\n[39] T. Mikolov, M. Karaﬁát, L. Burget, J. Cernocký, and S. Khudanpur,\n“Recurrent neural network based language model,” in Proc., 11th Ann.\nConf. Int. Speech Commun. Assoc., 2010, pp. 1045–1048.\n[40] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation\nof word representations in vector space,” in Proc. 1st Int. Conf. Learn.\nRepresentations, 2013, pp. 2–4.\n[41] P. Wang, Y . Qian, F. K. Soong, L. He, and H. Zhao, “Word embedding for\nrecurrent neural network based TTS synthesis,” in Proc. IEEE Int. Conf.\nAcoust., 2015, pp. 4879–4883.\n[42] W. Fang, Y . Chung, and J. R. Glass, “Toward transfer learning for end-\nto-end speech synthesis from deep pre-trained language models,” 2019,\narXiv:1906.07307v1.\n1328 IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 16, NO. 6, OCTOBER 2022\n[43] H. Ming, L. He, H. Guo, and F. K. Soong, “Feature reinforcement\nwith word embedding and parsing information in neural TTS,” 2019,\narXiv:1901.00707v2.\n[44] Y . Xiao, L. He, H. Ming, and F. K. Soong, “Improving prosody with lin-\nguistic and bert derived features in multi-speaker based mandarin chinese\nneural TTS,” in Proc. IEEE Int. Conf. Acoust., 2020, pp. 6704–6708.\n[45] G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou, “Improving\nprosody modelling with cross-utterance bert embeddings for end-to-end\nspeech synthesis,” in Proc. IEEE Int. Conf. Acoust., 2021, pp. 6079–6083.\n[46] W. Nakata et al., “Audiobook speech synthesis conditioned by cross-\nsentence context-aware word embeddings,” in Proc. 11th ISCA Speech\nSynth. Workshop(SSW 11), 2021, pp. 211–215.\n[47] S. Tyagi, M. Nicolis, J. Rohnke, T. Drugman, and J. Lorenzo-Trueba, “Dy-\nnamic prosody generation for speech synthesis using linguistics-driven\nacoustic embedding selection,” in Proc. 21st Annu. Conf. Int. Speech\nCommun. Assoc., 2020, pp. 4407–4411.\n[48] T. Kenter, M. Sharma, and R. Clark, “Improving the prosody of RNN-\nbased english text-to-speech synthesis by incorporating a BERT model,”\nin Proc. 21st Annu. Conf. Int. Speech Commun. Assoc., Virtual Event,\n2020, pp. 4412–4416.\n[49] S. Karlapati, “Prosodic representation learning and contextual sampling\nfor neural text-to-speech,” in Proc. IEEE Int. Conf. Acoust. , 2021,\npp. 6573–6577.\n[50] Y . Chung, Y . Wang, W. Hsu, Y . Zhang, and R. J. Skerry-Ryan, “Semi-\nsupervised training for improving data efﬁciency in end-to-end speech\nsynthesis,” in Proc. IEEE Int. Conf. Acoust., 2019, pp. 6940–6944.\n[51] M. E. Peters, S. Ruder, and N. A. Smith, “To tune or not to tune? Adapt-\ning pretrained representations to diverse tasks,” in Proc. 4th Workshop\nRepresentation Learn.I. S. Augenstein et al., Eds., 2019, pp. 7–14.\n[52] Q. Chen, W. Wang, and Q. Zhang, “Pre-training for spoken language\nunderstanding with joint textual and phonetic representation learning,”\nin Proc. Interspeech, 2021, pp. 1244–1248.\n[53] L. Chen, Y . Deng, X. Wang, F. K. Soong, and L. He, “Speech bert\nembedding for improving prosody in neural TTS,” in Proc. IEEE Int.\nConf. Acoust., Speech Signal Process., 2021, pp. 6563–6567.\nYusuke Yasuda (Member, IEEE) received the B.S.\nand M.S. degrees from Waseda University, Tokyo,\nJapan, in 2012 and 2014, respectively, and the Ph.D.\ndegree from the Graduate university for Advanced\nStudies, SOKENDAI, Hayama, Japan, in 2021. From\n2018 to 2021, he was a Research Assistant with the\nNational Institute of Informatics, Tokyo, Japan. Since\n2021, he has been an Assistant Project Professor with\nthe Information Technology Center, Nagoya Univer-\nsity, Nagoya, Japan. His research interests include\nstatistical machine learning and speech synthesis. He\nwas the recipient of the 20th Best Student Paper Award from the Acoustical\nSociety of Japan.\nTomoki Toda (Senior Member, IEEE) received the\nB.E. degree from Nagoya University, Tokyo, Japan,\nin 1999 and the M.E. and D.E. degrees from Nara\nInstitute of Science and Technology (NAIST), Ikoma,\nJapan, in 2001 and 2003, respectively. From 2003\nto 2005, he was a Research Fellow of the Japan\nSociety for the Promotion of Science. Then, he was\nan Assistant Professor from 2005 to 2011 and an\nAssociate Professor from 2011 to 2015 with NAIST.\nSince 2015, he has been a Professor with the Informa-\ntion Technology Center, Nagoya University, Nagoya,\nJapan. His research focuses on statistical approaches to sound media information\nprocessing. He was the recipient of more than 15 article/achievement awards,\nincluding the IEEE SPS 2009 Young Author Best Paper Award and the 2013\nEURASIP-ISCA Best Paper Award (Speech Communication Journal).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7359158396720886
    },
    {
      "name": "Stress (linguistics)",
      "score": 0.672467827796936
    },
    {
      "name": "Speech recognition",
      "score": 0.5805445909500122
    },
    {
      "name": "Speech synthesis",
      "score": 0.5205976963043213
    },
    {
      "name": "Natural language processing",
      "score": 0.5034508109092712
    },
    {
      "name": "Pitch accent",
      "score": 0.47608938813209534
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4322904050350189
    },
    {
      "name": "Linguistics",
      "score": 0.3262982964515686
    },
    {
      "name": "Prosody",
      "score": 0.15614047646522522
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60134161",
      "name": "Nagoya University",
      "country": "JP"
    }
  ],
  "cited_by": 14
}