{
  "title": "BERT Prescriptions to Avoid Unwanted Headaches: A Comparison of Transformer Architectures for Adverse Drug Event Detection",
  "url": "https://openalex.org/W3153955816",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3046205122",
      "name": "Beatrice Portelli",
      "affiliations": [
        "University of Udine"
      ]
    },
    {
      "id": "https://openalex.org/A3154824318",
      "name": "Edoardo Lenzi",
      "affiliations": [
        "University of Udine"
      ]
    },
    {
      "id": "https://openalex.org/A791988515",
      "name": "Emmanuele Chersoni",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2152709786",
      "name": "Giuseppe Serra",
      "affiliations": [
        "University of Udine"
      ]
    },
    {
      "id": "https://openalex.org/A2483174281",
      "name": "Enrico Santus",
      "affiliations": [
        "Bayer (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2972677859",
    "https://openalex.org/W3120476983",
    "https://openalex.org/W2131546905",
    "https://openalex.org/W2322190778",
    "https://openalex.org/W2972736338",
    "https://openalex.org/W3023337184",
    "https://openalex.org/W2171469118",
    "https://openalex.org/W2973016322",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2739746722",
    "https://openalex.org/W3099268449",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2532655604",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2071478164",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2148488766",
    "https://openalex.org/W2973038827",
    "https://openalex.org/W3087966870",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2955854688",
    "https://openalex.org/W2102742655",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2972833053",
    "https://openalex.org/W2886093254",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W141818611"
  ],
  "abstract": "Pretrained transformer-based models, such as BERT and its variants, have become a common choice to obtain state-of-the-art performances in NLP tasks. In the identification of Adverse Drug Events (ADE) from social media texts, for example, BERT architectures rank first in the leaderboard. However, a systematic comparison between these models has not yet been done. In this paper, we aim at shedding light on the differences between their performance analyzing the results of 12 models, tested on two standard benchmarks. SpanBERT and PubMedBERT emerged as the best models in our evaluation: this result clearly shows that span-based pretraining gives a decisive advantage in the precise recognition of ADEs, and that in-domain language pretraining is particularly useful when the transformer model is trained just on biomedical text from scratch.",
  "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1740–1747\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n1740\nBERT Prescriptions to Avoid Unwanted Headaches: A Comparison of\nTransformer Architectures for Adverse Drug Event Detection\nBeatrice Portelli1 Edoardo Lenzi1 Emmanuele Chersoni2\nGiuseppe Serra1 Enrico Santus3\n1AILAB UniUd - University of Udine, Italy\n2The Hong Kong Polytechnic University\n3Decision Science and Advanced Analytics for MAPV & RA, Bayer\n{portelli.beatrice,lenzi.edoardo}@spes.uniud.it\nemmanuele.chersoni@polyu.edu.hk\ngiuseppe.serra@uniud.it, enrico.santus@bayer.com\nAbstract\nPretrained transformer-based models, such as\nBERT and its variants, have become a com-\nmon choice to obtain state-of-the-art perfor-\nmances in NLP tasks. In the identiﬁcation\nof Adverse Drug Events (ADE) from social\nmedia texts, for example, BERT architectures\nrank ﬁrst in the leaderboard. However, a sys-\ntematic comparison between these models has\nnot yet been done. In this paper, we aim at\nshedding light on the differences between their\nperformance analyzing the results of 12 mod-\nels, tested on two standard benchmarks.\nSpanBERT and PubMedBERT emerged as\nthe best models in our evaluation: this re-\nsult clearly shows that span-based pretraining\ngives a decisive advantage in the precise recog-\nnition of ADEs, and that in-domain language\npretraining is particularly useful when the\ntransformer model is trained just on biomedi-\ncal text from scratch.\n1 Introduction\nThe identiﬁcation of Adverse Drug Events (ADEs)\nfrom text recently attracted a lot of attention in the\nNLP community. On the one hand, it represents a\nchallenge even for the most advanced NLP tech-\nnologies, since mentions of ADEs can be found in\ndifferent varieties of online text and present uncon-\nventional linguistic features (they may involve spe-\ncialized language, or consist of discontinuous spans\nof tokens etc.) (Dai, 2018). On the other hand, the\ntask has an industrial application of primary im-\nportance in the ﬁeld of digital pharmacovigilance\n(Sarker et al., 2015; Karimi et al., 2015b).\nThis raising interest is attested, for example, by\nthe ACL workshop series on Social Media Health\nMining (SMM4H), in which shared tasks on ADE\ndetection have been regularly organized since 2016\n(Paul et al., 2016; Sarker and Gonzalez-Hernandez,\n2017; Weissenbacher et al., 2018, 2019). With\nthe recent introduction of Transformers architec-\ntures and their impressive achievements in NLP\n(Vaswani et al., 2017; Devlin et al., 2019), it is not\nsurprising that these tools have become a common\nchoice for the researchers working in the area.\nThe contribution of this paper is a comparison\nbetween different Transformers on ADE detection,\nin order to understand which one is the most ap-\npropriate for tackling the task. Shared tasks are not\nthe best scenario for addressing this question, since\nthe wide range of differences in the architectures\n(which could include, for example, ensembles of\nTransformers and other types of networks) does\nnot allow a comparison on the same grounds. In\nour view, two key questions deserve a particular\nattention in this evaluation. First, whether there\nis an advantage in using a model with some form\nof in-domain language pretraining, given the wide\navailability of Transformers for the biomedical do-\nmain (Lee et al., 2020; Gu et al., 2020). Second,\nwhether a model trained to predict coherent spans\nof textinstead of single words can achieve a better\nperformance (Joshi et al., 2019), since our goal is\nto identify the groups of tokens corresponding to\nADEs as precisely as possible.\nTwo models that we introduce for the ﬁrst time in\nthis task, SpanBERT and PubMedBERT, achieved\nthe top performance. The former takes advantage\nof a span-based pretraining objective, while the\nlatter shows that in-domain language data are better\nused for training the model from scratch, without\nany general-domain pretraining.\n2 Related Work\n2.1 ADE Detection\nAutomatic extraction of ADE in social media\nstarted receiving more attention in the last few\n1741\nyears, given the increasing number of users that\ndiscuss their drug-related experiences on Twitter\nand similar platforms. Studies like Sarker and Gon-\nzalez (2015); Nikfarjam et al. (2015); Daniulaityte\net al. (2016) were among the ﬁrst to propose ma-\nchine learning systems for the detection of ADE in\nsocial media texts, using traditional feature engi-\nneering and word embeddings-based approaches.\nWith the introduction of the SMM4H shared\ntask, methods based on neural networks became\na more and more common choice for tackling the\ntask (Wu et al., 2018; Nikhil and Mundra, 2018),\nand ﬁnally, it was the turn of Transformer-based\nmodels such as BERT (Devlin et al., 2019) and\nBioBERT (Lee et al., 2020), which are the building\nblocks of most of the top performing systems in\nthe recent competitions (Chen et al., 2019; Mahata\net al., 2019; Miftahutdinov et al., 2019).\nAt the same time, the task has been indepen-\ndently tackled also by researchers in Named Entity\nRecognition, since ADE detection represents a clas-\nsical case of a challenging task where the entities\ncan be composed by discontinuous spans of text\n(Stanovsky et al., 2017; Dai et al., 2020; Wunnava\net al., 2020).\n2.2 Transformers Architectures in NLP\nThere is little doubt that Transformers (Vaswani\net al., 2017) have been the dominant class of NLP\nsystems in the last few years. The “golden child” of\nthis revolution is BERT (Devlin et al., 2019), which\nwas the ﬁrst system to apply the bidirectional train-\ning of a Transformer to a language modeling task.\nMore speciﬁcally, BERT is trained with a Masked\nLanguage Modeling objective: random words in\nthe input sentences are replaced by a [MASK] to-\nken and the model attempts to predict the masked\ntoken based on the surrounding context.\nFollowing BERT’s success, several similar archi-\ntectures have been introduced in biomedical NLP,\nproposing different forms of in-domain training\nor using different corpora (Beltagy et al., 2019;\nAlsentzer et al., 2019; Lee et al., 2020; Gu et al.,\n2020). Some of them already proved to be efﬁcient\nfor ADE detection: for example, the top system of\nthe SMM4H shared task 2019 is based on an en-\nsemble of BioBERTs (Weissenbacher et al., 2019).\nAnother potentially interesting addition to the\nlibrary of BERTs for ADE detection is SpanBERT\n(Joshi et al., 2019). During the training of Span-\nBERT, random contiguous spans of tokens are\nmasked, rather than individual words, forcing the\nmodel to predict the full span from the tokens at its\nboundaries. We decided to introduce SpanBERT\nin our experiments because longer spans and re-\nlations between multiple spans of text are a key\nfactor in ADE detection, and thus encoding such\ninformation is potentially an advantage.\n3 Experimental Settings\n3.1 Datasets\nThe datasets chosen for the experiments are two\nwidely used benchmarks. They are annotated for\nthe presence of ADEs at character level: each docu-\nment is accompanied by list of start and end indices\nfor the ADEs contained in it. We convert these an-\nnotations using the IOB annotation scheme for the\ntokens: B marks the start of a mention, I and O the\ntokens inside and outside a mention respectively.\nCADEC (Karimi et al., 2015a) contains 1250\nposts from the health-related forum “AskaPatient”,\nannotated for the presence of ADEs. We use the\nsplits made publicly available by Dai et al. (2020).\nSMM4H is the training dataset for Task 2 of the\nSMM4H shared task 2019 (Weissenbacher et al.,\n2019). It contains 2276 tweets which mention at\nleast one drug name, 1300 of which are positive\nfor the presence of ADEs while the other 976 are\nnegative samples. The competition includes a blind\ntest set, but in order to perform a deeper analysis on\nthe results, we use the training set only. As far as\nwe know there is no ofﬁcial split for the training set\nalone, so we partitioned it into training, validation\nand test sets (60:20:20), maintaining the propor-\ntions of positive and negative samples. This split\nand the code for all the experiments are available\nat https://github.com/AilabUdineGit/ADE.\nThe datasets correspond to different text genres:\nthe tweets of SMM4H are mostly short messages,\ncontaining informal language, while the texts of\nCADEC are longer and structured descriptions. To\nverify this point, we used the TEXTSTAT Python\npackage to extract some statistics from the texts of\nthe two datasets (see Appendix A).\n3.2 Metrics\nAs evaluation metrics we use the Strict F1 score,\nwhich is commonly adopted for this task (Segura-\nBedmar et al., 2013). It is computed at the entity\nlevel, and assigns a hit only in case of perfect match\nbetween the labels assigned by the model and the\nlabels in the gold annotation.\n1742\nIn CADEC around 10% of mentions are discon-\ntinuous (Dai et al., 2020) and it is possible to have\noverlaps and intersections of discontinuous spans.\nWe performed data tidying by merging overlapping\nADE mentions, keeping only the longer span (as it\nis customary in the literature) and splitting discon-\ntinuous spans in multiple continuous spans.\n3.3 Overview of the Models\n3.3.1 Pretrained BERT Variants\nApart from the original BERT, we experimented\nwith SpanBERT, for its peculiar pretraining pro-\ncedure which focuses on predicting and encoding\nspans instead of single words, and with four BERT\nvariants with in-domain knowledge, which differ\nfrom each other both for the corpus they were\ntrained on and for the kind of pretraining.\nBERT Standard model, pretrained on general\npurpose texts (Wikipedia and BookCorpus).\nSpanBERT This model is pretrained using the\nsame corpus as the original BERT, so it comes\nwith no in-domain knowledge. But the pretraining\nprocedure makes its embeddings more appropriate\nfor NER-like tasks. as it introduces an additional\nloss called Span Boundary Objective (SBO), along-\nside the traditional Masked Language Modelling\n(MLM) used for BERT.\nLet us consider a sentence S = [w1, w2, . . . , wk]\nand its substring Sm:n = [wm, . . . , wn]. wm−1 and\nwn+1 are the boundaries of Sm:n (the words imme-\ndiately preceding and following it). We mask S by\nreplacing all the words in Sm:n with the [MASK]\ntoken. SpanBERT reads the masked version of\nS and returns an embedding for each word. The\nMLM loss measures if it is possible to reconstruct\neach original word wi ∈Sm:n from the correspond-\ning embedding. The SBO loss measures if it is pos-\nsible to reconstruct each wi ∈Sm:n using the em-\nbeddings of the boundary words wm−1 and wn+1.\nBioBERT (Lee et al., 2020), pretrained from a\nBERT checkpoint, on PubMed abstracts.\nThe authors of BioBERT provide different versions\nof the model, pretrained on different corpora. We\nselected the version which seemed to have the great-\nest advantage on this task, according to the results\nby Lee et al. (2020). We chose BioBERT v1.1\n(+PubMed), which outperformed other BioBERT\nv1.0 versions (including the ones trained on full\ntexts) in NER tasks involving Diseases and Drugs.\nPreliminary experiments against BioBERT v.1.0\n(+PubMed+PMC) conﬁrmed this behaviour (see\nAppendix D).\nBioClinicalBERT (Alsentzer et al., 2019), pre-\ntrained from a BioBERT checkpoint, on clinical\ntexts from the MIMIC-III database.\nSciBERT (Beltagy et al., 2019), pretrained from\nscratch, on papers retrieved from Semantic Scholar\n(82% of medical domain).\nPubMedBERT (Gu et al., 2020), pretrained\nfrom scratch, on PubMed abstracts and full text arti-\ncles from PubMed Central. This model was created\nto prove that pretraining from scratch on a single\ndomain produces substantial gains on in-domain\ndownstream tasks. Gu et al. (2020) compared\nit with various other models pretrained on either\ngeneral texts, mixed-domain texts or in-domain\ntexts starting from a general-purpose checkpoint\n(e.g. BioBERT), showing that PubMedBERT out-\nperforms them on several tasks based on medical\nlanguage. The vocabulary of PubMedBERT con-\ntains more in-domain medical words than any other\nmodel under consideration. However, it should be\nkept in mind that ADE detection requires an un-\nderstanding of both medical terms and colloquial\nlanguage, as both can occur in social media text.\nNotice that two in-domain architectures were\npretrained from scratch (SciBERT and PubMed-\nBERT), meaning that they have a unique vocab-\nulary tailored on their pretraining corpus, and in-\nclude speciﬁc embeddings for in-domain words.\nBioBERT and BioClinicalBERT were instead pre-\ntrained starting from a BERT and BioBERT check-\npoint, respectively. This means that the vocabular-\nies are built from general-domain texts (similarly to\nBERT) and the embeddings are initialized likewise.\n3.3.2 Simple and CRF Architecture\nFor all of the BERT variants, we take into account\ntwo versions. The ﬁrst one simply uses the model\nto generate a sequence of embeddings (one for each\nsub-word token), which are then passed to a Linear\nLayer + Softmax to project them to the output space\n(one value for each output label) and turn them into\na probability distribution over the labels.\nThe second version combines the Transformer-\nbased model with a Conditional Random Field\n(CRF) classiﬁer (Lafferty et al., 2001; Papay et al.,\n2020). The outputs generated by the ﬁrst version\nbecome the input of a CRF module, producing an-\nother sequence of subword-level IOB labels. This\n1743\nstep aims at denoising the output labels produced\nby the previous components.\nThe output labels are calculated for sub-word\ntokens, then we aggregate each set of sub-word\nlabels {ℓi}into a word label Lusing the ﬁrst rule\nthat applies: (i) if ℓi = O for all i, then L= O;\n(ii) if ℓi = B for any i, then L= B; (iii) if ℓi = I\nfor any i, then L= I. The aggregated output is a\nsequence of word-level IOB labels.\n3.3.3 Baseline\nAs a strong baseline, we used the TMRLeiden ar-\nchitecture (Dirkson and Verberne, 2019), which\nachieved the 2nd best Strict F1-Score in the latest\nSMM4H shared task (Weissenbacher et al., 2019)\nand is composed of a BiLSTM taking as input a\nconcatenation of BERT and Flair embeddings (Ak-\nbik et al., 2019). We chose this baseline since the\nTMRLeiden code is publicly available.\n3.4 Implementation details\nTMRLeiden was re-implemented starting from its\nthe original code1 and trained according to the de-\ntails in the paper. As for the Transformers, all exper-\niments were performed using the TRANSFORMERS\nlibrary (Wolf et al., 2019) (see Appendix C).\nParameter-tuning was done via grid-search, using\ndifferent learning rates ([5e−4, 5e−5, 5e−6]) and\ndropout rates (from 0.15 to 0.30, increments of\n0.05). All the architectures were trained for 50\nepochs on the training set. Learning rate, dropout\nrate and maximum epoch were chosen evaluating\nthe models on the validation set.\nDuring evaluation all the models were then\ntrained using the best hyperparameters on the con-\ncatenation of the training set and the validation set,\nand tested on the test set. This procedure was re-\npeated ﬁve times with different random seeds, and\nﬁnally we averaged the results over the ﬁve runs.\n4 Evaluation\nThe results for the two datasets are shown in Ta-\nble 1 (we focus on the F1-score, but Precision and\nRecall are reported in Appendix D). For reference,\nwe reported the scores of the best architecture by\nDai et al. (2020), which is the state-of-the-art sys-\ntem on CADEC. At a glance, all systems perform\nbetter on CADEC, whose texts belong to a more\nstandardized variety of language. SpanBERT and\n1https://github.com/AnneDirkson/\nSharedTaskSMM4H2019\nSMM4H CADEC\nArchitecture F1 std F1 std\nDai et al. (2020) – – 68.90 –\nTMRLeiden 60.70 2.08 65.03 1.14\nBERT 54.74 1.40 65.20 0.47\nBERT+CRF 59.35 1.23 64.36 0.83\nSpanBERT 62.15 2.17 67.18 0.78\nSpanBERT+CRF 59.89 2.16 67.59 0.60\nPubMedBERT 61.88 0.79 67.16 0.52\nPubMedBERT+CRF 59.53 2.07 67.28 0.82\nBioBERT 57.83 2.59 65.59 1.10\nBioBERT+CRF 58.05 1.45 66.00 0.67\nSciBERT 57.75 1.55 65.61 0.54\nSciBERT+CRF 58.86 1.55 67.09 0.74\nBioClinicalBert 58.03 0.89 64.64 0.53\nBioClinicalBert+CRF 59.11 1.99 65.97 0.60\nTable 1: F1 scores with standard deviations for all mod-\nels (our best performing model is in bold).\nPubMedBERT emerge as the top performing mod-\nels, with close F1-scores, and in particular, the\nSpanBERT models achieve the top score on both\ndatasets, proving that modeling spans gives an im-\nportant advantage for the identiﬁcation of ADEs.\nFor both models, the addition of CRF gener-\nally determines a slight improvement on CADEC,\nwhile it is detrimental on SMM4H. On SMM4H,\nthe F1-scores of BioBERT, SciBERT and Bio-\nClinicalBERT consistently improve over the stan-\ndard BERT, but they are outperformed by its CRF-\naugmented version, while on CADEC they perform\nclosely to the standard model. The results suggest\nthat in-domain knowledge is consistently useful\nonly when training is done on in-domain text from\nscratch, instead of using general domain text ﬁrst.\nSciBERT is also trained from scratch, but on a cor-\npus that is less speciﬁc for the biomedical domain\nthan the PubMedBERT one (Gu et al., 2020).\nThe models are also being compared with TM-\nRLeiden: we can notice that both versions of\nSpanBERT and PubMedBERT outperform it on\nCADEC (the differences are also statistically sig-\nniﬁcant for the McNemar test at p <0.001), while\nonly the basic versions of the same models retain\nan advantage on it on SMM4H (also in this case,\nthe difference is signiﬁcant at p <0.001).\n4.1 Error Analysis\nWe analyzed the differences between the ADE en-\ntities correctly identiﬁed by the models and those\nthat were missed, using the text statistics that we\npreviously extracted with TEXTSTAT . As it was\n1744\n1 @hospitalpatient have been on humira 2years\nnow n get on off chest infections that\nsometimes need 2diff pills 2sort out should i\nb worried ?\n4 i have had no side effects been taking arthrotec a little over a\nyear, have not noticed any side effects. it does help alot i noticed\nthat when there are times when i forget to take it i can’t stand or\nwalk for any lengths of time.\n2 had a great few hours on my bike but exercise\ndrives my olanzapine #munchies . getting fed\nup with not being able to ﬁt into summer\nwardrobe\n5 works just ﬁne. if there are any side effects, they are deﬁnitely\nnot noticeable. what’s with all these older people (70’s)\ncomplaining about the lack of sex drive ? how much of what you\nare complaining about is simply related to getting older?\n3 this new baccy is just making my cough so\nmuch worse but ahh well need my nicotine\n6 what a great store @walmart is: i loss iq points , gained weight\n& got addicted to nicotine - all in under 15 min from going in !!\nTable 2: Examples of ADEs extracted by PubMedBERT (overlined in blue) and SpanBERT (underlined in red).\nActual ADEs in bold with gray background. The Samples belong to SMM4H (1–3, 6) and CADEC (4–5).\npredictable, it turns out that longer ADE spans\nare more difﬁcult to identify: for all models, we\nextracted the average word length of correct and\nmissed spans and we compared them with a two-\ntailed Mann-Whitney U test, ﬁnding that the latter\nare signiﬁcantly longer (Z = -6.176, p < 0.001).\nWe also extracted the average number of difﬁcult\nwords in the correct and in the missed spans, de-\nﬁned as words with more than two syllables that\nare not included in the TEXTSTAT list of words of\ncommon usage in standard English. We took this\nas an approximation of the number of ”technical”\nterms in the dataset instances. However, the av-\nerage values for correct and missed instances do\nnot differ (Z = 0.109, p > 0.1), suggesting that\nthe presence of difﬁcult or technical words in a\ngiven instance does not represent an inherent factor\nof difﬁculty or facilitation. Still, for some of the\nmodels – including SpanBERT, PubMedBERT and\nTMRLeiden – this difference reaches a marginal\nsigniﬁcance (p <0.05) exclusively on the SMM4H\ndataset, where correctly identiﬁed spans have more\ndifﬁcult words. A possible interpretation is that, as\nthe tweets’ language is more informal, such words\nrepresent a stronger ADE cue, compared to the\nmore technical language of the CADEC dataset.\nFinally, we performed a qualitative analysis,\ncomparing the predictions of SpanBERT and Pub-\nMedBERT. We selected the samples on which one\nof the architectures performed signiﬁcantly better\nthan the other one in terms of F1-Score, and ana-\nlyzed them manually. Some signiﬁcant samples can\nbe found in Table 2. We observed that most of the\nsamples in which PubMedBERT performed better\nthan SpanBERT contained medical terms, which\nSpanBERT had completely ignored (e.g. Sample\n1). The samples in which SpanBERT outperformed\nthe in-domain model contained instead long ADE\nmentions, often associated with informal descrip-\ntions (e.g. Samples 2, 3). As regards false positives,\nboth models make similar errors, which ﬁt into two\nbroad categories: (1) extracting diseases or symp-\ntoms of a disease (e.g. Samples 4, 6); (2) not being\nable to handle general mentions, hypothetical lan-\nguage, negations and similar linguistic constructs\n(e.g. Sample 5). While the second kind of error\nrequires a deeper reﬂection, the ﬁrst one might be\naddressed by training the model to extract multiple\nkinds of entities (e.g. both ADEs and Diseases).\n5 Conclusions\nWe presented a comparison between 12\ntransformers-based models, with the goal of\n“prescribing” the best option to the researchers\nworking in the ﬁeld. We also wanted to test\nwhether the span-based objective of SpanBERT\nand in-domain language pretraining were useful\nfor the task. We can positively answer to the ﬁrst\nquestion, since SpanBERT turned out to be the\nbest performing model on both datasets. As for\nthe in-domain models, PubMedBERT came as\na close second after SpanBERT, suggesting that\npretraining from scratch with no general domain\ndata is the best strategy, at least for this task.\nWe have been the ﬁrst, to our knowledge, to\ntest these two models in a systematic comparison\non ADE detection, and they delivered promising\nresults for future research. For the next step, a pos-\nsible direction would be to combine the strengths\nof their respective representations: the accurate\nmodeling of text spans on the one side, and deep\nbiomedical knowledge on the other one.\n1745\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\nFlair: An Easy-to-use Framework for State-of-the-\nart nlp. In Proceedings of NAACL.\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly Available Clinical\nBERT Embeddings. In Proceedings of the NAACL\nWorkshop on Clinical Natural Language Processing.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A Pretrained Language Model for Scientiﬁc\nText. In Proceedings of EMNLP.\nShuai Chen, Yuanhang Huang, Xiaowei Huang, Haom-\ning Qin, Jun Yan, and Buzhou Tang. 2019. HITSZ-\nICRC: A Report for SMM4H Shared Task 2019-\nAutomatic Classiﬁcation and Extraction of Adverse\nEffect Mentions in Tweets. In Proceedings of the\nACL Workshop on Social Media Mining for Health\nApplications.\nXiang Dai. 2018. Recognizing Complex Entity Men-\ntions: A Review and Future Directions. In Proceed-\nings of ACL 2018, Student Research Workshop.\nXiang Dai, Sarvnaz Karimi, Ben Hachey, and C ´ecile\nParis. 2020. An Effective Transition-based Model\nfor Discontinuous NER. In Proceedings of ACL.\nRaminta Daniulaityte, Lu Chen, Francois R Lamy,\nRobert G Carlson, Krishnaprasad Thirunarayan, and\nAmit Sheth. 2016. “When ‘Bad’ Is ‘Good’”: Iden-\ntifying Personal Communication and Sentiment in\nDrug-related Tweets. JMIR Public Health and\nSurveillance, 2(2):e162.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of NAACL.\nAnne Dirkson and Suzan Verberne. 2019. Transfer\nLearning for Health-related Twitter Data. In Pro-\nceedings of the ACL Workshop on Social Media Min-\ning for Health Applications.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nSpeciﬁc Language Model Pretraining for Biomed-\nical Natural Language Processing. arXiv preprint\narXiv:2007.15779.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2019.\nSpanBERT: Improving Pre-training by Representing\nand Predicting Spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nSarvnaz Karimi, Alejandro Metke-Jimenez, Madonna\nKemp, and Chenchen Wang. 2015a. Cadec: A Cor-\npus of Adverse Drug Event Annotations. Journal of\nBiomedical Informatics, 55:73–81.\nSarvnaz Karimi, Chen Wang, Alejandro Metke-\nJimenez, Raj Gaire, and Cecile Paris. 2015b. Text\nand Data Mining Techniques in Adverse Drug Re-\naction Detection. ACM Computing Surveys (CSUR),\n47(4):1–39.\nJohn Lafferty, Andrew Mccallum, and Fernando\nPereira. 2001. Conditional Random Fields: Prob-\nabilistic Models for Segmenting and Labeling Se-\nquence Data. In Proceedings of ICML.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. BioBERT: A Pre-trained\nBiomedical Language Representation Model\nfor Biomedical Text Mining. Bioinformatics,\n36(4):1234–1240.\nDebanjan Mahata, Sarthak Anand, Haimin Zhang,\nSimra Shahid, Laiba Mehnaz, Yaman Kumar, and\nRajiv Shah. 2019. MIDAS@ SMM4H-2019: Identi-\nfying Adverse Drug Reactions and Personal Health\nExperience Mentions from Twitter. In Proceedings\nof the ACL Workshop on Social Media Mining for\nHealth Applications.\nZulfat Miftahutdinov, Ilseyar Alimova, and Elena Tu-\ntubalina. 2019. KFU NLP Team at SMM4H 2019\nTasks: Want to Extract Adverse Drugs Reactions\nfrom Tweets? BERT to the Rescue. In Proceedings\nof the ACL Workshop on Social Media Mining for\nHealth Applications.\nAzadeh Nikfarjam, Abeed Sarker, Karen O’Connor,\nRachel E. Ginn, and Graciela Gonzalez-Hernandez.\n2015. Pharmacovigilance from Social Media: Min-\ning Adverse Drug Reaction Mentions Using Se-\nquence Labeling with Word Embedding Cluster Fea-\ntures. Journal of the American Medical Informatics\nAssociation : JAMIA, 22:671 – 681.\nNishant Nikhil and Shivansh Mundra. 2018. Neural\nDrugNet. In Proceedings of the EMNLP Workshop\non Social Media Mining for Health Applications.\nSean Papay, Roman Klinger, and Sebastian Pad´o. 2020.\nDissecting Span Identiﬁcation Tasks with Perfor-\nmance Prediction. In Proceedings of EMNLP.\nMichael Paul, Abeed Sarker, John Brownstein, Azadeh\nNikfarjam, Matthew Scotch, Karen Smith, and Gra-\nciela Gonzalez. 2016. Social Media Mining for Pub-\nlic Health Monitoring and Surveillance. In Biocom-\nputing 2016, pages 468–479.\nAbeed Sarker, Rachel Ginn, Azadeh Nikfarjam, Karen\nO’Connor, Karen Smith, Swetha Jayaraman, Te-\njaswi Upadhaya, and Graciela Gonzalez. 2015. Uti-\nlizing Social Media Data for Pharmacovigilance: A\nReview. Journal of Biomedical Informatics, 54:202–\n212.\nAbeed Sarker and Graciela Gonzalez. 2015. Portable\nAutomatic Text Classiﬁcation for Adverse Drug Re-\naction Detection via Multi-corpus Training. Journal\nof Biomedical Informatics, 53:196–207.\n1746\nAbeed Sarker and Graciela Gonzalez-Hernandez. 2017.\nOverview of the Social Media Mining for Health\n(SMM4H) Shared Tasks at AMIA 2017. Training,\n1(10,822):1239.\nIsabel Segura-Bedmar, Paloma Mart ´ınez, and Mar ´ıa\nHerrero-Zazo. 2013. SemEval-2013 Task 9 : Ex-\ntraction of Drug-Drug Interactions from Biomedical\nTexts (DDIExtraction 2013). In Proceedings of Se-\nmEval.\nGabriel Stanovsky, Daniel Gruhl, and Pablo Mendes.\n2017. Recognizing Mentions of Adverse Drug Reac-\ntion in Social Media Using Knowledge-Infused Re-\ncurrent Models. In Proceedings of EACL.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Proceedings of NIPS.\nDavy Weissenbacher, Abeed Sarker, Arjun Magge,\nAshlynn Daughton, Karen O’Connor, Michael Paul,\nand Graciela Gonzalez. 2019. Overview of the\nFourth Social Media Mining for Health (SMM4H)\nShared Tasks at ACL 2019. In Proceedings of the\nACL Social Media Mining for Health Applications\n(# SMM4H) Workshop & Shared Task.\nDavy Weissenbacher, Abeed Sarker, Michael Paul, and\nGraciela Gonzalez. 2018. Overview of the Social\nMedia Mining for Health (SMM4H) Shared Tasks\nat EMNLP 2018. In Proceedings of the EMNLP\nWorkshop on Social Media Mining for Health Appli-\ncations.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019.\nHuggingFace’s Transformers: State-of-the-art Natu-\nral Language Processing. ArXiv, abs/1910.03771.\nChuhan Wu, Fangzhao Wu, Junxin Liu, Sixing Wu,\nYongfeng Huang, and Xing Xie. 2018. Detecting\nTweets Mentioning Drug Name and Adverse Drug\nReaction with Hierarchical Tweet Representation\nand Multi-head Self-attention. In Proceedings of\nthe EMNLP Workshop on Social Media Mining for\nHealth Applications.\nSusmitha Wunnava, Xiao Qin, Tabassum Kakar, Xiang-\nnan Kong, and Elke Rundensteiner. 2020. A Dual-\nAttention Network for Joint Named Entity Recog-\nnition and Sentence Classiﬁcation of Adverse Drug\nEvents. In Findings of EMNLP.\nA Text statistics for datasets\nSome statistics for the texts of the two datasets\nhave been extracted with the TEXTSTAT Python\npackage and reported reported in Table A: we ex-\ntracted the counts of syllables, lexicon (how many\ndifferent word types are being used), sentences and\ncharacters. Difﬁcult words refers to the number of\npolysyllabic words with Syllable Count > 2 that\nare not included in the list of words of common\nusage in English.\nMetric CADEC SMM4H\nSyllable Count 116 ± 2.7 26 ± 0.2\nLexicon Count 83 ± 1.9 18 ± 0.1\nSentence Count 5 ± 0.1 1 ± 0.0\nCharacter Count 461 ± 10.5 104 ± 0.7\nDifﬁcult Words 14 ± 0.3 4 ± 0.1\nTable 3: Average metrics per dataset, computed by the\nTEXTSTAT Python library.\nB Best hyperparameters on the two\ndatasets\nTable 4 reports the best hyperparameters for all ar-\nchitectures on SMM4H and CADEC, respectively.\nSMM4H CADEC\nArchitecture lr dropout epochlr dropout epoch\nBERT 5e−5 0.20 4 5e−5 0.25 11\nBERT+CRF 5e−5 0.15 6 5e−5 0.15 7\nSpanBERT 5e−5 0.25 43 5e−5 0.25 19\nSpanBERT+CRF 5e−5 0.15 14 5e−5 0.15 11\nPubMedBERT 5e−5 0.25 21 5e−5 0.15 7\nPubMedBERT+CRF5e−5 0.25 13 5e−5 0.25 16\nBioBERT 5e−5 0.20 8 5e−5 0.25 12\nBioBERT+CRF 5e−5 0.15 6 5e−5 0.20 9\nSciBERT 5e−5 0.15 7 5e−5 0.15 6\nSciBERT+CRF 5e−5 0.25 13 5e−5 0.25 12\nBioClinicalBERT5e−5 0.25 10 5e−5 0.25 6\nBioClinicalBERT+CRF5e−5 0.25 12 5e−5 0.25 10\nTable 4: Best hyperparameters for all Transformer-\nbased architectures on SMM4H and CADEC.\nC General information on the models\nTable 5 is a summary of the information about the\nversion of all Transformer-based models used and\ntheir pretraining methods.\nD Detailed metrics of all the models\nTable 6 and 7 report as Strict and Partial metrics the\nF1-score, Precision and Recall calculated for all\narchitectures on SMM4H and CADEC respectively.\nResults are the average over ﬁve runs.\nThe Partial scores are standard metrics for this\ntask (Weissenbacher et al., 2019) and take into ac-\ncount “partial”matches, in which it is sufﬁcient for\na system prediction to partially overlap with the\ngold annotation to be considered as a true match.\n1747\nName Version Vocabulary Pretraining Pretraining Corpus\nBERT base uncased Wikipedia+BookCorpus from scratch Wikipedia+BookCorpus\nSpanBERT base cased Wikipedia+BookCorpus from scratch Wikipedia+BookCorpus\nPubMedBERT base uncased abstract+fulltext PubMed from scratch PubMed+PMC\nBioBERT base v1.1 (+PubMed) Wikipedia+BookCorpus from BERT PubMed\nBioBERT(v1.0) base v1.0 (+PubMed+PMC) Wikipedia+BookCorpus from BERT PubMed+PMC\nSciBERT scivocab cased Semantic Scholar from scratch Semantic Scholar\nBioClinicalBERT bio+clinical Wikipedia+BookCorpus from BioBERT MIMIC-III\nTable 5: Information about the version of all the Transformer-based models used and their pretraining.\nStrict Partial\nF1 P R Architecture F1 P R\n60.70±2.08 68.36±2.41 54.59±1.97 TMRLeiden 66.08±1.79 74.42±2.11 59.43±1.76\n54.74±1.40 48.50±1.67 62.84±1.12 BERT 64.53±1.09 57.17±1.52 74.08±0.78\n59.35±1.23 54.12±1.19 65.69±1.34 BERT+CRF 68.35±0.64 62.33±0.74 75.66±0.68\n62.15±2.17 54.54±3.06 72.31±1.30 SpanBERT 69.38±1.60 60.88±2.74 80.74±1.08\n59.89±2.16 54.86±3.10 66.05±1.93 SpanBERT+CRF 68.09±1.51 62.35±2.79 75.10±1.72\n61.88±0.79 58.70±0.83 65.45±1.39 PubMedBERT 69.82±0.60 66.23±0.86 73.84±1.26\n59.53±2.07 55.29±2.19 64.49±2.27 PubMedBERT+CRF67.94±1.48 63.10±1.69 73.61±1.84\n55.22±1.71 49.85±1.76 61.89±1.78 BioBERT v1.0 64.25±1.09 58.00±1.22 72.02±1.30\n57.83±2.59 53.68±3.20 62.72±2.30 BioBERT 66.58±1.34 61.79±2.25 72.23±1.42\n58.05±1.45 54.44±2.18 62.22±1.22 BioBERT+CRF 66.30±0.85 62.17±1.83 71.07±1.15\n57.75±1.55 53.49±0.97 62.75±2.54 SciBERT 66.49±0.83 61.61±0.61 72.25±1.89\n58.86±1.55 52.94±2.27 66.35±1.86 SciBERT+CRF 67.12±0.97 60.36±1.93 75.67±1.99\n58.03±0.89 51.63±1.51 66.26±0.46 BioClinicalBERT 66.90±0.57 59.52±1.29 76.39±0.99\n59.11±1.99 52.35±2.55 67.92±1.55 BioClinicalBERT+CRF67.41±1.19 59.69±1.92 77.48±1.40\nTable 6: Results on SMM4H, F1-scores, Precision and Recall calculated as Strict and Partial metrics, with standard\ndeviations for all models.\nStrict Partial\nF1 P R Architecture F1 P R\n65.03±1.14 67.50±1.01 62.75±1.26 TMRLeiden 77.08±0.78 79.99±0.60 74.36±0.97\n65.20±0.47 62.86±0.52 67.72±0.70 BERT 77.73±0.28 74.95±0.57 80.74±0.47\n64.36±0.83 62.47±0.97 66.36±0.79 BERT+CRF 77.23±0.45 74.97±0.72 79.63±0.41\n67.18±0.78 65.84±0.94 68.57±0.78 SpanBERT 79.18±0.61 77.60±0.79 80.82±0.72\n67.59±0.60 67.09±0.54 68.10±0.78 SpanBERT+CRF 79.43±0.27 78.84±0.24 80.02±0.60\n67.16±0.52 66.60±0.67 67.73±0.57 PubMedBERT 79.13±0.23 78.47±0.51 79.81±0.42\n67.28±0.82 66.69±0.99 67.88±0.91 PubMedBERT+CRF79.12±0.43 78.43±0.72 79.83±0.71\n65.54±0.47 64.24±0.48 66.90±0.46 BioBERT v1.0 77.86±0.34 76.32±0.36 79.47±0.33\n65.59±1.10 64.86±1.39 66.34±0.85 BioBERT 78.17±0.75 77.30±1.13 79.06±0.48\n66.00±0.67 65.52±0.97 66.48±0.63 BioBERT+CRF 78.24±0.43 77.68±0.81 78.82±0.58\n65.61±0.54 64.46±0.70 66.80±0.50 SciBERT 78.05±0.19 76.69±0.36 79.47±0.46\n67.09±0.74 65.99±0.74 68.23±0.80 SciBERT+CRF 79.01±0.35 77.72±0.36 80.35±0.50\n64.64±0.53 61.99±0.51 67.53±0.56 BioClinicalBERT 76.95±0.35 73.80±0.36 80.39±0.37\n65.97±0.60 64.23±1.16 67.82±0.60 BioClinicalBERT+CRF77.98±0.49 75.92±1.26 80.17±0.53\nTable 7: Results on CADEC, F1-scores, Precision and Recall calculated as Strict and Partial metrics, with standard\ndeviations for all models.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.564777672290802
    },
    {
      "name": "Transformer",
      "score": 0.5114104151725769
    },
    {
      "name": "Headaches",
      "score": 0.4566780924797058
    },
    {
      "name": "Medical prescription",
      "score": 0.45409294962882996
    },
    {
      "name": "Drug",
      "score": 0.44309794902801514
    },
    {
      "name": "Medicine",
      "score": 0.3171628713607788
    },
    {
      "name": "Electrical engineering",
      "score": 0.15530535578727722
    },
    {
      "name": "Pharmacology",
      "score": 0.1518518328666687
    },
    {
      "name": "Engineering",
      "score": 0.1206105649471283
    },
    {
      "name": "Voltage",
      "score": 0.08091610670089722
    },
    {
      "name": "Psychiatry",
      "score": 0.06502914428710938
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I129043915",
      "name": "University of Udine",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I150569930",
      "name": "Bayer (United States)",
      "country": "US"
    }
  ]
}