{
    "title": "StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models",
    "url": "https://openalex.org/W4380714711",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4287204346",
            "name": "Li, Yinghao Aaron",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2262659068",
            "name": "Han Cong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4380716675",
            "name": "Raghavan, Vinay S.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288869125",
            "name": "Mischler, Gavin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287204348",
            "name": "Mesgarani, Nima",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3123097577",
        "https://openalex.org/W3196667132",
        "https://openalex.org/W3091928890",
        "https://openalex.org/W4286869901",
        "https://openalex.org/W3110257065",
        "https://openalex.org/W3174758275",
        "https://openalex.org/W4297841714",
        "https://openalex.org/W3197324626",
        "https://openalex.org/W4372267432",
        "https://openalex.org/W4368755563",
        "https://openalex.org/W3197273793",
        "https://openalex.org/W3129651364",
        "https://openalex.org/W4319862675",
        "https://openalex.org/W4287117449",
        "https://openalex.org/W4319862245",
        "https://openalex.org/W4303519914",
        "https://openalex.org/W3197294703",
        "https://openalex.org/W2603777577",
        "https://openalex.org/W2976159681",
        "https://openalex.org/W4226132755",
        "https://openalex.org/W4226063663",
        "https://openalex.org/W3172148458",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4313679638",
        "https://openalex.org/W4362655923",
        "https://openalex.org/W3209059054",
        "https://openalex.org/W4224612669",
        "https://openalex.org/W3209984917",
        "https://openalex.org/W4281736089",
        "https://openalex.org/W4281969232",
        "https://openalex.org/W4360890968",
        "https://openalex.org/W4287761884",
        "https://openalex.org/W3198213150",
        "https://openalex.org/W4287121924",
        "https://openalex.org/W4296068816",
        "https://openalex.org/W4366460484",
        "https://openalex.org/W4224931676",
        "https://openalex.org/W2962780374",
        "https://openalex.org/W4320451749",
        "https://openalex.org/W4221152438",
        "https://openalex.org/W4200300291",
        "https://openalex.org/W3092028330",
        "https://openalex.org/W4367359628",
        "https://openalex.org/W3036601975",
        "https://openalex.org/W4225680573",
        "https://openalex.org/W4226380987",
        "https://openalex.org/W4319862431",
        "https://openalex.org/W2593414223",
        "https://openalex.org/W4367000116",
        "https://openalex.org/W2972359262",
        "https://openalex.org/W4285483538",
        "https://openalex.org/W4225566824",
        "https://openalex.org/W4320057414",
        "https://openalex.org/W2768282280",
        "https://openalex.org/W4323697273",
        "https://openalex.org/W4385822565",
        "https://openalex.org/W3169386841",
        "https://openalex.org/W4281661987",
        "https://openalex.org/W3034302232",
        "https://openalex.org/W4280561221",
        "https://openalex.org/W4287120591"
    ],
    "abstract": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.",
    "full_text": "StyleTTS 2: Towards Human-Level Text-to-Speech\nthrough Style Diffusion and Adversarial Training with\nLarge Speech Language Models\nYinghao Aaron Li Cong Han Vinay S. Raghavan\nGavin Mischler Nima Mesgarani\nColumbia University\n{yl4579,ch3212,vsr2119,gm2944,nm2764}@columbia.edu\nAbstract\nIn this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages\nstyle diffusion and adversarial training with large speech language models (SLMs)\nto achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by\nmodeling styles as a latent random variable through diffusion models to generate\nthe most suitable style for the text without requiring reference speech, achieving\nefficient latent diffusion while benefiting from the diverse speech synthesis offered\nby diffusion models. Furthermore, we employ large pre-trained SLMs, such as\nWavLM, as discriminators with our novel differentiable duration modeling for end-\nto-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses\nhuman recordings on the single-speaker LJSpeech dataset and matches it on the\nmultispeaker VCTK dataset as judged by native English speakers. Moreover, when\ntrained on the LibriTTS dataset, our model outperforms previous publicly available\nmodels for zero-shot speaker adaptation. This work achieves the first human-level\nTTS on both single and multispeaker datasets, showcasing the potential of style\ndiffusion and adversarial training with large SLMs. The audio demos and source\ncode are available at https://styletts2.github.io/.\n1 Introduction\nText-to-speech (TTS) synthesis has seen significant advancements in recent years, with numerous\napplications such as virtual assistants, audiobooks, and voice-over narration benefiting from increas-\ningly natural and expressive synthetic speech [1, 2]. Some previous works have made significant\nprogress towards human-level performance [3, 4, 5]. However, the quest for robust and accessible\nhuman-level TTS synthesis remains an ongoing challenge because there is still room for improvement\nin terms of diverse and expressive speech [5, 6], robustness for out-of-distribution (OOD) texts [7],\nand the requirements of massive datasets for high-performing zero-shot TTS systems [8].\nIn this paper, we introduce StyleTTS 2, an innovative TTS model that builds upon the style-based\ngenerative model StyleTTS [6] to present the next step towards human-level TTS systems. We model\nspeech styles as a latent random variable and sample them with a probabilistic diffusion model,\nallowing the model to efficiently synthesize highly realistic speech without the need for reference\naudio. Since it only needs to sample a style vector instead of the entire speech as a latent variable,\nStyleTTS 2 is faster than other diffusion TTS models while still benefiting from the diverse speech\nsynthesis enabled by diffusion models. One of the key contributions of StyleTTS 2 is the use of\nlarge pre-trained speech language models (SLMs) like Wav2Vec 2.0 [9], HuBERT [10], and WavLM\n[11] as discriminators, in conjunction with a novel differentiable duration modeling approach. This\nend-to-end (E2E) training setup leverages SLM representations to enhance the naturalness of the\nsynthesized speech, transferring knowledge from large SLMs for speech generation tasks.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.07691v2  [eess.AS]  20 Nov 2023\nOur evaluations suggest that speech generated by StyleTTS 2 surpasses human recordings as judged\nby native English speakers on the benchmark LJSpeech [ 12] dataset with statistically significant\ncomparative mean opinion scores (CMOS) of +0.28 (p <0.05). Additionally, StyleTTS 2 advances\nthe state-of-the-art by achieving a CMOS of +1.07 (p ≪ 0.01) compared to NaturalSpeech [ 5].\nFurthermore, it attains human-level performance on the multispeaker VCTK dataset [13] in terms of\nnaturalness (CMOS = −0.02, p ≫ 0.05) and similarity (CMOS = +0.30, p <0.1) to the reference\nspeaker. When trained on a large number of speakers like the LibriTTS dataset [ 14], StyleTTS 2\ndemonstrates potential for speaker adaptation. It surpasses previous publicly available models in\nthis task and outperforms Vall-E [8] in naturalness. Moreover, it achieves slightly worse similarity\nto the target speaker with only a 3-second reference speech, despite using around 250 times less\ndata compared to Vall-E, making it a data-efficient alternative for large pre-training in the zero-shot\nspeaker adaptation task. As the first model to achieve human-level performance on publicly available\nsingle and multispeaker datasets, StyleTTS 2 sets a new benchmark for TTS synthesis, highlighting\nthe potential of style diffusion and adversarial training with SLMs for human-level TTS synthesis.\n2 Related Work\nDiffusion Models for Speech Synthesis.Diffusion models have gained traction in speech synthesis\ndue to their potential for diverse speech sampling and fine-grained speech control [15]. They have\nbeen applied to mel-based text-to-speech [ 16, 17, 18, 19, 20], mel-to-waveform vocoder [21, 22,\n23, 24, 25, 26], and end-to-end speech generation [27, 28, 29]. However, their efficiency is limited\ncompared to non-iterative methods, like GAN-based models [30, 31, 32], due to the need to iteratively\nsample mel-spectrograms, waveforms, or other latent representations proportional to the target\nspeech duration [15]. Furthermore, recent works suggest that state-of-the-art GAN-based models\nstill perform better than diffusion models in speech synthesis [26, 33]. To address these limitations,\nwe introduce style diffusion, where a fixed-length style vector is sampled by a diffusion model\nconditioned on the input text. This approach significantly improves model speed and enables end-\nto-end training. Notably, StyleTTS 2 synthesizes speech using GAN-based models, with only the\nstyle vector dictating the diversity of speech sampled. This unique combination allows StyleTTS 2 to\nachieve high-quality synthesis with fast inference speed while maintaining the benefits of diverse\nspeech generation, further advancing the capabilities of diffusion models in speech synthesis.\nText-to-Speech with Large Speech Language Models.Recent advancements have proven the\neffectiveness of large-scale self-supervised speech language models (SLMs) in enhancing text-to-\nspeech (TTS) quality [34, 35, 36, 37] and speaker adaptation [8, 38, 29, 39]. These works typically\nconvert text input into either continuous or quantized representations derived from pre-trained SLMs\nfor speech reconstruction. However, SLM features are not directly optimized for speech synthesis,\nwhile tuning SLMs as a neural codec [ 34, 35, 8, 29] involves two-stage training. In contrast, our\nmodel benefits from the knowledge of large SLMs via adversarial training using SLM features without\nlatent space mapping, thus directly learning a latent space optimized for speech synthesis like other\nend-to-end (E2E) TTS models. This innovative approach signifies a new direction in TTS with SLMs.\nHuman-Level Text-to-Speech. Several recent works have advanced towards human-level TTS\n[3, 4, 5] using techniques like BERT pre-training [4, 40, 7] and E2E training [32, 5] with differentiable\nduration modeling [41, 42]. VITS [3] demonstrates MOS comparable to human recordings on the\nLJSpeech and VCTK datasets, while PnG-BERT [4] obtains human-level results on a proprietary\ndataset. NaturalSpeech [5], in particular, achieves both MOS and CMOS on LJSpeech statistically\nindistinguishable from human recordings. However, we find that there is still room for improvement\nin speech quality beyond these state-of-the-art models, as we attain higher performance and set a new\nstandard for human-level TTS synthesis. Furthermore, recent work shows the necessity for disclosing\nthe details of evaluation procedures for TTS research [43]. Our evaluation procedures are detailed in\nAppendix E, which can be used for reproducible future research toward human-level TTS.\n3 Methods\n3.1 StyleTTS Overview\nStyleTTS [6] is a non-autoregressive TTS framework using a style encoder to derive a style vector\nfrom reference audio, enabling natural and expressive speech generation. The style vector is incorpo-\n2\nrated into the decoder and duration and prosody predictors using adaptive instance normalization\n(AdaIN) [44], allowing the model to generate speech with varying duration, prosody, and emotions.\nThe model comprises eight modules, organized into three categories: (1) a speech generation system\n(acoustic modules) with a text encoder, style encoder, and speech decoder; (2) a TTS prediction\nsystem with duration and prosody predictors; and (3) a utility system for training, including a\ndiscriminator, text aligner, and pitch extractor. It undergoes a two-stage training process: the first\nstage trains the acoustic modules for mel-spectrogram reconstruction, and the second trains TTS\nprediction modules using the fixed acoustic modules trained in the first stage.\nIn the first stage, the text encoder T encodes input phonemes t into phoneme representations\nhtext = T(t), while the text aligner A extracts speech-phoneme alignment aalgn = A(x, t) from input\nspeech x and phonemes t to produce aligned phoneme representations halgn = htext · aalgn via dot\nproduct. Concurrently, the style encoder E obtains the style vector s = E(x), and the pitch extractor\nF extracts the pitch curve px = F(x) along with its energy nx = ∥x∥. Lastly, the speech decoder G\nreconstructs ˆx = G (halgn, s, px, nx), which is trained to match input x using a L1 reconstruction\nloss Lmel and adversarial objectives Ladv, Lfm with a discriminator D. Transferable monotonic aligner\n(TMA) objectives are also applied to learn optimal alignments (see Appendix G for details).\nIn the second stage, all components except the discriminator D are fixed, with only the duration\nand prosody predictors being trained. The duration predictor S predicts the phoneme duration\nwith dpred = S(htext, s), whereas the prosody predictor P predicts pitch and energy as ˆpx, ˆnx =\nP(htext, s). The predicted duration is trained to match the ground truth duration dgt derived from\nthe summed monotonic version of the alignment aalgn along the time axis with an L1 loss Ldur.\nThe predicted pitch ˆpx and energy ˆnx are trained to match the ground truth pitch px and energy\nnx derived from pitch extractor F with L1 loss Lf0 and Ln. During inference, dpred is used to\nupsample htext through apred, the predicted alignment, obtained by repeating the value 1 for dpred[i]\ntimes at ℓi−1, where ℓi is the end position of the ith phoneme ti calculated by summing dpred[k] for\nk ∈ {1, . . . , i}, and dpred[i] are the predicted duration of ti. The mel-spectrogram is synthesized by\nxpred = G(htext · apred, E(˜x), ˆp˜x, ˆn˜x) with ˜x an arbitrary reference audio that influences the style of\nxpred, which is then converted into a waveform using a pre-trained vocoder.\nDespite its state-of-the-art performance in synthesizing diverse and controllable speech, StyleTTS\nhas several drawbacks, such as a two-stage training process with an additional vocoding stage that\ndegrades sample quality, limited expressiveness due to deterministic generation, and reliance on\nreference speech hindering real-time applications.\n3.2 StyleTTS 2\nStyleTTS 2 improves upon the StyleTTS framework, resulting in a more expressive text-to-speech\n(TTS) synthesis model with human-level quality and improved out-of-distribution performance.\nWe introduce an end-to-end (E2E) training process that jointly optimizes all components, along\nwith direct waveform synthesis and adversarial training with large speech language models (SLMs)\nenabled by our innovative differentiable duration modeling. The speech style is modeled as a latent\nvariable sampled through diffusion models, allowing diverse speech generation without reference\naudio. We outline these important changes in the following sections with an overview in Figure 1.\n3.2.1 End-to-End Training\nE2E training optimizes all TTS system components for inference without relying on any fixed\ncomponents like pre-trained vocoders that convert mel-spectrograms into waveforms [ 3, 32]. To\nachieve this, we modify the decoder G to directly generate the waveform from the style vector,\naligned phoneme representations, and pitch and energy curves. We remove the last projection layer\nfor mel-spectrograms of the decoder and append a waveform decoder after it. We propose two types\nof decoders: HifiGAN-based and iSTFTNet-based. The first is based on Hifi-GAN [ 30], which\ndirectly generates the waveform. In contrast, the iSTFTNet-based decoder [45] produces magnitude\nand phase, which are converted into waveforms using inverse short-time Fourier transform for faster\ntraining and inference. We employ the snake activation function [46], proven effective for waveform\ngeneration in [31]. An AdaIN module [44] is added after each activation function to model the style\ndependence of the speech, similar to the original StyleTTS decoder. We replace the mel-discriminator\nin [6] with the multi-period discriminator (MPD) [ 30] and multi-resolution discriminator (MRD)\n3\nMel-Spectrogram Phonemes\nText Aligner\n Prosodic Text\nEncoder\nAcoustic Text\nEncoder \nAcoustic Style\nEncoder Pitch Extractor\nDecoder\nProsodic Style\nEncoder\nReconstructed\nWaveform\nProsody\nPredictor\nStyle Diffusion\nDenoiser\nMPD MRD\nDuration\nPredictor\n(a) Acoustic modules pre-training and joint training. To accelerate training, the pre-training first optimize\nmodules inside the blue box; the joint training then follows to optimize all components except the pitch extractor,\nwhich is used to provide the ground truth label for pitch curves. The duration predictor is trained with only Ldur.\nPhonemes\nProsodic Text\nEncoder\nAcoustic Text\nEncoder\nDuration\nPredictor\nStyle Diffusion\nSampler\nProsody\n Predictior\nDecoder\nSynthesized\nWaveforms\nReal or Fake?\nAcoustic modules\nBlock\nTTS prediction modules\nUtility modules for training\nAcoustic modules pre-training\nJoint training\nAcoustic modules pre-training\nscheme\n \nSLM discriminator\nWavLM\n Discriminative\nHead\nUpsample\n(b) SLM adversarial training and inference. WavLM is pre-trained and not tuned. Unlike (a),\nthe duration predictor is trained E2E with all components using Lslm (eq. 5) via differentiable\nupsampling. This process is separate from (a) during training as the input texts can be different, but\nthe gradients are accumulated for both processes in each batch to update the parameters.\nFigure 1: Training and inference scheme of StyleTTS 2 for the single-speaker case. For the multi-\nspeaker case, the acoustic and prosodic style encoders (denoted as E) first take reference audio xref\nof the target speaker and produce a reference style vector c = E(xref). The style diffusion model\nthen takes c as a reference to sample sp and sa that correspond to the speaker in xref.\n[47] along with the LSGAN loss functions [48] for decoder training, and incorporate the truncated\npointwise relativistic loss function [49] to enhance sound quality (see Appendix F and G for details).\nWe found that well-trained acoustic modules, especially the style encoder, can accelerate the training\nprocess for TTS prediction modules. Therefore, before jointly optimizing all components, we first\npre-train the acoustic modules along with the pitch extractor and text aligner via Lmel, Ladv, Lfm and\nTMA objectives for N epochs where N depends on the size of the training set, in the same way as\nthe first training stage of [6]. However, we note that this pre-training is not an absolute necessity:\ndespite being slower, starting joint training directly from scratch also leads to model convergence,\nAfter acoustic module pre-training, we jointly optimize Lmel, Ladv, Lfm, Ldur, Lf0 and Ln, where Lmel\nis modified to match the mel-spectrograms of waveforms reconstructed from predicted pitch ˆpx and\nenergy ˆnx (Fig 1a). During joint training, stability issues emerge from diverging gradients, as the\nstyle encoder must encode both acoustic and prosodic information. To address this inconsistency, we\nintroduce a prosodic style encoder Ep alongside the original acoustic style encoder Ea, previously\ndenoted as E in section 3.1. Instead of using sa = Ea(x), predictors S and P take sp = Ep(x) as\nthe input style vector. The style diffusion model generates the augmented style vector s = [sp, sa].\nThis modification effectively improves sample quality (see section 5.3). To further decouple the\nacoustic modules and predictors, we replace the phoneme representations htext from T, now referred\nto as acoustic text encoder, with hbert from another text encoder B based on BERT transformers,\ndenoted as prosodic text encoder. Specifically, we employ a phoneme-level BERT [7] pre-trained on\nextensive corpora of Wikipedia articles as the prosodic text encoder. This approach has been shown\nto enhance the naturalness of StyleTTS in the second stage [7], similar to our proposed usage here.\n4\nWith differentiable upsampling and fast style diffusion, we can generate speech samples during\ntraining in a fully differentiable manner, just as during inference. These samples are used to optimize\nLslm (eq. 5) during joint training to update the parameters of all components for inference (Fig 1b).\n3.2.2 Style Diffusion\nIn StyleTTS 2, we model the speech x as a conditional distribution p(x|t) =\nR\np(x|t, s)p(s|t) ds\nthrough a latent variable s that follows the distribution p(s|t). We call this variable the generalized\nspeech style, representing any characteristic in speech beyond phonetic content t, including but not\nlimited to prosody, lexical stress, formant transitions, and speaking rate [6]. We sample s by EDM\n[50] that follows the combined probability flow [51] and time-varying Langevin dynamics [52]:\ns =\nZ\n−σ(τ) [β(τ)σ(τ) + ˙σ(τ)] ∇s log pτ (s|t) dτ +\nZ p\n2β(τ)σ(τ) d ˜Wτ , (1)\nwhere σ(τ) is the noise level schedule and ˙σ(τ) is its time derivative, β(τ) is the stochasticity term,\n˜Wτ is the backward Wiener process for τ ∈ [T, 0] and ∇s log pτ (s|t) is the score function at time τ.\nWe follow the EDM [50] formulation with the denoiser K(s; t, σ) preconditioned as:\nK(s; t, σ) :=\n\u0010σdata\nσ∗\n\u00112\ns + σ · σdata\nσ∗ · V\n\u0012 s\nσ∗ ; t, 1\n4 ln σ\n\u0013\n, (2)\nwhere σ follows a log-normal distribution ln σ ∼ N(Pmean, P2\nstd) with Pmean = −1.2, Pstd = 1.2,\nσ∗ :=\np\nσ2 + σ2\ndata is the scaling term, σdata = 0.2 is the empirical estimate of the standard deviation\nof style vectors, and V is a 3-layer transformer [53] conditioned on t and σ which is trained with:\nLedm = Ex,t,σ,ξ∼N(0,I)\nh\nλ(σ) ∥K(E(x) + σξ; t, σ) − E(x)∥2\n2\ni\n, (3)\nwhere E(x) := [Ea(x), Ep(x)], and λ(σ) := (σ∗/(σ · σdata))2 is the weighting factor. Under this\nframework, equation 16 becomes an ODE where the score function depends on σ instead of τ:\nds\ndσ = −σ∇s log pσ(s|t) = s − K(s; t, σ)\nσ , s(σ(T)) ∼ N(0, σ(T)2I). (4)\nUnlike [50] that uses 2nd-order Heun, we solve eq. 4 with the ancestral DPM-2 solver [54] for fast\nand diverse sampling as we demand speed more than accuracy. On the other hand, we use the same\nscheduler as in [50] with σmin = 0.0001, σmax = 3 and ρ = 9. This combination allows us to sample\na style vector for high-quality speech synthesis with only three steps, equivalent to running a 9-layer\ntransformer model, minimally impacting the inference speed (see Appendix B for more discussions).\nV conditions on t through hbert concatenated with the noisy input E(x) + σξ, and σ is conditioned\nvia sinusoidal positional embeddings [ 53]. In the multispeaker setting, we model p(s|t, c) by\nK(s; t, c, σ) with an additional speaker embedding c = E(xref) where xref is the reference audio of\nthe target speaker. The speaker embedding c is injected into V by adaptive layer normalization [6].\n3.2.3 SLM Discriminators\nSpeech language models (SLMs) encode valuable information ranging from acoustic to semantic\naspects [55], and SLM representations are shown to mimic human perception for evaluating synthe-\nsized speech quality [45]. We uniquely transfer knowledge from SLM encoders to generative tasks\nvia adversarial training by employing a 12-layer WavLM [11] pre-trained on 94k hours of data 1 as\nthe discriminator. As the number of parameters of WavLM is greater than StyleTTS 2, to avoid dis-\ncriminator overpowering, we fix the pre-trained WavLM modelW and append a convolutional neural\nnetwork (CNN) C as the discriminative head. We denote the SLM discriminator DSLM = C ◦ W.\nThe input audios are downsampled to 16 kHz before being fed into DSLM to match that of WavLM.\nC pools features hSLM = W(x) from all layers with a linear map from 13 × 768 to 256 channels.\nWe train the generator components (T, B, G, S, P, V, denoted as G) and DSLM to optimize:\nLslm = min\nG\nmax\nDSLM\n(Ex[log DSLM (x)] + Et[log (1− DSLM (G(t)))]) , (5)\n1Available at https://huggingface.co/microsoft/wavlm-base-plus\n5\nwhere G(t) is the generated speech with text t, and x is the human recording. [56] shows that:\nD∗\nSLM (x) = PW◦T (x)\nPW◦T (x) + PW◦G(x), (6)\nwhere D∗\nSLM (x) is the optimal discriminator,T and G represent true and generated data distributions,\nwhile PT and PG are their respective densities. The optimal G∗ is achieved if PW◦T = PW◦G, mean-\ning that when converged, G∗ matches the generated and true distributions in the SLM representation\nspace, effectively mimicking human perception to achieve human-like speech synthesis.\nIn equation 5, the generator loss is independent of ground truth x and relies only on input text t. This\nenables training on out-of-distribution (OOD) texts, which we show in section 5.3 can improve the\nperformance on OOD texts. In practice, to prevent DSLM from over-fitting on the content of the\nspeech, we sample in-distribution and OOD texts with equal probability during training.\n3.2.4 Differentiable Duration Modeling\nThe duration predictor produces phoneme durations dpred, but the upsampling method described in\nsection 3.1 to obtainapred is not differentiable, blocking gradient flow for E2E training. NaturalSpeech\n[5] employs an attention-based upsampler [42] for human-level TTS. However, we find this approach\nunstable during adversarial training because we train our model using differentiable upsampling\nwith only the adversarial objective described in eq. 5 and without extra loss terms due to the length\nmismatch caused by deviations of dpred from dgt. Although this mismatch can be mitigated with soft\ndynamic time warping as used in [42, 5], we find this approach both computationally expensive and\nunstable with mel-reconstruction and adversarial objectives. To achieve human-level performance\nwith adversarial training, a non-parametric upsampling method is preferred for stable training.\nGaussian upsampling [41] is non-parametric and converts the predicted duration dpred into apred[n, i]\nusing a Gaussian kernel Nci(n; σ) centered at ci := ℓi − 1\n2 dpred[i] with the hyperparameter σ:\nNci(n; σ) := exp\n\u0012\n−(n − ci)2\n2σ2\n\u0013\n, (7) ℓi :=\niX\nk=1\ndpred[k], (8)\nwhere ℓi is the end position and ci is the center position of the ith phoneme ti. However, Gaussian\nupsampling has limitations due to its fixed width of Gaussian kernels determined byσ. This constraint\nprevents it from accurately modeling alignments with varying lengths depending on dpred. Non-\nattentive Tacotron [57] extends this by making σi trainable, but the trained parameters introduce\ninstability for E2E training with adversarial loss, similar to issues of attention-based upsamplers.\nWe propose a new non-parametric differentiable upsampler without additional training while taking\ninto account the varying length of the alignment. For each phoneme ti, we model the alignment as a\nrandom variable ai ∈ N, indicating the index of the speech frame the phoneme ti is aligned with. We\ndefine the duration of the ith phoneme as another random variable di ∈ {1, . . . , L}, where L = 50\nis the maximum phoneme duration hyperparameter, equivalent to 1.25 seconds in our setting. We\nobserve that ai =\niP\nk=1\ndk, but each dk is dependent on each other, making the sum difficult to model.\nInstead, we approximate ai = di + ℓi−1. The approximated probability mass function (PMF) of ai is\nfai[n] = fdi+ℓi−1 [n] = fdi[n] ∗ fℓi−1 [n] =\nX\nk\nfdi[k] · δℓi−1 [n − k], (9)\nwhere δℓi−1 is the PMF of ℓi−1, a constant defined in eq. 8. This delta function is not differentiable,\nso we replace it with Nℓi−1 defined in eq. 7 with σ = 1.5 (see Appendix C.2 for more discussions).\nTo modelfdi, we modify the duration predictor to outputq[k, i], the probability of theith phoneme hav-\ning a duration of at leastk for k ∈ {1, . . . , L}, optimized to be 1 when dgt ≥ k with the cross-entropy\nloss (see Appendix G). Under this new scheme, we can approximate dpred[i] :=\nLP\nk=1\nq[k, i], trained to\nmatch dgt with Ldur as in section 3.1. The vector q[:, i] can be viewed as the unnormalized version of\nfdi, though it is trained to be uniformly distributed across the interval [1, di]. Since the number of\nspeech frames M is usually larger than the number of input phonemes N, this uniform distribution\n6\naligns single phonemes to multiple speech frames as desired. Finally, we normalize the differentiable\napproximation ˜fai[n] across the phoneme axis, as in [41], to obtain apred using the softmax function:\napred[n, i] := e( ˜fai[n]))\nNP\ni=1\ne( ˜fai[n])\n, (10) ˜fai[n] :=\nˆMX\nk=0\nq[n, i] · Nℓi−1 (n − k; σ), (11)\nwhere ˆM := ⌈ℓN ⌉ is the predicted total duration of the speech and n ∈ {1, . . . ,ˆM}. An illustration\nof our proposed differentiable duration modeling is given in Figure 4 (Appendix C).\n4 Experiments\n4.1 Model Training\nWe performed experiments on three datasets: LJSpeech, VCTK, and LibriTTS. Our single-speaker\nmodel was trained on the LJSpeech dataset, consisting of 13,100 short audio clips totaling roughly\n24 hours. This dataset was divided into training (12,500 samples), validation (100 samples), and\ntesting (500 samples) sets, with the same split as [3, 5, 6]. The multispeaker model was trained on\nVCTK, comprising nearly 44,000 short clips from 109 native speakers with various accents. The data\nsplit was the same as [3], with 43,470 samples for training, 100 for validation, and 500 for testing.\nLastly, we trained our model on the combined LibriTTS train-clean-460 subset [14] for zero-shot\nadaptation. This dataset contains about 245 hours of audio from 1,151 speakers. Utterances longer\nthan 30 seconds or shorter than one second were excluded. We distributed this dataset into training\n(98%), validation (1%), and testing (1%) sets, in line with [6]. The test-clean subset was used for\nzero-shot adaptation evaluation with 3-second reference clips. All datasets were resampled to 24 kHz\nto match LibriTTS, and the texts were converted into phonemes using phonemizer [58].\nWe used texts in the training split of LibriTTS as the out-of-distribution (OOD) texts for SLM\nadversarial training. We used iSTFTNet decoder for LJSpeech due to its speed and sufficient\nperformance on this dataset, while the HifiGAN decoder was used for the VCTK and LibriTTS\nmodels. Acoustic modules were pre-trained for 100, 50, and 30 epochs on the LJSpeech, VCTK, and\nLibriTTS datasets, and joint training followed for 60, 40, and 25 epochs, respectively. We employed\nthe AdamW optimizer [59] with β1 = 0, β2 = 0.99, weight decay λ = 10−4, learning rate γ = 10−4\nand a batch size of 16 samples for both pre-training and joint training. The loss weights were adopted\nfrom [6] to balance all loss terms (see Appendix G for details). Waveforms were randomly segmented\nwith a max length of 3 seconds. For SLM adversarial training, both the ground truth and generated\nsamples were ensured to be 3 to 6 seconds in duration, the same as in fine-tuning of WavLM models\nfor various downstream tasks [11]. Style diffusion steps were randomly sampled from 3 to 5 during\ntraining for speed and set to 5 during inference for quality. The training was conducted on four\nNVIDIA A40 GPUs.\n4.2 Evaluations\nWe employed two metrics in our experiments: Mean Opinion Score of Naturalness (MOS-N) for\nhuman-likeness, and Mean Opinion Score of Similarity (MOS-S) for similarity to the reference\nfor multi-speaker models. These evaluations were conducted by native English speakers from the\nU.S. on Amazon Mechanical Turk. All evaluators reported normal hearing and provided informed\nconsent as monitored by the local institutional review board and in accordance with the ethical\nstandards of the Declaration of Helsinki 2. In each test, 80 random text samples from the test set\nwere selected and converted into speech using our model and the baseline models, along with ground\ntruth for comparison. Because [7] reported that many TTS models perform poorly for OOD texts, our\nLJSpeech experiments also included 40 utterances from Librivox spoken by the narrator of LJSpeech\nbut from audiobooks not in the original dataset as the ground truth for OOD texts. To compare\nthe difference between in-distribution and OOD performance, we asked the same raters to evaluate\nsamples on both in-distribution and OOD texts.\nOur baseline models consisted of the three highest-performing public models: VITS [3], StyleTTS [6],\nand JETS [32] for LJSpeech; and VITS, YourTTS [60], and StyleTTS for LibriTTS. Each synthesized\n2We obtained approval for our protocol (number IRB-AAAR8655) from the Institutional Review Board.\n7\nTable 1: Comparative mean opinion scores of naturalness and similarity for StyleTTS 2 with p-values\nfrom Wilcoxon test relative to other models. Positive scores indicate StyleTTS 2 is better.\nModel Dataset CMOS-N (p-value) CMOS-S (p-value)\nGround Truth LJSpeech +0.28 (p = 0.021) —\nNaturalSpeech LJSpeech +1.07 (p <10−6) —\nGround Truth VCTK −0.02 (p = 0.628) +0.30 (p = 0.081)\nVITS VCTK +0.45 (p = 0.009) +0.43 (p = 0.032)\nVall-E LibriSpeech (zero-shot) +0.67 (p <10−3) −0.47 (p <10−3)\nspeech set was rated by 5 to 10 evaluators on a 1-5 scale, with increments of 0.5. We randomized\nthe model order and kept their labels hidden, similar to the MUSHRA approach [61, 62]. We also\nconducted Comparative MOS (CMOS) tests to determine statistical significance, as raters can ignore\nsubtle differences in MOS experiments [3, 7, 5]. Raters were asked to listen to two samples and rate\nwhether the second was better or worse than the first on a -6 to 6 scale with increments of 1. We\ncompared our model to the ground truth and NaturalSpeech [5] for LJSpeech, and the ground truth\nand VITS for VCTK. For the zero-shot experiment, we compared our LibriTTS model to Vall-E [8].\nAll baseline models, except for the publicly available VITS model on LibriTTS from ESPNet Toolkit\n[63], were official checkpoints released by the authors, including vocoders used in StyleTTS. As\nNaturalSpeech and Vall-E are not publicly available, we obtained samples from the authors and the\nofficial Vall-E demo page, respectively. For fairness, we resampled all audio to 22.5 kHz for LJSpeech\nand VCTK, and 16 kHz for LibriTTS, to match the baseline models. We conducted ablation studies\nusing CMOS-N on LJSpeech on OOD texts from LibriTTS test-clean subset for more pronounced\nresults as in [7]. For more details of our evaluation procedures, please see Appendix E.\n5 Results\n5.1 Model Performance\nThe results outlined in Table 1 establish that StyleTTS 2 outperforms NaturalSpeech by achieving\na CMOS of +1.07 (p ≪ 0.01), setting a new standard for this dataset. Interestingly, StyleTTS 2\nwas favored over the ground truth with a CMOS of +0.28 (p <0.05). This preference may stem\nfrom dataset artifacts such as fragmented audiobook passages in the LJSpeech dataset that disrupt\nnarrative continuity, thus rendering the ground truth narration seemingly unnatural. This hypothesis is\ncorroborated by the performance of StyleTTS 2 on the VCTK dataset, which lacks narrative context,\nwhere it performs comparably to the ground truth (CMOS = −0.02, p≫ 0.05). Samples from our\nmodel were more similar to the reference audio speaker than the human recording, suggesting our\nmodel’s effective use of the reference audio for style diffusion. Moreover, StyleTTS 2 scored higher\nthan the previous state-of-the-art model VITS on VCTK, as evidenced by CMOS-N and CMOS-S.\nConsistent with the CMOS results, our model achieved a MOS of 3.83, surpassing all previous\nmodels on LJSpeech (Table 2). In addition, all models, except ours, exhibited some degrees of\nquality degradation for out-of-distribution (OOD) texts. This corroborates the gap reported in [7],\nwith our results providing additional ground truth references. On the other hand, our model did not\nshow any degradation and significantly outperformed other models in MOS for OOD texts (Table 2),\ndemonstrating its strong generalization ability and robustness towards OOD texts.\nTable 2: Comparison of MOS with 95% confidence in-\ntervals (CI) on LJSpeech. MOSID represents MOS-N for\nin-distribution texts, while MOSOOD is that for OOD texts.\nModel MOS ID (CI) MOS OOD (CI)\nGround Truth 3.81 ( ± 0.09) 3.70 ( ± 0.11)\nStyleTTS 2 3.83 (± 0.08) 3.87 (± 0.08)\nJETS 3.57 ( ± 0.09) 3.21 ( ± 0.12)\nVITS 3.34 ( ± 0.10) 3.21 ( ± 0.11)\nStyleTTS + HifiGAN 3.35 ( ± 0.10) 3.32 ( ± 0.12)\nIn zero-shot tests, StyleTTS 2 surpasses\nVall-E in naturalness with a CMOS\nof +0.67 (p ≪ 0.01), although it falls\nslightly short in similarity (Table 1). Im-\nportantly, StyleTTS 2 achieved these re-\nsults with only 245 hours of training\ndata, compared to Vall-E’s 60k hours, a\n250x difference. This makes StyleTTS\n2 a data-efficient alternative to large pre-\ntraining methods like Vall-E. The MOS\nresults in Table 3 support these findings,\nas our model exceeds all baseline models in both MOS-N and MOS-S. However, the difference in\n8\n(a) LJSpeech model.\n (b) Unseen speakers on LibriTTS.\n (c) Zoomed-in unseen speaker.\nFigure 2: t-SNE visualization of style vectors sampled via style diffusion from texts in five emotions,\nshowing that emotions are properly separated for seen and unseen speakers. (a) Clusters of emotion\nfrom styles sampled by the LJSpeech model. (b) Distinct clusters of styles sampled from 5 unseen\nspeakers by the LibriTTS model. (c) Loose clusters of emotions from Speaker 1 in (b).\nMOS-S between StyleTTS and StyleTTS 2 was not statistically significant, hinting at future directions\nfor improvement in speaker similarity.\n5.2 Style Diffusion\nTable 3: Comparison of MOS with 95% confidence intervals (CI)\non test-clean subset of LibriTTS for zero-shot speaker adaptation.\nModel MOS-N (CI) MOS-S (CI)\nGround Truth 4.60 ( ± 0.09) 4.35 ( ± 0.10)\nStyleTTS 2 4.15 (± 0.11) 4.03 (± 0.11)\nYourTTS 2.35 ( ± 0.07) 2.42 ( ± 0.09)\nVITS 3.69 ( ± 0.12) 3.54 ( ± 0.13)\nStyleTTS + HiFi-GAN 3.91 ( ± 0.11) 4.01 ( ± 0.10)\nFigure 2 shows t-SNE visual-\nizations of style vectors created\nusing our style diffusion pro-\ncess. Due to the lack of emotion-\nlabeled audiobook text datasets,\nwe used GPT-4 to generate 500\nutterances across five emotions\nfor this task [ 64]. In Figure 2a,\nstyle vectors from the LJSpeech\nmodel illustrate distinct emo-\ntional styles in response to input text sentiment, demonstrating the model’s capability to synthesize\nexpressive speech in varied emotions without explicit emotion labels during training. This process\nwas repeated with the LibriTTS model on five unseen speakers, each from a 3-second reference audio.\nAs depicted in Figure 2b, distinct clusters form for each speaker, showcasing a wide stylistic diversity\nderived from a single reference audio. Figure 2c provides a more nuanced view of the first speaker,\nrevealing visible emotion-based clusters despite some overlaps, indicating that we can manipulate the\nemotional tone of an unseen speaker regardless of the tones in the reference audio. These overlaps,\nhowever, can partly explain why the LibriTTS model does not perform as well as the LJSpeech\nmodel, as it is harder to disentangle texts from speakers in the zero-shot setting (see Appendix A.2 for\nmore results). Table 4 displays our synthesized speech diversity against several baseline models with\nthe coefficient of variation of duration (CVdur) and pitch curve (CVf0) from the same input text [3].\nOur model yields the highest variation, indicating superior potential for generating diverse speech.\nDespite being diffusion-based, our model is faster than VITS, FastDiff [28], and ProDiff [18], two of\nthe fastest diffusion-based TTS models, with 5 iterations of diffusion and iSTFTNet decoder.\n5.3 Ablation Study\nTable 4: Speech diversity metrics and real-time\nfactor (RTF). ProDiff and FastDiff were tested\nwith 4 steps of diffusion.\nModel CV dur ↑ CVf0 ↑ RTF ↓\nStyleTTS 2 0.0321 0.6962 0.0185\nVITS 0.0214 0.5976 0.0599\nFastDiff 0.0295 0.6490 0.0769\nProDiff 2e-16 0.5898 0.1454\nTable 5: CMOS-N relative to the StyleTTS 2\nbaseline on OOD texts in the ablation study.\nModel CMOS-N\nw/o style diffusion −0.46\nw/o differentiable upsampler −0.21\nw/o SLM adversarial training −0.32\nw/o prosodic style encoder −0.35\nw/o OOD texts −0.15\n9\nTable 5 details the ablation study, underlying the importance of our proposed components. When\nstyle vectors from style diffusion are substituted with randomly encoded ones as in [6], the CMOS is\n−0.46, highlighting the contribution of text-dependent style diffusion to achieving human-level TTS.\nTraining without our differentiable upsampler and without the SLM discriminator results in a CMOS\nof −0.21 and −0.32, validating their key roles in natural speech synthesis. Removing the prosodic\nstyle encoder also yields a −0.35 CMOS. Last, excluding OOD texts from adversarial training leads\nto a CMOS of −0.15, proving its efficacy for improving OOD speech synthesis. Table 6 in Appendix\nA.3 shows similar outcomes with objective evaluations, further affirming the effectiveness of various\ncomponents we proposed in this work. Figure 7 in Appendix D details a layer-wise analysis of input\nweights of the SLM discriminator, providing a different view of the efficacy of SLM discriminators.\n6 Conclusions and Limitations\nIn this study, we present StyleTTS 2, a novel text-to-speech (TTS) model with human-level perfor-\nmance via style diffusion and speech language model discriminators. In particular, it exceeds the\nground truth on LJSpeech and performs on par with it on the VCTK dataset. StyleTTS 2 also shows\npotential for zero-shot speaker adaption, with remarkable performance even on limited training data\ncompared to large-scale models like Vall-E. With our innovative style diffusion method, StyleTTS 2\ngenerates expressive and diverse speech of superior quality while ensuring fast inference time. While\nStyleTTS 2 excels in several areas, our results indicate room for improvement in handling large-scale\ndatasets such as LibriTTS, which contain thousands of or more speakers, acoustic environments,\naccents, and other various aspects of speaking styles. The speaker similarity in the aforementioned\nzero-shot adaptation speaker task could also benefit from further improvements.\nHowever, zero-shot speaker adaptation has the potential for misuse and deception by mimicking the\nvoices of individuals as a potential source of misinformation or disinformation. This could lead to\nharmful, deceptive interactions such as theft, fraud, harassment, or impersonations of public figures\nthat may influence political processes or trust in institutions. In order to manage the potential for\nharm, we will require users of our model to adhere to a code of conduct that will be clearly displayed\nas conditions for using the publicly available code and models. In particular, we will require users to\ninform those listening to samples synthesized by StyleTTS 2 that they are listening to synthesized\nspeech or to obtain informed consent regarding the use of samples synthesized by StyleTTS 2 in\nexperiments. Users will also be required to use reference speakers who have given consent to have\ntheir voice adapted, either directly or by license. Finally, we will make the source code publicly\navailable for further research in speaker fraud and impersonation detection.\nIn addition, while human evaluators have favored StyleTTS 2 over ground truth with statistical\nsignificance on the LJSpeech dataset, this preference may be context-dependent. Original audio\nsegments from larger contexts like audiobooks could inherently differ in naturalness when isolated,\npotentially skewing the evaluations in favor of synthesized speech. Additionally, the inherent\nvariability in human speech, which is context-independent, might lead to lower ratings when compared\nto the more uniform output from StyleTTS 2. Future research should aim to improve evaluation\nmethods to address these limitations and develop more natural and human-like speech synthesis\nmodels with longer context dependencies.\n7 Acknowledgments\nWe thank Menoua Keshishian, Vishal Choudhari, and Xilin Jiang for helpful discussions and feedback\non the paper. We also thank Grace Wilsey, Elden Griggs, Jacob Edwards, Rebecca Saez, Moises\nRivera, Rawan Zayter, and D.M. for assessing the quality of synthesized samples and providing\nfeedback on the quality of models during the development stage of this work. This work was funded\nby the national institute of health (NIHNIDCD) and a grant from Marie-Josee and Henry R. Kravis.\nReferences\n[1] Yishuang Ning, Sheng He, Zhiyong Wu, Chunxiao Xing, and Liang-Jie Zhang. A review of\ndeep learning based speech synthesis. Applied Sciences, 9(19):4050, 2019.\n10\n[2] Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. A survey on neural speech synthesis.arXiv\npreprint arXiv:2106.15561, 2021.\n[3] Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversar-\nial learning for end-to-end text-to-speech. In International Conference on Machine Learning,\npages 5530–5540. PMLR, 2021.\n[4] Ye Jia, Heiga Zen, Jonathan Shen, Yu Zhang, and Yonghui Wu. PnG BERT: Augmented BERT\non phonemes and graphemes for neural TTS. arXiv preprint arXiv:2103.15060, 2021.\n[5] Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng,\nYuanhao Yi, Lei He, Frank K. Soong, Tao Qin, Sheng Zhao, and Tie-Yan Liu. NaturalSpeech:\nEnd-to-End Text to Speech Synthesis with Human-Level Quality. ArXiv, abs/2205.04421, 2022.\n[6] Yinghao Aaron Li, Cong Han, and Nima Mesgarani. StyleTTS: A Style-Based Generative\nModel for Natural and Diverse Text-to-Speech Synthesis. arXiv preprint arXiv:2205.15439,\n2022.\n[7] Yinghao Aaron Li, Cong Han, Xilin Jiang, and Nima Mesgarani. Phoneme-Level Bert for\nEnhanced Prosody of Text-To-Speech with Grapheme Predictions. In ICASSP 2023-2023 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.\n[8] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen,\nYanqing Liu, Huaming Wang, Jinyu Li, et al. Neural Codec Language Models are Zero-Shot\nText to Speech Synthesizers. arXiv preprint arXiv:2301.02111, 2023.\n[9] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0:\nA framework for self-supervised learning of speech representations. Advances in neural\ninformation processing systems, 33:12449–12460, 2020.\n[10] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed. HuBERT: Self-Supervised Speech Representation Learning by\nMasked Prediction of Hidden Units. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 29:3451–3460, 2021.\n[11] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li,\nNaoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. WavLM: Large-Scale Self-Supervised\nPre-Training for Full Stack Speech Processing. IEEE Journal of Selected Topics in Signal\nProcessing, 16(6):1505–1518, 2022.\n[12] Keith Ito and Linda Johnson. The lj speech dataset. https://keithito.com/\nLJ-Speech-Dataset/, 2017.\n[13] Junichi Yamagishi, Christophe Veaux, Kirsten MacDonald, et al. CSTR VCTK Corpus: English\nMulti-Speaker Corpus for CSTR V oice Cloning Toolkit (version 0.92). 2019.\n[14] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen, and Yonghui\nWu. LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech. Proc. Interspeech\n2019, pages 1526–1530, 2019.\n[15] Chenshuang Zhang, Chaoning Zhang, Sheng Zheng, Mengchun Zhang, Maryam Qamar, Sung-\nHo Bae, and In So Kweon. A Survey on Audio Diffusion Models: Text To Speech Synthesis\nand Enhancement in Generative AI. arXiv preprint arXiv:2303.13336, 2, 2023.\n[16] Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim.\nDiff-TTS: A Denoising Diffusion Model for Text-to-Speech. In Interspeech, 2021.\n[17] Vadim Popov, Ivan V ovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-\nTTS: A Diffusion Probabilistic Model for Text-to-Speech. In International Conference on\nMachine Learning, pages 8599–8608. PMLR, 2021.\n[18] Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, and Yi Ren. ProDiff:\nProgressive Fast Diffusion Model for High-Quality Text-to-Speech. In Proceedings of the 30th\nACM International Conference on Multimedia, pages 2595–2605, 2022.\n[19] Songxiang Liu, Dan Su, and Dong Yu. DiffGAN-TTS: High-Fidelity and Efficient Text-to-\nSpeech with Denoising Diffusion GANs. arXiv preprint arXiv:2201.11972, 2022.\n[20] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu.\nDiffsound: Discrete Diffusion Model for Text-to-Sound Generation. IEEE/ACM Transactions\non Audio, Speech, and Language Processing, 2023.\n11\n[21] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan.\nWaveGrad: Estimating Gradients for Waveform Generation. In International Conference on\nLearning Representations.\n[22] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. DiffWave: A Versatile\nDiffusion Model for Audio Synthesis. ArXiv, abs/2009.09761, 2020.\n[23] Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen,\nSungroh Yoon, and Tie-Yan Liu. PriorGrad: Improving Conditional Denoising Diffusion Models\nwith Data-Dependent Adaptive Prior. In International Conference on Learning Representations.\n[24] Zehua Chen, Xu Tan, Ke Wang, Shifeng Pan, Danilo Mandic, Lei He, and Sheng Zhao. Infergrad:\nImproving diffusion models for vocoder by considering inference in training. In ICASSP 2022-\n2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\npages 8432–8436. IEEE, 2022.\n[25] Yuma Koizumi, Heiga Zen, Kohei Yatabe, Nanxin Chen, and Michiel Bacchiani. SpecGrad:\nDiffusion Probabilistic Model based Neural V ocoder with Adaptive Noise Spectral Shaping. In\nInterspeech, 2022.\n[26] Yuma Koizumi, Kohei Yatabe, Heiga Zen, and Michiel Bacchiani. WaveFit: An Iterative and\nNon-autoregressive Neural V ocoder based on Fixed-Point Iteration. In 2022 IEEE Spoken\nLanguage Technology Workshop (SLT), pages 884–891. IEEE, 2023.\n[27] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, Najim Dehak, and\nWilliam Chan. Wavegrad 2: Iterative refinement for text-to-speech synthesis. In Proceedings of\nInterspeech, pages 3765–3769, 2021.\n[28] Rongjie Huang, Max W. Y . Lam, J. Wang, Dan Su, Dong Yu, Yi Ren, and Zhou Zhao. FastDiff:\nA Fast Conditional Diffusion Model for High-Quality Speech Synthesis. In International Joint\nConference on Artificial Intelligence, 2022.\n[29] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and\nJiang Bian. NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and\nSinging Synthesizers. arXiv preprint arXiv:2304.09116, 2023.\n[30] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative Adversarial Networks\nfor Efficient and High Fidelity Speech Synthesis. Advances in Neural Information Processing\nSystems, 33:17022–17033, 2020.\n[31] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. BigVGAN: A\nUniversal Neural V ocoder with Large-Scale Training.arXiv preprint arXiv:2206.04658, 2022.\n[32] Dan Lim, Sunghee Jung, and Eesung Kim. JETS: Jointly training FastSpeech2 and HiFi-GAN\nfor end to end text to speech. arXiv preprint arXiv:2203.16852, 2022.\n[33] Matthew Baas and Herman Kamper. GAN You Hear Me? Reclaiming Unconditional Speech\nSynthesis from Diffusion Models. In 2022 IEEE Spoken Language Technology Workshop (SLT),\npages 906–911. IEEE, 2023.\n[34] Chenpeng Du, Yiwei Guo, Xie Chen, and Kai Yu. VQTTS: High-Fidelity Text-to-Speech\nSynthesis with Self-Supervised VQ Acoustic Feature. arXiv preprint arXiv:2204.00768, 2022.\n[35] Hubert Siuzdak, Piotr Dura, Pol van Rijn, and Nori Jacoby. WavThruVec: Latent speech repre-\nsentation as intermediate features for neural speech synthesis. arXiv preprint arXiv:2203.16930,\n2022.\n[36] Sang-Hoon Lee, Seung-Bin Kim, Ji-Hyun Lee, Eunwoo Song, Min-Jae Hwang, and Seong-\nWhan Lee. HierSpeech: Bridging the Gap between Text and Speech by Hierarchical Variational\nInference using Self-supervised Representations for Speech Synthesis. Advances in Neural\nInformation Processing Systems, 35:16624–16636, 2022.\n[37] Jinlong Xue, Yayue Deng, Fengping Wang, Ya Li, Yingming Gao, Jianhua Tao, Jianqing Sun,\nand Jiaen Liang. M2-CTTS: End-to-End Multi-scale Multi-modal Conversational Text-to-\nSpeech Synthesis. arXiv preprint arXiv:2305.02269, 2023.\n[38] Ruiqing Xue, Yanqing Liu, Lei He, Xu Tan, Linquan Liu, Edward Lin, and Sheng Zhao.\nFoundationTTS: Text-to-Speech for ASR Custmization with Generative Language Model.\narXiv preprint arXiv:2303.02939, 2023.\n12\n[39] Kenichi Fujita, Takanori Ashihara, Hiroki Kanagawa, Takafumi Moriya, and Yusuke Ijima.\nZero-shot text-to-speech synthesis conditioned using self-supervised speech representation\nmodel. arXiv preprint arXiv:2304.11976, 2023.\n[40] Guangyan Zhang, Kaitao Song, Xu Tan, Daxin Tan, Yuzi Yan, Yanqing Liu, G. Wang, Wei\nZhou, Tao Qin, Tan Lee, and Sheng Zhao. Mixed-Phoneme BERT: Improving BERT with\nMixed Phoneme and Sup-Phoneme Representations for Text to Speech. In Interspeech, 2022.\n[41] Jeff Donahue, Sander Dieleman, Mikołaj Bi´nkowski, Erich Elsen, and Karen Simonyan. End-\nto-End Adversarial Text-to-Speech. arXiv preprint arXiv:2006.03575, 2020.\n[42] Isaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang, Jia Ye, R. J. Skerry-Ryan, and Yonghui Wu.\nParallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration\nModeling. In Interspeech, 2021.\n[43] Cheng-Han Chiang, Wei-Ping Huang, and Hung-yi Lee. Why we should report the details in\nsubjective evaluation of tts more rigorously. arXiv preprint arXiv:2306.02044, 2023.\n[44] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance\nnormalization. In Proceedings of the IEEE international conference on computer vision, pages\n1501–1510, 2017.\n[45] Takuhiro Kaneko, Kou Tanaka, Hirokazu Kameoka, and Shogo Seki. iSTFTNet: Fast and\nlightweight mel-spectrogram vocoder incorporating inverse short-time Fourier transform. In\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 6207–6211. IEEE, 2022.\n[46] Liu Ziyin, Tilman Hartwig, and Masahito Ueda. Neural networks fail to learn periodic functions\nand how to fix it. Advances in Neural Information Processing Systems, 33:1583–1594, 2020.\n[47] Won Jang, Daniel Chung Yong Lim, Jaesam Yoon, Bongwan Kim, and Juntae Kim. UnivNet: A\nNeural V ocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform\nGeneration. In Interspeech, 2021.\n[48] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.\nLeast squares generative adversarial networks. In Proceedings of the IEEE international\nconference on computer vision, pages 2794–2802, 2017.\n[49] Yanli Li and Congyi Wang. Improve gan-based neural vocoder using truncated pointwise\nrelativistic least square gan. In Proceedings of the 4th International Conference on Advanced\nInformation Science and System, pages 1–7, 2022.\n[50] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of\ndiffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022.\n[51] Chin-Wei Huang, Jae Hyun Lim, and Aaron C Courville. A variational perspective on diffusion-\nbased generative models and score matching. Advances in Neural Information Processing\nSystems, 34:22863–22876, 2021.\n[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. arXiv\npreprint arXiv:2011.13456, 2020.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[54] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver:\nA Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. arXiv\npreprint arXiv:2206.00927, 2022.\n[55] Ankita Pasad, Ju-Chieh Chou, and Karen Livescu. Layer-wise analysis of a self-supervised\nspeech representation model. In 2021 IEEE Automatic Speech Recognition and Understanding\nWorkshop (ASRU), pages 914–921. IEEE, 2021.\n[56] Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger. Projected GANs Converge\nFaster. Advances in Neural Information Processing Systems, 34:17480–17492, 2021.\n[57] Jonathan Shen, Ye Jia, Mike Chrzanowski, Yu Zhang, Isaac Elias, Heiga Zen, and Yonghui Wu.\nNon-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised\nDuration Modeling. arXiv preprint arXiv:2010.04301, 2020.\n13\n[58] Mathieu Bernard and Hadrien Titeux. Phonemizer: Text to Phones Transcription for Multiple\nLanguages in Python. Journal of Open Source Software, 6(68):3958, 2021.\n[59] Ilya Loshchilov and Frank Hutter. Fixing Weight Decay Regularization in Adam, 2018.\n[60] Edresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren Gölge,\nand Moacir A Ponti. YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot V oice\nConversion for everyone. In International Conference on Machine Learning, pages 2709–2720.\nPMLR, 2022.\n[61] Yinghao Aaron Li, Ali Zare, and Nima Mesgarani. StarGANv2-VC: A Diverse, Unsu-\npervised, Non-parallel Framework for Natural-Sounding V oice Conversion. arXiv preprint\narXiv:2107.10394, 2021.\n[62] Yinghao Aaron Li, Cong Han, and Nima Mesgarani. Styletts-vc: One-shot voice conversion by\nknowledge transfer from style-based tts models. In 2022 IEEE Spoken Language Technology\nWorkshop (SLT), pages 920–927. IEEE, 2023.\n[63] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno,\nNelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Ren-\nduchintala, and Tsubasa Ochiai. ESPnet: End-to-end speech processing toolkit. In Proceedings\nof Interspeech, pages 2207–2211, 2018.\n[64] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He,\nAntong Li, Mengshen He, Zhengliang Liu, et al. Summary of ChatGPT/GPT-4 Research and\nPerspective Towards the Future of Large Language Models. arXiv preprint arXiv:2304.01852,\n2023.\n14\nAppendix A Additional Evaluation Results\nA.1 Feedback Analysis from Survey Participants\nOn the last page of our survey, we encouraged the raters to provide their feedback on the study. While\nthe majority opted not to comment, those who did provided valuable insights into our survey design\nand results interpretation. The main point of feedback revolved around the difficulty in discerning\ndifferences between different samples, specifically for the LJSpeech CMOS experiments. Below are\na few representative comments from our CMOS experiment participants:\n• Sounds great. At most points the differences were negligible.\n• I really liked the person who was reading the clips.\nShe sounds professional and was very good at changing her tone\nof voice. Sometimes it was hard to evaluate the clip because you\nhad to listen carefully when they sounded similar.\nI didn’t have any problems. I enjoyed participating Thank you!\n• For most of the rounds, it was difficult to notice a difference\nbetween the voices.\n• Interested to see the results - differences were sometimes\nnoticeable, often subtle\nThis feedback strengthens our conclusion that StyleTTS 2 has reached human-like text-to-speech\nsynthesis quality. Several participants offered specific observations regarding the differences:\n• Some sounded a little unnatural because they sounded like an actor\nauditioning for a role - a little too dramatic.\n• So, a couple of comments. I feel like the expressiveness of the\nspeaker made me think it was more natural, but hyper expressiveness\nwould be unnatural in certain contexts (and did feel odd with some of\nthem but it felt more real). Additionally, the different voices, in\nbeing expressive, sometimes tended to turn into different emotions\nand contextual these emotions sometimes felt unnatural but felt real\nat the same time.\nFun study though. I wish you all well!\n• some sound relatively unexpected/unnatural, but most often it was\njust a minor difference\nThese comments suggest that the differences between StyleTTS 2 and the original audio mainly lie in\nsubtle nuances like intonation and word emphasis.\nOur results indicate that StyleTTS 2 scored statistically higher than the original audio in both MOS\nand CMOS, likely due to the absence of contextual continuity in the common evaluation procedure\nused in TTS studies. This disruption of narrative continuity in the LJSpeech dataset, which consists of\nisolated audiobook clips, might have led to the perceived unnaturalness of the ground truth compared\nto samples generated by StyleTTS 2. Future research should aim to incorporate context-aware\nlong-form generation into human-like text-to-speech synthesis to improve evaluation fairness and\nrelevance.\nA.2 Speech Expressiveness\nOur model’s ability to synthesize diverse and expressive speech is demonstrated by synthesizing\nspeech from 500 text samples across five emotions generated using GPT-4. This reinforces the results\nvisualized in Figure 2, which maps style vectors from texts associated with different emotions using\nt-SNE. We use the mean value of the F0 and energy curves to evaluate emotive speech.\nAs depicted in Figure 3 (a), speech synthesized by our model exhibits visible F0 and energy charac-\nteristics for each emotion, notably for anger and surprise texts, which deviate from the ground truth\naverage. This substantiates our model’s capacity to generate emotive speech from texts associated\nwith specific emotions. Conversely, VITS shows certain degrees of insensitivity to emotional variance,\n15\n(a) StyleTTS2\n(b) VITS\n(c) JETS\nFigure 3: Histograms and kernel density estimation of the mean F0 and energy values of speech,\nsynthesized with texts in five different emotions. The blue color (\"Ground Truth\") denotes the\ndistributions of the ground truth samples in the test set. StyleTTS 2 shows distinct distributions for\ndifferent emotions and produces samples that cover the entire range of the ground truth distributions.\nas displayed in Figure 3 (b). JETS, shown in Figure 3 (c), fails to span the full distribution of F0 and\nenergy, producing samples clustered around the mode. In contrast, our model extends across both F0\nand energy distributions, approximating the range of the ground truth.\nSince both StyleTTS 2 and VITS are probabilistic while JETS is deterministic, this shows that\nprobabilistic models generate more expressive speech compared to deterministic ones. Moreover,\nour model demonstrates slightly better mode coverage compared to VITS at the right tail of the\nenergy mean distribution, likely because we use diffusion-based models as opposed to the variational\nautoencoder used in VITS.\nFor audio samples of the synthesized speech and their corresponding emotional text, please refer to\nour demonstration page at https://styletts2.github.io/#emo.\nA.3 Ablation Study\nThe ablation study results, presented in Table 6, illustrate the importance of each component in\nour proposed model using several measures: mel-cepstral distortion (MCD), MCD weighted by\nSpeech Length (SL) that evaluates both the length and the quality of alignment between two speeches\n(MCD-SL), root mean square error of log F0 pitch (F0 RMSE), mean absolute deviation of phoneme\nduration (DUR MAD), and word error rate (WER), following [36]. The MCD and F0 RMSE were\ncomputed between the samples generated by each model and the ground truth aligned with dynamic\ntime warping. The MCD-SL was computed using a publicly available library 3. The WER was\ncomputed with a pre-trained ASR model 4 from ESPNet [63].\nMost notably, using randomly encoded style vectors instead of those sampled through style diffusion\nconsiderably affects all metrics, including the subjective CMOS score, thereby underscoring its\n3https://github.com/chenqi008/pymcd\n4Available at https://zenodo.org/record/4541452\n16\nTable 6: Comparison of models in ablation study with mel-cepstral distortion (MCD), MCD weighted\nby Speech Length (MCD-SL), root mean square error of log F0 pitch (F0 RMSE), mean absolute\ndeviation of phoneme duration (DUR MAD), and word error rate (WER) on LJSpeech. The CMOS\nresults are copied directly from Table 5 as a reference.\nModel MCD MCD-SL F0 RMSE DUR MAD WER CMOS\nProposed model 4.93 5.34 0.651 0.521 6.50% 0\nw/o style diffusion 8.30 9.33 0.899 0.634 8.77% −0.46\nw/o SLM adversarial training 4.95 5.40 0.692 0.513 6.52% −0.32\nw/o prosodic style encoder 5.04 5.42 0.663 0.543 6.92% −0.35\nw/o differentiable upsampler 4.94 5.34 0.880 0.525 6.54% −0.21\nw/o OOD texts 4.93 5.45 0.690 0.516 6.58% −0.15\nsignificance. Since [6] establishes that the style vectors crucially influence all aspects of the speech,\nsuch as pauses, emotions, speaking rates, and sound quality, style diffusion is the most important\nfactor in producing speech close to the ground truth and natural human speech.\nExcluding adversarial training with large speech language models (SLMs) results in a slight decline\nin MCD-SL and F0 RMSE but does not impact WER. Interestingly, this version produces the lowest\nduration error, implying that SLM discriminators may cause minor underfitting on in-distribution\ntexts, which can also be observed in models trained without OOD texts during SLM adversarial\ntraining. Nonetheless, subjective evaluations reveal a significant difference for out-of-distribution\n(OOD) texts for models trained without SLM discriminators (see Appendix D for more discussions).\nTraining without the differentiable upsampler increases theF0 RMSE, but leaves the MCD, MCD-SL,\nDUR MAD, and WER unaffected. The removal of the prosodic style encoder affects all metric scores,\nthus underscoring its effectiveness in our model. Lastly, training without OOD texts only affects the\nF0 RMSE in our objective evaluation.\nFurther details, discussions, and audio samples regarding these model variations can be found in\nSection 8 of our demo page: https://styletts2.github.io/#ab.\nAppendix B Style Diffusion\nB.1 EDM Formulation\nEDM [50] formulates the sampling procedure of diffusion-based generative model as an ordinary\ndifferential equation (ODE) (Eq. 1 in [50]):\ndx = −˙σ(t)σ(t)∇x log p(x; σ(t)) dt, (12)\nwhere the noise level schedule and its derivative are represented by σ(t) and ˙σ(t), respectively. The\nscore function of p(x) is ∇x log p(x; σ(t)), and p(x) is the targeted data distribution (please refer to\nAppendix B in [50] for a detailed derivation). The equation can be rewritten in terms of dσ as:\ndx = −σ∇x log p(x; σ) dσ. (13)\nThe denoising score matching objective is defined as follows (Eq. 108 in [50]):\nEy∼pdata,ξ∼N(0,σ2I),ln σ∼N(Pmean,P2\nstd)\nh\nλ(σ) ∥D(y + ξ; σ) − y∥2\n2\ni\n. (14)\nHere, y denotes the training sample from data distribution pdata, ξ is the noise, and D is the denoiser,\ndefined as (Eq. 7 in [50]):\nD(x; σ) := cskip(σ)x + cout(σ)Fθ(cin(σ)x; cnoise(σ)). (15)\nIn the above equation, Fθ represents the trainable neural network, cskip(σ) modulates the skip\nconnection, cin(σ) and cout(σ) regulate input and output magnitudes respectively, and cnoise(σ) maps\n17\nnoise level σ into a conditioning input for Fθ. With this formulation, the score function is given by\n(Eq. 3 in [50]):\n∇x log p(x; σ(t)) = (D(x; σ) − x) /σ2. (16)\nIn Table 1 of [50], the authors define the scaling factors as follows:\ncskip(σ) := σ2\ndata/(σ2 + σ2\ndata) = (σdata/σ∗)2, (17)\ncout(σ) := σ · σdata/\nq\nσ2\ndata + σ2 = σ · σdata/σ∗, (18)\ncin(σ) := 1/\nq\nσ2\ndata + σ2 = 1/σ∗, (19)\ncnoise(σ) := 1\n4 ln(σ), (20)\nwhere σ∗ :=\np\nσ2\ndata + σ2, a shorthand notation for simplicity.\nInserting equations 17, 18, 19 and 20 into equation 15 yields:\nD(x; σ) = cskip(σ)x + cout(σ)Fθ(cin(σ)x; cnoise(σ))\n=\n\u0010σdata\nσ∗\n\u00112\nx + σ · σdata\nσ∗ · Fθ\n\u0012 x\nσ∗ ; 1\n4 ln σ\n\u0013\n, (21)\nCombining equation 13 and 16, we obtain\ndx\ndσ = −σ∇x log p(x; σ(t))\n= −σ (D(x; σ) − x) /σ2\n= x − D(x; σ)\nσ .\n(22)\nEquations 2 and 4 can be recovered by replacing x with s, p(x) with p(s|t), Fθ with V and D with\nK in equations 21 and 22 respectively.\nIn EDM, the time step {σ0, σ1, . . . , σN−1} for the total step N is defined as (Eq. 5 in [50]):\nσi<N :=\n\u0012\nσmax\n1\nρ + i\nN − 1\n\u0010\nσmin\n1\nρ − σmax\n1\nρ\n\u0011\u0013ρ\n, (23)\nwhere σmax = σ0, σmin = σN−1, and ρ is the factor that shortens the step lengths near σmin at the\nexpense of longer steps near σmax. Optimal performance is observed in [ 50] when σmax ≫ σdata\nand ρ ∈ [5, 10]. Through empirical tests, we set σmin = 0.0001, σmax = 3 and ρ = 9 in our work,\nallowing fast sampling with small step sizes while producing high-quality speech samples.\nB.2 Effects of Diffusion Steps\nTable 7: Comparision of mel-cepstral distortion (MCD), MCD weighted by Speech Length\n(MCD-SL), root mean square error of log F0 pitch (F0 RMSE), word error rate (WER), real-time\nfactor (RTF), coefficient of variation of duration (CVdur), and coefficient of variation of pitch (CVf0)\nbetween different diffusion steps.\nStep MCD ↓ MCD-SL ↓ F0 RMSE ↓ WER ↓ RTF ↓ CVdur ↑ CVf0 ↑\n4 4.90 5.34 0.650 6.72% 0.0179 0.0207 0.5473\n8 4.93 5.33 0.674 6.53% 0.0202 0.0466 0.7073\n16 4.92 5.34 0.665 6.44% 0.0252 0.0505 0.7244\n32 4.92 5.32 0.663 6.56% 0.0355 0.0463 0.7345\n64 4.91 5.34 0.654 6.67% 0.0557 0.0447 0.7245\n128 4.92 5.33 0.656 6.73% 0.0963 0.0447 0.7256\nThe impact of diffusion steps on sample quality, speed, and diversity was studied using multiple\nindicators. We assessed the sample quality using mel-cepstral distortion (MCD), MCD weighted by\n18\nSpeech Length (MCD-SL), root mean square error of log F0 pitch (F0 RMSE), and word error rate\n(WER). We also examined the influence of diffusion steps on computational speed using the real-time\nfactor (RTF), which was computed on a single Nvidia RTX 2080 Ti GPU. Given the application of\nthe ancestral solver, which introduces noise at each integration step [ 52], it was postulated that a\nhigher number of steps would lead to more diverse samples. This hypothesis was tested using the\ncoefficient of variation on the F0 curve (CVf0) and duration (CVdur).\nThe model was run with diffusion steps ranging from 4 to 128, as results with only two steps\nwere disregarded due to the characteristics of the noise scheduler in EDM. Here, σ0 = σmax and\nσN−1 = σmin, and therefore, with merely two diffusion steps, the attainment of samples of acceptable\nquality is not feasible.\nThe results presented in Table 7 reveal negligible disparities in sample quality across varying diffusion\nsteps, with satisfactory quality samples being producible with as few as three steps in our experiments.\nConsequently, we decided to randomly sample styles with diffusion steps ranging from 3 to 5 during\ntraining in order to save time and GPU RAM. Furthermore, we observed an incremental rise in speech\ndiversity relative to the number of diffusion steps, reaching a plateau around 16 steps. Thereafter, the\nincrease is marginal, with a slight decrease in diversity noticed when the steps are large, potentially\nattributable to the refining of the noise schedule with higher step counts. This could cause the\nancestral solver to converge to a fixed set of solutions despite the noise introduced at each step.\nNotably, optimal results in terms of sample quality and diversity, paired with acceptable computational\nspeed, were achieved with around 16 diffusion steps. Even though the RTF experienced a 30%\nincrease compared to 4 steps, it still outperformed VITS by twice the speed, making it a suitable\nchoice for real-time applications.\nB.3 Consistent Long-Form Generation\nAlgorithm 1Long-form generation with style diffusion\nprocedure LONGFORM (t, α) ▷ t is the input paragraph, α ∈ [0, 1] is the weighting factor\nT ← SPLIT (t) ▷ Split t into N sentences {T0, T1, . . . ,TN−1}\nfor i ∈ {0, . . . , N− 1} do\nscurr ← STYLE DIFFUSION (Ti) ▷ Sample a style vector with the current text Ti\nif sprev then ▷ Check if the previous style vector is defined\nscurr ← αscurr + (1 − α)sprev ▷ Convex combination interpolation\nend if\nxi ← SYNTHESIZE (Ti, scurr) ▷ Synthesize with the interpolated scurr\nsprev ← scurr ▷ Set sprev for the next iteration\nend for\nreturn CONCAT ({x0, x1, . . . ,xN−1}) ▷ Return concatenated speech from all sentences\nend procedure\nOur findings indicate that style diffusion creates significant variation in samples, a characteristic that\nposes challenges for long-form synthesis. In this scenario, a long paragraph is usually divided into\nsmaller sentences for generation, sentence by sentence, in the same way as real-time applications.\nUsing an independent style for each sentence may generate speech that appears inconsistent due\nto differences in speaking styles. Conversely, maintaining the same style from the first sentence\nthroughout the entire paragraph results in monotonic, unnatural, and robotic-sounding speech.\nWe empirically observe that the latent space underlying the style vectors generally forms a convex\nspace. Consequently, a convex combination of two style vectors yields another style vector, with\nthe speaking style somewhere between the original two. This allows us to condition the style of the\ncurrent sentence on the previous sentence through a simple convex combination. The pseudocode of\nthis algorithm, which uses interpolated style vectors, is provided in Algorithm 1.\nAn example of a long paragraph generated using this method with α = 0.7 is available on our demo\npage: https://styletts2.github.io/#long.\n19\n(a) Duration predictor output\nToken\nDuration\n   0     1     2     3     4     5     6     7     8     9\nSpeech Frame \nToken\n  1     2     3     4     5     6     7     8     9\n(c) Unnormalized predicted alignment\nSpeech Frame \nToken\n  1     2     3     4     5     6     7     8     9\n(d) Normalized predicted alignment\nSpeech Frame \n(b) Gaussian filter centered at\nFigure 4: Illustration of our proposed differentiable duration upsampler. (a) Probability output from\nthe duration predictor for 5 input tokens with L = 5 . (b) Gaussian filter Nℓi−1 centered at ℓi−1. (c)\nUnnormalized predicted alignment ˜fai[n] from the convolution operation between (a) and (b). (d)\nNormalized predicted alignment apred over the phoneme axis.\nB.4 Style Transfer\nOur model, contrary to traditional end-to-end (E2E) TTS systems that directly map texts to speech-\nrelated representations, decouples speech content and style via a style vector. This setup enables\nthe transfer of a style sourced from a text with specific emotions or stylistic nuances to any given\ninput text. The procedure involves generating a style vector from a text reflecting desired aspects\nof the target style (e.g., emotions, speaking rates, recording environments, etc.) and synthesizing\nspeech from any input text using this style vector. This process also extends to unseen speakers, as\ndemonstrated on our demo page at https://styletts2.github.io/#libri. It is important to\nnote that the relationship between speech style and input text is learned in a self-supervised manner\nwithout manual labeling, analogous to the CLIP embedding used in Stable Diffusion.\nAppendix C Differentiable Duration Modeling\nC.1 DSP-Based Formulation\nIn section 3.2.4, we formulated the differentiable duration model using probability theory. This\nsection provides an alternative formulation based on principles of digital signal processing (DSP).\nThe upsampler is formulated using the following DSP properties:\nx[n] ∗ δk[n] = x[n + k], (24) δc(x) = lim\nσ→0\n1\nσ\n√\n2π Nc(x; σ). (25)\nHere, δk[n] is the Kronecker delta function, δc(x) is the Dirac delta function, and Nc(x; σ) is the\nGaussian kernel centered at c, as defined in eq. 7. These properties state that a delta function, δk,\nthrough convolution operation, shifts the value of a function to a new position byk, and a properly\nnormalized Gaussian kernel converges to the delta function as σ → 0. Therefore, by properly\nselecting σ, we can approximate the shifting property of the delta function with Nc(x; σ) and adjust\nthe duration predictor output q[:, i] to the starting position ℓi−1 of the ith phoneme. Figure 4 illustrates\nour proposed differentiable duration modeling, and a real example of the duration predictor output,\nthe predicted alignment using the non-differentiable upsampling method described in section 3.1, and\nthe alignment using our proposed method is given in Figure 5.\n(a) Predictor output.\n (b) Non-differentiable upsampling.\n (c) Proposed upsampling with σ = 1.5.\nFigure 5: An example of duration predictor output and the predicted alignment with and without\ndifferentiable duration upsampler. (a) displays the log probability from the duration predictor for\nimproved visualization. Although (b) and (c) differs, the duration predictor is trained end-to-end with\nthe SLM discriminator, making the difference perceptually indistinguishable in synthesized speech.\n20\nC.2 Effects of σ on Training Stability and Sample Quality\nThe gradient propagation from the SLM discriminator to the duration predictor is crucial for our\nmodel’s performance. The norm of this gradient, computed using the chain rule, is the product of the\nnorms of individual functions’ gradients. The magnitude of the partial derivatives of Nc(x; σ) is:\n\f\f\f\f\n∂Nc(x; σ)\n∂x\n\f\f\f\f =\n\f\f\f\f\n∂Nc(x; σ)\n∂c\n\f\f\f\f =\n\f\f\f\f±(x − c)\nσ2 Nc(x; σ)\n\f\f\f\f = |x − c|\nσ2 |Nc(x; σ)| ≤C\nσ2 , (26)\nwhere C is a constant dependent on L. The gradient norm is thus of order O(1/σk), where k depends\non the input dimension. As σ → 0, Nc(x; σ) approaches δc(x), but the gradient norm ∥∇Nc(x; σ)∥\ngoes to infinity. Moreover, smaller σ values may not yield better δ approximations due to numerical\ninstability, particularly when dealing with small values such as the predictor output q ∈ [0, 1], which\nis multiplied by Nc(x; σ) in eq. 11, exacerbating numerical instability.\nWe investigate the trade-off by examining the impact ofσ on training stability and sample quality.\nTraining stability is represented by the gradient norm from the SLM discriminator to the duration\npredictor, and sample quality is measured by the mel cepstral distortion (MCD) between speech\nsynthesized using the predicted alignment generated through the non-differentiable upsampling\nmethod (section 3.1) and our proposed differentiable upsampling method. We use the maximum\ngradient over an epoch to represent training stability, as large gradient batches often cause gradient\nexplosion and unstable training. We compute these statistics for σ values from 0.01 to 10.0 on a log\nscale, with results shown in Figure 6.\n(a) Sample quality (MCD)\n (b) Training stability (max gradient norm)\nFigure 6: Effects of σ on MCD and max gradient norm. Our choice of σ = 1.5 is marked with\na star symbol. (a) MCD between samples synthesized with differentiable and non-differentiable\nupsampling over different σ. (b) The maximum norm of gradients from the SLM discriminator to the\nduration predictor over an epoch of training with different σ.\nOur chosen σ = 1.5 minimizes both MCD and gradient norm across a wide range ofσ values, making\nit an optimal choice for upsampling. This value also aligns with the typical phoneme duration of 2 to\n3 speech frames, as Gaussian kernels with σ = 1.5 span approximately 3 speech frames.\nAppendix D SLM Discriminators\nD.1 Layer-Wise Analysis\nFollowing the approach of the WavLM study [11], we performed a layer-wise feature importance\nanalysis for the speech language model (SLM) discriminator. The weights of the input linear\nprojection layer into the convolutional discriminative head were normalized using the average norm\nvalues of WavLM features for samples in the test set across LJSpeech, VCTK, and LibriTTS datasets.\nFigure 7 depicts the normalized weight magnitudes across layers.\nIn the LJSpeech and LibriTTS models, the initial (1 and 2) and middle layers (6 and 7) showed\nthe highest importance, while the final layers (10 and 11) were of diminishing importance. The\nleast influential was the final layer, consistent with previous findings that ascribe the primary role\nof this layer to pretext tasks over downstream tasks [55, 10, 11]. Notably, layers 0, 1, and 2, which\nencode acoustic information like energy, pitch, and signal-to-noise ratio (SNR), emerged as the most\n21\n(a) LJSpeech\n(b) VCTK\n(c) LibriTTS\nFigure 7: Layer-wise input weight magnitude to the SLM discriminators across different datasets.\nThe layer importance shows a divergent pattern for the VCTK model relative to the LJSpeech and\nLibriTTS models, showcasing the impact of contextual absence on the SLM discriminators.\ncrucial. Layers 5, 6, and 7, encoding semantic aspects like word identity and meaning [ 55], held\nsecondary importance. This indicates that the SLM discriminator learns to fuse acoustic and semantic\ninformation to derive paralinguistic attributes, such as prosody, pauses, intonations, and emotions,\nwhich are critical in distinguishing real and synthesized speech. The outcome reaffirms the SLM\ndiscriminator’s capacity to enhance the emotional expressiveness and prosody of synthesized speech.\nIn contrast, the SLM discriminator for the VCTK dataset demonstrated no distinct layer preference,\nlikely due to its limited contextual information. Unlike the storytelling and audiobook narrations in\nthe LJSpeech and LibriTTS datasets, the VCTK dataset only involves reading a standard paragraph\ndevoid of specific context or emotions. This context shortage could explain the marginal performance\nincrease of our model over VITS on the VCTK dataset compared to LJSpeech and LibriTTS datasets\n(see Tables 1, 2, 3), as the advantages of style diffusion and SLM adversarial training in our model\nare less pronounced in datasets with restricted expressiveness and emotional variety.\nD.2 Training Stability\nFigure 6(b) shows that the maximum norm gradient from the SLM discriminator to the duration\npredictor can reach up to 200 even with σ = 1.5, potentially destabilizing training, given that the\ngradient norms for other modules are generally less than 10. This instability is particularly concerning\nwhen the gradient is back-propagated to the prosodic text encoder, which employs a BERT model\nthat is notably sensitive to gradient explosion. To mitigate this issue, we adopt a strategy for scaling\nthe gradient from the SLM discriminator to the duration predictor, a common practice to prevent\ngradient explosion in GAN training [31].\n22\nWe found that the gradient norm of the entire predictor typically falls between 10 and 20. As such,\nwe implement a scaling factor of 0.2 to the gradient norm when it surpasses 20. Additionally, we also\naccount for potential instabilities arising from the sigmoid function used by the duration predictor for\nprobability output. We mitigate these by scaling the gradient to the last projection layer and the LSTM\nlayer within the duration predictor by a factor of 0.01. These scaling values have consistently shown\nstability across different datasets and can be modified to accommodate dataset-specific attributes. All\nthe models presented in this paper were trained with these scaling factors.\nAppendix E Subjective Evaluation Procedures\nIn this section, we propose a detailed and standardized evaluation procedure and elucidate our\nadherence to these guidelines in the current work.\nE.1 Proposed Framework for Subjective Evaluation\nIn light of the findings in [43] along with guidelines from [5], we recommend the following procedures\nfor assessing human-like text-to-speech models:\n1. Raters should be native speakers of the language used in the model’s training corpus, with\nprovisions in place to exclude falsely claimed native speakers.\n2. Attention checks should be incorporated into the evaluation to ensure thoughtful, careful\ncompletion, and raters who do not pass the attention checks should be excluded from the\noutcome analysis.\n3. The objectives of the evaluation should be clearly stated. For instance, when assessing\nnaturalness, its definition should be explicit within the evaluation framework.\n4. Ideally, all models used in the experiments should be publicly available, and official imple-\nmentations are preferred over unofficial implementations. If no official implementations are\npublicly available, the testing samples can be requested from the authors of the papers, or\nfrom implementations that have been employed in other papers.\n5. For model comparisons, the MUSHRA-based approach, where paired samples from all\nmodels are presented, should be employed, in contrast to the traditional MOS evaluation\nscheme that presents model samples in isolation.\n6. For nuanced model comparison and determining statistical significance, CMOS should be\nused in place of the MOS and MUSHRA evaluations. Each sample should be rated by\nat least 20 raters with more than 50 samples to claim statistical significance compared to\nground truth to assess human-level quality [5].\nE.2 Evaluation Details\nThe first criterion was ensured by selecting native English-speaking participants residing in the United\nStates. Upon launching our survey on MTurk, we activated the following filters:\n• HIT Approval Rate (%) for all Requesters’ HITS: greater than 95.\n• Location: is UNITED STATES (US).\n• Number of HITs Approved: greater than 50.\nFor each batch, completed responses were collected exclusively from self-identified native speakers\nin the United States, verified by residential IP addresses (i.e., not proxy or VPN) with an online\nservice 5.\nTo fulfill the second criterion, attention checks were implemented differently for MOS and CMOS\nevaluations. In the MOS assessment, we utilized the average score given by a participant to ground\ntruth audios, unbeknownst to the participants, to ascertain their attentiveness. We excluded ratings\nfrom those whose average score for the ground truth did not rank in the top three among all five\nmodels. In the CMOS evaluation, we assessed the consistency of the rater’s scoring; if the score’s\n5Avaiable at https://www.ipqualityscore.com/free-ip-lookup-proxy-vpn-test\n23\nsign (indicating whether A was better or worse than B) differed for over half the sample set (in our\ncase, 10 samples), the rater was disqualified. Six raters were eliminated through this process in all of\nour experiments.\nThe third criterion was ensured by providing explicit definitions of “naturalness\" and “similarity\"\nwithin our surveys, exemplified by the following lines (see our survey links 678 for more detail):\n• Naturalness:\nSome of them may be synthesized while others may be spoken by an\nAmerican audiobook narrator.\nRate how natural each audio clip sounds on a scale of 1 to 5 with\n1 indicating completely unnatural speech (bad) and 5 completely\nnatural speech (excellent).\nHere, naturalness includes whether you feel the speech is spoken\nby a native American English speaker from a human source.\n• Similarity:\nRate whether the two audio clips could have been produced by the\nsame speaker or not on a scale of 1 to 5 with 1 indicating\ncompletely different speakers and 5 indicating exactly the same\nspeaker.\nSome samples may sound somewhat degraded/distorted; for this\nquestion, please try to listen beyond the distortion of the\nspeech and concentrate on identifying the voice (including the\nperson’s accent and speaking habits, if possible).\nIn accordance with the fourth criterion, official model checkpoints were utilized in both MOS and\nCMOS evaluations. Specifically, for the MOS evaluation of LJSpeech, checkpoints for VITS 9,\nJETS10, and StyleTTS with HifiGAN vocoder11 were sourced directly from the authors. Likewise,\nfor the MOS evaluation on LibriTTS, official checkpoints for StyleTTS with the HifiGAN vocoder\nand YourTTS (utilizing the Exp 4 checkpoint)12 were used. Since there is no official checkpoint of\nVITS for LibriTTS, we used the VITS model from ESPNet 13 on LibriTTS, which was previously\nemployed as a baseline model in [ 6]. For the CMOS experiment on VCTK with VITS, we used\nofficial checkpoints and implementation. For the CMOS evaluation with NaturalSpeech, we obtained\n20 demo samples directly from the author. Regarding Vall-E’s CMOS experiment, we collected 30\nsamples from LibriTTS, along with their corresponding 3-second prompts from the official demo\npage14.\nFulfilling the fifth criterion, we employed a MUSHRA-based approach in our MOS evaluations.\nLastly, we assured that 20 raters evaluated each sample in our CMOS experiments, after excluding\nineligible raters. For NaturalSpeech and Vall-E, where additional samples were not available, we\ncompensated by doubling the number of raters.\nAppendix F Detailed Model Architectures\nThis section offers a comprehensive outline of the enhanced StyleTTS 2 architecture. The model\nintegrates eight original StyleTTS modules with the addition of an extra style diffusion denoiser,\nprosodic style encoder, and prosodic text encoder.\n6LJSpeech MOS: https://survey.alchemer.com/s3/7376864/MOS-MIX-ID-OOD-B1\n7LJSpeech CMOS: https://survey.alchemer.com/s3/7332116/CMOS-gt-styletts2-0430-b4\n8LibriTTS MOS: https://survey.alchemer.com/s3/7387883/MOS-LibriTTS-0508-b2\n9https://github.com/jaywalnut310/vits\n10https://huggingface.co/imdanboy/jets\n11https://github.com/yl4579/StyleTTS\n12https://github.com/Edresson/YourTTS\n13Available at https://huggingface.co/espnet/kan-bayashi_libritts_xvector_vits\n14https://www.microsoft.com/en-us/research/project/vall-e-x/vall-e/\n24\nThe model keeps the same architecture for the acoustic text encoder, text aligner, and pitch extractor\nas in the original StyleTTS [ 6]. The architecture of both the acoustic style encoder and prosodic\nstyle encoder follows that of the style encoder from StyleTTS [ 6]. The prosodic text encoder is\na pre-trained PL-BERT15 model [7]. Additionally, we adopt the same discriminators, MPD and\nMRD, as specified in [31]. The decoder is a combination of the original decoder from StyleTTS and\neither iSTFTNet [45] or HifiGAN [28], with the AdaIN [44] module appended after each activation\nfunction. The duration and prosody predictors follow the same architecture as in [ 6], albeit with\nalterations to the output of the duration predictor changing from 1 × N to L × N for probability\noutput modeling. Thus, this section focuses primarily on providing comprehensive outlines for the\nstyle diffusion denoiser and the discriminative head of the SLM discriminators. Readers seeking\ndetailed specifications for other modules can refer to the aforementioned references.\nThe Denoiser (Table 8) is designed to process an input style vector s, the noise level σ, the phoneme\nembeddings from the prosodic text encoder hbert and, when applicable, a speaker embedding c from\nthe acoustic and prosodic encoders in the multi-speaker scenario. Here, c is also considered a part of\nthe input and is introduced to the denoiser through adaptive layer normalization (AdaLN):\nAdaLN(x, s) = Lσ(s)x − µ(x)\nσ(x) + Lµ(s) (27)\nwhere x is the feature maps from the output of the previous layer, s is the style vector, µ(·) and σ(·)\ndenotes the layer mean and standard deviation, and Lσ and Lµ are learned linear projections for\ncomputing the adaptive gain and bias using the style vector s.\nTable 8: Denoiser architecture. N represents the input phoneme length of the mel-spectrogram, σ is\nthe noise level, hbert is the phoneme embeddings, and c is the speaker embedding. The size of s and\nc is 256 × 1, that of hbert is 768 × N, and σ is 1 × 1. Group(32) is the group normalization with a\ngroup size of 32.\nSubmodule Input Layer Norm Output Shape\nEmbedding\nσ Sinusoidal Embedding - 256 ×1\nc Addition - 256 ×1\n– Repeat for N times - 256 ×N\n– Linear 256 ×1024 - 1024 ×N\n– Output Embedding k - 1024 ×N\nInput\ns Repeat forN Times - 256 ×N\nhbert Concat Group(32) 1024 ×N\n– Conv 1024 ×1024 – 1024 ×N\nTransformer Block(×3)\nk Addition - 1024 ×N\nc 8-Head Self-Attention (64 head features) AdaLN ( ·, c) 1024 ×N\n– Linear 1024 ×2048 – 2048 ×N\n– GELU – 2048 ×N\n– Linear 2048 ×1024 – 1024 ×N\nOutput\n– Adaptive Average Pool - 1024 ×1\n– Linear 1024 ×256 – 256 ×1\nThe Discriminative Head(Table 9) is comprised of a 3-layer convolutional neural network, conclud-\ning with a linear projection.\nTable 9: Discriminative head architecture. T represents the number of frames (length) of the output\nfeature hslm from WavLM.\nLayer Norm Output Shape\nInput hslm - 13 ×768×T\nReshape - 9984 ×T\nLinear 9984×256 - 256 ×T\nConv 256×256 - 256 ×T\nLeaky ReLU (0.2) - 256 ×T\nConv 256×512 - 512 ×T\nLeaky ReLU (0.2) - 512 ×T\nConv 512×512 - 512 ×T\nLeaky ReLU (0.2) - 512 ×T\nConv 512×1 - 1 ×T\n15Available at https://github.com/yl4579/PL-BERT\n25\nAppendix G Detailed Training Objectives\nIn this section, we provide detailed training objectives for both acoustic modules pre-training and\njoint training. We first pre-train the acoustic modules for accelerated training, and we then proceed\nwith joint training with the pitch extractor fixed.\nG.1 Acoustic module pre-training\nMel-spectrogram reconstruction.The decoder is trained on waveform y, its corresponding mel-\nspectrogram x, and the text t, using L1 reconstruction loss as\nLmel = Ex,t\n\u0002\n∥x − M (G (htext · aalgn, sa, px, nx))∥1\n\u0003\n. (28)\nHere, htext = T(t) is the encoded phoneme representation, and the attention alignment is denoted by\naalgn = A(x, t). The acoustic style vector of x is represented by sa = Ea(x), px symbolizes the\npitch F0 and nx indicates energy of x, and M(·) represents mel-spectrogram transformation. Follow-\ning [6], half of the time, raw attention output from A is used as alignment, allowing backpropagation\nthrough the text aligner. For another 50% of the time, a monotonic version of aalgn is utilized via\ndynamic programming algorithms (see Appendix A in [6]).\nTMA objectives. We follow [6] and use the original sequence-to-sequence ASR loss function Ls2s\nto fine-tune the pre-trained text aligner, preserving the attention alignment during end-to-end training:\nLs2s = Ex,t\n\" NX\ni=1\nCE(ti, ˆti)\n#\n, (29)\nwhere N is the number of phonemes in t, ti is the i-th phoneme token of t, ˆti is the i-th predicted\nphoneme token, and CE(·) denotes the cross-entropy loss function.\nAdditionally, we apply the monotonic loss Lmono to ensure that soft attention approximates its\nnon-differentiable monotonic version:\nLmono = Ex,t\n\u0002\n∥aalgn − ahard∥1\n\u0003\n, (30)\nwhere ahard is the monotonic version of aalgn obtained through dynamic programming algorithms\n(see Appendix A in [6] for more details).\nAdversarial objectives. Two adversarial loss functions, originally used in HifiGAN [ 30], are\nemployed to enhance the sound quality of the reconstructed waveforms: the LSGAN loss function\nLadv for adversarial training and the feature-matching loss Lfm.\nLadv(G; D) = Et,x\nh\n(D ((G (htext · aalgn, sa, px, nx))) − 1)2\ni\n, (31)\nLadv(D; G) = Et,x\nh\n(D ((G (htext · aalgn, sa, px, nx))))2\ni\n+\nEy\nh\n(D(y) − 1)2\ni\n,\n(32)\nLfm = Ey,t,x\n\" ΛX\nl=1\n1\nNl\n\r\rDl(y) − Dl (G (htext · aalgn, sa, px, nx))\n\r\r\n1\n#\n, (33)\nwhere D represents both MPD and MRD, Λ is the total number of layers in D, and Dl denotes the\noutput feature map of l-th layer with Nl number of features.\nIn addition, we included the truncated pointwise relativistic loss function [49] to further improve the\nsound quality:\nLrel(G; D) = E{ˆy,y|D(ˆy)≤D(y)+mGD}\nh\nτ − ReLU\n\u0010\nτ − (D (ˆy) − D(y) − mGD)2\n\u0011i\n, (34)\n26\nLrel(D; G) = E{ˆy,y|D(y)≤D(ˆy)+mDG}\nh\nτ − ReLU\n\u0010\nτ − (D(y) − D (ˆy) − mDG)2\n\u0011i\n, (35)\nwhere ˆy = G (htext · aalgn, sa, py, ny) is the generated sample, ReLU(·) is the rectified linear unit\nfunction, τ is the truncation factor that is set to be 0.04 per [ 49], mGD and mDG are margin\nparameters defined by the median of the score difference:\nmGD = My,ˆy [D(ˆy) − D(y)] , (36)\nmDG = My,ˆy [D(y) − D(ˆy)] , (37)\nwhere M[·] is the median operation.\nAcoustic module pre-training full objectives.Our full objective functions in the acoustic modules\npre-training can be summarized as follows with hyperparameters λs2s and λmono:\nmin\nG,A,Ea,F,T\nLmel + λs2sLs2s + λmonoLmono\n+ Ladv(G; D) + Lrel(G; D) + Lfm\n(38)\nmin\nD\nLadv(D; G)\n+ Lrel(D; G)\n(39)\nFollowing [6], we set λs2s = 0.2 and λmono = 5.\nG.2 Joint training\nDuration prediction. We use the following cross-entropy loss to train the duration predictor:\nLce = Edgt\n\" NX\ni=1\nLX\nk=1\nCE (q[k, i], I(dgt[i] ≥ k))\n#\n, (40)\nwhere I(·) is the indicator function, q = S(hbert, sp) is the output of the predictor under the prosodic\nstyle vector sp = Ep(x) and prosodic text embeddings hbert = B(t), N is the number of phonemes\nin t, and dgt is the ground truth duration obtained by summing aalgn along the mel frame axis.\nAdditionally, we employ the L-1 loss to make sure the approximated duration is also optimized:\nLdur = Edgt\n\u0002\n∥dgt − dpred∥1\n\u0003\n, (41)\nwhere dpred[i] =\nLP\nk=1\nq[k, i] is the approximated predicted duration of the ith phoneme.\nProsody prediction. We use Lf0 and Ln, which are F0 and energy reconstruction loss, respectively:\nLf0 = Ex [∥px − ˆpx∥1] (42)\nLn = Ex [∥nx − ˆnx∥1] (43)\nwhere ˆpx, ˆnx = P (hbert, sp) are the predicted pitch and energy of x.\nMel-spectrogram reconstruction.During joint training, we modify Lmel as follows:\nLmel = Ex,t\n\u0002\n∥x − M (G (htext · aalgn, sa, ˆpx, ˆnx))∥1\n\u0003\n. (44)\nNow we use the predicted pitch ˆpx and energy ˆnx to reconstruct the speech. We also make similar\nchanges to Ladv, Lfm, and Lrel during joint training.\nSLM adversarial objective.The SLM discriminator can easily overpower the generator and disable\nthe gradient flow to the generator. To facilitate learning when the discriminator overpowers the\ngenerator, we use the LSGAN loss [48] instead of the min-max loss in eq. 5:\nLslm(G; D) = Et,x\nh\n(DSLM ((G (htext · apred, ˆsa, pˆx, nˆx))) − 1)2\ni\n, (45)\n27\nLslm(D; G) = Et,x\nh\n(DSLM ((G (htext · apred, ˆsa, pˆx, nˆx))))2\ni\n+\nEy\nh\n(DSLM (y) − 1)2\ni\n,\n(46)\nwhere ˆsa is the acoustic style sampled from style diffusion, apred is the predicted alignment obtained\nthrough the differentiable duration upsampler from the predicted duration dpred, and pˆx and nˆx are\nthe predicted pitch and energy with the sampled prosodic style ˆsp.\nJoint training full objectives.Our full objective functions in joint training can be summarized as\nfollows with hyperparameters λdur, λce, λf0, λn, λs2s, and λmono:\nmin\nG,A,Ea,Ep,T,B,V,S,P\nLmel + λceLce + λdurLdur + λf0Lf0 + λnLn\n+ λs2sLs2s + λmonoLmono + Ladv(G; D)\n+ Lrel(G; D) + Lfm + Lslm(G; D) + Ledm,\n(47)\nwhere Ledm is defined in eq. 3.\nThe discriminator loss is given by:\nmin\nD,C\nLadv(D; G) + Lrel(D; G) + Lslm(D; G) (48)\nFollowing [6], we set λs2s = 0.2, λmono = 5, λdur = 1, λf0 = 0.1, λn = 1 and λce = 1.\n28"
}