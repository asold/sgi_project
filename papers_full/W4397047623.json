{
    "title": "Evaluation of Large Language Model Performance on the Biomedical Language Understanding and Reasoning Benchmark: Comparative Study",
    "url": "https://openalex.org/W4397047623",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2016745467",
            "name": "Hui Feng",
            "affiliations": [
                "IQVIA (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A155802741",
            "name": "Francesco Ronzano",
            "affiliations": [
                "IQVIA (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A5098690816",
            "name": "Jude LaFleur",
            "affiliations": [
                "IQVIA (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2185794407",
            "name": "Matthew Garber",
            "affiliations": [
                "IQVIA (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2137626592",
            "name": "Rodrigo de Oliveira",
            "affiliations": [
                "IQVIA (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2006044154",
            "name": "Kathryn Rough",
            "affiliations": [
                "IQVIA (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A4291006828",
            "name": "Katharine Roth",
            "affiliations": [
                "IQVIA (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2183318397",
            "name": "Jay Nanavati",
            "affiliations": [
                "IQVIA (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2978177334",
            "name": "Khaldoun Zine El Abidine",
            "affiliations": [
                "IQVIA (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2158442448",
            "name": "Christina Mack",
            "affiliations": [
                "IQVIA (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2016745467",
            "name": "Hui Feng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A155802741",
            "name": "Francesco Ronzano",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5098690816",
            "name": "Jude LaFleur",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2185794407",
            "name": "Matthew Garber",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2137626592",
            "name": "Rodrigo de Oliveira",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2006044154",
            "name": "Kathryn Rough",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4291006828",
            "name": "Katharine Roth",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2183318397",
            "name": "Jay Nanavati",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2978177334",
            "name": "Khaldoun Zine El Abidine",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2158442448",
            "name": "Christina Mack",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4323050332",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4387821331",
        "https://openalex.org/W4386120650",
        "https://openalex.org/W4385620388",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W4366735603",
        "https://openalex.org/W4394782456",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W1572319397",
        "https://openalex.org/W6857464309",
        "https://openalex.org/W4399911912",
        "https://openalex.org/W4386875765",
        "https://openalex.org/W4389116614",
        "https://openalex.org/W4385572482",
        "https://openalex.org/W3164540570",
        "https://openalex.org/W4308028428",
        "https://openalex.org/W4401043375",
        "https://openalex.org/W4221153690"
    ],
    "abstract": "Abstract Background The availability of increasingly powerful large language models (LLMs) has attracted substantial interest in their potential for interpreting and generating human-like text for biomedical and clinical applications. However, there are often demands for high accuracy, concerns about balancing generalizability and domain-specificity, and questions about prompting robustness when considering the adoption of LLMs for specific use cases. There also is a lack of a framework or method to help choose which LLMs (or prompting strategies) should be adopted for specific biomedical or clinical tasks. Objective To address the speculations on applying LLMs for biomedical applications, this study aims to 1) propose a framework to comprehensively evaluate and compare the performance of a range of LLMs and prompting techniques on a suite of biomedical natural language processing (NLP) tasks; 2) use the framework to benchmark several general-purpose LLMs and biomedical domain-specific LLMs. Methods We evaluated and compared six general-purpose LLMs (GPT-4, GPT-3.5-Turbo, Flan-T5-XXL, Llama-3-8B-Instruct, Yi-1.5-34B-Chat, and Zephyr-7B-Beta) and three healthcare-specific LLMs (Medicine-Llama3-8B, Meditron-7B, and MedLLaMA-13B) on a set of 13 datasets – referred to as the Biomedical Language Understanding and Reasoning Benchmark (BLURB) – covering six commonly needed medical natural language processing tasks: named entity recognition (NER); relation extraction (RE); population, interventions, comparators, and outcomes (PICO); sentence similarity (SS); document classification (Class.); and question-answering (QA). All models were evaluated without further training or fine-tuning. Model performance was assessed according to a range of prompting strategies (formalized as a systematic, reusable prompting framework) and relied on the standard, task-specific evaluation metrics defined by BLURB. Results Across all tasks, GPT-4 outperformed other LLMs, achieving a score of 64.6 on the benchmark, though other models, such as Flan-T5-XXL and Llama-3-8B-Instruct, demonstrated competitive performance on multiple tasks. We found that general-purpose models achieved better overall scores than domain-specific models, sometimes by significant margins. We observed a substantial impact of strategically editing the prompt describing the task and a consistent improvement in performance when including examples semantically similar to the input text. Additionally, the most performant prompts for nearly half the models outperformed the previously reported best results for the PubMedQA dataset from the BLURB leaderboard. Conclusions These results provide evidence of the potential LLMs may have for biomedical applications and highlight the importance of robust evaluation before adopting LLMs for any specific use cases. Notably, performant open-source LLMs such as Llama-3-8B-Instruct and Flan-T5-XXL show promise for use cases where trustworthiness and data confidentiality are concerns, as these models can be hosted locally, offering better security, transparency, and explainability. Continuing to explore how these emerging technologies can be adapted for the healthcare setting, paired with human expertise, and enhanced through quality control measures will be important research to allow responsible innovation with LLMs in the biomedical area.",
    "full_text": null
}