{
  "title": "TraDE: Transformers for Density Estimation",
  "url": "https://openalex.org/W3014722687",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227491599",
      "name": "Fakoor, Rasool",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749553419",
      "name": "Chaudhari, Pratik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282444878",
      "name": "Mueller, Jonas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227491603",
      "name": "Smola, Alexander J.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2067619114",
    "https://openalex.org/W2948613292",
    "https://openalex.org/W3150807214",
    "https://openalex.org/W2110176078",
    "https://openalex.org/W1866230956",
    "https://openalex.org/W2170111110",
    "https://openalex.org/W2791004381",
    "https://openalex.org/W1516111018",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2895434480",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2409550820",
    "https://openalex.org/W2964343746",
    "https://openalex.org/W2964000438",
    "https://openalex.org/W2070898270",
    "https://openalex.org/W2212660284",
    "https://openalex.org/W1826234144",
    "https://openalex.org/W2121927366",
    "https://openalex.org/W2104067967",
    "https://openalex.org/W2970730223",
    "https://openalex.org/W2970631142",
    "https://openalex.org/W2952165242",
    "https://openalex.org/W1503398984",
    "https://openalex.org/W3017895332",
    "https://openalex.org/W1647376582",
    "https://openalex.org/W2970898247",
    "https://openalex.org/W3120740533",
    "https://openalex.org/W1583912456",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2595056910",
    "https://openalex.org/W2328111639",
    "https://openalex.org/W2079296583",
    "https://openalex.org/W2787049730",
    "https://openalex.org/W2951870209",
    "https://openalex.org/W2587284713",
    "https://openalex.org/W299440670",
    "https://openalex.org/W1979759959",
    "https://openalex.org/W2129905273",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3150304496",
    "https://openalex.org/W2557270725",
    "https://openalex.org/W2953318193",
    "https://openalex.org/W2962990490",
    "https://openalex.org/W2099057450",
    "https://openalex.org/W2135181320",
    "https://openalex.org/W2163166770",
    "https://openalex.org/W2154579312",
    "https://openalex.org/W2995273672",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W1571975558",
    "https://openalex.org/W2962892300",
    "https://openalex.org/W2939297204",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2936497627",
    "https://openalex.org/W1999486090",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2998572092",
    "https://openalex.org/W2766527293",
    "https://openalex.org/W2137911812",
    "https://openalex.org/W1639381857",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2150546315",
    "https://openalex.org/W1511986666",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2022851810",
    "https://openalex.org/W1551944942",
    "https://openalex.org/W1968208045",
    "https://openalex.org/W2963047245",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W3002709689"
  ],
  "abstract": "We present TraDE, a self-attention-based architecture for auto-regressive density estimation with continuous and discrete valued data. Our model is trained using a penalized maximum likelihood objective, which ensures that samples from the density estimate resemble the training data distribution. The use of self-attention means that the model need not retain conditional sufficient statistics during the auto-regressive process beyond what is needed for each covariate. On standard tabular and image data benchmarks, TraDE produces significantly better density estimates than existing approaches such as normalizing flow estimators and recurrent auto-regressive models. However log-likelihood on held-out data only partially reflects how useful these estimates are in real-world applications. In order to systematically evaluate density estimators, we present a suite of tasks such as regression using generated samples, out-of-distribution detection, and robustness to noise in the training data and demonstrate that TraDE works well in these scenarios.",
  "full_text": "TRADE: T RANSFORMERS FOR DENSITY ESTIMATION\nRasool Fakoor1 ∗, Pratik Chaudhari2, Jonas Mueller1, Alexander J. Smola1\n1 Amazon Web Services\n2 University of Pennsylvania\nABSTRACT\nWe present TraDE, a self-attention-based architecture for auto-regressive density\nestimation with continuous and discrete valued data. Our model is trained using\na penalized maximum likelihood objective, which ensures that samples from the\ndensity estimate resemble the training data distribution. The use of self-attention\nmeans that the model need not retain conditional sufﬁcient statistics during the auto-\nregressive process beyond what is needed for each covariate. On standard tabular\nand image data benchmarks, TraDE produces signiﬁcantly better density estimates\nthan existing approaches such as normalizing ﬂow estimators and recurrent auto-\nregressive models. However log-likelihood on held-out data only partially reﬂects\nhow useful these estimates are in real-world applications. In order to systematically\nevaluate density estimators, we present a suite of tasks such as regression using\ngenerated samples, out-of-distribution detection, and robustness to noise in the\ntraining data and demonstrate that TraDE works well in these scenarios.\n1 I NTRODUCTION\nDensity estimation involves estimating a probability density p(x), given independent, identically\ndistributed (iid) samples from it. This is a versatile and important problem as it allows one to\ngenerate synthetic data or perform novelty and outlier detection. It is also an important subroutine in\napplications of graphical models. Deep neural networks are a powerful function class and learning\ncomplex distributions with them is promising. This has resulted in a resurgence of interest in the\nclassical problem of density estimation.\nFigure 1: TraDE is well suited to density estimation of\nTransformers. Left: Bumblebee (true density), Right:\ndensity estimated from data.\nOne of the more popular techniques for den-\nsity estimation is to sample data from a simple\nreference distribution and then to learn a (se-\nquence of) invertible transformations that allow\nus to adapt it to a target distribution. Flow-based\nmethods (Durkan et al., 2019b) employ this with\ngreat success. A more classical approach is to\ndecompose p(x) in an iterative manner via con-\nditional probabilities p(xi+1|x1...i) and ﬁt this\ndistribution using the data (Murphy, 2013). One may even employ implicit generative models to\nsample from p(x) directly, perhaps without the ability to compute density estimates. This is the\ncase with Generative Adversarial Networks (GANs) that reign supreme for image synthesis via\nsampling (Goodfellow et al., 2014; Karras et al., 2017).\nImplementing these above methods however requires special care, e.g., the normalizing transform\nrequires the network to be invertible with an efﬁciently computable Jacobian. Auto-regressive models\nusing recurrent networks are difﬁcult to scale to high-dimensional data due to the need to store a\npotentially high-dimensional conditional sufﬁcient statistic (and also due to vanishing gradients).\nGenerative models can be difﬁcult to train and GANs lack a closed density model. Much of the\ncurrent work is devoted to mitigating these issues. The main contributions of this paper include:\n1. We introduce TraDE, a simple but novel auto-regressive density estimator that uses self-attention\nalong with a recurrent neural network (RNN)-based input embedding to approximate arbitrary\ncontinuous and discrete conditional densities. It is more ﬂexible than contemporary architectures\n∗Correspondence to: Rasool Fakoor [fakoor@amazon.com]\n1\narXiv:2004.02441v2  [cs.LG]  14 Oct 2020\nsuch as those proposed by Durkan et al. (2019b;a); Kingma et al. (2016); De Cao et al. (2019) for\nthis problem, yet it remains capable of approximatingany density function. To our knowledge, this\nis the ﬁrst adaptation of Transformer-like architectures for continuous-valued density estimation.\n2. Log-likelihood on held-out data is the prevalent metric to evaluate density estimators. However,\nthis only provides a partial view of their performance in real-world applications. We propose a suite\nof experiments to systematically evaluate the performance of density estimators in downstream\ntasks such as classiﬁcation and regression using generated samples, detection of out-of-distribution\nsamples, and robustness to noise in the training data.\n3. We provide extensive empirical evidence that TraDE substantially outperforms other density\nestimators on standard and additional benchmarks, along with thorough ablation experiments to\ndissect the empirical gains. The main feature of this work is the simplicity of our proposed method\nalong with its strong (systematically evaluated) empirical performance.\n2 B ACKGROUND AND RELATED WORK\nGiven a dataset\n{\nx1,...,x n}\nwhere each sample xl ∈Rd is drawn iid from a distribution p(x), the\nmaximum-likelihood formulation of density estimation ﬁnds a θ-parameterized distribution qwith\nˆθ= argmax\nθ\n1\nn\nn∑\nl=1\nlog q(xl; θ). (1)\nThe candidate distribution qcan be parameterized in a variety of ways as we discuss next.\nNormalizing ﬂows write x ∼q as a transformation of samples z from some base distribution pz\nfrom which one can draw samples easily (Papamakarios et al., 2019). If this mapping is fθ : z→x,\ntwo distributions can be related using the determinant of the Jacobian as\nq(x; θ) :=pz(z)\n⏐⏐⏐dfθ\ndz\n⏐⏐⏐\n−1\n.\nA practical limitation of ﬂow-based models is that fθ must be a diffeomorphism, i.e., it is invertible\nand both fθ and f−1\nθ are differentiable. Good performance using normalizing ﬂows imposes nontrivial\nrestrictions on how one can parametrize fθ: it must be ﬂexible yet invertible with a Jacobian that\ncan be computed efﬁciently. There are a number of techniques to achieve this, e.g., linear mappings,\nplanar/radial ﬂows (Rezende & Mohamed, 2015; Tabak & Turner, 2013), Sylvester ﬂows (Berg et al.,\n2018), coupling (Dinh et al., 2014) and auto-regressive models (Larochelle & Murray, 2011). One\nmay also compose the transformations, e.g., using monotonic mappings fθ in each layer (Huang\net al., 2018; De Cao et al., 2019).\nAuto-regressive models factorize the joint distribution as a product of univariate conditional distri-\nbutions q(x; θ) :=∏\niqi(xi|x1,...x i−1; θ).The auto-regressive approach to density estimation is\nstraightforward and ﬂexible as there is no restriction on how each conditional distribution is modeled.\nOften, a single recurrent neural network (RNN) is used to sequentially estimate all conditionals with\na shared set of parameters (Oliva et al., 2018; Kingma et al., 2016). For high-dimensional data, the\nchallenge lies in handling the increasingly large state space x1,...,x i−1 required to properly infer\nxi. In recurrent auto-regressive models, these conditioned-upon variables’ values are stored in some\nrepresentation hi which is updated via a function hi+1 = g(hi,xi). This overcomes the problem of\nhigh-dimensional estimation, albeit at the expense of loss in ﬁdelity. Techniques like masking the\ncomputational paths in a feed-forward network are popular to alleviate these problems further (Uria\net al., 2016; Germain et al., 2015; Papamakarios et al., 2017).\nMany existing auto-regressive algorithms are highly sensitive to the variable ordering chosen for\nfactorizing q, and some methods must train complex ensembles over multiple orderings to achieve\ngood performance (Germain et al., 2015; Uria et al., 2014). While autoregressive models are\ncommonly applied to natural language and time series data, this setting only involves variables\nthat are already naturally ordered (Chelba et al., 2013). In contrast, we consider continuous (and\ndiscrete) density estimation of vector valued data, e.g. tabular data, where the underlying ordering\nand dependencies between variables is often unknown.\nGenerative models focus on drawing samples from the estimated distribution that look resemble\nthe true distribution of data. There is a rich history of learning explicit models from variational\n2\ninference (Jordan et al., 1999) that allow both drawing samples and estimating the log-likelihood or\nimplicit models such as Generative Adversarial Networks (GANs, see (Goodfellow et al., 2014))\nwhere one may only draw samples. These have been shown to work well for natural images (Kingma\n& Welling, 2013) but have not obtained similar performance for tabular data. We also note the\nexistence of many classical techniques that are less popular in deep learning, such as kernel density\nestimation (Silverman, 2018) and Chow-Liu trees (Chow & Liu, 1968; Choi et al., 2011).\n3 T OOLS OF THE TRADE\nx1 x2 x3 x4 x5 x6 x7 x8\nx1 x8 x2 x7 x3 x6 x4 x5\nConsider the 8-dimensional Markov Random Field\nshown here, where the underlying graphical model\nis unknown in practice. Consider the following two\norders in which to factorize the autoregressive model:\n(1,2,3,4,5,6,7,8) and (1,8,2,7,3,6,4,5). In the\nlatter case the model becomes a simple sequence where e.g. p(x3|x1,8,2,7) = p(x3|x7) due to\nconditional independence. A latent variable auto-regressive model only needs to preserve the most\nrecently encountered state in this latter ordering. In the ﬁrst ordering, p(x3|x1,2) can be simpliﬁed\nfurther to p(x3|x2), but we still need to carry the precise value of x1 along until the end since\np(x8|x1...7) = p(x8|x1,2). This is a fundamental weakness in models employing RNNs such\nas (Oliva et al., 2018). In practice, we may be unable to select a favorable ordering for columns\nin a table (unlike for language where words are inherently ordered), especially as the underlying\ndistribution is unknown.\n3.1 V ERTEX ORDERING AND SUFFICIENT STATISTICS\nThe above problem is also seen in sequence modeling, and Transformers were introduced to better\nmodel such long-range dependencies through self-attention (Vaswani et al., 2017). A recurrent\nnetwork can, in principle, absorb this information into its hidden state. In fact, Long-Short-Term\nMemory (LSTM) (Hochreiter & Schmidhuber, 1997) units were engineered speciﬁcally to store\nlong-range dependencies until needed. Nonetheless, storing information costs parameter space. For\nan auto-regressive factorization where the true conditionals require one to store many variables’\nvalues for many time steps, the RNN/LSTM hidden state must be undesirably large to achieve low\napproximation error, which means these models must have many parameters (Collins et al., 2017).\nThe following simple lemma formalizes this.\nLemma 1. Denote by G the graph of an undirected graphical model over random variables\nx1,...,x d. Depending on the order vertices are traversed in our factorization the largest num-\nber of latent variables a recurrent auto-regressive model needs to store is bounded from above and\nbelow by the minimum and the maximum number of variables with a cut edge of the graph G.\nProof. Given a subset of known variables S ⊆ {1,...d }we want to estimate the conditional\ndistribution of the variables on the complement C := {1,...d }\\S. For this we need to decompose S\ninto the Markov blanket M of Cand its remainder. By deﬁnition M consists of the variables with a\ncut edge. Since p(xC|xS) =p(xC|xM) we are done.\nThis problem with long-dependencies in auto-regressive models has been noted before, and is\nexacerbated in density estimation with continuous-valued data, where values of conditioned-upon\nvariables may need to be precisely retrieved. For instance, recent auto-regressive models employ\nmasking to eliminate the sequential operations of recurrent models (Papamakarios et al., 2017). There\nare also models like Pixel RNN (Oord et al., 2016) which explicitly design a multi-scale masking\nmechanism suited for natural images (and also require discretization of continuous-valued data). Note\nthat while there is a natural ordering of random variables in text/image data, variables in tabular data\ndo not follow any canonical ordering.\nApplicable to both continuous and discrete valued data, our proposed TraDE model circumvents this\nissue by utilizing self-attention to retrieve feature values relevant for the conditioning, which doesnot\nrequire more parameters to retrieve the values of relevant features that happened to appear early in\nthe auto-regressive factorization order (Yun et al., 2019). Thus a major beneﬁt of self-attention here\nis its effectiveness at maintaining an accurate representation of the feature values xj for j <iwhen\ninferring xi (irrespective of the distance between iand j).\n3\n3.2 T HE ARCHITECTURE OF TRADE\nTraDE is an auto-regressive density estimator and factorizes the distribution qas\nq(x; θ) :=\nd∏\ni=1\nqi(xi|x1,...,x i−1; θ); (2)\nHere the ith univariate conditional qi conditions the feature xi upon the features preceding it, and\nmay be easier to model than a high-dimensional joint distribution. Parameters θare shared amongst\nconditionals. Our main observation is that auto-regressive conditionals can be accurately modeled\nusing the attention mechanism in a Transformer architecture (Vaswani et al., 2017).\nSelf-attention. The Transformer is a neural sequence transduction model and consists of a multi-layer\nencoder/decoder pair. We only need the encoder for building TraDE. The encoder takes the input\nsequence (x1,...,x d) and predicts the ith-conditional qi(xi|x1,...,x i−1; θ). For discrete data, each\nconditional is parameterized as a categorical distribution. For continuous data, each conditional\ndistribution is parametrized as a mixture of Gaussians, where the mean/variance/proportion of each\nmixture component depend on x1,...,x i−1:\nqi(xi|x1,...,x i−1; θ) =\nm∑\nk=1\nπk,i N(xi; µk,i,σ2\nk,i). (3)\nwith the mixture proportions ∑m\nk=1 πk,i = 1. All three of πk,i,µk,i and σk,i are predicted by the\nmodel as separate outputs at each feature i. They are parametrized by θand depend on x1,...,x i−1.\nThe crucial property of the Transformer’s encoder is the self-attention module outputs a representation\nthat captures correlations in different parts of its input. In a nutshell, the self-attention map outputs\nzi = ∑d\nj=1 αjϕ(xj) where ϕ(xj) is an embedding of the feature xj and normalized weights\nαj = ⟨ϕ(xi, ϕ(xj)⟩compute the similarity between the embedding of xi and that of xj. Self-\nattention therefore amounts to a linear combination of the embedding of each feature with features\nthat are more similar to xi getting a larger weight. We also use multi-headed attention like (Vaswani\net al., 2017) which computes self-attention independently for different embeddings. Self-attention is\nthe crucial property that allows TraDE to handle long-range and complex correlations in the input\nfeatures; effectively this eliminates the vanishing gradient problem in RNNs by allowing direct\nconnections between far away input neurons (Vaswani et al., 2017). Self-attention also enables\npermutation equivariance and naturally enables TraDE to be agnostic to the ordering of the features.\nMasking is used to prevent xi,xi+1,...,x d from taking part in the computation of the output qi and\nthereby preserve the auto-regressive property of our density estimator. We keep residual connections,\nlayer normalization and dropout in the encoder unchanged from the original architecture of (Vaswani\net al., 2017). The ﬁnal output layer of our TraDE model, is a position-wise fully-connected layer\nwhere, for continuous data, the ith position outputs the mixing proportions πk,i, means µk,i and\nstandard deviations σk,i that together specify a Gaussian mixture model. For discrete data, this ﬁnal\nlayer instead outputs the parameters of a categorical distribution.\nPositional encoding involves encoding the position k of feature xk as an additional input, and\nis required to ensure that Transformers can identify which values correspond to which inputs –\ninformation that is otherwise lost in self-attention. To circumvent this issue, Vaswani et al. (2017)\nappend Fourier position features to each input. Picking hyper-parameters for the frequencies is\nhowever difﬁcult and it does not work well either for density estimation (see Sec. 4). An alternative\nthat we propose is to use a simple recurrent network at the input to embed the input values at each\nposition. Here the time-steps of the RNN implicitly encode the positional information, and we use a\nGated Recurrent Unit (GRU) model to better handle long-range dependencies (Cho et al., 2014). This\nparallels recent ﬁndings from language modeling where (Wang et al., 2019) also used an initial RNN\nembedding to generate inputs to the transformer. Observe that the GRU does not slow down TraDE at\ninference time since sampling is performed in auto-regressive fashion and remains O(d). Complexity\nof training is marginally higher but this is more than made up for by the superior performance.\nRemark 2 (Compared to other methods, TraDE is simple and effective). Our architecture for\nTraDE can be summarized simply as using the encoder of a Transformer (Vaswani et al., 2017)\nwith appropriate masking to achieve auto-regressive dependencies with an output layer consisting\nof a mixture of multi-variate Gaussians and an input embedding layer built using an RNN. And\n4\nyet it is more ﬂexible than architectures for normalizing ﬂows without restrictive constraints on the\ninput-output map (e.g. invertible functions, Jacobian computational costs, etc.) As compared to other\nauto-regressive models, TraDE can handle long-range dependencies and does not need to permute\ninput features during training/inference like Germain et al. (2015); Uria et al. (2014). Finally, the\nobjective/architecture of TraDE is general enough to handle both continuous and discrete data, unlike\nmany existing density estimators. As experiments in Sec. 4 shows these properties make TraDE very\nwell-suited for auto-regressive density estimation.\nUnlike discrete language data, tabular datasets contain many numerical values and their categorical\nfeatures do not share a common vocabulary. Thus existing applications of Transformers to such data\nremain limited. More generally, successful applications of Transformer models to continuous-valued\ndata as considered here have remained rare to date (without coarse discretization as done for images\nin Parmar et al. (2018)), limited to a few applications in speech (Ren et al., 2019) and time-series (Li\net al., 2019; Benidis et al., 2020; Wu et al., 2020).\nBeyond Transformers’ insensitivity to feature order when conditioning on variables, they are a\ngood ﬁt for tabular data because their lower layers model lower-order feature interactions (starting\nwith pairwise interactions in the ﬁrst layer and building up in each additional layer). This relates\nthem to ANOV A models (Wahba et al., 1995) which only progressively blend features unlike in the\nfully-connected layers of feedforward neural networks.\nLemma 3. TraDE can approximate any continuous density p(x) on a compact domain x∈X⊂ Rd.\nProof. Consider any auto-regressive factorization of p(x). Here the ith univariate dis-\ntribution p(xi |x1,...,x i−1) can be approximated arbitrarily well by a Gaussian mixture∑m\nk=1 πk,i N(xi; µk,i,σ2\nk,i), assuming a sufﬁcient number of mixture components m (Sorenson\n& Alspach, 1971). By the universal approximation capabilities of RNNs (Siegelmann & Sontag,\n1995), the GRU input layer of TraDE can generate nearly any desired positional encodings that\nare input to the initial self-attention layer. Finally, Yun et al. (2019) establish that a Transformer\narchitecture with positional-encodings, self-attention, and position-wise fully connected layers can ap-\nproximate any continuous sequence-to-sequence function over a compact domain. Their constructive\nproof of this result (Theorem 3 in Yun et al. (2019)) guarantees that, even with our use of masking to\nauto-regressively restrict information ﬂow, the remaining pieces of the TraDE model can approximate\nthe mapping {(x1,...,x i−1)}d\ni=1 →{(πk,i,µk,i,σ2\nk,i)}d\ni=1.\n3.3 T HE LOSS FUNCTION OF TRADE\nThe MLE objective in (1) does not have a regularization term to deal with limited data. Furthermore,\nMLE-training only drives our network to consider how it is modeling each conditional distribution\nin (2), rather than how it models the full joint distribution p(x) and the quality of samples drawn\nfrom its joint distribution estimate q(x; ˆθ). To encourage both, we combine the MLE objective with a\nregularization penalty based on the Maximum Mean Discrepancy (MMD) to get the loss function of\nTraDE. The former ensures consistency of the estimate while the MMD term is effective in detecting\nobvious discrepancies when the samples drawn from the model do not resemble samples in the\ntraining dataset (details in Appendix A). The objective minimized in TraDE is a penalized likelihood:\nL(θ) =−1\nnd\nn∑\nl=1\nd∑\ni=1\nlog qi(xl\ni|xl\n1,...,x l\ni−1; θ) +λMMD2(ˆp(x),q(x; θ)) (4)\nwhere hyper-parameter λ≥0 controls the degree of regularization and ˆpdenotes the empirical data\ndistribution. This objective is minimized using stochastic gradient methods, where the gradient of the\nlog-likelihood term can be computed using standard back-propagation. Computing the gradient of\nthe MMD term involves differentiating the samples from qθ with respect to the parameters θ. For\ncontinuous-valued data, this is easily done using the reparametrization trick (Kingma & Welling,\n2013) since we model each conditional as a mixture of Gaussians. For categorical features, we\ncalculate the gradient using the Gumbel softmax trick (Maddison et al., 2016). The TraDE is thus\ngeneral enough to handle both continuous and discrete data, unlike many existing density estimators.\nWe also show experiments in Appendix B where MMD regularization helps density estimation\nwith limited data (beyond using other standard regularizers like weight decay and dropout); this\nis common in biological applications, where experiments are often too costly to be extensively\nreplicated (Krishnaswamy et al., 2014; Chen et al., 2020). Finally we note that there exist a large\n5\nnumber of classical techniques, such as maximum entropy and approximate moment matching\ntechniques (Phillips et al., 2004; Altun & Smola, 2006) for regularization in density estimation; MMD\nis conceptually close to moment matching.\n4 E XPERIMENTS\nFigure 2: Qualitative evaluation on 2-\ndimensional datasets. We train TraDE on\nsamples from six 2-dimensional densities and\nevaluate the model likelihood over the entire\ndomain by sampling a ﬁne grid; original densities\nare shown in rows 1 & 3 and estimated densities\nare shown in rows 2 & 4. This setup is similar\nto (Nash & Durkan, 2019). These distributions\nare highly multi-modal with complex correlations\nbut TraDE learns an accurate estimate of the true\ndensity across a large number of examples.\nWe ﬁrst evaluate TraDE both qualitatively (Fig. 2\nand Fig. S1 in the Appendix) and quantitatively on\nstandard benchmark datasets (Sec. 4.1). We then\npresent three additional ways to evaluate the per-\nformance of density estimators in downstream tasks\n(Sec. 4.3) along with some ablation studies (Sec. 4.2\nand Appendix A.2). Details for all the experiments\nin this section, including hyper-parameters, are pro-\nvided in the Appendix D.\nRemark 4 (Reporting log-likelihood for density\nestimators). The objective in (1) is a maximum like-\nlihood objective and therefore the maximizer need\nnot be close to p(x). This is often ignored in the\ncurrent literature and algorithms are compared based\non their log-likelihood on held-out test data. How-\never, the log-likelihood may be insufﬁcient to ascer-\ntain the real-world performance of these models, e.g.,\nin terms of verisimilitude of the data they generate.\nThis is a major motivation for us to develop a com-\nplementary evaluation methodologies in Sec. 4.3. We\nemphasize these evaluation methods are another\nnovel contribution of our paper. However, we also\nreport log-likelihood results for comparison with oth-\ners.\n4.1 R ESULTS ON BENCHMARK DATASETS\nWe follow the experimental setup of (Papamakar-\nios et al., 2017) to ensure the same train-\ning/validation/test dataset splits in our evaluation. In\nparticular, preprocessing of all datasets is kept the same as that of (Papamakarios et al., 2017). The\ndatasets named POWER, GAS (Vergara et al., 2012), HEPMASS, MINIBOONE and BSDS300 (Mar-\ntin et al., 2001) were taken from the UCI machine learning repository (Dua & Graff, 2017).\nTable 1: Negative average test log-\nlikelihood in nats (smaller is better)\non binarized MNIST.\nLOG-LIKELIHOOD\nVAE 82.14 ±0.07\nPLANARFLOWS 81.91±0.22\nIAF 80.79 ±0.12\nSYLVESTER 80.22±0.03\nBLOCKNAF 80.71 ±0.09\nPIXELRNN 79.20\nTRADE (OURS) 78.92±0.00\nThe MNIST dataset (LeCun et al., 1990) is used to evaluate\nTraDE on high-dimensional image-based data. We follow\nthe variational inference literature, e.g., (Oord et al., 2016),\nand use the binarized version of MNIST. The datasets for\nanomaly detection tasks, namely Pendigits, ForestCover and\nSatimage-2 are from the Outlier Detection DataSets (OODS)\nlibrary (Rayana, 2016). We normalized the OODS data by\nsubtracting the per-feature mean and dividing by the standard\ndeviation. We show the results on benchmark datasets in Ta-\nble 2. There is a wide diversity in the algorithms for density\nestimation but we make an effort to provide a complete compar-\nison of known results irrespective of the speciﬁc methodology.\nSome methods like Neural Spline Flows (NSF) by (Durkan et al., 2019b) are quite complex to imple-\nment; others like Masked Autoregressive Flows (MAF) (Papamakarios et al., 2017) use ensembles to\nestimate the density; some others like Autoregressive Energy Machines (AEM) of (Nash & Durkan,\n2019) average the log-likelihood over a large number of importance samples. As the table shows,\nTraDE obtains performance improvements over all these methods in terms of the log-likelihood.\n6\nTable 2: Average test log-likelihood in nats (higher is better) for benchmark datasets. Entries marked with\n∗ evaluate standard deviation across 3 independent runs of the algorithm; all others are mean ± standard error\nacross samples in the test dataset. TraDE achieves signiﬁcantly better log-likelihood than other algorithms on all\ndatasets except MINIBOONE.\nPOWER GAS HEPMASS MINIBOONE BSDS300\nREALNVP (DINH ET AL., 2016) 0.17 ±0.01 8.33 ±0.14 -18.71±0.02 -13.84±0.52 153.28±1.78\nMADE MOG (GERMAIN ET AL., 2015) 0.4 ±0.01 8.47 ±0.02 -15.15±0.02 -12.27±0.47 153.71±0.28\nMAF MOG (PAPAMAKARIOS ET AL.,2017) 0.3±0.01 9.59 ±0.02 -17.39±0.02 -11.68±0.44 156.36±0.28\nFFJORD (GRATHWOHL ET AL., 2018) 0.46 ±0.00 8.59 ±0.00 -14.92±0.00 -10.43±0.00 157.4 ±0.00\nNAF∗(HUANG ET AL., 2018) 0.62 ±0.01 11.96±0.33 -15.09 ±0.4 -8.86±0.15 157.43±0.3\nTAN (OLIVA ET AL., 2018) 0.6 ±0.01 12.06±0.02 -13.78±0.02 -11.01±0.48 159.8 ±0.07\nBNAF∗(DECAO ET AL., 2019) 0.61 ±0.01 12.06±0.09 -14.71±0.38 -8.95 ±0.07 157.36±0.03\nNSF∗(DURKAN ET AL., 2019B) 0.66 ±0.01 13.09±0.02 -14.01±0.03 -9.22 ±0.48 157.31±0.28\nAEM (NASH& DURKAN, 2019) 0.70 ±0.01 13.03±0.01 -12.85±0.01 -10.17±0.26 158.71±0.14\nTRADE∗(OURS) 0.73±0.00 13.27±0.01 -12.01±0.03 -9.49±0.13 160.01±0.02\nThis performance is persistent across all datasets except MINIBOONE where TraDE is competitive\nalthough not the best. The improvement is drastic for the POWER, HEPMASS and BSDS300.\nWe also evaluate TraDE on the MNIST dataset in terms of the log-likelihood on test data. As Table 1\nshows TraDE obtains high log-likelihood even compared to sophisticated models such as Pixel-\nRNN (Oord et al., 2016), V AE (Kingma & Welling, 2013), Planar Flows (Rezende & Mohamed,\n2015), IAF (Kingma et al., 2016), Sylvester (Berg et al., 2018), and Block NAF (De Cao et al., 2019).\nThis is a difﬁcult dataset for density estimation because of its high dimensionality. We also show the\nquality of the samples generated by our model in Fig. S1.\n4.2 A BLATION STUDY Table 3: Average test log-likelihood in nats (higher is better) on\nbenchmark datasets for ablation experiments.\nPOWER GAS HEPMASS MINIBOONE BSDS300\nRNN 0.51 6.26 -15.87 -13.13 157.29\nTransformer (without positionencoding) 0.71 12.95 -15.80 -22.29 134.71\nTransformer (with position en-coding) 0.73 12.87 -13.89 -12.28 147.94\nTraDE without MMD 0.72 13.26 -12.22 -9.44 159.97\nWe begin with an RNN trained as an\nauto-regressive density estimator; the\nperformance of this basic model in Ta-\nble 3 is quite poor for all datasets.\nNext, we use a Transformer net-\nwork trained in auto-regressive fash-\nion without position encoding; this\nleads to signiﬁcant improvements compared to the RNN but not on all datasets. Compare this\nto the third row which shows the results for Transformer with position encoding (instead of the\nGRU-based embedding in TraDE); this performs much better than a Transformer architecture without\nposition encoding. This suggests that incorporating the information about the position is critical for\nauto-regressive models (also see Sec. 3). A large performance gain on all datasets is observed upon\nusing a GRU-based embedding that replaces the position embedding of the Transformer.\nNote that although the MMD regularization helps produce a sizeable improvement for HEPMASS,\nthis effect is not consistent across all datasets. This is akin to any other regularizer; regularizers\ndo not generally improve performance on all datasets and models. MMD-penalization of the MLE\nobjective additionally helps obtain higher-ﬁdelity samples that are similar to those in the training\ndataset; this helps when few data are available for density estimation which we study in Appendix B.\nThis is also evident in the regression experiment (Table 4) where the MSE of TraDE is signiﬁcantly\nbetter than MLE-based RNNs and Transformers trained without the MMD term.\n4.3 S YSTEMATIC EVALUATION OF DENSITY ESTIMATORS\nWe propose evaluating density estimation in four canonical ways, regression using the generated\nsamples, a two-sample test to check the quality of generated samples, out-of-distribution detection,\nand robustness of the density estimator to noise in the training data. This section shows that TraDE\nperforms well on these tasks which demonstrates that it not only obtains high log-likelihood on\nheld-out data but can also be readily used for downstream tasks.\n7\n1. Regression using generated samples. We ﬁrst characterize the quality of samples generated by\nthe model, based on a regression task wherexd is regressed using data from the others(x1,...,x d−1).\nThe procedure is as follows: ﬁrst we use the training set of the HEPMASS ( d= 21) to ﬁt the density\nestimator, and then create a synthetic dataset with both inputsz= (x1,...,x d−1) and targets y= xd\nsampled from the model. Two random forest regressors are ﬁtted, one on the real data and another\non this synthetic data. These regressors are tested on real test data from HEPMASS. If the model\nsynthesizes good samples, we expect that the test performance of the regressor ﬁtted on synthetic data\nwould be comparable to that of the regressor ﬁtted on real data. Table 4 shows the results. Observe that\nthe classiﬁer trained on data synthesized by TraDE performs very similarly to the one trained on the\noriginal data. The MSE of a RNN-based auto-regressive density estimator, which is higher, is provided\nfor comparison. The MSE of a standard Transformer with position embedding is much worse at 1.37.\nTable 4: Mean squared error of\nregression on HEPMASS.\nReal data 0.773\nTraDE 0.780\nRNN 0.803\nTransformer 1.37\n2. Two-sample test on the generated data. A standard way\nto ascertain the quality of the generated samples is to perform\na two-sample test (Wasserman, 2006). The idea is similar to a\ntwo-sample test (Lopez-Paz & Oquab, 2016) in the discrimina-\ntor of a GAN: if the samples generated by the auto-regressive\nmodel are good, the discriminator should have an accuracy of\n50%. We train a density estimator on the training data; then ﬁt\na random forest classiﬁer to differentiate between real validation\ndata and synthesized validation data; then compute the accuracy of the classiﬁer on the real\ntest data and the synthesized test data. The accuracy using TraDE is 51 ±1% while the ac-\ncuracy using RNN is 55 ±4% and that of a standard Transformer with position embedding is\nmuch worse at 88 ±15%. Standard deviation is computed across different subsets of features\nx1,(x1,x2),..., (x1,...,x d) as inputs to the classiﬁer. This suggests that samples generated\nby TraDE are much closer those in the real dataset than those generated by the RNN model.\nTable 5: Average precision for out-\nof-distribution detection The results for\nNADE, NICE and TAN were (carefully) eye-\nballed from the plots of (Oliva et al., 2018).\nNADE NICE TAN TraDE\nPendigits 0.91 0.92 0.97 0.98\nForestCover 0.87 0.80 0.94 0.95\nSatimage-2 0.98 0.975 0.98 1.0\n3. Out-of-distribution detection. This is a classical ap-\nplication of density estimation techniques where we seek\nto discover anomalous samples in a given dataset. We\nfollow the setup of (Oliva et al., 2018): we call a datum\nout-of-distribution if the likelihood of a datum x under\nthe model q(x; θ) ≤tfor a chosen threshold t ≥0. We\ncan compute the average precision of detecting out-of-\ndistribution samples by sweeping across different values\nof t. The results are shown in Table 5. Observe that TraDE\nobtains extremely good performance, of more than 0.95\naverage precision, on the three datasets.\nTable 6: Average test log-\nlikelihood (nats) for HEP-\nMASS dataset with and\nwithout additive noise in the\ntraining data.\nClean Data Noisy Data\nNSF -14.51 -14.98\nTraDE -11.98 -12.43\n4. TraDE builds robustness to noisy data. Real data may contain\nnoise and in order to be useful on downstream tasks. Density estima-\ntion must be insensitive to such noise but the maximum-likelihood\nobjective is sensitive to noise in the training data. Methods such\nas NSF (Durkan et al., 2019b) or MAF (Papamakarios et al., 2017)\nindirectly mitigate this sensitivity using permutations of the input\ndata or masking within hidden layers but these operations are not\ndesigned to be robust to noisy data. We study how TraDE deals with\nthis scenario. We add noise to 10% of the entries in the training data;\nwe then ﬁt both TraDE and NSF on this noisy data; both models are\nevaluated on clean test data. As Table 6 shows, the degradation of both TraDE and NSF is about the\nsame; the former obtains a higher log-likelihood as noted in Table 2.\n5 D ISCUSSION\nThis paper demonstrates that self-attention is naturally suited to building auto-regressive models with\nstrong performance in continuous and discrete valued density estimation tasks. Our proposed method\nis a universal density estimator that is simpler and more ﬂexible (without restrictions of invertibility\n8\nand tractable Jacobian computation) than architectures for normalizing ﬂows, and it also handles\nlong-range dependencies better than other auto-regressive models based on recurrent structures. We\ncontribute a suite of downstream tasks such as regression, out of distribution detection, and robustness\nto noisy data, which evaluate how useful the density estimates are in real-world applications. TraDE\ndemonstrates state-of-the-art empirical results that are better than many competing approaches across\nseveral extensive benchmarks.\n9\nREFERENCES\nYasemin Altun and Alex Smola. Unifying divergence minimization and statistical inference via convex duality.\nIn International Conference on Computational Learning Theory, pp. 139–153. Springer, 2006.\nKonstantinos Benidis, Syama Sundar Rangapuram, Valentin Flunkert, Bernie Wang, Danielle Maddix, Caner\nTurkmen, Jan Gasthaus, Michael Bohlke-Schneider, David Salinas, Lorenzo Stella, et al. Neural forecasting:\nIntroduction and literature overview. arXiv preprint arXiv:2004.10240, 2020.\nRianne van den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester normalizing ﬂows\nfor variational inference. arXiv preprint arXiv:1803.05649, 2018.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson.\nOne billion word benchmark for measuring progress in statistical language modeling. arXiv:1312.3005, 2013.\nMengjie Chen, Qi Zhan, Zepeng Mu, Lili Wang, Zhaohui Zheng, Jinlin Miao, Ping Zhu, and Yang I Li.\nAlignment of single-cell rna-seq samples without over-correction using kernel density matching. bioRxiv,\n2020. doi: 10.1101/2020.01.05.895136.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine\ntranslation. arXiv preprint arXiv:1406.1078, 2014.\nMyung Jin Choi, Vincent YF Tan, Animashree Anandkumar, and Alan S Willsky. Learning latent tree graphical\nmodels. Journal of Machine Learning Research, 12(May):1771–1812, 2011.\nC Chow and Cong Liu. Approximating discrete probability distributions with dependence trees. IEEE transac-\ntions on Information Theory, 14(3):462–467, 1968.\nJasmine Collins, Jascha Sohl-Dickstein, and David Sussillo. Capacity and trainability in recurrent neural\nnetworks. In International Conference on Learning Representations, 2017.\nH. E. Daniels. The asymptotic efﬁciency of a maximum likelihood estimator. In Proceedings of the Fourth\nBerkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of\nStatistics, pp. 151–163, Berkeley, Calif., 1961. University of California Press.\nNicola De Cao, Ivan Titov, and Wilker Aziz. Block neural autoregressive ﬂow.arXiv preprint arXiv:1904.04676,\n2019.\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv\npreprint arXiv:1410.8516, 2014.\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv preprint\narXiv:1605.08803, 2016.\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017.\nRichard M Dudley. Real analysis and probability. Chapman and Hall/CRC, 2018.\nConor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Cubic-spline ﬂows. arXiv preprint\narXiv:1906.02145, 2019a.\nConor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline ﬂows. In Advances in\nNeural Information Processing Systems, pp. 7509–7520, 2019b.\nRobert Fortet and Edith Mourier. Convergence de la r ´epartition empirique vers la r ´epartition th´eorique. In\nAnnales scientiﬁques de l’´Ecole Normale Sup´erieure, volume 70, pp. 267–285, 1953.\nMathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked autoencoder for distribution\nestimation. In International Conference on Machine Learning, pp. 881–889, 2015.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing\nsystems, pp. 2672–2680, 2014.\nWill Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-form\ncontinuous dynamics for scalable reversible generative models. arXiv preprint arXiv:1810.01367, 2018.\nArthur Gretton, Kenji Fukumizu, Zaid Harchaoui, and Bharath K Sriperumbudur. A fast, consistent kernel\ntwo-sample test. In Advances in neural information processing systems, pp. 673–681, 2009.\nArthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch¨olkopf, and Alexander Smola. A kernel\ntwo-sample test. Journal of Machine Learning Research, 13(Mar):723–773, 2012.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\n1997.\n10\nChin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive ﬂows. arXiv\npreprint arXiv:1804.00779, 2018.\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax, 2016.\nMichael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational\nmethods for graphical models. Machine learning, 37(2):183–233, 1999.\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality,\nstability, and variation. arXiv preprint arXiv:1710.10196, 2017.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\nDiederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick.\nIn NIPS, pp. 2575–2583, 2015.\nDurk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved\nvariational inference with inverse autoregressive ﬂow. In Advances in neural information processing systems,\npp. 4743–4751, 2016.\nSmita Krishnaswamy, Matthew H. Spitzer, Michael Mingueneau, Sean C. Bendall, Oren Litvin, Erica Stone,\nDana Pe’er, and Garry P. Nolan. Conditional density-based analysis of t cell signaling in single-cell data.\nScience, 346(6213), 2014. ISSN 0036-8075. doi: 10.1126/science.1250689.\nHugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of the\nFourteenth International Conference on Artiﬁcial Intelligence and Statistics, pp. 29–37, 2011.\nYann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard, and\nLawrence D Jackel. Handwritten digit recognition with a back-propagation network. In Advances in neural\ninformation processing systems, pp. 396–404, 1990.\nChun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnab´as P´oczos. Mmd gan: Towards deeper\nunderstanding of moment matching network. In Advances in Neural Information Processing Systems, pp.\n2203–2213, 2017.\nShiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing\nthe locality and breaking the memory bottleneck of transformer on time series forecasting. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.),Advances in Neural Information\nProcessing Systems 32, pp. 5243–5253. Curran Associates, Inc., 2019.\nPo-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical and algorithmic\ntheory for local optima. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger\n(eds.), Advances in Neural Information Processing Systems 26, pp. 476–484. Curran Associates, Inc., 2013.\nDavid Lopez-Paz and Maxime Oquab. Revisiting classiﬁer two-sample tests. arXiv preprint arXiv:1610.06545,\n2016.\nChris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of\ndiscrete random variables. arXiv preprint arXiv:1611.00712, 2016.\nD. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application\nto evaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th Int’l Conf. Computer\nVision, volume 2, pp. 416–423, July 2001.\nAlfred M ¨uller. Integral probability metrics and their generating classes of functions. Advances in Applied\nProbability, 29(2):429–443, 1997.\nKevin P. Murphy. Machine learning : a probabilistic perspective. MIT Press, Cambridge, Mass. [u.a.], 2013.\nCharlie Nash and Conor Durkan. Autoregressive energy machines. arXiv preprint arXiv:1904.05626, 2019.\nJunier B Oliva, Avinava Dubey, Manzil Zaheer, Barnabas Poczos, Ruslan Salakhutdinov, Eric P Xing, and Jeff\nSchneider. Transformation autoregressive networks. arXiv preprint arXiv:1801.09819, 2018.\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv\npreprint arXiv:1601.06759, 2016.\nGeorge Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive ﬂow for density estimation. In\nAdvances in Neural Information Processing Systems, pp. 2338–2347, 2017.\nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan.\nNormalizing ﬂows for probabilistic modeling and inference. arXiv preprint arXiv:1912.02762, 2019.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran.\nImage transformer. In ICML, 2018.\n11\nSteven J Phillips, Miroslav Dud´ık, and Robert E Schapire. A maximum entropy approach to species distribution\nmodeling. In Proceedings of the twenty-ﬁrst international conference on Machine learning, pp. 83, 2004.\nShebuti Rayana. ODDS library, 2016. http://odds.cs.stonybrook.edu.\nYi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech: Fast, robust\nand controllable text to speech. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 3171–3180. Curran Associates,\nInc., 2019.\nDanilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. arXiv preprint\narXiv:1505.05770, 2015.\nHava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. Journal of computer\nand system sciences, 50(1):132–150, 1995.\nBernard W Silverman. Density estimation for statistics and data analysis. Routledge, 2018.\nHarold W Sorenson and Daniel L Alspach. Recursive Bayesian estimation using Gaussian sums. Automatica, 7\n(4):465–479, 1971.\nBharath Sriperumbudur et al. On the optimal estimation of probability measures in weak and strong topologies.\nBernoulli, 22(3):1839–1893, 2016.\nIngo Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines.Journal of machine\nlearning research, 2(Nov):67–93, 2001.\nEsteban G Tabak and Cristina V Turner. A family of nonparametric density estimation algorithms. Communica-\ntions on Pure and Applied Mathematics, 66(2):145–164, 2013.\nBenigno Uria, Iain Murray, and Hugo Larochelle. A deep and tractable density estimator, 2014.\nBenigno Uria, Marc-Alexandre Cˆot´e, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural autoregressive\ndistribution estimation. The Journal of Machine Learning Research, 17(1):7184–7220, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp.\n5998–6008, 2017.\nAlexander Vergara, Shankar Vembu, Tuba Ayhan, Margaret A Ryan, Margie L Homer, and Ram ´on Huerta.\nChemical gas sensor drift compensation using classiﬁer ensembles. Sensors and Actuators B: Chemical, 166:\n320–329, 2012.\nGrace Wahba, Yuedong Wang, Chong Gu, Ronald Klein, Barbara Klein, et al. Smoothing spline anova for\nexponential families, with application to the wisconsin epidemiological study of diabetic retinopathy: the\n1994 neyman memorial lecture. The Annals of Statistics, 23(6):1865–1895, 1995.\nChenguang Wang, Mu Li, and Alexander J Smola. Language models with transformers. arXiv preprint\narXiv:1904.09408, 2019.\nLarry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.\nNeo Wu, Bradley Green, Xue Ben, and Shawn O’Banion. Deep transformer models for time series forecasting:\nThe inﬂuenza prevalence case, 2020.\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are Transformers\nuniversal approximators of sequence-to-sequence functions? In International Conference on Learning\nRepresentations, 2019.\n12\nAppendix\nTraDE: Transformers for Density Estimation\nA R EGULARIZATION USING MAXIMUM MEAN DISCREPANCY (MMD)\nAn alternative to maximum likelihood estimation, and in some cases a dual to it, is to perform\nnon-parametric moment matching (Altun & Smola, 2006). One can combine a log-likelihood loss\nand a two-sample discrepancy loss to ensure high ﬁdelity, i.e., the samples resemble those from the\noriginal dataset.\nWe can test whether two distributions pand qsupported on a space Xare different using samples\ndrawn from each of them by ﬁnding a smooth function that is large on samples drawn from pand\nsmall on samples drawn from q. If x∼pand y∼q, then p= qif and only if Ex[f(x)] =Ey[f(y)]\nfor all bounded continuous functions f on X(Lemma 9.3.2 in (Dudley, 2018)). We can exploit this\nresult computationally by restricting the test functions to some class f ∈F and ﬁnding the worst\ntest function. This leads to the Maximum Mean Discrepancy metric deﬁned next (Fortet & Mourier,\n1953; M¨uller, 1997; Gretton et al., 2012; Sriperumbudur et al., 2016). For a class Fof functions\nf : X→ R, the MMD between distributions p,q is\nMMD[F,p,q ] = sup\nf∈F\n(\nEx∼p[f(x)] −Ey∼q[f(y)]\n)\n. (5)\nIt is cumbersome to ﬁnd the supremum over a general class of functions Fto compute the MMD. We\ncan however restrict Fto be the unit ball in a universal Reproducing Kernel Hilbert Space (RKHS)\n(Gretton et al., 2012)) with kernel k. The MMD is a metric in this case and is given by\nMMD2[k,p,q ] = E\nx,x′∼p\n[k(x,x′)] −2 E\nx∼p,y∼q\n[k(x,y)] + E\ny,y′∼p\n[k(y,y′)] (6)\nWith a universal kernel (e.g. Gaussian, Laplace), MMD will capture any difference between distri-\nbutions (Steinwart, 2001). We can easily obtain an empirical estimate of the MMD above using\nsamples (Gretton et al., 2012).\nA.1 G RADIENT OF MMD TERM\nEvaluating the gradient of the MMD term (6) involves differentiating the samples from qθ with\nrespect to the parameters θ. When TraDE handles continuous variables, it samples from mixture of\nGaussians for each conditional hence gradient of the MMD term is done using the reparametrization\ntrick (Kingma et al., 2015). On the other hand, gradient of the MMD term (6) is computed using the\nGumbel softmax trick (Maddison et al., 2016; Jang et al., 2016) for discrete variables (i.e., binarized\nMNIST). The objective of TraDE is thus general enough to easily handle both continuous and discrete\ndata distributions which is not usually the case with other methods.\nA.2 W HAT IF ONLY MMD IS USED AS A LOSS FUNCTION ?\nIn theory, MMD with a universal kernel would also produce consistent estimates (Gretton et al.,\n2009), but maximizing likelihood ensures our estimate is statistically efﬁcient (Daniels, 1961; Loh &\nWainwright, 2013). Although a two-sample discrepancy loss like MMD can help produce samples\nthat resemble those from the original dataset, using MMD as the only objective function will not\nresult in a good density estimator given limited data. To test this hypothesis, we run experiments in\nwhich only MMD is utilized as a loss function without the maximum-likelihood term.\nNote that using our MMD term with a ﬁxed kernel (without solving a min-max problem to optimize\nthe kernel) only implies that the distributions match up to the chosen kernel. In other words, the\nglobal minima of our MMD objective with a ﬁxed kernel are not necessarily the global minima of the\nmaximum likelihood term, an additional reason for the poor NLLs as shown in Table S1. While the\nMLE and MMD objectives should both result in a consistent estimator of p(x) in the limit of inﬁnite\ndata and model-capacity, in ﬁnite settings these objectives favor estimators with different properties.\nIn practice a combination of both objectives yields superior results, and makes performance less\ndependent on the MMD kernel bandwidth.\n13\nTable S1: Test likelihood (higher is better) using ONLY the MMD objective for training vs. TraDE.\nDataset Only MMD TraDE\nPOWER -5.00 ±0.24 0.73 ±0.00\nGAS -10.85 ±0.16 13.27 ±0.01\nHEPMASS -28.18 ±0.12 -12.01 ±0.03\nMINIBOONE -68.06 -9.49 ±0.13\nBSDS300 70.70 ±0.32 160.01 ±0.02\nB D ENSITY ESTIMATION WITH FEW DATA\nMMD regularization especially helps when we have few training data. Test likelihoods (higher is\nbetter) after training on a sub-sampled MINIBOONE dataset are: -55.62 vs. -55.53 (100 samples)\nand -49.07 vs. -48.90 (500 samples) for TraDE without and with MMD-penalization, respectively.\nDensity estimation from few samples is common in biological applications, where experiments are\noften too costly to be extensively replicated (Krishnaswamy et al., 2014; Chen et al., 2020).\nC S AMPLES FROM TRADE TRAINED ON MNIST\nWe also evaluate TraDE on the MNIST dataset in terms of the log-likelihood on test data. As Table 1\nshows TraDE obtains high log-likelihood even compared to sophisticated models such as Pixel-\nRNN (Oord et al., 2016). This is a difﬁcult dataset for density estimation because of the high\ndimensionality. Fig. S1 shows the quality of the samples generated by TraDE.\nFigure S1: Samples from TraDE ﬁtted on binary MNIST.\nD H YPER -PARAMETERS\nAll models are trained for 1000 epochs with the Adam optimizer (Kingma & Ba, 2014). The\nMMD kernel is a mixture of 5 Gaussians for all datasets, i.e. k(x,y) =∑5\nj=1 kj(x,y) where each\nkj(x,y) = e−∥x−y∥2\n2/σ2\nj with bandwidths σj ∈ {1,2,4,8,16}(Li et al., 2017). Each layer of\nTraDE consists of a GRU followed by self-attention block which consists of multi-head-self-attention\nand fully connected feed-forward layer. A layer normalization and a residual connection is used\naround each of these components (i.e. multi-head-self-attention and fully connected) in self-attention\nblock (Vaswani et al., 2017). Multiple layers of self-attention block can be stacked together in TraDE.\nFinally, the output layer of TraDE outputs πk,i,µk,i, and σk,i which specify a univariate Gaussian\nmixture model approximation of each conditional distribution in the auto-regressive factorization.\nTable S2 shows TraDE hyper-parameters. We used a coarse random search based on validation NLL\nto select suitable hyper-parameters. Note that dataset used in this work have very different number of\nsamples and dimensions as shown in Table S3\n14\nTable S2: Hyper-parameters for benchmark datasets.\nPOWER GAS HEPMASS MINIBOONE BSDS300 MNIST\nMMD coefﬁcientλ 0.2 0.1 0.1 0.4 0.2 0.1\nGaussian mixture components 150 100 100 20 100 1\nNumber of layers 5 8 6 8 5 6\nMulti-head attention head 8 16 8 8 2 4\nHidden neurons 512 400 128 64 128 256\nDropout 0.1 0.1 0.1 0.2 0.3 0.1\nLearning rate 3E-4 3E-4 5E-4 5E-4 5E-4 5E-4\nMini-batch size 512 512 512 64 512 16\nWeight decay 1E-6 1E-6 1E-6 0 1E-6 1E-6\nGradient clipping norm 5 5 5 5 5 5\nGumbel softmax temperature n/a n/a n/a n/a n/a 1.5\nTable S3: Dataset information. In order to ensure that results are comparable and same setups and data\nsplits are used as previous works, we closely follow the experimental setup of (Papamakarios et al., 2017) for\ntraining/validation/test dataset splits. In particular, the preprocessing of all the datasets is kept the same as that\nof (Papamakarios et al., 2017). Note that, these setups were used on all other baselines as well.\nDataset dimension Training size Validation Size Test size\nPOWER 6 1659917 184435 204928\nGAS 8 852174 94685 105206\nHEPMASS 21 315123 35013 174987\nMINIBOONE 43 29556 3284 3648\nBSDS300 63 1000000 50000 250000\nMNIST 784 50000 10000 10000\nForestCover 10 252499 28055 5494\nPendigits 16 5903 655 312\nSatimage-2 36 5095 566 142\n15",
  "topic": "Estimator",
  "concepts": [
    {
      "name": "Estimator",
      "score": 0.7595446109771729
    },
    {
      "name": "Covariate",
      "score": 0.6458948254585266
    },
    {
      "name": "Autoregressive model",
      "score": 0.579071044921875
    },
    {
      "name": "Computer science",
      "score": 0.5620018243789673
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5576733350753784
    },
    {
      "name": "Suite",
      "score": 0.5334285497665405
    },
    {
      "name": "Econometrics",
      "score": 0.48835107684135437
    },
    {
      "name": "Density estimation",
      "score": 0.4624498188495636
    },
    {
      "name": "Conditional probability distribution",
      "score": 0.4410785734653473
    },
    {
      "name": "Statistics",
      "score": 0.43000102043151855
    },
    {
      "name": "Data mining",
      "score": 0.3689844012260437
    },
    {
      "name": "Mathematics",
      "score": 0.2452985942363739
    },
    {
      "name": "Machine learning",
      "score": 0.2427428960800171
    },
    {
      "name": "Geography",
      "score": 0.083243727684021
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 13
}