{
  "title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
  "url": "https://openalex.org/W4393156822",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2103344185",
      "name": "Fangjun Li",
      "affiliations": [
        "University of Leeds"
      ]
    },
    {
      "id": "https://openalex.org/A1966384941",
      "name": "David C. Hogg",
      "affiliations": [
        "University of Leeds"
      ]
    },
    {
      "id": "https://openalex.org/A2149556533",
      "name": "Anthony G. Cohn",
      "affiliations": [
        "University of Leeds",
        "Turing Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2103344185",
      "name": "Fangjun Li",
      "affiliations": [
        "University of Leeds"
      ]
    },
    {
      "id": "https://openalex.org/A1966384941",
      "name": "David C. Hogg",
      "affiliations": [
        "University of Leeds"
      ]
    },
    {
      "id": "https://openalex.org/A2149556533",
      "name": "Anthony G. Cohn",
      "affiliations": [
        "University of Leeds",
        "The Alan Turing Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2775703036",
    "https://openalex.org/W6670372396",
    "https://openalex.org/W1871668702",
    "https://openalex.org/W4249889216",
    "https://openalex.org/W23606365",
    "https://openalex.org/W7023684241",
    "https://openalex.org/W3153839026",
    "https://openalex.org/W4224266081",
    "https://openalex.org/W6633008301",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W1525961042",
    "https://openalex.org/W4385570924",
    "https://openalex.org/W4366999212",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4330337479",
    "https://openalex.org/W2079249289",
    "https://openalex.org/W4385572793",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4385570088",
    "https://openalex.org/W1549715194"
  ],
  "abstract": "Artificial intelligence (AI) has made remarkable progress across various domains, with large language models like ChatGPT gaining substantial attention for their human-like text-generation capabilities. Despite these achievements, improving spatial reasoning remains a significant challenge for these models. Benchmarks like StepGame evaluate AI spatial reasoning, where ChatGPT has shown unsatisfactory performance. However, the presence of template errors in the benchmark has an impact on the evaluation results. Thus there is potential for ChatGPT to perform better if these template errors are addressed, leading to more accurate assessments of its spatial reasoning capabilities. In this study, we refine the StepGame benchmark, providing a more accurate dataset for model evaluation. We analyze GPT’s spatial reasoning performance on the rectified benchmark, identifying proficiency in mapping natural language text to spatial relations but limitations in multi-hop reasoning. We provide a flawless solution to the benchmark by combining template-to-relation mapping with logic-based reasoning. This combination demonstrates proficiency in performing qualitative reasoning on StepGame without encountering any errors. We then address the limitations of GPT models in spatial reasoning. To improve spatial reasoning, we deploy Chain-of-Thought and Tree-of-thoughts prompting strategies, offering insights into GPT’s cognitive process. Our investigation not only sheds light on model deficiencies but also proposes enhancements, contributing to the advancement of AI with more robust spatial reasoning capabilities.",
  "full_text": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation\nand Enhancement Using the StepGame Benchmark\nFangjun Li1, David C. Hogg1, Anthony G. Cohn1,2\n1School of Computing, University of Leeds, Leeds, UK\n2Alan Turing Institute, UK\nscfli@leeds.ac.uk, d.c.hogg@leeds.ac.uk, a.g.cohn@leeds.ac.uk\nAbstract\nArtificial intelligence (AI) has made remarkable progress\nacross various domains, with large language models like\nChatGPT gaining substantial attention for their human-like\ntext-generation capabilities. Despite these achievements, spa-\ntial reasoning remains a significant challenge for these mod-\nels. Benchmarks like StepGame evaluate AI spatial reason-\ning, where ChatGPT has shown unsatisfactory performance.\nHowever, the presence of template errors in the benchmark\nhas an impact on the evaluation results. Thus there is po-\ntential for ChatGPT to perform better if these template er-\nrors are addressed, leading to more accurate assessments of\nits spatial reasoning capabilities. In this study, we refine the\nStepGame benchmark, providing a more accurate dataset for\nmodel evaluation. We analyze GPT’s spatial reasoning per-\nformance on the rectified benchmark, identifying proficiency\nin mapping natural language text to spatial relations but limi-\ntations in multi-hop reasoning. We provide a flawless solution\nto the benchmark by combining template-to-relation mapping\nwith logic-based reasoning. This combination demonstrates\nproficiency in performing qualitative reasoning on StepGame\nwithout encountering any errors. We then address the limita-\ntions of GPT models in spatial reasoning. We deploy Chain-\nof-thought and Tree-of-thoughts prompting strategies, offer-\ning insights into GPT’s “cognitive process”, and achieving\nremarkable improvements in accuracy. Our investigation not\nonly sheds light on model deficiencies but also proposes en-\nhancements, contributing to the advancement of AI with more\nrobust spatial reasoning capabilities.\nIntroduction\nSpatial reasoning, the ability to understand and navigate re-\nlationships in physical space, is a fundamental aspect of hu-\nman cognition that significantly influences interactions with\nthe environment. Enhancing spatial reasoning in AI mod-\nels has the potential to enrich their comprehension of their\nsurroundings and response to user interactions, leading to\nmore advanced and immersive user experiences (Alomari\net al. 2022). In recent years, AI has revolutionized numer-\nous domains, from healthcare to finance to entertainment.\nNotably, OpenAI’s large language models (LLMs), such as\nChatGPT and GPT-4 (OpenAI 2023), have gained signifi-\ncant attention for their human-like text generation capabil-\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nities. However, despite their impressive abilities, LLMs en-\ncounter challenges in many logical reasoning aspects cru-\ncial for human communication, particularly spatial reason-\ning (Bang et al. 2023; Cohn and Hernandez-Orallo 2023).\nOne approach to evaluating spatial reasoning in an AI sys-\ntem is to use synthetic benchmarks such as StepGame (Shi,\nZhang, and Lipani 2022) and SpartQA (Mirzaee and Rajaby\n2021). Unfortunately, models like ChatGPT have shown un-\nsatisfactory performance on these benchmarks. Improving\nthe spatial reasoning capabilities of LLMs remains a primary\nfocus to enhance their overall performance and understand-\ning of complex environments.\nWhilst examining the StepGame benchmark we discov-\nered that it contains template errors that distort model per-\nformance evaluations. These errors were previously over-\nlooked, leading to studies conducted on a flawed benchmark,\ninaccurately assessing the capabilities of the LLMs (Bang\net al. 2023; Yang, Ishay, and Lee 2023). To rectify this is-\nsue, we present a more accurate version of the StepGame\ndataset for model evaluation, ensuring precise assessments\nof the models’ true capabilities and limitations1.\nWe then conducted evaluation tests on the rectified bench-\nmark across various test subsets, few-shot sets, and mod-\nels. We observed that larger GPT models demonstrate profi-\nciency in mapping natural language text to spatial relations.\nHowever, they struggle with multi-hop spatial reasoning.\nOur goal is not merely to critique, but also to propose po-\ntential improvements. To this end, we provide a flawless so-\nlution to the benchmark, and explore different approaches to\nenhance the spatial reasoning ability of LLMs.\nThe solution we propose for the benchmark entails com-\nbining template-based sentence-to-relation mapping with\nlogic-based spatial reasoning. The logical reasoner used in\nthis approach comes from (Yang, Ishay, and Lee 2023),\nwhere they integrated GPT-3 for the task. GPT-3 was em-\nployed to parse spatial descriptions into symbolic spatial re-\nlation representations, which were then passed to the log-\nical program for spatial reasoning. This fusion resulted in\nsignificant improvement in StepGame, achieving state-of-\nthe-art (SOTA) but not perfect results: around 90% accu-\n1The data associated with this paper are openly available\nfrom the University of Leeds Data Repository. https://doi.org/10.\n5518/1468. All code is available at https://github.com/Fangjun-\nLi/SpatialLM-StepGame.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18500\nracy for lower hops and 88.3% accuracy for 10-hop reason-\ning. They attributed10.7% faults to data-related issues. With\nour aforementioned work on rectifying the benchmark, We\ntake a step further to delve into the two components, an-\nalyzing the performance of each on our filtered version of\nthe dataset. Remarkably, we achieved100% accuracy for al-\nmost all hops, with only 2 errors among 1000 test exam-\nples, which were due to GPT-3’s incorrect semantic pars-\ning. Building on this, we replaced the GPT-3 parser with\nour sentence-to-relation mapping method and combined it\nwith the ASP reasoner, showcasing proficiency in perform-\ning qualitative reasoning without encountering any errors,\nthus demonstrating a method to achieve a perfect score on\nthe corrected benchmark.\nNeither our solution or the SOTA utilize LLMs for the\nactual spatial reasoning functionality. Thus, we proceed to\nenhance GPT’s capabilities as a native spatial reasoner. To\nachieve this, we employ Chain-of-Thoughts (CoT) and Tree-\nof-Thoughts (ToT) prompting strategies.\nCoT (Wei et al. 2022) incorporates a sequence of inter-\nmediate reasoning steps to facilitate problem-solving. How-\never, when applied to StepGame, previous studies (Yang,\nIshay, and Lee 2023) have shown that CoT does not con-\nsistently improve performance and may even reduce accu-\nracy in complex k-hop reasoning tasks. This observation is\nattributed to the higher probability of errors occurring in\nlengthy CoT processes. On the other hand, research on other\ntasks (Zhou et al. 2022; Creswell, Shanahan, and Higgins\n2022) has demonstrated that breaking down complex prob-\nlems into simpler subproblems and solving them sequen-\ntially can be beneficial. Given the ambiguity in the decom-\nposition of “thoughts”2 within CoT, we propose refining the\nCoT prompt to empower language models to perform better\nin spatial reasoning tasks.\nOn the other hand, (Yao et al. 2023) introduced ToT, a\nframework enabling LLMs to explore multiple reasoning\npaths, and they demonstrated its effectiveness in improving\nproblem-solving capabilities across tasks like the game of\n24, creative writing, and mini crosswords. In our work, we\ncustomize the ToT approach for object-linking chain build-\ning, a crucial subproblem in addressing spatial reasoning\nbenchmarks.\nOur customized CoT method showcases its advantages\nmore prominently in larger models such as GPT-4 and\nDavinci, maintaining accuracy even as the tasks become\nmore complex. Our ToT approach demonstrates its strengths\non the three GPT models: on the largest model, GPT-4, we\nare able to maintain an accuracy of around 90% even as the\ntasks become more complex. On Davinci, the accuracy is\nmaintained at around 50%, while Turbo achieves a lower\nlevel of accuracy at around 30%.\nBy identifying current deficiencies and proposing en-\nhancements, we aim to contribute to the ongoing discourse\n2In this paper we use the word ‘thoughts’ in the same way as\nis now being used in the literature on CoT and ToT, whilst noting\nthat these are not thoughts in the human sense but rather generated\ncoherent units of text , serving as intermediate steps in a problem\nsolving setting, and without wishing to ascribe an anthropomorphic\nmeaning to the word.\nin AI development, pushing the boundaries of what LLMs\ncan achieve. Ultimately, our investigation can pave the way\nfor the development of advanced, intuitive, and user-friendly\nAI systems with robust spatial reasoning capabilities.\nRelated Work\nThe field of spatial reasoning in language with artificial in-\ntelligence has evolved through sustained efforts over time,\nwith significant advancements achieved through both tradi-\ntional methods and modern LLMs.\nEarly strides in spatial reasoning in language were marked\nby the development of formal structures to represent spatial\nrelationships. (Kordjamshidi, Moens, and van Otterlo 2010)\nproposed a spatial ontology to formalize the representation\nof spatial relations. This work laid the groundwork for the\nsubsequent introduction of text-based spatial role labeling\n(Kordjamshidi, Van Otterlo, and Moens 2011), which aims\nto convert text into formal spatial representations.\nThen comes synthetic tasks designed to evaluate the text\nunderstanding and spatial reasoning capabilities of learning\nalgorithms. The positional reasoning task (Task 17) in the\nbAbI dataset (Weston et al. 2015) is for spatial reasoning and\nrequires models to reason using one or two sentences, which\nmakes this task comparatively simple. (Shi, Zhang, and Li-\npani 2022) advanced this field by creating the StepGame\nbenchmark to evaluate multi-hop spatial reasoning in text,\nwith richer variety in spatial relation descriptions. Both of\nthese datasets emphasize directional spatial relations (Cohn\nand Hazarika 2001; Skiadopoulos and Koubarakis 2001;\nCohn and Renz 2008; Chen et al. 2015). Three spatial QA\ndatasets: SpartQA(Mirzaee and Rajaby 2021), SPARTUN,\nand RESQ (Mirzaee and Kordjamshidi 2022) expanded the\nresource landscape by encompassing wider-ranging spatial\nlanguage expressions, posing challenges for traditional log-\nical programming, and are important benchmarks for evalu-\nating LLMs’ spatial reasoning capabilities.\nConcurrently, the advent of LLMs such as OpenAI’s\nChatGPT has opened up fresh pathways for spatial rea-\nsoning. These models, leveraging transformer architectures,\ncan generate human-like text and handle complex linguis-\ntic structures. However, while these models are indeed im-\npressive, their capabilities in spatial reasoning are yet to be\nfully explored and exploited. One recent approach to assess\nthese capabilities was taken by (Bang et al. 2023), who put\nChatGPT to the test using SpartQA and StepGame. Despite\nthe generally advanced capabilities of ChatGPT, the model\nshowed shortcomings in these tasks, signaling a need for fur-\nther enhancements in the realm of spatial reasoning.\nA promising technique known as ‘prompt engineering’\n(Bommasani et al. 2021) has been making its mark recently.\nThis approach involves crafting specific prompts to guide\nthe responses of the models, leading to outputs that are\nmore contextually apt and insightful. This method demon-\nstrates significant potential in enhancing the capabilities of\nLLMs like ChatGPT in various domains (Li, Hogg, and\nCohn 2022), including the challenging area of logical rea-\nsoning (Wang et al. 2023). For instance, when faced with\nmulti-step reasoning tasks, a method called ‘few-shot chain-\nof-thought’ (CoT) prompting (Zhang et al. 2022) comes into\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18501\nFigure 1: An illustrative example for demonstrating relation\nextraction and 1-hop spatial reasoning.\nplay. These demonstrations enable LLMs to explicitly gen-\nerate reasoning steps, thereby improving their accuracy in\nreasoning tasks. This technique involves a handful of manu-\nally curated step-by-step reasoning demonstrations.\nAs we review these developments, it is clear that while\nsignificant progress has been made, challenges remain in\nboth traditional and LLM approaches to spatial reasoning.\nThe limitations of models like ChatGPT indicate the need\nfor continued research and enhancement strategies. This pa-\nper aims to contribute to this by examining these limitations\nmore closely and proposing potential avenues for improve-\nment. We aim to explore the limit of GPT as a general prob-\nlem solver that explores its own thoughts and guides its own\nexploration with deliberate reasoning as heuristics.\nThe StepGame Benchmark for Evaluating\nSpatial Reasoning\nIn this paper, we focus on StepGame, in line with other stud-\nies that evaluate ChatGPT’s spatial reasoning proficiency\nusing StepGame and SpartQA. StepGame comprises story-\nquestion pairs in natural language, The objective is to answer\nquestions regarding the spatial relations between two speci-\nfied entities. The StepGame benchmark contains two sets of\ndata: a clean set where there are precisely k facts given for\nany given k-hop instance, and a noise set where there are\nmore than k facts given, and the extra facts are distracting.\nSpatial Reasoning Types\n• 1-Hop Spatial Reasoning. In 1-hop reasoning, we are\ngiven a relation description between two entities and are\nasked about the spatial relation from one entity to the\nother. 1-hop relation reasoning and relation extraction can\nbe considered similar processes. As exemplified in Fig-\nure 1, consider the story where J is diagonally above B\nto the right at a 45-degree angle. The question is What is\nthe spatial relation of agent J to agent B?. This is similar\nto relation extraction. However, if we change the question\nto What is the spatial relation of agent B to agent J?, it\nneeds a reverse reasoning process top\nright(“J”, “B”) →\ndown left(“B”,“J”). Both expressions are correct repre-\nsentations for relation extraction.\n• Multi-Hop Spatial Reasoning. Figure 2 provides one ex-\nample of 10-hop reasoning, which is from the ‘clean’ set.\nThe questions ask about the relation between two objects,\nFigure 2: Example of 10-hop reasoning, featuring a question\nregarding two entities that are not directly connected in the\nstories. The diagrams on the right do not form part of the\ninput to the AI system but are for illustrative purposes only.\neither directly or indirectly connected. Multi-hop reason-\ning adds more complexity to the problem, as it involves a\ngreater number of provided relations. To solve the prob-\nlem, one needs to identify useful relations and then pro-\nceed with relation inference step by step.\nProblems with the Dataset\nEight spatial relations (top, down, left, right, top-left, top-\nright, down-left, and down-right ) are utilized for the story\ngeneration of StepGame. These relations are expressed\nthrough sentences in natural language. All sentences/state-\nments are based on a crowd-sourced template base 3. Each\n“story” is accompanied by a question that seeks to identify\nthe relations between two objects, and it is labeled according\nto the intended relations at the time of story creation, rather\nthan the actual sentences used. A template is considered to\ncontain an error if the meaning conveyed by the sentence\ndoes not align with the relationship that was intended to be\nexpressed during the creation of stories and labels.\nTable 1 presents a detailed enumeration of errors in the\nrelation-to-sentence mappings identified in StepGame. Out\nof the 214 templates examined, 14 were found to contain er-\nrors. Of the eight different relationship mappings available,\nonly o1\nabove o2 and o1 left o2 are devoid of mistakes.\nThe question arises as to why there are so many errors in\nthe crowd-sourced expressions; presumably this is down to\ninsufficient quality control over the crowdworker reponses.\nFor each k value, the StepGame dataset includes 10,000\ntest samples. Table 2 displays the percentage of exam-\nples containing sentences derived from incorrect templates,\nwhich hints at a rising trend in inaccuracies as k increases,\nsuggesting a potential cumulative impact.\nAmong these 14 incorrect templates, four cannot be reme-\ndied in existing StepGame benchmark examples.\n• o1\nupperright o2: Object A is above object o1.\n• o1 upperleft o2 : o1 is diagonally left and above o1.\n• o1 lowerright o2 | o1 upperleft o2 | o1 upperright o2:\no1 is to the right and above o2 at an angle of about 45\ndegrees.\n3https://github.com/ZhengxiangShi/StepGame/blob/main/\nCode/template.py\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18502\nRelation Original Incorrect Template\nright\no2 and o1 are parallel, and o2 on the right of o1.\no2 and o1 are parallel, and o2 is to the right of o1.\no2 and o1 are horizontal and o2 is to the right of o1.\no2 and o1 are both there with the object o2 is to the\nright of object o1.\nbelow\no2 is placed at the bottom of o1.\no2 is at the bottom of o1 and is on the same vertical\nplane.\no2 presents below o1.\nlowerleft o2 is there and o1 is at the 10 position of a clock face.\no2 is positioned below o1 and to the left.\nupperright Object A is above object o1 and to the right of it, too.\no2 is diagonally to the upper right of o1.\nlowerright o1 is to the right and aboveo2 at an angle of about 45\ndegrees.\nupperleft o1 is to the right and aboveo2 at an angle of about 45\ndegrees.\no1 is diagonally left and above o1.\nTable 1: Incorrect sentence templates in StepGame. The Re-\nlation column signifies relation for o1 relation o2.\nk=1 k=2 k=3 k=4 k=5 k=6 k=7 k=8 k=9 k=10\nClean 7.64 15.03 20.87 26.39 32.54 37.66 41.71 47.20 51.50 54.29\nNoise 20.43 30.19 34.59 48.18 57.13 61.14 63.60 69.45 72.84 74.21\nTable 2: Percentage of incorrect instances out of all instances\nover k=1–10 test sets.\n• o1\nlowerleft o2 | o1 upperleft o2: o2 is there and o1 is\nat the 10 position of a clock face.\nThe first and second templates are irreparable because\nit is impossible to identify what o2 is when sentences\nare formed using them. The third and fourth templates\ncannot be corrected since they were applied to multiple\nspatial relations, although each accurately represents just\none. For example, for the sentence ‘Q is to the right and\nabove P at an angle of about 45 degrees’, three map-\nping relations exist: Q\nupperright P, Q lowerright P,\nand Q upperleft P. Although this sentence expresses the\nmeaning Q upperright P, it is uncertain which candidate\nwas used for the label. For such templates, a unique correc-\ntion could not be chosen, necessitating the removal of the\nsentences that use these template from the dataset.\nMethods\nSolution for the Corrected Benchmark\nOur error-free approach is entirely logic-based, without\nthe use of LLMs. We begin by performing template-based\nsentence-to-relation mapping, akin to semantic parsing.\nThen, we employ ASP for logical reasoning, utilizing the\nASP reasoner introduced by (Yang, Ishay, and Lee 2023).\nThese two components operate independently:\n• Sentence-to-Relation Mapping. When presented with a\nnatural language relation description r, we first identify\nthe template used in r through a comparison with the tem-\nplate base. This template is symbolized as o0\nν o1. Then,\nFigure 3: Sentence-to-Relation Mapping Examples.\nwe convert this template form into a structured represen-\ntation ν(o0, o1), where o0 and o1 correspond to the two\nobjects mentioned in r, and ν signifies the spatial relation\nbetween o0 and o1. Specifically, for questions inquiring\nabout relations from the start object o0 to the target ob-\nject ot, the template is query o0 ot, and the correspond-\ning ASP fact is represented as query(o0, ot). Illustrative\nexamples of this process can be found in Figure 3.\n• Logical Reasoning with ASP. The logical factsν(o0, o1),\ngenerated through semantic parsing for all relations in the\nstory R, are used as input to the ASP module for spa-\ntial reasoning. The ASP module was implemented us-\ning Clingo and includes rules specifically tailored for\nStepGame. These rules transform StepGame into a qual-\nitative spatial reasoning problem in a 2D grid space.\nThese rules incorporate offsets for 9 spatial relations,\nsuch as offset(right) = (0 , 1) and offset(lower-left) =\n(−1, −1). The main rule in the ASP module calculates the\nlocation of o0 to o1 by adding the offsets ν(o0, o1).\nWhile this approach offers a solution to the StepGame\nbenchmark challenge, it does require prior familiarity with\nthe templates and mandates updates to the template base\nwhen confronted with new stories employing novel tem-\nplates. In contrast, an LLM approach holds the potential\nto flexibly adjust to unfamiliar templates. Additionally, the\nmethod’s dependence on customized rules within the logical\nprogram constitutes another aspect to be mindful of.\nChain-of-Thought (CoT) Prompting\nWe devised a customized CoT for the spatial reasoning task.\nThe core idea of CoT is to introduce a chain of thoughts\nc1, . . . , ci, . . . , cn to bridge input x and output y, where i\nrepresents i-th step. In our customized CoT for StepGame,x\nconsists of the task description, few-shot examples, relation\nstory, and question, while y represents the answer regarding\nthe relations between the queried objects (from the start ob-\nject oi to the target object ot). Each thought ci is to identify\ndirect spatial connections between objects (oi and oi+1). We\ntake CoT a step further by decomposing each step of thought\nci to explore the potential advantages of incorporating a co-\nherent and detailed reasoning process.\nThought Categorisation. We categorise the thought into\nthree types: link establishment thoughts clink, relation map-\nping thoughts cmap, and coordinate calculation thoughts\nccalcu. At each reasoning step, these three types of thought\nare sequentially sampled as a continuous language sequence\nci = [clink\ni , cmap\ni , ccalcu\ni ] using the LLM.\n1. clink\ni : Guide the LLM to examine all relations in the story\n(R = [r1, . . . , rj, . . . , rk]) and select rj for the i-th step\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18503\nfor k-hop reasoning, ensuring it directly describes the re-\nlation with oi and has not been used in any previous step.\nFor the start object (i= 0), we use the prompt “Start with\no0. According to” and for the middle objects (i ≥ 1), we\nuse the prompt “Then search for oi. According to”. Full\ndetails of the prompts can be found in the Appendix4.\n2. cmap\ni : Map rj to a simple relation description such as “oi\nis to the ν of oi+1,” where ν represents the key spatial\nrelation from oi to oi+1. The prompt “This means” helps\nthe LLM perform this mapping.\n3. ccalcu\ni : Use rj to calculate the coordinates ofoi+1. We set\noo at (0,0), and each spatial relation is assigned an off-\nset to determine the positions of the objects. The prompt\n“oi+1 = oi + offset(rj) = ( xoi , yoi ) + ( xν, yν) =\n(xoi+1 , yoi+1 )” instructs the LLM on the calculation pro-\ncess. It computes the coordinates of oi+1 and generates\nthe output like “Therefore, B is at (xoi+1 , yoi+1 ).”\nTree-of-Thoughts (ToT) Prompting\nAlgorithm 1 is designed to enhance the reasoning chain-\nbuilding process, allowing LLMs to consider different path-\nways. This is useful because during the search for relations\nwith an object, distracting connections may arise, as shown\nin Figure 2. However, it is essential to follow a correct se-\nquence to successfully reach the target object. If an LLM\nmistakenly tracks an incorrect sequence, it could get stuck in\na dead end leading to incorrect reasoning conclusions such\nas “The story does not provide direct spatial information.”\nThe algorithm initiates by prompting the LLM to set up\nthe initial tree state, denoted as S0, using the input x, which\ncomprises a story and a question. S0 is in the form “chain:\no0 ->, target: ot, unused: R”. R represents all connections\nbetween objects in the story, in the form ofobject1-object2.\nThen it proceeds to construct a linking chain from o0 to ot\nin iterative steps, wherein for the i-th step (1 ≤ i ≤ 10),\nthe LLM considers the tree state Si−1 built up to that step.\nIf no state s in Si−1 reaches ot, the LLM is prompted to\ngenerate j candidate thoughts for each s in the current set\nof states, Si (j = 2 in this paper). G prompts the LLM\nto search for a potential object oi connected to the current\nobject oi−1 from the unused relations Runused\ni−1 . A check is\nmade (CheckExtn(c))) to see if the proposal made is a real\ncandidate extension. For all candidate thoughts, V prompts\nthe LLM to evaluate the state to determine if the chain can\nproceed with oi and the updated Runused\ni−1 to reach ot. The\ntop-rated b tree states in S\n′\ni are selected as Si. When there is\na state sf which reaches ot, the L will be prompted with the\nlinking chain construction prompt (Appendix D.4) to form\nthe final links l.\n• Thought Generation G(s, j). Given a tree state s, we let\nthe LLM propose j thoughts using the thought generation\nprompt “Use relations listed in unused relations to enu-\nmerate all potential expansions of the chain by considering\nunused relations that exhibit a direct link to the last object\n4The ArXiv version of this paper includes the Appendix con-\ntaining prompting examples.\nAlgorithm 1: Our ToT Approach\nRequire: LLM, input x\n1: S0 ← Init(x)\n2: i ← 1\n3: while no sf ∈ Si−1 has arrived at ot do\n4: S\n′\ni ← {s·c|c ∈ G(s, j)∧ChainExtn(c)∧s ∈ Si−1}\n5: if S\n′\ni = ∅ then return failure\n6: Si ← select(b, {⟨s, y⟩|s ∈ S′\ni ∧ y = Σn\n1 σ(V (s))})\n7: i = i + 1\n8: end while\n9: return Link(sf )\nwithin the chain.” In our experiment, we set j = 2, mean-\ning that we instruct the LLM to generate content twice for\neach state s ∈ Si−1.\n• State Evaluation V (s). Our approach involves a classi-\nfication methodology, using the designed value prompt\n“Evaluate whether the chain can reach the target (sure/-\nlikely/impossible). If the chain has already reached the tar-\nget, it’s ‘sure’. If the unused relations include the current\nobject, it’s ‘likely’. If there are no unused relations that\ninclude the current object, it’s ‘impossible’.” This prompt\nguides the LLM to sequentially examine all newly gen-\nerated states s ∈ S\n′\ni n times – using the stochasticity of\nthe LLM with a non zero temperature to increase the reli-\nability of the scoring. The three types of outputs - ‘sure’,\n‘likely’, and ‘impossible’ - are converted into numerical\nscores using a function σ() to facilitate the selection pro-\ncess among all newly generated states.\n• Search Algorithm The choice between utilizing breadth-\nfirst search (BFS) or depth-first search (DFS) depends on\nthe tree structure. In the StepGame benchmark, the tree\ndepth is limited (depth ≤ 10), and the number of thought\ncandidates k for each step is also limited (width ≤ 3 in\nmost cases). However, a deeper search does not necessar-\nily guarantee better results. In certain scenarios, o0 and\not may be directly connected in one relation statement,\nallowing for shorter linking chains between them, which\nis preferable. Therefore, we opt for BFS to maintain all\npromising states. We set the breadth width b = 3, main-\ntaining the three most promising linking-chain states per\nstep. The criterion for stopping searching is set when the\nlinking chain arrives at the target object.\nOur ToT approach is used to construct the reasoning\nchain from o0 to ot. Subsequently, the spatial relation be-\ntween these objects is computed following the previous CoT\nprompting method, with the use of cmap and ccalcu.\nExperimental Design\nModel Settings\nWe use the Azure OpenAI Service for ChatGPT (3.5-Turbo)\nand GPT-3 (Davinci), and GPT-4 API access. To yield more\nconcentrated and deterministic results, we set the tempera-\nture to 0 in CoT experiments. In ToT experiments, we follow\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18504\n(Yao et al. 2023), setting the temperature to 0.7 for gener-\nating varied thought proposals. The remaining parameters\nwere left at the standard configurations for these models.\nDifferent Test Subsets It is common practice in the stud-\nies cited (Bang et al. 2023), (Yang, Ishay, and Lee 2023) to\nuse a subset of 30 or 100 test examples from the full set of\n10,000 for each k value. While this method helps in con-\nserving token usage, it could potentially introduce biases or\ninaccurate estimations of the model performance.\nWe examine the effect of the number of test examples.\nSpecifically, we wanted to determine whether evaluating on\na limited number of test examples could introduce inaccura-\ncies. To achieve this, we conducted tests on a clean, filtered\ntest set for k-hop reasoning (k ∈ [1, 10]), thereby covering\na range of task complexities. Tests were carried out on 30,\n100, and 1000 test examples to assess the impact of the num-\nber of test examples on the evaluation.\nDifferent Few-Shot Sets We created three different few-\nshot prompting sets to evaluate the influence of input exam-\nples in prompts.\n• clean 5shot(1,3,5,7,10): Create a prompt consisting of five\nexamples, with one example each from tasks requiring 1-\nhop, 3-hop, 5-hop, 7-hop, and 10-hop reasoning.\n• clean 10shot : Formulate a prompt using ten examples,\neach one derived from a distinct k-hop task in clean set.\n• clean 5shot separate: Construct a prompt for each k-hop\nreasoning task, utilizing five examples from the corre-\nsponding k-hop training set as few-shot examples.\nExperimental Results\nEvaluation Results\nInfluence of Scale of Test Examples We employ the\nclean 10shot prompting setting. The results are presented\nin the left subplot of Figure 4. Upon evaluation of the ex-\npanded test set comprising 1000 examples, the model shows\na uniform decrement in performance as k increases from 1\nto 10. This trend indicates the increased complexity as the\nnumber of hops increases. With smaller test sets of 100 or\n30 examples, the trend is less consistent, and there are oc-\ncasional increases in performance at certain hop levels. The\nvariance in performance, particularly for the 30-example test\nset, may indeed be larger. This could be due to the smaller\nsample size providing less comprehensive coverage of the\npotential range of tasks, leading to more fluctuations in per-\nformance. This indicates larger test sets can provide a more\nstable and reliable indicator of a model’s performance across\ndifferent complexity levels (i.e., number of hops).\nInfluence of Prompting Examples The middle subplot in\nfigure 4 indicates that the choice of prompting strategy can\nimpact the model’s ability to handle tasks of varying com-\nplexity. Similar to the previous data, all prompting strategies\nshow a trend of decreasing accuracy as the number of hops\nincreases. This trend is consistent and suggests that the com-\nplexity of the tasks grows with the number of hops.\nThe performances of the three methods are close. While\ndifferences exist at specific hop levels, no single method\nleft/\nright\nabove\n/below\nlower left/\nupper right\nlower right/\nupper left\ntotal 44 53 50 53\ntext-curie-001 11 41 30 37\ntext-davinci-003 0 0 0 2\ngpt-3.5-turbo 2 2 3 1\nTable 3: The relation extraction performance of GPT. The\nnumbers in rows 2-4 are incorrect predictions numbers.\nconsistently outperforms the others across all hop levels.\nInterestingly, clean 5shot (1,3,5,7,10) performs better than\nclean 10shot (1∼10) at almost every hop level. This sug-\ngests that selecting examples from a wider range of hop lev-\nels (1,3,5,7,10) can be more beneficial than having an exam-\nple from each hop level from 1 to 10.\nInfluence of Models As indicated in a recent study (Ye\net al. 2023), Turbo demonstrates comparable performance\nto Davinci across many tasks. However, it falls short in the\nmachine reading comprehension, part-of-speech, and rela-\ntion extraction tasks, potentially owing to its smaller model\nsize. The StepGame spatial reasoning task requires the com-\nprehension of sequential spatial connections and the ability\nto draw deductions from them. According to the right sub-\nplot of Figure 4, the Davinci model generally outperforms\nthe Turbo model across varying levels of task complexity\n(number of hops). The differences in performance between\nthe two models are more significant at lower complexity lev-\nels, but they appear to converge as the complexity increases.\nResults of the Improved Methods\nResolution for the Benchmark The results of our resolu-\ntion (sentence-to-relation mapping + ASP-based reasoning)\nare displayed in the ‘Map+ASP’ row of Table 4. The num-\nbers in the table indicate accuracy scores, with higher values\nindicating better performance. This demonstrates the profi-\nciency achieved in spatial relation mapping and multi-hop\nspatial reasoning, all without encountering any errors.\nGPT for Relation Extraction + ASP for Reasoning We\nanalyze the performance of GPT in the relation extraction\nsubtask, as outlined in Table 3. Curie has the highest number\nof wrong predictions across different relations, Davinci and\nTurbo show better performance.\nThe state-of-the-art results achieved by (Yang, Ishay, and\nLee 2023) (using GPT-3 for semantic parsing and ASP for\nreasoning) are presented in the “SOTA” row of Table 4. They\nachieve approximately 90% accuracy for lower hops and\n88.3% accuracy for 10-hop reasoning. They attribute 10.7%\nof the inaccuracies to data-related concerns.\nWe provide an evaluation of their approach on-\nthe corrected dataset, with the results displayed in the\n“Curie+ASP” and “Davinci+ASP” rows. Among the 1000\ntest examples (100 for each k), only 2 errors were encoun-\ntered with Davinci. caused by semantic parsing: the sentence\n“If E is the center of a clock face, H is located between\n2 and 3.” was parsed incorrectly as right(“H”, “E”), but\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18505\nFigure 4: Accuracy comparison for varying numbers of hops (1-10) on the clean test set. On the left, we show the performance\nvariation of the Turbo model with 10shot prompting over different test set sizes (30, 100, and 1000 examples). The middle\nsection illustrates the performance of the Turbo model under three distinct prompting settings: 5shot(1,3,5,7,10), 10shot, and\n5shot separate. The right portion showcases the performance of two models - Davinci and Turbo - using 10shot prompting.\nk=1 k=2 k=3 k=4 k=5 k=6 k=7 k=8 k=9 k=10\nMap+ASP 100 100 100 100 100 100 100 100 100 100\nCurie+ASP 46 43 42 59 67 67 57 56 58 61\nDavinci+ASP 100 100 99 100 100 99 100 100 100 100\nSOTA 92.6 89.9 89.1 93.8 92.9 91.6 91.2 90.4 89.0 88.3\nTurbo\nbase 62 43 30 35 29 25 29 31 16 20\nCoT / 34 40 36 28 28 26 31 25 24\nToT CoT / / 35 35 25 45 15 40 40 35\nDavinci\nbase 77 42 21 26 25 30 23 23 22 22\nCoT / 48 53 46 46 48 40 45 41 32\nToT CoT / / 65 50 45 60 50 50 55 50\nGPT-4\nbase 100 70 55 45 40 25 40 35 35 25\nCoT / 80 75 95 85 85 90 80 60 65\nToT CoT / / 85 85 90 90 85 90 100 95\nTable 4: Accuracy comparison of GPT models on revised\nStepGame using different methods.\nsupposed to be up\nright(“H”, “E”).\nCoT and ToT The experimental results in Table 4 involv-\ning GPT-4 and ToT are based on a test set comprising 20\ninstances considering token usage, while for Davinci and\nTurbo, we used a larger test set of 100 samples. The re-\nsults for the base and CoT methods were obtained using the\n5shot separate prompting on the clean set. All the ToT\nCoT\nresults presented in the table involve the use of GPT-4 for\nbuilding the linking chain, followed by the application of\nTurbo, Davinci, and GPT-4 for CoT reasoning with the con-\nstructed linking chain. The GPT-4 model exhibits superior\nperformance across nearly all settings. With the basic input-\noutput prompt, despite starting at 100% accuracy for k = 1,\nits accuracy dips to 25% fork = 10, indicating that even the\nmost powerful GPT model struggles to maintain accuracy\nas task complexity rises. Humans would probably find this\nchallenging too.\nWith the implementation of our CoT and ToT approach,\nthe GPT-4 model demonstrates significant performance en-\nhancements for more complex tasks (ranging from k = 2 to\nk = 10). Our ToT and CoT method considerably enhances\nthe performance of the Davinci and GPT-4, particularly in\nlarger hops. For the Turbo model, although our CoT method\nbrings improvements ask increases, the gains are not as pro-\nfound as those observed with the Davinci and GPT-4. This\ncould be attributed to the long length of our prompts, requir-\ning a nuanced understanding of coordinates and relations.\nConclusion\nThis paper has introduced a revised version of the StepGame\nbenchmark, correcting template errors that distort model\nperformance evaluations, leading to a more accurate eval-\nuation of the spatial reasoning capabilities of AI systems at-\ntempting the challenge. We highlight Davinci and Turbo’s\nabilities in mapping texts to spatial relations and their lim-\nitations in multi-hop spatial reasoning. Our solution com-\nbines template-to-relation mapping with logic-based reason-\ning, effectively addressing challenges in this task. We also\nenhance LLMs’ spatial reasoning ability through prompt en-\ngineering, using CoT and ToT strategies.\nThis paper focuses on StepGame; future studies could ex-\ntend our findings to other benchmarks. Our methods are suit-\nable for adaptation to various 2D grid-based directional spa-\ntial tasks, such as the bAbI (task 17). This adaptation would\ninvolve customizing the template for the ASP-based solution\nand modifying task descriptions and few-shot examples for\nCoT and ToT approaches. For tasks that require a combina-\ntion of directional, topological, and distance reasoning, like\nSpartQA, it would be necessary to integrate additional rules\nand ontology into both the ASP program and the prompts to\nLLMs for effective solution development.\nThe effective resolution of the StepGame benchmark\nprompts a need for more challenging versions. While hav-\ning a well-defined set of spatial relations converted into nat-\nural language using a set of templates is appealing, it leads\nto controlled natural language which is more amenable to\nspecial purpose reasoning. Finding a way to generate more\nnaturalistic problem statements automatically would there-\nfore be highly desirable. Additionally, the current indepen-\ndent use of LLMs and logic programs suggests a potential\nresearch direction towards integrating these tools for more\ncomprehensive and cohesive problem-solving strategies.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18506\nAcknowledgments\nThis work has been partially supported by Microsoft Re-\nsearch - Accelerating Foundation Models Research pro-\ngram, with the provision of Azure resources to access GPT.\nThis work was also partially supported by the Turing’s\nDefence and Security programme through a partnership\nwith the UK government in accordance with the framework\nagreement between GCHQ and The Alan Turing Institute.\nAuthor Contributions\nAGC and DCH proposed the initial line of work. FL de-\nsigned the actual implementation, performed all the eval-\nuations, and wrote the initial paper draft. DCH and AGC\nsupervised FL. All authors contributed to subsequent paper\nrevisions.\nReferences\nAlomari, M.; Li, F.; Hogg, D. C.; and Cohn, A. G. 2022.\nOnline perceptual learning and natural language acquisition\nfor autonomous robots. Artificial Intelligence, 303: 103637.\nBang, Y .; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\nB.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; et al. 2023. A\nmultitask, multilingual, multimodal evaluation of ChatGPT\non reasoning, hallucination, and interactivity.arXiv preprint\narXiv:2302.04023.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.;\nArora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut,\nA.; Brunskill, E.; et al. 2021. On the opportunities and risks\nof foundation models. arXiv preprint arXiv:2108.07258.\nChen, J.; Cohn, A. G.; Liu, D.; Wang, S.; Ouyang, J.; and\nYu, Q. 2015. A survey of qualitative spatial representations.\nThe Knowledge Engineering Review, 30(1): 106–136.\nCohn, A. G.; and Hazarika, S. M. 2001. Qualitative spatial\nrepresentation and reasoning: An overview.Fundamenta in-\nformaticae, 46(1-2): 1–29.\nCohn, A. G.; and Hernandez-Orallo, J. 2023. Dialectical lan-\nguage model evaluation: An initial appraisal of the common-\nsense spatial reasoning abilities of LLMs. arXiv preprint\narXiv:2304.11164.\nCohn, A. G.; and Renz, J. 2008. Qualitative spatial represen-\ntation and reasoning. Foundations of Artificial Intelligence,\n3: 551–596.\nCreswell, A.; Shanahan, M.; and Higgins, I. 2022.\nSelection-inference: Exploiting large language models\nfor interpretable logical reasoning. arXiv preprint\narXiv:2205.09712.\nKordjamshidi, P.; Moens, M.-F.; and van Otterlo, M. 2010.\nSpatial role labeling: Task definition and annotation scheme.\nIn Proceedings of the Seventh conference on International\nLanguage Resources and Evaluation (LREC’10), 413–420.\nEuropean Language Resources Association (ELRA).\nKordjamshidi, P.; Van Otterlo, M.; and Moens, M.-F. 2011.\nSpatial role labeling: Towards extraction of spatial relations\nfrom natural language. ACM Transactions on Speech and\nLanguage Processing (TSLP), 8(3): 1–36.\nLi, F.; Hogg, D. C.; and Cohn, A. G. 2022. Ontol-\nogy Knowledge-enhanced In-Context Learning for Action-\nEffect Prediction. In Advances in Cognitive Systems. ACS-\n2022.\nMirzaee, R.; and Kordjamshidi, P. 2022. Transfer Learning\nwith Synthetic Corpora for Spatial Role Labeling and Rea-\nsoning. arXiv preprint arXiv:2210.16952.\nMirzaee, R.; and Rajaby, H. 2021. SpartQA: A Tex-\ntual Question Answering Benchmark for Spatial Reason-\ning. In The 2021 Annual Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguis-\ntics (NAACL-2021).\nOpenAI. 2023. GPT-4 Technical Report. ArXiv,\nabs/2303.08774.\nShi, Z.; Zhang, Q.; and Lipani, A. 2022. Stepgame: A new\nbenchmark for robust multi-hop spatial reasoning in texts.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, 11321–11329.\nSkiadopoulos, S.; and Koubarakis, M. 2001. Composing\ncardinal direction relations. In International Symposium on\nSpatial and Temporal Databases, 299–317. Springer.\nWang, L.; Xu, W.; Lan, Y .; Hu, Z.; Lan, Y .; Lee, R. K.-W.;\nand Lim, E.-P. 2023. Plan-and-Solve Prompting: Improving\nZero-Shot Chain-of-Thought Reasoning by Large Language\nModels. arXiv preprint arXiv:2305.04091.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.;\nLe, Q.; and Zhou, D. 2022. Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint\narXiv:2201.11903.\nWeston, J.; Bordes, A.; Chopra, S.; Rush, A. M.;\nVan Merri¨enboer, B.; Joulin, A.; and Mikolov, T. 2015. To-\nwards ai-complete question answering: A set of prerequisite\ntoy tasks. arXiv preprint arXiv:1502.05698.\nYang, Z.; Ishay, A.; and Lee, J. 2023. Coupling\nLarge Language Models with Logic Programming for Ro-\nbust and General Reasoning from Text. arXiv preprint\narXiv:2307.07696.\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao,\nY .; and Narasimhan, K. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv preprint\narXiv:2305.10601.\nYe, J.; Chen, X.; Xu, N.; Zu, C.; Shao, Z.; Liu, S.; Cui, Y .;\nZhou, Z.; Gong, C.; Shen, Y .; et al. 2023. A comprehensive\ncapability analysis of gpt-3 and gpt-3.5 series models. arXiv\npreprint arXiv:2303.10420.\nZhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2022. Auto-\nmatic chain of thought prompting in large language models.\narXiv preprint arXiv:2210.03493.\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q.; et al.\n2022. Least-to-most prompting enables complex reasoning\nin large language models. arXiv preprint arXiv:2205.10625.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18507",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.7787047624588013
    },
    {
      "name": "Spatial intelligence",
      "score": 0.553108811378479
    },
    {
      "name": "Computer science",
      "score": 0.500748872756958
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40137797594070435
    },
    {
      "name": "Natural language processing",
      "score": 0.32289767265319824
    },
    {
      "name": "Geography",
      "score": 0.1425984501838684
    },
    {
      "name": "Cartography",
      "score": 0.13297787308692932
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130828816",
      "name": "University of Leeds",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I125680101",
      "name": "Turing Institute",
      "country": "GB"
    }
  ]
}