{
  "title": "Analyzing the Impact of Large Language Models on Battery Consumption in Mobile Devices: An Empirical Study",
  "url": "https://openalex.org/W4393857595",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Novikau, Anton",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4226278401",
    "https://openalex.org/W6888490834",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4381250586",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "The rise of Large Language Models (LLMs) has led to a significant transformation across various applications, including natural language processing, machine learning, and artificial intelligence.Along with an increasing number of LLMs being launched in the cloud, a significant portion of them are also designed to be embedded in mobile devices.Therefore, their ability to influence battery consumption across mobile devices is going to be crucial.This work aims to empirically investigate the battery consumption of LLMs running on mobile devices across various configurations and runtime scenarios.We methodically measure the energy usage of several popular LLMs, considering factors such as model size and the complexity of the tasks performed.Our methodology involves a series of controlled experiments with mobile devices running these models under standardized conditions to provide a comparative analysis of their energy efficiency.Preliminary results indicate significant variances in battery consumption based on the model's operational parameters and the nature of the tasks executed.The study provides insights into the trade-offs between computational demands of LLMs and battery life, offering guidance for developers and researchers in optimizing LLM implementations for mobile environments.Furthermore, we discuss the implications of our findings for the future design and deployment of energy-efficient LLM applications on mobile devices.This research contributes to the emerging discourse on sustainable AI by highlighting the energy considerations of deploying advanced machine learning models in mobile computing contexts.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6159819960594177
    },
    {
      "name": "Battery (electricity)",
      "score": 0.6131650805473328
    },
    {
      "name": "Consumption (sociology)",
      "score": 0.5312240719795227
    },
    {
      "name": "Empirical research",
      "score": 0.5116518139839172
    },
    {
      "name": "Mobile device",
      "score": 0.4263985753059387
    },
    {
      "name": "Sociology",
      "score": 0.16834506392478943
    },
    {
      "name": "Social science",
      "score": 0.1161273717880249
    },
    {
      "name": "World Wide Web",
      "score": 0.1014864444732666
    },
    {
      "name": "Statistics",
      "score": 0.09265473484992981
    },
    {
      "name": "Mathematics",
      "score": 0.05192434787750244
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}