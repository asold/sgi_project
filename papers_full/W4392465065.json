{
  "title": "A novel Swin transformer approach utilizing residual multi-layer perceptron for diagnosing brain tumors in MRI images",
  "url": "https://openalex.org/W4392465065",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3088506798",
      "name": "Ishak Pacal",
      "affiliations": [
        "Iğdır Üniversitesi"
      ]
    },
    {
      "id": "https://openalex.org/A3088506798",
      "name": "Ishak Pacal",
      "affiliations": [
        "Iğdır Üniversitesi"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2034138274",
    "https://openalex.org/W4211193121",
    "https://openalex.org/W2039199416",
    "https://openalex.org/W3193929510",
    "https://openalex.org/W1969150916",
    "https://openalex.org/W3042445013",
    "https://openalex.org/W4291993480",
    "https://openalex.org/W4319303005",
    "https://openalex.org/W2613696505",
    "https://openalex.org/W2119419684",
    "https://openalex.org/W4289109286",
    "https://openalex.org/W4229375793",
    "https://openalex.org/W4378471231",
    "https://openalex.org/W4285676363",
    "https://openalex.org/W4382931048",
    "https://openalex.org/W4389223917",
    "https://openalex.org/W4309611244",
    "https://openalex.org/W3167977644",
    "https://openalex.org/W2970488713",
    "https://openalex.org/W4220663326",
    "https://openalex.org/W4309905685",
    "https://openalex.org/W3211740276",
    "https://openalex.org/W2955805844",
    "https://openalex.org/W3092530991",
    "https://openalex.org/W3024745283",
    "https://openalex.org/W2799476711",
    "https://openalex.org/W3214185682",
    "https://openalex.org/W4378619666",
    "https://openalex.org/W4309891817",
    "https://openalex.org/W3212769770",
    "https://openalex.org/W4378948997",
    "https://openalex.org/W4323530522",
    "https://openalex.org/W4307112373",
    "https://openalex.org/W4362603432",
    "https://openalex.org/W4286587621",
    "https://openalex.org/W4308327397",
    "https://openalex.org/W4303628775",
    "https://openalex.org/W4322102478",
    "https://openalex.org/W3163071387",
    "https://openalex.org/W4283080861",
    "https://openalex.org/W3121092655",
    "https://openalex.org/W2972838422",
    "https://openalex.org/W3134269260",
    "https://openalex.org/W3027411515",
    "https://openalex.org/W2946489756",
    "https://openalex.org/W3031839920",
    "https://openalex.org/W2988141759",
    "https://openalex.org/W4381093790",
    "https://openalex.org/W3034436104",
    "https://openalex.org/W2765490497",
    "https://openalex.org/W2945839551",
    "https://openalex.org/W2905017682",
    "https://openalex.org/W4387618826",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W6603884005",
    "https://openalex.org/W4312847199",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W4310030439",
    "https://openalex.org/W4318479026",
    "https://openalex.org/W4319303319",
    "https://openalex.org/W4321016247",
    "https://openalex.org/W4367320618",
    "https://openalex.org/W2604272474"
  ],
  "abstract": "Abstract Serious consequences due to brain tumors necessitate a timely and accurate diagnosis. However, obstacles such as suboptimal imaging quality, issues with data integrity, varying tumor types and stages, and potential errors in interpretation hinder the achievement of precise and prompt diagnoses. The rapid identification of brain tumors plays a pivotal role in ensuring patient safety. Deep learning-based systems hold promise in aiding radiologists to make diagnoses swiftly and accurately. In this study, we present an advanced deep learning approach based on the Swin Transformer. The proposed method introduces a novel Hybrid Shifted Windows Multi-Head Self-Attention module (HSW-MSA) along with a rescaled model. This enhancement aims to improve classification accuracy, reduce memory usage, and simplify training complexity. The Residual-based MLP (ResMLP) replaces the traditional MLP in the Swin Transformer, thereby improving accuracy, training speed, and parameter efficiency. We evaluate the Proposed-Swin model on a publicly available brain MRI dataset with four classes, using only test data. Model performance is enhanced through the application of transfer learning and data augmentation techniques for efficient and robust training. The Proposed-Swin model achieves a remarkable accuracy of 99.92%, surpassing previous research and deep learning models. This underscores the effectiveness of the Swin Transformer with HSW-MSA and ResMLP improvements in brain tumor diagnosis. This method introduces an innovative diagnostic approach using HSW-MSA and ResMLP in the Swin Transformer, offering potential support to radiologists in timely and accurate brain tumor diagnosis, ultimately improving patient outcomes and reducing risks.",
  "full_text": "Vol.:(0123456789)\nInternational Journal of Machine Learning and Cybernetics (2024) 15:3579–3597 \nhttps://doi.org/10.1007/s13042-024-02110-w\nORIGINAL ARTICLE\nA novel Swin transformer approach utilizing residual multi‑layer \nperceptron for diagnosing brain tumors in MRI images\nIshak Pacal1 \nReceived: 25 October 2023 / Accepted: 24 January 2024 / Published online: 5 March 2024 \n© The Author(s) 2024\nAbstract\nSerious consequences due to brain tumors necessitate a timely and accurate diagnosis. However, obstacles such as suboptimal \nimaging quality, issues with data integrity, varying tumor types and stages, and potential errors in interpretation hinder the \nachievement of precise and prompt diagnoses. The rapid identification of brain tumors plays a pivotal role in ensuring patient \nsafety. Deep learning-based systems hold promise in aiding radiologists to make diagnoses swiftly and accurately. In this \nstudy, we present an advanced deep learning approach based on the Swin Transformer. The proposed method introduces a \nnovel Hybrid Shifted Windows Multi-Head Self-Attention module (HSW-MSA) along with a rescaled model. This enhance-\nment aims to improve classification accuracy, reduce memory usage, and simplify training complexity. The Residual-based \nMLP (ResMLP) replaces the traditional MLP in the Swin Transformer, thereby improving accuracy, training speed, and \nparameter efficiency. We evaluate the Proposed-Swin model on a publicly available brain MRI dataset with four classes, using \nonly test data. Model performance is enhanced through the application of transfer learning and data augmentation techniques \nfor efficient and robust training. The Proposed-Swin model achieves a remarkable accuracy of 99.92%, surpassing previous \nresearch and deep learning models. This underscores the effectiveness of the Swin Transformer with HSW-MSA and ResMLP \nimprovements in brain tumor diagnosis. This method introduces an innovative diagnostic approach using HSW-MSA and \nResMLP in the Swin Transformer, offering potential support to radiologists in timely and accurate brain tumor diagnosis, \nultimately improving patient outcomes and reducing risks.\nKeywords Brain tumor analysis · Brain tumor classification · Vision transformer · Swin transformer · ResMLP\n1 Introduction\nThe term \"brain tumor\" describes the development of aber -\nrant cells inside the brain or near it. When the tumor origi-\nnates directly in the brain, it is classified as a primary tumor, \nwhereas a secondary tumor refers to cancer cells that have \nspread from another part of the body and migrated to the \nbrain. [1, 2]. There are two types of primary brain tumors: \nbenign and malignant. Malignant tumors are cancerous and \nmore destructive in nature [3]. Brain tumors’ characteristics, \nsuch as their size and location inside the brain, can differ \ngreatly and cause a vast range of symptoms [4, 5].\nEarly brain tumor discovery is essential for successful \ntreatment and management, as uncontrolled tumor growth \ncan reach severe and life-threatening levels, making control \nand treatment more challenging [6]. Therefore, determining \nthe diagnosis and categorization of brain tumors is crucial \nto ensuring the patients’ success. Researchers and scientists \nhave made tremendous progress in creating cutting-edge \ntools for their identification, considering the rising occur -\nrence of brain tumors and their major impact on persons \n[7]. For identifying abnormalities in brain tissues, magnetic \nresonance imaging (MRI) is commonly recognized as the \ngold standard imaging method [8 , 9]. MRI is a useful tool \nfor learning more about the shape, size, and exact location \nof tumors [ 10]. Although early and accurate detection of \nbrain cancers is essential, manually classifying brain tumor \ncan be challenging and time-consuming and mainly relies \non the radiologists’ knowledge [11, 12].\nIn recent years, automated approaches utilizing machine \nlearning algorithms have emerged as valuable tools to assist \nphysicians in brain tumor classification, aiming to stream-\nline the classification process and reduce dependence on \n * Ishak Pacal \n ishak.pacal@igdir.edu.tr\n1 Department of Computer Engineering, Faculty \nof Engineering, Igdir University, 76000 Igdir, Turkey\n3580 International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597\nradiologists [6, 7, 13, 14]. In the field of brain tumor diag-\nnosis, researchers have made significant efforts to reduce \nthe associated morbidity and mortality [11]. Traditionally, \nthe manual detection of brain tumors by radiologists has \nproven to be burdensome because of the numerous images \ninvolved. Computer-aided diagnosis systems (CADx) have \nbecome useful tools for overcoming this difficulty by auto-\nmating and streamlining the diagnostic procedure [15]. Deep \nlearning based CADx systems have exhibited remarkable \nsuccess rates in medical image analysis, cancer diagnosis, \nincluding brain tumors and other cancer types [16– 21]. \nThese systems not only aid in tumor detection and moni-\ntoring but also assist physicians in deciding on things with \nknowledge suitable treatment options, ultimately improving \npatient care [22–24].\nIn CAD applications, deep learning algorithms offer \na more accurate and efficient substitute for conventional \nmachine learning techniques, which mostly depend on \nmanually generated features [7 ]. Classical machine learn-\ning approaches necessitate feature engineering by domain \nexperts and can be time-consuming, especially with large \ndatasets. CNNs have shown outstanding outcomes in the \nprocessing of medical images, including identifying differ -\nent kinds of brain tumors [22, 25–27]. CNNs automatically \nglean pertinent characteristics from images, doing away with \nthe necessity for feature engineering by hand [28–30]. CNNs \nhave shown to be successful at extracting useful information \nfrom medical images, resulting in precise and effective cat-\negorization without the need for manually created features \n[31].\nFurthermore, vision transformers, a distinct architec-\nture from CNNs, have shown encouraging outcomes across \nvarious domains, including brain tumor-related diseased \n[32–34]. Vision transformers make use of the attention \nmechanism to record distant dependencies and relationships \nbetween image patches, enabling them to effectively model \ncomplex visual patterns. This architecture has demonstrated \noutstanding efficiency in applications involving natural lan-\nguage processing, and recently received interest in computer \nvision applications [35, 36]. Considering brain tumors clas-\nsification, vision transformers have exhibited the ability to \nrecord both global and local image characteristics, allow -\ning for more comprehensive and accurate analysis. Their \ncapacity to acquire significant representations directly from \nunprocessed data makes them a compelling alternative for \nmedical image analysis, offering potential advancements \nfor diagnosing of brain tumor [12]. Further exploration and \nevaluation of vision transformers’ capabilities in this domain \nhold significant promise for enhancing brain tumor catego-\nrization systems’ precision and effectiveness.\nDeep learning techniques have significantly contributed \nto the field of brain tumor diagnosis, with notable advance -\nments in tumor detection, classification, and treatment \nplanning [37]. However, there is still a need for continuous \nimprovement in terms of accuracy, efficiency, and accessi-\nbility in brain tumor diagnosis and management. Ongoing \nresearch and innovations hold the promise of revolution-\nizing this field by offering more effective techniques and \ntools for diagnosing of brain tumors, ultimately leading to \nenhanced outcomes for patients. The effectiveness of deep \nlearning methods in diagnosing various types of cancer \nhas served as a driving force for researchers in this area \n[38].\nNumerous research papers in the scientific literature \nfocus on brain tumor diagnosis. Upon analyzing reviews \nand surveys, it becomes evident that deep learning has head \nto several noteworthy findings in the field of brain tumor \ndiagnosis [6 , 22, 39]. The studies state that deep learning \nhas developed into a ground-breaking method with signifi-\ncant and beneficial implications for brain tumor diagnosis. \nDeep learning is a crucial ally in the medical industry since \nbrain tumors’ complexity necessitates accurate and prompt \ndiagnosis. These models may autonomously extract com-\nplex patterns and features suggestive of tumor existence and \ncharacteristics on large datasets of medical data, such as \nMRI. For more exact tumor delineation and more efficient \ntreatment planning, this capability offers accurate tumor \nsegmentation. Deep learning additionally makes it easier to \nclassify tumor kinds and differentiate between benign and \nmalignant tumors, both of which are essential for individu-\nalized therapeutic strategies. Deep learning’s capacity to \nhandle enormous volumes of data with astounding speed \nand accuracy has the potential to increase diagnostic effec-\ntiveness, hasten treatment decisions, and ultimately improve \npatient outcomes. However, to ensure that these AI tools are \nseamlessly incorporated into clinical practice, it is necessary \nfor AI experts and medical professionals to work closely \ntogether in order to instill confidence and interpretability, \nensuring that deep learning is used as a potent decision sup-\nport system rather than in place of medical expertise.\nClassifying brain tumors using deep learning-based meth-\nods presents challenges, including limited labeled data avail-\nability, inter-observer variability in diagnosis, overfitting, \nand the need for interpretability [40]. The scarcity of labeled \ndata necessitates the collection of diverse and well-annotated \ndatasets to improve model performance. Addressing inter-\nobserver variability requires establishing consensus among \nexperts. Techniques like regularization, data augmentation, \nand cross-validation help mitigate overfitting. Furthermore, \ndeveloping interpretable methods, such as attention maps or \nsaliency maps, aids in understanding the reasoning behind \ndeep learning predictions, promoting trust and acceptance \nin the medical community. By addressing these challenges, \ndeep learning models can be more reliable and effective in \nbrain tumor classification, resulting in better patient care and \ndiagnostic accuracy.\n3581International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597 \nThis study presents a novel approach to address challenges \nin brain tumor diagnosis, emphasizing the significance of early \ndetection for successful treatment. By introducing the Swin \nTransformer architecture, the study leverages its success in \nvision tasks and adapts it for brain tumor detection, aiming \nto provide rapid and accurate diagnoses with the aid of deep \nlearning-based CAD systems.\n• We developed a model by scaling the Swin architecture \nbased on the Swin-Base model for a set of 4-class brain \nMRI images. This scaled model provides improved detec-\ntion accuracy with fewer parameters at the same time and \nis shallower than previous Swin models.\n• The proposed model improves the Swin Transformer by \nintroducing the novel Hybrid Shifted Windows Self Atten-\ntion (HSW-MSA) module, enabling better processing of \noverlapping brain MRI regions. This enhancement allows \nthe model to capture fine details and long-range depend-\nencies more effectively, leading to improved accuracy in \ndetecting brain tumors and potentially reducing false nega-\ntives.\n• Furthermore, the paper replaces the Multi-Layer Percep-\ntron (MLP) in the Swin Transformer with a Residual-\nbased MLP (ResMLP). This architectural change results \nin higher accuracy, faster training, and improved parameter \nefficiency. The ResMLP’s ability to extract and represent \nfeatures more efficiently contributes to the exceptional per-\nformance of the Proposed-Swin model on the brain MRI \ndataset.\n• The extensive evaluation demonstrates an outstanding \naccuracy of 99.92% achieved by the Proposed-Swin model, \nsurpassing existing research and deep learning models. \nThis remarkable effectiveness highlights its potential for \npractical application in real-world settings for accurate \nbrain tumor diagnosis.\n• Additionally, we demonstrated the effectiveness of current, \nwell-liked vision transformer models and CNN models \nusing openly accessible MRI datasets to provide a thorough \ncomparison.\nThe study’s design has been enhanced for improved com-\nprehension. A thorough assessment of the literature is pre-\nsented in the second section, and the straightforward technique \nfor simple validation is highlighted in the third. Results and \ndiscussions from the experiment are covered in the fourth part. \nLastly, to help the reader understand the study’s contributions, \nthe conclusion offers a succinct summary of them.\n2  Related works\nImpressive progress has been made by deep learning algo-\nrithms in accurately diagnosing a variety of malignancies, \nwhich has led to substantial improvements the discipline \nof medical imaging. Deep learning approaches have shown \nencouraging results, particularly when used to analyze and \ndiagnose MRI pictures of brain tumors. These approaches \nhave illustrated great levels of precision in accurately iden-\ntifying and categorizing brain tumors, which may lead \nto advancements in patient care and treatment strategy. \nDeep learning’s success in this area has sparked additional \ninvestigation and study with the goal of improving these \nalgorithms’ capabilities and maximizing their potential in \norder to detect brain cancers. The following is a summary \nof several research that have been done and published in \nthe literature on brain tumor detection.\nKumar et al. proposed a deep network model that uses \nResNet50 with pooling techniques in order to overcome \ngradient vanishing and overfitting concerns. The effec -\ntiveness of the model is assessed using simulated studies \non a public MRI dataset with three different tumor types \n[41]. Talukder et al. [13] presented a cutting-edge deep \nlearning method for correctly classifying tumors utilizing \ntransfer learning. The suggested approach entails thorough \npre-processing, reconstruction of transfer learning frame-\nworks, and tweaking. On the 3064 pictures in the Figshare \nMRI brain tumor dataset, various transfer learning tech-\nniques were used and assessed. The suggested framework \nby Rehman et al. [42] includes three experiments that clas-\nsify meningioma, glioma, and pituitary brain cancers using \nseveral CNN architectures. On MRI slices from the brain \ntumor dataset downloaded from Figshare, transfer learn-\ning approaches are used. Increasing dataset size, lower -\ning overfitting risk, and improving generalization are all \nachieved by data augmentation. The best classification and \ndetection accuracy, up to 98.69%, was attained by the fine-\ntuned VGG16 architecture.\nThe approach suggested by Sharif et al. [43] calls for \noptimizing the fine-tnued Densenet201 model and apply -\ning transfer learning on imbalanced data. The average \npool layer, which contains useful information about each \ntype of tumor, is where the features of the trained model \nare retrieved from. But in order to improve the perfor -\nmance of the model for precise classification, two feature \nselection strategies are incorporated. To diagnose glioma \nbrain tumors as low-grade or high-grade utilizing the MRI \nsequence, Mzoughi et al. [44]. presented an automatic \nand effective deep multi-scale 3D CNN architecture. To \nefficiently combine useful contextual information while \nreducing weights, the design uses a 3D convolutional layer \nwith small filters. The suggested classification model by \n3582 International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597\nAmin et al. [26]. has seven layers, including a SoftMax \nlayer, three convolutional layers, and three ReLU acti-\nvation layers. The MRI image is segmented into a few \npatches, and the deep CNN is given the central pixel value \nof each patch. The segmentation of the image is made \npossible by the DNN, based on these center pixels, labels \nare assigned.\nAmin et al. [45] suggested a technique for de-noising \nand enhancing input slices using a Weiner filter with mul-\ntiple wavelet bands. It uses Potential Field (PF) clustering \nto isolate different tumor pixel subsets. Additionally, T2 \nMRI images, global thresholding and mathematical mor -\nphological techniques are used to identify the tumor site. \nFor the purpose of grading and diagnosing brain tumors \nusing MRI data, Tandel et al. [46] provided five clinically \nrelevant multiclass datasets with various numbers of classes. \nIn comparison to six existing machine learning classification \napproaches makes use of transfer learning using a CNN. On \nmultiple datasets of MR images, a deep CNN model that was \ninitially pre-trained as a discriminator in a GAN. The pre-\ntraining aids in the extraction of robust features and teaches \nthe convolutional layers of the algorithm the structure of MR \nimages. The entire deep model is then retrained as a clas-\nsifier to differentiate between tumor classes once the fully \nconnected layers are changed [47].\nTo categorize brain cancers in MRI data, Tabatabaei et al. \n[48] developed a hybrid model that integrates CNN with \nattention module. By taking into account both local and \nglobal variables, they developed a cross-fusion technique to \nmerge the branches, boosting classification accuracy. The \nmany types of brain tumors can be accurately identified by \nthis hybrid architecture. An optimized ResNet model with \na cutting-edge evolutionary approach was introduced by \nMehnatkesh et al. [33]. This method automatically improves \nthe deep ResNet model’s architecture and hyperparameters \nwithout the need for human specialists or manual archi-\ntecture design, making it appropriate for classifying brain \ntumors. The research also introduces an improved optimiza-\ntion method that incorporates ideas from differential evolu-\ntion strategy and multi-population operators. Deep CNN, a \nDolphin-SCA based deep learning technique for enhanced \naccuracy and efficient classification, was introduced by \nKumar et al. [49] pre-processing the raw MRI images is the \nfirst step in the procedure, which is then segmented using an \nimproved algorithm. Then, feature extraction is carried out \nusing statistical and power LDP features.\nAn automated approach for differentiating between \nmalignant and non-cancerous brain MRIs was proposed by \nAmin et al. [50] The technique uses a variety of techniques \nto divide up potential lesions before choosing shape, tex-\nture, and intensity-based attributes for each lesion. Then, \nin order to compare the proposed model’s precision, a \nSVM classifier is applied. Swati et al. [51] proposed a \ndeep CNN model that has already been trained is used, \nand a block-by-block fine-tuning approach based on trans-\nfer learning is suggested. A benchmark dataset MRI is \nutilized to evaluate the strategy’s efficacy. As a result of \navoiding handcrafted features, requiring no preprocess-\ning, and achieving an average accuracy of 94.82%, the \nmethod is notably more general. A CNN-based method \nfor classifying multi-grade brain tumors was introduced \nby Sajjad et al. [52] First, tumor areas from MR images \nare segmented using deep learning approaches. Second, a \nsignificant amount of data augmentation is used to effec-\ntively train the system, addressing the issue of a shortage \nof data in the categorization of multi-grade brain tumors \nfrom MRI. Finally, supplemented data is used to improve \na pre-trained CNN model for precise brain tumor grade \nclassification. Deepak and Ameer [23] developed a 3-class \nclassification issue incorporating them. The proposed clas-\nsification system uses transfer learning utilizing Goog-\nLeNet. The collected features are subsequently classified \nusing integrated, tested classifier models.\nTo enhance the accuracy and efficacy of MRI data-\ndriven diagnoses, it is evident from the summaries of \nresearch papers that there is a growing interest in explor -\ning deep neural networks for brain tumor-related studies. \nChallenges such as vanishing gradient, overfitting, imbal-\nanced data, and data heterogeneity have been effectively \naddressed using various strategies. Modifying well-known \nmodels like ResNet, VGG16, and Densenet201 for brain \ntumor classification through transfer learning consistently \nyields high accuracy. Increasing dataset sizes, improving \ngeneralization, and mitigating overfitting concerns have \nbeen achieved through the application of data augmenta-\ntion techniques. Additionally, some studies focus on 3D \nCNN architectures to extract both local and global con-\ntextual information from volumetric MRI data, leading \nto more precise tumor grade classification. Image quality \nand feature extraction have been enhanced by employing \npreprocessing techniques like denoising and contrast aug-\nmentation. Various feature selection methods, including \nwavelet transforms, local binary patterns, and statistical \nfeatures, have been integrated to boost the effectiveness \nof deep learning models.\nOverall, the research outlined in these publications \nunderscores the continual improvement in brain tumor \ncategorization, emphasizing deep learning approaches and \noptimizing model architectures. These innovative methods \nhold significant promise for enhancing the sensitivity and \naccuracy of brain tumor diagnoses, ultimately benefiting \npatients and medical professionals. To ascertain the appli-\ncability and generalizability of these proposed approaches, \nfurther clinical research and validation may be necessary.\n3583International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597 \n3  Material and methods\nIn this study, we introduce a cutting-edge deep learning \nmodel to diagnose brain tumors. A broad collection of brain \nMRI scans, a comprehensive dataset comprising three pub-\nlicly available datasets, that have been painstakingly col -\nlected from various medical institutes make up the dataset \nused for training and evaluation. Our deep learning system \nmakes use of cutting-edge vision transformer architecture, \nwhich has distinguished itself in tasks requiring picture \nunderstanding. The proposed method effectively detects and \ncategorizes brain cancers with high sensitivity and speci-\nficity by fusing the strength of the vision transformer with \nsophisticated data augmentation and transfer learning strat-\negies. To assure reproducibility and encourage additional \nstudy for other cancer-related diseases, complete implemen-\ntation and training methods are described.\n3.1  Dataset\nDue to their capability to learn and generalize on vast data, \ndeep learning models are becoming more and more popular. \nHowever, the size and quality of the training dataset strongly \ninfluence the effectiveness of these data-hungry models. The \ndataset is crucial in deep learning as it provides the neces-\nsary examples for the models to recognize and generalize \npatterns effectively. The model can extract pertinent features \nand make precise predictions on unobserved data with a siz-\nable and representative dataset. Ensuring high-quality data is \nessential to address biases, reduce overfitting or underfitting \nissues, and improve performance across different subsets. \nFor the autonomous classification of low-grade brain MRI \nimages, several publicly available datasets exist, including \nFigshare [53], SARTAJ [54], and Br35H [55], which are \nknown to be small-scale datasets. However, in this study, we \nutilized a publicly accessible brain MRI dataset shared on \nKaggle [56], which combines and incorporates these three \ndatasets to reveal the true capabilities of deep learning mod-\nels on this task. Sample images from this dataset depicting \nboth tumor and healthy cases are illustrated in Fig. 1.\nThe brain MRI dataset utilized for this study has been \ndivided into four major tumor classes: no-tumor, glioma, \nmeningioma, and pituitary. Malignant brain tumors include \ngliomas, which have an aggressive development tendency. \nOn the other hand, meningioma tumor is a benign tumor that \ngrows in the meninges of the brain and can go undetected for \na long time without exhibiting any clear symptoms. Pituitary \ntumors are a specific kind of tumor that develop in the pitui-\ntary gland and can cause hormonal abnormalities. The No-\ntumor class, which represents healthy brain circumstances, \nis also a crucial point of reference for control groups. Utiliz-\ning this extensive and varied dataset, we evaluated the deep \nlearning model’s capacity to correctly categorize each tumor \ntype and investigated its potential as a trustworthy tool for \nbrain tumor diagnosis.\nFig.1  Visual depiction of samples in the brain MRI dataset across no-tumor, glioma, meningioma, and pituitary classes\n3584 International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597\n3.2  Vision transformer\nArtificial intelligence has had a big impact on deep learn-\ning, particularly in computer vision applications like face \nrecognition, medical picture analysis, and autonomous \ndriving. CNNs, which are specifically engineered to han -\ndle visual input, have been instrumental in this revolution. \nConvolutional filtering and pooling techniques allow CNNs \nto reduce dimensionality and recognize a variety of picture \nattributes. CNNs are not without limitations either, particu-\nlarly when it comes to understanding relations within an \nimage and gathering global information. Researchers have \ncreated vision transformers as a solution to this [57, 58]. \nVision transformers leverage self-attention approaches to \ncapture long-range relationships in raw data, allowing them \nto outperform CNNs for visual scenarios.\nUnlike CNNs, vision transformers use positional embed-\ndings and self-attention instead of convolutional layers. They \nare able to record both local and global information in visual \nsceneries because to this special technique, which makes \nthem suitable for tasks requiring a thorough knowledge of \nimages. Recent research has demonstrated the high perfor -\nmance that vision transformers may accomplish in a range of \nuses for visual tasks. A key development in the disciplines of \ncomputer vision and deep learning is the creation of vision \ntransformers [59]. While CNNs remain the architecture of \nchoice for many artificial intelligence usages, vision trans-\nformers offer an additional tactic that is highly effective in \nobtaining global as well as local data.\n3.3  Swin transformer\nThe Swin Transformer, developed by Microsoft Research \nin 2021, is an impressive AI model designed for computer \nvision [60, 61]. It builds upon the Transformer model and \nintroduces two key concepts—hierarchical feature maps \nand shifted window attention. These advancements help \nefficiently handle large-scale image data, making it a prom-\nising tool for complex computer vision tasks. The Swin \nTransformer utilizes hierarchical feature maps to effectively \nrepresent different levels of features in images, leading to \na comprehensive understanding of context and improved \ncomprehension of input data. The shifted window atten-\ntion mechanism expands the interaction field of each block, \nenabling the model to capture variable-scale features more \neffectively.\nThe Swin Transformer’s four-stage architecture involves \ndividing the input image into patch layers, which are processed \nthrough Transformer blocks in the backbone. The resulting \npatches are sent to the transition block, maintaining the same \nnumber of patches. In the second stage, patch merging layers \nare used to create a hierarchical system by subsampling and \nreducing the number of tokens. Neighboring 2 × 2 patches’ \nfeatures are combined to obtain a 4C-dimensional feature vec-\ntor, which is transformed using linear layers while preserv -\ning a resolution of H/8 × W/8. This patch merging and fea -\nture transformation process is repeated twice in subsequent \nstages, resulting in output resolutions of H/16 × W/16 and \nH/32 × W/32, respectively. Overall, this architecture enables \nthe Swin Transformer to effectively process image data and \ncapture contextual information at different scales, contributing \nto its superior performance in various vision tasks.\nThe Swin Transformer Blocks (STBs) provided in Figs. 2 \nand 3 consist of two consecutive multi-head self-attention \n(MSA) modules: window-based MSA (W-MSA) and shifted \nwindow-based MSA (SW-MSA). Before each of these MSA \nmodules, a Layer Norm (LN) layer is used. Next, there is a \ntwo-layer MLP (multilayer perceptron) with GELU non-line-\narity in between. Each module has a link with the LN layer. In \nEqs. 1 and 2, MSA has a quadratic computational complexity \nwith respect to the number of tokens. This configuration signif-\nicantly improves the performance of the Swin Transformer and \nmakes it more efficient compared to the standard Transformer.\nWhere the first part exhibits a quadratic relationship with \nrespect to the patch number, denoted as hw, whereas the sec-\nond part demonstrates a linear dependency when the value of \nM is constant (typically set to 7 by default). Computing global \nself-attention becomes prohibitively expensive for a high value \nof hw, whereas window-based self-attention is scalable.\nIn the consecutive STBs, a shifted window partitioning \napproach is adopted to switch between two configurations. \nThis approach utilizes overlapping windows to introduce \ncross-window connections while efficiently calculating non-\noverlapping windows. In the first module, a regular window \npartitioning strategy is used, and an 8  × 8 feature map is \ndivided into 2 × 2 windows of size 4  × 4 (M = 4). Then, the \nsecond module provides a window configuration by shifting \nthe windows by \n/parenleft.s2/uni230A.s2\nM\n2\n/uni230B.s2\n,\n/uni230A.s2\nM\n2\n/uni230B.s2/parenright.s2\n pixels from the previously par-\ntitioned windows. The Transformer blocks are computed in \nEq. 3\nwhere zl and /uni0302.s1zl represent the output features for block l from \nthe (S)W-MSA module and the MLP module, respectively . \nW-MSA and SW-MSA refer to window-based multi-head \n(1)Ω(MSA ) = 4hwC2 + 2(hw)2C\n(2)Ω(W − MSA) = 4hwC2 + 2M 2hwC\n(3)\n̂zl = W − MSA /parenleft.s1LN/parenleft.s1zl−1 /parenright.s1/parenright.s1+ zl−1 ,\nzl = MPL\n/parenleft.s1\nLN\n/parenleft.s1\n̂zl/parenright.s1/parenright.s1\n+ ̂zl,\n̂zl+1 = SW − MSA /parenleft.s1LN/parenleft.s1zl/parenright.s1/parenright.s1+ zl,\nzl = MPL /parenleft.s1LN/parenleft.s1̂zl+1 /parenright.s1/parenright.s1+ ̂zl+1 .\n3585International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597 \nself-attention with standard and shifted window partition-\ning configurations, respectively.\nThe Swin Transformer adopts a specialized architecture \nto enhance computational efficiency compared to traditional \nTransformer models. It achieves this by using a cyclic shift-\ning operation between shifted token blocks (STBs). This \noperation divides the feature map pixels into regional \nblocks and cyclically shifts each block to the previous one. \nFig. 2  The general structure of the Proposed-Swin transformer architecture for brain tumor diagnosis\nFig. 3  The overall structure \nof default Swin Transformer \nblocks and Proposed-Swin \nTransformer blocks\n\n3586 International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597\nAs a result, each block can operate with masks applied to a \nsection of the feature map. This approach allows the Swin \nTransformer to process smaller blocks of data instead of the \nentire feature map at once, leading to more efficient feature \nextraction and preventing computational overhead in sliding \nwindows.\nThe Swin Transformer utilizes a self-attention mechanism \nthat incorporates relative positional bias to capture relation-\nships between positions. The attention function involves \nmapping queries (Q), keys (K), and values (V) to output vec-\ntors. For each query in the Q  matrix, attention weights are \ncalculated for corresponding key-value pairs. The resulting \noutput matrix is obtained through this computation process, \nwhich is formulates in Eq. 4\nWhere the query (Q),  key (K),  and value (V)  matrices \nare of size RM 2xd , where d represents the dimension of the \nquery/key vectors, and M2 is the number of patches in a \nwindow. In Swin Transformer, relative positions are defined \nalong each axis within the range [− M + 1, M − 1]. The rela-\ntive positional bias is parameterized as an offset matrix /uni0302.s1B ∈ \nR(2M −1)x(2M −1) , and the elements of matrix B are obtained \nfrom /uni0302.s1B.\nThe fundamental model of Swin Transformer is referred \nto as Swin-B. Swin-B has a comparable model size and \ncomputational complexity to ViT-B/DeiT-B. Similarly, the \nSwin-T and Swin-S models are designed to have compu-\ntational complexities comparable to ResNet-50 (DeiT-S) \nand ResNet-101, respectively. The dimensions of Swin \nTransformer models can vary depending on various factors, \nincluding the channel size of the initial feature map (C), the \nlayer size of the Swin Transformer block, the window size, \nand the expansion factor of the MLP layer.\n3.4  Proposed model\nThe proposed approach aims to develop a classification \nmodel based on the Swin Transformer for brain MRI images. \nIt seeks to achieve high classification accuracy and address \nchallenges related to distinguishing between similar lesion \ntypes and accurately identifying common ones. The pro-\nposed approach presents innovative enhancements to the \nSwin Transformer model for brain tumor diagnosis. Four \nessential elements make up the proposed approach for classi-\nfying brain tumors using the Swin Transformer architecture: \nscaling the model for 4-class classification of brain tumors, \nincorporating the Residual MLP module, incorporating \nhybrid shifted windows into the self-attention mechanism, \nand using transfer learning with data augmentation. Like \nother deep learning architectures, the Swin Transformer \n(4)Attention(Q, K , V ) = SoftMax\n�\nQK T\n√\nd\n+ B\n�\nV\nneeds to have its design and parameters scaled in order to \naccommodate a variety of workloads and dataset sizes. Vari-\nables like model size, stage depth, and embedding dimen -\nsions can all help achieve this. For example, larger variations \nof the Swin Transformer, such Swin-Base, and Swin-Large, \nwhich are made for datasets like ImageNet with 1000 \nclasses, offer improved capacity suitable for handling more \ndifficult tasks and bigger datasets. Swin-Small, Swin-Tiny \nmodel, on the other hand, produce more useful outcomes \nin scenarios with fewer classes while using less resources \nfor simpler tasks. The overall design of the Proposed-Swin \nTransformer model for detecting brain tumors is illustrated \nin Fig. 2.\nIn this work, the configuration of the Swin-Base model \nwith “ Embedding Dimension  = 128”, “depths  = (2, 2, 18, \n2)”, and “Number of Heads = (4, 8, 16, 32)” was changed to \n“Embedding Dimension = 96”, “depths = (2, 2, 4, 2)”, and \n“Number of Heads = (3, 6, 12)”, leading to a more adaptable \nmodel in terms of computation, convergence speed, and cost. \nNotably, the increased depth in the third step of the initial \nconfiguration aligns with the Swin Transformer’s hierarchi-\ncal approach and tries to capture complicated and high-level \ninformation. The proposed model (scaled model), with its \nintegrated components, performs more effectively than other \nmodels in the context of classifying brain tumors.\nBy introducing residual connections into the MLP mod-\nules, the model benefits from smoother transitions between \nlayers and improved gradient flow, facilitating the training \nof deeper models and achieving superior results. Moreo-\nver, the integration of hybrid shifted windows into the self-\nattention modules enables the model to process images at \nvarious scales and positions, leading to comprehensive fea-\nture extraction and more robust representations. By combin-\ning these learning approaches, the proposed method shows \npromise in creating a more comprehensive and powerful \nclassification model for brain tumor diseases, ultimately \nleading to more accurate and reliable outcomes in brain \ntumor diagnosis and treatment.\n3.4.1  Hybrid multi self‑attention module\nThe Swin-based models consist of two different multi-head \nself-attention layers, W-MSA and SW-MSA. In the proposed \nmodel, Hybrid Swin Transformer blocks were introduced, \nemploying a hybrid shifted window approach. This novel \ntechnique divides the input image into smaller patches and \napplies attention mechanisms to each patch, capturing rela-\ntionships between features in different patches and preserv-\ning the overall context. By considering relationships among \nvarious parts of the input image, the network can maintain \na broader perspective. The Swin-Tiny model, developed \nwith hybrid transformer blocks, incorporates a hybrid self-\nattention module that combines traditional shifted windows \n3587International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597 \nwith elongated rectangular shapes in horizontal and verti-\ncal directions. Unlike conventional transformer blocks, \nwhich use fully connected self-attention mechanisms, this \nhybrid module allows the model to flexibly capture informa-\ntion from windows of various sizes, addressing long-range \ndependencies while preserving local and detailed informa-\ntion. The ability to handle images at different scales and \norientations enhances the model’s applicability and reduces \ngeneralization issues, potentially leading to improved per -\nformance in challenging image analysis tasks such as brain \ntumor detection and other medical images. Figure  3 illus-\ntrates the pure Swin Transformer block alongside the hybrid \ntransformer blocks used in the proposed model.\nThe Hybrid transformer blocks in Fig.  3 consist of two \nself-attention modules. While the first layer of this structure \nremains the same as the layer in the original Swin Trans-\nformer, a more efficient layer is obtained by adding the \nhybrid layer, Hybrid SW-MSA, to the secondary layer, the \nSW-MSA layer. The HSW-MSA layer combines three dif-\nferent sliding window processes to enhance visual informa-\ntion exchange at various scales. In the first part, a SW-MSA \nmodule is applied for local pattern capture. In the second \npart, the input image is divided into horizontal and verti-\ncal stripe windows, enabling longer-range connections and \nbroader context. This approach enriches the HSW-MSA’s \nmultiple heads, facilitating comprehensive visual informa-\ntion exchange. It is particularly useful for improving perfor-\nmance in visual processing applications. The computation of \nhybrid Transformer blocks involves the sequential applica-\ntion of these two self-attentions is formulated in the Eq.  5.\nwhere zl and /uni0302.s1zl represent the output features for block l from \nthe (S) HSW-MSA module and the Res-MLP module, respec-\ntively. W-MSA and HSW-MSA refer to window-based multi-\nhead self-attention with hybrid shifted window partitioning \nconfigurations, respectively.\n3.4.2  Residual multilayer perceptron module (Res‑MLP)\nMLPs, short for multi-layer perceptron, are fundamental \ncomponents in the standard transformer architecture [62]. \nTypically, a transformer includes two main building blocks: \nthe self-attention mechanism and the MLP block. While the \nself-attention mechanism captures relationships between dif-\nferent tokens (or patches in image transformers), the MLP \nblock processes information individually for each token. \nIn the Swin Transformer architecture, depicted in Fig.  4, \nthe MLPs are similar to those found in other transformer \nstructures. However, instead of using a regular MLP, we \nintroduced a Residual MLP module, inspired by the ResNet \narchitecture [63] and ResMLP architecture [62], which has \ngained popularity recently. The proposed Res-MLP struc-\nture, a crucial component of the Proposed-Swin Transformer \nframework, is depicted in Fig. 4.\n(5)\n̂zl = W − MSA /parenleft.s1LN/parenleft.s1zl−1/parenright.s1/parenright.s1+ zl−1,\nzl = Re sMPL/parenleft.s1LN/parenleft.s1̂zl/parenright.s1/parenright.s1+ ̂zl,\n̂zl+1 = HSW − MSA /parenleft.s1LN/parenleft.s1zl/parenright.s1/parenright.s1+ zl,\nzl = Re sMPL/parenleft.s1LN/parenleft.s1̂zl+1/parenright.s1/parenright.s1+ ̂zl+1,\nFig. 4  Structure of the proposed Res-MLP module with the default MLP module in the Swin transformer model\n3588 International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597\nThe Swin Transformer leverages residual connections \nwithin the MLP blocks to address the vanishing gradient \nissue, allowing stable and efficient training of deep archi-\ntectures. The ability to skip uninformative layers through \nresidual connections enhances the model’s capacity to learn \ncomplex representations and handle challenging tasks effec-\ntively. Moreover, the Res-MLP design not only improves \nexpressiveness but also enhances generalization capabili-\nties. The Swin Transformer’s ability to capture non-linear \nrelationships between features makes it more adaptable to \ndiverse and complex datasets. The residual connections pro-\nvide resilience to changes in hyperparameter selections and \narchitectural configurations, facilitating the model develop-\nment process and supporting faster experimentation. Experi-\nmental results have shown that with these enhancements, \nthe Swin Transformer converges faster on brain tumor data \nand achieves higher accuracy. As seen in Fig.  4, by adding \nResidual layers to the MLP structure, more effective train-\ning and stronger generalization capabilities were achieved.\n4  Results and discussions\n4.1  Experimental design\nA Linux machine running the Ubuntu 22.04 operating sys-\ntem was used for this study. On an impressively powerful \nhigh-performance computer, the deep neural network mod-\nels were developed and evaluated. This PC featured a 13th \ngeneration Intel Core i5 with an NVIDIA RTX 3090 GPU \nwith 24GB of GDDR6X memory, along with 32 GB of \nDDR5 RAM. The most recent stable PyTorch framework \nwith NVIDIA CUDA support was used for the experiments. \nIn the same computing environment, each model was trained \nand tested, which ensured consistency by using the same \nparameters throughout.\n4.2  Data processing and transfer learning\nFor deep learning algorithms, medical images need to be \nproduced on an appropriate foundation. Data sets are fre-\nquently split into cross-validation, train-validation, or train-\ntest sets in the literature. However, few studies actually \nevaluate the real performance of deep learning algorithms \nusing the appropriate data split of train, validation, and test \nsets. To use the best data separation technique for assessing \nthe performance of deep learning models, we divided the \ndata set in our study into three separate subsets: training, \nvalidation, and testing. To assess the model’s effectiveness \nand lower the chance of overfitting, this division is required.\nWe employed a Kaggle data set that was available in sepa-\nrate train and test sets and was open to the public [56]. To \nguarantee an equitable comparison of our proposed model \nwith others, we employed 80% of the training data for the \nactual training process and reserved the remaining 20% for \nvalidation purposes. The test data set was left untouched for \nfair comparison. Table  1 displays the data distribution for \neach class in the Kaggle dataset.\nTable 1 summarizes the distribution of MRI dataset, cat-\negorizing samples into Glioma-tumor, Meningioma-tumor, \nPituitary-tumor, and No-tumor classes. The dataset is split \ninto three sets: Train, Validation, and Test. The dataset \nconsists of 7023 samples in total, with the No-tumor class \nhaving the highest number of samples (2000) and the Gli-\noma-tumor class having the fewest (1621). This dataset is \nessential for training and evaluating classification models.\nIn this study, we improved the robustness and generaliza-\ntion of our models using data augmentation techniques [52]. \nData augmentation involved applying various transforma-\ntions to the original images, creating new synthetic exam-\nples, and reducing overfitting risk. We specifically applied \ncropping, flipping, rotation, copy-paste, shear, and scaling \nduring model training, effectively expanding the dataset and \nenhancing its ability to generalize to unseen samples. This \naugmentation aimed to enhance the accuracy and reliability \nof our models in identifying brain tumors, ultimately con-\ntributing to more efficient screening and diagnostics. The \nImageNet dataset’s pre-trained weights were also used in \ntransfer learning by [64–66], leveraging the model’s learned \nknowledge and representations from millions of diverse \nimages. Fine-tuning the pre-trained model using our brain \nMRI dataset further improved its performance, saving train-\ning time, fast convergence, and computational resources.\n4.3  Performance metrics\nPerformance metrics are of utmost importance when evalu-\nating the effectiveness and generalizability of deep learning \nalgorithms. They serve as essential tools in the field, ena-\nbling the assessment of models throughout training and on \nvalidation and test datasets. By utilizing these metrics, one \ncan identify overfitting issues, gauge the effects of param-\neter adjustments, and gain a comprehensive understanding \nof the model’s overall performance. Accuracy, precision, \nand recall are among the widely used performance metrics \nin deep learning, as presented in Table 2.\nTable 1  Class-wise distribution of the brain MRI dataset\nClass name Train Validation Test Total\nGlioma-tumor 1057 264 300 1,621\nMeningioma-tumor 1072 267 306 1,645\nPituitary-tumor 1166 291 300 1,757\nNo-tumor 1276 319 405 2,000\nTotal 4571 1141 1,311 7,023\n3589International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597 \nFalse positive (FP) denotes inaccurate positive estima-\ntions, false negative (FN) denotes inaccurate negative pre-\ndictions, and true negative (TN) denotes accurate negative \npredictions. True positive (TP) denotes accurate positive \npredictions. The effectiveness of binary classification mod-\nels is evaluated using these metrics. Precision calculates \nthe percentage of accurate positive forecasts to all positive \npredictions, whereas accuracy evaluates the ratio of correct \npredictions to total predictions. On the other hand, recall \nquantifies the ratio of correctly predicted positive cases to \nall actual positive examples. By determining their harmonic \nmean, the F1 score strikes a compromise between precision \nand recall, ensuring a thorough assessment of model perfor-\nmance. Each metric adheres to the following mathematical \nformula.\n4.4  Training procedure\nThe performance of deep learning models could be enhanced \nby using a variety of methods and settings while they are \nbeing trained. Data augmentation and transfer learning \nare two efficient techniques. Furthermore, several hyper-\nparameters play a substantial effect in shaping a model’s \nperformance. These parameters include input size, type \nof optimizer, size of batch, learning rate, and repetition of \naugmentation. Adjusting the learning rate has the potential \nto impact the model’s effectiveness, whereas weight decay \nserves as a preventive measure against overfitting by impos-\ning penalties on substantial weights within the loss func-\ntion. Adjusting warmup epochs and learning rate gradually \nincreases the learning rate during initial epochs to avoid \ndivergence during training.\nIn our research, we adopted a multifaceted approach to \nensure the reproducibility and performance enhancement of \nboth the proposed model and other deep learning models. \nThe implemented techniques encompassed crucial aspects \nsuch as hyperparameter tuning, data preprocessing, transfer \nlearning, and data augmentation. Fundamental hyperpa-\nrameters, including input size, learning rates, momentum, \nweight decay, and optimizer selection, were consistently \nfine-tuned across all models, using default values to establish \na standardized foundation for model training. This meticu-\nlous parameter application aimed to foster reliability and \ncomparability in our experimental results. Additionally, \ndata-related hyperparameters such as scale, ratio, Mixup \nprobability, and others were carefully adjusted, introduc-\ning variability and sensitivity to diverse datasets, thereby \nenhancing the overall robustness of our models.\nIn a different vein, our approach involved tailoring spe-\ncific hyperparameter values for each model to ensure optimal \ntraining conditions. For example, the input size, determining \nthe dimensions of training images, was set at 224 × 224 (or \n256 × 256 for certain models like SwinV2). The learning \nrate, a pivotal parameter influencing the model’s learning \npace, was initialized at 0.00001. Essential parameters like \ninitial learning rate (lr_base), learning rate cycle decay (lr_\ncycle_decay), and exponential moving average (EMA) decay \nfor weights (model_ema_decay) were meticulously config-\nured. The lr_base was specifically set to 0.1 as an effec-\ntive starting point for learning rates. The lr_cycle_decay, \nindicating the factor by which the learning rate decreases \nafter each training cycle, was adjusted to 0.5 for a balanced \nconvergence and stability. The model_ema_decay, govern-\ning the update speed of weights using the EMA method, was \nselected as 0.9998 for a gradual and consistent adjustment.\nPractical considerations such as momentum (0.9) in the \nStochastic Gradient Descent (SGD) optimizer, weight decay \n(2.0e−05) to control overfitting, warm-up epochs (5) for a \ngradual learning rate increase at the start of training, and \nwarm-up learning rate (1.0e−05) were incorporated. These \nhyperparameter values were meticulously chosen to strike \na delicate balance between model training efficiency, sta-\nbility, and robustness. The amalgamation of these finely \ntuned hyperparameter configurations played a pivotal role \nin achieving optimal model performance while maintain-\ning consistency across experiments. Additionally, specific \ndata-related hyperparameters, such as scale, ratio, Mixup \nprobability, and others, were fine-tuned to ensure model sen-\nsitivity to various data characteristics.\nIn deep learning models, the issues of overfitting and \nunderfitting often adversely affect the model’s generaliza-\ntion capability and can lead to incorrect biases. Overfitting \noccurs when the model excessively fits the training data and \nfails to generalize to test data, while underfitting arises when \nthe model inadequately fits the training data, resulting in \npoor performance on test data. To address both problems \ncollectively, we divided our dataset into three parts: training, \nvalidation, and test. We evaluated the model’s generalization \nperformance solely on the test data. The training process was \nmonitored for 50 epochs, and if significant improvement did \nnot occur during this period, the training was stopped. This \napproach contributes to preventing overfitting and avoid-\ning unnecessary training of the model. Additionally, due \nto the limited scale of MRI data, we attempted to enhance \nthe model’s performance using transfer learning and data \naugmentation techniques. These strategies proved helpful \nin achieving better generalization with a constrained dataset.\nTable 2  Performance metrics Metric Formula\nAccuracy TP+TN\nTP+TN+FP+FN\nPrecision TP\nTP+FP\nRecall TP\nTP+FN\nF1-score 2 × Precision×recall\nPrecision+recall\n3590 International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597\nFurthermore, to tackle these issues, regularization tech-\nniques such as dropout and weight regularization were \napplied to all models during the training of both the base-\nline models and the Proposed-Swin model. Dropout reduces \noverfitting by randomly disabling neurons during training, \nwhile weight regularization helps prevent excessively large \nweights. Default hyperparameters were set for all models to \nachieve a balance between model complexity and dataset \nsize. On the other hand, underfitting is typically a problem \nobserved in large-scale datasets, but it is not the case with \nour MRI dataset. To address this issue, the complexity of \nthe Proposed-Swin model’s architecture, HSW-MSA, and \nResMLP improvements were leveraged. These components \nenhance the model’s ability to capture complex patterns in \nMRI data, thereby improving diagnostic accuracy.\n4.5  Results\nThe experimental findings of the proposed approach are \npresented in this part together with those of several popular \nCNN models and the most recent and extensively used vision \ntransformer models that can be found in the literature. The \nexperimental evaluations for each model were conducted \nexclusively on unseen data, specifically the test data that had \nbeen previously set aside. Testing the models on unseen data \nis the optimal choice as it demonstrates deep learning’s gen-\neralization capabilities and their applicability in real-world \nscenarios. Table 3 presents the experimental results on the \nBrain MRI dataset for the Proposed-Swin model compared \nto cutting-edge CNNs and vision transformer-based models.\nConsidering Table  3, a comparative analysis of experi-\nmental results on the brain MRI dataset reveals that the mod-\nels exhibit exceptional performance in accurately classifying \nbrain MRI images. All models demonstrate diagnostic accu-\nracy above 98%, and when the ResNet50 model is excluded, \nit becomes evident that all other models achieve diagnostic \naccuracy well above 99%. Taking Table 3 into account, the \nProposed-Swin model stands out by showcasing the high -\nest performance, reaching 99.92% accuracy and F1-score, \ndemonstrating a significant superiority over other models.\nThe Proposed-Swin model enhances its performance in \nbrain tumor classification tasks through the integration of \nHSW-MSA and ResMLP structures into its architecture. \nHSW-MSA provides a structure that improves attention \nmechanisms and better understands distance relationships \nbetween features. This allows the model to adapt better to \nthe complexity of objects and learn more general features. \nAdditionally, the ResMLP structure, when used instead of \ntraditional convolutional MLP structures, effectively focuses \non both large and small features in MRI images, helping \nthe model learn more comprehensive features. These two \nstructures play a critical role in enabling the Proposed-\nSwin model to achieve high accuracy, precision, recall, and \nF1-score values. As a result, the model excels in brain tumor \nclassification tasks, offering a more effective solution com-\npared to other architectures.\nAmong other models, following Proposed-Swin in \nterms of the highest performance are BeiT-Base and Mobi-\nleViTv2-150 models. BeiT-Base achieves an accuracy of \n0.9954 and an F1-score of 0.9950, while MobileViTv2-150 \nsimilarly exhibits high performance with an accuracy \nof 0.9954 and an F1-score of 0.9952. On the other hand, \nmodels with the lowest performance include ResNet50 and \nVGG16, with accuracy and F1-score values as follows: \nResNet50 (Accuracy: 0.9893, F1-score: 0.9886) and VGG16 \n(Accuracy: 0.9924, F1-score: 0.9917). These evaluations \nunderscore the outstanding performance of Proposed-Swin, \nand its more effective solution compared to other models.\nThe comparative analysis also highlights the significance \nof considering precision, recall, and F1-score alongside \naccuracy to evaluate model performance comprehensively. \nModels like “MobileNetv3-Small” and “MobileViT-Small” \nexhibited remarkable precision and recall values, indicating \ntheir proficiency in correctly identifying positive samples \nwhile minimizing false positives and negatives. Such high \nF1-scores, coupled with competitive accuracy, are indicative \nof robust models with balanced performance. Additionally, \nit is crucial to weigh the computational efficiency of each \nmodel, especially when deploying applications in real-world \nsettings. Models like Proposed-Swin with their exceptional \nperformance and computational efficiency, hold promise for \nTable 3  Experimental results on brain MRI dataset\nModel Accuracy Precision Recall F1-score\nVGG16 [67] 0.9924 0.9921 0.9917 0.9917\nResNet50 [63] 0.9893 0.9887 0.9886 0.9886\nEfficientNetv2-Medium \n[68]\n0.9924 0.9919 0.9917 0.9917\nMobileNetv3-Small [69] 0.9939 0.9936 0.9934 0.9935\nMobileViT-Small [70] 0.9947 0.9942 0.9942 0.9942\nMobileViTv2-150 [71] 0.9954 0.9953 0.9950 0.9952\nMaxViT-Base [72] 0.9931 0.9926 0.9927 0.9927\nDeiT-Base [73] 0.9947 0.9943 0.9942 0.9942\nDeiT3-Base [74] 0.9924 0.9919 0.9919 0.9919\nViT-Base-Patch32 [57] 0.9939 0.9934 0.9934 0.9934\nBeiT-Base [75] 0.9954 0.9951 0.9950 0.9950\nConViT-Base [76] 0.9931 0.9928 0.9925 0.9926\nTwins-Svt-Base [77] 0.9924 0.9918 0.9924 0.9921\nPiT-Base [78] 0.9947 0.9943 0.9942 0.9942\nSwin-Tiny [60] 0.9931 0.9927 0.9925 0.9926\nSwinv2-Window16-Tiny \n[61]\n0.9939 0.9935 0.9933 0.9934\nGcViT-Base [79] 0.9947 0.9944 0.9942 0.9942\nProposed-Swin 0.9992 0.9992 0.9992 0.9992\n3591International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597 \npractical implementation in medical imaging and diagnostic \nsystems. Among the models evaluated, the Proposed-Swin \nmodel stands out with exceptional results, achieving an out-\nstanding metrics of 0.9992. This demonstrates the Proposed-\nSwin model is highly effective in accurately classifying brain \nMRI images, making it a promising candidate for real-world \nclinical applications.\nComparing the Proposed-Swin model with the other \nmodels, we can observe that it outperforms almost all of \nthem in all metrics. For instance, the widely used VGG16 \nand ResNet50 models achieved an accuracy of 0.9924 and \n0.9893, respectively, which are slightly lower than the Pro-\nposed-Swin model. Similarly, other state-of-the-art models, \nsuch as EfficientNetv2-Medium, MobileNetv3-Small, and \nDeiT3-Base, demonstrated competitive performances but \nwere still outperformed by the Proposed-Swin model in \nterms of all metrics.\nMoreover, the Proposed-Swin model ‘s excellent results \neven surpass more complex models like ViT-Base-Patch32, \nPiT-Base, and GcViT-Base, which suggests that the model’s \ndesign and architecture are well-suited for the brain MRI \nclassification task. It is important to point out that the Pro-\nposed-Swin model’s exceptional performance comes with \nthe added advantage of being computationally efficient and \nlightweight. This characteristic makes it highly applica-\nble in resource-constrained environments, such as mobile \napplications or edge devices, without sacrificing predictive \naccuracy. The confusion matrix for a few Swin-based trans-\nformer model as well as a few other cutting-edge deep learn-\ning models with Proposed-Swin is shown in Fig. 5.\nConsidering Fig.  5 all models demonstrated high accu-\nracy to diagnose brain tumors. The Proposed-Swin, Swin-\nTiny, DeiT3-Base, and GcViT-Base models showcased \nconsistent and impressive results, with minimal misclas -\nsifications. The ResNet50 and ConViT-Base models also \nperformed well, albeit with slightly higher misclassification \nrates. MobileViT-Small and BeiT-Base models exhibited \nstrong performances but showed a few more errors com-\npared to the top-performing models. The performance of the \nProposed-Swin model in classifying brain tumor images into \nfour categories was outstanding, with almost all predictions \nbeing accurate. Only one misclassification was observed, \nwhere a sample from the Pituitary class was mistakenly pre-\ndicted as Meningioma.\nUpon observation, the No-tumor class emerges as the \nmost successfully diagnosed class across all models, with \nmeningioma having higher FP values and varying FN val-\nues among the models. While ResNet50 exhibits the lowest \nclass-specific performance, the Proposed Model consistently \ndemonstrates the highest performance across all classes. Fig-\nure 6 provides a detailed comparison of all models based \non the accuracy metric in a single line graph. As seen in \nFig. 6, the most successful model is the Proposed Model \n(Proposed-Swin), followed by MobileViTv2-150, BeiT-\nBase, MobileViT-Small, DeiT-Base, with ResNet50 being \nthe least performing model. Notably, the current model, \nDeiT3, shows lower performance compared to its predeces-\nsor, the DeiT model. This underscores the variability in per-\nformance that each model can exhibit on medical datasets.\n4.6  Efficiency of the proposed‑Swin model and Swin \ntransformer variants\nIn this section, we embark on a comprehensive compari-\nson between the Proposed Model and the Swin Transformer \narchitecture, both of which hold significant importance in \nthe field of deep learning. Our analysis encompasses an \nextensive range of model variants, ranging from the com-\npact Tiny and Small models to the more substantial Base and \nLarge models. Furthermore, we delve into the exploration \nof the SwinV2 Transformer, an evolved version of the Swin \nTransformer that introduces varying window sizes, present-\ning new opportunities for fine-tuning and optimization. To \nensure a rigorous evaluation, we utilize the test data from the \nBrain MRI dataset. Table 4 showcases a detailed comparison \nof these models.\nTable 4 analysis reveals that all Swin-based transformer \nmodels achieve an accuracy rate of over 99.30% in correctly \ndiagnosing brain MRI images. A noteworthy distinction is \nthat the Proposed-Swin model beats existing tiny models in \nterms of accuracy and other metrics while showing compara-\nble convergence speed and parameter count. The Proposed-\nSwin model achieves a much greater accuracy of 0.9992 \nwhen compared to the Swin-Tiny model, which achieves an \naccuracy of 0.9931, demonstrating a significant advantage.\nThe Proposed-Swin model performs better even when \ncompared to the Swin-Small model. The accuracy of the \nSwin-Small model is 0.9939, whereas that of the Proposed-\nSwin model is 0.9992. Similar to this, the Proposed-Swin \nmodel still triumphs when compared to the Swin-Base \nmodel. The accuracy of the Swin-Base model is 0.9954, \nwhile that of the Proposed-Swin model is 0.9992. Addition-\nally, the Proposed-Swin model still has an edge over the \nSwin-Large model. The accuracy of the Swin-Large model is \n0.9947, whereas that of the Proposed-Swin model is 0.9992. \nThe Proposed-Swin model additionally routinely exhibits \nimproved accuracy and superior performance across a vari-\nety of parameters when compared to existing Swinv2-based \nmodels. For devices with lesser computational and memory \nrequirements, it is a superior option because it can achieve \ngreater performance with fewer settings. Figure  7 provides \na detailed comparison of Swinv1 and Swinv2 models in a \nsingle line graph based on the accuracy metric. As seen in \nFig. 7, the most successful model is the Proposed Model \n(Proposed-Swin), followed by the Swinv2-Window8-Tiny, \nSwinv2-Window8-Small, and Swin-Base models, while the \n3592 International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597\nSwin model with the lowest performance is the Swin-Tiny \nmodel with an accuracy of 699.31. In general, Swin-based \nmodels exhibit high accuracy in diagnosing brain tumors, \nranging from 99.31 to 99.92%.\nWhen assessing complexity, the Proposed-Swin model \nfocuses on the HSW-MSA block and the scaled Swin-Base \nmodel. The Swin-Base model has 88 million parameters, \nwhile the scaled version, with 24 million parameters, is even \nlighter than the Swin-Tiny model (29M). The HSW-MSA \nlayer, a key parameter influencer, increases model param-\neters by 10% when replacing the SW-MSA block. However, \noverall scaling and ResMLP module make the model lighter \nthan Swin-Tiny and less complex in layer count. The HSW-\nMSA layer stands out by seamlessly combining three types \nof shifted windows. It strategically allocates 50% attention \nto traditional shifted windows and distributes the remaining \n25% to horizontal and vertical stripe windows. This inten-\ntional partitioning allows the model to capture local and \nglobal relationships, along with direction dependencies in \nboth horizontal and vertical axes. This enhances feature rep-\nresentation, demonstrating improved performance in medi-\ncal image processing, particularly in exploring brain tumor \nfeatures and achieving better diagnostic accuracy.\n4.7  Comparison with cutting‑edge methods\nThe rapid advancements in computer vision techniques and \nmedical imaging present new and significant opportunities \nFig. 5  Comparing confusion matrices: proposed-Swin model vs. some deep learning models\n3593International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597 \nfor the effective classification of brain MRI images. In this \ncontext, to assess the performance of our proposed model, \nwe conducted a comprehensive comparison with current \ncutting-edge methods. Specifically, we focused on some \nmethods that demonstrate superior success in the diagno-\nsis of brain tumors, as highlighted in Table  5, showcasing \nthe superior performance of our proposed model over other \nstate-of-the-art methods.\nTable 5 provides a comprehensive comparison of state-\nof-the-art models in the domain of brain MRI image \nclassification, particularly focusing on the vital task of \ndiagnosing brain tumors. Amidst the array of methodologies \npresented by different studies, Proposed-Swin (ViT) stands \nout as a pinnacle of performance in detecting brain abnor -\nmalities. The convergence of advanced computer vision \ntechniques and medical imaging is strikingly evident in the \nexceptional accuracy of Proposed-Swin (ViT) on Kaggle’s \ndataset, reaching an impressive 99.92%. This achievement \nnot only underscores the potential of the Swin-Based (ViT) \narchitecture in elevating the precision of brain tumor iden-\ntification but also positions it as a frontrunner in the field.\nAs we navigate through the intricate landscape of brain \nMRI classification, the diverse array of models in Table  5 \nreveals nuanced insights. CNN-based approaches, exem-\nplified by Talukder et al., 2023 [ 13] and Tabatabaei et al., \n2023 [48] on the Figshare dataset with accuracies of 99.68% \nand 99.30%, respectively, demonstrate noteworthy efficacy. \nOn the Kaggle dataset, alongside Proposed-Swin (ViT), \nother CNN-based models such as Rahman and Islam [82], \nMuezzinoglu et al. [83], and Ali et al. [84] also exhibit high \naccuracy rates. However, it is essential to highlight that \nProposed-Swin (ViT) not only surpasses these CNN-based \nmodels but excels as a benchmark for superior performance \nin brain tumor classification. Additionally, the comparative \nanalysis underscores the diversity in model performances \nand signifies the evolving landscape of methodologies in \nadvancing the accuracy of brain MRI-based diagnostics.\nFig. 6  Comparative analysis of accuracy across all deep learning models used in experiments\nTable 4  Experimental results of the Swin-based models\nModel Accuracy Precision Recall F1-score\nSwin-Tiny 0.9931 0.9927 0.9925 0.9926\nSwin-small 0.9939 0.9935 0.9933 0.9934\nSwin-base 0.9954 0.9951 0.9950 0.9950\nSwin-large 0.9947 0.9944 0.9942 0.9942\nSwinv2-Window8-Tiny 0.9962 0.9961 0.9959 0.9960\nSwinv2-Window16-Tiny 0.9939 0.9935 0.9933 0.9934\nSwinv2-Window8-Small 0.9954 0.9952 0.995 0.9951\nSwinv2-Window16-Small 0.9947 0.9942 0.9942 0.9942\nSwinv2-Window8-Base 0.9947 0.9944 0.9942 0.9942\nSwinv2-Window16-Base 0.9947 0.9943 0.9942 0.9942\nSwinv2-Window12-Large 0.9954 0.9953 0.995 0.9952\nProposed-Swin 0.9992 0.9992 0.9992 0.9992\n3594 International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597\n4.8  Limitations and future directions\nThis study introduces an advanced deep learning approach \nbased on the Swin Transformer, but it comes with certain \nlimitations. Among these limitations, the primary and most \nsignificant is the evaluation of the Proposed-Swin model’s \nperformance on a brain MRI dataset composed of a com-\nbination of a few datasets due to the scarcity of publicly \navailable datasets. Additionally, the limitation stems from \nthe relatively small scale of the dataset for deep learning \nmodels. Assessing the model’s generalizability across dif-\nferent datasets, imaging characteristics, patient populations, \nand tumor types is challenging. Essentially, further research \nis needed to explore the effectiveness of the model in differ-\nent datasets and clinical settings.\nA second limitation is the lack of comprehensive clinical \nstudies that validate the real-world clinical applicability of \nthe model’s success. The model’s performance needs veri-\nfication through studies involving various healthcare insti-\ntutions, encompassing clinical variability, patient-specific \nfactors, and the presence of rare tumor types. Furthermore, \nthere is a limitation related to the tendency of deep learning \nmodels to lack interpretability. Understanding the decision-\nmaking process of the model is crucial for gaining trust from \nhealthcare professionals.\nAmong the future directions of this study, the first and \nforemost is the multi-center validation on different datasets \nobtained from various healthcare institutions to enhance \nthe Swin Model’s performance and generalizability. This \nmulti-center validation is crucial for evaluating the model’s \nperformance across different imaging protocols and patient \ndemographics. Additionally, planned studies aim to demon-\nstrate the model’s performance on different medical images. \nOptimizing the Swin Model for real-time applications is also \na significant future direction. Improving the model’s archi-\ntecture and efficient inference strategies are essential for pro-\nviding timely and on-site diagnostic support to radiologists.\n5  Conclusion\nThis study introduces a groundbreaking deep learning \nmethod using the Swin Transformer for precise brain tumor \ndiagnosis. Addressing challenges like suboptimal imaging \nand diverse tumor types, we incorporated the HSW-MSA \nand ResMLP. Our Proposed-Swin model achieved an out-\nstanding 99.92% accuracy on a brain MRI dataset, surpass-\ning prior models. The Swin Transformer, enhanced by HSW-\nMSA and ResMLP, proves effective in improving accuracy \nand efficiency. Transfer learning and data augmentation bol-\nstered model robustness. Substituting ResMLP for the tradi-\ntional MLP not only enhanced accuracy but also improved \ntraining speed and parameter efficiency.\nFig. 7  Comparative analysis of accuracy across Swin transformer model\n3595International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597 \nThe significance of our findings lies in the potential \nsupport our method can provide to radiologists in mak -\ning accurate and timely diagnoses, ultimately leading to \nimproved patient outcomes and reduced risks associated \nwith brain tumors. The innovative diagnostic approach \nintroduced in this study, incorporating HSW-MSA and \nResMLP in the Swin Transformer, represents a valuable \ncontribution to the field of medical imaging and deep \nlearning applications. As we move forward, further vali-\ndation on diverse datasets and real-world clinical settings \nwill be essential to establish the generalizability and reli-\nability of the Proposed-Swin model. Nevertheless, our \nstudy lays a foundation for future research and develop -\nments in leveraging deep learning techniques for enhanc-\ning the diagnostic capabilities in neuroimaging, with the \nultimate goal of improving patient care and outcomes in \nthe realm of brain tumor diagnosis.\nAuthor contributions Ishak Pacal is solely responsible for all aspects \nof this work, including conceptualization, methodology design, data \ncollection, analysis, and the creation of visual aids. He authored the \nentirety of the manuscript, meticulously reviewed and edited it for clar-\nity, and granted final approval for its publication.\nFunding Open access funding provided by the Scientific and Techno-\nlogical Research Council of Türkiye (TÜBİTAK). The authors state no \nfinancial support was received for this manuscript.\nData availability MRI dataset can be accessed in Kaggle “https:// www. \nkaggle. com/ datas ets/ masou dnick parvar/ brain- tumor- mri- datas et”.\nDeclarations \nConflict of interest No competing interests declared.\nEthical approval No ethics approval was required for this work as it \ndid not involve human subjects, animals, or sensitive data that would \nnecessitate ethical review.\nConsent to participate No formal consent to participate was required \nfor this work as it did not involve interactions with human subjects or \nthe collection of sensitive personal information.\nConsent to publish This study did not use individual person’s data.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n 1. Bondy ML, Scheurer ME, Malmer B et al (2008) Brain tumor \nepidemiology: consensus from the Brain Tumor Epidemiology \nConsortium. Cancer 113:1953–1968\n 2. Herholz K, Langen KJ, Schiepers C, Mountz JM (2012) Brain \ntumors. Semin Nucl Med 42:356–370. https:// doi. org/ 10. 1053/j. \nsemnu clmed. 2012. 06. 001\n 3. Ostrom QT, Barnholtz-Sloan JS (2011) Current state of our \nknowledge on brain tumor epidemiology. Curr Neurol Neurosci \nRep 11:329–335. https:// doi. org/ 10. 1007/ s11910- 011- 0189-8\n 4. Miller KD, Ostrom QT, Kruchko C et al (2021) Brain and other \ncentral nervous system tumor statistics, 2021. CA Cancer J Clin \n71:381–406. https:// doi. org/ 10. 3322/ caac. 21693\n 5. Charles NA, Holland EC, Gilbertson R et al (2011) The brain \ntumor microenvironment. Glia 59:1169–1180. https:// doi. org/ 10. \n1002/ glia. 21136\n 6. Liu Z, Tong L, Chen L et al (2023) Deep learning based brain \ntumor segmentation: a survey. Complex Intell Syst 9:1001–1026. \nhttps:// doi. org/ 10. 1007/ s40747- 022- 00815-5\n 7. Jyothi P, Singh AR (2023) Deep learning models and traditional \nautomated techniques for brain tumor segmentation in MRI: a \nreview. Artif Intell Rev 56:2923–2969. https:// doi. org/ 10. 1007/ \ns10462- 022- 10245-x\n 8. Solanki S, Singh UP, Chouhan SS, Jain S (2023) Brain tumor \ndetection and classification using intelligence techniques: an over-\nview. IEEE Access 11:12870–12886\nTable 5  Proposed Model versus cutting-edge methods (Kaggle data-\nset consists of figshare, SARTAJ dataset, Br35H)\nAuthor and year Dataset Method Accuracy %\nTalukder et al., 2023 \n[13]\nFigshare CNN-based 99.68\nTabatabaei et al., 2023 \n[48]\nFigshare CNN + Attention 99.30\nDeepak and Ameer, \n2023 [80]\nFigshare CNN + SVM 95.60\nZulfiqar et al., 2023 [32] Figshare CNN-based 98.86\nGhassemi et al., 2020 \n[47]\nFigshare CNN + GAN 95.60\nMehnatkesh et al., 2023 \n[33]\nFigshare CNN-based 98.69\nSwati et al., 2019 [51] Figshare CNN-based 94.82\nSajjad et al., 2019 [52] Figshare CNN-based 90.67\nRehman et al., 2020 [42] Figshare CNN-based 98.69\nKumar et al., 2021 [41] Figshare CNN-based 97.48\nMzoughi et al., 2020 \n[44]\nBraTS CNN-based 96.49\nSharif et al., 2022 [43] BraTS CNN-based 98.80\nOzkaraca et al., 2023 \n[81]\nKaggle CNN-based 96.00\nRahman and Islam, 2023 \n[82]\nKaggle CNN-based 98.12\nMuezzinoglu et al., 2023 \n[83]\nKaggle CNN-based 98.10\nAli et al., 2023 [84] Kaggle CNN-based 95.70\nProposed-Swin Kaggle Swin-based (ViT) 99.92\n3596 International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597\n 9. Villanueva-Meyer JE, Mabray MC, Cha S (2017) Current clinical \nbrain tumor imaging. Clin Neurosurg 81:397–415. https:// doi. org/ \n10. 1093/ neuros/ nyx103\n 10. Ellingson BM, Wen PY, Van Den Bent MJ, Cloughesy TF (2014) \nPros and cons of current brain tumor imaging. Neuro Oncol \n16:vii2–vii11. https:// doi. org/ 10. 1093/ neuonc/ nou224\n 11. Xie Y, Zaccagna F, Rundo L et al (2022) Convolutional neural \nnetwork techniques for brain tumor classification (from 2015 to \n2022): review, challenges, and future perspectives. Diagnostics \n12:1850\n 12. Ali S, Li J, Pei Y et al (2022) A comprehensive survey on brain \ntumor diagnosis using deep learning and emerging hybrid tech-\nniques with multi-modal MR image. Arch Comput Methods Eng \n29:4871–4896\n 13. Talukder MA, Islam MM, Uddin MA et al (2023) An efficient \ndeep learning model to categorize brain tumor using reconstruc-\ntion and fine-tuning. Expert Syst Appl. https:// doi. org/ 10. 1016/j. \neswa. 2023. 120534\n 14. Rajeev SK, Pallikonda Rajasekaran M, Vishnuvarthanan G, \nArunprasath T (2022) A biologically-inspired hybrid deep \nlearning approach for brain tumor classification from magnetic \nresonance imaging using improved gabor wavelet transform \nand Elmann-BiLSTM network. Biomed Signal Process Control. \nhttps:// doi. org/ 10. 1016/j. bspc. 2022. 103949\n 15. Pacal I, Kılıcarslan S (2023) Deep learning-based approaches \nfor robust classification of cervical cancer. Neural Comput \nAppl. https:// doi. org/ 10. 1007/ s00521- 023- 08757-w\n 16. Coşkun D, Karaboğa D, Baştürk A et al (2023) A compara-\ntive study of YOLO models and a transformer-based YOLOv5 \nmodel for mass detection in mammograms. Turk J Electr Eng \nComput Sci 31:1294–1313. https:// doi. org/ 10. 55730/ 1300-  \n0632. 4048\n 17. Wang W, Pei Y, Wang SH et al (2023) PSTCNN: explainable \nCOVID-19 diagnosis using PSO-guided self-tuning CNN. Biocell \n47:373–384. https:// doi. org/ 10. 32604/ bioce ll. 2023. 025905\n 18. Pacal I, Karaboga D (2021) A robust real-time deep learning based \nautomatic polyp detection system. Comput Biol Med. https:// doi. \norg/ 10. 1016/j. compb iomed. 2021. 104519\n 19. Zhang Y-D, Govindaraj VV, Tang C et al (2019) High perfor -\nmance multiple sclerosis classification by data augmentation and \nAlexNet transfer learning model. J Med Imaging Health Inform \n9:2012–2021. https:// doi. org/ 10. 1166/ JMIHI. 2019. 2692\n 20. Wang W, Zhang X, Wang SH, Zhang YD (2022) COVID-19 diag-\nnosis by WE-SAJ. Syst Sci Control Eng 10:325–335. https:// doi. \norg/ 10. 1080/ 21642 583. 2022. 20456 45\n 21. Pacal I (2022) Deep learning approaches for classification of \nbreast cancer in ultrasound (US) images. J Inst Sci Technol. \nhttps:// doi. org/ 10. 21597/ jist. 11836 79\n 22. Amin J, Sharif M, Haldorai A et al (2022) Brain tumor detection \nand classification using machine learning: a comprehensive sur -\nvey. Complex Intell Syst 8:3161–3183. https:// doi. org/ 10. 1007/ \ns40747- 021- 00563-y\n 23. Deepak S, Ameer PM (2019) Brain tumor classification using \ndeep CNN features via transfer learning. Comput Biol Med. \nhttps:// doi. org/ 10. 1016/j. compb iomed. 2019. 103345\n 24. Wang SH, Govindaraj VV, Górriz JM et al (2021) Covid-19 clas-\nsification by FGCNet with deep feature fusion from graph convo-\nlutional network and convolutional neural network. Inform Fusion \n67:208–229. https:// doi. org/ 10. 1016/j. inffus. 2020. 10. 004\n 25. Chahal PK, Pandey S, Goel S (2020) A survey on brain tumor \ndetection techniques for MR images. Multimed Tools Appl \n79:21771–21814. https:// doi. org/ 10. 1007/ s11042- 020- 08898-3\n 26. Amin J, Sharif M, Yasmin M, Fernandes SL (2018) Big data \nanalysis for brain tumor detection: deep convolutional neural net-\nworks. Futur Gener Comput Syst 87:290–297. https:// doi. org/ 10. \n1016/j. future. 2018. 04. 065\n 27. Esmaeili M, Vettukattil R, Banitalebi H et al (2021) Explainable \nartificial intelligence for human-machine interaction in brain \ntumor localization. J Pers Med. https:// doi. org/ 10. 3390/ jpm11 \n111213\n 28. Zhang Y, Deng L, Zhu H et al (2023) Deep learning in food cat-\negory recognition. Inform Fusion. https:// doi. org/ 10. 1016/j. inffus. \n2023. 101859\n 29. Karaman A, Karaboga D, Pacal I et al (2022) Hyper-parameter \noptimization of deep learning architectures using artificial bee \ncolony (ABC) algorithm for high performance real-time automatic \ncolorectal cancer (CRC) polyp detection. Appl Intell. https:// doi. \norg/ 10. 1007/ s10489- 022- 04299-1\n 30. Pacal I, Karaman A, Karaboga D et al (2022) An efficient real-\ntime colonic polyp detection with YOLO algorithms trained by \nusing negative samples and large datasets. Comput Biol Med. \nhttps:// doi. org/ 10. 1016/J. COMPB IOMED. 2021. 105031\n 31. Pacal I, Alaftekin M (2023) Türk İşaret Dilinin Sınıflandırılması \niçin Derin Öğrenme Yaklaşımları. Iğdır Üniversitesi Fen Bilimleri \nEnstitüsü Dergisi 13:760–777. https:// doi. org/ 10. 21597/ jist. 12234 \n57\n 32. Zulfiqar F, Ijaz Bajwa U, Mehmood Y (2023) Multi-class classifi-\ncation of brain tumor types from MR images using EfficientNets. \nBiomed Signal Process Control. https:// doi. org/ 10. 1016/j. bspc. \n2023. 104777\n 33. Mehnatkesh H, Jalali SMJ, Khosravi A, Nahavandi S (2023) An \nintelligent driven deep residual learning framework for brain \ntumor classification using MRI images. Expert Syst Appl. https:// \ndoi. org/ 10. 1016/j. eswa. 2022. 119087\n 34. Shamshad F, Khan S, Zamir SW et al (2023) Transformers in \nmedical imaging: a survey. Med Image Anal 88:102802\n 35. Akinyelu AA, Zaccagna F, Grist JT et al (2022) Brain tumor diag-\nnosis using machine learning, convolutional neural networks, cap-\nsule neural networks and vision transformers, applied to MRI: a \nsurvey. J Imaging 8:205\n 36. Celard P, Iglesias EL, Sorribes-Fdez JM et al (2023) A survey \non deep learning applied to medical images: from simple artifi -\ncial neural networks to generative models. Neural Comput Appl \n35:2291–2323\n 37. Tummala S, Kadry S, Bukhari SAC, Rauf HT (2022) Classifica-\ntion of brain tumor from magnetic resonance imaging using vision \ntransformers ensembling. Curr Oncol 29:7498–7511. https:// doi. \norg/ 10. 3390/ curro ncol2 91005 90\n 38. Karaman A, Pacal I, Basturk A et al (2023) Robust real-time polyp \ndetection system design based on YOLO algorithms by optimiz-\ning activation functions and hyper-parameters with artificial bee \ncolony (ABC). Expert Syst Appl. https:// doi. org/ 10. 1016/j. eswa. \n2023. 119741\n 39. Nazir M, Shakil S, Khurshid K (2021) Role of deep learning in \nbrain tumor detection and classification (2015 to 2020): a review. \nComput Med Imaging Graph. https://  doi. org/ 10. 1016/j. compm \nedimag. 2021. 101940\n 40. Jiang Y, Zhang Y, Lin X et al (2022) SwinBTS: a method for 3D \nmultimodal brain tumor segmentation using Swin transformer. \nBrain Sci. https:// doi. org/ 10. 3390/ brain sci12 060797\n 41. Kumar RL, Kakarla J, Isunuri BV, Singh M (2021) Multi-class \nbrain tumor classification using residual network and global aver-\nage pooling. Multimed Tools Appl 80:13429–13438. https:// doi. \norg/ 10. 1007/ s11042- 020- 10335-4\n 42. Rehman A, Naz S, Razzak MI et al (2020) A deep learning-based \nframework for automatic brain tumors classification using transfer \nlearning. Circuits Syst Signal Process 39:757–775. https:// doi. org/ \n10. 1007/ s00034- 019- 01246-3\n 43. Sharif MI, Khan MA, Alhussein M et al (2022) A decision sup-\nport system for multimodal brain tumor classification using deep \nlearning. Complex Intell Syst 8:3007–3020. https:// doi. org/ 10.  \n1007/ s40747- 021- 00321-0\n3597International Journal of Machine Learning and Cybernetics (2024) 15:3579–3597 \n 44. Mzoughi H, Njeh I, Wali A et al (2020) Deep multi-scale 3D \nconvolutional neural network (CNN) for MRI gliomas brain tumor \nclassification. J Digit Imaging 33:903–915. https:// doi. org/ 10.  \n1007/ s10278- 020- 00347-9\n 45. Amin J, Sharif M, Raza M et al (2019) Brain tumor detection \nusing statistical and machine learning method. Comput Methods \nPrograms Biomed 177:69–79. https:// doi. org/ 10. 1016/j. cmpb. \n2019. 05. 015\n 46. Tandel GS, Balestrieri A, Jujaray T et al (2020) Multiclass mag-\nnetic resonance imaging brain tumor classification using artifi-\ncial intelligence paradigm. Comput Biol Med. https:// doi. org/ 10. \n1016/j. compb iomed. 2020. 103804\n 47. Ghassemi N, Shoeibi A, Rouhani M (2020) Deep neural network \nwith generative adversarial networks pre-training for brain tumor \nclassification based on MR images. Biomed Signal Process Con-\ntrol. https:// doi. org/ 10. 1016/j. bspc. 2019. 101678\n 48. Tabatabaei S, Rezaee K, Zhu M (2023) Attention transformer \nmechanism and fusion-based deep learning architecture for MRI \nbrain tumor classification system. Biomed Signal Process Control. \nhttps:// doi. org/ 10. 1016/j. bspc. 2023. 105119\n 49. Kumar S, Mankame DP (2020) Optimization driven deep convolution \nneural network for brain tumor classification. Biocybern Biomed Eng \n40:1190–1204. https:// doi. org/ 10. 1016/j. bbe. 2020. 05. 009\n 50. Amin J, Sharif M, Yasmin M, Fernandes SL (2020) A distinctive \napproach in brain tumor detection and classification using MRI. \nPattern Recognit Lett 139:118–127. https:// doi. org/ 10. 1016/j. \npatrec. 2017. 10. 036\n 51. Swati ZNK, Zhao Q, Kabir M et al (2019) Brain tumor classi-\nfication for MR images using transfer learning and fine-tuning. \nComput Med Imaging Graph 75:34–46. https:// doi. org/ 10. 1016/j. \ncompm edimag. 2019. 05. 001\n 52. Sajjad M, Khan S, Muhammad K et al (2019) Multi-grade brain tumor \nclassification using deep CNN with extensive data augmentation. J \nComput Sci 30:174–182. https:// doi. org/ 10. 1016/j. jocs. 2018. 12. 003\n 53. Brain tumor dataset. https:// figsh are. com/ artic les/ datas et/ brain_ \ntumor_ datas et/ 15124 27. Accessed 30 Jul 2023\n 54. Brain Tumor Classification (MRI) | Kaggle. https:// www. kag-\ngle. com/ datas ets/ sarta jbhuv aji/ brain- tumor- class ifica tion- mri. \nAccessed 30 Jul 2023\n 55. Br35H :: Brain Tumor Detection 2020 | Kaggle. https:// www. kag-\ngle. com/ datas ets/ ahmed hamad a0/ brain- tumor- detec tion? select= \nno. Accessed 30 Jul 2023\n 56. Brain Tumor MRI Dataset | Kaggle. https:// www. kaggle. com/  \ndatas ets/ masou dnick parvar/ brain- tumor- mri- datas et? select= Train \ning. Accessed 30 Jul 2023\n 57. Dosovitskiy A, Beyer L, Kolesnikov A et al (2020) An image \nis Worth 16 × 16 words: transformers for image recognition at \nscale. In: ICLR 2021—9th International Conference on Learning \nRepresentations\n 58. Pacal I (2024) Enhancing crop productivity and sustainability \nthrough disease identification in maize leaves: exploiting a large \ndataset with an advanced vision transformer model. Expert Syst \nAppl. https:// doi. org/ 10. 1016/j. eswa. 2023. 122099\n 59. Khan S, Naseer M, Hayat M et al (2021) Transformers in vision: \na survey. ACM Comput Surv. https:// doi. org/ 10. 1145/ 35052 44\n 60. Liu Z, Lin Y, Cao Y, et al (2021) Swin transformer: hierarchical \nvision transformer using shifted windows\n 61. Liu Z, Hu H, Lin Y, et al (2021) Swin transformer V2: scaling up \ncapacity and resolution\n 62. Touvron H, Bojanowski P, Caron M, et al (2021) ResMLP: feed-\nforward networks for image classification with data-efficient \ntraining\n 63. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for \nimage recognition. In: Proceedings of the IEEE Computer Society \nConference on Computer Vision and Pattern Recognition 2016-\nDecem, pp 770–778. https:// doi. org/ 10. 1109/ CVPR. 2016. 90\n 64. Russakovsky O, Deng J, Su H et al (2015) ImageNet large scale \nvisual recognition challenge. Int J Comput Vis 115:211–252. \nhttps:// doi. org/ 10. 1007/ s11263- 015- 0816-y\n 65. Krizhevsky A, Sutskever I, Hinton GE (2017) ImageNet classifi-\ncation with deep convolutional neural networks. Commun ACM \n60:84–90. https:// doi. org/ 10. 1145/ 30653 86\n 66. Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet clas -\nsification with deep convolutional neural networks. In: Pereira F, \nBurges CJ, Bottou L, Weinberger KQ (eds) Advances in neural \ninformation processing systems. Curran Associates Inc\n 67. Simonyan K, Zisserman A (2015) Very deep convolutional net-\nworks for large-scale image recognition. In: 3rd International Con-\nference on Learning Representations, ICLR 2015—Conference \nTrack Proceedings, pp 1–14\n 68. Tan M, Le Q V (2021) EfficientNetV2: smaller models and faster \ntraining\n 69. Howard A, Sandler M, Chen B, et al (2019) Searching for mobile-\nNetV3. In: Proceedings of the IEEE International Conference on \nComputer Vision. Institute of Electrical and Electronics Engineers \nInc., pp 1314–1324\n 70. Mehta S, Rastegari M (2021) MobileViT: light-weight, general-\npurpose, and mobile-friendly vision transformer. 3\n 71. Mehta S, Rastegari M (2022) Separable self-attention for mobile \nvision transformers\n 72. Tu Z, Talebi H, Zhang H, et al (2022) MaxViT: multi-axis vision \ntransformer. Lecture Notes in computer science (including sub-\nseries lecture notes in artificial intelligence and lecture notes in \nbioinformatics) 13684 LNCS, pp 459–479. https:// doi. org/ 10.  \n1007/ 978-3- 031- 20053-3_ 27\n 73. Touvron H, Cord M, Douze M, et al (2020) Training data-efficient \nimage transformers & distillation through attention, pp 1–22\n 74. Touvron H, Cord M, Ai M DeiT III : Revenge of the ViT. 1–27\n 75. Bao H, Dong L, Piao S, Wei F (2021) BEiT: BERT pre-training \nof image transformers\n 76. d’ Ascoli S, Touvron H, Leavitt M, et al (2021) ConViT: improv-\ning vision transformers with soft convolutional inductive biases. \nhttps:// doi. org/ 10. 1088/ 1742- 5468/ ac9830\n 77. Chu X, Tian Z, Wang Y et al (2021) Twins: revisiting the design \nof spatial attention in vision transformers. Adv Neural Inf Process \nSyst 12:9355–9366\n 78. Heo B, Yun S, Han D, et al (2021) Rethinking spatial dimensions \nof vision transformers\n 79. Hatamizadeh A, Yin H, Heinrich G, et al (2022) Global context \nvision transformers\n 80. Deepak S, Ameer PM (2023) Brain tumor categorization from \nimbalanced MRI dataset using weighted loss and deep feature \nfusion. Neurocomputing 520:94–102. https:// doi. org/ 10. 1016/j. \nneucom. 2022. 11. 039\n 81. Özkaraca O, Bağrıaçık Oİ, Gürüler H et al (2023) Multiple brain \ntumor classification with dense CNN architecture using brain MRI \nimages. Life. https:// doi. org/ 10. 3390/ life1 30203 49\n 82. Rahman T, Islam MS (2023) MRI brain tumor detection and clas-\nsification using parallel deep convolutional neural networks. Meas \nSens. https:// doi. org/ 10. 1016/j. measen. 2023. 100694\n 83. Muezzinoglu T, Baygin N, Tuncer I et al (2023) PatchResNet: \nMultiple patch division-based deep feature fusion framework for \nbrain tumor classification using MRI images. J Digit Imaging \n36:973–987. https:// doi. org/ 10. 1007/ s10278- 023- 00789-x\n 84. Ali MU, Hussain SJ, Zafar A et al (2023) WBM-DLNets: wrapper-\nbased metaheuristic deep learning networks feature optimization \nfor enhancing brain tumor detection. Bioengineering. https:// doi. \norg/ 10. 3390/ bioen ginee ring1 00404 75\nPublisher’s Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computational intelligence",
  "concepts": [
    {
      "name": "Computational intelligence",
      "score": 0.6698724031448364
    },
    {
      "name": "Residual",
      "score": 0.6593798995018005
    },
    {
      "name": "Computer science",
      "score": 0.589979350566864
    },
    {
      "name": "Transformer",
      "score": 0.5885196924209595
    },
    {
      "name": "Perceptron",
      "score": 0.5548961162567139
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4936321973800659
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4926295876502991
    },
    {
      "name": "Multilayer perceptron",
      "score": 0.4570707082748413
    },
    {
      "name": "Artificial neural network",
      "score": 0.31336313486099243
    },
    {
      "name": "Voltage",
      "score": 0.12336879968643188
    },
    {
      "name": "Algorithm",
      "score": 0.10494339466094971
    },
    {
      "name": "Engineering",
      "score": 0.10030326247215271
    },
    {
      "name": "Electrical engineering",
      "score": 0.07189947366714478
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I344703621",
      "name": "Iğdır Üniversitesi",
      "country": "TR"
    }
  ],
  "cited_by": 73
}