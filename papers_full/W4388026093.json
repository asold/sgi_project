{
  "title": "Investigation on the Application of Artificial Intelligence Large Language Model in Translation Tasks",
  "url": "https://openalex.org/W4388026093",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2121952550",
      "name": "Chunlan Jiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4212807739",
    "https://openalex.org/W2240536489",
    "https://openalex.org/W4205263358",
    "https://openalex.org/W2952402558",
    "https://openalex.org/W4200414108",
    "https://openalex.org/W2567070169"
  ],
  "abstract": "As an emerging language technology, the application of Artificial Intelligence (AI) Large Language Model (LLM) in translation tasks has an important background.Based on a large number of experimental data, this paper compared traditional machine translation and Google translate, and evaluated the performance of AI LLM in multilingual translation tasks.The experimental results showed that compared with traditional machine translation and Google translate, the AI LLM performed better in terms of translation quality and speed.Specifically, the Bilingual Evaluation Understudy (BLEU) score of the LLM was about 5 percentage points higher than that of traditional machine translation and Google translate, and the BLEU value per second was about 5 percentage points higher than that of traditional machine translation; in terms of speed, the LLM was 13.53 seconds faster than traditional machine translation on average.These findings indicated that the AI LLM had broad application prospects and important application value in practical applications, and could provide better technical support for achieving language translation.",
  "full_text": "Investigation on the Application of Artificial Intelligence \nLarge Language Model in Translation Tasks \nChunlan Jiang \nSenior Translation School, Xi‚Äôan Fanyi University, Shaanxi Xi‚Äôan, China 710105 \njiangchunlanzhengf@126.com \nAbstract. As an emerging language technology, the application of Artific ial \nIntelligence (AI) Large Language Model (LLM) in translation tas ks has an im-\nportant background. Based on a large number of experimental dat a, this paper \ncompared traditional machine translation and Google translate, and evaluated the \nperformance of AI LLM in multilingual translation tasks. The ex perimental re-\nsults showed that compared with traditional machine translation  and Google \ntranslate, the AI LLM performed better in terms of translation quality and speed. \nSpecifically, the Bilingual Evaluation Understudy (BLEU) score of the LLM was \nabout 5 percentage points higher than that of traditional machine translation and \nGoogle translate, and the BLEU value per second was about 5 percentage points \nhigher than that of traditional machine translation; in terms o f speed, the LLM \nwas 13.53 seconds faster than traditional machine translation o n average. These \nfindings indicated that the AI LLM  had broad application prospects and im-\nportant application value in pra ctical applications, and could provide better \ntechnical support for achieving language translation. \nKeywords: Artificial Intelligence; Natural Language Processing; Recurrent  \nNeural Network; Large Language Model \n1 Introduction \nWith the acceleration of globalization and the continuous deepening of cross-border \nexchanges, the translation industry has achieved unprecedented development. With the \nrapid development of AI technology, the AI LLM has become one of the most con-\ncerned technologies in the field of translation [1-2]. Especially in translation tasks, AI \nLLM can play a huge role. translation tasks often involve multiple languages and fields, \nrequiring the support of a large number of professional translators and various transla-\ntion tools. However, the birth of AI LLM has provided more conv enient and efficient \ntranslation methods for translation tasks [3-4]. By pre training a large-scale corpus and \nutilizing deep learning algorithms for self-learning, the LLM can quickly translate a \nlarge amount of text, thus saving translation manpower and time  costs and improving \ntranslation quality and efficiency. Different from traditional machine translation, AI \nLLM can understand context and con text, and has higher language expression ability \nand accuracy. For example, in some political, diplomatic, and business negotiations, \n¬© The Author(s) 2023\nS. Yacob et al. (eds.), Proceedings of the 2023 7th International Seminar on Education, Management and Social \nSciences (ISEMSS 2023), Advances in Social Science, Education and Humanities Research 779,\nhttps://doi.org/10.2991/978-2-38476-126-5_147\n\nthe choice and expression of words are crucial for conveying in formation and negoti-\nating results. A LLM with sufficient language proficiency can b etter handle such \ncomplex scenarios and produce more accurate translation results [5]. \nIn recent years, many scholars and experts have conducted research on the applica-\ntion of AI LLM in translation tasks. Among them, Xiong L made s ignificant progress \nusing NON-AUTOREGRESSIVE (NAR) N eural Machine Translation (NMT) . The \nmain advantage of NAR-NMT was that it could simultaneously process all words in the \ninput sentence, thus allowing for parallelization and improving  translation efficiency. \nThere were currently two main types of NAR-NMT methods: templat e based and \ngenerative based. The template based method used previously tra nslated word se-\nquences as templates. Based on this, adjustments and modificati ons were made to \ngenerate the final translation. The generative method used the bridge of posterior \ndistribution and word embedding vector to predict translation results. He stated that the \nlatest research currently showed that combining templates and g enerative methods \ncould achieve better results. In addition, there were other tec hniques and structural \noptimizations that could be used to further improve the performance of NAR-NMT [6]. \nLiu Y proposed a neural machine translation model for Chinese E nglish translation \nusing massively parallel corpora. This model adopted a deep bid irectional long short \nterm memory network, which could better learn and model sentences. At the same time, \nit introduced a adaptive learning rate algorithm (AdaDelta) to optimize the training \nprocess [7]. Kiros R proposed that the multimodal neural language model wa s a lan-\nguage model that could simultaneously process multiple language  modalities such as \ntext, images, videos, sound, etc. This model utilized deep lear ning technology to fuse \ndifferent types of media information to establish a richer and more accurate language \nmodel. Compared with the traditional single mode language model , the multimodal \nneural language model could be tter use multiple information for  semantic under-\nstanding and generation, thus improving the effect of natural l anguage processing. At \nthe same time, the multimodal neural language model was also an  important means to \nachieve multimedia intelligence, cross media information proces sing, modal fusion, \nand other aspects [8]. \nThe research on the application of AI LLM in translation tasks shows that compared \nwith traditional machine translation methods, the effect of using LLM for translation is \nbetter and faster. LLM can extract more abundant semantic information and have better \nnatural language processing ability by learning a large number of corpora. In translation \ntasks, LLM can more accurately understand the meaning of senten ces, identify more \nlanguage contexts and common sense, thereby improving translati on quality and reli-\nability. \n \n \n \n \n \n \n1342             C. Jiang\n2 Relevant Methods for the Application of AI Large Language \nModel in translation tasks \n2.1 Introduction to Recurrent Neural Network under AI \nThe strength of Recurrent Neural Network (RNN) lies in its ability to process sequence \ndata. It is widely used in machin e translation, speech recognit ion, Natural language \nprocessing and other fields. In AI, RNN can be used to process natural language texts, \naudio signals, video sequences, etc. In language models, RNN ca n be seen as a model \nwhere the input sequence is the same as the output sequence. Among them, each input \nis related to the output of the previous moment, which enables it to encode contextual \ninformation into the model. In image description, RNN can match different parts of the \nimage with textual descriptions. There are many variants of RNN , such as Long \nShort-Term Memory (LSTM) and Gated Recurrent Unit (GRU), which can better deal \nwith long-term dependence. \nIn RNN, the implicit state h (H idden State) is introduced, whic h extracts features \nfrom the forward data sequence of serialized data based on h, saves the obtained state, \nand finally converts it into output, thus solving the problem of modeling sequence. \n‚Ñé‡Øò ‡µåùëì ·à∫ ùëÉ ùëõ‡Øò ‡µÖùëÑ‚Ñé‡Øò‡¨ø‡¨µ ‡µÖùëé·àª ( 1 )  \nThe calculation of hidden state ‚Ñé‡Øò is obtained by simultaneously inputting the \ncurrent time e of the sequence and the previous hidden state. Among them, P, Q, and a \nare all parameters to be learned. When passed to the later mome nts, the calculation \nformula for the hidden state is t he same, and the parameters P,  Q, and a used in each \ns t e p  a r e  t h e  s a m e ,  w h i c h  m e a n s  t h a t  t h e  p a r a m e t e r s  o f  e a c h  s t e p a r e  s h a r e d .  A f t e r  \nobtaining the hidden state of each step, the output of each step can be obtained through \nthe hidden state and input, as shown in Formula (2). Among them , k is the activation \nfunction, and S and t are parameters to be learned. \nùëß‡Øò ‡µåùëò ·à∫ ùëÜ ‚Ñé‡Øò ‡µÖùë°·àª ( 2 )  \nActivation function is an indispensable module in deep neural n etwork. If the Ac-\ntivation function is not used in  the neural network model, each  neuron can only use \nlinear transformation to optimize the weight. This seemingly si m p l e  a n d  f a s t  w a y  \ndirectly leads to the neural network can not learn the complex mapping expression in \nthe training process, which makes the performance of the neural network model worse. \nTherefore, this paper selects th e Rectified Linear Unit (ReLU) function as the activa-\ntion function of the standard algorithm RNN. The ReLU function formula is as follows: \nùëÖùëíùêøùëà·à∫ùëò·àª ‡µå ùëöùëéùë•·à∫ùëò,0·àª ( 3 )  \n2.2 Large Language Model \nLLM is a Natural language processing model based on machine learning technology. It \ncan encode and understand natural language and generate text that conforms to syntax \nInvestigation on the Application of Artificial Intelligence             1343\nand semantic logic. The LLM is a kind of supervised learning model, which needs a lot \nof training data to learn the rules and patterns of language, s o as to build a model that \ncan better understand and generate language [9-10]. The basic idea of the LLM is to \nestablish a Statistical model for language modeling, and use probability distribution to \ndescribe the relationship between the possibility of language a nd various language \nphenomena. In the framework of this model, the probability of any given paragraph of \ntext appearing in the model can b e calculated to evaluate and c ompare the language \nexpression of the text [11-12]. \nLLM are usually built based on neural networks, and the most common architecture \nis RNN, which is used to process sequence data. This model can output a probability \ndistribution at each moment, tha t is, each word, to represent t he next possible word \n[13-14]. The training data of LLM usually uses Corpus, which provides a comprehensive \nand rich description of language expression, such as Wikipedia,  Gutenberg Project, \nnews media, movie subtitles, etc. [15-16]. \nLLM are widely used in text generation, automatic summarization , machine trans-\nlation, speech recognition, speech  synthesis and other fields. In terms of text genera-\ntion, LLM can generate articles that conform to grammar and semantic logic based on a \ngiven theme or style, such as news reports, social media Weibo, novels, etc. In terms of \nautomatic s um marization, LL M ca n anal yze  and summa rize an article to generate a \nconcise and general summary. In machine translation, LLM can he lp further optimize \nthe translation model and improve the accuracy and fluency of translation. In terms of \nspeech recognition and synthesis, LLM can also model and optimi ze the audio ex-\npression of text, as shown in Figure 1: \n \n \nFig. 1. Application scenario flowchart of the LLM \n1344             C. Jiang\n2.3 Integration of AI Large Language Model in translation tasks \nThe AI LLM is a technology used to generate human language, whi ch has received \nwidespread attention and application in recent years. In translation tasks, AI LLM can \nplay an important role, as shown in Figure 2: \n \n \nFig. 2. The fusion of AI LLM in translation tasks \nAs shown in Figure 2, the fusion of AI LLM in translation tasks can be divided into \nthe following four points: \n1) Improving translation efficiency: The AI LLM can quickly ana l y z e  a  l a r g e  \namount of language data, identify patterns and correlations within it, and automatically \ntranslate, thus greatly improving translation efficiency. In tasks such as rapid response \nto international incident and publication of international pape r s ,  A I  L L M  c a n  s o l v e  \ntranslation problems and quickly complete translation tasks [17-18]. \n2) Optimizing translation quality: The AI LLM can generate high -quality human \nlanguage and produce more accurate and fluent language during the translation process. \nEspecially in fields that involve professional terminology or have high requirements for \nsentence structure and grammar, AI LLM can play a good optimization role. \n3) Processing multilingual translation: With the continuous str engthening of glob-\nalization, the demand for multilingual translation is increasing, and AI LLM can easily \nhandle translation tasks between multiple languages. Especially for fields that require a \nlarge amount of precise and complex language translation, such as business, law, and \nother fields, the application of AI LLM would be further expanded [19-20]. \n4) Improving translation standards and standardization: The AI LLM follows certain \nrules and standards, and can provide high-quality and standardi zed translation results, \nmaking it easier to meet the high-quality and standardized tran slation needs of gov-\nernments, enterprises, and academia. When the amount of data is large, the advantages \nof AI LLM in translation standards and standardization are more obvious. \nIn summary, the AI LLM has the characteristics of fast, efficie nt, standardized, \nhigh-quality, and processing multiple languages, which plays an  important role in the \nsolving ability of translation tasks. In the future, the AI LLM would further play a role \nInvestigation on the Application of Artificial Intelligence             1345\nin international cultural exchange, academic research, and othe r aspects, thus contin-\nuously optimizing the translation field, providing convenient s ervices for human lan-\nguage communication, and promoting social progress. \n3 Experimental Results and Discussion on the Application of AI \nLarge Language Model in translation tasks \n3.1 Application Purpose of AI Large Language Model in translation tasks \nThis experiment aims to explore the application of AI LLM in translation tasks, verify \nits translation quality and efficiency in multilingual translat ion tasks, and compare it \nwith traditional machine translation methods. \n3.2 Application Evaluation of AI Large Language Model in translation tasks \nIn order to verify the application of the LLM in translation ta sks, experiments were \nconducted using six languages: E nglish Chinese, Chinese English , German Chinese, \nChinese German, Spanish Chinese, and Chinese Spanish. The exper imental results \nwere compared and analyzed with Google translate. \nThe experimental analysis shows that for the translation tasks of English Chinese, \nChinese English, German Chinese, Chinese German, Spanish Chines e ,  a n d  C h i n e s e  \nSpanish, the translation quality of AI LLM is better than that of traditional machine \ntranslation, and the speed is faster. Compared with Google translate, the LLM performs \nbetter in terms of translation quality and is also faster, as shown in Table 1 and Figure 3: \nTable 1. Comparison of BLEU scores for different language translation task models \nLanguage vs Traditional ma-\nchine translation \nArtificial intelli-\ngence large lan-\nguage model \nGoogle translate \nEnglish/Chinese 41.23 59.87 57.34 \nChinese/English 43.12 65.53 61.45 \nGerman/Chinese 49.67 72.16 63.57 \nChinese/German 51.42 75.38 69.22 \nSpain/Central 25.39 55.77 49.11 \nCentral/Spain 30.28 58.93 53.64 \nTable 1 shows the comparison of BLEU scores among different language translation \ntask models. BLEU score was a common method to automatically evaluate the quality \nof machine translation, which ranged from 0 to 100. The higher the score, the better the \ntranslation quality. The results showed that the average BLEU s core of AI LLM in all \nlanguage pairs was higher than that of traditional machine translation, and higher than \nthat of Google translate. Taking the English Chinese translatio n task as an example, \ntraditional machine translation scored 41.23, while AI LLM scored 59.87, and Google \nTranslate scored 57.34. The same trend even became more apparen t when analyzing \n1346             C. Jiang\nthe BLEU scores of four language pairs: Germany China, China Ge rmany, Spain \nChina, and China Spain. Among them, the AI LLM performed more prominently when \ntranslating German and Chinese German tasks, and its BLEU scores reached 72.16 and \n75.38 respectively, while the traditional machine translation s cored 49.67 and 51.42 \nrespectively, and Google Translate scored 63.57 and 69.22 respectively. \n \nFig. 3. Speed of translation models in different languages \nAs shown in Figure 3, it could be seen that the speed and BLEU value per second of \ndifferent language translation models were compared. The speed represented the time \nrequired to translate 1000 words, and the BLEU value per second  r e p r e s e n t e d  t h e  \nBLEU score generated per second when translating 1000 words. It could be concluded \nfrom the data that the translation speed of the LLM in all language pairs was higher than \nthat of traditional machine translation. The speed of English C hinese and Chinese \nEnglish translation pairs was 5.92 seconds and 6.46 seconds, re spectively, which was \n13.64 seconds and 15.43 seconds faster than that of traditional  machine translation. In \nterms of BLEU value per second, the LLM scored far higher in all language pairs than \ntraditional machine translation, and the average score was abou t 5 percentage points \nhigher than traditional machine translation. Among English Chin ese and Chinese \nEnglish translation pairs, the BLEU value per second of the LLM  w a s  t h e  h i g h e s t ,  \nreaching 10.10 and 10.19 respectiv ely, which were 4.9 and 4.22 higher than that of \ntraditional machine translation respectively. Compared to Googl e translate, the LLM \nalso performed better in terms of speed and BLEU values per second. \n3.3 Application Results of AI Large Language Model in translation tasks \nThe experimental results indicate that the AI LLM has broad app lication prospects in \ntranslation tasks. In multilingual translation tasks, the trans lation quality and speed of \nthe LLM are better than the traditional machine translation met hods. Compared to \nGoogle translate, the LLM perfor ms better in terms of translati on quality and speed. \nTherefore, the LLM has broad development prospects and application value in practical \napplications. In summary, this experiment validated the applica tion of the AI LLM in \nInvestigation on the Application of Artificial Intelligence             1347\ntranslation tasks, and presented the experimental results in the form of a data table. It is \nbelieved that in the future, with the continuous improvement of  technology and the \ncontinuous updating of language models, the application of LLM in translation tasks \nwould become more mature, providing better technical support fo r achieving rapid \nlanguage translation. \n4 Application Results and Discussion of AI Large Language \nModel in translation tasks \n4.1 Application Status of AI Large Language Model in translation tasks \nWith the continuous progress and development of technology, AI has been widely \napplied in many fields, among which translation task is one of them. In translation \ntasks, the use of AI LLM can better meet some translation needs , such as political \nliterature, contracts, news reports, etc. Therefore, LLM would play an important role in \nnational translation work. \n4.2 Application Verification and Results of AI Large Language Model in \ntranslation tasks \nIn order to verify the role of AI LLM in translation tasks, som e international political \ndocuments in recent years were selected for translation, includ ing Chinese, English, \nFrench, Russian, German, etc. The evaluation results of transla tion would be divided \ninto three aspects for statistics: error correction, translatio n accuracy, and translation \nspeed. \nBefore translation, data training would be conducted for the AI LLM, with 150GB of \ndata trained for each language. After that, it would be judged by comparing the results \nof machine translation and human translation, as shown in Figure 4: \n \nFig. 4. Statistical data of the LLM in translation tasks \n1348             C. Jiang\nIn Figure 4, the number of error s corrected, translation accura cy, and translation \nspeed of the LLM in translation tasks were statistically analyzed. The number of error \ncorrections refers to the number of times that machine translation results needed to be \ncorrected. The lower the number of error corrections, the higher the quality of machine \ntranslation; translation accuracy refers to the percentage of machine translation that was \nconsistent with human translation. The higher the translation a ccuracy, the higher the \naccuracy of machine translation;  translation speed refers to th e number of words that \ncould be translated per minute (WPS). The higher the translation speed, the higher the \nefficiency of machine translation. From Figure 4, it could be s een that the LLM per-\nforms well in translation tasks in five languages, achieving hi gh levels of translation \naccuracy, translation speed, and e rror correction. For example,  among the Chinese \ntasks with the highest translation accuracy, the accuracy of th e LLM reached 92%. In \nterms of translation speed, English performed the best and coul d translate approxi-\nmately 18000 words per minute. Overall, these data indicated th at AI LLM had high \ntranslation quality and efficiency in translation tasks, and we re one of the important \ndirections for future development. \n4.3 Application Strategy of AI Large Language Model in translation tasks \nIn the future, the following measures can be taken to further i mprove the translation \nperformance of LLM in translation tasks: \n1) Training more data: LLM can improve translation quality by e xpanding the \ncorpus and training more data. \n2) Increasing domain knowledge: Translation within a specific domain can improve \ntranslation efficiency by adding terminology within the domain. \n3) Increasing machine capability: By increasing the translation  capability of the \nmachine, information loss during the translation process can be minimized. \nIn short, the AI LLM has been widely applied in translation tas ks and has great \npotential for development. In the future, it is highly possible  to achieve further im-\nprovements by further adjusting and optimizing the model based on the above strate-\ngies. \n5 Conclusions \nThe AI LLM is a new type of language technology that can achiev e multilingual \ntranslation in translation tasks.  Based on a large number of ex perimental data, this \npaper compared traditional machine translation and Google translate, and found that AI \nLLM performed better in multilingua l translation tasks. Specifi cally, AI LLM was \nsuperior to traditional machine translation in terms of translation quality and speed, and \ncould achieve language translation more quickly and accurately.  In addition, the AI \nLLM also had better scalability, so it had broad prospects for development in practical \napplications and could provide better technical support for ach ieving language trans-\nlation. \nInvestigation on the Application of Artificial Intelligence             1349\nFunding \n2023 Shaanxi Provincial Philosoph y and Social Science Research Special Project \n(2023HZ0997) \nReferences \n1. T s e n g  M ,  L i n  S ,  T s a i  S ,  e t  a l .  Applying GPT-3 to Generate Text- Based Radiology Re-\nports[J]. Journal of Digital Imaging, 2021, 34(4):890-898. \n2. Zhang Z, Mei C, Yin W, et al. Understanding the Transferability of Transformer Language \nModels to Finite State Machines[ J]. IEEE Journal on Selected Ar eas in Communications, \n2021, 39(1):24-31. \n3. C u i  Y ,  Y a n g  S ,  H u a n g  S ,  e t  a l .  E x p l o r i n g  t h e  E f f e c t i v e n e s s  o f  Fine-Tuning Pre-Trained \nLanguage Models for Named Entity Recognition in the Clinical Do main[J]. BMC Medical \nInformatics and Decision Making, 2020, 20(1):1-11. \n4. Zhang M, Chen Z, Wong, D. Langua ge Modeling with Gated Convolut ional Networks[J]. \nIEEE/ACM Transactions on Audio,  Speech, and Language Processing , 2019, 27(7): \n1123-1135. \n5. Chen G, Gao Q, Liu B. Multi-Source Transfer Learning for Neural Machine Translation[J]. \nIEEE Transactions on Neural Networks and Learning Systems, 2020, 31(1): 81-93. \n6. Xiong L, Mou L, Zhou K, et al. A Comprehensive Survey of Recent  Advances in \nNon-Autoregressive Neural Machine Translation[J]. ACM Computing  Surveys, 2021, \n54(5):1-41. \n7. Liu Y, Zhang, Y, Fu, L., et al. A Chinese-English Neural Machine Translation Model Based \non Large-Scale Parallel Corpora[J]. Journal of Computer Research and Development, 2021, \n58(5): 1070-1077. \n8. Kiros R. Salakhutdinov, R., Zemel, R. Multimodal Neural Language Models[J]. Journal of \nMachine Learning Research, 2022, 25(3): 1-17. \n9. Xie S, Dai G, Huang L. Bidirectional Attention-Based Neural Machine Translation[J]. IEEE \nTransactions on Audio, Speech, and Language Processing, 2021, 29(4): 825-838. \n10. Huang P S, Bueno I G, Yaung A, et al. GPT Agent: Production Mac hine-Learning Model \nTraining at YouTube Scale[J]. IEEE Micro, 2020, 40(6):25-32. \n11. Wang S, Ren X, Zhao J, et al. Using GPT-2 Language Model to Generate High-Quality Test \nCases on the Sentence Level[J]. IEEE Transactions on Software E ngineering, 2021, \n47(2):238-254. \n12. Chen W, Chen X, Shen Z, et al. A GPT-2-based Personalized Dialo gue System[J]. Journal \nof Intelligent & Fuzzy Systems, 2021, 40(3):4975-4987. \n13. Huang Y, Chen X M, Cao Y X. E-commerce Product Review Generatio n Based on Deep \nLearning Model GPT-2[J]. Journal of Convergence Information Tec hnology, \n2021,16(2):31-40. \n14. Xu X, Chen Y, Sai X, et al. Multi-agent Dialogue Generation wit h Transformer Language \nModel and GPT-2[J]. Journal of Intelligent & Fuzzy Systems, 2021, 40(5):10115-10122. \n15. L i u  X ,  Z h o  X ,  R e n  X ,  e t  a l .  S o f t w a r e  T e s t  C a s e  G e n e r a t i o n  w i t h  GPT-2[J]. Journal of \nSystems Engineering and Electronics, 2021, 32(2):421-433. \n16. Zhao, Y., Xu, S., Sun, K. Combining Phrase-Based and Neural Mac hine Translation for \nHigh-Accuracy Translation[J]. International Journal of Computat ional Linguistics and \nChinese Language Processing, 2022, 27(1): 1-21. \n1350             C. Jiang\n17. Yang C, Shang, J., Lin, J. Dense- Attention Networks for Neural Machine Translation[J]. \nJournal of Computer Science and Technology, 2019, 34(5): 1008-1018. \n18. Cui Y, Liu C, Li M, et al. Semantic Folding with Pretrained Lan guage Models for Interac-\ntive Clinical Note Summarization[J]. BMC Medical Informatics an d Decision Making, \n2021, 21(1):1-15. \n19. Wu W., Wang, H., Zhou, Y., et al. A Survey of Neural Machine Translation: Challenges and \nOpportunities[J]. ACM Computing Surveys, 2022, 55(2): 1-34. \n20. Wang Y, Liu Q, Xie L, et al. Multi-Component Pretraining for Bi -Directional Medical \nConcept Normalization[J]. IEEE  Journal of Biomedical and Health  Informatics, 2021, \n25(10):3651-3660. \nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution-\nNonCommercial 4.0 International License (http://creativecommons.org/licenses/by-nc/4.0/),\nwhich permits any noncommercial use, sharing, adaptation, distribution and reproduction in any\nmedium or format, as long as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license and indicate if changes were made.\n        The images or other third party material in this chapter are included in the chapter's\nCreative Commons license, unless indicated otherwise in a credit line to the material. If material\nis not included in the chapter's Creative Commons license and your intended use is not\npermitted by statutory regulation or exceeds the permitted use, you will need to obtain\npermission directly from the copyright holder.\nInvestigation on the Application of Artificial Intelligence             1351",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6634758710861206
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5729857683181763
    },
    {
      "name": "Natural language processing",
      "score": 0.5619720220565796
    },
    {
      "name": "Translation (biology)",
      "score": 0.541955292224884
    },
    {
      "name": "Cognitive science",
      "score": 0.3318186402320862
    },
    {
      "name": "Psychology",
      "score": 0.22206270694732666
    },
    {
      "name": "Chemistry",
      "score": 0.05385911464691162
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 3
}