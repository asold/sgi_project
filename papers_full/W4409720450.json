{
    "title": "Can Large Language Models Grasp Abstract Visual Concepts in Videos? A Case Study on YouTube Shorts about Depression",
    "url": "https://openalex.org/W4409720450",
    "year": 2025,
    "authors": [
        {
            "id": null,
            "name": "Jiaying \"Lizzy\" Liu",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A2531638547",
            "name": "Yiheng Su",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A4294657231",
            "name": "Praneel Seth",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2610332539",
        "https://openalex.org/W2588050621",
        "https://openalex.org/W4366590580",
        "https://openalex.org/W1979290264",
        "https://openalex.org/W4391136507",
        "https://openalex.org/W4382538068",
        "https://openalex.org/W4404782964",
        "https://openalex.org/W2941513909",
        "https://openalex.org/W4380319170",
        "https://openalex.org/W4387606052",
        "https://openalex.org/W4396832002",
        "https://openalex.org/W4402780269",
        "https://openalex.org/W2592658867",
        "https://openalex.org/W4391043766",
        "https://openalex.org/W2003676674",
        "https://openalex.org/W2803769621",
        "https://openalex.org/W2566909055",
        "https://openalex.org/W2765108713",
        "https://openalex.org/W4385800557",
        "https://openalex.org/W4399362737",
        "https://openalex.org/W4392736278",
        "https://openalex.org/W4399205864",
        "https://openalex.org/W4402727764",
        "https://openalex.org/W4396218059",
        "https://openalex.org/W4400222414",
        "https://openalex.org/W2146120111",
        "https://openalex.org/W4392427296",
        "https://openalex.org/W2737677090",
        "https://openalex.org/W2611167769",
        "https://openalex.org/W4390038780",
        "https://openalex.org/W4252849059",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W3001362559",
        "https://openalex.org/W4387329158",
        "https://openalex.org/W4366548515",
        "https://openalex.org/W2284264830",
        "https://openalex.org/W2133665775",
        "https://openalex.org/W4396833706",
        "https://openalex.org/W4360978668",
        "https://openalex.org/W4402671996",
        "https://openalex.org/W4404356490",
        "https://openalex.org/W4389519587",
        "https://openalex.org/W4386908233",
        "https://openalex.org/W4402904028",
        "https://openalex.org/W4404331635",
        "https://openalex.org/W4396833376",
        "https://openalex.org/W2990138404"
    ],
    "abstract": "Large language models (LLMs) are increasingly used to assist computational social science research. While prior efforts have focused on text, the potential of leveraging multimodal LLMs (MLLMs) for online video studies remains underexplored. We conduct one of the first case studies on MLLM-assisted video content analysis, comparing AI's interpretations to human understanding of abstract concepts. We leverage LLaVA-1.6 Mistral 7B to interpret four abstract concepts regarding video-mediated self-disclosure, analyzing 725 keyframes from 142 depression-related YouTube short videos. We perform a qualitative analysis of MLLM's self-generated explanations and found that the degree of operationalization can influence MLLM's interpretations. Interestingly, greater detail does not necessarily increase human-AI alignment. We also identify other factors affecting AI alignment with human understanding, such as concept complexity and versatility of video genres. Our exploratory study highlights the need to customize prompts for specific concepts and calls for researchers to incorporate more human-centered evaluations when working with AI systems in a multimodal context.",
    "full_text": "Can Large Language Models Grasp Concepts in Visual Content? A\nCase Study on YouTube Shorts about Depression\nJiaying “Lizzy” Liu∗\nSchool of Information\nThe University of Texas at Austin\nAustin, Texas, USA\njiayingliu@utexas.edu\nYiheng Su∗\nArtificial Intelligence and\nHuman-Centered Computing\n(AI&HCC) Lab\nThe University of Texas at Austin\nAustin, Texas, USA\nsam.su@utexas.edu\nPraneel Seth\nComputer Science Department\nThe University of Texas at Austin\nAustin, Texas, USA\npraneelseth@utexas.edu\nAbstract\nLarge language models (LLMs) are increasingly used to assist com-\nputational social science research. While prior efforts have focused\non text, the potential of leveraging multimodal LLMs (MLLMs) for\nonline video studies remains underexplored. We conduct one of the\nfirst case studies on MLLM-assisted video content analysis, com-\nparing AI’s interpretations to human understanding of abstract\nconcepts. We leverage LLaVA-1.6 Mistral 7B to interpret four ab-\nstract concepts regarding video-mediated self-disclosure, analyzing\n725 keyframes from 142 depression-related YouTube short videos.\nWe perform a qualitative analysis of MLLM’s self-generated ex-\nplanations and found that the degree of operationalization can\ninfluence MLLM’s interpretations. Interestingly, greater detail does\nnot necessarily increase human-AI alignment. We also identify\nother factors affecting AI alignment with human understanding,\nsuch as concept complexity and versatility of video genres. Our\nexploratory study highlights the need to customize prompts for\nspecific concepts and calls for researchers to incorporate more\nhuman-centered evaluations when working with AI systems in a\nmultimodal context.\nCCS Concepts\n• Human-centered computing →Collaborative and social\ncomputing design and evaluation methods ; Empirical studies\nin HCI; • Computing methodologies →Computer vision ;\nKeywords\nComputational Social Science, Video-Mediated Communication,\nMultimodal Information, User-Generated Content, Large Language-\nand-Vision Assistant (LLaVA), Content Analysis, Mental Health\nACM Reference Format:\nJiaying “Lizzy” Liu, Yiheng Su, and Praneel Seth. 2025. Can Large Language\nModels Grasp Concepts in Visual Content? A Case Study on YouTube Shorts\nabout Depression. In Extended Abstracts of the CHI Conference on Human\nFactors in Computing Systems (CHI EA ’25), April 26-May 1, 2025, Yokohama,\n∗Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCHI EA ’25, Yokohama, Japan\n© 2025 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-1395-8/2025/04\nhttps://doi.org/10.1145/3706599.3719821\nJapan. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3706599.\n3719821\n1 Introduction\nVideo-sharing platforms such as YouTube [37], TikTok [49], and\nInstagram [2] are rich data sources for research in human-computer\ninteraction and computational social sciences. However, traditional\nmethods for analyzing videos, like digital ethnography [ 27] and\ncontent analysis [14], are labor-intensive with limited scalability [5].\nConsequently, there is a rising demand for automated approaches\nto analyze multimodal (visual, textual, audio) content [6].\nOne successful strategy is leveraging LLMs to augment text-\nbased content analysis, improving open coding efficiency [11] and\nenabling collaborative coding frameworks [17, 57]. Emerging Multi-\nmodal LLMs (MLLMs) like LLaVA [34] and GPT-4 [46] demonstrate\npromise for understanding visual information at scale [54]. How-\never, few works have investigated how MLLMs can best assist\ncontent analysis of videos [52, 60]. Preliminary work [42] suggests\nthat MLLMs may struggle to capture abstract visual concepts, such\nas video presentation style [38], limiting their applications beyond\nobjective entity or action recognition in video analysis [1, 9, 33].\nThis case study thus aims to explore the capability of MLLMs to\nunderstand abstract concepts in multimodal contexts. Specifically,\nwe investigate how LLaVA-1.6 Mistral 7B interprets four concepts\nrelated to depression and self-disclosure behaviors in short YouTube\nvideos, assessing the MLLM’s alignment with human understanding.\nWe aim to explore:\nRQ1: How can social concepts be operationalized to guide\nMLLMs in interpreting video content?\nRQ2: What factors affect MLLM’s alignment with human\ninterpretations of social concepts in videos?\nEchoing the emerging trend of LLM-assisted content analysis,\nour case study is one of the earliest efforts to leverage MLLMs\nfor video content analysis: 1) We experiment with harnessing an\nMLLM for annotating abstract visual concepts with structured and\nexplainable outputs; 2) We examine the MLLM’s explanations and\nreveal contextual factors that affect MLLM’s alignment with human\nunderstanding of abstract social concepts. 3) We discuss implica-\ntions for designing robust, human-centered workflows for future\nMLLM-assisted video content analysis.\narXiv:2503.05109v1  [cs.HC]  7 Mar 2025\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan Liu et al.\n2 Context: Mental Health Disclosure on\nVideo-Based Social Media\nIndividuals increasingly use digital platforms to share their men-\ntal health experiences and seek support online [ 15]. While prior\nresearch has extensively focused on text-based platforms like Twit-\nter [12] and Reddit [48], visual-based platforms like Instagram [3]\nand YouTube [24, 39] are growing in popularity for self-disclosure\ndocumentation.\nVisual content offers unique self-disclosure opportunities distinct\nfrom textual modalities [42]. Specific image genres like selfies, social\nrelationships, and captioned images can convey emotional distress,\ncalls for help, and vulnerability in powerful ways [ 2]. Similar to\nthe influence of linguistic features on engagement for text-based\nsocial media posts, prior studies have highlighted the significant\nrole of visual representations in shaping viewer perception [ 32]\nand supportive behaviors (e.g., comments) [22]. However, which\nfeatures of the visual representations and how they influence viewer\nengagement remain unclear.\nThis work thus aims to extend prior text-based online health\ncommunication research into the underexplored video-based social\nmedia, where self-disclosure may be communicated through inter-\nactive language and visual cues. Specifically, we investigate how\nvisual features moderate the relationship between self-disclosure\nand video engagement (e.g., likes and comments) in depression-\nrelated YouTube shorts. Addressing this question can reveal insights\nsuch as identifying visual markers of distress, rhetorical framing\nof health narratives, and emergent phenomena in visually diverse\ncontent to inform the design of more supportive communities on\nvideo-sharing platforms. Given the challenges of manual annota-\ntion for large-scale video content analysis, we leverage MLLMs for\nassistance.\nWe selected four concepts (Table 1) that shape video-mediated\nself-disclosure. Presenting and interacting styles represent distinct\napproaches to structuring and delivering video narratives, which\ninfluence audience engagement [2, 28]. Visual diversity and arousal\nare unique for video-based communication, influencing viewers’\nattention and perception of content engagement [44, 50]. These vi-\nsual characteristics are indicative cues to determine how effectively\nmental health content resonates with and engages viewers.\n3 Methodology\n3.1 Dataset\nUsing the query \"depression\" with the YouTube Data API, we col-\nlected the metadata (e.g., title, channel, duration) of 3,892 videos\nuploaded by February 2024. We randomly selected 150 videos and\ndownloaded them using YoutubeDownloader1. Following Liu et\nal. (2024) [38], due to computational constraints and the current\nMLLM’s limited context window to process videos [ 20], we ap-\nplied FFmpeg [53] to extract representative keyframes in videos.\nFFmpeg is a standard video processing method for identifying key\nmoments in videos [ 21, 29, 41]. Here, we employed FFmpeg to\nextract frames where the structural similarity index (SSIM) [ 55]\ndifference exceeded 0.3, ensuring the selection of visually distinct\n1https://github.com/Tyrrrz/YoutubeDownloader\nframes. Additionally, we filtered out low-quality frames (e.g., tran-\nsitional frames, blurry, black screens) through manual inspection\nand obtained 725 keyframes across 142 videos.\nOur study qualifies for exemption under our Institutional Review\nBoard guidelines. Nevertheless, recognizing the sensitive nature\nof mental health topics, we safeguard video creators’ privacy by\nanonymizing their identities through obscuring facial features. Fur-\nther discussion of ethical considerations can be found in Appen-\ndix C.\nFigure 1: Examples of human interpretations of the four\nselected concepts. We annotate Yes/No for presenting and\ninteracting, High/Low for diversity and arousal. We then\ncompare human interpretations with the MLLM interpreta-\ntions to evaluate human-AI alignment.\n3.2 MLLM Concept Annotation: Models and\nPrompts\nWe first tested Video-LLaMA [ 61] on 10 sample videos and\nobserved significant challenges in concept comprehension and\nhigh computational costs. This observation aligns with prior\nfindings that Video LLMs generally underperform compared to\nImage LLMs [ 40]. Given these limitations, we instead selected\nllava-v1.6-mistral-7b-hf2 [35] to analyze keyframes and will\nhenceforth refer to this model as (the) MLLM for convenience.\nTo investigate the MLLM’s comprehension of abstract visual\nconcepts (Table 1), operationalizing these concepts is essential for\narticulating them effectively. To address RQ1 and explore how to\noperationalize the concepts for MLLM prompt configuration, we\ntested four strategies and evaluated their effectiveness. Specifically,\nwe implemented four prompting configurations with progressively\nincreasing levels of operational guidance to strike a balance between\nclarity and flexibility. See Appendix A for all prompt configurations.\n2https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf\nCan Large Language Models Grasp Concepts in Visual Content?\nA Case Study on YouTube Shorts about Depression. CHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nTable 1: Four abstract visual concepts shaping video-mediated self-disclosure.\nConcept Definition\nPresenting Presenting style involves the delivery of information, typically accompanied by visual aids like slides or graphics [25].\nInteracting Interacting refers to creators establishing a simulated interpersonal relationship with their audience, fostering a sense of engagement and connection\n[25].\nDiversity Diversity of an image includes varied scenes, color variation, compositional complexity, and originality of the image [50].\nArousal Arousal refers to the degree of alertness or excitement elicited by the stimulus such as dynamic visual elements and emotional intensity [44].\n•Naive: The MLLM is directly queried for the presence or\nextent of the concept without any additional contexts.\n•Simple: A short definition is added to the naive query.\n•Detailed: A detailed definition with three abstract manifes-\ntations is added to the naive query.\n•Open-minded: Similar to the detailed prompt, but also ex-\nplicitly encourages the MLLM to consider other scenarios\nnot already stated.\nEstablished practices in prompt engineering inform our prompt-\ning configurations. For instance, the Detailed configuration aligns\nwith in-context learning by incorporating prototypical examples to\nserve as implicit \"demonstrations\" [45]. The Open-minded con-\nfiguration is inspired by chain-of-thought (CoT), a technique that\naims to improve LLM logical reasoning by incorporating directives\nlike \"think step-by-step\" in prompts [26]. In our context, we aim\nto mitigate potential constraints introduced by fixed definitions in\nSimple and Detailed configurations while balancing clarity and\nflexibility in how MLLMs interpret abstract concepts. Thus, we\nadapt CoT by explicitly instructing the model to \"be open-minded\"\nin the prompts to encourage divergent, reflexive thinking similar\nto tree-of-thought [58].\nWe do not experiment with advanced configurations such as\nin-context learning or fine-tuning [13, 19], as we are interested in\nassessing the MLLM’s off-the-shelf capabilities.\nWe tasked the MLLM with annotating each keyframe across\nfour concepts: Yes/No for interacting and presenting, and High/Low\nfor arousal and diversity. To ensure consistency, we prompted the\nMLLM to provide both interpretations and explanations simulta-\nneously, reducing the likelihood of generating contradictory or\nhallucinated explanations. Keyframes were queried in temporal\norder for each video, while the order of prompt configurations and\nassociated concepts were randomized per keyframe to mitigate\npotential biases. Occasionally, the MLLM combines annotations\n(e.g., Yes/No) with explanations [ 38]. To isolate explicit annota-\ntions, we utilized Llama-3.1-8B-Instruct3 to parse the MLLM’s\ninterpretations. Following this, we manually reviewed all extracted\nannotations to verify the accuracy of the parsing process.\n3.3 Human Annotation Process\nTo obtain human interpretations, two authors independently coded\na random sample of 200 keyframes, with a third author providing\nan additional vote to resolve disagreements. Figure 1 illustrates\nexamples of human interpretations. After discussing disputes in a\ngroup meeting and ensuring that Intercoder Reliability (ICR) [47]\nis higher than 75%, the three coders split the remaining keyframes\n3https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\nand coded them separately. We dropped ambiguous keyframes and\nlow-quality images (e.g., transitional frames, blurry, black screens)\nfrom further analysis. Ultimately, we obtain 725 frames across 142\nvideos with human concept annotations.\n3.4 Data Analysis\nQuantitative Comparisons. To compare the four prompt config-\nurations, we quantify human-AI (mis)alignment as the consistency\nbetween a prompt-concept pair and the corresponding human an-\nnotations. We then employ the bootstrapping approach from [7] to\nassess how human-AI alignment differs across configurations per\nconcept. Please see Appendix B for details. We discuss quantitative\ncomparisons in Section 4.1.\nQualitative Analysis. To investigate the underlying factors be-\nhind human-AI (mis)alignments, we first curated a focused dataset\nof instances where the MLLM’s annotations diverged when us-\ning different prompting configurations. Two authors then indepen-\ndently conducted thematic analysis [8] on the MLLM’s explanations\nfor these keyframes. They met weekly to discuss emerging themes\nand patterns in the data, resolve any coding discrepancies through\ndetailed discussion, and iterate on the coding scheme to establish\ndefinitions for each thematic category. The analysis focused on\nseveral key dimensions, including the nature and patterns of anno-\ntation changes, the MLLM’s reasoning and justification for mod-\nifications, contextual factors that appeared to influence changes,\nand the relationship between prompting configuration and anno-\ntation stability. We summarize recurring themes and patterns in\nSection 4.2.\n4 Findings\n4.1 Quantative Evaluation of MLLM-Human\nAlignment\nFigure 2 shows the distribution of bootstrapped alignment scores\nacross prompt configurations for each concept. The MLLM demon-\nstrates varying capabilities: no single prompt configuration consis-\ntently achieves the highest alignment.\nThe MLLM excels at abstract concepts like arousal and diversity\nbut exhibits lower alignment and more variance for performative\nconcepts like interacting and presenting. Under the naive approach,\nthe MLLM performs well for concepts like interacting, arousal, and\ndiversity, suggesting that the MLLM’s prior knowledge of these con-\ncepts (derived from pre-trained data) aligns well with corresponding\nhuman conceptions. We only observe substantial alignment gains\nwith more operationalization guidance for presenting. However,\nthis effect is not monotonic (e.g., a more detailed prompt does not\nalways lead to better alignment) and does not generalize to other\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan Liu et al.\nconcepts. Adding definitions may decrease alignment for present-\ning and interacting, restricting the MLLM’s capabilities. We discuss\nthe factors that impact annotations in Section 4.2.\nFigure 2: Distribution of bootstrap alignment scores across\nprompt configurations and concepts. The MLLM demon-\nstrates varying capabilities: no single prompt configuration\nconsistently achieves the highest alignment across all con-\ncepts.\n4.2 Factors Affecting MLLM-Human\n(Mis)Alignment\nEvidently, concept operationalization is a key factor influencing\nhuman-AI alignment. By analyzing the MLLM’s explanations, we\noffer qualitative insights into how and why operationalization im-\npacts alignment. Additionally, we identify two further factors con-\ntributing to human-AI (mis)alignment: concept complexity and the\ndiversity of genres.\n4.2.1 Varying Concept Specification. Concept specification\nrefers to the amount of detail in the prompts. For interacting and\npresenting (Figure 2), auxiliary definitions may inadvertently prior-\nitize \"what is in the prompt\" over the holistic context of the image,\ncausing the MLLM to be less aligned with human perceptions. In\ncontrast, the naive approach shows greater flexibility in captur-\ning novel categories of presenting and interacting communication\nstyles.\nFigure 3-(a) illustrates the variability in the MLLM’s interpreta-\ntion of presenting style. When prompted naively, the MLLM cor-\nrectly identifies (a) as presenting, stating that the superimposed cap-\ntion is “a common technique used in presentations” , complemented\nby “the person’s facial expression, which appears to be a smile” . Con-\nversely, when prompted with simple or detailed configurations,\nthe MLLM misclassifies (a), citing “no visible slide or graphic that\nwould be associated with a presentation” as evidence. This misclas-\nsification occurred because the detailed prompts explicitly exem-\nplified presenting styles as \"slides or graphics, \" limiting the MLLM\nfrom considering informal contexts of presenting style. In contrast,\nthe openminded configuration correctly identifies (a), further un-\nderscoring that additional details can enhance clarity but reduce\nalignment if not carefully operationalized.\nWithout definitional constraints, the naive configuration can\nbetter capture nuanced social dynamics. In Figure 3-(b), the MLLM\naccurately described the interactive potential, noting the “dynamic\nand engaging” style of the image to “[invite] the viewer to observe\nand possibly speculate about what is happening. ”\nHowever, when prompted with a detailed configuration, the\nMLLM incorrectly claims that the image “is a still photograph” with\n“no indication of a simulated interpersonal relationship or engagement\nwith an audience” .\nWe consistently observe this pattern of contradictory decisions\nfor presenting and interacting queries, where explanations often\nhighlight the absence of explicit elements outlined in the prompt.\nFor example, keyframes without human presence or overt conver-\nsational styles (Figure 3-(c)) were misclassified as non-interactive\ndespite employing engaging nontraditional styles such as memes.\n4.2.2 Varying Complexity of Concepts. The complexity and\nscope of the four analyzed concepts vary, making some more chal-\nlenging for the MLLM. For example, diversity is relatively straight-\nforward, as it involves identifying and counting visual categories,\na common pre-training task for MLLMs. Figure 1 illustrates this:\nthe low-diversity image shows a plain background with simple\ntext overlays, while the high-diversity image features a vibrant\nanime figure. Similarly, the MLLM effectively recognizes arousal\nlevels through visual cues like facial expressions, body language,\nand visual intensity. In Figure 1, the low-arousal image depicts a\ncalm individual with relaxed features, while the high-arousal image\nshows an abstract figure with intense body language indicating\ndistress.\nIn contrast, concepts like interacting and presenting are more\nchallenging because they require situating visual cues within con-\ntext. For instance, in the \"Presenting-Yes\" image (Figure 1), while\nthe hand gesture might initially suggest interaction, the gesture\nis not directed at the audience but instead presents the scenario\nencoded in the text overlay (“5th grade: Hey kid listen up” ). In multi-\nmodal contexts, the meaning of one element (e.g., a visual cue) can\ninfluence, support, or contradict another (e.g., text). This demand to\ninterpret co-dependent features holistically poses a novel challenge\nabsent in text-only settings.\nWhen MLLM’s pre-trained knowledge diverges from human\nconceptions, naive queries often result in misalignment. We ob-\nserve this quantitatively, as the Naive alignment for presenting is\nvery low (Figure 2). Qualitatively, in the \"Presenting-Yes\" image\n(Figure 1), the MLLM incorrectly states that the image does not\nshow presentation style, citing the absence of expected behaviors\nlike “a speaker standing at a podium or a lectern” and “a slide or a\ngraphic”. The MLLM fails to contextualize the informal setting and\ngesture as a valid presentation style, thus struggling to adapt to\nnovel communicative contexts outside pretraining. Prompt engi-\nneering can help, as the MLLM correctly identifies this image for\nall other configurations besides naive.\n4.2.3 Versatile Video Genres. The versatility of videos can chal-\nlenge the MLLM’s ability to understand social concepts. We identify\nCan Large Language Models Grasp Concepts in Visual Content?\nA Case Study on YouTube Shorts about Depression. CHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nFigure 3: Problematic MLLM Annotations.\ntwo genres with relatively low alignment, highlighting the com-\nplexities of interpreting diverse content.\nMixture of textual and visual elements. Short videos often com-\nbine visuals with overlaying text, as shown in Figure 3-(a, c, d).\nWhen visual signals conflict with textual information, MLLMs (typi-\ncally) prioritize textual over visual cues (since they were pre-trained\nwith more text data), potentially leading to misinterpretations. For\nexample, in Figure 3-(d), the MLLM reasons that the image “does\nnot directly portray an interacting style...as it is static” but the text\noverlay “implies a narrative or a message that is meant to convey a\nsense of interaction. ” Effectively synthesizing two potentially con-\nflicting sources of information—visual and textual—is a unique and\nopen challenge for MLLMs.\nNon-human genres. Resonating Zhong et al. [ 63], the MLLM\nstruggles to interpret non-human video genres such as cartoons,\nmemes, and abstract art, which often require cultural, emotional, or\nother contextual knowledge for accurate interpretations. For exam-\nple, Figure 3-(e) depicts a hand-drawn image of self-harm behaviors,\npotentially signaling interaction intentions such as a call for help.\nHowever, the MLLM failed to recognize implicit interaction cues\nand explained that “the drawing...does not exhibit any conversational\nlanguage or behaviors that would suggest an interacting style” .\nFine-tuning or more sophisticated prompt engineering is likely\nneeded to educate the MLLM on a broader range of visual story-\ntelling techniques and cultural references.\n5 Discussion and Future Work\nWe conduct an exploratory study with a single model, limited sam-\nples, and simple prompts, so our findings may not be generalizable.\nComputational constraints further prevented the inclusion of tem-\nporal context in videos, which may limit our findings. Despite these\nlimitations, our study offers insights into opportunities and chal-\nlenges of leveraging Multimodal Large Language Models (MLLMs)\nto assist visual content analysis that are relevant regardless of the\nemployed model. Recognizing the inherent subjectivity of social\nconcepts (even with high intercoder reliability), we use \"alignment\"\nrather than \"accuracy\" and contextualize our quantitative statis-\ntics with qualitative insights. Our analysis illuminates key factors\ncontributing to MLLM’s misalignment from human understanding,\nincluding concept specifications, concept complexity, and versa-\ntility of video genres, which must be considered carefully when\nengineering prompts for MLLMs-assisted video content analysis.\n5.1 Harnessing MLLMs for Large-Scale\nMultimodal Content Analysis:\nOpportunities and Challenges\nMLLMs show potential in scaling visual content analysis. With\nappropriate operationalization, our results show that the MLLM\ncan align highly with human perceptions, even for abstract con-\ncepts like presentation style. By expediting manual labeling, which\nis often time-intensive and costly [ 14], MLLM can enable more\ncomprehensive analyses of large datasets, potentially uncovering\nrare communication patterns that might otherwise go unnoticed\nin small-sample qualitative studies [43]. Furthermore, MLLMs can\nenhance data quality by serving as a proxy for human intervention.\nIn our pipeline, the MLLM accurately labeled low-quality frames as\n\"Not Applicable, \" distinguishing them from frames that genuinely\nlacked the desired concept. This capability can help researchers\nfilter noisy inputs by inspecting ambiguous model outputs and\nexplanations. As models continue to scale and improve, these\nstrengths will likely grow even more pronounced, offering\nabundant opportunities to support large-scale video content\nanalysis.\nDespite their potential, MLLMs can be misaligned with human\nperceptions.\nOur findings indicate that operationalizing abstract concepts\nwith greater detail can enhance alignment. However, it may also\nrisk constraining the MLLM’s ability to uncover novel social dy-\nnamics beyond the specified criteria. This contrasts with typical\nin-context or few-shot learning scenarios, where multiple demon-\nstrations help the model infer task structure and reduce ambiguity\nby leveraging patterns recognized during pretraining [ 45]. This\nchallenge will likely persist regardless of model size since\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan Liu et al.\nconcepts are intrinsically ambiguous. Recent work like [ 4]\nand [23] demonstrate that even state-of-the-art MLLMs like\nGPT4 still face difficulties interpreting abstract concepts like\nhumor in multimodal contexts such as memes, comics, or\nspoken conversations. In diverse social media content, models\nmust balance consistency with flexibility to adapt to dynamic con-\ntexts.\nAdditionally, when applying MLLM to analyze videos in the wild,\nwe highlight that video style diversity is a crucial factor impact-\ning model alignment. The short videos in our study are predom-\ninantly informal and casually filmed in everyday settings. They\ndiffer from vlogs, tutorials, streams, or product reviews, typically\nmore structured and polished. Our findings show that the MLLM\ncan struggle to capture and interpret unconventional visual cues,\nsuch as the novel yet subtle suggestion of suicide depicted in Figure\n3 (c).Although more advanced MLLMs may be more generally\naligned with human perceptions, these context-dependent\nand culturally specific signals often require situated aware-\nness that larger-scale pre-training alone may not sufficiently\naddress. Developing and evaluating models that can effectively\nnavigate such ambiguity while maintaining alignment on more\nstructured formats remains essential for advancing multimodal\nanalysis across diverse platforms.\n5.2 Limitations and Future Work\nWe emphasize three directions to improve human-AI alignment in\n(M)LLM-assisted visual content analysis: human-centered auditing,\nmultimodal synthesis, and temporality incorporation.\nImplementing MLLM response auditing. In our case study,\nMLLM interpretations often diverged from human concept under-\nstanding due to factors like concept complexity and the diversity of\nvideo genres. Specifically, the MLLM may systematically misunder-\nstand the visual cues of videos of specific genres, such as cartoons\nand memes, as suggested in Section 4.2.3. Thus, it is crucial to im-\nplement human-centered post hoc audits [56, 62]. Shen et al. [51]\ndeveloped a framework to audit the value alignment of humans\nand language models to improve transparency and ethical use of AI\nin social research. Future work can explore incorporating human-\ncentered evaluation as a standard step in MLLM-assisted content\nanalysis workflows [10, 18, 31, 59]. Such measures can facilitate\nthe iterative refinement of concept operationalization and prompt\nengineering to address known biases in an AI’s understanding of\nsocial concepts.\nSynthesizing multimodal inputs. In our current workflow, we\ndecode videos into keyframes and prompt the MLLM to annotate\nconcepts given isolated images. However, we can also incorporate\naudio or transcripts to provide a more comprehensive analysis,\nthough interpreting signals from multiple input sources remains\nchallenging. Additionally, as discussed in Section 4.2.3, conflicting\ninformation across different modalities can complicate interpre-\ntations. Developing more sophisticated methods for synthesizing\nmultimodal inputs is thus a promising avenue for future research.\nIncoporating video temporality. Some concepts require a\ntemporal context for accurate interpretations. For example, con-\ncepts like emotional valence and genre often depend on a holistic\nunderstanding of the video’s overall narrative [38], which isolated\nkeyframes cannot capture. Future work could explore MLLMs that\ndirectly interpret videos or a sequence of keyframes to provide\nmore contextual information.\n5.3 Ethical Considerations\nOur findings suggest that human-AI misalignment may result in\nsystematic biases. Previous studies have reported LLMs’ biases\ntowards minorities and underrepresented populations, including\npeople with disabilities [16] and socially subordinate groups [30].\nFuture studies can work on identifying the potential biases in LLMs.\n6 Conclusion\nWe conduct one of the earliest case studies on leveraging Mul-\ntimodal Large Language Models (MLLMs) to interpret abstract\nsocial concepts in video data. Whereas prior work primarily em-\nployed LLMs for text-based social media data, we demonstrate\nhow MLLMs can extend large-scale automated content analysis to\nvideo content, capturing abstract concepts of self-disclosure styles\nand subjective visual cues. Through quantitative and qualitative\ncomparisons, we highlight key factors undergirding misalignments\nbetween MLLM and human perceptions, such as concept opera-\ntionalization, complexity, and genre diversity. Interestingly, adding\nprototypical manifestations of abstract concepts does not consis-\ntently improve alignment. Our results underscore the importance\nof post-hoc auditing and human oversight to ensure agreement\nbetween AI outputs and human understanding. Future work should\nexplore the integration of multimodal inputs and experiment with\nfine-tuning or in-context learning to enhance the model’s ability to\nunderstand more complex social interactions.\nAcknowledgments\nWe appreciate the comments by Dr. Yunlong Wang. This project\nis partially supported by the University of Texas at Austin Univer-\nsity Graduate Continuing Fellowship, Cisco, NSF grant IIS2107524,\nand Good Systems4 a UT Austin Grand Challenge to develop re-\nsponsible AI technologies. The statements made herein are solely\nthe opinions of the authors and do not reflect the views of the\nsponsoring agencies.\nReferences\n[1] Dana Alsagheer, Rabimba Karanjai, Weidong Shi, Nour Diallo, Yang Lu, Suha\nBeydoun, and Qiaoning Zhang. 2024. Evaluating Irrationality in Large Language\nModels and Open Research Questions. (2024).\n[2] Nazanin Andalibi. 2017. Self-disclosure and Response Behaviors in Socially\nStigmatized Contexts on Social Media: The Case of Miscarriage. In Proceedings\nof the 2017 CHI Conference Extended Abstracts on Human Factors in Computing\nSystems (CHI EA ’17) . Association for Computing Machinery, New York, NY, USA,\n248–253. doi:10.1145/3027063.3027137\n[3] Nazanin Andalibi, Pinar Ozturk, and Andrea Forte. 2017. Sensitive Self-\ndisclosures, Responses, and Social Support on Instagram: The Case of #De-\npression. In Proceedings of the 2017 ACM Conference on Computer Supported\nCooperative Work and Social Computing . ACM, Portland Oregon USA, 1485–1500.\ndoi:10.1145/2998181.2998243\n[4] Ashwin Baluja. 2025. Text Is Not All You Need: Multimodal Prompting Helps\nLLMs Understand Humor. In Proceedings of the 1st Workshop on Computational\nHumor (CHum) , Christian F. Hempelmann, Julia Rayz, Tiansi Dong, and Tristan\nMiller (Eds.). Association for Computational Linguistics, Online, 9–17. https:\n//aclanthology.org/2025.chum-1.2/\n[5] Ava Bartolome and Shuo Niu. 2023. A Literature Review of Video-Sharing\nPlatform Research in HCI. In Proceedings of the 2023 CHI Conference on Human\n4https://bridgingbarriers.utexas.edu/good-systems\nCan Large Language Models Grasp Concepts in Visual Content?\nA Case Study on YouTube Shorts about Depression. CHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nFactors in Computing Systems (Hamburg, Germany) (CHI ’23) . Association for\nComputing Machinery, New York, NY, USA, Article 790, 20 pages. doi:10.1145/\n3544548.3581107\n[6] Ava Bartolome and Shuo Niu. 2023. A Literature Review of Video-Sharing\nPlatform Research in HCI. In Proceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems . 1–20.\n[7] Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An Empirical\nInvestigation of Statistical Significance in NLP. In Proceedings of the 2012 Joint\nConference on Empirical Methods in Natural Language Processing and Computa-\ntional Natural Language Learning , Jun’ichi Tsujii, James Henderson, and Marius\nPaşca (Eds.). Association for Computational Linguistics, Jeju Island, Korea, 995–\n1005. https://aclanthology.org/D12-1091\n[8] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology.\nQualitative research in psychology 3, 2 (2006), 77–101.\n[9] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao\nChen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang,\nPhilip S. Yu, Qiang Yang, and Xing Xie. 2024. A Survey on Evaluation of Large\nLanguage Models. ACM Trans. Intell. Syst. Technol. 15, 3, Article 39 (mar 2024),\n45 pages. doi:10.1145/3641289\n[10] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li,\nYue Shen, Lei Liang, Jinjie Gu, and Huajun Chen. 2024. Unified Hallucination\nDetection for Multimodal Large Language Models. arXiv:2402.03190 [cs.CL]\nhttps://arxiv.org/abs/2402.03190\n[11] Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, and Annice\nKim. 2023. LLM-Assisted Content Analysis: Using Large Language Models to\nSupport Deductive Coding. doi:10.48550/arXiv.2306.14924 Issue: arXiv:2306.14924\narXiv:2306.14924 [cs, stat].\n[12] Munmun De Choudhury and Sushovan De. 2014. Mental Health Discourse on\nreddit: Self-Disclosure, Social Support, and Anonymity. (May 2014). https:\n//www.scinapse.io/papers/2182854643\n[13] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia,\nJingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. 2024. A\nSurvey on In-context Learning. In Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing , Yaser Al-Onaizan, Mohit Bansal, and\nYun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida,\nUSA, 1107–1128. doi:10.18653/v1/2024.emnlp-main.64\n[14] James W. Drisko and Tina Maschi. 2016. Content Analysis. Oxford University\nPress. Google-Books-ID: 07GYCgAAQBAJ.\n[15] Jessica L. Feuston and Anne Marie Piper. 2019. Everyday Experiences: Small\nStories and Mental Illness on Instagram. InProceedings of the 2019 CHI Conference\non Human Factors in Computing Systems (CHI ’19) . Association for Computing\nMachinery, New York, NY, USA, 1–14. doi:10.1145/3290605.3300495\n[16] Vinitha Gadiraju, Shaun Kane, Sunipa Dev, Alex Taylor, Ding Wang, Emily\nDenton, and Robin Brewer. 2023. \"I wouldn’t say offensive but... \": Disability-\nCentered Perspectives on Large Language Models. InProceedings of the 2023 ACM\nConference on Fairness, Accountability, and Transparency (FAccT ’23) . Association\nfor Computing Machinery, New York, NY, USA, 205–216. doi:10.1145/3593013.\n3593989\n[17] Jie Gao, Yuchen Guo, Toby Jia-Jun Li, and Simon Tangi Perrault. 2023. Collab-\nCoder: A GPT-Powered WorkFlow for Collaborative Qualitative Analysis. In\nCompanion Publication of the 2023 Conference on Computer Supported Cooperative\nWork and Social Computing (CSCW ’23 Companion) . Association for Computing\nMachinery, New York, NY, USA, 354–357. doi:10.1145/3584931.3607500\n[18] Katy Ilonka Gero, Chelse Swoopes, Ziwei Gu, Jonathan K. Kummerfeld, and\nElena L. Glassman. 2024. Supporting Sensemaking of Large Language Model\nOutputs at Scale. In Proceedings of the CHI Conference on Human Factors in\nComputing Systems (CHI ’24) . Association for Computing Machinery, New York,\nNY, USA, 1–21. doi:10.1145/3613904.3642139\n[19] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. 2024.\nParameter-efficient fine-tuning for large models: A comprehensive survey. arXiv\npreprint arXiv:2403.14608 (2024).\n[20] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah,\nAbhinav Shrivastava, and Ser-Nam Lim. 2024. MA-LMM: Memory-Augmented\nLarge Multimodal Model for Long-Term Video Understanding. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) .\n[21] Sungeun Hong, Jongbin Ryu, Woobin Im, and Hyun S. Yang. 2018. D3: Recog-\nnizing dynamic scenes with deep dual descriptor based on key frames and key\nsegments. Neurocomputing 273 (2018), 611–621. doi:10.1016/j.neucom.2017.08.046\n[22] Pengwei Hu, Chenhao Lin, Jiajia Li, Feng Tan, Xue Han, Xi Zhou, and Lun Hu.\n2023. Making the Implicit Explicit: Depression Detection in Web across Posted\nTexts and Images. In 2023 IEEE International Conference on Bioinformatics and\nBiomedicine (BIBM) . 4807–4811. doi:10.1109/BIBM58861.2023.10385590 ISSN:\n2156-1133.\n[23] Zhe Hu, Tuo Liang, Jing Li, Yiren Lu, Yunlai Zhou, Yiran Qiao, Jing Ma,\nand Yu Yin. 2024. Cracking the Code of Juxtaposition: Can AI Mod-\nels Understand the Humorous Contradictions. In Advances in Neural Infor-\nmation Processing Systems , A. Globerson, L. Mackey, D. Belgrave, A. Fan,\nU. Paquet, J. Tomczak, and C. Zhang (Eds.), Vol. 37. Curran Associates,\nInc., 47166–47188. https://proceedings.neurips.cc/paper_files/paper/2024/file/\n540a6eefb60428c8547a27253f9a2a59-Paper-Conference.pdf\n[24] Jina Huh, Leslie S. Liu, Tina Neogi, Kori Inkpen, and Wanda Pratt. 2014. Health\nVlogs as Social Support for Chronic Illness Management. ACM Trans. Comput.-\nHum. Interact. 21, 4 (Aug. 2014), 23:1–23:31. doi:10.1145/2630067\n[25] Margot Kelly-Hedrick, Paul H. Grunberg, Felicia Brochu, and Phyllis Zelkowitz.\n2018. \"It’s Totally Okay to Be Sad, but Never Lose Hope\": Content Analysis of\nInfertility-Related Videos on YouTube in Relation to Viewer Preferences. Journal\nof Medical Internet Research 20, 5 (May 2018), e10199. doi:10.2196/10199 Number:\n5.\n[26] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and\nYusuke Iwasawa. 2022. Large Language Models are Zero-Shot Reasoners. In\nAdvances in Neural Information Processing Systems , S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,\nInc., 22199–22213. https://proceedings.neurips.cc/paper_files/paper/2022/file/\n8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf\n[27] Sebastian Kubitschko and Anne Kaun (Eds.). 2016. Innovative Methods in Media\nand Communication Research . Springer International Publishing, Cham. doi:10.\n1007/978-3-319-40700-5\n[28] E Megan Lachmar, Andrea K Wittenborn, Katherine W Bogen, and Heather L\nMcCauley. 2017. #MyDepressionLooksLike: Examining Public Discourse About\nDepression on Twitter. JMIR MENTAL HEALTH (2017), 11.\n[29] Bokyeung Lee, Hyunuk Shin, Bonhwa Ku, and Hanseok Ko. 2023. Frame Level\nEmotion Guided Dynamic Facial Expression Recognition With Emotion Group-\ning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) Workshops . 5681–5691.\n[30] Messi H.J. Lee, Jacob M. Montgomery, and Calvin K. Lai. 2024. Large Language\nModels Portray Socially Subordinate Groups as More Homogeneous, Consistent\nwith a Bias Observed in Humans. In Proceedings of the 2024 ACM Conference on\nFairness, Accountability, and Transparency (FAccT ’24) . Association for Computing\nMachinery, New York, NY, USA, 1321–1340. doi:10.1145/3630106.3658975\n[31] Florian Leiser, Sven Eckhardt, Valentin Leuthe, Merlin Knaeble, Alexander Mäd-\nche, Gerhard Schwabe, and Ali Sunyaev. 2024. HILL: A Hallucination Identifier\nfor Large Language Models. In Proceedings of the CHI Conference on Human\nFactors in Computing Systems (CHI ’24) . Association for Computing Machinery,\nNew York, NY, USA, 1–13. doi:10.1145/3613904.3642428\n[32] Shuailin Li, Shiwei Wu, Tianjian Liu, Han Zhang, Qingyu Guo, and Zhenhui Peng.\n2024. Understanding the Features of Text-Image Posts and Their Received Social\nSupport in Online Grief Support Communities. Proceedings of the International\nAAAI Conference on Web and Social Media 18 (May 2024), 917–929. doi:10.1609/\nicwsm.v18i1.31362\n[33] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Hao-\nqian Wang, and Lei Zhang. 2023. Motion-X: A Large-scale 3D Ex-\npressive Whole-body Human Motion Dataset. In Advances in Neu-\nral Information Processing Systems , A. Oh, T. Naumann, A. Globerson,\nK. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc.,\n25268–25280. https://proceedings.neurips.cc/paper_files/paper/2023/file/\n4f8e27f6036c1d8b4a66b5b3a947dd7b-Paper-Datasets_and_Benchmarks.pdf\n[34] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved Baselines\nwith Visual Instruction Tuning. arXiv:2310.03744 [cs.CV]\n[35] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved Baselines\nwith Visual Instruction Tuning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) . 26296–26306.\n[36] Jiaying Liu, Shijie Song, and Yan Zhang. 2021. Linguistic features and consumer\ncredibility judgment of online health information. University of Illinois (2021).\nhttps://www.researchgate.net/profile/Shijie-Song-2/publication/350511487_\nLinguistic_features_and_consumer_credibility_judgment_of_online_health_\ninformation/links/6063cdd8a6fdccbfea1a542a/Linguistic-features-and-\nconsumer-credibility-judgment-of-online-health-information.pdf\n[37] Jiaying Liu and Yan Zhang. 2024. Modeling Health Video Consumption Behaviors\non Social Media: Activities, Challenges, and Characteristics. Proceedings of\nthe ACM on Human-Computer Interaction 8, CSCW1 (April 2024), 208:1–208:28.\ndoi:10.1145/3653699\n[38] Jiaying (Lizzy) Liu, Yunlong Wang, Yao Lyu, Yiheng Su, Shuo Niu, Xuhai \"Orson\"\nXu, and Yan Zhang. 2024. Harnessing LLMs for Automated Video Content\nAnalysis: An Exploratory Workflow of Short Videos on Depression. InCompanion\nPublication of the 2024 Conference on Computer-Supported Cooperative Work and\nSocial Computing (CSCW Companion ’24) . Association for Computing Machinery,\nNew York, NY, USA, 190–196. doi:10.1145/3678884.3681850\n[39] Leslie S. Liu, Jina Huh, Tina Neogi, Kori Inkpen, and Wanda Pratt. 2013. Health\nvlogger-viewer interaction in chronic illness management. In Proceedings of the\nSIGCHI Conference on Human Factors in Computing Systems . ACM, Paris France,\n49–58. doi:10.1145/2470654.2470663\n[40] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen,\nXu Sun, and Lu Hou. 2024. TempCompass: Do Video LLMs Really Understand\nVideos? doi:10.48550/arXiv.2403.00476 Issue: arXiv:2403.00476 arXiv:2403.00476\n[cs].\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan Liu et al.\n[41] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic. 2017. Unsupervised\nVideo Summarization With Adversarial LSTM Networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) .\n[42] Lydia Manikonda and Munmun De Choudhury. 2017. Modeling and Under-\nstanding Visual Attributes of Mental Health Disclosures in Social Media. In\nProceedings of the 2017 CHI Conference on Human Factors in Computing Systems\n(CHI ’17) . Association for Computing Machinery, New York, NY, USA, 170–181.\ndoi:10.1145/3025453.3025932\n[43] Ryan McGrady, Kevin Zheng, Rebecca Curran, Jason Baumgartner, and Ethan\nZuckerman. 2023. Dialing for Videos: A Random Sample of YouTube. Journal of\nQuantitative Description: Digital Media 3 (2023).\n[44] Nikos Metallinos. 2013. Television aesthetics: Perceptual, cognitive and composi-\ntional bases . Routledge.\n[45] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh\nHajishirzi, and Luke Zettlemoyer. 2022. Rethinking the Role of Demonstrations:\nWhat Makes In-Context Learning Work?. InProceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing , Yoav Goldberg, Zornitsa\nKozareva, and Yue Zhang (Eds.). Association for Computational Linguistics,\nAbu Dhabi, United Arab Emirates, 11048–11064. doi:10.18653/v1/2022.emnlp-\nmain.759\n[46] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge\nAkkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam\nAltman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Bal-\ncom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,\nJake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,\nOleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks,\nMiles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis\nChantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben\nChess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah\nCurrier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien\nDeville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet,\nAtty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson,\nVik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gor-\ndon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane\nGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,\nJohannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,\nBrandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny\nJin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali\nKamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kil-\npatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner,\nJamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kon-\ndrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael\nLampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming\nLi, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan\nLowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov,\nYaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,\nScott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David\nMedina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati,\nOleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind\nNeelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub\nPachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascan-\ndolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng,\nAdam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde\nde Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly\nPowell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford,\nJack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders,\nShibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schul-\nman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,\nPranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina\nSlama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Pet-\nroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B.\nThompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle,\nNick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vi-\njayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang,\nBen Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter\nWelinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter,\nSamuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael\nWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba,\nRowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng,\nJuntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical Report.\narXiv:2303.08774 [cs.CL]\n[47] Cliodhna O’Connor and Helene Joffe. 2020. Intercoder reliability in qualitative\nresearch: debates and practical guidelines. International journal of qualitative\nmethods 19 (2020), 1609406919899220.\n[48] Sachin R. Pendse, Neha Kumar, and Munmun De Choudhury. 2023. Marginaliza-\ntion and the Construction of Mental Illness Narratives Online: Foregrounding\nInstitutions in Technology-Mediated Care. Proceedings of the ACM on Human-\nComputer Interaction 7, CSCW2 (Oct. 2023), 346:1–346:30. doi:10.1145/3610195\n[49] Anastasia Schaadhardt, Yue Fu, Cory Gennari Pratt, and Wanda Pratt. 2023.\n“Laughing so I don’t cry”: How TikTok users employ humor and compassion\nto connect around psychiatric hospitalization. In Proceedings of the 2023 CHI\nConference on Human Factors in Computing Systems (CHI ’23) . Association for\nComputing Machinery, New York, NY, USA, 1–13. doi:10.1145/3544548.3581559\n[50] Mirjam Seckler, Klaus Opwis, and Alexandre N Tuch. 2015. Linking objective\ndesign factors with subjective aesthetics: An experimental study on how struc-\nture and color of websites affect the facets of users’ visual aesthetic perception.\nComputers in Human Behavior 49 (2015), 375–389.\n[51] Hua Shen, Tiffany Knearem, Reshmi Ghosh, Yu-Ju Yang, Tanushree Mitra, and\nYun Huang. 2024. Valuecompass: A framework of fundamental values for human-\nai alignment. arXiv preprint arXiv:2409.09586 (2024).\n[52] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan\nZhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. 2023. Video understanding with\nlarge language models: A survey. arXiv preprint arXiv:2312.17432 (2023).\n[53] Suramya Tomar. 2006. Converting video formats with FFmpeg. Linux journal\n2006, 146 (2006), 10.\n[54] Jiaqi Wang, Hanqi Jiang, Yiheng Liu, Chong Ma, Xu Zhang, Yi Pan, Mengyuan\nLiu, Peiran Gu, Sichen Xia, Wenjun Li, et al . 2024. A Comprehensive Review\nof Multimodal Large Language Models: Performance and Challenges Across\nDifferent Tasks. arXiv preprint arXiv:2408.01319 (2024).\n[55] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Im-\nage quality assessment: from error measurement to structural similarity. IEEE\ntransactions on image processing 13, 1 (2004).\n[56] Ziang Xiao, Wesley Hanwen Deng, Michelle S. Lam, Motahhare Eslami, Juho Kim,\nMina Lee, and Q. Vera Liao. 2024. Human-Centered Evaluation and Auditing of\nLanguage Models. In Extended Abstracts of the 2024 CHI Conference on Human\nFactors in Computing Systems (CHI EA ’24) . Association for Computing Machinery,\nNew York, NY, USA, 1–6. doi:10.1145/3613905.3636302\n[57] Ziang Xiao, Xingdi Yuan, Q. Vera Liao, Rania Abdelghani, and Pierre-Yves\nOudeyer. 2023. Supporting Qualitative Analysis with Large Language Mod-\nels: Combining Codebook with GPT-3 for Deductive Coding. In Companion\nProceedings of the 28th International Conference on Intelligent User Interfaces (IUI\n’23 Companion) . Association for Computing Machinery, New York, NY, USA,\n75–78. doi:10.1145/3581754.3584136\n[58] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and\nKarthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with\nLarge Language Models. In Advances in Neural Information Processing Systems ,\nA. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36.\nCurran Associates, Inc., 11809–11822. https://proceedings.neurips.cc/paper_files/\npaper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf\n[59] Yakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan Weill, Royi Ronen, and Noam\nKoenigstein. 2024. InterrogateLLM: Zero-Resource Hallucination Detection in\nLLM-Generated Answers. In Proceedings of the 62nd Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers) , Lun-Wei Ku, Andre\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\nBangkok, Thailand, 9333–9347. https://aclanthology.org/2024.acl-long.506\n[60] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong\nXu, and Enhong Chen. 2024. A survey on multimodal large lan-\nguage models. National Science Review 11, 12 (11 2024), nwae403.\ndoi:10.1093/nsr/nwae403 arXiv:https://academic.oup.com/nsr/article-\npdf/11/12/nwae403/61201557/nwae403.pdf\n[61] Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-LLaMA: An Instruction-\ntuned Audio-Visual Language Model for Video Understanding. In Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing:\nSystem Demonstrations , Yansong Feng and Els Lefever (Eds.). Association for\nComputational Linguistics, Singapore, 543–553. doi:10.18653/v1/2023.emnlp-\ndemo.49\n[62] He Zhang, Chuhao Wu, Jingyi Xie, Yao Lyu, Jie Cai, and John M. Carroll. 2023.\nRedefining Qualitative Analysis in the AI Era: Utilizing ChatGPT for Efficient\nThematic Analysis. doi:10.48550/arXiv.2309.10771 Issue: arXiv:2309.10771\narXiv:2309.10771 [cs].\n[63] Yang Zhong and Bhiman Kumar Baghel. 2024. Multimodal Understanding of\nMemes with Fair Explanations. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) Workshops . 2007–2017.\nA LLaVA Prompts\nCan Large Language Models Grasp Concepts in Visual Content?\nA Case Study on YouTube Shorts about Depression. CHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nTable 2: LLaVA Prompts\nConcept Strategy Prompt\nInteracting Prompt 0 -\nNaive\n\"<image>\nUSER: Does this picture portray an interacting style, yes or no? Explain your answer.\nASSISTANT:\"\nPrompt 1\n- Simple\nDefinition\n\"<image>\nUSER: Interacting style refers to creators establishing a simulated interpersonal relationship with\ntheir audience, fostering a sense of engagement and connection. Does this picture portray an\ninteracting style, yes or no? Explain your answer.\nASSISTANT:\"\nPrompt 2 - De-\ntailed Defini-\ntion\n\"<image>\nUSER: Interacting style refers to creators establishing a simulated interpersonal relationship with\ntheir audience, fostering a sense of engagement and connection. This involves behaviors such as\ndirectly addressing the audience, using conversational language, or acknowledging comments or\nquestions from viewers. Does this picture portray an interacting style, yes or no? Explain your\nanswer.\nASSISTANT:\"\nPrompt 3 -\nOpenminded\n\"<image>\nUSER: Interacting style refers to creators establishing a simulated interpersonal relationship with\ntheir audience, fostering a sense of engagement and connection. This involves behaviors such as\ndirectly addressing the audience, using conversational language, or acknowledging comments or\nquestions from viewers. These are just several examples, so be open-minded to other potential\nscenarios of interacting style. Does this picture portray an interacting style, yes or no? Explain\nyour answer.\nASSISTANT:\"\nPresenting Prompt 0 -\nNaive\n\"<image>\nUSER: Does this picture communicate in a presenting style, yes or no? Explain your answer.\nASSISTANT:\"\nPrompt 1\n- Simple\nDefinition\n\"<image>\nUSER: Presenting style involves the delivery of information, typically accompanied by visual aids\nlike slides or graphics. Does this picture communicate in a presenting style, yes or no? Explain\nyour answer.\nASSISTANT:\"\nPrompt 2 - De-\ntailed Defini-\ntion\n\"<image>\nUSER: Presenting style involves the delivery of information, typically accompanied by visual aids\nlike slides or graphics, such as a businessman presenting slides, a student giving a speech on a\ntopic, or a general rallying troops for war. Does this picture communicate in a presenting style,\nyes or no? Explain your answer.\nASSISTANT:\"\nPrompt 3 -\nOpenminded\n\"<image>\nUSER: Presenting style involves the delivery of information, typically accompanied by visual aids\nlike slides or graphics, such as a businessman presenting slides, a student giving a speech on a\ntopic, or a general rallying troops for war. These are just several examples, so be open-minded to\nother potential scenarios of presenting style. Does this picture communicate in a presenting style,\nyes or no? Explain your answer.\nASSISTANT:\"\nArousal Prompt 0 -\nNaive\n\"<image>\nUSER: What level of arousal does this image communicate, low, moderate, or high? Explain your\nanswer.\nASSISTANT:\"\nCHI EA ’25, April 26-May 1, 2025, Yokohama, Japan Liu et al.\nConcept Strategy Prompt\nPrompt 1\n- Simple\nDefinition\n\"<image>\nUSER: Low arousal is associated with calmness, relaxation, or drowsiness. Moderate arousal is a\nbalanced state of alertness and engagement without overstimulation. High arousal is characterized\nby heightened physiological and emotional activity. What level of arousal does this image commu-\nnicate, low, moderate, or high? Explain your answer.\nASSISTANT:\"\nPrompt 2 - De-\ntailed\n\"<image>\nUSER: Low arousal is associated with calmness, relaxation, or drowsiness. For example, feeling\nfatigued or viewing a peaceful landscape or a calm, monochromatic image. Moderate arousal is a\nbalanced state of alertness and engagement without overstimulation, often linked with optimal\nperformance and involves minimal physiological activation. For example, feeling attentive or\nfocused; engaging in a conversation or viewing a moderately complex image. High arousal is\ncharacterized by heightened physiological and emotional activity. For example, feeling excited,\nanxious, or stressed; or viewing a dynamic or chaotic scene with bright colors or intense stimuli.\nWhat level of arousal does this image communicate, low, moderate, or high? Explain your answer?\n\\nASSISTANT:\"\nPrompt 3 -\nOpen Minded\n\"<image>\nUSER: Low arousal is associated with calmness, relaxation, or drowsiness. For example, feeling\nfatigued or viewing a peaceful landscape or a calm, monochromatic image. Moderate arousal is a\nbalanced state of alertness and engagement without overstimulation, often linked with optimal\nperformance and involves minimal physiological activation. For example, feeling attentive or\nfocused; engaging in a conversation or viewing a moderately complex image. High arousal is\ncharacterized by heightened physiological and emotional activity. For example, feeling excited,\nanxious, or stressed; or viewing a dynamic or chaotic scene with bright colors or intense stimuli.\nThese are just several examples so be open-minded to other potential scenarios of arousal levels.\nWhat level of arousal does this image communicate, low, moderate, or high? Explain your answer?\n\\nASSISTANT:\"\nDiversity Prompt 0 -\nNaive\n\"<image>\nUSER: What level of diversity does this image communicate, low, moderate, or high? Explain your\nanswer? \\nASSISTANT:\"\nPrompt 1 -\nDefinition\n\"<image>\nUSER: The diversity of an image includes the color variation, compositional complexity, and\noriginality of the image. What level of diversity does this image communicate, low, moderate, or\nhigh? Explain your answer? \\nASSISTANT:\"\nPrompt 2 - De-\ntailed\n\"<image>\nUSER: The diversity of an image includes the color variation, compositional complexity, and\noriginality of the image. Color variation involves assessing the range of colors across the image.\nCompositional complexity involves the arrangement of diverse elements within the image. Origi-\nnality assesses whether the image presents a new or uncommon perspective. What level of diversity\ndoes this image communicate, low, moderate, or high? Explain your answer? \\nASSISTANT:\"\nPrompt 3 -\nOpen Minded\n\"<image>\nUSER: The diversity of an image includes the color variation, compositional complexity, and\noriginality of the image. Color variation involves assessing the range of colors across the image.\nCompositional complexity involves the arrangement of diverse elements within the image. Origi-\nnality assesses whether the image presents a new or uncommon perspective. These are just several\nexamples so be open-minded to other instances of diversity. What level of diversity does this image\ncommunicate, low, moderate, or high? Explain your answer? \\nASSISTANT:\"\nCan Large Language Models Grasp Concepts in Visual Content?\nA Case Study on YouTube Shorts about Depression. CHI EA ’25, April 26-May 1, 2025, Yokohama, Japan\nB Bootstraping Details\nTo assess how human-AI alignment differs across configurations\nfor each concept, we employ a bootstrapping approach inspired by\nthe methodology outlined in [7]. We first collect a pool of generated\nannotations for each concept and prompt configuration to compute\nan initial alignment score. However, relying on a single measure\nfails to capture the variability inherent in the data, and observed\ndifferences across configurations may arise purely by chance. This\nlimitation makes it challenging to draw reliable conclusions about\nthe relative alignment of different prompt configurations.\nThe bootstrapping approach addresses this issue by repeatedly\nresampling the data to estimate the variability in alignment scores.\nSpecifically, we generate 𝑁 resampled datasets, each of size 𝐾, by\nrandomly drawing annotations with replacement from the original\npool. An alignment score is computed for each resampled dataset,\nresulting in a distribution of 𝑁 scores for each concept-prompt pair.\nThis distribution reflects the variability in alignment and enables us\nto assess, on average, how reliably each prompting configuration\naligns with human perceptions across the selected social concepts\nbeyond random chance. We visualize these score distributions in\nFigure 2) and discuss findings in Section 4.1.\nC Ethics Statement\nWe are committed to conducting ethically responsible research,\nensuring content creators’ privacy, and safeguarding research team\nmembers’ well-being. Since this study analyzes data on publicly\navailable platforms like YouTube, it qualifies for human subjects\nexemption under our university’s Institutional Review Board (IRB)\nguidelines, posing minimal risk to content creators (or individuals\npresent in the video). Nevertheless, we acknowledge that creators\ncould not provide explicit consent for the inclusion or exclusion of\ntheir content. To respect the creator’s privacy, we implemented ad-\nditional protections, such as anonymizing individuals in the video\nby obscuring facial features in any snapshots in this paper. Addi-\ntionally, we do not collect personally identifiable metadata about\nthe creators or individuals presented in the videos.\nAnother ethical factor is the well-being of researchers exposed\nto potentially distressing material, particularly during qualitative\nanalyses involving sensitive topics like depression. To mitigate\npotential emotional harm, we provided team members access to\nuniversity mental health resources, encouraged breaks during data\nanalysis, and fostered an environment of open communication\nabout the work’s emotional impact."
}