{
  "title": "Transformer-based deep neural network language models for Alzheimer’s disease risk assessment from targeted speech",
  "url": "https://openalex.org/W3125401821",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2078805234",
      "name": "Alireza Roshanzamir",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A349423707",
      "name": "Hamid Aghajan",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A399963275",
      "name": "Mahdieh Soleymani Baghshah",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2078805234",
      "name": "Alireza Roshanzamir",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A349423707",
      "name": "Hamid Aghajan",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A399963275",
      "name": "Mahdieh Soleymani Baghshah",
      "affiliations": [
        "Sharif University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4236726526",
    "https://openalex.org/W2033002462",
    "https://openalex.org/W2129497119",
    "https://openalex.org/W588041470",
    "https://openalex.org/W2069740663",
    "https://openalex.org/W2074037951",
    "https://openalex.org/W1994871153",
    "https://openalex.org/W2250883471",
    "https://openalex.org/W2151969795",
    "https://openalex.org/W1997994949",
    "https://openalex.org/W2252171711",
    "https://openalex.org/W1549115098",
    "https://openalex.org/W3094326096",
    "https://openalex.org/W1853705225",
    "https://openalex.org/W2514692010",
    "https://openalex.org/W2964226943",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2397273938",
    "https://openalex.org/W2514643877",
    "https://openalex.org/W2920663707",
    "https://openalex.org/W2945121608",
    "https://openalex.org/W2578432686",
    "https://openalex.org/W2962979297",
    "https://openalex.org/W2939515132",
    "https://openalex.org/W2972757121",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2251658415",
    "https://openalex.org/W2963545917",
    "https://openalex.org/W2905266130",
    "https://openalex.org/W4214601561",
    "https://openalex.org/W2793398224",
    "https://openalex.org/W1847168837",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W3023309979",
    "https://openalex.org/W2089109585",
    "https://openalex.org/W2490470819",
    "https://openalex.org/W2921134339",
    "https://openalex.org/W2137554996",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W36820562",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2892062011",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2772874006",
    "https://openalex.org/W1522435878",
    "https://openalex.org/W2914120296"
  ],
  "abstract": null,
  "full_text": "Roshanzamir et al. \nBMC Med Inform Decis Mak           (2021) 21:92  \nhttps://doi.org/10.1186/s12911-021-01456-3\nRESEARCH ARTICLE\nTransformer-based deep neural network \nlanguage models for Alzheimer’s disease risk \nassessment from targeted speech\nAlireza Roshanzamir1, Hamid Aghajan2 and Mahdieh Soleymani Baghshah1* \nAbstract \nBackground: We developed transformer-based deep learning models based on natural language processing for \nearly risk assessment of Alzheimer’s disease from the picture description test.\nMethods: The lack of large datasets poses the most important limitation for using complex models that do not \nrequire feature engineering. Transformer-based pre-trained deep language models have recently made a large leap in \nNLP research and application. These models are pre-trained on available large datasets to understand natural lan-\nguage texts appropriately, and are shown to subsequently perform well on classification tasks with small training sets. \nThe overall classification model is a simple classifier on top of the pre-trained deep language model.\nResults: The models are evaluated on picture description test transcripts of the Pitt corpus, which contains data of \n170 AD patients with 257 interviews and 99 healthy controls with 243 interviews. The large bidirectional encoder \nrepresentations from transformers  (BERTLarge) embedding with logistic regression classifier achieves classification \naccuracy of 88.08%, which improves the state-of-the-art by 2.48%.\nConclusions: Using pre-trained language models can improve AD prediction. This not only solves the problem of \nlack of sufficiently large datasets, but also reduces the need for expert-defined features.\nKeywords: Alzheimer’s disease, Early risk assessment, Picture description test, Deep learning, Transformer, Natural \nlanguage processing, Language model, Transfer learning\n© The Author(s) 2021. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creat iveco mmons .org/licen ses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creat iveco \nmmons .org/publi cdoma in/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nBackground\nAlzheimer’s disease (AD) is the most common type of \ndementia which currently cannot be cured or reversed \n[1]. According to the World Alzheimer Report 2019, \nthere were over 50 million people living with demen -\ntia in the world as estimated by Alzheimer’s Disease \nInternational (ADI), while the projected estimates for \n2050 reach above 150 millions [2]. The common symp -\ntoms of AD include decreased awareness, disinterest in \nunfamiliar subjects, increased distraction, and speech \nproblems [3]. However, if the disease is diagnosed in its \nearly stage, a series of pharmacological and behavioral \ntherapy approaches can be prescribed to reduce the pace \nor progression of the disease symptoms [4]. Clinical lev -\nels of cognitive impairment are categorized into 7 stages \nof: normal, normal ageing forgetfulness, mild cognitive \nimpairment (MCI), mild AD, moderate AD, moderately \nsevere AD, and severe AD [5]. In terms of observable \nlinguistic symptoms, in the first three stages, the partici -\npants need more time to respond and find words, or have \ntrouble to maintain focus on a conversation. In mild and \nmoderate AD stages, patients have difficulty in under -\nstanding and explaining abstract concepts, completing \nsentences, and following long conversations. In the two \nOpen Access\n*Correspondence:  soleymani@sharif.edu\n1 Department of Computer Engineering, Sharif University of Technology, \nAzadi Avenue, Tehran, Iran\nFull list of author information is available at the end of the article\nPage 2 of 14Roshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \nmost severe stages, patients cannot create grammatically \ncorrect sentences, almost lose the ability to understand \nwords, and finally, become completely mute [5–7].\nThe healthcare industry has quickly realized the impor-\ntance of data and as a result has started collecting them \nthrough a variety of methods such as electronic health \nrecords (EHR), sensors, and other sources. But analyzing \nthese data and making decisions based on them is very \ntime consuming and complicated. A large portion of this \ndata is textual which makes the analysis more challeng -\ning. On the other hand, there is a large amount of infor -\nmation and hidden relationships in these textual data, \nand extracting this information is difficult for humans. \nIn this regard, the use of machine learning and natu -\nral language processing (NLP) to analyze these data and \ninference based on the performed analysis has received \nincreased attention. Moreover, according to the recent \nincreasing power of deep learning techniques and their \nability to extract complex relationships, employing these \nmethods in medical text mining problems has been met \nwith increased interest in recent years. Given the impor -\ntance of the impact of AD on speech abilities of the \npatients, this study aims to develop a technique for AD \nrisk assessment from transcripts of targeted speech elic -\nited from the participants.\nThe task for acquiring speech data from the patients is \nthe Cookie-Theft picture description test [8]. Initially, the \ntest was used as a part of the Boston Diagnostic Apha -\nsia Examination [8] assessment tool which was designed \nfor diagnosing aphasia. Currently, the test is commonly \nused by speech-language pathologists to assess abnor -\nmal language production in patients with disorders such \nas aphasia, AD, right hemisphere lesions, schizophrenia, \nand etc [9]. In this test, an image is shown to the par -\nticipant and they are asked to describe what they see in \nit. Generally, the Cookie-Theft image includes a mother \nwashing the dishes in a sink while children try to steal \ncookies from a cookie jar.\nUnlike most earlier studies, the features are extracted \nin our approach by the model itself in an unsupervised \nmanner. As a result, more complex features are discov -\nered and used for prediction. More precisely, the models \nare pre-trained on a large dataset to learn a good high \ndimensional (such as 1024 dimensions) vector repre -\nsentation for the input sentence or text, which will be \nused as input to AD versus healthy control (HC) clas -\nsifiers. Another approach taken in this study to address \nthe problem of insufficiently-sized datasets is text aug -\nmentation. Similar to most related works, the methods \nare evaluated on the Cookie-Theft picture description \ntest transcripts of the Pitt corpus [10] from the Dementi -\naBank [10] dataset. As mentioned earlier, the overall clas-\nsification framework takes raw interview text as input. \nOur evaluation shows that pre-trained deep transformer-\nbased language models with a simple logistic regression \nclassifier work well in AD risk assessment and the results \ngenerally outperform those of the existing methods while \nthe proposed method does not require any hand-crafted \nfeatures for training the classifier.\nRelated work\nFeature‑based approaches\nFor the first time, a computational approach to diag -\nnosing Alzheimer’s disease using speech in English was \nintroduced by Bucks et al. [11]. In that study, 8 AD and \n16 HC participants were asked to speak about themselves \nand their experiences in 20–45 min sessions, and finally, \nsome specific questions were also asked. Then, a num -\nber of linguistic features such as the noun rate, adjective \nrate, pronoun rate, and verb rate were extracted from the \nrecorded speech and their distribution for the AD and \ncontrol samples were used to train a classifier. Since then, \nmany other studies have been conducted on this topic \nto improve the accuracy of AD prediction and study the \nvarious dimensions of AD (and other types of dementia) \neffects on speech. In general, most of these methods pro -\npose improvements based on increasing the number of \nexpert-defined features [12, 13], increasing the number \nof participants [14], using acoustic features in addition \nto linguistic ones [15, 16], involving AD severity [14] and \nother types of dementia scores in classification [17], con -\nsidering the impact of AD on other types of diseases [18], \nchanging the interview’s structure [16], using linguistic \nimpairment for predicting AD onset [19], and relating \nlinguistic features to neuropsychological tests [19].\nOne of the most comprehensive studies on this topic \nwas conducted by Fraser et  al. [20]. In that study, an \nextensive categorization of linguistic features was pre -\nsented, in which linguistic features were categorized into \nPOS (part-of-speech) tags, syntactic complexity, gram -\nmatical constituents, psycho-linguistics, vocabulary rich -\nness, information content, repetitiveness, and acoustics. \nAlso, the study categorized all different kinds of language \ndisorders into the four groups of semantic impairment, \nacoustic abnormality, syntactic impairment, and infor -\nmation impairment. The paper collected 370 linguistic \nfeatures from the data and reported the topmost 35 of \nthese features for AD prediction.\nIn all earlier works, in order to automatically diag -\nnose the disease using speech, information content \nunits were introduced by human experts, and a classi -\nfier used them in order to predict the participant’s cat -\negory. However, Yancheva et al. [21] and Sirts et al. [22] \ntried to enrich and enhance information content units \nof targeted speech by clustering pre-trained global vec -\ntor (GloVe) [23] embedding of words used by AD and \nPage 3 of 14\nRoshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \n \nHC participants. Using the mentioned clusters, they \nintroduced some cluster-based measures which were \nused along with a number of standard lexicosyntactic \nand acoustic features for AD prediction.\nIn languages other than English, Khodabakhsh et  al. \n[24] and Weiner et  al. [25] respectively examined the \nsubject in Turkish and German. Also, Li et  al. [26] \nand Fraser et  al. [27] both focused on a  multilingual \napproach for diagnosing AD using targeted speech. \nThey respectively tried to improve the AD prediction \nin Chinese and French languages (in which the existing \ndatasets were insufficient) using an English classifier \ntrained on a larger English dataset.\nDeep learning‑based approaches\nFor the first time, Orimaye et al. [28] used a deep neu -\nral network to predict MCI using speech. Unlike most \nprevious works, that study did not use any hand-crafted \nfeatures and the raw transcripts were fed to the model. \nThe dataset used in the study was part of the Pitt cor -\npus of the DementiaBank dataset, comprising 19 MCI \nand 19 control transcripts of the Cookie-Theft picture \ndescription test. They trained a separate deep neural \nnetwork language model for each category, and then \ncalculated the likelihood of the text in both language \nmodels. Finally, the class of the model with higher \nprobability was selected.\nKarlekar et  al. [29] also used a deep neural network \nmodel to diagnose AD using four types of interviews: \nthe Cookie-Theft picture description, sentence con -\nstruction, story recall, and vocabulary fluency which \nincluded an unbalanced 243 HC and 1017 AD tran -\nscripts. Three classifiers: a convolutional neural net -\nwork (CNN), a long-short term memory recurrent \nneural network (LSTM-RNN), and a CNN-LSTM were \ntrained, taking sentences as sequences of pre-trained \nword embedding. In addition to AD diagnosis, the \nauthors interpret the models using activation clustering \nand first derivative saliency heat map techniques which \ncluster the most significant utterances. The research \nused a highly unbalanced dataset, rendering the results \nsomewhat questionable as discussed in Sect. “ Why not \nuse the entire Pitt corpus?”.\nFritsch et  al. [30] used two different auto-regressive \nLSTM-based neural network language models to clas -\nsify AD and HC transcripts of the Pitt corpus from \nthe DementiaBank dataset. After that, Pan et  al. [31] \nworked on predicting AD using a stacked bidirectional \nLSTM and gated recurrent unit (GRU) layers equipped \nwith a hierarchical attention mechanism. The overall \nmodel takes the GloVe word embedding sequence as \ninput.\nMethods\nIn this study, the most challenging problem in develop -\ning data-driven (i.e., machine learning-based) methods \nfor recognizing Alzheimer’s patients from speech tran -\nscripts is the lack of a large dataset. Currently, the largest \navailable dataset is the Pitt corpus from the Dementi -\naBank dataset, which contains 500 picture description \ninterviews from the AD and control groups. For the \nmentioned reason, most of the earlier work was based \non features designed by experts, as it was not possible to \nuse models capable of learning informative features by \nthemselves. In this study, we employ the idea of utilizing \na highly pre-trained language model to address this issue. \nMoreover, data augmentation techniques are also utilized \nto alleviate the small dataset problem. Our implementa -\ntion of these ideas is described next.\nOverall classification framework\nThe overall process of classification is summarized in \nFig.  1. The process consists of five layers. Each layer \nuses the output of the previous layer as input. The aug -\nmenter layer enriches the dataset with methods that will \nbe introduced in Sect. “Dataset augmentation ” . Note that \nthis layer will be disabled in the test phase. The splitter \nlayer is optional and chooses whether we want to process \nthe whole text at once or break it down into sentences \n(and specify the final result by aggregating the results \non sentences). It could be disabled by being set to the \nidentity function when we intend to work on the whole \ntranscript. The embedder layer embeds each input ele -\nment (i.e. the entire transcript or a sentence) to a high-\ndimensional representation vector, and the classifier \nlayer predicts the label of each embedded input. In fact, \nthe classifier layer learns which of (and to what extent) \nthe features that BERT (or other embedders) offers are \nsuitable for predicting Alzheimer’s disease. Finally, if the \nclassifier layer outputs multiple labels (that may happen \nwhen working on sentences), the voter makes the final \ndecision using a majority voting mechanism. A layered \narchitecture makes it much easier to combine different \nsettings and understand the final model.\nIn our implementation, the augmenter and embedder \nlayers are trained outside the classification framework \nand are only used there. Therefore, if there is a pre-\ntrained embedding layer, training and inference will be \ndone very quickly. Details on how to train these layers are \nexplained in the following sections.\nIn this study, depending on the use of the splitter layer, \ntwo different approaches for classifying a transcript are \nimplemented. In the first approach, the entire transcript \nis passed to an embedder and then the embedded tran -\nscript is directly classified. In this approach (from now \non we will call it the text-level approach), the splitter \nPage 4 of 14Roshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \nand voter layers are disabled. In the second approach, \nthe transcript is first split into sentences, and then these \nsentences are embedded and are subsequently classified. \nFinally, the label of the entire transcript is decided by \nmajority voting on the labels of all sentences in the tran -\nscript. The second approach (from now on we will call it \nthe sentence-level approach) is more compliant with pre-\ntrained embedders since they are mostly pre-trained on \nsingle- or two-sentence inputs.\nPre‑trained deep language model\nA model that defines a probability distribution over a \nsequence of words is called a language model. If a compu-\ntational model wants to implement a language model, it \nis necessary to have a good understanding of the syntac -\ntic and semantic structures of that language. Therefore, \nusing a model that has already learned a probabilistic dis-\ntribution that correlates with these structures for classifi -\ncation reduces the need for large target-specific datasets. \nThe transfer of knowledge from one model to another \nwith a similar purpose is called transfer learning. We use \ntransformer-based language models that have offered a \nbreakthrough in many language understanding tasks in \nrecent years [32]. The general flow of using a pre-trained \nlanguage model for classification consists of three steps: \n1 Unsupervised training of the general language model \non a large dataset (such as Wikitext).\n2 Unsupervised fine-tuning of the pre-trained language \nmodel on the target dataset (such as the Cookie-\nTheft picture description transcripts).\n3 Using (with or without supervised fine-tuning) the \ntarget-specific pre-trained language model for the \nclassification task.\nTo address the problems facing recurrent models such \nas the issue of short-term memory and the challenges \nfacing the parallelization of training, Vaswani et  al. [33] \nintroduced transformers which consist of an extreme use \nof the attention mechanism that underpins many NLP \nFig. 1 Overall classification procedure. The classification procedure consists of the steps of augmentation, splitting, embedding, classification, and \nvoting, where augmentation is only used in the training phase. Also, when passing the entire transcript to the embedding layer, the splitting and \nvoting layers are disabled. The underlined models are trainable here, and the others are fixed\nPage 5 of 14\nRoshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \n \nmodels. The paper argues that the attention mechanism \nallows the model to focus on certain parts of the text for \ndecision making. This functionality makes the attention \nmechanism useful for modeling biomarkers related to \nAD.\nAl-Rfou et al. [34] used transformers for the first time \nas essential elements of a character-level language model. \nAfter that, Dai et al. [35] extended the model using rela -\ntive positional encoding and segment-level recurrence. \nAs a turning point in the transformer-based language \nmodels, we can refer to the bidirectional encoder repre -\nsentations from transformers (BERT) model proposed \nby Devlin et al. [36] at Google. In the training phase, the \ninput sentence is masked, which means 15% of tokens \nare replaced with the [MASK] token, and the model tries \nto learn such representation or embedding for the con -\ntext that considers both syntax and semantics to pre -\ndict the masked token using the context. On the other \nhand, in the test phase the model takes in a raw sentence \nfrom one or multiple languages and returns a 768- or \n1024-dimensional vector representation of the input text \nto be used as input to other classifiers such as LR, MLP , \netc. An enhanced version of BERT for multilingual lan -\nguage understanding tasks was introduced by Conneau \net  al. [37], called cross-lingual language model (XLM), \nwhich benefits from using the translated language model \n(TLM) as well as the masked language model (MLM). \nUnlike BERT, XLM takes two related masked sentences \nfrom two different languages and tries to predict masked \ntokens using the same and the other language input sen -\ntences. This allows XLM to understand multilingual texts \nbetter. Also, BERT suffers from the train and test phase \ndiscrepancy and independent prediction of masked \ntokens. To correct this, Yang et  al. [38] introduced an \nextended large network (XLNet) model based on a lan -\nguage model called Permutation Language Model.\nThe use of multilingual models offers a practical solu -\ntion to the problem of lacking of a large dataset in many \nlanguages. As there is a limited collection of text data \nfrom Alzheimer’s patients in many languages, training a \nmultilingual model in a source language (in which such \nlarge datasets are available) and applying it to making \ninference in the target language can offer a valuable solu -\ntion. On the other hand, a number of language features \nthat experts introduce are either specific to a particular \nlanguage or their implementation may be different in \ndifferent languages. Using multilingual models can also \nmitigate the need for such transfer of expert features \nbetween various languages.\nIn the current study, we use pre-trained BERT, XLNet, \nand XLM as deep networks for text embedding which \nconvert raw participant transcripts/sentences to 768- or \n1024-dimensional vectors. More precisely, to use these \nlanguage models for the embedding layer described in \nSect. “Overall classiffication framework” , the entire tran -\nscript (in the text-level approach) or sentence (in the sen-\ntence-level approach) are passed to the model, and then, \nthe last layer embedding of the [CLS] token is considered \nas the embedding of the entire input. The embedding \nmodels (which are used in this study as an embedder \nlayer) are only passed through Phases 1 and 3 of the flow \ndescribed earlier in this section. The reason for this is \nthat the employed dataset is insufficient for unsupervised \nfine-tuning (of language models on the target dataset) \neven when using vast augmentation methods. In practice, \nusing unsupervised fine-tuning is likely to have minimal \nimpact on the overall performance of the model used \nin the current research (the effect of this feature on the \nresults of the implemented model with the best perfor -\nmance is presented in Sect. “Evaluation results”). For \nthe first phase, all embedding models are pre-trained \nwith the corpus mentioned in the main article, and their \nimplementation is taken from the HuggingFace trans -\nformers library [39].\nDataset augmentation\nAnother approach to overcome the lack of access to \nlarge training input is dataset augmentation which \nmeans increasing the number of labeled samples of the \ndataset using some probabilistic or even heuristic algo -\nrithms. For example, the word “beautiful” in a sentence \nsuch as “What a beautiful car!” can be replaced with the \nword “nice” without changing the meaning of the sen -\ntence a lot. Augmentation in NLP can be done at differ -\nent levels of linguistic units, and in this study, the word \nand sentence level augmentations are used for enriching \nthe dataset. The most crucial challenge of augmenta -\ntion in the text classification task is preserving the text \nclass during augmentation. For example, a probabilistic \nmodel can replace “beautiful” with “dirty”  in the men -\ntioned sentence, which is grammatically and semantically \ncorrect but changes the sentence category. Two general \napproaches to augmentation have been used in this study, \nwhich are described below.\nSimilar word substitution augmentation\nIn this approach, a similarity measure must first be \ndefined. The most obvious definition of similarity for \nwords is the synonym relation which was first used in \nthe field of deep learning by Zhang et  al. [40] using the \nWordNet database [41]. Another common similarity \nmeasure is the inverse of the Euclidean distance or the \nCosine similarity between word embeddings which was \nfirst used by Wang et al. [42]. In the mentioned methods, \nthere is no guarantee of the correct grammar in the out -\nput sentence. It is also possible that the output sentence \nPage 6 of 14Roshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \ncategory changes by augmentation. For example, one \nof the markers of Alzheimer’s disease is the reduction \nin the vocabulary used in the conversation, so replac -\ning a simple word like “Delicious” with its sophisticated \nsynonym like “Scrumptious” can change the sentence \ncategory from patient to healthy and mislead the classi -\nfier. Another method that considers grammatical correct-\nness along with the sentence context was introduced by \nKobayashi [43] and is called contextual augmentation. In \nthe contextual augmentation method, there is a language \nmodel which takes both the word’s context (i.e. the sen -\ntence that contains the word) and the whole sentence’s \ncategory and returns a probability distribution over all \nvocabulary. Augmentation is done by sampling from \nthe returned probability distribution. Kobayashi [43] \ntrained a Bi-Directional LSTM language model with this \napproach, and Wu et al. [44] enhanced the approach by \nusing BERT as an underlying model.\nAll the mentioned methods were evaluated in this \nstudy, and the implementation was done using the \nNLPAug library [45] except for contextual augmentation \nfor which the released code by the authors of [43] was \nused.\nSentence removal augmentation\nAnother ad-hoc approach which does not change the \nsentence category and also retains grammatical correct -\nness is sentence removal. In this approach, one sentence \nis removed from the transcript, and it is expected that \nthe output is still a valid transcript in the same category. \nAlthough it can be argued that the label may be changed \nby reducing the length of the text, considering the results \nof using or not using this idea, it is appropriate to use it in \nmodels that process the entire text at once (not sentence \nby sentence).\nBaseline models\nIn this study, in addition to the transformer-based \nmodels, bidirectional-LSTM and convolutional neu -\nral networks over the GloVe word embedding were also \nevaluated as baseline models to illustrate the advan -\ntages of pre-trained transformer-based deep language \nmodels over conventional deep models. In these mod -\nels, the entire transcript is used as input. The reason for \nthis decision is that unlike pre-trained models, there is \nno pre-training on single-sentence (or maximum two-\nsentence) texts and hence their training has to be done \nfrom the beginning. Therefore, splitting the transcript \ninto sentences will not improve the performance of these \nmodels. In the CNN model, each transcript (truncated or \npadded to T number of words) is converted to a sequence \nof embedded words. Then the sequence is passed to \na number of stacked convolutional and max-pooling \nlayers followed by fully-connected layers and finally a \nsigmoid output layer that yields P(AD |transcript). Also, \nin the bidirectional-LSTM model, the embedded word \nsequence is passed to a number of stacked forward and \nbackward LSTM cells followed by fully-connected lay -\ners and a sigmoid output layer in a similar fashion. \nStructurally, if we move forward in the CNN layers, the \nmodel tries to conclude more semantic features using \nspatially close features in the previous layer. But in the \nLSTM model that considers long range dependencies, an \nattempt is made to learn new compound features from \nfeatures of all previous steps (or from features of the \nwhole sequence in the bidirectional LSTM). The main \nweakness of this model is the forgetting of distant fea -\ntures (spatially) to produce new compound features. In \nboth of these models, there is no attention mechanism.\nExperimental setup\nIn this section, we describe our implemented methods \nand their corresponding settings in the training and eval -\nuation phases.\nImplemented methods\nFor each layer of the overall framework, there were sev -\neral options from which the following were implemented. \nFor the augmenter layer, synonym-substitution and con -\ntextual augmentation were implemented along with ad-\nhoc sentence removal augmentation. As implemented by \nKobayashi et al. [43], the corresponding language model \nused in contextual augmentation was a single layer bidi -\nrectional LSTM. For the splitter layer, in addition to \nthe identity function, the sentence splitter was imple -\nmented for the sentence-level approach. For the pre-\ntrained embedder layer, BERT (base and large), XLNet \n(base and large), XLM, and the GloVe word embedding \nsequence (50-dimensional version) were investigated. For \nthe classifier layer, logistic regression, single hidden layer \nneural network, single-layer bidirectional LSTM, and \nthree-layer CNN were examined. Finally, for the voter \nlayer, in addition to the identity function, majority voting, \nand a single-layer bidirectional LSTM were implemented \nfor the sentence-level approach. Although different com -\nbinations of layers were implemented, only significant \ncases of each group have been reported in Sect. “Evalu -\nation results” .\nTraining settings\nFor the contextual augmentation, as implemented by \nKobayashi et al. [43], the cross-entropy loss function and \nthe Adam optimizer was used. The number of augmen -\ntations per transcript is a hyper-parameter for the aug -\nmentation layer. For the pre-trained embedding layer, \nonly the HuggingFace transformers library [39] was used \nPage 7 of 14\nRoshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \n \nand no additional training was done on the implemented \nmodels. For the classification layer, binary cross-entropy \nwas employed for the loss function and the Adam opti -\nmizer was used to minimize it. For the voter layer, only \nbidirectional LSTM was trainable for which, again the \nbinary cross-entropy loss function and the Adam opti -\nmizer were utilized. All models were evaluated using \n10-fold cross-validation without stratified sampling.\nResults\nDataset\nThe models are evaluated on the transcripts of the \nCookie-Theft picture description test of the Pitt corpus \nfrom the DementiaBank dataset, which contains 170 pos-\nsible or probable AD patients with 257 interviews and 99 \nhealthy control (HC) participants with 243 interviews.\nMost of the data were gathered as a part of the Alzhei -\nmer’s and related dementias study at the University of \nPittsburgh School of Medicine between 1983 and 1988. \nThe interviewer shows the participant the Cookie-Theft \npicture and asks him/her to state everything he/she sees \nin it. The audio records of all interviews were manually \ntranscribed and annotated with POS-tags in the CHAT \n[46] format. Detailed demographics of the data is speci -\nfied in Table 1.\nWhy not use the entire Pitt corpus?\nSome earlier studies based on the Pitt corpus (such \nas Kerlekar et  al. [29]) used all the tests of the corpus \nincluding the Cookie-Theft picture description, story \nrecall, sentence construction, and categorical/verbal flu -\nency for classification purposes. The first problem with \nusing the entire corpus is that the corpus is highly unbal -\nanced (note that Table  1 only provides demographics of \nthe Cookie-Theft picture description test from the Pitt \ncorpus, which is perfectly balanced, although the whole \ndataset is unbalanced and the number of AD/HC sam -\nples are 846/244 in the overall corpus), and as a result, a \nnaïve classifier that always outputs AD labels can achieve \na classification accuracy of 78% on such a dataset.\nThe second problem is that except for the Cookie-Theft \npicture description test, the Pitt corpus was only admin -\nistered to AD subjects for all the other tests, which means \nthat the classifier might learn invalid features for AD pre-\ndiction. For example, a classifier may just output an AD \nlabel by checking if the input is not from the Cookie-\nTheft picture description test, and otherwise, work as \nnormal. Using this approach, a normal classifier with 80% \naccuracy can achieve approximately 92% accuracy on the \nwhole Pitt corpus. Figure  2 provides an example of this \nproblem. The figure shows visualized two-dimensional \ntSNE [47] diagram for the  BERTBase embedding of the \nentire transcripts of all tests in the Pitt corpus. According \nto the figure, the tests are completely differentiable, and \nas a result, the mentioned problem is quite probable to \narise. Thus, in Sect. “Results” , studies based on the entire \ncorpus were not included.\nEvaluation measures\nThe most well-known measure to evaluate classifica -\ntion is the accuracy score which is the fraction of pre -\ndictions the model performed correctly. Most related \nstudies have reported accuracy as the quality of their \nclassification models and tried to improve this measure \nas an important goal. As discussed in the previous sec -\ntion, the accuracy measure alone does not provide a \ncomplete interpretation of the model performance (for \nexample, high accuracy can be achieved using the entire \nPitt corpus, while the model performance is not sufficient \nfor practical use). Two other practical measures are pre -\ncision and recall (also called sensitivity). In this study, \nprecision is the number of correct AD predicted samples \nover the total number of AD predicted samples and recall \nis the number of correct AD predicted samples over the \ntotal number of AD samples. These two measures should \nbe examined together and for this reason, the  F1 score is \ndefined. The  F1 score is the harmonic mean of the preci -\nsion and recall measures. A combined high precision and \nrecall results in a high  F1 score. In other words, highly \nimbalanced precision and recall indicates that the model \nhas not an approximately equal performance for detect -\ning all labels. All the aforementioned measures are in the \nrange of zero to one, and can be reported as a percentage. \nCompared to the accuracy score, fewer previous studies \nhave reported recall, precision, and  F1 measures. In this \nFig. 2 Visualized tSNE dimensionality reduction for the  BERTBase \nembedding of the entire Pitt corpus\nPage 8 of 14Roshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \nstudy, all the introduced measures are reported to make \nit possible to compare our work more comprehensively \nwith previous works.\nCompared methods\nWe compared the results of our models with all related \nstudies that evaluated their models on the Cookie-Theft \npicture description test of the Pitt corpus. Therefore, the \nbest models (according to the introduced performance \nmeasures) are selected for comparison. The first one \nis the method introduced in [20] which maintained the \nstatus of having the state-of-the-art accuracy score for \nseveral years. The second compared method was intro -\nduced by Yancheva et  al. [21]. They tried to enrich and \nenhance human-supplied information content units by \nclustering GloVe embedding of frequent words of each \ncategory. After that, Sirts et  al. [22] extended the idea \nof Yancheva et al. [21] by introducing propositional idea \ndensity features that work better on free-topic conver -\nsational speech. Hernández et  al. [48] introduced 105 \nhand-crafted features and used them to train a support \nvector machine (SVM) classifier. They reported all the \nwell-known and informative measures for the classifica -\ntion tasks and also achieved good results. Fritsch et  al. \n[30] trained two different auto-regressive LSTM-based \nlanguage models for each group and classified each tran -\nscript by calculating its perplexity on the models and \nselecting the model corresponding to the lowest perplex -\nity. Currently, that study has the best recall and accuracy \nscores for AD versus HC classification on the target data-\nset. Pan et al. [31] utilized a stacked bidirectional LSTM \nand GRU recurrent units equipped with a hierarchical \nattention mechanism. Up to now, this study has the best \nprecision and  F1 scores for AD versus HC classification \non the target dataset. The last two studies by Li et al. [26] \nand Fraser et  al. [27] were focused on multilingual AD \nprediction and hence their main goal was not to improve \nthe unilingual classification. Li et  al. [26] used 185 lexi -\ncosyntactic features for a logistic regression classifier and \nFraser et al. [27] utilized class-based language modeling \nand information-theoretic features for an SVM classifier.\nEvaluation results\nTable 3 reports precision, recall, accuracy, and  F1 scores \nof the compared methods as well as those of the proposed \nmethods in the framework introduced in this paper. The \nreported scores are averaged on a 10-fold cross-valida -\ntion (without stratified sampling) procedure. Note that \nfor the Fritsch et al. [30] method there is no such entity \nas a classifier and classification was performed by evalu -\nating perplexity of input transcripts on the trained lan -\nguage models of both classes. As mentioned earlier, two \ndifferent approaches have been implemented to use the \npre-trained embedders, the first one is passing the entire \ntext to the embedder (specified by a T- prefix in the \nmethod’s name) and the second one is passing each sen -\ntence of the text to the embedder separately (specified by \nan S- prefix in the method’s name). All the methods with \nthe first approach have been enriched by the one-sen -\ntence-removal augmentation method. Furthermore, the \nCNN method is used with the synonym substitution aug-\nmentation (SSA) method and the BiLSTM is used with \nthe SSA and contextual augmentation (CA) methods sep-\narately. The CA and SSA augmentations had almost no \neffect on the methods which used pre-trained language \nmodels, so they are not reported in Table  3. Also, for the \nmodel with the best accuracy score (S-BERT Large-LR), \ntwo additional versions with bidirectional LSTM classi -\nfier (S-BERTLarge-BiLSTM) and bidirectional LSTM voter \n(S-BERTLarge-LR-BiLSTM) are included.\nAs mentioned before, it seems that using unsupervised \nfine-tuning (using the MLM objective and next sentence \nprediction) on the Cookie-Theft picture description tran-\nscripts of the Pitt corpus does not have much effect on \nthe results due to the lack of sufficient data for the tar -\nget task. According to the experiments performed, using \nunsupervised fine-tuning for the model with the best \naccuracy score (S-BERTLarge-LR in equivalent settings on \naverage results in the accuracy and  F1 scores of 87.89% \nand 86.11%, which are almost no different from the \nscores of a version without this feature (note that due to \nthe fundamental differences of this approach with other \nmodels, we did not include it in Table 3).\nMoreover, Fig.  3 illustrates the mean 10-fold cross-\nvalidation classification accuracy, true positive rate (the \nnumber of correct predicted AD samples over total \nnumber of AD samples, also called the sensitivity), and \ntrue negative rate (the number of correct predicted HC \nsamples per total number of HC samples, also called \nthe specificity) plotted versus the mini-mental state \nexam (MMSE) [49] scores of the participants. The fig -\nure helps us to see how the model works for detecting \nlabel of participants with different AD severity levels. \nThe true positive rate for each MMSE score represents \nthe model performance in detecting AD from actual \nAD patients in that score. Similarly, the true negative \nrate represents the model performance in detecting HC \nlabel from actual HC participants in that score. Totally, \nthe accuracy score represents the model performance \nin detecting the correct label from both participant \ngroups in the corresponding MMSE score. Numbers in \nthe pink bars are true positive rates and in the blue bars \nare true negative rates. Also, the numbers on top of the \nbars are the total mean accuracy for that MMSE score. \nNote that all of the rates are scaled between 0 and 1. \nThe MMSE scores were not reported in the dataset for \nPage 9 of 14\nRoshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \n \nsome participants while their AD/HC labels were pre -\nsent. The results for these participants are grouped in \nthe “Unspecified” bar in this figure.\nIn addition to classification, models such as logis -\ntic regression and neural networks with a sigmoidal \nfinal activation function can also output the AD prob -\nability (or 1—health probability) of the current input. \nReferring to the continuity of linguistic impairments \nfrom perfect health to severe AD, this probability can \nbe interpreted as a correlated variable to the sever -\nity of the AD condition of the participant. Therefore, \nanother approach for interpreting the models and \nevaluating them is calculating the similarity between \ntheir predicted health probability and the MMSE score, \nscaled between 0 and 1. The results using two common \nsimilarity measures, the Pearson correlation and Spear -\nman’s rank correlation (which is the Pearson correlation \non the samples’ ranking), are reported in Table  2. Both \nmentioned correlation measures are reported between \n− 1 and 1.\nDiscussion\nInterpretation of results\nAccording to Table  3, among the models that use only \nhand-crafted features, Fraser et  al. [20] reports the best \naccuracy score, although it has not reported other evalua-\ntion measures. Among the baseline models introduced in \nour study (CNN + SSA, BiLSTM + SSA, and BiLSTM + \nCA), which are conventional deep neural network mod -\nels, the contextual augmented version of bidirectional-\nLSTM achieved the highest accuracy score of 77.36%. \nHowever, even with the extreme use of augmentation \nmethods these baseline models did not yield acceptable \nFig. 3 Mean 10-fold cross-validation classification accuracy, true positive rate, and true negative rate\nTable 1 Demographics of Cookie-Theft picture description test \nof the Pitt corpus\nAD HC\nParticipants 170 99\nSamples 257 243\nAge (years) 71.7 ± 8.5 64.2 ± 7.9\nGender (male/female) 87/170 88/155\nMini-mental state exam 18.6 ± 5.1 29.1 ± 1.1\n Number of words 100.9 ± 58.3 111.5 ± 57.2\nTable 2 The similarity between predicted health scores of the \nS-BERTLarge-LR model and MMSE [49] scores\nMeasure\nPhase Pearson correlation Spearman’s \nrank \ncorrelation\nTrain 0.78 0.81\nValidation 0.70 0.74\nPage 10 of 14Roshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \nresults compared to other methods. Overall, the sen -\ntence-level  BERTLarge embedding of sentences passed to \nlogistic regression (S-BERT Large-LR method) achieved \nthe highest accuracy score (88.08%) among all the mod -\nels introduced in this study as well as the models used \nin previous studies, and improved the accuracy score by \n2.48% (equivalently 17.22% error-rate reduction). At the \nsame time, this model achieved the best precision and \n F1 scores with 6.55% and 2.80% improvements, respec -\ntively. Still, Fritsch et al. [30] showed the best recall score \nwith 1.66% difference although they did not report the  F1 \nmeasure. The first advantage of our proposed methods \ncompared to Fritsch et  al. [30] is that we train a single \nlanguage model for both the AD and HC groups which \nTable 3 AD versus HC classification scores\nOther settings of the proposed framework with different classifiers or augmenters which did not have significant effects on the scores are not shown\nMethod Embedding Classifier Precision Recall Accuracy F1\nFraser et al. [20] 35 Hand-Crafted\nFeatures\nLR – – 81.92 –\nYancheva et al. [21] 12 Cluster-Based\nFeatures + LS&A\nRandom forest 80.00 80.00 80.00 80.00\nSirts et al. [22] Cluster+PID+SID\nFeatures\nLR 74.4\n±1.5\n72.5\n±1.2\n- 72.7\n±1.2\nHernández et al. [48] 105 Hand-Crafted\nFeatures\nSVM 81.00 81.00 79.00 81.00\nFritsch et al. [30] One-Hot Word\nEmbedding Sequence\n– – 86 85.6 –\nPan et al.\n[31]\nGloVe Word\nEmbedding Sequence\nBi-LSTM GRU \nHierarchical Attention\n84.02 84.97 – 84.43\nLi et al. [26] 185 Hand-Crafted\nFeatures\nLR – – 77 –\nFraser et al. [27] Info and\nLM Features\nSVM – – 75 77\nCNN + SSA GloVe Word\nEmbedding Sequence\nCNN 76.38\n±8.49\n77.47\n±8.97\n76.48\n±5.88\n76.36\n±5.91\nBiLSTM + SSA GloVe Word\nEmbedding Sequence\nBi-LSTM 74.71\n±1.92\n75.00\n±14.82\n75.51\n±5.77\n74.22\n±8.71\nBiLSTM + CA GloVe Word\nEmbedding Sequence\nBi-LSTM 78.40\n±6.60\n73.95\n±12.96\n77.36\n±6.19\n75.43\n±7.83\nT-BERTBase-LR BERTBase\n(Text Level)\nLR 85.09\n±3.11\n78.69\n±8.35\n82.76\n±3.74\n81.51\n±4.73\nT-BERTLarge-LR BERTLarge\n(Text Level)\nLR 88.21\n±5.33\n80.86\n±7.58\n85.10\n±3.43\n84.04\n±3.93\nT-XLNetBase-LR XLNetBase\n(Text Level)\nLR 84.74\n±6.31\n79.26\n±7.72\n81.92\n±5.88\n81.75\n±6.19\nT-XLNetLarge-LR XLNetLarge\n(Text Level)\nLR 82.30\n±5.15\n83.83\n±4.34\n82.87\n±3.14\n82.86\n±2.60\nT-XLM-LR XLM\n(Text Level)\nLR 80.31\n±5.29\n79.13\n±8.43\n80.21\n±4.94\n79.49\n±5.76\nS-BERTBase-LR BERTBase\n(Sentence Level)\nLR 90.31\n±7.36\n76.52\n±8.06\n84.46\n±6.31\n82.72\n±7.21\nS-BERTLarge-LR BERTLarge\n(Sentence Level)\nLR 90.57\n±3.18\n84.34\n±7.58\n88.08\n±4.48\n87.23\n±5.20\nS-BERTLarge-LR-BiLSTM BERTLarge\n(Sentence Level)\nLR 89.06\n±5.19\n77.71\n±7.33\n85.19\n±4.92\n83.61\n±5.69\nS-BERTLarge-BiLSTM BERTLarge\n(Sentence Level)\nBiLSTM 87.98\n±5.31\n75.03\n±5.99\n83.43\n±5.51\n81.49\n±5.31\nS-XLNetBase-LR XLNetBase\n(Sentence Level)\nLR 83.19\n±6.39\n74.34\n±8.12\n80.00\n±5.48\n78.32\n±6.16\nS-XLNetLarge-LR XLNetLarge\n(Sentence Level)\nLR 76.95\n±6.62\n71.30\n±8.29\n75.31\n±5.56\n73.75\n±6.14\nS-XLM-LR XLM (sentence level) LR 84.00\n±4.74\n73.47\n±9.80\n80.21\n±5.47\n78.14\n±6.72\nPage 11 of 14\nRoshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \n \nhelps the model to use samples from both classes for the \ndesired task. The other advantage is that our models are \nhighly pre-trained on large datasets which enables them \nto start training on new, smaller datasets with good ini -\ntialization parameters and also avoid overfitting.\nAmong the methods evaluated in this study, on aver -\nage, the models based on the BERT family of embedders \nworked better than the others. Although XLNet has his -\ntorically been designed to address BERT problems, BERT \nand its derivatives still perform better in many activi -\nties [32]. Moreover, employing word-level augmentation \ntechniques along with pre-trained deep language models \ndid not improve results (and hence the evaluation of their \nversions with augmentation was not reported in Table 3).\nTable 2 shows that the best model has a Pearson corre -\nlation of 0.78 and 0.70 for the train and validation phases, \nand a Spearman’s rank correlation of 0.81 and 0.74 for \nthese phases between the health score and the MMSE \nscore, indicating that the model has learned useful pat -\nterns for classification. Based on the reported similarity \nmeasures, it can be concluded that on average the MMSE \nscore and our model’s health score are linearly corre -\nlated. This is indeed an advantage for the proposed model \nin that while the MMSE score [49] is obtained through \na detailed interactive exam that evaluates visuospatial, \nexecutive, naming, memory, attention, language, abstrac -\ntion, delayed recall, and orientation cognitive skills, the \ndata collection task involved in the Cookie-Theft pic -\nture description test used in our model is a simple and \nshort pseudo-conversational procedure. Interestingly, the \nresults obtained in this section are related to the reported \nresults in the work of Eyigoz et al. [19]. The objective of \nthat study was to use linguistic markers to predict the \nonset of Alzheimer’s disease in cognitively normal indi -\nviduals. The study’s experiments showed that the stated \ngoal is possible to achieve and, in fact, using models \nbased on linguistic variables performed better than a \npredictive model based on non-linguistic variables (such \nas neuropsychological test results, age, gender, APOE \nε4 alleles, etc.). The results of this section show that the \nseverity of linguistic impairments is highly correlated \nwith the estimates based on non-linguistic variables (cor -\nroborating the results reported by Eyigoz et al. [19]).\nIn this study, neural network interpretation methods \nwere not used but in Table 4, two false negative and false \npositive classification errors are reported. In comparison, \nit is almost clear that the first sample has less grammati -\ncal fluency but both samples refer to similar information \nelements. In the S-BERT Large-LR model, the predicted \nAD probability is the mean of logistic regression classifier \noutputs for each sentence of the transcript. The impor -\ntant point is that in both samples, the predicted AD prob-\nabilities are very close to 0.5 which can be interpreted as \nthat the model has not learned a wrong feature, rather, it \nhas not learned a proper feature to predict AD from the \nreported samples.\nAdvantages and limitations\nAs mentioned in Sect. “Pre-trained deep language \nmodel” , the proposed approach takes advantage of the \npowerful pre-trained language models that attempt to \nlearn the structure and features of the language from a \nlarge dataset, and only uses the target dataset to learn \nhow to use these features for AD prediction. This not \nonly reduces the need for expert-defined language fea -\ntures, but also makes it possible for more complex fea -\ntures to be extracted from the data. The next advantage \nof sentence embedding models is that they consider \nthe entire raw text and there is no out-of-context word \nembedding layer that would convert each word to a rep -\nresentation vector without considering its context.\nAs mentioned earlier, even using augmentation meth -\nods, the largest currently available dataset for AD pre -\ndiction is still insufficient in size for unsupervised \nTable 4 Two invalid predicted transcripts by the model with the best accuracy score (S-BERTLarge-LR)\nPredicted AD probability ranges between 0 and 1\nTranscript Actual label Predicted label Predicted AD \nprobability\nAnd the boy in the cookie jar. And the girl reaching up to him. The \nstool slanting ready to topple. And the cookie jar is open. And the \nlid’s in there. And the door’s open. And mother’s drying the dishes \nand standing in a pool of water it looks water running down from \nthe sink. ...\nAD HC 0.483\nOkay. It was summertime and mother and the children were work-\ning in the kitchen. And the window was open and there was a \nslight breeze blowing in. Mother was daydreaming and forgot \nand left the water in the sink running and it was overflowing. The \nchildren were hungry and ...\nHC AD 0.532\nPage 12 of 14Roshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \nfine-tuning (Second phase specified in Sect. “Pre-trained \ndeep language model”) large transformer-based language \nmodels (e.g.,  BERTLarge has 340 million parameters). But \nif there is a large enough dataset, using language model \nfine-tuning, our approach can extract more complex \nand context-related features while the models based on \nexpert-defined features can only choose from a limited \nset of predefined features.\nThe most important limitation of the current study that \nneeds to be addressed in the future is that it is difficult to \nuse common neural network interpretation methods due \nto the large number of model parameters. Using inter -\npretation, we can understand why the model predicts a \nwrong label for a transcript. Also, in the case of a correct \nprediction, we can identify language features that the \nnetwork has paid more attention to. This is particularly \nuseful for studying Alzheimer’s disease as such inter -\npretation can reveal important attributes of the speech \nwhich can most effectively discriminate between the par -\nticipant groups.\nAlthough using deep embedding models instead of \nexpert (linguistic) features can improve the performance \nby extracting more complex relationships, it does not \nprovide clear features tied to clinical practice that can \nbe validated easily as opposed to expert features. In this \nregard, a suggested solution is to use interpretation tech -\nniques. But the training phase must also be conducted in \nsuch a way that the extracted features are both interpret -\nable and relatively sparse so that they could be validated \nclinically.\nFuture work\nOne of the most popular types of transformer-based lan -\nguage models is the class of multilingual models. With a \nproper use of multilingual models, similar to approaches \nby Li et  al. [26] and Fraser et  al. [27], the problem of \nlacking access to a large dataset in one language can be \naddressed by transferring the knowledge of AD predic -\ntion from another language in which a large dataset is \navailable. Using such transfer, the need to define lin -\nguistic features by experts in the target language is also \naddressed. In future work, we aim to improve multilin -\ngual AD prediction using pre-trained multilingual trans -\nformer-based language models along with cross-lingual \ntransfer learning.\nConclusions\nAccording to the results of earlier studies, Alzheimer’s \ndisease affects speech in the form of syntactic, semantic, \ninformation, and acoustic impairments. We employed \na transfer-learning approach to improve automatic AD \nprediction using a relatively small targeted speech data -\nset without using expert-defined linguistic features. We \nevaluated recently developed pre-trained transformer-\nbased language models that we enriched with augmen -\ntation methods on the Cookie-Theft picture description \ntest of the Pitt corpus. Using sentence level  BERTLarge \nwith a simple logistic regression classifier, the accuracy \nand  F1 scores of 88.08% and 87.23% were achieved which \nimproved the state-of-the-art results by 2.28% and 2.80%, \nrespectively. Pre-trained language models are available \nin many languages. Hence, the approach in this paper \ncan be examined in languages other than English as well. \nAlso, with the multilingual versions of these models, \nthe knowledge of AD prediction in one language can be \ntransferred to another language in which a sufficiently \nlarge dataset does not exist.\nAbbreviations\nAD: Alzheimer’s disease; ADI: Alzheimer’s disease international; BERT: Bidirec-\ntional encoder representations from transformers; CA: Contextual augmenta-\ntion; CNN: Convolutional neural network; XLM: Cross-lingual language model; \nXLNet: Extended large network; GRU : Gated recurrent unit; GloVe: Global \nvector; HC: Healthy control; LSTM: Long-short term memory; MLM: Masked \nlanguage model; MCI: Mild cognitive impairment; MMSE: Mini-mental state \nexam; NLP: Natural language processing; POS: Part-of-speech; RNN: Recur-\nrent neural network; TLM: Translated language model; SVM: Support vector \nmachine; SSA: Synonym substitution augmentation.\nAcknowledgements\nThe authors wish to thank the publisher of the DementiaBank dataset for \nproviding access to the Pitt corpus. The authors also wish to thank Professor \nZahra Vahabi of Tehran University of Medical Sciences for discussions on the \napplication of the proposed methodology to the Persian language version of \nthe assessment test which is the subject of on-going investigation.\nAuthors’ contributions\nAR and MSB analyzed the data. HA conceptualized the work. All authors \nwrote and edited the manuscript. All authors read and approved the final \nmanuscript.\nFunding\nNot applicable.\nAvailability of data and materials\nThe data (Pitt corpus from the DementiaBank dataset) that support the find-\nings of this study are available from the TalkBank project by request from the \ndataset publisher.\nDeclarations\n Ethics approval and consent to participate\nAs a data quality study based on existing and deidentified data, this work was \nnot classified as human subjects research and did not require Institutional \nReview Board approval.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1 Department of Computer Engineering, Sharif University of Technology, Azadi \nAvenue, Tehran, Iran. 2 Department of Electrical Engineering, Sharif University \nof Technology, Azadi Avenue, Tehran, Iran. \nPage 13 of 14\nRoshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \n \nReceived: 31 July 2020   Accepted: 23 February 2021\nReferences\n 1. Glenner GG. Alzheimers disease Biomedical Advances in Aging. \n1990;51–62.\n 2. International AD. World Alzheimer Report 2019: Attitudes to dementia. \nAlzheimer’s Disease Internationals London 2019.\n 3. Blanken G, Dittmann J, Haas J-C, Wallesch C-W. Spontaneous speech in \nsenile dementia and aphasia: implications for a neurolinguistic model of \nlanguage production. Cognition. 1987;27(3):247–74.\n 4. Sperling RA, Aisen PS, Beckett LA, Bennett DA, Craft S, Fagan AM, Iwat-\nsubo T, Jack CR Jr, Kaye J, Montine TJ, et al. Toward defining the preclinical \nstages of Alzheimer’s disease: recommendations from the national insti-\ntute on aging-alzheimer’s association workgroups on diagnostic guide-\nlines for alzheimer’s disease. Alzheimer’s Dementia. 2011;7(3):280–92.\n 5. Reisberg B, Sclan S, Franssen E, DeLeon M, Kluger A, Torossian C, Shulman \nE, Steinberg G, Monteiro I, McRae T, et al. Clinical stages of normal aging \nand Alzheimers-disease-the GDS staging system. Neurosci Res Commun. \n1993;13:51–4.\n 6. Mace NL, Rabins PV. The 36-hour Day: A Family Guide to Caring for People \nWho Have Alzheimer Disease, Related Dementias, and Memory Loss. JHU \nPress; 2011.\n 7. Ostuni E, Santo Pietro MJC. Getting Through: Communicating When \nSomeone You Care for Has Alzheimer’s Disease. Speech Bin; 1986.\n 8. Goodglass H, Kaplan E. The assessment of aphasia and related disorders, \nvol. 230. 2nd ed. Philadelphia: Dictionary of Biological Psychology, Lea & \nFebiger; 1983.\n 9. Mackenzie C, Brady M, Norrie J, Poedjianto N. Picture description in neu-\nrologically normal adults: concepts and topic coherence. Aphasiology. \n2007;21(3–4):340–54.\n 10. Becker JT, Boiler F, Lopez OL, Saxton J, McGonigle KL. The natural history \nof Alzheimer’s disease: description of study cohort and accuracy of diag-\nnosis. Arch Neurol. 1994;51(6):585–94.\n 11. Bucks RS, Singh S, Cuerden JM, Wilcock GK. Analysis of spontaneous, \nconversational speech in dementia of Alzheimer type: Evaluation of an \nobjective technique for analysing lexical performance. Aphasiology. \n2000;14(1):71–91.\n 12. Guinn CI, Habash A. Language analysis of speakers with dementia of the \nAlzheimer’s type. In: 2012 AAAI Fall Symposium Series; 2012.\n 13. Orimaye SO, Wong JS-M, Golden KJ. Learning predictive linguistic \nfeatures for Alzheimer’s disease and related dementias using verbal utter-\nances. In: Proceedings of the Workshop on Computational Linguistics \nand Clinical Psychology: From Linguistic Signal to Clinical Reality; 2014. p. \n78–87.\n 14. Thomas C, Keselj V, Cercone N, Rockwood K, Asp E. Automatic detection \nand rating of dementia of alzheimer type through lexical analysis of \nspontaneous speech. In: IEEE International Conference Mechatronics and \nAutomation, vol 3, 2005. IEEE; 2005. p. 1569–1574.\n 15. Meilán JJG, Martínez-Sánchez F, Carro J, López DE, Millian-Morell L, \nArana JM. Speech in Alzheimer’s disease: Can temporal and acoustic \nparameters discriminate dementia? Dement Geriatr Cogn Disord. \n2014;37(5–6):327–34.\n 16. König A, Satt A, Sorin A, Hoory R, Toledo-Ronen O, Derreumaux A, Manera \nV, Verhey F, Aalten P , Robert PH, et al. Automatic speech analysis for the \nassessment of patients with predementia and Alzheimer’s disease. Alzhei-\nmer’s Dement Diagnosis Assess Disease Monit. 2015;1(1):112–24.\n 17. Jarrold W, Peintner B, Wilkins D, Vergryi D, Richey C, Gorno-Tempini ML, \nOgar J. Aided diagnosis of dementia type through computer-based \nanalysis of spontaneous speech. In: Proceedings of the Workshop on \nComputational Linguistics and Clinical Psychology: From Linguistic Signal \nto Clinical Reality; 2014. p. 27–37.\n 18. Rentoumi V, Raoufian L, Ahmed S, de Jager CA, Garrard P . Features and \nmachine learning classification of connected speech samples from \npatients with autopsy proven Alzheimer’s disease with and without \nadditional vascular pathology. J Alzheimers Dis. 2014;42(s3):3–17.\n 19. Eyigoz E, Mathur S, Santamaria M, Cecchi G, Naylor M. Linguistic markers \npredict onset of Alzheimer’s disease. EClinicalMedicine. 2020;100583.\n 20. Fraser KC, Meltzer JA, Rudzicz F. Linguistic features identify Alzheimer’s \ndisease in narrative speech. J Alzheimers Dis. 2016;49(2):407–22.\n 21. Yancheva M, Rudzicz F. Vector-space topic models for detecting \nAlzheimer’s disease. In: Proceedings of the 54th Annual Meeting of the \nAssociation for Computational Linguistics (Volume 1: Long Papers); 2016. \np. 2337–2346.\n 22. Sirts K, Piguet O, Johnson M. Idea density for predicting Alzheimer’s \ndisease from transcribed speech. arXiv preprint arXiv :1706.04473  2017.\n 23. Pennington J, Socher R, Manning CD. Glove: Global vectors for word \nrepresentation. In: Proceedings of the 2014 Conference on Empirical \nMethods in Natural Language Processing (EMNLP); 2014. p. 1532–1543.\n 24. Khodabakhsh A, Demiroglu C. Analysis of speech-based measures for \ndetecting and monitoring Alzheimer’s disease. In: Data Mining in Clinical \nMedicine. Springer, 2015; p. 159–173\n 25. Weiner J, Herff C, Schultz T. Speech-based detection of Alzheimer’s \ndisease in conversational German. In: INTERSPEECH, 2016; p. 1938–1942.\n 26. Li B, Hsu Y-T, Rudzicz F. Detecting dementia in mandarin Chinese using \ntransfer learning from a parallel corpus. arXiv preprint arXiv :1903.00933  \n2019.\n 27. Fraser KC, Linz N, Li B, Fors KL, Rudzicz F, König A, Alexandersson J, Robert \nP , Kokkinakis D. Multilingual prediction of Alzheimer’s disease through \ndomain adaptation and concept-based language modelling. In: Proceed-\nings of the 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers); 2019. p. 3659–3670.\n 28. Orimaye SO, Wong JS-M, Fernandez JSG. Deep-deep neural network lan-\nguage models for predicting mild cognitive impairment. In: BAI@ IJCAI; \n2016. p. 14–20.\n 29. Karlekar S, Niu T, Bansal M. Detecting linguistic characteristics of Alzhei-\nmer’s dementia by interpreting neural models. In: Proceedings of the \n2018 Conference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Technologies, vol 2 (Short \nPapers). Association for Computational Linguistics, New Orleans, Louisi-\nana; 2018. p. 701–707\n 30. Fritsch J, Wankerl S, Nöth E. Automatic diagnosis of Alzheimer’s disease \nusing neural network language models. In: ICASSP 2019-2019 IEEE \nInternational Conference on Acoustics, Speech and Signal Processing \n(ICASSP). IEEE. 2019; p. 5841–5845\n 31. Pan Y, Mirheidari B, Reuber M, Venneri A, Blackburn D, Christensen H. \nAutomatic hierarchical attention neural network for detecting ad. Proc. \nInterspeech. 2019;2019:4105–9.\n 32. GLUE Benchmark. https ://glueb enchm ark.com/leade rboar d. Accessed 14 \nMar 2020\n 33. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, \nPolosukhin I. Attention is all you need. In: Advances in Neural Information \nProcessing Systems, 2017; p. 5998–6008.\n 34. Al-Rfou R, Choe D, Constant N, Guo M, Jones L. Character-level language \nmodeling with deeper self-attention. Proc AAAI Conf Artif Intell. \n2019;33:3159–66.\n 35. Dai Z, Yang Z, Yang Y, Carbonell J, Le QV, Salakhutdinov R. Transformer-xl: \nAttentive language models beyond a fixed-length context. arXiv preprint \narXiv :1901.02860  2019.\n 36. Devlin J, Chang M-W, Lee K, Toutanova K. Bert: pre-training of deep bidi-\nrectional transformers for language understanding. arXiv preprint arXiv \n:1810.04805  2018.\n 37. Conneau A, Lample G. Cross-lingual language model pretraining. In: \nAdvances in Neural Information Processing Systems; 2019. p. 7057–7067.\n 38. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le QV. Xlnet: general-\nized autoregressive pretraining for language understanding. In: Advances \nin Neural Information Processing Systems; 2019. p. 5754–5764.\n 39. Transformers—transformers 3.3.0 documentation. https ://huggi ngfac \ne.co/trans forme rs/index .html. Accessed 29 Sept 2020\n 40. Zhang X, Zhao J, LeCun Y. Character-level convolutional networks for text \nclassification. In: Advances in Neural Information Processing Systems; \n2015. p. 649–657.\n 41. Miller GA. Wordnet: a lexical database for English. Commun ACM. \n1995;38(11):39–41.\n 42. Wang WY, Yang D. That’s so annoying!!!: A lexical and frame-semantic \nembedding based data augmentation approach to automatic categori-\nzation of annoying behaviors using# petpeeve tweets. In: Proceedings of \nPage 14 of 14Roshanzamir et al. BMC Med Inform Decis Mak           (2021) 21:92 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \nthe 2015 Conference on Empirical Methods in Natural Language Process-\ning; 2015. p. 2557–2563.\n 43. Kobayashi S. Contextual augmentation: Data augmentation by words \nwith paradigmatic relations. arXiv preprint arXiv :1805.06201  2018.\n 44. Wu X, Lv S, Zang L, Han J, Hu S. Conditional bert contextual augmenta-\ntion. In: International Conference on Computational Science. Springer; \n2019. p. 84–95\n 45. Ma E. NLP Augmentation. https ://githu b.com/makce dward /nlpau g 2019.\n 46. MacWhinney B. The CHILDES Project: Tools for Analyzing Talk, Volume I: \nTranscription Format and Programs. Psychology Press; 2014.\n 47. Maaten LVd, Hinton G. Visualizing data using T-SNE. J Mach Learn Res. \n2008;9(Nov):2579–605.\n 48. Hernández-Domínguez L, Ratté S, Sierra-Martínez G, Roche-Bergua A. \nComputer-based evaluation of Alzheimer’s disease and mild cogni-\ntive impairment patients during a picture description task. Alzheimer’s \nDement Diagnosis Assessm Disease Monit. 2018;10:260–8.\n 49. Folstein M, Folstein S, McHugh P . Mini-mental state: a practical method \nfor grading the cognitive state of patients for the clinician. J Psychiatr Res. \n1975;12:189–98.\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8047885894775391
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6677014231681824
    },
    {
      "name": "Language model",
      "score": 0.6604570746421814
    },
    {
      "name": "Feature engineering",
      "score": 0.6166818141937256
    },
    {
      "name": "Transformer",
      "score": 0.5982587933540344
    },
    {
      "name": "Classifier (UML)",
      "score": 0.577176570892334
    },
    {
      "name": "Machine learning",
      "score": 0.5391180515289307
    },
    {
      "name": "Deep learning",
      "score": 0.5156158804893494
    },
    {
      "name": "Random forest",
      "score": 0.49522414803504944
    },
    {
      "name": "Artificial neural network",
      "score": 0.48815470933914185
    },
    {
      "name": "Natural language processing",
      "score": 0.48603373765945435
    },
    {
      "name": "Natural language",
      "score": 0.468833863735199
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I133529467",
      "name": "Sharif University of Technology",
      "country": "IR"
    }
  ],
  "cited_by": 101
}