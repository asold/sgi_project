{
  "title": "Evaluating Large Language Models in Echocardiography Reporting: Opportunities and Challenges",
  "url": "https://openalex.org/W4391050741",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4202403729",
      "name": "Chieh-Ju Chao",
      "affiliations": [
        "Mayo Clinic"
      ]
    },
    {
      "id": "https://openalex.org/A2122079672",
      "name": "Imon Banerjee",
      "affiliations": [
        "Mayo Clinic in Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A2029060730",
      "name": "Reza Arsanjani",
      "affiliations": [
        "Mayo Clinic in Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A2185756301",
      "name": "Chadi Ayoub",
      "affiliations": [
        "Mayo Clinic in Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A2479851799",
      "name": "Andrew Tseng",
      "affiliations": [
        "Mayo Clinic in Florida",
        "Jacksonville College"
      ]
    },
    {
      "id": "https://openalex.org/A4320554146",
      "name": "Jean-benoit Delbrouck",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2093475147",
      "name": "Garvan C. Kane",
      "affiliations": [
        "Mayo Clinic"
      ]
    },
    {
      "id": "https://openalex.org/A2151806446",
      "name": "Francisco Lopez-Jimenez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221177771",
      "name": "Zachi Attia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2337129330",
      "name": "Jae K. Oh",
      "affiliations": [
        "Mayo Clinic"
      ]
    },
    {
      "id": "https://openalex.org/A2492627921",
      "name": "Bradley Erickson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3204450805",
      "name": "Li Fei-Fei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104053700",
      "name": "Ehsan Adeli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4316441464",
      "name": "Curtis Langlotz",
      "affiliations": [
        "Stanford University",
        "Association for the Advancement of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4202403729",
      "name": "Chieh-Ju Chao",
      "affiliations": [
        "Mayo Clinic"
      ]
    },
    {
      "id": "https://openalex.org/A2122079672",
      "name": "Imon Banerjee",
      "affiliations": [
        "Mayo Clinic in Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A2029060730",
      "name": "Reza Arsanjani",
      "affiliations": [
        "Mayo Clinic in Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A2185756301",
      "name": "Chadi Ayoub",
      "affiliations": [
        "Mayo Clinic in Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A2479851799",
      "name": "Andrew Tseng",
      "affiliations": [
        "Jacksonville College",
        "Mayo Clinic in Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2093475147",
      "name": "Garvan C. Kane",
      "affiliations": [
        "Mayo Clinic"
      ]
    },
    {
      "id": "https://openalex.org/A2337129330",
      "name": "Jae K. Oh",
      "affiliations": [
        "Mayo Clinic"
      ]
    },
    {
      "id": "https://openalex.org/A3204450805",
      "name": "Li Fei-Fei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104053700",
      "name": "Ehsan Adeli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4316441464",
      "name": "Curtis Langlotz",
      "affiliations": [
        "Association for the Advancement of Artificial Intelligence",
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3107201183",
    "https://openalex.org/W2462887743",
    "https://openalex.org/W3202862056",
    "https://openalex.org/W3205138128",
    "https://openalex.org/W2165863779",
    "https://openalex.org/W2018549886",
    "https://openalex.org/W2091291798",
    "https://openalex.org/W3217446125",
    "https://openalex.org/W2948282488",
    "https://openalex.org/W3002705197",
    "https://openalex.org/W2896287590",
    "https://openalex.org/W4377966413",
    "https://openalex.org/W4388022708",
    "https://openalex.org/W6853019041",
    "https://openalex.org/W4386270925",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W4380993746",
    "https://openalex.org/W4368367885",
    "https://openalex.org/W4385988359",
    "https://openalex.org/W4387724657",
    "https://openalex.org/W2342434602",
    "https://openalex.org/W4388633644",
    "https://openalex.org/W4294579032",
    "https://openalex.org/W4285110479",
    "https://openalex.org/W4396519801",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4387994989",
    "https://openalex.org/W6903667370",
    "https://openalex.org/W2143017621",
    "https://openalex.org/W4366327625",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2967024383",
    "https://openalex.org/W4320558759",
    "https://openalex.org/W4283451457",
    "https://openalex.org/W6910466824",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W4386120650",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4307309335",
    "https://openalex.org/W3188252230",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4391971084",
    "https://openalex.org/W4321018175",
    "https://openalex.org/W4378672794",
    "https://openalex.org/W3027879771"
  ],
  "abstract": "Abstract Background The increasing need for diagnostic echocardiography (echo) tests presents challenges in preserving the quality and promptness of reports. While Large Language Models (LLMs) have proven effective in summarizing clinical texts, their application in echo remains underexplored. Aims To evaluate open-source LLMs in echo report summarization. Methods Adult echo studies conducted at the Mayo Clinic from January 1, 2017, to December 31, 2017, were categorized into two groups: development (all Mayo locations except Arizona) and Arizona validation sets. We adapted open-source LLMs (Llama-2, MedAlpaca, Zephyr, and Flan-T5) using In-Context Learning (ICL) and Quantized Low-Rank Adaptation (QLoRA) fine-tuning for echo report summarization from “Findings” to “Impressions.” Against cardiologist-generated Impressions, the models’ performance was assessed both quantitatively with automatic metrics and qualitatively by cardiologists. Results The development dataset included 97,506 reports from 71,717 unique patients, predominantly male (55.4%), with an average age of 64.3±15.8 years. EchoGPT, a QLoRA fine-tuned Llama-2 model, outperformed other LLMs with win rates ranging from 87% to 99% in various automatic metrics, and produced reports comparable to cardiologists in qualitative review (significantly preferred in conciseness (p&lt; 0.001), with no significant preference in completeness, correctness, and clinical utility). Correlations between automatic and human metrics were fair to modest, with the best being RadGraph F1 scores versus clinical utility (r=0.42) and automatic metrics showed insensitivity (0-5% drop) to changes in measurement numbers. Conclusions EchoGPT can generate draft reports for human review and approval, helping to streamline the workflow. However, scalable evaluation approaches dedicated to echo reports remains necessary. Clinical Perspectives 1. What is new? This study evaluated multiple open-source LLMs and different model adaptation methods in echocardiography report summarization. The resulting system, EchoGPT, can generate echo reports comparable in quality to cardiologists. Future metrics for echo report quality should emphasize factual correctness, especially on numerical measurements. 2. What are the clinical implications? EchoGPT system demonstrated the potential of introducing LLMs into echocardiography practice to generate draft reports for human review and approval.",
  "full_text": " \nTitle: EchoGPT: A Large Language Model for Echocardiography Report Summarization \n \nAuthors: Chieh-Ju Chao, MD1,2, Imon Banerjee, PhD3, Reza Arsanjani, MD4, Chadi Ayoub, MD, PhD4, \nAndrew Tseng, MD5, Garvan C. Kane, MD, PhD1, Jae K Oh, MD1, Li Fei-Fei, PhD2, Ehsan Adeli, PhD2 \nand Curtis Langlotz, MD, PhD6 \n \nAffiliations:  \n1 Department of Cardiovascular Medicine, Mayo Clinic, Rochester, Minnesota \n2 Stanford Institute for Human-Centered Artificial Intelligence, Palo Alto, California \n3 Department of Radiology, Mayo Clinic, Scottsdale, Arizona \n4 Department of Cardiovascular Diseases, Mayo Clinic Arizona, Scottsdale, Arizona \n5Department of Cardiovascular Medicine, Mayo Clinic Florida, Jacksonville, Florida \n6 Center for Artificial Intelligence in Medicine and Imaging (AIMI), Stanford University, Palo Alto, \nCalifornia \n \nCorresponding author: \nChieh-Ju Chao, MD \nAddress: Gonda 4-478, 200 1st Street SW, Rochester, MN 55095 \nEmail: chao.chiehju@mayo.edu \n \n \nKeywords: Artificial Intelligence, Large Language Model, Echocardiography, Natural Language \nProcessing, Cardiovascular Imaging \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \nAbbreviations: \n4C metrics: qualitative review metrics including completeness, correctness, conciseness, and clinical \nutility \nAI: Artificial Intelligence \nDL: Deep Learning \nEcho: Echocardiography \nFT: Fine-Tuning \nGPT: Generative Pre-trained Transformer \nICL: In-Context Learning \nNLP: Natural Language Processing \nSeq2seq: Sequence-to-sequence \nTTE: Transthoracic Echocardiography  \nTEE: Transesophageal Echocardiography \nLLM: Large Language Model \nNLP: Natural Language Processing \nQLoRA: Quantized Low-Rank Adaption \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nAbstract \nBackground \nThe increasing need for diagnostic echocardiography (echo) tests presents challenges in preserving the \nquality and promptness of reports. While Large Language Models (LLMs) have proven effective in \nsummarizing clinical texts, their application in echo remains underexplored. To address this, we proposed \nEchoGPT, a dedicated, domain-specific LLM focused on echo report summarization. \n \nMethods \nAdult echo studies conducted at the Mayo Clinic from January 1, 2017, to December 31, 2017, were \ncollected and categorized into two groups: development (all Mayo locations except Arizona) and external \nvalidation (Mayo Arizona). We adapted open-source LLMs (Llama-2, MedAlpaca, Zephyr, and Flan-T5) \nusing In-Context Learning (ICL) and Quantized Low-Rank Adaptation (QLoRA) fine-tuning for echo text \nsummarization. The models' performance was assessed both quantitatively with automatic metrics and \nqualitatively by cardiologists. \n \nResults \nThe development dataset included 97,506 reports from 71,717 unique patients, predominantly male \n(54.3%), with an average age of 64.1±16.1 years. The final split contains 95,506 for training, and 1,000 \neach for validation and testing. EchoGPT, a QLoRA fine-tuned Llama-2 model, outperformed other \nLLMs with about 90% win rates in various metrics (BLEU, METEOR, ROUGE-L, BERT Score, and \nRadGraph F1 Score), and produced reports comparable to cardiologists in 30 randomly selected cases for \nqualitative human review (significantly preferred in conciseness (p< 0.001), with no significant \npreference in completeness, correctness, and clinical utility). In the external validation set (n=1,000), \nEchoGPT consistently outperformed fine-tuned Zephyr model across the same automatic metrics (all p < \n0.0001). \n \nConclusions \nCapable of generating echocardiography reports on par with human experts, EchoGPT could be used to \ngenerate draft reports for human review and approval, with significant workflow advantages. \n \n \nClinical Perspective  \n1. What is new?  \n● This study is the first attempt to compare multiple open-source LLMs and different \nmodel adaptation methods in echocardiography report summarization. \n● The resulting system, EchoGPT, can generate echo reports comparable in quality to \ncardiologists. \n● Future metrics for echo report quality should emphasize factual correctness, especially on \nnumerical measurements. \n2. What are the clinical implications? \n● EchoGPT system demonstrated the potential of introducing LLMs into echocardiography \npractice. \n● EchoGPT could be used as an AI co-pilot to generate echo reports. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nIntroduction \n \nEchocardiography (echo) is the mainstay imaging modality in the current practice of cardiology1, \nproviding vital, non-invasive assessments of heart anatomy and physiology to guide clinical decisions2. In \nthe past decade, the rising demand for diagnostic echo tests3 has posed significant challenges in \nmaintaining the quality and timeliness of diagnostic reports4–7, underscoring the necessity for automated \nsolutions to enhance both efficiency and report quality8–10.  \n \nWith the recent emergence of artificial intelligence (AI), automated echo reporting has been proposed to \nuse deep learning (DL) models to generate diagnostic predictions and measurements to fill a pre-set report \ntemplate8,10,11. These frameworks focused on specific image processing tasks8,11 rather than the report text, \nand are technically equivalent to generating individual findings. However, these frameworks were not \ndesigned to handle the high-level cognitive activity of synthesizing clinically relevant impressions from \ndetailed findings12. In practice, physicians usually spend a significant amount of time summarizing \ndetailed findings to clinically relevant final impressions13,14. While this task is crucial, it can be time-\nconsuming and prone to errors15. \n \nThe advance of large language models (LLM) marked an important milestone for the application of AI in \nhealthcare to automate clinical information summarization13,14,16 and expert-level question-answering17. A \nmajor advantage of LLMs is the flexibility of input and output 18, as well as the capability of handling \nconversations and interaction with human experts 19. While similar functionality can be achieved through \ncommercially available LLMs (e.g., ChatGPT; OpenAI, San Francisco, CA) 20, only a few healthcare \ninstitutions have integrated ChatGPT21. Furthermore, fine-tuning ChatGPT for specific tasks still requires \nuploading data to a central server, which also raises privacy concerns22. In contrast, open-source LLMs are \nfree of charge and can be locally fine -tuned for specific tasks within each healthcare institution's secure \nconfines18. \n  \nPrevious studies predominantly focused on electronic health records13,16 and chest X-rays (CXR)13,18 have \nhighlighted the potential of using LLMs to summarize in clinical text. In contrast, echo-related studies were \nmainly on data extraction or classification, rather than report summarization23–25. Tang et al. used rule-based \nsystems and the BART (Bidirectional and Auto -Regressive Transformer) model 26 for this purpose and \ndemonstrated convincing results. However, BART -generated content was less favored by human experts \nmore than 50% of the time, perhaps due to the smaller number of parameters than current state -of-the-art \nLLMs26. The application of billion-parameter LLMs to generate echo reports remains under-explored27,28.  \n \nIn this work, we proposed to construct a local, domain -specific LLM (EchoGPT) dedicated to \nechocardiography report summarization through an instruction fine-tuning approach, which is known to be \nan effective strategy to adapt LLMs for similar tasks 13,18. We anticipate that the fine -tuning procedure \nimproves LLMs’ performance on the task of echocardiography report summarization. EchoGPT will fill \nthe knowledge gap for using open -source LLM in the domain of echocardiography reporting and could \nenhance the efficiency of the current workflow with uncompromised report quality.    \n \nMethod \nEchocardiography Report \nFollowing the American Society of Echocardiography recommendations29, a standard echocardiography \nreport at the Mayo Clinic contains the following major sections: Final Impressions, Findings, and \nMeasurements. The Measurements section contains only measurement values without free text. The key \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nmeasurements such as left ventricular ejection fraction, aortic valve area, and right ventricular systolic \npressure are included in the Findings section with corresponding statements. Considering the report \nstructure above, only the information from the Final Impressions and Findings sections was used in this \nwork (Figure 1).  \n \nDataset \nAll adult (> 18 years old) echocardiography studies performed from 1/1/2017 to 12/31/2017 at Mayo \nClinic Enterprise were retrieved. The types of studies include transthoracic echocardiography (TTE), \ntransesophageal echocardiography (TEE), stress echocardiography, and intracardiac echocardiography. \nText in the “Findings” and the “Final Impression” sections of each report was extracted for the current \nstudy (Figure 2).  The study was approved by Mayo Clinic IRB (protocol#: 22-010944).  \n \nData Curation and Preprocessing \nEchocardiography reports were excluded according to the following criteria: (1) reports without Finding \nor Final Impression sections, (2) reports whose Finding or Impression section contained less than 15 \nwords, as these are frequently canceled studies in our practice, and (3) labeled in report metadata as \nlimited report, fetal study, nuclear stress, or vascular study. After this filtering process, the text of each \nreport was further processed as follows: (1) Remove capitalized subheadings (e.g., LEFT VENTRICLE, \nVALVES, OTHER FINDINGS, etc.), (2) Remove template sentences that make comparisons to prior \nreports, as no information from previous reports has been provided, and (3) Remove quality control-\nrelated sentences such as “study performed per left ventricular function protocol” and  “the goals, risks, \nand alternatives to moderate sedation were explained to the patient.”  \n \nData split \nData from Rochester, Florida, and Mayo Clinic Healthcare sites were used as the model development set. \nThe Arizona site data was used as the external validation set. Within the development set, 1,000 non-\nduplicated cases were randomly selected for the test and validation sets, respectively; the rest of the cases \nwere used for fine-tuning (training set). Similarly, 1,000 non-duplicated random examples were selected \nfrom the Arizona external validation dataset. For basic dataset statistics, the token length was calculated \nbased on the natural language processing toolkit (NLTK) tokenizer30, and the lexical variance was defined \nas the ratio of the number of unique tokens to the number of total tokens in each example13. \n \nModel selection \nDue to patient privacy policy regulations, proprietary LLMs such as GPT-3.5 and GPT-4 were not \nconsidered in this work because versions of those models that were safe for protected health information \nwere not yet available. Among open-source models, we selected representative auto-regressive models \nincluding Llama-2-7b-chat27, Zephyr-7b28, and Med-Alpaca31 models considering their performance and \nmax input context length on general natural language processing (NLP) tasks and radiology report \nsummarization 13. For sequence-to-sequence (seq2seq) models, we used Flan-T5 (base) as the \nrepresentative model as it is known for accurate text summarization13,32. \n \nModel inference hyperparameter search \nLLM inference was conducted by using HuggingFace’s (Manhattan, NY) transformer pipeline via the \nopen-source LangChain framework33. After initial tests, text generation and summarization were used as \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nthe task type for auto-regressive models and seq2seq models, respectively. A subset (10%, n=100) of \nexamples were randomly selected from the test set for the hyperparameter search. We specifically tested \nthe following configuration parameters that can significantly affect performance: temperature (0.1, 0.5, \nand 0.9) and repetition penalty (1.1, 1.2, and 1.3). These two parameters were tested separately, when one \nparameter was being tested, the other was fixed at the lowest value. The generated contents were \nevaluated by both automatic metrics and qualitative assessment. We chose the following configuration for \nmodel inference: {temperature 0.1, repetition penalty 1.1} after comparing automatic metrics and \nqualitative assessments; see Supplemental Table 1). We did not complete a dedicated search procedure \nfor the optimal LLM inference configurations, but the configurations used in our study were similar to \nprior reports, and the generated contents were satisfying on qualitative review. Of note, the configurations \nwere tested in a zero-shot setting, and the best configuration was directly applied to the ICL and QLoRA \nfine-tuned models13.  \n \nModel Adaptation \nIn-context Learning (ICL): ICL has been proposed to improve LLM’s performance without changing the \nbase model weights13,34,35. Also, using relevant in-context examples is shown to have better model \nperformance compared to random examples in ICL. To obtain relevant in-context examples, we adopted \nthe approach to select m (m=1, 2, 4…) nearest neighbors from the training set for each test set case, after \nembedding both sets by the PubMedBERT model36. A prompt template was created with components of \nthe prefix, instruction, and suffix13 (Table 1). The final prompt was decided after qualitatively evaluating \nseveral different variants of each component on a small subset of the data. We also specified that the \nsummarization should be “concise” and use “a minimal amount of text” to avoid LLMs generating \nlengthy reports13.  \n \nInstruction tuning with quantized low-rank adaptation (QLoRA): \nDue to the size of candidate models, we opted for quantized low-rank adaptation (QLoRA)37, a type of \nparameter efficient fine-tuning (PEFT)38 to optimize our LLMs for echo report summarization tasks. The \nsame prompt template was used, and the Final Impression text from the same report was used as the target \noutput 13,14. \n \nWe configured the training process as follows: load model in 4-bit precision, with a LoRA configuration \nof (alpha = 16, LoRA dropout = 0.1, LoRA r= 64). The batch size and gradient accumulation were \nadjusted for each model to achieve an effective batch size of 24 that fits on a single NVIDIA RTX A5000 \n24G GPU setting. A paged-AdamW 32-bit optimizer was used, with an initial learning rate of 1e-3, which \ndecayed to 1e-4 (by a cosine scheduler) after the initial 100 warm-up steps. The above configuration \nprovided the most stable training process after attempting different configurations reported in prior \nstudies13,37. \n \nModel Performance Evaluation \nAutomatic NLP evaluation metrics: To evaluate the models' performance on the information \nsummarization task and compare it to prior works, we utilized four established automatic metrics: BLEU \n(Bilingual Evaluation Understudy)39, METEOR (Metric for Evaluation of Translation with Explicit \nORdering)40, ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation - Longest Common \nSubsequence), and the BERT (Bidirectional Encoder Representations from Transformers) score41. For \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nROUGE-L, we present the F1 score component26,42. For factual correctness, the RadGraph-F1 metric \n(level: all) was reported43,44. Considering the importance of measurements in echo studies, we attempted \nto evaluate whether the current automatic metrics can detect changes in measurement numbers. For this \npurpose, we generated synthetic reports by replacing all the measurement numbers with random numbers \nranging from 1 to 99 in the reports. We then compared the automatic metric scores with the corresponding \nreports with the original measurements. \n \nHuman expert evaluation metrics: We designed a human expert evaluation process based on previous \nclinical text summarization studies13,18,26. The Findings with corresponding ground truth (Final \nImpression) and LLM-generated summarization of 30 randomly selected cases were presented to four \nechocardiography-board-certified cardiologists for blinded quality review. We noted that physician-\nsummarized Impressions may contain free-text information beyond the Findings (e.g., documenting \nevents during the study or communication with the ordering provider), and the reviewers were instructed \nto ignore these. Each metric was rated for the preference (5 levels) between the two summarizations \n(Supplemental Figure 1)13. The “4C metrics” evaluated by echocardiography experts are completeness, \nconciseness, correctness, and clinical utility14, as described in Table 2.  \n \nAutomatic metric performance between each pair of models was compared using a two-tailed paired \nStudent's t-test or Wilcoxon signed-rank test for normally and non-normally distributed data, respectively. \nModels were also compared based on win rates, which are defined as the percentage of head-to-head \nvictories in performance between two models for each selected metric13. In human expert qualitative \nanalysis, the 4C metrics were compared by a one-sample Wilcoxon signed-rank test13, and the agreement \nof ratings between experts was assessed by Fleiss’ kappa coefficient45, and interpreted as recommended \nby Landis et al46. We conducted Pearson correlation analyses to explore the relationships between each \nhuman expert evaluation metric, assessing their independence. Additionally, we conducted similar \nanalyses to examine the correlation between human and automatic metrics. All the comparisons consider \na p-value < 0.05 as significant. \n \nResults \nPatient cohort and dataset \nOur development set contains 97,506 reports from 71,717 unique patients with a mean age of 64.1±16.1 \nyears, and 38,912 (54.3%) were male. The external validation dataset contains 19,557 reports from 15,853 \nunique patients with a mean age of 61.2 ±15.7 years, and 8,970 (56.6%) were male. Statistics of text data \nare summarized in Table 3. Transthoracic echocardiography was the predominant study type in both \ndevelopment and external validation datasets (81.9% and 76.5%, respectively); detailed distribution was \nsummarized in Supplemental Table 2.   \n \nZero-shot, ICL, and QLoRA fine-tuned performance \nTable 4 is a summary of the performance of zero-shot and fine-tuned LLMs, including MedAlpaca, \nLlama-2, and Zephyr. QLoRA fine-tuning significantly improved LLMs’ performance from baseline. \nNote that T5 and MedAlpaca were not fine-tuned so only zero-shot results were provided for reference. \nAmong the candidate models, Llama-2 generally had the best zero-shot performance, which was \nconsistent in ICL. While Flan-T5 had a similar or superior performance to Llama-2 across most metrics, it \nwas particularly worse on the RadGraph F1 score (Table 4; Figure 3). On qualitative review, we noted \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nthat T5 provided concise summaries, however important clinical information was missed in this process \n(Supplemental Table 3). \n \nIn ICL, LLMs that allow longer context length (Llama-2 and Zephyr) had the best performance across all \nmetrics when one example was provided (ICL-1). The performance then gradually trended down with \nmore examples (ICL-2 and ICL-4). In contrast, the performance of LLMs with shorter max context length \nstarted to trend down with one example (Figure 3).  \n \nBased on the zero-shot and ICL performance of the candidate models, Llama-2 and Zephyr were selected \nfor instruction fine-tuning. Compared to zero-shot, QLoRA significantly improved the performance of \nselected LLMs across all metrics (Table 4). For the head-to-head comparison in model win rates, fine-\ntuned Llama-2 was superior to all other models, including Zephyr (base and fine-tuned), MedAlpaca \n(base), and Flan-T5 (base) across all 5 automatic metrics (Figure 4). Llama-2 maintained similar \nperformance in the external validation data set (n=1,000) and was consistently superior to fine-tuned \nZephyr (Supplemental Table 5). Because Llama-2 had the best performance on both zero-shot and ICL \napproaches, it was selected as EchoGPT and used for the subsequent expert qualitative review. \n \nSignificance of Measurement Numbers in Automatic Metrics \nAfter replacing the measurement numbers with random numbers, we observed statistically significant \n(p<0.0001) but minor decreases in BLEU, METEOR, ROUGE-L, BERT, and RadGraph F1 scores. One \ncan see that in the provided examples, the report with random numbers doesn’t make clinical sense when \ncompared to the original content (Table 5). \n \nHuman expert evaluation \nWe observed fair agreement for conciseness and correctness, moderate agreement for completeness, and \ngood agreement for clinical utility (Table 6). Among the 4C metrics, we observed that EchoGPT \nsignificantly outperformed human experts in conciseness (p<0.001). There was no significant difference \namong the other three categories (Figure 5A). The 4C metrics were not completely independent. There \nwas a high correlation between clinical utility and completeness (Pearson’s r= 0.78),  and modest to \nmoderate correlations between other metrics (Figure 5B). We also observed that across all automatic \nmetrics, RadGraph F1 had modest to moderate correlations with all 4 human evaluation metrics (Figure \n5C). \n \nDiscussion \n \nTo the best of our knowledge, this study is the first attempt to compare multiple open-source LLMs in \nechocardiography report summarization through different model adaptation methods. Trained on one of \nthe largest echocardiography report datasets in the world, we demonstrated that QLoRA fine-tuning can \nsignificantly improve LLMs’ performance for the desired summarization task, with at least comparable \nqualities to human experts. Our results also indicate that ICL is associated with significant limitations for \nclinical practice, including compromised patient privacy and report quality. Additionally, we \ndemonstrated that current automatic metrics are not sensitive to the change in measurement numbers in \necho reports. The current study provides insights into the construction of a dedicated local LLM for echo \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nreport summarization and can pave the way for an AI-enabled echocardiography interpretation system \nwith a human-AI interaction interface.  \n \nModel adaption approaches: ICL vs. fine-tuning \nOur results suggested that both ICL and QLoRA fine-tuning improved LLMs’ performance over zero-\nshot, and fine-tuning performance was consistently above ICL across all automatic metrics (Figure 3). \nAdditionally, we observed substantial limitations of ICL for echo reporting, including relatively longer \ncontext length and compromised patient privacy, as discussed below.  \n \nCompared to CXR reports, echocardiography reports come with a relatively longer context, which \ndirectly affects the available choices of LLMs. In contrast to a prior study that used 32 or more \nexamples13, we were only able to test up to 4 examples for ICL, therefore not able to assess the models’ \nbehavior with more examples. However, across all metrics, LLMs’ had gradually down-trending \nperformance when more examples were provided (Figure 3), which is consistent with prior studies13,34. It \nis also important to consider that the computation time and resources required in ICL can increase with \nthe number of examples used34. Additionally, we note that LLMs integrate information from the example \nICL cases, which compromises the report quality as well as patient privacy (Supplemental Table 4). \nWhile this behavior was not reported in other studies13,34, we believe it could be a common condition, that \nis easier to identify with numerical values (in echo) compared to narrative statements (in CXR). \nTherefore, even in scenarios where ICL can outperform fine-tuning13, fine-tuning may be preferable.  \n  \nThe EchoGen study previously demonstrated that BART and other rule-based approaches are effective for \nsummarizing echocardiogram reports, with BART achieving ROUGE-based scores between 0.65 and \n0.7326. Although EchoGPT didn't match these scores in ROUGE-L, it compared favorably to human \nexperts in qualitative assessments. In contrast, human summaries were preferred a majority of the time \nover those generated by the BART model26. Additionally, in our study, we observed that T5 (as the \nrepresentative seq2seq model) generated summaries that were overly brief so important clinical \ninformation was missed (Supplemental Table 3). Although the EchoGen authors did not provide \nqualitative examples generated by their BART model, we assume that similar behavior occurred with \nBART which led to the unfavorable rating by physicians. \n \nEvaluation of Echo Report Summarization \nAutomatic evaluation of LLM in clinical text summarization tasks is an emerging area, and there is no \nsingle gold standard metric that can evaluate all aspects of a report13,26. Our study reinforces this \nconclusion. We noted that MedAlpaca and Zephyr can generate medical-professionally-sounding content \nthat frequently includes hallucinated information. These differences were mainly reflected by the factual \ncorrectness metric RadGraph F1 (Table 4).  \n \nConcerning the correlations between automatic metrics and human expert preference, our results were \nsimilar to the prior studies, showing that most of the metrics were not strongly correlated13. Notably, the \nhighest correlation was observed between the RadGraph F1 scores and the 4C metrics, particularly in \nterms of clinical utility (r=0.42) (Figure 5C). This suggests that the quality of echo reports judged by  \ncardiologists may not be well captured in automated metrics that do not capture notions of factual \ncorrectness.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \n \nAs a specific subtype of clinical text, echocardiography reports contain unique terminology, including \nmany precise measurements. Clinically, 25% and 55% LV ejection fraction values indicate a significant \ndifference, however, our study demonstrates that this distinction is difficult to capture with current \nautomatic metrics  (Table 5). While this aspect of reporting can be easily captured in qualitative analyses, \nsuch analyses are expensive to conduct at scale because of the limited availability of in-domain experts. A \ndedicated metric for echocardiography diagnostic quality evaluation, with emphasis on measurement \naccuracy, is still needed to address this knowledge gap. \n \nApplication of EchoGPT \nConstructing EchoGPT shows the feasibility of introducing LLMs into echocardiography practice. \nThrough the QLoRA fine-tuning process37, the model was able to learn clinically relevant knowledge to \nsummarize echo report findings at a quality level comparable to echocardiography-trained cardiologists \n(Figure 5A).  \n \nThe current study concentrated solely on the performance of EchoGPT in summarizing echocardiogram \nreports. However, as an LLM-based system, EchoGPT holds promise for broader applications, including \nproviding in-context clinical reasoning, answering questions based on patient data, and interacting with \nhuman experts or other models12,16,47. In contemplating these expanded functionalities, two critical factors \nemerge: the maximum permissible context length and the model's proficiency in managing conversations. \nThese considerations might restrict the use of existing seq2seq models in such scenarios32. \n \nWe envision that EchoGPT could be used as a reporting interface, or a co-pilot that could generate echo \nreports with various inputs48. EchoGPT inherits the limitations of LLMs, including hallucination13,18,49. \nAlthough the number of hallucinations improved after the fine-tuning process, additional efforts such as \noptimization for factual correctness43 or paired with a retrieval augmented generation system50 are still \nrequired to minimize hallucinations before clinical implementation. \n \nConclusion \nOur study successfully built EchoGPT through QLoRA fine-tuning of open-source LLMs, and \ndemonstrated that the model is capable of generating echocardiography reports on par with cardiologists, \nmarking an advancement in integrating LLMs into current echo practice. Through further optimizations in \nthe future, EchoGPT is envisioned to become a human-AI co-pilot for report generation. \n \nLimitations \nOur echocardiography reports are based on standardized statements, with an option to add free text. While \nthe lexical variance was high, the corpus could differ from reports composed entirely of free text contents. \nDue to patient privacy regulations, this work did not assess the performance of GPT-3.5 and GPT-4. \nHowever, we compared the performance of SOTA open-source LLMs, which provided important insights \nfor model selection when data privacy is a critical consideration. Instead of full fine-tuning, QLoRA was \nused as the fine-tuning approach, however, it has been demonstrated as an effective approach as full fine-\ntuning is often not feasible for LLMs. Although QLoRA fine-tuning demonstrated improvements in echo \nreport summarization tasks, our current approach does not include optimization for factual correctness \nand human expert preference.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nReference \n1.  Daubert MA, Tailor T, James O, Shaw LJ, Douglas PS, Koweek L. Multimodality cardiac imaging in \nthe 21st century: evolution, advances and future opportunities for innovation.  Br J Radiol. \n2021;94:20200780. \n2.  Carli MFD, Geva T, Davidoff R. The Future of Cardiovascular Imaging. Circulation. 2016;133:2640–\n2661. \n3.  Reeves RA, Halpern EJ, Rao VM. Cardiac Imaging Trends from 2010 to 2019 in the Medicare \nPopulation. Radiol Cardiothorac Imaging. 2021;3:e210156. \n4.  Tiver KD, Horsfall M, Swan A, Pasquale CD, Horsfall E, Chew DP, Pasquale CGD. Accuracy of \nHighly Limited Echocardiographic Screening Images for Determining a Structurally Normal Heart: The \nQuick-Six Study. Hear Lung Circ. 2022;31:462–468. \n5.  Habash-Bseiso DE, Rokey R, Berger CJ, Weier AW, Chyou P-H. Accuracy of Noninvasive Ejection \nFraction Measurement in a Large Community-Based Clinic. Clin Medicine Res. 2005;3:75–82. \n6.  Berlin L. Defending the “Missed” Radiographic Diagnosis. Am J Roentgenol. 2001;176:317–322. \n7.  Berlin L, Hendrix RW. Perceptual errors and negligence. Am J Roentgenol. 1998;170:863–867. \n8.  Tromp J, Seekings PJ, Hung C-L, Iversen MB, Frost MJ, Ouwerkerk W, Jiang Z, Eisenhaber F, Goh \nRSM, Zhao H, Huang W, Ling L-H, Sim D, Cozzone P, Richards AM, Lee HK, Solomon SD, Lam CSP, \nEzekowitz JA. Automated interpretation of systolic and diastolic function on the echocardiogram: a \nmulticohort study. Lancet Digital Heal. 2022;4:e46–e54. \n9.  Nolan MT, Thavendiranathan P. Automated Quantification in Echocardiography. JACC Cardiovasc \nImaging. 2019;12:1073–1092. \n10.  Ghorbani A, Ouyang D, Abid A, He B, Chen JH, Harrington RA, Liang DH, Ashley EA, Zou JY. \nDeep learning interpretation of echocardiograms. Npj Digital Medicine. 2020;3:10. \n11.  Zhang J, Gajjala S, Agrawal P, Tison GH, Hallock LA, Beussink-Nelson L, Lassen MH, Fan E, Aras \nMA, Jordan C, Fleischmann KE, Melisko M, Qasim A, Shah SJ, Bajcsy R, Deo RC. Fully Automated \nEchocardiogram Interpretation in Clinical Practice. Circulation. 2018;138:1623–1635. \n12.  Rajpurkar P, Lungren MP. The Current and Future State of AI Interpretation of Medical Images. New \nEngl J Med. 2023;388:1981–1990. \n13.  Veen DV, Uden CV, Blankemeier L, Delbrouck J-B, Aali A, Bluethgen C, Pareek A, Polacin M, \nCollins W, Ahuja N, Langlotz CP, Hom J, Gatidis S, Pauly J, Chaudhari AS. Clinical Text \nSummarization: Adapting Large Language Models Can Outperform Human Experts. arXiv. 2023; \n14.  Liu Z, Zhong A, Li Y, Yang L, Ju C, Wu Z, Ma C, Shu P, Chen C, Kim S, Dai H, Zhao L, Zhu D, \nLiu J, Liu W, Shen D, Li X, Li Q, Liu T. Radiology-GPT: A Large Language Model for Radiology. \narXiv. 2023; \n15.  Gershanik EF, Lacson R, Khorasani R. Critical finding capture in the impression section of radiology \nreports. AMIA Annu Symp Proc AMIA Symp. 2011;2011:465–9. \n16.  Fleming SL, Lozano A, Haberkorn WJ, Jindal JA, Reis EP, Thapa R, Blankemeier L, Genkins JZ, \nSteinberg E, Nayak A, Patel BS, Chiang C-C, Callahan A, Huo Z, Gatidis S, Adams SJ, Fayanju O, Shah \nSJ, Savage T, Goh E, Chaudhari AS, Aghaeepour N, Sharp C, Pfeffer MA, Liang P, Chen JH, Morse KE, \nBrunskill EP, Fries JA, Shah NH. MedAlign: A Clinician-Generated Dataset for Instruction Following \nwith Electronic Medical Records. arXiv. 2023; \n17.  Singhal K, Tu T, Gottweis J, Sayres R, Wulczyn E, Hou L, Clark K, Pfohl S, Cole-Lewis H, Neal D, \nSchaekermann M, Wang A, Amin M, Lachgar S, Mansfield P, Prakash S, Green B, Dominowska E, \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nArcas BA y, Tomasev N, Liu Y, Wong R, Semturs C, Mahdavi SS, Barral J, Webster D, Corrado GS, \nMatias Y, Azizi S, Karthikesalingam A, Natarajan V. Towards Expert-Level Medical Question \nAnswering with Large Language Models. arXiv. 2023; \n18.  Liu Z, Zhong A, Li Y, Yang L, Ju C, Wu Z, Ma C, Shu P, Chen C, Kim S, Dai H, Zhao L, Zhu D, \nLiu J, Liu W, Shen D, Li X, Li Q, Liu T. Radiology-GPT: A Large Language Model for Radiology. \narXiv. 2023; \n19.  Dave T, Athaluri SA, Singh S. ChatGPT in medicine: an overview of its applications, advantages, \nlimitations, future prospects, and ethical considerations. Front Artif Intell. 2023;6:1169595. \n20.  Liu Y, Han T, Ma S, Zhang J, Yang Y, Tian J, He H, Li A, He M, Liu Z, Wu Z, Zhao L, Zhu D, Li \nX, Qiang N, Shen D, Liu T, Ge B. Summary of ChatGPT-Related research and perspective towards the \nfuture of large language models. Meta-Radiol. 2023;1:100017. \n21.  Diaz N. 6 hospitals, health systems testing out ChatGPT [Internet]. 2023 [cited 2023 Jun 2];Available \nfrom: https://www.beckershospitalreview.com/innovation/4-hospitals-health-systems-testing-out-\nchatgpt.html \n22.  Latif E, Zhai X. Fine-tuning ChatGPT for Automatic Scoring. arXiv. 2023; \n23.  Nath C, Albaghdadi MS, Jonnalagadda SR. A Natural Language Processing Tool for Large-Scale \nData Extraction from Echocardiography Reports. PLoS ONE. 2016;11:e0153749. \n24.  Dong T, Sunderland N, Nightingale A, Fudulu DP, Chan J, Zhai B, Freitas A, Caputo M, Dimagli A, \nMires S, Wyatt M, Benedetto U, Angelini GD. Development and Evaluation of a Natural Language \nProcessing System for Curating a Trans-Thoracic Echocardiogram (TTE) Database. Bioengineering. \n2023;10:1307. \n25.  Zheng C, Sun BC, Wu Y-L, Ferencik M, Lee M-S, Redberg RF, Kawatkar AA, Musigdilok VV, \nSharp AL. Automated interpretation of stress echocardiography reports using natural language \nprocessing. Eur Hear J - Digit Heal. 2022;3:626–637. \n26.  Tang L, Kooragayalu S, Wang Y, Ding Y, Durrett G, Rousseau JF, Peng Y. EchoGen: Generating \nConclusions from Echocardiogram Notes. Proc 21st Work Biomed Lang Process. 2022;2022:359–368. \n27.  Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, Bashlykov N, Batra S, Bhargava P, \nBhosale S, Bikel D, Blecher L, Ferrer CC, Chen M, Cucurull G, Esiobu D, Fernandes J, Fu J, Fu W, \nFuller B, Gao C, Goswami V, Goyal N, Hartshorn A, Hosseini S, Hou R, Inan H, Kardas M, Kerkez V, \nKhabsa M, Kloumann I, Korenev A, Koura PS, Lachaux M-A, Lavril T, Lee J, Liskovich D, Lu Y, Mao \nY, Martinet X, Mihaylov T, Mishra P, Molybog I, Nie Y, Poulton A, Reizenstein J, Rungta R, Saladi K, \nSchelten A, Silva R, Smith EM, Subramanian R, Tan XE, Tang B, Taylor R, Williams A, Kuan JX, Xu P, \nYan Z, Zarov I, Zhang Y, Fan A, Kambadur M, Narang S, Rodriguez A, Stojnic R, Edunov S, Scialom T. \nLlama 2: Open Foundation and Fine-Tuned Chat Models. arXiv. 2023; \n28.  Tunstall L, Beeching E, Lambert N, Rajani N, Rasul K, Belkada Y, Huang S, Werra L von, Fourrier \nC, Habib N, Sarrazin N, Sanseviero O, Rush AM, Wolf T. Zephyr: Direct Distillation of LM Alignment. \narXiv. 2023; \n29.  Gardin JM, Adams DB, Douglas PS, Feigenbaum H, Forst DH, Fraser AG, Grayburn PA, Katz AS, \nKeller AM, Kerber RE, Khandheria BK, Klein AL, Lang RM, Pierard LA, Quinones MA, Schnittger I, \nEchocardiography AS of. Recommendations for a standardized report for adult transthoracic \nechocardiography: A report from the American Society of Echocardiography’s Nomenclature and \nStandards Committee and Task Force for a Standardized Echocardiography Report. J Am Soc \nEchocardiogr. 2002;15:275–290. \n30.  Loper E, Bird S. NLTK: the Natural Language Toolkit. Proc ACL-02 Work Eff tools Methodol Teach \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nNat Lang Process Comput linguistics -. 2002;63–70. \n31.  Han T, Adams LC, Papaioannou J-M, Grundmann P, Oberhauser T, Löser A, Truhn D, Bressem KK. \nMedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data. \narXiv. 2023; \n32.  Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y, Li W, Liu PJ. Exploring the \nLimits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv. 2019; \n33.  Cowan BR, Clark L, Følstad A, Skjuve M. Chatbots for customer service: user experience and \nmotivation. Proc 1st Int Conf Conversational User Interfaces. 2019;1–9. \n34.  Li M, Gong S, Feng J, Xu Y, Zhang J, Wu Z, Kong L. In-Context Learning with Many \nDemonstration Examples. arXiv. 2023; \n35.  Choi E, Jo Y, Jang J, Seo M. Prompt Injection: Parameterization of Fixed Inputs. arXiv. 2022; \n36.  Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, Naumann T, Gao J, Poon H. Domain-Specific \nLanguage Model Pretraining for Biomedical Natural Language Processing. arXiv. 2020; \n37.  Dettmers T, Pagnoni A, Holtzman A, Zettlemoyer L. QLoRA: Efficient Finetuning of Quantized \nLLMs. arXiv. 2023; \n38.  Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen W. LoRA: Low-Rank \nAdaptation of Large Language Models. arXiv. 2021; \n39.  Papineni K, Roukos S, Ward T, Zhu W-J. BLEU: a method for automatic evaluation of machine \ntranslation. Proc 40th Annu Meet Assoc Comput Linguistics - ACL ’02. 2002;311–318. \n40.  Lavie A, Agarwal A. Meteor: an automatic metric for MT evaluation with high levels of correlation \nwith human judgments. 2007;228–231. \n41.  Zhang T, Kishore V, Wu F, Weinberger KQ, Artzi Y. BERTScore: Evaluating Text Generation with \nBERT. arXiv. 2019; \n42.  Lin C-Y. ROUGE: A Package for Automatic Evaluation of Summaries [Internet]. Association for \nComputational Linguistics; p. 74–81.Available from: https://aclanthology.org/W04-1013 \n43.  Delbrouck J-B, Chambon P, Bluethgen C, Tsai E, Almusa O, Langlotz CP. Improving the Factual \nCorrectness of Radiology Report Generation with Semantic Rewards. arXiv. 2022; \n44.  Jain S, Agrawal A, Saporta A, Truong SQ, Duong DN, Bui T, Chambon P, Zhang Y, Lungren MP, \nNg AY, Langlotz CP, Rajpurkar P. RadGraph: Extracting Clinical Entities and Relations from Radiology \nReports. arXiv. 2021; \n45.  McHugh ML. Interrater reliability: the kappa statistic. Biochem Med. 2012;22:276–82. \n46.  Landis JR, Koch GG. The measurement of observer agreement for categorical data. Biometrics. \n1977;33:159–74. \n47.  Moor M, Banerjee O, Abad ZSH, Krumholz HM, Leskovec J, Topol EJ, Rajpurkar P. Foundation \nmodels for generalist medical artificial intelligence. Nature. 2023;616:259–265. \n48.  Wang S, Zhao Z, Ouyang X, Wang Q, Shen D. ChatCAD: Interactive Computer-Aided Diagnosis on \nMedical Image using Large Language Models. arXiv. 2023; \n49.  Mallio CA, Sertorio AC, Bernetti C, Zobel BB. Large language models for structured reporting in \nradiology: performance of GPT-4, ChatGPT-3.5, Perplexity and Bing.  Radiol Med. 2023;1–5. \n50.  Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N, Küttler H, Lewis M, Yih W, \nRocktäschel T, Riedel S, Kiela D. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. \narXiv. 2020; \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nFigure Legends \nFigure 1. The current workflow of summarizing echo Findings into clinically relevant Final Impressions.  \n \nFigure 2. Overview of the EchoGPT study. \n \nFigure 3. ICL performance of each LLM. Panel A to E correspond to BLEU, METEOR, ROUGE-L, \nBERT Score, and RadGraph F1 Score, respectively. Zero-shot and fine-tuned Llama-2 (EchoGPT; \nhorizontal purple dashed line) performance was included for reference. \n \nFigure 4. Model win rate heatmap illustrating the head-to-head win rate comparisons (in percentile) \namong different models based on the selected metrics. Cool colors indicate lower win rates and warmed \ncolors indicate higher win rates. We compared Llama-2 (base and fine-tuned), Zephyr (base and fine-\ntuned), T5 (base), and MedAlpaca (base). Fine-tuned Llama-2 consistently outperformed all other models \nacross all 5 automatic metrics. \nFigure 5. Human expert qualitative evaluation results. Panel A. In the 4 categories, EchoGPT \nsignificantly outperformed human experts in conciseness (p<0.001). We didn’t observe significant \ndifferences among the other three categories (completeness, correctness, and clinical utility). Panel B. \nshowed interdependence of the 4C metrics, especially the correlations between clinical utility and \ncompleteness (Pearson’s r= 0.78),  and modest to moderate correlations between other metrics. Panel C. \nCorrelations between automatic metrics and the 4C metrics. Across all automatic metrics, RadGraph F1 \nhad modest to moderate correlations with all 4 human evaluation metrics. \nSupplemental Materials \nSupplemental Figure 1. Echocardiography expert review questionnaire. Expert readers were asked to \nrate summaries A and B concerning the 4C metrics without knowing it’s a human- or LLM-generated \nsummary.  \n \n \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nTables \nTable 1. Prompt template \n Prompt Component \nPrefix “You are a knowledgeable cardiologist.” \nInstruction “For the following echocardiography report findings, \nplease write a concise summary with a minimal \namount of text.” \nSuffix (ICL only) “Use the following examples to guide word choice.” \n \nTable 2. Definition of the 4C Human Expert Evaluation Metrics \nMetric Definition \nCompleteness This metric evaluates whether the generated contents \ninclude all relevant details, elements, or aspects, \nleaving no important information gaps. \nConciseness This metric measures the quality of being clear and \nsuccinct, presenting information or content in a brief \nand straightforward manner without unnecessary \nelaboration or redundancy. \nCorrectness This metric assesses whether the generated \ninformation is true, free from mistakes, and aligned \nwith established facts or standards. \nClinical Utility This metric evaluates whether the generated \ninformation is useful in clinical practice. \n \nTable 3. Data distribution of the development and external validation sets. \n Number of \nSamples \nAverage \nNumber of \nToken \n(Findings) \nAverage \nNumber of \nToken (Final \nImpression) \nAverage \nLexical \nVariance \n(Findings) \nAverage \nLexical \nVariance \n(Final \nImpression) \nDevelopment set 97.5k 215.7 ± 55.8 88.2 ± 34.3 0.51 0.65 \nExternal validation \nset 19.6k 211.8 ± 80.5  99.8 ± 39.0 0.52 0.64 \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nTable 4.  Quantitative performance of zero-shot and QLoRA fine-tuned LLMs \nModel Flan-T5 MedAlpaca Llama-2 Zephyr \nMetric zero-shot zero-shot zero-shot QLoRA P-value* zero-shot QLoRA P-value* P-value** \nBLEU 9.8 ± 9.8 2.0 ± 4.4 6.8 ± 5.5 45.9 ± 18.9 <0.0001 3.5 ± 4.2 20.6 ± 15.5 <0.0001 <0.0001 \nMETEOR 35.6 ± 16.0 18.8 ± 9.6 21.6 ± 7.3 62.4 ± 18.0 <0.0001 21.8 ± 7.9 35.0 ± 16.1 <0.0001 <0.0001 \nROUGE-L 22.4 ± 12.0 17.5 ± 9.6 21.3 ± 8.5 55.7 ± 17.8 <0.0001 19.1 ± 7.4 32.8 ± 15.3 <0.0001 <0.0001 \nBERT \nScore 85.9 ± 2.5 81.6 ± 3.4 85.4 ± 2.0 91.6 ± 3.0 <0.0001 83.5 ± 2.7 87.4 ± 3.9 <0.0001 <0.0001 \nRadGraph \nF1 17.6 ± 10.8 11.2 ± 8.8 24.2 ± 11.6 47.7 ± 14.9 <0.0001 14.7 ± 9.4 29.3 ± 10.4 <0.0001 <0.0001 \n*Compared zero-shot to QLoRA performance of the same model; **Compared performance of QLoRA Llama-2 and \nZephyr. \n \nTable 5.  Comparison of Automatic Metrics between the Original Response and the Synthetic \nResponse \nModel: \nLlama-2 \nOriginal Response Synthetic Response: \nReplaced \nMeasurements with \nRandom Numbers \nQLoRA Random \nMeasurements  \nP-value \nBLEU Grade 1/4 left \nventricular diastolic \ndysfunction, consistent \nwith low to normal left \nventricular filling \npressure. estimated right \nventricular systolic \npressure 24 (systolic \nblood pressure 108). \ncalculated ejection \nfraction; 64%. no \nregional wall motion \nabnormalities. mildly \nenlarged mid ascending \naorta diameter of 40. \nGrade 9/53 left \nventricular diastolic \ndysfunction, consistent \nwith low to normal left \nventricular filling \npressure. estimated right \nventricular systolic \npressure 68 (systolic \nblood pressure 52). \nnormal left ventricular \nchamber size. calculated \nejection fraction; 98%. \nno regional wall motion \nabnormalities. mildly \nenlarged mid ascending \naorta diameter of 12.  \n45.9 ± 18.9 40.4 ± 16.4 <0.0001 \nMETEOR 62.4 ± 18.0 58.7 ± 16.8 <0.0001 \nROUGE-L 55.7 ± 17.8 52.2 ± 16.5 <0.0001 \nBERT \nScore \n91.6 ± 3.0 91.2 ± 2.8 <0.0001 \nRadGraph \nF1 \n47.7 ± 14.9 45.3 ± 13.7 <0.0001 \n \nTable 6. Agreement of the rating between echo-expert readers. \n \n \n \n \n \n \nMetric Fleiss’ Kappa between 4 raters \nCompleteness 0.43 \nConciseness 0.32 \nCorrectness 0.34 \nClinical Utility 0.61 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nSupplemental Tables \n \nSupplemental Table 1. Hyperparameter Search Results \n  Temperature (RP fixed at 1.1) \nRepetition Penalty (Temp \nfixed at 0.1) \n  0.1 0.5 0.9 1.2 1.3 \nLLaMA-2 \nBLEU 8.0 ± 5.4 8.8 ± 6.7 8.0 ± 6.1 5.5 ± 4.6 1.1 ± 1.8 \nMETEOR 23.3 ± 8.2 24.1 ± 8.2 23.0 ± 8.1 20.1 ± 6.9 12.9 ± 5.7 \nROUGE-L 22.3 ± 7.4 23.2 ± 7.8 23.3 ± 9.3 18.7 ± 7.0 11.6 ± 5.3 \nBERT Score 85.7 ± 1.5 86.0 ± 1.4 85.8 ± 1.6 84.7 ± 2.0 82.4 ± 1.6 \nRadGraph F1 26.2 ± 9.9 25.9 ± 10.7 26.2 ± 12.4 20.4 ± 9.9 10.3 ± 7.0 \nMedAlpaca \nBLEU 0.9 ± 2.8 0.5 ± 1.6 0.7 ± 2.0 0.1 ± 0.8 0.0 ± 0.0 \nMETEOR 12.3 ± 7.4 13.9 ± 8.9 13.1 ± 9.4 9.2 ± 6.1 5.6 ± 3.2 \nROUGE-L 9.9 ± 7.3 10.4 ± 7.9 9.4 ± 7.5 6.7 ± 4.9 3.9 ± 2.7 \nBERT Score 81.3 ± 2.7 81.6 ± 2.8 81.4 ± 3.1 80.4 ± 2.2 79.4 ± 1.6 \nRadGraph F1 7.1 ± 8.2 7.4 ± 7.1 7.9 ± 8.3 3.2 ± 3.9 1.0 ± 2.1 \nZephyr \nBLEU 4.4 ± 6.3 4.0 ± 4.5 4.4 ± 5.0 1.3 ± 2.7 0.1 ± 0.6 \nMETEOR 23.5 ± 8.3 22.8 ± 7.6 22.6 ± 9.1 18.1 ± 6.6 12.8 ± 5.8 \nROUGE-L 20.2 ± 7.8 19.6 ± 7.4 19.0 ± 7.6 14.6 ± 5.5 10.6 ± 5.1 \nBERT Score 83.6 ± 2.8 83.9 ± 2.5 83.5 ± 2.9 82.8 ± 2.0 81.5 ± 1.7 \nRadGraph F1 16.5 ± 9.6 16.4 ± 9.8 14.8 ± 10.8 7.9 ± 7.0 3.3 ± 4.7 \nFlan-T5 \nBLEU 9.6 ± 9.3 10.3 ± 10.2 9.8 ± 8.7 9.3 ± 8.2 9.9 ± 9.3 \nMETEOR 37.0 ± 14.8 37.9 ± 15.3 37.5 ± 15.0 37.8 ± 15.0 37.7 ± 14.0 \nROUGE-L 22.8 ± 10.2 23.2 ± 10.4 22.7 ± 9.6 22.6 ± 9.8 22.8 ± 10.0 \nBERT Score 85.8 ± 2.2 86.1 ± 2.2 85.9 ± 2.3 86.0 ± 2.3 85.9 ± 2.1 \nRadGraph F1 18.2 ± 10.3 19.5 ± 10.9 18.3 ± 10.0 17.9 ± 10.4 18.5 ± 9.7 \nTemp: temperature, Repetition Penalty: RP, BLEU: Bilingual Evaluation Understudy, METEOR: Metric \nfor Evaluation of Translation with Explicit ORdering, ROUGE-L: Recall-Oriented Understudy for \nGisting Evaluation - Longest Common Subsequence, BERT: Bidirectional Encoder Representations \nfrom Transformers.  \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nSupplemental Table 2. Data Distribution \n \nSupplemental Table 3. Representative Example of Zero-shot Results \nModel Ground Truth LLaMA-2 Flan-T5 Zephyr MedAlpaca \nRepresentative \nExample \nGlobal hypokinesis \nwith superimposed \nregional variation \n(please see wall \nmotion graphics).  \nestimated right \nventricular systolic \npressure 32 (systolic \nblood pressure 120). \nno pericardial \neffusion. no \nsignificant valvular \nheart disease. \nmoderate-severely \nenlarged left \nventricular chamber \nsize; calculated \nejection fraction \n25%. normal right \nventricular chamber \nsize. with borderline \ndecrease in systolic \nfunction. with normal \ninspiratory collapse \n(>50%) inferior vena \ncava size normal. \nindeterminate left \nventricular diastolic \nfunction.\" \nThe patient presents with global \nhypokinesia and regional \nvariation in left ventricular wall \nmotion. There is no evidence of \nintracardiac mass or thrombus, \nbut the left atrial appendage \ncannot be adequately visualized \nfor thrombus exclusion. The \nestimated right ventricular \nsystolic pressure is 32 mmHg, \nand there is an anterior \nepicardial fat layer. The \npulmonary valve is partially \nvisualized, and the abdominal \naorta has an incomplete Doppler \nflow pattern. The left ventricular \nwall thickness is normal, and \nthere is no atrial level shunt. \nHowever, the left atrium volume \nindex is slightly elevated, and \nthere is mild mitral valve \nregurgitation and trivial \npulmonary valve regurgitation. \nAdditionally, the patient has \nmoderate-severe left ventricular \nchamber enlargement, normal \nright ventricular chamber size, \nand normal aortic valve function. \nOverall, the patient's cardiac \nfunction appears compromised, \nlikely due to chronic heart \nfailure. Further evaluation and \nmanagement are warranted. \nCalculated 2-d \nlinear left \nventricular \nejection \nfraction 25. \nglobal \nhypokinesis \nwith \nsuperimposed \nregional \nvariation. No \nintracardiac \nmass or \nthrombus. \nLVEF: 25% with \nsuperimposed \nregional variation \n(hypokinesis) - No \nintracardiac mass, but \nLAA cannot be ruled \nout for thrombus \n(transthoracic only) - \nMild MR; trivial TR \nand PVR - Anterior \nepicardial fat; \npulmonary valve not \nwell visualized - \nNormal RV systolic \npressures, AV, and \nRA sizes - Moderate-\nsevere chamber \nenlargement; \nborderline RV syst. \nSummary: This is a good \nstudy from which I can \nconclude that the patient has \nmild MR and TR. His \nLVESD is slightly above \nnormal and his RVSV is \nslightly decreased. His PVR is \nalso low. He does have a \nthrombus in the left atrial \nappendage, although it may \nnot be noticeable on this \nparticular study. His \npulmonary artery velocity is \nnormal but the abdominal \naorta could not be measured \nadequately. So more \ninformation is needed for a \nfull diagnosis. There is some \nfat in the epicardium, which is \nalso nonspecific and might be \nseen in many patients. There \nis evidence of LV strain but \nagain, more information is \nneeded. The LA size is \nnormal as is the RV size, and \nthe IVC was normal. The \naorta was normal in size. In \ngeneral, there are no \nsignificant abnormalities and \nno cause for concern at this \ntime. \nComment -- Summary of the findings with \ntransitioning words that did not \nalter the original meaning. \nConcise \nsummary but \nomitted clinical \nimportant \ninformation. \nConcise bullet point \nsummary with false \ninformation. \nMedical Professional-\nsounding, but hallucinated \ninformation. \n \n \n \n \n  Development Set External Validation Set \nCategory Count Percentage Count Percentage \nAdult TTE 79902 81.9 14967 76.5 \nAdult TEE 7829 8.0 1247 6.4 \nExercise Stress 6781 7.0 2509 12.8 \nPharmacological Stress 2994 3.1 569 2.9 \nCarotid Ultrasound 0 0.0 262 1.3 \nIntracardiac echocardiography 0 0.0 3 0.0 \nTotal 97506 100 19557 100 \nTTE: transthoracic echocardiography, TEE: transesophageal echocardiography \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n \nSupplemental Table 4. Representative Example: In-context Learning Integrated Information from \nMultiple Examples \n \nSupplemental Table 5. Performance of Fine-tuned LLMs on the External Validation Set \nModel LLaMA-2 Zephyr   \n  QLoRA QLoRA P-value* \nBLEU 31.0 ± 18.6 17.5 ± 13.6 <0.0001 \nMETEOR 45.8 ± 19.1 32.9 ± 14.6 <0.0001 \nROUGE-L 42.0 ± 16.8 29.0 ± 13.8 <0.0001 \nBERT Score 88.9 ± 3.2 86.6 ± 3.9 <0.0001 \nRadGraph F1 36.9 ± 13.8 25.4 ± 12.3 <0.0001 \n*Compared performance of QLoRA Llama-2 and Zephyr. \n  \n \n \n  Sentence Model Summary \nCase Calculated 2-d linear left ventricular ejection fraction 57.  \"Calculated 2-d linear left ventricular ejection fraction \n57, 61, 75 (3 reports)\" ICL Examples Calculated 2-d linear left ventricular ejection fraction 56. \n  Calculated 2-d linear left ventricular ejection fraction 75 \n  Calculated 2-d linear left ventricular ejection fraction 69 \n  Calculated 2-d linear left ventricular ejection fraction 61 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \nEchoGPT\nFinding-ﬁnal impression pairs \nfrom Mayo echocardiography \nreports\nQLoRA Instruction Fine-Tuning\nIn-Context Learning\nModel Adaptation Methods\nClinical Reader Study by Echo \nExperts: the 4C metrics\nCompleteness\nConciseness\nCorrectness\nClinical Utility\nBest Model\nQuantitative  \nevaluation\n30 Cases\nQualitative \nevaluation\nFigure 2\nOpen Source LLMs (LLaMA-2, Zephyr, MedAlpaca, Flan T5)\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \nA B C\nD E\nFigure 3\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \nA B C\nD E\nFigure 4 - win rate\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \nFigure 5 - Human eval\nA B\nC\n**\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint \nSupplemental Figure 1\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 20, 2024. ; https://doi.org/10.1101/2024.01.18.24301503doi: medRxiv preprint ",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.7025777101516724
    },
    {
      "name": "Context (archaeology)",
      "score": 0.620021641254425
    },
    {
      "name": "Echo (communications protocol)",
      "score": 0.5318695306777954
    },
    {
      "name": "Computer science",
      "score": 0.45951688289642334
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4262325167655945
    },
    {
      "name": "Psychology",
      "score": 0.37283116579055786
    },
    {
      "name": "Medicine",
      "score": 0.3640073537826538
    },
    {
      "name": "Natural language processing",
      "score": 0.3550700843334198
    },
    {
      "name": "Geography",
      "score": 0.1762090027332306
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1330342723",
      "name": "Mayo Clinic",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210125099",
      "name": "Mayo Clinic in Arizona",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801572250",
      "name": "Jacksonville College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210146710",
      "name": "Mayo Clinic in Florida",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801652627",
      "name": "Association for the Advancement of Artificial Intelligence",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}