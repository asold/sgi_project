{
  "title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks",
  "url": "https://openalex.org/W4385572073",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2102945568",
      "name": "Zhengkun Zhang",
      "affiliations": [
        "Nankai University"
      ]
    },
    {
      "id": "https://openalex.org/A2115367857",
      "name": "Wenya Guo",
      "affiliations": [
        "Nankai University"
      ]
    },
    {
      "id": "https://openalex.org/A2130001558",
      "name": "Xiaojun Meng",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2111728759",
      "name": "Ya-sheng Wang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A3196791478",
      "name": "Yadao Wang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2097242334",
      "name": "Xin Jiang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2124214372",
      "name": "Zhenglu Yang",
      "affiliations": [
        "Huawei Technologies (Sweden)",
        "Nankai University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2560730294",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W2932893307",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2919290281",
    "https://openalex.org/W3173788106",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4287113019",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4289243726",
    "https://openalex.org/W4294925020",
    "https://openalex.org/W4294808066",
    "https://openalex.org/W4312884055",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W4288336812",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W3167118264",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4288026527",
    "https://openalex.org/W4287333395",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2923014074"
  ],
  "abstract": "With the scale and capacity of pretrained models growing rapidly, parameter-efficient language model tuning has emerged as a popular paradigm for solving various NLP and Vision-and-Language (V&L) tasks. In this paper, we design a unified parameter-efficient multitask learning framework that works effectively on both NLP and V&L tasks. In particular, we use a shared hypernetwork that takes trainable hyper-embeddings and visual modality as input, and outputs weights for different modules in a pretrained language model, such as the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning.). Our proposed framework adds fewer trainable parameters in multi-task learning while achieving superior performances and transfer ability compared to state-of-the-art methods. Empirical results on the GLUE benchmark and multiple V&L tasks confirm the effectiveness of our framework.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 11442‚Äì11453\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nHyperPELT: Unified Parameter-Efficient Language Model Tuning for Both\nLanguage and Vision-and-Language Tasks\nZhengkun Zhang1‚àó‚Ä†, Wenya Guo1‚Ä†, Xiaojun Meng2‚Ä†, Yasheng Wang2, Yadao Wang2\nXin Jiang2, Qun Liu2, Zhenglu Yang1‚Ä°\n1TKLNDST, CS, Nankai University, China,2Noah‚Äôs Ark Lab, Huawei Technologies,\nzhangzk2017@mail.nankai.edu.cn,wenyaguo@nankai.edu.cn\n{xiaojun.meng, wangyasheng, wangyadao, Jiang.Xin, qun.liu}@huawei.com,\nyangzl@nankai.edu.cn\nAbstract\nWith the scale and capacity of pretrained mod-\nels growing rapidly, parameter-efficient lan-\nguage model tuning has emerged as a popular\nparadigm for solving various NLP and Vision-\nand-Language (V&L) tasks. In this paper, we\ndesign a unified parameter-efficient multitask\nlearning framework that works effectively on\nboth NLP and V&L tasks. In particular, we\nuse a shared hypernetwork that takes trainable\nhyper-embeddings and visual modality as in-\nput, and outputs weights for different modules\nin a pretrained language model, such as the\nparameters inserted into multi-head attention\nblocks (i.e., prefix-tuning) and feed-forward\nblocks ( i.e., adapter-tuning.). Our proposed\nframework adds fewer trainable parameters in\nmulti-task learning while achieving superior\nperformances and transfer ability compared to\nstate-of-the-art methods. Empirical results on\nthe GLUE benchmark and multiple V&L tasks\nconfirm the effectiveness of our framework.\n1 Introduction\nPretraining and fine-tuning are now the prevalent\nparadigm in natural language processing, yield-\ning state-of-the-art performances on a variety of\ntasks (Devlin et al., 2019). With pre-trained lan-\nguage models (PLMs) growing rapidly in size, it\nbecomes increasingly infeasible to perform con-\nventional fine-tuning on the entire model parame-\nters. There has recently been one line of research\non Parameter-Efficient Language model Tuning\n(PELT)(Houlsby et al., 2019; Li and Liang, 2021;\nHe et al., 2021; Mao et al., 2022). They only up-\ndate a set of extra trainable task-specific parameters\nthat are newly introduced to PLMs. Although the\nnumber of new parameters is much fewer than the\n*Work is done at the internship of Noah‚Äôs Ark Lab, Huawei\nTechnologies.\n‚Ä†Equal Contribution.\n‚Ä°Corresponding authors.\noriginal PLM, training these parameters per sin-\ngle task is still costly, especially when targeting a\nnumber of tasks, i.e., multi-tasking scenario.\nTherefore, we are motivated to start with a uni-\nfied parameter-efficient language model tuning\nframework (He et al., 2021) and explore on a shared\nhypernetwork (von Oswald et al., 2020; Mahabadi\net al., 2021) that is able to take multi-task informa-\ntion as input, and generate weights for tuning differ-\nent task-specific modules of PLMs, such as the pa-\nrameters inserted into multi-head attention blocks\n(i.e., prefix-tuning) and feed-forward blocks (i.e.,\nadapter-tuning.). We name it HyperPELT. Be-\nsides, we propose a novel perspective of adopting\nparameter-efficient multimodal fusion for PLMs\nvia the hypernetwork. Thus we explore to use an\nadditional separate hypernetwork handling visual\ninput and generating visual-specific weights for\nmultiple modules of PLMs.\nEmpirical results on 8 tasks of GLUE bench-\nmark show that HyperPELT achieves superior per-\nformances (87.09 vs. 86.53) with a tenth of the\nparameters (0.24% vs. 2.96%) when compared to\nstate-of-the-art alternatives. Study on the few-shot\ntransfer learning indicates that HyperPELT is more\nstable and efficient than alternatives. It confirms\nthe effectiveness of our unified parameter-efficient\nmultitask learning framework. What‚Äôs more, we\nevaluate our framework on V&L multi-tasks (4\ntasks). Results show the promising performance of\nour novel fusion method on extending V&L ability\non top of PLMs via hypernetworks.\nIn summary, we make the following contribu-\ntions: (1) propose a unified parameter-efficient\nmultitask learning framework that is able to take\nmulti-task and multi-modality information as in-\nput, and generate weights for tuning different task-\nspecific modules of PLMs; (2) present a novel\nperspective of using hypernetworks to achieve the\nparameter-efficient multimodal fusion on top of\nPLMs; (3) design various experiments to compre-\n11442\nFor visual embedding\nNonlinear\nLayer norm\nWup bup\nWdown bdown\nIv\nBlock\nEmbedding\nTask\nEmbedding\nMLP\nPrefix\nKQ VPK PV\nMulti-head Attention\nLayer norm\nLayer norm\nAdapter\nN√ó\nEncoder Hyper-Embedding Computing\nAdapter\nPk\nPv\nIœÑ\nFFN\nUpdated layer Frozen layer\nLayer\nEmbeddingNonlinear\nLayer norm\nWup bupAdapter\nŒª√ó\nIœÑ\nPK\nPV\nWdown bdown\n‚ÑéùëÉùëÉ\nùë£ùë£\n‚ÑéùëÉùëÉ\nùúèùúè\n‚Ñéùê¥ùê¥\nùë£ùë£\n‚Ñéùê¥ùê¥\nùúèùúè\nFor task embedding\nBlock\nEmbedding\nVisual\nEmbedding\nMLPIv\nLayer\nEmbedding\nImage\nCLIP\nMapping \nLayer\nFigure 1: The model structure of the proposed unified pure language and V&L multi-task framework (left), and\nillustration of computing the hyper-embedding (right). We use green color to fill the trainable layers and grey color\nfor the frozen ones. And the dashed parts denote the modules for processing visual modality.\nhensively demonstrate the effectiveness of our pro-\nposed framework in multi-task learning and few-\nshot domain transfer scenarios.\n2 Related Work\nExisting research has explored a large amount of\nmethods on parameter-efficient tuning, such as the\nwidely used adapter-tuning (Houlsby et al., 2019),\nprefix-tuning (Li and Liang, 2021) and the mixed\nmethods (He et al., 2021; Mao et al., 2022). How-\never, it is time & space-consuming to deal with\na set of tasks in multi-task learning if we simply\nupdate and save separate replicas of model param-\neters per single task. In this work, we explore a\nhypernetwork-based multi-task learning framework\nto generate weights for different PELT modules.\nBesides, there has been a series of recent\nwork (Cho et al., 2021; Tsimpoukelli et al., 2021;\nSung et al., 2021; Alayrac et al., 2022) to equip a\nlanguage model with the ability of handling visual\ninput with a small number of trainable modules and\nparameters. Different from existing work, we pro-\npose a novel perspective of multimodal fusion via\nextending the proposed parameter-efficient multi-\ntask learning framework. We further review recent\nresearch on parameter-efficient tuning for pure lan-\nguage and V&L tasks, as well as the corresponding\nwork for multi-task learning in Appendix A.\n3 Proposed Method\nWe target a general multi-task learning problem,\nwhich is formulated in Appendix B. In this section,\nwe describe the hyper-embedding I for hypernet-\nworks to generate weights ‚àÜŒ∏and which modules\nof PLMs to insert these weights to achieve PELT.\nIn our methods, the hyper-embedding I consists of\ntwo: task-specific hyper-embedding IœÑ, and visual-\nspecific hyper-embedding Iv. We will mostly intro-\nduce the hyper-embedding IœÑ, and Iv is used in a\nsimilar parallel manner. A simple linear projection\nlayer is employed as the hypernetwork, for exam-\nple, hœÑ\nP(.) and hv\nP(.) are used for prefix-tuning,\nwhile hœÑ\nA(.) and hv\nA(.) are for adapter-tuning as\nshown in Figure 1. The hypernetwork takes the\nhyper-embedding I as input, and outputs weights\nfor multiple modules of PLMs.\n3.1 Hyper-Embedding for PELT\nConsidering a flexible parameterization of task-\nspecific parameters for Llayers of transformer, we\nintroduce a set of layer id embeddingsI= {li}L\ni=1,\nand block type embeddings B= {bj}5\nj=1, which\nspecify the position where the parameters ‚àÜŒ∏are\ninserted to. Then, we compute a hyper-embedding\nIœÑ ‚ààRdI for each individual task via a task projec-\ntor network, which is a multi-layer perceptron con-\nsisting of two feed-forward layers and a ReLU non-\nlinearity: IœÑ = MLP([zœÑ,li,bj]). Thus, it learns a\nsuitable compressed hyper-embedding from a con-\ncatenation of task embeddings zœÑ ‚ààRdœÑ, layer id\n11443\nembeddings li ‚ààRdœÑ, and block type embeddings\nbj ‚ààRdœÑ. In this way, the hypernetwork is able to\nproduce distinct weights for tuning each task, and\neach transformer block at each layer.\n3.2 HyperPELT: Incorporate with\nPrefix-tuning and Adapter-tuning\nTo further capture knowledge across tasks and\ntransfer to others, we follow the unified parameter-\nefficient framework (He et al., 2021), and input the\nhyper-embedding to a hypernetwork for generating\nthe weights in adapters as well as prefix vectors.\nWe extend the dimension for different embeddings\nto match the prefix length N, i.e., z ‚àà RN√ódœÑ,\nli ‚ààRN√ódœÑ, bj ‚ààRN√ódœÑ, and then compute the\nhyper-embedding IœÑ ‚ààRN√ódI. We finally employ\na hypernetwork hœÑ\nP(.) with trainable parameters\nŒ∏hœÑ\nP, to project IœÑ to prefix vectors PœÑ ‚ààRN√ód:\nPœÑ = hœÑ\nP(Œ∏hœÑ\nP,IœÑ) .\nBesides, as depicted in Figure 1, we introduce\na hypernetwork-based adapter layer with a train-\nable scaled parameter Œª, which is inserted par-\nallelly with feed-forward blocks. We generate\nadapter weights (WœÑ\nup,WœÑ\ndown) through a hypernet-\nwork hœÑ\nA(.): (WœÑ\nup,WœÑ\ndown) := hœÑ\nA(Œ∏hœÑ\nA,IœÑ), where\nWœÑ\ndown ‚ààRdmid√ód and WœÑ\nup ‚ààRd√ódmid .\n3.3 VL-HyperPELT: Incorporate with Visual\nModality\nAs illustrated in Fig. 1, we use CLIP (Radford\net al., 2021) with a trainable visual mapping layer,\nwhich projects the visual representation to the\nidentical dimension of task embedding, i.e., zv ‚àà\nRN√ódv,dv = dœÑ. Then we feed this visual rep-\nresentation zv to a visual projector network. In\nthis way, we learn the visual hyper-embedding\nIv ‚ààRdI. Finally, taking the visual-specific hyper-\nembeddings as input, we use visual-specific hyper-\nnetworks to generate visual-specific parameters to\ndifferent modules in PLMs. Similar to the Sec-\ntion 3.1 & 3.2, the incorporation of visual-specific\nparameters to PLMs are the same as task-specific\nones, e.g., used as prefix vectors via a prefix hyper-\nnetwork hv\nP(.) and adapter weights via an adapter\nhypernetwork hv\nA(.). We name it VL-HyperPELT.\n4 Results and Analysis\nWe conduct a series of experiments to verify the\neffectiveness of our proposed framework compared\nto existing ones.\n4.1 Implementation Details\nOur models are built on T5BASE (Raffel et al.,\n2020) 1, which contains 12 layers and 222M pa-\nrameters, and use the tokenizer of T5 to tokenize\ntext inputs. We set N = 49 , d = dœÑ = 768 ,\ndI = 64 for all the experiments. Following the\ntraining strategies from Raffel et al. (2020), we\nfine-tune all models with a constant learning rate of\n0.001, use 218 = 262144 steps in all experiments\nwith batch size of 128 and sample tasks via the\nconventional temperature-based sampler with tem-\nperature T = 2 , i.e., sample corresponding task\nproportional to p1/T\nœÑ , where pœÑ = NœÑ‚àëT\ni=1 NœÑ\nand\nNœÑ is the number of training samples for the œÑ-th\ntask. We did not experiment with other complex\nsampling strategies or tuning of T. For the experi-\nments under multi-task training settings, we save\na checkpoint every 1000 steps and report results\non a single checkpoint with the highest average\nvalidation performance across all tasks.\nIn terms of the vision-and-language scenarios,\nwe convert V&L tasks to the text generation for-\nmat as Cho et al. (2021). We use ResNet101 as\nour vision encoder, and initialize it with weights\nfrom pretrained CLIP (Radford et al., 2021). Input\nimages are resized to 224 √ó224 for memory effi-\nciency. We extract the 7√ó7 grid features produced\nby the last convolutional layer. The percentage of\nupdated parameters is also reported as one metric\nfor approach efficiency, and we do not take visual\nencoder into account since it is frozen in our exper-\niment.\n4.2 Datasets\nOur framework is evaluated on the GLUE bench-\nmark (Wang et al., 2019b) in terms of natural\nlanguage understanding. This benchmark cov-\ners multiple tasks of paraphrase detection (MRPC,\nQQP), sentiment classification (SST-2), natural lan-\nguage inference (MNLI, RTE, QNLI), and linguis-\ntic acceptability (CoLA). The original test sets are\nnot publicly available, and following Zhang et al.\n(2021), for datasets fewer than 10K samples (RTE,\nMRPC, STS-B, CoLA), we split the original valida-\ntion set into two halves, one for validation and the\nother for testing. For other datasets, we randomly\nsplit 1K samples from the training set for validation\nand test on the original validation set.\nIn addition, we evaluate the few-shot transfer\nperformance on four tasks and datasets: 1) the\n1https://huggingface.co/t5-base\n11444\nMethods #Total\nparams\n#Trained\nparams/taskCoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE Avg\nSingle-Task Training\nT5BASE‚Ä† 8.0√ó 100% 54.85 92.19 88.18/91.6191.46/88.6189.55/89.4186.4991.60 67.3984.67\nAdapters‚Ä† 1+8√ó0.01 0.87% 59.4993.4688.18/91.55 90.94/88.01 87.44/87.18 86.38 92.2668.8484.88\nMulti-Task Training\nT5BASE‚Ä† 1.0√ó 12.5% 54.88 92.54 90.15/93.01 91.13/88.07 88.84/88.53 85.66 92.04 75.3685.47\nAdapters‚Ä† 1.07√ó 0.82% 61.5393.00 90.15/92.91 90.47/87.26 89.86/89.4486.0993.1770.2985.83\nPrefix-tuning‚ô£ 1.14√ó 1.72% 56.67 93.9289.42/92.57 90.59/87.37 89.49/89.34 85.23 93.1779.1786.09\nMAMAdapters‚ô£ 1.15√ó 2.96% 56.53 93.58 91.35/93.9690.58/87.5388.89/88.76 85.98 92.77 81.9486.53\nHYPERFORMER++‚Ä† 1.02√ó 0.29% 63.73 94.0389.66/92.63 90.28/87.20 90.00/89.66 85.7493.0275.3686.48\nHyperPELT 1.02√ó 0.24% 65.9693.23 89.42/92.31 90.48/87.5489.15/89.07 85.35 92.79 82.6487.09\nTable 1: Performance of all models on the GLUE tasks. For each method, we report the total number of parameters\nacross all tasks and the number of parameters that are trained for each task as a multiple and proportion respectively\nof the baseline single-task T5 model. ‚Ä†: Results from the implementation of Mahabadi et al. (2021), ‚ô£: We\nimplement the methods of Li and Liang (2021) and He et al. (2021) on top of T5.\nFigure 2: Few-shot domain transfer results of five different tasks averaged across 5 seeds. We compute accuracy for\nall tasks and datasets. HyperPELT and HyperPELT TaskEmbed are respectively fine-tuning hypernetworks with all\nhyper-embeddings and only task embedding in the few-shot learning.\nnatural language inference (NLI) datasets CB and\n2) the question answering (QA) dataset BoolQ from\nSuperGLUE (Wang et al., 2019a); 3) the sentiment\nanalysis datasets IMDB (Maas et al., 2011); and\n4) the paraphrase detection dataset PAWS (Zhang\net al., 2019). For CB and BoolQ, since the test set\nis not available, we split the validation set into two\nhalves, one for validation and the other for testing.\nFor IMDB, since the validation set is not available,\nwe similarly split the test set to form validation.\nFor PAWS, we report on the original test set.\nTo evaluate our framework on V&L tasks, we ex-\nperiment on four datasets COCO (Lin et al., 2014),\nVQA (Goyal et al., 2017), VG-QA (Krishna et al.,\n2017) and GQA (Hudson and Manning, 2019). Fol-\nlowing Cho et al. (2021), we use VQA Karpathy\nsplit, which splits the VQA dataset into 605,102\n/ 26,729 / 26,280 image and question pairs sep-\narately as the train/validation/test set to evaluate\nVQA tasks in a generative manner. We further\nevaluate our framework on two datasets for V&L\nfew-shot transfer learning: OKVQA (Marino et al.,\n2019); SNLI-VE (Xie et al., 2018).\n4.3 Results on the GLUE Benchmark\nWe conduct experiments on GLUE for both single-\nand multi-task settings, as shown in Table 1. Com-\npared to the single-task Adapters that finetunes\nall newly introduced parameters in adapters, our\nmethod yields a significant improvement by 2.21%\nwith much fewer trainable parameters. It illustrates\nthe effectiveness of our proposed multi-task train-\ning framework. The comparison to MAMAdapter\nshows that using hypernetwork to tune each trans-\nformer module and thus learn the shared knowledge\nacross multitasks, leads to an improvement in task\nperformance (86.53 vs. 87.09) while training fewer\nparameters (2.96% vs. 0.24%). Overall, our Hy-\nperPELT obtains the best performance with less\ntrainable parameters.\n4.4 Few-shot Domain Transfer\nWe use the above models trained on GLUE as re-\nported in Table 1, and evaluate them on the test set\nof four different tasks, i.e., PAWS, IMDB, BoolQ,\nand CB, after being few-shot finetuned on each\ntarget training data, as shown in Figure 2. For the\n11445\nMethods Trained\nParams (%)\nVQAv2 VQA Karpathy test GQA COCO Caption\ntest-std in-domain out-domain overalltest-devB@4 M C S\nSingle-Task Training\nVL-T5‚Ä† 100% 70.3 71.4 13.1 67.9 60.0 34.6 28.8 116.1 21.9\nMulti-Task Training\nVL-T5‚Ä† 100% - - - 67.2 58.9 - - 110.8 -\nCLIP-T5‚Ä† 100% - - - 67.3 56.5 - - 113.1 -\nCLIP-T5‚ô† 100% 69.8 70.8 17.4 66.8 59.6 32.4 27.1 108.5 20.4\nVL-Adapter‚Ä† 7.98% - - - 67.6 56.2 - - 111.8 -\nVL-Adapter‚ô† 7.16% 69.4 70.0 16.4 65.9 57.6 31.4 27.2 105.6 20.1\nVL-HyperPELT6.62% 69.6 70.3 16.8 66.3 57.9 32.1 27.0 108.2 20.1\nTable 2: Experimental results on V&L banchmarks. We report vqa-score for VQA, gqa-score for GQA and various\nmetrics for image captioning (B@4: BLEU@4, M: METEOR, C: CIDEr, S: SPICE). ‚Ä†: Results from the paper of\nCho et al. (2021) and Sung et al. (2021), ‚ô†: Our re-implementation of Sung et al. (2021).\ntasks of CB and BoolQ from SuperGLUE, even\nthough the backbone T5 was previously trained on\nthe train sets of these two, the performance of all\nmethods differs a lot. The two baselines still do not\nwork with very few samples, like 4 and 16 samples.\nTherefore, we assume that the two baselines suf-\nfer from catastrophic forgetting problems to some\ndegree during multi-task training. In contrast, our\nproposed HyperPELT works effectively on these\ntwo tasks. We speculate that the reason might be\nthe use of hypernetworks on both prefix-tuning and\nadapter-tuning modules of transformer. We leave\nthis exploration to our future work.\nBesides, we show the results of Prompt-\ntuning (Lester et al., 2021) and fine-tuning only\nthe task embedding in our HyperPELT. Note\nthat in this comparison, we keep the same train-\nable parameters between these two methods, i.e.,\nRN√ódœÑ, where N denotes the prompt length in\nPrompt-tuning method. Our HyperPELT TaskEm-\nbed mostly achieves a comparable or even better\nperformance than Prompt-tuning.\n4.5 Results on Vision-and-Language\nBenchmarks\nWe compare the pre-trained and full fine-tuning\nVL-T5 (Cho et al., 2021), and other adapter-based\nmethods built on top of T5, i.e., CLIP-T5and VL-\nAdapter (Sung et al., 2021) in the multi-task train-\ning setting. The results and the number of trainable\nparameters are reported in Table 2. Since the used\ndataset is slightly different from Sung et al. (2021)\nand their checkpoint is not avaliable at this time,\nwe re-implement CLIP-T5 and VL-Adpater. Com-\npared to which, our method achieves a comparable\nperformance with a fewer number of trainable pa-\nrameters (e.g., 7.16% of VL-Adapter vs. 6.62% of\nVL-HyperPELT).\nWe further evaluate our models on multimodal\nfew shot learning tasks and show its superiority\nin appendix E.1. To our best knowledge, we are\nthe first to employ the visual modality to tune the\nvery few parameters of different transformer blocks,\ninstead of normally inserting image patch tokens\nto the input sequence. Experimental results evi-\ndence the effectiveness of our novel approach, thus\nproviding a new perspective on how to extend the\nmulti-modality capability on top of PLMs.\n5 Discussion and Conclusion\nIn this paper, we propose a unified parameter-\nefficient tuning framework for multitasks. On the\none hand, we use the hypernetwork to reduce the\nscale of trainable parameters of existing adapter-\ntuning and prefix-tuning modules. On the other\nhand, for the V&L tasks, we directly integrate the\nimage features into the prefix vectors as well as\nadapters, which further reduces the number of train-\nable parameters for processing visual input. Exten-\nsive experiments on pure language and V&L tasks\ndemonstrate the superiority of our proposed frame-\nwork in both multi-tasking and few-shot settings.\nIn the future, we plan to explore more combination\nof methods across tuning task-specific and visual-\nspecific parameters for different modules of PLMs.\nLimitations\nOur experiments are conducted based on the T5-\nbase pre-trained language model. Due to the com-\nputational resource constraints, we did not conduct\nexperiments on other similar PLMs, such as BART,\nand T5 model with larger scale, such as T5-large\n11446\nand T5-3B. Although we believe our conclusion\ncan generalize to other backbones since T5 is a\nclassical encoder-decoder model, we will conduct\nmore experiments to confirm for future work.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\n2019, Volume 1 (Long and Short Papers), pages 3874‚Äì\n3884. Association for Computational Linguistics.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne\nMonteiro, Jacob Menick, Sebastian Borgeaud, An-\ndrew Brock, Aida Nematzadeh, Sahand Sharifzadeh,\nMikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. 2022.\nFlamingo: a visual language model for few-shot\nlearning. CoRR, abs/2204.14198.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.\nUnifying vision-and-language tasks via text genera-\ntion. In Proceedings of the 38th International Con-\nference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, pages 1931‚Äì1942.\nPMLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171‚Äì4186. Association for Computational\nLinguistics.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei\nChen, Yang Liu, Jie Tang, Juanzi Li, and Maosong\nSun. 2022. Delta tuning: A comprehensive study of\nparameter efficient methods for pre-trained language\nmodels. CoRR, abs/2203.06904.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the V in VQA\nmatter: Elevating the role of image understanding in\nvisual question answering. In 2017 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2017, Honolulu, HI, USA, July 21-26, 2017, pages\n6325‚Äì6334. IEEE Computer Society.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021. Towards a\nunified view of parameter-efficient transfer learning.\nCoRR, abs/2110.04366.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research, pages 2790‚Äì2799.\nPMLR.\nDrew A. Hudson and Christopher D. Manning. 2019.\nGQA: A new dataset for real-world visual reason-\ning and compositional question answering. In IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pages 6700‚Äì6709. Computer Vision Founda-\ntion / IEEE.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A. Shamma,\nMichael S. Bernstein, and Li Fei-Fei. 2017. Vi-\nsual genome: Connecting language and vision us-\ning crowdsourced dense image annotations. Int. J.\nComput. Vis., 123(1):32‚Äì73.\nJaejun Lee, Raphael Tang, and Jimmy Lin. 2019. What\nwould elsa do? freezing layers during transformer\nfine-tuning. CoRR, abs/1911.03090.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021, pages 3045‚Äì\n3059. Association for Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pages 4582‚Äì\n4597. Association for Computational Linguistics.\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\nand C. Lawrence Zitnick. 2014. Microsoft COCO:\ncommon objects in context. In Computer Vision -\nECCV 2014 - 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V, volume 8693 of Lecture Notes in Computer\nScience, pages 740‚Äì755. Springer.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nCoRR, abs/2107.13586.\n11447\nYuhan Liu, Saurabh Agarwal, and Shivaram Venkatara-\nman. 2021b. Autofreeze: Automatically freez-\ning model blocks to accelerate fine-tuning. CoRR,\nabs/2102.01386.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn The 49th Annual Meeting of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Proceedings of the Conference, 19-24 June,\n2011, Portland, Oregon, USA, pages 142‚Äì150. The\nAssociation for Computer Linguistics.\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa\nDehghani, and James Henderson. 2021. Parameter-\nefficient multi-task fine-tuning for transformers via\nshared hypernetworks. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, Au-\ngust 1-6, 2021, pages 565‚Äì576. Association for Com-\nputational Linguistics.\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-\nhairi, Hao Ma, Jiawei Han, Scott Yih, and Madian\nKhabsa. 2022. Unipelt: A unified framework for\nparameter-efficient language model tuning. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 6253‚Äì6264. Association for Computa-\ntional Linguistics.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. OK-VQA: A visual\nquestion answering benchmark requiring external\nknowledge. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2019, Long Beach,\nCA, USA, June 16-20, 2019, pages 3195‚Äì3204. Com-\nputer Vision Foundation / IEEE.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, pages 8748‚Äì8763.\nPMLR.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67.\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2021.\nVl-adapter: Parameter-efficient transfer learning for\nvision-and-language tasks. CoRR, abs/2112.06825.\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi,\nS. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal few-shot learning with frozen language\nmodels. In Advances in Neural Information Pro-\ncessing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pages 200‚Äì212.\nJohannes von Oswald, Christian Henning, Jo√£o Sacra-\nmento, and Benjamin F. Grewe. 2020. Continual\nlearning with hypernetworks. In 8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019a. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada,\npages 3261‚Äì3275.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nNing Xie, Farley Lai, Derek Doran, and Asim Kadav.\n2018. Visual entailment task for visually-grounded\nlanguage learning. CoRR, abs/1811.10582.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Wein-\nberger, and Yoav Artzi. 2021. Revisiting few-sample\nBERT fine-tuning. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPAWS: paraphrase adversaries from word scrambling.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\n2019, Volume 1 (Long and Short Papers), pages 1298‚Äì\n1308. Association for Computational Linguistics.\nA Related Work\nIn this section, we review recent research on\nparameter-efficient tuning for pure language and\nV&L tasks, as well as the corresponding work for\nmulti-task learning.\n11448\nMethod Number of Tunable Parameters\nPrompt Tuning N √ó d\nPrefix Tuning N √ó d + (1 + 2√ó L) √ó dmid √ó d √ó Battn\nAdapter 2 √ó dmid √ó d √ó (Battn + Bffn) √ó L\nMAM Adapter N √ó d + (1 + 2√ó L) √ó dmid √ó d √ó Battn + 2√ó dmid √ó d √ó Bffn √ó L\nHYPERFORMER++ (N + Battn + Bffn + L) √ó dt + dt √ó dmid\nI + dmid\nI √ó dI + 2√ó dI √ó (dmid √ó d)\nHyperPELT (N + Battn + Bffn + L) √ó dt + dt √ó dmid\nI + dmid\nI √ó dI + 2√ó dI √ó d + 2√ó dI √ó (dmid √ó d)\nTable 3: Number of tunable parameters of various parameter-efficient tuning methods with T5 models.\nA.1 Parameter-Efficient Multi-task Learning\nAs recent models grow rapidly in size, how to fine-\ntune pretrained models with a small number of\ntrainable parameters becomes more crucial. Ex-\nisting research (Liu et al., 2021a; Ding et al.,\n2022) has explored a large amount of methods on\nparameter-efficient tuning. These methods gener-\nally include two categories according to whether\nnew trainable parameters are introduced. One cate-\ngory is that only a subset of model parameters can\nbe updated while freezing the remain (Liu et al.,\n2021b; Lee et al., 2019). The other is introduc-\ning a few task-specific new parameters to different\nparts of pretrained models, such as multi-head at-\ntention (Li and Liang, 2021) and feedforward lay-\ners (Houlsby et al., 2019). In this method, a small\nnetwork (often named as hypernetwork with the\ninput embedding named as hyper-embedding) is\noften used to generate weights for a main network.\nOn the other hand, learning a unified model to\nperform well on multiple tasks ( i.e., multi-task\nlearning) is a challenging problem. It has to ad-\ndress many challenges such as catastrophic forget-\nting, and model overfitting in low-resource tasks\nwhile underfitting in high-resource tasks (Aharoni\net al., 2019). Radford et al. (2019) highlights the\nability of language models to perform a wide range\nof multitasks in a zero-shot setting. Mahabadi et al.\n(2021) proposes to use a shared hypernetwork (von\nOswald et al., 2020) to generate weights for a small\nnumber of parameters in adapter modules, thus to\nallow the model to adapt to each individual task in\na parameter-efficient manner.\nA range of recent work aims to unify parameter-\nefficient tuning methods (He et al., 2021; Mao et al.,\n2022), to achieve better tuning performance. We ex-\nplore a framework to generate weights for different\nPELT methods using the hypernetwork. Compared\nto only generating weights for adapters, empirical\nresults indicate that generating weights for multiple\nmodules of PLMs achieves superior performance\nwith fewer trainable parameters.\nA.2 Parameter-Efficient Tuning towards\nVision-and-Language\nBuilding vision-and-language models on top of\nPLMs pretrained on pure large text corpora has led\nto a noticeable improvement to V&L tasks (Cho\net al., 2021). There is a series of recent work that\nextends the ability of language models to handle\nmultimodal input in a parameter-efficient manner.\nFor example, Frozen (Tsimpoukelli et al., 2021)\naligns the image representation to the text repre-\nsentation space of frozen GPT model which thus is\nable to generate captions for images. VL-Adapter\n(Sung et al., 2021) introduces a limited set of new\ntrainable parameters to T5 via the adapter-tuning\napproach that can match the performance of fine-\ntuning the entire model. Flamingo (Alayrac et al.,\n2022) uses an extra cross-attention module, whose\nkeys and values are generated via visual features,\nthus enabling language modeling conditioned on\nvisual inputs. Different from existing work, we\npropose a novel perspective of parameter-efficient\nmultimodal fusion. We introduce a seperate visual-\nspecific hypernetwork for handling visual input and\ngenerating weights for PLMs.\nB Mutli-task Learning Problem\nFormulation\nOur paper targets at a general multi-task learning\nproblem, where we are given the data from a set\nof tasks {DœÑ}T\nœÑ=1. T is the total number of tasks\nand DœÑ = {(xi\nœÑ,yi\nœÑ)}NœÑ\ni=1 is the training data of\nthe œÑ-th task with NœÑ samples. We are also given\na large-scale pretrained language model, i.e., T5,\nparameterized by Œ∏, which generates the output yi\nœÑ\nfor input xi\nœÑ. The standard multi-task finetuning\nminimizes the following loss on the training set:\nLtotal =\nT‚àë\nœÑ=1\n‚àë\n(xiœÑ,yiœÑ)‚ààDœÑ\nLtask(Œ∏,xi\nœÑ,yi\nœÑ), (1)\n11449\nTask Input Text Target Text\nGLUE Tasks\nCoLA cola sentence: [sentence] acceptable/unacceptable\nSST-2 sst2 sentence: [sentence] positive/negative\nMRPC mrpc sentence1: [sentence1] sentence2: [sentence2] equivalent/not_equivalent\nQQP qqp question1: [question1] question2: [question2] duplicate/not_duplicate\nSTS-B stsb sentence1: [sentence1] sentence2: [sentence2] 0.0 - 5.0\nMNLI mnli hypothesis: [hypothesis] premise: [premise] entailment/neutral/contradiction\nQNLI qnli question: [question] sentence: [sentence] entailment/not_entailment\nRTE rte sentence1: [sentence1] sentence2: [sentence2] entailment/not_entailment\nFew-shot Tasks\nCB cb hypothesis: [hypothesis] premise: [premise] entailment/neutral/contradiction\nBoolQ boolq question: [question] context: [context] True/False\nIMDB imdb sentence: [sentence] positive/negative\nPAWS paws sentence1: [sentence1] sentence2: [sentence2] equivalent/not_equivalent\nVision-and-Language Tasks\nCOCO caption: [caption]\nVQA vqa question: [question] [answer]\nGQA gqa question: [question] [answer]\nVision-and-Language Few-shot Tasks\nOKVQA okvqa question: [question] [answer]\nSNLI-VE snli-ve premise: [premise] entailment/neutral/contradiction\nTable 4: Input-output formats for NLU and Vision-and-Language tasks. Following Raffel et al. (2020); Cho et al.\n(2021), we use different prefixes (such as ‚Äúcola sentence:‚Äù, ‚Äúvqa question:‚Äù) for questions from different datasets.\nwhere Ltask is the loss function of the tasks that is\nusually defined as the cross-entropy loss. Our goal\nis to efficiently finetune the given model in this\nmulti-task learning setting, allowing knowledge\nsharing across tasks, and at the same time, enabling\nthe model to adapt to each individual task.\nWe aim to integrate a unified hypernetwork-\nbased parameter-efficient transfer learning method\ninto a multi-task transformer model. In other word,\nwe insert the parameters generated by the hyper-\nnetworks ‚àÜŒ∏ into the layer and attention blocks\nof PLMs. During training, we only update the hy-\npernetwork parameters Œ∏h with hyper-embedding\n{IœÑ}T\nœÑ=1 and parameters in layer normalization,\nwhile the remaining model parameters inŒ∏are fixed\nas in the Equation 2.\nLtotal =\nT‚àë\nœÑ=1\n‚àë\n(xiœÑ,yiœÑ)‚ààDœÑ\nLtask(‚àÜŒ∏,Œ∏,x i\nœÑ,yi\nœÑ)\n=\nT‚àë\nœÑ=1\n‚àë\n(xiœÑ,yiœÑ)‚ààDœÑ\nLtask(IœÑ,Œ∏h,Œ∏,x i\nœÑ,yi\nœÑ)\n,\n(2)\nC Number of Tunable Parameters\nFollowing He et al. (2021), to simplify the com-\nputation of tunable parameters, we compute the\nsum of parameter used in one encoder layer and\none decoder layer as the parameter overhead of\none single layer of the pre-trained encoder-decoder\nmodel. T5 has an encoder-decoder structure that\nhas L layers. Each layer has Battn blocks and\nBffn blocks. For the encoder-decoder models like\nT5, Battn = 3 : the encoder self-attention block,\nthe decoder self-attention block and the decoder\ncross-attention block and Bffn = 2: encoder feed-\nforward block and decoder feed-forward block.\nFor modifications applied at the attention blocks,\nthe number of tunable parameters is computed\nby Œ∏attn = Œ∏attn\nW √óBattn √óL, where Œ∏attn\nW denotes\nthe number of parameters used for one attention\nsub-layer. Similarly, the number of tunable pa-\nrameters for the FFN sub-layers is computed by\nŒ∏ffn = Œ∏ffn\nW √óBffn √óL. Finally, the total number\nof tunable parameters for prefix tuning and adapter\nvariants is Œ∏ = Œ∏attn + Œ∏ffn as applicable. Using\nT5 as an example, we present the number of pa-\nrameters used by several representative methods\nthroughout our paper in Tab. 3.\nD Experimental Setup\nD.1 Input-Output Formats\nAs shown in Tab. 4, we formulate the input text\nand labels from each task to the corresponding\n11450\nFigure 3: Few-shot domain transfer results of two differ-\nent V&L tasks averaged across 5 seeds. We report the\nvqa-score on OKVQA validation split, and the accuracy\non SNLI-VE test-P split.\ntarget text, and we learn these different tasks by\npredicting target text with the language modeling\nobjective in Eq. 2.\nE Additional Results and Analysis\nE.1 Multimodal Few-shot Learning\nWe further use the models trained on V&L tasks as\nreported in Figure 3 and evaluate them on the test\nset after few-shot fine-tuning on OKVQA (Marino\net al., 2019) and SNLI-VE (Xie et al., 2018). For\nOKVQA, since there is no test set, we split its origi-\nnal validation set into two halves, one for validating\nand the other for testing. For SNLI-VE, we use its\nvalidation set for validating, and test-P set for test-\ning and reporting results. We follow the methods\nin Section 4.4 to select samples, and report results\nin Figure 3.\nCompared with the full parameter fine-tuning,\ni.e., CLIP-T5, and the previous parameter-efficient\nV&L method VL-Adapter, our method achieves\nthe best performance. It is also worth noting that\nfor the used five random seeds, the variance of\nour method is generally smaller than VL-Adapter,\nwhich indicates that our method is more robust in\nthis few-shot learning scenario. We believe that our\nframework, though training less parameters, can\nstill capture knowledge across tasks and transfer\nthem in the multimodal few-shot setting.\n11451\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nSection 6\n‚ñ° A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nLeft blank.\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\nSection 4\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nAppendix D.2\n‚ñ°\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAll of the datasets used in this paper use open-source licenses, and we make no modiÔ¨Åcations to\nthe datasets in this paper. We will mark the open source licenses of the datasets in the open-source\nrepository.\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAppendix D.2\n‚ñ°\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThese problems have been discussed in the original paper or websites which published the datasets.\n‚ñ°\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nThese information have been stated in the original paper or websites which published the datasets.\nWe cite the link of each datasets used andthe reviewer can Ô¨Ånd these information there.\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nAppendix D.2\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n11452\nC ‚ñ°\u0013 Did you run computational experiments?\nSection 4\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4 and Appendix D.2\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix D.2\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix D.2\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n11453",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8244339227676392
    },
    {
      "name": "Language model",
      "score": 0.6598385572433472
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5536278486251831
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5397701263427734
    },
    {
      "name": "Adapter (computing)",
      "score": 0.5362469553947449
    },
    {
      "name": "Task (project management)",
      "score": 0.5253849625587463
    },
    {
      "name": "Natural language processing",
      "score": 0.3907369077205658
    },
    {
      "name": "Machine learning",
      "score": 0.3380647897720337
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I205237279",
      "name": "Nankai University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    }
  ]
}