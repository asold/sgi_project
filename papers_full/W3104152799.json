{
    "title": "X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers",
    "url": "https://openalex.org/W3104152799",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2126046087",
            "name": "Jaemin Cho",
            "affiliations": [
                "University of North Carolina at Chapel Hill",
                "University of North Carolina Health Care",
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2288290430",
            "name": "Jiasen L√º",
            "affiliations": [
                "University of North Carolina at Chapel Hill",
                "University of North Carolina Health Care"
            ]
        },
        {
            "id": "https://openalex.org/A2747214831",
            "name": "Dustin Schwenk",
            "affiliations": [
                "University of North Carolina Health Care",
                "University of North Carolina at Chapel Hill"
            ]
        },
        {
            "id": "https://openalex.org/A91410043",
            "name": "Hannaneh Hajishirzi",
            "affiliations": [
                "University of North Carolina at Chapel Hill",
                "University of North Carolina Health Care"
            ]
        },
        {
            "id": "https://openalex.org/A2096537899",
            "name": "Aniruddha Kembhavi",
            "affiliations": [
                "University of North Carolina at Chapel Hill",
                "University of North Carolina Health Care"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963836885",
        "https://openalex.org/W2962754210",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4297730426",
        "https://openalex.org/W2969035964",
        "https://openalex.org/W4301206121",
        "https://openalex.org/W2405756170",
        "https://openalex.org/W2975501350",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2557449848",
        "https://openalex.org/W2962770929",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3034727271",
        "https://openalex.org/W2970127127",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2963981733",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2979672901",
        "https://openalex.org/W4297808394",
        "https://openalex.org/W2321533354",
        "https://openalex.org/W2507319105",
        "https://openalex.org/W2988975212",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2963966654",
        "https://openalex.org/W2966792645",
        "https://openalex.org/W4297606427",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2785678896",
        "https://openalex.org/W2963518342",
        "https://openalex.org/W2944828972",
        "https://openalex.org/W2963163163",
        "https://openalex.org/W2502312327",
        "https://openalex.org/W2964216930",
        "https://openalex.org/W2267126114",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2331128040",
        "https://openalex.org/W3034878914",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2963800363",
        "https://openalex.org/W3088059392",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W3113747735",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2953318193",
        "https://openalex.org/W3014611590",
        "https://openalex.org/W2947898088",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W3035497460",
        "https://openalex.org/W2963799213",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3100427071",
        "https://openalex.org/W2603777577",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W4298289240",
        "https://openalex.org/W3023745006",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2948433173",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W2150333870",
        "https://openalex.org/W2913129712",
        "https://openalex.org/W4296979096",
        "https://openalex.org/W3007759824",
        "https://openalex.org/W2970869018",
        "https://openalex.org/W2981721547",
        "https://openalex.org/W3001555892",
        "https://openalex.org/W2969876226",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W3035574324",
        "https://openalex.org/W2971074500",
        "https://openalex.org/W3037695135",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W3148140980",
        "https://openalex.org/W2997591391",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W3018265077",
        "https://openalex.org/W2972328244",
        "https://openalex.org/W2964024144",
        "https://openalex.org/W2787579267",
        "https://openalex.org/W2962974533",
        "https://openalex.org/W2548275288",
        "https://openalex.org/W4299585995",
        "https://openalex.org/W2963373786",
        "https://openalex.org/W2950776302",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W4226348722",
        "https://openalex.org/W3016211260",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W2962749469"
    ],
    "abstract": "Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family ‚Äì LXMERT ‚Äì finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT‚Äôs image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.",
    "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8785‚Äì8805,\nNovember 16‚Äì20, 2020.c‚Éù2020 Association for Computational Linguistics\n8785\nX-LXMERT: Paint, Caption and Answer Questions\nwith Multi-Modal Transformers\nJaemin Cho1,2‚àó Jiasen Lu1 Dustin Schwenk1 Hannaneh Hajishirzi1,3 Aniruddha Kembhavi1,3\nAllen Institute for AI1 UNC Chapel Hill2 University of Washington3\njmincho@cs.unc.edu {jiasenl,dustins,hannah,anik}@allenai.org\nCode and Demo:https://prior.allenai.org/projects/x-lxmert\nAbstract\nMirroring the success of masked language\nmodels, vision-and-language counterparts\nlike V ILBERT , L XMERT and U NITER have\nachieved state of the art performance on a\nvariety of multimodal discriminative tasks\nlike visual question answering and visual\ngrounding. Recent work has also successfully\nadapted such models towards the generative\ntask of image captioning. This begs the\nquestion: Can these models go the other way\nand generate images from pieces of text?\nOur analysis of a popular representative from\nthis model family ‚Äì L XMERT ‚Äì Ô¨Ånds that it\nis unable to generate rich and semantically\nmeaningful imagery with its current training\nsetup. We introduce X-LXMERT , an extension\nto L XMERT with training reÔ¨Ånements includ-\ning: discretizing visual representations, using\nuniform masking with a large range of mask-\ning ratios and aligning the right pre-training\ndatasets to the right objectives which enables\nit to paint. X-L XMERT ‚Äôs image generation\ncapabilities rival state of the art generative\nmodels while its question answering and\ncaptioning abilities remains comparable\nto L XMERT . Finally, we demonstrate the\ngenerality of these training reÔ¨Ånements by\nadding image generation capabilities into\nUNITER to produce X-U NITER .\n1 Introduction\nThe past year has seen a spate of BERT-style (De-\nvlin et al., 2019) transformer-based architectures\n(Lu et al., 2019; Chen et al., 2019; Li et al., 2019)\nproposed for vision-and-language tasks. These\nmodels are typically pre-trained on large image\ncaptioning corpora, extending ideas from masked\nlanguage modeling to mask both the image and\ntext modalities and produce state of the art results\n‚àóThis work was done as part of the Pre-Doctoral Young\nInvestigator residency program at the Allen Institute for AI.\non a variety of vision and language tasks includ-\ning visual question answering, visual grounding\nand image retrieval. These impressive results as\nwell as recent probing mechanisms (Ilharco et al.,\n2020) suggest that these models are able to capture\na variety of semantics in images including objects,\nattributes and their relationships and ground these\nin natural language.\nWhile these models have been extensively eval-\nuated over several discriminative tasks, relatively\nlittle attention has been paid to their generative ca-\npabilities. Bidirectional transformer models like\nBERT which exploit context preceding and follow-\ning the current token are not explicitly designed for\ngeneration. Recent work for language-only trans-\nformers (Wang and Cho, 2019; Dong et al., 2019;\nLiao et al., 2020) adapt these models towards this\ncapability using sampling procedures. Such tech-\nniques have also been adapted successfully for im-\nage captioning - inputting an image and sampling\nthe textual side of the model to generate a relevant\ncaption (Zhou et al., 2020). This begs the question:\nCan we go the other way and sample images from\ninput pieces of text? i.e. Do vision-and-language\nBERT models know how to paint?\nIn this work, we probe the ability of a powerful\nand popular representative from this family of mod-\nels - LXMERT (Tan and Bansal, 2019), to produce\nhigh Ô¨Ådelity and semantically meaningful images\nconditioned on captions. Interestingly, our analy-\nsis leads us to the conclusion that LXMERT in its\ncurrent form does not possess the ability to paint -\nit produces images that have little resemblance to\nnatural images. This is a somewhat surprising Ô¨Ånd-\ning given LXMERT ‚Äôs masked training objectives\nfor both modalities and its impressive performance\non tasks that seemingly require a similar skill set.\nWe Ô¨Ånd that this is largely due to the regression\ntraining objective used by this family of models to\npredict masked features on the visual side. This is\n8786\nin contrast with the textual side, where they predict\nmasked tokens within a large discrete vocabulary\nusing a classiÔ¨Åcation objective. Regressing features\nin high dimensional spaces is challenging to opti-\nmize and introduces noise at inference. This gets\ncompounded when using iterative sampling proce-\ndures to predict the entire set of visual features. A\ndownstream image generator consuming these pre-\ndictions isn‚Äôt able to recover from this noise even\nwhen Ô¨Åne-tuned on L XMERT ‚Äôs predictions.\nWe introduce X-L XMERT that builds upon\nLXMERT and enables it to effectively perform dis-\ncriminative as well as generative tasks. Our key\nreÔ¨Ånements include: (a) simplifying the visual in-\nputs to use grid features instead of object detection\nbounding boxes, (b) discretizing visual representa-\ntions, (c) using uniform masking with a large range\nof masking ratios to enable the model to predict the\nentire set of visual clusters at inference time and (d)\naligning the right pre-training datasets to the right\nobjectives. When coupled with our proposed im-\nage generator, X-L XMERT is able to generate rich\nimagery that is semantically consistent with the\ninput captions. Importantly, X-L XMERT ‚Äôs image\ngeneration capabilities rival state-of-the-art image\ngeneration models (designed only for generation),\nwhile its question answering capabilities show little\ndegradation compared to LXMERT .\nThese reÔ¨Ånements are not LXMERT -speciÔ¨Åc.\nThey are designed to be easily applicable to a wide\nvariety of multimodal BERT models. We Ô¨Ånd that\nUNITER , a single stream model for vision-and-\nlanguage tasks, produces very poor images when\ncoupled with a generator, but with our extensions,\nthe resulting X-U NITER produces images of a sim-\nilar quality to X-L XMERT .\nIn summary, we present X-L XMERT , a uniÔ¨Åed\nmultimodal transformer model that can answer\nquestions, and also generate captions and images.\nOur extensions to enable these capabilities are not\ntied to LXMERT ‚Äôs underlying architecture. We ex-\npect that the entire family of multimodal BERT\nmodels can be enhanced with image generative\ncapabilities using our introduced strategy.\n2 Related works\nVisual-Language transformer models Recent\nmulti-modal pre-training models show signiÔ¨Åcant\nimprovements on a wide range of downstream\ntasks, including discriminiative (eg., visual ques-\ntion answering) and generation task (eg. image\ncaptioning (Zhou et al., 2020)). Some methods use\na single transformer architecture to jointly encode\ntext and image (Li et al., 2019; Su et al., 2019; Al-\nberti et al., 2019; Rahman et al., 2020; Li et al.,\n2020; Chen et al., 2019; Qi et al., 2020; Huang\net al., 2020), while others use two-stream architec-\ntures (Lu et al., 2019, 2020; Tan and Bansal, 2019).\nThese models typically consume object detection\nfeatures. We probe this family of models at the\ntask of image generation and present extensions\nthat enable them to reliably generate images.\nSequence generation with undirectional trans-\nformer When generating sequences with conven-\ntional transformer language models, it is natural to\nsample tokens from left to right. However, since\nundirectional transformers (eg. BERT) are not\ntrained with a speciÔ¨Åc generation order, a line of\nworks has investigated different strategies for se-\nquence generation with undirected models. Wang\nand Cho (2019) use Gibbs sampling from an all-\nmask sequence, and Dong et al. (2019); Bao et al.\n(2020) use causal attention during training for left-\nto-right generation. Liao et al. (2020); Mansimov\net al. (2019); Ghazvininejad et al. (2019) sample\nmasks from a uniform distribution during training\nfor arbitrary order or parallel generation. We adapt\nthese techniques for grid-based image generation.\nText-to-image synthesis Synthesizing images\nfrom text descriptions continues to be challeng-\ning. Since the pioneering work of Reed et al.\n(2016), many methods have adopted GANs (Good-\nfellow et al., 2014) to generate high-Ô¨Ådelity images.\nNguyen et al. (2017) generate images that maxi-\nmizes activation of a pretrained captioning model.\nRecent works (Zhang et al., 2017, 2018; Xu et al.,\n2018; Li et al., 2019) use multi-stage generation,\nwhere low-resolution images are initially sampled,\nthen gradually upsampled and improved in later\nstages. These models are specialized toward im-\nage generation, whereas our model can not just\ngenerate images, but also answer questions and\ngenerate captions. Also, our design is modular in\nnature. While we use a compact image genera-\ntor with X-L XMERT , one can also replace it with\neither of the aforementioned model architectures.\nThere is another line of works predicting object lay-\nouts from text and generating image based on the\nlayouts (Hong et al., 2018; Tan et al., 2019). These\nmodels use bounding box annotations to train lay-\nout predictors, while X-L XMERT implicitly learns\nthe layouts only from text and image alignments.\n8787\nGrid visual representation Compared to bound-\ning box representations which requires expensive\nobject detection annotations, grid representations\nof images can be naturally obtained from CNNs.\nJiang et al. (2020); Huang et al. (2020) have re-\ncently shown that these can be almost as pow-\nerful as bounding box representations for VQA.\nGrid representation have been widely used in vi-\nsion tasks, including self-supervised learning (Oord\net al., 2018; Henaff et al., 2019; Trinh et al., 2019;\nGidaris et al., 2020; Noroozi and Favaro, 2016) and\nimage generation (van den Oord et al., 2017; Lin\net al., 2019). We leverage grid visual representa-\ntions to enable LXMERT to generate images.\n3 Background: Revisiting LXMERT\nOver the past year, a large number of transformer\nbased architectures for multimodal data have pro-\nduced impressive results across a variety of dis-\ncriminative tasks. Some of these models have been\nshown to perform very well at the generative task of\nImage Captioning, but little attention has been paid\nto the reverse generative task: generating images\ngiven text. In this work, we Ô¨Årst probe one popular\nrepresentative from this family -LXMERT (Tan and\nBansal, 2019) - in its ability to paint; and propose\nextensions that enable it to paint.\nLXMERT is a cross modality transformer with in-\nputs: image Iand text T. This is represented as the\nsequence {v1,...,v T,CLS,w1,...,w T ,EOS}\nwhere {vi}T\ni=1 are image region features, {wj}T\nj=1\nare word tokens and CLS and EOS are special\ntokens. LXMERT outputs embeddings for each\ninput {hvi }T\ni=1, {hwj }T\nj=1 and hCLS, hEOS. hCLS\nis used as the cross-modality output. Internally,\nLXMERT consists of two types of encoders:\nsingle-modality encoders for each modality and\na cross-modality encoder using bi-directional\ncross attention to exchange information and align\nentities across the modalities.\nLXMERT is pretrained on several vision-and-\nlanguage datasets with Ô¨Åve objectives: Masked\nlanguage modeling (MLM), Masked visual fea-\nture regression (MVFR) - reconstructing randomly\nmasked words and regions given the remaining in-\nputs, Masked object classiÔ¨Åcation (MOC) - object\nclassiÔ¨Åcation on masked image regions, Image-text\nmatching (ITM) - image-caption alignment pre-\ndiction and Question answering (QA) - answering\na question given image input. After pretraining,\nLXMERT is Ô¨Ånetuned for various downstream tasks.\nUnless noted, we use the default settings and hy-\nperparameters of LXMERT in our experiments.\n4 Probing LXMERT‚Äôs Ability to Paint\nIn order to probe LXMERT ‚Äôs ability to paint, we\nÔ¨Årst modify its input image representation to a grid\nbased feature set (Sec. 4.1) and then pass these to\nan image generator (Sec. 4.2).\n4.1 Grid Image Features\nMost popular multimodal BERT models use im-\nage features extracted from the output of a Faster\nR-CNN (Ren et al., 2015) object detector. The de-\ntected objects typically have various locations and\nsizes. Passing these features into an image gen-\nerator poses some challenges: (1) LXMERT is not\ntrained to predict locations of given objects (2) it is\nnot trivial to predict both object classes and their\nlocations simultaneously (3) object detections do\nnot cover backgrounds.\nWe modify LXMERT to use a uniform N √óN\ngrid and use RoI Pooling to extract the grid features.\nNote that we use the same detection backbone pre-\ntrained on the Visual Genome dataset to maintain\nparity with the original LXMERT . Our experiments\nin Sec 6 show that moving to a grid based input\ncauses very little degradation to downstream QA\ntasks, a Ô¨Ånding consistent with Jiang et al. (2020).\nSampling grid features: Given text input, we\nsample predicted visual features {hvi }T\ni=1 where\nT = N√óN is the number of image regions, using\nGibbs sampling in a manner similar to language\ngeneration using BERT by Wang and Cho (2019).\n4.2 Image Generation\nWe use a compact image generator inspired by re-\ncent state of the art image synthesis methods lever-\naging Generative Adversarial Networks (GAN)\n(Goodfellow et al., 2014). Its takes as inputs an\nN √óN grid of visual features from the pretrained\nFaster-RCNN network and generates an image. As\nshown in Fig 1, the input grid features are projected\nthrough convolutional layers and then passed to an\nimage generator, which consists of multiple resid-\nual blocks (Miyato et al., 2018). Each generator\nresidual block has SPADE layer (Park et al., 2019)\nwhich guides generator to outptut high Ô¨Ådelity im-\nages given semantic grid layouts. In our experi-\nments, we use an image generator which takes8√ó8\ngrid features and outputs an 256 √ó256 image.\n8788\nRoIFeatQuantize\nPosFeat\nUniformMaskFeat+ ObjectRelEncoder\nAgiraffestanding on dirt groundnear a tree. [CLS]A[MASK]standing on dirt groundnear a [MASK].[EOS] + LanguageEncoder\nCross-ModalityEncoder\nCluster-CentroidClassification\nMaskedCross-ModalityLM\nMatch?{YES}Cross-ModalityMatching&QA\n112\n10\n2048x8 x 8ùêº\n1x1 Conv3x3Conv3x3Conv\n‚Ä¶ Generator\nDiscriminator\nFeatureMatchingLossAdversarialLoss\nAC-GANLossPerceptualLoss\"ùêºRoIFeatQuantize\nSPADEResBlk‚Ä¶\nSPADEResBlk\n[CLS]Agiraffestanding on dirt groundnear a tree.[EOS]\nFigure 1: Top: Overview of the proposed X-L XMERT model. Blocks in blue are the modiÔ¨Åcations we make to\nLXMERT model to enable it to paint. Bottom: Overview of the image generation architecture. The input to the\nmodel is a natural image that is compressed to a quantized latent map of size 8 √ó8 by RoI Pooling. We use a\ngenerator consisting of multiple residual blocks with SPADE layer which encodes 8 √ó8 grid features.\nTraining the image generator: The generator is\npre-trained using 8 √ó8 ground truth Faster-RCNN\nfeatures, akin to teacher forcing, without any inputs\nfrom LXMERT . We train the generator with the\nsame loss as Park et al. (2019), but replacing the\nsegmentation map with a grid feature map.\nFig. 2 (b) shows that our generation architecture\ncan successfully reconstruct images using ground\ntruth pre-trained grid features. Note that the gener-\nator still displays some reconstruction errors com-\npared with modern auto-encoders such as VQ-\nV AEv2 (Razavi et al., 2019) primarily due to (1)\nfreezing the encoder backbone in order to match\nLXMERT ‚Äôs training settings (2) restricting grid fea-\ntures to have a low (and manageable) dimension.\n4.3 Can LXMERT Paint?\nOur experiments in Section 6 reveal that LXMERT\nis unable to produce visual features that can be con-\nverted to a meaningful image by a generator. Fig-\nure 2 shows an example. Recall that the LXMERT\nloss function includes a regression loss - MVFR -\nthat corresponds to regressing target visual features\ngiven the textual and visual context. Unfortunately,\nat inference, this loss on the validation set remains\nhigh, causing the predicted visual features to be\nfairly noisy. In addition, the Gibbs sampling proce-\ndure causes this error to propagate over the entire\nset of features. The resulting predictions aren‚Äôt suit-\nable to be used for downstream image generation.\n5 X-LXMERT\nIn this section, we present X-L XMERT 1 that ex-\ntends LXMERT , enabling it to paint, while still\nmaintaining a high performance on discriminative\ntasks. X-L XMERT has three key reÔ¨Ånements that\nenable it to paint (Sec. 5.1): discretizing visual rep-\nresentations, using uniform masking with a large\nrange of masking ratios, and aligning the right pre-\ntraining datasets to the right objectives. We then\nleverage Gibbs sampling to generate visual features\ngiven textual input (Sec. 5.2).\n5.1 From LXMERT to X-LXMERT\nDiscrete visual representations: We observe that\nthe visual features regressed by LXMERT are not\nsuitable for image generation. Instead, akin to\nVideoBERT (Sun et al., 2019), we Ô¨Årst create a\nvisual vocabulary using K-mean clustering, approx-\nimate the target visual features via a nearest neigh-\nbor search, and modify LXMERT to predict the\ncluster ID for each masked visual token. A new\nCluster-Centroid ClassiÔ¨Åcation objective (CCC) is\nused to replace the previous regression objective\nwith a high cardinality classiÔ¨Åcation objective. Our\nexperiments show that discretizing visual represen-\ntations results helps in predicting better visual fea-\ntures, stems the propagation of feature noise over\nsampling iterations and generates rich imagery.\n1X-L XMERT is an LXMERT with a ‚Äúdisplay server‚Äù\n8789\nAgiraffestanding on dirt groundnear a tree.\nX-LXMERT\n212\nX-LXMERTAgiraffestanding on dirt groundnear a tree.\n212\n617Agiraffestanding on dirt groundnear a tree.\nX-LXMERT\n212520617\n111111200\n505032‚Ä¶‚Ä¶ Generator\nOriginal ImageReconstruction from GTSampling from LXMERT\nX-LXMERT w/o uniform masking \nX-LXMERT (ours)\nDM-GAN\nAgiraffestanding on dirt groundnear a tree.\nCaption\n(a) (b) (c) (d) (e) (f)\nFigure 2: Top: Image generation from X-L XMERT . Given the text input and all masked visual feature, we Ô¨Årst\nsample grid features by using Gibbs sampling with multiple iterations. Then the sampled grid features are fed\ninto the generator to generate the image. Bottom: Sampled images, from left to right (a) Original image (b)\nReconstruction from GT features (c) Sampling from L XMERT + Grid (d) Sampling from X-L XMERT without\nuniform masking pretraining (e) Our proposed X-LXMERT (f) Generated image from DM-GAN (Zhu et al., 2019).\nUniform instead of Bernoulli masking: Fol-\nlowing BERT, LXMERT uses Bernouli sampling\n(with p = 0.15) to determine positions of the\nmasked tokens on the visual and textual features. In\norder to generate an image from captions, all tokens\non the vision side must be masked and predicted.\nA low probability Bernoulli sampling procedure\ndoes not prepare the model well for the generation\ntask, and increasing the probability to very high\nvalues leads to poor pre-training. To resolve this,\nwe use Uniform masking on the vision modality.\nX-L XMERT ‚Äôs uniform masking Ô¨Årst samples the\nmasking ratio from a uniform prior distribution\n([0,1]), and then samples the desired number of\npositions randomly. This subjects the model to\na variety of masking ratios, and our experiments\nreveal that this greatly beneÔ¨Åts image generation.\nUpdating pre-training data: LXMERT uses a\nvariety of data to pre-train the model: QA data\nfrom multiple sources, caption data from COCO\nand captions from Visual Genome (VG). Since\nX-L XMERT uses the CCC loss function, predict-\ning visual features given questions like: ‚ÄúWhat is\nshown in the image?‚Äù is very ambiguous and re-\nsults in models that cannot predict visual clusters.\nSimilarly, many captions from VG (e.g., ‚ÄúA bag‚Äù\nor ‚ÄúGlasses on the hair‚Äù) tend to describe small re-\ngions of the image and not the whole image, which\nmakes them unsuited to train the CCC objective.\nX-L XMERT drops QA data and the captions from\nVG for CCC objective for visual cluster prediction.\n5.2 Sampling Strategies for X-LXMERT\nGiven text input, predicting the entire set of visual\nfeatures in one step does not produce good results.\nInstead, we employ Gibbs sampling to iteratively\nsample features at different spatial locations. In\ncontrast to text generation, where left-to-right is\nconsidered a natural order, there is no natural order\nfor generating images. The grid sampling process\nstarts with N2 grids Ô¨Ålled with the MASK special\ntoken. The model then iteratively updates locations\neither one-by-one or multiple in parallel. There\nare several sampling strategies for sampling loca-\ntions on the square grid, primarily falling into two\nbuckets: auto-regressive and parallel.\nAutoregressive sampling In each iteration, a grid\nposition is sampled, masked and predicted. Then\nthe corresponding MASK token is replaced with the\npredicted one, and the process is repeated until all\nlocations are updated.\n‚Äì TL‚ÜíBR: Positions are sequentially chosen from\ntop-left to bottom-right, similar to PixelRNN\n(van den Oord et al., 2016).\n‚Äì Random (Liao et al., 2020): Positions are se-\nlected in random order. AfterN2 steps, locations\nmay be updated more than once.\nNon-autoregressive sampling In each iteration,\nmultiple positions are sampled, masked withMASK,\npredicted and then replaced.\n‚Äì Mask-predict-K (Ghazvininejad et al., 2019):\nThis requires K sampling steps. In the Ô¨Årst iter-\nation, all N2 locations are updated. Then, we\nlinearly decay the number of tokens updated per\niteration. For example, for a 2 √ó2 grid whereby\nN2 = 4, if K = 4then (4, 3, 2, 1) positions are\nupdated in each iteration. Within each iteration,\npositions with the lowest conÔ¨Ådence are updated.\nOur experiments show that Mask-Predict-4 consis-\ntently produces good results across a variety of\n8790\ngeneration metrics and we propose using it for\nX-L XMERT . Our uniform masking aligns well\nwith the linear decay of Mask-Predict and makes\nthe model robust to a varied number of masked\nlocations.\n5.3 Training Details\nGenerator Following (Park et al., 2019), our\ngenerator and discriminator are jointly trained with\n4 losses: (1) hinge adversarial loss (Lim and Ye,\n2017; Tran et al., 2017), (2) AC-GAN loss (Odena\net al., 2017), (3) discriminator feature matching\nloss (Wang et al., 2018) and (4) perceptual loss\n(Johnson et al., 2016). The coefÔ¨Åcients for different\nloss are (1,1,10,10) respectively. The perceptual\nloss is calculated with ResNet-50 (He et al., 2016)\npre-trained on ImageNet (Deng et al., 2009). We\nuse Adam optimizer (Kingma and Ba, 2015) with\n(Œ≤1,Œ≤2) = (0,0.999) and two-time update rule\n(Heusel et al., 2017) with learning rate of 0.0004\nand 0.0001 for generator and discriminator respec-\ntively. We train the generator with batch size 96 for\n60 epochs. Note that the generator parameters are\nÔ¨Åxed after training and not Ô¨Ånetuned. Please refer\nSec. D for more details.\nPre-training Following LXMERT (Tan and\nBansal, 2019), we use AdamW optimizer\n(Loshchilov and Hutter, 2019) with (Œ≤1,Œ≤2) =\n(0.9,0.999) and learning rate 1e-5 with 5% lin-\near warmup schedule. We train X-L XMERT on\nwith batch size 920 for 20 epochs. Instead of us-\ning all pretraining tasks for each step, we Ô¨Årst uni-\nformly sample a modality to mask from [image,\ntext, no-mask] and run corresponding tasks.\nPlease refer to Sec. C.5 for more details.\nFinetuning For each downstream task, a task\nhead consisting of two fully connected layers is\ntrained along with pre-trained X-L XMERT . We\nused the same parameter setting with LXMERT .\nPlease refer to Sec. C.6 for more details.\n6 Experimental Setup\nIn this section we present experimental setups to\nevaluate image generation, visual question answer-\ning and visual reasoning.\n6.1 Evaluating Image Generation\nWe train and evaluate models using the MS COCO\ncaptioning dataset (Lin et al., 2014). We com-\npare X-L XMERT with LXMERT and state-of-the-\nart text-to-image generation methods: StackGAN\n(Zhang et al., 2018), PPGN (Nguyen et al., 2017),\nAttnGAN (Xu et al., 2018), ControlGAN (Li et al.,\n2019), and DM-GAN (Zhu et al., 2019). Image\ngeneration is a particularly difÔ¨Åcult task to evalu-\nate, due to the variability in acceptable outputs for\na given caption, as well as the subjective nature of\nperceiving image quality. We present a suite of au-\ntomated and manual metrics to compare models.\nAutomated Metrics: Evaluate image quality\nWe use Inception score (IS) (Salimans et al., 2016)\nto measure image diversity and Fr¬¥echet Inception\nDistance (FID) (Heusel et al., 2017) to measure\nauthenticity; using Inception v3 (Szegedy et al.,\n2016) as a surrogate net.\nAutomated Metrics: Evaluate semantics We\nuse two variants of R-precision (Xu et al., 2018),\nR-prec-easy and R-prec-hard to evaluate if the im-\nage is well conditioned on the input text. Given a\ngenerated image, a positive caption and negatives,\nR-precision measures the retrieval rate for the posi-\ntive caption using a surrogate multi-modal network.\nWe use an independent surrogate - ViLBERT-MT\n(Lu et al., 2020) for this purpose. R-prec-easy is the\nvariant of R-precision with easy negatives (sampled\nrandomly amongst the caption set). R-prec-hard is\nthe variant with hard negatives (swapping a word\nin a caption with another word within the same cat-\negory, e.g., red ‚áígreen). We choose words from\none of 4 categories: nouns (80 COCO objects), 64\nverbs, 10 colors and 10 numbers.\nThe above automatic metrics, while cheap and\nreproducible, are noisy because they depend on im-\nperfect surrogate models. The ultimate measure of\nquality and semantics for image generation contin-\nues to be crowd-sourced human studies.\nHuman Study: Pairwise preferences We con-\nduct a human preference evaluations between\nX-L XMERT and the best performing model in the\nautomated metrics‚ÄîDM-GAN. We measure (1) Se-\nmantic preferenceby showing two image and ask-\ning annotators to select the one that best matches\nthe source caption. (2) Fidelity preferenceby show-\ning the two images alone and asking which appears\nmore realistic. Both evaluations also allow a third\noption (Tie) to be selected. For each evaluation,\n5000 image pairs were used, and 357 unique crowd-\nworkers participated in total (median annotations\nper worker‚Äî17).\nHuman Study: Our new metric ‚Äì HUMMUS\nThe above pairwise test is very useful and widely\nused to evaluate generative models, but measur-\n8791\nText-to-Image Generation Visual Question Answering Visual Reasoning\nMethods IS ‚Üë FID‚Üì R-prec\n-easy‚Üë\nR-prec\n-hard‚Üë HUMMUS Human pairwise pref VQA GQA NLVR 2\nSemantics Fidelity test-dev test-std test-std dev test-P\nOriginal Image 36.6 - 89.6 47.6 0.73 - - - - - - -\nStackGAN 8.5 - - - - - - - - - - -\nPPGN 9.6 - - - - - - - - - - -\nAttnGAN 25.9 35.5 - - - - - - - - - -\nControlGAN 24.1 - - - - - - - - - - -\nDM-GAN 30.5 32.6 51.8 27.5 0.49 37.0 35.9 - - - - -\nX-LXMERT 22.7 37.4 40.8 25.1 0.49 52.0 50.0 68.6 68.7 58.4 72.4 72.4\nLXMERT*+Grid 1.6 316.7 0.5 6.6 0.27 71.1 71.2 60.1 74.6 74.0\nLXMERT - - - - - - - 72.4 72.5 60.3 74.9 74.5\nLXMERT* - - - - - - - 70.9 71.1 59.9 74.9 75.0\nTable 1: Comparing X-L XMERT , L XMERT and baselines on image generation, visual question answering and\nvisual reasoning tasks. The pairwise metric compares L XMERT and DM-GAN; numbers do not sum to 100 due\nto the TIE option provided to annotators. Note that X-L XMERT and LXMERT *+Grid are the only models that are\nable to produce results for all tasks. *: Our re-implementation of L XMERT .\ning new models becomes challenging, since they\nmust compare to all old models. To expand hu-\nman evaluation, we present a novel metric to test\nsemantic consistency between the caption and im-\nage inspired by masked token modeling, named\n- HUmans Measuring seMantics Using maSking\n(HUMMUS ). To compute HUMMUS , human anno-\ntators are shown an image and its caption with a\nsingle word masked out. They are asked to com-\nplete the partial caption based on information in\nthe image, and a match is counted only when a ma-\njority of annotators supply the correct word. The\ntotal score is reported as a ratio of these successful\nmatches. The task was run on 2800 image-caption\npairs (2289 unique images), with 5 annotators per\npair. A total of 280 unique crowdworkers com-\npleted the task, with a median of 13 images anno-\ntated per worker. A high HUMMUS score reveals\nthat the generated images contain the correspond-\ning semantics, well enough to be recognized. The\nmasked word is chosen from one of 3 categories:\n80 COCO nouns, verbs and colors.\n6.2 Evaluating Visual Question Answering\nWe train and evaluate models for visual question\nanswering using the VQA2.0 (Goyal et al., 2019)\nand GQA (Hudson and Manning, 2019) datasets,\nwhich provide an image and a question and require\nthe model to generate an answer.\n6.3 Evaluating Visual Reasoning\nWe train and evaluate models for visual reasoning\nusing the NLVR2 (Suhr et al., 2019) dataset and\nreport numbers on the dev and test-P splits. The\nNLVR2 dataset requires models to look at two im-\nages and determine if an accompanying caption\nis True or False. This is a particularly challenging\ndataset for present day vision and language models.\n7 Experimental Results\nWe now present a comparison of X-L XMERT with\nseveral baselines on the generative and discrimina-\ntive tasks, along with ablation studies and qualita-\ntive results. We also show the generality of our tech-\nniques via extending UNITER to create X-U NITER .\n7.1 Quantitative Results\nTable 1 provides detailed metrics for X-L XMERT\nand baselines. It also provides generation metrics\nfor the original image in the dataset for the cor-\nresponding input text. Note that X-L XMERT and\nLXMERT +Grid are the only models that are able to\nproduce results for all tasks.\nImage Generation As seen, X-L XMERT signiÔ¨Å-\ncantly outperforms LXMERT across all generation\nmetrics. X-L XMERT even outperforms two special-\nized generation models, comparable to AttnGAN\nand ControlGAN. Our model is lower compared\nto DM-GAN in terms of automated metric (IS and\nFID), however, it is competitive with DM-GAN at\nsemantic metric (R-prec-hard)3.\nNote that X-L XMERT ‚Äôs image generator is much\nsmaller than the one used by DM-GAN (1.7M vs\n22.3M parameters). While the transformer em-\nployed in X-L XMERT is large, it is a uniÔ¨Åed tex-\n2We use coco pre-trained model from https://\ngithub.com/MinfengZhu/DM-GAN\n3Note: R-prec and HUMMUS are reported only for DM-\nGAN (the strongest of the 5 baselines), since this was the only\nmodel with code and pretrained weights. IS and FID num-\nbers are from their respective publications or from Zhu et al.\n(2019). The detailed R-prec-hard numbers across categories\nare presented in the appendix.\n8792\nTwo people play video games while sitting on a couch.\nCaptionOriginal Ours\nA grassy tree filled field with a lot of kites in the air.\nA giraffe walking on a road with two cars approaching.\nDM-GAN\n A large painted clock tower in the middle of town.\nCaptionOriginal OursDM-GAN\nAwomanattempting to ski on a flat hill\nA full view of a home office with many computer screens.\nFigure 3: Images generated by DM-GAN2(Zhu et al., 2019) and images generated by our proposed X-L XMERT .\ntual and visual encoder used for multiple tasks\nand is not Ô¨Ånetuned for image generation. We ex-\npect X-L XMERT ‚Äôs image quality to improve further\nwhen coupled with a larger image generator such\nas the one by DM-GAN.\nTable 1 also presents HUMMUS scores. Here we\nsee that the semantics generated by X-L XMERT is\non par with DM-GAN and still signiÔ¨Åcantly better\nthan LXMERT . All models are still a distance away\nfrom the original image. HUMMUS matches on the\nlemmatized forms of masked words to allow for\nlexical variation, but it misses synonyms and other\nvalid descriptors. This causes the score for the\noriginal image to drop to its reported value. See the\nappendix for R-prec-hard and HUMMUS broken\ndown into categories.\nFinally we present human pairwise preference\nscores between X-L XMERT and DM-GAN (its\nclosest competitor). Here we see that human anno-\ntators clearly prefer X-L XMERT to DM-GAN for\nsemantics as well as Ô¨Ådelity.\nIn summary, X-L XMERT ‚Äôs generation capabil-\nities rival state of the art specialized generation\nmodels. In fact, our human studies demonstrate\nthat X-L XMERT produces better results than even\nDM-GAN, its closest competitor. Our analysis also\nshows the limitations of current automatic evalua-\ntion metric for text-to-image synthesis.\nVisual Question Answering Table 1 compares\nmodels on the VQA2.0 and GQA datasets. Convert-\ning LXMERT to use grid inputs causes a slight or no\ndrop, consistent with Ô¨Åndings by Jiang et al. (2020),\nbut hugely simpliÔ¨Åes the pipeline. X-L XMERT\nshows 1.5 - 2.5% drop on these datasets but note\nthat its numbers are still very competitive.\nVisual Reasoning Table 1 compares models on\nNLVR2 dataset. Consistent with VQA, grid inputs\ncause a slight drop. X-L XMERT shows a roughly\n2% drop but retains most of the massive jumps\nobtained by LXMERT on NLVR2 compared to the\nprevious generation of models.\nOur implementation of X-L XMERT uses a small\n8√ó8 grid. Increasing the grid size will likely shrink\ngaps in VQA2.0, GQA and NLVR2 datasets as per\nthe recent Ô¨Åndings by Jiang et al. (2020).\n7.2 From X-L XMERT to X-U NITER\nThe proposed reÔ¨Ånements (Sec. 5.1) to enable\nimage generation capabilities are not LXMERT -\nspeciÔ¨Åc. We apply these changes to UNITER (Chen\net al., 2019), a single stream multi-modal trans-\nformer architecture. Instead of following (Chen\net al., 2019) Table 2 shows thatUNITER + Grid pro-\nduces very poor images, butX-U NITER obtains im-\nage generation scores comparable to X-L XMERT ‚Äì\nshowing the generality of our extensions.\nIS‚Üë FID‚Üì\nUNITER + Grid 2.4 253.5\nX-U NITER 20.1 51.4\nLXMERT + Grid 1.6 316.7\nX-L XMERT 22.7 37.4\nTable 2: Adding image generation capabilities to\nLXMERT and UNITER .\n7.3 Qualitative Results\nFig 3 shows qualitative examples by X-L XMERT\ncompared to DM-GAN (Zhu et al., 2019). While\nthe images lack Ô¨Åne details, they do a reasonable\njob at preserving high level semantics, as revealed\nby the metrics. For complex scene, our model is\nable to preserve better semantics (e.g. ‚Äòtwo peo-\nple‚Äô, ‚Äòclock tower‚Äò and ‚Äòhome ofÔ¨Åce‚Äô) compared to\nDM-GAN. We do not show images produced by\nLXMERT since they tend to be incomprehensible.\n8793\nA man dances on top of picnic tables while it snows.\nA giraffe walking on a road with two cars approaching.A full view of a home office with many computer screens.A large painted clock tower in the middle of town.\ncaption#1 #10 #20 #30 #40#50#60#70#100#140#5\n #15 #25 #35\nFigure 4: Intermediate images generated by X-L XMERT at during 140 steps of random position sampling. Images\nare gradually improved as sampling steps proceed.\nAkite flying in the air with water in the background.Thewoman is wearing a red jacket.\nWherewas the picture taken, the beach or the harbor?\nWhatis the main color of the kite in front of the person that is standing on the?\nWhat is the color of the jacket the person with the kite is wearing?\nWhat is the color of the chair?\nWhatfood is on the plate?\nAyoung boy sitting in a chair with a birthday cake for his birthday.Thepiece of cake in the little blonde girl's mouth.\nWhereis the chair, on the right or on the left?\nIs thebowl to the right of the spoon red and round?Acake on a red tray sitting on top of a table.\nWhatis the name of the food that is on the plate?\nThehandle of the spoon is on the side of the bowl.\nWhereis the food that is on top of the table sitting?\nFigure 5: Captions generated by X-LXMERT using Gibbs sampling. We control the samples by providing different\npreÔ¨Åx word into the model. Those preÔ¨Åx words are common starting word such as ‚ÄòA‚Äô, ‚ÄòThe‚Äô, ‚ÄòWhat‚Äô, ‚ÄòWhere‚Äô.\nAblations IS ‚Üë FID‚Üì\nLXMERT + Grid 1.6 316.7\nX-L XMERT 22.7 37.4\nw/o discrete visual representations 1.5 304.4\nw/o uniform masking 2.1 227.9\nw/o updating pre-training data 21.6 46.1\nTable 3: An ablation study for the three reÔ¨Ånements.\nTo better understand the image generation pro-\ncess, We show intermediate images generate by\nX-L XMERT in Fig 4. We use random autoregres-\nsive sampling with 140 steps. Interestingly, the\nmodel Ô¨Årst coarsely generates salient objects (ex.\ngiraffe, monitors) in the caption followed by details\nand background.\nOur model is able to generate captions given\nimage. For each image, we sample text from\nX-L XMERT using Gibbs sampling as shown in\nFig 5. We control the samples by providing dif-\nferent preÔ¨Åx word into the model. Those preÔ¨Åx\nwords are common starting word such as ‚ÄòA‚Äô, ‚ÄòThe‚Äô,\n‚ÄòWhat‚Äô, ‚ÄòWhere‚Äô.X-L XMERT can produce long\nmeaningful captions as well as questions (like the\nones in VQA datasets).\n7.4 Ablation Studies\nWe examine the effects of our proposed reÔ¨Ånements\nand our sampling strategies to the image generation\nquality. Table 3 shows that two of the proposed\nIS‚Üë/FID‚Üì R-prec‚Üë HUMMUS‚Üë\neasy/hard Noun / Verb / Color / Avg.\nMask-Pred-4 22.7/37.4 40.8/25.1 0.55 / 0.42/ 0.50 /0.49\nTL‚ÜíBR 19.8/48.5 26.9/18.9 0.45 / 0.42/ 0.41 / 0.43\nRandom 22.6/ 35.9 39.5/24.7 0.52 / 0.42/ 0.51 / 0.48\nMask-Pred-1 19.5/51.4 36.8/21.4 0.48 / 0.40 / 0.54/ 0.47\nTable 4: Image quality across sampling strategies.\nreÔ¨Ånements to LXMERT (moving to discrete visual\nrepresentations and using uniform masking) are\ncritical to produce high quality images. The third\nreÔ¨Ånement ‚Äì updating pre-training data for the CCC\nobjective ‚Äì is less critical, but useful nonetheless.\nTable 4 shows that X-L XMERT is fairly robust\nto sampling strategy, particularly for image seman-\ntics, with the exception of TL‚ÜíBR which tends to\nproduce worse results. This is interesting in that\nTL‚ÜíBR is typically the default strategy used by\npractitioners (van den Oord et al., 2016, 2017).\n8 Conclusion\nWe develop a probing mechanism and Ô¨Ånd that\nLXMERT , a powerful vision-and-language trans-\nformer model, is not able to generate meaning-\nful images conditioned on text. We present\nX-L XMERT , a uniÔ¨Åed model for image generation,\ncaptioning, QA and visual reasoning, and show\nthat our extensions can easily be applied to other\nvision-and-language transformer models.\n8794\nReferences\nChris Alberti, Jeffrey Ling, Michael Collins, and David\nReitter. 2019. Fusion of Detected Objects in Text for\nVisual Question Answering. In EMNLP.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer Normalization.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-\nfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\nUniLMv2: Pseudo-Masked Language Models for\nUniÔ¨Åed Language Model Pre-Training.\nYen-chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2019. UNITER: Learning UNiversal\nImage-TExt Representations.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai\nLi, and Li Fei-Fei. 2009. ImageNet: A Large-Scale\nHierarchical Image Database. In CVPR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. UniÔ¨Åed Language\nModel Pre-training for Natural Language Under-\nstanding and Generation. In NeurIPS.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-Predict: Parallel De-\ncoding of Conditional Masked Language Models. In\nEMNLP.\nSpyros Gidaris, Andrei Bursuc, Nikos Komodakis,\nPatrick P ¬¥erez, and Matthieu Cord. 2020. Learning\nRepresentations by Predicting Bags of Visual Words.\nIn CVPR.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative\nAdversarial Networks. In NIPS.\nYash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas\nSummers-Stay, Dhruv Batra, and Devi Parikh. 2019.\nMaking the V in VQA Matter: Elevating the Role of\nImage Understanding in Visual Question Answering.\nInternational Journal of Computer Vision.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep Residual Learning for Image\nRecognition. In CVPR.\nOlivier J. Henaff, Ali Razavi, Carl Doersch, Ali S. M.\nEslami, and Aaron van den Oord. 2019. Data-\nEfÔ¨Åcient Image Recognition with Contrastive Pre-\ndictive Coding.\nDan Hendrycks and Kevin Gimpel. 2016. Gaussian Er-\nror Linear Units (GELUs).\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. 2017.\nGANs Trained by a Two Time-Scale Update Rule\nConverge to a Local Nash Equilibrium. In NIPS.\nSeunghoon Hong, Dingdong Yang, Jongwook Choi,\nand Honglak Lee. 2018. Inferring Semantic Layout\nfor Hierarchical Text-to-Image Synthesis. In CVPR.\nXun Huang and Serge Belongie. 2017. Arbitrary Style\nTransfer in Real-time with Adaptive Instance Nor-\nmalization. In ICCV.\nZhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei\nFu, and Jianlong Fu. 2020. Pixel-BERT: Aligning\nImage Pixels with Text by Deep Multi-Modal Trans-\nformers.\nDrew A. Hudson and Christopher D. Manning. 2019.\nGQA: A new dataset for real-world visual reasoning\nand compositional question answering. In CVPR.\nGabriel Ilharco, Rowan Zellers, Ali Farhadi, and Han-\nnaneh Hajishirzi. 2020. Probing Text Models for\nCommon Ground with Visual Representations.\nHuaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik\nLearned-Miller, and Xinlei Chen. 2020. In Defense\nof Grid Features for Visual Question Answering. In\nCVPR.\nJeff Johnson, Matthijs Douze, and Herv ¬¥e J¬¥egou. 2017.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data.\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016.\nPerceptual Losses for Real-Time Style Transfer and\nSuper-Resolution. In ECCV.\nTero Karras, Samuli Laine, and Timo Aila. 2019. A\nStyle-Based Generator Architecture for Generative\nAdversarial Networks. In CVPR.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hell-\nsten, Jaakko Lehtinen, and Timo Aila. 2020. An-\nalyzing and Improving the Image Quality of Style-\nGAN. In CVPR.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nMethod for Stochastic Optimization. In ICLR.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li Jia-Li, David Ayman Shamma,\nMichael Bernstein, and Li Fei-Fei. 2016. Visual\nGenome: Connecting Language and Vision Using\nCrowdsourced Dense Image Annotations. Interna-\ntional Journal of Computer Vision.\nBowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and\nPhilip H S Torr. 2019. Controllable Text-to-Image\nGeneration. In NeurIPS.\nGen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and\nMing Zhou. 2020. Unicoder-VL: A Universal En-\ncoder for Vision and Language by Cross-modal Pre-\ntraining. In AAAI.\n8795\nYi Liao, Xin Jiang, and Qun Liu. 2020. Probabilisti-\ncally Masked Language Model Capable of Autore-\ngressive Generation in Arbitrary Word Order. In\nACL.\nJae Hyun Lim and Jong Chul Ye. 2017. Geometric\nGAN. In NIPS.\nChieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen,\nDa-Cheng Juan, Wei Wei, and Hwann-Tzong Chen.\n2019. COCO-GAN : Generation by Parts via Condi-\ntional Coordinating. In ICCV.\nTsung Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar,\nand C. Lawrence Zitnick. 2014. Microsoft COCO:\nCommon Objects in Context. In ECCV.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nWeight Decay Regularization. In ICLR.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Ste-\nfan Lee. 2019. ViLBERT: Pretraining Task-\nAgnostic Visiolinguistic Representations for Vision-\nand-Language Tasks. In NeurIPS.\nJiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi\nParikh, and Stefan Lee. 2020. 12-in-1: Multi-Task\nVision and Language Representation Learning. In\nCVPR.\nElman Mansimov, Alex Wang, and Kyunghyun Cho.\n2019. A Generalized Framework of Sequence Gen-\neration with Application to Undirected Sequence\nModels.\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama,\nand Yuichi Yoshida. 2018. Spectral Normalization\nfor Generative Adversarial Networks. In ICLR.\nTakeru Miyato and Masanori Koyama. 2018. cGANs\nwith Projection Discriminator. In ICLR.\nAnh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Doso-\nvitskiy, and Jason Yosinski. 2017. Plug & Play Gen-\nerative Networks: Conditional Iterative Generation\nof Images in Latent Space. In CVPR.\nMehdi Noroozi and Paolo Favaro. 2016. Unsupervised\nLearning of Visual Representations by Solving Jig-\nsaw Puzzles. In ECCV.\nAugustus Odena, Christopher Olah, and Jonathon\nShlens. 2017. Conditional Image Synthesis With\nAuxiliary ClassiÔ¨Åer GANs. In ICML.\nAaron van den Oord, Nal Kalchbrenner, and Koray\nKavukcuoglu. 2016. Pixel Recurrent Neural Net-\nworks. In ICML.\nAaron van den Oord, Oriol Vinyals, and Koray\nKavukcuoglu. 2017. Neural Discrete Representa-\ntion Learning. In NIPS.\nAaron Van Den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation Learning with Contrastive\nPredictive Coding.\nTaesung Park, Ming-yu Liu, Ting-chun Wang, and Jun-\nyan Zhu. 2019. Semantic Image Synthesis with\nSpatially-Adaptive Normalization. In CVPR.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChana, Edward Yang, Zachary DeVito, Zeming Lin,\nAlban Desmaison, Luca Antiga, and Adam Lerer.\n2017. Automatic differentiation in PyTorch. In\nNIPS Workshop.\nDi Qi, Lin Su, Jia Song, Edward Cui, Taroon\nBharti, and Arun Sachet. 2020. ImageBERT:\nCross-modal Pre-training with Large-scale Weak-\nsupervised Image-Text Data.\nWasifur Rahman, Md Kamrul Hasan, Amir Zadeh,\nLouis-Philippe Morency, and Mohammed Ehsan\nHoque. 2020. M-BERT: Injecting Multimodal Infor-\nmation in the BERT Structure. In ACL.\nAli Razavi, Aaron van den Oord, and Oriol Vinyals.\n2019. Generating Diverse High-Fidelity Images\nwith VQ-V AE-2. InNeurIPS.\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen\nLogeswaran, Bernt Schiele, and Honglak Lee. 2016.\nGenerative adversarial text to image synthesis. In\nICML.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster R-CNN: Towards Real-Time Ob-\nject Detection with Region Proposal Networks. In\nNIPS.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba,\nVicki Cheung, Alec Radford, and Xi Chen. 2016.\nImproved Techniques for Training GANs. In NIPS.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2019. VL-BERT: Pre-\ntraining of Generic Visual-Linguistic Representa-\ntions. In ICLR.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A Corpus for\nReasoning About Natural Language Grounded in\nPhotographs. In ACL.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Mur-\nphy, and Cordelia Schmid. 2019. VideoBERT: A\nJoint Model for Video and Language Representation\nLearning. In ICCV.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. 2016. Re-\nthinking the Inception Architecture for Computer Vi-\nsion. In CVPR.\nFuwen Tan, Song Feng, and Vicente Ordonez. 2019.\nText2Scene: Generating Compositional Scenes\nFrom Textual Descriptions. In CVPR.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learn-\ning Cross-Modality Encoder Representations from\nTransformers. In EMNLP.\n8796\nDustin Tran, Rajesh Ranganath, and David M. Blei.\n2017. Hierarchical implicit models and likelihood-\nfree variational inference. In NIPS.\nTrieu H. Trinh, Minh-Thang Luong, and Quoc V . Le.\n2019. SelÔ¨Åe: Self-supervised Pretraining for Image\nEmbedding.\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempit-\nsky. 2016. Instance Normalization: The Missing In-\ngredient for Fast Stylization.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In NIPS.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a\nMouth, and It Must Speak: BERT as a Markov Ran-\ndom Field Language Model. In NAACL Workshop.\nTing-chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew\nTao, Jan Kautz, and Bryan Catanzaro. 2018. High-\nResolution Image Synthesis and Semantic Manipu-\nlation with Conditional GANs. In CVPR.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ¬¥emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace‚Äôs Trans-\nformers: State-of-the-art Natural Language Process-\ning.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, ≈Åukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google‚Äôs Neural Machine\nTranslation System: Bridging the Gap between Hu-\nman and Machine Translation.\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han\nZhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.\n2018. AttnGAN: Fine-Grained Text to Image Gen-\neration with Attentional Generative Adversarial Net-\nworks. In CVPR.\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang,\nand Senior Member. 2018. StackGAN++ : Realistic\nImage Synthesis with Stacked Generative Adversar-\nial Networks. IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence, pages 1‚Äì16.\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang,\nXiaogang Wang, Xiaolei Huang, and Dimitris\nMetaxas. 2017. StackGAN : Text to Photo-realistic\nImage Synthesis with Stacked Generative Adversar-\nial Networks. In ICCV.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\nHu, Jason J. Corso, and Jianfeng Gao. 2020. UniÔ¨Åed\nVision-Language Pre-Training for Image Captioning\nand VQA. In AAAI.\nMinfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang.\n2019. DM-GAN: Dynamic memory generative ad-\nversarial networks for text-to-image synthesis. In\nCVPR.\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\nFei. 2016. Visual7W: Grounded Question Answer-\ning in Images. In CVPR.\n8797\nA Qualitative samples\nMore qualitative samples In Fig 6, we show\nmore qualitative examples of images generated by\nDM-GAN, reconstruction from ground truth clus-\nters, LXMERT , our proposed X-L XMERT with dif-\nferent sampling strategies. Fig 7 shows images\ngenerated by X-L XMERT with the same subject\nplaced in a variety of contexts.\nB Source code\nPlease refer to the project page for more de-\ntails about this research, at https://prior.\nallenai.org/projects/x-lxmert. This in-\ncludes an animation of the iterative image gen-\neration process, a demo of X-L XMERT accessi-\nble at https://vision-explorer.allenai.org/\ntext_to_image_generation and code available\nat https://github.com/allenai/x-lxmert.\nC LXMERT / X-LXMERT details\nFor a fair comparison, we re-implement LXMERT\nand LXMERT with grid features. Our models have\n226.5M trainable parameters, slightly smaller than\n228M of original LXMERT implementation due to\nweight sharing of MVFR head and MOC head. We\nuse PyTorch (Paszke et al., 2017) and Hugging-\nface Transformers (Wolf et al., 2019) libraries for\nimplementation.\nC.1 LXMERT Architecture\nLXMERT architecture consists of text embedder,\nobject embedder, transformer backbone, and task-\nspeciÔ¨Åc heads.\nText embedder A text input is tokenized by\nWordPiece Tokenizer (Wu et al., 2016) and\nspecial tokens CLS and EOS are concatenated:\n{CLS,w1,...,w T ,EOS}. We use the same vocab-\nulary used in BERT4 and LXMERT with size 30522.\nText is truncated with maximum token length of\n20, including two special tokens. 768-dimensional\nembedding is learned for each token and position.\nFinal text embedding is obtained by sum of token\nembedding and positional embedding.\nObject embedder An input image is resized\nwithin minimum length 800 and maximum length\n1333 while preserving aspect ratio. We use Faster\nR-CNN trained on Visual Genome to extract 36\n4bert-base-uncased\nbounding boxes from each image5. We take fc6\nfeature, which is between RoI-Pool layer and\nÔ¨Ånal object classiÔ¨Åcation head and has 2048 di-\nmension. This is encoded into 768 dimensional\nvector followed by layer norm (Ba et al., 2016).\nFour bounding box coordinates (x0,x1,y0,y1) are\n[0,1]-normalized by width and height. Then they\nare also encoded into 768 dimensional vectors with\nfully connected layer followed by layer norm. Fi-\nnal object embedding is obtained by element-wise\naverage of object and positional feature.\nTransformer backbone Transformer backbone\nof LXMERT consists of object relation encoder, lan-\nguage encoder and cross modality encoder, which\nare composed of 9 self-attention layer (Vaswani\net al., 2017), 5 self-attention layer, and 5 cross-\nattention layer respectively. The self-attention lay-\ners are same as the ones used in BERT and the\ndimension of the layers is 768.\nTask-speciÔ¨Åc heads LXMERT is pretrained with\nÔ¨Åve objectives6 (MLM, MVFR, MOC, ITM, QA)\nas explained in Sec. 3. For MLM, MVFR, ITM,\nQA task, a task head consisting of two fully con-\nnected layers with GeLU activation (Hendrycks\nand Gimpel, 2016) and layer norm is trained. For\nMOC task, a fully connected layer is applied on\nouput of MVFR head, similar to original object de-\ntection pipeline7. For MLM, MVFR, MOC tasks,\ntask heads are applied on cross-modal encoder out-\nputs corresponding to masked tokens. For ITM,\nQA tasks, tasks heads are applied on CLS token.\nC.2 X-LXMERT Architecture\nX-L XMERT shares most components with\nLXMERT , except for minor modiÔ¨Åcations below.\nObject embedder ‚ÜíGrid embedder We ex-\ntract 8 √ó8 grid features of fc6 layer of Faster\nR-CNN, by giving positional information of 8 √ó8\ngrids into RoI-Pool layer. Then we quantize\nthese features with nearest neighborhood search\nfrom 10,000 cluster centroids. Remaining are same\nwith object embedder of LXMERT .\n5We use PyTorch version ( https://gitlab.\ncom/vedanuj/vqa-maskrcnn-benchmark), in-\nstead of Caffe version ( https://github.com/\npeteanderson80/bottom-up-attention) used in\noriginal implementation.\n6We do not use 400 object attributes predicted from Faster\nR-CNN, which were used by original implementation.\n7Original implementation trains separate head for MOC\ntask.\n8798\nA man dances on top of picnic tables while it snows.\nCaptionOriginal LXMERT RandomEasyFirstMaskPredictReconstruction\nTop Left -> Bottom Right\nA full view of a home office with many computer screens.\nA giraffe walking on a road with two cars approaching.\nA large painted clock tower in the middle of town.\nDM-GAN\nFigure 6: More qualitative examples of images generated by X-L XMERT .\nA giraffe walking in the field\nA giraffe walking near a carA giraffe eating leaves\nChildren playing soccerChildren playing ice hockeyChildren chasing each other\nA giraffe next to zebras\nChildren playing a video game\nFigure 7: Images generated by X-L XMERT demonstrating its ability to place objects within varied contexts.\nMOC, MVFR tasks ‚ÜíCCC task We replace\nMOC, MVFR tasks with CCC task (see Sec. 5.1)\nfor X-L XMERT . For CCC head, we simply modify\nthe output dimension of fully connected layer used\nin MOC task to the number of clusters (1600 ‚Üí\n10000).\nC.3 Datasets\nFor pretraining, we use same datasets used in\nLXMERT . We use vision-and-language datasets\nwhose images come from MS COCO (Lin et al.,\n2014) or Visual Genome (Krishna et al., 2016).\nBesides the two original captioning datasets, we\nalso aggregate three large image question answer-\ning (image QA) datasets: VQA v2.0 (Goyal et al.,\n2019), GQA balanced version (Hudson and Man-\nning, 2019), and VG-QA (Zhu et al., 2016). Ta-\nble 5 shows statistics of the datasets. Note that\nX-L XMERT only uses COCO captions for CCC\ntask.\n8799\nImage Split Images Sentences (or Questions)\nCOCO-Cap VG-Cap VQA GQA VG-QA All\nMS COCO - VG 72K 361K - 387K - - 0.75M\nMS COCO ‚à©VG 51K 256K 2.54M 271K 515K 724K 4.30M\nVG - MS COCO 57K - 2.85M - 556K 718K 4.13M\nAll 180K 617K 5.39M 658K 1.07M 1.44M 9.18M\nTable 5: Dataset statistics used in pretraining. Each image has multiple sentences/questions. ‚ÄòCap‚Äô is caption. ‚ÄòVG‚Äô\nis Visual Genome. Since MS COCO and VG share51K images, we list it separately to ensure disjoint image splits.\nThis table is from LXMERT (Tan and Bansal, 2019).\nC.4 Visual vocabulary clustering\nTo create visual vocabularies, we run K-means clus-\ntering on Faster R-CNN grid features of COCO\ntrain2014 images. train2014 has 82783 im-\nages, resulting 8 x 8 x 82783 = 5.3M grid features.\nWe use FAISS (Johnson et al., 2017) library for\nclustering. We sample 2.6M features in training\ndata and run 20 iteration, which takes 2 hours.\nC.5 Training\nWe train LXMERT and X-L XMERT for 20 epochs\nwith mixed precision using Apex8 (opt-level O1).\nWe use AdamW optimizer (Loshchilov and Hutter,\n2019) with (Œ≤1,Œ≤2) = (0.9,0.999) and learning\nrate 1e-5 with 5% linear warmup schedule. We\nuse gradient clipping with maximum norm 1.\nInstead of using all pretraining tasks for each\nstep, we Ô¨Årst uniformly sample a modality to\nmask from [image, text, no-mask] and\nrun corresponding tasks similar to (Chen et al.,\n2019; Lu et al., 2020). When image is selected,\nwe use MVFR, MOC for LXMERT and CCC for\nX-L XMERT . When text is selected, we use\nMLM. When no-mask is selected, we replace\ngiven text with a random sentence from training\ndata with 0.5 probability. If the text is replaced, we\nuse ITM. If not, we use ITM and QA.\nTraining LXMERT takes 60 hours with batch size\n1280, and training X-L XMERT takes 40 hours with\nbatch size 920. We use 4 Titan RTX GPUs ( 4 √ó\n24GB) for training both models.\nC.6 Finetuning\nDuring Ô¨Ånetuning on VQA/GQA/NLVR2, a task\nhead consisting of two fully connected layers with\nGeLU activation and layer norm is trained along\nwith pre-trained LXMERT and X-L XMERT . For\nVQA/GQA, the parameters are initialized from\n8https://github.com/NVIDIA/apex\npretrained QA head. We use AdamW optimizer\nwith learning rate 5e-4. We train LXMERT and\nX-L XMERT for 10 epochs for each task. For\nVQA/GQA/NLVR2, Ô¨Ånetuning takes 3/5/1 hours\nrespectively on 4 Titan RTX GPUs (4 √ó24GB).\nD Generator details\nOur image generation system adopts GAN (Good-\nfellow et al., 2014) framework and has two net-\nworks trained: generator and discriminator.\nD.1 Generator Architecture\nOur generator consists of multiple residual blocks\nfollowing SNGAN (Miyato and Koyama, 2018).\nThe generator takes (quantized) 8 √ó8 grid features\nof Faster R-CNN as input and outputs 256 √ó256\nRGB images. We use a generator with 5 residual\nblocks, where each block bilinearly-upsamples fea-\nture map by 2. We use 32 channels of 3x3 kernel for\nevery convolution layer in residual blocks. Note\nthat many existing generator architectures (Miy-\nato and Koyama, 2018; Wang et al., 2018; Karras\net al., 2019, 2020) have residual blocks starting\nfrom higher dimensions (eg. 512, 1024) in low-\nresolution then gradually decrease the dimension\nas feature maps are spatial upsampled. However,\nwe found that using Ô¨Åxed-sized small dimension\nfor all residual blocks makes training more stable.\nEach residual block has spatially adaptive instance\nnorm (SPADE) (Park et al., 2019; Huang and Be-\nlongie, 2017) that guides the residual block using\nspatial information of 8 √ó8 grid features. After\neach spatially adaptive instance norm, we multi-\nply spatial gaussian noise on feature maps to make\nmodel less focus on local texture following Style-\nGAN (Karras et al., 2019). We use spectral normal-\nization (Miyato et al., 2018) after each convolution\nlayer in generator. Following StyleGAN-v2 (Kar-\nras et al., 2020), we use skip connection for each\n8800\nresidual block to generate Ô¨Ånal output. Our gener-\nator has 1.7M trainable parameters. The detailed\narchitecture of our generator is illustrated at Fig. 8.\nD.2 Discriminator Architecture\nDiscriminator also consists of multiple residual\nblocks. We use a discriminator with 5 residual\nblocks, where each residual block downsamples\nfeature map by 2. We use 64 channels of 3x3 ker-\nnel for every convolution layer in residual blocks.\nWe use spectral normalization after each convolu-\ntion layer in discriminator. In contrasts to genera-\ntor, discriminator (1) uses instance norm (Ulyanov\net al., 2016) instead of adaptive instance norm, (2)\ndoes not gaussian noise multiplication and (3) does\nnot use skip connection. Output of the 5 residual\nblocks are 8 √ó8 feature map. Our discriminator\nhave two heads taking these feature maps: (1) ad-\nversarial head spatially averaging8√ó8 feature map\nand predicting whether input image is from origi-\nnal image domain or not and (2) classiÔ¨Åcation head\npredicting cluster ids of 8 √ó8 spatial layouts from\ninput image. Our discriminator has 0.5M train-\nable parameters. The detailed architecture of our\ndiscriminator is illustrated at Fig. 9.\nD.3 Dataset\nWe train our model on COCO train2014 split,\nwhich consits of 82783 images.\nD.4 Training\nOur generator and discrminator are trained with\n4 losses: (1) hinge adversarial loss (Lim and Ye,\n2017; Tran et al., 2017), (2) AC-GAN loss (Odena\net al., 2017), (3) discriminator feature match loss\n(Wang et al., 2018) and (4) perceptual loss (Johnson\net al., 2016) following (Park et al., 2019). Follow-\ning pix2pixHD (Wang et al., 2018), coefÔ¨Åcients for\nthe losses are (1, 1, 10, 10) respectively. Adversar-\nial loss guides generator to output images close to\noriginal images. The rest of the losses guide gener-\nator to output images close to speciÔ¨Åc target images\nusing spatial layout inputs. We use ResNet-50 (He\net al., 2016) for perceptual loss. Detail of losses\nare explained in Sec. D.5.\nWe use Adam optimizer (Kingma and Ba, 2015)\nwith (Œ≤1,Œ≤2) = (0,0.999) and two-time update\nrule (Heusel et al., 2017) with learning rate of\n0.0004 and 0.0001 for generator and discriminator\nrespectively. We train the image generator for 60\nepochs with batch size 96. Training takes 15 hours\non 8 NVIDIA Titan V GPUs (8 √ó12GB).\nD.5 Losses\nIn below equations, ÀÜX and X refer to generated\nimage and target image respectively.\nAdversarial loss\nLG\nadv = ‚àíDadv( ÀÜX) (1)\nLD\nadv = max(1 ‚àíDadv( ÀÜX),0)\n+ max(1 ‚àíDadv(X),0)\n(2)\nwhere DAdv is discriminator adversarial head.\nAC-GAN loss\nLACGAN = ‚àí 1\nN2\n‚àë\nh,w\nlog P(Dcls\nh,w( ÀÜX))\n‚àí 1\nN2\n‚àë\nh,w\nlog P(Dcls\nh,w(X))\n(3)\nwhere Dcls is discriminator classiÔ¨Åcation head.\nDiscriminator feature match loss\nLG\nFM =\n‚àë\nk\n1\nHkWkCk\n‚àë\nh,w,c\n‚Ñìhuber|Dk( ÀÜX)‚àíDk(X)|\n(4)\nwhere\n‚Ñìhuber(x) =\n{\n0.5 ‚àóx2, if |x|‚â§ 1\n|x|‚àí0.5, otherwise\nand Dk is discriminator‚Äôs k-th resblock.\nPerceptual loss\nLG\nFM ‚àíE =\n‚àë\nk\n1\nHkWkCk\n‚àë\nh,w,c\n‚Ñìhuber|Ek( ÀÜX)‚àíEk(X)|\n(5)\nwhere Ek is ResNet-50 (He et al., 2016)‚Äôs\nk-th resblock ( conv2 x, conv3 x, conv4 x,\nconv5 x).\nTotal loss\nLG = Œªadv ‚àóLG\nadv\n+ ŒªACGAN ‚àóLACGAN\n+ ŒªFM ‚àóLG\nFM\n+ ŒªFM ‚àíE ‚àóLG\nFM ‚àíE\n(6)\nLD = Œªadv ‚àóLD\nadv\n+ ŒªACGAN ‚àóLACGAN\n(7)\nwhere (ŒªGAN ,ŒªACGAN ,ŒªFM ,ŒªFM ‚àíE) =\n(1,1,10,10).\n8801\nVisual features2048 x 8 x 8\n‚äï‚äï‚äï‚äï\n3x3 Conv2x Upsample\nSpatialcondition\nC x H x W\n32x8x8Spatialcondition\nSPADENoiseLeakyReLU(0.2)2x Upsample3x3 ConvSPADENoiseLeakyReLU(0.2)3x3 Conv‚äï\n1) Generator Architecture2) Generator Resblock\nC x 2H x 2WGenerated image3 x 256 x 256\n32x8x8\n‚äï: Element-wise Addition\nResblock\nResblockResblockResblock\nResblock\n3x3 Conv\n1x1 Conv\n3x3 Conv\n32x16x16\n32x32x32\n32x64x64\n32x128x128\n32x256x256\n32x8x8\n256x8x8\n2x Upsample1x1 Conv2x Upsample\n2x Upsample\n2x Upsample\nSkip connection\n3x3 Conv\n3x3 Conv\n3x3 Conv\n3x3 Conv\nFigure 8: Generator architecture that takes 8x8 grid visual features and generates 256x256 images.\n8x8 cluster id prediction\nC x 2H x 2W\nLeakyReLU(0.2)3x3 ConvInstance NormLeakyReLU(0.2)3x3 Conv¬Ω AvgPool‚äï\n1) Discriminator Architecture2) Discriminator Resblock\nC x H x W‚äï: Element-wise Addition\nResblock\nResblockResblockResblock\nResblock\n32x16x16\n32x32x32\n32x64x64\n32x128x128\n32x8x8\n¬Ω AvgPool1x1 Conv\nInput image3 x 256 x 256\n3x3 ConvGlobal AvgPool3x3 Conv\nReal / Fake\nAdversarial headClassification head\nFigure 9: Discriminator architecture that takes 256x256 images.\n8802\nE Evaluation details\nE.1 Image metrics\nTo calculate image metrics, we follow Xu et al.\n(2018) and randomly sample 30000 images from\nMS COCO val2014 split and sample a caption\nfor each image. Then we generate images from\nthose 30000 captions for each method. We use\nsubset of these 30000 captions for automatic image\nevaluation.\nInception Score (IS) Following Zhu et al.\n(2019), we use all 30000 generated images. We use\nOpenAI implementation9 to calculate IS.\nFr¬¥echet Inception Distance (FID) Following\nZhu et al. (2019), we use all 30000 generated im-\nages. We use PyTorch port of ofÔ¨Åcial implementa-\ntion10 to calculate FID.\nR-precision-easy We use all 30000 generated\nimages. For R-precision-easy, we sample 99 nega-\ntive captions for each caption, where all negative\ncaptions correspond to different val2014 images.\nR-precision-hard For each R-precision-hard cat-\negory (noun/verb/color/number), we use 1000 ran-\ndomly sampled caption that contains a category\nword. Then we generate 9 negative captions by\nswapping the detected category word with another\nword with same category. We use POS-tagging\nwith spaCy11 to Ô¨Ånd category words from a caption.\nWe present per-category score of R-precision-hard\nat table 6.\nE.2 Human evaluation\nWe use Amazon Mechanical Turk12 for human eval-\nuation.\nHUMMUS score For each HUMMUS category\n(noun/verb/color), we use 100 randomly sampled\nimages. Then we mask out words in the same\nfashion as in R-precision-hard metric. A total of\n280 unique crowdworkers completed the task, with\na median of 13 images annotated per worker. We\npresent per-category score of HUMMUS score at\ntable 7. Fig 10 shows screenshot of HUMMUS\nscore (noun category) evaluation task.\n9https://github.com/openai/\nimproved-gan/tree/master/inception_score\n10https://github.com/\nmseitzer/pytorch-fid/tree/\n802da3963113b5b5f8154e0e27580ee4c97460ab\n11https://spacy.io/\n12https://www.mturk.com/\nPairwise preference For Semantic preference\ntask, we ask annotators (1) ‚ÄòWhich image best\nmatches the caption?‚Äô with caption. For Fidelity\npreference task, we ask annotators ‚ÄòWhich image\nlooks more realistic?‚Äô without providing the cap-\ntion. A total of 357 unique crowdworkers com-\npleted the task, with a median of 17 annotations\nperformed per worker.\nFig 11 shows screenshot of Semantic preference\nevaluation task, and Fig 12 shows screenshot of\nFidelity preference evaluation task.\n8803\nR-precision-hard ‚Üë R-precision-hard categories ‚Üë\nNoun Verb Color Number\nOriginal Image 47.6 80.4 25.3 53.4 31.4\nDM-GAN (Zhu et al., 2019) 27.5 48.9 9.5 35.8 15.7\nL XMERT 6.6 5.6 1.7 10.2 8.7\nX-L XMERT 25.1 41.4 9.8 30.7 18.5\nX-L XMERT sampling variations:\nAutoregressive\nTL ‚Üí BR 18.9 31.6 7.3 21.0 15.5\nRandom 24.7 41.2 10.1 28.8 18.7\nRandom-200 23.3 41.2 10.1 26.5 16.5\nEasy-First 22.0 35.6 8.1 25.3 18.9\nParallel\nMask-Predict-1 21.4 35.2 7.7 29.8 12.7\nMask-Predict-4 25.1 41.4 9.8 30.7 18.5\n= X-L XMERT\nMask-Predict-10 22.6 37.3 10.0 26.1 16.9\nTable 6: R-precision-hard per-category scores\nHUMMUS ‚Üë HUMMUS Categories‚Üë\nNoun Verb Color\nOriginal Image 0.73 0.79 0.52 0.89\nDM-GAN 0.49 0.42 0.45 0.60\nLXMERT 0.27 0.16 0.43 0.21\nX-L XMERT 0.49 0.55 0.42 0.50\nX-L XMERT sampling variations:\nTL‚ÜíBR 0.43 0.45 0.42 0.41\nRandom 0.48 0.52 0.42 0.51\nMask-Predict-1 0.47 0.48 0.40 0.54\nTable 7: Evaluating semantics with HUMMUS .\n8804\nFigure 10: Screenshot of HUMMUS score evaluation system\nFigure 11: Screenshot of Semantic preference evaluation system\n8805\nFigure 12: Screenshot of Fidelity preference evaluation system"
}