{
  "title": "Probing for Understanding of English Verb Classes and Alternations in Large Pre-trained Language Models",
  "url": "https://openalex.org/W4385574079",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2114669190",
      "name": "David Yi",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2482832856",
      "name": "James Bruno",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2109753862",
      "name": "Jiayu Han",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5033673070",
      "name": "Peter Zukerman",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A4209908596",
      "name": "Shane Steinert-Threlkeld",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W1728842521",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2607879133",
    "https://openalex.org/W2973957133",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W2962926715",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2039012772",
    "https://openalex.org/W2914924671",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1575172693",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3159900299",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W2109553965",
    "https://openalex.org/W2728435982"
  ],
  "abstract": "We investigate the extent to which verb alternation classes, as described by Levin (1993), are encoded in the embeddings of Large Pre-trained Language Models (PLMs) such as BERT, RoBERTa, ELECTRA, and DeBERTa using selectively constructed diagnostic classifiers for word and sentence-level prediction tasks. We follow and expand upon the experiments of Kann et al. (2019), which aim to probe whether static embeddings encode frame-selectional properties of verbs. At both the word and sentence level, we find that contextual embeddings from PLMs not only outperform non-contextual embeddings, but achieve astonishingly high accuracies on tasks across most alternation classes. Additionally, we find evidence that the middle-to-upper layers of PLMs achieve better performance on average than the lower layers across all probing tasks.",
  "full_text": "Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 142 - 152\nDecember 8, 2022 ©2022 Association for Computational Linguistics\nProbing for Understanding of English Verb Classes and Alternations in\nLarge Pre-trained Language Models\nDavid K. Yi, James V . Bruno, Jiayu Han, Peter Zukerman, Shane Steinert-Threlkeld\nDepartment of Linguistics, University of Washington\n{davidyi6, jbruno, jyhan126, pzuk, shanest}@uw.edu\nAbstract\nWe investigate the extent to which verb alter-\nnation classes, as described by Levin (1993),\nare encoded in the embeddings of Large Pre-\ntrained Language Models (PLMs) such as\nBERT, RoBERTa, ELECTRA, and DeBERTa\nusing selectively constructed diagnostic clas-\nsifiers for word and sentence-level prediction\ntasks. We follow and expand upon the exper-\niments of Kann et al. (2019), which aim to\nprobe whether static embeddings encode frame-\nselectional properties of verbs. At both the\nword and sentence level, we find that contextual\nembeddings from PLMs not only outperform\nnon-contextual embeddings, but achieve aston-\nishingly high accuracies on tasks across most\nalternation classes. Additionally, we find evi-\ndence that the middle-to-upper layers of PLMs\nachieve better performance on average than the\nlower layers across all probing tasks.\n1 Introduction\nWe investigate the extent to which verb alternation\nclasses are represented in word and sentence em-\nbeddings produced by Pre-trained Language Model\n(PLM) embeddings (Qiu et al., 2020). As first com-\nprehensively cataloged by Levin (1993), verbs pat-\ntern together into classes according to the syntactic\nalternations in which they can and cannot partic-\nipate. For example, (1) illustrates the causative-\ninchoative alternation. Break can be a transitive\nverb in which the subject of the sentence is the\nagent and the direct object is the theme, as in ex-\nample (1a). It can also alternate with the form in\n(1b), in which the subject of the sentence is the\ntheme and the agent is unexpressed. However, (2)\ndemonstrates that cut cannot participate in the same\nalternation, despite its semantic similarity.\n(1) a. Janet broke the cup.\nb. The cup broke.\n(2) a. Margaret cut the bread.\nb. * The bread cut.\n(3) demonstrates an alternation of a different\nclass – namely, the spray-load class, in which the\ntheme and locative arguments can be syntactically\nrealized as either direct objects or objects of the\npreposition. Spray can participate in the alternation,\nbut as shown in (4), pour cannot.\n(3) a. Jack sprayed paint on the wall.\nb. Jack sprayed the wall with paint.\n(4) a. Tamara poured water into the bowl.\nb. * Tamara poured the bowl with water.\nThe alternations in which a verb may partici-\npate is taken to be a lexical property of the verb\n(e.g. Pinker, 1989; Levin, 1993; Levin et al., 1995;\nSchafer, 2009). Moreover, we hypothesize that the\nalternations should be observable within large text\ncorpora, and are therefore available during the pre-\ntraining procedure for PLMs such as BERT (Devlin\net al., 2018). In contrast, ungrammatical examples\nsuch as (2b) and (4b) should be virtually absent\nfrom the training data. This leads us to hypothesize\nthat PLM embeddings should encode whether par-\nticular verbs are allowed to participate in syntactic\nframes of various alternation classes. Our research\nquestions are as follows:\n1. Do PLM word-level contextual representa-\ntions encode information about which syntac-\ntic frames an individual verb can participate\nin?\n2. At the sentence level, do PLM embeddings en-\ncode the frame-selectional properties of their\nmain verb?\nThrough our series of experiments, we find that\nPLM embeddings indeed encode information about\nverb alternation classes at both the word and sen-\ntence level. While performance is relatively con-\nsistent on the word-level task for the four PLMs\n142\nwe analyze, we find that ELECTRA (Clark et al.,\n2020) significantly outperforms the other models\nfor the sentence-level task. Furthermore, we find\nevidence suggesting that middle-to-upper layers\nencode more information about verb alternation\nclasses since they consistently improve upon the\nlower layers across all tasks.\nThe rest of the paper is organized as follows:\nafter a brief review of related literature in Section 2,\nwe present datasets and models that are relevant\nto our experiment in Sections 3 and 4. We then\npresent two experiments to answer our research\nquestions in Sections 5 and 7. Section 6 presents\nan additional control task(Hewitt and Liang, 2019)\nto test whether our linear probes are selective for\nthe given tasks. Finally, we offer a discussion in\nSection 8 and overall conclusions in Section 9.\n2 Related work\nOur work follows Kann et al. (2019), who at-\ntempt to predict verb class membership and sen-\ntence grammaticality judgments on the basis of\nGloVe embeddings (Pennington et al., 2014) and\nembeddings derived from the 100M-token British\nNational Corpus with a single-directional LSTM\n(Warstadt et al., 2019). For the sentence-level task,\nthey further process the input embeddings using a\nsentence encoder trained on a “real/fake” sentence\nclassification task. Varying multi-layer perceptron\n(MLP) architectures are used for the classification\nstep. Because their primary research focus has\nto do with how neural language models inform\nlearnability (in the sense of human language ac-\nquisition), they intentionally use smaller language\nmodels derived from “an amount of data similar\nto what humans are exposed to during language\nacquisition” and avoid models trained on “several\norders of magnitude more data than humans see in\na lifetime” (p. 291).\nAs described in Section 5, we depart and build\nupon Kann et al. 2019 by examining the embedding\nrepresentations of PLMs instead of static embed-\ndings. We then use an intentionally simple and\nselective linear diagnostic classifier to probe the\nrepresentations, as our research questions focuses\non the PLM embeddings themselves. We note that\nKann et al. (2019) achieves only modest perfor-\nmance in prediction accuracy and MCC, and only\nfor a limited number of verb classes. While this is a\nvaluable result for their research goals, our hypoth-\nesis is that PLMs will achieve better performance\ndue to a combination of their contextual represen-\ntations, complex architectures, and larger training\ncorpora.\nTo our knowledge, attempting to predict verb al-\nternation class membership along the lines of Levin\n1993 from PLM representations is novel. How-\never, two very closely related lines of work include\nthe experiments of Warstadt and Bowman (2019),\nwhich respectively evaluate the performance of var-\nious PLMs on the CoLA (Warstadt et al., 2019) and\nBliMP (Warstadt et al., 2020) benchmarks, which\ninclude acceptability judgment examples from a\nwide variety of linguistic phenomena (including\nverb argument structures). We distinguish our ex-\nperiments from these papers in two major ways.\nFirst, we attempt to directly probe the linguistic\nknowledge of individual PLM embedding layers\nwith a classification probe instead of specifically\nfinetuning the models to a specific task. Second,\nwe limit our focus to verb alternation classes and\npresent detailed analysis about patterns and trends\nacross different alternations and their correspond-\ning syntactic frames.\n3 Data\nIn our experiments, we use two dataset created\nby Kann et al. (2019). One is the Lexical Verb-\nframe Alternations dataset (LaV A), which is based\non the verbs and alternation classes defined in\nLevin (1993). It contains a mapping of 516 verbs\nto 5 alternation classes, which are further sub-\ndivided into two syntactic frames for each alter-\nnation. The broad categories of the alternation\nclasses are: Spray-Load, Causative-Inchoative, Da-\ntive, There-insertion, and Understood-object. Ta-\nble 11 provides the class distributions for each syn-\ntactic frame. Frames and Alternations of Verbs\n(FA V A), the other dataset, is a corpus of 9413 semi-\nautomatically generated sentences formed from the\nverbs in LaV A along with human grammaticality\njudgments. The sentences in FA V A are categorized\naccording to the relevant alternation class, and are\nseparated into train, development, and test sets by\nthe authors for each category.\n1A similar table appears in Kann et al. (2019), but we\npresent it again here because of discrepancies that we found\nin the distribution counts. Notably, it appears that the authors\nflipped the positive and negative counts for the there-Insertion\nand Understood-Object alternation classes which carries over\nto their results.\n143\nLEVIN -CLASS CAUS -INCH DATIVE SPRAY -LOAD there-INSERTION UNDERSTOOD -OBJECT\nInch. Caus. Prep. 2-Obj with loc. no-there there Refl No-Refl\nPositive 73 124 65 74 101 86 149 50 84 11\nNegative 144 0 377 442 242 257 0 192 419 503\nTotal 217 124 442 516 343 343 149 242 503 514\nTable 1: An updated overview of the LaV A dataset based on verb membership class distributions for each syntactic\nframe. “Postitive” refers to the number of verbs that can participate in the specified syntactic frame, while “Negative”\nrefers to the number of verbs that cannot participate.\n4 Models\nIn addition to BERT, we perform experiments on\nseveral recent Transformer-based PLMs including\nRoBERTa (Liu et al., 2019), DeBERTa (He et al.,\n2021), and ELECTRA (Clark et al., 2020) which\nvary from BERT in a few ways including modi-\nfications to BERT’s tokenization and pre-training\nprocedure and the size of their training corpus. To\nmake comparisons between each model fair, we\nuse the base architectures for each model which\nhave 12 layers, 12 attention heads, and a hidden\nlayer size of 768.2\n4.1 Model differences\nFor pre-training, BERT uses standard Masked Lan-\nguage Modeling (MLM) wherein tokens from a\ngiven input sequence are masked at random and the\nmodel attempts to recover the masked tokens from\nthe unmasked tokens and Next Sentence Prediction\n(NSP), in which the model tries to predict whether\none sentence follows another in a given text se-\nquence. The other PLMs drop NSP from their\npre-training procedure but make other significant\nchanges to the architecture and the MLM approach.\nRoBERTa introduces “dynamic” masking, in which\ndifferent tokens are masked across different train-\ning epochs (as opposed to the same training mask\nbeing used across epochs). DeBERTa uses a “dis-\nentangled attention mechanism” which computes\nattention weights using distinctly encoded position\nand context vectors, and also moves absolute posi-\ntion encodings from the input layer to the second-\nto-last layer. Lastly, instead of randomly mask-\ning input tokens, ELECTRA strategically replaces\ntokens with plausible alternatives using a trained\ngenerator network, and separately trains a discrimi-\nnative model which aims to predict whether each\ntoken in an input sequence was replaced by a gen-\nerator sample.\n2All further references to these models refer to their base\narchitectures.\n4.2 Training Data\nIn addition to variations in the pre-training methods,\nthe models are also trained on different datasets.\nBERT and ELECTRA are both trained on the En-\nglish Wikipedia Dump and BookCorpus (Zhu et al.,\n2015). DeBERTa is additionally trained on CC-\nStories (Trinh and Le, 2018) and OpenWebText\n(Gokaslan and Cohen, 2019). Finally, RoBERTa\nis pretrained on all of the aforementioned datasets\nas well as the CC-News corpus (Mackenzie et al.,\n2020).\n5 Experiment 1: Frame Membership\nfrom Word Embeddings\n5.1 Method\nIn order to answer the first question: “Do PLM\ntoken-level representations encode information\nabout which syntactic frames an individual verb\ncan participate in?”, we build a diagnostic classifier\nfor each syntactic frame which takes a verb’s layer\nembedding representation as input. For example,\nto probe the Spray-Load alternation, we build two\nbinary classifiers: one that predicts whether a verb\ncan participate in the “locative” frame and one that\npredicts whether a verb can participate in the “with”\nframe.\nFurthermore, we build a separate classifier for\neach model layer based on the embedding repre-\nsentations from that particular layer. For the token-\nembedding layer, the verb embedding is formed by\naveraging the pretrained token embeddings that cor-\nrespond to a particular verb. For layers 1–12, the\nverb embedding is formed by incorporating con-\ntextual information from the sentences in FA V A.\nSpecifically, for each verb, we pass the grammati-\ncal sentences from FA V A that contain the verb as\ninput to the PLM and average over the token em-\nbeddings corresponding to the verb. We choose\nto only include grammatical examples in the con-\nstruction of the word-level contextual embeddings\nsince we hypothesize that they represent sentences\n144\nMCC Accuracy\nRef. BERT DeBERTa ELECTRA RoBERTa Ref. BERT DeBERTa ELECTRA RoBERTa\nCAUSATIVE-INCHOATIVE\nInchoative 0.555 0.948 [11] 0.969 [11] 0.959 [5] 0.969 [7] 0.855 0.977 0.986 0.982 0.986\nCausative* 0.000 0.000 0.000 0.000 0.000 1.000 1.000 1.000 1.000 1.000\nDATIVE\nPreposition 0.320 0.954 [8]0.937 [12] 0.945 [11] 0.928 [9] 0.850 0.989 0.984 0.986 0.982\nDouble-Object 0.482 0.976 [10] 0.968 [10] 0.976 [12] 0.936 [9] 0.853 0.994 0.992 0.994 0.984\nSPRAY-LOAD\nWith 0.645 0.972 [10] 0.972 [12] 0.979 [8]0.930 [10] 0.839 0.988 0.988 0.991 0.971\nLocative 0.253 0.969 [10]0.961 [12] 0.961 [9] 0.953 [11] 0.734 0.989 0.985 0.985 0.983\nTHERE\nNo-There* 0.000 0.000 0.000 0.000 0.000 1.000 1.000 1.000 1.000 1.000\nThere 0.459 1.000 [9] 0.987 [7] 1.000 [10] 0.962 [10] 0.858 1.000 1.00 1.000 0.988\nUNDERSTOOD OBJECT\nRefl 0.000 0.868 [6] 0.869[12] 0.860 [5] 0.884 [11]0.732 0.964 0.964 0.962 0.968\nNon-Refl 0.219 0.850 [7] 0.850 [10] 0.855 [11] 0.794 [8] 0.976 0.994 0.994 0.994 0.992\nTable 2: Results from Word-Level experiments with static embeddings. Reference MCC is from Kann et al. (2019)’s\nCoLA word-level experiments. The * symbol indicates syntactic frames which only have positive examples, which\ntrivially achieve 100% accuracy and 0 MCC (see Footnote 1). The best performing model for a given frame is\ndenoted in bold (ties are not bolded), and the best performing layer for each model is denoted in brackets ‘[]’.\nin the actual training corpora of the PLMs more\naccurately than the ungrammatical examples. 3 We\nthen average over the verb representations for all\ninput sentences in each layer to form the “layer-\nembedding” for the verb.\nWe choose a Logistic Regression classifier with-\nout regularization as our diagnostic probe as imple-\nmented in scikit-learn (Buitinck et al., 2013)\nand show that it is sufficiently selective in Section\n6. Following Kann et al. (2019), we use strati-\nfied k-fold cross-validation to split the verbs into\n4 equally-sized folds: 3 of which are chosen to be\nthe training set and the remaining fold chosen to be\nthe test set.\nAlso following Kann et al. (2019), we re-\nport Matthews correlation coefficient (MCC)\n(Matthews, 1975) in addition to accuracy for model\nevaluation. MCC is better suited to data such as\nours, in which there is an extreme majority class\nbias for all syntactic frames. 4\n3A potential issue with constructing the embeddings in this\nmanner is that the classifier may simply “memorize” whether\nthere is a corresponding grammatical example for each verb\nin FA V A to trivially determine frame membership. However,\nwe included ungrammatical examples as well in preliminary\nexperiments and found negligible differences from our final\nresults.\n4All code and data needed to replicate our analysis can be\nfound at https://github.com/kvah/analyzing_\nverb_alternations_plms\n5.2 Results\nIn Figure 1, we present the layer-by-layer perfor-\nmance of each PLM and in Table 2, we report\na comparison between the best-performing layer\nfor each PLM alongside the performance of the\n“CoLA-style” reference embeddings from Kann\net al. (2019). Overall, we find that the contex-\ntual PLM embeddings dramatically outperform the\nreference embeddings in terms of both MCC and\naccuracy.\nSurprisingly, the PLMs perform well even for the\nmore challenging frames; for the “locative” frame,\nBERT achieves 0.969 MCC compared to 0.253\nwhen using the reference embeddings, and for the\n“non-reflexive” frame, ELECTRA achieves 0.855\nMCC compared to 0.219 when using the reference\nembeddings. Furthermore, we observe consistent\npatterns in performance across different layers of\neach PLM. As shown in Figure 2, the lower lay-\ners achieve low-to-moderate correlation on aver-\nage while the middle-to-upper layers consistently\nachieve strong correlation.\n6 Control Task\nA control task as described by (Hewitt and Liang,\n2019) aims to combat the Probe Confounder Prob-\nlem, which highlights the issue of supervised probe\nclassifiers “learning” a linguistic task by combin-\ning signals in the data that are irrelevant to the\nlinguistic property of interest. In the context of\nour first experiment, a confounding probe would\n145\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0mcc\ninchoative\nmodel\nbert-base-uncased\nroberta-base\nelectra-base-discriminator\ndeberta-base\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0mcc\npreposition\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0mcc\n2object\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0mcc\nwith\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0mcc\nlocative\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0mcc\nThere\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8mcc\nRefl\n0 1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8mcc\nNon-Refl\nFigure 1: MCC for each model layer across all syntactic frames on LA V A.\n0 2 4 6 8 10 12\nlayer\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7mcc\nMean MCC across Layers\nmodel\nbert-base-uncased\ndeberta-base\nelectra-base-discriminator\nroberta-base\nFigure 2: Mean MCC for each model layer across all\nsyntactic frames on LA V A.\nbe problematic since it suggests that good model\nperformance may be attributed to arbitrary signals\npicked up by the probe, as opposed to the PLM em-\nbeddings actually containing linguistic information\nabout the syntactic frames. To mitigate the Probe\nConfounder Problem, we implement an example\ncontrol task for the Spray-Load “with” syntactic\nframe for BERT.\nFor each verb vi in LaV A with a binary labelyi\ndenoting whether vi can participate in the syntax\nframe SL-WITH , we independently sample a con-\ntrol behavior C(v) by randomly assigning a binary\n“membership” value to vi based on the empirical\nmembership distribution of verbs that participate\nin the SL-WITH syntax frame. The control task is\nthe function that maps each verb, vi, to the label\nspecified by the control behavior C(Vi):\nfcontrol(vi) =C(vi)\nFollowing the experiment design of Hewitt and\nLiang (2019), we compare the selectivity of a lin-\near probe, an Multi-layer Perceptron with 1-hidden\nlayer (MLP-1), and an MLP with 2-hidden layers\n(MLP-2) where the selectivity of a model is de-\nfined by the difference between its accuracy on the\nreal task (i.e. predicting verb membership for the\nSL-WITH frame) and the control task. In addition,\nwe explore several “complexity control” methods\nincluding limitation of feature dimensionality, re-\nducing the number of training examples, and in-\ncreasing regularization.\n6.1 Complexity Hyperparameters\nIn this section, we describe the complexity control\nmethods in more detail and enumerate the hyperpa-\nrameters that we tried for each method. The control\nparameters were chosen based on the three most\neffective methods from the experiments of Hewitt\nand Liang (2019). To isolate the effect of each con-\ntrol method, we only change one of the complexity\nparameters in each experiment.\n6.1.1 Limiting Dimensionality\nFor the Logistic Regression model, we reduce the\ndimensionality of the feature embeddings by per-\nforming a Truncated Singular Value Decomposi-\ntion and limiting the output matrix to rank k. For\nthe MLP models, we simply limit the size of the\nhidden layer(s) to k.\nConsidering the input BERT embeddings which\nhave 768 dimensions, we limit k to the following\nvalues: {20, 100, 300, 500}.\n146\nFigure 3: Linguistic Task selectivities for the three complexity control methods.\nFigure 4: Linguistic Task accuracies for the three complexity control methods.\n6.1.2 Reducing Proportion of Training Data\nBecause LaV A is not split into train and test sets,\nwe use 4-fold cross validation as done in Kann et al.\n(2019) with 3 training folds and one test fold for\nevaluation on the control task. As an additional\nconstraint, we reduce the number of training sam-\nples in each training fold by randomly sampling a\nproportion p of the samples and discarding the rest.\nAlthough Zhang and Bowman (2018) recom-\nmend training on 1%, 10% and, 100% of the train-\ning data, our training data is relatively small and im-\nbalanced (71% of the train set verbs do not partici-\npate in the SL-WITH frame). Hence, we experiment\nwith larger values of p: {0.1, 0.3, 0.5, 0.7, 0.9}\n6.1.3 L2 Regularization\nFor both the linear and MLP models, we add L2\nregularization with the following strength values:\n{0.01, 0.1, 0.2, 0.5, 1}\n6.2 Results\nFigure 3 shows the high-level trends across experi-\nment configurations for model selectivity. We ob-\nserve that the linear model with default parameters\n(k = 768, p= 1, L2 = 0) outperforms both the\nMLP-1 and MLP-2 model in selectivity (0.420 v.s.\n0.397) with no significant decrease in linguistic task\naccuracy (0.985 for the linear and MLP-1 models\nv.s. 0.988 for the MLP-2 model).\nLooking at the effect of complexity control meth-\nods on model accuracy in Figure 4, we find that lim-\niting dimensionality and L2 regularization has little\nimpact across all configurations, with the worst\nmodel (linear: k = 20) achieving an accuracy\nof 0.983 and the best model (MLP-2: k = 100)\nachieving only a slightly higher accuracy of 0.991.\nOn the other hand, reducing the proportion of data\nin each training fold appears to have significant im-\npact on model performance. For the linear model,\nthere is a huge discrepancy in accuracy between\ntraining on 10% of the data (0.869) and the full\ntraining set (0.988). A nearly identical pattern can\nbe observed for both of the MLP models as well.\nComparing selectivity, the linear models out-\nperform both MLPs across all complexity control\nmethods. For dimensionality control, we see a\nlower selectivity in the linear model for lower val-\nues of k (k = 20, 100) but the best linear model\n(k = 300) achieved a higher selectivity (0.429)\n147\nthan the best MLP model (0.414). Similarly, the\nbest performing configuration for reduced training\nsamples and L2 regularization are linear models\nwith p = 0.9 (0.423) and L2 = 0.1 (0.431) respec-\ntively.\nWe arrive at two major conclusions from the\ncontrol task experiments. The first is that a linear\nprobe is a good choice for our linguistic task since\nit achieves higher selectivity than the MLP models\nwithout substantial loss in model accuracy across\na wide range of complexity control methods. The\nsecond is that limiting dimensionality, reducing\ntraining samples, and L2 regularization are all ef-\nfective methods for increasing model selectivity for\nboth the linear and MLP models. However, the best\nconfigurations are not significantly better (> 0.01\nimprovement in selectivity) than the default linear\nmodel so we did not make any modifications to our\nclassification probe. As we only performed these\nexperiments for BERT and the SL-WITH syntactic\nframe specifically, a great avenue for future work\nis to test whether our results extend to other PLMs\nand syntactic frames.\n7 Experiment 2: Grammar Judgments\nfrom Sentence-embeddings\n7.1 Method\nIn the second experiment, we investigate the extent\nto which PLMs encode frame-selectional properties\nof their main verb. For each PLM and embedding\nlayer, we fit a binary Logistic Regression classifier\non the FA V A training set for a given alternation\nclass which predicts whether a given sentence is\ngrammatical. We ignore the held out development\nset because the probe hyperparameters do not need\nto be tuned and directly evaluate each model on the\ntest set. The whole process can be described by the\nfollowing equation:\ncsi = f(Wsi + b)\nwhere si refers to the embedding of the whole sen-\ntence for layer i (by averaging all i layer’s hidden\nstates of words in the sentence s), f refers to the lo-\ngistic regression classifier, W and b are the parame-\nters of f, and csi is a binary value corresponding to\nwhether the sentence is grammatical. We then ex-\ntract the best performing layer for each model and\ncompare the results with the reference acceptability\njudgment model proposed by Kann et al. (2019).\n7.2 Results\nThe MCC and accuracy scores for each model and\nlayer are shown in Figure 5. From the figure, we\ncan see that there is significant variation in layer\nperformance aside from the Understood-object al-\nternation. Generally, we observe a trend in which\nperformance increases substantially from the lower\n(1-4) layers to the middle layers (5-9), with some\nmodels, most notably ELECTRA, continuing to\nimprove through the upper layers (10-12). This can\nbe seen more clearly in Figure 6, which shows the\nmean layer performance across each category. Fur-\nthermore, ELECTRA achieves the best MCC on 5\nof the 6 categories: Combined (0.818), Inchoative\n(0.864), Spray_Load (0.830), There (0.828), and\nUnderstood-Object (0.869). The outlier frame is\nRoBERTa, which achieves the best MCC (0.802)\non the Dative frame.\nTable 3 provides a comparison between the best\nperforming layer from each PLM and the reference\nembeddings from Kann et al. (2019) for each al-\nternation class. As defined by Kann et al. (2019)\nan MCC value between 0.5 and 0.7 demonstrates\na moderate correlation between predicted and true\nlabels while an MCC greater than 0.7 implies\nstrong correlation. From the table, we see all mod-\nels are able to obtain strong correlation for the\nUnderstood-Object alternation, the There frame,\nand the Causative-Inchoative frame. In contrast,\nBERT and RoBERTa are only able to achieve mod-\nerate correlation on the Spray-Load frame, while\nall models except RoBERTa only achieve moderate\ncorrelation on the Dative alternation. Consistent\nwith the CoLA-style embeddings, we find that the\nPLMs achieve the best performance on average\nfor sentences from the Understood-Object alterna-\ntion class. This is surprising since frames from the\nUnderstood-Object alternation were the hardest to\npredict for the word-level task for both the CoLA-\nstyle embeddings and the PLMs. Nevertheless, all\nPLM outperform the reference model across all\nalternation categories for the sentence acceptability\njudgment task.\n8 Discussion\nOn the word level prediction task, all PLMs achieve\nstrong correlation (> 0.7 MCC) across all syntactic\nframes with the strongest performance in the “there”\nframe (1.00, achieved by BERT and ELECTRA)\nand the weakest performance on the “non-reflexive”\nframe (0.794, achieved by RoBERTa). When look-\n148\nCOMBINED CAUSATIVE -INCHOATIVE DATIVE SPRAY-LOAD THERE UNDERSTOOD\nMCC\nREF. 0.290 0.603 0.413 0.323 0.528 0.753\nBERT 0.642 (10) 0.760 (8) 0.678 (6) 0.625 (10) 0.716 (10) 0.842 (9)\nDEBERTA 0.653 (9) 0.776 (5) 0.633 (8) 0.704 (12) 0.744 (6) 0.826 (1)\nELECTRA 0.818 (11) 0.864 (11) 0.670 (8) 0.830 (12) 0.828 (12) 0.869 (11)\nROBERTA 0.496 (8) 0.725 (8) 0.802 (2) 0.470 (5) 0.725 (11) 0.793 (1)\nACCURACY\nREF. 0.646 0.854 0.760 0.662 0.729 0.874\nBERT 0.840 (10) 0.920 (8) 0.880 (6) 0.820 (10) 0.890 (10) 0.921 (9)\nDEBERTA 0.847 (9) 0.924 (5) 0.902 (8) 0.858 (12) 0.912 (6) 0.909 (1)\nELECTRA 0.920 (11) 0.954 (11) 0.897 (8) 0.918 (12) 0.937 (12) 0.934 (11)\nROBERTA 0.787 (8) 0.909 (8) 0.944 (2) 0.747 (5) 0.899 (11) 0.893 (1)\nTable 3: Results from Sentence-Level experiments. REF refers to the reference probing model in (Kann et al., 2019).\nBolded values show the best result for each alternation class. ‘()’ indicates the best performing layer for each model.\n1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8mcc\ncombined\nmodel\nbert-base-uncased\nroberta-base\nelectra-base-discriminator\ndeberta-base\n1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8mcc\ndative\n1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8mcc\ninchoative\n1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8mcc\nspray_load\n1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8mcc\nthere\n1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8mcc\nunderstood\nFigure 5: Layer-by-layer MCC score for each alternation class on FA V A.\ning at accuracies, each model is able to predict\nwhether a verb belongs to a particular syntactic\nframe with excellent accuracy (> 0.95 across all\nalternation frames). Morever, looking at Table 2,\nwe see that the middle (5-9) and upper (10-12) lay-\ners consistently achieve the highest MCC, which is\nreinforced by the trend shown in Figure 2.\nFor the sentence-level experiments, we see a sim-\nilar outcome wherein the upper-middle PLM layers\nachieve the best performance on average. How-\never, we observe that there is much more variation\nin performance between each PLM. ELECTRA\nand BERT are relatively consistent, since their best\nperforming layer for all alternation classes either\ncome from the middle or upper layers. In con-\ntrast, the lower layers of RoBERTa achieve the best\nperformance on the Dative alternation, and both\nRoBERTa and DeBERTa achieve the best perfor-\nmance on the Understood-Object alternation from\nthe first layer. These anomalies can potentially be\nexplained by the claim that different alternation\nclasses require different types of linguistic knowl-\nedge (i.e. syntactic v.s. semantic) which are en-\ncoded in different PLM layers. However, the con-\nsistently strong performance of the upper layers for\nBERT and ELECTRA across all alternation classes\nprovides counter evidence against the claim.\nELECTRA is the best performing model overall\non the sentence-level acceptability task, achieving\nthe best MCC and accuracy on four of the five alter-\nnation classes (all except Dative). Unsurprisingly,\nELECTRA also excels on the combined dataset\ncompared to the other models (0.165 MCC over the\nsecond-best performing model, DeBERTa). While\nit is difficult to attribute the model’s success to a\nspecific property, one hypothesis is that its gener-\n149\nator/discriminator architecture closely resembles\nthe FA V A task of identifying acceptable sentences\nfrom linguistic minimal pairs. This idea is rein-\nforced by the authors as well, who note that the\nmodel’s relatively strong performance on CoLA po-\ntentially stems from the fact that the acceptability\njudgment task of CoLA “closely matches ELEC-\nTRA’s pre-training task of identifying fake tokens”\n(Clark et al., 2020, p.15).\nWhile we are optimistic about our results, there\nare several limitations to our experiments. First,\nwe only analyze five different alternation classes\nwhich is a small subset of the 83 classes presented\nin Levin (1993). In addition – although our control\ntask ensures that our classifier probe is relatively\nselective for the first experiment and BERT, it may\nnot necessarily generalize well to the second ex-\nperiment, other syntactic frames, and other models.\nIn the future, we hope to expand our selectivity\nexperiments to a wider array of syntactic frames\nand models.\n2 4 6 8 10 12\nlayer\n0.4\n0.5\n0.6\n0.7\n0.8mcc\nMean MCC across Layers\nmodel\nbert-base-uncased\ndeberta-base\nelectra-base-discriminator\nroberta-base\nFigure 6: Mean layer MCC score across all alternation\nclasses on FA V A.\n9 Conclusion and Future Work\nOverall, our results support the hypothesis that\nPLM contextual embeddings encode linguistic in-\nformation about verb alternation classes at both the\nword and sentence level. For the frame-selectional\nverb classification task, all PLMs achieve signifi-\ncant improvement upon the reference CoLA-style\nembeddings from Kann et al. (2019), especially for\nframes in which the CoLA-style embeddings obtain\nweak correlation (i.e. “locative”, “reflexive”, and\n“non-reflexive”). Also, it is clear that model perfor-\nmance tends to improve from lower to upper layers,\nwhich can be seen the most easily from the mean\nperformance across layer figures. For the sentence\nacceptability task, we arrive at similar conclusions,\nalbeit with greater distinction in results between\ndifferent models and layers. While there are nu-\nmerous factors that may be responsible for the im-\nproved performance from PLMs, we hypothesize\nthat the improvement can largely be attributed to\nthe attention-based encodings of transformer mod-\nels since we only saw modest improvements in\nperformance from the reference embeddings when\nusing the bottom “static” layers for each PLM.\nIn terms of future work, there are several interest-\ning avenues that we hope to explore. From the data\nperspective, it would certainly be worthwhile to test\nwhether our insights and conclusions extends to the\ndozens of alternations described in Levin (1993)\nthat are not present in the LA V A and FaV A datasets.\nThere are also several interesting adaptations that\ncan be made to our experiment methodology. For\nexample, instead of just analyzing the base archi-\ntecture for each PLM, we could also analyze small\nand large variants to directly evaluate the effect\nof scaling training data and model size within the\nsame model. Moreover, while we attempt to con-\ntrol the Probe Confounder Problemby building a\nselective probe, there is no guarantee that the clas-\nsifier probes do not pick up on arbitrary signals\nin the training data that lead to non-meaningful\nimprovements in performance. Two promising al-\nternative approaches that mitigate this risk include\nunsupervised evaluation of minimal pairs as shown\nin Warstadt et al. (2020) and “amnesic probing”,\nwhich tests whether a property that can be extracted\nfrom a probe is actually relevant to task importance\n(Elazar et al., 2021).\nReferences\nLars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian\nPedregosa, Andreas Mueller, Olivier Grisel, Vlad\nNiculae, Peter Prettenhofer, Alexandre Gramfort,\nJaques Grobler, Robert Layton, Jake VanderPlas, Ar-\nnaud Joly, Brian Holt, and Gaël Varoquaux. 2013.\nAPI design for machine learning software: experi-\nences from the scikit-learn project. In ECML PKDD\nWorkshop: Languages for Data Mining and Machine\nLearning, pages 108–122.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\n150\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nGoldberg. 2021. Amnesic probing: Behavioral expla-\nnation with amnesic counterfactuals. Transactions of\nthe Association for Computational Linguistics, 9:160–\n175.\nAaron Gokaslan and Vanya Cohen. 2019. Openwebtext\ncorpus.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2733–2743.\nKatharina Kann, Alex Warstadt, Adina Williams, and\nSamuel R. Bowman. 2019. Verb argument structure\nalternations in word and sentence embeddings. In\nProceedings of the Society for Computation in Lin-\nguistics (SCiL) 2019, pages 287–297.\nBeth Levin. 1993. English verb classes and alterna-\ntions: A preliminary investigation. University of\nChicago press.\nBeth Levin, Malka Rappaport Hovav, and Samuel Jay\nKeyser. 1995. Unaccusativity: At the syntax-lexical\nsemantics interface, volume 26. MIT press.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJoel Mackenzie, Rodger Benham, Matthias Petri, Jo-\nhanne R. Trippas, J. Shane Culpepper, and Alistair\nMoffat. 2020. Cc-news-en: A large english news\ncorpus. Proceedings of the 29th ACM International\nConference on Information & Knowledge Manage-\nment.\nBrian W Matthews. 1975. Comparison of the pre-\ndicted and observed secondary structure of t4 phage\nlysozyme. Biochimica et Biophysica Acta (BBA)-\nProtein Structure, 405(2):442–451.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(EMNLP), pages 1532–1543.\nSteven Pinker. 1989. Learnability and cognition: The\nacquisition of argument structure. Cambridge, MA:\nMIT Press.\nXiPeng Qiu, TianXiang Sun, YiGe Xu, YunFan Shao,\nNing Dai, and XuanJing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, 63(10):1872–\n1897.\nFlorian Schafer. 2009. The causative alternation. Lan-\nguage and linguistics compass, 3(2):641–681.\nTrieu H. Trinh and Quoc V . Le. 2018. A simple method\nfor commonsense reasoning.\nAlex Warstadt and Samuel R. Bowman. 2019. Lin-\nguistic analysis of pretrained sentence encoders with\nacceptability judgments.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: A benchmark of linguis-\ntic minimal pairs for English. In Proceedings of the\nSociety for Computation in Linguistics 2020, pages\n409–410, New York, New York. Association for Com-\nputational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nKelly Zhang and Samuel Bowman. 2018. Language\nmodeling teaches you more than translation does:\nLessons learned through auxiliary syntactic task anal-\nysis. In Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 359–361, Brussels, Bel-\ngium. Association for Computational Linguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\n151\nA Complexity Control Results\nDimensions (k) Training Prop. ( p) L2 Reg. Accuracy Selectivity\nDEFAULT PARAMS\nLinear 768 1.0 0.0 0.985 0.420\nMLP-1 768 1.0 0.0 0.985 0.397\nMLP-2 768 1.0 0.0 0.988 0.397\nLIMITING DIMENSIONS\nLinear 300 1.0 0.0 0.985 0.429\nMLP-1 100 1.0 0.0 0.985 0.414\nMLP-2 20 1.0 0.0 0.983 0.408\nREDUCING TRAINING SAMPLES\nLinear 768 0.9 0.0 0.988 0.423\nMLP-1 768 0.9 0.0 0.983 0.411\nMLP-2 768 0.9 0.0 0.985 0.414\nL2 REGULARIZATION\nLinear 768 1.0 0.1 0.985 0.431\nMLP-1 768 1.0 1.0 0.988 0.420\nMLP-2 768 1.0 1.0 0.988 0.420\nTable A1: Results from the Complexity Control Experiments. For each experiment, only the best performing\nconfiguration for each model is reported.\n152",
  "topic": "Alternation (linguistics)",
  "concepts": [
    {
      "name": "Alternation (linguistics)",
      "score": 0.8309158086776733
    },
    {
      "name": "Sentence",
      "score": 0.6646816730499268
    },
    {
      "name": "Computer science",
      "score": 0.6631088256835938
    },
    {
      "name": "ENCODE",
      "score": 0.6163825988769531
    },
    {
      "name": "Natural language processing",
      "score": 0.6077907085418701
    },
    {
      "name": "Word (group theory)",
      "score": 0.6047053337097168
    },
    {
      "name": "Verb",
      "score": 0.6011822819709778
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5964712500572205
    },
    {
      "name": "Linguistics",
      "score": 0.3366889953613281
    },
    {
      "name": "Biology",
      "score": 0.0888822078704834
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ]
}