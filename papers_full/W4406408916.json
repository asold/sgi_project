{
  "title": "The role of large language models in the peer-review process: opportunities and challenges for medical journal reviewers and editors",
  "url": "https://openalex.org/W4406408916",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2110377463",
      "name": "Jisoo Lee",
      "affiliations": [
        "Soonchunhyang University",
        "Bucheon University"
      ]
    },
    {
      "id": "https://openalex.org/A2115825107",
      "name": "Jieun Lee",
      "affiliations": [
        "Bucheon University",
        "Soonchunhyang University"
      ]
    },
    {
      "id": "https://openalex.org/A2115017563",
      "name": "Jeong Ju Yoo",
      "affiliations": [
        "Soonchunhyang University",
        "Bucheon University"
      ]
    },
    {
      "id": "https://openalex.org/A2115017563",
      "name": "Jeong Ju Yoo",
      "affiliations": [
        "Bucheon University",
        "Soonchunhyang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2004776253",
    "https://openalex.org/W3108959744",
    "https://openalex.org/W4402215122",
    "https://openalex.org/W4392791588",
    "https://openalex.org/W4366490383",
    "https://openalex.org/W4387669006",
    "https://openalex.org/W4387106968",
    "https://openalex.org/W4404404328",
    "https://openalex.org/W4403885547",
    "https://openalex.org/W4396784687",
    "https://openalex.org/W4392781833",
    "https://openalex.org/W4391508636",
    "https://openalex.org/W4400734098",
    "https://openalex.org/W4390784029",
    "https://openalex.org/W4379258778",
    "https://openalex.org/W4396676567",
    "https://openalex.org/W4386812123",
    "https://openalex.org/W4392223386",
    "https://openalex.org/W4405623605",
    "https://openalex.org/W4400414272",
    "https://openalex.org/W4399702760",
    "https://openalex.org/W4367051211",
    "https://openalex.org/W4403515212",
    "https://openalex.org/W4376866708",
    "https://openalex.org/W4392974967",
    "https://openalex.org/W4387241792",
    "https://openalex.org/W4390733549",
    "https://openalex.org/W2890758909",
    "https://openalex.org/W4367186868",
    "https://openalex.org/W4394967854",
    "https://openalex.org/W4391403616"
  ],
  "abstract": "The peer review process ensures the integrity of scientific research. This is particularly important in the medical field, where research findings directly impact patient care. However, the rapid growth of publications has strained reviewers, causing delays and potential declines in quality. Generative artificial intelligence, especially large language models (LLMs) such as ChatGPT, may assist researchers with efficient, high-quality reviews. This review explores the integration of LLMs into peer review, highlighting their strengths in linguistic tasks and challenges in assessing scientific validity, particularly in clinical medicine. Key points for integration include initial screening, reviewer matching, feedback support, and language review. However, implementing LLMs for these purposes will necessitate addressing biases, privacy concerns, and data confidentiality. We recommend using LLMs as complementary tools under clear guidelines to support, not replace, human expertise in maintaining rigorous peer review standards.",
  "full_text": "www.jeehp.org 1(page number not for citation purposes)\nJournal of Educational Evaluation\nfor Health ProfessionsJ Educ Eval Health Prof 2025;22:4 • https://doi.org/10.3352/jeehp.2025.22.4\neISSN: 1975-5937\nOpen Access\nEducational/Faculty development material\n2025 Korea Health Personnel Licensing Examination Institute \nThis is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, \nand reproduction in any medium, provided the original work is properly cited.\nThe peer review process ensures the integrity of scientific research. This is particularly important in the medical field, where research findings directly impact pa-\ntient care. However, the rapid growth of publications has strained reviewers, causing delays and potential declines in quality. Generative artificial intelligence, es-\npecially large language models (LLMs) such as ChatGPT , may assist researchers with efficient, high-quality reviews. This review explores the integration of \nLLMs into peer review, highlighting their strengths in linguistic tasks and challenges in assessing scientific validity, particularly in clinical medicine. Key points \nfor integration include initial screening, reviewer matching, feedback support, and language review. However, implementing LLMs for these purposes will ne-\ncessitate addressing biases, privacy concerns, and data confidentiality. We recommend using LLMs as complementary tools under clear guidelines to support, \nnot replace, human expertise in maintaining rigorous peer review standards.\nKeywords: Peer review; Large language models; Generative artificial intelligence; ChatGPT , Republic of Korea\nThe role of large language models in the peer-review process: \nopportunities and challenges for medical journal reviewers and \neditors\nJisoo Lee1, Jieun Lee2, Jeong-Ju Yoo2*\n1Department of Internal Medicine, Soonchunhyang University Bucheon Hospital, Bucheon, Korea\n2Division of Gastroenterology and Hepatology, Department of Internal Medicine, Soonchunhyang University Bucheon Hospital, Bucheon, Korea\n*Corresponding email: \npuby17@naver.com\nEditor: Sun Huh, Hallym University, Korea\nReceived: December 6, 2024\nAccepted: January 2, 2025\nPublished: January 16, 2025\nThis article is available from: \nhttp://jeehp.org\nThe role of large language models in the peer-review process: \nopportunities and challenges for medical journal reviewers and editors\nPerformance of LLM Potential integration points in peer review\nConclusion We recommend using large language model as a complementary tool \nto support human expertise in peer-review process.\nLLM struggles understanding \nclinical medicine papers.\nReviewer \nmatching\nHelping with \nstructured feedback\nGrammar and \nformat checking\nManuscript\nsubmitted\nJournal editor \nassess\nReviewers \nassess\nFinal accept \nand publish\nInitial \nscreening\nAll Understand LLM capability and limitation\nReviewers\nEditors\nDo not over-rely on LLM, focus on advanced aspects of peer review\nMake proper guidelines, continuously evaluate the impact of LLM usage\nRecommendations\n\n(page number not for citation purposes)\nJ Educ Eval Health Prof 2025;22:4 • https://doi.org/10.3352/jeehp.2025.22.4\nwww.jeehp.org 2\nIntroduction\nBackground/rationale\nEvolution and current challenges of peer review\nPeer review is the process through which independent experts \nevaluate scientific work for quality, novelty, and originality. Peer \nreview began in 1665 at the Royal Society of London and has \nevolved over time to become the gold standard for scientific va-\nlidity and integrity [1]. This process is particularly important in \nthe medical field, where research findings can directly affect pa-\ntient care and treatment outcomes [2]. Despite its fundamental \nrole, the traditional peer review system has faced challenges due \nto its rather time-consuming nature, as reviewers have been \nswamped by the recent expansion of the research landscape in \nmedicine. The exponential increase in publications—up by 47% \nfrom 2016 to 2022—has outpaced the expansion of practicing \nscientists [3]. This imbalance has created a substantial burden \non the peer review system. According to a 2024 survey by Wiley, \n70% of researchers take at least 4 days to complete their peer re-\nview of an article, and 62% of respondents cited time constraints \nas their biggest challenge [4].\nThe advent of generative AI in academia\nSince the release of ChatGPT by OpenAI in November 2022, \ngenerative artificial intelligence (AI) has transformed numerous \naspects of daily life. Generative AI is now widely used in educa-\ntion, business, healthcare, and other fields, and academic writing \nis no exception. Researchers have increasingly utilized AI tools \nfor their scholarly work, such as literature reviews [5], data anal-\nysis [6], and manuscript writing [7].\nThis technological advancement may reduce the time and re-\nsources required for peer review and improve efficiency. How-\never, the academic community has raised concerns about the \nimpact of utilizing AI on scholarly integrity and review quality. \nGiven these considerations, a thorough analysis of integrating \ngenerative AI into the peer review process is necessary.\nObjectives\nThe objective of this review is to analyze large language models \n(LLMs) in the peer review landscape, including their application \nand performance in scientific publishing. We analyzed both the \nbenefits and challenges of integrating generative AI into the peer \nreview process, and suggested future directions.\nCurrent status of generative AI in peer \nreview\nPrevalence of applying generative AI in peer review\nVarious sources have demonstrated widespread adoption of \ngenerative AI in academic workflows. A survey of 3,838 postdoc-\ntoral researchers revealed that 31% of responders used generative \nAI primarily for text refinement (63%) [8]. According to a Nature \nsurvey of 1,659 researchers [9], 11% of responders considered ex-\npediting peer review as the primary benefit of generative AI, with \n24% expecting this advantage to become the main function of \ngenerative AI in the future. However, some research claims other-\nwise; a study by Ng et al. [10] in 2024 reported that 44% of re-\nsearchers believed that AI chatbots are not helpful for assisting \nwith peer review, and 67% had never used AI chatbots to critique \nthe works of other researchers.\nThese mixed survey results raise questions about the actual \napplication of generative AI in peer review. Despite various stud-\nies exploring the actual usage of generative AI in academic set-\ntings, the results vary widely due to hindrances in differentiating \nLLM-generated reviews. Detecting LLM-generated reviews is \ndifficult due to 2 main obstacles: (1) authors often do not dis -\nclose their use of AI tools, and (2) current detection methods, \neven by human reviewers, are still unreliable due to high false \npositive or negative rates [11]. Nevertheless, recent studies have \nprovided empirical evidence on the extent of AI use in peer re-\nview. A commercial LLM detection service estimated that at \nleast 15.8% of reviews for an AI conference were written with AI \nassistance [12]. Liang et al. [13] developed a corpus-level detec-\ntion model based on word distribution patterns and demon -\nstrated LLM modification rates of 6.5% to 16.9% in AI confer-\nence peer reviews. In contrast, a similar analysis of journals in \nthe Nature journals showed no significant evidence of LLM-\nbased modifications, suggesting that the use of AI tools varies \nwidely across academic disciplines.\nCurrent performance of LLMs in the peer review process\nRecent studies evaluating LLM capabilities in peer review \ntasks have shown mixed results across different assessment \nmethodologies. These assessment approaches can be broadly \ncategorized into 2 main types. The first approach focuses on re-\nview generation tasks, and the quality of these LLM-generated \nreviews is evaluated through various metrics including similarity \nto human reviews, specificity of feedback, and overall helpful -\nness. The second approach evaluates the error detection capa-\nbilities of LLM by artificially introducing errors into manu -\nscripts and assessing its ability to identify and critique these \n(page number not for citation purposes)\nJ Educ Eval Health Prof 2025;22:4 • https://doi.org/10.3352/jeehp.2025.22.4\nwww.jeehp.org 3\nflaws. This method provides a more controlled environment for \nevaluating its critical analysis capabilities in the context of peer \nreview.\nPerformance in review generation tasks\nIn the medical field, Saad et al. [14] conducted a comparative \nanalysis of reviews generated by GPT-3.5, GPT-4, and human re-\nviewers for 21 medical manuscripts. The study revealed a limited \ncorrelation between AI-generated reviews and human reviews, \nand AI reviews showed a minimal association with the final accep-\ntance rate. These findings suggest that LLMs face significant lim-\nitations in effectively evaluating medical research papers.\nIn contrast, Liang et al. [15] reported more encouraging results \nfrom a large-scale study involving Nature portfolio journals and AI \nconference papers. They found that the overlap between \nGPT-4-generated reviews and human reviews was comparable to \nthe overlap between 2 human reviewers. Amongst the 308 re -\nsearchers in the study, 57.4% found feedback by GPT-4 helpful, \nand 82.4% rated its performance superior to at least some human \nreviewers. D’ Arcy et al. [16] proposed a multi-agent system ap-\nproach, in which multiple AI agents with specialized roles collabo-\nrate to generate reviews, similar to the panel of human reviewers \nin traditional peer review systems.\nCompared to the single-agent approach, the multi-agent meth-\nod received high ratings for specificity (70.8% versus 40.0%) and \noverall (21.4% versus 8.6%) by human reviewers. Nonetheless, \n38% to 48% of all comments generated by AI agents were rated as \nhighly inaccurate, demonstrating the limitation of LLMs in peer \nreview tasks.\nError detection capabilities\nStudies evaluating the ability of LLMs to detect intentionally \nembedded errors have exhibited varying results. Liu and Shah \n[17] found that GPT-4 could identify 7 out of 13 intentionally in-\nserted errors in research papers, performing comparably to human \nreviewers. However, Kadi and Aslaner [18] reported that GPT-4 \nstruggled to detect major issues, such as mismatches between ti-\ntles and contents, and disproportionately focused on minor typo-\ngraphical errors in 15 short medical articles.\nLLMs demonstrated stronger capabilities in detecting lan -\nguage-related errors. Lechien et al. [19] focused specifically on \nlinguistic accuracy, testing the ability of GPT-4 to review papers \nwritten by non-native English-speaking otolaryngologists. The re-\nsults were favorable, with GPT-4 successfully identifying 83.7% of \ngrammatical errors. These results suggest that LLMs could be \nparticularly useful for improving the linguistic quality of academic \nmanuscripts, especially for non-native English speakers.\nFactors affecting LLM performance\nSeveral factors including drastic improvements in LLM tech-\nnology, variations in prompting strategies, and discipline-specific \nlimitations affect LLM performance and evaluation. Thanks to \nthe rapid evolution and continual changes in LLM technology, \nthe timing of studies vastly impacts study outcomes. For instance, \nthere is a significant difference in performance between GPT-3.5 \nand GPT-4, which were released within a period of 1 or 2 years. \nThese rapid and unpredictable shifts in LLM capabilities prevent \nresearchers from drawing definitive conclusions on the effective-\nness of LLMs in peer review.\nVariation in prompting strategies across studies emerges as an-\nother crucial factor affecting LLM performance. For instance, \nwhile Saad et al. [14] used simple prompts requesting 3 advantag-\nes and disadvantages for each manuscript, other studies adopted \nmore sophisticated methods, such as adopting specific reviewer \npersonas or structured evaluation frameworks. Santu et al. [20] \nsystematically investigated this effect in their meta-review genera-\ntion experiment, comparing 4 levels of prompt complexity. Per-\nformance improved significantly from basic (level 1) to moderate-\nly-structured prompts (level 2), but high complexity prompts \n(levels 3 and 4) showed diminishing returns. This suggests the \nneed for an optimal balance in prompt design for peer review.\nInterestingly, LLMs consistently underperform in evaluating \nmedical research papers compared to other fields. This trend is \nsupported by the analysis of Thelwall [21] of 34 academic disci-\nplines, which revealed that clinical medicine was the only field \nwhere the quality scores of GPT-4o mini displayed a negative cor-\nrelation with the actual paper quality. Among the three medical \nstudies mentioned earlier [14,18,19], all except one study[19]—\nfocusing on language correction—reported that LLM perfor -\nmance was generally unsatisfactory. focusing on language correc-\ntion. This distinctive pattern in clinical medicine may arise from \nseveral factors. First, medical papers often use a characteristically \ncautious tone due to potential health impacts. Also, most clinical \nmedicine studies rely heavily on statistical results and precise nu-\nmerical data, which LLMs struggle to interpret effectively. These \ndomain-specific challenges suggest that applying LLMs in medi-\ncal peer review may require specialized approaches or additional \nsafeguards, unlike in other academic disciplines.\nImplications\nThorough examinations of LLM capabilities have allowed re-\nsearchers to understand the supplementary role of LLM in scien-\ntific writing. LLMs demonstrate considerable proficiency in lan-\nguage-related tasks, such as identifying grammatical errors, gener-\nating structured feedback, and detecting linguistic inconsistencies.\n(page number not for citation purposes)\nJ Educ Eval Health Prof 2025;22:4 • https://doi.org/10.3352/jeehp.2025.22.4\nwww.jeehp.org 4\nHowever, their strong focus on linguistic elements presents an \nunexpected limitation: their increased sensitivity to language pat-\nterns may hinder their evaluation of fundamental scientific analy-\nses in research publications. This bias toward linguistic presenta-\ntion can result in the misinterpretation of cautiously worded state-\nments, especially in fields like clinical medicine. LLMs may strug-\ngle to differentiate between appropriately cautious writing and a \nlack of research confidence, which may compromise their efficacy \nin assessing such articles.\nThese findings suggest that LLMs are best positioned as com-\nplementary tools rather than automatic, standalone reviewers in \nthe peer review process. Their strengths in language-related tasks \nare valuable for initial screening and basic feedback, but their lim-\nitations in assessing scientific validity beyond linguistic presenta-\ntion demand ongoing human supervision. This is especially cru-\ncial in clinical medicine, in which interpreting statistical data and \nevaluating clinical significance require deep domain expertise that \ncurrent LLMs have not yet mastered.\nIntegrating LLMs into the peer review process\nThe traditional peer review process is resource-intensive and \ntime-consuming, presenting opportunities to improve efficiency \nand accuracy. Fig. 1 illustrates potential points where LLMs can \nsignificantly improve efficiency and accuracy.\nInitial screening\nInitial manuscript screening, a critical but time-consuming step \nin the editorial process, involves evaluating submissions for scope \nalignment, quality standards, and technical requirements. Apply-\ning LLMs in screening manuscripts by titles and abstracts has \nbeen discussed mainly in the context of systematic reviewing and \nguideline writing. A recent study using GPT-4 Turbo demonstrat-\ned high accuracy in the evaluation of titles and abstracts when em-\nploying specific prompt strategies [ 22]. Dennstädt et al. [ 23] \nevaluated various LLM models, reporting sensitivity rates from \n81.93% to 97.58% and specificity of over 99.9%. Journal editors \nmay utilize this technology to effectively screen manuscripts that \ncorrespond with the aim of their journals, thereby potentially de-\ncreasing the administrative burden associated with initial manu-\nscript screening. Efficient pre-screening with LLMs enables re-\nviewers to focus only on relevant manuscripts during the match-\ning stage, optimizing the use of editorial resources.\nReviewer matching\nReviewer matching is another potential application of LLMs in \nthe peer review process. Identifying appropriate reviewers is a \ncomplex task for journal editors, involving multiple considerations \nFig. 1. Peer review process with large language model (LLM) integration. Conventional peer review process and the potential LLM integration points (highlighted in \npurple).\nInitial screening\n• Verifying checklist\n•  Screening relevance to journal’s aim \nand scope\nAuthor submits \nrevised manuscript\nMajor/minor \nrevision\nHelping structured \nfeedback\nReviewer matching \nassistance\nManuscript \nrejected\nAuthor submits \nmanuscript\nJournal \neditor assess \nthe reviews \nmanuscript\nJournal editor \nassess the reviews\nReviewers assess \nthe manuscript Accepted Publication\nPotential LLM assistance point\nJournal editor step\nAuthor step\nReviewer step\nGrammar check\nFormatting correction\nManuscript sent to \nreviewers\n(page number not for citation purposes)\nJ Educ Eval Health Prof 2025;22:4 • https://doi.org/10.3352/jeehp.2025.22.4\nwww.jeehp.org 5\nsuch as aligning manuscript topics with reviewer expertise, screen-\ning for conflicts of interest, confirming availability, and ensuring \ndiverse academic perspectives. By employing machine learning al-\ngorithms, LLMs can analyze patterns of reviewer preferences and \npast performances, optimizing the selection process and broaden-\ning the pool of available reviewers [24]. Farber [25] reported a \n42% overlap between GPT-4-suggested reviewers and those man-\nually selected by editors and 37% additional qualified reviewers \nidentified by GPT-4, who were not initially considered. Notably, \nGPT-4 reduced reviewer selection time by 73%, from 45 to 12 \nminutes, demonstrating its potential to improve efficiency and \nidentify experts across diverse fields.\nHelping with structured feedback\nThe current peer review system heavily relies on the subjective \njudgment of reviewers, hindering consistency and objectivity in \nreview outcomes. With the rapid increase in academic submis-\nsions, reviewers often struggle to dedicate sufficient time to each \nreview; sometimes, time pressure and anonymity have even led to \nunconstructive or aggressive comments. Once integrated into the \nreview workflow, LLMs can efficiently assist in conducting struc-\ntured, refined reviews. LLMs can help human reviewers by ana-\nlyzing manuscripts against predefined evaluation criteria, ensuring \nthat critical aspects are not overlooked. Additionally, LLMs can \nrefine the tone of review reports, facilitating more constructive \nand less aggressive comments [26].\nGrammar and format checking\nLLMs are highly effective tools for formatting and grammar \ncorrection. Even skeptics acknowledge the linguistic strengths of \nLLMs [18,27]. This is especially beneficial for non-native En -\nglish-speaking researchers, enabling proficient writing and reduc-\ning linguistic bias in academic publishing [28,29]. By improving \nthe linguistic quality of academic papers, LLMs may serve as valu-\nable supplementary tools that improve the overall clarity and ac-\ncessibility of scholarly communication.\nKey challenges\nWhile LLMs can improve efficiency in tasks such as initial \nscreening and structured feedback, it is essential to address a \nbroad range of ethical and practical considerations—including \npotential language biases, as well as privacy and confidentiality \nconcerns—in order to ensure the fairness and integrity of scien-\ntific publishing.\nBias\nBias in LLM-assisted peer review represents a complex chal-\nlenge that requires careful consideration and systematic manage-\nment. Although traditional peer review already exhibits biases, \nsuch as geographical disparities in reviewer selection and lan -\nguage-based discrimination [30], the integration of LLMs may \nadd further complexities to these challenges.\nThe primary challenge comes from the biases in data used in \nLLM training. Since these models are mainly trained on En -\nglish-language academic texts from leading institutions, LLMs \nrisk prioritizing dominant academic perspectives while underrep-\nresenting research from non-English-speaking regions. However, \nafter careful implementation, LLMs have the potential to mitigate \nsome human biases, particularly those related to language. A Eu-\nropean Research Council survey found that 75% of surveyees an-\nticipated that generative AI could reduce language barriers in re-\nsearch by 2030. Beyond language, standardized LLM-assisted re-\nview protocols may also minimize other biases, such as preferenc-\nes for institutional prestige, nationalities of authors or specific \nmethodologies, promoting fairness and inclusivity in the peer-re-\nview process.\nPrivacy and confidentiality\nPrivacy and confidentiality concerns are critical challenges in \nintegrating LLMs into peer review processes, particularly in medi-\ncal publishing. There are 3 key concerns: pre-publication data \nprotection, model data retention, and healthcare-specific compli-\nance requirements.\nThe primary risk involves the exposure of unpublished research \ndata during the review process. Manuscripts processed through \nLLM systems are vulnerable to premature data disclosure, which \ncould compromise the integrity of the blind review process and \nviolate publication embargoes. Beyond immediate risks, LLMs \nalso retain processed information in their training datasets, creat-\ning long-term vulnerabilities related to data security and the pro-\ntection of intellectual property. Such risks for privacy breaches are \nparticularly severe in medical manuscript reviews, in which manu-\nscripts frequently contain sensitive patient information and clini-\ncal trial data. Many third-party LLM services rely on cloud-based \nprocessing systems with varying security protocols and data han-\ndling policies [31]. These vulnerabilities lead to additional com-\nplexity in maintaining compliance with healthcare data protection \nrequirements [32].\nRecommendations\nFor the academic community\nPeer reviewers and editors need to have a clear understanding \nof LLM usage. Fears of LLMs, sometimes reflecting overestima-\n(page number not for citation purposes)\nJ Educ Eval Health Prof 2025;22:4 • https://doi.org/10.3352/jeehp.2025.22.4\nwww.jeehp.org 6\ntions or underestimations, often stem from a lack of knowledge \nabout their capabilities and limitations. Stakeholders must culti-\nvate an accurate understanding of the capabilities and limitations \nof LLMs through regular workshops and accessible educational \nresources. Although a comprehensive understanding may be chal-\nlenging due to the rapid development of LLMs, ongoing efforts \nmust be made to keep current with their latest updates. As previ-\nously discussed, particular caution must be taken to maintain data \nsecurity and confidentiality while managing sensitive research \ndata or clinical information in LLMs.\nFor reviewers\nReviewers can utilize LLMs to optimize basic review tasks, such \nas grammar and format checking, allowing reviewers to focus on \nadvanced aspects of peer review. These include critically analyzing \nthe overall significance, novelty and impact of the research, distin-\nguishing subtle differences in methodologies or findings, and pro-\nviding field-specific insights.\nReviewers must use LLMs responsibly and avoid over-reliance. \nAnalyses or suggestions provided by LLMs should be critically as-\nsessed, especially those pertaining to statistical analyses or com-\nplex methodologies. Human reviewers should treat LLMs solely \nas tools, remaining accountable for their proper use by carefully \nverifying the accuracy and reliability of LLM outputs and report-\ning any potential issues to the editor.\nFor editors\nEditors should establish clear guidelines for LLM usage to en-\nsure quality and integrity enhancement—not impairment—of \nacademic communication. While many journal-specific guide-\nlines had rarely addressed generative AI by early 2023—or mainly \nfocused on writers, had they addressed generative AI—there has \nlately been a notable progress. A study by Ganjavi et al. [ 33] \nshowed a 25% increase in generative AI-related guidelines among \nthe top 100 medical journals between March and October 2023, \nwith growing attention to reviewer usage. However, many non-\ntop-tier journals still lack relevant guidelines [27]. Additionally, \nthere is a considerable variation in guidelines across journals and \npublishers and in terms such as generative AI, LLMs, and AI, \nwhich are often used inconsistently. Editors must address these is-\nsues by providing reviewers with clear guidance.\nIt is also essential to continuously evaluate and adjust the im-\npact of LLM usage in the peer review process. Over-reliance on \nLLMs in reviews may urge researchers to shape their work based \non LLM-based evaluation criteria, introducing potential biases \nthat need to be guarded against. Researchers must be encouraged \nto transparently disclose, rather than conceal, their use of LLMs.\nConclusion\nWe identified 4 potential integration points for LLMs in the \npeer review process: initial screening, reviewer matching, struc-\ntured feedback assistance, and grammar and format checking. \nWhile challenges such as bias, privacy, and security in regard to \nunpublished data must be addressed, LLMs hold great potential \nto complement human expertise and enhance the efficiency, equi-\nty, and inclusivity of peer review. The effective and ethical applica-\ntion of LLMs relies on not only the technology itself, but also the \nexpertise, critical judgment, and caution of researchers. Therefore, \nLLMs should be viewed as complementary tools, not as replace-\nments for human expertise. T o manage LLMs responsibly and \ntransparently, it is essential for all members of the academic com-\nmunity to engage openly in discussions, share experiences, and \ncollectively develop best practices for implementing LLMs in sci-\nentific writing. Editors and reviewers should establish clear guide-\nlines for AI use, ensure transparency and focus human resources \non complex tasks that require critical analysis and specialized \nknowledge.\nORCID\nJisoo Lee: https://orcid.org/0009-0000-6246-8336; Jieun Lee: \nhttps://orcid.org/0000-0001-9494-2493; Jeong-Ju Y oo: https://\norcid.org/0000-0002-7802-0381\nAuthors’ contributions\nConceptualization: JJY. Data curation: JL ( Jisoo Lee). Method-\nology/formal analysis/validation: JL ( Jisoo Lee), JJY. Project ad-\nministration: JJY. Funding acquisition: JJY. Writing–original draft: \nJL ( Jisoo Lee). Writing–review & editing: JL ( Jisoo Lee), JL \n( Jieun Lee), JJY.\nConflict of interest\nNo potential conflict of interest relevant to this article was re-\nported.\nFunding\nThis study was supported by Soonchunhyang University Re-\nsearch Fund (Fundref ID: 10.13039/501100002560). The \nfunders had no role in study design, data collection and analysis, \ndecision to publish, or preparation of the manuscript.\n(page number not for citation purposes)\nJ Educ Eval Health Prof 2025;22:4 • https://doi.org/10.3352/jeehp.2025.22.4\nwww.jeehp.org 7\nData availability\nNot applicable.\nAcknowledgments\nNone.\nSupplementary materials\nSupplement 1. Audio recording of the abstract.\nReferences\n1. Spier R. The history of the peer-review process. T rends Bio-\ntechnol 2002;20:357-358. https://doi.org/10.1016/s0167-\n7799(02)01985-6 \n2. Kharasch ED, Avram MJ, Clark JD, Davidson AJ, Houle TT , \nLevy JH, London MJ, Sessler DI, Vutskits L. Peer review matters: \nresearch quality and the public trust. Anesthesiology 2021;134:1-\n6. https://doi.org/10.1097/ALN.0000000000003608 \n3. Hanson MA, Barreiro PG, Crosetto P , Brockington D. The \nstrain on scientific publishing. Quant Sci Stud 2024;5:823-843. \nhttps://doi.org/10.1162/qss_a_00327 \n4. Navigating the Peer Review Landscape [Internet]. Wiley; 2024 \n[cited 2024 Dec 5]. Available from: https://www.wiley.com/en-\nus/network/publishing/research-publishing/editors/navigat-\ning-the-peer-review-landscape \n5. Khraisha Q, Put S, Kappenberg J, Warraitch A, Hadfield K. Can \nlarge language models replace humans in systematic reviews?: \nevaluating GPT-4’s efficacy in screening and extracting data \nfrom peer-reviewed and grey literature in multiple languages. \nRes Synth Methods 2024;15:616-626. https://doi.org/10.10 \n02/jrsm.1715 \n6. Ravichandran P , Machireddy JR, Rachakatla SK. Data analytics \nautomation with AI: a comparative study of traditional and gen-\nerative AI approaches. J Bioinform Artif Intell 2023;3:168-190. \n7. Altmae S, Sola-Leyva A, Salumets A. Artificial intelligence in \nscientific writing: a friend or a foe? Reprod Biomed Online \n2023;47:3-9. https://doi.org/10.1016/j.rbmo.2023.04.009 \n8. Nordling L. How ChatGPT is transforming the postdoc experi-\nence. Nature 2023;622:655-657. https://doi.org/10.1038/\nd41586-023-03235-8 \n9. Van Noorden R, Perkel JM. AI and science: what 1,600 re-\nsearchers think. Nature 2023;621:672-675. https://doi.org/ \n10.1038/d41586-023-02980-0 \n10. Ng JY, Maduranayagam SG, Suthakar N, Li A, Lokker C, Iorio \nA, Haynes RB, Moher D. Attitudes and perceptions of medical \nresearchers towards the use of artificial intelligence chatbots in \nthe scientific process: an international cross-sectional survey. \nLancet Digit Health 2025;7:e94-e102. https://doi.org/10. \n1016/S2589-7500(24)00202-4 \n11. Yu S, Luo M, Madasu A, Lal V , Howard P . Is your paper being \nreviewed by an LLM?: investigating AI text detectability in peer \nreview. arXiv [Preprint] 2024 Oct 3. https://doi.org/10.48550/\narXiv.2410.03019 \n12. Latona GR, Ribeiro MH, Davidson TR, Veselovsky V , West R. \nThe AI Review Lottery: widespread AI-assisted peer reviews \nboost paper scores and acceptance rates. arXiv [Preprint] 2024 \nMay 3. https://doi.org/10.48550/arXiv.2405.02150 \n13. Liang W, Izzo Z, Zhang Y, Lepp H, Cao H, Zhao X, Chen L, Y e \nH, Liu S, Huang Z, McFarland DA. Monitoring ai-modified \ncontent at scale: a case study on the impact of ChatGPT on AI \nconference peer reviews. arXiv [Preprint] 2024 Mar 11. https://\ndoi.org/10.48550/arXiv.2403.07183 \n14. Saad A, Jenko N, Ariyaratne S, Birch N, Iyengar KP , Davies AM, \nVaishya R, Botchu R. Exploring the potential of ChatGPT in \nthe peer review process: an observational study. Diabetes Metab \nSyndr 2024;18:102946. https://doi.org/10.1016/j.dsx.2024. \n102946 \n15. Liang W, Zhang Y, Cao H, Wang B, Ding DY, Yang X, Vodrahalli \nK, He S, Smith DS, Yin Y, McFarland DA. Can large language \nmodels provide useful feedback on research papers?: a large-\nscale empirical analysis. NEJM AI 2024;1:AIoa2400196. \nhttps://doi.org/10.1056/AIoa2400196 \n16. D’ Arcy M, Hope T , Birnbaum L, Downey D. Marg: Multi-agent \nreview generation for scientific papers. arXiv [Preprint] 2024 \nJan 8. https://doi.org/10.48550/arXiv.2401.04259 \n17. Liu R, Shah NB. ReviewerGPT?: an exploratory study on using \nlarge language models for paper reviewing. arXiv [Preprint] \n2023 Jun 1. https://doi.org/10.48550/arXiv.2306.00622 \n18. Kadi G, Aslaner MA. Exploring ChatGPT’s abilities in medical \narticle writing and peer review. Croat Med J 2024;65:93-100. \nhttps://doi.org/10.3325/cmj.2024.65.93 \n19. Lechien JR, Gorton A, Robertson J, Vaira LA. Is ChatGPT-4 \naccurate in proofread a manuscript in otolaryngology-head and \nneck surgery? Otolaryngol Head Neck Surg 2024;170:1527-\n1530. https://doi.org/10.1002/ohn.526 \n20. Santu SK, Sinha SK, Bansal N, Knipper A, Sarkar S, Salvador J, \nMahajan Y, Guttikonda S, Akter M, Freestone M, Williams Jr \nMC. Prompting LLMs to compose meta-review drafts from \npeer-review narratives of scholarly manuscripts. arXiv [Preprint] \n2024 Feb 23. https://doi.org/10.48550/arXiv.2402.15589 \n21. Thelwall M. Evaluating research quality with large language \n(page number not for citation purposes)\nJ Educ Eval Health Prof 2025;22:4 • https://doi.org/10.3352/jeehp.2025.22.4\nwww.jeehp.org 8\nmodels: an analysis of ChatGPT’s effectiveness with different \nsettings and inputs. J Data Inf Sci 2025;10:1-19. https://doi.\norg/10.2478/jdis-2025-0011 \n22. Oami T , Okada Y, Nakada TA. Performance of a large language \nmodel in screening citations. JAMA Netw Open 2024;7:e2420 \n496. https://doi.org/10.1001/jamanetworkopen.2024.20496 \n23. Dennstadt F, Zink J, Putora PM, Hastings J, Cihoric N. Title and \nabstract screening for literature reviews using large language mod-\nels: an exploratory study in the biomedical domain. Syst Rev \n2024;13:158. https://doi.org/10.1186/s13643-024-02575-4 \n24. Strolger LG, Pegues J, King T , Miles N, Ramsahoye M, Ceruti II \nK, Blacker B, Reid IN. PACMan2: next steps in proposal review \nmanagement. Astron J 2023;165:215. https://doi.org/10.3847/ \n1538-3881/acc2c4 \n25. Farber S. Enhancing peer review efficiency: a mixed‐methods \nanalysis of artificial intelligence‐assisted reviewer selection \nacross academic disciplines. Learn Publ 2024;37:e1638. \nhttps://doi.org/10.1002/leap.1638 \n26. Hosseini M, Horbach SP . Fighting reviewer fatigue or amplify-\ning bias?: considerations and recommendations for use of \nChatGPT and other large language models in scholarly peer re-\nview. Res Integr Peer Rev 2023;8:4. https://doi.org/10.1186/\ns41073-023-00133-5 \n27. Yin S, Lu P , Xu Z, Lian Z, Y e C, Li C. A systematic examination \nof generative artificial intelligence (GAI) usage guidelines for \nscholarly publishing in medical journals. medRxiv [Preprint] \n2024 Mar 19. https://doi.org/10.1101/2024.03.19.24304550 \n28. Biswas S, Dobaria D, Cohen HL. ChatGPT and the future of \njournal reviews: a feasibility study. Yale J Biol Med 2023;96: \n415-420. https://doi.org/10.59249/SKDH9286 \n29. Mollaki V . Death of a reviewer or death of peer review integrity? \nthe challenges of using AI tools in peer reviewing and the need \nto go beyond publishing policies. Res Ethics 2024;20:239-250. \nhttps://doi.org/10.1177/17470161231224552 \n30. Vesper I. Peer reviewers unmasked: largest global survey reveals \ntrends [Internet]. Nature; 2018 [cited 2024 Dec 5]. Available \nfrom: https://doi.org/10.1038/d41586-018-06602-y \n31. Li H, Moon JT , Purkayastha S, Celi LA, T rivedi H, Gichoya JW. \nEthics of large language models in medicine and medical re-\nsearch. Lancet Digit Health 2023;5:e333-e335. https://doi.\norg/10.1016/S2589-7500(23)00083-3 \n32. Denecke K, May R; LLMHealthGroup; Rivera Romero O. Po-\ntential of large language models in health care: Delphi study. J \nMed Internet Res 2024;26:e52399. https://doi.org/10.2196/ \n52399 \n33. Ganjavi C, Eppler MB, Pekcan A, Biedermann B, Abreu A, Col-\nlins GS, Gill IS, Cacciamani GE. Publishers’ and journals’ in-\nstructions to authors on use of generative artificial intelligence \nin academic and scientific publishing: bibliometric analysis. \nBMJ 2024;384:e077192. https://doi.org/10.1136/bmj-2023-\n077192 ",
  "topic": "Confidentiality",
  "concepts": [
    {
      "name": "Confidentiality",
      "score": 0.6026073098182678
    },
    {
      "name": "Process (computing)",
      "score": 0.5647947788238525
    },
    {
      "name": "Computer science",
      "score": 0.5074576735496521
    },
    {
      "name": "Peer review",
      "score": 0.5015459060668945
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4935086965560913
    },
    {
      "name": "Matching (statistics)",
      "score": 0.4207966923713684
    },
    {
      "name": "Field (mathematics)",
      "score": 0.41604065895080566
    },
    {
      "name": "Engineering ethics",
      "score": 0.3417931795120239
    },
    {
      "name": "Data science",
      "score": 0.3250967264175415
    },
    {
      "name": "Medicine",
      "score": 0.21851789951324463
    },
    {
      "name": "Political science",
      "score": 0.12564924359321594
    },
    {
      "name": "Pathology",
      "score": 0.11778849363327026
    },
    {
      "name": "Computer security",
      "score": 0.08915203809738159
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63562232",
      "name": "Bucheon University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I24541011",
      "name": "Soonchunhyang University",
      "country": "KR"
    }
  ],
  "cited_by": 4
}