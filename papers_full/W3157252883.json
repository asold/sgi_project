{
    "title": "AMMU: A survey of transformer-based biomedical pretrained language models",
    "url": "https://openalex.org/W3157252883",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2918889124",
            "name": "Katikapalli Subramanyam Kalyan",
            "affiliations": [
                "National Institute of Technology Tiruchirappalli"
            ]
        },
        {
            "id": "https://openalex.org/A2007694338",
            "name": "Ajit Rajasekharan",
            "affiliations": [
                "Nference (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A4221384454",
            "name": "Sivanesan Sangeetha",
            "affiliations": [
                "National Institute of Technology Tiruchirappalli"
            ]
        },
        {
            "id": "https://openalex.org/A2918889124",
            "name": "Katikapalli Subramanyam Kalyan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2007694338",
            "name": "Ajit Rajasekharan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221384454",
            "name": "Sivanesan Sangeetha",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6600424091",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W2165698076",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W6600299915",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W6774957663",
        "https://openalex.org/W4220967417",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W3099750501",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W3093814160",
        "https://openalex.org/W3024922541",
        "https://openalex.org/W3112340563",
        "https://openalex.org/W2982424689",
        "https://openalex.org/W2963923670",
        "https://openalex.org/W6604009900",
        "https://openalex.org/W6765671411",
        "https://openalex.org/W3115462295",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W3160137267",
        "https://openalex.org/W3089168780",
        "https://openalex.org/W3035725276",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W6780218876",
        "https://openalex.org/W3095092693",
        "https://openalex.org/W2955483668",
        "https://openalex.org/W3041263301",
        "https://openalex.org/W3100452049",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W3116423158",
        "https://openalex.org/W3172427031",
        "https://openalex.org/W6600384961",
        "https://openalex.org/W6601454296",
        "https://openalex.org/W6607919353",
        "https://openalex.org/W3153239180",
        "https://openalex.org/W3153955816",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W2970368801",
        "https://openalex.org/W3095642204",
        "https://openalex.org/W3098469895",
        "https://openalex.org/W3013838212",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2888120268",
        "https://openalex.org/W3157876196",
        "https://openalex.org/W3129160532",
        "https://openalex.org/W6628189516",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W6742058293",
        "https://openalex.org/W6600182862",
        "https://openalex.org/W3193158708",
        "https://openalex.org/W3109919947",
        "https://openalex.org/W3094834348",
        "https://openalex.org/W3037063616",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3016187590",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W6763701032",
        "https://openalex.org/W2125674401",
        "https://openalex.org/W2004910511",
        "https://openalex.org/W2002514548",
        "https://openalex.org/W6604735148",
        "https://openalex.org/W2046788142",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W6603795979",
        "https://openalex.org/W1869282115",
        "https://openalex.org/W3045332379",
        "https://openalex.org/W3104186312",
        "https://openalex.org/W3105801792",
        "https://openalex.org/W3015347994",
        "https://openalex.org/W3087623576",
        "https://openalex.org/W3101058639",
        "https://openalex.org/W3106298421",
        "https://openalex.org/W3104578551",
        "https://openalex.org/W3116099796",
        "https://openalex.org/W3166508187",
        "https://openalex.org/W3179963059",
        "https://openalex.org/W3136657289",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W3033639018",
        "https://openalex.org/W3099035367",
        "https://openalex.org/W2417936835",
        "https://openalex.org/W1896888216",
        "https://openalex.org/W3023545062",
        "https://openalex.org/W2986154550",
        "https://openalex.org/W3032299950",
        "https://openalex.org/W3161430317",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3164540570",
        "https://openalex.org/W3166593409",
        "https://openalex.org/W3105601216",
        "https://openalex.org/W3099008231",
        "https://openalex.org/W3132130152",
        "https://openalex.org/W2914514892",
        "https://openalex.org/W2895083984",
        "https://openalex.org/W3011762034",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W6600159499",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3189279073",
        "https://openalex.org/W3211088500",
        "https://openalex.org/W3091383840",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W3103272618",
        "https://openalex.org/W2970771675",
        "https://openalex.org/W6684484140",
        "https://openalex.org/W3135084718",
        "https://openalex.org/W2131546905",
        "https://openalex.org/W6768382422",
        "https://openalex.org/W2145870108",
        "https://openalex.org/W6704842505",
        "https://openalex.org/W2169099542",
        "https://openalex.org/W3012137864",
        "https://openalex.org/W6768772580",
        "https://openalex.org/W3002784191",
        "https://openalex.org/W3114962450",
        "https://openalex.org/W3015281440",
        "https://openalex.org/W2810509939",
        "https://openalex.org/W2735784619",
        "https://openalex.org/W3037439385",
        "https://openalex.org/W3103720397",
        "https://openalex.org/W6678830454",
        "https://openalex.org/W2137407193",
        "https://openalex.org/W2166474856",
        "https://openalex.org/W2170189740",
        "https://openalex.org/W2052217781",
        "https://openalex.org/W6600745881",
        "https://openalex.org/W2989472987",
        "https://openalex.org/W2987875462",
        "https://openalex.org/W3012108798",
        "https://openalex.org/W3127429900",
        "https://openalex.org/W3122886537",
        "https://openalex.org/W4284698419",
        "https://openalex.org/W2972833053",
        "https://openalex.org/W6634681119",
        "https://openalex.org/W2891113091",
        "https://openalex.org/W2963506049",
        "https://openalex.org/W2970482702",
        "https://openalex.org/W3100468923",
        "https://openalex.org/W2913640436",
        "https://openalex.org/W6601771408",
        "https://openalex.org/W2062908157",
        "https://openalex.org/W2964315079",
        "https://openalex.org/W2914437773",
        "https://openalex.org/W2977683229",
        "https://openalex.org/W3028438438",
        "https://openalex.org/W3023360076",
        "https://openalex.org/W2963997607",
        "https://openalex.org/W3092008332",
        "https://openalex.org/W3034255912",
        "https://openalex.org/W2971296908",
        "https://openalex.org/W3084758521",
        "https://openalex.org/W2996851481",
        "https://openalex.org/W2947415936",
        "https://openalex.org/W3000344024",
        "https://openalex.org/W3181414820",
        "https://openalex.org/W4224281327",
        "https://openalex.org/W6602629387",
        "https://openalex.org/W3104453885",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W6600655081",
        "https://openalex.org/W3102483398",
        "https://openalex.org/W6826056734",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3156170450",
        "https://openalex.org/W3104163040",
        "https://openalex.org/W3174234060",
        "https://openalex.org/W6781275321",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W3125986339",
        "https://openalex.org/W3119997354",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W3038495045",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2974058876",
        "https://openalex.org/W3032020872",
        "https://openalex.org/W3101278968",
        "https://openalex.org/W2979977993",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2979694518",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W3097163126",
        "https://openalex.org/W3002832564",
        "https://openalex.org/W3030156796",
        "https://openalex.org/W3154872984",
        "https://openalex.org/W3204526376",
        "https://openalex.org/W3141797743",
        "https://openalex.org/W3119207684",
        "https://openalex.org/W3017549762",
        "https://openalex.org/W3114551148",
        "https://openalex.org/W3016422795",
        "https://openalex.org/W3152494620",
        "https://openalex.org/W3120094169",
        "https://openalex.org/W2346011863",
        "https://openalex.org/W3135158964",
        "https://openalex.org/W3017885892",
        "https://openalex.org/W2975357369",
        "https://openalex.org/W3088592174",
        "https://openalex.org/W3120737131",
        "https://openalex.org/W3032007299",
        "https://openalex.org/W3031200717",
        "https://openalex.org/W3173151551",
        "https://openalex.org/W3081505754",
        "https://openalex.org/W115293911",
        "https://openalex.org/W3099782249",
        "https://openalex.org/W2168041406",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3210120707",
        "https://openalex.org/W2120615054",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3096403953",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W3133650345",
        "https://openalex.org/W3188437627",
        "https://openalex.org/W3024622987",
        "https://openalex.org/W2917650994",
        "https://openalex.org/W2149369282",
        "https://openalex.org/W3047636089",
        "https://openalex.org/W3122382002",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W3027260829",
        "https://openalex.org/W3168090480",
        "https://openalex.org/W2979250794",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W3105867580",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W3117450517",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W3031593649",
        "https://openalex.org/W3011718307",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3190572136",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W3017637887",
        "https://openalex.org/W2898700502",
        "https://openalex.org/W2978268358",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W3106051020",
        "https://openalex.org/W2976111877",
        "https://openalex.org/W3171087246",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W3187418919",
        "https://openalex.org/W2963153906",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2047782770",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3126541881",
        "https://openalex.org/W2346452181"
    ],
    "abstract": null,
    "full_text": "1\nAMMU : A Survey of Transformer-based\nBiomedical Pretrained Language Models\nKatikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan Sangeetha\nAbstract—Transformer-based pretrained language models (PLMs) have started a new era in modern natural language processing\n(NLP). These models combine the power of transformers, transfer learning, and self-supervised learning (SSL). Following the success\nof these models in the general domain, the biomedical research community has developed various in-domain PLMs starting from\nBioBERT to the latest BioELECTRA and BioALBERT models. We strongly believe there is a need for a survey paper that can provide\na comprehensive survey of various transformer-based biomedical pretrained language models (BPLMs). In this survey, we start with a\nbrief overview of foundational concepts like self-supervised learning, embedding layer and transformer encoder layers. We discuss core\nconcepts of transformer-based PLMs like pretraining methods, pretraining tasks, ﬁne-tuning methods, and various embedding types\nspeciﬁc to biomedical domain. We introduce a taxonomy for transformer-based BPLMs and then discuss all the models. We discuss\nvarious challenges and present possible solutions. We conclude by highlighting some of the open issues which will drive the research\ncommunity to further improve transformer-based BPLMs. The list of all the publicly available transformer-based BPLMs along with their\nlinks is provided at https://mr-nlp.github.io/posts/2021/05/transformer-based-biomedical-pretrained-language-models-list/ .\nIndex Terms—Biomedical Pretrained Language Models, BioBERT, Survey, PubMedBERT, Transformers, Self-Supervised Learning\n!\nCONTENTS\n1 Introduction 2\n1.1 Literature Search and Selection . . . 3\n2 Foundations 5\n2.1 Embedding Layer . . . . . . . . . . . 5\n2.2 Transformer Encoder . . . . . . . . . 6\n2.2.1 Self-Attention (SA) . . . . 6\n2.2.2 Multi-Head Self Attention\n(MHSA) . . . . . . . . . . . 6\n2.2.3 Position-wise Feed For-\nward Network (PFN) . . . 6\n2.2.4 Add and Norm . . . . . . 6\n2.3 Self-Supervised Learning . . . . . . . 7\n3 T-BPLMs Core Concepts 7\n3.1 Pretraining Methods . . . . . . . . . . 7\n3.1.1 Mixed-Domain\nPretraining (MDPT) . . . . 7\n3.1.2 Domain-Speciﬁc Pretrain-\ning (DSPT) . . . . . . . . . 8\n3.1.3 Task Adaptive Pretraining\n(TAPT) . . . . . . . . . . . . 8\n3.2 Pretraining Tasks . . . . . . . . . . . 8\n3.2.1 Main Pretraining Tasks . . 8\n• K.S.Kalyan is with the Department of Computer Applications, National\nInstitute of Technology Trichy, Trichy, Tamil Nadu, India, 620015.\nE-mail: kalyan.ks@yahoo.com, Website: https://mr-nlp.github.io\n• Ajit Rajasekharan is with the Nference.ai as CTO, Cambridge, MA, USA,\n02142.\n• S.Sangeetha is with the Department of Computer Applications, National\nInstitute of Technology Trichy, Trichy, Tamil Nadu, India, 620015..\nPreprint under review - The paper is named (AMMU) in the memory of one\nof the close friends of K.S.Kalyan (https://mr-nlp.github.io).\n3.2.2 Auxiliary Pretraining Tasks 10\n3.3 Fine-Tuning Methods . . . . . . . . . 10\n3.3.1 Intermediate Fine-Tuning\n(IFT) . . . . . . . . . . . . . 11\n3.3.2 Multi-Task Fine-Tuning . . 11\n3.4 Embeddings . . . . . . . . . . . . . . 11\n3.4.1 Main Embeddings . . . . . 11\n3.4.2 Auxiliary Embeddings . . 12\n4 T-BPLMs TAXONOMY 13\n4.1 Pretraining Corpus . . . . . . . . . . 13\n4.1.1 Electronic Health Records 13\n4.1.2 Radiology Reports . . . . . 14\n4.1.3 Social Media . . . . . . . . 15\n4.1.4 Scientiﬁc Literature . . . . 15\n4.1.5 Hybrid Corpora . . . . . . 16\n4.2 Extensions . . . . . . . . . . . . . . . 17\n4.2.1 Language-Speciﬁc . . . . . 17\n4.2.2 Ontology Enriched . . . . 17\n4.2.3 Green Models . . . . . . . 19\n4.2.4 Debiased Models . . . . . 19\n4.2.5 Multi-Modal Models . . . 19\n5 BIOMEDICAL NLP TASKS 20\n5.1 Natural Language Inference . . . . . 20\n5.2 Entity Extraction . . . . . . . . . . . . 20\n5.3 Semantic Textual Similarity . . . . . 20\n5.4 Relation Extraction . . . . . . . . . . 21\n5.5 Text Classiﬁcation . . . . . . . . . . . 21\n5.6 Question Answering . . . . . . . . . 21\n5.7 Text Summarization . . . . . . . . . . 22\n6 Evaluation 22\narXiv:2105.00827v2  [cs.CL]  2 Sep 2021\n2\n7 CHALLENGES AND SOLUTIONS 22\n7.1 Low Cost Domain Adaptation . . . . 22\n7.2 Ontology Knowledge Injection . . . 23\n7.3 Small Datasets . . . . . . . . . . . . . 23\n7.4 Robustness to Noise . . . . . . . . . . 23\n7.5 Quality In-Domain Word Represen-\ntations . . . . . . . . . . . . . . . . . . 24\n7.6 Low Resource (In-Domain Corpus)\nPretraining . . . . . . . . . . . . . . . 24\n7.7 Quality Sequence Representation . . 24\n8 FUTURE DIRECTIONS 24\n8.1 Mitigating Bias . . . . . . . . . . . . . 24\n8.2 Privacy Issues . . . . . . . . . . . . . 24\n8.3 Domain Adaptation . . . . . . . . . . 25\n8.4 Novel Pretraining Tasks . . . . . . . 25\n8.5 Benchmarks . . . . . . . . . . . . . . . 25\n8.6 Intrinsic Probes . . . . . . . . . . . . 25\n8.7 Efﬁcient Models . . . . . . . . . . . . 25\n9 Limitations 26\n10 Conclusion 26\nReferences 26\n1 I NTRODUCTION\nT\nRANSFORMER [1] based PLMs like BERT [2],\nRoBERTa [3], T5 [4] have started a new era in\nmodern NLP . These models combine the power of\ntransformers, transfer learning, and self-supervised\nlearning. Transformers use self-attention which can be\nrun in parallel and can model long-range relationships\nwith ease. In transfer learning [5], knowledge gained\nby the model in the source task is transferred to the\ntarget task. For example, computer vision models are\ntrained over large labeled datasets, and then these\npretrained models are used in similar tasks where the\nlabeled datasets are small [6], [7]. The main advantages\nof pretrained models are a) they learn language\nrepresentations that are useful across tasks and b) no\nneed to train the downstream models from scratch.\nHowever, in NLP , it is quite expensive and difﬁcult to\nobtain such large, annotated datasets. So, transformer-\nbased PLMs are pretrained over large unlabeled text\ndata using self-supervised learning. Self-supervised\nlearning is in between supervised and unsupervised\nlearning. Supervised learning requires human-annotated\ninstances while unsupervised learning does not require\nany labeled instances. Self-supervised learning relies\non labels like supervised and semi-supervised learning.\nHowever, these labels are not human assigned but\ncreated automatically by using the relationships\nbetween various sections of the input data. Once the\nmodel is pre-trained over large volumes of text, it can\nbe used in various downstream tasks by ﬁne-tuning\nafter adding task-speciﬁc layers [2].\nIn the initial days, NLP systems are mostly rule-\nbased. The development of rule-based systems is quite\ndifﬁcult as it requires signiﬁcant human intervention\nin the form of domain expertise to frame the rules.\nIt is required to reframe the rules with even with a\nsmall change in the input data which makes it expen-\nsive and laborious. Machine learning systems to some\nextent brought ﬂexibility in developing NLP systems.\nMachine learning systems learn the rules during training\nand thereby avoids the laborious process of manual\nrule framing. However, the main drawback in machine\nlearning models is the requirement of feature engineer-\ning which again requires domain expertise. With the\ndevelopment of various deep learning models like con-\nvolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) which can learn features automatically\nand better hardware like GPUs, NLP researchers shifted\nto deep learning models with dense word vectors as\ninput [8], [9]. Traditional text representation methods like\ntf-idf and one-hot vectors are high-dimensional which\ndemand more computational resources. Moreover, these\nrepresentations are unable to encode syntactic and se-\nmatic information. This requirement of low-dimensional\ntext vectors which can also encode language informa-\ntion leads to the development of embedding models\nlike Word2Vec [10], Glove [11]. As these models cannot\nencode sub-word information and suffer from the out of\nvocabulary (OOV) problem, FastText [12] is proposed.\nSome of the drawbacks of using CNN or RNN with\ndense word vectors as input are a) Embeddings models\nlike Word2Vec, Glove, and FastText are based on shallow\nneural networks. Shallow neural networks with only two\nor three layers are unable to capture more language\ninformation into word vectors. Being context insensitive\nfurther limits the quality of these word vectors. b) Even\nthough word embeddings are pre-trained on text cor-\npus, the parameters of models like CNN and RNN are\nrandomly initialized and learned during model training.\nLearning model parameters from scratch requires a large\nnumber of training instances.\nSelf-attention computes the representation of every\ntoken in the input based on its interaction with every\ntoken in the input. As a result, the self-attention mecha-\nnism can better handle long distance word relationships\ncompared to CNN and RNN [1], [13], [14]. Moreover,\ntransformers can learn complex language information\nby applying self-attention layers iteratively i.e., by us-\ning a stack of self-attention layers. Transformers with\nself-attention as the core component have become the\nprimary choice of architecture for pretrained language\nmodels in NLP . Transformer-based PLMs like BERT [2],\nRoBERTa [3], ALBERT [15], T5 [4] achieved tremendous\nsuccess in many of the NLP tasks. These models elimi-\nnate the requirement of training a downstream model\nfrom scratch. With the success of these models, pre-\ntraining the model on large volumes of text and then\nﬁne-tuning it on task-speciﬁc datasets has become a\nstandard approach in modern NLP . Following the suc-\n3\nFig. 1: Key milestones in T-BPLMs\ncess of transformer-based PLMs in the general domain,\nbiomedical NLP researchers have developed models like\nBioBERT [16], ClinicalBERT [17], and BlueBERT [18]. All\nthese models are obtained by further pretraining general\nBERT on biomedical texts except ClinicalBERT which is\ninitialized from BioBERT.\nLee et al. [16] proposed BioBERT in January 2019\nand it is the ﬁrst transformer-based BPLM. After that,\nnumber of models are proposed like ClinicalBERT [17],\nClinicalXLNet [19], BlueBERT [18], PubMedBERT [20],\nouBioBERT [21]. Since BioBERT, around 40+ BPLMs are\nproposed to push the state-of-the-art in various biomed-\nical NLP tasks. Figure 1 summarizes key milestones\nin transformer-based BPLMs. Transformer-based BPLMs\nhave become the ﬁrst choice for any task in biomedical\nNLP . However, there is no survey paper that presents\nthe recent trends in the transformer-based PLMs in\nbiomedical NLP .\nCurrently, there are three survey papers that provide a\ncomprehensive review of embeddings in the biomedical\ndomain and three survey papers that provide a com-\nprehensive review of transformer-based PLMs in the\ngeneral domain. The survey paper written by Kalyan\nand Sangeetha [22] is the ﬁrst comprehensive survey on\nembeddings in biomedical NLP . This paper a) classify\nand compare various biomedical corpora b) present a\nbrief overview of various context insensitive embedding\nmodels and compare them c) classify and explain various\nbiomedical embeddings d) present solutions to various\nchallenges in biomedical embeddings. The survey papers\nwritten by Chiu and Baker [23], Khattak et al. [24] also\npresent the same contents differently. All these three sur-\nvey papers provide information mostly on context insen-\nsitive biomedical embeddings with very little emphasis\non transformer-based BPLMs. The paper by Wang et al.\n[25] provides empirical evaluation of word embeddings\ntrained from various corpora. The survey papers written\nby Qiu et al. [13], Liu et al. [26] and Kalyan et al. [14]\npresent a review of various transformer-based PLMs in\nthe general domain only. So, we strongly believe there is\na need for a survey paper that presents the recent trends\nrelated to transformer-based BPLMs (T-BPLMs). Figure\n2 summarizes the contents of this survey paper.\n1.1 Literature Search and Selection\nFigure 3 shows the PRISMA ﬂow chart for litera-\nture search and selection. For the literature survey, we\nsearched in databases like PubMed, ACM Digital Li-\nbrary, IEEE Xplore, ACL Web Anthology, Google Scholar\nand ScienceDirect. The ﬁrst transformer-based BPLM i.e.,\n4\nFig. 2: Summmary of AMMU survey paper\nBioBERT was released in January 2019. So, we gath-\nered articles published in between January 2019 and\nJuly 2021. For the literature search, we initially used\nkeywords like “biomedical pretrained models”, “clin-\nical pretrained models”, “BioBERT”, “PubMedBERT”,\n“BlueBERT”, “ClinicalBERT”, “transformer-based lan-\nguage models” and “in-domain pretrained models”. We\niteratively added new keywords from the gathered arti-\ncles and ﬁnally arrived at this list of keywords ”biomed-\nical pretrained models”, ”clinical pretrained mod-\nels”, ”BioBERT”, ”PubMedBERT”, ”BlueBERT”, ”Clin-\nicalBERT”, ”transformer-based language models”, ”in-\ndomain pretrained models”, “BioELECTRA”, “BioAL-\nBERT”, “BLUE benchmark”, “BLURB benchmark”,\n“transformers”, “domain-speciﬁc pretrained models”,\n“medical language models”, “multi-modal pretrained\nmodels”. Finally, we collected around 4567 articles out of\nwhich 1246 articles were duplicate. After excluding the\nduplicate and irrelevant articles, there were 121 articles.\nWe considered an article as irrelevant based on the\nfollowing\n(a) article is not related to natural language processing\n(321 articles)\n(b) article is related to natural language processing but\nnot related to biomedical domain (2509 articles)\n(c) article is related to biomedical domain but the\napproach is mainly based on context insensitive\nembeddings models and cited T-BPLMs papers in\nfuture work (155 articles).\n(d) article is related to biomedical domain and approach\nis based on T-BPLMs but the approach involves\nmere application of T-BPLMs without much novelty\n(215 articles).\nThe highlights of this survey paper are\n• First survey paper to present the recent trends in\ntransformer-based BPLMs.\n• We present a brief overview of various foundational\nconcepts like embedding layer, transformer encoder\nlayer and self-supervised learning (Section 2).\n• We explain various core concepts related to\ntransformer-based BPLMs like pretraining methods,\npretraining tasks, ﬁne-tuning methods, and embed-\ndings. We discuss each concept in detail, classify and\ncompare various methods in each (Section 3).\n• We present a taxonomy of transformer-based\nBPLMs and present a brief overview of all the\nmodels (Section 4).\n• We explain how transformer-based BPLMs are ap-\nplied in various biomedical NLP tasks (Section 5).\n• We present solutions to some of the challenges\nlike low-cost domain adaptation, small biomedical\ndatasets, ontology knowledge injection, robustness\nto noise, quality in-domain word representations,\nquality sequence representation and pretraining us-\ning less in-domain corpora (Section 7).\n• We discuss possible future directions which\nwill drive the researchers to further enhance\ntransformer-based BPLMs (Section 8).\n5\nFig. 3: PRISM ﬂowchart for literature selection\nFig. 4: T-PLM like BERT and RoBERTa\n2 F OUNDATIONS\nIn general, the core components of transformer-based\nPLMs like BERT and RoBERTa are embedding and trans-\nformer encoder layers (refer Figure 4). The embedding\nlayer takes input tokens and returns a vector for each.\nThe embedding layer has three or more sub-layers each\nof which provides a vector of speciﬁc embedding type\nfor each of the input tokens. The ﬁnal input vector\nfor each token is obtained by summing all the vec-\ntors of each embedding type. The transformer encoder\nlayer enhances each input token vector by encoding\nglobal contextual information using the self-attention\nmechanism. By applying a sequence of such transformer\nencoder layers, the model can encode complex language\ninformation in the input token vectors.\n2.1 Embedding Layer\nFig. 5: Final input vectors obtained by summing all the\nthree vectors.\nUsually, the embedding layer consists of three sub-\nlayers with each sub-layer representing a particular em-\nbedding type. In some models, there are more than\nthree also. For example, embedding layer of BERT-EHR\n[27] contains code, position, segment, age and gender\nembeddings. A detailed description of various embed-\nding types is presented in Section 3.4. The ﬁrst sub-\nlayer converts input tokens to a sequence of vectors\nwhile the other two sub-layers provide auxiliary infor-\nmation like position and segmentation. The ﬁrst sub-\nlayer can be char, sub-word, or code embedding based.\nFor example, BioCharBERT [28] uses CharCNN [29] on\nthe top of character embeddings, BERT uses WordPiece\n[30] embeddings while BEHRT [27], MedBERT [31] and\nBERT-EHR [32] models use code embeddings. Unlike\nBioCharBERT and BERT models, the input for BEHRT,\nMedBERT, BERT-EHR models is patient visits where\n6\neach patient visit is expressed as a sequence of codes.\nThe ﬁnal input representation X for the given input\ntokens {x1, x2, . . . xn}is obtained by adding the embed-\ndings from the three sub-layers (for simplicity, we have\nincluded only three embedding types – refer Figure 5).\nX = I + P + S (1)\nWhere X ∈ Rn ×e represents ﬁnal input embeddings\nmatrix and I ∈ Rn ×e, P ∈ Rn ×e and S ∈ Rn ×e\nrepresents the three embedding type matrices. Here n\nrepresents length of input sequence and e represents\nembedding size.\n2.2 Transformer Encoder\nFig. 6: Transformer Encoder\nMulti-Head Self Attention (MHSA), Position-wise\nFeed Forward Network (PFN), Add and Norm consti-\ntutes a transformer encoder layer (refer Figure 6). MHSA\napplies self-attention (SA) multiple times independently\nto relate each token to all the tokens in the input se-\nquence, while PFN is applied on each token vector to\ngenerate non-linear hierarchical features. Add and Norm\nrepresents residual and layer norm normalization which\nare included on top of both MHSA and PFN to stay away\nfrom vanishing and exploding gradients.\n2.2.1 Self-Attention (SA)\nSA is a much better alternative compared to convolution\nand recurrent layers to encode global contextual infor-\nmation. For a sequence of input tokens, SA updates each\ninput token vector by encoding global contextual infor-\nmation i.e., it expresses each token vector as a weighted\nsum of all the token vectors where the weights are\ngiven by attention scores. The ﬁnal input representation\nmatrix X is transformed into Query ( Q ∈Rn ×q), Key\n(K ∈ Rn ×k) and Value ( V ∈ Rn ×v) matrices using\nthree weight matrices WQ ∈Re ×q, WK ∈Re ×k and\nWV ∈Re ×v. Here q = k = v = e\nh. Here h represents the\nnumber of self-attention heads. The output of SA layer\nis computed as\n1) Compute similarity matrix ( S ∈Rn ×n) as Q.KT .\n2) To obtain stable gradients, scale the similarity matrix\nvalues using √q and then use softmax to convert\nsimilarity scores to probability values to get matrix\nP ∈Rn ×n. Formally, P = Softmax ((Q.KT )/√q)\n3) Compute the ﬁnal weighted values matrix Z ∈\nRn ×v as P.V\n2.2.2 Multi-Head Self Attention (MHSA)\nWith only one self-attention layer, the meaning of a\nword may largely depend on the same word itself. To\navoid this, SA is applied multiple times in parallel each\nwith different weight matrices. Thus, MHSA allows the\ntransformer to attend to multiple positions while encod-\ning a word. Let Z1, Z2, Z3,..,Zh represent the weighted\nvalues matrices of h self-attention heads. Then the ﬁnal\nweighted value matrix is obtained by concatenating all\nthese individual weight matrices and then projecting it.\nMHSA (X) = [Z1, Z2, Z3, . . . , Zh].WO (2)\nWhere MHSA (X) ∈ Rn ×e, WO ∈ Rhv ×e and\n[Z1, Z2, Z3, . . . , Zh] ∈Rn ×hv\n2.2.3 Position-wise Feed Forward Network (PFN)\nTwo linear layers with a non-linear activation constitutes\nthe PFN. PFN is applied to every input token vector.\nModels like BERT uses Gelu [33] activation function.\nHere the parameters of PFNs applied on each of the\ntoken vectors are the same. Formally,\nPFN (y) =Gelu(yW1 + b1)W2 + b2 (3)\n2.2.4 Add and Norm\nAdd represents residual connection while Norm repre-\nsents layer normalization. Add and Norm is applied on\nboth MHSA and PFN of transformer encoder to stay\naway from vanishing and exploding gradients.\nIn general, a transformed-based PLM consists of a\nsequence of transformer encoder layers after the em-\nbedding layer. Each transformer encoder layer updates\nthe input token vectors by encoding global contextual\ninformation. By updating the input token vector using\na sequence of transformer encoders help the model to\nencode more language information. Formally,\nˆEm−1 = LN(Em−1 + MHSA (Em−1)) (4)\nEm = LN( ˆEm−1 + PFN ( ˆEm−1)) (5)\nHere LN represents Layer Normalization, ˆEm−1 rep-\nresents the output after applying Add and Norm over\nthe output of MHSA and Em represents the output after\napplying Add and Norm over the output of PFN in mth\nencoder layer. Overall, Em represents the output of mth\nencoder layer with Em−1 as input. Here the input for the\nﬁrst transformer encoder layer is, E0 = X.\n7\nFig. 7: Continual Pretraining (CPT)\n2.3 Self-Supervised Learning\nDeep learning algorithms dominated rule-based and\nmachine learning algorithms in the last decade. This is\nbecause deep learning models can learn features auto-\nmatically which eliminates the requirement of expen-\nsive feature engineering and process the inputs in an\nend-to-end manner i.e., take raw inputs and give the\ndecisions. The success of the deep learning algorithms\ncomes from the knowledge gained during training from\nhuman-labeled instances. However, supervised learning\nhas a lot of limitations a) with less data, the model\nmay get overﬁtted and prone to bias b) some of the\ndomains like biomedical are supervision starved i.e.,\ndifﬁcult to get labeled data. In general, we expect models\nto be close to human intelligence i.e., more general and\nmake decisions with just a few samples. This desire of\ndeveloping models with more generalization ability and\nlearning from fewer samples has made the researchers\nfocus on other learning paradigms like Self-Supervised\nLearning [34].\nRobotics is the ﬁrst AI ﬁeld to use self-supervised\nlearning methods [34]. Over the last ﬁve years, self-\nsupervised learning has become popular in other AI\nﬁelds like natural language processing [13], [14], [26],\ncomputer vision [35], [36], and speech processing [37],\n[38]. SSL is a new learning paradigm that draws inspi-\nration from both supervised and unsupervised learning\nmethods. SSL is similar to unsupervised learning as it\ndoes not depend on human-labeled instances. It is also\nsimilar to supervised learning as it learns using supervi-\nsion. However, in SSL the supervision is provided by\nthe pseudo labels which are generated automatically\nfrom the pretraining data. SSL involves pretraining the\nmodel over a large unlabelled corpus using one or\nmore pretraining tasks. The pseudo labels are generated\ndepending on the deﬁnitions of pre-training tasks. SSL\nmethods fall into three categories namely Generative,\nContrastive, and Generate-Contrastive [34]. In Gener-\native SSL, encoder maps input vector x to vector y,\nand decoder recovers x from y (e.g., masked language\nmodeling). In Contrastive SSL, the encoder maps input\nvector x to vector y to measure similarity (e.g., mutual\ninformation maximization). In Generate-Contrastive SSL,\nfake samples are generated using encoder-decoder while\nthe discriminator identiﬁes the fake samples (e.g., re-\nplaced token detection). For more details about different\nSSL methods, please refer to the survey paper written\nby Liu et al. [34].\n3 T-BPLM S CORE CONCEPTS\n3.1 Pretraining Methods\nSSL involves pretraining on large volumes of unlabeled\ndata using one or more tasks. Pretraining allows the\nmodel to learn language representations that are useful\nacross tasks. Moreover, pretraining gives the model a\nbetter initialization which avoids training from scratch\nand overﬁtting in low data situations. Pretraining meth-\nods in biomedical NLP fall into three categories as shown\nin Figure 9.\nFig. 8: Simultaneous Pretraining (SPT)\n3.1.1 Mixed-Domain Pretraining (MDPT)\nMixed domain pretraining involves training the model\nusing both general and in-domain text. Depending on\nwhether the pretraining is done simultaneously or not,\nmixed domain pretraining can be classiﬁed into a) Con-\ntinual pretraining – initially the model is pre-trained\nover general domain text and then adapted to the\nbiomedical domain [16] and b) Simultaneous pretraining\n– the model is pre-trained over the combined corpora\nhaving both general and in-domain text where the in-\ndomain text is up sampled to ensure balanced pretrain-\ning [21].\nContinual Pretraining (CPT) : It is the standard ap-\nproach followed by the biomedical NLP research com-\nmunity to develop transformer-based BPLMs. It is also\nreferred to as further pretraining. In this approach, the\nmodel is initialized with general PLM weights and then\nthe model is adapted to in-domain by further pretrain-\ning on large volumes of in-domain text (refer Figure\n7). For example, BioBERT is initialized with general\nBERT weights and then further pretrained on PubMed\nabstracts and PMC full-text articles [16]. In the case of\nBlueBERT, the authors used both PubMed abstracts and\nMIMIC-III clinical notes for continual pretraining [18].\nSimultaneous Pretraining (SPT): Continual pretrain-\ning achieved good results by adapting general models\nto the biomedical domain [16], [18], [19], [39], [40].\nHowever, it requires large volumes of in-domain text.\nOtherwise, CPT may result in suboptimal performance.\nSimultaneous pretraining comes to the rescue when only\na small amount of in-domain text is available. Here,\nthe pretraining corpora consist of both in-domain and\ngeneral domain text where the in-domain text is up\nsampled to ensure a balanced pretraining (refer Figure\n8). For example, BERT (jpCR+jpW) [41] is developed\nby simultaneous pretraining over a small amount of\nJapanese clinical text and a large amount of Japanese\nWikipedia text. This model outperformed UTH-BERT in\nclinical text classiﬁcation. UTH-BERT [42] is trained from\nscratch over Japanese clinical text.\n8\nFig. 9: Pretraining Methods\nFig. 10: Domain-Speciﬁc Pretraining (DSPT)\n3.1.2 Domain-Speciﬁc Pretraining (DSPT)\nThe main drawback in continual pretraining is the gen-\neral domain vocabulary. For example, the WordPiece\nvocabulary in BERT is learned over English Wikipedia\nand Books Corpus [2]. As a result, the vocabulary does\nnot represent the biomedical domain and hence many\nof the biomedical words are split into several subwords\nwhich hinders the model learning during pretraining\nand ﬁne-tuning. Moreover, the length of the input se-\nquence also increases as many of the in-domain words\nare split into several subwords. DSPT over in-domain\ntext allows the model to have the in-domain vocabulary\n(refer Figure 10). For example, PubMedBERT is trained\nfrom scratch using PubMed abstracts and PMC full-text\narticles [20]. PubMed achieved state-of-the-art results\nin the BLURB benchmark. Similarly, RoBERTa-base-PM-\nM3-Voc is trained from scratch over PubMed and PMC\nand MIMIC-III clinical notes [43].\n3.1.3 Task Adaptive Pretraining (TAPT)\nFig. 11: Task Adaptive Pretraining (TAPT)\nBoth DSPT and MDPT require training the model\nover large volumes of text to allow the model to learn\ndomain-speciﬁc knowledge which helps to perform it\nto perform better in downstream tasks. Pretraining over\nin-domain text allows the model to learn universal in-\ndomain representations which are useful to all the in-\ndomain tasks. However, pretraining over large volumes\nof text is expensive in terms of both computational\nresources and time. Task Adaptive Pretraining (TAPT)\nis based on the hypothesis that pretraining over task-\nrelated unlabelled text allows the model to learn both\ndomain and task-speciﬁc knowledge [44] (refer Figure\n11). In TAPT, task-related unlabelled sentences are gath-\nered, and then the model is further pretrained. TAPT is\nless expensive compared to other pretraining methods as\nit involves pretraining the model over a relatively small\ncorpus of task-related unlabelled sentences.\n3.2 Pretraining Tasks\nDuring pretraining, the language models learn language\nrepresentations based on the supervision provided by\none or more pretraining tasks. A pretraining task is a\npseudo-supervised task whose labels are generated au-\ntomatically. A pretraining task can be main or auxiliary.\nThe main pretraining tasks allow the model to learn lan-\nguage representations while auxiliary pretraining tasks\nallow the model to gain knowledge from human-curated\nsources like Ontology [34], [45]–[47]. The classiﬁcation\nof pretraining tasks is given in Figure 12 and a brief\nsummary of various pretraining tasks is presented in\nTable 1.\n3.2.1 Main Pretraining Tasks\nThe main pretraining tasks allow the model to learn\nlanguage representations. Some of the commonly used\nmain pretraining tasks are masked language modelling\n(MLM) [2], replaced token detection (RTD) [50], sentence\nboundary objective (SBO) [49], next sentence prediction\n(NSP) [2] and sentence order prediction (SOP) [15].\nMasked Language Modeling (MLM) . It is an im-\nproved version of Language Modeling which utilizes\nboth left and right contexts to predict the missing tokens\n[2]. The main drawback in Unidirectional LM (Forward\nLM or Backward LM) is the inability to utilize both left\nand right contexts at the same time to predict the tokens.\nHowever, the meaning of a word depends on both the\nleft and right contexts. Devlin et al. [2] utilized MLM as a\npretraining task for learning the parameters of the BERT\nmodel. Formally, for a given sequence x with tokens\n9\nFig. 12: Pretraining Tasks\nPretraining\nTask\nType Key points Models\nMLM word-level Model learns by predicting the masked tokens. Less training signal per\ninstance as the model predicts only 15% of the tokens.\nBERT [2]\nMLM +\nDynamic\nMasking\nword-level Dynamic Masking allows masking different tokens in the sentences for\ndifferent epochs due to which the model learns more by predicting different\ntokens every time.\nRoBERTa [3]\nMLM + Whole\nWord Masking\nword-level Whole word masking is more challenging as it is difﬁcult to predict the\nentire word compared to a subword.\nPubMedBERT [20]\nMLM + Whole\nEntity Masking\nword-level Whole entity masking allows the model to learn entity-centric knowledge. MC-BERT [48]\nMLM + Whole\nSpan Masking\nword-level Whole span masking allows the model to learn more linguistic knowledge. MC-BERT [48]\nNSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful\nin tasks like NLI. Less challenging as it involves topic prediction which is\na relatively easy task.\nBERT [2]\nSOP sentence-level Allows the model to learn sentence-level reasoning skills by modeling inter-\nsentence coherence. More challenging compared to NSP as SOP involves\nonly sentence coherence.\nALBERT [15]\nSBO phrase-level Model predicts the masked tokens in a span based on boundary token\nrepresentations and position embeddings.\nSpanBERT [49]\nRTD word-level Model checks every token whether it is replaced or not. More efﬁcient\ncompared to MLM as it involves all the tokens in the input.\nELECTRA [50]\nTABLE 1. Summary of pretraining tasks.\n{x1, x2, . . . , xm}, a subset of tokens is randomly chosen\nand these tokens are replaced. The authors replaced\ntokens, 80% of the time with a special token ‘[MASK]’,\n10% of the time with a random token, and 10% of the\ntime with the same token. This is done to handle the\nmismatch between pretraining and ﬁne-tuning phases.\nFormally,\nLMLM = − 1\n|m(x)|\n∑\ni∈m(x)\nlogP (xi/ˆx) (6)\nwhere ˆx is the masked version of x and m(x) represents\nthe set of masked token positions.\nSome of the improvements like dynamic masking [3],\nwhole word masking [2], [20], [51], whole entity masking\n[48], [52], and whole span masking [48] are introduced\nin MLM to further improve its efﬁciency as a pretraining\ntask. Delvin et al. [2] used static masking to replace the\ntokens i.e., the input sentences are masked once during\npre-processing and the model predicts the same masked\ntokens in the input sentences for every epoch during\npretraining. In the case of dynamic masking [3], different\ntokens are masked in the input sentence for different\nepochs which prevents the model from predicting the\nsame masked tokens in every epoch and hence it learns\nmore. Whole word masking is much more challenging as\nthe model has to predict the entire word rather than part\nof a word. In the case of the whole entity and span mask-\nings, in-domain entities and phrases in the input sen-\ntences are identiﬁed and then masked rather than mask-\ning the randomly chosen tokens. As a result, the model\nlearns entity-centric and in-domain linguistic knowledge\nduring pretraining which enhances the performance of\nthe model in downstream tasks [48], [52]. For example,\nZhang et al. [48] trained MC-BERT using NSP and MLM\nwith whole entity and span maskings. Michalopoulos\n10\net al. [46] used novel multi-label loss-based MLM along\nwith NSP to further pretrain ClinicalBERT on MIMIC-III\nclinical notes to get UmlsBERT. Novel multilabel loss-\nbased MLM allows the model to connect all the words\nunder the same concept. Sequence-to-Sequence MLM\n(Seq2SeqLM) is an extension of MLM to models based\non encoder-decoder architecture. Models like T5 [4] are\npretrained using Seq2SeqLM pretraining task.\nReplaced Token Detection (RTD) [50]. It is a novel\npretraining task that involves verifying whether each\ntoken in the input is replaced or not. Initially, some\nof the tokens in the input sentences are replaced with\nwords predicted by a small generator network, and then\nthe model (discriminator) is asked to predict the status\nof each word as replaced or not. The two advantages\nof RTD over MLM are a) RTD provides more training\nsignal compared to MLM as RTD involves checking the\nstatus of every token in the input rather than a subset of\nrandomly chosen tokens like MLM. and b) Unlike MLM,\nRTD does not use any special tokens like ‘[MASK]’ to\ncorrupt the input. So, it avoids the mismatch problem\nthat the special token ‘[MASK]’ is seen only during\npretraining but not during ﬁne-tuning. Formally,\nLRTD = −1\n|ˆx|\n|ˆx|∑\ni=1\nlogP (t/ ˆxi) (7)\nwhere ˆx is the corrupted version of x and t = 1 when\nthe token is not a replaced one.\nSpan Boundary Objective (SBO) [49]. It is a novel pre-\ntraining task that involves predicting the entire masked\nspan based on the context. Initially, a contiguous span\nof tokens is randomly chosen and masked and then the\nmodel is asked to predict the masked tokens in the span\nbased on the token representations at the boundary. In\nthe case of MLM, the model predicts the masked token\nbased on the ﬁnal hidden vector of the masked token.\nHowever, in the case of SBO, the model predicts the\nmasked token in the span based on the ﬁnal hidden\nvectors of the boundary tokens and the position embed-\nding of the masked token. SBO is more challenging as\nit is difﬁcult to predict the entire span “frequent bathroom\nruns” than predicting “frequent” when the model already\nsees “bathroom runs” . SBO helps the model to achieve\nbetter results in span extraction-based tasks like entity\nextraction and question answering [49], [53]. Let s and\ne represent the start and end indices of the span in\nthe input sequence. Then, each token xi in the span\nis predicted based on the ﬁnal hidden vectors of the\nboundary tokens xs−1, xe+1 and its position embedding\npi−s+1. Then\nLSBO = −1\n|S|\n∑\ni∈S\nlogP (xi/yi) (8)\nwhere yi = g(xs−1, xe+1, pi−s+1), g() represents feed-\nforward network of two layers and S represents the\npositions of tokens in contiguous span.\nNext Sentence Prediction (NSP) [2] . NSP is a\nsentence-level pretraining task that involves predict-\ning whether given two sentences appear consecutively\nor not . It is basically a two-way sentence pair clas-\nsiﬁcation task. Formally, for a given sentence pair\n(x, y), the model has to predict one of the two labels\n{IsNext, IsNotNext} depending on whether the two\nsentences are consecutive or not. NSP helps the model\nto learn sentence-level reasoning skills which are use-\nful in downstream tasks involving sentence pairs like\nnatural language inference, text similarity, and question\nanswering [2]. For a balanced pretraining, the training\nexamples are chosen in a 1:1 ratio i.e., 50% are positive\nand the rest negative. Let z represents aggregate vector\nrepresentation of the sentence pair (x, y). Then,\nLNSP = −logP (t/z) (9)\nwhere t = 1 when the two sentences x and y are\nconsecutive.\nSentence Order Prediction (SOP) [15]. SOP is a novel\nsentence-level pretraining task which models inter-\nsentence coherence. Like NSP , SOP is a two-way sentence\npair classiﬁcation. Formally, for a given sentence pair\n(x, y), the model has to predict one of the two labels\n{IsSwapped, IsNotSwapped}depending on whether the\nsentences are swapped or not. For a balanced pretrain-\ning, the training examples are chosen in a 1:1 ratio\ni.e., 50% are swapped and the rest are not swapped.\nUnlike NSP which involves the prediction of both topic\nand coherence, SOP involves only sentence coherence\nprediction [15]. Topic prediction is comparatively easier\nwhich questions the effectiveness of NSP as a pretraining\ntask [3], [15], [49]. Let z represent aggregate vector\nrepresentation of the sentence pair (x, y). Then,\nLSOP = −logP (t/z) (10)\nwhere t = 1 when the two sentences x and y are not\nswapped.\n3.2.2 Auxiliary Pretraining Tasks\nAuxiliary pretraining tasks help to inject knowledge\nfrom human-curated sources like UMLS [54] into in-\ndomain models to further enhance them. For example,\nthe triple classiﬁcation pretraining task involves identi-\nfying whether two concepts are connected by the relation\nor not [45]. This auxiliary task is used by Hao et al.\n[45] to inject UMLS relation knowledge into in-domain\nmodels. Yuan et al. [47] used two auxiliary pretraining\ntasks based on multi-similarity Loss and Knowledge\nembedding loss to further pretrain BioBERT on UMLS.\nSimilarly, Liu et al. [34] used multi-similarity loss-based\npretraining task to inject UMLS synonym knowledge\ninto PubMedBERT.\n3.3 Fine-Tuning Methods\nPretraining allows the model to learn general or in-\ndomain knowledge which is useful across the tasks.\n11\nHowever, for a model to perform well in a particular\ntask, it must have task-speciﬁc knowledge along with\ngeneral or in-domain knowledge. The model gains task-\nspeciﬁc knowledge by ﬁne-tuning on the task-speciﬁc\ndatasets. Task-speciﬁc layers are included on the top\nof transformer-based BPLMs. For example, to perform\ntext classiﬁcation, we need a) a contextual encoder to\nlearn contextual token representations from the given\ninput token vectors and b) a classiﬁer to project the ﬁnal\nsequence vector and then generate the probability vector.\nHere classiﬁer is the task-speciﬁc layer which is usually a\nsoftmax layer in text classiﬁcation. Fine-tuning methods\nfall into two categories.\n3.3.1 Intermediate Fine-Tuning (IFT)\nIFT on large, related datasets allows the model to learn\nmore domain or task-speciﬁc knowledge which im-\nproves the performance on small target datasets. IFT can\nbe done in following four ways\nSame Task Different Domain– Here, the source and\ntarget datasets are from the same task but different\ndomains. Model can be ﬁne-tuned on general domain\ndatasets before ﬁne-tuning on small in-domain datasets\n[55]–[58]. For example, Cengiz et al. [55] ﬁne-tuned in-\ndomain model on general NLI datasets like SNLI [59]\nand MNLI [60] before ﬁne-tuning on MedNLI [61].\nSame Task Same Domain – Here, the source and\ntarget datasets are from the same task and domain. But\nthe source dataset is a more generic one while the target\ndataset is more speciﬁc [62], [63]. For example, Gao et al.\n[63] ﬁne-tuned BlueBERT on a large general biomedical\nNER corpus like MedMentions [64] or Semantic Medline\nbefore ﬁne-tuning on the small target NER corpus.\nDifferent Task Same Domain– Here, the source and\ntarget datasets are from different tasks but the same\ndomain. Fine-tuning on source dataset which is from the\nsame domain allows the model to gain more domain-\nspeciﬁc knowledge which improves the performance of\nthe model on the same domain target task [65]. McCreery\net al. [65] ﬁne-tuned the model on the medical question-\nanswer pairs dataset to enhance its performance on the\nmedical question similarity dataset.\nDifferent Task Different Domain– Here, the source\nand target datasets are from different tasks and differ-\nent domains. For example, Jeong et al. [66] ﬁne-tuned\nBioBERT on a general MultiNLI dataset to improve the\nperformance of the model in biomedical QA. Here the\nmodel learns sentence level reasoning skills which are\nuseful in biomedical QA.\n3.3.2 Multi-Task Fine-Tuning\nMulti-task ﬁne-tuning allows the model to be ﬁne-tuned\non multiple tasks simultaneously [67]–[69]. Here the\nembedding and transformer encoder layers are common\nfor all the tasks and each task has a separate task-speciﬁc\nlayer. Multi-task ﬁne-tuning allows the model to gain\ndomain as well as task-speciﬁc reasoning knowledge\nfrom multiple tasks. At the same time, due to the in-\ncrease in training set size, the model is less prone to\nover-ﬁtting. Multi-task ﬁne-tuning is more useful in low\nresource scenarios which are common in the biomedical\ndomain [69]. Moreover, having a single model for multi-\ntasks eliminates the need of deploying separate models\nfor each task which saves computational resources, time,\nand deployment costs [70]. Multi-task ﬁne-tuning may\nnot provide the best results all the time [70]. In such\ncases, multi-task ﬁne tuning can be applied iteratively\nto identify the best possible subset of tasks [71]. For\nexample, Mahanjan et al. [71] applied multi-task ﬁne-\ntuning iteratively to choose the best subset of related\ndatasets. Finally, the authors ﬁne-tuned the model on\nbest subset of related datasets and achieved the best\nresults on the Clinical STS [72] dataset. After multi-task\nﬁne-tuning, the model can be further ﬁne-tuned on the\ntarget speciﬁc dataset separately to further enhance the\nperformance of the model [73].\n3.4 Embeddings\nEmbeddings represent the data in a low-dimensional\nspace. Embeddings in transformer-based BPLMs fall into\ntwo categories namely main and auxiliary. The main em-\nbeddings map the given input sequence to a sequence of\nvectors while auxiliary embeddings provide additional\nuseful information. Figure 13 shows the classiﬁcation of\nembeddings.\n3.4.1 Main Embeddings\nText embeddings map the given sequence of words into\na sequence of vectors. Text embeddings can be char,\nsubword or code-based.\nCharacter Embeddings - In character embeddings,\nthe vocabulary consists of letters, punctuation symbols,\nspecial characters and numbers only. Each character is\nrepresented using an embedding. These embeddings are\ninitialized randomly and learned during model pretrain-\ning. ELMo embedding model uses CharCNN to generate\nword representations from character embeddings [74].\nInspired by ELMo, BioCharBERT also uses CharCNN\non the top of character embeddings to generate word\nrepresentations [28]. AlphaBERT [75] also uses character\nembeddings. Unlike CharacterBERT, AlphaBERT directly\ncombines character embeddings with position embed-\ndings and then applies a stack of transformer encoders.\nThe main advantage with character embeddings is the\nsmall size of vocabulary as it includes only characters.\nThe disadvantage is longer pretraining times [28]. As the\nsequence length increases with character level embed-\ndings, models are slow to pre-train.\nSubword Embeddings- In subword embeddings, the\nvocabulary consists of characters and the most frequent\nsubwords and words. The main principle driving the\nsubword embedding vocabulary construction is that fre-\nquent words should be represented as a single word and\nrare words should be represented in terms of meaningful\n12\nFig. 13: Embeddings in T-BPLMs\nsubwords. Subword embedding vocabularies are always\nmoderate in size as they use sub-words to represent\nrare and misspelled words. Some of the popular algo-\nrithms to generate vocabulary for sub-word embeddings\nare Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77],\nWord-Piece [30], Unigram [78], and Sentencepiece [79].\nByte-Pair Encoding (BPE) [76] - It starts with a base\nvocabulary having all the unique characters in the train-\ning corpus. It augments the base vocabulary with the\nmost frequent pairs until the desired vocabulary size is\nachieved. Byte-Pair Encoding algorithm can be summa-\nrized as\n1) Prepare a large training corpus and ﬁx the vocabu-\nlary size.\n2) Generate a base vocabulary having all the unique\ncharacters in the training corpus.\n3) Calculate the frequency of all the words in the\ncorpus.\n4) Augment the vocabulary with the most frequently\noccurring pair.\n5) Until the desired vocabulary size is achieved, repeat\nstep 4.\nByte-Level BPE [77] - In Byte-Level BPE, each character\nis represented as a byte, and the rest of the procedure is\nthe same as in BPE. Text is converted into a sequence of\nbytes and the most frequent byte pair is added into the\nbase vocabulary until the desired size is achieved. Byte-\nLevel BPE is extremely beneﬁcial in the multilingual\nscenario. GPT-2 [77] and RoBERTa [3] use Byte-Level BPE\nembeddings.\nWordPiece [30] - The working of WordPiece is almost\nthe same as BPE. Word-Piece and BPE differ in the\nstrategy used in selecting the symbol pair to augment\nthe base vocabulary. BPE chooses the most frequent\nsymbol pair while Word-Piece uses a language model to\nchoose the symbol pair. BERT [2], DistilBERT [80], and\nELECTRA model use WordPiece embeddings.\nSentencePiece [79] - A common problem in BPE and\nWordPiece is that they assume that words in the input\nsentences are separated by space. However, this assump-\ntion is not applicable in all languages. To overcome this,\nSentencePiece treats space as a character and includes it\nin the base vocabulary. The ﬁnal vocabulary is generated\niteratively using BPE or Unigram. XLNet [81], ALBERT\n[15], and T5 [4] models use SentencePiece embeddings.\nUnigram [78] - Unigram is similar to BPE and Word-\nPiece in ﬁxing the vocabulary size at the beginning\nitself. BPE and Word-piece start with a small base vo-\ncabulary and then augments the base vocabulary for\na certain number of iterations. Unlike BPE and Word-\nPiece, Unigram starts with a large base vocabulary and\nthen iteratively trims the symbols to arrive at a small\nﬁnal vocabulary. It is not used directly in any of the\nmodels. SentencePiece uses the Unigram algorithm to\ngenerate the ﬁnal vocabulary.\nCode embeddings- Code embeddings map the given\nsequence of codes into a sequence of vectors. For exam-\nple, in the case of models like BERT-EHR [32], MedBERT\n[31], and BEHRT [27], the input is not a sequence of\nwords. Instead, input is patient visits. Each patient visit\nis represented as a sequence of codes. The number of\ncode embeddings varies from model to model. For ex-\nample, MedBERT and BEHRT include embeddings only\nfor disease codes while BERT-EHR includes embeddings\nfor disease, medication, procedure, and clinical notes.\n3.4.2 Auxiliary Embeddings\nMain embeddings represent the given input sequence\nin low dimensional space. The purpose of auxiliary\nembeddings is to provide additional information to the\nmodel so that the model can learn better. For each input\ntoken, a representation vector is obtained by summing\nthe main and two or more auxiliary embeddings. The\nvarious auxiliary embeddings are\nPosition Embeddings- Position embeddings enhance\nthe ﬁnal input representation of a token by providing its\nposition information in the input sequence. As there is\nno convolution or recurrence layers which can learn the\norder of input tokens automatically, we need to explicitly\nprovide the location of each token in the input sequence\nthrough position embeddings. Position embeddings can\nbe pre-determined [27], [32] or learned during model\npretraining [2].\nSegment Embeddings- Segment embeddings help to\n13\nFig. 14: T-BPLMs taxonomy\ndistinguish tokens of different input sequences. Segment\nembedding is the same for all the tokens in the same\ninput sequence.\nAge Embeddings - In models like BEHRT [27] and\nBERT-EHR [32], age embeddings are used in addition\nto other embeddings. Age embeddings provide the age\nof the patient and help the model to leverage temporal\ninformation. Age embedding is the same for all the codes\nin a single patient visit.\nGender Embeddings- In models like BEHRT [27] and\nBERT-EHR [32], gender embeddings are used in addition\nto other embeddings. Gender embeddings provide the\ngender information of the patient to the model. Gender\nembedding is the same for all the codes in all the patient\nvisits.\nSemantic Group Embeddings - Semantic group em-\nbeddings are used in UmlsBERT [46] to explicitly inform\nthe model to learn similar representations for words\nfrom the same semantic group i.e., semantic group em-\nbedding is same for all the words which fall into the\nsame semantic group. Besides, it also helps to provide\nbetter representations for rare words.\n4 T-BPLM S TAXONOMY\nFigure 14 shows transformer-based BPLMs taxonomy.\n4.1 Pretraining Corpus\n4.1.1 Electronic Health Records\nIn the last decade, most hospitals have been using\nElectronic Health Records (EHRs) to record patient as\nwell as treatment details right from admission to dis-\ncharge [82]. EHRs contain a vast amount of medical\ndata which can be used to provide better patient care by\nknowledge discovery and the development of better al-\ngorithms. As EHR contains sensitive information related\nto patients, medical data must be de-identiﬁed before\nsharing. EHRs include both structured and unstruc-\ntured data [83], [84]. Structured data includes laboratory\ntest results, various medical codes, etc. Unstructured\ndata include clinical notes like medication instructions,\nprogress notes, discharge summaries, etc. Clinical notes\ninclude the most valuable patient information which is\ndifﬁcult and expensive to extract manually. So, there\nis a need for automatic information extraction meth-\nods to utilize the abundant medical data from EHRs\nin research as well as applications [84]–[86]. Following\nthe success of transformer-based PLMs in the general\ndomain, researchers in biomedical NLP also developed\nEHR-based T-BPLMs by pretraining over clinical notes\nor medical codes or both. MIMIC [87], [88] is the largest\npublicly available dataset of medical records. MIT Lab\n14\nModel Type Pretrained\nfrom\nCorpus\nPublicly\nAvailable\nEvaluation\nClinicalBERT [17] EHR BioBERT MIMIC-III Clinical Notes Yes MedNLI and Clinical Concept\nExtraction\nClinicalBERT\n(discharge) [17]\nEHR BioBERT MIMIC-III Discharge sum-\nmaries\nYes MedNLI and Clinical Concept\nExtraction\nMIMIC-BERT [40] EHR General\nBERT\nMIMIC-III Clinical Notes Yes Clinical Concept Extraction\nClinicalXLNet\n(nursing) [19]\nEHR General XL-\nNet\nMIMIC-III Nursing notes Yes Prolonged Mechanical Ventila-\ntion Prediction problem\nClinicalXLNet (dis-\ncharge) [19]\nEHR General XL-\nNet\nMIMIC-III Discharge notes Yes Prolonged Mechanical Ventila-\ntion Prediction problem\nBERT-MIMIC [39] EHR General\nBERT\nMIMIC-III Clinical Notes Yes Clinical Concept Extraction\nELECTRA-MIMIC\n[39]\nEHR General\nELECTRA\nMIMIC-III Clinical Notes Yes Clinical Concept Extraction\nXLNet-MIMIC [39] EHR General XL-\nNet\nMIMIC-III Clinical Notes Yes Clinical Concept Extraction\nRoBERTa-MIMIC\n[39]\nEHR General\nRoBERTa\nMIMIC-III Clinical Notes Yes Clinical Concept Extraction\nALBERT-MIMIC\n[39]\nEHR General\nALBERTA\nMIMIC-III Clinical Notes Yes Clinical Concept Extraction\nDeBERTa-MIMIC\n[39]\nEHR General De-\nBERTa\nMIMIC-III Clinical Notes Yes Clinical Concept Extraction\nLongformer-\nMIMIC [39]\nEHR General\nLongformer\nMIMIC-III Clinical Notes Yes Clinical Concept Extraction\nMedBERT [31] EHR Scratch Private EHR No Disease Prediction\nBEHRT [27] EHR Scratch Private EHR No Disease Prediction\nBERT-EHR [32] EHR Scratch Private EHR No Disease Prediction\nAlphaBERT [75] EHR Scratch Private EHR No Text Summarization\nTABLE 2. Summary of EHR-based T-BPLMs.\nresearchers gathered medical records from Beth Israel\nDeaconess Medical Center, de-identiﬁed the sensitive\npatient information, and then released four versions of\nthe MIMIC dataset.\nAlsentzer et al. [17] further pretrained BioBERT\n(BioBERT-Base v1.0 + PubMed 200K + PMC 270K) on\nMIMIC III clinical notes to get ClinicalBERT. It took\naround 17- 18 days to pretrain the model using one\nGTX TITAN X GPU. It is the ﬁrst publicly available\nmodel pretrained on clinical notes. Si et al. [40] further\npre-trained general BERT (base and large) on MIMIC-\nIII clinical notes to get MIMIC-BERT. The authors eval-\nuated the models on various clinical entity extraction\ndatasets. Huang et al. [19] further pre-trained XLNet-\nbase on MIMIC-III Clinical Notes. The authors released\ntwo models namely a) pretrained model on nursing\nnotes and b) pretrained model on discharge summaries.\nYang et al. [39] further pre-trained general models like\nBERT, ELECTRA, RoBERTa, XLNet, and ALBERT on\nMIMIC-III and released in-domain PLMs. It is the ﬁrst\nwork to release in-domain models based on all the\npopular transformer-based PLMs. Unlike the above pre-\ntrained models which are pretrained on clinical text,\nrecent works [27], [31], [39] released models which are\npre-trained on disease codes or multi-modal EHR data.\nBEHRT [27] is trained from scratch using 1.6 million\npatient EHR data with MLM as pretraining task. The\nauthors used code, position, age, and segment embed-\ndings. Med-BERT [31] is trained from scratch using\n28,490,650 patient EHR data with MLM and LOS (Length\nof Stay) as pretraining tasks. The authors used code,\nserialization and visit embeddings. BERT-EHR [32] is\ntrained from scratch using multi-modal data from 43967\npatient records with MLM as a pretraining task. Table 2\ncontains summary of various EHR based BPLMs.\n4.1.2 Radiology Reports\nFollowing the success of EHR-based T-BPLMs, recently\nresearchers focused on developing PLMs speciﬁcally\nfor radiology reports. RadCore [90] dataset consists\nof around 2 million radiology reports. These reports\nwere gathered from three major healthcare organiza-\ntions: Mayo Clinic, MD Anderson Cancer Center, and\nMedical College of Wisconsin in 2007. Meng et al. [89]\nfurther pre-trained general BERT on radiology reports\nwith impression section headings from RadCore dataset\n15\nModel Type Pretrained\nfrom\nCorpus Publicly\nAvailable\nEvaluation\nRadBERT [89] Radiology General\nBERT\nRadCore [90] No Radiology Reports Classiﬁca-\ntion\nFS-BERT [91] Radiology Scratch Private Radiology Reports No Radiology Reports Classiﬁca-\ntion\nRAD-BERT [91] Radiology German\ngeneral\nBERT\nPrivate Radiology Reports No Radiology Reports Classiﬁca-\ntion\nTABLE 3. Summary of radiology reports-based T-BPLMs.\nModel Type Pretrained\nfrom\nCorpus Publicly\nAvailable\nEvaluation\nCT-BERT [92] Social Media General\nBERT\nCovid Tweets Yes Text Classiﬁcation\nBERTweetCovid19\n[93]\nSocial Media BERTweet Covid Tweets Yes Text Classiﬁcation\nBioRedditBERT [94] Social Media BioBERT Health related Reddit posts Yes Unsupervised Medical Concept\nNormalization\nRuDR-BERT [95] Social Media Multilingual\nBERT\nRussian health reviews Yes Sentence classiﬁcation and Clin-\nical Entity Extraction\nEnRuDR-BERT [95] Social Media Multilingual\nBERT\nRussian and English health\nreviews\nYes ADR (Adverse Drug Reaction)\nTweets Classiﬁcation\nEnDR-BERT [95] Social Media Multilingual\nBERT\nEnglish health reviews Yes ADR Tweets Classiﬁcation and\nADR Normalization\nTABLE 4. Summary of social media-based T-BPLMs.\nto get RadBERT. The authors used RadBERT to classify\nradiology reports. Bressem et al. [91] released two T-\nBPLMs for radiology reports namely FS-BERT and RAD-\nBERT. FS-BERT is obtained by training from scratch us-\ning around 3.8 M radiology reports having 415M words\n(3.6GB) and custom WordPiece vocabulary. RAD-BERT\nis obtained by further pretraining German general BERT\nwith custom WordPiece vocabulary over 3.8 M radiology\nreports having 415M words (3.6GB). Table 3 contains a\nsummary of radiology reports-based T-BPLMs.\n4.1.3 Social Media\nIn the last decade, social media has become the ﬁrst\nchoice for internet users to express their thoughts. Apart\nfrom views about general topics, various social media\nplatforms like Twitter, Reddit, AskAPatient, WebMD are\nused to share health-related experiences in the form of\ntweets, reviews, questions, and answers [107], [108]. Re-\ncent works have shown that health-related social media\ndata is useful in many applications to provide better\nhealth-related services [109], [110]. The performance of\ngeneral T-PLMs on Wikipedia and Books Corpus is lim-\nited on health-related social media datasets [92]. This is\nbecause social media text is highly informal with a lot of\nnonstandard abbreviations, irregular grammar, and ty-\npos. Researchers working at the intersection of social me-\ndia and health, trained social media text-based T-BPLMs\nto handle social media texts. CT-BERT [92] is initialized\nfrom BERT-large and further pretrained on a corpus of\n22.5M covid related tweets and this model showed up to\n30% improvement compared to BERT-large, on ﬁve dif-\nferent classiﬁcation datasets. BioRedditBERT [94] is ini-\ntialized from BioBERT and further pretrained on health-\nrelated Reddit posts. The authors showed that BioRed-\nditBERT outperforms in-domains models like BioBERT,\nBlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in\nnormalizing health-related entity mentions. RuDR-BERT\n[95] is initialized from Multilingual BERT and pretrained\non the raw part of the RuDReC corpus (1.4M reviews).\nThe authors showed that RuDR-BERT outperforms mul-\ntilingual BERT and Russian BERT on Russian sentence\nclassiﬁcation and clinical entity extraction datasets by\nlarge margins. EnRuDR-BERT [95] and EnDR-BERT [95]\nare obtained by further pretraining multilingual BERT on\nRussian and English health reviews and English health\nreviews respectively. Table 4 contains summary of social\nmedia text-based BPLMs.\n4.1.4 Scientiﬁc Literature\nIn the last few decades, the amount of biomedical litera-\nture is growing at a rapid scale. As knowledge discovery\nfrom biomedical literature is useful in many applica-\ntions, biomedical text mining is gaining popularity in\nthe research community [16]. However, biomedical text\nsigniﬁcantly differs from the general text with a lot of\ndomain-speciﬁc words. As a result, the performance of\ngeneral T-PLMs is limited in many of the tasks. So,\nbiomedical researchers focused on developing in-domain\nT-PLMs to handle biomedical text. PubMed and PMC are\nthe two popular sources of biomedical text. PubMed con-\n16\nModel Type Pretrained\nfrom\nCorpus Publicly\nAvailable\nEvaluation\nBioBERT [16] Scientiﬁc\nLiterature\nGeneral\nBERT\nPubMed and PMC Yes Biomedical NER, RE, and QA.\nRoBERTa-base-\nPM [43]\nScientiﬁc\nLiterature\nGeneral\nRoBERTa\nPubMed and PMC Yes Sequence Labelling and Text Classi-\nﬁcation\nRoBERTa-base-\nPM-Voc [43]\nScientiﬁc\nLiterature\nScratch PubMed and PMC Yes Sequence Labelling and Text Classi-\nﬁcation\nBioALBERT\n[96]\nScientiﬁc\nLiterature\nGeneral\nALBERT\nPubMed and PMC Yes Biomedical Concept Extraction\nBioBERTpt-bio\n[97]\nScientiﬁc\nLiterature\nMultilingual\nBERT\nBrazilian Biomedical corpus No Clinical Concept Extraction\nPubMedBERT\n[20]\nScientiﬁc\nLiterature\nScratch PubMed and PMC Yes BLURB\nBioELECTRA\n[98]\nScientiﬁc\nLiterature\nScratch PubMed Yes Biomedical NER, QA and RE.\nBioELECTRA\n++ [98]\nScientiﬁc\nLiterature\nBioELECTRA PMC Yes Biomedical NER, QA and RE.\nBioMegatron\n[99]\nScientiﬁc\nLiterature\nScratch PubMed and PMC No Biomedical NER, RE and QA.\nOuBioBERT\n[21]\nScientiﬁc\nLiterature\nScratch PubMed Yes BLUE\nBlueBERT-PM\n[18]\nScientiﬁc\nLiterature\nGeneral\nBERT\nPubMed Yes BLUE\nBioMedBERT\n[100]\nScientiﬁc\nLiterature\nGeneral\nBERT\nBREATHE 1.0 No Biomedical NER, IR and QA.\nELECTRAMed\n[101]\nScientiﬁc\nLiterature\nScratch PubMed Yes Biomedical NER, RE and QA\nBioELECTRA-P\n[102]\nScientiﬁc\nLiterature\nScratch PubMed Yes BLURB, BLUE\nBioELECTRA-\nPM [102]\nScientiﬁc\nLiterature\nScratch PubMed, PMC Yes BLURB, BLUE\nBioALBERT-P\n[103]\nScientiﬁc\nLiterature\nALBERT PubMed Yes BLURB\nBioALBERT-\nPM [103]\nScientiﬁc\nLiterature\nALBERT PubMed, PMC Yes BLURB\nTABLE 5. Summary of scientiﬁc literature-based BPLMs. NER - Named Entity Recognition, RE - Relation Extraction, IR -\nInformation Retrieval, QA - Question Answering.\ntains only biomedical literature citations and abstracts\nonly while PMC contains full-text biomedical articles.\nAs of March 2020, PubMed includes 30M citations and\nabstracts while PMC contains 7.5M full-text articles. Due\nto the large collection and broad coverage, these two are\nthe ﬁrst choice to pretrain T-BPLMs [16], [43], [96].\nAs DSPT is expensive, most of the works developed\nin-domain T-PLMs by initializing from general BERT\nmodels and then further pretraining on biomedical text.\nBioBERT [16] is the ﬁrst biomedical pre-trained language\nmodel which is obtained by further pretraining general\nBERT on biomedical literature. BioBERTpt-bio [97] is\nobtained by further pretraining BERT Multilingual (base)\non Brazilian biomedical corpus - scientiﬁc papers from\nPubMed (0.8M- only literature titles) + Scielo (health\n– 12.4M + biological- 3.2M: both titles and abstracts).\nBioMedBERT [100] is obtained by further pretraining\nBERT-large on BREATHE 1.0 corpus. BioMedBERT out-\nperformed BioBERT on biomedical question answering.\nThe key reason for the better performance of BioMed-\nBERT is the diversity of biomedical text in the BREATHE\ncorpus. The main drawback in developing biomedical\nmodels by further pretraining general models is the\ngeneral vocabulary. To overcome this, researchers started\nto develop biomedical models by using DSPT. Microsoft\nresearchers developed PubMed [20] model by DSPT\nwith in-domain vocabulary and whole word masking\nstrategy. OuBioBERT [21] is trained from scratch using\nfocused PubMed abstracts (280M words) as core corpora\nand PubMed abstracts (2800M words) as satellite cor-\npora. It outperforms BioBERT and BlueBERT in many\nof the tasks in the BLUE benchmark. Table 5 contains\nsummary of scientiﬁc literature-based T-BPLMs.\n4.1.5 Hybrid Corpora\nIt is difﬁcult to obtain a large amount of in-domain\ntext in some cases. For example, MIMIC [87], [88] is\nthe largest publicly available dataset of medical records.\n17\nModel Type Pretrained\nfrom\nCorpora Publicly\nAvailable\nEvaluation\nBlueBERT-P-M3\n[18]\nHybrid General\nBERT\nPubMed + MIMIC-III Yes BLUE\nRoBERTa-base-\nP-M3 [43]\nHybrid General\nRoBERTa\nPubMed + MIMIC-III Yes Sequence Labelling and Text Classi-\nﬁcation\nRoBERTa-base-\nP-M3-Voc [43]\nHybrid Scratch PubMed + MIMIC-III Yes Sequence Labelling and Text Classi-\nﬁcation\nBioBERTpt-All\n[97]\nHybrid Multilingual\nBERT\nBrazilian Clinical Text +\nBiomedical Text\nYes Clinical Concept Extraction\nBioCharBERT\n[28]\nHybrid General\nCharacter-\nBERT\nPMC Abstracts + MIMIC-III Yes Clinical NER, MedNLI, RE and STS.\nBERT (sP\n+W+BC) [41]\nHybrid Scratch PubMed + English Wikipedia +\nBooksCorpus\nYes BLUE\nBERT\n(jpCR+jpW)\n[41]\nHybrid Scratch Japanese Clinical Text +\nJapanese Wikipedia Text\nNo Text Classiﬁcation\nAraBioBERT\n[104]\nHybrid AraBERT General Arabic Text, Arabic\nBiomedical Text\nNo NER\nSciBERT [105] Hybrid Scratch Biomedical + Computer Science\nLiterature Text\nYes NER and RE\nSciFive-P [106] Hybrid T5 C4, PubMed Yes NER, RE, MedNLI, QA\nSciFive-PM\n[106]\nHybrid T5 C4, PubMed, PMC Yes NER, RE, MedNLI, QA\nBioALBERT-P-\nM3 [103]\nHybrid ALBERT PubMed, MIMIC-III Yes BLURB\nBioALBERT-\nPM-M3 [103]\nHybrid ALBERT PubMed, PMC, MIMIC-III Yes BLURB\nTABLE 6. Summary of hybrid corpora-based T-BPLMs.\nThe MIMIC dataset is small compared to the gen-\neral Wikipedia corpus or biomedical scientiﬁc litera-\nture (from PubMed + PMC). However, to pretrain a\ntransformer-based PLM from scratch, we require large\nvolumes of text. To overcome this, some of the models\nare pretrained on general + in-domain text [41], [104]\nor, in-domain + related domain text [18], [28], [43],\n[97]. For example, BERT (jpCR+jpW) [41] - Japanese\nmedical BERT is pretrained from scratch using Japanese\nDigital Clinical references (jpCR) and Japanese general\nWikipedia (jpW). This model outperformed UTH-BERT\n[42] on Japanese clinical document classiﬁcation. BERT\n(sP +W+BC)[41] - trained from scratch using small\nPubMed abstracts as core corpora and English Wikipedia\n(W) + BooksCorpus(BC) as satellite corpora. This model\nachieved performance comparable to OuBioBERT [41]\nin the BLUE benchmark. AraBioBERT [104] is obtained\nby further pretraining AraBERT [41] on general Arabic\ncorpus + biomedical Arabic text. The author showed\npretraining a monolingual BERT model like AraBERT\non a small-scale domain-speciﬁc corpus can still improve\nthe performance of the model. Table 6 contains hybrid\ncorpora-based T-BPLMs.\n4.2 Extensions\n4.2.1 Language-Speciﬁc\nFollowing the success of BioBERT, ClinicalBERT, Pub-\nMedBERT in English biomedical tasks, researchers fo-\ncused on developing T-BPLMs for other languages also\nby pretraining from scratch [41], [91], [111] or pretraining\nfrom Multilingual BERT [95], [97] or pretraining from\nmonolingual BERT [48], [91], [112] models. For exam-\nple, CHMBERT [112] is the ﬁrst Chinese medical BERT\nmodel which is initialized from the general Chinese\nBERT model and further pretrained on a huge (185GB)\ncorpus of Chinese medical text gathered from more than\n100 hospitals. MC-BERT [48] is also initialized from\ngeneral Chinese BERT and further pre-trained on hybrid\ncorpora which includes general, biomedical, and medical\ntexts. Table 7 contains a summary of language-speciﬁc T-\nBPLMs.\n4.2.2 Ontology Enriched\nT-BPLMs like BioBERT, BlueBERT and PubMedBERT\nhave achieved good results in many biomedical NLP\ntasks. These models acquire domain-speciﬁc knowledge\nby pretraining on large volumes of biomedical text.\nRecent works [34], [45]–[47] showed that these models\nacquire only the knowledge available in pretraining\ncorpora and the performance of these models can be\n18\nModel Language Pretrained\nfrom\nCorpora Publicly\nAvailable\nEvaluation\nBERT\n(jpCR+jpW)\n[41]\nJapanese Scratch Japanese Clinical Text +\nJapanese Wikipedia\nNo Text Classiﬁcation\nBioBERTpt-bio\n[97]\nPortuguese Multilingual\nBERT\nBrazilian Biomedical Text Yes Clinical Concept Extraction\nBioBERTpt-clin\n[97]\nPortuguese Multilingual\nBERT\nBrazilian Clinical Text Yes Clinical Concept Extraction\nBioBERTpt-all\n[97]\nPortuguese Multilingual\nBERT\nBrazilian Clinical Text +\nBiomedical Text\nYes Clinical Concept Extraction\nRuDR-BERT\n[95]\nRussian Multilingual\nBERT\nRussian Health Reviews Yes Text classiﬁcation and Clinical NER\nEnRuDR-BERT\n[95]\nRussian Multilingual\nBERT\nRussian and English Health Re-\nviews\nYes ADR Tweets Classiﬁcation\nFS-BERT [91] German Scratch Private Radiology Reports No Radiology Reports Classiﬁcation\nRAD-BERT [91] German General\nGerman\nBERT\nPrivate Radiology Reports No Radiology Reports Classiﬁcation\nCHMBERT\n[112]\nChinese General\nChinese\nBERT\nPrivate EHRs No Disease Prediction and Department\nRecommendation\nSpanishBERT\n[111]\nSpanish Scratch Spanish Biomedical Text No Biomedical NER\nAraBioBERT\n[104]\nArabic AraBERT General Arabic Text+ Arabic\nBiomedical Text\nNo Biomedical NER\nCamemBioBERT\n[113]\nFrench CamemBERT\n[114]\nFrench Biomedical Corpus No Biomedical NER\nMC-BERT [48] Chinese General\nChinese\nBERT\nChinese Biomedical Text, Ency-\nclopedia , Medical records\nYes ChineseBLUE\nUTH-BERT [42] Japanese Scratch Japanese Clinical Text Yes Text Classiﬁcation\nSINA-BERT\n[115]\nPersian ParsBERT\n[116]\nPersian Medical Corpus No Medical Question Classiﬁcation,\nMedical Question Retrieval and\nMedical Sentiment Analysis\nmBERT-Galen\n[117]\nSpanish Multilingual\nBERT\nSpanish Clinical Text corpus Yes Medical Coding\nBETO-Galen\n[117]\nSpanish BETO [118] Spanish Clinical Text corpus Yes Medical Coding\nXLM-R-Galen\n[117]\nSpanish XLM-R\n[119]\nSpanish Clinical Text corpus Yes Medical Coding\nTABLE 7. Summary of language-speciﬁc T-BPLMs.\nfurther enhanced by integrating knowledge from various\nhuman-curated knowledge sources like UMLS. UMLS is\na human-curated knowledge source connecting medical\nterms from various clinical vocabularies. In UMLS, each\nmedical concept has a Concept Unique Identiﬁer (CUI),\npreferred term, and synonym terms. UMLS concepts\nare linked by various semantic relationships. Domain\nknowledge of T-BPLMs can be further enhanced by fur-\nther pretraining them on UMLS synonyms and relations\n[34], [45]–[47] .\nClinical Kb-BERT and Clinical Kb-ALBERT [45] are\nobtained by further pretraining BioBERT and ALBERT\nmodels on MIMIC-III clinical notes and UMLS relation\ntriplets. Here, pretraining involves three loss functions\nnamely MLM, NSP , and triple classiﬁcation. Triple clas-\nsiﬁcation involves identifying whether two concepts are\nconnected by the relation or not and helps to inject\nUMLS relationship knowledge into the model. Umls-\nBERT [46] is initialized from ClinicalBERT and further\npretrained on MIMIC-III clinical notes using novel multi-\nlabel loss-based MLM and NSP . The novel multi-label\nloss function allows the model to connect all the words\nunder the same CUI. CoderBERT [47] is initialized from\nBioBERT and further pre-trained on UMLS concepts\nand relations using multi-similarity loss and knowledge\nembedding loss. Multi-similarity loss helps to learn close\nembeddings for entities under the same CUI and Knowl-\nedge embedding loss helps to inject relationship knowl-\nedge. SapBERT [120] is initialized from PubMedBERT\nand further pre-trained on UMLS synonyms using multi-\nsimilarity loss. Table 8 contains a summary of ontology\nenriched T-BPLMs.\n19\nModel Pretrained\nfrom\nUMLS data Publicly\nAvailable\nEvaluation\nClinical Kb-\nBERT [45]\nBioBERT UMLS Relations Yes Clinical NER and NLI\nClinical Kb-\nALBERT [45]\nGeneral\nALBERT\nUMLS Relations Yes Clinical NER and NLI\nUmlsBERT [46] ClinicalBERT UMLS Synonyms Yes Clinical NER and NLI\nCoderBERT [47] BioBERT UMLS Synonyms and Rela-\ntions\nYes Unsupervised Medical Concept Normalization, Seman-\ntic Similarity, and Relation Classiﬁcation.\nCoderBERT-\nALL [47]\nMultilingual\nBERT\nUMLS Synonyms and Rela-\ntions\nYes Unsupervised Medical Concept Normalization, Seman-\ntic Similarity, and Relation Classiﬁcation.\nSapBERT [120] PubMedBERT UMLS Synonyms Yes Medical Concept Normalization\nSapBERT-\nXLMR [120]\nXLM-\nRoBERTa\nUMLS Synonyms Yes Medical Concept Normalization\nKeBioLM [121] PubMedBERT UMLS Relations Yes Named Entity Recognition and Relation Extraction\nTABLE 8. Summary of ontology enriched T-BPLMs.\n4.2.3 Green Models\nCPT allows the general T-PLMs to adapt to in-domain\nby further pretraining on large volumes of the in-domain\ncorpus. As these models contain vocabulary, which is\nlearned over general text, they cannot represent in-\ndomain words in a meaningful way as many of the in-\ndomain words are split into a number of subwords. This\nkind of representation increases the overall length of the\ninput as well as hinders the model learning. DSPT or\nSPT allows the model to have an in-domain vocabulary\nthat is learned over in-domain text. However, both these\napproaches involve learning the model parameters from\nscratch which is highly expensive. These approaches\nbeing expensive, are far away from the small research\nlabs, and with long runtimes, they are environmentally\nunfriendly also [122], [123].\nRecently there is raising interest in the biomedical\nresearch community to adapt general T-PLMs mod-\nels to in-domain in a low cost way and the models\ncontain in-domain vocabulary also [122], [123]. These\nmodels are referred to as Green Models as they are\ndeveloped in a low cost environment-friendly approach.\nGreenBioBERT [122] - extends general BERT to the\nbiomedical domain by reﬁning the embedding layer with\ndomain-speciﬁc word vectors. The authors generated in-\ndomain vocabulary using Word2Vec and then aligned\nthem with general WordPiece vectors. With the addition\nof domain-speciﬁc word vectors, the model acquires\ndomain-speciﬁc knowledge. The authors showed that\nGreenBioBERT achieves competitive performance com-\npared to BioBERT in entity extraction. This approach is\ncompletely inexpensive as it requires only CPU. exBERT\n[123] - extends general BERT to the biomedical domain\nby reﬁning the model with two additions a) in-domain\nWordPiece vocabulary in addition to existing general\ndomain WordPiece vocabulary b) extension module. The\nin-domain WordPiece vectors and extension module pa-\nrameters are learned during pretraining on biomedical\ntext. During pretraining, as the parameters of general\nBERT are kept ﬁxed, this approach is quite inexpensive.\nTable 9 contains summary of Green T-BPLMs.\n4.2.4 Debiased Models\nT-PLMs are shown to exhibit bias i.e., the decisions taken\nby the models may favor a particular group of people\ncompared to others. The main reason for unfair decisions\nof the models is the bias in the datasets on which the\nmodels are trained [124]–[126]. It is necessary to identify\nand reduce any form of bias that allows the model to\ntake fair decisions without favoring any group. Zhang\net al. [127] further pretrained SciBERT [105] on MIMIC-\nIII clinical notes and showed that the performance of\nthe model is different for different groups. The authors\napplied adversarial pretraining debiasing to reduce the\ngender bias in the model. The authors released both\nthe models publicly to encourage further research in\ndebiasing T-BPLMs.\n4.2.5 Multi-Modal Models\nT-PLMs have achieved success in many of the NLP tasks\nincluding in speciﬁc domains like Biomedical. Recent\nresearch works have focused on developing pretrained\nmodels that can handle multi-modal data i.e., video +\ntext [128], [129], image + text [130]–[134] etc. In Biomed-\nical domain, models like BERTHop [134] and Medical-\nVLBERT [133] have been proposed recently to handle\nimage + text data. BERTHop [134] is a multi-modal\nT-BPLM developed for Chest X-ray disease diagnosis.\nLike ViLBERT [131] and LXMERT [132], BERTHop uses\nseparate encoder to encoder image and text inputs.\nBERTHop uses PixelHop++ [135] to encode image data\nand BlueBERT as text encoder. Medical-VLBERT is de-\nveloped for automatic report generation from COVID-\n19 scans. Unlike BERTHop, Medical-VLBERT [133] uses\nshared encoder based on VL-BERT [130] to encode image\nand text data.\n20\nModel Type In-Domain Vocabulary Adds Further\npretrained\nGreenBioBERT\n[122]\nGreen Generated using Word2Vec over biomedical text and\nthen further aligned\nIn-domain vocabulary No\nexBERT [123] Green Generated using WordPiece over biomedical text. In-domain vocabulary and\nextension module\nYes\nTABLE 9. Summary of Green T-BPLMs.\n5 BIOMEDICAL NLP TASKS\n5.1 Natural Language Inference\nNatural Language Inference (NLI) is an important NLP\ntask that requires sentence-level semantics. It involves\nidentifying the relationship between a pair of sentences\ni.e., whether the second sentence entails or contradicts\nor neural with the ﬁrst sentence. Training the model on\nNLI datasets helps the models to learn sentence-level\nsemantics, which is useful in many tasks like paraphrase\nmining, information retrieval [136] in the general do-\nmain and medical concept normalization [137], [138],\nsemantic relatedness [139], question answering [66] in\nthe biomedical domain. NLI is framed as a three-way\nsentence pair classiﬁcation problem. Here models like\nBERT learn the representation of given two sentences\njointly and the three-way task-speciﬁc softmax classiﬁer\npredicts the relationship between the given sentence\npair. MedNLI [61] is the in-domain NLI dataset with\naround 14k instances generated from MIMIC-III [88]\nclinical notes. As MedNLI contains sentence pairs taken\nfrom EHRs, Kanakarajan et al. [140] further pretrained\nBioBERT [16] on MIMIC-III and then ﬁne-tuned the\nmodel on MedNLI to achieve an accuracy of 83.45%.\nCengiz et al. [55] applied an ensemble of two BioBERT\nmodels and achieved an accuracy of 84.7%. The authors\nﬁne-tuned each of the BioBERT models on general NLI\ndatasets like SNLI [59] and MultiNLI [60] and then ﬁne-\ntuned them on MedNLI.\n5.2 Entity Extraction\nEntity Extraction is the ﬁrst step in unlocking valuable\ninformation in unstructured text data. Entity Extraction\nis useful in many tasks like entity linking, relation ex-\ntraction, knowledge graph construction, etc. Extracting\nclinical entities like drug and adverse drug reactions\nis useful in pharmacovigilance and biomedical entities\nlike proteins, chemicals and drugs is useful to discover\nknowledge in scientiﬁc literature. Some of the popular\nentity extraction datasets are I2B2 2010 [141], VAERS\n[142], CADEC [143], N2C2 2018 [144] , BC4CHEMD\n[145], B5CDR-Chem [146], JNLPBA [147] and NCBI-\nDisease [148].\nMost of the existing work approaches entity extrac-\ntion as sequence labeling or machine reading compre-\nhension. In case of sequence labelling approach, BERT\nbased models generate contextual representations for\neach token and then softmax layer [62], [149], [150],\nBiLSTM+Softmax [150], BiLSTM+CRF [62], [151]–[153]\nor CRF [53], [62], [151] is applied. Recent works showed\nadding BiLSTM on the top of the BERT model does\nnot show much difference in performance [151], [152].\nThis is because transformer encoder layers in BERT\nbased models do the same job of encoding contextual in\ntoken representations like BiLSTM. Some of the works\nexperimented with general BERT for extracting clinical\nand biomedical entities. For example, Portelli et al. [53]\nshowed that SpanBERT+CRF outperformed in-domain\nBERT models also in extracting clinical entities in social\nmedia text. Boudjellal et al. [104] developed ABioNER by\nfurther pretraining AraBERT [41] on general Arabic cor-\npus + biomedical Arabic text and showed that ABioNER\noutperformed both multilingual BERT [2] and AraBERT\non Arabic biomedical entity extraction. This shows that\nfurther pretraining general AraBERT on small in-domain\ntext corpus improves the performance of the model. As\nin-domain datasets are comparatively small, some of\nthe recent works [62], [63], [154] initially ﬁne-tuned the\nmodels on similar datasets before ﬁne-tuning on small\ntarget datasets. This intermediate ﬁne-tuning allows the\nmodel to learn more task-speciﬁc knowledge which im-\nproves the performance of the model on small target\ndatasets. For example, Gao et al. [63] proposed a novel\napproach entity extraction approach based on interme-\ndiate ﬁne-tuning and semi-supervised learning. Here\nintermediate ﬁne-tuning allows the model to learn more\ntask-speciﬁc knowledge while semi-supervised learning\nallows to leverage task-related unlabelled data by as-\nsigning pseudo labels. Recently Sun et al. [62] formu-\nlated biomedical entity extraction as question answering\nand showed that BioBERT+QA outperformed BioBERT+\n(Softmax / CRF / BiLSTM-CRF) on six datasets.\n5.3 Semantic Textual Similarity\nSemantic Textual Similarity quantiﬁes the degree of\nsemantic similarity between two phrases or sentences.\nUnlike NLI which classiﬁes the given sentence pair into\none of three classes, semantic textual similarity returns\na value that indicates the degree of similarity. Both NLI\nand STS require sentence-level semantics. STS is useful\nin tasks like concept relatedness [139], medical concept\nnormalization [137], [138] , duplicate text detection [155],\nquestion answering [65], [156] and text summarization\n[157]. Moreover, Reimers et al. [136] showed that train-\ning transformer-based PLMs on STS datasets allow the\nmodel to learn sentence-level semantics and hence better\n21\nrepresent variable-length texts like phrases or sentences.\nModels like BERT learn the joint representation of given\nsentence pair and a task-speciﬁc sigmoid layer gives the\nsimilarity value. BIOSSES [158] and Clinical STS [72] are\nthe commonly used datasets to train and evaluate in-\ndomain STS models.\nRecent works exploited general models for clinical\nSTS [56], [57], [65], [159]. For example, Yang et al. [56]\nachieved the best results on the 2019 N2C2 STS dataset\nusing Roberta-large model. As clinical STS datasets are\nsmall in size, recent works initially ﬁne-tuned the models\non general STS datasets and then ﬁne-tuned them on\nclinical datasets [56], [57], [65], [71]. Xiong et al. [160]\nenhanced in-domain BERT-based text similarity using\nCNN-based character level representations and TransE\n[161] based entity representations. Mutinda et al. [155]\nachieved a Pearson correlation score of 0.8320 by ﬁne-\ntuning Clinical BERT on a combined training set having\ninstances from both general and clinical STS datasets.\nMahajan et al. [71] proposed a novel approach based\non ClinicalBERT ﬁne-tuned using iterative multi-task\nlearning and achieved the best results on the Clinical STS\ndataset. Iterative multi-task learning a) helps the model\nto learn task-speciﬁc knowledge from related datasets\nand b) choose the best-related datasets for intermediate\nmulti-task ﬁne-tuning. The main drawback in the above\nexisting works is giving ‘[CLS]‘ vector as sentence pair\nrepresentation to the sigmoid layer. This is because the\n‘[CLS]‘ vector contains only partial information. Unlike\nexisting works, Wang et al. [159] applied hierarchical\nconvolution on the ﬁnal hidden state vectors and then\napplied max and min pooling to get the ﬁnal sentence\npair representation and achieved better results.\n5.4 Relation Extraction\nRelation extraction is a crucial task in information ex-\ntraction which identiﬁes the semantic relations between\nentities in text. Entity extraction followed by relation\nextraction helps to convert unstructured text into struc-\ntured data. Extracting relations between entities is use-\nful in many tasks like knowledge graph construction,\ntext summarization, and question answering. Some of\nrelation extraction datasets are I2B2 2012 [162], AIMED\n[163], ChemProt [164], DDI [165], I2B2 2010 [141] and\nEU-ADR [166]. Wei et al. [167] achieved the best re-\nsults on two datasets using MIMIC-BERT [40] +Soft-\nmax. Thillaisundaram and Togia [168] applied SciBERT\n+Softmax to extract relations from biomedical abstracts\nas part of AGAC track of BioNLP-OST 2019 shared\ntasks [169]. Liu et al. [170] proposed SciBERT+Softmax\nfor relation extraction in biomedical text. They showed\nSciBERT+Softmax outperforms BERT+Softmax on three\nbiomedical relation extraction datasets. The main draw-\nback in the above existing works is that they utilize\nonly the partial knowledge from the last layer in the\nform of ‘[CLS]‘ vector. Su et al. [171] added attention\non the top of BioBERT to fully utilize the information\nin the last layer and achieved the best results on three\nbiomedical extraction datasets. The authors generated\nthe ﬁnal representation by concatenating ‘[CLS]‘ vector\nand weighted sum vector of ﬁnal hidden state vectors.\nWhen compared to applying LSTM on the ﬁnal hidden\nstate vectors, the attention layer gives better results.\n5.5 Text Classiﬁcation\nText classiﬁcation involves assigning one of the prede-\nﬁned labels to variable-length texts like phrases, sen-\ntences, paragraphs, or documents. Text classiﬁcation\ninvolves an encoder which is usually a transformer-\nbased PLM and a task-speciﬁc softmax classiﬁer. ‘[CLS]‘\nvector or weighted sum of ﬁnal hidden state vectors is\ntreated as an aggregate representation of given text. The\nfully connected dense layer in the classiﬁer projects text\nrepresentation vector into n-dimensional vector space\nwhere ‘n’ represents the number of predeﬁned labels.\nFinally, the softmax function is applied to get the prob-\nabilities of all the labels. Garadi et al. [172] formulated\nprescription medication (PM) identiﬁcation from tweets\nas four-way text classiﬁcation. They achieved good re-\nsults using models like BERT, RoBERTa, XLNet, ALBERT,\nand DistillBERT. They trained different ML-based meta\nclassiﬁers with predictions from pre-trained models as\ninputs and further improved the results.\nShen et al. [173] applied various in-domain BERT\nfor Alzheimer disease clinical notes classiﬁcation and\nachieved the best results using PubMedBERT and\nBioBERT. They generated labels for the training instances\nusing a rule-based NLP algorithm. Chen et al. [174]\nfurther pretrained the general BERT model on 1.5 drug-\nrelated tweets and showed that further pretraining im-\nproves the performance of the model on ADR tweets\nclassiﬁcation. Recent works [175], [176] showed that\nadding attention on the top of the BERT model improves\nthe performance of the model in clinical text classiﬁ-\ncation. They introduced a custom attention model to\naggregate the encoder output and showed that this leads\nto improved performance and interpretability.\n5.6 Question Answering\nQuestion Answering (QA) aims to extract answers for\nthe given queries. QA helps to quickly ﬁnd information\nin clinical notes or biomedical literature and thus saves a\nlot of time. The main challenge in developing automated\nQA systems for the clinical or biomedical domain is\nthe small size of datasets. Developing large QA datasets\nin the clinical or biomedical domain is quite expensive\nand requires a lot of time also. Some of the popular\nin-domain QA datasets are emrQA [177], CliCR [178],\nPubMedQA [179] COVID-QA [180], MASH-QA [181]\nand Health-QA [182]. Chakraborty et al. [183] showed\nBioMedBERT obtained by further pretraining BERT-\nLarge on BREATHE 1.0 corpus outperformed BioBERT\non biomedical question answering. The main reason for\nthis is the diversity of text in BREATHE 1.0 corpus.\n22\nModel NER PICO RE SS DC QA BLURB\nScore\nBioELECTRA [102] 86.67 74.13 81.44 92.76 84.20 76.38 82.60\nPubMedBERT [20] 86.13 73.72 80.59 92.31 82.62 73.61 81.50\nBioBERT [16] 85.81 73.18 79.79 89.52 81.54 72.19 80.34\nScibert [105] 85.43 73.12 79.56 86.25 80.66 68.12 78.86\nClinicalBERT [17] 83.99 72.06 76.91 91.23 80.74 58.79 77.29\nBlueBERT [18] 84.50 72.54 76.13 85.38 80.48 58.57 76.27\nTABLE 10. Performance comparison of various T-BPLMs (scores from BLURB benchmark). NER- Named Entity Recognition,\nPICO - Patient Population Interventions Comparator and Outcomes, RE-Relation Extraction, SS - Sentence Similarity, DC -\nDocument Classiﬁcation and QA - Question Answering.\nPergola et al. [52] introduced Biomedical Entity Mask-\ning (BEM) which allows the model to learn entity-centric\nknowledge during further pretraining. They showed that\nBEM improved the performance of both general and in-\ndomain models on two in-domain QA datasets. Recent\nworks used intermediate ﬁne-tuning on general QA\n[58], [183] or NLI [66] datasets or multi-tasking [184] to\nimprove the performance of in-domain QA models. For\nexample, Soni et al. [183] achieved the best results on\na) CliCR by intermediate ﬁne-tuning on SQuaD using\nBioBERT and b) emrQA by intermediate ﬁne-tuning on\nSQuaD and CliCR using Clinical BERT. Yoon et al. [58]\nshowed that intermediate ﬁne-tuning on general domain\nSquad datasets improves the performance on biomedical\nquestion answering datasets. Akdemir et al. [184] pro-\nposed a novel multi-task model based on BioBERT for\nbiomedical question answering. They used biomedical\nNER as an auxiliary task and showed that transfer\nlearning from the bioNER task improves performance\non question answering tasks.\n5.7 Text Summarization\nIn general, sources of biomedical information like clinical\nnotes, scientiﬁc papers, radiology reports are length in\nnature. Researchers and domain experts need to go\nthrough a number of biomedical documents. As biomed-\nical documents are length in nature, there is a need for\nautomatic biomedical text summarization which reduces\nthe effort and time for researchers and domain experts\n[185], [186]. Text summarization falls into two broad\ncategories namely extractive summarization which iden-\ntiﬁes the most relevant sentences in the document while\nabstractive summarization generates new text which\nrepresents the summary of the document [187]. There are\nno standard datasets for biomedical text summarization.\nResearchers usually treat scientiﬁc papers as documents\nand their abstracts as summaries [188], [189].\nMoradi et al. [188] proposed a novel approach to\nsummarize biomedical scientiﬁc articles. They embed-\nded sentences, generated clusters, and then extracted the\nmost informative sentences from each of the clusters.\nThey showed that BERT-large outperformed other mod-\nels including the in-domain BERT models like BioBERT.\nIn the case of small models, BioBERT outperformed\nothers. Moradi et al. [189] proposed a novel approach\nbased on word embeddings and graph ranking to sum-\nmarize the biomedical text. They generated a graph with\nsentences as nodes and edges as relations whose strength\nis measured by cosine similarity between sentence vec-\ntors generated by averaging BioBERT and Glove em-\nbeddings and ﬁnally, graph ranking algorithms identify\nthe important sentences. Du et al. [190] introduced a\nnovel approach called BioBERTSum to summarize the\nbiomedical text. BioBERTSum uses BioBERT to encode\nsentences, transformer decoder + sigmoid to calculate\nthe scores for each sentence. The sentences with the\nhighest score are considered as the summary. Chen et al.\n[75] proposed a novel clinical text summarization based\non AlphaBERT.\n6 E VALUATION\nBenchmarks are useful to evaluate the progress in pre-\ntrained models. GLUE is the ﬁrst benchmark proposed to\nevaluate pretrained models. Following GLUE, a number\nof benchmarks are proposed in general NLP . Inspired\nby the benchmarks in general NLP , Biomedical research\ncommunity proposed benchmarks like BLUE, BLURB\nand CBLUE. We summarize the performance of various\nT-BPLMs in Table 10.\n7 CHALLENGES AND SOLUTIONS\n7.1 Low Cost Domain Adaptation\nThe two popular approaches for developing T-BPLMs\nare MDPT and DSPT. These approaches involve pre-\ntraining on large volumes of in-domain text using high-\nend GPUs or TPUs for days. These two approaches are\nquite successful in developing BPLMs. However, these\napproaches are quite expensive requiring high comput-\ning resources with long pretraining durations [122]. For\nexample, BioBERT - it took around ten days to adapt\ngeneral BERT to the biomedical domain using eight\nGPUs [16]. Moreover, DSPT is more expensive compared\nto continual pretraining as it involves learning model\nweights from scratch [122], [123]. So, there is a need for\nlost cost domain adaptation methods to adapt general\n23\nApproach Description Pros Cons\nIntermediate Fine-Tuning Model is ﬁne-tuned on source\ndataset before ﬁne-tuning on\ntarget dataset.\nAllows the model to gain do-\nmain or task-speciﬁc knowl-\nedge.\nRequirement of labeled\ndatasets.\nMulti-Task Fine-tuning Model is ﬁne-tuned on multiple\ntasks simultaneously.\nAllow the model to learn from\nmultiple tasks simultaneously.\nRequirement of labeled\ndatasets. Fine-tuning must\nbe done iteratively to identify\nthe best subset of tasks.\nData Augmentation Augment the training set using\nBack Translation or EDA tech-\nniques.\nVery simple and easy to imple-\nment.\nLabel preserving is not guaran-\nteed.\nSemi-Supervised Learning Fine-tunes the model on train-\ning instances along with pseudo\nlabeled instances\nAllows the model to lever-\nage task-related unlabelled in-\nstances.\nFine-tuning must be done iter-\natively to reduce the noisy la-\nbeled instances.\nTABLE 11. Summary of various approaches to handle small biomedical datasets using t-BPLMs.\nBERT models to the biomedical domain. Two such low-\ncost domain adaptation methods are a) Task Adaptative\nPretraining (TAPT) - It involves further pretraining on\ntask-related unlabelled instances [44]. TAPT allows the\nmodels to learn domain as well as task-speciﬁc knowl-\nedge by pretraining on a relatively small number of task-\nrelated unlabelled instances. The number of unlabelled\ninstances can be increased by retrieving task-related\nunlabelled sentences using lightweight approaches like\nVAMPIRE [191]. b) Extending embedding layer - General\nT-PLMs can be adapted to the biomedical domain by\nreﬁning the embedding layer with the addition of new\nin-domain vocabulary [122], [123]. The new in-domain\nvocabulary can be i) generated over in-domain text using\nWord2Vec and then aligned with general word-piece\nvocabulary [122] or ii) generated over in-domain text\nusing word-piece [123].\n7.2 Ontology Knowledge Injection\nModels like BioBERT, PubMedBERT have achieved good\nresults in many of the tasks. However, these models lack\nknowledge from human-curated knowledge sources.\nThese models can be further enhanced by ontology\nknowledge injection. Ontology knowledge injection can\nbe done in many ways namely a) continual pretraining\nthe models on UMLS synonyms [34], [46] or relations\n[45] or both [47] b) continual pretraining the models on\nUMLS concept deﬁnitions [192] and c) feature vector\nconstructed using ontology is added to the sequence\nvector learned by the models [174].\n7.3 Small Datasets\nPretraining on large volumes of in-domain text allows\nthe model language representations while ﬁne-tuning al-\nlows the model to learn task-speciﬁc knowledge. During\nﬁne-tuning the model must learn sufﬁcient task-speciﬁc\nknowledge to achieve good results on the task where the\ninput distribution, as well as label space, is different from\npretraining [193], [194]. With small target datasets, the\nmodels are not able to learn enough task-speciﬁc which\nlimits the performance. To over the small size of target\ndatasets, the possible solutions are\nIntermediate Fine-Tuning– Intermediate ﬁne-tuning\non large, related datasets allows the model to learn more\ndomain or task-speciﬁc knowledge which improves the\nperformance on small target datasets [55]–[58], [62], [63],\n[65], [66] and\nMulti-Task Fine-Tuning - Multi-task ﬁne-tuning al-\nlows the model to be ﬁne-tuned on multiple tasks si-\nmultaneously [67]–[69]. Here the embedding and trans-\nformer encoder layers are common for all the tasks and\neach task has a separate task-speciﬁc layer. Multi-task\nﬁne-tuning allows the model to gain domain as well as\ntask-speciﬁc reasoning knowledge from multiple tasks.\nMulti-Task ﬁne-tuning is more useful in low resource\nscenarios which are common in the biomedical domain\n[69]–[71], [73].\nData Augmentation - Data augmentation helps us to\ncreate new training instances from existing instances.\nThese newly creating training instances are close to orig-\ninal training data and helpful in low resource scenarios.\nBack translation and EDA [195] are the top popular\ntechniques for data augmentation. For example, Wang et\nal. [159] used back translation to augment the training\ninstances to train the clinical text similarity model. The\ndomain-speciﬁc ontologies like UMLS can also be used\nto augment the training instances [153].\nSemi-Supervised Learning - Semi-supervised learn-\ning augments the training set with pseudo-labeled in-\nstances. The model which is ﬁne-tuned on the original\ntraining set is used to label the task-related unlabelled in-\nstances. The model is again ﬁne-tuned on the augmented\ntraining set and this process is repeated until the model\nconverges [63], [196] . Table 11 contains a brief summary\nof these approaches.\n7.4 Robustness to Noise\nTransformed based PLMs have achieved the best results\nin many of the tasks. However, the performance of these\nmodels on noisy test instances is limited [197]–[200].\nThis is because the model is mostly trained on less\n24\nnoisy instances. As these models mostly never encounter\nnoisy instances during ﬁne-tuning, the performance is\nsigniﬁcantly reduced on noisy instances. Apart from this,\nthe noisy words in the instances are split into several\nsubtokens which affect the model learning. The robust-\nness of models is crucial particularly insensitive domains\nlike biomedical [199], [200]. Two possible solutions are\na) CharBERT [28] – replaced the WordPiece based em-\nbedding layer with CharCNN based embedding layer.\nHere word representation is generated from character\nembeddings using CharCNN. b) Adversarial Training\n[200] – Here, the training set is augmented with the noisy\ninstances. Training the model on an augmented training\nset exposes the model to noisy instances and hence the\nmodel performs better on noisy instances.\n7.5 Quality In-Domain Word Representations\nContinual pretraining allows the general T-PLMs to\nadapt to in-domain by further pretraining on large vol-\numes of in-domain text. Though the models are adapted\nto in-domain, they still contain general vocabulary. As\nthe vocabulary is learned over general text, it mostly\nincludes subwords and words which are speciﬁc to the\ngeneral domain. As a result, many of the in-domain\nwords are not represented in a meaningful way. The\ntwo possible options to represent in-domain words in\na meaningful way are a) in-domain vocabulary through\nDSPT [20] b) extending the general vocabulary with in-\ndomain vocabulary [122], [123].\n7.6 Low Resource (In-Domain Corpus) Pretraining\nCPT or DSPT involves pretraining the language model\non large volumes of in-domain text. During pretraining,\nthe model learns language representations that are useful\nacross many tasks. The size of the pretraining corpus\ninﬂuences how well the model learns the language rep-\nresentations. It is not possible to get a large volume\nof in-domain text all the time. In such scenarios with\nless in-domain corpus, the model may not learn well\nwhen trained using any of the above two methods. The\npossible solution for this is simultaneous pretraining. In\nsimultaneous pretraining [21], the model is trained on\ncombined corpora having both general and in-domain\ntext. As the in-domain text is comparatively less, up-\nsampling can be used to have a balanced pretraining.\n7.7 Quality Sequence Representation\nFor text classiﬁcation or sentence pair classiﬁcation tasks\nlike NLI and STS, Devlin et al. [2] suggested to use\nthe ﬁnal hidden vector of the special token added at\nthe beginning of the input sequence as the ﬁnal input\nsequence representation. According to Devlin et al. [2],\nthe ﬁnal hidden vector of the special token aggregates\nthe entire sequence information. The ﬁnal hidden vec-\ntor is given to a linear layer which projects into n-\ndimensional vector space whether n represents the size\nof label space. Finally, a softmax is applied to convert\nit into a vector of probabilities. However, some of the\nrecent works showed that involving all the ﬁnal hidden\nvectors using max-pooling [136], attention [171], [175],\n[176], or hierarchical convolution layers [57], [159] gives\na much better ﬁnal sequence representation compared to\nusing only special token vector.\n8 FUTURE DIRECTIONS\n8.1 Mitigating Bias\nWith the success of deep learning models in various\ntasks, deep learning-based systems are used to automate\nthe decisions like department recommendation [112],\ndisease prediction [31], [32] etc. However, these models\nare shown to exhibit bias i.e., the decisions taken by the\nmodels may favor a particular group of people compared\nto others. The main reason for unfair decisions of the\nmodels is the bias in the datasets on which the models\nare trained [124]–[126]. Real-world datasets have a bias\nin many forms. It can be based on various attributes\nlike gender, age, ethnicity, and marital status. These\nattributes are considered as protected or sensitive [201].\nFor example, in the MIMIC-III dataset [88] a) heart dis-\nease is more common in males compared to females– an\nexample of gender bias b) there are fewer clinical studies\ninvolving black patients compared to other groups –an\nexample of ethnicity bias. It is necessary to identify\nand reduce any form of bias that allows the model to\ntake fair decisions without favoring any group. There\nare few works that identiﬁed and addressed bias in\ntransformer-based biomedical language models. Zhang\net al. [127] using a simple word competition task showed\nthat SciBERT [105] exhibits ethnicity bias. Moreover,\nthe authors showed SciBERT model when further-pre-\ntrained on clinical notes exhibits performance differences\nfor different protected attributes. They further showed\nthat adversarial pretraining debiasing has little impact in\nreducing bias. Minot et al. [202] proposed an approach\nbased on data augmentation to identify and reduce\ngender bias in patient notes. This is an area that needs to\nbe explored further to improve reduce bias and improve\nthe fairness in model decisions.\n8.2 Privacy Issues\nEvery patient visit is recorded in the clinical records.\nApart from patient visits, clinical records contain the\npast and the present medical history of the patient. Such\nsensitive data should not be disclosed as it may harm\nthe patients physically or mentally [203]. Usually, the\nclinical records are shared for research purposes only\nafter de-identifying the sensitive information. However,\nit is possible to recover sensitive patient information\nfrom the de-identiﬁed medical records. Recent works\nshowed that there is data leakage from pre-trained\nmodels in the general domain i.e., it is possible to\nrecover personal information present in the pretraining\n25\ncorpora [204], [205]. Due to data leakage, the models\npre-trained on proprietary corpora, cannot be released\npublicly. Recently, Nakamura et al. [203] proposed KART\nframework which can conduct various attacks to assess\nthe leakage of sensitive information from pre-trained\nbiomedical language models. We strongly believe there\nis a need for more work in this area to assess as well as\naddress the data leakage in biomedical language models.\n8.3 Domain Adaptation\nIn the beginning, the standard approach to develop\nBPLMs is to start with general PLMs and then further\npretrain them on large volumes of biomedical text [16].\nThe main drawback of this approach is the lack of in-\ndomain vocabulary. Without domain-speciﬁc vocabulary,\nmany of the in-domain are split into a number of sub-\nwords which hinders model learning during pretraining\nor ﬁne-tuning. Moreover, continual pretraining is quite\nexpensive as it involves pretraining on large volumes of\nunlabeled text. To overcome these drawbacks, there are\nlow-cost domain adaptation approaches that extend the\ngeneral domain vocabulary with in-domain vocabulary\n[122], [123]. The extra in-domain vocabulary is generated\nusing Word2vec and then aligned [122] or generated\ndirectly using WordPiece [123] over biomedical text. The\nmain drawback in these low-cost domain adaptation\napproaches is an increase in the size of the model with\nthe addition of in-domain vocabulary. Further research\non this topic can result in more novel methods for low-\ncost domain adaptation.\n8.4 Novel Pretraining Tasks\nMost of the biomedical language models (except\nELECTRA-based models) are pre-trained using MLM.\nIn MLM, only 15% of tokens are randomly masked\nand the model learns by predicting that 15% of masked\ntokens only. Here the main drawbacks are a) as tokens\nare randomly chosen for masking, the model may not\nlearn much by predicting random tokens b) as only\n15% of tokens are predicted, the training signal per\nexample is less. So, the model has to see more examples\nto learn enough language information which results\nin the requirement of large pretraining corpora and\nmore computational resources. There is a need for novel\npretraining tasks like Replaced Token Detection (RTD)\nwhich can provide more training signal per example.\nMoreover, when the model is pretrained using multiple\npretraining tasks, the model receives more training sig-\nnals per example and hence can learn enough language\ninformation using less pretraining corpora and compu-\ntational resources [206].\n8.5 Benchmarks\nIn general, a benchmark is a tool to evaluate the perfor-\nmance of models across different NLP tasks. A bench-\nmark is required because we expect the pre-trained lan-\nguage models to be general and robust i.e., the models\nperform well across tasks rather than on one or two\nspeciﬁc tasks. A benchmark with one or more datasets\nfor multiple NLP tasks helps to assess the general ability\nand robustness of models. In general domain, we have\na number of benchmarks like GLUE [207] and Super-\nGLUE [208] (general language understanding), XGLUE\n[209] (cross lingual language understanding) and LinCE\n[210] (code switching). In biomedical domain there are\nthree benchmarks namely BLUE [18], BLURB [20] and\nChineseBLUE [48]. BLUE introduced by Peng et al. [18]\ncontains ten datasets for ﬁve biomedical NLP tasks,\nwhile BLURB contains thirteen datasets for six tasks and\nChineseBLUE contains eight tasks with nine datasets.\nBLUE and ChineseBLUE include both EHR and sci-\nentiﬁc literature-based datasets, while BLURB includes\nonly biomedical scientiﬁc literature-based datasets. The\nsemantics of EHR and medical social media texts are\ndifferent from biomedical scientiﬁc literature. So, there\nis a need of exclusive benchmarks for EHR and medical\nsocial media-based datasets.\n8.6 Intrinsic Probes\nDuring pretraining, PLMs learn syntactic, semantic\nknowledge along with factual and common-sense\nknowledge available in the pretraining corpus [14]. In-\ntrinsic probes through light on the knowledge learned\nby PLMs during pretraining. In general NLP , researchers\nproposed several intrinsic probes like LAMA, Negated\nand Misprimed LAMA [211], XLAMA [212], X-FACTR\n[213], MickeyProbe [214] to understand the knowledge\nencoded in pretrained models. For example, LAMA [211]\nprobes the factual and common-sense knowledge of En-\nglish pretrained models, while X-FACTR [213] probes the\nfactual knowledge of multi-lingual pretrained models.\nHowever, there is no such intrinsic probes in Biomedical\ndomain to through light on the knowledge learned by\nBPLMs during pretraining. This is an area which requires\nmuch attention from Biomedical NLP community.\n8.7 Efﬁcient Models\nPretraining provides BPLMs with necessary background\nknowledge which can be transferred to downstream\ntasks. However, pretraining is computationally very ex-\npensive and also requires large volumes of pretraining\ndata. So, there is need of novel model architecture which\nreduces the pretraining time as well as the amount of\npretraining corpus. In general NLP , recently efﬁcient\nmodels like ConvBERT [215] and DeBERTa [216] are pro-\nposed which reduces the pretraining time and amount\nof pretraining corpus required respectively. DeBERTa\nwith two novel improvements (Disentangled attention\nmechanism and enhanced masked decoder) achieves\nbetter compared to RoBERTa. DeBERTa is pretrained\non just 78GB of data while RoBERTa is pretrained on\n160GB of data. ConvBERT with mixed attention block\noutperforms ELECTRA while using just 1/4th of its\npretraining cost. Biomedical NLP research community\n26\nmust focus on developing pretrained models based on\nthese novel model architectures.\n9 L IMITATIONS\nWe have comprehensively covered the research works\nrelated to T-BPLMs. As our focus is on T-BPLMs, we\nhave not included any papers related to context insen-\nsitive biomedical embeddings. For detailed information\nregarding context insensitive biomedical embeddings,\nplease refer the survey paper written by Kalyan and\nSangeetha [22]. As it is a survey focused on T-BPLMs, we\nhave covered the foundation concepts like transformers\nand self-supervised learning in a very brief way only.\n10 C ONCLUSION\nHere, we present the recent trends in transformer-based\nBPLMs. We explain various core concepts like pre-\ntraining methods, pretraining tasks, ﬁne-tuning meth-\nods and embedding types. We present a taxonomy for\ntransformer-based BPLMs. Finally, we discuss some of\nthe challenges and possible solutions and ﬁnally con-\nclude with a discussion on open issues.\nACKNOWLEDGMENTS\nKalyan would like to thank his father Katikapalli Sub-\nramanyam for giving a) $750 to buy a new laptop, 24-\ninch monitor and study table. b) $180 for one year sub-\nscription of Medium, Overleaf and Edraw MindMaster\nsoftware. Edraw MindMaster is used to create all the\ndiagrams in the paper.\nREFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in neural information processing systems , 2017, pp.\n5998–6008.\n[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language un-\nderstanding,” in Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers) ,\n2019, pp. 4171–4186.\n[3] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A ro-\nbustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692, 2019.\n[4] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P . J. Liu, “Exploring the limits of transfer\nlearning with a uniﬁed text-to-text transformer,” Journal of Ma-\nchine Learning Research , vol. 21, no. 140, pp. 1–67, 2020.\n[5] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE\nTransactions on knowledge and data engineering , vol. 22, no. 10, pp.\n1345–1359, 2009.\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classi-\nﬁcation with deep convolutional neural networks,” Advances in\nneural information processing systems, vol. 25, pp. 1097–1105, 2012.\n[7] C. Szegedy, W. Liu, Y. Jia, P . Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V . Vanhoucke, and A. Rabinovich, “Going deeper with\nconvolutions,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition , 2015, pp. 1–9.\n[8] P . Blunsom, E. Grefenstette, and N. Kalchbrenner, “A convolu-\ntional neural network for modelling sentences,” in Proceedings\nof the 52nd Annual Meeting of the Association for Computational\nLinguistics. Proceedings of the 52nd Annual Meeting of the\nAssociation for Computational . . . , 2014.\n[9] P . Liu, X. Qiu, and X. Huang, “Recurrent neural network for\ntext classiﬁcation with multi-task learning,” in Proceedings of the\nTwenty-Fifth International Joint Conference on Artiﬁcial Intelligence ,\n2016, pp. 2873–2879.\n[10] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient esti-\nmation of word representations in vector space,” arXiv preprint\narXiv:1301.3781, 2013.\n[11] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vec-\ntors for word representation,” in Proceedings of the 2014 conference\non empirical methods in natural language processing (EMNLP) , 2014,\npp. 1532–1543.\n[12] P . Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching\nword vectors with subword information,” Transactions of the\nAssociation for Computational Linguistics, vol. 5, pp. 135–146, 2017.\n[13] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained\nmodels for natural language processing: A survey,” Science China\nTechnological Sciences, pp. 1–26, 2020.\n[14] K. S. Kalyan, A. Rajasekharan, and S. Sangeetha, “Ammus:\nA survey of transformer-based pretrained models in natural\nlanguage processing,” arXiv preprint arXiv:2108.05542 , 2021.\n[15] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P . Sharma, and\nR. Soricut, “Albert: A lite bert for self-supervised learning of\nlanguage representations,” in International Conference on Learning\nRepresentations, 2019.\n[16] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and\nJ. Kang, “Biobert: a pre-trained biomedical language represen-\ntation model for biomedical text mining,” Bioinformatics, vol. 36,\nno. 4, pp. 1234–1240, 2020.\n[17] E. Alsentzer, J. Murphy, W. Boag, W.-H. Weng, D. Jindi, T. Nau-\nmann, and M. McDermott, “Publicly available clinical bert em-\nbeddings,” in Proceedings of the 2nd Clinical Natural Language\nProcessing Workshop, 2019, pp. 72–78.\n[18] Y. Peng, S. Yan, and Z. Lu, “Transfer learning in biomedical\nnatural language processing: An evaluation of bert and elmo\non ten benchmarking datasets,” in Proceedings of the 18th BioNLP\nWorkshop and Shared Task , 2019, pp. 58–65.\n[19] K. Huang, A. Singh, S. Chen, E. Moseley, C.-Y. Deng, N. George,\nand C. Lindvall, “Clinical xlnet: Modeling sequential clinical\nnotes and predicting prolonged mechanical ventilation,” in Pro-\nceedings of the 3rd Clinical Natural Language Processing Workshop ,\n2020, pp. 94–100.\n[20] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu,\nT. Naumann, J. Gao, and H. Poon, “Domain-speciﬁc language\nmodel pretraining for biomedical natural language processing,”\narXiv preprint arXiv:2007.15779 , 2020.\n[21] S. Wada, T. Takeda, S. Manabe, S. Konishi, J. Kamohara, and\nY. Matsumura, “Pre-training technique to localize medical bert\nand enhance biomedical bert,” arXiv preprint arXiv:2005.07202 ,\n2020.\n[22] K. S. Kalyan and S. Sangeetha, “Secnlp: A survey of embeddings\nin clinical natural language processing,” Journal of biomedical\ninformatics, vol. 101, p. 103323, 2020.\n[23] B. Chiu and S. Baker, “Word embeddings for biomedical natural\nlanguage processing: A survey,” Language and Linguistics Com-\npass, vol. 14, no. 12, p. e12402, 2020.\n[24] F. K. Khattak, S. Jeblee, C. Pou-Prom, M. Abdalla, C. Meaney,\nand F. Rudzicz, “A survey of word embeddings for clinical text,”\nJournal of Biomedical Informatics: X , vol. 4, p. 100057, 2019.\n[25] Y. Wang, S. Liu, N. Afzal, M. Rastegar-Mojarad, L. Wang,\nF. Shen, P . Kingsbury, and H. Liu, “A comparison of word\nembeddings for the biomedical natural language processing,”\nJournal of biomedical informatics , vol. 87, pp. 12–20, 2018.\n[26] Q. Liu, M. J. Kusner, and P . Blunsom, “A survey on contextual\nembeddings,” arXiv preprint arXiv:2003.07278 , 2020.\n[27] Y. Li, S. Rao, J. R. A. Solares, A. Hassaine, R. Ramakrish-\nnan, D. Canoy, Y. Zhu, K. Rahimi, and G. Salimi-Khorshidi,\n“Behrt: transformer for electronic health records,” Scientiﬁc re-\nports, vol. 10, no. 1, pp. 1–12, 2020.\n[28] H. El Boukkouri, O. Ferret, T. Lavergne, H. Noji, P . Zweigen-\nbaum, and J. Tsujii, “Characterbert: Reconciling elmo and bert for\nword-level open-vocabulary representations from characters,” in\n27\nProceedings of the 28th International Conference on Computational\nLinguistics, 2020, pp. 6903–6915.\n[29] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush, “Character-aware\nneural language models,” in Thirtieth AAAI conference on artiﬁcial\nintelligence, 2016.\n[30] Y. Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey et al., “Google’s neural\nmachine translation system: Bridging the gap between human\nand machine translation,” arXiv preprint arXiv:1609.08144 , 2016.\n[31] L. Rasmy, Y. Xiang, Z. Xie, C. Tao, and D. Zhi, “Med-bert:\npretrained contextualized embeddings on large-scale structured\nelectronic health records for disease prediction,” NPJ digital\nmedicine, vol. 4, no. 1, pp. 1–13, 2021.\n[32] Y. Meng, W. F. Speier, M. K. Ong, and C. Arnold, “Bidirectional\nrepresentation learning from transformers using multimodal\nelectronic health record data to predict depression,” IEEE Journal\nof Biomedical and Health Informatics , 2021.\n[33] D. Hendrycks and K. Gimpel, “Gaussian error linear units\n(gelus),” arXiv preprint arXiv:1606.08415 , 2016.\n[34] X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and\nJ. Tang, “Self-supervised learning: Generative or contrastive,”\nIEEE Transactions on Knowledge and Data Engineering , 2021.\n[35] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and\nM. Shah, “Transformers in vision: A survey,” arXiv preprint\narXiv:2101.01169, 2021.\n[36] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang,\nA. Xiao, C. Xu, Y. Xu et al. , “A survey on visual transformer,”\narXiv preprint arXiv:2012.12556 , 2020.\n[37] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:\nA framework for self-supervised learning of speech representa-\ntions,” Advances in Neural Information Processing Systems , vol. 33,\n2020.\n[38] A. Sivaraman and M. Kim, “Self-supervised learning from con-\ntrastive mixtures for personalized speech enhancement,” arXiv\npreprint arXiv:2011.03426, 2020.\n[39] X. Yang, J. Bian, W. R. Hogan, and Y. Wu, “Clinical concept\nextraction using transformers,” Journal of the American Medical\nInformatics Association, vol. 27, no. 12, pp. 1935–1942, 2020.\n[40] Y. Si, J. Wang, H. Xu, and K. Roberts, “Enhancing clinical concept\nextraction with contextual embeddings,” Journal of the American\nMedical Informatics Association , vol. 26, no. 11, pp. 1297–1304,\n2019.\n[41] W. Antoun, F. Baly, and H. Hajj, “Arabert: Transformer-based\nmodel for arabic language understanding,” in LREC 2020 Work-\nshop Language Resources and Evaluation Conference 11–16 May 2020,\np. 9.\n[42] Y. Kawazoe, D. Shibata, E. Shinohara, E. Aramaki, and K. Ohe,\n“A clinical speciﬁc bert developed with huge size of japanese\nclinical narrative,” medRxiv, 2020.\n[43] P . Lewis, M. Ott, J. Du, and V . Stoyanov, “Pretrained language\nmodels for biomedical and clinical tasks: Understanding and\nextending the state-of-the-art,” in Proceedings of the 3rd Clinical\nNatural Language Processing Workshop , 2020, pp. 146–157.\n[44] S. Gururangan, A. Marasovi ´c, S. Swayamdipta, K. Lo, I. Beltagy,\nD. Downey, and N. A. Smith, “Don’t stop pretraining: Adapt\nlanguage models to domains and tasks,” in Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics ,\n2020, pp. 8342–8360.\n[45] B. Hao, H. Zhu, and I. Paschalidis, “Enhancing clinical bert\nembedding using a biomedical knowledge base,” in Proceedings\nof the 28th international conference on computational linguistics, 2020,\npp. 657–661.\n[46] G. Michalopoulos, Y. Wang, H. Kaka, H. Chen, and A. Wong,\n“Umlsbert: Clinical domain knowledge augmentation of con-\ntextual embeddings using the uniﬁed medical language system\nmetathesaurus,” in Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2021, pp. 1744–1753.\n[47] Z. Yuan, Z. Zhao, and S. Yu, “Coder: Knowledge infused cross-\nlingual medical term embedding for term normalization,” arXiv\npreprint arXiv:2011.02947, 2020.\n[48] N. Zhang, Q. Jia, K. Yin, L. Dong, F. Gao, and N. Hua, “Con-\nceptualized representation learning for chinese biomedical text\nmining,” arXiv preprint arXiv:2008.10813 , 2020.\n[49] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and\nO. Levy, “Spanbert: Improving pre-training by representing and\npredicting spans,” Transactions of the Association for Computational\nLinguistics, vol. 8, pp. 64–77, 2020.\n[50] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, “Electra: Pre-\ntraining text encoders as discriminators rather than generators,”\nin International Conference on Learning Representations , 2019.\n[51] Y. Cui, W. Che, T. Liu, B. Qin, Z. Yang, S. Wang, and G. Hu,\n“Pre-training with whole word masking for chinese bert,” arXiv\npreprint arXiv:1906.08101, 2019.\n[52] G. Pergola, E. Kochkina, L. Gui, M. Liakata, and Y. He, “Boosting\nlow-resource biomedical qa via entity-aware masking strate-\ngies,” in Proceedings of the 16th Conference of the European Chapter\nof the Association for Computational Linguistics: Main Volume , 2021,\npp. 1977–1985.\n[53] B. Portelli, E. Lenzi, E. Chersoni, G. Serra, and E. Santus, “Bert\nprescriptions to avoid unwanted headaches: A comparison of\ntransformer architectures for adverse drug event detection,” in\nProceedings of the 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main Volume , 2021, pp.\n1740–1747.\n[54] O. Bodenreider, “The uniﬁed medical language system (umls):\nintegrating biomedical terminology,” Nucleic acids research ,\nvol. 32, no. suppl 1, pp. D267–D270, 2004.\n[55] C. Cengiz, U. Sert, and D. Yuret, “Ku ai at mediqa 2019: Domain-\nspeciﬁc pre-training and transfer learning for medical nli,” in\nProceedings of the 18th BioNLP Workshop and Shared Task , 2019,\npp. 427–436.\n[56] X. Yang, X. He, H. Zhang, Y. Ma, J. Bian, and Y. Wu, “Measure-\nment of semantic textual similarity in clinical texts: Comparison\nof transformer-based models,” JMIR medical informatics , vol. 8,\nno. 11, p. e19735, 2020.\n[57] Y. Wang, K. Verspoor, and T. Baldwin, “Learning from unlabelled\ndata for clinical semantic textual similarity,” in Proceedings of the\n3rd Clinical Natural Language Processing Workshop , 2020, pp. 227–\n233.\n[58] W. Yoon, J. Lee, D. Kim, M. Jeong, and J. Kang, “Pre-trained lan-\nguage model for biomedical question answering,” arXiv preprint\narXiv:1909.08229, 2019.\n[59] S. Bowman, G. Angeli, C. Potts, and C. D. Manning, “A large\nannotated corpus for learning natural language inference,” in\nProceedings of the 2015 Conference on Empirical Methods in Natural\nLanguage Processing, 2015, pp. 632–642.\n[60] A. Williams, N. Nangia, and S. Bowman, “A broad-coverage\nchallenge corpus for sentence understanding through inference,”\nin Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers) , 2018, pp. 1112–1122.\n[61] A. Romanov and C. Shivade, “Lessons from natural language\ninference in the clinical domain,” in Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing ,\n2018, pp. 1586–1596.\n[62] C. Sun, Z. Yang, L. Wang, Y. Zhang, H. Lin, and J. Wang,\n“Biomedical named entity recognition using bert in the machine\nreading comprehension framework,” Journal of Biomedical Infor-\nmatics, vol. 118, p. 103799, 2021.\n[63] S. Gao, O. Kotevska, A. Sorokine, and J. B. Christian, “A pre-\ntraining and self-training approach for biomedical named entity\nrecognition,” PloS one, vol. 16, no. 2, p. e0246310, 2021.\n[64] S. Mohan and D. Li, “Medmentions: A large biomedical corpus\nannotated with umls concepts,” in Automated Knowledge Base\nConstruction (AKBC), 2018.\n[65] C. McCreery, N. Katariya, A. Kannan, M. Chablani, and X. Ama-\ntriain, “Domain-relevant embeddings for medical question sim-\nilarity,” arXiv preprint arXiv:1910.04192 , 2019.\n[66] M. Jeong, M. Sung, G. Kim, D. Kim, W. Yoon, J. Yoo, and J. Kang,\n“Transferability of natural language inference to biomedical\nquestion answering,” arXiv preprint arXiv:2007.00217 , 2020.\n[67] X. Liu, P . He, W. Chen, and J. Gao, “Multi-task deep neural\nnetworks for natural language understanding,” in Proceedings\nof the 57th Annual Meeting of the Association for Computational\nLinguistics, 2019, pp. 4487–4496.\n[68] Y. Zhang and Q. Yang, “A survey on multi-task learning,” IEEE\nTransactions on Knowledge and Data Engineering , 2021.\n[69] M. R. Khan, M. Ziyadi, and M. AbdelHady, “Mt-bioner: Multi-\ntask learning for biomedical named entity recognition using\ndeep bidirectional transformers,” arXiv preprint arXiv:2001.08904,\n2020.\n28\n[70] A. Mulyar and B. T. McInnes, “Mt-clinical bert: scaling clinical\ninformation extraction with multitask learning,” arXiv preprint\narXiv:2004.10220, 2020.\n[71] D. Mahajan, A. Poddar, J. J. Liang, Y.-T. Lin, J. M. Prager,\nP . Suryanarayanan, P . Raghavan, and C.-H. Tsou, “Identiﬁca-\ntion of semantically similar sentences in clinical notes: Iterative\nintermediate training using multi-task learning,” JMIR medical\ninformatics, vol. 8, no. 11, p. e22508, 2020.\n[72] Y. Wang, S. Fu, F. Shen, S. Henry, O. Uzuner, and H. Liu, “The\n2019 n2c2/ohnlp track on clinical semantic textual similarity:\noverview,” JMIR Medical Informatics , vol. 8, no. 11, p. e23375,\n2020.\n[73] Y. Peng, Q. Chen, and Z. Lu, “An empirical study of multi-task\nlearning on bert for biomedical text mining,” in Proceedings of\nthe 19th SIGBioMed Workshop on Biomedical Language Processing ,\n2020, pp. 205–214.\n[74] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark,\nK. Lee, and L. Zettlemoyer, “Deep contextualized word repre-\nsentations,” in Proceedings of NAACL-HLT, 2018, pp. 2227–2237.\n[75] Y.-P . Chen, Y.-Y. Chen, J.-J. Lin, C.-H. Huang, and F. Lai, “Modi-\nﬁed bidirectional encoder representations from transformers ex-\ntractive summarization model for hospital information systems\nbased on character-level tokens (alphabert): development and\nperformance evaluation,” JMIR medical informatics , vol. 8, no. 4,\np. e17787, 2020.\n[76] R. Sennrich, B. Haddow, and A. Birch, “Neural machine trans-\nlation of rare words with subword units,” in Proceedings of the\n54th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) , 2016, pp. 1715–1725.\n[77] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever\net al., “Language models are unsupervised multitask learners.”\n[78] T. Kudo, “Subword regularization: Improving neural network\ntranslation models with multiple subword candidates,” in Pro-\nceedings of the 56th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , 2018, pp. 66–75.\n[79] T. Kudo and J. Richardson, “Sentencepiece: A simple and lan-\nguage independent subword tokenizer and detokenizer for neu-\nral text processing,” in Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: System Demon-\nstrations, 2018, pp. 66–71.\n[80] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a\ndistilled version of bert: smaller, faster, cheaper and lighter,”\narXiv preprint arXiv:1910.01108 , 2019.\n[81] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V . Le, “Xlnet: Generalized autoregressive pretraining for lan-\nguage understanding,” Advances in neural information processing\nsystems, vol. 32, 2019.\n[82] D. Charles, M. Gabriel, and M. F. Furukawa, “Adoption of\nelectronic health record systems among us non-federal acute care\nhospitals: 2008-2014,” ONC data brief , vol. 9, pp. 1–9, 2013.\n[83] G. S. Birkhead, M. Klompas, and N. R. Shah, “Uses of electronic\nhealth records for public health surveillance to advance public\nhealth,” Annual review of public health , vol. 36, pp. 345–359, 2015.\n[84] P . B. Jensen, L. J. Jensen, and S. Brunak, “Mining electronic health\nrecords: towards better research applications and clinical care,”\nNature Reviews Genetics , vol. 13, no. 6, pp. 395–405, 2012.\n[85] D. Demner-Fushman, W. W. Chapman, and C. J. McDonald,\n“What can natural language processing do for clinical decision\nsupport?” Journal of biomedical informatics , vol. 42, no. 5, pp. 760–\n772, 2009.\n[86] T. Botsis, G. Hartvigsen, F. Chen, and C. Weng, “Secondary use of\nehr: data quality issues and informatics opportunities,” Summit\non Translational Bioinformatics, vol. 2010, p. 1, 2010.\n[87] M. Saeed, M. Villarroel, A. T. Reisner, G. Clifford, L.-W. Lehman,\nG. Moody, T. Heldt, T. H. Kyaw, B. Moody, and R. G. Mark,\n“Multiparameter intelligent monitoring in intensive care ii\n(mimic-ii): a public-access intensive care unit database,” Critical\ncare medicine, vol. 39, no. 5, p. 952, 2011.\n[88] A. E. Johnson, T. J. Pollard, L. Shen, H. L. Li-Wei, M. Feng,\nM. Ghassemi, B. Moody, P . Szolovits, L. A. Celi, and R. G. Mark,\n“Mimic-iii, a freely accessible critical care database,” Scientiﬁc\ndata, vol. 3, no. 1, pp. 1–9, 2016.\n[89] X. Meng, C. H. Ganoe, R. T. Sieberg, Y. Y. Cheung, and S. Has-\nsanpour, “Self-supervised contextual language representation of\nradiology reports to improve the identiﬁcation of communica-\ntion urgency,” AMIA Summits on Translational Science Proceedings,\nvol. 2020, p. 413, 2020.\n[90] S. Hassanpour and C. P . Langlotz, “Information extraction from\nmulti-institutional radiology reports,” Artiﬁcial intelligence in\nmedicine, vol. 66, pp. 29–39, 2016.\n[91] K. K. Bressem, L. C. Adams, R. A. Gaudin, D. Tr ¨oltzsch,\nB. Hamm, M. R. Makowski, C.-Y. Sch¨ule, J. L. Vahldiek, and S. M.\nNiehues, “Highly accurate classiﬁcation of chest radiographic\nreports using a deep learning natural language model pre-\ntrained on 3.8 million text reports,” Bioinformatics, vol. 36, no. 21,\npp. 5255–5261, 2020.\n[92] M. M ¨uller, M. Salath ´e, and P . E. Kummervold, “Covid-twitter-\nbert: A natural language processing model to analyse covid-19\ncontent on twitter,” arXiv preprint arXiv:2005.07503 , 2020.\n[93] D. Q. Nguyen, T. Vu, and A. T. Nguyen, “Bertweet: A pre-trained\nlanguage model for english tweets,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nSystem Demonstrations, 2020, pp. 9–14.\n[94] M. Basaldella, F. Liu, E. Shareghi, and N. Collier, “Cometa:\nA corpus for medical entity linking in the social media,” in\nProceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , 2020, pp. 3122–3137.\n[95] E. Tutubalina, I. Alimova, Z. Miftahutdinov, A. Sakhovskiy,\nV . Malykh, and S. Nikolenko, “The russian drug reaction corpus\nand neural models for drug reactions and effectiveness detection\nin user reviews,” Bioinformatics, vol. 37, no. 2, pp. 243–249, 2021.\n[96] U. Naseem, M. Khushi, V . Reddy, S. Rajendran, I. Razzak, and\nJ. Kim, “Bioalbert: A simple and effective pre-trained language\nmodel for biomedical named entity recognition,” arXiv preprint\narXiv:2009.09223, 2020.\n[97] E. T. R. Schneider, J. V . A. de Souza, J. Knafou, L. E. S. e Oliveira,\nJ. Copara, Y. B. Gumiel, L. F. A. de Oliveira, E. C. Paraiso,\nD. Teodoro, and C. M. C. M. Barra, “Biobertpt-a portuguese\nneural language model for clinical named entity recognition,”\nin Proceedings of the 3rd Clinical Natural Language Processing\nWorkshop, 2020, pp. 65–72.\n[98] I. B. Ozyurt, “On the effectiveness of small, discriminatively\npre-trained language representation models for biomedical text\nmining,” in Proceedings of the First Workshop on Scholarly Document\nProcessing, 2020, pp. 104–112.\n[99] H.-C. Shin, Y. Zhang, E. Bakhturina, R. Puri, M. Patwary,\nM. Shoeybi, and R. Mani, “Bio-megatron: Larger biomedical\ndomain language model,” in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , 2020,\npp. 4700–4706.\n[100] S. Chakraborty, E. Bisong, S. Bhatt, T. Wagner, R. Elliott, and\nF. Mosconi, “Biomedbert: A pre-trained biomedical language\nmodel for qa and ir,” in Proceedings of the 28th International\nConference on Computational Linguistics , 2020, pp. 669–679.\n[101] G. Miolo, G. Mantoan, and C. Orsenigo, “Electramed: a new\npre-trained language representation model for biomedical nlp,”\narXiv preprint arXiv:2104.09585 , 2021.\n[102] K. raj Kanakarajan, B. Kundumani, and M. Sankarasubbu, “Bio-\nelectra: Pretrained biomedical text encoder using discrimina-\ntors,” in Proceedings of the 20th Workshop on Biomedical Language\nProcessing, 2021, pp. 143–154.\n[103] U. Naseem, A. G. Dunn, M. Khushi, and J. Kim, “Benchmarking\nfor biomedical natural language processing tasks with a domain\nspeciﬁc albert,” arXiv preprint arXiv:2107.04374 , 2021.\n[104] N. Boudjellal, H. Zhang, A. Khan, A. Ahmad, R. Naseem,\nJ. Shang, and L. Dai, “Abioner: a bert-based model for ara-\nbic biomedical named-entity recognition,” Complexity, vol. 2021,\n2021.\n[105] I. Beltagy, K. Lo, and A. Cohan, “Scibert: A pretrained language\nmodel for scientiﬁc text,” in Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), 2019, pp. 3615–3620.\n[106] L. N. Phan, J. T. Anibal, H. Tran, S. Chanana, E. Bahadroglu,\nA. Peltekian, and G. Altan-Bonnet, “Sciﬁve: a text-to-text\ntransformer model for biomedical literature,” arXiv preprint\narXiv:2106.03598, 2021.\n[107] K. K. Subramanyam and S. Sangeetha, “Deep contextualized\nmedical concept normalization in social media text,” Procedia\nComputer Science, vol. 171, pp. 1353–1362, 2020.\n[108] K. S. Kalyan and S. Sangeetha, “Medical concept normalization\nin user-generated texts by learning target concept embeddings,”\nin Proceedings of the 11th International Workshop on Health Text\nMining and Information Analysis , 2020, pp. 18–23.\n29\n[109] K. O’Connor, P . Pimpalkhute, A. Nikfarjam, R. Ginn, K. L. Smith,\nand G. Gonzalez, “Pharmacovigilance on twitter? mining tweets\nfor adverse drug reactions,” in AMIA annual symposium proceed-\nings, vol. 2014. American Medical Informatics Association, 2014,\np. 924.\n[110] N. Limsopatham and N. Collier, “Adapting phrase-based ma-\nchine translation to normalise medical terms in social media\nmessages,” in Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing , 2015, pp. 1675–1680.\n[111] L. Akhtyamova, “Named entity recognition in spanish biomed-\nical literature: Short review and bert model,” in 2020 26th\nConference of Open Innovations Association (FRUCT) . IEEE, 2020,\npp. 1–7.\n[112] J. Wang, G. Zhang, W. Wang, K. Zhang, and Y. Sheng, “Cloud-\nbased intelligent self-diagnosis and department recommendation\nservice using chinese medical bert,” Journal of Cloud Computing ,\nvol. 10, no. 1, pp. 1–12, 2021.\n[113] J. Copara, J. Knafou, N. Naderi, C. Moro, P . Ruch, and\nD. Teodoro, “Contextualized french language models for\nbiomedical named entity recognition,” in Proceedings of the 6th\njoint conference Journ ’e es d’ etudes sur la parole (JEP , 33rd ’e dition),\nAutomatic Processing of Natural Languages (TALN, 27th ’e dition),\nMeeting of ´E Research Students in Computer Science for Automatic\nLanguage Processing (R ’E CITAL, 22e ´ e dition). Workshop D ’E ﬁ\nText Excavation, 2020, pp. 36–48.\n[114] L. Martin, B. Muller, P . J. O. Su ´arez, Y. Dupont, L. Romary, ´E. V .\nDe La Clergerie, D. Seddah, and B. Sagot, “Camembert: a tasty\nfrench language model,” in Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics , 2020, pp. 7203–\n7219.\n[115] N. Taghizadeh, E. Doostmohammadi, E. Seifossadat, H. R.\nRabiee, and M. S. Tahaei, “Sina-bert: A pre-trained language\nmodel for analysis of medical texts in persian,” arXiv preprint\narXiv:2104.07613, 2021.\n[116] M. Farahani, M. Gharachorloo, M. Farahani, and M. Manthouri,\n“Parsbert: Transformer-based model for persian language under-\nstanding,” arXiv preprint arXiv:2005.12515 , 2020.\n[117] G. L ´opez-Garc´ıa, J. M. Jerez, N. Ribelles, E. Alba, and F. J.\nVeredas, “Transformers for clinical coding in spanish,” IEEE\nAccess, vol. 9, pp. 72 387–72 397, 2021.\n[118] J. Canete, G. Chaperon, R. Fuentes, and J. P ´erez, “Spanish pre-\ntrained bert model and evaluation data,” Pml4dc at iclr, vol. 2020,\n2020.\n[119] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wen-\nzek, F. Guzm ´an, ´E. Grave, M. Ott, L. Zettlemoyer, and V . Stoy-\nanov, “Unsupervised cross-lingual representation learning at\nscale,” in Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , 2020, pp. 8440–8451.\n[120] F. Liu, E. Shareghi, Z. Meng, M. Basaldella, and N. Collier, “Self-\nalignment pretraining for biomedical entity representations,” in\nProceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, 2021, pp. 4228–4238.\n[121] Z. Yuan, Y. Liu, C. Tan, S. Huang, and F. Huang, “Improving\nbiomedical pretrained language models with knowledge,” in\nProceedings of the 20th Workshop on Biomedical Language Processing,\n2021, pp. 180–190.\n[122] N. Poerner, U. Waltinger, and H. Sch ¨utze, “Inexpensive do-\nmain adaptation of pretrained language models: Case studies\non biomedical ner and covid-19 qa,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nFindings, 2020, pp. 1482–1490.\n[123] W. Tai, H. Kung, X. L. Dong, M. Comiter, and C.-F. Kuo, “exbert:\nExtending pre-trained models with domain-speciﬁc vocabulary\nunder constrained training resources,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nFindings, 2020, pp. 1433–1439.\n[124] C. Meng, L. Trinh, N. Xu, and Y. Liu, “Mimic-if: Interpretability\nand fairness evaluation of deep learning models on mimic-iv\ndataset,” arXiv preprint arXiv:2102.06761 , 2021.\n[125] I. Y. Chen, P . Szolovits, and M. Ghassemi, “Can ai help reduce\ndisparities in general medical and mental health care?” AMA\njournal of ethics , vol. 21, no. 2, pp. 167–179, 2019.\n[126] K.-H. Yu and I. S. Kohane, “Framing the challenges of artiﬁcial\nintelligence in medicine,” BMJ quality & safety , vol. 28, no. 3, pp.\n238–241, 2019.\n[127] H. Zhang, A. X. Lu, M. Abdalla, M. McDermott, and M. Ghas-\nsemi, “Hurtful words: quantifying biases in clinical contextual\nword embeddings,” in proceedings of the ACM Conference on\nHealth, Inference, and Learning , 2020, pp. 110–120.\n[128] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid,\n“Videobert: A joint model for video and language representation\nlearning,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision , 2019, pp. 7464–7473.\n[129] C. Sun, F. Baradel, K. Murphy, and C. Schmid, “Learning video\nrepresentations using contrastive bidirectional transformer,”\narXiv preprint arXiv:1906.05743 , 2019.\n[130] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, “Vl-\nbert: Pre-training of generic visual-linguistic representations,” in\nInternational Conference on Learning Representations , 2019.\n[131] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: pretraining task-\nagnostic visiolinguistic representations for vision-and-language\ntasks,” in Proceedings of the 33rd International Conference on Neural\nInformation Processing Systems , 2019, pp. 13–23.\n[132] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder\nrepresentations from transformers,” in Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019, pp. 5100–5111.\n[133] G. Liu, Y. Liao, F. Wang, B. Zhang, L. Zhang, X. Liang, X. Wan,\nS. Li, Z. Li, S. Zhang et al. , “Medical-vlbert: Medical visual\nlanguage bert for covid-19 ct report generation with alternate\nlearning,” IEEE Transactions on Neural Networks and Learning\nSystems, 2021.\n[134] M. Monajatipoor, M. Rouhsedaghat, L. H. Li, A. Chien, C.-C. J.\nKuo, F. Scalzo, and K.-W. Chang, “Berthop: An effective vision-\nand-language model for chest x-ray disease diagnosis,” arXiv\npreprint arXiv:2108.04938, 2021.\n[135] Y. Chen, M. Rouhsedaghat, S. You, R. Rao, and C.-C. J. Kuo,\n“Pixelhop++: A small successive-subspace-learning-based (ssl-\nbased) model for image classiﬁcation,” in 2020 IEEE International\nConference on Image Processing (ICIP). IEEE, 2020, pp. 3294–3298.\n[136] N. Reimers and I. Gurevych, “Sentence-bert: Sentence embed-\ndings using siamese bert-networks,” in Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019, pp. 3982–3992.\n[137] K. S. Kalyan and S. Sangeetha, “Target concept guided medical\nconcept normalization in noisy user-generated texts,” in Proceed-\nings of Deep Learning Inside Out (DeeLIO): The First Workshop on\nKnowledge Extraction and Integration for Deep Learning Architec-\ntures, 2020, pp. 64–73.\n[138] ——, “Social media medical concept normalization using roberta\nin ontology enriched text similarity framework,” in Proceedings\nof Knowledgeable NLP: the First Workshop on Integrating Structured\nKnowledge and Neural Networks for NLP , 2020, pp. 21–26.\n[139] ——, “A hybrid approach to measure semantic relatedness in\nbiomedical concepts,” arXiv preprint arXiv:2101.10196 , 2021.\n[140] K. raj Kanakarajan, S. Ramamoorthy, V . Archana, S. Chatterjee,\nand M. Sankarasubbu, “Saama research at mediqa 2019: Pre-\ntrained biobert with attention visualisation for medical natural\nlanguage inference,” in Proceedings of the 18th BioNLP Workshop\nand Shared Task, 2019, pp. 510–516.\n[141] ¨O. Uzuner, B. R. South, S. Shen, and S. L. DuVall, “2010 i2b2/va\nchallenge on concepts, assertions, and relations in clinical text,”\nJournal of the American Medical Informatics Association , vol. 18,\nno. 5, pp. 552–556, 2011.\n[142] J. Du, Y. Xiang, M. Sankaranarayanapillai, M. Zhang, J. Wang,\nY. Si, H. A. Pham, H. Xu, Y. Chen, and C. Tao, “Extracting\npostmarketing adverse events from safety reports in the vaccine\nadverse event reporting system (vaers) using deep learning,”\nJournal of the American Medical Informatics Association , 2021.\n[143] S. Karimi, A. Metke-Jimenez, M. Kemp, and C. Wang, “Cadec: A\ncorpus of adverse drug event annotations,” Journal of biomedical\ninformatics, vol. 55, pp. 73–81, 2015.\n[144] S. Henry, K. Buchan, M. Filannino, A. Stubbs, and O. Uzuner,\n“2018 n2c2 shared task on adverse drug events and medication\nextraction in electronic health records,” Journal of the American\nMedical Informatics Association , vol. 27, no. 1, pp. 3–12, 2020.\n[145] M. Krallinger, O. Rabal, F. Leitner, M. Vazquez, D. Salgado, Z. Lu,\nR. Leaman, Y. Lu, D. Ji, D. M. Lowe et al., “The chemdner corpus\n30\nof chemicals and drugs and its annotation principles,” Journal of\ncheminformatics, vol. 7, no. 1, pp. 1–17, 2015.\n[146] J. Li, Y. Sun, R. J. Johnson, D. Sciaky, C.-H. Wei, R. Leaman, A. P .\nDavis, C. J. Mattingly, T. C. Wiegers, and Z. Lu, “Biocreative v cdr\ntask corpus: a resource for chemical disease relation extraction,”\nDatabase, vol. 2016, 2016.\n[147] N. Collier and J.-D. Kim, “Introduction to the bio-entity recog-\nnition task at jnlpba,” in Proceedings of the International Joint\nWorkshop on Natural Language Processing in Biomedicine and its\nApplications (NLPBA/BioNLP), 2004, pp. 73–78.\n[148] R. I. Do ˘gan, R. Leaman, and Z. Lu, “Ncbi disease corpus: a re-\nsource for disease name recognition and concept normalization,”\nJournal of biomedical informatics , vol. 47, pp. 1–10, 2014.\n[149] A. E. Johnson, L. Bulgarelli, and T. J. Pollard, “Deidentiﬁcation\nof free-text medical records using pre-trained bidirectional trans-\nformers,” in Proceedings of the ACM Conference on Health, Inference,\nand Learning, 2020, pp. 214–221.\n[150] K. C. Fraser, I. Nejadgholi, B. De Bruijn, M. Li, A. LaPlante,\nand K. Z. El Abidine, “Extracting umls concepts from medical\ntext using general and domain-speciﬁc deep learning models,”\nEMNLP-IJCNLP 2019, p. 157, 2019.\n[151] X. Yu, W. Hu, S. Lu, X. Sun, and Z. Yuan, “Biobert based named\nentity recognition in electronic medical record,” in 2019 10th\nInternational Conference on Information Technology in Medicine and\nEducation (ITME). IEEE, 2019, pp. 49–52.\n[152] M. Chen, F. Du, G. Lan, and V . S. Lobanov, “Using pre-trained\ntransformer deep learning models to identify named entities\nand syntactic relations for clinical protocol analysis.” in AAAI\nSpring Symposium: Combining Machine Learning with Knowledge\nEngineering (1), 2020.\n[153] T. Kang, A. Perotte, Y. Tang, C. Ta, and C. Weng, “Umls-based\ndata augmentation for natural language processing of clinical\nresearch literature,” Journal of the American Medical Informatics\nAssociation, vol. 28, no. 4, pp. 812–823, 2021.\n[154] Z. Miftahutdinov, I. Alimova, and E. Tutubalina, “On biomedical\nnamed entity recognition: experiments in interlingual transfer\nfor clinical and social media texts,” Advances in Information\nRetrieval, vol. 12036, p. 281.\n[155] F. W. Mutinda, S. Nigo, D. Shibata, S. Yada, S. Wakamiya, and\nE. Aramaki, “Detecting redundancy in electronic medical records\nusing clinical bert,” 2020.\n[156] D. Hoogeveen, A. Bennett, Y. Li, K. M. Verspoor, and T. Bald-\nwin, “Detecting misﬂagged duplicate questions in community\nquestion-answering archives,” in Twelfth international AAAI con-\nference on web and social media , 2018.\n[157] Y. A. AL-Khassawneh, N. Salim, and A. I. Obasae, “Sentence\nsimilarity techniques for automatic text summarization,” Journal\nof Soft Computing and Decision Support Systems , vol. 3, no. 3, pp.\n35–41, 2016.\n[158] G. So ˘gancıo˘glu, H. ¨Ozt ¨urk, and A. ¨Ozg ¨ur, “Biosses: a semantic\nsentence similarity estimation system for the biomedical do-\nmain,” Bioinformatics, vol. 33, no. 14, pp. i49–i58, 2017.\n[159] Y. Wang, F. Liu, K. Verspoor, and T. Baldwin, “Evaluating the\nutility of model conﬁgurations and data augmentation on clinical\nsemantic textual similarity,” in Proceedings of the 19th SIGBioMed\nWorkshop on Biomedical Language Processing , 2020, pp. 105–111.\n[160] Y. Xiong, S. Chen, Q. Chen, J. Yan, and B. Tang, “Using character-\nlevel and entity-level representations to enhance bidirectional\nencoder representation from transformers-based clinical seman-\ntic textual similarity model: Clinicalsts modeling study,” JMIR\nMedical Informatics, vol. 8, no. 12, p. e23357, 2020.\n[161] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and\nO. Yakhnenko, “Translating embeddings for modeling multi-\nrelational data,” Advances in neural information processing systems ,\nvol. 26, 2013.\n[162] W. Sun, A. Rumshisky, and O. Uzuner, “Evaluating temporal\nrelations in clinical text: 2012 i2b2 challenge,” Journal of the\nAmerican Medical Informatics Association , vol. 20, no. 5, pp. 806–\n813, 2013.\n[163] R. Bunescu, R. Ge, R. J. Kate, E. M. Marcotte, R. J. Mooney, A. K.\nRamani, and Y. W. Wong, “Comparative experiments on learn-\ning information extractors for proteins and their interactions,”\nArtiﬁcial intelligence in medicine , vol. 33, no. 2, pp. 139–155, 2005.\n[164] M. Krallinger, O. Rabal, S. A. Akhondi, M. P . P´erez, J. Santamar´ıa,\nG. P . Rodr´ıguez, G. Tsatsaronis, and A. Intxaurrondo, “Overview\nof the biocreative vi chemical-protein interaction track,” in Pro-\nceedings of the sixth BioCreative challenge evaluation workshop, vol. 1,\n2017, pp. 141–146.\n[165] M. Herrero-Zazo, I. Segura-Bedmar, P . Mart´ınez, and T. Declerck,\n“The ddi corpus: An annotated corpus with pharmacological\nsubstances and drug–drug interactions,” Journal of biomedical\ninformatics, vol. 46, no. 5, pp. 914–920, 2013.\n[166] E. M. Van Mulligen, A. Fourrier-Reglat, D. Gurwitz,\nM. Molokhia, A. Nieto, G. Triﬁro, J. A. Kors, and L. I.\nFurlong, “The eu-adr corpus: annotated drugs, diseases, targets,\nand their relationships,” Journal of biomedical informatics , vol. 45,\nno. 5, pp. 879–884, 2012.\n[167] Q. Wei, Z. Ji, Y. Si, J. Du, J. Wang, F. Tiryaki, S. Wu, C. Tao,\nK. Roberts, and H. Xu, “Relation extraction from clinical nar-\nratives using pre-trained language models,” in AMIA Annual\nSymposium Proceedings, vol. 2019. American Medical Informatics\nAssociation, 2019, p. 1236.\n[168] A. Thillaisundaram and T. Togia, “Biomedical relation extraction\nwith pre-trained language representations and minimal task-\nspeciﬁc architecture,” in Proceedings of The 5th Workshop on\nBioNLP Open Shared Tasks , 2019, pp. 84–89.\n[169] Y. Wang, K. Zhou, M. Gachloo, and J. Xia, “An overview of the\nactive gene annotation corpus and the bionlp ost 2019 agac track\ntasks,” in Proceedings of The 5th Workshop on BioNLP Open Shared\nTasks, 2019, pp. 62–71.\n[170] X. Liu, J. Fan, S. Dong et al., “Document-level biomedical relation\nextraction leveraging pretrained self-attention structure and en-\ntity replacement: Algorithm and pretreatment method validation\nstudy,” JMIR Medical Informatics , vol. 8, no. 5, p. e17644, 2020.\n[171] P . Su and K. Vijay-Shanker, “Investigation of bert model on\nbiomedical relation extraction based on revised ﬁne-tuning\nmechanism,” in 2020 IEEE International Conference on Bioinfor-\nmatics and Biomedicine (BIBM) . IEEE, 2020, pp. 2522–2529.\n[172] M. A. Al-Garadi, Y.-C. Yang, H. Cai, Y. Ruan, K. O’Connor, G.-H.\nGraciela, J. Perrone, and A. Sarker, “Text classiﬁcation models for\nthe automatic detection of nonmedical prescription medication\nuse from social media,” BMC medical informatics and decision\nmaking, vol. 21, no. 1, pp. 1–13, 2021.\n[173] Z. Shen, Y. Yi, A. Bompelli, F. Yu, Y. Wang, and R. Zhang,\n“Extracting lifestyle factors for alzheimer’s disease from clinical\nnotes using deep learning with weak supervision,” arXiv preprint\narXiv:2101.09244, 2021.\n[174] S. Chen, Y. Huang, X. Huang, H. Qin, J. Yan, and B. Tang, “Hitsz-\nicrc: a report for smm4h shared task 2019-automatic classiﬁcation\nand extraction of adverse effect mentions in tweets,” in Pro-\nceedings of the fourth social media mining for health applications (#\nSMM4H) workshop & shared task , 2019, pp. 47–51.\n[175] M. Tang, P . Gandhi, M. A. Kabir, C. Zou, J. Blakey, and X. Luo,\n“Progress notes classiﬁcation and keyword extraction using\nattention-based deep learning models with bert,” arXiv preprint\narXiv:1910.05786, 2019.\n[176] D. A. Wood, J. Lynch, S. Kaﬁabadi, E. Guilhem, A. Al Busaidi,\nA. Montvila, T. Varsavsky, J. Siddiqui, N. Gadapa, M. Townend\net al., “Automated labelling using an attention model for radiol-\nogy reports of mri scans (alarm),” in Medical Imaging with Deep\nLearning. PMLR, 2020, pp. 811–826.\n[177] A. Pampari, P . Raghavan, J. Liang, and J. Peng, “emrqa: A large\ncorpus for question answering on electronic medical records,” in\nProceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, 2018, pp. 2357–2368.\n[178] S. Suster and W. Daelemans, “Clicr: a dataset of clinical case\nreports for machine reading comprehension,” inProceedings of the\n2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1\n(Long Papers), 2018, pp. 1551–1563.\n[179] Q. Jin, B. Dhingra, Z. Liu, W. Cohen, and X. Lu, “Pubmedqa:\nA dataset for biomedical research question answering,” in Pro-\nceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP) , 2019, pp. 2567–\n2577.\n[180] T. M ¨oller, A. Reina, R. Jayakumar, and M. Pietsch, “Covid-qa:\nA question answering dataset for covid-19,” in Proceedings of the\n1st Workshop on NLP for COVID-19 at ACL 2020 , 2020.\n[181] M. Zhu, A. Ahuja, D.-C. Juan, W. Wei, and C. K. Reddy, “Ques-\ntion answering with long multiple-span answers,” in Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language\nProcessing: Findings, 2020, pp. 3840–3849.\n31\n[182] M. Zhu, A. Ahuja, W. Wei, and C. K. Reddy, “A hierarchical\nattention retrieval model for healthcare question answering,” in\nThe World Wide Web Conference , 2019, pp. 2472–2482.\n[183] S. Soni and K. Roberts, “Evaluation of dataset selection for\npre-training and ﬁne-tuning transformer language models for\nclinical question answering,” in Proceedings of The 12th Language\nResources and Evaluation Conference , 2020, pp. 5532–5538.\n[184] A. Akdemir and T. Shibuya, “Transfer learning for biomedical\nquestion answering.” 2020.\n[185] R. Mishra, J. Bian, M. Fiszman, C. R. Weir, S. Jonnalagadda,\nJ. Mostafa, and G. Del Fiol, “Text summarization in the biomed-\nical domain: a systematic review of recent research,” Journal of\nbiomedical informatics, vol. 52, pp. 457–467, 2014.\n[186] M. Moradi and N. Ghadiri, “Different approaches for identifying\nimportant concepts in probabilistic biomedical text summariza-\ntion,” Artiﬁcial intelligence in medicine , vol. 84, pp. 101–116, 2018.\n[187] P . Gigioli, N. Sagar, A. Rao, and J. Voyles, “Domain-aware\nabstractive text summarization for medical documents,” in 2018\nIEEE International Conference on Bioinformatics and Biomedicine\n(BIBM). IEEE, 2018, pp. 2338–2343.\n[188] M. Moradi, G. Dorffner, and M. Samwald, “Deep contextu-\nalized embeddings for quantifying the informative content in\nbiomedical text summarization,” Computer methods and programs\nin biomedicine, vol. 184, p. 105117, 2020.\n[189] M. Moradi, M. Dashti, and M. Samwald, “Summarization of\nbiomedical articles using domain-speciﬁc word embeddings and\ngraph ranking,” Journal of Biomedical Informatics , vol. 107, p.\n103452, 2020.\n[190] Y. Du, Q. Li, L. Wang, and Y. He, “Biomedical-domain\npre-trained language model for extractive summarization,”\nKnowledge-Based Systems, vol. 199, p. 105964, 2020.\n[191] S. Gururangan, T. Dang, D. Card, and N. A. Smith, “Variational\npretraining for semi-supervised text classiﬁcation,” in Proceedings\nof the 57th Annual Meeting of the Association for Computational\nLinguistics, 2019, pp. 5880–5894.\n[192] Y. Mao and K. W. Fung, “Use of word and graph embedding to\nmeasure semantic relatedness between uniﬁed medical language\nsystem concepts,” Journal of the American Medical Informatics\nAssociation, vol. 27, no. 10, pp. 1538–1546, 2020.\n[193] J. Phang, T. F ´evry, and S. R. Bowman, “Sentence encoders\non stilts: Supplementary training on intermediate labeled-data\ntasks,” arXiv preprint arXiv:1811.01088 , 2018.\n[194] Y. Pruksachatkun, J. Phang, H. Liu, P . M. Htut, X. Zhang, R. Y.\nPang, C. Vania, K. Kann, and S. Bowman, “Intermediate-task\ntransfer learning with pretrained language models: When and\nwhy does it work?” in Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics , 2020, pp. 5231–5247.\n[195] J. Wei and K. Zou, “Eda: Easy data augmentation techniques for\nboosting performance on text classiﬁcation tasks,” in Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , 2019, pp. 6382–6388.\n[196] H. Yu, X.-L. Mao, Z. Chi, W. Wei, and H. Huang, “A robust\nand domain-adaptive approach for low-resource named entity\nrecognition,” in 2020 IEEE International Conference on Knowledge\nGraph (ICKG). IEEE, 2020, pp. 297–304.\n[197] D. Jin, Z. Jin, J. T. Zhou, and P . Szolovits, “Is bert really robust? a\nstrong baseline for natural language attack on text classiﬁcation\nand entailment,” in Proceedings of the AAAI conference on artiﬁcial\nintelligence, vol. 34, no. 05, 2020, pp. 8018–8025.\n[198] D. Pruthi, B. Dhingra, and Z. C. Lipton, “Combating adversarial\nmisspellings with robust word recognition,” in Proceedings of the\n57th Annual Meeting of the Association for Computational Linguis-\ntics, 2019, pp. 5582–5591.\n[199] K. S. Kalyan and S. Sangeetha, “Bertmcn: Mapping colloquial\nphrases to standard medical concepts using bert and highway\nnetwork,” Artiﬁcial Intelligence in Medicine , vol. 112, p. 102008,\n2021.\n[200] V . Araujo, A. Carvallo, C. Aspillaga, and D. Parra, “On ad-\nversarial examples for biomedical nlp tasks,” arXiv preprint\narXiv:2004.11157, 2020.\n[201] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Gal-\nstyan, “A survey on bias and fairness in machine learning,”ACM\nComputing Surveys (CSUR) , vol. 54, no. 6, pp. 1–35, 2021.\n[202] J. R. Minot, N. Cheney, M. Maier, D. C. Elbers, C. M. Danforth,\nand P . S. Dodds, “Interpretable bias mitigation for textual data:\nReducing gender bias in patient notes while maintaining classi-\nﬁcation performance,” arXiv preprint arXiv:2103.05841 , 2021.\n[203] Y. Nakamura, S. Hanaoka, Y. Nomura, N. Hayashi, O. Abe,\nS. Yada, S. Wakamiya, and E. Aramaki, “Kart: Privacy leak-\nage framework of language models pre-trained with clinical\nrecords,” arXiv preprint arXiv:2101.00036 , 2020.\n[204] V . Misra, “Black box attacks on transformer language models,”\nin ICLR 2019 Debugging Machine Learning Models Workshop , 2019.\n[205] S. Hisamoto, M. Post, and K. Duh, “Membership inference\nattacks on sequence-to-sequence models: Is my data in your\nmachine translation system?” Transactions of the Association for\nComputational Linguistics, vol. 8, pp. 49–63, 2020.\n[206] S. Aroca-Ouellette and F. Rudzicz, “On losses for modern lan-\nguage models,” in Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) , 2020, pp. 4970–\n4981.\n[207] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman,\n“Glue: A multi-task benchmark and analysis platform for natural\nlanguage understanding,” in Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks\nfor NLP, 2018, pp. 353–355.\n[208] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael,\nF. Hill, O. Levy, and S. R. Bowman, “Superglue: a stickier\nbenchmark for general-purpose language understanding sys-\ntems,” in Proceedings of the 33rd International Conference on Neural\nInformation Processing Systems , 2019, pp. 3266–3280.\n[209] Y. Liang, N. Duan, Y. Gong, N. Wu, F. Guo, W. Qi, M. Gong,\nL. Shou, D. Jiang, G. Cao et al. , “Xglue: A new benchmark\ndatasetfor cross-lingual pre-training, understanding and gener-\nation,” in Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , 2020, pp. 6008–6018.\n[210] G. Aguilar, S. Kar, and T. Solorio, “Lince: A centralized bench-\nmark for linguistic code-switching evaluation,” in Proceedings of\nthe 12th Language Resources and Evaluation Conference , 2020, pp.\n1803–1813.\n[211] F. Petroni, T. Rockt ¨aschel, S. Riedel, P . Lewis, A. Bakhtin, Y. Wu,\nand A. Miller, “Language models as knowledge bases?” in\nProceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP) , 2019, pp. 2463–\n2473.\n[212] N. Kassner, P . Dufter, and H. Sch¨utze, “Multilingual lama: Inves-\ntigating knowledge in multilingual pretrained language mod-\nels,” in Proceedings of the 16th Conference of the European Chapter\nof the Association for Computational Linguistics: Main Volume , 2021,\npp. 3250–3258.\n[213] Z. Jiang, A. Anastasopoulos, J. Araki, H. Ding, and G. Neubig,\n“X-factr: Multilingual factual knowledge retrieval from pre-\ntrained language models,” in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , 2020,\npp. 5943–5959.\n[214] B. Y. Lin, S. Lee, X. Qiao, and X. Ren, “Common sense beyond\nenglish: Evaluating and improving multilingual language mod-\nels for commonsense reasoning,” arXiv preprint arXiv:2106.06937,\n2021.\n[215] Z.-H. Jiang, W. Yu, D. Zhou, Y. Chen, J. Feng, and S. Yan, “Con-\nvbert: Improving bert with span-based dynamic convolution,”\nAdvances in Neural Information Processing Systems , vol. 33, 2020.\n[216] P . He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced\nbert with disentangled attention,” in International Conference on\nLearning Representations, 2020."
}