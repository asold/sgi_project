{
    "title": "Radiology in the era of large language models: the near and the dark side of the moon",
    "url": "https://openalex.org/W4383301848",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2972682597",
            "name": "Pilar López-Úbeda",
            "affiliations": [
                "Complejo Hospitalario de Jaén"
            ]
        },
        {
            "id": "https://openalex.org/A2346647402",
            "name": "Teodoro Martín-Noguerol",
            "affiliations": [
                "Complejo Hospitalario de Jaén"
            ]
        },
        {
            "id": "https://openalex.org/A2103149344",
            "name": "Antonio Luna",
            "affiliations": [
                "Complejo Hospitalario de Jaén"
            ]
        },
        {
            "id": "https://openalex.org/A2972682597",
            "name": "Pilar López-Úbeda",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2346647402",
            "name": "Teodoro Martín-Noguerol",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103149344",
            "name": "Antonio Luna",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4226510082",
        "https://openalex.org/W4318931874",
        "https://openalex.org/W4324129116",
        "https://openalex.org/W4387356888",
        "https://openalex.org/W3112103703",
        "https://openalex.org/W3169068430",
        "https://openalex.org/W3164718925",
        "https://openalex.org/W2105210082"
    ],
    "abstract": null,
    "full_text": "Vol.:(0123456789)1 3\nEuropean Radiology (2023) 33:9455–9457 \nhttps://doi.org/10.1007/s00330-023-09901-9\nOPINION\nRadiology in the era of large language models: the near and the dark \nside of the moon\nPilar López‑Úbeda1  · Teodoro Martín‑Noguerol2 · Antonio Luna2\nReceived: 6 March 2023 / Revised: 9 April 2023 / Accepted: 28 April 2023 / Published online: 6 July 2023 \n© The Author(s), under exclusive licence to European Society of Radiology 2023\nRecently, the interest in GPT-3, ChatGPT, GPT-4 (OpenAI), \nand similar large language models (PaLM/LaMDA from \nGoogle or open-source alternatives like LLaMA and Alpaca) \nthat employ deep learning (trained with massive datasets \nfrom the internet) to be capable of generating realistic text \nsimulating human writing, has been growing exponentially \n[1]. General applications of large language models (LLMs) \ngo beyond the hype that ChatGPT has been experiencing in \nthe last months. To date, many artificial intelligence (AI) \napplications using LLMs have been applied in the radiologi-\ncal domain for different purposes such as report generation \n[2, 3] and medical image captioning [4 ]. However, some \nwarnings regarding LLMs and how to integrate or use it into \nactual radiology practice must be considered.\nThe near side of the moon\nGiven the widely demonstrated capability of e.g. ChatGPT \nto perform various tasks such as answering real-time queries \nor generating text, one of its potential applications could be \nto assist radiologists during the image evaluation and radiol-\nogy reporting process. Moreover, due to the vast amount of \ndata used for training ChatGPT, it could be used as a real-\ntime consultant for seeking scientific publications or for con-\nsulting specific classifications of patterns of diseases (i.e., \nbone fracture classifications). In short, using ChatGPT as a \nquestion-answering tool can assist radiologists in making \ndifferential diagnoses and classifying lesions. These issues \ncould have a potential added value especially for less expe-\nrienced radiologists or for teaching or learning purposes.\nGPT-based applications are continually being updated, as \nevidenced by the recent release of the new GPT-4 version \n[5, 6]. These tools are revolutionizing the field of medical \nwriting by automating specific tasks and improving the effi-\nciency of the writing process [7]. Along the same lines, other \npotential applications of LLMs could be related to the radi-\nology report creation process as one of the main capabilities \nof LLMs is language generation, so it is not surprising that \nit can adapt to the radiological context and generate reports \nin a human-like manner. The advantages of using this type \nof technology may include time savings and efficiency, flex-\nibility since it is available to everyone, and reports should \ntheoretically be more consistent and less prone to error [8 ]. \nCommon mistakes in radiology reports related to grammar \nissues, misspellings, gender, or laterality questions may be \nreduced to the minimum using LLMs as a supporting tool.\nThe dark side of the moon\nAs with almost all the AI solutions available in healthcare, \nGPT-3, ChatGPT, and other LLMs raise an important ques-\ntion: how does it work? In other words, what is inside the \nopacity of the LLM’s “black box”? GPT-based applications \nare showing promising results and will improve daily, when \nthey learn from specific examples they have seen before but \nstumble when confronted with something new. OpenAI has \nnot disclosed the full details of how its algorithms work, so \nanyone relying on it to answer questions should pay special \nattention to its output. Explainability is constantly over -\nlooked in AI as long as the results are satisfactory. However, \nregarding healthcare, we must be cautious about its use and \naware of all its drawbacks.\nNevertheless, the output provided by e.g. GPT-3 or Chat-\nGPT is not perfect. Although it can perform tasks such as \ncreating short texts or basic applications, its results lose use-\nfulness when asked to produce something longer or more \ncomplex because it repeats itself semantically [9]. Moreover, \nit is not uncommon that an LLM contradicts itself when \nasked about a specific issue (Fig.  1). In addition, those \n * Pilar López-Úbeda \n p.lopez@htmedica.com\n1 R+D+I Department, HT Medica, Jaén, Spain\n2 MRI Unit, Radiology Department, HT Medica, Jaén, Spain\n9456 European Radiology (2023) 33:9455–9457\n1 3\nmodels lack pure common sense, and do not understand \nreal-world sensitivity and consistency, so it would be unre-\nalistic to consider, for example, GPT-3 as a substitute for \nan appointment assistant as it would not be able to detect a \nchange in tone expressed by a patient that might indicate a \nclinical emergency [10].\nAccording to experts, GPT-3 has the same architecture \nas GPT-2. The only difference is the large scale of the data \nwith which it was trained. However, GPT-3 for example was \ntrained only with data available before September 2021, so \nit cannot provide outputs related to new diseases, drugs, or \ntreatments, which constitutes another potential drawback for \nits use for radiology-related purposes in particular, and, by \nextension, to healthcare issues.\nAt this point, beyond technical or ethical questions, the \nmain Achilles heel for usage of LLMs may lie in the cost \ninvolved with its use. The API required to access GPT-3 \nis rather expensive and puts it out of reach for a particular \nuse or small healthcare groups. One of the most advanced \nmodels (OpenAI Davinci) costs $0.02 per thousand tokens. \nTokens are considered fragments of words, where one \nthousand tokens equal 750 words, according to OpenAI. \nMRI reports usually contain an average of 200 tokens and \ndo not usually exceed 500 tokens. Therefore, producing \nhigh-volume text and generating content at this price is \nnot affordable for everyone. Moreover, it is not uncommon \nthat radiology departments continuously seek ways to use \nAI to make their workflows more efficient; however, there \nare unresolved regulatory issues for usage of AI tools and \nLLMs in clinical practice. The Food and Drug Administra-\ntion (FDA) has approved more than 500 AI algorithms by \nJanuary 2023, and most of them are related to medical imag-\ning [12]; however (at the moment), GPT-based models and \nLLMs are not among them.\nConclusions\nLLMs may hold potential in terms of cost savings, workflow \nefficiency, and improvements to diagnostic accuracy. These \nfeatures could make GPT-3 and other large language models \nan ideal diagnostic aid for medical professionals seeking to \nimprove patient care while reducing costs. Similar to Geof-\nfrey Hinton’s famous quote from 2016 warning that AI may \nbe better suited than radiologists to read images (“People \nshould stop training radiologists now it is just completely \nobvious within five years deep learning is going to do bet-\nter than radiologist because this can be able to hit a lot \nFig. 1  An example of answers provided by GPT-3 when it is asked \n(Q) about the most accurate grade for a tibial plateau fracture follow -\ning the well-known Schatzker classification using the same sentence \nthat appears on the internet at almost all available resources (Radio-\npaedia (https:// radio paedia. org/ artic les/ tibial- plate au- fract ure), Wiki-\npedia (https:// en. wikip edia. org/ wiki/ Tibial_ plate au_ fract ure), and \npapers published at the leading radiology journals [11]). You can get \nas many answers as you need, so we show up to 3 answers (A1, A2, \nand A3) offered by the GPT-3 system. However, only one answer (A2 \nin green color) provided by the system was correct for describing the \ntype of tibial plateau fracture (type IV)\n9457European Radiology (2023) 33:9455–9457 \n1 3\nmore experience.”), some argue that LLMs might pose a \nsimilar threat. It is therefore important to become familiar \nwith the near and dark side of GPT-3, ChatGPT, and similar \nlarge language models. While these may have the potential \nto assist in radiology, or even replace physicians, their per -\nformance needs to be held to a high standard, and so far, \nin 2023, no single radiologist has been replaced—neither \nby large language models, not any other AI tool. Instead, \nthere is consensus that AI applied to radiology is more com-\nplicated than it seems at first sight. Therefore, as of today, \nAI will not replace human intelligence because systems can \nonly answer the what of things, not the why  let alone the \nwhat-if.\nFunding This study has received funding by Ministry of Science and \nInnovation (MCIN/AEI/10.13039/501100011033), grant number \nPTQ2021-012120.\nDeclarations \nGuarantor The scientific guarantor of this publication is Antonio Luna, \nHT médica.\nConflict of interest The authors of this manuscript declare no relation-\nships with any companies, whose products or services may be related \nto the subject matter of the article.\nStatistics and biometry Not applicable.\nInformed consent Not applicable.\nEthical approval Not applicable.\nStudy subjects or cohorts overlap Not applicable.\nMethodology Not applicable.\nReferences\n 1. Manning CD (2022) Human language understanding & reasoning. \nDaedalus 151:127–138. https:// doi. org/ 10. 1162/ daed_a_ 01905\n 2. Biswas S (2023) ChatGPT and the future of medical writing. Radi-\nology 307:223312\n 3. Ramesh V, Chi NA, Rajpurkar P (2022) Improving radiology \nreport generation systems by removing hallucinated references to \nnon-existent priors. In: Machine Learning for Health. pp 456–473\n 4. Selivanov A, Rogov OY, Chesakov D et al (2023) Medical \nimage captioning via generative pretrained transformers. Sci Rep \n13:4171. https:// doi. org/ 10. 1038/ s41598- 023- 31223-5\n 5. OpenAI (2023) GPT-4 technical report\n 6. Endicott S (2023) Microsoft brings GPT-4 to healthcare profes-\nsionals with DAX express. https:// www. windo  wscen tral. com/ \nmicro soft/ micro soft- brings- gpt-4- to- healt hcare- profe ssion als- \nwith- dax- expre ss. Accessed  29 June 2023\n 7. Jeblick K, Schachtner B, Dexl J et al (2022) ChatGPT makes \nmedicine easy to swallow: an exploratory case study on simpli-\nfied radiology reports. arXiv preprint arXiv:221214882\n 8. Dale R (2021) GPT-3: what’s it good for? Nat Lang Eng \n27:113–118\n 9. Chintagunta B, Katariya N, Amatriain X, Kannan A (2021) Medi-\ncally aware GPT-3 as a data generator for medical dialogue sum-\nmarization. In: Machine Learning for Healthcare Conference. pp \n354–372\n 10. Korngiebel DM, Mooney SD (2021) Considering the possibilities \nand pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in \nhealthcare delivery. NPJ Digit Med 4:93\n 11. Markhardt BK, Gross JM, Monu J (2009) Schatzker classification \nof tibial plateau fractures: use of CT and MR imaging improves \nassessment. Radiographics 29:585–597\n 12. Dave Fornell (2023) FDA has now cleared more than 500 health-\ncare AI algorithms. https:// healt hexec. com/ topics/ artifi  cial- intel \nligen ce/ fda- has- now- clear ed- more- 500- healt hcare- ai- algor ithms. \nAccessed 27 Feb 2023\nPublisher's note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}