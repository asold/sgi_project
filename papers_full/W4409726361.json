{
  "title": "An automatic machine fault identification method using the knowledge graph–embedded large language model",
  "url": "https://openalex.org/W4409726361",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2108925152",
      "name": "Pengcheng Wu",
      "affiliations": [
        "Southwest University",
        "Shanghai Center for Brain Science and Brain-Inspired Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3127038415",
      "name": "Xun Mou",
      "affiliations": [
        "Southwest University",
        "Shanghai Center for Brain Science and Brain-Inspired Technology"
      ]
    },
    {
      "id": null,
      "name": "Leihao Gong",
      "affiliations": [
        "Shanghai Center for Brain Science and Brain-Inspired Technology",
        "Southwest University"
      ]
    },
    {
      "id": "https://openalex.org/A5112193793",
      "name": "Haobei Tu",
      "affiliations": [
        "Southwest University",
        "Shanghai Center for Brain Science and Brain-Inspired Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2766875671",
      "name": "Linqiong Qiu",
      "affiliations": [
        "Queensland University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2052563419",
      "name": "Bo Yang",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2108925152",
      "name": "Pengcheng Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3127038415",
      "name": "Xun Mou",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Leihao Gong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5112193793",
      "name": "Haobei Tu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2766875671",
      "name": "Linqiong Qiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2052563419",
      "name": "Bo Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4213159173",
    "https://openalex.org/W4313419600",
    "https://openalex.org/W3217212503",
    "https://openalex.org/W2789904726",
    "https://openalex.org/W3190646162",
    "https://openalex.org/W4221121733",
    "https://openalex.org/W2998506103",
    "https://openalex.org/W4225127971",
    "https://openalex.org/W1966634360",
    "https://openalex.org/W4390977378",
    "https://openalex.org/W3003265726",
    "https://openalex.org/W4399070913",
    "https://openalex.org/W4393231265",
    "https://openalex.org/W6704017408",
    "https://openalex.org/W4392607872",
    "https://openalex.org/W3082340968",
    "https://openalex.org/W2751753082",
    "https://openalex.org/W4386126398",
    "https://openalex.org/W3196750366",
    "https://openalex.org/W4386516837",
    "https://openalex.org/W4400110542",
    "https://openalex.org/W4399697689",
    "https://openalex.org/W4303183291",
    "https://openalex.org/W4283790835",
    "https://openalex.org/W4391407054",
    "https://openalex.org/W4394834299",
    "https://openalex.org/W4403791927",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W3128513378"
  ],
  "abstract": "Abstract Computer numerical control (CNC) machines are prone to various faults during long-term operation, which can compromise production safety. However, accurately identifying fault sources and obtaining rapid troubleshooting solutions remains a significant challenge. To tackle this challenge, this study proposes an automated machining process decision-making system that integrates a knowledge graph and a large language model (LLM). The system first constructs a graph-based fault knowledge representation using BERT-Transformer-CRF for machining knowledge extraction. The developed machining process knowledge graph is then enhanced and endowed with knowledge reasoning capabilities via the large language model. By combining the graph-structured machining process representing form and knowledge inference ability, the proposed system significantly improves fault diagnosis and troubleshooting efficiency. To evaluate its performance, a functional test was conducted, comparing the system with conventional approaches in terms of accuracy, knowledge inference ability, and user-friendliness. Experimental results in practical industrial scenarios demonstrate that the proposed model achieves 97.50% accuracy in fault diagnosis and troubleshooting. Additionally, subjective evaluations indicate high usability, with scores of 9.4 for user-friendliness and 9.1 for knowledge inference ability. These findings highlight the system’s potential to enhance fault troubleshooting in industrial machining processes.",
  "full_text": "Vol.:(0123456789)\nThe International Journal of Advanced Manufacturing Technology (2025) 138:725–739 \nhttps://doi.org/10.1007/s00170-025-15555-2\nORIGINAL ARTICLE\nAn automatic machine fault identification method using \nthe knowledge graph–embedded large language model\nPengcheng Wu1,2,3 · Xun Mou1,2,3 · Leihao Gong1,2,3 · Haobei Tu1,2,3 · Linqiong Qiu4 · Bo Yang5\nReceived: 12 November 2024 / Accepted: 13 April 2025 / Published online: 23 April 2025 \n© The Author(s) 2025\nAbstract\nComputer numerical control (CNC) machines are prone to various faults during long-term operation, which can compromise \nproduction safety. However, accurately identifying fault sources and obtaining rapid troubleshooting solutions remains a significant \nchallenge. To tackle this challenge, this study proposes an automated machining process decision-making system that integrates \na knowledge graph and a large language model (LLM). The system first constructs a graph-based fault knowledge representation \nusing BERT-Transformer-CRF for machining knowledge extraction. The developed machining process knowledge graph is then \nenhanced and endowed with knowledge reasoning capabilities via the large language model. By combining the graph-structured \nmachining process representing form and knowledge inference ability, the proposed system significantly improves fault diagnosis \nand troubleshooting efficiency. To evaluate its performance, a functional test was conducted, comparing the system with conventional \napproaches in terms of accuracy, knowledge inference ability, and user-friendliness. Experimental results in practical industrial \nscenarios demonstrate that the proposed model achieves 97.50% accuracy in fault diagnosis and troubleshooting. Additionally, \nsubjective evaluations indicate high usability, with scores of 9.4 for user-friendliness and 9.1 for knowledge inference ability. These \nfindings highlight the system’s potential to enhance fault troubleshooting in industrial machining processes.\nKeywords Knowledge graph · Knowledge inference · Large language model · Machining process · Fault localization\nAbbreviations\nCNC  Computer numerical control\nRBR  Rule-based reasoning\nCBR  Case-based reasoning\nKG  Knowledge graph\nLLMs  Large language models\nAIGC  Artificial intelligence generated content\nMPKG  Machining process knowledge graph\nBTC  BERT-Transformer-CRF\nNLP  Natural language processing\nKGE  Knowledge graph engineering\nLoRA  Low-rank adaptation of large language models\nKPA  Knowledge prefix adapter\nSEP  Structural embedding pre-training\nCRF  Conditional random field\nAcc  Accuracy\nPr  Precision\nRe  Recall\nRDF  Resource description framework\nD2R  Database to RDF\nNER  Named entity recognition\n1 Introduction\nComputer numerical control (CNC) machine tools have \nbeen regarded as the most important role in modern digital \nmanufacturing, which is widely used for transforming the \nraw material into the final designed product using cutting, \n * Pengcheng Wu \n wupengcheng_mech@163.com; wu20230730@swu.edu.cn\n * Linqiong Qiu \n emily.qiu345@gmail.com\n1 College of Artificial Intelligence, Southwest University, \nChongqing 400715, China\n2 National & Local Joint Engineering Research Center \nof Intelligent Transmission and Control Technology, \nChongqing, China\n3 Chongqing Key Laboratory of Brain-Inspired Computing \nand Intelligent Chips, Chongqing, China\n4 School of Mathematical Science, Queensland University \nof Technology, Brisbane, QLD, Australia\n5 State Key Laboratory of Mechanical Transmission \nfor Advanced Equipment, Chongqing University, Chongqing, \nChina\n726 The International Journal of Advanced Manufacturing Technology (2025) 138:725–739\nshearing, boring and other forms of shaping or machining \nmethods [1, 2]. Due to long-term running, the machine tool \nis in an inevitable descending process, resulting in abrasions \nand fatigue of its mechanical components [3 ]. These abra-\nsions and fatigue of its mechanical components often result \nin poor product quality, and nearly 79.6% of the downtime of \na machine tool is even caused by those mechanical failures \n[4]. Thus, how to effectively address the problem of fault \nlocalization in industry scenarios, especially within system \nfunctional testing contexts, is highly desired to increase \nproduct quality and efficiency, reducing the machine down-\ntime during the CNC machining stage [5 ]. To this end, this \nstudy proposed to utilize the joint knowledge enhancement \nmodel to develop an expert system for the CNC machin-\ning process; by identifying the root cause of faults through \nfault phenomena, the problem of fault localization during \nthe CNC machining process can be effectively addressed, \nproviding the engineering personnel with more comprehen-\nsive and effective knowledge reasoning tools and auxiliary \ndecision-making tools during the fault diagnosis process.\nResearchers have long been interested in developing \nexpert systems for fault localization and root cause iden-\ntification during the CNC machining process [6 , 7]. In the \ninitial stage, this task is mostly fulfilled by manually using \nthe query-answer way, with the guidance of experienced \nworkers. Unfortunately, this method is highly dependent \non the skills of the workers, and an experienced worker \ncannot be readily obtained in actual industrial environ-\nments. Besides, this manual method is often time-consum-\ning and resource-intensive due to the high complexity of \nthe machining process [8 , 9]. In the later stage, rule-based \nreasoning (RBR) and case-based reasoning (CBR) have \nemerged as a further development of the manual performed \nway to conduct the expert system for fault localization \nduring the CNC machining process. To a certain extent, \nthis evolved method has alleviated the dependence on \nexperienced workers and helped improve the efficiency in \nthe fault diagnosing stage. However, the solutions refer to \nfaults that occurred in the CNC machining stage in CBR- \nand RBR-based methods largely depend on the scenarios \nmet in the past, which is trying to generate a solution by \nfinding or matching a similarity disposition measure met \nin the past. While considering the machining process is a \ndynamic complex descending process, a highly similar and \neffective method cannot be always found and thus hinders \ntheir application in the actual industry.\nMotivated by the development of artificial-related tech-\nnology, knowledge graph (KG) technology has appeared \nand been used to construct knowledge reasoning tools and \nauxiliary decision-making systems for engineering person-\nnel [10]. A knowledge graph is a structured representation \nof knowledge that captures entities (such as people, places, \nand things) and their relationships (connections, attributes) \nin a graph format. It is used to organize and connect infor -\nmation in a way that is understandable for machines, ena-\nbling more sophisticated data analysis and retrieval [11]. \nBy using the KG, the engineering personnel can get insight \ninto the machining decision-making system in a predefined \nand organized way, which is much more effective and easier \nthan the conventional RBR and CBR studies. Although the \nKG has proved its advantage in handling the decision-mak-\ning in the machining process system, it has suffered for its \npoor ability in the creativity and user interface in the actual \nexploitation stage, and these shortcomings have hindered its \nreal application in the industry scenarios.\nBenefiting from the breakthrough in artificial intelligence \ngenerated content (AIGC)-based techniques, the large lan-\nguage models (LLMs) have been regarded as a powerful \ntool in developing expert systems for constructing the expert \nsystem for the machining process [12, 13]. The LLMs is a \nsophisticated artificial intelligence system trained on vast \namounts of text data, and it is designed to understand and \ngenerate human-like text, capable of comprehending context, \nsemantics, and even nuanced meanings within sentences and \ndocuments [14]. By taking advantage of the LLMs, a query-\nand-answer system for the decision-making system in terms \nof the machining process can be developed. Admittedly, the \nLLMs can improve the intelligence of the machining process \ndecision-making system, while it suffers from hallucination \nduring the application stage, which generates some misguid-\ning and wrong information toward the decision-making. This \nphenomenon is often attributed to the training procedure \nin the LLMs, which is conducted in a black box way and \neasier to be trapped in obtaining factual knowledge of the \nmachining process.\nRecent research on the integration of LLMs and KGs has \ndemonstrated promising performance in dealing with the \nproblem [15]. However, these joint models face significant \nchallenges in real-world manufacturing environments, limit-\ning their practical applicability. The key challenges are as \nfollows:\n(1) Limited coverage of knowledge graphs in dynamic \nmachining environments:\n  In conventional studies, KGs are primarily used \nto generate corpora for LLM training. However, it \nis inherently challenging—if not infeasible—for a \nKG to encompass all possible working conditions in \nmanufacturing. The machining environment is highly \ndynamic and unpredictable, with unforeseen condi-\ntions frequently emerging. These variations impose \nhigh demands on the adaptability and generalization \ncapabilities of models, thereby limiting the effective-\nness of traditional research approaches. To bridge this \ngap, a novel method for knowledge graph construction \nis required.\n727The International Journal of Advanced Manufacturing Technology (2025) 138:725–739 \n(2) Challenges in knowledge extraction and model optimi-\nzation:\n  The conventional approach to constructing KGs \nand LLMs primarily relies on manual tuning, which \npresents several limitations. In the case of KGs, manual \nconstruction makes it difficult to identify essential \nrelationships among diverse knowledge entities, \nthereby hindering effective knowledge extraction for \nLLM training. Similarly, for LLMs, it is challenging \nto fully leverage the knowledge corpus for inference, \noften resulting in hallucination issues during real-world \napplications. These limitations underscore the need for \na novel and automated approach to the construction \nof KGs and LLMs, ensuring enhanced accuracy, \nadaptability, and practical usability in manufacturing \nsettings.\nIn this paper, a novel framework that integrates the KG \nand LLMs is employed. Firstly, the transformer employed in \nthis paper constructs a machining process knowledge graph \n(MPKG). Especially, the BERT-Transformer-CRF (BTC) \nis employed in this MPKG constructing process. Then, the \nLLMs are employed in this paper to complete the devel -\noped MPKG and develop an interface between the operators \nand the machine. The key contributions of this study are as \nfollows:\n(1) Novel embedding method for KGs in LLMs\n  This study proposes a novel method for embedding \nthe KG into LLMs specifically within the machining \nprocess domain. The proposed method transforms the \nMPKG into a two-step process that leverages the infer-\nence ability of the LLMs. By harnessing the learning \nabilities of LLMs, the method enhances their inference \nabilities, making them more applicable and effective in \nthe actual manufacturing industry.\n(2) Enhanced logical representation and reduced hallucination\n  This paper presents a novel approach to modeling \nthe LLMs by integrating knowledge graphs, which \nfully utilizes knowledge guidance of the LLMs \ntraining process. This integration enhances the logical \nrepresentation of the LLMs, and significantly reduces \nthe hallucination in the application of the machining \nprocess domain.\nThe remainder of this study is structured as follows: \nSection  2 provides a review of the latest research in this \nfield. Section  3 presents the methodology for constructing \nMPKG-embedded LLMs. Section  4 details the experimen-\ntal verification process and discusses the results. Finally, \nSection  5 concludes the study, summarizing key findings \nand potential future directions.\n2  Related work\nIn the past decades, AI and data-driven techniques have \nattracted wide attention in the field of intelligent decision-\nmaking for the machining process [16, 17]. Research on \nthe KGs and LLMs has been taken as the mainstream and \npowerful tool in developing that knowledge reasoning or \nauxiliary decision-making tool.\nKGs can organize experience or knowledge in a struc-\ntured way. The KG-based techniques are the most widely \nemployed tools in constructing the expert system in the field \nof machining process decision-making [18]. Guo et al. have \nintroduced an automatic construction framework for the \nprocess knowledge base in the field of machining [ 19]. A \nprototype system for the quality analysis is also conducted. \nBased on this, Guo et al. have conducted a further study; \na process knowledge–based construction framework using \nthe transformer was proposed by them [20]. Bao et al. have \nconstructed a machining domain knowledge graph, based on \nthe as-fabricated information model and combined it with \nthe essential characteristics and development requirements \nof the machining process [21]. Liang et al. have developed \na process design intent content model for representing and \ncapturing useful design intent knowledge in the machining \nprocess, and a knowledge graph of process design intent is \nbuilt based on the model structure [22]. Diamantini et al. \nhave built an ontology describing the fundamental elements \ninvolved in IIoT and their relations and discussed the con-\nstruction of the Process-aware IIoT Knowledge Graph [23]. \nAs mentioned, the KGs can take good advantage of the expe-\nrience and knowledge, demonstrating their capabilities in \nhandling the process design, condition monitoring, and some \nother decision-making functions in the machining process \nfield. However, the KGs are an essential search tool that tries \nto find similar disposition measures taken in the past, result-\ning in poor creativity in unseen conditions. Besides, the KGs \nhave strict requirements about the formal style and query \ncontent, which impede its user experience further.\nLLMs originated from the natural language processing \n(NLP) techniques, which have put forward the text under -\nstanding and knowledge reasoning ability to a higher level. \nIn the initial stage, the LLMs have been developed by some \ncompanies and employed as a tool for general purposes. \nOpen AI has developed the ChatGPT, which is regarded as \nthe most remarkable product among commercially employed \nLLMs. Following ChatGPT, Google developed the Gemini. \nBaidu published ERNIE. Meta announced the release of \nLlama, and Huawei launched PanguLM. In the next few \ndays, some research centers or universities also publish their \nlarge language models, such as ChatGML from Tsinghua, \nand YuLan from Renming University. Currently, the LLMs \nhave shown their potential in handling general NLP tasks. \n728 The International Journal of Advanced Manufacturing Technology (2025) 138:725–739\nWhile the NLP application in the decision-making machin-\ning process is seldom published. The reason why there is \nlittle application of the LLMs in the field of the machin-\ning process may lie in two folds. Firstly, the performance \nof the LLMs relies greatly on the quality and quantity of \nthe supplied corpus, which is often hard to be satisfied in \nthe machining domain. Secondly, it may be time- and cost-\nexpensive to apply such an application. Those limitations \nhave hindered the application of LLMs in the field of the \nmachining process. Besides, some researchers have tried \nto transfer the general LLMs directly into the machining \ndomain. While the hallucination during the application stage \nis hard to avoid.\nRecent research has explored the integration of LLMs and \nKGs to leverage their combined strengths. Generally, there \nare three primary methods for integrating KGs with LLMs: \n(1) embedding KGs into LLMs during the pretraining pro-\ncess [15], (2) utilizing dynamic knowledge fusion [24], and \n(3) employing retrieval-enhanced knowledge fusion [25]. \nThese approaches have been tested by different research-\ners. Lars-Peter Meyer et al. conducted a comprehensive \nexperiment with ChatGPT to explore its potential to support \nknowledge graph engineering (KGE) [26]. Zhou et al. intro-\nduced an industrial structure causal knowledge–enhanced \nlarge language model for the cause analysis of quality defects \nin aerospace product manufacturing [ 27]. Liu et al. have a \nknowledge-enhanced joint model that incorporates aviation \nassembly KG embedding into large language models [15]. \nIt is well established that KG-enhanced LLMs have been \nincreasingly applied in manufacturing processes. A review \nof those existing studies indicates that most implementa-\ntions primarily employ KG-guided prefix training for LLMs. \nIn such cases, the resulting KG-enhanced LLMs generate \nresponses based on structured knowledge representations, \nproviding approximate yet contextually relevant answers to \ninput queries.\nThese responses often face limitations in inference abil-\nity, which hinder their practical application in the machin-\ning process field. In conventional studies, KGs are primarily \nused to generate corpora for LLMs. However, it remains \nhighly challenging—if not impossible—for a KG to com -\nprehensively represent all possible working conditions in a \ndynamic machining environment, where unforeseen scenar-\nios frequently emerge. Such emergent scenarios impose high \ndemands on the creativity of the developed models, restrict-\ning the utility of traditional research approaches. Moreover, \nthe manual construction of KGs and LLMs further compli-\ncates system development, making large-scale implementa-\ntion increasingly challenging. These constraints significantly \nimpede the practical deployment of KG-enhanced LLMs in \nreal-world industrial applications, highlighting the need for \nautomated and scalable solutions.\nTo address these challenges, this study introduces a novel \nKG-embedded LLM. The primary difficulties encountered in \nthis research can be categorized into two key phases:\n(1) Challenges in knowledge graph generalization:\n  Developing a highly generalized KG requires the \nintegration of diverse knowledge sources for the \nknowledge extraction and modeling phase. However, \nconventional KG modeling methods struggle to cover \nall potential working conditions due to their reliance on \nlimited worker experience and domain expertise. As a \nresult, these models fail to generalize effectively over \nunseen working conditions. To address this limitation, \na novel KG modeling method that integrates with the \nLLMs and can generalized over unseen working condi-\ntions is required to be proposed.\n(2) Limitations in KG-LLM integration and pretraining:\n  In conventional approaches, the integration of KGs \nand LLMs typically relies on a shallow pretraining pro-\ncess. This suboptimal integration often leads to halluci-\nnation issues during the exploitation stage, significantly \nlimiting the model’s reliability in real-world manufac-\nturing applications. To overcome this challenge, a novel \nKG-embedded structure for the LLMs pretraining pro-\ncess is required to be proposed.\nTo address these challenges, this paper proposes a novel \nmethod that can guide the generalization ability of LLMs. First, \nthe ontology rules of the machining process are designed. Then, \nthe transformer-based model is proposed for the machining \nknowledge extraction task, leading to the construction of the \nMPKG. The developed knowledge graph and its generated \nsubgraphs are then utilized to generate prompts as prefixes, \nwhich are integrated into the LLM training process at each layer.\nThe novelty of this paper can be attributed to two phases. \nFirst, a novel approach is introduced to improve KG gener-\nalization across unseen working conditions by leveraging the \ninference capabilities of LLMs. Second, a specialized LLM \narchitecture for machining processes is developed, providing \na more professional guide for the machining fault diagnosis.\n3  Construction of the MPKG‑embedded \nLLMs\nIn this section, the MPKG-embedded LLM method is pro-\nposed to enable the automated construction of machining \nprocess knowledge representations. The BERT-Transformer-\nCRF model is employed for knowledge extraction, with the \ndetailed construction process outlined in Section 3.1. Subse-\nquently, methods for embedding the MPKG into LLMs are \nintroduced in Section 3.2. The proposed MPKG-embedded \n729The International Journal of Advanced Manufacturing Technology (2025) 138:725–739 \nLLM framework consists of two key components: structural \nembedding pre-training and a knowledge prefix adapter for \nLLM integration. Through these processes, the LLMs are \nenhanced with domain-specific knowledge, enabling them to \neffectively perform the machining process knowledge graph \ncreation task.\n3.1  Construction of the MPKG\nIn this study, the MPKG is established by integrating a \ntop-down schema layer with a bottom-up data layer, as \nreferenced in [19, 20]. The framework for this construction \nprocess is illustrated in Fig.  1. The schema layer serves as \nthe foundation for the process KG and is developed uti-\nlizing Protégé software. Notably, the schema layer of the \nMPKG is constructed through an ontology-based method, \neffectively managing heterogeneous process knowledge \nacross various dispersed machining domains. The ontol-\nogy construction process employed in this paper follows \nthe seven-step method from Stanford University, which \nis depicted in Fig.  2. Regarding the data layer, this study \nemploys the BTC model, with its structure illustrated \nin Fig.  3. As shown, the BTC model consists of three \ncomponents: BERT, transformer, and CRF. BERT func-\ntions as the character encoder to extract word features, \nfollowed by the transformer, which acts as the sentence \nencoder to capture sentence-level features. Finally, the \nCRF serves as the decoding layer for labelling purposes. \nThrough these operations, entities pertinent to the machin-\ning process are extracted from the collected data, contrib-\nuting to the development of the data layer. Subsequently, \nthe information derived from the data layer is utilized \nto create a comprehensive KG of the machining process \nthrough knowledge fusion, entity linking, and integration \nwith the schema layer. The following paragraphs introduce \nthe key modules involved in constructing the MPKG in \ndetail.\nThe concept of ontology refers to the terms used to \ndescribe a domain, organized hierarchically and struc-\ntured systematically. Domain ontology for the machin-\ning process formally defines the concepts and relation-\nships between concepts within this field. By developing \na domain ontology model for the machining process, het-\nerogeneous knowledge from various machining fields can \nbe effectively managed, transforming unstructured process \nknowledge into a logical and relatively complex system.\nFig. 1  The flowchart for constructing the MPKG\nFig. 2  The flowchart for seven \nsteps method\n\n730 The International Journal of Advanced Manufacturing Technology (2025) 138:725–739\nThe construction method for the machining process \ndomain ontology model is based on Stanford University’s \nontology construction seven-method, as depicted in Fig.  2. \nThe machining process ontology, which defines different \nattributes, such as manual perspective, and equipment per -\nspective, is developed using the Protégé as the ontology edi-\ntor, in a manual perspective. Once the attributes are defined, \nthe properties for the machining process are then manually \nconstructed.\nThe next step involves knowledge extraction, where the \nknowledge must be represented in a standardized format. \nIn this study, the resource description framework (RDF), a \ndata model that uses XML, is employed to achieve this goal. \nOnce the RDF representation for the machining process is \nestablished, the knowledge extraction is then performed. In \nthis paper, the knowledge extraction for the machining pro-\ncess is classified into three types based on the data class, i.e., \nstructured data, unstructured data, and semi-structured data.\nIn this study, knowledge extraction for the structured data \nis compiled using the database to RDF (D2R) under manual \nguidance. Given the limited amount of structured data, this \nmethod remains feasible and effective. Similarly, knowledge \nextraction for semi-structured data is also achieved using \nD2R. However, unlike the conventional approach of directly \napplying D2R, the semi-structured data must first undergo \ntransformation into a structured format. For the unstructured \nformat data, knowledge extraction is carried out with the \nhelp of deep learning-based methods.\nIn general, knowledge extraction from unstructured data \ninvolves two key steps, i.e., named entity recognition (NER) \nFig. 3  The flowchart of the NER using the BTC\n731The International Journal of Advanced Manufacturing Technology (2025) 138:725–739 \nand relation extraction. As the central focus of this paper \nis on knowledge extraction, the discussion is limited to the \nNER for the machining process is discussed. The NER is \na core technique that is employed in the construction of \nMPKG, aiming to identify named entities in unstructured \ntext data and classify them according to predefined catego-\nries. In this study, the NER is implemented using the BTC. \nThe flowchart for BTC is shown in Fig. 3, and details about \nthe BTC are introduced in the following paragraphs.\nAs shown in Fig.  3, BERT is used for word embed-\nding, serving as the foundation for entity extraction in the \nmachining process domain. BERT transforms the machin-\ning knowledge into a standard form that can be processed \nby a computer. After word embedding, the features of \nsentences in the machining field are extracted using a \ntransformer to achieve context encoding. The employed \ntransformer structure, depicted in Fig.  3, includes mul-\ntiple encoders, each composed of a multi-head attention \nmechanism and feed-forward network [28]. The attention \nmechanism employed aids in identifying the entity cat-\negories and speeds up the training process for the model. \nThe decoder in the transformer can output a word matrix \nthat automatically extracts the sentence features. Using \nthese attention mechanisms and the decoder, the tags of \nmachining knowledge are classified and sorted. Then, \nthe conditional random field (CRF) is then used as the \ndecoding layer for labelling purposes [29]. Since there \nwould be some violations during the word embedding and \ncontext encoding process, the CRF is employed to correct \nthe naming irregularities. During the CRF process, the \nViterbi algorithm is employed to give a solution to the \ncalculation procedure.\n3.2  MPKG‑embedded LLMs pretraining\nThe architectural framework of MPKG-embedded LLMs \nis systematically illustrated in Fig.  4. The implementation \ninvolves three progressive phases to achieve knowledge-\nenhanced language modeling. Initially, structural embedding \npretraining is conducted to capture hierarchical patterns and \nsemantic associations between entities and relations within \nthe MPKG. This phase employs graph neural networks or \nknowledge graph embedding techniques to generate dense \nvector representations that encode topological character -\nistics, entity relational dependencies, and domain-specific \ncontextual features.\nSubsequently, these distilled structural embeddings \nundergo spatial transformation through a specialized struc-\ntural prefix adapter module. This critical component projects \nthe graph-derived numerical embeddings into the LLM’s \nnative textual representation space using learnable linear \ntransformations or cross-modal attention mechanisms. \nThe transformed embeddings are then prepended as prefix \ntokens to the original input sequence S, forming a hybrid \nknowledge-text sequence. This integration strategy enables \nthe LLM to process structural patterns as contextual prompts \nwhile maintaining compatibility with standard tokenization \nworkflows, as visually depicted in the right panel of Fig. 4.\nThe final optimization stage involves fine-tuning the aug-\nmented LLM using the structurally enriched input sequences \nthrough task-specific objectives. This dual-phase training \nparadigm—combining pretrained structural knowledge \nwith linguistic patterns—facilitates enhanced reasoning \ncapabilities while preserving the model’s foundational lan-\nguage understanding. Central to this architecture are two \ninnovatively designed components: the structural embed-\nding pre-training (SEP) module, responsible for graph \nknowledge distillation, and the knowledge prefix adapter \n(KPA), which bridges discrete knowledge representations \nwith continuous text embeddings. Subsequent sections pro-\nvide comprehensive technical specifications for these core \nsubsystems, detailing their algorithmic implementations, \ntraining protocols, and synergistic interactions within the \noverall framework.\nFig. 4  The overall structure of the MPKG-embedded LLMs\n732 The International Journal of Advanced Manufacturing Technology (2025) 138:725–739\n(1) SEP for the MPKG-embedded LLMs\nIn this study, the mathematical form of the developed \nMPKG is denoted as G = (E, R , T, D) . Here the E denotes the \nentity in the machining process knowledge; R denotes the \nrelationship set. T = {(h, r, t)/uni007C.varh, t∈ E} represents the triple \nset and description set for each entity and relation, respec-\ntively. By defining these terms in MPKG, the SEP for the \nMPKG-embedded LLMs can be performed. As previously \nmentioned, the KPA is used to extract the structural infor -\nmation of the entities and relations in MPKG through SEP, \nwhich is conducted in a self-supervised way. Details about \nthe procedure for SEP are as follows.\nFirstly, the structural embeddings e ∈ Rde , r ∈ Rdr are \nlearned using the Graph AutoEncoder (GAE). Here, the e \nand r denote each entity and each relation, with e ∈ E and \nr ∈ R . The GAE is composed of two parts, i.e., the encoder \nand decoder. The encoder transforms the raw graph fea -\ntures X to the hidden embeddings H. On the contrary, the \ndecoder in GAE transforms H to the final embeddings Z. \nAnd Z = {e ∈ Rde , r ∈ Rdr} . The encoder process can be for-\nmulated as follows:\nwhere fE  denotes the encoder function, which includes a \nlinear layer and activation layer; W E denotes the learning \nparameters matrix; A is the MPKG adjacency matrix. Espe-\ncially, the encoder function can be represented as follows by \nadding the aggregate function:\nwhere the /u1D70E denotes the activation function such as ReLU, \nSigmoid. The Aggregate denotes neighbor information \naggregation operation, and it can be represented as follows:\nIn this way, the encoder function can be obtained as \nfollows:\nwhere W E denotes the learning parameters matrix, which is \na trainable parameter denoted as weights in the neural net-\nwork using the back-propagation algorithm, and bE denotes \nthe bias, which is also a trainable parameter using the back-\npropagation algorithm. Accounting for the MPKG adjacency \nmatrix A, it is constructed from the triples (head entity, rela-\ntion, tail entity) of the knowledge graph. Then, the adjacency \nmatrix A (i,j,k) = 1 denoted   as the head entity ei has the k th \nrelationship with tail entity ej . Here, the e and r denote each \n(1)H = fE\n/parenleft.s1A, X, W E/parenright.s1\n(2)H = fE\n/parenleft.s1A, X, W E /parenright.s1= /u1D70E/parenleft.s1W E ⋅ Aggregate(A, X)\n/parenright.s1\n(3)/uni0303.s1X i =\n/uni2211.s1\nj∈/u1D718(i)\nA i,j ⋅ X j\n(4)H = /u1D70E\n/parenleft.s2\nW E ⋅ /uni0303.s1X + bE\n/parenright.s2\nentity and each relation, with e ∈ E and r ∈ R . The raw \ngraph features X is obtained by using the BERT mentioned \nin the above part, and it is represented as XϵRN ×dx , where dx \ndenotes the feature dimensional.\nThe physical formula for the decoder operation is also \npresented as follows:\nwhere fD  denotes the decoder function, and it is also com-\nposed of a linear layer and an activation layer. And WD is the \nlearnable parameters matrix. The embedding operations can \nbe achieved using the above encode and decode operations.\nAfter the embedding operation, the embeddings are \nadapted into the textual representation space of LLMs. \nDuring the encoding and decoding stages, a score func -\ntion F(h, r, t) is used to measure the plausibility of the triple \n(h, r, t) . In this study, negative sampling is utilized to achieve \na self-supervised pre-training objective. This operation can \nbe expressed with the formula as follows:\nwhere /u1D6FE denotes the margin; /u1D70E denotes the sigmoid activa-\ntion function; (h �\ni, r�\ni, t�\ni) denotes the K negative sampling of \n(h, r, t) . The i=( 1,2,… ,K ) . pi denotes the self-adversarial \nweights. Lpre denotes the pre-training loss.\nThe structural embeddings of each entity and relation in \nthe MPKG are refined through an iterative optimization pro-\ncess driven by a multi-objective pretraining loss function. \nThis loss function typically combines contrastive learning \nobjectives to distinguish valid triples from corrupted coun-\nterparts, distance-based geometric constraints, and seman-\ntic similarity metrics that preserve contextual relationships \nacross heterogeneous knowledge domains. During pretrain-\ning, graph-aware neural architectures—such as relational \ngraph convolutional networks (R-GCNs) or rotated knowl -\nedge embeddings—iteratively update entity/relation vec-\ntors through message-passing operations across connected \ntriples, enabling the embeddings to holistically encode both \nlocal neighborhood structures (e.g., immediate connections) \nand global graph topology (e.g., hierarchical dependencies). \nThe optimization process explicitly enforces that the learned \nembeddings satisfy two critical properties:\nTriple faithfulness: The geometric relationships between \nentity pairs are preserved in the vector space, ensuring \nembeddings mathematically reflect all associated triples. \nStructural invariance: The embeddings capture invariant \npatterns across subgraph motifs through adaptive atten-\ntion mechanisms that weight multi-hop relational paths. To \nachieve this, the SEP module employs hierarchical graph \nattention layers that aggregate features from neighbor -\ning nodes while dynamically adjusting to relation-specific \n(5)Z = fD\n/parenleft.s1H, W D /parenright.s1\n(6)Lpre = 1\n/uni007C.varT/uni007C.var\n/uni2211.s1\n(h,r,t)∈T\n(−log/u1D70E(/u1D6FE− F(h,r,t)))−\nK/uni2211.s1\ni=1\npilog/u1D70E(F(h�\ni,r�\ni,t�\ni)−/u1D6FE))\n733The International Journal of Advanced Manufacturing Technology (2025) 138:725–739 \nsemantics, coupled with negative sampling strategies that \ngenerate hard negatives by replacing entities/relations in tri-\nples based on type constraints. Additionally, edge dropout \nand relation-aware regularization techniques are applied to \nprevent overfitting to sparse subgraphs and enhance gener -\nalization across diverse knowledge perspectives.\nThrough this optimization, the final structural embed-\ndings not only encapsulate explicit triple facts but also \nimplicitly model latent dependencies—such as entity clus-\nters, relation chains, and cross-domain analogies—forming \na continuous representation space where geometric prox-\nimities correlate with semantic and structural affinities. \nThese enriched embeddings are subsequently formatted \nas learnable prefix tokens through the KPA module, ena-\nbling LLMs to condition their predictions on both tex-\ntual context and the geometric relationships encoded in \nthe MPKG. The SEP’s modular design allows seamless \nintegration with various KG embedding paradigms while \nmaintaining scalability for large-scale, multi-relational \nknowledge graphs, thereby establishing a robust founda-\ntion for downstream knowledge-text alignment tasks.\n(2) KPA for the MPKG-embedded LLMs\nAs mentioned earlier, once the SEP for the MPKG is \ncompleted, the structural embeddings (h, r, t) for a triple \n(h, r, t) can be obtained. These embeddings encapsulate the \nMPKG’s structural information. Since the LLMs cannot \ndirectly interpret these structural embeddings due to their \ndifferent representation spaces compared to the textual rep-\nresentation space of LLMs M , the KPA is used to map these \nstructural embeddings into the textual token representation \nspace of LLMs. This process converts the structural embed-\ndings into several virtual knowledge tokens K by P using the \nfollowing physical formula representation:\nwhere P denotes the adapter and is developed with a sin-\ngle linear projection layer, designed to align the pretrained \nencoded features with the LLMs while keeping other lan-\nguage components unchanged. ⊕ is the textual token con-\ncatenation operation. Following, the K is placed ahead of \nthe original input sequence S, serving as the prefix to the \ninstruction. Accordingly, the triple prompt can be repre-\nsented as follows:\nwhere X denotes the triple prompt; I represents the instruc-\ntion prompt; and Iit denotes the details of completing the \ntriple classification task in MPKG. By doing this, the prefix \nK and all the following text tokens are processed together, \ndue to the unidirectional attention in decoder-only LLMs. \n(7)K = P(h ) ⊕ P(r) ⊕ P(t)\n(8)Skpa = K⊕Iit ⊕ X\nBased on this, unidirectional attention can be directed \ntoward the structural embeddings of the input triple by the \ntextual tokens. Accordingly, the structure-aware prompt will \nbe used during fine-tuning and inference.\nBy defining the above notations and mathematical for -\nmula, the MPKGC for LLMs can be achieved. Consequently, \nthe training process can be initiated. During this training, the \npre-trained structural embeddings from previous chapters, \nare initially frozen. Then, the adapter needs to be optimized, \nto learn the mapping relationship from structural knowledge \nto textual representation, enabling generalization to new tri-\nples in the inference stage. This operation benefits the textual \ndescription and provides the triple information from an addi-\ntional perspective, enhancing predictions. Following these \nsteps, the KPA for the MPKG can be completed, allowing \nthe MPKG to be transferred and embedded into the LLMs, \nwhich leverages the strengths of both LLMs and MPKG.\nTo illustrate the proposed method more clearly, the \nexploitation of the method will be discussed in detail in the \nfollowing paragraphs. For the LLM backbone, the Alpaca- \n7B model is employed. Alpaca- 7B is an improved version \nof the Llama model fine-tuned on instruction-following \ndata. For the tuning process, this paper utilizes Alpaca using \nlow-rank adaptation of large language models (LoRA) with \nrank 64 [30]. Using LoRA, the weights in the pre-trained \nmodel are frozen, and trainable rank decomposition matri-\nces can be integrated into each layer of the transformer \narchitecture, significantly reducing the number of trainable \nparameters for downstream tasks. The optimizer used in \nthis paper is the AdamW with a fixed size. By incorporat-\ning these components, the LLMs can be employed for the \nMPKGC task under the guidance of the existing KG, trans-\nforming the conventional black-box training into a more \ntransparent process. The originally constructed MPKG \ncan be further compensated using the LLMs for enhanced \nperformance.\n4  Experimental verification and result \ndiscussion\n4.1  Experimental settings\nThe development environment for this study consists of a \nWindows 10 operating system with a 64-bit architecture, \nan Intel Core i9 − 11950H CPU, 128 GB of memory, and \n16 A100 GPU. The entire training process is supported by \nHuawei, Chengdu. Python 3.9.16 is employed in this paper \nfor the model development, and in terms of the graph data-\nbase, Neo4j version 4.3.4 is employed. The key modules in \nPython are torch 2.0.0 and peft 0.3.0.\nIn this study, the data employed in this paper is continu-\nously collected from real-world industrial scenarios at an \n734 The International Journal of Advanced Manufacturing Technology (2025) 138:725–739\nenterprise for manufacturing gear-cutting machines. The \ndataset is stored by the companies, and this corpus contains \nmore than 500 different types of fault localization cases with \nfailure phenomena, fault causes, and troubleshooting meth-\nods, which are collected from more than 1000 different gear-\ncutting machines. In this experiment, the data were collected \ncontinuously for more than 10 years with the data volume \nof 6.112 Tb. Especially, some raw data are firstly recorded \nusing handwritten text, and such data are transferred into \nelectric data using OCR-related techniques. Detailed speci-\nfications of the data collection information are presented in \nthe Table 1.\nThe proposed method follows a seven-step ontology con-\nstruction process during the conducting procedure. First, \nentity extraction from unstructured machining process data \nis performed using the proposed BTC model with the trans-\nformers 4.28.0 Python module. This method allows for the \nautomatic construction of the machining process knowledge \ngraph. As mentioned above, the MPKG with the machin-\ning of gear is used as a case for developing the knowledge \ngraph, which is ultimately implemented using Neo4j. For \nthe second part, the Alpaca- 7B is used as the backbone of \nLLMs, and the number of epochs is searched in {3, 4, 5} \nand the learning rate is tuned within the scope of {1e  − 4, \n3e − 4, 5e − 4}, the AdamW optimizer is used with a fixed \nbatch size of 12. A trail-and-error method is employed to \ndetermine the best hyperparameters.\nTo evaluate the performance of the final developed model, \nit is taken as the classification task, specifically binary clas-\nsification with all test datasets being label-balanced. The \nmetrics employed in this paper include accuracy (Acc), pre-\ncision (Pr), and recall (Re), and the corresponding calcula-\ntion formula is as follows:\n(9)Pr = TP\nTP + FP ∗ 100%\nwhere TP and FP denote the right and wrong solution given \nby the model; FN denotes the false decision for the negative \ndecision.\nTo demonstrate the effectiveness of our proposed meth-\nods, the comparative methods are depicted in this part. The \nbaseline methods for comparison include the conventional \nKGs, LLMs, and some other KG-LLMs. The KGs mentioned \nin this study are developed using the BTC, and the LLMs \ndenote the general LLMs without elaborating the profes-\nsional knowledge, which denotes the training of LLMs using \nthe corpus directly without a knowledge graph involvement. \nAnd the KG-LLMs denote the traditional KG-guided LLMs \nin a manual training way. The KG-LLMs denote the integra-\ntion of the KG with the LLMs in the conventional way, in \nwhich it is conducted in a pretraining way only.\n4.2  Experimental results and comparison \nof the model on the hallucination problems\nAs previously mentioned, the performance of the proposed \nmethod is discussed and compared with the conventional \nmethods to highlight its advantages and effectiveness. The \nevaluation questions are randomly selected from the corpus, \nwhich has been previously organized and recorded, and are \nmarked as normal questions in this paper. Additionally, an \nexperiment is conducted to test the robustness of the devel-\noped model. Some prompts that contradict common sense \nare input into the developed model, which are marked as \nhallucination problems. The responses generated by various \nmodels are compared to assess their understanding of knowl-\nedge semantics and their knowledge inference abilities in the \n(10)Re = TP\nTP + FN ∗ 100%\n(11)Acc = TP + TN\nTP + FP + FN + TN ∗ 100%\nTable 1  Specifications of the \ndata collection information Types Information\nData acquisition method (1) Hardware: OCR, camera\n(2) Software: scraping\nAcquisition time (1) From 2013.01.01\n(2) To 2024.01.01\nData volume (1) 6.112 Tb\nData type (1) Handwritten text\n(2) Electric text\n(3) Image information\n(4) Video information (transfer into image)\nData labeling (1) more than 500 different types\n(2) more than 1000 different gear cutting machines\n(1) Human labors\n735The International Journal of Advanced Manufacturing Technology (2025) 138:725–739 \nmachining process. This comprehensive evaluation aims to \nassess the model’s performance from multiple perspectives.\nThis study presents a paradigmatic two-class experiment. \nAnd the generated confusion matrix for various models is \ndepicted in Fig.  5. As shown in Fig.  5, a total of 120 data \npoints is employed in this paper. The TP, FP, TN, and FN are \nused to evaluate the correctness of the final developed sys-\ntem. The TP and FN indicate correct and incorrect answers \nfor the normal questions, while TN and FP represent the \napproximate and inapproximate measures taken in response \nto the hallucination problems. The higher number of TN \nindicates the proposed method effectively addresses hallu-\ncination problems, while the FP suggests the opposite. The \ncomparison includes KG, LLM, and KG-LLM methods, \nconstructed using a knowledge graph, large language model, \nand knowledge graph-embedded LLM, and the proposed \nmethod, referred to as “Pro,” is also evaluated.\nFigure 5 clearly shows that the proposed method dem-\nonstrates the most stable performance over different tasks, \nwith the highest number in TP and TN. Additionally, the \nKG performs exceptionally well in handling hallucina-\ntion problems. This phenomenon is largely attributed to its \nlimited inference ability for the unknown problem, which \nresults in meaningless results for certain input questions. In \ncontrast, the LLMs, encountered significant issues with the \nhallucination problem during the exploitation stage due to \nFig. 5  The confusion matrix for the different models\n736 The International Journal of Advanced Manufacturing Technology (2025) 138:725–739\nthe black-box nature of their training processes. The KG-\nLLM has shown improved performance in managing both \nnormal questions and hallucination problems in the train-\ning dataset; however, it still underperforms relative to the \nproposed method, highlighting the advantages of the latter \nmethod across both task types. To further demonstrate the \neffectiveness of the proposed and compared method over the \nhallucination problems, two hallucination-related questions \nare input into the model and the output response for the dif-\nferent models are presented in the Table 2.\nTo demonstrate the effectiveness and advantage of the \nproposed method, a numerical statistical analysis of the \nperformance over different models is given and presented \nin Table 3. The physical metrics used in the table are based \non formulas (9), (10), and (11). The values for TP, TN, FN, \nand FP come from Fig.  5. As shown in Fig.  5, there are \nfour sub-matrices. The upper-left corner, lower-left corner, \nupper-right corner, and lower-right corner in the sub-matrix \ndenote the TP, FN, FP, and TN, respectively.\nAs shown in Table 3, the KG exhibits the highest preci-\nsion, achieving 100%. This exceptional precision is attrib-\nuted to the KG’s capability to address hallucination prob-\nlems effectively. However, this advantage is counterbalanced \nby the KG’s limited performance in the recall and accuracy, \nwhich hinders their application. This poor performance is \nalso caused by its poor inference ability. The LLM dem-\nonstrates the lowest performance, largely due to its black-\nbox training characteristic that adversely affects its gener -\nalization capabilities. In comparison, the proposed method, \nPro, performs an overall performance, attaining 97.50% in \naccuracy, 98.30% in recall, and 96.67% in precision. These \nresults have underscored the proposed method’s effective-\nness in decision-making within the machining process.\n4.3  Experimental results on the inference ability \nof the joint knowledge enhancement model\nThe second part of the study assesses the inference ability \nof various models. In this part, newly generated problems \nfrom the factory, which were not part of the training corpus, \nare used as input for the models. To evaluate the inference \nability of the proposed method, ten different workers were \ninvited to conduct a blind evaluation of the final answers \ngenerated by the models. During this evaluation, the workers \nassessed the correctness of the generated answers without \nknowing the generated model and assigned scores from 1 to \n10, reflecting their judgment of the answers’ accuracy and \nrelevance. This approach ensures an unbiased assessment of \nthe models’ inference abilities and user-friendliness.\nThe statistical results regarding the inference ability of \nvarious models are presented in Table  4. It can be obtained \nfrom Table  3 that the proposed method demonstrated the \nhighest performance in inference ability, achieving a score \nof 9.1. This result indicated that the proposed method can \neffectively guide the workers in the actual manufactur -\ning industry. Conversely, the poorest performance was \nobserved in the KG model. This is largely due to the KG’s \nheavy reliance on past recorded experiences. While such \nan approach can be reliable and effective when dealing \nTable 2  The hallucination problems and corresponding response\nHallucination questions Models Response\n1. Give me a maintenance suggestion, \nmake the machine tool vibration to 0\nKG None\nLLMs To make the vibrations to zero, here are some maintenance suggestions, etc\nKG-LLMs Minimizing machine tool vibration to absolute zero is challenging, etc\nPro We are sorry to generate such plans, achieving zero vibration in a machine tool is prac-\ntically impossible, as some level of vibration is inherent in any mechanical system. \nThe vibrations can only be reduced, here are some advice, etc\n2. The vibration of tool is 0, how to solve it KGs None\nLLMs If this is the case, it is generally considered a desirable situation as minimal or zero \nvibration indicates stability and precision in the machining process\nKG-LLMs Zero vibration is generally desirable as it implies stability and precision, it’s important \nto clarify whether you are experiencing an issue or if you expect some level of vibra-\ntion\nPro Zero vibration is typically not achievable during machine tool operation. If zero vibra-\ntion occurs, it is essential to check the data acquisition system or the equipment used \nto measure and collect vibration data. Here are a few steps to consider: etc\nTable 3  Comparison of experimental results\nMethods KG LLM KG-LLM Pro\nPr 100% 75.43% 80.30% 96.67%\nRe 81.67% 71.66% 88.33% 98.30%\nAcc 90.83% 74.16% 83.33% 97.50%\n737The International Journal of Advanced Manufacturing Technology (2025) 138:725–739 \nwith similar questions, this cannot always be satisfied \nin the manufacturing industry. The relatively poor per -\nformance of LLM is largely caused by a lack of corpus \ndiversity, which reduces its inference ability. Although the \nconventional joint KG-LLM model improved the inference \nability, with a score of 7.8, it still underperforms compared \nto the proposed method.\nTo further evaluate the effectiveness and user-friendli-\nness of our proposed method and other compared methods, \nthe previously invited workers were invited to score the \nuser-friendliness of the final generated models, and the \ncorresponding scores of various models are presented in \nTable  3. The KG has the poorest performance in user-\nfriendliness; this is largely caused by the graph-based \ninterface used during the application stage, which requires \npre-trained skills to navigate and increases the complexity \nfor users. The LLM, KG-LLM, and our proposed method \nreceived relatively similar scores in user-friendliness, \nwhich was also brought by its little involvement in the \ntraining stage for using these methods.\n4.4  Deployment in the manufacturing domain\nTo further demonstrate the effectiveness and applicability \nof this proposed method, a further case study that deploys \nthe proposed method in a real manufacturing industry is \ngiven in this part. The whole deployment architecture of this \nproposed method is depicted in Fig.  6, which is conducted \nunder the context of data-driven smart manufacturing. As \nshown in Fig.  6, it can be obtained that it is composed of \nthree main parts, data collection layer, data processing layer, \nand model function layer.\nIn terms of the data collection layer, it can be obtained \nthat it is composed of two main parts, i.e., online inspection \ndata and offline daily working records. Accounting for the \nonline inspection data, it is used for real-time monitoring \nof machine tools. In this part, the data is collected from the \nCNC systems, which are conducted with the help of OPC \nUA. In the meantime, sensors are installed on the machine \nto further help realize the machine’s fault, especially some \nother information that would be provided by the information \nsystem, such as ERP and MES. By using such kind of infor-\nmation, the real-time running behaviors that can be used to \nreflect the fault features can be captured. In the aspect of \noffline working records, it is denoted that the maintenance \ninformation is recorded by the operators. As is known to us, \nthe online inspection data cannot reflect the fault features in \na comprehensive perspective. Thus, the expertise knowledge \nis employed in this paper to help fill the gap caused by the \nshortcomings.\nAfter the data collection layers, the data would be trans-\nferred into the data processing layers, which are conducted on \nthe edge side and cloud side in the real manufacturing indus-\ntry. In this part, the data would be cleaned at first to eliminate \nthe abnormal data. Especially, the structure of the collected \nwould be different; thus, a specialized data cleaning algorithm \nis required for each kind of data. Finally, data transformation \nis employed to transfer those different structures of data into \na unified form. Finally, the collected data would be stored and \ninput into the maintenance part to realize an automation fault \ndiagnosis.\nTable 4  Comparison of evaluation results\nMethods KG LLM KG-LLM Pro\nInference ability 4 5.6 7.8 9.1\nUser friendly 3 9.5 9.4 9.4\nFig. 6  The structure of the \ndeployment environment\n\n738 The International Journal of Advanced Manufacturing Technology (2025) 138:725–739\nThe proposed KG-guided LLMs would be deployed in the \nfinal layer, and it can be obtained from Fig. 6 that the diag-\nnosed fault accompanied by the collected would serve as input \nfor the developed model. Then, the developed model would \ngenerate the corresponding response about the root cause of \nthe fault and the maintenance advice for the fault. By using \nthe proposed structure, an end-to-end framework that helps \nrealizing the smart machining can be developed, in which the \nmachine tools can be monitored, diagnosed, and maintained \nwithout human involvement.\nTo further evaluate the effectiveness of the proposed struc-\nture, the calculation time for the employed structure is pre-\nsented in Table 5. It can be obtained from the table that the \nfault diagnosis time is 10.47 s, which can realize a near real-\ntime diagnosis way. Accounting for the maintenance response \ngeneration, it costs 59.85 s. This time is averaged results of \nmultiple different response generation processes, and this \nhigher computation time is mostly due to the complexity of \nthe input questions and hardware environment. The total cal-\nculation time is 411.55 s; this is mostly related to the data \nprocessing and transferring part, which costs a longer time. \nThis would be improved by developing a novel IOT structure, \nand it would be discussed in this paper.\n5  Conclusion and future work\nAn expert system that integrates the KG and LLMs was \ndeveloped in the domain of the machining process, leveraging \nthe synergistic benefits of professional KGs and LLMs. This \nstudy aims to enhance the knowledge inference ability for the \njoint MPKG and LLMs, in the context of decision-making \nfor fault localization and root cause identification during the \nCNC machining process. By integrating those models into the \ndeveloped system, the ability to address the unseen problem \ncan be enhanced.\nThis study first proposes an automatic knowledge \ngraph-constructing method using the BTC. Leveraging \nthe capabilities of the transformers, the unconstructed \nknowledge related to fault localization, root cause identi-\nfication, and troubleshooting can be mapped in the graph \nstructure. Then, the LLMs are employed to complete the \ndeveloped MPKG and learn the factual knowledge from \nthe developed MPKG. By doing so, the hallucination \nproblem met in the LLMs has been significantly reduced, \nand knowledge inference ability is also improved. Finally, \nthis proposed method is compared with the current exist-\ning method in terms of accuracy, precision, and recall, \nachieving impressive results of 97.50%, 96.67%, and \n97.5%, respectively, indicating its superiority. Besides, \nthe knowledge inference ability and user-friendliness \nwere objectively compared with the conventional methods, \nyielding scores of 9.1 and 9.4, demonstrating its practical \neffectiveness in the actual manufacturing industry.\nAlthough the proposed method has shown improvements \nfor the final model, it has suffered a knowledge boundary \nwhen addressing the unseen problems. This limitation is \nprimarily due to constraints in feature extraction and re-\norganization during the exploitation stage. In future work, \nwe aim to expand the boundary of the proposed method to \naddress these challenges.\nAuthor contributions Pengcheng Wu: data curation, writing, original \ndraft, methodology, software; Xun Mou: formal analysis; Leihao \nGong: data processing; Haobei Tu: data processing; Linqiong Qiu: \nconceptualization, investigation, funding, experiment; Bo Yang, funding.\nFunding Open Access funding enabled and organized by CAUL and \nits Member Institutions. This study was supported by the National Key \nResearch and Development Program of China (no. 2023YFB3306800) \nand in part supported by the Fundamental Research Funds for the \nCentral Universities (grant number SWu-KQ24023).\nData availability The data can be obtained by reasonable requirement.\nCode availability Not applicable.\nDeclarations \nEthics approval Not applicable.\nConsent to participate Not applicable.\nConsent for publication Yes.\nConflict of interest No potential conflict of interest was reported by \nthe authors.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n 1. Wu P, He Y, Li Y, Wang Y, Wang S (2022) Online prediction \nof cutting temperature using self-adaptive local learning and \ndynamic CNN. IEEE Trans Ind Informat 18(12):8629–8640\nTable 5  The statistics of the computation time\nItems Fault diagnose Maintenance advice generation Total\nTime 10.47 s 59.85 s 411.55 s\n739The International Journal of Advanced Manufacturing Technology (2025) 138:725–739 \n 2. M.-Q. Tran, H.-P. Doan, V. Q. Vu and L. T. Vu, “Machine learning \nand IoT-based approach for tool condition monitoring: a review \nand future prospects”, Measurement, vol. 207, Feb. 2023.\n 3. Cao H, Shao H, Zhong X, Deng Q, Yang X (2022) Unsupervised \ndomain-share CNN for machine fault transfer diagnosis from \nsteady speeds to time-varying speeds. J Manuf Syst 62:186–198\n 4. Luo B, Wang H, Liu H, Li B, Peng F (2019) Early fault detection \nof machine tools based on deep learning and dynamic identifica-\ntion. IEEE Trans Ind Electron 66(1):509–518\n 5. Li Y, Wang X, He Y, Wang Y, Wang Y, Wang S (2022) Deep \nspatial-temporal feature extraction and lightweight feature \nfusion for tool condition monitoring. IEEE Trans Ind Electron \n69(7):7349–7359\n 6. Li X, Liu X, Yue C, et al (2022) Systematic review on tool break-\nage monitoring techniques in machining operations. Int J Mach \nTools Manuf 176:103882\n 7. Lei Y, Yang B, Jiang X, et al (2020) Applications of machine \nlearning to machine fault diagnosis: A review and roadmap. Mech \nSyst Signal Process 138:106587\n 8. Chen M, Qu R, Fang W (2022) Case-based reasoning system for \nfault diagnosis of aero-engines. Expert Syst Appl 202:117350\n 9. Dou D, Yang J, Liu J et al (2012) A rule-based intelligent method \nfor fault diagnosis of rotating machinery[J]. Knowl-Based Syst \n36(1):1–8\n 10. Cai C, Jiang Z, Wu H, Wang J, Liu J, Song L (2024) Research \non knowledge graph-driven equipment fault diagnosis method \nfor intelligent manufacturing. Int J Adv Manuf Technol \n130(9–10):4649–4662\n 11. Ji S, Pan S, Cambria E, Marttinen P, Yu PS (2022) A survey on \nknowledge graphs: representation acquisition and applications. \nIEEE Trans Neural Netw Learn Syst 33(2):494–514\n 12. Makatura L, Foshey M, Wang B, et al (2024) How can large lan-\nguage models help humans in design and manufacturing? Part 2: \nSynthesizing an end-to-end LLM-enabled design and manufactur-\ning workflow. Harvard Data Sci Rev\n 13. Kernan Freire S, Wang C, Foosherian M, Wellsandt S, Ruiz-Are-\nnas S, Niforatos E (2024) Knowledge sharing in manufacturing \nusing LLM-powered tools: user study and model benchmarking”. \nFront Artif Intell 7:1293084\n 14. Yin W, Xu M, Li Y, Liu X (2024) LLM as a system service on \nmobile devices. arXiv: 2403. 11805. Accessed 3 Aug 2024\n 15. Liu P, Qian L, Zhao X, Tao B (2024) Joint knowledge graph and \nlarge language model for fault diagnosis and its application in \naviation assembly. IEEE Trans Ind Informat 20(6):8160–8169\n 16. Li Pulin et al (2022) Investigation on industrial dataspace for \nadvanced machining workshops: enabling machining operations \ncontrol with domain knowledge and application case studies. J \nIntell Manuf 33:103–119\n 17. Cheng Kai et al (2017) Smart cutting tools and smart machining: \ndevelopment approaches, and their implementation and applica-\ntion perspectives. Chin J Mech Eng 30:1162–1176\n 18. Xiao Y, Zheng S, Shi J, Du X, Hong J (2023) Knowledge graph-\nbased manufacturing process planning: a state-of-the-art review. \nJ Manuf Syst 70:417–435\n 19. Guo L, Yan F, Li T, Yang T, Lu Y (2022) An automatic method for \nconstructing machining process knowledge base from knowledge \ngraph. Robot Comput-Integr Manuf 73\n 20. Guo L, Li X, Yan F, Lu Y, Shen W (2024) A method for construct-\ning a machining knowledge graph using an improved transformer. \nExpert Syst Appl 237:121448\n 21. Bao Q, Zheng P, Dai S (2024) Hierarchical construction and \napplication of machining domain knowledge graph based on as-\nfabricated information model. Adv Eng Inform 62:102638\n 22. Liang J et al (2024) A knowledge graph-based approach to mod-\neling & representation for machining process design intent. Adv \nEng Inform 62:102645\n 23. Diamantini C, Mircoli A, Potena D, Storti E (2023) Process-aware \nIIoT knowledge graph: a semantic model for industrial IoT inte-\ngration and analytics. Futur Gener Comput Syst 139:224–238\n 24. Andrus BR, Nasiri Y, Cui S, Cullen B, Fulda N (2022) Enhanced \nstory comprehension for large language models through dynamic \ndocument-based knowledge graphs. Proc AAAI Conf Artif Intell \n10436–10444\n 25. Yang L, Chen H, Li Z, et al (2024) Give us the facts: Enhancing \nlarge language models with knowledge graphs for fact-aware lan-\nguage modeling. IEEE Trans Knowl Data Eng 36:3091–3110\n 26. Meyer L-P et al. (2024) LLM-assisted knowledge graph engineer-\ning: experiments with ChatGPT. 103–115\n 27. Zhang Y, Chen Z, Guo L, et al (2024) Making large language \nmodels perform better in knowledge graph completion. In: Pro-\nceedings of the 32nd ACM International Conference on Multime-\ndia. pp 233–242\n 28. Han K et al (2023) A survey on vision transformer. IEEE Trans \nPattern Anal Mach Intell 45(1):87–110\n 29. Acheampong FA, Nunoo-Mensah H, Chen W (2021) Transformer \nmodels for text-based emotion detection: a review of BERT-based \napproaches. Artif Intell Rev 1–41\n 30. Hu EJ, Shen Y, Wallis P, et al (2022) Lora: Low-rank adaptation \nof large language models. ICLR 1:3\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6210335493087769
    },
    {
      "name": "Identification (biology)",
      "score": 0.6144798398017883
    },
    {
      "name": "Knowledge graph",
      "score": 0.5166106820106506
    },
    {
      "name": "Graph",
      "score": 0.4888898432254791
    },
    {
      "name": "Fault (geology)",
      "score": 0.4647461771965027
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4182513356208801
    },
    {
      "name": "Data mining",
      "score": 0.3438637852668762
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2502734363079071
    },
    {
      "name": "Geology",
      "score": 0.09083834290504456
    },
    {
      "name": "Seismology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}