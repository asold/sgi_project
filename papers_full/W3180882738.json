{
  "title": "A Note on Learning Rare Events in Molecular Dynamics using LSTM and Transformer",
  "url": "https://openalex.org/W3180882738",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2347547168",
      "name": "Zeng Wen-qi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2645439310",
      "name": "Cao Si-qin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2294516364",
      "name": "Huang XuHui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104181513",
      "name": "Yao Yuan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3017231357",
    "https://openalex.org/W2974515621",
    "https://openalex.org/W2086264514",
    "https://openalex.org/W2043782614",
    "https://openalex.org/W2025899185",
    "https://openalex.org/W2046285452",
    "https://openalex.org/W1993622915",
    "https://openalex.org/W2964917500",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2164857730",
    "https://openalex.org/W2088647884",
    "https://openalex.org/W1980055293",
    "https://openalex.org/W2133620840",
    "https://openalex.org/W2070359441",
    "https://openalex.org/W3113077770",
    "https://openalex.org/W2785425405",
    "https://openalex.org/W2043012057",
    "https://openalex.org/W585413800",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2091707670"
  ],
  "abstract": "Recurrent neural networks for language models like long short-term memory (LSTM) have been utilized as a tool for modeling and predicting long term dynamics of complex stochastic molecular systems. Recently successful examples on learning slow dynamics by LSTM are given with simulation data of low dimensional reaction coordinate. However, in this report we show that the following three key factors significantly affect the performance of language model learning, namely dimensionality of reaction coordinates, temporal resolution and state partition. When applying recurrent neural networks to molecular dynamics simulation trajectories of high dimensionality, we find that rare events corresponding to the slow dynamics might be obscured by other faster dynamics of the system, and cannot be efficiently learned. Under such conditions, we find that coarse graining the conformational space into metastable states and removing recrossing events when estimating transition probabilities between states could greatly help improve the accuracy of slow dynamics learning in molecular dynamics. Moreover, we also explore other models like Transformer, which do not show superior performance than LSTM in overcoming these issues. Therefore, to learn rare events of slow molecular dynamics by LSTM and Transformer, it is critical to choose proper temporal resolution (i.e., saving intervals of MD simulation trajectories) and state partition in high resolution data, since deep neural network models might not automatically disentangle slow dynamics from fast dynamics when both are present in data influencing each other.",
  "full_text": "A Note on Learning Rare Events in Molecular Dynamics using\nLSTM and Transformer\nWenqi Zeng,1 Siqin Cao,2 Xuhui Huang,2, 3 and Yuan Yao1, 3\n1)Department of Mathematics, Hong Kong University of Science and Technology\n2)Department of Chemistry, Hong Kong University of Science and Technology\n3)Department of Chemical and Biological Engineering, Hong Kong University of Science and Technology\n(*Authors to whom correspondence should be addressed: xuhuihuang@ust.hk or yuany@ust.hk)\n(Dated: 15 July 2021)\nRecurrent neural networks for language models like long short-term memory (LSTM) have been utilized as a tool for\nmodeling and predicting long term dynamics of complex stochastic molecular systems. Recently successful examples\non learning slow dynamics by LSTM are given with simulation data of low dimensional reaction coordinate. How-\never, in this report we show that the following three key factors signiﬁcantly affect the performance of language model\nlearning, namely dimensionality of reaction coordinates, temporal resolution and state partition. When applying re-\ncurrent neural networks to molecular dynamics simulation trajectories of high dimensionality, we ﬁnd that rare events\ncorresponding to the slow dynamics might be obscured by other faster dynamics of the system, and cannot be efﬁ-\nciently learned. Under such conditions, we ﬁnd that coarse graining the conformational space into metastable states\nand removing recrossing events when estimating transition probabilities between states could greatly help improve the\naccuracy of slow dynamics learning in molecular dynamics. Moreover, we also explore other models like Transformer,\nwhich do not show superior performance than LSTM in overcoming these issues. Therefore, to learn rare events of\nslow molecular dynamics by LSTM and Transformer, it is critical to choose proper temporal resolution (i.e., saving\nintervals of MD simulation trajectories) and state partition in high resolution data, since deep neural network models\nmight not automatically disentangle slow dynamics from fast dynamics when both are present in data inﬂuencing each\nother. a\nI. INTRODUCTION\nMolecular dynamics (MD) has been widely used as a tool\nto study conformation dynamics in recent years. In a molec-\nular dynamics (MD) simulation, molecules are modelled with\ninteracting beads and chemical bonds, where the interactions\nbetween beads are modelled with force ﬁeld. The result of\nMD simulations are normally stored as snapshots of atomic\ncoordinates of the simulated molecular system on a discrete\ntime sequence, which are called trajectories. The original dy-\nnamics of MD systems evolved in a high dimensional space\nthat contains a huge degree of freedom. However, for most\ncases only the slow dynamics are of interest as they often un-\nderline the function of biological macromolecules. This pro-\nvides possibility to model the MD dynamics with low dimen-\nsions, e.g., reaction coordinates or a state model like Markov\nState Model (MSM) 1–26. In these models, the dynamics of\ntrajectories are stored in very low dimensions (e.g. in a state\nmodel the dynamics are stored as a sequence of state index),\nand thus provides possibility to model the dynamics with other\nnon-simulation methods due to their sequential characteris-\ntics. These models are required to make correct predictions\nof the future state of the molecular systems based on infor-\nmation of the past with appropriate length, e.g. a state model\nof molecular dynamics system will produce a series of num-\nbers with sequence over time. On the other hand, natural hu-\nman language is composed of sequential states that conform\na)The code is available at: https://github.com/Wendysigh/\nLSTM-Transformer-for-MD\nto a certain logic or rule, which may also be similar to pre-\ndict molecular dynamics. In recent years, deep learning re-\ncurrent neural network methods such as gated recurrent unit\n(GRU)27, long short-term memory (LSTM) 28 and their vari-\nants have shown great potential in processing sequentiality29,\nand there are now studies on using them to analyse trajectories\nfrom simulation systems3031 .\nBack to the application of recurrent neural network to\nmolecular dynamics, related researches are still limited. A\nconservative approach is to incorporate LSTM into the nu-\nmerical integrator that solves Newton’s equations in molec-\nular dynamics simulations 32. Another applies the recurrent\nneural network directly onto the low dimensional trajectories\nand predicts the next token in the sequential data 33. They\nproved the training under cross-entropy loss is equivalent to\nlearning a path entropy and captured both Boltzmann statis-\ntics and kinetics. In this work, the authors project their MD\nsimulation trajectories onto a one-dimension reaction coordi-\nnate and further discretized the MD conformations by equal\ndistance binning. Pre-processing of MD simulation trajecto-\nries to low dimension has been shown to render the LSTM\nmodel effective to learn the rare events. However, the appli-\ncability of the LSTM and other language models directly on\nhigh-dimensional data haven’t been extensively examined.\nIn this work, we examined the performance of the LSTM\nand Transformer model in two scenarios on the alanine dipep-\ntide system: (a). Pre-processing of the MD simulation tra-\njectories by projecting them onto 2 torsional angles: φ and\nψ. (b). Directly decompose MD simulation trajectories into\nstates using the root mean square displacement (RMSD) dis-\ntances without any pre-processing of the high-dimensional\nMD data. The performance of LSTM on slow dynamics pre-\narXiv:2107.06573v1  [cs.AI]  14 Jul 2021\n2\ndiction can severely drop if the saving interval of the MD sim-\nulation trajectories is small with both alanineφψ and high reso-\nlution data alanineRMSD. To address such an issue, we investi-\ngate some methods to capture the rare events in long sequence\ndata. Effective approaches include kinetic lumping34 in phys-\nical chemistry to do different partition on states and remove\nrecrossing35 to reduce fast dynamics noise in the input data\nand enhance slow dynamics signal. On the other hand, Trans-\nformer models are not effective in learning slow dynamics as\nrare events in long sequences, due to that both fast and slow\ndynamics in training data entangled deep language models in\nefﬁciently capturing the latter.\nII. METHODS\nThe basic principle of the task is that the trajectory of\nmolecular dynamics can be discretized into sequential data,\nwhere the prediction of the current state is related to the\nknown states of the past. This is similar to natural language\nprocessing like LSTM or Transformer. To further introduce\nthese deep learning models, we roughly divide the whole\nworkﬂow into three parts. Firstly, an embedding layer is used\nto represent raw input trajectory as vectors X by multiplying\nlearnable weights, and pass X through next part to generate\nhigh dimensional representation h. Finally, a fully connected\nlayer will map the learned representation h to the predicted\nprobability for the current state. Among the three sequential\npart, the main difference between LSTM and Transformer lies\nin the second part, learning representation h.\nAs in Figure 1 (a), denote Xt as the tth state input for the\nLSTM layer. Each Xt generates ht from LSTM layer. The\nLSTM itself consists of the following elements: the forget\ngate ft , the input gate it , the output gate ot , the cell state ct ,\nand ht which is the hidden state vector and output from the\nLSTM layer. Each gate processes information in different as-\npects. Brieﬂy, the forget gate decides which information to be\nerased, the input gate decides which information to be writ-\nten, and the output gate decides which information to be read\nfrom the cell state to the hidden state. The relations among\nthese elements can be written as follows:\nft = σ(Wf Xt + Uf ht−1 + bf )\nit = σ(WiXt + Uiht−1 + bi)\not = σ(WoXt + Uoht−1 + bo)\n˜ct = tanh(WcXt + Ucht−1 + bc)\nct = ft ◦ct−1 + it ◦˜ct\nht = ot ◦tanhct\nwhere W and b are the corresponding weight matrices and\nbias. The operator ◦stands for the Hadamard product.\nIn LSTM, the calculation of representation of tth state ht\ndepends on ht−1 and ct−1 while Transformer use attention\nmechanism to avoid such dependency. Transformer is consist\nof encoders and decoders. In an encoder of Transformer of\nFigure 1 (b), X will go through a module called “Multi-Head\nAttention” to get a weighted feature vector Z:\nQ = WQ ×X\nK = WK ×X\nV = WV ×X\nZ = softmax( QKT\n√dk\n)V\nwhere W are the corresponding weight matrices and dk is the\ndimension of K vectors.\nAfter obtaining Z, it will be sent to a Feed Forward neural\nnetwork with ﬁrstly an activation function as ReLU and then a\nlinear function with wight matrices W1, W2 and bias b1, b2.\nFFN(Z) =max(0,ZW1 + b1)W2 + b2\nThe K and V vectors from encoder will be combined with\nthe Q vectors in decoder to outputZ and a ﬁnal representation\nh = FFN(Z) to be mapped into categorical probability by the\nlinear layer.\nLSTM can be very effective for data with sequential char-\nacteristics with the ability to mine timing and semantic infor-\nmation in domains including but not limited to speech recog-\nnition, machine translation, and timing analysis. There is no\ndoubt these variants can learn long-term information 36, but\nthey also suffer from the memory loss despite of the existence\nof gating mechanism 37. Essentially, vanilla LSTM only ex-\npects to inﬂuence future decisions with past states, which is\nknown as “unidirectional”. In the training process, we could\nadopt a “bidirectional” trick that both the past and future states\nare considered, which can provide more sufﬁcient contents to\nencode. The third issue comes from unparallelizable recur-\nrence in these models, which limits the acceleration of paral-\nlel computing by GPU. Transformer38 solves the above three\nlimitations through attention mechanism, that is, by calculat-\ning the similarity scores between each word and other words,\nthe distance between any two words is 1, which is neither re-\nstricted by long-distance dependence nor subject to unidirec-\ntional operations like LSTM. Based on the attention mech-\nanism, Transformer is more suitable for GPU acceleration\nwithout using sequence aligned LSTM.\nIII. RESULTS\nA. Datasets, Settings and Evaluation Metrics\nHere we choose conformational dynamics of alanine dipep-\ntide in water to test LSTM and Transformer. The full data set\n3\n(a) LSTM\n(b) Transformer with n encoders and n decoders\nFIG. 1. Procedure for LSTM and Transformer\nof alanine dipeptide includes 100 trajectories of an alanine-\ndipeptide and 888 water molecules, the length of each tra-\njectories is 10ns and the snapshots are saved every 0.1ps. In\nthe original data set, alanine dipeptide contains 22 atoms and\n66 cartisian coordinates. As most degrees of freedom are re-\nlated with fast dynamics (like vibration of chemical bonds),\nwe derived the original trajectories into two low-dimensional\ndata sets: the alanine φψ data set and alanine RMSD data set.\nIn the alanineφψ data set, we used the torsional angles φ and\nψ to capture the conformation changes of backbone. In the\nalanineRMSD data set, the conformation space is split into 100\nstates using k-center clustering method39 that ﬁnds an approx-\nimate ε-cover of samples40–42 according to their RMSD dis-\ntance of heavy (i.e. non-hydrogen) atoms. In the ﬁrst kind of\nmodels, the φ-ψ based model, each frame of the input data\nset only includes the φ or ψ angle; while in the RMSD based\nmodel, each frame of the input data set only includes a state\nindex ranging from 0 to 99. Here our φ-ψ based model is\nconsistent with the model in a previous paper 33, where we\nalso adopted the 20-bin states corresponding to the values of\nφ or ψ.\nFor LSTM, we set the embedding dimension to 128 and\nLSTM units to 1024. The trajectories were batched into\nsequences with length of 100 and batch size of 64. For\nTransformer, we change the embedding size to 512 and out-\nput dimension to 2048 with 8 heads, 2 stacks, 0.1 drop-out\nratio. Speciﬁcally for Transformer, we apply Adam opti-\nmizer with β1 = 0.9, β2 = 0.98 and ε = 10−9. We var-\nied the learning rate over the course of training according\nto the formula: lrate = d−0.5\nmodel ·min(step-num−0.5, step-num ·\nwarmup-steps−1.5). This corresponds to increase the learning\nrate linearly for the ﬁrst warmup-up training, and decrease it\nthereafter proportionally to the inverse square root of the step\nnumber. Here we use warmup-steps = 4000.\nIn generating process we use the trained model to recur-\nsively generate future predictions by appending the predicted\nvalues to the original sequence and shift the old values. We\ngenerate 100 trajectories with sequence length of 10,000 and\ndo 50 times bootstrap to evaluate.\nAs for evaluation, we consider free energy\n(−log(population)) and other evaluation on kinetics like\nImplied Time Scales (ITS) which is calculated from the\nMarkov model eigenvalues, and Mean First-Passage Time\n(MFPT). In MSM, the ITS is used to estimate the Marko-\nvian lag time, which is also representative to the order of\nmagnitude of dynamics:\nITSi = − τ\nlnλ(τ)i\n(1)\nwhere τ is the lag time to compute auto-correlation function\nof states C(τ) = x(t)x(t + τ), x(t) is the state index at time\nt, and λi(τ) is the i-th eigenvalue of the transition probability\nmatrix T (τ). The transition probability matrix is a normalized\nauto-correlation function: Ti j(τ) =Ci j(τ)/∑i Ci j(τ).\nThe MFPT denotes the transition time between each pair of\nstates. For a Markov chain, the MFPT can be calculated by43:\nti j = τ + ∑\nk\nTiktk j (2)\nwhere the transition probability matrix T needs to be Marko-\nvian.\nB. Experiments on LSTM and Transformer\nIn this section we used the conformational dynamics of\nalanine dipeptide in water to test deep learning models (i.e.\nLSTM and Transformer). Both these two data sets contain the\ndegree of freedom of slow dynamics, and deep learning mod-\nels are used to reconstruct the dynamics in the reduced dimen-\nsionality. In the following part of this section, ﬁrstly LSTM\nis tested on both data sets. Then the Transformer algorithm is\npresented as a possible alternative to the LSTM model.\n1. Results on alanineφψ\nIn this part we show that with 1ps or 2ps saving intervals,\none can learn LSTM well for the 20-state on φ −ψ plane in\nterms of both thermodynamic equilibrium distribution and the\nlong term dynamic behavior. However, when the saving in-\nterval varies to 0.1ps, LSTM fails to capture the long term\nbehaviour.\n4\nThe LSTM can not only correctly predict the thermody-\nnamic properties in Figure 2 (a)(b), but also be able to cor-\nrectly capture the slow dynamics of the alanine φψ data sets.\nAs can be seen from the results in Figure 2 (d)(e), the dy-\nnamic information can be captured by LSTM and then mim-\nicked in predicted trajectories at saving interval 1ps or 2ps,\non both φ and ψ angle. Therefore, it is feasible to learn and\nmodel slow dynamics for alanine dipeptide by deep learning\nlanguage models like LSTM. The only requirement for using\nLSTM is sufﬁcient amount of data, which is already available\nand circumvents highly resource consumption in prediction\nby molecular dynamic simulation system. In a similar way,\nLSTM may not work ﬁne on the data sets with 10ps saving\ninterval, as each of our trajectories has 10ns data and the 10ps\nsaving interval data sets may suffer from insufﬁciency of data.\nHowever, if we use shorter saving interval like 0.1 ps and\nmore frames would be needed to capture the slow dynamics,\nLSTM would not produce accurate results comparing to the\noriginal MD simulations (see Figure 2 (g) ). This indicates\nthat the saving interval will indeed have an adverse impact,\nespecially in moving from metastable state C7eq to C7ax with\nrespect to angle deﬁned in Figure 2 (c)φ.\nFIG. 2. (a) and (b) are free energy for φ and ψ on 0.1ps saving\ninterval respectively. (c) 2D projection ontoφ-ψ angles (d) MD sim-\nulation data with 1ps saving interval (e) MD simulation data with\n2ps saving interval (f) MD simulation data with 10ps saving inter-\nval (g) MD simulation data with 0.1ps saving interval. (d)(e) differs\nobviously from (f)(g)\n2. Results on alanineRMSD\nReaction coordinates, which distinguish alanine RMSD and\nalanineφψ , and saving interval serve as two inﬂuencing fac-\ntors in this part. When the data set comes to alanine RMSD ,\nwith 1ps saving interval, the performance of LSTM is accept-\nable although it is not as good as on alanine φψ . Still, LSTM\nperforms much worse on 0.1ps saving interval than alanineφψ ,\nfailing to capture the slowest motion.\nOn alanineRMSD dataset, the slowest motion, that is, the\nﬁrst ITS represented by the black line, is the dominant one\nand can be captured with saving interval as 1ps. In Figure 3\n(a), the ﬁrst ITS of predicted 1ps saving interval dataset is\naround 700ps while that of MD trajectories is around 1000ps.\nLSTM is able to be applied on a more complicated data set as\nalanineRMSD considering alanineRMSD contains all conforma-\ntional dynamics of alanine dynamics, which is more challeng-\ning than the one-dimensional data sets alanine φψ . However,\nthe negative impact of small saving interval like 0.1ps is still\ninevitable. The gap between LSTM prediction and benchmark\ngets enlarged on 0.1ps saving interval dataset as shown in Fig-\nure 3 (b).\nFIG. 3. ITS on alanineRMSD for LSTM with sequence length=100.\n(a) result on 0.1ps saving interval. (b) result on 1ps saving interval.\n1ps data outperforms 0.1ps data.\n3. Improvement on 0.1ps saving interval\nBased on results of previous two datasets, LSTM works at\nsaving interval 1ps but fails when the saving interval is 0.1ps\nwith high temporal resolution, and its performance also gets\naffected by the pre-processing performed on MD simulation\ntrajectories. We proposed several effective solutions based on\nthe idea to suppress the fast dynamic modes as noises in favor\nof capturing slow motions as rare events.\nAs we have seen, LSTM performs worse when predict-\ning slow dynamics as rare events on data sets with 0.1ps\nsaving interval than the low resolution of 1ps. Therefore,\nrare events may not be well handled in the high resolution\ndataset for LSTM. To explore possible reasons, we further\nstudy the difference between 0.1ps and 1ps saving interval\non the alanine RMSD data set. There are quick transitions be-\ntween states in 0.1ps data whose frequency of occurrence is\nlow, which makes LSTM hard to tell the signals from noise.\nAs a result, LSTM tends to learn some high frequency event\nor noise, and ignore the low frequency event.\nConsidering that the rare events as slow dynamics are very\nlikely to be masked in the 0.1ps data, we choose the following\nways to tackle.\nUse kinetic lumping methods to ﬁnd metastable states .\nIn k-center clustering, which is sensitive to noise, states are\nnot metastable and there exists fast intra-metastable distur-\nbance which adds noise to the trajectories to confuse the lan-\nguage models. Lumping such states toward metastable macro-\nstates thus reduce fast dynamics and enlarge the signal, the\nslow dynamics. For example, by turning the 100 state model\ninto a 4 metastate model, the fast intra-metastable dynam-\nics will be removed from the data set, and most rare events\n5\ncan be included in new 4-states transitions. Many lumping\nmethods can be used here, e.g. the Hierarchical Nyström\nmethod42, Perron-Cluster Cluster Analysis (PCCA) method44,\nPCCA+45, Maximum PowerPoint (MPP)46. Here we adopted\nPCCA+ to do lumping.\nRemove recrossing. The recrossing refers to some high\nfrequently jumps back and forth, which can be seen as noise\nand removed from the data set before processed with LSTM.\nThe results in Table I (a) shows lumping method and re-\ncrossing removal has positive effect on improving prediction\nby LSTM while changes on input data expression does not\nsee improvement. Therefore, denoising by reducing the fast\ndynamics helps language models to learn the slow dynamics\nappeared as rare events.\nFinally we note that another way of using a different expres-\nsion of input data does not work well in our experiments. In\nthis different expression, the original input data is condensed\ninto data points in the form of “states-consecutive length”,\nwhere the former “states” stands for the interstate transition\nand latter “consecutive length” for intrastate transition. For\nexample, original data like \"4,4,4,4,4,3,3,3,2,2,2,2,...\" will be\ntransformed into \"4-5, 3-3,2-4,...\" in a short format of “state-\nlength\". But this does not help to reduce the noise shown in\nTable I (a).\n4. Ablation Studies\nIn this part we present two ablation studies on sequence\nlength and Transformer, both of which, however, are not ef-\nfective in improving the 0.1ps case.\nVariations on sequence length. Despite that we have more\ndetailed information on the 0.1ps data, it performs worse than\nother larger saving interval. There are two speculations about\nthis counter intuitive phenomenon: one is each long sequence\nin batched training data has introduced large amount of fast\ndynamics, it may be helpful to shorten the sequence length to\n15, 20, 50 to reduce the fast dynamics in processing each se-\nquence. The other is on the contrary. With the same sequence\nlength, compared with 1ps, events are even sparser in 0.1ps\ndata because slow dynamics requires more frames to capture.\nFor the second conjecture, we increase the sequence length to\n1000. Nevertheless, this approach does not see any effect be-\ncause the sequence length are indeed not restricted under the\nsetting of “stateful\" LSTM as shown next.\nIn comparison, we also bring in a parameter in LSTM\ncalled “stateful\" versus “stateless\". “Stateful\" indicates LSTM\nwill remember the content of the previous batches and “state-\nless\" will throw them away. Stateful LSTM will ﬁnd the de-\npendencies among all input sequences and may have longer\nmemory than input sequence length. From results in Table I\n(b) the sequence length is similar to a hyperparameter like\nlearning rate or batch size that has an impact on the model,\nbut it does not greatly improve the performance on 0.1ps\nalanineRMSD. Moreover, stateful LSTM outperforms stateless\nLSTM on this task and we keep stateful as basic setting in all\nLSTM experiments.\nExperiments with Transformer . This idea originates\nfrom some intrinsic drawbacks of LSTM model like mem-\nory loss in dealing with long sequence. Here long sequence\nissue refers to the fact that the dynamic can stay in one state\nfor a long time, where LSTM could suffer from memory leak\ndespite of the existence of gating mechanism, thus we ap-\nply Transformer to explore whether it outperforms LSTM on\nlong sequence matter. On alanine φψ , free energy landscape\nis barely satisfactory in Figure 4 (a)(b) but the kinetic infor-\nmation of predictions by Transformer like ITS or MFPT is\nlargely affected by saving interval and model from different\nepochs although the training loss has already converged to a\nsmall number, as shown in Figure 4 (c)(d). The performance\non Transformer ﬂuctuate so much that most of them deviate\nfrom the groundtruth. On alanine φψ , once the saving inter-\nval is increased to, for example, 30 times to 3ps, the trajecto-\nries predicted by Transformer will show completely different\nproperties from MD trajectories. Also, on 1ps saving inter-\nval alanineRMSD shown in Figure 4 (e)(f), the ITS of Trans-\nformer predictions keeps growing while that of LSTM predic-\ntions will converge close to the benchmark. On the whole, the\nperformance of Transformer does not reﬂect the superiority of\nattention mechanism over LSTM especially with large saving\ninterval.\nNext, we also tested the impact of unidirectional and bidi-\nrectional encoding on Transformer. Due to the two-way en-\ncoding used by original Transformer encoder, we applied a\ntriangular mask to cover subsequent positions to achieve the\npurpose of unidirectional encoding. However, since the orig-\ninal bidirectional Transformer performs too bad to be refer-\nenced, although the unidirectional results are quite different\nfrom bidirectional ones still it is not rigorous to conclude\nwhich is better in Table II.\nIV. CONCLUSIONS\nLearning rare events as slow dynamics prediction by neu-\nral language models are attractive due to the success of deep\nTABLE I. (a) 1st ITS on 0.1ps alanineRMSD for solution based on\nrare events. (b)1st ITS on 0.1ps alanineRMSD for solution based on\nsequence length. Different sequence length does not help to improve.\nStateful predictions outperforms stateless predictions. Using lump-\ning method and removing recrossing see some improvement.\n6\nFIG. 4. (a) (b) are free energy on phi and psi by transformer respec-\ntively. (c) MFPT for predicted trajectories by Transformer different\nepochs. (d) corresponding training loss for epochs used in (c). (e)\nITS for LSTM (f) ITS for Transformer on alanineRMSD dataset with\nsaving interval=1ps. In total free energy landscape is barely satisfac-\ntory. ITS of LSTM predictions will converge while Transformer’s\nnot.\nTABLE II. (a)(b): MFPT on φ and ψ respectively (c)(d): ITS on φ\nand ψ respectively for unidirectional and bidirectional Transformer\npredictions on alanineφψ with saving interval=0.1ps. Different but\nboth poor performance.\nlearning in recent years. Yet, in this report we show that\nthere are three key factors for the performance of deep learn-\ning methods like LSTM and Transformer on learning the rare\nevents in MD simulation trajectories: namely reaction coordi-\nnate, temporal resolution like saving interval and state parti-\ntion.\nReaction coordinate differentiate our two datasets,\nalanineφψ and alanineRMSD. With one-dimensional sequential\ndata on angle φ or ψ, alanine φψ is an easier dataset than\nalanineRMSD which contains more complicated transitions.\nSpeciﬁcally, LSTM is able to correctly predict the free\nenergy landscape and dynamics for both the alanine φψ and\nalanineRMSD data sets when the saving intervals are 1ps or\n2ps. However, LSTM works poorly when the save intervals\nare 0.1ps or 10ps. For a large saving interval like 10ps, the\ndeep learning model may suffer from insufﬁciency of data.\nWhile for a small saving interval like 0.1 ps, the LSTM may\nsuffer from rare events mixing up with fast dynamics and thus\ncannot capture slow dynamics efﬁciently.\nAlthough the free energy landscape predicted with 0.1ps\nsaving interval still overlaps with MD, the dynamics (both ITS\nand MFPT) of LSTM is much faster than the MD simulations.\nWith the intention of calculating long-term dynamic proper-\nties from ﬁner transitions like 0.1ps, we applied methods from\nthe perspective of alleviating the failure to predict the slowest\ndynamics of the system and took effect. Increasing saving\nintervals and removing recrossing would largely reduce the\nshort-term dynamics that can be regarded as noise, while ap-\nplying state partition to alanine RMSD could also help in view\nof turning original k-center clustering states into metastable\nones and reducing fast intrastate transitions. But increasing\nthe complexity of data sets (e.g. change data form) would\nlead to even worse performance of LSTM. Then considering\nthe limitation of LSTM on dealing with long sequence data,\nwe brought about Transformer. The experimental results show\nTransformer cannot surpass LSTM on learning slow dynam-\nics from MD simulation tajectories. The advantages of the at-\ntention mechanism, such as the processing of long sequences\nwithout memory loss and bidirectional encoding, have not\ncome into force. The possible reason is that stateful LSTM\nsees information beyond sequence length while Transformer\nis limited to given sequence length of 100.\nRegarding the effect of learning rare events on slow dynam-\nics prediction using LSTM and Transformer, choosing proper\ntemporal resolution like saving intervals and state partition are\ncritical for deep learning models. In a summary, deep lan-\nguage models such as LSTM and Transformer, are not able\nto disentangle the slow dynamics from fast dynamics when\nboth are present in the training data, the former playing a role\nas signal while the latter as noise in learning rare events as\nslow dynamics. Therefore, a denoising mechanism is helpful\nto improve the accuracy of rare event learning by language\nmodels with complex molecular dynamics data. Under such\nassistance, LSTM and Transformer could learn slow dynam-\nics more effectively with high dimensional MD simulation\ndata. Moreover, it is desired to develop advanced deep learn-\ning models toward multiscale analysis of molecular dynamics\ndata, that can automatically learn and separate time scales in\ntrajectories. Such a great potential of deep learning models in\nmolecular dynamics appliations are our future directions.\nREFERENCE\n1John D Chodera and Frank Noé. Markov state models of biomolecular\nconformational dynamics. Current opinion in structural biology , 25:135–\n144, 2014. I\n2Brooke E Husic and Vijay S Pande. Markov state models: From an art to\na science. Journal of the American Chemical Society , 140(7):2386–2396,\n2018.\n3Jan-Hendrik Prinz, Hao Wu, Marco Sarich, Bettina Keller, Martin Senne,\nMartin Held, John D Chodera, Christof Schütte, and Frank Noé. Markov\nmodels of molecular kinetics: Generation and validation. The Journal of\nchemical physics, 134(17):174105, 2011.\n4Robert D Malmstrom, Christopher T Lee, Adam T Van Wart, and Rom-\nmie E Amaro. Application of molecular-dynamics based markov state mod-\nels to functional proteins. Journal of chemical theory and computation, 10\n(7):2648–2657, 2014.\n5Gregory R Bowman, Vijay S Pande, and Frank Noé. An introduction to\nMarkov state models and their application to long timescale molecular sim-\nulation, volume 797. Springer Science & Business Media, 2013.\n6John D Chodera, Nina Singhal, Vijay S Pande, Ken A Dill, and William C\nSwope. Automatic discovery of metastable states for the construction of\nmarkov models of macromolecular conformational dynamics. The Journal\nof chemical physics, 126(15):04B616, 2007.\n7Albert C Pan and Benoît Roux. Building markov state models along path-\nways to determine free energies and rates of transitions. The Journal of\nchemical physics, 129(6):064107, 2008.\n7\n8Marco Sarich, Frank Noé, and Christof Schütte. On the approximation\nquality of markov state models. Multiscale Modeling & Simulation , 8(4):\n1154–1177, 2010.\n9Frank Noé and Feliks Nuske. A variational approach to modeling slow pro-\ncesses in stochastic dynamical systems.Multiscale Modeling & Simulation,\n11(2):635–655, 2013.\n10Hao Wu and Frank Noé. Variational approach for learning markov pro-\ncesses from time series data. Journal of Nonlinear Science , 30(1):23–66,\n2020.\n11Jingwei Weng, Maohua Yang, Wenning Wang, Xin Xu, and Zhongqun Tian.\nRevealing thermodynamics and kinetics of lipid self-assembly by markov\nstate model analysis. Journal of the American Chemical Society , 142(51):\n21344–21352, 2020.\n12Xiangze Zeng, Lizhe Zhu, Xiaoyan Zheng, Marco Cecchini, and Xuhui\nHuang. Harnessing complexity in molecular self-assembly using computer\nsimulations. Physical Chemistry Chemical Physics , 20(10):6767–6776,\n2018.\n13Bin W Zhang, Wei Dai, Emilio Gallicchio, Peng He, Junchao Xia, Zhiqiang\nTan, and Ronald M Levy. Simulating replica exchange: Markov state mod-\nels, proposal schemes, and the inﬁnite swapping limit.The Journal of Phys-\nical Chemistry B, 120(33):8289–8301, 2016.\n14Gregory R Bowman, Xuhui Huang, and Vijay S Pande. Network models\nfor molecular kinetics and their initial applications to human health. Cell\nresearch, 20(6):622–630, 2010.\n15Yuan Yao, Raymond Z Cui, Gregory R Bowman, Daniel-Adriano Silva,\nJian Sun, and Xuhui Huang. Hierarchical nyström methods for constructing\nmarkov state models for conformational dynamics.The Journal of chemical\nphysics, 138(17):05B602_1, 2013.\n16Lin-Tai Da, Fátima Pardo Avila, Dong Wang, and Xuhui Huang. A two-\nstate model for the dynamics of the pyrophosphate ion release in bacterial\nrna polymerase. PLoS computational biology, 9(4):e1003020, 2013.\n17Ilona Christy Unarta, Siqin Cao, Shintaroh Kubo, Wei Wang, Peter Pak-\nHang Cheung, Xin Gao, Shoji Takada, and Xuhui Huang. Role of bacterial\nrna polymerase gate opening dynamics in dna loading and antibiotics inhi-\nbition elucidated by quasi-markov state model.Proceedings of the National\nAcademy of Sciences, 118(17), 2021.\n18Faruck Morcos, Santanu Chatterjee, Christopher L McClendon, Paul R\nBrenner, Roberto López-Rendón, John Zintsmaster, Maria Ercsey-Ravasz,\nChristopher R Sweet, Matthew P Jacobson, Jeffrey W Peng, et al. Mod-\neling conformational ensembles of slow functional motions in pin1-ww.\nPLoS computational biology, 6(12):e1001015, 2010.\n19Xuhui Huang, Gregory R Bowman, Sergio Bacallado, and Vijay S Pande.\nRapid equilibrium sampling initiated from nonequilibrium data. Proceed-\nings of the National Academy of Sciences, 106(47):19765–19769, 2009.\n20Nicolae-Viorel Buchete and Gerhard Hummer. Coarse master equations for\npeptide folding dynamics. The Journal of Physical Chemistry B , 112(19):\n6057–6069, 2008.\n21Frank Noé, Christof Schütte, Eric Vanden-Eijnden, Lothar Reich, and\nThomas R Weikl. Constructing the equilibrium ensemble of folding path-\nways from short off-equilibrium simulations. Proceedings of the National\nAcademy of Sciences, 106(45):19011–19016, 2009.\n22Gregory R Bowman, Vincent A V oelz, and Vijay S Pande. Taming the\ncomplexity of protein folding. Current opinion in structural biology, 21(1):\n4–11, 2011.\n23Ignasi Buch, Toni Giorgino, and Gianni De Fabritiis. Complete recon-\nstruction of an enzyme-inhibitor binding process by molecular dynamics\nsimulations. Proceedings of the National Academy of Sciences , 108(25):\n10184–10189, 2011.\n24Daniel-Adriano Silva, Gregory R Bowman, Alejandro Sosa-Peinado, and\nXuhui Huang. A role for both conformational selection and induced ﬁt\nin ligand binding by the lao protein. PLoS computational biology , 7(5):\ne1002054, 2011.\n25Frank Noé, Illia Horenko, Christof Schütte, and Jeremy C Smith. Hier-\narchical analysis of conformational dynamics in biomolecules: Transition\nnetworks of metastable states. The Journal of chemical physics , 126(15):\n04B617, 2007.\n26Gregory R Bowman, Daniel L Ensign, and Vijay S Pande. Enhanced model-\ning via network theory: Adaptive sampling of markov state models.Journal\nof chemical theory and computation, 6(3):787–794, 2010. I\n27Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bah-\ndanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning\nphrase representations using rnn encoder-decoder for statistical machine\ntranslation. arXiv preprint arXiv:1406.1078, 2014. I\n28Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neu-\nral computation, 9(8):1735–1780, 1997. I\n29Mantas Lukoševiˇcius and Herbert Jaeger. Reservoir computing approaches\nto recurrent neural network training. Computer Science Review, 3(3):127–\n149, 2009. I\n30Mohammad Javad Eslamibidgoli, Mehrdad Mokhtari, and Michael H Eik-\nerling. Recurrent neural network-based model for accelerated trajectory\nanalysis in aimd simulations. arXiv preprint arXiv:1909.10124, 2019. I\n31Jaideep Pathak, Brian Hunt, Michelle Girvan, Zhixin Lu, and Edward Ott.\nModel-free prediction of large spatiotemporally chaotic systems from data:\nA reservoir computing approach. Physical review letters, 120(2):024102,\n2018. I\n32JCS Kadupitiya, Geoffrey C Fox, and Vikram Jadhao. Deep learning based\nintegrators for solving newton’s equations with large timesteps. arXiv\npreprint arXiv:2004.06493, 2020. I\n33Sun-Ting Tsai, En-Jui Kuo, and Pratyush Tiwary. Learning molecular dy-\nnamics with simple language model built upon long short-term memory\nneural network. Nature communications, 11(1):1–11, 2020. I, III A\n34Genyuan Li and Herschel Rabitz. A general analysis of exact lumping in\nchemical kinetics. Chemical engineering science, 44(6):1413–1430, 1989.\nI\n35Zhihong Wang, Jennifer S Hirschi, and Daniel A Singleton. Recrossing and\ndynamic matching effects on selectivity in a diels–alder reaction. Ange-\nwandte Chemie, 121(48):9320–9323, 2009. I\n36Andrej Karpathy. The unreasonable effectiveness of recurrent neural net-\nworks. Andrej Karpathy blog, 21:23, 2015. II\n37Jingyu Zhao, Feiqing Huang, Jia Lv, Yanjie Duan, Zhen Qin, Guodong Li,\nand Guangjian Tian. Do rnn and lstm have long memory? In International\nConference on Machine Learning, pages 11365–11375. PMLR, 2020. II\n38Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. arXiv preprint arXiv:1706.03762, 2017. II\n39Yutong Zhao, Fu Kit Sheong, Jian Sun, Pedro Sander, and Xuhui Huang.\nA fast parallel clustering algorithm for molecular simulation trajectories.\nJournal of computational chemistry, 34(2):95–104, 2013. III A\n40Jian Sun, Yuan Yao, Xuhui Huang, Vijay Pande, Gunnar Carlsson, and\nLeonidas J. Guibas. A well-controlled fast geometric clustering method\non conformation space of biomolecules. In Biomedical Computation at\nStanford (BCATS), 2008. III A\n41Yuan Yao, Jian Sun, Xuhui Huang, Gregory Bowman, Gurjeet Singh,\nMichael Lesnick, Vijay Pande, Leonidas J. Guibas, and Gunnar Carlsson.\nTopological methods for exploring low-density states in biomolecular fold-\ning pathways. J. Chem. Phys., 130(14):144115, 2009.\n42Yuan Yao, Raymond Z. Cui, Gregory R. Bowman, Daniel A. Silva, Jian\nSun, and Xuhui Huang. Hierarchical nyström methods for constructing\nmarkov state models for conformational dynamics. J. Chem. Phys. , 138:\n174106, 2013. III A, III B 3\n43Theodore J Sheskin. Computing mean ﬁrst passage times for a markov\nchain. International Journal of Mathematical Education in Science and\nTechnology, 26(5):729–735, 1995. III A\n44Peter Deuﬂhard and Marcus Weber. Robust perron cluster analysis in con-\nformation dynamics. Linear algebra and its applications , 398:161–184,\n2005. III B 3\n45Susanna Röblitz and Marcus Weber. Fuzzy spectral clustering by pcca+:\napplication to markov state models and data classiﬁcation. Advances in\nData Analysis and Classiﬁcation, 7(2):147–179, 2013. III B 3\n46Trishan Esram and Patrick L Chapman. Comparison of photovoltaic array\nmaximum power point tracking techniques. IEEE Transactions on energy\nconversion, 22(2):439–449, 2007. III B 3\n8\nV. SUPPLEMENTARY INFORMATION\nThis part contains supplements to more evaluation metrics\non experiments in previous text.\nTable III shows the ﬁrst ITS of alanineφψ on different\nsaving interval, serving as a supplement to the MFPT results\nusing LSTM. Similar to MFPT, with larger saving intervals,\nLSTM performs better.\nTable IV shows the effect of sequence length on LSTM\nby former three ITS. In main text we only showed results on\nthe ﬁrst ITS. On all three ITS, sequence length does not have\ndominant effect.\nFigure 5 shows different epochs of Transformer model pos-\nsess robust performance on thermodynamic equilibrium dis-\ntribution. Table V and Table VI contain detailed results us-\ning different epochs of Transformer model or different sav-\ning intervals to convey that the long term dynamic behavior\nis harder to be captured than equilibrium distribution. Fig-\nure 6 depicts training and validation loss for Transformer to\nput away worries such as non convergence.\nTABLE III. ITS on alanineφψ for LSTM with different saving in-\nterval. (a) saving interval=0.1ps (b) saving interval=1ps (c) saving\ninterval=2ps (d) saving interval=10ps. Larger saving interval, better\nperformance.\nFIG. 5. Free energy landscapes for different epochs of Transformer\non alanineφψ , 1ps. Basically satisfying.\n9\nTABLE IV . (a)(b)(c) are 1st, 2nd, 3rd ITS on 0.1ps alanineRMSD\nrespectively. Different sequence length does not help to improve.\nStateful predictions outperforms stateless predictions.\nTABLE V . MFPT for Transformer on alanineφψ dataset. std gets\ncalculated on the basis of 50 times bootstrap. (a)(b) are from 1ps\nsaving interval with different epochs while (c)(d) are from epoch 90\nwith different saving intervals. Groundtruth is set from Molecular\nDynamic simulation with saving interval=1ps. Different saving in-\nterval or epochs would lead to different MFPT results although their\nconverged loss would be very much close as shown in Figure 6.\n10\nTABLE VI. ITS for Transformer on alanineφψ dataset. std gets cal-\nculated on the basis of 50 times bootstrap. (a)(b) are from 1ps sav-\ning interval with different epochs while (c)(d) are from epoch 90\nwith different saving interval. Groundtruth is set from Molecular\nDynamic simulation with saving interval=1ps. Different saving in-\nterval or epochs would lead to different ITS results although their\nconverged loss would be very much close as shown in Figure 6.\nFIG. 6. Loss plot for Transformer on φ and ψ respectively.\nTABLE VII. (a)(b): MFPT on φ and ψ respectively (c)(d): ITS on φ\nand ψ respectively for unidirectional and bidirectional Transformer\npredictions on alanineφψ with saving interval=0.1ps. Different but\nboth poor performance.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7125325202941895
    },
    {
      "name": "Curse of dimensionality",
      "score": 0.6989133358001709
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6146697998046875
    },
    {
      "name": "Rare events",
      "score": 0.5710365176200867
    },
    {
      "name": "Metastability",
      "score": 0.5421280860900879
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5315333008766174
    },
    {
      "name": "Molecular dynamics",
      "score": 0.4948691725730896
    },
    {
      "name": "Artificial neural network",
      "score": 0.48101526498794556
    },
    {
      "name": "Granularity",
      "score": 0.4612594544887543
    },
    {
      "name": "Deep learning",
      "score": 0.4538314938545227
    },
    {
      "name": "Transformer",
      "score": 0.45127785205841064
    },
    {
      "name": "Temporal resolution",
      "score": 0.4125634431838989
    },
    {
      "name": "Machine learning",
      "score": 0.3802070915699005
    },
    {
      "name": "Physics",
      "score": 0.14312440156936646
    },
    {
      "name": "Mathematics",
      "score": 0.12075468897819519
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 15
}