{
  "title": "The performance of large language models on quantitative and verbal ability tests: Initial evidence and implications for unproctored high‐stakes testing",
  "url": "https://openalex.org/W4397006720",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2620097988",
      "name": "Louis Hickman",
      "affiliations": [
        "Virginia Tech",
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2102974271",
      "name": "Patrick D. Dunlop",
      "affiliations": [
        "Curtin University"
      ]
    },
    {
      "id": "https://openalex.org/A5102979627",
      "name": "Jasper Leo Wolf",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2620097988",
      "name": "Louis Hickman",
      "affiliations": [
        "University of Pennsylvania",
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2102974271",
      "name": "Patrick D. Dunlop",
      "affiliations": [
        "Curtin University"
      ]
    },
    {
      "id": "https://openalex.org/A5102979627",
      "name": "Jasper Leo Wolf",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4380291159",
    "https://openalex.org/W1979462775",
    "https://openalex.org/W2101049208",
    "https://openalex.org/W2159670345",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4385236484",
    "https://openalex.org/W4387232979",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4383913712",
    "https://openalex.org/W4387929411",
    "https://openalex.org/W2005721028",
    "https://openalex.org/W1976351130",
    "https://openalex.org/W4378470708",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W6856853697",
    "https://openalex.org/W3181182990",
    "https://openalex.org/W2076115808",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4385440981",
    "https://openalex.org/W2063866393",
    "https://openalex.org/W2127013010",
    "https://openalex.org/W1500843515",
    "https://openalex.org/W4287799740",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W4388787315",
    "https://openalex.org/W7006587735",
    "https://openalex.org/W2047444967",
    "https://openalex.org/W4387398178",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W4389452675",
    "https://openalex.org/W4200365118",
    "https://openalex.org/W2136971664",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W2891202243",
    "https://openalex.org/W2073766844",
    "https://openalex.org/W2163534692",
    "https://openalex.org/W4312090934",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4387937446",
    "https://openalex.org/W4378473736",
    "https://openalex.org/W21230259",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4387560599"
  ],
  "abstract": "Abstract Unproctored assessments are widely used in pre‐employment assessment. However, widely accessible large language models (LLMs) pose challenges for unproctored personnel assessments, given that applicants may use them to artificially inflate their scores beyond their true abilities. This may be particularly concerning in cognitive ability tests, which are widely used and traditionally considered to be less fakeable by humans than personality tests. Thus, this study compares the performance of LLMs on two common types of cognitive tests: quantitative ability (number series completion) and verbal ability (use a passage of text to determine whether a statement is true). The tests investigated are used in real‐world, high‐stakes selection. We also examine the performance of the LLMs across different test formats (i.e., open‐ended vs. multiple choice). Further, we contrast the performance of two LLMs (Generative Pretrained Transformers, GPT‐3.5 and GPT‐4) across multiple prompt approaches and “temperature” settings (i.e., a parameter that determines the amount of randomness in the model's output). We found that the LLMs performed well on the verbal ability test but extremely poorly on the quantitative ability test, even when accounting for the test format. GPT‐4 outperformed GPT‐3.5 across both types of tests. Notably, although prompt approaches and temperature settings did affect LLM test performance, those effects were mostly minor relative to differences across tests and language models. We provide recommendations for securing pre‐employment testing against LLM influences. Additionally, we call for rigorous research investigating the prevalence of LLM usage in pre‐employment testing as well as on how LLM usage affects selection test validity.",
  "full_text": null,
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.8420813679695129
    },
    {
      "name": "Cognitive psychology",
      "score": 0.45327675342559814
    },
    {
      "name": "Social psychology",
      "score": 0.33863264322280884
    },
    {
      "name": "Linguistics",
      "score": 0.33460789918899536
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}