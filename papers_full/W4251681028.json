{
    "title": "Deep spatial transformers for autoregressive data-driven forecasting of geophysical turbulence",
    "url": "https://openalex.org/W4251681028",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5011234657",
            "name": "Ashesh Chattopadhyay",
            "affiliations": [
                "Lawrence Berkeley National Laboratory",
                "Rice University"
            ]
        },
        {
            "id": "https://openalex.org/A5103814242",
            "name": "Mustafa Mustafa",
            "affiliations": [
                "Lawrence Berkeley National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A5052673105",
            "name": "Pedram Hassanzadeh",
            "affiliations": [
                "Rice University"
            ]
        },
        {
            "id": "https://openalex.org/A5071655569",
            "name": "Karthik Kashinath",
            "affiliations": [
                "Lawrence Berkeley National Laboratory"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2808559252",
        "https://openalex.org/W2808400960",
        "https://openalex.org/W3099160209",
        "https://openalex.org/W2908155528",
        "https://openalex.org/W2956255334",
        "https://openalex.org/W2963973018",
        "https://openalex.org/W2953065922",
        "https://openalex.org/W2809789958",
        "https://openalex.org/W2043524092",
        "https://openalex.org/W2984776965",
        "https://openalex.org/W2796108131",
        "https://openalex.org/W3005220771",
        "https://openalex.org/W6618372016",
        "https://openalex.org/W2965580012",
        "https://openalex.org/W2970834748",
        "https://openalex.org/W2974527409",
        "https://openalex.org/W3048291105",
        "https://openalex.org/W3105945687",
        "https://openalex.org/W3026779722",
        "https://openalex.org/W4249857356",
        "https://openalex.org/W4253516748",
        "https://openalex.org/W603908379"
    ],
    "abstract": "A deep spatial transformer based encoder-decoder model has been developed to autoregressively predict the time evolution of the upper layer's stream function of a two-layered quasi-geostrophic (QG) system without any information about the lower layer's stream function. The spatio-temporal complexity of QG flow is comparable to the complexity of 500hPa Geopotential Height (Z500) of fully coupled climate models or even the Z500 which is observed in the atmosphere, based on the instantaneous attractor dimension metric. The ability to predict autoregressively, the turbulent dynamics of QG is the first step towards building data-driven surrogates for more complex climate models. We show that the equivariance preserving properties of modern spatial transformers incorporated within a convolutional encoder-decoder module can predict up to 9 days in a QG system (outperforming a baseline persistence model and a standard convolutional encoder decoder with a custom loss function). The proposed data-driven model remains stable for multiple years thus promising us of a stable and physical data-driven climate model.",
    "full_text": "This is a non-peer reviewed pre-print submitted to EarthArxiv 1 MOTIVATION\nDeep spatial transformers for autoregressive\ndata-driven forecasting of geophysical turbulence\nAshesh Chattopadhyay1,2∗, Mustafa Mustafa2, Pedram Hassanzadeh1, Karthik Kashinath2\nAbstract\nA deep spatial transformer based encoder-decoder model has been developed to autoregressively predict the\ntime evolution of the upper layer’s stream function of a two-layered quasi-geostrophic (QG) system without any\ninformation about the lower layer’s stream function. The spatio-temporal complexity of QG ﬂow is comparable\nto the complexity of 500hPa Geopotential Height (Z500) of fully coupled climate models or even the Z500 which\nis observed in the atmosphere, based on the instantaneous attractor dimension metric. The ability to predict\nautoregressively, the turbulent dynamics of QG is the ﬁrst step towards building data-driven surrogates for more\ncomplex climate models. We show that the equivariance preserving properties of modern spatial transformers\nincorporated within a convolutional encoder-decoder module can predict up to 9 days in a QG system (outper-\nforming a baseline persistence model and a standard convolutional encoder decoder with a custom loss function).\nThe proposed data-driven model remains stable for multiple years thus promising us of a stable and physical\ndata-driven climate model.\n1 Motivation\nNumerical weather prediction (NWP) models remain the state-of-the-art to predict daily weather and usually require\nan enormous amount of computational resources to resolve the atmospheric ﬁelds. These models employ highly\naccurate and expensive numerical methods that solve the discretized ﬂuid dynamics equations in the atmosphere\nalong with the equations of other physical processes such as radiation, phase change thermodynamics, chemistry and\nheat transfer. Resolving the fully turbulent ﬂow in the atmosphere demands solving the Navier-Stokes equations on\na ﬁne grid, thus making it very expensive and challenging for even the most advanced NWPs to remain accurate\nfor long periods of forecasting time. For that, NWPs often employ parameterization schemes that approximate the\nphysics of processes that are unresolved (sub-grid scale processes). Data-driven models possess an advantage over\nthese numerical models in that, if trained on samples from very high resolution simulations of the ﬂow dynamics,\nor observed ﬂow dynamics, they can more accurately represent the true tendencies of these otherwise unresolved or\nunder-resolved processes. Such work has been done in the past for atmospheric dynamics and, more generally, for\nchaotic dynamical systems [1, 2, 3, 4], including ocean dynamics [5]. Another approach involves using the resolved\nﬁeld of a complex chaotic and turbulent ﬂow directly in a data-driven model to be trained on and subsequently\nevolved forward in time as has been shown in [6, 7, 8, 9]. The assumption here, is that the data-driven model learns\nthe eﬀects of the unresolved or under-resolved sub-grid scale processes directly on the resolved ﬁeld from a long time\nhistory of the ﬁelds in the training dataset. In this paper, we build on the problem set-up similar to Chattopadhyay\net al. [8], where only part of the system’s dynamics is available for training. Instead of the Lorenz 96 system, as used\nin Chattopadhyay et al. [8] we consider a fully turbulent ﬂow model represented by the two-layered quasi-geostrophic\nequations (QG) with a baroclinically unstable jet [10, 11]. The model complexity of this QG system based on the\ninstantaneous attractor dimension [12] of the upper layer’s stream function (Ψ 1) is about 20 .9 and comparable to\nthe instantaneous attractor dimension of Z500 in the observed atmosphere [6].\nIn this paper, in an eﬀort to perform fully data-driven forecasting from partial observations, we:\n• Develop a deep learning model that can accurately predict the short term dynamics of Ψ 1 without any infor-\nmation about the lower layer’s stream function Ψ2 (even during training) although the dynamics of Ψ 1 and Ψ2\nare strongly coupled in the governing equations (see section 2).\n• Implement an encoding block inside the deep learning model that can embed (within feature maps) relative\ntranslation, scale, rotation, and generic warping of local features of turbulent vortices. The advantages of\n∗Corresponding author: A Chattopadhyay, akc6@rice.edu1 Rice University2 Lawrence Berkeley National Laboratory\nThis is a non-peer reviewed pre-print submitted to EarthArxiv 3 DEEP LEARNING ARCHITECTURE\nincorporating equivariance in convolutional networks has been demonstrated in previous studies, especially in\nthe application of deep learning to physical systems [13].\n• Compare two custom loss functions that improve on the performance and long-term climate stability of the\nmodels compared to standard root-mean-squared error loss functions.\nWe perform prediction of Ψ1(t) autoregressively; i.e., Ψ1(t+ 1) is predicted from previously predicted (or initially\ngiven) Ψ1(t) where t is in days. The encoding block inside the deep learning model implements a spatial trans-\nformer [14] that is equivariant in feature space. Similar capabilities can also be observed in capsule networks which\nhas been used successfully to improve on the performance of predicting extreme weather events in the atmosphere\nover traditional convolutional neural networks [15].\n2 Two-layered Quasi-Geostrophic Equations\nThe non-dimensional dynamical equations of QG has been developed following Lutsko et al. [10], and Nabizadeh et\nal. [11].\n∂qk\n∂t + J(Ψk,qk) = −1\nτd\n(−1)k (Ψ1 −Ψ2 −ΨR)\n−1\nτf\nδk2∇2Ψk −ν∇4qk\n(1)\nqk = ∇2Ψk + (−1)k (Ψ1 −Ψ2) + βy (2)\nIn Eq. (1) and (2), q is the potential vorticity, k denotes the upper ( k = 1) and lower ( k = 2) level. τd is the\nNewtonian relaxation time scale while τf is the Rayleigh friction time scale, which only acts on the lower level\nrepresented by the Kronecker δ function (δk2). J denotes the Jacobian, β is the Rossby parameter, and ν denotes\nthe hyperdiﬀusion coeﬃcient. We have induced a baroclinically unstable jet at the center of a horizontally (zonally)\nperiodic channel by setting Ψ 1 −Ψ2 to be equal to a hyperbolic secant centered at y = 0. When eddy ﬂuxes are\nabsent, Ψ2 is identically zero, making zonal velocity in the upper layer U1(y) = −∂Ψ1\n∂y = −∂ΨR\n∂y where\n−∂ΨR\n∂y = sech2( y\nσ) (3)\nσ being the width of the jet. Eq. (1) is solved using a pseudo-spectral method with a uniform Fourier grid along x\n(128 grid points between x= [0,46]) and Chebyshev grid along y (192 grid points between y= [−1,1]). For training\n(see section 3) some of the upper and lower grid points along y are ignored (retaining y = [−0.6,0.6]) in order to\nremove standing waves that are formed there due to the absence of sponge layers. For more details on the solution\nof Eq. (1) see Nabizadeh et al. [11]. Eq. (1) has been integrated for 120000 days with ∆ t = 0 .005 days using a\nsecond order Adams-Bashforth method. Flow obtained from this simulation is fully turbulent, has an inverse energy\ncascade, and closely mimics the key features of the mid-latitude circulation variability. It presents a challenging test\ncase to develop and test new data-driven models aimed at forecasting geophysical turbulence. The objective of the\ndeep learning framework is to predict Ψ 1(t) as a function of time autoregressively without any information available\non Ψ2(t), although, physically, both Ψ1 and Ψ2 are coupled and they aﬀect each other’s dynamics, as is evident from\nEq. (1).\n3 Deep learning Architecture\nIn this paper, we explore the capability of a state-less convolutional encoder-decoder style architecture with an\nembedded spatial-transformer module to track rotational variances in the feature space of the input. Building on the\nsuperior performance of capsule networks on extreme weather prediction [15], we believe that, spatial transformers\npreserving the key equivariance property in the feature maps of subsequent convolutions [14] (similar to capsules),\nallow us to track the dynamics of vortices in turbulent ﬂow. Compared to capsules, spatial transformers are more\nmemory eﬃcient and easier to implement as a fully diﬀerentiable layer inside any deep learning architecture. The\ndetails of the architecture is shown in the schematic given in Fig 1.\nThis is a non-peer reviewed pre-print submitted to EarthArxiv 3 DEEP LEARNING ARCHITECTURE\n3.1 Localization Network or Encoding Block\nThe architecture takes in an input snapshot Ψ 1(t)88×128 and projects it onto a low dimensional encoding space via\na standard convolutional encoding block. This encoding block performs three convolutions (without changing the\nspatial dimensions) and max-pooling which connects to a dense network with three layers. The encoded feature map\nconnects into the spatial transformer module (see section 3.2). The convolutions inside the encoder block ensure\nperiodic boundary conditions zonally by performing circular convolutions on each feature map inside the encoder\nblock [16].\n3.2 Spatial Transformer Module\nThe spatial transformer takes the feature map as the input and then applies an aﬃne transformation Ω( θ), where\nthe parameters θare learnt through backpropagation. One could use any other transformation but since the vortices\nrotate on the x−y plane in our system, we have chosen an aﬃne transformation so that that relative rotational\nand stretching features in the ﬂow can be captured. The aﬃne transformation results into transforming the original\nco-ordinate space xo\ni and yo\ni into xs\ni and ys\ni following\n[xs\ni\nys\ni\n]\n= Ω(θ)\n[xo\ni\nyo\ni\n]\n(4)\nThe transformer module then applies a diﬀerentiable sampling kernel (a bi-linear interpolation kernel in this case)\nto the input feature map (denoted by F) at the locations given by xs\ni and ys\ni to produce the output feature map G.\nThis can be written as\nGi =\nn=88∑\nn=0\nm=128∑\nm=0\nFnmk(xs\ni −m; Φx)k(ys\ni −n; Φy) (5)\nHere i ∈ [1,2,···88 ×128] and k is a generic sampling kernel with sampling parameters Φ x and Φ y. In our\nimplementation k is a bi-linear interpolation kernel which is diﬀerentiable.\n3.3 Decoding Block\nThe decoding block is a standard series of deconvolution layers (convolution with zero-padded upsampling) that\nbring the features encoded by the transformer module back into its original dimension.\n3.4 Training\nThe architecture is trained on pairs of Ψ train\n1 (t) and Ψ train\n1 (t+ 1) for 100000 days (out of the generated 120000\ndays) with 20% left for validation. The remaining 20000 days have been used for testing. A total of 10 initial\nconditions have been chosen from those remaining 20000 days to test the performance of the architecture in terms\nof short term forecasting and sub-seasonal to seasonal scale (S2S) forecasting. Multi-timestep prediction is done\nautoregressively, i.e. during testing, a randomly chosen initial condition with Ψ test\n1 (t) is inputted to the architecture\nto obtain Ψ test\n1 (t+ 1). This predicted Ψ test\n1 (t+ 1) is fed back in to the architecture to obtain Ψ test\n1 (t+ 2) and so\non and so forth in an autoregressive manner. We have conducted such prediction up to 1000 days and found the\narchitecture to yield a stable and non-drifting climate.\n3.5 Loss Function\nAlong with the standard root-mean-squared error loss function, we have explored the possibility of using custom loss\nfunction aimed at restricting drift in long term climate. For that purpose we introduce loss functions\nL1 = (1 −γ) 1\nN\n∑\nN\n(\nΨpred\n1 −Ψtrue\n1\n)2\n+ 1\nNγ\n∑\nN\nΨpred\n1 (6)\nand\nL2 = (1 −γ) 1\nN\n∑\nN\n(\nΨpred\n1 −Ψtrue\n1\n)2\n+γ 1\nN\n∑\nN\n(\n∇xyΨpred\n1 −∇xyΨtrue\n1\n)2 (7)\nThis is a non-peer reviewed pre-print submitted to EarthArxiv 4 RESULTS\nwhere N is the number of samples per batch during training and γ is a hyper-parameter. Ψ pred\n1 and Ψ true\n1 are\nthe predicted and true values of the upper layer’s stream function respectively. These changes to the loss function\nenable longer, stable, and non-drifting climate comparable to a baseline persistence model while partially improving\nshort-term forecasting performance. In our experiments we found the hyper-parameter γ to be 0 .1 for L1 and 0.25\nfor L2 to yield the best performance. L1 enables the jet-stream from drifting in any direction by pulling it back\ntowards the mid-latitude while L2 is meant for capturing sharp features through the gradient which may otherwise\ncontribute to the error that drifts the jet-stream.\n4 Results\nIn this section we compare the performance of the data-driven deep spatial transformer based convolutional encoder-\ndeocoder against a baseline persistence model and a standard convolutional encoder-decoder model. We investigate\nthe performance with loss functions L1, L2, and a standard root-mean-squared loss function.\n4.1 Short-term performance\nThe short-term performance is measured as the correlation coeﬃcient ( R) between Ψ pred\n1 (t) and Ψ true\n1 (t) at each\nday t starting from a random initial condition on an unseen dataset. Furthermore, we compute this coeﬃcient on\neach snapshot between y = −0.25 and y = 0.25 so that only regions of maximum variance in the turbulent jet are\nconsidered, thus making R a fair metric. Regions further away from the jet show low variability and thus would\nalways be predicted well. We also report the algebraic value of Rinstead of the absolute value so as to determine the\ndrift in long-term data-driven climate, e.g. a negative Rwould suggest that the data-driven model has un-physically\npredicted large low-pressure anomalies near the jet that are not supposed to be there in the true ﬂow.\nFigure 2 shows time snapshots for upto 5 days prediction by the deep learning model with L1 loss compared\nagainst the baseline persistence model. The deep learning model shows R = 70% at 5 days which outperforms\npersistence by 10%. While certain high wave number features in individual snapshots are not properly captured,\nthe overall pattern is well captured along with the amplitude near the turbulent jet. This could be attributed to\nthe equivariance embedded into the neural network that allows it to track, and thus predict rotational features and\nstretching in turbulent eddies.\n4.2 Sub-seasonal to seasonal scale prediction\nAs seen in Weyn et al. [7], persistence performs well further away in time from the initial condition where forecasting\nbegan (sub-seasonal lead days show more correlation with persistence). This is not surprising, since in a chaotic\nsystem, small errors quickly accumulate and the predicted trajectory diverges away from the true trajectory of\nthe dynamical system. Hence, it is important to see how well the data-driven model performs as we increase the\nprediction timescale from 5 days to several months (seasonal scale). We can see in Fig 3 that the data-driven model\nwith L1 and L2 losses outperform persistence and the baseline encoder-decoder without a spatial transformer even\nat ≈10 days (sub-seasonal scale). Moreover, at longer time scales ( ≈20 −90 days) the model remains physical and\nthe predicted jet-stream does not drift. This allows us to compute a physical climate from the data-driven model.\nThe deep learning model with the standard root-mean-squared loss does not simulate a physical climate and shows\na drift in the jet where low pressure anomalies dominate.\n4.3 Climatology\nFigure 4 shows long-term averaged quantities of the dynamics and how they behave in the data-driven models\nproposed as compared to the true long-term dynamics. The zonally averaged Ψ1 has been obtained by, ﬁrst, averaging\nover 1000 days of predicted Ψ 1 to obtain the time mean of Ψ 1 from the data driven model (and compared against\ntrue Ψ1) and then zonally averaging this obtained time mean. A similar analysis has been done for the meridionally\naveraged Ψ1. We can see that the long-term mean of the meridionally averaged Ψ 1 resembles the true long-term\nmean of the system more closely than persistence while in the case of zonally averaged Ψ 1, both persistence and\ndata-driven models resemble the truth quite well.\n4.4 Energetics of the predicted ﬂow\nThe ﬁrst left singular vector and 2nd left singular vector corresponds to the largest and the second-largest singular\nvalues obtained from a singular value decomposition of the long-term mean of Ψ 1. These would denote the most\nThis is a non-peer reviewed pre-print submitted to EarthArxiv REFERENCES\nenergetically dominant modes of the ﬂow. The data-driven models, both with L1 and L2 loss functions capture the\ntrue singular vectors and slightly outperforms persistence. Long-term averaged quantities could not be calculated for\nthe data-driven model with a standard root-mean-squared loss function because, at sub-seasonal scales the turbulent\njet drifts and produces un-physical climate as is evident from the negative R values in Fig 3.\n5 Discussion\nIn this paper we have developed an equivariant convolutional encoder-decoder with a spatial transformer for predicting\nturbulent ﬂow in a two-layered QG system. The rotational and scale invariance inside the spatial transformer [14]\nallows the deep learning architecture to track the rotational variances in the features of turbulent vortices in the QG\nﬂow with limited information of the dynamics during training and testing. We show excellent short-term performance\nupto 5 days outperforming a persistence model. While short-term forecasting of turbulent ﬂow is a very challenging\nproblem in geophysical and engineering applications, a fully data-driven model such as the one proposed in this\npaper shows great promise. We intend to extend these rotationally invariant or equivariant frameworks [13, 15] to\nmore complicated datasets, e.g. re-analyses data, which strongly resemble the observed atmosphere. Beyond fully\ndata-driven models, equivariant neural networks can be incorporated in building data-driven surrogates for sub-grid\nscale closure models or parameterization schemes that are used in complicated fully-coupled climate models [1].\nThe loss functions introduced in this paper help the model in avoiding long-term drift and producing a physically\nmeaningful climate. While the gradient based loss function enables the architecture to capture sharper features\nnear the turbulent jet, the exact mechanism that enables the model to deliver a physically consistent climate at\nthe seasonal scale should be investigated further. The deep learning architecture without L1 and L2 produces an\nunphysical climate with strong drifts in the jet. Although we expect prediction errors in chaotic systems errors to\naccumulate quickly based on the Lyapunov exponent, the reason behind producing an unphysical climate needs to be\ninvestigated more closely. Carefully constructed stateful neural architectures [8] should be explored where predicted\nLyapunov exponent can be compared and corrected compared to the true Lyapunov exponent of the system.\nIn summary, the key contributions of this paper are:\n• a novel deep neural network architecture that incorporates equivariance to track turbulent ﬂow dynamics better\nthan existing data-driven methods.\n• signiﬁcantly improved short-term data-driven forecasting of QG turbulence.\n• stable long-term predictions with a physically meaningful climate and no drift.\nWhile the QG problem is a challenging test case, scaling this architecture for more realistic atmospheric data\n(e.g. re-analysis data) is the next step. Careful ablation studies on the eﬀect of the spatial transformer, number\nof inputs into the architecture, and the limits of spatial resolution (on how big a domain can we predict?) are\nbeing undertaken. We believe that incorporating symmetry [13] and equivariance [15] in neural architectures would\nsigniﬁcantly improve fully data-driven prediction in atmospheric dynamics.\nAcknowledgments\nThis research used resources of the National Energy Research Scientiﬁc Computing Center (NERSC), a U.S. Depart-\nment of Energy Oﬃce of Science User Facility operated under Contract No. DE-AC02-05CH11231. AC with the help\nof KK and MM developed the deep learning codes. The QG code was developed by PH. AC wrote the manuscript.\nAll authors helped in analysing the results and editing the manuscript.\nReferences\n[1] S. Rasp, M. S. Pritchard, and P. Gentine, “Deep learning to represent subgrid processes in climate models,”\nProceedings of the National Academy of Sciences , vol. 115, no. 39, pp. 9684–9689, 2018.\n[2] N. D. Brenowitz and C. S. Bretherton, “Prognostic validation of a neural network uniﬁed physics parameteri-\nzation,” Geophysical Research Letters, vol. 45, no. 12, pp. 6289–6298, 2018.\n[3] P. A. O’Gorman and J. G. Dwyer, “Using machine learning to parameterize moist convection: Potential for\nmodeling of climate, climate change, and extreme events,” Journal of Advances in Modeling Earth Systems ,\nvol. 10, no. 10, pp. 2548–2563, 2018.\nThis is a non-peer reviewed pre-print submitted to EarthArxiv REFERENCES\nFigure 1: Schematic of the deep learning architecture with the spatial transformer module. The architecture is\ntrained with pairs of Ψ 1(t) and Ψ1(t+ 1).\nFigure 2: Short term prediction performance on Ψ 1(upto 5 days) of the deep learning framework with L1 as the loss\nfunction starting from a single random initial condition. The algebraic value of R(correlation coeﬃcient between two\nmatrices) calculated between y = −0.25 and y = 0.25 (where the maximum variability of the turbulent jet occurs)\noutperforms baseline persistence model by ≈10% at 5 days.\nThis is a non-peer reviewed pre-print submitted to EarthArxiv REFERENCES\nFigure 3: R, the algebraic value of the correlation coeﬃcient between two matrices for short term and sub-seasonal to\nseasonal prediction with 3 diﬀerent network architectures. L1 and L2 losses are deﬁned in section 3.5 and compared\nagainst standard root mean squared error loss. While in the very short term, the performance of all three architectures\nare similar, at the seasonal scale ( ≈3 months), L1 and L2 improve stability and reduce drift resulting in a more\nphysical climate with R remaining around 60% even after 3 months. All deep learning architectures outperform\npersistence for short term prediction upto 4 days. Beyond 4 days the architectures with L1 and L2 losses outperform\npersistence for upto 10 days (sub-seasonal scale) and remain comparable with persistence for upto 90 days, with\na stable physical climate. All of the analyses were repeated for 10 diﬀerent initial conditions chosen from the test\nset and separated by at least 1000 days. The mean (symbols) and standard deviation (shading) are reported in the\nﬁgure. The top plot shows R upto 90 days; the bottom plot zooms into the ﬁrst 10 days.\nThis is a non-peer reviewed pre-print submitted to EarthArxiv REFERENCES\nFigure 4: Figure shows long-term averaged dynamical quantities predicted by the deep learning models and compared\nagainst persistence and truth. For both meridionally and zonally averaged Ψ 1, Ψ1 has been averaged over 1000 days.\nBoth the left singular vectors are computed by taking the singular value decomposition of the time averaged Ψ1. The\narchitecture that uses a standard root-mean-squared loss function (or the baseline convolutional encoder-decoder)\nhas not been shown because it does not achieve a stable physical climate for 1000 days and has a drifting turbulent\njet with unphysical low pressure anomalies beyond 10 days.\nThis is a non-peer reviewed pre-print submitted to EarthArxiv REFERENCES\n[4] A. Chattopadhyay, A. Subel, and P. Hassanzadeh, “Data-driven super-parameterization using deep learning:\nExperimentation with multi-scale lorenz 96 systems and transfer-learning,” arXiv preprint arXiv:2002.11167 ,\n2020.\n[5] T. Bolton and L. Zanna, “Applications of deep learning to ocean data inference and subgrid parameterization,”\nJournal of Advances in Modeling Earth Systems , vol. 11, no. 1, pp. 376–399, 2019.\n[6] S. Scher and G. Messori, “Weather and climate forecasting with neural networks: using general circulation\nmodels (gcms) with diﬀerent complexity as a study ground,” Geoscientiﬁc Model Development , vol. 12, no. 7,\npp. 2797–2809, 2019.\n[7] A. Weyn, Jonathan, R. Durran, Dale, and R. Caruana, “Can machines learn to predict weather? using deep\nlearning to predict gridded 500-hPa geopotential height from historical weather data,” Journal of Advances in\nModeling Earth Systems , vol. 10, no. 8, pp. 2680–2693, 2019.\n[8] A. Chattopadhyay, P. Hassanzadeh, D. Subramanian, and K. Palem, “Data-driven prediction of a multi-scale\nlorenz 96 chaotic system using a hierarchy of deep learning methods: Reservoir computing, ann, and rnn-lstm,”\n2019.\n[9] P. D. Dueben and P. Bauer, “Challenges and design choices for global weather and climate models based on\nmachine learning,” Geoscientiﬁc Model Development, vol. 11, no. 10, pp. 3999–4009, 2018.\n[10] N. J. Lutsko, I. M. Held, and P. Zurita-Gotor, “Applying the ﬂuctuation–dissipation theorem to a two-layer\nmodel of quasigeostrophic turbulence,” Journal of the Atmospheric Sciences, vol. 72, no. 8, pp. 3161–3177, 2015.\n[11] E. Nabizadeh, P. Hassanzadeh, D. Yang, and E. A. Barnes, “Size of the atmospheric blocking events: Scaling\nlaw and response to climate change,” Geophysical Research Letters, vol. 46, no. 22, pp. 13488–13499, 2019.\n[12] D. Faranda, G. Messori, and S. Vannitsem, “Attractor dimension of time-averaged climate observables: insights\nfrom a low-order ocean-atmosphere model,” Tellus A: Dynamic Meteorology and Oceanography , vol. 71, no. 1,\np. 1554413, 2019.\n[13] R. Wang, R. Walters, and R. Yu, “Incorporating symmetry into deep dynamics models for improved general-\nization,” arXiv preprint arXiv:2002.03061 , 2020.\n[14] M. Jaderberg, K. Simonyan, A. Zisserman, et al., “Spatial transformer networks,” in Advances in neural infor-\nmation processing systems, pp. 2017–2025, 2015.\n[15] A. Chattopadhyay, E. Nabizadeh, and P. Hassanzadeh, “Analog forecasting of extreme-causing weather patterns\nusing deep learning,” Journal of Advances in Modeling Earth Systems , vol. 12, no. 2, p. e2019MS001958, 2020.\n[16] S. Schubert, P. Neubert, J. P¨ oschmann, and P. Pretzel, “Circular convolutional neural networks for panoramic\nimages and laser data,” in 2019 IEEE Intelligent Vehicles Symposium (IV) , pp. 653–660, IEEE, 2019."
}