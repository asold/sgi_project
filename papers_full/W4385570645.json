{
    "title": "A Length-Extrapolatable Transformer",
    "url": "https://openalex.org/W4385570645",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2113540864",
            "name": "Yutao Sun",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A1974723233",
            "name": "Li Dong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2940149276",
            "name": "Barun Patra",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2099570760",
            "name": "Shuming Ma",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2612910427",
            "name": "Shaohan Huang",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2518092394",
            "name": "Alon Benhaim",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2913883794",
            "name": "Vishrav Chaudhary",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2102676534",
            "name": "Xia Song",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2171151462",
            "name": "Furu Wei",
            "affiliations": [
                "Tsinghua University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4287019748",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963963993",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W4226285793",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4292945941",
        "https://openalex.org/W4309793872",
        "https://openalex.org/W4296932804",
        "https://openalex.org/W4281381535",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3121309507",
        "https://openalex.org/W4306672396",
        "https://openalex.org/W4323654151",
        "https://openalex.org/W4309957186",
        "https://openalex.org/W4312091411",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3171639395",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W3132607382",
        "https://openalex.org/W4388979610",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W4385573804",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3037798801",
        "https://openalex.org/W4224308101"
    ],
    "abstract": "Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, Furu Wei. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 14590–14604\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nA Length-Extrapolatable Transformer\nYutao Sun1∗, Li Dong2, Barun Patra2, Shuming Ma2, Shaohan Huang2\nAlon Benhaim2, Vishrav Chaudhary2, Xia Song2, Furu Wei2\nTsinghua University1\nMicrosoft2\nhttps://github.com/microsoft/torchscale\nAbstract\nPosition modeling plays a critical role in Trans-\nformers. In this paper, we focus on length ex-\ntrapolation, i.e., training on short texts while\nevaluating longer sequences. We define at-\ntention resolutionas an indicator of extrapo-\nlation. Then we propose two designs to im-\nprove the above metric of Transformers. Specif-\nically, we introduce a relative position embed-\nding to explicitly maximize attention resolu-\ntion. Moreover, we use blockwise causal at-\ntention during inference for better efficiency.\nThe proposed architecture is named Length-\nExtrapolatable (LEX) Transformer. We evalu-\nate different Transformer variants on language\nmodeling. Experimental results show that our\nmodel achieves better performance in both in-\nterpolation and extrapolation settings. The\ncode will be available at https://aka.ms/\nLeX-Transformer.\n1 Introduction\nTransformer (Vaswani et al., 2017) has shown\nstrong performance in NLP and become a de-facto\nbackbone (Dosovitskiy et al., 2020; Radford et al.,\n2021; Wang et al., 2022). However, most of them\nhave a crucial shortcoming: they can only deal\nwith the in-distribution size of inputs. Figure 1\nshows that the perplexity of previous Transform-\ners increases rapidly when the input sequence is\ngetting longer. It is usually infeasible to train a\nmodel with all possible input lengths. Therefore, a\nlength-extrapolatable Transformer is essential for\nwider usage.\nIn sequence modeling, position information\nplays a crucial role in building the correct repre-\nsentation and understanding of the latent mean-\ning. For Recurrent Neural Networks such as\nLSTM (Hochreiter and Schmidhuber, 1997), the\ncalculation is done along the sequence order in\nO(N) time. However, the parallel attention module\n∗Work done during internship at Microsoft Research.\nFigure 1: The perplexity of different Transformer de-\nsigns with various input lengths.\nmakes it hard to encode position effectively. First,\nVaswani et al. (2017) proposes absolute sinusoidal\nposition embedding, and Devlin et al. (2019) ad-\njusts it to a learnable one. The absolute design is\ncomputation-efficient, but not comparable with sub-\nsequent relative ones (Shaw et al., 2018; Su et al.,\n2021; Press et al., 2021). Among many relative po-\nsition embeddings, ROPE (Su et al., 2021) shows\nbetter performance and is used to many PLMs such\nas PaLM (Chowdhery et al., 2022). However, it\ncan’t deal with sequences with exceeding length.\nAlibi (Press et al., 2021) mitigates the extrapolation\nproblem but sacrifices the general performance.\nSince different strategies concentrate on some\npart of the position feature, it is essential to build a\ncomprehensive view and guide the Transformer’s\ndesign systematically. First, a Transformer should\nbe sensitive to order. Otherwise, it will degener-\nate into a bag-of-word model which confuses the\nwhole meaning. Then, position translation can’t\nhurt the representation, especially, when a prefix\nis added to the target sentence, the representation\nshould stay the same with an attention mask on the\nprefix. After that, a good sequence model needs to\n14590\nModels Translation Invariance Length Extrapolation\nAbsolute Position Modeling\nTransformer (Sinusoidal) ✘ ✘✘\nGPT-2 (Learnable) ✘ ✘✘\nRelative Position Modeling\nPaLM / Roformer (ROPE) ✔ ✘\nT5 ✔ ✘\nBLOOM / Alibi ✔ ✔\nLEX Transformer (Ours) ✔ ✔✔\nTable 1: Position modeling capabilities of Transformer variants for language modeling.\ndeal with any input length. As illustrated before,\nthe length problem is not universal but special for\nTransformer. Especially, when a Transformer is\npre-trained under a maximal length, it is not afford-\nable to re-train for applying to tasks with longer\nsequences. Finally, when a Transformer satisfies\nthe principles above, we evaluate its performance,\nwhich requires thorough experiments and empirical\nanalysis.\nConsidering all the properties above, we propose\nExtrapolatable Position Embedding (XPOS), which\nis a universal-good design for Transformers. Based\non ROPE’s design, we propose attention resolu-\ntion as a metric to measure position monotonicity.\nThen, we generalize its mathematical form, where\nan exponential decay is added to the rotation ma-\ntrix. XPOS preserves the advantage of ROPE, and\nbehaves stably at long-term dependency. Besides,\ninspired by sparse attention methods (Child et al.,\n2019; Beltagy et al., 2020; Zaheer et al., 2020;\nXiong et al., 2021), we choose blockwise causal\nattention to increase attention resolution, which im-\nproves the performance of length extrapolation for\nlanguage modeling.\nWe train different Transformers from scratch.\nWe evaluate models on PG22 and QMSum (Zhong\net al., 2021) with various input lengths. On the in-\nterpolation experiments, LEX Transformer reaches\nminimal perplexity. In the extrapolation experi-\nments, our methods can continue decreasing the\nperplexity while other methods either can’t extrap-\nolate (i.e., perplexity increases) when the input\nlength is very long. Figure 1 shows clearly that\nLEX Transformer has an opposite tendency com-\npared with others.\nWe summarize our contributions as follows:\n• We summarize the design principles of Trans-\nformers for position modeling.\n• We define attention resolution to indicate a\nTransformer’s capability on encoding posi-\ntion.\n• We propose an extrapolatable position embed-\nding and use blockwise causal attention to\nimprove length extrapolation.\n• We conduct experiments on language model-\ning and show that the proposed LEX Trans-\nformer achieves strong performance on both\nshort and long texts.\n2 Design Principles of Transformers for\nPosition Modeling\n2.1 Order Variance\nA transformer without position information is actu-\nally a bag-of-word model. Although bag-of-words\nmodels can achieve comparable performance for\nsome tasks (Wang et al., 2020a), position infor-\nmation is essential for sequence modeling. Most\nof the existing position modeling satisfies this\ngoal (Vaswani et al., 2017; Devlin et al., 2019;\nShaw et al., 2018; Wang et al., 2020a; Raffel et al.,\n2020; Su et al., 2021). With effective position in-\nformation, Transformer models should be variant\nwith permuting the order (Dufter et al., 2022). Give\na permutation function Pπ(X) : [x1,x2,...,x n] →\n[xπ1 ,xπ2 ,...,x πn], where [π1,π2,...,π n] is a ran-\ndom order, a Transformer model f(input) should\nsatisfy:\nf(Pπ(X)) ̸= Pπ(f(X)) (1)\n2.2 Translation Invariance\nThe representation of a sequence should be ro-\nbust with the translation of its positions. For\ninstance, a sentence’s meaning is invariant with\npadding before or after the whole sentence. Similar\n14591\nto (Wang et al., 2020a), we give a general form\nfor translation invariance: given a Transformer\nmodel f(input,mask), any input sequence X =\n[x0,x1,...,x n] with mask M = [m0,m1,...,m n],\nthe output should be same with the padding ones:\nXpad = [0]i ⊕X⊕[0]j\nMpad = [0]i ⊕M ⊕[0]j\nf(X,M) =f(Xpad,Mpad)[i: i+ n]\n(2)\nRelative positions (Shaw et al., 2018; Raffel\net al., 2020; Wang et al., 2020a; Su et al., 2021)\nsatisfy this condition, while most of the absolute\npositions do not (Vaswani et al., 2017; Devlin et al.,\n2019). Although sinusoidal embedding has a simi-\nlar property (Vaswani et al., 2017): PEpos+k can\nbe represented as a linear function of PEpos, the\naddition operation in the initial word embedding\nmesses the attention weight, where the spread form\nof QKT has 4 components whose geometric con-\nnection with position is unclear.\n2.3 Length Extrapolation\nAs the cost of pre-training is getting bigger due to\nthe larger model size and corpus, it is infeasible to\nretrain a model for a longer context. A Transformer\nmodel with a suitable design should be capable of\ndealing with any input length.\nFirst, learnable absolute position embedding (De-\nvlin et al., 2019) is not able to extrapolate because\nit does not have any pre-defined position knowl-\nedge. With the evaluation of perplexity on dif-\nferent lengths (Press et al., 2021), almost every\nposition embedding’s performance drops signifi-\ncantly (Vaswani et al., 2017; Raffel et al., 2020;\nSu et al., 2021). Alibi (Press et al., 2021) solves\nthis problem by adding an exponential decay on the\nattention matrix, which lower the influence of out-\nof-distribution position like a soft sliding window.\nHowever, the absence of long-term dependency\ncontributes to a performance drop compared with\nother relative strategies. Table 2 shows that Alibi’s\nperplexity is larger than ROPE by about 0.3.\nHowever, the extrapolation ability needs a sys-\ntematic design where position embedding is a cru-\ncial but not only component. With the proper atten-\ntion mask, the relative position can deal with long\ntext. The ideal situation is to use the long context\nin the right way, in that case, the perplexity should\ndecrease as the input length increases.\n3 A Length-Extrapolatable Transformer\nWe define attention resolution as the indicator of\nthe Transformer’s capability on encoding position\nin Section 3.1. Then we propose two ways to maxi-\nmize the resolution metric, i.e., improve the length\ninterpolation and extrapolation of Transformers.\nFirst, we introduce a relative position encoding\nmethod (Section 3.2) to explicitly maximize atten-\ntion resolution. Second, we propose to use block-\nwise causal masking (Section 3.3) during extrapo-\nlation inference for improved resolution.\nIn the following section, we denote das the hid-\nden dimension and las the input length. For each\nattention layer, query, key, and value are calculated\nby input x: q= WQx,k = WKx,v = WVx.\n3.1 Attention Resolution\nThe monotonicity of attention scores is essential\nto represent distance in language models. In an\nattention layer of the vanilla Transformer, we mea-\nsure the attention score expectation as s[n] when\nthe distance of two tokens is n:\ns[n] = E\n0≤i≤N\n(qi+nkT\ni√\nd\n) (3)\nWe define attention resolutionR(s) as a metric\nto evaluate attention’s ability to recognize position:\nR(s) =\nN∑\ni=0\nes[i](es[i] −es[i+1])\n(∑N\ni=0 es[i])2 (4)\nFirst, s[i] >s[i+ 1]is preferred to ensure mono-\ntonicity. Besides, we implement softmax opera-\ntion on s[n] to simulate the attention probability. To\nmitigate the influence of long-tail distribution, the\nfactor es[i] is multiplied. We can estimate s[n] and\nR(s) quantitatively when we design Transformers.\n3.2 Improve Resolution by Position Encoding\nSu et al. (2021) propose that by adding absolute\nposition embedding on query and key, the attention\nmatrix is actually encoded with relative position\ninformation. ROPE shows a strong performance in\ninterpolation tasks, but its s[n] oscillates dramati-\ncally in Figure 2, which harms the resolution.\nWe use a similar but generalized strategy to im-\nprove resolution. First, a pseudo inner product is\ndefined as ⟨x,y⟩= ∑ Re(xi·y∗\ni), which is consis-\ntent with the exact inner product’s definition when\nwe map Cd/2 →Rd. Before calculating attention,\n14592\nthe query and key are encoded with position in-\nformation. Generally, the attention function is as\nfollows:\naij = ⟨fq(qi,i),fk(kj,j)⟩√\nd\noi =\ni∑\nj=0\neaij\n∑i\nj=0 eaij\nvj\n(5)\nFormally, the encoding must satisfy:\n⟨fq(q,n+r),fk(k,n)⟩= ⟨fq(q,r),fk(k,0)⟩(6)\nA simple solution is as follows:\nfq(q,n) =Aqqeλn\nfk(k,n) =Akke−λ∗n (7)\nThe scaling factor Aq,Ak is unnecessary be-\ncause q,k is obtained by a linear transformation.\nSince λ∈Cd/2, it can be represented asλ= ξ+iθ\nwhere ξ,θ ∈Rd/2:\nfq(q,n) =qeξn+iθn\nfk(k,n) =ke−ξn+iθn (8)\nIf ξ = 0, the form is the same as ROPE (Su\net al., 2021). Geometrically, the transformation\nprovides a rotation on vectors. If the relative an-\ngle between qand kis larger, the inner product is\nsmaller. However, the cosine function is not mono-\ntonic if the rotating angle is large than π, which\ncauses an unstable phenomenon in that the expec-\ntation of the inner product oscillates dramatically\nwith the growth of relative distance. Following the\nparameters (Vaswani et al., 2017; Su et al., 2021)\nθ = {θi = 10000−2i/d,i ∈ [0,1,...,d/ 2]}, we\nwill calculate the expectation as follows. For gen-\nerative models, we assume E(∠q) ≤ E(∠k) to\nensure the monotonicity:\nE[⟨qemξ+imθ,ke−nξ+inθ⟩]\n=\nd/2∑\nx=0\nE[Re(qxkxe(m−n)ξx+i(m−n)θx)]\n≤\nd/2∑\nx=0\nRe(E[|qxkx|]e(m−n)ξx+i(m−n)θx)\n∝\nd/2∑\nx=0\ncos(m−n)θxe(m−n)ξx\n(9)\nThe inference here is different from (Su et al.,\n2021) because of two reasons: 1) there is an addi-\ntional assumption brought by generative language\nmodels where E(∠q) ≤E(∠k); 2) the inequality\nscaling of (Su et al., 2021) is too strong to lose\ngenerality. We calculate expectation instead of the\nupper bound.\nNow we define a function to represent the prop-\nerty of relative position:\ngζ[n] =\nd/2∑\ni=0\ncos nθiζn\ni (10)\ng[n] simplifies Equation 9 by defining ζi = eξi.\nStabilizing the g[n] curve is intuitive. Although\nattention bias can achieve this goal, we try to avoid\nadditional position calculations. Instead, we can\naccomplish this goal using a good ζ to maximize\nR(gζ).\nObviously, the oscillation mainly comes from\nlarge θi. Manually setting ζcan achieve this goal:\n˜ζi = i/(d/2) +γ\n1 +γ ∈[0,1] (11)\nwhere ˜ζi becomes smaller when θi is larger. In this\nway, we punish the oscillation of unstable dimen-\nsions and keep the distribution of stable ones.\nNumerical optimization methods are tried to find\noptimal values for ζ. However, the results rely on\nthe initial value and lack control when the hidden\ndimension changes. Besides, the numerical preci-\nsion should be considered because of fp16’s range.\nFinally, we find a sub-optimal solution by manually\nsetting γto both satisfy the resolution is recogniz-\nable (R(gζ) is partially optimized) and ζn\ni can be\nrepresented by fp16 when n is big (8192 in our\nsetting). Besides, in implementation, the position\nis re-scaled with base Bin the exponential calcula-\ntion to avoid overflow and underflow (eξn →eξn/B\nin Equation (8)). We use γ = 0.4 and B = 512as\nthe final implementation in LEX Transformer.\nThe curves of ζ = 1,ˆζ are shown in Figure 2.\nThe default rotary embedding contributes to a dra-\nmatic oscillation, especially in the large relative\ndistance, which causes bad extrapolation perfor-\nmance and restricts the model’s convergence speed.\nAfter adding a decay, the curve is almost stable, es-\npecially on long-term dependency. What’s more, it\ndoes not hurt pure rotation’s fitting ability because\nζn\ni ≈1 when iis large or nis small. In that way,\nshort-term and long-term dependencies are divided\ncontinuously.\nFinally, we have Extrapolatable Position Embed-\n14593\n0 1000 2000 3000 4000\nRelative Distance\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAttention Expectation\nRoPE\nxPos (Ours)\nFigure 2: The long dependency curve of attention ex-\npectation. ROPE’s dramatic oscillation confuses the\nattention resolution at long distances. In contrast, XPOS\nprovides stable and accurate position modeling.\nAlgorithm 1:Attention with XPOS\ndef rot(x):\nreturn [−x1,x0,−x3,x2,...]\nInitialization:\nθi = 1/100002⌊i/2⌋/d, θ∈Rd\nˆζi = (2⌊i/2⌋/d+ γ)/(1 +γ), ˆζ ∈Rd\nInput: Q,K,V ∈Rh×l×d,M ∈Rd×d\nCmn = cosmθn,C ∈Rl×d\nSmn = sinmθn,S ∈Rl×d\nTmn = ˆζm\nn ,T ∈Rl×d\nQ= Q⊙(C⊙T) + rot(Q) ⊙(S⊙T)\nK = K⊙(C⊙T−1)+rot( K)⊙(S⊙T−1)\noutput= softmax(QKT\n√\nd ·M)V\nreturn output\nding (XPOS):\nfq(q,n) =\n\n\nq1 cos nθ1 ˆζn/B\n1 − q2 sin nθ1 ˆζn/B\n1\nq2 cos nθ1 ˆζn/B\n1 + q1 sin nθ1 ˆζn/B\n1\n...\nqn−1 cos nθd/2 ˆζn/B\nd/2 − qn sin nθd/2 ˆζn/B\nd/2\nqn cos nθd/2 ˆζn/B\nd/2 + qn−1 sin nθd/2 ˆζn/B\nd/2\n\n\nfk(k,n) =\n\n\nk1cosnθ1 ˆζ−n/B\n1 − k2 sin nθ1 ˆζ−n/B\n1\nk2 cos nθ1 ˆζ−n/B\n1 + k1 sin nθ1 ˆζ−n/B\n1\n...\nkn−1 cos nθd/2 ˆζ−n/B\nd/2 − kn sin nθd/2 ˆζ−n/B\nd/2\nkn cos nθd/2 ˆζ−n/B\nd/2 + kn−1 sin nθd/2 ˆζ−n/B\nd/2\n\n\n(12)\nIn the implementation, the transformation for\nkey and value can be easily calculated by paral-\nlel addition and multiplication as shown in Algo-\nrithm 1. Since position embedding’s sizeC,S,T ∈\nRl×d is much smaller than batched multi-head at-\nTraining Phase Inference Phase\nFigure 3: Our language model is trained on shorter texts\nin the same way as vanilla Transformers, i.e., using\ncausal masking. During inference, we use blockwise\ncausal attention for longer sequences, which recurrently\nreuses the overlapped parts (i.e., key and value vectors).\ntention matrix and doesn’t require gradients, the\ncost is almost the same with ROPE and bigger than\nAbsolute Position with 6% additional time.\n3.3 Blockwise Causal Attention\nTo deal with length extrapolation, a simple way\nto improve attention resolution (Section 3.1) is us-\ning windowed attention. During inference, we use\nblockwise masking (Dai et al., 2019; Zaheer et al.,\n2020; Xiong et al., 2021) for self-attention. Notice\nthat other window strategies, such as sliding win-\ndow (Child et al., 2019), also work. We use block-\nwise causal attention because it is cache-friendly\nand easy to implement.\nAs shown in Figure 3, if the pre-training length\nis l, we divide the query as blocks with l/2 length,\nand each query interacts with its own block and\nthe last block. In this way, the context information\ncan be delivered by the reuse of key and value.\nThe window constraint helps models encode longer\ninput with improved resolution.\nDifferent from training a long-sequence model\nwith a stop gradient, we use vanilla attention in\nthe training phase, because the pre-training corpus\nis not very long on average. However, during the\ninference phase, when dealing with long sequences,\nwe directly implement BCA to help the model to\nbe more position-recognizable.\n4 Experiments\n4.1 Pre-training\nTo fairly evaluate different Transformer variants,\nwe pre-train the Transformer from scratch. We use\n1024 hidden dimensions, 16 heads, and 24 layers,\n14594\ni.e., comparable to medium-size GPT-3 (Brown\net al., 2020). The training corpus includes a\nsubset of the Pile (Gao et al., 2020): Books3,\nOpenWebText2, Stack Exchange, PubMed Ab-\nstracts, Wikipedia, Gutenberg (PG-19), BookCor-\npus2, NIH ExPorter, and Pile-CC datasets. The\ntraining procedure is performed on 16 ×V100\nGPUs. We use the tokenizer from GPT2 (Radford\net al., 2019). The maximal length is 1024 for saving\nmemory and extrapolation evaluation. The learning\nrate is 3×10−4 and polynomial decay is used to ad-\njust the learning rate. The global batch size is 512\nto follow GPT-3 (Brown et al., 2020), i.e., 0.5M\ntoken size. We use Adam (Kingma and Ba, 2015)\noptimizer with β1 = 0.9,β2 = 0.98,ϵ = 10−6.\nThe code is based on TorchScale (Ma et al., 2022a).\n4.2 Language Modeling\nWe measure perplexity on long document datasets,\nwhich can show the model’s ability for long-\ndependency modeling. We use books from Project\nGutenberg whose years are later than 2019 to en-\nsure no overlap with PG19, and we name it as\nPG22. Besides, we pick QMSum (Zhong et al.,\n2021) from SCROLLS (Shaham et al., 2022) with\nabove 9k length on average. We care about the per-\nformance on different input lengths to evaluate the\nmodel’s interpolation and extrapolation capability.\nFor experiment results in Table 2, we divide the\nsame input into the target length to fairly compare\nthe perplexity of different lengths.\nFor interpolation capability, we analyze the re-\nsults where the length is no more than 1024. Since\nthe validation distribution is very similar to training\ndata, all Transformers’ generalization capabilities\nare also close. XPOS have a stable advantage on\nothers with a 0.09 perplexity drop on PG22, and\n0.27 on QMSum, which proves thatXPOS increases\nthe interpolation ability.\nFor extrapolation lengths, we do not use BCA\nin other Transformers, and the following ablation\nstudy will discuss the performance with that. Press\net al. (2021)’s experiment shows that most of the po-\nsition strategies can’t deal with input length longer\nthan pre-training directly. XPOS shows a stable de-\ncrease when the sequence length increases, which\nsatisfies the assumption that a longer context makes\nthe prediction better. While others’ perplexity in-\ncreases when the input length is 4096.\nTo illustrate the tendency of perplexities, Fig-\nure 1 visualizes the relation between input length\nand perplexity. When the length is larger than 4096,\nAlibi’s perplexity increases gradually. However,\nLEX’s perplexity decreases continuously when the\nlength extends to 8192.\nThe experiment shows that XPOS gets better per-\nformance on language modeling. With the stable\nadvantage of any length, users can input any sen-\ntence freely without the concern of position. Be-\nsides, results also indicate that is not essential to\nbuild an explicit decay on the attention matrix, In-\nstead, a proper design for an attention mask is actu-\nally better to deal with long-context tasks.\n4.3 Measuring Resolution\nWe empirically evaluate the resolution of different\nTransformer variants. In the previous section, we\ndefine attention resolution as a quality indicator of\nposition modeling in Transformers. The expecta-\ntion of s[n] is computed as:\nE[s[n]] = 1\nN −nE[\nN−1∑\ni=n\nai(i−n)] (13)\nwhere aij has the same meaning in Equation 5.\nThen the attention resolution can be calculated\nby combining Equation (4) and Equation (13). The\nfinal expectation is averaged over input texts and\ndifferent layers.\nTable 3 reports the average resolution of various\nTransformer variants. The results show that XPOS\nmakes the position more recognizable in both 1024\n(i.e., training length) and 2048 (i.e., length extrap-\nolation). For Alibi (Press et al., 2021), the stable\nresolution comes from explicit decay, but it pre-\nvents the model from learning position dependency\nitself. In addition, we ablate BCA in 1024 and 2048.\nThe results support that BCA helps the model dis-\ntinguish positions better, achieving higher attention\nresolution.\n4.4 Ablation Studies\n4.4.1 Rotation Computation\nAs shown in Table 4, we discuss the necessity of\nthe combination of vector rotation and exponential\ndecay. XPOS without rotation means Equation (12)\ndegenerates to θi = 0:\n˙fq(q,n) =\n\n\nq1 ˆζn\n1\nq2 ˆζn\n1\n...\nqn−1 ˆζn\nd/2\nqnˆζn\nd/2\n\n\n˙fk(k,n) =\n\n\nk1 ˆζ−n\n1\nk2 ˆζ−n\n1\n...\nkn−1 ˆζ−n\nd/2\nknˆζ−n\nd/2\n\n\n14595\nLength 256 512 1024 2048 4096 8192\nInterpolation Extrapolation\nPG22\nTransformer 38.1 33.5 30.54 132.46 1446.95 12747.41\nAlibi 34.25 30.01 27.34 26.01 28.46 32.8\nRoformer 33.27 29.2 26.68 68.86 235.71 458.83\nLEX Transformer (Ours) 33.18 29.11 26.59 25.53 25.07 24.89\nQMSum\nTransformer 24.25 18.81 16.05 86.56 1196.92 10781.38\nAlibi 22.85 17.74 15.17 13.97 15.36 18.37\nRoformer 22.66 17.65 15.12 36.54 146.61 331.56\nLEX Transformer (Ours) 22.01 17.24 14.85 13.92 13.56 13.48\nTable 2: Results of perplexity with different lengths. The language models are trained with a length of 1024 and\nthen evaluated on various lengths. LEX obtains better performance not only on shorter texts (i.e., interpolation) but\nalso on longer texts (i.e., extrapolation). The red color indicates that the perplexity begins increasing compared with\nthe shorter length. L EX is the only method that has lower perplexity along with increased evaluation length.\nLength 1024 2048\nInterpolation Extrapolation\nTransformer 0.87 0.28\nAlibi 0.81 0.88\nRoformer 0.91 0.08\nLEX (Ours) 0.98 1.08\n−BCA 0.98 0.54\nTable 3: Results of resolution with different Transformer\nvariants. Higher resolution indicates that the architec-\nture tends to better distinguish context tokens. “BCA”\nis short for blockwise causal attention.\nMoreover, the setting ofζ = 0is RoPE (Su et al.,\n2021), which can be viewed as a special case of our\nmethod. Besides, we discuss the situation when\nζ is a scalar instead of a vector, where we choose\nζ = γ/(1 +γ) as the value.\nAfter pre-training on 1024, we evaluate the per-\nplexity of PG22 with 1024 and 8192 lengths. Ta-\nble 4 shows that simple scaling operation can-\nnot match the performance of LEX. The vector\nζ also performs better than ζ = 0and γ/(1 +γ).\nTherefore, the combination of rotation and decay\nmeans the combination of in-distribution and out-\nof-distribution capability in terms of length.\n4.4.2 Blockwise Causal Attention\nAs shown in Table 5, we run the evaluation using\ndifferent position embeddings (i.e., Absolute, Al-\nibi, ROPE, and XPOS) with or without blockwise\nMethods 1024 8192\nInterpolation Extrapolation\nLEX 26.59 24.89\nw/o Rotation 37.11 34.5\nζ = 0 26.68 26.16\nScalar ζ 26.85 25.1\nTable 4: Ablation results on the PG22 set show that\nrotation of XPOS is necessary for strong performance.\ncausal attention.\nFirst, Blockwise Causal Attention works for\nROPE whose perplexity will explode without that.\nAlibi performs well without windowed attention\nbecause its “soft window” is broader than a hard\nblock window. However, when the sequence length\nincreases to 8192, windowed attention outperforms\nvanilla attention again (also shown in Figure 1).\nXPOS’s perplexity without BCA increases by about\n1.5 in 2048, and 40 in 8192. However, with its\nhigh resolution, XPOS can recognize position with\nBCA’s constraint.\nBesides, we compare BCA with Sliding Atten-\ntion (Child et al., 2019). In this experiment, we set\nthe window size as 1024 to align with pre-training.\nSliding Attention performs better as shown in the\nlast row of Table 5 because its interaction range is\nbroader than Block Causal Attention. The reason\nto use block windows instead of sliding windows\nis efficiency. According to (Xiong et al., 2021),\nthe training speed of Blockwise Attention is 1.5x\n14596\nMethods 2048 8192\nExtrapolation\nAbsolute 132.46 12747.41\nAbsolute + BCA 322.73 28787.01\nROPE 68.86 458.83\nROPE + BCA 26.37 26.16\nAlibi 26.01 32.8\nAlibi + BCA 27.53 31.82\nXPOS 27.29 63.99\nXPOS + BCA 25.53 24.89\nXPOS + Sliding Window 25.33 24.61\nTable 5: Results of perplexity on PG22 dataset. “BCA”\nis short for blockwise causal attention.\nfaster than using sliding windows. Therefore, LEX\nmakes a trade-off and uses BCA in our implemen-\ntation. Without losing generality, our method is\nalso compatible with Sliding Attention and other\nlocal attention variants.\n5 Related Work\n5.1 Long-Sequence Transformers\nLong-sequence Transformers aim to solve two key\nproblems. First, the computation or memory con-\nsumption is not efficient enough for long sequences.\nSecond, there is a trade-off between performance\nand efficiency.\nOne popular solution (Wang et al., 2020b;\nKatharopoulos et al., 2020; Choromanski et al.,\n2020) is linear attention, i.e., using a kernel-based\nor low-rank approximation to replace vanilla atten-\ntion. The methods typically target efficiency while\nunderperforming vanilla Transformers for regular\nlength. Another strand is sparse attention (Child\net al., 2019; Beltagy et al., 2020; Zaheer et al., 2020;\nXiong et al., 2021), which usually leverages struc-\ntured sparsity to reduce computation. For causal se-\nquence modeling, the recurrent-style designs (Dai\net al., 2019; Hutchins et al., 2022; Ma et al., 2022b)\nare also competitive.\nIn comparison, we focus on length extrapola-\ntion (Press et al., 2021) for language modeling, i.e.,\ntraining on short texts while evaluating long texts.\nThe training process is kept the same as vanilla\nTransformers. The capability of long-sequence\nmodeling is given for free during inference. So\ntraining efficiency (which is typically expensive\nfor large-scale language models) is not affected\ncompared with previous work. Moreover, the per-\nformance on regular length is perfectly retained,\nwithout trade-offs for long-sequence modeling.\n5.2 Position Modeling\n5.2.1 Absolute Position Embedding\nAbsolute sinusoidal position embedding is pro-\nposed by Vaswani et al. (2017), which is the ini-\ntial design of the Transformer. For each dimen-\nsion, different frequencies are encoded from 2πto\n10000 ×2π:\nPE(pos,2i) = cos(pos/100002i/dmodel )\nPE(pos,2i+1) = sin(pos/100002i/dmodel )\n(14)\nwhere PEpos+k is represented as a linear function\nof PEpos to restore a relative-position property.\n5.2.2 Relative Position Embedding\nShaw et al. (2018) propose relative position em-\nbedding as an alternative approach. Denote aij as\nattention weight, αij = softmax(aij), oi as output,\nwe have:\naij = qi ·kj√\nd\n=⇒\nqi ·(kj + pK\nij)√\nd\noi =\n∑\nj\nαijvj =⇒\n∑\nj\nαij(vj + pV\nij)\n(15)\nwhere pK\nij = ωK\nmin(i−j,k),pV\nij = ωV\nmin(i−j,k), and\nωK and ωV are learnable parameters. The clipping\nstrategy helps length generalization but cannot dis-\ntinguish the positions that are larger than k. Yang\net al. (2019) and He et al. (2020) further reparam-\neterize the relative position vectors for better per-\nformance. T5 (Raffel et al., 2020) uses a simpler\nstrategy to encode relative position:\naij = qi ·kj√\nd\n+ pbucket(i−j) (16)\nwhere log-bucket scalars are added to attention\nscores. Recently, pre-defined position embedding\nis brought back by ROPE (Su et al., 2021). Al-\nibi (Press et al., 2021) proposes to explicitly build\nan exponential decay on the attention matrix, which\ncontributes to length extrapolation:\naij = qi ·kj√\nd\n−m(i−j), m(·) >0 (17)\nwhere the values of m(·) are manually defined.\nHowever, Alibi (Press et al., 2021)’s performance\ntends to be inferior to ROPE for the context whose\n14597\nlength is shorter than the pre-training length. In this\nwork, we propose a theoretically derived relative\nposition embedding XPOS that optimizes the atten-\ntion resolution between tokens. The XPOS method\nnot only has the nice property of length extrapola-\ntion but also achieves strong performance.\n6 Conclusion\nWe proposeLEX Transformer to accurately capture\nposition information for Transformers. We define\nattention resolution as the metric of length extrapo-\nlation and design a solution to improve the model-\ning. Extensive experiments on language modeling\nshow that our method achieves lower perplexity\non longer sequences while training on short texts.\nThe simplicity also makes the method a go-to aug-\nmentation for Transformer-based language models.\nIn addition, attention resolution provides a more\nprincipled view for position modeling, which sheds\nlight on future architecture design.\nLimitations\nIn this work, we focus on causal language mod-\neling. It needs additional efforts to integrate the\nproposed methods into bidirectional attention, such\nas masked language modeling (Devlin et al., 2019).\nMoreover, XPOS introduces about 6% inference\ncost compared with absolute position embeddings,\nalthough it accelerates training convergence.\nReferences\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nTa-Chung Chi, Ting-Han Fan, Peter J Ramadge, and\nAlexander I Rudnicky. 2022a. Kerple: Kernelized\nrelative positional embedding for length extrapola-\ntion. arXiv preprint arXiv:2205.09921.\nTa-Chung Chi, Ting-Han Fan, and Alexander I Rud-\nnicky. 2022b. Receptive field alignment enables\ntransformer length extrapolation. arXiv preprint\narXiv:2212.10356.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. URL\nhttps://openai.com/blog/sparse-transformers.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2020. Rethinking attention with\nperformers. arXiv preprint arXiv:2009.14794.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek B Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinodku-\nmar Prabhakaran, Emily Reif, Nan Du, Benton C.\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier García,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Oliveira Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Díaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff\nDean, Slav Petrov, and Noah Fiedel. 2022. PaLM:\nScaling language modeling with pathways. ArXiv,\nabs/2204.02311.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a fixed-length context. arXiv preprint\narXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929.\n14598\nPhilipp Dufter, Martin Schmitt, and Hinrich Schütze.\n2022. Position information in transformers: An\noverview. Computational Linguistics, 48(3):733–\n763.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9:1735–\n1780.\nDeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan\nDyer, and Behnam Neyshabur. 2022. Block-recurrent\nTransformers. In Advances in Neural Information\nProcessing Systems.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In International Conference on Machine\nLearning, pages 5156–5165. PMLR.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Interna-\ntional Conference on Learning Representations, San\nDiego, CA.\nTomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris\nDyer, Karl Moritz Hermann, Gábor Melis, and Ed-\nward Grefenstette. 2018. The narrativeqa reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317–328.\nShuming Ma, Hongyu Wang, Shaohan Huang, Wenhui\nWang, Zewen Chi, Li Dong, Alon Benhaim, Barun\nPatra, Vishrav Chaudhary, Xia Song, and Furu Wei.\n2022a. TorchScale: Transformers at scale. CoRR,\nabs/2211.13184.\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian\nHe, Liangke Gui, Graham Neubig, Jonathan May,\nand Luke Zettlemoyer. 2022b. Mega: Moving\naverage equipped gated attention. arXiv preprint\narXiv:2209.10655.\nOfir Press, Noah A Smith, and Mike Lewis. 2021.\nTrain short, test long: Attention with linear biases\nenables input length extrapolation. arXiv preprint\narXiv:2108.12409.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori\nYoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor\nGeva, Jonathan Berant, et al. 2022. Scrolls: Stan-\ndardized comparison over long language sequences.\narXiv preprint arXiv:2201.03533.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\narXiv preprint arXiv:1803.02155.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yun-\nfeng Liu. 2021. Roformer: Enhanced transformer\nwith rotary position embedding. arXiv preprint\narXiv:2104.09864.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 December\n2017, Long Beach, CA, USA, pages 6000–6010.\nBenyou Wang, Lifeng Shang, Christina Lioma, Xin\nJiang, Hao Yang, Qun Liu, and Jakob Grue Simon-\nsen. 2020a. On position embeddings in bert. In\nInternational Conference on Learning Representa-\ntions.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020b. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768.\nWenhui Wang, Hangbo Bao, Li Dong, Johan\nBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit\nSom, et al. 2022. Image as a foreign language: BEiT\npretraining for all vision and vision-language tasks.\narXiv preprint arXiv:2208.10442.\nWenhan Xiong, Barlas O ˘guz, Anchit Gupta, Xilun\nChen, Diana Liskovich, Omer Levy, Wen-tau Yih,\nand Yashar Mehdad. 2021. Simple local attentions\nremain competitive for long-context tasks. arXiv\npreprint arXiv:2112.07210.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems, volume 32. Curran\nAssociates, Inc.\n14599\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33:17283–17297.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\nCelikyilmaz, Yang Liu, Xipeng Qiu, et al. 2021.\nQmsum: A new benchmark for query-based multi-\ndomain meeting summarization. arXiv preprint\narXiv:2104.05938.\n14600\nA Additional Experiments\nBesides the experiments in Section 4, we run lan-\nguage modeling evaluation on Arxiv and Narra-\ntiveQA (Ko ˇcisk`y et al., 2018). The results are\nshown in Table 6. These datasets have their short-\ncomings. The article length of Arxiv is usually less\nthan 8192, and part of NarrativeQA’s corpus is sam-\npled from PG19, which is in the training dataset.\nTherefore, we show them in the appendix instead\nof the main content.\nB Hyperparameters for Pre-Training\nAs shown in Table 7, we present the hyper-\nparameters for pre-training. The setting keeps\nthe same among all Transformer variants. We\nfollow medium-size GPT3 (Brown et al., 2020),\n24 layers, 1024 hidden size, 4096 FFN inner hid-\nden size, and 16 attention heads. The number\nof batch tokens is 0.5M, for pre-training 1024,\nand the number of batch sentences is 512. We\nuse Adam (Kingma and Ba, 2015) optimizer with\nβ1 = 0.9,β2 = 0.98,ϵ = 10−6. The warmup steps\nare 20k, and we use 50k checkpoints for evaluation.\n14601\nLength 256 512 1024 2048 4096\nInterpolation Extrapolation\narXiv\nTransformer 29.74 23.6 19.59 102.09 1240.77\nAlibi 26.53 21.07 17.53 15.38 16.88\nRoformer 25.89 20.6 17.24 49.29 199.25\nLEX Transformer (Ours) 25.73 20.48 17.14 15.81 15.19\nNarrativeQA\nTransformer 16.74 14.42 13.02 58.95 574.91\nAlibi 15.58 13.45 12.15 11.4 12.09\nRoformer 15.21 13.16 11.93 20.72 35.14\nLEX Transformer (Ours) 14.82 12.86 11.67 11.14 10.93\nTable 6: Results of perplexity with different lengths. The language models are trained with a length of 1024 and\nthen evaluated on various lengths. LEX obtains better performance not only on shorter texts (i.e., interpolation) but\nalso on longer texts (i.e., extrapolation). The red color indicates that the perplexity begins increasing compared with\nthe shorter length. L EX is the only method that has lower perplexity along with increased evaluation length.\nHyperparameters Value\nLayers 24\nHidden size 1024\nFFN inner hidden size 4096\nAttention heads 16\nTraining steps 50K\nBatch tokens per task 0.5M\nAdam ϵ 1e-6\nAdam β (0.9, 0.98)\nLearning rate 3e-4\nLearning rate schedule Polynomial\nWarmup steps 20,000\nGradient clipping 2.0\nWeight decay 0.01\nTable 7: Hyperparameters used for language model pre-\ntraining.\n14602\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nIn the ﬁnal part after conclusion\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. It is fundamental research and not tied to particular applications.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. These tools and models are publicly available and free of use for research purposes.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nThe use is consistent with their intended use.\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nSection 4\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection4.1 and Appendix B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14603\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection4.1 and Appendix B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4.1\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n14604"
}