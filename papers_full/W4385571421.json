{
  "title": "Limitations of Language Models in Arithmetic and Symbolic Induction",
  "url": "https://openalex.org/W4385571421",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2084034187",
      "name": "Jing Qian",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A106695634",
      "name": "Hong Wang",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2133641497",
      "name": "Zekun Li",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2108352657",
      "name": "Shiyang Li",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2116657824",
      "name": "Xifeng Yan",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4285287883",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4303633609",
    "https://openalex.org/W2167224731",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4286986405",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3133029875",
    "https://openalex.org/W2251935656",
    "https://openalex.org/W4283768109",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W1931877416",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4310625358"
  ],
  "abstract": "Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models. However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs. Experimental results show that none of these techniques can solve the simplest addition induction problem completely. In the end, we introduce LMs with tutor, which demonstrates every single step of teaching. LMs with tutor is able to deliver 100% accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 9285–9298\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLimitations of Language Models in Arithmetic and Symbolic Induction\nJing Qian∗, Hong Wang∗, Zekun Li, Shiyang Li, Xifeng Yan\nUniversity of California, Santa Barbara\n{jing_qian, hongwang600, zekunli, shiyangli, xyan}@cs.ucsb.edu\nAbstract\nRecent work has shown that large pretrained\nLanguage Models (LMs) can not only perform\nremarkably well on a range of Natural Lan-\nguage Processing (NLP) tasks but also start\nimproving on reasoning tasks such as arith-\nmetic induction, symbolic manipulation, and\ncommonsense reasoning with increasing size\nof models (Wei et al., 2022; Chowdhery et al.,\n2022). However, it is still unclear what the\nunderlying capabilities of these LMs are. Sur-\nprisingly, we find that these models have limi-\ntations on certain basic symbolic manipulation\ntasks such as copy, reverse, and addition. When\nthe total number of symbols or repeating sym-\nbols increases, the model performance drops\nquickly. We investigate the potential causes\nbehind this phenomenon and examine a set of\npossible methods, including explicit positional\nmarkers, fine-grained computation steps, and\nLMs with callable programs. Experimental re-\nsults show that none of these techniques can\nsolve the simplest addition induction problem\ncompletely. In the end, we introduce LMs with\ntutor, which demonstrates every single step of\nteaching. LMs with tutor is able to deliver\n100% accuracy in situations of OOD and re-\npeating symbols, shedding new insights on the\nboundary of large LMs in induction.\n1 Introduction\nTransformer-based large pretrained Language Mod-\nels, such as GPT3 and T5 (Vaswani et al., 2017;\nBrown et al., 2020; Raffel et al., 2020), have been\nwidely used as few-shot learners in many NLP\ntasks. Recent work even finds these models can\nachieve state-of-the-art performance in arithmetic\nand symbolic reasoning (Nye et al., 2021; Wei et al.,\n2022). Although these models exhibit surprisingly\nimpressive capabilities in complex arithmetic rea-\nsoning tasks, such as MultiArith (Roy and Roth,\n2015) and GSM8k (Cobbe et al., 2021), it has also\n∗ The first two authors (Jing and Hong) contributed\nequally to this work.\nFigure 1: Examples of addition: the baseline setting\n(top) and Scratchpad (Nye et al., 2021) with intermedi-\nate steps (bottom). A similar method with more detailed\ndemonstration is introduced in (Recchia, 2021).\nbeen pointed out that they tend to make certain\ncalculation errors and perform significantly worse\nwhen the number of math operations increases in\nequations (Wei et al., 2022). Brown et al. (2020)\nfind that GPT3 displays strong proficiency in 2-\ndigit arithmetic addition, but struggles in arithmetic\naddition on numbers with more than three digits.\nNogueira et al. (2021) also observe that the fine-\ntuned T5 model can not correctly add or subtract\narbitrarily long numbers. Larger models might\nperform better on the testing data, but worse on\nnumbers that are longer than the training data (out-\nof-distribution, OOD) (Nogueira et al., 2021).\nFigure 1 shows two possible addition exemplars\nfor LMs on addition problem. The scratchpad ver-\nsion gives more details on how humans do basic\narithmetic. Nye et al. (2021) show that with more\nfine-grained demonstrations, the accuracy of addi-\ntion can be improved dramatically with fine-tuning.\nYet, it still can not achieve 100% on OOD data,\neven with thousands of training data points. Figure\n2 shows the performance of GPT-3 and T5 on addi-\ntion using the scratchpad version of training data.\nThe problem becomes more severe when there are\n9285\nFigure 2: The horizontal axis is the number of digits and the vertical axis is the accuracy. The prompts for GPT3\nconsist of 4 examples. The T5 models are trained on 1-5 digits of up to 2,000 examples and each training example\nconsists of random numbers in the format of 2 4 1 . In-dist: in-distribution. Out-of-dist.: out-of-distribution\n(OOD). In-distribution refers to training on up to k-digit numbers and testing on up to k-digit numbers while out-of-\ndistribution refers to training on up to k-digit numbers and testing on numbers with more digits. αindicates the\nrepetition level of the examples. An example x1 ···xn with ndigits are sampled with the next digit probability\np(xi+1|xi) =α, when xi+1 = xi; otherwise, (1 −α)/9. Larger αindicates a higher repetition level.\nrepeating digits in the addition operands.\nAs the performance drops with repeating digits,\nwe suspect that LMs might not handle the repeating\nsymbols well. Figure 2 illustrates the performance\nof GPT-3 and T5 on the copy task, one of the sim-\nplest symbolic manipulation operations. GPT-3\nand T5 still can not perform well on OOD. We\nfurther do a preliminary experiment where a T5\nmodel is fine-tuned using the data containing re-\npeating numbers of up to 80 digits, T5 still can\nnot achieve 100% in-distribution accuracy on long\nrepeating digits. The results indicate that there are\ntwo problems intervening: Transformers are not\ngood at handling repeating symbols and OOD gen-\neralization. The repeating symbols can also be a\nproblem even for in-distribution data. We believe\nthat overcoming the aforementioned limitations is\nof critical importance for the future application\nof Transformer-based LMs to reasoning-intensive\ntasks such as data format conversion and robotic\nprocess automation.\nIn this paper, we investigate the potential causes\nbehind this phenomenon and examine a set of pos-\nsible mitigation solutions including fine-grained\ncomputation steps, positional markers, and LMs\nwith callable programs. Since incorporating com-\nputation steps improves the OOD generalization in\narithmetic addition (Nye et al., 2021), one possible\ndirection is to provide more fine-grained compu-\ntation steps in the fine-tuning data or the few-shot\nprompt. However, it may not be sufficient to alle-\nviate the problem of repeating numbers. When a\nhuman does addition, the position of each digit is\nused to differentiate the repeating digits. However,\nthe self-attention mechanism in the Transformer\nmay not tell which “1” is referred to in the input.\nThis prompts us to explore using positional markers\nto differentiate the important tokens. Using these\ntwo methods to augment the reasoning process, we\nfind that the performance of pretrained LMs still\ncan not reach satisfying results. Then we resort to\na method where the copy operation is implemented\nas a primitive function and explore whether the LM\ncan further boost its performance.\nWe experiment with three symbolic manipula-\ntion tasks: copying, reversing, and addition. Exper-\nimental results show that although generalization\nin these symbolic manipulation tasks is straightfor-\nward for humans, it is still challenging for LMs, and\nnone of these mitigation methods fully solves the\nproblems. In the end, we introduce LMs with tutor\nwhich demonstrates every single step of teaching,\npinpointing where these digits come from. LMs\nwith tutor is able to deliver 100% accuracy in situa-\ntions of OOD and repeated symbols. In this design,\nLMs are used to generate actions that mimic opera-\ntions in multiple tape Turing machines, rather than\nthe intermediate results. These actions generate the\nintermediate results on tapes. We hope this could\nshed light on the capability of Transformer-based\nLMs in addition to providing large training datasets\nor scaling up the size of these models.\nTo conclude, our main contributions are:\n• We identify a set of simple symbolic manipu-\nlation tasks and uncover the limitations of the\nLMs in arithmetic and symbolic induction.\n• We examine a set of potential techniques in-\ncluding positional markers, fine-grained com-\nputation steps, and LMs with callable pro-\ngrams. Though they could mitigate the limita-\ntions of the LMs, none of them can completely\n9286\nsolve the generalization problem.\n• Finally, we demonstrate that LMs with tutor\nis able to deliver 100% accuracy in situations\nof OOD and repeated symbols. Our analysis\ncould inspire new thoughts to overcome the\nlimitation of LMs in symbolic manipulation.\n2 Related Work\nLarge Pretrained Language Models: Brown et al.\n(2020) show that GPT3 exhibits strong proficiency\non 2-digit addition and subtraction using simply\nfew-shot prompting, without any task-specific train-\ning. Furthermore, the larger the LM, the better the\nperformance. Following GPT3, Chowdhery et al.\n(2022) further scale the Transformer-based LMs\nto a 540-billion parameter model, called Pathways\nLanguage Model (PaLM). Same as Brown et al.\n(2020), Chowdhery et al. (2022) find that scaling\nthe LMs consistently results in better arithmetic\nreasoning ability with few-shot prompting. How-\never, the reasoning ability of the large LMs is still\nlimited. GPT3 struggles with 3-digit arithmetic\nand with direct prompting, even 540B PaLM can\nnot achieve high performance on complex tasks\nrequiring multi-step reasoning. Therefore Wei et al.\n(2022) propose the following prompting method\nfor large pretrained LMs.\nChain-of-Thought Prompting: This prompting\nmethod provides a few chain-of-thought demonstra-\ntions, which is a series of intermediate reasoning\nsteps, as exemplars in the prompting. Therefore,\ngiven a complex reasoning task, the model is al-\nlowed to calculate the intermediate results step-\nby-step before generating the final answer. With\nchain-of-thought prompting, a complex reasoning\ntask is decomposed into a list of simple operations\nand LMs can derive these operations one by one.\nKim et al. (2022) adopt faithful explanations that\naccurately represent the reasoning process behind\nsolving a math word problem. Wei et al. (2022)\nshow that combining chain-of-thought prompting\nand a sufficiently large LM, 540B PaLM, can sig-\nnificantly improve the LMs’ reasoning ability on\ncomplex tasks, such as math word problems.\nFine-tuning with Large Training Datasets: In-\nstead of few-shot prompting, another direction is\nto fine-tune large LMs with a sufficient amount\nof training data. Nogueira et al. (2021) fine-tune\nT5 with different ways of representing numbers,\nbut even with the best-performing representation,\nthe fine-tuned model can not achieve as good ac-\ncuracy on out-of-distribution testing examples as\nin-distribution testing examples. Nye et al. (2021)\npropose to use Scratchpad to improve the out-of-\ndistribution accuracy. Scratchpad combines step-\nby-step reasoning with fine-tuning. The training\nexamples include the intermediate steps of an algo-\nrithm in target, so the model is trained to generate\nnot only the final answer, but also the intermediate\nsteps, which is similar to chain-of-thought, but re-\nquires more training data. Nye et al. (2021) show\nthat using the training data augmented with interme-\ndiate steps significantly improves the model perfor-\nmance, but even with 100k augmented training ex-\namples for the addition task, the fine-tuned 1B LM\nstill does not perform well on out-of-distribution\naddition. Our work is also related to Graves et al.\n(2014), which extends the capabilities of Recurrent\nNeural Networks to two simple symbolic manipula-\ntion tasks, copy and sort, by augmenting the model\nwith external memory resources.\n3 Mitigation Methods\n3.1 Positional Markers\nWe first explore possible methods to mitigate the\nproblem of repeating numbers. We introduce two\ntypes of positional markers: implicit positional\nmarkers and explicit ones.\nMost Transformer-based LMs encode the posi-\ntional information into positional vectors and add\neach of them to the corresponding word vector.\nAlthough large LMs have already incorporated po-\nsitional encoding in the model architecture (Fig-\nure 3), results in Figure 2 indicate that the posi-\ntional encoding commonly used in large LMs may\nnot be sufficient to locate each repeating digit ef-\nfectively. Instead of representing each token by the\nsum of its contextual token embedding and the po-\nsition embedding, DeBERTa (He et al., 2021) rep-\nresents each token with a token embedding and a\nposition embedding, respectively, and the attention\nweights are computed using disentangled matrices\nbased on both embeddings, respectively (Figure 3).\nIn other words, the self-attention in DeBERTa is\ndisentangled. With the disentangled relative po-\nsition embeddings, the attention scores between\ntokens depend not only on the content but also\non the relative position between the tokens, so the\ndisentangled relative position embeddings act as\nimplicit position markers within DeBERTa, which\nmight make it easier for the model to learn the la-\ntent position relationship in the training data of the\n9287\nFigure 3: An illustration of standard Transformer atten-\ntion (left) and DeBERTa disentangled attention (right).\nsymbolic manipulation tasks.\nAlthough DeBERTa uses disentangled attention\nmechanism, it was not originally introduced to en-\nhance the locating capability of LMs, so no pre-\ntraining task was specifically proposed for training\nthe position embeddings in DeBERTa. This may\npotentially lead to its limited generalization ability\non the induction tasks requiring accurate locating.\nRather than relying on implicit positional markers,\nanother, more straightforward approach is to add\nexplicit positional markers in the model input. For\nexample, the input string 2 2 2 is augmented with\npositional markers A, B, C, ···. We explore two\nmethods of adding explicit positional markers:\nOrdered marker: The markers are inserted into\nthe input in order. 2 2 2 →A 2 B 2 C 2\nRandom marker: The markers are inserted into\nthe input in random order. 2 2 2 →E 2 X 2 J 2\nWith the explicit positional markers, each repeat-\ning 2 becomes different for the model. When do-\ning symbolic manipulation, the Transformer-based\nLMs can easily locate the digit by recognizing the\nexplicit positional markers. Essentially, adding\nexplicit positional markers breaks the repeating\nnumbers into a non-repeating input sequence. This\nmethod is also related to pointer networks (Vinyals\net al., 2015), which uses attention as a pointer to\nselect the position indexes of the input tokens as\nthe output. A hybrid pointer-generator network can\nalso be leveraged to copy number from the source\ntext, while retaining the ability to produce new\nnumbers through the generator (See et al., 2017).\n3.2 Fine-grained Computation Steps\nWe then explore possible methods to alleviate the\nOOD generalization problem. One observation is\nthat the complexity of addition with long digits\nis larger than that of the 1-digit addition. Thus,\nthe model should be given more computation time\non the task when the numbers are large. The fine-\ntuned T5 and prompted GPT3 mentioned above,\nhowever, is required to generate the answer with\na fixed amount of computation, so one possible\ndirection to mitigate this limitation is to allow the\nmodel to operate step-by-step instead of generating\nthe answer in one forward pass. For example, in k-\ndigit addition, the model is allowed to break it down\ninto k simple 1-digit addition and the model is\nallowed to generate k intermediate addition results\nto get the final answer.\nGenerating fine-grained computation steps can\npotentially alleviate the generalization problem, but\nmay not contribute to the locating capability of\nthe Transformer-based LMs. To mitigate the locat-\ning problem, we add positional markers to scratch-\npad (Nye et al., 2021) (Figure 4).\nquestion: 1 1 + 2 5solution:convert 1 1 into ☞1, ☛1.convert 2 5 into ☞2, ☛5.☛1 5, carry 0, so 1 + 5 + 0 = 6. carry 0, step result 6. combine 6 and result, get result 6.☞1 2, carry 0, so 1 + 2 + 0 = 3. carry 0, step result 3. combine 3 and result 6, get result 3 6.carry 0, combine 0 and result 3 6, final result3 6.\nFigure 4: The prompt for GPT3 on the addition task.\nWe use /hand-p⌢int-rightand /hand-p⌢int-rightto denote optional different markers\nas described in Section 3.1 if they are applied.\nWe also experiment a more comprehensive\nscheme where we directly copy the number associ-\nated with the explicit positional marker to its later\nappearance. For example, for the explicit marker\nS[B], we copy its value 1 to the later appearance in\nthe fourth line as shown in Figure 5. More detail\nand experimental results are put in appendix A.4.\nquestion: question: S[B] 1S[A]1 + T[B]2T[A] 5solution:S[A] 1 + T[A] 5 + Z[A]0 = R[A]6, Z[B]0S[B] 1 + T[B] 2 + Z[B]0 = R[B]3, Z[C]0result: Z[C]0 R[B]3 R[A]6\nFigure 5: The demonstration of comprehensive scheme\nfor addition problem. Position markers are marked in\nred and reference markers are marked in green.\n9288\n3.3 LM with Callable Programs\nSince callable programs do not have the general-\nization problem, we combine LMs with callable\nprograms to replace the basic symbolic operations\nwhen possible. For example, when combined with\nthe fine-grained computation steps in the addition\ntask, the convert, add, or combine operations can\nbe considered callable programs. When the LM\ngenerates the text sequence add(1,5), the callable\nfunction add will be invoked and return the result\nin text: carry C: 0, result 6.\nFollowing the example in Section 3.2, with\ncallable functions, the prompt format is as follows:\nquestion: 1 1 + 2 5solution:call convert (1 1, 2 5), return ☞(1 2), ☛(1 5).☛(1 5), call add (1, 5), return carry C: 0, result 6.call combine(6, ), return 6.☞(1 2), call add (C: 0, 1, 2), return carry C: 0, result 3.call combine(3, 6), return 3 6.call combine(C: 0, 3 6), return 3 6, final result3 6.\nFigure 6: The prompt for GPT3 on the addition task\nwith callable programs. /hand-p⌢int-rightand /hand-p⌢int-rightare positional mark-\ners. Different callable programs (convert, add and com-\nbine) are marked in different colors, and the results they\nreturned are underlined with the corresponding color.\nGiven a testing example, the prompted GPT3\nfirst generates the solution step by step. During\nthe process, the results of the function calls will be\nappended to the generated result to be used in the\nfollowing steps. Callable programs can be viewed\nas decomposing a complex task to smaller, simpler\njobs. The remaing issue is to learn chaining these\nsmaller jobs together to complete the task.\nCallable programs can guarantee the correctness\nof output given correct input for a given job. How-\never, LMs may still suffer from the locating prob-\nlem since the callable programs rely on LMs to\ndecide which token to copy (Figure 11 in the ap-\npendix). Unfortunately, LMs cannot guarantee the\ncorrectness of this copy action.\n3.4 LM with Tutor\nScratchpad (Nye et al., 2021) ignores the visual\nprocess when an elementary school tutor visually\nillustrates how to perform addition step by step:\npinpointing where each digit in the output sequence\ncomes from, adding single digits together and iter-\nating. It turns out that these details and abstractions\nFigure 7: An illustration of doing copy with pattern\nmatching.\nare important in order to simplify the learning pro-\ncess and help kids learn addition in a few shots.\nA tutor shows every single step visually and\nsometimes calls an already learned sub-module to\ncomplete a task. In this way, the hypothesis space\nbetween two consecutive steps can be dramatically\nsimplified; hence the chance of learning a correct\nmodel can be improved.\nTake copy as an example. Instead of providing a\ntraining example: copy: 1 1 1 2 2 2 result:\n1 1 1 2 2 2, we need to demonstrate where the\nfirst 1, the second 1, and the third 1 in the output\nsequence come from, which exactly imitates the\nfinest action a human could do to perform such an\noperation. Suppose there is a cursor placed at the\nbeginning of the input sequence, a “rmov” oper-\nation moves the cursor one token to the right. A\n“cpy” operation copies a single digit to the output\nsequence. An “end” operation checks if the marker\nreaches the end of the sequence. “T” and “F” rep-\nresent true and false respectively. We assume all\nthese actions have been learned. Then a possible\naction sequence to complete the copy operation is\nas follows:\nrmov, end=F, cpy, rmov, end=F, cpy, . . . ,\nrmov, end=T.\nThis fine-grained action sequence accurately de-\nscribes the whole copy operation. Certainly, there\nare other ways to perform copying. For example,\ninstead of using a cursor, one can use a pattern\nmatch to perform the copy operation (Figure 7).\nWe suspect that the copy operation learned from\nTransformer is following this pattern-matching ap-\nproach, which is error-prone when the pattern has\nrepeating symbols and when the long pattern is\nout-of-distribution. Positional markers do not help\neither as they seem unable to handle the OOD gen-\neralization problem.\nIf we take the action sequence “rmov, end=F,\n. . . ” to train a Transformer for copying, the hypoth-\nesis space is simplified, thus making it possible\nto find the simplest model that can simulate the\nwhole action sequence. This setting involves train-\n9289\nFigure 8: An illustration of the LM with Tutor method.\nWith the tutor (right), the LM or just a transformer (left)\ngenerates an action sequence that simulates how humans\ndo arithmetic addition.\ning a learner to predict the next action based on\nthe input and the actions demonstrated by experts,\nwhich is similar to the setting of imitation learning\n(Pomerleau, 1988; Ross et al., 2011). Although\nthere is no guarantee that Transformer can defi-\nnitely find the correct model, the chance is much\nhigher. One can also relate the setting with a multi-\nple tape Turing machine where the state transition\nis conducted among the positions of tape heads and\nread/write operations. The Transformer is trained\nto learn such state transitions, thus completing the\nprogramming of a Turing machine.\nAs for the addition operation, a similar action\nsequence can be obtained to simulate how humans\ntutor kids do addition at an early age (Figure 8).\nLet “lmov” denote moving the cursor one token\nto the left. The “add” operation adds three single\ndigits together, one from each of the two operands\nand the third one from the carry digit, appends the\nresult to the output, and updates the carry digit.\nAssume “add” is a callable program as kids have\nlearned how to do single digits addition. Suppose\nthe cursor starts from the end of the operands. The\nentire action sequence looks like the following.\nlmov, end=F, add, lmov, end=F, add, . . . ,\nlmov, end=T.\nThe main difference between the tutor and the\nScratchpad method (Nye et al., 2021) is the abstract\ncallable function and detailed action sequence. The\naction sequence includes all the state transitions\nneeded to complete the task. It perfectly overcomes\nthe OOD issue and does not require many training\nexamples in order to achieve 100% accuracy.\nWhile there is a great effort to enlarge\nTransformer-based LMs such as PALM (Chowdh-\nery et al., 2022) and Minerva (Lewkowycz et al.,\n2022), to improve the performance in symbolic and\nlogical reasoning, our result reveals that it might\nbe necessary to demonstrate the action sequence\nwith reasonable abstraction to the Transformer to\nleverage its full strength.\nIn cases where action sequences are not avail-\nable, e.g., only a problem specification is given, it\nmight be more appropriate to develop an LLM (al-\ngorithm generator) to generate an algorithm sketch\nand then run another LLM to execute the sketch\nto get the answer. The sketch need not to be in\nthe form of program codes. A human understand-\nable step-by-step instruction is good enough. The\nsketch can be viewed as an intermediate model\nwhose complexity is much smaller than the LLM\nitself. Hence it has a better chance of solving the\ngeneralization/OOD issue.\n4 Experiments\nIn this section, we conduct experiments on three\ndifferent problems including copying, addition, and\nanother basic symbolic manipulation operation, re-\nverse. We illustrate the limitation of LMs in sym-\nbolic and arithmetic induction and the improvement\nthat could be achieved by the mitigation methods.\n4.1 Copy Operation\nCopying is the most basic operation. We experi-\nment with the following methods and make sure\neach digit is tokenized into a single token by sepa-\nrating the digits with blanks:\nGPT3: We prompt GPT3 to output the same to-\nkens as the given input. Full prompt can be found\nin appendix (Figure 12).\nDeBERTa / T5: The training example is as follows:\ncopy: 1 2 3 4 result: 1 2 3 4\nT5 + ordered marker: The training data is aug-\nmented with explicit positional markers. copy: A\n1 B 2 C 3 result: A 1 B 2 C 3\nT5 + random marker : Same as above, but the\naugmented positional markers are in random order.\ncopy: E 1 A 2 F 3 result: E 1 A 2 F 3\nT5 / GPT3 + tutor: The training and testing exam-\nples are as described in Section 3.4.\nWe experiment with the T5-base (220M) model,\nDeBERTa-base (140M) model, and GPT3 text-\ndavinci-002. The models are initiated with the\npretrained parameters and further fine-tuned on the\ntraining data. For GPT3 or T5 with tutor, the train-\ning data consists of 15 examples of up to 5 digits.\nFor all the other T5 models and DeBERTa, the\n9290\nFigure 9: Experimental results. (a): results of copying repeating numbers. (b)(c): results of reversing the list.\n(d)(e)(f): results on arithmetic addition. The x-axis is the number of digits or number of items.\ntraining data consists of 2,000 random numbers\nof up to 5 digits. We evaluate all the models on\ncopying repeating numbers of up to 80 digits. The\nresults are illustrated in Figure 9(a).\nAs shown in Figure 9(a), GPT3 achieves 100%\naccuracy on the in-distribution testing data (1-5 dig-\nits) but the fine-tuned T5 achieves 78% accuracy on\nthe 5-digit repeating numbers although they are in-\ndistribution. Augmented with random or ordered\npositional markers, the T5 models achieve 100%\nin-distribution accuracy, and so does using implicit\npositional markers (DeBERTa). This suggests that\nboth implicit positional markers and explicit po-\nsitional markers may help with the locating capa-\nbility of LMs. However, using explicit positional\nmarkers, either ordered or random, the model ex-\nhibits significantly better generalization to OOD\ntesting data whereas DeBERTa fails on OOD data.\nGPT3 exhibits better OOD generalization than T5\nwith positional markers but it does not generalize\nwell beyond 30 digits. Both T5 + tutor and GPT3\n+ tutor keeps 100% accuracy on OOD testing data.\n4.2 Addition\nFor arithmetic addition, we experiment with the\nfollowing methods:\nGPT3: We prompt GPT3 to directly output the\nsum for given addition equation. Full prompt can\nbe found in appendix (Figure 13).\nGPT3 + coarse-grained steps: The exemplar is\nsimilar to that in Figure 4, but the instructions for\nthe result combination and the computation of the\ncarry digit and step result are omitted.\nGPT3 + fine-grained steps (+ ordered marker):\nThe exemplar we use is as shown in Figure 4.\nGPT3 + callable programs : The exemplar is\nshown in Figure 6.\nDeBERTa / T5: The training data follows the for-\nmat of the exemplar for GPT3.\nDeBERTa / T5 + fine-grained steps: The training\ndata used in this setting follow the format as the\nexemplar in GPT3 + fine-grained steps.\nT5 + ordered / random marker: The training ex-\nample is augmented with ordered or random mark-\ners. For example, question: G 1 C 1 + G 2 C\n5 result: G 3 C 6. For the ordered marker, we\napply it to the digits as the following: C 2 B 2 A 2.\nT5 + fine-grained steps + ordered / random\nmarker: The training data in this setting follow\na similar format as the exemplar in GPT3 + fine-\ngrained steps + ordered marker, but the positional\nmarkers can be in random order.\nT5 / GPT3 + tutor: The training and testing exam-\nples are as described in Section 3.4.\n9291\nThe model settings are the same as in the above\ncopy experiments. For LMs with tutor, the training\ndata or prompt consists of 15 examples of up to 5\ndigits. In other settings, the training data consists of\n1,000 examples of 1-5 digit addition and for GPT3,\nthe prompt includes 4 examples. We evaluate all\nthe models on the addition of up to 30 digits. The\nresults are shown in Figure 9(d)(e)(f).\nAs shown in Figure 9(d), both coarse-grained\nand fine-grained computation steps contribute to\nthe in-distribution performance of GPT3, and us-\ning finer-grained steps achieves larger performance\ngains on both in-distribution data and OOD data.\nThe performance is further boosted with explicit\npositional markers. Experiments on T5 (Figure\n9(e)(f)) also show the effectiveness of using explicit\npositional markers, with or without fine-grained\ncomputation steps, indicating that the explicit po-\nsitional markers might make it easier for LMs to\nlearn the induction in the arithmetic reasoning tasks.\nSimilar to the results on the copying task, both De-\nBERTa andDeBERTa + fine-grained steps achieve\nnear 100% in-distribution accuracy but 0% OOD\naccuracy, suggesting that the relative position em-\nbedding of DeBERTa might have limited OOD\ngeneralization ability. On T5, incorporating fine-\ngrained computation steps does not improve the\nOOD performance as significantly as on GPT3\n(Figure 9(f)). The reason might be that fine-tuning\nT5 tends to overfit more easily than prompting\nGPT3. Unsurprisingly, GPT3 + callable programs\nachieves much better OOD generalization. How-\never, its OOD performance still degrades as the\nnumber of digits increases. Same as in the copy\nexperiments, LMs + tutor keeps 100% accuracy on\nall the experimented numbers of digits.\n4.3 Reverse List\nBesides copying and addition, we also experiment\nwith reversing. Reversing is similar to copying.\nBoth require replicating the items in the input, but\nreversing might be more challenging than copying\nin the terms of locating. In copying, the distance\nbetween each source digit and the replicated digit\nis the same for each digit in the number. However,\nwhen reversing, the distance between the source\nitem and the replicated item keeps increasing dur-\ning the generation. For this problem, we experi-\nment with the following methods:\nGPT3: We prompt GPT3 to directly output the\nreversed list of items without intermediate steps.\nFull prompt can be found in appendix (Figure 14).\nDeBERTa / T5 : reverse the list: bike,\napple, book result: bike, cat, pen\nGPT3 / DeBERTa / T5 + fine-grained steps: The\ntraining example for T5 and the exemplar for GPT3\nare shown in Figure 10.\nreverse the list: bike, cat, pensolution:A is bike. B is cat. C is pen.Now to reverse, change the order to:C is pen. B is cat. A is bike.Result: pen, cat, bike\nFigure 10: The prompt for GPT3 on the reverse task\nwith fine-grained steps.\nT5 + ordered marker : The list items are aug-\nmented with the ordered positional markers in the\ninput. reverse the list: A bike, B cat, C\npen result: pen, cat, bike.\nT5 / GPT3 + tutor: The training and testing exam-\nples are very similar to that for the copy task. The\nonly difference is the direction for move operation.\n“rmov” in the copy task is replaced by “lmov” here.\nThe model settings are the same as in the above\nexperiments and the training data consists of ex-\namples of 1-5 items, which are randomly sampled\nfrom a predefined list of single-token nouns. For\nLMs with tutor, the training data or prompt consists\nof 15 examples of up to 5 items. For T5, the train-\ning data consists of 1,000 examples. For GPT3,\neach prompt includes 4 examples. We evaluate all\nthe models on reversing the list of up to 30 items.\nThe results are shown in Figure 9(b)(c).\nAlthough GPT3 can generalize to 80 digits\non copying random numbers (Figure 2), it does\nnot generalize well beyond 20 items on revers-\ning, which suggests that reversing might require\nstronger locating capability than copying. This\nproblem also occurs on DeBERTa and T5. When\ntested on the OOD data, the models tends to gener-\nate only a sublist of the input. Using fine-grained\nsteps (Figure 9(b)) or positional markers, whether\nimplicit or explicit (Figure 9(c)), does not signif-\nicantly improve the generalization of the experi-\nmented models. The reason might be the increasing\ndistance between the source item and the replicated\nitem as stated above. Again, LMs + tutor maintains\n100% accuracy throughout the experiments. We\nput more discussion about the results in appendix\nA.5 due to the page limit.\n9292\n5 Conclusion\nIn this work, we explore the limitations of pre-\ntrained LMs on arithmetic reasoning and symbolic\nmanipulation. We experiment with three simple\nsymbolic manipulation tasks and show that improv-\ning the locating and induction capability of LMs\ncan be important for further improving their perfor-\nmance. Our method that combines abstraction and\nfinest-grained step-by-step tutoring demonstrates\nits potential to generalize correctly, shedding light\non possible directions orthogonal to scaling up LMs\nfor future work in this area.\n6 Limitations\nIn this work, we experiment with GPT3, T5, and\nDeBERTa. Other large pretrained LMs, such as\nPaLM (Chowdhery et al., 2022), is not covered in\nthis work. We do not experiment with methods\nsuch as fine-tuning GPT3 due to the computation\ncost. The main purpose of this work is to uncover\nand analyze the fundamental limitations of LMs\non symbolic and arithmetic induction instead of\nimproving their performance of reasoning tasks, so\nwe do not directly compare the mitigation methods\nwith the previous work such as scratchpad (Nye\net al., 2021) and (Wei et al., 2022) in our experi-\nments. We leave more advanced methods for future\nwork.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nJacob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training verifiers to solve\nmath word problems. CoRR, abs/2110.14168.\nAlex Graves, Greg Wayne, and Ivo Danihelka. 2014.\nNeural turing machines. CoRR, abs/1410.5401.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nBugeun Kim, Kyung Seo Ki, Sangkyu Rhim, and Gah-\ngene Gweon. 2022. EPT-X: An expression-pointer\ntransformer model that generates eXplanations for\nnumbers. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 4442–4458.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay V . Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag,\nTheo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur,\nGuy Gur-Ari, and Vedant Misra. 2022. Solving quan-\ntitative reasoning problems with language models.\nCoRR, abs/2206.14858.\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2021.\nInvestigating the limitations of the transformers with\nsimple arithmetic tasks. CoRR, abs/2102.13019.\nMaxwell I. Nye, Anders Johan Andreassen, Guy Gur-\nAri, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten\nBosma, David Luan, Charles Sutton, and Augustus\nOdena. 2021. Show your work: Scratchpads for inter-\nmediate computation with language models. CoRR,\nabs/2112.00114.\nDean Pomerleau. 1988. ALVINN: an autonomous land\nvehicle in a neural network. In Advances in Neural\nInformation Processing Systems 1, [NIPS Confer-\nence, Denver, Colorado, USA, 1988], pages 305–313.\nMorgan Kaufmann.\n9293\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nGabriel Recchia. 2021. Teaching autoregressive lan-\nguage models complex tasks by demonstration. Com-\nputing Research Repository, abs/2109.02102. Ver-\nsion 3.\nStéphane Ross, Geoffrey J. Gordon, and Drew Bag-\nnell. 2011. A reduction of imitation learning and\nstructured prediction to no-regret online learning. In\nProceedings of the Fourteenth International Confer-\nence on Artificial Intelligence and Statistics, AIS-\nTATS 2011, Fort Lauderdale, USA, April 11-13, 2011,\nvolume 15 of JMLR Proceedings, pages 627–635.\nJMLR.org.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2015, Lisbon, Portugal,\nSeptember 17-21, 2015, pages 1743–1752. The As-\nsociation for Computational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in Neural\nInformation Processing Systems 28: Annual Confer-\nence on Neural Information Processing Systems 2015,\nDecember 7-12, 2015, Montreal, Quebec, Canada ,\npages 2692–2700.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nA Appendix\nA.1 Error case for LM with callable program\nHere we show one error case for LM with callable\nprogram in Figure 11.\nFigure 11: An error example of GPT3 with callable\nfunctions. The error is highlighted.\nA.2 GPT3 prompts\nHere we show the prompts of GPT3 used for copy,\naddition and reverse tasks in Figure 12, 13 and 14.\ncopy: 8 3 2 2result: 8 3 2 2copy: 7 7 7 7result: 7 7 7 7copy: 3 9 4 3 2result: 3 9 4 3 2copy: 6 6 6 6 6result: 6 6 6 6 6\nFigure 12: The prompt for GPT3 on the copy task.\nquestion: 1 1 + 2 5result: 3 6question: 5 0 2 + 7 0 3result: 1 2 0 5question: 1 9 2 7 + 4 2 1 8result: 6 1 4 5question: 3 1 3 9 8 + 4 7 2 7 1result: 7 8 6 6 \nFigure 13: The prompt for GPT3 on the addition task\nwithout intermediate steps.\nreverse the list: bike, cat, penresult: pen, cat, bikereverse the list: chair, bike, apple, bookresult: book, apple, bike, chairreverse the list: book, phone, fish, orange, fishresult: fish, orange, fish, phone, book\nFigure 14: The prompt for GPT3 on the reverse task\nwithout intermediate steps.\n9294\nquestion: S[F] 5 S[E] 2 S[D] 8 S[C] 1 S[B] 7 S[A] 1 +T[F] 6 T[E] 5 T[D] 0 T[C] 2 T[B] 4 T[A] 5solution: S[A] 1 + T[A] 5 + Z[A] 0 = R[A] 6, Z[B] 0. S[B] 7 + T[B] 4 + Z[B] 0 = R[B] 1, Z[C] 1. S[C] 1 + T[C] 2 + Z[C] 1 = R[C] 4, Z[D] 0. S[D] 8 + T[D] 0 + Z[D] 0 = R[D] 8, Z[E] 0. S[E] 2 + T[E] 5 + Z[E] 0 = R[E] 7, Z[F] 0. result: Z[F] 0 R[E] 7 R[D] 8 R[C] 4 R[B] 1 R[A] 6\nFigure 15: Error case for T5 model with positional and\nreference marker on addition problem.\nA.3 Experiment configuration\nFor fine-tuning the T5-base and DeBERTa model,\nwe use the learning rate 5e-5, batch size 16, train-\ning epochs 200. The maximum generation length\nis set to 512. The checkpoints are evaluated every\n1000 optimization steps. The random seed is fixed\nto 42. We use the implementation for Hugging-\nFace (Wolf et al., 2020). For GPT3, we set tem-\nperature=0, top_p=1, frequency_penalty=0, and\npresence_penalty=0. All the experiments are con-\nducted on NVIDIA RTX A6000 GPUs.\nA.4 Reference marker\nAs shown in Figure 5, we apply two different mark-\ners in the demonstration. The positional marker\nis used to define the value stored in the marker,\nwhile reference marker is used to explicitly copy\nthe value from the positional marker with the\nsame name. Each number in this demonstration\nis uniquely marked with positional or reference\nmarker. For the positional marker, the model needs\nto generate both the marker and its value. For the\nreference marker, the model only needs to generate\nthe marker and the value will be explicitly copied\nfrom its corresponding positional marker.\nSimilar to previous experiments on the addition\nproblem, we train the model on 1-5 digits and\ntest its performance on both in-domain (1-5 digits)\nand out-of-domain (6-10 digits) settings. The ex-\nperimental results show that the model is able to\nachieve 100% accuracy on in-domain data, but get\n0% accuracy on out-of-domain data. We also tried\nto extend the in-domain to 10 digits and get the\nsame results that the model can solve in-domain\nproblems, but fail to generalize to out-of-domain.\nWe show one error case of this model in Figure\n15, where the error step is highlighted in yellow. On\nthis 6-digit addition problem, the model skipped\nthe last digit and directly jump to the result, which\ncauses the error. The problem is the model doesn’t\nlearn to how to generalize from 1-5 digits to 6 digits.\nInstead, it is overfitting to the training data, which\nmakes it directly output the results after adding 5\ndigits. How to reduce the hypothesis space and\nforce the model to learn to generalize to out-of-\ndomain data would be one future research direction\nto solve this problem.\nA.5 Discussion\nFrom the experimental results, we observe that fine-\ngrained computation steps may improve the LM’s\ninduction ability on the arithmetic reasoning tasks\nand the granularity of the steps has an impact on the\nperformance improvement. Finer-grained compu-\ntation steps may contribute to larger performance\nimprovement.\nPositional markers, whether implicit or explicit,\nimproves LMs’ in-distribution performance on all\nthe symbolic manipulation tasks in our experi-\nments. However, We find that augmented with\nthe relative position embeddings, DeBERTa tends\nto face more severe over-fitting than T5 during\nfine-tuning. In the reversing experiment, using the\nT5 model without pretrained parameters, the fine-\ntuned model can not achieve a good in-distribution\nperformance after 200k optimization steps. How-\never, the DeBERTa model without pretrained pa-\nrameters achieves 100% in-distribution accuracy\nwithin only 2k optimization steps while the OOD\naccuracy drops, indicating that it has overfitted\nwithin 2k optimization steps. In other words, the\nrelative position embeddings in DeBERTa signifi-\ncantly improve the model’s capacity of positions,\nwhich improves in-distribution performance on\nsimple symbolic manipulation tasks, but may not\ngeneralize well on OOD data. Compared with the\nimplicit positional markers (relative position em-\nbeddings in DeBERTa), explicit positional markers\nmight have better OOD generalization ability. How-\never, incorporating symbolic manipulation tasks in\nthe LM pretraining stage might alleviate this prob-\nlem, so incorporating implicit positional markers\ncan still be a possible direction of improving the\nLM’s performance on reasoning tasks requiring\nlocating ability.\nUsing LM with callable programs exhibits strong\nOOD performance on addition, suggesting that the\nLMs’ ability to perform simple symbolic opera-\ntions, such as copying, splitting, and combining,\ncan be critical for improving their performance on\n9295\nreasoning tasks. How to further improve the LMs’\nperformance on more complex reasoning tasks in\nthis direction is left for future work.\n9296\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n6\n□\u0017 A2. Did you discuss any potential risks of your work?\nWe don’t think our work has any potential risks.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\n4\n□\u0017 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9297\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nA.3\n□\u0017 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nI reported the results from a single run\n□\u0017 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNo used.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n9298",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8033939599990845
    },
    {
      "name": "TUTOR",
      "score": 0.6024143099784851
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5675904750823975
    },
    {
      "name": "Computation",
      "score": 0.5078375935554504
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4519200325012207
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4383113384246826
    },
    {
      "name": "Arithmetic",
      "score": 0.4131041467189789
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3959139883518219
    },
    {
      "name": "Programming language",
      "score": 0.338728129863739
    },
    {
      "name": "Mathematics",
      "score": 0.13152152299880981
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    }
  ]
}