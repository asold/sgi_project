{
  "title": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization",
  "url": "https://openalex.org/W4393149346",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101989053",
      "name": "Yanjing Li",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5100604665",
      "name": "Sheng Xu",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5017033486",
      "name": "Mingbao Lin",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5038809760",
      "name": "Xianbin Cao",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5031128205",
      "name": "Chuanjian Liu",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5100642898",
      "name": "Xiao Sun",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A5101503795",
      "name": "Baochang Zhang",
      "affiliations": [
        "Beihang University",
        "Nanchang Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970601456",
    "https://openalex.org/W2772955562",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W4285410455",
    "https://openalex.org/W1902934009",
    "https://openalex.org/W6683826617",
    "https://openalex.org/W4287101401",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W4382491647",
    "https://openalex.org/W4386081242",
    "https://openalex.org/W4306295203",
    "https://openalex.org/W3216998657",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3010631141",
    "https://openalex.org/W6797854001",
    "https://openalex.org/W2887447938",
    "https://openalex.org/W6810353043",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W2300242332",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W6809815429",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4362598148",
    "https://openalex.org/W6849640399",
    "https://openalex.org/W4297631367",
    "https://openalex.org/W4304194126",
    "https://openalex.org/W3188427387",
    "https://openalex.org/W2945785363",
    "https://openalex.org/W3197529273",
    "https://openalex.org/W3205764742",
    "https://openalex.org/W2952899695",
    "https://openalex.org/W4225737925",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3104151879",
    "https://openalex.org/W3211787299",
    "https://openalex.org/W4312925101",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4386076174",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W4312736238",
    "https://openalex.org/W4382239283",
    "https://openalex.org/W4386075873",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W4382517209",
    "https://openalex.org/W2242818861",
    "https://openalex.org/W4382319001",
    "https://openalex.org/W2916954108",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W4308536459",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4312866765",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3096609285"
  ],
  "abstract": "Vision transformers (ViTs) quantization offers a promising prospect to facilitate deploying large pre-trained networks on resource-limited devices. Fully-binarized ViTs (Bi-ViT) that pushes the quantization of ViTs to its limit remain largely unexplored and a very challenging task yet, due to their unacceptable performance. Through extensive empirical analyses, we identify the severe drop in ViT binarization is caused by attention distortion in self-attention, which technically stems from the gradient vanishing and ranking disorder. To address these issues, we first introduce a learnable scaling factor to reactivate the vanished gradients and illustrate its effectiveness through theoretical and experimental analyses. We then propose a ranking-aware distillation method to rectify the disordered ranking in a teacher-student framework. Bi-ViT achieves significant improvements over popular DeiT and Swin backbones in terms of Top-1 accuracy and FLOPs. For example, with DeiT-Tiny and Swin-Tiny, our method significantly outperforms baselines by 22.1% and 21.4% respectively, while 61.5x and 56.1x theoretical acceleration in terms of FLOPs compared with real-valued counterparts on ImageNet. Our codes and models are attached on https://github.com/YanjingLi0202/Bi-ViT/ .",
  "full_text": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization\nYanjing Li1*, Sheng Xu1*, Mingbao Lin2, Xianbin Cao1‚Ä†, Chuanjian Liu3, Xiao Sun4 ‚Ä†, Baochang\nZhang5,6,7\n1 Beihang University\n2 Tencent\n3 Huawei Noah‚Äôs Ark Lab\n4 Shanghai Artificial Intelligence Laboratory\n5 Zhongguancun Laboratory\n6 Hangzhou Research Institute, Beihang University\n7 Nanchang Institute of Technology\nAbstract\nVision transformers (ViTs) quantization offers a promising\nprospect to facilitate deploying large pre-trained networks on\nresource-limited devices. Fully-binarized ViTs (Bi-ViT) that\npushes the quantization of ViTs to its limit remain largely\nunexplored and a very challenging task yet, due to their un-\nacceptable performance. Through extensive empirical analy-\nses, we identify the severe drop in ViT binarization is caused\nby attention distortion in self-attention, which technically\nstems from the gradient vanishing and ranking disorder. To\naddress these issues, we first introduce a learnable scaling\nfactor to reactivate the vanished gradients and illustrate its\neffectiveness through theoretical and experimental analyses.\nWe then propose a ranking-aware distillation method to rec-\ntify the disordered ranking in a teacher-student framework.\nBi-ViT achieves significant improvements over popular DeiT\nand Swin backbones in terms of Top-1 accuracy and FLOPs.\nFor example, with DeiT-Tiny and Swin-Tiny, our method\nsignificantly outperforms baselines by 22.1% and 21.4% re-\nspectively, while 61.5 √ó and 56.1 √ó theoretical acceleration\nin terms of FLOPs compared with real-valued counterparts\non ImageNet. Our codes and models are attached on https:\n//github.com/YanjingLi0202/Bi-ViT/\nIntroduction\nTransformers, which have gained far-flung fame in natural\nlanguage processing (NLP) area (Devlin et al. 2018; Qin\net al. 2022), are also attracting increasing attention in lots of\ncomputer vision (CV) tasks, such as object detection (Car-\nion et al. 2020), image classification (Dosovitskiy et al.\n2020) and many others (He et al. 2022; Tian et al. 2022),\nimpelling the widespread research on vision transformers\n(ViTs). There has a natural fit for ViTs to achieve better\nperformance simply by training a larger model on a larger\ndata set. For example, historical records show better perfor-\nmance of a ViT-H model (Dosovitskiy et al. 2020) accompa-\nnying with astonishing 632M parameters and 162G FLOPs.\nSuch a high model complexity poses a great challenge to\n*These authors contributed equally.\n‚Ä†Corresponding author.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n70605040302010328 4 3 2 1\n 328 4 3 2 1\n70605040302010\n80\nAccuracy\nAccuracy\n# Bits # Bits\nSharp Drop~ 52.6 % ‚ÜìSharp Drop~ 61.9 % ‚Üì\n(a) DeiT-Tiny (b) DeiT-Small\nFigure 1: Performance of real-valued and quantized\nDeiT (Touvron et al. 2021) with varying bit-widths. We re-\nport results with (a) DeiT-Tiny and (b) DeiT-Small on Im-\nageNet (Krizhevsky, Sutskever, and Hinton 2012), respec-\ntively. Here 8-bit DeiT is quantized with PTQ method (Lin\net al. 2022) and 2/3/4 bit DeiT is trained with QAT\nmethod (Li et al. 2022). The binarized ViT is conducted with\nthe baseline method Bi-Real Net (Liu et al. 2018).\ndeploy models on platforms with short resource supplies.\nTherefore, both academia and industry call for an ultimate\ncompression of these large models, and the past years have\nwitnessed some promising techniques such as network prun-\ning (Yang et al. 2021; Chen et al. 2023), low-rank decompo-\nsition (Denil et al. 2013), knowledge distillation (Hao et al.\n2021; Xu et al. 2022b; Li et al. 2023c), and quantization (Li\net al. 2022; Xu et al. 2023a; Li et al. 2023a).\nNetwork quantization, which represents weights and ac-\ntivations in a low-bit format, has got great earnestness of\nmany researchers for its reduced memory access costs and\nincreased compute efficiency as well as performance bene-\nfit. Using the lower-bit quantized data, in particular to the\nextreme 1-bit case, requires less data movement, both on-\nchip and off-chip, and therefore reduces memory bandwidth\nand saves significant energy. Existing documentary records\nobserve 32√ó less network size and 58√ó speedups beneficial\nfrom xnor and bit-count logics for 1-bit networks (Raste-\ngari et al. 2016). Earlier attempts (Liu et al. 2021b; Lin\net al. 2022) apply post-training quantization (PTQ) (Banner,\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3243\nNahshan, and Soudry 2019; Zhong et al. 2022) directly to\nViTs without data-driven fine-tuning, causing sub-optimal\nperformance, in particular to impotent 1-bit ViTs. There-\nfore, by quantizing while training, quantization-aware train-\ning (QAT) methods are more congenial to 1-bit ViTs. Ex-\ntensive empirical studies (Liu et al. 2020; Xu et al. 2022a;\nQin et al. 2022; Xu et al. 2023b) have well demonstrated the\nefficacy of QAT methods in 1-bit convolutional neural net-\nworks (CNNs) or BERTs, however, the application to 1-bit\nViTs remains not to be fully explored so far.\nIn this paper, we first build a fully-binarized ViT baseline,\na straightforward solution constructed upon popular bina-\nrized QAT method of Bi-Real Net (Liu et al. 2018). Through\nan empirical study of this baseline, we observe significant\nperformance drops on the ImageNet dataset (Krizhevsky,\nSutskever, and Hinton 2012), as shown in Fig. 1. For in-\nstance, extending Bi-Real Net to binarize DeiT-Tiny (Tou-\nvron et al. 2021) incurs a tremendous performance gap of\n52.6% in the Top-1 accuracy compared to the 2-bit quantized\ncounterpart. Similar performance drops occur in DeiT-Small\nas well. Delving into a deeper analysis, we find that the in-\ncompatibility of existing QAT methods mainly stems from\nthe binarized self-attention module in ViTs, where a simple\napplication of existing binarization methods (Liu et al. 2018)\nleads to severe attention distortion, as plotted in Fig. 2 (a)\nand Fig. 2 (b), especially in the diagonal scores of the map\nwhich are supposed to be the most attentive.\nIn this paper we dig deeper into this attention distor-\ntion problem. Through empirical analysis, we find that this\nphenomenon is mainly caused by gradient vanishing due\nto the straight-through-estimator (STE) (Bengio, L ¬¥eonard,\nand Courville 2013) and non-scaled binarization in self-\nattention. Meanwhile, a simple distillation utilizing distilla-\ntion token in DeiT (Touvron et al. 2021) and KL-divergence\nin ReActNet (Liu et al. 2020) is ineffective in dismissing\nthe ranking disorder, since it neglects the relative order of\nthe attention map between the binarized ViTs and their real-\nvalued counterpart. To address the aforementioned issues,\na fully-binarized ViT (Bi-ViT) is developed by reactivating\nthe vanished gradients through a learnable scaling factor in\nself-attention and a ranking-aware distillation to further ef-\nfectively rectify the disordered ranking of attention (see the\noverview in Fig. 3). In addition, we also provide both empir-\nical and theoretical analysis about how our method can rec-\ntify the distorted attention and thus promote the optimization\nof Bi-ViT. The contributions of our work are summarized as:\n‚Ä¢ We identify the bottleneck of a fully-binarized ViT\nthrough empirical analyses and formulate the problem\nin a theoretical perspective. Based on these, we intro-\nduce learnable head-wise scaling factor into binarized\nself-attention to reactivate the vanished gradients.\n‚Ä¢ We develop a ranking-aware distillation scheme to elim-\ninate attention distortion. Our distillation method fully\nutilizes the ranking-aware knowledge from the real-\nvalued teacher to promote the optimization of Bi-ViT.\n‚Ä¢ Our Bi-ViT is the first promising way to push the limit\nof ViT quantization to the fully-binarized version. Ex-\ntensive experiments on the ImageNet benchmark demon-\n(a) Bi-Real Net\n+ Distill Token\n(c) Bi-ViT (d) Real-valued\nHead 0Head 1Head 2\n(b) Bi-Real Net\n+ KL divergence\nFigure 2: Visualization of the attention map before softmax\nin the first block of DeiT-Tiny (Touvron et al. 2021) on Im-\nageNet (Krizhevsky, Sutskever, and Hinton 2012). From the\nleft to right, is the baseline method (Liu et al. 2018), previ-\nous binarization method (Xu et al. 2022a), our Bi-ViT and\nreal-valued counterpart.\nstrate that Bi-ViT surpasses both the baseline and prior\nbinarized methods by a significant margin, achieving a\nremarkable acceleration rate of up to 61.5√ó.\nRelated Work\nVision Transformer. Unlike traditional CNN-based mod-\nels, ViTs are capable of capturing long-range visual rela-\ntionships through the self-attention mechanism, and offer\na more generalizable paradigm without inductive bias spe-\ncific to images. The starting ViT (Dosovitskiy et al. 2020)\nviews an image as a sequence of 16 √ó 16 patches and\nuses a unique class token to predict the classification, yield-\ning promising results. Subsequently, many works, such as\nDeiT (Touvron et al. 2021) and PVT (Wang et al. 2021),\nhave improved upon ViT, making it more efficient and appli-\ncable to downstream tasks. However, these high-performing\nViTs have also accompanied with a significant number of\nparameters and high computational overhead, limiting their\nwidespread applications. Thus, designing smaller and faster\nViTs has become a new trend. DynamicViT (Rao et al. 2021)\nproposes a dynamic token sparsification framework to pro-\ngressively and dynamically prune redundant tokens, achiev-\ning a competitive complexity and accuracy trade-off. Evo-\nViT (Xu et al. 2022c) proposes a slow-fast updating mecha-\nnism that ensures information flow and spatial structure, re-\nducing both the training and inference complexity. While the\naforementioned works focus on efficient model design, this\npaper aims to boost compression and acceleration through\nbinarization.\nNetwork Binarization. Network binarization is a technique\noriginally proposed to train convolutional neural networks\n(CNNs) with binary weights. BinaryConnect (Courbariaux,\nBengio, and David 2015) is the precursor to BinaryNet,\nwhere the parameters are binary while the activations re-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3244\nquery ùêö! key ùêö\" value\tùêö#\nùêõ$!\nAttention score ùêÄùêõ%\nself-attention\nadd & norm\nadd & normBi-MLP\nBi-FC\nLearnable ScaleBinarize\nBi-FC Bi-FC\nBinarize\nInput ùêóArchitecture of binarized block in Bi-ViTùêóùêó ùêó\nùêõ$\" ùêõ$#softmax(ùêõ$!‚ãÖùêõ$\"&)\nRanking-aware Distillation in MHSA\nReal-valued ùêÄùíØ\nBinarized ùêÄùíÆ\n ‚Ñí)$*\"Learnable ScaleBinarizeLearnable ScaleBinarize\nFigure 3: Overview of the proposed Bi-ViT framework. We introduce the learnable scaling factor in an architecture perspective\nand a ranking-aware distillation scheme incorporated in the optimization process. From left to right, we respectively show the\ndetailed architecture of single block in Bi-ViT and the distillation framework of Bi-ViT.\nmain in full-precision states. XNOR-Net (Rastegari et al.\n2016) was introduced to improve convolution efficiency by\nbinarizing the weights and inputs of convolution kernels. Bi-\nReal Net (Liu et al. 2018) explores a new variant of resid-\nual structure to preserve the information of real activations\nbefore the sign function, with a tight approximation to the\nderivative of the non-differentiable sign function. ReAct-\nNet (Liu et al. 2020) replaces the conventional PReLU and\nthe sign function of the BNNs with RPReLU and RSign\nwith a learnable threshold, thus improving the performance\nof BNNs. RBONN (Xu et al. 2022a) introduces a recurrent\nbilinear optimization to address the asynchronous conver-\ngence problem for BNNs, which further improves the perfor-\nmance of BNNs. DCP-NAS (Li et al. 2023b) proposes an ar-\nchitecture with better performance on binarized format than\nreal-valued counterpart. These techniques improve the effi-\nciency and accuracy of binary neural networks (BNNs) and\nallow them to be applied in practical applications. Majori-\nties of these techniques consider non-scaled binarization in\nactivations, which is beneficial to conventional CNNs while\ncausing gradient mismatch issue for the pecularity of self-\nattention mechanism in ViTs.\nBackground\nMulti-Head Self-Attention and Binarization\nFor a multi-head self-attention (MHSA) module, we de-\nnote its query, key, and value set as {a{q,k,v} ‚àà Rh√óN√ód},\nwhere h denotes head number, N and d represent the patch\nand channel numbers of each head. Specifically, N =\n(Win//WP\nin) √ó (Hin//HP\nin) where Win and Hin are the\nwidth and height of the feature, WP\nin, HP\nin are the width\nand height of patch maps respectively. Then, the attention\nscore A and MHSA module output aout are computed as\nfollows (Vaswani et al. 2017):\nA = softmax[(aq ¬∑ a‚ä§\nk )/\n‚àö\nd],\naout = A ¬∑ a‚ä§\nv ,\n(1)\nwhere softmax(¬∑) represents the softmax operation. Intend-\ning to represent query, key, value and attention score, i.e.,\naq, ak, av and A, in a 1-bit format, Eq. (1) changes into:\nA = softmax[(baq ¬∑ b‚ä§\nak )/\n‚àö\nd],\naout = bA ¬∑ b‚ä§\nav .\n(2)\nWe follow the common network binarization meth-\nods (Rastegari et al. 2016) that use the sign function\nb¬∑ = sign(¬∑) in the binary forward pass, and STE (Ben-\ngio, L ¬¥eonard, and Courville 2013) ‚àÇb¬∑\n‚àÇ¬∑ = 1|¬∑|‚â§1 to com-\npute the gradient for sign function in its backward pass.\nWe omit the non-linear function here for simplicity. For\nall the projection and linear layers in binarized ViTs, we\nconduct binarization following (Qin et al. 2022; Liu et al.\n2018) as aout = bain ¬∑ (Œ±w ‚ó¶ bw)‚ä§ = Œ±w ‚ó¶ (bain ¬∑ b‚ä§\nw)\nwhere Œ±w = {Œ±1\nw, Œ±2\nw, ..., Œ±Cout\nw } ‚ààRCout\n+ is known as the\nchannel-wise scaling factor vector (Rastegari et al. 2016)\nand ‚ó¶ represents channel-wise multiplication. The matrix\nmultiplication process, i.e., bain ¬∑ b‚ä§\nw, can be executed by\nthe efficient XNOR and Bit-count instructions on edge de-\nvices.\nBottleneck of Fully-Binarized ViTs\nThe high-performing ViTs are built on premise of trans-\nformer‚Äôs supreme ability to model the long-range relation-\nships thanks to the attention mechanism within the MHSA\nmodule. Unfortunately, a binarized version of weights and\ninputs significantly weakens the representation ability. In ad-\ndition, the sign function and clamp operation also damage\nthe optimization of backpropagation. To be more evident, we\nperform quantitative ablative experiments where we replace\nweights or activations in each module of the real-valued\nDeiT-Tiny (Touvron et al. 2021) with a binarized one and\nreport the resulting Top-1 accuracy drop on the ImageNet\ndataset (Krizhevsky, Sutskever, and Hinton 2012) after a to-\ntal of 50 training epochs. Fig. 4 reports the results and we go\non a deeper analysis below.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3245\nMHSAw32a32MHSAw1a32MLPw32a32MLPw1a32w1a1\nMLPw1a1\nMHSAw1a1\nLinearw1a32Linearw1a1Linearw1a32Linearw1a1\nSelf-Attentiona32\nSelf-Attentiona1\nw/ MLP w1a1\n48.826.36.34.43.4\n37.626.27.63.4(a) (b)\nTop-1 Acc. (%)Top-1 Acc. (%)\nFigure 4: Performance of fully-binarized DeiT-Tiny on Im-\nageNet (Krizhevsky, Sutskever, and Hinton 2012) with dif-\nferent binarized/real-valued settings.\nModule Degradation . By gradually replacing the multi-\nlayer perceptron (MLP) and MHSA modules with real-\nvalued weights or activations, we have discovered that main-\ntaining the MLP as ‚Äú w1a1‚Äù (all weights and activations in\nthe MLP are binarized) still results in satisfactory perfor-\nmance. For instance, keeping MLP as ‚Äúw1a1‚Äù while keeping\nMHSA as ‚Äú w1a32‚Äù obtains 26.3% Top-1 accuracy, which\nmight be acceptable comparing to the 55.2% of real-valued\nDeiT-Tiny when taking into consideration 47.3 √ó accelera-\ntion rates. On the contrast, when maintaining MHSA mod-\nule as ‚Äúw1a1‚Äù, we observe a significant drop in performance.\nTo be more specific, even when the MLP was maintained\nas ‚Äúw32a32‚Äù, we still observe a significant 50.8% decrease\nin Top-1 accuracy (from 55.2% to 4.4%). This result in-\ndicates that using binarized weights and activations in the\nMHSA module can have a substantial negative impact on\nthe model‚Äôs performance, even when other parts retain in\nreal-valued states.\nOperation Degradation. To better understand the impact of\nfully-binarized ViT‚Äôs performance, we conduct further anal-\nyses by examining the operations within the MHSA module.\nSpecifically, when we maintain the self-attention activations\nin Eq. (1) as real-valued (‚Äú a32‚Äù), we observe only a rela-\ntively small decrease in performance from 48.8% to 37.6%.\nHowever, when the self-attention activations in Eq. (2) are\nbinarized, significant drops in accuracy occur from 48.8%\nto 7.6%. This finding highlights the importance of the self-\nattention process within the MHSA module and suggests\nmore efforts to mitigate the negative impact of binarization\non the MHSA module.\nOur Bi-ViT\nIn this section, we propose to dismiss the affect of gradient\nmismatch mentioned in Sec. 4.1 from perspectives of gradi-\nent approximation in Sec. 4.2 and intermediate distillation in\nSec. 4.3.\nGradient Mismatch in Self-Attention\nWith conclusion from the experimental results in Sec. 3.2\nthat self-attention process, i.e., Eq. (2), is the most critical\npart causing the performance drops. We attempt to analyze\nùúïùêõ!!ùúïùêö\"\nùêö#$%\"ùúïùêÄùúïùêö\"=ùúïùêÄùúïùê©‚ãÖùúïùê©ùúïùêõ!!‚ãÖùúïùêõ!!ùúïùêö\"= 0significant\n(a) Curve of &ùêÄ&ùê©, regarding ùê© (b) Gradient vanishing of &ùêÄ&ùêö!\n-1 10\n1\nùúïùêÄùúïùê©\nùê©\nFigure 5: Gradient mismatch between Eq. (5) and Eq. (7).\nthe underlying reasons for this phenomenon from an opti-\nmization perspective. For simplicity, we derive the gradient\nmismatch in aq as an example, and the analysis can be ap-\nplicable to explain ak as well. We first represent the features\nbefore softmax(¬∑) in Eq. (2) as:\np = (baq ¬∑ b‚ä§\nak .)/\n‚àö\nd. (3)\nThe gradient of ahi,n,c\nq w.r.t.A is formulated as:\n‚àÇA\n‚àÇahi,n,c\nq\n= ‚àÇA\n‚àÇphi,n,n‚Ä≤ ¬∑ ‚àÇphi,n,n‚Ä≤\n‚àÇbhi,n,c\naq\n¬∑ ‚àÇbhi,n,c\naq\n‚àÇahi,n,c\nq\n, (4)\nwhere hi ‚àà Rh, n & n‚Ä≤ ‚àà RN , c ‚àà Rd and the gradient of\nak is likewise. The explicit form of the first item ‚àÇA\n‚àÇphi,n,n‚Ä≤\nin Eq. (4) is:\n‚àÇA\n‚àÇphi,n,n‚Ä≤\n= ‚àÇ softmax(phi,n,n‚Ä≤)\n‚àÇphi,n,n‚Ä≤\n= Ahi,n,n‚Ä≤ ‚äó (1 ‚àí Ahi,n,n‚Ä≤),\n(5)\nwhere ‚äó denotes Hadamard product. And the second item is\nformulated as:\n‚àÇphi,n,n‚Ä≤\n‚àÇbhi,n,c\naq\n= ‚àÇbhi,n,c\naq ¬∑ b‚ä§hi,c,n‚Ä≤\nak\n‚àÇbhi,n,c\naq\n= b‚ä§hi,c,n‚Ä≤\nak /\n‚àö\nd,\n(6)\nresult of which is therefore correlated with bak . The third\nitem is solved through STE (Bengio, L¬¥eonard, and Courville\n2013) as:\n‚àÇbhi,n,c\naq\n‚àÇahi,n,c\nq\n= 1|a\nhi,n,c\nq |‚â§1. (7)\nCombing Eq. (5)‚àíEq. (7), we have the final gradient form\nin fully-binarized ViTs as:\n‚àÇA\n‚àÇahi,n,c\nq\n= ‚àÇA\n‚àÇphi,n,n‚Ä≤ ¬∑ ‚àÇphi,n,n‚Ä≤\n‚àÇbhi,n,c\naq\n¬∑ ‚àÇbhi,n,c\naq\n‚àÇahi,n,c\nq\n= Ahi,n,n‚Ä≤(1 ‚àí Ahi,n,n‚Ä≤) ¬∑ bhi,c,n‚Ä≤\nak ¬∑ 1|ahi,n,c\nq |‚â§1/\n‚àö\nd.\n(8)\nConsidering bhi,n,:\naq = [1 , ¬∑¬∑¬∑ , 1] and ¬∑bhi,n‚Ä≤,:\nak =\n[1, ¬∑¬∑¬∑ , 1] as the extreme condition, bhi,n,:\naq ¬∑ b‚ä§hi,:,n‚Ä≤\nak = d.\nTherefore, a specific element in baq ¬∑b‚ä§\nak is ‚àà {‚àíd, ¬∑¬∑¬∑ , d}.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3246\nWe plot the curve of a specific element in the first item be-\ntween [‚àí64, 64] in Fig. 5 (a) as d = 64 in DeiT-Tiny (Tou-\nvron et al. 2021). We observe ‚àÇA\n‚àÇphi,n,n‚Ä≤ sharply magni-\nfied when phi,n,n‚Ä≤ increases. As shown in Fig. 5 (b), when\nphi,n,n‚Ä≤ has a large magnitude, |aq| > 1 and\n‚àÇb\nhI,n,c\naq\n‚àÇa\nhi,n,c\nq\n=\n0 . Thus the multiplication of these two items leads to\n‚àÇA\n‚àÇa\nhi,n,c\nq\n= 0, likewise for ak. Therefore we formulate the\ngradient mismatch phenomenon in the aforementioned the-\noretical analysis. And such gradient mismatch leads to dis-\ntorted gradient in the optimization of aq & ak and therefore\ndegrades performance of fully-binarized ViTs.\nLearnable Head-wise Scaling Factor\nAs one of the solution to the above mentioned problem,\nwe propose a head-wise scaling factor binarization scheme\nfor the self-attention process, where the scaling factors are\nlearned during training to first modify the gradient clip range\nin Fig. 5(b). Eq. (2) is changed into:\nÀúA = softmax(Àúp),\nÀúp = (Œ±q ‚äó Œ±k) ‚ó¶ (baq ¬∑ b‚ä§\nak )/\n‚àö\nd\n= Œ±q;k ‚ó¶ (baq ¬∑ b‚ä§\nak )/\n‚àö\nd,\n(9)\nand\nÀúaout = (Œ±A ‚ó¶ bA) ¬∑ (Œ±v ‚ó¶ bav )‚ä§\n= (Œ±A ‚äó Œ±v) ‚ó¶ (bA ¬∑ b‚ä§\nav )\n= Œ±A;v ‚ó¶ (bA ¬∑ b‚ä§\nav ),\n(10)\nwhere ba¬∑ = sign(a¬∑\nŒ±¬∑\n), Œ±q, Œ±k, Œ±v and Œ±A are the head-\nwise learnable scaling factors in binarized MHSA, where\nŒ±{q,k,v,A} = {Œ±1\n{q,k,v,A}, Œ±2\n{q,k,v,A}, ¬∑¬∑¬∑ , Œ±h\n{q,k,v,A}} ‚àà\nRh\n+. The second rows in Eq. (9) & Eq. (10) are established\nsince the scaling factors are aligned with the head dimen-\nsion, which is independent with the matrix multiplication\noperation. Thus, Œ±q;k = {Œ±1\nq;k, Œ±2\nq;k, ¬∑¬∑¬∑ , Œ±h\nq;k} ‚ààRh\n+ and\nŒ±A;v = {Œ±1\nA;v, Œ±2\nA;v, ¬∑¬∑¬∑ , Œ±h\nA;v} ‚ààRh\n+.\nConsequently, the gradient ‚àÇ ÀúA\n‚àÇa:,n,c\nq\nin Eq. (8) is further for-\nmulated in our Bi-ViT as:\n‚àÇ ÀúA\n‚àÇahi,n,c\nq\n= ÀúAhi,n,n‚Ä≤\n(1 ‚àí ÀúAhi,n,n‚Ä≤\n)| {z }\n‚àÇ ÀúA\n‚àÇphi,n,n‚Ä≤\n¬∑Œ±hi\nq;k ‚ó¶ bhi,c,n‚Ä≤\nak\n| {z }\n‚àÇphi,n,n‚Ä≤\n‚àÇbhi,n,c\naq\n¬∑1|ahi,n,c\nq |‚â§Œ±q\n| {z }\n‚àÇbhi,n,c\naq\n‚àÇahi,n,c\nq\n.\n(11)\nSince softmax(.) and ‚ó¶ are aligned with different\ndimensions, the value of Eq. (5) remains unchanged\n(softmax(p) = softmax(Œ±q;k ‚ó¶ p)). As can be seen, the\nthreshold of gradient clip in Eq. (7) changes from 1 into Œ±q,\nwhich means that we can surpass the occurance of gradient\nmismatch by modifying the value of Œ±q. Note that the scal-\ning factor (Œ±q) is to imitate the magnitude of the latent ac-\ntivations. When Àúp has a large magnitude, i.e., in the circled\npart of Fig. 5 (a), Œ±q also tends to be larger and ahi,n,c\nq lo-\ncates in the field that\n‚àÇb\nhi,n,c\naq\n‚àÇa\nhi,n,c\nq\n> 0. Thus the vanishing gradi-\nents are reactivated through the introduced learnable scaling\nfactor.\nRanking-aware Distillation for Bi-ViT\nFig. 2 illustrates a significant difference in the attention\nmap‚Äôs relative order between Bi-RealNet (a) and its real-\nvalued counterpart (c). This difference could result in a no-\ntable decrease in performance. To address this issue during\nbinarized training, a ranking-aware distillation in a teacher-\nstudent framework is introduced:\nLranking =\nLX\nl=1\n‚à•œà(AT ) ‚àí œà(AS)‚à•2, (12)\nwhere AT and AS represents the attention scores from the\nreal-valued teacher and binarized student. œà(¬∑) denotes the\nfunction for obtaining the ranking, i.e., relative order of an\nattention score, which is formulated as:\nœà(A:,n,:) =\n(\nA:,n,: ‚àí A:,n‚àí1,:, if 0 < n‚â§ N ‚àí 1\nA:,0,: ‚àí A:,N‚àí1,:, otherwise .\n(13)\nDetailed relative order computation can be seen in the right\npart of Fig. 3. We implement our Bi-ViT under the teacher-\nstudent framework (Touvron et al. 2021), thus the final ob-\njective of our method is formulated as:\nL = Ldist + ŒªLranking, (14)\nwhere Œª is a hyper-parameter to balance these two loss func-\ntions.\nExperiments\nIn this section, we evaluate the performance of the proposed\nBi-ViT model for image classification task using popular\nDeiT (Touvron et al. 2021) & Swin (Liu et al. 2021a) back-\nbones and object detection task using Mask R-CNN (He\net al. 2017) & Cascade (Cai and Vasconcelos 2018) Mask\nR-CNN with Swin-Tiny (Liu et al. 2021a) backbone. To the\nbest of our knowledge, there is no publicly available source\ncodebase on fully-binarized ViTs at this point, so we re-\nimplement the baseline i.e., Bi-Real Net (Liu et al. 2018)\nmethods.\nDatasets and Implementation Details\nDatasets. The experiments are conducted on the ImageNet\nILSVRC12 dataset (Krizhevsky, Sutskever, and Hinton\n2012) for image classification task. The ImageNet dataset is\nchallenging due to its large scale and greater diversity. There\nare 1000 classes and 1.2 million training images, and 50k\nvalidation images in it. In our experiments, we use the clas-\nsic data augmentation method described in (Touvron et al.\n2021).\nExperimental settings. In our experiments, we initialize\nthe weights of binarized model with the pretrained real-\nvalued model. The binarized model is trained for 300 epochs\nwith batch-size 512 and the base learning rate 5e‚àí4 with-\nout warm-up scheme. For all the experiments, we apply\nLAMB (You et al. 2020) optimizer with weight decay set as\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3247\nFigure 6: Effect of hyper-parameter Œª on Ima-\ngeNet (Krizhevsky, Sutskever, and Hinton 2012).\n0, following DeiT III (Touvron, Cord, and J¬¥egou 2022). Note\nthat we keep the patch embedding (first) layer and the clas-\nsification (last) layer as real-valued, following (Esser et al.\n2019).\nBackbone. We evaluate our binarized method on two pop-\nular vision transformer networks: DeiT (Touvron et al.\n2021) and Swin Transformer (Liu et al. 2021a). The DeiT-\nTiny, DeiT-Small, DeiT-Base, Swin-Tiny and Swin-Small\nare adopted as the backbone models, whose Top-1 accuracy\non ImageNet dataset are 72.2%, 79.9%, 81.8%, 81.2%, and\n83.2% respectively. For a fair comparison, we utilize the of-\nficial implementation of DeiT and Swin Transformer.\nAblation Study\nHyper-parameter Selection. We Œª of Eq. (14) in this\npart, with experiments conducted on ImageNet (Krizhevsky,\nSutskever, and Hinton 2012) dataset. We show the model\nperformance (Top-1 accuracy) with different setups of\nhyper-parameter Œª in Fig. 6, in which the performances in-\ncrease first and then decrease with the uplift of Œª from left\nto right. Since Œª controls the importance of Lranking, we\nshow that the vanilla baseline (Œª = 0) performs worse than\nany versions with Ranking-aware Distillation loss (Œª > 0),\nshowing the proposed distillation scheme is necessary. With\nthe varying value of Œª, we find Œª = 5 boost the perfor-\nmance of our Bi-ViT, achieving 28.7%, 40.9% and 50.7%\nTop-1 accuracy on ImageNet (Krizhevsky, Sutskever, and\nHinton 2012) with DeiT-Tiny, DeiT-Small and Swin-Tiny\nbackbone, respectively.\nEffectiveness of components. We conduct the ablative ex-\nperiments regarding the proposed components on DeiT-Tiny\nnetwork. Firstly, we compose the baseline network using\nthe binarization method following Bi-Real Net (Liu et al.\n2018). As shown in the third row of Tab. ??, the baseline\nnetworks only obtains 6.6% Top-1 accuracy, which is far\nfrom satisfactory. With the introduction of our first novelty,\ni.e., learnable scaling factor (LSF), the baseline network is\nboosted by 17.8%, achieving 24.4% Top-1 accuracy. We also\nobserve the other contribution Ranking-aware Disitllation\n(RD) singly promotes the baseline network by 5.9%, which\nis also significant on ImageNet dataset. By combining the\ntwo main contributions together, we get Bi-ViT, outperform-\ning the vanilla baseline by 22.1%.\nMethod #Bits Top-1 (%)\nReal-valued 32-32 72.1\nBaseline (Bi-Real Net) 1-1 6.6\n+ Learnable Scaling Factor (LSF) 1-1 24.4 +17.8\n+ Ranking-aware Distillation (RD) 1-1 12.5 +5.9\n+ LSF + RD (Bi-ViT) 1-1 28.7+22.1\nTable 1: Evaluating the components of Bi-ViT based on\nDeiT-Tiny (Touvron et al. 2021) backbone. ‚Äú#Bits‚Äù denotes\nthe bit-width of weights and activations.\nResults on Image Classification\nThe experimental results are shown in Tab. ??. We compare\nour method with 1-bit methods including BiBERT (Qin et al.\n2022), RBONN (Xu et al. 2022a), and Bi-Real Net (Liu et al.\n2018) based on the same frameworks for the task of im-\nage classification with the ImageNet dataset. We also report\nthe classification performance of the low-bit training-aware\nquantization method Q-ViT (Li et al. 2022) for further ref-\nerence. We use model size and OPs following (Liu et al.\n2018) in comparison to other bit-width models for further\nreference. We firstly evaluate the proposed method on DeiT\nmodels. For DeiT-Tiny backbone, compared with other bi-\nnary methods, our Bi-ViT achieves significant performance\nimprovements. For example, our Bi-ViT surpasses the base-\nline Bi-Real Net (Liu et al. 2018) by 22.1% Top-1 accuracy,\nwhich is significant and meaningful for real-world applica-\ntions. And it is worth noting that the proposed 1-bit model\nsignificantly compresses the DeiT-Tiny by 61.5√ó on OPs.\nThe proposed method also boosts the performance of base-\nline by 21.7% with the same architecture and bit-width us-\ning DeiT-Small bacobone, a significant improvement on the\nImageNet dataset. For larger DeiT-B, as shown in Tab. ??,\nthe performance of the proposed method outperforms the\nBi-Real Net by 20.8%, a large margin. Also note that the\nproposed 1-bit model significantly compresses the DeiT-B\nby 60.2√ó on OPs and 28.6√ó on model size.\nAlso, our method obtains convincing results on Swin-\ntransformers. As shown in Tab. ??, the performance of the\nproposed method with Swin-Tiny outperforms the baseline\nmethod by 21.4%, a large margin. For larger Swin-Small, the\nperformance of the proposed method outperforms the 1-bit\nbaseline by 21.5%. Also note that our method theoritically\naccelerates the network by 58.3√ó , which demonstrates the\neffectiveness and efficiency of our Bi-ViT.\nConclusion\nIn this paper, we present Bi-ViT, an improved version of\nfully-binarized ViTs that offers a high compression ratio and\nacceptable performance. Initially, we establish a empirical\nframework for fully-binarized ViT and analyze the bottle-\nnecks of the baseline. Our empirical analysis shows that at-\ntention distortion in MHSA is the primary cause of the sig-\nnificant drop in ViT binarization, which results from gra-\ndient vanishing and ranking disorder. To address these is-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3248\nNetwork Method #Bits Size (MB) OPs(108) Top-1(%) Top-5(%)\nReal-valued 32-32 22.8 12.3 72.2 91.1\n4-4 3.0 1.6 74.3 91.7\n3-3 2.3 0.8 71.5 91.2Q-ViT (Li et al. 2022) 2-2 1.7 0.4 59.0 81.8\nBiBERT (Qin et al. 2022)\n1.0 0.2\n5.9 16.0\nRBONN (Xu et al. 2022a) 6.3 16.9\nBi-Real Net (Liu et al. 2018) 6.6 17.1\nDeiT-Tiny\nBi-ViT\n1-1\n28.7+22.1 51.7+34.6\nReal-valued 32-32 88.2 45.5 79.9 95.0\n4-4 11.4 5.8 80.9 94.9\n3-3 8.7 3.0 79.0 94.2Q-ViT (Li et al. 2022) 2-2 6.0 1.5 72.1 90.3\nBiBERT (Qin et al. 2022)\n3.4 0.8\n17.4 29.7\nRBONN (Xu et al. 2022a) 18.5 30.0\nBi-Real Net (Liu et al. 2018) 19.2 30.3\nDeiT-Small\nBi-ViT\n1-1\n40.9+21.7 65.0+34.7\nReal-valued 32-32 346.2 174.7 81.8 95.6\n4-4 44.1 22.0 83.0 96.1\n3-3 33.4 11.1 81.0 95.1Q-ViT (Li et al. 2022) 2-2 22.7 5.7 74.2 92.2\nBiBERT (Qin et al. 2022)\n12.1 2.9\n24.5 36.3\nRBONN (Xu et al. 2022a) 26.1 38.6\nBi-Real Net (Liu et al. 2018) 26.5 38.8\nDeiT-Base\nBi-ViT\n1-1\n47.3+20.8 72.8+34.0\nReal-valued 32-32 114.2 44.9 81.2 95.5\n4-4 14.6 5.8 82.5 97.3\n3-3 11.2 3.0 80.9 96.1Q-ViT (Li et al. 2022) 2-2 10.0 1.6 74.7 92.5\nBiBERT (Qin et al. 2022)\n4.2 0.8\n34.0 46.9\nRBONN (Xu et al. 2022a) 33.8 46.7\nBi-Real Net (Liu et al. 2018) 34.1 46.9\nSwin-Tiny\nBi-ViT\n1-1\n55.5+21.4 79.4+32.5\nReal-valued 32-32 199.8 87.5 83.2 96.2\n4-4 25.3 11.1 84.4 98.3\n3-3 19.2 5.6 82.7 97.5Q-ViT (Li et al. 2022) 2-2 13.0 2.9 76.9 94.9\nBiBERT (Qin et al. 2022)\n6.9 1.5\n39.4 53.0\nRBONN (Xu et al. 2022a) 39.0 52.7\nBi-Real Net (Liu et al. 2018) 39.2 52.8\nSwin-Small\nBi-ViT\n1-1\n60.7+21.5 83.9+31.1\nTable 2: Experiments with DeiT (Touvron et al. 2021) and Swin (Liu et al. 2021a) on ImageNet (Krizhevsky, Sutskever,\nand Hinton 2012). ‚Äú#Bits‚Äù denotes the bit-width of weights and activations. We report the Top-1 (%) and Top-5(%) accuracy\nperformances. The bold denotes the best result with binarized weights and activations.\nsues, we introduce a learnable scaling factor that reactivates\nvanished gradients, which we illustrate through both theo-\nretical and experimental analysis. Additionally, we propose\nranking-aware distillation for Bi-ViT, which rectifies dis-\nordered ranking in a teacher-student framework. Our work\nprovides a comprehensive analysis and effective solutions\nfor the crucial issues in ViT full binarization, paving the way\nfor the extreme compression of ViT.\nAcknowledgements\nThis research was supported by Zhejiang Provincial Natural\nScience Foundation of China under Grant No. D24F020011,\nBeijing Natural Science Foundation L223024, National Nat-\nural Science Foundation of China under Grant 62076016\nand under Grant 61827901, Foundation of China Energy\nProject GJNY-19-90. The work was also supported by the\nNational Key Research and Development Program of China\n(Grant No. 2023YFC3300029) and ‚ÄúOne Thousand Plan‚Äù\nprojects in Jiangxi Province Jxsg2023102268 and ATR key\nlaboratory grant 220402. 232-CXCY-A01-08-06-01.\nReferences\nBanner, R.; Nahshan, Y .; and Soudry, D. 2019. Post train-\ning 4-bit quantization of convolutional networks for rapid-\ndeployment. In Proc. of NeurIPS, 7950‚Äì7958.\nBengio, Y .; L¬¥eonard, N.; and Courville, A. 2013. Estimat-\ning or propagating gradients through stochastic neurons for\nconditional computation. arXiv preprint arXiv:1308.3432.\nCai, Z.; and Vasconcelos, N. 2018. Cascade r-cnn: Delving\ninto high quality object detection. In Proc. of CVPR, 6154‚Äì\n6162.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In Proc. of ECCV, 213‚Äì229.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3249\nChen, M.; Lin, M.; Li, K.; Shen, Y .; Wu, Y .; Chao, F.; and Ji,\nR. 2023. Cf-vit: A general coarse-to-fine method for vision\ntransformer. In Proc. of AAAI, 1‚Äì13.\nCourbariaux, M.; Bengio, Y .; and David, J.-P. 2015. Bi-\nnaryconnect: Training deep neural networks with binary\nweights during propagations. In Proc. of NeurIPS, 3123‚Äì\n3131.\nDenil, M.; Shakibi, B.; Dinh, L.; Ranzato, M.; and De Fre-\nitas, N. 2013. Predicting parameters in deep learning. In\nProc. of NeurIPS, 2148‚Äì2156.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. InProc.\nof ICLR, 1‚Äì22.\nEsser, S. K.; McKinstry, J. L.; Bablani, D.; Appuswamy, R.;\nand Modha, D. S. 2019. Learned step size quantization.\narXiv preprint arXiv:1902.08153.\nHao, Z.; Guo, J.; Jia, D.; Han, K.; Tang, Y .; Zhang, C.; Hu,\nH.; and Wang, Y . 2021. Learning Efficient Vision Trans-\nformers via Fine-Grained Manifold Distillation. In Proc. of\nNeurIPS, 1‚Äì11.\nHe, K.; Chen, X.; Xie, S.; Li, Y .; Doll¬¥ar, P.; and Girshick, R.\n2022. Masked autoencoders are scalable vision learners. In\nProc. of CVPR, 16000‚Äì16009.\nHe, K.; Gkioxari, G.; Doll¬¥ar, P.; and Girshick, R. 2017. Mask\nr-cnn. In Proc. of ICCV, 2961‚Äì2969.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nageNet Classification with Deep Convolutional Neural Net-\nworks. In Proc. of NeurIPS, 1097‚Äì1105.\nLi, Y .; Xu, S.; Cao, X.; Sun, X.; and Zhang, B. 2023a. Q-\nDM: An Efficient Low-bit Quantized Diffusion Model. In\nProc. of NeurIPS, 1‚Äì12.\nLi, Y .; Xu, S.; Cao, X.; Zhuo, L.; Zhang, B.; Wang, T.; and\nGuo, G. 2023b. DCP‚ÄìNAS: Discrepant Child‚ÄìParent Neural\nArchitecture Search for 1-bit CNNs. International Journal\nof Computer Vision, 131(11): 2793‚Äì2815.\nLi, Y .; Xu, S.; Lin, M.; Yin, J.; Zhang, B.; and Cao, X. 2023c.\nRepresentation Disparity-aware Distillation for 3D Object\nDetection. In Proc. of ICCV, 6715‚Äì6724.\nLi, Y .; Xu, S.; Zhang, B.; Cao, X.; Gao, P.; and Guo, G.\n2022. Q-ViT: Accurate and Fully Quantized Low-bit Vision\nTransformer. In Proc. of NeurIPS, 1‚Äì12.\nLin, Y .; Zhang, T.; Sun, P.; Li, Z.; and Zhou, S. 2022. FQ-\nViT: Fully Quantized Vision Transformer without Retrain-\ning. In Proc. of IJCAI, 1173‚Äì1179.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021a. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proc. of ICCV,\n10012‚Äì10022.\nLiu, Z.; Shen, Z.; Savvides, M.; and Cheng, K.-T. 2020. Re-\nActNet: Towards Precise Binary Neural Network with Gen-\neralized Activation Functions. In Proc. of ECCV, 143‚Äì159.\nLiu, Z.; Wang, Y .; Han, K.; Zhang, W.; Ma, S.; and Gao, W.\n2021b. Post-training quantization for vision transformer. In\nProc. of NeurIPS.\nLiu, Z.; Wu, B.; Luo, W.; Yang, X.; Liu, W.; and Cheng,\nK.-T. 2018. Bi-real net: Enhancing the performance of 1-\nbit cnns with improved representational capability and ad-\nvanced training algorithm. In Proc. of ECCV, 722‚Äì737.\nQin, H.; Ding, Y .; Zhang, M.; Yan, Q.; Liu, A.; Dang, Q.;\nLiu, Z.; and Liu, X. 2022. BiBERT: Accurate Fully Bina-\nrized BERT. In Proc. of ICLR, 1‚Äì24.\nRao, Y .; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh, C.-J.\n2021. Dynamicvit: Efficient vision transformers with dy-\nnamic token sparsification. Proc. of NeurIPS, 1‚Äì14.\nRastegari, M.; Ordonez, V .; Redmon, J.; and Farhadi, A.\n2016. Xnor-net: Imagenet classification using binary con-\nvolutional neural networks. In Proc. of ECCV, 525‚Äì542.\nTian, Y .; Xie, L.; Wang, Z.; Wei, L.; Zhang, X.; Jiao, J.;\nWang, Y .; Tian, Q.; and Ye, Q. 2022. Integrally Pre-\nTrained Transformer Pyramid Networks. arXiv preprint\narXiv:2211.12735.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J¬¥egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In Proc. of ICML,\n10347‚Äì10357.\nTouvron, H.; Cord, M.; and J ¬¥egou, H. 2022. Deit iii: Re-\nvenge of the vit. In Proc. of ECCV, 516‚Äì533.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In Proc. of NeurIPS, 1‚Äì11.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. In Proc. of ICCV, 568‚Äì578.\nXu, S.; Li, Y .; Lin, M.; Gao, P.; Guo, G.; L¬®u, J.; and Zhang,\nB. 2023a. Q-DETR: An Efficient Low-Bit Quantized Detec-\ntion Transformer. In Proc. of CVPR, 3842‚Äì3851.\nXu, S.; Li, Y .; Ma, T.; Lin, M.; Dong, H.; Zhang, B.; Gao, P.;\nand Lu, J. 2023b. Resilient binary neural network. In Proc.\nof AAAI, 10620‚Äì10628.\nXu, S.; Li, Y .; Wang, T.; Ma, T.; Zhang, B.; Gao, P.; Qiao, Y .;\nL¬®u, J.; and Guo, G. 2022a. Recurrent bilinear optimization\nfor binary neural networks. In Proc. of ECCV, 19‚Äì35.\nXu, S.; Li, Y .; Zeng, B.; Ma, T.; Zhang, B.; Cao, X.; Gao,\nP.; and Lu, J. 2022b. IDa-Det: An Information Discrepancy-\naware Distillation for 1-bit Detectors. In Proc. of ECCV,\n346‚Äì361.\nXu, Y .; Zhang, Z.; Zhang, M.; Sheng, K.; Li, K.; Dong, W.;\nZhang, L.; Xu, C.; and Sun, X. 2022c. Evo-vit: Slow-fast\ntoken evolution for dynamic vision transformer. In Proc. of\nAAAI, 2964‚Äì2972.\nYang, H.; Yin, H.; Molchanov, P.; Li, H.; and Kautz, J. 2021.\nNvit: Vision transformer compression and parameter redis-\ntribution. arXiv preprint arXiv:2110.04869.\nYou, Y .; Li, J.; Reddi, S.; Hseu, J.; Kumar, S.; Bhojanapalli,\nS.; Song, X.; Demmel, J.; Keutzer, K.; and Hsieh, C.-J. 2020.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3250\nLarge batch optimization for deep learning: Training bert in\n76 minutes. Proc. of ICLR, 1‚Äì37.\nZhong, Y .; Lin, M.; Chen, M.; Li, K.; Shen, Y .; Chao, F.; Wu,\nY .; and Ji, R. 2022. Fine-grained data distribution alignment\nfor post-training quantization. In Proc. of ECCV, 70‚Äì86.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3251",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.637694239616394
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.42635637521743774
    },
    {
      "name": "Computer science",
      "score": 0.41630133986473083
    },
    {
      "name": "Computer vision",
      "score": 0.32833153009414673
    },
    {
      "name": "Electrical engineering",
      "score": 0.28795120120048523
    },
    {
      "name": "Engineering",
      "score": 0.19597607851028442
    },
    {
      "name": "Voltage",
      "score": 0.0541815459728241
    }
  ]
}