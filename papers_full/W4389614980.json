{
    "title": "Protein language model-embedded geometric graphs power inter-protein contact prediction",
    "url": "https://openalex.org/W4389614980",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3127484366",
            "name": "Yunda Si",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2127731696",
            "name": "Chengfei Yan",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2076710572",
        "https://openalex.org/W2508425869",
        "https://openalex.org/W2130479394",
        "https://openalex.org/W2095157453",
        "https://openalex.org/W2957157683",
        "https://openalex.org/W4296032638",
        "https://openalex.org/W2153983456",
        "https://openalex.org/W3202105508",
        "https://openalex.org/W2152764495",
        "https://openalex.org/W3133793061",
        "https://openalex.org/W4309190357",
        "https://openalex.org/W2808950571",
        "https://openalex.org/W3185197558",
        "https://openalex.org/W2120836664",
        "https://openalex.org/W4223581484",
        "https://openalex.org/W6795866128",
        "https://openalex.org/W6782527804",
        "https://openalex.org/W2136799255",
        "https://openalex.org/W3157271872",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W2108211735",
        "https://openalex.org/W2944535254",
        "https://openalex.org/W3119075013",
        "https://openalex.org/W4310154745",
        "https://openalex.org/W3207244478",
        "https://openalex.org/W2107867854",
        "https://openalex.org/W2951690294",
        "https://openalex.org/W2918063357",
        "https://openalex.org/W3133458480",
        "https://openalex.org/W6791955017",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W4210646117",
        "https://openalex.org/W2114340287",
        "https://openalex.org/W3194835771",
        "https://openalex.org/W4282912737",
        "https://openalex.org/W4319826837",
        "https://openalex.org/W3199468887",
        "https://openalex.org/W2152495216",
        "https://openalex.org/W2953008890",
        "https://openalex.org/W2972411752",
        "https://openalex.org/W3084150557",
        "https://openalex.org/W2102461176",
        "https://openalex.org/W2593619857",
        "https://openalex.org/W2154019529",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2949867299",
        "https://openalex.org/W1979762151",
        "https://openalex.org/W6849104972",
        "https://openalex.org/W4386161992",
        "https://openalex.org/W3211403530",
        "https://openalex.org/W3133540795",
        "https://openalex.org/W2804651207",
        "https://openalex.org/W2161151688",
        "https://openalex.org/W2102245393",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4313413450"
    ],
    "abstract": "Accurate prediction of contacting residue pairs between interacting proteins is very useful for structural characterization of protein–protein interactions. Although significant improvement has been made in inter-protein contact prediction recently, there is still a large room for improving the prediction accuracy. Here we present a new deep learning method referred to as PLMGraph-Inter for inter-protein contact prediction. Specifically, we employ rotationally and translationally invariant geometric graphs obtained from structures of interacting proteins to integrate multiple protein language models, which are successively transformed by graph encoders formed by geometric vector perceptrons and residual networks formed by dimensional hybrid residual blocks to predict inter-protein contacts. Extensive evaluation on multiple test sets illustrates that PLMGraph-Inter outperforms five top inter-protein contact prediction methods, including DeepHomo, GLINTER, CDPred, DeepHomo2, and DRN-1D2D_Inter, by large margins. In addition, we also show that the prediction of PLMGraph-Inter can complement the result of AlphaFold-Multimer. Finally, we show leveraging the contacts predicted by PLMGraph-Inter as constraints for protein–protein docking can dramatically improve its performance for protein complex structure prediction.",
    "full_text": "Yunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 1 of 34\nComputational and Systems Biology\nProtein language model embedded\ngeometric graphs power inter-\nprotein contact prediction\nYunda Si, Chengfei Yan\nSchool of Physics, Huazhong University of Science and Technology, China\nhttps://en.wikipedia.org/wiki/Open_access\nCopyright information\nAbstract\nAccurate prediction of contacting residue pairs between interacting proteins is very useful\nfor structural characterization of protein-protein interactions (PPIs). Although significant\nimprovement has been made in inter-protein contact prediction recently, there is still large\nroom for improving the prediction accuracy. Here we present a new deep learning method\nreferred to as PLMGraph-Inter for inter-protein contact prediction. Specifically, we employ\nrotationally and translationally invariant geometric graphs obtained from structures of\ninteracting proteins to integrate multiple protein language models, which are successively\ntransformed by graph encoders formed by geometric vector perceptrons and residual\nnetworks formed by dimensional hybrid residual blocks to predict inter-protein contacts.\nExtensive evaluation on multiple test sets illustrates that PLMGraph-Inter outperforms five\ntop inter-protein contact prediction methods, including DeepHomo, GLINTER, CDPred,\nDeepHomo2 and DRN-1D2D_Inter by large margins. In addition, we also show that the\nprediction of PLMGraph-Inter can complement the result of AlphaFold-Multimer. Finally, we\nshow leveraging the contacts predicted by PLMGraph-Inter as constraints for protein-protein\ndocking can dramatically improve its performance for protein complex structure prediction.\neLife assessment\nThis study presents a useful deep learning-based inter-protein contact prediction\nmethod named PLMGraph-Inter which combines protein language models and\ngeometric graphs. The evidence supporting the claims of the authors is solid,\nalthough it could have information leakage between training and test sets, and\nalthough more emphasis should be given to predictions starting from unbound\nmonomer structures. The authors show that their approach may be useful in some\ncases where AlphaFold-Multimer performs poorly. This work will be of interest to\nresearchers working on protein complex structure prediction, particularly when\naccurate experimental structures are available for one or both of the monomers in\nisolation.\nReviewed Preprint\nPublished from the original\npreprint after peer review\nand assessment by eLife.\nAbout eLife's process\nReviewed preprint posted\nDecember 12, 2023 (this version)\nSent for peer review\nSeptember 4, 2023\nPosted to bioRxiv\nAugust 24, 2023\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 2 of 34\nIntroduction\nProtein-protein interactions(PPIs) are essential activities of most cellular processes(Alberts,\n1998     ; Spirin & Mirny, 2003     ). Structure characterization of PPIs is important for mechanistic\ninvestigation of these cellular processes and therapeutic development(Goodsell & Olson, 2000     ).\nHowever, currently experimental structures of many important PPIs are still missing as\nexperimental methods to resolve complex structures such as X-ray crystallography, nuclear\nmagnetic resonance, cryo-electron microscopy are costly and time-consuming(Berman et al.,\n2000     ). Therefore, it is necessary to develop computational methods to predict protein complex\nstructures(Bonvin, 2006     ). Predicting contacting residue pairs between interacting proteins can\nbe considered as an intermediate step for protein complex structure prediction(Hopf et al.,\n2014     ; Ovchinnikov et al., 2014     ), as the predicted contacts can be integrated into protein-\nprotein docking algorithms to assist protein complex structure prediction(Dominguez et al.,\n2003     ; H. Li & Huang, 2021     ; Sun et al., 2020     ). Besides, the predicted contacts can also be very\nuseful to guide protein interfacial design(Martino et al., 2021     ) to and the inter-protein contact\nprediction methods can be further extended to predict novel PPIs (Cong et al., 2019     ; Green et al.,\n2021     ).\nBased on the fact that contacting residue pairs often vary co-operatively during evolution,\ncoevolutionary analysis methods(Weigt et al., 2009     ) have been used in previous studies to\npredict inter-protein contacts(Hopf et al., 2014     ; Ovchinnikov et al., 2014     ). However,\ncoevolutionary analysis methods do have certain limitations. For examples, effective\ncoevolutionary analysis requires a large number of interolog sequences, which are often difficult\nto obtain, especially for heteromeric PPIs(R. M. Rao et al., 2021a     ); and it is difficult to distinguish\ninter-protein and intra-protein coevolutionary signals for homomeric PPIs(Uguzzoni et al.,\n2017     ). Inspired by its great success in intra-protein contact prediction(Hanson et al., 2018     ; Ju\net al., 2021     ; Y. Li et al., 2019     ; Si & Yan, 2021     ; Wang et al., 2017     ), deep learning has also\nbeen applied to predict inter-protein contacts(Guo et al., 2022     ; Roy et al., 2022     ; Xie & Xu,\n2022     ; Yan & Huang, 2021     ; Zeng et al., 2018     ). ComplexContact(Zeng et al., 2018     ), to the\nbest of our knowledge, the first deep learning method for inter-protein contact prediction, has\nsignificantly improved the prediction accuracy over coevolutionary analysis methods. However,\nits performance on eukaryotic PPIs is still quite limited, partly due to the difficulty to accurately\ninfer interologs for eukaryotic PPIs. In a later study, coming from the same group as\nComplexContact, Xie et al. developed GLINTER(Xie & Xu, 2022     ), another deep learning method\nfor inter-protein contact prediction. Comparing with ComplexContact, GLINTER leverages\nstructures of interacting monomers, from which their rotational invariant graph representations\nare used as additional input features. GLINTER outperforms ComplexContact in the prediction\naccuracy, although there is still large room for improvement, especially for heteromeric PPIs. It is\nworth mentioning that CDPred(Guo et al., 2022     ), a recently developed method, further\nsurpasses GINTER in prediction accuracy with 2D attention-based neural networks. Apart from\nthese methods developed to predict inter-protein contacts for both homomeric and heteromeric\nPPIs, inter-protein contact prediction methods specifically for homomeric PPIs were also\ndeveloped(Roy et al., 2022     ; Wu et al., 2022     ; Yan & Huang, 2021     ), as predicting the inter-\nprotein contacts for homomeric PPIs is generally much easier due to the symmetric restriction,\nrelatively larger interfaces and the trivialness of interologs identification. For example, Yan et al.\ndeveloped DeepHomo(Yan & Huang, 2021     ), a deep learning method specifically to predict inter-\nprotein contacts of homomeric PPIs, which also significantly outperforms coevolutionary analysis-\nbased methods. However, DeepHomo requires docking maps calculated from structures of\ninteracting monomers, which is computationally expensive and is also sensitive to the quality of\nmonomeric structures. Besides, coming from the same group, Lin et al. further developed\nDeepHomo2(P. Lin et al., 2023     ) for inter-protein contact prediction for homomeric PPIs by\nincluding the MSA (multiple sequence alignment) embeddings and attentions from an MSA-based\nprotein language model (MSA transformer)(R. M. Rao et al., 2021b     ) in their prediction model,\nwhich further improved the prediction performance. In almost the same time with DeepHomo2,\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 3 of 34\nwe proved that embeddings from protein language models(R. Rao et al., 2021     ; Rives et al.,\n2021     ) (PLMs) are very effective features to predict inter-protein contacts for both homomeric\nand heteromeric PPIs, and we further show the sequence embeddings, MSA embeddings and the\ninter-protein coevolutionary information complement each other in the prediction, with which we\ndeveloped DRN-1D2D_Inter(Si & Yan, 2023     ). Extensive benchmark results show that DRN-\n1D2D_Inter significantly outperforms DeepHomo and GLINTER in inter-protein contact prediction,\nalthough DRN-1D2D_Inter makes the prediction purely from sequences.\nIn this study, we developed a structure-informed method to predict inter-protein contacts. Given\nstructures of two interacting proteins, we first build rotationally and translationally (SE(3))\ninvariant geometric graphs from the two monomeric structures, with which encoded both the\ninter-residue distance and orientation information in the geometric graphs. We further embedded\nthe single sequence embeddings, multiple sequence alignment (MSA) embeddings and structure\nembeddings from PLMs in the graph nodes of the corresponding residues to build the PLM\nembedded geometric graphs, which are then transformed by graph encoders formed by geometric\nvector perceptrons to generate graph embeddings for interacting monomers. The graph\nembeddings are further combined with inter-protein pairwise features and transformed by\nresidual networks formed by dimensional hybrid residual blocks to predict inter-protein contacts.\nThe developed method referred to as PLMGraph-Inter was extensive benchmarked on multiple\ntests with application of either experimental or predicted structures of interacting monomers as\nthe input. The result shows that in both cases, PLMGraph-Inter outperforms other top prediction\nmethods including DeepHomo, GLINTER, CDPred, DeepHomo2 and DRN-1D2D_Inter by large\nmargins. In addition, we also compared the prediction results of PLMGraph-Inter with the protein\ncomplex structures generated by AlphaFold-Multimer(Evans et al., 2022     ). The result shows that\nfor many targets which AlphaFold-Multimer made poor predictions, PLMGraph-Inter yielded\nbetter results. Finally, we show leveraging the contacts predicted by PLMGraph-Inter as\nconstraints for protein-protein docking can dramatically improve its performance for protein\ncomplex structure prediction.\nResults\nOverview of PLMGraph-Inter\nThe method of PLMGraph-Inter is summarized in Figure 1     . PLMGraph-Inter consists of three\nmodules: the graph representation module, the graph encoder module and the residual network\nmodule. Each interacting monomer is first transformed into a PLM-embedded graph by the graph\nrepresentation module, then the graph is passed through the graph encoder module to obtain a 1D\nrepresentation of each protein. The two protein representations are transformed into 2D pairwise\nfeatures through outer concatenation and further concatenated with other 2D pairwise features,\nwhich are then transformed by the residual network module to obtain the predicted inter-protein\ncontact map.\nThe graph representation module\nThe first step of the graph representation module is to represent the protein 3D structure as a\ngeometric graph, where each residue is represented as a node, and an edge is defined if the Cα\natom distance between two residues is less than 18Å. For each node and edge, we use scalars and\nvectors extracted from the 3D structures as their geometric features. To make the geometric graph\nSE(3) invariant, we use a set of local coordinate systems to extract the geometric vectors. The SE(3)\ninvariance of representation of each interacting monomer is important, as in principal, the inter-\nprotein contact prediction result should not depend on the initial positions and orientations of\nprotein structures. A detailed description can be found in the Methods section. The second step is\nto integrate the single sequence embedding from ESM-1b(Rives et al., 2021     ), the MSA embedding\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 4 of 34Yunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 4 of 34\nFigure 1.\nOverview of PLMGraph-Inter. (a) The network architecture of PLMGraph-Inter. (b) The graph representation module. (c) The\ngraph encoder module, s denotes scalar features, v denotes vector features. (d) The dimensional hybrid residual block (“IN”\ndenotes Instance Normalization).\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 5 of 34\nfrom ESM-MSA-1b(R. Rao et al., 2021     ), the Position-Specific Scoring Matrix (PSSM) calculated\nfrom the MSA and structure embedding from ESM-IF(Hsu et al., 2022     ) for each interacting\nmonomer using its corresponding geometric graph. Where ESM-1b and ESM-MSA-1b are\npretrained PLMs learned from large datasets of sequences and MSAs respectively without label\nsupervision, and ESM-IF is a supervised PLM trained from 12 million protein structures predicted\nby AlphaFold2(Jumper et al., 2021     ) for fixed backbone design. The embeddings from these\nmodels contain high dimensional representations of each residue in the protein, which are\nconcatenated and further combined with the PSSM to form additional features of each node in the\ngeometric graph. Since the sequence embeddings, the MSA embeddings, the PSSM and the\nstructure embeddings are all SE(3) invariant, the PLM-embedded geometric graph of each protein\nis also SE(3) invariant.\nThe graph encoder module\nThe graph encoder module is formed by geometric vector perceptron (GVP) and GVP convolutional\nlayer (GVPConv)(Jing, Eismann, Soni, et al., 2021     ; Jing, Eismann, Suriana, et al., 2021     ). Where\nGVP is a graph neural network module consisting of a scalar track and a vector track, which can\nperform SE(3) invariant transformations on scalar features and SE(3) equivariant on vector\nfeatures of nodes and edges; GVPConv follows the message passing paradigm of graph neural\nnetwork and mainly consists of GVP, which updates the embedding of each node by passing\ninformation from its neighboring nodes and edges. A detailed description of GVP and GVPConv\ncan be found in the Methods section and also in the work of GVP(Jing, Eismann, Soni, et al., 2021     ;\nJing, Eismann, Suriana, et al., 2021     ). For each protein graph, we first use a GVP module to reduce\nthe dimension of the scalar features of each node from 2586 to 256, which is then transformed\nsuccessively by three GVPConv layers. Finally, we stitch the scalar features and the vector features\nof each node to form the 1D representation of the protein. Since the input protein graph is SE(3)\ninvariant and the GVP and GVPConv transformations are SE(3) equivariant, the 1D representation\nof each interacting monomer is also SE(3) invariant.\nThe residual network module\nThe residual network module block is mainly formed by nine dimensional hybrid residual blocks.\nOur previous study illustrated the effective receptive field can be enlarged with the application of\nthe dimensional hybrid residual block, thus helps improve the model performance(Si & Yan,\n2021     ). Specifically, we first use a convolution layer with kernel size of 1*1 to reduce the number\nof channels of the input 2D maps from 1044 to 96, which are then transformed successively by\nnine dimensional hybrid residual blocks and another convolution layer with kernel size of 1*1 for\nthe channel reduction (from 96 to 1). Finally, we use the sigmoid function to transform the feature\nmap to obtain the predicted inter-protein contact map.\nEvaluation of PLMGraph-Inter on\nHomoPDB and HeteroPDB test sets\nWe first evaluated PLMGraph-Inter on two self-built test sets which are non-redundant to the\ntraining dataset of PLMGraph-Inter: HomoPDB and HeteroPDB. Where HomoPDB is the test set for\nhomomeric PPIs containing 400 homodimers and HeteroPDB is the test set for heteromeric PPIs\ncontaining 200 heterodimers. For comparison, we also evaluated DeepHomo, GLINTER,\nDeepHomo2, CDPred and DRN-1D2D_Inter on the same datasets. Since DeepHomo and\nDeepHomo2 was developed to predict inter-protein contacts only for homomeric PPIs, its\nevaluation was only performed on HomoPDB. In all the evaluations, the structural related features\nwere drawn from experimental bound structures of interacting monomers separated from\ncomplex structures of PPIs (i.e. native structures) after randomizing their initial positions and\norientations (DRN-1D2D_Inter does not use structural information). It should be noted that since\nHomoPDB and HeteroPDB are not de-redundant with the training sets of DeepHomo, DeepHomo2,\nCDPred and GLINTER, the performances of the four methods may be overestimated.\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 6 of 34\nTable 1      shows the mean precision of each method on the HomoPDB and HeteroPDB when top\n(5, 10, 50, L/10, L/5) predicted inter-protein contacts are considered, where L denotes the sequence\nlength of the shorter protein in the PPI and (Note: GLINTER encountered errors in 81 targets in\nHomoPDB and 15 targets in HeteroPDB at run time and did not produce predictions, thus we\nremoved these targets in the evaluation of the performance of GLINTER. The performances of\nother methods on HomoPDB and HeteroPDB after the removal of these targets are shown in Table\nS1     ). As can be seen from the table, the mean precision of PLMGraph-Inter far exceeds those of\nother algorithms in each metric for both datasets. In particular, the mean precision of PLMGraph-\nInter is substantially improved in each metric on each dataset compared to our previous method\nDRN-1D2D_Inter which used most features of PLMGraph-Inter except these drawn from structures\nof the interacting monomers, illustrating the importance of the inclusion of structural\ninformation. Besides, GLINTER, CDPred and DeepHomo2 also use structural information and\nPLMs, has much lower performance than PLMGraph-Inter, illustrating the efficacy of our deep\nlearning framework.\nIt can also be seen from the result that all the methods tend to have better performances on\nHomoPDB than those on HeteroPDB. This is reasonable as complex structures of homodimers are\ngenerally C2 symmetric, which largely restricts the configurational spaces of PPIs, making the\ninter-protein contact prediction a much easier task (e.g., the inter-protein contact maps for\nhomodimers are also symmetric). Besides, comparing with heteromeric PPIs, we may more likely\nto successfully infer the inter-protein coevolutionary information for homomeric PPIs to assist the\ncontact prediction for two reasons: First, it is straightforward to pair sequences in the MSAs for\nhomomeric PPIs, thus the paired MSA for homomeric PPIs may have higher qualities; second,\nhomomeric PPIs may undergo stronger evolutionary constraints, as homomeric PPIs are generally\npermanent interactions, but many heteromeric PPIs are transient interactions.\nIn addition to using the mean precision on each test set to evaluate the performance of each\nmethod, the performance comparisons between PLMGraph-Inter and other models on the top 50\npredicted contacts for each individual target in HomoPDB and HeteroPDB are shown in Figure 2a-\nb     . Specifically, PLMGraph-Inter achieved the best performance for 60% of the targets in\nHomoPDB and 58% of the targets in HeteroPDB. We further group targets in each dataset\naccording to their inter-protein contact densities defined as \n , where LA and LB are\nthe protein lengths. In Figure 2c-d     , we show the mean precisions of the top 50 predicted\ncontacts on targets within different ranges of contact densities. As can be seen from the figure that\nthe predicted contacts tend to have lower precision when contact densities of the targets are\nlower, however, PLMGraph-Inter consistently achieved the best performance.\nImpact of the monomeric structure quality on contact prediction\nIn real practice, native structures of interacting monomers generally do not exist, and only\nunbound or predicted structures can be employed in the prediction, which often contain certain\nlevels of structural variances comparing with native structures. An effective inter-protein contact\nprediction method should be robust to structural variances of interacting monomers. Recently,\nAlphaFold2 achieved unprecedented progress in protein structure prediction, which can predict\nprotein structures with the accuracy close to experimental results for many proteins. Therefore, it\nis necessary to evaluate the performance of PLMGraph-Inter using the predicted structures\ngenerated by AlphaFold2.\nWe used AlphaFold2 to predict structures of interacting monomers for all PPIs in HomoPDB and\nHeteroPDB. Considering that interacting monomers in HomoPDB and HeteroPDB are not de-\nredundant with the training set of AlphaFold2, using default settings of AlphaFold2 may\noverestimate its performance. To mimic the performance of AlphaFold2 in real practice and\nproduce predicted monomeric structures with more diverse qualities, we only used the MSA\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 7 of 34Yunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 7 of 34\nTable 1.\nThe performances of DeepHomo, GLINTER, DRN-1D2D_Inter and PLMGraph-\nInter on the HomoPDB and HeteroPDB test sets using native structures.\nFigure 2.\nThe performances of PLMGraph-Inter and other methods on the HomoPDB and HeteroPDB test sets. (a) ∼ (b): The head-to-\nhead comparison of the precisions (%) of the top 50 contacts predicted by PLMGraph-Inter and other methods for each\ntarget in (a) HomoPDB and (b) HeteroPDB. (c) ∼ (d): The mean precisions of the top 50 contacts predicted by each method on\nPPIs within different intervals of contact densities in (c) HomoPDB and (d) HeteroPDB.\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 8 of 34\nsearched from Uniref100(Suzek et al., 2015     ) protein sequence database as the input to the\nmodel_5_ptm of AlphaFold2, and did not use other sequence databases and prediction models\nincluded in AlphaFold2. Besides, we set to not use templates in the structure prediction. The\npredicted structures yielded a mean TM-score 0.88, which is close to the performance of\nAlphaFold2 for CASP14 targets (mean TM-score 0.85)(Z. Lin et al., 2023     ). The predicted structures\nof interacting monomers were then severed as the input to PLMGraph-Inter and other prediction\nmethods (except for DRN-1D2D_Innter, which does not use structural information) for inter-\nprotein contact prediction.\nAs shown in Figures 3a-b     , when the predicted structures were used by PLMGraph-Inter for\ninter-protein contact prediction, mean precisions of the predicted inter-protein contacts in each\nmetric on both HomoPDB and HeteroPDB test sets decrease by about 5%, indicating qualities of the\ninput structures do have certain impact on the prediction performance. However, the\nperformance of PLMGraph-Inter using the predicted monomer structures still dramatically\noutperforms DRN-1D2D_Inter and other prediction methods (see Table S2     ).\nWe further explored the impact of the monomeric structure quality on the inter-protein contact\nprediction performance of PLMGraph-Inter. Specifically, for a given PPI, the TM-score(Zhang &\nSkolnick, 2004     ) was used to evaluate the quality of the predicted structure for each interacting\nmonomer, and the TM-score of the predicted structure with lower quality was used to evaluate the\noverall monomeric structure prediction quality for the PPI, denoted as “DTM-score”. In Figure\n3c     , we show the performance gaps (using the mean precisions of the top 50 predicted contacts\nas the metric) between applying the predicted structures and applying the experimental\nstructures in the inter-protein contact prediction, in which we grouped targets according to DTM-\nscores of their monomeric structure prediction, and in Figure 3d     , we show the performance\ncomparison for each specific target. From Figure 3c-d     , we can clearly see that when the DTM-\nscore is lower, the prediction using the prediction structure tends to have lower accuracy.\nHowever, when the DTM-score is greater than or equal to 0.8, there is almost no difference\nbetween the applying the predicted structures and applying the experimental structure, which\nshows the robustness of PLMGraph-Inter to the structure quality.\nAblation study\nTo explore the contribution of each input component to the performance of PLMGraph-Inter, we\nconducted ablation study on PLMGraph-Inter. The graph representation from the structure of\neach interacting proteins is the base feature of PLMGraph-Inter, so we first trained the baseline\nmodel using only the geometric graphs as the input feature, denoted as model a. Our previous\nstudy in DRN-1D2D_Inter has shown that the single sequence embeddings, the MSA 1D features\n(including the MSA embeddings and PSSMs) and the 2D pairwise features from the paired MSA\nplay important roles in the model performance. To further explore the importance of these\nfeatures when integrated with the geometric graphs, we trained model b-d separately (model b:\ngeometric graphs + sequence embeddings, model c: geometric graphs + sequence embeddings +\nMSA 1D features, model d: geometric graphs + sequence embeddings + MSA 1D features + 2D\nfeatures). Finally, we included the structure embeddings as additional features to train the model e\n(model e uses all the input features of PLMGraph-Inter). All the five models were trained using the\nsame protocol as PLMGraph-Inter on the same training and validation partition without cross\nvalidation. We further evaluated performances of five models together with PLMGraph-Inter (i.e.,\nmodel f: model e + cross validation) on HomoPDB and HeteroPDB using native structures of\ninteracting monomers respective.\nIn Figure 4a     , we show the mean precisions of the top 50 predicted contacts by model a-f on\nHomoPDB and HeteroPDB respectively. It can be seen from Figure 4a      that including the\nsequence embeddings in the geometric graphs has a very good boost to the model performance\n(model b versus model a), while the additional introduction of MSA 1D features and 2D pairwise\nfeatures can further improve the model performance (model d versus model c versus model b).\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 9 of 34Yunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 9 of 34\nFigure 3.\nThe performances of PLMGraph-Inter when using native and AlphaFold2 predicted structures as the input. (a) ∼ (b): The\nperformance comparison of PLMGraph-Inter when using native structures and AlphaFold2 predicted structures as the input\non (a) HomoPDB and (b) HeteroPDB. (c) The performance gaps (measured as the difference of the mean precision of the top\n50 predicted contacts) of PLMGraph-Inter with the application of AlphaFold2 predicted structures and native structures as the\ninput when the PPIs are within different intervals of DTM-score. (d) The comparison of the precision of top 50 contacts\npredicted by PLMGraph-Inter for each target when using native structures and AlphaFold2 predicted structures as the input.\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 10 of 34\nDRN-1D2D_Inter also uses the same set of sequence embeddings, MSA 1D features and 2D pairwise\nfeatures as the input, and our model d shows a significant performance improvement over DRN-\n1D2D_Inter (single model) (the model trained on the same training and validation partition\nwithout cross validation) on both HomoPDB and HeteroPDB (the mean precision improvement:\nHomoPDB:14%, HeteroPDB:5.6%), indicating that the introduced graph representation is\nimportant for the model performance. The head-to-head comparison of model d and DRN-\n1D2D_Inter (single model) on each specific target in Figure 4b      further demonstrates the value\nof the graph representation. Besides, the additional introduced structure embeddings from ESM-IF\ncan further improve the mean precisions of the predicted contacts by 3 ∼ 4% on both HomoPDB and\nHeteroPDB (model e versus model d) and the application of the cross validation can also improve\nthe precisions by 1.0 % on HomoPDB and 2.7% on HeteroPDB (model f versus model d) (see Table\nS3     ).\nTo demonstrate the efficacy of our proposed graph representation of protein structures, we also\ntrained a model using the structural representation proposed in the work of GVP(Jing, Eismann,\nSoni, et al., 2021     ; Jing, Eismann, Suriana, et al., 2021     ) (denoted as “GVP Graph”), as a control.\nOur structural representation differs significantly from GVP Graph. For example, we extracted\ninter-residue distances and orientations between five atoms (C,O,Cα,N and a virtual Cβ) from the\nstructure as the geometric scalar and vector features, in which the vector features are calculated\nin a local coordinate system. However, GVP Graph only uses the distances and orientations\nbetween Cα atoms as the geometric scalar and vector features and the vector features are\ncalculated in a global coordinate system. In addition, after the geometric graph is transformed by\nthe graph encoder module, GVP Graph only uses the scalar features of each node as the node\nrepresentation, while we concatenate the scalar and vector features of the node as the node\nrepresentation. In Figure 4c     , we show the performance comparison between this model and\nour base model (model a). From Figure 4c     , we can clear see that our base model significantly\noutperforms the GVP Graph-based model on both HomoPDB and HeteroPDB, illustrating the high\nefficacy of our proposed graph representation.\nEvaluation of PLMGraph-Inter on DHTest and DB5.5 test sets\nWe further evaluated PLMGraph-Inter on DHTest and DB5.5. The DHTest test set was formed by\nremoving PPIs redundant to our training set from the original test set of DeepHomo, which\ncontains 130 homomeric PPIs. The DB5.5 test set was formed by removing PPIs redundant to our\ntraining dataset from the heterodimers in Protein-protein Docking Benchmark 5.5, which contains\n59 heteromeric PPIs. Still, both the native structures and the predicted structures (generated using\nthe same protocol as in HomoPDB and HeteroPDB) of the interacting monomers were used\nrespectively in the inter-protein contact prediction. It should be noted that since DHTest and DB5.5\nare not de-redundant with the training sets of CDPred and GLINTER, particularly, all PPIs in the\nDHTest test set are included in the training set of CDPred, thus the performances of the two\nmethods may be overestimated.\nAs shown in Figure 5a-b     , when using the native structures in the prediction, the mean\nprecisions of the top 50 contacts predicted by PLMGraph-Inter are 71.9% on DHTest and 29.5% on\nDB5.5 (also see Table S4     ), which are dramatically higher than DeepHomo, GLINTER,\nDeepHomo2 and DRN-1D2D_Inter (Note: GLINTER encountered errors for 47 targets in DHTest and\n3 targets in DB5.5 at run time and did not produce predictions, thus we removed these targets in\nthe evaluation of the performance of GLINTER. The performances of other methods on DHTest\nand DB5.5 after the removal of these targets are shown in Table S5     ). We can also see that\nalthough PLMGraph-Inter achieved significantly better performance than CDPred on DB5.5, its\nperformance on DHTest is quite close to CDPred. However, it should be noted that the\nperformance of CDPred on DHTest might be grossly overestimated since PPIs in DHTest are fully\nincluded in the training set of CDPred. The distributions of the precisions of top 50 predicted\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 11 of 34Yunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 11 of 34\nFigure 4.\nThe ablation study of PLMGraph-Inter on the HomoPDB and HeteroPDB test sets. (a) The mean precisions of the top 50\ncontacts predicted by different ablation models on the HomoPDB and HeteroPDB test sets. (b) The head-to-head\ncomparisons of mean precisions of the top 50 contacts predicted by model d and DRN-1D2D_Inter (single model) for each\ntarget in HomoPDB and HeteroPDB. (c) The head-to-head comparison of mean precisions of the top 50 contacts predicted by\nthe model using our geometric graphs and the GVP geometric graphs.\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 12 of 34\ncontacts by different methods on DHTest and DB5.5 are shown in Figure 5d-e     , from which we\ncan clearly see that PLMGraph-Inter can make high-quality predictions for more targets on both\nDHTest and DB5.5.\nWhen using the predicted structures in the prediction, the mean precisions of top 50 contacts\npredicted by PLMGraph-Inter show reasonable decrease to 61.1% on DHTest and 23.8% on DB5.5\nrespectively (see Figure 5a-b      and Table S6     ). We also analyzed the impact of the monomeric\nstructure quality on the inter-protein contact prediction performance of PLMGraph-Inter. As\nshown in Figure 5c     , when the DTM-score is greater than or equal to 0.8, there is almost no\ndifference between applying the predicted structures and the native structures, which is\nconsistent with our analysis in HomoPDB and HeteroPDB.\nWe noticed that the performance of PLMGraph-Inter on the DB5.5 is significantly lower than that\non HeteroPDB, and so are the performances of other methods. That the targets in DB5.5 have\nrelatively lower mean contact densities (1.01% versus 1.29%) may partly explain this\nphenomenon. In Figure 5f     , we show the variations of the precisions of predicted contacts with\nthe variation of contact density. As can be seen from Figure 5f     , as the contact density increases,\nprecisions of predicted contacts tend to increase regardless of whether the native structures or\npredicted structures are used in the prediction. 37.29% targets in DB5.5 are with inter-protein\ncontact densities lower than 0.5%, for which precisions of predicted contacts are generally very\nlow, making the overall inter-protein contact prediction performance on DB5.5 relatively low.\nComparison of PLMGraph-Inter with AlphaFold-Multimer\nAfter the development of AlphaFold2, DeepMind also released AlphaFold-Multimer, as an\nextension of AlphaFold2 for protein complex structure prediction. The inter-protein contacts can\nalso be extracted from the complex structures generated by AlphaFold-Multimer. It is worth to\nmaking a comparison between the performances of AlphaFold-Multimer and PLMGraph-Inter on\ninter-protein contact prediction. Therefore, we also employed AlphaFold-Multimer to generate\ncomplex structures for all the PPIs in the four datasets which we used to evaluate PLMGraph-Inter.\nWe then selected the 50 inter-protein residue pairs with the shortest heavy atom distances in each\ngenerated protein complex structures as the predicted inter-protein contacts. It should be noted\nthat AlphaFold-Multimer used all protein complex structures in Protein Data Bank deposited\nbefore 2018-04-30 for the model development, thus these PPIs may have a large overlap with the\ntraining set of AlphaFold-Multimer. Therefore, there is no doubt that the performance of\nAlphaFold-Multimer would be overestimated here. Besides, comparing with AlphaFold-Multimer,\nPLMGraph-inter is a much lighter model which was trained on a single GPU for less than two days,\nbut the development of AlphaFold-Multimer utilized huge computational resources.\nIn Figure 6a     , we show the relationship between the quality of the generated protein complex\nstructure (evaluated with DockQ) and the precision of the top 50 inter-protein contacts extracted\nfrom the protein complex structure for each PPI in the homomeric PPI (DHTest + HomoPDB) and\nheteromeric PPI (DB5.5 + HeteroPDB) datasets. As it can be seen from the figure that the precision\nof the predicted contacts is highly correlated with the quality of the generated structure. Especially\nwhen the precision of the contacts is higher than 50%, most of the generated complex structures\nhave at least acceptable qualities (DockQ≥0.23), in contrast, almost all the generated complex\nstructures are incorrect (DockQ<0.23) when the precision of the contacts is below 50%. Therefore,\n50% can be considered as a critical precision threshold for inter-protein contact prediction (the\ntop 50 contacts).\nIn Figure 6b     , we show the comparison of the precisions of top 50 contacts predicted by\nAlphaFold-Multimer and PLMGraph-Inter for each target when using the native monomeric\nstructures as the input for PLMGraph-Inter respectively (The comparison when using the\nAlphaFold2 predict structures is shown in Figure S2     ). It can be seen from the figure that\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 13 of 34Yunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 13 of 34\nFigure 5.\nThe performances of PLMGraph-Inter and other methods on the DHTest and DB5.5 test sets. (a) ∼ (b): The mean precisions of\ntop 50 contacts predicted by PLMGraph-Inter, GLINTER, DeepHomo2, CDPred and DeepHomo on (a) DHTest and (b) DB5.5\nwhen using native structures and AlphaFold2 predicted structures as the input, where the green lines indicate the\nperformance of DRN-1D2D_Inter. (c) The performance gaps (measured as the difference of the mean precision of the top 50\npredicted contacts) of PLMGraph-Inter with the application of AlphaFold2 predicted structures and native structures as the\ninput when the PPIs are within different intervals of DTM-score. (d) ∼ (e): The distributions of precisions of the top 50 contacts\npredicted by PLMGraph-Inter and other methods for PPIs in (d) DHTest and (e) DB5.5. (f) The mean precisions of the top 50\ncontacts predicted by PLMGraph-Inter on PPIs within different intervals of contact densities in DB5.5.\nFigure 6.\nThe comparison of PLMGraph-Inter with AlphaFold-Multimer.\n(a) The head-to-head comparison between the qualities of the protein complex structures generated by AlphaFold-Multimer\n(evaluated with DockQ) and the precision of the top 50 inter-protein contacts extracted from the generated protein complex\nstructures. The red horizontal lines represent the threshold (DockQ=0.23) to determine whether the complex structure\nprediction is successful or not. (b) The head-to-head comparisons of precisions of the top 50 inter-protein contacts predicted\nby PLMGraph-Inter and AlphaFold-Multimer for each target in the homomeric PPI and heteromeric PPI datasets. (c) ∼ (d): The\nmean precisions of top 50 inter-protein contacts predicted by PLMGraph-Inter and AlphaFold-Multimer on the PPI subsets\nfrom (c) “ DHTest+HomoPDB” and (d) “DB5.5+HeteroPDB” in which the precision of the top 50 inter-protein contacts\npredicted by AlphaFold-Multimer is lower than 50% or the DockQ of the complex structure predicted by AlphaFold-Multimer is\nlower than 0.23.\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 14 of 34\nalthough for most of the targets, AlphaFold-Multimer yielded better results, but for a significant\nnumber of the targets that AlphaFold-Multimer made poor predictions (precision<50%), the results\nof PLMGraph-Inter can have certain improvement over the AlphaFold-Multimer predictions.\nWe further explored the performance of PLMGraph-Inter on the PPIs which AlphaFold-Multimer\nfailed to make correct predictions. Specifically, we denoted a PPI for which the precision of top 50\ninter-protein contacts predicted by AlphaFold-Multimer is lower than 50% or the DockQ of protein\ncomplex structure predicted by AlphaFold-Multimer is less than 0.23 as “AFM-precision-Failed”\nand “AFM-DockQ-Failed”, then the mean precisions of top 50 contacts predicted by PLMGraph-\nInter and AlphaFold-Multimer on the “AFM-precision-Failed” and “AFM-DockQ-Failed” sub-test sets\nfrom “DHTest+HomoPDB” and “DB5.5+HeteroPDB” are shown in Figure 6c-d     . From Figure 6c-\nd      we can see that the mean precisions of contacts predicted by PLMGraph-Inter are much\nhigher than the mean precisions of contacts predicted by AlphaFold-Multimer, demonstrating that\nPLMGraph-Inter can complement AlphaFold-Multimer well.\nPLMGraph-Inter can significantly improve\nprotein-protein docking performance\nPrior to AlphaFold-Multimer, protein-protein docking is generally used for protein complex\nstructure prediction. HADDOCK(Honorato et al., 2021     ; van Zundert et al., 2016     ) is a widely\nused information-driven protein-protein docking approach to model complex structures of PPIs,\nwhich allows us to encode predicted inter-protein contacts as constraints to drive the docking. In\nthis study, we used HADDOCK (version 2.4) to explore the contribution of PLMGraph-Inter to\nprotein complex structure prediction.\nWe prepared the test set of homomeric PPIs by merging HomoPDB and DHTest and the test set of\nheteromeric PPIs by merging HeteroPDB and DB5.5, where the monomeric structures generated\npreviously by AlphaFold2 were used as the input to HADDOCK for protein-protein docking, in\nwhich the top 50 contacts predicted by PLMGraph-Inter with the application of the predicted\nmonomeric structures were used as the constraints. Since HADDOCK generally cannot model large\nconformational changes in protein-protein docking, we filtered PPIs in which either of the\nAlphaFold2 generated interacting monomeric structure has a TM-score lower than 0.8. Finally, the\nhomomeric PPI test set contains 462 targets, denoted as Homodimer, and the heteromeric PPI test\nset contains 174 targets, denoted as Heterodimer.\nFor each PPI, we used the top 50 contacts predicted by PLMGraph-Inter and other methods as\nambiguous distance restraints between the alpha carbons (CAs) of residues (distance=8Å, lower\nbound correction=8Å, upper-bound correction=4Å) to drive the protein-protein docking. All other\nparameters of HADDOCK were set as the default parameters. In each protein-protein docking,\nHADDOCK output 200 predicted complex structures ranked by the HADDOCK scores. As a control,\nwe also performed protein-protein docking with HADDOCK in ab initio docking mode (center of\nmass restraints and random ambiguous interaction restraints definition). Besides, for homomeric\nPPIs, we additionally added the C2 symmetry constraint in both cases.\nAs shown in Figure 7a-c     , the success rate of docking on Homodimer and Heterodimer test sets\ncan be significantly improved when using the PLMGraph-Inter predicted inter-protein contacts as\nrestraints. Where on the Homodimer test set, the success rate (DockQ ≥ 0.23) of the top 1 (top 10)\nprediction of HADDOCK in ab initio docking mode are 15.37% (39.18%), and when predicted by\nHADDOCK with PLMGraph-Inter predicted contacts, the success rate of the top 1 (top 10)\nprediction 57.58% (61.04%). On the Heterodimer test set, the success rate of top 1 (top 10)\npredictions of HADDOCK in ab initio docking mode is only 1.72% (6.32%), and when predicted by\nHADDOCK with PLMGraph-Inter predicted contacts, the success rate of top 1 (top 10) prediction is\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 15 of 34\n29.89% (37.93%). From Figure 7a     -7b      we can also see that integrating PLMGraph-Inter\npredicted contacts with HADDOCK not only allows for a higher success rate, but also more high-\nquality models in the docking results.\nWe further explored the relationship between the precision of top 50 contacts predicted by\nPLMGraph-Inter and the success rate of the top prediction of HADDOCK with PLMGraph-Inter\npredicted contacts. It can be clearly seen from Figure 7d      that the success rate of protein-protein\ndocking increases with the precision of contact prediction. Especially, when the precision of the\npredicted contacts reaches 50%, the docking success rate of both homologous and heterologous\ncomplexes can reach 80%, which is consistent with our finding in AlphaFold-Multimer. Therefore,\nwe think this threshold can be used as a critical criterion for inter-protein contact prediction. It is\nimportant to emphasize that for some targets, although precisions of predicted contacts are very\nhigh, HADDOCK still failed to produce acceptable models. We manually checked these targets and\nfound many of these targets have at least one chain totally entangled by another chain (e.g., PDB\n3DFU in Figure S1     ). We doubt large structural rearrangements may exist in forming the\ncomplex structures, which is difficult to model by traditional protein-protein docking approach.\nDiscussion\nIn this study, we proposed a new method to predict inter-protein contacts, denoted as PLMGraph-\nInter. PLMGraph-Inter is based on the SE(3) invariant geometric graphs obtained from structures\nof interacting proteins which are embedded with multiple PLMs. The predicted inter-protein\ncontacts are obtained by successively transforming the PLM embedded geometric graphs with\ngraph encoders and residual networks. Benchmarking results on four test datasets show that\nPLMGraph-Inter outperforms five state-of-the-art inter-protein contact prediction methods\nincluding GLINTER, DeepHomo, CDPred, DeepHomo2 and DRN-1D2D_Inter by large margins,\nregardless of whether the native or predicted monomeric structures are used in building the\ngeometric graphs. The ablation study further shows that the integration of the PLMs with the\nprotein geometric graphs can dramatically improve the model performance, illustrating the\nefficacy of the PLM embedded geometric graphs in protein representations. The protein\nrepresentation framework proposed in this work can also be used to develop models for other\ntasks like protein function prediction, PPI prediction, etc. We further show PLMGraph-Inter can\ncomplement the result of AlphaFold-Multimer and leveraging the inter-protein contacts predicted\nby PLMGraph-Inter as constraints in protein-protein docking implemented with HADDOCK can\ndramatically improve its performance for protein complex structure prediction.\nWe noticed that although PLMGraph-Inter has achieved remarkable progress in inter-protein\ncontact prediction, there is still room for further improvement, especially for heteromeric PPIs.\nUsing more advanced PLMs, larger training datasets and explicitly integrating physicochemical\nfeatures of interacting proteins are directions worthy of exploration. Besides, since protein-protein\ndocking approach generally have difficulties in modelling large conformational changes in PPIs,\ndeveloping new approaches to integrate the predicted inter-protein contacts in the more advanced\nfolding- and-docking framework like AlphaFold-Multimer can also be a future research direction.\nMethods\nTraining and test datasets\nWe used the training set and test sets prepared in our previous work DRN-1D2D_Inter(Si & Yan,\n2023     ) to train and evaluate PLMGraph-Inter. More details for the dataset generation can be\nfound in the previous work. Specifically, we first prepared a non-redundant PPI dataset containing\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 16 of 34Yunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 16 of 34\nFigure 7.\nProtein-protein docking performances on the Homodimer and Heterodimer test sets. (a) ∼ (b) The protein-protein docking\nperformance comparison between HADDOCK with and without (ab-initio) using PLMGraph-Inter predicted contacts as\nrestraints on (a) Homodimer and (b) Heterodimer. The left side of each column shows the performance when the top 1\npredicted model for each PPI is considered, and the right side shows the performance when the top 10 predicted models for\neach PPI are considered. (c) The head-to-head comparison of qualities of the top 1 model predicted by HADDOCK with and\nwithout using PLMGraph-Inter predicted contacts as restraints for each target PPI. The red lines represent the threshold\n(DockQ=0.23) to determine whether the complex structure prediction is successful or not. (d) The success rates (the top 1\nmodel) for protein complex structure prediction when only including targets for which precisions of the predicted contacts\nare higher than certain thresholds.\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 17 of 34\n4828 homomeric PPIs and 3134 heteromeric PPIs (with sequence identity 40% as the threshold),\nand after randomly selecting 400 homomeric PPIs (denoted as HomoPDB) and 200 heteromeric\nPPIs (denoted as HeteroPDB) as independent test sets, the remaining 7362 homomeric and\nheteromeric PPIs were used for training and validation.\nDHTest and DB5.5 were also prepared in the work of DRN-1D2D_Inter by removing PPIs which are\nredundant (with sequence identity 40% as the threshold) to our training and validation set from\nthe test set of DeepHomo and Docking Benchmark 5.5. DHTest contains 130 homomeric PPIs, and\nDB5.5 contains 59 heteromeric PPIs. Therefore, all the test sets used in this study are non-\nredundant (with sequence identity 40% as the threshold) to the dataset for the model\ndevelopment.\nInter-protein contact definition\nFor a given PPI, two residues from the two interacting proteins are defined to be in contact if the\ndistance of any two heavy atoms belonging the two residues is smaller than 8 A.\nPreparing the Input features\nGeometric graphs from structures of interacting monomers\nWe first represent the protein as a graph, where each residue is represented as a node, and an\nedge is defined if the heavy atom distance between two residues is less than 18Å (In our small-\nscale tests, increasing the cutoff used for defining edges can slightly increase the performance of\nthe model. However, due to GPU memory limitations, we set the cutoff as 18Å). For each node and\nedge, we use scalars and vectors extracted from the 3D structures as their geometric features.\nFor each residue, we use its C,O,Cα,N and a virtual Cβ atom coordinates to extract information, the\nvirtual Cβ coordinates are calculated using the following formula(Dauparas et al., 2022     ): b = Cα -\nN, c = C -Cα, a = cross(b, c), Cβ = −0.58273431*a + 0.56802827*b - 0.54067466*c + Cα.\nTo achieve a SE(3) invariant graph representation, as shown in Figure S3b     , we define a local\ncoordinate system on each residue(Jumper et al., 2021     ; Pagès et al., 2019     ). Specifically, for\neach residue, the unit vector in the Cα - C direction is set as the \n  axis, the unit vector in the Cα-C-\nN plane and perpendicular \n  to is used as \n , and the z-direction is obtained through the cross\nproduct of \n  and \n .\nFor the ith node, we use the three dihedral angles (□, ψ, ω) of the corresponding residue as the\nscalar features of the node (Figure S3a     ), and the unit vectors between the Ci, Ni,Oi, Cαi, and Cβi\natoms of the corresponding residue and the Ci-1, Ni-1, Oi-1, Cαi-1, and Cβi-1 atoms of the forward\nresidue and the Ci+1, Ni+1, Oi+1, Cαi+1, and Cβi+1 atoms of backward residue as the vector features\nof the node. In total, for each node, the dimension of the scalar features is 6 (each dihedral angle is\nencoded with its sine and cosine) and the dimension of the vector features is 50*3.\nFor the edge between ith node and jth node, we use the distances and directions between the\natoms of the two residues as the scalar features and vector features(See Figure S3b     ). The\ndistances between the Ci, Ni,Oi, Cαi, and Cβi atoms of ith residue and the Cj, Nj,Oj, Cαj, and Cβj\natoms of the jth residue are used as scalar features after encoded with the 16 Gaussian radial basis\nfunctions(Jing, Eismann, Suriana, et al., 2021     ). The position difference between i and j (j-i) is also\nused as a scalar feature after sinusoidal encoding(Vaswani et al., 2017     ). The unit vectors\nbetween the Ci, Ni,Oi, Cαi, and Cβi atoms of ith residue and the Cj, Nj,Oj, Cαj, and Cβj atoms of the\njth residue are used as vector features. In total, for each edge, the dimension of the scalar features\nis 432 and the dimension of the vector features is 25*3.\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 18 of 34\nEmbeddings of single sequence, MSA and structure\nThe single sequence embedding is obtained by feeding the sequence into ESM-1b, and the\nstructure embedding is obtained by feeding the structure into ESM-IF. To obtain the MSA\nembedding, we first search the Uniref100 protein sequence database for the sequence using\nJACKHMMER(Potter et al., 2018     ) with the parameter (--incT L/2) to obtain the MSA, which is then\ninputted to hhmake(Steinegger et al., 2019     ) to get the HMM file, and to the LoadHMM.py script\nfrom RaptorX_Contact(Wang et al., 2017     ) to obtain the PSSM. The number of sequences of MSA\nis limited to 256 by hhfilter(Steinegger et al., 2019     ) and then input to ESM-MSA-1b to get the\nMSA embedding. The dimensions of the sequence embedding, PSSM, MSA embedding and\nstructural embeddings are 1280, 20, 768 and 512 respectively. After adding embeddings to the\nscalar features of the nodes, the dimension of the scalar features of each node is 2586.\n2D feature from paired MSA\nFor homomeric PPIs, the paired MSA is formed by concatenating two copies of the MSA. For\nheteromeric PPIs, the paired MSA is formed by pairing the MSAs through the phylogeny-based\napproach described in (https://github.com/ChengfeiYan/PPI_MSA-taxonomy_rank     )(Si & Yan, 2022).\nWe input the paired MSA into CCMpred(Seemayer et al., 2014     ) to get the evolutionary coupling\nmatrix, and into alnstats(Jones et al., 2015     ) to get mutual information matrix, APC-corrected\nmutual information matrix and contact potential matrix. The number of sequences of paired MSA\nis limited to 256 by hhfilter(Steinegger et al., 2019     ) and then input to ESM-MSA-1b to get the\nattention maps. In total, the channel of 2D features is 148.\nGVP and GVPConv\nGVP is a two-track neural network module consisting of a scalar track and a vector track, which\ncan perform SE(3) invariant transformations on scalar features and SE(3) equivariant\ntransformations on vector features. A detailed description can be found in the work of GVP(Jing,\nEismann, Soni, et al., 2021     ; Jing, Eismann, Suriana, et al., 2021     ).\nGVPConv is a message passing based graph neural network, which mainly consists of a message\nfunction and a feedforward function. Where the message function contains a sequence of three\nGVP modules and the feedforward function contains a sequence of two GVP modules. GVPConv is\nused to transform the node features. Specifically, the input node features are first processed by the\nmessage function. We denote the features of node i by hi, the feature of edge (j→i) by hj→i, the set\nof nodes connected to node i by εi, and the three GVP modules of the message function by gm, then\nthe node features processed by the message function can be represented as:\nWhere len(εi) denotes the number of nodes connected to node i. After sequential normalization\n(Equation 2     ) and feedforward function (Equation 3     ), the features of node i updated by\nGVPConv Layer are obtained:\nWhere gs denotes the two GVP modules of the feedforward function, \n  denotes the outputs of\nGVPConv Layer.\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 19 of 34\nTraining protocol\nOur training set contains 7362 PPIs, and we used seven-fold cross-validation to train PLMGraph-\nInter. Specifically, we randomly divided the training set into seven subsets, and each time, we\nselected six subsets as the training set and the remaining subset as the validation set. Seven\nmodels were trained in total, and the final prediction was the average of the predictions from the\nseven models. Each model was trained using AdamW optimizer with 0.001 as the initial learning\nrate, in which the singularity enhanced loss function proposed by in our previous study(Si & Yan,\n2021     ) was used calculate the training and validation loss. During training, if the validation loss\ndid not decrease within 2 epochs, we would decay the learning rate by 0.1. The training stopped\nafter the learning rate decayed twice and the model with the highest top-50 mean precision on the\nvalidation dataset was saved as the prediction model.\nPLMGraph-Inter was implemented with pytorch (v.1.11) and trained on one NVIDIA TESLA A100\nGPU with batch size equaling to 1. Due to memory limitation of GPU, the length of each protein\nsequence was limited to 400. When a sequence was longer than 400, a fragment with sequence\nlength equaling to 400 was randomly selected in the model training.\nQuality assessment of the predicted protein complex structures\nWe evaluated the models generated by AlphaFold-Multimer and HADDOCK using DockQ(Basu &\nWallner, 2016     ), a score ranging between 0 and 1. Specifically, a model with DockQ<0.23 means\nthat the prediction is incorrect; 0.23≤DockQ<0.49 means the model is an acceptable prediction;\n0.49≤DockQ<0.8 corresponds to a medium quality prediction; and 0.8≤ DockQ corresponds to a\nhigh quality prediction.\nData Availability\nThe PDB accession codes for the all the training and test sets are provided in https://github.com\n/ChengfeiYan/PLMGraph-Inter/tree/main/data     . Other data for supporting the finds of this study are\navailable from the corresponding author upon request.\nCode Availability\nThe code for implementing PLMGraph-Inter is provided in https://github.com/ChengfeiYan\n/PLMGraph-Inter     .\nAcknowledgements\nThe work was supported by the National Natural Science Foundation of China (32101001) and new\nfaculty startup grant (3004012167) of Huazhong University of Science and Technology. The\ncomputation is completed in the HPC Platform of Huazhong University of Science and Technology.\nAuthor Contributions\nY.S. and C.Y. designed and performed the experiments. Y.S. and C.Y. wrote the manuscript. C.Y.\nsupervised the work.\nEthics declarations\nCompeting interests\nThe authors declare no competing interests.\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 20 of 34\nSupplementary\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 21 of 34Yunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 21 of 34\nFigure S1.\n3D structure of the homodimer (PDB: 3DFU).\nFigure S2.\nThe comparison of PLMGraph-Inter with AlphaFold-Multimer.\n(a) The head-to-head comparisons of precisions of the top 50 inter-protein contacts predicted by PLMGraph-Inter(using\nAlphaFold2 predicted structures) and AlphaFold-Multimer for each target in the homomeric PPI and heteromeric PPI\ndatasets. (b) ∼ (c): The mean precisions of top 50 inter-protein contacts predicted by PLMGraph-Inter(using AlphaFold2\npredicted structures as input) and AlphaFold-Multimer on the PPI subsets from (b)“DHTest+HomoPDB” and (c)\n“DB5.5+HeteroPDB” in which the precision of the top 50 inter-protein contacts predicted by AlphaFold-Multimer is lower than\n50% or the DockQ of the complex structure predicted by AlphaFold-Multimer is lower than 0.23.\nFigure S3.\nThe graph representation of protein structures. (a) Dihedral angles of the protein backbone. (b) The local coordinate system\nof each amino acid. (c) The scalar (distances) and vector (directions) of the edge i->j.\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 22 of 34Yunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 22 of 34\nTable S1.\nThe performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2,\nCDPred and PLMGraph-Inter on HomoPDB and HeteroPDB after the\nremoval of targets which GLINTER failed to make the prediction\nTable S2.\nThe performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred and\nPLMGraph-Inter on HomoPDB and HeteroPDB using AlphaFold2 predicted structures\nTable S3.\nThe performances of different ablation study models on the HomoPDB and HeteroPDB test sets\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 23 of 34Yunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 23 of 34\nTable S4.\nThe performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred\nand PLMGraph-Inter on the DHTest and DB5.5 test sets using native structures\nTable S5.\nThe performances of DeepHomo, GLINTER, DRN-1D2D_Inter,\nDeepHomo2, CDPred and PLMGraph-Inter on DHTest and DB5.5 after\nthe removal of targets which GLINTER failed to make the prediction\nTable S6.\nThe performances of DeepHomo, GLINTER, DRN-1D2D_Inter, DeepHomo2, CDPred and\nPLMGraph-Inter on the DHTest and DB5.5 test sets using AlphaFold2 predicted structures\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 24 of 34\nReferences\nAlberts B (1998) The Cell as a Collection of Protein Machines: Preparing the Next\nGeneration of Molecular Biologists Cell 92:291–294https://doi.org/10.1016/S0092\n-8674(00)80922-8\nBasu S., Wallner B (2016) DockQ: A Quality Measure for Protein-Protein Docking Models\nPLOS ONE 11 https://doi.org/10.1371/journal.pone.0161879\nBerman H. M., Westbrook J., Feng Z., Gilliland G., Bhat T. N., Weissig H., Shindyalov I. N., Bourne\nP. E (2000) The Protein Data Bank Nucleic Acids Research 28:235–242https://doi.org/10.1093\n/nar/28.1.235\nBonvin A. M (2006) Flexible protein–protein docking Current Opinion in Structural Biology\n16:194–200https://doi.org/10.1016/j.sbi.2006.02.002\nCong Q., Anishchenko I., Ovchinnikov S., Baker D (2019) Protein interaction networks\nrevealed by proteome coevolution Science 365:185–189https://doi.org/10.1126/science\n.aaw6718\nDauparas J. et al. (2022) Robust deep learning–based protein sequence design using\nProteinMPNN Science 0 https://doi.org/10.1126/science.add2187\nDominguez C., Boelens R., Bonvin A. M. J. J (2003) HADDOCK: A Protein−Protein Docking\nApproach Based on Biochemical or Biophysical Information Journal of the American Chemical\nSociety 125:1731–1737https://doi.org/10.1021/ja026939x\nEvans R. et al. (2022) Evans, R., O’Neill, M., Pritzel, A., Antropova, N., Senior, A., Green, T.,\nŽídek, A., Bates, R., Blackwell, S., Yim, J., Ronneberger, O., Bodenstein, S., Zielinski, M.,\nBridgland, A., Potapenko, A., Cowie, A., Tunyasuvunakool, K., Jain, R., Clancy, E., …\nHassabis, D. (2022). Protein complex prediction with AlphaFold-Multimer (p.\n2021.10.04.463034). bioRxiv. 10.1101/2021.10.04.463034 Protein complex prediction with\nAlphaFold-Multimer 2021:10–4https://doi.org/10.1101/2021.10.04.463034\nGoodsell D. S., Olson A. J (2000) Structural Symmetry and Protein Function Annual Review of\nBiophysics and Biomolecular Structure 29:105–153https://doi.org/10.1146/annurev.biophys.29.1\n.105\nGreen A. G., Elhabashy H., Brock K. P., Maddamsetti R., Kohlbacher O., Marks D. S (2021) Large-\nscale discovery of protein interactions at residue resolution using co-evolution calculated\nfrom genomic sequences Nature Communications 12 https://doi.org/10.1038/s41467-021\n-21636-z\nGuo Z., Liu J., Skolnick J., Cheng J (2022) Prediction of inter-chain distance maps of protein\ncomplexes with 2D attention-based deep neural networks Nature Communications 13 https:\n//doi.org/10.1038/s41467-022-34600-2\nHanson J., Paliwal K., Litfin T., Yang Y., Zhou Y (2018) Accurate prediction of protein contact\nmaps by coupling residual two-dimensional bidirectional long short-term memory with\nconvolutional neural networks Bioinformatics 34:4039–4045https://doi.org/10.1093\n/bioinformatics/bty481\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 25 of 34\nHonorato R. V., Koukos P. I., Jiménez-García B., Tsaregorodtsev A., Verlato M., Giachetti A.,\nRosato A., Bonvin A. M. J. J (2021) Structural Biology in the Clouds: The WeNMR-EOSC\nEcosystem Frontiers in Molecular Biosciences 8 https://doi.org/10.3389/fmolb.2021.729513\nHopf T. A., Schärfe C. P. I., Rodrigues J. P. G. L. M., Green A. G., Kohlbacher O., Sander C., Bonvin\nA. M. J. J., Marks D. S (2014) Sequence co-evolution gives 3D contacts and structures of\nprotein complexes ELife 3 https://doi.org/10.7554/eLife.03430\nHsu C., Verkuil R., Liu J., Lin Z., Hie B., Sercu T., Lerer A., Rives A (2022) Hsu, C., Verkuil, R., Liu,\nJ., Lin, Z., Hie, B., Sercu, T., Lerer, A., & Rives, A. (2022). Learning inverse folding from\nmillions of predicted structures (p. 2022.04.10.487779). bioRxiv. 10.1101/2022.04.10.487779\nLearning inverse folding from millions of predicted structures 2022:4–10https://doi.org/10.1101\n/2022.04.10.487779\nJing B., Eismann S., Soni P. N., Dror R. O. (2021) Equivariant Graph Neural Networks for 3D\nMacromolecular Structure (arXiv:2106.03843)\nJing B., Eismann S., Suriana P., Townshend R. J. L., Dror R. O. (2021) LEARNING FROM PROTEIN\nSTRUCTURE WITH GEOMETRIC VECTOR PERCEPTRONS\nJones D. T., Singh T., Kosciolek T., Tetchner S (2015) MetaPSICOV: Combining coevolution\nmethods for accurate prediction of contacts and long range hydrogen bonding in\nproteins. Bioinformatics (Oxford England 31:999–1006https://doi.org/10.1093\n/bioinformatics/btu791\nJu F., Zhu J., Shao B., Kong L., Liu T. Y., Zheng W. M., Bu D (2021) CopulaNet: Learning residue\nco-evolution directly from multiple sequence alignment for protein structure prediction\nNature Communications 12 https://doi.org/10.1038/S41467-021-22869-8\nJumper J. et al. (2021) Highly accurate protein structure prediction with AlphaFold Nature\n596https://doi.org/10.1038/s41586-021-03819-2\nLi H., Huang S.-Y (2021) Protein–protein docking with interface residue restraints\\ast\nChinese Physics B 30 https://doi.org/10.1088/1674-1056/abc14e\nLi Y., Hu J., Zhang C., Yu D. J (2019) ResPRE: High-accuracy protein contact prediction by\ncoupling precision matrix with deep residual neural networks Bioinformatics 35:4647–\n4655https://doi.org/10.1093/bioinformatics/btz291\nLin P., Yan Y., Huang S.-Y (2023) DeepHomo2.0: Improved protein–protein contact\nprediction of homodimers by transformer-enhanced deep learning Briefings in\nBioinformatics 24 https://doi.org/10.1093/bib/bbac499\nLin Z. et al. (2023) Evolutionary-scale prediction of atomic-level protein structure with a\nlanguage model Science 379:1123–1130https://doi.org/10.1126/science.ade2574\nMartino E., Chiarugi S., Margheriti F., Garau G (2021) Mapping, Structure and Modulation of\nPPI Frontiers in Chemistry 9 https://doi.org/10.3389/fchem.2021.718405\nOvchinnikov S., Kamisetty H., Baker D (2014) Robust and accurate prediction of residue–\nresidue interactions across protein interfaces using evolutionary information ELife\n3 https://doi.org/10.7554/eLife.02030\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 26 of 34\nPagès G., Charmettant B., Grudinin S (2019) Protein model quality assessment using 3D\noriented convolutional neural networks Bioinformatics 35:3313–3319https://doi.org/10\n.1093/bioinformatics/btz122\nPotter S. C., Luciani A., Eddy S. R., Park Y., Lopez R., Finn R. D (2018) HMMER web server: 2018\nupdate Nucleic Acids Research 46:W200–W204https://doi.org/10.1093/nar/gky448\nRao R., Liu J., Verkuil R., Meier J., Canny J. F., Abbeel P., Sercu T., Rives A. (2021) MSA\nTransformer BioRxiv\nRao R. M., Liu J., Verkuil R., Meier J., Canny J., Abbeel P., Sercu T., Rives A. (2021) Rao, R. M., Liu,\nJ., Verkuil, R., Meier, J., Canny, J., Abbeel, P., Sercu, T., & Rives, A. (2021a). MSA\nTransformer. Proceedings of the 38th International Conference on Machine Learning,\n8844–8856. https://proceedings.mlr.press/v139/rao21a.html MSA Transformer. Proceedings\nof the 38th International Conference on Machine Learning, 8844–8856\nRao R. M., Liu J., Verkuil R., Meier J., Canny J., Abbeel P., Sercu T., Rives A. (2021) Rao, R. M., Liu,\nJ., Verkuil, R., Meier, J., Canny, J., Abbeel, P., Sercu, T., & Rives, A. (2021b). MSA\nTransformer. Proceedings of the 38th International Conference on Machine Learning,\n8844–8856. https://proceedings.mlr.press/v139/rao21a.html MSA Transformer. Proceedings\nof the 38th International Conference on Machine Learning, 8844–8856\nRives A. et al. (2021) Biological structure and function emerge from scaling unsupervised\nlearning to 250 million protein sequences Proceedings of the National Academy of Sciences of\nthe United States of America 118:1–46https://doi.org/10.1073/pnas.2016239118\nRoy R. S., Quadir F., Soltanikazemi E., Cheng J (2022) A deep dilated convolutional residual\nnetwork for predicting interchain contacts of protein homodimers Bioinformatics 38:1904–\n1910https://doi.org/10.1093/bioinformatics/btac063\nSeemayer S., Gruber M., Söding J (2014) CCMpred—Fast and precise prediction of protein\nresidue-residue contacts from correlated mutations. Bioinformatics (Oxford England\n30:3128–3130https://doi.org/10.1093/bioinformatics/btu500\nSi Y., Yan C (2021) Improved protein contact prediction using dimensional hybrid residual\nnetworks and singularity enhanced loss function Briefings in Bioinformatics 22 https://doi\n.org/10.1093/bib/bbab341\nSi Y., Yan C (2022) Protein complex structure prediction powered by multiple sequence\nalignments of interologs from multiple taxonomic ranks and AlphaFold2 Briefings in\nBioinformatics 23 https://doi.org/10.1093/bib/bbac208\nSi Y., Yan C (2023) Improved inter-protein contact prediction using dimensional hybrid\nresidual networks and protein language models. Briefings in Bioinformatics bbad\n39 https://doi.org/10.1093/bib/bbad039\nSpirin V., Mirny L. A (2003) Protein complexes and functional modules in molecular\nnetworks Proceedings of the National Academy of Sciences 100:12123–12128https://doi.org/10\n.1073/pnas.2032324100\nSteinegger M., Meier M., Mirdita M., Vöhringer H., Haunsberger S. J., Söding J (2019) HH-suite3\nfor fast remote homology detection and deep protein annotation BMC Bioinformatics\n20 https://doi.org/10.1186/s12859-019-3019-7\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 27 of 34\nSun D., Liu S., Gong X (2020) Review of multimer protein–protein interaction complex\ntopology and structure prediction\\ast Chinese Physics B 29 https://doi.org/10.1088/1674\n-1056/abb659\nSuzek B. E., Wang Y., Huang H., McGarvey P. B., Wu C. H., the UniProt Consortium (2015)\nUniRef clusters: A comprehensive and scalable alternative for improving sequence\nsimilarity searches Bioinformatics 31:926–932https://doi.org/10.1093/bioinformatics/btu739\nUguzzoni G., John Lovis S., Oteri F., Schug A., Szurmant H., Weigt M (2017) Large-scale\nidentification of coevolution signals across homo-oligomeric protein interfaces by direct\ncoupling analysis Proceedings of the National Academy of Sciences 114:E2662–E2671https://doi\n.org/10.1073/pnas.1615068114\nvan Zundert G. C. P., Rodrigues J. P. G. L. M., Trellet M., Schmitz C., Kastritis P. L., Karaca E.,\nMelquiond A. S. J., van Dijk M., de Vries S. J., Bonvin A. M. J. J. (2016) The HADDOCK2.2 Web\nServer: User-Friendly Integrative Modeling of Biomolecular Complexes Journal of Molecular\nBiology 428:720–725https://doi.org/10.1016/j.jmb.2015.09.014\nVaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A. N., Kaiser Ł., Polosukhin I.\n(2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł.,\n& Polosukhin, I. (2017). Attention is All you Need. Advances in Neural Information\nProcessing Systems, 30.\nhttps://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-\nAbstract.html Attention is All you Need. Advances in Neural Information Processing Systems 30\nWang S., Sun S., Li Z., Zhang R., Xu J (2017) Accurate De Novo Prediction of Protein Contact\nMap by Ultra-Deep Learning Model PLOS Computational Biology 13 https://doi.org/10.1371\n/journal.pcbi.1005324\nWeigt M., White R. A., Szurmant H., Hoch J. A., Hwa T (2009) Identification of direct residue\ncontacts in protein–protein interaction by message passing Proceedings of the National\nAcademy of Sciences 106:67–72https://doi.org/10.1073/pnas.0805923106\nWu T., Huang H., Li J., Wang W., Gong X (2022) Inter-chain contact map prediction for\nprotein complex based on graph attention network and triangular multiplication update\n2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) :2143–2148https://\ndoi.org/10.1109/BIBM55620.2022.9995360\nXie Z., Xu J (2022) Deep graph learning of inter-protein contacts Bioinformatics 38:947–\n953https://doi.org/10.1093/bioinformatics/btab761\nYan Y., Huang S. Y (2021) Accurate prediction of inter-protein residue-residue contacts for\nhomo-oligomeric protein complexes Briefings in Bioinformatics 22:1–13https://doi.org/10\n.1093/bib/bbab038\nZeng H., Wang S., Zhou T., Zhao F., Li X., Wu Q., Xu J (2018) ComplexContact: A web server for\ninter-protein contact prediction using deep learning Nucleic Acids Research 46:W432–\nW437https://doi.org/10.1093/nar/gky420\nZhang Y., Skolnick J (2004) Scoring function for automated assessment of protein structure\ntemplate quality. Proteins: Structure Function, and Bioinformatics 57:702–710https://doi.org\n/10.1002/prot.20264\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 28 of 34\nArticle and author information\nYunda Si\nSchool of Physics, Huazhong University of Science and Technology, China\nChengfei Yan\nSchool of Physics, Huazhong University of Science and Technology, China\nFor correspondence: chengfeiyan@hust.edu.cn\nORCID iD: 0000-0002-2010-6668\nCopyright\n© 2023, Yunda Si & Chengfei Yan\nThis article is distributed under the terms of the Creative Commons Attribution License,\nwhich permits unrestricted use and redistribution provided that the original author and\nsource are credited.\nEditors\nReviewing Editor\nAnne-Florence Bitbol\nEcole Polytechnique Federale de Lausanne (EPFL), Switzerland\nSenior Editor\nAleksandra Walczak\nÉcole Normale Supérieure - PSL, France\nReviewer #1 (Public Review):\nSummary:\nGiven knowledge of the amino acid sequence and of some version of the 3D structure of two\nmonomers that are expected to form a complex, the authors investigate whether it is possible\nto accurately predict which residues will be in contact in the 3D structure of the expected\ncomplex. To this effect, they train a deep learning model that takes as inputs the geometric\nstructures of the individual monomers, per-residue features (PSSMs) extracted from MSAs for\neach monomer, and rich representations of the amino acid sequences computed with the pre-\ntrained protein language models ESM-1b, MSA Transformer, and ESM-IF. Predicting inter-\nprotein contacts in complexes is an important problem. Multimer variants of AlphaFold, such\nas AlphaFold-Multimer, are the current state of the art for full protein complex structure\nprediction, and if the three-dimensional structure of a complex can be accurately predicted\nthen the inter-protein contacts can also be accurately determined. By contrast, the method\npresented here seeks state-of-the-art performance among models that have been trained end-\nto-end for inter-protein contact prediction.\nStrengths:\nThe paper is carefully written and the method is very well detailed. The model works both for\nhomodimers and heterodimers. The ablation studies convincingly demonstrate that the\nchosen model architecture is appropriate for the task. Various comparisons suggest that\nPLMGraph-Inter performs substantially better, given the same input than DeepHomo,\nGLINTER, CDPred, DeepHomo2, and DRN-1D2D_Inter. As a byproduct of the analysis, a\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 29 of 34\npotentially useful heuristic criterion for acceptable contact prediction quality is found by the\nauthors: namely, to have at least 50% precision in the prediction of the top 50 contacts.\nWeaknesses:\nMy biggest issue with this work is the evaluations made using *bound* monomer structures\nas inputs, coming from the very complexes to be predicted. Conformational changes in\nprotein-protein association are the key element of the binding mechanism and are\nchallenging to predict. While the GLINTER paper (Xie & Xu, 2022) is guilty of the same sin, the\nauthors of CDPred (Guo et al., 2022) correctly only report test results obtained using predicted\nunbound tertiary structures as inputs to their model. Test results using experimental\nmonomer structures in bound states can hide important limitations in the model, and thus\nsay very little about the realistic use cases in which only the unbound structures\n(experimental or predicted) are available. I therefore strongly suggest reducing the\nimportance given to the results obtained using bound structures and emphasizing instead\nthose obtained using predicted monomer structures as inputs.\nIn particular, the most relevant comparison with AlphaFold-Multimer (AFM) is given in\nFigure S2, *not* Figure 6. Unfortunately, it substantially shrinks the proportion of structures\nfor which AFM fails while PLMGraph-Inter performs decently. Still, it would be interesting to\ninvestigate why this occurs. One possibility would be that the predicted monomer structures\nare of bad quality there, and PLMGraph-Inter may be able to rely on a signal from its\nlanguage model features instead. Finally, AFM multimer confidence values (\"iptm + ptm\")\nshould be provided, especially in the cases in which AFM struggles.\nBesides, in cases where *any* experimental structures - bound or unbound - are available\nand given to PLMGraph-Inter as inputs, they should also be provided to AlphaFold-Multimer\n(AFM) as templates. Withholding these from AFM only makes the comparison artificially\nunfair. Hence, a new test should be run using AFM templates, and a new version of Figure 6\nshould be produced. Additionally, AFM's mean precision, at least for top-50 contact\nprediction, should be reported so it can be compared with PLMGraph-Inter's.\nIt's a shame that many of the structures used in the comparison with AFM are actually in the\nAFM v2 training set. If there are any outside the AFM v2 training set and, ideally, not\nsequence- or structure-homologous to anything in the AFM v2 training set, they should be\ndiscussed and reported on separately. In addition, why not test on structures from the\n\"Benchmark 2\" or \"Recent-PDB-Multimers\" datasets used in the AFM paper?\nIt is also worth noting that the AFM v2 weights have now been outdated for a while, and\nbetter v3 weights now exist, with a training cutoff of 2021-09-30.\nAnother weakness in the evaluation framework: because PLMGraph-Inter uses structural\ninputs, it is not sufficient to make its test set non-redundant in sequence to its training set. It\nmust also be non-redundant in structure. The Benchmark 2 dataset mentioned above is an\nexample of a test set constructed by removing structures with homologous templates in the\nAF2 training set. Something similar should be done here.\nFinally, the performance of DRN-1D2D for top-50 precision reported in Table 1 suggests to me\nthat, in an ablation study, language model features alone would yield better performance\nthan geometric features alone. So, I am puzzled why model \"a\" in the ablation is a \"geometry-\nonly\" model and not a \"LM-only\" one.\nReviewer #2 (Public Review):\nThis work introduces PLMGraph-Inter, a new deep-learning approach for predicting inter-\nprotein contacts, which is crucial for understanding protein-protein interactions. Despite\nadvancements in this field, especially driven by AlphaFold, prediction accuracy and\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 30 of 34\nefficiency in terms of computational cost) still remains an area for improvement. PLMGraph-\nInter utilizes invariant geometric graphs to integrate the features from multiple protein\nlanguage models into the structural information of each subunit. When compared against\nother inter-protein contact prediction methods, PLMGraph-Inter shows better performance\nwhich indicates that utilizing both sequence embeddings and structural embeddings is\nimportant to achieve high-accuracy predictions with relatively smaller computational costs\nfor the model training.\nThe conclusions of this paper are mostly well supported by data, but test examples should be\nrevisited with a more strict sequence identity cutoff to avoid any potential information\nleakage from the training data. The main figures should be improved to make them easier to\nunderstand.\n1. The sequence identity cutoff to remove redundancies between training and test set\nwas set to 40%, which is a bit high to remove test examples having homology to\ntraining examples. For example, CDPred uses a sequence identity cutoff of 30% to\nstrictly remove redundancies between training and test set examples. To make their\nresults more solid, the authors should have curated test examples with lower\nsequence identity cutoffs, or have provided the performance changes against\nsequence identities to the closest training examples.\n2. Figures with head-to-head comparison scatter plots are hard to understand as scatter\nplots because too many different methods are abstracted into a single plot with\nmultiple colors. It would be better to provide individual head-to-head scatter plots as\nsupplementary figures, not in the main figure.\n3. The authors claim that PLMGraph-Inter is complementary to AlphaFold-multimer as it\nshows better precision for the cases where AlphaFold-multimer fails. To strengthen\nthe point, the qualities of predicted complex structures via protein-protein docking\nwith predicted contacts as restraints should have been compared to those of\nAlphaFold-multimer structures.\n4. It would be interesting to further analyze whether there is a difference in prediction\nperformance depending on the depth of multiple sequence alignment or the type of\ncomplex (antigen-antibody, enzyme-substrates, single species PPI, multiple species PPI,\netc).\nAuthor Response\nPublic Reviews:\nReviewer #1 (Public Review):\nSummary:\nGiven knowledge of the amino acid sequence and of some version of the 3D structure of\ntwo monomers that are expected to form a complex, the authors investigate whether it is\npossible to accurately predict which residues will be in contact in the 3D structure of the\nexpected complex. To this effect, they train a deep learning model that takes as inputs\nthe geometric structures of the individual monomers, per-residue features (PSSMs)\nextracted from MSAs for each monomer, and rich representations of the amino acid\nsequences computed with the pre-trained protein language models ESM-1b, MSA\nTransformer, and ESM-IF. Predicting inter-protein contacts in complexes is an important\nproblem. Multimer variants of AlphaFold, such as AlphaFold-Multimer, are the current\nstate of the art for full protein complex structure prediction, and if the three-dimensional\nstructure of a complex can be accurately predicted then the inter-protein contacts can\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 31 of 34\nalso be accurately determined. By contrast, the method presented here seeks state-of-\nthe-art performance among models that have been trained end-to-end for inter-protein\ncontact prediction.\nStrengths:\nThe paper is carefully written and the method is very well detailed. The model works both\nfor homodimers and heterodimers. The ablation studies convincingly demonstrate that\nthe chosen model architecture is appropriate for the task. Various comparisons suggest\nthat PLMGraph-Inter performs substantially better, given the same input than\nDeepHomo, GLINTER, CDPred, DeepHomo2, and DRN-1D2D_Inter. As a byproduct of the\nanalysis, a potentially useful heuristic criterion for acceptable contact prediction quality\nis found by the authors: namely, to have at least 50% precision in the prediction of the\ntop 50 contacts.\nWe thank the reviewer for recognizing the strengths of our work!\nWeaknesses:\nMy biggest issue with this work is the evaluations made using bound monomer\nstructures as inputs, coming from the very complexes to be predicted. Conformational\nchanges in protein-protein association are the key element of the binding mechanism\nand are challenging to predict. While the GLINTER paper (Xie & Xu, 2022) is guilty of the\nsame sin, the authors of CDPred (Guo et al., 2022) correctly only report test results\nobtained using predicted unbound tertiary structures as inputs to their model. Test\nresults using experimental monomer structures in bound states can hide important\nlimitations in the model, and thus say very little about the realistic use cases in which\nonly the unbound structures (experimental or predicted) are available. I therefore\nstrongly suggest reducing the importance given to the results obtained using bound\nstructures and emphasizing instead those obtained using predicted monomer structures\nas inputs.\nWe thank the reviewer for the suggestion! We evaluated PLMGraph-Inter with the predicted\nmonomers and analyzed the result in details (see the “Impact of the monomeric structure\nquality on contact prediction” section and Figure 3). To mimic the real cases, we even\ndeliberately reduced the performance of AF2 by using reduced MSAs (see the 2nd paragraph\nin the ““Impact of the monomeric structure quality on contact prediction” section). We leave\nsome of the results in the supplementary of the current manuscript (Table S2). We will move\nthese results to the main text to emphasize the performance of PLMGraph-Inter with the\npredicted monomers in the revision.\nIn particular, the most relevant comparison with AlphaFold-Multimer (AFM) is given in\nFigure S2, not Figure 6. Unfortunately, it substantially shrinks the proportion of\nstructures for which AFM fails while PLMGraph-Inter performs decently. Still, it would be\ninteresting to investigate why this occurs. One possibility would be that the predicted\nmonomer structures are of bad quality there, and PLMGraph-Inter may be able to rely\non a signal from its language model features instead. Finally, AFM multimer confidence\nvalues (\"iptm + ptm\") should be provided, especially in the cases in which AFM struggles.\nWe thank the reviewer for the suggestion! Yes! The performance of PLMGraph-Inter drops\nwhen the predicted monomers are used in the prediction. However, it is difficult to say which\nis a fairer comparison, Figure 6 or Figure S2, since AFM also searched monomer templates\n(see the third paragraph in 7. Supplementary Information : 7.1 Data in the AlphaFold-\nMultimer preprint: https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2.full) in the\nprediction. When we checked our AFM runs, we found that 99% of the targets in our study\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 32 of 34\n(including all the targets in the four datasets: HomoPDB, HeteroPDB, DHTest and DB5.5)\nemployed at least 20 templates in their predictions, and 87.8% of the targets employed the\nnative templates. We will provide the AFM confidence values of the AFM predictions in the\nrevision.\nBesides, in cases where any experimental structures - bound or unbound - are available\nand given to PLMGraph-Inter as inputs, they should also be provided to AlphaFold-\nMultimer (AFM) as templates. Withholding these from AFM only makes the comparison\nartificially unfair. Hence, a new test should be run using AFM templates, and a new\nversion of Figure 6 should be produced. Additionally, AFM's mean precision, at least for\ntop-50 contact prediction, should be reported so it can be compared with PLMGraph-\nInter's.\nWe thank the reviewers for the suggestion! We would like to notify that AFM also searched\nmonomer templates (see the third paragraph in 7. Supplementary Information : 7.1 Data in\nthe AlphaFold-Multimer preprint: https://www.biorxiv.org/content/10.1101/2021.10.04\n.463034v2.full) in the prediction. When we checked our AFM runs, we found that 99% of the\ntargets in our study (including all the targets in the four datasets: HomoPDB, HeteroPDB,\nDHTest and DB5.5) employed at least 20 templates in their predictions, and 87.8% of the\ntargets employed the native template.\nIt's a shame that many of the structures used in the comparison with AFM are actually in\nthe AFM v2 training set. If there are any outside the AFM v2 training set and, ideally, not\nsequence- or structure-homologous to anything in the AFM v2 training set, they should\nbe discussed and reported on separately. In addition, why not test on structures from the\n\"Benchmark 2\" or \"Recent-PDB-Multimers\" datasets used in the AFM paper?\nWe thank the reviewer for the suggestion! The biggest challenge to objectively evaluate AFM\nis that as far as we known, AFM does not release the PDB ids of its training set and the\n“Recent-PDB-Multimers” dataset. “Benchmark 2” only includes 17 heterodimer proteins, and\nthe number can be further decreased after removing targets redundant to our training set.\nWe think it is difficult to draw conclusions from such a small number of targets. In the\nrevision, we will analyze the performance of AFM on targets released after the date cutoff of\nthe AFM training set, but with which we cannot totally remove the redundancy between the\ntraining and the test sets of AFM.\nIt is also worth noting that the AFM v2 weights have now been outdated for a while, and\nbetter v3 weights now exist, with a training cutoff of 2021-09-30.\nWe thank the reviewer for reminding the new version of AFM. The only difference between\nAFM V3 and V2 is the cutoff date of the training set. Our test set would have more overlaps\nwith the training set of AFM V3, which is one reason that we think AFM V2 is more\nappropriate to be used in the comparison.\nAnother weakness in the evaluation framework: because PLMGraph-Inter uses structural\ninputs, it is not sufficient to make its test set non-redundant in sequence to its training\nset. It must also be non-redundant in structure. The Benchmark 2 dataset mentioned\nabove is an example of a test set constructed by removing structures with homologous\ntemplates in the AF2 training set. Something similar should be done here.\nWe agree with the reviewer that testing whether the model can keep its performance on\ntargets with no templates (i.e. non-redundant in structure) is important. We will perform the\nanalysis in the revision.\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 33 of 34\nFinally, the performance of DRN-1D2D for top-50 precision reported in Table 1 suggests\nto me that, in an ablation study, language model features alone would yield better\nperformance than geometric features alone. So, I am puzzled why model \"a\" in the\nablation is a \"geometry-only\" model and not a \"LM-only\" one.\nUsing the protein geometric graph to integrate multiple protein language models is the main\nidea of PLMGraph-Inter. Comparing with our previous work (DRN-1D2D_Inter), we consider\nthe building of the geometric graph as one major contribution of this work. To emphasize the\nefficacy of this geometric graph, we chose to use the “geometry-only” model as the base\nmodel. We will further clarity this in the revision.\nReviewer #2 (Public Review):\nThis work introduces PLMGraph-Inter, a new deep-learning approach for predicting\ninter-protein contacts, which is crucial for understanding protein-protein interactions.\nDespite advancements in this field, especially driven by AlphaFold, prediction accuracy\nand efficiency in terms of computational cost) still remains an area for improvement.\nPLMGraph-Inter utilizes invariant geometric graphs to integrate the features from\nmultiple protein language models into the structural information of each subunit. When\ncompared against other inter-protein contact prediction methods, PLMGraph-Inter\nshows better performance which indicates that utilizing both sequence embeddings and\nstructural embeddings is important to achieve high-accuracy predictions with relatively\nsmaller computational costs for the model training.\nThe conclusions of this paper are mostly well supported by data, but test examples\nshould be revisited with a more strict sequence identity cutoff to avoid any potential\ninformation leakage from the training data. The main figures should be improved to\nmake them easier to understand.\nWe thank the reviewer for recognizing the significance of our work! We will revise the\nmanuscript carefully to address the reviewer’s concerns.\n1. The sequence identity cutoff to remove redundancies between training and test set was\nset to 40%, which is a bit high to remove test examples having homology to training\nexamples. For example, CDPred uses a sequence identity cutoff of 30% to strictly remove\nredundancies between training and test set examples. To make their results more solid,\nthe authors should have curated test examples with lower sequence identity cutoffs, or\nhave provided the performance changes against sequence identities to the closest\ntraining examples.\nWe thank the reviewer for the valuable suggestion! Using different thresholds to reduce the\nredundancy between the test set and the training set is a very good suggestion, and we will\nperform the analysis in the revision. In the current version of the manuscript, the 40%\nsequence identity is used as the cutoff for many previous studies used this cutoff (e.g. the\nRecent-PDB-Multimers used in AlphaFold-Multimer (see: 7.8 Datasets in the AlphaFold-\nMultimer paper); the work of DSCRIPT: https://www.cell.com/action/showPdf?pii=S2405-4712\n%2821%2900333-1 (see: the PPI dataset paragraph in the METHODS DETAILS section of the\nSTAR METHODS)). One reason for using the relatively higher threshold for PPI studies is that\nPPIs are generally not as conserved as protein monomers.\nWe performed a preliminary analysis using different thresholds to remove redundancy when\npreparing this provisional response letter:\nYunda Si et al., 2023 eLife. https://doi.org/10.7554/eLife.92184.1 34 of 34\nAuthor response table 1.\nTable1. The performance of PLMGraph-Inter on the HomoPDB and HeteroPDB test sets using\nnative structures(AlphaFold2 predicted structures).\nMethod:\nTo remove redundancy, we clustered 11096 sequences from the training set and test sets\n(HomoPDB, HeteroPDB) using MMSeq2 with different sequence identity threshold (40%, 30%,\n20%, 10%) (the lowest cutoff for CD-HIT is 40%, so we switched to MMSeq2). Each sequence is\nthen uniquely labeled by the cluster (e.g. cluster 0, cluster 1, …) to which it belongs, from\nwhich each PPI can be marked with a pair of clusters (e.g. cluster 0-cluster 1). The PPIs\nbelonging to the same cluster pair (note: cluster n - cluster m and cluster n-cluster m were\nconsidered as the same pair) were considered as redundant. For each PPI in the test set, if the\npair cluster it belongs to contains the PPI belonging to the training set, we remove that PPI\nfrom the test set.\nWe will perform more detailed analyses in the revised manuscript.\n1. Figures with head-to-head comparison scatter plots are hard to understand as scatter\nplots because too many different methods are abstracted into a single plot with multiple\ncolors. It would be better to provide individual head-to-head scatter plots as\nsupplementary figures, not in the main figure.\nWe thank the reviewer for the suggestion! We will include the individual head-to-head scatter\nplots as supplementary figures in the revision.\n1. The authors claim that PLMGraph-Inter is complementary to AlphaFold-multimer as it\nshows better precision for the cases where AlphaFold-multimer fails. To strengthen\nthe point, the qualities of predicted complex structures via protein-protein docking\nwith predicted contacts as restraints should have been compared to those of\nAlphaFold-multimer structures.\nWe thank the reviewer for the suggestion! We will add this comparison in the revision.\n1. It would be interesting to further analyze whether there is a difference in prediction\nperformance depending on the depth of multiple sequence alignment or the type of\ncomplex (antigen-antibody, enzyme-substrates, single species PPI, multiple species PPI,\netc).\nWe thank the reviewer for the suggestion! We will perform such analysis in the revision."
}