{
  "title": "More than Encoder: Introducing Transformer Decoder to Upsample",
  "url": "https://openalex.org/W3175265357",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2667760188",
      "name": "LI Yijiang",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Cai, Wentian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1901532557",
      "name": "Gao Ying",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2041536916",
      "name": "Li, Chengming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228630918",
      "name": "Hu, Xiping",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3204700807",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W2915126261",
    "https://openalex.org/W2157494358",
    "https://openalex.org/W2952339589",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W2921781974",
    "https://openalex.org/W3134449767",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2592929672",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3156509901",
    "https://openalex.org/W2987322772",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3083961261",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2963031226",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2610332124",
    "https://openalex.org/W2141200610",
    "https://openalex.org/W3137561054",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W3007268491",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2293078015",
    "https://openalex.org/W2905810301",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3097510987",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W2769910914",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2533800772",
    "https://openalex.org/W3188404242",
    "https://openalex.org/W3111448578",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3139459064",
    "https://openalex.org/W2963046541",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2893299523"
  ],
  "abstract": "Medical image segmentation methods downsample images for feature extraction and then upsample them to restore resolution for pixel-level predictions. In such a schema, upsample technique is vital in restoring information for better performance. However, existing upsample techniques leverage little information from downsampling paths. The local and detailed feature from the shallower layer such as boundary and tissue texture is particularly more important in medical segmentation compared with natural image segmentation. To this end, we propose a novel upsample approach for medical image segmentation, Window Attention Upsample (WAU), which upsamples features conditioned on local and detailed features from downsampling path in local windows by introducing attention decoders of Transformer. WAU could serve as a general upsample method and be incorporated into any segmentation model that possesses lateral connections. We first propose the Attention Upsample which consists of Attention Decoder (AD) and bilinear upsample. AD leverages pixel-level attention to model long-range dependency and global information for a better upsample. Bilinear upsample is introduced as the residual connection to complement the upsampled features. Moreover, considering the extensive memory and computation cost of pixel-level attention, we further design a window attention scheme to restrict attention computation in local windows instead of the global range. We evaluate our method (WAU) on classic U-Net structure with lateral connections and achieve state-of-the-art performance on Synapse multi-organ segmentation, Medical Segmentation Decathlon (MSD) Brain, and Automatic Cardiac Diagnosis Challenge (ACDC) datasets. We also validate the effectiveness of our method on multiple classic architectures and achieve consistent improvement.",
  "full_text": "More than Encoder: Introducing Transformer\nDecoder to Upsample\nYijiang Li1,2, Wentian Cai1,2, Ying Gao 1,2,B , Chengming Li 3 and Xiping Hu 4\n1School of Computer Science and Engineering , South China University of Technology , Guangzhou, China\n2Guangdong Provincial Key Laboratory of Artiﬁcial Intelligence in Medical Image Analysis and Application ,\nGuangdong Provincial People’s Hospital, Guangdong Academy of Medical Sciences , Guangzhou, China\n3School of Intelligent Systems Engineering , Sun Yat-sen University, Shenzhen, China\n4School of Medical Technology , Beijing Institute of Technology , Beijing, China\n{csliyijiang3000, cscaiwentian}@mail.scut.edu.cn, gaoying@scut.edu.cn, lichengming@mail.sysu.edu.cn, huxp@bit.edu.cn\nAbstract—Medical image segmentation methods downsample\nimages for feature extraction and then upsample them to restore\nresolution for pixel-level predictions. In such schema, upsample\ntechnique is vital in restoring information for better performance.\nHowever, existing upsample techniques leverage little information\nfrom downsampling paths. The local and detailed feature from\nthe shallower layer such as boundary and tissue texture is\nparticularly more important in medical segmentation compared\nwith natural image segmentation. To this end, we propose\na novel upsample approach for medical image segmentation,\nWindow Attention Upsample (W AU), which upsamples features\nconditioned on local and detailed features from downsampling\npath in local windows by introducing attention decoders of Trans-\nformer. W AU could serve as a general upsample method and be\nincorporated into any segmentation model that possesses lateral\nconnections. We ﬁrst propose the Attention Upsample which\nconsists of Attention Decoder (AD) and bilinear upsample. AD\nleverages pixel-level attention to model long-range dependency\nand global information for a better upsample. Bilinear upsample\nis introduced as the residual connection to complement the\nupsampled features. Moreover, considering the extensive memory\nand computation cost of pixel-level attention, we further design\na window attention scheme to restrict attention computation\nin local windows instead of the global range. We evaluate our\nmethod (W AU) on classic U-Net structure with lateral connections\nand achieve state-of-the-art performance on Synapse multi-organ\nsegmentation, Medical Segmentation Decathlon (MSD) Brain,\nand Automatic Cardiac Diagnosis Challenge (ACDC) datasets.\nWe also validate the effectiveness of our method on multiple\nclassic architectures and achieve consistent improvement.\nIndex Terms—Transformer, upsampling, semantic segmenta-\ntion, medical image analysis.\nI. I NTRODUCTION\nDeep learning revolutionizes many ﬁelds of machine in-\ntelligence including multimedia processing [39], [40], [56],\nscene understanding [51], [55], and Computer Aided Diagno-\nsis (CAD) [3], [54] area. In CAD area, particularly, medical\nimage segmentation plays a crucial role in clinical diagnosis\nand treatment processes. Long et al . [32] proposes the fa-\nmous FCN architecture which downsamples high resolution\nimages to extract semantic information and then upsamples\nthem to provide dense predictions. UNet [41] extends it to\na U-shape architecture with lateral connections between the\nB Corresponding author: Ying Gao, gaoying@scut.edu.cn\nUp1 Up2 Up3 Up4\nResUNet\nOurs\nUp1 Up2 Up3 Up4\nFig. 1. Visualization of decoded feature maps during upsampling on MSD\nBrain and Synapse datasets.\ndownsampling and upsampling path. This architecture and\nits variants become dominant in medical image segmentation\n[5], [21], [23], [62]. The encoder-decoder structure enlarges\nthe receptive ﬁeld making the Convolution Neural Network\n(CNN) better at capturing semantic information. Its pyramid\nstructure enables the model to have multi-scale perception\nand reduces computation complexity. However, the reduction\nof resolution inevitably loses information, so maintaining\nsemantic information while recovering the spatial resolution\nbecomes challenging. To resolve this issue, multiple upsample\ntechniques [58]–[60] have been proposed. However, existing\nupsample techniques leverage little information from down-\nAccepted by IEEE BIBM 2022\narXiv:2106.10637v3  [cs.CV]  24 Nov 2022\nsampling path.\nThe prosper of Transformer in the ﬁeld of Natural Lan-\nguage Processing (NLP) inspires researchers to explore its\napplicability to Computer Vision (CV). ViT [12] takes only\nthe encoder of the transformer and obtains comparable results\nas CNN. Swin Transformer [31] adopts and modiﬁes the ViT\narchitecture [12] into the one that constructs a hierarchical rep-\nresentation with reduced computation. These works prove the\nadaptability of pure Transformer to CV downstream tasks such\nas object detection and segmentation which requires modeling\nover multi-scale objects and dense pixels. Interestingly, we\nnotice that transformer also possess an encoder and decoder.\nSo, while most researchers focus on the encoder and explore\nits feature extracting ability, we instead look at the idea of the\ndecoder in transformer and its applicability to segmentation\narchitectures.\nA typical decoder in transformer takes the input token\nembedding of the last position to generate query and obtains\nthe output from encoder to produce key and value [46]. Given\nthe circumstances of translation, the output of the decoder\nis conditioned on the last output tokens while also paying\nattention to the input sequence tokens. Intuitively, we can\nview this decoding process where the output of encoders\nis decoded conditioned on input token embedding. Notice\nthat, the input token sequence may not be as long as the\nembedding from the encoder. Consequently, if the former is\nlonger, the decoder outputs longer embedding. In a way, we\ncan view it as being upsampled. Inspired by the above analogy,\nwe propose a novel upsample approach, Window Attention\nUpsample (W AU), which upsamples features conditioned on\nlocal and detailed information from the downsampling path\nin local windows. Attention upsample can enrich the seman-\ntic information based on spatial and local information and\nstill outputs features of desired larger shapes. Considering\nthat large feature maps are unaffordable in global attention,\nwe propose Window Attention Decoder (W AD) to trade-off\nbetween the global attention and computation expense. To\nfurther ease the learning, we use bilinear upsample to form\na residual connection. To the best of our knowledge, we are\nthe ﬁrst to utilize the transformer decoder in segmentation\nupsample and explore its ability to upsample feature maps and\nrestore information. We evaluate our method on classic U-Net\nstructure with lateral connection and achieve state-of-the-arts\nperformance on Synapse multi-organ segmentation, Medical\nSegmentation Decathlon (MSD) Brain and Automatic Cardiac\nDiagnosis Challenge (ACDC) datasets. We also validate our\nmethod on multiple classic architectures and achieve consistent\nimprovement.\nIn a nutshell, contributions of our work can be summarized\nas follows:\n• We propose the idea of upsampling images using the\ntransformer decoder and provide an effective U-shaped\narchitecture for medical image segmentation.\n• We adopt window-based self-attention to better model\npixel-level information and reduce computational cost\nand memory usage. To further exploit the potential, con-\nvolution projection is raised to model locality and residual\nconnection through bilinear interpolation to complement\nthe upsampled feature maps.\n• Extensive experiments on different datasets using various\narchitectures prove the effectiveness and the generaliza-\ntion ability of our Window Attention Upsample method.\nII. R ELATED WORK\nFCN [32] introduces the encoder-decoder architecture and\nsuccessfully boosts performance in the ﬁeld of segmentation\nby a large margin. U-Net [41] builds upon the idea of FCN\nand introduces a U-shape network with lateral connections\nbetween the downsampling and upsampling path which prop-\nagate context information to better localize. Since then, U-\nshape architecture thrives in many later works of 2D image\nsegmentation [21], [61] and 3D image segmentation [7], [34].\nUpsample is widely used in semantic segmentation to re-\nstore the low resolution feature maps obtained from downsam-\npling path. Conventionally, Interpolation (nearest, bilinear, and\ncubic) is adopted for the reconstruction of pixels in image pro-\ncessing with each point generated based on its neighbor pixels.\nNearest interpolation generates directly from the nearest pixel.\nBilinear interpolation [1] estimates a pixel value by averaging\nthe two nearest pixels while cubic [25] evaluate the values of\nneighbor volumes. Transposed convolution [32] is proposed to\nlearn an upsample strategy in an end-to-end manner through\nbackpropagation. Besides, latter works including PixelShufﬂe\n[43], Dupsampling [45], Meta-Upscale [20] and CAPAFE [47]\nare also later development of upsampling techniques.\nAttention mechanisms have long been proved useful both in\nthe ﬁeld of CV and NLP. SENet [19] boosts the performance\nby weighting each channel before it outputs to the next layer.\nNon-local [16], [50], [52] utilizes pixel-level global attention\nand models the long-range and global dependencies between\npixels. However, global attention at low level layers with large\nfeature maps is impractical for a quadratic complexity with\nrespect to token number [31], thus, Non-local only performs\npixel-level attention on low resolution feature maps ( e.g. the\nlast layer). Our work also models attention upon pixels. To\nreduce computation, we trade-off between global and local\nattention by using window attention [31].\nTransformer was ﬁrst introduced in [46] for machine trans-\nlation and since becomes the dominant method in many NLP\ntasks [9], [27]. Recent works starting with ViT [11] prove the\ntransformer’s adaptability in CV . ViT models 16×16 patches\nof an image as token input to a pure transformer. Swin\nTransformer leverages local window and shift operation to\ntrade-off between computation and performance. Later works\npropose numerous techniques that could maintain a reasonable\ncomputation budget without overly sacriﬁcing performance\n[10], [18], [22], [42]. Notice that despite our implementa-\ntion uses local window, these state-of-the-art techniques for\nsparse and efﬁcient attention can also be incorporated into\nour method. CvT introduces convolution in the modeling\nof tokens [53] by leveraging a convolution layer before the\nstandard self-attention layer. Different from their approach,\nwe leverage convolution projection in the self-attention layer\nto project query, key and value respectively. This incorporates\nthe locality prior into the model and in the meantime reduces\ncomputation and parameters (fully connected layer is much\nlarger than a convolution layer and uses more memory when\nthere are numerous tokens).\nThere are also some recent work demonstrating the trans-\nformer’s adaptability in medical image segmentation [5], [8],\n[38], [48]. TransUNet uses a U-shape encoder-decoder archi-\ntecture. This work exploits the feature extracting ability of ViT\nand adopts the upsampling path from regular UNet structure\nwhere lateral connection passes on local and detailed features\nfor better localization. Furthermore, Hatamizadeh et al . [14]\npropose UNETR using solely transformer to extract 3D fea-\ntures. In this work, the transformer encoder proves to be good\nat modeling long dependency over a 3D input sequence of\nimages. Karimi et al. [24] introduce a convolution-free model\nwhich utilize solely the transformer as a feature extractor.\nGiven a 3D image block, the corresponding embedding of each\npatches is computed and the segmentation map is generated\naccording to the correlation between patches via self-attention\nmechanism. This work is based entirely on transformer without\nusing convolution, which further promotes the application of\ntransformer in medical image processing.\nIII. M ETHOD\nA. Decoder to Upsample\nDecoder adopts the idea of dot product attention, much like\nencoder prevalent in recent work of transformer in vision.\nUnlike the patch encoder, our decoder attention acts on pixel-\nlevel instead of patch level in order to better model the dense\ninformation. So here, we refer to one pixel as one token.\nFor the purpose of upsampling, we are majorly concerned\nabout two factors: whether it can maintain or even enrich\nsemantic information necessary for segmentation and whether\nit outputs feature maps of higher resolution. Transformer\ndecoder inherently uses additional information ( i.e. query\ntoken) to instruct the process of attention by imposing a\nlarger weighting on tokens whose key are similar with query\nand a smaller weighting otherwise. In our Attention Decoder\n(AD), we use the feature maps of larger resolution from\ndownsampling path to generate query and input features from\nupsampling path to generate key and value. In this way,\nlarger resolution feature can be generated conditioned on rich\ninformation from downsampling path. This can be formulated\nas below:\nˆzl = AD(LN(ˆz(l−1)), LN(ˆa(l))) + ˆzl−1 (1)\nwhere LN(·) represents layer normalization, ˆzl−1 ∈\nRHl−1×Wl−1×Cl−1\ndenotes features of layer l −1 in upsam-\npling path and ˆa(l) ∈RHl×Wl×Cl\ndenotes the corresponding\nfeature maps from downsampling path.\nHl = n ·Hl−1, Wl = n ·Wl−1 (2)\nwhere Hl, Wl denotes the height and width of the feature\nmap and n an integer larger than 1. By taking the context\ninformation from the downsampling path, decoder manages to\nmodel the global semantic information conditioned on corre-\nsponding low level features. Intuitively, context information\nwill increase the weighting of relevant tokens that beneﬁts\nthe upsampling, so the semantic information from upsampling\npath can be maintained and even enriched.\nB. Locality and Computation Considerations\nLocality is an excellent property of CNN, which helps\nmodel the local features such as edges and corners. The\nreconstruction of higher resolution should focus more on\nneighboring regions. However, the transformer attends to all\ntokens deprived of the this good property. Despite its ability to\nmodel long-range dependency, transformer may lose focus on\nthe signiﬁcant and relevant tokens when there are numerous\ntokens, which is an essential problem in the pixel upsample\nprocess. Moreover, global attention among all tokens possess\na quadratic complexity and memory usage with respect to the\nnumber of tokens, which is unaffordable for modeling pixel-\nlevel attention, especially at upper layers where resolution\nis high. To restrict the model’s attention in the local area\nand to reduce computation overhead, we propose to leverage\nconvolution projection and local window attention as detailed\nin the following sections.\n1) Introducing convolution to projection: To better model\nlocal information, we try to incorporate convolution into\nprojection prior to attention block. As shown in Figure 2, we\nuse a kernel of size larger than 1, typically 3 to replace the\nlinear projection that is widely used in transformer attention\nblock. In our paper, all convolutions use kernel of 3 ×3 and\nmaintains sizes ( i.e. ”same” padding). After the projection,\nthree matrices, key ( k), value ( v) and query ( q) are obtained\nand then ﬂattened into 1D for subsequent multi-head attention\nprocess. Notice, since our input feature maps for query are\nlarger than that of key and value, 1D query sequence are\nlonger than key and value sequence. The output of decoder\nis the same size as query. After reshaping the output back to\n2D, the resolution of the output are the same with feature maps\nfrom downsampling path. In this way, upsampling is done. The\nconvolution projection can be written as follows:\nˆzi\nq = F(sq\nc ∗LN( ˆai)) (3)\nˆzi\nk/v = F(sk/v\nc ∗LN( ˆzi)) (4)\nHere * denotes the convolution operator, sc = [s1\nc, s2\nc, ··· , sC′\nc ]\nwhere C′ is the number of output channels. ˆzi\nq/k/v is the\ncorresponding k, q, v matrices obtained and F(·) denotes an\noperation that ﬂattens 2D images into 1D sequence. Then we\napply dot attention on k, q, v and computes the upsampled\nfeature maps:\nˆzl = s′\nc ∗reshape(softmax( ˆzi\nq ˆzi\nkT\n√dk\n) ˆzi\nv) (5)\nHere, reshape(·) denotes an operation that reshapes the 1D\nsequence back to 2D feature maps. Another convolution with\nkernel s′\nc is applied after the attention function.\nflatten\nflatten\nflatten\nConvolution\nStride = 1\n…………\nquery\nkey\nvalue16\n16\n64\nflatten\nflatten\nflatten\nConvolution\nStride = 1\n…………\nquery\nkey\nvalue16\n16\n64\nflatten\nflatten\nflatten\nConvolution\nStride = 1\n……\nquery\nkey\nvalue\n16\n(b)(a) Window Partition\nConv Projection\nFig. 2. Demonstration of convolution projection in (a) Attention Decoder and (b) Window Attention Decoder with W = H = 4, n = 2 and M = 4.\n2) Attention in Local Window: Inspired by [31], we propose\nlocal window attention for the attention decoder. Since self-\nattention works on one group of tokens, one window is\nenough. However, in W AD, we have tokens from two different\nresolution feature maps, so windows with different sizes are\nrequired to align the output key, value and query. As shown\nin Figure 2, we apply windows with different sizes to feature\nmaps from lateral connection and tokens from upsampling\npath. Inherit from the preceding formulation, feature map from\nlateral connection ˆa(l) ∈RHl×Wl×Cl\nis n times the size of\nthat from upsampling path ˆzl−1 ∈RHl−1×Wl−1×Cl−1\n. In order\nto align the number windows in query and key, value, windows\nsizes ratio between the two should also be n. With window\nattention, our W AD can be formulated as below:\nˆzl = WAD (LN(ˆz(l−1)), LN(ˆa(l))) (6)\nIn computational aspect, suppose we have feature maps ˆa ∈\nRH1×W1×C from lateral connection and ˆz∈RH2×W2×C from\nupsampling path , where H1 = n ·H2, W1 = n ·W2. For\nW AD, we use window size of M1, M2 for ˆa, ˆz respectively,\nwhere M1 = n ·M2.\nΩ(AD) = 2H2W2C2k2(n2 + 1) + 2(H2W2)2Cn2 (7)\nΩ(WAD ) = 2H2W2C2k2(n2 + 1) + 2(H2W2)Cn2M2\n2 (8)\nwhere k is the kernel size for our convolution projection. We\nshow here that, with large H2, W2, AD is generally impractical\nfor a quadratic computation complexity with respect to H2W2\nwhile W AD is linear to H2W2 with some ﬁxed M2 and n. As\nfor memory consideration, we have the following:\nΩ(AD) = H2W2C(n2 + 2) +n2(H2W2)2 (9)\nΩ(WAD ) = H2W2C(n2 + 2) +n2M2\n2H2W2 (10)\nNotice that the above is the memory usage of intermediate\nmatrices ( i.e. k, q, v matrices and attention weights). We\nshow that AD without window attention occupies quadratic\nmemory with respect to H2W2 while W AD is linear. With a\nhyper-parameter M, the method shows great scalability. Given\nany speciﬁc tasks, one can adjust the window size M for a\nbetter performance provided limited computation and memory\nresources.\n3) Discussion: Swin transformer leverages the local win-\ndow attention to save computation resources. Despite its low\ncomputation overhead, window attention limits the model’s\nlong-range dependency and leads to a degraded performance\n[31]. That’s to say, larger window sizes generally leads to\nbetter performance [29]. To compensate for the loss of long-\nrange dependency, Swin leverages shifted operation to increase\nthe attention range. However, in this work, we discover that\nwhen attending to a large number of tokens (at pixel-level),\nthe attention mechanism loses its focus and pays attention\nto irrelevant parts of the feature map [57] as we observe a\ndrop in performance when using larger window sizes in Figure\n6. To restrict attention in local areas, which is important for\nupsampling, window attention is used to conﬁne the attention\nin local windows. We also conduct an ablation study by adding\nan additional shifted window attention layer before our upsam-\npling module and observe an even lower performance (72.34\nDSC compared with 73.65 DSC). This further demonstrates\nthat simply enlarging receptive ﬁelds may be sub-optimal for\nupsampling.\nC. Residual Connection Through Bilinear\nIn order to complement the features and form a residual-like\noperation, we propose to use bilinear interpolation to upsample\nand adds the two upsampled features together as output. This\nbilinear upsampled feature serves as a supplement as well as\na residual connection that ease the training of W AD.\nˆzl = WAD (LN(ˆz(l−1)), LN(ˆa(l))) + Bilinear(ˆz(l−1))\n(11)\nwhere ˆzl is the output feature map of decoder upsample\nmodule l and ˆa(l) are corresponding feature maps of twice\nthe resolution from downsampling path.\n…\nContracting path Window Attention Upsample (WAU)\n16\n…\nBilinear Interpolation\nQuery Key\nValue\nWindow Attention Decoder (WAD)\nDownsample\nUpsample\nConv Proj.\nFlatten\nIdentical\nElement-wise\nAddition\nDownsample\nUpsample\nConv Proj.\nFlatten\nIdentical\nElement-wise\nAddition\nFig. 3. Demonstration of W AU with W = H = 4, n = 2 and M = 4.\nW AD leverages features of larger resolution from downsampling path and\nfeatures from upsampling path to generate query and key/value respectively,\nas the embedding of query is longer than the embedding of key/value, the\nfeatures are then upsampled. Outputs of W AD and Bilinear interpolation are\nelement-wise added to generate upsampled features.\nD. Window Attention Upsample\nCombining ideas from the above, we have Window Atten-\ntion Upsample (W AU). As shown in Figure 3, W AU possesses\ntwo branches, the Window Attention Decoder branch and\nBilinear Interpolation branch. Each window of pixels are\npassed from lateral connection as query and corresponding\nwindow from upsampling path serves as key and value. Dot\nattention is performed on key and query to compute attention\nweights. The ﬁnal output of such window is obtained by\nmultiplying the attention weights and the value matrix. All\nwindows are computed simultaneously to form a larger feature\nmap. After both W AD and Bilinear Interpolation is done, the\nresults of the two are summed as the ﬁnal output.\nE. Instantiation\nTo evaluate the effectiveness of our upsample method, we\nincorporate our method into several classic and state-of-the-\nart network architectures such as ResUNet, 3D-UNet, FCN,\nand DeepLabV3. Generally, to incorporate our method into an\nexisting architecture, we simply replace the original upsample\nlayer with W AU. Since W AU requires the feature map from the\ndownsampling path, we build up lateral connections to pass\nthe dowsampled feature maps of the desired shape to each of\nthe W AU module. Take ResUNet as an example, we replace all\nof the original bilinear upsample modules with W AU. Then,\nwe leverage the lateral connection which feeds a feature map\nof exactly twice the size. Consequently, each W AD upsamples\nthe feature map twice the size and progressively upsamples the\nfeature map to the original input sizes. We present an example\nof instantiations where the classic UNet is combined with\nW AU, as shown in Figure 4. Detailed architecture description\nof each instantiation is provided in Appendix B.\nIV. E XPERIMENT\nA. Dataset\nWe evaluate our model on the MSD Task01 BrainTumour\ndataset (MSD Brain) [44], Synapse multi-organ segmentation\ndataset (Synapse), and Automatic Cardiac Diagnosis Chal-\nlenge (ACDC) datasets. MSD Brain contains 484 multimodal\nmultisite MRI data (FLAIR, T1w, T1gd, T2w) and four la-\nbels including background, edema (Ed), non-enhancing tumor\n(NET), and enhancing tumor (ET). For MSD Brain, we apply\nz-scoring normalization to preprocess each case. To alleviate\nthe problem of class imbalance, we remove all blank slices\nwith zero values and crop each slice to the region of nonzero\nvalues. Each slice is cropped to 128 ×128 before feeding into\nthe model. Synapse contains 30 cases with a total of 3779\nslices of resolution 512×512. Each case consists of 14 labeled\norgans from MICCAI 2015 Multi-Atlas Abdomen Labeling\nChallenge. Following the settings of TransUNet [5], we select\n8 organs for model evaluation and divide cases into train set\nand validation set with the ratio of 3:2 (i.e. 18 cases for training\nand 12 for validation). Preprocess pipeline includes clipping\nthe values of each pixel to [-125, 275] and normalizing them\nto [0, 1]. Both datasets are trained on 2D slices and validated\non 3D volume following standard evaluation procedure. ACDC\ncontains 100 cases of MRI scans from different patients whose\ngoal is to segment the myocardium (Myo) of the left ventricle\nand the cavity of the right (RV) and left ventricle (LV). Fol-\nlowing the settings of [2], 80 and 20 subjects are divided into\ntraining and validation set respectively with the resolution re-\nsampled to 160 ×160. Multiple data preprocessing techniques\nare done with the same settings of [36].\nB. Implementation Details\nFor all experiments, we perform some slight data aug-\nmentation, e.g. , random rotation, and horizontal and vertical\nﬂipping. For model invariant, to coincide with the typical U-\nNet structure, we set n = 2 meaning to upsample by 2 at each\nW AU module. We use a base window size of 4 for the MSD\nBrain dataset and 7 for the Synapse dataset. All models are\ntrained using Adam [26] with betas of 0.9 and 0.999 (default\nsetting) and Cosine Annealing learning rate [33] with a warm\nup [15] of 2 epochs. The initial learning rate is 0.0001 with\na batch size of 12 for MSD Brain and 32 for Synapse. No\npre-training is used and all experiments are conducted using\ntwo NVIDIA RTX2080Ti GPU.\nC. Results\n1) Results on MSD Brain Dataset: Results of our model\nand other state-of-the-art methods are shown in Table I 1. On\nthe MSD BrainTumour Dataset, our model achieves the best\nperformance of 74.75% Dice similarity coefﬁcient (DSC) with\n80.73%, 63.23%, and 80.29% on edema, non-enhancing tu-\nmor, and enhancing tumor respectively. When comparing with\n1Results of UNet, VNet, AHNet, Att-UNet, SegResNet and UNETR are\nfrom [14], results of two nnUNet models are from [23], results of DiNTS are\nfrom [17].\nk vq k vq\n160²\n80²\n40²\n128 256\n4 64\n64 128\n20²\n10²256 512\n512\nk vq k vq\nk vq k vq\n160²\nk vq k vq\n512\n256\n128\n64\n432\nb\nb\nb\nb\nk vq WAD\nConv 3× 3, ReLU\nMax Pooling\nLateral Connection\nb Bilinear Interpolation, \nElement-wise Addition\nFig. 4. An example of instantiations where the classic UNet is combined with W AU. Input image is ﬁrst downsampled 4 times, then upsampled through our\nproposed W AU module. At each W AU, queries from encoding path are passed through lateral connection to query the feature maps (processed as keys) from\nbelow.\nour baseline model ResUNet, we achieve a signiﬁcant increase\nof 2.83%. Compared with nnUNet [23] 2D version, which\nalso builds upon the U-Net architecture, our method obtains\nan improvement of 3.19%. Moreover, when compared with\nensemble 3D nnUNet, we also outperform by 0.86%. We also\nmake a comparison between state-of-art Transformer-based\nmodels including recent 3D network UNETR [14] and 2D\nnetwork SwinUNet [4] which we outperform by a margin of\n2.94% and 1.55% on average DSC respectively. To provide a\ndemonstration of results on MSD Brain dataset, the ﬁrst two\nrows of Figure 7 offer a sample of (a) gt label, (b) ResUNet,\n(c) TransUNet, and (d) Ours. Our baseline ResUNet model\nshows to be under-segmented prone, i.e. the ﬁrst row of (b)\nshows an incomplete segmentation region while transformer-\nbased models, i.e. TransUNet and Ours, can produce more\ncomplete and accurate results via the establishment of long-\nrange dependencies. We can also see that both ResUNet\nand TransUNet face the problem of providing false positive\npredictions, i.e. the second row (b) and (c) show a false\npositive prediction of ET instead of NET (gt). Compared\nwith TransUnet, our model shows great performance in local\nand marginal regions. This could be attributed to pixel-level\ncorrelation in the local window that could better model the\nlocal features.\n2) Results on Synapse Dataset: Experiment on Synapse\ndataset (Table II) demonstrates the effectiveness and gener-\nalization ability to multi-organ tasks of our upsample method.\nWe make comparison with baseline model ResUNet, recent\nwork TransUNet [5] and SwinUNet [4] where our method\noutperforms ResUNet by 5.41%, TransUNet by 2.92% and\nTABLE I\nCOMPARISON WITH STATE-OF-THE -ART ON THE MSD B RAIN DATASET .\nMethods DSC ↑ Ed NET ET\nFCN32s [32] 60.40 70.03 46.96 64.99\nFCN16s [32] 66.25 74.84 52.93 70.97\nFCN8s [32] 69.21 76.61 56.17 74.83\nUNet [41] 67.65 75.03 57.87 70.06\nDeepLabV3 [6] 68.86 77.42 57.11 72.04\nTransUNet [5] 71.11 77.38 59.04 76.91\nnnUNet (2D) [23] 71.56 78.60 58.65 77.42\nTransBTS [49] 71.79 78.62 60.14 76.61\nResUNet 71.92 77.73 59.47 78.57\nSegTran R50 [28] 73.48 80.20 61.81 78.42\nSwinUNet [4] 73.20 79.41 61.38 73.20\nVNet [34] 65.77 75.96 54.99 66.38\nAHNet [30] 66.63 75.8 57.58 66.50\nAtt-UNet [37] 67.07 75.29 57.11 68.81\nSegResNet [35] 69.65 76.37 59.56 73.03\nUNETR [14] 71.81 79.00 60.62 75.82\nDiNTS [17] 72.97 80.20 61.09 77.63\n3D-UNet [7] 72.15 79.45 60.42 76.59\nnnUNet (3D) [23] 73.89 80.79 61.72 79.16\nOurs 74.75 80.73 63.23 80.29\nSwinUNet by 1.27% on average DSC 2. Moreover, we make\na comparison on Hausdorff (HD) metrics which measures\nmodels’ sensitivity to edge segmentation. As per the table, we\nalso achieve a state-of-the-art performance of 18.50. Specif-\nically, We achieve the best performance on Kidney(L) with\n84.47%, Kidney(R) with 81.04%, Liver with 94.40%, Pancreas\nwith 61.01%, and Stomach with 79.35%. This experiment\nshows our model’s ability to generalize to multiple organs’\nsegmentation. To provide a demonstration of results on the\nSynapse dataset, the middle two rows of Figure 7 offer a\n2Results of V-Net, DARR, R50 U-Net, R50 Att-UNet, R50 ViT and\nTransUNet are from [5].\nTABLE II\nCOMPARISON WITH STATE-OF-THE -ART ON THE SYNAPSE DATASET .\nMethods DSC ↑ HD ↓ Aorta GB Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\nV-Net [34] 68.81 - 75.34 51.87 77.10 80.75 87.84 40.05 80.56 56.98\nDARR [13] 69.77 - 74.74 53.77 72.31 73.24 94.08 54.18 89.90 45.96\nR50 U-Net [5] 74.68 36.87 87.74 63.66 80.60 78.19 93.74 56.90 85.87 74.16\nR50 Att-UNet [5] 75.57 36.97 55.92 63.91 79.20 72.71 93.56 49.37 87.19 74.95\nR50 ViT [5] 71.29 32.87 73.73 55.13 75.80 72.20 91.51 45.99 81.99 73.95\nTransUNet [5] 77.48 31.69 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\nSwinUNet [4] 79.13 21.55 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\nU-Net [41] 73.09 40.05 83.17 58.74 80.40 73.36 93.13 45.43 83.90 66.59\nResUNet 74.99 27.57 88.55 59.93 83.14 71.63 93.16 52.51 84.23 66.77\nOurs 80.40 18.50 88.40 60.64 84.47 81.04 94.40 66.01 88.92 79.35\nTABLE III\nSTATISTICS OF MODEL PARAMETERS AND FLOP S.\nMethods Params GFLOPs\nResUNet 17.27M 14.64\nwide-ResUNet 30.82M 26.08\nTransUNet 93.19M 11.71\nSegTran R50 128.82M 53.47\nSwinUNet 27.12M 5.49\nOurs 21.80M 15.94\nsample of (a) gt label, (b) ResUNet and (c) TransUNet, and (d)\nOurs. From the graph presented, we can also notice the same\nproblem mentioned in Section IV-C1, incomplete prediction\ncompared with gt, i.e. in the orange region of the third row,\n(b) shows no positive prediction and (c) shows little positive\npredictions, and misclassiﬁcation of the label, i.e. in the green\nand orange rectangle of forth row, both (b) and (c) make false\npositive predictions.\n3) Results on ACDC Dataset: Table IV demonstrates our\nmodel’s performance on ACDC dataset comparing with state-\nof-the-arts 3. On ACDC, we achieve more than 2 points\nimprovement on a benchmark dataset with an average DSC\nof 90%+, which we consider quite tremendous on such\ndataset. It’s worth mentioning that after being carefully tuned,\nResUNet is capable of achieving a performance of 90.06%,\neven higher than other state-of-the-arts such as SwinUNet\nand TransUNet. However, our model can outperform it by\nnearly 2 points in DSC. The bottom two rows of Figure 7\noffers a sample of gt label(a), ResUNet(b) and TransUNet(c)\nand Ours(d) from ACDC dataset. We can also observe an\nincomplete segmentation problem from (b) and (c) of the two\nrows and our more complete results. This also proves that\nour method can maintain and even enrich the information in\nfeature maps during upsample process.\nD. Analytical study\n1) Comparison with Baseline: We compare our W AD with\ndifferent upsample methods including bilinear interpolation,\ntransposed convolution, pixel shufﬂe [43] and CARAFE [47].\nTable V shows the performance of various upsample methods\n3Results of R50 U-Net, R50 Att-UNet, ViT-CUP, R50-ViT-CUP, and\nTransUNet are from [5], results of SwinUNet is from [4].\nTABLE IV\nCOMPARISON WITH STATE-OF-THE -ART ON THE ACDC DATASET.\nMethods DSC ↑ RV Myo LV\nR50-U-Net [5] 87.55 87.10 80.63 94.92\nR50-AttnUNet [5] 86.75 87.58 79.20 93.47\nViT-CUP [5] 81.45 81.46 70.71 92.18\nR50-ViT-CUP [5] 87.57 86.07 81.88 94.75\nTransUNet [5] 90.05 90.14 86.00 94.00\nSwinUNet [4] 90.00 88.55 85.62 95.83\nResUNet 90.06 88.86 86.75 94.57\nOurs 92.00 91.65 88.95 95.40\non different backbones, we can make the following obser-\nvations: (i) Despite the difference in upsample method, the\noverall performance of ResUNet is better than that of classic\nU-Net, which is why we use ResUNet as the backbone. (ii)\nBilinear interpolation is slightly better than the other three\nupsample methods, but the performance of our proposed W AU\nfar exceeds them all, which suggests that the classic decoder\ndesign can be better replaced by our Window Attention\nUpsample (W AU) strategy.\nIt is worth mentioning that we also compare the number of\nparameters and FLOPs used in ResUNet, TransUNet, Swin-\nUNet, SegTran R50, and our model (Table III). We suppose\nthe better performance of TransUNet over ResUNet could be\nattributed to the large parameters. Our model, however, uses\nmuch fewer parameters (only 1/3 of TransUNet) and relatively\nacceptable operations to achieve much better performance on\nall three datasets. To make a fair comparison with baseline\nResUNet in terms of parameters, we increase the base channels\nfrom 64 to 72, resulting in the wide-ResUNet with more\nparameters and ﬂops (30.82M and 26.08 GFLOPs). As per\nthe third row of table V, we can observe that our method still\noutperforms this improved baseline by more than 2 DSC. This\ndemonstrates that the improvement is not the result of simply\nadding more parameters and ﬂops.\n2) Residual Connection through Bilinear: We adopt Bilin-\near Interpolation to form a residual connection. We argue that\nthis process feeds identical mapping forward, and thus can\nease the training process. Moreover, the Bilinear Interpolation,\nin a way, can be viewed as a complement of the upsampled\nfeatures maps. In this section, we perform an ablation study\non this operation. Particularly, we train models with and\nTABLE V\nCOMPARISON OF DIFFERENT UPSAMPLE STRATEGY ON MSD B RAIN\nDATASET.\nMethods Backbone Upsample DSC ↑\nUNet\nUNet Bilinear 71.91 (+0.11)\nTransposed 71.80\nwide-ResUNet Bilinear 72.14\nResUNet\nBilinear 71.92 (+0.61)\nTransposed 71.85 (+0.54)\npixelShufﬂe [43] 71.31\nCARAFE [47] 71.63 (+0.32)\nW AD 73.84 (+2.53)\nW AU (W AD w/ Bilinear) 74.75 (+2.83)\nUNet 3D Bilinear 72.15\nW AU 72.51 (+0.36)\nDeepLab DeepLabV3 Bilinear 68.86\nW AU 70.33 (+1.47)\nFCN\nFCN 32s Transposed 60.40\nW AU 65.89 (+5.49)\nFCN 16s Transposed 66.25\nW AU 68.97 (+2.72)\nFCN 8s Transposed 69.20\nW AU 71.32 (+2.12)\nwithout bilinear residual connection ( i.e. , W AU and W AD)\non the MSD Brain dataset. From Table V, we can see that\nadding Bilinear Interpolation increases DSC by 0.79, which\nsufﬁciently proved the effectiveness of residual connection\nthrough Bilinear Interpolation.\nSegmentation map Window Attention maps\n Segmentation map Window Attention maps\n(a) (b)\nFig. 5. Visualization of Window Attention weights on (a) Synapse and (b)\nMSD Brain datasets.\n3) Convolution Matters: In Section III-B1, we introduce\nthe convolution projection to obtain the key, query, and value\nmatrices. Compared with linear projection, convolution op-\neration provides modeling on local features which beneﬁts\nthe reconstruction of high-resolution features. In this section,\nwe explore the performance of different convolution opera-\ntions. In particular, we explore Group Convolution, Depthwise\nSeparable Convolution, and Regular Convolution operation on\nMSD Brain dataset. Results in Figure 6 reveal that Depthwise\nSeparable convolution is slightly better than the other two\nconvolution operations with a window size of 4. This could\nbe attributed to the fact that Depthwise Separable Convolution\npossesses fewer parameters and thus provides better perfor-\nmance on a relatively small dataset. We also compare the effect\nof different kernel sizes in Appendix A.\n2×2 4×4 5×5 8×8 10×10\nWindow Size\n72.0\n72.5\n73.0\n73.5\n74.0DSC (%)\n73.31\n73.65\n73.4\n72.77\n72.4\n73.64\n73.84\n72.87\n73.16\n72.66\n73.44\n73.6\n73.3\n73.06\n72.85\nmean\nregularConv\ndepthwiseConv\ngroupConv\nFig. 6. Ablation study on different window sizes and convolution types.\n4) Ablation study on Window Size: We conduct ablation\nstudy on different window sizes with W AD and ResUNet\narchitecture on MSD Brain dataset. As per Figure 6, we ﬁnd\nthat the optimal window size is 4. Increasing the window size\nleads to a drop in the performance. We hypothesize that the\nmodel might lose focus when the window size is too large and\nthis is particularly problematic for upsampling as it depends\non the detailed and local features in neighboring regions.\nSmaller window is better as it restricts model’s attention\nspatially. However, using too small a window also degrades\nthe performance since it gets rid of the long-range dependency.\nWe can observe a drop in performance when the window size\nis set to 2.\n5) Generalizability of WAU with Different Architecture:\nWe argue that our proposed W AU can be incorporated into\nany architecture that possesses lateral connections. To prove\nthe generalizability of our proposed W AU, we incorporate\nour method into different architectures and observe consistent\nimprovements in all experiments. Speciﬁcally, we incorporate\nW AU into UNet 3D and observe an improvement of 0.36 DSC.\nThis demonstrates that our method can be used in 3D volume\nsegmentation which comprises a large category of medical\nsegmentation methods. We also observe an improvement of\nat most 5 points on the three variants of classic FCN and an\nimprovement of 1.47 on the DeepLabV3 model. Results are\ndisplayed in table V.\nE. Visualization\nIn this part, we provide visualization of Window Attention\nweights and the upsampled feature maps of different models\nin the upsampling path. To obtain our Window Attention\nweights, we retrieve the attention weights in W AD. Since\neach attention is computed inside local windows, we select\nthe activated regions (the positive region in ground truth) and\nshow the average attention weights of these windows with\npositive pixels. Feature maps after every upsample module are\nalso visualized to demonstrate the effectiveness of our method.\nFigure 5 is the visualization of our Window Attention weights\nand shows how the attention is focused on the relevant pixels\nof the target area in each window. This further demonstrates\n(a) (b) (c) (d) \nFig. 7. Qualitative results from the MSD Brain, Synapse and ACDC datasets.\nWe compare (a) Ground Truth with the outputs of (b) ResUNet, (c) TransUNet\nand (d) Ours.\nthe validation of our methods that by imposing an attention,\nmodel is prone to focus more on target. This enriches the\ninformation needed for segmentation task and leads to better\nperformance. Figure 1 presents the upsampled feature maps\nafter every upsample procedure on MSD Brain and Synapse\ndatasets. It can be seen that our upsample method, taking\nadvantage of self-attention, focuses better on target area than\npure CNN-based method (i.e. ResUNet). Also, compared with\nResUNet, our method shows a clear lesion that could further\nassist the diagnosis.\nV. C ONCLUSION\nIn this paper, we present the ﬁrst study to explore the\nadaptability of transformer decoder in segmentation and its\nusage in upsample. Our work proves that decoder can also\nbe adopted to model visual information and performs even\nbetter than traditional upsample techniques. To leverage the\nability of such architecture, we propose our Window Attention\nUpsample that reconstruct semantic pixels to desired shape\nconditioned on local and detailed information. With this, we\nprovide a better alternative to the basic upsample operation\nand can be fused in any segmentation model that requires\nupsample. Moreover, our work partly exploits the possibility\nof adopting a pure transformer with encoder and decoder into\nCV .\nACKNOWLEDGMENT\nThis work is jointly supported by Guangdong Provincial\nKey Laboratory of Artiﬁcial Intelligence in Medical Image\nAnalysis and Application (No. 2022B1212010011) and the In-\nternational Cooperation Project of Guangdong Province under\ngrants 2021A0505030017.\nREFERENCES\n[1] G. R. Arce, J. Bacca, and J. L. Paredes, “3.2 - nonlinear ﬁltering for\nimage analysis and enhancement,” in Handbook of Image and Video\nProcessing (Second Edition) , second edition ed., ser. Communications,\nNetworking and Multimedia, A. BOVIK, Ed. Burlington: Academic\nPress, 2005, pp. 109–IV . [Online]. Available: https://www.sciencedirect.\ncom/science/article/pii/B9780121197926500711\n[2] C. F. Baumgartner, L. M. Koch, M. Pollefeys, and E. Konukoglu, “An\nexploration of 2d and 3d deep learning techniques for cardiac mr image\nsegmentation,” in Statistical Atlases and Computational Models of the\nHeart. ACDC and MMWHS Challenges , M. Pop, M. Sermesant, P.-M.\nJodoin, A. Lalande, X. Zhuang, G. Yang, A. Young, and O. Bernard,\nEds. Cham: Springer International Publishing, 2018, pp. 111–119.\n[3] W. Cai, L. Xie, W. Yang, Y . Li, Y . Gao, and T. Wang, “Dftnet: Dual-\npath feature transfer network for weakly supervised medical image\nsegmentation,” IEEE/ACM Transactions on Computational Biology and\nBioinformatics, pp. 1–12, 2022.\n[4] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang,\n“Swin-unet: Unet-like pure transformer for medical image segmenta-\ntion,” arXiv preprint arXiv:2105.05537 , 2021.\n[5] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L.\nYuille, and Y . Zhou, “TransUNet: Transformers Make Strong Encoders\nfor Medical Image Segmentation,” arXiv:2102.04306 [cs] , Feb. 2021,\narXiv: 2102.04306. [Online]. Available: http://arxiv.org/abs/2102.04306\n[6] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking\natrous convolution for semantic image segmentation,” arXiv preprint\narXiv:1706.05587, 2017.\n[7] ¨O. C ¸ ic ¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,\n“3d u-net: learning dense volumetric segmentation from sparse anno-\ntation,” in International conference on medical image computing and\ncomputer-assisted intervention. Springer, 2016, pp. 424–432.\n[8] Y . Dai, Y . Gao, and F. Liu, “Transmed: Transformers advance\nmulti-modal medical image classiﬁcation,” Diagnostics, vol. 11, no. 8,\n2021. [Online]. Available: https://www.mdpi.com/2075-4418/11/8/1384\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) . Minneapolis,\nMinnesota: Association for Computational Linguistics, Jun. 2019, pp.\n4171–4186. [Online]. Available: https://aclanthology.org/N19-1423\n[10] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and\nB. Guo, “Cswin transformer: A general vision transformer backbone\nwith cross-shaped windows,” in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , 2022, pp. 12 124–\n12 134.\n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” in International\nConference on Learning Representations , 2021. [Online]. Available:\nhttps://openreview.net/forum?id=YicbFdNTTy\n[13] S. Fu, Y . Lu, Y . Wang, Y . Zhou, W. Shen, E. Fishman, and A. Yuille,\n“Domain adaptive relational reasoning for 3d multi-organ segmenta-\ntion,” in International Conference on Medical Image Computing and\nComputer-Assisted Intervention. Springer, 2020, pp. 656–666.\n[14] A. Hatamizadeh, D. Yang, H. Roth, and D. Xu, “UNETR:\nTransformers for 3D Medical Image Segmentation,” arXiv:2103.10504\n[cs, eess] , Mar. 2021, arXiv: 2103.10504. [Online]. Available:\nhttp://arxiv.org/abs/2103.10504\n[15] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770–778.\n[16] X. He, S. Yang, G. Li, H. Li, H. Chang, and Y . Yu, “Non-local context\nencoder: Robust biomedical image segmentation against adversarial at-\ntacks,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nvol. 33, no. 01, 2019, pp. 8417–8424.\n[17] Y . He, D. Yang, H. Roth, C. Zhao, and D. Xu, “Dints: Differentiable\nneural network topology search for 3d medical image segmentation,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 5841–5850.\n[18] J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Salimans, “Ax-\nial attention in multidimensional transformers,” arXiv preprint\narXiv:1912.12180, 2019.\n[19] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 7132–7141.\n[20] X. Hu, H. Mu, X. Zhang, Z. Wang, T. Tan, and J. Sun, “Meta-sr: A\nmagniﬁcation-arbitrary network for super-resolution,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2019.\n[21] H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang, Y . Iwamoto, X. Han,\nY .-W. Chen, and J. Wu, “UNet 3+: A Full-Scale Connected UNet\nfor Medical Image Segmentation,” in ICASSP 2020 - 2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). Barcelona, Spain: IEEE, May 2020, pp. 1055–1059.\n[Online]. Available: https://ieeexplore.ieee.org/document/9053405/\n[22] Z. Huang, X. Wang, L. Huang, C. Huang, Y . Wei, and W. Liu, “Ccnet:\nCriss-cross attention for semantic segmentation,” in Proceedings of the\nIEEE/CVF international conference on computer vision , 2019, pp. 603–\n612.\n[23] F. Isensee, J. Petersen, A. Klein, D. Zimmerer, P. F. Jaeger, S. Kohl,\nJ. Wasserthal, G. Koehler, T. Norajitra, S. Wirkert, and K. H.\nMaier-Hein, “nnU-Net: Self-adapting Framework for U-Net-Based\nMedical Image Segmentation,” arXiv:1809.10486 [cs] , Sep. 2018,\narXiv: 1809.10486. [Online]. Available: http://arxiv.org/abs/1809.10486\n[24] D. Karimi, S. Vasylechko, and A. Gholipour, “Convolution-Free\nMedical Image Segmentation using Transformers,” arXiv:2102.13645\n[cs, eess] , Feb. 2021, arXiv: 2102.13645. [Online]. Available:\nhttp://arxiv.org/abs/2102.13645\n[25] R. Keys, “Cubic convolution interpolation for digital image processing,”\nIEEE transactions on acoustics, speech, and signal processing , vol. 29,\nno. 6, pp. 1153–1160, 1981.\n[26] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track\nProceedings, Y . Bengio and Y . LeCun, Eds., 2015. [Online]. Available:\nhttp://arxiv.org/abs/1412.6980\n[27] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\nV . Stoyanov, and L. Zettlemoyer, “BART: Denoising sequence-to-\nsequence pre-training for natural language generation, translation,\nand comprehension,” in Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics . Online: Association\nfor Computational Linguistics, Jul. 2020, pp. 7871–7880. [Online].\nAvailable: https://aclanthology.org/2020.acl-main.703\n[28] S. Li, X. Sui, X. Luo, X. Xu, Y . Liu, and R. Goh, “Medical image\nsegmentation using squeeze-and-expansion transformers,”arXiv preprint\narXiv:2105.09511, 2021.\n[29] Y . Li, H. Mao, R. Girshick, and K. He, “Exploring plain vi-\nsion transformer backbones for object detection,” arXiv preprint\narXiv:2203.16527, 2022.\n[30] S. Liu, D. Xu, S. K. Zhou, O. Pauly, S. Grbic, T. Mertelmeier,\nJ. Wicklein, A. Jerebko, W. Cai, and D. Comaniciu, “3d anisotropic\nhybrid network: Transferring convolutional features from 2d images to\n3d anisotropic volumes,” in International Conference on Medical Image\nComputing and Computer-Assisted Intervention . Springer, 2018, pp.\n851–858.\n[31] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” arXiv preprint arXiv:2103.14030 , 2021.\n[32] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2015, pp. 3431–3440.\n[33] I. Loshchilov and F. Hutter, “SGDR: stochastic gradient descent\nwith warm restarts,” in 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings . OpenReview.net, 2017. [Online].\nAvailable: https://openreview.net/forum?id=Skq89Scxx\n[34] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional\nneural networks for volumetric medical image segmentation,” in 2016\nfourth international conference on 3D vision (3DV) . IEEE, 2016, pp.\n565–571.\n[35] A. Myronenko, “3d mri brain tumor segmentation using autoencoder reg-\nularization,” in International MICCAI Brainlesion Workshop. Springer,\n2018, pp. 311–320.\n[36] N. M. Nguyen and N. Ray, “End-to-end learning of convolutional\nneural net and dynamic programming for left ventricle segmentation,”\nin Proceedings of the Third Conference on Medical Imaging with Deep\nLearning, ser. Proceedings of Machine Learning Research, T. Arbel,\nI. Ben Ayed, M. de Bruijne, M. Descoteaux, H. Lombaert, and C. Pal,\nEds., vol. 121. PMLR, 06–08 Jul 2020, pp. 555–569. [Online].\nAvailable: https://proceedings.mlr.press/v121/nguyen20a.html\n[37] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa,\nK. Mori, S. McDonagh, N. Y . Hammerla, B. Kainz et al. , “Atten-\ntion u-net: Learning where to look for the pancreas,” arXiv preprint\narXiv:1804.03999, 2018.\n[38] O. Petit, N. Thome, C. Rambour, and L. Soler, “U-Net Transformer:\nSelf and Cross Attention for Medical Image Segmentation,”\narXiv:2103.06104 [cs, eess] , Mar. 2021, arXiv: 2103.06104. [Online].\nAvailable: http://arxiv.org/abs/2103.06104\n[39] H. Rao, S. Wang, X. Hu, M. Tan, Y . Guo, J. Cheng, X. Liu, and B. Hu,\n“A self-supervised gait encoding approach with locality-awareness for 3d\nskeleton based person re-identiﬁcation,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , pp. 1–1, 2021.\n[40] H. Rao, S. Xu, X. Hu, J. Cheng, and B. Hu, “Augmented skeleton based\ncontrastive action learning with momentum lstm for unsupervised action\nrecognition,” Information Sciences, vol. 569, pp. 90–109, 2021.\n[41] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in International Conference on\nMedical image computing and computer-assisted intervention. Springer,\n2015, pp. 234–241.\n[42] A. Roy, M. Saffar, A. Vaswani, and D. Grangier, “Efﬁcient content-\nbased sparse attention with routing transformers,” Transactions of the\nAssociation for Computational Linguistics , vol. 9, pp. 53–68, 2021.\n[Online]. Available: https://aclanthology.org/2021.tacl-1.4\n[43] W. Shi, J. Caballero, F. Husz ´ar, J. Totz, A. P. Aitken, R. Bishop,\nD. Rueckert, and Z. Wang, “Real-time single image and video super-\nresolution using an efﬁcient sub-pixel convolutional neural network,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 1874–1883.\n[44] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani,\nB. Van Ginneken, A. Kopp-Schneider, B. A. Landman, G. Litjens,\nB. Menze et al. , “A large annotated medical image dataset for the\ndevelopment and evaluation of segmentation algorithms,” arXiv preprint\narXiv:1902.09063, 2019.\n[45] Z. Tian, T. He, C. Shen, and Y . Yan, “Decoders matter for semantic\nsegmentation: Data-dependent decoding enables ﬂexible feature aggre-\ngation,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2019, pp. 3126–3135.\n[46] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nu. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings\nof the 31st International Conference on Neural Information Processing\nSystems, ser. NIPS’17. Red Hook, NY , USA: Curran Associates Inc.,\n2017, p. 6000–6010.\n[47] J. Wang, K. Chen, R. Xu, Z. Liu, C. C. Loy, and D. Lin, “Carafe:\nContent-aware reassembly of features,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 2019, pp. 3007–3016.\n[48] W. Wang, C. Chen, M. Ding, J. Li, H. Yu, and S. Zha, “TransBTS:\nMultimodal Brain Tumor Segmentation Using Transformer,”\narXiv:2103.04430 [cs] , Mar. 2021, arXiv: 2103.04430. [Online].\nAvailable: http://arxiv.org/abs/2103.04430\n[49] W. Wang, C. Chen, M. Ding, H. Yu, S. Zha, and J. Li, “Transbts:\nMultimodal brain tumor segmentation using transformer,” in Interna-\ntional Conference on Medical Image Computing and Computer-Assisted\nIntervention. Springer, 2021, pp. 109–119.\n[50] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\nworks,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7794–7803.\n[51] X. Wang, X. Yang, S. Zhang, Y . Li, L. Feng, S. Fang, C. Lyu, K. Chen,\nand W. Zhang, “Consistent targets provide better supervision in semi-\nsupervised object detection,” arXiv preprint arXiv:2209.01589 , 2022.\n[52] Z. Wang, N. Zou, D. Shen, and S. Ji, “Non-local u-nets for biomed-\nical image segmentation,” in Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, vol. 34, no. 04, 2020, pp. 6315–6322.\n[53] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\n“Cvt: Introducing convolutions to vision transformers,” in Proceedings\nof the IEEE/CVF International Conference on Computer Vision , 2021,\npp. 22–31.\n[54] L. Xie, W. Cai, and Y . Gao, “Dmcgnet: A novel network for medical\nimage segmentation with dense self-mimic and channel grouping mech-\nanism,” IEEE Journal of Biomedical and Health Informatics , vol. 26,\nno. 10, pp. 5013–5024, 2022.\n[55] X. Xie, Y . Li, Y . Gao, C. Wu, P. Gao, B. Song, W. Wang,\nand Y . Lu, “Weakly supervised object localization with soft\nguidance and channel erasing for auto labelling in autonomous\ndriving systems,” ISA Transactions , 2022. [Online]. Available: https:\n//www.sciencedirect.com/science/article/pii/S0019057822004001\n[56] S. Xu, H. Rao, H. Peng, X. Jiang, Y . Guo, X. Hu, and B. Hu, “Attention\nbased multi-level co-occurrence graph convolutional lstm for 3d action\nrecognition,” IEEE Internet of Things Journal , pp. 1–1, 2020.\n[57] J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, and J. Gao, “Focal\nself-attention for local-global interactions in vision transformers,” arXiv\npreprint arXiv:2107.00641, 2021.\n[58] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-\ntional networks,” in European conference on computer vision. Springer,\n2014, pp. 818–833.\n[59] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, “Decon-\nvolutional networks,” in 2010 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition , 2010, pp. 2528–2535.\n[60] M. D. Zeiler, G. W. Taylor, and R. Fergus, “Adaptive deconvolutional\nnetworks for mid and high level feature learning,” in 2011 International\nConference on Computer Vision , 2011, pp. 2018–2025.\n[61] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang,\n“Unet++: A nested u-net architecture for medical image segmentation,”\nin Deep Learning in Medical Image Analysis and Multimodal Learning\nfor Clinical Decision Support , D. Stoyanov, Z. Taylor, G. Carneiro,\nT. Syeda-Mahmood, A. Martel, L. Maier-Hein, J. M. R. Tavares,\nA. Bradley, J. P. Papa, V . Belagiannis, J. C. Nascimento, Z. Lu,\nS. Conjeti, M. Moradi, H. Greenspan, and A. Madabhushi, Eds. Cham:\nSpringer International Publishing, 2018, pp. 3–11.\n[62] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++:\nA Nested U-Net Architecture for Medical Image Segmentation,”\narXiv:1807.10165 [cs, eess, stat] , Jul. 2018, arXiv: 1807.10165.\n[Online]. Available: http://arxiv.org/abs/1807.10165\nAPPENDIX A\nABLATION STUDY ON DIFFERENT KERNEL SIZES\n1×1 3×3 5×5 7×7\nConv. Size\n74.60\n74.65\n74.70\n74.75\n74.80\n74.85\n74.90\n74.95DSC (%)\n74.6\n74.75\n74.57\n74.94\nFig. 8. Ablation study on different kernel sizes of convolution projection.\nWe conduct an ablation study on different kernel sizes\nusing W AU with a base window size of 4 on the MSD Brain\ndataset. Figure 8 shows the performance of different kernel\nsizes in convolution projection, where we can ﬁnd that 7 ×7\nconvolution kernels can achieve slightly better performance\nthan the 3 ×3 convolution kernels. For simplicity, we leverage\n3×3 convolution kernels in W AU.\nAPPENDIX B\nIMPLEMENTATION DETAILS OF DIFFERENT\nARCHITECTURES\nIn Table V we demonstrate the generalizability of our\nproposed W AU method. Here we provide the details of imple-\nmentation for incorporating W AU into different architectures.\nWe ﬁrst discuss the modiﬁcations from UNet to ResUNet.\nWe add a residual connection via an additional 3 ×3 convo-\nlution layer on every two convs block of the original UNet.\nThe rest remains the same as UNet. For the UNet family,\nincluding UNet, ResUNet, and UNet 3D, we directly leverage\nits skip connections from downsampling path to upsampling\npath to form qurey vector. We use the feature maps in lower\nresolution from the previous layer to form key and value\nvectors. According to Equation 5, the low-resolution feature\nmap will be upsampled conditioned on the larger feature\nmap from the downsampling path. The overall architecture\nis illustrated in Figure 4.\nFor the DeepLab family, the qurey vector is acquired\nfrom the encoder. We utilize the feature maps with the same\nresolution as the input images. This is also the expected\nresolution of the upsampled feature maps. The key and value\nvectors are formed by the output of the ASPP module. The\nencoded feature maps will be upsampled only once by 16 ×\nvia W AU.\nThe FCN methods are similar to the DeepLab series. We\nreplace the original 32 ×, 16×, and 8 ×upsampling by W AU\nwith query feature from the encoder. For FCN 32s, we utilize\nW AU only once and upsample the feature maps to input sizes.\nFor FCN 16s and 8s, multiple W AU modules are inserted to\nreplace the original upsample module. Each W AU leverages\nthe feature maps from a speciﬁc layer of the encoder to\ngenerate query.",
  "topic": "Upsampling",
  "concepts": [
    {
      "name": "Upsampling",
      "score": 0.7481074333190918
    },
    {
      "name": "Segmentation",
      "score": 0.6937890648841858
    },
    {
      "name": "Computer science",
      "score": 0.6688829660415649
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6192178726196289
    },
    {
      "name": "Pixel",
      "score": 0.5447443723678589
    },
    {
      "name": "Image segmentation",
      "score": 0.5297074317932129
    },
    {
      "name": "Encoder",
      "score": 0.48167479038238525
    },
    {
      "name": "Computer vision",
      "score": 0.4554252028465271
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.4274730384349823
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34133589267730713
    },
    {
      "name": "Image (mathematics)",
      "score": 0.16457727551460266
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}