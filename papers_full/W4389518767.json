{
  "title": "Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding",
  "url": "https://openalex.org/W4389518767",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2079979880",
      "name": "Shuwen Deng",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A2508825474",
      "name": "Paul Prasse",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A2013158180",
      "name": "David Reich",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A2599020151",
      "name": "Tobias Scheffer",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A3177161686",
      "name": "Lena Jäger",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4224945216",
    "https://openalex.org/W2964194677",
    "https://openalex.org/W2886369120",
    "https://openalex.org/W4293612949",
    "https://openalex.org/W2949472041",
    "https://openalex.org/W3092785544",
    "https://openalex.org/W4386566666",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4389524564",
    "https://openalex.org/W2545936089",
    "https://openalex.org/W2898936689",
    "https://openalex.org/W3204795127",
    "https://openalex.org/W2067922586",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W285890983",
    "https://openalex.org/W3102610439",
    "https://openalex.org/W4294925020",
    "https://openalex.org/W2760692555",
    "https://openalex.org/W3104502007",
    "https://openalex.org/W2153076044",
    "https://openalex.org/W4377014233",
    "https://openalex.org/W2763768707",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2739890004",
    "https://openalex.org/W4317829471",
    "https://openalex.org/W4385803064",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2788643701"
  ],
  "abstract": "Human gaze data offer cognitive information that reflects natural language comprehension. Indeed, augmenting language models with human scanpaths has proven beneficial for a range of NLP tasks, including language understanding. However, the applicability of this approach is hampered because the abundance of text corpora is contrasted by a scarcity of gaze data. Although models for the generation of human-like scanpaths during reading have been developed, the potential of synthetic gaze data across NLP tasks remains largely unexplored. We develop a model that integrates synthetic scanpath generation with a scanpath-augmented language model, eliminating the need for human gaze data. Since the model's error gradient can be propagated throughout all parts of the model, the scanpath generator can be fine-tuned to downstream tasks. We find that the proposed model not only outperforms the underlying language model, but achieves a performance that is comparable to a language model augmented with real human gaze data. Our code is publicly available.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6500–6507\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nPre-Trained Language Models Augmented with Synthetic Scanpaths for\nNatural Language Understanding\nShuwen Deng1, Paul Prasse1, David R. Reich1, Tobias Scheffer1, Lena A. Jäger1,2\n1 Department of Computer Science, University of Potsdam, Germany\n2 Department of Computational Linguistics, University of Zurich, Switzerland\n{deng, prasse, david.reich, tobias.scheffer}@uni-potsdam.de\njaeger@cl.uzh.ch\nAbstract\nHuman gaze data offer cognitive information\nthat reflects natural language comprehension.\nIndeed, augmenting language models with hu-\nman scanpaths has proven beneficial for a range\nof NLP tasks, including language understand-\ning. However, the applicability of this approach\nis hampered because the abundance of text cor-\npora is contrasted by a scarcity of gaze data.\nAlthough models for the generation of human-\nlike scanpaths during reading have been devel-\noped, the potential of synthetic gaze data across\nNLP tasks remains largely unexplored. We\ndevelop a model that integrates synthetic scan-\npath generation with a scanpath-augmented lan-\nguage model, eliminating the need for human\ngaze data. Since the model’s error gradient\ncan be propagated throughout all parts of the\nmodel, the scanpath generator can be fine-tuned\nto downstream tasks. We find that the proposed\nmodel not only outperforms the underlying lan-\nguage model, but achieves a performance that\nis comparable to a language model augmented\nwith real human gaze data. Our code is publicly\navailable.1\n1 Introduction and Related Work\nWhen humans read, they naturally engage in the\ncognitive process of comprehending language,\nwhich, in turn, is reflected in their gaze behav-\nior (Just and Carpenter, 1980). In a nutshell, a\nscanpath (i.e., sequence of consecutive fixations)\non a stimulus text approximates the reader’s at-\ntention, which can be exploited to inform Natural\nLanguage Processing (NLP) tasks.\nGaze data has been shown to be beneficial in vari-\nous NLP tasks, such as part-of-speech-tagging (Bar-\nrett et al., 2016), named entity recognition (Hol-\nlenstein and Zhang, 2019), generating image cap-\ntions (Takmaz et al., 2020) and question answer-\ning (Sood et al., 2021). Researchers have explored\n1https://github.com/aeye-lab/\nEMNLP-SyntheticScanpaths-NLU-PretrainedLM .\nLanguage Model (BERT)\n[CLS]Tok 1Tok 2Tok 3Tok 4Tok 5[SEP]\nC T1 T2 T3 T4 T5 T[SEP]\nScanpath Encoder (GRU)\nT2 T2 T4 T5 T3\nFixation index:\nEyettention\n2 2 4 5 3\nScanpath Generation Model\nScanpath-Augmented Language Model\nrearrange\nC\nPrediction\nFigure 1: Synthetic scanpath-augmented language\nmodel: the Scanpath Generation Model predicts a se-\nquence of fixations for an input sentence; token embed-\ndings are rearranged according to the order of fixations.\nthe use of aggregated word-level gaze features\nto regularize neural attention mechanisms (Bar-\nrett et al., 2018; Sood et al., 2020). Moreover,\nnon-aggregated scanpaths, which capture the com-\nplete sequential ordering of the reader’s gaze be-\nhavior, have also demonstrated promise in NLP\ntasks (Mishra et al., 2017, 2018a; Yang and Hollen-\nstein, 2023).\nHowever, collecting gaze data is a resource-\nintensive endeavor, even for very small text corpora.\nHence, human gaze data is scarce, and NLP task-\nspecific gaze recordings are even scarcer. More-\nover, applying a language model that additionally\nconsumes gaze data requires gaze data to be avail-\nable for the input text at deployment time—which\nis unrealistic for most use cases. To overcome these\nlimitations, researchers have proposed a multi-task\nlearning approach for NLP tasks such as sentence\ncompression (Klerke et al., 2016), sentiment analy-\nsis (Mishra et al., 2018b), and predicting text read-\nability (González-Garduño and Søgaard, 2017). In\nthis approach, labeled data for the specific NLP\ntask is used as the primary task, while a separate\neye-tracking corpus is utilized as an auxiliary task.\nWhile this approach helps mitigate the need for\ntask-specific gaze data during training and testing,\nthe problem of general scarcity of gaze samples\n6500\nremains and hinders effective supervision for data-\nintensive architectures.\nIn this paper, we propose an alternative approach\nby using synthetic gaze data, which can be gener-\nated easily for any given text, to provide cogni-\ntive signals across NLP tasks. The seminal work\nof Sood et al. (2020), which integrates eye move-\nment data generated by a computational cognitive\nmodel of eye-movement-control-during-reading for\ntasks such as sentence compression and paraphrase\ngeneration, demonstrated the potential of synthetic\neye-gaze data. Khurana et al. (2023) explored a\nproof-of-concept model that integrated synthetic\ngaze data across multiple NLP tasks, but their re-\nsults did not reach the performance of a fine-tuned\nBERT model (Devlin et al., 2019) without eye gaze\non the General Language Understanding Evalua-\ntion (GLUE) benchmark. In our work, we build\non recent advances in the development of machine-\nlearning models for generating human-like scan-\npaths during reading (Deng et al., 2023; Bolliger\net al., 2023; Khurana et al., 2023; Nilsson and\nNivre, 2011).\nWe develop a model that combines synthetic\nscanpath generation with a scanpath-augmented\nlanguage model, eliminating the need for human\ngaze data. The model allows for fine-tuning the\nscanpath generator to downstream tasks by propa-\ngating the error gradient through the entire model.\nOur approach not only outperforms the underlying\nlanguage model in multiple tasks on the GLUE, es-\npecially in low-resource settings, but even reaches a\nperformance comparable to an eye-gaze augmented\nmodel that uses real, rather than synthetic, eye\nmovement data in sentiment classification.\n2 Model\nWe develop a model that combines a scanpath gen-\neration model with a scanpath-augmented language\nmodel to perform NLP downstream tasks. Figure 1\ndepicts the proposed model architecture.\nScanpath Generation Model We adopt Eyetten-\ntion (Deng et al., 2023), an open-source state-of-\nthe-art model for scanpath generation over text.\nEyettention predicts consecutive fixation locations,\nrepresented as word indices, based on a stimulus\nsentence and the preceding fixations. It consists\nof two encoders, one for embedding the stimulus\nsentence, and the other for embedding the scanpath\nhistory. A cross-attention layer aligns the outputs\nof the two encoders, and a decoder produces a\nprobability distribution over saccade ranges at each\ntimestep. The next fixated word index is deter-\nmined by sampling from this distribution.\nScanpath-Augmented Language Model We\nadopt the PLM-AS framework (Yang and Hollen-\nstein, 2023), which augments pre-trained language\nmodels with human scanpaths for sentiment classi-\nfication. This framework uses a language model to\nextract token embeddings for a sentence, associat-\ning each embedding with its position index. By uti-\nlizing a human scanpath (fixation index sequence)\nas input, the model rearranges the token embedding\nsequence based on the order in which the words are\nfixated by the reader. The transformed sequence is\nthen fed into a scanpath encoder, implemented as\na layer of gated recurrent units (GRU), where the\noutput of the last step is used as the final feature for\nsentiment classification. This framework allows for\nthe use of different language models and achieves\nhigh performance through fine-tuning. In this work,\nwe employ BERTBASE2 (Devlin et al., 2019) as the\nlanguage model, following Yang and Hollenstein\n(2023).\nJoint Modeling for NLP Tasks To eliminate the\nneed for human gaze data, we integrate the syn-\nthetic scanpath generated by the Eyettention model\nconsisting of a fixation index sequence into the\nPLM-AS framework. Before integration, the word\nindex sequence generated by Eyettention is con-\nverted into a token index sequence. During train-\ning, the error gradient of the scanpath-augmented\nlanguage model can be back-propagated through\nthe Eyettention model, allowing its parameters to\nbe adapted for a specific NLP task. To handle the\nnon-differentiable sampling from a categorical dis-\ntribution involved in scanpath generation, we em-\nploy the Gumbel-softmax distribution (Jang et al.,\n2017) as a fully differentiable approximation. The\ntraining process consists of two phases. First, we\npre-train the Eyettention model on a natural read-\ning task. Second, we train the entire model, which\nincludes fine-tuning the language model and the\nEyettention model, as well as training the scanpath\nencoder from scratch. For the Eyettention model,\nwe add residual connections in both encoders to\nenhance its performance.\n2Note that BERT can be substituted with other advanced\npre-trained language models, potentially leading to further\nenhancements in task performance.\n6501\n3 Experiments\nIn this section, we describe the data and present the\nevaluation results of our model for a wide range of\nNLP tasks. Further details about training and hyper-\nparameter tuning can be found in Appendix B.\n3.1 Data Sets\nCELER (Berzak et al., 2022): We pre-train the\nscanpath generation model Eyettention on the L1\nsubset of CELER, which contains eye-tracking\nrecordings collected from 69 native speakers of\nEnglish during natural reading of 5,456 sentences.\nETSA (Mishra et al., 2016) contains task-\nspecific gaze recordings for sentiment classifica-\ntion of 7 subjects who each read 383 positive and\n611 negative sentences, including sarcastic quotes,\nshort movie reviews, and tweets.\nGLUE (Wang et al., 2018) includes sentiment\nanalysis (SST-2), linguistic acceptability (CoLA),\nsimilarity and paraphrase tasks (MRPC, STS-B,\nQQP), and natural language inference tasks (MNLI,\nQNLI, RTE). No gaze data are available.\n3.2 Sentiment Classification\nTable 1 presents the results of our model on the\nsentiment classification task ETSA (Mishra et al.,\n2016), in comparison to BERT and previous state-\nof-the-art eye-gaze augmented models. We follow\na 10-fold cross-validation regime. In each iteration,\nBERT is fine-tuned on the training portion of the\nETSA text corpus, and PLM-AS is fine-tuned on\nthe training portion of the ETSA text corpus and\ngaze data. Our model is fine-tuned on the training\nportion of the ETSA text corpus and, instead of the\nETSA gaze data, synthetic gaze data generated by\nEyettention. Since each sentence is associated with\nmultiple scanpaths, we compute the final predic-\ntion by averaging the pre-softmax logits obtained\nfrom the models across all scanpaths for the PLM-\nAS baseline. Our model averages equally many\nsynthetic scanpaths. We make multiple notable ob-\nservations in Table 1:\n(a) Our model outperforms both BERT and the\nstate-of-the-art ScanTextGAN (Khurana et al.,\n2023) augmented with gaze data.\n(b) Our model, augmented withsynthetic scanpaths,\nachieves comparable performance to the PLM-AS\nmodel augmented with human scanpaths, eliminat-\ning the need for human scanpaths.\n(c) Ablation experiments (bottom two rows) show\nthat when the Eyettention model is frozen or\nModel Scanpath (#) F1 AUC\nBERT⋆ - 82.93 2.26 92.421.62\nScanTextGAN real 83.34 -\nScanTextGAN synthetic 84.77 -\nPLM-AS⋆ real (7) 85.811.16 94.791.02\nOurs⋆ synthetic (7) 85.35 1.77 94.900.94\nEyettention (frozen)⋆ synthetic (7) 84.52 1.79 94.501.03\nEyettention (scratch)⋆ synthetic (7) 85.03 1.6 94.771.03\nTable 1: Results for sentiment classification on ETSA,\nwith standard errors indicated as subscript. Results ob-\ntained from our experiments are marked with ⋆; other\nresults are from the respective papers for recapitulation.\n1 3 5 7 9 11 13 15 17 19\n# Scanpaths\n80\n82\n84\n86F1\nBERT\nPLM-AS w/ human scanpaths\nOurs w/ synthetic scanpaths\nFigure 2: Sentiment classification performance on\nETSA with varying numbers of scanpaths at training\nand application time. Error bars show the standard error.\nnot pre-trained, the performance decreases. This\ndemonstrates the importance of both pre-training\nand task-specific fine-tuning of the scanpath gener-\nator.\nVarying the number of scanpaths We analyze\nthe impact of the number of scanpaths sampled\nboth at training and at application time on model\nperformance. Figure 2 shows the F1 score as a\nfunction of the number of scanpaths used by BERT\nwithout eye gaze, PLM-AS with human scanpaths,\nand our model with synthetic scanpaths. We ob-\nserve that the performance of scanpath-augmented\nmodels improves as the number of scanpaths in-\ncreases, reaching its peak at seven scanpaths.3 Im-\nportantly, our model outperforms BERT and, when\nbeing augmented with five or more synthetic scan-\npaths, approaches the performance of PLM-AS\naugmented with human scanpaths.\nLow-Resource Performance We hypothesize\nthat eye gaze might be most beneficial in low-\nresource settings. To test this hypothesis, we sam-\nple a small subset of the training sentences K =\n{200, 400, 600} from the total number of around\n800 training instances, and evaluate the perfor-\nmance of our model augmented with seven syn-\n3The optimal number of scanpaths to be used by the model\nis considered a hyperparameter for the subsequent experi-\nments.\n6502\nMNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.\nK Model Gaze 392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\n200\nBERT × 42.901.51 57.422.03 73.070.16 78.781.10 16.952.74 79.430.69 81.180.04 54.301.50 60.50\nOurs ✓ 48.970.83 61.631.78 70.460.62 80.760.74 24.083.55 74.941.20 81.850.17 59.351.47 62.75\n500\nBERT × 52.091.05 65.130.37 77.040.19 82.550.47 35.611.74 83.140.41 81.530.29 60.720.61 67.23\nOurs ✓ 56.480.38 67.810.23 77.600.26 84.630.50 36.411.39 81.990.58 82.320.52 61.881.24 68.64\n1000\nBERT × 58.970.58 67.350.49 78.880.36 85.800.55 39.891.64 85.420.21 84.181.00 63.390.99 70.49\nOurs ✓ 61.280.25 70.650.14 80.740.10 86.060.29 41.190.50 85.130.43 84.610.68 64.551.18 71.78\nall BERT × 82.9 69.7 90.1 93.1 53.9 84.8 87.7 66.1 78.54\nOurs ✓ 83.6 69.6 90.1 93.8 50.2 85.8 87.7 67.3 78.51\nTable 2: Results on the GLUE benchmark with K = {200, 500, 1000, all} training samples. Below each task, the\ntotal number of training samples for each dataset is indicated. We use F1 for QQP and MRPC, Spearman correlation\nfor STS-B, Matthews correlation for CoLA, and accuracy for the remaining tasks. The standard error is indicated as\nthe subscript.\n200 400 600 all (804)\n# Training sentence instances\n55\n60\n65\n70\n75\n80\n85F1\nBERT\nOurs w/ 7 synthetic scanpaths\nFigure 3: Sentiment classification performance on\nETSA in the low-resource setting. Error bars repre-\nsent the standard error.\nthetic scanpaths (the best-performing configuration\nfrom the previous experiments). The performance\ncomparison between our model and the baseline\nmodel BERT is shown in Figure 3. Our model con-\nsistently outperforms BERT, with larger improve-\nments observed when using less training data.\n3.3 GLUE Benchmark\nIn contrast to the small and single task-specific\nETSA data set, we extended our evaluation to as-\nsess whether gaze data could enhance language\nmodels across different tasks, including scenarios\nwith substantial text data. To achieve this, we eval-\nuate our model on the GLUE benchmark, a com-\nprehensive collection of 8 diverse NLP tasks with a\nlarge number of text samples. As no eye gaze data\nis available for GLUE, we focus on the comparison\nwith the BERT baseline, and investigate both, high-\nand low-resource settings.\nHigh-Resource Performance The results of our\nmodel on the GLUE test set using all training sam-\nples (K = all) are reported in the bottom two rows\nof Table 2. The results are obtained from the GLUE\nleaderboard. Our model outperforms BERT in 4\nout of 8 tasks, and achieves comparable perfor-\nmance in 3 tasks. However, our model’s perfor-\nmance is notably poor in the CoLA task, possibly\ndue to the model’s emphasis on gaze sequence or-\ndering, potentially overshadowing the importance\nof the original word order, which is critical to de-\ntermine linguistic acceptability of sentences.\nLow-Resource Performance We present the re-\nsults on the GLUE benchmark with K = {200, 500,\n1000} training samples in Table 2. We take ad-\nditional 1,000 samples from the original training\nset as the development set used for early stopping.\nThe original development set is utilized for testing.\nWe perform 5 runs with different random seeds to\nshuffle the data and report the average results.\nOverall, our model consistently outperforms\nBERT across tasks, except for the STS-B task. In\nterms of average score, our model shows perfor-\nmance gains of 2-4% compared to BERT.\n4 Discussion and Conclusion\nWe developed a model that integrates synthetic\nscanpath generation into a scanpath-augmented lan-\nguage model. We observe that the model achieves\nresults that are comparable to a language model\naugmented with human scanpaths, which elimi-\nnates the need for human scanpaths during both\ntraining and testing. Human gaze data are only\navailable for a very limited number of NLP tasks\nand data sets. At application time, under any\nstandard use case scenario of NLP tasks, no gaze\nrecordings are available. Synthetic gaze data not\nonly open the possibility to train high-capacity\ngaze-augmented models across tasks, which would\notherwise require the collection of an impractical\nlarge volume of gaze data, but also allow for the\n6503\nexploitation of eye gaze signals as model input at\napplication time.\nUsing the GLUE benchmark, we observe that\ngaze signals show benefits not only for sentiment\nclassification tasks (SST-2), as reported in previ-\nous research, but also for entailment classification\ntasks (MNLI, RTE) and a sentence similarity task\n(STS-B). This highlights the potential of integrat-\ning cognitive signals from eye gaze into a wider\nrange of NLP tasks in the future. Nevertheless, it\nis evident that not all tasks derive equal benefits\nfrom gaze data. It remains up to future research\nto explore which types of tasks benefit most from\ngaze signals.\nOur results further show that the potential bene-\nfit of augmenting language models with gaze data\nis higher for low-resource settings. Hence, we be-\nlieve that the augmentation with gaze data might\nbe particularly interesting for low-resource lan-\nguages. Two ongoing multi-lab efforts to collect\nlarge multilingual eye-tracking-while-reading cor-\npora (MECO4 and MultiplEYE5) include a range\nof low-resource languages, which will allow for\ntraining scanpath generators and augmenting lan-\nguage models with synthetic eye gaze for these\nlanguages in the near future.\nLimitations\nOne limitation of our work is that the scanpath\ngeneration model Eyettention was pre-trained on\neye-tracking data recorded on isolated sentences\n(single sentence reading paradigm). Since the ma-\njority of tasks in the GLUE benchmark involve\ntwo-sentence classification, future work could in-\nvolve pre-training the model on an eye-tracking\ndata set specifically designed for two-sentence read-\ning tasks to enhance its performance. Additionally,\nscanpath augmentation turned out to be detrimental\nto the language model’s performance for the task\nof identifying linguistically acceptable sentences\n(CoLA). This finding was to be expected as the\nactual word order is more relevant for linguistic\nacceptability of a sentence than the order in which\nthe words are fixated. Pre-training the scanpath\ngenerator on an eye-tracking corpus that includes\nboth acceptable and unacceptable sentences may be\nbeneficial for improving the model’s performance.\nFurthermore, in our proposed framework, the\nsampling process involved in scanpath generation\n4https://meco-read.com\n5https://multipleye.eu\nduring training and at inference time is not con-\nducive to a high model efficiency. Future work\ncould explore alternative scanpath generation mod-\nels that do not rely on auto-regressive architectures\nto improve efficiency.\nEthics Statement\nIt is crucial to acknowledge potential privacy risks\nin collecting, sharing, and processing human gaze\ndata. Since eye movements are highly individual,\nit can be possible to extract a participant’s identity\nfrom gaze data (Jäger et al., 2020; Makowski et al.,\n2021). Other personal information such as gen-\nder (Sammaknejad et al., 2017) and ethnicity (Blig-\nnaut and Wium, 2014) that can be detected to some\ndegree today may turn out to be extractable accu-\nrately in the future, which incurs a risk of leakage\nof personal information from gaze data. Synthetic\ngaze data can reduce the need for large-scale ex-\nperiments with human subjects, even though some\namount of human gaze data is still necessary to\ntrain generative models.\nAcknowledgements\nThis work was partially funded by the German\nFederal Ministry of Education and Research under\ngrant 01| S20043.\nReferences\nMaria Barrett, Joachim Bingel, Nora Hollenstein, Marek\nRei, and Anders Søgaard. 2018. Sequence classi-\nfication with human attention. In Proceedings of\nthe 22nd Conference on Computational Natural Lan-\nguage Learning (CoNLL), pages 302–312, Brussels,\nBelgium.\nMaria Barrett, Frank Keller, and Anders Søgaard. 2016.\nCross-lingual transfer of correlations between parts\nof speech and gaze features. In Proceedings of the\n26th International Conference on Computational Lin-\nguistics (COLING): Technical Papers, pages 1330–\n1339, Osaka, Japan.\nYevgeni Berzak, Chie Nakamura, Amelia Smith, Emily\nWeng, Boris Katz, Suzanne Flynn, and Roger Levy.\n2022. CELER: A 365-participant corpus of eye\nmovements in L1 and L2 English reading. Open\nMind, pages 1–10.\nPieter Blignaut and Daniël Wium. 2014. Eye-tracking\ndata quality as affected by ethnicity and experimental\ndesign. Behavior Research Methods, 46:67–80.\nLena S. Bolliger, David R. Reich, Patrick Haller, Debo-\nrah N. Jakobi, Paul Prasse, and Lena A. Jäger. 2023.\n6504\nScanDL: A Diffusion Model for Generating Syn-\nthetic Scanpaths on Texts. In Proceedings of Em-\npirical Methods in Natural Language Processing\n(EMNLP), Singapore, Singapore.\nKyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the properties\nof neural machine translation: Encoder–decoder ap-\nproaches. In Proceedings of SSST-8, Eighth Work-\nshop on Syntax, Semantics and Structure in Statistical\nTranslation, pages 103–111, Doha, Qatar.\nShuwen Deng, David R Reich, Paul Prasse, Patrick\nHaller, Tobias Scheffer, and Lena A Jäger. 2023. Eye-\nttention: An attention-based dual-sequence model for\npredicting human scanpaths during reading. Proceed-\nings of the ACM on Human-Computer Interaction ,\n7(ETRA):1–24.\nJacob Devlin, Ming Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies (NAACL-HLT), pages\n4171–4186, Minneapolis, MN, USA.\nAna Valeria González-Garduño and Anders Søgaard.\n2017. Using gaze to predict text readability. In Pro-\nceedings of the 12th Workshop on Innovative Use of\nNLP for Building Educational Applications, EMNLP,\npages 438–443, Copenhagen, Denmark.\nNora Hollenstein and Ce Zhang. 2019. Entity recog-\nnition at first sight: Improving NER with eye move-\nment information. In Proceedings of North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (NAACL-\nHLT), pages 1–10, Minneapolis, Minnesota.\nLena A. Jäger, Silvia Makowski, Paul Prasse, Liehr\nSascha, Maximilian Seidler, and Tobias Scheffer.\n2020. Deep Eyedentification: Biometric identifica-\ntion using micro-movements of the eye. In Machine\nLearning and Knowledge Discovery in Databases.\nECML PKDD 2019, volume 11907 of Lecture Notes\nin Computer Science, pages 299–314, Cham, Switzer-\nland. Springer International Publishing.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Cate-\ngorical reparameterization with gumbel-softmax. In\nProceedings of the 5th International Conference on\nLearning Representations (ICLR), Toulon, France.\nMarcel A Just and Patricia A Carpenter. 1980. A theory\nof reading: From eye fixations to comprehension.\nPsychological Review, 87(4):329.\nVarun Khurana, Yaman Kumar, Nora Hollenstein, Ra-\njesh Kumar, and Balaji Krishnamurthy. 2023. Syn-\nthesizing human gaze feedback for improved NLP\nperformance. In Proceedings of the 17th Confer-\nence of the European Chapter of the Association\nfor Computational Linguistics (EACL), pages 1895–\n1908, Dubrovnik, Croatia.\nSigrid Klerke, Yoav Goldberg, and Anders Søgaard.\n2016. Improving sentence compression by learning\nto predict gaze. In Proceedings of North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (NAACL-\nHLT), pages 1528–1533, San Diego, California.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In Proceedings of In-\nternational Conference on Learning Representations\n(ICLR), New Orleans, Louisiana, United States.\nSilvia Makowski, Paul Prasse, David R Reich, Daniel\nKrakowczyk, Lena A Jäger, and Tobias Scheffer.\n2021. Deepeyedentificationlive: Oculomotoric bio-\nmetric identification and presentation-attack detec-\ntion using deep neural networks. IEEE Transac-\ntions on Biometrics, Behavior, and Identity Science,\n3(4):506–518.\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-\nhairi, Hao Ma, Jiawei Han, Scott Yih, and Madian\nKhabsa. 2022. UniPELT: A unified framework for\nparameter-efficient language model tuning. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 6253–6264, Dublin, Ireland.\nAbhijit Mishra, Pushpak Bhattacharyya, Abhijit Mishra,\nand Pushpak Bhattacharyya. 2018a. Scanpath com-\nplexity: modeling reading/annotation effort using\ngaze information. Cognitively Inspired Natural Lan-\nguage Processing: An Investigation Based on Eye-\ntracking, pages 77–98.\nAbhijit Mishra, Kuntal Dey, and Pushpak Bhattacharyya.\n2017. Learning cognitive features from gaze data\nfor sentiment and sarcasm classification using con-\nvolutional neural network. In Proceedings of the\n55th Annual Meeting of the Association for Computa-\ntional Linguistics (ACL), pages 377–387, Vancouver,\nCanada.\nAbhijit Mishra, Diptesh Kanojia, and Pushpak Bhat-\ntacharyya. 2016. Predicting readers’ sarcasm under-\nstandability by modeling gaze behavior. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 30, Phoenix, Arizona, USA.\nAbhijit Mishra, Srikanth Tamilselvam, Riddhiman\nDasgupta, Seema Nagar, and Kuntal Dey. 2018b.\nCognition-cognizant sentiment analysis with multi-\ntask subjectivity summarization based on annotators’\ngaze behavior. In Proceedings of the AAAI Con-\nference on Artificial Intelligence , volume 32, New\nOrleans, Lousiana, USA.\nMattias Nilsson and Joakim Nivre. 2011. Entropy-\ndriven evaluation of models of eye movement con-\ntrol in reading. In Proceedings of the 8th Interna-\ntional NLPCS Workshop , pages 201–212, Copen-\nhagen, Denmark.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\n6505\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In Proceed-\nings of the 33rd Conference on Neural Information\nProcessing Systems (NeurIPS) , pages 8024–8035,\nVancouver, Canada.\nNegar Sammaknejad, Hamidreza Pouretemad, Changiz\nEslahchi, Alireza Salahirad, and Ashkan Alinejad.\n2017. Gender classification based on eye movements:\nA processing effect during passive face viewing. Ad-\nvances in Cognitive Psychology, 13(3):232.\nEkta Sood, Fabian Kögel, Philipp Müller, Dominike\nThomas, Mihai Bace, and Andreas Bulling. 2021.\nMultimodal integration of human-like attention in vi-\nsual question answering. Computing Research Repos-\nitory.\nEkta Sood, Simon Tannert, Philipp Müller, and Andreas\nBulling. 2020. Improving natural language process-\ning tasks with human gaze-guided neural attention.\nIn Proceedings of the 34th Conference on Neural\nInformation Processing Systems (NeurIPS) , pages\n6327–6341, Online.\nEce Takmaz, Sandro Pezzelle, Lisa Beinborn, and\nRaquel Fernández. 2020. Generating image descrip-\ntions via sequential cross-modal alignment guided by\nhuman gaze. In Proceedings of Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4664–4677, Online.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, pages 353–355,\nBrussels, Belgium.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven\nLe Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Pro-\nceedings of Empirical Methods in Natural Language\nProcessing (EMNLP): System Demonstrations, pages\n38–45, Online.\nDuo Yang and Nora Hollenstein. 2023. PLM-AS: Pre-\ntrained language models augmented with scanpaths\nfor sentiment classification. In Proceedings of the\nNorthern Lights Deep Learning Workshop, Tromsø,\nNorway.\n6506\nTable 3: Optimal number of scanpaths used for our model in GLUE Benchmark with K = {200, 500, 1000, all}\ntraining sentences.\nK MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE\n200 5 3 3 7 3 7 3 7\n500 7 5 3 3 3 7 3 7\n1000 3 5 5 7 3 5 7 7\nall 2 2 4 2 3 3 3 3\nAppendix for Pre-Trained Language\nModels Augmented with Synthetic\nScanpaths for Natural Language\nUnderstanding\nA Model Details\nPLA-AS Framework For the PLM-AS frame-\nwork, we adhere to the design of the original pa-\nper (Yang and Hollenstein, 2023). The scanpath en-\ncoder consists of a single-direction GRU layer (Cho\net al., 2014) with a hidden size of 768 and a dropout\nrate of 0.1. We initialize the hidden state of the\nscanpath encoder using the [CLS] token outputs\nfrom the final layer of BERT.\nB Training Details\nWe train all neural networks using the Py-\nTorch (Paszke et al., 2019) library on an NVIDIA\nA100-SXM4-40GB GPU using the NVIDIA\nCUDA platform. For training, we use the AdamW\noptimizer (Loshchilov and Hutter, 2019), and a\nbatch size of 32. We train 20 epochs and select\nthe model with the best validation performance\nfor evaluation. The training is early stopped if\nthe validation performance does not increase for\n3 consecutive epochs. During the training of our\nmodel, we employ the Gumbel-softmax distribu-\ntion with a temperature hyperparameter set to 0.5.\nWe use the pre-trained checkpoints from the Hug-\ngingFace repository (Wolf et al., 2020) for the lan-\nguage model BERTBASE.\nSentiment Classification During training, each\nscanpath associated with one sentence is treated\nas a separate instance. However, during evalua-\ntion, the pre-softmax logits obtained from multiple\nscanpaths associated with the same sentence are\naveraged to generate a single prediction for this sen-\ntence. We use a learning rate of 1e-5 for training\nall investigated models.\nGLUE Benchmark We evaluate each GLUE\ndata set using the metric specified in the bench-\nmark. We use the code provided in the Hugging-\nFace repository 6 to train the BERT model and\ncompute the metrics.\nIn the high-resource setting, we fine-tune the\nBERT model using the hyperparameter tuning pro-\ncedure outlined in the original paper (Devlin et al.,\n2019). We select the best learning rate from {5e-5,\n4e-5, 3e-5, 2e-5} for each task based on the perfor-\nmance on the development set. The same learning\nrate is used for training our model.\nAdditionally, for our model, we perform a hyper-\nparameter search on the development set to deter-\nmine the optimal number of scanpaths to be used\nby the model for each task. We explore different\nnumbers of scanpaths from {2, 3, 4} and select the\nconfiguration that achieves the best performance\non the development set. The optimal configuration\nfor each task can be found in Table 3.\nIn the low-resource setting, we use the same\nlearning rate that was found optimal in the high-\nresource setting for each task. Besides, we perform\na hyperparameter search on the development set,\ninvestigating different numbers of scanpaths from\n{3, 5, 7} to be used by our model. The optimal con-\nfigurations for each task can be found in Table 3.\nTo reduce variance, we apply shuffling to the\ntraining data using 5 different random seeds. We\nuse the first K samples as the new training set,\nand the subsequent 1,000 samples as the develop-\nment set. The data seeds used for shuffling are\n{111,222,333,444,555}, while the seed s=42 is con-\nsistently used for model training across all models.\nThe procedure was adapted from Mao et al. (2022).\n6https://github.com/huggingface/transformers/\ntree/main/examples/pytorch/text-classification\n6507",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8586603999137878
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6577677130699158
    },
    {
      "name": "Language model",
      "score": 0.6565331220626831
    },
    {
      "name": "Natural language generation",
      "score": 0.6550351977348328
    },
    {
      "name": "Gaze",
      "score": 0.5806432962417603
    },
    {
      "name": "Natural language processing",
      "score": 0.5551250576972961
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.47233453392982483
    },
    {
      "name": "Natural language",
      "score": 0.4395100474357605
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I176453806",
      "name": "University of Potsdam",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I202697423",
      "name": "University of Zurich",
      "country": "CH"
    }
  ],
  "cited_by": 9
}