{
  "title": "Image-Level Harmonization of Multi-site Data Using Image-and-Spatial Transformer Networks",
  "url": "https://openalex.org/W3038656636",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2097112472",
      "name": "Robert Robinson",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A2150735622",
      "name": "Qi Dou",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A2595767304",
      "name": "Daniel Coelho de Castro",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A2254647456",
      "name": "Konstantinos Kamnitsas",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A2115334546",
      "name": "Marius de Groot",
      "affiliations": [
        "GlaxoSmithKline (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2250190093",
      "name": "Ronald M Summers",
      "affiliations": [
        "National Institutes of Health Clinical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2169057019",
      "name": "Daniel Rueckert",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A1865420375",
      "name": "Ben Glocker",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A2097112472",
      "name": "Robert Robinson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150735622",
      "name": "Qi Dou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2595767304",
      "name": "Daniel Coelho de Castro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2254647456",
      "name": "Konstantinos Kamnitsas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115334546",
      "name": "Marius de Groot",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250190093",
      "name": "Ronald M Summers",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2169057019",
      "name": "Daniel Rueckert",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1865420375",
      "name": "Ben Glocker",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4252492797",
    "https://openalex.org/W2995750307",
    "https://openalex.org/W2980144156",
    "https://openalex.org/W2811386582",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2979640220",
    "https://openalex.org/W2979801810",
    "https://openalex.org/W2562469482",
    "https://openalex.org/W2911992640",
    "https://openalex.org/W2984306354",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2980223643",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2130915922",
    "https://openalex.org/W1824528708",
    "https://openalex.org/W2082704080",
    "https://openalex.org/W2522628945",
    "https://openalex.org/W2607804943",
    "https://openalex.org/W2997880344",
    "https://openalex.org/W2502884210",
    "https://openalex.org/W603908379",
    "https://openalex.org/W3041035153",
    "https://openalex.org/W2922176593",
    "https://openalex.org/W3044971416"
  ],
  "abstract": null,
  "full_text": "Image-level Harmonization of Multi-Site Data\nusing Image-and-Spatial Transformer Networks\nR. Robinson1, Q. Dou1, D. C. Castro 1, K. Kamnitsas1, M. de Groot 2,\nR.M. Summers3, D. Rueckert1, B. Glocker1\n1 BioMedIA, Department of Computing, Imperial College London, UK\n2 Research & Development, GlaxoSmithKline, UK\n3 Department of Radiology and Imaging Sciences, Clinical Center,\nNational Institutes of Health, USA\nAbstract. We investigate the use of image-and-spatial transformer net-\nworks (ISTNs) to tackle domain shift in multi-site medical imaging data.\nCommonly, domain adaptation (DA) is performed with little regard for\nexplainability of the inter-domain transformation and is often conducted\nat the feature-level in the latent space. We employ ISTNs for DA at the\nimage-level which constrains transformations to explainable appearance\nand shape changes. As proof-of-concept we demonstrate that ISTNs can\nbe trained adversarially on a classiﬁcation problem with simulated 2D\ndata. For real-data validation, we construct two 3D brain MRI datasets\nfrom the Cam-CAN and UK Biobank studies to investigate domain shift\ndue to acquisition and population diﬀerences. We show that age regres-\nsion and sex classiﬁcation models trained on ISTN output improve gen-\neralization when training on data from one and testing on the other site.\n1 Introduction\nDomain shift (DS) concerns the problem of mismatch between the statistics\nof the training data used for model development and the statistics of the test\ndata seen after model deployment. DS can cause signiﬁcant drops in predictive\nperformance, which has been observed in almost all recent imaging challenges\nwhen ﬁnal test data was coming from diﬀerent clinical sites [1]. DS is a major\nhurdle for successfully translating predictive models into clinical routine.\nAcquisition and population shift are two common forms of DS that appear\nin medical image analysis [2]. Acquisition shift is observed due to diﬀerences in\nimaging protocols, modalities or scanners. Such a shift will be observed even\nif the same subjects are scanned. Population shift occurs when cohorts of sub-\njects under investigation exhibit diﬀerent statistics, e.g., varying demographics\nor disease prevalence. It is not uncommon for both types of DS to occur simul-\ntaneously, in particular in multi-center studies. It is essential to tackle DS in\nmachine learning to perform reliable analysis of large populations across sites\nand to avoid introducing biases into results. Recent work has shown that even\nafter careful pre-processing, site-speciﬁc diﬀerences remain in the images [3,4].\narXiv:2006.16741v1  [eess.IV]  30 Jun 2020\n2 Robinson et al.\nWhile methods like ComBat [5] aim to harmonize image-derived measurements,\nwe focus on the images themselves.\nOne solution is domain adaptation (DA), a transductive [6] transfer learning\ntechnique that aims to modify the source domain’s marginal distribution of the\nfeature space such that it resembles the target domain. In medical imaging,\nlabelled data is scarce and typically unavailable for the target domain. It is\nalso unlikely to have the same subjects in both domains. Thus, we focus on\n‘unsupervised’ and ‘unpaired’ DA, wherein labelled data is available only in the\nsource domain and no matching samples exist between source and target.\nMany DA approaches focus on learning domain-invariant feature represen-\ntations, by either forcing latent representations of the inputs to follow similar\ndistributions, or ‘disentangling’ domain-speciﬁc features from generic features\n[7]. This can be achieved with some divergence measure based on data statistics\nor by training adversarial networks to model the divergence between the feature\nrepresentations [8]. These methods have been applied to brain lesions [9] and\ntumours [10] in MRI, and in contrast to non-contrast CT segmentation [11]\nWhile these approaches seem appealing and have shown some success, they\nlack a notion of explainability as it is diﬃcult to know what transformations are\napplied to the feature space. Additionally, although the learned task model may\nperform equally well on both domains, it is not guaranteed to perform as well\nas separate models trained on the individual domains.\nWe explore model-agnostic DA by working at the image level. Our approach is\nbased on domain mapping (DM), which aims to learn the pixel-level transforma-\ntions between two image domains, and includes techniques such as style transfer.\nPix2Pix [12] (supervised) and CycleGAN [13] (unsupervised) take images from\none domain through some encoder-decoder architecture to produce images in\nthe new domain. The method in [8] uses CycleGAN to improve segmentation\nacross scanners and applies DA at both image and feature levels, thus losing\ninterpretability. It does not decompose the image and spatial transformations.\nMethods for DM primarily use UNet-like architectures to learn image-to-\nimage transformations that are easier to interpret, as one can visually inspect\nthe output. For medical images of the same anatomy, but from diﬀerent scan-\nners, we assume that domain shift manifests primarily in appearance changes\n(contrast, signal-to-noise, resolution) and anatomical variation (shape changes),\nplus further subtle variations caused by image reconstruction or interpolation.\nContributions: We propose the use of image-and-spatial transformer networks\n(ISTNs) [14] to tackle domain shift at image-feature level in multi-site imaging\ndata. ISTNs separate and compose the transformations for adapting appearance\nand shape diﬀerences between domains. We believe our approach is the ﬁrst to\nuse such an approach with retraining of the downstream task model on images\ntransferred from source to target. We show that ISTNs can be trained adver-\nsarially in a task model-agnostic way. The transferred images can be visually\ninspected, and thus, our approach adds explainability to domain adaptation—\nwhich is important for validating the plausibility of the learned transformations.\nImage-level Harmonization of Multi-Site Data using ISTNs 3\nFig. 1: (left) The domain shift problem can be mitigated by retraining or ﬁnetuning a\ntask model on images S2T. (Middle) The ISTN is trained adversarially such that the\ndiscriminator D becomes better at identifying real ( S and T) and transformed ( S2T)\nimages. The ISTN simultaneously produces better transformations S2T of S that look\nmore like the images T. The training process can also be done bidirectionally (right).\nOur results demonstrate the successful recovery of performance on classiﬁcation\nand regression tasks when using ISTNs to tackle domain shift. We explore both\nunidirectional and bidirectional training schemes and compare retraining the\ntask model from scratch versus ﬁnetuning. We present proof-of-concept results\non synthetic images generated with Morpho-MNIST [15] for a 3-class classi-\nﬁcation task. Our method is then validated on real multi-site data with 3D\nT1-weighted brain MRI. Our results indicate that ISTNs improve generalization\nand predictive performance can be recovered close to single-site accuracy.\n2 Method\nWe propose adversarial training of ISTNs to perform model-agnostic DA via ex-\nplicit appearance and shape transformations between the domains. We explore\nunidirectional and bidirectional training schemes as illustrated in Figure 1.\nModels. ISTNs have two components: an image transformer network (ITN)\nand a spatial transformer network (STN) [16,14]. Here, we additionally require\na discriminator model for adversarial training of the ISTN.\nITN: The ITN performs appearance transformations such as contrast and\nbrightness changes, and other localised adaptations at the image-level. A com-\nmon image-to-image (I2I) translation network based on UNet with residual skip\nconnections can be employed. We use upsample-convolutions to reduce chequer-\nboard artifacts compared with transposed convolution. We use batch normaliza-\ntion, dropout layers and ReLU activations with a ﬁnal tanh activation for the\noutput. All input images are pre-normalized to the [ −1,1] intensity range.\n4 Robinson et al.\nSTN: We experiment with both the aﬃne and B-spline STNs described in the\noriginal ISTN paper. Aﬃne STNs learn to regress the parameters of linear spa-\ntial transforms with translation, rotation, scaling, and shearing. B-spline STNs\nregress control point displacements. Linear interpolation is used throughout.\nNote that in this work, Aﬃne and B-Spline STNs are considered independently\nand are not composed.\nDiscriminator: In both Morpho-MNIST and brain MRI experiments, we use a\nstandard fully-convolutional classiﬁcation network with instance normalization,\ndropout layers and a sigmoid output.\nTask models: The employed classiﬁers and regressors follow the same fully-\nconvolutional structure as the discriminator, reducing the dimensions of the\ninput images to a multi-class or continuous value prediction, depending on the\ntask. We use cross-entropy or mean-squared error loss functions, respectively.\nAppendices C and D provide details about the architectures of diﬀerent net-\nworks. All implementations are in PyTorch [17] with code available online. 1.\nTraining. The output from the ITN is directly fed into the STN. They are\nthen composed into a single ISTN unit, and are trained jointly end-to-end. Dis-\ncriminator: The images S (from the source domain) are passed through the\nISTN to generate images S2T, where T indicates images from the target do-\nmain. Next, the S2T are passed through the discriminator DT to yield a score\nin the range (0 ,1) denoting whether the image is a real sample from domain\nT or a transformed one. The discriminator is trained by minimizing the binary\ncross-entropy loss Lbce between the predicted and true domain labels. Eq. (1)\nshows the total discriminator loss. Soft labels for the true domain are used to\nstabilize early training of the discriminator. We replace the hard ‘0’ and ‘1’ do-\nmain labels by random uniform values in the ranges [0 .00,0.03] and [0.97,1.00],\nrespectively.\nISTN : The ISTN is trained as a generator. The ISTN output S2T is passed\nthrough the discriminator and forced to be closer to domain T by computing\nthe adversarial loss Ladv = Lbce(DT(S2T),1). Soft labels are also used here. We\nexpect that when images T are passed through the ISTN, the outputT2T should\nbe unchanged as it is already in domain T. This is enforced by the identity loss\nLidt = ℓ1(T,T 2T) acting on image intensities of T and T2T. A weighting factor\nλ is applied to Lidt giving the total loss function for the ISTN in Eq. (3)c.\nWe compare with the CycleGAN [18] training approach, which trains both\ndirections simultaneously using two ISTNs (ISTN S2T and ISTN T2S) and two\ndiscriminators (DS and DT). The CycleGAN introduces the cycle-consistency\nterm to Listn such that when ISTN T2S is used to transform S2T, the result\nS2T2S is forced to be close to S. Figure 1 shows the two ISTNs, their outputs\nand associated losses. The loss functions for ISTN S2T are shown in Eq. (3).\nOptimization is done using the Adam optimizer.\nDownstream Tasks:The goal of our work is to demonstrate that such explicit\nappearance and spatial transformations via ISTNs can successfully tackle DS in\n1 https://github.com/mlnotebook/domain_adapation_istn\nImage-level Harmonization of Multi-Site Data using ISTNs 5\nFig. 2: (Top) Examples from Morpho-MNIST datasets from domains (left-to-\nright) Athin un-slanted digits; B thickened digits; C slanted digits; Dthickened\nand slanted digits. Each contains ‘healthy’, ‘fractured’ and ‘swollen’ classes.\n(Bottom) Examples of source domain images before (left) and after (right) ISTN-\ntransformation showing ISTN recovery of appearance and shape changes.\ncertain applications. Ideally, we would like to observe that the performance of a\npredictor trained on S2T and tested on T can recover to single-site performance.\nTo demonstrate this, prior to training the ISTN, we train a task model (e.g. clas-\nsiﬁer or regressor) TS on domain S. The performance of TS(S) is likely to be our\n‘best performance’ whilst TS(T) will degrade due to DS. During ISTN training,\nwe simultaneously re-train TS on the ISTN output of S2T. This model TS2T is\ntrained to achieve maximum performance on the transformed imagesTS2T(S2T)\nusing labels from S. We assess the performance ‘recovery’ of TS2T by comparing\nTS(T) with TS2T(T). In practice, data fromT would be unlabelled. Our approach\nensures that test data from the new domain T is not modiﬁed in any way. Addi-\ntionally, in scenarios where the original model TS is deployed, it is likely to have\nbeen trained on a large, well-curated, high-quality dataset; we cannot assume\nsimilar would be available for each new test domain. Our model-agnostic unsu-\npervised DA is validated on two problems: i) proof-of-concept showing recovery\nof a classiﬁer’s performance on digit recognition, ii) classiﬁcation and regression\ntasks with real-world, multi-site T1-weighted brain MRI.\nLdisT = 1\n2 [Lbce(DT(S2T),0) + Lbce(DT(T),1)] . (1)\nLS2T\nistn = Lbce(DT(S2T),1) + 1\n2 λ∥T2T −T∥1 . (2)\nLS2T\nistn = Lbce(DS(T2S),0) + 1\n2 λ∥S2S−S∥1 + λ∥S2T2S−S∥1 . (3)\n3 Materials\n3.1 Proof-of-concept: Morpho-MNIST Experiments\nData. Morpho-MNIST is a framework that enables applying medically-inspired\nperturbations, such as local swellings and fractures, to the well-known MNIST\ndataset [15]. The framework also allows us to control transformations to obtain\nthickening and shearing of the original digits. We ﬁrst create a dataset with\nthree classes: ‘healthy’ digits with no transformations; ‘fractured’ digits with a\nsingle thin disruption and ‘swollen’ digits which exhibit a localized, tumor-like\n6 Robinson et al.\nabnormal growth. A digit is only either fractured or swollen, not both. We spec-\nify a set of ‘thin’ digits (2.5 pixels across) to be source domain A. To simulate\ndomain shift, we create three more datasets—domain B: thickened, 5.0 pixels\ndigits; domain C: slanted digits created by shearing the image by 20–25 ◦ and\ndomain D: thickened-slanted digits at 5.0 pixels and 20–25 ◦ shearing. Datasets\nB–Dcontain the same three classes as A, while each set has its own data charac-\nteristics simulating diﬀerent types of domain shift. All images are single-channel\nand 28 ×28 pixels. Figure 2 shows some visual examples.\nTask. The downstream task in this experiment is a 3-class classiﬁcation prob-\nlem: ‘healthy’ vs. ‘fractured’ vs. ‘swollen’. We train a small, fully-convolutional\nclassiﬁer to perform the classiﬁcation on domain A. We use ISTNs to retrain\nthe classiﬁer on transformed images A2B, A2C, and A2D, and evaluate each on\ntheir corresponding test domains B, C, and D.\nWe run training for 100 epochs and perform grid search to ﬁnd suitable hyper-\nparameters including learning rate, trade-oﬀ λ and the control-point spacing\nof the B-spline STN. We conduct experiments using ITN only, STN only and\ncombinations of aﬃne and B-spline ISTNs to determine the best model for the\ntask. We also consider both transfer directions, switching the roles of source and\ntarget domains.\n3.2 Application to Brain MRI Experiments\nWe apply the same methodology to a real-world domain shift problem where we\nobserve a signiﬁcant drop in prediction accuracy when naively training on one\nsite and testing on another without any DA. We utilise 3D brain MRI from two\nsites that employ similar but not identical imaging protocols.\nData. We construct two datasets of T1-weighted brain MRI from subjects with\nno reported pathology, where n = 565 are taken from the Cambridge Centre\nfor Ageing and Neuroscience study (Cam-CAN) [19,20] and n = 689 from the\nUK Biobank imaging study (UKBB) [21,22,23]. From each site, 450 subjects\nare used for training and the remainder for testing. The UKBB dataset con-\ntains equal numbers of male and female subjects between the ages of 48 and 71\n(µ= 59.5). In the classiﬁcation task, to simulate the eﬀect of population shift our\nCam-CAN dataset has a wider age range (30–87, µ = 57.9) but maintains the\nmale-to-female ratio. We match the age range of both datasets in the regression\ntask, limiting DS only to the more subtle scanner eﬀects. UKBB images were\nacquired at the UKBB imaging centre, and Cam-CAN images were acquired at\nthe Medical Research Council Cognition and Brain Sciences Unit in Cambridge,\nUK. Both sites acquire 1 mm isotropic images using the 3D MPRAGE pulse\nsequence on Siemens 3 T scanners with a 32-channel receiver head coil and in-\nplane acceleration factor 2. Appendix A presents the acquisition parameters that\ndiﬀer between the two sites. We note that generally the acquisition parameters\nof both sites are similar, and the images cannot be easily distinguished visually.\nFor pre-processing, all images are aﬃnely aligned to MNI space, skull-stripped,\nImage-level Harmonization of Multi-Site Data using ISTNs 7\nTable 1: 3-class classiﬁcation results on MorphoMNIST. Images transferred from\nclassiﬁer domain A: ‘thin unslanted’ to three target domains. Accuracies shown\nfor classiﬁers retrained on the ISTN output from scratch (Acc s) and ﬁnetuned\n(Accf). ∆is model improvement from baseline. Control-point spacings indicated\nfor B-Spline STNs. First row is the original classiﬁer without DA.\nTarget Thick Unslanted Thin Slanted Thick Slanted\nITN STN Accs ∆ Accf ∆ Accs ∆ Accf ∆ Accs ∆ Accf ∆\nno no 41.2 45.7 32.8\nyes no 79.0 37.8 83.3 42.1 83.4 37.7 83.3 37.6 82.4 49.6 84.6 51.8\nno Aﬃne 52.4 11.2 68.9 27.7 92.4 46.7 93.0 47.3 54.8 22.0 64.8 32.0\nno B-spline (4) 39.0 -2.2 54.4 13.2 92.1 46.4 93.1 47.4 36.0 3.2 57.2 24.4\nno B-spline (8) 49.2 8.0 61.5 20.3 92.5 46.8 92.3 46.6 37.0 4.2 61.8 29.0\nyes Aﬃne 78.8 37.6 77.1 35.9 86.7 41.0 88.4 42.7 81.9 49.1 83.1 50.3\nyes B-spline (4) 66.3 25.1 75.8 34.6 92.7 47.0 91.0 45.3 79.3 46.5 82.7 49.9\nyes B-spline (8) 69.5 28.3 77.2 36.0 91.8 46.1 93.4 47.7 79.0 46.2 80.8 48.0\nTable 2: Sex classiﬁcation results on 3D Brain MRI\nSource UKBB Cam-CAN\nMethod Uni-ISTN CycleGAN Bi-ISTN Uni-ISTN CycleGAN Bi-ISTN\nITN STN Accs ∆ Accf ∆ Accs ∆ Accf ∆ Accs ∆ Accf ∆ Accs ∆ Accf ∆\nno no 54.8 54.8 64.3 64.3\nyes no 79.1 24.3 72.2 17.4 80.0 25.2 80.8 26.0 86.2 21.9 78.2 13.9 80.8 16.5 79.9 15.6\nyes Aﬃne 80.9 26.1 75.7 20.9 70.4 15.6 82.4 27.6 79.9 15.6 79.1 14.8 82.4 18.1 72.0 7.7\nyes B-spline (8) 78.3 23.5 76.5 21.7 79.1 24.3 78.7 23.9 80.3 16.0 84.5 20.2 78.7 14.4 80.8 16.5\nyes B-spline (16) 80.0 25.2 78.3 23.5 73.0 18.2 67.8 13.0 85.4 21.1 84.1 19.8 67.8 3.5 68.6 4.3\nbias-ﬁeld-corrected, and intensity-normalised to zero mean unit variance within\na brain mask. Voxels outside the mask are set to 0. Images are passed through\na tanh function before being consumed by the networks.\nTask. We consider two prediction tasks, namely sex classiﬁcation and age re-\ngression using the UKBB and Cam-CAN sets, each once as source and once\nas target domain. The task networks are retrained on the transformed images\nproduced by the ISTN and evaluated on the corresponding target domain.\n4 Experimental Results\nMorpho-MNIST. Quantitative results for the synthetic experiments are sum-\nmarized in Table 1. ITNs are able to harmonize local appearance such as thick-\nness between source and target domains, while STNs perform well in recovering\nshape variations such as slant. Where both thickness and slant are varied be-\ntween source and target domains, we note an ITN-only performs as well (or\nslightly better) than a joint ISTN, suggesting that thickness is more important\nfor the classiﬁcation task. In Fig. 2 we show visual results on how the ISTNs are\nable to recover both appearance and shape diﬀerences between domains.\nBrain MRI.Quantitative results are summarized in Tables 2 and 3. The sex\nclassiﬁer trained and tested on UKBB achieves 84.3% accuracy. This drops to\n54.8% when tested on Cam-CAN. Similarly, training and testing on Cam-CAN\nyields 91.6%, dropping to 64.3% when testing on UKBB. Using ISTNs for domain\n8 Robinson et al.\nTable 3:Age regression results on 3D Brain\nMRI. MAE s is the task model retrained\nfrom scratch.\nSource UKBB Cam-CAN\nMethod Uni-ISTN Uni-ISTN\nITN STN MAEs ∆ MAEs ∆\nno no 5.13 4.61\nyes no 4.71 0.42 4.57 0.04\nyes Aﬃne 4.58 0.55 5.00 -0.39\nyes B-spline (16) 5.06 0.07 4.90 -0.29\nFig. 3: Examples of (left-to-right)\nsource domain, transformed ISTN out-\nput and diﬀerence image.\nadaptation, and retraining the classiﬁers increases the accuracy substantially on\nCam-CAN from 54.8% to 80.9%, and on UKBB from 64.3% to 86.2%, which is\nclose to the single-site performance. Training the classiﬁer from scratch performs\nsimilarly well to ﬁne-tuning. Bidirectional training with CycleGAN seems not to\nprovide substantial improvements over the simpler unidirectional scheme. The\nISTNs are able to overcome some of the acquisition and population shifts between\nthe two domains. The age regressor trained and tested on UKBB achieves mean\nabsolute error (MAE) of 4.25 years increasing to 5.13 when evaluated on Cam-\nCAN. The regressor trained and tested on Cam-CAN yields 4.10 years MAE\nincreasing to 4.61 when tested on UKBB. Despite the initially smaller drop in\nperformance for age regression, ISTNs still improve performance. The UKBB-\ntrained regressor recovers to 4.58 years MAE and the Cam-CAN-trained one to\n4.56 years. Note, we had limited the population shift here by constraining the\nage range, thus the recovery is likely due to a reduction in acquisition shift.\n5 Conclusion\nWe explored adversarially-trained ISTNs for model-agnostic domain adaptation.\nThe learned image-level transformations help explainability, as the resulting im-\nages can be visually inspected and checked for plausibility (cf. Fig. 3). Further\ninterrogation of deformations ﬁelds also adds to explainability, e.g. Appendix B.\nImage-level DA seems suitable in cases of subtle domain shift caused by acquisi-\ntion and population diﬀerences in multi-center studies. Predictive performance\napproached single-site accuracies. The choice of STN and control-point spacings\nmay need to be carefully considered for speciﬁc use cases. An extension of our\nwork to many-sites may be possible by simultaneously adapting to multiple sites.\nA quantitative comparison to feature-level DA would be a natural next step for\nfuture work. Another interesting direction could be to integrate the ISTN com-\nponent in a fully end-to-end task-driven optimisation, where the ISTN and the\ntask network are trained jointly.\nAcknowledgements: RR funded by KCL & Imperial EPSRC CDT in Medical\nImaging (EP/L015226/1) and GlaxoSmithKline; This research received fund-\ning from the European Research Council (ERC) under the European Union’s\nHorizon 2020 research and innovation programme (grant agreement No 757173,\nproject MIRA, ERC-2017-STG). DCC is supported by the EPSRC Centre for\nImage-level Harmonization of Multi-Site Data using ISTNs 9\nDoctoral Training in High Performance Embedded and Distributed Systems\n(HiPEDS, grant ref EP/L016796/1). The research was supported in part by\nthe National Institutes of Health, Clinical Center.\nReferences\n1. Crimi, A., Bakas, S., Kuijf, H., Keyvan, F., Reyes, M., van Walsum, T., eds.: Brain-\nlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. Springer\nInternational Publishing (2019)\n2. Castro, D.C., Walker, I., Glocker, B.: Causality matters in medical imaging.\narXiv:1912.08142 (2019)\n3. Wachinger, C., Becker, B.G., Rieckmann, A., P¨ olsterl, S.: Quantifying confounding\nbias in neuroimaging datasets with causal inference. In: Medical Image Computing\nand Computer-Assisted Intervention – MICCAI 2019, Springer (2019) 484–492\n4. Glocker, B., Robinson, R., Castro, D.C., Dou, Q., Konukoglu, E.: Machine learning\nwith multi-site imaging data: An empirical study on the impact of scanner eﬀects.\nIn: Medical Imaging meets NeurIPS. (2019)\n5. Yu, M., Linn, K.A., Cook, P.A., Phillips, M.L., McInnis, M., Fava, M., Trivedi,\nM.H., Weissman, M.M., Shinohara, R.T., Sheline, Y.I.: Statistical harmonization\ncorrects site eﬀects in functional connectivity measurements from multi-site fmri\ndata. Human Brain Mapping 39(11) (2018) 4213–4227\n6. Pan, S., Yang, Q.: A survey on transfer learning. IEEE Transactions on Knowledge\nand Data Engineering 22(10) 13451359,\n7. Yang, J., Dvornek, N.C., Zhang, F., Chapiro, J., Lin, M., Duncan, J.S.: Unsuper-\nvised domain adaptation via disentangled representations: Application to cross-\nmodality liver segmentation. In Shen, D., Liu, T., Peters, T.M., Staib, L.H., Essert,\nC., Zhou, S., Yap, P.T., Khan, A., eds.: Medical Image Computing and Computer\nAssisted Intervention – MICCAI 2019, Cham, Springer International Publishing\n(2019) 255–263\n8. Yan, W., Wang, Y., Gu, S., Huang, L., Yan, F., Xia, L., Tao, Q.: The domain\nshift problem of medical image segmentation and vendor-adaptation by unet-gan.\nIn Shen, D., Liu, T., Peters, T.M., Staib, L.H., Essert, C., Zhou, S., Yap, P.T.,\nKhan, A., eds.: Medical Image Computing and Computer Assisted Intervention –\nMICCAI 2019, Cham, Springer International Publishing (2019) 623–631\n9. Kamnitsas, K., Baumgartner, C., Ledig, C., Newcombe, V., Simpson, J., Kane,\nA., Menon, D., Nori, A., Criminisi, A., Rueckert, D., et al.: Unsupervised domain\nadaptation in brain lesion segmentation with adversarial networks. In: Interna-\ntional Conference on Information Processing in Medical Imaging, Springer (2017)\n597–609\n10. Dai, L., Li, T., Shu, H., Zhong, L., Shen, H., Zhu, H.: Automatic brain tumor\nsegmentation with domain adaptation. In Crimi, A., Bakas, S., Kuijf, H., Keyvan,\nF., Reyes, M., van Walsum, T., eds.: Brainlesion: Glioma, Multiple Sclerosis, Stroke\nand Traumatic Brain Injuries, Cham, Springer International Publishing (2019)\n380–392\n11. Sandfort, V., Yan, K., Pickhardt, P.J., Summers, R.M.: Data augmentation using\ngenerative adversarial networks (CycleGAN) to improve generalizability in CT\nsegmentation tasks. Scientiﬁc Reports 9(1) (November 2019)\n12. Isola, P., Zhu, J., Zhou, T., Efros, A.: Image-to-image translation with conditional\nadversarial networks. In: 2017 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR, Honolulu, HI 59675976\n10 Robinson et al.\n13. Zhu, J., Park, T., Isola, P., Efros, A.: Unpaired image-to-image translation using\ncycle-consistent adversarial networks. In: 2017 IEEE International Conference on\nComputer Vision (ICCV, Venice 22422251\n14. Lee, M.C.H., Oktay, O., Schuh, A., Schaap, M., Glocker, B.: Image-and-spatial\ntransformer networks for structure-guided image registration. In Shen, D., Liu, T.,\nPeters, T.M., Staib, L.H., Essert, C., Zhou, S., Yap, P.T., Khan, A., eds.: Medical\nImage Computing and Computer Assisted Intervention – MICCAI 2019, Cham,\nSpringer International Publishing (2019) 337–345\n15. Castro, D.C., Tan, J., Kainz, B., Konukoglu, E., Glocker, B.: Morpho-MNIST:\nQuantitative assessment and diagnostics for representation learning. Journal of\nMachine Learning Research 20(178) (2019)\n16. Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial transformer networks.\nIn: Advances in Neural Information Processing Systems 28. (2015) 2017–2025\n17. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,\nLin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,\nRaison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.:\nPytorch: An imperative style, high-performance deep learning library. In Wallach,\nH., Larochelle, H., Beygelzimer, A., d’Alch´ e Buc, F., Fox, E., Garnett, R., eds.:\nAdvances in Neural Information Processing Systems 32. Curran Associates, Inc.\n(2019) 8024–8035\n18. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In: Computer Vision (ICCV), 2017\nIEEE International Conference on. (2017)\n19. Shafto, M.A., Tyler, L.K., Dixon, M., Taylor, J.R., Rowe, J.B., Cusack, R., Calder,\nA.J., Marslen-Wilson, W.D., Duncan, J., Dalgleish, T., et al.: The Cambridge\nCentre for Ageing and Neuroscience (Cam-CAN) study protocol: a cross-sectional,\nlifespan, multidisciplinary examination of healthy cognitive ageing. BMC Neurol-\nogy 14(1) (2014) 204\n20. Taylor, J.R., Williams, N., Cusack, R., Auer, T., Shafto, M.A., Dixon, M., Tyler,\nL.K., Henson, R.N., et al.: The Cambridge Centre for Ageing and Neuroscience\n(Cam-CAN) data repository: structural and functional MRI, MEG, and cognitive\ndata from a cross-sectional adult lifespan sample. NeuroImage 144 (2017) 262–269\n21. Sudlow, C., Gallacher, J., Allen, N., Beral, V., Burton, P., Danesh, J., Downey,\nP., Elliott, P., Green, J., Landray, M., Liu, B., Matthews, P., Ong, G., Pell, J.,\nSilman, A., Young, A., Sprosen, T., Peakman, T., Collins, R.: UK Biobank: An\nopen access resource for identifying the causes of a wide range of complex diseases\nof middle and old age. PLoS Medicine 12(3) (2015) e1001779\n22. Miller, K.L., Alfaro-Almagro, F., Bangerter, N.K., Thomas, D.L., Yacoub, E., Xu,\nJ., Bartsch, A.J., Jbabdi, S., Sotiropoulos, S.N., Andersson, J.L.R., Griﬀanti, L.,\nDouaud, G., Okell, T.W., Weale, P., Dragonu, I., Garratt, S., Hudson, S., Collins,\nR., Jenkinson, M., Matthews, P.M., Smith, S.M.: Multimodal population brain\nimaging in the UK Biobank prospective epidemiological study. Nature Neuro-\nscience 19(11) (2016) 1523–1536\n23. Alfaro-Almagro, F., Jenkinson, M., Bangerter, N.K., Andersson, J.L., Griﬀanti,\nL., Douaud, G., Sotiropoulos, S.N., Jbabdi, S., Hernandez-Fernandez, M., Vallee,\nE., Vidaurre, D., Webster, M., McCarthy, P., Rorden, C., Daducci, A., Alexander,\nD.C., Zhang, H., Dragonu, I., Matthews, P.M., Miller, K.L., Smith, S.M.: Image\nprocessing and quality control for the ﬁrst 10,000 brain imaging datasets from UK\nBiobank. NeuroImage 166 (feb 2018) 400–424\nSupplementary Material\nA Acquisition Parameters.\nTable 4: Acquisition parameters for the multi-site brain MRI datasets.\nSite Scanner TR (ms) TE (ms) TI (ms) TA (s) FOV (mm)\nCam-CAN Siemens TIM Trio 2250 2.99 900 272 256x240x192\nUKBB Siemens Skyra 2000 2.01 880 294 208x256x256\n12 Robinson et al.\nB ISTN Transformation Visualization.\nFig. 4: The original image (left) passes through the ISTN. The transformations\napplied by the ITN and subsequently by the STN are visualized by showing\ndiﬀerence images. The transformation applied by the STN can also be visualized\nas a spatial deformation ﬁeld (right). This is shown for the Aﬃne (top) and B-\nSpline (bottom) STNs.\nImage-level Harmonization of Multi-Site Data using ISTNs 13\nC Morpho-MNIST Architectures.\nTable 5: ITN (left) and discriminator (right) architectures for Morpho-MNIST exper-\niments. nf: number of channels, k: square kernel size, s: stride, in and out: layer input\nand output dimensions, N: normalization (BN: batch normalization, IN: instance nor-\nmalization), D: Dropout keep-rate, A: activation function. ‘up’ is composed of bilinear\nupsampling followed by zero-padding of 1 and convolution shown in the table.\nITN Architecture - Morpho-MNIST\nlayer nf k s p in out N D A\nin - - - - [1,28,28] - - -\nconv 16 3 1 1 [1,28,28] [16,28,28] BN - ReLU\nconv 32 3 2 1 [16,28,28] [32,14,14] BN - ReLU\nconv 64 3 2 1 [32,14,14] [64,7,7] BN - ReLU\nconv 128 3 1 1 [64,7,7] [128,7,7] BN - ReLU\nconv 64 3 1 1 [128,7,7] [64,7,7] BN - ReLU\nup 32 3 1 1 [64,7,7] [32,14,14] BN - ReLU\nup 16 3 1 1 [32,14,14] [16,28,28] BN - ReLU\nup 1 3 1 1 [16,28,28] [1,28,28] - - tanh\nout - - - - [1,28,28] - - -\nDiscriminator Architecture - Morpho-MNIST\nlayer nf k s p in out N D A\nin - - - - [1,28,28] - - -\nconv 32 3 1 1 [1,28,28] [32,28,28] - - ReLU\nconv 64 3 2 1 [32,28,28] [64,14,14] IN - ReLU\nconv 128 3 2 1 [64,14,14] [128,7,7] IN - ReLU\nconv 256 3 2 1 [128,7,7] [256,4,4] IN 0.5 ReLU\nconv 1 3 2 1 [256,4,4] [1,1,1] - - sigmoid\nout - - - - [1,1,1] - - -\n3-Class Classiﬁer Architecture - Morpho-MNIST\nlayer nf k s p in out N D A\nin - - - - [1,64,64,64] - - -\nconv 16 3 1 1 [1,24,24] [16,24,24] - - ReLU\nconv 32 3 2 1 [16,14,14] [32,7,7] BN - ReLU\nconv 64 3 2 1 [32,7,7] [64,4,4] BN - ReLU\nconv 128 3 2 1 [64,4,4] [128,1,1] BN 0.5 ReLU\nconv 3 3 2 0 [128,1,1] [3,1,1] - - sigmoid\nout - - - - [3,1,1] - - -\n14 Robinson et al.\nD Brain MRI Architectures.\nTable 6: Architectures for Brain MRI experiments. nf: number of channels, k: square\nkernel size, s: stride, in and out: layer input and output dimensions, N: normaliza-\ntion (BN: batch or IN: instance normalization), D: Dropout keep-rate, A: activation\nfunction. ‘up’ is composed of linear upsampling, zero-padding and convolution.\nITN Architecture - Brain MRI\nlayer nf k s p in out N D A\nin - - - - [1,64,64,64] - - -\nconv 8 3 1 1 [1,64,64,64] [8,64,64,64] BN - ReLU\nconv 16 3 2 1 [8,64,64,64] [16,32,32,32] BN - ReLU\nconv 32 3 2 1 [16,32,32,32] [32,16,16,16] BN - ReLU\nconv 64 3 2 1 [32,16,16,16] [64,8,8,8] BN - ReLU\nconv 64 3 1 1 [64,8,8,8] [64,8,8,8] BN - ReLU\nconv 64 3 1 1 [64,8,8,8] [64,8,8,8] BN - ReLU\nup 32 3 1 1 [64,8,8,8] [32,16,16,16] BN 0.5 ReLU\nup 16 3 1 1 [32,16,16,16] [16,32,32,32] BN 0.5 ReLU\nup 8 3 1 1 [16,32,32,32] [8,64,64,64] BN 0.5 ReLU\nup 1 3 1 1 [8,64,64,64] [1,64,64,64] - - tanh\nout - - - - [1,64,64,64] - - -\nDiscriminator Architecture - Brain MRI\nlayer nf k s p in out N D A\nin - - - - [1,64,64,64] - - -\nconv 32 3 1 1 [1,64,64,64] [32,64,64,64] - - ReLU\nconv 64 3 2 1 [32,64,64,64] [64,32,32,32] IN - ReLU\nconv 128 3 2 1 [64,32,32,32] [128,16,16,16] IN - ReLU\nconv 256 3 2 1 [128,16,16] [256,8,8] IN - ReLU\nconv 256 3 2 1 [256,8,8] [256,4,4] IN 0.5 ReLU\nconv 1 3 2 1 [256,4,4] [1,1,1] - - sigmoid\nout - - - - [1,1,1] - - -\nSex Classiﬁer Architecture - Brain MRI\nlayer nf k s p in out N D A\nin - - - - [1,64,64,64] - - -\nconv 8 5 2 2 [1,64,64,64] [8,64,64,64] - - ReLU\nconv 16 5 2 2 [8,64,64,64] [16,32,32,32] BN - ReLU\nconv 32 5 2 2 [16,32,32,32] [32,16,16,16] BN - ReLU\nconv 64 5 2 2 [32,16,16] [64,8,8] BN 0.5 ReLU\nconv 128 2 2 2 [64,8,8] [128,4,4] BN 0.5 ReLU\nconv 128 2 2 2 [128,4,4] [128,1,1] BN 0.5 ReLU\nconv 1 5 1 2 [128,1,1] [1,1,1] - - sigmoid\nout - - - - [1,1,1] - - -\nAge Regressor Architecture - Brain MRI\nlayer nf k s p in out N D A\nin - - - - [1,64,64,64] - - -\nconv 16 3 1 1 [1,64,64,64] [8,64,64,64] - - ReLU\nMaxPool - - 2 1 [8,64,64,64] [8,32,32,32] - - -\nconv 32 3 2 1 [8,32,32,32] [32,32,32,32] - - ReLU\nMaxPool - - 2 1 [32,32,32,32] [32,16,16,16] - - -\nLinear 128 - - - [32*32*32*32,1] [128] - - ReLU\nLinear 64 - - - [128] [64] - - ReLU\nLinear 32 - - - [64] [32] - - ReLU\nLinear 1 - - - [32] [1] - -\nout - - - - [1] - - -",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7875413298606873
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6145704388618469
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.48741868138313293
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.48674654960632324
    },
    {
      "name": "Image (mathematics)",
      "score": 0.4584583342075348
    },
    {
      "name": "Domain adaptation",
      "score": 0.4564816951751709
    },
    {
      "name": "Data mining",
      "score": 0.44834205508232117
    },
    {
      "name": "Population",
      "score": 0.44471853971481323
    },
    {
      "name": "Generalization",
      "score": 0.4400036931037903
    },
    {
      "name": "Machine learning",
      "score": 0.37675410509109497
    },
    {
      "name": "Computer vision",
      "score": 0.3282530605792999
    },
    {
      "name": "Mathematics",
      "score": 0.09348681569099426
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Demography",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47508984",
      "name": "Imperial College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I188950975",
      "name": "GlaxoSmithKline (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210155647",
      "name": "National Institutes of Health Clinical Center",
      "country": "US"
    }
  ],
  "cited_by": 17
}