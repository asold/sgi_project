{
    "title": "Large chemical language models for property prediction and high-throughput screening of ionic liquids",
    "url": "https://openalex.org/W4409893803",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5100954623",
            "name": "Yuxin Qiu",
            "affiliations": [
                "East China University of Science and Technology",
                "State Key Laboratory of Chemical Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A5058493921",
            "name": "Zhen Song",
            "affiliations": [
                "East China University of Science and Technology",
                "State Key Laboratory of Chemical Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A5055191233",
            "name": "Guzhong Chen",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A5091762005",
            "name": "Wenyao Chen",
            "affiliations": [
                "East China University of Science and Technology",
                "State Key Laboratory of Chemical Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A5100630554",
            "name": "Long Chen",
            "affiliations": [
                "East China University of Science and Technology",
                "State Key Laboratory of Chemical Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A5025301828",
            "name": "Kake Zhu",
            "affiliations": [
                "East China University of Science and Technology",
                "State Key Laboratory of Chemical Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A5100627048",
            "name": "Zhiwen Qi",
            "affiliations": [
                "East China University of Science and Technology",
                "State Key Laboratory of Chemical Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A5047128288",
            "name": "Xuezhi Duan",
            "affiliations": [
                "East China University of Science and Technology",
                "State Key Laboratory of Chemical Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A5043284449",
            "name": "De Chen",
            "affiliations": [
                "East China University of Science and Technology",
                "State Key Laboratory of Chemical Engineering"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4400051417",
        "https://openalex.org/W2005307077",
        "https://openalex.org/W2575597094",
        "https://openalex.org/W2593612875",
        "https://openalex.org/W2000614400",
        "https://openalex.org/W3159181932",
        "https://openalex.org/W4376484134",
        "https://openalex.org/W4389900272",
        "https://openalex.org/W2163646971",
        "https://openalex.org/W4404815146",
        "https://openalex.org/W4405204829",
        "https://openalex.org/W4385507287",
        "https://openalex.org/W4405634872",
        "https://openalex.org/W4328050959",
        "https://openalex.org/W2123296654",
        "https://openalex.org/W4390879807",
        "https://openalex.org/W4220729429",
        "https://openalex.org/W4386884500",
        "https://openalex.org/W4308979880",
        "https://openalex.org/W2584013293",
        "https://openalex.org/W2073328513",
        "https://openalex.org/W3005866495",
        "https://openalex.org/W2105065508",
        "https://openalex.org/W3031432971",
        "https://openalex.org/W3207986612",
        "https://openalex.org/W2999735474",
        "https://openalex.org/W4320473078",
        "https://openalex.org/W4293051828",
        "https://openalex.org/W4317853069",
        "https://openalex.org/W3196480699",
        "https://openalex.org/W4400805295",
        "https://openalex.org/W4399722457",
        "https://openalex.org/W4394786800",
        "https://openalex.org/W4401694629",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W3157265962",
        "https://openalex.org/W2092872388",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W3093934881",
        "https://openalex.org/W4383955629",
        "https://openalex.org/W3202104925",
        "https://openalex.org/W4294000454",
        "https://openalex.org/W3116651029",
        "https://openalex.org/W4389076417",
        "https://openalex.org/W4290723813",
        "https://openalex.org/W2921250649",
        "https://openalex.org/W3021934323",
        "https://openalex.org/W1508604947",
        "https://openalex.org/W3138781613",
        "https://openalex.org/W4378715362",
        "https://openalex.org/W4386302049",
        "https://openalex.org/W4288375898",
        "https://openalex.org/W2951358838",
        "https://openalex.org/W3171775013",
        "https://openalex.org/W4387521360",
        "https://openalex.org/W2895677720"
    ],
    "abstract": "Chemical language models show excellent potential for predicting multiple properties of ionic liquids (ILs), offering a new approach to high-throughput screening of task-specific ILs from the vast chemical space.",
    "full_text": "Large chemical language models for property\nprediction and high-throughput screening of ionic\nliquids†\nYuxin Qiu,a Zhen Song, *ac Guzhong Chen, b Wenyao Chen, a Long Chen,a\nKake Zhu, *a Zhiwen Qi, a Xuezhi Duan *a and De Chen a\nIonic liquids (ILs) possess unique physicochemical properties and exceptional tunability, making them\nversatile materials for a wide range of applications. However, their immense designﬂexibility also poses\nsigniﬁcant challenges in eﬃciently identifying outstanding ILs for speciﬁc tasks within the vast chemical\nspace. In this study, we introduce ILBERT, a large-scale chemical language model designed to predict\ntwelve key physicochemical and thermodynamic properties of ILs. By leveraging pre-training on over 31\nmillion unlabeled IL-like molecules and employing data augmentation techniques, ILBERT achieves\nsuperior performance compared to existing machine learning methods across all twelve benchmark\ndatasets. As a case study, we highlight ILBERT's ability to screen ILs as potential electrolytes from\na database of 8 333 096 synthetically feasible ILs, demonstrating its reliability and computational\neﬃciency. With its robust performance, ILBERT serves as a powerful tool for guiding the rational\ndiscovery of ILs, driving innovation in their practical applications.\n1 Introduction\nIonic liquids (ILs) are typically dened as compounds consist-\ning entirely of ions with melting points below 100 °C.1,2 Their\ndistinctive properties, including nonvolatility, wide liquidus\nrange, high thermal stability, and high ionic conductivity, have\nfacilitated their applications across a wide range ofelds.3–5 The\ndiverse combinations of cations and anions provide signicant\ndesign exibility, enabling the tailoring of ILs to meet specic\napplications.6 However, this diversity also necessitates consid-\nerable time and costs for experimental evaluation of various\ncombinations of cations and anions. Consequently, eﬃcient\nand accurate tools for predicting the properties of ILs are highly\ndesirable.\n7,8\nAs a result of continuous e ﬀorts over the last decades,\nresearchers have developed a variety of computational methods\nto predict the properties of ILs, including but not limited to\nequation of state (EoS) methods, group contribution (GC)\nmethods, quantum chemistry (QC) calculations, and conductor-\nlike screening model (COSMO) based methods.\n6,9–14 EoS\nmethods possess a solid theoretical foundation in thermody-\nnamics, while their application is hindered by complexity for\nestimating the required model parameters.15,16 GC methods\nassume that the contributions of functional groups to a specic\ntarget property are additive, which has been shown to perform\nwell in estimating certain properties (such as density and heat\ncapacity); nevertheless, not all properties adhere to the simple\nadditivity rule.\n17–19 QC calculations can provide in-depth\ninsights into the characteristics and behaviors of ILs at the\nmicroscopic scale, while the high computational costs restrict\ntheir application in large-scale screening.20 The COSMO-RS and\nCOSMO-SAC models are versatile predictive methods for ther-\nmodynamic properties ofuids and their mixtures, including\nILs.\n21,22 However, COSMO-based models necessitate prior\navailability of the s-proles of all involved molecules and in\nsome cases provide qualitative rather than quantitative\nprediction.\n8,23,24\nApart from the methods mentioned above, quantitative\nstructure–property relationship (QSPR) models that correlate\nmolecular properties with their corresponding chemical struc-\ntures have gained signicant popularity driven by advance-\nments in machine learning (ML).\n25–31 These methods can utilize\nvarious molecular representations, such as groups, descriptors\nand ngerprints, demonstrating considerable exibility and\naccuracy.\n20,32–34 However, these molecular representations are\nessentially manually engineered based on expert knowledge,\nwhich requires feature engineering tailored to specic types of\nILs or target properties. This dependence may limit their scal-\nability to other IL property prediction tasks.\n7 In recent years,\naState Key Laboratory of Chemical Engineering, School of Chemical Engineering, East\nChina University of Science and Technology, 130 Meilong Road, Shanghai 200237,\nChina. E-mail: songz@ecust.edu.cn; kakezhu@ecust.edu.cn; xzduan@ecust.edu.cn\nbDepartment of Chemical Engineering, Columbia University, New York, NY 10027, USA\ncEngineering Research Center of Resource Utilization of Carbon-Containing Waste with\nCarbon Neutrality (Ministry of Education), East China University of Science and\nTechnology, 130 Meilong Road, Shanghai 200237, China\n† Electronic supplementary information (ESI) available. See DOI:\nhttps://doi.org/10.1039/d5dd00035a\nCite this:Digital Discovery,2 0 2 5 ,4,\n1505\nReceived 24th January 2025\nAccepted 28th April 2025\nDOI: 10.1039/d5dd00035a\nrsc.li/digitaldiscovery\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 | 1505\nDigital\nDiscovery\nPAPER\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\ndeep learning, as a subset of ML, has achieved remarkable\nsuccess in variouselds.35 One key principle of deep learning is\nto design suitable deep neural networks and train them on large\namounts of raw data, which allows models to automatically\nlearn feature representations and reduces the need for manual\nfeature engineering.\n36 Nevertheless, when it comes to the task of\nIL property prediction based on deep learning, databases such\nas ILThermo that have even been elaborately accumulated for\nyears are still far from suﬃcient compared to the vast potential\nchemical space.\n37\nThe challenge of data scarcity faced by IL property prediction\ntasks is essentially also encountered in natural language pro-\ncessing (NLP), that is, unlimited unlabeled datasets versus\nlimited labeled datasets. As a signicant advancement in the\nNLP eld, transformer architecture proposed by Vaswaniet al.\n38\nlaid the foundation for subsequent research, particularly with\nthe emergence of pre-trained large language models such as\nBERT (Bidirectional Encoder Representations from Trans-\nformers) and GPT (Generative Pre-trained Transformer). 39,40\nThese models operate within a pre-training and ne-tuning\nframework, leveraging large-scale unlabeled text data during\npre-training, resulting in impressive performance across diverse\ndownstream tasks. The success of NLP has enlightened\nmolecular property prediction based on chemical languages,\nsuch as the Simpli ed Molecular Input Line Entry System\n(SMILES).\n41 For instance, Chithrananda et al.42 collected 77\nmillion SMILES strings from PubChem and constructed\na chemical language model named ChemBERTa, which\ndemonstrates a competitive performance against the best\nmodels on MoleculeNet. Kuenneth and Ramprasad\n43 intro-\nduced a model based on chemical languages, termed polyBERT,\nwhich is capable of predicting a wide range of polymer prop-\nerties and identifying suitable candidates with exceptional\nspeed and accuracy. As for ILs, Chenet al.\n7 proposed ILtransR\nthat can predict IL properties from SMILES by combining\ntransformer and convolutional neural network (CNN) architec-\ntures, which well manifests the potential of chemical languages\nfor IL representation. However, these eﬀorts are predominantly\nconstrained by their reliance on SMILES representations and\npurely NLP methods, which limits the exploration of alternative\nchemical languages and the domain-speci c characteristics\ninherent to chemical structures.\nIn this work, building upon the aforementioned content, we\nintroduce ILBERT, a BERT-based chemical language model for\npredicting twelve properties of ILs. ILBERT follows the frame-\nwork of pre-training andne-tuning based on the collection of\n31 million unlabeled IL-like molecules and twelve IL property\ndatasets. By comprehensively evaluating twelve IL property\nprediction tasks, ILBERT exhibits superior performance\ncompared to other ML-based methods of corresponding litera-\nture. Moreover, the comparative analyses of how di ﬀerent\nchemical languages and tokenization methods aﬀect model\nperformance are conducted, and the impacts of pre-training\ndataset size and the number of model parameters are investi-\ngated. Apart from model performance, the attention mecha-\nnism is utilized to analyze the learned representation from\nILBERT to provide the interpretability of the model. As an\nexemplary application, ILBERT successfully identied electro-\nlyte candidates with high electrical conductivity and low\nviscosity from 8 333 096 synthetically feasible ILs. To facilitate\nthe widespread use of ILBERT for assisting researchers in\ndesigning ILs for specic processes, a web server thereon is\ndeveloped at https://ai4solvents.com/prediction, and source\ncodes of ILBERT and data are also provided in the GitHub\nrepository athttps://github.com/Yu-Xin-Qiu/ILBERT.\n2 Methodology\n2.1 Work ow\nThe workow of ILBERT proposed herein is illustrated in Fig. 1,\nencompassing three stages of pre-training, ne-tuning and\nhigh-throughput screening. During the pre-training stage, the\nmasked language model (MLM) is utilized to learn the implicit\ncontext information of chemical languages based on 31 million\nunlabeled molecules. During the ne-tuning stage, twelve\nbenchmark datasets are compiled to evaluate the performance\nof the model, covering various physicochemical and thermo-\ndynamic properties of ILs. Comprehensive experiments are\ncarried out to determine the optimal chemical language and\ntokenization methods for IL property prediction. Subsequently,\nwe investigate the impacts of pre-training data quantity and the\nnumber of model parameters on model performance and\nconduct ablation studies to evaluate the eﬃcacy of pre-training\nand data augmentation. Finally, a high-throughput screening\ncase study is exemplied to screen superior IL electrolytes from\n8 333 096 synthetically feasible ILs.\n2.2 Chemical language and tokenization\nTo explore which chemical language is more suitable for IL\nproperty prediction, prominent chemical languages, including\nSMILES,\n51 DeepSMILES,52 InChI53 and SELFIES,54 are used for\ncomparative analysis. It is noteworthy that the default cong-\nuration of SELFIES is unable to encode all anions and cations,\nsuch as PF\n6\n−. To overcome this limitation, the constraints for\nphosphorus in the hypervalence constraints are relaxed to\nenable the encoding of all ILs. Additionally, we also investigate\nvarious tokenization methods: character-level (CL), atom-level\n(AL), SMILES pair encoding (SPE), and atom-in-SMILES (AIS),\nwith AIS being applicable only to SMILES.\n55,56 The vocabularies\nare constructed based on the entirene-tuning dataset for IL\nproperty prediction. Tables S1 and S2 † show the diﬀerent\ntokenization results for the same IL across various chemical\nlanguages. Fig. S1† illustrates the length distribution of thene-\ntuning dataset under di ﬀerent combinations of chemical\nlanguages and tokenization methods.\n2.3 Data augmentation\nTo address the challenge of relatively scarce labeled data of ILs,\ndata augmentation (DA) is employed based on SMILES\nenumeration. Specically, for each canonical SMILES input, 9\nadditional non-canonical SMILES strings are generated. During\nthe training period, all SMILES strings are utilized to enable the\nmodel to recognize molecular structures from various\n1506 | Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n“perspectives”. During testing or validation periods, thenal\nprediction value is obtained by averaging the predictions from\nall 10 SMILES strings. It should be noted that diﬀerent SMILES\nstrings for the same IL do not appear in the training and test\nsets at the same time to avoid data leakage.\n2.4 Dataset collection\nA large unlabeled database is compiled by collecting 1 billion\nSMILES strings from ZINC (https://zinc.docking.org/) and 117\nmillion SMILES strings from PubChem ( https://\npubchem.ncbi.nlm.nih.gov/). As this study focuses solely on\npredicting the properties of ILs, two specic ltering criteria\nare employed to screen molecules that are closely related to\nILs. The rst criterion is that SMILES strings must contain\neither ionic bonds represented by “.” or both “+” and “-”\ncharges, while the second criterion speci es that sequence\nlengths must be less than 100. Following data cleaning and\nstandardization, approximately 31 million distinct SMILES\nstrings are retained for pre-training. In thene-tuning stage,\ntwelve datasets of physicochemical and thermodynamic\nproperties are collected from literature, which can be divided\ninto three types. Therst type of property is independent of\ntemperature and pressure (melting pointT\nm, glass transition\ntemperature Tg, thermal decomposition temperature Td, and\ncytotoxicity towards the leukemia rat cell line IPC-81\n(log\n10EC50), the second type of property is dependent on\ntemperature (electrical conductivity lnk, viscosity lnh, surface\ntension g, refractive index nD and heat capacity Cp), and the\nthird type of property is associated with both temperature and\npressure (thermal conductivityl, density r and CO2 solubility\nxCO2). It should be noted that while viscosity and electrical\nconductivity exhibit both temperature and pressure\ndependence, the majority of available experimental data\ncorrespond to ambient pressure conditions. In this work, we\nfocus specically on the temperature-dependent behavior of\nviscosity and electrical conductivity. Detailed information\nabout these twelve datasets is presented in Table 1, and the\ndistribution of each dataset is depicted in Fig. S2.† The total\nnumber of SMILES strings involved in the pre-training and\nne-tuning datasets are 30 526 093 and 64 226, respectively,\nwith their length distribution shown in Fig. S3.†\n2.5 Implementation details and model construction\nDuring the data preprocessing stage, RDKit ( https://\nwww.rdkit.org) is employed to process SMILES, including\nltering invalid SMILES, SMILES standardization, and SMILES\nenumeration. Additionally, the deepsmiles, RDKit, and seles\npackages are utilized to convert canonical SMILES into\nDeepSMILES, InChI, and SELFIES, respectively.\nIn the pre-training stage, 31 million canonical SMILES\nstrings are utilized as inputs for our model. Aer tokenization,\n15% of the tokens are randomly masked before being fed into\nthe BERT model. The objective of the pre-training task is to\npredict the masked tokens and minimize the cross-entropy loss\nassociated with the MLM. Compared to the original BERT\nmodel, the data volume and complexity of the IL property\nprediction task are relatively smaller. Consequently, the\nnumber of transformer encoder layers, heads in multi-head\nattention, and embedding dimensions in BERT-base are\nmodied to construct three pre-trained models with varying\nmodel parameters, as detailed in Table S3.† The Hugging Face\nlibrary (https://huggingface.co/) is employed to build the pre-\ntrained models, training with the Adam optimizer for ve\nepochs, an initial learning rate of 1× 10\n−4, and other settings\nconsistent with BERT-base. To independently evaluate the\nFig. 1 Workﬂow of the proposed ILBERT. (A and B) Pre-training andﬁne-tuning framework. (C) High-throughput IL screening case study for\nelectrolytes.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 | 1507\nPaper Digital Discovery\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nperformance of pre-trained models, 1% of the pre-training\ndataset was randomly selected as the validation set.\nIn thene-tuning stage, both transfer learning (TL) andne-\ntuning (FT) approaches are employed for IL property prediction\ntasks. For the TL approach, the weights of the pre-trained\ntransformer encoder are frozen and a convolutional neural\nnetwork (CNN) model is added, followed by the inclusion of\nconditional variables such as temperature and pressure, before\nnally inputting these into Multilayer Perceptron (MLP) for IL\nproperty prediction. For the FT approach, not only the weights\nof the CNN and MLP but also the pre-trained models are\nupdated to better adapt to the target task. Mean Squared Error\n(MSE) is developed as the loss function and hyperparameter\nsearch is performed for each task. Table S4† summarizes the\noptimal hyperparameters for each IL property prediction task.\nTo avoid overtting, the early stopping strategy is adopted, and\nthe training is suspended if no loss reduction was observed\nwithin 15 epochs.\nIn our previous work, we highlighted the distinction between\ntwo dataset split strategies: data point-based and IL-based.\n7,32,33\nWhen handling tasks related to temperature/pressure, the data\npoint-based dataset split strategy allows the same IL (with only\nad iﬀerence in temperature/pressure) to appear in both the\ntraining and test sets, leading to data leakage and over-\nestimation of model performance. In contrast, the IL-based\nsplit strategy mitigates this problem by ensuring that the\nsame IL does not appear in both sets, thus providing more\nrigorous evaluation. Unless specically noted, this study follows\nthe rigorous IL-based dataset split strategy, andve-fold cross-\nvalidations (CVs) are repeated ve times to report the nal\nresults. The nal model is integrated with ve individual\nmodels that are obtained fromve-fold cross-validations. The\naverage prediction across these models is used as the nal\nresult, and the standard deviation serves as the estimate of\nuncertainty.\n2.6 Experiment details\nBased on the results of high-throughput screening, two ILs, 1-\nethyl-3-methylimidazolium dicyanamide ([EMIM][DCA]) and\n1,3-diethylimidazolium dicyanamide ([DEIM][DCA]), were\nselected to measure their melting point, electrical conductivity,\nviscosity and density. [EMIM][DCA] (CAS: 370865-89-7,$98%,\nw\nwater = 0.4884%) was purchased from Adamas-beta. Addi-\ntionally, [DEIM][DCA] ($98%, wwater = 0.09466%) was rst\nsynthesized in this study, with the synthesis process shown in\nFig. S4.† The chemical structure and composition of both ILs\nwere conrmed using\n1H NMR and 13C NMR spectroscopy.\nNMR spectra were recorded on a NMR spectrometer (AV-400\nMHz, Bruker, Switzerland) using DMSO-d6 as the solvent.\nWater content was measured by Karl-Fischer volumetric titra-\ntion (AQV-300, Hiranuma, Japan). The\n1H NMR and13C NMR\nspectra of [EMIM][DCA] and [DEIM][DCA] are presented in\nFig. S5–S8.†\nThe melting point was determined by diﬀerential scanning\ncalorimetry (DSC 25, TA Instruments, USA) and the DSC curves\nare provided in Fig. S9 and S10.† Electrical conductivity was\nmeasured with a conductivity meter (SD30, Mettler Toledo,\nSwitzerland) and a conductivity sensor (InLab731, Mettler\nToledo, Switzerland) inside a glove box (MKUS2-2309-0069,\nMikrouna, China) that maintained water and oxygen levels\nbelow 0.01 ppm. Viscosity and density were measured using an\nautomated falling ball viscometer (Lovis 2000 ME, Anton Paar,\nAustria). All experiments were carried out at temperatures\nranging from 293.15 K to 323.15 K.\n3 Results and discussion\nIn this section, comprehensive experiments of the proposed\nILBERT are conducted to answer these six questions: (1) which\nchemical language and tokenization method are the most\nappropriate for IL property prediction tasks? (2) how do the\namount of pre-training data and the number of model param-\neters aﬀect the performance of ILBERT? (3) how do transfer\nlearning, ne-tuning and data augmentation strategies inu-\nence the performance of ILBERT? (4) how does ILBERT perform\nin diﬀerent IL property prediction tasks compared with other\nML-based methods? (5) what insights could we acquire from the\nrepresentations learned by ILBERT? (6) could ILBERT eﬃciently\nTable 1 The 12 IL property datasets involved in this work\nProperty Number of data points Number of ILs Units Data source\nMelting pointTm 2673 2673 K Makarov et al.44\nGlass transition temperatureTg 798 798 K Makarov et al.45\nThermal decomposition temperatureTd 2780 2780 K Makarov et al.45\nCytotoxicity towards the leukemia\nrat cell line IPC-81 log10EC50\n355 355 1 Wang et al.46\nElectrical conductivity lnk 2168 242 S m −1 Chen et al.33\nViscosity lnh 15 368 1964 mPa s Chen et al.7\nSurface tensiong 6051 542 mN m −1 Baran and Kloskowski47\nRefractive indexnD 2963 350 1 Cao et al.32\nHeat capacityCp 11 521 256 J mol −1 K−1 Liaqat et al.18\nThermal conductivityl 606 44 W m −1 K−1 Wan et al.48\nDensity r 31 167 2257 kg m −3 Paduszy´nski49\nCO2 solubility xCO2 10 116 124 mol% Song et al.50\n1508 | Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nand accurately identify promising candidates from the vast\nchemical space for specic tasks?\n3.1 Chemical language and tokenization (Q1)\nTo investigate the most eﬀective chemical languages and toke-\nnization methods for predicting IL properties, various combi-\nnations of chemical languages and tokenization methods are\ncompared across three IL property prediction tasks (Tm,l nh\nand r, each as an example of the three types of IL properties) in\nFig. 2. It can be found that SMILES, DeepSMILES, and SELFIES\nprovide approximate prediction results for all three tasks.\nHowever, InChI performs relatively poorly across all tokeniza-\ntion methods, indicating that InChI may not be suitable for\npredicting IL properties. This is probably due to more compli-\ncated syntax and arithmetic rules, which are challenging for\nlanguage models. Moreover, SELFIES + CL results in notably\npoor performance due to excessively long sequence lengths (see\nFig. S1D†) and erroneous splitting of multicharacter entities\nsuch as “[Ring1]” and “[Branch1]”. It should be noted that\nincreasing the maximum sequence lengths of SMILES in the\nne-tuning stage does not increase the performance of the\nmodel, but leads to a redundant increase in computational cost\n(see Table S5 †). Among all combinations, AIS + SMILES\nconsistently achieves the best prediction performance, sug-\ngesting that the classic SMILES representation is highly eﬀective\nfor IL property modeling. Compared with other tokenization\nmethods, the AIS tokenization method not only eliminates\nambiguities inherent in SMILES tokens but also better reects\nthe chemical environment around the corresponding atoms,\nresulting in superior modeling performance. Consequently, we\nrecommend utilizing AIS + SMILES for IL property prediction.\n3.2 Pre-training dataset size and the number of model\nparameters (Q2)\nTo thoroughly examine the impacts of pre-training dataset size\nand the number of model parameters on model performance,\nthree pre-trained models with varying parameters (see Table\nS3†) are constructed to evaluate their performance in the pre-\ntraining task and four representative downstream tasks (T\nm,\nln k, r and l). As illustrated in Fig. 3, the performance of the pre-\ntraining task and therst three downstream tasks (Tm,l nk, and\nr) generally improves with an increase in pre-training dataset\nsize. However, the performance increment gradually dimin-\nishes once the dataset size exceeds millions. Similarly, the same\ntrend can be observed for the number of model parameters (see\nFig. 3A–D). In contrast, for the specic task ofl, the above trend\nFig. 2 Impact of chemical language and tokenization on model performance in property prediction tasks. (A) Melting point. (B) Electrical\nconductivity. (C) Density.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 | 1509\nPaper Digital Discovery\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\ndoes not always remain consistent (see Fig. 3E), which is mainly\nbecause of the highest sparsity of data points (606) and IL types\n(44) among all properties. It is important to note that continu-\nously increasing the number of model parameters and the pre-\ntraining dataset size also brings about higher and unaﬀordable\ncomputational costs. In this context, considering the trade-oﬀ\nbetween computational costs and performance, the pre-\ntrained model with a moderate number of parameters (14 M)\nis selected for further modeling.\n3.3 Transfer learning, ne-tuning and data augmentation\n(Q3)\nTo demonstrate the eﬀectiveness of pre-training, we compared\nthe performance of three strategies: training from scratch,\ntransfer learning, and ne-tuning of the twelve IL property\nprediction tasks, as illustrated in Fig. 4A. In comparison with\ntraining from scratch, transfer learning achieves an average\nreduction of 12.08% in MAE across all twelve tasks, whilene-\ntuning achieves a slightly greater reduction of 13.74%, with\nspecic results detailed in Table S6.† The pre-training strategy\nproves to be benecial for ten out of the twelve tasks, while\nmaking only trivial changes for the heat capacity and CO\n2\nsolubility prediction tasks. Possible reasons for the latternd-\nings can be attributed to: (1) the data distribution of the heat\ncapacity dataset is highly imbalanced, where a single IL\naccounts for 15% and the top three ILs make up 27.2% of the\ndata points (see Fig. S11†); (2) CO\n2 solubility is strongly inu-\nenced by the interaction between ILs and CO 2, which pre-\ntrained models may not capture as they primarily focus on\ngeneral IL features without considering such speci c\ninteractions.\nThe impact of data augmentation on model performance is\nfurther analyzed for the tasks with fewer than 10 000 data\npoints. The results of ablation study (see Fig. 4B and Table S7†)\nindicate that both data augmentation and ne-tuning inde-\npendently enhance the performance of the model, respectively.\nFurthermore, when applied together, they lead to additional\nimprovements in the model performance, achieving an average\nreduction of 20.87% in MAE. Fig. 5 illustrates the results ofve-\nfold cross-validation across all twelve IL property prediction\ntasks, verifying that most of the data points are concentrated\nalong the diagonal region in the parity plot. In conclusion, the\ncombination of ne-tuning and data augmentation is highly\neﬀective for IL property prediction and successfully mitigates\nthe challenge of data scarcity.\n3.4 Model performance of ILBERT (Q4)\nTo comprehensively evaluate the performance of ILBERT, the\nnal models in the corresponding literature from which we\ncollected the twelve IL property datasets are compared as\nbenchmarks, respectively, including various ML-based predic-\ntion methods such as group contribution (GC) + ML,\n18,32\nCOSMO-RS derived descriptors + ML,33,48 RDKit descriptors +\nML,46 graph convolutional networks (GCNs), 47 transformer-\nCNN,44 ILTransR,7 and the consensus model.45 To ensure a fair\ncomparison, we maintain consistency in the dataset and dataset\nsplit strategies during the evaluation. The detailed results are\npresented in Table 2.\nFig. 3 Impact of pre-training dataset size and the number of model parameters on model performance in property prediction tasks. (A) Pre-\ntraining task. (B) Melting point. (C) Electrical conductivity. (D) Viscosity. (E) Thermal conductivity.\n1510 | Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nAs seen, the models proposed in this work demonstrate\nsuperior performance across all twelve IL properties compared\nwith the reference models in the corresponding literature. For\nproperties related only to the molecular structure (namelyT\nm,\nTg, Td, and log10EC50), the root mean square error (RMSE)\ndecreases by 3.00%, 6.12%, 2.89%, and 13.33%, respectively.\nNotably, for the T\ng and Td tasks, our model outperforms the\nensemble results of four descriptor-less deep learning models,\ndemonstrating its strong predictive capability. As for the second\nand third types of IL properties that are dependent on\ntemperature and/or pressure, ILBERT also surpasses all the\nreference models, decreasing the MAE ranging from 6.85% for\nln h to 39.82% forxCO\n2. To further assess whether our model\ncould eﬀectively capture temperature and/or pressure depen-\ndence, an IL namely 1-methyl-1-propylpyrrolidinium bis(tri-\nuoromethanesulfonyl)imide ([C3MPr][NTf2]) that appears in\nall twelve datasets is chosen as an example. The results shown\nin Fig. S12† indicate that the model accurately captures the\ntemperature and/or pressure dependence of these properties in\na wide range. Even in some cases of the lnh, g, C\np, and r\ndatasets, ILBERT still demonstrates robustness against data of\nuncertain quality. To further illustrate the diﬀerences between\nthe two data split strategies (data point-based and IL-based),\ntheir impacts on the performance of 5-fold cross-validation\nevaluation are compared using conductivity, viscosity, and\nsurface tension datasets as examples (see Table S9 †). This\ndemonstrates that the prediction metrics following the IL-based\nsplit strategy are signicantly decreased, as the splitting ensures\nthat the same IL does not appear simultaneously in both the\ntraining and testing sets. This approach enables a more\nrigorous assessment of model performance on unseen ILs,\nthereby providing a more reliable evaluation of the model's\ngeneralization capability. Furthermore, Table S10 † presents\na more extensive comparison of ILBERT's performance with\nthat of models from other literature, further conrming its\nexceptional predictive capabilities.\n3.5 Interpretability of ILBERT learned representations (Q5)\nIn addition to modeling performance, the interpretability of the\nmodel is another critical aspect that warrants further attention.\nTo reveal the intrinsic knowledge learned by ILBERT, the\nattention mechanism of the transformer model is leveraged to\nvisualize and interpret the attention scores from ILBERT using\n[C3MPr][NTf2] as an example (see Fig. 6A). It is noteworthy that\nthe breaking andattening of rings at specic atoms can result\nin non-adjacent positions in SMILES for atoms that are actually\nbonded in the IL structure. For instance, the tokens“[N+]1”and\n“C1”, as well as the brown-colored “O]S(]O)” and yellow-\ncolored “C(F)(F)F”, are not directly adjacent or explicitly\nrelated in the SMILES string, while these components are\nadjacent in the actual IL structure. ILBERT successfully diﬀer-\nentiates the information of substructures and the connectivity\nof atoms from SMILES directly. Moreover, two visualization\ntools, Attention Visualizer and BertViz,\n57,58 are employed to\ninterpret the ILBERT model from diﬀerent perspectives. Atten-\ntion Visualizer provides an intuitive illustration of token\nimportance in transformer-based encoder models. Using the\nmelting point prediction task as an example, we analyzed the\ncontributions of SMILES tokens from four representative ILs\n(belonging to pyrrolidinium, phosphonium, imidazolium and\npyridinium, respectively) to melting point prediction, with\ncontribution magnitude represented by color intensity (see\nFig. S13†). The results showed that the model primarily focused\non the positive and negative charge centers of the ILs, which are\ninherently determined by their ionic composition, and assigned\nhigher attention scores to specic functional groups, such as\nhydroxyl, ether, and tertiary amine groups. Additionally, Bert-\nViz, an interactive tool, is used to visualize the attention\nmechanisms in transformer-based language models. Fig. 6B\nand C illustrate the attention scores between input tokens in the\nrst and sixth (nal) layers, aggregated across four attention\nheads. Fig. 6D depicts the visualization eﬀects of therst heads\nin the sixth layer. BertViz provided a comprehensive view of the\nimplicit relationships learned by the ILBERT model in the\nchemical language, with certain heads focusing on functional\ngroups and charge centers (e.g., Fig. 6D). However, given the\ncomplexity of deep learning models, we acknowledge that this\ninterpretation represents only a preliminary understanding,\nand further research is required to fully explain such complex\nmodels.\nFig. 4 Overview of model performance and ablation study for IL\nproperty prediction. (A) Comparison of training from scratch, transfer\nlearning, andﬁne-tuning approaches for modeling twelve IL proper-\nties. (B) Ablation study of data augmentation (DA).\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 | 1511\nPaper Digital Discovery\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nTo further analyze the learned representations, t-Distributed\nStochastic Neighbor Embedding (t-SNE) analysis is employed\nfor dimensionality reduction. Specically, we collect all distinct\ncations from the density dataset (that includes 31 167 data\npoints, 2257 types of ILs, and 763 types of cations). A er\nextracting features using ILBERT, each cation is represented by\na 512-dimensional feature vector. The t-SNE visualization of\nlearned representations is shown in Fig. 7A, while Fig. S14A†\ndisplays the visualization of extended connectivityngerprints\n(ECFPs) as comparisons. Even without further ne-tuning,\nILBERT eﬀectively separates almost all cation types, demon-\nstrating its ability to capture rich structural information from\nFig. 5 Results ofﬁve-fold cross-validation across all twelve IL property prediction tasks. (A) Melting point. (B) Glass transition temperature. (C)\nThermal decomposition temperature. (D) Cytotoxicity towards the leukemia rat cell line IPC-81. (E) Electrical conductivity. (F) Viscosity. (G)\nSurface tension. (H) Refractive index. (I) Heat capacity. (J) Thermal conductivity. (K) Density. (L) CO\n2 solubility.\n1512 | Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nchemical languages during pre-training. Additionally, the\nfeatures ne-tuned for specic tasks (such as the melting point)\nare also visualized in Fig. 7B, with S14B † showing the\ncorresponding results of ECFPs. The ILs with high melting\npoints (depicted in light colors) are primarily composed of\nsmaller halide anions. This is because smaller anions increase\nTable 2 Model performance on the twelve IL property prediction tasks\nProperties Models Split by MAE RMSE R2 Source\nTm This work IL 26.03 /C60.17 35.04 /C60.25 0.782 /C60.003 Makarov et al.44\nTransformer-CNN — 36 0.77\nTg This work IL 9.88 /C60.23 15.96 /C60.23 0.708 /C60.009 Makarov et al.45\nConsensus modela 10.4 17 0.67\nTd This work IL 21.88 /C60.20 32.63 /C60.32 0.816 /C60.004 Makarov et al.45\nConsensus modela 24.6 33.6 0.81\nlog10EC50 This work IL 0.2007 /C60.0039 0.2777 /C60.0026 0.9400 /C60.0011 Wang et al.46\nSVM 0.2628 0.3204 0.9202\nln k This work IL 0.350 /C60.010 0.530 /C60.012 0.888 /C60.005 Chen et al.33\nML boosting COSMO-RS 0.396 — 0.870\nln h This work IL 0.326 /C60.002 0.555 /C60.006 0.883 /C60.002 Chen et al.7\nILTransR 0.35 ——\ng This work IL 2.34 /C60.15 3.65 /C60.24 0.835 /C60.021 Baran and Kloskowski 47\nGCNb 2.71 /C60.12b 4.09 /C60.11b 0.794 /C60.011b\nnD This work IL 0.0055 /C60.0001 0.0086 /C60.0002 0.9538 /C60.0018 Cao et al.32\nGC + XGBoost — 0.0149 0.863\nCp This work Random 15.89 /C63.18 24.30 /C63.38 0.990 /C60.003 Liaqat et al.18\nGC —— 0.987\nl This work Random 0.0021 /C60.0001 0.0029 /C60.0001 0.9880 /C60.0010 Wan et al.48\nCOSMO-RS+MLR — 0.004281 0.9733\nr This work IL 13.24 /C60.26 26.20 /C60.43 0.979 /C60.001 Chen et al.7\nILTransR 16.46 ——\nxCO2 This work IL 0.0343 /C60.0004 0.0595 /C60.0014 0.937 /C60.003 Chen et al.7\nILTransR 0.057 ——\nFig. 6 Visualization of model interpretability, taking [C3MPr][NTf2] as an example. (A) Attention scores within SMILES strings, and the higher\nscores indicate higher correlation of tokens. Attention visualization of SMILES tokens in ILBERT provided by BertViz. (B) Layer 1 (all head). (C) Layer\n6 (all head). (D) Theﬁrst head in Layer 6.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 | 1513\nPaper Digital Discovery\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nthe melting point by promoting crystal eﬀective accumulation\nas their volume decreases. It is evident that the clustering\nperformance of the learned representations before and aer\nne-tuning surpasses that of the ECFPs, which partially\nexplains the superior predictive performance of ILBERT.\n3.6 Model application case study: high-throughput\nscreening of IL electrolyte (Q6)\nUsing the proposed ILBERT model, the twelve physicochemical\nand thermodynamic properties of ILs can be predicted eﬃ-\nciently and accurately, enabling the design of task-specic ILs.\nIn this study, a large-scale virtual screening database of 8 333\n096 possible combinations of ILs established by Venkatraman\net al.\n59 is utilized to demonstrate high-throughput screening.\nThanks to the eﬃciency of ILBERT, predicting each property\ntakes only about 2.5 hours on a single RTX A6000 GPU. The\nprediction results for all 12 predicted properties are available at\nhttps://github.com/Yu-Xin-Qiu/ILBERT.\nAs a case study, IL electrolytes are screened for their poten-\ntial suitability in lithium-ion batteries. ILs are increasingly\nrecognized as promising electrolyte materials due to their\nunique properties, such as high ionic conductivity at room\ntemperature, excellent thermal stability, and improved safety\ncompared to traditional organic solvents.\n60 Key characteristics\ninclude high ionic conductivity and low viscosity, while other\nfactors such as the melting point, thermal decomposition\ntemperature, and toxicity are also crucial for practical applica-\ntions.\n61 Based on the review of literature and practical applica-\ntion requirements, the screening criteria in this study are as\nfollows: T\nm < 298 K,Td > 473 K, log10EC50 > 3.4;k > 1.2 S m−1 and\nh < 100 mPa s atT = 298.15 K andP = 1 bar. Followed by these\ncriteria, 50 candidates are retained, and their predicted viscosity\nand electrical conductivity are shown in Fig. 8A. From these,ve\nILs with the highest electrical conductivity are selected for\nfurther analysis. It can be observed that all ve ILs share\ndicyanamide anions paired with imidazolium, pyrrolidinium\nand pyridinium cations (see Fig. 8B). To further illustrate\nILBERT's ability to predict ILs not included in the training set,\nwe select therst two ILs ([EMIM][DCA] and [DEIM][DCA]) with\nthe highest conductivity for experimental validation, which do\nnot appear in the conductivity training set. To be speci c,\n[EMIM][DCA] is commercially available while [DEIM][DCA] is\nsynthesized for therst time in this work.\nExperimental data on melting points, electrical conductivity,\nviscosity, and density for the two ILs are shown in Fig. 8C–H and\nFig. 7 t-SNE analysis of learned representations before and afterﬁne-tuning for speciﬁc tasks (melting point). (A) Learned representation (before\nﬁne-tuning) for cation classiﬁcation. (B) Learned representation (afterﬁne-tuning) for melting point prediction.\n1514 | Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nprovided in Tables S11 and S12.† The results demonstrate that\nILBERT maintains high predictive accuracy for novel ILs not\nincluded in the training set, with maximum errors within the\ncross-validation margin. In summary, ILBERT oﬀers a reliable\nand eﬃcient tool for predicting IL properties and enables large-\nscale high-throughput screening, paving the way for the design\nof advanced IL-based materials.\n4 Conclusion\nIn this work, we present ILBERT, aexible and powerful model\nfor predicting IL properties based on chemical language. To\naddress the challenge of limited data in IL property prediction,\nILBERT is pre-trained on 31 million unlabeled molecular\nstructures, enabling it to capture the inherent contextual\ninformation in chemical language. This pre-trained model is\nthen ne-tuned on specic IL property prediction datasets.\nBenchmark tests across twelve representative IL properties\n(including both physicochemical and thermodynamic proper-\nties) demonstrate that ILBERT consistently outperforms other\nML methods, proving its reliability and versatility.\nOur analysis of diﬀerent chemical languages and tokeniza-\ntion combinations reveals that using SMILES with the AIS\nmethod is the most eﬀective strategy for IL property prediction.\nFig. 8 High-throughput screening results and experimental validation for ILs as electrolytes. (A) Viscosity and electrical conductivity of 50\ncandidate ILs. The topﬁve ILs with the highest electrical conductivity are highlighted with red stars. (B) Structures of theﬁve screened ILs with the\nhighest electrical conductivity. (C–E) Electrical conductivity, viscosity and density of [EMIM][DCA]. (F–H) Electrical conductivity, viscosity and\ndensity of [DEIM][DCA].\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 | 1515\nPaper Digital Discovery\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nIn general, enlarging the pre-training dataset and the number of\nmodel parameters gradually improves prediction accuracy, but\nvaries to a large extent and even does not hold depending on the\nsize and distribution of the downstream database. Rigorous\nablation studies conrm the benets of transfer learning,ne-\ntuning and data augmentation. Notably,ne-tuning reduces the\nMAE by an average of 13.74% across all twelve prediction tasks\ncompared to training from scratch. For tasks with fewer than 10\n000 data points, combining data augmentation withne-tuning\nachieves an average MAE reduction of 20.87%.\nFinally, we demonstrate ILBERT's capability in high-\nthroughput screening of a large chemical space. In this case\nstudy, ILBERT is applied to identify promising IL candidates as\nelectrolytes from a large dataset of 8 333 096 potential ILs. Two\nof the top candidates are experimentally validated and found to\nexhibit excellent electrochemical properties. We believe that\nILBERT will serve as a valuable tool for the rational design of\ntask-specic ILs, advancing their applications in diverseelds.\nMoving beyond, this study underscores that large chemical\nlanguage models combining advanced natural language pro-\ncessing techniques with chemical informatics hold the power to\ntransform the paradigms in computational chemistry and\nmaterials discovery.\nWhile ILBERT represents a signicant advancement in pre-\ndicting physicochemical properties of ILs, we acknowledge that\nfuture research directions will focus on addressing its limita-\ntions, including high computational demands, challenges in\ninterpretability, and overdependence on training data. To\nfurther move forward, given the widespread presence of data\nimbalance in IL datasets, one of the key focuses of future work\nwill be to develop eﬀective solutions through in-depth research\nto address this challenge. Additionally, other advanced deep\nlearning models, such as Graphormer that has demonstrated\neﬀectiveness in various applications, have the potential to\nimprove the accuracy of IL property predictions. Besides, future\nstudies are highly worthwhile to investigate advanced modeling\napproaches for even more complex mixture systems such as\ndeep eutectic solvents (DESs), aiming to guide the rational\ndiscovery of mixture systems and unlock their diverse\napplications.\nData availability\nThe datasets, code and trained models for this work have been\nmade publicly available at Github https://github.com/Yu-Xin-\nQiu/ILBERT and with DOI – 10.5281/zenodo.14601047. The\nversion of the code employed for this study is version v1.0.0.\nAuthor contributions\nYuxin Qiu: conceptualization, methodology, writing– original\ndra. Zhen Song: conceptualization, validation, formal analysis,\nwriting – review & editing, project administration. Guzhong\nChen: data curation, writing– review & editing. Wenyao Chen:\ninvestigation, resources. Long Chen: formal analysis, resources.\nKake Zhu: conceptualization, supervision, resources, project\nadministration. Zhiwen Qi: resources, supervision. Xuezhi\nDuan: resources, supervision, project administration, funding\nacquisition. De Chen: supervision, funding acquisition.\nConﬂicts of interest\nThere are no conicts to declare.\nAcknowledgements\nThis research was supported by the National Key Research and\nDevelopment Program of China under the grant of\n2024YFA1510302 and the National Natural Science Foundation\nof China (NSFC) under the grants of 22208098 and 22278134. Z.\nS. also achknowledges the support from the Shanghai Munic-\nipal Commission of Education.\nReferences\n1 Z. Lei, C. Dai, J. Hallett and M. Shiett, Chem. Rev., 2024,124,\n7533–7535.\n2 J. P. Hallett and T. Welton,Chem. Rev., 2011,111, 3508–3576.\n3 M. Watanabe, M. L. Thomas, S. Zhang, K. Ueno, T. Yasuda\nand K. Dokko,Chem. Rev., 2017,117, 7190–7239.\n4 B. Wang, L. Qin, T. Mu, Z. Xue and G. Gao,Chem. Rev., 2017,\n117, 7113–7131.\n5 H. Struebing, Z. Ganase, P. G. Karamertzanis, E. Siougkrou,\nP. Haycock, P. M. Piccione, A. Armstrong, A. Galindo and\nC. S. Adjiman,Nat. Chem., 2013,5, 952–957.\n6 S. Koutsoukos, F. Philippi, F. Malaret and T. Welton,Chem.\nSci., 2021,12, 6820–6843.\n7 G. Chen, Z. Song, Z. Qi and K. Sundmacher,Digital Discovery,\n2023, 2, 591–601.\n8 Z. Song, J. Chen, J. Cheng, G. Chen and Z. Qi,Chem. Rev.,\n2024, 124, 248–317.\n9 R. L. Gardas and J. A. P. Coutinho,AIChE J., 2009,55, 1274–\n1290.\n10 M. L. Alcantara, G. L. Bressan, P. V. A. Santos, M. F. V. Nobre,\nJ. A. P. Coutinho, C. A. O. Nascimento and L. A. Follegatti-\nRomero, J. Mol. Liq., 2025,417, 126616.\n11 A. K. Halder, R. Haghbakhsh, E. S. C. Ferreira, A. R. C. Duarte\nand M. N. D. S. Cordeiro,J. Mol. Liq., 2025,418, 126707.\n12 A. Roosta, R. Haghbakhsh, A. R. C. Duarte and S. Raeissi,J.\nMol. Liq., 2023,388, 122747.\n13 N. Hayer, T. Wendel, S. Mandt, H. Hasse and F. Jirasek,\nChem. Eng. J., 2024, 158667, DOI:10.1016/j.cej.2024.158667.\n14 F. Jirasek and H. Hasse,Annu. Rev. Chem. Biomol. Eng., 2023,\n14,3 1–51.\n15 J. A. P. Coutinho, P. J. Carvalho and N. M. C. Oliveira,RSC\nAdv., 2012,2, 7322–7346.\n16 R. Zhu, H. Kang, Q. Liu, M. Song, C. Gui, G. Li and Z. Lei,Ind.\nEng. Chem. Res., 2024,63, 1670–1679.\n17 D. K. Mital, P. Nancarrow, T. H. Ibrahim, N. Abdel Jabbar\nand M. I. Khamis,Ind. Eng. Chem. Res., 2022,61, 4683–4706.\n18 S. Liaqat, M. d. B. Shahin, P. Nancarrow, S. Zeinab,\nT. Ibrahim, N. Abdel Jabbar, M. Khamis and\nS. McCormack,Ind. Eng. Chem. Res., 2023,62, 16093–16112.\n1516 | Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n19 A. Roosta, R. Haghbakhsh, A. R. C. Duarte and S. Raeissi,\nFluid Phase Equilib., 2023,565, 113672.\n20 E. I. Izgorodina, Z. L. Seeger, D. L. A. Scarborough and\nS. Y. S. Tan,Chem. Rev., 2017,117, 6696–6754.\n21 A. Klamt,J. Phys. Chem., 1995,99, 2224–2235.\n22 I. H. Bell, E. Mickoleit, C.-M. Hsieh, S.-T. Lin, J. Vrabec,\nC. Breitkopf and A. J¨ager, J. Chem. Theory Comput., 2020,\n16, 2635–2646.\n23 M. G. Freire, L. M. Santos, I. M. Marrucho and\nJ. A. P. Coutinho,Fluid Phase Equilib., 2007,255, 167–178.\n24 K. Paduszy´nski and M. Kr´olikowska, Ind. Eng. Chem. Res.,\n2020, 59, 11851–11863.\n25 K. Klimenko and G. V. S. M. Carrera,J. Cheminf., 2021,13, 83.\n26 J. Jiang, W. Duan, Q. Wei, X. Zhao, L. Ni, Y. Pan and\nC.-M. Shu,J. Mol. Liq., 2020,301, 112471.\n27 B. Winter, C. Winter, T. Esper, J. Schilling and A. Bardow,\nFluid Phase Equilib., 2023,568, 113731.\n28 B. Winter, J. Schilling and A. Bardow,Chem. Ing. Tech., 2022,\n94, 1320.\n29 L. Fleitmann, P. Ackermann, J. Schilling, J. Kleinekorte,\nJ. G. Rittig, F. vom Lehn, A. M. Schweidtmann, H. Pitsch,\nK. Leonhard, A. Mitsos, A. Bardow and M. Dahmen,Energy\nFuels, 2023,37, 2213–2229.\n30 F. Jirasek and H. Hasse, Fluid Phase Equilib., 2021, 549,\n113206.\n31 R. Gurnani, S. Shukla, D. Kamal, C. Wu, J. Hao, C. Kuenneth,\nP. Aklujkar, A. Khomane, R. Daniels, A. A. Deshmukh,\nY. Cao, G. Sotzing and R. Ramprasad,Nat. Commun., 2024,\n15, 6107.\n32 P. Cao, J. Chen, G. Chen, Z. Qi and Z. Song,Chem. Eng. Sci.,\n2024, 298, 120395.\n33 Z. Chen, J. Chen, Y. Qiu, J. Cheng, L. Chen, Z. Qi and Z. Song,\nACS Sustain. Chem. Eng., 2024,\n12, 6648–6658.\n34 H. Tran, R. Gurnani, C. Kim, G. Pilania, H.-K. Kwon,\nR. P. Lively and R. Ramprasad,Nat. Rev. Mater., 2024, 9,\n866–886.\n35 Y. LeCun, Y. Bengio and G. Hinton,Nature, 2015,521, 436–\n444.\n36 X.-C. Zhang, C.-K. Wu, Z.-J. Yang, Z.-X. Wu, J.-C. Yi,\nC.-Y. Hsieh, T.-J. Hou and D.-S. Cao,Briengs Bioinf., 2021,\n22, bbab152.\n37 Q. Dong, C. D. Muzny, A. Kazakov, V. Diky, J. W. Magee,\nJ. A. Widegren, R. D. Chirico, K. N. Marsh and M. Frenkel,\nJ. Chem. Eng. Data, 2007,52, 1151–1159.\n38 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser and I. Polosukhin, arXiv, 2017,\npreprint, arXiv:1706.03762, DOI:10.48550/arXiv.1706.03762.\n39 J. Devlin, M.-W. Chang, K. Lee and K. Toutanova,arXiv, 2018,\npreprint, arXiv:1810.04805, DOI:10.48550/arXiv.1810.04805.\n40 A. Radford, K. Narasimhan, T. Salimans and I. Sutskever,\nImproving language understanding by generative pre-training,\n2018.\n41 D. Weininger,J. Chem. Inf. Comput. Sci., 1988,28,3 1–36.\n42 S. Chithrananda, G. Grand and B. Ramsundar,arXiv, 2020,\npreprint, arXiv:2010.09885, DOI:10.48550/arXiv.2010.09885.\n43 C. Kuenneth and R. Ramprasad,Nat. Commun., 2023, 14,\n4099.\n44 D. M. Makarov, Y. A. Fadeeva, L. E. Shmukler and I. V. Tetko,\nJ. Mol. Liq., 2021,344, 117722.\n45 D. M. Makarov, Y. A. Fadeeva, L. E. Shmukler and I. V. Tetko,\nJ. Mol. Liq., 2022,366, 120247.\n46 Z. Wang, Z. Song and T. Zhou,Processes, 2020,9, 65.\n47 K. Baran and A. Kloskowski,J. Phys. Chem. B, 2023, 127,\n10542–10555.\n48 R. Wan, M. Li, F. Song, Y. Xiao, F. Zeng, C. Peng and H. Liu,\nInd. Eng. Chem. Res., 2022,61, 12032–12039.\n49 K. Paduszy´nski, Ind. Eng. Chem. Res., 2019,58, 5322–5338.\n50 Z. Song, H. Shi, X. Zhang and T. Zhou,Chem. Eng. Sci., 2020,\n223, 115752.\n51 D. Weininger,J. Chem. Inf. Comput. Sci., 1988,28,3 1–36.\n52 N. O'Boyle and A. Dalke, DeepSMILES: An Adaptation of\nSMILES for Use in Machine-Learning of Chemical Structures,\n2018.\n53 S. R. Heller, A. McNaught, I. Pletnev, S. Stein and\nD. Tchekhovskoi,J. Cheminf., 2015,7, 23.\n54 M. Krenn, F. H¨ase, A. Nigam, P. Friederich and A. Aspuru-\nGuzik, Mach. Learn.: Sci. Technol., 2020,1, 045024.\n55 X. Li and D. Fourches,J. Chem. Inf. Model., 2021, 61, 1560–\n1569.\n56 U. V. Ucak, I. Ashyrmamatov and J. Lee,J. Cheminf., 2023,15,\n55.\n57 A. Alam Falaki and R. Gras, arXiv, 2023, preprint,\narXiv:2308.14850, DOI:10.48550/arXiv.2308.14850.\n58 J. Vig, arXiv, 2019, preprint, arXiv:1904.02679, DOI:\n10.48550/arXiv.1904.02679.\n59 V. Venkatraman, S. Evjen and K. Chellappan Lethesh,Data,\n2019, 4, 88.\n60 H. Niu, L. Wang, P. Guan, N. Zhang, C. Yan, M. Ding, X. Guo,\nT. Huang and X. Hu,J. Energy Storage, 2021,40, 102659.\n61 D. M. Makarov, Y. A. Fadeeva and L. E. Shmukler,J. Mol. Liq.,\n2023, 391, 123323.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 5 0 5–1517 | 1517\nPaper Digital Discovery\nOpen Access Article. Published on 28 April 2025. Downloaded on 11/5/2025 6:38:24 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online"
}