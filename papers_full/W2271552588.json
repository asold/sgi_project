{
  "title": "A language model based approach towards large scale and lightweight language identification systems",
  "url": "https://openalex.org/W2271552588",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2751474809",
      "name": "Srivastava, Brij Mohan Lal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751211330",
      "name": "Vydana, Hari Krishna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2329050228",
      "name": "Vuppala Anil Kumar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1953456123",
      "name": "Shrivastava Manish",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2408175559",
    "https://openalex.org/W2185814970",
    "https://openalex.org/W2405140950",
    "https://openalex.org/W2098859361",
    "https://openalex.org/W2156885227",
    "https://openalex.org/W2474824677"
  ],
  "abstract": "Multilingual spoken dialogue systems have gained prominence in the recent past necessitating the requirement for a front-end Language Identification (LID) system. Most of the existing LID systems rely on modeling the language discriminative information from low-level acoustic features. Due to the variabilities of speech (speaker and emotional variabilities, etc.), large-scale LID systems developed using low-level acoustic features suffer from a degradation in the performance. In this approach, we have attempted to model the higher level language discriminative phonotactic information for developing an LID system. In this paper, the input speech signal is tokenized to phone sequences by using a language independent phone recognizer. The language discriminative phonotactic information in the obtained phone sequences are modeled using statistical and recurrent neural network based language modeling approaches. As this approach, relies on higher level phonotactical information it is more robust to variabilities of speech. Proposed approach is computationally light weight, highly scalable and it can be used in complement with the existing LID systems.",
  "full_text": "A LANGUAGE MODEL BASED APPROACH TOWARDS LARGE SCALE AND\nLIGHTWEIGHT LANGUAGE IDENTIFICATION SYSTEMS\nBrij Mohan Lal Srivastava† Hari Krishna Vydana† Anil Kumar Vuppala† Manish Shrivastava⋆\nInternational Institute of Information Technology, Hyderabad, India\n† Speech & Vision Lab\n⋆ Language Technology Research Center\n{brijmohanlal.s, hari.vydana}@research.iiit.ac.in\n{anil.vuppala, m.shrivastava}@iiit.ac.in\nABSTRACT\nMultilingual spoken dialogue systems have gained promi-\nnence in the recent past necessitating the requirement for a\nfront-end Language Identiﬁcation (LID) system. Most of the\nexisting LID systems rely on modeling the language discrim-\ninative information from low-level acoustic features. Due to\nthe variabilities of speech (speaker and emotional variabili-\nties, etc.), large-scale LID systems developed using low-level\nacoustic features suffer from a degradation in the perfor-\nmance. In this approach, we have attempted to model the\nhigher level language discriminative phonotactic information\nfor developing an LID system. In this paper, the input speech\nsignal is tokenized to phone sequences by using a language\nindependent phone recognizer. The language discriminative\nphonotactic information in the obtained phone sequences are\nmodeled using statistical and recurrent neural network based\nlanguage modeling approaches. As this approach, relies on\nhigher level phonotactical information it is more robust to\nvariabilities of speech. Proposed approach is computationally\nlight weight, highly scalable and it can be used in complement\nwith the existing LID systems.\nIndex Terms— Language Identiﬁcation, Recurrent neu-\nral network language model (RNNLM), SRI language model\n(SRILM), Phone recognizer followed by language model\n(PRLM), phonotactics.\n1. INTRODUCTION\nLanguage identiﬁcation (LID) refers to the task of automat-\nically identifying the language from the speech utterance.\nAn LID system is a vital module for a wide range of mul-\ntilingual applications like, call centers, multilingual Spoken\nDialog Systems, emergency services and speech-to-speech\ntranslation systems. Human-Computer interaction through\nspeech can be taken more deeply into human society if the\ninteraction is through multiple regional languages, for that\nLID system is a preliminary requirement. A lot of scien-\ntiﬁc interest is being shown in developing an LID system,\nspeciﬁcally a lightweight system to lower the overhead cost.\nIn relevance to the task of developing an LID system, re-\ncent works have focused on developing algorithms to extract\nsuitable features for language identiﬁcation. Lately i-vector\nbased features are explored and they have exhibited better\nperformance compared to the conventional spectral features\nlike Mel-frequency cepstral coefﬁcients (MFCC), Linear pre-\ndictive cepstral coefﬁcients (LPCC) and Shifted delta cepstral\ncoefﬁcients (SDC) in NIST evaluations for speaker and lan-\nguage recognition tasks [1]. Neural networks have also been\nemployed as feature extractors to compute the stacked bottle-\nneck features for LID [2]. Multilingual bottleneck, multilin-\ngual tandem bottleneck obtained by stacking the SDC with\nthe corresponding bottleneck features are explored in [3, 4].\nIn most of the recent approaches the power of DNN (Deep\nneural networks) is explored for the task of language iden-\ntiﬁcation. Various approaches featuring Feed Forward Deep\nNeural Networks (FF-DNNs) and Long-Short Term Memory\nRecurrent Neural Networks (LSTM-RNNs) have been em-\nployed for developing the language classiﬁers. Additionally,\nconvolutional neural networks have been studied to develop\nan end-to-end LID system for 8 languages in[5].\nThe approaches mentioned above rely on the language\ndiscriminative capability present in the lower-level acous-\ntic information with some contextual information in time\nneighborhood. The better performance of i-vector based\napproaches compared to the conventional spectral features\ncan be attributed to its better context modeling capability.\nThe systems which try to model the language discriminative\ninformation at higher-level (phones, phone frequency and\nphonotactics) have exhibited better performance [6]. In this\npaper, we propose an approach to capture higher-level lan-\nguage discriminative information, which can be used as a\ncomplementary information to the existing low-level acoustic\ninformation modeling LID systems. A language independent\nphone recognizer followed by a language dependent phone\nmodel (PRLM) for capturing the phonotactics of 4 languages\narXiv:1510.03602v1  [cs.SD]  13 Oct 2015\nis used in [7]. In [7], language dependent phone recognizer\nfor every language operated in parallel (PPRLM) is used to\ndecode the test utterance and the language model with large\nnumber of uni-gram and bi-gram counts is used as an in-\ndication to the spoken language identity. Though PPRLM\n(Parallel-Phone recognizer followed by language model) is\nquite efﬁcient compared to PRLM, it is computationally inef-\nﬁcient to decode the test utterance using all language’s phone\nrecognizer. Scalability is the major issue for a PPRLM sys-\ntem whenever a new language has to be incorporated into the\nexisting LID system.\nIn this paper, we attempt to explore the signiﬁcance of\nPRLM based approaches for developing a large scale LID\nsystem. As a part of our experiments, we develop an LID\nsystem comprising of 176 different languages. The pipeline\nof our system includes a language independent phone recog-\nnizer to generate the phone sequences from raw signal and\ntwo different language modeling approaches (SRILM and\nRNNLM) to capture the phonotactic information. The pro-\nposed approach is computationally quite efﬁcient hence can\nbe quite handy to add complementary evidence to the exist-\ning approaches. As the proposed approach mostly relies on\nthe large durational phonotactic information of a language,\nit is more robust compared to low-level acoustic modeling\napproaches. The rest of the paper is organized as follows:\nSection 2 describes the dataset used in this approach. The\nproposed approach as such is described in section 3. Results\nand relevant discussions are presented in section 4. Conclu-\nsion and future scope are discussed in section 5.\n2. DATA DESCRIPTION\nThe language data which is made openly available by Top-\ncoder 1 as a part of Spoken Language Recognition challenge\nis used in this work. Data sets comprises of recorded speech\nin 176 languages. The dataset contains 375 utterances per\nlanguage and the language labels of these utterances are also\navailable. Each utterance has a duration of 10 seconds. Each\nspeech recording is given in a separate ﬁle and only one lan-\nguage is spoken in each ﬁle. The available data is reorganized\ninto training, testing, and validation sets. For training SRILM\nn-grams, 330 utterances were used for developing language\nmodels and the remaining 45 utterances are used for testing\nthe models. In case of RNNLMs, 300 utterances were used\nfor training, 30 utterances were used for validation and 45\nutterances are used to test the developed models. The data\nprovided has speech recordings in mp3 format, which are\nlater converted to W A V format with sampling rate as 16 kHz.\n1https://community.topcoder.com/tc?module=\nMatchDetails&rd=16555\n3. PROPOSED APPROACH\nIn this approach, we mostly rely on the higher-level phonotac-\ntic information extracted from speech for developing an LID\nsystem. To extract the higher level phonotactic information\ninput speech signal is to be tokenized. For tokenizing the in-\nput speech a language independent phone recognizer is used\nand the phone sequence is obtained. Although the phone rec-\nognizer is independent of the language that is being decoded,\nwe assume that the similar sounding acoustic patterns will be\ndecoded as approximately the same phone labels. By using\nthe language independent phone recognizer, we are relying on\nthe consistency of the phone recognizer rather than the accu-\nracy i.e., similar sounding acoustic sounds will be tagged with\nsame phone label regardless of the language. In this work, we\nhypothesize that the statistical patterns present in the obtained\nphone sequence have the language-discriminative informa-\ntion. For that purpose, SRILM and RNNLM are explored to\nmodel the statistical patterns that convey the language dis-\ncriminative information from the tokenized phone sequence.\nThe block diagram of the proposed approach is presented in\nFig 1. The details of the phone recognizer and the language\nmodeling techniques employed are described in the following\nsubsections.\n3.1. Language-independent phone recognizer\nThe goal of the language-independent phone recognizer is\nto provide maximum coverage of phone units present in all\nthe languages for which the system is being developed. We\nemploy PocketSphinx[8] as the front-end phone recognizer\nwhich uses HMM-based phone decoder from speech signal.\nA phonetically tied-mixture (PTM) model is used for efﬁcient\ndecoding. It contains 256 mixture components per state and\nassigns different mixture weights to the shared states of tri-\nphones. This model provides a good balance between speed\nand accuracy. Since it can be trained over huge data, it gives a\ndecent decoding result in under real time. We use US English\nphone set with 40 phones and an unbiased phonetic language\nmodel for decoding. The phone recognizer can be improved\nby training over multiple languages which will certainly in-\ncrease the coverage of common phonetic and acoustic pat-\nterns. Current acoustic model is trained on US English speech\ndata.\n3.2. Language modeling\nSRILM n-grams (uni-gram to 6-grams) and RNNLM have\nbeen used for experimentations to model the statistical pat-\nterns in the phone sequences.\nFig. 1. Block diagram of the proposed method.\nFig. 2. Architecture of RNNLM used in this work.\nRNNLM as shown in Figure 2, uses the current phone to-\nken w(t) and previous state of hidden layers(t−1) to predict\nthe probability of next tokeny(t). The neuron in hidden layer\ns(t) uses sigmoid activation function. Once the network is\ntrained, we can use the output layer y(t) as the probability\ndistribution of the next word given current word and the state\nof hidden layer. Here, c(t) represents the class layer which\ncan be optionally used to reduce the computational complex-\nity of model. As observed in the experiments conducted by\nus, models with lesser classes generally perform better at a\nhigher computation cost. The matrix W represents recurrent\nweights of the network which is trained using backpropaga-\ntion through time (BPTT) algorithm. Training of W can be\nimproved by choosing optimum number of steps to propagate\nthe error back in time. This can be task-dependent and for\ngood LID models this value can be 4 to 6.\n4. RESULTS & DISCUSSION\nSystem Min Max Avg Accuracy\nSRILM 1-gram 43.79 60 46.53\nSRILM 2-gram 77.77 83.61 81.77\nSRILM 3-gram 84.07 88.88 85.45\nSRILM 4-gram 81.11 86.66 83.94\nSRILM 5-gram 77.77 86.66 83.15\nSRILM 6-gram 77.77 86.66 82.98\nRNNLM 1-class 83.13 93.33 84.42\nRNNLM 6-classes 84.44 90.58 87.69\nRNNLM 100-classes 82.22 89.44 85.38\nTable 1. Language identiﬁcation accuracies for each lan-\nguage model (explicit approach)\nColumn 1 of Table. 1 are the various language models devel-\noped during the study. Column 2 speciﬁes the performance of\nlanguage model with minimum accuracy among all the 176\nlanguage models. Similarly, column 3 speciﬁes the perfor-\nmance of language model with maximum accuracy among all\nthe 176 language models. Column 4 is the average percent-\nage of correctly detected testcases in all the 176 languages.\nColumns 2, 3 are intended to show that the performance of\nLID system is consistant across the languages. In Table. 1,\nrow 2-7 are the performances using SRILM and rows 8-10\nare the performaces obtained using RNNLM.\nLanguage models are estimated for each of the 176 lan-\nguages. While training, we experimented with different\nclasses (1, 6, 100) and sizes of the hidden layers (30, 40)\nin RNNLM. Model with 6 classes and 40 units in hidden\nlayer gives best accuracy averaged over all languages. The\nmaximum accuracy obtained using RNNLM is 93.33% for\nDangaleat language. Although the performance of RNNLM\nmodels for LID can be boosted at the cost of higher computa-\ntional complexity, we observed that tri-gram models with low\ncomplexity can be used to achieve comparable results.\nRNNLM is known to exhibit high sequence learning ca-\npabilities [9] so the language-discriminative patterns from\nthe phone sequences is captured to a good extent which can\nobserved from the results of section 4. We also notice that\nn-grams accuracy increases drastically when we go from\nuni-gram to trigram models and then comes down gradually\nwhile we move to 6-gram models. For certain languages\nn-grams even surpass the accuracy obtained from a 100-class\nRNNLM. This observation can be utilized in order to linearly\ninterpolate the scores obtained from both models to boost the\naccuracy to maximum. For testing, sentence-level perplex-\nity obtained from each model is compared and the language\nwhich gives lowest perplexity value is chosen as the most\nprobable label.\nBased on the conducted experiments, we observed that\nRNNLM performance increases when we use optimum num-\nber of classes to decompose the vocabulary, i.e., when number\nof classes are approximately equal to\n√\n|V |, where |V |is the\nvocabulary size. In case of RNNLM 6-classes, error has been\npropagated to 4 steps back in time, which lets the model pre-\ndict the next word probability with the knowledge of higher\ndimensional features captured by the history. According to\nTable 1, this model gives highest average language identi-\nﬁcation performance of 87.69% at a much higher computa-\ntional cost. Rest of the models use simple RNN to model the\nlanguage.\n5. CONCLUSION\nIn this work, we have studied the scalability of PRLM based\napproaches for language identiﬁcation. The proposed ap-\nproach employs a language independent phone recognizer for\ntokenizing the input speech to phone sequences. We have\nexplored the language modeling approaches such as SRILM\nand RNNLM for modeling the statistical patterns in the phone\nsequence. As the proposed approach relies on the phonotactic\ninformation this can be used as a complementary information\nto the approaches that rely on language information from\nlow-level acoustic features. The proposed approach is highly\ncomputationally efﬁcient and it can come handy to enhance\nmany other approaches of LID systems. From the results\nit can be observed that both SRILM and RNNLM based\nlanguage models have shown equally good performance for\ndeveloping a large scale LID systems.\nWe have developed LID systems using sentence-level\nprobabilities and n-best scores obtained from various lan-\nguage models. Currently, the language models are developed\nindependent of each other. Some of the future tasks can be\nto develop linearly and non-linearly interpolated language\nmodels for LID, synthesizing LID-speciﬁc acoustic models\nand Language model pruning. Less phonetic coverage could\nbe a reason for low recognition accuracy of some languages\nhence more generalized phone recognizer with large phonetic\ncoverage could be developed speciﬁcally for LID systems.\nBased on the statistics obtained from the language-speciﬁc\nlanguage models, further pruning can be done.\n6. REFERENCES\n[1] Alan McCree and Daniel Garcia-Romero, “Dnn senone\nmap multinomial i-vectors for phonotactic language\nrecognition,” in Proc. of interspeech, Dresden, Germany,\n2015, pp. 394–397.\n[2] Pavel Matejka, Le Zhang, Tim Ng, HS Mallidi, Ondrej\nGlembek, Jeff Ma, and Bing Zhang, “Neural network\nbottleneck features for language identiﬁcation,” Proc.\nIEEE Odyssey, pp. 299–304, 2014.\n[3] Radek F ´er, Pavel Mat ˇejka, Franti ˇsek Gr ´ezl, Old ˇrich Pl-\nchot, and Jan ˇCernock`y, “Multilingual bottleneck fea-\ntures for language recognition,” in Proc. of interspeech,\n2015, pp. 389–393.\n[4] Wang Geng, Jie Li, Shanshan Zhang, Xinyuan Cai, and\nBo Xu, “Multilingual tandem bottleneck feature for lan-\nguage identiﬁcation,” in Proc. of interspeech, 2015, pp.\n413–417.\n[5] Alicia Lozano-Diez, Ruben Zazo-Candil, Javier\nGonzalez-Dominguez, Doroteo T Toledano, and Joaquin\nGonzalez-Rodriguez, “An end-to-end approach to\nlanguage identiﬁcation in short utterances using convo-\nlutional neural networks,” in Proc. of interspeech, 2015,\npp. 403–407.\n[6] Marc A Zissman and Kay M Berkling, “Automatic lan-\nguage identiﬁcation,” Speech Communication, vol. 35,\nno. 1, pp. 115–124, 2001.\n[7] Marc Zissman and Elliot Singer, “Automatic lan-\nguage identiﬁcation of telephone speech messages using\nphoneme recognition and n-gram modeling,” in Proc.\nICASSP. IEEE, 1994, vol. 1, pp. 305–308.\n[8] David Huggins-Daines, Mohit Kumar, Arthur Chan,\nAlan W Black, Mosur Ravishankar, Alex Rudnicky,\net al., “Pocketsphinx: A free, real-time continuous speech\nrecognition system for hand-held devices,” in Proc.\nICASSP. IEEE, 2006, vol. 1, pp. 185–188.\n[9] Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar\nBurget, and Jan Cernocky, “Rnnlm-recurrent neural net-\nwork language modeling toolkit,” in Proc. of the 2011\nASRU Workshop, 2011, pp. 196–201.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.830856204032898
    },
    {
      "name": "Discriminative model",
      "score": 0.8126944899559021
    },
    {
      "name": "Phonotactics",
      "score": 0.7816929221153259
    },
    {
      "name": "Language model",
      "score": 0.7261121869087219
    },
    {
      "name": "Language identification",
      "score": 0.5807408094406128
    },
    {
      "name": "Speech recognition",
      "score": 0.5774171352386475
    },
    {
      "name": "Phone",
      "score": 0.576979398727417
    },
    {
      "name": "Scalability",
      "score": 0.5387995839118958
    },
    {
      "name": "Identification (biology)",
      "score": 0.5035960078239441
    },
    {
      "name": "Spoken language",
      "score": 0.4773830771446228
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44057855010032654
    },
    {
      "name": "Natural language processing",
      "score": 0.43628379702568054
    },
    {
      "name": "Complement (music)",
      "score": 0.4273393154144287
    },
    {
      "name": "Speech processing",
      "score": 0.4115835726261139
    },
    {
      "name": "Natural language",
      "score": 0.33630040287971497
    },
    {
      "name": "Phonology",
      "score": 0.13875389099121094
    },
    {
      "name": "Linguistics",
      "score": 0.10017290711402893
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Complementation",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Phenotype",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": []
}