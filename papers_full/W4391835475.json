{
  "title": "Learning to Generate Context-Sensitive Backchannel Smiles for Embodied AI Agents with Applications in Mental Health Dialogues",
  "url": "https://openalex.org/W4391835475",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5087870411",
      "name": "Maneesh Bilalpur",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5088122258",
      "name": "Mert Ä°nan",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A5092964569",
      "name": "Dorsa Zeinali",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A5000246394",
      "name": "Jeffrey F. Cohn",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5025559955",
      "name": "Malihe Alikhani",
      "affiliations": [
        "Northeastern University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2088846517",
    "https://openalex.org/W2962613364",
    "https://openalex.org/W2163546575"
  ],
  "abstract": "Addressing the critical shortage of mental health resources for effective screening, diagnosis, and treatment remains a significant challenge. This scarcity underscores the need for innovative solutions, particularly in enhancing the accessibility and efficacy of therapeutic support. Embodied agents with advanced interactive capabilities emerge as a promising and cost-effective supplement to traditional caregiving methods. Crucial to these agents' effectiveness is their ability to simulate non-verbal behaviors, like backchannels, that are pivotal in establishing rapport and understanding in therapeutic contexts but remain under-explored. To improve the rapport-building capabilities of embodied agents we annotated backchannel smiles in videos of intimate face-to-face conversations over topics such as mental health, illness, and relationships. We hypothesized that both speaker and listener behaviors affect the duration and intensity of backchannel smiles. Using cues from speech prosody and language along with the demographics of the speaker and listener, we found them to contain significant predictors of the intensity of backchannel smiles. Based on our findings, we introduce backchannel smile production in embodied agents as a generation problem. Our attention-based generative model suggests that listener information offers performance improvements over the baseline speaker-centric generation approach. Conditioned generation using the significant predictors of smile intensity provides statistically significant improvements in empirical measures of generation quality. Our user study by transferring generated smiles to an embodied agent suggests that agent with backchannel smiles is perceived to be more human-like and is an attractive alternative for non-personal conversations over agent without backchannel smiles.",
  "full_text": "Learning to Generate Context-Sensitive Backchannel Smiles\nfor Embodied AI Agents with Applications in Mental Health\nDialogues\nManeesh Bilalpur1,*, Mert Inan 2, Dorsa Zeinali 2, Jeffrey F. Cohn 1 and Malihe Alikhani 2\n1University of Pittsburgh, Pittsburgh, Pennsylvania, USA\n2Northeastern University, Boston, Massachusetts, USA\nAbstract\nAddressing the critical shortage of mental health resources for effective screening, diagnosis, and treatment remains a\nsignificant challenge. This scarcity underscores the need for innovative solutions, particularly in enhancing the accessibility\nand efficacy of therapeutic support. Embodied agents with advanced interactive capabilities emerge as a promising and\ncost-effective supplement to traditional caregiving methods. Crucial to these agentsâ€™ effectiveness is their ability to simulate\nnon-verbal behaviors, like backchannels, that are pivotal in establishing rapport and understanding in therapeutic contexts but\nremain under-explored. To improve the rapport-building capabilities of embodied agents we annotated backchannel smiles in\nvideos of intimate face-to-face conversations over topics such as mental health, illness, and relationships. We hypothesized\nthat both speaker and listener behaviors affect the duration and intensity of backchannel smiles. Using cues from speech\nprosody and language along with the demographics of the speaker and listener, we found them to contain significant predictors\nof the intensity of backchannel smiles. Based on our findings, we introduce backchannel smile production in embodied\nagents as a generation problem. Our attention-based generative model suggests that listener information offers performance\nimprovements over the baseline speaker-centric generation approach. Conditioned generation using the significant predictors\nof smile intensity provides statistically significant improvements in empirical measures of generation quality. Our user study\nby transferring generated smiles to an embodied agent suggests that agent with backchannel smiles is perceived to be more\nhuman-like and is an attractive alternative for non-personal conversations over agent without backchannel smiles.\n1. Introduction\nFewer than a third of the US population has sufficient\naccess to mental health professionals [1]. This highlights\nthe need for additional resources to help mental health\nprofessionals meet the communityâ€™s demands. Problems\nlike symptom detection and evaluating treatment efficacy\nhave made great strides with AI [2, 3, 4]. Alternatively,\nEmbodied agents such as Ellie [ 5] through their multi-\nmodal behavior offer promising solutions to support the\ngrowing mental health needs. However, the development\nof such systems presents numerous challenges. These\ninclude the scarcity of mental health-related datasets,\nlimited access to domain experts for designing reliable\nand robust systems, and the ethical considerations crucial\nto their design and adaptation. Among such challenges,\none aspect that stands out is the agentâ€™s ability to es-\ntablish a common ground with users. Addressing this\nis particularly crucial when the agent functions as a lis-\ntener. Effective grounding in such scenarios relies heavily\non multimodal non-verbal behaviors like backchannels.\nMachine Learning for Cognitive and Mental Health Workshop\n(ML4CMH), AAAI 2024, Vancouver, BC, Canada.\n*Corresponding author.\n/envelâŒ¢pe-âŒ¢penmab623@pitt.edu (M. Bilalpur); inan.m@northeastern.edu\n(M. Inan); zeinali.d@northeastern.edu (D. Zeinali);\njeffcohn@pitt.edu (J. F. Cohn); m.alikhani@northeastern.edu\n(M. Alikhani)\nÂ© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License\nAttribution 4.0 International (CC BY 4.0).\nFigure 1: Overview of steps for backchannel smile generation\nin an embodied agent in a human-agent interaction: Speaker\nand listener (agent) turns are used to generate the listenerâ€™s\nresponse facial expression as landmarks. The landmarks are\nthen integrated with the embodied agent and added to the\nconversation flow represented as a dotted arrow.\nThese subtle yet impactful cues are pivotal in building\nrapport and understanding between the user and the\nagent. Hence, understanding and incorporating these be-\nhaviors into embodied agents is not only challenging but\nalso essential for creating a supportive and empathetic\nenvironment for individuals seeking mental health sup-\narXiv:2402.08837v1  [cs.CL]  13 Feb 2024\nport. Addressing these challenges can pave the way for\nmore effective, accessible, and empathetic digital mental\nhealth interventions.\nIn dyadic conversations, at any given time one person\nmay have the floor (i.e., is speaking) while the other is\nlistening. Backchannels (BC) refer to behaviors of the\nlistener that do not interrupt the speaker. BCs signal\nattention, agreement, and emotional response to what is\nsaid. Inappropriate BC smiles such as ones that appear\ntoo short or too long or for which the timing appears\nâ€œoffâ€ can disrupt the conversational rapport and result in\nunsuccessful or disrupted conversations. Our objective\nis to understand appropriate BC smiles from dyadic con-\nversations and how an embodied agent can employ them\nwhen interacting with a human.\nConversational agents typically realize BC smiles us-\ning rule-based systems, discriminative approaches, or\nsometimes simply mimicking the smiles of the speaker.\nMimicking, however, fails to generalize to situations that\nrequire a contextually relevant smile. And rule-based\nand discriminative approaches offer limited coverage due\nto the diversity of smiles [6].\nWe present a generative approach for BC smiles in\nlisteners to address these limitations and enable contextu-\nally relevant BC smiles in embodied agents. An overview\nof the approach is presented in Figure 1. Unlike existing\nworks that solely depend on speaker behavior for BC pro-\nduction (see related work section), we use both speaker\nand listener behaviors to study how they affect the in-\ntensity and duration of the BC smile. We use cues from\nprosody, language, and the demographics of dyads to\nidentify statistically significant predictors (referred to as\na conditioning vector) of smiles. In addition to the audio\nfeatures from both interaction participants, we leverage\nthe conditioning vector in generating the BC smiles. In\nthis paper, we:\n1. Annotate backchannel smiles in a face-to-face\ninteraction dataset1 of dyads that differ in their\ncomposition of biological sex and type of relation-\nship.\n2. Present our statistical analysis to identify vari-\nous speaker and listener-specific cues that sig-\nnificantly predict the duration and intensity of\nbackchannel smiles.\n3. Generate backchannel smiles using an attention-\nbased generative model that uses the listener and\nspeaker turn features with the identified signifi-\ncant predictors.\n4. Bridge the gap between the model-based genera-\ntion of non-verbal behaviors (as facial landmarks)\nand their physical realization by emulating the\ngenerated behavior with an embodied agent.\n1Data and code: https://github.com/bmaneesh/Generating-Context-\nSensitive-Backchannel-Smiles/\n5. Show that our BC smile generation yields appro-\npriate and natural-looking smiles through a user\nstudy involving the embodied agent.\nResults suggest speaker sex, their use of negations,\nloudness, word count in the listenerâ€™s turn, their usage of\ncomparisons, and mean pitch are significant predictors\nof BC smile intensity. Our generative approach shows\nthat taking listenersâ€™ behavior into account improves\nperformance, and adding the conditioning vector offers\nsignificant improvements in terms of empirical metrics\nsuch as Average Pose Error (APE) and Probability of\nCorrect Keypoints (PCK).\n2. Related Work\nExisting works have validated the efficacy of an agent-\ndriven conversation in mental health dialogue and coun-\nseling situations. DeVault et al. [5], through their agent-\nbased interviews for distress and trauma symptoms,\nfound that participants were comfortable interacting with\nthe agent as well as sharing intimate information. Utami\nand Bickmore [7] used embodied agents for couples coun-\nseling. Participants reported significantly improved affect\nand intimacy with their partner and generally enjoyed\nthe agent-driven counseling session. Our work builds\non this line of research to improve the BC capabilities of\nagents.\nBackchannel behaviors were traditionally produced\nusing a set of predefined rules based on prosodic or lin-\nguistic cues of the speaker. Both Ward and Tsukahara\n[8], Benus et al. [9] have found prosodic cues (particu-\nlarly pitch and its changes) to be reliable predictors for\nvocal BC occurrence. In contrast, we use prosody and\nlinguistic cues from both speaker and listener to identify\nsignificant predictors of BC smiles.\nIn the multimodal context, Bertrand et al. [10] stud-\nied prosodic, morphological, and discourse markers for\ntheir effect on vocal and gestural backchannels (hand ges-\ntures, smiles, eyebrows), and Truong et al. [11] explored\nvisual BCs by often limiting them to head nods and, at\ntimes, grouping different BCs into the same category [12]\nwithout accounting for their intrinsic differences. They\ndepended on the speakerâ€™s behavior to identify the occur-\nrence and ignored the listener. In addition to leveraging\nthe listener behavior, we specifically study smiles because\nof their diversity and include both unimodal (visual) and\nbimodal (visual together with vocal activity) BC smiles.\nWang et al. [13] introduced diversity in generated\nsmiles by conditioning on a specific class and sampling\nusing a variational autoencoder. Learn2Smile [14] used\nthe facial landmarks of the speaker to generate com-\nplete listener behavior by separately predicting the low-\nfrequency (nods) and high-frequency (blinks) compo-\nnents of facial motion. Ng et al. [15] leverage the speaker\nand listenerâ€™s motion and speech features to predict\nthe listenerâ€™s future motion information. Unlike earlier\nworks that have been limited to facial expression genera-\ntion using landmarks, their usage of 3D Morphable Mod-\nels to define facial expressions offers a flexible solution\nto generate realistic facial expressions in the presence\nof diverse head orientations. These solutions focus on\nthe entire listenerâ€™s behavior and offer no insights about\nspecific BC behaviors. Their integrations are also limited\nto 3D Morphable Models.\nThe BC smiles produced in this work not only leverage\nthe speaker and listener activity but also condition the\ngeneration on salient factors that were found to be signif-\nicant predictors of smile attributes â€“ duration (the time\nelapsed between the onset of a smile and its offset) and\nintensity (maximum amplitude of a smile). Using an em-\nbodied agent, we also bridge the gap between generated\nlandmarks and their physical realization.\n3. Dataset\nOne of the primary challenges in studying non-verbal\nbehavior in mental health interactions is access to an\nappropriate dataset. Patient-therapist interactions or in-\nteractions with mental health professionals are access-\nrestricted to protect the identifiable information of the\nindividuals. As a result, we use a YouTube-based large-\nscale dataset of face-to-face dyadic interactionsâ€“RealTalk\n[16]. The RealTalk dataset consists of individuals taking\nturns asking predefined, intimate questions about family,\ndreams, relationships, illness, and mental health 2. We\nbelieve intimate conversations are among the closest ac-\ncessible alternatives to studying BC behaviors for mental\nhealth applications. In this section, we elaborate on our\ncontributions in terms of the annotations for BC smiles\nand discuss how they differ by the demographics of the\ndyads and features from the speaker and listener turn\npreceding it.\nFigure 2: Distribution of speaker and listener sex across differ-\nent interpersonal relationships in annotated RealTalk dataset.\nRelationships are color-coded: siblings (pink), friends (orange),\npaternal (green), and romantic couple (grey).\n2The original videos can be accessed from https://www.youtube.\ncom/c/TheSkinDeep\n3.1. Annotating Backchannel Smiles\nWe manually annotated 191 BC smiles from 48 (out of 692)\ndyadic interactions in the RealTalk dataset. The dyads\ncomprised male and female participants from different\nethnicities, and social relationships such as siblings, pa-\nternal, romantic, and fraternal. The smiles were nearly\nbalanced across the different interpersonal relationships\n(see Figure 2). An automated facial expression prediction\nframework [17] was used to evaluate the reliability of\nthe manual annotations. About 83% (i.e., 158 smiles) of\nthe 191 annotated smiles had an A-level or higher in-\ntensity. One outlier smile was dropped because of the\nextremely long duration. The resultant 157 smiles, along\nwith their predicted intensity, were used in this work.\nIn addition to the video recordings at 25 fps and 720p\nresolution, the dataset also contains speaker-identified\nturn-level text obtained through automatic transcription\n[18]. The individuals in the dyadic interaction occupied\nfixed positions (left and right) in the videos. In this work,\nthe biological sex of the participants was inferred from\nthe videos. Videos where sex could not be established\nwith confidence were discarded.\n3.2. Effect of Sex and Relationship on\nSmile Attributes\nGiven various interpersonal relationships in the dataset\nof individuals of both sexes, we compared the mean du-\nration of backchannel smiles across the factors using\nANOVA (Table 1) with type-III sum of squares to account\nfor imbalance between males and females. Two-way in-\nteractions between sex, and sex and relationship were\nalso included. The ANOVA analysis suggests that the\nduration of backchannel smiles differs significantly by\nlistener sex and the interaction effect of the listener sex\nand relationship. A post hoc Tukey revealed that male\nlisteners, when interacting with their siblings (regardless\nof speaker sex), express longer BC smiles (p<0.05).\nTable 1\nANOVA of listener sex, speaker sex, and relationship on dura-\ntion of smile. â€˜*â€™ indicates p<0.05 and â€˜**â€™ indicates p<0.01).\nDf Sum Sq Mean Sq F value Pr( >F)\nğ‘ ğ‘’ğ‘¥ğ‘™ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘’ğ‘Ÿ 1 12.36 12.36 4.59 0.0339 *\nğ‘ ğ‘’ğ‘¥ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ 1 1.29 1.29 0.48 0.4907\nğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ â„ğ‘–ğ‘ 3 4.18 1.39 0.52 0.6709\nğ‘ ğ‘’ğ‘¥ğ‘™ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘’ğ‘Ÿ*\nğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ â„ğ‘–ğ‘ 3 42.80 14.27 5.29 0.0017 **\nğ‘ ğ‘’ğ‘¥ğ‘™ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘’ğ‘Ÿ*\nğ‘ ğ‘’ğ‘¥ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ\n1 0.90 0.90 0.33 0.5652\nğ‘ ğ‘’ğ‘¥ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ*\nğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ â„ğ‘–ğ‘ 3 9.70 3.23 1.20 0.3123\nResiduals 144 388.03 2.69\nSimilarly, the intensity of smiles marginally differed\nby the speakerâ€™s sex. The post hoc Tukey revealed that\nthe smiles as a response to a male speaker are less in-\ntense than a female speaker (p<0.1). ANOVA analysis is\npresented in the appendix as Table 4.\n3.3. Effect of Context Cues\nOur contextual cues were extracted from prosody and\nspeech features independently derived from the turns of\nboth the speaker and the listener just before the smile on-\nset. Since the speakerâ€™s turn continues while the listener\nbackchannels, speaker activity till the onset of the smiles\nwas considered in this study. The audio was trimmed\nto the onset to obtain corresponding contextual cues,\nand the Montreal Forced Aligner (MFA) [19] was used to\nextract corresponding transcription information.\nFigure 3: Regression slopes showing the effect of context cues\non the intensity of BC smiles. A positive slope indicates the\nsmile intensity increases with a given feature (vice-versa for a\nnegative slope). * indicates slope is significant at p<0.05 and Â·\nindicates marginal significance at p<0.1.\nProsody cues: Our prosodic features consisted of some\nof the fundamental characteristics of speech, such as\nmean pitch during the turn, range of the pitch, and Root\nMean Square (RMS) energy of the audio signal. These fea-\ntures were chosen because of their relevance (see related\nwork) in BC behavior and also due to the ease of interpre-\ntation as well as their ability to convey various behavioral\ntraits. For example, RMS energy conveys traits such as\nconfidence, doubtfulness, and enthusiasm [ 20]. Lastly,\nusing the OpenSMILE [21] software, prosodic features\nwere obtained.\nSpeech cues: The spoken content of speaker and lis-\ntener turns was also accounted for through variables\nfrom the Linguistic Inquiry and Word Count (LIWC) [22]\nframework. These variables were word count, usage of\nnegations (no, not, never), comparisons (greater, best,\nafter), interrogative words (how, when, what), valence\nof the turns (positive or negative emotion), and focus on\nevents in the past, present and future.\nA generalized linear model predicted the smile inten-\nsity from context cues and dyad demographics. Results\nusing an inverse link function (model explained variance\nğ‘…2 = 0.243) with the prosody and speech cues from\nthe audio signal are presented as Figure 3. Note that\nthe speakersâ€™ and listenersâ€™ context cues were Z-score\nnormalized. Speaker characteristics such as sex and nega-\ntions were found to be significant predictors of intensity.\nFemale speakers elicited significantly narrower smiles\nfrom their listeners, but the speakerâ€™s usage of negations\nresulted in wider smiles. The speakerâ€™s loudness (RMS\nenergy) had a marginally significant negative correlation\nwith the smile intensity. Listener behavior also signif-\nicantly impacted their BC smiles. Using comparative\nwords by the listener and their mean pitch in their pre-\nceding turn resulted in significantly narrower smiles. In\ncontrast, their word count had a marginally significant\npositive correlation with intensity. A similar analysis for\nduration did not reveal any significant correlations.\n4. Modeling Smiles\nTo automatically generate BC smile and non-smile ac-\ntivity in listeners, we use the audio from the speakerâ€™s\ncurrent turn and the listenerâ€™s last turn as input. 15 smiles\nwere dropped due to difficulties in the preprocessing steps\nwith MFA. The remaining 142 annotated smile instances\nwere augmented with an equal number of non-smile in-\nstances. The non-smile instances were identified so that\nthey were at least two seconds away from the onset of\nthe closest smile instance, a strategy adopted from [23]\nfor turn-taking prediction. The mean duration of smiling\nand non-smiling instances was ensured to be the same.\nAttention-based generative model: The generative\nmodel (Figure 4) for facial landmark prediction primarily\nconsisted of an encoder and a decoder with a one-layer\nGRU each. Inputs to the model were embeddings from\nspeaker and listener turns extracted using the pretrained\nvggish model [24]. We limited the input context length to\nuse turn durations of 60 seconds. The output context was\nlimited to predicting one second of facial activity. The\nspeaker vggish embeddings were used as input to the en-\ncoder. The hidden state of the GRU was initialized as the\nmean of the listenerâ€™s turn embeddings. The final hidden\nstate of the encoder was concatenated with the condi-\ntioning vector, and a linear layer with ReLU activation\nwas used to match the dimensionality of the decoderâ€™s\nhidden state. At each decoding step, attention [25] was\napplied between the encoder output and the decoderâ€™s\nFigure 4: Architecture of a generative model incorporating the significant predictors (conditioning vector) for backchannel\nsmiles. Encoder input contains speech embeddings of listener and speaker from the pretrained vggish model. The encoderâ€™s\nfinal hidden state is concatenated with the conditioning vector and then used to initialize the decoderâ€™s hidden state. Decoder\noutput landmarks are sequentially fed (dotted curves) to generate the next landmarks in the output sequence.\nlast hidden state (Equation 1) to use as the input to the\nnext step.\nğ‘(ğ‘ ğ‘¡âˆ’1, â„ğ‘–) =ğ‘£ğ‘‡ ğ‘¡ğ‘ğ‘›â„(ğ‘Šğ‘â„ğ‘– + ğ‘Šğ‘ğ‘ ğ‘¡âˆ’1) (1)\nwhere ğ‘(ğ‘ ğ‘¡âˆ’1, â„ğ‘–) is the attention between decoder\nlast hidden state (ğ‘ ğ‘¡âˆ’1) and encoder output (â„ğ‘–). ğ‘Šğ‘–s and\nğ‘£ are linear layers.\n4.1. Implementation details\nThe videos were split into two vertical halves, one cor-\nresponding to each individual in the dyadic interaction.\nThese were used for facial landmark extraction using the\nAFARtoolbox [17]. To account for various facial shapes,\nwe normalized landmarks to the mean face of the dataset\nusing the approach described in [ 26]. Because of the\nhigh degree of correlation between successive frames,\nframes were downsampled by a factor of three, to use\nevery third frame. Displacement was then calculated as\nthe difference between the landmarks from successive\nframes. These were further subjected to a min-max nor-\nmalization to allow for individual differences in smiling\ndynamics. The normalized displacements were predicted\nusing the attention-based generative model. The pre-\ndicted frame-level displacements were incorporated into\nthe last known listener facial expression to generate the\nsequence of facial landmarks recursively.\nWe enforced teacher-forcing with simulated annealing\nduring training and linearly decreased the likelihood of\nusing ground truth at every 20 epochs. Stochastic Gra-\ndient Descent with a learning rate initialized at 1ğ‘’ âˆ’4\nweight decay and 0.99 momentum were used to mini-\nmize the Mean Squared Error (MSE) between predictions\nand the ground truth. The learning rate was halved when\nvalidation loss plateaued for 20 consecutive epochs. Data\nwas partitioned into 75 (train), 15 (validation), and 15\n(test) split in terms of the number of dyads. Models were\ntrained for 250 epochs, and validation loss was used to de-\ntermine the best model for testing. This was repeated 10\ntimes to evaluate the statistical significance of differences\nagainst baseline speaker-based BC generation setting.\nMetrics: Objective measures of performance from ges-\nture generation approaches, including Average Pose Er-\nror (APE) and Probability of Correct Keypoints (PCK),\nwere adopted to quantify the generated landmarks\nagainst the ground truth from the AFAR toolbox. APE\n(Equation 2) is equivalent to the mean squared error be-\ntween predicted facial expression and ground truth facial\nexpression. PCK (Equation 3) is a proximity-based metric\nthat considers the landmark to be correctly predicted if\nthe difference with ground truth falls below a margin.\nWe report mean PCK for ğœ = 0.1 and 0.2.\nğ´ğ‘ƒ ğ¸= 1\nğ‘˜\nğ‘˜âˆ‘ï¸\nğ‘¦=1\nâ€–(ğ‘¦Ë†(ğ‘) âˆ’ğ‘¦(ğ‘))â€–2 (2)\nwhere ğ‘˜ is the number of landmarks, ğ‘¦Ë†(ğ‘) is the pre-\ndiction and ğ‘¦(ğ‘) is the groundtruth.\nğ‘ƒ ğ¶ğ¾ğœ = 1\nğ‘˜\nğ‘˜âˆ‘ï¸\nğ‘¦=1\nğ›¿(â€–(ğ‘¦Ë†(ğ‘) âˆ’ğ‘¦(ğ‘))â€–2 â‰¤ğœ) (3)\nwhere ğ›¿ is an indicator function and ğœ is the margin.\n4.2. Results\nUsing listener behavior and conditioning vector together\nwith the speaker behavior resulted in improved perfor-\nmance compared to the baseline speaker behavior-based\nprediction. As shown in Table 2, APE decreased by 0.273\npoints while PCK increased by 0.004; these gains were sta-\ntistically significant. When listener behavior was added\nTable 2\nAverage Pose Error (APE) and Probability of Correct Keypoints\n(PCK) metrics for generated facial expressions under various\nexperimental settings. A downward-facing arrow indicates\nlower value implies better generation. â€˜*â€™ indicates significance\nwith p <0.05 with â€˜ Â·â€™ indicates marginal significance with p\n<0.1.\nModel APE â†“ PCKâ†‘\nSpeaker only (Baseline) 9.552 0.219\nSpeaker and Listener 9.346 Â· 0.220Â·\nSpeaker and Listener with\nConditioning vector 9.279* 0.223*\nSpeaker and Conditioning vector 9.615 0.218 Â·\nto the speaker behavior, marginally significant improve-\nments were observed. APE reduced by 0.206 points while\nPCK increased by 0.001 points. These reiterate our hy-\npothesis that both speaker and listener contribute to BC\nbehaviors. When speaker behavior was augmented with\nthe conditioning vector, only nominal differences were\nobserved against the baseline. APE increased by 0.063\npoints, and PCK decreased by 0.001.\nTo understand how the performance varies with dif-\nferent smiles, we predicted APE (and PCK) as a linear\ncombination of duration, intensity, and the model config-\nuration using a regression model. Results from Figure 5\nshow that duration significantly affects the PCK. Interest-\ningly, the positive slope suggests that longer smiles are\ngenerated better over shorter smiles. Only a marginally\nsignificant effect of duration can be observed for APE.\nWith the increase in the intensity of the smile, the gen-\neration performance decreases. This is significant for\nD-level and E-level smiles. Using listener features and\nthe conditioning vector along with the speaker features\nimproves the performance (negative and positive slopes\nfor APE and PCK, respectively) compared to the baseline\nspeaker-based generation. However, this effect is not\nstatistically significant.\nQualitative evaluation of ground truth landmarks from\nFigure 6 suggest the deficiencies of the existing facial\nlandmark prediction approaches [17] to accurately track\nlip corners both in the presence and absence of non-\nfrontal head pose. While a visually noticeable difference\ncan be observed as the smile evolves, the ground truth\nlandmarks fail to capture the subtle lip corner motion.\nThis limitation in the ground truth has resulted in nom-\ninal motion in the predicted landmarks. We also found\nthat BC smiles that co-occur with vocal activity are chal-\nlenging to predict. Figure 7 shows one example where\nthe vertical distance between the upper and lower lips\nincreases and decreases because of the simultaneousyeah\nutterance. However, the model fails to capture this verti-\ncal motion.\nMetrics like APE and PCK provide an objective mea-\nFigure 5: Effect of duration and intensity of smile along with\nablation of inputs on generative model performance measured\nusing APE (top) and PCK (bottom). S & C-speaker and condi-\ntioning vector, S & L-speaker and listener, and S, L & C-speaker\nand listener and conditioning vector as inputs to the model.\nâ€˜Â·â€™, â€˜*â€™ and â€˜***â€™ indicate significance with p <0.1, p <0.05 and p\n<0.001 respectively.\nsure of the prediction. However, evaluating concepts\nsuch as realism and contextual relevance of the BC predic-\ntion requires subjective ratings from human evaluation.\nA convention in evaluating landmark or keypoint-based\ngenerative approaches is the human comparison of pre-\ndicted keypoints against the ground truth [14, 27]. While\nthis might work for problems such as gesture genera-\ntion that involve a strong motion component, evaluating\nsubtle behaviors like facial expressions using a similar\nstrategy could be challenging. To address this concern,\nwe leverage the emulated version of an embodied agent:\nFurhat [28].\nFigure 6: Two sample smiles from the dataset showing their onsets (left-most frame to widest smile frame) and offsets (widest\nsmile frame to right-most frame). Note that while the evolution of smile is noticeable in ground truth landmarks (second\nrow) of the top smile, subtle changes between successive frames of the bottom smile are not captured by its ground truth\nlandmarks. This is also observed in the generated landmarks (third row). Zoom-in recommended. The faces used are from the\nRealTalk dataset.\nFigure 7: Limitation of the current approach in generating\na bimodal backchannel smile. The frames highlighted in red\nbox correspond to the co-occurring verbal â€œyeahâ€. Notice that\nground truth landmarks (second row) fail to capture the verti-\ncal mouth movement. This is also observed in the generated\nlandmarks (third row). Zoom-in recommended. The faces\nused are from the RealTalk dataset.\n5. Smiles on an Embodied Agent\nSo far, we have shown modeling smiles by generating\nfacial landmarks. However, users in real-world scenarios\ndo not expect to see such abstract representations of\nfaces. Aligning these facial landmarks with embodied\nagents is key for an interactable conversational agent.\nTo achieve this, we describe the procedure to transfer\ngenerated landmarks to an embodied robotic simulation\nsystem called Furhat. We then conduct a user study for\nsubjective perceived differences in Furhatâ€™s behavior due\nto BC smile.\n5.1. Emulation Setup\nFurhat allows users to control facial expressions using\na set of facial parameters called BasicParams 3 (ex.\nMOUTH_SMILE_LEFT and MOUTH_SMILE_RIGHT\nto control the left and the right lip corners;\nBROW_UP_LEFT, BROW_UP_RIGHT to control\nthe left and right eyebrows, etc.). Our setup uses these\nparameters to enable the embodied agentâ€™s smile and\nexpress associated eyebrow actions. The landmarks from\na generated smile expression were used to calculate\nthe displacement between successive frames and nor-\nmalized to the [0, 1] range. For eyebrows, only vertical\ndisplacement was used. Our inputs to the Furhat API\nconsisted of the lip corner and eyebrow displacements\ncorresponding to the frame with the widest smile\n(maximum horizontal displacement between the lip\ncorners). The duration of the Furhat smile was set to\nthe duration of the generated smile. Figure 8 shows an\nexample of the resultant expression. The user study was\nconducted using the Furhat Desktop SDK. However, we\ndo not foresee difficulties transferring the emulation\nsetup to a physically embodied Furhat.\n5.2. User Study Procedure\nWe conducted a small-scale user study of participants\nwatching two pre-recorded videos of the Furhat inter-\nacting with an individual. They differ only in terms of\nFurhat expressing a BC smile. In both interactions, Furhat\n3https://docs.furhat.io/remote-api/#python-remote-api\nFigure 8: Four frames of an example Furhat robot emulation\nwith different levels of smiles used as backchannels during\nthe conversation in our user study.\nstarts with a brief introduction of itself, followed by a\nshort questionâ€“â€œHow have you been feeling over the last\ntwo weeks?â€. As the user responds, a smile is generated\nat the appropriate location (see Figure 8). We refer to\nthis scenario as the backchannel setting. Another video\nof the same individual interacting with Furhat with no\nBC (non-backchannel) serves as our baseline. Seven grad-\nuate students then rated each video recording separately.\nNote that raters were not primed on the studyâ€™s outcome,\nand no explicit instructions about smiles were given.\nTo quantify the userâ€™s perception of Furhat interacting\nwith an individual, the influence of BC smile in addition\nto the effect of its intensity and duration, and their will-\ningness to interact with one was quantified through the\nfollowing questions on a 5-point Likert scale (1: strongly\nagree, 5: strongly disagree).\n1. The Furhatâ€™s smiles looked human-like.\n2. The Furhatâ€™s smiles looked natural and friendly.\n3. I would talk to this agent frequently.\n4. I felt the brightness of Furhatâ€™s smiles was appro-\npriate.\n5. The Furhat was smiling for longer or shorter du-\nration than it was expected.\n6. I would feel comfortable talking to this agent\nabout non-personal topics.\n7. I would feel comfortable talking to this agent\nabout personal topics.\nIn addition, open-ended feedback was also a part of the\nquestionnaire. We believe these questions help identify\nsome user-facing challenges in generating BC behav-\niors and how they influence usersâ€™ attitudes to embodied\nagent-based dialogue systems for conversations related\nto mental health.\n5.3. Results\nTable 3 shows that more users (5/7) expressed moderate\nor higher agreement that the Furhat agent with BC smile\nwas human-like than its counterpart without BC smile\n(4/7). One user expressed interest in frequently interact-\ning with the agent in backchannel setting while the lack\nof backchannels resulted in increased hesitancy among\nusers in frequently using it. Three (out of 7) users found\nTable 3\nNumber of responses that expressed moderate or strong agree-\nment along various factors related to the BC smiles when\ninteracting with Furhat with and without backchannel behav-\niors.\nQuestion Backchannel Non-backchannel\nHuman-like 5 4\nNatural 6 6\nWilling to interact 1 0\nAppropriate brightness 3 5\nLonger or shorter smiles 2 0\nPersonal conversations 1 1\nNon-personal conversations 3 2\nthat the brightness of the BC smile was appropriate while\ntwo found that the duration of BC smile was longer or\nshorter than expected. While no difference was observed\nin terms of usersâ€™ preference for Furhat for personal con-\nversations based on the presence of the BC smile, more\nusers (3/7) responded that they would use Furhat with\nBC smiles for non-personal conversations over Furhat\nwithout BC smiles (2/7).\n6. Discussion\nOur quantitative results suggest that both speaker and lis-\ntener behavior are important in generating BC behavior.\nUsing listener behavior together with the conditioning\nvector offered statistically significant improvements in\nperformance when compared to the baseline speaker-\nonly model. This effect was observed both in terms of\nAPE and PCK. We also found that our attention-based\ngenerative model can predict low-intensity smiles better\nthan high-intensity smiles. Our user study shows that\nmore people find our agent human-like when it was able\nto express BC smiles. Participants prefer to interact with\nit over the agent with no BC smile capabilities for non-\npersonal conversations. However, for intimate personal\nconversations, the presence of a BC smile did not sway\ntheir decision.\nSome limitations of this work include the following.\nWe employed an affordable measure of reliability for BC\nsmile annotations using a prediction model over a hu-\nman rater. A robust approach would involve at least one\nmore human annotator to perform reliability annotations\non a portion of the dataset. The statistical analysis also\nassumes that the smiles were independent of the individ-\nuals and dyads. However, a given individual typically\nproduces multiple smiles. Grouping of smiles by factors\nsuch as individuals and dyads can be better modelled us-\ning a mixed-effects model. Our user study was designed\nto demonstrate the feasibility of transferring generated\nfacial landmarks to an embodied agent together with un-\nderstanding perceived differences between interactions\nwith and without BC smiles. An appropriate evaluation\nframework would include the user interacting with the\nagent. Followed by a comparison of qualitative subjec-\ntive ratings of user experience and quantified parameters\n(such as difference in turn duration, language usage, etc.)\nof the interaction with and without BC smiles. We believe\nsuch approaches provide a holistic evaluation to identify\ncritical instances in the interaction. Lastly, we focused on\nBC smiles leaving out other conventional signals such as\nvocal and headpose-based BCs, and how they are affected\nby the cues from the speaker and listener.\n7. Conclusion\nTo enable BCs in embodied agents for mental health\napplications, we proposed an annotated dataset of face-\nto-face conversations including topics related to mental\nhealth. Our statistical analysis showed that speaker gen-\nder together with prosodic and linguistic cues from both\nspeaker and listener turns are significant predictors of\nthe BC smile intensity. Using the significant predictors\ntogether with the speaker and listener behaviors to gen-\nerate BC smiles offers significant improvements in terms\nof empirical metrics over the baseline speaker-centric\ngeneration.\nWe bridge the gap between conventional non-verbal\nbehavior generation approaches such as landmarks and\nposes and their realization by showing that generated\nlandmarks can be transferred to an embodied agent. Thus\ncreating the opportunity for evaluation with a human-\nlike manifestation over a traditional evaluation by com-\nparing generated landmark (or keypoint) outputs. Our\nsmall-scale user study suggests our Furhat agent that\nbackchannels is more human-like and are more likely to\nattract users for non-personal interactions. In addition\nto these contributions, we also discussed some limita-\ntions in existing technology towards generating accurate\nground truth landmarks through examples such as failure\nto capture mouth movement in bimodal BCs and how\nthey affect the generated outputs. We believe these limi-\ntations also serve as directions for future research. Our\nwork serves as a baseline for computer scientists inter-\nested in behavior generation, and an attractive source of\nBC smiles for behavioral scientists to study the effect of\ncontext cues on BC smiles in intimate conversations.\n8. Ethical Statement\nWe proposed a generative approach for backchannel\nsmile production to enable naturalistic interactions with\nembodied AI agents for mental health dialogue. While\nour dataset offers diverse smiles from people in different\ninterpersonal relationships, like many existing genera-\ntive approaches, the choice of pretrained embeddings,\nimbalance between males and females, lack of male-male\nromantic relationships, and lack of age and ethnicity in-\nformation in the dataset might have resulted in biased\ngenerations. We also acknowledge that using embodied\nagents in such sensitive applications should undergo rig-\norous evaluations by technical and domain experts and\nregulatory bodies. In our work, we do not interpret em-\nbodied agents as a substitute for professionals in mental\nhealth or allied areas of healthcare but to provide tools\nfor them to better serve the communityâ€™s demands. We\nbelieve that the advantages and limitations of embod-\nied agents in mental health should be presented to the\nusers and the healthcare experts to provide maximum\nbenefits. The information used in this work is identified\nfrom a publicly available dataset. Also, special attention\nhas been paid to privacy and copyright requirements for\nrelevant images showing individual faces. The user study\nraters were voluntary participants, and the University of\nPittsburgh IRB approved the data collection.\n9. Acknowledgments\nBilalpur and Cohn were supported by the U.S. National In-\nstitutes of Health through award MH R01-096951. Zeinali\nwas supported through the Khoury Distinguished Fel-\nlowship at Northeastern University.\nReferences\n[1] H. Modi, K. Orgera, A. Grover, Exploring barriers to\nmental health care in the u.s. (2022). doi:10.15766/\nrai_a3ewcf9p.\n[2] S. Song, S. Jaiswal, L. Shen, M. Valstar, Spectral rep-\nresentation of behaviour primitives for depression\nanalysis, IEEE Transactions on Affective Comput-\ning 13 (2020) 829â€“844.\n[3] F. Ceccarelli, M. Mahmoud, Multimodal temporal\nmachine learning for bipolar disorder and depres-\nsion recognition, Pattern Analysis and Applications\n25 (2022) 493â€“504.\n[4] Y. Yang, C. Fairbairn, J. F. Cohn, Detecting depres-\nsion severity from vocal prosody, IEEE transactions\non affective computing 4 (2012) 142â€“150.\n[5] D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast,\nA. Gainer, K. Georgila, J. Gratch, A. Hartholt,\nM. Lhommet, et al., Simsensei kiosk: A virtual hu-\nman interviewer for healthcare decision support, in:\nProceedings of the 2014 international conference\non Autonomous agents and multi-agent systems,\n2014, pp. 1061â€“1068.\n[6] Z. Ambadar, J. F. Cohn, L. I. Reed, All smiles are not\ncreated equal: Morphology and timing of smiles\nperceived as amused, polite, and embarrassed/n-\nervous, Journal of nonverbal behavior 33 (2009)\n17â€“34.\n[7] D. Utami, T. Bickmore, Collaborative user responses\nin multiparty interaction with a couples counselor\nrobot, in: 2019 14th ACM/IEEE International Con-\nference on Human-Robot Interaction (HRI), IEEE,\n2019, pp. 294â€“303.\n[8] N. Ward, W. Tsukahara, Prosodic features which\ncue back-channel responses in english and japanese,\nJournal of pragmatics 32 (2000) 1177â€“1207.\n[9] S. Benus, A. Gravano, J. B. Hirschberg, The prosody\nof backchannels in american english (2007).\n[10] R. Bertrand, G. FerrÃ©, P. Blache, R. Espesser,\nS. Rauzy, Backchannels revisited from a multimodal\nperspective, in: Auditory-visual Speech Processing,\n2007, pp. 1â€“5.\n[11] K. P. Truong, R. Poppe, I. de Kok, D. Heylen, A mul-\ntimodal analysis of vocal and visual backchannels\nin spontaneous dialogs., in: INTERSPEECH, 2011,\npp. 2973â€“2976.\n[12] A. Gravano, J. Hirschberg, Backchannel-inviting\ncues in task-oriented dialogue, in: Tenth Annual\nConference of the International Speech Communi-\ncation Association, 2009.\n[13] W. Wang, X. Alameda-Pineda, D. Xu, P. Fua, E. Ricci,\nN. Sebe, Every smile is unique: Landmark-guided\ndiverse smile generation, in: Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 7083â€“7092.\n[14] W. Feng, A. Kannan, G. Gkioxari, C. L. Zit-\nnick, Learn2smile: Learning non-verbal interac-\ntion through observation, in: 2017 IEEE/RSJ In-\nternational Conference on Intelligent Robots and\nSystems (IROS), IEEE, 2017, pp. 4131â€“4138.\n[15] E. Ng, H. Joo, L. Hu, H. Li, T. Darrell, A. Kanazawa,\nS. Ginosar, Learning to listen: Modeling non-\ndeterministic dyadic facial motion, in: Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2022, pp. 20395â€“20405.\n[16] S. Geng, R. Teotia, P. Tendulkar, S. Menon, C. Von-\ndrick, Affective faces for goal-driven dyadic com-\nmunication, arXiv preprint arXiv:2301.10939 (2023).\n[17] I. O. Ertugrul, L. A. Jeni, W. Ding, J. F. Cohn, Afar:\nA deep learning based tool for automated facial\naffect recognition, in: 2019 14th IEEE international\nconference on automatic face & gesture recognition\n(FG 2019), IEEE, 2019, pp. 1â€“1.\n[18] S. Schneider, A. Baevski, R. Collobert, M. Auli,\nwav2vec: Unsupervised pre-training for speech\nrecognition, arXiv preprint arXiv:1904.05862 (2019).\n[19] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner,\nM. Sonderegger, Montreal Forced Aligner: Train-\nable Text-Speech Alignment Using Kaldi, in:\nProc. Interspeech 2017, 2017, pp. 498â€“502. doi:10.\n21437/Interspeech.2017-1386.\n[20] S. A. Memon, Acoustic correlates of the voice qual-\nifiers: A survey, arXiv preprint arXiv:2010.15869\n(2020).\n[21] F. Eyben, M. WÃ¶llmer, B. Schuller, Opensmile: the\nmunich versatile and fast open-source audio fea-\nture extractor, in: Proceedings of the 18th ACM\ninternational conference on Multimedia, 2010, pp.\n1459â€“1462.\n[22] J. W. Pennebaker, R. L. Boyd, K. Jordan, K. Black-\nburn, The development and psychometric proper-\nties of LIWC2015, Technical Report, 2015.\n[23] E. Ekstedt, G. Skantze, Voice activity projec-\ntion: Self-supervised learning of turn-taking events,\narXiv preprint arXiv:2205.09812 (2022).\n[24] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke,\nA. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A.\nSaurous, B. Seybold, et al., Cnn architectures for\nlarge-scale audio classification, in: 2017 ieee inter-\nnational conference on acoustics, speech and signal\nprocessing (icassp), IEEE, 2017, pp. 131â€“135.\n[25] D. Bahdanau, K. Cho, Y. Bengio, Neural machine\ntranslation by jointly learning to align and translate,\narXiv preprint arXiv:1409.0473 (2014).\n[26] S. Stoll, N. C. CamgÃ¶z, S. Hadfield, R. Bowden, Sign\nlanguage production using neural machine transla-\ntion and generative adversarial networks, in: Pro-\nceedings of the 29th British Machine Vision Con-\nference (BMVC 2018), British Machine Vision Asso-\nciation, 2018.\n[27] C. Ahuja, D. W. Lee, R. Ishii, L.-P. Morency, No ges-\ntures left behind: Learning relationships between\nspoken language and freeform gestures, in: Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020, 2020, pp. 1884â€“1895.\n[28] S. Al Moubayed, J. Beskow, G. Skantze,\nB. GranstrÃ¶m, Furhat: a back-projected human-\nlike robot head for multiparty human-machine\ninteraction, in: Cognitive Behavioural Systems:\nCOST 2102 International Training School, Dresden,\nGermany, February 21-26, 2011, Revised Selected\nPapers, Springer, 2012, pp. 114â€“130.\n10. Appendix\n10.1. Distribution of Intensity and\nDuration of Smiles\nFigure 9: Distribution of intensity and duration of BC smiles\nin the annotated dataset. The spread of the histograms shows\nthe diversity of the annotated smiles.\nFigure 9 shows the distribution of annotated Backchan-\nnel (BC) smiles in terms of their intensity and duration.\nThe predicted intensity using the automated approach\nshowed that over 50% of smiles were of B-level intensity,\nand fewer instances of high-intensity smiles (D and E-\nlevels) were also present. The mean duration was 3.18 Â±\n1.71 seconds.\n10.2. Effect of Sex and Relationship on\nSmile Intensity\nTable 4\nANOVA of listener sex, speaker sex, and relationship on inten-\nsity of smile. â€˜Â·â€™ indicates significant at p<0.1.\nDf Sum Sq Mean Sq F value Pr( >F)\nğ‘ ğ‘’ğ‘¥ğ‘™ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘’ğ‘Ÿ 1 0.53 0.53 0.60 0.4417\nğ‘ ğ‘’ğ‘¥ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ 1 2.93 2.93 3.31 0.0710 Â·\nğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ â„ğ‘–ğ‘ 3 3.23 1.08 1.22 0.3055\nğ‘ ğ‘’ğ‘¥ğ‘™ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘’ğ‘Ÿ*\nğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ â„ğ‘–ğ‘ 3 2.00 0.67 0.75 0.5225\nğ‘ ğ‘’ğ‘¥ğ‘™ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘’ğ‘Ÿ*\nğ‘ ğ‘’ğ‘¥ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ\n1 0.10 0.10 0.11 0.7424\nğ‘ ğ‘’ğ‘¥ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ*\nğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ â„ğ‘–ğ‘ 3 3.15 1.05 1.19 0.3176\nResiduals 144 127.49 0.89\nNote that the intensity of the smile differs marginally\nby the speaker sex. It is not affected by other factors such\nas relationship, listener sex and their interaction.",
  "topic": "Embodied cognition",
  "concepts": [
    {
      "name": "Embodied cognition",
      "score": 0.8568905591964722
    },
    {
      "name": "Psychology",
      "score": 0.6108160018920898
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5922281742095947
    },
    {
      "name": "Mental health",
      "score": 0.538480818271637
    },
    {
      "name": "Affect (linguistics)",
      "score": 0.4692782163619995
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.410788357257843
    },
    {
      "name": "Cognitive psychology",
      "score": 0.4077039062976837
    },
    {
      "name": "Computer science",
      "score": 0.26462066173553467
    },
    {
      "name": "Communication",
      "score": 0.24321669340133667
    },
    {
      "name": "Psychotherapist",
      "score": 0.1308828592300415
    },
    {
      "name": "Artificial intelligence",
      "score": 0.11982855200767517
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}