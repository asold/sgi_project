{
    "title": "A Survey of Clinicians' Views of the Utility of Large Language Models",
    "url": "https://openalex.org/W4392757369",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2061271392",
            "name": "Matthew Spotnitz",
            "affiliations": [
                "Columbia University Irving Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A4286553684",
            "name": "Betina Idnay",
            "affiliations": [
                "Columbia University Irving Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2483038344",
            "name": "Emily R Gordon",
            "affiliations": [
                "Columbia University",
                "Columbia University Irving Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2771485394",
            "name": "Rebecca Shyu",
            "affiliations": [
                "Columbia University Irving Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2114730160",
            "name": "Gongbo Zhang",
            "affiliations": [
                "Columbia University Irving Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2068681617",
            "name": "Cong Liu",
            "affiliations": [
                "Columbia University Irving Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A557438043",
            "name": "James J. Cimino",
            "affiliations": [
                "University of Alabama at Birmingham",
                "Columbia University Irving Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2113027333",
            "name": "Chunhua Weng",
            "affiliations": [
                "Columbia University Irving Medical Center"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4205865577",
        "https://openalex.org/W4362608470",
        "https://openalex.org/W4386045865",
        "https://openalex.org/W4323050332",
        "https://openalex.org/W4327946446",
        "https://openalex.org/W4364363895",
        "https://openalex.org/W4377820139",
        "https://openalex.org/W4386120650",
        "https://openalex.org/W4377987327",
        "https://openalex.org/W4376872703",
        "https://openalex.org/W4366769280",
        "https://openalex.org/W4378214315",
        "https://openalex.org/W4320920036",
        "https://openalex.org/W4366351124",
        "https://openalex.org/W4319301505",
        "https://openalex.org/W4367627617",
        "https://openalex.org/W4321167341",
        "https://openalex.org/W4372060667",
        "https://openalex.org/W4376866715",
        "https://openalex.org/W4322761615",
        "https://openalex.org/W4379508361",
        "https://openalex.org/W4380730209",
        "https://openalex.org/W2143749588",
        "https://openalex.org/W4367060129",
        "https://openalex.org/W4387363029"
    ],
    "abstract": "Abstract Objectives Large language models (LLMs) like Generative pre-trained transformer (ChatGPT) are powerful algorithms that have been shown to produce human-like text from input data. Several potential clinical applications of this technology have been proposed and evaluated by biomedical informatics experts. However, few have surveyed health care providers for their opinions about whether the technology is fit for use. Methods We distributed a validated mixed-methods survey to gauge practicing clinicians' comfort with LLMs for a breadth of tasks in clinical practice, research, and education, which were selected from the literature. Results A total of 30 clinicians fully completed the survey. Of the 23 tasks, 16 were rated positively by more than 50% of the respondents. Based on our qualitative analysis, health care providers considered LLMs to have excellent synthesis skills and efficiency. However, our respondents had concerns that LLMs could generate false information and propagate training data bias. Our survey respondents were most comfortable with scenarios that allow LLMs to function in an assistive role, like a physician extender or trainee. Conclusion In a mixed-methods survey of clinicians about LLM use, health care providers were encouraging of having LLMs in health care for many tasks, and especially in assistive roles. There is a need for continued human-centered development of both LLMs and artificial intelligence in general.",
    "full_text": "A Survey of Clinicians’ Views of the Utility of\nLarge Language Models\nMatthew Spotnitz1 Betina Idnay1 Emily R. Gordon1,2 Rebecca Shyu1 Gongbo Zhang1 Cong Liu1\nJames J. Cimino1,3 Chunhua Weng1\n1 Department of Biomedical Informatics, Columbia University Irving\nMedical Center, New York, New York, United States\n2 Department of Dermatology, Vagelos College of Physicians and\nSurgeons, Columbia University Irving Medical Center, New York,\nNew York, United States\n3 Department of Biomedical Informatics and Data Science,\nInformatics Institute, Heersink S chool of Medicine, University of\nAlabama at Birmingham, Birmingham, Alabama, United States\nAppl Clin Inform 2024;15:306– 312.\nAddress for correspondence Matthew Spotnitz, MD, MPH,\nDepartment of Biomedical Informatics, Columbia University Irving\nMedical Center, 630 West 168th Street New York, NY 10032,\nUnited States (e-mail: mes698@mail.harvard.edu).\nChunhua Weng, PhD, Department of Biomedical Informatics,\nColumbia University Irving Medical Center, 630 West 168th Street\nNew York, NY 10032, United States\n(e-mail: cw2384@cumc.columbia.edu).\nBackground and Signiﬁcance\nLarge language models (LLMs), which are a type of artiﬁcial\nintelligence (AI), are designed to process and understand\nhuman language. They are usually trained on massive\namounts of text. For example, ChatGPT is a very efﬁcient\nLLM that has garnered a great deal of public attention for its\ncapabilities since its recent introduction in late 2022.1– 3 The\npossible health care applications of LLMs are numerous.\nRepresentative examples include generating clinical\nKeywords\n► artificial intelligence\n► clinical decision\nsupport\n► clinical informatics\n► clinical information\nsystems\nAbstract Objectives Large language models (LLMs) like Generative pre-trained transformer\n(ChatGPT) are powerful algorithms that have been shown to produce human-like text\nfrom input data. Several potential clinical applications of this technology have been\nproposed and evaluated by biomedical informatics experts. However, few have surveyed\nhealth care providers for their opinions about whether the technology isﬁt for use.\nMethods We distributed a validated mixed-methods survey to gauge practicing\nclinicians ’ comfort with LLMs for a breadth of tasks in clinical practice, research, and\neducation, which were selected from the literature.\nResults A total of 30 clinicians fully completed the survey. Of the 23 tasks, 16 were\nrated positively by more than 50% of the respondents. Based on our qualitative analysis,\nhealth care providers considered LLMs to have excellent synthesis skills and efﬁciency.\nHowever, our respondents had concerns that LLMs could generate false information\nand propagate training data bias.\nOur survey respondents were most comfortable with scenarios that allow LLMs to\nfunction in an assistive role, like a physician extender or trainee.\nConclusion In a mixed-methods survey of clinicians about LLM use, health care\nproviders were encouraging of having LLMs in health care for many tasks, and especially\nin assistive roles. There is a need for continued human-centered development of both\nLLMs and artiﬁcial intelligence in general.\nreceived\nDecember 1, 2023\naccepted after revision\nFebruary 15, 2024\naccepted manuscript online\nMarch 5, 2024\nDOI https://doi.org/\n10.1055/a-2281-7092.\nISSN 1869-0327.\n© 2024. The Author(s).\nThis is an open access article published by Thieme under the terms of the\nCreative Commons Attribution-NonDeri vative-NonCommercial-License,\npermitting copying and reproduction so long as the original work is given\nappropriate credit. Contents may not be used for commercial purposes, or\nadapted, remixed, transformed or built upon. (https://creativecommons.org/\nlicenses/by-nc-nd/4.0/)\nGeorg Thieme Verlag KG, Rüdigerstraße 14, 70469 Stuttgart,\nGermany\nResearch Article\nTHIEME\n306\nAccepted Manuscript online: 2024-03-05   Article published online: 2024-04-17\ndocumentation, personalized educational materials, and\noriginal scientiﬁc manuscripts.4,5\nOne well-known limitation of ChatGPT is its tendency for\n“hallucination,” the generation of text that is perceived as\nconvincing but is not accurate.6 A second limitation is that\nChatGPT can propagate bias that is intrinsic in the training\ndata. These issues have raised concerns about the safety of\nLLM use in health care. Speciﬁcally, some researchers envi-\nsion scenarios where ChatGPT could provide clinical care\nadvice that is outdated, inaccurate, or incomplete.7– 10\nDetermining the best uses of LLMs in health care has been\nthe focus of recent studies. In a prior publication, clinicians\nwith informatics expertise evaluated LLMs for clinical deci-\nsion support and concluded that they may provide valuable\nassistance (Liu et al, 2023).\n11 However, that study did not\naddress the experience of novice LLM users. Furthermore,\nfew studies have investigated health care provider comfort\nwith LLMs or used both quantitative and qualitative meth-\nods. Those studies either asked general questions about the\nsuitability of LLMs in different health care domains\n12,13 or\ncompared it to human performance on one health care\ndelivery service.14\nPresently, applications of LLMs are being developed at a\nrapid pace and could have widespread adoption within\nhealth care by novice and expert users alike. The most ethical\nand effective implementation of the technology must con-\nsider user requirements and concerns from representative\nstakeholders of this technology in the clinical setting. In this\nstudy, we surveyed diverse practicing clinicians about using\nLLMs for tasks in clinical practice, research, and education\nand summarized their perceptions of the potential and\nlimitations of LLMs, to inform the development of clinically\nmeaningful evaluation standards for LLMs to ensure their\nappropriate and ethical implementation in clinical settings.\nMethods\nStudy Design and Sampling\nThe survey instrument, which is shown in►Supplementary\nAppendix 1 (available in the online version only), was\ndeveloped by two authors with both clinical and informatics\nexperience (M.S., B.I.) and reﬁned based on feedback from a\nthird author (E.R.G.). It was implemented through Qualtrics\n(Qualtrics, Provo, UT), took approximately 15 minutes to\ncomplete, and gauged clinicians’ perceptions on the appro-\npriateness of using LLMs in clinical practice, research, and\neducation.\nThe opening questions quantiﬁed participants’ experi-\nence in clinical medicine and informatics with multiple\nchoice answers. Next, there were questions that asked if\nthe amount of LLM experience in health care within the past\nyear had exceeded 50 hours. Then, there were questions that\nprompted rating the appropriateness of LLM use for 23\ndifferent tasks in clinical practice, research, and education\non a 5-point Likert scale (i.e., Highly Appropriate to Highly\nInappropriate). Those tasks represented a sample of pro-\nposed LLM uses that were synthesized from the literature\nand included, but were not limited to, optimizing alerts for\nclinical decision support, providing a differential diagnosis,\nwriting a discharge summary, recommending treatment\noptions, translating radiology reports into layperson lan-\nguage, writing scientiﬁc manuscripts, and generating per-\nsonalized study plans for students or trainees among\nothers.\n2,6– 8,15– 25\nWe distributed the proposed clinical practice tasks over\ntwo questions, and had one question for research tasks and\none question for education tasks. The purpose of this section\nwas to measure the appropriateness of these tasks by cate-\ngory and to determine if any individual tasks were negative\nor positive outliers. We hypothesized that perceptions about\nthe strengths, limitations, and ethical concerns about LLMs\ncould contribute to the ratings. Therefore, we had open-\nended questions about each of those as well as an open-\nended question about other possible uses of LLMs.\nData Collection\nWe recruited participants with an email invitation that was\nsent to a listserv of clinicians at Columbia University Irving\nMedical Center and by word of mouth. To be eligible,\nparticipants needed to be practicing clinicians af ﬁliated\nwith Columbia University within the past 12 months and\nwere able to comprehend and communicate ﬂuently in\nEnglish. Respondents were compensated with a $20 Amazon\nGift Card for completing the survey.\nData Analysis\nWe calculated descriptive statistics on the participants and\ntabulated their ratings for each question. Two independent\nreviewers performed an inductive thematic analysis with the\nnarrative comments. Both of them performed independent\ncoding of free text using NVivo (Version 14) with generation\nof themes. They met regularly for a total of three times and\ndeveloped themes iteratively. Once a consensus was reached,\nthe reviewers determined aﬁnal list of themes and applied\nthem to the narrative comments. A third reviewer was\navailable to resolve any discrepancies.\nResults\nWe recruited practicing clinicians from internal medicine,\notolaryngology, ophthalmology, pediatrics, urology, anes-\nthesiology, neurosurgery, and general surgery. We distribut-\ned a prescreening survey to 350 clinicians, among whom 108\nresponded, and 30 were eligible and enrolled. All completed\nthe survey. Their demographics are shown in\n►Table 1.\nSurvey Ratings\nHeat maps of the ratings for clinical, research, and education\ntasks are shown in►Tables 2– 4. Of the 23 tasks, 16 (69.6%)\nhad positive ratings by at least half of the participants. The\nhighest rated tasks were“assist with vaccine development by\npredicting the antigenicity of different proteins from geno-\nmic data” (25 positive ratings from 30 participants),“model\nthe spread and transmission of an infectious disease ”\n(25 positive ratings), “generate case studies for training\npurposes” (24 positive ratings), “monitor data for an\nApplied Clinical Informatics Vol. 15 No. 2/2024 © 2024. The Author(s).\nClinician Large Language Model Survey Spotnitz et al. 307\n\nemerging disease cluster” (24 positive ratings), and“gener-\nate alerts to improve compliance with clinical guidelines” (24\npositive ratings). In contrast, 7 out of 23 tasks had positive\nratings by fewer than half of the participants. Two of the tasks\nwith the lowest number of positive ratings also had the\nhighest number of negative ratings, which were“Respond to\npatient questions about a radiology report ” (7 positive\nratings, 16 negative ratings), and“Write an original scientiﬁc\nmanuscript” (5 positive ratings, 20 negative ratings).\nThematic Analysis\nWe received 20 open-ended responses about LLM strengths,\nlimitations, and ethical concerns, respectively. The responses\nabout the limitations and ethical concerns of LLMs were very\nsimilar; therefore, we combined them for a total of 40\nresponses. There were 19 responses about additional uses\nof LLMs. The themes and corresponding examples are shown\nin\n►Table 5. Some respondent answers addressed multiple\nthemes and were mapped to each of them. The full responses\nto our open-ended questions are shown in►Supplementary\nAppendix 2 (available in the online version only).\nDiscussion\nLLMs promise to transform health care. A human-centered\napproach is critical to ensure ethical and effective imple-\nmentation of this powerful technology in clinical settings.\nThis was theﬁrst study of clinical practitioners, who were\nmostly novice LLM users and from diverse health care\ndomains, to rate tasks that may be improved by LLMs.\nThe fundamental theorem of biomedical informatics is\nuser augmentation so that“a person working in partnership\nwith an information resource is better than that same person\nunassisted.”\n26 Similarly, the clinicians who we studied were\nencouraging of having LLMs as their assistants. The tasks that\nTable 2 Heatmap of ratings for large language model uses in clinical practice tasks (orange¼lowest; yellow¼highest)\nTask Highly Inappropriate Inappropriate Neutral Appropriate Highly Appropriate\nGenerate alerts to improve\ncompliance with clinical\nguidelines\n12 3 1 9 5\nProvide a differential diagnosis 0 2 9 16 3\nDescribe how to perform a\nprocedure\n07 6 1 6 1\nTranslate radiology reports into\nlayperson language\n22 5 1 5 6\nSynthesize and present patient\ndata from the electronic health\nrecord for clinical decision\nsupport\n25 4 1 5 4\nWrite discharge summaries 2 6 2 13 7\nSuggest patient management or\ntreatment options\n25 8 1 3 2\nCheck for inaccuracies in a\nradiology report, and notify\nproviders of them\n2 1 10 11 6\nReport current information on a\ntopic for clinical decision\nsupport\n3 1 14 10 2\nWrite radiology reports 3 8 12 6 1\nRespond to patient questions\nabout a radiology report\n21 4 7 6 1\nTable 1 Participant information\nSurvey characteristics N (%)\nClinical Training\n1– 2y e a r s 1 2( 4 0 )\n3– 5y e a r s 3( 1 0 )\n> 5y e a r s 1 5( 5 0 )\nInformatics Training\nNone 26 (86.7)\n1– 2y e a r s 3( 1 0 )\n3þ years 1 (3.3)\nLLM use within the past 12 months\n< 50 hours 28 (93.3)\n50þ hours 2 (6.7)\nAbbreviations: LLM, large language model;N, number; %, percent.\nApplied Clinical Informatics Vol. 15 No. 2/2024 © 2024. The Author(s).\nClinician Large Language Model Survey Spotnitz et al.308\n\nleveraged LLMs for supportive roles were rated the highest.\nIn the qualitative analysis, emerging themes were that LLMs\nwere highly skilled at different tasks; however, there were\nethical concerns about using the technology. Supportive LLM\nroles may have been more popular, because in those scenari-\nos clinicians could correct for false information that the\nalgorithms might generate.\nTherefore, we expect that clinicians would prefer to have\nLLMs function more like trainees or physician extenders than\nattending physicians. LLMs could assist clinicians by drafting\nnotes and reports, making suggestions for patient triage,\nextracting patient information from charts, and identifying\ndiscrepancies from standard patient care. Since LLMs are\nvery skilled at processing large amount of data, they could\nhelp monitor patients in critical care and perioperative\nsettings. Also, they could help translate medical information\nbetween languages, or from technical jargon into layperson\nlanguage. The contributions of LLMs to these tasks could be\nreviewed by a clinician.\nHowever, the notion of allowing LLMs to function without\nsupervision in clinical practice raises ethical concerns. They\nhave a propensity to produce false information and propa-\ngate data bias, which could lead to incorrect medical deci-\nsions. Furthermore, LLMs lack human empathy, which could\nbe a source of mistrust with patients. Instead, patients are\nmore likely to trust medical advice from a clinician because\nof the human connection. Overall, we believe that clinicians\nwould prefer to have LLMs assist them instead of replace\ntheir practice.\nOur study participants were encouraging of LLM assis-\ntance in the research and education domains as well. In\nresearch, the processing power of LLMs would allow them to\nhelp with a range of statistical analyses. Also, their linguistic\ncapabilities could translate ideas across human and pro-\ngramming languages. Those skills could be especially useful\nin large research networks, which consist of individuals from\ndifferent countries and who have different programming\nskills. However, having LLMs author an original manuscript\nTable 3 Heatmap of ratings for large language model uses in research tasks (orange¼lowest; yellow¼highest)\nTask Highly Inappropriate Inappropriate Neutral Appropriate Highly Appropriate\nSuggest how to interpret a\ndataset\n12 7 1 8 2\nAssist with vaccine development\nby predicting the antigenicity of\ndifferent proteins from genomic\ndata\n10 4 1 6 9\nModel the spread and\ntransmission of an infectious\ndisease\n11 3 1 6 9\nMonitor data for an emerging\ndisease cluster\n12 3 1 5 9\nGenerate programming code 0 0 10 14 6\nWrite a literature review for a\nresearch publication\n58 8 8 1\nWrite an original scientiﬁc\nmanuscript\n10 10 5 3 2\nTable 4 Heatmap of ratings for large language model uses in education tasks (orange¼lowest; yellow¼highest)\nTask Highly Inappropriate Inappropriate Neutral Appropriate Highly Appropriate\nGenerate interactive simulations\nfor training purposes\n01 6 1 7 6\nGenerate case studies for\ntraining purposes\n01 5 1 5 9\nWrite quizzes and self-\nassessments for students or\ntrainees\n02 6 1 5 7\nGenerate a personalized study\nplan for a medical student\n01 8 1 5 6\nWrite personalized patient\neducation texts for students or\ntrainees\n33 1 2 8 4\nApplied Clinical Informatics Vol. 15 No. 2/2024 © 2024. The Author(s).\nClinician Large Language Model Survey Spotnitz et al. 309\n\ninstead of a researcher would raise similar ethical concerns\nto allowing LLMs to function as an autonomous clinician. The\neducation tasks raised the fewest ethical concerns, perhaps\nbecause students have regular supervision and a smaller role\nin direct patient care than clinicians or researchers.\nOur sampling method followed a deﬁned set of recruit-\nment criteria and enrolled a total of 30 practicing clinicians\nwho completed the survey for this study. While a larger\nnumber of respondents would have been desirable, ours\ncovered a variety of clinical domains and provided valuable,\noriginal insights regarding the ethical and reliable uses of\nLLMs in clinical settings. Given the unusually rapid evolution\nof LLM technology, this early study is timely and makes\nmeaningful contributions by including the voices of key\nstakeholders of implementing LLMs for clinical tasks.\nA limitation of our study, and a potential source of\nsampling bias, is that only a relatively small number of\nparticipants from a single medical center were recruited\nTable 5 Summary of narrative comments about perceived advantages, ethical concerns, and clinical applications of large language\nmodels with representative examples\nAdvantages (n ¼20) Ethical concerns ( n ¼40) Recommended clinical\napplications for using LLMs (n\n¼19)\nAptitude for speciﬁct a s k s\n(n ¼10)\nAbility to generateﬁrst drafts with\nlow effort\nIt can also help students and\nproviders come up with a\ndifferential diagnosis\nTheoretically could reduce\npaperwork/administrative work\nAbility to write code for novice\nprogrammers\nFalse information (n ¼15)\nHallucination, fabrication, reinforcement of\nassumptions and biases\nWith the confabulation/hallucination issue, does not\nallow for the uncertainty that is almost always present\nin medicine\nIts propensity to make up information\nDrafting documentation (n ¼8)\nNote templates/drafts, especially for\nroutine and predictable things like\nprocedures\nSynthesis ability (n ¼9)\nSynthesize large amounts of data\nquickly\nGood at synthesizing information\nin a clear concise fashion\nWorsens patient care (n ¼14)\nThis technology if unchecked at a patient care level\nmay have serious implications of harm to patients\nMajor concern about inappropriate use by lay public to\nself-diagnose\nAlso worry about who gets care from health care\nworkers versus from direct-to-patient LLM which could\nbe less personalized, and initially less validated and\ntrustworthy\nDecision support (n ¼5)\nAnything providing recommendations\nto patients or providers\nFlagging concerning trends (VS,\nlaboratory values) earlier; providing\nguidance in managing chronic\nconditions\nEfﬁciency (n ¼8)\nSaves time and improves efﬁciency\nData bias (n ¼12)\nResults are only as good as the datasets that are fed\ninto the LLM\nGiven the fast pace of evidence in health care, can be\ntrained on old evidence\nPoor data quality leads to poor answers\nReplicates existing biases\nPatient communication (n ¼4)\nDrafting replies to patient messages in\nthe outpatient inbox that are modeled\noff of the provider’s communication\nstyle\nAccuracy (\nn ¼5)\nFairly accurate and provide higher\nquality, more personalized\ninformation than most patient-\nfacing information available on\nthe internet\nHuman oversight critical (n ¼7)\nThey should not replace informed decision-making for\npatients or clinical decision-making for doctors\ncompletely\n–\nAccessibility (n ¼4)\nTranslating medical documents\ninto plain English\nImpersonal (n ¼6)\nWorry about who gets care from health care workers\nversus from direct-to-patient LLM which could be less\npersonalized\n–\n– Legal concerns (n ¼5)\nGray area of ethical/legal limitations\n–\n– Privacy (n ¼3)\nWorries of patient conﬁdentiality\n–\n– W o r s e n sc l i n i c i a n s(n ¼2)\nIf we become reliant on LLMs, we may lose\nopportunities to practice interpreting/synthesizing\ndata ourselves\n–\nAbbreviation: LLM, large language model; VS, vital signs.\nApplied Clinical Informatics Vol. 15 No. 2/2024 © 2024. The Author(s).\nClinician Large Language Model Survey Spotnitz et al.310\n\nby convenience sampling. Also, we used self-reported data as\nkey elements of our analysis. These data may have introduced\nbiases due to varying accuracy in self-reports and varying\nawareness of the problems by reporting individuals. Despite\nthese limitations, we have developed an instrument that is\ncapable of discerning different opinions about LLM use. We\nhope ourﬁndings can be taken into consideration by devel-\nopers as theﬁeld continues its rapid evolution. As further\nprogress is made, and clinicians have more signiﬁcant expe-\nrience with this technology, subsequent studies can build on\nour methods and experience to study larger sample sizes at\nmultiple institutions to gain additional insights for future\ndirections.\nFuture studies with a larger and more diverse sample will\nbe warranted to ensure the generalizability of the results and\nallow for stratiﬁcation by variables that could affect percep-\ntions of LLM use, such as age, duration of clinical training,\nprovider specialty, and experience with the technology.\nThose perceptions could be tracked longitudinally to gauge\nhow they change over time. A more robust study about\nparticipants’ general knowledge of LLMs and AI would\nstrengthen future studies. Speci ﬁcally, gauging to what\nextent participants understand how an AI algorithm is able\nto work, predict, learn, and generate responses, would be a\nvaluable part of an analysis. Furthermore, comparing the\nperceptions regarding different LLMs, and how LLM-gener-\nated errors compare with human errors, may provide a more\nbalanced view of the technology.\nOur study found that health care providers would prefer\nto have LLMs assist than replace them. That ﬁnding has\nimplications for future development and implementation\nof LLMs in clinical practice, research, and education. Studying\nactive clinicians with novice LLM experience helped identify\nthat preference. Therefore, for optimal development and\nimplementation of LLMs in health care, continued human\ncentered development is critical.\nConclusion\nClinicians are generally supportive of the use of LLMs for many\ntasks in clinical practice, research, and education, especially\nwhere LLMs play a supportive role to humans. Continued\nhuman centered development of the technology is critical.\nClinical Relevance Statement\nWe studied health care providers about the best uses of LLMs\nin health care. The clinicians who we studied were encour-\naging of having LLMs assist them for a range of tasks. The\nresults of our work have implications for implementation of\nLLMs in health care.\nMultiple Choice Questions\n1. Which of the following are ethical concerns about LLM\nuse?\na. Efﬁciency\nb. Confabulation or hallucination\nc. Ability to synthesize information\nd. Capacity to make technical language accessible\nAnswer: b. Confabulation or hallucination can cause the\nLLMs to generate false information, which can lead to\nincorrect medical decisions. The other answer choices are\nadvantages of the technology.\n2. What is the fundamental theorem of biomedical infor-\nmatics?\na. An information resource is better without assistance\nfrom a person.\nb. An information resource working in partnership with a\nperson is better than an information resource unassisted.\nc. A person working in partnership with an information\nresource is better than that same person unassisted.\nd. A person is better without an information resource.\nAnswer: c. The fundamental theorem of biomedical infor-\nmatics states that people are more effective when part-\nnered with an information resource. The alternatives,\nwhich are to have no cooperation with information\nresources and people, or to have people assist information\nresources, are less effective.\nProtection of Human Subjects\nThe study was performed in compliance with the World\nMedical Association Declaration of Helsinki on Ethical\nPrinciples for Medical Research Involving Human Sub-\njects, and was reviewed by Columbia University Irving\nMedical Center Institutional Review Board (AAAU7954).\nFunding\nThis work was supported by National Library of Medicine\n(NLM) grants R01LM014344 and R01LM009886 to C.W.,\nNational Human Genome Institute grant R01HG012655 to\nC.L., and by National Center for Advancing Clinical and\nTranslational Science grant UL1TR001873 to Columbia\nUniversity Irving Medical Center. B.I. and R.S. acknowl-\nedge support from NLM grant T15LM007079.\nConﬂict of Interest\nNone declared.\nReferences\n1 Sezgin E, Sirrianni J, Linwood SL. Operationalizing and imple-\nmenting pretrained, large artiﬁcial intelligence linguistic models\nin the US health care system: outlook of generative pretrained\ntransformer 3 (GPT-3) as a service model. JMIR Med Inform 2022;\n10(02):e32875\n2 Elkassem AA, Smith AD. Potential use cases for ChatGPT in\nradiology reporting. AJR Am J Roentgenol 2023;221(03):373– 376\n3 Rao A, Pang M, Kim J, et al. Assessing the utility of ChatGPT\nthroughout the entire clinical workﬂow: development and us-\nability study. J Med Internet Res 2023;25:e48659\n4 Cascella M, Montomoli J, Bellini V, Bignami E. Evaluating the\nfeasibility of ChatGPT in healthcare: an analysis of multiple\nclinical and research scenarios. J Med Syst 2023;47(01):33\n5 Sallam M. ChatGPT utility in healthcare education, research, and\npractice: systematic review on the promising perspectives and\nvalid concerns. Healthc Pap 2023;11(06):867\nApplied Clinical Informatics Vol. 15 No. 2/2024 © 2024. The Author(s).\nClinician Large Language Model Survey Spotnitz et al. 311\n\n6 Athaluri SA, Manthena SV, Kesapragada VSRKM, Yarlagadda V,\nDave T, Duddumpudi RTS. Exploring the boundaries of reality:\ninvestigating the phenomenon of artiﬁcial intelligence hallucina-\ntion in scientiﬁc writing through ChatGPT references. Cureus\n2023;15(04):e37432\n7 Peng Y, Rousseau JF, Shortliffe EH, Weng C. AI-generated text may\nhave a role in evidence-based medicine. Nat Med 2023;29(07):\n1593– 1594\n8 Tang L, Sun Z, Idnay B, et al. Evaluating large language models on\nmedical evidence summarization. NPJ Digit Med 2023;6(01):158\n9 Deik A. Potential beneﬁts and perils of incorporating ChatGPT to\nthe movement disorders clinic. J Mov Disord 2023;16(02):\n158– 162\n10 Shashavar Y, Choudhury A. User intentions to use ChatGPT for\nself-diagnosis and health-related purposes: cross-sectional sur-\nvey study. JMIR Hum Factors 2023;10:e47564\n11 Liu S, Wright AP, Patterson BL, et al. Using AI-generated sugges-\ntions from ChatGPT to optimize clinical decision support. J Am\nMed Inform Assoc 2023;30(07):1237– 1245\n12 Hosseini M, Gao CA, Leibovitz DM, et al. An exploratory survey\nabout using ChatGPT in education, healthcare, and research.\nmedRxiv 2023.03.31.23287979\n13 Choudhury A, Shamszare H. Investigating the impact of user trust\non the adoption and use of ChatGPT: survey analysis. J Med\nInternet Res 2023;25:e47184\n14 Dash D, Rahul T, Banda JM, et al. Evaluation of GPT-3.5 and GPT-4\nfor supporting real-world information needs in healthcare deliv-\nery. 2023. arXiv:2304.13714\n15 Hirosawa T, Harada Y, Yokose M, Sakamoto T, Kawamura R,\nShimizu T. Diagnostic accuracy of differential-diagnosis lists\ngenerated by generative pretrained transformer 3 chatbot for\nclinical vignettes with common chief complaints: a pilot study. Int\nJ Environ Res Public Health 2023;20(04):3378\n16 Cheng K, Li Z, He Y, et al. Potential use of artiﬁcial intelligence in\ninfectious disease: take ChatGPT as an example. Ann Biomed Eng\n2023;51(06):1130– 1135\n17 Patel SB, Lam K. ChatGPT: the future of discharge summaries?\nLancet Digit Health 2023;5(03):e107– e108\n18 Galido PV, Butala S, Chakerian M, Agustines D. A case study\ndemonstrating applications of ChatGPT in the clinical management\nof treatment-resistant schizophrenia. Cureus 2023;15(04):e38166\n19 Sharma S, Pajai S, Prasad R, et al. A critical review of ChatGPT as a\npotential substitute for diabetes educators. Cureus 2023;15(05):\ne38380\n20 Macdonald C, Adeloye D, Sheikh A, Rudan I. Can ChatGPT draft a\nresearch article? An example of population-level vaccine effec-\ntiveness analysis. J Glob Health 2023;13:01003\n21 Májovský M, Černý M, Kasal M, Komarc M, Netuka D. Artiﬁcial\nintelligence can generate fraudulent but authentic-looking scien-\ntiﬁc medical articles: Pandora’s box has been opened. J Med\nInternet Res 2023;25:e46924\n22 Abd-Alrazaq A, AlSaad R, Alhuwail D, et al. Large language models\nin medical education: opportunities, challenges, and future\ndirections. JMIR Med Educ 2023;9:e48291\n23 Eysenbach G. The role of ChatGPT, generative language models,\nand artiﬁcial intelligence in medical education: a conversation\nwith ChatGPT and a call for papers. JMIR Med Educ 2023;9:\ne46885\n24 Karabacak M, Ozkara BB, Margetis K, Wintermark M, Bisdas S. The\nadvent of generative language models in medical education. JMIR\nMed Educ 2023;9:e48163\n25 Kanjee Z, Crowe B, Rodman A. Accuracy of a generative artiﬁcial\nintelligence model in a complex diagnostic challenge. JAMA 2023;\n330(01):78– 80\n26 Friedman CPA. A“fundamental theorem” of biomedical informat-\nics. J Am Med Inform Assoc 2009;16(02):169– 170\nApplied Clinical Informatics Vol. 15 No. 2/2024 © 2024. The Author(s).\nClinician Large Language Model Survey Spotnitz et al.312\n"
}