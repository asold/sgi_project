{
  "title": "Vision-language foundation model for 3D medical imaging",
  "url": "https://openalex.org/W4413030191",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2103539842",
      "name": "Jing Wu",
      "affiliations": [
        "Central South University",
        "Second Xiangya Hospital of Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A2097511648",
      "name": "Yuli Wang",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2950460920",
      "name": "Zhusi Zhong",
      "affiliations": [
        "Rhode Island Hospital",
        "Providence College"
      ]
    },
    {
      "id": "https://openalex.org/A2474119896",
      "name": "Weihua Liao",
      "affiliations": [
        "Central South University",
        "Xiangya Hospital Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A3174853315",
      "name": "Natalia Trayanova",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2570256168",
      "name": "Zhicheng Jiao",
      "affiliations": [
        "Providence College",
        "Rhode Island Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2133274663",
      "name": "Harrison X. Bai",
      "affiliations": [
        "Johns Hopkins Medicine",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2103539842",
      "name": "Jing Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097511648",
      "name": "Yuli Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2950460920",
      "name": "Zhusi Zhong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2474119896",
      "name": "Weihua Liao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3174853315",
      "name": "Natalia Trayanova",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2570256168",
      "name": "Zhicheng Jiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133274663",
      "name": "Harrison X. Bai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2919356958",
    "https://openalex.org/W2979373466",
    "https://openalex.org/W4391690006",
    "https://openalex.org/W3083622693",
    "https://openalex.org/W3167960955",
    "https://openalex.org/W2567292221",
    "https://openalex.org/W4392120599",
    "https://openalex.org/W4404509535",
    "https://openalex.org/W4379259189",
    "https://openalex.org/W4313439128",
    "https://openalex.org/W4385645306",
    "https://openalex.org/W6854691855",
    "https://openalex.org/W4393248026",
    "https://openalex.org/W4388890632",
    "https://openalex.org/W4393904064",
    "https://openalex.org/W4395687388",
    "https://openalex.org/W1670132599",
    "https://openalex.org/W3045599073",
    "https://openalex.org/W2947059957",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W4287813795",
    "https://openalex.org/W3177477686",
    "https://openalex.org/W4385546024",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W4396758996",
    "https://openalex.org/W4394780903",
    "https://openalex.org/W4319793437",
    "https://openalex.org/W3082123846",
    "https://openalex.org/W4398157498",
    "https://openalex.org/W3089474066",
    "https://openalex.org/W2963201472",
    "https://openalex.org/W4372336638",
    "https://openalex.org/W4405873576",
    "https://openalex.org/W3175737276",
    "https://openalex.org/W3150587331",
    "https://openalex.org/W4308023490",
    "https://openalex.org/W4200150166",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4281902802",
    "https://openalex.org/W4388642243",
    "https://openalex.org/W4405919180",
    "https://openalex.org/W4312428231",
    "https://openalex.org/W3181186005",
    "https://openalex.org/W4392103968",
    "https://openalex.org/W4321599983",
    "https://openalex.org/W4293193211",
    "https://openalex.org/W4387211524",
    "https://openalex.org/W4403650369",
    "https://openalex.org/W4393213251",
    "https://openalex.org/W4400136387",
    "https://openalex.org/W4403089476",
    "https://openalex.org/W4399597796",
    "https://openalex.org/W4415334474",
    "https://openalex.org/W4406841233",
    "https://openalex.org/W4405626241",
    "https://openalex.org/W4405253680",
    "https://openalex.org/W4409356531",
    "https://openalex.org/W4404648584",
    "https://openalex.org/W4403995561",
    "https://openalex.org/W4403581960",
    "https://openalex.org/W4403444353",
    "https://openalex.org/W4403851339",
    "https://openalex.org/W4403811765",
    "https://openalex.org/W4400434448",
    "https://openalex.org/W4408190450",
    "https://openalex.org/W4300485340",
    "https://openalex.org/W2945976633",
    "https://openalex.org/W4401808768",
    "https://openalex.org/W4324144441",
    "https://openalex.org/W4394780588",
    "https://openalex.org/W4406208390",
    "https://openalex.org/W4391849646",
    "https://openalex.org/W4367299897",
    "https://openalex.org/W4366280062",
    "https://openalex.org/W4318566866"
  ],
  "abstract": null,
  "full_text": "npj |artiﬁcial intelligence Review\nhttps://doi.org/10.1038/s44387-025-00015-9\nVision-language foundation model for 3D\nmedical imaging\nCheck for updates\nJing Wu1,7,Y u l iW a n g2,7, Zhusi Zhong3, Weihua Liao4,N a t a l i aT r a y a n o v a5,Z h i c h e n gJ i a o3 &\nHarrison X. Bai2,6\nRecent advances in AI, especially vision-language foundation models (VLFMs), show promise in\nautomating radiology report generation from complex 3D medical imaging data. Our review analyzes\n23 studies on VLFMs, focusing on model architectures, capabilities, training datasets, and evaluation\nmetrics. We discuss AI’s evolution in radiology, emphasizing the need for diverse datasets and\nstandardized metrics, as challenges remain in producing consistent, high-quality reports.\nRadiology occupies a central role in contemporary healthcare, serving\nas a fundamental tool in the diagnosis, treatment planning, and mon-\nitoring of a myriad of diseases\n1,2. Among the advancements in medical\nimaging, 3D imaging technologies have revolutionized the ﬁeld by\nproviding unprecedented detail an d depth, allowing for intricate\nvisualizations of anatomical structures3–5. Despite these advancements,\nthe complexity of interpreting 3D imaging data adds a signi ﬁcant\nburden on radiologists, who must integrate vast information swiftly to\nmake accurate diagnostic decisions\n6.T h i sg r o w i n gc o m p l e x i t y ,c o m -\npounded by an increasing volume of imaging studies and a global\nshortage of trained radiology professionals, creates a critical bottleneck\nin healthcare systems worldwide.\nThe advent of artiﬁcial intelligence (AI), particularly vision-language\nfoundation models (VLFMs), has introduced the possibility of mitigating\nthese challenges by automating the generation of radiological reports. These\nVLFMs are designed to synergistically combine the capabilities of advanced\nimage processing algorithms and sophisticated natural language processing\ntechniques7,8. By doing so, they aim to mimic the analytical processes of\nhuman radiologists, interpreting complex imaging data and articulating\nﬁndings in the form of coherent, detailed reports.\nThis integration of visual and linguistic AI capabilities is particularly\npromising for handling the intricaciesof 3D medical imaging. The models are\nnot only tasked with recognizing and categorizing pathological features but\nalso with describing these features andtheir implications for patient care in a\nclinically relevant and accessible manner. Despite the signiﬁcant potential of\nthese models to enhance diagnostic workﬂows, their practical application faces\nseveral challenges. Current AI technologies, while rapidly evolving, still\nstruggle to consistently produce high-quality reports that can match the depth\nof analysis and insight provided by human experts. The need for extensive\ntraining data, the ability to generalize across diverse clinical scenarios, and the\ndevelopment of evaluation metrics that adequately capture the clinical utility\nand accuracy of AI-generated reports are among the foremost challenges.\nThis review paper aims to explore the recent advancements and the\ncurrent state\n8,9 of VLFMs speciﬁcally designed for 3D medical imaging.\nFigure1 presents an overview of the VLFMsframework, encompassing key\ncomponents such as datasets, model architecture, evaluation strategies, and\nthe integration of VLFMs into clinical workﬂows. It assesses their cap-\nabilities, the challenges they face, and outlines future directions necessary for\ntheir integration into clinical practice. The structure of this paper is orga-\nnized as follows:“Methods“section details our literature search strategy and\nspeciﬁes the inclusion criteria for studies on VLFMs considered in this\nreview.“Findings and results”(i) introduces datasets used for VLFMs based\non characteristics such as availability and size; (ii) investigates the technical\nfoundations and architectural designs of these models; (iii) presents a\nsummary of state-of-the-art evaluation metrics, including assessments by\nboth human and machines; (iv) analyzes the quantitative performance of\nthese models. “Discussion” highlights potential new research avenues,\nemphasizing the importance of incorporating additional datasets from\nvarious radiological modalities and the need for enhanced evaluation\nmetrics as pivotal areas for future research and development.“Conclusion”\nprovides a comprehensive summary of our perspectives on the current state\nof VLFMs.\nMethods\nLiterature search strategy\nA comprehensive literature search wasc o n d u c t e da c r o s ss e v e r a ld a t a b a s e s\nincluding PubMed, Google Scholar,Scopus and Arvix. The search was\nfocused on identifying studies that describe the application of AI models,\nparticularly VLFMs, for generating radiology reports and/or visual question\nanswering (VQA) from 3D medical imaging data. Keywords used in the\n1Department of Radiology, The Second Xiangya Hospital, Central South University, Changsha, Hunan, China.2Department of Radiology and Radiological Science,\nJohns Hopkins University School of Medicine, Baltimore, MD, USA.3Department of Diagnostic Imaging, Rhode Island Hospital, Providence, RI, USA.4Department\nof Radiology, Xiangya Hospital, Central South University, Changsha, Hunan, China.5Department of Biomedical Engineering, Johns Hopkins University, Baltimore,\nMD, USA.6Present address: Department of Radiology, University of Colorado, Aurora, CO, USA.7These authors contributed equally: Jing Wu, Yuli Wang.\ne-mail: Harrison.bai@cuanschutz.edu\nnpj Artiﬁcial Intelligence|            (2025) 1:17 1\n1234567890():,;\n1234567890():,;\nsearch included combinations of“3D medical imaging”, “AI in radiology”,\n“vision-language models”, “foundation model”, “visual question answer-\ning”,a n d“automated report generation”.\nInclusion and exclusion criteria\nWe included studies that speciﬁcally addressed the development or appli-\ncation of VLFMs capable of interpreting 3D imaging data and generating\ncorresponding radiological reports and/or performing VQA. Only studies\nthat provided empirical evidence of the models’ performance through\nrecognized evaluation metrics were included. Conversely, studies were\nexcluded if they did not focus on 3D imaging, were not original research\n( e . g . ,r e v i e w so rc a s er e p o r t ) ,o rd i dn o toffer quantitative evaluation results.\nA comprehensive literature search was performed using electronic data-\nbases including PubMed, Google scholar, Scopus and Arxiv. The keywords\nincluded combinations of“CT”and/or“MRI”and/or“3D imaging”; “large\nlanguage model” and/or “vision language model” and/or “foundation\nmodel”; “visual question answering” or “VQA”;a n d“report generation”.\nOnly studies published in English from January 2000 to May 2025 were\nincluded. From this search, 23 studies met all the inclusion criteria,\nunderscoring the nascent stage of this research area. Summary of studies on\nreport generation and/or VQA of 3D medical imaging using VLFMs is\ns h o w ni nT a b l e1.I nt h ef a s t - p a c e dﬁeld of AI-driven radiology, it’s essential\nto balance peer-reviewed and non-peer-reviewed sources. Although peer-\nreviewed research remains the gold standard, valuable insights often surface\nthrough less formal channels, such as preprints and Arxiv. By including\nthese sources in our review paper, we aim to highlight emerging trends and\naddress unmet needs, offering a thorough perspective on both established\nand emerging research areas.\nFindings and results\nDatasets assessment\nData extracted from the selected studies included details on the AI model\narchitecture, the evaluation metrics employed, and the reported model\nperformance as shown in Table2.\nIn the development and training of VLFMs for medical imaging, the\navailability and quality of datasetsplay a crucial role. While there are\nnumerous datasets available for 2D imaging, such as the widely-used\nMIMIC dataset\n10 for chest X-rays, the landscape for 3D medical imaging\ndatasets is less robust. Foundation models (FMs)11,12 often rely on image-\ncaption pairs derived from publications on PubMed and various public\ndatasets to enhance their learning capabilities. However, publicly available\n3D datasets are signiﬁcantly limited. To address this issue, a recent study\nintroduced CT-RATE13, a 3D medical imaging dataset that pairs images\nwith textual reports. CT-RATE consists of 25,692 non-contrast chest CT\nvolumes, expanded to 50,188 through various reconstructions, from 21,304\nunique patients, along with corresponding radiology text reports. This\ndataset represents a valuable resource for training and validating VLFMs\ntailored speciﬁcally for chest CT. One limitation is that the reports have been\ntranslated from Turkish to English, potentially introducing nuances that\naffect the learning of medical terminology and report structuring. Fur-\nthermore, the availability of INSPECT\n14, which includes 23,248 CT pul-\nmonary angiography images as detailed in the paper, with 21,266 of these\nimages publicly available, represents a signiﬁcant advancement in diversi-\nfying data sources for advanced diagnostic AI tools. These developments\nunderscore the need for more diverse and extensive 3D medical imaging\ndatasets to further enhance the capabilities of AI-driven diagnostic tech-\nnologies. Additionally, the M3D dataset\n15 comprises 120 K image-text pairs\nand 662 K instruction-response pairs covering 8 tasks, including Image-\nText Retrieval, Report Generation, Referring Expression Comprehension,\nReferring Expression Generation, Semantic Segmentation, and Referring\nExpression Segmentation. This multimodal dataset introduces richer dia-\nlogue corpus and three-dimensional segmentation tasks, highlighting\nimportant trends in current VLFM research.\nTechnical foundation for vision language models in 3D medical\nimaging\nIn the domain of medical imaging, particularly for the generation of radi-\nology reports from 3D images, VLFMs play a crucial role by leveraging\nmethodologies from computer vision and large language models (LLMs). As\ndepicted in Fig.2a, these models utilize a deep neural network architecture\nfeaturing distinct encoders for images and text. Here, 3D CNNs and\nTransformer-based networks extract volumetric features from 3D imaging\ndata, while text encoders process input data tokens. These neural network\narchitectures are essential for taskss u c ha sl e s i o nd e t e c t i o na n do r g a n\nsegmentation, preserving the inherentspatial relationships inherent in 3D\nstructures. For instance, RadGenome-Chest CT\n16 is designed to train a\nFig. 1 | Overview of the VLFMs’ framework. The training phase incorporates\ndiverse input modalities— medical images, textual reports, metadata, and historical\nrecords— followed by large-scale pre-training1. Benchmark is performed across a\nwide range of medical datasets, such as CT-Rate, Rad-ChestCT, UKBiobank and\nothers\n2. The framework employs advanced model structures including 3D Vision\nTransformer (ViT), 3D Perceiver, 3D ResNet etc. for the vision part and Gemini,\nLlama, Phi-3 etc. for the language model3. Evaluation includes both automated\nmetrics— such as BLEU and ROUGE— and clinical-speciﬁc tools like the RadBERT,\ncomplemented by expert human evaluation4. VLFMs are integrated into clinical\nworkﬂows, offering actionable insights that enhance diagnostic accuracy and sup-\nport clinical decision-making.\nhttps://doi.org/10.1038/s44387-025-00015-9 Review\nnpj Artiﬁcial Intelligence|            (2025) 1:17 2\nVLFM to generate reports based on both images and corresponding seg-\nmentations, enhancing interpretability with visual cues and improving\nmodel performance. Moving from 2D to 3D, VLFMs employ techniques\nsuch as multi-view learning\n17 and volume rendering18 to enhance the\ninterpretation of complex anatomical structures and direct visualization of\n3D volumes. During pre-training, as illustrated in Fig.2b), these models use\nframeworks that facilitate robust interaction between image features and\ntextual data. Contrastive learning19 aligns 3D image features with textual\nTable 1 | Summary of studies on report generation and/or VQA of 3D medical imaging using vision language foundation model\nPaper Model name Dataset name Dataset\navailability\nModality Patients E/R Images Language Task\nHamamci et al.13 CT-CLIP CT-RATE Public CT 21,304 25,692 50,188 English VQA\nRAD-ChestCT NA 3630 NA\nBai et al.15 M3D-LaMed M3D-Cap Public CT NA 42,496 120,092 English RG\nM3D-VQA NA 96,170 VQA\nWu et al.11 RadFM RP3D-Caption Public CT, MRI NA 51k 51k English RG\nRP3D-VQA 142k 142k VQA\nHamamci et al.55 CT2Rep NA NA CT 21,314 25,701 25,701 NA RG\nCT2RepLong 2807 7195 7195\nChen et al.\n56 Dia-LLaMA CTRG-Chest-548K Public CT NA 1804 1804 Chinese and English RG\nYang et al.31 Med-Gemini CT-US1 Private CT NA 681,460 681,460 NA RG\nBlankemeier\net al.57\nMerlin NA Public CT 18,321 25,528 25,528 English RG\nZhang et al.58 NA Prostate-MRI-USBiopsy\ndataset\nPublic MRI 615 NA NA NA VQA\nNA Private 206\nLiu et al.59 HILT CT-RATE Public CT NA 24,284 NA English RG\nBIMCV-RG Private 5328\nZhong et al.60 Abn-BLIP INSPECT Public CT 19,402 23,248 NA English RG\nBUH Private 19,565 59,754\nShui et al.61 fVLM MedVL-CT69K Private CT 69,086 272,124 272,124 English RG\nLee et al.62 MS-VLM CT-RATE Public CT 21,304 25,692 50,188 English RG, VQA\nIn-house Rectal MRI\ndataset\nPrivate MRI 311 311 NA\nNi et al.63 MG-3D CTRG-Chest Public CT NA 1804 1,804 English RG\nCT-RATE 50,188 50,188 VQA\nChen et al.64 Reg2RG RadGenome-ChestCT Public CT 21,304 25,692 25,692 English RG\nCTRG-Chest-548K NA 1804 1,804\nLee et al.65 PIRTA UKBiobank Public MRI 38,532 NA NA English RG\nSNUH + SNUBH Private 1045 1045\nBRMH Public 580 580\nISLES 206 206\nShi et al.\n66 Med-2E3 M3D-Cap Public CT NA NA 117 K English RG\nM3D-VQA 866 K VQA\nLai et al.67 E3D-GPT BIMCV-R Public CT NA 7518 NA English RG, VQA\nCT-RATE 50,188\nPrivate Data Private 300,360 NA\nLuo et al.\n68 VividMed CT-RATE Public CT NA 25,692 25,692 English RG\nLi et al.69 ViT3D Alignment of\nLLaMA3\nAMOS-MM 2024 Dataset Public CT NA 2088 2,088 English RG, VQA\nCT-RATE 21,340 50,188 50,188 RG\nZheng et al.70 PCRL CTRG-Brain Public CT NA 6000 160,336 Chinese RG\nChen et al.71 3D-CT-GPT CT-RATE Public CT NA 8070 8070 English RG, VQA\nDataset-XY Private 1887 1887\nAlkhaldi et al.72 MiniGPT-Med NLST Public CT NA 7625 7625 English RG, VQA\nMIMIC CT, MRI 227,835 377,110\nLi et al.73 BrainGPT 3D-BrainCT Private CT 9689 18,885 NA English RG\nCQ500 Public NA 133\nVQA visual question answering,E/R number of examinations and reports,RG report generation.\nhttps://doi.org/10.1038/s44387-025-00015-9 Review\nnpj Artiﬁcial Intelligence|            (2025) 1:17 3\nTable 2 | Detailed summary of the key aspects of published VLFMs, including architecture, evaluation metrics, and performance\nPaper Architecture (image) Architecture (text) NLG metrics CE metrics Performance summary\nHamamci et al.13 Encoder of CT-ViT, CXR-\nBert text encoder\nCLIP-based NA (directly match) AUROC, ACC, precision, and\nF1 scores\nMulti-abnormality classiﬁcation: AUROC: 0.643, F1 score:\n0.663, ACC: 0.621, Precision: 0.353.\nBai et al.15 3D ViT, 3D perceiver LLaMA, SegVol BLEU, ROUGE, METEOR, BERT-Score,\nLLM Qwen-72B*\nRecall For RG, BLEU: 15.15, ROUGE: 0.196, METEOR: 0.144, BERT-\nScore: 0.885, LLM Qwen-72B*: 0.849. For VQA, recall: 0.990.\nWu et al.11 Visual encoder, perceiver\nmodule\nLLM BLEU, ROUGE, BERT-sim,\nUMLS_Precision, UMLS_Recall\nACC, F1 score For RG, BLEU: 0.128, ROUGE: 0.182, UMLS Precision: 0.225,\nUMLS Recall: 0.121, BERT-Sim: 0.586. For VQA, BLEU: 0.306,\nROUGE: 0.364, UMLS Precision: 0.318, UMLS Recall: 0.249,\nBERT-Sim: 0.678.\nHamamci et al.\n55 3D vision feature extractor,\ntransformer encoder\nTransformer decoder,\nmultimodal transformer\ndecoder\nBLEU, METEOR, ROUGE-L Precision, recall, F1 score For RG, BLEU: 0.460, METEOR: 0.295, ROUGE-L: 0.459,\nPrecision: 0.749, Recall: 0.548, F1 score: 0.534.\nChen et al.\n56 Pre-trained ViT3D, perceiver LLaMA BLEU, METEOR, ROUGE-L Precision, recall, F1 score For RG, BLEU: 0.512, METEOR: 0.263, ROUGE-L: 0.422,\nPrecision: 0.421, Recall: 0.387, F1 score: 0.372.\nYang et al.31 Gemini 1.5 Pro vision\nencoder, Gemini video\nencoder\nTransformer decoder CIDEr, BLEU4, Rouge-L ACC, RadGraph F1-score The ﬁrst large multimodal model-based RG for 3D CT volumes,\nwith 53% of AI reports considered clinically acceptable.\nBlankemeier\net al.57\nImage encoder (inﬂated 3D\nResNet152)\nCLIP-based decoder,\nRadLlama\nRadGraph-F1, BERT Score, ROUGE-2,\nBLEU score\nRecall For RG, BLEU: 0.102, ROUGE-2: 0.262, BERT: 0.588,\nRadGraph-F1: 0.293.\nZhang et al.58 Image encoder, domain\nadapter layer\nLLaMA NA ACC, MSE, MAE, precision,\nRecall, F1 metrics\nThe approach boosts the performance of current scoring\ntechniques, with improvement of 0.048-0.070 in ACC.\nLiu et al.59 ViT-Base Llama3-8B-Instruct with\nLoRA adaptation\nBLEU 1-4, METEOR, ROUGE 1-2,\nL, CIDEr\nNA HILT surpasses baselines in 3DHRG, scoring 0.461 (CT-RATE)\nand 0.410 (BIMCV-RG) across resolutions and zero-shot, using\nonly 64 tokens efﬁciently.\nZhong et al.60 I3D ResNet152 Abn-QFormer BLEU-1, BLEU-4, ROUGE-1, ROUGE-L,\nMETEOR, BERT-F1\nACC, AUC, Sensitivity,\nSpeciﬁcity, F1-score\nAbn-BLIP excels in RG: BLEU-1 0.677, BLEU-4 0.112, ROUGE-L\n0.873 comparing to state-of-the-art (SOTA).\nShui et al.61 ViT-base BERT-base ACC, GREEN, BLEU-1, BLEU-2, BLEU-\n3, BLEU-4, METEOR, ROUGE-L, CIDEr\nAUC, ACC, Speciﬁcity,\nSensitivity, F1-score,\nPrecision\nfVLM excels in RG: ACC 0.645, GREEN 0.402, BLEU-1 0.522,\nROUGE-L 0.464.\nLee et al.62 2D ViT-B/16 (DINO pre-\ntrained) + Z-former\nVicuna-7B-v1.5 (ﬁne-tuned\nwith LoRA)\nBLEU-4, ROUGE-L, METEOR Precision, Recall, F1-score CT-RATE RG: BLEU-4: 0.232, ROUGE-L: 0.438, METEOR:\n0.396, F1: 0.261; CT-RATE VQA: F1: 0.297; Rectal MRI RG:\nQualitatively superior, no quantitative metrics provided.\nNi et al.63 3D Swin Transformer RadBERT BLEU-1, BLEU-2, BLEU-3, BLEU-4 AUC, Recall CTRG-Chest RG: BLEU-1: 0.626, BLEU-2: 0.522, BLEU-3:\n0.426, BLEU-4: 0.372; CT-RATE VQA: AUC: 0.770.\nChen et al.64 ViT3D, 3-layer ViT3D LLaMA2-7B BLEU-1, BLEU-2, BLEU-3, BLEU-4,\nMETEOR, ROUGE-L\nPrecision, Recall, F1 RadGenome-ChestCT RG: BLEU-1: 0.473, BLEU-2: 0.365,\nBLEU-3: 0.296, BLEU-4: 0.249, METEOR: 44.070, F1: 0.253;\nCTRG-Chest-548K Report Generation: BLEU-1: 49.630, BLEU-\n2: 41.430, BLEU-3: 35.910, BLEU-4: 32.040, METEOR: 49.710.\nLee et al.65 3D ViT NA NA ACC@1 (Accuracy at k = 1) PIRTA ’s ACC@1 is signiﬁcantly higher than baselines (GPT-4o,\nLLaVA-Med), but exact numeric values were not provided in\nthe text.\nShi et al.66 M3D-CLIP Phi-3 (LLM) BLEU@1, ROUGE@1, METEOR, BERT ACC RG: BLEU@1: 0.515, ROUGE@1: 0.545, BERT: 0.909; VQA:\nBLEU@1: 0.586, ACC: 0.824\nLai et al.67 3D ViT-Base+ 3D\nConvolution Perceiver\nVicuna 7B (LLM) with LoRA BLEU-1, ROUGE-1, METEOR, BERT-F1 Balanced Accuracy,\nPrecision, Recall, F1-score\nRG (CT-RATE): BLEU-1: 0.412, BERT-F1: 0.880; VQA (CT-\nRATE): BLEU-1: 0.520, BERT-F1: 0.938\nLuo et al.68 ViT encoder Vicuna-1.5-7B BLEU-4, ROUGE-L, METEOR Macro RadBERT F1,\nMicro RadBERT F1, Macro\nRadBERT FNR, Micro\nRadBERT FNR\nRG:BLEU-4: 0.373, ROUGE-L: 0.373, METEOR: 0.312, Macro\nRadBERT F1: 0.312, Micro RadBERT F1: 0.395, Macro\nRadBERT FNR: 0.156, Micro RadBERT FNR: 0.149\nhttps://doi.org/10.1038/s44387-025-00015-9 Review\nnpj Artiﬁcial Intelligence|            (2025) 1:17 4\ndescriptions by mapping two elements (e.g., an image patch and the cor-\nresponding section of the report) to similar points in a learned space, relative\nto dissimilar elements (negatives). Inﬁne-tuning, depicted in Fig.2 (c),\ntechniques such as Region of Interest (RoI) Pooling adapted for 3D20 focus\non speciﬁc volumetric areas, while cross-modalﬁne-tuning21 ensures that\ntextual output closely aligns with 3D image features. The evaluation of these\nmodels in clinical settings often involves zero-shot and few-shot predictions\nto assess their ability to answer speciﬁc prompt questions (i.e., VQA) and/or\ngenerate radiology reports.\nEvaluation of AI-generated radiology reports\nThe evaluation of VLFMs using both natural language generation (NLG)\nmetrics and clinical efﬁcacy metrics is critical for assessing the accuracy,\ncoherence, and clinical relevance of AI-generated radiology reports (Fig.3).\nNLG metrics like BiLingual Evaluation Understudy (BLEU)\n22,r e c a l l -\noriented understudy for gisting evaluation (ROUGE)23, metric for evalua-\ntion of translation with explicit ordering (METEOR)24, and consensus-\nbased image description evaluation (CIDEr)25 offer insights into linguistic\nalignment with traditionally authoredreports. They assess various aspects\nfrom n-gram co-occurrence to semantic similarity, using advanced tech-\nniques such as synonym matching and contextual embedding analysis from\nBERT\n26. However, while robust for linguistic assessment, these metrics often\ndo not fully capture the clinical utility of reports. They may overlook the\ndepth of semantic and contextual understanding required for accurately\ninterpreting complex 3D medical images.\nTo overcome these challenges, clinical efﬁcacy metrics such as CheX-\npert/CheXbert\n27 Labeling Scores and RadGraph F128 have been introduced.\nThese tools assess the clinical accuracyof AI-generated reports by analyzing\ntext to identify and evaluate speciﬁcc l i n i c a lﬁndings essential in 3D imaging,\nsuch as detailed anatomical structures and their spatial relationships. The\nRadiology Report Clinical Quality (RadCliQ\n29) metric integrates these\nmethodologies to provide a holistic assessment of a report’s quality,\nencompassing both linguistic precision and clinical accuracy.\nDespite these advancements, the evaluation process faces challenges\nincluding inherent subjectivity and variability in human evaluations, the\nresource-intensive nature of expert reviews, and the complexities involved\nin error analysis of AI-generated reports. These evaluations are pivotal, as\nthey entail radiologists assessing AI-generated reports against clinical\nbenchmarks for readability, informational completeness, and accuracy, with\na special focus on critical 3D contextual details. However, the current lim-\nitations of evaluation metrics highlight the need for ongoing development to\nbetter align AI model performance with the nuanced requirements of\nclinical practice in radiology, especially as it pertains to complex 3D medical\nimaging.\nPerformance of VLFM on 3D medical imaging\nSeveral VLFMs have demonstrated notable performances, each employing\nunique approaches and datasets. The M3D-LaMed\n15 model, distinguished\nby its integration of multi-modal LLMs for complex 3D image analysis, has\nshown superior performance in tasks such as image-text retrieval, report\ngeneration, and visual question answering. Trained on a variety of 3D\nmedical images, M3D-LaMed leverages a pre-trained 3D vision encoder and\na spatial pooling perceiver to effectively handle the intricate data structures\ninherent in 3D imaging. Rad-FM\n11, utilizing a comprehensive radiology\nimage and report database, improved clinical-speciﬁc metrics, achieving\n22.49% for UMLS_Precision30 and 12.07% for UMLS Recall. CT-CLIP13\nexcelled in zero-shot multi-abnormality detection on CT scans, notably\nenhancing mean AUROC and F1 scores over state-of-the-art, fully super-\nvised methods by 0.144 and 0.097 in internal validation and 0.099 and 0.074\nin external validation, utilizing CT scans paired with electronic health\nrecords. Google’s Med-Gemini\n31, trained across multiple medical mod-\nalities, including radiology and genomics, reported that 53% of its AI-\ngenerated reports were clinically acceptable, though it acknowledged the\nneed for advancements to reach expert-level quality. These models, with its\nadvanced capabilities in handling 3D datasets, underscore signiﬁcant\nTable 2 (continued) | Detailed summary of the key aspects of published VLFMs, including architecture, evaluation metrics, and performance\nPaper Architecture (image) Architecture (text) NLG metrics CE metrics Performance summary\nLi et al.69 ViT3D image encoder from\nM3D-CLIP\nAsclepius-Llama3-8B Green Score ACC RG(AMOS-MM 2024): Green Score (Avg.): 0.300, Green Score\n(Chest): 0.200, Green Score (Abdomen): 0.320, Green Score\n(Pelvis): 0.370; VQA: ACC: 0.610.\nZheng et al.70 ResNet101 LLaMA3-8B BLEU-1, BLEU-2, BLEU-3, BLEU-4,\nMETEOR, ROUGE-L, CIDEr\nF1 Score RG: BLEU-1: 0.620, BLEU-2: 0.547, BLEU-3: 0.494, BLEU-4:\n0.453, METEOR: 0.331, ROUGE-L: 0.577, CIDEr: 0.964, F1\nScore: 0.706.\nChen et al.71 CT-ViT (from CT-CLIP) Vicuna-7B BLEU, ROUGE-1, ROUGE-2, ROUGE-L,\nMETEOR, BERTScore_F1\nNA RG: BLEU: 0.384, ROUGE-1: 0.475, ROUGE-L: 0.328, METEOR:\n0.357, BERTScore_F1: 0.889.\nAlkhaldi et al.72 EVA LLaMA-2-chat BERT-Sim, CheXbert-Sim BERT-Sim(VQA), IoU RG: BERT-Sim: 0.720, CheXbert-Sim: 0.301; VQA: BERT-\nSim: 0.580.\nLi et. Al.73 CLIP ViT-L/14 LLaMA-7B BLEU-1, BLEU-4, METEOR, ROUGE-L,\nCIDEr-R\nFORTE (F1-score for degree,\nlandmark, feature,\nimpression)\nRG(internal testing): BLEU-1: 0.444, BLEU-4: 0.204, METEOR:\n0.301, ROUGE-L: 0.476, CIDEr-R: 0.212; RG(external\nvalidation): ACC (midline shift): 0.910.\n*Qwen-72B serves as an LLM-based evaluation tool within M3D-Bench, facilitating automated and precise assessment of model performance.NLG natural language generation,CE clinical efﬁcacy, LLM large language model,ACC accuracy, AUROC area under the ROC\ncurve, BLEU BiLingual Evaluation Understudy,ROUGE recall-oriented understudy for gisting evaluation,BERT bidirectional encoder representations from transformers,METEOR metric for evaluation of translation with explicit ordering,RadGraph-F1 measures accuracy in\nextracting clinical entities from radiology reports, combining precision and recall into one score,UMLS uniﬁed medical language system,UMLS Precisionevaluates the proportion of correctly identiﬁed medical entities matching standardized UMLS terms,UMLS Recall\nassesses completeness in capturing relevant medical entities from text, according to UMLS terms,CIDEr consensus-based image description evaluation,MSE mean squared error,MAE mean absolute error,Green Scoreevaluates the clinical accuracy of generated\nradiology reports by measuring overlap with expert-written reports.\nhttps://doi.org/10.1038/s44387-025-00015-9 Review\nnpj Artiﬁcial Intelligence|            (2025) 1:17 5\nadvancements in applying AI to enhance the accuracy and utility of medical\nimaging report generation. They have the potential to improve the analytical\ncapabilities of medical practitioners with detailed and reliable interpreta-\ntions of complex 3D medical data.\nDiscussion\nChallenges and future directions\nEnhancement of high-quality and real-world datasets of image-\nreports pairs. A key challenge in advancing AI applications for 3D\nmedical imaging, particularly for radiology report generation, is the lack\nof large, annotated datasets that encompass a wide range of pathologies\nacross diverse patient populations. Comprehensive datasets are essential\nfor training VLFMs that can accurately generate radiology reports from\n3D images. Efforts to expand these datasets should focus on including a\ngreater variety of 3D imaging types and ensuring detailed annotations\nthat correlate imagingﬁndings with clinical reports.\nAs i g n iﬁcant limitation of current methodologies in radiology report\ngeneration from 3D medical images is their heavy reliance on metadata\nextracted from DICOMﬁles. These metadataﬁelds, typically only providing\nbasic information about image modality and the body parts imaged, are\ninherently restrictive. This approach results in low-quality ground truth and\noften lacks the depth and context necessary for creating nuanced and\nclinically relevant reports. They fail to capture complex diagnostic infor-\nmation, critical nuances in pathology,and other subtleties essential for a\ncomprehensive radiology report. Consequently, models trained on such\ndatasets may develop a superﬁcial understanding of the images, leading to\ngeneric and potentially inaccurate report generation.\nThis underscores the need for developing datasets that go beyond mere\nmetadata to include rich, contextual annotations that directly relate speciﬁc\nimaging ﬁndings to detailed clinical insights. Collaborative initiatives with\nhospitals and research institutions to anonymize and share 3D imaging data\ncould be vital in achieving this goal. The true potential of such collaborations\ncan be realized through the establishment of large-scale, multi-modality\nimaging and report data consortiums. By pooling resources and datasets\nfrom diverse geographic and demographic sources, these consortia can\ncreate a more comprehensive and varied dataset that reﬂects a broader\nspectrum of pathologies, treatment outcomes, and patient populations.\nThis approach would not only enhance the volume and variety of data\nbut also improve the robustness andgeneralizability of the AI models\ntrained on them. Additionally, multi-site collaboration facilitates the stan-\ndardization of data collection, annotation, and processing protocols, further\nenriching the quality of the data. Such an enriched dataset can serve as a\ncornerstone for developing more precise and contextually aware AI tools,\nultimately leading to improved accuracy in medical imaging report gen-\neration and better patient care outcomes.\nDomain-speciﬁc insights in medical imaging beyond general\ncomputer vision. To signiﬁcantly improve the performance of VLFMs in\ngenerating accurate radiology reports from 3D medical images, it is\ncrucial to focus on both the development and reﬁnement of model\narchitectures tailored to the inherent complexities of 3D medical scans\n9.\nThe intricate spatial relationships and detailed anatomical structures\npresent in these images necessitate the use of enhanced 3D convolutional\nlayers, speciﬁcally designed to better capture the spatial hierarchies\nessential for accurately interpreting medical images.\nMoreover, the integration of advanced language processing modules is\nindispensable. These modules must not only understand the clinical lan-\nguage but also articulate medicalﬁndings with high precision, effectively\nincorporating medical terminologies and nuanced patient data. Such\nFig. 2 | Overview of the VLFM Framework Across Pre-training and Fine-tuning\nPhases. Schematic diagram illustratinga the overall framework of VLFM, depicting\nits components during different phases:b pre-training strategy of VLFM andc ﬁne-\ntuning strategy of VLFM. The diagram indicates that certain parts of the model may\nbe frozen (not updated) during theﬁne-tuning phase. Typically, this would be the\nImage Encoder and Text Encoder to preserve the learned visual/text features. The\nMultimodal Perceiver isﬁne-tuned during both the pre-training andﬁne-tuning\nphases.\nFig. 3 | Comparison of Machine and Radiologist Evaluations of Radiology\nReports. Comparisons betweena machine metrics evaluation andb radiologist\nevaluation. Ina, scores are calculated using speciﬁc metrics for a given radiology\nreport. Inb, radiologists evaluate the report by scoring clinical-related errors across\nvarious error categories.\nhttps://doi.org/10.1038/s44387-025-00015-9 Review\nnpj Artiﬁcial Intelligence|            (2025) 1:17 6\ncapabilities require a deep fusion of visual and textual understanding within\nthe model architecture, ensuring that the generated reports are both\nmedically accurate and contextually relevant.\nFurther augmenting the efﬁcacy of these models, advanced training\ntechniques like multi-task learning play a pivotal role. By enabling the model\nto simultaneously learn to identify speciﬁc medical conditions from 3D\nimages and generate descriptive, clinically relevant text, multi-task learning\nenhances the model’s ability to handle multiple tasks that mirror the\nworkﬂow of human radiologists. This approach ensures a more holistic\nlearning process, fostering models that are not only technically proﬁcient\nbut also practically applicable in clinical settings.\nIn addition to these architectural and training enhancements, the\napplication of anatomical guidance tools such as TotalSegmentor can\nrevolutionize model training\n32. By allowing precise segmentation of speciﬁc\norgans or regions within the 3D scans, these tools help create anatomical\nguidance in the image-text pair alignments. This guidance signiﬁcantly aids\nthe model in distinguishing between different anatomical features and their\ncorresponding clinical descriptions, thereby reﬁning the accuracy and\nrelevance of the generated reports. Collectively, these strategies form a\nrobust approach to overcoming current limitations and setting new\nbenchmarks in the AI-driven generation of radiology reports from complex\n3D medical imaging data.\nAdvanced metrics for assessing VLFMs in medical imaging report\naccuracy and clinical utility. Current metrics for evaluating VLFMs in\nmedical imaging often fall short. These metrics, adapted from traditional\nNLP models, primarily measure textual similarity rather than clinical\naccuracy. This limitation can result in high scores for reports that appear\ntextually accurate but miss critical diagnosticﬁndings, impressions, and\nrecommendations.\nAlthough there are some efforts to include radiologists’ subjective\nevaluations\n33, these studies are limited to 2D chest X-rays and do not address\nthe complexities of 3D imaging, which is more relevant to actual diagnostics\nand treatment. Evaluating VLFMs in 3D imaging requires more sophisti-\ncated metrics.\nStudies\n29,34–36 have highlighted theﬂaws in current metrics. BLEU\nstruggles with identifying falseﬁndings, while BERTScore has higher errors\nin locatingﬁndings compared to CheXbert. These issues underscore the\nneed for improved evaluation methods. The proposed RadCliQ29 metric\ncombines existing metrics with linear regression to create a more balanced\nevaluation model. However, RadCliQ’s reliance on text overlap-based\nmetrics and its testing on 2D datasets reveal limitations when applied to 3D\nimaging.\nFuture research should focus on developing metrics that accurately\nassess the clinical relevance of the generated reports, including diagnostic\naccuracy and terminology appropriateness. Advanced NLP techniques\ncould also compare generated reports with a database of clinician-validated\nreports. By improving these metrics, researchers can establish more effective\nbenchmarks for VLFMs in 3D medical imaging, ensuring the generated\nreports are not only accurate but also valuable in clinical practice, thereby\nenhancing patient care and diagnostic processes.\nControversy on simulated/synthesized 3D data for model training.\nThe use of simulated or augmented 3D data in training VLFMs for\nmedical applications presents signi ﬁcant challenges alongside its\nbeneﬁts. While it ﬁlls gaps in data availability, especially for rare or\ncomplex diagnostic scenarios, there is substantial debate about its\nreliability. Critics\n37,33 argue that because simulated data do not origi-\nnate from actual clinical experiences, they may not accurately reﬂect\nthe complexities and variability necessary for training truly effective\nmedical AI models.\nDespite these concerns, advancements in generative AI offer promising\nsolutions to enhance the reliability and quality control of synthesized\ndata\n38–40. High-quality, AI-generated 3D datasets can now mimic real-world\ndata with greaterﬁdelity, reducing the gap between simulated and actual\nclinical scenarios. This evolution in data generation technology allows for an\naugmentation of existing 3D datasets, enhancing the breadth and depth of\ntraining environments for VLFMs without compromising the integrity of\nthe models. By utilizing sophisticatedgenerative techniques, including 3D-\nGAN\n41, 3D diffusion probabilistic model42, Neural Radiance Fields43 and so\non, developers can create augmented 3D datasets that are not only diverse\nbut also closely aligned with real clinical conditions. These 3D datasets\nprovide a controlled environment for testing and validating VLFMs,\nensuring that the models are exposed to a wide range of medical scenarios,\nincluding rare and complex conditions. This approach not only enhances\nthe model’s diagnostic capabilities butalso ensures that the VLFMs are\nrobust and reliable, thereby potentially improving healthcare outcomes\nthrough more accurate and comprehensive medical imaging reports.\nAddressing computational and clinical challenges in 3D\ntransformer-based models. The integration of 3D transformer-based\nvision-language models in medical imaging represents a signiﬁcant\nadvancement, holding considerable promise for improved diagnostic\naccuracy and clinical reporting. However, the translation of these models\nfrom research prototypes to clinically deployable solutions face compu-\ntational and clinical challenges that must be thoroughly addressed.\nA primary obstacle to the widespread clinical adoption of traditional\n3D transformers is their computational complexity. Speciﬁcally, these\nmodels suffer from quadratic memory and computational requirements\nassociated with self-attention mechanisms, especially when processing\nhigh-resolution volumetric medical data\n44,45. Due to these constraints,\npractical implementations often necessitate signiﬁcant compromises in\ninput data resolution, thereby reducing the ability to captureﬁne-grained\nanatomical details essential for clinical diagnostics, such as subtle\nabnormalities, delicate vascular structures, nodules, and small lesions\n46–48.\nConsequently, these limitations directlyimpact the clinical applicability and\ndiagnostic utility of transformer-based vision-language systems.\nTo overcome these computational challenges, recent developments in\ntransformer architectures have introduced promising solutions. Hier-\narchical 3D swin transformers, for example, employ multi-scale hierarchical\nstructures and locality-sensitive computations to substantially reduce\nmemory usage without sacriﬁcing model accuracy\n49. Additionally, sparse\nattention strategies, such as axial attention50 a n dw i n d o w e da t t e n t i o n\nmechanisms51, have emerged as effective methods to selectively prioritize\ncomputations, signiﬁcantly decreasing resourcerequirements while main-\ntaining robust performance. Furthermore, hybrid CNN-transformer\nmodels exploit the complementary strengths of convolutional neural net-\nworks and transformers, combining efﬁcient local feature extraction from\nCNNs with the global contextual understanding facilitated by transformers,\nthereby achieving a practical balance between computational efﬁciency and\nclinical efﬁcacy\n52,53. Recent advances, such as the hierarchical attention\napproach proposed by Zhou et al.54, exemplify these improvements by\nconsiderably reducing memory usage and computational demands. This\nstrategy enables the analysis of higher-resolution volumetric data, resulting\nin improved performance in clinically relevant segmentation tasks,\nincluding pulmonary vessels and airways segmentation\n54. Such innovative\nsolutions demonstrate signiﬁcant progress toward resolving existing bar-\nriers, yet further advancements remain essential.\nTo address ongoing computational and clinical challenges compre-\nhensively, future research should prioritize several key directions. Enhanced\nhierarchical attention methods capable of dynamically allocating compu-\ntational resources based on clinical signiﬁcance represent a promising area\nof exploration. Additionally, the development of transformer architectures\nspeciﬁcally tailored for medical imaging, incorporating domain knowledge\nto optimize performance and efﬁciency, remains a critical research priority.\nFinally, investigating memory-efﬁcient training paradigms, including\nmodel distillation, quantization, and efﬁcient training strategies, will be\ncrucial to improving practical feasibility. Explicitly recognizing and sys-\ntematically addressing these computational and clinical limitations in this\nreview aims to provide valuable insights and actionable guidance for\nhttps://doi.org/10.1038/s44387-025-00015-9 Review\nnpj Artiﬁcial Intelligence|            (2025) 1:17 7\nresearchers committed todeveloping practical, efﬁcient, and clinically\nimpactful 3D transformer-based vision-language models in radiology.\nLessons learned from 2D VLFMs and a roadmap for future research\non 3D VLFMs. The development and deployment of VLFMs in clinical\nenvironments represent promising yet challenging objectives. Although\n2D VLFMs have exhibited encouraging results in controlled research\nenvironments, their real-world applicability and acceptance in clinical\nworkﬂows have thus far remained limited. Understanding these limita-\ntions offers valuable insights that can guide the development and eventual\nclinical translation of more complex 3D VLFMs.\nSeveral critical insights have been identiﬁed from examining the lim-\nitations inherent to existing 2D VLFMs. First,interpretability and clinical\ntrust present signiﬁcant challenges. Despite high quantitative performance\ndemonstrated in research studies, clinicians frequently express reservations\nregarding the interpretabilityand transparency of these models’decisions,\nunderscoring the necessity of incor porating explainability methods,\nuncertainty quantiﬁcation, and clear visual justiﬁcations into model pre-\ndictions. Second, the issue ofdomain generalization and robustnessis a\ncrucial barrier. Models trained on datasets from speciﬁc institutions or\nlimited imaging protocols often struggle to generalize effectively to diverse\nclinical environments. This limitation highlights the importance of robust\ntraining methodologies, domain adaptation techniques, and comprehensive\nvalidation on heterogeneous datasets. Third,computational efﬁciency and\npractical feasibilityremain pivotal for real-world deployment. Despite being\nsimpler than 3D models, current 2D VLFMs still encounter challenges\nrelated to computational demands, limiting their integration into clinical\nworkﬂows, especially in resource-constrained settings. Lastly,regulatory\napproval and ethical considerationssigniﬁcantly impact clinical adoption.\nFactors such as patient privacy, data security, and algorithmic biases must be\ncomprehensively addressed to facilitate the real-world integration of these\nmodels into healthcare practice. These lessons and their corresponding\nimplications for clinical adoption are summarized in Table3.\nBased on these insights from 2D VLFMs, we propose the following\npractical roadmap for future research and clinical integration of 3D VLFMs,\nwhich is shown in Fig.4. Explicitly integrating these considerations into the\nresearch agenda will signiﬁcantly enhance the likelihood of translating 3D\nVLFMs into clinically valuable tools,ultimately improving patient care\nthrough more precise and insightful medical image interpretation.\nIn the short-term (1–2 years), efforts should prioritize establishing\ntechnical feasibility and computational efﬁciency. This involves reducing\ncomputational complexity by developing memory-efﬁcient transformer\narchitectures, such as hierarchical (e.g., Swin), axial, or sparse attention\nmechanisms, speciﬁcally tailored for medical 3D imaging. Additionally,\nexploring hybrid modeling strategies that integrate transformer models with\nconvolutional neural networks will help balance interpretability, accuracy,\nand computational demands. Initial validations can be conducted using\nstandard public medical imaging datasets, focusing on benchmarking tasks\nsuch as ﬁne-grained abnormality detection and segmentation (e.g., lung\nnodules, brain tumors). The mid-term goals (2–4 years) should focus on\nenhancing clinical relevance and interpretability. This phase includes inte-\ngrating transformer models with advanced explainability techniques,\nincluding attention visualization and feature attribution methods. Fur-\nthermore, close collaboration with clinical experts is essential to clearly\ndeﬁne realistic clinical scenarios such as preliminary report generation,\nscreening assistance, and triage prioritization. Robustness and general-\nizability of models must be thoroughly validated through multi-institutional\nstudies that encompass diverse populations and imaging modalities,\nalongside establishing standardized reporting criteria for consistent eva-\nluation. In the long-term (4–7 years), extensive prospective clinical valida-\ntion studies are essential, involving large-scale, multicenter clinical trials to\nrigorously demonstrate the clinical effectiveness, safety, and value of these\nmodels. Concurrently, proactive engagement with regulatory bodies (e.g.,\nFDA, CE marking) is crucial to ensure compliance with ethical standards,\ntransparency, fairness, bias mitigation, and reproducibility. Finally, suc-\ncessful integration into clinical workﬂows requires the development of\ninteroperability standards, facilitating seamless integration with existing\nhealthcare systems such as PACS, RIS, and EMR. Post-deployment, con-\ntinuous monitoring and evaluation must be maintained to ensure sustained\nclinical beneﬁt and system reliability.\nOverall, the primary aimed scenarios for VLFMs focus on realistic,\npractical applications rather than full automation of medical diagnosis.\nSpeciﬁcally, VLFMs are well-suited for supportive roles such as preliminary\nanomaly detection and triage assistance to streamline clinical workﬂows, as\nTable 3 | Summary of limitations of 2D vision-language foundation models (VLFMs), example studies, and clinical impact\nLimitation category Example studies Impact on clinical adaptation\nInterpretability and Clinical Trust Rajpurkar et al. 74; Rudin et al.75 Reduced trust; Poor interpretability; Reluctance to integrate models into clinical practice.\nDomain Generalization and Robustness Alijani et al.76;\nManzari et al.77\nLimited reliability across diverse clinical environments.\nComputational Efﬁciency and Feasibility Ghosh et al. 78; Awais et al.79 Limited practical feasibility; integration barriers in clinical workﬂows.\nRegulatory and Ethical Considerations Mennella et al. 80 Increased scrutiny; slower adoption due to regulatory and ethical barriers.\nFig. 4 | Future Roadmap for Clinical Integration of 3D VLFMs.The practical roadmap for future research and clinical integration of 3D VLFMs, including the short-term,\nmedium-term and long-tern goals.\nhttps://doi.org/10.1038/s44387-025-00015-9 Review\nnpj Artiﬁcial Intelligence|            (2025) 1:17 8\nwell as assisting radiologists in generating structured clinical reports by\nautomating routine descriptive tasks.Additionally, patient-focused appli-\ncations involve providing simpliﬁed, patient-friendly explanations of ima-\nging results, enhancing patient understanding and health literacy.\nEmphasizing interpretability, robust validation, clinician collaboration, and\nintegration into existing clinical workﬂows will ensure these models have\nmeaningful clinical impact and broad acceptance.\nConclusion\nImproving VLFMs for radiology report generation from 3D medical ima-\nging will have profound implications for theﬁeld of radiology, enhancing\nthe speed and accuracy of diagnostic processes. These advancements will\nnot only optimize the workﬂow within medical imaging departments but\nalso potentially improve patient outcomes by providing faster and more\naccurate diagnostic information. As these AI systems become more inte-\ngrated into clinical practice, they will play a crucial role in supporting\nradiologists by providing reliable preliminary reports that can be quickly\nreviewed and adjusted, if necessary, thereby streamlining the diagnostic\nworkﬂow and allowing radiologists to focus on more complex cases.\nMoreover, the advancements in developing a vision-language foundation\nmodel speciﬁcally for 3D medical imaging not only revolutionize radi-\nological practices but also hold profound implications for the broaderﬁeld\nof medicine, such as pathology and radiation oncology. By integrating and\ninterpreting complex imaging data more effectively, this technology can\ne n h a n c ed i a g n o s t i ca c c u r a c ya c r o s sv a rious specialties, facilitate persona-\nlized treatment plans, and ultimately improve patient outcomes.\nData availability\nNo datasets were generated or analysed during the current study.\nAbbreviations\nAI arti ﬁcial intelligence\nVLFMs vision-language foundation models\nVQA visual question answering\nFMs Foundation models\nRoI Region of Interest\nNLP Natural Language Processing\nCNN Convolutional Neural Network\nNLG Natural Language Generation\nCE Clinical Ef ﬁcacy\nLLM Large Language Model\nACC Accuracy\nAUROC Area under the ROC curve\nBLEU BiLingual Evaluation Understudy\nROUGE Recall-Oriented Understudy for Gisting Evaluation\nBERT Bidirectional Encoder Representations from Transformers\nMETEOR Metric for Evaluation of Translation with Explicit\nOrdering\nUMLS Uni ﬁed Medical Language System\nCIDEr Consensus-based Image Description Evaluation\nMSE Mean Squared Error\nMAE Mean Absolute Error\nRadCliQ Radiology Report Clinical Quality\nViT Vision Transformer\nReceived: 16 July 2024; Accepted: 18 June 2025;\nReferences\n1. Saba, L. et al. The present and future of deep learning in radiology.Eur.\nJ. Radiol.114,1 4– 24 (2019).\n2. Ilyas, F., Burbridge, B. & Babyn, P. Health care-associated infections\nand the Radiology Department.J. Med. Imaging Radiat. Sci.50,\n596– 606.e1 (2019).\n3. Abramson, Z. et al. Current and emerging 3D visualization\ntechnologies in radiology.Pediatr. Radiol.54, 684– 692 (2024).\n4. Singh, S. P. et al. 3D deep learning on medical images: a review.\nSensors 20, 5097 (2020).\n5. Liu, X. et al. Advances in deep learning-based medical image analysis.\nHealth Data Sci.2021, 8786793 (2021).\n6. Gegenfurtner, A. et al. The challenges of studying visual expertise in\nmedical image diagnosis.Med. Educ.51,9 7– 104 (2017).\n7. Yildirim, N. et al. Multimodal healthcare AI: identifying and designing\nclinically relevant vision-language applications for radiology. in:\nProceedings of the CHI Conference on Human Factors in Computing\nSystems,p p1– 22 (2024).\n8. Hartsock, I. & Rasool, G. Vision-language models for medical report\ngeneration and visual question answering: a review.Front Artif Intell.\n7, 1430984 (2024).\n9. Li, C. et al. Llava-med: training a large language-and-vision assistant\nfor biomedicine in one day.Adv. Neural Inf. Process Syst36,\n28541– 28564 (2024).\n10. Johnson, A. E. W. et al. MIMIC-IV, a freely accessible electronic health\nrecord dataset.Sci. Data10, 219 (2023).\n11. Wu, C., Zhang X., Zhang Y., Wang Y. & Xie W. Towards generalist\nfoundation model for radiology. Preprint atarXiv https://doi.org/10.\n48550/arXiv.2308.02463 (2023).\n12. Moor, M. et al. Med-ﬂamingo: a multimodal medical few-shot learner.\nMachine Learning for Health (ML4H). PMLR. (2023).\n13. Hamamci, I. E. et al. A foundation model utilizing chest CT volumes\nand radiology reports for supervised-level zero-shot detection of\nabnormalities. Preprint atarXiv https://arxiv.org/html/2403.17834v1\n(2024).\n14. Huang, S.-C. et al. INSPECT: a multimodal dataset for pulmonary\nembolism diagnosis and prognosis. Preprint atarXiv https://doi.org/\n10.48550/arXiv.2311.10798 (2023).\n15. Bai, F., Du, Y., Huang, T., Meng, MQ-H. & Zhao, B. M3D: advancing 3D\nmedical image analysis with multi-modal large language models.\nPreprint atarXiv https://doi.org/10.48550/arXiv.2404.00578 (2024).\n16. Zhang, X. et al. RadGenome-Chest CT: a grounded vision-language\ndataset for chest CT analysis. Preprint atarXiv https://doi.org/10.\n48550/arXiv.2404.16754 (2024).\n17. Xu, C., Tao, D. & Xu, C. A survey on multi-view learning. Preprint at\narXiv https://doi.org/10.48550/arXiv.1304.5634 (2013).\n18. Yariv, L., Gu, J., Kasten, Y. & Lipman, Y. Volume rendering of neural\nimplicit surfaces. Adv. Neural Inf. Process Syst.34\n, 4805– 4815\n(2021).\n19. Park, T. et al. Contrastive learning for unpaired image-to-image\ntranslation. Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX\n16. (Springer, 2020).\n20. Sun, Y. et al. Roi pooled correlationﬁlters for visual tracking.\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp 5776-– 5784 (2019).\n21. Huang, Z. et al. Ccnet: Criss-cross attention for semantic\nsegmentation. Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp 603– 612 (2019).\n22. Papineni, K. et al. Bleu: a method for automatic evaluation of machine\ntranslation. Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics. (2002).\n23. Lin, C.-Y. et al. Rouge: a package for automatic evaluation of\nsummaries. Text Summarization Branches Out. (2004).\n24. Banerjee, S. et al. METEOR: An automatic metric for MT evaluation\nwith improved correlation with human judgments.Proceedings of the\nAcl Workshop on Intrinsic and Extrinsic Evaluation Measures for\nMachine Translation and/or Summarization. (2005).\n25. Vedantam, R. et al. Cider: consensus-based image description\nevaluation. Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp 4566-4575 (2015).\nhttps://doi.org/10.1038/s44387-025-00015-9 Review\nnpj Artiﬁcial Intelligence|            (2025) 1:17 9\n26. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: pre-training of\ndeep bidirectional transformers for language understanding.\nProceedings of the 2019 conference of the North American chapter of\nthe association for computational linguistics: human\nlanguagetechnologies, vol1 (long and short papers) (2019).\n27. Smit, A. et al. CheXbert: combining automatic labelers and expert\nannotations for accurate radiology report labeling using BERT.\nPreprint atarXiv https://doi.org/10.48550/arXiv.2004.09167 (2020).\n28. Jain, S. et al. Radgraph: extracting clinical entities and relations from\nradiology reports.(version 1.0.0).PhysioNet.RRID:SCR_007345.\n(2021).\n29. Yu, F. et al. Evaluating progress in automatic chest x-ray radiology\nreport generation.Patterns 4, 100802 (2023).\n30. Bodenreider, O. The uniﬁed medical language system (UMLS):\nintegrating biomedical terminology.Nucleic Acids Res.32,\n267D– 270D (2004).\n31. Yang, L. et al. Advancing multimodal medical capabilities of\nGemini. Preprint atarXiv https://doi.org/10.48550/arXiv.2405.\n03162 (2024).\n32. Wang, S. et al. CopilotCAD: empowering radiologists with report\ncompletion models and quantitative evidence from medical image\nfoundation models. Preprint atarXiv https://doi.org/10.48550/arXiv.\n2404.07424 (2024).\n33. Lu, Y. et al. Machine learning for synthetic data generation: a review.\nPreprint atarXiv https://doi.org/10.48550/arXiv.2302.04062 (2023).\n34. Castillo, C., Steffens, T., Sim, L. & Caffery, L. The effect of clinical\ninformation on radiology reporting: a systematic review.J. Med.\nRadiat. Sci.68,6 0– 74 (2021).\n35. Sloan, P., Clatworthy, P., Simpson, E. & Mirmehdi, M. Automated\nradiology report generation: a review of recent advances.IEEE Rev.\nBiomed. Eng.(2024).\n36. Hartung, M. P., Bickle, I. C., Gaillard, F. & Kanne, J. P. How to create a\ngreat radiology report.Radiographics 40, 1658– 1670 (2020).\n37. Tremblay, J. et al. Training deep networks with synthetic data:\nBridging the reality gap by domain randomization. InProceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition\nWorkshops. (2018).\n38. Khader, F. et al. Denoising diffusion probabilistic models for 3D\nmedical image generation.Sci. Rep.13, 7303 (2023).\n39. Li, J. et al. MedShapeNet-a large-scale dataset of 3D medical shapes\nfor computer vision.Biomed. Tech.70,7 1– 90 (2023).\n40. Thambawita, V. et al. SinGAN-Seg: synthetic training data generation\nfor medical image segmentation.PloS ONE17, e0267976 (2022).\n41. Cirillo, M. D. et al. Vox2Vox: 3D-GAN for brain tumour segmentation.\nBrainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain\nInjuries: 6th International Workshop, BrainLes 2020, Held in\nConjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Revised\nSelected Papers, Part I 6. (Springer, 2021).\n42. Wu J. et al. Medsegdiff: medical image segmentation with diffusion\nprobabilistic model. Medical Imaging with Deep Learning. PMLR.\n(2024).\n43. Mildenhall, B. et al. Nerf: Representing scenes as neural radiance\nﬁelds for view synthesis.Commun. ACM65,9 9–\n106 (2021).\n44. Raffel, C. et al. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer.J. Mach. Learn. Res.21,1 – 67 (2020).\n45. Chen, Z., Ma, M., Li, T., Wang, H. & Li, C. Long sequence time-series\nforecasting with deep learning: a survey.Inf. Fusion97, 101819 (2023).\n46. Li, J. et al. Transforming medical imaging with Transformers? A\ncomparative review of key properties, current progresses, and future\nperspectives. Med. Image Anal.85, 102762 (2023).\n47. Rane N.Transformers for medical image analysis: applications,\nchallenges, and future scope.Chall. Future Scope. (2023).\n48. Ahmad, I. S., Dai, J., Xie, Y. & Liang, X. Deep learning models for CT\nimage classiﬁcation: a comprehensive literature review.Quant.\nimaging Med. Surg.15, 962– 1011 (2025).\n49. Tang, Y. et al. Self-supervised pre-training of swin transformers for 3d\nmedical image analysis. In:Proceedings of the Ieee/cvf Conference on\nComputer Vision and Pattern Recognition(2022).\n50. Ren, H. et al. Combiner: Full attention transformer with sparse\ncomputation cost.Adv. Neural Inf. Process. Syst.34, 22470– 22482\n(2021).\n51. Pinasthika, K., Laksono, B. S. P., Irsal, R. B. P., Shabiyya, S. H. &\nYudistira, N. SparseSwin: swin transformer with sparse transformer\nblock. Neurocomputing 580, 127433 (2024).\n52. Deng, Y. et al. TChange: a hybrid transformer-CNN change detection\nnetwork. Remote Sens.15, 1219 (2023).\n53. Zhao, M., Cao, G., Huang, X. & Yang, L. Hybrid transformer-CNN for\nreal image denoising.IEEE Signal Process. Lett.29, 1252– 1256\n(2022).\n54. Zhou, W. et al. HENet: hierarchical enhancement network for\npulmonary vessel segmentation in non-contrast CT images.\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention(Springer, 2023).\n55. Hamamci, I. E., Er, S. & Menze, B. Ct2rep: automated radiology report\ngeneration for 3d medical imaging.International Conferenceon\nMedical Image Computing and Computer-Assisted Intervention, pp.\n476– 486 (2024).\n56. Chen, Z., Luo, L., Bie, Y. & Chen, H. Dia-LLaMA: towards large\nlanguage model-driven CT report generation. Preprint atarXiv https://\ndoi.org/10.48550/arXiv.2403.16386 (2024).\n57. Blankemeier, L. et al. Merlin: a vision language foundation model for\n3D computed tomography. Research Square, rs-3 (2024).\n58. Zhang, T. et al. Incorporating clinical guidelines through adapting\nmulti-modal large language model for prostate cancer PI-RADS\nscoring. International Conference on Medical Image Computing and\nComputer-Assisted Intervention, pp. 360– 370 (2024).\n59. Liu, C. et al. Benchmarking and boosting radiology report generation\nfor 3D high-resolution medical images. Preprint atarXiv https://arxiv.\norg/html/2406.07146v2 (2024).\n60. Zhong, Z. et al. Abn-BLIP: abnormality-aligned bootstrapping\nlanguage-image pre-training for pulmonary embolism diagnosis and\nreport generation from CTPA. Preprint atarXiv\nhttps://doi.org/10.\n48550/arXiv.2503.02034 (2025).\n61. Shui, Z. et al. Large-scale andﬁne-grained vision-language pre-\ntraining for enhanced CT image understanding. Preprint atarXiv\nhttps://doi.org/10.48550/arXiv.2501.14548 (2025).\n62. Lee, C. et al. Read like a radiologist: efﬁcient vision-language model\nfor 3D medical imaging interpretation. Preprint atarXiv https://doi.org/\n10.48550/arXiv.2412.13558 (2024).\n63. Ni, X. et al. MG-3D: Multi-grained knowledge-enhanced 3D medical\nvision-language pre-training. Preprint atarXiv https://doi.org/10.\n48550/arXiv.2412.05876 (2024).\n64. Chen, Z., Bie, Y., Jin, H. & Chen, H. Large language model with region-\nguided referring and grounding for CT report generation.\nIEEETransactions on Medical Imaging(2025).\n65. Lee, J. et al. Improving Factuality of 3D Brain MRI Report Generation\nwith Paired Image-domain Retrieval and Text-domain Augmentation.\narXiv preprint arXiv:15490. (2024).\n66. Shi, Y. et al. Med-2E3: a 2D-enhanced 3D medical multimodal large\nlanguage model. Preprint atarXiv https://doi.org/10.48550/arXiv.\n2411.12783 (2024).\n67. Lai, H. et al. E3D-GPT: enhanced 3D visual foundation for medical\nvision-language model. Preprint atarXiv https://doi.org/10.48550/\narXiv.2410.14200 (2024).\n68. Luo, L., Tang, B., Chen, X., Han, R. & Chen, T. VividMed: vision\nlanguage model with versatile visual grounding for medicine. Preprint\nat arXiv https://doi.org/10.48550/arXiv.2410.12694 (2024).\n69. Li, S., Xu, B., Luo, Y., Nie, D. & Zhang, L. ViT3D alignment of LLaMA3:\n3D medical image report generation. PreprintarXiv https://doi.org/10.\n48550/arXiv.2410.08588 (2024).\nhttps://doi.org/10.1038/s44387-025-00015-9 Review\nnpj Artiﬁcial Intelligence|            (2025) 1:17 10\n70. Zheng, C., Ji, J., Shi, Y., Zhang, X. & Qu L. See detail say clear: towards\nbrain CT report generation via pathological clue-driven representation\nlearning. Preprint atarXiv https://doi.org/10.48550/arXiv.2409.19676\n(2024).\n71. Chen H. et al. 3d-ct-gpt: generating 3d radiology reports through\nintegration of large vision-language models. Preprint atarXiv https://\ndoi.org/10.48550/arXiv.2409.19330 (2024).\n72. Alkhaldi A. et al. Minigpt-med: Large language model as a general\ninterface for radiology diagnosis. Preprint atarXiv https://doi.org/10.\n48550/arXiv.2407.04106 (2024).\n73. Li, C.-Y. et al. Towards a holistic framework for multimodal LLM in 3D\nbrain CT radiology report generation.Nat. Commun.16, 2258 (2025).\n74. Rajpurkar, M. et al. CheXNet: radiologist-level pneumonia detection\non chest X-rays with deep learning. Preprint atarXiv https://doi.org/\n10.48550/arXiv.1711.05225 (2017).\n75. Rudin, C. Stop explaining black box machine learning models for high\nstakes decisions and use interpretable models instead.Nat. Mach.\nIntell. 1, 206– 215 (2019).\n76. Alijani, S., Fayyad, J. & Najjaran, H. Vision transformers in domain\nadaptation and domain generalization: a study of robustness.Neural\nComput. Appl.36, 17979– 18007 (2024).\n77. Manzari, O. N., Ahmadabadi, H., Kashiani, H., Shokouhi, S. B. &\nAyatollahi, A. MedViT: a robust vision transformer for generalized\nmedical image classiﬁcation. Comput. Biol. Med.157, 106791 (2023).\n78. Ghosh A., Acharya A., Saha S., Jain V. & Chadha A. Exploring the\nfrontier of vision-language models: a survey of current methodologies\nand future directions. Preprint atarXiv https://doi.org/10.48550/arXiv.\n2404.07214 (2024).\n79. Awais, M. et al. Foundation models deﬁning a new era in vision: a survey\nand outlook.IEEE Trans. Pattern Anal. Mach. Intell.47, 2245–2264 (2025).\n80. Mennella, C., Maniscalco, U., De Pietro, G. & Esposito, M. Ethical and\nregulatory challenges of AI technologies in healthcare: a narrative\nreview. Heliyon 10, e26297 (2024).\nAuthor contributions\nJ.W. and Y.W. contributed equally to this work as co-ﬁrst authors. They were\nresponsible for the initial literature search, critical evaluation of the sources,\nand drafting of the initial manuscript. Z.Z. and Z.J. provided substantial\ncontributions to the conceptualization of the review and the analysis of the\nliterature. W.L. and N.T. contributed to the clinical perspective of the review\nand offered critical feedback on the intellectual content of the review. H.B.\nserved as the corresponding author, overseeing all stages of the review\nprocess, including theﬁnal approval of the version to be submitted. All\nauthors have read and approved theﬁnal version of the review.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondenceand requests for materials should be addressed to\nHarrison X. Bai.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s44387-025-00015-9 Review\nnpj Artiﬁcial Intelligence|            (2025) 1:17 11",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.7917152643203735
    },
    {
      "name": "Computer science",
      "score": 0.476458340883255
    },
    {
      "name": "Medical physics",
      "score": 0.3433103561401367
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3398517966270447
    },
    {
      "name": "Medicine",
      "score": 0.2954155206680298
    },
    {
      "name": "History",
      "score": 0.1949661672115326
    },
    {
      "name": "Archaeology",
      "score": 0.11013159155845642
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139660479",
      "name": "Central South University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210153856",
      "name": "Second Xiangya Hospital of Central South University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2799853436",
      "name": "Johns Hopkins Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I196272386",
      "name": "Providence College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800446032",
      "name": "Rhode Island Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210159865",
      "name": "Xiangya Hospital Central South University",
      "country": "CN"
    }
  ],
  "cited_by": 1
}