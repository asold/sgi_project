{
  "title": "Natural Language Processing Methods for Language Modeling",
  "url": "https://openalex.org/W3215499059",
  "year": 2021,
  "authors": [
    {
      "id": null,
      "name": "Nemeskey, Dávid Márk",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6660690085",
    "https://openalex.org/W14331692",
    "https://openalex.org/W6752788575",
    "https://openalex.org/W2946558277",
    "https://openalex.org/W2244692426",
    "https://openalex.org/W2970119519",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W1966812932",
    "https://openalex.org/W6683948737",
    "https://openalex.org/W2251803266",
    "https://openalex.org/W1513718494",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W1558797106",
    "https://openalex.org/W2152808281",
    "https://openalex.org/W1806891645",
    "https://openalex.org/W2096175520",
    "https://openalex.org/W4245107743",
    "https://openalex.org/W6681943951",
    "https://openalex.org/W2056250865",
    "https://openalex.org/W6639619044",
    "https://openalex.org/W6990097241",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2731822910",
    "https://openalex.org/W1505680913",
    "https://openalex.org/W6676373471",
    "https://openalex.org/W2249953602",
    "https://openalex.org/W6685172698",
    "https://openalex.org/W6670726752",
    "https://openalex.org/W6678277124",
    "https://openalex.org/W6674571003",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W6691892052",
    "https://openalex.org/W6683103995",
    "https://openalex.org/W2833383446",
    "https://openalex.org/W1490960179",
    "https://openalex.org/W1566315437",
    "https://openalex.org/W1539309091",
    "https://openalex.org/W6629637077",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W6635178583",
    "https://openalex.org/W2217098601",
    "https://openalex.org/W2515910099",
    "https://openalex.org/W6811685372",
    "https://openalex.org/W1593045043",
    "https://openalex.org/W6683738474",
    "https://openalex.org/W6767737316",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2138122637",
    "https://openalex.org/W6631902868",
    "https://openalex.org/W2103496339",
    "https://openalex.org/W2001792610",
    "https://openalex.org/W6681698864",
    "https://openalex.org/W2049901611",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W6696775231",
    "https://openalex.org/W6681896989",
    "https://openalex.org/W2460788634",
    "https://openalex.org/W2387546565",
    "https://openalex.org/W2096072088",
    "https://openalex.org/W6663947504",
    "https://openalex.org/W6769430610",
    "https://openalex.org/W2101926813",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W6677408996",
    "https://openalex.org/W6629956336",
    "https://openalex.org/W2512498397",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W6754723338",
    "https://openalex.org/W2082092506",
    "https://openalex.org/W6730267373",
    "https://openalex.org/W45102278",
    "https://openalex.org/W6676699771",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W6748304040",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W6684821475",
    "https://openalex.org/W6637409405",
    "https://openalex.org/W1572063013",
    "https://openalex.org/W2406530069",
    "https://openalex.org/W2251291469",
    "https://openalex.org/W6753056052",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W1583614607",
    "https://openalex.org/W194249466",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W2137983211",
    "https://openalex.org/W2164019165",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2609517418",
    "https://openalex.org/W2134587001",
    "https://openalex.org/W2058373514",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W1485981043",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W581956982",
    "https://openalex.org/W6692563993",
    "https://openalex.org/W6607467106",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W6687291333",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W47415966",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2120101509",
    "https://openalex.org/W7011001382",
    "https://openalex.org/W6736198902",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2127836646",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2102439588",
    "https://openalex.org/W169539560",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W6811356141",
    "https://openalex.org/W6635469476",
    "https://openalex.org/W6691649953",
    "https://openalex.org/W2125031621",
    "https://openalex.org/W1615991656",
    "https://openalex.org/W1010415138",
    "https://openalex.org/W6684503276",
    "https://openalex.org/W2251765408",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W6640446804",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W6670669432",
    "https://openalex.org/W6741459021",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2121299803",
    "https://openalex.org/W1566049083",
    "https://openalex.org/W2614244594",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W6996569244",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2126725946",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W6607333740",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W2759848268",
    "https://openalex.org/W4231678899",
    "https://openalex.org/W2120861206",
    "https://openalex.org/W36903255",
    "https://openalex.org/W6678887340",
    "https://openalex.org/W2162456950",
    "https://openalex.org/W1846321791",
    "https://openalex.org/W6732698874",
    "https://openalex.org/W2075201173",
    "https://openalex.org/W1497026191",
    "https://openalex.org/W2120011233",
    "https://openalex.org/W1517947178",
    "https://openalex.org/W6691678190",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2762652133",
    "https://openalex.org/W2740626591",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W6756040250",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6679063059",
    "https://openalex.org/W2578576916",
    "https://openalex.org/W2093390569",
    "https://openalex.org/W6726320248",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2612883315",
    "https://openalex.org/W6648914341",
    "https://openalex.org/W1590952807",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W2286496058",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2293634267",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W6678555148",
    "https://openalex.org/W2437096199",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W6697814554",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W6648982606",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W1813659000",
    "https://openalex.org/W6679915538",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W6605210145",
    "https://openalex.org/W6731765786",
    "https://openalex.org/W6713098461",
    "https://openalex.org/W6607974698",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W2124657827",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W7051469422",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W2126596035",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1692991189",
    "https://openalex.org/W2250888076",
    "https://openalex.org/W6725207838",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W1971129545",
    "https://openalex.org/W6732742072",
    "https://openalex.org/W2057653135",
    "https://openalex.org/W1674799117",
    "https://openalex.org/W1508567213",
    "https://openalex.org/W2767321762",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W6681099935",
    "https://openalex.org/W1970961429",
    "https://openalex.org/W6635446068",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2740840489",
    "https://openalex.org/W6806468881",
    "https://openalex.org/W6720905350",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2111741689",
    "https://openalex.org/W2890560993",
    "https://openalex.org/W4294555862",
    "https://openalex.org/W4297801177",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3098861490",
    "https://openalex.org/W2080018251",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4255825300",
    "https://openalex.org/W2946119234",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2147152072",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W630532510",
    "https://openalex.org/W4233559841",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2742448943",
    "https://openalex.org/W2605203995",
    "https://openalex.org/W2108321481",
    "https://openalex.org/W182831726",
    "https://openalex.org/W2546302380",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2979401726",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2250667763",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2013196554",
    "https://openalex.org/W1928085876",
    "https://openalex.org/W2159077101",
    "https://openalex.org/W126222424",
    "https://openalex.org/W2148321140",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2251771443",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963052942",
    "https://openalex.org/W4300601563",
    "https://openalex.org/W2081193615",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4206765718",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2170599822",
    "https://openalex.org/W2140452151",
    "https://openalex.org/W2570920354",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2583402933",
    "https://openalex.org/W4237284205",
    "https://openalex.org/W2963932686",
    "https://openalex.org/W4295162875",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W2116261113",
    "https://openalex.org/W2053921957",
    "https://openalex.org/W2127314673",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2517054195",
    "https://openalex.org/W2169200297",
    "https://openalex.org/W4242989628",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2187953100",
    "https://openalex.org/W2026255932",
    "https://openalex.org/W2155693943",
    "https://openalex.org/W2133280805",
    "https://openalex.org/W4303633609",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2128499899",
    "https://openalex.org/W1494910745",
    "https://openalex.org/W2154642048",
    "https://openalex.org/W2993383518",
    "https://openalex.org/W4252015580",
    "https://openalex.org/W2126807068",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W2163929346",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4297797397",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W3217309209",
    "https://openalex.org/W2950621961",
    "https://openalex.org/W50420911",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2963419157",
    "https://openalex.org/W2250653840",
    "https://openalex.org/W1502916507",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W4234410495",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W1592796124"
  ],
  "abstract": "A disszertációban két kérdésre koncentráltunk. Egyfelől célnak tűztük ki a magyar nyelvmodellezési eredmények javítását; másrészt azt vizsgáltuk, hogy alkalmasak-e erre a célra a modern természetesnyelv-feldolgozás (vagy a hagyományos nyelvészet) módszerei. A 2. fejezet bemutatta a problémákat, amivel a magyar nyelv szószintű modellezésekor találkozunk. Itt mutattuk be a „gluténmentes” formátumot, egy morfológiai szegmentáló algoritmust, ami ha nem is orvosolja, de csökkenti a szóalakok túlburjánzása okozta problémákat. A 3. fejezet egy értelmező szótárakon alapuló módszert javasolt többjelentésű szóbeágyazások kiértékelésére. A 5. fejezet az ellenkező irányra ad példát, amikor nyelvmodellek alkalmazása javítja egy nyelvfeldolgozó rendszer eredményét. Kutatásunk keretében több módon is korszerűsítettük a magyar nyelvmodellezés állapotát. Egyrészt több nyelvmodellt is kimértünk három magyar korpuszon. Ezek közül egyiket, a Magyar Webkorpusz előfeldolgozott változatát javasoltuk sztenderd nyelvmodellkiértékelő korpusznak a 2. fejezetben. A Webkorpusz 2.0 létrehozását mutatja be a 4. fejezet. Az új korpusz a Common Crawl webarchívumból és a magyar Wikipédiából tevődik össze. Kilenc milliárd szavas méretével a legnagyobb magyar korpusz, mérete 3,5-szörösen haladja meg a legnagyobb (kereskedelmi) korpuszét. Végül a 5. fejezet mutatja be az emBERT modult, ami lehetővé teszi modern kontextualizált beágyazások integrálását az e-magyar szövegfeldolgozó rendszerbe. A kísérleti magyar BERT modellünkre építő főnévicsoport-felismerő 2,8% F1-ponttal múlja felül a korábbi legjobb rendszert. Minden bemutatott erőforrás (korpuszok, modellek és szoftver) szabadon letölthető.",
  "full_text": "Eötvös Loránd University\nF aculty of Informatics\nPh.D. Dissertation\nDá vid Márk Nemeskey\nNatural Language Processing Methods for\nLanguage Modeling\nDoctoral School of Informatics\nDr. Csuhaj V arjú Erzsébet D.Sc.\nF oundation and Methodology of Informatics Doctoral Programme\nZoltán Horváth Ph.D.\nSupervisors:\nAndrás Benczúr Ph.D.\nAndrás Kornai D.Sc.\nBudapest, 2020\nEötvös Loránd T udományegyetem\nInformatikai Kar\nDoktori Értekezés\nNemeskey Dá vid Márk\nTermészetes nyel vfeldolgozási módszerek a\nnyel vmodellezésben\nInformatika Doktori Iskola\nDr. Csuhaj V arjú Erzsébet D.Sc.\nAz informatika alapjai és módszertana Doktori Program\nHorváth Zoltán Ph.D.\nTémavezetők:\nifj. Benczúr András Ph.D.\nKornai András D.Sc.\nBudapest, 2020\nAcknowledgements\nI would like to thank my advisors András Kornai and András Benczúr for their ongoing\nsupport, for providing a motivating work environment and particularly for their infinite\npatience.\nThe thesis benefited immensely from the insightful remarks of the two reviewers, And-\nrás Lukács and András Micsik.\nI would also like to thank all my current and former colleagues at the SZT AKI HL T\nResearch Group: Judit Ács, Gábor Borbély , Dániel Lévai, Márton Makrai, Katalin Paj-\nkossy , Gábor Recski, Eszter Simon and Attila Zséder, as well as those in the wider NLP\n“family” . I feel honored to be part of such an inspiring community .\nLast, but definitely not least, I would like to thank my family for their love and\nunderstanding, especially in the hectic final weeks. This thesis would have never been\nfinished without their support.\n∗ ∗ ∗\nResearch partially supported by National Research, Development and Innovation Office\n(NKFIH) grants #115288: “ Algebra and algorithms ” and #120145: “ Deep Learning of\nMorphological Structure”, as well as by National Excellence Programme 2018-1.2.1-NKP-\n00008: “ Exploring the Mathematical F oundations of Artificial Intel ligence” .\nExperiments in Chapter 2 were made possible by a hardware grant from NVIDIA Cor-\nporation; huBERT in Chapter 4 was trained on TPUs provided by the T ensorflow Research\nCloud program. Their support is gratefully acknowledged.\n1\nContents\nIntroduction 6\nTheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\nResources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1 Language Modeling 10\n1.1 Natural language processing . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.1.1 NLP tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n1.1.2 Machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n1.2 Statistical language modeling . . . . . . . . . . . . . . . . . . . . . . . . . 13\n1.2.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n1.2.2 Mathematical formulation . . . . . . . . . . . . . . . . . . . . . . . 15\n1.2.3 T raining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n1.2.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n1.2.5 Discrete and continuous methods . . . . . . . . . . . . . . . . . . . 18\n1.3 n-grams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n1.3.1 T raining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n1.3.2 Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n1.3.3 Class-based models . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n1.3.4 Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n1.4 The first neural models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n1.4.1 Neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n1.4.2 T raining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n1.4.3 Bengio’s model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.4.4 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n1.5 Recurrent neural network language models . . . . . . . . . . . . . . . . . . 31\n1.5.1 Recurrent neural networks . . . . . . . . . . . . . . . . . . . . . . . 32\n1.5.2 Gated architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n2\n1.5.3 Language modeling advances . . . . . . . . . . . . . . . . . . . . . 35\n1.6 T ransformer-based language models . . . . . . . . . . . . . . . . . . . . . . 41\n1.6.1 Neural machine translation . . . . . . . . . . . . . . . . . . . . . . 41\n1.6.2 The T ransformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n1.6.3 T ransformers in language modeling . . . . . . . . . . . . . . . . . . 44\n1.6.4 Performance considerations . . . . . . . . . . . . . . . . . . . . . . 46\n1.7 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n1.7.1 V ector space semantics . . . . . . . . . . . . . . . . . . . . . . . . . 48\n1.7.2 Static embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n1.7.3 Multi-sense embeddings . . . . . . . . . . . . . . . . . . . . . . . . 51\n1.7.4 Contextual word embeddings . . . . . . . . . . . . . . . . . . . . . 51\n1.7.5 Embeddings in NLP . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n1.8 Language modeling and NLP . . . . . . . . . . . . . . . . . . . . . . . . . 54\n2 emLam – a Hungarian Language Modeling baseline 56\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n2.2 The Hungarian Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n2.2.1 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n2.2.2 Corpus Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n2.2.3 The Benchmark Corpus . . . . . . . . . . . . . . . . . . . . . . . . 61\n2.3 Language model evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n2.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n2.4.1 n-grams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n2.4.2 Class-based n-grams . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n2.4.3 Cross-evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n2.4.4 RNN language models . . . . . . . . . . . . . . . . . . . . . . . . . 65\n2.4.5 Pseudo-Hungarian . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n2.4.6 Into the Unknown . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n2.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n2.5.1 F uture work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n3 Evaluating multi-sense embeddings for semantic resolution 71\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n3.2 Comparing lexical headwords to multiple sense vectors . . . . . . . . . . . 72\n3.2.1 Resources to be evaluated . . . . . . . . . . . . . . . . . . . . . . . 72\n3.2.2 Lexical resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.2.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n3\n3.3 Parts of speech and word frequency . . . . . . . . . . . . . . . . . . . . . . 75\n3.4 Cross-linguistic treatment of concepts . . . . . . . . . . . . . . . . . . . . . 77\n3.5 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n4 Habeas Corpus 79\n4.1 Goals and design considerations . . . . . . . . . . . . . . . . . . . . . . . . 80\n4.1.1 Goals and constraints . . . . . . . . . . . . . . . . . . . . . . . . . 80\n4.1.2 Design considerations . . . . . . . . . . . . . . . . . . . . . . . . . 80\n4.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n4.2.1 Preexisting corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n4.2.2 Common Crawl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n4.2.3 As a training corpus . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n4.3 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n4.3.1 Computing environment . . . . . . . . . . . . . . . . . . . . . . . . 85\n4.3.2 The pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n4.4 Running the pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n4.4.1 Download . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n4.4.2 Boilerplate removal . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n4.4.3 Content-based filtering . . . . . . . . . . . . . . . . . . . . . . . . . 88\n4.4.4 Deduplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n4.4.5 Linguistic analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n4.4.6 Final statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n4.5 Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n4.5.1 Wikihopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n4.5.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n4.6 huBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n4.6.1 Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n4.6.2 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n4.7 Conclusion and future work . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5 emBERT: language modeling for NLP 101\n5.1 Deep learning in NLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5.2 BER T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n5.2.1 Why BER T? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n5.2.2 Does multi-BER T speak Hungarian? . . . . . . . . . . . . . . . . . 103\n5.3 The emBERT module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n5.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n4\n5.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n5.5.1 Chunking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n5.5.2 Named entity recognition . . . . . . . . . . . . . . . . . . . . . . . 107\n5.6 F uture work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n5.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n6 Conclusions 110\nAppendices 112\nA Abbreviations used in the thesis 113\nB Sample texts generated by T ransformer models 114\n5\nIntroduction\nThe field of natural language processing (NLP) is contemporaneous with computers. Ma-\nchine translation systems were developed as early as the 1950s, and the widespread use\nof personal computers and digitalization of business processes led to an overabundance\nof textual data that had to be processed, organized, and even understood to a certain\ndegree. NLP systems arose to meet these challenges, so that today millions of computer\nusers routinely use spell and syntax checkers and translation services in their daily work\nand speech-to-text interfaces when interacting with their devices. Behind the scenes, NLP\nmodules such as lemmatizers, named entity recognizers and parsers are cornerstones in\nmany information processing systems.\nCompared to this ubiquity , language modeling has been, until recently , an obscure field\neven among NLP practitioners. This was in spite of the fact that language models played\nan integral role in some of the tasks mentioned above, particularly machine translation and\nspeech recognition. Only in recent times, with the advent of deep, neural language models\nhas the situation changed. W ord embeddings revolutionized computational semantics,\nand unsupervised representations supplanted linguistic features in NLP systems. T oday ,\nlanguage modeling has become pervasive in all fields of NLP .\nIn this thesis, we study the interaction between language modeling and NLP , with\nspecial focus on two aspects. First, most of the work done for language modeling, surely all\nsuccess stories, concentrated on English. In this work, we examine how language modeling\ntechniques developed for English perform on Hungarian, and what modifications might\nbe needed to adapt them to a different language. Second, historically the inclusion of\nlanguage modeling advanced natural language processing systems; here we are concerned\nwith the opposite direction and study how linguistics or NLP can help in training or\nevaluating language models.\nThe thesis is structured as follows. Chapter 1 presents an overview of the field of\nlanguage modeling, with particular focus on its connection to natural language processing.\nThe main discrete and continuous techniques for autoregressive language modeling are\nintroduced, as well as word embeddings, a staple in modern NLP .\n6\nIn Chapter 2, we evaluate some of the state-of-the-art language modeling methods on\nHungarian. T o standardize language model assessment in the future, a benchmark corpus\nis introduced. W e also present a morphological method for alleviating the vocabulary\nproblem, which is much more pronounced in Hungarian than it is in English. Continuing\nwith the evaluation theme, Chapter 3 proposes a novel method for assessing multi-sense\nembeddings based on sense distinctions made in monolingual dictionaries.\nW ork on Hungarian contextualized embeddings is presented in the next two chapters.\nChapter 4 details our work of compiling W ebcorpus 2.0, a new Hungarian gigaword cor-\npus, from the Common Crawl and the Hungarian Wikipedia. Its main purpose being a\ntraining set for contextual embeddings, it is the largest Hungarian corpus yet by a fac-\ntor of 3.5. Results on huBERT, a preliminary Hungarian BER T model, are also reported\nhere. Chapter 5 describes emBERT, a module of the e-magyar text processing system, that\nallows integration of contextualized embedding-based token classifiers into the pipeline.\nThe NP chunker model is the current state-of-the-art for Hungarian. Chapter 6 wraps up\nthe thesis.\nTheses\nThe main theses of the dissertation are the following:\n(T1) An exhaustive evaluation of discrete and continuous language modeling methods for\nHungarian and the associated benchmark corpus\n(T2) The “gluten-free” format, a morphological method of alleviating the vocabulary\nproblem in Hungarian language modeling\n(T3) A novel intrinsic evaluation method for multi-sense embeddings based on sense dis-\ntinctions made in monolingual dictionaries\n(T4) W ebcorpus 2.0, the largest Hungarian corpus to date by a factor of 3.5, compiled\nfrom the Common Crawl web archive and the Hungarian Wikipedia\n(T5) huBERT, a preliminary Hungarian BER T model based on Wikipedia, which outper-\nforms the multi-language BER T on four Hungarian benchmark tasks\n(T6) The emBERT module that allows integration of contextualized embedding-based clas-\nsifiers into the e-magyar pipeline, thereby improving the state of the art in NP\nchunking\n7\nContributions\nAll work presented in the thesis is the contribution of the author. While Chapter3 is based\non joint research with Gábor Borbély , Márton Makrai and András Kornai, and Chapter 4\nbenefits from prior work by Balázs Indig, the parts detailed in the text constitute the\nauthor’s own.\nThe research described in the thesis has been partly presented in the following papers,\nlisted in the order of the corresponding chapters:\n(T1, T2) Results of Chapter 2 were published in Nemeskey ( 2017);\n(T3) The evaluation method described in Chapter 3 constitutes the first half of\nBorbély; Makrai, et al. ( 2016);\n(T4, T5) Chapter 4 will be published later; W ebcorpus 2.0 and huBERT, introduced in\nthe chapter, will be uploaded to the SZT AKI HL T repository;\n(T6) The emBERT module appeared in Nemeskey ( 2020), which has won Special\nA ward at the XVI. Conference on Hungarian Computational Linguistics.\nResources\nV arious resources have been created during our research, all of which are publicly available\nfor download under permissive or public domain licenses. T able 1 lists the two corpora\npresented in Chapters 2 and 4.\nName Description Introduced in\nemLam Language modeling benchmark corpus for Hungarian Chapter 2\nW ebcorpus 2.0 Our 9 billion token corpus built from Common Crawl and Wikipedia Chapter 4\nT able 1: Corpora introduced in this dissertation\nhuBERT (T5) and the models trained by the emBERT module (T6) are listed in T able 2.\nName Description\nhuBERT Hungarian BER T model trained on Wikipedia\nszeged_ner_bioes NER model trained on the Szeged NER corpus\nszeged_basenp_bioes State-of-the-art base NP chunker model\nszeged_maxnp_bioes State-of-the-art maximal NP chunker model\nT able 2: huBERT and the emBERT models discussed in Chapter 5\n8\nAll software used to create the corpora and models described here are available for down-\nload under permissive open source licenses. T able 3 enumerates all packages and the\nGitHub repositories they are preserved in.\nPackage Description Introduced in\nemLam Preprocessing and training scripts for Hungarian language modeling Chapter 2\ncc_corpus T ools for compiling corpora from Common Crawl Chapter 4\nzim_to_corpus Scripts to extract Wikipedia pages from .zim archives. Chapter 4\nemBER T emtsv module for pre-trained T ransformer-based models Chapter 5\nT able 3: Software libraries presented in the thesis\nThe hyperlinks given in blue above take readers of the pdf version directly to the right\nwebpage. F or the convenience of readers of the paper version we note that the SZT AKI\nHL T repository that hosts the corpora and models is accessible at https://hlt.bme.hu/\nen/resources; the emLam software is accessible from the e-magyar GitHub organization\nat https://github.com/dlt-rilmta; and the rest of the software is hosted under the\nauthor’s GitHub account at https://github.com/DavidNemeskey.\n9\nChapter 1\nLanguage Modeling\nThis work studies the interaction between natural language processing and language mod-\neling. In this chapter, we define the task of language modeling and survey the mainstream\ntechniques used in the last three decades to address it.\nT o put the main question into context however, in Section 1.1 we begin with a brief\noverview of the field of natural language processing and the tasks it deals with. Section 1.2\ngives the mathematical and historical background behind language modeling. Section 1.3\npresents the main language modeling method prior to the advance of deep learning: n-\ngrams. Neural network language models are introduced in Section 1.4, and the most\npopular neural architectures, LSTM and transformers, are discussed in Sections 1.5 and\n1.6, respectively . An overview of “the crown jewels of NLP”, word embeddings 1, is given\nin Section 1.7. Finally , in Section 1.8, we examine the role natural language processing\nand language modeling had in each other’s development.\n1.1 Natural language processing\nThe field Natural language processing (NLP) is concerned with creating computer algo-\nrithms and systems to process and analyze natural language data. In this thesis we assume\na passing familiarity on part of the reader both with NLP and with (high-school level)\nlinguistics. What follows is a short summary of the main tasks in NLP . W e also review the\nbasics of machine learning, which underlies most modern NLP systems. F or those wishing\nto know more, the go-to book is Jurafsky and Martin ( 2009)3.\n1http://bit.ly/1ipv72M2, cited as bad example of early overenthusiasm about embeddings in Baroni\net al. ( 2014).\n2All URLs in the thesis were retrieved on May 16, 2020.\n3The draft of the 3 rd edition is available at https://web.stanford.edu/~jurafsky/slp3/.\n10\n1.1.1 NLP tasks\nThe workhorses of NLP are the text processing pipelines that perform routine linguistic\nanalysis on large amounts of text. F or English, the most well-known software libraries are\nStanford CoreNLP4 and spaCy5. The tasks they address have well-established formalisms\nand efficient algorithms to solve them. T able1.1 and Figure 1.1 illustrate how the sentence\n“Mary had a little lamb. ” is analyzed by Stanford CoreNLP .\nT okenization is the task of breaking the input text into tokens, typically words and\npunctuation marks (which are split from words).\nLemmatization is performed to determine the lemma (dictionary form) of a word.\nPart-of-speech (POS) tagging decides the part-of-speech category (noun, verb, etc.)\nof words. In languages with richer morphological structure, full morphological anal-\nysis is performed instead, which assigns a tag to each morpheme (e.g. Hungarian\nláthatjátok ‘you can see’, may be analyzed aslát[/V]hat[_Mod/V]játok[Prs.Def.2Pl]).\nNamed Entity Recognition (NER) is the task of finding named entities in the text\nand determining their type (people, organizations, locations, etc.).\nSyntactic parsing is performed to build a parse tree of the sentence that represents the\nrelations between its words. The two most popular formalisms are Phrase Structure\nGrammar (PSG) (Chomsky, 1957) and Dependency Grammar (T esniére, 1959); see\nFigure 1.1 for an example for both.\nSentence Mary had a little lamb.\nT okens Mary had a little lamb .\nLemmas Mary have a little lamb .\nPart-of-speech NNP VBD DT JJ NN .\nNamed entities PERSON\nT able 1.1: Analysis of an example sentence.\nThere are tasks that are usually tackled by stand-alone systems, though some pipelines\n(like CoreNLP) do include them:\nCoreference resolution finds expressions that refer to the same entity . F or instance, if\nthe sentence following the example starts with “ It ... ”, it presumably refers to the\nlamb, not Mary .\n4https://stanfordnlp.github.io/CoreNLP/\n5https://spacy.io/\n11\nS\n.\n.\nVP\nNP\nNN\nlamb\nJJ\nlittle\nDT\na\nVBD\nhad\nNP\nNNP\nMary Mary had a little lamb .\nROOT\nnsubj\npunct\ndobj\namod\ndep\nFigure 1.1: Constituency (left) and dependency (right) parse of the example sentence.\nSentiment analysis identifies the sentiment expressed in text (usually ‘positive’ or ‘neg-\native’).\nFinally , NLP also features high-level natural language understanding (NLU) tasks.\nThese are hard problems which are far from being solved. As such, they are always\naddressed by standalone (mostly research) systems. These include but are not limited to\nquestion answering ; machine translation ; text summarization and textual entailment , in\nwhich the system has to decide whether a statement can be inferred from a piece of text.\n1.1.2 Machine learning\nWith the exception of tokenization and morphological analysis, all tasks mentioned in the\nprevious section are tackled by machine learning (also: statistical) models. Here we briefly\nintroduce the basic machine learning concepts used in the rest of the thesis.\nThe goal of machine learning is to discover patterns in data. There are two basic\nparadigms for agentless machine learning: supervised and unsupervised learning. In the\nformer, models learn a mapping between inputs and known (typically human-supplied)\noutputs; an example is sentiment analysis, where the inputs are a sentence and its lin-\nguistic features, and the output is the sentiment judgement ( +/−) made by a human.\nUnsupervised learning, on the other hand, looks for patterns in unlabeled data; an ex-\nample is clustering, which tries to group similar objects together based solely on their\nproperties.\nSupervised learning can be further divided intoclassification, when there is a predefined\nset of output labels (POS tags, sentiments) and regression, where the output can take\nany real value. NLP tasks, such as POS tagging and NER, belong to a special case of\nclassification called sequence labeling. As with regular classification, the task is to tag\neach item in the sequence with the correct label; however, the items are not independent\n12\n(words in a sentence rarely are), and only algorithms that exploit the dependencies in the\nsequence can hope to perform well 6.\nSupervised models are trained on training data that consists of manually labeled train-\ning examples ; i.e. input–output pairs. It is a well-established practice to split this data\ninto training, validation and tests sets. The model is trained on the training set, and\nits performance is evaluated on the test set. The validation set can be used to fine-tune\nhyperparameters of the model.\nF or NLP tasks, the training data is typically a labeled corpus; syntactic parsers are\ntrained on treebanks, where each training example consists of a sentence and its parse tree.\nOne example is the Penn T reeBank (PTB)(Marcus et al., 1993), which has served as the\ngold standard dataset for various NLP tasks, such as POS and NER taggers, syntactic\nparsers, etc. Higher-level tasks have their own set of benchmarks: for question answering,\nthe Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016, 2018) is the\nmost popular, while GLUE (W ang et al., 2018), with its 9 tasks, provides a rich testbed\nfor various NLU systems.\nAnnotated datasets tend to be small (50–100 thousand sentences) due to the financial\ncosts associated with the linguistic expertise required to create them.\n1.2 Statistical language modeling\nStatistical language modeling (LM), at its most generic, is the task of capturing regularities\nof natural language in a probabilistic model (Rosenfeld,2000). In almost all cases however,\nlanguage modeling is understood to be the task of estimating the probability of a sequence\nof words. The history of LM can be traced back to Markov (1913) and Shannon (1948), who\nused n-grams to predict the next character or word, respectively , in natural language text.\nLanguage modeling, however, got its real start when it began to be included in automatic\nspeech recognition (ASR) systems to improve their performance (Bahl et al., 1983; Baker,\n1975; Jelinek et al., 1975). Other NLP fields soon followed suit and before long, language\nmodels were incorporated into systems addressing a variety of tasks, including optical\ncharacter recognition (OCR) (Breuel, 1994; Kornai, 1994); machine translation (MT)\n(Brown et al., 1990); and information retrieval (IR) (Berger and Lafferty, 1999; Ponte and\nCroft, 1998). Lately , the deep learning revolution has made language models ubiquitous\nin all areas of NLP by supplanting (or at least complementing) manual features with word\nembeddings (see Section 1.7.5).\n6F or instance, there is no way to tell whether the word ‘ saw’ in a sentence is a noun or the past form\nof the verb ‘ see’ without looking at the rest of the words.\n13\nCapturing regularities in language is traditionally the domain of linguistic theories of\ngrammar (and morphology , pragmatics, etc). These approaches are based on explicit lin-\nguistic knowledge, and are typically top-down in the sense that word order and placement\nare governed by higher-level linguistic relations and structures such as phrases 7.\nIn contrast, language models are mostly built from the bottom up, from corpus statis-\ntics and the word sequence itself. They do not aim to explain linguistic phenomena and\nany linguistic regularities learned by the models are implicit. This is not to say that\nsuch regularities are not captured, however. Even n-grams are able to find idiomatic\nphrases, and continuous models are able to represent more. As we shall see, the geometry\nof embeddings is strongly influenced by the semantic and syntactic properties of words\n(Mikolov; Sutskever, et al., 2013), while contextual models are able to realize syntactic\nand dependency relations (Hewitt and Manning, 2019).\nW e return to the interaction between language models and NLP in Section 1.8.\n1.2.1 Motivation\nLanguage models naturally arise from the application of thenoisy channel model (Shannon,\n1948) to “recognition” tasks such as ASR, OCR or machine translation (Jurafsky and\nMartin, 2009, p.287, 875). F ollowing Brown et al. (1990), here we derive language modeling\nfrom the latter. The derivation is analogous in the other tasks as well.\nLet us take a F rench-English machine translation system, which takes F rench sentences,\nsuch as “ les enfants et les femmes enceintes ” and translates them into English. In this\nparticular example, we hope to see “ children and pregnant women ” instead of “ pregnant\nchildren and women ”, although the latter mistranslation was actually generated by an\nearly MT system (Hutchins, 1995).\nThe intuition behind the noisy channel model is that all communication happens in\nEnglish; that the words we see are F rench is the result of the “noise” introduced by the\ncommunications channel. The task of machine translation is then to recover the source\nlanguage text S whose distortion most likely resulted in the target language sentence T.\nMore formally , if we have several (source-language) candidate translations, we are looking\nfor ˆS that maximizes P(S|T):\nˆS = arg max\nS\nP(S|T). (1.1)\nUsing Bayes’ theorem, we can rewrite this as\n7This is irrespective of whether the algorithm used to compute the phrases is itself top-down or not.\nThe CYK algorithm (Y ounger, 1967), for instance, is bottom-up.\n14\nˆS = arg max\nS\nP(T|S)P(S)\nP(T) . (1.2)\nSince P(T) does not depend on S, we arrive at the final form by omitting it:\nˆS = arg max\nS\nP(T|S)P(S). (1.3)\nHere P(S), the prior probability , is the language model. The likelihood P(T|S) is\ncalled the translation probability in MT, the acoustic model in ASR, etc; it represents the\ntransformation affected by the noisy channel on S.\nAs we can see, the language model is an integral part of “recognition” tasks. Its\nmain purpose is to ensure that the text we arrive at is indeed a sentence in the source\nlanguage. It is easy to see that while the likelihood is task-specific, the language modeling\ncomponent is universal, and the same model used in an MT system can be easily plugged\ninto a speech or optical character recognizer. This is also the reason why we can study\nlanguage modeling as a separate task.\n1.2.2 Mathematical formulation\nIn this section, we take a closer look at P(S) and present a simple mathematical formu-\nlation for language modeling. Let us assume that we wish to model a sentence S, which\nconsists of N words S = w1w2...w N . Then, our goal is to estimate the probability\nP(S) =P(w1w2...w N ). (1.4)\nWhile the formulation in Equation 1.4 is still too generic to be of practical use, it can\nbe rewritten using the chain rule of probability as\nP(w1w2...w N ) =P(w1)P(w2|w1)...P (wN |w1...w N− 1)\n=\nN∏\ni=1\nP(wi|w1...w i− 1)\n=\nN∏\ni=1\nP(wi|hi),\n(1.5)\nwhere hi = w1...w i− 1 is called the history of word wi. Equation 1.5 essentially rephrases\nthe problem of estimating the probability of a whole sentence into guessing the probability\nof the next word, given all the words that precede it. F or instance, given the sentence\n“Language modeling is ”, a good language model will estimate a high probability\n15\nto continuations like “hard” and “fun”, and close to zero to unrelated tokens such as “ sit”\nand “!” .\nWhile this formulation is mathematically motivated, it also fits well with the linearity\nand sequentiality of human language. Since computation follows the left-to-right (or right-\nto-left, depending on language) direction of writing, it can be used for text generation .\nStarting from a predefined (possibly empty) history , the distribution P(wi|hi) is sampled,\nand the chosen word is appended to the history . The process is continued until a certain\nnumber of iterations pass, or until a special end-of-document token is generated. Mathe-\nmatically speaking, generation is a random process where the output only depends on its\nprevious values, and models that support it are called autoregressive language models.\nF or early language models, such asn-grams, generation was more of a theoretical pos-\nsibility , as the resulting text was of very low quality . However, recent T ransformer-based\nLMs (see Section 1.6) are able to produce texts of close to human-level grammaticality .\nA few examples are given in Appendix B.\nThe unidirectional language model, while both intuitive and useful, is not the only\nway the probability of a word sequence can be computed. A more generic decomposition\nexists:\nP(w1w2...w N ) =\nN∏\ni=1\nP(wi|ci), (1.6)\nwhere ci is the context of wi. The history is one possible context, but not the only one:\n1. In bidirectional models such as BER T (Devlin et al.,2019), the context includes the\nwhole sentence, except the target word;\n2. In skip-gram models, such as w2v (Mikolov; Chen, et al., 2013), the context includes\na number of randomly selected words in a window around the target word;\n3. The syntactic properties of a word, such as dependency relations, can also serve as\ncontext (Dyer et al., 2016).\n1.2.3 T raining\nLanguage modeling is a machine learning task, though it seems to fall between the two\nmain paradigms. On the one hand, it does not require labeled training data, and it is often\ntermed ‘unsupervised’ . On the other hand, it is essentially a sequence labeling problem,\nwhere the ground truth comes from the data itself. Because of this, a more appropriate\nterm would be self-supervised.\n16\nIn order to build a language model, theP(wi|ci) probabilities must be established. Like\nall statistical models, the predictions a language model makes are data-driven, and so the\nprobabilities are learned from data during a training procedure. F or language models,\ndata is natural language text, which is acquired from a training corpus. Once the model\nis trained, it can be used for inference: i.e. computing text probabilities.\nEquation 1.6 serves as the theoretical basis for language model training. Broadly\nspeaking, we maintain a P(wi|ci) table for all word – context pairs. The training process\nvisits every word in the corpus, records its history (or context) and updates the condi-\ntional probabilities accordingly . The exact details are different for each LM technique.\nn-gram models maintain the conditional probability table in memory , and compute the\nprobabilities from global statistics. Most other models encode the estimates implicitly in\ntheir parameter space and update it iteratively based on the local context of each word in\nthe corpus.\nIn practice, the naive process outlined above is intractable. The main culprit is the\nunbounded memory: the prediction of the ith word is conditioned on al l preceding words,\nfrom the beginning of a potentially very long sentence or document8. Under this condition,\nalmost all word occurrences will have a unique context. This is problematic for two reasons.\nFirst, storing very long, unique histories would grow the size of the P(wi|ci) table beyond\nany measure. Second, it leads to the issue of data sparsity: during inference, most words\nwill have previously unseen histories, and so all probability estimates will be zero.\nBecause of this, all language modeling techniques impose a limit on the history taken\ninto account for a particular word. n-grams and attention-based neural models restrict\nthe number of context tokens; recurrent models have a set memory capacity , and only\nremember the – relatively – recent part of the history .\n1.2.4 Evaluation\nThe quality of a language model is assessed on new or held-out data, typically the dedicated\ntest set of the training corpus. Performance is most commonly reported in terms of\nperplexity (PPL) (Jelinek et al., 1977). Given a held-out data of w1w2...w N , perplexity is\ndefined as:\nPerplexity(M) = 2H(P,M ) = 2−\n∑N\ni=1 P(wi) logM(wi), (1.7)\nwhere P is the real data distribution, M is the model distribution and H(P, M ) is the cross\n8Proust’s À la recherche du temps perdustarts and ends with the same word. F or a language model to\ngive a good prediction for the last word, it might have to include in its history all seven volumes of the\nbook. Needless to say , such extremes are rare, but it showcases the problems of unbounded history .\n17\nentropy of the test data given the model. The lower the perplexity is, the closer M is to\nthe “true” data distribution. Unfortunately , the latter is not known, so it is approximated\nwith a degenerate distribution centered on wi:\nP(wj) =\n\n\n\n1, if wj = wi\n0, otherwise.\n(1.8)\nWith this, the cross entropy can be replaced with the average log likelihood of the held-out\ndata. This leads to the formula used to compute perplexity for LMs:\nPerplexity(M) = 2\n∑N\ni=1 log M(wi). (1.9)\nW e note that most language modeling libraries use basee instead of base 2 (nats instead of\nbits as standard in information theory), and the widely used SRILM system (Stolcke et al.,\n2011) uses base 10. Luckily , this choice does not affect perplexity , as the exponentiation\nand the logarithm cancel out.\nPerplexity also has an intuitive meaning. If a model assigns equal probability to 40\nwords, the perplexity will be exactly 40. Consequently , it can be seen as the average\n“branching factor” of a language model; the average number of words it considers at\nany given context. Interestingly , for character-level language models, entropy is usually\nreported instead of perplexity; perhaps because the data compression aspect of LMs is\nmore important there.\nY et for all its intuitiveness and ubiquity , there are signs indicating that perplexity\nmight not be the most accurate measure of LM quality . It has been shown to not correlate\nvery well with downstream (e.g. translation) performance (Goodman, 2001); and while\nthere are no exact measurements, it was found that it might take a perplexity reduction of\nup to 30% to translate into improvements in speech recognition (Rosenfeld,2000). Because\nof this, papers that study language modeling in relation to speech recognition routinely\nreport the word error rate (WER) to assess how the language model benefits the system\nas a whole (Chen et al., 1998). However, for evaluating language models in separation,\nperplexity remains the preferred metric.\n1.2.5 Discrete and continuous methods\nLanguage modeling techniques can be divided into two groups: discrete and continuous\nspace methods. The two differ fundamentally in how words are represented: discrete\nmodels treat words as isolated entities, while continuous models embed them into a vector\nspace, typically Rd.\n18\nThe two approaches utilize radically different branches of mathematics. Continuous\nmodels employ linear operations, whereas discrete methods depend on set operations,\nMarkov processes and on populating (database) tables. Another crucial difference is that\ncontinuous models can impose a distance metric between words, while in discrete space,\nwe can only check for identity .\nHistorically , discrete methods predate continuous ones. In the next sections, we take\na survey of the most important language modeling techniques, in the order of their ap-\npearance.\n1.3 n-grams\nBefore the advent of neural net language models, n-grams were the language modeling\ntechnique of choice for three decades. Nowadays, they are only used as baselines for more\nadvanced methods. Due to the simplicity of the idea, however, n-grams are still taught at\nuniversities9 as an introduction to language modeling (Jurafsky and Martin, n.d., ch. 3).\nn-gram models are a direct approximation of Equation 1.5. Instead of computing the\nprobability of a word given its entire history , an n-gram model truncates the history to\nthe last n − 1 words:\nP(wi|w1w2...w i− 1) ≈ P(wi|wi− n+1wi− n+2...w i− 1) =: P(wi|wi− 1\ni− n+1) (1.10)\nIn other words, wi is assumed to be conditionally independent of the rest of the se-\nquence, given its immediate predecessors. This is called the n − 1-order Markov assump-\ntion, and it helps to make the estimation problem tractable. The choice of n depends on\nthe size of the corpus and of available memory; for a corpus of about 1B words, 6-grams\nalready provide diminishing returns. The other end of the spectrum, when n = 1, is called\nthe unigram model. In unigram models, words are taken to be completely independent of\none another. Since this ignores any text cohesion, unigrams are mainly used for modeling\nbag-of-word content, such as queries in a web search engine.\nA vailable memory imposes a strong limit on the n-gram order. F or a moderate vo-\ncabulary of 30,000 words, there are 30, 0004 = 810 quadrillion possible 4-grams. Luckily ,\nwith finite training data, only a fraction of this number actually manifests: a corpus of 1B\nwords contains at most one billion different 4-grams. While much more manageable, with\nfour 16-bit word ids for the history and 32-bit floating point numbers for the probability ,\nthe P(wi|hi) table still occupies up to 12GB – not negligible even by today’s standards.\n9https://web.stanford.edu/class/cs124/\n19\nGiven the fact that – as we shall see presently – a full n-gram language model includes\nsub-models of all orders from 1 to n, it is evident that high order n-gram language models\ncan use an inordinate amount of memory . F or this reason, trigrams stayed in use until\na few years back (T arján et al., 2016), even though the performance advantage 5-grams\nhold over them had long been proven (Goodman, 2001).\nT able1.2 shows the history taken into account by n-grams of increasing order when\nestimating the probability of the last word in the sentence “ That Sam-I-Am! That Sam-\nI-Am! I do not like that Sam-I-Am! ”10. It can be seen how the Markov assumption\nprevents the model from utilizing the context necessary to predict the right word. The\nhistory of orders 2 through 6 is very generic and contains only function words. It is only\nin the 7-gram model that a content word (incidentally , the one to guess) finally appears.\nUnfortunately , as we have seen, the memory requirements of7-grams make them unusable\nin practice.\nThis loss of context is typical to n-grams and it severely restricts their language mod-\neling performance. It is worth mentioning though that the effect is much less pronounced\nwith the character-level n-grams used in e.g. OCR. As the character vocabulary is much\nsmaller11 it is possible to train character n-gram models of much higher orders.\nOrder Unseen History\n1 That Sam-I-Am I do not like that\n2 That Sam-I-Am I do not like that\n3 That Sam-I-Am I do not like that\n4 That Sam-I-Am I do not like that\n5 That Sam-I-Am I do not like that\n6 That Sam-I-Am I do not like that\n7 That Sam-I-Am I do not like that\nT able 1.2: The history considered by n-grams of various orders for the last word of the\nsentence “That Sam-I-Am! That Sam-I-Am! I do not like that Sam-I-Am! ” . Punctuation\nmarks are omitted for clarity .\n10F romGreen Eggs and Hamby Dr. Seuss. The idea of using this example comes from Jurafsky and\nMartin (2009, ch. 4).\n11Clearly , this argument does not stand for languages with a logographic writing system, such as Chinese.\nIn this work, when referring to character-level language models, we always mean small-vocabulary models,\non the order of a few hundred characters at most. This includes all languages with alphabetic, syllabic,\nmoraic and related scripts.\n20\n1.3.1 T raining\nThe probabilities of n-grams can be estimated from relative frequency counts. Continuing\nwith the example above, we count how many times the history ( do not like that ) occurs\nin the training corpus, and how many of these is followed by the word Sam-I-am:\nP(Sam-I-am|do not like that ) ≈ C(do not like that Sam-I-am )\nC(do not like that ) . (1.11)\nIn the general case, 1.11 becomes\nP(wi|wi− 1\ni− n+1) ≈ C(wi− 1\ni− n+1wi)\nC(wi− 1\ni− n+1) . (1.12)\nEquation 1.12 is called the maximum likelihood estimation (MLE) , because a language\nmodel with n-gram probabilities obtained this way is the most likely to reproduce the\ncounts observed in the training data. Given a large enough training corpus representative\nof the language, MLE is expected to approximate the “real” n-gram probabilities.\n1.3.2 Smoothing\nThe maximum likelihood estimation as described above has several problems due to data\nsparsity . The most glaring issue is thatn-grams not found in the training corpus will have\n0 probability . Consequently , these n-grams will never be produced in generation mode,\nand will be rejected outright during inference, when the model is used in e.g. machine\ntranslation or speech recognition, hurting downstream performance. Even for n-grams\npresent in the training corpus, MLE will yield poor estimates when the counts are small.\nThese issues can be mitigated somewhat with larger training corpora. However, hu-\nman language is a creative process, and no corpus can cover all the possible sentences or\nutterances the model will have to predict. F or this reason, instead of using the maximum\nlikelihood estimates, n-gram models always employ some form of smoothing. This comes\nin three flavors.\nDiscounting methods aim to flatten the probability distribution following a particular\nhistory by taking some of the probability mass from the most frequent continuations and\nredistributing it to the less frequent (or zero-count) words. This ensures that all n-grams\nhave nonzero probabilities, irrespective of whether they occur in the training corpus or\nnot.\nA related technique to address the zero-count problem is using the n-gram “hierarchy” .\nThe idea is that lower order n-grams are less sparse, so we can use them to estimate the\nprobability of missing higher-order n-grams. This can be done in two ways: backoff models\n21\nrecursively fall back to coarser (n−1, n−2...-gram) models when the context of a word was\nnot seen during training, while interpolated models always incorporate the lower orders\ninto the probability estimation:\nP(wi|wi− 1\ni− n+1) =λPMLE (wi|wi− 1\ni− n+1) + (1− λ)P(wi|wi− 1\ni− n+2), (1.13)\nwhere P is the smoothed probability .\nA variety of smoothing models have been proposed over the years: Laplace (addi-\ntive) smoothing (Lidstone, 1920) and Good-T uring discounting (Good,1953) are not very\nefficient by themselves, but the latter is used as a basis for other methods. Katz back-\noff (Katz, 1987), Jelinek-Mercer (Jelinek and Mercer, 1980) and Kneser-Ney (Ney et al.,\n1994) smoothing were the “workhorses” ofn-gram modeling in the 90s. F or higher ordern-\ngrams, the most effective method is the modified Kneser-Ney (KN) smoothing introduced\nin Chen and Goodman ( 1999).\nOut-of-vocabulary (OOV) words (unigrams), i.e. those not seen during training are\ncommonly accounted for by introducing a new token, usually written <unk>. In order to\nget a good estimate for it, occurrences of low-frequency words in the corpus are replaced\n(wholly or partially) with <unk>prior to training. The exact cutoff is corpus- and language\nspecific; for English, 3 or 5 are common choices (Chelba et al., 2014).\n1.3.3 Class-based models\nA major disadvantage of discrete methods, n-grams included, is the lack of a similarity\nmeasure between words. This prevents the model to exploit the semantic clustering of\nwords. F or example, if the training corpus contains phrases like “ lives in London ” or\n“lives in Berlin”, we might expect the model to also give a higher probability to the words\n“Budapest” or “ T okyo” after “ lives in ”, even if those phrases have not been observed.\nHowever, this falls beyond the capabilities of regular n-grams.\nClass-based models, first proposed in Brown et al. (1992), establish connections between\nwords by clustering the vocabulary . This enables the model to condition not (just) on the\nprevious words, but their classes as well. One possible formulation is\nP(wi|wi− 1\ni− n+1) =P(wi|Ci)P(Ci|Ci− 1\ni− n+1), (1.14)\nwhere Ci is the class of the ith word, and Ci|Ci− 1\ni− n+1 is a class n-gram.\nF or the mathematically inclined, this formula provides an interesting parallel to Equa-\ntion 1.10 describing n-grams in general. While n-grams can be thought of as n − 1-order\nMarkov chains, Equation 1.14 clearly defines an n− 1-order Hidden Markov model (HMM).\n22\nNote that while the factorization allows for a compression of the language model (specif-\nically , the class transition probabilities) (Goodman and Gao, 2000), it also introduces a\nbottleneck, since the emission probability of a word is conditioned solely on its class.\nAs the performance of a class-based model depends heavily on the quality of clustering,\na lot of effort has gone into finding the best clusters (Brown et al., 1992; Kneser and Ney,\n1993; Ney et al., 1997; Pereira et al., 1993). Automatic clustering was found to work better\nthan part-of-speech (POS) tags (Niesler et al., 1998). This should come as no surprise, as\nPOS categories are coarser and have a syntactic, and not semantic, function.\nIn general, class-based models by themselves usually perform worse than their word-\nlevel counterparts (Martin et al., 1998; Niesler et al., 1998), while interpolating the two\nis said to reduce perplexity by 5–20% (Emami and Jelinek, 2005; Kneser and Ney, 1993).\nHowever, it is worth mentioning that the improvements were reported over weak bi- and\ntrigram baselines. More thorough work reveals that class-based models fail to bring any\nadvantage over 5-gram (Goodman, 2001).\nLackluster performance notwithstanding, the idea of using semantic relatedness in\nlanguage modeling is well grounded, and as we shall see in Sections 1.4.3 and 1.7, it is one\nof the major advantages continuous models have over their discrete cousins.\n1.3.4 Outlook\nIn this section, we present a bird’s-eye view on the landscape of discrete language modeling\ntechniques beyond n-grams. Some of the methods are extensions to n-gram models; others\nare based on completely different theoretical foundations. Since these methods are not\ncentral to the thesis, we only touch on them briefly; the interested reader may refer to\nthe papers cited below for details, or to Goodman ( 2001) and Rosenfeld (2000) for a more\ncomplete survey .\nSkipping (or skip-gram) models are a simple extension to n-grams, which condition on\na discontinuous history (e.g. P(wi|wi− 3wi− 1)) (Huang et al., 1993; Ney et al., 1994). While\nthe improvement they provide is negligible (Goodman, 2001), the idea makes a comeback\nin Section 1.7.2.\nCache models (Kuhn and De Mori, 1990) aim to rectify the context loss that comes\nfrom the truncated n-gram history by maintaining a cache of recent words. Kuhn and\nDe Mori (1990) reports up to 60% perplexity improvements, and the idea works well with\nany type of language model.\nAn interesting line of research focused on incorporating linguistic information to lan-\nguage models. Moore et al. ( 1995) supplanted a trigram model with syntactic and se-\nmantic features extracted from a lexicon, while Chelba and Jelinek ( 1998) and Charniak\n23\n(2001) used a statistical CFG parser as model. All papers reported 10%-25% perplexity\nimprovements over their respective baselines.\nIn F actored Language Models (FLMs) (Bilmes and Kirchhoff, 2003; Kirchhoff et al.,\n2008), each word is represented by a feature vector comprised of linguistic features, such\nas morphological classes, stems, etc. An n-gram model is then trained over these vectors.\nThe papers also introduce the idea of generalized paral lel backoff, in which a feature- n-\ngram can fall back to a lower order of any subset of its features. Unfortunately , the\nfull backoff model was computationally too expensive to train, and GPB-FLM 3-grams\nfailed to decisively outperform regular word trigrams. On the other hand, the joint LM\nof Filimonov and Harper ( 2009) successfully integrated FLMs and CFG features into a\ndecision tree, and even outperformed 5-gram KN models by a small margin.\nExponential, or Maximum Entropy (ME) models (Darroch and Ratcliff, 1972) are a\nstep towards continuous space language modeling. The generic formula is\nP(w|h) = 1\nZ(h) exp\n(∑\ni\nλifi(w, h )\n)\n, (1.15)\nwhere the features fi are arbitrary functions of w and h (n-grams, skip-grams, etc.), Z(h)\nis a normalizing term and λi are the parameters to train. The main advantages of ME\nmodels are their ability to incorporate various features into the model (Berger et al., 1996;\nDella Pietra et al., 1992; Lau et al., 1993; Rosenfeld, 1994) and their smoothing effect\n(Chen and Rosenfeld, 1999). However, at the time they were computationally intensive to\ntrain (this is no longer true, compared to modern neural language models), and did not\noutperform KN 5-grams consistently .\n1.4 The first neural models\nBy the beginning of the millenium, discrete language modeling has reached its limits:\nno method seemed to provide any improvement over cached Kneser-Ney 5-grams (see\nSection 1.3.4). This prompted very cautious or outright pessimistic paper titles such as\n“A Bit of Progress in Language Modeling” (Goodman, 2001) or “Two decades of Statistical\nLanguage Modeling: Where Do W e Go F rom Here? ” (Rosenfeld, 2000). The solution,\nhowever, came not from the discrete world, but with the introduction of neural (net)\nlanguage models (NNLM) .\n24\n1.4.1 Neural networks\nBefore we delve into the details on NNLMs, a short summary of neural networks is in order.\nReaders familiar with the concepts outlined here can safely skip this section. Those who\nwish to learn more may find numerous books on the subject; we recommend Goodfellow\net al. ( 2016)12.\nArtifical Neural Networks (ANNs) or Neural Networks (NN) for short, are a family\nof machine learning models loosely inspired by biological neural networks. Many neural\nnetwork architectures exist, such as convolutional networks (F ukushima, 1980; Le Cun,\n1989), Boltzmann machines (Ackley et al., 1985; Smolensky, 1986), or recurrent and at-\ntention networks, which we shall introduce later. Figure 1.2 shows the most common\narchitecture, the F eedforward neural network (FFNN)or Multilayer Perceptron (MLP):\nx1\nx2\nx3\nInput\nlayer\nHidden\nlayers\nOutput\nlayer\ny1\ny2\nFigure 1.2: A feedforward network with two hidden layers 13\nThe network is made up of layers, each of which consists of a number of neurons. The\nNN takes its inputs x = [x1x2 · · ·xn] through its input layer, and present its outputs\ny = [y1y2 · · ·ym] in the output layer. Both x and y are real-valued vectors. The neurons\nin the hidden layers perform the following transformation:\ny = f (w⊺x + b) =f (\n∑\ni\nwixi + b). (1.16)\nHere x and y are the inputs and outputs of the neuron, respectively . The input weights\n(w), and the bias term ( b) are parameters of the model, which must be trained. f is\nthe activation function that defines the output of the neuron in terms of its inputs. The\nactivation function is almost always non-linear to allow the network to model non-linear\nproblems (Minsky and Papert, 1969); hence, it is often called “ the nonlinearity” .\n12https://www.deeplearningbook.org/\n13Figure based on https://github.com/PetarV-/TikZ/tree/master/Multilayer%20perceptron\n25\nF or efficiency reasons, modern deep learning libraries, such as Pytorch (Paszke et al.,\n2017, 2019) or T ensorflow (Abadi et al., 2016a), treat the layer as the basic unit instead\nof the neuron. Also, the implementation might split the right side of Equation 1.16 into\ntwo layers: a linear one, which performs the matrix multiplication and a nonlinear layer,\nwhich applies the activation function.\nAlthough the original perceptron (Rosenblatt, 1957) used the Heaviside step function,\ndifferentiable alternatives such as the logistic sigmoid or the hyperbolic tangent functions\nsoon took its place. Recently , the Rectified Linear Unit (ReLU) (Hahnloser et al., 2000;\nJarrett et al., 2009) has gained popularity . Each of these functions have various pros and\ncons: sigmoid functions tend to saturate easily , meaning in most of their domain, they are\nvery flat and their value does not change much when the input does. F or the most part,\nReLUs rectify this problem, but are prone to becoming inactive when x < 0.\nIn regression (real-valued function approximation) problems, the outputs of the net-\nwork, ( yi in Figure 1.2) can be used as is. F or classification tasks, such as language\nmodeling, a final softmax nonlinearity\nσ(yi) = eyi\n∑\nj eyj\n(1.17)\nis applied to the outputs in order to normalize them into a probability distribution. In\nthis case, the yis are called logits.\nIt has been shown that feedforward networks are universal function approximators\n(Hornik et al., 1989). In fact, even a single hidden layer is sufficient to model any function\n(Cybenko, 1989). However, using multiple hidden layers is preferred to a single one for two\nreasons. First, a single layer might require an exponential number of neurons to perform\nthe same task. Second, stacking several layers allow each layer to “specialize” and extract\ndifferent features; later layers typically learn higher level features. This phenomenon is\nespecially well documented in image classification (LeCun et al., 1998; Zeiler and F ergus,\n2014), where convolutional neural networks (CNNs) can be hundreds of layers deep (He\net al., 2016). This gave rise to the current name of the field: deep learning.\n1.4.2 T raining\nDiscrete models, such as n-grams, usually employ global optimization strategies using\ncorpus-level statistics. This is made possible by the fact that their parameter space es-\nsentially coincides with these statistics (such as n-gram frequencies). The parameters, or\nweights θ of continuous models, on the other hand, are a property of the model itself,\nand not derived from the training data. This necessitates a completely different training\n26\nregime.\nThe aim of the training process is to tune the weights of the model, so that it can predict\nthe right output(s) y for each input x. The quality of the prediction is characterized by a\ncost function, which we want to minimize:\nJ(θ) =E(x,y )∼ ˆpdata L(f(x; θ), ˆy), 14 (1.18)\nwhere x is the input for a training example, ˆy and y := f(x; θ) are the target and the\npredicted output, respectively , and L is the per-example loss function. In effect, the cost\nfunction is the averaged loss w.r.t. the empirical distribution ˆpdata.\nThe loss function for classification tasks is typically the cross-entropy loss:\nL(y, ˆy) =−\n∑\ni\nˆyi log y .= − log yi. (1.19)\nThe dotted equation applies when ˆy is a one-hot vector, i.e. it is 1 for the true class ˆyi\nand 0 otherwise (see also Equation 1.8).\nNeural networks are trained via iterative optimization algorithms, such as stochastic\ngradient descent (SGD) (Robbins and Monro, 1951) or one of its variants. Such algorithms\ntry to find a local minimum of J(θ) by iteratively taking steps in the weight space along\nthe negative gradient of J. Regular gradient descent (Cauchy, 1847) computes the real\ngradient on the whole training set. This requires that we store the gradients for each\ntraining example, and is only feasible for very small training sets. Stochastic methods like\nSGD visit a (mini)batch of examples at each step instead, and compute an approximate\ngradient from the batch. The weights are then updated as follows:\nθ := θ − η∇\n(1\nn\nn∑\ni=1\nL(f(x, θ, ˆyi))\n)\n, (1.20)\nwhere n is the size of the minibatch and η is the learning rate, a hyperparameter. The\nupdate in neural networks is effectuated by the backpropagation (backprop) algorithm\n(Rumelhart et al., 1986), which recursively applies the chain rule from last layer to first\nusing dynamic programming techniques for efficiency .\nDuring training, SGD sweeps through the whole training set; one “sweep” is called an\nepoch or iteration. The training lasts either until a specific number of epochs elapse, or\nuntil the cost (measured on the validation set to avoidoverfitting our model to the training\nset) stops decreasing. The latter strategy is called early stopping.\nNeural networks have existed for over six decades; however, they have only become\n14F ormulation from Goodfellow et al. (2016, ch.8)\n27\nmainstream in the last ten years. The reason for this is that, efficient as backpropagation\nis, they are very slow to train. It is the availability of powerful GPUs, actually driven\nby the gaming industry , that finally made deep learning possible. Nowadays, the largest\nmodels run on specialized architectures, such as clusters of data center GPUs15 or Google’s\ntensor processing units (TPUs)16.\nThe outline above is but a very brief summary of how neural networks are trained.\nIn practice, the process is somewhat brittle; specifically , there are no guarantees that it\nfinds a good local minimum. This is because there are many free variables and hyperpa-\nrameters, which all interact in unpredictable ways (Greff et al., 2015). What architecture\nto choose and how big should the model be? What learning rate should to use, what\noptimizer? Should the learning rate be fixed or changed according to a schedule? Even\nseemingly innocuous choices, such as the batch size, can affect performance (author’s own\nexperiments, or e.g. Liu et al. ( 2019)). Because of this, it is considered good practice to\ntry to find the best settings through a hyperparameter search; unfortunately , it is not an\noption for those on a constrained budget. Regularization techniques may also alleviate\nsome of the issues; nevertheless, training a neural network still remains an art to some\ndegree.\n1.4.3 Bengio’s model\nW e can now return to neural language modeling. While a proof-of-concept neural LM had\nexisted before (Xu and Rudnicky, 2000), the first mature model, one that also serves as a\nbasis for today’s architectures, was published in Bengio et al. ( 2003).\nThe model itself is the neural equivalent of n-gram models. When predicting the ith\nword, the model takes the previous n − 1 words as input. At this point, the words are\nrepresented with their index in the vocabulary V . These word ids are then converted into\nword feature vectors (real valued vectors in Rd) via a table lookup in the V × d matrix C.\nThe vectors are concatenated and passed through the hidden FF layer. Finally , a softmax\nlayer is applied to produce the output distribution. The model architecture is depicted in\nFigure 1.3.\nAt its heart, the model is a regular neural classification architecture. The main contri-\nbution of the paper is the look-up table matrix C, which converts words to vectors in Rd.\nThis component is not fixed, but trained together with the rest of the network. Its main\nuse is to embed discrete entities (words) into continuous space; this gives it the name it is\nknown today: embedding. Such a component is a must for all neural language models to\n15https://www.nvidia.com/en-us/data-center/\n16https://en.wikipedia.org/wiki/Tensor_processing_unit\n28\n· · · · · · · · ·\n· · · · · ·\n· · · · · ·\nsoftmax\nmost computation here\ntanh\nMatrix C\nshared parameters\nacross words\nindex forwt−1index forwt−2\nC(wt−2) C(wt−1)C(wt−n+1)\nTable\nlook-up\nin C\nindex forwt−n+1\ni-th output= P(wt = i |context)\nFigure 1.3: Architecture of a neural feed forward language model\nbridge the gap between the discrete nature of the task and the continuous representation\nof the model.\nY et the embedding matrix is also what gives the model most of its power. Because its\nweights are trained along with the rest of the architecture, it can potentially learn to map\nwords that occur frequently in the same context to similar vectors. Bengio et al. ( 2003)\nquotes the two sentences “ The cat is walking in the bedroom ” and “ A dog was running in\na room”, which gives good examples for words with similar semantic and/or grammatical\nrole at every word position (a and the, cat and dog, etc). Embeddings allow the model the\ngeneralize better to previously unseen sentences, as “the presence of only one of the above\nsentences in the training data will increase the probability , not only of that sentence, but\nalso of its combinatorial number of “neighbors” in sentence space” (Bengio et al., 2003).\nExploiting word similarity is not a new idea; Section 1.3.3 already introduced class-\nbased n-grams, an early attempt. However, utilization of word similarity through embed-\ndings is an innate property of all neural LMs, and is exempt from the disadvantages of\nclass-based n-grams such as the emission bottleneck in HMM-style models.\nA 4-gram version of the model was evaluated on the Brown corpus (1.2M tokens) and\nthe AP News corpus (14M tokens). Compared to the best performing n-grams ( 3-gram\non Brown, KN 5-gram on AP), the neural model achieved 20% (312 to 252) and 7% (117\nto 109) perplexity reduction, respectively .\n29\n1.4.4 Performance\nn-grams and neural language models have widely different performance characteristics.\nn-grams can be trained relatively quickly , but due to the curse of dimensionality marring\ndiscrete methods, the models may take up a large amount of memory , potentially on the\norder of |V |n (see Section 1.3). Neural LMs, on the other hand, can be very economic with\ntheir parameter space. Since the embedding matrix is shared among word positions, and\nthe size of hidden layer increases only linearly with n, the model introduced above could\neasily accommodate a longer history than what is possible with n-grams.\nThe power and economy in memory space comes at a price, however. Neural LMs are\ninfamously slow to train. The model above, with very small embedding and hidden layer\nsizes (30 and 50, respectively), was trained for 3 weeks on 40 CPUs. While this particular\nnetwork could be trained today in a few minutes on modern GPUs or TPUs, since recent\nmodels use more complex and larger architectures, training times of days or weeks are still\ncommon.\nAs Figure 1.3 indicates, the main offender is the final softmax layer. The total com-\nputational cost of processing a single training example in the n-gram setup is (in order of\nlayers) O(n × d + (n × d) × h + h × |V |), where d is the embedding dimension and h the\nsize of the hidden layer. Since in language modeling, |V | ≫ d (in Bengio et al. ( 2003),\n|V |is around 16-18,000, while d is 30 or 60), the final softmax term h × |V |dominates\nthe sum. T aking the best performing system in Bengio et al. ( 2003), where |V |= 16000,\nd = 30, n = 5 and h = 50, softmax is responsible for 99% of the total computation and\nmemory cost.\nMuch research has been devoted to develop computationally favorable alternatives to\nsoftmax. Both Noise Contrastive Estimation (NCE) (Gutmann and Hyvärinen, 2010;\nMnih and T eh, 2012) and Importance Sampling (Bengio and Senécal, 2003, 2008) sample\nk random “noise” words for each training example and try to model softmax as a classi-\nfication between true and noise words (Jozefowicz et al., 2016), thereby bringing the cost\ndown from |V | ×h to k × h. Hierarchical softmax (Morin and Bengio, 2005) builds a\ndecision tree based on a hierarchical clustering of V , where each leaf node stores the prob-\nability of a single word, while inner nodes keep track of the probability mass associated\nwith their children. The tree can compute the probability of a word with a single lookup,\nchanging the complexity to log2 |V |. Differentiated Softmax (Chen et al., 2016) is based\non the intuition that we know less about rare words and assigns fewer parameters to them\nthan to their frequent siblings. Implementation-wise, the dense softmax layer is replaced\nwith a sparse diagonal block matrix, each block corresponding to a bucket of words with\nsimilar counts. Finally ,CNN-Softmax (Jozefowicz et al., 2016) does away with the matrix\n30\ncompletely , and computes the output embedding with a character-based convolutional\nnetwork.\nThe methods described above have all been used more or less successfully to train\nneural language models. Several experiments prove that models equipped with importance\nsampling, differentiated and hierarchical softmax attain perplexity scores similar to the\nfull softmax case; NCE and CNN-Softmax, on the other hand, generally perform much\nworse and are not recommended for language modeling (Chen et al., 2016; Jozefowicz\net al., 2016). The speed-up for the sampling methods and hierarchical softmax can fall\nanywhere between 19 (Bengio and Senécal, 2003) and 258 (Morin and Bengio, 2005); in\nreasonably sized recurrent LMs, it is much closer to the former. With differentiated and\nCNN-Softmax, the speed-up is negligible; however, these are the only methods that also\ndecrease the memory usage of the softmax layer.\nAnother line of study attempted to compute the exact loss efficiently instead of ap-\nproximating it (Brébisson and Vincent, 2015; Vincent et al., 2015). While a 3,000-fold\nspeed-up was reported on the spherical softmax loss family , the results could not be applied\nto regular softmax.\nAn early approach meant to circumvent the performance problem by relegating the\nneural LM to augment a mainn-gram model (Schwenk and Gauvain,2005; Schwenk, 2007).\nIn such systems, then-gram would be used for high-frequency words, with the neural model\nas the backoff. Alternatively , the neural LM can be used to rescore the top words predicted\nby the n-gram model. The approximate methods described above, together with advances\nin GPU technology have made such hybrid systems mostly outdated. n-grams and neural\nmodels were still routinely interpolated for a time, but strictly for the perplexity reduction\nachievable in this way .\n1.5 Recurrent neural network language models\nWhile Bengio et al. (2003) showed the potential of continuous space language modeling, it\nwas not until the emergence of recurrent neural network (RNN)language models that such\nmethods became widespread. The first system, based on “vanilla” RNN, was presented by\n(Mikolov, 2010), who continued to develop the concept and related techniques in Mikolov\n(2012), Mikolov; Kombrink, et al. ( 2011), and Mikolov; Deoras, et al. ( 2011). The first\nmodel based on the more advanced LSTM cell was proposed in (Sundermeyer et al.,2012).\nRNN LMs have dominated the language modeling landscape ever since their introduc-\ntion. Only in the last two years have attention-based models (see Section 1.6.1) become\nmature enough to compete with them. While attention-based models will probably emerge\n31\nvictorious, RNN LMs can be regarded as being the first widely successful family of neural\nlanguage models.\n1.5.1 Recurrent neural networks\nThe modern notion of recurrent neural networks and their name was probably17 established\nin (Jordan, 1986; Rumelhart et al., 1985). The idea of recurrence, however, had been\naround much longer, at least since Minsky and Papert ( 1969).\nNeural networks can be thought of as directed graphs, where each node is a neuron\nand activation spreads along the directed edges (see Figure 1.2). In general, a network\ncan be called recurrent, if this graph has cycles in it. In modern RNNs, however, units\nhave recurrent connections only to units in their own layer. The left side of Figure 1.4\nillustrates the idea with a single recurrent unit with input x, output y and inner state h18.\nNote that in the simplest case, y = h.\nx\nh\ny\nxt− 1\nht− 1\nyt− 1\nxt\nht\nyt\nxt+1\nht+1\nyt+1\n... ...\nFigure 1.4: Left: a recurrent unit; right: the same unit unrolled for three timesteps\nThe recurrent connection can be thought of as memory , as it allows the layer to retain\ninformation from previous inputs. Because of this, RNNs lend themselves naturally to\ntemporal or, more generally , sequence modeling. In these tasks, the inputs are part of a\nsequence, and the network has to predict the next item in the sequence based on the items\nseen thus far. This is in stark contrast to non-recurrent networks that processes training\nexamples in isolation. Accordingly , RNNs have been used for tasks such as music com-\nposition (Mozer, 1992), speech recognition (Graves et al., 2013), handwriting recognition\n(Graves and Schmidhuber, 2009), sequence transduction (Graves, 2012), text generation\n(Sutskever et al., 2011) – or indeed, language modeling.\nIn the mathematic formulation, x, h and y are indexed with the time step t ∈ 1...T to\n17It is hard to find definitive evidence as to the origins of RNNs. Even high-profile papers, such as\nHochreiter and Schmidhuber ( 1997), weasel out of the question by omitting citations altogether when\nintroducing the concept.\n18Based on https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg\n32\naccount for the temporal dimension. The formulae below are the recurrent counterpart to\nEquation 1.16 that describes the feedforward neuron:\nht = f (w⊺\nxhxt + w⊺\nhhht− 1 + bh)\nyt = w⊺\nhyht + by\n(1.21)\nAs before, f is the activation function; wxh, whh and why are weights on the various\nconnections; bh and by are bias terms, to be trained along with the weight vectors.\nRNNs are trained with the Backpropagation Through Time (BPTT) algorithm (W er-\nbos, 1988; Williams and Zipser, 1995), which is a more involved version of regular back-\nprop. In effect, the algorithm presents the input sequence (x1, ..., xT ) to the network\none-by-one. The error gradients for all outputs (y1, ..., yT ) are collected, and propagated\nback to the weights of the network. The algorithm is based on an observation by Minsky\nand Papert ( 1969) that for every recurrent network, there exists a feedforward network\nwith identical behavior (if T is finite). The algorithm converts the RNN to a multilayer\nfeedforward network by unrol ling it along the time dimension (as shown on the right\nside of Figure 1.4); the resulting network will thus have one layer for each time step.\nBackpropagation can then be applied to this network as usual.\nAs the length of the modeled sequences can be large, the unrolled feedforward network\ncan be potentially be so deep that it does not fit into (GPU) memory 19. T o make training\nfeasible, it is a ubiquitous practice, to split the sequence into T-sized chunks and treat\neach as a separate training example. The main drawback of this approach, a naive version\nof truncated BPTT (Williams and Peng, 1990), is that temporal dependencies crossing\nchunk boundaries are lost to training. One possible strategy to mitigate this issue is to\nsample the chunk size from a normal distribution centered on T, which makes sure that\nthe chunk boundaries vary between epochs (Merity et al., 2018).\n1.5.2 Gated architectures\nDeep neural networks are difficult to train due to the vanishing gradient problem (Glorot\nand Bengio, 2010; Hochreiter, 1991). Since backpropagation uses the chain rule to compute\nthe gradients for each layer, the error signal diminishes as it is propagated back from the\nlast toward the first layers. This causes the earlier layers to effectively stop training. The\n19F or neural networks, it is always the accelerator (GPU or TPU) memory that counts, for two reasons.\nFirst, large networks can only ever be trained efficiently on accelerators; second, it is a more constrained\nresource than main memory .\n33\nopposite problem, that of exploding gradients, also exists: in this case, the gradients get\nexponentially large, inducing numerical errors in the weight update process.\nThe problem is even more pronounced for recurrent networks, where the layers share\nweights (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997). It has been shown that\nif the norm of the hidden-hidden weigh matrix Whh is less than one, the long term compo-\nnents of the gradient will vanish, preventing the network from learning long dependencies\n(Pascanu et al., 2013).\nThe exploding gradient problem can be easily remedied by clipping the norm of the\ngradients (Mikolov, 2012; Pascanu et al., 2013). F or recurrent networks, the vanishing\ngradient problem is addressed by replacing the “vanilla” RNN with a gated architecture.\nThe Long Short-term Memory (LSTM) cell (Hochreiter and Schmidhuber, 1997) ex-\ntends a standard recurrent unit with multiplicative gates to enforce constant error flow\nthrough the recurrent connection. It introduces another internal state variable, ct, in ad-\ndition to ht, that is responsible for regulating the error flow. The cell architecture and its\nmathematical formulation are depicted in Figure 1.5.\nxt\nyt\nσ\n σ\n tanh\n σ\ntanh\nct− 1\nht− 1 ht\nctft it ˜ct ot\nht\n... ...\nft = σ(Wf ·[ht− 1, x t] +bf ])\nit = σ(Wi ·[ht− 1, x t] +bi])\not = σ(Wo ·[ht− 1, x t] +bo])\n˜ct = tanh(Wc ·[ht− 1, x t] +bc)\nct = ft ∗ct− 1 + it ∗˜ct\nht = ot ∗tanh(ct)\n(1.22)\nFigure 1.5: Structure and equations of the LSTM cell.\nit, ot and ft are the input, output and forget gates, respectively . ft might be the most\nimportant one, as it controls how much information should be retained from the previous\nstate ct− 1. Its bias (bf ) is often initialized to 1.0 to allow free gradient backflow; in layman’s\nterms, the cell starts remembering everything, and it gradually “learns to forget” (Gers\net al., 2000).\nThe main reason why LSTM is less affected by vanishing gradients is the way its inner\nstate is updated. While regular RNNs compute ht by applying a vector product and an\nactivation function to ht− 1 (see Equation 1.21), LSTM computes the state update ˜ct from\nct− 1. The new value of ct is then a linear combination of ct− 1 and ˜ct, allowing the gradient\nupdate to flow through the cell unchanged (Jozefowicz et al., 2015).\nOwing to its novel structure, the LSTM can “remember” much farther back in time\nthan regular RNNs. In an artificial task, LSTMs could recall information about an item\n34\nafter 1,000 time steps, while RNNs trained with BPTT failed after 10 (Hochreiter and\nSchmidhuber, 1997). LSTMs excel on real tasks as well; most of the RNN examples in\nthe last section were realized with LSTM networks.\nThe success of LSTM prompted the appearance of other gated architectures. The GRU\nunit (Cho et al., 2014) is a simpler variant of LSTM, which omits the output gate. There\nwere large-scale experiments to find which components make gated cells effective (Greff\net al., 2015) and to find better architectures with a genetic algorithm (Jozefowicz et al.,\n2015) or reinforcement learning (Zoph and Le, 2017). While these experiments yielded\ninsights into the inner workings of gated cells, no variant was significantly better than\nLSTM or GRU. In particular, LSTM was found to outperform all other cells (including\nGRU) in language modeling (Jozefowicz et al., 2015; Melis et al., 2018).\n1.5.3 Language modeling advances\nIn this section, we examine how RNN-based, especially LSTM-based, systems pushed the\nstate-of-the-art in language modeling. Most of the improvements were due to increasingly\nadvanced machine learning techniques, as well as to our growing familiarity with how\nRNNs should be trained. W e touch on these ideas very briefly; the interested reader is\nreferred to the original papers for details.\nMost of the research focused on modeling one of three corpora, which have become the\nde facto standard benchmarks for RNN LMs. Having standard datasets greatly facilitated\nthe comparability and evolution of different approaches. T able1.3 lists the main attributes\nof each corpus.\nThe Penn T reeBank (PTB) is the smallest of the three corpora. While the original\ncorpus is behind a paywall, Mikolov (2012) has released a preprocessed version into public\ndomain; it is this corpus we shall refer to as “PTB” henceforth. This is a rather small\ndataset at one million tokens, and is heavily preprocessed: numbers and punctuation\nmarks have been omitted and all words have been lowercased. The vocabulary is capped\nat 10,000 word types (i.e. unique tokens).\nThe WikiT ext-2 (WT2)corpus (Merity; Xiong, et al.,2017) consists of curated Wikipedia\npages. It is about twice the size of PTB, and its vocabulary consists of 30,000 words.\nThe One Bil lion W ord Benchmark (1B)is a gigaword corpus released by Chelba et al.\n(2014) to measure progress in language modeling; it consists of about 800 million tokens.\nW ords below 3 occurrences were replaced with <UNK> leaving a vocabulary of 793,471\nword types. Due to its size, few papers took up the challenge, and even fewer attempted\nto model the whole vocabulary , opting instead to cap it at 10 or 100 thousand. This is\nthe only corpus where the sentence order is randomized, preventing models from utilizing\n35\nlong-range dependencies.\nDataset Sentences T okens V ocabulary OOV s Sentence order\nPTB 49,199 1,134,978 10,000 53,299 normal\nWT-2 – 2,506,962 33,254 81,561 normal\n1B 30,607,716 829,250,940 793,471 509,258 shuffled\nT able 1.3: Comparison of the three LM benchmark corpora\nThe PTB and WT2, at 1M and 2.5M tokens respectively , are very small as far as\ncorpora go; (n-gram) language models were already trained on much larger corpora (365M\nin Brown et al. ( 1992) and 284M in Goodman ( 2001)). Models trained on the PTB and\nWT2 are prone to overfit the training data, making the method of regularization the\ndeciding factor in LM performance.\nPenn T reeBank\nT ables1.4 and 1.520 list the most influential systems developed for the PTB and WT2,\nrespectively . In what follows, we only survey single model performance. Early papers\noften report results for ensemble models, which usually outperform the single model by\n5–10 points of perplexity . Later papers omit these, most likely because under a certain\nperplexity threshold, ensembles ceased to provide any discernible benefit.\nThe first two lines in T able 1.4 represent the best discrete baselines on the PTB from\nMikolov (2012); it also illustrates that the real impact of a cache over a state-of-the-art\nn-gram model is much less than the 60% reported by Kuhn and De Mori ( 1990) (see\nalso Section 1.3.4). The same paper introduced the first RNN language model, which\nalready outperformed the baseline by a small margin. Based on a vanilla RNN, it could\nnot utilize the history to its full effect due to the vanishing gradient problem. Mikolov\nand Zweig ( 2012) dealt with the issue by encoding history into a “context vector” using\nLatent Dirichlet Allocation (LDA) (Blei et al., 2003). While this approach achieved a\nlower perplexity score, future systems would instead follow the example of Sundermeyer\net al. ( 2012), who was the first apply LSTM to the problem of language modeling.\nIn experimenting with 2-layer LSTM networks, Zaremba et al. ( 2014) found that only\nsmall (200 cells per layer) models can be trained without regularization. Dropout (Sri-\nvastava et al., 2014), the most successful regularization technique for neural networks,\nhowever, did not work for RNNs at the time. Dropout induces noise in the training pro-\ncess by masking a different part of the network in each training batch, which forces the\n20T ables1.4 and 1.5 are slightly modified and extended versions of those in Merity et al. ( 2018)\n36\nModel Parameters Perplexity\nMikolov (2012) - KN 5-gram 2M 141.2\nMikolov (2012) - KN 5-gram + cache 2M 125.7\nMikolov (2012) - RNN 6M 124.7\nMikolov and Zweig ( 2012) - RNN-LDA 7M 113.7\nMikolov and Zweig ( 2012) - RNN-LDA + KN 5 + cache 9M 92.0\nSundermeyer et al. ( 2012) - LSTM 4.3M? 118.0\nZaremba et al. ( 2014) - LSTM (small) 4.6M 114.5\nZaremba et al. ( 2014) - LSTM (medium) 20M 82.7\nZaremba et al. ( 2014) - LSTM (large) 66M 78.4\nDyer et al. ( 2016) - RNN grammar – 114.5\nGal and Ghahramani ( 2016) - V ariational LSTM (medium) 20M 78.6\nGal and Ghahramani ( 2016) - V ariational LSTM (large) 66M 73.4\nKim et al. ( 2016) - CharCNN 19M 78.9\nMerity; Xiong, et al. ( 2017) - Pointer Sentinal LSTM 21M 70.9\nGrave et al. ( 2017) - LSTM + continuous cache pointer – 72.1\nInan et al. ( 2017)† - V ariational LSTM (medium) + augmented loss 24M 73.2\nInan et al. ( 2017)† - V ariational LSTM (large) + augmented loss 51M 68.5\nZilly et al. ( 2017)† - V ariation RHN 23M 65.4\nZoph and Le ( 2017)† - NAS Cell (medium) 25M 64.0\nZoph and Le ( 2017)† - NAS Cell (large) 54M 62.4\nMelis et al. ( 2018)† - 4-layer skip connection LSTM 24M 58.3\nMerity et al. ( 2018)† - 3-layer A WD-LSTM 24M 57.3\nMerity et al. ( 2018)† - 3-layer A WD-LSTM + continuous cache pointer 24M 52.8\nY ang et al. (2018)† - A WD-LSTM + MoS 22M 54.44\nY ang et al. (2018)† - A WD-LSTM + MoS + dynamic evaluation 22M 47.69\nGong et al. ( 2018)† - A WD-LSTM + cache pointer + FRAGE 24M 51.8\nGong et al. ( 2018)† - A WD-LSTM + MoS + dynamic evaluation + FRAGE 24M 46.54\nT able 1.4: Single model perplexity of various neural (mostly RNN) LMs on the PTB.\nn-gram models are included for reference; †indicates tied input and output embeddings.\nremaining units to learn better representations. Unfortunately , the recurrent connections\nin an RNN amplify this noise, which was found to hurt training.\nZaremba et al. ( 2014) sidestepped this problem by applying dropout on the for-\nward connections between layers only . This allowed them to train LSTM LMs with 650\n(medium) and 1,500 (large) cells per layer, outperforming even the interpolated RNN +\nn-gram model by 15%. Later, Gal and Ghahramani ( 2016) successfully applied dropout\nto the recurrent connections as well, by using the same mask in each time-step 21.\nThe Zaremba et al. ( 2014) model was the first to show the power of LSTM LMs, and\nhas become widely successful. It quickly became the new baseline for language modeling\n21Semeniuta et al. ( 2016) proposed applying dropout on the state update vector ˜ct. Sadly , the network\nused in the experiments was much smaller than those in Gal and Ghahramani ( 2016) and Zaremba et al.\n(2014), preventing direct comparison.\n37\non the PTB; it was also one of the first models to be included in T ensorflow 22. Its main\ndisadvantage (especially in the large configuration) is the number of parameters it uses,\nwhich also negatively affects training times. Different systems came up with different\nanswers to this issue. Kim et al. ( 2016) replaced the embedding with character n-gram\nfeatures (followed by a highway layer (Srivastava et al., 2015) to transform orthographic\nfeatures to semantic vectors), decreasing the number of parameters by 60%. Both Inan\net al. ( 2017) and Press and W olf ( 2017) found that sharing the weight matrix between\nthe input embedding and the softmax layer (also called “output embedding”) not only\ndecreases the number of parameters by a wide margin, but also improves performance by\nincreasing the frequency of updates to the embedding layer. W eight tying has thus become\na common feature in all subsequent systems.\nArchitectural improvements seemingly also yielded considerable perplexity reductions.\nZilly et al. (2017) developed a new network structure by arranging LSTM cells in a many-\nlayer deep highway network. Parallelly , Zoph and Le ( 2017) employed a reinforcement-\nlearning based architecture search to optimized the cell structure itself. Both claimed su-\nperiority over the by then traditional 2-layer LSTM setup. However, it was subsequently\nproven by an independent reevaluation of several architectures with large-scale hyperpa-\nrameter tuning (Melis et al., 2018) that regular LSTM models actually outperform the\nmore modern contenders.\nAn orthogonal line of research, which rekindled the idea of the (discrete) word cache,\nproved more successful. F ollowing the prior work of Graves et al. ( 2014), Vinyals et al.\n(2015), and W eston et al. (2015) on memory augmented networks , both Pointer Sentinel\n(Merity; Xiong, et al., 2017) and continuous cache (Grave et al., 2017) models extend\nthe base LM with a neural memory component. Using the memory as a model of recent\nhistory , both systems outperform their respective base models by 10–30 perplexity points.\nSimilar to its discrete counterpart, the continuous cache requires no training and can be\ncombined with any language model.\nLSTM models also benefited from regularization techniques beyond dropout. Inan\net al. ( 2017) derived an additional loss term, based on a better estimation for the true\ndata distribution 23; the gains were negligible (less than 1 PPL). A WD-LSTM (Merity et\nal., 2018), among many other tricks, applied activation regularization (AR) and tempo-\nral activation regularization (T AR) (Merity; McCann, et al., 2017) to the model. Most\nregularization methods, such as L2, weight decay or dropout, affect the weights; instead,\nAR and T AR penalizes large activation values and large inner state shifts, respectively .\n22https://github.com/tensorflow/models/tree/r1.10.0/tutorials/rnn/ptb\n23T o understand why this is important, refer back to Equation 1.18.\n38\nT ogether, they are responsible for about 3 perplexity points reduction.\nFinally , machine learning experts identified weaknesses in the model outside the central\nRNN component. Gong et al. ( 2018) developed FRAGE, an embedding training method\nthat improves the representation of rare words, achieving a slight perplexity reduction.\nY ang et al. ( 2018) found that the softmax layer presents a bottleneck for a high-rank\nproblems such as language modeling. Replacing it with a mixture of softmaxes (MoS\nin T ables 1.4 and 1.5) yielded substantial improvements. Lastly , Merity et al. ( 2018)\nsupplanted regular SGD withNT-AvSGD, a non-monotonically triggered, averaged version\nof SGD. According to ablation tests, this change in the optimization strategy alone was\nresponsible for a 10% perplexity reduction on the PTB.\nWikiT ext-2\nT able1.5 lists results for systems tested on the WikiT ext-2 corpus. Since its characteristics\nare close to the PTB, the same techniques proved effective there. Consequently , models\nperform similarly relative to each other, though the exact numbers differ from those in\nT able1.4. An interesting observation can be made here: counting only the systems listed\nin both tables, while the perplexity scores start higher in T able 1.5, they also end lower,\nresulting in a 55% improvement on Inan et al. (2017), as opposed to the 32% on the PTB.\nT o our knowledge, this discrepancy has, as of yet, not been addressed by any study .\nModel Parameters Perplexity\nInan et al. ( 2017) - V ariation LSTM 28M 87.7\nInan et al. ( 2017) - V ariation LSTM + augmented loss 58M 87.0\nGrave et al. ( 2017) - LSTM + continuous cache pointer – 68.9\nMelis et al. ( 2018) - 1-layer LSTM 24M 65.9\nMerity et al. ( 2018) - 3-layer A WD-LSTM 33M 65.8\nMerity et al. ( 2018) - 3-layer A WD-LSTM + continuous cache pointer 33M 52.0\nY ang et al. (2018) - A WD-LSTM + MoS 35M 61.45\nY ang et al. (2018) - A WD-LSTM + MoS + dynamic evaluation 35M 40.68\nGong et al. ( 2018) - A WD-LSTM + cache pointer + FRAGE 33M 49.3\nGong et al. ( 2018) - A WD-LSTM + MoS + dynamic evaluation + FRAGE 35M 39.14\nT able 1.5: Single model perplexity of various LSTM LMs on WikiT ext-2.\nBefore turning to the 1B corpus, let us make a final remark about the accuracy of\nthese results. W e have seen that common datasets facilitate comparison; however, naively\nsorting systems by the published numbers may lead to invalid observations. Most systems\nuse different code bases, are implemented with different deep learning libraries, and en-\njoy different levels of hyperparameter tuning. Without a thorough reevaluation (such as\nperformed in Melis et al. ( 2018)), any survey is inevitably on a best-effort basis.\n39\nLuckily , most of the development in the field is open source, making such reevaluations\npossible. Authors customarily upload the code associated with their paper to GitHub or\nother software hosting sites. Reproducibility , however, can still be a problem. F or instance,\nA WD-LSTM was originally implemented in PyT orch version 0.1.12. When adapted to the\nchanges in PyT orch 0.4 or later, the reported perplexity numbers could no longer be repli-\ncated. As far as we know, no one has yet made the endeavor to run a full hyperparameter\nsearch to ensure that the model reaches its previous performance.\nOne Billion W ord\nModel Parameters Perplexity\nChelba et al. ( 2014) KN 5-gram 1.76B 67.6\nChelba et al. ( 2014) RNN-1024 + ME 9-gram features 20B 51.3\nJozefowicz et al. ( 2016) LSTM-1024-512 0.82B 48.2\nJozefowicz et al. ( 2016) LSTM-2048-512 0.83B 43.7\nJozefowicz et al. ( 2016) 2-layer LSTM-8192-1024 (Big) 1.8B 30.6\nJozefowicz et al. ( 2016) Big LSTM + CNN inputs 1.04B 30.0\nJozefowicz et al. ( 2016) Big LSTM + CNN inputs + CNN Softmax 0.39B 35.8\nJozefowicz et al. ( 2016) Big LSTM + CNN inputs + char LSTM 0.23B 47.9\nKuchaiev and Ginsburg ( 2017) Big LSTM G-4 1.75B? 28.17\nKuchaiev and Ginsburg ( 2017) Big LSTM G-4 (2 weeks) 1.75B? 24.29\nY ang et al. (2018)* 2-layer LSTM 119M 42.77\nY ang et al. (2018)* 2-layer LSTM + MoS 113M 37.1\nT able 1.6: Single model perplexity of RNN LMs on the One Billion W ord (1B) benchmark.\nThe systems marked with an asterisk ( *) are limited to the top 100k words.\nLanguage modeling results for the One Billion W ord benchmark are presented in T a-\nble 1.6. As before, the first block represent the best n-gram model and a mixed RNN–\nmaximum entropy model by (Chelba et al., 2014). The latter shows clearly how the curse\nof dimensionality affects discrete models: the number of parameters increased tenfold from\nthe 5-gram model.\nThe second block reports results for LSTM LMs that model the whole vocabulary .\nSince the corpus is huge, regularization is not such a burning issue as with the other two\ndatasets. Jozefowicz et al. ( 2016) essentially experimented with inflated versions of the\noriginal (Zaremba et al., 2014) model; the largest LSTM architecture contains 8192 hidden\nunits per layer. The models deviate from the Zaremba ones in one respect: a projection\nlayer (Sak et al., 2014) is added before the softmax layer to keep the number of parameters\nin check. (This is the second number in the table: 2048–512 means hidden layer(s) with\n2048 cells and a projection layer with 512 outputs.) After a week of training on 32 GPUs,\n40\nthe largest models achieve perplexity scores around 30 – a 42% improvement over the best\ndiscrete model, and 55% over the best n-gram.\nThe main challenge on 1B is that of scaling: both the corpus and its vocabulary is\nhuge, so computation and memory cost of the models must be controlled.\nJozefowicz et al. (2016) focused on making the input embedding and the softmax layer\nmore efficient. Computational cost of the latter was addressed by using importance sam-\npling instead of raw softmax. On the memory front, several techniques were tested. The\nmodel with “CNN inputs” reuses the idea of Kim et al. (2016) and replaces the embedding\nwith character convolutions. This variant proved to be the best model, outperforming the\nraw LSTM model by a small margin. In contrast, CNN Softmax hurts language modeling\nperformance, albeit at 35.8 PPL it still improves on the 5-gram model by 41%, while using\n78% less parameters. Another variant predicted output words with a character LSTM.\nAlthough it needed the fewest parameters, it performed much worse than CNN Softmax.\nAnother way of attacking the performance problem is to make the LSTM network\nfaster. Kuchaiev and Ginsburg ( 2017) developed a method to factor the weight matrix\nof an LSTM layer into smaller matrices, and the state and input vectors into 4 groups,\ndecreasing the number of parameters and increasing parallelism. The resulting speed-up\nallowed the model to train faster, thereby achieving 2 points lower perplexity in the same\ntimeframe as the previous best model.\nThe last two lines in the table show that the Mixture of Softmaxes approach is beneficial\non 1B as well. However, the results are not comparable with the others, as only a fraction\nof the vocabulary was modeled.\n1.6 T ransformer-based language models\nWhile RNNs had the language modeling stage to themselves for a good 6 years, they\nhave recently been overshadowed by T ransformer-based LMs. This section reviews what\nT ransformers are, where they came from and how they are used in language modeling.\n1.6.1 Neural machine translation\nThe origins of the T ransformer can be traced back to the field of neural machine trans-\nlation (NMT) . The history of MT starts shortly after the invention of the (electronic)\ncomputer (Dostert, 1955). By the end of the noughties, commercial grade systems, such\nas Google T ranslate, were widely in use, and research was carried out in both rule-based\nand statistical (classical machine learning) directions: see Apertium (Corbı́-Bellot et al.,\n41\n2005; F orcada et al., 2011) for the former, and Moses (Koehn et al., 2007) for the latter.\nThe first end-to-end NMT system was only introduced in 2014.\nIn his seminal paper, Sutskever et al. ( 2014) introduced the sequence to sequence\n(seq2seq) model, which is a generic framework for sequence learning. It consists of two\ncomponents: an encoder and a decoder, both of which are LSTM networks; for machine\ntranslation in particular, both are LSTM language models. The encoder consumes the\ninput sequence and maps it to a vector of fixed length. The decoder’s hidden state is\ninitialized from this vector, and it is run in generation mode to predict the output 24. The\nsystem achieved close to state-of-the-art results. Figure 1.6 illustrates its application in\nEnglish–F rench translation25.\nFigure 1.6: Sequence to sequence architecture at work\nThe main drawback of the seq2seq architecture is the bottleneck presented by the fixed\nlength vector output by the encoder. In most cases, its capacity is not enough to represent\nall necessary information in the source sequence. The attention mechanism (Bahdanau\net al., 2015; Luong et al., 2015b) alleviates this issue by allowing the decoder to access\nthe hidden states of the encoder at each time step. This enables the decoder to find which\nsource words are most relevant for predicting a particular output word. In effect, the\nmodel learns a (soft) alignment between the input and output sequences.\n(Hard) word- (Brown et al., 1993) and phrase-based (Och et al., 1999) alignment has\nbeen an important part of the classical MT toolbox since IBM System 1. It is basically a\nbipartite graph, in which source and target-language words (phrases) are the nodes and\nan edge between two means that they are (part of the) translations of each other. Soft\nalignment is based on the same principles, but the alignments an output word participates\nin are represented by a probability distribution on the whole input sentence. In other\nwords, an output word may attend to any number of input words to different degrees.\nFigure 1.7 illustrates the idea with a per-row heat map. Brighter squares indicate a higher\nlevel of attention26.\n24W e do not go into details in this section, as it is only tangentially related to our main topic.\n25Image taken from https://d2l.ai/chapter_recurrent-modern/seq2seq.html\n26Heat map generated with https://git.io/JfITo\n42\nFigure 1.7: Soft attention plot for an English–Spanish sentence pair\nAttention proved very effective in machine translation; the first system already out-\nperformed Moses when <unk> tokens were disallowed (Bahdanau et al., 2015) and Luong\net al. ( 2015a) established state-of-the-art results on several MT benchmarks.\n1.6.2 The T ransformer\nThe T ransformer is a machine translation model that was introduced in the seminal paper\n“Attention is al l you need ” (V aswani et al., 2017). It is an encoder-decoder architecture\nthat does away with the RNN components and rely solely on the attention mechanism to\nmodel the input and output sequences, as well as their relation to each other.\nFigure 1.8: A single-layer T ransformer, encoder on the left.\nBoth the encoder and the decoder consist of identical layers stacked on one another. An\n43\nencoder layer contains two sublayers: a FF component and a self-attention layer in which\na position (word) attends to all positions in the output of the previous layer. The decoder\nlayer is similar in structure, with two differences. First, it contains an encoder-decoder\nattention sublayer; this corresponds to the regular attention component in Bahdanau et al.\n(2015). Second, in the self-attention layer, words can only attend on positions on their\nleft to make output generation possible. The original model had 6 encoder and decoder\nlayers, but this number is not set in stone; Figure 1.8 portrays a single-layer model.\nT wo details of the architecture are worth mentioning. One is that T ransformer uses\nmulti-head attention in all layers. The multi-head attention mechanism builds several (one\nper “head”) attention distribution for each word position, potentially drawing information\nfrom different representation subspaces (V aswani et al., 2017). It can provide a richer\nrepresentation than regular attention, though it was later found that many of the heads\nin the encoder can be pruned (V oita et al., 2019). The other detail is the augmentation\nof the regular embeddings with positional encodings. Since the architecture contains no\nrecurrence, the temporal information necessary to model sequences is injected by adding\nto the embedding of each word the value of a sinusoid function of the word position.\nThe T ransformer, at the very beginning, outperformed all previous single models on\nthe two MT datasets it was tested on, even including all ensembles of earlier models on\none.\n1.6.3 T ransformers in language modeling\nW e have seen how LSTM language models inspired neural machine translation, which in\nturn gave birth to the T ransformer. It was only a matter of time until things went full\ncircle and the first T ransformer-based language models appeared.\nT ransformer LMs come in two forms, depending on the layer type used. Regular left-to-\nright language models are built on a variant of the decoder, which has the encoder-decoder\nattention sublayer removed (Liu et al., 2018). Bidirectional models use the encoder. Here\nwe review the most successful autoregressive LMs; bidirectional models are described in\nSection 1.7.4.\nSadly , it is impossible to give a similarly detailed overview of T ransformer language\nmodels as we did for RNNs in Section 1.5.3. As we shall see, the models are very large, and\nrequire huge amounts of data to train. The lack of large, openly available preprocessed\ncorpora, however, prompted every research group to assemble its own corpus. This pre-\ncludes meaningful comparison between the models, and the fact that their creators refrain\nfrom publishing their datasets or results on standard corpora makes attempts at repro-\nducibility more or less futile and T able 1.7, which lists the most important T ransformer\n44\nLMs, grievously incomplete.\nThe purpose of language modeling has changed as well. Previously , quantitative evalu-\nation based on perplexity was the norm. Qualitatively , text generated by the models often\nleft a lot to be desired. T ransformer models, on the other hand, can generate much more\nconsistent and syntactically (though not semantically) flawless texts. While this makes\nquantitative evaluation less relevant, it is not clear yet how these close-to-human-level\nmodels should be assessed.\nEarly decoder-based T ransformer LMs were used for various tasks, such as document\nsummarization (Liu et al., 2018) and generative pretraining for high-level NLP tasks (Rad-\nford et al., 2018). F or these early systems, language modeling was but a tool, and not the\ngoal; yet they laid the foundation for others to build on.\nThe first language model that showed the power of the T ransformer approach was\nGPT-2 (Radford et al., 2019). It was trained on 40GB of web text, and made available\nin four configurations, with parameter budgets from 117M to 1.5B. The generated texts,\nespecially for the large model, are qualitatively barely distinguishable from those written\nby humans27. The model achieves state-of-the-art performance on several LM benchmarks,\nwith 1B being the notable exception (see T able1.7). This may be justified by the fact that\nT ransformer language models are typically trained with full documents in order to take\nadvantage of long-term dependencies – something which the sentence-shuffled 1B lacks.\nThe CTRL model (Keskar et al., 2019) is similar to GPT-2, but it conditions the\nlanguage model on control codes that can be used to dictate the style, topic and task-\nspecific behavior (e.g. question answering) of the generated text. The mechanism works\nwell for the most part, but abrupt style shifts do happen, and the control codes are\nhaphazard, obviously based on what datasets were at hand (Wikipedia, MT tasks, etc.).\nModel Corpus Parameters PTB WT2 1B\nGPT (Radford et al., 2018) Books (1B words) 110M – – –\nGPT-2 (Radford et al., 2019) W eb scrape (40GB)\n117M 65.85 29.41 75.20\n345M 47.33 22.76 55.72\n762M 40.31 19.93 44.58\n1.5B 35.76 18.34 42.16\nCTRL W eb scrape (140GB) 1.63B – – –\nT ransformer XL (Dai et al., 2019) Same as evaluation 24M–0.8B 54.52 – 21.8\nT able 1.7: Performance of T ransformer language models on standard corpora.\n27So much so, that OpenAI only published the smallest model at first “ due to concerns about large\nlanguage models being used to generate deceptive, biased, or abusive language at scale” (Radford; W u;\nAmodei, et al., n.d.). Only when other large models became available did they decide on releasing the\nrest of the models.\n45\nBoth RNN and T ransformer language models are trained by chunking the corpus into\nsegments and processing a single segment in a training step. However, the recurrent hidden\nstates of RNNs are preserved between segments, allowing them to retain information from\nprevious steps as much as their capacity permits. Since T ransformers have no recurrent\nstate, they cannot learn dependencies that span segments. T ransformer XL (Dai et al.,\n2019) overcomes this issue by allowing the model to look back (but not backpropagate to)\nthe latest preceding segment, introducing a limited form of recurrence.\nT ransformer-XL is unique among the rest of the models in that it was trained and\nevaluated on the benchmark datasets instead of a proprietary corpus, allowing a direct\ncomparison with RNN methods. Notably , it achieved state-of-the-art performance on all\ndatasets.\n1.6.4 Performance considerations\nThe T ransformer architecture promised performance improvements over RNNs, at least\nas far as training speed is concerned. The speed-up comes from the lack of recurrent\nconnections and the fact that all words are processed in parallel. The increased efficiency ,\nhowever, is offset by the sheer size of modern T ransformer models.\nThere is one aspect in which RNNs actually outperform T ransformers: evaluation or\ntext generation. At each time step, the RNN reads a single input word and predicts the\nnext word (see e.g. the right side of Figure 1.6). This is possible because the RNN can\n“remember” the input history via its recurrent state. The T ransformer, which lacks such\na recurrent state, has to re-read the whole history sequence at each time step.\nY et the main bottleneck for attention-based models is memory . While RNNs run in\nmemory linear in the sequence size n, attention uses O(n2). When training a T ransformer\nmodel on the same GPU as an RNN with the same parameter budget, either the sequence\nsize or the batch size (or both) must be decreased to fit into memory . The first choice\nsacrifices model performance; the second leads to longer training times. Most models\nabove or in Section 1.7.4 have many times the parameters of a standard LSTM model and\nare also trained on much larger corpora (see T able 1.7). These factors all add up, and as\na result, training a modern T ransformer model takes days or weeks on hundreds of GPUs\nor TPUs.\nThe hardware requirements and the length of the training cycle have left their mark\non research culture as well. The financial costs incurred by such a training regimen are\nprohibitive for smaller laboratories, preventing them from participating in state-of-the-\nart research. T o counter this ‘big science’ effect, it has become customary to release\nthe models along with the paper introducing them, allowing less well endowed research\n46\ngroups to use or experiment with them. However, this does not change the fact that the\nleaderboards of shared tasks are dominated by huge T ransformer models created by large\norganizations (usually companies) (Rogers, 2019), and that the unequal playing field raises\nvarious ethical issues (Parra Escartı́n et al., 2017).\nIt is important to note that there already exist solutions to the technical issues raised\nabove. The limited recurrence introduced by T ransformer XL puts it on par with RNNs\nfor evaluation speed, achieving a huge (uup to 1,800x) speed-up compared to regular\nT ransformer models (Dai et al., 2019). The quadratic memory usage of the attention\nmechanism has also been addressed recently . The Reformer architecture uses locality-\nsensitive hashing to bring the memory footprint down to O(n log n) (Kitaev et al., 2020).\nThese solutions are not yet widespread, and it will be interesting to see how they affect\nthe T ransformer landscape.\nThere is one area where T ransformer models improve on memory usage and compu-\ntation cost compared to RNNs: their handling of the embedding and softmax layers. In\nRNN language models, the computational cost was addressed by approximate methods\n(see Section 1.4.4). T ransformers mitigate the problem using a linguistically motivated\napproach. Since the size of the softmax layer is proportional to that of the vocabulary ,\ndecreasing the latter will keep the former small as well. An obvious solution would be to\nswitch to character-based language modeling, where the vocabulary consists of a few hun-\ndred characters at most; however, their performance leaves a lot to be desired (Radford\net al., 2019).\nSubword- or wordpiece-level vocabularies, generated by algorithms such as W ordPiece-\nModel (Schuster and Nakajima, 2012), Byte Pair Encoding (BPE) (Sennrich et al., 2016)\nor the confusingly named unigram language model (Kudo, 2018), represent a reasonable\ncompromise between character- and word-level approaches. They include the most fre-\nquent character sequences in the training data, from individual characters to the most\ncommon words. Inputs to the language model are segmented into subword tokens accord-\ning to the vocabulary; prediction is also done at the subword level, although perplexity\nscores are always normalized with the word count. Most models have a vocabulary of\nabout 30,000 subwords.\nOf course, subword vocabularies are not T ransformer-specific, and can be used with\nany language modeling technique. However, it was T ransformer LMs that made them\nubiquitous. Y et their effect on the modeling performance has not been studied in much\ndetail: the author is not aware of any work that evaluates word- against subword-level\nvocabulary with the same T ransformer (or RNN) model.\n47\n1.7 Embeddings\nW e have seen how embeddings are an integral (and mandatory) part of neural language\nmodels (see Section 1.4.3. Much of the performance of a model depends on the quality\nof vector representations learned by the embedding. In this section, we explore why\nembeddings work and how they became a household term in NLP .\n1.7.1 V ector space semantics\nIn the seminal paper dedicated to this topic, Mikolov; Yih, et al. ( 2013) discovered that\nembedding vectors encode meaningful syntactic and semantic information about words.\nThe language model uses this information to make its predictions.\nEmbeddings map words to vectors in Rd, which is an inner product space. It defines\nthree vector operations: addition, scalar multiplication and dot product. It is not at all\nobvious that this should be the case, but all three operations have a linguistic interpreta-\ntion:\nSimilarity Embeddings assign similar words to similar vectors (Bengio et al., 2003).\nNormalized dot product (i.e. the cosine of the angle between two vectors) is therefore\na good measure of word similarity .\nRelatedness Syntactic and semantic relationships seem to be encoded as approximately\nconstant vector offsets between word pairs sharing a relation (Mikolov; Yih, et al.,\n2013). Figure 1.9 illustrates a semantic (gender) and a syntactic (plural) relation 28.\nF requency Since the length of a vector seems proportional to the logarithm of the word\nfrequency (Arora et al., 2016), scalar multiplication or normalization of a vector does\nnot change its semantics. This property ensures that similarity and relatedness can\nbe computed as described above.\nThe properties above make it possible to treat more advanced concepts, such asanalogy,\nin terms of vector operations as well. The question “What is to king as woman is to man?”\ncan be formalized as vking − vman + vwoman, which (given a large enough training corpus)\nyields a vector close to vqueen. Syntactic analogies (e.g. vapple : vorange ≈ vapples : voranges)\nwork similarly . Later studies showed that embeddings capture all kinds of linguistic in-\nformation about words from POS category (Borbély; Kornai, et al., 2016) to sentiment or\nconcreteness (Rothe et al., 2016).\n28Figure reproduced from Mikolov; Yih, et al. ( 2013).\n48\nMAN\nWOMAN\nUNCLE\nKING\nQUEEN\nAUNT\nKINGS\nQUEENS\nKING\nQUEEN\nFigure 1.9: Left panel shows vector offsets for the “gender” relation. Right panel shows\ntwo relations: “gender” and (the grammatical) “number” .\nAs embeddings use real vectors for word representation, they belong to thevector space\nmodels of semantics . Another name for this field is distributional semantics , because it\nis based on the distributional hypothesis (Firth, 1957; Harris, 1954), which asserts that\nsimilar words occur in similar contexts. Since language models use the context to predict\nthe next word, the quality of representations learnt this way is a strong argument in favor\nof the hypothesis.\nDistributional semantics is not a new concept. W ord frequency count-based methods,\nsuch as Latent Semantic Analysis (LSA) (Deerwester et al., 1990) or Pointwise Mutual\nInformation (PMI) (Church and Hanks, 1990) have been around for decades. There are\nconflicting assessments about whether embeddings clearly outperform (Baroni et al.,2014),\nor are mostly on par with (Levy et al., 2015), count-based models; there is some evidence\nthat both optimize the same objective (Levy and Goldberg, 2014b). However, embeddings\nclearly improve on earlier methods in two regards. First, they capture syntactic and mor-\nphological regularities in addition to semantics. Second, they are much lower-dimensional,\ntypically d =300–1000, as opposed to O(|V |) in the count-based case, allowing them to be\nused outside of research environments.\nEmbeddings can be evaluated in two ways. Intrinsic evaluation assesses the learned\nrepresentations on similarity (such as SimLex-999 (Hill et al., 2014)) or analogy (see e.g.\nWS-353 (Finkelstein et al., 2002) or the Google analogy dataset (Mikolov; Yih, et al.,\n2013)) tasks. Extrinsic evaluation measures what performance improvements an embed-\nding brings to downstream tasks (see Section 1.7.5). Unfortunately , there seems to be\nlittle correlation between the two approaches (F aruqui et al., 2016). While earlier work\nconcentrated on intrinsic benchmarks, the focus has recently shifted to extrinsic evalua-\ntion.\nThe vector space properties were first demonstrated for embeddings of RNN language\n49\nmodels. However, their performance is usually substandard, due to the lack of a right-\nhand side context and the generic LM objective (Mikolov; Chen, et al.,2013). Stand-alone\nembeddings algorithms, better suited to representation learning, soon appeared. They\nbelong to two main groups: static embeddings assign a single vector to a word type; i.e. all\noccurrences of a word are mapped to the same vector;dynamic (contextualized) embeddings\ncompute a different vector for each occurrence based on its immediate context.\n1.7.2 Static embeddings\nThe first general purpose embedding,word2vec (w2v) was published in the seminal Mikolov;\nChen, et al. (2013), although a similar method has already been described in Collobert and\nW eston (2008). W ord2vec is trained with one of two language modeling objectives based\non skip-grams. F our words are selected in a local context window of k positions around\nthe current word, and either the current word has to be predicted based on the other four,\nor conversely . The architecture is based on the feedforward NNLM in Section 1.4.3, but\nwithout the hidden layer. W ord2vec significantly outperforms embeddings extracted from\nearly RNN models, and it is very fast to train (Mikolov; Chen, et al., 2013). It has become\nthe first deep learning success story in NLP , even though its NN architecture is anything\nbut deep.\nW ord2vec was followed by other embedding algorithms, all of which tried to improve on\nit in various ways. GloV e(Pennington et al., 2014) mixes local context window methods\nwith global corpus statistics; Levy and Goldberg ( 2014a) eschews linear context in favor\nof a syntactic one based on dependency parse-trees. F astT ext (Bojanowski et al., 2017)\nrepresents words as a bag of character n-grams. Y et performance-wise, neither method\nis superior to the others: some of them are better at syntactic, others at semantic tasks;\nsome at word similarity , others at relatedness (Levy et al., 2015). Nowadays, F astT ext is\nusually preferred, because of its ability to model out-of-vocabulary words.\nBesides OOV s, embeddings algorithms have problems learning good representations\nfor rare words as well. The neighbors of infrequent words tend to be other, unrelated\nrare words, with actual semantic neighbors placed far away (Gong et al., 2018); their\nneighborhoods are also less stable across different training runs (W endlandt et al., 2018).\nThere are attempts to address this issue directly , such as FRAGE (Gong et al.,2018), but\ndue to this instability , hyperparameter choice is as important as ever (Levy et al., 2015;\nMimno and Thompson, 2017).\n50\n1.7.3 Multi-sense embeddings\nThe embeddings discussed thus far represent each word with a single vector. This is\nproblematic for words with multiple meanings, such as bank ‘financial institute’ and ‘river\nbank’ . Bank is a homonym, i.e. its senses are unconnected, and it is only a coincidence\nthat they share the same word form – evidenced by the fact that other languages do\nnot partition these meanings to the same character sequence (Y oun et al., 2016). The\nalgorithm, however, tries to consolidate the senses into one, ending up with a vector that\nis a blend of the different meanings. Polysemes such as head (‘body part’, ‘top part’,\n‘leader’), where the senses are closely related, are perhaps less affected, as we expect their\nvectors to be close as well. Y et there is a continuum of words between homonymy and\npolysemy , and for most, the meanings (or usage patterns) are separate enough to warrant\ndistinct vectors.\nMulti-sense embeddings (MSEs) , proposed by Reisinger and Mooney ( 2010), model\ndifferent meanings of word forms with different vectors. Several algorithms have been\nproposed over the years using techniques such as spherical clustering (Huang et al., 2012),\nstochastic modeling (Li and Jurafsky, 2015), or multi-sense extensions to w2v (Bartunov\net al., 2016; Neelakantan et al., 2014). Some of these are simpler, assigning a fix number\nof senses for lexically ambiguous words, others try to find the optimal number statisti-\ncally . All of them report improvements over single-sense embeddings, though the gains\nseem surprisingly small even on datasets created specifically to test MSEs, such Stanford\nContextual W ord Similarities (Huang et al., 2012).\nAssessing MSE performance is a complex matter; we return to in Chapter 3.\n1.7.4 Contextual word embeddings\nThe main drawback of static embeddings is that a word is represented by the same vector\nregardless of context. Even current MSEs have to choose between a number of precom-\nputed sense vectors for each occurrence of the word. As such, when encountering a word in\na specific sentence, static embeddings cannot accurately reflect the syntactic or semantic\nrole it plays in its current context.\nIn contextual embeddings, such as ELMo (Peters et al., 2018) or BER T (Devlin et al.,\n2019), the vector of a word depends on its immediate surroundings as well. Accordingly ,\neach occurrence of a word is assigned a different vector, which is then able to implicitly\nencode the function of the word in the sentence.\nW e have seen how static embeddings came to be by “shedding” their language modeling\nskin and keeping only the embedding matrix. Contextual embeddings mark the return of\n51\nthe full language modeling machinery; in effect, a contextual embedding is nothing more\nthan a (usually bidirectional) language model, where the word representation is taken from\nthe output of the network (or specific hidden layers). The input is usually a full sentence.\nThis allows the model to adapt the word representations to the context; hence the name.\nThis is in contrast to static embeddings, where the vector is the output of the embedding\nlayer, which operates on a per-word basis.\nContextual embeddings are pretrained on very large corpora upwards of 1B (One Billion\nW ord (Chelba et al.,2014)) or 2,5B (English Wikipedia29) tokens all the way up to 2.5TB\nof filtered Common Crawl data (Conneau and Lample, 2019). They are evaluated on\nhigh-level downstream tasks, such as question answering, textual entailment or named\nentity recognition; for contextual embeddings, the idea of intrinsic evaluation has been\ncompletely abandoned.\nThe first contextual embedding, ELMo (Embeddings from Language Models , Peters\net al. (2018)) employs a single-layer bidirectional LSTM (i.e. a left-to-right and a right-to-\nleft sublayer); the word representation is the outputs of the two sublayers concatenated.\nELMo can be used as a drop-in replacement for static embeddings in NLP systems; such\nsystems attained state-of-the-art results on SQuAD and four other high-level tasks.\nBesides English, ELMo models have been released for another 43 languages (Che et al.,\n2018). However, the training data for each language was limited (20M words apiece), so\nthe models are not expected to perform as well as ‘regular’ ELMo.\nKeeping with the Sesame Street 30 theme, ELMo was soon followed by BER T (Bidi-\nrectional Encoder Representations from T ransformers, Devlin et al. ( 2019)). As its name\nimplies, it repurposes the T ransformer encoder as a generic language representation model.\nIt is pretrained on two tasks. In masked language modeling, the system is presented with\na single sentence in which some words are masked. The model has to guess these words 31.\nThis is analogous to the traditional LM objective of guessing the next word; however,\nthe parallel processing of words in the T ransformer encoder makes standard left-to-right\nconditioning impossible. In the next sentence prediction task, BER T is presented with two\nsentences, and has to decide whether the second sentence follows the first in the training\ncorpus.\nThe model can be adapted to downstream tasks by adding a classifier on top. Even\nwith a simple feed-forward network as the classifier, BER T was able to achieve state-of-\nthe-art performance on several high-level benchmarks, such as SQuAD or GLUE (Devlin\net al., 2019). The model is fine-tuned separately for each task. While pretraining BER T is\n29https://en.wikipedia.org/wiki/Main_Page\n30https://en.wikipedia.org/wiki/Sesame_Street\n31In linguistics, masked LM is known as a cloze test(T aylor,1953)\n52\nvery costly , fine-tuning is relatively inexpensive, and (depending on the size of the training\ndata) takes only a few hours on a modern GPU.\nThe English BER T models are available in two sizes: the Base model has 110 million\nparameters, BER TLarge 340 million; each with a cca. 30 thousand wordpiece vocabulary .\nThere is also a multi-language model that was pretrained jointly on 104 languages, with\na vocabulary of approximately 120 thousand wordpieces.\nBER T begat a whole family of models. One line of research kept the architecture\nand sought improvements by finetuning its details: XLNet (Y ang et al., 2019) enhanced\nhandling of masked tokens; SpanBER T(Joshi et al., 2020) extended the masking scheme\nto word spans and added a span boundary detection pretraining task. ALBER T (Lan et\nal., 2019) replaced the next sentence prediction task with the more general sentence order\nprediction, while RoBER T a(Liu et al., 2019) abandoned the next sentence prediction task\naltogether. It also used larger batches and trained with a dynamically changing masking\npattern.\nOther systems, such as BAR T (Lewis et al., 2019) and UniLM (Dong et al., 2019),\nopted to use the full T ransformer architecture instead of just the encoder and introduced\nsequence-to-sequence training objectives. Finally , the XLM family of models (Conneau and\nLample, 2019) focused on improving multi-language performance, and XLM-RoBER T ais\neven competitive with monolingual models (Conneau et al., 2019). All models reported\nstate-of-the-art results on various downstream tasks and benchmarks.\nPretraining a BER T (or derived) model is no easier or cheaper than training an au-\ntoregressive T ransformer LM (see Section 1.6.4). DistilBER T (Sanh et al., 2019) aims to\nalleviate the impact on end users by training a leaner BER T model via knowledge dis-\ntil lation (Hinton et al., 2015); i.e. with the objective to reproduce the original model’s\nbehavior. ALBER T incorporates cross-layer parameter sharing, making the model not\nonly cheaper to use, but faster to train as well. Y et pretraining a state-of-the-art contex-\ntual embedding is still out of the reach of most research groups.\n1.7.5 Embeddings in NLP\nIn NLP systems, most sequence- or token classification tasks (POS tagging, NER, senti-\nment analysis, etc.) are solved by machine learning. These systems typically consist of\ntwo parts: a featurizer that creates features for each token of its input (generally , a word)\nand a classifier, that takes these features as its input and, depending on the task, emits\na token or sequence label. In the traditional paradigm, features are created manually ,\nbased on the linguistic and orthographic properties of words. The features are then fed\ninto a usually off-the-shelf classifier, such as a logistic regression or conditional random\n53\nfield (CRF) model.\nThe idea that word embeddings can supplant manual features goes back to (Collobert\nand W eston, 2008; T urian et al., 2010). Collobert et al. ( 2011) trained a neural tagging\nsystem jointly for several NLP tasks (POS, chunking, NER and semantic role labeling).\nThe results with purely automatic features were already competitive with the state-of-the-\nart, and adding manual features allowed the system to slightly improve on them.\nThe first public NLP pipeline to adopt word embeddings was Stanford CoreNLP , where\nthey were used to augment manual features in the parser (Socher et al., 2013). T oday ,\nword vectors can be found in most NLP software packages (spaCy , UDPipe32, etc.). T o\nour knowledge, however, the only NLP library to fully eschew manual features in favor of\npretrained vectors is Flair33; in fact, the core idea of Flair is to make “mixing and match-\ning” various embeddings as simple as possible (Akbik; Bergmann; Blythe, et al., 2019).\nAt the time of writing, Flair represents the state-of-the-art in POS tagging, chunking and\nNER (Akbik et al., 2018; Akbik; Bergmann, and V ollgraf, 2019).\nDeep contextualized embeddings have not yet reached this level of penetration. While\nthere exist libraries that allow their integration into NLP pipelines (such as spacy-\ntransformers34), most of the models are published as-is, with only the scripts required\nto reproduce results in the associated papers. Also, BER T and its lineage excel in higher\nlevel tasks, such as question answering and the various language understanding tasks in\nGLUE, which are not regularly part of a text processing pipeline. However, if the ubiquity\nof static embeddings is any indication, this is bound to change in the near future.\n1.8 Language modeling and NLP\nReading the previous sections, an attentive reader might have noticed how little role\nlinguistics, or NLP , played in the evolution of language modeling techniques. Even though\nlanguage modeling has its origins in two NLP tasks: speech recognition and machine\ntranslation, there seems to be little influx of ideas from linguistics to the new field. The\nbasic axioms (i.e. languages have vocabularies that consist of words; (written) words\nare composed of characters, etc.) are exceptions, but those can hardly be considered\nrevolutionary .\nThere are linguistically inspired techniques, such as subwords, vocabulary clustering in\nclass-based n-grams and hierarchical softmax, or the idea of training bidirectional T rans-\n32https://ufal.mff.cuni.cz/udpipe\n33https://github.com/zalandoresearch/flair\n34https://github.com/explosion/spacy-transformers\n54\nformer LMs with the Cloze task. But only a handful of systems (such as the joint LM of\nFilimonov and Harper (2009), the RNN grammar of Dyer et al. (2016) and the dependency-\nbased embedding of (Levy and Goldberg, 2014a)) used actual NLP techniques (CFG fea-\ntures in the former and dependency parsing in the latter two) to address issues in language\nmodeling. Even then, these systems are regularly outperformed by models built without\nany (deep) linguistic thought; see Section 1.5.3.\nY et the opposite direction seems exceptionally fruitful: modern LMs generate texts of\nhuman-level local consistency , while embeddings have revolutionized how NLP systems are\nbuilt. The difference is especially palpable on high-level natural language understanding\ntasks, which suddenly seem doable. While the AlphaGo (Silver et al., 2016) moment of\nNLP has not yet come, it certainly seems closer than ever before.\nIn a way , these developments are not surprising. Linguists have long struggled to\nexplain the workings of language; and given the number of competing theories, it may\nsafely be said that they have not yet succeeded. Y et humans have no issues using language,\nirrespective of the level of grammar education they might have had. There is one crucial\ndifference between LM training and first language acquisition: modern language models\nneed orders of magnitude more text to reach the performance reported in the papers than\nhumans do to learn their mother tongue. There is still room for improvement.\nAs there might also be room for NLP in language modeling. W e might observe that the\nresults presented in this chapter were all achieved by English-language models on English\nbenchmarks. Due to it being an analytic language, English lends itself well to word-level\nlanguage modeling. Agglutinative languages, such as Hungarian, might pose problems for\nthe language modeling techniques presented above. In this thesis, we explore some of the\nways in which language models and linguistic methods interact, with a major focus on\nHungarian.\n55\nChapter 2\nemLam – a Hungarian Language\nModeling baseline\n2.1 Introduction\nW e saw in Chapter 1 how language modeling is an integral part of several NLP applica-\ntions, such as speech recognition, optical character recognition and machine translation.\nIt has been shown that the quality of the LM has a significant effect on the performance of\nthese systems (Brants et al., 2007; Chelba et al., 2012). Accordingly , evaluating language\nmodeling techniques is a crucial part of research. F or English, a thorough benchmark of\ndiscrete techniques was carried out by Goodman ( 2001); and Section 1.5.3 demonstrated\nhow the availability of standard benchmark corpora facilitated progress in neural lan-\nguage modeling. All three corpora mentioned there, the preprocessed version of the Penn\nT reeBank (PTB), WikiT ext-2 (WT2) and the One Billion W ord Benchmark (1B), were\npublished for the sole reason of measuring advances in statistical language modeling.\nChapter 1 has also shown the dramatic advances the last decade saw in language\nmodeling. T raining corpora grew from a few million words (e.g. the Brown corpus) to\ngigaword and beyond, while vocabulary size increased from a few 10k to several hundred\nthousands (as in the 1B). Neural networks overtook n-grams as the language model of\nchoice. State-of-the-art LSTM and T ransformer models achieve up to 63–67% reductions\nin perplexity compared to 5-gram models (see T ables 1.4, 1.6 and 1.7).\nSurprisingly , these developments left few traces in the Hungarian NLP literature. Aside\nfrom an interesting line of work on morphological modeling for speech recognition (Mihajlik\net al., 2010; Németh et al., 2007), no study is known to the author that addresses issues of\nHungarian language modeling. While quality works have been published in related fields,\nlanguage model performance is often not reported, or is not competitive: e.g. in their\n56\notherwise state-of-the-art system, T arján et al. ( 2016) use a 3-gram model that achieves\na perplexity of 400 1 on the test set — a far cry from the numbers reported in Chapter 1\nand here.\nHungarian poses a challenge to word-level LM because of its agglutinative nature.\nWhile verb conjugation is mostly fusional (i.e. person, number and tense are fused into a\nsingle inflectional suffix, such as in “ lát·tuk” ‘we saw’), verbs still have about 40 surface\nforms, as opposed to the maximum four 2 in English. The nominal paradigm, on the other\nhand, is fully agglutinative, so nouns, adjectives and numbers may have as many as 700\ndifferent inflected forms.\nThe proliferation of word forms inflates the vocabulary , which has various adverse\neffects. Firstly , it increases the memory usage of all language models and increases the\ncomputational cost associated with the softmax layer (see Section 1.4.4). Secondly , and\nmore importantly , it decreases the number of contexts a word form is seen during training\n(and conversely: the number of word forms seen after a specific context), making the data\nsparsity problem much more pronounced than it is for English. The results are worse\nprobability estimates and ultimately , lower performance. Lastly , even frequent words will\nhave forms not present in the training corpus, increasing the number of OOV s in open-\nvocabulary language modeling.\nIn this chapter, we intend to decrease the gap between English and Hungarian language\nmodeling in two ways. First, we report baselines for various language modeling methods\non three publicly available Hungarian corpora. In light of the issues mentioned above, it\nwill be especially interesting to see how the performance of the tested methods translate\nto Hungarian.\nSecond, we present a version of the Hungarian W ebcorpus (Halácsy et al., 2004) that\ncan be used as a benchmark for language models. Our motivation was to create the\nHungarian equivalent of the One Billion W ord Benchmark corpus for English: a freely\navailable data set that is large enough to enable the building of high-quality LMs, yet\nsmall enough not to pose a serious barrier to entry for researchers. W e hope that the\navailability of the corpus will facilitate research into newer and better LM techniques for\nHungarian.\nThe software components required to reproduce this work, as well as the benchmark\ncorpus, comprise the emLam module3 of e-magyar4 (Váradi et al., 2017). The scripts have\nbeen released as free software under the MIT license, and can be downloaded from the\n1Personal communication with the author.\n2Namely present, past simple, past participle and gerund.\n3http://e-magyar.hu/hu/textmodules/emlam\n4http://e-magyar.hu\n57\nemLam repository 5.\nThe rest of the chapter is organized as follows. The benchmark corpora, as well as\nour solution to the data sparsity problem is described in Section 2.2. In Section 2.3, we\nintroduce the LM methods to be evaluated. Results are presented in Section 2.4. Finally ,\nSection 2.5 contains our conclusions and ideas left for future work.\n2.2 The Hungarian Datasets\nW e selected three publicly available Hungarian corpora for benchmarking. The corpora\nare of various sizes and domains, which enabled us to evaluate both small- and large-\nvocabulary LM configurations. The corpus sizes roughly correspond to those of the En-\nglish corpora commonly used for LM benchmarks, making a comparison between the two\nlanguages easier.\nThe Szeged T reebank (Csendes et al.,2003; Vincze et al., 2014) is the largest manually\nannotated corpus of Hungarian. The treebank consists of CoNLL-style tsv files; we used\na version in which the morphological features had been converted to KR codes to keep in\nline with the automatic toolchain described below. At around 1.5 million tokens, it falls\nbetween PTB and WT2 in size, allowing us a direct comparison of small-vocabulary LM\ntechniques.\nThe filtered version of the Hungarian W ebcorpus (Halácsy et al., 2004) is a semi-\ngigaword corpus at 589M tokens. It consists of webpages downloaded from the.hudomain\nthat contain no more than 4% word tokens unrecognized by HunSpell; i.e. “fewer typos\nthan average printed materials” (Halácsy et al.,2004). The downloadable corpus is already\ntokenized; we further processed it by performing lemmatization, morphological analysis\nand disambiguation with Hunmorph (T rón et al.,2005): ocamorph for the former two and\nhunlex for the latter.\nThe Hungarian Gigaword Corpus (Oravecz et al.,2014) is the largest public Hungarian\ncorpus. It is based on the Hungarian National Corpus 6 (Váradi, 2002), a 187M word\ncorpus with newswire, literature, scientific and legal texts supplemented with material\ndownloaded from online forums. This core was gradually expanded to 1.5 billion tokens\nwith additional data crawled from the web. In this paper, we used an in-progress version,\ndenoted MNSZ2. At around one billion tokens, it is comparable in size to the English 1B\ncorpus. The raw text was preprocessed with the same tools as above.\nW e decided to use the ‘old’ hun* tools because at the time of writing, the e-magyar\n5https://github.com/dlt-rilmta/emLam\n6Magyar Nemzeti Szövegtár\n58\ntoolchain was not yet production ready , and the version of the Szeged corpus that uses the\nnew universal POS tags still contained conversion errors. Therefore, the results published\nhere might be slightly different from what one can attain by running the scripts in the\nemLam repository , should the issues above be addressed. However, any such differences will\nmost likely be inconsequential.\n2.2.1 Preprocessing\nAs mentioned before, the main challenge of modeling an agglutinative language is the\nnumber of distinct word forms. The solution that works well for English — putting all\nword forms into the vocabulary — is not reasonable: on one hand, the vocabulary size\nwould explode (see T able 2.1); on the other, there is a good chance the training set does\nnot contain all possible word forms in the language.\nThe most common solution in the literature is to break up the words into smaller\nsegments (Afify et al., 2006; Botha and Blunsom, 2014; Hirsimäki et al., 2005). The two\nmain directions are statistical and morphological word segmentation. In this chapter, we\nstudy the latter. Not only is it linguistically more motivated, it also ensures that the\ntokens we end up with are meaningful, making the LM easier to debug.\nW e ran the pipeline described above on all words in the corpus, and split all inflectional\nsuffixes (as well as some derivational ones, such as <COMPAR>, the tag for the comparative\nmarker ‘-bb’ and <SUPERLAT>, the tag for the superlative marker ‘leg-’) into separate tokens.\nOnly inflections marked by the KR code are included; the default zero morphemes (the\nnominative case marker and the present-tense third person singular for verbs) are not. A\nfew examples:\njelmondatával → jelmondat <POSS> <CAS<INS>>\nakartak → akar <PAST> <PLUR>\notthonainkba → otthon <PLUR> <POSS<1> <PLUR>> <CAS<ILL>>\nOne could say that by normalizing agglutinative forms like this, we “deglutenized” it;\ntherefore, the resulting variants of the corpora shall be referred to as gluten-free (GLF)\nfrom now on. The full preprocessing pipeline is as follows:\n1. T okenization and normalization. The text was lowercased, converted to utf-8 and\ndeglutenized\n2. (W ebcorpus only) Duplicate sentences were removed, resulting in a 32.5% reduction\nin corpus size.\n59\n3. T okens below a certain frequency count were converted into <unk> tokens. The\nword distribution proved different from English: with the same threshold as in the\n1B corpus (3), much more distinct types remained. T o be able to test LMs with a\nvocabulary size comparable to 1B, we worked with different thresholds for the two\ngigaword corpora: W ebcorpus was cut at 5 words, MNSZ2 at 10. An additional\nthresholding level was introduced at 30 (50) tokens to make RNN training tractable.\n4. Sentence order was randomized. F or the Szeged corpus, we also created a version\nwithout sentence shuffling. The two are differentiated with subscripts: S marks the\nshuffled version, while O the corpus with the original sentence order.\n5. The data was divided into train, development and test sets; 90%–5%–5% respec-\ntively . F or shuffled datasets, this is a simple matter of random sampling. With\nSzegedO, the situation is not so straightforward: from the 15 files of the corpus we\ncould not single out a single one as test, as they all represent different genres or\ndomains; nor could we randomly pick 5% of all documents, as the boundaries are\nunmarked. In the end, we set apart the last 10% of each file for the validation and\ntest sets.\n2.2.2 Corpus Statistics\nT able2.1 lists the main attributes of the datasets created from the three corpora. Where\nnot explicitly marked, the default count threshold (3) is used. It is clear from comparing\nthe raw and GLF datasets that deglutenization indeed decreases the size of the vocabulary\nand the number of OOV s by about 50%. Although not shown in the table, this reduction\nratio remains consistent among the various thresholding levels.\nAlso apparent is that, compared to the English corpora in T able 1.3, the number of\nunique tokens is much bigger even in the default Hungarian GLF datasets. Preliminary\ninquiry into the data revealed that three phenomena account for the majority of the token\ntypes between the 3 and 30 (50) count marks: compound nouns, productive derivations and\nnamed entities (with mistyped words coming in at fourth place). Since neither the Szeged\ncorpus, nor (consequently) the available morphological disambiguators take compounding\nand derivation into account, no immediate solution was available for tackling these issues.\nTherefore, we decided to circumvent the problem by introducing the higher frequency\nthresholds and concentrating on the problem of inflections in this study .\nThe preprocessing scripts are available in the emLam repository .\n60\nDataset Sentences T okens V ocabulary OOV s Analysis\nSzeged 81,967 1,504,801 38,218 125,642 manualSzeged GLF 2,016,972 23,776 55,067\nW ebcorpus\n26,235,007\n481,392,824 1,971,322 5,750,742\nautomaticW ebcorpus GLF 683,643,265 960,588 3,519,326\nW ebcorpus GLF-5 id. 625,283 4,647,706\nW ebcorpus GLF-30 id. 185,338 9,393,015\nMNSZ2\n44,329,309\n624,830,138 2,988,629 11,614,583\nautomaticMNSZ2 GLF 852,232,675 1,714,844 5,729,509\nMNSZ2 GLF-10 id. 630,863 10,845,301\nMNSZ2 GLF-50 id. 197,542 19,547,859\nT able 2.1: Comparison of the three Hungarian corpora\n2.2.3 The Benchmark Corpus\nOf the three corpora above, the Hungarian W ebcorpus is the only one that is freely down-\nloadable and available under a share-alike license (Open Content7). Therefore, we decided\nto make not only the scripts, but the preprocessed corpus as well, similarly available for\nresearchers.\nThe corpus can be downloaded as a list of tab-separated files. The three columns\nare the word, lemma and disambiguated morphological features. A unigram (word and\nlemma) frequency dictionary is also attached, to help create count-thresholded versions.\nThe corpus is available under the Creative Commons ShareAlike (CC SA) license.\nSuch a corpus could facilitate language modeling research in two ways. First, any\nresult published using the corpus is easily reproducible. Second, the fact that it has been\npreprocessed similarly to the English 1B corpus, makes comparisons such as those in this\npaper possible and meaningful.\n2.3 Language model evaluation\nW e had neither the time, nor the means to evaluate all (or even most) of the language\nmodeling methods described in Chapter 1. W e opted instead to test the ones that were suf-\nficiently state-of-the-art and either had an implementation available or could be recreated\nwith reasonable effort.\nW e chose a 5-gram model with modified Kneser-Ney (KN) smoothing as our discrete\nbaseline, since it is simple, and it reportedly outperforms all other n-gram models (Good-\n7http://www.opencontent.org/definition/\n61\nman, 2001). W e used the implementation in the SRILM (Stolcke et al., 2011) library ,\nand tested two configurations: a pruned backoff (the default) 8 and, similar to Chelba\net al. ( 2014), an unpruned interpolated model 9. All datasets described in T able 2.1 were\nevaluated; in addition, we also tested POS models (both full-KR and GLF, where the\ninflectional tags are split from the main category), where lemmas were replaced with their\nrespective part-of-speech tags. These test the model’s ability to learn basic syntax and\nmorphotactics (as encoded by POS sequences) and it also serves as the state transition\ntable for class-based n-grams (see below). Due to the considerably reduced vocabulary\n(less than two thousand for full KR and two hundred for GLF), this task is much easier\nthan word-level language modeling.\nIt is worth mentioning that the GLF format (and subwords in general) is not optimal\nfor n-gram modeling. As words are broken up into multiple tokens, they use up more of\nthe n-gram’s history , which is already a very limited resource. In the extreme case, such as\nthe last example under Section 2.2.1, the whole history could be taken up by inflectional\ntokens, preventing the model from accessing relevant context. RNN LMs are less affected\nby this issue as they can retain information from their history much longer.\nW e also evaluate a class-based n-gram setup to see if they can improve on regular 5-\ngrams. Our working hypothesis, based on Section 1.3.3, was that they cannot. Out of the\nvarious clustering options available, we elected to use part-of-speech categories as clusters,\nsince a full morphological analysis was already available as a by-product of deglutenization.\nW e ran three RNN baselines:\n1. the Medium regularized LSTM setup in Zaremba et al. ( 2014). W e used the imple-\nmentation10 in T ensorflow (Abadi et al., 2016b)\n2. A WD-LSTM (Merity et al.,2018) with the hyperparameter settings tuned to WT2,\nas the size of its vocabulary matches Szeged’s better than PTB’s does.\n3. LSTM-512-512, the smallest configuration described in Jozefowicz et al. (2016). The\nmodel was reimplemented in T ensorflow, and is available from theemLam repository .\nDue to time and resource constraints, the first two baselines were only run on the\nSzeged corpus, and the last one only on the smallest, GLF-30 (50) variants of the gigaword\ncorpora.\n8-kndiscount\n9-kndiscount -gt1min 1 -gt2min 1 -gt3min 1 -gt4min 1 -gt5min 1 -interpolate1\n-interpolate2 -interpolate3 -interpolate4 -interpolate5\n10https://github.com/tensorflow/models/tree/r1.10.0/tutorials/rnn/ptb\n62\nLanguage models typically perform worse when tested on a different corpus, due to the\ndifferences in vocabulary , word distribution, style, etc. T o see how significant this effect\nis, the models trained on the two gigaword corpora were evaluated not only on the test\nset of their training corpus, but on the other corpus as well.\n2.4 Results\n2.4.1 n-grams\nThe results achieved by the n-gram models are reported in T able 2.2–2.5. T able 2.2 lists\nthe perplexities achieved by KN 5-grams of various kinds; the odd one out is POS GLF,\nwhere the limited vocabulary enabled us to create up to 9-gram models. F or MNSZ2, the\nreported score is from the 7-gram model, which outperformed 8- and 9-grams. Results of\nEnglish models on the PTB and 1B are included for comparison.\nA glance at the table reveals how the vocabulary problem affects language modeling\nperformance: the perplexities attained by word-level 5-grams are anywhere between 85%–\n265% higher than the results of the corresponding English models. GLF 5-grams performed\neven worse than word-level models, confirming our suspicion that subword-level n-grams\nare suboptimal.\nSeveral interesting observations can be made. First, with the exception of the first\nW ebcorpus word model, models trained on larger corpora attain lower perplexity scores,\nunderlying the importance of training data size. Second, it seems that as the size of the\nvocabulary decreases (with larger frequency thresholds on one hand, and with the POS\nmodels on the other), so does the perplexity – POS n-grams are probably at their limit\nat 10–12 points.\nFinally , an interesting trend emerges when comparing the results of the two gigaword\ncorpora: the perplexities of both word and GLF models are about 50% higher on W eb-\ncorpus than on MNSZ2. Finding the cause of this discrepancy requires further research.\nT wo possible candidates are data sparsity (at the same vocabulary size, W ebcorpus is 25%\nsmaller) and a difference in the distribution of inflection configurations.\nF orn-gram models, SzegedS and SzegedO are almost equivalent, as they cannot benefit\nfrom long-term dependencies. Here we only report results for the former.\nT able2.3 shows the best n-gram perplexities achieved by GLF models. It can be seen\nthat interpolated, unpruned models perform much better than backoff models.\n63\nCorpus Threshold W ord GLF F ull POS POS GLF\nSzegedS 3 262.77 637.29 35.20 66.48\nW ebcorpus\n1 N/A N/A 10.21 12.89\n5 328.22 399.49 N/A N/A\n30 259.79 362.74 N/A N/A\nMNSZ2\n1 N/A N/A 11.88 12.47\n10 233.52 277.94 N/A N/A\n50 174.65 239.57 N/A N/A\nPTB (Mikolov and Zweig, 2012) N/A 141.2\n1B (Chelba et al., 2014) 3 90\nT able 2.2: 5-gram (9 for POS GLF ) KN test results (PPL)\nModel Pruned backoff Unpruned interpolated\nSzegedS GLF 637.29 587.11\nW ebcorpus GLF-5 399.49 324.24\nW ebcorpus GLF-30 362.74 291.75\nMNSZ2 GLF-10 277.94 214.57\nMNSZ2 GLF-50 239.57 186.63\nPTB (Mikolov and Zweig, 2012) 141.2 N/A\n1B (Chelba et al., 2014) 90 67.6\nT able 2.3: The best KN 5-gram results\n2.4.2 Class-based n-grams\nOur class-based experiments confirmed our working hypothesis regarding clustered models.\nContrary to the general consensus, our findings (T able 2.4) show that interpolating class-\nand token-level (in this case, GLF) LMs do not lead to any improvement over the latter.\nThe class-based model could only improve on the unigram model, and failed to do so for\nthe higher orders.\nWhy the discrepancy? The most likely explanation is that as the size of the vocabulary\ngrows larger, the emission entropy increases, which is mirrored by the perplexity . This\nwould explain why class-based n-grams seem to work on small corpora, such as the PTB,\nbut not on MNSZ2. Part of the blame also lies with us for using POS-based clusters, which\nreportedly underperform automatic methods (Niesler et al., 1998). Our results confirm\nthat POS-based clusters simply do not work.\nAnother point of interest is the diminishing returns of PPL reductions as the n-gram\norders grow. While we have not experimented with 6-grams or higher orders, it seems\n64\nprobable that performance of GLF models would peak at 6- or 7-grams on MNSZ2 (and\nW ebcorpus). F or word-level models, this saturation point arrives much earlier: while not\nreported here, the perplexity difference between 4- and 5-gram models is only 1-2 point.\nThis implies that GLF models are less affected by data sparsity .\nModel GLF-10 POS → GLF-10 GLF-50 POS → GLF-50\n1-gram 34,208 6,918 35,654 5,719\n2-gram 741.76 2,698 649.57 2,238\n3-gram 426.87 2,329 349.93 1,932\n4-gram 303.59 2,121 262.69 1,760\n5-gram 277.94 1,986 239.57 1,649\nT able 2.4: Comparison of GLF and class-based model performance on the MNSZ2. POS\n→ GLF-n denotes a HMM model with POS as class and GLF- n as the surface model.\n2.4.3 Cross-evaluation\nIt is a well-known fact that the performance of LMs degrade substantially when they are\nnot evaluated on the corpus they were trained on. This effect is clearly visible in T able2.5,\nwhich shows how word-level and GLF n-gram models trained on one of the two gigaword\ncorpora perform on the other.\nAgain, the two corpora display wildly different characteristics. W ebcorpus word models\nexhibit the smallest perplexity increase of 12-15%, followed by the GLF models, whose\nscore grows by three times as much. Models trained on the MNSZ2 are affected the most;\ninterestingly , both the word and GLF models see their perplexity increased by about 120–\n140%. Contrasting this result with how the models perform on their own training corpus\nsee the Evaluated on self ) seems to suggest that there exists a trade-off between predictive\npower and universality .\nAs POS models capture syntactic regularities, they are affected to a much lesser extent\nthan token-level models (with the exception of the W ebcorpus word model). Curiously ,\nthe W ebcorpus GLF model seems to be more stable than the full POS one; a pattern not\nreplicated in the MNSZ2 case. This, and the fact that there is performance degradation in\nPOS models at all again hints at a different distribution of inflections in the two corpora.\n2.4.4 RNN language models\nT able2.6 reports the perplexities achieved by the RNN models. Disregarding the last\ncolumn for a moment, three clear conclusions can be drawn from the numbers. First, in\n65\nT rained on Threshold Evaluated on\nself other Increase\nW ebcorpus word 5 328.22 377.88 15%\n30 259.79 291.98 12%\nMNSZ2 word 10 233.52 566.60 142%\n50 174.65 397.13 127%\nW ebcorpus GLF 5 399.49 606.43 52%\n30 362.74 495.38 37%\nMNSZ2 GLF 10 277.94 619.81 123%\n50 239.57 548.76 129%\nW ebcorpus full POS 1 10.21 16.14 58%\nMNSZ2 full POS 1 11.88 16.49 39%\nW ebcorpus POS GLF 1 12.89 18.08 40%\nMNSZ2 POS GLF 1 12.47 18.25 46%\nT able 2.5: Performance of word and GLF LMs when evaluated on their own training\ndataset and on the other gigaword corpus. Perplexities in the Evaluated on self column\nare copied over from T able 2.2.\nline with what has been reported for English by many authors, RNNs clearly outperform\neven the best n-gram models. Second, the performance of GLF models is much closer to\nthe word-level ones than it was for n-grams (about 30% increase on Szeged as opposed to\nthe 140% in T able 2.2), proving that the technique is a much better fit for neural LMs.\nFinally , models that preserve the overall text sequence order perform better than shuffled\ndatasets, if not by much. However, keeping the text structure intact allows the application\nof neural cache or pointer sentinel extensions, which further improve results by 7%–9%.\nIt is important to note that while the GLF format makes neural modeling of large-\nvocabulary corpora (such as W ebcorpus and MNSZ2) possible, the results are not quite\ncomparable to what Jozefowicz et al. ( 2016) reported for 1B; especially considering that\nthese GLF models used very high frequency thresholds, and the GLF or GLF-10 config-\nurations might have fared slightly worse (as in T able 2.2). The odd one out is Szeged,\nwhere the performance of both the word and GLF models are reasonably close to their\nPTB equivalents. This gives us hope that the “curse of agglutination” can be dealt with\nin future work.\n2.4.5 Pseudo-Hungarian\nAttentive readers might have noticed that the perplexity scores reported here are different\nfrom those in the original paper. The reason for this is that in Nemeskey ( 2017), we\n66\nModel Dataset Perplexity Per-token PPL\nMedium regularized SzegedS 101.74\nMedium regularized SzegedO 98.88\nA WD-LSTM SzegedS 71.70\nA WD-LSTM SzegedO 68.30\nA WD-LSTM + pointer SzegedO 62.08\nMedium regularized SzegedS GLF 131.39 38.07\nMedium regularized SzegedO GLF 126.78 37.07\nA WD-LSTM SzegedS GLF 94.19 29.70\nA WD-LSTM SzegedO GLF 89.17 28.51\nA WD-LSTM + pointer SzegedO GLF 81.13 26.57\nLSTM-512-512 W ebcorpus GLF-30 191.51 40.46\nLSTM-512-512 MNSZ2 GLF-50 146.82 38.78\nMedium regularized PTB 82.07\nA WD-LSTM PTB 57.3\nA WD-LSTM + pointer PTB 52.8\nLSTM-512-512 1B 54.1\nT able 2.6: LSTM model performance\npublished per-token perplexity , which gave incorrect results for the GLF datasets, where a\nword may consist of multiple tokens. In this chapter, all perplexity values are normalized\nfor the word count. While this results in 34%–42% higher perplexity scores (depending on\nthe corpus), all findings of the original paper still stand; in particular, GLF models still\nachieve lower perplexity than word-level ones.\nThis mistake, however, was not entirely without merit. It allowed us to inadvertently\nperform a language modeling experiment on a fully analytic version of Hungarian. In\nEnglish, many of the inflectional tokens in the GLF format are actually separate words:\nmost case markings are expressed with prepositions, such as <DAT> with ‘ for’; and for\nmost adjectives, the <COMPAR>ative and <SUPERLAT>ive forms are marked with the words\n‘more’ and ‘ most’, respectively . Other GLF tokens, such as the plural marker <PLUR>\nand those related to the verb conjugation are either part of the word in English, or are\nmissing altogether; yet we find examples for them in other languages (Japanese Ầ ‘tachi’\nfor the plural and e.g. ƍŸŧ ‘nasai’ for imperative). This goes to show how arbitrary\nthe realization of grammatical functions is across languages – and that our “analytic\nHungarian” could well be a real language. F or this reason, we also record the per-token\nperplexity values in T able2.6 as a curiosity .\n67\n2.4.6 Into the Unknown\nAs we have seen previously , the GLF format has various advantages: it reduces the size\nof the vocabulary and the number of OOV s, which benefits neural language modeling.\nHowever, GLF models have a higher perplexity than the corresponding word models.\nIn this section, we argue that the practice of replacing low-frequency words with<unk>\ntokens and using “vanilla” perplexity as the metric does not accurately represent the\nuncertainty present in the model. While the model learns when to predict a “generic\ninfrequent word”, it cannot make an informed guess as to its identity . Y et frequency\nthresholding is merely a technical necessity rather than proper modeling of the language.\nThis is especially evident in text generation, where the presence of <unk> tokens in the\noutput is undesirable. In order to generate valid text, we need to replace them with actual\nwords.\nIn order to evaluate the performance of a model that is not allowed to generate<unk>s,\nwe propose a new metric: rectified perplexity. It augments the “vanilla” perplexity score\nwith the perplexity of the model that predicts valid words for each <unk> token:\nPPLcorr = exp\n(|C|log(PPLraw) +|OOV s|log(PPLunk)\n|C|\n)\n(2.1)\nwhere PPLraw is the uncorrected perplexity , PPLunk is the perplexity of the sublanguage\ncovered by the <unk> token, and |OOV s|and |C|are the number of <unk> tokens and the\ntotal number of tokens in the corpus, respectively .\nThis rectified metric is equivalent with the perplexity of a class-based model where\nwords in the vocabulary are singleton classes and <unk> emits out-of-vocabulary words.\nThere are several options to estimate the emission probabilities; one example would be a\ncharacter-level model, such as the character LSTM in Jozefowicz et al. ( 2016). Since we\noperate in a closed vocabulary setting, we use the MLE of the token distribution under\nthe frequency threshold.\nT able 2.7 shows the performance of the main models under the new metric. It is\napparent at first sight that GLF models are much less affected by the perplexity correction\ncoming from OOV s; a natural consequence of them having about half the number of OOV s\ncompared to word models. While the Szeged and W ebcorpus word n-gram models still\nperform better than their GLF counterparts, the tables are already turned on MNSZ2. In\nthe RNN case, the GLF version attain about 33% lower perplexity than the word model,\nshowing its superiority in fully specified language modeling.\n68\nModel W ord GLF\nSzegedS 5-gram 262.77 → 655.92 (149%) 637.29 → 848.50 (33%)\nW ebcorpus5-gram (5) 328.22 → 389.11 (19%) 399.49 → 426.38 (7%)\nW ebcorpus5-gram (30) 259.79 → 398.73 (53%) 362.74 → 416.35 (15%)\nMNSZ2 5-gram (10) 233.52 → 362.24 (55%) 277.94 → 337.33 (21%)\nMNSZ2 5-gram (30) 174.65 → 401.20 (130%) 239.57 → 336.52 (40%)\nSzegedO medium regularized 98.88 → 258.66 (162%) 126.78 → 169.65 (34%)\nSzegedO A WD-LSTM 68.30 → 178.67 (162%) 89.17 → 119.32 (34%)\nSzegedO A WD-LSTM + pointer 62.08 → 162.40 (162%) 81.13 → 108.56 (34%)\nT able 2.7: Perplexity correction for OOV tokens\n2.5 Conclusion\nThe work presented in this chapter contributes to Hungarian language modeling in two\nways. First, we reported state-of-the-art LM baselines for three Hungarian corpora, from\nmillion to gigaword size. W e found that word-level LMs performed worse than they do\nfor English, not in the least because of the increased vocabulary size and number of OOV\ntokens. The vocabulary problem could be alleviated with splitting words into lemmas\nand inflectional affixes (the ”gluten-free” format). GLF models had a higher “vanilla”\nperplexity than word models, but outperformed them when no <unk> tokens were allowed\nin the output.\nSecond, we introduced a benchmark corpus for language modeling. T o our knowl-\nedge, this is the first such dataset for Hungarian. This specially prepared version of the\nHungarian W ebcorpus is freely available, allowing researchers to easily and reproducibly\nexperiment with new language modeling techniques. It is comparable in size to the One\nBillion W ord Benchmark corpus of English, making comparisons between the two lan-\nguages easier.\n2.5.1 F uture work\nWhile the methods reported here can be called state-of-the-art, many similarly effective\nmodeling approaches are missing. Evaluating them could provide additional insight into\nhow Hungarian “works” or how Hungarian and English should be modeled differently .\nUnderstanding the unusual behavior of word models on W ebcorpus also calls for further\ninquiry into language and corpus structure.\nThe performance of the models here was measured in isolation. Putting them into use\n(maybe with some adaptation) in NLP applications such as ASR or MT could answer the\n69\nquestion of whether the reduction in perplexity translates to similar reductions in WER\nor BLEU.\nThe most glaring problem touched upon, but not addressed, in this paper, is the effect\nof compounding and derivation on vocabulary size. A way to reduce the number of words\ncould be a more thorough deglutenization algorithm, which would split compound words\ninto their parts and strip productive derivational suffixes, while leaving frozen ones such\nas ház·as·ság untouched. This could indeed be a case when a gluten free diet does make\none slimmer.\n70\nChapter 3\nEvaluating multi-sense embeddings\nfor semantic resolution\nAs promised in Section 1.7.3, in this chapter we return to the problem of evaluating\nmulti-sense embeddings. W e propose a method that evaluates MSEs intrinsically , based\non a comparison with monolingual dictionaries. The content of this chapter is a result\nof joint work presented in Borbély; Makrai, et al. ( 2016). The parts included here are\nall contributions of the author, with the exception of Section 3.4, which gives a short\nsummary of the rest of the paper for context.\n3.1 Introduction\nGladkova and Drozd ( 2016) calls polysemy “the elephant in the room” as far as evalu-\nating embeddings are concerned. Here we attack this problem head on, by proposing a\nmethod for evaluating multi-sense word embeddings (MSEs), where allegedly polysemous\nor homonymous words have multiple vectors, ideally one per sense.\nW ork on the evaluation of MSEs (for lexical relatedness) goes back to the seminal\nReisinger and Mooney (2010), who note that usage splits words more finely (with synonyms\nand near-synonyms ending up in distant clusters) than semantics. The differentiation of\nword senses is fraught with difficulties, especially when we wish to distinguish homonymy ,\nusing the same written or spoken form to express different concepts, such as Russian\nmir ‘world’ and mir ‘peace’ from polysemy , where speakers feel that the two senses are\nvery strongly connected, such as in Hungarian nap ‘day’ and nap ‘sun’ . T o quote Zgusta\n(1971): “Of course it is a pity that we have to rely on the subjective interpretations of the\nspeakers, but we have hardly anything else on hand” . Etymology makes clear that different\nlanguages make different lump/split decisions in the conceptual space, so much so that\n71\ntranslational relatedness can, to a remarkable extent, be used to recover the universal\nclustering (Y oun et al., 2016).\nAnother confounding factor is part-of-speech. V ery often, the entire distinction is\nlodged in the POS, as in divorce (Noun) and divorce (V erb). In this case, both words\nrelate to the same concept, so a fully semantic MSE would map them to the same vector.\nHowever, at other times the connection is less clear: compare the verbal to bank ‘rely on a\nfinancial institution’ and to bank ‘tilt’ . Clearly the former is strongly related to the nominal\nbank ‘financial institution’ while the semantic relation ‘sloping sideways’ that connects the\ntilting of the airplane to the side of the river is somewhat less direct, and not always\nperceived by the speakers. This ambiguity makes the evaluation of such word pairs in\nMSEs all the more difficult.\nIn this chapter, we propose an MSE evaluation method based on sense distinctions\nmade in traditional monolingual dictionaries. W e investigate the correlation between\nthe number of senses of each word-form in the embedding and in the manually created\ninventory as a proxy measure of how well embedding vectors correspond to concepts in\nspeakers’ (or at least, the lexicographers’) mind. The details and results are discussed in\nSection 3.2. Section 3.3 investigates what linguistic factors other than polysemy might\naffect the training of MSEs and to what extent. Section 3.4 briefly summarizes the other\nevaluation method proposed in Borbély; Makrai, et al. ( 2016). Interested readers are\nreferred to the original paper for details. Finally , some very preliminary conclusions are\noffered in Section 3.5, more in regards to the feasibility of the evaluation method we\npropose than about the merits of the systems we evaluated.\n3.2 Comparing lexical headwords to multiple sense\nvectors\nW e present a preliminary evaluation of four MSE algorithms on two languages, English\nand Hungarian.\n3.2.1 Resources to be evaluated\nThe four implementations include the released result of the spherical context clustering\nmethod huang (Huang et al., 2012) (English only); the learning process of Neelakantan\net al. (2014) with adaptive sense numbers (we report results using their release MSEs and\ntheir tool itself, calling both neela); the parametrized Bayesian learner of Bartunov et al.\n(2016) where the number of senses is controlled by a parameter α for semantic resolution,\n72\nhere referred to as AdaGram; and jiweil (Li and Jurafsky, 2015).\nMSEs with multiple instances are suffixed with their most important parameters, i.e.\nthe learning rate for AdaGram ( a = 0. 5); the number of multi-prototype words and\nwhether the model is adaptive (NP) for release neela; and the number of induced word\nsenses (s = 4) for our non-adaptive neela runs.\nWhere applicable, MSEs were trained on UMBC W ebbase (Han et al.,2013) for English\nand W ebkorpusz (Halácsy et al., 2004) for Hungarian.\n3.2.2 Lexical resources\nT wo dictionaries per language serve as ground truth. F or English, we use the Col lins-\nCOBUILD (CED, Sinclair ( 1987)) dictionary and the Longman Dictionary of contempo-\nrary English (LDOCE, Boguraev and Briscoe ( 1989)). F or Hungarian, we had access to\nthe Explanatory Dictionary of Hungarian 1 (EKSZ, Pusztai ( 2003)) and a subsample (the\nletter ‘ b’) of the Comprehensive Dictionary of Hungarian 2 (NSZ, Ittzés ( 2011)). F or the\nHungarian dictionaries, we relied on the versions created in Miháltz ( 2010) and Recski\net al. ( 2016).\nOur lexicographic sources take different positions as to whether semantic concepts or\nPOS categories should be the main organizational factor. CED starts with the semantic\ndistinctions and subordinates POS distinctions to these, while LDOCE starts with a POS-\nlevel split and puts the semantic split below. Of the Hungarian dictionaries, NSZ is closer\nto CED, while EKSZ is closer to LDOCE in this regard. Since we expect MSEs to exhibit\nsemantic properties, we consider CED as our primary ground truth.\nIn addition to the machine-readable versions of paper-based dictionaries, we include\na lexical database that was purportedly designed for program control from the get-go:\nW ordNet (Miller, 1995). In terms of size, the English database is on par with the more\ntraditional dictionaries; in fact, it is the largest resource for English. The Hungarian\nedition (Miháltz et al., 2008) is less developed, and also contains spurious entries that\nwe had to filter before the experiments. In W ordNet, synsets are differentiated by both\nsemantic and syntactic roles, which introduces a high variance in sense numbers: the\nEnglish W ordNet contains a word with as many as 75 different meanings. The quality of\nthe entries is also questionable. F or the word guard, five out of the ten noun synsets are\nabout the same position in various team sports. Additionally , the entries include very fine\ndifferentiation of concepts, such as “ a position on a basketbal l team” and “ the person who\nplays the position of guard on a basketbal l team ”; a distinction entirely missing for other\n1Magyar értelmező kéziszótár\n2A magyar nyelv nagyszótára III-IV\n73\nprofessions, such as CEO, colonel or engineer. F or these reasons, we do not recommend\nW ordNet as ground truth.\nW e simulate the case of languages without a machine-readable monolingual dictionary\nwith OSub, a dictionary extracted from the OpenSubtitles parallel corpus (Tiedemann,\n2012) automatically: the number of the senses of a word in a source language is the\nnumber of words it translates to, averaged among many languages. More precisely , we use\nthe unigram perplexity of the translations instead of their count to reduce the considerable\nnoise present in automatically created dictionaries.\n3.2.3 Evaluation\nT able3.1 summarizes the distribution of word senses (how many words with 1,…,6+ senses)\nand the major statistics (size, mean, and variance) both for our lexicographic sources and\nfor the automatically generated MSEs.\nResource Size 1 2 3 4 5 6+ Mean Std\nCED 82,024 80,003 1,695 242 69 13 2 1.030 0.206\nLDOCE 30,265 26,585 3,289 323 56 11 1 1.137 0.394\nOSub 75,718 58,043 14,849 2,259 431 111 25 1.354 0.492\nW ordNet 149,400 122,993 15,571 5,048 2,178 1,166 2,444 1.387 1.447\nAdaGram 476,827 122,594 330,218 11,341 5,048 7,626 0 1.836 0.663\nhuang 100,232 94,070 0 0 0 0 6,162 1.553 2.161\nneela.30k 99,156 69,156 0 30,000 0 0 0 1.605 0.919\nneela.NP .6k 99,156 94,165 2,967 1,012 383 202 427 1.101 0.601\nneela.NP .30k 99,156 71,833 20,175 4,844 1,031 439 834 1.411 0.924\nneela.s4 578,405 574,405 0 0 4,000 0 0 1.021 0.249\nEKSZ 121,578 66,849 628 57 11 1 0 1.012 0.119\nNSZ (b) 5,594 5,225 122 13 3 0 0 1.029 0.191\nOSub 169,244 159,843 9,169 229 3 0 0 1.144 0.199\nW ordNet 47,767 41,248 4,493 1,153 440 204 229 1.220 0.739\nAdaGram 238,462 135,052 76,096 15,353 5,448 6,513 0 1.626 0.910\njiweil 285,856 57,109 92,263 75,710 39,624 15,153 5,997 2.483 1.181\nneela.s2 771,870 767,870 4,000 0 0 0 0 1.005 0.072\nneela.s4 771,870 767,870 0 0 4,000 0 0 1.016 0.215\nT able 3.1: Size (in words), sense distribution, mean, and standard deviation of the number\nof senses in English and Hungarian lexicographic and automatically generated resources\nWhile the lexicographic sources all show roughly exponential decay of the number of\nsenses, only some of the automatically generated MSEs replicate this pattern, and only at\nwell-chosen hyperparameter settings. huang has a hard switch between single-sense (94%\n74\nof the words) and 10 senses (for the remaining 6%), and the same behavior is shown by\nthe released Neela.300D.30k (70% one sense, 30% three senses). The English AdaGram\nand the Hungarian jiweil have the mode shifted to two senses, which makes no sense in\nlight of the dictionary data. Altogether, we are left with only two English candidates, the\nadaptive (NP) neelas; and one Hungarian, AdaGram, that replicate the basic exponential\ndecay .\nThe figure of merit we propose is the correlation between the number of senses obtained\nby the automatic method and by the manual (lexicographic) method. W e experimented\nboth with Spearman ρ and Pearson r values, the entropy-based measures Jensen-Shannon\nand KL divergence, and cosine similarity and Cohen’s κ. The entropy-based measures\nfailed to meaningfully distinguish between the various resource pairs. The cosine similari-\nties and κ values would also have to be taken with a grain of salt: the former does not take\nthe exact number of senses into account, while the latter penalizes all disagreements the\nsame, regardless of how far the guesses are. On the other hand, the Spearman and Pearson\nvalues are so highly correlated that T able3.2 shows only ρ of sense numbers attributed to\neach word by different resources, comparing lexicographic resources to one another (top\npanel); automated to lexicographic (mid panel); and different forms of automated English\n(bottom panel). The top two values in each column are highlighted in the last two panels,\nn is the number of headwords shared between the two resources.\nThe dictionaries themselves are quite well correlated with each other. The Hungarian\nvalues are considerably larger both because we only used a subsample of NSZ (the letter\nb) so there are only 5,363 words to compare, and because NSZ and EKSZ come from the\nsame Hungarian lexicographic tradition, while CED and LDOCE never shared personnel\nor editorial outlook. The Hungarian W ordNet does not correlate well with the traditional\nlexical resources, and the English edition more with LDOCE, casting further doubts on\nthe validity of W ordNet as ground truth data.\nT wo English MSEs neela and huang, show perceptible correlation with a lexical re-\nsource, LDOCE, and only two systems, AdaGram and neela, correlate well with each\nother (ignoring different parametrizations of the same system, which of course are often\nwell correlated to one another).\n3.3 Parts of speech and word frequency\nSince no gold dataset exists, against which the results could be evaluated and the errors\nanalyzed, we had to consider if there exist factors that might have affected the results. In\nparticular, the better correlation of the adaptive methods with LDOCE than with CED\n75\nResources compared n ρ\nLDOCE vs CED 23,702 0.266\nCED vs W ordNet 41,076 0.195\nLDOCE vs W ordNet 26,376 0.477\nEKSZ vs NSZ (b) 3,484 0.648\nEKSZ vs W ordNet 16,786 0.103\nNSZ (b) vs W ordNet 966 0.052\nneela.30k vs CED 23,508 0.089\nneela.NP .6k vs CED 23,508 0.084\nneela.NP .30k vs CED 23,508 0.112\nneela.30k vs LDOCE 21,715 0.226\nneela.NP .6k vs LDOCE 21,715 0.292\nneela.NP .30k vs LDOCE 21,715 0.278\nhuang vs CED 23,706 0.078\nhuang vs LDOCE 21,763 0.280\nneela.s4 vs EKSZ 45,401 0.067\njiweil vs EKSZ 32,007 0.023\nAdaGram vs EKSZ 26,739 0.086\nAdaGram.a05 vs EKSZ 26,739 0.088\nneela.30k vs huang 99,156 0.349\nneela.NP .6k vshuang 99,156 0.901\nneela.NP .30k vshuang 99,156 0.413\nneela.s4 vs jiweil 283,083 0.123\nAdaGram vs neela.s4 199,370 0.389\nAdaGram vs jiweil 201,291 0.140\nT able 3.2: W ord sense distribution simi-\nlarity between various resources\nResources compared n ρ\nCED vs POS 42,532 0.052\nLDOCE vs POS 28,549 0.206\nW ordNet vs POS 46,802 0.052\nOSub vs POS 48,587 0.141\nEKSZ vs POS 52,158 0.080\nNSZ vs POS 3,532 0.046\nhuang vs POS 98,405 0.026\nCED vs freq 36,709 0.124\nLDOCE vs freq 27,859 0.317\nW ordNet vs freq 52,042 0.432\nAdaGram vs freq 399,985 0.343\nhuang vs freq 94,770 0.376\nneela.s4 vs freq 94,044 0.649\nneela.NP .30k vs freq 94,044 0.368\nneela.NP .6k vs freq 94,044 0.635\nUMBC POS vs freq 136,040 -0.054\nT able 3.3: W ord sense distribution sim-\nilarity with POS tag perplexity (top\npanel) and word frequency (bottom\npanel)\nraises suspicions. The former groups entries by part of speech, the latter by meaning,\nimplying that the methods in question might be counting POS tags instead of meanings.\nAnother possible bias that might have influenced the results is word frequency (Manin,\n2008). This is quite apparent in the release version of the non-adaptive methodshuangand\nneela: the former expressly states in the README that the 6,162 words with multiple\nmeanings “roughly correspond to the most frequent words” .\nT o examine the effect of these factors, we measured their correlation with the number\nof meanings reported by the methods above. F or each word, the frequency and the POS\nperplexity was taken from the same corpora we ran the MSEs on: UMBC for English and\nW ebkorpusz for Hungarian. T able3.3 shows the results for both English and Hungarian.\nThe correlation of automatically generated resources with POS tags is negligible: all other\n76\nembeddings correlate even weaker than huang, the only one shown. F rom the English\ndictionaries, LDOCE produces the highest correlation, followed by OSub; the correlation\nwith CED, as expected, is very low. The Hungarian dictionaries are around the level of\nCED.\nIn comparison, the correlation between sense numbers and word frequency is much\nmore evident. Almost all English resources correlate with the word frequency by at least\n0. 3 (the notable exception being CED which is the closest to a gold standard we have);\nfurthermore, the highest correlation we measured are between two versions of neela and\nthe word frequency . Adding to this the low correlation of the gold CED against the\nother resources (see T able3.2), it appears the multi-prototype embeddings included in the\nstudy were trained to assign more vectors to frequent words instead of trying this for truly\npolysemous ones.\nT o disentangle these factors further, we performed partial correlation analysis with the\neffect of frequency (or its log) or POS perplexity removed. Recall that LDOCE and CED\noriginally correlated only to ρ = 0. 266. After removing POS, we obtain 0.545, removing\nfrequency yields 0.546, and removing log frequency brings this up to 0.599. On select\nembeddings such as neela.NP .6k correlations with CED improve from a negligible 0.093\nto a respectable 0.397 if POS, and an impressive 0.696 if log frequency is factored out.\n3.4 Cross-linguistic treatment of concepts\nSince monolingual dictionaries are an expensive resource, Borbély; Makrai, et al. ( 2016)\nalso proposes an automatic evaluation of MSEs based on the discovery of Mikolov; Le, et al.\n(2013) that embeddings of different languages are so similar that a linear transformation\ncan map vectors of the source language words to the vectors of their translations. The\nlinear mapping is trained on a seed of 5,000 thousand word pairs, and evaluated on 1,000.\nThe proposed quality measure is the ratio of correctly translated words between the\nmulti-sense and the single-sense cases. Of the two MSEs that could be trained for Hungar-\nian, Adagram clearly improved the translation with a ratio around 2; while jiweil with\na ratio of 0.25, had a highly detrimental effect.\n3.5 Conclusions\nT o summarize, we have proposed a monolingual method that evaluates word embeddings in\nterms of their semantic resolution (ability to distinguish multiple senses). Our monolingual\n77\ntask, match with the sense-distribution of a dictionary , yields an intrinsic measure in the\nsense of Chiu et al. ( 2016).\nThe original paper proposes an extrinsic bilingual evaluation metric based on word\ntranslation. F or now, the two measures are not particularly well correlated, though the\nlow/negative result of jiweil in T able3.1 could be taken as advance warning for its low\nperformance in MT. The reason, we feel, is that both kinds of performance are very far\nfrom expected levels, so little correlation can be expected between them: only if the MSE\ndistribution of senses replicates the exponential decay seen in dictionaries (both profes-\nsional lexicographic and crowdsourced products) is there any hope for further progress.\nThe central linguistic/semantic/psychological property we wish to capture is that of\na concept, the underlying word sense unit. T o the extent standard lexicographic practice\noffers a reasonably robust notion (this is of course debatable, but we consider a straight\ncorrelation of 0.27 and and a frequency-effect-removed correlation of 0.60 over a large\nvocabulary a strong indication of consistency), this is something that MSEs should aim at\ncapturing. W e leave the matter of aligning word senses in different dictionaries for future\nwork, but we expect that it can improve the inter-dictionary (inter-annotator) agreement\nconsiderably , to provide a more robust gold standard.\nSince no manual steps are involved, other researchers can accurately reproduce these\nkinds of evaluations. Some glue code for this project can be found at https://github.\ncom/hlt-bme-hu/multiwsi.\n78\nChapter 4\nHabeas Corpus\nThis chapter describes the new Hungarian W ebcorpus 2.0, built from the Hungarian sub-\nset of the Common Crawl. On the one hand, our work is the spiritual successor of the\nHungarian W ebcorpus (see Section2.2): a freely downloadable, cleaned crawl of the Hun-\ngarian web. On the other, it is a continuation of the effort described in Indig ( 2018). The\nauthor of the paper generously provided us with their corpus, which we used to bootstrap\nours. The software published along with the paper 1 also served as a basis for our code.\nThe main contributions of our work are\n• an incremental, continuous expansion of the corpus;\n• greatly improving the corpus quality by language identification, various deduplica-\ntion steps, lemmatization and POS tagging;\n• including a snapshot of the Hungarian Wikipedia;\n• a full software stack used to perform the tasks mentioned above;\n• preliminary work on pretraining a Hungarian BER T model on the corpus.\nThe rest of the chapter is structured as follows. Section 4.1 discusses the goals and\ndesign considerations behind the new corpus. In Section 4.2, we survey already existing\nHungarian corpora and introduce the Common Crawl briefly . Our work is presented in\nthe next four sections. The software architecture is outlined in Section 4.3; the corpus\nbuilding procedure and its individual subtasks are elucidated in Section 4.4. Section 4.5\ndescribes how the Hungarian Wikipedia was incorporated into the corpus. Section 4.6\npresents initial work the Hungarian BER T trained on W ebcorpus 2.0. Finally , we draw\nour conclusions and outline our plans for future improvements in Section 4.7.\n1https://github.com/ppke-nlpg/commoncrawl-downloader\n79\n4.1 Goals and design considerations\nW e set out with the aim of creating the largest Hungarian corpus to date that can be\nused to train modern T ransformer language models and contextual embeddings. Here we\nreview the requirements of such a corpus and the design choices implied by them.\n4.1.1 Goals and constraints\nT o expand on the motivation above, the corpus should\n• be in the gigaword range, at least as large as the English Wikipedia (2.5B tokens);\n• consist of high quality pages based on some criteria that can be enforced at said\nmagnitude;\n• be expandable with new data and it should be easy to regenerate from raw data if\nthe processing pipeline changes;\n• be, most importantly , freely available for NLP practitioners.\n4.1.2 Design considerations\nRepresentativeness\nT raditionally , corpus compilation is preceded by careful consideration of various factors in\norder to ensure representativeness of the final product (Biber, 1993). These include the\namount and type of texts to include, a (possible hierarchical) categorization of the text\ninto genres and subcorpora, and well-defined population boundaries, such British English\ntexts from 1961 2 for the Lancaster–Oslo/Bergen (LOB) corpus (Johansson et al., 1978).\nWith well defined population(s), it possible to select sampling rates to ensure that the\nresulting corpus represents the targeted language well.\nWhile not a trivial undertaking, such a rigorous text selection process is feasible on the\nscale of the LOB or the Brown corpus (F rancis and Kucera, 1979) (cca. 1M words). As\ncorpora got bigger and the web became the main source of text, representativity proved\nunattainable and the focus moved on to the much more laxly definedbalancedness (Váradi,\n2002). Finally , if we fast-forward to the last 1–2 years, we can see state-of-the-art T rans-\nformer LMs trained on web crawl data blithely ignoring any attempt at representativity\nor balancedness.\n2https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/\nlist/private/LOB/lob.html\n80\nIn this work we follow the latter practice, and leave the compilation of a balanced\nsubcorpus for future work.\nT ext organization\nModern, contextual language models are able to learn long-distance dependencies in text.\nHowever, many of these dependencies, such as topics and coreferences, are inter-sentential\nin nature. As a result, training contextual language models necessitates that the document\nstructure remains intact.\nThe main goal of our corpus is to serve as training data for statistical NLP models. This\nsignificantly decreases the importance of document metadata, which would be essential\nfor corpus linguistic research. In keeping with the main goal, we include only the basic\nproperties: URL, domain, date of acquisition, etc.\nQuality\nEven if we do not aim for representativeness, document quality remains an important\nfactor. T ransformer LMs try to ensure quality by using data sources with at least some\neditorial control: BER T was trained on Wikipedia and e-books (Devlin et al., 2018), and\nRoBER T a’s sources include newswire texts (Liu et al., 2019). Since such corpora are\ngenerally hard to come by , especially on web scale, they are usually augmented with data\nacquired from noisier sources, which are then cleaned with heuristic methods. Examples\ninclude W ebT ext, used to train GPT-2, which includes web pages linked from Reddit\n(Radford et al., 2019); or Stories, pages filtered from Common Crawl that resemble the\nstructure of Winograd schemas (T rinh and Le, 2018).\nF or Hungarian, we face the additional issue of not having access to semi-edited corpora\non the same scale. W e are including the Hungarian Wikipedia in our corpus, but it is only\nabout 7% of the English edition, and the situation is similar with other resources, such as\nProject Gutenberg 3. Indirect quality indicators, like Reddit karma, are also unavailable.\nIn light of this, we decided to use a threshold on document length as the quality metric.\nWhile far from perfect, this heuristic allows us to filter microblogs, news headers, and\nautomatically generated content, such as soft error messages (i.e. those unmarked by\nHTML error codes).\n3https://www.gutenberg.org/\n81\nUniqueness\nF or the sake of simplicity , and so as to be able to use the URL as a document ID, we\ndecided to keep a single copy of each document. In Common Crawl, however, a URL can\nbe included in any number of the monthly crawls, which raises the question: don’t we\nlose valuable data by ignoring later snapshots of the same page? It is hard to say for sure\nwithout a thorough study , but we argue that the pages whose content changes frequently\nmost likely contain data we are less interested in. An example would be the front pages of\nonline news outlets, which consist of the headers of the latest news items. Such fragmented\ncontent would not serve the goal of LM training very well, as opposed to the full text of\nthe individual news items themselves (which are hopefully in the crawl as well).\nF uture-proofing\nOnce compiled, regular corpora never change. Our ambition is to keep our corpus up-to-\ndate, which requires some future-proofing.\nFirst, we do not believe that our current processing pipeline is perfect, and it is possible\nthat some of its parts (e.g. boilerplate removal, deduplication) might be replaced with\nbetter alternatives in the future. In order to allow the quick (as quick as size permits)\nregeneration of the corpus in such cases, Indig ( 2018) has already argued for the storing\nof raw HTML data. Since our pipeline is much longer, we opted for storing the results of\nthe intermediary operations as well.\nSecond, we plan to incrementally extend our corpus with each monthly snapshot of\nthe Common Crawl. In order to do that, not just the corpus, but temporary working files\n(URL cache, the LSH database, etc.; see Section 4.4) have to be retained as well.\nLicensing\nW e intend the corpus to be freely available, in the same way its predecessor, W ebcorpus,\nor the upstream Common Crawl are. Since the crawl data comes from Common Crawl,\nwe publish them under the same terms of use. Our additions, mainly the morphological\nanalysis and lemmatization performed on the documents, as well as the Wikipedia part\nof the corpus is licensed under the same Creative Commons Attribution-ShareAlike 4.0\nInternation (CC BY-SA 4.0) 4 license that Wikipedia itself uses.\nUnfortunately , the introduction of the General Data Protection Regulation (GDPR)\n(Council of European Union, 2016) creates uncertainties as to the legality of web crawls.\nNo clear guidelines have yet been published that address this very issue, which gives rise to\n4https://creativecommons.org/licenses/by-sa/4.0/\n82\nvarious interpretations. In the worst case, crawlers could be required to request permission\nfrom the copyright owner of each source – an impossible requirement at web scale (it is an\ninteresting question whether a permissive robots.txt constitutes consent). In any case,\nwe publish the corpus in good faith and so implement a take down policy that allows\nowners of copyrighted material to request removal of their data.\nThe corpus can be downloaded fromhttps://hlt.bme.hu/en/resources/webcorpus2.\n4.2 Related work\nHungarian is in a fortunate situation, as there are several good quality corpora available\nfor it. In this section, we first review these earlier efforts, then introduce Common Crawl\nand the studies that used it as a training corpus for language models.\n4.2.1 Preexisting corpora\nW e have already introduced Szeged T reebank, W ebcorpus 1.0 and the Hungarian Gigaword\nCorpus (MNSZ2) in Section 2.2. The first two are well below the one billion word mark.\nMNSZ2 is a proper gigaword corpus, but at 1.5 billion tokens, it is still smaller than the\nEnglish Wikipedia used to train BER T (see Section 1.7.4).\nThe largest Hungarian corpus to date is the huT enT en corpus (Jakubı́ček et al.,2013;\nSuchomel; Pomikálek, et al., 2012). It is part of the T enT en family of corpora5 by Sketch\nEngine and comprises of “almost 2.5 billion words” crawled from the web in 2012 6. While\nthe corpus is of the right size and it is the type of data we are after, it is unfortunately\nbehind a paywall.\nThere are two Hungarian corpora based on Common Crawl. OSCAR (Ortiz Suárez\net al., 2019), created by Inria7, is a multilingual corpus compiled from the November 2018\ncrawl data. It has 166 language-specific subcorpora of various sizes. The deduplicated\nHungarian subcorpus is 2.3 billion words in size. Unfortunately , OSCAR is shuffled at the\nline level, so language models trained on it will not be able to take advantage of long-range\ndependencies in real documents.\nPreliminary work on a Hungarian Common Crawl corpus was started by Indig ( 2018).\nThe work aimed at downloading the whole Hungarian “vertical”, i.e. Hungarian pages from\nall monthly snapshots. Due to technical difficulties, deduplication was not performed, and\nthe end product has not been published. However, the files contain the documents in their\n5https://www.sketchengine.eu/documentation/tenten-corpora/\n6https://www.sketchengine.eu/hutenten-hungarian-corpus/\n7https://www.inria.fr/en\n83\nentirety and the code that came with the paper is free and open-source; hence, we used\nthis work as the basis for our own. The author of the paper generously provided us with\ntheir corpus, which we used to bootstrap ours.\n4.2.2 Common Crawl\nThe Common Crawl (CC)8 is a web crawl archive, continually created and maintained by\nthe Common Crawl F oundation with the aim of “democratizing access to web information\nby producing and maintaining an open repository of web crawl data that is universal ly\naccessible and analyzable ”9. The data is available on Amazon S3 10 in (roughly) monthly\ndigests since November 2013, in the following formats:\n• raw HTML in W eb ARChive (W ARC) format\n• metadata in W A T files\n• plaintext extracted from HTML in WET files.\nIt is not trivial to ascertain the size of the Common Crawl corpus. The homepage\ndoes not contain exact numbers. Based on the listing in the Wikipedia page 11, in August\n2018 the corpus consisted of about 129 billion pages for a total of 8.8 PiB of uncompressed\ncontent. Still, it is not clear whether these numbers include only the W ARC archives, or\nthe W A T and WET files as well.\nAside from the main archive files, monthly snapshots also include a URL index. This\ncontains the W ARC file and offset of each URL in the snapshot, allowing a URL-based\nprefiltering of the data prior to downloading. In our case, we only keep URLs in the .hu\ndomain.\nIt goes without saying that the sets of Hungarian pages and pages from Hungary\ndo not overlap completely . On the one hand, foreign-language pages, such as English\nversions of organizational webpages are served under .hu. On the other, with about\n25% of native speakers living outside Hungary (Balázs, 2013), a significant number of\nHungarian electronic text sources are found under foreign top-level domains. The first\nissue can be dealt with language detection; the second we leave for future work.\n8http://commoncrawl.org/\n9https://commoncrawl.org/about/\n10https://aws.amazon.com/s3/\n11https://en.wikipedia.org/wiki/Common_Crawl#History_of_Common_Crawl_data\n84\n4.2.3 As a training corpus\nThere are differing accounts as to the effectiveness of Common Crawl as a training corpus.\nIt has been used successfully to train various flavors of language models, include n-grams\n(Buck et al., 2014), static (Pennington et al., 2014) and contextual embeddings (Baevski\net al., 2019), both in mono and in multi-language setups (Grave et al.,2018). In particular,\nboth Baevski et al. ( 2019) and Buck et al. ( 2014) report better results for their systems\nwhen trained on CC as opposed other corpora news task in WMT 2018 (Bojar et al.,\n2018),\nY et there are also concerns about the quality of text in CC. T rinh and Le ( 2018)\nfound many documents whose “content are mostly unintelligible” . They arrived at a good\ntraining corpus by keeping only the documents that were similar in structure to their\ntarget task, the Winograd Schema Challenge (Levesque et al., 2011).\nOur main takeaway from this section is that Common Crawl is a very useful resource\nthat can be used to train state-of-the-art systems provided there is some form of quality\nassurance in place.\n4.3 Architecture\nDue to the sheer size of the Common Crawl, processing it for any purpose requires a lot of\ncompute and storage. Derived corpora are created either by utilizing massive map-reduce\nclusters (Panchenko et al., 2018) or from a subset of CC, such as a single monthly snapshot\n(Grave et al., 2018; Ortiz Suárez et al., 2019).\nAs our targeted data (Hungarian pages across all monthly releases) is comparable in\nsize with a monthly crawl, it is still possible to process it using a small cluster of machines\n– with a few caveats.\n4.3.1 Computing environment\nOur cluster consists of four desktop-grade machines with 4 hyperthreading (HT) cores at\n4GHz and 32GB memory , and a single server machine with an 8 HT core Xeon processor\nthat clocks at 2.4GHz. The latter compensates its slower CPU with 217GB RAM.\nAll machines are connected to the LAN over a 10Gb connection. Data is kept on a\n96TB network file system; the local disks are only used for storing code and execution\nenvironments.\nThe machines run Linux. W e implemented all steps of our pipeline in Python 3.6.\n85\nMiniconda12 was used to ensure an identical Python environment on all hosts.\nNote that our cluster setup has a few peculiarities, which we take advantage of. The\namount of memory on our main machine might not be readily available in homogeneous\nclusters. Having a large NFS share is by no means unheard of, but is also not a given.\nSince some of our scripts (to be introduced in Section 4.4) depend on these resources, they\nmight have to be reworked to some extent if the code is to be used in a different cluster\nsetup.\n4.3.2 The pipeline\nInstead of putting much engineering effort into building a complex processing pipeline\n(that works on a cluster of machines in a distributed fashion), we decided to employ\nsimple Python scripts for each step. F ollowing the Unix philosophy “do one thing and do\nit wel l”13, each script performs a single task on a single machine. The scripts take their\ninputs from the shared file system, process them in parallel and write the results back to\na separate directory . This allows us to snapshot the compilation process, thereby ensuring\nreproducibility of successive steps (see Section 4.1.2).\nParallelism is of utmost importance when processing data at web scale. While we do\nnot employ a map-reduce framework, the basic ideas translate well to our setup. Our\nscripts can also be classified into ‘ map’ and ‘ reduce’ types. The former transform the\ndata document-by-document; the latter perform aggregations. Examples for ‘map’ tasks\ninclude boilerplate removal and filtering, while document deduplication is a ‘reduce’ oper-\nation. Most reduce tasks are run on the main machine of our cluster where they can take\nadvantage of the additional memory .\nMap tasks are run in parallel on all machines in the cluster. The data is distributed\ninto per-host directories so that the input/output directory-based logic of the scripts can\nremain unchanged. W e used Ansible14 to distribute the tasks, which are executed in a\ntmux15 multiplexer. Logs are saved to the shared file system. Collecting the per-host\noutputs is as simple as copying them to an “aggregate” output directory . However, this is\nonly necessary if the subsequent task is a reduce operation; a next map task can simply\ntake the output of the last one as its input.\n12https://docs.conda.io/en/latest/miniconda.html\n13https://en.wikipedia.org/wiki/Unix_philosophy#Do_One_Thing_and_Do_It_Well\n14https://www.ansible.com/\n15https://github.com/tmux/tmux/wiki\n86\n4.4 Running the pipeline\nThis section elucidates the various steps in our corpus compilation pipeline. The effect of\neach step on the corpus size is listed in T able 4.2 for the bootstrap corpus and T able 4.1\nfor our additions up to April 2019 at the end of this section. The former starts at the\n“Boilerplate removed” step, because that is the dataset we received.\nIdeally , the Common Crawl archives should be processed month-by-month to minimize\nthe amount of new data that needs to be processed at the same time and to maximize the\nfiltering effect of deduplication steps based on the already downloaded monthly batches.\nIn practice, we had to deal with much larger data sizes for two reasons. First, we already\nhad in our possession a bootstrap corpus that we had to process in bulk. Second, we\ndid not have the benefit of hindsight, and on occasion processed as much as 11 monthly\narchives in one batch. While this means that our scripts are proven to work under a much\nlarger load, it also resulted in a few suboptimal design choices. W e return to this issue in\nSection 4.7.\n4.4.1 Download\nThe download step is largely identical to the one described in Indig ( 2018). The one\nsignificant change we decided to make was to move boilerplate removal to a separate\nscript. Our reason for decoupling the two steps was twofold: first, it allows us to keep\nthe raw crawl data; second, it leaves the door open for trying other boilerplate removal\nmethods in the future. F or the intricacies of the downloading process, the reader is referred\nto Indig ( 2018).\n4.4.2 Boilerplate removal\nW eb pages (including those downloaded by and from Common Crawl) incorporate not only\ntextual content, but various forms of boilerplate: scripts, navigation elements, headers and\nfooters, etc. Most pages are also marred by an overabundance of advertisements.\nAs is well known, boilerplate causes problems when included in a search index or text\ncorpora (Kohlschütter et al., 2010). It is, therefore, important to detect and extract only\nthe main content from the raw HTML. Unfortunately , this is not a trivial task and some\n(typically , textual) boilerplate is expected to have made its way into the corpus.\nThere are several tools available for boilerplate removal. In order to maintain compat-\nibility with the bootstrap corpus, we decided to use Justext (Pomikálek, 2011). Justext\nhas the additional benefit of splitting the text into paragraphs, a subdivision we maintain\n87\ngoing forward.\n4.4.3 Content-based filtering\nW e already emphasized the importance of quality in Section 4.1.2. T wo filtering steps\nwere applied to the downloaded corpus in order to improve its quality .\nLanguage-based filtering T o remove non-Hungarian content, language identification\nwas performed on the corpus. W e used the CLD216 and langid.py17 (Lui and Bald-\nwin, 2012) libraries, the former via the cld2-cffi18) wrapper. Our primary tool\nwas CLD2, as langid.py proved too slow to consider, given the size of the corpus.\nHowever, it was kept in the toolchain as a backup for texts that CLD2 could not\nreliably identify .\nLanguage identification was run on a per-paragraph basis. This allowed us to handle\ndocuments which were mostly in Hungarian, but included foreign-language para-\ngraphs (quotations, translations, etc.). Had we done the filtering on a document\nlevel, such documents would have been either removed entirely , or left in the corpus\nwith the foreign text intact; either outcome would have been undesirable.\nLength filtering As mentioned in Section 4.1.2, we set up a document length threshold\nto catch “low quality” pages. In absence of a theoretically justified value, we heuris-\ntically decided on a minimum length of 1500 characters, which roughly corresponds\nto three paragraphs of text. Documents below the threshold were dropped from the\ncorpus.\n4.4.4 Deduplication\nW eb crawl data shows a high degree of duplication on several levels. W e already mentioned\nin Section 4.1.2 how a URL can appear many times in the data. W e assume that each\noccurrence of the same URL points to the same document, and discard all but the first\ninstance.\nEven if we only consider pages with different URLs, we are still left with a lot of\nmatching content. These come from three main sources:\n• variations of the same URL with different parameters might point to the same page;\n16https://github.com/CLD2Owners/cld2\n17https://github.com/saffsd/langid.py\n18https://pypi.python.org/pypi/cld2-cffi\n88\n• the same content can be replicated under different URLs, such as the news taken\nverbatim from MTI, the Hungarian news agency by news websites;\n• sometimes duplication only affects certain paragraphs. Examples include site-specific\nboilerplate or paragraphs copied from another page.\nThe first two cases, while different, can be tackled with document-level deduplication.\nW e do not perform paragraph-level deduplication, as it would disrupt the text flow in docu-\nments. The only exception is site-specific boilerplate paragraphs, such as the introductory\npanel in the sidebar on the blog.hu pages, and intra-document paragraph repetition.\nThe sections below describe these deduplication steps in a little more detail.\nURL deduplication\nW e performed URL deduplication on the monthly URL indices, so as not to download du-\nplicate documents needlessly . Given the roughly month-by-month processing of Common\nCrawl, there were two separate deduplication (sub-)steps, within-month and across-month.\nFiltering URLs from earlier indices. T o this end, we created a URL list file that\nkeeps track of all URLs we have downloaded previously . When a monthly index is\nfetched, this file is consulted first, and all URLs in it are removed from the index.\nThis step can be trivially parallelized.\nDeduplicated URLs in the same index. It is not uncommon to see the same URL\nmore than once even in a single monthly index. Deduplicating a single dataset is\nan inherently sequential process. In order to do it in parallel we employ a Redis19\ndatabase to keep track of each URL seen in any of the index files. All Python\nprocesses have concurrent access to the database. The first process to see a URL\nregisters it in Redis, and any further occurrences are dropped from the index. In\nthe end, all URLs in the database are added to the static URL list mentioned in the\nprevious paragraph, and hence automatically skipped in future indices as well.\nDocument deduplication\nDeduplication is a much less precise matter for documents than it is for URLs, as usually\nwe are not looking for exact matches. Even downloading the same webpage from the same\nURL twice might results in slightly different documents due to dynamic elements on the\npage, such as a simple clock; and a document that plagiarizes another might introduce\n19https://redis.io/\n89\nmore subtle differences. The standard practice of finding such near duplicates is to convert\ndocuments into sets and look for sets with a relatively large intersection (Leskovec et al.,\n2014, ch. 3). Due to the size of the corpus, both steps must use approximate methods to\nremain feasible.\nMinHashing MinHash (Broder et al., 1998) is an algorithm that converts documents\ninto a set of n-grams. Instead of working with the whole, potentially very large set,\na fingerprint of the document is created from several permutations of the set. W e\nused 256 permutations computed from character 5-grams.\nCompared to e.g. language filtering, MinHash is computationally expensive, but it\ncan be similarly parallelized (i.e. it is a ‘map’ task).\nLocality-sensitive Hashing (LSH) A full deduplication of a corpus with |C|docu-\nments would require O(|C|2) comparisons, already impracticable at much lower\nscales than ours. W e used Locality-sensitive Hashing (Gionis et al., 1999; Indyk\nand Motwani, 1998), an approximate method, to speed up fingerprint matching.\nDocuments that produce a Jaccard similarity over 0.95 to an already processed doc-\nument are deemed duplicates and are removed from the corpus.\nSimilarly to URL deduplication, LSH is performed in two steps. First, pages in the\nmonthly crawl are deduplicated against documents already in the corpus; in order to\ndo this, a database with the minhashes of all documents is maintained. Second, the\nremaining documents in the monthly batch are deduplicated against one another.\nBoth steps above were implemented with the datasketch20 library .\nSite-level deduplication of paragraphs\nSome sites contain “boilerplate” paragraphs that occur in many of their documents. This\nkind of repetition adversely affects all language models: their presence disrupts text cohe-\nsion and makes count-based methods overestimate the importance of words in the duplicate\nparagraphs. Their detection and removal is, however, more involved than the document\ndeduplication case described above. It consists of the following steps.\nIndexing In order to detect paragraphs common to pages on a site, the documents first\nmust be grouped by network domain. T o avoid sorting the whole corpus, we create\nan index of the documents and sort only the index. It is then partitioned so that\ndocuments in a domain are not separated, and the partitions are distributed to each\nmachine in the cluster for parallel processing.\n20https://github.com/ekzhu/datasketch\n90\nF requent paragraphs The index shards are processed domain-by-domain to find “boil-\nerplate” paragraphs in each. Since at this point the HTML structure of the pages\nis no longer available, we redefine the task in terms of frequency: a paragraph is\n“boilerplate” if it appears in at least 50% of a site’s pages and at least in three.\nOnce again, we depend on MinHash and LSH for efficient near-duplicate finding.\nAs a domain may contain an arbitrary large number of paragraphs, keeping the\nMinHash of each in memory is infeasible. Instead, we reconceptualize the task as\nfrequent item search in streams. Each domain can be considered a stream of doc-\numents; the items we count are the paragraphs. W e implemented the algorithm in\nLeskovec et al. (2014, sec. 4.7.3), which uses a decaying window to limit the number\nof paragraphs in memory at any given time. While this is an approximate algo-\nrithm, and therefore lossy , it works well in practice, as the number of “boilerplate”\nparagraphs on any given site is bound to be small, typically one.\nFiltering Once we have the frequent paragraphs for each domain, we collect those that\noccur in at least 50% of the site’s pages and filter them from the corpus. Using the\nstream analogy again, we keep the first occurrence of each “boilerplate” paragraph\nand discard the rest.\nAs before, the frequent paragraph list is not thrown away after filtering, but is stored\nbetween the monthly expansions of the corpus. W e consider domain “streams” continuous\nbetween monthly crawls and always initialize the stream history from the list saved in the\nlast month.\nDocument-level deduplication of paragraphs\nThere is yet another form of paragraph duplication, not addressed above: when paragraphs\nare repeated within a document. Occasionally , this repetition may come naturally from\nthe logic of the text, but most of the time, it is purely an artifact of (bad?) HTML page\ndesign (e.g. including the same content once for static and once for dynamic presentation)\nand Justext’s inability to cope with it.\nT o clean up such pages, we included a script that deletes all repeated paragraphs within\nthe same document. As opposed to the steps above, the scripts only filters exact duplicates\nto minimize the chance of deleting near duplicates included in the text for pragmatical\nreasons.\nBoth paragraph deduplication steps decrease the length of the documents affected,\npossibly below the threshold established in Section 4.1.2. Such documents are removed\nfrom the corpus by re-running the length filtering script.\n91\n4.4.5 Linguistic analysis\nAs a last step, the corpus is annotated with theemtsv21 (Indig et al., 2019) pipeline. Due to\nthe computational cost associated with the higher-level components (parsing, NER, etc.),\nonly tokenization, morphological analysis and lemmatization was performed. W e handled\neach paragraph separately to the pipeline. At the time of writing, the tokenizer included\nin emtsv, quntoken22 (Mittelholcz, 2017), handled short documents very inefficiently , and\nit quickly became a bottleneck. W e fixed this issue in our own fork 23, and also provided\na Python interface to quntoken via the Python C API. As our changes have not yet been\nmerged into the main line, all scripts use the fork above.\nemtsv outputs a tsv file with separate columns for each annotation. Each line contains\nthe annotations for a single token, with empty lines between sentences. W e expanded\nthis format with CoNLL-U Plus 24-style comments to store the document URL and the\nraw untokenized text in the file as well, so that the original text-only format can be\nreproduced.\nDataset Documents Paragraphs W ords Bytes / Characters *\nF ull index 244,544,787 4,521,082,715,740\nFiltered index 181,504,355 2,915,894,695,649\nDeduplicated index 68,940,595 1,182,608,541,578\nDownloaded 69,065,628 6,147,433,875,452\nBoilerplate removed 36,396,615 446,695,117 15,368,435,601 115,037,549,741\nLanguage filtering 35,832,801 429,280,674 15,204,586,348 113,955,430,850\nLength filtering 15,063,882 310,009,838 13,240,540,507 99,222,973,992\nDocument deduplication 9,965,023 189,777,575 8,637,317,522 64,975,071,293\nF requent P filtering 9,062,484 158,772,773 7,194,048,746 53,790,414,086\nSame-doc. P dedup. 9,052,140 153,732,983 7,086,763,519 52,986,174,365\nFinal length filtering 8,326,246 149,945,430 6,989,318,926 52,263,895,796\nT able 4.1: Effects of the various filtering steps on corpus size (2018) ( *Number of com-\npressed bytes above the middle rule; number of characters below)\n4.4.6 Final statistics\nThe almost valid CoNLL-U Plus format described in the last section is the canonical\nformat of our corpus. W e split up the corpus into files of 2,500 documents each to make\nit easier to download and work with the corpus; it also enables prospective users to scale\n21https://github.com/dlt-rilmta/emtsv\n22https://github.com/dlt-rilmta/quntoken\n23https://github.com/DavidNemeskey/quntoken/tree/v1\n24https://universaldependencies.org/ext-format.html\n92\nDataset Documents Paragraphs W ords Characters\nBoilerplate removed 85,744,991 118,0528,666 44,648,104,301 333,609,180,321\nLanguage filtering 84,144,867 951,425,101 43,535,586,773 326,744,152,415\nLength filtering 33,231,844 785,179,604 39,053,562,970 292,548,775,478\nURL deduplication with 2018 13,917,392 280,518,344 10,414,399,879 76,099,744,178\nDocument deduplication† 3,587,552 64,064,899 2,408,994,751 17,555,305,848\nDocument deduplication with 2018 † 2,905,457 52,885,668 1,931,680,268 14,092,687,812\nF requent P filtering 2,594,896 47,242,751 2,159,385,560 16,061,541,290\nSame-doc. P dedup. 2,591,241 45,718,462 2,126,606,691 15,815,393,328\nFinal length filtering 2,412,516 44,706,281 2,101,516,357 15,628,819,466\nT able 4.2: Effects of the various filtering steps on corpus size (2013–2017) ( †A small\nnumber of duplicate URLs remained in this dataset)\nthe data they wish to work with. The order of the documents is randomized to make the\neach minibatch as representative of the whole corpus as possible, which is very important\nfor stochastic gradient-based learning algorithms (Bengio, 2012).\nThe size of the final corpus is shown in T able 4.3. With a little over 9 billion tokens,\nthe corpus is about 3.5 times the size of huT enT en and 4 times the size of the deduplicated\nOSCAR corpus. The Wikipedia subcorpus, described in the next section, is also included\nin the table for comparison. As can be seen, with about 56 times as many tokens, the\nCommon Crawl portion of the corpus dwarfs Wikipedia’s contribution. Y et the usefulness\nof a clean, semi-edited source should not be overlooked.\nSource Documents Paragraphs W ords Characters\nCommon Crawl 10,738,762 194,651,711 9,090,835,283 67,892,715,262\nWikipedia 301,312 7,513,420 163,450,200 1,118,833,863\nSum 11,040,074 202,165,131 9,254,285,483 69,011,549,125\nT able 4.3: The size of the current version of W ebcorpus 2.0\nT able4.4 lists all domains with at least 0.5% of the total number of documents in\nthe corpus. As can be seen, the top is dominated by news outlets (8.81%), blog services\n(7.23%) and document players (4.17%), which host longer documents such as legal texts\nand slides. At a glance, the top domains are of good quality and the content distribution\nseems useful for a training corpus. Blogs, news and legal documents will probably be three\npillars of a future balanced corpus as well.\nThe presence of the e-commerce site arukereso.hu in the list is questionable, and\nneeds further investigation.\n93\nDomain Pages Percentage\nblog.hu 589,346 5.49%\ndocplayer.hu 384,136 3.58%\nindex.hu 199,790 1.86%\n24.hu 187,686 1.75%\nhvg.hu 131,918 1.23%\nblogspot.hu 130,690 1.22%\ndelmagyar.hu 114,585 1.07%\nkisalfold.hu 106,402 0.99%\nnapi.hu 75,487 0.70%\narukereso.hu 70,789 0.66%\norigo.hu 68,465 0.64%\nslideplayer.hu 63,701 0.59%\nfidelio.hu 61,818 0.58%\nxfree.hu 56,068 0.52%\nwebbeteg.hu 55,510 0.52%\nT otal 2,296,391 21.4%\nT able 4.4: T op domains in terms of the number of documents\n4.5 Wikipedia\nIn order to use as many semi-edited sources as possible, we are including the Hungarian\nWikipedia (WP)25 in W ebcorpus 2.0. Wikipedia is not incorporated into Common Crawl,\nso it had to be obtained and processed separately . The code that implements the steps\nbelow can be downloaded from GitHub 26.\n4.5.1 Wikihopping27\nExtracting text from Wikipedia proved more challenging than we previously imagined.\nOn the face of it, the procedure is simple: the Wikimedia F oundation regularly publishes\ndumps for each language. These dumps can be downloaded and processed with any of\nthe third party parsers28 available. As we have learned, however, nothing could be farther\nfrom the truth. W e include our whole Odyssey towards a working solution to help readers\nwho find themselves in the same boat.\nAs it turns out, while many parsers can extract data from infoboxes on WP pages,\nonly WikiExtractor29, also endorsed on the BER T GitHub page30, can parse the main\ntext. Unfortunately , WikiExtractor fails to expand some templates in the text, which\n25https://hu.wikipedia.org/wiki/Kezdőlap\n26https://github.com/DavidNemeskey/zim_to_corpus\n27https://www.urbandictionary.com/define.php?term=wikihopping\n28https://www.mediawiki.org/wiki/Alternative_parsers\n29https://github.com/attardi/wikiextractor\n30https://github.com/google-research/bert\n94\nmakes certain elements, such as numbers with units (e.g. 40 miles) to disappear from the\noutput31.\nNot wanting to lose sentence cohesion, we turned next to the official MediaWiki soft-\nware. While there is no way to extract text from the dump directly , there are two tools\nthat can import the dump to a local MediaWiki server, from which the pages can be\nqueried via a REST API. In practice, the built-in importDump.php is prohibitively slow,\nand MWDumper32, while much faster, failed to build a valid local WP mirror.\nThe final solution came in the form of parsing not the MediaWiki source, but the HTML\npages themselves. Unfortunately , Wikimedia discontinued the distribution of static dumps\nin 2008; however, we found another source in the form of the Kiwix ZIM archives33. These\narchives are updated less frequently (every 1–2 years), but are freely available and they can\nbe parsed with the open source libzim34 library . W e converted the 2017 Hungarian WP\nZIM archive into a set of files in the JSON Lines 35 format. Disambiguation and redirect\npages were purposely omitted from the output, as they contain no coherent text.\n4.5.2 Processing\nThe HTML source of Wikipedia pages is free of advertisements and similar boilerplate,\nand extracting the main text is straightforward via CSS selectors. Data duplication is also\nnonexistent, as each page corresponds to a different topic, usually a named entity . As a\nconsequence, we could arrive at a good quality corpus with only two preprocessing steps.\nFirst, sections that consist only of enumerations (e.g. Lásd még ‘See also’ or F orrások\n‘Sources’) are not useful for language modeling. T o filter them, we assembled a list of\nsection titles that 1. usually occur around the end of the document (last 1–2 paragraphs)\nand 2. contain only a list 80% of the time. While the numbers are ad-hoc, the resulting\nlist seems to include most of the worst offenders. The listed sections were then removed\nfrom all documents.\nSecond, low quality pages were also removed; the metric, once again, was length. Doc-\numents shorter than 100 words were deleted from the corpus; these mostly concerned one-\nsentence descriptions of asteroids, probably autogenerated from an astronomical database.\nAs with the Common Crawl part, the filtered corpus was converted to tsv format, and\nmorphologically analyzed and lemmatized with emtsv. Since here we parsed the HTML\nformat directly (as opposed to having it done for us by Justext), we could give meaningful\n31https://github.com/attardi/wikiextractor/issues/189\n32https://www.mediawiki.org/wiki/Manual:MWDumper\n33https://www.mirrorservice.org/sites/download.kiwix.org/zim/wikipedia/\n34https://github.com/openzim/libzim\n35http://jsonlines.org/\n95\nIDs to the paragraphs based on their location in the DOM. W e use these IDs to convert the\ntsvs into popular language modeling formats, such as BER T, which requires the removal\nof lists and headers, and WT2.\nDataset Documents Paragraphs Sentences W ords\nF ull data 418,673 8,197,320 13,952,709 169,541,528\nLength filter 301,312 7,513,420 13,098,808 163,450,200\nBER T 260,181 1,996,905 6,093,173 114,093,719\nT able 4.5: Effects of the various filtering steps on corpus size (Hungarian Wikipedia)\nThe full WP contains 418,673 pages; as T able 4.5 shows, removing short documents\nleft us with about 72% of that, though we only lost 3.5% of tokens. Conversion to the\nBER T format reduces the token count by another 30%.\n4.6 huBERT\nOne of our main goals with the corpus was to train a Hungarian BER T model, nicknamed\nhuBERT. Unfortunately , at the time of writing, no results were available yet. In this section,\nwe present preliminary work based on the Wikipedia subcorpus.\n4.6.1 Pretraining\nAs we have seen in 1.6.4, training of large T ransformer models is prohibitively costly\nfor smaller research groups; and in Hungarian NLP , all groups can be considered small.\nLuckily , the Wikipedia subcorpus is small enough that a BER T Base model can still be\npretrained on a single TPU. W e had access to a small number v3 TPUs via Google’s\nT ensorFlow Research Cloud36 program, which allowed us to train the Wikipedia BER T\nmodels in a couple of days.\nBER T models usually come in two flavors:cased and uncased. The former operates on\nunprocessed text; in the latter, tokens are lower cased and diacritical marks are removed.\nIn keeping with this practice, we also trained two variants. However, as diacritics are\ndistinctive in Hungarian, we could not afford to lose them, and replaced the uncased\nmodel with a lower cased one.\nAs is the case with the English BER T, our models are all pretrained on sequences of\nup to 512 wordpieces. Since the training configurations published in the literature are for\n36https://www.tensorflow.org/tfrc\n96\nmuch larger corpora, they are not directly adaptable to our case. Hence, we experimented\nwith different training regimens for both the cased and lower cased variants:\n1. Three models were trained with full-length sequences for 50,000, 100,000 and 200,000\nsteps. These roughly correspond to 90, 180 and 360 epochs, respectively;\n2. F ollowing the recommendation in the BER T repository , one model was trained with\na sequence length of 128 for 500,000 steps (600 epochs) and with a sequence length\nof 512 for an additional 100,000 steps (or 180 epochs).\nAll models were trained with a maximum learning rate of 10− 4 and the maximum possible\nbatch size: 1024 for the model with 128-long sequences and 384 for the rest.\nModel Seq. length Steps Hours Masked LM Next sentence\nCased\n512 50,000 13 0.5544961 0.97125\n128 500,000 59 0.6669028 0.995\n512 +100,000 25 0.6657926 0.99\nLower\n512 50,000 13 0.5538445 0.9825\n512 100,000 25 0.6100383 0.9975\n512 200,000 50 0.6273391 0.9975\n128 500,000 59 0.6425686 0.99125\n512 +100,000 25 0.665879 0.9975\nT able 4.6: T raining times and accuracies of the different BER T models on the two training\ntasks\nT able4.6 compares the different configurations. In the cased case, the TPU went down\nfor maintenance during the training, so the 100,000 and 200,000-step models are missing\nfrom the results. Even without them, several observations can be made. First, the 50,000-\nstep models clearly underfit the data, even though they were trained for twice as many\nepochs as the English BER T. On the other hand, the difference between the 100,000 and\n200,000-step models is much smaller than between the 50,000 and 100,000-step models,\nsuggesting a performance peak around 300,000–400,000 steps.\nSecond, in line with the findings of Lan et al. ( 2019) and Liu et al. ( 2019), the next\nsentence prediction task seems very easy , as all but the first models attain over 99%\naccuracy . In contrast, the masked LM task proved much harder, and its accuracy seems\nrather low. Unfortunately , the evaluation results for the English BER T are not published\nanywhere, which makes it difficult to put the numbers in context. Based on the diminishing\nreturns, the longest-trained models are likely to be close to the maximum achievable on\nthe Wikipedia subcorpus.\n97\nFinally , our experiences confirmed that the two-stage training regiment recommended\nin the BER T repository leads to better results. The rationale behind this method is that\nthe first phase trains most of the model weights and the second phase is “ mostly needed to\nlearn positional embeddings, which can be learned fairly quickly”37. While this seems to be\nthe case for the cased model, the masked LM accuracy of the lower cased model improved\nby more than 2% in the second phase, indicating that substantial learning still happens\nat this stage.\n4.6.2 Evaluation\nBER T models are usually evaluated on high-level natural language understanding tasks,\nsuch as question answering or textual entailment. Unfortunately , no Hungarian benchmark\ndatasets exist for these tasks. Because of this, we evaluate our models by contrasting their\nperformance to the multi-language version of BER T in three ways:\n1. W e compare their accuracy in the two training tasks on a held-out portion of the\nWikipedia subcorpus.\n2. W e include our models in the emBERT module (see Chapter 5) and measure their\nperformance on named entity recognition and NP chunking.\n3. The quality of the tokenization vocabulary used by the models is also compared.\nT able4.7 presents the results of the first experiment. Both our cased and lower cased\nmodels achieve similar accuracies on the held-out set as on the training data, allaying any\nsuspicion of overfitting. In addition, the cased model clearly outperforms multi-BER T on\nboth tasks (multi-BER T is only available in the cased configuration). In fact, the accuracy\nof multi-BER T on masked LM is equivalent to a perplexity of about 50,000, which, given\nits vocabulary of 120,000 wordpieces, is little better than random. As multi-BER T was\nalso trained on Wikipedia, this abysmal performance is especially surprising.\nThe other two experiments will be discussed in Chapter5, but to summarize the results\nbriefly , our models outperform multi-BER T in both aspects, proving the effectiveness of\nnative Hungarian contextual embeddings over multi-language models and their necessity\nfor progress in Hungarian NLP .\nThe models can be downloaded from https://hlt.bme.hu/en/resources/hubert.\n37https://github.com/google-research/BERT/#pre-training-tips-and-caveats\n98\nModel Masked LM Next sentence accuracy\naccuracy loss accuracy\nCased 128–512 0.6414373 1.85 0.98625\nmulti-BER T 0.0857188 10.89 0.51625\nLower 128–512 0.6415611 1.84 0.99\nT able 4.7: Performance of our best models and multi-language BER T in the two training\ntasks on the held-out set.\n4.7 Conclusion and future work\nIn this chapter, we presented our work on a W ebcorpus 2.0, the largest Hungarian corpus\nto date. The corpus is composed of the Hungarian pages in the Common Crawl archives\nand a copy of the Hungarian Wikipedia. At around 10 billion tokens, the resulting corpus\nis 3.5–4 times as large as the previous record holders huT enT en and OSCAR.\nThe main purpose of the new corpus is to provide training grounds for modern T ransfor-\nmer-based language models. As preliminary work to a Hungarian BER T, we experimented\nwith a model pretrained on the Wikipedia subcorpus. Our model outperformed the multi-\nlanguage version of BER T in all experiments we conducted.\nThe largest remaining task is a BER T model built on the whole W ebcorpus 2.0. W e\nare working towards this goal, and intend to publish the results soon.\nA key issue is that of training text quality . BER T training corpora exclude headers\nand lists. W e followed the convention in the bootstrap corpus that each extracted text\nchunk is considered a paragraph, irrespective of its originating HTML tag. Since this gives\nus no information on how the paragraphs should be interpreted, we cannot put a similar\nfiltering step in place. The exception is the Wikipedia subcorpus, which does feature typed\nparagraphs. It turns out, however, that it is possible to extend this solution to the main\ncorpus as well, by using the xpath attribute of the paragraphs emitted by Justext. In the\nnext version of the corpus, we intend to make use of this feature to create a better quality\nBER T training corpus. Filtering lists and tables will probably result in the downsampling\nof domains like arukereso.hu as well.\nW e are grateful for Indig’s bootstrap corpus, which allowed us to start our work.\nHowever, having to deal with a huge dataset not yet deduplicated, and without the original\nW ARC sources, meant we could not fully operate on a month-by-month basis. This made\nthe code more complex, and as mentioned above, prevented us from changing the output\nof the boilerplate removal. In the next version of the corpus, we intend to re-download\nthe 2013–2017 archives and integrate them fully into our pipeline.\n99\nThe next version of the corpus might also explore some of the issues raised in this\nchapter, such as the compilation of a balanced subcorpus or the inclusion of Hungarian\npages from the Common Crawl archives of neighboring countries.\nLastly , the implementation also might need some overhaul. Python is a good language\nfor fast prototyping and due to its excellent third party library support. However, its pref-\nerence of multiprocessing over multithreading results in reduced performance, increased\nmemory usage or the need to introduce a database (see Section 4.4.4) for tasks that need a\nlarge shared resource (e.g. a URL index). Reimplementing the tasks affected in a language\nthat supports multithreading could speed them up considerably .\n100\nChapter 5\nemBERT: language modeling for NLP\nIn the previous chapters, we presented various ways in which “traditional” natural lan-\nguage processing can be used to evaluate or improve certain aspects of language modeling.\nNotably , modern language modeling depends on the existence of gigaword corpora, such\nas the one introduced in Chapter 4 – which utilizes numerous NLP tools, from language\nidentification and boilerplate removal to tokenization and morphological analysis.\nIn this chapter, we give an example of the opposite direction, when language modeling\nis used to improve an NLP system. Specifically , we train classifiers based on contextual\nembeddings for two NLP tasks: NP chunking and named entity recognition. Our work is\ncertainly not without precedent (see Section 1.7.5); however, to our knowledge, it is the\nfirst time such a study is conducted for Hungarian.\nMost of the content of this chapter was published in Nemeskey ( 2020), which has won\nSpecial A ward at the XI. Conference on Hungarian Computational Linguistics.\n5.1 Deep learning in NLP\nW e have already discussed how in the last couple of years deep, contextual embed-\ndings superseded traditional, manually compiled feature sets in most NLP tasks (see\nSection 1.7.5). In spite of this development, Hungarian text processing pipelines, namely\ne-magyar (Váradi et al., 2017) and magyarlanc (Zsibrita et al., 2013), still operate on\nmanual features. In this chapter, we introduce the emBERT module, which enables the\nintegration of sequence classifiers based on contextual embeddings into emtsv, the new\nversion of e-magyar. W e trained models for two tasks: NP chunking and named entity\nrecognition (NER). emBERT performs comparably to the best NER tagger for Hungarian,\nwhile achieving state-of-the-art performance on NP chunking.\nAnother key aspect of the deep learning revolution is the retirement of off-the-shelf\n101\nclassifiers in favor of custom, usually very large neural architectures. The exploration of\nhow much this development affects or will affect NLP is beyond the scope of this thesis; at\nleast for now, traditional machine learning models seem prevalent in NLP toolkits. While\nthe system described in this chapter uses a neural classifier, it is much simpler than the\nmaximum entropy Markov models or CRF s commonly used for token classification.\nThe results presented in Section 1.7.5 were all achieved by English-language models\non English benchmarks. Owing to the resource-hungry nature of training large contextual\nembeddings, only a handful of languages have followed suit1. In this chapter, we investigate\nif contextual embeddings are able to achieve similar results for Hungarian. Due to the\nlack of evaluation datasets akin to SQuAD or GLUE, we assess model performance on\ntwo token classification tasks: NP chunking and NER. The models are integrated into the\ne-magyar text processing system as a new module.\n5.2 BER T\n5.2.1 Why BER T?\nF rom the list of contextual embeddings introduced in Section 1.7.4, few are available in\nlanguages other than English. In particular, only one of them has a Hungarian version,\nalthough some of the models have configurations trained on multilingual text. ELMo (Che\net al., 2018) has individual models for 44 languages, trained on 20-million-word samples\nof the CoNLL 2017 shared task Multilingual Parsing from Raw T ext to Universal Depen-\ndencies (Zeman et al., 2017). BER T, XLM and XLM-RoBER T a have joint multilingual\nmodels, which have been trained on roughly 100 languages (BER T on 104). These models\nhave an extended vocabulary (120 thousand for BER T, 240 thousand for XLM) compared\nto their single-language versions, but the number of parameters is the same; additionally ,\nthe multi-language BER T is only available in the Base (110M) parameter configuration.\nmulti-BER Twas trained on Wikipedia; XML-RoBER T a on 2.5TB of Common Crawl data.\nIn this study , we decided to focus on BER T. The main advantage over ELMo is that\nBER T models can befine-tuned on downstream tasks easily , whereas ELMo only replaces\nthe featurizer component. Secondly , the BER T family of models generally achieve better\nperformance on high-level tasks. NP chunking and NER are notable exceptions from this\nrule, so replacing manual features with ELMo in emChunk and emNER is a possible future\navenue for research.\nW e include two BER T models in our study . The first one is multi-BER T; the second is\n1https://huggingface.co/transformers/pretrained_models.html\n102\nW ord Multilingual BER T Hungarian BPE English gloss\nNemzeti Nemzeti Nemzeti National\nAndersen Andersen Andersen Andersen\nlabdarúgó labdarúgó labdarúgó footballer\nzambiai za mbia i z amb iai of Zambia\nmegmaradt meg maradt megmaradt remained\nhétfő hét f ő hétfő Monday\nkeddtől ke dd től kedd től from T uesday\nedényben ed ény ben edény ben in a pot\nHétfőn H ét f ő n Hétfőn On Monday\ntájékoztatták tá j ék ozta tták tájékoztat ták have been notified\nleggazdagabb leg ga zda gab b leggazdagabb wealthiest\nelpártolt el pá rto lt el párt olt renounced\nT able 5.1: T okens generated from a few words by the dictionary of multilingual BER T\nand a Hungarian BPE dictionary built from W ebcorpus 2.0 (see Chapter 4)\nthe preliminary huBERT model introduced in Section 4.6. W e use the cased huBERT model\nin all experiments, as case information is important in named entity recognition.\n5.2.2 Does multi-BER T speak Hungarian?\nThe multi-language BER T model was trained on 104 languages. This raises the question:\nhow well does it model Hungarian? More specifically , we can consider the following two\nfacets:\n1. T o what extend do wordpieces in a tokenized sentence correspond to (Hungarian)\nmorphemes?\n2. Are wordpiece vectors semantically correct? This question is especially interesting\nwith regard to interlingual homographs; e.g. “ leg” (the superlative prefix in Hun-\ngarian), “old” (Hungarian solve)?\nA proper discussion of the second question would be beyond the scope of this chapter.\nAn implicit answer can be construed from the results presented in Section 5.5.\nIn order to answer the first, we tokenized all words in the Szeged NER corpus into word-\npieces in two ways. First with the tokenizer of multi-BER T; second, with a 30,000-token\nBPE vocabulary trained on W ebcorpus 2.0 that was introduced in Chapter 4. T able 5.1\nlists a few select examples.\n103\nW ord type Multilingual BER T Hungarian BPE Difference\nlowercased 2.24 1.34 67%\ncapitalized 1.86 1.75 6%\nall 2.14 1.44 49%\nlowercased (type) 3.97 2.41 65%\ncapitalized (type) 4.65 4.27 9%\nall (type) 4.12 2.83 45%\nT able 5.2: Mean number of subwords per token (above) and type (below)\nAs seen in the table, words can be divided into roughly three groups. W ords in the\nfirst group are handled similarly by the two tokenizers; either because they are present\nin both vocabularies (in other words, they are wordpieces themselves), or because neither\ntokenizer can split them up into meaningful units. “ zambiai” is an example for the latter.\nIn case of the second group, the Hungarian BPE tokenizer always splits words into\nmorphologically sound units, whereas multi-BER T’s tokenization also contains meaning-\nless n-grams. F urthermore, the number of wordpieces on the multilingual side is always\ngreater than on the Hungarian side.\nIn the third group, the gap widens further. While the Hungarian BPE tokenization is\nstill semantic, multi-BER T’s wordpieces deteriorate into random n-grams. The latter also\nneeds 4–5 wordpieces to cover longer words, as opposed to the 1–2 needed by the Hungarian\ntokenizer. Additionally , the fact that “hétfő” and “Hétfőn” are tokenized differently hints\nat a difficulty in properly handling sentence-initial words and named entities.\nT able5.2 provides quantitative confirmation for the observations above. Multi-BER T\nproduces on average 50% more wordpieces than the Hungarian BPE. Since punctuation\nmarks and the most common function words, such as “ a”, “ az” (English the) and “ és”\n(English and), are included in both vocabularies, the effect on content words is likely even\nmore pronounced.\nThe disparity holds for words and types alike. Comparing the top and bottom half\nof T able 5.2, however, reveals that the average number of subwords for types is about\n78% higher than it is for downcased tokens. Since infrequent words account for a much\nhigher proportion, this hints at how much worse infrequent words are represented across\nthe board.\nInterestingly , there is no significant (quantitative) difference in how capitalized words\nare handled: both tokenizers split them up into 4–5 wordpieces. This can be explained by\nthe relative sparsity of capitalized words in the text. At the same time, it also suggests\nthat BER T (multi-language or otherwise) might not be the optimal choice for NER.\n104\nInspecting huBERT’s vocabulary in a similar way , we find that it behaves very similarly\nto the Hungarian BPE vocabulary . Minor differences include the morphologically valid\nsplitting of “hétfő” into tokens “hét” és “fő”, and a 1–8% increase in wordpieces, compared\nto the numbers in T able 5.2. The differences, however, are insignificant compared to the\nones between multi-BER T and the two native Hungarian vocabularies.\n5.3 The emBERT module\nSo that researchers and NLP practitioners may benefit from the models resulting from\nthis study , we decided to integrate them into the e-magyar pipeline. The new version of\ne-magyar, emtsv (Indig et al., 2019), has greatly simplified the the extension of the core\nsystem with new modules. Thus emBERT was born.\nemBERTfollows the same conventions as otheremtsvmodules. When installed, the tools\nbert-base-chunk, bert-max-chunk and bert-ner are made available in emtsv. Like the\nrest of the modules, the tools expect tokenized text as input. Since BER T operates on raw\ntext, in theory ,emBERT could as well; however, in order to keep the tokenization consistent\nwith the rest of the pipeline, we delegate the task to emToken. In contrast with other\nhigh-level modules ( emChunk and emNER in particular) emBERT requires no morphological\ninformation, rendering morphological analysis and lemmatization unnecessary 2.\nW e used Huggingface’stransformers3 (W olf et al.,2019) library to fine-tune and run\nBER T. A considerable advantage oftransformers is that it contains the implementations\nof not only BER T, but other T ransformer-based architectures (XLNet, RoBER T a) as well.\nThis makes it possible to later improve emBERT by integrating other embeddings into it.\nAs opposed to most other emtsv modules4, emBERT contains code for both training\nand running BER T-based models, and it also works as a standalone Python package. W e\nchose to include the training code for two reasons: first, the complexity (or lack thereof) of\nthe code did not justify a segregation of the two functions; and second, it allows users to\nfreely experiment with the models and possibly tailor them to their own needs. Similarly\nto the rest of the modules, the code lives in a GitHub repository 5.\nAt around 700MB apiece, BER T models are sizable even in theBase configuration. In\norder to keep the code manageable, the models have been moved into a separate GitHub\nrepository ,emBERT-models6. As with the HunT ag3 and parser modules, the models can\n2Unless the user or other modules need that information, of course.\n3https://github.com/huggingface/transformers\n4HunT ag3 is an exception\n5https://github.com/DavidNemeskey/emBERT\n6https://github.com/dlt-rilmta/emBERT-models\n105\nbe acquired in two ways: on the one hand, the emBERT repository includes emBERT-models\nas a submodule, so when cloned recursively , the models are downloaded as well. On the\nother, we included our models in emtsv’s model downloader script to give users more\ncontrol over their emtsv setup, and to protect them from spurious downloads.\n5.4 Experiments\nThe performance of the models was measured on two token classification tasks: NP chunk-\ning and named entity recognition. In order to facilitate comparison with previous work,\nthe models were trained and evaluated on standard benchmark corpora.\nSimilarly to all Hungarian statistical NP-chunkers ( hunchunk (Recski, 2010) and its\nsuccessors), our chunker models were trained on the Szeged T reebank 2.0 (Csendes et al.,\n2005). The corpus contains 82,099 sentences. These we split randomly into train, valida-\ntion and test sets (80%–10%–10%). Both subtasks, minimal and maximal NP chunking,\nwere handled the same way: the models were fine-tuned for 4 epochs and evaluated on\nthe test set. W e used a linear learning rate schedule with 10% warm-up, with no early\nstopping.\nThe NER model was fine-tuned on the Szeged NER corpus (Szarvas et al., 2006), a\nsubset of the Szeged T reebank. Since the corpus is much smaller than the full T reebank\n(with 8172–502–900 train/validation/test cuts), several different hyperparameter config-\nurations were explored. The best model was trained for 30 epochs with a linear learning\nrate schedule peaking at 10− 5.\nThe experiments were implemented with the PyT orch version of the transformers\nlibrary . T raining was run parallely on three GeF orce R TX 2080 Ti cards, with a batch\nsize of 16. This configuration allowed us to train both the NER and chunking models\n(the latter of which ran for far less epochs) in 3 hours. F or chunking, the values of most\nhyperparameters were left at their defaults. In case of NER, the hyperparameter settings\nthat performed the best were also using the default values, with the number of epochs and\nthe learning rate being the notable exceptions.\nThe exact values of all hyperparameters are recorded in the configuration files of the\ndownloaded models.\n106\n5.5 Results\n5.5.1 Chunking\nT able 5.3 compares the performance of emBERT and members of the hunchunk family .\nClearly , bothemBERT models outperform all previous systems and achieve state-of-the-art\nresults in both minimal and maximal NPs chunking.\nF or minimal NPs, multi-BER T’s improvement overhunchunk, the only model trained\nfor recognizing them, is not significant. On the other hand, the maximal NP chunker\nsurpasses Huntag3, e-magyar’s tagger of choice, by a considerable 1.5% F1. huBERT\nimproves both scores by a further 1–1.5%, improving on the previous state of the art by\n1.16% in minimal and 2.82% in maximal NP chunking.\nSystem Minimal Maximal\nhunchunk/HunT ag (Recski,2010) 95.48% 89.11%\nHunT ag3 (Endrédy and Indig, 2015) – 93.59%\nemBERT w/ multi-BER T 95.58% 95.05%\nemBERT w/ huBERT 96.64% 96,41%\nT able 5.3: Comparison of Hungarian NP chunkers\n5.5.2 Named entity recognition\nThe results for named entity recognition are more mixed (see T able 5.4). While emBERT\nachieves 2% higher F1 score than Szarvas et al. ( 2006) and V arga and Simon ( 2007), it\nfalls shorts of Huntag3’s performance. Also, multi-BER T and huBERT perform virtually\nidentically in this task. This implies a bottleneck outside the main model, probably in the\nsimple feedforward classifier placed on top.\nspaCy differs from the other systems in that its training data was augmented with the\nhunNERwiki corpus (Nemeskey and Simon, 2012). As such, any comparison with it is\npurely informal, and it is only listed for the sake of completeness.\nDuring NER training, we ran into a typical problem that plagues most machine, and\nespecially deep, learning systems: while results depend heavily on hyper-parameter choice,\nfinding the optimal hyperparameters is extremely costly . This issue affects small corpora,\nsuch as Szeged NER, even more, as the model has magnitudes more parameters than there\nare training instances.\n107\nSystem F1\n(Szarvas et al., 2006) 94,77%\nhunner (V arga and Simon,2007) 95.06%\nHunT ag3 (Endrédy and Indig, 2015) 97.87%\nemBERT w/ multi-BER T 97,08%\nemBERT w/ huBERT 97,03%\nspaCy7 93,95%\nT able 5.4: Comparison of Hungarian NER taggers\nCompared to NER, chunker training was extremely stable: we experimented with\ntraining regimens of various epoch length, with no discernible effect on the final result.\nThis indicates that the hyper-parameter problem can at least be mitigated with a large\nenough training corpus. W e believe NER training could also benefit from the availability\nof a larger and more varied NER corpus; a possible option could be the inclusion of a\nmanually revised subset of hunNERwiki.\n5.6 F uture work\nThe emBERT module, while improves on the previous state-of-the-art in NP chunking,\nmust still be considered proof-of-concept. W e review the outstanding issues, as well as the\ncorresponding lines of future research, below.\nFirst, we have seen that basing the module on the multi-language BER T is undeniably\nsuboptimal. Apart from it being available only in the Base configuration, the capacities\nof both the model and the vocabulary are divided among 104 languages. The fact that\nhuBERT outperforms multi-BER T by more than a full percent in both chunking tasks\nsuggests that a BER T model trained on the full W ebcorpus 2.0, especially a Large one,\nwould presumably achieve further improvements. As mentioned in Chapter 4, we plan to\ntrain and publish such models in the future.\nSecond, BER T is but one of the many contextual embeddings published in the last\ntwo years. As we have seen, ELMo, RoBER T a or Flair surpass BER T in certain tasks;\nand named entity recognition is one of them. W e intend to include other embeddings in\nemBERT as well, should a multi-language or Hungarian version become available.\nThird, we should find out what language tasks, other than chunking and NER, can\nbenefit from BER T. The obvious candidate is morphological analysis, for which a deep\n108\nlearning approach has already proven successful (Ugray, 2019). Provided that resources\nsimilar to GLUE (W ang et al.,2018) or SQuAD (Rajpurkar et al., 2016) become available\nfor Hungarian, the model could also be adapted to higher-level tasks such as sentiment\nanalysis, paraphrase detection or question answering. In incorporating them, emBERT\nwould not only improve on functions already provided by e-magyar, but could bring new\ncapabilities to it as well.\n5.7 Conclusion\nIn this chapter, we have have introduced emBERT, a new module of the e-magyar text\nprocessing system. emBERT allows the integration of classifiers based on contextual em-\nbedding into the pipeline. W e fine-tuned both the multilingual BER T and the preliminary\nhuBERT models on the tasks of NP chunking and named entity recognition. Our models\nperform comparably to previous NE recognizers, and achieve a new state-of-the-art in NP\nchunking.\nemBERT offers various opportunities for improvement. The module can easily be ex-\ntended to support additional embeddings and tasks, as long as the necessary resources (a\ntraining corpus and the embedding itself) are available.\n109\nChapter 6\nConclusions\nIn this thesis we concentrated on two issues: how modern NLP (or traditional linguistic)\ntechniques can improve language modeling, and how to improve the state of the art for\nHungarian.\nChapter 2 highlighted the problems of word-based language modeling for Hungarian\nand introduced the “gluten-free” format, a morphological segmentation algorithm that al-\nleviates the adverse effects of the overabundance of word forms in the language. Chapter 3\nproposed a novel method for evaluating multi-sense embeddings based on lexicographical\nresources. Chapter 5 gave an example for the opposite direction, when language models\nare used to improve the performance of an NLP system.\nW e improved on the state of the art in Hungarian language modeling in several ways.\nFirst, we presented a set of language modeling benchmarks on three Hungarian corpora\nin Chapter 2. A preprocessed version of the Hungarian W ebcorpus has been released to\nserve as a standard dataset for language model assessment. A new Hungarian corpus\nhas been created in Chapter 4. W ebcorpus 2.0 was compiled from Hungarian pages in\nthe Common Crawl and the Hungarian Wikipedia. At 9 billion tokens, it is 3.5 times\nthe size of the previous largest (commercial) corpus, and can serve as training data for\nlarge-scale language models. Finally , the emBERT module developed in Chapter 5 enables\nthe integration of modern contextualized embedding-based classifiers into the e-magyar\npipeline. Based on our preliminary Hungarian BER T model, the NP chunker outperforms\nthe previous best system by 2.8% in F1 score.\nAll resources (corpora, models and software alike) presented in the thesis are freely\ndownloadable under permissive licenses.\n110\nÖsszefoglalás\nA disszertációban két kérdésre koncentráltunk. Egyfelől célnak tűztük ki a magyar nyelv-\nmodellezési eredmények javítását; másrészt azt vizsgáltuk, hogy alkalmasak-e erre a célra\na modern természetesnyelv-feldolgozás (vagy a hagyományos nyelvészet) módszerei.\nA 2. fejezet bemutatta a problémákat, amivel a magyar nyelv szószintű modellezésekor\ntalálkozunk. Itt mutattuk be a „ gluténmentes” formátumot, egy morfológiai szegmen-\ntáló algoritmust, ami ha nem is orvosolja, de csökkenti a szóalakok túlburjánzása okozta\nproblémákat. A 3. fejezet egy értelmező szótárakon alapuló módszert javasolt többjelen-\ntésű szóbeágyazások kiértékelésére. A 5. fejezet az ellenkező irányra ad példát, amikor\nnyelvmodellek alkalmazása javítja egy nyelvfeldolgozó rendszer eredményét.\nKutatásunk keretében több módon is korszerűsítettük a magyar nyelvmodellezés ál-\nlapotát. Egyrészt több nyelvmodellt is kimértünk három magyar korpuszon. Ezek közül\negyiket, a Magyar W ebkorpusz előfeldolgozott változatát javasoltuk sztenderd nyelvmodell-\nkiértékelő korpusznak a 2. fejezetben. A W ebkorpusz 2.0 létrehozását mutatja be a\n4. fejezet. Az új korpusz a Common Crawl webarchívumból és a magyar Wikipédiából\ntevődik össze. Kilenc milliárd szavas méretével a legnagyobb magyar korpusz, mérete 3,5-\nszörösen haladja meg a legnagyobb (kereskedelmi) korpuszét. Végül a 5. fejezet mutatja\nbe az emBERT modult, ami lehetővé teszi modern kontextualizált beágyazások integrálását\naz e-magyar szövegfeldolgozó rendszerbe. A kísérleti magyar BER T modellünkre építő\nfőnévicsoport-felismerő 2,8% F1-ponttal múlja felül a korábbi legjobb rendszert.\nMinden bemutatott erőforrás (korpuszok, modellek és szoftver) szabadon letölthető.\n111\nAppendices\n112\nAppendix A\nAbbreviations used in the thesis\n1B One Billion W ord Benchmark ME maximum entropy\nAR activation regularization MEMM maximum entropy Markov model\nBER T Bidirectional Encoder\nRepresentations from T ransformers\nML machine learning\nMLE maximum likelihood estimation\nBPTT backpropagation through time MLP multilayer perceptron\nBPE byte-pair encoding MSE multi-sense embedding\nCC Common Crawl NER named entity recognition\nCLD2 Compact Language Detector 2 NFS network file system\nCNN Convolutional neural network NLP natural language processing\nCRF conditional random field NLU natural language understanding\nDOM document object model (N)MT (neural) machine translation\nELMo Embeddings from Language Models NNLM neural net language model\nFFNN feedforward neural network NP noun phrase (as in “NP chunking”)\nFLM factored language model OOV out-of-vocabulary word\nGLF gluten-free (form) POS part-of-speech\nGPU graphics processing unit PTB Penn T reeBank\nHMM hidden Markov model QA question answering\nKN Kenser-Ney (n-gram model) SGD stochastic gradient descent\nLAN local area network TPU tensor processing unit\nLM language model(ing) tsv tab separated values\nLOB Lancaster–Oslo/Bergen (corpus) w2v word2vec\nLSH locality-sensitive hashing WP Wikipedia\nLSTM long short-term memory WT2 WikiT ext-2\n113\nAppendix B\nSample texts generated by\nT ransformer models\nThere is little doubt how President T rump ...\nviews Russia’s invading of Ukraine and annexation of Crimea. He sees the actions of\nRussian leader Vladimir Putin as evidence of Russian political subversion in Amer-\nica.\nW e may never know the extent to which T rump and his advisers also suspect that a\nkey feature of American politics is a ”deep state” that is working to subvert the will\nof the American people at home and to promote Kremlin policies abroad. One sees\nthem attempting to prevent T rump’s nomination as a Republican candidate and\nthen to sabotage his presidency by publishing damaging articles about him from\ninside his administration.\nY ou want to know the best thing President T rump said that day? ”I won’t stand\nup to Russia. ”\nThat’s not such a bad thing. T rump\nChess great Magnus Carlsen made history T uesday as he extended his un-\nbeaten run to 111 successive classical games, breaking the 110-game streak set\nby Sergei Tiviakov in 2005.\nCarlsen, who is ranked No. 1 on the world rankings, won a tense match against\nfellow Norwegian V assily Ivanchuk of Russia at the W orld Y outh Championship\ntournament held in Moscow, Russia. The 20-year-old has now broken the old mark\nthree times and tied it once. He also broke the previous record, which was established\nwhen Vladimir Kramnik achieved 113 games last year.\n”I think I am just getting started,” said Carlsen. ”This will be my ninth year playing\nchess. I’ve been trying all these years but this time I’m really happy with how things\nare going. ”\n114\nA few weeks ago, Carlsen became only the second player since 1972 iafter Anatoly\nKarpov ito reach 100 consecutive wins from one start in competition. But that\ndidn’t stop him from celebrating what would have been his 100th win T uesday .\nBendable glass is the missing ingredient in today’s foldable phones. But what\nif bendable screens were made from one of the hardest materials on Earth?\nThat’s exactly what’s being proposed by a team at Stanford University . The group\nhas developed an extremely hard and lightweight material that they say would be\nideal for making flexible displays. They call it ”diamond” glass. It’s not as strong\nor durable as carbon fiber, but it’s still strong. In fact, this new composite will\nbend like paper when squeezed with your fingers. If you can make something out of\ndiamond, then we might just have to start using these phones more often.\n[Stanford’s T echnology Review]\n115\nBibliography\nAbadi, Martı́n et al. (2016a). TensorFlow: Large-Scale Machine Learning on Heterogeneous Dis-\ntributed Systems. arXiv: 1603.04467 [cs.DC].\nAbadi, Martı́n et al. (2016b). TensorFlow: Large-Scale Machine Learning on Heterogeneous Dis-\ntributed Systems. arXiv: 1603.04467 [cs.DC].\nAckley , David H; Hinton, Geoffrey E, and Sejnowski, T errence J (1985). “A learning algorithm\nfor Boltzmann machines” . In: Cognitive science9.1, pp. 147–169.\nAfify , Mohamed; Sarikaya, Ruhi; Kuo, Hong-Kwang Jeff; Besacier, Laurent, and Gao, Y uqing\n(2006). “On the use of morphological analysis for dialectal Arabic speech recognition.” In:\nINTERSPEECH, pp. 277–280.\nAkbik, Alan; Blythe, Duncan, and V ollgraf, Roland (Aug. 2018). “Contextual String Embeddings\nfor Sequence Labeling” . In: Proceedings of the 27th International Conference on Computa-\ntional Linguistics. Santa F e, New Mexico, USA: Association for Computational Linguistics,\npp. 1638–1649. url: https://www.aclweb.org/anthology/C18-1139.\nAkbik, Alan; Bergmann, T anja; Blythe, Duncan; Rasul, Kashif; Schweter, Stefan, and V oll-\ngraf, Roland (June 2019). “FLAIR: An Easy-to-Use F ramework for State-of-the-Art NLP” .\nIn: Proceedings of the 2019 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations). Minneapolis, Minnesota: Association\nfor Computational Linguistics, pp. 54–59. doi: 10 . 18653 / v1 / N19 - 4010. url: https :\n//www.aclweb.org/anthology/N19-4010.\nAkbik, Alan; Bergmann, T anja, and V ollgraf, Roland (June 2019). “Pooled Contextualized Em-\nbeddings for Named Entity Recognition” . In: Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Associa-\ntion for Computational Linguistics, pp. 724–728. doi: 10.18653/v1/N19-1078. url: https:\n//www.aclweb.org/anthology/N19-1078.\nArora, Sanjeev; Li, Y uanzhi; Liang, Yingyu; Ma, T engyu, and Risteski, Andrej (2016). “RAND-\nW ALK: A Latent V ariable Model Approach to W ord Embeddings” . In:Transactions of the\nAssociation for Computational Linguistics (TACL)4, pp. 385–399.\nBaevski, Alexei; Edunov, Sergey; Liu, Yinhan; Zettlemoyer, Luke, and Auli, Michael (Nov. 2019).\n“Cloze-driven Pretraining of Self-attention Networks” . In: Proceedings of the 2019 Confer-\n116\nence on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China: Asso-\nciation for Computational Linguistics, pp. 5360–5369. doi: 10.18653/v1/D19- 1539 . url:\nhttps://www.aclweb.org/anthology/D19-1539.\nBahdanau, Dzmitry; Cho, Kyunghyun, and Bengio, Y oshua (2015). “Neural machine transla-\ntion by jointly learning to align and translate” . In: International Conference on Learning\nRepresentations (ICLR 2015).\nBahl, Lalit R.; Jelinek, F red, and Mercer, Robert (1983). “A Maximum Likelihood Approach to\nContinuous Speech Recognition” . In:IEEE Trans. PAMI5.2, pp. 179–190.\nBaker, James (1975). “The DRAGON system–An overview” . In:IEEE Transactions on Acoustics,\nspeech, and signal Processing23.1, pp. 24–29.\nBalázs, Kapitány (2013). “Kárpát-medencei népszámlálási körkép” . In:Demográfia 56.1, pp. 25–\n64.\nBaroni, M.; Dinu, G., and Kruszewski, G. (2014). “Don’t count, predict! A systematic comparison\nof context-counting vs. context-predicting semantic vectors” . In: Proceedings of ACL 2014,\npp. 237–247.\nBartunov, Sergey; Kondrashkin, Dmitry; Osokin, Anton, and V etrov, Dmitry (May 2016). “Break-\ning Sticks and Ambiguities with Adaptive Skip-gram” . In: Proceedings of Machine Learning\nResearch51: Artificial Intelligence and Statistics, pp. 130–138.\nBengio, Y oshua; Simard, Patrice, and F rasconi, Paolo (1994). “Learning long-term dependencies\nwith gradient descent is difficult” . In:IEEE transactions on neural networks5.2, pp. 157–166.\nBengio, Y oshua; Ducharme, Réjean; Vincent, Pascal, and Janvin, Christian (2003). “A Neural\nProbabilistic Language Model” . In:Journal of Machine Learning Research3, pp. 1137–1155.\nurl: http://www.jmlr.org/papers/v3/bengio03a.html.\nBengio, Y oshua and Senécal, Jean-Sébastien (2003). “Quick T raining of Probabilistic Neural Nets\nby Importance Sampling” . In: AISTATS.\nBengio, Y oshua and Senécal, Jean-Sébastien (2008). “Adaptive importance sampling to accelerate\ntraining of a neural probabilistic language model” . In:IEEE Transactions on Neural Networks\n19.4, pp. 713–722.\nBengio, Y oshua (2012). “Practical recommendations for gradient-based training of deep archi-\ntectures” . In:Neural networks: Tricks of the trade. Springer, pp. 437–478.\nBerger, A.L.; Pietra, S.A. Della, and Pietra, V.J. Della (1996). “A maximum entropy approach\nto natural language processing” . In: Computational Linguistics22.1.\nBerger, Adam and Lafferty, John (1999). “Information Retrieval as Statistical T ranslation” . In:\nProceedings of the 22nd Annual International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval. SIGIR n99. Berkeley , California, USA: Association\nfor Computing Machinery, pp. 222–229. isbn: 1581130961. doi: 10.1145/312624.312681 .\nurl: https://doi.org/10.1145/312624.312681.\n117\nBiber, Douglas (1993). “Representativeness in corpus design” . In: Literary and linguistic com-\nputing 8.4, pp. 243–257.\nBilmes, Jeff A. and Kirchhoff, Katrin (2003). “F actored Language Models and Generalized Par-\nallel Backoff” . In:Proceedings of HLT/NACCL, pp. 4–6.\nBlei, David M; Ng, Andrew Y, and Jordan, Michael I (2003). “Latent dirichlet allocation” . In:\nJournal of machine Learning research3.Jan, pp. 993–1022.\nBoguraev, Branimir K. and Briscoe, Edward J. (1989). Computational Lexicography for Natural\nLanguage Processing. Longman.\nBojanowski, Piotr; Grave, Edouard; Joulin, Armand, and Mikolov, T omas (2017). “Enriching\nW ord V ectors with Subword Information” . In:Transactions of the Association for Compu-\ntational Linguistics 5, pp. 135–146. issn: 2307-387X. url: https : / / transacl . org / ojs /\nindex.php/tacl/article/view/999.\nBojar, Ondřej; F edermann, Christian; Fishel, Mark; Graham, Y vette; Haddow, Barry; Koehn,\nPhilipp, and Monz, Christof (Oct. 2018). “Findings of the 2018 Conference on Machine\nT ranslation (WMT18)” . In:Proceedings of the Third Conference on Machine Translation:\nShared Task Papers. Belgium, Brussels: Association for Computational Linguistics, pp. 272–\n303. doi: 10.18653/v1/W18-6401. url: https://www.aclweb.org/anthology/W18-6401.\nBorbély , Gábor; Kornai, András; Nemeskey, Dávid, and Kracht, Marcus (2016). “Denoising com-\nposition in distributional semantics” . In: DSALT: Distributional Semantics and Linguistic\nTheory. poster.\nBorbély , Gábor; Makrai, Márton; Nemeskey, Dávid Márk, and Kornai, András (2016). “Evaluat-\ning multi-sense embeddings for semantic resolution monolingually and in word translation” .\nIn: Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP.\nBerlin, Germany: Association for Computational Linguistics, pp. 83–89. doi: 10.18653/v1/\nW16-2515. url: http://www.aclweb.org/anthology/W16-2515.\nBotha, Jan A and Blunsom, Phil (2014). “Compositional Morphology for W ord Representations\nand Language Modelling” . In: ICML, pp. 1899–1907.\nBrants, Thorsten; Popat, Ashok C.; Xu, Peng; Och, F ranz J., and Dean, Jeffrey (June 2007).\n“Large Language Models in Machine T ranslation” . In:Proceedings of the 2007 Joint Confer-\nence on Empirical Methods in Natural Language Processing and Computational Natural Lan-\nguage Learning (EMNLP-CoNLL). Prague, Czech Republic: Association for Computational\nLinguistics, pp. 858–867. url: http://www.aclweb.org/anthology/D/D07/D07-1090.\nBrébisson, Alexandre de and Vincent, Pascal (2015). “An Exploration of Softmax Alternatives\nBelonging to the Spherical Loss F amily” .\nBreuel, Thomas M (1994). “A system for the off-line recognition of handwritten text” . In:Proceed-\nings of the 12th IAPR International Conference on Pattern Recognition, Vol. 3-Conference\nC: Signal Processing (Cat. No. 94CH3440-5). V ol. 2. IEEE, pp. 129–134.\n118\nBroder, Andrei Z; Charikar, Moses; F rieze, Alan M, and Mitzenmacher, Michael (1998). “Min-\nwise independent permutations” . In:Proceedings of the thirtieth annual ACM symposium on\nTheory of Computing. ACM, pp. 327–336.\nBrown, P .F.; Pietra, V.J. Della; Souza, P .V. de; Lai, J.C., and Mercer, R.L. (1992). “Class–based\nn–gram models of natural language” . In: Computational Linguistics18.4, pp. 467–480.\nBrown, Peter; Cocke, John; Pietra, Stephen Della; Pietra, Vincent J. Della; Jelinek, F redrick;\nLafferty, John D.; Mercer, Robert L., and Roossin, Paul S. (1990). “A statistical approach\nto machine translation” . In: Computational Linguistics16, pp. 79–85.\nBrown, Peter F; Pietra, Vincent J Della; Pietra, Stephen A Della, and Mercer, Robert L (1993).\n“The mathematics of statistical machine translation: Parameter estimation” . In: Computa-\ntional linguistics19.2, pp. 263–311.\nBuck, Christian; Heafield, Kenneth, and Ooyen, Bas van (May 2014). “N-gram Counts and\nLanguage Models from the Common Crawl” . In: Proceedings of the Ninth International\nConference on Language Resources and Evaluation (LREC’14). Reykjavik, Iceland: Euro-\npean Language Resources Association (ELRA), pp. 3579–3584. url: http : / / www . lrec -\nconf.org/proceedings/lrec2014/pdf/1097_Paper.pdf.\nCauchy , Augustin (1847). “Méthode générale pour la résolution des systemes d néquations si-\nmultanées” . In:Comp. Rend. Sci. Paris25.1847, pp. 536–538.\nCharniak, Eugene (June 2001). “Immediate-Head Parsing for Language Models” . In:Proceedings\nof the 39th Annual Meeting of the Association for Computational Linguistics. T oulouse,\nF rance: Association for Computational Linguistics, pp. 124–131.url: https://www.aclweb.\norg/anthology/P01-1017.\nChe, W anxiang; Liu, Yijia; W ang, Y uxuan; Zheng, Bo, and Liu, Ting (Oct. 2018). “T owards\nBetter UD Parsing: Deep Contextualized W ord Embeddings, Ensemble, and T reebank Con-\ncatenation” . In:Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw\nText to Universal Dependencies. Brussels, Belgium: Association for Computational Linguis-\ntics, pp. 55–64. url: http://www.aclweb.org/anthology/K18-2005.\nChelba, Ciprian and Jelinek, F rederick (Aug. 1998). “Exploiting Syntactic Structure for Language\nModeling” . In: 36th Annual Meeting of the Association for Computational Linguistics and\n17th International Conference on Computational Linguistics, Volume 1. Montreal, Quebec,\nCanada: Association for Computational Linguistics, pp. 225–231. doi: 10 . 3115 / 980845 .\n980882. url: https://www.aclweb.org/anthology/P98-1035.\nChelba, Ciprian; Bikel, Dan; Shugrina, Maria; Nguyen, Patrick, and Kumar, Shankar (2012).\nLarge Scale Language Modeling in Automatic Speech Recognition. T ech. rep. Google. url:\nhttps://research.google.com/pubs/pub40491.html.\nChelba, Ciprian; Mikolov, T omas; Schuster, Mike; Ge, Qi; Brants, Thorsten; Koehn, Phillipp, and\nRobinson, T ony (2014). “One billion word benchmark for measuring progress in statistical\n119\nlanguage modeling” . In:INTERSPEECH 2014, 15th Annual Conference of the International\nSpeech Communication Association, Singapore, September 14-18, 2014, pp. 2635–2639.\nChen, Stanley; Beeferman, Douglas, and Rosenfeld, Ronald (1998). “Evaluation Metrics F or\nLanguage Models” . In: Proceedings of the DARPA Broadcast News Transcription and Un-\nderstanding Workshop, pp. 275–280.\nChen, Stanley F and Goodman, Joshua (Oct. 1999). “An empirical study of smoothing techniques\nfor language modeling” . In: Computer Speech & Language13.4, pp. 359–394.\nChen, Stanley F. and Rosenfeld, Ronald (1999). A Gaussian prior for smoothing maximum\nentropy models. T ech. rep. CMU-CS-99-108. Computer Science Department, Carnegie Mellon\nUniversity.\nChen, W enlin; Grangier, David, and Auli, Michael (Aug. 2016). “Strategies for T raining Large\nV ocabulary Neural Language Models” . In: Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers). Berlin, Germany: As-\nsociation for Computational Linguistics, pp. 1975–1985. url: http : / / www . aclweb . org /\nanthology/P16-1186.\nChiu, Billy; Korhonen, Anna, and Pyysalo, Sampo (2016). “Intrinsic Evaluation of W ord V ectors\nF ails to Predict Extrinsic Performance” . In:Proc. RepEval (this volume). Ed. by Omer Levy.\nACL.\nCho, Kyunghyun; Merriëenboer, Bart van; Gulcehre, Caglar; Bougares, F ethi; Schwenk, Holger,\nand Bengio, Y oshua (2014). “Learning Phrase Representations using RNN Encoder-Decoder\nfor Statistical Machine T ranslation” . In: Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP). Doha, Qatar, pp. 1724–1734. url: http:\n//www.aclweb.org/anthology/D14-1179.\nChomsky , Noam (1957). Syntactic Structures. The Hague: Mouton.\nChurch, Kenneth W. and Hanks, Patrick (1990). “W ord association norms, mutual information,\nand lexicography” . In:Computational Linguistics16.1, pp. 22–29.\nCollobert, R.; W eston, J.; Bottou, L.; Karlen, M.; Kavukcuoglu, K., and Kuksa, P . (2011). “Nat-\nural Language Processing (Almost) from Scratch” . In:Journal of Machine Learning Research\n(JMLR).\nCollobert, Ronan and W eston, Jason (2008). “A Unified Architecture for Natural Language\nProcessing: Deep Neural Networks with Multitask Learning” . In: Proceedings of the 25th\nInternational Conference on Machine Learning. ICML ’08. Helsinki, Finland: ACM, pp. 160–\n167.\nConneau, Alexis and Lample, Guillaume (2019). “Cross-lingual Language Model Pretraining” .\nIn: Advances in Neural Information Processing Systems, pp. 7057–7067.\nConneau, Alexis et al. (2019). “Unsupervised cross-lingual representation learning at scale” .\nCorbı́-Bellot, Antonio M.; F orcada, Mikel L.; Ortiz-Rojas, Sergio; Pérez-Ortiz, Juan Antonio;\nRamı́rez-Sánchez, Gema; Sánchez-Martı́nez, F elipe; Alegria, Iñaki; Mayor, Aingeru, and Sara-\n120\nsola, Kepa (May 2005). “An open-source shallow-transfer machine translation engine for the\nromance languages of Spain” . In: Proceedings of the Tenth Conference of the European As-\nsociation for Machine Translation. Budapest, Hungary, pp. 79–86.\nCouncil of European Union (2016). “REGULA TION (EU) 2016/679 OF THE EUROPEAN\nP ARLIAMENT AND OF THE COUNCIL of 27 April 2016 on the protection of natural\npersons with regard to the processing of personal data and on the free movement of such\ndata, and repealing Directive 95/46/EC (General Data Protection Regulation)” . In: Official\nJournal L 119, pp. 1–88. url: https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/\n?uri=CELEX:32016R0679.\nCsendes, Dóra; Hatvani, Csaba; Alexin, Zoltán; Csirik, János; Gyimóthy, Tibor; Prószéky, Gábor,\nand Váradi, T amás (Dec. 2003). “Kézzel annotált magyar nyelvi korpusz: a Szeged Korpusz” .\nIn: I. Magyar Számítógépes Nyelvészeti Konferencia előadásai: MSZNY 2003, pp. 238–245.\nCsendes, Dóra; Csirik, János; Gyimóthy, Tibor, and Kocsor, András (2005). “The Szeged Treebank” .\nIn: Lecture Notes in Computer Science: Text, Speech and Dialogue. Springer, pp. 123–131.\nCybenko, George (1989). “Approximation by superpositions of a sigmoidal function” . In: Math-\nematics of control, signals and systems2.4, pp. 303–314.\nDai, Zihang; Y ang, Zhilin; Y ang, Yiming; Carbonell, Jaime; Le, Quoc, and Salakhutdinov, Ruslan\n(July 2019). “T ransformer-XL: Attentive Language Models beyond a Fixed-Length Context” .\nIn: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\nFlorence, Italy: Association for Computational Linguistics, pp. 2978–2988. doi: 10.18653/\nv1/P19-1285. url: https://www.aclweb.org/anthology/P19-1285.\nDarroch, J.N. and Ratcliff, D. (1972). “Generalized iterative scaling for log-linear models” . In:\nThe Annals of Mathematical Statistics43, pp. 1470–1480.\nDeerwester, Scott C.; Dumais, Susan T, and Harshman, Richard A. (1990). “Indexing by latent\nsemantic analysis” . In:Journal of the American Society for Information Science41.6, pp. 391–\n407.\nDella Pietra, S.; Della Pietra, V.; Mercer, R. L., and Roukos, S. (1992). “Adaptive Language\nModeling Using Minimum Discriminant Estimation” . In:Speech and Natural Language: Pro-\nceedings of a Workshop Held at Harriman, New York, February 23-26, 1992. url: https:\n//www.aclweb.org/anthology/H92-1020.\nDevlin, Jacob; Chang, Ming-W ei; Lee, Kenton, and T outanova, Kristina (Oct. 11, 2018). “BER T:\nPre-training of Deep Bidirectional T ransformers for Language Understanding” . V ersion 1. In:\narXiv preprint arXiv:1810.04805. arXiv: 1810.04805v1 [cs.CL]. url: http://arxiv.org/\nabs/1810.04805v1.\nDevlin, Jacob; Chang, Ming-W ei; Lee, Kenton, and T outanova, Kristina (2019). “BER T: Pre-\ntraining of Deep Bidirectional T ransformers for Language Understanding” . In:Proc.ofNAACL .\nDong, Li; Y ang, Nan; W ang, W enhui; W ei, F uru; Liu, Xiaodong; W ang, Y u; Gao, Jianfeng;\nZhou, Ming, and Hon, Hsiao-W uen (2019). “Unified language model pre-training for natural\n121\nlanguage understanding and generation” . In: Advances in Neural Information Processing\nSystems, pp. 13042–13054.\nDostert, Leon E (1955). “The georgetown-ibm experiment” . In: Machine Translation of Lan-\nguages: Fourteen Essays. MIT Press, pp. 124–135.\nDyer, Chris; Kuncoro, Adhiguna; Ballesteros, Miguel, and Smith, Noah A. (June 2016). “Recur-\nrent Neural Network Grammars” . In:Proceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies. San Diego, California: Association for Computational Linguistics, pp. 199–209. arXiv:\n1602.07776 [cs.CL]. url: http://www.aclweb.org/anthology/N16-1024.\nEmami, Ahmad and Jelinek, F rederick (2005). “Random Clusterings for Language Modeling” .\nIn: ICASSP (1), pp. 581–584.\nEndrédy, István and Indig, Balázs (2015). “HunT ag3, a General-purpose, Modular Sequential\nT agger ⚶ Chunking Phrases in English and Maximal NPs and NER for Hungarian” . In:\n7th Language & Technology Conference. Poznan: Uniwersytet im. Adama Mickiewicza w\nPoznaniu, 213–218.\nF aruqui, Manaal; T svetkov, Y ulia; Rastogi, Pushpendre, and Dyer, Chris (2016). “Problems With\nEvaluation of W ord Embeddings Using W ord Similarity T asks” .\nFilimonov, Denis and Harper, Mary (2009). “A joint language model with fine-grain syntactic\ntags” . In: Proceedings of the 2009 Conference on Empirical Methods in Natural Language\nProcessing: Volume 3-Volume 3. Association for Computational Linguistics, pp. 1114–1123.\nFinkelstein, Lev; Gabrilovich, Evgeniy; Matias, Y ossi; Rivlin, Ehud; Solan, Zach; W olfman, Gadi,\nand Ruppin, Eytan (2002). “Placing Search in Context: The Concept Revisited” . In: ACM\nTransactions on Information Systems20(1), pp. 116–131.\nFirth, John R. (1957). “A synopsis of linguistic theory” . In: Studies in linguistic analysis. Black-\nwell, pp. 1–32.\nF orcada, Mikel L; Ginestí-Rosell, Mireia; Nordfalk, Jacob; OnRegan, Jim; Ortiz-Rojas, Sergio;\nPérez-Ortiz, Juan Antonio; Sánchez-Martínez, F elipe; Ramírez-Sánchez, Gema, and Tyers,\nF rancis M (2011). “Apertium: a free/open-source platform for rule-based machine transla-\ntion” . In:Machine translation25.2, pp. 127–144.\nF rancis, W Nelson and Kucera, Henry (1979). Brown Corpus manual: Manual of information\nto accompany a standard corpus of present-day edited American English for use with digital\ncomputers. Providence, Rhode Island, USA: Brown University.\nF ukushima, Kunihiko (1980). “Neocognitron: A self-organizing neural network model for a mech-\nanism of pattern recognition unaffected by shift in position” . In: Biological cybernetics36.4,\npp. 193–202.\nGal, Y arin and Ghahramani, Zoubin (2016). “A Theoretically Grounded Application of Dropout\nin Recurrent Neural Networks” . In: Advances in Neural Information Processing Systems 29.\nEd. by D. D. Lee; M. Sugiyama; U. V. Luxburg; I. Guyon, and R. Garnett. Curran Associates,\n122\nInc., pp. 1019–1027. arXiv: 1512.05287 [stat.ML]. url: http://papers.nips.cc/paper/\n6241 - a - theoretically - grounded - application - of - dropout - in - recurrent - neural -\nnetworks.pdf.\nGers, F elix A; Schmidhuber, Jürgen, and Cummins, F red (2000). “Learning to forget: Continual\nprediction with LSTM” . In: Neural computation12.10, pp. 2451–2471.\nGionis, Aristides; Indyk, Piotr; Motwani, Rajeev, et al. (1999). “Similarity search in high dimen-\nsions via hashing” . In: VLDB. 6, pp. 518–529.\nGladkova, Anna and Drozd, Aleksandr (2016). “Intrinsic Evaluations of W ord Embeddings: What\nCan W e Do Better?” In: Proc. RepEval (this volume). Ed. by Omer Levy. ACL.\nGlorot, Xavier and Bengio, Y oshua (May 2010). “Understanding the difficulty of training deep\nfeedforward neural networks” . In:Proceedings of the Thirteenth International Conference on\nArtificial Intelligence and Statistics. Ed. by Y ee Whye T eh and Mike Titterington. V ol. 9.\nProceedings of Machine Learning Research. Chia Laguna Resort, Sardinia, Italy: PMLR,\npp. 249–256. url: http://proceedings.mlr.press/v9/glorot10a.html.\nGong, Chengyue; He, Di; T an, Xu; Qin, T ao; W ang, Liwei, and Liu, Tie-Y an (2018). “FRAGE:\nF requency-Agnostic W ord Representation” . In:Advances in Neural Information Processing\nSystems 31. Ed. by S. Bengio; H. W allach; H. Larochelle; K. Grauman; N. Cesa-Bianchi, and\nR. Garnett. Curran Associates, Inc., pp. 1334–1345. url: http://papers.nips.cc/paper/\n7408-frage-frequency-agnostic-word-representation.pdf .\nGood, I.J. (1953). “The population frequencies of species and the estimation of population pa-\nrameters” . In:Biometrika 40, pp. 237–264.\nGoodfellow, Ian; Bengio, Y oshua, and Courville, Aaron (2016). Deep Learning. MIT Press. url:\nhttp://www.deeplearningbook.org.\nGoodman, Joshua and Gao, Jianfeng (2000). “Language Model Size Reduction By Pruning And\nClustering” . In:In ICSLPn00, pp. 110–113.\nGoodman, Joshua T. (2001). “A Bit of Progress in Language Modeling” . In: Computer Speech &\nLanguage 15.4, pp. 403–434.\nGrave, Edouard; Joulin, Armand, and Usunier, Nicolas (2017). “Improving neural language\nmodels with a continuous cache” . In: International Conference on Learning Representations\n(ICLR 2017). url: https://openreview.net/pdf?id=B184E5qee.\nGrave, Edouard; Bojanowski, Piotr; Gupta, Prakhar; Joulin, Armand, and Mikolov, T omas\n(2018). “Learning W ord V ectors for 157 Languages” . In: Proc. of LREC. url: https : / /\nwww.aclweb.org/anthology/L18-1550.\nGraves, Alex and Schmidhuber, Jürgen (2009). “Offline handwriting recognition with multidi-\nmensional recurrent neural networks” . In:Advances in neural information processing systems,\npp. 545–552.\nGraves, Alex (2012). “Sequence transduction with recurrent neural networks” . In:Representation\nLearning Workshop, ICML 2012. Edinburgh, Scotland. arXiv: 1211.3711 [cs.NE].\n123\nGraves, Alex; Mohamed, Abdel-rahman, and Hinton, Geoffrey (2013). “Speech recognition with\ndeep recurrent neural networks” . In:2013 IEEE international conference on acoustics, speech\nand signal processing. IEEE, pp. 6645–6649.\nGraves, Alex; W ayne, Greg, and Danihelka, Ivo (2014).Neural turing machines. T ech. rep. arXiv:\n1410.5401 [cs.NE].\nGreff, Klaus; Srivastava, Rupesh Kumar; Koutník, Jan; Steunebrink, Bas R, and Schmidhuber,\nJürgen (2015). “LSTM: A search space odyssey” . In: arXiv: 1503.04069 [cs.NE].\nGutmann, Michael and Hyvärinen, Aapo (2010). In: Proceedings of the Thirteenth International\nConference on Artificial Intelligence and Statistics.\nHahnloser, Richard HR; Sarpeshkar, Rahul; Mahowald, Misha A; Douglas, Rodney J, and Seung,\nH Sebastian (2000). “Digital selection and analogue amplification coexist in a cortex-inspired\nsilicon circuit” . In: Nature 405.6789, pp. 947–951.\nHalácsy , Péter; Kornai, András; Németh, László; Rung, András; Szakadát, István, and T rón, Vik-\ntor (2004). “Creating open language resources for Hungarian” . In: Proceedings of the Fourth\nInternational Conference on Language Resources and Evaluation (LREC 2004). ELRA,\npp. 203–210.\nHan, Lushan; Kashyap, Abhay L.; Finin, Tim; Mayfield, James, and W eese, Jonathan (June\n2013). “UMBC_EBIQUITY-CORE: Semantic T extual Similarity Systems” . In:Second Joint\nConference on Lexical and Computational Semantics (*SEM). Atlanta, Georgia, USA: Asso-\nciation for Computational Linguistics, pp. 44–52.\nHarris, Zellig S. (1954). “Distributional structure” . In: Word10.23, pp. 146–162.\nHe, Kaiming; Zhang, Xiangyu; Ren, Shaoqing, and Sun, Jian (2016). “Deep residual learning for\nimage recognition” . In: Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 770–778.\nHewitt, John and Manning, Christopher D (2019). “A structural probe for finding syntax in word\nrepresentations” . In:Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pp. 4129–4138.\nHill, F elix; Reichart, Roi, and Korhonen, Anna (2014). “Simlex-999: Evaluating semantic models\nwith (genuine) similarity estimation” . In: Computational Linguistics41.4, pp. 665–695.\nHinton, Geoffrey; Vinyals, Oriol, and Dean, Jeff (2015). “Distilling the knowledge in a neural\nnetwork” .\nHirsimäki, T eemu; Creutz, Mathias; Siivola, V esa, and Kurimo, Mikko (June 2005). “Morpho-\nlogically Motivated Language Models in Speech Recognition” . In: Proceedings of AKRR’05,\nInternational and Interdisciplinary Conference on Adaptive Knowledge Representation and\nReasoning. Ed. by Timo Honkela; Ville Könönen; Matti Pöllä, and Olli Simula. Espoo, Fin-\nland: Helsinki University of T echnology , Laboratory of Computer and Information Science,\npp. 121–126. url: http://www.cis.hut.fi/AKRR05/papers/akrr05tuulos.pdf.\n124\nHochreiter, Sepp (1991). “Untersuchungen zu dynamischen neuronalen Netzen” . Diplomarbeit.\nMunich: Josef Hochreiter Institut für Informatik, T echnische Universität München.\nHochreiter, Sepp and Schmidhuber, Jürgen (Nov. 1997). “Long Short-T erm Memory” . In:Neural\nComputation 9.8, pp. 1735–1780.\nHornik, Kurt; Stinchcombe, Maxwell; White, Halbert, et al. (1989). “Multilayer feedforward\nnetworks are universal approximators.” In: Neural networks2.5, pp. 359–366.\nHuang, Eric; Socher, Richard; Manning, Christopher, and Ng, Andrew (2012). “Improving W ord\nRepresentations via Global Context and Multiple W ord Prototypes” . In: Proceedings of the\n50th Annual Meeting of the Association for Computational Linguistics (ACL 2012). Jeju\nIsland, Korea: Association for Computational Linguistics, pp. 873–882.\nHuang, Xuedong; Alleva, Fileno; Hon, Hsiao-W uen; Hwang, Mei-Y uh; Lee, Kai-F u, and Rosen-\nfeld, Ronald (1993). “The SPHINX-II speech recognition system: an overview” . In: Com-\nputer Speech & Language7.2, pp. 137–148. issn: 0885-2308. doi: https : / / doi . org / 10 .\n1006/csla.1993.1007 . url: http://www.sciencedirect.com/science/article/pii/\nS0885230883710077.\nHutchins, John (May 1995). “”The whisky was invisible”, or Persistent myths of MT” . In: MT\nNews International(11), pp. 32–34.\nInan, Hakan; Khosravi, Khashayar, and Socher, Richard (2017). “Tying W ord V ectors and W ord\nClassifiers: A Loss F ramework for Language Modeling” . In: International Conference on\nLearning Representations (ICLR 2017). arXiv: 1611.01462 [cs.LG].\nIndig, Balázs (2018). “Közös crawlnak is egy korpusz a vége – Korpuszépítés a CommonCrawl\n.hu domainjából” . In: XIV. Magyar Számítógépes Nyelvészeti Konferencia (MSZNY 2018).\nEd. by V eronika Vincze. Szegedi T udományegyetem Informatikai Intézet. Szeged: Szegedi\nT udományegyetem Informatikai T anszékcsoport, 125–134.\nIndig, Balázs; Sass, Bálint; Simon, Eszter; Mittelholcz, Iván; Kundráth, Péter, and V adász, Noémi\n(2019). “emtsv ⚶Egy formátum mind felett [emtsv – One format to rule them all]” . In: XV.\nMagyar Számítógépes Nyelvészeti Konferencia (MSZNY 2019). Ed. by Gábor Berend; Gábor\nGosztolya, and V eronika Vincze. Szegedi T udományegyetem Informatikai T anszékcsoport,\npp. 235–247.\nIndyk, Piotr and Motwani, Rajeev (1998). “Approximate nearest neighbors: towards removing\nthe curse of dimensionality” . In: Proceedings of the thirtieth annual ACM symposium on\nTheory of computing. ACM, pp. 604–613.\nIttzés, Nóra, ed. (2011). A magyar nyelv nagyszótára III-IV. Akadémiai Kiadó.\nJakubı́ček, Miloš; Kilgarriff, Adam; Kovář, V ojtěch; Rychlỳ, Pavel, and Suchomel, Vı́t (2013).\n“The tenten corpus family” . In:7th International Corpus Linguistics Conference CL, pp. 125–\n127.\n125\nJarrett, Kevin; Kavukcuoglu, Koray; Ranzato, Marc’Aurelio, and LeCun, Y ann (2009). “What is\nthe best multi-stage architecture for object recognition?” In: 2009 IEEE 12th international\nconference on computer vision. IEEE, pp. 2146–2153.\nJelinek, F.; Bahl, L.R., and Mercer, R.L. (1975). “Design of a linguistic statistical decoder for the\nrecognition of continuous speech” . In: IEEE Transactions on Acoustics, Speech and Signal\nProcessingIT-21, pp. 250–256.\nJelinek, F rederick; Mercer, Robert L.; Bahl, Lalit R., and Baker, James K. (Nov. 1977). “Per-\nplexity – a measure of the difficulty of speech recognition tasks” . In:Journal of the Acoustical\nSociety of America62. Supplement 1, S63.\nJelinek, F rederick and Mercer, Robert (1980). “Interpolated estimation of Markov source param-\neters from sparse data” . In:Proceedings of the Workshop on Pattern Recognition in Practice.\nEd. by E. S. Geltsema and L. N. Kanal. Amsterdam: North-Holland.\nJohansson, Stig; Leech, Geoffrey N, and Goodluck, Helen (1978). Manual of information to ac-\ncompany the Lancaster-Oslo/Bergen Corpus of British English, for use with digital computer.\nDepartment of English, University of Oslo.\nJordan, Michael I. (May 1986). Serial order: a parallel distributed processing approach. T ech. rep.\nICS 8604. San Diego, California: Institute for Cognitive Science, University of California.\nJoshi, Mandar; Chen, Danqi; Liu, Yinhan; W eld, Daniel S; Zettlemoyer, Luke, and Levy, Omer\n(2020). “SpanBER T: Improving pre-training by representing and predicting spans” . In:Trans-\nactions of the Association for Computational Linguistics8, pp. 64–77.\nJozefowicz, Rafal; Zaremba, W ojciech, and Sutskever, Ilya (2015). “An empirical exploration of\nrecurrent network architectures” . In: Proceedings of the 32nd International Conference on\nMachine Learning (ICML-15), pp. 2342–2350.\nJozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam, and W u, Y onghui (2016).\n“Exploring the limits of language modeling” . In: arXiv preprint arXiv:1602.02410.\nJurafsky , Daniel and Martin, James H. (n.d.).Speech and Language Processing. 3rd edition. url:\nhttps://web.stanford.edu/~jurafsky/slp3/.\nJurafsky , Daniel and Martin, James H. (2009). Speech and Language Processing. 2nd edition.\nPearson.\nKatz, S. (1987). “Estimation of probabilities from sparse data for the language model component\nof a speech recognizer” . In: IEEE Transactions on Acoustics, Speech and Signal processing\n35.3, pp. 400–401.\nKeskar, Nitish Shirish; McCann, Bryan; V arshney, Lav R; Xiong, Caiming, and Socher, Richard\n(2019). “CTRL: A conditional transformer language model for controllable generation” .\nKim, Y oon; Jernite, Y acine; Sontag, David, and Rush, Alexander M (2016). “Character-A ware\nNeural Language Models” . In: Proceedings of the Thirtieth AAAI Conference on Artificial\nIntelligence (AAAI-16). AAAI Press, pp. 2741–2749.\n126\nKirchhoff, Katrin; Bilmes, Jeff, and Duh, Kevin (2008). Factored Language Models Tutorial.\nT ech. rep. UWEETR-2008-00048. https://www.ee.washington.edu/techsite/papers/\nrefer/UWEETR-2008-0004.html . University of W ashington, Dept. of EE.\nKitaev, Nikita; Kaiser, Łukasz, and Levskaya, Anselm (2020). “Reformer: The Efficient T rans-\nformer” . In:InternationalConferenceonLearningRepresentations . url: https://openreview.\nnet/forum?id=rkgNKkHtvB.\nKneser, Reinhard and Ney, Hermann (1993). “Improved clustering techniques for class-based\nstatistical language modelling” . In: Eurospeech. V ol. 93, pp. 973–76.\nKoehn, Philipp et al. (2007). “Moses: Open source toolkit for statistical machine translation” .\nIn: Proceedings of the 45th annual meeting of the ACL. Association for Computational Lin-\nguistics, pp. 177–180.\nKohlschütter, Christian; F ankhauser, Peter, and Nejdl, W olfgang (2010). “Boilerplate detection\nusing shallow text features” . In: Proceedings of the third ACM international conference on\nWeb search and data mining. ACM, pp. 441–450.\nKornai, András (1994). “Language models: where are the bottlenecks?” In: AISB Quarterly88,\npp. 36–40.\nKuchaiev, Oleksii and Ginsburg, Boris (2017). “F actorization tricks for LSTM networks” . In:\nInternational Conference on Learning Representations (ICLR 2017). arXiv: 1703 . 10722\n[cs.CL]. url: https://openreview.net/forum?id=ByxWXyNFg.\nKudo, T aku (July 2018). “Subword Regularization: Improving Neural Network T ranslation Mod-\nels with Multiple Subword Candidates” . In: Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers). Melbourne, Australia:\nAssociation for Computational Linguistics, pp. 66–75. doi: 10.18653/v1/P18- 1007 . url:\nhttps://www.aclweb.org/anthology/P18-1007.\nKuhn, Roland and De Mori, Renato (1990). “A cache-based natural language model for speech\nrecognition” . In:IEEEtransactionsonpatternanalysisandmachineintelligence 12.6, pp. 570–\n583.\nLan, Zhenzhong; Chen, Mingda; Goodman, Sebastian; Gimpel, Kevin; Sharma, Piyush, and\nSoricut, Radu (2019). “ALBER T: A lite bert for self-supervised learning of language repre-\nsentations” .\nLau, Raymond; Rosenfeld, Ronald, and Roukos, Salim (1993). “T rigger-Based Language Models:\nA Maximum Entropy Approach” . In:Proceedings of the 1993 IEEE International Conference\non Acoustics, Speech, and Signal Processing: Speech Processing - Volume II. ICASSP n93.\nMinneapolis, Minnesota, USA: IEEE Computer Society, pp. 45–48. isbn: 0780309464.\nLe Cun, Y ann (June 1989). Generalization and Network Design Strategies. T ech. rep. CRG-TR-\n89-4. Department of Computer Science, University of T oronto.\nLeCun, Y ann; Bottou, Léon; Bengio, Y oshua, and Haffner, Patrick (1998). “Gradient-based learn-\ning applied to document recognition” . In: Proceedings of the IEEE86.11, pp. 2278–2324.\n127\nLeskovec, Jure; Rajaraman, Anand, and Ullman, Jeffrey David (2014).MiningofMassiveDatasets .\n2nd. USA: Cambridge University Press. isbn: 1107077230.\nLevesque, Hector J; Davis, Ernest, and Morgenstern, Leora (2011). “The Winograd schema\nchallenge.” In: AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning.\nV ol. 46, p. 47.\nLevy , Omer and Goldberg, Y oav (June 2014a). “Dependency-Based W ord Embeddings” . In:\nProceedings of the 52nd Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers). Baltimore, Maryland: Association for Computational Linguistics,\npp. 302–308. url: http://www.aclweb.org/anthology/P14-2050.\nLevy , Omer and Goldberg, Y oav (2014b). “Neural W ord Embedding as Implicit Matrix F actor-\nization” . In:Advances in Neural Information Processing Systems 27. Ed. by Z. Ghahramani;\nM. W elling; C. Cortes; N.D. Lawrence, and K.Q. W einberger, pp. 2177–2185.\nLevy , Omer; Goldberg, Y oav, and Dagan, Ido (2015). “Improving Distributional Similarity with\nLessons Learned from W ord Embeddings” . In:Transactions of the Association for Computa-\ntional Linguistics3, pp. 211–225.\nLewis, Mike; Liu, Yinhan; Goyal, Naman; Ghazvininejad, Marjan; Mohamed, Abdelrahman;\nLevy, Omer; Stoyanov, V es, and Zettlemoyer, Luke (2019). “Bart: Denoising sequence-to-\nsequence pre-training for natural language generation, translation, and comprehension” .\nLi, Jiwei and Jurafsky, Dan (Sept. 2015). “Do Multi-Sense Embeddings Improve Natural Lan-\nguage Understanding?” In: Proceedings of the 2015 Conference on Empirical Methods in\nNatural Language Processing. Lisbon, Portugal: Association for Computational Linguistics,\npp. 1722–1732. doi: 10.18653/v1/D15- 1200. url: http://www.aclweb.org/anthology/\nD15-1200.\nLidstone, G.J. (1920). “Note on the general case of the Bayes-Laplace formula for inductive or a\nposteriori probabilities” . In:\nLiu, Peter J.; Saleh, Mohammad; Pot, Etienne; Goodrich, Ben; Sepassi, Ryan; Kaiser, Lukasz,\nand Shazeer, Noam (2018). “Generating Wikipedia by Summarizing Long Sequences” . In:\nInternational Conference on Learning Representations. url: https : / / openreview . net /\nforum?id=Hyg0vbWC-.\nLiu, Yinhan et al. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv: 1907.\n11692 [cs.CL].\nLui, Marco and Baldwin, Timothy (2012). “langid.py: An off-the-shelf language identification\ntool” . In:Proceedings of the ACL 2012 system demonstrations. Association for Computational\nLinguistics, pp. 25–30.\nLuong, Minh-Thang; Pham, Hieu, and Manning, Christopher D (2015a). “Bilingual W ord Rep-\nresentations with Monolingual Quality in Mind” . In: Proceedings of NAACL-HLT, pp. 151–\n159.\n128\nLuong, Thang; Pham, Hieu, and Manning, Christopher D. (2015b). “Effective Approaches to\nAttention-based Neural Machine T ranslation” . In: Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing. Lisbon, Portugal: Association for Com-\nputational Linguistics, pp. 1412–1421. doi: 10 . 18653 / v1 / D15 - 1166. url: http : / / www .\naclweb.org/anthology/D15-1166.\nManin, Dmitrii Y. (2008). “Zipf ns Law and A voidance of Excessive Synonymy” . In: Cognitive\nScience 32 (7), pp. 1075–1098.\nMarcus, Mitchell; Santorini, Beatrice, and Marcinkiewicz, Mary Ann (1993). “Building a Large\nAnnotated Corpus of English: The Penn T reebank” . In:ComputationalLinguistics 19, pp. 313–\n330.\nMarkov, A. A. (1913). “Essai d’une recherche statistique sur le texte du roman “Eugene Onegin”\nillustrant la liaison des epreuve en chain (‘Example of a statistical investigation of the text of\n“Eugene Onegin” illustrating the dependence between samples in chain’)” . In:Izvistia Imper-\natorskoi Akademii Nauk (Bulletin de l’Académie Impériale des Sciences de St.-Pétersbourg).\n6th ser. 7. English translation by Morris Halle, 1956., pp. 153–162.\nMartin, Sven; Liermann, Jörg, and Ney, Hermann (1998). “Algorithms for bigram and trigram\nword clustering” . In:Speech communication24.1, pp. 19–37.\nMelis, Gábor; Dyer, Chris, and Blunsom, Phil (2018). “On the State of the Art of Evaluation in\nNeural Language Models” . In: International Conference on Learning Representations (ICLR\n2018). arXiv: 1707.05589 [cs.CL]. url: https://openreview.net/forum?id=ByJHuTgA-.\nMerity , Stephen; Xiong, Caiming; Bradbury, James, and Socher, Richard (2017). “Pointer Sen-\ntinel Mixture Models” . In: International Conference on Learning Representations (ICLR\n2017). arXiv: 1609.07843 [cs.LG].\nMerity , Stephen; McCann, Bryan, and Socher, Richard (2017).Revisiting Activation Regulariza-\ntion for Language RNNs. arXiv: 1708.01009 [cs.CL].\nMerity , Stephen; Keskar, Nitish Shirish, and Socher, Richard (2018). “Regularizing and optimiz-\ning lstm language models” . In:International Conference on Learning Representations (ICLR\n2018). arXiv: 1708.02182 [cs.LG].\nMihajlik, Péter; T uske, Zoltán; T arján, Balázs; Németh, Bottyán, and F egyó, Tibor (2010). “Im-\nproved recognition of spontaneous Hungarian speech iMorphological and acoustic modeling\ntechniques for a less resourced task” . In:IEEE Transactions on Audio, Speech, and Language\nProcessing18.6, pp. 1588–1600.\nMiháltz, Márton; Hatvani, Csaba; Kuti, Judit; Szarvas, György; Csirik, János; Prószéky, Gábor,\nand Váradi, T amás (2008). “Methods and results of the Hungarian WordNet project” . In:\nProceedings of the Fourth Global WordNet Conference (GWC-2008). Citeseer.\nMiháltz, Márton (2010). “Semantic resources and their applications in Hungarian natural lan-\nguage processing” . PhD thesis. Pázmány Péter Catholic University. url: https : / / itk .\nppke.hu/uploads/articles/163/file/Mihaltz_diss.pdf.\n129\nMikolov, T omas and Zweig, Geoffrey (2012). “Context dependent recurrent neural network lan-\nguage model” . In: SLT, pp. 234–239.\nMikolov, T omas (2012). “Statistical Language Models Based On Neural Networks” . PhD thesis.\nF aculty of Information T echnology , Brno University of T echnology.\nMikolov, T omas; Sutskever, Ilya; Chen, Kai; Corrado, Greg S, and Dean, Jeff (2013). “Distributed\nRepresentations of W ords and Phrases and their Compositionality” . In: Advances in Neu-\nral Information Processing Systems 26. Ed. by C.J.C. Burges; L. Bottou; M. W elling; Z.\nGhahramani, and K.Q. W einberger. Curran Associates, Inc., pp. 3111–3119. url: https :\n//bit.ly/39HikH8.\nMikolov, T omas; Chen, Kai; Corrado, G.s., and Dean, Jeffrey (May 2013). “Efficient Estimation\nof W ord Representations in V ector Space” . In: 1st International Conference on Learning\nRepresentations, ICLR 2013, Workshop Track Proceedings. Ed. by Y. Bengio and Y. LeCun.\narXiv: 1301.3781 [cs.CL]. url: http://arxiv.org/abs/1301.3781.\nMikolov, T omas; Le, Quoc V, and Sutskever, Ilya (2013). “Exploiting similarities among lan-\nguages for machine translation” . arXiv preprint arXiv:1309.4168.\nMikolov, T omas; Yih, W en-tau, and Zweig, Geoffrey (2013). “Linguistic Regularities in Con-\ntinuous Space W ord Representations” . In:Proceedings of the 2013 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies (NAACL-HLT 2013). Atlanta, Georgia: Association for Computational Linguistics,\npp. 746–751.\nMikolov, T omáš (2010).Recurrent neural network based language model. Presentation at Google.\nMikolov, T omáš; Kombrink, Stefan; Burget, Lukáš; Černocký, Jan, and Khudanpur, Sanjeev\n(2011). Extensions of recurrent neural network language model. Presentation at Google.\nMikolov, T omáš; Deoras, Anoop; Povey, Daniel; Burget, Lukáš, and Černocký, Jan (2011).\n“Strategies for training large scale neural network language models” . In: Automatic Speech\nRecognition and Understanding (ASRU), 2011 IEEE Workshop on. IEEE, pp. 196–201.\nMiller, George A. (1995). “W ordNet: a lexical database for English” . In:Communications of the\nACM 38.11, pp. 39–41.\nMimno, Davidan and Thompson, Laure (2017). “The strange geometry of skip-gram with neg-\native sampling” . In: Proceedings of the 2017 Conference on Empirical Methods in Natu-\nral Language Processing. Copenhagen, Denmark: Association for Computational Linguistics,\npp. 2873–2878. doi: 10.18653/v1/D17- 1308. url: http://aclweb.org/anthology/D17-\n1308.\nMinsky , Marvin and Papert, Seymour (1969). “An introduction to computational geometry” . In:\nMittelholcz, Iván (2017). “emToken: Unicode-képes tokenizáló magyar nyelvre” . In:XIII. Magyar\nSzámítógépes Nyelvészeti Konferencia (MSZNY2017). Szeged, (this volume).\n130\nMnih, Andriy and T eh, Y ee Whye (2012). “A F ast and Simple Algorithm for T raining Neural\nProbabilistic Language Models” . In: Proceedings of the 29th International Coference on Ma-\nchineLearning. ICMLn12. Edinburgh, Scotland: Omnipress, pp. 419–426.isbn: 9781450312851.\nMoore, Robert; Appelt, Douglas; Dowding, John; Gawron, J. Mark, and Moran, Douglas (1995).\n“Combining Linguistic and Statistical Knowledge Sources in Natural-Language Processing\nfor A TIS” . In:In ARPA Spoken Language Technology Workshop.\nMorin, F rederic and Bengio, Y oshua (2005). “Hierarchical Probabilistic Neural Network Language\nModel” . In:Aistats. V ol. 5. Citeseer, pp. 246–252.\nMozer, Michael C (1992). “Induction of multiscale temporal structure” . In: Advances in neural\ninformation processing systems, pp. 275–282.\nNeelakantan, Arvind; Shankar, Jeevan; Passos, Alexandre, and McCallum, Andrew (2014). “Ef-\nficient Non-parametric Estimation of Multiple Embeddings per W ord in V ector Space” . In:\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Process-\ning (EMNLP). Doha, Qatar: Association for Computational Linguistics, pp. 1059–1069. doi:\n10.3115/v1/D14-1113. url: http://www.aclweb.org/anthology/D14-1113.\nNemeskey , Dávid Márk and Simon, Eszter (2012). “Automatically generated NE tagged corpora\nfor English and Hungarian” . In: Proceedings of the 4th Named Entity Workshop. Association\nfor Computational Linguistics, pp. 38–46.\nNemeskey , Dávid Márk (2017). “ emLam – a Hungarian Language Modeling baseline” . In: XIII.\nMagyar Számítógépes Nyelvészeti Konferencia (MSZNY2017). Szeged, pp. 91–102. arXiv:\n1701.07880 [cs.CL].\nNemeskey , Dávid Márk (2020). “Egy emBERT próbáló feladat” . In: XVI. Magyar Számítógépes\nNyelvészeti Konferencia (MSZNY2020). Szeged, pp. 409–418.\nNémeth, Bottyán; Mihajlik, Péter; Tikk, Domonkos, and T rón, Viktor (Nov. 2007). “Statisztikai\nés szabály alapú morfológiai elemzők kombinációja beszédfelismerő alkalmazáshoz” . In: Pro-\nceedings of MSZNY 2007. Ed. by Attila T anács and Dóra Csendes. Szegedi T udománye-\ngyetem, pp. 95–105.\nNey , Hermann; Essen, Ute, and Kneser, Reinhard (1994). “On structuring probabilistic depen-\ndences in stochastic language modelling” . In: Comput. Speech Lang.8, pp. 1–38.\nNey , Hermann; Martin, Sven, and W essel, F rank (1997). “Statistical language modeling using\nleaving-one-out” . In: Corpus-based methods in Language and Speech processing. Springer,\npp. 174–207.\nNiesler, Thomas R; Whittaker, Edward WD, and W oodland, Philip C (1998). “Comparison of\npart-of-speech and automatically derived category-based language models for speech recog-\nnition” . In: Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998 IEEE\nInternational Conference on. V ol. 1. IEEE, pp. 177–180.\nOch, F ranz Josef; Tillmann, Christoph, and Ney, Hermann (1999). “Improved Alignment Mod-\nels for Statistical Machine T ranslation” . In: 1999 Joint SIGDAT Conference on Empirical\n131\nMethods in Natural Language Processing and Very Large Corpora, pp. 20–28. url: https:\n//www.aclweb.org/anthology/W99-0604.\nOravecz, Csaba; Váradi, T amás, and Sass, Bálint (2014). “The Hungarian Gigaword Corpus” .\nIn: Proceedings of the Ninth International Conference on Language Resources and Evaluation\n(LREC-2014). Reykjavik, Iceland: European Language Resources Association (ELRA). url:\nhttp://www.aclweb.org/anthology/L14-1536.\nOrtiz Suárez, Pedro Javier; Sagot, Benoît, and Romary, Laurent (July 2019). “Asynchronous\nPipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures” . In: 7th\nWorkshop on the Challenges in the Management of Large Corpora (CMLC-7). Cardiff, United\nKingdom. url: https://hal.inria.fr/hal-02148693.\nPanchenko, A; Ruppert, E; F aralli, S; Ponzetto, S.P, and Biemann, C (2018). “Building a W eb-\nScale Dependency-Parsed Corpus from Common Crawl” . In: Proceedings of LREC 2018.\nELRA.\nParra Escartı́n, Carla; Reijers, W essel; Lynn, T eresa; Moorkens, Joss; W ay, Andy, and Liu,\nChao-Hong (Apr. 2017). “Ethical Considerations in NLP Shared T asks” . In: Proceedings\nof the First ACL Workshop on Ethics in Natural Language Processing. V alencia, Spain:\nAssociation for Computational Linguistics, pp. 66–73. doi: 10.18653/v1/W17- 1608 . url:\nhttps://www.aclweb.org/anthology/W17-1608.\nPascanu, Razvan; Mikolov, T omas, and Bengio, Y oshua (2013). “On the difficulty of training\nrecurrent neural networks” . In:ICML (3)28, pp. 1310–1318.\nPaszke, Adam et al. (2017). “Automatic Differentiation in PyT orch” . In:NIPS Autodiff Workshop.\nPaszke, Adam et al. (2019). “PyT orch: An Imperative Style, High-Performance Deep Learning\nLibrary” . In: Advances in Neural Information Processing Systems 32. Ed. by H. W allach;\nH. Larochelle; A. Beygelzimer; F. d’Alché-Buc; E. F ox, and R. Garnett. Curran Associates,\nInc., pp. 8026–8037. url: http://papers.nips.cc/paper/9015-pytorch-an-imperative-\nstyle-high-performance-deep-learning-library.pdf .\nPennington, Jeffrey; Socher, Richard, and Manning, Christopher (2014). “Glove: Global V ectors\nfor W ord Representation” . In: Proceedings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP). Doha, Qatar: Association for Computational\nLinguistics, pp. 1532–1543. doi: 10.3115/v1/D14- 1162 . url: http://www.aclweb.org/\nanthology/D14-1162.\nPereira, F ernando; Tishby, Naftali, and Lee, Lillian (1993). “Distributional clustering of En-\nglish words” . In: Proceedings of the 31st annual meeting on Association for Computational\nLinguistics. Association for Computational Linguistics, pp. 183–190.\nPeters, Matthew; Neumann, Mark; Iyyer, Mohit; Gardner, Matt; Clark, Christopher; Lee, Ken-\nton, and Zettlemoyer, Luke (2018). “Deep Contextualized W ord Representations” . In: Pro-\nceedings of the 2018 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Volume 1 (Long Papers). New Orleans,\n132\nLouisiana: Association for Computational Linguistics, pp. 2227–2237. doi: 10 . 18653 / v1 /\nN18-1202. url: http://aclweb.org/anthology/N18-1202.\nPomikálek, Jan (2011). “Removing boilerplate and duplicate content from web corpora” . PhD\nthesis. Brno, Czech Republic: F aculty of informatics, Masaryk university.\nPonte, Jay M. and Croft, W. Bruce (1998). “A language modeling approach to information\nretrieval” . In:Proc SIGIR. ACM Press, pp. 275–281.\nPress, Ofir and W olf, Lior (Apr. 2017). “Using the Output Embedding to Improve Language\nModels” . In:Proceedings of the 15th Conference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Papers. V alencia, Spain: Association for\nComputational Linguistics, pp. 157–163. arXiv: 1608 . 05859 [cs.CL]. url: http : / / www .\naclweb.org/anthology/E17-2025.\nPusztai, F erenc, ed. (2003). Magyar értelmező kéziszótár. Akadémiai Kiadó.\nRadford, Alec; W u, Jeffrey; Amodei, Dario; Amodei, Daniela; Clark, Jack; Brundage, Miles, and\nSutskever, Ilya (n.d.). Better Language Models and Their Implications. https : / / openai .\ncom/blog/better-language-models/ . Accessed: 2020-03-18.\nRadford, Alec; Narasimhan, Karthik; Salimans, Tim, and Sutskever, Ilya (2018). “Improving lan-\nguage understanding by generative pre-training” .https://s3-us-west-2.amazonaws.com/\nopenai- assets/research- covers/language- unsupervised/language_understanding_\npaper.pdf.\nRadford, Alec; W u, Jeffrey; Child, Rewon; Luan, David; Amodei, Dario, and Sutskever, Ilya\n(2019). “Language Models are Unsupervised Multitask Learners” . https : / / github . com /\nopenai/gpt-2. url: https://d4mucfpksywv.cloudfront.net/better-language-models/\nlanguage-models.pdf.\nRajpurkar, Pranav; Zhang, Jian; Lopyrev, Konstantin, and Liang, Percy (Nov. 2016). “SQuAD:\n100,000+ Questions for Machine Comprehension of T ext” . In:Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language Processing. Austin, T exas: Association\nfor Computational Linguistics, pp. 2383–2392. doi: 10.18653/v1/D16- 1264 . url: https:\n//www.aclweb.org/anthology/D16-1264.\nRajpurkar, Pranav; Jia, Robin, and Liang, Percy (July 2018). “Know What Y ou Don’t Know:\nUnanswerable Questions for SQuAD” . In: Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers). Melbourne, Australia:\nAssociation for Computational Linguistics, pp. 784–789. doi: 10.18653/v1/P18-2124. url:\nhttps://www.aclweb.org/anthology/P18-2124.\nRecski, Gábor (2010). “Főnévi csoportok azonosítása szabályalapú és hibrid módszerekkel” . In:\nVII. Magyar Számı́tógépes Nyelvészeti Konferencia. Ed. by Attila T anács and V eronika\nVincze, pp. 333–341.\nRecski, Gábor; Borbély, Gábor, and Bolevácz, Attila (2016). “Building definition graphs using\nmonolingual dictionaries of Hungarian” . In:XI. Magyar Számitógépes Nyelvészeti Konferencia\n133\n[11th Hungarian Conference on Computational Linguistics. Ed. by Attila T anács; Viktor\nV arga, and V eronika Vincze.\nReisinger, Joseph and Mooney, Raymond J (2010). “Multi-prototype vector-space models of word\nmeaning” . In:The 2010 Annual Conference of the North American Chapter of the Association\nfor Computational Linguistics. Association for Computational Linguistics, pp. 109–117.\nRobbins, Herbert and Monro, Sutton (1951). “A stochastic approximation method” . In: The\nannals of mathematical statistics, pp. 400–407.\nRogers, Anna (June 2019). How the Transformers broke NLP leaderboards. url: https : / /\nhackingsemantics.xyz/2019/leaderboards/.\nRosenblatt, F rank (1957). The Perceptron: a perceiving and recognizing automaton. T ech. rep.\n85-460-1.\nRosenfeld, R. (1994). “Adaptive Statistical Language Modeling: A Maximum Entropy Approach” .\nPhD thesis. Carnegie Mellon University.\nRosenfeld, Ronald (Aug. 2000). “T wo decades of Statistical Language Modeling: Where Do W e\nGo F rom Here?” In: Proceedings of the IEEE88.8.\nRothe, Sascha; Ebert, Sebastian, and Schütze, Hinrich (June 2016). “Ultradense W ord Em-\nbeddings by Orthogonal T ransformation” . In: Proceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies. San Diego, California: Association for Computational Linguistics, pp. 767–777.\narXiv: 1602.07572 [cs.CL]. url: http://www.aclweb.org/anthology/N16-1091.\nRumelhart, David E; Hinton, Geoffrey E, and Williams, Ronald J (Sept. 1985).Learning internal\nrepresentations by error propagation. T ech. rep. ICS 8504. San Diego, California: Institute\nfor Cognitive Science, University of California.\nRumelhart, David E.; Hinton, Geoffrey E., and Williams, Ronald J (1986). “Learning represen-\ntations by back-propagating errors” . In: Nature 323.6088, pp. 533–536.\nSak, Hasim; Senior, Andrew W, and Beaufays, F rançoise (2014). “Long short-term memory re-\ncurrent neural network architectures for large scale acoustic modeling.” In: INTERSPEECH,\npp. 338–342.\nSanh, Victor; Debut, Lysandre; Chaumond, Julien, and W olf, Thomas (2019). “DistilBER T, a\ndistilled version of BER T: smaller, faster, cheaper and lighter” .\nSchuster, Mike and Nakajima, Kaisuke (2012). “Japanese and korean voice search” . In: 2012\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,\npp. 5149–5152.\nSchwenk, Holger and Gauvain, Jean-Luc (2005). “T raining neural network language models on\nvery large corpora” . In: Proceedings of the conference on Human Language Technology and\nEmpirical Methods in Natural Language Processing. Association for Computational Linguis-\ntics, pp. 201–208.\n134\nSchwenk, Holger (July 2007). “Continuous Space Language Models” . In: Comput. Speech Lang.\n21.3, pp. 492–518. issn: 0885-2308. doi: 10.1016/j.csl.2006.09.003 . url: http://dx.\ndoi.org/10.1016/j.csl.2006.09.003.\nSemeniuta, Stanislau; Severyn, Aliaksei, and Barth, Erhardt (2016). “Recurrent Dropout without\nMemory Loss” . In: The 26th International Conference on Computational Linguistics (COL-\nING). Osaka, Japan, pp. 1757–1766. url: http://www.aclweb.org/anthology/C16-1165.\nSennrich, Rico; Haddow, Barry, and Birch, Alexandra (Aug. 2016). “Neural Machine T ransla-\ntion of Rare W ords with Subword Units” . In: Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers). Berlin, Germany:\nAssociation for Computational Linguistics, pp. 1715–1725. doi: 10 . 18653 / v1 / P16 - 1162.\nurl: https://www.aclweb.org/anthology/P16-1162.\nShannon, Claude E. (1948). “A Mathematical Theory of Communication” . In:Bell System Tech-\nnical Journal27, pp. 379–423, 623–656.\nSilver, David et al. (2016). “Mastering the game of Go with deep neural networks and tree\nsearch” . In: Nature 529, pp. 484–503. url: http : / / www . nature . com / nature / journal /\nv529/n7587/full/nature16961.html.\nSinclair, John M. (1987). Looking up: an account of the COBUILD project in lexical computing.\nCollins EL T.\nSmolensky , Paul (1986). “Information Processing in Dynamical Systems: F oundations of Har-\nmony Theory” . In: Parallel Distributed Processing: Explorations in the Microstructure of\nCognition, Vol. 1: Foundations. Ed. by James Lloyd McClelland and David Everett Rumel-\nhart. Cambridge, MA, USA: MIT Press, pp. 194–281. isbn: 026268053X.\nSocher, Richard; Bauer, John; Manning, Christopher D., and Andrew Y., Ng (2013). “Pars-\ning with Compositional V ector Grammars” . In: Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (ACL 2013). Sofia, Bulgaria: Association for\nComputational Linguistics, pp. 455–465.\nSrivastava, Nitish; Hinton, Geoffrey E; Krizhevsky, Alex; Sutskever, Ilya, and Salakhutdinov,\nRuslan (2014). “Dropout: a simple way to prevent neural networks from overfitting” . In:\nJournal of Machine Learning Research15.1, pp. 1929–1958.\nSrivastava, Rupesh K; Greff, Klaus, and Schmidhuber, Jürgen (2015). Training very deep net-\nworks, pp. 2377–2385.\nStolcke, Andreas; Zheng, Jing; W ang, W en, and Abrash, Victor (2011). “SRILM at sixteen: Up-\ndate and outlook” . In:Proceedings of IEEE Automatic Speech Recognition and Understanding\nWorkshop. V ol. 5.\nSuchomel, Vı́t; Pomikálek, Jan, et al. (2012). “Efficient web crawling for large text corpora” . In:\nProceedings of the seventh Web as Corpus Workshop (WAC7), pp. 39–43.\nSundermeyer, Martin; Schlüter, Ralf, and Ney, Hermann (2012). “LSTM Neural Networks for\nLanguage Modeling” . In: INTERSPEECH, pp. 194–197.\n135\nSutskever, Ilya; Martens, James, and Hinton, Geoffrey E (2011). “Generating text with recurrent\nneural networks” . In:Proceedings of the 28th International Conference on Machine Learning\n(ICML-11), pp. 1017–1024.\nSutskever, Ilya; Vinyals, Oriol, and Le, Quoc V. (2014). “Sequence to Sequence Learning with\nNeural Networks” . In:Proc. NIPS. Montreal, CA, pp. 3104–3112. url: http://arxiv.org/\nabs/1409.3215.\nSzarvas, György; F arkas, Richárd, and Kocsor, András (2006). “A Multilingual Named Entity\nRecognition System Using Boosting and C4.5 Decision T ree Learning Algorithms” . In: Dis-\ncovery Science, 9th International Conference, DS 2006, Barcelona, Spain, October 8-10,\n2006, Proceedings, pp. 268–278.\nT arján, Balázs; V arga, Ádám; T obler, Zoltán; Szaszák, György; F egyó, Tibor; Bordás, Csaba,\nand Mihajlik, Péter (2016). “Magyar nyelvű, élő közéleti- és hírműsorok gépi feliratozása” .\nIn: Proc. MSZNY 2016. Ed. by Attila T anács; Viktor V arga, and V eronika Vincze. Szegedi\nT udományegyetem, pp. 89–99.\nT aylor, Wilson L. (1953). “ oCloze Procedure p: A New T ool for Measuring Readability” . In:\nJournalism Quarterly30.4, pp. 415–433. doi: 10.1177/107769905303000401.\nT esniére, Lucien (1959). Élements de syntaxe structurale. Paris: Klincksieck.\nTiedemann, Jörg (May 2012). “Parallel Data, T ools and Interfaces in OPUS” . In:LREC. Ed. by\nNicoletta Calzolari. Istanbul, T urkey: European Language Resources Association (ELRA).\nisbn: 978-2-9517408-7-7. url: http : / / www . lrec - conf . org / proceedings / lrec2012 /\nsummaries/463.html.\nT rinh, T rieu H and Le, Quoc V (2018). A simple method for commonsense reasoning. arXiv:\n1806.02847 [cs.AI].\nT rón, Viktor; Gyepesi, György; Halácsky, Péter; Kornai, András; Németh, László, and V arga,\nDániel (2005). “Hunmorph: Open Source W ord Analysis” . In:Proceedings of the ACL Work-\nshop on Software. Ann Arbor, Michigan: Association for Computational Linguistics, pp. 77–\n85.\nT urian, Joseph; Ratinov, Lev-Arie, and Bengio, Y oshua (2010). “W ord Representations: A Simple\nand General Method for Semi-Supervised Learning” . In: Proceedings of the 48th Annual\nMeeting of the Association for Computational Linguistics. Uppsala, Sweden: Association for\nComputational Linguistics, pp. 384–394.\nUgray , Gábor (2019). “PoS-tagging and lemmatization with a deep recurrent neural network” .\nIn: XV. Magyar Számítógépes Nyelvészeti Konferencia (MSZNY2019). Szeged, pp. 215–224.\nVáradi, T amás (2002). “The Hungarian National Corpus” . In:Proceedings of the Third Interna-\ntional Conference on Language Resources and Evaluation, pp. 385–389.\nVáradi, T amás et al. (2017). “ e-magyar: digitális nyelvfeldolgozó rendszer” . In: XIII. Magyar\nSzámítógépes Nyelvészeti Konferencia (MSZNY2017). Szeged.\n136\nV arga, Dániel and Simon, Eszter (F eb. 2007). “Hungarian Named Entity Recognition with a\nMaximum Entropy Approach” . In:Acta Cybern.18.2, pp. 293–301.\nV aswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N;\nKaiser, Łukasz, and Polosukhin, Illia (2017). “Attention is All you Need” . In: Advances in\nNeural Information Processing Systems 30. Ed. by I. Guyon; U. V. Luxburg; S. Bengio; H.\nW allach; R. F ergus; S. Vishwanathan, and R. Garnett. Curran Associates, Inc., pp. 5998–\n6008. arXiv: 1706.03762 [cs.CL]. url: http://papers.nips.cc/paper/7181-attention-\nis-all-you-need.pdf .\nVincent, Pascal; Brébisson, Alexandre de, and Bouthillier, Xavier (2015). “Efficient exact gradi-\nent update for training deep networks with very large sparse targets” . In:Advances in Neural\nInformation Processing Systems, pp. 1108–1116.\nVincze, V eronika; V arga, Viktor; Simkó, Katalin Ilona; Zsibrita, János; Nagy, Ágoston; F arkas,\nRichárd, and Csirik, János (May 2014). “Szeged Corpus 2.5: Morphological Modifications in a\nManually POS-tagged Hungarian Corpus” . English. In:Proceedings of the Ninth International\nConference on Language Resources and Evaluation (LREC’14). Ed. by Nicoletta Calzolari\n(Conference Chair); Khalid Choukri; Thierry Declerck; Hrafn Loftsson; Bente Maegaard;\nJoseph Mariani; Asuncion Moreno; Jan Odijk, and Stelios Piperidis. Reykjavik, Iceland:\nEuropean Language Resources Association (ELRA). isbn: 978-2-9517408-8-4.\nVinyals, Oriol; F ortunato, Meire, and Jaitly, Navdeep (2015). “Pointer networks” . In: Advances\nin Neural Information Processing Systems, pp. 2692–2700.\nV oita, Elena; T albot, David; Moiseev, F edor; Sennrich, Rico, and Titov, Ivan (July 2019). “Ana-\nlyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be\nPruned” . In: Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics. Florence, Italy: Association for Computational Linguistics, pp. 5797–5808. doi:\n10.18653/v1/P19-1580. url: https://www.aclweb.org/anthology/P19-1580.\nW ang, Alex; Singh, Amanpreet; Michael, Julian; Hill, F elix; Levy, Omer, and Bowman, Samuel R\n(2018). Glue: A multi-task benchmark and analysis platform for natural language understand-\ning. arXiv: 1804.07461 [cs.CL].\nW endlandt, Laura; Kummerfeld, Jonathan K., and Mihalcea, Rada (June 2018). “F actors In-\nfluencing the Surprising Instability of W ord Embeddings” . In:Proceedings of the 2018 Con-\nference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Papers). New Orleans, Louisiana: Associ-\nation for Computational Linguistics, pp. 2092–2102. doi: 10 . 18653 / v1 / N18 - 1190. url:\nhttps://www.aclweb.org/anthology/N18-1190.\nW erbos, Paul J (1988). “Generalization of backpropagation with application to a recurrent gas\nmarket model” . In: Neural networks1.4, pp. 339–356.\nW eston, Jason; Chopra, Sumit, and Bordes, Antoine (2015). “Memory networks” . In: Interna-\ntional Conference on Learning Representations (ICLR 2015). arXiv: 1410.3916 [cs.CL].\n137\nWilliams, Ronald J and Peng, Jing (1990). “An efficient gradient-based algorithm for on-line\ntraining of recurrent network trajectories” . In: Neural computation2.4, pp. 490–501.\nWilliams, Ronald J and Zipser, David (F eb. 1995). “Gradient-based learning algorithms for re-\ncurrent networks and their computational complexity” . In:Back-propagation: Theory, archi-\ntectures and applications. Ed. by Y ves Chauvin and David E Rumelhart. Chap. 13, pp. 433–\n486. isbn: 978-0-8058125-9-6.\nW olf, Thomas et al. (2019).HuggingFace’s Transformers: State-of-the-art Natural Language Pro-\ncessing. arXiv: 1910.03771 [cs.CL].\nXu, W ei and Rudnicky, Alexander I (2000). “Can artificial neural networks learn language\nmodels?” In: International Conference on Statistical Language Processing. Beijing, China,\npp. 202–205.\nY ang, Zhilin; Dai, Zihang; Salakhutdinov, Ruslan, and Cohen, William W. (2018). “Breaking\nThe Softmax Bottleneck: A High-Rank RNN Language Model” . In:International Conference\non Learning Representations (ICLR 2018). arXiv: 1711.03953 [cs.LG].\nY ang, Zhilin; Dai, Zihang; Y ang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan, and Le,\nQuoc V (2019). “XLNet: Generalized Autoregressive Pretraining for Language Understand-\ning” . In: Advances in neural information processing systems, pp. 5754–5764. arXiv: 1906 .\n08237 [cs.CL] . url: https : / / papers . nips . cc / paper / 8812 - xlnet - generalized -\nautoregressive-pretraining-for-language-understanding.pdf .\nY oun, Hyejin; Sutton, Logan; Smith, Eric; Moore, Cristopher; Wilkins, Jon F.; Maddieson, Ian;\nCroft, William, and Bhattacharya, T anmoy (2016). “On the universal structure of human\nlexical semantics” . In:PNAS 113.7, pp. 1766–1771.\nY ounger, Daniel H. (1967). “Recognition and Parsing of Context-F ree Languages in Time n3” .\nIn: Information and Control10, pp. 189–208.\nZaremba, W ojciech; Sutskever, Ilya, and Vinyals, Oriol (2014). “Recurrent neural network regu-\nlarization” . If you want to cite this paper, please cite Pham:2014 instead.\nZeiler, Matthew D and F ergus, Rob (2014). “Visualizing and understanding convolutional net-\nworks” . In:European conference on computer vision. Springer, pp. 818–833.\nZeman, Daniel et al. (Aug. 2017). “CoNLL 2017 Shared T ask: Multilingual Parsing from Raw\nT ext to Universal Dependencies” . In:Proceedings of the CoNLL 2017 Shared Task: Multilin-\ngual Parsing from Raw Text to Universal Dependencies. V ancouver, Canada: Association for\nComputational Linguistics, pp. 1–19.\nZgusta, Ladislav (1971). Manual of lexicography. Prague: Academia.\nZilly , Julian Georg; Srivastava, Rupesh Kumar; Koutník, Jan, and Schmidhuber, Jürgen (2017).\n“Recurrent Highway Networks” . In: Proceedings of the 34th International Conference on\nMachine Learning. arXiv: 1607.03474 [cs.LG].\n138\nZoph, Barret and Le, Quoc (2017). “Neural Architecture Search with Reinforcement Learning” .\nIn: International Conference on Learning Representations (ICLR 2017). url: https : / /\nopenreview.net/forum?id=r1Ue8Hcxg.\nZsibrita, János; Vincze, V eronika, and F arkas, Richárd (2013). “magyarlanc: A T ool for Mor-\nphological and Dependency Parsing of Hungarian” . In:Proceedings of the International Con-\nference Recent Advances in Natural Language Processing (RANLP 2013). Hissar, Bulgaria:\nINCOMA Ltd. Shoumen, pp. 763–771.\n139\n1ADATLAP \na doktori értekezés nyilvánosságra hozatalához\nI. A doktori értekezés adatai\nA szerző neve: Nemeskey Dávid Márk\nMTMT-azonosító: 10019809\nA doktori értekezés címe és alcíme: Natural Language Processing Methods for Language Modeling\nDOI-azonosító2: 10.15476/ELTE.2020.066\nA doktori iskola neve: Informatika Doktori iskola\nA doktori iskolán belüli doktori program neve: Az informatika alapjai és módszertana\nA témavezető neve és tudományos fokozata: Benczúr András Ph.D. , Kornai András D.Sc.\nA témavezető munkahelye: Számítástechnikai és Automatizálási Kutatóintézet\nII. Nyilatkozatok \n1. A doktori értekezés szerzőjeként3 \na) hozzájárulok, hogy a doktori fokozat megszerzését követően a doktori értekezésem és a tézisek\nnyilvánosságra  kerüljenek  az  ELTE  Digitális  Intézményi  Tudástárban.  Felhatalmazom  az  Informatika\nDoktori Iskola hivatalának ügyintézőjét, Kulcsár Adinát, hogy az értekezést és a téziseket feltöltse az\nELTE Digitális Intézményi Tudástárba, és ennek során kitöltse a feltöltéshez szükséges nyilatkozatokat. \nb)  kérem,  hogy  a  mellékelt  kérelemben  részletezett  szabadalmi,  illetőleg  oltalmi  bejelentés\nközzétételéig a doktori értekezést ne bocsássák nyilvánosságra az Egyetemi Könyvtárban és az ELTE\nDigitális Intézményi Tudástárban;4\nc)  kérem,  hogy  a  nemzetbiztonsági  okból  minősített  adatot  tartalmazó  doktori  értekezést  a\nminősítés (dátum)-ig tartó időtartama alatt ne bocsássák nyilvánosságra az Egyetemi Könyvtárban és az\nELTE Digitális Intézményi Tudástárban;5\nd)  kérem,  hogy  a  mű  kiadására  vonatkozó  mellékelt  kiadó  szerződésre  tekintettel  a  doktori\nértekezést a könyv megjelenéséig ne bocsássák nyilvánosságra az Egyetemi Könyvtárban, és az ELTE\nDigitális  Intézményi  Tudástárban  csak  a  könyv  bibliográfiai  adatait  tegyék  közzé. Ha  a  könyv  a\nfokozatszerzést  követőn  egy évig nem jelenik meg, hozzájárulok, hogy a doktori értekezésem  és a\ntézisek  nyilvánosságra  kerüljenek  az  Egyetemi  Könyvtárban  és  az  ELTE  Digitális  Intézményi\nTudástárban.6\n2. A doktori értekezés szerzőjeként kijelentem, hogy \na) az  ELTE Digitális Intézményi Tudástárba  feltöltendő doktori értekezés és a tézisek  saját eredeti ,\nönálló szellemi munkám és legjobb tudomásom szerint nem sértem vele senki szerzői jogait; \nb) a doktori értekezés és a tézisek nyomtatott változatai és az elektronikus adathordozón benyújtott\ntartalmak (szöveg és ábrák) mindenben megegyeznek.\n3. A  doktori  értekezés  szerzőjeként  hozzájárulok  a  doktori  értekezés  és  a  tézisek  szövegének\nplágiumkereső adatbázisba helyezéséhez és plágiumellenőrző vizsgálatok lefuttatásához.\nKelt: Budaörs, 2020. május 11.\nNemeskey Dávid Márk\n1 Beiktatta az Egyetemi Doktori Szabályzat módosításáról szóló CXXXIX/2014. (VI. 30.) Szen. sz. határozat.\nHatályos: 2014. VII.1. napjától.2 A kari hivatal ügyintézője tölti ki.3 A megfelelő szöveg aláhúzandó. 4 A doktori értekezés benyújtásával egyidejűleg be kell adni a tudományági doktori tanácshoz a szabadalmi,\nilletőleg oltalmi bejelentést tanúsító okiratot és a nyilvánosságra hozatal elhalasztása iránti kérelmet.5 A doktori értekezés benyújtásával egyidejűleg be kell nyújtani a minősített adatra vonatkozó közokiratot. 6 A doktori értekezés benyújtásával egyidejűleg be kell nyújtani a mű kiadásáról szóló kiadói szerződést.\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7823599576950073
    },
    {
      "name": "Natural language processing",
      "score": 0.6988704204559326
    },
    {
      "name": "Language model",
      "score": 0.6857437491416931
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5903191566467285
    },
    {
      "name": "Pipeline (software)",
      "score": 0.5797891616821289
    },
    {
      "name": "Word (group theory)",
      "score": 0.5362837314605713
    },
    {
      "name": "Word embedding",
      "score": 0.5195487141609192
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5012333393096924
    },
    {
      "name": "Natural language",
      "score": 0.45288822054862976
    },
    {
      "name": "Embedding",
      "score": 0.3622143268585205
    },
    {
      "name": "Linguistics",
      "score": 0.3422679305076599
    },
    {
      "name": "Programming language",
      "score": 0.15907368063926697
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}