{
  "title": "Retrieval-augmented Video Encoding for Instructional Captioning",
  "url": "https://openalex.org/W4385570540",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3113931551",
      "name": "Yeonjoon Jung",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2114836216",
      "name": "Minsoo Kim",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2463447918",
      "name": "Seungtaek Choi",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2156159218",
      "name": "Ji-Hyuk Kim",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2124480462",
      "name": "Min-Ji Seo",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2162241770",
      "name": "Seung-won Hwang",
      "affiliations": [
        "Yonsei University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2250965435",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W2594270457",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W2252269235",
    "https://openalex.org/W3160179442",
    "https://openalex.org/W3213100861",
    "https://openalex.org/W3130289102",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W2519328139",
    "https://openalex.org/W2097732278",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2951098185",
    "https://openalex.org/W2607119937",
    "https://openalex.org/W4286336838",
    "https://openalex.org/W3092714372",
    "https://openalex.org/W2995289474",
    "https://openalex.org/W3105441977",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W3108772932",
    "https://openalex.org/W3131087869",
    "https://openalex.org/W4312761738",
    "https://openalex.org/W4214663214",
    "https://openalex.org/W3123599388",
    "https://openalex.org/W2982651953",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3128081289",
    "https://openalex.org/W2798708692",
    "https://openalex.org/W2963916161",
    "https://openalex.org/W1740041947",
    "https://openalex.org/W4327988621"
  ],
  "abstract": "Instructional videos make learning knowledge more efficient, by providing a detailed multimodal context of each procedure in instruction.A unique challenge posed by instructional videos is key-object degeneracy, where any single modality fails to sufficiently capture the key objects referred to in the procedure.For machine systems, such degeneracy can disturb the performance of a downstream task such as dense video captioning, leading to the generation of incorrect captions omitting key objects.To repair degeneracy, we propose a retrieval-based framework to augment the model representations in the presence of such key-object degeneracy.We validate the effectiveness and generalizability of our proposed framework over baselines using modalities with key-object degeneracy.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 8554–8568\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nRetrieval-augmented Video Encoding for Instructional Captioning\nYeonjoon Jung♠ Minsoo Kim♠ Seungtaek Choi♣\nJihyuk Kim♡ Minji Seo♠ Seung-won Hwang∗♠\n♠Seoul National University ♣Riiid AI Research ♡Yonsei University\n{y970120, minsoo9574, minjiseo, seungwonh}@snu.ac.kr\n{seungtaek.choi}@riiid.co {jihyukkim}@yonsei.ac.kr\nAbstract\nInstructional videos make learning knowledge\nmore efficient, by providing a detailed mul-\ntimodal context of each procedure in instruc-\ntion. A unique challenge posed by instructional\nvideos is key-object degeneracy, where any sin-\ngle modality fails to sufficiently capture the\nkey objects referred to in the procedure. For\nmachine systems, such degeneracy can disturb\nthe performance of a downstream task such as\ndense video captioning, leading to the genera-\ntion of incorrect captions omitting key objects.\nTo repair degeneracy, we propose a retrieval-\nbased framework to augment the model repre-\nsentations in the presence of such key-object\ndegeneracy. We validate the effectiveness and\ngeneralizability of our proposed framework\nover baselines using modalities with key-object\ndegeneracy.\n1 Introduction\nInstructions, which provide detailed information\nabout the procedures required to achieve the de-\nsired goal, are a central part of how humans acquire\nprocedural knowledge. Instructions decompose a\nsequence of complex procedures into key objects\nand the associated actions expressed as verbs. As\nmachine systems increasingly aim to provide real-\nworld utility for humans, their ability to translate\nhuman goals into natural language instructions to\nfollow becomes essential (Ahn et al., 2022). In\nthis light, instructional captioning, summarizing in-\nstructional videos into a set of succinct instructions,\nis thus an important component of enabling the dis-\ntillation of human-level procedural knowledge to\nmachines.\nFor instructional captioning, we focus on the\ntask of dense video captioning (DVC) (Krishna\net al., 2017) which aims to produce a precise set\nof instructions from visual input (e.g. instructional\nvideos). For example, to illustrate the procedure\n∗Corresponding author.\ns2 in Figure 1, the instructional video details the\nprocedure, while simultaneously showing how this\naction is performed. DVC system can then summa-\nrize this video into a set of salient captions, form-\ning a set of instructions that enhances the visual\ndemonstration with informative text descriptions.\nWhile the task of extracting a salient instruction\nfrom complex visual input can be effortless for hu-\nmans, it presents a unique challenge for machine\nsystems, which we denote as key-object degener-\nacy. That is, machine systems can often fail at the\nfundamental task of key-object recognition, which\nis core to instructions. This is due to the fact that\nfrequently, key objects are not easily recognized\nfrom either images (Shi et al., 2019a; Zhou et al.,\n2018a) or transcripts of the frames (Huang* et al.,\n2018) during a demonstrative and conversational\npresentation. While humans can impute such miss-\ning information by flexibly aggregating across var-\nious available modalities, key-object degeneracy\ncan cause critical failures in existing DVC systems.\nInput Modality Recognizability\nImage(X) 56.07\n+Transcript(X,T) 63.16\n+Instructional Script(X,T,R) 74.60\nTable 1: Statistics of the key objects in recognizable\nforms, recognizability.\nTo quantify the degeneracy in instructional\nvideos, we first conduct a study measuring the num-\nber of recognizable key objects from the images X\nand transcripts T in one of our target instructional\nvideo corpora, YouCook2 (Zhou et al., 2018a)1. We\ndefine recognizability as the percentage of key ob-\njects which are recognizable in at least one modal-\nity, and present the statistics in Table 1.\nFrom the result in Table 1, we can observe that\nmany key objects are not recognizable from the\nimage alone. Though we can observe that recog-\nnizability improves when the image is augmented\n1We provide detail of computing degeneracy in Sec. 7.2\n8554\nFigure 1: Overall illustration of our framework and a real-life example. The key object “Chicken” of procedure\ns3 is hard to recognize from the images and transcripts of Frame 3 and 4 of the instructional video VS (right top),\nwhich we call degeneracy. To repair degeneracy, we supervise the machine system to retrieve procedural sentences\n(left middle) aligned to each video frame utilizing key object aware inter-frame information (connected with yellow\nline), unless it fails to distinguish Frame 2 and 3, 4 and retrieve recipe sentence aligned to Frame 2 for Frame 3,4\n(connected with green line). We feed frame representation augmented with the retrieved procedural sentence to the\ndownstream task model, DVC, whose generated caption of s3 (left bottom) becomes more detailed and contains the\nkey object.\nwith the temporally paired transcript, this does not\nentirely resolve key-object degeneracy, as nearly\n40% of key objects remain unrecognized. For in-\nstance, in Figure 1, the key object of procedure s3,\nchicken, is not recognizable from either the image\nor transcript of Frame 3.\nHaving different reasons for degeneracy, each\nmodality has distinct methods to make key objects\nrecognizable: 1) reducing occlusion of key objects\nin images or 2) reducing ambiguity by mentioning\nthe key objects with nouns in text. Based on the\npreliminary study, we pursue the latter, and propose\na disambiguation method based on retrieval from\ninstructional scripts, such as recipes for cooking.\nThe sufficient condition of instructional scripts\nfor our method is that they contain disambiguated\nkey objects, and provide adequate coverage of\nvalid (key-object, action) pairs. For the YouCook2\ndataset, we quantitatively confirm the efficacy of\ninstructional scripts in repairing degeneracy, in Ta-\nble 1, where it is shown that the instructional scripts\ncan successfully make the unrecognized key ob-\njects recognizable. For example, in Figure 1, the\nunrecognizable key object in the third and fourth\nframes, chicken, becomes recognizable after the\nprocedural sentence r3 ∈RS (middle left of Fig-\nure 1) explicitly mentioning “chicken” is paired\nwith the image and transcript.\nWhile such well-aligned procedural sentences\ncan reduce key-object degeneracy, in most cases,\nthere exists no alignment supervision between the\nvideo frame and procedural sentences, as the two\nare generated independently. Our solution is to\ngenerate such alignment using a machine retriever.\nHowever, key-object degeneracy in the video frame\nnegatively affects the existing retrieval systems as\nwell, e.g., image-text retrieval, from retrieving the\naligned procedural sentence.\nInspired by the contextualized understanding of\nprevious/following frames (Qi et al., 2022), our\ndistinction is to guide the retriever to achieve key-\nobject-aware alignment with procedural sentences,\nby conducting retrieval based on aggregating inter-\nframe information in an object-centric manner. For\nthis goal, we propose Key Object aware Frame\nContrastive Learning (KOFCL) for improved dif-\nferentiation of nearby frames of distinctive proce-\ndures, and more robust contextualization of the key\nobject beyond a single procedure.\nOur major contributions are threefold: 1) pro-\n8555\npose a temporal description retrieval task to find\nthe procedural sentences procedurally aligned to\neach frame in instructional videos, 2) propose a\nkey object-aware frame contrastive learning ob-\njective (KOFCL) to improve temporal description\nretrieval, and 3) show the improved temporal de-\nscription retrieval repairs degeneracy and improves\nDVC significantly.\n2 Preliminaries and Related Work\nWe first introduce our target domain, namely, in-\nstruction, and its representations and previous re-\nsearch on their characteristics (§2.1). Our goal is\nto improve the encoding of video frame G (§2.2).\nThen we provide a concise overview of our down-\nstream task, DVC (§2.3).\n2.1 Target Domain: Instruction and Video,\nScript\nInstruction Instruction refers to structured\nknowledge explaining how to perform a wide va-\nriety of real-world tasks. An instruction Scan be\nrepresented as a list ofNprocedures, S = {sj}N\nj=1,\nwhere each procedure describes the action required\nfor the task, as a tuple of verb aj and key object set\nˆOj, sj = (aj,Oj). For example, the instruction\nfor cooking chicken parmesan would be a list com-\nposed of tuples such as (coat, [chicken, mixture])\nwhich is written in text or shown in the video for\nhuman consumption as depicted in Figure 1.\nInstructional Video Instructional video, denoted\nas VS, is a video explaining instruction S. It\nconsists of a list of frames, VS = {vj\ni |i ≤\n|VS|and j ≤N}. The procedure sj is represented\nin the key clip kj, the subset of video frames start-\ning at bj and ending at ej. Then, the i-th frame, vj\ni ,\nrepresents the corresponding procedure sj when it\nis included in the key clip kj or the null procedure\ns0 if it is not covered by any key clip. For exam-\nple, Frame 1 in Figure 1 explains its procedure by\nshowing and narrating its key objects in its image\nxj\ni and transcript tj\ni .\nIt is widely known that degeneracy is prevalent in\neach modality of instructional videos (Zhou et al.,\n2018a). Specifically, this indicates a large differ-\nence between the key object set Oj and the key\nobjects recognizable in the frame vj\ni , ˆOj\ni . There\nhave been previous works that discovered and ad-\ndressed the degeneracy in a single modality of im-\nage (Shi et al., 2019b) or transcript (Huang* et al.,\n2018). However, our approach aims to repair the\ndegeneracy in both modalities, by leveraging the\nprocedural sentences from instructional transcripts.\nInstructional Script An instructional script\nRS = {rj}N\nj=1 consists of procedural sentences\nwhere each procedural sentence rj represents its\ncorresponding procedure sj explicitly as words\ndescribing the action aj and the key objects Oj.\nRepresenting procedures in disambiguated form,\nprevious works construct instruction S from its\ncorresponding instructional script RS (Lau et al.,\n2009; Maeta et al., 2015; Kiddon et al., 2015). We\npropose to adopt RS to disambiguate the unrecog-\nnizable key object for mitigating degeneracy.\n2.2 Baseline: Representation gj\ni\nA baseline to overcome degeneracy is to encode the\ntemporally paired image and transcript(xj\ni ,tj\ni ) into\njoint multi-modal representation gj\ni . For such pur-\npose, we leverage pretrained LXMERT (Tan and\nBansal, 2019)2, as it is widely adopted to encode\nthe paired image transcript of video frame (Kim\net al., 2021; Zhang et al., 2021). Specifically, the\ntranscript tj\ni and image xj\ni of the video frame vj\ni\nare fed together to pretrained LXMERT. We utilize\nthe representation at the special [CLS] token as the\nframe representation gj\ni as follows:\ngj\ni = LXMERT (xj\ni ,tj\ni ). (1)\nWe use the resulting representation G = {gj\ni |i ≤\n|VS|and j ≤N}as features of individual frames\nthat will be fed to DVC systems.\n2.3 Target Task: DVC\nGiven an instructional video VS describing instruc-\ntion S, DVC consists of two subtasks of key clip\nextraction and caption generation.\nKey Clip Extraction Given a sequence of video\nframes, key clip extraction module predicts key clip\nˆk= (ˆb,ˆe) by regressing its starting/ending time ˆb\nand ˆe(Zhou et al., 2018a; Wang et al., 2021). It\nalso outputs the likelihood Pk(ˆk) estimating the\npredicted clip ˆk to be a key clip which is further\nused to select the key clips for caption generation.\nCaption Generaton The caption generation task\naims to generate caption ˆcdescribing the predicted\nkey clip ˆk. The predicted key clip ˆkis fed to the\n2We refer to a survey (Du et al., 2022) for overview of\nmulti-modal representation techniques, as our focus is not on\nenhancing multi-modal representation.\n8556\ncaptioning module which generates each word ˆwi\nby estimating the probability distribution over vo-\ncabulary set W conditioned on key clip ˆk:\nˆwi = argmaxw∈W P(w|w≤i−1,ˆk). (2)\nWe adopt EMT and PDVC, DVC systems which\nare widely adopted or SOTA, as our DVC systems.\nWe refer (Zhou et al., 2018b; Wang et al., 2021)\nfor further details, as our focus is not on improv-\ning downstream task models, but on repairing the\ndegeneracy of input instructional videos, which is\napplicable to any underlying models.\n3 Our Approach\nBuilding on preliminaries, we now describe our\nretrieval augmented encoding framework in detail.\nFirst, we explain how instructional scripts can\ncontribute to repairing the degeneracy (§3.1). Our\nframework combines a cross-modal TDR module\n(§3.2), which can aggregate the key objects across\nframes (§3.3), to build robust multi-modal repre-\nsentations which repair key-object degeneracy.\n3.1 Representation Augmentation with\nProcedural Sentence\nOur hypothesis to mitigate degeneracy is that a pro-\ncedural sentence rj\ni in RS represent a procedure ˜sj\ni\nsimilar to the procedure sj of each frame vj\ni . Ex-\nplaining a similar procedure, the key object set ˜Oj\ni\nof rj\ni has common key objects sufficient to repair\ndegeneracy. Our first distinction is to augment the\nindividual frame representation gj\ni with the repre-\nsentation dj\ni of such procedural sentence rj\ni . Thus,\nwhen procedural sentence rj\ni is provided with video\nframe vj\ni , more key objects become recognizable,\nn(Oj\ni ∩Oj) ≤n((Oj\ni ∪˜Oj\ni ) ∩Oj), (3)\nand the degeneracy in video frames can be reduced.\n3.2 Temporal Description Retrieval (TDR)\nCross-modal Retrieval for Aligning Sentences\nwith Frames The preliminary study in Sec. 3.1\nestablishes the potential of procedural sentences\nto repair key-object degeneracy. However, it as-\nsumes the ideal scenario where the procedure de-\nscribed by the procedural sentence rj, matches\nthat of the frame vj\ni , which we call procedural\nalignment. However, such procedural alignment\nbetween procedural sentences and frames is not\navailable in practice, as data of the two modalities\nare generated completely independently.\nWe, therefore, propose a cross-modal retrieval\ntask, Temporal Description Retrieval (TDR), as a\nsolution to learn such procedural alignments. We\ntrain a frame-sentence retriever, ϕ(vj\ni ,RS) to take\nthe query frame vj\ni from video VS, and the instruc-\ntional script RS as input, and predict, for every\nprocedural sentence ri ∈RS, their relevance.The\ngoal of ϕis to find the procedural sentenceˆri which\nbest explains the procedure sj.\nHere, it is important to note that the retrieval task\nitself is also susceptible to key-object degeneracy,\nmaking TDR more challenging. In the presence of\nkey-object degeneracy, single-modality (image or\ntext) encodings can exacerbate this problem, due\nto a potential information imbalance between the\ntwo modalities. Therefore, we formulate the cross-\nmodal TDR as retrieving text encodings using a\njoint image-text query, using the LXMERT joint\nimage-text representation, gj\ni .\nFinally, we augment the feature vector gj\ni of the\nframe with vector representation dj\ni of the retrieved\nprocedural sentence ˆri as depicted in Figure 1.\nDense Retrieval for Efficiency There can be sev-\neral options to implement the frame-sentence re-\ntriever ϕ(vj\ni ,RS). Existing architectures fall into\ntwo categories, cross retrievers and dense retriev-\ners (Humeau et al., 2020). These differ in how\nthe interaction between the query frame vj\ni and the\nprocedural sentence rl is modeled.\nAs TDR conducts retrieval for each frame in\nVS, efficiency should be prioritized, and we mainly\nconsider the dense retrieval architecture. First archi-\ntecture, the cross retrieval requires the exhaustive\ncomputation of O(|VS|×| RS|) as the vj\ni and rl\ninteract within a single neural network. However,\nthe dense retrieval conducts the retrieval with little\ncomputation cost, at O(|VS|+ |RS|), by reusing\nthe encoding of the vj\ni and rl.\nSpecifically, the dense retriever consists of two\ndistinct encoders ΩV and ΩR, which encode the\nquery frame vj\ni and the procedural sentence rl in-\ndependently. Then, the interaction between vj\ni and\nrl is modeled as a simple dot product operation,\nresulting in retrieval as follows:\nˆri = argmaxrlΩV (vj\ni ) ·ΩR(rl). (4)\nFor training, we adopt the contrastive learning\nobjective (Mnih and Kavukcuoglu, 2013), denoted\nby LTDR, that guides the retriever to assign larger\nrelevance for the gold procedural sentence r+ than\n8557\nthat of negative procedural sentences r−:\nLTDR = −log exp(ΩV (vj\ni )·ΩR(r+))\nexp(ΩV (vj\ni )·ΩR(r+))+∑exp(ΩV (vj\ni )·ΩR(r−)),\n(5)\nWe utilize the caption cj as the gold procedural\nsentence r+, as there is no available gold proce-\ndural sentence, and this approach is reported to be\neffective in previous work (Gur et al., 2021). We\nalso utilize in-batch negatives, treating all other\ngold procedural sentences representing different\nprocedures from the identical instructional video,\nas negative procedural sentences.\n3.3 Key Object-aware Frame Contrastive\nLearning (KOFCL)\nThe key aspect separating instructional videos from\nstandard image-text or textual retrieval is the addi-\ntional temporal dimension. In order to repair key-\nobject degeneracy, it is critical to aggregate inter-\nframe information across this temporal dimension.\nTo illustrate, consider the key object of frames 3\nand 4 in Figure 1, “chicken”, which is not recog-\nnizable from either the transcript or the images of\nFrame 3 and 4, but is clearly recognizable in both\nimage v1\n1 and transcript t1\n1 of Frame 1.\nWe adopt LSTM as a sequence encoder simi-\nlar to existing video works (Zhou et al., 2018a)\nand build LXMERT- I2 which encodes prece-\ndent/following frames, g≤j\n≺i and g≥j\n≻i , and outputs\nthe resulting query frame encoding ← →g j\ni as follows:\n← →g j\ni = FCN(←−−−→LSTM(gj\ni ,g≤j\n≺i ,g≥j\n≻i )). (6)\nHowever, the locality of the frame-level pro-\ncedure annotations biases such model to sim-\nply encode temporally local inter-frame informa-\ntion (Wang et al., 2020), not the key objects. Specif-\nically, the procedures are represented as tempo-\nrally local frames and such local frames of identi-\ncal procedures can contribute to repair degeneracy.\nHowever, as all local frames are not of identical\nprocedures, e.g. boundaries of the key clips, en-\ncoding such frames cannot repair degeneracy and\nrather confuse the models to consider as the preced-\ning/following procedures. For Frame 3 in Figure 1,\ntemporally local inter-frame information of Frame\n2 and 3 is redundant with the given frame, adding\nlittle new information. Even worse, confusing that\nFrame 2 and 3 describe the identical procedure, the\nmodel misaligns Frame 3 to the procedural sen-\ntence r2 of the different procedure. On the other\nhand, identifying the key object which appears in\nFrame 1, and binding this information into the en-\ncoding for Frame 3, would successfully repair the\nkey-object degeneracy of Frame 3.\nA recent approach, frame contrastive learning\n(FCL) (Dave et al., 2022), partially addresses the\ntemporal locality bias. It regards the arbitrary frame\npair (vj\ni ,vm\nn ) as positive when they represent iden-\ntical procedure and negative otherwise as follows:\n1(vj\ni ,vm\nn ) =\n{\n1, ifj = m\n0. otherwise (7)\nWhat makes FCL address the temporal locality bias\nis that it supervises the difference in the procedures\nbetween the local frames so that local frames of\ndifferent procedures, such as Frame 2 for given\nFrame 3 in Figure 1, can be less aggregated.\nThen, the frame encoder is supervised to map\nthe frames of identical procedures close together in\nthe representation space, while pushing away those\nof different procedures by FCL loss, Laux(vj\ni ,vm\nn ),\ndefined as follows:\nyin = σ(← →g j\ni ·Waux ·← →g m\nn ) (8)\nLaux(vj\ni ,vk\nl ) = BCE(1(vj\ni ,vm\nn ),yin), (9)\nwhere σis sigmoid function andWaux is parameter\nof bilinear layer. Finally, the retriever is optimized\nto simultaneously minimize LTDR and Laux:\nL= LTDR + λauxLaux, (10)\nwhere λaux is a hyper-parameter weighing contri-\nbution of Laux during training.\nHowever, FCL is limited to contextualizing local\nframes of identical procedure as the inter-frame\ninformation. To extend such contextualization be-\nyond the single procedure, we propose key object-\naware frame contrastive learning (KOFCL), which\nencourages contextualizing the frames of different\nprocedures when they share common key objects,\nbased on a globally shared notion of key objects.\nThe clear advantage of such contextualization is\nthat it enables retrieving the correctly aligned pro-\ncedural sentence, even when key objects are hardly\nrecognizable in the query frame, by leveraging key-\nobject information. For example, the missing key\nobject “chicken” of Frames 3 and 4 in Figure 1 can\nbe found in Frame 1 of procedures1, where Frames\n1, 3, and 4 will be encouraged to share similar rep-\nresentations through KOFCL. More concretely, we\nlabel the frame pair vj\ni and vm\nn as positive when\nthey have common key objects. To measure how\n8558\nmany key objects a frame pair shares, we com-\nputed the intersection of union (IoU) between the\nkey object set of frame pair3 as follows:\nIoUobj(vj\ni ,vm\nn ) = n(Oj ∩Om)\nn(Oj ∪Om). (11)\nUsing IoUobj(vj\ni ,vm\nn ), we labeled the frame pair,\nvj\ni and vm\nm, when they share key objects over pre-\ndefined threshold µas follows:\n1obj(vj\ni ,vm\nn ) =\n{\n1, if IoUobj(vj\ni ,vm\nn ) >µ\n0. otherwise\n(12)\nConverting the FCL label in Eq.(7) into our pro-\nposed label in Eq.(12), KOFCL supervises to map\nframe pair, vj\ni and vm\nn , close when they not only\ndescribe the identical procedure but also share key\nobjects. Thus, the retriever can build a more robust\nunderstanding of the key objects in the query frame\nvj\ni with key object aware inter-frame information.\n4 Experimental Setup\n4.1 Dataset\nWe used two distinct instructional video datasets,\nYouCook2 (Zhou et al., 2018a), a dataset of in-\nstructional cooking videos and IVD (Alayrac et al.,\n2017), a dataset of instructional videos with 5\ndistinct goals such as CPR, jump the car. As\neach video provides its goals, we collected the\ninstructional scripts by querying its goal to the\nweb archive4 for YouCook2 following previous\nwork (Kiddon et al., 2015) and the Google search\nengine for IVD dataset. Our instructional script\ncollection contains an average of 15.33 scripts with\n10.15 sentences for each goal in YouCook2 and 1\ninstructional script with an average of 7.4 sentences\nfor each goal in IVD dataset. We used transcripts\ngenerated by YouTube ASR engine following pre-\nvious works (Xu et al., 2020; Shi et al., 2019a,\n2020).5\n4.2 Evaluation Settings\nTDR We evaluated TDR in two distinctive set-\ntings to utilize both gold captions and our collected\ninstructional scripts. First, we report the recall\nmetric (R@K) of the gold captions, where all the\n3Human-annotated key object is limited to subset of videos.\nTherefore, we applied pos-tagging on the groud-truth caption\nand filtered out the nouns and proper nouns.\n4www.allrecipes.com\n5We provide further details of our datasets in Appendix 7.4\ncaptions in the same video are considered candi-\ndates for retrieval. Second, we evaluated TDR\nperformance on our collected instructional scripts\nusing NDCGROUEGE −L metric (Messina et al.,\n2021a,b). It replaces the relevance annotation be-\ntween the query frame and procedural sentences\nwith lexical similarity score, ROUGE-L, between\ngold captions and procedural sentences. We report\neach metric on top-1/3/5 retrieval result. Especially,\nfor recall metrics, we mainly considered the top-1\nretrieval result as our priority is to address key ob-\nject degeneracy. Specifically, retrieving sentences\nof different procedures containing the same key\nobjects may result in a slightly lower R@3,5.\nDVC For the caption generation of DVC, follow-\ning convention (Krishna et al., 2017; Zhou et al.,\n2018b), we report lexical similarity of generated\ncaptions with gold captions, using BLEU@4 (Pap-\nineni et al., 2002), METEOR (Banerjee and Lavie,\n2005), CIDEr (Vedantam et al., 2015), and Rouge-\nL (Lin, 2004), abbreviated as B-4, M, C, and R. For\nthe key clip extraction, we report the average recall\nof the predicted key clips denoted as AR follow-\ning convention (Escorcia et al., 2016; Zhou et al.,\n2018b). For every metric, we provide the average\nand standard deviation of 5 repetitive experiments.\n5 Results\nWe now present our experimental results, aiming to\naddress each of the following research questions:\nRQ1: Is our cross-modal retrieval using joint\nimage-text query more effective than standard re-\ntrieval approaches for TDR?\nRQ2: Does KOFCL address key-object degeneracy\nin TDR, and help the retriever to build a robust\nunderstanding of key objects?\nRQ3: Does retrieval-augmentation using procedu-\nral sentences improve DVC by repairing key-object\ndegeneracy?\n5.1 RQ1: Effectiveness of joint image-text\nquery formulation for TDR\nQuery Encoder Input R@1 R@3 R@5\nBM25 tj\ni 35.02 59.34 74.88\nBERT tj\ni 41.45 72.4 86.95\nTERAN xj\ni 39.73 72.39 86.75\nNAAF xj\ni 39.37 72.89 88.17\nLXMERT xj\ni,tj\ni 47.30 78.50 91.14\nLXMERT (NAIVE DISAMB.) xj\ni,τj\ni 44.75 77.31 90.42\nLXMERT-I2+KOFCL (Ours)xj\ni,tj\ni 56.83 84.49 94.45\nTable 2: Recall (R@1,3,5) for Youcook2 Retrieval with\ndifferent query frame modality\n8559\nDataset YouCook2 IVD\nMetric NDCG R@K R@K\nQuery EncoderK=1 K=3 K=5 K=1 K=3 K=5 K=1 K=3 K=5\nLXMERT 39.56 41.93 43.5047.30 78.50 91.1430.83 62.10 78.54\nLXMERT-I2 41.90 44.21 45.9955.24 85.86 95.0940.35 77.77 89.83\n+FCL 42.01 44.25 45.8255.88 85.55 94.8940.51 74.46 87.15\n+KOFCL (OURS) 42.73 44.92 46.5056.83 84.49 94.4543.42 76.58 87.86\nTable 3: Temporal description retrieval results ablated on inter-frame information\nTo verify the effectiveness of our joint image-\ntranscript query formulation for TDR, we compare\nour approach with baselines consisting of existing\ntextual and image-text retrieval systems as follows:\n• BM25 (Robertson, 2009) and BERT (Devlin\net al., 2019) are widely used approaches in\ntext retrieval. We adopt them as a baseline\nusing the transcript as a query.\n• TERAN (Messina et al., 2021a) and\nNAAF (Zhang et al., 2022) are the state-of-\nthe-art image-text retrievers. We adopt them\nas baselines using the image xj\ni as a query.\nTable 2 shows TDR result of the baselines and\nour joint image-text query formulation LXMERT\nfor the YouCook2 dataset. We can observe that\nbaselines using single modality queries, i.e. BM25\nor TERAN , are insufficient for finding the aligned\nprocedural sentence, with R@1 score lower than\n40%. LXMERT shows higher TDR results with\nlarge margins over baselines in every metric, con-\nfirming the effectiveness of our proposed joint\nimage-transcript query. For comparison, we also\ninclude the TDR result of our full model, which\nfurther improves significantly over LXMERT.\nAdditionally, we compare a straightforward\nmethod to repair degeneracy, by disambiguat-\ning pronouns in transcripts. Following previous\nwork (Huang* et al., 2018), we use a co-reference\nmodule (Gardner et al., 2017) to convert transcripts\ninto their disambiguated versions, τj\ni . Interestingly,\nwe observe a degradation of TDR in every metric.\nWe hypothesize that the co-reference resolution\nintroduces noise from several sources, including\nthe module’s inaccuracy itself, but also incorrect\npronoun resolution using key objects belonging to\nother, adjacent procedures.\n5.2 RQ2: KOFCL contextualize key objects\nand improves TDR.\nNext, we evaluate the effectiveness of inter-frame\ninformation, in conjunction with KOFCL, in im-\nproving the performance of TDR. In Table 3, we re-\nport the respective results of TDR on the YouCook2\nand IVD datasets, with varying inter-frame infor-\nmation supervision approaches.\nFirst, on both datasets, we observe a large im-\nprovement of LXMERT- I2 over LXMERT , re-\nflecting the importance of inter-frame information\nfor TDR. Next, we focus on the effect of jointly\nsupervising LXMERT- I2 with FCL or KOFCL.\nWhen LXMERT- I2 supervised by FCL, the in-\ncrease in R@1 is negligible. In contrast, when\nsupervised with our proposed KOFCL, we can ob-\nserve a meaningful improvement in R@1, on both\ndatasets. These results indicate that KOFCL im-\nproves TDR by capturing key-object aware inter-\nframe information in a generalizable manner.\nQuery Encoder R@1\nLXMERT-I2 55.04\n+FCL 55.16\n+KOFCL (OURS) 56.99\nTable 4: Recall@1 score on the isolated set.\nIn order to further verify that KOFCL contextual-\nizes key objects and repairs key-object degeneracy,\nwe collect an isolated subset of YouCook2, where\nnearby frames are prone to confuse frame-sentence\nretrievers with a temporal locality bias. Specifi-\ncally, we collect the query frames vj\ni whose cor-\nresponding procedure sj has distinct6 key objects\nfrom neighboring procedures sj−1 and sj+1.\nWe report the R@1 score on this isolated set\nin Table 4. Whereas FCL fails to improve over\nLXMERT- I2, R@1 improves meaningfully when\nthe frame-sentence retriever is supervised with\nour proposed KOFCL. These results indicate that\nKOFCL contributes to the contextualization of key\nobjects, and alleviates the temporal locality bias.\n5.3 RQ3: Retrieved procedural sentences\nrepair degeneracy and improve DVC\nNext, we evaluate the impact of repairing degener-\nacy on improving downstream task of dense video\n6We considered procedure sj to have distinct key objects\nwith neighboring procedures when their IoUobj defined in\nEq.(12) is lower than 0.05\n8560\nDVC Model EMT PDVC\nRepresentation Captioning KCE Captioning KCE\nM C R B-4 AR M C R B-4 AR\ngji 7.140.20 18.201.09 20.130.52 0.800.09 65.912.95 6.210.42 28.762.61 14.460.75 1.180.16 17.171.09\ngji;dji w/τji 8.030.25 21.680.61 21.950.80 1.000.08 66.552.99 6.800.44 31.222.10 15.580.94 1.290.14 19.061.27\ngji;dji w/ LXMERT-I2+ KOFCL8.370.25 24.370.67 22.950.44 1.400.17 68.931.72 7.170.15 33.860.78 16.550.45 1.320.13 20.160.83\nTable 5: BLEU-4, METEOR, CIDEr, Rouge-L for captioning, Average Recall (AR) for Key Clip Extraction (KCE).\ncaptioning, which is the main objective of this work.\nWe evaluate our proposed approach, which uses a\ntrained retriever to retrieve procedural sentences\nfrom instructional scripts to augment frame rep-\nresentations, with a baseline without any consid-\neration of key-object degeneracy, as well as an\nadvanced baseline, which augments frame repre-\nsentations using the disambiguated version of the\ntranscript τj\ni , instead of procedural sentences.\nWe first report the DVC performance on\nYouCook2 in Table 5. The advanced baseline,\nwhich augments the baseline representation gj\ni\nwith dj\ni using τj\ni , improves performance on both\ncaptioning and key clip extraction, showing that\nDVC can be improved by augmenting frame rep-\nresentations with disambiguated key-object infor-\nmation. Notably, our proposed framework, which\naugments using procedural sentences retrieved us-\ning the LXMERT- I2 + KOFCL retriever, signifi-\ncantly outperforms both baselines, on all metrics\nmeasured, for both tasks. These results indicate\nthat by repairing key-object degeneracy, our re-\ntrieved procedural sentences are a better source to\naugment frame representations for DVC. Moreover,\nour augmented representations improve results on\nboth EMT and PDVC downstream models, which\nconfirms that our method can be easily applied to\nimprove standard DVC systems, without dramatic\nmodification of the downstream task models.\nRepresentation Captioning KCEM C R B-4 ARgji 7.140.20 18.201.09 20.130.52 0.800.09 65.912.95\ngji;djiw/τji 8.030.25 21.680.61 21.950.80 1.000.08 66.552.99\ngji;djiw/ LXMERT 7.690.21 20.400.69 21.910.49 1.120.15 66.851.08\ngji;djiw/ LXMERT-I2 7.970.33 21.801.21 22.340.50 1.200.15 67.670.25\ngji;djiw/ LXMERT-I2+ KOFCL8.370.25 24.370.67 22.950.44 1.400.17 68.931.72\nTable 6: BLEU-4, METEOR, CIDEr, Rouge-L for cap-\ntioning, Average Recall (AR) for Key Clip Extraction\n(KCE).\nRepresentation Captioning KCEM C R AR\ngji 9.20.73 61.693.73 14.880.61 36.072.08\ngji;dji w/ LXMERT-I2 16.011.26 102.656.84 24.521.13 27.830.97\ngji;dji w/ LXMERT-I2+ KOFCL19.760.85 123.694.88 29.790.96 37.971.61\nTable 7: Dense video captioning results on IVD dataset.\nMETEOR, CIDEr, Rouge-L for captioning, Average\nRecall (AR) for Key Clip Extraction (KCE).\nNext, we conduct an ablation study of the contri-\nbution of each of our framework components. In\nTables 6 and 7, we report the results of DVC on\nYouCook2 and IVD respectively, using the EMT\nmodel with various frame-sentence retrievers. The\nresults confirm that the improvement in the retrieval\noutcomes translates to better downstream perfor-\nmance on DVC, with LXMERT- I2 and KOFCL\nmeaningfully improving DVC performance on both\ndatasets. Also, our proposed retrieval augmen-\ntation method showed more improvement in the\nIVD dataset than YouCook2. The key difference\nbetween the Youcook2 and IVD datasets is that\nthe IVD dataset is composed of more distinctive\ninstructions, such as “jump the car”, “re-pot the\nplant” and “make coffee”, than YouCook2, which\ncontains only cooking instructions. For such dis-\ntinctive instructions, knowing the key objects can\nact as clarifying information about the instruction\nand thus can help generate more accurate captions.\nRepresentation Definite Degenerative\ngj\ni 16.38 13.61\ngj\ni;dj\ni w/ LXMERT-I2 + KOFCL 15.33 17.15\nTable 8: CIDEr scores results on definite/degenerative\nsets.\nFinally, to verify that the improvement in DVC\nperformance is attributable to the repair key-object\ndegeneracy, we divided the test set into definite\nand degenerative sets and compared the results\nof baseline representation gj\ni and our augmented\nrepresentation gj\ni ; dj\ni w/ LXMERT- I2 + KOFCL.\nSpecifically, the caption cj is considered degener-\native when the video frames corresponding to the\nground-truth key clip kj have lower than 60% rec-\nognizability of image and transcript, and definite\nwhen the recognizability is higher than 80%. In\nTable 8, in contrast to representation gj\ni , whose\nCIDEr score decreases on the degenerative set, our\naugmented representation gj\ni ; dj\ni w/ LXMERT- I2 +\nKOFCL increases the score on the degenerative set,\nshowing that our augmented representation using\nretrieved procedural sentences is effective in re-\n8561\nsolving the key-object degeneracy in instructional\nvideos.\n6 Conclusion\nWe proposed retrieval-augmented encoding, to\ncomplement video frames, by repairing degener-\nacy and considering correlations between steps.\nOur evaluation results validated that our proposed\nframework improves existing DVC systems signifi-\ncantly.\nLimitations\nOur method overcomes degeneracy in instructional\nvideos under the assumption of the existence of\ntextual instructional scripts describing the exact\ninstructions of instructional videos. Thus, our\nmethod is applicable to instructional videos hav-\ning such recipe documents. However, we note that\nsimilar documents exist for various types of instruc-\ntions other than cooking, such as topics in other\ndatasets (Alayrac et al., 2017), e.g., how to jump\nstart a car, or change a tire.\nAcknowledgements\nThis research was supported by MSIT (Ministry of\nScience and ICT), Korea, under the ITRC (Informa-\ntion Technology Research Center) support program\n(IITP-2023-2020-0-01789) and grants [NO.2021-\n0-0268, AI Hub, SNU], [No.2022-0-00077, AI\nTechnology Development for Commonsense Ex-\ntraction, Reasoning, and Inference from Heteroge-\nneous Data], and [NO.2021-0-01343, AI Graduate\nSchool] supervised by the IITP (Institute for Infor-\nmation & Communications Technology Planning\n& Evaluation).\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yev-\ngen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol\nHausman, Alex Herzog, Daniel Ho, Jasmine Hsu,\nJulian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,\nRosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-\nmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalash-\nnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey\nLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-\nter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\nRettinghouse, Diego Reyes, Pierre Sermanet, Nico-\nlas Sievers, Clayton Tan, Alexander Toshev, Vincent\nVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do as i can,\nnot as i say: Grounding language in robotic affor-\ndances.\nJean-Baptiste Alayrac, Josef Sivic, Ivan Laptev, and\nSimon Lacoste-Julien. 2017. Joint discovery of ob-\nject states and manipulation actions. In International\nConference on Computer Vision (ICCV).\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\n2018. Bottom-up and top-down attention for image\ncaptioning and visual question answering.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved cor-\nrelation with human judgments. In Proceedings of\nthe acl workshop on intrinsic and extrinsic evaluation\nmeasures for machine translation and/or summariza-\ntion, pages 65–72.\nIshan Dave, Rohit Gupta, Mamshad Nayeem Rizve, and\nMubarak Shah. 2022. Tclr: Temporal contrastive\nlearning for video representation. Computer Vision\nand Image Understanding, 219:103406.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao.\n2022. A survey of vision-language pre-trained mod-\nels.\nVictor Escorcia, Fabian Caba Heilbron, Juan Carlos\nNiebles, and Bernard Ghanem. 2016. Daps: Deep\naction proposals for action understanding. In Com-\nputer Vision – ECCV 2016, pages 768–784, Cham.\nSpringer International Publishing.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\nPeters, Michael Schmitz, and Luke S. Zettlemoyer.\n2017. Allennlp: A deep semantic natural language\nprocessing platform.\nShir Gur, Natalia Neverova, Chris Stauffer, Ser-Nam\nLim, Douwe Kiela, and Austin Reiter. 2021. Cross-\nmodal retrieval augmentation for multi-modal classi-\nfication.\nDe-An Huang*, Shyamal Buch*, Lucio Dery, Animesh\nGarg, Li Fei-Fei, and Juan Carlos Niebles. 2018.\nFinding “it”: Weakly-supervised, reference-aware\nvisual grounding in instructional videos. In IEEE\nConference on Computer Vision and Pattern Recog-\nnition (CVPR).\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architec-\ntures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\n8562\nChloé Kiddon, Ganesa Thandavam Ponnuraj, Luke\nZettlemoyer, and Yejin Choi. 2015. Mise en place:\nUnsupervised interpretation of instructional recipes.\nIn EMNLP.\nKyungho Kim, Kyungjae Lee, and Seung won Hwang.\n2021. Instructional video summarization using atten-\ntive knowledge grounding. In Machine Learning and\nKnowledge Discovery in Databases. Applied Data\nScience and Demo Track - European Conference,\nECML PKDD 2020, Proceedings , pages 565–569,\nGermany.\nRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,\nand Juan Carlos Niebles. 2017. Dense-captioning\nevents in videos. In International Conference on\nComputer Vision (ICCV).\nTessa Lau, Clemens Drews, and Jeffrey Nichols. 2009.\nInterpreting written how-to instructions. In Proceed-\nings of the 21st International Joint Conference on\nArtificial Intelligence, IJCAI’09, page 1433–1438,\nSan Francisco, CA, USA. Morgan Kaufmann Pub-\nlishers Inc.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nHirokuni Maeta, Tetsuro Sasada, and Shinsuke Mori.\n2015. A framework for procedural text understand-\ning. In Proceedings of the 14th International Confer-\nence on Parsing Technologies, pages 50–60, Bilbao,\nSpain. Association for Computational Linguistics.\nNicola Messina, Giuseppe Amato, Andrea Esuli,\nFabrizio Falchi, Claudio Gennaro, and Stéphane\nMarchand-Maillet. 2021a. Fine-grained visual tex-\ntual alignment for cross-modal retrieval using trans-\nformer encoders. ACM Trans. Multimedia Comput.\nCommun. Appl., 17(4).\nNicola Messina, Fabrizio Falchi, Andrea Esuli, and\nGiuseppe Amato. 2021b. Transformer reasoning net-\nwork for image- text matching and retrieval. In 2020\n25th International Conference on Pattern Recogni-\ntion (ICPR), pages 5222–5229.\nAndriy Mnih and Koray Kavukcuoglu. 2013. Learning\nword embeddings efficiently with noise-contrastive\nestimation. In Advances in neural information pro-\ncessing systems, pages 2265–2273.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nJiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu\nLiu, Xiang Bai, Serge Belongie, Alan Yuille, Philip\nTorr, and Song Bai. 2022. Occluded video instance\nsegmentation: A benchmark. International Journal\nof Computer Vision.\nS. Robertson. 2009. The Probabilistic Relevance Frame-\nwork: BM25 and Beyond. Foundations and Trends®\nin Information Retrieval, 3(4):333–389.\nZhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong\nChen, Yu-Gang Jiang, and Xiangyang Xue. 2017.\nWeakly supervised dense video captioning. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nBotian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen,\nZhendong Niu, and Ming Zhou. 2019a. Dense proce-\ndure captioning in narrated instructional videos. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6382–\n6391, Florence, Italy. Association for Computational\nLinguistics.\nBotian Shi, Lei Ji, Zhendong Niu, Nan Duan, Ming\nZhou, and Xilin Chen. 2020. Learning Semantic Con-\ncepts and Temporal Alignment for Narrated Video\nProcedural Captioning, page 4355–4363. Associa-\ntion for Computing Machinery, New York, NY , USA.\nJing Shi, Jia Xu, Boqing Gong, and Chenliang Xu.\n2019b. Not all frames are equal: Weakly-supervised\nvideo grounding with contextual similarity and visual\nclustering losses. In 2019 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR),\npages 10436–10444.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566–4575.\nTeng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng,\nRan Cheng, and Ping Luo. 2021. End-to-end dense\nvideo captioning with parallel decoding.\nZhenzhi Wang, Ziteng Gao, Limin Wang, Zhifeng Li,\nand Gangshan Wu. 2020. Boundary-aware cascade\nnetworks for temporal action segmentation. In ECCV\n(25), volume 12370 of Lecture Notes in Computer\nScience, pages 34–51. Springer.\nFrank F. Xu, Lei Ji, Botian Shi, Junyi Du, Graham Neu-\nbig, Yonatan Bisk, and Nan Duan. 2020. A bench-\nmark for structured procedural knowledge extraction\nfrom cooking videos.\nKun Zhang, Zhendong Mao, Quan Wang, and Yongdong\nZhang. 2022. Negative-aware attention framework\nfor image-text matching. In 2022 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 15640–15649.\nYanhao Zhang, Qiang Wang, Pan Pan, Yun Zheng,\nCheng Da, Siyang Sun, and Yinghui Xu. 2021. Fash-\nion focus: Multi-modal retrieval system for video\ncommodity localization in e-commerce. Proceedings\n8563\nof the AAAI Conference on Artificial Intelligence ,\n35(18):16127–16128.\nLuowei Zhou, Chenliang Xu, and Jason J Corso. 2018a.\nTowards automatic learning of procedures from web\ninstructional videos. In AAAI Conference on Artifi-\ncial Intelligence, pages 7590–7598.\nLuowei Zhou, Yingbo Zhou, Jason J. Corso, Richard\nSocher, and Caiming Xiong. 2018b. End-to-end\ndense video captioning with masked transformer.\nCoRR, abs/1804.00819.\n7 Appendix\n7.1 Implementation Details\n7.1.1 Temporal Description Retrieval\nFor temporal description retrieval, we followed the\nconvention of (Krishna et al., 2017; Zhou et al.,\n2018b; Shi et al., 2019a) and obtained the image\nframes from the video by down-sampling for every\n4.5s. The obtained image frames are then fed to\npre-trained object detector (Anderson et al., 2018)\nto yield the sequence of object region features. For\nimage encoder Ωv and the text encoder Ωr, we\nused the image encoder of pretrained LXMERT\nand BERT-base-uncased (Devlin et al., 2019), re-\nspectively. For training temporal description re-\ntrieval, we used one video as a batch, resulting in\nall the sampled frames and recipe sentences in a\nbatch coming from the same video. We adopt an\nAdam optimizer with a learning rate of 0.0001. We\nset the weighing contribution λaux in Eq. 10 to\nbe 0.05 and the threshold µfor KOFCL to be 0.1,\nbased on validation set result.\n7.2 Computation of Recognizability\nTo compute the joint recognizability of the image\nand transcript, instructional script, we first com-\nputed the recognizability in each modality. In the\nimage, we considered the key objects to be recog-\nnizable when they are labeled to be inside the image\nwithout occlusion in human annotation (Shen et al.,\n2017). In the textual modality, transcript and in-\nstructional script, the key objects are considered to\nbe recognizable when they are lexically referred in\ntranscripts or instructional scripts. Then, we con-\nsidered the key objects to be recognizable when\nthey are in the union of the recognizable key object\nset of each modality.\n7.3 Ablation on Sequence Encoder\nHere, we show the result of TDR with distinct\nsequence encoders. In Table 9, LSTM showed the\nSequence Encoder R@1\nCNN 50.65\nTRANSFORMER 43.69\nLSTM (O URS ) 55.04\nTable 9: Recall@1 score with different sequence en-\ncoder.\nhighest R@1 score. While we adopted LSTM as\nour sequence encoder, our KOFCL is orthogonal\nto any sequence encoder and can be adapted to any\nexisting sequence encoder.\n7.3.1 Dense Video Captioning\nEMT For the key clip extraction task, we follow\nthe convention of (Zhou et al., 2018b) to use 16\ndifferent kernel sizes for the temporal convolution\nlayer, i.e., from 3 to 123 with the interval step of\n8, which can cover the different lengths. We use\na transformer encoder and decoder with 768 inner\nhidden sizes, 8 heads, and 2 layers which we fed\ncontext-aware recipe sentences and video frame\nfeatures after concatenation. We adopt an AdamW\noptimizer with learning rate of 0.00001 to train the\nmodel. The batch size of training is 12 and we use\none RTX2080Ti GPU to train our model.\nPDVC We use single transformer models with\n768 inner hidden sizes, 12 heads, and 2 layers\nwhich we fed context-aware recipe sentences and\nvideo frame features after concatenation. We adopt\nan AdamW optimizer with learning rate of 0.00005\nto train the model. The batch size of training is\n1 and we use one RTX2080Ti GPU to train our\nmodel.\n7.4 Dataset\nWe conducted experiments on the two distinct in-\nstructional video datasets, YouCook2 (Zhou et al.,\n2018a), a dataset of instructional cooking videos\nand IVD dataset (Alayrac et al., 2017), a dataset of\ninstructional videos with 5 distinct topics.\nThough YouCook2 originally provides 2000\nvideos, as some videos are unavailable on YouTube,\nwe collect the currently available videos, obtain-\ning 1,356 videos. For the dataset split, we follow\nthe original split ratio from (Zhou et al., 2018a)\nto YouCook2: 910 for training, 312 for validation,\nand 135 for testing for YouCook2. For the IVD\ndataset, we used 104 for training, 17 for validation,\nand 32 for testing.\nThis split is used for both TDR and DVC. Each\n8564\nvideo is labeled with starting and ending times\nof key clips, and their textual descriptions. For\ntranscripts, we use YouTube’s ASR engine. We\ncollected the instructional documents from the\nweb archive7 for YouCook2 following previous\nwork (Kiddon et al., 2015) and top-1 retrieved re-\nsult from the google search engine for IVD dataset.\nOur instructional document collection contains an\naverage of 15.33 documents with 10.15 sentences\nfor YouCook2 dataset and 1 instructional document\nwith 20 sentences for IVD dataset.\n7.5 Qualitative Results\nHere, we provide the generated result of EMT with-\nout/with our retrieved recipes in Figure 2. In all\nexamples, there exist the key objects hardly recog-\nnizable from the images which EMT fail to men-\ntion in the generated caption. However, our re-\ntrieved recipes provide the disambiguated reference\nof such key objects and enable EMT to generate\nmore accurate caption containing them.\n7www.allrecipes.com\n8565\nFigure 2: Example of the retrieved procedural sentence and generated captions without/with retrieved procedural\nsentence. Top 2 figures are from YouCook2 dataset and bottom figure is from IVD dataset.\n8566\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□ A1. Did you describe the limitations of your work?\nLeft blank.\n□ A2. Did you discuss any potential risks of your work?\nLeft blank.\n□ A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□ A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □ Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n8567\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □ Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n8568",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9204724431037903
    },
    {
      "name": "Computer science",
      "score": 0.8118389844894409
    },
    {
      "name": "Key (lock)",
      "score": 0.7280083298683167
    },
    {
      "name": "Degeneracy (biology)",
      "score": 0.7120261192321777
    },
    {
      "name": "Generalizability theory",
      "score": 0.6933460235595703
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6140387654304504
    },
    {
      "name": "Object (grammar)",
      "score": 0.4962647557258606
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.4949468970298767
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49445605278015137
    },
    {
      "name": "Encoding (memory)",
      "score": 0.46217790246009827
    },
    {
      "name": "Information retrieval",
      "score": 0.4252222180366516
    },
    {
      "name": "Modalities",
      "score": 0.4224385619163513
    },
    {
      "name": "Natural language processing",
      "score": 0.4135458469390869
    },
    {
      "name": "Image (mathematics)",
      "score": 0.11913004517555237
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Bioinformatics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I193775966",
      "name": "Yonsei University",
      "country": "KR"
    }
  ],
  "cited_by": 2
}