{
  "title": "Consistent Accelerated Inference via Confident Adaptive Transformers",
  "url": "https://openalex.org/W3154922002",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2897093679",
      "name": "Tal Schuster",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2439531980",
      "name": "Adam Fisch",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2661060019",
      "name": "Tommi Jaakkola",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2080327427",
      "name": "Regina Barzilay",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W2752099845",
    "https://openalex.org/W4231284028",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2767421475",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3118640283",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4287326542",
    "https://openalex.org/W2964059111",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W2137591261",
    "https://openalex.org/W2098824882",
    "https://openalex.org/W3127584070",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3170180819",
    "https://openalex.org/W3104738015",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3204656437",
    "https://openalex.org/W3009557165",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2170207925",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2944701285",
    "https://openalex.org/W2963494066",
    "https://openalex.org/W2963613748",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2171585602",
    "https://openalex.org/W2963393494",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2610140147",
    "https://openalex.org/W2950014519",
    "https://openalex.org/W3157809193",
    "https://openalex.org/W3016339201",
    "https://openalex.org/W3167950131",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W3022525124",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3034658202",
    "https://openalex.org/W4287617603",
    "https://openalex.org/W4287370462",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W3034292689",
    "https://openalex.org/W2618169590",
    "https://openalex.org/W582134693",
    "https://openalex.org/W1553101044",
    "https://openalex.org/W2325237720",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W3035030897",
    "https://openalex.org/W4287813307",
    "https://openalex.org/W3206159334",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W4287727380",
    "https://openalex.org/W2951244744",
    "https://openalex.org/W3129666340",
    "https://openalex.org/W3154971029"
  ],
  "abstract": "We develop a novel approach for confidently accelerating inference in the large and expensive multilayer Transformers that are now ubiquitous in natural language processing (NLP). Amortized or approximate computational methods increase efficiency, but can come with unpredictable performance costs. In this work, we present CATs – Confident Adaptive Transformers – in which we simultaneously increase computational efficiency, while guaranteeing a specifiable degree of consistency with the original model with high confidence. Our method trains additional prediction heads on top of intermediate layers, and dynamically decides when to stop allocating computational effort to each input using a meta consistency classifier. To calibrate our early prediction stopping rule, we formulate a unique extension of conformal prediction. We demonstrate the effectiveness of this approach on four classification and regression tasks.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4962–4979\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n4962\nConsistent Accelerated Inference via Conﬁdent Adaptive Transformers\nTal Schuster∗ Adam Fisch∗ Tommi Jaakkola Regina Barzilay\nComputer Science and Artiﬁcial Intelligence Laboratory\nMassachusetts Institute of Technology\n{tals,fisch,tommi,regina}@csail.mit.edu\nAbstract\nWe develop a novel approach for conﬁdently\naccelerating inference in the large and ex-\npensive multilayer Transformers that are now\nubiquitous in natural language processing\n(NLP). Amortized or approximate computa-\ntional methods increase efﬁciency, but can\ncome with unpredictable performance costs.\nIn this work, we present CATs— Conﬁdent\nAdaptive Transformers—in which we simul-\ntaneously increase computational efﬁciency,\nwhile guaranteeing a speciﬁable degree of con-\nsistency with the original model with high con-\nﬁdence. Our method trains additional pre-\ndiction heads on top of intermediate layers,\nand dynamically decides when to stop allocat-\ning computational effort to each input using a\nmeta consistency classiﬁer. To calibrate our\nearly prediction stopping rule, we formulate a\nunique extension of conformal prediction. We\ndemonstrate the effectiveness of this approach\non four classiﬁcation and regression tasks.1\n1 Introduction\nLarge pre-trained language models have become\nthe de facto standard approach for solving natu-\nral language processing tasks (Devlin et al., 2019;\nLiu et al., 2019). Despite their impressive perfor-\nmance, however, their often massive computational\nburden makes them costly to run (Schwartz et al.,\n2019; Sharir et al., 2020). Concerns about their efﬁ-\nciency have kindled a large body of research in the\nﬁeld (Sanh et al., 2020; Schwartz et al., 2020; Fan\net al., 2020). For multilayered architectures such\nas the Transformer, a popular approach is adap-\ntive early exiting (Schwartz et al., 2020; Xin et al.,\n2020a, inter alia). Early exiting takes advantage of\nthe observation that task instances vary in complex-\nity. In this setting, “early” classiﬁers are added on\ntop of the simpler features of intermediate layers\n*The ﬁrst two authors contributed equally.\n1/external_linkhttps://github.com/TalSchuster/CATs\nFigure 1: Our CAT modelGcan save computational re-\nsources by exiting early on certain inputs—while guar-\nanteeing predictive consistency with the full model F.\nin the base model, and can trigger a prediction be-\nfore the full model is executed. Naively deciding\nwhen to preempt computation, however, can result\nin unpredictable decreases in model accuracy.\nQuantifying the uncertainty in a prediction\nin order to decide when additional computation\nis needed (or not) is critical to making predic-\ntions quickly without excessively sacriﬁcing per-\nformance. In this paper, we present Conﬁdent\nAdaptive Transformers (CATs), a general method\nfor increasing Transformer-based model efﬁciency\nwhile remaining conﬁdent in the quality of our pre-\ndictions. Speciﬁcally, given a ﬁxed, expensive l-\nlayer model F(x), we create an amortized model\nG(x) that includes early classiﬁers {F1,..., Fl}.2\nWe then make Gprovably consistent with the origi-\nnal Fwith arbitrarily high probability (e.g., 95%\nof the time). This process is illustrated in Figure 1.\nOur approach builds on conformal prediction\n(CP), a model-agnostic and distribution-free frame-\nwork for creating well-calibrated predictions (V ovk\net al., 2005). Concretely, suppose we have been\n2We simply deﬁne the ﬁnal Fl as Fl(x) ≜ F(x) ∀x.\n4963\n(Ex.1) Claim: All airports in Guyana were closed for all international passenger ﬂights until 1 May 2020.\nEvidence: Airports in Guyana are closed to all international passenger ﬂights until 1 May 2020.\n(Ex.2) Claim: Deng Chao broke sales record for a romantic drama.\nEvidence: The ﬁlm was a success and broke box ofﬁce sales record for mainland-produced romance ﬁlms.\nFigure 2: Conﬁdence levels given by our meta model regarding the consistency of our prediction as computation\nprogresses. Ex.1 from the VitaminC fact veriﬁcation dataset is “easy”, and is classiﬁed consistently by all early\nclassiﬁers Fk (Supports). The meta conﬁdence captures this, and increases with time. Ex.2 is harder—and\nthe prediction changes (Refutes/NEI) as it propagates though the Transformer layers. Appropriately, the meta\nconﬁdence is low. The exact exit layer of Gis determined as a function of a user-speciﬁed tolerance ϵ, see Eq. (1).\ngiven n examples, Xi ∈ X, i = 1,...,n , as\nunlabeled calibration data, that have been drawn\nexchangeably from some underlying distributionP.\nLet Xn+1 ∈X be a new exchangeable test exam-\nple for which we would like to make a prediction.\nThe aim of our method is to construct Gsuch that\nit agrees with Fwith distribution-free marginal\ncoverage at a tolerance level ϵ∈(0,1), i.e.,\nP\n(\nG(Xn+1) =F(Xn+1)\n)\n≥1 −ϵ. (1)\nWe consider Gto be ϵ-consistent if the frequency\nof error, G(Xn+1) ̸= F(Xn+1), does not exceed\nϵ.3 By design, this ensures that Gpreserves at\nleast (1 −ϵ)-fraction of F’s original performance.\nWithin these constraints, the remaining challenge\nis to make Grelatively efﬁcient (e.g., a consistent,\nbut vacuous, model is simply the identity G≜ F).\nIn order to support an efﬁcient G, we need a reli-\nable signal for inferring whether or not the current\nprediction is likely to be stable. Past work (e.g.,\nSchwartz et al., 2020) rely on potentially poorly\ncorrelated metrics such as the early classiﬁer’s soft-\nmax response. We address this challenge by instead\ndirectly learning meta “consistency predictors” for\neach of the l−1 early classiﬁers of our l layer\nmodel, by leveraging patterns in past predictions.4\nFigure 2 demonstrates the progression of meta con-\nﬁdence scores across layers when applied to “easy”\nversus “hard” instances from the VitaminC fact\nveriﬁcation task (Schuster et al., 2021).\n3For regression, we deﬁne equality as |G(·) −F(·)|≤ τ.\n4We refer to the meta aspect of the classiﬁer, not the opti-\nmization process (i.e., not to be confused with meta-learning).\nWe pair the scores of our meta classiﬁer for each\nlayer with a stopping rule that is calibrated using a\nunique twist on standard conformal prediction. Tra-\nditionally, CP is used to construct prediction sets\nthat cover the desired target (e.g., Yn+1) with high\nprobability. We invert the CP problem to ﬁrst infer\nthe multi-label set of inconsistent layers, and then\nexit at the ﬁrst layer that falls in its complement.\nWe then demonstrate that this can be reduced to\nsetting a simple (but well-calibrated) exit threshold\nfor the meta classiﬁer scores. Our resulting algo-\nrithm is (1) fast to compute in parallel to the main\nTransformer, (2) requires only unlabeled data, and\n(3) is statistically efﬁcient in practice, in the sense\nthat it ﬁnds low exit layers on average while still\nmaintaining the required predictive consistency.\nWe validate our method on four diverse NLP\ntasks—covering both classiﬁcation and regression,\ndifferent label space sizes, and varying amounts of\ntraining data. We ﬁnd that it constitutes a simple-\nyet-effective approach to conﬁdent adaptive pre-\ndiction with minimal interventions and desirable\ntheoretical guarantees. In short, we provide:\n1. A novel theoretical extension of conformal pre-\ndiction to accommodate adaptive prediction;\n2. An effective meta consistency classiﬁer for de-\nriving a conﬁdent “early exiting” model;\n3. A demonstration of the utility of our frame-\nwork on both classiﬁcation and regression tasks,\nwhere we show signiﬁcant efﬁciency improve-\nments, while guaranteeing high consistency.\n4964\n2 Related Work\nAdaptive computation. Reducing the computa-\ntional cost of neural models has received intense\ninterest. Adaptive approaches adjust the amount of\ncomputation per example toamortize the total infer-\nence cost (see Teerapittayanon et al., 2017; Graves,\n2017; Huang et al., 2018; Kaya et al., 2019; Wang\net al., 2018, inter alia). As discussed in §1, our\nmethod is inspired by the approach of Schwartz\net al. (2020) and others (Liu et al., 2020; Geng\net al., 2021; Zhou et al., 2020), where they preempt\ncomputation if the softmax value of any early clas-\nsiﬁer is above a predeﬁned threshold. Yet unlike\nour approach, their model is not guaranteed to be\naccurate. In concurrent work, Xin et al. (2021) pro-\npose a meta conﬁdence classiﬁer similar to ours.\nHowever, as in previous work, they do not address\nthe calibration part to guarantee consistency.\nConﬁdent prediction. A large amount of re-\nsearch has been dedicated towards calibrating the\nmodel posterior, pθ(ˆyn+1|xn+1), such that the ac-\ncuracy, yn+1 = ˆyn+1, is indeed equal to the esti-\nmated probability (Niculescu-Mizil and Caruana,\n2005; Gal and Ghahramani, 2016; Guo et al., 2017).\nIn theory, these estimates could be leveraged to cre-\nate conﬁdent early exits—e.g., similar to Schwartz\net al. (2020). Ensuring calibrated probabilities of\nthis form is hard, however, and existing methods\noften still suffer from miscalibration. Additionally,\nmany methods exist for bounding the true error of a\nclassiﬁer (Langford, 2005; Park et al., 2021), but do\nnot give end-users opportunities to control it. More\nsimilar to our work, selective classiﬁcation (Geif-\nman and El-Yaniv, 2017) allows the model to ab-\nstain from answering when not conﬁdent, in order\nto maintain a target error rate only over answered\ninputs. Our work gives a different and statistically\nefﬁcient technique applied to consistent prediction.\nConformal prediction. CP (V ovk et al., 2005)\ntypically is formulated in terms of prediction sets\nC(Xn+1), where ﬁnite-sample, distribution-free\nguarantees can be given over the event that Ccon-\ntains Yn+1. As we discuss in §4, internally our\nmethod follows a similar approach in which we try\nto conservatively identify the inadmissible set of\nall layers that are inconsistent (and exit at the ﬁrst\nlayer that falls in that set’s complement). Most rel-\nevant to our work, Cauchois et al. (2021) presents\nalgorithms for conformal multi-label predictions.\nWe leverage similar methods in our model, but\nformulate our solution in terms of the comple-\nment of a multi-label set of inconsistentpredictions.\nOur work adds to several recent directions that ex-\nplore CP in the context of risk-mitigating applica-\ntions (Lei and Candès, 2020; Romano et al., 2020;\nBates et al., 2020; Fisch et al., 2021a, inter alia),\nor meta-learning settings (Fisch et al., 2021b).\n3 Early Exiting Transformers\nIn the following, we describe our dynamic early ex-\niting model. We summarize early classiﬁcation (fol-\nlowing previous work) for convenience (§3.1), and\nthen present our novel meta consistency classiﬁer\n(§3.2). We focus on classiﬁcation and regression\ntasks, given a model F(x) =y. We assume that F\nmaps the input x∈X into a series of feature rep-\nresentations before making the prediction y ∈Y.\nHere, Fis a multilayered Transformer (Vaswani\net al., 2017) composed of l layers (although our\nmethod can be applied to any multilayer network).\nFor all downstream tasks we follow stan-\ndard practice and assume that the input con-\ntains a [CLS] token whose representation is\nused for prediction. For classiﬁcation, we use a\ntask-speciﬁc head, softmax(Wo(φ(Wph[CLS]))),\nwhere h[CLS] ∈Rd is the hidden representation of\nthe [CLS] token,5 φis a nonlinear activation, and\nW∗are linear projections, where Wp ∈Rd×d and\nWo ∈R|Y|×d. Regression is treated similarly, but\nuses a 1-d output projection, wo ·h[CLS].\n3.1 Early predictors\nF’s structure yields a sequence of hidden[CLS]\nrepresentations, {h(1)\n[CLS],..., h(l)\n[CLS]}, where\nh(k)\n[CLS] ∈Rd is the representation after applying\nlayer k. After each intermediate layer k < l, we\ntrain an early classiﬁcation head that is similar to\nthe head used in F, but reduce the dimensionality\nof the ﬁrst projection to W(k)\np ∈Rde×d (this is\npurely for efﬁciency6). The ﬁnal Fl is unchanged\nfrom F. These extra (l−1)×(de×d+de×|Y|) pa-\nrameters are quick to tune on top of a ﬁxed F, and\nwe can reuse F’s training data asDtune.7 The clas-\nsiﬁer Fk(x) = softmax(W(k)\no (φ(W(k)\np h(k)\n[CLS])))\nis then used after layer kto get an early prediction\ncandidate. Early regression is handled similarly.\n3.2 Meta early exit classiﬁer\nTo decide when to accept the current prediction\nand stop computation, we require some signal as\n5dvaries by F. In Albert-xlarge d= 2048.\n6We simply set de = 32in all our experiments.\n7Or if Dtune is unlabeled, we can use F(x) as labels.\n4965\nMeta\nFeature\nDescription\nˆyk The current prediction.\nhistory The past k−1 predictions, ˆy1:k−1\n(For classiﬁcation we give pk(ˆyk|x)).\npmax\nk Prob. of the prediction, pk(ˆyk|x).\npdiﬀ\nk Difference in prob. of top predictions,\npk(ˆyk|x) −argmaxyk̸=ˆyk pk(yk|x).\nTable 1: Additional meta features used as input to the\nmeta early exit classiﬁer, Mk. Where speciﬁed, the\nprobability pk is taken from the model’s early softmax.\npmax\nk and pdiﬀ\nk are only used for classiﬁcation tasks.\nto how likely it is that Fk(x) = F(x). Previ-\nous work relies on intrinsic measures (e.g., soft-\nmax response). Here, we present a meta classi-\nﬁer to explicitly estimate the consistency of an\nearly predictor. Given ﬁxed Fk and F, we train\na small binary MLP, Mk(x) ∈ R, on another\nunlabeled (limited) sample of task in-domain data,\nDmeta. As input, we provide the current “early”\nhidden state φ(W(k)\np h(k)\n[CLS]), in addition to sev-\neral processed meta features, see Table 1. We then\ntrain Mk with a binary cross entropy objective,\nwhere we maximize the likelihood of predicting\n1{Fk(xi) =F(xi)}for xi ∈Dmeta.\nUsing the trained Fk and Mk, we deﬁne the full\nadaptive model Gusing the prediction rule\nG(x; τ) :=\n\n\n\nF1(x) if M1(x) >τ1,\nF2(x) else if M2(x) >τ2,\n...\nFl(x) otherwise,\n(2)\nwhere τ = (τ1,...,τ l−1) are conﬁdence thresh-\nolds. The key challenge is to calibrate τk such that\nGguarantees ϵ-consistent performance per Eq. (1).\n3.3 Warmup: development set calibration\nA simple approach to setting τ is to optimize per-\nformance on a development set Ddev, subject to a\nconstraint on the empirical inconsistency:\nτ ∗:= minimize\n(τ1,...,τl−1)\nˆEdev[exit(G(X; τ ))]\ns.t. ˆEdev[1{G(X; τ ) =F(X)}] ≥1 −ϵ,\n(3)\nwhere exit(·) measures the exit layer, and ˆEdev is\nsimply the average over Ddev. Using a standard\nerror bound (Langford, 2005) over a separate split,\nDcal, we can then derive the following guarantee:\nProposition 3.1. Let Xi, i = 1,...,n be an i.i.d.\nsample with s = ∑n\ni=1 1{G(Xi; τ ) = F(Xi)}.\nThen, up to a conﬁdence level δ, we have that\nP(P(G(X; τ ) =F(X)) ≥1 −˜ϵ) ≥1 −δ, (4)\nwhere ˜ϵis the solution to Beta(s,n −s+ 1) =δ,\nand Beta is the incomplete beta function.\nA proof is given in Appendix A. Though in prac-\ntice ˜ϵmight be close to ϵfor most well-behaved\ndistributions, unfortunately Eq. (4) does not give a\nfully speciﬁable guarantee as per Eq. (1). Readjust-\ning τ based on Dcal requires correcting for multi-\nple testing in order to remain theoretically valid,\nwhich can quickly become statistically inefﬁcient.\nIn the next section, we provide a novel calibration\napproach that allows us to guarantee a target per-\nformance level with strong statistical efﬁciency.\n4 Conformalized Early Exits\nWe now formulate the main contribution of this pa-\nper, which is adistribution-free and model-agnostic\nmethod based on CP for guaranteeing any perfor-\nmance bound an end-user chooses to specify.8 Our\ntraining (§3), conformal calibration (§4), and infer-\nence pipelines are summarized in Algorithm 1.\n4.1 Conformal formulation\nLet I(x) :={i: Fi(x) ̸= F(x)}be the index set\nof layers that areinconsistent with the ﬁnal model’s\nprediction. To maintain ϵ-consistency, we must\navoid using any of the predictions speciﬁed by this\nset, Fi(x) where i∈I(x), more than ϵ-fraction of\nthe time for x∈X. In §4.2, we show how M1:l−1\ncan be paired with a conformal procedure to obtain\ncalibrated thresholds τ = (τ1,...,τ l−1) such that\nwe obtain a conservative prediction of I(x),\nCϵ(x) :={k: Mk(x) ≤τk}, (5)\nwhere we ensure that I(x) ⊆Cϵ(x) with probabil-\nity at least 1 −ϵ. Proposition 4.1 states our guaran-\ntee when τ is paired with Gfollowing Eq. (2).\nProposition 4.1. Assume that examples Xi, i =\n1,...,n + 1are exchangeable. For anyϵ∈(0,1),\nlet the index set Cϵ (based on the ﬁrst nexamples)\nbe the output of conformal procedure satisfying\nP(I(Xn+1) ⊆Cϵ(Xn+1)) ≥1 −ϵ. (6)\nDeﬁne K := min{j : j ∈C c\nϵ(Xn+1)}, the ﬁrst\nexit layer selected by Gfollowing Eq. (2).9 Then\nP(FK(Xn+1) =F(Xn+1)) ≥1 −ϵ. (7)\nRemark 4.2. Note that Eq. (6) is stricter than\nnecessary. Fundamentally, we only require that\n8See Shafer and V ovk (2008) for a concise review of CP.\n9Here Ac denotes the complement index set {i: i̸∈A}.\n4966\nP(K ∈Ic(Xn+1)) ≥1 −ϵ. Nevertheless, Eq. (6)\nis easier to calibrate, and leads to strong empirical\nresults despite being theoretically conservative.\nRemark 4.3. During inference we do not fully con-\nstruct Cϵ; it is only used to calibrate τ beforehand.\n4.2 Conformal calibration\nWe now describe our conformal procedures for\ncalibrating τ . Conformal prediction is based on\nhypothesis testing, where for a given input xand\npossible output y, a statistical test is performed to\naccept or reject the null hypothesis that the pairing\n(x,y) is correct. In our setting, we consider the null\nhypothesis that layer kis inconsistent, and we use\nMk(x) as our test statistic. Since Mk is trained\nto predict 1{Fk(xi) = F(xi)}, a high value of\nMk(x) indicates how “surprised” we would be\nif layer kwas in fact inconsistent with layer lfor\ninput x. Informally, a low level of surprise indicates\nthat the current input “conforms” to past data. To\nrigorously quantify the degree of conformity via the\nthreshold τk for predictor Mk, we use a held-out\nset of nunlabeled, exchangeable examples, Dcal.\n4.2.1 Independent calibration\nAs a ﬁrst approach, we construct Cϵ(x) by compos-\ning l−1 separate tests for Fk(x) ̸= F(x), each\nwith signiﬁcance αk, where αk are corrected for\nmultiple testing. Let v(1:n,∞)\nk denote the inﬂated\nempirical distribution of inconsistent layer scores,\n{Mk(xi): xi ∈Dcal,Fk(xi) ̸= F(xi)}∪{∞}.\nInﬂating the empirical distribution is critical to our\nﬁnite sample guarantee, see Appendix A. We then\ndeﬁne τind\nk = Quantile\n(\n1 −αk,v(1:n,∞)\nk\n)\n, and\npredict the inconsistent index set at x∈X as\nCind\nϵ (x) =\n{\nk: Mk(x) ≤τind\nk\n}\n. (8)\nThe following theorem states how to set each αk\nsuch that the quantiles τind\nk yield a valid Cind\nϵ .\nTheorem 4.4. Let αk = ωk ·ϵ, where ωk is a\nweighted Bonferroni correction, i.e.,∑l−1\nk=1 ωk = 1.\nThen Cind\nϵ (Xn+1) is a valid set that satisﬁes Eq.(6).\nRemark 4.5. ω1:l−1 can be tuned on a develop-\nment set Ddev as long as Ddev is distinct from Dcal.\n4.2.2 Shared calibration\nCind\nϵ has the advantage of calibrating each layer\nindependently. As lgrows, however,αk will tend\nto 0 in order to retain validity (as speciﬁed by The-\norem 4.4). As a result, Cind\nϵ will lose statistical\nAlgorithm 1 Consistent accelerated inference.\nDeﬁnitions: Fis a multilayered classiﬁer trained on Dtrain.\nDtune, Dmeta and Dscale are collections of in-domain unla-\nbeled data points (in practice, we reuse Dtrain and divide it to\n70/20/10%, respectively). Dcal has in-domain unlabeled ex-\namples not in Dtrain (in practice, we take a subset of the task’s\nvalidation set). ϵis the user-speciﬁed consistency tolerance.\n1: function TRAIN (F, Dtune, Dmeta)\n2: # Learns F1...l−1 and M1...l−1 components\n3: # of amortized model Gfor Eq. (2) (see §3.1 and §3.2).\n4: Initialize Gfrom Fand add early prediction heads.\n5: # (All of F’s base parameters inGare frozen.)\n6: Train prediction heads F1...l−1 on Dtune.\n7: Add meta early exit classiﬁers M1...l−1 to G.\n8: # (All of G’s other parameters are frozen.)\n9: Train meta early exit classiﬁers M1...l−1 on Dmeta.\n10: Optionally apply temperature scaling using Dscale.\n11: return G\n12: function CALIBRATE (G, Dcal, ϵ)\n13: # Sets thresholds τ of amortized model Gfor Eq. (2)\n14: # using shared calibration (see §4.2.2).\n15: M ←{∞}\n16: for x∈Dcal do\n17: S←{}\n18: # Record all inconsistent layers for input x.\n19: # Keep the highest (false) conﬁdence score.\n20: for k∈[1,l −1] do\n21: if Fk(x) ̸= F(x) then\n22: S←S∪Mk(x)\n23: M ←M ∪max (S)\n24: # Share one threshold across layers.\n25: τshare ←Quantile\n(\n1 −ϵ,M\n)\n26: return [τshare] ×(l−1)\n27: function PREDICT (G,τ , x)\n28: # Implements Eq. (2) to exit early with conﬁdence.\n29: for k∈[1,l −1] do\n30: Compute the k-th prediction head of G, Fk(x).\n31: if Mk(x) >τk then\n32: return Fk(x)\n33: # Fallback to prediction using full computation.\n34: return Fl(x)\nefﬁciency. Following a similar approach to Cau-\nchois et al. (2021) and Fisch et al. (2021a), we\ncompute a new test statistic, Mmax, as\nMmax(x) = max\nk∈[l−1]\n{Mk(x) :Fk(x) ̸= F(x)}. (9)\nWe discard ill-deﬁned values when Mmax(x) =\nmax ∅. Mmax(x) reﬂects the worst-case conﬁ-\ndence across inconsistent layers for input x(i.e.,\nwhere Mk(x) predicts a high consistency likeli-\nhood for layer kwhen layer kis, in fact, inconsis-\ntent). This worst-case statistic allows us to keep a\nconstant signiﬁcance level ϵ, even as lgrows. Let\nm(1:n,∞) denote the inﬂated empirical distribution,\n{Mmax(xi): xi ∈Dcal,∃kFk(xi) ̸= F(xi)}∪{∞}.\nWe then deﬁne a single threshold shared across\nlayers, τshare = Quantile\n(\n1 −ϵ,m(1:n,∞))\n, and\n4967\nDataset |Y| Train Dev. Test Ftest perf.\nIMDB 2 20K 5K 25K 94.0\nVitaminC 3 370K 10K ∗ 55K 90.6\nAG News 4 115K 5K 7.6K 94.4\nSTS-B ∞ 5.7K 1.5K 1.4K 89.8\nTable 2: Task dataset and label space sizes. The right-\nmost column reports either test accuracy (classiﬁcation)\nor Pearson-correlation (regression). ∗We downsample\nthe 63K public development set to expedite validation.\npredict the inconsistent index set at x∈X as\nCshare\nϵ (x) =\n{\nk: Mk(x) ≤τshare\n}\n(10)\nTheorem 4.6. For any number of layers l ∈N+,\nCshare\nϵ (Xn+1) is a valid set that satisﬁes Eq. (6).\n5 Experimental Setup\nFor our main results, we use an Albert-xlarge\nmodel (Lan et al., 2020) with 24 Transformer lay-\ners. Results using an Albert-base model and a\nRoBERTa-large model (Liu et al., 2019) are in Ap-\npendix C. See Appendix B for implementation de-\ntails. We did not search across different values for\nthe hyper-parameters of For Gas our approach is\ngeneral and guarantees consistency for any Fwith\nany nonconformity measure (See Appendix C.2).\nTuning the hyper-parameters could further improve\nthe efﬁciency of Gwhile preserving consistency.\n5.1 Tasks\nWe evaluate our methods on three classiﬁcation\ntasks with varying label space size |Y|and difﬁ-\nculty: IMDB (Maas et al., 2011) sentiment analy-\nsis on movie reviews, VitaminC (Schuster et al.,\n2021) fact veriﬁcation with Wikipedia articles, and\nAG (Gulli, 2004; Zhang et al., 2015) news topic\nclassiﬁcation. We also evaluate on the STS-B (Cer\net al., 2017) semantic textual similarity regression\ntask where Y ∈[0,5] ⊂ R. Dataset statistics,\nalong with the test set performance of our original\nFmodel (Albert-xlarge), are contained in Table 2.\n5.2 Baselines\nIn addition to our main methods discussed in §4.2,\nwe compare to several non-CP baselines. Note that\nthe following methods are not guaranteed to give\nwell-calibrated performance (as our CP ones are).\nStatic. We use the same number of layers for all\ninputs. We choose the exit layer as the ﬁrst one that\nobtains the desired consistency on average on Dcal.\nSoftmax threshold. Following Schwartz et al.\n(2020), we exit on the ﬁrst layer where pmax\nk ≥\n1 −ϵ, where pmax\nk denotes the maximum softmax\nresponse of our early classiﬁer. Softmax values are\ncalibrated using temperature scaling (Guo, 2017)\non another held-out (labeled) data split, Dscale.\nMeta threshold. Even if perfectly calibrated,\npmax\nk from softmax thresholding is not measuring\nconsistency likelihood P(G(X) = F(X) |X =\nx), but rather P(G(X) = Y |X = x). This is\nequivalent if Fis an oracle, but breaks down when\nFis not. We also experiment with thresholding the\nconﬁdence value of our meta classiﬁer (§3.2) in a\nsimilar way (i.e., exiting when it exceeds 1 −ϵ).\n5.3 Evaluation\nFor each task, we use a proper training, validation,\nand test set. We use the training set to learn Fand\nG. We perform model selection on the validation\nset, and report ﬁnal numbers on the test set. For all\nmethods, we report the marginalized results over 25\nrandom trials, where in each trial we partition the\ndata into 80% Dcal (x1:n) and 20% Dtest (xn+1).\nIn order to compare different methods across all\ntolerance levels, we plot each metric as a function\nof ϵ. Shaded regions show the 16-84th percentiles\nacross trials. We report the following metrics:\nConsistency. We measure the percent of inputs\nfor which the prediction of the CAT model Gis the\nsame as the full Transformer on our test prediction,\ni.e., G(Xn+1) = F(Xn+1). For regression tasks,\nwe count a prediction as consistent if it is within\na small margin τ from the reference (we use τ =\n0.5). As discussed in §1, if Gis ϵ-consistent, we can\nalso derive an average performance lower bound: it\nwill be at least(1−ϵ)×F’s average performance.10\nLayers ( \u0003). We report the computational cost\nof the model as the average number of Trans-\nformer layers used. Our goal is to improve the\nefﬁciency (i.e., use fewer layers) while preserving\nϵ-consistency. We choose this metric over abso-\nlute run-time to allow for implementation-invariant\ncomparisons, but we provide a reference analysis\nnext, to permit easy approximate conversions.\n5.4 Absolute runtime analysis\nThe exact run-time of Gdepends on the efﬁciency\nof the hardware, software, and implementation\nused. Ideally, the early and meta classiﬁers can run\nin parallel with the following Transformer layer\n(layer k+ 1). As long as they are faster to compute\n10In practice, the performance is likely to be higher than\nthis lower bound, since inconsistencies with Fcould lead to a\ncorrect prediction when Fwould have otherwise been wrong.\n4968\n(a) IMDB\n (b) VitaminC\n (c) AG News\nFigure 3: Classiﬁcation results (dev). While both our CP-based methods give valid consistencies (above diagonal),\nshared calibration generally results in earlier exits. This advantage is especially pronounced at smaller tolerance\nlevels (right-hand side), where it signiﬁcantly outperforms other approaches. Our meta-learned conﬁdence measure\nMk improves over using the softmax response as a drop-in replacement, especially for tasks with larger|Y|. Note\nthat we care more about the right-hand side behavior, (i.e., larger 1 −ϵ), as it corresponds to higher consistency.\nconcurrently than a single layer, this will avoid in-\ncurring any additional time cost. An alternative\nnaive synchronous implementation could lead to\ninefﬁciencies when using a small tolerance ϵ.\nWe provide a reference timing for the IMDB task\nimplemented with the Transformers (Wolf et al.,\n2020) library, PyTorch 1.8.1 (Paszke et al., 2019),\nand an A100-PCIE-40GB Nvidia GPU with CUDA\n11.2. A full forward path of an Albert-xlarge takes\n22.32ms per input, 0.85ms ×24 for the transformer\nlayers and 1.95ms for the embedding layer and\ntop classiﬁer. Our early classiﬁer takes 0.20ms\nand the meta classiﬁer takes 0.11ms. Therefore,\nwith a naive implementation, a CAT modelGwith\nan average exit layer less than 17.6 with the meta\nclassiﬁer, or 19.5 without, will realize an overall\nreduction in wall-clock time relative to the full F.\nWe report example speedup times with the\nnaive implementation in §6.3, as well as an imple-\nmentation invariant multiply-accumulate operation\n(MACs) reduction measure. The added computa-\ntional effort per layer of the early predictor and\nmeta-classiﬁer is marginal (only 66,304 and 1,920\nMACs, respectively). In comparison, Albert-xlarge\nwith an input length of 256 has ∼3 ·1011 MACs.\n6 Experimental Results\nWe present our main results. We experiment with\nboth our meta classiﬁer Mk conﬁdence score\n(Meta, §3.2), and, for classiﬁcation tasks, the early\nclassiﬁer’s softmax response,pmax\nk (SM), as a drop-\nin replacement for Mk (at no additional computa-\ntional cost). Appendix C reports results with other\ndrop-in Mk replacements, in addition to results us-\ning our naive development set calibration approach\n(§3.3). Appendix D provides qualitative examples.\n6.1 Classiﬁcation results\nFigure 3 summarizes the average consistency and\nnumber of layers used byGas a function ofϵ, while\nTable 3 presents results for speciﬁc ϵon task test\nsets. Independent calibration proves to be quite\nconservative due to the loss of statistical power\nfrom the loose union bound of the Bonferroni cor-\nrection for large l(here l= 24). At some levels of\nϵ, non-CP baselines perform competitively, how-\never, they lack formal guarantees. Overall, for the\nmost critical tolerance levels (small ϵ, right-hand\nside of the plots), our shared method leads to sig-\nniﬁcant efﬁciency gains while still maintaining the\ndesired level of consistency (above the diagonal).\nThe effectiveness of our meta predictor, Mk, is\nmost pronounced for tasks with |Y| > 2, where\nthe drop-in softmax score (SM) becomes less in-\ndicative of consistency. Both SM and Meta are\nrelatively well-calibrated for IMDB and VitaminC,\nwhich makes the threshold-based exit rule a com-\npetitive baseline. Still, our Shared/ Meta method\nprovides both reliable and signiﬁcant gains.\nThe computational advantage of our CAT model\n4969\nMethod IMDB VitaminC AG News\nConsist. Acc. Layers Consist. Acc. Layers Consist. Acc. Layers\n1 −ϵ= 0.95: (88.50) (86.10) (89.02)\nStatic 95.54 92.88 18.36 95.51 89.40 21.00 95.48 93.20 22.00\nThres./ SM 99.65 94.01 16.55 99.83 90.59 20.07 100.00 94.44 22.28\nThres./ Meta 99.98 93.96 17.73 99.73 90.59 19.67 99.41 94.00 16.21\nIndep./ Meta 99.66 93 .82 15 .69 99.07 89 .97 19 .60 99.81 94 .31 20 .58\nShared/ SM 97.17 93 .24 12 .65 96.87 88 .99 17 .58 97.15 93 .43 13 .24\nShared/ Meta 97.15 92 .71 10.83 96.91 89 .01 16.79 97.08 92 .50 10.17\n1 −ϵ= 0.90: (83.84) (81.57) (84.33)\nStatic 90.82 89.47 14.00 92.57 87.80 19.00 90.88 89.10 14.00\nThres./ SM 98.88 93.93 14.71 99.05 90.27 18.91 99.68 94.21 19.53\nThres./ Meta 99.75 93.86 15.30 99.10 90.31 18.45 98.90 93.82 13.50\nIndep./ Meta 99.39 93 .67 14 .85 98.29 89 .42 18 .50 99.60 94 .18 17 .65\nShared/ SM 94.34 91 .77 10 .30 93.73 87 .00 16 .40 94.50 92 .01 10 .79\nShared/ Meta 94.36 90 .78 9.01 93.83 86 .89 15.33 94.29 90 .26 8.35\nTable 3: Classiﬁcation results (test) for speciﬁc tolerance levels. We report the accuracy lower bound guaranteed\nby our CP methods in parentheses. Shared/ Meta is reliably the most efﬁcient method (and isϵ-consistent). Greyed\nrows reﬂect approaches without guarantees; our CAT approaches with guarantees are presented below them.\nFigure 4: Dev results for the STS-B regression task.\nis dependent on the average difﬁculty of the task\nand the implementation. As Table 3 shows, allow-\ning up to an ϵof 10% inconsistency, for two of the\ntasks we cut down the average Transformer layer\nto only 9 out of 24 using our Shared/ Meta model.\nThis leads to an approximate speedup of 1.8×with\na synchronous implementation and of 2.7×with a\nconcurrent one, compared to running the full model.\nMoreover, Figure 5 illustrates the user’s control\nover available computational resources via mod-\nulating ϵ. Decreasing ϵincreases the conﬁdence\nlevel required before committing to the early clas-\nsiﬁer’s prediction (thereby increasing the average\nnumber of required layers), and vice-versa.\nMethod Consist. Layers\n1 −ϵ= 0.95:\nStatic 100.00 24.00\nThres./ Meta 99.87 19.19\nIndep./ Meta 99.29 23 .60\nShared/ Meta 96.42 17.64\n1 −ϵ= 0.90:\nStatic 92.51 20.00\nThres./ Meta 99.19 18.53\nIndep./ Meta 97.77 20 .26\nShared/ Meta 92.65 17.29\nTable 4: Test results for the STS-B regression task.\n6.2 Regression results\nTable 4 and Figure 4 present results for our regres-\nsion task, where we see similar trends. Here, an\nattractive advantage of our meta conﬁdence pre-\ndictor is its generalizability to multiple task output\ntypes. Notice that the event space of 1{G(X) =\nF(X)}= {0,1}always, regardless of the original\nY.11 This allows it to be easily adapted to tasks\nbeyond classiﬁcation, such as regression, where\ntraditional softmax-based conﬁdence measures (as\nused in, e.g., Schwartz et al. (2020)) are absent.\n6.3 Example efﬁciency gains\nFollowing the analysis in §5.4, we compute the\namortized inference time with a naive implementa-\ntion and report its percentage out of the full model.\nAs Table 5 shows, our Shared calibration is the\nmost efﬁcient method on all four tasks. For tasks\nwith many easy inputs (IMDB and AG News), our\nShared/ Meta method can save 45% - 49% of the\n11As long as equality is suitably deﬁned, e.g., for STS-B\nwe deﬁne consistent outputs as being within τ = 0.5 away.\n4970\nFigure 5: Distribution of exit layers per tolerance level ϵfor the IMDB task (dev set) with Shared/ Meta. Larger ϵ\nallows the CAT model to shift its predictions earlier by permitting for more inconsistencies with the full modelF.\nMethod Amortized time (100 ·TG/TF) MACs reduction (|F|/|G|)\nIMDB VitaminC AG News STS-B IMDB VitaminC AG News STS-B\nThres./ SM 76.91 96.66 99.58 N/A 1.63 1.27 1.23 N/A\nThres./ Meta 87.22 103.59 77.87 104.01 1.57 1.30 1.78 1.30\nIndep./ Meta 84.88 103 .85 99 .44 113 .00 1.62 1 .30 1 .36 1 .18\nShared/ SM 56.16 84.86 58.47 N/A 2.33 1 .46 2 .22 N/A\nShared/ Meta 54.53 % 87 .38 % 51.10 % 97.56 % ×2.66 ×1.57 ×2.87 ×1.39\nTable 5: Reference time speedup and model complexity reduction for 1 −ϵ = 0.90 (see Table C.2 for 0.95). We\ncompute the amortized time with the naive synchronous implementation (§5.4). A more efﬁcient implementation\ncan further reduce the time of G. The MACs reduction measure is implementation agnostic and expresses the ratio\nof computational effort saved byG. Our CAT models (non-greyed lines) not only guarantee1 −ϵconsistency with\nF, but are also signiﬁcantly more efﬁcient in practice when using Shared calibration.\ninference time when 1 −ϵ= 0.90. Unsurprisingly,\nthe absolute speedup is less signiﬁcant for harder\ntasks, but increases with higher tolerance levels.\nOn VitaminC, even though the Meta measure\nallows exiting on earlier layers, its additional meta\nclassiﬁers result in slightly slower inference on\naverage at this tolerance level, compared to our\nShared/ SM. With a more efﬁcient concurrent im-\nplementation, the Meta measure will be favorable.\nWe also compute the MACs reduction metric\nwhich is independent of the speciﬁc implementa-\ntion or hardware and shows the number of multiply-\naccumulate operations of the full model compared\nto our CAT model. As demonstrated in Table 5, our\nShared/ Meta method is most effective in reducing\nthe computational effort across all tasks for the two\nexamined tolerance levels.\n7 Conclusion\nThe ability to make predictions quickly without\nexcessively degrading performance is critical to\nproduction-level machine learning systems. In fact,\nbeing capable of quantifying the uncertainty in a\nprediction and deciding when additional computa-\ntion is needed (or not) is a key challenge for any\nintelligent system (e.g., see the System 1 vs. Sys-\ntem 2 dichotomy explored in Kahneman (2011)).\nIn this work, we addressed the crucial challenge\nof deciding when to sufﬁciently trust an early pre-\ndiction of Transformer-based models by learning\nfrom their past predictions. Our Conﬁdent Adap-\ntive Transformers (CATs) framework leverages\nmeta predictors to accurately assess whether or not\nthe prediction of a simple, early classiﬁer trained\non an intermediate Transformer representation is\nlikely to already be consistent with that of the full\nmodel F(X) (i.e., after all llayers of Fare com-\nputed). Importantly, we develop a new confor-\nmal prediction approach for calibrating the con-\nﬁdence of the meta classiﬁer that is (1) simple to\nimplement, (2) fast to compute alongside the Trans-\nformer, (3) requires only unlabeled data, and (4)\nprovides statistically efﬁcient marginal guarantees\non the event that the prediction of the faster, amor-\ntized CAT model is consistent with that of the full\nF. Our results on multiple tasks demonstrate the\ngenerality of our approach, and its effectiveness in\nconsistently improving computational efﬁciency—\nall while maintaining a reliable margin of error.\nAcknowledgements\nWe thank the MIT NLP group for helpful dis-\ncussions. TS is supported in part by DSO grant\nDSOCL18002. AF is supported in part by an NSF\nGRFP. This work is also supported in part by DSTA\naward DST00OECI20300823.\n4971\nReferences\nStephen Bates, Anastasios Nikolas Angelopoulos, Li-\nhua Lei, Jitendra Malik, and Michael I. Jordan. 2020.\nDistribution free, risk controlling prediction sets.\narXiv preprint: arXiv 2101.02703.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nMaxime Cauchois, Suyash Gupta, and John C. Duchi.\n2021. Knowing what you know: valid and vali-\ndated conﬁdence sets in multiclass and multilabel\nprediction. Journal of Machine Learning Research,\n22(81):1–42.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nC. J. Clopper and E. S. Pearson. 1934. The use of con-\nﬁdence or ﬁducial limits illustrated in the case of the\nbinomial. Biometrika, 26(4):404–413.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020.\nReducing transformer depth on demand with struc-\ntured dropout. In International Conference on\nLearning Representations (ICLR).\nAdam Fisch, Tal Schuster, Tommi Jaakkola, and\nRegina Barzilay. 2021a. Efﬁcient conformal predic-\ntion via cascaded inference with expanded admis-\nsion. In International Conference on Learning Rep-\nresentations (ICLR).\nAdam Fisch, Tal Schuster, Tommi Jaakkola, and\nRegina Barzilay. 2021b. Few-shot conformal pre-\ndiction with auxiliary tasks. In International Con-\nference on Machine Learning (ICML).\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as\na bayesian approximation: Representing model un-\ncertainty in deep learning. In International Confer-\nence on Machine Learning (ICML) , volume 48 of\nProceedings of Machine Learning Research , pages\n1050–1059. PMLR.\nYonatan Geifman and Ran El-Yaniv. 2017. Selec-\ntive classiﬁcation for deep neural networks. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 30.\nShijie Geng, Peng Gao, Zuohui Fu, and Yongfeng\nZhang. 2021. Romebert: Robust training of multi-\nexit bert.\nAlex Graves. 2017. Adaptive computation time for re-\ncurrent neural networks.\nAntonio Gulli. 2004. Ag’s corpus of news articles.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In International Conference on Machine\nLearning (ICML), volume 70, pages 1321–1330, In-\nternational Convention Centre, Sydney, Australia.\nPMLR.\nHongyu Guo. 2017. A deep network with visual text\ncomposition behavior. In Proceedings of the 55th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 2: Short Papers) , pages\n372–377, Vancouver, Canada. Association for Com-\nputational Linguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Deep residual learning for image recog-\nnition.\nGao Huang, Danlu Chen, Tianhong Li, Felix Wu, Lau-\nrens van der Maaten, and Kilian Weinberger. 2018.\nMulti-scale dense networks for resource efﬁcient im-\nage classiﬁcation. In International Conference on\nLearning Representations (ICLR).\nDaniel Kahneman. 2011. Thinking, fast and slow. Far-\nrar, Straus and Giroux, New York.\nYigitcan Kaya, Sanghyun Hong, and Tudor Dumi-\ntras. 2019. Shallow-deep networks: Understand-\ning and mitigating network overthinking. In Inter-\nnational Conference on Machine Learning (ICML) ,\nvolume 97, pages 3301–3310. PMLR.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR).\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations (ICLR).\nJohn Langford. 2005. Tutorial on practical prediction\ntheory for classiﬁcation. Journal of Machine Learn-\ning Research, 6(10):273–306.\n4972\nLihua Lei and Emmanuel Candès. 2020. Conformal in-\nference of counterfactuals and individual treatment\neffects.\nWeijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao,\nHaotang Deng, and Qi Ju. 2020. FastBERT: a self-\ndistilling BERT with adaptive inference time. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6035–\n6044, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nAlexandru Niculescu-Mizil and Rich Caruana. 2005.\nPredicting good probabilities with supervised learn-\ning (icml). In International Conference on Machine\nLearning (ICML).\nSangdon Park, Shuo Li, Insup Lee, and Osbert Bastani.\n2021. PAC conﬁdence predictions for deep neural\nnetwork classiﬁers. In International Conference on\nLearning Representations.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Advances in Neural Information Pro-\ncessing Systems, volume 32. Curran Associates, Inc.\nYaniv Romano, Rina Foygel Barber, Chiara Sabatti,\nand Emmanuel Candès. 2020. With malice toward\nnone: Assessing uncertainty via equalized coverage.\nHarvard Data Science Review.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin c! robust fact veriﬁcation with con-\ntrastive evidence. In North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT).\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren\nEtzioni. 2019. Green ai.\nRoy Schwartz, Gabriel Stanovsky, Swabha\nSwayamdipta, Jesse Dodge, and Noah A. Smith.\n2020. The right tool for the job: Matching model\nand instance complexities. In Proceedings of the\n58th Annual Meeting of the Association for Com-\nputational Linguistics , pages 6640–6651, Online.\nAssociation for Computational Linguistics.\nGlenn Shafer and Vladimir V ovk. 2008. A tutorial on\nconformal prediction. Journal of Machine Learning\nResearch (JMLR), 9:371–421.\nOr Sharir, Barak Peleg, and Yoav Shoham. 2020. The\ncost of training nlp models: A concise overview.\narXiv preprint: arXiv 2006.06138.\nSurat Teerapittayanon, Bradley McDanel, and H. T.\nKung. 2017. Branchynet: Fast inference via early\nexiting from deep neural networks.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS).\nVladimir V ovk, Alex Gammerman, and Glenn Shafer.\n2005. Algorithmic Learning in a Random World .\nSpringer-Verlag, Berlin, Heidelberg.\nXin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and\nJoseph E. Gonzalez. 2018. Skipnet: Learning dy-\nnamic routing in convolutional networks. In The Eu-\nropean Conference on Computer Vision (ECCV).\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nJi Xin, Rodrigo Nogueira, Yaoliang Yu, and Jimmy Lin.\n2020a. Early exiting BERT for efﬁcient document\nranking. In Proceedings of SustaiNLP: Workshop on\nSimple and Efﬁcient Natural Language Processing ,\npages 83–88, Online. Association for Computational\nLinguistics.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020b. DeeBERT: Dynamic early ex-\niting for accelerating BERT inference. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 2246–2251,\nOnline. Association for Computational Linguistics.\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\n2021. BERxiT: Early exiting for BERT with better\n4973\nﬁne-tuning and extension to regression. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 91–104, Online. Association for\nComputational Linguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. Bert loses pa-\ntience: Fast and robust inference with early exit. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 18330–18341. Curran Associates,\nInc.\nA Proofs\nWe ﬁrst state the following useful lemma on in-\nﬂated sample quantiles.\nLemma A.1. Let Quantile(α; F) denote the\nα quantile of distribution F. Let V1:n de-\nnote the empirical distribution over random\nvariables {V1,...,V n}. Furthermore, assume\nthat Vi, i = 1 , ...,n + 1 are exchange-\nable. Then for any α ∈ (0,1), we have\nP(Vn+1 ≤Quantile(α,V1:n ∪{∞})) ≥α.\nProof. This is a well-known result. Given support\npoints v1,...,v n ∈R for a discrete distribution\nF, let q = Quantile(α; F). Any points vi > q\ndo not affect this quantile, i.e., if we consider a\nnew distribution ˜F where all points vi > qare\nmapped to arbitrary values also larger than qthen\nQuantile(α; F) = Quantile(α; ˜F). Accordingly,\nfor the exchangeable Vi, we have\nVn+1 >Quantile(α; V1:n ∪{∞}) ⇐⇒\nVn+1 >Quantile(α; V1:(n+1)).\nEquivalently, we also have that\nVn+1 ≤Quantile(α; V1:n ∪{∞}) ⇐⇒\nVn+1 ≤Quantile(α; V1:(n+1)).\nGiven the discrete distribution over the n+ 1vari-\nables Vi, Vn+1 ≤Quantile(α; V1:(n+1)) implies\nthat Vn+1 is among the ⌈α(n+ 1)⌉smallest of\nV1:(n+1). By exchangeability, this event occurs\nwith probability at least ⌈α(n+1)⌉\nn+1 ≥α.\nA.1 Proof of Proposition 3.1\nProof. This result is based on Clopper-Pearson\nconﬁdence interval for Binomial random vari-\nables (Clopper and Pearson, 1934). As the bi-\nnary events 1{G(Xi; τ ) = F(Xi)}are i.i.d., the\nsum sis Binomial. Directly applying a one-sided\nClopper-Pearson lower bound on the true success\nrate, P(G(Xi; τ ) =F(Xi)), gives the result.\nA.2 Proof of Proposition 4.1\nProof. We prove by simple calculation using the\nproperty assumed in Eq. (6).\nP(FK(Xn+1) =F(Xn+1))\n= P(min Cc\nϵ(Xn+1) ∈Ic(Xn+1))\n≥P(Cc\nϵ(Xn+1) ⊆Ic(Xn+1))\n= P(I(Xn+1) ⊆Cϵ(Xn+1))\n≥1 −ϵ.\nA.3 Proof of Theorem 4.4\nProof. For a given k, let V(i)\nk := Mk(Xi) denote\nthe random meta conﬁdence values used for cal-\nibration, and V(n+1)\nk := Mk(Xn+1) the random\ntest point. For all k, Mk is trained and evaluated\non separate data (Dmeta vs Dcal ∪Dtest), preserv-\ning exchangeability. Therefore, as X1:n+1 are ex-\nchangeable, then V(1:n+1)\nk are also exchangeable.\nLayer k is included in Cind\nϵ iff V(n+1)\nk ≤\nQuantile(1 −αk,V (1:n)\nk ∪{∞}). For a given k,\nthis happens with probability at least 1 −αk by\nLemma A.1. Taken over all k ∈I(Xn+1) where\n|I(Xn+1)|is at most l−1 (i.e., all early layers are\ninconsistent), we have\nP(I(Xn+1) ⊆Cind\nϵ (Xn+1))\n= 1−P\n(⋃\nk∈I\n{k̸∈Cind\nϵ (Xn+1)}\n)\n≥1 −\n∑\nk∈I\nP(k̸∈Cind\nϵ (Xn+1)\n= 1−\n∑\nk∈I\nαk\n≥1 −ϵ.\nThe last inequality is given by the Bonferroni con-\nstraint, i.e., αk = ωk ·ϵ, where ∑l−1\ni=1 ωi = 1\nA.4 Proof of Theorem 4.6\nProof. By the same argument as Theorem 4.4, the\nmeta scores Mk(Xi) are exchangeable. Since\nMmax operates symmetrically across all Xi,\nM(i) = Mmax(Xi) are also exchangeable.\nLet M(n+1) denote the maximum meta score\nacross inconsistent layers for the new test point.\nBy Lemma A.1, this falls below Quantile(1 −\n4974\nϵ,M(1:n) ∪{∞}) with probability at least 1 −ϵ.\nSince M(n+1) reﬂects the maximum meta score,\nthis entails that the meta scores of all other incon-\nsistent layers k∈I(Xn+1) for Xn+1 will be below\nQuantile(1 −ϵ,M(1:n) ∪{∞}) if M(n+1) is, and\nthereby be included in Cshare\nϵ (Xn+1). This gives\nthe bound in Eq. (6).\nB Implementation Details\nWe implement our early exit Transformers (§3)\non top of the Transformers library (Wolf et al.,\n2020).12 We set de to 32 in our experiments. For\neach task we ﬁx a pre-trained Fand train the early\nand meta classiﬁers. We reuse the same training\ndata that was used for Fand divide it to 70/10/20%\nportions for Dtune,Dscale and Dmeta, respectively.\nFor classiﬁcation tasks, we add the temperature\nscaling step (Guo et al., 2017) after the early train-\ning to improve the calibration of the softmax. We\nrun the scaling for 100 steps on Dscale using an\nAdam optimizer (Kingma and Ba, 2015) with a\nlearning rate of 10−3. For the early and meta train-\ning we use the same optimizer as for F.\nWe ﬁx Frather than train it jointly with the new\ncomponents of Gto avoid any reduction in F’s\nperformance (Xin et al., 2020b). This also makes\nour method simple to train over any existing Trans-\nformer without having to retrain the whole model\nwhich could be very costly. Training all parameters\nof Gjointly can lead to more efﬁcient inference as\nthe early representations will be better suited for\nclassiﬁcation (Schwartz et al., 2020; Geng et al.,\n2021), but potentially with the cost of reducing\nthe accuracy of Fl. In the case of joint training,\nour CATs will provide consistency guarantees with\nrespect to the jointly-trained Fl.\nWe implement the conformal calibration process\nin Python and perform retrospective analysis with\ndifferent random splits of Dcal and Dtest. For The-\norem 4.4, we simply use the uniform Bonferroni\ncorrection, setting wk = 1\nl−1 ∀k. For the naive\ndevelopment set calibration, we use a shared thresh-\nold across all layers in order to reduce the examined\nsolution space in Equation 3.\nC Additional Results\nIn this section, we provide complementary results\nfor the experiments in the main paper. All results,\n12As discussed in §3, our methods can also be applied to\nany multilayered model such as BERT (Devlin et al., 2019),\nGPT (Brown et al., 2020), ResNet (He et al., 2015), and others.\nexcept for sections C.4 and C.5, are with an Albert-\nxlarge model as F, similar to the main paper. How-\never, we note that the results in these tables are\nbased on the development sets, while the tables in\nthe main paper report the test set results.\nC.1 Naive development set calibration\nFor completeness, we evaluate the simple, but\nnaive, calibration method described in §3.3. Recall\nthat in this approach we ﬁrst tune τ on a develop-\nment set, and then bound the resultingG’s accuracy\nusing another heldout calibration split. The bound\nwe get is static; we are not able to guarantee that it\nwill satisfy our performance constraint in Eq. (1).\nTable C.1 gives results for our models when us-\ning either the Meta or SM conﬁdence measures\n(which we threshold with τ ). We use half of\nDcal to ﬁnd the minimal threshold that provides\nϵ-consistency. Then, we evaluate the threshold on\nthe second half of Dcal to get the empirical error.\nWe compute the test set bound on this error with\na conﬁdence of δ= 10−2. As expected, the lower\nbound we compute is often signiﬁcantly below1−ϵ,\nas it reﬂects the uncertainty that our measured con-\nsistency is accurate. Often the measured empirical\nconsistency is also slightly below 1 −ϵ. At a high\nlevel, the overall consistency vs. efﬁciency trade-\noff is otherwise broadly similar to the one obtained\nby the Shared CP calibration.\nC.2 Nonconformity measure comparison\nThe test statistic used for a conformal prediction\nis typically called a nonconformity measure (i.e.,\nin our work this is Mk(x)). We experiment with\ndifferent nonconformity measures as drop-in re-\nplacements for Mk(x), and report the results in\nTable C.2. The conformal calibration guarantees\nvalidity with any measure, even a random one, as\nlong as they retain exchangeability. Good measures\nare ones that are statistically efﬁcient, and will min-\nimize the number of layers required for prediction\nat the required conﬁdence level. This is a result of\nsmaller Cϵ sets, that tightly cover the inconsistent\nlayers (and hence are more judicious with the com-\nplement, Cc\nϵ). To be consistent with previous work\nwhere softmax metrics are used (such as Schwartz\net al., 2020), we use pmax\nk as our non-Meta baseline\nin the main paper. In some settings, however, pdiﬀ\nk\nperforms slightly better.\nC.3 Exit layer statistics\nFigure C.1 depicts the distribution of exit layers for\nthe different tasks with three reference tolerance\n4975\nNonconformity IMDB VitaminC AG News\nmeasure Consist. Bound Layers Consist. Bound Layers Consist. Bound Layers\n1 −ϵ= 0.95:\nSM 95.16 93 .74 10 .39 94.84 94 .04 16 .60 95.02 93 .75 11 .63\nMeta 94.96 93 .72 9 .13 94.93 94 .12 15 .60 94.86 93 .58 9 .37\n1 −ϵ= 0.9:\nSM 90.22 88 .30 7 .35 89.85 88 .59 14 .93 89.72 88 .01 8 .98\nMeta 90.19 88 .36 7 .13 90.00 88 .70 13 .67 90.14 88 .48 6 .85\nTable C.1: Results (dev) using the naive development set calibration method (see §3.3). This method tunes the\nearly exit thresholds to get efﬁcient ϵ-consistent predictions on a development set, but does not guarantee that\nprediction will be ϵ-consistent on new data. “Consist.” measures the empirical consistency on a test set, from\nwhich we compute a guaranteed lower bound (“Bound”) to 99% conﬁdence. The bound is signiﬁcantly lower than\nour target 1 −ϵ, and the measured consistency in our experiments also falls slightly bellow 1 −ϵin some cases.\nNonconformity IMDB VitaminC AG News\nmeasure Consist. Acc. Layers Consist. Acc. Layers Consist. Acc. Layers\n1 −ϵ= 0.95: (88.50) (85.17) (89.02)\nRandom 97.23 91 .56 21 .57 96.91 87 .42 22 .71 97.11 91 .58 21 .60\nDKL(pk−1||pk) 97.36 92 .49 19 .33 96.84 88 .85 22 .28 97.08 92 .46 20 .18\nH(pk) 97.28 92 .84 12 .49 96.79 88 .28 17 .44 97.15 92 .79 14 .55\npdiﬀ\nk 97.28 92 .84 12 .49 96.83 88 .38 17 .42 96.96 92 .80 12 .89\npmax\nk (SM) 97.28 92 .84 12 .49 96.79 88 .31 17 .40 97.08 92 .81 13 .23\nMeta 96.99 92 .24 10 .75 96.91 88 .29 16 .49 96.98 91 .98 10 .60\n1 −ϵ= 0.90: (83.84) (80.69) (84.33)\nRandom 94.52 89 .68 19 .21 93.94 85 .44 21 .47 94.27 89 .28 19 .01\nDKL(pk−1||pk) 94.48 91 .36 12 .13 93.76 86 .81 20 .49 93.88 89 .98 14 .59\nH(pk) 94.49 91 .31 9 .91 93.67 86 .41 16 .29 94.54 90 .80 13 .08\npdiﬀ\nk 94.49 91 .31 9 .91 93.67 86 .53 16 .11 94.02 90 .56 10 .69\npmax\nk (SM) 94.49 91 .31 9 .91 93.68 86 .44 16 .13 94.05 90 .76 11 .01\nMeta 94.40 90 .45 8 .80 93.74 86 .17 15 .09 94.08 89 .72 8 .88\nTable C.2: Results (dev) of our Shared model on the classiﬁcation tasks using different nonconformity measures.\npdiﬀ\nk and pmax\nk are deﬁned in Table 1, DKL(pk−1||pk) is the Kullback-Leibler Divergence between the previous\nlayer’s softmax outputs and the current layer, and H(pk) is the entropy of the softmax outputs. Our CP-based\nShared method provides the guaranteed consistency with any measure, even random. The beneﬁt, however, of\nusing a better measure is in conﬁdently exiting earlier. Our Meta measure allows the use of least Transformer\nlayers meeting the consistency requirement with enough conﬁdence.\nlevels. Reducing ϵrequires greater conﬁdence be-\nfore exiting, resulting in later exits on average. We\nprovide example inputs with their respective exit\nlayer in Appendix D.\nC.4 Albert-base results\nFigure C.2 reports the classiﬁcation and regression\nresults with an Albert-base 12-layers model. The\ntrends are similar to the larger 24-layers version.\nAgain, we see the efﬁcacy of our Shared conformal\ncalibration and the Meta nonconformity scores. For\nexample, the AG News CAT Shared/ Meta model\ncan preserve 95% consistency while using less than\n5 Transformer layers on average.\nC.5 RoBERTa-large results\nFigure C.3 shows the results of our methods on\ntop of the RoBERTa-large 24-layers Transformer.\nOne main difference between RoBERTa and Albert,\nis that Albert shares the same parameters across\nall layers, essentially applying the same function\nrecursively, whereas RoBERTa learns different pa-\nrameters per layer. Yet, our method is agnostic\nto such differences and, as observed in the plots,\nresults in similar trends. The value of our Meta\nclassiﬁer compared to the softmax response is even\ngreater with the RoBERTa model.\nD Example Predictions\nTable D.1 reports examples of inputs for different\ntasks and the number of layers that our Albert-\n4976\n(a) VitaminC\n(b) AG News\n(c) STS-B\nFigure C.1: Distribution of exit layers per tolerance level ϵ(dev sets) with our Shared/ Meta Albert-xlarge model.\nSee Figure 5 for IMDB.\nMethod Amortized time (100 ·TG/TF) MACs reduction (|F|/|G|)\nIMDB VitaminC AG News STS-B IMDB VitaminC AG News STS-B\nThres./ SM 85.56 102.12 112.52 N/A 1.45 1.20 1.08 N/A\nThres./ Meta 99.85 109.93 91.95 107.44 1.35 1.22 1.48 1.25\nIndep./ Meta 89.25 109 .57 114 .66 130 .36 1.53 1 .22 1 .17 1 .02\nShared/ SM 67.22 90.41 69.99 N/A 1.90 1 .37 1 .81 N/A\nShared/ Meta 63.99 % 94 .97 % 60.56 % 99.38 % ×2.22 ×1.43 ×2.36 ×1.36\nTable C.2: Complementary results for Table 5 with 1 −ϵ= 0.95.\nxlarge CAT withϵ= 0.1 required. These examples\nsuggest that “easier” inputs (e.g., containing cue\nphrases or having large overlaps in sentence-pair\ntasks) might require less layers. In contrast, more\ncomplicated inputs (e.g., using less common lan-\nguage or requiring numerical analysis) can lead to\nadditional computational effort until the desired\nconﬁdence is obtained.\n4977\n(a) IMDB\n (b) VitaminC\n (c) AG News\n(d) STS-B\nFigure C.2: Development set results with an Albert-base 12-layers model as F.\n4978\n(a) IMDB\n (b) VitaminC\n (c) AG News\n(d) STS-B\nFigure C.3: Development set results with an RoBERTa-large 24-layers model asF.\n4979\nExit\nlayer\nGold\nlabel\nInput\nIMDB (Maas et al., 2011)\n1 Pos Without question, ﬁlm is a powerful medium, more so now than ever before, due to the accessibility of\nDVD/video, which gives the ﬁlmmaker the added assurance that his story or message is going to be seen\nby possibly millions of people. [...]\n4 Neg This movie was obscenely obvious and predictable. The scenes were poorly written and acted even worse.\n10 Pos I think Gerard’s comments on the doc hit the nail on the head. Interesting ﬁlm, but very long. [...]\n15 Pos here in Germany it was only shown on TV one time. today, as everything becomes mainstream, it’s\nabsolute impossible, to watch a ﬁlm like this again on the screen. maybe it’s the same in USA [...]\n20 Neg I tried to be patient and open-minded but found myself in a coma-like state. I wish I would have brought\nmy duck and goose feather pillow... [...]\n24 Neg Hypothetical situations abound, one-time director Harry Ralston gives us the ultimate post-apocalyptic\nglimpse with the world dead, left in the streets, in the stores, and throughout the landscape, sans in the\nmiddle of a forgotten desert. [...]\nVitaminC (Schuster et al., 2021)\n3 Sup Claim: Another movie titled The SpongeBob Movie: Sponge on the Run is scheduled for release in 2020.\nEvidence: A second ﬁlm titled The SpongeBob Movie : Sponge Out of Water was released in 2015, and\nanother titled The SpongeBob Movie: Sponge on the Run is scheduled for release in 2020.\n5 Sup Claim: Julie Bishop offered a defence of her nation’s intelligence cooperation with America.\nEvidence: The Australian Foreign Minister Julie Bishop stated that the acts of Edward Snowden were\ntreachery and offered a staunch defence of her nation’s intelligence co-operation with America.\n10 NEI Claim: The character Leslie hurts her head on the window in the ﬁlm 10 Cloverﬁeld Lane.\nEvidence: Michelle realizes Howard was right and returns his keys.\n15 Sup Claim: Halakha laws are independent of being physically present in the Land of Israel.\nEvidence: The codiﬁcation efforts that culminated in the Shulchan Aruch divide the law into four sections,\nincluding only laws that do not depend on being physically present in the Land of Israel.\n20 Sup Claim: Germany has recorded less than 74,510 cases of coronavirus , including under 830 deaths.\nEvidence: 74,508 cases have been reported with 821 deaths and approximately 16,100 recoveries.\n24 NEI Claim: For the 2015-16 school year , the undergraduate fee at USF is under $43,000.\nEvidence: Undergraduate tuition at USF is $44,040 for the 2016-17 school year.\nAG News (Gulli, 2004; Zhang et al., 2015)\n1 Business Crude Oil Rises on Speculation Cold Weather May Increase Demand Crude oil futures are headed for\ntheir biggest weekly gain in 21 months [...]\n5 Sports NHL Owner Is Criticized for Talking of Replacement Players The day before the regular season was\nsupposed to open [...]\n15 World Scotch Whisky eyes Asian and Eastern European markets (AFP) AFP - A favourite tipple among connois-\nseurs the world over, whisky is treated with almost religious reverence on the Hebridean [...]\n20 Business Arthritis drug withdrawn after trial A prescription painkiller used by more than 250,000 Australians to\ntreat arthritis has been withdrawn from sale after a clinical trial found it doubled the risk [...]\n24 Sci/Tech Airbus drops out of Microsoft appeal Aircraft builder withdraws its request to intervene in Microsoft’s\nantitrust appeal; Boeing also forgoes intervention.\nSTS-B (Cer et al., 2017)\n10 0.6 Sent. 1: A child wearing blue and white shorts is jumping in the surf.\nSent. 2: A girl wearing green twists something in her hands.\n15 2.8 Sent. 1: Saudi Arabia gets a seat at the UN Security Council\nSent. 2: Saudi Arabia rejects seat on UN Security Council\n20 4.2 Sent. 1: a small bird sitting on a branch in winter.\nSent. 2: A small bird perched on an icy branch.\n24 3.0 Sent. 1: It depends entirely on your company and your contract.\nSent. 2: It depends on your company.\nTable D.1: Number of Transformer layers used for example inputs from the task’s test sets with our Shared/Meta\nCAT with a tolerance level ofϵ= 0.1",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7710626125335693
    },
    {
      "name": "Inference",
      "score": 0.7616969347000122
    },
    {
      "name": "Transformer",
      "score": 0.5745567679405212
    },
    {
      "name": "Machine learning",
      "score": 0.4913295805454254
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47444331645965576
    },
    {
      "name": "Classifier (UML)",
      "score": 0.46257641911506653
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.4119894504547119
    },
    {
      "name": "Data mining",
      "score": 0.3204417824745178
    },
    {
      "name": "Voltage",
      "score": 0.08834207057952881
    },
    {
      "name": "Engineering",
      "score": 0.08388996124267578
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 26
}