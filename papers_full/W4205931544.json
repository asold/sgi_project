{
  "title": "Compilation and evaluation of the Spanish SatiCorpus 2021 for satire identification using linguistic features and transformers",
  "url": "https://openalex.org/W4205931544",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2244707873",
      "name": "José Antonio García-Díaz",
      "affiliations": [
        "Universidad de Murcia"
      ]
    },
    {
      "id": "https://openalex.org/A2303035916",
      "name": "RAFAEL VALENCIA-GARCÍA",
      "affiliations": [
        "Universidad de Murcia"
      ]
    },
    {
      "id": "https://openalex.org/A2244707873",
      "name": "José Antonio García-Díaz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2303035916",
      "name": "RAFAEL VALENCIA-GARCÍA",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3118996286",
    "https://openalex.org/W3102281673",
    "https://openalex.org/W3081483437",
    "https://openalex.org/W3036066542",
    "https://openalex.org/W2951732656",
    "https://openalex.org/W6600617704",
    "https://openalex.org/W2612649659",
    "https://openalex.org/W3014870144",
    "https://openalex.org/W3118913631",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2989947381",
    "https://openalex.org/W2610005984",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2140910804"
  ],
  "abstract": "Abstract Satirical content on social media is hard to distinguish from real news, misinformation, hoaxes or propaganda when there are no clues as to which medium these news were originally written in. It is important, therefore, to provide Information Retrieval systems with mechanisms to identify which results are legitimate and which ones are misleading. Our contribution for satire identification is twofold. On the one hand, we release the Spanish SatiCorpus 2021, a balanced dataset that contains satirical and non-satirical documents. On the other hand, we conduct an extensive evaluation of this dataset with linguistic features and embedding-based features. All feature sets are evaluated separately and combined using different strategies. Our best result is achieved with a combination of the linguistic features and BERT with an accuracy of 97.405%. Besides, we compare our proposal with existing datasets in Spanish regarding satire and irony.",
  "full_text": "Complex & Intelligent Systems (2022) 8:1723–1736\nhttps://doi.org/10.1007/s40747-021-00625-1\nORIGINAL ARTICLE\nCompilation and evaluation of the Spanish SatiCorpus 2021 for satire\nidentiﬁcation using linguistic features and transformers\nJosé Antonio García-Díaz 1 · Rafael Valencia-García 1\nReceived: 23 July 2021 / Accepted: 17 December 2021 / Published online: 5 January 2022\n© The Author(s) 2022\nAbstract\nSatirical content on social media is hard to distinguish from real news, misinformation, hoaxes or propaganda when there are\nno clues as to which medium these news were originally written in. It is important, therefore, to provide Information Retrieval\nsystems with mechanisms to identify which results are legitimate and which ones are misleading. Our contribution for satire\nidentiﬁcation is twofold. On the one hand, we release the Spanish SatiCorpus 2021, a balanced dataset that contains satirical\nand non-satirical documents. On the other hand, we conduct an extensive evaluation of this dataset with linguistic features\nand embedding-based features. All feature sets are evaluated separately and combined using different strategies. Our best\nresult is achieved with a combination of the linguistic features and BERT with an accuracy of 97.405%. Besides, we compare\nour proposal with existing datasets in Spanish regarding satire and irony.\nKeywords Satire identiﬁcation · Text classiﬁcation · Feature engineering · Linguistic corpus · Natural language processing\nIntroduction\nFrom ages, the spirit of satire have seek to make, through\nhumor and mockery, a constructive criticism of society. Satire\nhas the ability to make the weak strong and the strong weak.\nThere are no relevant people who are immune to the sharpen\ncomments of satyrs, that have not hesitated to parody the ﬁg-\nure of kings, pontiffs or politicians. This way, satire reveals\nthe vices of the society and raises people’s awareness con-\ncerning rooted social conventions that are taboo and difﬁcult\nto report by means other than humor. Apart from traditional\nsatirical press, the rise of social media have allowed indi-\nviduals to impersonate and parody relevant people with the\nsame critical sense that satire possesses. It is important, how-\never, to not to confuse satire with fake news and propaganda,\nwhich objective is to confuse people and inﬂuence political\nopinion with half-truths or hoaxes.\nB Rafael V alencia-García\nvalencia@um.es\nJosé Antonio García-Díaz\njoseantonio.garcia8@um.es\n1 Facultad de Informática, Universidad de Murcia, Campus de\nEspinardo 30100, Murcia, Spain\nAccording to the Poe’s law, 1 popular on the Internet cul-\nture, parody and extreme views are impossible to discern\nwithout clear indicators of the writer’s intentions. This fact\noccurs very frequently when satirical, real news, and hoaxes\nare shared in social media by users or even by other news\nmedia who get confused and take satirical news and hoaxes\nas real. If satire is hard to identify for humans, their auto-\nmatic identiﬁcation is a even more challenging task. Satyrs\nrely on ﬁgurative language, in which words move away from\ntheir conventional meaning. Speciﬁcally, satire makes use of\nsarcasm and irony to play with words to hide criticism. Figu-\nrative language hinders Information Retrieval (IR) tasks that\nmay misinterpret satire as truthful information. Moreover,\nthere is a need to improve resources related to languages\nother than English in order to improve automatic satire iden-\ntiﬁcation.\nAs far as our knowledge goes, automatic satire identiﬁca-\ntion in Spanish was ﬁrst addressed by [ 3] and then by [ 21],\nin which the authors proposed, respectively, novel datasets\ncontaining tweets from media sites and build automatic clas-\nsiﬁers by means of linguistic and statistical features together\nwith traditional machine learning approaches such as Ran-\ndom Forest and Support V ector Machines. Other researches\nhave focused on identifying linguistic phenomena related to\n1 https://www.urbandictionary.com/deﬁne.php?term=Poe’s Law.\n123\n1724 Complex & Intelligent Systems (2022) 8:1723–1736\nsatire, such as sarcasm or irony and recently new resources\nhave been published such as IroSV A 2019 [ 18].\nRegarding the techniques involved on satire identiﬁcation,\nthe authors of [ 2] compared traditional machine-learning\nwith deep-learning classiﬁers in order to outperform the\nresults proposed by [ 21]. They discover, however, that mod-\nern Spanish pre-trained word embeddings do not outperform\nthe results achieved by classical term-counting features. This\nfact suggests that the presence of certain keywords inﬂuences\ntoo much in the classiﬁcation, which may be indicative of\ncertain biases in the corpus. Accordingly, we examined the\nexisting datasets regarding satire identiﬁcation and discover\nsome biases, most of them related to the distant supervi-\nsion techniques employed and the limited number of different\naccounts. In order to address these issues, our contribution\nis two fold. On the one hand, we present the Spanish Sati-\nCorpus 2021, a balanced dataset for identifying satire with\n18,207 satiric utterances and, on the other, we evaluate this\ndataset with several deep-learning architectures and feature\nsets, including linguistic features and contextual and non-\ncontextual word and sentence embeddings.\nThe rest of the manuscript is organised as follows: First,\nSect. 2 summarises research focused on satire identiﬁcation\nbut including also research concerning automatic sarcasm\nand irony detection, due to their close relationship with satire.\nNext, Sect. 3 presents and describes the compilation process\nof the Spanish SatiCorpus 2021. In Sect. 4, the reader can\nﬁnd details as regard of the pipeline used for evaluating the\ncorpus, including the feature sets and the deep-learning archi-\ntectures involved. The results achieved by each model, and its\nconsequently analysis, are described, respectively, in Sects.\n5 and 6. Finally, the conclusions of this research and some\npromising research directions are listed in Sect. 7.\nBackground information\nFirst, it is important to remark the similitude and differences\nbetween verbal irony, sarcasm, and satire because these terms\nare sometimes misinterpreted. Satire and sarcasm are both\nforms of expression. However, whereas satire has moralising\npurpose in which vices and abuses are caricatured, sarcasm is\nintended to be hurtful and scathing and it is used as a rhetor-\nical device to subdue an adversary. Both, satire and sarcasm,\nmake use of ﬁgurative language to their end. Examples of\nﬁgurative language are understatements, similes, metaphors,\npersoniﬁcations, or idioms among others. Of these literary\ndevices, the verbal irony stands out. V erbal irony consists in\nthat the writer intends to be understood with expressions that\ncontrasts with the literal meaning of what he says. The reader\ncan ﬁnd a detail overview regarding ﬁgurative language, its\ndiscriminant features, and techniques employed for its iden-\ntiﬁcation in [ 20].\nRegarding sarcasm identiﬁcation, in [ 17] the authors\npresent a framework for sarcasm identiﬁcation based on\nbidirectional recurrent neural networks and term-weighted\ntrigrams, that they refer as inverse gravity moment. The\nobjective for the term weight strategy is to boost critical\nwords and joint words at the same time that word order is\npreserved. During their experimentation, the authors com-\npare their framework with models based on word embeddings\nas well as supervised and unsupervised weighting functions,\nsuch as term frequency, odds ratio, balanced distributional\nconcentration or regularised entropy, just to name a few. This\nframework was evaluated with three sarcasm corpus. The ﬁrst\none is based on Twitter, annotated using distant supervision.\nThe second one is based on the Internet Argument Corpus,\nand the third one is created from extracting news headlines\nfrom satirical news sites such as The Onion.\n2\nIn [ 12], the authors proposed a BERT-based model called\nViLBERT and evaluate it against a dataset composed of\nimages and headlines from satirical web sites. This study\nwas conducted from a multi-modal perspective as it incor-\nporates the analysis of images in satirical news that contains\nimages. They observed that these images were created using\nphoto manipulation techniques to create ﬁctional and unreal\nscenarios. Their results suggests that multi-modal perspec-\ntive outperforms text-based solutions.\nThe identiﬁcation of satire, verbal irony, and sarcasm\nhave been explored in other languages apart from English.\nBesides, other researches have focused on multi-lingual\napproaches. The work presented in [ 9], for instance, focuses\non French, English and Arabic for irony detection. It is\nworth mentioning that the identiﬁcation of complex language\nphenomena, such as irony, is heavily cultural dependant.\nTherefore, the authors compare multicultural languages\nIndo-European languages with less culturally close lan-\nguages. Their proposal indicate that monolingual-based\nmodels trained from multilingual word representation are\nbeneﬁcial for irony detection in situations in which anno-\ntated datasets are not available.\nIt is possible to ﬁnd novel research and datasets concern-\ning satire, sarcasm, and irony in other languages apart from\nEnglish, such as Turkish [ 16], Bangla [ 1], or Persian [ 10].\nAs far as our knowledge goes, the principal research focused\non satire identiﬁcation in Spanish was performed ﬁrst by\nBarbieri [ 3] and next by Salas-Zárate [ 21], in which both\nauthors evaluated satire identiﬁcation from datasets based\non satirical and non-satirical headlines. The approach fol-\nlowed by [ 21] was based on linguistic features extracted\nfrom Linguistic Inquery Word Count (LIWC) [ 25] whereas\nthe approach followed by [ 3] compared statistical features\nand manually crafted linguistic features. It is worth noting\nthat the dataset from [ 21] were revisited by [ 2], in which the\n2 https://www.theonion.com/.\n123\nComplex & Intelligent Systems (2022) 8:1723–1736 1725\nauthors evaluated other features based on count-term features\nwith traditional machine learning classiﬁers and word and\nsentence embeddings from novel Spanish pretrained word\nembeddings. However, their results show an increment in\nthe accuracy over the Mexican Spanish but not in the Euro-\npean Spanish dataset by employing term-counting features.\nThe non-contextual features achieved lower accuracy in the\nEuropean Spanish dataset compared with approaches based\non linguistic features or term-counting features. The reason\nof this was that the Mexican Spanish dataset was slightly\nbiased because, on the one hand, there were fewer different\nTwitter accounts so classiﬁers can focus on speciﬁc linguis-\ntic patterns from the community managers and, on the other,\nas the editorial line of each media is focused on particular\ntopics, these topics biased automatic classiﬁers. The prob-\nlems identiﬁed for the compilation of dataset related to satire\nidentiﬁcation are taken into account for the compilation of\nthe Spanish SatiCorpus 2021, which is described below.\nCorpus\nWe rely on the UMUCorpusClassiﬁer tool [ 6] for extract-\ning satirical and non-satirical texts from Twitter. We named\nthis novel dataset as the Spanish SatiCorpus 2021, a bal-\nanced dataset consisted into satirical and non-satirical tweets\nmostly compiled from news sites (from Europe and Latin\nAmerica). We select four satirical news sites from Spain,\nnamely, El Mundo Today,\n3 El Jueves, 4 El Intermedio, 5 and\nRevista Mogolia.6 Three from Mexico, namely, El Deforma, 7\nEl Dizque, 8 and El Univerfail. 9 Finally, there is one more\nfrom V enezuela: and the V enezuelan medium El Chigüire\nBipolar.\n10 In addition, to prevent bias from satirical news\nsites, we added some accounts from Twitter used for imper-\nsonate and satirise real political actors and relevant people.\nWe use distant supervision for corpus annotation, follow-\ning a similar criteria as described in [ 21] and [ 3], assuming\nthat all documents written for satiric news media are satiric.\nHowever, we observe that there are several tweets from the\nsatirical accounts used for promoting events. In order to avoid\nthe problems previously identiﬁed in other satirical datasets,\nwe conduct a sanity checking in order to discard tweets that\n3 https://www.elmundotoday.c.om/.\n4 https://www.eljueves.es/.\n5 https://twitter.com/El_Intermedio.\n6 https://www.revistamongolia.com/.\n7 https://eldeforma.com/.\n8 https://www.eldizque.com/.\n9 http://www.eluniverfail.com/\n10 https://www.elchiguirebipolar.net/\nTable 1 Corpus distribution per label and split\nLabel Train Development Test Total\nsatire 10923 3642 3642 18207\nnon-satire 10923 3642 3642 18207\ntotal 21846 7284 7284 36414\nstarted from some words such as entrevista11 or words related\nto merchandising. We also remove some clues of the texts, as\nwe found that some media uses hooks such as “INCREIBLE\nPERO CIERTO”.\n12 We also remove those tweets with less\nthan 15 words\nOne of the problems identiﬁed in the previous datasets is\nthat satirical and non-satirical news may not focus on the\nsame events, thus introducing some bias when automatic\nclassiﬁers can discern among satiric and non-satiric utter-\nances just looking at what the news are about. To mitigate\nthis inconvenient we select the non-satirical tweets in base\nof the satirical ones. For this, we look for the most similar\ndocuments for each satiric document using Text Similarity\nbased on TF-IDF and calculating their cosine distance. We\nbuild a matrix in which the rows are the indexes of the satiric\ndocuments and the columns the indexes of the non-satiric\ndocuments and each component of the matrix represents\ntheir distance. Then, we apply an iterative process to select\nthe most similar satiric and non-satiric documents. Once\nselected, we remove the row and column from the matrix\nand repeat this process until there are no rows left, that is, we\nﬁnd a match for each satirical document.\nThe ﬁrst release of the Spanish SatiCorpus 2021 con-\ntains 18,207 satiric and 18,207 non-satiric tweets. This\ndataset contains tweets between March, 2018 to June,\n2021. We divided the dataset into three splits, namely,\ntrain, development, and testing, in a ratio of 60-20-20. The\ndataset is available for the research community at https://\npln.inf.um.es/corpora/satire/spanish-saticorpus-2021.zip .\nHowever, according to the Twitter guidelines,\n13 we only\nshare the identiﬁers of the tweets as well as the labels and\nthe split (train, development, test) to preserve users’ rights\nover their content. Table 1 contains the statistics per corpus\nregarding label and split.\nSome satiric and non satiric examples from the dataset are\ndepicted in Fig. 1. In the ﬁrst row, we can compare two news\nfrom Mexico. The non-satirical one comments that during\nAsh Wednesday (a catholic holiday), the ash was distributed\nin individual bags in order to avoid agglomerations due to\nthe COVID pandemic. However, the satirical counter-news\n11 In English: interview.\n12 In English: Incredible, but true.\n13 https://developer.twitter.com/en/developer-terms/agreement-and-\npolicy.\n123\n1726 Complex & Intelligent Systems (2022) 8:1723–1736\nwarns citizens not to confuse these sachets with similar ones\nwith inhaled drugs. As we can observe, forcing the datasets\nto contains similar satiric and non satirical tweets prevents\nmachine learning models to overﬁt regarding certain key-\nwords and topics related to the domain. In the second row,\nboth satirical and non-satirical news are related to an event\nthat was debated in the congress of the deputies in Spain, in\nrelation to the facilitating mental health treatments in Spain.\nAn open microphone caused that the comment “ Go to the\ndoctor” from a politician was heard throughout the hemicy-\ncle. This fact was criticised in the press because it downplays\nmental illness issues. However, from a satirical point of view,\nthe news looks like this politician has been a hero, and that\nhis comment saved the life of the of the politician who was\nexposing at that time, for something that had nothing to do\nwith mental health.\nMaterials and methods\nIn order to evaluate the Spanish SatiCorpus 2021, we con-\nducted a deep-learning pipeline to evaluate it with different\nfeature sets and deep-learning architectures and so, evaluate\nwhat the strong features for automatic satire identiﬁcation\nare. Figure 2 depicts the architecture of our proposal. In\na nutshell, it can be described as follows. First, we apply\na text pre-processing stage to clean the dataset (see Sect.\n4.1). Second, we divided the dataset into training, valida-\ntion, and testing in a ratio of 60-20-20 (see Sect. 3). For\nthis, we use a stratiﬁed split to ensure the ratio of satiric and\nnon-satiric documents remains balanced. Third, we conduct\na feature extraction stage to obtain the linguistic features\nand the embedding based features (see Sect. 4.2). Fourth, we\nperform an hyperparameter optimisation stage to evaluate\nseveral machine learning models that combines each feature\nset separately and combined (see Sect. 4.3) and, ﬁnally, the\nbest models for each feature set are evaluated with the test\ndataset.\nText pre-processing\nIn this stage we remove some parts of the texts that could bias\nthe deep-learning classiﬁers. First, we remove social media\njargon such as hyperlinks, hashtags, or mentions. Besides, we\nreplace digits with a token. Next, to work with features that\nare based on lexicons, we remove expressive lengthening,\nthat it is a linguistic device used to emphasis some words\nand we ﬁx misspellings with the PSPELL tool.\nIt is important to remark that we maintain different ver-\nsions of the documents to extract some of the linguistic\nfeatures. For example, to extract features related to Part-of-\nSpeech features and named entities we keep a normalised\nversion but in which the case was preserved.\nFeature extraction\nIn this section we describe the feature sets evaluated. First,\nwe evaluate linguistic features and different kind of embed-\ndings, including pretrained non-contextual word and sen-\ntence embeddings from different models as well as pretrained\ncontextual sentence and word embeddings from different ver-\nsions of BERT.\nThe linguistic features (LF) are extracted using the\nUMUTextStats tool [7,8]. This tool is inspired in LIWC [ 25],\nbut designed to the Spanish in mind. It captures a total of 365\ndifferent linguistic features organised in the following cate-\ngories:\n– (COR) Correction and style : This category checks the\ncorrect usage of writing communication. In particular, it\ndistinguish among (1) orthographic errors, related to the\nnumber of misspelled words or the bad use of Spanish\naccentuation; (2) stylistics errors, related to the presence\nof sentences that starts with numbers or with the same\nword; and (3) performance, looking for duplicated words,\nthe usage of dot after exclamation or question signs or the\npresence of common errors and redundant expressions.\n– (PHO) Phonetics : This category is related to expressive\nlengthening, which consists in the elongation of certain\nletters with an emphasising purpose. As Spanish lan-\nguage contains words in which the same letter appears\ncontiguous twice such as coordinar\n14 we consider only\nthe repetition of three or more letters and, in the case of\nvowels, whether or not they have an accent.\n– (MOR) Morphosyntax : This category includes features\nthat represent how words and sentences are composed.\nSpeciﬁcally, there are features that capture (1) gram-\nmatical gender, discerning among masculine, feminine,\nand neutral words; (2) number, to discern among plu-\nral and singular; (3) afﬁxes, to capture a ﬁne-grained\nvariety of sufﬁxes (nominals, adjectivizers, verbalizers,\nadverbializers, augmentative, diminutives, or despective)\nand preﬁxes. There are features also organised by PoS\ncategories, such as nouns, verbs, adjectives, adverbs,\ndeterminers, pronouns, prepositions, conjunctions, or\ninterjections among others. To capture these features we\napply a mixed approach based on Stanza [ 22], to cap-\nture the main categories, and lexicons that capture ﬁne\ngrained subcategories for capturing sub-types.\n– (SEM) Semantics . This category captures four seman-\ntic features: (1) onomatopoeia, to identify words that\nare composed as regards of the sound they produce. For\nexample, the word “ achís” in Spanish makes reference\nto the sound that it is produced when you sneeze; (2)\neuphemism, that are softer versions of expressions that\n14 In English: to coordinate.\n123\nComplex & Intelligent Systems (2022) 8:1723–1736 1727\nFig. 1 Spanish SatiCorpus 2021\nexamples\ncould be considered too rude in some contexts; (3) dys-\nphemism, that are vulgar words used for replacing other\nconsidered more neutral; and (4) synecdoche, that are a\ntype of literary trope used for representing a part as a\nwhole. For example, in “Se quedó con cuatro bocas que\nalimentar”,\n15 the statement “cuatro bocas” refers to four\nchildren.\n– (PRA) Pragmatics . This category captures the presence\nof ﬁgurative language devices, distinguish among under-\nstatements, hyperboles, idiomatic expressions, rhetorical\nquestions, verbal irony, metaphors or similes among oth-\ners. It also contains several linguistic features to see how\ndifference sentences are connected by means of discourse\nmarkers. In addition, there are features for capturing some\ntypical Spanish courtesy forms.\n– (STY) Stylometry : This category captures a (1) wide\nvariety of punctuation symbols, (2) corpus statistics, such\nas the Token-Type Ratios (TTR), as well as (3) other\nmetrics related to the number of words, syllables or sen-\ntences.\n– (LEC) Lexical : The aim of this category is to capture\nthe topics in the text. For this, we analyse from abstract\ntopics (analytical thinking, achievement, friendship, reli-\ngion, or certainty among others) to general topics, such\nas locations, organisations, animals, clothes, food, and a\nlist of professions.\n– (PLI) Psycho-linguistic processes : This category is\nrelated to lexicons and emojis related to sentiments (pos-\nitive, negative) and emotions (anger, sadness, anxiety).\n– (REG) Register : This category captures how people use\nlanguage to communicate, as it could be the presence\nof informal or cultured language. We also capture topics\nrelated to offensive speech.\n– (SOC) Social media jargon : This category captures fea-\ntures concerning clues that reveals the speaker domains\n15 She was left alone with four mouths to feed.\nthe social media jargon as it can be speciﬁc terminology\nused on social media or the usage of mechanisms such\nas hyperlinks, mentions or emojis.\nWith respect the embeddings, we evaluate non-contextual\nand contextual word and sentence embeddings. Embeddings\nare dense vectors that represent words within a latent space.\nThese embeddings are usually learned from unsupervised\ngeneric tasks, such as next-word prediction. Word embed-\ndings cluster together words that are semantically similar at\nthe same time they maintain the distance with other clusters\nof words. Their main drawback, however, is that traditional\n(non-contextual) embeddings are not aware about polysemy,\nso the same word have the same representation regardless\ntheir signiﬁcance in a sentence. Contextual words embed-\ndings solve this drawback generating the embeddings by\ntaking into account the context of a word, that is, the words\nthat are next to it. Contextual word embeddings and models\nbased on transformers have meant a great qualitative leap in\nmany NLP tasks, although they are computationally more\ndemanding. This fact is partially mitigated with the usage\nof sentence embeddings, in which instead of words, sen-\ntences are encoded within the latent space. There are different\nstrategies to obtain sentence embeddings, generally taking\naveraging from the word embeddings. In addition, another of\nthe key-advantages of embeddings, regardless if they are con-\ntextual or not, is that they can be learn from generic datasets,\nwhich provides two major beneﬁts. One the one hand, pre-\ntrained embeddings already conveys general meaning they\nconverge faster and, on the other, they are a form of trans-\nfer learning, in which the embeddings have been learnt with\nconcepts that could not be in the domain.\nThe non-contextual word embeddings (WE) evalu-\nated in this work are based on pretrained models based on\nword2vec [14], fastText [15], and gloV e [19]. It is worth not-\ning that word embeddings allow to explore speciﬁc types\nof neural networks architectures, such as convolutional and\n123\n1728 Complex & Intelligent Systems (2022) 8:1723–1736\nFig. 2 System architecture\nTraining\n(60%)\nTesting\n(20%)\nSplitter\nSpanish\nSatiCorpus 2021\n(36,414 tweets)\nValidation\n(20%)\nBERT embeddings\n(BF)\nWord Embeddings\n(WE)\nLinguistic Features\n(LF)\nSentence Embeddings\n(SE)\nFinal report\nText\nPreprocessing\nHyper-parameter\ntunning\nMulti-layer\nPerceptron\nMLP\nConvolutional\nNeural Network\nCNN\nRecurrent\nNeural Network\nRNN\nModel selection\n(Macro F1-score)\nFeature\nextraction\nrecurrent neural networks, that are capable for taking proﬁt\nas regards of the space and temporal dimension of language.\nThat is, convolutional neural networks can generate high-\norder features by clustering joint words whose signiﬁcance\ndiffers from the one it can be obtained individually, as it can\nhappen with the words New and York. Recurrent neural net-\nworks, on the other hand, explore the temporal dimension, so\nthey take into account the order of the words. Particularly, we\nevaluate two bidirectional recurrent neural networks based\non Long-Short Term Memory (LSTM) and Gated Recurrent\nUnits (GRU). The non-contextual sentence embeddings\n(SE) are extracted from FastText [ 11], which Spanish model\nis trained from Wikipedia and CommonCrawl.\nThe contextual word embeddings (BETO) are evaluated\nfrom the Spanish version of BERT [ 4]. These contex-\ntual embeddings are focused on Spanish. In addition, we\nhave compared their reliability with multi-lingual embed-\ndings from BERT (mBERT) [5] and its distilled version\n(dmBERT) [24]. For all these embeddings we rely on the\nHuggingFace transformers library to ﬁne tune the model with\nthe Spanish SatiCorpus 2021. However, these kind of embed-\ndings are difﬁcult to combine with other feature sets as they\nare time consuming. Therefore, for the contextual sentence\nembeddings (BF) we extracted the ﬁxed representation of\nthe [CLS] token as suggested in [ 23] and we use this repre-\nsentation to combine the contextual embeddings more easily\n123\nComplex & Intelligent Systems (2022) 8:1723–1736 1729\nwith the rest of the feature sets. Our results suggest that the\nperformance of contextual sentence and word embeddings\nare similar in terms of precision, recall, and accuracy, with\nthis and other datasets.\nAfter the feature sets were extracted, we conduct a feature\nselection and a feature normalisation stage. First, we apply\na MinMax scaler to the linguistic features as they contains\nfeatures in different scales with raw counts and percentages.\nNext, we select the best features using Information Gain,\ndiscarding those features that belong to the last quartile.\nHyper-parameter tuning\nAn hyper-optimisation stage is conducted in order to evaluate\nthe conﬁguration of several neural networks architectures for\nsatire identiﬁcation. Our strategy consist into train a indepen-\ndent neural network per feature set. We rank the models by\naccuracy. The hyperparameters evaluated are: (1) the number\nof hidden layers as well as the number of neurons per layer;\n(2) the dropout rate, to avoid overﬁtting; (3) the activation\nfunction; (4) the learning rate and the usage of a time-based\ndecay scheduler; and (5) the batch size.\nThe best architecture and hyperparameters for feature set\ncan be seen in Table 2. It can be appreciated that the best\naccuracy is always obtained with shallow neural networks,\nthat is, neural network with one of two hidden layers maxi-\nmum. We observe this fact both in individual feature sets (LF,\nSE, WE, BF), or when combined in groups of two, three or\nfour within the same neural network. However, the number\nof neurons per layer vary achieving better results with larger\nnumber of neurons in models in which the LF are present.\nWe notice that the number of neurons with WE is small, with\nonly 2 neurons. A similar ﬁnding results when combining\nSE and BF, with 4 neurons. Concerning the dropout rate, we\nobserve than only four experiments provided better results\nwithout it: WE, BF with SE, and LF, SE, WE, and BF. With\nrespect to the activation function, ReLu is the one that appears\nmore often, regardless when the feature sets are in isolation\nor combined.\nResults\nFor each model we evaluate the precision (see Eq. 1), recall\n(see Eq. 2), and F1-score (see Eq. 3) of the satirical and non-\nsatirical class. In addition, we include the macro-averaged\nscores of both labels together with the accuracy (see Eq. 4)\nto compare the overall performance of each model.\nPrecision = TP /(TP + FP ) (1)\nRecall = TP /(TP + FN ) (2)\nF1_Measure = 2 × Precision × Recall\nPrecision + Recall (3)\nAccuracy = TP + TN /(TP + TN + FP + FN ) (4)\nTable 3 depicts the results for each feature set. It can be\nobserved that the best result is achieved with BF (96.78%\nof accuracy), which accuracy outperforms largely the rest of\nthe feature sets: LF (85.14% of accuracy), WE (84.25% of\naccuracy), and SE (82.04% of accuracy). We can observe\nthat LF are good indicators for satire identiﬁcation, improv-\ning the results from SE and WE. The precision between\nsatiric and non-satiric sentences is similar with all feature\nsets, although we can observe in SE the highest difference,\nobserving larger precision for satiric documents. A similar\nbut slighter behaviour is appreciated in case of multi-lingual\nBERT. When comparing the results achieved with different\nversion of transformers (BETO, multi-lingual BERT and dis-\ntilled multi-lingual BERT), learning the embeddings from an\nspeciﬁc language rather than a multilingual corpus is bene-\nﬁcial, as there is difference of accuracy of 6.37% between\nBETO and mBERT, and a difference of accuracy of 13.042%\nbetween BETO and distilled mBERT. This ﬁnding is inline\nwith other research that compare embeddings learnt from\nmonolingual or multilingual data sources [ 13,26]. The accu-\nracy achieved with BF is similar to the achieved with the\npretrained model from BETO (96.897% of accuracy). Thus,\nthe ﬁxed vectors from the [CLS] token extracted for the\nﬁne-tuned model contains the relevant information of the\nsentence without sacriﬁcing performance. Moreover, we can\nobserve from Table 2, that the best results were achieved\nwith a shallow neural network composed of one hidden layer\nof 48 neurons. However, this ﬁxed sentence representation\neases their combination with other feature sets improving the\nperformance.\nNext, Table 4 shows the results achieved by combining\ndifferent feature sets in the same neural network, applying a\nknowledge integration strategy. LF improves the results when\ncombined with SE (89.044% of accuracy), WE (87.740%\nof accuracy) and BF (97.405% of accuracy). This ﬁnd-\ning indicates that the linguistic features contains additional\ninformation that the one than can be extracted from the\nembeddings. Other combinations based only in embeddings,\nhowever, do not improve their results signiﬁcantly, as it can\nbe observed from SE and WE (84.308%) and SE with BF\n(96.870%). It is worth mentioning that accuracy achieved\nwith the combination of LF, SE, and BF (97.32% of accu-\nracy) is lower to the one obtained from combining LF and\nBF. The combination of all features (LF, SE, WE, and BF)\nis also slower (97.268%) but it achieves very good precision\nand recall with both satiric and non-satiric documents.\nNext, we evaluate the combination of the models by means\nof ensembles. These results are shown in Table 5. Speciﬁ-\ncally, three ensemble learning strategies were evaluated: (1)\nhighest probability, (2) weighted mode, and (3) the average\nof probabilities. The highest probability consist into inspect\n123\n1730 Complex & Intelligent Systems (2022) 8:1723–1736\nTable 2 Hyperparameter\nevaluation over the validation\ndataset per feature set in\nisolation or combined within the\nsame neural network\nFeatures Architecture Shape \\# of layers \\# of neurons Dropout lr Activation\nlf Dense Brick 2 48 0.2 1 e − 2S i g m o i d\nse Dense Brick 2 256 0.1 1 e − 3R e L u\nwe Dense Brick 1 2 None 1 e − 3R e L u\nbf Dense Brick 1 48 0.3 1 e − 3R e L u\nlf-se dense Brick 1 512 0.3 1 e − 2R e L u\nlf-we Dense Brick 1 128 0.2 1 e − 2R e L u\nlf-bf Dense Brick 2 128 0.3 1 e − 2T a n h\nse-we Dense Brick 1 64 0.2 1 e − 3 None\nse-bf Dense Brick 1 4 None 1 e − 2 None\nwe-bf Dense Brick 1 64 None 1 e − 3R e L u\nlf-se-bf Dense Brick 2 48 0.2 1 e − 2T a n h\nlf-se-we Dense Brick 2 256 False 1 e − 3R e L u\nlf-we-bf Dense Brick 2 16 0.3 1 e − 2R e L u\nlf-se-we-bf Dense Brick 2 256 False 1 e − 3R e L u\nthe probability reported by each model that one text belongs\nto the satirical or non satirical category and select the high-\nest one. As we can observe, this strategy reports the highest\nprecision for the non-satire label and the highest recall for\nthe satire class. Therefore, this strategy can be suited in sys-\ntems that focus on one of these metrics. However, both the\nrecall of the non-satire class and the precision of the satire\nclass are limited. Next, the weighted mode strategy consists\nin that each model emits a vote for determining whether a\ntext is satiric or not. This strategy is also known as soft vot-\ning. As not all classiﬁers have the same, we ranked them\naccording to their results with the validation split, so the vote\nfor BF is slightly superior to the rest. Note that the weighted\nmode strategy achieves the overall best result compared with\nthe rest of strategies evaluated, reporting a macro weighted\nF1-score of 95.510%, performing well regardless of the met-\nric evaluated (precision or recall) and regardless the label\n(satiric or non satiric). The last strategy evaluated consisted\nin to average the predictions of all classiﬁers and so emit\nthe ﬁnal vote. This strategy achieves slightly lower results\nthan the weighted mode, with a macro weighted F1-score of\n93.712125% but with similar performance in both labels and\nmetrics.\nAnalysis\nThe Spanish SatiCorpus 2021 has been evaluated applying\ndifferent feature sets in isolation (see Table 3) or combined\nby means of ensembles or within the same neural network\n(see Tables 4 and 5, respectively). These results are all very\ncompetitive. We observe that there is a signiﬁcant quality\nleap with contextual word embeddings, regardless if they\nare sentence or word-based. Moreover, contextual embed-\ndings based on BETO [ 4] improves signiﬁcantly the results\nachieved by embeddings learnt from multi-lingual corpus\n(mBERT). We also observe that the LF in isolation achieved\nsuperior accuracy than the models based on non-contextual\nembeddings, improving SE (85.145% of accuracy with LF vs\n82.042% of accuracy with SE) and WE (85.145% of accuracy\nwith LF vs 84.253% of accuracy with WE). In addition, a rel-\nevant ﬁnding is that the combination of the LF with other kind\nof ensembles results in more reliable systems. However, the\ncombination of non-contextual ensembles does not outper-\nform the results achieved separately. We observe a signiﬁcant\nincrement of the combination of the LF with SE (89.044%\nof accuracy) and with WE (87.740% of accuracy). However,\nthe increment with BF was less signiﬁcant (from 96.787% to\n97.405% of accuracy).\nIt is worth mentioning that the training of neural networks\nadopting the knowledge integration strategy is a time-eating\ntask. First of all, because we perform a extra hyperparame-\nters optimisation process for these neural networks instead\nof combining the best architectures individually. Secondly,\nbecause the neural networks that rely on word embeddings\n(as opposite of sentence embeddings) requires millions of\nparameters rather than the thousands evaluated with sen-\ntence embeddings. In contrast, the ensemble learning strategy\nis faster, as we combine the predictions of already existing\nmodels, at the same time their results are only slightly lower\n(from an accuracy of 97.323% combining LF, SE, and BF to\n95.511%, with an ensemble of LF, SE, WE, and BF applying\nthe weighted mode.\nAs the linguistic features provide some kind of inter-\npretability, we calculate the contribution of each linguistic\ncategory separately. This information is shown in Table 6.W e\ncan observe than stylometry (STY) is the linguistic category\nthat provides major reliability regarding precision and recall\n123\nComplex & Intelligent Systems (2022) 8:1723–1736 1731\nTable 3 Precision, recall, F1-score of satiric and non-satiric labels by\nfeature set separately\nFeature set Precision Recall F1-score Accuracy\n(LF) Linguistic features\nNon-satire 86.240 83.635 84.918 –\nSatire 84.115 86.656 85.367 –\nMacro-avg 85.178 85.146 85.142 85.146\n(SE) Non-contextual sentence embeddings\nNon-satire 79.831 85.750 82.685 –\nSatire 84.609 78.336 81.352 –\nMacro-avg 82.220 82.043 82.018 82.043\n(WE) Non-contextual word embeddings\nNon-satire 83.983 84.651 84.316 –\nSatire 84.528 83.855 84.190 –\nMacro-avg 84.255 84.253 84.253 84.253\n(BETO) Contextual words embeddings\nNon-satire 97.550 96.211 96.876 –\nSatire 96.262 97.584 96.918 –\nMacro-avg 96.906 96.897 96.897 96.897\n(mBERT) Contextual words embeddings\nNon-satire 88.398 93.300 90.783 –\nSatire 92.907 87.754 90.257 –\nMacro-avg 90.652 90.527 90.520 90.527\n(dmBERT) Contextual words embeddings\nNon-satire 85.411 81.658 83.492 –\nSatire 82.430 86.052 84.202 –\nMacro-avg 83.920 83.855 83.847 83.855\n(BF) Contextual sentence embeddings\nNon-satire 97.228 96.321 96.772 –\nSatire 96.355 97.254 96.802 –\nMacro avg 96.792 96.787 96.787 96.787\nMacro-averaged precision, recall, F1-score, and accuracy of the overall\nresult\n(76.3130% of precision, 76.126% of recall). We observe that\nthe feature categories that contains fewer linguistic features,\nthat are, phonetics (PHO) and semantics (SEM), are the ones\nthat achieved lower results, with an 34.115% and a 34.366%\nof F1-score respectively. In fact, when observe the results\nachieved per class, it can be noticed that the recall of these\nmodels were biased to the non satirical category predicting\nthis feature almost a 100% of the times. The reason of this\nis that the LF does not appear very often in the dataset. For\nexample, only a small proportion of the documents make use\nof elongation (PHO), so any model is not capable of making\nreliable predictions with only this information.\nAs we can observe, the results achieved with the LF\ncategories merged outperforms largely the results achieved\nseparately, achieving an accuracy of 85.142% (see Table 3).\nThis fact indicates that the linguistic categories are comple-\nTable 4 Precision, recall, F1-score of satiric and non-satiric labels by\nfeature sets combined\nLabel Precision Recall F1-score Accuracy\n(LF + SE)\nNon-satire 89.217 88.825 89.020 –\nSatire 88.874 89.264 89.068 –\nMacro-avg 89.045 89.044 89.0444 89.044\n(LF + WE)\nNon-satire 89.531 85.475 87.456 –\nSatire 86.105 90.005 88.012 –\nMacro-avg 87.818 87.740 87.734 87.740\n(LF + BF)\nNon-satire 97.945 96.842 97.391 –\nSatire 96.878 97.968 97.420 –\nMacro-avg 97.411 97.405 97.405 97.405\n(SE + WE)\nNon-satire 84.545 83.965 84.254 –\nSatire 84.074 84.651 84.362 –\nMacro-avg 84.310 84.308 84.308 84.308\n(SE + BF)\nNon-satire 97.549 96.156 96.847 –\nSatire 96.210 97.584 96.892 –\nMacro-avg 96.879 96.870 96.870 96.870\n(WE + BF)\nNon-satire 97.347 96.733 97.039 –\nSatire 96.753 97.364 97.058 –\nMacro-avg 97.050 97.048 97.048 97.048\n(LF + SE + WE)\nNon-satire 88.061 88.303 88.182 –\nSatire 88.271 88.029 88.150 –\nMacro-avg 88.166 88.166 88.166 88.166\n(LF + WE + BF)\nNon-satire 97.108 96.815 96.961 –\nSatire 96.825 97.117 96.971 –\nMacro-avg 96.966 96.966 96.966 96.966\n(LF + SE + BF)\nNon-satire 97.782 96.842 97.310 –\nSatire 96.872 97.803 97.336 –\nMacro-avg 97.327 97.323 97.323 97.323\n(LF + SE + WE + BF)\nNon-satire 97.281 97.254 97.268 –\nSatire 97.255 97.282 97.268 –\nMacro-avg 97.268 97.268 97.268 97.268\nMacro-averaged precision, recall, F1-score, and accuracy of the overall\nresult\nmentary. For more clarity, we calculate the average of all\nfeatures organised by linguistic category and label (satire\nand non-satire) in order to observe this differences in a polar\nchart (see Fig. 3). It draws our attention than the average\n123\n1732 Complex & Intelligent Systems (2022) 8:1723–1736\nTable 5 Precision, recall, F1-score of satiric and non-satiric labels by\nensemble learning strategy for LF, SE, WE, and BF\nLabel Precision Recall F1-score Accuracy\nHighest probability\nNon-satire 99.752 66.310 79.664 –\nSatire 74.769 99.835 85.503 –\nMacro-avg 87.260 83.072 82.583 83.072\nWeighted mode\nNon-satire 95.623 95.387 95.505 –\nSatire 95.399 95.634 95.516 –\nMacro-avg 95.511 95.511 95.511 95.511\nProbability average\nNon-satire 93.332 94.152 93.740 –\nSatire 94.100 93.273 93.685 –\nMacro-avg 93.716 93.712 93.712 93.712\nMacro-averaged precision, recall, F1-score, and accuracy of the overall\nresult\nof each linguistic category is similar regardless the class,\nwith the notable exception of correction and style (COR),\nin which non-satiric documents are most relevant. As it can\nbe observed, there are no data regarding semantics (SEM),\nphonetics (PHO), and pragmatics (PRA). Regarding psycho-\nlinguistic process (PLI) we observe no signiﬁcant difference\nand only small difference concerning stylometry (STY). With\nrespect to the usage of social media jargon (SOC) and regis-\nter (REG), there are differences among satiric and non-satiric\nutterances although these categories do not appear regularly\nin the dataset.\nNext, we calculate the contribution of each linguistic fea-\nture to the class by obtaining the mutual information. We\nselect the top most discriminatory features and we observed\nif they appear more often in satirical or non-satirical docu-\nments. This information is depicted in Fig. 4. We can observe\nthan features related to readability (STY), that involves the\nnumber of words, syllables and sentences are strong discrim-\ninatory features. However, in general, these features have a\nsimilar representation for satiric and non-satiric documents.\nSurprisingly, from correction and style category (COR), we\nobserve than the number of orthographic errors are more\ncommon in non satirical documents than in satirical doc-\numents. A similar ﬁnding is observed with the number of\nhashtags, that appear mostly in non-satirical documents.\nConcerning morphological features, the use of pronouns and\nnouns are discriminatory features, being slightly more fre-\nquently the pronouns in satirical documents whereas nouns\nare more common on non-satirical documents. This ﬁnding\nsuggests that satirical documents makes use of personiﬁca-\ntion to perform the satire.\nTable 6 Ablation analysis per linguistic category\nLinguistic category Precision Recall F1-score Accuracy\n(COR) Correction and style\nNon-satire 63.686 57.688 60.539 –\nSatire 61.330 67.106 64.088 –\nMacro-avg 62.508 62.397 62.313 62.397\n(PHO) Phonetics\nNon-satire 50.172 99.973 66.813 –\nSatire 96.296 0.714 1.417 –\nMacro-avg 73.234 50.343 34.115 50.343\n(MOR) Morphosyntax\nNon-satire 73.539 60.132 66.163 –\nSatire 66.280 78.364 71.817 –\nMacro-avg 69.909 69.248 68.990 69.248\n(SEM) Semantics\nNon-satire 50.062 99.121 66.525 –\nSatire 56.164 1.126 2.207 –\nMacro-avg 53.113 50.124 34.366 50.124\n(PRA) Pragmatics\nNon-satire 51.490 87.781 64.907 –\nSatire 58.605 17.298 26.712 –\nMacro-avg 55.047 52.540 45.810 52.540\n(STI) Stylometry\nNon-satire 74.095 80.340 77.091 –\nSatire 78.531 71.911 75.075 –\nMacro-avg 76.313 76.083 76.484 76.126\n(LEC) Lexical\nNon-satire 62.529 58.649 60.527 –\nSatire 61.065 64.854 62.903 -\nMacro-avg 61.797 61.752 61.715 67.752\n(PLI) Psycho linguistics processes\nNon-satire 54.430 53.130 53.772 –\nSatire 54.224 54.519 54.864 –\nMacro-avg 54.327 54.325 54.318 54.325\n(REG) Register\nNon-satire 54.366 82.235 65.457 –\nSatire 63.549 30.972 41.647 –\nMacro-avg 58.957 56.604 53.552 56.604\n(SOC) Social media jargon\nNon-satire 67.512 50.439 57.740 –\nSatire 60.443 75.728 67.227 –\nMacro-avg 63.977 63.083 62.484 63.083\nPrecision, recall, F1-score of satiric and non-satiric labels and the\nmacro-averaged precision, recall, F1-score, and accuracy of the overall\nresult\nComparison with previous datasets\nIn order to compare the reliability of our proposal, we com-\npare our methods with the dataset compiled by Barbieri [ 3],\n123\nComplex & Intelligent Systems (2022) 8:1723–1736 1733\nFig. 3 Polar chart per linguistic category arranged by satiric and non\nsatiric documents\nthe two datasets compiled by Salas-Zárate [ 21] concern-\ning European Spanish and Mexican Spanish, and the three\ndatasets for the IroSV A 2019 shared task [ 18] based on Euro-\npean Spanish and Spanish from Cuba and Mexico. Table 7\ncontains the results for each related feature set. We compare\nthe results with our approach based on the combination of LF\nwith BF, applying a knowledge integration strategy, as this\napproach provide our best results with the Spanish SatiCor-\npus 2021. It is important to remark that neither [ 21] nor [ 3]\nprovided the distribution of the labels for training and testing\nas they rely on ten-cross fold validation. However, in order\nto maintain our pipeline, we divided these datasets into train-\ning, validation, and testing similar as we do with the Spanish\nSatiCorpus 2021.\nThe ﬁrst experiments conducted by Barbieri [ 3], achieved\na macro F1-score of 85.200% by using linguistic features and\nhand-craft features. Our approach achieves a macro F1-score\nof 90.007%. However, these results should be compared with\ncaution, as Barbieri report their results with ten-cross valida-\ntion whereas we report us with a test set of 20%. We observed\nthat the most relevant linguistic features are related to stylom-\netry and punctuation, including the readability and the length\nof the documents. We also observe that common nouns and\nsufﬁxes were relevant morphosyntax features.\nWhen compared with Salas-Zarate [ 21], we can observe\nthan for the European Spanish dataset our approach based\non LF and BF largely outperforms the results achieved with\nLIWC, from an 85.5% of accuracy to a 95.6477% of accu-\nracy. In this sense, the results achieved by Salas-Zarate using\nonly linguistic features are competitive, but far for the per-\nformance that contextual embeddings provides. In case of\nthe Mexican Spanish dataset, our results also outperform\nthe ones from Salas-Zarate, from a 84% of accuracy to a\n92.8427%. In case of the European Spanish, we found that\nmorphological features stand out from the stylometric clues\nfor the classiﬁcation, including indicative simple present and\nimperative verbs tenses and prepositions, but also the pres-\nence of orthographic mistakes. In case of Mexican Spanish,\nhowever, due to the biases of the dataset, the social media\nfeatures (hashtags, mentions) are the most relevant features.\nThere are also relevant features related to misspellings and\nstylometry and punctuation.\nFinally, we compare our approach with IroSV A 2019. Note\nthat as this competition is recent, the approaches submitted\nby the participants of the task were very competitive, and\nour approach only outperforms the results achieved with the\nCuban dataset with a macro F1-score of 66.3360%, com-\npared with the macro F1-score of 65.9600%, which was the\nofﬁcial best result of the task. In case of Spanish and Mexi-\ncan, however, the results were slightly inferior. 70.9440% vs\n71.6700% of macro F1-score with European Spanish, and\n66.3950% vs 68.03% of macro F1-score with Mexican Span-\nish. We found some differences among the linguistic features\namong all datasets. First, we only found as relevant perfor-\nmance errors for the Spanish, whereas we could not identify\nany kind of correction and style relevant feature for Mexi-\ncan and Cuban. It also draws our attention the presence of\npsycho linguistic process in Mexican and Cuban. However,\nthese features were positive for the Mexican and related to\nanger in Cuban. Besides, we found a small bias in the Spanish\ndataset with the presence of hyperlinks as relevant feature.\nThe most common linguistic category among all datasets is\nstylometry, with features related to the length of the docu-\nments (Spanish), the TTR standard Token (Mexican), and the\nnumber of sentences (Cuban).\nAfter the analysis of the results and the comparison with\nother datasets, we achieve the following insights:\n– The results achieved by contextual sentence and word\nembeddings are similar. However, sentence embeddings\nare easier and faster to combine with other features.\n– The usage of embeddings learnt from monolingual\napproaches are superior from the ones learnt with multi-\nlingual datasets. as the results achieved with the Spanish\nversion of BERT outperform largely the results achieved\nwith multilingual BERT.\n– The identiﬁcation of satire by means of linguistic features\nare more conﬁdent with the usage of stylometric clues\nrather than other clues such as semantics, pragmatics, or\nby sentiment.\n123\n1734 Complex & Intelligent Systems (2022) 8:1723–1736\nFig. 4 Information gain of the\ntop ranked linguistic features\nper label\n(STY) Inflesz\n(STY) Readability\n(COR) Orthographics\n(SOC) Hashtags\n(STY) Word length average\n(MOR) Nominal suffixes\n(STY) Syllabes per word\n(MOR) Pronouns\n(STY) Space symbol\n(MOR) Nouns\n0% 25% 50% 75% 100%\nnon-satire satire\nTable 7 Comparison of\naccuracy and macro averaged\nprecision, recall, and F1-score\nwith other datasets related to\nsatire and irony identiﬁcation\nDataset Approach Spanish variant M-P M-R M- F1-score Accuracy\nSatire [ 3] (2015) SVM European – – 85.200 –\nLF + BF European 90.049 90.037 90.001 90.037\nSatire [ 21] (2017) LIWC European 85.500 85.500 85.500 85.500\nLF + BF European 95.602 95.655 95.627 95.648\nLIWC Mexican 84.600 84.000 84.000 84.000\nLF + BF Mexican 92.845 92.862 92.842 92.843\nIrony [ 18] (2019) – European – – 71.670 –\nLF + BF European 70.530 72.375 70.944 72.833\n- Mexican – – 68.030 –\nLF + BF Mexican 67.292 65.897 66.395 71.333\n- Cuban – – 65.960 –\nLF + BF Cuban 67.555 65.750 66.336 71.500\n– Linguistic features are complementary with all kind of\nembeddings, regardless if they are word or sentence-\nbased, and regardless if they are contextual or non-\ncontextual. However, the contribution of the linguistic\nfeatures with the contextual sentence embeddings is lim-\nited, due to the high performance achieved with these\nembeddings in isolation.\n– There is no one linguistic category that stands out from\nthe rest. Although the best results for each characteristic\nwere obtained with the stylometry category, the general\nresults obtained by LF are superior. We also observed\nthan some categories are no relevant for satire iden-\ntiﬁcation, specially those related to phonetics (PHO),\nsemantics (SEM), and pragmatics (PRA).\nConclusions and further work\nIn this paper we have described the compilation of the 2021\nSatiCorpus 2021 dataset for satire identiﬁcation. This corpus\nis balanced and contains a total of 36414 documents. During\nits compilation, we focused on avoid the problems identiﬁed\nwhen we analysed past resources that were related to the lim-\nitation of the number of different accounts, the heterogeneity\nbetween the topics discussed on satirical and non-satirical\ndocuments, and the presence of clues within the texts that\ncan biased the results in order to identify the author of the\ntweet rather than if the document is satirical or not. As com-\nmented in Section 3, this dataset has been released for the\nscientiﬁc community. In addition, this dataset has been com-\n123\nComplex & Intelligent Systems (2022) 8:1723–1736 1735\nprehensively analysed with different feature sets and their\ncombinations, including linguistic features and a wide vari-\nety of embeddings.\nAs future promising research lines, we suggest to extend\nthe Spanish SatiCorpus 2021 to include information regard-\ning what mechanisms for performing satire are involved. On\nthe one hand, this proposal is harder to resolve than sim-\nply binary classiﬁcation. On the other hand, these insights\nwill provide major understanding concerning satire identiﬁ-\ncation. We also propose to investigate the interpretability of\nthe linguistic features avoiding model-agnostic approaches,\nand focusing of how they perform within the neural network,\nin order to gain better understanding about in which cases LF\nand embeddings are complementary. Regarding the selection\nof the model, we will evaluate to change the hyperparame-\nter optimisation stage, based on random search, for other\napproaches such as Bayes optimisation. In this sense, we\nwill evaluate other strategies to compare the models and the\nadoption of nested-cross validation strategies to obtain mod-\nels that generalise better.\nAcknowledgements This paper is part of the research project LaTe4PSP\n(PID2019-107652RB-I00) funded by MCIN/ AEI/10.13039/50110001\n1033. In addition, José Antonio García-Díaz is supported by Banco San-\ntander and the University of Murcia through the Doctorado industrial\nprogramme.\nDeclarations\nConﬂict of interest The authors declare that they have no conﬂict of\ninterest.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\n1. Al Imran A, Wahid Z, Ahmed T (2020) Bnnet: a deep neural\nnetwork for the identiﬁcation of satire and fake Bangla news. Inter-\nnational conference on computational data and social networks.\nSpringer, Berlin, pp 464–475\n2. Apolinario-Arzube Ó, García-Díaz JA, Medina-Moreira J, Luna-\nAveiga H, V alencia-García R (2020) Comparing deep-learning\narchitectures and traditional machine-learning approaches for\nsatire identiﬁcation in Spanish tweets. Mathematics 8(11):2075\n3. Barbieri F, Ronzano F, Saggion H (2015) Is this tweet satirical? A\ncomputational approach for satire detection in Spanish. Procesam\ndel Leng Nat 55:135–142\n4. Cañete J, Chaperon G, Fuentes R, Ho JH, Kang H, Pérez J (2020)\nSpanish pre-trained bert model and evaluation data. In: PML4DC\nat ICLR 2020\n5. Devlin J, Chang MW, Lee K, Toutanova K (2019) BERT: pre-\ntraining of deep bidirectional transformers for language under-\nstanding. In: Proceedings of the 2019 conference of the North\nAmerican chapter of the association for computational linguistics:\nhuman language technologies, pp 4171–4186\n6. García-Díaz JA, Almela Á, Alcaraz-Mármol G, V alencia-García\nR (2020) Umucorpusclassiﬁer: compilation and evaluation of lin-\nguistic corpus for natural language processing tasks. Procesa del\nLeng Nat 65:139–142\n7. García-Díaz JA, Cánovas-García M, Palacios RC, V alencia-García\nR (2021) Detecting misogyny in Spanish tweets. An approach\nbased on linguistics features and word embeddings. Future Gener\nComput Syst 114:506–518. https://doi.org/10.1016/j.future.2020.\n08.032\n8. García-Díaz JA, Cánovas-García M, V alencia-García R (2020)\nOntology-driven aspect-based sentiment analysis classiﬁcation: an\ninfodemiological case study regarding infectious diseases in Latin\nAmerica. Future Gener Comput Syst 112:641–657. https://doi.org/\n10.1016/j.future.2020.06.019\n9. Ghanem B, Karoui J, Benamara F, Rosso P , Moriceau V (2020)\nIrony detection in a multilingual context. Adv Inf Retr 12036:141\n10. Golazizian P , Sabeti B, Asli SAA, Majdabadi Z, Momenzadeh O,\nFahmi R (2020) Irony detection in persian language: a transfer\nlearning approach using emoji prediction. In: Proceedings of The\n12th Language Resources and Evaluation Conference, pp 2839–\n2845\n11. Krasnowska-Kiera´ s K, Wróblewska A (2019) Empirical linguis-\ntic study of sentence embeddings. In: Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics,\npp 5729–5739\n12. Liu, R., & Zhou, X. (2021, August). Grenzlinie at SemEval-2021\nTask 7: Detecting and Rating Humor and Offense. In Proceed-\nings of the 15th International Workshop on Semantic Evaluation\n(SemEval-2021) pp. 281–285\n13. Libovick` y J, Rosa R, Fraser A (2019) How language-neutral is\nmultilingual bert? arXiv preprint arXiv:1911.03310\n14. Mikolov T, Chen K, Corrado G, Dean J (2013) Efﬁcient esti-\nmation of word representations in vector space. arXiv preprint\narXiv:1301.3781\n15. Mikolov T, Grave E, Bojanowski P , Puhrsch C, Joulin A (2018)\nAdvances in pre-training distributed word representations. In: Pro-\nceedings of the International Conference on Language Resources\nand Evaluation (LREC 2018)\n16. Onan A, Toço˘glu MA (2020) Satire identiﬁcation in Turkish news\narticles based on ensemble of classiﬁers. Turk J Electr Eng Comput\nSci 28(2):1086–1106\n17. Onan A, Toço˘ glu MA (2021) A term weighted neural language\nmodel and stacked bidirectional lstm based framework for sarcasm\nidentiﬁcation. IEEE Access 9:7701–7722\n18. Ortega-Bueno R, Rangel F, Hernández Farıas D, Rosso P , Montes-\ny Gómez M, Medina Pagola JE (2019) Overview of the task on\nirony detection in Spanish variants. Proc Iber Lang Eval Forum\n2421:229–256\n19. Pennington J, Socher R, Manning CD (2014) Glove: Global vectors\nfor word representation. In: Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP),\npp 1532–1543\n20. del Pilar Salas-Zárate M, Alor-Hernández G, Sánchez-Cervantes\nJL, Paredes-V alverde MA, García-Alcaraz JL, V alencia-García R\n(2020) Review of English literature on ﬁgurative language applied\n123\n1736 Complex & Intelligent Systems (2022) 8:1723–1736\nto social networks. Knowl Inf Syst 62(6):2105–2137. https://doi.\norg/10.1007/s10115-019-01425-3\n21. del Pilar Salas-Zárate M, Paredes-V alverde MA, Rodríguez-García\nMÁ, V alencia-García R, Alor-Hernández G (2017) Automatic\ndetection of satire in twitter: a psycholinguistic-based approach.\nKnowl Based Syst 128:20–33. https://doi.org/10.1016/j.knosys.\n2017.04.009\n22. Qi P , Zhang Y , Zhang Y , Bolton J, Manning CD (2020) Stanza:\na python natural language processing toolkit for many human\nlanguages. In: Proceedings of the 58th annual meeting of the asso-\nciation for computational linguistics: system demonstrations, pp\n101–108\n23. Reimers N, Gurevych I, Reimers N, Gurevych I, Thakur N, Reimers\nN, Gurevych I (2019) Sentence-BERT: sentence embeddings using\nSiamese BERT-Networks. In: Proceedings of the 2019 Conference\non empirical methods in natural language processing. Association\nfor computational linguistics\n24. Sanh V , Debut L, Chaumond J, Wolf T (2019) Distilbert, a dis-\ntilled version of bert: smaller, faster, cheaper and lighter. ArXiv\narXiv:abs/1910.01108\n25. Tausczik YR, Pennebaker JW (2010) The psychological meaning\nof words: Liwc and computerized text analysis methods. J Lang\nSoc Psychol 29(1):24–54\n26. Virtanen A, Kanerva J, Ilo R, Luoma J, Luotolahti J, Salakoski T,\nGinter F, Pyysalo S (2019) Multilingual is not enough: Bert for\nﬁnnish. arXiv preprint arXiv:1912.07076\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6456249356269836
    },
    {
      "name": "Misinformation",
      "score": 0.6406134366989136
    },
    {
      "name": "Identification (biology)",
      "score": 0.6359873414039612
    },
    {
      "name": "Hoax",
      "score": 0.6083868741989136
    },
    {
      "name": "Natural language processing",
      "score": 0.5803750157356262
    },
    {
      "name": "Transformer",
      "score": 0.548216700553894
    },
    {
      "name": "Authorship attribution",
      "score": 0.5393459796905518
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5238747000694275
    },
    {
      "name": "Social media",
      "score": 0.4908004701137543
    },
    {
      "name": "Linguistics",
      "score": 0.48938149213790894
    },
    {
      "name": "Irony",
      "score": 0.4393317699432373
    },
    {
      "name": "Information retrieval",
      "score": 0.33573612570762634
    },
    {
      "name": "World Wide Web",
      "score": 0.14337345957756042
    },
    {
      "name": "Engineering",
      "score": 0.08584591746330261
    },
    {
      "name": "Computer security",
      "score": 0.07160007953643799
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Alternative medicine",
      "score": 0.0
    }
  ]
}