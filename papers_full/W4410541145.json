{
  "title": "Modern artificial intelligence and large language models in graduate medical education: a scoping review of attitudes, applications &amp; practice",
  "url": "https://openalex.org/W4410541145",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2286965369",
      "name": "Basil George Verghese",
      "affiliations": [
        "Unity Health System",
        "University of Rochester",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A3004763996",
      "name": "Charoo Iyer",
      "affiliations": [
        "West Virginia University"
      ]
    },
    {
      "id": "https://openalex.org/A3181012459",
      "name": "Tanvi Borse",
      "affiliations": [
        "Parkview Health"
      ]
    },
    {
      "id": "https://openalex.org/A3194450421",
      "name": "Shiamak Cooper",
      "affiliations": [
        "Rochester General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2096766185",
      "name": "Jacob White",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2918295671",
      "name": "Ryan N. Sheehy",
      "affiliations": [
        "University of Kansas Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2286965369",
      "name": "Basil George Verghese",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3004763996",
      "name": "Charoo Iyer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3181012459",
      "name": "Tanvi Borse",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3194450421",
      "name": "Shiamak Cooper",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096766185",
      "name": "Jacob White",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2918295671",
      "name": "Ryan N. Sheehy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2966555834",
    "https://openalex.org/W3007086729",
    "https://openalex.org/W4367039164",
    "https://openalex.org/W1978203510",
    "https://openalex.org/W4307300340",
    "https://openalex.org/W4220874490",
    "https://openalex.org/W4297201393",
    "https://openalex.org/W2945476560",
    "https://openalex.org/W3189751383",
    "https://openalex.org/W3209030113",
    "https://openalex.org/W4388024652",
    "https://openalex.org/W2075950485",
    "https://openalex.org/W2105321265",
    "https://openalex.org/W2891378911",
    "https://openalex.org/W3203379271",
    "https://openalex.org/W2788527903",
    "https://openalex.org/W4200027632",
    "https://openalex.org/W4286295814",
    "https://openalex.org/W4296889643",
    "https://openalex.org/W3136664331",
    "https://openalex.org/W4386375719",
    "https://openalex.org/W2984220309",
    "https://openalex.org/W4321350185",
    "https://openalex.org/W4318542923",
    "https://openalex.org/W4224995026",
    "https://openalex.org/W2136906253",
    "https://openalex.org/W4283214177",
    "https://openalex.org/W3194609937",
    "https://openalex.org/W3093461680",
    "https://openalex.org/W2966447025",
    "https://openalex.org/W4292981884",
    "https://openalex.org/W2974089311",
    "https://openalex.org/W4306941749",
    "https://openalex.org/W2990883981",
    "https://openalex.org/W4307644840",
    "https://openalex.org/W2936675185",
    "https://openalex.org/W2073725554",
    "https://openalex.org/W2050682733",
    "https://openalex.org/W2783760302",
    "https://openalex.org/W3009131687",
    "https://openalex.org/W2152607248",
    "https://openalex.org/W4390988578",
    "https://openalex.org/W4390399123",
    "https://openalex.org/W3109721643",
    "https://openalex.org/W3177350052",
    "https://openalex.org/W3111931458",
    "https://openalex.org/W4323306041",
    "https://openalex.org/W3201909745",
    "https://openalex.org/W4323807148",
    "https://openalex.org/W4311927723",
    "https://openalex.org/W4385332627",
    "https://openalex.org/W3187352571",
    "https://openalex.org/W4280582321",
    "https://openalex.org/W3119875509",
    "https://openalex.org/W2013347800",
    "https://openalex.org/W4286722203",
    "https://openalex.org/W4327908244",
    "https://openalex.org/W4381189340",
    "https://openalex.org/W4385778931",
    "https://openalex.org/W4323925118",
    "https://openalex.org/W3016669704",
    "https://openalex.org/W4200487260",
    "https://openalex.org/W4380291947",
    "https://openalex.org/W2020458978",
    "https://openalex.org/W2346239344",
    "https://openalex.org/W1988284258",
    "https://openalex.org/W4321789894",
    "https://openalex.org/W2892946435",
    "https://openalex.org/W4365137614",
    "https://openalex.org/W4380291159",
    "https://openalex.org/W4389137712",
    "https://openalex.org/W4386700994",
    "https://openalex.org/W4368360859",
    "https://openalex.org/W4388584567",
    "https://openalex.org/W4386438954",
    "https://openalex.org/W4388948883",
    "https://openalex.org/W4377711218",
    "https://openalex.org/W4383311938",
    "https://openalex.org/W4389211385",
    "https://openalex.org/W4386249608",
    "https://openalex.org/W3092018978",
    "https://openalex.org/W3041695882",
    "https://openalex.org/W3033158784",
    "https://openalex.org/W4200245798",
    "https://openalex.org/W3014403957",
    "https://openalex.org/W4365147231",
    "https://openalex.org/W1989495673",
    "https://openalex.org/W2959684014",
    "https://openalex.org/W4385655444",
    "https://openalex.org/W4308417763",
    "https://openalex.org/W3006531750",
    "https://openalex.org/W4385781378",
    "https://openalex.org/W4307298858",
    "https://openalex.org/W4213378800",
    "https://openalex.org/W4200052368",
    "https://openalex.org/W2939294732",
    "https://openalex.org/W4385261389",
    "https://openalex.org/W4387918100",
    "https://openalex.org/W4389639728",
    "https://openalex.org/W4391990577",
    "https://openalex.org/W3182158678",
    "https://openalex.org/W4389179910",
    "https://openalex.org/W2943800384",
    "https://openalex.org/W4317581943",
    "https://openalex.org/W3179492679",
    "https://openalex.org/W3157626300",
    "https://openalex.org/W4310822639",
    "https://openalex.org/W4309099432",
    "https://openalex.org/W4225567947",
    "https://openalex.org/W4382247801",
    "https://openalex.org/W4353048399",
    "https://openalex.org/W2769768418",
    "https://openalex.org/W1979788616",
    "https://openalex.org/W2949302541",
    "https://openalex.org/W2250706791"
  ],
  "abstract": null,
  "full_text": "RESEARCH Open Access\n© The Author(s) 2025, corrected publication 2025. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit \nto the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other \nthird party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. \nIf material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds \nthe permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  v e c  o m \nm  o n s .  o r  g / l i c e n s e s / b y / 4 . 0 /.\nVerghese et al. BMC Medical Education          (2025) 25:730 \nhttps://doi.org/10.1186/s12909-025-07321-5\nBMC Medical Education\n*Correspondence:\nBasil George Verghese\nbverghe1@jhu.edu\nFull list of author information is available at the end of the article\nAbstract\nBackground Artificial intelligence (AI) holds transformative potential for graduate medical education (GME), yet, a \ncomprehensive exploration of AI’s applications, perceptions, and limitations in GME is lacking.\nObjective To map the current literature on AI in GME, identifying prevailing perceptions, applications, and research \ngaps to inform future research, policy discussions, and educational practices through a scoping review.\nMethods Following the Joanna Briggs Institute guidelines and the PRISMA-ScR checklist a comprehensive search of \nmultiple databases up to February 2024 was performed to include studies addressing AI interventions in GME.\nResults Out of 1734 citations, 102 studies met the inclusion criteria, conducted across 16 countries, predominantly \nfrom North America (72), Asia (14), and Europe (6). Radiology had the highest number of publications (21), followed \nby general surgery (11) and emergency medicine (8). The majority of studies were published in 2023. Several key \nthematic areas emerged from the literature. Initially, perceptions of AI in graduate medical education (GME) were \nmixed, but have increasingly shifted toward a more favorable outlook, particularly as the benefits of AI integration \nin education become more apparent. In assessments, AI demonstrated the ability to differentiate between skill \nlevels and offer meaningful feedback. It has also been effective in evaluating narrative comments to assess resident \nperformance. In the domain of recruitment, AI tools have been applied to analyze letters of recommendation, \napplications, and personal statements, helping identify potential biases and improve equity in candidate selection. \nFurthermore, large language models consistently outperformed average candidates on board certification and \nin-training examinations, indicating their potential utility in standardized assessments. Finally, AI tools showed \npromise in enhancing clinical decision-making by supporting trainees with improved diagnostic accuracy and \nefficiency.\nConclusions This scoping review provides a comprehensive overview of applications and limitations of AI in GME \nbut is limited with potential biases, study heterogeneity, and evolving nature of AI.\nModern artificial intelligence and large \nlanguage models in graduate medical \neducation: a scoping review of attitudes, \napplications & practice\nBasil George Verghese1,2*, Charoo Iyer3, Tanvi Borse4, Shiamak Cooper5, Jacob White6 and Ryan Sheehy7\nPage 2 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nIntroduction\nModern AI refers to data-driven systems that perform \ntasks requiring human intelligence, such as learning, \nnatural language processing (NLP), pattern recognition, \nproblem-solving, and autonomous decision-making [ 1]. \nThese systems, which utilize techniques like machine \nlearning, deep learning, and neural networks, continu -\nously adapt and improve with large datasets. This con -\ntrasts with traditional AI systems, which rely on static, \nrule-based logic and require manual configuration and \noversight, lacking the adaptive, data-driven learning \ncapabilities of modern models [ 2]. For instance, earlier \nsystems required faculty to manually interpret and sum -\nmarize resident performance data, whereas modern AI \ntools like large language models can automate this anal -\nysis, offering timely insights and reducing administra -\ntive burden [ 3]. Modern AI is transforming educational \nmethodologies from a one-size-fits-all approach to per -\nsonalized learning tailored to individual strengths and \nneeds [4] and is increasingly being considered to address \nthese diverse training requirements [5].\nGraduate Medical Education (GME) plays a crucial role \nin developing healthcare provider’s skills and competen -\ncies to meet the evolving healthcare landscape [ 6] While \nAI holds transformative potential, its integration into \ngraduate medical education (GME) must be considered \nwithin the context of the evolving challenges that GME \nfaces [ 7]. These challenges—such as adapting to chang -\ning healthcare paradigms and maintaining relevant cur -\nricula—necessitate innovative solutions, including AI, to \nenhance educational outcomes and better prepare future \nmedical professionals [8].\nHowever, despite its growing presence in healthcare, \nthe integration of AI into GME remains underexplored. \nWhile individual studies highlight AI’s applications in \nspecific areas, such as clinical decision support [ 9] or \nsimulation-based training [ 10], a comprehensive under -\nstanding of its impact across GME programs is lacking.\nPrevious scoping reviews have examined the role of \nAI in undergraduate medical education [ 11], surgery \n[12], and pharmacy [ 13], showcasing its promise in these \ndomains. However, GME—a critical phase of physician \ntraining—has yet to be systematically reviewed. This \ngap in the literature underscores the need for a scoping \nreview to map the existing evidence on AI’s applications, \nidentify perceptions and barriers, and pinpoint research \ngaps in this field.\nFollowing the Joanna Briggs Institute (JBI) guidelines \n[14], this scoping review seeks to provide a foundation \nfor future research, policy discussions, and educational \npractices regarding AI’s role in GME. By doing so, it aims \nto ensure that AI is effectively leveraged to enhance the \ntraining and development of future physicians.\nMethodology\nOur scoping review followed the Joanna Briggs Institute \n(JBI) guidelines [ 15] and used the Preferred Reporting \nItems for Systematic Reviews and Meta-Analysis exten -\nsion for Scoping Reviews (PRISMA-ScR) [ 16] checklist. \nWe registered our protocol on the Open Science Forum \n[17].\nEligibility criteria\nPopulation\nOriginal peer-reviewed studies explicitly mentioning \nGraduate Medical Education (GME) were included. Eli -\ngible studies focused on residents (across any specialty), \npostgraduate trainees, fellows, or faculty.\nIntervention\nAI interventions including a range of technologies such \nas large language models (LLMs), natural language pro -\ncessing (NLP), recurrent neural networks (RNNs), arti -\nficial neural networks (ANNs), convolutional neural \nnetworks (CNNs), and other related approaches.\nComparator\nGiven the nature of the scoping review, specific compara-\ntors were not required.\nOutcomes\nThe primary outcomes of interest were AI’s perceptions, \napplication, and limitations in GME.\nStudy selection and extraction\nWe performed a comprehensive search across multiple \ndatabases, including MEDLINE-Ovid, ERIC, EMBASE, \nCINAHL, Web of Science Core Collection, Com -\npendex, Scopus, and IEEE Xplore, from inception to \nFebruary 2024. These databases were selected to ensure \nbroad coverage of medical education literature, includ -\ning specialized fields such as healthcare technology \n(IEEE Xplore) and educational research (ERIC), while \nalso capturing interdisciplinary perspectives from both \nmedical and engineering disciplines. This broad selec -\ntion was intended to maximize the inclusivity of relevant \nstudies across diverse healthcare sectors. The search \nKeywords Artificial intelligence in graduate medical education, AI integration in medical residency training, \nApplications of AI in GME, Large language models in medical education, Perceptions of AI in medical training, \nAI-driven assessments in medical education\nPage 3 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nstrategy, refined in consultation with a research librarian, \nis detailed in Table 1.\nStudies were included if they [ 1] were original, peer-\nreviewed research articles; [ 2] involved participants in \nGME; [ 3] examined or evaluated artificial intelligence \n(AI) technologies—including machine learning, natural \nlanguage processing, or large language models (LLMs) \nand [ 4] reported outcomes related to education, assess -\nment, recruitment, or clinical training.\nDuring the full-text review stage, studies were excluded \nif they were duplicates, did not report relevant outcomes, \nwere categorized as gray literature such as editorials, \nopinion pieces or commentaries, lacked a clear AI inter -\nvention, were not related to GME populations or set -\ntings, or if the full text was unavailable. These exclusions \nensured that included studies aligned with the scope and \nobjectives of the review.\nAfter the initial pilot screening, we employed a parallel \ndual review process for study selection, extraction, and \nanalysis. Two independent reviewers (BGV, SC, TB, and \nCI) screened the titles, abstracts, and full texts, resolving \ndisagreements with a third reviewer (RS). We used the \nCovidence platform for initial screening and deduplica -\ntion. One reviewer (BGV) performed the data extraction, \nverified by a second reviewer (SC, TB, or CI) for accuracy \nand consistency. All the authors reviewed and reached a \nconsensus on the final articles for data extraction.\nCategorization and analysis\nWe employed a descriptive analysis approach to synthe -\nsize the study findings, categorizing studies based on AI \napplications, perceptions, and challenges in GME. These \ncategories were developed through an iterative thematic \ncoding process, where key themes were identified based \non study objectives, methodologies, and reported out -\ncomes. This approach ensured a comprehensive and \napplicable review.\nResults\nOverview\nThe PRISMA diagram in Fig.  1 shows the screening pro -\ncess. After removing duplicates from the 1,734 database \ncitations, 1,112 records remained. After title and abstract \nscreening, 441 articles were eligible for full-text evalu -\nation. Of these, 102 were included in the final scoping \nreview for data extraction.\nFigure 2 details the included studies conducted across \n16 countries, with eight studies involving three or more \ncountries. The origins include North America [ 72], Asia \n[14], multiple regions [8], Europe [6], and Oceania [2].\nMost studies were published between 1997 and 2024, \nwith the highest number published in 2023.\nTable 2 (end of manuscript) presents a detailed break-\ndown of the studies included. Radiology had the most \npublications [ 21], followed by general surgery [ 11] and \nemergency medicine [ 8] 37 studies involved multiple \nparticipant types, 34 studies included residents and chief \nresidents, 14 studies had no human participants, 10 stud -\nies focused solely on applicants, 6 studies involved faculty \nand program directors (PDs), and 1 study included only \nmedical students.\nThe most common study types were development and \nvalidation studies [ 32], observational studies [ 26], non-\nrandomized experimental studies [ 16], survey studies \n[14], and predictive studies [ 10]. Only one randomized \ncontrolled trial was identified and one development study \nwithout validation. These studies explored a wide range \nof AI applications, from skill assessment tools and diag -\nnostic support to recruitment analytics and educational \ninterventions. Further details on their specific findings \nand implications are discussed below.\nApplication of AI in GME\nAdoption, perception, and attitudes towards AI\nOut of twelve studies reviewed, nine focused on radiol -\nogy. Concerns about AI influencing specialty choices \nwere evident, with one survey reporting a significant \ndecrease in radiology’s appeal as a first-choice specialty \namong medical students ( P < 0.001), deterring 17% of \nrespondents [ 18]. Concerns about job security due to \nAI were also noted [ 19]. Similarly, radiology and oncol -\nogy residents expressed worries about AI reducing the \ndemand for professionals in their fields [ 20, 21]. Huis -\nman [ 22] found that limited knowledge about AI often \nleads to fear of job replacement, whereas intermediate to \nadvanced knowledge fosters positive attitudes. Younger \nindividuals, those with scientific backgrounds, and active \nsocial media users tended to view AI more favorably.\nRecent studies [ 23, 24] indicate a shift in percep -\ntion among radiology residents, with most now viewing \nAI positively. According to one study, 76% of residents \nfind AI exciting, 80% would still choose radiology as a \nTable 1 Search strategy. Database: Ovid MEDLINE(R) and Epub \nahead of print, In-Process, In-Data-Review & other Non-Indexed \ncitations and daily < 1946 to February 02, 2024. Adapted for \nEMBASE, IEEE, CINAHL, PsycINFO, Scopus, ERIC, web of science: \ncore collection & compendex\n# Query\n1 exp Artificial Intelligence/\n2 (artificial intelligence or “natural language” or “deep learning” \nor “neural network*” or “support vector machine” or svm or \n“machine learning”).ti, ab, kw, jn.\n3 Education, Medical, Graduate/ or “Internship and Residency”/\n4 (“education, residency” or “residency educat*” or “residency \ntrain*” or “residential education” or “house staff” or “graduate \nmedical educat*”).ti, ab, kw.\n5 1 or 2\n6 3 or 4\n7 5 and 6\nPage 4 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nFig. 1 Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) flow chart\n \nPage 5 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nspecialty, and 74% see no threat to job security from AI \n[23]. Marquis [ 25] summarized a 2020 survey by the \nAmerican Alliance of Academic Chief Residents in Radi -\nology, reporting that 74% of respondents believe AI poses \nno threat to the job market and could enhance workflow \nefficiency. However, despite 95% of residents recognizing \nthe importance of AI and machine learning education, \n20% of training programs still lack formal AI education, \nas highlighted in a study by Salastekar [26].\nThere was one study on faculty perception wherein \nanesthesiology faculty broadly agree that AI will benefit \npatient care (80%), enhance resident training (86.7%), and \nimprove post-graduate medical education (81.4%), but \n67% were concerned about accountability for AI inaccu -\nracies [27].\nRole of AI in formative & summative assessment\nResearch has increasingly focused on AI’s impact on both \nformative and summative assessments in medical edu -\ncation. For instance, the Intelligent Continuous Exper -\ntise Monitoring System (ICEMS) successfully identified \ndifferent levels of surgical performance among neuro -\nsurgeons, trainees, and students in a virtual reality (VR) \nsimulation, with performance levels strongly correlating \nto years of training ( p = 0.005) [28]. This was also noted \nsimilarly in other surgical specialties [ 29–35] highlight -\ning the efficacy of AI in differentiating skill levels in sur -\ngical training, indicating AI’s potential to improve the \naccuracy of resident training evaluations. In robotic sur -\ngery, Quinn [ 36] validated robotic performance metrics \nagainst human observation, showing a high correlation \n(r = 0.98, p < 0.0001),\nAI’s potential for real-time assessment is also signifi -\ncant. Anh [ 37] compared feature extraction techniques \nfor automated surgical skill assessment using motion \nanalysis, showing promising results. Likewise, Ruzicki \n[38] reported high tool detection accuracy (AUC 0.933 \nto 0.998) in cataract surgery, although skill classification \naccuracy varied. Similar results were also noted in studies \nby Holden [ 39] & Oropesa [ 40] while Kumar [ 41] devel-\noped an objective framework for robotic surgery train -\ning, offering valuable feedback.\nFig. 2 Study breakdown based on country where it was done\n \nPage 6 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nAbbott [110] 2021; Natural \nLanguage Processing to Estimate \nClinical Competency Committee \nRatings; United States\nExamine whether NLP can be used \nto estimate CCC ratings.\nPredictive \nStudy\nGeneral Surgery; \n24 Residents\nNatural Language Pro-\ncessing; Real world; AI vs. \nHuman performance; AI \nvs. other AI tools\nCombining NLP and non-NLP predictors (AUC = 0.87) outper-\nforms using either non-NLP predictors (AUC = 0.84) or NLP predic-\ntors (AUC = 0.83) alone, showing enhanced model performance. \nIntegrating both types of data improves predictive accuracy.\nAli [76] 2023; Performance of Chat-\nGPT, GPT-4, and Google Bard on a \nNeurosurgery Oral Boards Prepara-\ntion Question Bank; United States\nTo assess performance of three LLMs \n(GPT-3.5, GPT-4, and Google Bard) on \na question bank designed specifi-\ncally for neurosurgery oral boards \nexamination preparation.\nNon-Ran-\ndomized \nExperimen-\ntal Study\nNeurosurgery; \nNo human \nparticipants\nLarge Language Models; \nTheoretical; AI vs. other \nAI tools\nGPT-4 outperformed GPT-3.5 and Bard in answering higher-\norder questions and imaging questions, with fewer halluci-\nnations, demonstrating superior accuracy and reliability (all \nP-values < 0.05).\nAlkadri [31] 2021; Utilizing a mul-\ntilayer perceptron artificial neural \nnetwork to assess a virtual reality \nsurgical procedure; Canada\nTo demonstrate the benefits of \nartificial neural network algorithms \nin assessing and analyzing virtual \nsurgical performances, focusing on a \nsimulated annulus incision task dur-\ning an anterior cervical discectomy \nand fusion scenario.\nDevelop-\nment & \nValidation \nStudy\nNeurosurgery; \nOrthopedics; \n23 Residents; \nFellows\nArtificial Neural Net-\nwork; Simulations; No \ncomparator\nThe artificial neural network model demonstrated an 80% test-\ning accuracy when trained on nine selected surgical metrics, \nindicating reliable predictive performance across the evaluated \ncategories.\nAmirhajlou [43] 2019; Application \nof data mining techniques for \npredicting residents’ performance \non pre-board examinations: A case \nstudy.; Iran\nTo predict the performance of \nresidents on pre-board examinations \nbased on the results of in-training \nexaminations (ITE) using various \neducational data mining (DM) \ntechniques\nPredictive \nStudy\nMultiple; 841 \nResidents\nArtificial Neural Network; \nOther: SVM, Linear regres-\nsion; Real world; AI vs. \nother AI tools\nITE scores for PGY-2, PGY-3, and specialty training type predicted \npreboard examination scores, with MLP-ANN achieving the best \nperformance and lowest error metrics (RMSE and MAE)\nAndrews [105] 2021; Gender bias \nin resident evaluations: Natural lan-\nguage processing and competency \nevaluation; United States\nTo examine the differences in word \nuse, competency themes, and length \nwithin written evaluations of internal \nmedicine residents, considering the \nimpact of both faculty and resident \ngender.\nObserva-\ntion Study\nMedicine; \nRadiology; 413 \nFaculty\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nNLP was used to analyze 3,864 evaluations of internal medicine \nresidents to assess sentiment and core competencies, and found \nno significant gender differences in the quantity or quality of \nfeedback, despite female evaluators writing longer evaluations.\nAnh [37] 2020; Towards near real-\ntime assessment of surgical skills: \nA comparison of feature extraction \ntechniques.; Australia\nTo compare different feature extrac-\ntion techniques for automated surgi-\ncal skill assessment in near real-time \nusing motion analysis data.\nObserva-\ntion Study\nGeneral Surgery; \n11 Residents; \nFaculty\nMachine Learning; \nOther: Convolutional \nNeural Networks (CNN), \nLong Short Term \nMemory (LSTM), Principal \nComponent Analysis \n(PCA), Discrete Fourier \nTransform (DFT), Discrete \nCosine Transform (DCT), \nautoencoder, and others; \nSimulations; AI vs. other \nAI tools\nDeep CNN achieved the highest classification accuracy for surgi-\ncal tasks (96.84% suturing, 92.75% knot tying, 95.36% needle \npassing), while PCA also performed strongly (95.63% suturing, \n90.17% knot tying, 90.26% needle passing\nTable 2 Study description with summarized key results\nPage 7 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nAriaeinejad [42] 2020; A perfor-\nmance predictive model for emer-\ngency medicine residents; Canada\nTo develop a machine learning algo-\nrithm to identify patterns in resident \nperformance and early identification \nof residents at risk.\nDevelop-\nment & \nValidation \nStudy\nEmergency \nMedicine; \nnot specified; \nResidents\nArtificial Neural Network, \nSupport Vector Machines \n(SVM), k-Nearest Neigh-\nbor (kNN); Real world; AI \nvs. other AI tools\nThe SVM model showed the highest accuracy in identifying at-\nrisk residents with a sensitivity of 0.54, specificity of 0.74, and an \nAUC of 0.64\nÖtleş [111] 2021; Using Natural Lan-\nguage Processing to Automatically \nAssess Feedback Quality: Findings \nFrom 3 Surgical Residencies; United \nStates\nTo evaluate which NLP techniques \nbest classify the quality of surgical \ntrainee formative feedback recorded \nas part of a workplace assessment.\nNon-Ran-\ndomized \nExperimen-\ntal Study\nGeneral Surgery; \nNo human \nparticipant\nNatural Language Pro-\ncessing; Real world; AI vs. \nother AI tools\nThe SVM NLP model achieved the highest mean accuracy for \nclassifying feedback quality, with 0.64 for 4-category classifica-\ntion and 0.83 for binary classification, demonstrating the model’s \ncapability to automate feedback quality assessment.\nBaloul [32] 2022; Video Commen-\ntary & Machine Learning: Tell Me \nWhat You See, I Tell You Who You \nAre.; USA, Saudi Arabia\nTo apply Machine Learning (ML) in \nthe context of a structured Video \nCommentary (VC) assessment to \npredict surgical residents training \nlevels.\nPredictive \nStudy\nGeneral Surgery; \n81 Residents\nMachine Learning; Simu-\nlations; AI vs. Standard \nmethods\nIndividual VC clip scores strongly correlated with PGY level \n(p = 0.001), and using a supervised machine learning model to \npredict PGY levels improved accuracy by 40% over traditional \nstatistical analysis.\nBartoli [77] 2024; Probing artificial \nintelligence in neurosurgical train-\ning: ChatGPT takes a neurosurgical \nresidents written exam; Switzerland\nTo assess how ChatGPT performs in \ngenerating and answering questions \nfor a neurosurgical residents’ written \nexam.\nObserva-\ntion Study\nNeurosurgery; \n10 Residents\nLarge Language Models; \nSimulations; AI vs. Human \nperformance\nChatGPT ranked 6th out of 11 participants, scoring 25.4 out of 42 \nquestions.\nBissonnette [35] 2019; Artificial \nIntelligence Distinguishes Surgical \nTraining Levels in a Virtual Reality \nSpinal Task.; Canada\nTo evaluate the potential of artificial \nintelligence (AI) as an assessment \ntool in virtual reality spine surgery \nsimulation. Specifically, to determine \nif AI can uncover novel metrics of \nsurgical performance and distinguish \nbetween senior and junior par-\nticipants performing a virtual reality \nhemilaminectomy.\nObserva-\ntion Study\nNeurosurgery; \nOrthopedics; \n41 Residents; \nFellows; Fac-\nulty; Medical \nStudents\nMachine Learning; Other: \nMachine Learning (Sup-\nport Vector Machine, k-\nNearest Neighbors, Linear \nDiscriminant Analysis, \nNaive Bayes, Decision \nTree); Simulations; AI vs. \nother AI tools\nSVM achieved the highest accuracy (97.6%) in differentiating be-\ntween senior and junior participants in a virtual reality spinal task, \noutperforming kNN (92.7%), LDA (87.8%), Decision Tree (70.7%), \nand Naive Bayes (65.9%)\nBond [10] 2019; Virtual Standardized \nPatient Simulation: Case Develop-\nment and Pilot Application to High-\nValue Care.; United States\nTo develop virtual standardized \npatient (VSP) cases and provide \npreliminary evidence supporting \ntheir ability to provide experiential \nlearning in high-value care (HVC).\nDevelop-\nment & \nValidation \nStudy\nMedicine; \nMed-Peds; 14 \nResidents\nNatural Language Pro-\ncessing; Simulations; AI \nvs. Human performance\nIn the study context, faculty and platform ratings of learner suc-\ncess in history-taking correlated strongly (ρ = 0.80, P = 0.02), with \nhigh interrater reliability (0.87). Learners ordered a median of two \nunnecessary tests, correctly diagnosed within the top three 82% \nof the time, and completed 56% of optimal treatments.\nBoolchandani [53] 2023; Words \nUsed in Letters of Recommendation \nfor Pediatric Residency Applicants: \nDemographic Differences and Im-\npact on Interviews.; United States\nTo describe differences in agentic \n(achievement) and communal (rela-\ntionship) terms in LORs for pediatric \nresidency candidates by applicant \nand letter writer demographics and \nto examine if LOR language is associ-\nated with interview statusâ€‹â€‹.\nObserva-\ntion Study\nPediatrics; 573 \nResidency \napplicants\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nNLP was used to analyze 2,094 pediatric residency LORs. 53% of \nLORs were agency biased, 25% communal biased, and 23% neu-\ntral. There were no significant differences in LOR language based \non the applicant’s gender or race, but male letter writers used \nsignificantly more agentic terms than female writers. Neutral-\ntoned LORs were more likely associated with interview invitations\nTable 2 (continued)\n \nPage 8 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nBooth [112] 2023; Competency-\nBased Assessments: Leveraging \nArtificial Intelligence to Predict \nSubcompetency Content; United \nStates\nTo develop and evaluate a Natural \nLanguage Processing (NLP) algo-\nrithm that automatically categorizes \nnarrative feedback into correspond-\ning Accreditation Council for Gradu-\nate Medical Education Milestone 2.0 \nsubcompetenciesâ€‹â€‹.\nDevelop-\nment & \nValidation \nStudy\nAnesthesiology; \n376 Residents; \nFaculty\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nThe model performed well for professionalism, interpersonal and \ncommunication skills, and practice-based learning (AUC 0.79, \n0.79, and 0.75), with fair to excellent results for medical knowl-\nedge and patient care (AUC 0.66–0.84 and 0.63–0.88), but poorly \nfor systems-based practice (AUC 0.59). It also provided quick, \norganized feedback for trainees via a web-based application.\nBrown [113] 2024; Evaluating the \nImpact of Assessment Metrics for \nSimulated Central Venous Catheter-\nization Training.; United States\nTo examine the effectiveness of the \nDHRT in training residents on needle \ntip tracking and aspiration skills.\nObserva-\ntion Study\nMultiple; 163 \nResidents\nDecision support sys-\ntems; Simulations; AI vs. \nStandard methods\nTip tracking rates above 40% were 2.3 times more likely to result \nin successful venous access than below 10%, and aspiration rates \nabove 80% were 2.6 times more likely to result in success than \nbelow 10%. Proper techniques reduced complications, with resi-\ndent performance improving in all metrics except tip tracking\nBrunye [106] 2023; Machine learn-\ning classification of diagnostic ac-\ncuracy in pathologists interpreting \nbreast biopsies; United States\nTo explore the feasibility of using \nmachine learning to predict accurate \nversus inaccurate diagnoses made \nby pathologists based on their spa-\ntiotemporal viewing behavior when \nevaluating digital breast biopsy \nimages.\nPredictive \nStudy\nPathology; \n140 Residents; \nFaculty\nMachine Learning; Real \nworld; AI vs. Human \nperformance\nRandom Forest classifier achieved the best performance with a \ntest accuracy of 0.81 and AUROC of 0.86. Attention distribution \nand focus on critical regions predicted diagnostic accuracy, with \nincremental improvements from case-level and pathologist-level \ninformation.\nBurk-Rafel [58] 2021; Development \nand Validation of a Machine Learn-\ning-Based Decision Support Tool for \nResidency Applicant Screening and \nReview.; United States\nTo develop and validate a machine \nlearning-based decision support tool \n(DST) for residency applicant screen-\ning and review.\nDevelop-\nment & \nValidation \nStudy\nMedicine; 8243 \nResidency \napplicants\nMachine Learning; Real \nworld; AI vs. Human \nperformance\nThe Random Forest classifier showed high performance (AUROC \n0.95, AUPRC 0.76, sensitivity 91%, specificity 85%). Removing \nUSMLE scores slightly reduced performance (AUROC 0.94, AUPRC \n0.72). The DST identified 20 initially overlooked applicants for \ninterviews.\nChassagnon [98] 2023; Learning \nfrom the machine: AI assistance is \nnot an effective learning tool for \nresident education in chest x-ray \ninterpretation; France\nTo assess whether a computer-aided \ndetection (CADe) system could \nserve as a learning tool for radiol-\nogy residents in chest X-ray (CXR) \ninterpretation.\nObserva-\ntion Study\nRadiology; 8 \nResidents\nDeep Learning; Real \nworld; AI vs. Human \nperformance\nThe CADe system improved residents’ sensitivity (43–53%), \nspecificity (90–94%), and accuracy (81–86%) during use, but \nperformance returned to baseline post-intervention, indicating \ntemporary enhancement.\nChen [74] 2019; Developing a More \nResponsive Radiology Resident \nDashboard; United States\nTo develop and evaluate a radiology \nresident dashboard (Trove) that uses \nNLP and machine learning to track \nand visualize resident caseloads, \nproviding insights into their training \nprogress and identifying gaps.\nDevelop-\nment & \nValidation \nStudy\nRadiol-\nogy; No human \nparticipants\nMachine Learning; \nNatural Language \nProcessing; Other: Deep \nLearning; Real world; No \ncomparator\nCNNs achieved 96.4% accuracy for large datasets across various \nICD codes, while GRUs achieved an even higher accuracy of \n97.0%, demonstrating high model performance.\nChen [23] 2023; Radiology Residents \nPerceptions of Artificial Intelligence: \nNationwide Cross-Sectional Survey \nStudy; China\nRadiology residents perception of AI Survey \nstudy\nRadiology; 3666 \nResidents\nGeneral AI; Machine \nLearning; Real world; No \ncomparator\n74% of respondents see AI as non-threatening and beneficial to \nworkflow, while 95% of residents want AI education, yet 20% of \nprograms lack formal AI training.\nTable 2 (continued)\n \nPage 9 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nCheung [86] 2023; ChatGPT versus \nhuman in generating medical \ngraduate exam multiple choice \nquestions-A multinational prospec-\ntive study (Hong Kong S.A.R., \nSingapore, Ireland, and the United \nKingdom).; Multinational\nTo assess the quality of MCQs \nproduced by ChatGPT for graduate \nmedical examinations compared \nto questions written by university \nprofessoriate staff.\nSurvey \nstudy\nGeneral; 7 \nFaculty\nLarge Language Models; \nReal world; AI vs. Human \nperformance\nAI-generated MCQs matched human-created ones in quality \nacross most domains but were slightly inferior in relevance. Chat-\nGPT notably produced MCQs much faster than humans.\nCohen [81] 2023; Performance of \nChatGPT in Israeli Hebrew OBGYN \nnational residency examinations.; \nIsrael\nTo evaluate the performance of \nChatGPT in Hebrew OBGYN (Phase 1) \nexamination and compare it to the \nperformance of real-life OBGYN resi-\ndents and ChatGPT’s performance in \nEnglish medical exams.\nNon-Ran-\ndomized \nExperimen-\ntal Study\nObGyn; not \nspecified \nResidents\nLarge Language Models; \nReal world; AI vs. Human \nperformance\nChatGPT scored 38.7% on Hebrew OBGYN questions, significantly \nlower than residents’ 68.4% (p < 0.001), and performed better on \nEnglish tests with 60.7% accuracy compared to 38.7% in Hebrew \n(p < 0.001)\nCollado-Mesa [19] 2018; The Role of \nArtificial Intelligence in Diagnostic \nRadiology: A Survey at a Single Ra-\ndiology Residency Training Program; \nUnited States\nTo establish a baseline understand-\ning of the awareness and percep-\ntions of AI among radiology trainees \nand attending radiologists.\nSurvey \nstudy\nRadiology; 69 \nResidents; Fel-\nlows; Faculty\nGeneral AI; Real world; No \ncomparator\n36% of participants hadn’t read an AI article in the past year, 29% \nused AI tools daily, and trainees were more concerned about AI’s \nimpact on future job roles compared to attending radiologists.\nDimitroyannis [104] 2023; Residency \nEducation Practices in Endoscopic \nSkull Base Surgery; United States\nTo analyze endoscopic skull base \nsurgery education methods by \nsurveying NASBS members.\nSurvey \nstudy\nNeurosurgery, \nOtolaryngology; \n60 Faculty\nGeneral AI; Real world; No \ncomparator\nStandardization and use of simulation, AI, and VR should be at the \nforefront of educational practices in the next 5–10 years.\nDiPietro [107] 2019; Segmenting \nand classifying activities in robot-\nassisted surgery with recurrent \nneural networks.; United States\nTo improve automated segmenta-\ntion and classification of surgical \nactivities in robot-assisted surgery \nusing RNNs\nObserva-\ntion Study\nGeneral; 15 sub-\njects for MISTIC-\nSL dataset; 8 \nsubjects for \nJIGSAWS dataset \nResidents; Medi-\ncal Students\nRecurrent Neural Net-\nworks (RNNs); Simula-\ntions; AI vs. Standard \nmethods\nThis study utilized recurrent neural networks (RNNs) to segment \nand classify activities in robot-assisted surgery, comparing \nfour RNN architectures. The models achieved state-of-the-art \nperformance, with the GRU architecture yielding the lowest error \nrate of 8.6% and normalized edit distance of 9.3% for maneuver \nrecognition. The results demonstrated that RNNs could effectively \nrecognize both gestures and higher-level maneuvers, providing a \nfoundation for automated, targeted assessment and feedback in \nsurgical training.\nDrum [57] 2023; Using Natural \nLanguage Processing and Machine \nLearning to Identify Internal Medi-\ncine-Pediatrics Residency Values in \nApplications.; United States\nTo use NLP and ML to identify values \nassociated with resident success in \ninternal medicine-pediatrics resi-\ndency applications\nObserva-\ntion Study\nInternal Med-\nPediatrics; 185 \nResidency \napplicants\nMachine Learning; Natu-\nral Language Processing; \nReal world; AI vs. Human \nperformance\nMLM showed moderate sensitivity (0.64), high specificity (0.97), \nPPV (0.64), NPV (0.97), and an F1 score of 0.63 in identifying values \nfrom unstructured narrative data, with the mean number of an-\nnotations per application significantly correlating with interview \ninvitation status.\nEbina [114] 2022; Objective evalua-\ntion of laparoscopic surgical skills in \nwet lab training based on motion \nanalysis and machine learning.; \nJapan\nTo build a skill assessment system \nproviding objective feedback to \ntrainees based on motion metrics of \nlaparoscopic surgical instruments\nDevelop-\nment & \nValidation \nStudy\nMultiple; 70 \nResidents; \nFaculty; Medical \nStudents\nMachine Learning; Simu-\nlations; No comparator\nSpeed-related parameters positively correlated with mean GOALS \nscores, while efficiency-related parameters negatively correlated. \nSVR had the highest accuracy in the tissue dissection task (MAE-\nmedian = 2.2352), and PCA-SVR in the parenchymal-suturing task \n(MAEmedian = 1.2714)\nTable 2 (continued)\n \nPage 10 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nEl Saadawi [70] 2008; A natural lan-\nguage intelligent tutoring system \nfor training pathologists: implemen-\ntation and evaluation.; United States\nTo develop and evaluate a Natural \nLanguage Interface (NLI) for an \nIntelligent Tutoring System (ITS) \nin Diagnostic Pathology, focusing \non teaching residents to examine \npathologic slides and write accurate \npathology reports while providing \nimmediate feedback on errors.\nDevelop-\nment & \nValidation \nStudy\nPathology; 20 \nResidents\nNatural Language Pro-\ncessing; Simulations; No \ncomparator\nNLP system showed a precision of 0.90 and a recall of 0.84. \nReport writing skills significantly improved after one tutoring \nsession, with a four-fold increase in learning gains, unaffected by \nthe timing of feedback.\nFang [96] 2022; Artificial intelli-\ngence-based pathologic myopia \nidentification system in the ophthal-\nmology residency training program; \nChina\nTo evaluate the effectiveness of \nthe AI-based pathologic myopia \n(PM) identification system in the \nophthalmology residency training \nprogram and assess the residentsâ€™ \nfeedback on this system.\nRandom-\nized \ncontrolled \ntrial\nOphthalmology; \n90 Residents\nDecision support sys-\ntems; Real world; AI vs. \nHuman performance\nPost-lecture scores significantly improved in the AI group \n(p < 0.0001) compared to both traditional groups, which showed \nno significant improvement (p = 0.302 and p = 0.158). Residents \nreported high satisfaction with the AI system, noting its effective-\nness in enhancing their learning and understanding of PM\nFeng [92] 2023; Deep Neural \nNetwork Augments Performance \nof Junior Residents in Diagnosing \nCOVID-19 Pneumonia on Chest \nRadiographs; Singapore\nTo assess the effectiveness of a \ndeep neural network in distinguish-\ning COVID-19 from other types of \npneumonia, and to determine its \npotential contribution to improving \nthe diagnostic precision of junior \nresidents.\nNon-Ran-\ndomized \nExperimen-\ntal Study\nRadiology; 3 \nResidents\nDeep neural net-\nwork; Real world; No \ncomparator\nThe AI model achieved an AUC of 0.9520 on the internal test \nset and 0.8588 on the external test set. With AI assistance, junior \nresidents showed significant improvements in diagnostic ac-\ncuracy: JR1’s sensitivity for COVID-19 pneumonia increased from \n0.3889 to 0.6250, and specificity for non-pneumonia from 0.9091 \nto 0.9339. Similarly, JR2’s specificity for non-pneumonia improved \nfrom 0.9008 to 0.9835.\nGao [102] 2019; Constructing a \nChinese electronic medical record \ncorpus for named entity recognition \non resident admit notes.; China\nTo construct a Chinese electronic \nmedical record corpus for named \nentity recognition on resident admit \nnotes and to evaluate the effective-\nness of an end-to-end deep neural \nnetwork model for medical named \nentity recognition (NER).\nDevelop-\nment & \nValidation \nStudy\nGeneral; 255 \nnotes Resident\nDeep neural net-\nwork; Real world; No \ncomparator\nThe annotation scheme and the model for NER in this paper are \neffective to extract medical named entity from RANs and provide \nthe basis for fully excavating the patients information.\nGates [115] 2023; Association of \nGender and Operative Feedback \nQuality in Surgical Residents.; United \nStates\nTo explore the quality of narrative \nfeedback among trainee-faculty gen-\nder dyads in an operative workplace-\nbased assessment (WBA).\nObserva-\ntion Study\nGeneral Surgery; \n2319 Residents\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nNLP analyzed 67,434 operative performance evaluations for \ngender-based differences in feedback quality among surgical \nresidents. Male faculty provided more narrative feedback than \nfemale faculty. Female residents received higher quality feedback \ncompared to male residents, but no significant differences were \nfound based on faculty-resident gender dyad.\nGong [101] 2021; Characterizing \nstyles of clinical note production \nand relationship to clinical work \nhours among first-year residents.; \nUnited States\nTo characterize variation in clinical \ndocumentation production patterns \nand their relationship to work hours\nObserva-\ntion Study\nMedicine; Resi-\ndents; 50\nMachine Learning; Real \nworld; No comparator\nUnsupervised machine learning analyzed clinical note produc-\ntion among 50 first-year residents, identifying 10 note-level and \n5 user-level patterns. Residents writing in dispersed sessions had \nhigher median daily work hours, while single-session writers had \nlower hours.\nTable 2 (continued)\n \nPage 11 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nGray [56] 2023; Examining Implicit \nBias Differences in Pediatric Surgical \nFellowship Letters of Recommen-\ndation Using Natural Language \nProcessing.; United States\nTo analyze the prevalence and type \nof bias in letters of recommendation \n(LOR) for pediatric surgical fellowship \napplications from 2016–2021 using \nnatural language processing (NLP).\nObserva-\ntion Study\nPediatric \nsurgery; 182 \nResidency \napplicants\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nNLP analyzed bias in 701 pediatric surgical fellowship LORs from \n182 applicants (2016–2021). Results showed Black applicants had \nthe highest mean polarity, while Hispanic applicants had the low-\nest. Significant differences in “anger” intensity were found among \nBlack, Asian, and Hispanic applicants. NLP effectively identified \nsubtle biases, impacting fellowship matching.\nGupta [78] 2023; Applying GPT-4 to \nthe Plastic Surgery Inservice Train-\ning Examination; United States\nTo determine if GPT-4 could be \nexploited as an instrument for plastic \nsurgery graduate medical education \nby evaluating its performance on \nthe Plastic Surgery Inservice Training \nExamination (PSITE).\nObserva-\ntion Study\nPlastic surgery; \nNo human \nparticipant\nLarge Language \nModels; Simulations; No \ncomparator\nGPT-4 achieved 77.3% accuracy on the PSITE exam, using logical \nreasoning in 95.0% of cases, internal information in 98.3%, and \nexternal information in 97.5%. A significant difference in logical \nreasoning usage was noted between correct and incorrect \nanswers.\nHan [91] 2020; Augmented Intel-\nligence Dermatology: Deep Neural \nNetworks Empower Medical Profes-\nsionals in Diagnosing Skin Cancer \nand Predicting Treatment Options \nfor 134 Skin Disorders.; South Korea\nThe performance of CNN needs to \nbe tested in\nan environment similar to real \npractice, which requires dis-\ntinguishing skin cancer from numer-\nous other skin disorders\nincluding inflammatory and infec-\ntious conditions. In addi-\ntion, the robustness and repeatability \nof this approach require\nfurther validations\nPredictive \nStudy\nDermatology; \n47 Residents; \nFaculty\nConvolutional Neural \nNetworks; Simulations; \nNo comparator\nThe algorithm improved clinicians’ malignancy prediction \nsensitivity by 12.1% and specificity by 1.1% (both P < 0.0001), \nand increased non-medical professionals’ sensitivity by 83.8% \n(P < 0.0001). It also enhanced top-1 and top-3 accuracies for four \ndoctors in classifying 134 diseases.\nHernandez-Rodriguez [116] 2023; \nDevelopment and validation of \nan educational software based in \nartificial neural networks for training \nin radiology (JORCAD) through an \ninteractive learning activity.; Spain\nTo validate an educational software \ntool (JORCAD) for training residents \nin Radiology\nDevelop-\nment & \nValidation \nStudy\nRadiology; \n26 Residents; \nFaculty\nCNN; Real world; No \ncomparator\nJORCAD had high mean ratings for software tools (4.12–4.77), \nsignificant learning utility (4.54–4.77), and strong agreement \namong participants on the software’s effectiveness for training in \nthorax CT and mammography cases. The CAD tool was particu-\nlarly effective in reinforcing diagnostic confidence and enhancing \neducational experiences \nHolden [39] 2019; Machine learning \nmethods for automated technical \nskills assessment with instructional \nfeedback in ultrasound-guided \ninterventions.; Canada\nDevelop and validate skills assess-\nment methods in ultrasound-guided \ninterventions that are transparent \nand configurable\nDevelop-\nment & \nValidation \nStudy\nGeneral; 24 Resi-\ndents; Faculty\nOther: Decision Trees, \nFuzzy Inference Systems; \nSimulations; AI vs. Human \nperformance; AI vs. Stan-\ndard methods\nDecision Trees had median errors of 1.7 in-plane and 1.5 out-of-\nplane, while Fuzzy Inference Systems had errors of 1.8 in-plane \nand 3.0 out-of-plane, performing comparably to state-of-the-art \nmethods and providing useful feedback.\nHolmes [80] 2023; Evaluating Large \nLanguage Models in Ophthalmol-\nogy; China, USA\nEvaluate and compare the perfor-\nmance of three LLMs in answering \nophthalmology questions\nNon-Ran-\ndomized \nExperimen-\ntal Study\nOphthalmology; \n18 Residents; \nFaculty; Medical \nStudents\nLarge Language Models; \nSimulations; AI vs. Human \nperformance; AI vs. other \nAI tools\nGPT-4 performed at a level comparable to attending physicians, \nwhile GPT-3.5 and PaLM2 were slightly below the master’s level. \nGPT-4 also demonstrated higher stability (mean correlation \n0.83) and confidence (45% accuracy) compared to GPT-3.5 and \nPaLM2. All LLMs, outperformed medical undergraduates in \ngeneral assessments.\nTable 2 (continued)\n \nPage 12 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nHomayounieh [90] 2021; An Artifi-\ncial Intelligence-Based Chest X-ray \nModel on Human Nodule Detection \nAccuracy from a Multicenter Study; \nUSA, Germany\nTo assess if a novel AI algorithm can \nhelp detect pulmonary nodules on \nradiographs at different levels of \ndetection difficulty\nNon-Ran-\ndomized \nExperimen-\ntal Study\nRadiology; 9 \nResidents & \nFaculty\nMachine Learning; Real \nworld; AI vs. Human \nperformance\nAI-aided interpretation improved mean detection accuracy \nby 6.4% (95% CI, 2.3–10.6%) and partial AUCs by 5.6% (95% CI, \n− 1.4–12.0%). Junior radiologists showed greater sensitivity im-\nprovement (12% vs. 9%) with AI aid, while senior radiologists had \nsimilar specificity improvement (4%).\nHuisman [22] 2021; An international \nsurvey on AI in radiology in 1,041 \nradiologists and radiology residents \npart 1: fear of replacement, knowl-\nedge, and attitude.; Multinational\nInvestigate the knowledge and \nattitude towards AI among radiolo-\ngists and residents, focusing on fear \nof replacement by AI and positive \nattitudes towards AI .\nSurvey \nstudy\nRadiology; Resi-\ndents; Faculty; \n1041\nGeneral AI; Deep learn-\ning; Real world; No \ncomparator\nLimited AI knowledge is linked to a fear of being replaced, \nwhereas intermediate to advanced knowledge corresponds with \npositive attitudes. This fear is more common among males and \nthose with basic AI understanding. Conversely, positive attitudes \nare typically found in younger individuals, males, those with \nscientific backgrounds, and active professional social media users.\nHumar [79] 2023; ChatGPT Is Equiva-\nlent to First-Year Plastic Surgery \nResidents: Evaluation of ChatGPT \non the Plastic Surgery In-Service \nExamination; United States\nEvaluate the performance of Chat-\nGPT on the Plastic Surgery In-Service \nExamination and compare it to resi-\ndents’ performance nationallyâ€‹â€‹.\nObserva-\ntion Study\nPlastic surgery; \nNo human \nparticipants\nLarge Language Models; \nReal world; AI vs. Human \nperformance\nChatGPT answered 55.8% of exam questions correctly, perform-\ning on par with a first-year integrated plastic surgery resident but \npoorly compared to more advanced residents.\nJalali [117] 2017; LiveBook: Compe-\ntence Assessment with Virtual-\nPatient Simulations; Canada\nTo present and evaluate the Live-\nBook interactive simulation system \nfor competence assessment with \nvirtual-patient simulations\nDevelop-\nment & \nValidation \nStudy\nPediatrics; 23 \nResidents\nNatural Language Pro-\ncessing; Decision support \nsystems; Simulations; No \ncomparator\nThe article discusses the creation and ongoing validation process \nof Livebook, noting that no actual study results have been com-\npleted at the time of publication.\nJohnstone [64] 2023; Artificial \nintelligence software can generate \nresidency application personal \nstatements that program directors \nfind acceptable and difficult to \ndistinguish from applicant composi-\ntions; United States\nTo determine if AI-generated per-\nsonal statements for anesthesiology \nresidency applications are accept-\nable to program directors\nNon-Ran-\ndomized \nExperimen-\ntal Study\nAnesthesiol-\nogy; 94 Resi-\ndency program \ndirectors\nNatural Language Pro-\ncessing; Real world; No \ncomparator\n61% of program directors did not identify AI-generated state-\nments related to athletics, and 80% failed to detect those related \nto cooking. Overall, AI-generated statements were rated as ac-\nceptable and were challenging to distinguish from those written \nby humans\nKelahan [71] 2016; Call Case \nDashboard: Tracking R1 Exposure \nto High-Acuity Cases Using Natural \nLanguage Processing.; United States\nTo track first-year radiology residents’ \nexposure to high-acuity cases using \nNatural Language Processing (NLP).\nObserva-\ntion Study\nRadiology; 4 \nResidents\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nNLP achieved a specificity of 95-100% and a sensitivity of 92.3-\n100% for various conditions, accurately tracking high-acuity cases \nand identifying educational deficiencies in resident training.\nKennedy [21] 2022; Canadian \nOncology Residents Knowledge \nof and Attitudes Towards Artificial \nIntelligence and Machine Learning; \nCanada, Brazil\nExamine perceptions and knowledge \nof Canadian oncology residents \nand fellows with respect to AI \ntechnologies\nSurvey \nstudy\nRadiation Oncol-\nogy, Medical \nOncology; \n57 Residents; \nFellows\nGeneral AI; Real world; No \ncomparator\n67% of residents recognized the importance of AI education, and \n70% wanted to learn more, yet 88% lacked formal AI education. \nAdditionally, 98% believed AI could replace some to all medical \nactivities in radiation oncology, and 63% felt limited knowledge \nin math and programming hindered their understanding of AI.\nTable 2 (continued)\n \nPage 13 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nKocerTulgar [27] 2023; Anesthesi-\nologists’ Perspective on the Use of \nArtificial Intelligence in Ultrasound-\nGuided Regional Anaesthesia in \nTerms of Medical Ethics and Medical \nEducation: A Survey Study.; Turkey\nDetermine how anesthesiology and \nreanimation specialists in Turkey \nperceive the use of AI in ultrasound-\nguided regional anesthetic applica-\ntions in terms of medical ethics and \neducation, as well as their perspec-\ntives on potential ethical issues.\nSurvey \nstudy\nAnesthesiology; \n285 Faculty\nMachine Learning; Real \nworld; No comparator\n80% of respondents believed AI would benefit patients, 86.7% \nsaw advantages for resident training, 81.4% for post-graduate \nmedical education, and 80.7% thought it would reduce practice \ncomplications. Ethically, 78.25% were fine with anonymous \nsonographic data capture, but 67% had concerns about account-\nability for AI inaccuracies.\nKumar [41] 2012; Objective mea-\nsures for longitudinal assessment \nof robotic surgery training.; United \nStates\nDevelopment and validation of a \nnovel automated and objective \nframework for the assessment of \ntraining in robotic surgery\nDevelop-\nment & \nValidation \nStudy\nCardiothoracic \nsurgery; 12 Resi-\ndents; Fellows; \nFaculty\nOther: Automated mo-\ntion recognition system; \nReal world; Simulations; \nNo comparator; AI vs. \nHuman performance\nThe system could objectively differentiate between opera-\ntional and technical skills. Over time, trainees’ skill parameters \nconverged toward those of expert surgeons. The study reported \nsignificant improvements in task completion times and motion \nefficiency, with high accuracy in classifying expert versus non-\nexpert performance (suturing task: 83.33%, manipulation task: \n76.25%\nLee [97] 2020; Application of deep \nlearning to the diagnosis of cervical \nlymph node metastasis from thyroid \ncancer with CT: external validation \nand clinical utility for resident train-\ning; South Korea\nTo validate a deep learning model’s \ndiagnostic performance for CT diag-\nnosis of cervical LNM from thyroid \ncancer and evaluate its clinical utility \nfor resident training.\nDevelop-\nment & \nValidation \nStudy\nRadiology; \n7 Residents; \nFaculty\nOther: CNN; Real world; \nAI vs. other AI tools\nThe overall AUROC of the eight deep learning algorithms was \n0.846, with Xception being the best-performing model (AUROC \n0.884). Its diagnostic accuracy, sensitivity, specificity, positive pre-\ndictive value, and negative predictive value were 82.8%, 80.2%, \n83.0%, 83.0%, and 80.2%, respectively. Introducing the CAD \nsystem provided more assistance to underperforming trainees \n(p = 0.046) and significantly increased overall confidence levels \nfrom 3.90 to 4.30 (p < 0.001).\nLin [72] 2014; A content-boosted \ncollaborative filtering algorithm for \npersonalized training in interpreta-\ntion of radiological imaging.; China, \nCanada\nDevelop a personalized training \nprogram in radiology education \nusing a content-boosted collabora-\ntive filtering (CBCF) algorithm\nDevelop-\nment & \nValidation \nStudy\nRadiology; 15 \nResidents\nOther: Content-Based \nFiltering (CBF), Collab-\norative Filtering (CF); Real \nworld; AI vs. other AI tools\nCBCF outperformed pure CBF and CF methods by 13.33% and \n12.17% in prediction precision, respectively, indicating its po-\ntential for developing personalized training systems in radiology \neducation.\nLui [52] 2018; Tracking resident \ncognitive maturation with natural \nlanguage processing; United States\nTo develop and test software for \ntracking the cognitive maturation \nof medical residents using NLP to \nanalyze freetext evaluations.\nDevelop-\nment & \nValidation \nStudy\nEmergency \nMedicine; 100 \nResidents\nNatural Language Pro-\ncessing; Simulations; No \ncomparator\nThe algorithm correctly identified 22 out of 25 notes where the \n“laggard” archetype predominated, demonstrating its accuracy. \nFeedback mechanisms were validated through the significant \ncorrelation between the identified linguistic markers and the \ndevelopmental progression noted by faculty, supporting the sys-\ntem’s effectiveness in real-time tracking and tailored educational \ninterventions\nTable 2 (continued)\n \nPage 14 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nLum [83] 2023; Can Artificial Intel-\nligence Pass the American Board of \nOrthopaedic Surgery? An Analysis \nof 3900 Questions; United States\nTo determine if ChatGPT can pass \nthe American Board of Orthopaedic \nSurgery Examination\nNon-Ran-\ndomized \nExperimen-\ntal Study\nOrthopaedics; \nNo human \nparticipant\nLarge Language Models; \nReal world; AI vs. Human \nperformance\nChatGPT correctly answered 47% of OITE questions, with accu-\nracy decreasing as question complexity increased. It performed \nat the 40th percentile for PGY1, 8th percentile for PGY2, and \n1st percentile for PGY3-PGY5, not meeting the 10th percentile \npassing threshold for PGY5s on the ABOS exam. By question \ntaxonomy, it scored 54% on recognition and recall, 51% on \ncomprehension and interpretation, and 34% on application of \nknowledge questions.\nMadhavan [118] 2014; Evaluation of \nDocumentation Patterns of Trainees \nand Supervising Physicians Using \nData Mining.; United States\nEvaluate documentation patterns of \ntrainees and supervising physicians \nusing data mining\nObserva-\ntion Study\nMultiple; 26,802 \nnotes Residents; \nFaculty\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nMost resident notes were entered in the afternoon (33%) and late \nmorning (31%). Attending physicians provided teaching attesta-\ntions within 24 h for 73% of records. Surgical residents placed \nnotes more often before noon, while nonsurgical faculty typically \nattested within 24 h.\nMahajan [85] 2023; Assessment of \nArtificial Intelligence Performance \non the Otolaryngology Residency \nIn-Service Exam; United States\nDetermine the potential use and \nreliability of a large language model \n(ChatGPT) for answering otolaryn-\ngology in-service exam questions \nand assess its efficacy for surgical \ntrainees.\nObserva-\ntion Study\nOtolaryngol-\nogy; No human \nparticipant\nLarge Language \nModels; Real world; No \ncomparator\nChatGPT’s performance varied by question difficulty, with overall \ncorrect answer and explanation rates of 53% and 54%. For easy \nquestions, the rates were 68% and 69%; for moderate questions, \n52% and 53%; and for hard questions, 38% and 39%. Performance \ndifferences were statistically significant across difficulty levels.\nMahtani [63] 2023; A New Tool \nfor Holistic Residency Application \nReview: Using Natural Language \nProcessing of Applicant Experiences \nto Predict Interview Invitation.; \nUnited States\nDevelop an NLP tool to automate the \nreview of residency applicantsâ€™ \nnarrative experience entries and \npredict interview invitations.\nDevelop-\nment & \nValidation \nStudy\nMedicine; 6403 \nResidency \napplicants\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nNLP-only model achieved an AUROC of 0.80 and AUPRC of 0.49, \nwhile the structured data-only model had superior performance \nwith an AUROC of 0.92 and AUPRC of 0.74. Combining NLP \nand structured data, the model maintained an AUROC of 0.92 \nand AUPRC of 0.73. Terms indicating active leadership, research \ninvolvement, and efforts in social justice and health disparities \npositively correlated with interview invitations\nMarchetti [94] 2020; Computer algo-\nrithms show potential for improving \ndermatologists’ accuracy to diag-\nnose cutaneous melanoma: Results \nof the International Skin Imaging \nCollaboration 2017.; United States\nDetermine if computer algorithms \nfrom an international melanoma \ndetection challenge can improve \ndermatologist melanoma diagnostic \naccuracy.\nObserva-\ntion Study\nDermatology; \n17 Residents; \nFaculty\nDeep learning, neural \nnetworks; Real world; AI \nvs. Human performance\nThe top computer algorithm outperformed dermatologists and \nresidents in melanoma classification with an ROC area of 0.87, \ncompared to 0.74 and 0.66, respectively (p < 0.001). Dermatolo-\ngists’ sensitivity was 76.0%, while the algorithm’s specificity was \nhigher at 85.0% versus 72.6% (p = 0.001). Imputing algorithm \nclassifications for uncertain dermatologist evaluations increased \nsensitivity to 80.8% and specificity to 72.8%.\nMarquis [25] 2023; Results of the \n2020 Survey of the American Alli-\nance of Academic Chief Residents in \nRadiology; United States\nSummarize the 2020 A3CR2 chief res-\nident survey, focusing on residency \nprogram practices, benefits, training \nchoices, and perceptions on corpora-\ntization, NPPs, and AI in radiology.\nSurvey \nstudy\nRadiology; 174 \nChief residents\nMachine Learning; \nGeneral AI; Real world; No \ncomparator\n74% of respondents believe AI does not threaten the job market \nand could improve workflow efficiency. Despite this, 95% of resi-\ndents expressed a need for AI and machine learning education, \nbut 20% of programs currently do not offer formal AI education.\nTable 2 (continued)\n \nPage 15 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nMerritt [68] 2022; Implementa-\ntion and Evaluation of an Artificial \nIntelligence Driven Simulation to \nImprove Resident Communication \nWith Primary Care Providers; United \nStates\nImplement and evaluate an AI-driven \nsimulation to improve resident \ncommunication with primary care \nproviders (PCPs).\nDevelop-\nment & \nValidation \nStudy\nPediatrics; 17 \nResidents\nOther: IBM Watson’s \nintegrated cognitive \ncomputing and linguistic \nmodeling; Simulations; \nNo comparator\n94% of participants were fully engaged during the AI simulation. \nMost found it beneficial: 76.4% said it reinforced key communica-\ntion elements, and 70.6% believed it would positively impact \nfuture communications with PCPs. The AI simulation was rated \nequally or more favorably than reading (76.5%), didactics (70.6%), \nand online education (88.3%), but less favorably than live simu-\nlated encounters (64.7%). Only 52.9% saw it as less effective than \nreal-time observation.\nMuntean [73] 2023; Artificial Intelli-\ngence for Personalised Ophthalmol-\nogy Residency Training; Romania\nTo develop an AI framework for per-\nsonalized case-based ophthalmology \nresidency training\nDevelop-\nment \nStudy\nOphthalmology; \n10 Residents\nDeep learning; Real \nworld; No comparator\nPotential to standardize and personalize ophthalmology resi-\ndency training\nNeves [51] 2021; Using Machine \nLearning to Evaluate Attending \nFeedback on Resident Performance.; \nUnited States\nUse machine learning to evaluate \nattending feedback on resident \nperformance\nPredictive \nStudy\nAnesthesiology; \n146 Faculty\nMachine Learning; Real \nworld; No comparator\nModels predicting feedback traits achieved 74.4-82.2% accuracy. \nUtility category predictions were 82.1% accurate, with 89.2% \nsensitivity and 89.8% precision for low-utility predictions. Quality \ncategory predictions were 78.5% accurate, with 86.1% sensitiv-\nity and 85.0% precision for low-quality predictions. The program \nprocessed data and generated predictions within minutes, \ncompared to 15 h for manual scoring of 200 comments over two \nweeks.\nNori [75] 2023; Capabilities of GPT-4 \non Medical Challenge Problems; \nUnited States\nTo investigate the capabilities \nof GPT-4 on medical challenge \nproblems.\nNon-Ran-\ndomized \nExperimen-\ntal Study\nGeneral; \nNo human \nparticipant\nLarge Language Models; \nTheoretical; AI vs. other \nAI tools\nGPT-4 surpasses the USMLE passing score by over 20 points, \noutperforming GPT-3.5 and Med-PaLM. It shows improved \ncalibration, accurately predicting its correct answers’ likelihood. \nA case study also highlights GPT-4’s ability to explain medical \nreasoning, tailor explanations to students, and create interactive \ncounterfactual scenarios.\nOlsson [93] 2006; Decision support \nfor the initial triage of patients with \nacute coronary syndromes.; Sweden\nTo develop an automated tool for \nECG analysis for detecting transmural \nischaemia and assess its impact on \ninterns’ classifications\nDevelop-\nment & \nValidation \nStudy\nCardiology; 3 \nResidents\nArtificial Neural Network; \nReal world; AI vs. Human \nperformance\nThree interns improved their sensitivity from 68–93% and speci-\nficity from 92–87% with neural network assistance. The neural \nnetwork alone achieved 95% sensitivity and 88% specificity. The \n23–26% sensitivity increase for all three interns was statistically \nsignificant (P < 0.001).\nOoi [24] 2021; Attitudes toward ar-\ntificial intelligence in radiology with \nlearner needs assessment within \nradiology residency programmes: a \nnational multi-programme survey; \nSingapore\nTo assess attitudes and learner needs \nof radiology residents and faculty \nregarding AI/ML in radiology\nSurvey \nstudy\nRadiology; \n125 Residents; \nFaculty\nMachine Learning; Other: \nGeneral AI; Real world; No \ncomparator\n88.8% of respondents believe AI/ML will drastically change \nradiology, with 76% finding it exciting and 80% still choosing to \nspecialize in it. While 64.8% consider themselves AI/ML novices, \n76% want to improve their knowledge, and 67.2% are interested \nin AI/ML research. A majority (84.8%) think AI/ML should be \nincluded in residency curricula, but 59.2% report insufficient AI/\nML education in their programs. Male and tech-savvy individuals \nshow better technical understanding and engagement in AI/ML \nactivities.\nTable 2 (continued)\n \nPage 16 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nOropesa [40] 2014; Supervised \nclassification of psychomotor \ncompetence in minimally invasive \nsurgery based on instruments mo-\ntion analysis.; Netherlands\nTo compare three classification \nmethods for assessing psychomotor \nskills in minimally invasive surgery \n(MIS).\nNon-Ran-\ndomized \nExperimen-\ntal Study\nGeneral Surgery; \n42 Residents; \nFaculty; Medical \nStudents\nOther: Support Vector \nMachines, Linear Discrim-\ninant Analysis, Adaptive \nNeuro-Fuzzy Inference \nSystems; Simulations; AI \nvs. other AI tools\nThe classifiers achieved mean accuracies of 71% (LDA), 78.2% \n(SVM), and 71.7% (ANFIS). No statistically significant differences \nwere found between the classifiers, demonstrating that machine \nlearning classifiers can effectively assess surgical competence, \nsuggesting their integration into surgical training programs for \nobjective evaluations.\nOrtiz [62] 2023; Words matter: using \nnatural language processing to pre-\ndict neurosurgical residency match \noutcomes.; United States\nTo compare the performance of \nmachine learning models trained on \napplicant NLORs and demographic \ndata to predict neurosurgical resi-\ndency match outcomes and investi-\ngate whether narrative language is \npredictive of SLOR rankings.\nPredictive \nStudy\nNeurosurgery; \n391 Residency \napplicants\nNatural Language Pro-\ncessing; Real world; AI vs. \nHuman performance\nBoth NLOR and demographic models predicted match out-\ncomes with similar effectiveness (AUC values of 0.75 and 0.80, \nrespectively). Words like “outstanding,” “seamlessly,” and “AOA” were \npredictive of match success. NLORs provided additional insights \ninto applicant fit beyond demographic data.\nOuyang [103] 2024; Leveraging \nHistorical Medical Records as a \nProxy via Multimodal Modeling \nand Visualization to Enrich Medical \nDiagnostic Learning; China\nEnhance the learning experience of \ninterns and novice physicians in di-\nagnostic skills through the use of ML \nmodels trained on historical medical \ndata and multimodal modeling and \nvisualization\nDevelop-\nment & \nValidation \nStudy\nGeneral; 5 Resi-\ndents; Faculty\nOthers: Multimodal mod-\nels including ClinicalBERT, \nConvNeXt, XGBoost; Real \nworld; No comparator\nIt achieved an accuracy of 91.6% (decision-level fusion) in \ndiagnosing cervical spine disorders. The system was validated \nthrough two case studies and expert interviews, demonstrating \nimproved learning outcomes for interns and novice physicians by \nproviding visual diagnostics and comparative analysis of patient \ndata\nPatel [65] 2023; Distinguishing \nAuthentic Voices in the Age of Chat-\nGPT: Comparing AI-Generated and \nApplicant-Written Personal State-\nments for Plastic Surgery Residency \nApplication.; United States\nTo explore whether residency ap-\nplication reviewers could discern \nChatGPT-generated personal state-\nments from those written by human \napplicants.\nNon-Ran-\ndomized \nExperimen-\ntal Study\nPlastic surgery; \n10 Residents; \nFaculty\nLarge Language Models; \nReal world; AI vs. Human \nperformance\nThere was no significant difference in ratings for readability, origi-\nnality, authenticity, and overall quality between computer-gener-\nated and applicant essays (all P > 0.05). Although raters were less \ninclined to grant interviews to computer-generated essays, the \ndifference was not significant (58% vs. 78%, P = 0.12).\nPaul [95] 2023; Impact of an Artificial \nIntelligence Algorithm on Diabetic \nRetinopathy Grading by Ophthal-\nmology Residents; United States\nTo determine whether AI signifi-\ncantly affects the performance of \ndiabetic retinopathy (DR) grading by \nophthalmology residents.\nNon-Ran-\ndomized \nExperimen-\ntal Study\nOphthalmology; \n4 Residents\nCNN; Simulations; AI vs. \nHuman performance\nThe study found no significant difference in five-class DR \ngrading performance with AI (QWK differences: +0.010–0.017, \np = 0.20–0.74). The PGY-3 resident had improved accuracy \n(+ 6.0%, p = 0.045) and specificity (71.8–80.0%, p = 0.019) with AI. \nAI increased intergrader agreement (FK + 0.072, p = 0.003) and \nconfidence in 3 out of 4 residents (p < 0.0001).\nPilon [61] 1997; Neural network \nand linear regression models in \nresidency selection; United States\nTo compare the effectiveness of \nlinear regression and neural network \nmodels in generating provisional \nrank lists of residency applicants.\nNon-Ran-\ndomized \nExperimen-\ntal Study\nEmergency \nMedicine; 74 \nResidency \napplicants\nArtificial Neural Network; \nReal world; AI vs. Stan-\ndard methods\nThe neural network matched the performance of the linear \nregression model, achieving correlation coefficients of 0.77 and \n0.74, respectively, with R² values of 59.4% for the neural network \nand 54.0% for the linear regression model.\nQuinn [36] 2023; The robot doesn’t \nlie: real-life validation of robotic per-\nformance metrics.; United States\nTo validate robotic performance \nmetrics against human observation \nin assessing resident participation \nduring robotic surgeries.\nDevelop-\nment & \nValidation \nStudy\nGeneral Surgery; \n4 Residents; \nFaculty\nOther: Robotic surgical \nsystem (Da Vinci Surgical \nSystem); Real world; AI vs. \nHuman performance\nRobotic metrics showed a high correlation with human observa-\ntions (r = 0.98, p < 0.0001), indicating strong agreement. Residents’ \nself-assessments and faculty assessments were found to be less \naccurate.\nTable 2 (continued)\n \nPage 17 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nReeder [18] 2022; Impact of artificial \nintelligence on US medical students’ \nchoice of radiology; United States\nTo examine the impact of AI on US \nmedical students’ choice of radiology \nas a career, and how such impact is \ninfluenced by students’ opinions on \nand exposures to AI and radiology.\nSurvey \nstudy\nRadiology; \n463 Medical \nStudents\nGeneral AI; Real world; No \ncomparator\nAI concerns reduced students’ preference for radiology as a first \nchoice (P < 0.001), with 17% deterred by issues like lesser under-\nstanding, perceived job reduction, and peer and professional \ninfluences. Students preferred AI education during their rotations.\nRees [59] 2023; Machine Learning \nfor The Prediction of Ranked Ap-\nplicants and Matriculants to an In-\nternal Medicine Residency Program; \nUnited States\nEvaluate the usefulness of the \nRandom Forest machine learning \nalgorithm to predict ranked appli-\ncants and matriculants in an internal \nmedicine residency program\nPredictive \nStudy\nMedicine; 5067 \nResidency \napplicants\nMachine Learning; Real \nworld; No comparator\nThe algorithm was used to predict ranked and matriculated \napplicants among 5,067 applications to an internal medicine resi-\ndency program over 3 years. The RF model achieved an AUROC \nof 0.925 for distinguishing ranked from unranked applicants and \n0.597 for identifying matriculants among ranked candidates.\nReich [30] 2022; Artificial Neural \nNetwork Approach to Competency-\nBased Training Using a Virtual \nReality Neurosurgical Simulation; \nCanada\nTo outline the educational utility of \nusing an ANN in the assessment and \nquantitation of surgical expertise.\nDevelop-\nment & \nValidation \nStudy\nNeurosurgery; \nOrthopedics; 21 \nResidents; Fel-\nlows; Faculty\nArtificial Neural Net-\nwork; Simulations; No \ncomparator\n21 participants were evaluated, and the ANN model, trained \non six safety metrics, achieved 83.3% accuracy in classifying \nexpertise levels. ANNs identified continuous and discontinuous \nlearning patterns, showing potential to enhance competency-\nbased surgical training.\nRizzo [82] 2024; The performance of \nChatGPT on orthopaedic in-service \ntraining exams: A comparative \nstudy of the GPT-3.5 turbo and \nGPT-4 models in orthopaedic edu-\ncation; United States\nTo investigate the application of \nLLMs within the realm of orthopae-\ndic in-service training examinations.\nNon-Ran-\ndomized \nExperimen-\ntal Study\nOrthopedics; \nNo human \nparticipant\nLarge Language Models; \nReal world; AI vs. other \nAI tools\nGPT-4 consistently outperformed GPT-3.5 Turbo across various \nyears and question categories, achieving 67.63% accuracy \nin 2022, 58.69% in 2021, and 59.53% in 2020, compared to \nGPT-3.5 Turbo’s 50.24%, 47.42%, and 46.51% respectively. Both \nmodels performed better on non-media questions, with GPT-4 \nconsistently scoring higher on both first-order and higher-order \nquestions.\nRuzicki [38] 2023; Use of Machine \nLearning to Assess Cataract Surgery \nSkill Level With Tool Detection; \nCanada\nTo develop a method for objective \nanalysis of reproducible steps in rou-\ntine cataract surgery and distinguish \nbetween expert and trainee surgical \nmovements.\nDevelop-\nment & \nValidation \nStudy\nOphthalmol-\nogy; No human \nparticipant\nMachine Learning; \nOther: Deep neural \nnetworks; Real world; No \ncomparator\nTool detection achieved high accuracy with AUC ranging from \n0.933 to 0.998, while skill classification showed lower accu-\nracy with AUC ranging from 0.550 to 0.692 depending on the \nscenario.\nRyder [48] 2024; Using Artificial \nIntelligence to Gauge Competency \non a Novel Laparoscopic Training \nSystem.; Canada\nTo develop an NLP model for \nevaluating the quality of supervisor \nnarrative comments in CBME\nDevelop-\nment & \nValidation \nStudy\nEmergency \nMedicine; 50 \nResidents; \nFaculty\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nThe system demonstrated high accuracy in identifying low-\nquality comments, achieving an 87% accuracy rate within 1 point \nof the human-rated score.\nSalastekar [26] 2023; Artificial Intelli-\ngence/Machine Learning Education \nin Radiology: Multi-institutional \nSurvey of Radiology Residents in \nthe United States.; United States\nTo evaluate radiology residents \nperspectives regarding the inclusion \nof artificial intelligence/machine \nlearning (AI/ML) education in the \nresidency curriculum.\nSurvey \nstudy\nRadiology; 209 \nResidents\nMachine Learning; Other: \nGeneral AI; Real world; No \ncomparator\n83% of radiology residents supported integrating AI/ML educa-\ntion into their residency curriculum, preferring hands-on AI/\nML laboratories (67%) and lecture series (61%). Most residents \nfavored a continuous AI/ML course spanning the entire residency \nfrom R1 to R4.\nTable 2 (continued)\n \nPage 18 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nSarraf [54] 2021; Use of artificial \nintelligence for gender bias analysis \nin letters of recommendation for \ngeneral surgery residency candi-\ndates; United States\nExamine gender bias in LoRs written \nfor surgical residency candidates \nacross three decades\nObserva-\ntion Study\nGeneral Surgery; \n171 Residency \nApplicants\nNatural Language Pro-\ncessing; Other: Sentiment \nanalysis; Real world; No \ncomparator\nThe study analyzed 611 LoRs using AI to detect gender bias. Gen-\ndered wording was more common in LoRs for female applicants \n(p = 0.04), especially those with lower clerkship grades (p = 0.01). \nSentiment analysis showed male-authored LoRs for male appli-\ncants had more positive sentiment (p = 0.02). LoRs written before \n2000 were shorter but showed no significant gender differences \nin word count (p = 0.18). Gender bias increased over time with \nmore male-biased LoRs.\nSewell [29] 2008; Providing metrics \nand performance feedback in a \nsurgical simulator.; United States\nTo present and validate metrics and \nfeedback mechanisms for a mastoid-\nectomy simulator\nObserva-\ntion Study\nOtolaryngology; \n15 Residents; \nFaculty; Medical \nStudents\nMachine Learning; Simu-\nlations; No comparator\nHMM’s classified expert and novice surgical performance with \n87.5% accuracy. The logistic regression classifier showed high \naccuracy for metrics like bone visibility (100%) and excessive \nforces near the facial nerve (87.5%). Feedback mechanisms were \nvalidated by significant correlations with instructor-assigned \nscores, proving their effectiveness in improving surgical skills\nShah [99] 2022; Artificial Intelli-\ngence-Powered Clinical Decision \nSupport and Simulation Platform \nfor Radiology Trainee Education.; \nUnited States\nInvestigate the use of AI-based CDS \nsoftware for automated feedback to \nradiology trainees\nObserva-\ntion Study\nRadiology; \n10 Residents; \nFellows\nDecision support \nsystems; Other: Bayesian \ninference-based CDS; \nSimulations; AI vs. Stan-\ndard methods\nTrainees rated the educational value of simulation cases with \nCDS higher and had slightly lower confidence in their findings \ncompared to clinical cases without CDS (p < 0.05). No significant \ndifferences were found in timing or ratings of clinical cases with \nor without CDS\nShiang [9] 2022; Artificial intelli-\ngence-based decision support \nsystem (AI-DSS) implementation in \nradiology residency: Introducing \nresidents to AI in the clinical setting.; \nUnited States\nEvaluate residents’ real-time experi-\nences and perceptions using AI-DSS \nin the clinical setting and provide \nrecommendations on improving AI \ncurriculums in residency programs.\nSurvey \nstudy\nRadiology; 15 \nResidents\nDecision support \nsystems; Other: AI-based \ndecision support system \n(AI-DSS); Real world; No \ncomparator\n91.6% supported AI integration into the curriculum. AI-DSS was \nfound helpful for triaging (83.3%) and troubleshooting (66.7%), \nbut less so for diagnostic purposes (speed: 41.7%, accuracy: \n33.3%). Most residents (83.3%) felt positive about AI’s impact on \nradiology and 50% were motivated to learn more about AI.\nSiyar [34] 2018; Using classifiers to \ndistinguish neurosurgical skill levels \nin a virtual reality tumor resection \ntask; Iran, Canada\nTo distinguish between skilled and \nless skilled operators in a virtual \nreality neurosurgical simulator by \napplying classifiers.\nObserva-\ntion Study\nNeurosurgery; \n115 Residents; \nFellows; Medical \nStudents\nDecision support \nsystems; Simulations; No \ncomparator\nThe Fuzzy K-Nearest-Neighbors (FKNN) classifier showed the \nbest performance with an average equal error rate (EER) as low \nas 9.6%.\nSmith [84] 2023; Will code one \nday run a code? Performance of \nlanguage models on ACEM primary \nexaminations and implications.; \nAustralia\nTo explore the performance of LLMs \non ACEM primary examinations and \ndiscuss their implications for medical \neducation and practice.\nNon-Ran-\ndomized \nExperimen-\ntal Study\nEmergency \nMedicine; \nNo human \nparticipant\nLarge Language Models; \nSimulations; AI vs. other \nAI tools\nGPT-4.0 outperformed average candidates, while Bard and \nBing achieved passing marks but did not outperform the mean \ncandidate.\nSolano [50] 2021; Natural Language \nProcessing and Assessment of \nResident Feedback Quality; United \nStates\nTo validate the performance of a \nnatural language processing (NLP) \nmodel in characterizing the quality \nof feedback provided to surgical \ntrainees.\nDevelop-\nment & \nValidation \nStudy\nGeneral Surgery; \nNo human \nparticipant\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nNLP model demonstrated high accuracy (0.83) and specificity \n(0.97) in classifying feedback quality, with an AUROC of 0.86. \nHowever, its sensitivity was relatively low at 0.37.\nTable 2 (continued)\n \nPage 19 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nSpadafore [46] 2024; Using Natural \nLanguage Processing to Evaluate \nthe Quality of Supervisor Narrative \nComments in Competency-Based \nMedical Education; Canada\nTo develop an NLP model to evalu-\nate the quality of supervisor narrative \ncomments in CBME\nDevelop-\nment & \nValidation \nStudy\nEmergency \nMedicine; 50 \nResidents; \nFaculty\nMachine Learning; \nNatural Language Pro-\ncessing; Real world; No \ncomparator\nQuAL model predicted the exact human-rated score or within \none point in 87% of instances. It performed excellently, especially \nin subtasks on suggestions for improvement and linking resident \nperformance to improvement suggestions, with balanced ac-\ncuracies of 85% and 82%, respectively.\nStahl [49] 2021; Natural language \nprocessing and entrustable profes-\nsional activity text feedback in \nsurgery: A machine learning model \nof resident autonomy.; United States\nTo analyze EPA assessment narrative \ncomments using NLP to enhance \nunderstanding of resident entrust-\nment in practice.\nObserva-\ntion Study\nGeneral Surgery; \nEmergency \nMedicine; 144 \nResidents; \nFaculty\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nOver 18 months, 1,015 EPA microassessments were collected \nfrom 64 faculty for 80 residents. LDA analysis identified topics that \nmapped 1:1 to EPA entrustment levels (Gammas > 0.99), showing \ncoherent trends with high-entrustment words in high-entrust-\nment topics and low-entrustment words in low-entrustment \ntopics.\nSummers [60] 2021; Analysis of the \nImpact of Step 1 Scores on Rank \nOrder for the NRMP Match.; United \nStates\nTo determine the impact of chang-\ning USMLE Step 1 score reporting \nto Pass/Fail on the rank order of \nresidency applicants.\nPredictive \nStudy\nMedicine; Not \nspecified Resi-\ndency applicants\nArtificial Neural Net-\nwork; Real world; No \ncomparator\nThis study used a deep neural network to model the impact of \nUSMLE Step 1 scores on residency rank order. The model showed \na high correlation (0.983) between rank lists with and without \nStep 1 scores. Key factors affecting ranking were interview scores, \nevaluation scales, Step 2 scores, and graduation year.\nThanawala [100] 2022; Overcoming \nSystems Factors in Case Logging \nwith Artificial Intelligence Tools.; \nUnited States\nTo identify and measure the impact \nof systems and human factors on \ncase logging in general surgery train-\ning programs\nObserva-\ntion Study\nGeneral Surgery; \n171 Residents\nMachine Learning; Other: \nGeneral AI, Reinforce-\nment learning; Real \nworld; AI vs. Human \nperformance\n31,385 cases were logged by 171 residents using the platform. \nIntelligent case logging increased logging rates from 1.44 to 4.77 \ncases per resident per week (p < 0.00001). Even with manual data \nentry during connectivity pauses, logging increased to 2.85 cases \nper week (p = 0.0002).\nVasan [55] 2023; Letters of recom-\nmendations and personal state-\nments for rhinology fellowship: A \ndeep learning linguistic analysis.; \nUnited States\nTo evaluate general and linguistic \ncategory differences in Rhinology fel-\nlowship letters of recommendation \nand personal statements between \napplicant genders and between in-\nternational medical graduates (IMGs) \nand US-trained candidates\nObserva-\ntion Study\nRhinology; \n56 Fellowship \napplicants\nNatural Language \nProcessing; Other: Deep \nlearning; Real world; No \ncomparator\nFemale applicants used more words associated with negative \nemotions, leadership, and feminism. US-trained applicants used \nmore optimistic words, while IMG applicants used more leader-\nship and work-related words.\nVasoya [119] 2019; ReadMI: An \nInnovative App to Support Training \nin Motivational Interviewing; United \nStates\nTo improve the (Motivational \nInterviewing) MI training process in \ngraduate medical education using a \ntool (ReadMI) that provides real-time \nfeedback\nDevelop-\nment & \nValidation \nStudy\nMedicine; \nnot specified \nResidents\nNatural Language \nProcessing; Other: Deep \nlearning; Real world; AI vs. \nStandard methods\nReadMI demonstrated high accuracy: 92% in transcript genera-\ntion, 95% in conversation time reporting, and 92% for both \nopen- and closed-ended questions. It significantly enhanced \nMotivational Interviewing (MI) training by providing real-time \nfeedback, improving training efficiency and effectiveness.\nWebb [69] 2023; Proof of Concept: \nUsing ChatGPT to Teach Emergency \nPhysicians How to Break Bad News.; \nUnited States\nTo demonstrate the potential of \nChatGPT in designing realistic clinical \nscenarios, enabling active roleplay, \nand delivering effective feedback for \nteaching physicians how to break \nbad news\nDevelop-\nment & \nValidation \nStudy\nEmergency \nMedicine; \nNo human \nparticipants\nNatural Language Pro-\ncessing; Simulations; No \ncomparator\nChatGPT successfully set up realistic training scenarios, enabled \nroleplay, and provided clear feedback using the SPIKES frame-\nwork for breaking bad news.\nTable 2 (continued)\n \nPage 20 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nWinkler-Schwartz [33] 2019; \nMachine Learning Identification of \nSurgical and Operative Factors As-\nsociated with Surgical Expertise in \nVirtual Reality Simulation; Canada\nTo identify surgical and operative \nfactors selected by a machine learn-\ning algorithm to accurately classify \nparticipants by level of expertise in a \nvirtual reality surgical procedure\nObserva-\ntion Study\nNeurosurgery; \n50 Residents; \nFellows; Fac-\nulty; Medical \nStudents\nMachine Learning; Simu-\nlations; No comparator\nThe K-nearest neighbor algorithm achieved the highest accuracy \nat 90%. AI identified key metrics related to instrument move-\nment, force, resection accuracy, and bleeding control, highlight-\ning its potential to enhance surgical training and assessment.\nWoods [45] 2023; ‘Your comment is \nnot as helpful as it could be.do you \nstill want to submit?’ Using natural \nlanguage processing to identify \nthe quality of supervisor narrative \ncomments in competency based \nmedical education; Canada\nTo develop an NLP model for ap-\nplying the QuAL score to supervisor \nnarrative comments\nDevelop-\nment & \nValidation \nStudy\nEmergency \nMedicine; 50 \nResidents; \nFaculty\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nThe NLP model showed reasonable accuracy in rating narrative \ncomments, with balanced accuracies of 0.615 for performance \ndescriptions, 0.85 for suggestions for improvement, and 0.902 for \nlinking performance with suggestions. The overall QuAL score \nhad a balanced accuracy of 0.52 and a top-2 accuracy of 0.83, \nindicating potential for integration into learning management \nsystems.\nWu [87] 2020; Comparison of Chest \nRadiograph Interpretations by \nArtificial Intelligence Algorithm vs. \nRadiology Residents.; United States\nTo assess the performance of AI \nalgorithms in realistic radiology \nworkflows by performing an objec-\ntive comparative evaluation of the \npreliminary reads of AP frontal chest \nradiographs by an AI algorithm and \nradiology residents.\nNon-Ran-\ndomized \nExperimen-\ntal Study\nRadiology; 5 \nResidents\nMachine Learning; Real \nworld; AI vs. Human \nperformance\nThe AI achieved a mean image-based sensitivity of 0.716 (95% \nCI, 0.704–0.729) versus 0.720 (95% CI, 0.709–0.732) for residents \n(P = 0.66). The AI had higher PPV (0.730 vs. 0.682, P < 0.001) and \nspecificity (0.980 vs. 0.973, P < 0.001). There was no significant dif-\nference in sensitivity, but the AI had better specificity and PPV\nWu [20] 2022; Factors Influencing \nTrainees Interest in Breast Imaging; \nCanada\nTo gauge the level of interest in \nbreast imaging (BI) and determine \nfactors impacting traineesâ€™ deci-\nsion to pursue this subspecialty\nSurvey \nstudy\nRadiology; 157 \nResidents; Medi-\ncal Students\nGeneral AI; Real world; No \ncomparator\n36% of residents and 65% of medical students were interested in \nBI/WI fellowships Trainees disinterested in BI/WI believed that AI \nwill decrease the need for breast radiologists.\nYi [88] 2020; Can AI outperform \na junior resident? Comparison of \ndeep neural network to first-year \nradiology residents for identification \nof pneumothorax; United States\nTo develop a deep learning system \n(DLS) using a deep convolutional \nneural network (DCNN) for identifica-\ntion of pneumothorax, compare its \nperformance to first-year radiology \nresidents, and evaluate the abil-\nity of a DLS to augment radiol-\nogy residents by detecting missed \npneumothoraces.\nDevelop-\nment & \nValidation \nStudy\nRadiology; 2 \nResidents\nOther: Deep CNN; Real \nworld; AI vs. Human \nperformance\nThe DCNN achieved an AUC of 0.841, with a sensitivity of 85% \nand specificity of 67%, while the residents achieved higher AUCs \nof 0.942 and 0.905 but at a significantly slower rate (2 images/\nmin vs. 1980 images/min for the DCNN). The DCNN identified 3 \nadditional pneumothoraces missed by the residents\nYi [66] 2023; A novel use of an \nartificially intelligent Chatbot and a \nlive, synchronous virtual question-\nand answer session for fellowship \nrecruitment; United States\nTo determine if an Artificially Intelli-\ngent Chatbot and a Virtual Question-\nand-Answer Session can aid in \nrecruitment in a post-COVID-19 \neraâ€‹â€‹.\nSurvey \nstudy\nAnesthesiology \n(Pain fellowship); \n48 Fellows\nNatural Language Pro-\ncessing; Real world; No \ncomparator\nOut of 48 pain fellowship applicants, 18.6% responded. Among \nrespondents, 73% used the website chatbot, and 84% of those \nreported the chatbot successfully provided the information they \nwere seeking.\nTable 2 (continued)\n \nPage 21 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nStudy Details (First author Year; \nTitle; Country)\nAim/ Objective of the Study Study \nDesign\nStudy Char-\nacteristics (Spe-\ncialty; Number; \nParticipants)\nAI Summary (Type of \nAI mentioned in the \nstudy; Study setting; AI \nbeing compared to)\nKey AI-Related Results Summarized\nYilmaz [28] 2022; Continuous \nmonitoring of surgical bimanual ex-\npertise using deep neural networks \nin virtual reality simulation; Canada\nTo develop and validate the Intel-\nligent Continuous Expertise Monitor-\ning System (ICEMS) for assessing \nsurgical bimanual performance in \nreal-time using deep neural networks \nin a virtual reality simulation.\nDevelop-\nment & \nValidation \nStudy\nNeurosurgery; \n50 Residents; \nFellows; Fac-\nulty; Medical \nStudents\nMachine Learning; Simu-\nlations; No comparator\nICEMS differentiated performance levels among neurosurgeons, \nsenior trainees, junior trainees, and students. Average scores \nvaried significantly (p < 0.001), with seniors scoring higher than \njuniors (mean difference = 0.367, p = 0.004) and juniors scoring \nhigher than novices (mean difference = 0.289, p = 0.04). Scores \ncorrelated with training years, increasing by 0.092 per year \n(p = 0.005).\nYost [44] 2015; Predicting academic \nperformance in surgical training; \nUnited States\nTo determine if residents at risk \nfor substandard performance on \nthe American Board of Surgery In-\nTraining Examination (ABSITE) can be \nidentified based on their behavioral \nand motivational characteristics.\nPredictive \nStudy\nGeneral Surgery; \n117 Residents\nArtificial Neural Network; \nReal world; AI vs. Human \nperformance\nFor senior residents, higher theoretical scores were associated \nwith lower pass rates on the ABSITE (p = 0.043) while for junior \nresidents, higher internal role awareness scores were associated \nwith higher pass rates (p = 0.004). The neural network model \naccurately predicted ABSITE performance, identifying key behav-\nioral and motivational factors.\nZhang [47] 2012; Automated assess-\nment of medical training evaluation \ntext.; United States\nTo assess the feasibility and value \nof an automated approach for syn-\nthesizing evaluation comments of \nresidency trainees using text-mining \ntechniques for sentiment and topic \nanalysis\nDevelop-\nment & \nValidation \nStudy\nMedicine; Pedi-\natrics; No human \nparticipant\nMachine Learning; \nNatural Language Pro-\ncessing; Real world; No \ncomparator\nSVM achieved 93% accuracy in sentiment analysis. For topic \nclassification, performance varied by competency, with an overall \nprecision and recall of 76.3%.\nZhao [67] 2020; Comparison of \nMultiple Quantitative Evaluation In-\ndices of Theoretical Knowledge and \nClinical Practice Skills and Training \nof Medical Interns in Cardiovascular \nImaging Using Blended Teaching \nand the Case Resource Network \nPlatform (CRNP).; China\nTo compare multiple quantitative \nevaluation indices of theoretical \nknowledge and clinical practice skills \nin training medical interns in car-\ndiovascular imaging using blended \nteaching (BT) and the Case Resource \nNetwork Platform (CRNP)\nObserva-\ntion Study\nCardiology; 110 \nResidents\nOther: Case Resource \nNetwork Platform (CRNP) \nintegrating artificial intel-\nligence; Real world; AI vs. \nStandard methods\nThe BT group showed significant improvements in CT angiog-\nraphy scores (6.53 vs. 5.76, p = 0.022), cardiac MRI scores (5.69 vs. \n4.73, p = 0.016), and average scores (6.32 vs. 5.69, p = 0.002).\nZhao [89] 2020; Reducing the \nnumber of unnecessary biopsies \nof US-BI-RADS 4a lesions through a \ndeep learning method for residents-\nin-training: a cross-sectional study.; \nChina\nTo explore the potential value of \nS-Detect for residents-in-training, a \ncomputer-assisted diagnosis system \nbased on a deep learning algorithm\nObserva-\ntion Study\nRadiology; 195 \nfocal breast \nlesions (patients) \nResidents\nDeep learning; Real \nworld; AI vs. Human \nperformance\nS-Detect tool demonstrated high specificity (77.88%) and sen-\nsitivity (85.37%), with an AUC of 0.82, indicating better specific-\nity compared to residents who had high sensitivity but lower \nspecificity\nAbbreviations used: ABSITE: American Board of Surgery In-Training Examination, AI: Artificial Intelligence, ANN: Artificial Neural Network, AUC: Area Under the Curve, CCC: Clinical Competency Committee, CNN: Convolutional \nNeural Network, CT: Computed Tomography, DLS: Deep Learning System, DNN: Deep Neural Network, EMR: Electronic Medical Record, EPA: Entrustable Professional Activity, GME: Graduate Medical Education, GOALS: \nGlobal Operative Assessment of Laparoscopic Skills, ITE: In-Training Examination, JBI: Joanna Briggs Institute, LLM: Large Language Model, LOR: Letter of Recommendation, MI: Motivational Interviewing, ML: Machine \nLearning, NER: Named Entity Recognition, NLI: Natural Language Interpretation, NLP: Natural Language Processing, NRMP: National Resident Matching Program, OBGYN: Obstetrics and Gynecology, OITE: Orthopaedic In-\nTraining Examination, PRISMA: Preferred Reporting Items for Systematic Reviews and Meta-Analyses, RNN: Recurrent Neural Network, SVM: Support Vector Machine, SVR: Support Vector Regression, US: Ultrasound, USMLE: \nUnited States Medical Licensing Examination, VR: Virtual Reality\nTable 2 (continued)\n \nPage 22 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nAI has also been used for predictions, with Ariaeinejad \n[42] using a machine learning algorithm to identify per -\nformance patterns, achieving a sensitivity of 0.54, speci -\nficity of 0.74, and AUC of 0.64. Additonally, studies by \nAmirhajlou [43] and Yost [ 44] using neural networks to \npredict board certification exam performance based on \nITE and ABSITE scores, achieving high accuracy.\nAI in trainee & faculty evaluations\nWoods [45] and Spadafore [ 46] developed NLP models \nto assess supervisor narrative comments in competency-\nbased medical education. Woods’ model showed bal -\nanced accuracies of 0.615 for performance descriptions, \n0.85 for suggestions for improvement, and 0.902 for link -\ning performance. Similarly, Spadafore’s model predicted \nthe exact human-rated score or within one point in 87% \nof instances, with balanced accuracies of 85% for sugges -\ntions for improvement and 82% for linking performance. \nZhang [47] extended these efforts by assessing text-min -\ning techniques to synthesize evaluation comments for \nresidents, achieving 93% accuracy in sentiment analysis \nand 76.3% precision and recall for topic classification. \nSimilarly, Ryder [48] developed an NLP system to evalu -\nate supervisor narrative comments, achieving 87% accu -\nracy within 1 point of the human-rated score.\nSeveral studies have focused on enhancing feedback \nanalysis using NLP . Stahl [49] used NLP to analyze EPA \nassessment narrative comments, enhancing understand -\ning of resident entrustment in practice. Solano [ 50] vali-\ndated an NLP model for characterizing feedback quality \nto surgical trainees, demonstrating high accuracy (83%) \nand specificity (97%), though with low sensitivity (37%). \nIn a similar vein, Neves [ 51] employed machine learning \nto evaluate feedback on anesthesiology resident perfor -\nmance, achieving 74.4-82.2% accuracy in predicting feed -\nback traits and high precision in low-quality predictions \nwhile processing data significantly faster than manual \nscoring.\nOther studies have leveraged NLP for broader edu -\ncational outcomes. For example, Lui [ 52] used NLP to \ntrack medical residents’ cognitive maturation, correlating \nlinguistic markers with faculty assessments, providing \ninsights into their developmental progress.\nAI in GME recruitment\nAI has been used to analyze Letters of Recommendation \n(LORs), applications, personal statements, as well as to \npredict rankings or interview invitations and recruitment \noutcomes.\nIn studies on LORs, Boolchandani [ 53] found no sig -\nnificant differences in language based on gender or \nrace. However, Sarraf [ 54] observed differences in how \nlanguage was used to describe male and female general \nsurgery candidates, indicating gender-related variations \nin linguistic style. Additionally, Vasan [ 55] identified dis -\ntinct language patterns between U.S.-trained applicants \nand international medical graduates (IMGs) in the field \nof rhinology. Gray [ 56] noted bias in sentiment, with \nBlack applicants receiving the highest positive sentiment \nand Hispanic applicants the lowest (p = 0.03).\nIn residency applications analysis, Drum [ 57] showed \nML’s moderate sensitivity (0.64) and high specificity \n(0.97) in identifying values from unstructured data, cor -\nrelating with interview invitations and enhancing equity. \nBurk-Rafel’s [58] ML-based decision support tool identi -\nfied 20 overlooked candidates through holistic screening. \nAI also predicted applicant rankings [ 59–62] and inter -\nview invitations [63].\nJohnstone [64] found that AI-generated personal state -\nments were challenging to distinguish from human-writ -\nten ones, with 70% of program directors unable to detect \na difference. Patel [ 65] reported no significant difference \nin ratings for readability, originality, authenticity, and \noverall quality (all P > 0.05) between computer-generated \nand applicant essays. However, raters were less inclined \nto grant interviews to computer-generated essays (58% \nvs. 78%, P = 0.12).\nAI’s role extends beyond document analysis to inter -\nactive tools aimed at enhancing the recruitment experi -\nence. For example, Yi [ 66] highlighted AI’s potential for \nrecruitment through a chatbot for virtual Q&A, suggest -\ning it improved program perception.\nAI in medical education and training\nBond [ 10] found that incorporating AI in virtual stan -\ndardized patient (VSP) simulations significantly increased \nclinical training effectiveness, with learners accurately \ndiagnosing 82% of cases and boosting their decision-\nmaking confidence. Similarly, Zhao [ 67] reported that a \nblended teaching approach improved overall learning \nengagement while Merritt [ 68] further suggested that AI \ntechnology could overcome common obstacles of con -\nventional simulation methods. For example, Webb [ 69] \ndemonstrated ChatGPT’s potential in designing realistic \nclinical scenarios for training emergency physicians in \nbreaking bad news.\nEl Saadawi [ 70] showed AI-based tutoring systems \ndeliver customized educational experiences by accurately \ninterpreting learner inputs. Kelahan [ 71], supported by \nLin [ 72] and Muntean [ 73], demonstrated AI’s ability \nto adapt to learners’ progress, providing personalized \nfeedback and enhancing learner experience. Chen [ 74] \ndescribed an NLP-incorporated dashboard to track resi -\ndent caseloads, identifying training gaps and improving \neducation.\nPage 23 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nAI in standardized examinations\nSeveral studies tested the performance of AI tools in \nstandardized multiple-choice examinations across vari -\nous medical subspecialties.\nNori [75] investigated GPT-4’s capabilities on medical \nchallenge problems, surpassing the USMLE passing score \nby over 20 points. Ali [ 76] assessed GPT-3.5, GPT-4, and \nGoogle Bard on a neurosurgery oral boards preparation \nquestion bank, with GPT-4 achieving the highest accu -\nracy at 82.6%. Bartoli [77] evaluated ChatGPT in generat-\ning and answering neurosurgical written exam questions, \nranking 6th out of 11 participants. Gupta [ 78] applied \nGPT-4 to the Plastic Surgery Inservice Training Examina-\ntion, achieving 77.3% accuracy, while Humar [ 79] found \nChatGPT performed on par with a first-year resident on \nthe Plastic Surgery In-Service Examination, but lower \nthan more advanced residents. Holmes [ 80] compared \nLLM performance in ophthalmology questions, with \nGPT-4 performing comparably to attending physicians. \nCohen [81] evaluated ChatGPT’s performance in Hebrew \nOBGYN exams, scoring 38.7% compared to 68.4% by res-\nidents, and found it performed better on English medical \ntests (60.7% accuracy).\nRizzo [ 82] compared GPT-3.5 Turbo and GPT-4 on \northopaedic in-service training exams, with GPT-4 con -\nsistently outperforming GPT-3.5 Turbo while Lum [ 83] \nfound it performed below passing thresholds for higher \nyears of training. Smith [ 84] explored LLM performance \non ACEM primary examinations, finding GPT-4 outper -\nformed average candidates. Mahajan [ 85] assessed Chat -\nGPT’s performance on the Otolaryngology Residency \nIn-Service Exam, with accuracy varying by question \ndifficulty.\nAI has also been studied to generate questions rel -\nevant for GME. Cheung [ 86] found AI-generated MCQs \nmatched human-created ones in quality across most \ndomains but were slightly inferior in relevance. ChatGPT \nnotably produced MCQs much faster than humans.\nAI in clinical decision making and training\nWu [ 87] and Yi [ 88] compared the diagnostic abilities \nof AI and resident physicians. Wu found that AI’s per -\nformance on chest radiographs was comparable to that \nof residents while, Yi noted that a deep learning sys -\ntem analyzed pneumothorax images about 1000 times \nfaster, potentially reducing misdiagnosis and enhancing \ntraining.\nAI has also been used in different specialties and sce -\nnario to improve diagnostic accuracy and confidence of \nresidents and fellows [89–96]. For example, Lee [97] used \na deep learning model in the CT diagnosis of cervical \nlymph node metastasis in thyroid cancer to help under -\nperforming residents. Chassagnon [ 98] reported that a \ncomputer-aided detection (CADe) system improved the \ndiagnostic sensitivity, specificity, and accuracy of resi -\ndents with active use of the system. Interestingly, Shah \n[99] found that simulation cases with Clinical Decision \nSupport (CDS) were valued educationally, but resulted \nin lower confidence in findings and diagnosis. Shiang [ 9] \nhighlighted that radiology residents supported AI-DSS \nfor triaging patients and troubleshooting tasks.\nAI use has also been studied in other aspects of GME \nworkflow. For example, Thanawala [ 100] showed AI \nincreased case logging efficiency in general surgery train -\ning programs, while Gong explored AI’s potential in ana -\nlyzing clinical notes to reduce work hours. Gao [ 102] \nand Ouyang [ 103] leveraged historical medical records \nto enhance diagnostic learning for interns and novice \nphysicians.\nLimitations identified in various studies\nThe reviewed studies reported several limitations that \naffected the generalizability and accuracy of their find -\nings. Key methodological concerns include self-reporting \nbiases, low response rates, and survey design challenges \n[18, 19, 22, 23, 68, 104]. Additionally, small sample sizes \nwere a recurrent issue. For example, Bond et al. [ 10] \ntested AI-driven virtual standardized patient simulations \non only 14 residents, limiting the ability to draw broad \nconclusions about its effectiveness across different spe -\ncialties. Similarly, studies assessing AI’s predictive abil -\nity for board certification exams [ 43, 44] often used data \nfrom a single institution or a limited cohort, making it \ndifficult to apply findings universally.\nAnother critical limitation is the variability in AI appli -\ncations, which affects comparability across studies. AI \ntools used for predicting board exam performance [ 42, \n43] rely on structured data inputs, whereas AI applica -\ntions in clinical decision-making [86, 87, 97] involve deep \nlearning models trained on imaging datasets. This fun -\ndamental difference in AI methodologies means findings \nfrom one domain cannot be extrapolated to another. For \ninstance, an AI system that performs well in surgical skill \nassessment [28, 29] using motion tracking may not trans-\nlate to radiology decision support systems [88, 89].\nFurthermore, algorithmic biases are a recurring issue. \nSarraf et al. [ 54] identified gender-related variations in \nhow AI analyzed letters of recommendation for surgery \napplicants, raising concerns about fairness in recruit -\nment. Similarly, studies applying NLP to performance \nevaluations [ 45, 46] identified potential bias in feedback \ninterpretation, which could reinforce pre-existing dispar-\nities in medical training.\nThe ongoing refinement and validation of AI technolo -\ngies are frequently recommended because of perfor -\nmance concerns [56, 58, 59, 64, 92, 102, 105, 106]. Many \nAI models are developed in controlled environments but \nPage 24 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nrequire further testing in real-world clinical settings to \nensure robustness.\nClinical context limitations and participant-related \nissues highlight the challenges in applying AI across \ndiverse clinical settings and populations [ 33, 40, 41, 81, \n86, 107]. For example, an AI-driven competency assess -\nment tool tested in a single specialty or training site may \nnot be effective when applied to a multispecialty resi -\ndency program with different educational needs.\nImplications for future research and practice\nFuture research should validate current findings and \nexplore additional AI applications for GME. Studies need \nlarge sample sizes, multiple sites, and comprehensive \ndata to enhance generalizability and reliability. Evaluat -\ning and improving AI models, including large language \nmodels (LLMs) and neural networks, is crucial. Address -\ning algorithmic biases identified in the reviewed studies \nis essential for ensuring fairness and accuracy.\nWhile this review summarizes predictive accuracy \nmetrics (e.g., AUC, sensitivity, specificity), direct com -\nparisons to established clinical benchmarks are beyond \nthe scope of a scoping review. Future studies should sys -\ntematically assess the clinical significance of AI models \nby comparing them against human performance stan -\ndards or widely accepted diagnostic thresholds.\nIn addition to addressing biases, ensuring fair repre -\nsentation, including gender and nonbinary consider -\nations, is essential. Algorithmic biases in recruitment, \nassessment, and performance evaluation may perpetu -\nate disparities, making fairness audits and model valida -\ntion crucial before widespread implementation. Ethical \nconsiderations such as data privacy, informed consent, \nand transparency in AI algorithms must be prioritized \nto maintain the effectiveness and reliability of AI-driven \ntools. AI systems trained on clinical or educational data \npose risks of data misuse, necessitating strong regulatory \noversight to ensure compliance with privacy laws (e.g., \nHIPAA, GDPR). Additionally, questions of accountabil -\nity remain unresolved—when AI contributes to residency \nevaluations or patient care decisions, clear guidelines are \nneeded to delineate responsibility between AI develop -\ners, educators, and clinicians.\nGiven the gap in AI training within residency pro -\ngrams, incorporating AI education into curricula is rec -\nommended along with faculty training for effectively \nutilizing AI tools and interpreting AI-generated data. \nOne approach could be adapting the competency-based \nmedical education (CBME) framework, which is widely \nused in medical education, to include AI skills alongside \nclinical competencies. For instance, Tippur [ 108] dis -\ncusses how AI education can be integrated into curricula, \nfocusing on bridging the gap between current training \nmethodologies and the evolving technological landscape. \nSimilarly, other fields have successfully integrated AI edu-\ncation into their curricula. Southworth [ 109] presented \na model for embedding AI across the higher education \ncurriculum, highlighting the importance of AI literacy \nfor future professionals, which could serve as a valuable \nframework for GME programs.\nResearch should also explore AI applications in broader \naspects of medical education. This includes tailoring \nteaching to individual learning needs, optimizing edu -\ncational outcomes and providing personalized feedback. \nLongitudinal studies are needed to assess the long-term \nimpact of AI tools on clinical competencies and patient \noutcomes to understand AI’s broader implications in \nGME.\nAI’s role in resident and fellow recruitment and selec -\ntion processes shows promise but also highlights biases \nand disparities. Future studies should refine AI algo -\nrithms to ensure fairness and equity in recruitment. \nExploring AI integration in holistic review processes can \nfurther enhance fairness.\nFinally, developing standardized evaluation metrics to \nassess AI interventions will ensure consistency across \nstudies. Evaluating the cost-effectiveness of AI applica -\ntions, considering both initial investment and long-term \nbenefits, is essential. Future research should explore how \nAI can support trainees in preparing for standardized \nexams and improving their test performance.\nLimitations of this scoping review\nStudy heterogeneity in design, AI applications, and out -\ncome measures posed a significant challenge to synthesis. \nThe included studies varied in methodology, participant \npopulations, and AI implementation strategies, making \ndirect comparisons difficult. Some studies focused on \npredictive modeling, while others assessed AI’s role in \neducation, evaluation, or clinical decision-making, lead -\ning to variability in reported effectiveness and applicabil -\nity. This heterogeneity necessitated a descriptive rather \nthan a comparative synthesis, limiting our ability to draw \ndefinitive conclusions about AI’s overall impact on GME.\nTo mitigate this challenge in future research, efforts \nshould be made to standardize study designs and report -\ning frameworks. Establishing common evaluation metrics \nfor AI interventions in GME would improve comparabil -\nity across studies. Additionally, multi-institutional col -\nlaborations using shared methodologies can enhance \nthe generalizability of findings. Implementing structured \nreporting guidelines—such as those used in AI applica -\ntions in clinical medicine—could further reduce incon -\nsistencies in study outcomes.\nWhile this review provides a broad overview of AI’s \napplications in GME, the rapidly evolving nature of AI \nnecessitates ongoing updates to maintain relevance. \nFuture research should not only focus on validating \nPage 25 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \nfindings across multiple settings but also on developing \nstandardized methodologies that facilitate clearer syn -\nthesis of AI’s role in medical education.\nConclusion\nThis scoping review provides a comprehensive overview \nof the applications, benefits, and challenges of AI inte -\ngration in GME. However, study heterogeneity, small \nsample sizes, and limited multi-institutional validation \nhinder generalizability. Additionally, biases in AI models, \nparticularly in recruitment and performance evaluation, \nhighlight the need for fairness audits and robust valida -\ntion before widespread implementation.\nFuture research should focus on standardized evalu -\nation frameworks for AI-driven assessments, ensuring \ntheir reliability and applicability across diverse training \nenvironments. Addressing the gap in AI literacy among \ntrainees and faculty is also critical, with competency-\nbased educational models offering a potential pathway \nfor structured AI integration into curricula. Additionally, \nrefining AI algorithms to promote equity in residency \nrecruitment and performance assessments remains a \ncrucial area for ongoing investigation.\nAs AI continues to evolve, periodic re-evaluations of \nits impact on GME will be necessary to ensure its ben -\nefits outweigh risks. Expanding research efforts across \nmultiple specialties and global settings will provide more \ninclusive insights, ultimately guiding the responsible \nadoption of AI in GME.\nAcknowledgements\nThis review was conducted as part of the capstone project for the Master \nof Education in the Health Professions (MEHP) program at Johns Hopkins \nUniversity.\nAuthor contributions\nB.G.V. contributed to the conceptualization, methodology, investigation, data \ncuration, formal analysis, writing of the original draft, visualization, supervision, \nproject administration, and review and editing of the manuscript. C.I. assisted \nin the methodology, investigation, and review and editing of the manuscript. \nT.B. participated in the investigation, data curation, and review and editing of \nthe manuscript. S.C. was involved in the investigation, validation, visualization, \nand review and editing of the manuscript. J.W. provided resources, assisted \nin the methodology and investigation, and contributed to the review \nand editing of the manuscript. R.S. contributed to the conceptualization, \nmethodology, supervision, and review and editing of the manuscript. All \nauthors have read and approved the final version of the manuscript.\nFunding\nNot applicable.\nData availability\nThe datasets used and/or analyzed during the current study are available from \nthe corresponding author on reasonable request.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nClinical trial number\nNot applicable.\nAuthor details\n1Education for Health Professions Program, School of Education, Johns \nHopkins University, 2800 N Charles St, Baltimore, MD 21218, USA\n2Internal Medicine Residency Program, Rochester, NY, USA\n3West Virginia University, Morgantown, WV, USA\n4Internal Medicine, Parkview Health, Fort Wayne, IN, USA\n5Internal Medicine, Rochester General Hospital, Rochester, NY, USA\n6Welch Medical Library, Johns Hopkins University, Baltimore, MD, USA\n7School of Medicine, University of Kansas Medical Center, Salina, KS \ncampus, Kansas City, KS, USA\nReceived: 2 October 2024 / Accepted: 9 May 2025\nReferences\n1. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436–44.\n2. Murphy R. Introduction to AI robotics. Cambridge, MA: The MIT Press; 2018.\n3. Amisha, Malik P , Pathania M, Rathaur VK. Overview of artificial intelligence in \nmedicine. J Fam Med Prim Care. 2019;8(7):2328–31.\n4. Van Der Niet AG, Bleakley A. Where medical education Meets artificial intel-\nligence: ‘does technology care?’ . Med Educ. 2021;55(1):30–6.\n5. Nagi F, Salih R, Alzubaidi M, Shah H, Alam T, Shah Z, et al. Applications of \nartificial intelligence (AI) in medical education: A scoping review. Stud Health \nTechnol Inf. 2023;305:648–51.\n6. Maldonado ME, Fried ED, DuBose TD, Nelson C, Breida M. The role that gradu-\nate medical education must play in ensuring health equity and eliminating \nhealth care disparities. Ann Am Thorac Soc. 2014;11(4):603–7.\n7. Stawicki P , Kumar S, Firstenberg KNS, Orlando MP , Papadimos JJ, Paul T et al. E, \nIntroductory Chapter: Navigating Challenges and Opportunities in Modern \nGraduate Medical Education. In: P . Stawicki S, S. Firstenberg M, P . Orlando J, J. \nPapadimos T, editors. Contemporary Topics in Graduate Medical Education - \nVolume 2. IntechOpen; 2022 [cited 2024 Jan 8]. Available from:  h t t p  s : /  / w w w  . i  \nn t e  c h o  p e n .  c o  m / c h a p t e r s / 7 9 7 4 4\n8. Boms O, Shi Z, Mallipeddi N, Chung JJ, Marks WH, Whitehead DC, et al. \nIntegrating innovation as a core objective in medical training. Nat Biotechnol. \n2022;40(3):434–7.\n9. Shiang T, Garwood E, Debenedectis CM. Artificial intelligence-based decision \nsupport system (AI-DSS) implementation in radiology residency: introducing \nresidents to AI in the clinical setting. Clin Imaging. 2022;92:32–7.\n10. Bond WF, Lynch TJ, Mischler MJ, Fish JL, McGarvey JS, Taylor JT, et al. Virtual \nstandardized patient simulation: case development and pilot application to \nHigh-Value care. Simul Healthc J Soc Simul Healthc. 2019;14(4):241–50.\n11. Lee J, Wu AS, Li D, Kulasegaram K (Mahan), editors. Artificial Intelligence \nin Undergraduate Medical Education: A Scoping Review. Acad Med. \n2021;96(11S):S62–70.\n12. Kirubarajan A, Young D, Khan S, Crasto N, Sobel M, Sussman D. Artificial intel-\nligence and surgical education: A systematic scoping review of interventions. \nJ Surg Educ. 2022;79(2):500–15.\n13. Abdel Aziz MH, Rowe C, Southwood R, Nogid A, Berman S, Gustafson K. A \nscoping review of artificial intelligence within pharmacy education. Am J \nPharm Educ. 2023;100615.\n14. Arksey H, O’Malley L. Scoping studies: towards a methodological framework. \nInt J Soc Res Methodol. 2005;8(1):19–32.\n15. Peters MDJ, Godfrey CM, Khalil H, McInerney P , Parker D, Soares CB. Guid-\nance for conducting systematic scoping reviews. Int J Evid Based Healthc. \n2015;13(3):141–6.\n16. Tricco AC, Lillie E, Zarin W, O’Brien KK, Colquhoun H, Levac D, et al. PRISMA \nextension for scoping reviews (PRISMA-ScR): checklist and explanation. Ann \nIntern Med. 2018;169(7):467–73.\nPage 26 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \n17. Sheehy R, White J, Verghese B, Iyer C. Protocol for A Scoping Review of Arti-\nficial Intelligence in Graduate Medical Education. OSF Registries; 2024 [cited \n2024 Jul 16]. Available from: https://osf.io/uw2n7/\n18. Reeder K, Lee H. Impact of artificial intelligence on US medical students’ \nchoice of radiology. Clin Imaging. 2022;81:67–71.\n19. Collado-Mesa F, Alvarez E, Arheart K. The role of artificial intelligence in diag-\nnostic radiology: A survey at a single radiology residency training program. J \nAm Coll Radiol. 2018;15(12):1753–7.\n20. Wu T, Law W, Islam N, Yong-Hing CJ, Kulkarni S, Seely J. Factors influencing \ntrainees’ interest in breast imaging. Can Assoc Radiol J. 2022;73(3):462–72.\n21. Kennedy T, Collie L, Nabhen J, Safavi A, Brundage M, De Moraes FY. 136: \nCanadian oncology residents’ knowledge of and attitudes towards artificial \nintelligence and machine learning. Radiother Oncol. 2022;174:S58–9.\n22. Huisman M, Ranschaert E, Parker W, Mastrodicasa D, Koci M, Pinto De Santos \nD, et al. An international survey on AI in radiology in 1,041 radiologists and \nradiology residents part 1: fear of replacement, knowledge, and attitude. Eur \nRadiol. 2021;31(9):7058–66.\n23. Chen Y, Wu Z, Wang P , Xie L, Yan M, Jiang M, et al. Radiology residents’ percep-\ntions of artificial intelligence: nationwide Cross-Sectional survey study. J Med \nInternet Res. 2023;25:e48249.\n24. Ooi S, Makmur A, Soon Y, Fook-Chong S, Liew C, Sia D, et al. Attitudes toward \nartificial intelligence in radiology with learner needs assessment within radi-\nology residency programmes: a National multi-programme survey. Singap \nMed J. 2021;62(3):126–34.\n25. Marquis KM, Hoegger MJ, Shetty AS, Bishop GL, Balthazar P , Gould JE, et \nal. Results of the 2020 survey of the American alliance of academic chief \nresidents in radiology. Clin Imaging. 2023;98:67–73.\n26. Salastekar NV, Maxfield C, Hanna TN, Krupinski EA, Heitkamp D, Grimm LJ. \nArtificial intelligence/machine learning education in radiology: Multi-\ninstitutional survey of radiology residents in the united States. Acad Radiol. \n2023;30(7):1481–7.\n27. Kocer Tulgar Y, Department of Medical History and Ethics, Medicine SU, \nTurkey S, Tulgar S, Department of Anaesthesiology and Reanimation, Samsun \nUniversity Faculty of Medicine, Samsun Training and Research Hospital, \nSamsun, Turkey, Kose G, Kose S et al. HC,. Anesthesiologists’ Perspective on \nthe Use of Artificial Intelligence in Ultrasound-Guided Regional Anaesthesia \nin Terms of Medical Ethics and Medical Education: A Survey Study. Eurasian J \nMed. 2023 May 5 [cited 2024 May 18]; Available from:  h t t p  s : /  / w w w  . e  a j m  . o r  g / \n/ e  n /  a n e  s t h  e s i o  l o  g i s  t s -  p e r s  p e  c t i  v e -  o n - t  h e  - u s  e - o  f - a r  t i  fi   c  i a l  - i n t  e l  l i g  e n c  e - i n  - u  l t r  \na s o  u n d -  g u  i d e  d - r  e g i o  n a  l - a  n a e  s t h e  s i  a - i  n - t  e r m s  - o  f - m  e d i  c a l -  e t  h i c  s - a  n d - m  e d  i c a \nl - e d u c a t i o n - a - s u r v e y - s t u d y - 1 3 3 4 7 5\n28. Yilmaz R, Winkler-Schwartz A, Mirchi N, Reich A, Christie S, Tran DH, et al. \nContinuous monitoring of surgical bimanual expertise using deep neural \nnetworks in virtual reality simulation. Npj Digit Med. 2022;5(1):54.\n29. Sewell C, Morris D, Blevins NH, Dutta S, Agrawal S, Barbagli F, et al. Providing \nmetrics and performance feedback in a surgical simulator. Comput Aided \nSurg. 2008;13(2):63–81.\n30. Reich A, Mirchi N, Yilmaz R, Ledwos N, Bissonnette V, Tran DH, et al. Artificial \nneural network approach to Competency-Based training using a virtual real-\nity neurosurgical simulation. Oper Neurosurg. 2022;23(1):31–9.\n31. Alkadri S, Ledwos N, Mirchi N, Reich A, Yilmaz R, Driscoll M, et al. Utilizing a \nmultilayer perceptron artificial neural network to assess a virtual reality surgi-\ncal procedure. Comput Biol Med. 2021;136:104770.\n32. Baloul MS, Yeh VJH, Mukhtar F, Ramachandran D, Traynor MD, Shaikh N, et al. \nVideo commentary & machine learning: tell me what you see, I tell you who \nyou are. J Surg Educ. 2022;79(6):e263–72.\n33. Winkler-Schwartz A, Yilmaz R, Mirchi N, Bissonnette V, Ledwos N, Siyar S, et \nal. Machine learning identification of surgical and operative factors associ-\nated with surgical expertise in virtual reality simulation. JAMA Netw Open. \n2019;2(8):e198363.\n34. Siyar S, Azarnoush H, Rashidi S, Winkler-Schwartz A. Using classifiers to dis-\ntinguish neurosurgical skill levels in a virtual reality tumor resection task. Int J \nComput Assist Radiol Surg. 2018;13(S1):1–273.\n35. Bissonnette V, Mirchi N, Ledwos N, Alsidieri G, Winkler-Schwartz A, Del \nMaestro RF, et al. Artificial intelligence distinguishes surgical training levels in \na virtual reality spinal task. J Bone Jt Surg. 2019;101(23):e127.\n36. Quinn KM, Chen X, Runge LT, Pieper H, Renton D, Meara M, et al. The robot \ndoesn’t Lie: real-life validation of robotic performance metrics. Surg Endosc. \n2023;37(7):5547–52.\n37. Anh NX, Nataraja RM, Chauhan S. Towards near real-time assessment of surgi-\ncal skills: A comparison of feature extraction techniques. Comput Methods \nPrograms Biomed. 2020;187:105234.\n38. Ruzicki J, Holden M, Cheon S, Ungi T, Egan R, Law C. Use of machine learning \nto assess cataract surgery skill level with tool detection. Ophthalmol Sci. \n2023;3(1):100235.\n39. Holden MS, Xia S, Lia H, Keri Z, Bell C, Patterson L, et al. Machine learn-\ning methods for automated technical skills assessment with instructional \nfeedback in ultrasound-guided interventions. Int J Comput Assist Radiol Surg. \n2019;14(11):1993–2003.\n40. Oropesa I, Sánchez-González P , Chmarra MK, Lamata P , Pérez-Rodríguez R, \nJansen FW, et al. Supervised classification of psychomotor competence \nin minimally invasive surgery based on instruments motion analysis. Surg \nEndosc. 2014;28(2):657–70.\n41. Kumar R, Jog A, Vagvolgyi B, Nguyen H, Hager G, Chen CCG, et al. Objective \nmeasures for longitudinal assessment of robotic surgery training. J Thorac \nCardiovasc Surg. 2012;143(3):528–34.\n42. Ariaeinejad A, Samavi DR. A Performance Predictive Model for Emergency \nMedicine Residents.\n43. Amirhajlou L, Sohrabi Z, Alebouyeh MR, Tavakoli N, Haghighi RZ, Hashemi \nA et al. Application of data mining techniques for predicting residents’ per-\nformance on pre–board examinations: A case study. J Educ Health Promot. \n2019;8.\n44. Yost MJ, Gardner J, Bell RM, Fann SA, Lisk JR, Cheadle WG, et al. Predicting \nacademic performance in surgical training. J Surg Educ. 2015;72(3):491–9.\n45. Woods R, Spadafore M, Yilmaz Y, Rally V, Russell M, Thoma B, et al. Your com-\nment is not as helpful as it could be… do you still want to submit?’ using \nnatural Language processing to identify the quality of supervisor narrative \ncomments in competency based medical education. Can J Emerg Med. \n2023;25(S1):S47.\n46. Spadafore M, Yilmaz Y, Rally V, Chan TM, Russell M, Thoma B, et al. Using \nnatural Language processing to evaluate the quality of supervisor nar-\nrative comments in Competency-Based medical education. Acad Med. \n2024;99(5):534–40.\n47. Zhang R. Automated Assessment of Medical Training Evaluation Text.\n48. Ryder CY, Mott NM, Gross CL, Anidi C, Shigut L, Bidwell SS, et al. Using artificial \nintelligence to gauge competency on a novel laparoscopic training system. J \nSurg Educ. 2024;81(2):267–74.\n49. Stahl CC, Jung SA, Rosser AA, Kraut AS, Schnapp BH, Westergaard M, et al. \nNatural Language processing and entrustable professional activity text \nfeedback in surgery: A machine learning model of resident autonomy. Am J \nSurg. 2021;221(2):369–75.\n50. Solano QP , Hayward L, Chopra Z, Quanstrom K, Kendrick D, Abbott KL, et al. \nNatural Language processing and assessment of resident feedback quality. J \nSurg Educ. 2021;78(6):e72–7.\n51. Neves SE, Chen MJ, Ku CM, Karan S, DiLorenzo AN, Schell RM, et al. Using \nmachine learning to evaluate attending feedback on resident performance. \nAnesth Analg. 2021;132(2):545–55.\n52. Lui A, Chary M, Yoneda N, Parikh S. Tracking resident cognitive maturation \nwith natural language processing. West J Emerg Med., (Lui A, Chary M, \nYoneda N, Parikh S.) New York Presbyterian Queens, Flushing, NY, United \nStates):S46.\n53. Boolchandani H, Osborn R, Tiyyagura G, Sheares B, Chen L, Phatak UP , et al. \nWords used in letters of recommendation for pediatric residency appli-\ncants: demographic differences and impact on interviews. Acad Pediatr. \n2023;23(8):1614–9.\n54. Sarraf D, Vasiliu V, Imberman B, Lindeman B. Use of artificial intelligence \nfor gender bias analysis in letters of recommendation for general surgery \nresidency candidates. Am J Surg. 2021;222(6):1051–9.\n55. Vasan V, Cheng C, Lerner DK, Signore AD, Schaberg M, Govindaraj S, et \nal. Letters of recommendations and personal statements for rhinology \nfellowship: A deep learning linguistic analysis. Int Forum Allergy Rhinol. \n2023;13(10):1971–3.\n56. Gray GM, Williams SA, Bludevich B, Irby I, Chang H, Danielson PD, et al. \nExamining implicit Bias differences in pediatric surgical fellowship letters \nof recommendation using natural Language processing. J Surg Educ. \n2023;80(4):547–55.\n57. Drum B, Shi J, Peterson B, Lamb S, Hurdle JF, Gradick C. Using natural Lan-\nguage processing and machine learning to identify internal Medicine–Pedi-\natrics residency values in applications. Acad Med. 2023;98(11):1278–82.\n58. Burk-Rafel J, Reinstein I, Feng J, Kim MB, Miller LH, Cocks PM, et al. Develop-\nment and validation of a machine Learning-Based decision support tool for \nresidency applicant screening and review. Acad Med. 2021;96(11S):S54–61.\nPage 27 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \n59. Rees CA, Ryder HF. Machine learning for the prediction of ranked applicants \nand matriculants to an internal medicine residency program. Teach Learn \nMed. 2023;35(3):277–86.\n60. Summers JA. Analysis of the impact of step 1 scores on rank order for the \nNRMP match. J Gen Intern Med. 2021;36(11):3582–3.\n61. Pilon S, Tandberg D. Neural network and linear regression models in resi-\ndency selection. Am J Emerg Med. 1997;15(4):361–4.\n62. Ortiz AV, Feldman MJ, Yengo-Kahn AM, Roth SG, Dambrino RJ, Chitale RV, et \nal. Words matter: using natural Language processing to predict neurosurgical \nresidency match outcomes. J Neurosurg. 2023;138(2):559–66.\n63. Mahtani AU, Reinstein I, Marin M, Burk-Rafel J. A new tool for holistic resi-\ndency application review: using natural Language processing of applicant \nexperiences to predict interview invitation. Acad Med. 2023;98(9):1018–21.\n64. Johnstone RE, Neely G, Sizemore DC. Artificial intelligence software can \ngenerate residency application personal statements that program directors \nfind acceptable and difficult to distinguish from applicant compositions. J \nClin Anesth. 2023;89:111185.\n65. Patel V, Deleonibus A, Wells MW, Bernard SL, Schwarz GS. Distinguishing \nauthentic voices in the age of ChatGPT: comparing AI-Generated and \nApplicant-Written personal statements for plastic surgery residency applica-\ntion. Ann Plast Surg. 2023;91(3):324–5.\n66. Yi PK, Ray ND, Segall N. A novel use of an artificially intelligent chatbot and a \nlive, synchronous virtual question-and answer session for fellowship recruit-\nment. BMC Med Educ. 2023;23(1):152.\n67. Zhao XX, Wu SP , Wang JY, Gong XY, He XR, Xi MJ et al. Comparison of Multiple \nQuantitative Evaluation Indices of Theoretical Knowledge and Clinical Prac-\ntice Skills and Training of Medical Interns in Cardiovascular Imaging Using \nBlended Teaching and the Case Resource Network Platform (CRNP). Med Sci \nMonit Int Med J Exp Clin Res. 2020;26(dxw, 9609063):e923836.\n68. Merritt C, Glisson M, Dewan M, Klein M, Zackoff M. Implementation and \nevaluation of an artificial intelligence driven simulation to improve resident \ncommunication with primary care providers. Acad Pediatr. 2022;22(3):503–5.\n69. Webb JJ. Proof of Concept: Using ChatGPT to Teach Emergency Physicians \nHow to Break Bad News. Cureus. 2023 May 9 [cited 2024 May 18]; Available \nfrom:  h t t p  s : /  / w w w  . c  u r e  u s .  c o m /  a r  t i c  l e s  / 1 5 4  3 9  1 - p  r o o  f - o f  - c  o n c  e p t  - u s i  n g  - c h  a t \ng  p t - t  o -  t e a  c h -  e m e r  g e  n c y  - p h  y s i c  i a  n s - h o w - t o - b r e a k - b a d - n e w s\n70. El Saadawi GM, Tseytlin E, Legowski E, Jukic D, Castine M, Fine J, et al. A \nnatural Language intelligent tutoring system for training pathologists: imple-\nmentation and evaluation. Adv Health Sci Educ. 2008;13(5):709–22.\n71. Kelahan LC, Fong A, Ratwani RM, Filice RW. Call case dashboard: tracking R1 \nexposure to High-Acuity cases using natural Language processing. J Am Coll \nRadiol. 2016;13(8):988–91.\n72. Lin H, Yang X, Wang WA, Content-Boosted. Collaborative filtering algorithm \nfor personalized training in interpretation of radiological imaging. J Digit \nImaging. 2014;27(4):449–56.\n73. Muntean GA, Groza A, Marginean A, Slavescu RR, Steiu MG, Muntean V, et al. \nArtificial intelligence for personalised ophthalmology residency training. J \nClin Med. 2023;12(5):1825.\n74. Chen H, Gangaram V, Shih G. Developing a more responsive radiology resi-\ndent dashboard. J Digit Imaging. 2019;32(1):81–90.\n75. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of GPT-4 on \nMedical Challenge Problems. 2023.\n76. Ali R, Tang OY, Connolly ID, Fridley JS, Shin JH, Zadnik Sullivan PL, et al. Perfor-\nmance of ChatGPT, GPT-4, and Google bard on a neurosurgery oral boards \nPreparation question bank. Neurosurgery. 2023;93(5):1090–8.\n77. Bartoli A, May AT, Al-Awadhi A, Schaller K. Probing artificial intelligence in \nneurosurgical training: ChatGPT takes a neurosurgical residents written exam. \nBrain Spine. 2024;4:102715.\n78. Gupta R, Park JB, Herzog I, Yosufi N, Mangan A, Firouzbakht PK, et al. Applying \nGPT-4 to the plastic surgery inservice training examination. J Plast Reconstr \nAesthet Surg. 2023;87:78–82.\n79. Humar P , Asaad M, Bengur FB, Nguyen V. ChatGPT is equivalent to First-Year \nplastic surgery residents: evaluation of ChatGPT on the plastic surgery In-\nService examination. Aesthet Surg J. 2023;43(12):NP1085–9.\n80. Holmes J, Ye S, Li Y, Wu SN, Liu Z, Zhao H et al. Evaluating Large Language \nModels in Ophthalmology.\n81. Cohen A, Alter R, Lessans N, Meyer R, Brezinov Y, Levin G. Performance of \nChatGPT in Israeli Hebrew OBGYN National residency examinations. Arch \nGynecol Obstet. 2023;308(6):1797–802.\n82. Rizzo MG, Cai N, Constantinescu D. The performance of ChatGPT on ortho-\npaedic in-service training exams: A comparative study of the GPT-3.5 turbo \nand GPT-4 models in orthopaedic education. J Orthop. 2024;50:70–5.\n83. Lum ZC. Can artificial intelligence pass the American board of orthopaedic \nsurgery examination?? orthopaedic residents versus ChatGPT. Clin Orthop. \n2023;481(8):1623–30.\n84. Smith J, Choi PM, Buntine P . Will code one day run a code? Performance of \nLanguage models on ACEM primary examinations and implications. Emerg \nMed Australas. 2023;35(5):876–8.\n85. Mahajan AP , Shabet CL, Smith J, Rudy SF, Kupfer RA, Bohm LA. Assessment \nof Artificial Intelligence Performance on the Otolaryngology Residency In-\nService Exam. OTP Open. 2023;7(4). Available from:  h t t p  s : /  / w w w  . s  c o p  u s .  c o m /  \ni n  w a r  d / r  e c o r  d .  u r i  ? e i  d = 2 -  s 2  . 0 -  8 5 1  7 8 2 3  5 7  2 4 &  d o i  = 1 0 .  1 0  0 2 %  2 f o  t o 2 .  9 8  & p a  r t n  e r \nI D  = 4  0 & m  d 5 =  d 4 7 2  c 8  4 8 d 1 7 d f 9 6 2 9 b d 2 6 8 5 f 1 d c 5 7 c 3 2\n86. Cheung BHH, Lau GKK, Wong GTC, Lee EYP , Kulkarni D, Seow CS et al. J Wang \neditor 2023 ChatGPT versus human in generating medical graduate exam \nmultiple choice questions—A multinational prospective study (Hong Kong \nS.A.R., Singapore, Ireland, and the united Kingdom). PLoS ONE 18 8 e0290691.\n87. Wu JT, Wong KCL, Gur Y, Ansari N, Karargyris A, Sharma A, et al. Comparison of \nchest radiograph interpretations by artificial intelligence algorithm vs radiol-\nogy residents. JAMA Netw Open. 2020;3(10):e2022779.\n88. Yi PH, Kim TK, Yu AC, Bennett B, Eng J, Lin CT. Can AI outperform a junior resi-\ndent? Comparison of deep neural network to first-year radiology residents for \nidentification of pneumothorax. Emerg Radiol. 2020;27(4):367–75.\n89. Zhao C, Xiao M, Liu H, Wang M, Wang H, Zhang J, et al. Reducing the number \nof unnecessary biopsies of US-BI-RADS 4a lesions through a deep learn-\ning method for residents-in-training: a cross-sectional study. BMJ Open. \n2020;10(6):e035757.\n90. Homayounieh F, Digumarthy S, Ebrahimian S, Rueckel J, Hoppe BF, Sabel \nBO, et al. An artificial Intelligence–Based chest X-ray model on human \nnodule detection accuracy from a multicenter study. JAMA Netw Open. \n2021;4(12):e2141096.\n91. Han SS, Park I, Eun Chang S, Lim W, Kim MS, Park GH, et al. Augmented intel-\nligence dermatology: deep neural networks empower medical professionals \nin diagnosing skin Cancer and predicting treatment options for 134 skin \ndisorders. J Invest Dermatol. 2020;140(9):1753–61.\n92. Feng Y, Sim Zheng Ting J, Xu X, Bee Kun C, Ong Tien En E et al. Irawan \nTan Wee Jun H,. Deep Neural Network Augments Performance of Junior \nResidents in Diagnosing COVID-19 Pneumonia on Chest Radiographs. Diag-\nnostics. 2023;13(8):1397.\n93. Olsson S, Ohlsson M, Öhlin H, Dzaferagic S, Nilsson M, Sandkull P , et al. Deci-\nsion support for the initial triage of patients with acute coronary syndromes. \nClin Physiol Funct Imaging. 2006;26(3):151–6.\n94. Marchetti MA, Liopyris K, Dusza SW, Codella NCF, Gutman DA, Helba B, et al. \nComputer algorithms show potential for improving dermatologists’ accuracy \nto diagnose cutaneous melanoma: results of the international skin imaging \ncollaboration 2017. J Am Acad Dermatol. 2020;82(3):622–7.\n95. Paul SK, Kim CU, Shieh D, Zhou XY, Pan I, Mehra AA et al. Impact of an Artifi-\ncial Intelligence Algorithm on Diabetic Retinopathy Grading by Ophthalmol-\nogy Residents. medRxiv. 2023;((Paul S.K., samantha.paul2@uhhospitals.org; \nKim C.U.; Shieh D.; Mehra A.A.; Sobol W.M.) Department of Ophthalmology, \nUniversity Hospitals Cleveland Medical Center, Case Western Reserve Univer-\nsity, School of Medicine, Cleveland, OH, United States(Zhou X.Y.). Available \nfrom:  h t t p  s : /  / w w w  . e  m b a  s e .  c o m /  s e  a r c  h / r  e s u l  t s  ? s u  b a c  t i o n  = v  i e w  r e c  o r d &  i d  = L \n2  0 2 6  9 4 5 8  0 4  & f r o m = e x p o r t\n96. Fang Z, Xu Z, He X, Han W. Artificial intelligence-based pathologic myopia \nidentification system in the ophthalmology residency training program. \nFront Cell Dev Biol. 2022;10:1053079.\n97. Lee JH, Ha EJ, Kim D, Jung YJ, Heo S, Jang Y, ho, et al. Application of deep \nlearning to the diagnosis of cervical lymph node metastasis from thyroid \ncancer with CT: external validation and clinical utility for resident training. Eur \nRadiol. 2020;30(6):3066–72.\n98. Chassagnon G, Billet N, Rutten C, Toussaint T, Cassius De Linval Q, Collin M, \net al. Learning from the machine: AI assistance is not an effective learn-\ning tool for resident education in chest x-ray interpretation. Eur Radiol. \n2023;33(11):8241–50.\n99. Shah C, Davtyan K, Nasrallah I, Bryan RN, Mohan S. Artificial Intelligence-Pow-\nered clinical decision support and simulation platform for radiology trainee \neducation. J Digit Imaging. 2022;36(1):11–6.\n100. Thanawala R, Jesneck J, Shelton J, Rhee R, Seymour NE. Overcoming \nsystems factors in case logging with artificial intelligence tools. J Surg Educ. \n2022;79(4):1024–30.\n101. Gong JJ, Soleimani H, Murray SG, Adler-Milstein J. Characterizing styles of \nclinical note production and relationship to clinical work hours among first-\nyear residents. J Am Med Inf Assoc. 2021;29(1):120–7.\nPage 28 of 28\nVerghese et al. BMC Medical Education          (2025) 25:730 \n102. Gao Y, Gu L, Wang Y, Wang Y, Yang F. Constructing a Chinese electronic medi-\ncal record corpus for named entity recognition on resident admit notes. BMC \nMed Inf Decis Mak. 2019;19(S2):56.\n103. Ouyang Y, Wu Y, Wang H, Zhang C, Cheng F, Jiang C, et al. Leveraging \nhistorical medical records as a proxy via multimodal modeling and visualiza-\ntion to enrich medical diagnostic learning. IEEE Trans Vis Comput Graph. \n2024;30(1):1238–48.\n104. Dimitroyannis R, Thodupunoori S, Polster SP , Das P , Roxbury CR. Residency \neducation practices in endoscopic skull base surgery. J Neurol Surg Part B \nSkull Base. 2023;a–2226.\n105. Andrews J, Chartash D, Hay S. Gender bias in resident evaluations: \nnatural Language processing and competency evaluation. Med Educ. \n2021;55(12):1383–7.\n106. Brunyé TT, Booth K, Hendel D, Kerr KF, Shucard H, Weaver DL, et al. Machine \nlearning classification of diagnostic accuracy in pathologists interpreting \nbreast biopsies. J Am Med Inf Assoc. 2024;31(3):552–62.\n107. DiPietro R, Ahmidi N, Malpani A, Waldram M, Lee GI, Lee MR, et al. Segment-\ning and classifying activities in robot-assisted surgery with recurrent neural \nnetworks. Int J Comput Assist Radiol Surg. 2019;14(11):2005–20.\n108. Mathematics and Science Academy at the University of Texas Rio Grande Val-\nley, Edinburg TX, Tippur A. Bridging the Gap: Integrating Artificial Intelligence \ninto Medical Education. DHR Proc. 2023 [cited 2025 Jan 22]; Available from:  h t \nt p  s : /  / d h r  p r  o c e  e d i  n g s .  o r  g / i  n d e  x . p h  p /  D H R  P / a  r t i c  l e  / v i e w / 9 4 / 5 8\n109. Southworth J, Migliaccio K, Glover J, Glover J, Reed D, McCarty C, et al. \nDeveloping a model for AI across the curriculum: transforming the higher \neducation landscape via innovation in AI literacy. Comput Educ Artif Intell. \n2023;4:100127.\n110. Abbott KL, George BC, Sandhu G, Harbaugh CM, Gauger PG, Ötleş E, et al. \nNatural Language processing to estimate clinical competency committee \nratings. J Surg Educ. 2021;78(6):2046–51.\n111. Ötleş E, Kendrick DE, Solano QP , Schuller M, Ahle SL, Eskender MH, et al. Using \nnatural Language processing to automatically assess feedback quality: find-\nings from 3 surgical residencies. Acad Med. 2021;96(10):1457–60.\n112. Booth GJ, Ross B, Cronin WA, McElrath A, Cyr KL, Hodgson JA, et al. \nCompetency-Based assessments: leveraging artificial intelligence to predict \nsubcompetency content. Acad Med. 2023;98(4):497–504.\n113. Brown DC, Gonzalez-Vargas JM, Tzamaras HM, Sinz EH, Ng PK, Yang MX, et al. \nEvaluating the impact of assessment metrics for simulated central venous \ncatheterization training. Simul Healthc J Soc Simul Healthc. 2024;19(1):27–34.\n114. Ebina K, Abe T, Hotta K, Higuchi M, Furumido J, Iwahara N, et al. Objective \nevaluation of laparoscopic surgical skills in wet lab training based on motion \nanalysis and machine learning. Langenbecks Arch Surg. 2022;407(5):2123–32.\n115. Gates RS, Marcotte K, Moreci R, George BC, Kim GJ, Kraft KH, et al. Association \nof gender and operative feedback quality in surgical residents. J Surg Educ. \n2023;80(11):1516–21.\n116. Hernández-Rodríguez J, Rodríguez-Conde MJ, Santos-Sánchez JÁ, Cabrero-\nFraile FJ. Development and validation of an educational software based \nin artificial neural networks for training in radiology (JORCAD) through an \ninteractive learning activity. Heliyon. 2023;9(4):e14780.\n117. Jalali S, Stroulia E, Foster S, Persad A, Shi D, Forgie S, LiveBook. Competence \nAssessment with Virtual-Patient Simulations. In: 2017 IEEE 30th International \nSymposium on Computer-Based Medical Systems (CBMS). Thessaloniki: IEEE; \n2017 [cited 2024 May 18]. pp. 47–52. Available from:  h t t p :   /  / i e e e x p  l o r   e . i  e e  e  . o   r \ng  / d o c  u m e   n t  / 8 1 0 4 1 5 5 /\n118. Madhavan R, Tang C, Bhattacharya P , Delly F, Basha MM. Evaluation of Docu-\nmentation patterns of trainees and supervising physicians using data mining. \nJ Grad Med Educ. 2014;6(3):577–80.\n119. Vasoya MM, Shivakumar A, Pappu S, Murphy CP , Pei Y, Bricker DA, et al. \nReadMI: an innovative app to support training in motivational interviewing. J \nGrad Med Educ. 2019;11(3):344–6.\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.",
  "topic": "Medical education",
  "concepts": [
    {
      "name": "Medical education",
      "score": 0.5824792385101318
    },
    {
      "name": "Engineering ethics",
      "score": 0.3439291715621948
    },
    {
      "name": "Psychology",
      "score": 0.337898313999176
    },
    {
      "name": "Medicine",
      "score": 0.33395975828170776
    },
    {
      "name": "Engineering",
      "score": 0.15243220329284668
    }
  ]
}