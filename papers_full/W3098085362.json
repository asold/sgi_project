{
    "title": "Attention-Based Transformers for Instance Segmentation of Cells in Microstructures",
    "url": "https://openalex.org/W3098085362",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2734362729",
            "name": "Tim Prangemeier",
            "affiliations": [
                "ARC Centre of Excellence in Synthetic Biology"
            ]
        },
        {
            "id": "https://openalex.org/A2119790440",
            "name": "Christoph Reich",
            "affiliations": [
                "ARC Centre of Excellence in Synthetic Biology"
            ]
        },
        {
            "id": "https://openalex.org/A1845223583",
            "name": "Heinz Koeppl",
            "affiliations": [
                "ARC Centre of Excellence in Synthetic Biology"
            ]
        },
        {
            "id": "https://openalex.org/A2734362729",
            "name": "Tim Prangemeier",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2119790440",
            "name": "Christoph Reich",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1845223583",
            "name": "Heinz Koeppl",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963150697",
        "https://openalex.org/W2340897893",
        "https://openalex.org/W3001767870",
        "https://openalex.org/W2941515423",
        "https://openalex.org/W2901968262",
        "https://openalex.org/W3101045001",
        "https://openalex.org/W2999219213",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3015600294",
        "https://openalex.org/W2158706178",
        "https://openalex.org/W3010968348",
        "https://openalex.org/W2968050549",
        "https://openalex.org/W2967608019",
        "https://openalex.org/W2082398508",
        "https://openalex.org/W2889062215",
        "https://openalex.org/W2951271590",
        "https://openalex.org/W2063856853",
        "https://openalex.org/W2548342201",
        "https://openalex.org/W6769388637",
        "https://openalex.org/W2946901414",
        "https://openalex.org/W2889148412",
        "https://openalex.org/W3094813819",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3102402181",
        "https://openalex.org/W3097065222",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6765603740",
        "https://openalex.org/W2966926453",
        "https://openalex.org/W2963870605",
        "https://openalex.org/W2222512263",
        "https://openalex.org/W2962766617",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W6764322716",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W2798040152",
        "https://openalex.org/W3011199263",
        "https://openalex.org/W2996221596",
        "https://openalex.org/W2981298304",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3112906279",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W4288325606",
        "https://openalex.org/W2950141105",
        "https://openalex.org/W2961254972",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Detecting and segmenting object instances is a common task in biomedical applications. Examples range from detecting lesions on functional magnetic resonance images, to the detection of tumours in histopathological images and extracting quantitative single-cell information from microscopy imagery, where cell segmentation is a major bottleneck. Attention-based transformers are state-of-the-art in a range of deep learning fields. They have recently been proposed for segmentation tasks where they are beginning to outperform other methods. We present a novel attention-based cell detection transformer (CellDETR) for direct end-to-end instance segmentation. While the segmentation performance is on par with a state-of-the-art instance segmentation method, Cell-DETR is simpler and faster. We showcase the method's contribution in a the typical use case of segmenting yeast in microstructured environments, commonly employed in systems or synthetic biology. For the specific use case, the proposed method surpasses the state-of-the-art tools for semantic segmentation and additionally predicts the individual object instances. The fast and accurate instance segmentation performance increases the experimental information yield for a posteriori data processing and makes online monitoring of experiments and closed-loop optimal experimental design feasible. Code and data sample is available at https://git.rwth-aachen.de/ bcs/projects/cell-detr.git.",
    "full_text": "2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)\nAttention-Based Transformers for Instance\nSegmentation of Cells in Microstructures\nTim Prangemeier, Christoph Reich, Heinz Koeppl ‡\nCentre for Synthetic Biology,\nDepartment of Electrical Engineering and Information Technology, Department of Biology,\nTechnische Universit¨at Darmstadt\n‡heinz.koeppl@bcs.tu-darmstadt.de\nAbstract—Detecting and segmenting object instances is a\ncommon task in biomedical applications. Examples range from\ndetecting lesions on functional magnetic resonance images, to the\ndetection of tumours in histopathological images and extracting\nquantitative single-cell information from microscopy imagery,\nwhere cell segmentation is a major bottleneck. Attention-based\ntransformers are state-of-the-art in a range of deep learning\nﬁelds. They have recently been proposed for segmentation tasks\nwhere they are beginning to outperform other methods. We\npresent a novel attention-based cell detection transformer (Cell-\nDETR) for direct end-to-end instance segmentation. While the\nsegmentation performance is on par with a state-of-the-art\ninstance segmentation method, Cell-DETR is simpler and faster.\nWe showcase the method’s contribution in a the typical use case\nof segmenting yeast in microstructured environments, commonly\nemployed in systems or synthetic biology. For the speciﬁc use\ncase, the proposed method surpasses the state-of-the-art tools for\nsemantic segmentation and additionally predicts the individual\nobject instances. The fast and accurate instance segmentation\nperformance increases the experimental information yield for a\nposteriori data processing and makes online monitoring of exper-\niments and closed-loop optimal experimental design feasible.\nCode and data samples are available at https://git.rwth-aachen.\nde/bcs/projects/cell-detr.git.\nIndex Terms—attention, instance segmentation, transformers,\nsingle-cell analysis, synthetic biology, microﬂuidics, deep learning\nI. I NTRODUCTION\nInstance segmentation is a common task in biomedical\napplications. It is comprised of both detecting individual object\ninstances and segmenting them [1], [2]. Prevalent examples in\nhealthcare and life sciences include the detection of individual\ntumour or cell entities and the segmentation of their shape. Re-\ncent advances in automated single-cell image processing, such\nas instance segmentation, have contributed to early tumour\ndetection, personalised medicine, biological signal transduc-\ntion and insight into the mechanisms behind cell population\nheterogeneity, amongst others [3]–[6]. An example of instance\nsegmentation is shown in Fig. 1, with four separate cell and\ntwo trap microstructures detected and segmented individually.\nObject detection and panoptic segmentation are closely\nrelated to instance segmentation [2], [7]. Carion et al.recently\nproposed a novel attention-based detection transformerDETR\nfor panoptic segmentation [8]. DETR achieves state-of-the-\nart panoptic segmentation performance, while exhibiting a\ncomparatively simple architecture that is easier to implement\nand is computationally more efﬁcient than previous approaches\n[8]. Its simplicity promises to be beneﬁcial for its adoption in\nreal-world applications.\nTime-lapse ﬂuorescence microscopy (TLFM) is a powerful\ntechnique for studying cellular processes in living cells [4],\n[9]–[11]. The vast amount of quantitative data TLFM yields,\npromises to constitute the backbone of the rational design ofde\nnovo biomolecular functionality [10], [11]. Ideally in synthetic\nbiology, well characterised parts are combined in silico in a\nquantitatively predictive bottom up approach [11]–[13], for\nexample, to detect and kill cancer cells [14], [15].\nQuantitative TLFM with high-throughput microﬂuidics is an\nessential technique for concurrently studying the heterogeneity\nand dynamics of synthetic circuitry on the single cell level\n[4], [9], [11]. A typical TLFM experiment yields thousands of\nspecimen images (Fig. 1) requiring automated segmentation,\nexamples include [5], [16], [17]. Segmenting each individual\ncell enables its pertinent information to be extracted quanti-\ntatively. For example, the abundance of a ﬂuorescent reporter\ncan be measured, giving insight into the cell’s inner workings.\ntraptrap\ncell\ncell\ncell\ncell\nCell-DETR\nFig. 1. Schematic of Cell-DETR direct instance segmentation discerning\nindividual cell (colour) and trap microstructure (grey) object instances.\nInstance segmentation is a major bottleneck in quantify-\ning single-cell microscopy data and manual analysis is pro-\nhibitively labour intensive [9], [11], [16], [18], [19]. The vast\nmajority of single-cell segmentation methods are designed for\na posterioridata processing and often require post-processing\nfor instance detection or manual input [9]. This is not only\na drawback on the amount of experiments that can be per-\nformed, but also limits the type of experiments [18], [20].\nFor example, harnessing the potential of advanced closed-\nloop optimal experimental design techniques [12], [21], [22]\nrequires online monitoring with fast instance segmentation\n978-1-7281-6215-7/20/$31.00 ©2020 IEEE\narXiv:2011.09763v2  [cs.CV]  20 Nov 2020\ncapabilities. Attention-based methods, such as the recently\nproposed detection transformer DETR [8], are increasingly\noutperforming other methods [8], [23]. For the yeast-trap\nconﬁguration (Fig. 1) direct instance segmentation has yet to\nbe employed and attention-based transformers have yet to be\napplied for segmentation in the biomedical ﬁelds in general.\nIn this study, we present Cell-DETR, a novel attention-based\ndetection transformer for instance segmentation of biomedical\nsamples based on DETR [8]. We address the automated cell\ninstance segmentation bottleneck for yeast cells in microstruc-\ntured environments (Fig. 1) and showcase Cell-DETR on this\napplication. Section II introduces the previous segmentation\napproaches and the microstructured environment. Our experi-\nmental setup for ﬂuorescence microscopy, the tested architec-\ntures and our approach to training and evaluation are presented\nin Section III. We analyse the proposed method’s performance\nin Section IV and compare it to the application speciﬁc\nstate-of-the-art, as well as to a general instance segmentation\nbaseline. After interpreting the results and highlighting the\nmethod’s future potential in Section V, we summarise and\nconclude the study in Section VI. Our model surpasses the\nprevious application baseline and is on par with a general state-\nof-the-art instance segmentation method. The relatively short\ninference runtimes enable higher throughput a posteriori data\nprocessing and make online monitoring of experiments with\napproximately 1000 cell traps feasible.\nII. B ACKGROUND\nAn extensive body of research into the automated processing\nof microscopy imagery dates back to the middle of the 20-th\ncentury. Recent studies demonstrate the utility of deep learning\nsegmentation approaches, for example [6], [9], [19], [24],\n[25]. Comprehensive reviews of the many methods to segment\nyeast on microscopy imagery are available elsewhere [3], [20].\nHere we focus on dedicated tools for segmenting cells in\ntrapped microstructures. U-Net convolutional neural networks\n(CNNs) with an encoder-decoder architecture bridged by skip\nconnections have been shown to perform semantic segmen-\ntation well for E. coli mother machines [9], [19] and yeast\nin microstructured environments [6]. In the case of trapped\nyeast, the previous state-of-the-art tool DISCO [16] was based\non conventional methods (template matching, support vector\nmachine, active contours), until recently being superseded by\nU-Nets [6]. The current baseline for semantic segmentation\nof yeast in microstructured environments, as measured by the\ncell class intersection-over-union, is 0.82 [6]. Additional post-\nprocessing, of the segmentation maps is required to attain each\nindividual cell instance [6].\nFor instance segmentation in general, recent state-of-the-art\nmethods are available, for example Mask R-CNN [1]. It is\na proposal-based instance segmentation model, which com-\nbines a CNN backbone, region proposals with non-maximum-\nsuppression, region-of-interest (ROI) pooling, and multiple\nprediction heads [1]. Attention-based methods are increas-\ningly outperforming convolutional methods and are currently\nstate-of-the-art in natural language processing [23]. Beyond\nFig. 2. Single-cell ﬂuorescence measurement setup. Microﬂuidic chip on\nthe microscope table (top right), microscope imagery and design of the\nyeast trap microstructures. The trap chamber (green rectangle) contains an\narray of approximately 1000 traps. Single specimen images show a pair of\nmicrostructures and ﬂuorescent cells, violet contours indicate segmentation of\ntwo separate cell instances with corresponding ﬂuorescence measurement F1\nand F2; black scale bar 1 mm, white scale bar 10 µm.\nnatural language processing, attention-based approaches, such\nas axial-attention modules [26], have demonstrated promising\nresults in computer vision applications [8]. Recently, the ﬁrst\ntransformer-based method (DETR [8]) for object detection and\npanoptic segmentation was reported. DETR achieves state-of-\nthe-art results on par with Faster R-CNN and constitutes a\npromising approach for further improvements in automated\nobject detection and segmentation performance.\nThe microﬂuidic trap microstructures we consider here are\ndesigned for long-term culture of yeast cells ( Saccharomyces\ncerevisiae) within the focal plane of a microscope [17]. The\non-chip environment is tightly controlled and conducive to\nyeast growth. Examples of its routine employ include Fig.\n2 and [4]–[6], [11], [16]. A constant ﬂow of yeast growth\nmedia hydrodynamically traps the cells in the microstructures\nand allows the introduction of chemical perturbations. An\nautomated microscope records an entire trap chamber of up\nto 1000 traps by imaging both the brightﬁeld and ﬂuorescent\nchannels at approximately 20 neighbouring positions. Typical\nexperiments each produce hundreds of GB of image data.\nTime-lapse recordings allow individual cells to be tracked\nthrough time. Robust instance segmentation facilitates tracking\n[9], [20], which itself can be a limiting factor with regard to\nthe data yield of an experiment [4], [9], [19], [20].\nIII. M ETHODOLOGY\nA. Live-cell microscopy dataset and annotations\nThe individual specimen images each contain a single\nmicroﬂuidic trap and some yeast cells, as depicted in Fig.\n1. These are extracted from larger microscope recordings,\nwhereby each exposure contains up to 50 traps (Fig. 2 mid-\ndle). Ideally, a single mother cell persists in each trap, with\nsubsequent daughter cells being removed by the constant ﬂow\nof yeast growth media. In practice, multiple cells accumulate\naround some traps, while other traps remain empty (Fig. 3).\nWe distinguish between three classes on the specimen image\nannotations, as depicted in Fig. 3. The yeast cells in violet\nare the most important class for biological applications. To\nbrightfield background traps cells\nFig. 3. Example of class and instance annotations for a specimen image;\nbrightﬁeld image (left), background label in light greylight grey ■ ■, instances of the\ntrap class in shades of dark greydark grey ■ ■and instances of the cell class in shades\nof violetviolet ■ ■(left to right respectively); scale bar 10 µm.\ncounteract traps being segmented as cells, we employ a distinct\nclass for them (dark grey). The background (light grey) is\nannotated for semantic segmentation training, for example of\nU-Nets. For instance segmentation training we introduce a no-\nobject class ∅ in place of the background class.\nEach instance of cells or trap structures are annotated indi-\nvidually with a bounding box, class speciﬁcation and separate\npixel-wise segmentation map. Here we omit the bounding\nboxes to enable an unobscured view of the contours. Instead,\nthe distinct cell instances and their individual segmentation\nmaps are indicated by different shades of violet in Fig. 3.\nThe annotated set of 419 specimen images from various\nexperiments was randomly assigned for network training,\nvalidation and testing ( 76 %, 12 % and 12 % respectively).\nExamples are shown in Fig. 4. Images include a balance of the\ncommon yeast-trap conﬁgurations: 1) empty traps, 2) single\ncells (with daughter) and 3) multiple cells. Slight variations in\ntrap fabrication, debris, contamination, focal shift, illumination\nlevels and yeast morphology were included. Further scenarios\nor strong variations, such as trap design geometries, model\norganisms and signiﬁcant focal shift, were omitted.\nB. The Cell-DETR instance segmentation architecture\nThe proposed Cell-DETR models A and B are based on\nthe DETR panoptic segmentation architecture [8]. We adapted\nthe architecture for non-overlapping instance segmentation and\nreduced it in size for faster inference. The main differences\nbetween DETR and our variants Cell-DETR A and B are\nsummarised in Table I. The Cell-DETR variants have ap-\nproximately one order of magnitude less parameters than the\noriginal (∼40 ×106 reduced to ∼5 ×106 parameters). The\nmain building blocks of the Cell-DETR model are detailed in\nFig. 5. They are the backbone CNN encoder, the transformer\nencoder-decoder, the bounding box and class prediction heads,\nand the segmentation head.\nThe CNN encoder (left in Fig. 5) extracts image features\nof the brightﬁeld specimen image input. It is based on four\nResNet-like [27] blocks with 64, 128, 256 and 256 convo-\nlutional ﬁlters. After each block a 2 ×2 average pooling\nlayer is utilised to downsample the intermediate feature maps.\nThe Cell-DETR variants employ different activations and\nconvolutions, as detailed in Table I.\nThe transformer encoder determines the attention between\nimage features. The transformer decoder predicts the attention\nFig. 4. Characteristic selection of specimen images and corresponding\nannotations, including empty or single trap structures, trapped single cells\n(with single daughter adjacent) and multiple trapped cells; trap instances in\nshades of dark greydark grey ■ ■, cell instances in shades of violetviolet ■ ■and transparent\nbackground; scale bar 10 µm.\nregions for each of the N = 20 object queries. They are both\nbased on the DETR architecture [8]. We reduced the number\nof transformer encoder blocks to three and decoder blocks to\ntwo, each with 512 hidden features in the feed-forward neural-\nnetwork (FFNN). The 128 backbone features are ﬂattened\nbefore being fed into the transformer. In contrast to the original\nDETR, we employed learned positional encodings. While\nCell-DETR A employs leaky ReLU [28] activations, Pad ´e\nactivation units [29] are utilised for Cell-DETR B.\nTABLE I\nOVERVIEW OF DIFFERENCES BETWEEN DETR, C ELL -DETR A AND B.\n——-\nModel\nActivation\nfunctions\nConvolutions Feature\nfusion\nParam.\n×106\nDETR [8] ReLU standard\nspatial\naddition ⪆ 40\nC-DETR A leaky ReLU\n[28]\nstandard\nspatial\naddition 4.3\nC-DETR B Pad ´e [29] deformable\n(v2) [30]\npix.-adapt.\nconv. [31]\n5.0\nThe prediction heads for the bounding box and classiﬁcation\nare each a FFNN. They map the transformer encoder-decoder\noutput to the bounding box and classiﬁcation prediction. These\nFFNN process each query in parallel and share parameters\nover all queries. In addition to the cell and trap classes, the\nclassiﬁcation head can also predict the no-object class ∅.\ninput image\nbackbone\nCNN\nencoder\nimage features\ntransformer\nencoder-\ndecoder\nFFNN\nFFNN class & BB\npredictions\ncell\ncell\ncell\ntrap trap\nencoder features multi-head-\nattention\nCNN\ndecoder\n||\nskip connections\nsegmentation\nprediction\nFig. 5. Architecture of the end-to-end instance segmentation network, with brightﬁeld specimen image input and an instance segmentation prediction as output.\nThe backbone CNN encoder extracts image features that then feed into both the transformer encoder-decoder for class and bounding box prediction, as well\nas to the CNN decoder for segmentation. The transformer encoded features, as well as the transformer decoded features, are fed into a multi-head-attention\nmodule and together with the image features from the CNN backbone feed into the CNN decoder for segmentation. Skip connections additionally bridge\nbetween the backbone CNN encoder and the CNN decoder. Input and output resolution is 128 × 128 pixels.\nThe segmentation head is composed of a multi-head atten-\ntion mechanism and a CNN decoder to predict the segmen-\ntation maps for each object instance. We employ the original\nDETR [8] two-dimensional multi-head attention mechanism\nbetween the transformer encoder and decoder features. The\nresulting attention maps are concatenated channel-wise onto\nthe image features and fed into the CNN decoder. The three\nResNet-like decoder blocks decrease the feature channel size\nwhile increasing the spatial dimensions. Long skip connections\nbridge between the CNN encoder and CNN decoder blocks’\nrespective outputs. The features are fused by element-wise\naddition in Cell-DETR A and by pixel-adaptive convolutions\nin Cell-DETR B. A fourth convolutional block incorporates\nthe queries in the feature dimension and returns the original\ninput’s spatial dimension for each query. Non-overlapping\nsegmentation is ensured by a softmax over all queries.\nC. Training Cell-DETR\nWe employ a combined loss function and a direct set\nprediction to train our Cell-DETR networks end-to-end. The\nset prediction ˆy = {ˆyi = {ˆpi,ˆbi,ˆsi}}N=20\n1 is comprised of\nthe respective predictions for class probability ˆpi ∈RK (here\nK = 3 classes, no-object, trap, cell), bounding box ˆbi ∈R4\nand segmentation ˆsi ∈R128×128 for each of the N queries.\nWe assigned each instance set label yσ(i) to the corresponding\nquery set prediction ˆyi with the Hungarian algorithm [8], [32].\nThe indices σi denote the best matching permutation of labels.\nThe combined loss Lis comprised of a classiﬁcation loss\nLp, a bounding box loss Lb, and a segmentation loss Ls\nL=\nN∑\ni=1\n(\nLp + 1{pi̸=∅}Lb + 1{pi̸=∅}Ls\n)\n,\nwith N = 20 object instance queries in this case. We employ\nclass-wise weighted cross entropy for the classiﬁcation loss\nLp\n(\npσ(i),ˆpi\n)\n= −\nK∑\nk=1\nβkpσ(i),klog(ˆpi,k),\nwith weights β = [0 .5,0.5,1.5] for the K = 3 classes, no-\nobject, trap and cell classes respectively. The bounding box\nloss is itself composed of two weighted loss terms. These are\na generalised intersection-over-unionLJ [33], and a L1 loss,\nwith respective weights λJ = 0.4 and λL1 = 0.6\nLb\n(\nbσ(i),ˆbi\n)\n= λJLJ\n(\nbσ(i),ˆbi\n)\n+ λL1\n⏐⏐⏐\n⏐⏐⏐bσ(i) −ˆbi\n⏐⏐⏐\n⏐⏐⏐\n1\n.\nThe segmentation loss Ls is a weighted sum of the focal\nloss LF [34] and Sørensen-Dice loss LD [6], [8]\nLs\n(\nsσ(i),ˆsi\n)\n= λFLF\n(\nsσ(i),ˆsi; γ\n)\n+ λDLD\n(\nsσ(i),ˆsi; ϵ\n)\n.\nThe respective weights are λF = 0 .05 and λD = 1 , with\nfocusing parameter γ = 2 and ϵ= 1 for numerical stability.\nD. Evaluation and implementation\nWe employ a number of metrics to quantitatively analyse\nthe performance of the trained networks with regard to classi-\nﬁcation, bounding box and segmentation performance. Given\nthe ground truth Y and the prediction ˆY (in the corresponding\ninstance-matched permutation), we evaluate the segmentation\nperformance with variants of the Jaccard index J and the\nSørensen-Dice Dcoefﬁcient [6], [8], omitting the background\nD(Y, ˆY) = 2|Y ∩ˆY|\n|Y|+ |ˆY|\n; Jk(Yk, ˆYk) = |Yk ∩ˆYk|\n|Yk ∪ˆYk|\n, (1)\nwith Jk intuitively the intersection-over-union for each class\nk. With respect to the metrological application in image\ncytometry, the cell class is of most importance, therefore,\nwe consider the Jaccard index for the cell class alone ( Jc).\nSimilarly, in the case of instance segmentation, we compute\nJi for each instance i and average over all I object instances\nto compute the mean instance Jaccard index ¯JI = 1\nI\n∑I\ni=1 Ji.\nWe utilise the accuracy as the proportion of correct pre-\ndictions for classiﬁcation. The bounding boxes are evaluated\nwith the Jaccard index ¯Jb. It is deﬁned analogously to the\nobject instance Jaccard index (compare Eqn. 1), yet computed\nimplicitly with the bounding box coordinates.\nWe compare the proposed method with our own imple-\nmentations of both the state-of-the-art for the trapped yeast\napplication (U-Net [6]), as well as more generally with a\nstate-of-the-art instance segmentation meta algorithm (Mask\nR-CNN [1]). The multiclass U-Net for semantic segmentation\nwas implemented in PyTorch, with the architecture, pre- and\npost-processing described in [6]. We implemented a Mask R-\nCNN [1] with Torchvision (PyTorch) and a ResNet-18 [27]\nbackbone, which was pre-trained for image classiﬁcation.\nWe implemented the proposed Cell-DETR A and B archi-\ntectures with PyTorch. We used the MMDetection toolkit [35]\nfor deformable convolutions and the PyTorch/Cuda implemen-\ntation for the Pad´e activation units [29]. We trained the models\nusing AdamW [36] for optimisation with a weight decay of\n10−6. The initial learning rate was 10−5 for the backbone\nand 10−4 for the rest of the model. The learning rates were\ndecreased by an order of magnitude after 50 and again 100\nepochs of the total 200 epochs. The additional ﬁrst and second-\norder momentum moving average factors were 0.9 and 0.999\nrespectively. We selected the best performing model based\non the cell class Jaccard index Jc, typically after 80 to 140\nepochs with mini batch size 8. The training data was randomly\naugmented by elastic deformation [6], [24], horizontal ﬂipping\nor by the addition of noise with a probability of 0.6. Inference\nruntimes for one forward pass were averaged over 1000 runs\non a Nvidia RTX 2080 Ti for all three methods (U-Net, Mask\nR-CNN and Cell-DETR).\nE. Data acquisition setup\nYeast cells were cultured in a tightly controlled microﬂuidic\nenvironment. A temperature of 30 °C and the ﬂow of yeast\ngrowth media enables yeast to grow for prolonged periods and\nover multiple cell-cycles. The microﬂuidic chips conﬁned the\ncells to the focal plane of the microscope. Continuous media\nﬂow hydrodynamically traps the living cells in the microstruc-\ntures. The Polydimethylsiloxane (PDMS) microstructures con-\nstrain the cells in XY , while axial constraints in Z are provided\nby the cover slip and PDMS ceiling. The space between cover\nslip and the PDMS ceiling is on the order of a cell diameter\nto facilitate continuously uniform focus of the cells.\nWe recored time-lapse brightﬁeld (transmitted light) and\nﬂuorescent channel imagery of the budding yeast cells every\n10 min with a computer controlled microscope (Nikon Eclipe\nTi with XYZ stage; µManager; 60x objective). A CoolLED\npE-100 and a Lumencor SpectraX light engine illuminated the\nrespective channels, which were captured with a ORCA Flash\n4.0 (Hamamatsu) camera. Multiple lateral and axial positions\nwere recorded sequentially at each timestep (Fig. 2).\nIV. R ESULTS\nA. Cell-DETR variant results\nA sample of segmentation results for the two Cell-DETR\nvariants is shown in Fig. 6. The cell and trap instances are all\ndetected and classiﬁed correctly with slight variations in seg-\nmentation contours. Separate instances of cells and traps are\nindicated by the shades of violet and grey respectively. Variant\nB demonstrates slightly better segmentation performance. A\nqualitative example of this is shown in in Fig. 6, where Cell-\nDETR A in contrast to B excludes a small section of one cell.\nbrightfield C-DETR A C-DETR B label\nFig. 6. Qualitative comparison of Cell-DETR A and B segmentation\nexamples for a selected test image (left) and label (right); trap instances in\nshades of dark greydark grey ■ ■, cell instances in shades in violetviolet ■ ■; scale bar 10 µm.\nThe quantitative comparison between the segmentation per-\nformance of the Cell-DETR variants is summarised in Table\nII. We modiﬁed model B for better performance on our appli-\ncation, as described in Section III-B. The mean Jaccard index\nover all object instances increased from ¯JI = 0.84 for model\nA to ¯JI = 0.85 for model B, while the cell class Jaccard index\nincreased by a similar margin from Jc = 0.83 to Jc = 0.84.\nTaking the background into account, a segmentation accuracy\nof 0.96 is achieved. Both Cell-DETR surpass the segmentation\nperformance (Jc) of the previous state-of-the-art methods for\nthe trapped yeast application [6], [16], in addition to directly\nattaining the instances.\nTABLE II\nSEGMENTATION PERFORMANCE OF CELL -DETR A AND B.\n———\nModel\nSørensen\nDice\nD\nMean\ninstance\n¯JI\n——–\nCell class\nJc\nSeg.\naccuracy\nC-DETR A 0.92 0.84 0.83 0.96\nC-DETR B 0.92 0.85 0.84 0.96\nThe bounding box and classiﬁcation performance is sum-\nmarised in Table III. Again, both models perform similarly\nwell. They correctly classify the object instances (classiﬁcation\naccuracy of 1.0) and detect the correct number of instances\nfor each class. They also perform similarly well at localising\nthe instances, achieve a bounding box intersection-over-union\nof Jb = 0 .81, for the standard formulation as well as the\ngeneralised form employed for training.\nTABLE III\nBOUNDING BOX AND CLASSIFICATION PERFORMANCE METRICS FOR\nCELL -DETR A AND B.\nBounding box Classiﬁcation\nModel Jaccard Jb accuracy\nC-DETR A 0.81 1.0\nC-DETR B 0.81 1.0\nThe slight increase in segmentation performance that model\nB yields is a trade off with increased computational cost. The\nnumber of parameters is increased from approximately 4×106,\nto over 5 ×106 (Table I). This leads to an increase in runtime\nfrom 9.0 ms for model A to 21.2 ms for model B. These times\nare orders of magnitude faster than the previous state-of-the-\nart method DISCO [16] and on the same order of magnitude as\nthe currently fastest reported network for this application [6].\nRuntimes on this order of magnitude sufﬁce for in-the-loop\nexperimental techniques.\nWe select model B for further analysis, based on the im-\nproved performance and sufﬁciently fast runtimes. A selection\nof segmentation predictions for the three most typical scenar-\nios in the test dataset is given in Fig. 7. The detection of cell\nand trap instances, without any overlap between instances, is\nsuccessful for single cells (middle row), multiple cells (bottom\nrow), and empty traps are correctly identiﬁed. The introduction\nof multiple classes (traps, cells), as well as individual object\ninstances facilitated individually segmenting each cell entity\nand discerning these from both the traps and other cells.\nbrightfield overlay pred. mask\n label\nFig. 7. Example of different scenarios from the test dataset segmented with\nCell-DETR B: an empty trap (top row), a single trapped cell (middle row)\nand multiple cells; columns are brightﬁeld, an overlay of the prediction, the\nprediction mask and the ground truth label (left to right respectively). Colours\nindicate traps in shades of greygrey ■ ■and cell instances in shades of violetviolet ■ ■;\nscale bar 10 µm.\nThe intended application of our method is to deliver seg-\nmentation masks for each cell instance for subsequent single-\ncell ﬂuorescence measurements. We trialled this application\non unlabelled and unseen data as depicted in Fig. 8. The\ncell instances are detected based on the brightﬁeld image\n(left) and the resulting object segmentation predictions are\nused as masks to measure the individual cell ﬂuorescence on\nthe ﬂuorescent channel (right). An overlay of the brightﬁeld,\nﬂuorescent images with the segmentation contours is depicted\nin the middle, along with the green ﬂuorescent protein (GFP)\nchannel. The individual cell area ( A1 and A2) is measured as\nthe number of pixels in the instance segmentation mask and\nindicated on the GFP channel. The cell instance ﬂuorescence\n(F1 and F2) is summed over the mask area and indicated on\nthe right for each individual cell in arbitrary ﬂuorescence units.\nbrightfield bf + gfp gfp gfp masks\nF1 = 6.1x106 a.u.\nF2 = 2.5x106 a.u.\nA1 = 929 pix.\nA2 = 241 pix.\nFig. 8. Example of individual cell ﬂuorescence measurement application with\na segmentation mask contour for each individual cell (violet contours violetviolet\n■ ■) based on the brightﬁeld image (left); scale bar 10 µm.\nB. Comparison with state-of-the-art methods\nWe compare our proposed method with the state-of-the-art\nfor the trapped yeast application (DISCO [16], U-Net [6]),\nas well as with a general state-of-the-art method for instance\nsegmentation (Mask R-CNN [1]). We implemented both the\nU-Net and Mask R-CNN methods in this study (Section III-D).\nA characteristic qualitative example of the results is given in\nFig. 9, with the ground truth on the left, followed by Cell-\nDETR B, Mask R-CNN and U-Net segmentations results. All\nthree methods segment two trap microstructures and all four\ncells in separate classes, without any overlap or touching cells.\nCell-DETR B and Mask R-CNN additionally segment each\ncell or trap object as an individual instance. The contours are\nslightly smaller for the U-Net, which is deemed a result of\nthe emphasis on avoiding touching cells and the associated\ndifﬁculty of discerning these in subsequent post-processing.\nlabel C-DETR B M. R-CNN U-Net\nFig. 9. Example segmentation for our implementations of Cell-DETR B, Mask\nR-CNN and U-Net. Trap instances in shades of greygrey ■ ■and cell instances in\nshades of violetviolet ■ ■(no instance detection for U-Net); scale bar 10 µm.\nAccurate segmentation of the cells is particularly important\nfor the measurement of cell morphology or ﬂuorescence. We\ncompare the cell class Jaccard index Jc of our proposed\nmethods Cell-DETR A and B with the application state-of-\nthe-art methods DISCO, U-Net and Mask R-CNN. The com-\nparison is summarised in Table IV. U-Net recently superseded\nDISCO [16] ( Jc ∼0.7) as the state-of-the-art trapped yeast\nsegmentation method, achieving Jc = 0.82. Our Cell-DETR\nvariants both further improve on this result, with model B\nachieving the same Jc = 0.84 on par with our Mask R-CNN\nimplementation. Cell-DETR and Mask R-CNN additionally\nprovide each cell object instance.\nWe measured the average runtime of a forward pass of each\nmethod on a single specimen image (Table IV). For DISCO\nTABLE IV\nCOMPARISON OF CELL -DETR PERFORMANCE WITH THE\nSTATE-OF-THE -ART METHODS FOR THE TRAPPED YEAST APPLICATION\n(DISCO, U-N ET) AND INSTANCE SEGMENTATION (MASK R-CNN).\n———–\nModel\nCell\nClass Jc\nInference\nruntime1\n——\nInstances\nDISCO [16]2 ∼0.70 ∼1300 ms ×\nU-Net 0.82 1 .8 ms ×\nMask R-CNN 0.84 29 .8 ms ✓\nCell-DETR A 0.83 9 .0 ms ✓\nCell-DETR B 0.84 21 .2 ms ✓\n1 Runtimes for U-Net, Mask R-CNN, and Cell-DETR averaged\nover 1000 runs ( ∼300 different images) on a Nvidia RTX\n2080 Ti; measurement uncertainty is below ±5%.\n2 Reported literature values [16].\n[16] we consider the reported values that include some pre-\nand post-processing steps to detect cells individually. The deep\nmethods are signiﬁcantly faster than DISCO, making online\nmonitoring of live experiments feasible. The U-Net is the\nfastest, taking 1.8 ms for a forward pass, in contrast to 29.8 ms\nfor the Mask R-CNN [1]. However, the U-Net requires further\npost-processing steps to detect the object instances and has\nbeen reported to take approximately 20 ms in conjunction with\nwatershed post-processing [6]. The Cell-DETR variants take\nthe middle ground with 9.0 ms and 21.2 ms.\nV. D ISCUSSION\nA. Analysis of the instance segmentation performance\nCell-DETR has some beneﬁts in comparison to state-of-\nthe-art methods, such as Mask R-CNN. The Cell-DETR\narchitecture is comparatively simple and avoids common\nhand designed components of Mask R-CNNs, such as non-\nmaximum suppression and ROI pooling. This reduces Cell-\nDETR’s reliance on hyperparameters and facilitates end-to-\nend training with a single combined loss function. In contrast,\nMask R-CNNs require additional supervision to train the\nregion proposal network. As a result of these differences, Cell-\nDETR is easier to implement, has less parameters and is faster\nthan Mask R-CNN for the same segmentation performance.\nWhile Cell-DETR does not rely on explicit region proposals,\nit does utilise attention maps that highlight the pertinent\nfeatures in the latent space. The mapping of these is learnt\nduring the end-to-end training. The loss curves of individual\nprediction tasks are shown in Fig. 10. The classiﬁcation loss\nLp (blue) converges ﬁrst, indicating that the network ﬁrst\nlearns how many objects are present in an image and to which\nclass they belong. The bounding box loss Lb (red) converges\nnext, with the network learning the approximate location of\neach object. Finally, the model learns to reﬁne the pixel-\nwise segmentation maps with the segmentation loss Ls (green)\nconverging last.\nWith respect to the speciﬁc single-cell measurement appli-\ncation, Cell-DETR offers robust and repeatable instance seg-\nmentation of yeast cells in microstructures. The key cell class\nsegmentation performance surpasses the previous state-of-the-\nart semantic segmentation methods [6], [16] with a cell class\n200 400 600 800 1000\n0.2\n0.4\n0.6\n0.8\n1\ntraining steps\nloss\nLp classiﬁcation\nLb bounding box\nLs segmentation\nFig. 10. Classiﬁcation, bounding boxes and segmentation loss curves for\nCell-DETR B; thick lines are running averages (window size 30).\nJaccard index of 0.84. Additionally, the proposed technique\ndirectly detects individual object instances and classiﬁes the\nobjects robustly (near 100 % accuracy). The robust instance\nsegmentation performance promises to facilitate cell tracking,\nincrease the experimental information yield and enables Cell-\nDETR to be employed without human intervention.\nB. Limitations, outlook and future potential\nThe presented models are trained for a speciﬁc microﬂuidic\nconﬁguration and trap geometry. While they are relatively\nrobust and fulﬁl their intended purpose, their utility could be\nbroadened by expanding the dataset to include more classes,\nfor example different trap geometries. More generally as an in-\nstance segmentation method, Cell-DETR offers a platform for\nincorporating future advances in attention mechanisms as they\nare increasingly outperforming convolutional approaches. For\nexample, replacing the convolutional elements in the backbone\nand segmentation head with axial-attention [26] may lead to\nfurther improved performance. Currently, Cell-DETR achieves\nstate-of-the-art performance and as an instance segmentation\nmethod is generally suitable for and readily adaptable to a\nwide range of biomedical imaging applications.\nThe presented Cell-DETR methods can be harnessed for\nhigh-content quantitative single-cell TLFM. Cell-DETR, Mask\nR-CNN and U-Net achieve runtimes orders of magnitudes\nfaster than the previous state-of-the-art trapped yeast method\n(DISCO [16]). These runtimes coupled with Cell-DETRs\nrobust instance segmentation make both online monitoring\nand closed-loop optimal experimental design of typical ex-\nperiments with approximately 1000 traps feasible. Harnessing\nthis potential promises to provide increased experimental in-\nformation yields and greater biological insights in the future.\nVI. C ONCLUSION\nIn summary, we present Cell-DETR, an attention-based\ntransformer method for direct instance segmentation and show-\ncase it on a typical application. To the best of our knowl-\nedge, this is the ﬁrst application of detection transformers on\nbiomedical data. The proposed method has fewer parameters\nand is 30% faster while matching the segmentation perfor-\nmance of a state-of-the-art Mask R-CNN. A simpler Cell-\nDETR variant exhibits slightly lesser segmentation perfor-\nmance (Jc = 0.83 instead of 0.84) while requiring 1/3rd of a\nMask R-CNN’s runtime. As a general instance segmentation\nmodel, Cell-DETR achieves state-of-the-art performance and\nis deemed suitable and readily adaptable for a range of\nbiomedical imaging applications.\nShowcased on a typical systems or synthetic biology ap-\nplication, the proposed Cell-DETR robustly detects each cell\ninstance and directly provides instance-wise segmentation\nmaps suitable for cell morphology and ﬂuorescence measure-\nments. In comparison to the previous semantic segmentation\ntrapped yeast baselines, Cell-DETR provides better segmen-\ntation performance with a cell class Jaccard index Jc = 0.84\nwhile additionally detecting each individual cell instance and\nmaintaining comparable runtimes. This promises to reduce\nmeasurement uncertainty, facilitate cell tracking efﬁcacy and\nincrease the experimental data yield in future applications. The\nresulting runtimes and accurate instance segmentation make\nfuture online monitoring feasible, for example for closed-loop\noptimal experimental control.\nACKNOWLEDGEMENTS\nWe thank Christian Wildner for insightful discussions,\nAndr´e O. Franc ¸ani and Jan Basrawi for contributing to la-\nbelling and Markus Baier for aid with the computational setup.\nThis work was supported by the Landesoffensive f ¨ur wis-\nsenschaftliche Exzellenz as part of the LOEWE Schwerpunkt\nCompuGene. H.K. acknowledges support from the European\nResearch Council (ERC) with the consolidator grant CONSYN\n(nr. 773196).\nREFERENCES\n[1] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick, “Mask R-CNN,” inIEEE\nICCV, 2017, pp. 2961–2969.\n[2] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benen-\nson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for\nsemantic urban scene understanding,” in IEEE/CVF CVPR, 2016.\n[3] J. Sun, A. T ´arnok, and X. Su, “Deep Learning-Based Single-Cell Optical\nImage Studies,” Cytom. Part A, vol. 97, no. 3, pp. 226–240, 2020.\n[4] M. Leygeber, D. Lindemann, C. C. Sachs, E. Kaganovitch, W. Wiechert,\nK. N ¨oh, and D. Kohlheyer, “Analyzing Microbial Population Hetero-\ngeneity - Expanding the Toolbox of Microﬂuidic Single-Cell Cultiva-\ntions,” J. Mol. Biol., 2019.\n[5] A. Hofmann, J. Falk, T. Prangemeier, D. Happel, A. K ¨ober, A. Christ-\nmann, H. Koeppl, and H. Kolmar, “A tightly regulated and adjustable\nCRISPR-dCas9 based AND gate in yeast,” Nucleic Acids Res., vol. 47,\nno. 1, pp. 509–520, 2019.\n[6] T. Prangemeier, C. Wildner, A. O. Franc ¸ani, C. Reich, and H. Koeppl,\n“Multiclass yeast segmentation in microstructured environments with\ndeep learning,” IEEE CIBCB, 2020.\n[7] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Doll ´ar, “Panoptic\nsegmentation,” in IEEE/CVF CVPR, 2019, pp. 9404–9413.\n[8] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,”\narXiv:2005.12872, 2020.\n[9] J.-B. Lugagne, H. Lin, and M. J. Dunlop, “DeLTA: Automated cell\nsegmentation, tracking, and lineage reconstruction using deep learning,”\nPLoS Comput Biol, vol. 16, no. 4, 2020.\n[10] R. Pepperkok and J. Ellenberg, “High-throughput ﬂuorescence mi-\ncroscopy for systems biology,” Nat. Rev. Mol. Cell Biol., p. 690, 2006.\n[11] T. Prangemeier, F. X. Lehr, R. M. Schoeman, and H. Koeppl, “Microﬂu-\nidic platforms for the dynamic characterisation of synthetic circuitry,”\nCurr. Opin. Biotechnol., vol. 63, pp. 167–176, 2020.\n[12] D. G. Cabeza, L. Bandiera, E. Balsa-Canto, and F. Menolascina, “Infor-\nmation content analysis reveals desirable aspects of in vivo experiments\nof a synthetic circuit,” in IEEE CIBCB, 2019, pp. 1–8.\n[13] F.-X. Lehr, M. Hanst, M. V ogel, J. Kremer, H. U. G ¨oringer, B. Suess,\nand H. Koeppl, “Cell-free prototyping of and-logic gates based on\nheterogeneous rna activators,” ACS Synth. Biol., p. 2163, 2019.\n[14] Z. Xie, L. Wroblewska, L. Prochazka, R. Weiss, and Y . Benenson,\n“Multi-input RNAi-based logic circuit for identiﬁcation of speciﬁc\ncancer cells,” Science, vol. 333, pp. 1307–1312, 2011.\n[15] W. Si, C. Li, and P. Wei, “Synthetic immunology: T-cell engineering\nand adoptive immunotherapy,” Synth. Syst. Biotechnol., vol. 3, no. 3,\npp. 179–185, 2018.\n[16] E. Bakker, P. S. Swain, and M. M. Crane, “Morphologically constrained\nand data informed cell segmentation of budding yeast,” Bioinformatics,\nvol. 34, no. 1, pp. 88–96, 2018.\n[17] M. M. Crane, I. B. N. Clark, E. Bakker, S. Smith, and P. S. Swain,\n“A Microﬂuidic System for Studying Ageing and Dynamic Single-Cell\nResponses in Budding Yeast,” PLoS One, vol. 9, p. e100042, 2014.\n[18] D. A. Van Valen, T. Kudo, K. M. Lane, D. N. Macklin, N. T.\nQuach, M. M. DeFelice, I. Maayan, Y . Tanouchi, E. A. Ashley, and\nM. W. Covert, “Deep Learning Automates the Quantitative Analysis of\nIndividual Cells in Live-Cell Imaging Experiments,” PLoS Comput Biol,\nvol. 12, no. 11, pp. 1–24, 2016.\n[19] J. Sauls, J. Schroeder, S. Brown, G. Treut, F. Si, D. Li, J. Wang, and\nS. Jun, “Mother machine image analysis with MM3,” bioRxiv, 2019.\n[20] E. Moen, D. Bannon, T. Kudo, W. Graf, M. Covert, and D. Van Valen,\n“Deep learning for cellular image analysis,” Nat. Methods, vol. 16,\nno. 12, p. 1233, 2019.\n[21] T. Prangemeier, C. Wildner, M. Hanst, and H. Koeppl, “Maximizing\ninformation gain for the characterization of biomolecular circuits,” in\nProc. 5th ACM/IEEE NanoCom, 2018, pp. 1–6.\n[22] L. Bandiera, D. Gomez-Cabeza, J. Gilman, E. Balsa-Canto, and F. Meno-\nlascina, “Optimally Designed Model Selection for Synthetic Biology,”\nACS Synth. Biol., 2020.\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS,\n2017, pp. 5998–6008.\n[24] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional Net-\nworks for Biomedical Image Segmentation,” in MICCAI, 2015, p. 234.\n[25] N. Dietler, M. Minder, V . Gligorovski, A. M. Economou, D. A. H. L.\nJoly, A. Sadeghi, C. H. M. Chan, M. Kozi ´nski, M. Weigert, A.-F.\nBitbol, and S. J. Rahi, “A convolutional neural network segments yeast\nmicroscopy images with high accuracy,” Nat. Commun., p. 5723, 2020.\n[26] H. Wang, Y . Zhu, B. Green, H. Adam, A. Yuille, and L.-C. Chen,\n“Axial-deeplab: Stand-alone axial-attention for panoptic segmentation,”\narXiv:2003.07853, 2020.\n[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in IEEE/CVF CVPR, 2016, pp. 770–778.\n[28] A. L. Maas, A. Y . Hannun, and A. Y . Ng, “Rectiﬁer nonlinearities\nimprove neural network acoustic models,” in ICML, 2013, p. 3.\n[29] A. Molina, P. Schramowski, and K. Kersting, “Pad ´e activation units:\nEnd-to-end learning of ﬂexible activation functions in deep networks,”\nin ICLR, 2019.\n[30] X. Zhu, H. Hu, S. Lin, and J. Dai, “Deformable convnets v2: More\ndeformable, better results,” in IEEE/CVF CVPR, 2019, pp. 9308–9316.\n[31] H. Su, V . Jampani, D. Sun, O. Gallo, E. Learned-Miller, and J. Kautz,\n“Pixel-adaptive convolutional neural networks,” in IEEE/CVF CVPR,\n2019, pp. 11 166–11 175.\n[32] H. W. Kuhn, “The hungarian method for the assignment problem,” Naval\nResearch Logistics Quarterly, vol. 2, no. 1-2, pp. 83–97, 1955.\n[33] H. Rezatoﬁghi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese,\n“Generalized intersection over union: A metric and a loss for bounding\nbox regression,” in IEEE/CVF CVPR, 2019, pp. 658–666.\n[34] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar, “Focal loss for\ndense object detection,” in IEEE ICCV, 2017, pp. 2980–2988.\n[35] K. Chen, J. Wang, J. Pang et al., “MMDetection: Open mmlab detection\ntoolbox and benchmark,” arXiv:1906.07155, 2019.\n[36] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\nin ICLR, 2019."
}