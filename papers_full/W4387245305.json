{
  "title": "Dynamic Multi-Graph Convolution-Based Channel-Weighted Transformer Feature Fusion Network for Epileptic Seizure Prediction",
  "url": "https://openalex.org/W4387245305",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098698267",
      "name": "Yifan Wang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2161228152",
      "name": "Wei-gang Cui",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2097645590",
      "name": "Tao Yu",
      "affiliations": [
        "Capital Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2108833886",
      "name": "Xiao Li Li",
      "affiliations": [
        "Beijing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2019949022",
      "name": "Xiaofeng Liao",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2079581369",
      "name": "Yang Li",
      "affiliations": [
        "Beihang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2976017519",
    "https://openalex.org/W3192474593",
    "https://openalex.org/W2970007912",
    "https://openalex.org/W6633205339",
    "https://openalex.org/W3185809598",
    "https://openalex.org/W4360995440",
    "https://openalex.org/W3023283343",
    "https://openalex.org/W2871023201",
    "https://openalex.org/W4226034202",
    "https://openalex.org/W2913081068",
    "https://openalex.org/W2965277555",
    "https://openalex.org/W2091211352",
    "https://openalex.org/W2799610518",
    "https://openalex.org/W3165065619",
    "https://openalex.org/W2911226018",
    "https://openalex.org/W2946344027",
    "https://openalex.org/W4312092881",
    "https://openalex.org/W2805050064",
    "https://openalex.org/W4225685855",
    "https://openalex.org/W3006560451",
    "https://openalex.org/W3024961463",
    "https://openalex.org/W2592790850",
    "https://openalex.org/W2614978180",
    "https://openalex.org/W3190673580",
    "https://openalex.org/W3185980597",
    "https://openalex.org/W4214712775",
    "https://openalex.org/W4225491329",
    "https://openalex.org/W4313004006",
    "https://openalex.org/W3126795333",
    "https://openalex.org/W2963727766",
    "https://openalex.org/W2981428193",
    "https://openalex.org/W3186844663",
    "https://openalex.org/W2991139962",
    "https://openalex.org/W2962984603",
    "https://openalex.org/W2804824909",
    "https://openalex.org/W2780723646",
    "https://openalex.org/W2976267777",
    "https://openalex.org/W3181212342",
    "https://openalex.org/W3127389222",
    "https://openalex.org/W2899459625",
    "https://openalex.org/W4225622513",
    "https://openalex.org/W3189867217",
    "https://openalex.org/W1556131344"
  ],
  "abstract": "Electroencephalogram (EEG) based seizure prediction plays an important role in the closed-loop neuromodulation system. However, most existing seizure prediction methods based on graph convolution network only focused on constructing the static graph, ignoring multi-domain dynamic changes in deep graph structure. Moreover, the existing feature fusion strategies generally concatenated coarse-grained epileptic EEG features directly, leading to the suboptimal seizure prediction performance. To address these issues, we propose a novel multi-branch dynamic multi-graph convolution based channel-weighted transformer feature fusion network (MB-dMGC-CWTFFNet) for the patient-specific seizure prediction with the superior performance. Specifically, a multi-branch (MB) feature extractor is first applied to capture the temporal, spatial and spectral representations fromthe epileptic EEG jointly. Then, we design a point-wise dynamic multi-graph convolution network (dMGCN) to dynamically learn deep graph structures, which can effectively extract high-level features from the multi-domain graph. Finally, by integrating the local and global channel-weighted strategies with the multi-head self-attention mechanism, a channel-weighted transformer feature fusion network (CWTFFNet) is adopted to efficiently fuse the multi-domain graph features. The proposed MB-dMGC-CWTFFNet is evaluated on the public CHB-MIT EEG and a private intracranial sEEG datasets, and the experimental results demonstrate that our proposed method achieves outstanding prediction performance compared with the state-of-the-art methods, indicating an effective tool for patient-specific seizure warning. Our code will be available at: https://github.com/Rockingsnow/MB-dMGC-CWTFFNet.",
  "full_text": "1 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n \nYifan Wang, Weigang Cui, Tao Yu, Xiaoli Li, Xiaofeng Liao and Yang Li, Senior Member, IEEE \n \n Abstractâ€”Electroencephalogram (EEG) based seizure prediction \nplays an important role in the  closed-loop neuromodulation \nsystem. However, most existing seizure prediction methods based \non graph convolution network only focus ed on constructing the \nstatic graph , ignoring multi-domain dynamic changes in  deep \ngraph structure. Moreover, the existing feature fusion strategies \ngenerally concatenate d coarse-grained epileptic EEG features  \ndirectly, leading to the suboptimal seizure prediction performance. \nTo address these issues, we propose a novel multi-branch dynamic \nmulti-graph convolution  based channel-weighted transformer \nfeature fusion network ( MB-dMGC-CWTFFNet) for the patient-\nspecific seizure prediction with the superior performance. \nSpecifically, a multi-branch (MB) feature extractor is first applied \nto capture the temporal, spatial and spectral representation s \nfromthe epileptic EEG jointly. Then, we design a  point-wise \ndynamic multi -graph convolution net work (dMGCN) to \ndynamically learn deep graph structures , which can effectively \nextract high-level features from the multi-domain graph. Finally, \nby integrating the local and global channel -weighted strategies \nwith the multi-head self-attention mechanism, a channel-weighted \ntransformer feature fusion net work (CWTFFNet) is adopted to \nefficiently fuse the multi -domain graph features.  The proposed \nMB-dMGC-CWTFFNet is evaluated on the public CHB-MIT \nEEG and a private intracranial sEEG dataset s, and the \nexperimental results demonstrate that our proposed method \nachieves outstanding prediction performance compared with the \nstate-of-the-art methods, indicating an effective tool for patient -\nspecific seizure warning.  Our code will be available at:  \nhttps://github.com/Rockingsnow/MB-dMGC-CWTFFNet. \n \nIndex Terms â€”seizure prediction, multi -graph convolution \nnetwork, transformer, EEG, epilepsy \nI. INTRODUCTION \nPILEPSY is one of the most common brain diseases of \nnervous system, producing recurrent seizures  and \nthreatening the patients' life [1]. Recently, more than 50 million \npeople worldwide suffer from epilepsy , and there are \napproximately 30% of patients dete riorating into refractory \nepilepsy, despite both drug and surgical treatment  [2]. \n \nThis work was supported in part by Beijing Natural Sci ence Foundation \n[Z220017]; in part by the National Natural Science Foundation of China \n[62201023]; in part by China Postdoctoral Science Foundation [2023M730175]; \nand in part by the Beijing Municipal Education Commission - Natural Science \nFoundation [KZ202 110025036] and the Beijing United Imaging Research \nInstitute of Intelligent Imaging Foundation [CRIBJQY202103] . (Yifan Wang \nand Weigang Cui contributed equally to this work) (Corresponding author: \nYang Li) \nY. F. Wang is with the Depart ment of Automation Sciences and Electrical \nEngineering, Beihang University, Beijing, 100191, China (e-mail: \nby2003156@buaa.edu.cn).  \nW. G. Cui is with the School of Engineering Medicine, Beihang University, \nBeijing, 100191, China (e-mail: cwg1994@buaa.edu.cn). \nFortunately, the seizure prediction based on \nElectroencephalography (EEG) provides an additional solution \nfor these refractory epilepsy patients, who can give early \nwarning for advanced neuromodulation treatments [3], so as to \nsuppress seizures effectively. The previous studies divided the \nlong-term recorded epileptic EEG signals into four \nneurophysiological periods: inter-ictal, pre-ictal, ictal and post-\nictal periods [4, 5]. Therefore, the core problem for the epileptic \nseizure prediction is how to accurately distinguish the  pre-ictal \nperiod from inter -ictal period , promoting intelligent waring \nbefore seizure onset for patients and clinicians [6]. \n      For automatic EEG seizure prediction, the  primary \nchallenge is to extract discriminative EEG features of the \nepileptic activity. Due to the high temporal resolution of EEG , \nthe long short -term memory (LSTM)  [7, 8] was introduced to \nthe seizure prediction model s to capture the temporal \ninformation of the epileptic EEG . In addition, to exploit the \nspectral representation  in epileptic rhythms , the wavelet \ntransformation [9] and the short -time Fourier transform  [10] \nwere combined with the con volution neural network (CNN) , \nwhich can learn  quantitative time-frequency characteristics  to \nfacilitate the classification of inter -ictal and pre -ictal periods.  \nMoreover, Ahmet et al.  [4] proposed a 3D-CNN seizure \nprediction framework to evaluate the spatio-temporal evolution \ncorrelation from multi-channel EEG time  series. Zhang et al. \n[11] designed a spatial filter of common spatial pattern to \nextract distinguishing spatial features from epileptic EEG, \nwhich further fed into a shallow CNN to discriminate between \nthe pre-ictal and inter-ictal states. However, these methods just \nobtained the coarse- grained EEG features in single or multi \ndomains by a fixed mode, without taking full advantage of the  \npatient-specific temporal, spectral and spatial signatures  \nsimultaneously, which may lead to the loss of essential epileptic \nactivity information. Thus, a multi-branch feature extractor is \nneeded to capture the multi-level fine-grained representations \nfrom the epileptic EEG in multiple domains. \nAnother existing issue is that the CNN framework in seizure \nT. Yu is with the Beijing Institute of Functional Neurosurgery, Xuanwu  \nHospital, Capital Medical University, Beijing , 100053, China ( e-mail: \nyutaoly@sina.com). \nX. L. Li is with the State Key Laboratory of Cognitive Neuroscience and \nLearning, Beijing Normal University, Beijing , 100875, China \n(e-mail: xiaoli@bnu.edu.cn). \nX. F. Liao is with the College of Computer Science, Chongqing University, \nChongqing 400715, China (e-mail: xfliao@cqu.edu.cn). \nY. Li is with the Beijing Advanced Innovation Center for Big Data and Brain \nComputing, State Key Laboratory of Virtual Reality Technology and Systems, \nDepartment of Automation Sciences and Electric al Engineering,  Beihang \nUniversity, Beijing, 100191, China (e-mail: liyang@buaa.edu.cn). \n \nDynamic Multi-Graph Convolution based Channel-\nWeighted Transformer Feature Fusion Network for \nEpileptic Seizure Prediction \nE \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3321414\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nprediction task can only learn low-dimensional spatial \ncorrelations among EEG channels,  due to its regular \nconvolution operation  and the local receptive field [12]. It is \ndifficult to track the complex non -Euclidean structure in the \nepileptic  seizures [13]. To deal with this problem, the  graph \nconvolutional network (GCN) were investigated in recent \nstudies of the seizure prediction field [14, 15] . The common \nprocedure in GCN is to define the prior adjacency matrix for \nconstructing the graph structure among channels , which helps \nto convert the epileptic EEG signals to a graph representation  \nwith graph nodes and edges [16]. For example, Wang et al.[17] \nemployed phase locking value ( PLV) in EEG data to construct \nthe adjacency matrix of graph edges. The differential entropy \n(DE) was applied in the inference of the spatial coupling in \nnetwork topology to calculate the temporal correlations of EEG \nand yield the graph nodes  [18]. Unfortunately, these GCN \nmethods based on the information theory  depended on the \nhandcrafted features to generate EEG graphs , neglecting the \ndynamic changes in patient-specific graph construction. Lian et \nal.[14] developed a joint graph structure and representation \nlearning network (JGRN) to predict seizures, where the graph \nstructures can be jointly optimized with patient -specific \nconnection weights of temporal  channels. A similar study \nproposed a subject -independent seizure predictor by using \ngeometric deep learning , realizing the seizure prediction from \nLSTM EEG graph synthesis [15]. It is notable that most of these \nmodels ignored the spatial position relationship among  EEG \nchannels, and only focused on the single and shallow static EEG \ngraph construction without the spatial position guidance, which \ncannot fully represent the  dynamic changes of individualized \nchannel connectivity in multiple domains. Therefore, a novel \nGCN is highly required to jointly characterize high-level multi-\ndomain features, and map patient-specific dynamic EEG graph \nrepresentations. \nAdditionally, in order to integrate comprehensive feature \ninformation for the precise seizure prediction, some feature \nfusion strategies were designed to fuse the EEG features from \ndifferent scales and domains. For example, Li et al.  [19] \nadopted a temporal-spectral squeenze-and-excitation scheme to \nfuse the hierarchical multi -domain representations of epileptic \nEEG, which reduced the information redundancy of high -\ndimensional features.  Gao et al.  [20] combined the attention \nmechanism with dilated convolution to aggregate spatio-\ntemporal multi-scale features, providing a promising solution \nfor EEG -based seizure prediction.  Although these feature \nfusion methods obtained a comprehensive feature, they  only \nconsidered the general fusion of low-level features in Euclidean \nspace [21]. High-level EEG graph node features, embedded in \nnon-Euclidean graph structures, urgently need a specific fusion \napproach to enable robust seizure prediction.  \nThe main motivation of our study aims to break through \nlimitations of the existing prediction methods, including coarse-\ngrained EEG features in single domain , shallow static EEG \ngraph construction without spatial position guidance  and \ndifficulties in high-level graph feature fusion. Thus, we propose \na novel multi-branch dynamic multi-graph convolution based \nchannel-weighted transformer feature fusion network  (MB-\ndMGC-CWTFFNet), for the patient-specific seizure prediction. \nFirst, a multi-branch (MB) feature extractor is used to capture \nmulti-level fine-grained features from epileptic EEG in multiple \ndomain. Second,  in order to extract  multi-domain graph \nfeatures, a point-wise dynamic multi- graph convolution \nnetwork (dMGCN) is constructed to adaptively learn three-\nview dynamic graph structure with spatial position guidance . \nFinally, we investigate a channel-weighted transformer feature \nfusion net work (CWTFFNet) to efficiently fuse the multi-\ndomain graph features, which introduces the channel-weighted \nself-attention mechanism to map discriminative fused \nrepresentations for  the seizure prediction. The proposed MB-\ndMGC-CWTFFNet is evaluated on two kinds of epileptic \ndatasets, i.e., CHB -MIT EEG dataset and our Xuanwu \nintracranial stereo-electroencephalography (sEEG) dataset, and \nachieves the promising performance compared with the state-\nof-the-art methods, which validates its outstanding capability in \nseizure prediction task. \nIn general, the main contributions of our study are \nsummarized as follows: \n1) A novel MB-dMGC-CWTFFNet is proposed to predict \nseizures for the individual epilepsy patient, which can \nefficiently fuse multi- domain graph features , yielding the \nhighest prediction performance on both CHB-MIT and Xuanwu \ndatasets, respectively. \n2) We design a MB feature extractor, including three parallel \nsub-branches in temporal, spatial and frequency domains \nrespectively, to capture the multi -level fine -grained features \njointly, which offsets the inadequate representation of  coarse-\ngrained EEG features in traditional feature extractors. \n3) A dMGCN is constructed by point-wise dynamic graph \nneural network, which can learn dynamic changes of three-view \ngraph structures with spatial position guidance, and extract deep \nmulti-domain graph features, and thus  overcomes insufficient \nexpression of spatial connectivity in shallow static EEG graph. \n4) A CWTFFNet is developed by introducing both the local \nand the global channel -weighted self -attention into the \ntransformer network . The local graph edge weights are \ncomplementary to the global channel position information , \nwhich can implement efficient fusion of high -level graph \nfeatures against current feature fusion strategies. \nII. METHODOLOGY \nThe seizure prediction framework  of our proposed MB-\ndMGC-CWTFFNet is displayed in Fig. 1. The overall \narchitecture mainly consists of the multi-branch feature \nextractor, the point -wise dynamic multi -graph convolution \nnetwork and the channel-weighted transformer feature fusion \nnetwork, summarized as follows:  1) The MB feature extractor  \nis primarily designed to extract the multi-domain temporal-\nspatial-spectral features from EEG signals. 2) The dMGCN is \nfurther employed to transform the temporal-spatial-spectral \nfeatures into high-level graph representations from temporal, \nspatial and spectral views . 3) The CWTFFNet  is adopt ed to \nobtain the fused feature maps , and the fully connected layers  \nare utilized to generate the recognition results  ultimately. The \nwell-trained MB-dMGC-CWTFFNet is then transformed into a \npractical seizure warning system by a post-processing strategy. \nDetails of each step are given in following subsection. \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3321414\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nA. Multi-Branch Feature Extractor \nThe epileptic EEG signals are defined as  ð¸ð¸ =  {(ð‘¥ð‘¥ð‘–ð‘–, ð‘¦ð‘¦ð‘–ð‘–)| ð‘–ð‘– =\n1,2, . . . , ð‘ð‘}, where ð‘¥ð‘¥ð‘–ð‘– âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð‘†ð‘† represents the i-th EEG trial with \nC channels and S sampling points. N is the total number of EEG \ntrials. ð‘¦ð‘¦ð‘–ð‘–  is a binarized label of pre- ictal or inter -ictal state \ncorresponding to  ð‘¥ð‘¥ð‘–ð‘–.  \nConsidering the individualized differences of epileptic \nactivities in both time domain , frequency domain  and spatial \ndomain, we firstly construct the MB feature extractor to capture \nthe temporal-spatial-spectral representations from epileptic \nEEG signals. In Fig. 1, the MB feature extractor includes three \nsub-branches: the multi-scale temporal-conv branch, the multi-\nband spectral -conv branch and the multi -channel spatial-\nencoding branch. \n1) Multi-scale temporal-conv branch \nEpileptic seizure recordings involved the critical \nelectrophysiological fluctuation from inter -ictal period to pre-\nictal period  [22]. In order to capture the comprehensive  \ntemporal information of EEG with its higher time resolution in \ntime domain , a multi -scale temporal -conv branch is first \ndesigned with n parallel temporal convolution (TConv)  layers. \nThus, we can gain the multi-scale temporal features  with \ndifferent sizes from TConv -1 to TConv -n, denoted as ð¹ð¹ð‘˜ð‘˜ âˆˆ\nð‘…ð‘…ð¶ð¶Ã—ð‘‡ð‘‡ð‘˜ð‘˜ , k = 1 , 2, â‹¯, n , where ð‘‡ð‘‡ð‘˜ð‘˜ is the output scale of the \ntemporal feature from  the k-th TConv. Additionally, the batch \nnormalization and exponential linear unit (ELU) are also \napplied in the each TConv of the multi-scale temporal -conv \nbranch to accelerate the training and convergence of the \nproposed model. Accordingly, these multi-scale temporal \nfeatures are concatenated to generate the overall feature map in \ntime domain: ð¹ð¹\nð‘‡ð‘‡ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð·ð·ð‘‡ð‘‡, where ð·ð·ð‘‡ð‘‡ is the sum of ð‘‡ð‘‡ð‘˜ð‘˜, k = 1, 2, \nâ‹¯, n.  \n2) Multi-band spectral-conv branch \nPrevious studies have proven that the epileptic activities may \nbe of different frequencies for epilepsy patients  [23]. Thus, \naccording to the clinical five  frequency sub-bands: ð›¿ð›¿ band (0-\n4Hz), ðœƒðœƒ band (4-8Hz), ð›¼ð›¼ band (8- 13Hz), ð›½ð›½ band (13- 30Hz), ð›¾ð›¾ \nband (30- 50Hz) [24], the multi- band spectral-conv branch is \nadopted to contain the hierarchical wavelet convolutions  \n(WaveConv) based on Daubechies order-4 (Db4) wavelet [25]. \nThe wavelet decomposition can be accomplished on the EEG  \ntrials due to its high correlation coefficients with the epileptic \nsignal [26] to obtain the wavelet spectral features in the fiv e \nsub-bands. The hierarchical W aveConv layers perform \nsuccessive spectral analysis by means of L -level iteration, \nwhere ð¿ð¿ = âŒŠð‘™ð‘™ð‘™ð‘™ð‘™ð‘™\n2(ð‘“ð‘“ð‘ ð‘ )âŒ‹âˆ’3 , determined by the EEG sampling \nrate  ð‘“ð‘“ð‘ ð‘ , and âŒŠâ‹…âŒ‹ represents the rou nding-down operation  [27]. \nThen, the frequency boundaries of the l -th WaveConv are \n(0, ð‘“ð‘“ð‘ ð‘ /2(ð‘™ð‘™+1) ) and (ð‘“ð‘“ð‘ ð‘ /2(ð‘™ð‘™+1), ð‘“ð‘“ð‘ ð‘ /2ð‘™ð‘™), respectively, where l  = 1, \n2,â‹¯, L. After inputting the EEG  trial ð‘¥ð‘¥ð‘–ð‘– âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð‘†ð‘† into the multi-\nband spectral -conv branch,  we can obtain the multi-band \nwavelet spectral features: ð¹ð¹ð›¿ð›¿ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð»ð»1, ð¹ð¹ðœƒðœƒ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð»ð»2, ð¹ð¹ð›¼ð›¼ âˆˆ\nð‘…ð‘…ð¶ð¶Ã—ð»ð»3, ð¹ð¹ð›½ð›½ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð»ð»4, ð¹ð¹ð›¾ð›¾ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð»ð»5 corresponding to five standard \nphysiological frequency sub-bands, where ð»ð» = ð‘†ð‘†/2ð‘™ð‘™  is the \noutput dimension of the wavelet spectral features  generated \nfrom the l -th WaveConv. Additionally, due to the similar of \ntime-frequency analysis with the discrete wavelet transform, \nthe WaveConv operators have no learnable parameters in the \nprocessing of the feature extraction, which weights are fixed \nand given by Db4 wavelet filter.  Then, the five -band wavelet \nspectral features are concatenated into the integral spectral \nfeature map in frequency domain: ð¹ð¹ð‘…ð‘… âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð·ð·ð‘…ð‘…, where ð·ð·ð‘…ð‘… is the \nsum of ð»ð»1, ð»ð»2, . . . ð»ð»5. \n3) Multi-channel spatial-encoding branch \nApart from the multi-scale temporal-conv branch and multi-\nband spectral -conv branch, we also propose a multi-channel \nspatial-encoding branch to excavate the representations of \nchannel mapping in spatial domain . Specially,  the multi-\nchannel EEG trials are transposed to the channel -wise slices, \n \nFig. 1. The framework of the proposed MB-dMGC-CWTFFNet for the seizure prediction. \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3321414\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nwhich are imported into the channel position encoder and \nspatial feature encoder  to complete the channel correlation  \nconstruction and the spatial feature extraction, respectively .  \nFor the channel position encoder, a distance set ð‘ˆð‘ˆ is foremost \nestablished by ð‘ˆð‘ˆ =  {ð‘¢ð‘¢ð‘–ð‘–ð‘–ð‘–| ð‘–ð‘–, ð‘—ð‘— âˆˆ [1, ð¶ð¶], ð‘–ð‘– â‰   ð‘—ð‘—} , where ð‘¢ð‘¢ð‘–ð‘–ð‘–ð‘– \nrepresents the Euclidean distance of the i -th channel and the j -\nth channel, C is the total number of channels. we can get the ð‘¢ð‘¢ð‘–ð‘–ð‘–ð‘– \nfrom the international standard electrode system [28]. Then the \ninitialized channel adjacency matrix ð´ð´ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð¶ð¶ is generated by \nthe following position embedding method: \nð‘Žð‘Žð‘–ð‘–ð‘–ð‘– =\nâŽ©âŽªâŽ¨\nâŽªâŽ§\n1\nð‘¢ð‘¢ð‘–ð‘–ð‘–ð‘–\n                                              ð‘–ð‘–ð‘“ð‘“ ð‘¢ð‘¢ð‘–ð‘–ð‘–ð‘–  <  ð‘€ð‘€(ð‘ˆð‘ˆ)\n0                                                  ð‘–ð‘–ð‘“ð‘“ ð‘¢ð‘¢ð‘–ð‘–ð‘–ð‘–  â‰¥  ð‘€ð‘€(ð‘ˆð‘ˆ)\n1\nð‘€ð‘€({ð‘¢ð‘¢ð‘–ð‘–ð‘–ð‘–|ð‘¢ð‘¢ð‘–ð‘–ð‘–ð‘–<ð‘€ð‘€(ð‘ˆð‘ˆ)})                                    ð‘–ð‘–ð‘“ð‘“   ð‘–ð‘– = ð‘—ð‘—\n   (1) \nwhere M(â‹…) is the mean operation, ð‘Žð‘Žð‘–ð‘–ð‘–ð‘– is the element of the i -th \nrow and the j -th column of adjacency matrix ð´ð´ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð¶ð¶ . \nTherefore, the channel adjacency matrix A contains the global \nposition information of the multi-channel relationship, which \nwill be used to construct the dynamic graph in the following  \npoint-wise dMGCN. Additionally, we adopt the spatial feature \nencoder based on the channel-wise spatial convolution [29, 30] \nto extract the multi -channel spatial characteri stics, which are \nultimately concatenated and reshaped as ð¹ð¹ð‘†ð‘† âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð·ð·ð‘†ð‘†, where ð·ð·ð‘†ð‘† \nis the output dimension of integrated spatial feature map.  \nB. Point-Wise Dynamic Multi-Graph Convolution Network \nTo further learn deep dynamic connectivity of different brain \nregions for the individual epilepsy patient, in this subsection, a \nnovel point-wise dMGCN is proposed to extract multi-domain \ngraph features. Three synchronized dynamic graph convolution \nnetworks are involved by temporal, spatial and spectral views. \nThree views constitute the point-wise dMGCN in Fig. 2, which \ncan explore the deep channel relationship of the temporal \nfeature map ð¹ð¹ð‘‡ð‘‡, the spatial feature map ð¹ð¹ð‘†ð‘† and the wavelet \nspectral feature map ð¹ð¹ð‘…ð‘…  , respectively. For each graph \nconvolution view, the i nitialized adjacency matrix  ð´ð´ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð¶ð¶, \ndepicting original distance between any two channels, has been \ncalculated by the channel position encoder  of the MB feature \nextractor. To further guide the dynamic evolution process of the \nchannel relationship from three kinds of views, a self -gating \nstrategy is employed in i nitialized adjacency matrix A as \nfollows: \nð´ð´Ìƒð‘‡ð‘‡ = ðœŽðœŽ(ð‘Šð‘Š11ð›¿ð›¿(ð‘Šð‘Š12(ð´ð´Ìƒ1)))                        (2) \nð´ð´Ìƒð‘†ð‘† = ðœŽðœŽ(ð‘Šð‘Š21ð›¿ð›¿(ð‘Šð‘Š22(ð´ð´Ìƒ2)))                        (3) \nð´ð´Ìƒð‘…ð‘… = ðœŽðœŽ(ð‘Šð‘Š31ð›¿ð›¿(ð‘Šð‘Š32(ð´ð´Ìƒ3)))                        (4) \nwhere ð´ð´Ìƒ1, ð´ð´Ìƒ2 , ð´ð´Ìƒ3 âˆˆ ð‘…ð‘…(ð¶ð¶Ã—ð¶ð¶)Ã—1 are reshaped from  ð´ð´ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð¶ð¶ , \nð‘Šð‘Š12, ð‘Šð‘Š22, ð‘Šð‘Š32 âˆˆ ð‘…ð‘…((ð¶ð¶Ã—ð¶ð¶)/ð‘Ÿð‘Ÿ)Ã—(ð¶ð¶Ã—ð¶ð¶)  and ð‘Šð‘Š11,  ð‘Šð‘Š21,  ð‘Šð‘Š31 âˆˆ\nð‘…ð‘…(ð¶ð¶Ã—ð¶ð¶)Ã—((ð¶ð¶Ã—ð¶ð¶)/ð‘Ÿð‘Ÿ) are weight matrixes of fully-connected layers, \nð‘Ÿð‘Ÿ is the reduction ratio, ð›¿ð›¿(â‹…) and ðœŽðœŽ(â‹…) are the ELU activation \nfunction and the rectified linear unit (ReLU) . Hence the three \ndynamic adjacency matrix es ð´ð´ð‘‡ð‘‡,  ð´ð´ð‘†ð‘†, ð´ð´ð‘…ð‘… âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð¶ð¶ \ncorresponding to temporal , spatial and spectral graph \nconvolution net s are acquired  by reshaping ð´ð´Ìƒð‘‡ð‘‡, ð´ð´Ìƒð‘…ð‘…, ð´ð´Ìƒð‘†ð‘† âˆˆ\nð‘…ð‘…(ð¶ð¶Ã—ð¶ð¶)Ã—1 into ð‘…ð‘…(ð¶ð¶Ã—ð¶ð¶). \nAfter constructing the dynamic connectivit y of epileptic \nactivities from three views, the operations of the dynamic \ngraph convolution are performed on the temporal feature map \nð¹ð¹ð‘‡ð‘‡ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð·ð·ð‘‡ð‘‡ , the spatial feature map ð¹ð¹ð‘†ð‘† âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð·ð·ð‘†ð‘†  and the \nwavelet spectral feature map ð¹ð¹ð‘…ð‘… âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð·ð·ð‘…ð‘…, respectively, which \nare formulated by: \n                     ðºðºð‘‡ð‘‡ = ð›¿ð›¿(ð·ð·ð‘‡ð‘‡\nâˆ’1ð´ð´ð‘‡ð‘‡ð›¿ð›¿(ð¹ð¹ð‘‡ð‘‡ð›©ð›©11)ð›©ð›©12 + ð¹ð¹ð‘‡ð‘‡)                   (5) \n                     ðºðºð‘†ð‘† = ð›¿ð›¿(ð·ð·ð‘†ð‘†\nâˆ’1ð´ð´ð‘†ð‘†ð›¿ð›¿(ð¹ð¹ð‘†ð‘†ð›©ð›©21)ð›©ð›©22 + ð¹ð¹ð‘†ð‘†)                     (6) \n                     ðºðºð‘…ð‘… = ð›¿ð›¿(ð·ð·ð‘…ð‘…\nâˆ’1ð´ð´ð‘…ð‘…ð›¿ð›¿(ð¹ð¹ð‘…ð‘…ð›©ð›©31)ð›©ð›©32 + ð¹ð¹ð‘…ð‘…)                   (7) \nwhere ðºðºð‘‡ð‘‡ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð·ð·ð‘‡ð‘‡,  ðºðºð‘†ð‘† âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð·ð·ð‘†ð‘†, ðºðºð‘…ð‘… âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð·ð·ð‘…ð‘…  are the \ndynamic graph features corresponding to ð¹ð¹ð‘†ð‘†,  ð¹ð¹ð‘‡ð‘‡,  ð¹ð¹ð‘…ð‘… with the \nhidden non-Euclidean topology in epileptic activities ,  ð·ð·ð‘‡ð‘‡\nð‘–ð‘–ð‘–ð‘– =\nâˆ‘ ð´ð´ð‘‡ð‘‡\nð‘–ð‘–ð‘–ð‘–\nð‘–ð‘– , ð·ð·ð‘†ð‘†\nð‘–ð‘–ð‘–ð‘– = âˆ‘ ð´ð´ð‘†ð‘†\nð‘–ð‘–ð‘–ð‘–\nð‘–ð‘– , ð·ð·ð‘…ð‘…\nð‘–ð‘–ð‘–ð‘– = âˆ‘ ð´ð´ð‘…ð‘…\nð‘–ð‘–ð‘–ð‘–\nð‘–ð‘–  are the degree matrix es \ncorresponding to  ð´ð´ð‘‡ð‘‡,  ð´ð´ð‘†ð‘†, ð´ð´ð‘…ð‘…  respective, ð›©ð›©11, ð›©ð›©12 âˆˆ\nð‘…ð‘…ð·ð·ð‘‡ð‘‡Ã—ð·ð·ð‘‡ð‘‡,  ð›©ð›©21, ð›©ð›©22 âˆˆ ð‘…ð‘…ð·ð·ð‘†ð‘†Ã—ð·ð·ð‘†ð‘†, ð›©ð›©32, ð›©ð›©32 âˆˆ ð‘…ð‘…ð·ð·ð‘…ð‘…Ã—ð·ð·ð‘…ð‘… represent the \nweight matrixes of convolution kernels  in the point -wise \nconvolution unit [31]. Therefore, we obtain the dynamic multi-\ndomain graph features ðºðºð‘‡ð‘‡,  ðºðºð‘†ð‘†, ðºðºð‘…ð‘… with their corresponding \ndynamic adjacency matrix ð´ð´ð‘‡ð‘‡,  ð´ð´ð‘†ð‘†, ð´ð´ð‘…ð‘…, which will be fed into \nthe CWTFFNet to conduct the final feature fusion in the next \nsubsection. \nC. Channel-Weighted Transformer Feature Fusion Network \nTo further fuse the high-level graph features ðºðºð‘‡ð‘‡,  ðºðºð‘†ð‘†, ðºðºð‘…ð‘…, the \nCWTFFNet is proposed by combining the dynamic adjacency \nmatrix ð´ð´ð‘‡ð‘‡,  ð´ð´ð‘†ð‘†, ð´ð´ð‘…ð‘… with multi -head self -attention mechanism. \nIn Fig. 3, the CWTFFNet can be divided into a  local channel-\nweighted multi-head self-attention (Local CW -MHSA), a \nglobal channel-weighted feature fusion block (Global CW-FFB) \nand the multi-layer perception (MLP).  \nThe Local CW -MHSA consists of three heads : the ð´ð´ð‘‡ð‘‡ -\nweighted self-attention unit (SAU), the  ð´ð´ð‘†ð‘†-weighted SAU and \nthe ð´ð´ð‘…ð‘…-weighted SAU. For each self-attention head, three kinds \nof weight matrixes, denoted as ð‘Šð‘Šð‘„ð‘„ âˆˆ ð‘…ð‘…ð·ð·Ã—ð‘‘ð‘‘ð¾ð¾, ð‘Šð‘Šð¾ð¾ âˆˆ\nð‘…ð‘…ð·ð·Ã—ð‘‘ð‘‘ð¾ð¾, ð‘Šð‘Šð‘‰ð‘‰ âˆˆ ð‘…ð‘…ð·ð·Ã—ð‘‘ð‘‘ð‘‰ð‘‰,  are i nitially introduced to encode the \ninput graph features , where   ð‘‘ð‘‘ð¾ð¾  and ð‘‘ð‘‘ð‘‰ð‘‰  both represent the \nhyperparameters. So, the query ð‘„ð‘„, the key K and the value V are \ncalculated via: \nð‘„ð‘„ = ðºðºð‘Šð‘Šð‘„ð‘„                                      (8) \n    ð¾ð¾ = ðºðºð‘Šð‘Šð¾ð¾                                      (9) \n                                          ð‘‰ð‘‰ = ðºðºð‘Šð‘Šð‘‰ð‘‰                                    (10) \nwhere the ðºðº âˆˆ {ðºðºð‘‡ð‘‡,  ðºðºð‘†ð‘†, ðºðºð‘…ð‘…}  indicates three kinds of graph \nfeatures. We introduce the local channel-weighted strategy by \napplying the dynamic adjacency matrix es in multi-head self-\n \nFig. 2. The architecture of the point -wised dynamic multi-graph \nconvolution network (dMGCN). \n \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3321414\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nattention mechanism.  Then, the local features  ð‘ð‘ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð‘‘ð‘‘ð¾ð¾ \nfrom Local CW-MHSA is obtained by: \nð‘ð‘ð‘‡ð‘‡ = ð´ð´ð‘‡ð‘‡ð‘ ð‘ ð‘™ð‘™ð‘“ð‘“ð‘ ð‘ ð‘ ð‘ ð‘Žð‘Žð‘¥ð‘¥(ð‘„ð‘„1ð¾ð¾1\nð‘‡ð‘‡\nï¿½ð‘‘ð‘‘ð¾ð¾\n)ð‘‰ð‘‰1 (11) \nð‘ð‘ð‘†ð‘† = ð´ð´ð‘†ð‘†ð‘ ð‘ ð‘™ð‘™ð‘“ð‘“ð‘ ð‘ ð‘ ð‘ ð‘Žð‘Žð‘¥ð‘¥(ð‘„ð‘„2ð¾ð¾2\nð‘‡ð‘‡\nï¿½ð‘‘ð‘‘ð¾ð¾\n)ð‘‰ð‘‰2 (12) \nð‘ð‘ð‘…ð‘… = ð´ð´ð‘…ð‘…ð‘ ð‘ ð‘™ð‘™ð‘“ð‘“ð‘ ð‘ ð‘ ð‘ ð‘Žð‘Žð‘¥ð‘¥(ð‘„ð‘„3ð¾ð¾3\nð‘‡ð‘‡\nï¿½ð‘‘ð‘‘ð¾ð¾\n)ð‘‰ð‘‰3 (13) \nð‘ð‘ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™  =  ð¶ð¶ð‘™ð‘™ð¶ð¶ð¶ð¶ð‘Žð‘Žð‘ ð‘ (ð‘ð‘ð‘‡ð‘‡, ð‘ð‘ð‘†ð‘†, ð‘ð‘ð‘…ð‘…) (14) \nwhere ð‘ð‘ð‘‡ð‘‡,  ð‘ð‘ð‘†ð‘†, ð‘ð‘ð‘…ð‘… are the outputs from the three self -attention \nheads respectively , and ð¶ð¶ð‘™ð‘™ð¶ð¶ð¶ð¶ð‘Žð‘Žð‘ ð‘ (â‹…) is the concatenation  \nfunction. To further capture the global information, we employ \nthe Global CW-FFB and MLP in local features ð‘ð‘ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ according \nto the following equations: \n      ð‘ð‘ð‘”ð‘”ð‘™ð‘™ð‘™ð‘™ð‘”ð‘”ð‘™ð‘™ð‘™ð‘™ = ð‘ ð‘ ð‘™ð‘™ð‘“ð‘“ð‘ ð‘ ð‘ ð‘ ð‘Žð‘Žð‘¥ð‘¥(ð´ð´)ð‘ð‘ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘ ð‘ ð‘™ð‘™ð‘“ð‘“ð‘ ð‘ ð‘ ð‘ ð‘Žð‘Žð‘¥ð‘¥(ðºðºð´ð´ðºðº(ð‘ð‘ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™)) (15) \n     ð‘ð‘ð‘“ð‘“ð‘¢ð‘¢ð‘ ð‘ ð‘“ð‘“ð‘‘ð‘‘ = â„±ð‘“ð‘“ð‘™ð‘™\n2 (ð‘…ð‘…ð‘…ð‘…ð¿ð¿ð‘ˆð‘ˆ(â„±ð‘“ð‘“ð‘™ð‘™\n1 (ð¿ð¿ð‘ð‘(ð‘ð‘ð‘”ð‘”ð‘™ð‘™ð‘™ð‘™ð‘”ð‘”ð‘™ð‘™ð‘™ð‘™)))) + ð¹ð¹ð‘€ð‘€(ð‘ð‘ð‘”ð‘”ð‘™ð‘™ð‘™ð‘™ð‘”ð‘”ð‘™ð‘™ð‘™ð‘™) (16) \nwhere ð‘ð‘ð‘”ð‘”ð‘™ð‘™ð‘™ð‘™ð‘”ð‘”ð‘™ð‘™ð‘™ð‘™ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð‘‘ð‘‘ð¾ð¾ is the global features,  ð´ð´ âˆˆ ð‘…ð‘…ð¶ð¶Ã—ð¶ð¶ is the \ninitialized adjacency matrix from the channel position encoder, \nðºðºð´ð´ðºðº(â‹…) represents the global average pooling, ð¿ð¿ð‘ð‘(â‹…) means the \nlayer normalization, ð‘…ð‘…ð‘…ð‘…ð¿ð¿ð‘ˆð‘ˆ(â‹…) is the rectified linear unit,  â„±ð‘“ð‘“ð‘™ð‘™\n1  \nand â„±ð‘“ð‘“ð‘™ð‘™\n2  denote the FC layers, and ð¹ð¹ð‘€ð‘€(â‹…)  is a feedf orward \nmodule (including two feedforward layers and an ELU \nactivation). Therefore, we acquire the fused features  ð‘ð‘ð‘“ð‘“ð‘¢ð‘¢ð‘ ð‘ ð‘“ð‘“ð‘‘ð‘‘ âˆˆ\nð‘…ð‘…ð¶ð¶Ã—ð‘‘ð‘‘ð¾ð¾ through the constructed CWTFFNet. \n   Finally, two fully-connected layers are used to conduct the \ndecoding for the fused features. They are  flattened into a 1 -\ndimensional tensor to feed into the fully connected layers, then \nthe classification probabilities of the pre-ictal and the inter-ictal \nstates are estimated by the Softmax function , and the index \ncorresponding to the ma ximum of probabilities represents the \nfinal result of seizure prediction. Moreover, a cross-entropy loss \nfunction is employed for the patient -specific model training of \nproposed MB-dMGC-CWTFFNet, the cross -entropy loss â„’ð¶ð¶ð¶ð¶ \nbetween the prediction result and the label is minimized by: \nâ„’ð¶ð¶ð¶ð¶ = ï¿½ï¿½âˆ’ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™(ð‘ð‘ð‘–ð‘–)ðœ‘ðœ‘(ð‘¦ð‘¦ð‘–ð‘– = ð‘™ð‘™ð‘–ð‘–)\nð‘€ð‘€\nð‘–ð‘–=1\nð‘ð‘\nð‘–ð‘–=1\n+ ðœ†ðœ†â€–ðœƒðœƒâ€– (17) \nwhere ð‘ð‘ð‘–ð‘– is the conditional probability of the i -th EEG trial \noutputted by the proposed MB -dMGC-CWTFFNet, ð‘™ð‘™ð‘–ð‘– is the \nclass from the label set,  ðœ‘ðœ‘(â‹…) represents the indicator function, \nN is the total number of samples, M = 2 is the number of classes. \nðœ†ðœ†â€–ðœƒðœƒâ€– belongs to the trade -off regularization term of Eq. (17), \nand aims to alleviate the overfitting problem during the model \ntraining, where ðœ†ðœ† is the regularization parameter  and the ðœƒðœƒ \ndenotes the updatable parameters of the model.  As a result, a \npersonalized well-trained model of the proposed MB -dMGC-\nCWTFFNet is generated, and will be perform ed the individual \nseizure prediction by the following post-processing [32]. \nD. Post-Processing Strategy \nEventually, the well-trained MB-dMGC-CWTFFNet is then \ntransformed into a practical seizure warning system by a post -\nprocessing strategy [33]. Specifically, after inputting the \nconsecutive EEG signals into  the well-trained MB-dMGC-\nCWTFFNet, the probability series P(i) belonging to pre -ictal \nclass from i-th epoch is generated. T hen we employ a moving \naverage filter on P (i) to reduce the oscillation and obtain the \nsmoothed probability series ðºðºð‘ ð‘ (ð‘–ð‘–) over time [32]. The lengths of \nthe moving average filter are configured to 15s and 25s for \nCHB-MIT and our Xuanwu dataset respectively, which will be \ndiscussed by the experimental results in Section IV. B. \nIII. EXPERIMENTAL RESULTS \nA. Dataset Description \nThe performance of the proposed MB-dMGC-CWTFFNet is \nevaluated on two epileptic datasets, wich is given as follows: \n1) CHB-MIT scalp EEG dataset [34]: The CHB-MIT dataset \ncontains the scalp EEG signals  from 23 patients, which were \nrecorded with 18 common electrodes and sampled at 256Hz in \nthe Childrenâ€™s Hospital Boston. In this study, there were at least \ntwo seizures and thr ee-hour inter -ictal recordings from each \npatients, who were selected for the patient -specific model \nevaluation of seizure prediction  [4]. In addition, the neur al \nrecordings within two hours after a seizure are removed to \nexclude the effect of post-ictal period [32]. Specially, if several \nseizures cluster within  two hours , only the first seizure \nprediction is considered as an effective evaluation, because a \nsuccessful warning depends on whether the model can predict \nthe leading seizure [35]. \n2) Xuanwu intracranial sEEG dataset: The Xuanwu dataset \nis collected by the Xuanwu Hospital of Capital Medical \nUniversity, Beijing, China, which consists of sEEG recordings \non the intracranial depth electrode for 5 focal epilepsy patients, \n \nFig. 3. The outline of channel -weighted transformer feature fusion \nnetwork (CWTFFNet). \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3321414\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nwhich sampled at 256Hz with 15 channels. From Table I, there \nare total 16 seizures, and the  recording duration of sEEG from \nthese patients is 42 hours. The labels of the inter-ictal, pre-ictal \nand ictal states were marked by the professional clinicians. This \nstudy was approved by the Ethics Committee of Xuanwu \nHospital, Capital Medical University (LYS2018041) in Beijing \nand complied with the ethical standards of the Declaration of \nHelsinki. Informed consent was obtained from all patients. \n \nB. Experimental Settings and Evaluation Metrics \nIn this study, based on the recent research [4, 11] , the EEG \nsignals from CHB-MIT and Xuanwu datasets are both cropped \ninto 5-second clips  before fed into the proposed MB -dMGC-\nCWTFFNet. Additionally, the pre -ictal period was popularly \ndefined by 15 minutes before seizure onset in the latest methods \n[4, 32] . Thus, we adopt the identical setting to the pre -ictal \nperiod, and the inter-ictal period is defined at least 2 hours away \nprior to seizure onset and after seizure ending [10]. \n    To conduct a comprehensive performance evaluation of the \nproposed MB-dMGC-CWTFFNet, the patient -specific leave-\none-out cross -validation (LOOCV)  [20] is employed in this \nstudy. Since the inter -ictal period is much larger than the pre-\nictal periodin the model traini ng stage, the inter-ictal clips are \nrandomly down-sampled to the same number of the pre- ictal \nclips [8]. Then, assuming that there are total ð‘ð‘ð‘–ð‘– seizures for the \ni-th patient, in each leave -one-out loop, ð‘ð‘ð‘–ð‘– âˆ’1 seizures are \nutilized for training while the left one is for testing , during the \ntraining stage, the cross-validation is use to divide training set \nand validation set.  It is repeated with ð‘ð‘ð‘–ð‘–  loops until the \nproposed model completes the prediction evaluation of  all \nseizures for the i-th patient. Since the number of seizures and \nthe recording duration are both different for each patient in two \ndatasets, for each leave -one-out loop,  the number of  training \ndata samples varies from 3028 to 18241, the validation data size \nvaries from 572 to 3543, the testing data size varies from 3131 \nto 7508, and the total data size varies from 6731 to 29292. The \nproposed MB-dMGC-CWTFFNet is evaluated via four metrics, \nincluding area under curve (AUC), sensitivity ( ð‘†ð‘†ð‘›ð‘› ), false \nprediction rate (FPR/h) and the p-value. AUC mainly  reflects \nthe classification performance for the inter -ictal and the pre -\nictal states. ð‘†ð‘†ð‘›ð‘›  denotes the ratio of successfully  predicted \nseizures to the total number of seizures . FPR/h indicates the \nnumber of false alarms per hour, and the p-value represents the \nsignificance of an improvement over chance -level, which is \nused to evaluate statistical ly significance whether the seizure \nwarning system is better than a random predictor [4]. \nC. Overall Performance \nIn order to illustrate the patient-specific prediction efficiency \nof the proposed MB -dMGC-CWTFFNet, we compare our \nproposed model with the following state -of-the-art methods in \nthe same chance-level, which are tested on two datasets. \n1) DCNN-Bi-LSTM [8]: This is a typical deep learning \nmethod by combin ing the deep convolutional network \nwith a bidirectional long short -term memory, extracting \nthe spatial and temporal features of epileptic EEG signals \nrespectively, which were used for the seizure prediction.  \n2) CE-stSENet [19]: This method used a temporal-spectral \nsqueenze-and-excitation network to capture hierarchical \nmulti-domain representations, which introduced the \nattention mechanism into the epileptic seizure detection \ntask and improved the recognition performance. \n3) TS-MS-DCNN [20]: This advanced model encoded the \nmulti-scale EEG features by designing  temporal and \nspatial multi-scale stages, and a dilated convolution block \nwas constructed to further e xpand the feature receptive \nand achieving the EEG-based seizure prediction. \nThe experimental results of the patient-specific comparison \non public CHB-MIT and our Xuanwu datasets are listed in \nTable II and Table III  respectively. From T able II, we can  \nobserve that  the DCNN-Bi-LSTM, CE-stSENet and TS-MS-\nDCNN gain the average AUC of 0.865, 0.857 and 0.890  \nrespectively on CHB-MIT dataset, while our proposed MB-\ndMGC-CWTFFNet reaches the highest average AUC of 0.935. \nEspecially for Patient 1, 8, 13 and 23, which the AUC are all \ngreater than 0.9 85, indicating an excellent performance of our \nmethod in distinguishing between the inter -ictal state and the \npre-ictal state. In the seizure prediction scenario , the average \nsensitivity of our proposed model achieves an ideal  97.8% as \nwell, which outperforms three baseline methods with 7.1%, \n11.8% and 6.3% respectively. The distinct advancement on ð‘†ð‘†ð‘›ð‘› \ndemonstrates that our proposed model , which is transformed \ninto the seizure warning system , performed a more successful \nseizure warning for an individual patient. In addition, our \nproposed model yields the lowest  average FPR/h of 0.0 59, \nwhich is at least 5 8.7% improvement against other  methods. \nMeanwhile, the p -value of our seizure warning system is less \nthan or equal to 0.001 for all patients, implying  the \nimprovement-over-chance of our seizure predictor is \nstatistically significant with  99.9% confidence interval. It \nindicates that our proposed MB-dMGC-CWTFFNet has the \nsignificantly patient-specific capability for the epileptic seizure \nprediction. \nAdditionally, to further validate the effectiveness of our \nproposed method, Table III lists the prediction results for five \nfocal epilepsy patients on Xuanwu dataset. It is obvious that our \nproposed model achieves more excellent performance on AUC \nand ð‘†ð‘†ð‘›ð‘› with average 0.984 and 100% respectively, which are at \nleast 5.1% and 10.0 % higher than that of  the state-of-the-art \nmodels. The average FPR/h of our method is 0.0 79, which is \nlower than other methods. These encouraging experimental \nresults demonstrate the remarkable performance ( p<0.05) of \nour MB-dMGC-CWTFFNet framework in the subject -\nindependent intracranial seizure prediction task, which makes it \npossible to predict seizure by implanting intracranial  deep \nelectrode, and it enables more convenient treatment for \nrefractory epilepsy patients [36].  \nTABLE I \nPATIENTSâ€™ DETAILS OF THE XUANWU INTRACRANIAL SEEG DATASET \nPatient ID Age \n(years) Gender No. of seizures Recording \nduration/h \n1 16 M 4 11.0 \n2 17 F 3 7.0 \n3 18 F 2 7.0 \n4 31 F 4 11.0 \n5 26 M 3 6.0 \nTotal - - 16 42.0 \n \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3321414\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nIV. DISCUSSIONS \nA. Ablation Studies \nTo prove the innovation of each component of our proposed \nMB-dMGC-CWTFFNet, the ablation studies are conducted on \nboth CHB -MIT and Xuanwu datasets. In this subsection, we \ndiscuss the efficacy  of each  innovation by comparing the \nproposed method with and without this component , which \ncontributes to justifying the  positive influence. The overall \nexperimental result of ablation studies is presented in Table IV, \nand the impact s of MB feature extractor , point-wise dMGCN \nand CWTFFNet are demonstrated respectively as follows: \n1) Impact of MB feature extractor \nIn order to give a comprehensive assessment for the proposed \nMB feature extractor, we compare our MB-dMGC-CWTFFNet \nwith three simplified sub -models: a) the model without \ntemporal-conv; b) the model without spatial-encoding; c) the \nmodel without spectral-conv. From Table IV and Fig. 4, firstly, \nwhen using the temporal -conv to extract the multi -scale \ntemporal features on two datasets, the AUC of our MB-dMGC-\nCWTFFNet increased by 2. 6% and 3.1% respectively \ncompared with the model without the temporal-conv. The ð‘†ð‘†ð‘›ð‘› \nalso get the improvement s of 3.0% on CHB -MIT dataset, \n15.0% on our Xuanwu dataset. Additionally, the FPR/h declines \nby 79.9% and 45. 9% on two datasets respectively, which \nillustrates the availability of the temporal -conv in capturing \nmulti-scale temporal evolution, and indicates the effectiveness \nof MB feature extractor in extracting fine- grained temporal \nfeatures. \nMeanwhile, the spatial-encoding also plays an important role \nin the feature extraction of multi -channel spatial features. For \ninstance, compared to the model without the spatial-encoding, \nthe AUC of the proposed model increase from 0.907 and 0.948 \nto 0.935 and 0.984 on two datasets respectively, and the ð‘†ð‘†ð‘›ð‘› are \nimproved from 94. 8% and 85 % into 97. 8% and 100%. The \nFPR/h decrease from 0. 337 and 0. 243 to 0.0 59 and 0.079 . It \nverifies that the spatial -encoding branch enables exact spatial \nexpression with cortical multi -channel representations, which \ncontributes to the seizure prediction with distinct improvements \nof performance metrics. \n   In addition to the above two branches proposed in MB feature \nextractor, the effect of the spectral-conv is further discussed. \nFrom Table IV , we can find that the proposed method with \nspectral-conv shows a better performance. For the CHB -MIT \ndataset, its evaluation metrics of AUC  and ð‘†ð‘†ð‘›ð‘› are 4.8% and \n4.4% higher than that of the model without spectral -conv, and \nthe FPR/h declines by 8 8.5%. Similarly, when using the \nspectral-conv on the Xuanwu dataset,  our MB-dMGC-\nCWTFFNet achieves the improvement of 2.3% AUC and 5.0% \nð‘†ð‘†ð‘›ð‘› over the model without spectral-conv, whose FPR/h are \nreduced by 37.8% accordingly. These evaluation results also \nprove that the designed spectral -conv can extract \ncomprehensive spectrum characteristics in five clinical \nphysiological rhythms, which facilitates the construction of the \npatient-specific MB feature extractor  by combining with \ntemporal-conv and spatial-encoding branches. \nMoreover, to further validate the superiority of the proposed \nMB feature extractor  intuitively, the t-SNE is applied to \nTABLE II \nTHE PATIENT-SPECIFIC OVERALL COMPARISON OF PERFORMANCE ON CHB-MIT DATASET  \nPatient \nID \nDCNN-Bi-LSTM [8] CE-stSENet [19] TS-MS-DCNN [20] Our MB-dMGC-CWTFFNet \nAUC ð‘†ð‘†ð‘›ð‘›(%) FPR/h p AUC ð‘†ð‘†ð‘›ð‘›(%) FPR/h p AUC ð‘†ð‘†ð‘›ð‘›(%) FPR/h p AUC ð‘†ð‘†ð‘›ð‘›(%) FPR/h p \n1 0.989 100.0 0.000 0.070 0.999 100.0 0.000 <0.001 0.999  100.0 0.000 <0.001 0.999  100.0 0.000  <0.001 \n2 0.693 100.0 0.000 0.004 0.799 66.7 0.000 <0.001 0.861 100.0 0.000 <0.001 0.834  100.0 0.452  <0.001 \n3 0.838 100.0 0.000 <0.001 0.892 83.3 0.070 <0.001 0.733  66.7  0.163  <0.001 0.913  100.0 0.021  <0.001 \n5 0.845 100.0 0.000 <0.001 0.824 100.0 0.052 <0.001 0.923  100.0  0.000  <0.001 0.926  100.0 0.000  <0.001 \n6 0.824 85.7 5.616 <0.001 0.832 100.0 0.274 <0.001 0.924  100.0  0.000  <0.001 0.931  100.0 0.000  <0.001 \n7 0.712 33.3 0.000 0.004 0.743 66.7 0.000 0.006 0.623  66.7  0.367  <0.001 0.827  100.0 0.181  0.001 \n8 0.930 80.00 0.000 0.001 0.927 100.0 0.000 0.003 0.976  100.0  0.000  <0.001 0.986  100.0 0.000  <0.001 \n9 0.919 100.0 0.000 0.002 0.712 50.0 0.000 <0.001 0.964  100.0  0.000  <0.001 0.933  75.0 0.025  <0.001 \n10 0.929 100.0 0.000 <0.001 0.961 83.3 0.000 <0.001 0.848  100.0  0.000  <0.001 0.984  100.0 0.000  <0.001 \n11 0.938 100.0 0.060 <0.001 0.897 66.7 0.000 <0.001 0.964  100.0  0.061  <0.001 0.921  100.0 0.037  <0.001 \n13 0.847 80.00 1.084 <0.001 0.943 100.0 0.000 <0.001 0.975  80.0  0.000  <0.001 0.986  100.0 0.000  <0.001 \n14 0.707 83.3 0.105 <0.001 0.708 66.7 0.210 <0.001 0.761  100.0  1.232  <0.001 0.854  83.3 0.234  <0.001 \n16 0.926 100.0 0.000 <0.001 0.916 100.0 0.000 <0.001 0.980  100.0  0.000  <0.001 0.952  100.0 0.000  <0.001 \n17 0.892 100.0 0.059 0.001 0.882 100.0 0.059 0.001 0.832  66.7  0.847  0.002 0.947  100.0 0.000  <0.001 \n18 0.833 100.0 0.000 <0.001 0.817 83.3 0.036 <0.001 0.846  83.3  0.053  <0.001 0.959  100.0 0.000  <0.001 \n20 0.994 100.0 0.000 <0.001 0.982 100.0 0.000 <0.001 0.996  100.0  0.000  <0.001 0.982  100.0 0.000  <0.001 \n21 0.765 75.0 3.438 <0.001 0.778 100.0 2.181 <0.001 0.875  75.0  0.000  <0.001 0.863  100.0 0.175  <0.001 \n22 0.916 100.0 0.884 0.015 0.679 66.7 4.133 0.016 0.834  100.0  0.000  <0.001 0.980  100.0 0.000  <0.001 \n23 0.943 85.7 0.000 <0.001 0.998 100.0 0.000 <0.001 0.999  100.0  0.000  <0.001 0.991  100.0 0.000  <0.001 \nAver 0.865 90.7 0.592 - 0.857 86.0 0.369 - 0.890 91.5 0.143 - 0.935  97.8 0.059  - \nwhere the bold values indicate the best average results. \nTABLE III \nTHE PATIENT-SPECIFIC OVERALL COMPARISON OF PERFORMANCE ON XUANWU DATASET \nPatient \nID \nDCNN-Bi-LSTM [8] CE-stSENet [19] TS-MS-DCNN [20] Our MB-dMGC-CWTFFNet \nAUC ð‘†ð‘†ð‘›ð‘›(%) FPR/h p AUC ð‘†ð‘†ð‘›ð‘›(%) FPR/h p AUC ð‘†ð‘†ð‘›ð‘›(%) FPR/h p AUC ð‘†ð‘†ð‘›ð‘›(%) FPR/h p \n1 0.664 50.00 0.993 0.024 0.819 75.0 1.780 0.003 0.878  75.0  1.156  <0.001 0.972  100.0 0.047 <0.001 \n2 0.982 100.0 0.000 0.013 0.817 66.67 0.000 0.003 0.895  100.0  0.355  0.019 0.984  100.0 0.022 0.001  \n3 0.984 100.0 0.329 0.015 0.984 100.0 0.000 0.359 0.912  100.0  0.000  0.013 0.988  100.0 0 0.011  \n4 0.996 100.0 0.000 0.004 0.940 100.0 0.733 0.004 0.996  75.0  0.000  0.003 0.976  100.0 0.326 0.002  \n5 0.995 100.0 0.000 0.018 0.999 100.0 0.000 0.019 0.999  100.0  0.000  0.018 0.999  100.0 0 0.014  \nAver 0.924 90.0 0.264 - 0.912 88.3 0.503 - 0.936  90.0 0.302  - 0.984  100.0 0.079 - \nwhere the bold values indicate the best average results. \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3321414\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nvisualize the temporal -spatial-spectral features , which were \nextracted by the models with and without MB feature extractor. \nThe t-SNE visualization in 2D embedding space of inter -ictal \nand pre-ictal features on two datasets is shown in Fig. 5. We can \nsee that the binary-class feature distributions, learned by MB \nfeature extractor , presents a better discrimination than the \nmodel without MB feature extractor  on both CHB -MIT and \nXuanwu datasets. Especially for the models without sp atial-\nencoding, some inter-ictal and pre -ictal features are confused \ntogether. In contrast, the proposed model using MB feature \nextractor obtain more discriminative features , embodied in  \nvisible inter-class distance and dense intra-class distribution on \nboth two datasets. These phenomena also explain that the best \nseizure prediction performance can b e produced by combining \nthe temporal-conv, the spatial-encoding and the spectral-conv \nbranches simultaneously, which fully illustrates the innovation \nof MB feature extractor  in extracting the multi -level fine -\ngrained features jointly. \n2) Influence of point-wise dMGCN \nTo judge the contribution of the proposed point -wise \ndMGCN, we perform an  ablation experiments for our point -\nwise dMGCN to investigate its influence in the patient-specific \nseizure prediction.  Fig. 6 shows the comparison of AUC \nbetween the proposed models with and  without point-wise \ndMGCN for each patient. For CHB-MIT dataset, we can see \nthat the average AUC of our MB-dMGC-CWTFFNet is 7.3% \nhigher than that of the model without point -wise dMGCN . \nEspecially, a maximum AUC increase of 0. 18 (about 22.5% \nimprovement) occurs on Patient 13.  For Xuanwu dataset, the \nAUC of our proposed model increased by about 7.5% compared \nto the model without point-wise dMGCN, where the AUC gets \nan encouraging ascending from 0. 876 to 0.98 8 on Patient 3. \nMoreover, the relatively low standard deviations of 0.055 and \n0.01 on CHB-MIT and Xuanwu demonstrates the robustness of \nour proposed model with point -wise dMGCN . These \nimprovements of AUC on two datasets validate the outstanding \ncapability of the proposed point-wise dMGCN  in spatial \nrepresentation learning. Besides, in Table IV, after employing \npoint-wise dMGCN, the ð‘†ð‘†ð‘›ð‘› of our model outperform that of the \nablation model with 8.1% and 1 6.7% on two datasets \nrespectively, and the FPR/h is decreased from 0.326 into 0.059 \non CHB-MIT, from 0.651 into 0.079 on Xuanwu dataset. These \nenhancements give substantial evidences that point -wise \ndMGCN can better learn  the three-view graph structures with \nspatial position guidance, and extract deep multi-domain graph \nfeatures, which promotes the overall performance in seizure \nprediction warning. \n3) Efficacy of CWTFFNet \nIn order to fuse the dynamic multi-domain graph features, the \nCWTFFNet is adopted integrate the local and global \nrepresentation based on the channel-weighted self -attention \nmechanism. Therefore, we further compare the efficacy \nbetween our MB -dMGC-CWTFFNet and the model without \nCWTFFNet. T he results of the ablation experiment on two \ndatasets are presented in Fig. 7 . It can be noted that the \nCWTFFNet increases the average AUCs from 0.880 and 0.936 \nto 0.935 and 0.984 on two datasets, respectively. The standard \ndeviations achieve 0.013 and 0.032 lower than that of the model \nwithout CWTFFNet, which indicate the better generalization \nperformance of our proposed CWTFFNet across multiple \npatients. Especially for Patient 7 from CHB-MIT and Patient 3 \nfrom Xuanwu, their AUCs achieve greater improvements of  \n21.0% and 1 3.7% respectively. In addition, compared to the \nmodel without CWTFFNet  in Table IV , the proposed model \ngains higher ð‘†ð‘†ð‘›ð‘› of 7.5% on CHB-MIT and 13.3% on Xuanwu, \nand FPR/h values get decline of 0.135 and 0.227 after utilizing \nCWTFFNet on two datasets . It proves the advantage of \nincorporating the multi-domain graph features by means of the \nCWTFFNet.The local graph edges can be fully weighted into \nTABLE IV \nABLATION STUDIES ON TWO DATASETS \nDataset Methods AUC ð‘†ð‘†ð‘›ð‘›(%) FPR/h \nCHB-MIT \nwithout Temporal-Conv  0.911 94.8 0.294 \nwithout Spatial-Encoding 0.907 92.3 0.337 \nwithout Spectral-Conv 0.892 93.4 0.515 \nwithout dMGCN 0.871 89.7 0.326 \nwithout CWTFFNet 0.880 90.3 0.194 \nOur MB-dMGC-CWTFFNet 0.935 97.8 0.059 \nXuanwu \nwithout Temporal-Conv  0.954 85.0 0.146 \nwithout Spatial-Encoding 0.948 88.3 0.243 \nwithout Spectral-Conv 0.962 95.0 0.127 \nwithout dMGCN 0.915 83.3 0.651 \nwithout CWTFFNet 0.936 86.7 0.306 \n Our MB-dMGC-CWTFFNet 0.984 100.0 0.079 \nwhere the bold values indicate the proposed method. \n \n \nFig. 4. Performance comparison of AUC between the models with and \nwithout MB feature extractor on CHB-MIT and Xuanwu datasets. \n \nFig. 5. The t-SNE visualization in 2D embedding space of inter -ictal \nand pre-ictal features by comparing the models with and without MB \nfeature extractor on CHB-MIT and Xuanwu datasets. \n \nFig. 6. Performance comparison of AUC between the proposed models \nwith and without point-wise dMGCN on (a) CHB-MIT dataset and (b) \nXuanwu dataset. \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3321414\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nthe multi-head attention, and complement global channel \nposition information , which generates more distinguishable  \nfused features for the seizure prediction.  \nTo further evaluate the ability of the proposed CWTFFNet \ncontributing to the performance of the seizure prediction, we \nconduct the comparison of the prediction time with and without \nCWTFFNet on CHB-MIT and Xuanwu respectively, and the \nresults are unfolded in Fig. 8 . The proposed model s with and \nwithout CWTFFNet both successfully implement the seizure \nprediction in pre -ictal periods . H owever, for the identical \nseizure from CHB-MIT dataset, the model without CWTFFNet \njust achieves the seizure prediction with 5  minutes prior to the \nseizure onset, while our proposed model using CWTFFNet  \nobtain a 9 -minute advance of prediction time. For Xuanwu \ndataset, the prediction time was improved from 1 3 minutes to \n15 minutes before the seizure onset, and these improvements in \nseizure prediction time strongly embod y the innovative \ncontribution of CWTFFNet. Specifically, the channel-weighted \ntransformer in our CWTFFNet can reinforce the learning of the \nmodel by multi-channel-weighted self -attention mechanism . \nAccordingly, the fused features , containing local multi-graph \nstructures and global channel information, are more conducive \nto the seizure prediction.  \nB. Parameters Analysis of Post-Processing  \nIn this subsection, we mainly analyze the influence of two \nhyperparameters for our training model, the filter length and the \nthreshold Ï‰, on the seizure warning system transformed by our \nMB-dMGC-CWTFFNet. In the post -processing, the moving \naverage filter can smooth the probability outputs of our model \nby filtering the outliers, resulting in practical seizure prediction \nresults. Hence, the filter length in the moving average filter is \nset from 5 to  60 with a step of 5 ( unit: second), and the \ncorresponding variations of ð‘†ð‘†\nð‘›ð‘› and FPR/h on two datasets are \nshown in Fig. 9 (a) respectively. Interestingly, in both two \ndatasets, the larger filter length leads to the unsatisfactory ð‘†ð‘†ð‘›ð‘›, \nwhile the smaller filter length causes the poor FPR/h. The main \nreason is that a large filter length may result in over -smoothed \nprediction results, and thus some short -duration warnings are \nprobably missed. However, a small filter length may retain \nmore predicted outliers, which greatly increases the probability \nof false alarm [32] . C onsequently, to maintain the trade-off \nbetween ð‘†ð‘†ð‘›ð‘› and FPR/h, the filter lengths are configured to 15 \nfor CHB-MIT dataset and 25 for Xuanwu dataset. \nSince the seizure warning depends on whether the predicted \nprobability exceeds threshold Ï‰ , we discuss this \nhyperparameter to evaluate its sensitivity on the proposed \nmodel. The threshold Ï‰ is varied in the range from 0.1 to 0.9 \nwith a stride of 0.1, and the performance trade- off between ð‘†ð‘†\nð‘›ð‘› \nand FPR/h is also displayed in Fig. 9 (b). As can be observed, \nthe variation trends of two evaluation metrics along with the \nthreshold Ï‰ are similar to that with the filter length. The best \ntrade-off results between ð‘†ð‘†\nð‘›ð‘› and FPR/h both appear in the 0.6 \nthreshold on two datasets. Thus, to achieve optimal sei zure \nprediction after the post-processing, we set Ï‰ to 0.6 as the final \nfixed threshold for CHB -MIT and Xuanwu datasets, which is \nconsistent with existing studies [9, 32]. \nC. Performance Comparison of the state-of-the art Methods \nThe performance comparison of the state-of-the-art seizure \nprediction methods on CHB -MIT dataset  is summarized in \nTable V. In order to  discuss the advantages of our proposed \nmodel, we conduct an objective comparative analysis among \nthese methods. For example, Truong et al. [10] and Yang et al. \n[35] both employed the short-time Fourier transform (STFT) in \nthe CNN of seizure prediction frameworks, which were tested \non 13 patients and achieved the sensitivities of 81.2% and \n89.25% respectively, lower  than our MB -dMGC-CWTFFNet. \nThis is mainly because our proposed MB feature extractor can \nextract multi-level fine-grained features compared to traditional \ntime-frequency feature extraction methods. Compared with two \ndeep learning methods using spectr al power [4] and common \nspatial pattern ( CSP) [11] respectively, our proposed method \napplies the point -wise dMGCN learns three -view graph \nstructures and captures deep multi-domain graph features, so it \nyields 10.8% , 5.81% higher in ð‘†ð‘†ð‘›ð‘› and 0.127, 0.061 lower in \nFPR/h. Unlike the study [20]  that fused the multi -scale \n \nFig. 7. Performance comparison of AUC between the proposed model \nwith and without CWTFFN et on (a) CHB- MIT dataset and (b) \nXuanwu dataset. \n \n \n \nFig. 8. Performance comparison of the seizure prediction time with and \nwithout CWTFFNet from (a) CHB-MIT and (b) Xuanwu dataset. \n \nFig. 9. Performance comparison of  ð‘†ð‘†ð‘›ð‘› and FPR/h with different post-\nprocessing parameters. (a) filter length; (b) threshold. \n \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3321414\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \ntemporal-spatial features by attention mechanism based dilated \nCNN, our MB -dMGC-CWTFFNet introduces the local and \nglobal channel -weighted strategy into the multi -head self -\nattention units, which is beneficial to efficient feature fusion for \ncomplex graph structures and outperforms the TS -MS-DCNN \nwith 4.51% ð‘†ð‘†ð‘›ð‘› . Although some advanced methods [9,  15] \ngained the suboptimal performance in seizure prediction, their  \nvalidation scheme using 10 -Fold CV shuffled original EEG \nsignals and destroyed the continuity of epileptic activity over \ntime, and is not conducive to real -time seizure warning \ncompared with our adopted LOOCV scheme. Additionally, \ncompared with the model proposed by Liang et al.  [37], our \nMB-dMGC-CWTFFNet achieves 9.01% higher in ð‘†ð‘†ð‘›ð‘›  and \n0.123 lower in FPR/h. Because our proposed method considers \nmulti-domain variable information and constructs the multi -\ngraph fram ework, it offsets the lack of partial domain  \ninformation from the feature alignment strategy in SSDA-SPM. \nIn summary, compared to most of existing studies , the main \ndifferences and a dvantages of our MB-dMGC-CWTFFNet \ninclude that it can dynamically learn changes in multi-graph \ntopologies with spatial position guidance . Meanwhile, it \nefficiently fuses multi-domain graph features by using channel-\nweighted multi-head self-attention mechanism. \nD. Limitations and Future Directions \nAlthough our proposed prediction framework achieves \nsatisfactory seizure prediction  performance, two limitations \nstill exist in our study. First, our MB -dMGC-CWTFFNet can \nrealize the end -to-end seizure warning without complicated \nEEG pre-processing, but the artifacts in epileptiform discharges \nand potential bad channels may interfere with the predictor and \ncause some false positives in practical warning scenario. \nTherefore, we will devote to exploring the adaptive channel \nselection [38, 39] and unsupervised artifact removal algorithms \n[40], and further eliminating the redundant information in raw \nepileptic signals. Second, our proposed method conducts a  \npatient-specific seizure prediction by training with the same \npatientâ€™s data, while it is difficult to complete the model fine -\ntuning across patients . Thus, we will combine the domain -\nadversarial transfer learning strategies [41, 42] with our seizure \nprediction framework in the future work, which aims to handle \nthe drifting distribution between  target domain  and source \ndomain, and contributes to the cross-patient seizure prediction. \nV. CONCLUSION \nIn this study, we propose a novel  EEG-based MB-dMGC-\nCWTFFNet framework for patient-specific seizure prediction. \nThe MB feature extractor  is adopted to effectively capture the \nmulti-level fine -grained representations in multiple \ndomains.The designed point-wise dMGCN is further employed \nto dynamically learn  the deep graph structures  with spatial \nposition guidance, which contributes to extracting the multi-\ndomain graph features from temporal, spatial and spectral \nviews. Finally, the CWTFFNet  utilizes the local and global \nchannel-weight strategy to facilitate the efficient fusion of high-\nlevel graph features. Furthermore, we conduct the comparative \nexperiments on two epileptic datasets, and the results show our \nproposed MB-dMGC-CWTFFNet obtains a better evaluation \nmetrics, whose AUC, ð‘†ð‘†ð‘›ð‘› , FPR/h achieve 0.935 and 0.984, \n97.8% and 100.0%, 0.059 and 0.079 on CHB-MIT and Xuanwu \ndatasets respectively,  outperforming the state -of-the-art \nmethods. These findings prove the outstanding performance of \nour proposed MB-dMGC-CWTFFNet in patient -specific \nseizure prediction, and indicate  its potential application \nprospect in neurostimulation treatment of refractory epilepsy  \npatients. \nREFERENCES \n[1] E. Beghi et al., \"Global, regional, and national burden of epilepsy, 1990â€“\n2016: a systematic analysis for the Global Burden of Disease Study \n2016,\" Lancet Neurol., vol. 18, no. 4, pp. 357-375, 2019. \n[2] W. H. Organization, Epilepsy: a public health imperative. World Health \nOrganization, 2019. \n[3] N. Rincon, D. Barr, and N. Velez- Ruiz, \"Neuromodulation in Drug \nResistant Epilepsy,\" Aging Dis., vol. 12, no. 4, pp. 1070-1080, 2021. \n[4] A. R. Ozcan and S. Erturk, \"Seizure Prediction in Scalp EEG Using 3D \nConvolutional Neural Networks With an Image-Based Approach,\" IEEE \nTrans. Neural Syst. Rehabil. Eng., vol. 27, no. 11, pp. 2284-2293, 2019. \n[5] Y. Li, W. G. Cui, H. Huang, Y. Z. Guo, K. Li, and T. Tan, \"Epileptic \nseizure detection in EEG signals using sparse multiscale radial ba sis \nfunction networks and the Fisher vector approach,\" Knowledge -Based \nSyst., vol. 164, pp. 96-106, 2019. \n[6] T. Dissanayake, T. Fernando, S. Denman, S. Sridharan, and C. Fookes, \n\"Deep Learning for Patient -Independent Epileptic Seizure Prediction \nUsing Scalp EEG Signals,\" IEEE Sens. J., vol. 21, no. 7, pp. 9377-9388, \n2021. \n[7] Îš. Îœ. Tsiouris et al., \"A long short-term memory deep learning network \nfor the prediction of epileptic seizures using EEG signals,\" Comput. Biol. \nMed., vol. 99, pp. 24-37, 2018. \nTABLE V \nEXPERIMENTAL SETTINGS AND PERFORMANCE COMPARISON OF THE STATE-OF-THE-ART METHODS ON CHB-MIT \nAuthors Year Methods No. of \nPatients \nNo. of \nSeizures \nValidation \nScheme \nInterictal-Preictal \nIntervals (Min) ð‘†ð‘†ð‘›ð‘›(%) FPR/h \nKhan et al. [9]  2018 WT-CNN 15 18 10-Fold CV 10-10 87.80 0.147 \nTruong et al. [10]  2018 STFT-CNN 13 64 LOOCV 240-30 81.20 0.160 \nOzcan et al. [4]  2019 SP-3DCNN 16 77 LOOCV 120-60 87.01 0.186 \nZhang et al. [11]  2020 CSP-CNN 23 156 LOOCV NR-30 92.00 0.120 \nYang et al. [35]  2021 STFT-RDANet 13 64 LOOCV 240-30 89.25 0.122 \nGao et al. [20]  2022 TS-MS-DCNN 16 85 LOOCV 60-30 93.30 0.007 \nDissanayake et al. [15]  2022 GDL 23 NR 10-Fold CV NR-60 95.94 NR \nLiang et al. [37] 2023 SSDA-SPM 13 64 LOOCV 240-30 88.80 0.182 \nThis Work 2023 Our MB-dMGC-CWTFFNet 19 99 LOOCV 120-15 97.81 0.059 \nwhere the bold values indicate the proposed method. NR is the nor reported values. \n \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3321414\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n[8] H. Daoud and M. A. Bayoumi, \"Efficient Epileptic Seizure Prediction \nBased on Deep Learning,\" IEEE Trans. Biomed. Circuits Syst., vol. 13, \nno. 5, pp. 804-813, 2019. \n[9] H. Khan, L. Marcuse, M. Fields, K. Swann, and B. Yener, \"Focal Onset \nSeizure Prediction Using Convolutional Networks,\" IEEE Trans. Biomed. \nEng., vol. 65, no. 9, pp. 2109-2118, 2018. \n[10] N. D. Truong et al., \"Convolutional neural networks for seizure prediction \nusing intracranial and scalp electroencephalogram,\" Neural Netw, vol. \n105, pp. 104-111, 2018. \n[11] Y. Zhang, Y. Guo, P. Yang, W. Chen, and B. Lo, \"Epilepsy Seizure \nPrediction on EEG Using Common Spatial Pattern and Convolutional \nNeural Network,\" IEEE J. Biomed. Health Inform., vol. 24, no. 2, pp. 465-\n474, 2020. \n[12] T. Zhang, X. Wang, X. Xu, and C. P. Chen, \"GCB -Net: Graph \nconvolutional broad network and its application in emotion recognition,\" \nIEEE Trans. Affect. Comput., vol. 13, no. 1, pp. 379-388, 2019. \n[13] D. Zhang, L. Yao, K. Chen, S. Wang, P. D. Haghighi, and C. Sullivan, \n\"A graph -based hierarchical attention model for movement intention \ndetection from EEG signals,\" IEEE Trans. Neural Syst. Rehabil. Eng., vol. \n27, no. 11, pp. 2247-2253, 2019. \n[14] Q. Lian, Y. Qi, G. Pan, and Y. Wang, \"Learning graph in graph \nconvolutional neural ne tworks for robust seizure prediction,\" J. Neural \nEng., vol. 17, no. 3, p. 035004, 2020. \n[15] T. Dissanayake, T. Fernando, S. Denman, S. Sridharan, and C. Fookes, \n\"Geometric Deep Learning for Subject Independent Epileptic Seizure \nPrediction Using Scalp EEG Signals,\" IEEE J. Biomed. Health Inform., \nvol. 26, no. 2, pp. 527-538, 2022. \n[16] S. Jang, S.-E. Moon, and J.-S. Lee, \"EEG-based video identification using \ngraph signal modeling and graph convolutional neural network,\" in 2018 \nIEEE International Conference  on Acoustics, Speech and Signal \nProcessing (ICASSP), 2018, pp. 3066-3070: IEEE. \n[17] M. Wang, H. El-Fiqi, J. Hu, H. A. Abbass, and Security, \"Convolutional \nneural networks using dynamic functional connectivity for EEG -based \nperson identification in diverse human states,\" IEEE Trans. Inf. Forensic \nSecur., vol. 14, no. 12, pp. 3259-3272, 2019. \n[18] P. Zhong, D. Wang, and C. Miao, \"EEG-based emotion recognition using \nregularized graph neural networks,\" IEEE Trans. Affect. Comput., vol. 13, \nno. 3, pp. 1290-1301, 2020. \n[19] Y. Li, Y. Liu, W. G. Cui, Y. Z. Guo, H. Huang, and Z. Y. Hu, \"Epileptic \nSeizure Detection in EEG Signals Using a Unified Temporal -Spectral \nSqueeze-and-Excitation Network,\" IEEE Trans. Neural Syst. Rehabil. \nEng., vol. 28, no. 4, pp. 782-794, 2020. \n[20] Y. Gao et al., \"Pediatric Seizure Prediction in Scalp EEG Using a Multi-\nScale Neural Network With Dilated Convolutions,\" IEEE J. Transl. Eng. \nHealth Med., vol. 10, pp. 1-9, 2022. \n[21] H. He and D. Wu, \"Transfer Learning for Brain -Computer Interfaces: A \nEuclidean Space Data Alignment Approach,\" IEEE Trans. Biomed. Eng., \nvol. 67, no. 2, pp. 399-410, 2020. \n[22] S. Supriya, S. Siuly, H. Wang, and Y. C. Zhang, \"Epilepsy Detection \nFrom EEG Using Complex Network Techniques: A Review,\" IEEE Rev. \nBiomed. Eng., vol. 16, pp. 292-306, 2023. \n[23] F. Bartolomei et al., \"Defining epileptogenic networks: Contribution of \nSEEG and signal analysis,\" Epilepsia, vol. 58, no. 7, pp. 1131-1147, 2017. \n[24] Y. Li, X. D. Wang, M. L. Luo, K. Li, X. F. Yang, and Q. Guo, \"Epileptic \nSeizure Classification of EEGs Using Time -Frequency Analysis Based \nMultiscale Radial Basis Functions,\" IEEE J. Biomed. Health Inform., vol. \n22, no. 2, pp. 386-397, 2018. \n[25] Q. F. Li, L. L. Shen, S. Guo, and Z. H. Lai, \"WaveCNet: Wavelet \nIntegrated CNNs to Suppress Aliasing Effect for Noise -Robust Image \nClassification,\" IEEE Trans. Image Process., vol. 30, pp. 7074 -7089, \n2021. \n[26] K. Fukumori, N. Yoshida, H. Sugano, M. Nakajima, and T. Tanaka, \n\"Epileptic Spike Detection Using Neural Networks With Linear -Phase \nConvolutions,\" IEEE J. Biomed. Health Inform., vol. 26, no. 3, pp. 1045-\n1056, 2022. \n[27] Y. Li, L. Guo, Y. Liu, J. Liu, and F. Meng, \"A Temporal-Spectral-Based \nSqueeze-and- Excitation Feature Fusion Network for Motor Imagery \nEEG Decoding,\" IEEE Trans. Neural Syst. Rehabil. Eng., vol. 29, pp. \n1534-1545, 2021. \n[28] J. N. Acharya and V. J. Acharya, \"Overview of EEG Montages and \nPrinciples of Localization,\" J. Clin. Neurophysiol., vol. 36, no. 5, pp. 325-\n329, 2019. \n[29] Y. Chen et al., \"Channel-Unet: A Spatial Channel -Wise Convolutional \nNeural Network for Liver and Tumors Segmentation,\" Front. Genet., \nOriginal Research vol. 10, 2019. \n[30] G. Xie, K. Yang, T. Zhang, J. Wang, and J. Lai, \"Balanced Decoupled \nSpatial Convolution for CNNs,\" IEEE Trans. Neural Netw. Learn. Syst., \nvol. 30, no. 11, pp. 3419-3432, 2019. \n[31] C. L. Liu et al., \"RB-Net: Training Highly Accurate and Efficient Binary \nNeural Networks With Reshaped Point-Wise Convolution and Balanced \nActivation,\" IEEE Trans. Circuits Syst. Video Technol., vol. 32, no. 9, pp. \n6414-6424, 2022. \n[32] Y. Li, Y. Liu, Y. Z. Guo, X. F. Liao, B. Hu, and T. Yu, \"Spatio-Temporal-\nSpectral Hierar chical Graph Convolutional Network With \nSemisupervised Active Learning for Patient-Specific Seizure Prediction,\" \nIEEE T. Cybern, vol. 52, no. 11, pp. 12189-12204, 2022. \n[33] D. E. Snyder, J. Echauz, D. B. Grimes, and B. Litt, \"The statistics of a \npractical seizure warning system,\" J. Neural Eng., vol. 5, no. 4, p. 392, \n2008. \n[34] A. H. Shoeb, \"Application of machine learning to epileptic seizure onset \ndetection and treatment,\" Ph.D. dissertation, Harvard -MIT Health Sci. \nTechnol., Massachusetts Institute of Technology, Cambridge, MA, USA, \n2009. \n[35] X. Yang, J. Zhao, Q. Sun, J. Lu, and X. Ma, \"An Effective Dual Self -\nAttention Residual Network for Seizure Prediction,\" IEEE Trans. Neural \nSyst. Rehabil. Eng., vol. 29, pp. 1604-1613, 2021. \n[36] T. Yu et al., \"High-frequency stimulation of anterior nucleus of thalamus \ndesynchronizes epileptic network in humans,\" Brain, vol. 141, no. 9, pp. \n2631-2643, 2018. \n[37] D. Liang, A. Liu, Y. Gao, C. Li, R. Qian, and X. Chen, \"Semi-Supervised \nDomain-Adaptive Seizure Predictio n via Feature Alignment and \nConsistency Regularization,\" IEEE Trans. Instrum. Meas., vol. 72, pp. 1-\n12, 2023. \n[38] J. X. Wang, L. Shi, W. Q. Wang, and Z. G. Hou, \"Efficient Brain \nDecoding Based on Adaptive EEG Channel Selection and \nTransformation,\" IEEE Trans. Emerg. Top. Comput. Intell., vol. 6, no. 6, \npp. 1314-1323, 2022. \n[39] A. Nagarajan, N. Robinson, and C. Guan, \"Relevance -based channel \nselection in motor imagery brain-computer interface,\" J. Neural Eng., vol. \n20, no. 1, 2023. \n[40] M. Wang  et al. , \"Mu ltidimensional Feature Optimization Based Eye \nBlink Detection Under Epileptiform Discharges,\" IEEE Trans. Neural \nSyst. Rehabil. Eng., vol. 30, pp. 905-914, 2022. \n[41] S. Tan, K. L. Wang, H. B. Shi, and B. Song, \"A Novel Multiview \nPredictive Local Adversarial Network for Partial Transfer Learning in \nCross-Domain Fault Diagnostics,\" IEEE Trans. Instrum. Meas., vol. 72, \n2023, Art. no. 3504712. \n[42] Y. Li, A. M. Zhang, J. J . Huang, and Z. Xu, \"An Approach Based on \nTransfer Learning to Lifetime Degradation Rate Prediction of the Dry -\nType Transformer,\" IEEE Trans. Ind. Electron., vol. 70, no. 2, pp. 1811-\n1819, 2023. \n \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3321414\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7468284368515015
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.6165392994880676
    },
    {
      "name": "Graph",
      "score": 0.534852147102356
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5340228080749512
    },
    {
      "name": "Feature extraction",
      "score": 0.47176697850227356
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2579369843006134
    }
  ]
}