{
  "title": "REPT: Bridging Language Models and Machine Reading Comprehension via Retrieval-Based Pre-training",
  "url": "https://openalex.org/W3176756782",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5045631551",
      "name": "Fangkai Jiao",
      "affiliations": [
        "Shandong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5054313646",
      "name": "Yangyang Guo",
      "affiliations": [
        "Shandong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5109718198",
      "name": "Yilin Niu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5110717773",
      "name": "Feng Ji",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5101808283",
      "name": "Feng-Lin Li",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5038612499",
      "name": "Liqiang Nie",
      "affiliations": [
        "Shandong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3104417388",
    "https://openalex.org/W2944668088",
    "https://openalex.org/W2951561177",
    "https://openalex.org/W3020987135",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3003186568",
    "https://openalex.org/W3035290244",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W4287633642",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W3192587235",
    "https://openalex.org/W3103188966",
    "https://openalex.org/W2990928880",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W3126822054",
    "https://openalex.org/W3102663935",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3035027743",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2912904516",
    "https://openalex.org/W4288548690",
    "https://openalex.org/W3103838460",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3103318178",
    "https://openalex.org/W4288614645",
    "https://openalex.org/W2996164352",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3105055324",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2788448041",
    "https://openalex.org/W2970900584",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W3034651559"
  ],
  "abstract": "Pre-trained Language Models (PLMs) have achieved great success on Machine Reading Comprehension (MRC) over the past few years.Although the general language representation learned from large-scale corpora does benefit MRC, the poor support in evidence extraction which requires reasoning across multiple sentences hinders PLMs from further advancing MRC.To bridge the gap between general PLMs and MRC, we present REPT, a REtrieval-based Pre-Training approach.In particular, we introduce two self-supervised tasks to strengthen evidence extraction during pre-training, which is further inherited by downstream MRC tasks through the consistent retrieval operation and model architecture.To evaluate our proposed method, we conduct extensive experiments on five MRC datasets that require collecting evidence from and reasoning across multiple sentences.Experimental results demonstrate the effectiveness of our pre-training approach.Moreover, further analysis shows that our approach is able to enhance the capacity of evidence extraction without explicit supervision.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 150–163\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n150\nREPT: Bridging Language Models and Machine Reading Comprehension\nvia Retrieval-Based Pre-training\nFangkai Jiao1∗, Yangyang Guo1, Yilin Niu2, Feng Ji3, Feng-Lin Li3, Liqiang Nie1†\n1 School of Computer Science and Technology, Shandong University, Qingdao, China\n2 Department of Computer Science and Technology, Tsinghua University, Beijing, China\n3 Damo Academy, Alibaba Group, Hangzhou, China\njiaofangkai@hotmail.com guoyang.eric@gmail.com niuyl14j@gmail.com\n{zhongxiu.jf, fenglin.lfl}@alibaba-inc.com nieliqiang@gmail.com\nAbstract\nPre-trained Language Models (PLMs) have\nachieved great success on Machine Reading\nComprehension (MRC) over the past few\nyears. Although the general language represen-\ntation learned from large-scale corpora does\nbeneﬁt MRC, the poor support in evidence ex-\ntraction which requires reasoning across mul-\ntiple sentences hinders PLMs from further ad-\nvancing MRC. To bridge the gap between gen-\neral PLMs and MRC, we present REPT, a\nREtrieval-based Pre-Training approach. In\nparticular, we introduce two self-supervised\ntasks to strengthen evidence extraction dur-\ning pre-training, which is further inherited by\ndownstream MRC tasks through the consis-\ntent retrieval operation and model architecture.\nTo evaluate our proposed method, we conduct\nextensive experiments on ﬁve MRC datasets\nthat require collecting evidence from and rea-\nsoning across multiple sentences. Experimen-\ntal results demonstrate the effectiveness of our\npre-training approach. Moreover, further anal-\nysis shows that our approach is able to enhance\nthe capacity of evidence extraction without ex-\nplicit supervision.\n1 Introduction\nMachine Reading Comprehension (MRC) is an im-\nportant task to evaluate the machine understanding\nof natural language. Given a set of documents and a\nquestion (with possible options), an MRC system is\nrequired to provide the correct answer by either re-\ntrieving a meaningful span (Rajpurkar et al., 2018a)\nor selecting the correct option from a few candi-\ndates (Lai et al., 2017; Sun et al., 2019; Guo et al.,\n2019, 2021). Recently, with the development of\nself-supervised learning, the pre-trained language\nmodels (Devlin et al., 2019; Y ang et al., 2019b)\n*Work is done during internship at Alibaba Group.\n†Corresponding author: Liqiang Nie.\nﬁne-tuned on several machine reading comprehen-\nsion benchmarks (Reddy et al., 2019; Kwiatkowski\net al., 2019) have achieved superior performance.\nThe dominant reason lies in the strong and general\ncontextual representation learned from large-scale\nnatural language corpora. Nevertheless, PLMs fo-\ncus more on the general language representation\nand semantics to beneﬁt various downstream tasks,\nwhile MRC demands the capability of extracting\nevidence across one or multiple documents and per-\nforming reasoning over the collected clues (Fang\net al., 2020; Y ang et al., 2018). Put it differently,\nthere exists an obvious gap, indicating an insufﬁ-\ncient exploitation of PLMs over MRC.\nSome efforts have been made to bridge the gap\nbetween PLMs and downstream tasks, which can\nbe roughly divided into two categories: knowledge\nenhancement and task-oriented pre-training (Qiu\net al., 2020). The former introduces commonsense\nor world knowledge into the pre-training (Zhang\net al., 2019; Sun et al., 2020; V arkel and Globerson,\n2020; Y e et al., 2020) or ﬁne-tuning (Y ang et al.,\n2019a) for better performance over knowledge-\ndriven tasks. And the latter includes some deli-\ncately designed pre-training tasks, e.g., the con-\ntrastive approach of learning discourse knowledge\ntowards textual entailment task (Iter et al., 2020).\nAlthough these approaches have achieved some im-\nprovements on certain tasks, few of them are specif-\nically designed for evidence extraction, which is\nindeed indispensable to MRC.\nIn fact, equipping PLMs with the capability of\nevidence extraction in MRC is challenging due\nto the following two factors. 1) The process of\ncollecting clues from a document is difﬁcult to be\nintegrated into PLMs without designing speciﬁc\nmodel architectures or pre-training tasks (Qiu et al.,\n2020; Zhao et al., 2020). And 2) large-scale pre-\ntraining process would make PLMs overﬁt to pre-\n151\nInput Order:        12 3 4 5 6 7 8\nOriginal Order: 1 34 2 5678\nRecovery: \n1. [MASK A] -> education\n2. [MASK B] -> care\n3. [MASK C] -> Orana\nQuery\n1. History The Mentally Retarded Children’s Society of SA Inc. was\nestablished in 1950 by a group of parents who wanted [MASK A]\nemployment and accommodation opportunities for their children within\nthe local community at a time when institutionalised [MASK B] in\nAdelaide was their only alternative.\n2. Today [MASK C] provides assisted employme nt, assisted accommodation\nand respite services to people with intellectual disabilities.\nPassage\n3. The society’s aims were to seek education or training facilities for people\nwith intellectual disabilities to establish sheltered workshops and to\nestablish residential hostels.\n4. A number of sheltered workshops were established and in 1980, the name\nwas changed to the Aboriginal word Orana which means Welcome.\n5. Orana’s current and previous clients include Mitsubishi Motors Clipsal\nRAA Elders Limited and Billycart Kids.\n6. Orana was one of the first disability service organisations to achieve\nQuality Accreditation.\n7. After the unveiling of the Australi an Government’s Commonwealth Home\nSupport Programme CHSP and seeing it as a natural step of progression\nOrana now provides quality tailored aged care at home.\n8. The well resourced organization delivers help across a range of areas\nhelping the elderly remain where they want to be in the comfort of their\nown home during their later years.\nFigure 1: A running example obtained from our\nmethod. The query sentences are extracted from the\noriginal document with some crucial information be-\ning randomly masked, i.e., the sentence 1 and 2. The\nmodel is required to predict the preceding and follow-\ning sentence for each query in the original document\nand recover the masked clues, i.e., infer the original or-\nder from input order and ﬁll the [MASK] with the ini-\ntial token. The phrases in boxes are the possible clues\nfor recovering the masked tokens and the correct order.\ntraining tasks (Chung et al., 2021; Tamkin et al.,\n2020). In other words, it is difﬁcult to take full\nadvantage of the pre-training merits if the training\nobjectives of pre-training and downstream MRC\nare greatly separated.\nTo deal with the aforementioned challenges, we\npropose a novel retrieval-based pre-training ap-\nproach, REPT, to bridge the gap between PLMs\nand MRC. Firstly, to unify the training objective,\nwe design a novel pre-training task, namely Sur-\nrounding Sentences Prediction (SSP), as illustrated\nin Figure1. Given a document, several sentences\nwill be ﬁrstly selected as queries, and the others are\njointly treated as a passage1. Thereafter, for each\nquery, the model should predict its preceding and\nfollowing sentences in the original document by\ncollecting clues from each sentence, which is com-\npatible with evidence extraction in MRC tasks. It\nis worth emphasizing that, the repeated occurrence\nof entities or nouns across different sentences of-\n1We usepassage here to keep consistent with MRC tasks.\nAnd document refers to the combination of queries andpas-\nsage.\nten lead to information short-cut (Lee et al., 2020),\nfrom which the order of sentences can be easily re-\ncovered. In view of this, we propose to mask such\nexplicit clues. As a result, the model is enforced to\ninfer the correct positions of queries by gathering\nevidence with the incomplete information. Sec-\nondly, to preserve the effectiveness of contextual\nrepresentation, the masked clues are also required\nto be recovered through retrieving relevant infor-\nmation from other parts of the document, which\nis implemented via our Retrieval based Masked\nLanguage Modeling (RMLM) task.\nIn this way, the pre-training stage can be prop-\nerly aligned with MRC: 1) the training objectives\nare connected through the introduction of the two\npre-training tasks, which will be inherited by down-\nstream MRC tasks through consistent retrieval oper-\nation. And 2) the capability of evidence extraction\nfrom documents or sentences is enhanced during\npre-training, and will be smoothly transferred to\nMRC. Our contributions in this paper are summa-\nrized as follows:\n1. We present REPT, a novel pre-training ap-\nproach, to bridge the gap between PLMs and\nMRC through retrieval-based pre-training.\n2. We design two self-supervised pre-training\ntasks, i.e., SSP and RMLM, to augment PLMs\nwith the ability of evidence extraction with\nthe help of retrieval operation and eliminating\ninformation short-cut, which can be smoothly\ntransferred to downstream MRC tasks.\n3. We evaluate our method over ﬁve reading\ncomprehension benchmarks of two different\ntask forms: Multiple Choice QA (MCQA)\nand Span Extraction (SE). The substantial im-\nprovements over strong baselines demonstrate\nthe effectiveness of our pre-training approach.\nWe conduct an empirical study to verify that\nour method are able to enhance evidence ex-\ntraction as expected.\n2 Related Work\nMRC has received increasing attention in recent\nyears. Many challenging benchmarks have been\nestablished to examine various forms of reasoning\nabilities, e.g., multi-hop (Y ang et al., 2018), dis-\ncrete (Dua et al., 2019), and logic reasoning (Yu\net al., 2020). To solve the problem, a typical design\nis to gather possible clues through entity linking\n152\n(Zhao et al., 2020) or self-constructed graph (Fang\net al., 2020; Ran et al., 2019), and then perform\nmulti-step reasoning. It is worth noting that, gath-\nering clues is vital but challenging, especially for\nlong document understanding. Some efforts have\nbeen dedicated to improving evidence extraction\nvia direct (Wang et al., 2018) or distant supervision\n(Niu et al., 2020).\nGenerally, the ﬁne-tuned PLMs (Devlin et al.,\n2019; Y ang et al., 2019b) can obtain superior per-\nformance in MRC due to their strong and general\nlanguage representation. However, there still exist\nsome gaps between PLMs and various downstream\ntasks, since certain abilities required by the down-\nstream tasks cannot be learned through the existing\npre-training tasks (Qiu et al., 2020). In order to\ntake full advantage of PLMs, a few studies attempt\nto align the pre-training and ﬁne-tuning stages. For\nexample, Tamborrino et al. (2020) reformulated\nthe commonsense question answering task as scor-\ning via leveraging the predicted probabilities for\nMasked Language Modeling (MLM) in RoBERTa\n(Liu et al., 2019). With the help of the common-\nsense learned through MLM, the method achieves\ncomparable results with supervised approaches in\nzero-shot setting, indicating that bridging the gap\nbetween these two stages yields considerable im-\nprovement. Chung et al. (2021) tried to address\nthe overﬁtting problem during pre-training through\ndecoupling input and output embedding weights\nand enlarging the embedding size during decoding.\nThe resultant model is therefore more transferable\nacross tasks and languages.\nIn addition, some task-oriented pre-training\nmethods have also been developed. For instance,\nWang et al.(2020) proposed a novel pre-training\nmethod for sentence representation learning, where\nthe masked tokens in a sentence are forced to be\nrecovered from other sentences through sentence-\nlevel attention. Based on this, the attention weights\ncan be directly ﬁne-tuned to rank the candidates\nin answer selection or information retrieval.Lee\net al.(2019) tried to learn the dense document rep-\nresentation for information retrieval by minimizing\nthe distance between the representation of an query\nsentence and its context.Guu et al.(2020) designed\nan augmented MLM tasks to jointly train a neural\nretriever and a language model for Open-domain\nQA. Different from these methods ranking the doc-\numents for open-domain QA, our approach focuses\non enhancing the ability of evidence extraction in\nMRC, where the MLM based task by it alone is\ninsufﬁcient.\n3 Method\nIn this section, we present the details of the pro-\nposed method, REPT. We ﬁrstly describe the data\npre-processing part (§3.1), and then illustrate the\ntwo pre-training tasks, i.e., SSP and RMLM (§3.3)\nand the training objectives (§3.4). Finally, we detail\nhow to ﬁne-tune our pre-trained model for down-\nstream tasks through retrieval-based evidence ex-\ntraction (§3.5).\n3.1 Data Pre-processing\nFor pre-training, we use the English Wikipedia2 as\nour training data. We divide each Wikipedia article\ninto segments, each containing up to 500 tokens3\nwithout overlapping. We treat each segment as a\ndocument and split it into several sentences4.\nIn order to increase the difﬁculty and efﬁciency\nof pre-training, for each document, we select 30%\nof the most important sentences as queries and the\nrest in their original order as a passage. Speciﬁcally,\nthe importance of each sentence in a document is\nmeasured through the summation of the importance\nof entities and nouns it contains, which is further\ndeﬁned as the number of sentences an entity/noun\noccurs. Hereafter, masking is introduced to enti-\nties and nouns in queries according to pre-deﬁned\nratios to eliminate information short-cut. More de-\ntails about the masking strategy are described in\nAppendix A and an example after pre-processing\ncan be found in Figure1.\n3.2 Task Deﬁnition\nWe treat a document as a sequence ofn sequen-\ntial sentences with m tokens. Supposing that\nthere are t sentences selected as queries follow-\ning §3.1, the rearranged sequence is deﬁned asS =\n[s1,s2,··· ,st,··· ,sn], and the index of queries is\nQ = {1,2,··· ,t}. Besides, we deﬁne a mapping\nfunction r to map the rearranged sentences to their\noriginal position. Taking Figure1 as an example,\nthe mapping r(s1)=1 ,r (s2)=4 ,r (s3)=2\nand (s4)=3 indicates that the original order is\n{s1,s3,s4,s2,···} .\nTaking S as input, the Surrounding Sentences\nPrediction task should predict the correct sentence\n2We use the 2020/05/01 dump.\n3The tokenized sub-words following BERT and RoBERTa.\n4Any sentence with less than ﬁve tokens is concatenated\nto its previous one.\n153\nb) Sentence-Level Retrieval for SSP\nand MCQA\nMLP\nAttention\nSSP\nMCQA\nTransformer\nଵ[CLS]ଶڮே[SEP] [SEP]ڮ\nMulti-Head\nAttention\nଵ[CLS]ଶڮே[SEP] [SEP]ڮ\nଵଶଷ\na) Encoder\nc) Document-Level Retrieval for RMLM, ODQA and\nSpan Extraction (SE)\nRMLM\nSE\nMLP\nQuery Representation\nToken Representation\nPre-training\nFine-tuning\nWeighted Average of Tokens\n௜\n௝\n௜\n௝\nFigure 2: Framework of our model. a) Encoder composed of a pre-trained Transformer encoder and a query\ngenerator based on multi-head attention. b) The attention-based sentence-level retrieval for evidence extraction for\neach sentence, which will be further adopted by SSP during pre-training and MCQA during ﬁne-tuning. c) The\nattention-based document-level retrieval for evidence extraction among the input sequence, which is employed for\nRMLM. For SE, the similarity function is directly ﬁne-tuned.\nindex a and b for each querysq with q ∈Q 5:\n{ r(sa)= r(sq)−1,\nr(sb)= r(sq)+1 . (1)\nAs for the Retrieval based Masked Language\nModeling (RMLM) task, the model should recover\nall the masked tokens in each querysq.\n3.3 Model\nFirst of all, we leverage a pre-trained Transformer\n(V aswani et al., 2017), such as BERT, as our en-\ncoder to obtain the contextual representation of sen-\ntences. The output of Transformer is formulated as:\nH =[ hcls,··· ,hm,hsep] = Encoder(˜S), (2)\nwhere H ∈ Rd×(m+3), anddis the hidden size. For\na better illustration, we will useHi to represent\nthe hidden state matrix of tokens that belong to\nsentence si, such that:\nH =[ H1,H2,··· ,Hn], Hi ∈ Rd×li ,\nwhere li is the length of sentencesi and m = ∑\ni li.\nSince the process for each query is exactly the same,\nwe useq ∈Q as a representative to introduce the\ncalculation with respect to each query below.\n5Speciﬁcally, forr(sq)=1 or r(sq)= n, the correspond-\ning prediction task is removed since its preceding or following\nsentence does not exist.\n3.3.1 Query Representation\nIn order to gather potential clues from a docu-\nment or sentences, we adopt the multi-head at-\ntention mechanism proposed by (V aswani et al.,\n2017) to obtain the sentence-level representation\nfor each query. Formally, the attention mechanism\nis deﬁned asMHA(Q,K,V), whereQ,K,V are\nquery, key and value matrices, respectively. To con-\nsider the global information, we leveragehcls as\nthe query vector, andHq as K and V:\nvq\n0\n⊤ = MHA(h⊤\ncls,Hq,Hq). (3)\nDuring pre-training, we reuse the layer deﬁned\nby Equation 3 with Q = vq\n0 and K = V = Hq,\nto generate the task-speciﬁc query representation\nvq, which is designed to alleviate the overﬁtting\nproblem (He et al., 2021).\n3.3.2 Surrounding Sentence Prediction\nTo enhance the capability of pre-trained models for\nevidence extraction, we have carefully designed the\nSSP task, where the model should predict the pre-\nceding and following sentences for a given query by\nextracting the relevant evidence from each sentence.\nConsequently, we introduce a retrieval operation,\nwhich is implemented via a single-head attention\n154\nmechanism6:\nui\nq\n⊤\n= Att(vq⊤ ,Hi,Hi), (4)\nwhere ui\nq is the representation of sentencesi, high-\nlighting the evidence information pertaining to\nquery sq. Finally, the score of each sentence in\nthe document with regard tosq is obtained through:\noi\nq = W2(tanh(W1ui\nq +b1))+ b2. (5)\n3.3.3 Retrieval based MLM\nSince the masking noise introduced when construct-\ning queries could also bring inconsistency between\npre-training and ﬁne-tuning, we further designed a\nretrieval based MLM task to alleviate this problem.\nIn the RMLM task, the model should predict the\nmasked entities or nouns through retrieving rele-\nvant information from a document. More speciﬁ-\ncally, the query-aware evidence representation of\nthe input sequence is obtained via:\ngq⊤ = Att(vq⊤ ,H,H). (6)\nDenoting the index of a masked token in query\nsq as z, the representation of the masked tokensq\nz\nused for recovering is:\n˜hq\nz = f(hz,gq), (7)\nwhere the functionf(·,·)is implemented as a nor-\nmalized 2-layer feed-forward network, and the de-\ntails are illustrated in AppendixB.2.\n3.4 Optimization\nAs the deﬁnition in Equation1,g i v e na and b as\nthe index of the original preceding and following\nsentences of the query\nsq in S, the corresponding\nprobabilities for surrounding sentences are formu-\nlated as:\n⎧\n⎪⎪⎪⎪\n⎨\n⎪⎪\n⎪\n⎪\n⎩\np\nssp(a|q,S)= exp(oa\nq )\n∑n\nj=1,j /∈{b,q} exp(oj\nq)\n,\npssp(b|q,S)= exp(ob\nq)\n∑n\nj=1,j /∈{a,q} exp(oj\nq)\n.\n(8)\nThe objective of SSP is subsequently deﬁned as:\nLssp = E(− 1\n|Q|\n∑\nq\n(log pssp(a|q,S)+\nlogpssp(b|q,S))).\n(9)\n6The details are illustrated in AppendixB.1.\nAs for RMLM, supposing the index set of masked\ntokens in querysq is Zq, and the set of correspond-\ning original tokens isXq, the probability for recov-\nering a masked token is:\nprmlm(xz|z,q, S)= exp(e(xz)⊤ ˜hq\nz)∑\nx′ exp(e(x′)⊤ ˜hq\nz)\n, (10)\nwhere z ∈Z q,xz ∈X q, x′is a token in vocabu-\nlary, and e(x) denotes the word embedding ofx.\nThen the objective of RMLM is:\nLrmlm = E(−\n∑\nq\n∑\nz logppmlm(xz|z,q, S)∑\nq |Zq| ).\n(11)\nDuring pre-training, the model tries to optimize\nthe two objectives jointly:\nL = Lssp +Lrmlm. (12)\n3.5 Fine-tuning\nDuring ﬁne-tuning, the input contains a query sen-\ntence and a passage. For multiple choice QA tasks,\nwe concatenate a question with an option to form\na question-option pair and use it as a whole query.\nIn this section, we useq =0 to represent the index\nof the query and the sentences of passage are kept\nin their original order. The input sequence can be\nthus denoted as:\nS =[ sq,s1,s2,··· ,sn].\nTo inherit the evidence extraction ability aug-\nmented during pre-training, we incorporate the\nsame retrieval operation into ﬁne-tuning to collect\nclues from the passage. Firstly, we reuse the at-\ntention mechanism deﬁned in Equation3 to obtain\nthe query representationvq. As for the evidence\nextraction process, we formulate it differently for\nMultiple Choice QA and Span Extraction.\n3.5.1 Multiple Choice QA\nSimilar to Equation4, we adopt an attention mech-\nanism, whereby the query-aware sentence represen-\ntation ui\nq is obtained via gathering evidence from\neach sentence:\nui\nq\n⊤\n= Att(vq⊤ ,Hi,Hi),i ̸= q. (13)\nAnd the ﬁnal passage representation highlighting\nthe evidence can be obtained via the sentence-level\nevidence extraction:\nvp = Att(vq⊤ ,U,U), (14)\n155\nwhere U =[ u1\nq,··· ,un\nq ] and U ∈ Rd×n. Finally,\nwe represent the probability of each optionc using\nboth the queryvq and the passagevp:\npmc\nc ∝ exp(W6(tanh(W5[vq;vp]+ b5))+ b6).\n(15)\nSpeciﬁcally, for Multi-RC, since the number of\ncorrect answer options for each question is uncer-\ntain, the task is often treated as a binary classiﬁca-\ntion problem for each option. As a result, we adopt\na MLP to get the probability of whether an option\nc is correct:\npmc\nc = σ(W8(tanh(W7[vq;vp]+ b7))+ b8),\n(16)\nwhere σ is thesigmoid function.\n3.5.2 Span Extraction\nSince answer spans are often consistent with corre-\nsponding evidences, we directly leverage the query\nto extract relevant spans. The probability of select-\ning start positionsand end positioneof an answer\nspan is given by:\n{\npspan\ns ∝ exp(vq⊤ W9hs),\npspan\ne ∝ exp(vq⊤ W10he).\n(17)\n4 Experiment\n4.1 Dataset\n4.1.1 Multiple Choice Question Answering\nDREAM (Sun et al., 2019) contains 10,197 multi-\nple choice questions for 6,444 dialogues collected\nfrom English Examinations designed by human\nexperts, in which 85% of the questions require rea-\nsoning across multiple sentences, and 34% of the\nquestions also involve commonsense knowledge.\nRACE (Lai et al., 2017) is a large-scale reading\ncomprehension dataset collected from English Ex-\naminations and created by domain experts to test\nstudents’ reading comprehension skills. It has a\nwide variety of question types, e.g., summariza-\ntion, inference, deduction and context matching,\nand requires complex reasoning techniques.\nMulti-RC\n(Khashabi et al., 2018) is a dataset of\nshort paragraphs and multi-sentence questions. The\nnumber of correct answer options for each question\nis not pre-speciﬁed and the correct answer(s) is not\nrequired to be a span in the text. Moreover, the\ndataset provides annotated evidence sentence.\nReClor\n(Y u et al., 2020) is extracted from logical\nreasoning questions of standardized graduate ad-\nmission examinations. Existing studies show that\nthe state-of-the-art models perform poorly on Re-\nClor, indicating the deﬁciency of logical reasoning\nability of current PLMs.\n4.1.2 Span Extraction\nHotpot QA (Y ang et al., 2018) is a question an-\nswering dataset involving natural and multi-hop\nquestions. The challenge contains two settings, the\ndistractor setting and the full-wiki setting. In this\npaper, we focused on the full-wiki setting, where\nthe system should retrieve the relevant paragraphs\nfrom Wikipedia and then predict the answer.\nSQuAD2.0 (Rajpurkar et al., 2018b) is reading\ncomprehension dataset, consisting of questions\nposed by crowdworkers on a set of Wikipedia arti-\ncles, where the answer to every question is a seg-\nment of text, or span, from the corresponding read-\ning passage, or the question might be unanswer-\nable.\n4.2 Implementation Detail\nWe leave the details about the implementation and\npre-training corpora in AppendixA due to the limi-\ntation of space.\n4.3 Baseline\nSince our method is used for further pre-\ntraining, we mainly compared our model with\nBERT/RoBERTa and their variants. For Hotpot\nQA, we integrated our models into an open-sourced\nand well-accepted system (Asai et al., 2020) and\nevaluated the performance. The details of baselines\nare summarized as follows:\n4.3.1 Multiple Choice QA\nBERT is the BERT-base model with 2-layer MLP\nas the task-speciﬁc module.\nBERT-Q& RoBERTa-Q refer to the designed but\nnot further trained models, which include an extra\nmulti-head attention for generating query represen-\ntation via Equation3, and our retrieval operation\nfor evidence extraction as in §3.5.1 and §3.5.2.\nBERT-Q w. R/S & RoBERTa-Q w. R/S refer\nto the designed models further trained with our\nproposed SSP and RMLM tasks (denoted asS and\nR, respectively).\nBERT-Q w. R& BERT-Q w. Srefer to the mod-\nels further trained with only one pre-training task,\nRMLM or SSP .\nBERT-Q w. M\n& BERT w. Mrefer to the models\nfurther trained with MLM. For fair comparison, we\n156\nRACE DREAM ReClor Multi-RC\nModel / Dataset Dev Test Dev Test Dev Test Dev\nAcc. Acc. Acc. Acc. Acc. Acc. EM F1 a F1m\nBERT-base† – 65.0 63.4 63.2 54.6 47.3 –––\nBERT w. M 67.7 66.3 62.9 63.2 51.6 45.1 26.6 71.8 74.2\nBERT-Q 67.2 65.2 62.9 62.3 48.4 45.0 22.8 69.6 72.0\nBERT-Q w. M 67.7 66.9 61.8 62.2 48.8 48.3 23.8 70.1 72.6\nBERT-Q w. R 65.5 64.7 59.0 58.6 46.8 45.1 26.4 71.5 74.0\nBERT-Q w. S 69.5 66.5 64.8 62.2 52.0 46.5 30.0 73.0 75.8\nBERT-Q w. R/S 70.1 68.1 64.4 64.0 50.6 49.2 31.9 73.8 76.3\nRoBERTa-base 76.0 75.5 71.2 69.8 54.8 50.8 38.7 77.1 79.2\nRoBERTa-Q 76.8 75.7 70.9 69.5 56.0 49.7 34.6 75.4 77.4\nRoBERTa-Q w. R/S 77.1 74.9 70.9 70.8 54.8 50.3 40.4 77.6 80.0\nTable 1: Results on multiple choice question answering tasks. (F1a: F1 score on all answer-options; F1m: macro-\naverage F1 score of all questions.) We ran all experiments usingfour different random seeds with the same hyper-\nparameters, and report the average performance, except for ReClor and Multi-RC. For ReClor, we submitted the\nbest model on the development set to the leaderboard to get the results on the test set. For MultiRC, we merely\nreported the performance on development set since the test set is unavailable.†: The results are reported by the\nleaderboard.\nfurther train BERT with the same Wikipedia corpus\nfor equivalent steps.\n4.3.2 Hotpot QA\nFor hotpot QA, we constructed the system based\non Graph-based Recurrent Retriever (Asai et al.,\n2020), which includes a retriever and a reader based\non BERT. We simply replaced the reader with our\nmodels and evaluated their performance in compar-\nison with several published strong baselines on the\nleaderboard7.\n5 Results and Analyses\n5.1 Results for Multiple Choice QA\nTable 1 shows the results of the baselines and our\nmethod on multiple choice question answering.\nFrom Table1, we can observe that: 1) Compared\nwith BERT-Q and BERT, our method signiﬁcantly\nimproves the performance over all the datasets,\nwhich validates the effectiveness of our proposed\npre-training method. 2) As for the model structure,\nBERT-Q obtains similar or worse results compared\nwith BERT, which suggests that the retrieval opera-\ntion can hardly improve the performance without\nspecialised pre-training. 3) Taking the rows of\nBERT, BERT-Q, BERT w. M, BERT-Q w. M for\ncomparison, the models with further pre-training\nusing MLM achieve similar or slightly higher per-\nformance. The results show that further training\nBERT using MLM and the same corpus can only\nachieve very limited improvements. 4) Regarding\n7https://hotpotqa.github.io/.\nthe two pre-training tasks, BERT-Q w. R/S leads\nto similar performance on the development sets\ncompared with BERT-Q w. S, but a much higher\naccuracy on the test sets, which suggests RMLM\ncan help to maintain the effectiveness of contex-\ntual language representation. However, there is a\nsigniﬁcant degradation over all datasets for BERT-\nQ w. R. The main reason is possibly because the\nmodel cannot tolerate the sentence shufﬂing noise,\nwhich may lead to the discrepancy between pre-\ntraining and MRC, and thus need to be alleviated\nthrough SSP . And 5) considering the experiments\nover RoBERTa-based models, RoBERTa-Q w. R/S\noutperforms RoBERTa-Q and RoBERTa-base with\nconsiderable improvements over Multi-RC and the\ntest set of DREAM, which also indicates that our\nmethod can beneﬁt stronger PLMs.\n5.2 Performance on Span Extraction QA\nThe results of span extraction on Hotpot QA are\nshown in Table2. We constructed the system using\nthe Graph Recurrent Retriever (GRR) proposed by\nAsai et al.(2020) and different readers. As shown\nin the table, GRR + BERT-Q w. R/S outpeforms\nGRR + BERT-base by more than 2.5% absolute\npoints on both EM and F1. And GRR + RoBERTa-\nQ w. R/S also achieves a signiﬁcant improvement\nover GRR + RoBERTa-base. During the test stage,\nour best system, GRR + RoBERTa-Q w. R/S per-\nforms better than the strong baselines and get closer\nto GRR + BERT-wwm-large. The above results\nstrongly demonstrate the effectiveness of our pre-\n157\nModel / Dataset Dev Test\nEM F1 EM F1\nTransformer-XH (Zhao et al., 2020) 54.0 66.2 51.6 64.7\nHGN (Fang et al., 2020) –– 56.7 69.2\nGRR + BERT-wwm-Large* 60.5 73.3 60.0 73.0\nGRR + BERT-base* 52.7 65.8 ––\nGRR + BERT-Q w. R/S 55.2 68.4 ––\nGRR + RoBERTa-base 56.8 69.6 ––\nGRR + RoBERTa-Q w. R/S 58.4 71.3 58.1 71.0\nTable 2: Results of our method and other strong base-\nlines on Hotpot QA.GRR means the Graph Recurrent\nRetriever proposed byAsai et al.(2020), GRR + BERT-\nbase means the system whose retriever is GRR and\nreader is built on BERT-base. *: The results are re-\nported byAsai et al.(2020).\nModel / Dataset EM F1\nBERT-Q 71.7 74.9\nBERT-Q w. R/S 77.2 80.4\nRoBERTa-Q 80.3 83.7\nRoBERTa-Q w. R/S 81.7 85.0\nTable 3: Results of our method and other baselines on\nthe dev set of SQuAD2.0.\ntraining method on the task requiring multi-hop\nevidence extraction and reasoning.\nBesides, we also conducted experiments on the\nmost common benchmark, SQuAD2.0. The results\non development set shown in Table3 have also ver-\niﬁed the effectiveness of our proposed pre-training\nmethod.\n5.3 Evaluation of Evidence Extraction\nTo evaluate the performance of our method for\nevidence extraction in the setting of implicit super-\nvision (with only answers), we ranked sentences in\na passage using their attention weights obtained in\nEquation 4 and chose those sentences with higher\nweights as the evidences.\nAs shown in Table4, the models with our pro-\nposed pre-training tasks obtain considerable im-\nprovements on the precision and recall of evidence\nextraction, which veriﬁes that our pre-training\nmethod is able to effectively equip PLMs with the\ncapability for gathering evidence without explicit\nsupervision. For a better illustration, we further\nprovided two examples in AppendixC.\n5.4 Effect of Different Masking Ratio During\nPre-training\nTable 5 shows the results of our model pre-trained\nwith different masking ratios. Due to the small\namount of entities contained in the document, we\nModel P@1 R@1 P@2 R@2\nBERT-Q 21.83 9.66 20.24 17.73\nBERT-Q w. R/S 45.30 20.38 38.51 34.55\nRoBERTa-Q 28.25 12.45 26.93 23.74\nRoBERTa-Q w. R/S 35.34 15.76 30.33 26.85\nTable 4: Results of evidence extraction on the develop-\nment set of Multi-RC.\nRACE Multi-RC\nModel/Dataset Dev Test Dev\nAcc. Acc. EM F1 a F1m\nB.Q w.R/S (30%) 70.1 68.1 31.9 73.8 76.3\nB.Q w.R/S (60%) 70.2 67.3 32.0 73.8 76.3\nB.Q w.R/S (90%) 70.4 68.2 31.0 73.5 76.2\nB.Q w.S (No Mask) 69.0 67.2 29.0 72.7 75.4\nTable 5: Results on RACE and Multi-RC using mod-\nels pre-trained with different mask ratios. B.Q means\nBERT-Q.\nonly consisdered the masking ratio of nouns as\nthe variable. Formally, we considered three ratios:\n30%, 60%, 90%, and an extra setting, where the\nentities and nouns are all kept and the RMLM task\nis also removed during pre-training.\nAs shown in the table, with more possible clues\nbeing masked, the model tend to obtain better re-\nsults on the downstream tasks. For example, BERT-\nQ w. R/S (90%) achieves the best accuracy on\nRACE, and BERT-Q w. R/S (60%) obtains the\nhighest performance over Multi-RC. And all mod-\nels that employ masking outperform BERT-Q w. S\n(no masking). The main reason can be that with\nmore explicit information short-cut being elimi-\nnated, it is more difﬁcult for models to collect po-\ntential clues, and PLMs are enhanced with stronger\nreasoning ability of evidence extraction. However,\nthere also exists a trade-off: as higher masking\nratio leads to more noise, it could worsen the mis-\nmatch between pre-training and ﬁne-tuning, and\ncause performance degradation, e.g., BERT-Q w.\nR/S (90%) performs the worst on Multi-RC.\n5.5 Performance in Low Resource Scenario\nFigure 3 depicts the performance of BERT-Q w.\nR/S on the development and test set of RACE with\nlimited training set. For each speciﬁc relative ra-\ntio, four reduced training sets are automatically\ngenerated using different random seeds and the cor-\nresponding accuracies are plotted on the ﬁgure. It\nis observed that with 70% training data, our model\noutperforms the baseline, BERT-Q, which was ini-\ntialized using BERT and has not been further pre-\n158\n67.2%\n65.2%\nBERT-Q  Dev\nBERT-Q Test\nBERT-Q w. R/S Dev\nBERT-Q w. R/S Test\nFigure 3: The accuracy of BERT-Q w. R/S on the de-\nvelopment and test of RACE. The horizontal axis refers\nto the ratioK of training data compared to the original\ntraining set.\ntrained. The results indicate that our method can\nhelp to reduce the amount of annotated training\ndata for downstream MRC tasks, which is espe-\ncially useful in low resource scenarios.\n6 Conclusion and Future Work\nIn this paper, we present a novel pre-training ap-\nproach, REPT, to bridge the gap between pre-\ntrained language models and machine reading com-\nprehension through retrieval-based pre-training.\nSpeciﬁcally, we design two retrieval-based pre-\ntraining tasks equipped with self-supervised learn-\ning, namely Surrounding Sentences Prediction\n(SSP) and Retreval based Masked Language Model-\ning (RMLM), to enhance PLMs with the capability\nof evidence extraction for MRC. The experiments\nover ﬁve different datasets validate the effective-\nness of our proposed method. In the future, we\nplan to extend the proposed pre-training approach\nto the more challenging open-domain settings.\n7 Acknowledgements\nThis work is supported by the National Key Re-\nsearch and Development Project of New Genera-\ntion Artiﬁcial Intelligence, No.:2018AAA0102502,\nand the Alibaba Research Intern Program of Al-\nibaba Group.\nReferences\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learn-\ning to retrieve reasoning paths over wikipedia graph\nfor question answering.I n ICLR.\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E.\nHinton. 2016. Layer normalization . CoRR,\nabs/1607.06450.\nHyung Won Chung, Thibault Fevry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2021. Re-\nthinking embedding coupling in pre-trained lan-\nguage models.I n ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding.I n NAACL, pages 4171–4186. ACL.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs.I n NAACL,\npages 2368–2378. ACL.\nY uwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuo-\nhang Wang, and Jingjing Liu. 2020. Hierarchical\ngraph network for multi-hop question answering.I n\nEMNLP, pages 8823–8838. ACL.\nY angyang Guo, Zhiyong Cheng, Liqiang Nie, Yibing\nLiu, Yinglong Wang, and Mohan Kankanhalli. 2019.\nQuantifying and alleviating the language prior prob-\nlem in visual question answering.I n SIGIR, pages\n75–84.\nY angyang Guo, Liqiang Nie, Zhiyong Cheng, Feng Ji,\nJi Zhang, and Alberto Del Bimbo. 2021. Adavqa:\nOvercoming language priors with adapted margin\ncosine loss. InIJCAI.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020.REALM: retrieval-\naugmented language model pre-training . CoRR,\nabs/2002.08909.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. DEBERTA: Decoding-\nenhanced bert with disentangled attention.I n ICLR.\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\nDan Iter, Kelvin Guu, Larry Lansing, and Dan Jurafsky.\n2020. Pretraining with contrastive sentence objec-\ntives improves discourse performance of language\nmodels.I n ACL, pages 4859–4870.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanbert: Improving pre-training by representing\nand predicting spans. TACL, 8:64–77.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nBeyond the Surface: A challenge set for reading\ncomprehension over multiple sentences.I n NAACL,\npages 252–262. ACL.\n159\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur P . Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural Questions: a benchmark for question an-\nswering research. TACL, 7:452–466.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Y ang,\nand Eduard H. Hovy. 2017.RACE: large-scale read-\ning comprehension dataset from examinations.I n\nEMNLP, pages 785–794. ACL.\nHaejun Lee, Drew A. Hudson, Kangwook Lee, and\nChristopher D. Manning. 2020. SLM: learning a\ndiscourse language representation with sentence un-\nshufﬂing.I n EMNLP, pages 1551–1562. ACL.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering.I n ACL, pages 6086–\n6096.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and V eselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization.I n ICLR.\nYilin Niu, Fangkai Jiao, Mantong Zhou, Ting Y ao, Jing-\nfang Xu, and Minlie Huang. 2020. A self-training\nmethod for machine reading comprehension with\nsoft evidence extraction.I n ACL, pages 3916–3927.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Y unfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nCoRR, abs/2003.08271.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018a.\nKnow what you don’t know: Unanswerable ques-\ntions for squad.I n ACL, pages 784–789.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018b.\nKnow what you don’t know: Unanswerable ques-\ntions for squad.I n ACL, pages 784–789.\nQiu Ran, Y ankai Lin, Peng Li, Jie Zhou, and Zhiyuan\nLiu. 2019. Numnet: Machine reading comprehen-\nsion with numerical reasoning.I n EMNLP-IJCNLP,\npages 2474–2484. ACL.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019. CoQA: A conversational question answering\nchallenge. TACL, 7:249–266.\nKai Sun, Dian Y u, Jianshu Chen, Dong Y u, Y ejin Choi,\nand Claire Cardie. 2019. DREAM: A challenge\ndataset and models for dialogue-based reading com-\nprehension. TACL, 7:217–231.\nY u Sun, Shuohuan Wang, Y u-Kun Li, Shikun Feng,\nHao Tian, Hua Wu, and Haifeng Wang. 2020.\nERNIE 2.0: A continual pre-training framework\nfor language understanding.I n AAAI, pages 8968–\n8975.\nAlexandre Tamborrino, Nicola Pellican`o, Baptiste Pan-\nnier, Pascal V oitot, and Louise Naudin. 2020.\nPre-\ntraining is (almost) all you need: An application to\ncommonsense reasoning.I n ACL, pages 3878–3887.\nACL.\nAlex Tamkin, Trisha Singh, Davide Giovanardi, and\nNoah D. Goodman. 2020.Investigating transferabil-\nity in pretrained language models.I n EMNLP: Find-\nings, pages 1393–1401. ACL.\nY uval V arkel and Amir Globerson. 2020.Pre-training\nmention representations in coreference models.I n\nEMNLP, pages 8534–8540. ACL.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.I n NeurIPS, pages 5998–6008.\nShuohang Wang, Y uwei Fang, Siqi Sun, Zhe Gan,\nY u Cheng, Jingjing Liu, and Jing Jiang. 2020.\nCross-thought for sentence encoder pre-training.I n\nEMNLP, pages 412–421. ACL.\nShuohang Wang, Mo Y u, Xiaoxiao Guo, Zhiguo Wang,\nTim Klinger, Wei Zhang, Shiyu Chang, Gerry\nTesauro, Bowen Zhou, and Jing Jiang. 2018. R3:\nReinforced ranker-reader for open-domain question\nanswering.I n AAAI, pages 5981–5988.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. CoRR, abs/1910.03771.\nAn Y ang, Quan Wang, Jing Liu, Kai Liu, Y ajuan Lyu,\nHua Wu, Qiaoqiao She, and Sujian Li. 2019a.En-\nhancing pre-trained language representations with\nrich knowledge for machine reading comprehension.\nIn ACL, pages 2346–2357. ACL.\nZhilin Y ang, Zihang Dai, Yiming Y ang, Jaime G.\nCarbonell, Ruslan Salakhutdinov, and Quoc V . Le.\n2019b. Xlnet: Generalized autoregressive pretrain-\ning for language understanding.I n NeurIPS, pages\n5754–5764.\nZhilin Y ang, Peng Qi, Saizheng Zhang, Y oshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. HotpotQA: A\ndataset for diverse, explainable multi-hop question\nanswering.I n EMNLP, pages 2369–2380. ACL.\nDeming Y e, Y ankai Lin, Jiaju Du, Zhenghao Liu, Peng\nLi, Maosong Sun, and Zhiyuan Liu. 2020. Coref-\nerential reasoning learning for language representa-\ntion.I n EMNLP, pages 7170–7186. ACL.\n160\nWeihao Y u, Zihang Jiang, Y anfei Dong, and Jiashi\nFeng. 2020. ReClor: A reading comprehension\ndataset requiring logical reasoning.I n ICLR.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: en-\nhanced language representation with informative en-\ntities.I n ACL, pages 1441–1451. ACL.\nChen Zhao, Chenyan Xiong, Corby Rosset, Xia\nSong, Paul N. Bennett, and Saurabh Tiwary. 2020.\nTransformer-XH: Multi-evidence reasoning with ex-\ntra hop attention.I n ICLR.\n161\nA Implementation Detail\nWe built our model on Huggingface’s Pytorch trans-\nformer repository (Wolf et al., 2019), and used\nAdamW (Loshchilov and Hutter, 2019) as the opti-\nmizer. We used the pre-trained BERT-base-uncased\nand RoBERTa-base checkpoint to initialize our en-\ncoder, and performed pre-training using 16 P100\nGPUs simultaneously. The pre-training processes\nlast around 16 hours for BERT and 4 days for\nRoBERTa, which takes 20,000 steps and 80,000\nsteps with the batch size as 512, respectively. All\nhyper-parameters can be found in Table6 for pre-\ntraining and Table7 for ﬁne-tuning.\nDuring constructing the training sample for pre-\ntraining, we controlled the masking ratio for entity\nand noun in query. For BERT, we masked 90%\nentities and 30% nouns. For RoBERTa, we con-\nstructed two datasets, where the masking ratios for\nentity and noun are set to 90%, 30% and 90%, 90%,\nrespectively. And we mixed the two for jointly\ntraining. We also explored the effect of different\nmasking ratios and the analysis is detailed in §5.\nAs for the ﬁne-tuning stage, for multiple choice\nQA, we ran all experiments using for different ran-\ndom seeds (i.e., 33, 42, 57 and 67) and reported the\naverage performance, except for ReClor, in which\nwe only submitted the results obtained from the\nmodel which performs the best on development set\nto the leaderboard because the limitation of submis-\nsion times. For Hotpot QA, we mainly followed the\nhyper-parameters ofAsai et al.(2020) and thus did\nnot repeat the experiments using different random\nseeds. Due to the submission limitation, we only\nsubmitted our best model on the development set\nto the leaderboard and reported its performance on\ntest set.\nB The Details About Modeling\nB.1 Single-head Attention\nTo reduce the extra parameters introduced, we de-\nﬁne a single-head attention mechanism compared\nto the multi-head one. Given the query matrixQ,\nkey matrixK and value matrixV, the simple atten-\ntion mechanism is formualted as:\nAtt(Q,K,V)=s o f t m a x ( (QW +b)⊤ K)V,\nwhere W and b is the learnable parameters.\nB.2 Normalized Feed-forward Network\nWe adopt a 2-layer feed-forward network with\nGeLU activation (Hendrycks and Gimpel, 2016)\nand layer normalization (Ba et al., 2016) to predict\nthe masked entities and nouns. Following Span-\nBERT (Joshi et al., 2020), the Equation7 is decom-\nposed as:\n⎧\n⎨\n⎩\nh0 =[ hz;gq],\nh1 = LayerNorm(GeLU(W3h0 +b3)),\n˜hq\nz = LayerNorm(GeLU(W4h1 +b4)).\nC Case Study About Evidence\nExtraction\nIn §5.3, the results show that our pre-training\nmethod can augment the ability to extract the cor-\nrect evidence. To give an intuitive clariﬁcation over\nthis, we select two cases shown in Figure4.A sw e\ncan see, BERT-Q w. R/S and RoBERTa-Q w. R/S\ncan select the correct evidence sentences, while\nthe baselines models attend to the wrong sentences.\nBesides, Figure5 shows the attention maps of the\ntwo groups of comparison. It can be observed that\nour pre-training approach can help the model learn\na uniform attention distribution over the possible\nevidence sentences.\nD Analysis of Extra Parameters\nIntroduced\nFor fair comparison, we try to introduce as few\nadditional parameters as possible. Since the output\nlayer is highly task-speciﬁc and the single head-\nattention deﬁned in AppendixB.1 is simple, we\nmain analyze the extra parameters introduced for\nquery representation learning deﬁned in§3.3.1.A\nsingle layer of Transformer comprises of a multi-\nhead attention module and a feed-forward network.\nAs a result, the multi-head attention module gener-\nating the query representation has introduced 2.8%\nextra parameters compared with a 12-layer Trans-\nformer without consideration to the parameters in\nembedding layer and layer normalization.\n162\nHyperParam BERT-base RoBERTa-base\nPeak Learning Rate 2e-4 5e-5\nLearning Rate Decay Linear Linear\nBatch Size 512 512\nMax Steps 20,000 80,000\nWarmup Steps 2,000 4,000\nWeight Decay 0.01 0.01\nGradient Clipping 1.0 0.0\nAdam ϵ 1e-6 1e-6\nAdam β\n1 0.9 0.9\nAdam β2 0.999 0.98\nMax Sequence Length 512 512\nQuery Generator Dropout 0.1 0.1\nSSP Dropout 0.1 0.1\nRMLM Dropout 0.1 0.1\nFP16 option level O2 O2\nTable 6: Hyper-parameters for pre-training.\nHyperParam RACE DREAM ReClor MultiRC Hotpot QA\nPeak Learning Rate 4e-5 ♣/2e-5♠ 3e-5♣/2e-5♠ 2e-5♣/1e-5♠ 3e-5 5e-5 ♣/3e-5♠\nLearning Rate Decay Linear Linear Linear Linear Linear\nBatch Size 32 ♣/16♠ 24 24 32 32 ♣/48♠\nEpoch 4 8 10 8.0 3 ♣/4♠\nWarmup Proportion 0.1 ♣/0.06♠ 0.1 0.1 0.1 0.1\nWeight Decay 0.01 0.01 0.01 0.01 0.01\nAdam ϵ 1e-6 1e-6 1e-6 1e-6 1e-6 ♣/1e-8♠\nAdam β1 0.9 0.9 0.9 0.9 0.9\nAdam β2 0.999♣/0.98♠ 0.999♣/0.98♠ 0.999♣/0.98♠ 0.999 0.999\nGradient Clipping 1.0 ♣/0.0♠ 0.0♣/5.0♠ 0.0 1.0 0.0\nMax Sequence Length 512 512 256 512 384 ♣/386♠\nMax Query Length 128 512 256 512 64\nDropout 0.1 0.1 0.1 0.1 0.1\nTable 7: Hyper-parameters for ﬁne-tuning.♣: Hyper-parameters for BERT-based models.♠: Hyper-parameters\nfor RoBERTa-based models.\n163\nCase 1\nPassage:\n(0)A group of researchers at a remote jungle island outpost discover the natives are practicing voodoo and black magic. … (4)She returns years\nlater as an adult with a group of mercenaries to attempt to uncover what happe ned to her parents. (5)Shortly after arriving at the island their boat's\nengine dies, stranding them. (6)Meanwhile elsewhere on the island a trio of hikers discover a cave, the same cave leading to the underground\ntemple where the original curse was created. (7)After accidentally revi ving the curse, the dead once again return to kill any who trespass on their\nisland. (8)The mercenaries encounter their first zombie, who injures a member of the team. (9)Taking shelter in the remains of the old research\nfacilities medical quarters they are soon joined by Chuck, the only survi ving hiker. (10)Arming themselves with weapons left behind by the long\ndead research team, they make their stand as the dead once again rise. (11)One by one they are injured or killed, one of whom sacrifices himself\nto blow up the medical facility and his newly undead team members. (12)Jenny and Chuck flee, the only survivors remaining. (13)They stumble\nupon the cave once again, where the zombies appear and attack.\nQuestion: Where did Chuck find weapons?\nOption: From the previous research team.\nSentences Used: 9, 10.\nBERT-Q:                 Answer: False    Evidence: 0\nBERT-Q w. R/S:     Answer: True     Evidence: 10, 9\nCase 2\nPassage:\n(0)The film opens with Sunita, a medical student , and her friends working on a project about the human brain. (1)She wants to investigate the\ncurious case of Sanjay Singhania, a notable city businessman, who is repo rted to have anterograde amnesia. (2)Her professor denies access to\nSanjay's records as it is currently under criminal investigation. (3)Sunita, nonetheless, decides to inves tigate the matter herself. (4)Sanjay is\nintroduced as he brutally murders a man. (5)He takes a Polaroid picture of the man, and writes on it ``done''. (6)It is revealed that Sanjay has\nanterograde amnesia where he loses his memory every 15 minutes. (7) Sanjay uses a system of photographs, notes, and tattoos on his body to\nrecover his memory after each cycle. (8)It is revealed that Sanjay is ultimately out to avenge the death of his sweetheart Kalpana , and that he is\nsystematically killing the people who were responsible for it. (9)His main target is ``Ghajini'', a notable social personality in the city. (10)Poli ce\nInspector Arjun Yadav, on the case of the serial murders, tracks Sanjay down to his flat and attacks and disables him. (11)Yadav finds two diaries\nwhere Sanjay has chronicled the events of 2005 and 2006. …\nQuestion: Who denies Sunita access to Sanjay's records, who is reported to have anterograde amnesia, because they are under criminal \ninvestigation? \nOption: Sunita's professor&Arjun Yadav.\nSentences Used: 1, 2.\nRoBERTa-Q:                 Answer: False    Evidence: 0\nRoBERTa-Q w. R/S:     Answer: True     Evidence: 2, 1\nFigure 4: Two cases from the development set of Multi-RC.\n(a) Normalized attention weights for Case 1 in Figure 4.\n(b) Normalized attention weights for Case 2 in Figure 4.\nFigure 5: Two cases of the normalized attention\nweights of evidence extraction.",
  "topic": "Bridging (networking)",
  "concepts": [
    {
      "name": "Bridging (networking)",
      "score": 0.8486936688423157
    },
    {
      "name": "Computer science",
      "score": 0.8030385375022888
    },
    {
      "name": "Natural language processing",
      "score": 0.612130343914032
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5944273471832275
    },
    {
      "name": "Reading comprehension",
      "score": 0.5917962789535522
    },
    {
      "name": "Comprehension",
      "score": 0.5609768629074097
    },
    {
      "name": "Bridge (graph theory)",
      "score": 0.557848334312439
    },
    {
      "name": "Language model",
      "score": 0.5278137922286987
    },
    {
      "name": "Machine learning",
      "score": 0.39374080300331116
    },
    {
      "name": "Reading (process)",
      "score": 0.36541908979415894
    },
    {
      "name": "Linguistics",
      "score": 0.10124513506889343
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I80143920",
      "name": "Shandong University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    }
  ],
  "cited_by": 8
}