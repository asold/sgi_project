{
  "title": "Advanced deep learning and large language models: Comprehensive insights for cancer detection",
  "url": "https://openalex.org/W4408785507",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2656352879",
      "name": "Yassine Habchi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2187790822",
      "name": "Hamza Kheddar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2999988285",
      "name": "Yassine Himeur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A660988112",
      "name": "Adel Belouchrani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A323870348",
      "name": "Erchin Serpedin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2294938877",
      "name": "Fouad Khelifi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3010356274",
      "name": "Muhammad E. H. Chowdhury",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3014269620",
    "https://openalex.org/W4387422012",
    "https://openalex.org/W3204312334",
    "https://openalex.org/W4402664711",
    "https://openalex.org/W3105613063",
    "https://openalex.org/W6860130056",
    "https://openalex.org/W4200323263",
    "https://openalex.org/W4401075392",
    "https://openalex.org/W4387702753",
    "https://openalex.org/W4393247259",
    "https://openalex.org/W6776803797",
    "https://openalex.org/W4310496604",
    "https://openalex.org/W6852037750",
    "https://openalex.org/W4312204690",
    "https://openalex.org/W4392487628",
    "https://openalex.org/W3183452672",
    "https://openalex.org/W6861712363",
    "https://openalex.org/W4385563491",
    "https://openalex.org/W4388020455",
    "https://openalex.org/W4308784174",
    "https://openalex.org/W4391025999",
    "https://openalex.org/W4394941837",
    "https://openalex.org/W4399367209",
    "https://openalex.org/W4394755367",
    "https://openalex.org/W4396536011",
    "https://openalex.org/W6860687810",
    "https://openalex.org/W6692790205",
    "https://openalex.org/W3087507349",
    "https://openalex.org/W4386929739",
    "https://openalex.org/W4392520145",
    "https://openalex.org/W6873424231",
    "https://openalex.org/W6853768588",
    "https://openalex.org/W3111295039",
    "https://openalex.org/W3192519463",
    "https://openalex.org/W6803806542",
    "https://openalex.org/W4367303100",
    "https://openalex.org/W4400843441",
    "https://openalex.org/W4401374964",
    "https://openalex.org/W6871261865",
    "https://openalex.org/W4398250954",
    "https://openalex.org/W4367183221",
    "https://openalex.org/W2936503027",
    "https://openalex.org/W2995454717",
    "https://openalex.org/W6761477394",
    "https://openalex.org/W4310852955",
    "https://openalex.org/W4400447718",
    "https://openalex.org/W6860775412",
    "https://openalex.org/W4292198371",
    "https://openalex.org/W6789620883",
    "https://openalex.org/W3096948214",
    "https://openalex.org/W3154846286",
    "https://openalex.org/W6787505888",
    "https://openalex.org/W4402916808",
    "https://openalex.org/W4399204497",
    "https://openalex.org/W4225404354",
    "https://openalex.org/W6851133557",
    "https://openalex.org/W3005004133",
    "https://openalex.org/W6857453420",
    "https://openalex.org/W4308702916",
    "https://openalex.org/W4379982863",
    "https://openalex.org/W6852380414",
    "https://openalex.org/W3208181060",
    "https://openalex.org/W6848906209",
    "https://openalex.org/W4392283342",
    "https://openalex.org/W6840802379",
    "https://openalex.org/W6791493996",
    "https://openalex.org/W6801838286",
    "https://openalex.org/W4385302721",
    "https://openalex.org/W4403635653",
    "https://openalex.org/W6869781069",
    "https://openalex.org/W6856376549",
    "https://openalex.org/W6993021106",
    "https://openalex.org/W2775469102",
    "https://openalex.org/W4382449860",
    "https://openalex.org/W6861152264",
    "https://openalex.org/W6754250128",
    "https://openalex.org/W2967573033",
    "https://openalex.org/W6860837012",
    "https://openalex.org/W6869905375",
    "https://openalex.org/W4387004670",
    "https://openalex.org/W4211126909",
    "https://openalex.org/W6848208804",
    "https://openalex.org/W2919412094",
    "https://openalex.org/W2971725207",
    "https://openalex.org/W3205365855",
    "https://openalex.org/W2973274053",
    "https://openalex.org/W4281769084",
    "https://openalex.org/W6853868262",
    "https://openalex.org/W6862932626",
    "https://openalex.org/W6862340589",
    "https://openalex.org/W6866111562",
    "https://openalex.org/W4367837660",
    "https://openalex.org/W6850066086",
    "https://openalex.org/W6807951185",
    "https://openalex.org/W4386012856",
    "https://openalex.org/W4389863652",
    "https://openalex.org/W4386806239",
    "https://openalex.org/W6862135376",
    "https://openalex.org/W6859976899",
    "https://openalex.org/W4382465016",
    "https://openalex.org/W4400410010",
    "https://openalex.org/W6862408828",
    "https://openalex.org/W6862109438",
    "https://openalex.org/W6856121276",
    "https://openalex.org/W6860252737",
    "https://openalex.org/W6858509788",
    "https://openalex.org/W6861280168",
    "https://openalex.org/W4393969756",
    "https://openalex.org/W4375929067",
    "https://openalex.org/W6780159769",
    "https://openalex.org/W6853894229",
    "https://openalex.org/W4308553718",
    "https://openalex.org/W4386014405",
    "https://openalex.org/W6806183133",
    "https://openalex.org/W4389452301",
    "https://openalex.org/W6861819087",
    "https://openalex.org/W4396853172",
    "https://openalex.org/W3182336762",
    "https://openalex.org/W4400350642",
    "https://openalex.org/W4400411511",
    "https://openalex.org/W6869542652",
    "https://openalex.org/W6869923148",
    "https://openalex.org/W6871090982",
    "https://openalex.org/W4385337322",
    "https://openalex.org/W4387556984",
    "https://openalex.org/W4282982312",
    "https://openalex.org/W6855310028",
    "https://openalex.org/W6842494223",
    "https://openalex.org/W4205942412",
    "https://openalex.org/W3197116488",
    "https://openalex.org/W4211119275",
    "https://openalex.org/W3004902522",
    "https://openalex.org/W2928842276",
    "https://openalex.org/W3163936961",
    "https://openalex.org/W4284680265",
    "https://openalex.org/W4296995864",
    "https://openalex.org/W4382402870",
    "https://openalex.org/W4389741118",
    "https://openalex.org/W6784018549",
    "https://openalex.org/W2955805844",
    "https://openalex.org/W4391216267",
    "https://openalex.org/W3009928129",
    "https://openalex.org/W2986279317",
    "https://openalex.org/W4283742965",
    "https://openalex.org/W4306916628",
    "https://openalex.org/W4388642365",
    "https://openalex.org/W3148908072",
    "https://openalex.org/W6848882217",
    "https://openalex.org/W4385421647",
    "https://openalex.org/W6861240019",
    "https://openalex.org/W4391887075",
    "https://openalex.org/W4394579841",
    "https://openalex.org/W6804420085",
    "https://openalex.org/W4220777209",
    "https://openalex.org/W6851992689",
    "https://openalex.org/W4381093790",
    "https://openalex.org/W4384300005",
    "https://openalex.org/W4387376188",
    "https://openalex.org/W6809225160",
    "https://openalex.org/W4303628775",
    "https://openalex.org/W6841560160",
    "https://openalex.org/W4311694002",
    "https://openalex.org/W4286437542",
    "https://openalex.org/W4360994757",
    "https://openalex.org/W6863460575",
    "https://openalex.org/W6854420318",
    "https://openalex.org/W4365135358",
    "https://openalex.org/W6865607608",
    "https://openalex.org/W4311423536",
    "https://openalex.org/W4293030607",
    "https://openalex.org/W4313484965",
    "https://openalex.org/W6839070709",
    "https://openalex.org/W4386303561",
    "https://openalex.org/W6846996263",
    "https://openalex.org/W4282582687",
    "https://openalex.org/W6869556045",
    "https://openalex.org/W4320712817",
    "https://openalex.org/W4310255585",
    "https://openalex.org/W6862147626",
    "https://openalex.org/W4308333450",
    "https://openalex.org/W6863065226",
    "https://openalex.org/W4293659888",
    "https://openalex.org/W4309566957",
    "https://openalex.org/W4307843183",
    "https://openalex.org/W6810003034",
    "https://openalex.org/W6876091198",
    "https://openalex.org/W6858151278",
    "https://openalex.org/W4393928610",
    "https://openalex.org/W6863316860",
    "https://openalex.org/W6870919079",
    "https://openalex.org/W6869530227",
    "https://openalex.org/W6858107118",
    "https://openalex.org/W6870892895",
    "https://openalex.org/W6865928609",
    "https://openalex.org/W4386932783",
    "https://openalex.org/W4391175800",
    "https://openalex.org/W6858676058",
    "https://openalex.org/W4392022584",
    "https://openalex.org/W2951402759",
    "https://openalex.org/W3028161460",
    "https://openalex.org/W6756504730",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W6759999440",
    "https://openalex.org/W2957927798",
    "https://openalex.org/W6768281725",
    "https://openalex.org/W6838639484",
    "https://openalex.org/W6861317059",
    "https://openalex.org/W6777615688",
    "https://openalex.org/W4295535230",
    "https://openalex.org/W4399922492",
    "https://openalex.org/W4387490572",
    "https://openalex.org/W4403443437",
    "https://openalex.org/W4286470556",
    "https://openalex.org/W4388462051",
    "https://openalex.org/W4401753041",
    "https://openalex.org/W4231518865",
    "https://openalex.org/W4294591782",
    "https://openalex.org/W4403621441",
    "https://openalex.org/W4393987625",
    "https://openalex.org/W4393113071",
    "https://openalex.org/W4386310596",
    "https://openalex.org/W4400480180",
    "https://openalex.org/W4390833874",
    "https://openalex.org/W4384523324",
    "https://openalex.org/W4285118112",
    "https://openalex.org/W4392465065",
    "https://openalex.org/W4381744692",
    "https://openalex.org/W4400104551",
    "https://openalex.org/W4386275539",
    "https://openalex.org/W4386394413",
    "https://openalex.org/W4402561936",
    "https://openalex.org/W4391323066",
    "https://openalex.org/W4317940168",
    "https://openalex.org/W4390507093",
    "https://openalex.org/W4401226333",
    "https://openalex.org/W4241830422",
    "https://openalex.org/W4392543811",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4388487668",
    "https://openalex.org/W4248647617",
    "https://openalex.org/W4400068628",
    "https://openalex.org/W4391573229",
    "https://openalex.org/W3013377280",
    "https://openalex.org/W4399653104",
    "https://openalex.org/W4233182706",
    "https://openalex.org/W4362570503",
    "https://openalex.org/W4312287924",
    "https://openalex.org/W4400378587",
    "https://openalex.org/W4401056676",
    "https://openalex.org/W4230621011",
    "https://openalex.org/W2900702386",
    "https://openalex.org/W4237088529",
    "https://openalex.org/W4231952255",
    "https://openalex.org/W4391903936",
    "https://openalex.org/W4399118458",
    "https://openalex.org/W4399118499",
    "https://openalex.org/W4396826850",
    "https://openalex.org/W4404584849",
    "https://openalex.org/W4283707062",
    "https://openalex.org/W4246442863",
    "https://openalex.org/W4391620736",
    "https://openalex.org/W4399794237",
    "https://openalex.org/W4400509956",
    "https://openalex.org/W4221142221",
    "https://openalex.org/W4246988441",
    "https://openalex.org/W4389296056",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4221161782",
    "https://openalex.org/W3104130891",
    "https://openalex.org/W4234842379",
    "https://openalex.org/W4403837711",
    "https://openalex.org/W4210654312",
    "https://openalex.org/W4210409755",
    "https://openalex.org/W4396213874",
    "https://openalex.org/W4324291205",
    "https://openalex.org/W4244731318",
    "https://openalex.org/W4287322212"
  ],
  "abstract": null,
  "full_text": "Advanced Deep Learning and Large Language Models: Comprehensive\nInsights for Cancer Detection\nYassine Habchia, Hamza Kheddarb, Yassine Himeurc, Adel Belouchranid, Erchin Serpedine,\nFouad Khelifif and Muhammad E.H. Chowdhuryg\naInstitute of Technology, University Center Salhi Ahmed, Naama, Algeria\nbLSEA Laboratory, Electrical Engineering Department, Faculty of Technology, University of Medea, 26000, Algeria\ncCollege of Engineering and Information Technology, University of Dubai, Dubai, UAE\ndEcole Nationale Polytechnique/ LDCCP lab., El Harrach, Algiers, Algeria\neECEN Department, Texas A&M University, College Station TX 77843-3128 USA\nfDepartment of Computer Science and Digital Technologies, Engineering and Environment, Northumbria University at Newcastle\ngDepartment of Electrical Engineering, Qatar University, Doha 2713, Qatar\nARTICLE INFO\nKeywords:\nCancer diagnosis\nFederated learning\nTransfer learning\nReinforcement learning\nTransformer-based learning\nLarge language models.\nABSTRACT\nIn recent years, the rapid advancement of machine learning (ML), particularly deep learning (DL),\nhas revolutionized various fields, with healthcare being one of the most notable beneficiaries. DL has\ndemonstrated exceptional capabilities in addressing complex medical challenges, including the early\ndetection and diagnosis of cancer. Its superior performance, surpassing both traditional ML methods and\nhuman accuracy, has made it a critical tool in identifying and diagnosing diseases such as cancer. Despite\nthe availability of numerous reviews on DL applications in healthcare, a comprehensive and detailed\nunderstanding of DLâ€™s role in cancer detection remains lacking. Most existing studies focus on specific\naspectsofDL,leavingsignificantgapsinthebroaderknowledgebase.Thispaperaimstobridgethesegaps\nby offering a thorough review of advanced DL techniques, namely transfer learning (TL), reinforcement\nlearning (RL), federated learning (FL), Transformers, and large language models (LLMs). These cutting-\nedge approaches are pushing the boundaries of cancer detection by enhancing model accuracy, addressing\ndata scarcity, and enabling decentralized learning across institutions while maintaining data privacy. TL\nenables the adaptation of pre-trained models to new cancer datasets, significantly improving performance\nwith limited labeled data. RL is emerging as a promising method for optimizing diagnostic pathways and\ntreatment strategies, while FL ensures collaborative model development without sharing sensitive patient\ndata.Furthermore,TransformersandLLMs,traditionallyutilizedinnaturallanguageprocessing(NLP),are\nnowbeingappliedtomedicaldataforenhancedinterpretabilityandcontext-basedpredictions.Inaddition,\nthis review explores the efficiency of the aforementioned techniques in cancer diagnosis, it addresses key\nchallenges such as data imbalance, and proposes potential solutions. It aims to be a valuable resource\nfor researchers and practitioners, offering insights into current trends and guiding future research in the\napplication of advanced DL techniques for cancer detection.\n1. Introduction\nResearchers and clinicians face the formidable challenge of combating cancer, a leading cause of death globally. The World\nHealth Organization has issued warnings about the anticipated increase in cancer-related deaths if effective solutions are not\ndeveloped [1]. Consequently, early detection of cancer has become crucial for saving numerous lives. However, the traditional\nprocess of diagnosing cancer through manual inspection of medical images is error-prone and time-consuming, highlighting the\nurgentneedformoreaccurateandefficientdetectionmethods.Thecomputer-aideddiagnosis(CAD)systemshaveaidedphysicians\nbyenhancingtheaccuracyandefficiencyofmedicalimageanalysis,ataskwherefeatureextractioniscrucialandheavilyrelianton\nmachine learning (ML) techniques [2]. While various feature extraction methods have been explored for different types of cancer,\ntheseapproachesfaceinherentlimitationsthatMLstrivestoovercome.Deeplearning(DL),asubsetofMLalgorithms,hasgained\nsignificant prominence due to its layered structure, which makes it highly effective in medical fields, particularly in response to\nthe increasing patient numbers, rapid technological advancements, and the exponential growth of medical data [3]. The use of DL\nin medical diagnostics has broadened to cover a wide range of cancer types, including breast [4], lung [5], kidney [6], skin [7],\nleukemia[8],thyroid[9],braincancer[10],amongothers.Byleveragingitsmulti-layeredarchitecture,DLoutperformstraditional\nneural networks (Figure 1). The capability of DL algorithms to effectively extract diverse features from data underscores their\nimportance, particularly in cancer detection. The architecture of convolutional neural networks (CNN), a prominent class of DL,\nmirrors the structure of the human visual cortex and is composed of multiple layers, including convolutional, pooling, and fully\nconnected(FC)layers.CNNsexcelatlearningfeaturesdirectlyfromrawdatawithoutrequiringhumanintervention,allowingthem\nto develop hierarchical data representations that are highly valuable in image analysis [11].\nâ‹†\nDr. Hamza Kheddar is the corresponding author.\nhabchi@cuniv-naama.dz (Y. Habchi);kheddar.hamza@univ-medea.dz (H. Kheddar);yhimeur@ud.ac.ae (Y. Himeur);\nadel.belouchrani@g.enp.edu.dz (A. Belouchrani);eserpedin@tamu.edu (E. Serpedin);fouad.khelifi@northumbria.ac.uk (F. Khelifi);\nchowdhury@qu.edu.qa (M.E.H. Chowdhury)\nORCID(s):\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 1 of 40\narXiv:2504.13186v1  [eess.IV]  30 Mar 2025\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nMedical dataset\ncollection\nMedical\nimages\nMedical data cleaning\n- Noise removal\n- Contrast enhancement\n- Resize\nMedical dataset\nSegmentation\nEdge-based\nRegion based\nActive contours\nFeature extraction\nAsymmetry\nDiameters\nBorders\nCompactness\nMedical \ndataset\nTrain\ndataset\nTest\ndataset\nModel\ntraining\nTrained model\nPerformance\nevaluation\n[ACC, SEN,,,,]\nImage \nclassification\n[CNN, SVM,,,,]\nPredictions\n[normal,\nabnormal}\nX-ray, \nUltrasound, \nMRA,\nCT scan, MRI \nand PET scan\nRegion-based\nMorphological\nMedical data\npre-processing\nMedical data\nclassification\nUnderstand the problem\nAnd\nIdentify medical data\nSelect DL\nalgorithm\nModel \ntest \nOutput \nresults\nFigure 1:The general process of cancer detection via DL\nMany methods are designed to efficiently classify diseases despite data imbalance. For example, the ENTAIL framework [12]\ndemonstrates strong robustness on an unbalanced dataset, achieving high accuracy and sensitivity. However, limited dataset sizes\nandimbalanceddataareparticularlysignificantchallengesinthefieldofmedicalcancerresearch.Collectingcomprehensivecancer\ndatasetsisoftenconstrainedbyprivacyregulations,ethicalconsiderations,andthehighcostandcomplexityofmedicalprocedures.\nConsequently, the available datasets tend to be small, especially for rare types of cancer, which can hinder the development of\nrobust and generalizable ML models. This limitation can lead to overfitting, where models perform well on training data but fail to\ngeneralizetonew,unseendata.Additionally,imbalanceddataisaprevalentissue,ascertaintypesofcanceraremuchmorecommon\nthanothers.Thisimbalancecancausemodelstobecomebiasedtowardsthemajorityclass,reducingtheirabilitytoaccuratelydetect\nand diagnose less common cancers.\nToaddresstheseissues,techniquessuchasdataaugmentationwheresyntheticdatagenerationcanbeutilizedtoexpandthesize\nof training datasets, and transfer learning (TL) can be used [13, 14]. Moreover, federated learning (FL) offers a promising solution\nby enabling the training of ML models on decentralized datasets distributed across various institutions without the need to share\nsensitive patient data [15], This will encourage patients to accept participating in gathering their own private data. This approach\nnotonlypreservesprivacybutalsoincreasesthediversityofdatausedinmodeltraining,whichenhancesthemodelrobustnessand\ngeneralizability.Inaddition,reinforcementlearning(RL)canbeemployedinclinicaldecision-making,wheremodelsaretrainedto\noptimize treatment plans and diagnostic pathways by learning from their interactions with the environment [16, 17]. This adaptive\nlearning strategy allows the system to continuously improve and personalize recommendations based on real-time data.\nThe emergence of Transformer-based models and large language models (LLMs) has further advanced the fields of computer\nvisionandnaturallanguageprocessing(NLP)[18],andfoundapplicationsintheanalysisofmedicaldata[19].Unlikeconventional\nDL models such as CNNs and recurrent neural networkss (RNNs), which rely on localized feature extraction and sequential\ndependencies, Transformers introduce self-attention mechanisms that enable them to capture long-range dependencies and global\ncontextual relationships across large datasets. This capability is particularly crucial in cancer diagnostics, where heterogeneous\ndatasourcesâ€“includingmedicalimages,electronichealthrecords,andgenomicsequencesâ€“mustbeanalyzedholisticallytoimprove\ndiagnostic precision.\nTransformersexcelatprocessingsequentialdataandcanbeappliedtomedicalrecords,imagingdata,andgenomics,providing\nenhanced interpretability and context-aware predictions [20]. Furthermore, LLMs, with their ability to process vast amounts of\nunstructured medical text, extend these capabilities by enabling automated medical reasoning and decision support. By leveraging\nlarge-scale textual and multimodal data, LLMs facilitate tasks such as medical literature review, patient history analysis, and\nclinicaldecision-making,surpassingtraditionalmethodsthatprimarilyrelyonstructureddatainputs.Ontheotherside,traditional\nresamplingmethods,cost-sensitivelearning,andensembletechniquescanhelpmitigatetheeffectsofdataimbalance,ensuringthat\nML models are more accurate and reliable across different types of cancer. By effectively integrating advanced techniques such as\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 2 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nFL,RL,Transformers,andLLMsalongsidetraditionalmethods,theperformanceandgeneralizabilityofartificialintelligence(AI)\nmodels in cancer diagnosis and treatment can be significantly improved.\nDespite numerous reviews [3, 21], this coverage of ML in cancer detection often lacks depth, leading to an incomplete\nunderstandingofitschallenges.ThispaperproposestocoverthisgapbyofferingacomprehensivereviewofDLincancerdetection,\ndiscussingthekeychallenges,currentapplications,andmostrecentAImodels.Ourcontributionincludesanin-depthexamination\noftheaforementionedalgorithmsTL,RL,FL,andTransformers,whichareconsideredasthestate-of-the-artDLtools,andhighlight\ntheirkeyfeatures.Inaddition,thispaperoverviewsthecontributionsofDLinearlydetectionofcanceraswellasinimprovingthe\ndiagnosis and outcome prediction. Unlike other reviews that focus on specific aspects such as FL or TL [22, 23], this paper delves\nintoabroaderspectrum,includingemergingtechniqueslikeRL,visiontransformerss(ViTs),andLLMs.Whilesomereviewslimit\ntheirdiscussiontoparticularcancers,suchaslungorbreastcancer[24,25],thisreviewoffersathoroughanalysisofmultiplecancer\ntypes, showcasing the versatility of DL models across various domains. Additionally, other reviews such as [26] focuses solely on\nTransformers or omit certain advanced algorithms. In contrast, our review integrates these advanced algorithms alongside FL, RL,\nand LLMs, providing a more holistic view of their contributions to the detection of many types of cancer, includingskin, brain,\nthyroid, liver, kidney, pancreas, lung, leukemia, breast, cervical, ovarian, stomach, bladder, andcolorectal cancer. Furthermore,\nthis review addresses critical gaps by exploring challenges such as data privacy, model scalability, and dataset limitations, topics\nthat other reviews often overlook. By covering evaluation metrics, model performance, and the role of advanced architectures,\nour survey bridges the gaps in existing literature and offers valuable insights into the future potential of DL for improving cancer\ndiagnosis and treatment planning. Table 1 presents a summary of the topics covered, highlighting the similarities and differences\ncompared to existing reviews. It also identifies important topics often overlooked in the literature by current reviews, which our\npaper seeks to cover.\nTable 1: Comparison with existing advanced DL-based cancer diagnosis. The markersâ¬¥ and â¬¦ signify that a specific topic has\nbeen addressed or ignored, respectively.\nRef Year The similarity with our review Differentiate from our review DL RL FL TL ViT LLM\n[22] 2023\nThe work highlights FL for disease detection, em-\nphasizing its impact on diagnostic accuracy, data\nprivacy, and model effectiveness.\nThis paper explores, in a general manner, various models for\ndisease detection using FL alone. However in our paper, the\ndisease type is identified as cancer.\nâ¬¥ â¬¦ â¬¥ â¬¦ â¬¦ â¬¦\n[23] 2023\nThisstudyexaminesTLinmedicalimageanalysis,\ndetailing its methods, applications, source and tar-\nget data, and the use of public or private imaging\ndatasets.\nComparedtoourwork,itlacksexplorationofotheradvanced\nDL techniques and fails to highlight the significance of these\nmethods in cancer image analysis.\nâ¬¥ â¬¦ â¬¦ â¬¥ â¬¦ â¬¦\n[27] 2024\nIn this work, RL has emerged as a dynamic and\ntransformativeparadigminthefieldofAI,offering\nthe promise of intelligent decision making, espe-\ncially in robotics and healthcare.\nAlthough RL is discussed, this work does not review its ap-\nplicationinthecontextofcancerdiagnosis.Moreover,itonly\npresents a comparative study of RL algorithms, excluding\nothers like LLMs and ViTs.\nâ¬¥ â¬¥ â¬¦ â¬¦ â¬¦ â¬¦ .\n[28] 2024 The review offers a comprehensive analysis of the\nuse of advanced AI models in healthcare.\nThe survey emphasizes the evolution of LLMs and their\nperformancemetricsinthebiomedicaldomain,withoutmen-\ntioning the other methods used to detect cancer.\nâ¬¥ â¬¦ â¬¦ â¬¦ â¬¦ â¬¥\n[24] 2024\nThis review highlights the DL techniques for lung\ncancer diagnosis. This study emphasizes the high\nperformance of these models compared to tradi-\ntional methods.\nThe review focuses on DL techniques such as CNN for lung\ncancer. It covers public datasets and addresses challenges in\ndeploying models clinically, but did not examine the role of\nadvanced DL in the diagnosis of several type of cancers.\nâ¬¥ â¬¦ â¬¦ â¬¦ â¬¦ â¬¦\n[26] 2024 This work explores Transformer models in health-\ncare,focusingontheirapplicationtocomplexdata.\nThe provided work offers a broad overview types of Trans-\nformer applications in various healthcare settings, including\nsurgical outcomes and drug synthesis, without delving into\nspecific models or datasets related to cancer.\nâ¬¥ â¬¦ â¬¦ â¬¦ â¬¥ â¬¦\n[25] 2024 ThefocusisontheapplicationofDLtechniquesto\nbreast cancer imaging.\nThis work explores discusses only DL techniques for cancer\ndetectionandfocusesexclusivelyonapplicationsfordiagnos-\ning of the breast cancer.\nâ¬¥ â¬¦ â¬¦ â¬¦ â¬¦ â¬¦\n[29] 2024 This article illustrates the importance of ViT for\ncancer diagnosis.\nA comprehensive study on the application of ViTs in ad-\ndressing intractable diseases is lacking, and other relevant\nalgorithms have not been thoroughly explored either.\nâ¬¥ â¬¦ â¬¦ â¬¦ â¬¥ â¬¦\nOurs 2024\nThis paper reviews the DL tools for cancer diagno-\nsis by highlighting their key features and applica-\ntions.\nThis survey first presents the conventional DL tools. Our\nsurveyfirstpresentsthebackgroundofconventionalDLtech-\nniques, followed by advanced approaches for cancer diagno-\nsis,includingRL,FL,TL,LLMs,andViTs.Itexaminestheir\neffectiveness, challenges, datasets, evaluation metrics, and\npotential to enhance detection, classification, and treatment.\nâ¬¥ â¬¥ â¬¥ â¬¥ â¬¥ â¬¥\nThe remaining of the paper is structured to provide a comprehensive review of DL in cancer detection. Section 2 offers\nbackground information on DL fundamentals. Section 3 overviews various DL training modes. Section 4 delves into the most\ncommonly used advanced DL networks for cancer detection, namely TL, RL, FL, and Transformers. Section 5 discusses existing\ncomputational approaches. Section 6 addresses research challenges and future directions. Finally, Section 7 concludes this paper.\nFigure 2 illustrates the road-map structure of this review in terms of sections and subsections.\n2. Background\nThissectionpresentsthebasicsofDL,coveringthenecessarylayersforbuildingmodelsinmedicalimagerecognition,suchas\nconvolutional,pooling,andFClayers.Italsodiscussestheconceptsofactivationfunctionactivationfunctions(AFs),regularization,\nhyper-parameter optimization, training, and other essential components required to construct efficient DL models.\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 3 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nBackground \n- When and why to Apply DL \n- DL basics\n- Evaluation metrics and datasets\nDL learning \nmodes \n- Supervised\n- Semi-supervised\n- Unsupervised\nAdvanced DL \ntechniques\nComputational \napproaches \nResearch \nchallenges and \nfuture directions\n- Reinforcement learning\n- Federated learning\n- Transfer learning\n- Transformers\n- Large language model\n- CPU-based\n- GPU-based\n- FPGA\n- Challenges\n- Future directions\nFigure 2:Road-map outlining the structure of the review.\n2.1. When and why applying DL in the medical domain\nDL is applied in the medical field when addressing complex tasks that require advanced data processing and interpretation.\nOne key area is medical imaging, where DL models are used for classifying, segmenting, and detecting abnormalities in images,\nsuchasdetectingtumorsanddiagnosingdiseasesfromradiologicalscans.Itplaysacriticalroleininterpretingmagneticresonance\nimaging(MRI)andcomputedtomography(CT)scanstoidentifyirregularities[30].DLisalsoinstrumentalindiseasediagnosisand\nprognosisbybeingcapabletoanalyzevastdatasetsofmedicalrecordsandimages,providingmoreprecisepredictionsandenabling\npersonalized treatments for patients. In drug discovery and development, DL accelerates the process by analyzing molecular data\ntoidentifypotentialdrugcandidates,predicttheireffectiveness,andmodeltheirinteractionswithbiologicalsystems.Additionally,\nDL supports electronic health records (EHR) analysis by parsing large datasets to generate clinical insights, predict outcomes, and\nidentify disease patterns. Genomics and precision medicine benefit from DL by analyzing genetic data to identify anomalies and\npredict disease risks, while NLP applications in healthcare aid in extracting information from unstructured medical texts [31].\nDL is favored in the medical domain because of its ability to handle large-scale and complex data, such as medical images,\ngenomicdata,andelectronichealthrecords.Itscapacityforfeatureextractionandrepresentationlearningallowsittoautomatically\nlearnmeaningfulfeaturesfromrawmedicaldata,whichisparticularlyusefulfortaskslikemedicalimageanalysis.DLmodelsalso\nprovideimprovedaccuracyandpredictivepower,oftenoutperformingtraditionalmethodsandevensurpassinghumanperformance\nincertaindiagnostictasks.Anotheradvantageisinpersonalizedmedicine,whereDLleveragesextensivepatientdatatocustomize\ntreatmentplans,improvingdiseasemanagementandtreatmentoutcomes.Moreover,DLenhancesefficiencybyautomatingroutine\ntasks, thus alleviating the workload on medical professionals and allowing them to focus on patient care. In research and drug\ndiscovery, DL accelerates the discovery process and helps reduce the cost and time involved in clinical trials. Finally, DL models\ncontinually improve their performance with more data, making them highly adaptable to the evolving needs of the medical field.\nFigure 3 summarizes the benefits of DL in the healthcare domain [32].\n2.2. DL and CNN basics\nDLhasrevolutionizedmedicalimageanalysisbyenablingautomatedfeatureextractionandhierarchicalrepresentationlearning.\nUnlike traditional ML approaches that require handcrafted feature engineering, DL models, particularly CNNs, learn patterns\ndirectly from raw medical images, improving accuracy and diagnostic reliability. These models leverage multiple layers, including\nconvolutional layers for spatial feature extraction, pooling layers for dimensionality reduction, and fully connected layers for\nclassification. Regularization techniques and optimization strategies further enhance model generalization and performance.\nCNNs have been widely applied incancer detection, excelling in tumor segmentation, classification, and localization. Their\nhierarchical structure allows them to capture intricate patterns within medical images, making them highly effective for tasks such\nas identifying malignant lesions in radiology scans. Table 2 summarizes the main CNN components and their roles in cancer\ndetection.UnliketraditionalML-basedCADsystems,CNNsautomaticallyextracttumor-relatedfeatureswithoutrequiringmanual\nselection, streamlining the diagnostic process and improving efficiency. Table 3 summarizes the most frequently used activation,\nloss, and pooling functions in DL, particularly in CNN-based approaches.\n2.3. Evaluation metrics and datasets\nWhen evaluating the performance of DL, Transformers, and LLM models, selecting an appropriate metric is of paramount\nimportance. Numerous metrics have been proposed and employed across various DL applications [33, 34]. Table 4 presents a\nconcise summary of commonly used metrics that are particularly suited for assessing the performance of AI algorithms in cancer\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 4 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nHospitals (Agent/\nEnvironment)\nDoctors\n(Agent) \nPatients\n(Environment)\nAmbulance\n(Agent/ Environment)\nState/ action/\nreward\nHospitals\nDoctors \nPatients\nAmbulance\nAggregation \nserver\nData center\nimagery\nHospitals\nDoctors \nPatients\n Medical images\nTarget model\nData center\nimagery\nHospitals\nDoctors \nPatients\n Medical images\nViT/ LLM \nModel\nRequest\nResponse\nSource model\nRL\nFL\nTL\nLLM\\ ViT\nFine tuning\nInternet and \ncloud computing\nFigure 3:Application of advanced DL methods in healthcare, showcasing technologies such as RL, FL, TL, ViT, and LLMs. It highlights\nscenarios like real-time decision-making, privacy-preserving model training, and personalized treatment. The ecosystem integrates agents\nlike hospitals, doctors, and ambulances with cloud computing, pretrained models, and aggregation servers. Key use cases include\noptimizing resource allocation, enhancing diagnostic accuracy, and improving scalability. Advanced DL solutions address challenges such\nas data privacy, limited labeled datasets, and computational efficiency, ultimately supporting accurate, efficient, and personalized patient\ncare [32].\nTable 2\nKey CNN components for cancer detection.\nComponent Function Relevance to cancer detection\nConvolutional layer Extracts spatial features by applying\nfilters to input images.\nDetects tumor patterns and abnormal tissue\nstructures.\nPooling layer Reduces feature map size, preserving\nkey information.\nEnhances computational efficiency without\nlosing critical features.\nActivation function Introduces non-linearity to improve\nmodel learning.\nHelps differentiate between cancerous and\nnon-cancerous regions.\nFully connected layer Aggregates extracted features for fi-\nnal classification.\nUsed to distinguish between cancer types or\nstages.\nRegularization (Dropout,\nBatch norm.)\nPrevents overfitting by stabilizing\nlearning.\nImproves model generalization to unseen\nmedical data.\ndetection.Additionally,severaldatasetshavebeenemployedinrecentyearsacrossvariousmedicaldomainstoevaluateperformance\nand accelerate progress. Table 5 lists the most widely used datasets in different medical applications.\n1http://cimalab.intec.co/?lang=en&mod=project&id=31\n2http://homes.di.unimi.it/scotti/all/\n3https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease\n4https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/\n5https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n6https://luna16.grand-challenge.org/\n7https://www.cancerimagingarchive.net/collection/prostatex/\n8https://www.cancerimagingarchive.net/collection/stageii-colorectal-ct/\n9https://paip2019.grand-challenge.org/Dataset/\n10https://www.synapse.org/Synapse:syn51156910\n11https://cdas.cancer.gov/datasets/plco/10/\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 5 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nTable 3\nTypes of functions used in CNN-based cancer detection.\nFunction Mathematical formula Role overview\nSigmoid ğ‘“(ğ‘¥) = 1\n1 +ğ‘’âˆ’ğ‘¥\nThe input is real numbers, while the output is between \"0\" and \"1\".\nActivation functions\nTanh ğ‘“(ğ‘¥) = ğ‘’ğ‘¥ âˆ’ ğ‘’âˆ’ğ‘¥\nğ‘’ğ‘¥ + ğ‘’âˆ’ğ‘¥\nIts input is real numbers, and the output is between -1 and 1.\nReLU ğ‘“(ğ‘¥) =ğ‘šğ‘ğ‘¥(0,ğ‘¥) It converts the input values to positive numbers and presents low\ncomputational complexity. Is characterized with lower computational load.\nLeaky ReLU ğ‘“(ğ‘¥) =\n{ ğ‘¥, ğ‘¥> 0\nğ‘š.ğ‘¥, ğ‘¥ â‰¤ 0\nIt has a small slope for negative values. It is employed to overcome the\nDying ReLU problem. m: the leak factor.\nNoisy ReLU ğ‘“(ğ‘¥) =ğ‘šğ‘ğ‘¥(ğ‘¥+ ğ‘Œ) It employs a Gaussian distributionğ‘Œ âˆ¼ ğ‘(0,ğœ(ğ‘¥)) to create ReLU noisy.\nParametriclinearunits ğ‘“(ğ‘¥) =\n{ ğ‘¥, ğ‘¥> 0\nğ‘.ğ‘¥, ğ‘¥ â‰¤ 0\nIt is similar to Leaky ReLU, but it differs in that the leak factor is updated\nduring the model training process. a:learnable weight.\nLoss functions\nSLF ğ»(ğ‘,ğ‘¦) = âˆ’âˆ‘\nğ‘– ğ‘¦ğ‘–ğ‘™ğ‘œğ‘”(ğ‘ğ‘–), Is employed for measuring the performance.ğ‘ƒğ‘– = ğ‘’ğ‘ğ‘–\nâˆ‘ğ‘\nğ‘˜=1 ğ‘’ğ‘\nğ‘˜\n,ğ‘– âˆˆ [1,ğ‘] Its\noutput is the probabilityğ‘âˆˆ [0,1].\nELF ğ»(ğ‘,ğ‘¦) = 1\n2ğ‘\nâˆ‘ğ‘\nğ‘–=1(ğ‘ğ‘–âˆ’ğ‘¦ğ‘–)2 It is called mean square error and is used in regression problems.\nHLF ğ»(ğ‘,ğ‘¦) =âˆ‘ğ‘\nğ‘–=1 ğ‘šğ‘ğ‘¥(0,ğ‘š âˆ’\n(2ğ‘¦ğ‘– âˆ’ 1)ğ‘ğ‘–) It is employed in binary classification problems; this is important for SVMs.\nMax pooling ğ‘¦= max(ğ‘¥ğ‘–) Retains the most significant feature by selecting the maximum activation\nin each pooling region. Preserves texture details.\nPooling functions\nAverage pooling ğ‘¦= 1\nğ‘›\nâˆ‘ğ‘›\nğ‘–=1 ğ‘¥ğ‘–\nAverages the activations in each pooling region. Helps retain background\ninformation.\nGlobal pooling ğ‘¦= 1\nğ‘\nâˆ‘ğ‘\nğ‘–=1 ğ‘¥ğ‘–\nReduces feature maps to a single value per channel, helping in model\nregularization. Often used in fully convolutional architectures.\nStochastic pooling ğ‘ƒ(ğ‘¥ğ‘–) = ğ‘¥ğ‘–âˆ‘\nğ‘— ğ‘¥ğ‘—\nRandomly selects activation based on a probability distribution. Helps\nprevent overfitting.\nRank-based pooling ğ‘¦= Rank(ğ‘¥ğ‘–) Ranks the activations instead of selecting absolute values. Offers stronger\nrobustness in noisy environments.\nOrdinal pooling Sort (ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘›) Sorts activations in ascending or descending order to retain structured\ninformation. Speeds up training.\n3. Training modes in DL\nDL techniques can be trained using three primary modes: supervised learning (SL), semi-supervised learning (SSL), and\nunsupervised learning (USL). A detailed explanation of each mode is provided below, along with examples relevant to their\nrespectivecategory.Table7providesanoverviewofstate-of-the-artstudiesemployingtheseDLtrainingmodesforvariouscancer\ndetectionapplications.Table6providesanoverviewofthesemodes,theirapplicationsincancerdetection,andtheirkeylimitations.\n3.1. Deep SL\nDeep SL is a specialized branch of ML that trains algorithms to predict or make decisions based on labeled data sets. The\nprimary objective of deep SL is to develop a function that maps inputs to outputs by identifying patterns within the data. The\n\"deep\" aspect of SL pertains to the application of deep neural networks (DNN), which are characterized by multiple hidden layers.\nTraining involves iterative adjustments of the networkâ€™s weights and biases to reduce the disparity between its predictions and the\nactuallabels,usuallyemployingalossfunction(LF)tomeasurethisdifference.Back-propagationisusedtoupdateparametersand\nincrementally enhance performance.\nIn the context of image processing, SL can be further divided into two sub-categories: fully-supervised DL and weakly-\nsupervised DL algorithms. Fully-supervised DL algorithms require segmentationâ€”either manual or automated and cropping of\nthe region of interest (ROI) before inputting the data into classifiers. In contrast, weakly-supervised DL algorithms do not require\nimage annotation of the lesionâ€™s ROI. Instead, they leverage class activation maps generated by incorporating a global average\npooling (GAP) layer into the final convolutional layer to visualize detected regions. Figure 4 illustrates a comparison between\nweakly-supervised and fully-supervised DL algorithms for breast mass classification and localization.\nMany studies employing advanced DL rely on SL. For example, in [48], the authors explore the use of TL and SL techniques\nto enhance the classification of breast cancer histopathology images. The proposed approach aims to automate the differentiation\nbetween benign and malignant tissue by employing various feature extractors and classifiers, addressing challenges associated\nwith image analysis. Moreover, in [49], the authors examine how FL can facilitate the detection of boundaries in rare cancers\nusing big data. This research highlights the integration of FL with SL techniques to enhance the accuracy and effectiveness of\ncancer boundary detection while protecting patient privacy. At the same time, it enables the use of diverse datasets to improve\nthe modelâ€™s performance in detecting rare cancer types. Moving forward, the study [50], proposes a two-stage transformer-based\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 6 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nTable 4\nSummary of metrics used in evaluating DL, Transformers, and LLM-based cancer detection.\nTask type Metric Description\nClassification Acc = TP + TN\nTP + FP + TN + FN\nGives the correct percent of the total number of\npositive and negative predictions.\nClassification ğ‘†ğ‘ğ‘’ = ğ‘‡ğ‘\nğ‘‡ğ‘ + ğ¹ğ‘ƒ\nIt is the ratio of correctly predicted negative samples\nto the total negative samples.\nClassification Sen = TP\nTP + FN\nIt is a quantifiable measure of real positive cases\npredicted as true positive cases.\nClassification P = TP\nTP + FP\n100%\nMeasures the proportion of true positive predictions\nmade by the model, out of all positive predictions.\nClassification F1 = 2 Ã—Precision Ã— Recall\nPrecision + Recall\nIt is the harmonic mean of precision and sensitivity\nof the classification.\nClassification NPV = TN\nTN + FN\nThe proportion of negative results in diagnostic\ntests; a higher value indicates better diagnostic\naccuracy.\nSimilarity JSC = |A âˆ© B|\n|A âˆª B| = TP\nTP + FP + FN\nProposed by Paul Jaccard to gauge the similarity\nand variety in samples.\nError rate FPR = FP\nFP + TN\n= 1 âˆ’ SP\nMeasures the proportion of negative samples incor-\nrectly classified as positive by the model.\nCorrelation\n(ViT and LLM) MCC = TP.TN âˆ’ FP.FN\nâˆš\n(TP + FP)(TP + FN)(TN + FP)(TN + FN)\nAssesses binary classification by balancing true/false\npositives and negatives across varying class sizes.\nAdversarial robust\n(ViT and LLM) FR = Number of samples with changed predictions\nTotal number of adversarial samples\nMeasures misclassified samples after adversarial ma-\nnipulation, crucial for attack evaluation.\nAnomaly detection\n(ViT and LLM) AS = Score - Max baseline score\nBaseline standard deviation\nAssesses anomaly deviations from normal activity,\nadjusting thresholds to balance false positives and\nnegatives.\nAbbreviations: Accuracy (Acc); Alert score (AS); Specificity (Spe); Sensitivity (Sen); Fooling rate (FR); Matthewâ€™s correlation coefficient (MCC); Fallout or False\npositive rate (FPR); Jaccard similarity index (JSI); Negative predictive value (NPV); F1 score (F1); Precision (P).\nTable 5\nSummary of the most commonly used datasets in different cancer type.\nDatasets Available? Format Bit per pixel Resolution N of images Type of images Application Related work\nDDTI Yes 1 JPG 8-bit - 134 Ultrasound images Thyroid [35]\nALL-IDB Yes 2 JPG 24-bit 2592x1944 108 Microscope images Leukemia [36]\nTCGA Yes 3 TIFF, DICOM 16-bit 1000x1000 - Histopathological, radiology Kidney [37]\nBreakHis Yes 4 JPEG 8-bit 700x460 7,909 Histopathological Breast [38]\nHAM10000 Yes 5 JPEG 8-bit 600x450 10,015 Dermatoscopic Skin [39]\nLUNA16 Yes 6 - 16-bit 512x512 1,200 Lung CT Lung [40]\nProstateX Yes 7 DICOM 16-bit 384x384 1,000 Prostate MRI Prostate [41]\nTCIA - Colorectal Histology Yes8 PNG 8-bit 150x150 5,000 Histopathological Colorectal [42]\nPAIP 2019 Yes 9 TIFF 16-bit 20kx20k 100 Histopathological Liver [43]\nBraTS 2023 dataset Yes 10 NIfTI 16-bit 240x240x155 15,000 MRI Brain [44]\nNIH Pancreatic Yes 11 DICOM 16-bit 512x512 100+ MRI Pancreatic [45]\nweakly SL framework, called SSRViT, to support histo-pathological diagnosis of lung cancer. Due to the large size of whole-\nslide images and the difficulty of obtaining precise annotations, SSRViT aims to leverage weak labels for efficient learning.\nThe framework uses a Shuffle-remix ViT to extract discriminative local features, which are then aggregated for slide-level\nclassification via a simple transformer-based classifier. The method demonstrates superior performance in distinguishing between\nlung adenocarcinoma, pulmonary sclerosing pneumocytoma, and normal lung tissue. Sushil et al. [51] explored the use of LLMs\nto reduce the need for extensive data annotation in breast cancer pathology. A manually labeled dataset of 769 reports with 13\ncategories was used to compare the zero-shot classification performance of generative pretrained transformer (GPT)-4 and GPT-\n3.5 with traditional supervised models like random forests, long short-term memory (LSTM)-Attention, and UCSF-bidirectional\nencoder representations from transformers (BERT). GPT-4 performed as well as or better than the best SL models, particularly in\nhandlingimbalancedlabeltasks.ThefindingssuggestthatLLMscansignificantlyreducethelabor-intensivedatalabelingprocess,\nadvancing cancer diagnosis without sacrificing accuracy.\n3.2. Deep USL\nDeep USL is a branch of ML where neural networks are trained on large datasets without specific guidance or supervision.\nUnlike SL, which use labeled data, USL aims to identify inherent patterns and structures in the data autonomously. A prevalent\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 7 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nTable 6\nOverview of DL training modes for cancer diagnosis.\nTraining mode Key characteristic Application in cancer diagnosis Limitations\nSL Requires labeled data\nfor training\nUsed in cancer image classification, tumor segmentation,\nand disease diagnosis with well-annotated datasets [46].\nHigh labeling cost; performance depends on\ndataset quality; limited generalization to un-\nseen data.\nSSL Uses a mix of labeled\nand unlabeled data\nReduces the reliance on labeled data, making it useful for\nmedical imaging where obtaining labeled samples is costly.\nApplied in histopathology and radiology [20].\nPerformance is sensitive to the proportion\nof labeled data; risk of learning incorrect\npatterns from noisy unlabeled data.\nUSL Extractspatternsfrom\nunlabeled data\nUsed in clustering cancer subtypes, anomaly detection in\nmedical images, and identifying novel biomarkers [19].\nLack of ground truth validation; difficult\nto interpret results; risk of overfitting to\nirrelevant patterns.\nWeakly-supervised\nalgorithm\nFully-supervised\nalgorithm\nClassifierÂ :\nVGG16, ResNet 34, GoogleNet \nDiagnosis\nDiagnosis\n+Diagnosis\nCAM\nFigure 4:SL for ultrasound diagnosis of breast cancer images using both fully-supervised and weakly-supervised DL algorithms [47].\ntechnique in deep USL is the use of auto-encoder (AE)s, which are designed to compress data into a lower-dimensional space\nbefore reconstructing it to its original form, thereby learning the essential structures and features without labels. Other significant\nmethods in deep USL include generative adversarial networkss (GANs), which learn to create new data mimicking the original by\nunderstanding its distribution. Clustering is another common unsupervised strategy. Deep USLâ€™s primary benefit is its ability to\nreveal critical data features that are not readily visible, aiding in tasks such as image recognition where it can uncover crucial\nvisual patterns like edges and textures. This insight can improve the accuracy and robustness of models for tasks like object\nrecognition. However, deep USL presents several challenges: (1)Interpretability: The complexity of deep USL models makes\nit difficult to interpret how decisions are made or to diagnose errors effectively. (2)Data requirements: Deep USL requires large\ndatasets for effective learning; insufficient data may hinder accurate pattern recognition. (3)Model complexity: Deep USL models,\noften consisting of numerous layers and parameters, can complicate training and demand significant computational resources and\ntime. (4)Fine-tuning issues: While deep USL excels at learning general features, adapting these models to specific tasks remains\nchallenging.(5) Noise sensitivity:Thesemodelsarevulnerabletonoise,whichcanmisleadthelearningprocessandobscurerelevant\npatterns. (6)Pattern bias: Deep USL may introduce bias towards certain patterns or data points, potentially reducing diversity in\nthe learned features [52, 53].\nIn this context, numerous studies have been proposed in the literature, with many schemes adopting AE as the primary\nunsupervised approach, as depicted in Figure 5. For example, the study [55] employed USL and TL to assess epidermal growth\nfactorreceptormutationstatusinlungcancerusingCTimages.AconvolutionalAEwasdevelopedtoreconstructimagesandextract\nfeaturesfromthreeROI:thenodule,thelungcontainingthenodule,andbothlungs.ByleveragingTL,themodelimprovedfeature\nextractionandanalysis.Thestudyfoundthatanalyzingbeyondthenodulecapturedmorerelevantinformation,enhancingprediction\naccuracy. Bercea et al. [56] introduce FedDis, a novel FL approach that integrates USL to address data heterogeneity in medical\nimaging.FedDisdisentanglesmodelparametersintoshapeandappearancecomponents,sharingonlytheshapeparametersamong\nclients. This method leverages the assumption that anatomical structures in brain MRI images are consistent across institutions,\nimprovinganomalydetection,suchascanceridentification.Byutilizinghealthybrainscansfromvarioussources,FedDissegments\nabnormal structures in pathological databases, including Glioblastoma cases. Similarly, Stember et al. [57] propose a method that\ncombines USL (clustering) with RL to segment brain lesions in MRI scans. Initially, clustering generates candidate lesion masks\nfor each image. Users then select the best mask for a subset of images, which is subsequently used to train an RL algorithm to\nidentify the optimal masks. This approach was compared to a U-net SL network. While the SL model suffered from overfitting\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 8 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nUSL with auto-encoder DNN SL for classification\nFigure 5: An example on USL for cancer diagnosis using AE [54]. A 5-layer AE is trained in an USL manner to extract features from\nhigh-dimensional gene expression data, compressing them into a 30-dimensional transcriptomic feature vector. These feature vectors are\nsubsequently used for supervised training in a fully connected deep softmax classifier, which distinguishes between normal and tumor\nsamples and classify tumors by grade and stage.\nand performed poorly, the combined USL and RL approach achieved a high Dice score, demonstrating its effectiveness in lesion\nsegmentation with minimal radiologist input. Additionally, Pinaâ€™s study [58] addresses the challenges of digital pathology image\nanalysisforbreastcancer,focusingonthevariabilityinhistopathologicalslidesanddifferencesinstainingtechniques.Themanual\nannotation of large datasets across different stains (Estrogen receptor, progesterone receptor, human epidermal growth factor\nreceptor2,andKi-67protein)ishighlylabor-intensive.Toovercomethis,thestudyappliesUSLcombinedwithdomainadaptation\ntechniques and ViT to enhance cell detection tasks. By leveraging adversarial feature learning, the research improves pipelines\nfor CAD, significantly boosting diagnostic accuracy across various staining methods. Moving forward, the study [59] presents\na comprehensive exploration of USL techniques, which were employed using topic modeling methods such as latent Dirichlet\nallocation (LDA), non-negative matrix factorization (NMF), and combined topic models (CMT). These unsupervised methods\nwereappliedtoanalyzeandcategorizelargevolumesoftextdatafromtheWebofScienceandLexisNexisconcerningdiscussions\non LLMs. The use of USL enabled the study to evaluate topic coherence and diversity across different models, with BERTopic\nemerging as a top performer in both metrics [60].\n3.3. Deep SSL\nDeepSSLisastrategythatmergesthestrengthsofSLandUSL.InSL,modelsaretrainedwithlabeleddata,whileUSLinvolves\ntraining with unlabeled data. Deep SSL use a mix of a small set of labeled data with a significantly larger batch of unlabeled data,\nenhancing the training process under conditions where labeled data is scarce or costly to acquire. This hybrid approach allows the\nlabeled data to steer the learning while the unlabeled data aids in discerning broader data attributes. However, several challenges\nimpact its effectiveness: (1)Data quality and quantity: The efficacy of a deep SSL model is highly reliant on the volume and\nquality of the available unlabeled data. Insufficient or non-representative unlabeled data can degrade model performance. (2)Data\ndistribution: Performancecanalsosufferfromdisparitiesinthedistributionbetweenlabeledandunlabeleddata,especiallyiflabeled\ndata represents limited classes. (3)Hyperparameter selection: Deep SSL requires careful tuning of numerous hyperparameters\nsuch as the amount of labeled versus unlabeled data, the strength of regularization, and learning rates, which can be complex and\ntime-consuming to optimize. (4)Model interpretability:The complexity of DNN architectures in deep SSL makes it challenging\nto interpret how decisions are made or to pinpoint which data features are most influential in those decisions. (5)Adversarial\nvulnerability: Deep SSL models are susceptible to adversarial attacks, where manipulated inputs can lead the model to incorrect\noutputs [73]. Figure 6 illustrates a colorectal cancer study where SSL and SL are applied to labeled and unlabeled image patches\nfrom 70% of Dataset-PATT, generating models. Patient-level tests and human-AI competitions classify subjects as cancerous if\nclusters of positive patches are detected in whole slide images.\nThe study [69], proposes SDTL framework for diagnosing benign and malignant pulmonary nodules. By utilizing TL and\nan iterated feature-matching-based SSL method, the model benefits from both a pre-trained classification network and a large\ndatasetofunlabelednodules.TheSDTLframeworkgraduallyincorporatesunlabeledsamplestooptimizetheclassificationnetwork.\nExperimentalresultsshowthatTLandSSLsignificantlyimprovediagnosticaccuracy,highlightingtheframeworkâ€™spotentialasan\neffective tool in clinical practice for lung nodule diagnosis. In addition, Bdair et al. [74], introduces FedPerl, a semi-supervised\nFL method for skin cancer detection that addresses the challenge of limited annotated data. FedPerl integrates peer learning\nand ensemble averaging to improve pseudo label accuracy by fostering collaboration among distributed clients. Unlike previous\nmethods, [75] introduces DGARL, a novel approach for end-to-end semi-supervised segmentation of medical images, including\ncancerdetection.DGARLcombinesdeepRLwithGANs,improvingbothtumordetectionandsegmentationtaskssimultaneously.\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 9 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nTable 7\nSummary of classification examples employing various training modes in different medical domains.\nRef. Year C Algo. Disease Dataset Modality Metrics\nAcc. F1 FPR AUC Pre. Sens.\n[50] 2024 DSL ViT Lung Cancer TCGA-LUAD Whole Slide Images 96.90 â€“ â€“ 99.60 â€“ â€“\n[51] 2024 DSL LLM Breast cancer BreaKHis Pathology Images â€“ 83.00 â€“ â€“ â€“ â€“\n[55] 2021 DUSL TL Lung cancer LIDC-IDRI Chest CT Images â€“ â€“ â€“ 68.00 â€“ â€“\n[61] 2024 DSL LLM Multiple cancers TCGA\nPatchCamelyon Histopathology 90.00 â€“ 80.00 â€“ â€“ â€“\n[62] 2023 DSL TL Lung cancer LIDC-IDRI DICOM 96.00 â€“ â€“ 95.84 â€“ â€“\n[63] 2022 DSL TL Lung cancer LIDC-IDRI, LUNA16 CT Scans 91.10 81.50 â€“ 95.80 84.9 â€“\n[64] 2020 DSL TL Breast cancer INbreast, DDSM Mammography 89.77 88.91 â€“ â€“ â€“ 83.78\n[65] 2023 DSL FL Multiple Cancers Histopath. images JPEG, PNG 96.66 96.64 â€“ â€“ 97.14 â€“\n[66] 2023 DSL FL Lung Cancer LUNA16 CT Scans 83.41 83.40 â€“ 88.38 83.41 83.38\n[67] 2023 DSL FL Breast Cancer VINDR-MAMMO,\nCMMD, INbreast Mammography 95.00 â€“ â€“ â€“ â€“ â€“\n[68] 2023 DSL ViT Breast cancer Thermal dataset Thermal Images 95.78 â€“ â€“ â€“ â€“ â€“\n[69] 2021 DSSL TL Pulmonary nodules LUNA16 Chest CT Images 88.30 â€“ â€“ 91.00 â€“ â€“\n[70] 2023 DSSL TL Skin cancer Custom dataset 2D Dermoscopy 98.00 98.00 â€“ â€“ â€“ 98.00\n[71] 2023 DSSL TL Prostate cancers Custom PET/CT\ndataset 3D PET/CT 83.00 â€“ â€“ 86.00 â€“ â€“\n[72] 2024 DSSL ViT Breast cancer BreakHis Histopath. Images 98.12 98.41 â€“ â€“ 98.17 â€“\nAbbreviations: Category (C);\nSegmentation\nLoss\nEvaluation\nLoss\nAdversarial\nLoss\nSemi-supervised Fully supervised\nAnnotated \ndata\nUnannotated data\nBUS-S  Network BUS-E  Network\nFigure 6:An example on recognition of breast cancer with SSL [47].\nThis method incorporates a task-joint GAN with two discriminators to link detection outcomes with segmentation performance,\nenabling mutual optimization.Furthermore, a bidirectional explorationRL technique is employed toaddress challenges associated\nwith unlabeled data. Experiments conducted on datasets of brain, liver, and pancreas tumors demonstrate that DGARL enhances\nsegmentation accuracy, underlining its potential effectiveness in cancer diagnosis. Wang et al. [72] propose a SSL framework for\nbreast cancer detection using the ViT, which has demonstrated superior performance compared to traditional CNNs across various\ntasks. While CNNs have been extensively studied for breast cancer detection, the use of ViT in this domain has been relatively\nlimited. Nonetheless, validation on ultrasound and histopathology datasets reveals that this method consistently outperforms CNN\nbaselines across multiple tasks. Kumari et al. [76] introduce LLM-SegNet, a SSL model for 3D medical image segmentation,\nincludingcancerimaging,thatreducestherelianceonextensivevoxel-levelannotations.ByincorporatingaLLMintoitsco-training\nframework,LLM-SegNetimproveslearningfromunannotatedsamples,enhancingthemodelâ€™sabilitytoidentifycancerousregions.\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 10 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nTable 8\nComprehensive comparison of advanced DL techniques for cancer diagnosis.\nDL\nmethod Scalability Real-world implementation Computational com-\nplexity Accuracy Privacy\nconsiderations Research gaps\nRL\nModerate(limited\nscalability due to\nneed for interac-\ntions with environ-\nment and reward\nstructure complex-\nity)\nUsed for tumor segmentation,\nautomated lesion localization,\nand personalized treatment\nplanning by continuously re-\nfining predictions based on re-\nwards [18].\nHigh (trial-and-\nerror learning\nrequires significant\ncomputational\nresources)\nHigh (if well-defined\nreward functions are\nused)\nLow(centralized data\nrequired for training)\nLimited application in\ncancer detection; re-\nquires large datasets\nfor effective learning.\nFL\nHigh (scalable\nacross institutions\nwithout data\nsharing)\nTrains models across decen-\ntralized datasets from differ-\nent hospitals, enhancing gen-\neralizabilitywhileensuringpa-\ntient data confidentiality [20].\nModerate (network\nbandwidth and\nencryption overhead\naffect performance)\nComparableto cen-\ntralized models (when\nsufficient data diver-\nsity exists)\nHigh (data remains\ndecentralized, improv-\ning security)\nModel divergence,\ncommunication\ndelays, and potential\nsecurity risks in\ndistributed learning.\nTL\nHigh (leverages\npre-trained models\nfor adaptation)\nFine-tunes pre-trained models\non cancer imaging datasets,\nimproving accuracy with min-\nimal labeled data [19].\nLow (reduces training\ntime by reusing pre-\ntrained models)\nHigh (if source and\ntarget domains are\nwell-matched)\nModerate (requires\nlabeled data, but\ntraining can be\nprivacy-preserving)\nDomain shift\nchallenges when\napplied to different\ndatasets.\nTransf.\nHigh (well-suited\nfor large-scale\ndatasets)\nImproves feature extraction in\ncancer histopathology, radiol-\nogy, and genomics by leverag-\ning self-attention mechanisms\n[77].\nVery High\n(self-attention\ncomputations are\nexpensive)\nHigh(state-of-the-art\nperformance in medi-\ncal imaging tasks)\nLow(requires central-\nized data for training)\nOverfitting in small\nmedical datasets;\nhigh resource\nrequirements.\nLLM\nModerate\n(requires fine-\ntuning for\nspecific medical\napplications)\nAssists in clinical decision-\nmaking by analyzing patient\nhistories, summarizing pathol-\nogy reports, and automat-\ning medical literature reviews\n[46].\nVery High\n(pretraining and\ninference are\ncomputationally\ndemanding)\nHigh (excellent for\ntext-based analysis,\nbut needs domain\nadaptation)\nLow(requires central-\nized data for pre-\ntraining, posing pri-\nvacy risks)\nEthical concerns, hal-\nlucination issues, and\nregulatory challenges\nin healthcare applica-\ntions.\nExperimental results on multiple datasets demonstrate that LLM-SegNet outperforms existing models in terms of segmentation\naccuracy.\n4. Advanced DL techniques\nTo improve cancer diagnosis, various advanced DL techniques have been integrated into medical applications. This section\nsummarizeskeyproposedtechniques,emphasizingtheireffectiveness.Table8presentsanoverview,detailingtheiradvantagesand\nspecific applications in cancer detection.\n4.1. Reinforcement learning\nRL is a ML paradigm in which an agent learns to make decisions by interacting with an environment, aiming to maximize\ncumulativerewardsovertime.Theagentselectsactionsbasedonapolicy( ğœ‹)thatmapsstates( ğ‘ )toactions( ğ‘),receivingfeedback\nintheformofrewards( ğ‘Ÿ)fromtheenvironment.RLproblemsaretypicallymodeledasMarkovdecisionprocesses(MDPs),where\nthe agentâ€™s objective is to learn an optimal policy that maximizes long-term returns. Various types of RL have been explored in\nthe literature (Figure 7), with a summary of their key findings presented in Table 9. While most RL approaches have already been\nreviewed in [78], the most commonly employed types in cancer diagnosis include:\n4.1.1. Q-learning and DQN\nAimstofindtheoptimalaction-selectionpolicybymaximizingcumulativerewardsinagivenenvironment.However,Q-learning\nstruggles to handle large state spaces where storing a Q-table becomes inefficient. To address this, the deep Q-network (DQN)\nalgorithm extends Q-learning by using neural networks to approximate the Q-values, solving problems with large, discrete action\nspaces.DQNintroducesexperiencereplayandtargetnetworks,whichdecouplethetargetandlearnedQ-values,stabilizinglearning.\nThis approach is particularly useful in environments where exploration is critical. The key functions associated with the DQN are\n[75,81]:(i)TheQ-functionapproximatestheexpectedcumulativerewardoftakinganaction ğ‘inagivenstate ğ‘ andfollowingthe\npolicy afterward,ğ‘„(ğ‘ ,ğ‘) =ğ”¼[ğ‘…ğ‘¡|ğ‘ ğ‘¡ = ğ‘ ,ğ‘ğ‘¡ = ğ‘], where,ğ‘„(ğ‘ ,ğ‘) is the Q-value (action-value),ğ‘ is the state, andğ‘is the action and\nğ‘…ğ‘¡ is the cumulative reward from timeğ‘¡onward. (ii) The update rule for DQN based on the Bellman equation is:\nğ‘„(ğ‘ ,ğ‘) â† ğ‘„(ğ‘ ,ğ‘) +ğ›¼\n(\nğ‘Ÿ+ ğ›¾max\nğ‘â€²\nğ‘„(ğ‘ â€²,ğ‘â€²) âˆ’ğ‘„(ğ‘ ,ğ‘)\n)\n(1)\nWhere ğ‘Ÿis the reward received after taking actionğ‘in stateğ‘ , ğ‘ â€² is the next state andğ›¾ is the discount factor, andğ›¼is the learning\nrate. (iii) The loss function minimizes the difference between the target and predicted Q-values:\nğ¿(ğœƒ) =ğ”¼\n[(\nğ‘Ÿ+ ğ›¾max\nğ‘â€²\nğ‘„(ğ‘ â€²,ğ‘â€²; ğœƒâˆ’) âˆ’ğ‘„(ğ‘ ,ğ‘; ğœƒ)\n)2]\n(2)\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 11 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nDeep\nDeterministic \npolicy\ngradient\nProximal\npolicy\noptimizationRecurrent\nattention\nmodel\nInv erse\nRL\nDeep\nQ-network\nAsy nchronous\nadv antage\nactor-critic\nReinf orcementtransf er \nlearning\nAction\nState\nAgent\nEnv ironment\nReward\nFigure 7:RL types used in cancer diagnosis. DQN [79, 80, 81]; RAN [82]; PPO [83]; DDPG [84]; IRL [85, 86]; A3C [87]; RTL [88, 89].\nWhere, ğœƒand ğœƒâˆ’ represent the parameters of the Q-network and target Q-network. Many studies related to DQN-based cancer\ndetection have been proposed in the literature. For example, [79] focuses on enhancing the localization of malignant cervical\ncells, a key aspect of detecting tumors. It employs a DQN algorithm to fine-tune bounding boxes around cancerous nuclei to\nimprove localization accuracy through reward-based reinforcement. This method addresses DL overfitting issues by incorporating\nrandomness, leading to successful and competitive localization performance compared to existing techniques. Similarly, [80]\naddressestheinefficienciesofmanuallungnoduledetectionmethodsandproposestheLLC-QEmodel,whichintegratesensemble\nlearning with DQN. The model is pre-trained using the artificial bee colony algorithm to avoid local optima. It employs multiple\nCNNs to extract and combine feature vectors for classification. Trained on the LIDC-IDRI dataset, the model tackles dataset\nimbalance by using RL to prioritize accurate classification of underrepresented classes. Moving on, Dahdouh in [81], proposes\nan advanced model for early skin cancer detection by integrating DL and DQN. The approach uses the watershed algorithm\nfor segmentation to isolate affected areas. A deep CNN classifies the lesions into seven categories: actinic keratosis, basal cell\ncarcinoma, benign keratosis, dermatofibroma, melanocytic nevi, melanoma, and vascular skin lessions. The model is further\nrefinedusingtheDQNalgorithm,whichoptimizesperformancethroughRL.Likewise,Tao[90],contributesbyproposingSeqSeg\nframework, a novel method for reliable carcinoma segmentation in MRI. SeqSeg addresses background dominance issues at two\nscales: instance level and feature level. It uses a DQN-based model to focus attention on the tumor and reduce segmentation\nbackground scale, and employs high-level semantic features to guide FL. Evaluated on a large dataset, SeqSeg outperforms state-\nof-the-art methods and demonstrates superior performance across multi-device and multi-center datasets. Praneeth et al. [91],\npresents RL2NdgsNet, a DL network enhanced by RL for diagnosing mediastinal lymph nodes and distinguishing between benign\nand malignant cases. The proposed approach leverages radiological modalities like X-ray, ultrasound, CT, and MRI, which are\nnon-invasive and painless. The RL2NdgsNet network incorporates a custom DQN policy to optimize its performance. Various\nstate-of-the-art AFs and exploration fractions were tested to refine the networkâ€™s capabilities. The results demonstrate that the\nRL2NdgsNet achieves superior diagnostic performance, improving upon existing DL-based methods for medical imaging and\noffering a promising alternative to traditional invasive procedures. The reference [92] explores the potential of deep RL in lung\ncancer detection, emphasizing its integration with medical big data from the medical Internet of Things (IoT). This work discusses\nthe use of DQN and its variants, such as double DQN and hierarchical DQN.\n4.1.2. Proximal policy optimization (PPO)\nIs another popular type of RL algorithm designed to balance exploration and exploitation. It improves training stability\nby updating policies using clipped objective functions within a trust region to avoid large updates, ensuring smoother policy\nupdates. The key functions involved in PPO are: (i) The policyğœ‹ğœƒ(ğ‘|ğ‘ ) represents the probability of taking actionğ‘given state\nğ‘ , parameterized byğœƒ. (ii) The advantage functionÌ‚ğ´ğ‘¡ which represents how much better a particular action is compared to the\naverage action taken at stateğ‘ ğ‘¡ where:\nÌ‚ğ´ğ‘¡ = ğ‘…ğ‘¡ âˆ’ ğ‘‰(ğ‘ ğ‘¡) (3)\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 12 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nThe ğ‘…ğ‘¡ is the cumulative reward at time stepğ‘¡and ğ‘‰(ğ‘ ğ‘¡) is the value function, representing the expected cumulative reward from\nstate ğ‘ ğ‘¡. The objective is to maximize:\nğ¿(ğœƒ) =ğ”¼ğ‘¡\n[min (ğ‘Ÿğ‘¡(ğœƒ) Ì‚ğ´ğ‘¡,clip(ğ‘Ÿğ‘¡(ğœƒ),1 âˆ’ğœ–,1 +ğœ–) Ì‚ğ´ğ‘¡\n)] (4)\nWhere ğ‘Ÿğ‘¡(ğœƒ) is the probability ratio of the new policy to the old policy,Ì‚ğ´ğ‘¡ is the advantage estimate at time stepğ‘¡, ğœ– is a small\nhyper-parameter that defines the clipping range and clip(ğ‘Ÿğ‘¡(ğœƒ),1 âˆ’ğœ–,1 +ğœ–) limits the policy update to be within a trusted region.\nWithin the class of PPO-based cancer detection methods, the study [93] introduces a RL-based neural architecture search\nmethod to automate the development of DL models for cancer data. This approach streamlines the creation of predictive models\nbyincorporatingdomain-specificcharacteristicsandreducingrelianceonmanualtrial-and-errormethods.Custombuildingblocks\nare designed to cater to the specific needs of cancer data, leading to the discovery of DNN architectures with fewer trainable\nparameters and shorter training times, while achieving comparable or superior accuracy to manually designed models. Similarly,\n[83] presents RLogist, a deep RL method based on the PPO algorithm designed to improve the efficiency of whole-slide image\nanalysis in computational pathology. Unlike traditional methods that require extensive sampling of high-magnification patches,\nRLogist emulates the diagnostic process of human pathologists to strategically identify valuable regions for observation. This\napproach reduces the need for dense, high-magnification patch analysis by learning to select representative features from multiple\nresolution levels. Evaluated on tasks such as detecting metastases in lymph node sections and subtyping lung cancer, RLogist\ndemonstrates competitive performance and provides interpretable decision-making pathways, potentially offering educational and\nassistive benefits for pathologists.\n4.1.3. Deep deterministic policy gradient (DDPG)\nIs a type of RL used to handle problems with continuous action spaces. It is an actor-critic (AC) method, which means it\ncombines two networks: an actor network that suggests actions and a critic network that evaluates them. DDPG is an off-policy\nalgorithm,meaningitlearnsthevaluefunctionfromactionstakenbyabehaviorpolicydifferentfromthetargetpolicy[94].DDPG\nconsists of two neural networks:\n(i) Actor network:outputs a deterministic action given the state. The deterministic policy function is defined as:ğ‘ğ‘¡ = ğœ‡(ğ‘ ğ‘¡|ğœƒğœ‡)\nwhereğ‘ ğ‘¡ isthecurrentstate, ğœ‡(ğ‘ ğ‘¡) isthelearnedpolicy,and ğœƒğœ‡ aretheparametersoftheactornetwork.Theactornetworkisupdated\nusing the deterministic policy gradient (DPG):\nâˆ‡ğœƒğœ‡ğ½ â‰ˆ ğ”¼ [âˆ‡ğ‘ğ‘„(ğ‘ ,ğ‘|ğœƒğ‘„)|ğ‘= ğœ‡(ğ‘ )âˆ‡ğœƒğœ‡ğœ‡(ğ‘ |ğœƒğœ‡)]\nThis update ensures that the actor selects actions that maximize the criticâ€™s estimated Q-values.\n(ii) Critic network:estimates the Q-value (expected return) for state-action pairs. The action-value function is approximated as:\nğ‘„(ğ‘ ,ğ‘|ğœƒğ‘„) =ğ”¼ [ğ‘Ÿğ‘¡ + ğ›¾ğ‘„(ğ‘ ğ‘¡+1,ğœ‡(ğ‘ ğ‘¡+1|ğœƒğœ‡)|ğœƒğ‘„)] (5)\nwhere, ğ‘Ÿğ‘¡ is the reward at timeğ‘¡, ğ›¾ is the discount factor, andğœƒğ‘„ are the parameters of the critic network. The loss function for\nupdating the critic network is:\nğ¿(ğœƒğ‘„) =ğ”¼ [(ğ‘¦ğ‘¡ âˆ’ ğ‘„(ğ‘ ğ‘¡,ğ‘ğ‘¡|ğœƒğ‘„))2], (6)\nwhereğ‘¦ğ‘¡ = ğ‘Ÿğ‘¡+ ğ›¾ğ‘„(ğ‘ ğ‘¡+1,ğœ‡(ğ‘ ğ‘¡+1|ğœƒğœ‡)|ğœƒğ‘„).ThisminimizestheerrorbetweenthepredictedQ-valueandthetargetQ-value.Asan\nexample, [94] employed DDPG as RL method for skin lession segmentation. The method mimics physiciansâ€™ delineation of ROI,\ntraining an agent to refine segmentation through continuous actions using the DDPG algorithm.\n4.1.4. Other RL methods\nMany other types of RL methods have been investigated in the field of cancer detection, including:\nâ€¢ Asynchronous advantage actor-critic (A3C):optimizes policy parameters using the advantage function, which reduces\nvariance in gradient estimation. The policy gradient update is given by:\nâˆ‡ğœƒğ½ = ğ”¼ [âˆ‡ğœƒlog ğœ‹ğœƒ(ğ‘|ğ‘ )ğ´(ğ‘ ,ğ‘)] (7)\nwhere ğ´(ğ‘ ,ğ‘) = ğ‘„(ğ‘ ,ğ‘) âˆ’ğ‘‰(ğ‘ ) is the advantage function, andğœ‹ğœƒ(ğ‘|ğ‘ ) is the stochastic policy parameterized byğœƒ. Unlike\nstandardACmethods,A3Crunsmultipleagentsasynchronously,reducingtrainingtimeandpreventingcorrelationintraining\nsamples.Forexample,[87]addressesthechallengeofskincancerdiagnosisbydevelopinganautomatedsystemusinganovel\ndeep RL technique based on asynchronous advantage A3C. This assumes multiple independent CNN agents that interact.\nThese agents interact with skin images to perform segmentation, guided by policies that maximize rewards and minimize\nerrors.\nâ€¢ Residualattentionnetwork(RAN): integratesattentionmechanisms(cf.Section4.4)intoRL,whereanagentsequentially\nfocuses on informative regions of an observation, and it is formulated as:\nğ‘”ğ‘¡ = ğ‘“ğ‘”(ğ‘ ğ‘¡,ğ‘™ğ‘¡; ğœƒğ‘”) (8)\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 13 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nwhere ğ‘”ğ‘¡ represents the extracted features at time stepğ‘¡, ğ‘ ğ‘¡ is the state,ğ‘™ğ‘¡ is the attention location, andğœƒğ‘” are the learnable\nparameters. The agent optimizes its policy to selectğ‘™ğ‘¡ such that the accumulated reward is maximized. This is particularly\neffective in image-based RL tasks, reducing computation by selectively processing high-information regions. For example,\n[82] employed RAN to enhance cancer detection by guiding neural networks to focus on lesions in chest radiographs. RL\npenalizes irrelevant areas, while attention improves lesion localization for accurate classification.\nâ€¢ Reinforcement transfer learning (RTL):Leverages pre-trained policies to accelerate learning in new environments. The\nknowledge transfer objective is:\nğ½TL = ğ”¼ğ‘ ,ğ‘ [ğœ†ğ‘„source(ğ‘ ,ğ‘) + (1 âˆ’ğœ†)ğ‘„target(ğ‘ ,ğ‘)] (9)\nwhere ğ‘„source(ğ‘ ,ğ‘) is the action-value function from a pre-trained model, andğ‘„target(ğ‘ ,ğ‘) is the value function for the new\ntask. The parameterğœ† controls the influence of the transferred knowledge. This approach is effective in robotics, where\nmotorskillslearnedinsimulatedenvironmentscanbetransferredtoreal-worldapplications.Forinstance,Xuetal.[89]have\nemployedRTLtotoimprovethequalityofMRIimagesandextractsignificanttumorfeatures.Theaimistoimprovetheearly\ndetectionofbraintumorsthroughtheuseofadvancedDLnetworksforsegmentationandclassification.Theproposedmethod\nemployedaspecifictypeofRLcalledAC,withU-NetandResNetarchitecturesformulti-classificationofbraintumors.This\napproach leverages these models to extract significant features from MRI slices, thereby improving diagnostic accuracy and\nefficiency.\nâ€¢ Inverse reinforcement learning (IRL):Infers a reward functionğ‘…(ğ‘ ,ğ‘) from expert demonstrations. The optimal reward\nfunction is estimated by solving:\nmax\nğ‘…\nâˆ‘\nğ‘¡\nğ›¾ğ‘¡ğ‘…(ğ‘ ğ‘¡,ğ‘ğ‘¡) âˆ’ğœ†||âˆ‡ğ‘…||2 (10)\nwhere ğ›¾ is the discount factor, andğœ†||âˆ‡ğ‘…||2 is a regularization term to prevent overfitting. IRL enables learning human-\nlike policies by inferring implicit goals from observed expert behavior, making it a powerful tool in imitation learning\napplications.In[86],IRLenhancescancerscreeningbyderivingrewardfunctionsfromexpertdecisions.Utilizingmaximum\nentropy IRL, partially observable Markov decision process (MDP) models optimize screening strategies, achieving expert-\nlevel recommendations for breast and lung cancer detection. Furthermore, IRL improves specificity, reduces false positives,\nand enhances early cancer detection while maintaining expert-level accuracy.\nTable 9: Summary of RL in cancer diagnosis.\nRef Model(s) used Image dataset Contribution Bestresult(%) Limitation\n[80] RL-EL Lung nodule CombinesRLandensemblelearning(EL)improve\nfor lung nodule detection.\nF1: 89.80\nPre: 87.70\nIncreased computational demands limit scala-\nbility.\n[81] CNN-DQN HAM10000 CombiningCNNandDQNforaccurateskincancer\nclassification. Acc: 80.00 Reliance on one dataset limits modelâ€™s cover-\nage.\n[84] Attention-RL Skin lesion RL for skin lesion segmentation with attention\nmechanism. Acc: 97.10 Computational complexity of attention-\nguided model.\n[87] AC-CNN PH2, ISIC Deep RL for skin cancer localization. Acc: 98.80 Increased computational complexity with\nCNN agents.\n[88] RL-DBN 3D Brain tumor Automated brain tumor segmentation using RL\nmodels.\nAcc: 97.57\nPre: 97.15\nAUC: 69.59\nPreprocessed data may miss tumor variations.\n[89] SKT-RL Liver tumor\nLivertumordetectionwithoutcontrastagentsusing\nspatiotemporal knowledge teacher student (SKT)-\nRL.\nAcc: 97.35\nRec: 74.60\nSpe: 98.44\nHigh computational resources required.\n[90] DQL-RANet nasopharyngeal\ncarcinoma SeqSeg framework for carcinoma segmentation. Dice: 80.32\nRec: 87.57 High computational complexity of SeqSeg.\n[91] DL-RL Lymph nodes RLforbenignandmalignantlymphnodediagnosis.\nAcc: 98.20\nSen: 98.03\nAUC: 98.19\nComplexity of RL increases demands.\n[94] DDPG ISIC, PH2, HAM10000 Deep RL for skin lesion segmentation.\nAcc: 96.33\nSpe: 98.60\nSen: 96.79\nRequires accurate annotations and high com-\nputational demands.\n[95] RL-Attention Breast DCE-MRI Post-hoc breast cancer screening using RL. AUC: 91.00 Challengeswithconvergenceanddatasetcon-\nsistency.\n[96] Customizedre-\nward Breast ultrasound video RL for keyframe extraction in breast ultrasound\nvideos. AUC: 84.15 Requires extensive data for class imbalance.\n[97] TL-RL Lung cancer Adaptive RL for early lung cancer detection. Acc: 92.00 Substantial computational resources required.\n[98] TL-RL Thyroid nodule RL for thyroid nodule feature extraction and local-\nization.\nAcc: 98.32\nRec: 93.84\nComplexity and computational constraints for\nimplementation.\n[99] DQN-CNN Lung cancer DQN for non-small cell lung cancer prognosis. Pre: +12.5 Computationalcomplexityofintegratingmul-\ntiple techniques.\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 14 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nRef Model(s)Used Image dataset Contribution Bestresult(%) Limitation\n[100] Attention-\nCNN-RL Breast cancer RL-based CNN for early breast cancer detection. Acc: 99.35\nFNR: 0.34\nComplexity limits practical clinical imple-\nmentation.\n4.2. Federated learning\nFL is a distributed ML paradigm that allows multiple decentralized devices (clients) to collaboratively train a shared model\nwhile retaining their data locally, thereby enhancing privacy preservation. A central server orchestrates the training process by\naggregatingupdatesfromindividualclientswithoutdirectlyaccessingtheirdata[15].FLencompassesvariousapproaches,suchas\nhorizontalfederatedlearning(HFL),verticalfederatedlearning(VFL),andfederatedsemi-supervisedlearning(FSSL).Theoverall\nFL process can be formally expressed as:\nğ‘¤ğ‘¡+1 =\nğ‘âˆ‘\nğ‘–=1\n|ğ·ğ‘–|\nâˆ‘ğ‘\nğ‘—=1 |ğ·ğ‘—|\nğ‘¤ğ‘¡\nğ‘– (11)\nWhereğ‘¤ğ‘¡+1 denotestheupdatedglobalmodelparametersaftertheaggregationinthe (ğ‘¡+1)-thround, ğ‘¤ğ‘¡\nğ‘– standsforlocalmodel\nparameters from clientğ‘–after local training at roundğ‘¡and |ğ·ğ‘–| is the number of data samples at clientğ‘–, and this determines the\nweightofeachclientâ€™scontributiontotheglobalmodel.Figure8summarizesthemostcommonlyusedFLtechniquesinthecontext\nof cancer detection, including both well-established methods from the literature and approaches specific to certain schemes.\nSemi-superv ised FL\nFederated dropout av eraging\nShapley  and game theoretical FL\nPractical f ederated gradient boosting decision trees\nWeight av eraging-based FL\nEdge FL\nFederated post-deploy ment adaptation\nFederated av eraging\nPersonalized FL\nFederated proximal\nFederated batch normalization\nSecure FL\nFederated attention consistent learning\nFederated conv olutional neural networks\nMultimodal FL\nCentralized FL\nCentralized client-serv er architecture\nVertical FL\nHorizontal FL \nCross-silo FL\nFigure 8:FL types used in the diagnosis of cancer. Cross-silo FL [101, 102]; FSSL [74]; HFL [103]; VFL [104]; Centralized client-server\narchitecture [67]; Centralized FL [105]; Federated dropout averaging [106]; Federated averaging [107]; Federated-CNN [108]; Practical\nfederated gradient boosting decision trees [109]; Shapley and game theoretical FL [110]; Weight averaging-based FL [111]; Edge\nFL [112]; Federated post-deployment adaptation [113]; personalized FL [114]; Federated proximal [115]; Secure FL [116]; Federated\nattention-consistent learning [117].\nHFL, also known as sample-based federated learning, is a type of FL where the participants (e.g., clients or institutions) have\ndatasets that share the same features (i.e., the same feature space) but represent different individuals or entities (i.e., different\nsamples). In this scenario, all participants hold data about different people or entities, but the structure of the data (the features)\nis consistent across them [66]. However, VFL trains the models on datasets with identical sample spaces but different feature\nspaces. Participants collaborate by aligning data entities and sharing encrypted model updates, ensuring privacy while leveraging\ncomplementaryfeaturesforimprovedperformance[15].FSSLisaspecializedvariationofFLdesignedtohandledistributeddatasets\nwheresomeclients(devicesororganizations)havelabeleddatawhileothershaveunlabeledorpartiallylabeleddata.FSSLcombines\ntheprinciplesofFLandSSLtocollaborativelytrainmodelswithoutrequiringallparticipantstohavelabeleddata[118].Asshown\nin Figure 9, the proposed SSL-FL-BT pipeline enhances histopathological image classification by integrating SSL with FL. This\napproach improves feature extraction and model generalization, making it well-suited for cancer diagnosis.\nIn the context of cancer detection, numerous FL techniques have been recently reported. Reference [119, 120, 121, 122, 123,\n124, 125, 126, 66] HFL as main technique for cancer detection. Study [119] introduced a distributed learning methodology for\nparticle swarm optimization-based fuzzy cognitive maps that prioritizes data privacy. Applied to cancer detection, this approach\ndemonstratesimprovedmodelperformancethroughFL,achievingresultscomparabletothosefoundinexistingliterature.Reference\n[120] introduced \"Skin-net,\" a CNN model for skin cancer detection using progressively private HFL. This method ensures data\nconfidentiality during training, achieving high performance while addressing privacy concerns in medical image analysis. [121]\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 15 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nproposed a knowledge-sharing model HFL using a Unet-based mask generator featuring classification-guided discriminator and\nan adversarial network for improved detection performance of pulmonary nodules. The paper [122] addressed the challenge of\naccessinglargedatasetsfordiseasedetectionwhileensuringdataprivacy.HFLisproposedasanalternativetoconventionalmethods,\nusing X-ray images for detecting lung cancer and tuberculosis. Reference [123] presented a HFL approach for predicting breast\ncancer while ensuring data privacy. The study [124] introduced the privacy-embedded lightweight and efficient automated (PLA)\nmethod for breast cancer diagnosis within the framework of internet of medical things (IoMT). This approach combines privacy-\npreserving techniques, efficiency, and automation. The PLA model utilizes a ViT backbone optimized for IoMT environments,\nensuringlightweightclassificationandeffectiveglobalinformationprocessingofbreastcancerdata.ItalsoemploysHFLtoprotect\npatientprivacyandincorporatestextureanalysisasanauxiliarytaskalongsidetheprimaryclassificationtask.ThePLAframework\nachievesimpressiveperformancemetrics.Thestudy[125],addressedthechallengesinlungcancerdetectioncausedbyfragmented\npatient data across various medical institutions and privacy concerns that hinder centralized data analysis. HFL is proposed as a\nsolution to train models on decentralized data and leveraging TL to set initial weights. The approach achieved a high accuracy\nin detecting lung cancer from medical images. Reference [126] tackled the challenge of detecting hepatocellular carcinoma by\nintegrating2Dand3DDLmodelswithinHFLframeworkforaccurateliver andtumorsegmentationinCTscans. Using131scans\nfromthelivertumorsegmentationchallenge,theHybrid-ResUNetmodeloutperformedResNetandEfficientNetmodels,achieving\na high dice score and AUC. The horizontal FL approach ensures privacy and facilitates large-scale clinical trials, addressing\ndata imbalances and demonstrating robust local model performance. Likewise, [66] proposed a HFL framework combined with\na ResNet18-based dual path DL model for lung nodule detection. This approach addresses privacy issues while training a global\nmodel across multiple institutions. The dual path ResNet18 architecture improves feature extraction and detection accuracy. The\nmethod demonstrates effective lung nodule detection while preserving patient data privacy.\nReference[104]presentsanewparadigmforcancerdetectionusingacombinationofVFL,AE,andXGBoostmethodswithina\ndistributed fog computing environment. This approach addresses challenges in digital healthcare such as security, execution delay,\nandaccuracy.Theproposedmulti-cancermulti-omicsclinicaldatasetlaboratories(MCMOCL)schemeintegratesmulti-omicsdata\n(RNA, miRNA, and methylation) for improved cancer prediction. The method achieved high accuracy, reduced processing delay,\nand enhanced security compared to existing models in heterogeneous fog cloud computing environments.\nCross-siloFLisanotherpopularmethodthatboosttheboundaryofcancerdetection.Forexample,Heidarietal[101]proposea\nnovelmethodforlungcancerdetection,addressingchallengesrelatedtodataprivacyandinter-hospitalcollaborationbyemploying\nblockchain-basedcross-siloFL.Theproposedmethodemployscapsulenetworksforlocallungcancerclassificationandintroduces\na data normalization technique to handle variability in CT data. Extensive experiments demonstrated the effectiveness of this\ntechnique, achieving a high accuracy. Similarly, [127], contributes by proposing a novel memory-aware curriculum cross-silo\nFL approach for breast cancer classification using mammography images. This method addresses the challenge of imbalanced\ndatasets, particularly the scarcity of positive samples in routine screenings, by controlling the order of training samples in the\nFL setting. This method prioritizes forgotten samples to improve local model consistency and overall global model performance.\nAdditionally, the method incorporates unsupervised domain adaptation to handle domain shifts while ensuring privacy. Agbley et\nal. This study demonstrates that the FL model performed comparably to a centralized learning model, with only minor differences\nin F1-Score. Agbley et al [102] proposed a novel approach for breast tumor classification that integrates different magnification\nfactors of histopathological images using a residual network and information fusion in a cross-silo FL framework. This method\naddressesthechallengeoflimitedpubliclyavailablemedicaldatabypreservingprivacyandenablingcollaborativemodeltraining.\nThe approach was evaluated using the BreakHis dataset, showing superior performance compared to centralized learning models.\nAdditionally,visualizationsforexplainableAIwereprovided,andthemodelsareintendedfordeploymentinhealthcareinstitutionsâ€™\nIoMT systems for timely diagnosis and treatment.\nOtherproposedmethodsarebasedonFSSLforidentifyingcancer.Forinstance,[74],suggestedtheFedPerlscheme,anFSSL\nmethodthatenhancesskinlesionclassificationusingpeerlearningandensembleaveragingtechniques.Thisapproachaddressesthe\nchallengeoflimitedannotateddatainthemedicalfieldbyallowingmodelstolearnfromeachotherthroughcommunity-basedpeer\nlearning and producing accurate pseudo labels. The peer anonymization technique preserves privacy and reduces communication\ncosts without adding complexity.\nMoreover, various aggregation techniques have been proposed and implemented to construct the global model on the central\nserver within the FL framework. For example, [107] introduces a federated Averaging (FedAvg) aggregation approach for breast\ncancer detection along with CNN. This method addresses privacy concerns associated with sharing patient data and the challenge\nof training on limited, localized datasets. The approach is tested on many large-scale datasets, achieving a high detection accuracy.\nThe study demonstrates that FedAvg can enhance detection accuracy while preserving data privacy, offering a robust and scalable\nsolutionforbreastcancerdiagnostics.Theworkin[106],introducedFedDropoutAvg,anaggregationapproachfortumordetection\nin histology images that integrates dropout techniques into both client selection and federated averaging processes. This method\nleverages dropout to mitigate overfitting and improve model generalization. Another aggregation technique proposed in [113]\nemploys federated post-deployment adaptation (FedPDA), which integrates FL to address distribution shifts in medical imaging\nmodels. This approach enables remote gradient exchange between the deployed model and source data by maximizing gradient\nalignmentbetweensourceandtargetdomains,facilitatingmoreeffectivelearning.FedPDAcustomizesthemodelfortargetdomains,\nenhancing performance in cancer metastasis detection and skin lesion classification. Addition, the study in [115] addresses the\nchallengeofdevelopingaccurateandprivateAImodelsforbreastcancerdetectionusingultrasoundimaging.Itproposesleveraging\nFLtomanagesensitivemedicaldatawhilepreservingprivacy.Toenhancemodelperformanceonnon-independentandidentically\ndistributed (non-IID) datasets, the study incorporates the federated proximal (FedProx) aggregation method, combined with a\nmodified U-Net model featuring attention mechanisms. FedProx mitigates heterogeneity in FL by introducing a proximal term\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 16 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nCentral server\nSelf-supervised learning\nFederated learning\n Initialization\nPredict results\nCentral \nserver\nMulti-task \nSSL\nGAN\nGAN\nGAN\nLocal servers\nLocal servers\nCenter 1\nCenter 2\nCenter N\nCenter 1\nCenter 2\nCenter N\nMulti-central \npseudo data\nPre-trained \nbackbone\nPseudo data\nFigure 9: The proposed SSL-FL-BT classification pipeline is designed for histopathological image analysis, which is widely used in\ncancer diagnosis. The scheme consists of two main stages: the SSL stage and the FL stage. During the SSL stage, a specially designed\nmulti-task SSL approach is applied to all pseudo images to pre-train the backbone network. In the FL stage, the pre-trained backbone\nserves as the initialization network for the FL-BT model [65].\nintothelocalobjectivefunction,ensuringstabilityandimprovingconvergence.Thisapproachresultedinaglobalmodelwithhigh\naccuracy for tumor segmentation, demonstrating the effectiveness of FedProx in addressing non-IID medical data and improving\nsegmentation performance. Building on this, [128] explored leveraging genomic big data for stomach adenocarcinoma detection\nusingAIandDL.Thestudyintroduces Fed_ANN11,anaggregationtechniquedevelopedwithinaFLframework,integratingnovel\nfeature extraction methods based on Electro-Ion interaction pseudo-potential values and Kidera factors.Fed_ANN11 demonstrates\nsuperior performance, achieving high testing accuracies in federated environments and showing significant improvements over\nexisting methods, all while maintaining strong data privacy and security.\nResearchershavesuggestedmethodsthatcombinebothFLandTLhavebeenreportedin[111,129].Thestudy[111]addresses\nthe challenge of prostate cancer detection by proposing a FL-based approach that ensures data confidentiality while using weight\naveraging to detect anomalies. The performance of a customized basic CNN model with three Conv2D layers, along with VGG19\nand Xception models, was compared in both centralized and decentralized (federated) settings. The results demonstrate that the\ndecentralized method achieves accuracy comparable to state-of-the-art centralized models, effectively balancing data privacy with\nhigh detection performance. Similarly,[129] report a novel FL framework for breast cancer segmentation that addresses data\ninconsistencies and privacy concerns. It employs random ROI and bilinear interpolation to augment data, and a U-Net model with\na pretrained VGG backbone. The Gaussian mixture model is applied to enhance segmentation quality by managing diverse data\ndistributionsandimprovingtumordetection.Experimentsshowthatthisapproach,usingFedAvgandfederatedbatchnormalization\n(BN), outperforms several state-of-the-art methods on five public breast cancer datasets.\nShapley values and game theory were utilized in [110] to develop a unique FL approach specifically designed for breast cancer\nprediction. Leveraging the Wisconsin Diagnostic Breast Cancer dataset, this approach enhances privacy and prediction accuracy\nby identifying key features and incentivization high-performing clients.\nIn addition to the previously discussed types of FL, other variations are tailored to specific applications, such as IoT. For\ninstance,[112]proposesalightweightandscalableIoTframeworkforskincancerdetection,leveragingedgeFLtoenhancereal-time\nprocessing and privacy at the edge. The framework supports integration with other computer vision models and features a mobile\napplication that detects skin cancer.\nOther FL approaches focus on securing the global model (aggregated model). For example, [116] investigated the integration\nof fully homomorphic encryption with secure FL using mammogram data from Belgian medical records. The research focuses on\nevaluatingmemoryconstraintswhenapplyingfullyhomomorphicencryptiontosensitivemedicaldata.Despitenotablelimitations\ninmemoryusage,theresultsdemonstratethatfullyhomomorphicencryptionmaintainscomparableperformanceintermsofROC\ncurves,showcasingitsrobustnessinsecureMLapplications.Thisapproachpreservesdataconfidentialityandenhancesthesecurity\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 17 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nTable 10\nSummary of FL in cancer diagnosis.\nRef Model used Images\ndataset Contribution Best perf.\n(%) Limitation\n[101] Blockchain-based FL Chest CT Proposed TL, Blockchain, FL, and CapsNets\nframework enhances lung cancer detection. Acc: 99.69 Blockchain increases computational over-\nhead.\n[104] FL, AE, XGBoost Omics data Integrates federated AE with XGBoost to de-\ntect multi-omics cancer. Acc: 98.00 Scalability of the scheme unaddressed.\n[105] FL Skin cancer Presents a secure FL approach for skin cancer\ndetection, mitigating data poisoning attacks.\nAcc: 95.20\nPre: 94.80\nRec: 96.10\nScalability issues with larger datasets.\n[107] CNN-FL Breast cancer Introduces a FL using CNNs for breast cancer\ndetection across diverse datasets.\nAcc: 98.90\nSen: 95.00\nSpe: 98.00\nData privacy management across feder-\nated networks.\n[108] CNN-FL Lung cancer\nDemonstrates FL with CNNs for scalable,\nprivacy-preserving lung cancer severity diagno-\nsis.\nAcc: 95.76 Privacy and data integration issues.\n[109] GBDTMO-FL Breast cancer\nGradient boosting decision tree-based mayfly\noptimization (GBDTMO) with FL for breast\ncancer diagnosis.\nAcc: 94.47\nRec: 98.52\nF1: 97.52\nROC: 96.32\nCommunication overhead, complexity in\nsynchronization.\n[110] FL, Shapley values Breast Cancer\nIntegrates shapley values, game theory, and\nhorizontal FL to enhance breast cancer pre-\ndiction.\nAcc: 94.73\nPre: 95.28\nRec: 95.48\nROC: 98.97\nChallenges scaling to larger datasets.\n[115] FL Breast cancer A novel FL integrates FedProx and Attention\nU-Net for precise breast cancer segmentation.\nAcc: 96.07\nSpe: 99.19\nF1: 70.76\nComputational overhead unaddressed.\n[117] FL-Attention Prostate cancer\nFederated attention-consistent learning inte-\ngrates FL and attention consistency to en-\nhance prostate cancer diagnosis and grading.\nAUC: 97.18 FL adds communication overhead.\n[120] Skin-net with FL Skin cancer Skin-net CNN with privacy-preserving FL for\nskin cancer.\nAcc: 98.30\nSen: 98.80\nSpe: 97.90\nComplexity of progressively private FL\nimplementation.\n[122] FL X-ray lung\nCompares sequential and ensemble model\n(EM)s in FL and centralized approaches for\ndisease detection.\nAcc: 99.00 Sequential models add computational\ncomplexity.\n[124] FL-ViT Breast dataset\nProposes a privacy-preserving, lightweight\nPLA model using ViT for breast cancer diag-\nnosis.\nAcc: 95.30\nRec: 99.80\nPre: 98.80\nDataset details lacking, scalability con-\ncerns.\n[125] FL-TL Lung cancer Developed a FL framework leveraging TL for\nrobust lung cancer detection.\nAcc: 91.03\nRec: 89.44\nAUC: 98.55\nPre: 98.80\nFL synchronization adds complexity.\n[126] DL-FL Liver CT Designed a robust Hybrid-ResUNet with FL for\naccurate liver tumor segmentation.\nDice: 94.33\nAUC: 99.65\nCommunication overhead affects perfor-\nmance.\n[127] Memory-aware curricu-\nlum FL Mammography\nIntegrated curriculum learning into federated\nsettings for improved consistency, and multi-\nsite breast cancer classification.\nAUC: 79.00 Increased computational complexity.\n[128] Fed_ANN11 Stomach Ade-\nnocarcinoma\nIntegrates generative AI and FL for sequence-\nbased stomach adenocarcinoma detection. Acc: 99.00 Communication overhead and latency is-\nsues.\n[130] CNN-FL Brain tumor Proposed CNN-based FL for brain tumor de-\ntection. Acc: 96.00 Data heterogeneity and communication\noverhead.\n[131] FL Breast cancer Federated YOLO-ResNet fusion ensures high-\naccuracy breast cancer detection.\nAcc: 98.73\nPre: 98.73\nRec: 98.73\nFL with multiple clients adds complexity.\nofexchangesbetweenparticipantsandthecentralserver.Thestudy[108]incorporateCNNintoFLtoenhancelungcancerdiagnosis.\nReference [114] incorporates client-specific AEs and hierarchical clustering to address the challenges posed by non-independent\nand identically distributed data across clients. This framework aims to enhance the effectiveness of FL in heterogeneous medical\ndata environments while preserving data privacy.\nDistinct from other methods, [117] introduced a federated attention-consistent learning framework designed to advance AI in\nmedical imaging by addressing challenges related to large-scale pathological images and data heterogeneity. Federated attention-\nconsistent learning enhances model generalization by ensuring attention consistency between local clients and the central server\nmodel. To further safeguard data, differential privacy is incorporated by adding noise during parameter transfers. The framework\nwas evaluated using a substantial dataset of prostate cancer images from various centers, demonstrating improved performance in\ncancerdiagnosisandGleasongrading(aggressivenessofprostatecancerassessment)comparedtoexistingmethods.Thisapproach\noffers a robust, privacy-preserving solution for training AI models in medical imaging.\n4.3. Transfer learning\nTL is a ML strategy designed to improve the performance of models on a target task by transferring knowledge from a related\nsource task, especially when labeled data for the target task is scarce. This technique capitalizes on pre-trained models that have\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 18 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nTable 11\nComparison of various TL techniques.\nTL type ğ·ğ‘† ğ·ğ‘‡ Labels in ğ·ğ‘‡ Objective function\nInductive Same/Diff Same/Diff Labeled Ì‚ğœƒğ‘‡ = arg minğœƒ îˆ¸ğ‘‡(ğ‘“ğ‘‡(ğœƒ,ğ·ğ‘‡),ğ‘‡ğ‘‡)\nTransductive Diff Diff Unlabeled Ì‚ğœƒğ‘‡ = arg minğœƒ îˆ¸ğ‘†(ğ‘“ğ‘†(ğœƒ,ğ·ğ‘†),ğ‘‡ğ‘†) +ğœ†â‹… îˆ¸ğ‘‡(ğ‘“ğ‘‡(ğœƒ,ğ·ğ‘‡),ğ‘‡ğ‘‡)\nUnsupervised Diff Diff Unlabeled Ì‚ğœƒğ‘‡ = arg minğœƒ îˆ¸ğ‘†(ğ‘“ğ‘†(ğœƒ,ğ·ğ‘†),ğ‘‡ğ‘†) +ğœ†â‹… îˆ¸ğ‘ˆ(ğ‘“ğ‘ˆ(ğœƒ,ğ·ğ‘‡))\nbeen developed using large and diverse datasets, which can then be fine-tuned to specific problems in domains with limited data\nsuch as medical imaging [132, 13], 3D data representation [133], and NLP [134] tasks. The rationale behind TL is that features\nlearned from large-scale tasks, such as detecting edges, shapes, or general patterns in images, can be beneficial for smaller, related\ntasks without starting the learning process from scratch. The essential mathematical foundation of TL lies in the optimization of\na loss function that allows the model to learn and adapt to new tasks. The general equation for loss function optimization in TL is\ngiven by Equation 12:\nğ¿(ğœƒ) = 1\nğ‘\nğ‘âˆ‘\nğ‘›=1\nğ¿(ğ‘¦ğ‘–),ğ‘“(ğ‘¥ğ‘–; ğœƒ) (12)\nWhere N denotes the number of samples,ğœƒ represents the model parameters,ğ‘¦ğ‘– is the true label, andğ‘“(ğ‘¥ğ‘–; ğœƒ) is the modelâ€™s\nprediction. This loss function is minimized using optimization algorithms like stochastic gradient descent (SGD), adjusting the\nmodelâ€™sweightsforbettergeneralizationonthetargettask.TLencompassesvariousapproaches,includinginductive,transductive,\nand unsupervised TL, allowing models to generalize better to new tasks while improving training efficiency. Regularization\ntechniques like dropout help mitigate overfitting during fine-tuning, and feature extraction methods utilize pre-trained models\nas fixed feature extractors for simpler models. Knowledge distillation further enhances efficiency by training smaller models to\nreplicate the performance of larger ones [134, 14, 135, 133]. Table 11 provides a summary of the types of TL, including inductive,\ntransductive,andunsupervisedTL,highlightingtheirdistinctcharacteristicsandkeydifferences.Table12summarizesseveralstate-\nof-the-artstudiesintermsoftheAImodelsused,datasets,contributions,limitations,andbestperformancesachieved.Additionally,\npopular pre-trained models are comprehensively reviewed in [134, 14, 135]. Figure 11 illustrates an example of the proposed\napproach, which involves unsupervised pre-training of a convolutional autoencoder (CAE) to extract features from CT images,\ncombined with the TL technique to enhance lung cancer prediction.\nInductive TL\nFine-tuning \nLearning \nusing \nprivileged \ninformation\nMultistage TL\nAdaptive \ndomain TL\nSemi-\nSupervised\n deep TL\nFeature \nrepresentation \ntransfer\nOnline TL\nSource \ntask1\nSource \ntask2Target \ntask\nTL\nFigure 10: TL types used in the diagnosis of cancer and their respective related work. Inductive TL [136]; Fine-tuning [137, 138, 63];\nLearning using privileged information [64]; Multistage TL [139]; Adaptive domain TL [140]; Semi-supervised deep TL [69]; Feature\nrepresentation TL [141]; Online TL [142].\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 19 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nFigure 11: Example of a proposed approach, which involves unsupervised pre-training of a CAE to extract features from CT images,\nfollowed by an end-to-end classifier for predicting the mutation status of the EGFR.This latter is a transmembrane protein involved in\ncell signaling pathways that regulate cell proliferation, survival, and differentiation; mutations in this receptor are commonly associated\nwith non-small cell lung cancer. TL enables the reuse of the encoder, pre-trained on unlabeled data from theLIDC-IDRI database, as\na feature extractor for EGFR mutation classification in theNSCLC-Radiogenomics database [55].\nRecently, numerous AI methods based on TL have been proposed to enhance cancer diagnosis and detection as summarized\nin Figure 10, with most relying on inductive learning and fine-tuning techniques. For instance in [136], the authors addressed the\ncritical issue of breast cancer detection and classification by proposing a novel CAD method utilizing ResNet18, ShuffleNet, and\nInception-V3Net in an inductive TL mode. The method is tested on the BreakHis dataset, with final image dimensions of 224Ã—224\nfor ResNet18 and ShuffleNet, and 299Ã—299 for Inception-V3Net. This approach demonstrates high accuracy in both binary and\nmulti-class classification. Similarly, [143] proposed a novel DL framework for detecting and classifying breast cancer in cytology\nimages using inductive TL. Pre-trained CNN architectures, including GoogLeNet, VGGNet, and ResNet, are employed for feature\nextraction, followed by classification of malignant and benign cells. Saber et al. [144] contributed by developing a DL model\nbased on inductive TL to assist in the automatic detection and diagnosis of breast cancer. This study utilizes pre-trained CNN\narchitectures such as Inception V3, ResNet50, VGG-19, VGG-16, and Inception-V2 ResNet and evaluates the model on the MIAS\ndataset. The findings demonstrate that VGG16 is the most effective architecture for breast cancer diagnosis using mammographic\nimages. Reference [145] introduced a DL model for detecting skin cancer in its benign and malignant stages using inductive\nTL. The model builds upon the pre-trained VGG16 architecture by adding a flattened layer, two dense layers with LeakyReLU\nactivation, and a final dense layer with sigmoid activation to improve accuracy. This model aims to assist dermatologists in early\nskin cancer diagnosis. Continuing diagnois scheme cancer, [146], proposed an automatic detection system using the sparrow\nsearch algorithm (SpaSA) for hyperparameter optimization. This study employs five U-Net models (U-Net, U-Net++, Attention\nU-Net, V-net, Swin U-Net) for segmentation and eight pre-trained CNN models (VGG16, VGG19, MobileNet variants, NASNet\nvariants) for classification. For segmentation, U-Net++ with DenseNet201 achieved the best results on â€œskin cancer segmentation\nand classificationâ€ dataset, while Attention U-Net with DenseNet201 performed best on the â€œPH2â€ dataset. MobileNet pre-trained\nmodelsachievedthehighestaccuracyontheâ€œISIC2019and2020Melanomaâ€andâ€œHAM10Kâ€datasets,andMobileNetV2excelled\nwiththeâ€œskindiseasesimageâ€dataset.Theproposedmethodiscomparedwith13relatedstudies,demonstratingitseffectivenessin\nskincancerdiagnosis.AbreastcancerdiagnosticsystemthatintegratesdeepTLwithIoTandfogcomputingwasproposedin[147].\nUsing mammography images from the Cancer Imaging Archive, the system employed ResNet50, InceptionV3, AlexNet, VGG16,\nand VGG19 architectures alongside a support vector machine (SVM) classifier. Fog computing played a crucial role in enhancing\nprivacy, reducing server load, and improving overall system efficiency.\nReference [138] applies deep TL fine-tuning of a pre-trained model combined with hybrid optimization, using adaptive\nmoment estimation (adam), RMSprop, and SGD optimizers, to improve detection and diagnosis of oral cancer. This study utilizes\nmodels such as ResNet50, MobileNetV2, VGG19, VGG16, and DenseNet on both real-time and histopathologic datasets. Image\npreprocessing techniques, including Gaussian blur and morphological operations, are used to prepare the data. Likewise, [63]\npresented a fine-tuned model to classify three widespread lung cancer types: Squamous cell carcinoma, large cell carcinoma,\nand Adenocarcinoma. The models used include VGG16, ResNet152V2, MobileNetV3 (small and large), InceptionResNetV2,\nand EfficientNetV2. InceptionResNetV2 emerged as the best model with the highest performance metrics (accuracy, precision,\nAUC, and F1-score) in classifying the lung cancers from normal samples. Study [148] presented a novel approach to skin lesion\nclassification by combining VGG16 and VGG19 architectures into a modified AlexNet network. This combined model was fine-\ntuned on a dermatology dataset of 2,541 images without relying on data augmentation techniques. Dropout was used to address\noverfitting, and the model was evaluated using K-fold cross-validation. The proposed method achieved a significant improvement\nin classification accuracy. In a different study [149] contributed by proposing a real-time data augmentation-based fine-tuning TL\nmodel for breast cancer diagnosis using histopathological images. It compares two popular models, InceptionV3 and Xception,\ntrained on the BreakHis dataset. The findings show that the Xception model, when trained using fine-tuning TL from ImageNet\nweights,achievedhighaccuracy,outperformingmodelstrainedfromscratchandsurpassingpreviousstate-of-the-artresultsonthe\nBreakHisdataset.Deepaketal.[150]suggestedabraintumorclassificationsystemusingfine-tuningTLbyemployingapre-trained\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 20 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nGoogLeNettoextractfeaturesfromMRIimagesofglioma,meningioma,andpituitarytumors,andintegratesprovenclassifiersfor\naccurate classification.\nTable 12: Summary of TL in cancer diagnosis.\nRef Model(s) used Image dataset Contribution Bestresult(%) Limitation\n[136] DNN-TL BreakHis Fine-tuning multiple DNN models with TL for ac-\ncurate breast cancer classification.\nAcc: 97.81\nPre: 97.65\nSen: 97.65\nSingle dataset may cause overfitting.\n[137] ResNet-50 Herlev, UIC TL enhances cervical cancer detection using Pap\nsmear images. Acc: 92.03 Requires significant computational power.\n[138] DL-TL Oral cancerous,\nhistopathologic\nEfficientoralcancerdiagnosisusingDenseNetwith\nfine-tuning and hybrid optimization.\nAcc: 95.41\nLoss: 0.70\nHybridoptimizationaddscomputationalcom-\nplexity.\n[139] EfficientNetB2 Ultrasound breast\ncancer\nProposed multistage TL method significantly im-\nproves breast cancer ultrasound classification.\nAcc: 99.00\nF1: 98.90\nExtensive pre-processing may introduce bi-\nases.\n[141] AlexNet Histopathology TLforlungandcolonhistopathologyimageclassi-\nfication. Acc: 98.40 Contrast enhancement method limits class\ngeneralizability.\n[142] Online TL Thyroid cancer Online TL distinguishes thyroid nodules using ul-\ntrasound.\nAUC: 98.00\nSen: 98.70\nSpe: 98.80\nRelies on specific datasets.\n[143] CNN-TL Breast cancer DLframeworkforbreastcancerclassificationusing\nTL. Acc: 97.67 Requires substantial computational resources.\n[144] DL-CNN Mammographic TL for breast cancer detection using pre-trained\nCNNs.\nAcc: 98.96\nSen: 97.83\nAUC: 99.50\nRelies on single dataset.\n[145] DL-TL Skin cancer\nFine-tuned VGG16 with added layers, data aug-\nmentation, and hyperparameter optimization for\nskin cancer diagnosis.\nAcc: 89.09 Model complexity increases training time.\n[146] U-Net PH2, ISIC,\nHAM10K\nFine-tuningCNNswithSpaSAoptimizerimproves\nskincancersegmentation,classification,anddetec-\ntion efficiency.\nAcc: 98.83\nAUC: 99.45 Model complexity adds challenges.\n[147] CNN-TL Mammography Integrates TL with fog computing for breast cancer\ndiagnosis.\nAcc: 97.99\nPre: 99.51\nSpe: 80.08\nHigh computational power needed.\n[148] VGG16,\nVGG19, TL\nDermatology\ndataset\nIntroduces a fine-tuned acTL approach combin-\ning VGG16/VGG19 architectures for superior\nmelanoma detection.\nAcc: 98.18 Dataset size limits generalization.\n[149] InceptionV3,\nXception BreakHis ProposedaTL-basedfine-tuningandreal-timeaug-\nmentation model for breast cancer classification.\nAcc: 90.86\nSen: 92.51\nSpe: 80.85\nLimited data increases overfitting risk.\n[150] TL-CNN MRI TL for brain tumor classification using MRI. Acc: 98.00 Dataset may not cover all tumor types.\n[151] TL-MLP-SVM Breast cancer Intelligent breast cancer diagnosis with ensemble\nTL models.\nAUC: 94.70\nRec: 85.80 Data quality limits real-time use.\n[152] ResNet-50,\nDenseNet Breast cancer Automated breast cancer detection using TL. Acc: 90.96\nF1: 94.11 Larger dataset required for validation.\n[153] CNN-TL Cervicalhistopatho-\nlogical\nProposedafine-tuneddeepTLframeworkforinter-\npretable and automated cervical histopathological\nimage diagnosis.\nAcc: 97.42\nSpe: 98.93\nSen: 95.88\nRequires substantial computational resources.\n[154] TL Mammogram TL for breast lesion diagnosis with hybrid DL. Acc: 98.80 Computational complexity due to the multi-\nstep pipeline.\n[155] Hybrid TL CE-MRI TL model for brain tumor classification using\ncontrast-enhanced (CE)-MRI.\nAcc: 99.51\nSen: 98.90 Relies on specific CE-MRI dataset.\n[156] VGG16 JUMC IVNet uses TL for breast cancer gradingt. Acc: 97.00 Despite employing TL, the relatively small\ndataset size could still cause overfitting.\nAnotherTLtechniqueconsistofmultistageTLassuggestedin[139]dedicatedforultrasoundbreastcancerimageclassification.\nItemploysthreepre-trainedmodelsEfficientNetB2,InceptionV3,andResNet50â€”alongwithadam,Adagrad,andSGDoptimizers.\nStudy [140] employed domain adaptation TL techniques for early lung cancer diagnosis using multi-omics data. The approach\ncombines CNNs and convolutional AEs to handle the challenges of high-dimensional, low-sample-size, and noisy omics data. The\nAEsreducedimensionalitytoenhancemigrationlearning,whiletheCNNmodelistrainedonboththeoriginalandlabeleddatasets.\nThe proposed method outperforms five other ML models, demonstrating superior performance in classifying and predicting lung\ncancerfromgenedatasets.Similarlyin[157],wherethedomainadaptationapproachhavebeenusedtoaddresstheissueoflimited\nlabeled data in medical imaging. Traditional TL methods, which often use pretrained models from datasets like ImageNet, may\nbe ineffective due to feature mismatches between natural and medical images. To overcome this, the authors propose training DL\nmodels on large unlabeled medical image datasets before fine-tuning them on smaller labeled datasets. They also introduce a new\ndeep CNN model that incorporates recent advancements in the field. The proposed method has been empirically validated through\nexperiments on skin and breast cancer classification tasks, showing significant performance improvements.\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 21 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nThe scheme proposed in [69] is a semi-supervised TL framework for diagnosing benign and malignant pulmonary nodules\nin chest CT images. This approach combines TL with a pre-trained classification network and an iterated feature-matching-based\nsemi-supervisedmethodtohandlethechallengeoflimitedandimbalancedpathologicaldatasets.Unlabeleddatasetcontains14,735\nnodules from 4,391 subjects and are used by the semi-SL framework. However, the work in [154] proposed a TL-based feature\nextraction approach for breast lesion diagnosis using mammograms. It addresses two key challenges: enhancing input data by\ngenerating pseudo-colored images using CLAHE and pixel-wise intensity adjustment, and mitigating multicollinearity in high-\nlevelfeaturesthroughanovelLR-PCAmethod.Theproposedsystemachieveshighperformancewithaccuraciesdemonstratingits\neffectiveness for breast cancer diagnosis.\n4.4. Transformer-based learning\nTransformers have revolutionized computer vision by modeling long-range dependencies through self-attention mechanisms\n[77], unlike CNNs, which rely on local receptive fields. Since cancer diagnosis is image-based, the Transformer architecture em-\nployedinthisdomainisViT,whichenablesmoreeffectivefeatureextractionfortumordetection,segmentation,andclassification.\nVariousViTarchitectureshavebeendesignedtoaddresschallengessuchascomputationalcomplexity,spatialinformationloss,and\nmulti-scalefeaturerepresentation.ViTreplacestraditionalCNNsbydividingimagesintopatchesandprocessingthemassequences,\nsimilar to how words are handled in NLP [16, 20].\nIn general, ViT divides an image of sizeğ»Ã— ğ‘Š Ã— ğ¶ into ğ‘ = ğ»Ã—ğ‘Š\nğ‘ƒ2 patches, where each patch is flattened into a vector and\nlinearly transformed using a learnable embedding matrixğ¸ âˆˆ â„(ğ‘ƒ2â‹…ğ¶)Ã—ğ·, resulting in patch embeddings:\nğ‘§ğ‘–\n0 = ğ‘¥ğ‘– â‹… ğ¸ (13)\nfor eachğ‘–-th patch. To retain spatial information, positional encodingsğ‘ğ‘– are added:\nğ‘§ğ‘–\n0 = ğ‘§ğ‘–\n0 + ğ‘ğ‘–. (14)\nThese embeddings pass through the Transformer encoder, which applies the multi-head self-attention (MHSA) mechanism:\nAttention(ğ‘„,ğ¾,ğ‘‰ ) =softmax\n(\nğ‘„ğ¾ğ‘‡\nâˆš\nğ‘‘ğ‘˜\n)\nğ‘‰ (15)\nwhereğ‘„,ğ¾,ğ‘‰ represent the query, key, and value matrices, andğ‘‘ğ‘˜ is the dimension of the keys. The MHSA operation aggregates\nmultiple attention heads, such that: MHSA(ğ‘‹) =Concat(head1,â€¦ ,headâ„)ğ‘Šğ‘‚. A feed-forward network (FFN) is applied to each\ntoken:\nFFN(ğ‘¥) = max(0,ğ‘¥.ğ‘Š1 + ğ‘1).ğ‘Š2 + ğ‘2, (16)\nwhereğ‘Š1,ğ‘Š2 are learnable weight matrices andğ‘1,ğ‘2 are biases. A special classification token (CLS token) is added to the patch\nsequence, and after the final Transformer layer, the CLS token is passed to a multi-layer perceptron (MLP) classifier:\nğ‘¦= MLP(ğ‘§CLS\nğ¿ ), (17)\nwhereğ‘§CLS\nğ¿ is the processed CLS token.\nWhile the standard ViT structure has shown competitive performance, several variants have been developed to enhance\nefficiency,improvelocalandglobalfeatureextraction,andincorporateconvolutionalpriors.Table13summarizeskeyViTvariants,\ntheiradvantages,applicationsincancerdiagnosis,andlimitations.Thesevariantsaddresssomeofthecomputationalandstructural\nchallenges of ViT, making them more suitable for tasks such as medical image analysis, where feature locality and efficiency are\ncritical. A comprehensive overview of these architectures can be found in [20, 77].\nAmongthetypesofViT,thereisaswintransformer,whichisatypeofViTdesignedtoefficientlyhandlehigh-resolutionvisual\ndataandcomplexvisiontasks.UnliketheoriginalViT,whichtreatsanentireimageasasequenceofpatches,theswinTransformer\nintroduces a hierarchical structure and operates on non-overlapping windows (patches). This structure progressively increases the\nmodelâ€™sabilitytocapturebothlocalandglobalcontextualinformation[58].ThecompactViTisanothervariantoftheViTdesigned\ntoaddresssomelimitationsoftheoriginalViT,particularlyitsneedforlargeamountsofdataandlackofinductivebiasescommonly\nfound in CNNs. The compact ViT introduces mechanisms like sequence pooling and multi-scale features to improve the modelâ€™s\nefficiency and performance, especially when working with smaller datasets or when computational resources are limited [158].\nFigure12 provides thetaxonomy ofdifferent typesof ViT,including somecommonly usedmodels andothers proposedin specific\nstudies.\nMany suggested strategies for cancer detection are based on Transformers. In particular, the study in [169] compares different\nViT models including pooling-based vision transformer (PiT), convolutional vision transformer (CvT), CrossFormer, CrossViT,\nNesT, MaxViT, and separable vision transformer (SepViT) for classifying breast cancer in digital histopathology images. MaxViT\nwas identified as the best-performing model. [170] proposed a ViT model designed for the detection and classification of prostate\ncancer by first extracting patches from ROI in the whole slide images and then applying the ViT model for classification. The\nclassified patches are subsequently scored and graded according to the Gleason system. Mali et al [158] investigate the use of ViT\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 22 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nTable 13\nComparison of ViT variants in cancer diagnosis.\nViT variant Advantages Use case Limitations\nSelf-Supervised ViT Learns representations without labeled data,\nimproves generalization\nDetects rare cancer types with lim-\nited labeled data\nRequires extensive computational resources for\npretraining\nMulti-Axis ViT Processes spatial axes separately for better\nfeature extraction\nEnhances tumor boundary delin-\neation in histopathology images\nIncreased model complexity affects real-time\nprocessing\nSwin Transformer Uses shifted windows to reduce computation\nand improve efficiency\nEfficient large-scale whole-slide im-\nage analysis Limited ability to model global dependencies\nTokens-to-Token ViT Preserves spatial relationships via progressive\ntokenization\nImproves segmentation of tumor re-\ngions in MRI scans\nHigher memory requirements due to hierarchi-\ncal tokenization\nConvolutional ViT\n(CvT)\nIncorporates convolutional layers for local fea-\nture extraction\nBetter texture recognition for skin\nand breast cancer detection\nPartially loses global receptive field advantage\nof ViTs\nCrossFormer & Cross\nViT Multi-scale feature fusion improves robustness Enhances multi-modal imaging (e.g.,\nPET-CT fusion for cancer staging)\nComputationally intensive due to cross-scale\nprocessing\nSeparable ViT Reduces computational complexity by factor-\nizing attention\nSpeedsupinferenceforreal-timecan-\ncer detection systems\nTrade-off in accuracy due to reduced interac-\ntion between features\nMulti-View ViT Aggregates different perspectives for robust\nrepresentations\nAssists in multi-angle breast cancer\nmammography analysis High data requirements for multi-view learning\nBoosted ViT Applies ensemble principles to enhance predic-\ntion accuracy\nImproves classification of rare cancer\nsubtypes Training complexity increases significantly\nLocal-Global ViT Balances fine-grained and high-level feature\nextraction\nHelps identify tumor microenviron-\nments in histopathology Computationally expensive for large images\nPooling-Based ViT Reduces dimensionality using pooling mecha-\nnisms\nSpeeds up processing in large-scale\ncancer imaging datasets May lose fine-grained spatial information\nNested Hierarchical\nTransformer\nMulti-levelfeatureextractionenhancescontex-\ntual understanding\nImproves differentiation of overlap-\nping cell structures in pathology\nIncreased model depth leads to high training\ntime\nfocusingondatapreprocessingandtheTransformermodelsareutilizedfordetectingcolorectalcancer.Thisstudyimplementsthree\ntransformer-basedmodelsViT,CvT,andcompactconvolutionalTransformer(CCT)toclassifyhistopathologicalimages,exploring\ntheimpactofdifferentpatchsizesandmodelconfigurations,demonstratingthatViTsoutperformtraditionalmethodsinaccuracy.It\nprovides insights into optimization techniques specific to colorectal cancer detection. Similarly, the research in [171], explored the\nuse of an ensemble of ViT for classifying brain tumors from MRI scans. Pretraining and fine-tuned four ViT models is performed\nonImageNet.ThedatasetcomprisedT1-weightedcontrast-enhancedMRIsliceswithmeningiomas,gliomas,andpituitarytumors.\nIt shows that combining multiple ViT models enhances classification accuracy and reliability compared to single ViT models\nand traditional methods. Pachetti et al [172] evaluate the effectiveness of 3D ViT in predicting prostate cancer aggressiveness. It\ndemonstratesthat3DViTscaneffectivelycaptureandanalyzespatialinformationin3Dmedicalimages,potentiallyoutperforming\ntraditional 2D approaches and 3D CNN. This study optimized 3D Vision Transformer models for prostate cancer aggressiveness\nprediction by selecting five key MRI slices per lesion, harmonizing pixel dynamics, and rescaling and center-cropping images to\nfocus on the prostate gland. Ayana et al. [162] investigated the use of a novel type of Transformer called ViTCol, a boosted vision\nTransformer model specifically designed for classifying endoscopic pathological findings. Additionally, they proposed PUTS, a\nSwin-Unet transformer-based model for polyp segmentation. Both models represent significant advancements in classification\nand segmentation tasks, which are critical for early CRC diagnosis. Their findings demonstrate that these enhanced ViTs can\nsignificantly improve detection performance and provide valuable insights in pathology. Sun et al. [173] propose thyroid cancer-\nViT,whichintegratescontrastivelearningtoenhancetheclassificationaccuracyofthyroidnodulesinultrasoundimages.Themain\nissue addressed is the lack of distinguishing features between benign nodules, particularly those classified as TI-RADS level 3 and\nmalignant nodules, which can lead to diagnostic inconsistencies, overdiagnosis, and unnecessary biopsies. The thyroid cancer-ViT\nmodel leverages ViTâ€™s capability to capture global features and contrastive learning to reduce the representation distance between\nnodules of the same category, improving the consistency of global and local feature representations. Similarly, Chen et al. [174]\ndemonstrated that transformer-based models improve breast cancer diagnosis by utilizing unregistered multi-view mammograms\n(CC and MLO) from both sides (right and left breasts). The outputs are concatenated and processed through global Transformer\nblocks to jointly learn patch relationships across the four images. The model was trained and evaluated on a balanced dataset of\nmammogram sets, comprising malignant and normal cases, and showed that this approach significantly outperforms traditional\nmethods in handling unregistered images.\nSeveralstudies[175,176,177,178,179]havecombinedViTwithTLtoenhancecancerdetection.Yangetal.[175]developed\namodelforskincancerclassification,wherethenetworkwaspretrainedontheImageNetdatasetandfine-tunedontheHAM10000\ndataset.Ayanaetal.[176]focusedonbreastcancermassclassification,introducingthreeViT-basedTLarchitecturespretrainedon\nImageNet and evaluating their performance using ultrasound and mammogram datasets. The comparative analysis demonstrated\nthat ViT-based TL achieved superior performance. Similarly, Nejad et al. [177] proposed a hybrid DL framework combining ViTs\nand CNNs with TL for improved lung cancer detection. Their hybrid ViT model effectively addressed the challenges of lung\nnodule detection by extracting features from chest CT images to classify nodules as normal, benign, or malignant. Tested on a\ndataset of CT images, the model achieved high accuracies in training, validation, and testing, demonstrating superior performance\ncompared to existing methods. Hossain et al. [178] explored advanced methods for brain tumor detection and classification using\nMRI images. The study addressed the challenges of inconsistent diagnostic results among specialists and emphasized the need for\nreliablemulti-classtumorclassification.ItevaluatedseveralDLarchitectures,includingVGG16,InceptionV3,VGG19,ResNet50,\nInceptionResNetV2,andXception.AnewTL-basedmodel,IVX16,wasproposedbycombiningfeaturesfromthebest-performing\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 23 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nSelf -\nsuperv ised\nViT\nMulti-axis\nViT\nSwin\ntransf ormerT okens-\nto-\ntoken ViT\nConv olutional\nViT\nCross\nFormer\nCross\nViT\nSeparable\nViT\nMulti-v iew\nViT\nBoosted \nViT\nLocal-\nglobal\nViT \nPooling-\nbased\nViT\nNested\nhierarchical\ntransf ormer\nFigure 12: ViT types used in the diagnosis of cancer. Separable ViT [159]; Cross-validation ViT [160]; Multi-Axis ViT [161]; Boosted\nViT [162]; Swin Transformer [163]; Tokens-to-token ViT [164]; Local-global ViT [165]; Self-supervised ViT [166]; CNN-ViT [167];\nCrossFormer [168]; CrossViT [169]; and nested hierarchical Transformer [169].\narchitectures. Tested on a dataset of 3264 images, the model demonstrated improved classification performance. Additionally,\nexplainable AI (XAI) was employed to assess model performance, while ViT models were compared with other TL methods and\nEMs, highlighting their effectiveness.\nSeveral frameworks combined ViT with CNN as an efficient solution for detecting various types of cancer. Zhou et al. [180]\nintroducedahybridarchitecture,RFIA-Net,forclassifyingstageImulti-modalityoesophagealcancerimages.RFIA-Netleveraged\nthelocalmodelingcapabilitiesofCNNsandtheglobalinformationextractionofViTs,enhancedbyastructuralreparameterization\nstrategy. The architecture included a feature extraction module and a feature enhancement module, which exchanged information\nbetween branches to improve performance. An asymmetric fusion module further strengthened feature relationships through\nspatial translation and channel swapping. Tested on the XJMU-XJU dataset, RFIA-Net achieved high performance. Xin et al.\n[181] proposed an enhanced ViT model, SkinTrans, for classifying skin cancer from dermoscopic images. Traditional CNN-based\nmethods,whileeffective,struggledtoextractfeaturesfromlarge,criticalregionsinimages.Toaddressthis,SkinTransutilizedself-\nattentionmechanismstofocusonsignificantfeatureswhilesuppressingnoise.Themodelincorporatedmulti-scalepatchembedding,\noverlapping sliding windows, and contrastive learning to enhance classification accuracy. Similarly, Zhang [182] developed multi-\nstage hybrid transformer (MSHT), a hybrid model that combined the local spatial feature extraction capabilities of CNNs with the\nglobal feature capture and long-range dependency handling of Transformers. The MSHT architecture integrated a CNN backbone\nto extract multi-scale local features, guiding the Transformer in capturing global features. This approach enabled the model to\neffectivelyleveragebothlocalandglobalinformation.Lastly,Tabatabaeietal.[167]exploredanadvancedframeworkforclassifying\nbraintumorsinMRIimagesusingahybridarchitecturecombiningself-attentionunitsandCNNs.Theirproposedmodelenhanced\nclassification accuracy by combining local features extracted by CNNs with global features captured by Transformers through a\ncross-fusionstrategy.Additionally,thestudyintroducedanimprovedCNNarchitecture,iResNet,optimizedfordistinguishingtumor\nfeatures in MRI images.\nTokens-to-token vision transformers (T2T-ViT) also attracted significant attention from researchers in the field of cancer\ndiagnosis. For instance, Yin et al. [183] introduced a novel lightweight architecture, Pyramid T2T-ViT, to address challenges in\nclassifying histopathological thyroid images. Traditional methods using CNNs struggled with high-resolution whole slide images\nduetoincreasedmodelparametersandcomputationalcomplexity.ThePyramidT2T-ViTcombinedT2T-ViTwithanimagepyramid\napproachtobalancelocalandglobalfeatureseffectivelywhilereducingcomputationaldemands.Themodelincorporatedafeature\nextractor to minimize parameters and employed multiple receptive fields to enhance classification accuracy. In the same manner,\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 24 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nZhaoetal.[164]tackledthechallengesofcervicalcancerscreening,suchaslimitedpublicdatasets,imbalancedclassdistribution,\nand varying image quality. They proposed a cervical cell image generation model, CCG-taming Transformers, to create high-\nquality,balanceddatasets.Theresearchersenhancedthemodelâ€™sencoderwithSE-blocksandmulti-Res-blocksforimprovedfeature\nextraction and introduced a normalization layer to optimize data processing. Additionally, SMOTE-Tomek links were used to\nbalance the dataset, and T2T-ViT combined with TL was employed for classification. The model was validated on three public\ndatasets, demonstrating its effectiveness.\nTable 14: Summary of Transformer-based cancer diagnosis.\nRef Model(s) used Image dataset Contribution Bestresult(%) Limitation\n[162] ViT Colorectal\npathology\nEnhanced ViT improves colorectal cancer early detec-\ntion.\nAUC: 99.99\nMCC: 90.48\nModel complexity increases computational\ndemands.\n[164] T2T-ViT Cervical cancer Hybrid T2T-ViT model improves cervical cancer classi-\nfication.\nAcc: 99.98\nSen: 98.34 High computational complexity.\n[167] ATM-CNN Brain tumor MRI Combines attention Transformers and CNN for brain\ntumor classification. Acc: 99.30 High computational resource requirements.\n[169] ViT Breast cancer\nhistopathology ViT models compared for breast cancer classification. Acc: 92.12 Study only compares ViT models.\n[170] ViT Prostatetissueslides ViT improves prostate cancer detection from slide im-\nages.\nPre: 99.00\nF1: 94.00 High computational resources needed.\n[171] ViT Brain tumor MRI\nscans\nEnsembleViTsimprovebraintumorclassificationaccu-\nracy.\nAcc: 98.70\nSen: 97.78\nEnsemble models increase computational\ncomplexity.\n[172] 3D ViT 3D MRI prostate\nscans\n3DViTimprovesprostatecanceraggressivenesspredic-\ntion.\nAUC: 77.50\nSpe: 75.00 High computational cost for training.\n[173] ViT-CL Thyroid nodule ul-\ntrasound\nCombinesViTwithcontrastivelearning(CL)forthyroid\nclassification. Acc: 86.90 High computational demands for CL.\n[174] Multi-view\nViT Breast cancer Multi-viewViTimprovesfeaturemanagementincancer\ndiagnosis.\nAUC: 81.40\nPre: 79.70 Depends on availability of multi-view data.\n[175] ViT-TL Skin cancer ViT-TL enhances skin cancer classification. Acc: 80.5 Requires significant computational resources.\n[176] ViT-TL Breast mass ViT-TL improves breast mass classification. AUC: 100 Depends on diverse diagnostic modalities.\n[177] ViT-TL Lung cancer Hybrid ViT-TL improves lung cancer diagnosis accu-\nracy. Acc: 99.09 Hybrid model increases computational com-\nplexity.\n[178] ViT-EM-TL Brain tumor EnsembleViTsimprovebraintumorclassificationaccu-\nracy. Acc: 96.94 Requires diverse imaging data.\n[180] ViT-CNN Oesophageal cancer Enhances oesophageal cancer diagnosis using ViT +\nCNN.\nAcc: 99.02\nAUC: 99.73\nComplex model increases computational de-\nmands.\n[181] SkinTrans Skin cancer Improved Transformer network enhances skin cancer\nclassification. Acc: 94.30 High computational demands.\n[182] MSHT Pancreatic cancer MSHT improves pancreatic cancer diagnosis using\nTransformers.\nAcc: 95.68\nSpe: 96.95\nNPV: 96.35\nIncreased computational complexity.\n[184] ViT Colorectal histology ViT predicts biomarkers from colorectal histology im-\nages. Sen: 99.00 Limited applicability to other data types.\n[185] ViT-DeiT Breast\nhistopathology\nViT+data-efficientimagetransformers(DeiT)improve\nbreast cancer classification.\nAcc: 98.17\nPre: 98.18\nF1: 98.12\nHigh computational resources required.\n[186] ViT-CNN Skin lesion Compares ViT and CNN for skin lesion segmentation. Acc: 92.11\nDice: 89.84\nHigh computational demands for both meth-\nods.\n[187] ViT-CNN Mammography IntegratingViTwithCNNimprovesbreastcancerdetec-\ntion. Acc: 99.22 Increased computational complexity.\n[188] LCDEiT Brain tumor MRI\nlinear complexity data-efficient image transformer\n(LCDEiT) reduces Transformer model complexity in\nbrain classification.\nAcc: 98.11\nF1: 93.69 Requires extensive tuning for optimal results.\n[189] YOLOv8-DeiT Brain tumor Combines you only look once version 8 (YOLOv8) and\nDeiT for brain tumor detection.\nAcc: 100\nF1: 100 High computational demands.\n[190] Swin Skin lesion Swin Transformer improves multiclass skin lesion clas-\nsification accuracy.\nAcc: 97.20\nSpe: 98.00 Requires extensive computational resources.\n[191] Swin -MLP Brain tumor MRI Hybrid swin Transformer and multilayer perceptron\n(MLP) improves brain tumor diagnosis.\nAcc: 99.92\nRec: 99.92\nSignificant computational resource require-\nments.\n[192] Ensemble\nswins\nBreast\nhistopathology\nBreaST-Net uses swin Transformers for breast cancer\nclassification.\nAcc: 99.60\nMCC: 98.90 Single dataset limits generalization.\n[193] ViT-LSTM Breast\nhistopathology\nViT and LSTM improves breast cancer classification\naccuracy. Acc: 99.20 Complexity increases computational\ndemands.\n[194] RMTF-Net Brain tumor MRI residual mix transformer fusion network (RMTF-Net)\nimproves brain tumor segmentation accuracy. Dice: 93.50 Limited to 2D imaging scenarios.\n[195] RadFormer Gallbladder cancer RadFormer improves gallbladder cancer detection using\nglobal attention.\nAcc: 92.10\nSpe: 96.10 The complexity of the attention mechanisms.\nThestudy[163],introducesanovelapproachforsegmentingcholangiocarcinomahistopathologicalimagesusinghyperspectral\nimaging. Hyperspectral imaging offers richer spectral information compared to RGB imaging, which can enhance segmentation\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 25 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nperformance. The study proposes a swin-spectral Transformer network, which integrates spectral and spatial features more\neffectively. The network employs a spectral multi-head self-attention mechanism to handle the spectral dimension as a sequence,\nratherthantreatingitasanadditionalspatialdimension.Theswin-spectralTransformercombinesspectral-multi-headself-attention\nwithshiftedwindow-basedmulti-headself-attentiontocapturebothspectralandspatialfeatures.Additionally,aspectralaggregation\ntokenisintroducedforeffectivedimensionalreduction,producinga2Dsegmentationresult.Theexperimentalresultsdemonstrate\nthat the proposed method significantly outperforms existing techniques.\nSwin Transformer uses a hierarchical feature maps and shifted windows for efficient image modeling and scalability. In\nparticular, [190] proposed a Swin Transformer model for multi-class skin lesion classification, leveraging the strengths of both\nTransformersandCNNs.Thismodelbenefitsfromend-to-endlearningcapabilitiesanddoesnotrequirepriorknowledge.Toaddress\nclass imbalance, this approach uses a weighted cross-entropy loss, enhancing the modelâ€™s performance in dealing with imbalanced\ndatasets. The proposed method was evaluated on the skin lesion imaging. This study demonstrates that the Swin Transformer\nmodel outperforms many existing state-of-the-art methods, showing superior balanced accuracy and effectiveness in multiclass\nskin lesion classification. In contrast [191] addressed the critical need for accurate and timely brain tumor diagnosis. This study\nintroduces an advanced DL approach using the Swin Transformer model as depicted in Figure 13. Key innovations include hybrid\nshiftedwindows,multi-headself-attentionmodulesandaresidual-basedMLPintegratedintotheswinTransformer.TheRes-MLP\nreplaces the traditional MLP to further improve accuracy, training speed, and parameter efficiency. The proposed-swin model was\nevaluatedonthepubliclyavailablebrainMRIdataset.ThemodelbenefitsfromTLanddataaugmentationtechniques.Thisapproach\noffers a novel and robust diagnostic tool, supporting radiologists in achieving timely and accurate brain tumor diagnoses. The\nstudy [196] addresses the challenge of distinguishing benign from malignant colorectal adenomas. The proposed method, multiple\ninstance learning network (MIST), leverages the swin Transformer as its backbone for feature extraction. This approach utilizes\nself-supervised contrastive learning and integrates a dual-stream multiple instance learning network to classify whole slide images\nbased solely on slide-level labels, eliminating the need for labor-intensive manual annotation. MIST was trained and validated on\na dataset of 666 whole slide images from 480 patients, encompassing six common types of colorectal adenomas. Reference [192],\nexplored the use of swin Transformers for classifying breast cancer subtypes from histopathological images.\nSwin \n Transformer \nblock\nPatch partition\nx2\nStage 4\n(H/32)x((W/32)x8C\nSwin \n Transformer \nblock\nPatch partition\nx4\nStage 3\n(H/16)x((W/16)x4C\nSwin \n Transformer \nblock\nPatch partition\nx2\nStage 2\n(H/8)x((W/8)x2C\nSwin \n Transformer \nblock\nLinear embedding\nx2\nStage 1\n(H/4)x((W/4)x48C\nPatch partition\nStage 0: Input \nHxWx3\nStage 5: Output \nClassification\nGlioma-tumor\nMeningioma-tumor\nPituitary-tumor\nNo-tumor\nFigure 13: An example of the structure of Swin Transformer architecture for brain tumor diagnosis [191]. The Swin Transformer\narchitecture comprises essential blocks for brain tumor diagnosis. The input module, preprocesses brain scans by normalizing and\nresizing them. The Patch partitioning block, divides images into fixed-size patches, embedding them into tokens. The Swin Transformer\nblocks, employ hierarchical learning with shifted window attention, ensuring efficient local-global feature extraction while multi-head\nself-attention identifies critical regions. Feature Aggregation, progressively reduces spatial dimensions, consolidating key features. The\nclassification head, maps aggregated features to tumor classes using fully connected layers and softmax. Finally, the output delivers\ndiagnostic results, excelling in capturing spatial hierarchies for accurate brain tumor detection.\nOtherproposedTransformersincludedthemulti-axisViTintroducedbyPacaletal.[161],anadvancedarchitecturalframework\ndesigned to enhance the early detection of cervical cancer using Pap smear images. The multi-axis ViT was specifically adapted\nfor Pap smear data, featuring a lightweight structure that improved both accuracy and inference speed. [197] replaced MBConv\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 26 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nblocks in the MaxViT architecture with ConvNeXtv2 blocks and MLP blocks with gated residual network gated residual network\n(GRN)-basedMLPs.Thesemodificationssignificantlyreducedthenumberofparametersandenhancedthemodelâ€™sgeneralization\ncapabilities. The proposed method was rigorously evaluated on the SIPaKMeD and Mendeley LBC Pap smear datasets, utilizing\n106DLmodelsâ€”53CNNsand53ViTmodelsforeachdataset.Barzekaretal.[198]presentedanovelDNNarchitecture,MultiNet,\ndesigned to improve the multiclass classification of medical images with a focus on cancer diagnosis. This study integrated\nTransformers within a multiclass framework to enhance data representation and improve classification accuracy. The proposed\nMultiNetViTmodelwasevaluatedonpubliclyavailabledatasets,employingvariousassessmentmetricstoensurereliability.Results\ndemonstratedthatMultiNetsignificantlyimprovedcancerdiagnosisaccuracythroughimageclassification,potentiallyreducingthe\nreliance on costly and time-consuming manual analyses by radiologists and pathologists. This approach offered a scalable solution\nfor large-scale biological image classification, paving the way for more efficient and effective cancer detection. Chaudhury et al.\n[193] explored the integration of ViT and LSTM networks for breast cancer diagnosis, comparing ultrasonography and histology\nas diagnostic modalities. While traditional CNNs effectively extracted visual features, this study leveraged bidirectional encoder\nrepresentations from Transformers, optimized for image processing. The integration of ViT and LSTM demonstrated significant\npotential in improving breast cancer diagnosis, providing a powerful tool for more precise and reliable healthcare decisions.\n4.5. Large language models\nLLMs, such as GPT, BERT, and their variants, are neural network models designed for processing and understanding human\nlanguage at scale. LLMs, for example, have also been explored in many medical domains, such as diabetes diagnosis, where\nLSTM networks and chatbot-based medical support, such as the WizardLM-based DiabeTalk, have been investigated to improve\nclassification accuracy and patient interaction [199]. LLMs are typically based on Transformer architectures, which rely on self-\nattentionmechanismstocapturetherelationshipsbetweenwordsinasequence.Theself-attentioncomputesaweightedsumofthe\nvalues,withweightsdeterminedbythesimilaritybetweenqueriesandkeys.LLMsaretrainedusinglargecorporathroughmasked\nlanguage modeling for BERT or autoregressive prediction for GPT, optimizing the following cross-entropy loss:\nğ¿= âˆ’\nâˆ‘\nğ‘–\nlog ğ‘(ğ‘¤ğ‘–|ğ‘¤<ğ‘–) (18)\nwhereğ‘(ğ‘¤ğ‘– âˆ£ ğ‘¤<ğ‘–) istheprobabilityofword ğ‘¤ğ‘– [77,28,200].Amongthetypesofadvancedmethods,GPT-4Vision(GPT-4V)isa\nmulti-modal LLM developed by OpenAI, extending the capabilities of GPT-4 by enabling it to process and generate both text and\nvisualdata.ThisintegrationofvisionandlanguageallowsGPT-4Vtoperformawiderangeoftasksinvolvingtheunderstandingand\nreasoningofimagesincombinationwithtext[61,201].Anotherexampleisthegeneralistlanguagemodel(GLaM),asparsemixture-\nof-expertsmodeldesignedbyGoogletoprovideefficientandscalablelanguagemodelingwithsignificantlyreducedcomputational\ncost compared to dense models like GPT-3. GLaM achieves this by activating only a subset of its parameters during each forward\npass, making it more efficient in terms of memory and processing power while maintaining high performance [202]. Additionally,\nLlama-2-70b-chatisaversionoftheLlama-2model,anopen-sourcemodeldevelopedbyMeta(formerlyFacebook)aspartoftheir\nLLaMA series. Specifically, Llama-2-70b-chat is a dialogue-optimized model with 70 billion parameters, designed to generate\nnatural and coherent text for conversation-like tasks. It builds on the success of the original LLaMA models and is optimized for\nchatbot and conversational applications [203, 204]. Figure 14 provides an overview of the key LLMs explored and analyzed in the\ncontext of cancer detection. Additionally, Table 15 provides a summary of several state-of-the-art studies that employed LLMs as\nthe primary approach for cancer diagnosis.\nThe paper [200] introducedCancerLLM, a specialized LLM designed for the cancer domain to address the need for more\nfocusedandefficientmodelsinhealthcare(seeFigure15).TheauthorshighlightedthatwhileexistingLLMssuchasClinicalCamel\n70B and Llama3-OpenBioLLM 70B are large and computationally expensive, CancerLLM provided a more efficient alternative\nwith 7 billion parameters and a Mistral-style architecture. They provided three datasets specifically designed for cancer phenotype\nextraction, cancer diagnosis generation, and cancer treatment plan generation. The model was pre-trained on over 2.6 million\nclinical notes and 500,000 pathology reports covering 17 cancer types and fine-tuned on these tasks. The findings demonstrated\nthatCancerLLMsignificantlyenhancedclinicalAIsystems,contributingtoimprovedresearchandhealthcaredeliveryinoncology.\nWu et al. [202] proposed a liver cancer diagnosis assistant that combined large and small models to improve diagnostic accuracy,\nparticularly for less experienced doctors in primary healthcare settings. The framework addressed limitations such as inadequate\nunderstanding of medical images, insufficient consideration of liver blood vessels, and inaccurate medical information extraction.\nSmall models were optimized for precise perception, with specialized methods for liver tumor and vessel segmentation to enhance\ninformation extraction. The large model employed Chain-of-Thought technology to mimic the reasoning process of experienced\ndoctors and utilized retrieval-augmented generation for responses based on reliable domain knowledge. Results showed improved\nsegmentation performance and higher evaluation scores from doctors for the assistantâ€™s responses compared to control methods,\nmakingitavaluabletoolforlivercancerdiagnosis.Sun[205]focusedondevelopingfive-yearsurvivalpredictionmodelsforbladder\ncancer patients undergoing neoadjuvant chemotherapy and radical cystectomy. The study explored the feasibility of using LLMs,\nsuch as Vicuna and Dolly, to extract clinical descriptors from reports, which were then incorporated into a nomogram model. It\nalso examined the impact of combining these descriptors with radiomics and DL features derived from CTU images, leveraging\nback-propagationneuralnetworks(BPNNs).Themodels,developedandvalidatedusingdatafrom163patients,includedvariations\nbasedonclinicaldescriptors(C),radiomicsdescriptors(R),DLdescriptors(D),andtheircombinations.Theperformanceofmodels\nusing LLM-extracted descriptors was comparable to those using manually extracted descriptors, demonstrating the potential of\nLLMs in clinical information extraction for survival prediction. Deng et al. [206] proposedGeneLLM, a novel LLM designed\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 27 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nVicuna \nand\ndolly  \nGemini\nand\n bing\nClaude\nLlama \nGoogleâ€™s\ngemini\npro\nGPT\nBard Clinical\nCamel\nchat\nGLM-6B\nMed42-70\nB\nCancer\nLLM\nLLM \ntypes\nFigure 14: Type of LLM for cancer diagnosis. CancerLL M [200]; Vicuna and dolly [205]; GeneLLM [206]; Llama [207]; GPT [208];\nGemini and bing [208]; Claude and Bard [209]; Privacy-preserving LLM [210]; Off-the-shelf LLMs [211]; Googleâ€™s Gemini-pro [212];\nClinicalCamel [207]; ChatGLM-6B [202].\nPre training Dataâ€\nCancer knowledge injection Cancer Instruction Tuning\nEvaluationApplication\nPre trainingâ€\nLanguage Model\nCancer Adapted Model\nInstruction data\nInstruction Tuning\nCancer SFT Model\nEvaluation Metrix\nRobustness Testbeds\nPhenotype extraction\nDiagnosis generation\nFigure 15:An example of a LLM in cancer domain [200]. The process of adapting an LLM for the cancer domain begins with pre-training\na general-purpose language model on diverse datasets. Cancer-specific knowledge is then injected using specialized datasets, publications,\nand terminologies. Instruction data is created for domain-specific tasks like phenotype extraction and diagnosis generation. The model\nundergoes instruction tuning and supervised fine-tuning (SFT) to specialize in cancer-related applications. The resulting Cancer-SFT\nmodel is evaluated using metrics and robustness tests to ensure reliability. Finally, the model is applied in real-world scenarios, aiding in\nphenotype extraction, diagnostic insights, and treatment recommendations, enhancing precision and efficiency in cancer research and\nclinical workflows.\nto directly interpret plasma cell-free RNA (cfRNA) sequences for cancer screening, bypassing traditional genome annotations.\nUnlike conventional methods reliant on bioinformatics tools to count known genes, GeneLLM identified cfRNA from previously\nunknown genes, termed \"dark matters,\" which serve as pseudo-biomarkers for cancer detection. This approach not only improved\ndetection accuracy but also made the process more accessible and cost-effective. The findings highlighted GeneLLMâ€™s potential\nto revolutionize biomarker discovery and enhance understanding of intercellular communication through novel RNA molecules.\nChangetal.[207]investigatedtheuseofopen-sourceclinicalLLMs(Llama-2-70b-chat,ClinicalCamel-70B,andMed42-70B)for\nclassifying cancer stages, specifically extracting pathologic tumor-node-metastasis (TNM) staging information from unstructured\nclinical reports. Unlike traditional NLP approaches that require labor-intensive labeled datasets, this study demonstrated the\nfeasibility of using LLMs without labeled training data. The experiments compared LLMs with a BERT-based model fine-tuned\non labeled data. Results showed that while LLMs performed sub-optimally in Tumor (T) classification, they achieved comparable\nresults in Metastasis (M) classification and outperformed in Node (N) classification when appropriate prompting strategies were\napplied.ThisdemonstratedthepotentialofLLMsforefficientlyextractingstaginginformationforoncologypatientsfromreal-world\npathology reports.Googleâ€™sGeminiwas alsoemployed inthe contextof cancerdetection by Lammertet al.[212], who developed\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 28 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nan advanced system namedMEREDITH to address the limitations of LLMs in precision oncology. MEREDITH integrated\nPubMed clinical studies, trial databases, and oncology guidelines with LLMs to enhance the accuracy and relevance of treatment\nrecommendations. The study evaluated the system using 10 fictional patient cases involving 7 tumor types and 59 molecular\nalterations, assessed by the Molecular Tumor Board (MTB) at the Center of Personalized Medicine (ZPMTUM). MEREDITH\nemployed a retrieval-augmented generation system enhanced by Googleâ€™s Gemini Pro, along with chain-of-thought prompting.\nIterative improvements were made based on expert feedback from MTB, comparing LLM-generated recommendations to those\nannotatedbyclinicalexperts.ThestudyconcludedthatincorporatingexpertfeedbackintoLLMtrainingsignificantlyenhancedthe\nmodelâ€™s alignment with clinical reasoning, presenting a promising tool for clinical decision support in precision oncology. Lee et\nal. [210] evaluated a privacy-preservingFastChat-T5for automating question answering from thyroid cancer surgical pathology\nreports. Eighty-four reports were analyzed by the LLM and two independent reviewers across 12 medical questions related to\nstaging and recurrence risk. FastChat-T5 significantly reduced task completion time while maintaining accuracy comparable to\nhuman reviewers.\nSivarajkumar et al. [204] investigated the use ofLLAMA-2 to automate the extraction of treatment response information,\nparticularly disease progression, from EHRs. Their study involved 1,953 primary lung cancer patients from the UPMC Hillman\nCancer Center, using approximately 113,000 clinical notes. Disease progression was defined based on RECIST guidelines, with\na gold standard dataset created from 50 manually annotated notes. The researchers fine-tuned LLAMA-2 and compared its\nperformance to a traditional rule-based NLP system. The results highlighted the potential of LLMs to improve accuracy and\nscalability in treatment response assessments. Rajaganapathy et al. [213] appliedLLAMA-2 to automatically generate synoptic\nreportsfrom7,774cancer-relatedpathologyreportsannotatedwithreferencesynopticreportsfromMayoClinicEHRs.LLAMA-2\nwas fine-tuned to produce reports containing 22 unique data elements. The evaluation showed that fine-tuned LLMs effectively\nautomated synoptic report generation, demonstrating their potential to enhance efficiency and accuracy in clinical documentation.\nMultiple versions of ChatGPT have been utilized in LLM-based cancer detection and treatment. As examples, Cao et al. [208]\nevaluated the performance ofChatGPT-3.5(OpenAI), Gemini (Google), andBing (Microsoft) in answering questions related to\nhepatocellularcarcinoma(livercancer).Theirworkassessedtheaccuracy,reliability,andreadabilityofresponsesto20livercancer-\nrelated questions concerning diagnosis and management. Six fellowship-trained physicians from three academic liver transplant\ncenters evaluated the responses, categorizing them as accurate (all information true and relevant), inadequate (true but incomplete\norirrelevant),orinaccurate(falseinformation).Meanscoreswithstandarddeviationswererecordedtodetermineoverallaccuracy\nand reliability, while readability was assessed using the Flesch Reading Ease Score and Flesch-Kincaid Grade Level. Choi et al.\n[214] explored the use ofGPT-4for extracting clinical factors from breast cancer pathology and ultrasound reports. Using 2,931\npatient records, they developed prompts for GPT to extract data more time- and cost-efficiently than manual methods. The work\ndemonstratedthatLLMscouldsignificantlyimprovetheefficiencyofclinicaldataextraction.Matsuoetal.[215]evaluated GPT-3.5\nTurboforTumor,node,andmetastasis(TNM)classificationinlungcancerradiologyreports,focusingonmultilingualcapabilities\nin Japanese and English. The LLM achieved its highest accuracy when provided full TNM definitions in English, showcasing\nthe relevance of LLMs in radiological applications. Deng et al. [216] assessed the performance ofChatGPT-3.5, ChatGPT-4.0,\nand Claude 2 in breast cancer clinical scenarios. ChatGPT-4.0 provided the most accurate and relevant responses, particularly in\npsychosocialsupportandtreatmentdecision-making,outperformingothermodelsandhighlightingitspotentialinclinicaloncology.\nFerber et al. [61] testedGPT-4V for cancer histopathology tasks using in-context learning. The LLM matched or surpassed\nspecialized neural networks in colorectal tissue classification, colon polyp subtyping, and breast tumor detection, demonstrating\nits potential for medical image analysis without domain-specific training. Sushil et al. [51] investigated zero-shot breast cancer\npathology classification usingChatGPT-4 and ChatGPT-3.5. ChatGPT-4 outperformed traditional models, particularly in tasks\nwith label imbalance, highlighting LLMs as a viable alternative for reducing data annotation burdens. Shiraishi et al. [217]\nexploredLLMs (GPT-4, Bard, and BingAI) for diagnosing skin lesions. Using prompts with lesion images, they evaluated LLMs\nin determining malignancy and specific diagnoses. The findings underscored their potential for assisting dermatology patients in\nseekingprofessionalconsultations.Similarly,Zhouetal.[209]comparedtheperformanceofLLM-poweredchatbotsinaddressing\ncolorectal cancer queries with oncology physicians. Eight chatbots, includingClaude 2.1, ChatGPT-3.5, ChatGPT-4, and Doctor\nGPT, were tested on 150 questions, alongside responses from nine oncology physicians. Each response was scored for consistency\nwith clinical guidelines. Claude 2.1 outperformed residents, fellows, and attendings in accuracy, while Doctor GPT also surpassed\nresidents and fellows. Results indicated that LLMs can provide more accurate information on colorectal cancer than human\nphysicians in certain cases.\nAdditionally,BERThasalsobeenexploredinthesamecontext.Karimetal.[218]addressedchallengesinutilizingbiomedical\ndata for cancer diagnosis and treatment. They introduced a domain-specific knowledge graph (KG), supported by the OncoNet\nOntology (ONO), which integrated information from structured and unstructured sources. The KG, enriched withBioBERT and\nSciBERT, enabled semantic reasoning for cancer biomarker discovery and interactive question answering, fine-tuned using LLMs\nto incorporate the latest research.\nTable 15: Summary of LLMs-based approaches in cancer diagnosis.\nRef Model(s) used Dataset Contribution Bestresult(%) Limitation\n[200] CancerLLM Cancer LLM for cancer diagnosis and treatment recommendations. F1: 95.52\nRec: 95.52 Depends on dataset quality and diversity.\nContinued on next page\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 29 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nRef Model(s) used Dataset Contribution Bestresult(%) Limitation\n[202] ChatGLM-6B Liver cancer LLM for liver cancer diagnosis with domain-specific knowl-\nedge.\nAcc: 99.50\nRec: 92.90\nConstrained by integrated knowledge\nsources.\n[204] LLAMA-2 Lung cancer LLM for detecting lung cancer treatment progression. F1: 92.00\nSpe: 91.00 Variability in patient records.\n[205] Vicuna and\nDolly Bladder cancer LLM for survival predictions from clinical reports. AUC: 89.00 Relies on clinical report quality.\n[207] Llama-2-70b-\nchat Clinical text LLM to classify cancer stages from clinical text. Pre: 98.00\nPre: 94.00 Dependent on text data consistency.\n[206] GeneLLM cfRNA sequencing LLM for processing cfRNA reads for cancer screening. Acc > 78.00\nAUC>90.00\nRequires extensive computational\nresources.\n[209] Claude 2.1 Colorectal cancer LLM chatbots for responding to cancer queries. Acc: 82.67 Limited by general knowledge base.\n[213] LLAMA-2 Pathology reports LLM for summarizing cancer pathology reports. F1: 94.00\nAcc: 81.00 Summaries may miss clinical nuances.\n[214] GPT Breast cancer LLM for clinical information extraction from pathology re-\nports. Acc: 98.20 Report formatting variability.\n[215] GPT-3.5-turbo Lung cancer Multilingual LLM for lung cancer TNM staging classifica-\ntion. Acc: 74.00 Variability in report language.\n[216] ChatGPT, and\nClaude2 Breast cancer Comparison of ChatGPT and Claude2 for breast cancer sce-\nnarios.\nFleissâ€™ kappa:\n0.345 Requires further validation.\n[218] BioBERT, and\nSciBERT\nBiomedical\nliterature LLM and KGs for cancer biomarker discovery. Pre: 91.36\nRec: 90.75 Complex integration of KGs.\n[219] GPT-3.5 and\nGPT-4 Radiation oncology LLM for decision support in radiation oncology. Acc: 74.57 Needs real-world clinical validation.\n5. Computational approaches\nAdvancedDLmethods,includingFL,RL,TL,Transformers,andLLMs,havebecomeessentialforaddressingcomputationally\nintensive tasks, enabled by advancements in algorithms, computational power, and access to vast datasets. These methods tackle\nchallengessuchasscalability,privacy,andefficiency,unlockingapplicationspreviouslyunattainable.ModernDLarchitecturesvary\ninlayerconnectivityanddepth,withnetworkslikeResNetevolvingtowardunprecedentedscales,potentiallyreaching1000layers.\nOptimization strategies, such as SGD, fine-tune parameters to boost accuracy, while FL ensures privacy-preserving training across\ndistributed datasets. Training large models like ResNet on datasets such as ImageNet, with over 14 million images, often requires\ntensofthousandsofiterationsandimmensecomputationalpower,exceeding 1020 FLOPS.RLoptimizesdynamicdecision-making\ntasks, TL reduces labeled data requirements by leveraging pretrained models, and Transformers/LLMs excel in tasks requiring\ncontextualandmulti-modalanalysis.DespiteimprovementsinhardwarelikeGPUsandmemorybandwidth,risingDLcomplexity\namplifies computational demands, particularly in networks with varying computation-to-bandwidth ratios. These advanced DL\ntechniques are reshaping the landscape of AI, delivering scalable, efficient, and privacy-conscious solutions across diverse fields\n[220, 221].\n5.1. CPU\nAdvanced deep learning (DL) methods, including FL, RL, TL, Transformers, and LLMs, often require a balance between\ncomputational power, memory, and adaptability, making the choice of hardware critical. Central processing unit (CPU) nodes\nare ideal for tasks that prioritize robust network connections, extensive memory, and storage capabilities, offering flexibility and\nease of integration into diverse systems. While CPUs lack the raw computational throughput of specialized hardware like field-\nprogrammable gate arrays (FPGAs) and GPUs, they are well-suited for DL techniques like FL, where distributed training across\nnodesnecessitatesstrongnetworkingandmemoryresources.Similarly,CPUscansupportRLforreal-timedecision-makingandTL\nforadaptingpretrainedmodelstospecifictaskswithoutrequiringimmensecomputationalloads.However,computational-intensive\ntasks such as fine-tuning Transformers and LLMs on large datasets often benefit from the parallel processing capabilities of GPUs\nor the customizability of FPGAs. Despite this, the adaptability and versatility of CPUs make them indispensable for many DL\napplications where network and memory demands outweigh the need for pure processing speed [222, 223].\n5.2. GPU\nAdvanced DL methods, including FL, RL, TL, Transformers, and LLMs, often rely on GPUs for their ability to handle\ncomputationallyintensivetaskswithexceptionalefficiency.GPUsexcelinexecutingfundamentalDLoperationssuchasAFs,matrix\nmultiplication, and convolutions due to their highly parallel computing capabilities. Modern GPUs, equipped with integrated high\nbandwidthmemory(HBM)stackedmemory,significantlyboostbandwidthandoptimizeresourceutilization,enablingdenselinear\nalgebra operations to outperform CPUs by 10â€“20 times. This makes GPUs indispensable for tasks like training DNNs, fine-tuning\nTransformers, and optimizing LLMs for large-scale datasets. For FL, their parallel processing power supports the aggregation of\nmodels from distributed nodes, while in RL, GPUs enable real-time learning and decision-making. TL also benefits from GPUsâ€™\nspeedinadaptingpretrainedmodelstonewtasks,reducingtrainingtime.Withuptosixty-fourcomputationalunits,eachfeaturing\nmultiplesingleinstruction,multipledata(SIMD)engines,GPUsachievepeakperformancesof25TFLOPS(fp16)and10TFLOPS\n(fp32), ensuring efficiency in both training and inference. The ability to combine addition and multiplication functions for vector\noperationsandutilizeinnerproductinstructionsfurtherenhancesGPUperformance,makingthemaversatileandessentialtoolfor\nadvanced DL applications [224, 225].\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 30 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\n5.3. FPGA\nAdvanced DL methods, including FL, RL, TL, Transformers, and LLMs, benefit significantly from FPGA technology,\nparticularly in inference acceleration tasks where efficiency and customization are paramount. Unlike GPUs, which offer high\nfloating-point performance, FPGAs excel in minimizing unnecessary functions and overhead, delivering low-latency and energy-\nefficientsolutions.Theirabilitytodynamicallyreconfigurearraycharacteristicsinreal-timeandsupportcustomdesignsmakesthem\nhighlyadaptableforspecializedDLoperations.FPGAsachievesuperiorperformanceperwattthroughstrategieslikeimplementing\ncustom high-performance hardware, pruned networks, and reduced arithmetic precision, often outperforming GPUs and CPUs\nin these areas. For tasks such as CNN inference, FPGAs deliver over 15 TOPs in peak performance, reaching more than 80%\nefficiency with 8-bit accuracy. Additionally, pruning techniques, particularly in LSTM models, enable significant model size\nreductionsupto20timesfacilitatingoptimaldeploymentinresource-constrainedenvironments.Recentadvancementsinlowering\narithmetic precision to 8-bit fixed-point or custom floating-point formats further enhance FPGA performance, making them ideal\nfor applications requiring highly efficient and tailored solutions, such as in federated learning or LLMs deployed at edge devices\n[226, 227].\n6. Research challenges and future directions\n6.1. Challenges\nDL is a widely used and highly effective method for training AI systems with large datasets, but it also presents significant\nchallengesthatrequireinnovativesolutions.Thesechallengesincludeissueslikedataprivacy,computationalcosts,limitedlabeled\ndata, and model interpretability. Advanced DL techniques offer promising alternatives to address these difficulties. FL tackles\nprivacyconcernsbyenablingcollaborativemodeltrainingacrossdecentralizeddatasetswithoutsharingsensitivedata.RLoptimizes\ndecision-makingincomplex,dynamicenvironments,makingitsuitablefortaskslikepersonalizedtreatmentplanning.TLmitigates\nthe need for extensive labeled data by leveraging pretrained models to adapt to specific tasks with smaller datasets. Additionally,\nTransformers and LLMs have revolutionized AI by improving the scalability, accuracy, and interpretability of models in tasks like\nNLPunderstandingandmedicaldataanalysis.Adoptingtheseadvancedapproachesnotonlyaddressesthelimitationsoftraditional\nDL methods but also enhances the effectiveness and deployment of AI systems across diverse applications.\n(a) Training data: In DL, training data is critical for robust model performance, but data insufficiency remains a challenge. FL\naddressesthisbytrainingmodelsacrossdecentralizeddatawithoutsharingsensitiveinformation;however,ensuringdataprivacy,\nmanagingcommunicationoverhead,andaddressingheterogeneityacrossdevicesremainkeychallenges.TLleveragesknowledge\nfromrelatedtaskstoimproveperformanceonlimiteddatasets,butitoftenstruggleswithnegativetransferwhensourceandtarget\ntasksarenotcloselyrelated.Transformers,pivotalinmodernAI,excelinprocessingsequentialdatabutrequireextensivetraining\ndata,computationalresources,andarepronetooverfittingonsmallerdatasets.RLenhancesmodeladaptabilitythroughtrial-and-\nerror, yet suffers from sample inefficiency and exploration challenges. LLMs face hurdles like data bias, resource intensity, and\nensuring generalization across diverse tasks. Overcoming these challenges necessitates innovative strategies in data simulation,\noptimization, and efficient architecture design.\n(b) Imbalanceddata: Imbalanceddata,whereoneclasssignificantlyoutweighsanother,poseschallengesfortrainingDLmodels,\nparticularlyinmaintainingpredictiveaccuracyacrossallclasses.FLcanexacerbatethisissuewhendataimbalanceoccursacross\ndistributed nodes, making global model updates less effective and complicating convergence. TL offers a solution by using\npre-trained models from balanced datasets, which can be fine-tuned to improve performance on minority classes. However,\nthe relevance of source domains remains critical. RL, while adaptive, struggles with reward sparsity and policy optimization\nin imbalanced datasets, further amplifying bias toward majority classes. Transformers and LLMs, although powerful, require\nbalanced data for effective learning and risk significant bias amplification in imbalanced settings, particularly when fine-tuned\nwithout addressing class disparities.\n(c) Interpretabilityofdata: InterpretabilityofdataisacrucialchallengeinDLasmodelsgrowincomplexity.FL,whilepreserving\ndataprivacy,limitstransparencyduetodistributeddatastorage,makingithardertounderstandhowlocaldatainfluencesglobal\nmodels. TL, though effective, raises questions about the relevance and transferability of learned features, particularly when\nadapting to tasks with differing data distributions. RL models face interpretability issues in understanding decision-making\nprocesses, especially when reward structures are complex or sparsely distributed. Transformers and LLMs present significant\nchallenges due to their vast number of parameters and opaque attention mechanisms, making it difficult to discern why specific\npredictions are made. The added complexity of federated, reinforcement, and transfer learning setups, combined with the\nexpansive architectures of transformers and LLMs, exacerbates these interpretability concerns.\n(d) Uncertainty scaling: Uncertainty scaling poses significant challenges in FL, TL, RL, Transformers, and LLMs, adding\ncomplexities that hinder reliable deployment. Confidence scores, crucial for robust predictions, are often poorly calibrated\nacross these domains. FL and TL face added difficulties due to heterogeneous data distributions across clients or tasks,\nintensifyinguncertaintyquantificationchallenges.InRL,therelianceoncumulativerewardsmakestheconfidenceinexploration-\nexploitation trade-offs unreliable, complicating policy optimization. Transformers and LLMs, while successful, frequently\nproduce overconfident predictions due to softmax-based probabilities, particularly when generalizing to unseen or out-of-\ndistribution data. These challenges are further amplified in critical fields such as healthcare and autonomous systems, where\npoorly scaled uncertainties can result in life-threatening decisions or system failures.\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 31 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\n(e) Overfitting: Data overfitting is a prevalent issue in DL models during training, caused by the complex correlations among a\nlargenumberofparameters.Thisoverfittingreducesthemodelâ€™seffectivenessonnew,unseendataandisachallengenotlimited\nto any single field but common across various tasks, including FL, TL, RL, Transformers, and LLMs. In FL, the decentralized\nnature and heterogeneous client data exacerbate overfitting risks. Similarly, TL struggles with overfitting when the source and\ntargetdomainshavelimitedsimilarity.InRL,thesparserewardstructureandhigh-dimensionalstate-actionspacesoftenleadto\npoliciesoverfittingthetrainingenvironment.TransformersandLLMsfaceoverfittingchallengesduetotheirimmenseparameter\nspaces and reliance on pretraining finetuning paradigms, particularly when adapting to domain-specific tasks.\n(f) Unified assessment: In numerous studies, there is a notable absence of comprehensive details concerning the technical facets\nof the experiments conducted. The choice of measurement indicators and baseline methods often seems random, leading to a\nnon-standardized evaluation process. Researchers primarily focus on metrics such as accuracy, sensitivity, or specificity of DL\nnetworks. However, Unified Assessment poses additional challenges across advanced methodologies. For FL, standardizing\nevaluations is difficult due to data heterogeneity, privacy concerns, and inconsistent benchmarks for data. RL struggles\nwith variability in reward structures, task complexity, and cross-environment benchmarking. TL lacks clarity in reporting\ndomain adaptation success and the influence of pre-trained models. Transformers face challenges in scalability, computational\noverhead,andvariationsinresultsacrossimplementations.LLMscomplicateassessmentwithissuesoffine-tuningeffectiveness,\ninterpretability, and resource-intensive evaluations. A standardized framework addressing these challenges is vital for cancer\ndetection.\n(g) High computational requirements: Transformers, particularly ViTs, are computationally intensive and require significant\nGPU/TPU resources for training and inference. This makes it difficult to deploy these models in real-time clinical settings,\nespeciallyinresource-constrainedenvironments.Similarly,FLamplifiescomputationaldemandsduetofrequentcommunication\noverheads between clients and servers, as well as the need to handle non-IID data across devices. RL is hindered by\nhigh computational costs associated with iterative policy optimization and simulation-based training, particularly in complex\nenvironments. TL also faces challenges when fine-tuning large pre-trained models on domain-specific datasets, which often\nrequires extensive computational resources. Transformers exacerbate the problem with their quadratic complexity concerning\ninput sequence length, making them unsuitable for low-resource scenarios. LLMs demand immense memory and processing\npower during both training and inference, further complicating their integration into clinical workflows. Addressing these\nchallenges is critical for scalable deployment in healthcare.\n6.2. Future directions\nAs advanced DL methods represent a significant leap in modern AI techniques, current and future research focuses heavily on\ntheirapplicationincancerdiagnosis.Despiteextensiveresearcheffortsinthisfield,theuseofthesemethodsisstillinitsearlystages,\noffering vast potential for future development. The complexity of cancer diagnosis arises not only from the variety of types and\ndetection methods but also from handling complex medical data, including radiological images and genetic analyses. To overcome\nthesechallenges,AIsystemsrequireadvancedcapabilitiesinanalysis,decision-making,toolutilization,andmemorymanagement.\nConsequently, researchers have identified several key aspects that need to be addressed in systems based on advanced DL methods\nfor cancer diagnosis.\n(a) TL: TL will continue to advance by addressing challenges in negative transfer, where knowledge from a pretrained model\nadversely impacts target tasks. Research is focusing on improving domain adaptation techniques to ensure efficient knowledge\ntransfer across diverse datasets. TL in multitask learning is another promising area, enabling simultaneous optimization of\nmultiple related tasks. The development of scalable and lightweight TL models for deployment on edge devices is also a key\nfocus.Additionally,incorporatingTLintocontinuallearningframeworkswillallowmodelstoadaptincrementallytonewtasks,\nensuring long-term usability without catastrophic forgetting.\n(b) FL: FL is poised to expand in privacy-preserving applications through advancements in differential privacy and homomorphic\nencryption,ensuringsecuremodeltrainingonsensitivedatalikehealthcarerecords.AmajorfuturedirectionincludesadaptiveFL\nframeworksthatcanhandlenon-IIDdataandaddressclientvariability.FLinedgecomputingisalsoexpectedtogrow,enabling\ndecentralized AI systems on low-power devices for real-time applications. Additionally, the integration of FL with emerging\ntechnologies like blockchain for secure data sharing and quantum computing for accelerated training promises to redefine its\nscalability and efficiency in large-scale deployments.\n(c) RL: RL is moving toward more sample-efficient algorithms to reduce training time, making it suitable for real-time\napplications such as computer network [228], and healthcare. A key future direction is the integration of RL with deep learning\narchitectures, like Transformers, for solving complex, high-dimensional problems. Multi-agent RL is also gaining traction,\nenablingcollaborationandcompetitionamongagentsindynamicenvironments.Furthermore,RLisanticipatedtoplayapivotal\nrole in optimizing autonomous systems, such as self-driving cars and energy management. Bridging RL with unsupervised and\nsemi-supervised learning will enhance its adaptability in scenarios with limited labeled data.\n(d) Transformers and LLM: The future of Transformers and LLMs is closely tied to advancements in retrieval-augmented\ngeneration (RAG) [229], which combines LLMs with external knowledge retrieval systems. RAG enables LLMs to access and\nutilizevastdatabases,reducingthedependencyonmodelsizebydynamicallyfetchingrelevantinformation.Futureresearchwill\nfocusonoptimizingretrievalmechanismsfordomain-specifictasks,improvingbothspeedandaccuracy.Additionally,integrating\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 32 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\nRAG with real-time knowledge updates ensures that models remain current and relevant without extensive retraining. The\ndevelopmentofprivacy-preservingRAGsystems,particularlyforsensitivedomainslikehealthcare,willbecrucial.RAGâ€™sability\ntoenhanceinterpretabilitybylinkingmodeloutputstospecificknowledgesourcesalsoaddressestransparencyandaccountability\nchallenges, paving the way for its broader adoption in fields like personalized medicine, legal analysis, and scientific research.\n(e) Hybridapproaches: FuturehybridmethodsinadvanceddeeplearningpromisetorevolutionizeAIbycombiningthestrengthsof\ntechniquessuchasFL,RL,TL,Transformers,andRAG.Forinstance,FLandTLcanworktogethertodeployprivacy-preserving,\npersonalizedmodelsacrossdecentralizedsystems,especiallyinhealthcareandedgecomputing.RLintegratedwithTransformers\ncan enhance sequential decision-making in applications like robotics and conversational agents, while TL combined with RAG\nenables domain-specific LLMs to incorporate real-time knowledge dynamically. Additionally, FL and RL hybrids can optimize\nresource allocation in decentralized networks, ensuring privacy and efficiency. Transformers coupled with multimodal learning\ntechniques will further enhance cross-modal understanding for applications in autonomous vehicles and medical diagnostics.\nBy strategically integrating these methods, hybrid approaches will create scalable, efficient, and context-aware AI systems,\naddressingreal-worldchallengesinpersonalizedhealthcare,smartcities,andknowledge-intensivefieldslikelawandmedicine.\n(f) Blockchainfordatasecurity: Inthecontextofcancerdetection,thetransitionfromtraditionalmodelstoadvancedDLmethods,\nsuchasFL,RL,TL,Transformers,LLMs,isdrivenbytheneedforimprovedpredictionaccuracy,scalability,andreal-timedata\nprocessing of heterogeneous datasets. Traditional models, which relied on centralized datasets, posed significant privacy and\nsecurityrisks,makingthemunsuitableforhandlingsensitivepatientdata.AdvancedmethodslikeFL,integratedwithblockchain,\nenable decentralized model training while ensuring secure, tamper-proof data sharing and patient confidentiality. RL enhances\ndecision-makingindynamicscenarios,suchaspersonalizedtreatmentplansanddiagnosticworkflows,withblockchainproviding\nsecure logging of decision-making processes and auditability. TL, when paired with blockchain, enables efficient adaptation of\npretrained models to cancer datasets, ensuring data provenance and reducing risks of tampered inputs. Transformers and LLMs\ndeliver exceptional accuracy and contextual understanding for cancer diagnostics, with blockchain enhancing transparency and\ntraceabilityinmultimodaldataintegrationandmodelupdates.Together,thesetechnologiesensuresecure,distributed,andprecise\nsystems tailored to the complex challenges of cancer detection.\n(g) Interpretability and scalability:The application of advanced DL techniques, such as FL, RL, TL, Transformers, and LLMs,\nholds transformative potential for cancer detection. These methods can improve interpretability by providing clearer insights\ninto patterns and anomalies in medical images or genetic data, making diagnostic results more actionable for physicians.\nLLMs and Transformers excel in multimodal data analysis, enabling better differentiation between malignant and normal\ntissues, while FL ensures privacy-preserving collaboration across decentralized datasets. Scalability remains a critical future\ndirection, particularly in integrating these techniques with large, heterogeneous datasets to support diverse cancer types and\nclinicalscenarios.Advancedarchitectures,suchaslightweightLLMsandoptimizedFLframeworks,canaddressthechallenges\nof computational efficiency and resource constraints. Future research should prioritize improving interpretability to enhance\nphysician trust and understanding, alongside developing scalable models that deliver precise and personalized cancer diagnosis\nat population-wide levels.\n7. Conclusion\nIn this comprehensive review, we have explored the transformative potential of advanced DL methodologies, such as RL, FL,\nTL, Transformers, and LLMs, in enhancing cancer detection and diagnosis. These techniques represent significant progress in\novercoming longstanding challenges in medical applications, such as data privacy, model scalability, and the scarcity of labeled\ndatasets.RLhasshownpromiseinoptimizingdiagnosticandtreatmentpathways,offeringdynamiclearningcapabilitiesthatadapt\ntoreal-timeclinicalscenarios.Byleveragingreward-basedmechanisms,RLhasdemonstrateditsutilityintasksrangingfromtumor\nlocalizationtotreatmentdecision-making.FL,ontheotherhand,providesaprivacy-preservingframeworkforcollaborativemodel\ntraining across decentralized datasets, enabling the inclusion of diverse data while addressing privacy concerns. Its application in\ncancer diagnosis highlights the growing trend toward ethical AI deployment in healthcare. TL continues to mitigate the challenge\noflimitedlabeleddatabyenablingtheadaptationofpretrainedmodelstospecificcancerdatasets.Thisapproachnotonlyenhances\nmodelperformancebutalsosignificantlyreducesthecomputationalcostsassociatedwithtrainingfromscratch.Theintroductionof\nTransformer-based models and LLMs has further revolutionized the field by enabling complex, context-aware analysis of medical\ndata, including imaging, genomics, and unstructured text. Their ability to process vast datasets and deliver contextually rich\npredictions exemplifies the potential of cutting-edge NLP and vision models in oncology.\nThe review also addresses critical technical challenges, including data imbalance and generalizability issues, by discussing\nadvancedsolutionslikedataaugmentation,ensemblemethods,andcost-sensitivelearning.TheseapproachesensurethatAImodels\nmaintain robustness across diverse cancer types and clinical conditions. Furthermore, the incorporation of evaluation metrics and\ndatasetsprovidesarobustfoundationforassessingmodelperformanceandidentifyingareasforfutureimprovement.Despitethese\nadvancements,severalchallengesremain.IssuessuchastheinterpretabilityofcomplexDLmodels,thehighcomputationaldemands\noftraining,andtheethicalimplicationsofAIdeploymentinclinicalsettingsmustbeaddressedtofullyrealizethepotentialofthese\ntechnologies. Future research directions should focus on integrating DL techniques with emerging technologies, such as quantum\ncomputingandedgeAI,tofurtherenhanceefficiencyandscalability.Inconclusion,theintegrationofadvancedDLtechniquesinto\ncancer detection and diagnosis is reshaping the landscape of healthcare. By addressing both technical and ethical challenges, these\nmethodologiesholdthepromiseofimprovingdiagnosticaccuracy,personalizingtreatmentplans,andultimatelyenhancingpatient\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 33 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\noutcomeshealthcondition.Thisreviewservesasafoundationalresourceforresearchersandpractitioners,highlightingthecurrent\nstate, challenges, and future directions in applying advanced DL in oncology.\nReferences\n[1] A.K.-y.Lam,Squamouscellcarcinomaofthyroid:auniquetypeofcancerinworldhealthorganizationclassification,Endocrine-relatedcancer27(6)(2020)\nR177â€“R192.\n[2] M.-r. Kwon, I. Youn, M. Y. Lee, H.-A. Lee, Diagnostic performance of artificial intelligenceâ€“based computer-aided detection software for automated breast\nultrasound, Academic Radiology 31 (2) (2024) 480â€“491.\n[3] K. A. Tran, O. Kondrashova, A. Bradley, E. D. Williams, J. V. Pearson, N. Waddell, Deep learning in cancer diagnosis, prognosis and treatment selection,\nGenome Medicine 13 (1) (2021) 1â€“17.\n[4] B. Jiang, L. Bao, S. He, X. Chen, Z. Jin, Y. Ye, Deep learning applications in breast cancer histopathological imaging: diagnosis, treatment, and prognosis,\nBreast Cancer Research 26 (1) (2024) 137.\n[5] Y.Xie,W.-Y.Meng,R.-Z.Li,Y.-W.Wang,X.Qian,C.Chan,Z.-F.Yu,X.-X.Fan,H.-D.Pan,C.Xie,etal.,Earlylungcancerdiagnosticbiomarkerdiscovery\nby machine learning methods, Translational oncology 14 (1) (2021) 100907.\n[6] S. Lal, A. K. Chanchal, J. Kini, G. K. Upadhyay, FPGA implementation of deep learning architecture for kidney cancer detection from histopathological\nimages, Multimedia Tools and Applications (2024) 1â€“19.\n[7] K.Das,C.J.Cockerell,A.Patil,P.Pietkiewicz,M.Giulini,S.Grabbe,M.Goldust,Machinelearninganditsapplicationinskincancer,InternationalJournal\nof Environmental Research and Public Health 18 (24) (2021) 13409.\n[8] M. Jawahar, L. J. Anbarasi, S. Narayanan, A. H. Gandomi, An attention-based deep learning for acute lymphoblastic leukemia classification, Scientific\nReports 14 (1) (2024) 17447.\n[9] Y. Habchi, Y. Himeur, H. Kheddar, A. Boukabou, S. Atalla, A. Chouchane, A. Ouamane, W. Mansoor, AI in thyroid cancer diagnosis: Techniques, trends,\nand future directions, Systems 11 (10) (2023) 519.\n[10] S. K. Mathivanan, S. Sonaimuthu, S. Murugesan, H. Rajadurai, B. D. Shivahare, M. A. Shah, Employing deep learning and transfer learning for accurate\nbrain tumor detection, Scientific Reports 14 (1) (2024) 7232.\n[11] A.Ajit,K.Acharya,A.Samanta,Areviewofconvolutionalneuralnetworks,in:2020internationalconferenceonemergingtrendsininformationtechnology\nand engineering (ic-ETITE), IEEE, 2020, pp. 1â€“5.\n[12] A. Auriemma Citarella, L. Di Biasi, F. De Marco, G. Tortora, Entail: yet another amyloid fibrils classifier, BMC bioinformatics 23 (1) (2022) 517.\n[13] A. C. Mazari, H. Kheddar, Deep learning-and transfer learning-based models for covid-19 detection using radiography images, in: 2023 International\nConference on Advances in Electronics, Control and Communication Systems (ICAECCS), IEEE, 2023, pp. 1â€“4.\n[14] Y. Himeur, S. Al-Maadeed, H. Kheddar, N. Al-Maadeed, K. Abualsaud, A. Mohamed, T. Khattab, Video surveillance using deep transfer learning and deep\ndomain adaptation: Towards better generalization, Engineering Applications of Artificial Intelligence 119 (2023) 105698.\n[15] Y. Himeur, I. Varlamis, H. Kheddar, A. Amira, S. Atalla, Y. Singh, F. Bensaali, W. Mansoor, Federated learning for computer vision, arXiv preprint\narXiv:2308.13558 (2023).\n[16] H. Kheddar, M. Hemis, Y. Himeur, Automatic speech recognition using advanced deep learning approaches: A survey, Information Fusion (2024) 102422.\n[17] S. K. Zhou, H. N. Le, K. Luu, H. V. Nguyen, N. Ayache, Deep reinforcement learning in medical imaging: A literature review, Medical image analysis 73\n(2021) 102193.\n[18] N. Djeffal, H. Kheddar, D. Addou, A. C. Mazari, Y. Himeur, Automatic speech recognition with BERT and CTC transformers: A review, in: 2023 2nd\nInternational Conference on Electronics, Energy and Measurement (IC2EM), Vol. 1, IEEE, 2023, pp. 1â€“8.\n[19] D. P. Panagoulias, M. Virvou, G. A. Tsihrintzis, Evaluating LLMâ€“Generated Multimodal Diagnosis from Medical Images and Symptom Analysis, arXiv\npreprint arXiv:2402.01730 (2024).\n[20] Y.Habchi,H.Kheddar,Y.Himeur,A.Boukabou,A.Chouchane,A.Ouamane,S.Atalla,W.Mansoor,Machinelearningandvisiontransformersforthyroid\ncarcinoma diagnosis: A review, arXiv preprint arXiv:2403.13843 (2024).\n[21] H. M. Rai, J. Yoo, A comprehensive analysis of recent advancements in cancer detection using machine learning and deep learning models for improved\ndiagnostics, Journal of Cancer Research and Clinical Oncology 149 (15) (2023) 14365â€“14408.\n[22] S.Sharma,K.Guleria,Acomprehensivereviewonfederatedlearningbasedmodelsforhealthcareapplications,ArtificialIntelligenceinMedicine146(2023)\n102691.\n[23] S.Atasever,N.Azginoglu,D.S.Terzi,R.Terzi,Acomprehensivesurveyofdeeplearningresearchonmedicalimageanalysiswithfocusontransferlearning,\nClinical imaging 94 (2023) 18â€“41.\n[24] H. T. Gayap, M. A. Akhloufi, Deep machine learning for medical diagnosis, application to lung cancer detection: a review, BioMedInformatics 4 (1) (2024)\n236â€“284.\n[25] A.Carriero,L.Groenhoff,E.Vologina,P.Basile,M.Albera,Deeplearninginbreastcancerimaging:Stateoftheartandrecentadvancementsinearly2024,\nDiagnostics 14 (8) (2024) 848.\n[26] S.Nerella,S.Bandyopadhyay,J.Zhang,M.Contreras,S.Siegel,A.Bumin,B.Silva,J.Sena,B.Shickel,A.Bihorac,etal.,Transformersandlargelanguage\nmodels in healthcare: A review, Artificial Intelligence in Medicine (2024) 102900.\n[27] M.N.Al-Hamadani,M.A.Fadhel,L.Alzubaidi,B.Harangi,Reinforcementlearningalgorithmsandapplicationsinhealthcareandrobotics:Acomprehensive\nand systematic review, Sensors 24 (8) (2024) 2461.\n[28] Z. A. Nazi, W. Peng, Large language models in healthcare and medical domain: A review, in: Informatics, Vol. 11, MDPI, 2024, p. 57.\n[29] X.Jiang,S.Wang,Y.Zhang,Visiontransformerpromotescancerdiagnosis:Acomprehensivereview,ExpertSystemswithApplications252(2024)124113.\n[30] K. N. Ryu, W. Jin, J. S. Park, Radiography, MRI, ct, bone scan, and pet-ct, in: Osteonecrosis, Springer, 2025, pp. 243â€“261.\n[31] F.Piccialli,V.DiSomma,F.Giampaolo,S.Cuomo,G.Fortino,Asurveyondeeplearninginmedicine:Why,howandwhen?,InformationFusion66(2021)\n111â€“137.\n[32] H. Zhang, Y. Qie, Applying deep learning to medical imaging: a review, Applied Sciences 13 (18) (2023) 10521.\n[33] H. Kheddar, M. Hemis, Y. Himeur, D. MegÃ­as, A. Amira, Deep learning for steganalysis of diverse data types: A review of methods, taxonomy, challenges\nand future directions, Neurocomputing (2024) 127528.\n[34] M. Hemis, H. Kheddar, S. Bourouis, N. Saleem, Deep learning techniques for hand vein biometrics: A comprehensive review, Information Fusion (2024)\n102716.\n[35] M. Bal-Ghaoui, M. H. E. Y. Alaoui, A. Jilbab, A. Bourouhou, Approaching cross-disease features for improved classification of thyroid and breast cancer in\nultrasound images, in: 2023 3rd International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET), IEEE, 2023,\npp. 1â€“5.\n[36] N. Bibi, M. Sikandar, I. Ud Din, A. Almogren, S. Ali, IoMT-based automated detection and classification of leukemia using deep learning, Journal of\nhealthcare engineering 2020 (2020) 1â€“12.\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 34 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\n[37] M. Mohammed, H. Mwambi, I. B. Mboya, M. K. Elbashir, B. Omolo, A stacking ensemble deep learning approach to cancer type classification based on\nTCGA data, Scientific reports 11 (1) (2021) 15626.\n[38] P. Agarwal, A. Yadav, P. Mathur, Breast cancer prediction on breakhis dataset using deep CNN and transfer learning model, in: Data Engineering for Smart\nSystems: Proceedings of SSIC 2021, Springer, 2022, pp. 77â€“88.\n[39] E. Gomathi, M. Jayasheela, M. Thamarai, M. Geetha, Skin cancer detection using dual optimization based deep learning network, Biomedical Signal\nProcessing and Control 84 (2023) 104968.\n[40] N.Y.Gharaibeh,R.DeFazio,B.Al-Naami,A.-R.Al-Hinnawi,P.Visconti,Automatedlungcancerdiagnosisapplyingbutterworthfiltering,bi-levelfeature\nextraction, and sparce convolutional neural network to luna 16 CT images, Journal of Imaging 10 (7) (2024).\n[41] J. C. Cai, H. Nakai, S. Kuanar, A. T. Froemming, C. W. Bolan, A. Kawashima, H. Takahashi, L. A. Mynderse, C. D. Dora, M. R. Humphreys, et al., Fully\nautomated deep learning model to detect clinically significant prostate cancer at MRI, Radiology 312 (2) (2024) e232635.\n[42] A. Santhoshi, A. Muthukumaravel, Optimizing deep learning algorithms for colorectal tumor identification via noise removal, in: 2024 International\nConference on Knowledge Engineering and Communication Systems (ICKECS), Vol. 1, IEEE, 2024, pp. 1â€“6.\n[43] H. Wang, E. Ahn, J. Kim, A multi-resolution self-supervised learning framework for semantic segmentation in histopathology, Pattern Recognition 155\n(2024) 110621.\n[44] A. M. Mostafa, M. Zakariah, E. A. Aldakheel, Brain tumor segmentation using deep learning on MRI images, Diagnostics 13 (9) (2023) 1562.\n[45] A. Dutta, Using machine learning to identify the risk factors of pancreatic cancer from the nih plco dataset (2023).\n[46] J. M. Johnson, T. M. Khoshgoftaar, Survey on deep learning with class imbalance, Journal of big data 6 (1) (2019) 1â€“54.\n[47] L. Han, Y. Huang, H. Dou, S. Wang, S. Ahamad, H. Luo, Q. Liu, J. Fan, J. Zhang, Semi-supervised segmentation of lesion from breast ultrasound images\nwith attentional generative adversarial network, Computer methods and programs in biomedicine 189 (2020) 105275.\n[48] M.N.Q.Bhuiyan,M.Shamsujjoha,S.H.Ripon,F.H.Proma,F.Khan,Transferlearningandsupervisedclassifierbasedpredictionmodelforbreastcancer,\nin: Big Data Analytics for Intelligent Healthcare Management, Elsevier, 2019, pp. 59â€“86.\n[49] S. Pati, U. Baid, B. Edwards, M. Sheller, S.-H. Wang, G. A. Reina, P. Foley, A. Gruzdev, D. Karkada, C. Davatzikos, et al., Federated learning enables big\ndata for rare cancer boundary detection, Nature communications 13 (1) (2022) 7346.\n[50] J.An,Y.Wang,Q.Cai,G.Zhao,S.Dooper,G.Litjens,Z.Gao,Transformer-basedweaklysupervisedlearningforwholeslidelungcancerimageclassification,\nIEEE Journal of Biomedical and Health Informatics (2024).\n[51] M. Sushil, T. Zack, D. Mandair, Z. Zheng, A. Wali, Y.-N. Yu, Y. Quan, A. J. Butte, A comparative study of zero-shot inference with large language models\nand supervised modeling in breast cancer pathology classification, Research Square (2024).\n[52] X.Liu,C.Yoo,F.Xing,H.Oh,G.ElFakhri,J.-W.Kang,J.Woo,etal.,Deepunsuperviseddomainadaptation:Areviewofrecentadvancesandperspectives,\nAPSIPA Transactions on Signal and Information Processing 11 (1) (2022).\n[53] K. Raza, N. K. Singh, A tour of unsupervised deep learning for medical image analysis, Current Medical Imaging 17 (9) (2021) 1059â€“1077.\n[54] B.Yuan,D.Yang,B.E.Rothberg,H.Chang,T.Xu,Unsupervisedandsupervisedlearningwithneuralnetworkforhumantranscriptomeanalysisandcancer\ndiagnosis, Scientific Reports 10 (1) (2020) 19106.\n[55] F. Silva, T. Pereira, J. Morgado, J. Frade, J. Mendes, C. Freitas, E. Negrao, B. F. De Lima, M. C. Da Silva, A. J. Madureira, et al., EGFR assessment in lung\ncancer CT images: analysis of local and holistic regions of interest using deep unsupervised transfer learning, IEEE Access 9 (2021) 58667â€“58676.\n[56] C.I.Bercea,B.Wiestler,D.Rueckert,S.Albarqouni,Feddis:Disentangledfederatedlearningforunsupervisedbrainpathologysegmentation,arXivpreprint\narXiv:2103.03705 (2021).\n[57] J. N. Stember, H. Shalu, Unsupervised deep clustering and reinforcement learning can accurately segment MRI brain tumors with very small training sets,\nin: International Symposium on Intelligent Informatics, Springer, 2022, pp. 255â€“270.\n[58] O. Pina, V. Vilaplana, Unsupervised domain adaptation for multi-stain cell detection in breast cancer with transformers, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2024, pp. 5066â€“5074.\n[59] H. S. Jung, H. Lee, Y. S. Woo, S. Y. Baek, J. H. Kim, Expansive data, extensive model: Investigating discussion topics around llm through unsupervised\nmachine learning in academic papers and news, Plos one 19 (5) (2024) e0304680.\n[60] M. Grootendorst, BERTopic: Neural topic modeling with a class-based tf-idf procedure, arXiv preprint arXiv:2203.05794 (2022).\n[61] D. Ferber, G. WÃ¶lflein, I. C. Wiest, M. Ligero, S. Sainath, N. G. Laleh, O. S. El Nahhas, G. MÃ¼ller-Franzes, D. JÃ¤ger, D. Truhn, et al., In-context learning\nenables multimodal large language models to classify cancer pathology images, arXiv preprint arXiv:2403.07407 (2024).\n[62] H. Huang, R. Wu, Y. Li, C. Peng, Self-supervised transfer learning based on domain adaptation for benign-malignant lung nodule classification on thoracic\nct, IEEE Journal of Biomedical and Health Informatics 26 (8) (2022) 3860â€“3871.\n[63] S. Dadgar, M. Neshat, Comparative hybrid deep convolutional learning framework with transfer learning for diagnosis of lung cancer, in: International\nConference on Soft Computing and Pattern Recognition, Springer, 2022, pp. 296â€“305.\n[64] T.A.Shaikh,R.Ali,M.S.Beg,Transferlearningprivilegedinformationfuelscaddiagnosisofbreastcancer,MachineVisionandApplications31(1)(2020)\n9.\n[65] Y. Zhang, Z. Li, X. Han, S. Ding, J. Li, J. Wang, S. Ying, J. Shi, Pseudo-data based self-supervised federated learning for classification of histopathological\nimages, IEEE Transactions on Medical Imaging (2023).\n[66] L. Liu, K. Fan, M. Yang, Federated learning: a deep learning model based on RESNET18 dual path for lung nodule detection, Multimedia Tools and\nApplications 82 (11) (2023) 17437â€“17450.\n[67] S.Kumbhare,A.B.Kathole,S.Shinde,Federatedlearningaidedbreastcancerdetectionwithintelligentheuristic-baseddeeplearningframework,Biomedical\nSignal Processing and Control 86 (2023) 105080.\n[68] L. S. Garia, M. Hariharan, Vision transformers for breast cancer classification from thermal images, in: Robotics, Control and Computer Vision: Select\nProceedings of ICRCCV 2022, Springer, 2023, pp. 177â€“185.\n[69] F.Shi,B.Chen,Q.Cao,Y.Wei,Q.Zhou,R.Zhang,Y.Zhou,W.Yang,X.Wang,R.Fan,etal.,Semi-superviseddeeptransferlearningforbenign-malignant\ndiagnosis of pulmonary nodules in chest CT images, IEEE Transactions on medical imaging 41 (4) (2021) 771â€“781.\n[70] N. Thungprue, N. Tamronganunsakul, M. Hongchukiat, K. Sumetpipat, T. Leeboonngam, Using semi-supervised transfer learning for classification of solar\nlentigo,lentigomaligna,andlentigomalignamelanoma,in:202214thBiomedicalEngineeringInternationalConference(BMEiCON),IEEE,2022,pp.1â€“5.\n[71] K.H.Leung,S.P.Rowe,M.S.Sadaghiani,J.P.Leal,E.Mena,P.L.Choyke,Y.Du,M.G.Pomper,Deepsemisupervisedtransferlearningforfullyautomated\nwhole-body tumor quantification and prognosis of cancer on PET/CT, Journal of Nuclear Medicine 65 (4) (2024) 643â€“650.\n[72] W.Wang,R.Jiang,N.Cui,Q.Li,F.Yuan,Z.Xiao,Semi-supervisedvisiontransformerwithadaptivetokensamplingforbreastcancerclassification,Frontiers\nin Pharmacology 13 (2022) 929755.\n[73] X. Yang, Z. Song, I. King, Z. Xu, A survey on deep semi-supervised learning, IEEE Transactions on Knowledge and Data Engineering (2022).\n[74] T. Bdair, N. Navab, S. Albarqouni, FedPerl: Semi-supervised peer learning for skin lesion classification, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer, 2021, pp. 336â€“346.\n[75] C.Barata,V.Rotemberg,N.C.Codella,P.Tschandl,C.Rinner,B.N.Akay,Z.Apalla,G.Argenziano,A.Halpern,A.Lallas,etal.,Areinforcementlearning\nmodel for ai-based decision support in skin cancer, Nature Medicine 29 (8) (2023) 1941â€“1946.\n[76] S. Kumari, A. Das, S. K. Roy, I. Joshi, P. Singh, Leveraging task-specific knowledge from llm for semi-supervised 3d medical image segmentation, arXiv\npreprint arXiv:2407.05088 (2024).\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 35 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\n[77] H. Kheddar, Transformers and large language models for efficient intrusion detection systems: A comprehensive survey, arXiv preprint arXiv:2408.07583\n(2024).\n[78] H. Kheddar, D. W. Dawoud, A. I. Awad, Y. Himeur, M. K. Khan, Reinforcement-learning-based intrusion detection in communication networks: A review,\nIEEE Communications Surveys & Tutorials (2024).\n[79] R. Khajuria, A. Sarwar, Active reinforcement learning based approach for localization of target roi (region of interest) in cervical cell images, Multimedia\nTools and Applications (2024) 1â€“13.\n[80] S. Luo, Lung cancer classification using reinforcement learning-based ensemble learning, International Journal of Advanced Computer Science and\nApplications (IJACSA) 14 (8) (2023).\n[81] Y. Dahdouh, A. Anouar Boudhir, M. Ben Ahmed, A new approach using deep learning and reinforcement learning in healthcare: skin cancer classification,\nInternational journal of electrical and computer engineering systems 14 (5) (2023) 557â€“564.\n[82] E. Pesce, S. J. Withey, P.-P. Ypsilantis, R. Bakewell, V. Goh, G. Montana, Learning to detect chest radiographs containing pulmonary lesions using visual\nattention networks, Medical image analysis 53 (2019) 26â€“38.\n[83] B. Zhao, J. Zhang, D. Ye, J. Cao, X. Han, Q. Fu, W. Yang, Rlogist: fast observation strategy on whole-slide images with deep reinforcement learning, in:\nProceedings of the AAAI Conference on Artificial Intelligence, Vol. 37, 2023, pp. 3570â€“3578.\n[84] R. P. Kumar, K. Venkatraman, C. Jawahar, B. Harish, S. Bharathraj, K. Mukesh, Attention-guided residual network for skin lesion classification using deep\nreinforcement learning, in: 2023 International Conference on Integrated Intelligence and Communication Systems (ICIICS), IEEE, 2023, pp. 1â€“7.\n[85] P. Petousis, S. X. Han, W. Hsu, A. A. Bui, Generating reward functions using irl towards individualized cancer screening, in: International Workshop on\nArtificial Intelligence in Health, Springer, 2018, pp. 213â€“227.\n[86] P. Petousis, A. Winter, W. Speier, D. R. Aberle, W. Hsu, A. A. Bui, Using sequential decision making to improve lung cancer screening performance, Ieee\nAccess 7 (2019) 119403â€“119419.\n[87] G. Renith, A. Senthilselvi, Automated skin cancer diagnosis and localization using deep reinforcement learning, IETE Journal of Research (2024) 1â€“15.\n[88] S. Arora, M. Lamba, 3d brain image based tumor classification using ensemble of reinforcement transfer-based belief neural networks, Multimedia Tools\nand Applications (2024) 1â€“28.\n[89] C. Xu, Y. Song, D. Zhang, L. K. Bittencourt, S. H. Tirumani, S. Li, Spatiotemporal knowledge teacherâ€“student reinforcement learning to detect liver tumors\nwithout contrast agents, Medical Image Analysis 90 (2023) 102980.\n[90] G. Tao, H. Li, J. Huang, C. Han, J. Chen, G. Ruan, W. Huang, Y. Hu, T. Dan, B. Zhang, et al., SeqSeg: a sequential method to achieve nasopharyngeal\ncarcinoma segmentation free from background dominance, Medical Image Analysis 78 (2022) 102381.\n[91] P.Praneeth,A.D.Lakshmi,H.Tekchandani,S.Verma,N.D.Londhe,RL2NdgsNet:Reinforcementlearningbasedefficientclassifierformediastinallymph\nnodesmalignancydetectioninCTimages,in:202213thInternationalConferenceonComputingCommunicationandNetworkingTechnologies(ICCCNT),\nIEEE, 2022, pp. 1â€“5.\n[92] Z. Liu, C. Yao, H. Yu, T. Wu, Deep reinforcement learning with its application for lung cancer detection in medical internet of things, Future Generation\nComputer Systems 97 (2019) 1â€“9.\n[93] P.Balaprakash,R. Egele,M.Salim,S. Wild,V.Vishwanath,F.Xia, T.Brettin,R.Stevens, Scalablereinforcement-learning-basedneuralarchitecturesearch\nfor cancer deep learning research, in: Proceedings of the international conference for high performance computing, networking, storage and analysis, 2019,\npp. 1â€“33.\n[94] U. A. Usmani, J. Watada, J. Jaafar, I. A. Aziz, A. Roy, A reinforcement learning algorithm for automated detection of skin lesions, Applied Sciences 11 (20)\n(2021) 9367.\n[95] G.Maicas,A.P.Bradley,J.C.Nascimento,I.Reid,G.Carneiro,Preandpost-hocdiagnosisandinterpretationofmalignancyfrombreastDCE-MRI,Medical\nimage analysis 58 (2019) 101562.\n[96] R. Huang, Q. Ying, Z. Lin, Z. Zheng, L. Tan, G. Tang, Q. Zhang, M. Luo, X. Yi, P. Liu, et al., Extracting keyframes of breast ultrasound video using deep\nreinforcement learning, Medical Image Analysis 80 (2022) 102490.\n[97] J.Balajee,U.K.B.Haritha,A.S.Reddy,K.Mounika,K.S.Kumar,Pulmonarychestnoduledetectionthroughadaptivereinforcementlearningmodel(arlm),\nin: 2023 Second International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT), IEEE, 2023, pp. 01â€“05.\n[98] Q. Zhang, X. Liu, Y. Wang, Detection of benign and malignant thyroid nodules: an reinforcement region selection network framework, in: 2023 5th\nInternational Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI), IEEE, 2023, pp. 1â€“9.\n[99] S. Narad, K. Reddy, Efficient model for prediction of non-small cells lung cancer via deep q-learning, in: Congress on Intelligent Systems, Springer, 2023,\npp. 403â€“413.\n[100] N. Thakur, P. Kumar, A. Kumar, Reinforcement learning (RL)-based semantic segmentation and attention based backpropagation convolutional neural\nnetwork (ABB-CNN) for breast cancer identification and classification using mammogram images, Neural Computing and Applications (2024) 1â€“27.\n[101] A.Heidari,D.Javaheri,S.Toumaj,N.J.Navimipour,M.Rezaei,M.Unal,AnewlungcancerdetectionmethodbasedonthechestCTimagesusingfederated\nlearning and blockchain systems, Artificial Intelligence in Medicine 141 (2023) 102572.\n[102] B. L. Y. Agbley, J. P. Li, A. U. Haq, E. K. Bankas, C. B. Mawuli, S. Ahmad, S. Khan, A. R. Khan, Federated fusion of magnified histopathological images\nfor breast tumor classification in the internet of medical things, IEEE Journal of Biomedical and Health Informatics (2023).\n[103] Y. Lan, L. Xie, X. Cai, L. Wang, A many-objective evolutionary algorithm based on integrated strategy for skin cancer detection, KSII Transactions on\nInternet and Information Systems (TIIS) 16 (1) (2022) 80â€“96.\n[104] M. A. Mohammed, A. Lakhan, K. H. Abdulkareem, B. Garcia-Zapirain, Federated auto-encoder and xgboost schemes for multi-omics cancer detection in\ndistributed fog computing paradigm, Chemometrics and Intelligent Laboratory Systems 241 (2023) 104932.\n[105] A. H. Omran, S. Y. Mohammed, M. Aljanabi, Detecting data poisoning attacks in federated learning for healthcare applications using deep learning, Iraqi\nJournal for Computer Science and Mathematics 4 (4) (2023) 225â€“237.\n[106] G. N. Gunesli, M. Bilal, S. E. A. Raza, N. M. Rajpoot, A Federated Learning Approach to Tumor Detection in Colon Histology Images, Journal of Medical\nSystems 47 (1) (2023) 99.\n[107] H. AlSalman, M. S. Al-Rakhami, T. Alfakih, M. M. Hassan, Federated Learning Approach for Breast Cancer Detection Based on DCNN, IEEE Access\n(2024).\n[108] V. Jindal, V. Kukreja, D. P. Singh, S. Vats, S. Mehta, Modernizing Lung Cancer Detection: The Federated Learning CNN Approach, in: 2023 4th IEEE\nGlobal Conference for Advancement in Technology (GCAT), IEEE, 2023, pp. 1â€“6.\n[109] A. Vibith, J. Christ, GBDTMO: as new option for early-stage breast cancer detection and classification using machine learning, Automatika: Äasopis za\nautomatiku, mjerenje, elektroniku, raÄunarstvo i komunikacije 64 (4) (2023) 858â€“867.\n[110] Y. Supriya, R. Chengoden, Breast cancer prediction using shapely and game theory in federated learning environment, IEEE Access (2024).\n[111] S. U. Salma, M. S. Sakib, N. Yasaar, M. M. M. Alvee, M. T. Reza, M. Z. Parvez, Privacy focused classification of prostate cancer using federated learning,\nin: International Conference on Information Technology and Applications, Springer, 2022, pp. 265â€“281.\n[112] K. R. Kanjula, A. R. Datla, T. Chen, M. F. Kabir, An edge internet of things framework for machine learning-based skin cancer detection models, in: 2023\nInternational Conference on Machine Learning and Applications (ICMLA), IEEE, 2023, pp. 2167â€“2173.\n[113] F. Wagner, Z. Li, P. Saha, K. Kamnitsas, Post-deployment adaptation with access to source data via federated learning and source-target remote gradient\nalignment, in: International Workshop on Machine Learning in Medical Imaging, Springer, 2023, pp. 253â€“263.\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 36 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\n[114] B.J.Ayekai,C.Wenyu,G.E.S.Addai,A.W.Xornam,K.S.Mawuena,C.B.Mawuli,D.Kulevome,B.L.Agbley,R.E.Turkson,E.E.Kuupole,Personalized\nfederated learning for histopathological prediction of lung cancer, in: 2023 20th International Computer Conference on Wavelet Active Media Technology\nand Information Processing (ICCWAMTIP), IEEE, 2023, pp. 1â€“7.\n[115] E. Gad, M. Abou Khatwa, M. A. Elattar, S. Selim, A Novel Approach to Breast Cancer Segmentation Using U-Net Model with Attention Mechanisms and\nFedProx, in: Annual Conference on Medical Image Understanding and Analysis, Springer, 2023, pp. 310â€“324.\n[116] X. Lessage, L. Collier, C.-H. B. Van Ouytsel, A. Legay, S. Mahmoudi, P. Massonet, Secure federated learning applied to medical imaging with fully\nhomomorphic encryption, in: 2024 IEEE 3rd International Conference on AI in Cybersecurity (ICAIC), IEEE, 2024, pp. 1â€“12.\n[117] F.Kong,X.Wang,J.Xiang,S.Yang,X.Wang,M.Yue,J.Zhang,J.Zhao,X.Han,Y.Dong,etal.,Federatedattentionconsistentlearningmodelsforprostate\ncancer diagnosis and gleason grading, Computational and Structural Biotechnology Journal 23 (2024) 1439â€“1449.\n[118] L. Qiu, J. Cheng, H. Gao, W. Xiong, H. Ren, Federated semi-supervised learning for medical image segmentation via pseudo-label denoising, IEEE journal\nof biomedical and health informatics 27 (10) (2023) 4672â€“4683.\n[119] J.L.Salmeron,I. ArÃ©valo,Aprivacy-preserving,distributedand cooperativefcm-basedlearningapproachforcancer research,in:RoughSets:International\nJoint Conference, IJCRS 2020, Havana, Cuba, June 29â€“July 3, 2020, Proceedings, Springer, 2020, pp. 477â€“487.\n[120] S. Iqbal, A. N. Qureshi, M. Alhussein, K. Aurangzeb, K. Javeed, R. Ali Naqvi, Privacy-preserving collaborative ai for distributed deep learning with cross-\nsectional data, Multimedia Tools and Applications (2023) 1â€“23.\n[121] H. Zhu, G. Han, J. Hou, X. Liu, Y. Ma, Knowledge sharing for pulmonary nodule detection in medical cyber-physical systems, IEEE Journal of Biomedical\nand Health Informatics 27 (2) (2022) 625â€“635.\n[122] K. F. Zubair Nafis, S. Maisha Tarannum, K. Haque Charu, M. H. Kabir Mehedi, A. Alim Rasel, Comparative analysis of federated learning and centralized\napproachfordetectingdifferentlungdiseases,in:Proceedingsofthe20239thinternationalconferenceoncomputertechnologyapplications,2023,pp.60â€“66.\n[123] S. Dagli, K. Dedhia, V. Sawant, A proposed solution to build a breast cancer detection model on confidential patient data using federated learning, in: 2021\nIEEE Bombay Section Signature Conference (IBSSC), IEEE, 2021, pp. 1â€“6.\n[124] C. Yan, X. Zeng, R. Xi, A. Ahmed, M. Hou, M. H. Tunio, PLAâ€”A Privacy-Embedded Lightweight and Efficient Automated Breast Cancer Accurate\nDiagnosis Framework for the Internet of Medical Things, Electronics 12 (24) (2023) 4923.\n[125] G.Mostafa,M.S.Hamidi,D.M.Farid,Detectinglungcancerwithfederatedandtransferlearning,in:202326thInternationalConferenceonComputerand\nInformation Technology (ICCIT), IEEE, 2023, pp. 1â€“6.\n[126] C.-H. Hsiao, F. Y.-S. Lin, T.-L. Sun, Y.-Y. Liao, C.-H. Wu, Y.-C. Lai, H.-P. Wu, P.-R. Liu, B.-R. Xiao, C.-H. Chen, et al., Precision and Robust Models on\nHealthcare Institution Federated Learning for Predicting HCC on Portal Venous CT Images, IEEE Journal of Biomedical and Health Informatics (2024).\n[127] A. JimÃ©nez-SÃ¡nchez, M. Tardy, M. A. G. Ballester, D. Mateus, G. Piella, Memory-aware curriculum federated learning for breast cancer classification,\nComputer Methods and Programs in Biomedicine 229 (2023) 107318.\n[128] M. Sikandar, I. U. Din, A. Almogren, Integrating generative AI and federated learning for privacy preserved sequence-based stomach adenocarcinoma\ndetection, IEEE Transactions on Consumer Electronics (2024).\n[129] Y. N. Tan, P. D. Lam, V. P. Tinh, D.-D. Le, N. H. Nam, T. A. Khoa, Joint federated learning using deep segmentation and the gaussian mixture model for\nbreast cancer tumors, IEEE Access (2024).\n[130] S.Bhadauriya,T.Merothiya,S.C.Yadav,M.ChandraPrabha,DetectionofbraintumourusingCNNinfederatedmachinelearning,in:20235thInternational\nConference on Advances in Computing, Communication Control and Networking (ICAC3N), IEEE, 2023, pp. 653â€“658.\n[131] N. K. Trivedi, S. Jain, S. Kaswan, V. Jain, Federated Learning Empowered Breast Cancer Detection in Images: A YOLO and ResNet-50 Fusion Approach,\nin: 2024 International Conference on Computational Intelligence and Computing Applications (ICCICA), Vol. 1, IEEE, 2024, pp. 24â€“29.\n[132] Y. Habchi, H. Kheddar, Y. Himeur, A. Boukabou, S. Atalla, W. Mansoor, H. Al-Ahmad, Deep transfer learning for kidney cancer diagnosis, arXiv preprint\narXiv:2408.04318 (2024).\n[133] S. S. Sohail, Y. Himeur, H. Kheddar, A. Amira, F. Fadli, S. Atalla, A. Copiaco, W. Mansoor, Advancing 3d point cloud understanding through deep transfer\nlearning: A comprehensive survey, Information Fusion (2024) 102601.\n[134] H. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, F. Bensaali, Deep transfer learning for automatic speech recognition: Towards better generalization,\nKnowledge-Based Systems 277 (2023) 110851.\n[135] H.Kheddar,Y.Himeur,A.I.Awad,Deeptransferlearningforintrusiondetectioninindustrialcontrolnetworks:Acomprehensivereview,JournalofNetwork\nand Computer Applications 220 (2023) 103760.\n[136] H.Aljuaid,N.Alturki,N.Alsubaie,L.Cavallaro,A.Liotta,Computer-aideddiagnosisforbreastcancerclassificationusingdeepneuralnetworksandtransfer\nlearning, Computer Methods and Programs in Biomedicine 223 (2022) 106951.\n[137] M.M.Kalbhor,S.V.Shinde,Cervicalcancerdiagnosisusingconvolutionneuralnetwork:featurelearningandtransferlearningapproaches,SoftComputing\n(2023) 1â€“11.\n[138] K.Bansal,R.Bathla,Y.Kumar,Deeptransferlearningtechniqueswithhybridoptimizationinearlypredictionanddiagnosisofdifferenttypesoforalcancer,\nSoft Computing 26 (21) (2022) 11153â€“11184.\n[139] G. Ayana, J. Park, J.-W. Jeong, S.-w. Choe, A novel multistage transfer learning for ultrasound breast cancer image classification, Diagnostics 12 (1) (2022)\n135.\n[140] Z. Rong, D. Lingyun, L. Jinxing, G. Ying, Diagnostic classification of lung cancer using deep transfer learning technology and multi-omics data, Chinese\nJournal of Electronics 30 (5) (2021) 843â€“852.\n[141] S.Mehmood,T.M.Ghazal,M.A.Khan,M.Zubair,M.T.Naseem,T.Faiz,M.Ahmad,Malignancydetectioninlungandcolonhistopathologyimagesusing\ntransfer learning with class selective image processing, IEEE Access 10 (2022) 25657â€“25668.\n[142] H. Zhou, K. Wang, J. Tian, Online transfer learning for differential diagnosis of benign and malignant thyroid nodules with ultrasound images, IEEE\nTransactions on Biomedical Engineering 67 (10) (2020) 2773â€“2780.\n[143] S. Khan, N. Islam, Z. Jan, I. U. Din, J. J. C. Rodrigues, A novel deep learning based framework for the detection and classification of breast cancer using\ntransfer learning, Pattern Recognition Letters 125 (2019) 1â€“6.\n[144] A. Saber, M. Sakr, O. M. Abo-Seida, A. Keshk, H. Chen, A novel deep-learning model for automatic detection and classification of breast cancer using the\ntransfer-learning technique, IEEe Access 9 (2021) 71194â€“71209.\n[145] V. Anand, S. Gupta, A. Altameem, S. R. Nayak, R. C. Poonia, A. K. J. Saudagar, An enhanced transfer learning based classification for diagnosis of skin\ncancer, Diagnostics 12 (7) (2022) 1628.\n[146] H. M. Balaha, A. E.-S. Hassan, Skin cancer diagnosis based on deep transfer learning and sparrow search algorithm, Neural Computing and Applications\n35 (1) (2023) 815â€“853.\n[147] A. Pati, M. Parhi, B. K. Pattanayak, D. Singh, V. Singh, S. Kadry, Y. Nam, B.-G. Kang, Breast cancer diagnosis based on IoT and deep transfer learning\nenabled by fog computing, Diagnostics 13 (13) (2023) 2191.\n[148] A.Faghihi,M.Fathollahi,R.Rajabi,DiagnosisofskincancerusingVGG16andVGG19basedtransferlearningmodels,MultimediaToolsandApplications\n83 (19) (2024) 57495â€“57510.\n[149] R. Rai, D. S. Sisodia, Real-time data augmentation based transfer learning model for breast cancer diagnosis using histopathological images, in: Advances\nin Biomedical Engineering and Technology: Select Proceedings of ICBEST 2018, Springer, 2021, pp. 473â€“488.\n[150] S. Deepak, P. Ameer, Brain tumor classification using deep CNN features via transfer learning, Computers in biology and medicine 111 (2019) 103345.\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 37 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\n[151] K. S. Rao, P. V. Terlapu, D. Jayaram, K. K. Raju, G. K. Kumar, R. Pemula, V. Gopalachari, S. Rakesh, Intelligent ultrasound imaging for enhanced breast\ncancer diagnosis: Ensemble transfer learning strategies, IEEE Access (2024).\n[152] Y. Celik, M. Talo, O. Yildirim, M. Karabatak, U. R. Acharya, Automated invasive ductal carcinoma detection based using deep transfer learning with\nwhole-slide images, Pattern Recognition Letters 133 (2020) 232â€“239.\n[153] Y. Chen, X. Qin, J. Xiong, S. Xu, J. Shi, H. Lv, L. Li, H. Xing, Q. Zhang, Deep transfer learning for histopathological diagnosis of cervical cancer using\nconvolutional neural networks with visualization schemes, Journal of Medical Imaging and Health Informatics 10 (2) (2020) 391â€“400.\n[154] N.A.Samee,A.A.Alhussan,V.F.Ghoneim,G.Atteia,R.Alkanhel,M.A.Al-Antari,Y.M.Kadah,AhybriddeeptransferlearningofCNN-basedLR-PCA\nfor breast lesion diagnosis via medical breast mammograms, Sensors 22 (13) (2022) 4938.\n[155] N.A.Samee,N.F.Mahmoud,G.Atteia,H.A.Abdallah,M.Alabdulhafith,M.S.Al-Gaashani,S.Ahmad,M.S.A.Muthanna,Classificationframeworkfor\nmedical diagnosis of brain tumor with an effective hybrid transfer learning model, Diagnostics 12 (10) (2022) 2541.\n[156] S.Aziz,K.Munir,A.Raza,M.S.Almutairi,S.Nawaz,IVNet:Transferlearningbaseddiagnosisofbreastcancergradingusinghistopathologicalimagesof\ninfected cells, IEEE Access 11 (2023) 127880â€“127894.\n[157] L. Alzubaidi, M. Al-Amidie, A. Al-Asadi, A. J. Humaidi, O. Al-Shamma, M. A. Fadhel, J. Zhang, J. SantamarÃ­a, Y. Duan, Novel transfer learning approach\nfor medical imaging with limited labeled data, Cancers 13 (7) (2021) 1590.\n[158] M.T.Mali,E.Hancer,R.Samet,Z.YÄ±ldÄ±rÄ±m,N.Nemati,Detectionofcolorectalcancerwithvisiontransformers,in:2022InnovationsinIntelligentSystems\nand Applications Conference (ASYU), IEEE, 2022, pp. 1â€“6.\n[159] Q. Abbas, Y. Daadaa, U. Rashid, M. E. Ibrahim, Assist-dermo: A lightweight separable vision transformer model for multiclass skin lesion classification,\nDiagnostics 13 (15) (2023) 2531.\n[160] T. Gulsoy, E. B. Kablan, Diagnosis of lung cancer based on CT scans using vision transformers, in: 2023 14th International Conference on Electrical and\nElectronics Engineering (ELECO), IEEE, 2023, pp. 1â€“5.\n[161] I. Pacal, Maxcervixt: A novel lightweight vision transformer-based approach for precise cervical cancer detection, Knowledge-Based Systems 289 (2024)\n111482.\n[162] G.Ayana,H.Barki,S.-w.Choe,Pathologicalinsights:Enhancedvisiontransformersfortheearlydetectionofcolorectalcancer,Cancers16(7)(2024)1441.\n[163] Z.Zhou,S.Qiu,Y.Wang,M.Zhou,X.Chen,M.Hu,Q.Li,Y.Lu,Swin-spectraltransformerforcholangiocarcinomahyperspectralimagesegmentation,in:\n2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), IEEE, 2021, pp. 1â€“6.\n[164] C. Zhao, R. Shuai, L. Ma, W. Liu, M. Wu, Improving cervical cancer classification with imbalanced datasets combining taming transformers with t2t-vit,\nMultimedia tools and applications 81 (17) (2022) 24265â€“24300.\n[165] L. Wang, J. Liu, P. Jiang, D. Cao, B. Pang, Lgvit: Local-global vision transformer for breast cancer histopathological image classification, in: ICASSP\n2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2023, pp. 1â€“5.\n[166] R. J. Chen, R. G. Krishnan, Self-supervised vision transformers learn visual concepts in histopathology, arXiv preprint arXiv:2203.00585 (2022).\n[167] S. Tabatabaei, K. Rezaee, M. Zhu, Attention transformer mechanism and fusion-based deep learning architecture for MRI brain tumor classification system,\nBiomedical Signal Processing and Control 86 (2023) 105119.\n[168] L. Chen, H. Ge, J. Li, Crossformer: Multi-scale cross-attention for polyp segmentation, IET Image Processing 17 (12) (2023) 3441â€“3452.\n[169] A. Sriwastawa, J. A. Arul Jothi, Vision transformer and its variants for image classification in digital breast cancer histopathology: A comparative study,\nMultimedia Tools and Applications 83 (13) (2024) 39731â€“39753.\n[170] K.Ikromjanov,S.Bhattacharjee,Y.-B.Hwang,R.I.Sumon,H.-C.Kim,H.-K.Choi,Wholeslideimageanalysisanddetectionofprostatecancerusingvision\ntransformers, in: 2022 international conference on artificial intelligence in information and communication (ICAIIC), IEEE, 2022, pp. 399â€“402.\n[171] S. Tummala, S. Kadry, S. A. C. Bukhari, H. T. Rauf, Classification of brain tumor from magnetic resonance imaging using vision transformers ensembling,\nCurrent Oncology 29 (10) (2022) 7498â€“7511.\n[172] E.Pachetti,S.Colantonio,M.A.Pascali,Ontheeffectivenessof3Dvisiontransformersforthepredictionofprostatecanceraggressiveness,in:International\nConference on Image Analysis and Processing, Springer, 2022, pp. 317â€“328.\n[173] J.Sun,B.Wu,T. Zhao,L.Gao,K.Xie,T.Lin,J.Sui, X.Li,X.Wu,X.Ni,Classificationforthyroid noduleusingvitwithcontrastivelearninginultrasound\nimages, Computers in biology and medicine 152 (2023) 106444.\n[174] X.Chen,K.Zhang,N.Abdoli,P.W.Gilley,X.Wang,H.Liu,B.Zheng,Y.Qiu,Transformersimprovebreastcancerdiagnosisfromunregisteredmulti-view\nmammograms, Diagnostics 12 (7) (2022) 1549.\n[175] G. Yang, S. Luo, P. Greer, A novel vision transformer model for skin cancer classification, Neural Processing Letters 55 (7) (2023) 9335â€“9351.\n[176] G. Ayana, S.-w. Choe, Vision transformers-based transfer learning for breast mass classification from multiple diagnostic modalities, Journal of Electrical\nEngineering & Technology (2024) 1â€“20.\n[177] R. R. Nejad, S. Hooshmand, HViT4Lung: hybrid vision transformers augmented by transfer learning to enhance lung cancer diagnosis, in: 2023 5th\nInternational Conference on Bio-engineering for Smart Technologies (BioSMART), IEEE, 2023, pp. 1â€“7.\n[178] S. Hossain, A. Chakrabarty, T. R. Gadekallu, M. Alazab, M. J. Piran, Vision transformers, ensemble model, and transfer learning leveraging explainable ai\nfor brain tumor detection and classification, IEEE Journal of Biomedical and Health Informatics 28 (3) (2023) 1261â€“1272.\n[179] J.Yang,A.Rusak,A.Belozubov,Enhancingbraintumorclassificationusingdata-efficientimagetransformer,in:2024InternationalRussianSmartIndustry\nConference (SmartIndustryCon), IEEE, 2024, pp. 339â€“343.\n[180] Z. Zhou, G. Sun, L. Yu, S. Tian, G. Xiao, J. Wang, S. Zhou, Rfia-net: Rich CNN -transformer network based on asymmetric fusion feature aggregation to\nclassify stage i multimodality oesophageal cancer images, Engineering Applications of Artificial Intelligence 118 (2023) 105703.\n[181] C. Xin, Z. Liu, K. Zhao, L. Miao, Y. Ma, X. Zhu, Q. Zhou, S. Wang, L. Li, F. Yang, et al., An improved transformer network for skin cancer classification,\nComputers in Biology and Medicine 149 (2022) 105939.\n[182] T. Zhang, Y. Feng, Y. Zhao, G. Fan, A. Yang, S. Lyu, P. Zhang, F. Song, C. Ma, Y. Sun, et al., Msht: Multi-stage hybrid transformer for the rose image\nanalysis of pancreatic cancer, IEEE Journal of Biomedical and Health Informatics 27 (4) (2023) 1946â€“1957.\n[183] P. Yin, B. Yu, C. Jiang, H. Chen, Pyramid tokens-to-token vision transformer for thyroid pathology image classification, in: 2022 Eleventh International\nConference on image processing theory, tools and applications (IPTA), IEEE, 2022, pp. 1â€“6.\n[184] S. J. Wagner, D. ReisenbÃ¼chler, N. P. West, J. M. Niehues, J. Zhu, S. Foersch, G. P. Veldhuizen, P. Quirke, H. I. Grabsch, P. A. van den Brandt, et al.,\nTransformer-based biomarker prediction from colorectal cancer histology: A large-scale multicentric study, Cancer Cell 41 (9) (2023) 1650â€“1661.\n[185] A. Alotaibi, T. Alafif, F. Alkhilaiwi, Y. Alatawi, H. Althobaiti, A. Alrefaei, Y. Hawsawi, T. Nguyen, Vit-deit: An ensemble model for breast cancer\nhistopathological images classification, in: 2023 1st International Conference on Advanced Innovations in Smart Cities (ICAISC), IEEE, 2023, pp. 1â€“6.\n[186] Y. Gulzar, S. A. Khan, Skin lesion segmentation based on vision transformers and convolutional neural networksâ€”a comparative study, Applied Sciences\n12 (12) (2022) 5990.\n[187] S. S. Boudouh, M. Bouakkaz, Advancing precision in breast cancer detection: a fusion of vision transformers and CNNs for calcification mammography\nclassification, Applied Intelligence (2024) 1â€“14.\n[188] G.J.Ferdous,K.A.Sathi,M.A.Hossain,M.M.Hoque,M.A.A.Dewan,Lcdeit:Alinearcomplexitydata-efficientimagetransformerforMRIbraintumor\nclassification, IEEE Access 11 (2023) 20337â€“20350.\n[189] S. M. H. Hashemi, L. Safari, A. D. Taromi, Realism in action: Anomaly-aware diagnosis of brain tumors from medical images using YOLOv8 and DeiT,\narXiv preprint arXiv:2401.03302 (2024).\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 38 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\n[190] S. Ayas, Multiclass skin lesion classification in dermoscopic images using Swin transformer model, Neural Computing and Applications 35 (9) (2023)\n6713â€“6722.\n[191] I. Pacal, A novel Swin transformer approach utilizing residual multi-layer perceptron for diagnosing brain tumors in MRI images, International Journal of\nMachine Learning and Cybernetics (2024) 1â€“19.\n[192] S. Tummala, J. Kim, S. Kadry, Breast-net: Multi-class classification of breast cancer from histopathological images using ensemble of Swin transformers,\nMathematics 10 (21) (2022) 4109.\n[193] S.Chaudhury,K.Sau,N.Shelke,Transformingbreastcancerimageclassificationwithvisiontransformersandlstmintegration,in:2024IEEEInternational\nStudentsâ€™ Conference on Electrical, Electronics and Computer Science (SCEECS), IEEE, 2024, pp. 1â€“8.\n[194] D. Gai, J. Zhang, Y. Xiao, W. Min, Y. Zhong, Y. Zhong, Rmtf-net: Residual mix transformer fusion net for 2d brain tumor segmentation, Brain Sciences\n12 (9) (2022) 1145.\n[195] S. Basu, M. Gupta, P. Rana, P. Gupta, C. Arora, Radformer: Transformers with globalâ€“local attention for interpretable and accurate gallbladder cancer\ndetection, Medical Image Analysis 83 (2023) 102676.\n[196] H. Cai, X. Feng, R. Yin, Y. Zhao, L. Guo, X. Fan, J. Liao, Mist: multiple instance learning network based on Swin transformer for whole slide image\nclassification of colorectal adenomas, The Journal of Pathology 259 (2) (2023) 125â€“135.\n[197] Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. Bovik, Y. Li, Maxvit: Multi-axis vision transformer, in: European conference on computer vision,\nSpringer, 2022, pp. 459â€“479.\n[198] H. Barzekar, Y. Patel, L. Tong, Z. Yu, Multinet with transformers: a model for cancer diagnosis using images, arXiv preprint arXiv:2301.09007 (2023).\n[199] D. Rossi, A. A. Citarella, F. De Marco, L. Di Biasi, G. Tortora, Comparative analysis of diabetes diagnosis: We-lstm networks and wizardlm-powered\ndiabetalk chatbot, in: 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), IEEE, 2024, pp. 6859â€“6866.\n[200] M. Li, A. Blaes, S. Johnson, H. Liu, H. Xu, R. Zhang, Cancerllm: A large language model in cancer domain, arXiv preprint arXiv:2406.10459 (2024).\n[201] N. Naik, A. Khandelwal, M. Joshi, M. Atre, H. Wright, K. Kannan, S. Hill, G. Mamidipudi, G. Srinivasa, C. Bifulco, et al., Applying large language models\nfor causal structure learning in non small cell lung cancer, in: 2024 IEEE 12th International Conference on Healthcare Informatics (ICHI), IEEE, 2024, pp.\n688â€“693.\n[202] X. Wu, G. Li, X. Wang, Z. Xu, Y. Wang, J. Xian, X. Wang, G. Li, K. Yuan, Diagnosis assistant for liver cancer utilizing a large language model with three\ntypes of knowledge, arXiv preprint arXiv:2406.18039 (2024).\n[203] C.-H. Change, M. M. Lucas, G. Lu-Yao, C. C. Yang, Classifying cancer stage with open-source clinical large language models, in: 2024 IEEE 12th\nInternational Conference on Healthcare Informatics (ICHI), IEEE, 2024, pp. 76â€“82.\n[204] S. Sivarajkumar, S. Edupuganti, M. Bhattacharya, D. Lazris, M. Davis, Y. Huang, Y. Wang, Automating the detection of treatment progression in patients\nwith lung cancer using large language models. (2024).\n[205] D.Sun,L. Hadjiiski,J.Gormley,H.-P. Chan,E.M.Caoili,R. Cohan,A.Alva,R. Mihalcea,C.Zhou,V.Gulani, Largelanguagemodel-assistedinformation\nextraction from clinical reports for survival prediction of bladder cancer patients, in: Medical Imaging 2024: Computer-Aided Diagnosis, Vol. 12927, SPIE,\n2024, pp. 449â€“454.\n[206] S. Deng, L. Sha, Y. Jin, T. Zhou, C. Wang, Q. Liu, H. Guo, C. Xiong, Y. Xue, X. Li, et al., Genellm: A large cfRNA language model for cancer screening\nfrom raw reads, bioRxiv (2024) 2024â€“06.\n[207] C.-H.Chang,M.M.Lucas,G.Lu-Yao,C.C.Yang,Classifyingcancerstagewithopen-sourceclinicallargelanguagemodels,arXivpreprintarXiv:2404.01589\n(2024).\n[208] J. J. Cao, D. H. Kwon, T. T. Ghaziani, P. Kwo, G. Tse, A. Kesselman, A. Kamaya, J. R. Tse, Large language modelsâ€™ responses to liver cancer surveillance,\ndiagnosis, and management questions: accuracy, reliability, readability, Abdominal Radiology (2024) 1â€“9.\n[209] S. Zhou, X. Luo, C. Chen, H. Jiang, C. Yang, G. Ran, J. Yu, C. Yin, The performance of large language model powered chatbots compared to oncology\nphysicians on colorectal cancer queries, International Journal of Surgery (2024) 10â€“1097.\n[210] D. Lee, A. Vaid, K. Menon, R. Freeman, D. Matteson, M. Marin, G. Nadkarni, Development of a privacy preserving large language model for automated\ndata extraction from thyroid cancer pathology reports, medRxiv (2023) 2023â€“11.\n[211] P. Manjunath, B. Lerner, T. Dunn, Towards interactive and interpretable image retrieval-based diagnosis: Enhancing brain tumor classification with llm\nexplanations and latent structure preservation, in: International Conference on Artificial Intelligence in Medicine, Springer, 2024, pp. 335â€“349.\n[212] J. Lammert, T. F. Dreyer, A. M. LÃ¶rsch, J. Jung, S. Lange, N. Pfarr, A. Durner, M. B. Kiechle, U. A. Schatz, S. Mathes, et al., Large language models for\nprecision oncology: Clinical decision support through expert-guided learning. (2024).\n[213] S. Rajaganapathy, S. Chowdhury, V. Buchner, Z. He, X. Jiang, P. Yang, J. R. Cerhan, N. Zong, Synoptic reporting by summarizing cancer pathology reports\nusing large language models, medRxiv (2024) 2024â€“04.\n[214] H.S.Choi,J.Y.Song,K.H.Shin,J.H.Chang,B.-S.Jang,Developingpromptsfromlargelanguagemodelforextractingclinicalinformationfrompathology\nand ultrasound reports in breast cancer, Radiation Oncology Journal 41 (3) (2023) 209.\n[215] H.Matsuo,M.Nishio,T.Matsunaga,K.Fujimoto,T.Murakami,Exploringmultilinguallargelanguagemodelsforenhancedtnmclassificationofradiology\nreport in lung cancer staging, arXiv preprint arXiv:2406.06591 (2024).\n[216] L.Deng,T.Wang,Z.Zhai,W.Tao,J.Li,Y.Zhao,S.Luo,J.Xu,etal.,Evaluationoflargelanguagemodelsinbreastcancerclinicalscenarios:acomparative\nanalysis based on chatgpt-3.5, chatgpt-4.0, and claude2, International Journal of Surgery 110 (4) (2024) 1941â€“1950.\n[217] M. Shiraishi, K. Kanayama, R. Yang, M. Okazaki, Preliminary evaluation of the potential of commercially available large language models in diagnosing\nskin tumours, Clinical and Experimental Dermatology (2023) llad430.\n[218] M. R. Karim, L. M. Comet, M. Shajalal, O. Beyan, D. Rebholz-Schuhmann, S. Decker, From large language models to knowledge graphs for biomarker\ndiscovery in cancer, arXiv preprint arXiv:2310.08365 (2023).\n[219] F. Putz, M. Haderlein, S. Lettmaier, S. Semrau, R. Fietkau, Y. Huang, Exploring the capabilities and limitations of large language models for radiation\noncology decision support, International Journal of Radiation Oncology, Biology, Physics 118 (4) (2024) 900â€“904.\n[220] A. Darwish, A. E. Hassanien, S. Das, A survey of swarm and evolutionary computing approaches for deep learning, Artificial intelligence review 53 (2020)\n1767â€“1812.\n[221] W. Cui, A. Aouidate, S. Wang, Q. Yu, Y. Li, S. Yuan, Discovering anti-cancer drugs via computational methods, Frontiers in pharmacology 11 (2020) 733.\n[222] M. Biswas, V. Kuppili, L. Saba, D. R. Edla, H. S. Suri, E. Cuadrado-Godia, J. R. Laird, R. T. Marinhoe, J. M. Sanches, A. Nicolaides, et al., State-of-the-art\nreview on deep learning in medical imaging, Frontiers in Bioscience-Landmark 24 (3) (2019) 380â€“406.\n[223] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imperative style, high-\nperformance deep learning library, Advances in neural information processing systems 32 (2019).\n[224] E. Gianniti, L. Zhang, D. Ardagna, Performance prediction of GPU-based deep learning applications, in: 2018 30th International Symposium on Computer\nArchitecture and High Performance Computing (SBAC-PAD), IEEE, 2018, pp. 167â€“170.\n[225] S.Hossain,D.-j.Lee,Deeplearning-basedreal-timemultiple-objectdetectionandtrackingfromaerialimageryviaaflyingrobotwithGPU-basedembedded\ndevices, Sensors 19 (15) (2019) 3371.\n[226] T.Wang,C.Wang,X.Zhou,H.Chen,AnoverviewofFPGAbaseddeeplearningaccelerators:challengesandopportunities,in:2019IEEE21stInternational\nConference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference\non Data Science and Systems (HPCC/SmartCity/DSS), IEEE, 2019, pp. 1674â€“1681.\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 39 of 40\nAdvanced Deep Learning and Large Language Models for Cancer Detection\n[227] A. Namburu, D. Sumathi, R. Raut, R. H. Jhaveri, R. K. Dhanaraj, N. Subbulakshmi, B. Balusamy, FPGA-based deep learning models for analysing corona\nusing chest X-ray images, Mobile Information Systems 2022 (2022) 1â€“14.\n[228] A. Gueriani, H. Kheddar, A. C. Mazari, Deep reinforcement learning for intrusion detection in IoT: A survey, in: 2023 2nd International Conference on\nElectronics, Energy and Measurement (IC2EM), Vol. 1, IEEE, 2023, pp. 1â€“7.\n[229] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. KÃ¼ttler, M. Lewis, W.-t. Yih, T. RocktÃ¤schel, et al., Retrieval-augmented generation for\nknowledge-intensive NLP tasks, Advances in Neural Information Processing Systems 33 (2020) 9459â€“9474.\nDr. Y Habchi et al.:Preprint submitted to Elsevier Page 40 of 40",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5989173650741577
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5873230695724487
    },
    {
      "name": "Cancer detection",
      "score": 0.5849546194076538
    },
    {
      "name": "Deep learning",
      "score": 0.5461515188217163
    },
    {
      "name": "Natural language processing",
      "score": 0.5087307095527649
    },
    {
      "name": "Machine learning",
      "score": 0.4173704981803894
    },
    {
      "name": "Data science",
      "score": 0.3201913833618164
    },
    {
      "name": "Cancer",
      "score": 0.3059386610984802
    },
    {
      "name": "Medicine",
      "score": 0.12336179614067078
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 5
}