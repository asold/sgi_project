{
  "title": "Lightweight transformers for clinical natural language processing",
  "url": "https://openalex.org/W4390789641",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4223402701",
      "name": "Rohanian, Omid",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A4227475935",
      "name": "Nouriborji, Mohammadmahdi",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4306887250",
      "name": "Jauncey, Hannah",
      "affiliations": [
        "University of Oxford",
        "Infectious Diseases Data Observatory"
      ]
    },
    {
      "id": "https://openalex.org/A2519834063",
      "name": "Kouchaki, Samaneh",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A4287528018",
      "name": "Nooralahzadeh, Farhad",
      "affiliations": [
        "University Hospital of Zurich",
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A4201025057",
      "name": "- -",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A3176290025",
      "name": "Clifton Lei",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A4306887255",
      "name": "Merson, Laura",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A4226347835",
      "name": "Clifton, David A.",
      "affiliations": [
        "Suzhou Research Institute",
        "University of Oxford"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W1914425800",
    "https://openalex.org/W3010607409",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2137407193",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W3046293795",
    "https://openalex.org/W3165327186",
    "https://openalex.org/W4386566689",
    "https://openalex.org/W2984974929",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W6793900039",
    "https://openalex.org/W2915045810",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W2164572080",
    "https://openalex.org/W6982663466",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W6640146235",
    "https://openalex.org/W4200212713",
    "https://openalex.org/W3214897310",
    "https://openalex.org/W2910485119",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2225495139",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W4214738824",
    "https://openalex.org/W6773815586",
    "https://openalex.org/W4206491499",
    "https://openalex.org/W2963572446",
    "https://openalex.org/W2911321984",
    "https://openalex.org/W3115897805",
    "https://openalex.org/W2788388592",
    "https://openalex.org/W3134399782",
    "https://openalex.org/W3215823844",
    "https://openalex.org/W3171324702",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4321749402",
    "https://openalex.org/W1541954861",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3214234195",
    "https://openalex.org/W3106298421",
    "https://openalex.org/W6770054164",
    "https://openalex.org/W4308768904",
    "https://openalex.org/W3034340181",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2955483668",
    "https://openalex.org/W1034374084",
    "https://openalex.org/W4288684979",
    "https://openalex.org/W3196412195",
    "https://openalex.org/W4249023852",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W4226452184"
  ],
  "abstract": "Abstract Specialised pre-trained language models are becoming more frequent in Natural language Processing (NLP) since they can potentially outperform models trained on generic texts. BioBERT (Sanh et al., Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv: 1910.01108 , 2019) and BioClinicalBERT (Alsentzer et al., Publicly available clinical bert embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop , pp. 72–78, 2019) are two examples of such models that have shown promise in medical NLP tasks. Many of these models are overparametrised and resource-intensive, but thanks to techniques like knowledge distillation, it is possible to create smaller versions that perform almost as well as their larger counterparts. In this work, we specifically focus on development of compact language models for processing clinical texts (i.e. progress notes, discharge summaries, etc). We developed a number of efficient lightweight clinical transformers using knowledge distillation and continual learning, with the number of parameters ranging from $15$ million to $65$ million. These models performed comparably to larger models such as BioBERT and ClinicalBioBERT and significantly outperformed other compact models trained on general or biomedical data. Our extensive evaluation was done across several standard datasets and covered a wide range of clinical text-mining tasks, including natural language inference, relation extraction, named entity recognition and sequence classification. To our knowledge, this is the first comprehensive study specifically focused on creating efficient and compact transformers for clinical NLP tasks. The models and code used in this study can be found on our Huggingface profile at https://huggingface.co/nlpie and Github page at https://github.com/nlpie-research/Lightweight-Clinical-Transformers , respectively, promoting reproducibility of our results.",
  "full_text": "Natural Language Engineering(2024), 30, pp. 887–914\ndoi:10.1017/S1351324923000542\nARTICLE\nLightweight transformers for clinical natural\nlanguage processing\nOmid Rohanian1,2 , Mohammadmahdi Nouriborji 2,3, Hannah Jauncey 4, Samaneh Kouchaki 5,\nFarhad Nooralahzadeh6,7, ISARIC Clinical Characterisation Group 8, Lei Clifton 9,L a u r aM e r s o n8\nand David A. Clifton 1,10\n1Department of Engineering Science, University of Oxford, Oxford, UK, 2NLPie Research, Oxford, UK, 3Sharif University\nof Technology, Tehran, Iran, 4Infectious Diseases Data Observatory (IDDO), University of Oxford, Oxford, UK,\n5Department of Electrical and Electronic Engineering, University of Surrey, Guildford, UK, 6University of Zürich, Zürich,\nSwitzerland, 7University Hospital of Zürich, Zürich, Switzerland, 8ISARIC, Pandemic Sciences Institute, University of\nOxford, Oxford, UK, 9Nufﬁeld Department of Population Health, University of Oxford, Oxford, UK, and 10Oxford-Suzhou\nCentre for Advanced Research, Suzhou, China\nCorresponding author: Omid Rohanian; Email: omid.rohanian@eng.ox.ac.uk\n(Received 18 June 2023; revised 23 November 2023; accepted 26 November 2023; ﬁrst published online 12 January 2024)\nAbstract\nSpecialised pre-trained language models are becoming more frequent in Natural language Processing\n(NLP) since they can potentially outperform models trained on generic texts. BioBERT (Sanh et al.,\nDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv: 1910.01108,\n2019) and BioClinicalBERT (Alsentzer et al., Publicly available clinical bert embeddings. In Proceedings\nof the 2nd Clinical Natural Language Processing Workshop, pp. 72–78, 2019) are two examples of such\nmodels that have shown promise in medical NLP tasks. Many of these models are overparametrised and\nresource-intensive, but thanks to techniques like knowledge distillation, it is possible to create smaller\nversions that perform almost as well as their larger counterparts. In this work, we speciﬁcally focus\non development of compact language models for processing clinical texts (i.e. progress notes, discharge\nsummaries, etc). We developed a number of efﬁcient lightweight clinical transformers using knowledge\ndistillation and continual learning, with the number of parameters ranging from 15 million to 65 mil-\nlion. These models performed comparably to larger models such as BioBERT and ClinicalBioBERT and\nsigniﬁcantly outperformed other compact models trained on general or biomedical data. Our extensive\nevaluation was done across several standard datasets and covered a wide range of clinical text-mining tasks,\nincluding natural language inference, relation extraction, named entity recognition and sequence classiﬁ-\ncation. To our knowledge, this is the ﬁrst comprehensive study speciﬁcally focused on creating efﬁcient\nand compact transformers for clinical NLP tasks. The models and code used in this study can be found\non our Huggingface proﬁle at https://huggingface.co/nlpie and Github page at https://github.com/nlpie-\nresearch/Lightweight-Clinical-Transformers, respectively, promoting reproducibility of our results.\nKeywords: Machine learning; natural language processing for biomedical texts\n1. Introduction\nLarge language models pre-trained on generic texts serve as the foundation upon which most\nstate-of-the-art Natural language Processing (NLP) models are built. There is ample evidence that,\nfor certain domains and downstream tasks, models that are pre-trained on specialised data out-\nperform baselines that have only relied on generic texts (Sanh et al.2019; Alsentzer et al.2019;\nBeltagy, Lo, and Cohan 2019; Nguyen, Vu, and Nguyen 2020; Chalkidis et al.2020).\nC⃝ The Author(s), 2024. Published by Cambridge University Press. This is an Open Access article, distributed under the terms of the\nCreative Commons Attribution licence ( http://creativecommons.org/licenses/by/4.0/), which permits unrestricted re-use, distribution and\nreproduction, provided the original article is properly cited.\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n888 O. Rohanian et al.\nThese models, however, are heavy in size and number of parameters, making them unsuitable\nfor devices with limited memory and processing capacity. Furthermore, the rate at which leading\ntechnology corporations build these progressively larger and more resource-intensive models is a\nsubject of debate in the Artiﬁcial Intelligence (AI) community (Bender et al.2021), and there is\ninterest in developing methods that would make these tools more accessible by creating smaller\nand faster versions of them that would run reasonably well on smaller devices (Li et al. 2020;\nSchick and Schütze 2021). This would allow independent researchers, particularly those from low-\nincome nations, to contribute to the advancement of AI.\nFrom the point of view of energy consumption and environmental impact, developing smaller\ntransformer-based language models can be thought of as a step towards green AI(Schwartz et al.\n2020), an approach to developing AI that prioritises sustainable use of computational resources\nand development of models with minimal carbon footprint (Strubell, Ganesh, and McCallum\n2019).\nNumerous works exist in the NLP literature with the aim to develop fast, efﬁcient and\nlightweight versions of larger transformer-based models (Sanh et al. 2019; Jiao et al. 2020;S u n\net al.2020;D e h g h a n iet al.2021). However, there are comparatively fewer compact models devel-\noped for special domains like law, biology and medicine (Ozyurt 2020; Bambroo and Awasthi\n2021; Rohanian et al. 2022). The present work focuses on development of efﬁcient lightweight\nlanguage models speciﬁcally developed for clinical NLP tasks. These models can be used to pro-\ncess a range of different clinical texts including patient history, discharge summaries and progress\nnotes. The contributions of this work are as follows:\n We pre-trained ﬁve different compact clinical models using either distillation or continual\nlearning on the MIMIC-III notes dataset.\n We used three different distillation techniques for training models in varying sizes and\narchitectures.\n We evaluated our models on named entity recognition (NER), relation extraction (RE) and\nsequence classiﬁcation (CLS) on four widely used clinical datasets plus an internal cancer\nidentiﬁcation dataset.\n We are the ﬁrst to focus exclusively on developing compact clinical language models, and\nwe make all of our models publicly available on Huggingface.\na\n2. Clinical notes in electronic health records (EHR)\nClinical notes are written documents generated by medical practitioners in order to communicate\ninformation about a patient treated at a health facility (Rethans, Martin, and Metsemakers 1994).\nThese documents are regarded as ‘unstructured data’. This means that, unlike tabular data that\nare categorised and quantiﬁable, clinical notes are irregular, disorganised and not coded using\npredeﬁned terms that domain experts would all understand (Boulton and Hammersley 2006;\nRosenbloom et al. 2011). Clinical notes contain a wide range of information about a patient,\nranging from medical history and response to medication to discharge summaries and even\nbilling. Fast Healthcare Interoperability Resources (FHIR)\nb identiﬁes eight different types of clin-\nical notes, including consultations notes, imaging narratives, laboratory reports and procedure\nnotes (Bender and Sartipi 2013).\nUnstructured data constitutes 80% of all electronic health records (EHR) data (Kong 2019;\nMahbub et al.2022) and can potentially contain information that is otherwise not present else-\nwhere in the patient’s EHR (Zhang et al.2022). They can, therefore, be exploited by computational\nahttps://huggingface.co/nlpie\nbhttps://hl7.org/fhir/us/core/stu4/clinical-notes-guidance.html\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 889\nmodels to infer more information about a patient or develop predictive models for patient mon-\nitoring. There are numerous examples in the literature where clinical notes have been used,\nsometimes in conjunction with structured data, to develop diagnostic or predictive models. Some\nexamples include adverse drug effects (Dandala, Joopudi, and Devarakonda 2019), (Mahendran\nand McInnes 2021), self-harm and drug abuse prediction (Obeid et al.2020; Ridgway et al.2021),\nhospitalisation and readmission risk (Huang, Altosaar, and Ranganath 2019; Song et al. 2022),\nmortality prediction (Si and Roberts 2019;Y e et al.2020) and automatic phenotype annotation\n(Zhang et al.2022).\nUnless preprocessed and redacted, clinical notes might contain private information about\npatients, and models trained on this data are known to be prone to adversarial attacks (Lehman\net al. 2021). For this reason, they are yet to be widely used for research and different de-\nidentiﬁcation methods have been developed to automatically remove personal identiﬁable infor-\nmation from text documents in order for them to be securely shared with other researchers\n(Melamud and Shivade 2019; Hartman et al.2020).\n2.1. Biomedical vs clinical texts\nThere is a distinction in the literature between clinical and biomedical texts and they are under-\nstood to be different in terms of their linguistic qualities (Alsentzer et al.2019). Clinical notes are\ncollected by healthcare professionals when the patient is seen or being treated. They are free-text,\nwithout a ﬁxed structure, can contain spelling errors, abbreviations, non-standard grammar, dif-\nferences in personal style and words and phrases from different languages. These characteristics\ncontribute to the fact that they are still underutilised as a resource (Sanyal, Rubin, and Banerjee\n2022).\nWhat is referred to as biomedical texts, on the other hand, are often compilations of scientiﬁc\ntexts in the biomedical and life sciences from resources such as PubMed. They are written in a\nmore polished standard style,\nc and while they do overlap with clinical texts, they are larger in size\nand easier to process using standard NLP methods.\n2.2. Language models for clinical texts\nDue to the differences between biomedical and clinical texts (Section 2.1), transformer-based lan-\nguage models that have been only trained on generic and biomedical texts are not always sufﬁcient\nto capture all the complexities of clinical notes. For this reason, it is common to use pre-trained\nmodels as a starting point and either use ﬁne-tuning to adapt them to clinical notes (van Aken\net al.2021; Agnikula Kshatriya et al.2021) or use continual learning and further pre-train a model\nlike BERT or BioBERT (Lee et al.2020a) on clinical texts (Si et al.2019; Alsentzer et al.2019;Q i u\net al.2020).\n2.3. Adapting to new domains via continual learning\nContinual learning is a powerful alternative to the standard transfer learning approach which\ninvolves pre-training and ﬁne-tuning on a target task. In this paradigm, models can adapt to\nnew domains during the pre-training stage. This linearity resembles biological learning and also\nalleviates the need for excessive model re-training (Mehta et al.2021).\nThe idea here is to adapt the model to new streams of data while retaining knowledge of the\nprevious domains (Parisi et al.2019). Using this strategy, we may pre-train models that have pre-\nviously been trained on biomedical texts and expose them to clinical notes and the vocabulary\ncIt should be noted that some types of clinical notes resemble biomedical texts more than others. For instance, discharge\nsummaries tend to be longer and more coherent than short and hastily written progress notes.\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n890 O. Rohanian et al.\nassociated with them. While this method is not speciﬁcally related to model compression, we\ncan develop lightweight clinical models by using already compressed biomedical models, such as\nBioMobileBERT, and through continual learning adapt them to the clinical domain. We explore\nthis approach in this work in Section 3.2.\n2.4. Language model compression\nAs discussed in Section 1, issues like overparameterisation, computational overhead and the\nnegative environmental impact of large pre-trained language models have led researchers to\ndevelop strategies for compressing these models into smaller, faster, but almost equally performant\nversions (Sun et al.2019).\nKnowledge distillation (KD) (Hinton et al.2015) is a well studied and powerful technique that\nis designed to create such models in a ‘teacher–student’ setup, where the smaller student model\nlearns to mimic its teacher, either task-speciﬁcally by using the teacher’s outputs as soft labels or\ntask-agnostically by looking at the outputs of a pre-training objective such as Masked Language\nModelling (Devlin et al.2019). The latter option allows for greater ﬂexibility because the student\nmay then independently be ﬁne-tuned on the target task (Wang et al.2020).\nDistilBERT (Sanh et al.2019) is a notable example of such an effort in NLP , inspiring a slew of\nalternative ‘distilled’ versions of commonly used models. DistilGPT2, DistilRoBERTa, DistilBART\nand DistilT5 are a few examples. More recently, other powerful approaches have also appeared in\nthe literature, some of which will be covered in Section 3.\nThe efﬁcacy of KD-based compression in specialised domains like biomedical and clinical texts\nis still understudied. Rohanian et al.(2022) is an example of a work that focuses on development\nof compact biomedical transformers. To the best of our knowledge, there is no work speciﬁcally\ntargeting KD for models trained on clinical texts. As discussed in Section 2.1, these texts con-\ntain linguistic features and terminology that differ from generic biomedical texts, necessitating a\nseparate treatment.\n3. Methods\nIn this work, we utilise KD methods (Section 2.4) to train small-sized and efﬁcient language mod-\nels specialised for processing of clinical texts. First, we use KD approaches to directly extract\ncompact models from the BioClinicalBERT model; second, we employ continual learning to pre-\ntrain existing compact biomedical models (e.g. BioDistilBERT and BioMobileBERT) using the\nMIMIC-III notes dataset (Johnson et al.2016).\n3.1. Compressing clinical models via distillation\nIn order to distil compact models from BioClinicalBERT, three different KD methods are explored\nin this work: DistilClinicalBERT, TinyClinicalBERT and ClinicalMiniALBERT (Fig. 3). These\nmethods are described in detail below.\n3.1.1. DistilClinicalBERT\nThis approach follows the distillation process outlined in DistilBERT Sanh et al.(2019) with the\naim of aligning the output distributions of the student and teacher models based on the Masked\nLanguage Modelling (MLM) objective, as well as aligning their last hidden states. The loss used\nfor this approach is deﬁned as\nL(X, Y) = λ1Loutput(fs(X), ft(X)) (1)\n+ λ2Lalign(hs(X), ht(X))\n+ λ3Lmlm(fs(X), Y)\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 891\nwhere X is the input to the model, Y represents the MLM labels, fs(X)a n d ft(X) denote the outputs\nof the student and teacher models, respectively, hs(X)a n d ht(X) are the last hidden states of the\nstudent and teacher, Loutput is a KL-Divergence loss for aligning the output distributions of the\nstudent and teacher, Lalign is a cosine embedding loss for aligning the last hidden states of the\nstudent and teacher, Lmlm represents the original MLM loss and λ1 to λ3 are hyperparameters\ncontrolling the weighting of each component in the loss function.\nThe student used in this approach uses six hidden layers, a hidden dimension size of 768 and\nan expansion rate of four for the MLP blocks, resulting in 65M parameters in total. For initialising\nthe student model, we follow the method as described by Sanh et al.(2019) and Lee et al.(2020b).\nThis involves using the same embedding size as the teacher and borrowing pre-trained weights\nfrom a subset of the teacher’s layers.\n3.1.2. TinyClinicalBERT\nThis is a layer-to-layer distillation approach based on TinyBERT (Jiao et al. 2020), which is\nintended to align the hidden states and attention maps of each layer of the student with a spe-\nciﬁc layer of the teacher. Because the student network typically uses a smaller hidden dimension\nsize compared to its teacher, an embedding alignment loss is also included. The combined loss in\nthis approach is deﬁned below:\nL(X) = λ\n0Lembed(es(X), et(X)) (2)\n+\nN∑\nl=1\nλlLatt(al\ns(X), ag(l)\nt (X))\n+\nN∑\nl=1\nλlLhid(hl\ns(X), hg(l)\nt (X))\n+ λ(N+1)Lout(fs(X), ft(X))\nwhere N represents the number of layers in the student model. The embedding vectors for the\nstudent and teacher models before passing to the transformer encoder are represented by es(X)\nand et(X), respectively. For the ith layer of both the student and teacher models, the attention maps\nand hidden states are represented by ai\ns(X), ai\nt(X), hi\ns(X)a n d hi\nt(X), respectively. The mapping\nfunction g(.) is used to determine the corresponding teacher layer index for each student layer\nand is the same mapping function used in TinyBERT. The mean squared error (MSE) loss Lembed\nis used to align the embeddings of the student and teacher models, while the MSE losses Latt and\nLhid align their attention maps and hidden states, respectively. The cross-entropy loss Lout aligns\ntheir output distributions. Finally, hyperparameters λ0 to λ(N+1) control the signiﬁcance of each\nloss component.\nThe student model in this approach has four hidden layers, with a hidden dimension of 312\nand an MLP expansion rate of 4, totalling 15 million parameters. Due to the difference in hidden\ndimensions between the student and teacher models, the student model is initialised with random\nweights.\n3.1.3. ClinicalMiniALBERT\nThis is another layer-to-layer distillation approach with the difference that the student is not a\nfully-parameterised transformer, but a recursive one (e.g. ALBERT (Lan et al. 2019)). We fol-\nlow the same distillation procedure introduced in MiniALBERT (Nouriborji et al.2022), which\nis similar to Equation ( 2). The recursive student model in this method uses cross-layer parameter\nsharing and embedding factorisation in order to reduce the number of parameters and employs\nbottleneck adapters for layer-wise adaptation. Its architecture features a hidden dimension of\n768, an MLP expansion rate of 4 and an embedding size of 312, which add up to a total of 18\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n892 O. Rohanian et al.\nmillion parameters. Similar to TinyClinicalBERT (Section 3.1.2), the student model is initialised\nrandomly.\n3.2. Adapting compressed models to clinical tasks via continual learning\nWe investigate an alternative method for compressing clinical models through continual learning.\nIn this approach, already compressed biomedical models are further specialised by pre-training\non a collection of clinical texts using the MLM objective. Two different models, namely,\nClinicalDistilBERT and ClinicalMobileBERT, are developed in this fashion.\nTo obtain ClinicalDistilBERT, we use the pre-trained BioDistilBERT (Rohanian et al. 2022)\nmodel and train it further for three epochs on MIMIC-III. This model has the same architecture\nas DistilClinicalBERT (as described in Section 3.1.1). ClinicalMobileBERT, on the other hand, is\nbased on the pre-trained BioMobileBERT (Rohanian et al.2022) model and is also further trained\non MIMIC-III for three epochs. This model has a unique architecture that allows it to have a depth\nof up to 24 hidden layers while having only 25 million parameters.\n4. Datasets and tasks\nThis section discusses the NLP tasks and datasets used in this work. We brieﬂy explain the goals\nand the nature of each task, followed by information on the datasets used to evaluate the proposed\nmodels.\n4.1. Tasks\nWe explored four prominent tasks in clinical NLP: NER, Relation Extraction (RE), natural\nlanguage inference (NLI) and lassiﬁcation (CLS). Below, we provide deﬁnitions and concrete\nexamples from each task to illustrate their objectives and characteristics.\n4.1.1. Named entity recognition (NER)\nNER is the task of automatically processing a text and identifying named entities, such as per-\nsons, organisations, locations and medical terms. For example, in the sentence ‘The patient was\ndiagnosed with heart disease by Dr. Johnson at JR Hospital’, NER would identify ‘patient’, ‘heart\ndisease’, ‘Dr. Johnson’ and ‘JR Hospital’ as named entities and classify them as ‘Person’, ‘Disease’,\n‘Person’ and ‘Hospital’, respectively.\n4.1.2. Relation extraction (RE)\nRE is the task of recognising and extracting links between entities such as genes, proteins, dis-\neases, treatments, tests and medical conditions. For example, in the sentence ‘The EGFR gene has\nbeen associated with increased risk of lung cancer’, the RE system may recognise the association\nbetween the EGFR gene and lung cancer as ‘connected to increased risk’.\n4.1.3. Natural language inference (NLI)\nIn natural language inference (NLI), the goal is to determine the connection between two texts,\nsuch as a premise and a hypothesis. The connection may be deﬁned as entailment, contradiction\nor neutral. For example, if given the premise ‘The patient is diagnosed with inﬂuenza’ and the\nhypothesis ‘The patient is being treated for bacterial infection’, the NLI system would ﬁnd that the\nconnection is ‘contradiction’. This task helps improve understanding of the connections between\nbiomedical concepts in language.\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 893\nTable 1.Training samples from the MedNLI dataset\nSentence 1 Sentence 2 Label\nHemi-sternotomy and lead extraction on\n[∗∗3090-11-5∗∗]\nPatient has no cardiac history Contradiction\n............................... ................................. ................................ ............................... ..................................... .................................... ................................\nHe was sent for Stat Head CT, neurology was\ncalled\nThe patient has had a stroke Entailment\n............................... ................................. ................................ ............................... ..................................... .................................... ................................\nHis O2Sat remains above 90% on room air but\nappears to drop when patient falls asleep\nPatient has OND Neutral\n4.1.4. Sequence classiﬁcation (CLS)\nCLS is the task of assigning class labels to word sequences in a biomedical text, such as sentences\nor paragraphs. The aim is to correctly predict the sequence’s class label based on the contextual\ninformation provided in the text. For example, given the text ‘Patient has high fever and severe\nheadache’, the system may predict the class label ‘symptoms of an illness’.\nWhen the class labels contain assertion modiﬁers (negation, uncertainty, hypothetical, con-\nditional, etc.) and reﬂect degrees of certainty, the task is referred to as assertion detection (AD)\n(Chen 2019), which can be regarded as a subtask of CLS. For example, in the statement ‘Patient has\nheightened sensitivity to light and mild headache, which may indicate migraine’, the AD system\nwould predict the class label ‘uncertain’ or ‘possible’ based on the context.\n4.2. Datasets\nWe evaluate all of our models on four publicly available datasets, namely, MedNLI, i2b2-2010,\ni2b2-2012, i2b2-2014, and one internal dataset named ISARIC Clinical Notes (ICN).\n4.2.1. MedNLI\nMedNLI Romanov and Shivade ( 2018) is a NLI task designed for medical texts, in which two sen-\ntences are given to the model and the model should predict one of the entailments, contradiction\nor neutral labels as the relation of the two given sentences, as shown in Table 1.\n4.2.2. i2b2 datasets\ni2b2-2010 (Uzuner et al. 2011) is a medical relation extraction dataset, d in which the model is\nrequired to output the relation between two concepts in a given text. The relations are between\n‘medical problems and treatments’, ‘medical problems and tests’ and ‘medical problems and other\nmedical problems’. In total, this dataset uses nine labels which are as follows:\n1. Treatment improves medical problem (TrIP)\n2. Treatment worsens medical problem (TrWP)\n3. Treatment causes medical problem (TrCP)\n4. Treatment is administered for medical problem (TrAP)\n5. Treatment is not administered because of medical problem (TrNAP)\n6. Test reveals medical problem (TeRP)\n7. Test conducted to investigate medical problem (TeCP)\n8. Medical problem indicates medical problem (PIP)\n9. No relations\ndThis dataset is also used for NER, with the same entity labels as i2b2-2012\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n894 O. Rohanian et al.\nTable 2. i2b2-2010 samples taken from the dataset’s guideline Uzuner et al.(2011). The concept pairs for\nwhich a relationship should be predicted are displayed in boldface. Following the pre-processing used in the\nBLUE benchmark, the concepts are replaced with tags and then passed to the model as shown in the second\ncolumn\nRaw Pre-Processed Label\nWas discharged to home to be followed\nfor her coronary artery disease\nfollowing two failed bypass graft\nprocedure\nWas discharged to home to be followed\nmedically for @treatment$ following\n@problem$\nTrWP\n................................. ................................ ................................. ............................. ................................... .................................... ..................................\nShe has an elevated cholesterol\ncontrolled with Zocor\nShe has an @problem$ controlled with\n@treatment$\nTrIP\n................................. ................................ ................................. ............................. ................................... .................................... ..................................\nBactrim could be a cause of these\nabnormalities\n@treatment$ could be a cause of\n@problem$\nTrCP\n................................. ................................ ................................. ............................. ................................... .................................... ..................................\nA lung biopsywas performed, revealing\nchorio carcinoma\nA @test$ was performed, revealing\n@problem$\nTeRP\nFigure 1. Samples from the i2b2-2012 dataset.\nFor ﬁne-tuning our models on this dataset, we follow the same pre-processing used in the BLUE\nbenchmark, which models the relation extraction task as a sentence classiﬁcation by replacing the\nconcepts with certain tags, as shown in Table 2\ni2b2-2012 (Sun, Rumshisky, and Uzuner 2013) is a temporal relation extraction dataset that\ncontains 310 discharge summaries from Partners Healthcare and the Beth Israel Deaconess\nMedical Center. It contains inline annotations for each discharge summary in four categories:\nclinical concepts, clinical departments, evidentials and occurrences. In our experiments, it is used\nas an NER dataset with the following entity labels:\n1. Medical Problem (PR)\n2. Medical Treatment (TR)\n3. Medical Test (TE)\n4. Clinical Department (CD)\n5. Evidental (EV)\n6. Occurrence (OC)\n7. None (NO)\nSome samples from the training dataset are provided in Figure 1.\ni2b2-2014 (Stubbs, Kotﬁla, and Uzuner 2015) consists of two sub-tasks: De-identiﬁcation and\nheart disease risk factors identiﬁcation. In our experiments, we focus on the de-identiﬁcation task\nin which the goal is to remove protected health information (PHI) from the clinical notes. The\ndata in this task contain over 1300 patient records and has inline annotations for PHIs in each\nnote. Similar to i2b2-2012, this task is also framed as NER with 22 labels. Figure 2 shows some\nexamples taken from the training subset of the dataset.\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 895\nFigure 2. Samples from i2b2-2014. Names have been anonymised for privacy. Labels are ‘PA’ for Patient’, ‘PR’ for\n‘Professional’, ‘OR’ for ‘Organisation’ and ‘DO’ for ‘Doctor’. See AppendixA for the complete list of labels.\n4.2.3. ISARIC clinical notes (ICN)\nThe ISARIC COVID-19 Clinical Database e consists of data from patients hospitalised with\nCOVID-19 who are enrolled in an ISARIC Partner clinical study (Garcia-Gallo et al. 2022).\nThe data (which are standardised to CDISC STDM format f) include hospital admission and\ndischarge records, signs and symptoms, comorbidities, vital signs, treatments and outcomes. Non-\nprespeciﬁed terms related to one of ﬁve categories; medical history, complications, signs and\nsymptoms, new diagnosis at follow-up or category not recorded. The non-prespeciﬁed terms may\nconsist of one or multiple diagnoses or clinical events within the same category.\nIn December 2021, when the initial stratiﬁed sample of non-prespeciﬁed (free text) med-\nical terms from the ISARIC COVID-19 Clinical Database was extracted, the database com-\nprised of data from over 708,231 participants. The sample was formed of 125,381 non-\nprespeciﬁed terms and all ﬁve of the aforementioned categories were represented. We have\nreleased a relevant subset of this dataset along with this work and a copy of ICN, with neces-\nsary modiﬁcations for patient conﬁdentiality, is available at https://github.com/nlpie-research/\nLightweight-Clinical-Transformers/tree/main/ICN.\nFor the experiments in this work, each instance of the data consists of non-prespeciﬁed (free\ntext) terms describing clinical and adverse events along with a classiﬁcation label from a number\nof possible choices. An annotator with clinical training used the following three labels to annotate\nthe free-text notes:\n1. Malignancy\n2. No Malignancy\n3. Possible Malignancy (Ambiguous)\nThis annotation scheme is associated with the task of AD van Aken et al.(2021) as explained\nin Section 4.1.4. We refer to this annotated portion as ISARIC Clinical Notes (ICN) cancer\nclassiﬁcation dataset. Table 3 contains a few examples from ICN.\n5. Experimental setup\n5.1. Pre-training details and hyperparameters\nWe pre-train all of our models on the MIMIC-III dataset for a total of three epochs using either the\nMLM objective or Knowledge Distillation. We follow the same pre-processing used in Alsentzer\net al.(2019) for MIMIC and use the BERT tokeniser from the Huggingface with a max length of\neThe ISARIC COVID-19 Data Platform is a global partnership of more than 1700 institutions across more than 60 countries\n(Group 2021). Accreditation of the individuals and funders that contributed to this effort can be found in the supplementary\nmaterial. These partners have combined data and expertise to accelerate the pandemic response and improve patient out-\ncomes. For more information on ISARIC, see https://isaric.org. A subset of data is available for access via application to the\nData Access Committee at www.iddo.org/covid-19.\nfhttps://www.cdisc.org/standards/foundational/sdtm\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n896 O. Rohanian et al.\nTable 3.Some sample clinical notes along with their annotation from the ICN dataset\nClinical Note Label\nLung cancer and hypertension Malignancy\n............................... ......................... .......................... ...................... ....................... ............................ .......................... .........................\nDVT, now on heparin No malignancy\n............................... ......................... .......................... ...................... ....................... ............................ .......................... .........................\nArthritis, cataracts, under investigation for colon ca Possible malignancy (ambiguous)\n256 tokens. The details of the hyperparameters used for pre-training and ﬁne-tuning our models\nare available in Tables 7 and 8.\n5.2. Results\nWe evaluated the proposed models and the baselines on ﬁve datasets: MedNLI, i2b2-2010, i2b2-\n2012, i2b2-2014 and ICN. All reported results are median of three runs with different seeds. As\nshown in Table 4, compact clinical models signiﬁcantly outperform their general and biomed-\nical baselines and achieve competitive results against BioBERT-v1.1 and BioClinicalBERT. g\nClinicalDistilBERT and ClinicalMobileBERT, which are trained using continual learning, obtain\nthe best average results among all compact models (Table 4). ClinicalMiniALBERT outperforms\nboth DistilClincialBERT and TinyClinicalBERT in terms of average results among our distilled\nmodels.\n6. Discussion and analysis\n6.1. Effect of different initialisations\nFollowing the work of Alsentzer et al.(2019), we explore the effect of different initialisations for\nour continually learned models, as shown in Table 5. We ﬁnd that initialising ClinicalDistilBERT\nwith the trained weights from a pre-existing biomedical model signiﬁcantly improved the model’s\naverage performance, particularly on the MedNLI dataset. However, we discovered that initialis-\ning the ClinicalMobileBERT with a similar biomedical checkpoint did not result in a signiﬁcant\nperformance boost.\n6.2. Performance evaluation through latency and GMACs\nLatency and Giga Multiply-Accumulate Operations per second (GMACs) are two important met-\nrics used to evaluate the efﬁciency of machine learning models. Latency measures the amount of\ntime it takes for a model to process a single input and produce an output and is typically measured\nin milliseconds. GMACs, on the other hand, stands for Giga Multiply-Accumulate Operations\nand is a measure of how many computational operations a model needs to perform its tasks.\nGMACs are expressed in terms of billions of operations and provides a way of quantifying the\ncomputational resources required by a machine learning model.\nThe results of the latency and GMACs of the models, as well as the model sizes, are presented\nin Table 6. These measurements were conducted using a V100 32G GPU. The results show that\nthere is a trade-off between accuracy and efﬁciency of the models. The ClinicalBioBERT model,\nwhich is listed only as representative of a class of larger-sized models (110m parameters, as listed\nin Table 4), has the best performance on the test sets but has the highest latency and GMACs,\nmaking it less suitable for deployment in resource-constrained environments. On the other hand,\ngThis is in line with our expectation to achieve comparable results, rather than beat signiﬁcantly larger models.\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 897\nTable 4.The results of the baselines (above the double line) and our pre-trained models on clinical down-\nstream tasks. The metrics used for reporting scores are accuracy for the MedNLI, micro-averaged F1 for\ni2b2-2010 (RE), macro-averaged F1 for ICN, and Exact F1 for the others. Bold numbers denote the best\nperformance and underlined numbers denote the second-best performance\nModel #Params MedNLI i2b2-2010 i2b2-2012 i2b2-2014 ICN Avg\nNLI RE/NER NER NER CLS\nBERTbase 110M 78.27 92.75/77.18 80.19 96.77 90.88 87.55\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nBioBERT-v1.1 110M 84.10 93.70 /82.54 83.00 96.69 93.06 88.84\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nBioClinicalBERT 110M 82.41 93.58/84.27 82.98 96.72 93.30 88.87\nDistilBERT 65M 73.41 92.75 /76.43 79.15 96.23 93.75 85.28\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nMobileBERT 25M 76.16 93.16 /77.65 76.03 96.87 89.03 84.81\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nTinyBERT 15M 74.75 89.34/66.00 68.51 91.92 70.82 76.89\nDistilBioBERT 65M 71.80 92.98/79.96 80.78 95.55 93.44 85.75\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nBioDistilBERT 65M 77.77 93.19/79.58 81.11 96.13 93.10 86.81\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nBioMobileBERT 25M 78.97 93.56 /81.21 78.77 96.90 89.61 86.50\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nTinyBioBERT 15M 67.72 91.19/69.12 73.30 92.18 85.44 79.82\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nBioMiniALBERT 18M 75.52 92.89/78.32 80.04 95.28 92.31 85.72\nDistilClinicalBERT 65M 78.05 93.38/83.12 82.06 95.45 90.70 87.12\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nClinicalDistilBERT 65M 81.29 93.39/82.36 82.30 95.81 97.75 88.81\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nClinicalMobileBERT 25M 80.66 93.88/84.92 81.28 96.91 95.80 88.90\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nTinyClinicalBERT 15M 71.30 91.55/76.91 77.46 92.15 89.71 83.17\n................................ ................................ ................................ ............................... ..................................... ................................... .................................\nClinicalMiniALBERT 18M 78.90 93.43 /82.93 82.34 95.11 93.73 87.74\nTable 5.The effect of different initialisations on the continual learning of compact models\nModel isBio MedNLI i2b2-2010 i2b2-2012 i2b2-2014 ICN Avg\nClinicalDistilBERT /Box/checked81.29 93.39 /82.36 82.30 95.81 97.75 88.81\n................................. ............................... ................................ ............................... ............................... ................................ ............................... ...............................\n/XBox76.72 93.32/80.30 81.37 96.20 96.13 87.33\n................................. ............................... ................................ ............................... ............................... ................................ ............................... ...............................\nClinicalMobileBERT /Box/checked80.66 93.88 /84.92 81.28 96.91 95.80 88.90\n................................. ............................... ................................ ............................... ............................... ................................ ............................... ...............................\n/XBox80.23 93.71/84.55 80.39 97.25 96.17 88.71\nthe TinyClinicalBERT model has the lowest latency, GMACs and size, but its performance may\nnot be as good as that of ClinicalBioBERT.\nThe DistilClinicalBERT, ClinicalDistilBERT, ClinicalMobileBERT and ClinicalMiniALBERT\nmodels offer a good balance between performance and efﬁciency with relatively lower latency,\nGMACs and smaller sizes compared to the ClinicalBioBERT model. The BioClinicalBERT and\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n898 O. Rohanian et al.\nTable 6. Comparing the efﬁciency of the proposed models with ClinicalBioBERT. ↓\ndenotes that less is better for that particular metric\nModel Latency (ms) ↓ GMACs ↓ Size (MB) ↓\nClinicalBioBERT 94.61±6.11 0.970 410\n................................ ........................ ........................ ........................ ....................... ............................... ............................\nClinicalDistilBERT 58.27 ±3.98 0.588 248\n................................ ........................ ........................ ........................ ....................... ............................... ............................\nDistilClinicalBERT 58.27 ±3.98 0.588 248\n................................ ........................ ........................ ........................ ....................... ............................... ............................\nTinyClinicalBERT 16.92±2.74 0.123 52\n................................ ........................ ........................ ........................ ....................... ............................... ............................\nClinicalMobileBERT 85.18 ±6.06 0.184 94\n................................ ........................ ........................ ........................ ....................... ............................... ............................\nClinicalMiniALBERT 59.01 ±2.99 0.598 68\nBioBERT-v1.1 models, with 110 million parameters, offer the highest performance but are also\nthe most computationally intensive.\nIn a real-world setting, the choice of the appropriate model should depend on the spe-\nciﬁc requirements of the application, such as the required accuracy, computational resources\nand memory constraints. Based on the results presented in Table 4, the DistilClinicalBERT,\nClinicalDistilBERT, ClinicalMobileBERT and ClinicalMiniALBERT models with 65 million, 25\nmillion and 18 million parameters, respectively, provide a good balance between performance,\nlatency, GMACs and model size.\n6.3. ICN error analysis\n6.3.1. Preparation of the error analysis subset\nTo perform error analysis, we chose three of the best-performing models, namely, BioBERT,\nBioClinicalBERT and our proposed ClinicalDistilBERT. In order to evaluate the models on a truly\nunseen dataset for this analysis, we selected the internal ICN dataset (Section 4.2.3). It consists of\napproximately 125,000 clinical notes, with 6000 of them having been labelled by clinicians from\nthe ISARIC consortium. We ﬁne-tuned the pre-trained models on the labelled section of the ICN\nand then used the resulting models to predict labels for all 125,000 samples. The samples in which\nat least two of the models disagree on the predicted label were identiﬁed as corner cases, resulting\nin approximately 1500 clinical notes.\nTo perform the error analysis, these corner cases were annotated by a clinician and the outputs\nof the three ﬁne-tuned models were analysed and compared with the expert human annotations.\nMore information about the speciﬁcs of this adjudication is provided in the Appendix C.F i g u r e4\nprovides the confusion matrices for performance of the models both on the test set and on the cor-\nner cases. Based on the information from the confusion matrices, ClinicalDistilBERT performed\nbetter than the rest of the models on these cases. BioBERT, on the other hand, fared comparatively\npoorly, which can be attributed to its lack of clinical domain knowledge. In the following section,\nwe present the analysis and observations of the human expert annotator about the outputs of each\nmodel on the corner cases and investigate if there are any common mistakes or recurring patterns\nthat have caused confusion for any of the models.\n6.3.2. Patterns observed in error analysis\nInterpretation of ‘ca’ Abbreviation: In the portion of ICN on which ClinicalDistilBERT was\ntrained, the abbreviation ‘ca’ often refers to cancer. However, these two letters could also be\nused in other contexts; one such example was a free text term containing ‘low ca +’. It was\nassumed this free text was referring to low calcium levels, therefore a ‘No Malignancy’ label was\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 899\nassigned by the human annotator. This was in direct contrast to the ‘malignant neoplasm’ label\noutput of ClinicalDistilBERT. On this occasion, description of the level (i.e. low) preceding ‘ca’\nand the addition sign succeeding this abbreviation indicated to human annotator that this term\nrefers to an ion outside an expected range. It could, therefore, be reasonably assumed this term\nrefers to a calcium measurement rather than cancer. This example shows how ClinicalDistilBERT\nhas the potential to generate more reliable results if it is further developed to more accurately\nprocess contextual information in cases where free texts comprise abbreviations with multiple\ninterpretations.\nMislabelling of Benign Terms (Adenoma, Hemangioma, Angioma): A frequently occurring\ninconsistency between the labels assigned by a human annotator and ClinicalDistilBERT related to\ncertain free text terms containing the words ‘adenoma’, ‘hemangioma’ or ‘angioma’. Some exam-\nples of false positives in Clinical DistilBERT related to the term ‘adenoma’ are as follows: ‘bilateral\nadenomas’, ‘benign adrenal adenoma’, ‘benign ovarian cystadenoma’ and ‘ﬁbroadenoma of right\nbreast’. In all of these cases, the human annotator decided the note was ‘No Malignancy’ but the\nmodel labelled it as ‘Malignancy’. It is particularly interesting that ClinicalDistilBERT incorrectly\nlabelled these, given the fact the some samples start with the word ’benign’. The model may have\nbeen misled due to the similarities between the terms ‘adenocarcinoma’ (a type of malignancy)\nand ‘adenoma’ (a benign neoplasm).\nWe discovered a pattern of discrepancy between ClinicalDistilBERT labels and human anno-\ntator labels in samples containing words such as ‘hemangioma’ or ‘angioma’. For example, the\nmodel labelled ‘liver hemangioma’ and ‘right frontal cavernous angioma’ as ‘Possible Malignancy\n(Ambiguous)’ but the human annotator deemed these terms as ‘No Malignancy’. This may have\noccurred as the structure of these words is broadly similar to a number of conditions, such as\n‘astrocytoma’ and ‘meningioma’, for which it is difﬁcult to decipher the benign or malignant nature\nof the free text term; these terms were therefore previously seen by the model in its training sam-\nples associated with possible malignancy. More speciﬁcally, the aforementioned diagnoses with\nhuman and model annotation disparity end in ‘-oma’; free text terms ending in ‘-oma’ may often\nrefer to a malignancy, but this is not universally correct. The model may have identiﬁed these\nsamples incorrectly due to spelling similarities to malignancy terms encountered during training,\nbut it has not yet developed the ability to differentiate between them consistently.\nAcronym Misinterpretation (CLL, ALL, NHL): An analysis of label discrepancies among\nthe three models revealed a pattern of incorrect labelling of certain acronyms by BioBERT and\nBioClinicalBERT. The models frequently mislabelled three-letter acronyms such as ‘CLL’, ‘ALL’\nand ‘NHL’ as ‘No Malignancy’, while both ClinicalDistilBERT and human annotation identi-\nﬁed them as malignant conditions. These acronyms can commonly be understood to refer to\nchronic lymphocytic leukaemia, acute lymphoblastic leukaemia and non-Hodgkin lymphoma,\nrespectively. Despite the fact that these acronyms were included in the ‘Malignancy’ training sam-\nples, BioBERT and BioClinicalBERT often labelled free text terms containing these acronyms as\n‘No Malignancy’. On the other hand, these models sometimes labelled certain acronyms such\n‘gbs’ as ‘Malignancy’, while human annotation and ClinicalDistilBERT did not identify them as\nmalignant.\nh\nImpact of Text Length on Diagnosis (Term ‘Calculi’): The word ‘calculi’ frequently led to\ninaccuracies in the diagnosis label of all three models. Out of the 1544 corner cases studied dur-\ning the error analysis, 33 contained the term ‘calculi’ and were all annotated as ‘No Malignancy’\nduring human annotation. ClinicalDistilBERT, however, labelled 17 of these as malignant. These\nsamples usually contained short free text terms, with 10 having 3 or fewer words and none having\nmore than 5 words. Examples include ‘bladder calculi’, ‘left renal calculi’ and ‘staghorn calculi’.\nThe 16 samples which were correctly labelled as ‘No Malignancy’ by ClinicalDistilBERT were fre-\nquently of a greater length, with 8 samples containing 4–20 words. Examples of these samples are:\nhWe assume this term refers to Guillain-Barré Syndrome.\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n900 O. Rohanian et al.\n‘psoriasis/ depression/ renal calculi’, ‘gall bladder + common bile duct (cbd) calculi’ and ‘calculi\nin gallbladder and common bile duct’. BioBERT and BioClinicalBERT only correctly labelled one\nfree text term containing ‘calculi’ as ‘No Malignancy’.\n7. Conclusion and future works\nIn this work, we have presented ﬁve compact clinical transformers that have achieved signiﬁcant\nimprovement over their baselines on a variety of clinical text-mining tasks. To train our models,\nwe utilised both continual learning and knowledge distillation techniques. For continual learning,\nwe employed the BioDistilBERT and BioMobileBERT models, which are compact models derived\nusing standard techniques and trained on large biomedical data. Our experiments showed that\nthe average performance of these models can increase by up to 2% after training on the MIMIC-\nIII clinical notes dataset, and they even outperformed larger baselines on the i2b2-2010 and ICN\ndatasets (Table 4). In order to determine the best approach for knowledge distillation in clinical\nNLP , we explored a range of different methods including standard (Sanh et al. 2019), layer-to-\nlayer (Jiao et al.2020) and recursive distillation (Nouriborji et al.2022).\nMoreover, to conﬁrm the efﬁcacy of our methods on an unseen private dataset, we evaluated\nthe performance of the top models on the ICN dataset by looking at the corner cases in the test\nset where at least two of the models disagreed and then asked an expert annotator with clinical\ntraining to adjudicate the corner cases. In this way, we managed to further assess the models on\nmore complicated samples and provided a more in-depth analysis of where the models tend to fail\nand what recurring themes exist in the more challenging cases.\nWe also evaluated the models in terms of efﬁciency criteria such as latency and GMACs and\ncompared the proposed models with BioClinicalBERT. We subsequently provided guidance on\nselecting the optimal model based on performance and efﬁciency trade-offs. We hope that by\nmaking our lightweight models public, we will make clinical text-mining methods more accessible\nto hospitals and academics who may not have access to GPU clusters or specialised hardware,\nparticularly those in developing countries. The top-performing models produced in this study will\nbe integrated into the data curation pipeline under development by ISARIC and Global.health,\ni\nthereby facilitating the rapid aggregation and analysis of global data for outbreak response.\nThe work presented here has some limitations, however. Currently, our experiments are lim-\nited to datasets in English and it remains to be seen how the models would perform on datasets\nin other languages. We have also not tested our models on the task of connecting named entities\nto medical knowledge-bases such as SNOMED CT or ICD-10, which is an important task in clin-\nical NLP. In future work, we aim to extend our research to more tasks and languages in order to\naddress these limitations.\nAnother potential avenue for future work is the integration of information from other modal-\nities such as images or electronic health records containing tabular data, including clinical\nlaboratory test results and radiology images. This would allow us to train and evaluate multi-\nmodal architectures, such as CLIP (Radford et al.2021), and explore their utility for clinical NLP\ntasks.\nEthics statement. Ethics Committee approval for the collection and analysis of ISARIC Clinical Notes was given by the\nHealth Organisation Ethics Review Committee (RPC571 and RPC572 on 25 April 2013). National and/or institutional ethics\ncommittee approval was additionally obtained by participating sites according to local requirements.\nThis work is a part of a global effort to accelerate and improve the collection and analysis of data in the context of\ninfectious disease outbreaks. Rapid characterisation of novel infections is critical to an effective public health response. The\nmodel developed will be implemented in data aggregation and curation platforms for outbreak response – supporting the\nunderstanding of the variety of data collected by frontline responders. The challenges of implementing robust data collection\nihttps://www.global.health/\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 901\nefforts in a health emergency often result in non-standard data using a wide range of terms. This is especially the case in\nlower-resourced settings where data infrastructure is lacking. This work aims to improve data processing and will especially\ncontribute to lower-resource settings to improve health equity.\nCompeting interests. The authors declare that they have no competing interests.\nFinancial support. This work was made possible with the support of UK Foreign, Commonwealth and Development\nOfﬁce and Wellcome [225288/Z/22/Z]. Collection of data for the ISARIC Clinical Notes was made possible with the support\nof UK Foreign, Commonwealth and Development Ofﬁce and Wellcome [215091/Z/18/Z, 222410/Z/21/Z, 225288/Z/22/Z,\n220757/Z/20/Z and 222048/Z/20/Z] and the Bill & Melinda Gates Foundation [OPP1209135]; CIHR Coronavirus Rapid\nResearch Funding Opportunity OV2170359 and was coordinated out of Sunnybrook Research Institute; was supported by\nendorsement of the Irish Critical Care- Clinical Trials Group, co-ordinated in Ireland by the Irish Critical Care- Clinical\nTrials Network at University College Dublin and funded by the Health Research Board of Ireland [CTN-2014-12]; grants from\nRapid European COVID-19 Emergency Response research (RECOVER) [H2020 project 101003589] and European Clinical\nResearch Alliance on Infectious Diseases (ECRAID) [965313]; Cambridge NIHR Biomedical Research Centre; Wellcome\nTrust fellowship [205228/Z/16/Z] and the National Institute for Health Research Health Protection Research Unit (HPRU) in\nEmerging and Zoonotic Infections (NIHR200907) at the University of Liverpool in partnership with Public Health England\n(PHE), in collaboration with Liverpool School of Tropical Medicine and the University of Oxford; the dedication and hard\nwork of the Norwegian SARS-CoV-2 study team.\nResearch Council of Norway grant no 312780 and a philanthropic donation from Vivaldi Invest A/S owned by Jon\nStephenson von Tetzchner; PJMO was supported by the UK’s National Institute for Health Research (NIHR) via Imperial’s\nBiomedical Research Centre (NIHR Imperial BRC), Imperial’s Health Protection Research Unit in Respiratory Infections\n(NIHR HPRU RI), the Comprehensive Local Research Networks (CLRNs) and is an NIHR Senior Investigator (NIHR201385);\nInnovative Medicines Initiative Joint Undertaking under Grant Agreement No. 115523 COMBACTE, resources of which\nare composed of ﬁnancial contribution from the European Union’s Seventh Framework Programme (FP7/2007-2013) and\nEFPIA companies, in-kind contribution; Stiftungsfonds zur Förderung der Bekämpfung der Tuberkulose und anderer\nLungenkrankheiten of the City of Vienna; Project Number: APCOV22BGM; Australian Department of Health grant\n(3273191); Gender Equity Strategic Fund at University of Queensland, Artiﬁcial Intelligence for Pandemics (A14PAN) at\nUniversity of Queensland, The Australian Research Council Centre of Excellence for Engineered Quantum Systems (EQUS,\nCE170100009), The Prince Charles Hospital Foundation, Australia; grants from Instituto de Salud Carlos III, Ministerio de\nCiencia, Spain; Brazil, National Council for Scientiﬁc and Technological Development Scholarship number 303953/2018-7;\nthe Firland Foundation, Shoreline, Washington, USA; The French COVID cohort (NCT04262921) is sponsored by INSERM\nand is funding by the REACTing (REsearch & ACtion emergING infectious diseases) consortium and by a grant of the\nFrench Ministry of Health (PHRC n\n◦20-0424); the South Eastern Norway Health Authority and the Research Council of\nNorway; and a grant from the Oxford University COVID-19 Research Response fund (grant 0009109); Institute for Clinical\nResearch (ICR), National Institutes of Health (NIH) supported by the Ministry of Health Malaysia; a grant from foundation\nBevordering Onderzoek Franciscus.\nThe investigators acknowledge the philanthropic support of the donors to the University of Oxford’s COVID-19\nResearch Response Fund; COVID clinical management team, AIIMS, Rishikesh, India; COVID-19 Clinical Management\nteam, Manipal Hospital Whiteﬁeld, Bengaluru, India; Italian Ministry of Health “Fondi Ricerca corrente-L1P6” to IRCCS\nOspedale Sacro Cuore-Don Calabria and Preparedness work conducted by the Short Period Incidence Study of Severe Acute\nRespiratory Infection; The dedication and hard work of the Groote Schuur Hospital Covid ICU Team, supported by the\nGroote Schuur nursing and University of Cape Town registrar bodies coordinated by the Division of Critical Care at the\nUniversity of Cape Town.\nThis work uses data provided by patients and collected by the NHS as part of their care and support #DataSavesLives.\nThe data used for this research were obtained from ISARIC4C. We are extremely grateful to the 2648 frontline NHS clinical\nand research staff and volunteer medical students who collected these data in challenging circumstances; and the generos-\nity of the patients and their families for their individual contributions in these difﬁcult times. The COVID-19 Clinical\nInformation Network (CO-CIN) data were collated by ISARIC4C Investigators. Data and Material provision were supported\nby grants from: the National Institute for Health Research (NIHR; award CO-CIN-01), the Medical Research Council (MRC;\ngrant MC_PC_19059) and by the NIHR Health Protection Research Unit (HPRU) in Emerging and Zoonotic Infections\nat University of Liverpool in partnership with Public Health England (PHE), (award 200907), NIHR HPRU in Respiratory\nInfections at Imperial College London with PHE (award 200927), Liverpool Experimental Cancer Medicine Centre (grant\nC18616/A25153), NIHR Biomedical Research Centre at Imperial College London (award ISBRC-1215-20013) and NIHR\nClinical Research Network providing infrastructure support. We also acknowledge the support of Jeremy J Farrar and Nahoko\nShindo.\nThis work was supported in part by the National Institute for Health Research (NIHR) Oxford Biomedical Research\nCentre (BRC) and in part by an InnoHK Project at the Hong Kong Centre for Cerebro-cardiovascular Health Engineering\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n902 O. Rohanian et al.\n(COCHE). OR acknowledges the support of the Medical Research Council (grant number MR/W01761X/). DAC was sup-\nported by an NIHR Research Professorship, an RAEng Research Chair, COCHE and the Pandemic Sciences Institute at the\nUniversity of Oxford. The views expressed are those of the authors and not necessarily those of the NHS, NIHR, MRC,\nCOCHE or the University of Oxford.\nReferences\nAgnikula Kshatriya B. S. , Sagheb E. , Wi C.-I., Yoon J. , Seol H. Y. , Juhn Y. and Sohn S. (2021). Identiﬁcation of asthma\ncontrol factor in clinical notes using a hybrid deep learning model. BMC Medical Informatics and Decision Making21(7),\n1–10.\nAlsentzer E. , Murphy J. , Boag W. , Weng W.-H. , Jindi D. , Naumann T. and McDermott M. (2019). Publicly available\nclinical bert embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop, pp. 72–78.\nBambroo P. and Awasthi A. (2021). Legaldb: long distilbert for legal document classiﬁcation. In 2021 International\nConference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT). IEEE, pp. 1–4.\nBeltagy I. , Lo K. and Cohan A. (2019). Scibert: a pretrained language model for scientiﬁc text. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pp. 3615–3620.\nBender D. and Sartipi K. (2013). Hl7 fhir: an agile and restful approach to healthcare information exchange. In Proceedings\nof the 26th IEEE International Symposium on Computer-Based Medical Systems. IEEE, pp. 326–331.\nBender E. M. , Gebru T., McMillan-Major A. and Shmitchell S. (2021). On the dangers of stochastic parrots: can language\nm o d e l sb et o ob i g ?I nProceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 610–623.\nBoulton D. and Hammersley M. (2006). Analysis of unstructured data. Data Collection and Analysis2, 243–259.\nChalkidis I. , Fergadiotis M., Malakasiotis P., Aletras N. and Androutsopoulos I. (2020). Legal-bert: the muppets straight\nout of law school. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 2898–2904.\nChen L. (2019). Attention-based deep learning system for negation and assertion detection in clinical notes. International\nJournal of Artiﬁcial Intelligence and Applications10(1).\nDandala B. , Joopudi V. and Devarakonda M. (2019). Adverse drug events detection in clinical notes by jointly modeling\nentities and relations using neural networks. Drug Safety42(1), 135–146.\nDehghani M., Tay Y., Arnab A., Beyer L. and Vaswani A. (2021). The Efﬁciency Misnomer. In International Conference on\nLearning Representations.\nDevlin J. , Chang M.-W. , Lee K. and Toutanova K. (2019). BERT: pre-training of deep bidirectional transformers for\nlanguage understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, MN:\nAssociation for Computational Linguistics, pp. 4171–4186.\nGarcia-Gallo E. , Merson L. , Kennon K. , Kelly S. , Citarella B. W. , Fryer D. V. , Shrapnel S. , Lee J. , Duque S. , Fuentes Y.\nV., Balan V., Smith S., Wei J., Gonçalves B. P. , Russell C. D. , Sigfrid L., Dagens A., Olliaro P. L. , Baruch J., Kartsonaki\nC., Dunning J., Rojek A., Rashan A., Beane A., Murthy S. and Reyes L. F. (2022). Isaric-covid-19 dataset: a prospective,\nstandardized, global dataset of patients hospitalized with covid-19. Scientiﬁc Data9(1), 1–22.\nISARIC Clinical Characterisation Group (2021). The value of open-source clinical science in pandemic response: lessons\nfrom isaric. The Lancet. Infectious Diseases21(12), 1623–1624.\nHartman T. , Howell M. D. , Dean J. , Hoory S. , Slyper R. , Laish I. , Gilon O. , Vainstein D., Corrado G. , Chou K. , Po M.\nJ., Williams J., Ellis S., Bee G., Hassidim A., Amira R., Beryozkin G., Szpektor I. and Matias Y. (2020). Customization\nscenarios for de-identiﬁcation of clinical notes. BMC Medical Informatics and Decision Making20(1), 1–9.\nHinton G. , Vinyals O. and Dean J. (2015). Distilling the knowledge in a neural network. In NIPS 2014 Deep Learning\nWorkshop.\nHuang K., Altosaar J. and Ranganath R. (2019). Clinicalbert: modeling clinical notes and predicting hospital readmission.\narXiv preprint arXiv:1904.05342.\nJiao X., Yin Y., Shang L., Jiang X., Chen X., Li L., Wang F. and Liu Q. (2020). Tinybert: distilling bert for natural language\nunderstanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4163–4174.\nJohnson A. E. , Pollard T. J. , Shen L., Lehman L.-W. H. , Feng M., Ghassemi M., Moody B., Szolovits P., Anthony Celi L.\nand Mark R. G. (2016). Mimic-III, a freely accessible critical care database. Scientiﬁc Data3(1), 1–9.\nKong H.-J. (2019). Managing unstructured big data in healthcare system. Healthcare Informatics Research25(1), 1–2.\nLan Z. , Chen M., Goodman S., Gimpel K., Sharma P. and Soricut R. (2019). Albert: a lite bert for self-supervised learning\nof language representations. arXiv preprint arXiv:1909.11942.\nLee J. , Yoon W. , Kim S. , Kim D. , Kim S. , So C. H. and Kang J. (2020a). Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinformatics 36(4), 1234–1240.\nLee J. , Yoon W. , Kim S. , Kim D. , Kim S. , So C. H. and Kang J. (2020b). Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinformatics 36(4), 1234–1240.\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 903\nLehman E., Jain S., Pichotta K., Goldberg Y. and Wallace B. C. (2021). Does bert pretrained on clinical notes reveal sensitive\ndata? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 946–959.\nLi Z., Wallace E., Shen S., Lin K., Keutzer K., Klein D. and Gonzalez J. (2020). Train big, then compress: rethinking model\nsize for efﬁcient training and inference of transformers. In International Conference on Machine Learning. PMLR, pp.\n5958–5968.\nMahbub M. , Srinivasan S. , Danciu I. , Peluso A. , Begoli E. , Tamang S. and Peterson G. D. (2022). Unstructured clinical\nnotes within the 24 hours since admission predict short, mid & long-term mortality in adult icu patients. Plos One17(1),\ne0262182.\nMahendran D. and McInnes B. T. (2021). Extracting adverse drug events from clinical notes. AMIA Summits on\nTranslational Science Proceedings420, 2021.\nMehta S. V. , Patil D., Chandar S. and Strubell E. (2021). An empirical investigation of the role of pre-training in lifelong\nlearning. arXiv preprint arXiv:2112.09153.\nMelamud O. and Shivade C. (2019). Towards automatic generation of shareable synthetic clinical notes using neural\nlanguage models. In Proceedings of the 2nd Clinical Natural Language Processing Workshop, pp. 35–45.\nNguyen D. Q. , Vu T. and Nguyen A.-T. (2020). Bertweet: a pre-trained language model for english tweets. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 9–14.\nNouriborji M. , Rohanian O. , Kouchaki S. and Clifton D. A. (2022). Minialbert: model distillation via parameter-efﬁcient\nrecursive transformers. arXiv preprint arXiv:2210.06425.\nObeid J. S. , Dahne J., Christensen S., Howard S., Crawford T., Frey L. J. , Stecker T. and Bunnell B. E. (2020). Identifying\nand predicting intentional self-harm in electronic health record clinical notes: deep learning approach. JMIR Medical\nInformatics 8(7), e17784.\nOzyurt I. B. (2020). On the effectiveness of small, discriminatively pre-trained language representation models for biomedical\ntext mining. In Proceedings of the First Workshop on Scholarly Document Processing, pp. 104–112.\nParisi G. I. , Kemker R. , Part J. L. , Kanan C. and Wermter S. (2019). Continual lifelong learning with neural networks: a\nreview. Neural Networks113, 54–71.\nQiu X., Sun T., Xu Y., Shao Y., Dai N. and Huang X. (2020). Pre-trained models for natural language processing: a survey.\nScience China Technological Sciences63(10), 1872–1897.\nRadford A. , Kim J. W. , Hallacy C. , Ramesh A. , Goh G. , Agarwal S. , Sastry G. , Askell A. , Mishkin P. , Clark J. , Krueger\nG. and Sutskever I. (2021). Learning transferable visual models from natural language supervision. In International\nConference on Machine Learning. PMLR, pp. 8748–8763.\nRethans J.-J., Martin E. and Metsemakers J. (1994). To what extent do clinical notes by general practitioners reﬂect actual\nmedical performance? A study using simulated patients. British Journal of General Practice44(381), 153–156.\nRidgway J. P. , Uvin A., Schmitt J., Oliwa T., Almirol E., Devlin S. and Schneider J. (2021). Natural language processing of\nclinical notes to identify mental illness and substance use among people living with HIV: retrospective cohort study. JMIR\nMedical Informatics9(3), e23456.\nRohanian O. , Nouriborji M. , Kouchaki S. and Clifton D. A. (2022). On the effectiveness of compact biomedical\ntransformers. arXiv preprint arXiv:2209.03182.\nRomanov A. and Shivade C. (2018). Lessons from natural language inference in the clinical domain. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, pp. 1586–1596.\nRosenbloom S. T. , Denny J. C. , Xu H. , Lorenzi N. , Stead W. W. and Johnson K. B. (2011). Data from clinical notes: a\nperspective on the tension between structure and ﬂexible documentation. Journal of the American Medical Informatics\nAssociation 18(2), 181–186.\nSanh V., Debut L., Chaumond J. and Wolf T. (2019). Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.\narXiv preprint arXiv:1910.01108.\nSanyal J., Rubin D. and Banerjee I. (2022). A weakly supervised model for the automated detection of adverse events using\nclinical notes. Journal of Biomedical Informatics126, 103969.\nSchick T. and Schütze H. (2021). It’s not just size that matters: small language models are also few-shot learners. In\nProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pp. 2339–2352.\nSchwartz R., Dodge J., Smith N. A. and Etzioni O. (2020). Green AI. Communications of the ACM63(12), 54–63.\nSi Y. and Roberts K. (2019). Deep patient representation of clinical notes via multi-task learning for mortality prediction.\nAMIA Summits on Translational Science Proceedings2019, 779.\nSi Y., Wang J., Xu H. and Roberts K. (2019). Enhancing clinical concept extraction with contextual embeddings. Journal of\nthe American Medical Informatics Association26(11), 1297–1304.\nSong J., Hobensack M., Bowles K. H., McDonald M. V., Cato K., Rossetti S. C., Chae S., Kennedy E., Barrón Y., Sridharan\nS. and Topaz M. (2022). Clinical notes: an untapped opportunity for improving risk prediction for hospitalization and\nemergency department visit during home health care. Journal of Biomedical Informatics128, 104039.\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n904 O. Rohanian et al.\nStrubell E., Ganesh A. and McCallum A. (2019). Energy and policy considerations for deep learning in NLP. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3645–3650.\nStubbs A. , Kotﬁla C. and Uzuner Ö. (2015). Automated systems for the de-identiﬁcation of longitudinal clinical narra-\ntives: overview of 2014 i2b2/uthealth shared task track 1. Journal of Biomedical Informatics58, S11–S19. Supplement:\nProceedings of the 2014 i2b2/UTHealth Shared-Tasks and Workshop on Challenges in Natural Language Processing for\nClinical Data.\nSun S. , Cheng Y. , Gan Z. and Liu J. (2019). Patient knowledge distillation for bert model compression. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pp. 4323–4332.\nSun W., Rumshisky A. and Uzuner O. (2013). Evaluating temporal relations in clinical text: 2012 i2b2 Challenge. Journal of\nthe American Medical Informatics Association20(5), 806–813.\nSun Z., Yu H., Song X., Liu R., Yang Y. and Zhou D. (2020). Mobilebert: a compact task-agnostic bert for resource-limited\ndevices. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2158–2170.\nUzuner Ö., South B. R. , Shen S. and DuVall S. L. (2011). 2010 i2b2/V A challenge on concepts, assertions, and relations in\nclinical text. Journal of the American Medical Informatics Association18(5), 552–556.\nvan Aken B. , Trajanovska I. , Siu A. , Mayrdorfer M., Budde K. and Löser A. (2021). Assertion detection in clinical notes:\nmedical language models to the rescue? In Proceedings of the Second Workshop on Natural Language Processing for Medical\nConversations, pp. 35–40.\nWang W., Wei F., Dong L., Bao H., Yang N. and Zhou M. (2020). Minilm: deep self-attention distillation for task-agnostic\ncompression of pre-trained transformers. Advances in Neural Information Processing Systems33, 5776–5788.\nYe J. , Yao L., Shen J. , Janarthanam R. and Luo Y. (2020). Predicting mortality in critically ill patients with diabetes using\nmachine learning and clinical notes. BMC Medical Informatics and Decision Making20(11), 1–7.\nZhang J., Trujillo L. D. B. , Tanwar A.\n, Ive J., Gupta V. and Guo Y. (2022). Clinical utility of automatic phenotype annotation\nin unstructured clinical notes: intensive care unit use. BMJ Health & Care Informatics29(1), e100519.\nAppendix A: I2B2-2014 labels\nThe following entity labels were used for framing the de-identiﬁcation task as named entity\nrecognition:\nCity Date Email\nState Fax Medicalrecord\n................................ ........................ ........................ ........................ ....................... ..........................\nPhone Bioid Profession\n................................ ........................ ........................ ........................ ....................... ..........................\nIdnum Zip Healthplan\n................................ ........................ ........................ ........................ ....................... ..........................\nDevice Street Hospital\n................................ ........................ ........................ ........................ ....................... ..........................\nLocation Patient Organisation\n................................ ........................ ........................ ........................ ....................... ..........................\nUrl Country Doctor\n................................ ........................ ........................ ........................ ....................... ..........................\nAge Username\nAppendix B: Details of the hyperparameters used in the experiment\nIn this section, we provide the details of the hyperparameters used in our experiments. Table 7 lists\nthe hyperparameters used for pre-training models on the MIMIC-III dataset, and Table 8 shows\nthe hyperparameters chosen for the purpose of ﬁne-tuning on downstream tasks.\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 905\nTable 7. Hyperparameters used for pre-training\nmodels on MIMIC-III\nParam Value\nLearning rate {5e-4, 5e-5}\n.................................................................................................................\nScheduler Linear\n.................................................................................................................\nOptimiser AdamW\n.................................................................................................................\nWeight decay 1 e − 4\n.................................................................................................................\nTotal batch size 192\n.................................................................................................................\nWarmup steps 5000\n.................................................................................................................\nEpochs 3\nTable 8. Hyperparameters used for ﬁne-tuning on\ndownstream tasks\nParam Value\nLearning rate {5e-5, 2e-5, 1e-5}\n.................................................................................................................\nScheduler Linear\n.................................................................................................................\nOptimiser AdamW\n.................................................................................................................\nWeight decay 0.01\n.................................................................................................................\nBatch size {16, 32}\n.................................................................................................................\nEpochs 3\nAppendix C: Adjudication of classiﬁcation outputs\nThe clinician who has assessed and adjudicated the classiﬁcation differences, has opted to make\nthe following decisions which are mentioned below for reference:\n If the terms used in the text were vague or unclear (e.g. cayn and loa), the note is judged as\n‘no malignancy’.\n Indeterminate lymph nodes are assumed to be possible malignancy (ambiguous).\n Those notes that were written in a foreign language were excluded from the adjudication.\n ‘Glioma’ is typically indicative of possible malignancy (ambiguous), as it might be benign\nor malignant. However, if it has been described with a qualiﬁer that signals benign (e.g.\nlow-grade glioma or benign glioma), it has been deemed as ‘no malignancy’.\n ‘Angiorhyolipoma’, assumed to be angiomyolipoma was considered ‘no malignancy’.\n Text that refers to a possible sign/symptom of cancer, such as elevated ca125 or a mass\ncausing bowel obstruction, has been labelled as ‘no malignancy’ if no indication of cancer\nor suspicion/investigation has been mentioned.\n As chemotherapy and radiotherapy are frequently used for cancer treatment, if either of\nthese appear in free text without another cancer term listed, they are labelled ‘possible\nmalignancy (ambiguous)’.\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n906 O. Rohanian et al.\n(A) (B)\n(C) (D)\nFigure 3. (A) Training loss of the ClinicalDistilBERT and the ClinicalMobileBERT when optimised for the MLM objective.\n(B) Training loss of the DistilClinicalBERT on the distillation objective as described in Section 3.1.1. (C) Training loss of\nTinyClinicalBERT on the distillation objective, as explained in Section 3.1.2. (D) Training loss of ClinicalMiniALBERT on the\ndistillation objective, as introduced in Nouriborji et al.(2022).\n Treatments that could be used for cancer but also have other applications, such as bone\nmarrow, BMT, stem cell transplant, nephrectomy, prostatectomy and mastectomy, are\nlabelled ‘no malignancy’ if no further reference to cancer is made in the text.\n ADT (used for prostate cancer) is considered to represent possible malignancy (ambigu-\nous).\n ‘Lump’ or ‘mass’ with no indication of cancer (e.g. breast lump) are labelled ‘no malig-\nnancy’.\n Where resected year is redacted and ‘no recurrence’ is mentioned, the text is deemed ‘no\nmalignancy’.\n Grade one meningioma is ‘no malignancy’. Benign menigioma would also be ‘no malig-\nnancy’, but if only ‘meningioma’ is mentioned, this would be ‘possible malignancy\n(ambiguous)’.\n Lynch syndrome is ‘no malignancy’. It increases the risk of malignancy but is not a cancer\nin itself.\n ‘Anti-cancer therapy‘ is when mentioned without further context could refer to treatment\nor prophylactic so ‘possible malignancy (ambiguous)’ was used.\n Kappa lambda light chain was considered as ‘no malignancy’.\n Tumour lysis syndrome was classiﬁed as a ‘malignancy’ as the condition refers to release of\nthe contents of cancer cells.\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 907\n(A) (C) (D)\n(E) (F) (G)\nFigure 4. (A) and (E) represent the confusion matrices for BioBERT on the test set and the corner cases (Section 6.3.1), respec-\ntively. (B) and (F) refer to the confusion matrices for ClinicalBioBERT on the test set and corner cases. (C) and (G) denote the\nconfusion matrices for ClinicalDistilBERT on the test set and corner cases. ‘No M’ indicates ‘No Malignancy’, ‘P M’ represents\n‘Possible Malignancy’ and ‘M’ signiﬁes ‘Malignancy’.\nAppendix D: ISARIC Clinical Characterisation Group\nAli Abbas, Sheryl Ann Abdukahil, Nurul Najmee Abdulkadir, Ryuzo Abe, Laurent Abel, Amal\nAbrous, Lara Absil, Kamal Abu Jabal, Nashat Abu Salah, Subhash Acharya, Andrew Acker, Shingo\nAdachi, Elisabeth Adam, Francisca Adewhajah, Enrico Adriano, Diana Adrião, Saleh Al Ageel,\nShakeel Ahmed, Marina Aiello, Kate Ainscough, Eka Airlangga, Tharwat Aisa, Ali Ait Hssain,\nYounes Ait Tamlihat, Takako Akimoto, Ernita Akmal, Eman Al Qasim, Razi Alalqam, Aliya\nMohammed Alameen, Angela Alberti, Tala Al-dabbous, Senthilkumar Alegesan, Cynthia Alegre,\nMarta Alessi, Beatrice Alex, Kévin Alexandre, Abdulrahman Al-Fares, Huda Alfoudri, Adam\nAli, Imran Ali, Naseem Ali Shah, Kazali Enagnon Alidjnou, Jeffrey Aliudin, Qabas Alkhafajee,\nClotilde Allavena, Nathalie Allou, Aneela Altaf, João Alves, João Melo Alves, Rita Alves, Joana\nAlves Cabrita, Maria Amaral, Nur Amira, Heidi Ammerlaan, Phoebe Ampaw, Roberto Andini,\nClaire Andréjak, Andrea Angheben, François Angoulvant, Sophia Ankrah, Séverine Ansart,\nSivanesen Anthonidass, Massimo Antonelli, Carlos Alexandre Antunes de Brito, Kazi Rubayet\nAnwar, Ardiyan Apriyana, Yaseen Arabi, Irene Aragao, Francisco Arancibia, Carolline Araujo,\nAntonio Arcadipane, Patrick Archambault, Lukas Arenz, Jean-Benoît Arlet, Christel Arnold-\nDay, Ana Aroca, Lovkesh Arora, Rakesh Arora, Elise Artaud-Macari, Diptesh Aryal, Motohiro\nAsaki, Angel Asensio, Elizabeth A. Ashley, Muhammad Ashraf, Namra Asif, Mohammad Asim,\nJean Baptiste Assie, Amirul Asyraf, Minahel Atif, Anika Atique, AM Udara Lakshan Attanyake,\nJohann Auchabie, Hugues Aumaitre, Adrien Auvet, Eyvind W. Axelsen, Laurène Azemar, Cecile\nAzoulay, Hakeem Babatunde, Benjamin Bach, Delphine Bachelet, Claudine Badr, Roar Bævre-\nJensen, Nadia Baig, J. Kenneth Baillie, J Kevin Baird, Erica Bak, Agamemnon Bakakos, Nazreen\nAbu Bakar, Andriy Bal, Mohanaprasanth Balakrishnan, Valeria Balan, Irene Bandoh, Firouzé\nBani-Sadr, Renata Barbalho, Nicholas Yuri Barbosa, Wendy S. Barclay, Saef Umar Barnett,\nMichaela Barnikel, Helena Barrasa, Audrey Barrelet, Cleide Barrigoto, Marie Bartoli, Cheryl\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n908 O. Rohanian et al.\nBartone, Joaquín Baruch, Mustehan Bashir, Romain Basmaci, Muhammad Fadhli Hassin Basri,\nDenise Battaglini, Jules Bauer, Diego Fernando Bautista Rincon, Denisse Bazan Dow, Abigail\nBeane, Alexandra Bedossa, Ker Hong Bee, Netta Beer, Husna Begum, Sylvie Behilill, Karine\nBeiruti, Albertus Beishuizen, Aleksandr Beljantsev, David Bellemare, Anna Beltrame, Beatriz\nAmorim Beltrão, Marine Beluze, Nicolas Benech, Lionel Eric Benjiman, Dehbia Benkerrou,\nSuzanne Bennett, Binny Benny, Luís Bento, Jan-Erik Berdal, Delphine Bergeaud, Hazel Bergin,\nJosé Luis Bernal Sobrino, Giulia Bertoli, Lorenzo Bertolino, Simon Bessis, Adam Betz, Sybille\nBevilcaqua, Karine Bezulier, Amar Bhatt, Krishna Bhavsar, Isabella Bianchi, Claudia Bianco, Farah\nNadiah Bidin, Moirangthem Bikram Singh, Felwa Bin Humaid, Mohd Nazlin Bin Kamarudin,\nFrançois Bissuel, Patrick Biston, Laurent Bitker, Mustapha Bittaye, Jonathan Bitton, Pablo\nBlanco-Schweizer, Catherine Blier, Frank Bloos, Mathieu Blot, Lucille Blumberg, Filomena Boccia,\nLaetitia Bodenes, Debby Bogaert, Anne-Hélène Boivin, Isabela Bolaños, Pierre-Adrien Bolze,\nFrançois Bompart, Patrizia Bonelli, Aurelius Bonfasius, Joe Bonney, Diogo Borges, Raphaël\nBorie, Hans Martin Bosse, Elisabeth Botelho-Nevers, Lila Bouadma, Olivier Bouchaud, Sabelline\nBouchez, Dounia Bouhmani, Damien Bouhour, Kévin Bouiller, Laurence Bouillet, Camile\nBouisse, Thipsavanh Bounphiengsy, Latsaniphone Bountthasavong, Anne-Sophie Boureau, John\nBourke, Maude Bouscambert, Aurore Bousquet, Jason Bouziotis, Bianca Boxma, Marielle Boyer-\nBesseyre, Maria Boylan, Fernando Augusto Bozza, Axelle Braconnier, Cynthia Braga, Timo\nBrandenburger, Filipa Brás Monteiro, Luca Brazzi, Dorothy Breen, Patrick Breen, Kathy Brickell,\nHelen Brotherton, Alex Browne, Shaunagh Browne, Nicolas Brozzi, Sonja Hjellegjerde Brunvoll,\nMarjolein Brusse-Keizer, Petra Bryda, Nina Buchtele, Polina Bugaeva, Marielle Buisson, Danilo\nBuonsenso, Erlina Burhan, Aidan Burrell, Ingrid G. Bustos, Denis Butnaru, André Cabie,\nSusana Cabral, Eder Caceres, Cyril Cadoz, Rui Caetano Garcês, Mia Callahan, Kate Calligy,\nJose Andres Calvache, Caterina Caminiti, João Camões, Valentine Campana, Paul Campbell,\nJosie Campisi, Cecilia Canepa, Mireia Cantero, Janice Caoili, Pauline Caraux-Paz, Sheila Cárcel,\nChiara Simona Cardellino, Filipa Cardoso, Filipe Cardoso, Nelson Cardoso, Soﬁa Cardoso,\nSimone Carelli, Francesca Carlacci, Nicolas Carlier, Thierry Carmoi, Gayle Carney, Inês Carqueja,\nMarie-Christine Carret, François Martin Carrier, Ida Carroll, Gail Carson, Leonor Carvalho,\nMaire-Laure Casanova, Mariana Cascão, Siobhan Casey, José Casimiro, Bailey Cassandra,\nSilvia Castañeda, Nidyanara Castanheira, Guylaine Castor-Alexandre, Ivo Castro, Ana Catarino,\nFrançois-Xavier Catherine, Paolo Cattaneo, Roberta Cavalin, Giulio Giovanni Cavalli, Alexandros\nCavayas, Adrian Ceccato, Masaneh Ceesay, Shelby Cerkovnik, Minerva Cervantes-Gonzalez,\nMuge Cevik, Anissa Chair, Catherine Chakveatze, Bounthavy Chaleunphon, Adrienne Chan,\nMeera Chand, Christelle Chantalat Auger, Jean-Marc Chapplain, Charlotte Charpentier, Julie\nChas, Allegra Chatterjee, Mobin Chaudry, Jonathan Samuel Chávez Iñiguez, Anjellica Chen,\nYih-Sharng Chen, Léo Chenard, Matthew Pellan Cheng, Antoine Cheret, Alfredo Antonio\nChetta, Thibault Chiarabini, Julian Chica, Suresh Kumar Chidambaram, Leong Chin Tho,\nCatherine Chirouze, Davide Chiumello, Hwa Jin Cho, Sung-Min Cho, Bernard Cholley, Danoy\nChommanam, Marie-Charlotte Chopin, Ting Soo Chow, Yock Ping Chow, Nathaniel Christy,\nHiu Jian Chua, Jonathan Chua, Jose Pedro Cidade, José Miguel Cisneros Herreros, Barbara\nWanjiru Citarella, Anna Ciullo, Emma Clarke, Jennifer Clarke, Rolando Claure-Del Granado,\nSara Clohisey, Perren J. Cobb, Cassidy Codan, Caitriona Cody, Alexandra Coelho, Megan\nColes, Gwenhaël Colin, Michael Collins, Sebastiano Maria Colombo, Pamela Combs, Jennifer\nConnolly, Marie Connor, Anne Conrad, Sofía Contreras, Elaine Conway, Graham S. Cooke,\nMary Copland, Hugues Cordel, Amanda Corley, Sabine Cornelis, Alexander Daniel Cornet,\nArianne Joy Corpuz, Andrea Cortegiani, Grégory Corvaisier, Emma Costigan, Camille Coufﬁgnal,\nSandrine Coufﬁn-Cadiergues, Roxane Courtois, Stéphanie Cousse, Rachel Cregan, Charles Crepy\nD’Orleans, Cosimo Cristella, Sabine Croonen, Gloria Crowl, Jonathan Crump, Claudina Cruz,\nJuan Luis Cruz Bermúdez, Jaime Cruz Rojo, Marc Csete, Alberto Cucino, Ailbhe Cullen, Matthew\nCummings, Ger Curley, Elodie Curlier, Colleen Curran, Paula Custodio, Ana da Silva Filipe,\nCharlene Da Silveira, Al-Awwab Dabaliz, Andrew Dagens, John Arne Dahl, Darren Dahly,\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 909\nPeter Daley, Heidi Dalton, Jo Dalton, Seamus Daly, Juliana Damas, Umberto D’Alessandro,\nFederico D’Amico, Nick Daneman, Corinne Daniel, Emmanuelle A Dankwa, Jorge Dantas,\nFrédérick D’Aragon, Mark de Boer, Menno de Jong, Gillian de Loughry, Diego de Mendoza,\nEtienne De Montmollin, Rafael Freitas de Oliveira França, Ana Isabel de Pinho Oliveira,\nRosanna De Rosa, Cristina De Rose, Thushan de Silva, Peter de Vries, Jillian Deacon, David\nDean, Alexa Debard, Bianca DeBenedictis, Marie-Pierre Debray, Nathalie DeCastro, William\nDechert, Lauren Deconninck, Romain Decours, Eve Defous, Isabelle Delacroix, Eric Delaveuve,\nKaren Delavigne, Nathalie M. Delfos, Ionna Deligiannis, Andrea Dell’Amore, Christelle Delmas,\nPierre Delobel, Corine Delsing, Elisa Demonchy, Emmanuelle Denis, Dominique Deplanque,\nPieter Depuydt, Mehul Desai, Diane Descamps, Mathilde Desvallées, Santi Dewayanti, Pathik\nDhanger, Alpha Diallo, Sylvain Diamantis, André Dias, Andrea Dias, Fernanda Dias Da Silva,\nJuan Jose Diaz, Priscila Diaz, Rodrigo Diaz, Bakary Dibba, Kévin Didier, Jean-Luc Diehl, Wim\nDieperink, Jérôme Dimet, Vincent Dinot, Fara Diop, Alphonsine Diouf, Yael Dishon, Félix\nDjossou, Annemarie B. Docherty, Helen Doherty, Arjen M Dondorp, Andy Dong, Christl A.\nDonnelly, Maria Donnelly, Chloe Donohue, Sean Donohue, Yoann Donohue, Peter Doran, Céline\nDorival, Eric D’Ortenzio, Phouvieng Douangdala, James Joshua Douglas, Renee Douma, Nathalie\nDournon, Triona Downer, Joanne Downey, Mark Downing, Tom Drake, Aoife Driscoll, Amiel\nA. Dror, Murray Dryden, Claudio Duarte Fonseca, Vincent Dubee, François Dubos, Audrey\nDubot-Pérès, Alexandre Ducancelle, Toni Duculan, Susanne Dudman, Abhijit Duggal, Paul\nDunand, Jake Dunning, Mathilde Duplaix, Emanuele Durante-Mangoni, Lucian Durham III,\nBertrand Dussol, Juliette Duthoit, Xavier Duval, Anne Margarita Dyrhol-Riise, Sim Choon Ean,\nMarco EcheverriaVillalobos, Giorgio Economopoulos, Michael Edelstein, Siobhan Egan, Linn\nMargrete Eggesbø, Carla Eira, Mohammed El Sanharawi, Subbarao Elapavaluru, Brigitte Elharrar,\nJacobien Ellerbroek, Merete Ellingjord-Dale, Philippine Eloy, Tarek Elshazly, Iqbal Elyazar,\nIsabelle Enderle, Tomoyuki Endo, Chan Chee Eng, Ilka Engelmann, Vincent Enouf, Olivier\nEpaulard, Martina Escher, Mariano Esperatti, Hélène Esperou, Catarina Espírito Santo, Marina\nEsposito-Farese, Lorinda Essuman, João Estevão, Manuel Etienne, Nadia Ettalhaoui, Anna Greti\nEverding, Mirjam Evers, Isabelle Fabre, Marc Fabre, Ismaila Fadera, Amna Faheem, Arabella Fahy,\nCameron J. Fairﬁeld, Zul Fakar, Komal Fareed, Pedro Faria, Ahmed Farooq, Hanan Fateena, Arie\nZainul Fatoni, Karine Faure, Raphaël Favory, Mohamed Fayed, Niamh Feely, Laura Feeney, Jorge\nFernandes, Marília Andreia Fernandes, Susana Fernandes, François-Xavier Ferrand, Eglantine\nFerrand Devouge, Joana Ferrão, Carlo Ferrari, Mário Ferraz, Benigno Ferreira, Bernardo Ferreira,\nIsabel Ferreira, Sílvia Ferreira, Ricard Ferrer-Roca, Nicolas Ferriere, Céline Ficko, Claudia\nFigueiredo-Mello, William Finlayson, Juan Fiorda, Thomas Flament, Clara Flateau, Tom Fletcher,\nAline-Marie Florence, Letizia Lucia Florio, Brigid Flynn, Deirdre Flynn, Federica Fogliazza, Claire\nFoley, Jean Foley, Victor Fomin, Tatiana Fonseca, Patricia Fontela, Karen Forrest, Simon Forsyth,\nDenise Foster, Giuseppe Foti, Erwan Fourn, Robert A. Fowler, Marianne Fraher, Diego Franch-\nLlasat, Christophe Fraser, John F Fraser, Marcela Vieira Freire, Ana Freitas Ribeiro, Craig French,\nCaren Friedrich, Ricardo Fritz, Stéphanie Fry, Nora Fuentes, Masahiro Fukuda, Argin G, Valérie\nGaborieau, Rostane Gaci, Massimo Gagliardi, Jean-Charles Gagnard, Nathalie Gagné, Amandine\nGagneux-Brunon, Abdou Gai, Sérgio Gaião, Linda Gail Skeie, Phil Gallagher, Elena Gallego\nCurto, Carrol Gamble, Yasmin Gani, Arthur Garan, Rebekha Garcia, Noelia García Barrio, Julia\nGarcia-Diaz, Esteban Garcia-Gallo, Navya Garimella, Federica Garofalo, Denis Garot, Valérie\nGarrait, Basanta Gauli, Nathalie Gault, Aisling Gavin, Anatoliy Gavrylov, Alexandre Gaymard,\nJohannes Gebauer, Eva Geraud, Louis Gerbaud Morlaes, Nuno Germano, praveen kumar ghisu-\nlal, Jade Ghosn, Marco Giani, Carlo Giaquinto, Jess Gibson, Tristan Gigante, Morgane Gilg, Elaine\nGilroy, Guillermo Giordano, Michelle Girvan, Valérie Gissot, Jesse Gitaka, Gezy Giwangkancana,\nDaniel Glikman, Petr Glybochko, Eric Gnall, Geraldine Goco, François Goehringer, Siri Goepel,\nJean-Christophe Goffard, Jin Yi Goh, Jonathan Golob, Rui Gomes, Kyle Gomez, Joan Gómez-\nJunyent, Marie Gominet, Bronner P. Gonçalves, Alicia Gonzalez, Patricia Gordon, Yanay Gorelik,\nIsabelle Gorenne, Conor Gormley, Laure Goubert, Cécile Goujard, Tiphaine Goulenok, Margarite\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n910 O. Rohanian et al.\nGrable, Jeronimo Graf, Edward Wilson Grandin, Pascal Granier, Giacomo Grasselli, Lorenzo\nGrazioli, Christopher A. Green, Courtney Greene, William Greenhalf, Segolène Greffe, Domenico\nLuca Grieco, Matthew Griffee, Fiona Grifﬁths, Ioana Grigoras, Albert Groenendijk, Anja Grosse\nLordemann, Heidi Gruner, Yusing Gu, Fabio Guarracino, Jérémie Guedj, Martin Guego, Dewi\nGuellec, Anne-Marie Guerguerian, Daniela Guerreiro, Romain Guery, Anne Guillaumot, Laurent\nGuilleminault, Maisa Guimarães de Castro, Thomas Guimard, Marieke Haalboom, Daniel Haber,\nHannah Habraken, Ali Hachemi, Amy Hackmann, Nadir Hadri, Fakhir Haidri, Sheeba Hakak,\nAdam Hall, Matthew Hall, Sophie Halpin, Jawad Hameed, Ansley Hamer, Rebecca Hamidfar,\nBato Hammarström, Terese Hammond, Lim Yuen Han, Rashan Haniffa, Kok Wei Hao, Hayley\nHardwick, Ewen M. Harrison, Janet Harrison, Samuel Bernard Ekow Harrison, Alan Hartman,\nMohd Shahnaz Hasan, Junaid Hashmi, Muhammad Hayat, Ailbhe Hayes, Leanne Hays, Jan\nHeerman, Lars Heggelund, Ross Hendry, Martina Hennessy, Aquiles Henriquez-Trujillo, Maxime\nHentzien, Diana Hernandez, Jaime Hernandez-Montfort, Daniel Herr, Andrew Hershey, Liv\nHesstvedt, Astarini Hidayah, Dawn Higgins, Eibhlin Higgins, Rupert Higgins, Rita Hinchion,\nSamuel Hinton, Hiroaki Hiraiwa, Hikombo Hitoto, Antonia Ho, Yi Bin Ho, Alexandre Hoctin,\nIsabelle Hoffmann, Wei Han Hoh, Oscar Hoiting, Rebecca Holt, Jan Cato Holter, Peter Horby,\nJuan Pablo Horcajada, Koji Hoshino, Kota Hoshino, Ikram Houas, Catherine L. Hough, Stuart\nHoultham, Jimmy Ming-Yang Hsu, Jean-Sébastien Hulot, Stella Huo, Abby Hurd, Iqbal Hussain,\nSamreen Ijaz, Arfan Ikram, Hajnal-Gabriela Illes, Patrick Imbert, Mohammad Imran, Rana Imran\nSikander, Aftab Imtiaz, Hugo Inácio, Carmen Infante Dominguez, Yun Sii Ing, Elias Iosiﬁdis,\nMariachiara Ippolito, Vera Irawany, Sarah Isgett, Tiago Isidoro, Nadiah Ismail, Margaux Isnard,\nMette Stausland Istre, Junji Itai, Asami Ito, Daniel Ivulich, Danielle Jaafar, Salma Jaafoura, Julien\nJabot, Clare Jackson, Abubacarr Jagne, Nina Jamieson, Victoria Janes, Pierre Jaquet, Waasila Jassat,\nColine Jaud-Fischer, Stéphane Jaureguiberry, Jeffrey Javidfar, Denise Jaworsky, Florence Jego,\nAnilawati Mat Jelani, Synne Jenum, Ruth Jimbo-Sotomayor, Ong Yiaw Joe, Ruth N. Jorge García,\nSilje Bakken Jørgensen, Cédric Joseph, Mark Joseph, Swosti Joshi, Mercé Jourdain, Philippe\nJouvet, Jennifer June, Anna Jung, Hanna Jung, Dafsah Juzar, Ouiﬁya Kaﬁf, Florentia Kaguelidou,\nNeerusha Kaisbain, Thavamany Kaleesvran, Sabina Kali, Alina Kalicinska, Karl Trygve Kalleberg,\nSmaragdi Kalomoiri, Muhammad Aisar Ayadi Kamaluddin, Zul Amali Che Kamaruddin, Nadiah\nKamarudin, Kavita Kamineni, Darshana Hewa Kandamby, Chris Kandel, Kong Yeow Kang,\nDarakhshan Kanwal, Dyah Kanyawati, Pratap Karpayah, Todd Karsies, Christiana Kartsonaki,\nDaisuke Kasugai, Anant Kataria, Kevin Katz, Aasmine Kaur, Tatsuya Kawasaki, Christy Kay,\nLamees Kayyali, Hannah Keane, Seán Keating, Andrea Kelly, Aoife Kelly, Claire Kelly, Niamh\nKelly, Sadie Kelly, Yvelynne Kelly, Maeve Kelsey, Ryan Kennedy, Kalynn Kennon, Sommay\nKeomany, Maeve Kernan, Younes Kerroumi, Sharma Keshav, Evelyne Kestelyn, Imrana Khalid,\nOsama Khalid, Antoine Khalil, Coralie Khan, Irfan Khan, Quratul Ain Khan, Sushil Khanal, Abid\nKhatak, Amin Khawaja, Michelle E Kho, Denisa Khoo, Ryan Khoo, Saye Khoo, Nasir Khoso, Khor\nHow Kiat, Yuri Kida, Harrison Kihuga, Peter Kiiza, Beathe Kiland Granerud, Anders Benjamin\nKildal, Jae Burm Kim, Antoine Kimmoun, Detlef Kindgen-Milles, Alexander King, Nobuya\nKitamura, Eyrun Floerecke Kjetland Kjetland, Paul Klenerman, Rob Klont, Gry Kloumann\nBekken, Stephen R Knight, Robin Kobbe, Paa Kobina Forson, Chamira Kodippily, Malte Kohns\nVasconcelos, Sabin Koirala, Mamoru Komatsu, Franklina Korkor Abebrese, Volkan Korten,\nCaroline Kosgei, Arsène Kpangon, Karolina Krawczyk, Sudhir Krishnan, Vinothini Krishnan,\nOksana Kruglova, Deepali Kumar, Ganesh Kumar, Mukesh Kumar, Pavan Kumar Vecham,\nDinesh Kuriakose, Ethan Kurtzman, Neurinda Permata Kusumastuti, Demetrios Kutsogiannis,\nGalyna Kutsyna, Ama Kwakyewaa Bedu-Addo, Konstantinos Kyriakoulis, Raph L. Hamers, Marie\nLachatre, Marie Lacoste, John G. Laffey, Nadhem Lafhej, Marie Lagrange, Fabrice Laine, Olivier\nLairez, Sanjay Lakhey, Antonio Lalueza, Marc Lambert, François Lamontagne, Marie Langelot-\nRichard, Vincent Langlois, Eka Yudha Lantang, Marina Lanza, Cédric Laouénan, Samira Laribi,\nDelphine Lariviere, Stéphane Lasry, Naveed Latif, Odile Launay, Didier Laureillard, Yoan Lavie-\nBadie, Andrew Law, Cassie Lawrence, Teresa Lawrence, Minh Le, Clément Le Bihan, Cyril Le\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 911\nBris, Georges Le Falher, Lucie Le Fevre, Quentin Le Hingrat, Marion Le Maréchal, Soizic Le\nMestre, Gwenaël Le Moal, Vincent Le Moing, Hervé Le Nagard, Paul Le Turnier, Ema Leal,\nMarta Leal Santos, Biing Horng Lee, Heng Gee Lee, James Lee, Jennifer Lee, Su Hwan Lee,\nTodd C. Lee, Yi Lin Lee, Gary Leeming, Bénédicte Lefebvre, Laurent Lefebvre, Benjamin Lefèvre,\nSylvie LeGac, Jean-Daniel Lelievre, François Lellouche, Adrien Lemaignen, Véronique Lemee,\nAnthony Lemeur, Gretchen Lemmink, Ha Sha Lene, Jenny Lennon, Rafael León, Marc Leone,\nMichela Leone, François-Xavier Lescure, Olivier Lesens, Mathieu Lesouhaitier, Amy Lester-\nGrant, Andrew Letizia, Sophie Letrou, Bruno Levy, Yves Levy, Claire Levy-Marchal, Katarzyna\nLewandowska, Erwan L’Her, Gianluigi Li Bassi, Janet Liang, Ali Liaquat, Geoffrey Liegeon, Kah\nChuan Lim, Wei Shen Lim, Chantre Lima, Bruno Lina, Lim Lina, Andreas Lind, Maja Katherine\nLingad, Guillaume Lingas, Sylvie Lion-Daolio, Samantha Lissauer, Keibun Liu, Marine Livrozet,\nPatricia Lizotte, Antonio Loforte, Navy Lolong, Leong Chee Loon, Diogo Lopes, Dalia Lopez-\nColon, Jose W. Lopez-Revilla, Anthony L. Loschner, Paul Loubet, Bouchra Loufti, Guillame\nLouis, Silvia Lourenco, Lara Lovelace-Macon, Lee Lee Low, Marije Lowik, Jia Shyi Loy, Jean\nChristophe Lucet, Carlos Lumbreras Bermejo, Carlos M. Luna, Olguta Lungu, Liem Luong,\nNestor Luque, Dominique Luton, Nilar Lwin, Ruth Lyons, Olavi Maasikas, Oryane Mabiala, Sarah\nMacDonald, Moïse Machado, Sara Machado, Gabriel Macheda, Juan Macias Sanchez, Jai Madhok,\nHashmi Madiha, Guillermo Maestro de la Calle, Jacob Magara, Giuseppe Maglietta, Rafael\nMahieu, Sophie Mahy, Ana Raquel Maia, Lars S. Maier, Mylène Maillet, Thomas Maitre, Maria\nMajori, Maximilian Malfertheiner, Nadia Malik, Paddy Mallon, Fernando Maltez, Denis Malvy,\nPatrizia Mammi, Victoria Manda, Jose M. Mandei, Laurent Mandelbrot, Frank Manetta, Julie\nMankikian, Edmund Manning, Aldric Manuel, Ceila Maria Sant‘Ana Malaque, Daniel Marino,\nFlávio Marino, Samuel Markowicz, Charbel Maroun Eid, Ana Marques, Catherine Marquis, Brian\nMarsh, Laura Marsh, Megan Marshal, John Marshall, Celina Turchi Martelli, Dori-Ann Martin,\nEmily Martin, Guillaume Martin-Blondel, Alessandra Martinelli, Ignacio Martin-Loeches, Martin\nMartinot, Alejandro Martín-Quiros, Ana Martins, João Martins, Nuno Martins, Caroline Martins\nRego, Gennaro Martucci, Olga Martynenko, Eva Miranda Marwali, Marsilla Marzukie, Juan\nFernado Masa Jimenez, David Maslove, Phillip Mason, Sabina Mason, Sobia Masood, Basri Mat\nNor, Moshe Matan, Henrique Mateus Fernandes, Meghena Mathew, Daniel Mathieu, Mathieu\nMattei, Romans Matulevics, Laurence Maulin, Michael Maxwell, Javier Maynar, Mayfong Mayxay,\nThierry Mazzoni, Lisa Mc Sweeney, Colin McArthur, Naina McCann, Aine McCarthy, Anne\nMcCarthy, Colin McCloskey, Rachael McConnochie, Sherry McDermott, Sarah E. McDonald,\nAine McElroy, Samuel McElwee, Victoria McEneany, Natalie McEvoy, Allison McGeer, Chris\nMcKay, Johnny McKeown, Kenneth A. McLean, Paul McNally, Bairbre McNicholas, Elaine\nMcPartlan, Edel Meaney, Cécile Mear-Passard, Maggie Mechlin, Maqsood Meher, Omar Mehkri,\nFerruccio Mele, Luis Melo, Kashif Memon, Joao Joao Mendes, Ogechukwu Menkiti, Kusum\nMenon, France Mentré, Alexander J. Mentzer, Emmanuelle Mercier, Noémie Mercier, Antoine\nMerckx, Mayka Mergeay-Fabre, Blake Mergler, Laura Merson, Tiziana Meschi, António Mesquita,\nRoberta Meta, Osama Metwally, Agnès Meybeck, Dan Meyer, Alison M. Meynert, Vanina\nMeysonnier, Amina Meziane, Mehdi Mezidi, Giuliano Michelagnoli, Céline Michelanglei, Isabelle\nMichelet, Efstathia Mihelis, Vladislav Mihnovit, Hugo Miranda-Maldonado, Nor Arisah Misnan,\nNik Nur Eliza Mohamed, Tahira Jamal Mohamed, Asma Moin, Elena Molinos, Brenda Molloy,\nSinead Monahan, Mary Mone, Agostinho Monteiro, Claudia Montes, Giorgia Montrucchio,\nSarah Moore, Shona C. Moore, Lina Morales Cely, Lucia Moro, Diego Rolando Morocho\nTutillo, Ben Morton, Catherine Motherway, Ana Motos, Hugo Mouquet, Clara Mouton Perrot,\nJulien Moyet, Caroline Mudara, Aisha Kalsoom Mufti, Ng Yong Muh, Dzawani Muhamad,\nJimmy Mullaert, Fredrik Müller, Karl Erik Müller, Daniel Munblit, Syed Muneeb, Nadeem\nMunir, Laveena Munshi, Aisling Murphy, Lorna Murphy, Patrick Murray, Marlène Murris,\nSrinivas Murthy, Himed Musaab, Alamin Mustafa, Carlotta Mutti, Himasha Muvindi, Gugapriyaa\nMuyandy, Dimitra Melia Myrodia, Farah Nadia Mohd-Hanaﬁah, Behzad Nadjm, Dave Nagpal,\nAlex Nagrebetsky, Mangala Narasimhan, Nageswaran Narayanan, Rashid Nasim Khan, Alasdair\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n912 O. Rohanian et al.\nNazerali-Maitland, Nadège Neant, Holger Neb, Coca Necsoi, Nikita Nekliudov, Matthew Nelder,\nErni Nelwan, Raul Neto, Emily Neumann, Bernardo Neves, Pauline Yeung Ng, Anthony Nghi,\nJane Ngure, Duc Nguyen, Orna Ni Choileain, Niamh Ni Leathlobhair, Alistair Nichol, Prompak\nNitayavardhana, Stephanie Nonas, Nurul Amani Mohd Noordin, Marion Noret, Nurul Faten\nIzzati Norharizam, Lisa Norman, Anita North, Alessandra Notari, Mahdad Noursadeghi, Karolina\nNowicka, Adam Nowinski, Saad Nseir, Jose I Nunez, Nurnaningsih Nurnaningsih, Dwi Utomo\nNusantara, Elsa Nyamankolly, Anders Benteson Nygaard, Fionnuala O Brien, Annmarie O\nCallaghan, Annmarie O’Callaghan, Giovanna Occhipinti, Derbrenn OConnor, Max O’Donnell,\nTawnya Ogston, Takayuki Ogura, Tak-Hyuk Oh, Sophie O’Halloran, Katie O’Hearn, Shinichiro\nOhshimo, Agnieszka Oldakowska, João Oliveira, Larissa Oliveira, Piero L. Olliaro, Conar O’Neil,\nDavid S.Y. Ong, Jee Yan Ong, Wilna Oosthuyzen, Anne Opavsky, Peter Openshaw, Saijad\nOrakzai, Claudia Milena Orozco-Chamorro, Andrés Orquera, Jamel Ortoleva, Javier Osatnik,\nLinda O’Shea, Miriam O’Sullivan, Siti Zubaidah Othman, Paul Otiku, Nadia Ouamara, Rachida\nOuissa, Clark Owyang, Eric Oziol, Maïder Pagadoy, Justine Pages, Amanda Palacios, Massimo\nPalmarini, Giovanna Panarello, Prasan Kumar Panda, Hem Paneru, Lai Hui Pang, Mauro\nPanigada, Nathalie Pansu, Aurélie Papadopoulos, Paolo Parducci, Edwin Fernando Paredes Oña,\nRachael Parke, Melissa Parker, Vieri Parrini, Taha Pasha, Jérémie Pasquier, Bruno Pastene, Fabian\nPatauner, Mohan Dass Pathmanathan, Luís Patrão, Patricia Patricio, Juliette Patrier, Laura Patrizi,\nLisa Patterson, Rajyabardhan Pattnaik, Christelle Paul, Mical Paul, Jorge Paulos, William A.\nPaxton, Jean-François Payen, Kalaiarasu Peariasamy, Miguel Pedrera Jiménez, Giles J. Peek,\nFlorent Peelman, Nathan Peiffer-Smadja, Vincent Peigne, Mare Pejkovska, Paolo Pelosi, Ithan\nD. Peltan, Rui Pereira, Daniel Perez, Luis Periel, Thomas Perpoint, Antonio Pesenti, Vincent\nPestre, Lenka Petrou, Michele Petrovic, Ventzislava Petrov-Sanchez, Frank Olav Pettersen, Gilles\nPeytavin, Scott Pharand, Ooyanong Phonemixay, Soulichanya Phoutthavong, Michael Piagnerelli,\nWalter Picard, Olivier Picone, Maria de Piero, Carola Pierobon, Djura Piersma, Carlos Pimentel,\nRaquel Pinto, Valentine Piquard, Catarina Pires, Isabelle Pironneau, Lionel Piroth, Roberta Pisi,\nAyodhia Pitaloka, Riinu Pius, Simone Piva, Laurent Plantier, Hon Shen Png, Julien Poissy,\nRyadh Pokeerbux, Maria Pokorska-Spiewak, Sergio Poli, Georgios Pollakis, Diane Ponscarme,\nJolanta Popielska, Diego Bastos Porto, Andra-Maris Post, Douwe F. Postma, Pedro Povoa, Diana\nPóvoas, Jeff Powis, Soﬁa Prapa, Viladeth Praphasiri, Sébastien Preau, Christian Prebensen, Jean-\nCharles Preiser, Anton Prinssen, Mark G. Pritchard, Gamage Dona Dilanthi Priyadarshani, Lucia\nProença, Sravya Pudota, Oriane Puéchal, Bambang Pujo Semedi, Mathew Pulicken, Matteo\nPuntoni, Gregory Purcell, Luisa Quesada, Vilmaris Quinones-Cardona, Víctor Quirós González,\nElse Quist-Paulsen, Mohammed Quraishi, Fadi-Fadi Qutishat, Maia Rabaa, Christian Rabaud,\nEbenezer Rabindrarajan, Aldo Rafael, Marie Raﬁq, Gabrielle Ragazzo, Mutia Rahardjani, Ahmad\nKashﬁ Haji Ab Rahman, Rozanah Abd Rahman, Arsalan Rahutullah, Fernando Rainieri, Giri\nShan Rajahram, Pratheema Ramachandran, Nagarajan Ramakrishnan, José Ramalho, Kollengode\nRamanathan, Ahmad Aﬁq Ramli, Blandine Rammaert, Grazielle Viana Ramos, Anais Rampello,\nAsim Rana, Rajavardhan Rangappa, Ritika Ranjan, Elena Ranza, Christophe Rapp, Aasiyah\nRashan, Thalha Rashan, Ghulam Rasheed, Menaldi Rasmin, Indrek Rätsep, Cornelius Rau,\nFrancesco Rausa, Tharmini Ravi, Ali Raza, Andre Real, Stanislas Rebaudet, Sarah Redl, Brenda\nReeve, Attaur Rehman, Liadain Reid, Liadain Reid, Dag Henrik Reikvam, Renato Reis, Jordi\nRello, Jonathan Remppis, Martine Remy, Hongru Ren, Hanna Renk, Anne-Sophie Resseguier,\nMatthieu Revest, Oleksa Rewa, Luis Felipe Reyes, Tiago Reyes, Maria Ines Ribeiro, Antonia\nRicchiuto, David Richardson, Denise Richardson, Laurent Richier, Siti Nurul Atikah Ahmad\nRidzuan, Jordi Riera, Ana L Rios, Asgar Rishu, Patrick Rispal, Karine Risso, Maria Angelica\nRivera Nuñez, Nicholas Rizer, Doug Robb, Chiara Robba, André Roberto, Charles Roberts,\nStephanie Roberts, David L. Robertson, Olivier Robineau, Anna Roca, Ferran Roche-Campo,\nPaola Rodari, Simão Rodeia, Julia Rodriguez Abreu, Bernhard Roessler, Claire Roger, Pierre-Marie\nRoger, Emmanuel Roilides, Amanda Rojek, Juliette Romaru, Roberto Roncon-Albuquerque Jr,\nMélanie Roriz, Manuel Rosa-Calatrava, Michael Rose, Dorothea Rosenberger, Andrea Rossanese,\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\nNatural Language Engineering 913\nMatteo Rossetti, Sandra Rossi, Bénédicte Rossignol, Patrick Rossignol, Stella Rousset, Carine Roy,\nBenoît Roze, Desy Rusmawatiningtyas, Clark D. Russell, Maeve Ryan, Maria Ryan, Stefﬁ Ryckaert,\nAleksander Rygh Holten, Isabela Saba, Luca Sacchelli, Sairah Sadaf, Musharaf Sadat, Valla Sahraei,\nNadia Saidani, Maximilien Saint-Gilles, Pranya Sakiyalak, Nawal Salahuddin, Leonardo Salazar,\nJodat Saleem, Nazal Saleh, Gabriele Sales, Stéphane Sallaberry, Charlotte Salmon Gandonniere,\nHélène Salvator, Olivier Sanchez, Xavier Sánchez Choez, Kizy Sanchez de Oliveira, Angel Sanchez-\nMiralles, Vanessa Sancho-Shimizu, Gyan Sandhu, Zulﬁqar Sandhu, Pierre-François Sandrine,\nOana Sandulescu, Marlene Santos, Shirley Sarfo-Mensah, Bruno Sarmento Banheiro, Iam Claire\nE. Sarmiento, Benjamine Sarton, Sree Satyapriya, Rumaisah Satyawati, Egle Saviciute, Parthena\nSavvidou, Yen Tsen Saw, Justin Schaffer, Tjard Schermer, Arnaud Scherpereel, Marion Schneider,\nStephan Schroll, Michael Schwameis, Gary Schwartz, Brendan Scicluna, Janet T. Scott, James\nScott-Brown, Nicholas Sedillot, Tamara Seitz, Jaganathan Selvanayagam, Mageswari Selvarajoo,\nCaroline Semaille, Malcolm G. Semple, Rasidah Bt Senian, Eric Senneville, Claudia Sepulveda,\nFilipa Sequeira, Tânia Sequeira, Ary Serpa Neto, Pablo Serrano Balazote, Ellen Shadowitz, Syamin\nAsyraf Shahidan, Mohammad Shamsah, Anuraj Shankar, Shaikh Sharjeel, Pratima Sharma,\nCatherine A. Shaw, Victoria Shaw, John Robert Sheenan, Ashraf Sheharyar, Dr. Rajesh Mohan\nShetty, Haixia Shi, Nisreen Shiban, Mohiuddin Shiekh, Takuya Shiga, Nobuaki Shime, Hiroaki\nShimizu, Keiki Shimizu, Naoki Shimizu, Sally Shrapnel, Pramesh Sundar Shrestha, Shubha Kalyan\nShrestha, Hoi Ping Shum, Nassima Si Mohammed, Ng Yong Siang, Moses Siaw-Frimpong, Jeanne\nSibiude, Bountoy Sibounheuang, Atif Siddiqui, Louise Sigfrid, Piret Sillaots, Fatoumata Sillah,\nCatarina Silva, Maria Joao Silva, Rogério Silva, Benedict Sim Lim Heng, Wai Ching Sin, Dario\nSinatti, Budha Charan Singh, Punam Singh, Pompini Agustina Sitompul, Karisha Sivam, Vegard\nSkogen, Sue Smith, Benjamin Smood, Coilin Smyth, Michelle Smyth, Morgane Snacken, Dominic\nSo, Tze Vee Soh, Lene Bergendal Solberg, Joshua Solomon, Tom Solomon, Emily Somers, Agnès\nSommet, Myung Jin Song, Rima Song, Tae Song, Jack Song Chia, Michael Sonntagbauer, Azlan\nMat Soom, Arne Søraas, Camilla Lund Søraas, Albert Sotto, Edouard Soum, Ana Chora Sousa,\nMarta Sousa, Maria Sousa Uva, Vicente Souza-Dantas, Alexandra Sperry, Elisabetta Spinuzza, B.\nP. Sanka Ruwan Sri Darshana, Shiranee Sriskandan, Sarah Stabler, Thomas Staudinger, Stephanie-\nSusanne Stecher, Trude Steinsvik, Ymkje Stienstra, Birgitte Stiksrud, Eva Stolz, Amy Stone,\nAdrian Streinu-Cercel, Anca Streinu-Cercel, Ami Stuart, David Stuart, Richa Su, Decy Subekti,\nGabriel Suen, Jacky Y. Suen, Prasanth Sukumar, Asﬁa Sultana, Charlotte Summers, Dubravka\nSupic, Deepashankari Suppiah, Magdalena Surovcová, Atie Suwarti, Andrey Svistunov, Sarah\nSyahrin, Konstantinos Syrigos, Jaques Sztajnbok, Konstanty Szuldrzynski, Shirin Tabrizi, Fabio\nS. Taccone, Lysa Tagherset, Shahdattul Mawarni Taib, Ewa Talarek, Sara Taleb, Jelmer Talsma,\nRenaud Tamisier, Maria Lawrensia Tampubolon, Kim Keat Tan, Le Van Tan, Yan Chyi Tan, Clarice\nTanaka, Hiroyuki Tanaka, Taku Tanaka, Hayato Taniguchi, Huda Taqdees, Arshad Taqi, Coralie\nTardivon, Pierre Tattevin, M Azhari Tauﬁk, Hassan Tawﬁk, Richard S. Tedder, Tze Yuan Tee, João\nTeixeira, Soﬁa Tejada, Marie-Capucine Tellier, Sze Kye Teoh, Vanessa Teotonio, François Téoulé,\nPleun Terpstra, Olivier Terrier, Nicolas Terzi, Hubert Tessier-Grenier, Adrian Tey, Alif Adlan\nMohd Thabit, Anand Thakur, Zhang Duan Tham, Suvintheran Thangavelu, Elmi Theron, Vincent\nThibault, Simon-Djamel Thiberville, Benoît Thill, Jananee Thirumanickam, Niamh Thompson,\nShaun Thompson, David Thomson, Emma C. Thomson, Surain Raaj Thanga Thurai, Duong Bich\nThuy, Ryan S. Thwaites, Andrea Ticinesi, Paul Tierney, Vadim Tieroshyn, Peter S. Timashev,\nJean-François Timsit, Noémie Tissot, Fiona Toal, Jordan Zhien Yang Toh, Maria Toki, Kristian\nTonby, Sia Loong Tonnii, Marta Torre, Antoni Torres, Margarida Torres, Rosario Maria Torres\nSantos-Olmo, Hernando Torres-Zevallos, Michael Towers, Tony Trapani, Huynh Trung Trieu,\nThéo Trioux, Cécile Tromeur, Ioannis Trontzas, Tiffany Trouillon, Jeanne Truong, Christelle\nTual, Sarah Tubiana, Helen Tuite, Jean-Marie Turmel, Lance C.W. Turtle, Anders Tveita, Pawel\nTwardowski, Makoto Uchiyama, PG Ishara Udayanga, Andrew Udy, Roman Ullrich, Alberto\nUribe, Asad Usman, Effua Usuf, Timothy M. Uyeki, Cristinava Vajdovics, Piero Valentini, Luís\nVal-Flores, Ana Luiza Valle, Amélie Valran, Ilaria Valzano, Stijn Van de Velde, Marcel van den\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press\n914 O. Rohanian et al.\nBerge, Machteld Van der Feltz, Job van der Palen, Paul van der Valk, Nicky Van Der Vekens,\nPeter Van der Voort, Sylvie Van Der Werf, Marlice van Dyk, Laura van Gulik, Jarne Van Hattem,\nCarolien van Netten, Frank van Someren Greve, Gitte Van Twillert, Ilonka van Veen, Hugo\nVan Willigen, Noémie Vanel, Henk Vanoverschelde, Pooja Varghese, Michael Varrone, Shoban\nRaj Vasudayan, Charline Vauchy, Shaminee Veeran, Aurélie Veislinger, Sebastian Vencken, Sara\nVentura, Annelies Verbon, James Vickers, José Ernesto Vidal, César Vieira, Deepak Vijayan, Joy\nAnn Villanueva, Judit Villar, Pierre-Marc Villeneuve, Andrea Villoldo, Nguyen Van Vinh Chau,\nBenoit Visseaux, Hannah Visser, Chiara Vitiello, Manivanh Vongsouvath, Harald Vonkeman,\nFanny Vuotto, Noor Hidayu Wahab, Suhaila Abdul Wahab, Nadirah Abdul Wahid, Marina\nWainstein, Laura Walsh, Wan Fadzlina Wan Muhd Shukeri, Chih-Hsien Wang, Steve Webb,\nJia Wei, Katharina Weil, Tan Pei Wen, Sanne Wesselius, T. Eoin West, Murray Wham, Bryan\nWhelan, Nicole White, Paul Henri Wicky, Aurélie Wiedemann, Surya Otto Wijaya, Keith Wille,\nSuzette Willems, Virginie Williams, Evert-Jan Wils, Ng Wing Yiu, Calvin Wong, Teck Fung Wong,\nXin Ci Wong, Yew Sing Wong, Natalie Wright, Gan Ee Xian, Lim Saio Xian, Kuan Pei Xuan,\nIoannis Xynogalas, Sophie Yacoub, Siti Rohani Binti Mohd Yakop, Masaki Yamazaki, Yazdan\nYazdanpanah, Nicholas Yee Liang Hing, Cécile Yelnik, Chian Hui Yeoh, Stephanie Yerkovich,\nTouxiong Yiaye, Toshiki Yokoyama, Hodane Yonis, Obada Yousif, Saptadi Yuliarto, Akram\nZaaqoq, Marion Zabbe, Gustavo E Zabert, Kai Zacharowski, Masliza Zahid, Maram Zahran, Nor\nZaila Binti Zaidan, Maria Zambon, Miguel Zambrano, Alberto Zanella, Konrad Zawadka, Nurul\nZaynah, Hiba Zayyad, Alexander Zoufaly, David Zucman, Mazankowski Heart Institute.\nCite this article: Rohanian O, Nouriborji M, Jauncey H, Kouchaki S, Nooralahzadeh F, ISARIC Clinical\nCharacterisation Group, Clifton L, Merson L and Clifton DA (2024). Lightweight transformers for clinical natural\nlanguage processing. Natural Language Engineering30, 887–914. https://doi.org/10.1017/S1351324923000542\nhttps://doi.org/10.1017/S1351324923000542 Published online by Cambridge University Press",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9047940969467163
    },
    {
      "name": "Transformer",
      "score": 0.6444318294525146
    },
    {
      "name": "Natural language processing",
      "score": 0.43351277709007263
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36519378423690796
    },
    {
      "name": "Electrical engineering",
      "score": 0.11527121067047119
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}