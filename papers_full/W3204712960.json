{
  "title": "Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models",
  "url": "https://openalex.org/W3204712960",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1975226938",
      "name": "Robert Wolfe",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2408201350",
      "name": "Aylin Caliskan",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034847753",
    "https://openalex.org/W4297801368",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2955828557",
    "https://openalex.org/W2898081668",
    "https://openalex.org/W3002330681",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2251109971",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2962883166",
    "https://openalex.org/W2963723885",
    "https://openalex.org/W4231165370",
    "https://openalex.org/W2963366552",
    "https://openalex.org/W3213052799",
    "https://openalex.org/W2952349219",
    "https://openalex.org/W3037528277",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W4294103325",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2967952350",
    "https://openalex.org/W3123127232",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2964073004",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2792949345",
    "https://openalex.org/W2890560993",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W3115462295",
    "https://openalex.org/W3185212449",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W46679369",
    "https://openalex.org/W3035102548",
    "https://openalex.org/W2788481061",
    "https://openalex.org/W3156227657",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3035207248",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963759780",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2140534852",
    "https://openalex.org/W3098861490",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2162068690",
    "https://openalex.org/W2953307569",
    "https://openalex.org/W3094306499",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2740983644"
  ],
  "abstract": "We use a dataset of U.S. first names with labels based on predominant gender and racial group to examine the effect of training corpus frequency on tokenization, contextualization, similarity to initial representation, and bias in BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white names are less frequent in the training corpora of these four language models. We find that infrequent names are more self-similar across contexts, with Spearman’s rho between frequency and self-similarity as low as -.763. Infrequent names are also less similar to initial representation, with Spearman’s rho between frequency and linear centered kernel alignment (CKA) similarity to initial representation as high as .702. Moreover, we find Spearman’s rho between racial bias and name frequency in BERT of .492, indicating that lower-frequency minority group names are more associated with unpleasantness. Representations of infrequent names undergo more processing, but are more self-similar, indicating that models rely on less context-informed representations of uncommon and minority names which are overfit to a lower number of observed contexts.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 518–532\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n518\nLow Frequency Names Exhibit Bias and Overﬁtting in Contextualizing\nLanguage Models\nRobert Wolfe\nUniversity of Washington\nrwolfe3@uw.edu\nAylin Caliskan\nUniversity of Washington\naylin@uw.edu\nAbstract\nWe use a dataset of U.S. ﬁrst names with la-\nbels based on predominant gender and racial\ngroup to examine the effect of training corpus\nfrequency on tokenization, contextualization,\nsimilarity to initial representation, and bias in\nBERT, GPT-2, T5, and XLNet. We show that\npredominantly female and non-white names\nare less frequent in the training corpora of\nthese four language models. We ﬁnd that infre-\nquent names are more self-similar across con-\ntexts, with Spearman’s ρ between frequency\nand self-similarity as low as −.763. Infre-\nquent names are also less similar to initial rep-\nresentation, with Spearman’s ρ between fre-\nquency and linear centered kernel alignment\n(CKA) similarity to initial representation as\nhigh as .702. Moreover, we ﬁnd Spearman’s\nρ between racial bias and name frequency in\nBERT of .492, indicating that lower-frequency\nminority group names are more associated\nwith unpleasantness. Representations of infre-\nquent names undergo more processing, but are\nmore self-similar, indicating that models rely\non less context-informed representations of un-\ncommon and minority names which are overﬁt\nto a lower number of observed contexts.\n1 Introduction\nHuman social perception is linked to frequency of\nobservation. Hughes et al. (2019) show using func-\ntional magnetic resonance imaging (fMRI) scans\nthat humans are more aware of variation in the\nfaces of members of their own race, and perceive\nmembers of other races as repeated instances of a\nsocial class, rather than as individuals. Most people\ninteract more with individuals of their own race,\nand develop better cognitive skills for differentiat-\ning members of the race they see most frequently.\nRecent research indicates that state-of-the-art AI\nsystems mirror such biased and unequal human\nperceptions. For example, Buolamwini and Gebru\n(2018) show that under-representation in the train-\ning data of computer vision models causes poor\nperformance on classiﬁcation tasks for women and\nminority racial groups.\nFirst names are used in both social psychology\nand Natural Language Processing (NLP) as a proxy\nto ground truth data for studying racial and gender\nbiases. The implicit association test (IAT) of Green-\nwald et al. (1998) ﬁnds that study participants per-\nceive European-American names as more pleasant\nthan African-American names, and Caliskan et al.\n(2017) demonstrate with the word embedding as-\nsociation test (WEAT) that human biases observed\nin the IAT exist in static word embeddings, learned\nvector representations of words widely used in NLP.\nWe use a list of ﬁrst names labeled by gender and\nracial group based on U.S. Social Security Admin-\nistration data and the names dataset of Tzioumis\n(2018) to analyze how name frequency affects mi-\nnority social groups in four neural language models:\nBERT, GPT-2, XLNet, and T5. Neural language\nmodels have advanced the state of the art in NLP,\nand are found in consequential NLP contexts such\nas Google Search (Nayak, 2019). These models\nproduce contextualized word embeddings, which\nincorporate information from the context in which\nthe word occurs over a series of neural network\nlayers. May et al. (2019) and Guo and Caliskan\n(2021) show that racial, gender, and intersectional\nbiases exist in neural language models. We exam-\nine whether under-representation in the training\ncorpora of such models causes them to overﬁt non-\nwhite and female names, reducing model general-\nization for underrepresented minorities. We list our\nresearch questions and contributions:\n(1) Are minority social group names less fre-\nquent in the training corpora of neural lan-\nguage models? We process four training corpora\nand ﬁnd that white and male names are the most\nfrequent in all training corpora.\n(2) Are infrequent and minority group names\nsplit into subwords by neural language models\nmore than frequent and majority group names?\n519\nWe report the percentage of names singly tokenized\nfor eight demographic groups. Minority and female\ngroup names are singly tokenized less than white\nand male names. Single tokenization correlates\nwith frequency, with Spearman’sρup to .835.\n(3) Are infrequent minority group names more\nbiased than frequent minority group names?\nWe take Spearman’sρof name frequency and bias\nusing the WEAT. We ﬁnd that infrequent racial mi-\nnority group names are more negative in BERT,\nwith Spearman’sρof frequency and pleasantness\nassociation of .492. Common female names exhibit\ngreater gender bias in BERT, with Spearman’sρof\nfrequency and career/family bias of -.553.\n(4) Are infrequent and minority group names\nless contextualized than frequent and majority\ngroup names? Is this the result of overﬁtting,\nor of underﬁtting? We examine intra-layer self-\nsimilarity of embeddings across contexts to de-\ntermine how contextualized a name is, and mea-\nsure its similarity to initial representation in the\nmodel’s embedding lookup matrix using linear cen-\ntered kernel alignment (CKA) of Kornblith et al.\n(2019). Inter-layer similarity to initial representa-\ntion shows how much processing a name undergoes,\nindicating whether a poorly contextualized word\nis overﬁt to few observed contexts, if it’s notably\ndifferent from initial representation, or underﬁt,\nif it’s similar to the representation in the embed-\nding lookup matrix. We ﬁnd that infrequent and\nminority group names exhibit higher intra-layer\nself-similarity, with Spearman’sρof frequency and\nintra-layer self-similarity as low as -.763. Infre-\nquent and minority group names are less similar to\ninitial representation, with Spearman’sρbetween\nname frequency and similarity to initial representa-\ntion up to .702, suggesting overﬁtting.\nWe use Spearman’s ρ rather than Pearson’s ρ\nto capture monotonic with name frequency, as we\nobserve effects primarily on a log scale. The null\nhypothesis is that frequency does not affect tok-\nenization, social bias, and contextualization, and\nwe disprove it by obtaining a two-tailed p-value.\n2 Related Work\nWe survey prior work related to frequency’s inﬂu-\nence on static and contextualized word embeddings.\nStatic word embeddings are vector representations\nof words based on co-occurrence statistics of words\nin a training corpus. Contextualized word embed-\ndings are vector representations of words from a\nneural language model which incorporates infor-\nmation from context to minimize loss on a train-\ning objective, such as next-word prediction (lan-\nguage modeling). Static word embeddings have\none vector per word, while contextualized embed-\ndings vary with context, allowing them to capture,\nfor example, the sense of a polysemous word.\nThe WEAT of Caliskan et al. (2017) shows\nthat the strength of association of a set of target\nstatic word embeddings ( e.g., two social groups)\nto two sets of polar attribute static word embed-\ndings (e.g., pleasant/unpleasant) encodes widely\nshared non-social biases, stereotypes, and factual\ninformation about the world. May et al. (2019)\nextend the WEAT to neural language models with\nthe Sentence Encoder Association Test (SEAT),\nwhich inserts WEAT target words into semanti-\ncally bleached sentence templates such as \"This\nis <word>\" and measures the association of sen-\ntence vectors rather than word vectors. Kurita et al.\n(2019) mask target and attribute words in template\nsentences, and directly query BERT’s masked lan-\nguage modeling objective to compute an associa-\ntion of target words to attributes. Guo and Caliskan\n(2021) evaluate the overall magnitude of bias in lan-\nguage models by extending the WEAT to the Con-\ntextualized Embedding Association Test (CEAT).\nBrunet et al. (2019) perturb the sparse word co-\noccurrence matrix of the GloVe static word em-\nbedding algorithm of Pennington et al. (2014) by\nomitting co-occurrence information from a partic-\nular subsection of the corpus, and show that em-\nbeddings of rare words are the most biased and\nthe most sensitive to corpus perturbations. Wang\net al. (2020) ﬁnd that the frequency of a word in a\ntraining corpus can twist gender direction and cre-\nate features in static word embeddings which vary\nbased on frequency. Our work analyzes neural lan-\nguage models, which do not form representations\ndirectly based on co-occurrence statistics.\nGong et al. (2018) examine static Word2Vec and\nTransformer word embeddings used in machine\ntranslation and ﬁnd that embeddings of high and\nlow-frequency words lie in separate subregions of\nthe embedding space, and that an embedding of\na rare word and a common word can be distant\neven if they are semantically similar. The authors\ntrain an adversarial model to produce embeddings\nnot separable based on frequency. Provilkov et al.\n(2020) propose byte-pair encoding dropout, a sub-\nword regularization algorithm which allows multi-\n520\nple byte-pair encoding segmentations of the same\nword and corrects the problem of rare subtokens ex-\nisting in a separate subregion of embedding space.\nMu and Viswanath (2018) show that the top\ntwo directions using Principal Component Anal-\nysis (PCA) in Skip-Gram, GloVe, and Continuous\nBag of Words (CBOW) static word embeddings en-\ncode frequency-related features, and that removing\nfrequency-related features improves performance\non tasks related to word similarity. Ott et al. (2018)\nﬁnd that beam search in neural machine translation\nprefers common tokens, including common sub-\nword tokens, to uncommon tokens. Wendlandt et al.\n(2018) measure the stability of static word embed-\ndings, deﬁned as consistency in the percent overlap\nof nearest neighbors, and ﬁnd that frequency con-\ntributes to semantic stability in word embeddings.\nEthayarajh (2019) demonstrate that contextual-\nized word embeddings in BERT, GPT-2, and ELMo\noccupy an increasingly anisotropic vector space\nas they incorporate context. The most context-\nspeciﬁc, least self-similar tokens are the most fre-\nquent in the training corpora, and in BERT, a token\nembedding becomes more similar to the embed-\ndings in the context around it as it is contextual-\nized. Zhou et al. (2021) ﬁnd that frequency distorts\nthe geometry of contextualized embeddings from\nBERT, which causes the model to over or under-\nestimate semantic similarity of words based on\nfrequency in the training corpus.\n3 Background: Neural Language Models\nOur work considers BERT, GPT-2, T5, and XLNet.\nWe choose these neural language models both be-\ncause they are commonly used and studied, and\nbecause they use three different subword tokeniz-\ners, allowing us to conduct our tokenization anal-\nysis across three different algorithms. We use the\n12-layer cased implementation of each transformer\nin Python with TensorFlow and the Hugging Face\nTransformers library of Wolf et al. (2020). Ap-\npendix A provides details on these models.\n3.1 Subword Tokenization\nMost neural language models use subword tokens\nto represent text. Each subword is tied to a vector\nin the model’s embedding lookup matrix, which\nis trained with the model. Some common words\nin the training corpus are represented with a sin-\ngle embedding, but most are broken into subcom-\nponents and mapped to multiple subword embed-\ndings. Subword tokenization solves the out-of-\nvocabulary (OOV) problem, which occurs when\na language model encounters a word not in its vo-\ncabulary. Subword tokenization allows a model to\nmaintain a smaller vocabulary than a model with a\nfull-word vocabulary, like TransformerXL of Dai\net al. (2019), which has a vocabulary size of over\n267,000, and uses the OOV token for words not\nin its vocabulary. Subword tokenization is faster\nthan character convolutions to form an initial em-\nbedding, as in Peters et al. (2018) in ELMo, or\nBoukkouri et al. (2020) in Character-BERT. All\nfour language models examined use subword to-\nkenization. BERT uses WordPiece, GPT-2 uses\nByte-Pair Encoding, and XLNet and T5 use Sen-\ntencePiece. Appendix B provides further details.\n3.2 Representational Similarity Measures\nV oita et al. (2019) use projection-weighted canoni-\ncal correlation analysis (PWCCA) to measure em-\nbedding change from layer to layer of language\nmodels trained for different tasks, a method devel-\noped by Morcos et al. (2018) which can measure\nthe evolution of neural network representations.\nMore recently, Kornblith et al. (2019) measure\nlayer differences using linear CKA.\nKiela et al. (2015) use dispersion of image vec-\ntors as a measure of the generality of an associated\nword to distinguish hypernyms from hyponyms.\nEthayarajh (2019) use self-similarity to measure\ncontextualization in neural language models.\n3.3 Contextualized Embedding Extraction\nBommasani et al. (2020) note that intermediate lay-\ners of neural language models are often used in\ndownstream applications, and Ethayarajh (2019)\nﬁnds that static embeddings formed from the ﬁrst\ntwo layers of BERT and GPT-2 are accurate for sev-\neral common NLP tasks. Tenney et al. (2019) show\nthat layers of BERT attend primarily to a certain\nNLP task (such as coreference), and occur in an\nexpected order. V oita et al. (2019) ﬁnd that a model\npretrained for language modeling (next-word pre-\ndiction), such as GPT-2, loses information about\nthe current token while forming a prediction about\nthe future, meaning that the top layer is poorly\nsuited to analysis of the input token.\nWe use the ValNorm method of Toney-Wails\nand Caliskan (2021) to choose the layer of each\nmodel which reﬂects the semantics of the input\ntoken. ValNorm obtains a valence score for each\nnon-social group word in a lexicon by calculating\n521\nits association with two sets of pleasant and un-\npleasant words based on cosine similarity. It then\nobtains Pearson’s ρ to measure the similarity of\nthe valence scores of the word embeddings with\na set of human-evaluated ratings. This is referred\nto as the ValNorm score, which the authors ﬁnd\nis stable across languages, historical periods, and\nword embedding algorithms. ValNorm was devel-\noped for static word embeddings, and we extend\nit to measure the semantic quality of contextual-\nized word embeddings in each layer of a neural\nlanguage model.\n4 Datasets\nWe use a dataset of ﬁrst names from Tzioumis\n(2018) cross-referenced with U.S. Social Security\nAdministration (SSA) data to analyze frequency in\nlanguage models. To obtain contextualized repre-\nsentations of the names from language models, we\ngather contexts from the Reddit corpus of Baum-\ngartner et al. (2020). For name frequency statistics,\nwe process the training corpus of each model.\n4.1 First Names\nWe obtain a dataset of ﬁrst names segmented by\ndemographic group by cross-referencing a list de-\nveloped by Tzioumis (2018) of ﬁrst names labeled\nby racial self-identiﬁcation with 1990 SSA data,\nwhich includes information about the gender and\nthe frequency of a ﬁrst name based on births in\n1990. Tzioumis’ list combines data from three\nmortgage datasets, and Tzioumis estimates that it\ncovers 85.6% of the U.S. population, based on U.S.\ncensus data. We assign each name a label corre-\nsponding to the race as which the highest number\nof individuals possessing that name self-identify.\nEach name is assigned a gender based on the gender\nwith the greater number of births with the name in\nthe 1990 SSA data. The resulting dataset of 3,757\nnames includes two genders (Female, Male) and\nfour racial groups (Asian, Black, Hispanic, White).\nAppendix C includes more information.\n4.2 Contexts\nTo measure contextualization, we extract contex-\ntualized embeddings from a variety of contexts.\nFollowing Guo and Caliskan (2021), we harvest\ncontexts from the Reddit corpus of Baumgartner\net al. (2020) at pushshift.io . We remove\ncontexts in which a name exists in a ﬁrst-last name\ncombination which refers to a public ﬁgure (e.g.,,\n\"Taylor Swift\"), which could skew our analysis.\nTo control for the inﬂuence of context, we har-\nvest 1,000 contexts for the relatively unisex name\n\"Taylor\" (7,258 female births vs. 6,577 male births\nin 1990 SSA data), and create a set of identical con-\ntexts for every name in our dataset with each name\nreplacing \"Taylor.\" For example, we change “I saw\nTaylor last week” to “I saw Latisha last week” to\nrepresent a black woman.\n4.3 Training Corpora\nWe obtain ground truth data for name frequency\nin the training corpora of each language model.\nMost of the training corpora are not available to the\npublic, but can be replicated or approximated.\nBERT was trained on the BookCorpus and En-\nglish Wikipedia. The BookCorpus is no longer\navailable, and we use the open source reproduction\nof Kobayashi (2018). We use the September 20,\n2017 English Wikipedia dump made available by\nWikimedia1, from the time period BERT was being\ntrained. We combine name counts from the Book-\nCorpus and Wikipedia to obtain name frequencies\nfor BERT and XLNet. XLNet is trained on three\nadditional corpora which are available for a fee or\nwhich would be difﬁcult to reconstruct. The .792\nSpearman coefﬁcient we obtain from combined\nBookCorpus and Wikipedia frequencies and sin-\ngle tokenization of names by XLNet indicates that\nthese frequencies are representative of the entire\nXLNet training corpus. The WebText corpus on\nwhich GPT-2 was trained was not made available\nto the public. We use the open source replication\nOpenWebText by Gokaslan and Cohen (2019), pro-\nduced based on the web crawling heuristic of Rad-\nford et al. (2019). T5 was trained on the Colossal\nCleaned Crawled Corpus (C4). We obtain ground\ntruth name frequencies for C4 from the 800GB\ncleaned version reconstructed by AllenAI2.\nTable 1 shows that white male names have the\nhighest median frequency in every training corpus,\nand the median frequency of any male group is\nhigher than that of any female group in OpenWeb-\nText, Wikipedia, and C4. Black, Hispanic, and\nAsian females have the lowest median frequency\nin every training corpus.\n1archive.org/details/enwiki-20170920\n2github.com/allenai/allennlp/\n522\nMedian Frequency of Names by Demographic Group\nTraining Corpus AF BF HF WF AM BM HM WM\nOpenWebText 1,458 1,034 679 2,142 4,172 12,138 2,215 13,593\nBookCorpus 378 578 219 1,191 483 2,644 274 3,074\nWikipedia 5,660 1,566 3,372 5,968 10,102 21,857 8,384 28,455\nC4 32,257 16,843 13,149 41,710 66,661 125,743 30,760 163,014\nTable 1: White names and male names are the most common in reconstructions of training corpora.\n5 Approach and Experiments\nWe measure the effects of frequency on tokeniza-\ntion, social bias, and contextualization to under-\nstand disparities between majority social groups\nand the minority racial and gender social groups\nidentiﬁable in our names dataset.\n5.1 Tokenization\nWe tokenize names in our dataset using the de-\nfault Transformers library tokenizers of Wolf et al.\n(2020) for BERT, GPT-2, T5, and XLNet. We ob-\ntain the proportion of names singly tokenized for\nthe racial and gender groups in our data. Tokeniza-\ntion by social group informs our other experiments,\nas contextualization varies based on tokenization.\n5.2 Bias\nWe quantify the bias association of each name for\nﬁve WEATs described by Caliskan et al. (2017).\nThese include pleasant/ unpleasant (25 words, 8\nwords), career/family, math/art, and science/art.\nFollowing WEAT, each set contains at least 8 words\nto satisfy concept representation signiﬁcance. Ac-\ncordingly, the limitations of not adhering to this\nmethodological robustness rule of WEAT, which\nare outlined by Ethayarajh et al. (2019), are mit-\nigated. Pleasant/unpleasant tests measure racial\nbias, and career/family, math/art, and science/art\ntests measure gender bias. The formula for the\nsingle-value WEAT follows. w refers to a target\nword, and Aand Bto sets of polar attribute words.\nmeana∈Acos( ⃗ w,⃗ a) −meanb∈Bcos( ⃗ w,⃗b)\nstd_devx∈A∪Bcos( ⃗ w,⃗ x)\nWe extract contextualized word embeddings us-\ning semantically bleached sentences as described\nby May et al. (2019). For names, we use the seman-\ntically bleached sentence template “This person’s\nname is <name>.” For WEAT attribute words, we\nuse the context “This is <word>.” Unlike May et al.\n(2019), we extract a contextualized word embed-\nding of the stimuli rather than a sentence vector.\nThe WEAT measures association using cosine\nsimilarity, which requires that vectors are of equal\nlength, so we must choose a pooling method to\nrepresent multiply subtokenized words. Follow-\ning Bommasani et al. (2020), who show that mean\npooling produces accurate representations for word\nsimilarity tasks, we use the mean of subword vec-\ntors to represent multiply tokenized words.\nWe use the ValNorm method of Toney-Wails\nand Caliskan (2021) to select an intermediate layer\nfrom which to extract contextualized word embed-\ndings. ValNorm evaluates semantic quality based\non the single-value WEAT. To adapt ValNorm for\nlanguage models, we pool single-value WEAT com-\nparisons to obtain a combined effect size as de-\nscribed by Guo and Caliskan (2021), and select\nthe layer which produces embeddings best corre-\nsponding to human evaluations of word valence.\nBecause our work uses the WEAT to measure bias,\nValNorm is an especially useful method for eval-\nuating which layers encode semantic information\nrelated to the current token. With ValNorm, we\nobtain Pearson’sρof .881 in BERT layer 9, .859\nin GPT-2 layer 7, .892 in T5 encoder layer 12, and\n.854 in XLNet layer 5. We refer to the layer with\nthe highest ValNorm score as the semantic layer.\nWe obtain bias scores in the semantic layer for\neach name using the SV-WEAT on each bias test,\nand take Spearman’sρbetween bias and frequency\nfor each minority group name in ﬁve tests.\n5.3 Contextualization\nContextualization measures how much information\nfrom context a word incorporates. Words that incor-\nporate information from context generalize well in\ncontextualizing language models, whereas words\nwhich do not incorporate contextual information\ngeneralize poorly. To quantify contextualization,\nwe measure intra-layer self-similarity across con-\ntexts. If a word is less contextualized, we seek to\nunderstand whether this is due to overﬁtting, or\nunderﬁtting. How much processing a word under-\n523\ngoes in a model can help to indicate whether it is\noverﬁt or underﬁt. If the model changes the word\nsigniﬁcantly from initial representation, but fails to\ncontextualize it, this suggests that the word is over-\nﬁt, and processed to be similar to its embedding in\nfew other contexts. If the model does not change\nthe word much from its initial representation, and\nfails to contextualize it, this suggests that the model\nrelies on a general representation, similar to what it\nsees in its embedding lookup matrix, indicating that\nthe representation may be underﬁt. We measure\ninter-layer self-similarity to initial representation\nto determine whether poorly contextualized words\nare overﬁt or underﬁt.\nFor contextualization, we use concatenations\nof subword vectors for multiply tokenized words,\nrather than mean pooling, to compare representa-\ntions closest to the way the model sees them.\n5.3.1 Intra-Layer Self-Similarity\nWe measure intra-layer self-similarity for each\nname in 1,000 identical contexts. Intra-layer self-\nsimilarity is the mean cosine similarity of contextu-\nalized embeddings for a word generated in a single\nlayer of a language model, and ranges between 0\n(dissimilar) and 1 (similar). The formula below\nwas described by Ethayarajh (2019), but removes\na function to map a word to a layer and sentence\nindex.\ns(w) = 1\nn2 −n\n∑\ni\n∑\nj̸=i\ncos( ⃗ wi, ⃗ wj)\nWe take Spearman’s ρof frequency and intra-\nlayer self-similarity, and compare mean self-\nsimilarity for singly and multiply tokenized names.\n5.3.2 Inter-Layer Similarity\nWe use linear CKA, a similarity index ranging be-\ntween 0 (dissimilar) and 1 (similar), to measure\nsimilarity across layers. Proposed by Kornblith\net al. (2019) for measuring similarity in neural net-\nwork representations, linear CKA is not invariant\nto invertible linear transformation, but is invariant\nto orthogonal transformation and isotropic scal-\ning, and identiﬁes correspondences between lay-\ners more accurately than PWCCA. Invariance to\nisotropic scaling is useful, as Ethayarajh (2019)\nfound that upper layers of BERT and GPT-2 exhibit\nhigh anisotropy. Linear CKA operates on matrices\nof observations. We form matrices of 1,000 contex-\ntualized word embeddings for each name, inserted\ninto the same 1,000 contexts as every other name.\nBelow is the formula for linear CKA.\n||YT X||2\nF\n||XT X||F ||YT Y||F\nX and Y are matrices of examples, and ||·|| F\nrefers to the Froebenius norm. We report Spear-\nman’sρ of name frequency and CKA similarity,\nand compare mean CKA similarity for singly and\nmultiply tokenized names.\n6 Results\nWe ﬁnd that common names are singly tokenized\nmore than less common names; that name fre-\nquency correlates with social bias in BERT, and to\na lesser extent in GPT-2 and T5; and that infrequent\nand multiply tokenized names exhibit higher intra-\nlayer self-similarity and lower inter-layer similarity\nto initial representation, suggesting overﬁtting.\nFigure 1: Male vs. female name tokenization\n6.1 Tokenization\nCommon names are singly tokenized more than un-\ncommon names. Spearman’sρbetween frequency\nand single tokenization is .835 for BERT, .772 for\nGPT-2, .630 for T5, and .792 for XLNet. Commen-\nsurate with these correlations, Table 2 shows that\nwhite male names are the most singly tokenized\nin each language model. In BERT and XLNet,\nmale names are singly tokenized more than female\nnames, and white names are singly tokenized more\nthan Asian, black, and Hispanic names.\n6.2 Bias\nTable 3 shows Spearman’sρof bias score and train-\ning corpus frequency for minority group names.\n524\nProportion of Singly Tokenized Words by Demographic Group\nModel AF BF HF WF AM BM HM WM\nBERT .218 .081 .122 .308 .331 .563 .329 .619\nGPT-2 .250 .541 .085 .211 .376 .438 .243 .535\nT5 .199 .027 .074 .122 .198 .219 .119 .328\nXLNet .308 .162 .079 .262 .392 .531 .337 .590\nTable 2: White male names are most singly tokenized.\nPleasant/unpleasant (PU) coefﬁcients measure cor-\nrelation between frequency and association with\npleasantness for non-white names. Career/family\n(CF), math/art (MA), and science/art (SA) measure\ncorrelation between frequency and association with\ngender stereotypes for female names.\nCorrelation of Bias and Frequency\nBias Test BERT GPT-2 T5 XLNet\nRace PU25 .492 -.011 .020 -.139\nRace PU8 -.021 -.022 .229 .020\nGender CF -.553 .065 .063 -.333\nGender MA -.311 .139 .094 -.199\nGender SA -.304 .244 -.080 .164\nTable 3: Infrequent non-white names are more negative\nin BERT and T5.\nBERT and T5 exhibit positive correlation be-\ntween pleasantness and minority group name fre-\nquency, with Spearman’sρof .492 and p-value of\n10−167 for BERT. For these two models, more fre-\nquent minority group names are more associated\nwith pleasantness. BERT also exhibits negative\ncorrelations between gender bias and frequency,\nindicating that more frequent observation of a fe-\nmale name reinforces its association with female\nstereotypes. The correlation between frequency\nin the training corpus and association with career\nas opposed to family in BERT is −.553 for fe-\nmale names, with a p-value of 10−161. XLNet ex-\nhibits similar correlations for the career/family and\nmath/arts WEATs. P-values for correlations greater\nthan 0.1 or less than −0.1 are less than 10−10.\n6.3 Contextualization\nInfrequent names exhibit higher intra-layer self-\nsimilarity in each examined layer of each language\nmodel, and exhibit lower inter-layer similarity to\ninitial representation, indicating that infrequent\nnames generalize poorly to context and are overﬁt\nto the contexts in which they have been observed.\n6.3.1 Intra-Layer Similarity\nTable 4 shows negative Spearman’s ρ in BERT,\nGPT-2, T5, and XLNet, ranging between −.523\nand −.763, with p-values < 10−263. Most of\nthe least negative correlations occur in the top\nlayer, possibly due to high anisotropy in upper lay-\ners of language models observed by Ethayarajh\n(2019). Correlation of frequency and intra-layer\nself-similarity is highest in the ﬁrst layer of GPT-2\nand XLNet.\nCorrelation of Frequency and Self-Similarity\nLayer BERT GPT-2 T5 XLNet\nFirst -.688 -.703 -.523 -.763\nSecond -.683 -.681 -.574 -.732\nSemantic -.693 -.633 -.573 -.698\nOutput -.598 -.640 -.573 -.649\nTable 4: Infrequent names exhibit higher intra-layer\nself-similarity, and are less contextualized.\nFigure 2: Multiply tokenized and infrequent names ex-\nhibit high intra-layer self-similarity in BERT.\nTable 5 shows that multiply tokenized names ex-\nhibit higher intra-layer self-similarity than singly\ntokenized names. More common names, and\nnames of majority group members, are more con-\ntextualized in language models than are infrequent\nnames and names of minority group members.\n525\nMean Intra-Layer Self-Similarity by Tokenization\nLayer BERT GPT-2 T5 XLNet\nSingle Multi Single Multi Single Multi Single Multi\nFirst .901 .923 .761 .855 .933 .974 .848 .915\nSecond .863 .926 .762 .815 .916 .965 .759 .846\nSemantic .772 .840 .736 .826 .702 .769 .613 .653\nOutput .771 .808 .810 .974 .702 .769 .863 .926\nTable 5: Multiply tokenized names exhibit higher intra-layer self-similarity.\n6.3.2 Inter-Layer Similarity\nTable 6 reports statistically signiﬁcant positive cor-\nrelations between frequency and inter-layer CKA\nsimilarity to initial representation, except in the\nﬁrst layer of GPT-2. Spearman’sρranges between\n.174 and .702, with p-values smaller than 10−27.\nCorrelation of Frequency and CKA Similarity\nLayer BERT GPT-2 T5 XLNet\nFirst .461 -.299 .174 .623\nSecond .592 .487 .382 .636\nSemantic .702 .465 .513 .677\nOutput .480 .242 .513 .338\nTable 6: Infrequent names exhibit lower inter-layer\nCKA Similarity to initial representation.\nEmbeddings of infrequent names are less simi-\nlar to initial representation, indicating that poorly\ncontextualized infrequent names are overﬁt to pre-\nviously observed contexts, and that representations\nin the embedding lookup matrix carry more infor-\nmation for common and singly tokenized names.\nResults for tokenization and mean inter-layer\nself-similarity are reported in Table 7. We observe\nthat contextualized representations of multiply tok-\nenized names are less similar to their initial repre-\nsentations than singly tokenized names.\n7 Discussion\nWhite male names occur more than any other social\ngroup in all training corpora we process. Black and\nHispanic female names are least frequent, show-\ning that frequency-based disparities in language\nmodels have notable consequences for non-white\nfemales. Each language model represents white\nmale names with a single token more than any\nother group. Black and Hispanic female names are\nthe least represented with a single subtoken, with\nless than 10% singly tokenized in most models,\nreﬂecting disparities based on gender and race.\nFigure 3: Infrequent names are less similar to initial\nrepresentation in the semantic layer of XLNet.\nIn BERT, we observe Spearman’s ρ of name\nfrequency and racial bias of .492, indicating that\nless frequent minority names are more associated\nwith unpleasantness. Gender bias in BERT exhibits\nnegative correlation with frequency, indicating that\nit is reinforced by frequently observing a female\nname. While increased representation may address\ndisparities in contextualization, fair and diverse\ncontext occurrence is also important to ensure that\nlanguage models do not learn biased embeddings.\nInfrequent and multiply tokenized names exhibit\nhigher intra-layer self-similarity across language\nmodels, but lower inter-layer similarity to initial\nrepresentation. Such names incorporate less infor-\nmation from context, and change more from initial\nrepresentation, suggesting that disparities in con-\ntextualization are the result of overﬁtting.\nOur work focuses on ﬁrst names, but under-\nrepresentation of minority names suggests that\nwords and ideas speciﬁc to minority groups are\nalso likely under-represented in the training cor-\npora of neural language models, and will have the\nsame issues with regard to tokenization, bias, and\ncontextualization. Increased representation of mi-\nnority groups in training corpora would mitigate\n526\nMean CKA Similarity by Tokenization\nLayer BERT GPT-2 T5 XLNet\nSingle Multi Single Multi Single Multi Single Multi\nFirst .860 .831 .020 .026 .705 .660 .835 .763\nSecond .736 .633 .013 .006 .564 .475 .674 .515\nSemantic .149 .088 .004 .001 .398 .201 .370 .238\nOutput .064 .051 .016 .015 .398 .201 .002 .001\nTable 7: Multiply tokenized names are less similar to initial representation than singly tokenized names.\nmany frequency-related problems, and correct bi-\nases in the corpora themselves. However, some\nwords will always be more common than others.\nTo reduce frequency-based disparities, language\nmodels trained on multiple objectives, such as T5,\nmight incorporate an adversarial training objective\nsuch as the method described by Gong et al. (2018).\nLimitations of our work relate to using a list of\nnames, which cannot capture many characteristics\nwhich may be biased in language models, such as\nsexual orientation. Please see our Ethical Consid-\nerations section for a detailed discussion of ethical\nconcerns and limitations related to our work.\n8 Conclusion\nWe show that names predominantly belonging to\nmembers of minority social groups occur less fre-\nquently than majority group names in training cor-\npora of neural language models, and that low fre-\nquency results in lower rates of single tokenization,\nless contextualization, and overﬁt representations\nin neural language models.\n9 Ethical Considerations\nOur work with large-scale demographic datasets\nhas involved simpliﬁcations which, while useful\nfor studying the impact of language models on\nmarginalized demographic groups, has also intro-\nduced a number of limitations to our work. These\nare primarily related to the use of categorical labels\nfor gender and race, but also concern the properties\nof the demographic datasets employed and the role\nof frequency in interpreting our results.\nA central limitation of our work is that categori-\ncal labels are assigned to each name based on gen-\nder and race. We understand race and gender not\nas essential characteristics, but as deﬁned within a\nparticular culture, time period, and social structure.\nOur research question is whether underrepresen-\ntation (i.e., low frequency) of social groups in a\nlanguage model training corpus results in overﬁt-\nting and exacerbated bias in the trained language\nmodel. To answer this, we analyze frequency of\nrepresentation and its effects along two axes of de-\nmographic bias: gender bias, wherein males are\nmore represented than females; and racial bias,\nwherein people considered to be white are repre-\nsented more than people considered to be black,\nHispanic, or Asian. The range of social groups\nstudied reﬂects not an ideal division of humanity\ninto immutable categories, but a comprehensible\nway of interpreting the disadvantages caused by\nunderrepresentation in our current cultural context.\nOur work can be thought of an exposure study\nwhich examines the effects of one aspect of gender\nand race: underrepresentation. In the context of\nstudies of race, Sen and Wasow (2016) usefully\ndeﬁne an exposure study as one which uses \"a cue\nor signal that generates some reaction,\" and note\nthat \"names often act as a proxy for for traits associ-\nated with racial or ethnic groups.\" Well-established\nNLP methods such as the WEAT of Caliskan et al.\n(2017) have demonstrated the efﬁcacy of using ﬁrst\nnames for observing biases in AI. As described in\nSection 4.1, we use U.S. Social Security Adminis-\ntration (SSA) data to give each ﬁrst name a gender\nlabel, and a dataset which uses the same racial cate-\ngories as the U.S. census does for surnames to give\neach ﬁrst name a racial label. We do not intend to\nessentialize the name, or individuals bearing the\nname, but to identify the gender or racial associ-\nation most likely to be perceived by a language\nmodel within the cultural context in which it was\ntrained. We regret that many individuals are not\nrepresented by this set of categories. This is true\nfor transgender and nonbinary individuals, as the\nSSA data reﬂects a gender binary, and also for peo-\nple whose racial identity is not captured by one of\na few categories.\nCensus data includes information about racial\nidentiﬁcation for surnames, but not for ﬁrst names,\nprompting us to seek another source for this data.\n527\nWe choose the dataset of Tzioumis (2018) because\nit has wide coverage (85.6% of the U.S. population\nas estimated by Tzioumis); is ethically anonymized;\nis based on at least 30 observations for 91.2% of\nincluded names; and contains ﬁrst name data based\non self-identiﬁcation, considered a “gold standard”\nfor demographic classiﬁcation, as noted by Larson\n(2017) in the context of labeling data by gender.\nTzioumis’ dataset uses the same racial categories\nas U.S. census data for surnames, indicating that\nthese categories correspond to our cultural context.\nThat many names occur in more than one racial\ngroup or gender group is also a limitation, and may\nintroduce noise into our analyses. However, more\nthan 80% of names examined in our work have a\nself-identiﬁcation rate of at least 70% with a single\nracial group in the Tzioumis dataset. Based on SSA\ndata, 31.3% of the names in our study have only\nmale occurrences, 38.6% of the names in our study\nhave only female occurrences, and 30.1% of names\noccur for both male and female individuals. 88.0%\nof names with male and female occurrences have\nat least 70% of occurrences associated with one\ngender. Thus, assigning a label based on the group\nwith the most occurrences is likely to capture both\nwhich groups are most affected by our ﬁndings, as\nwell as the linguistic signals related to race and\ngender which make ﬁrst names a useful proxy to\nthis information.\nWe also note that the time periods from which\ndemographic and language modeling data are de-\nrived may inject some noise into our analysis. We\ncross-reference SSA data on births in 1990 with the\nTzioumis dataset, which is based on 2007 and 2010\nmortgage applications. The populations studied in\nthese two datasets are unlikely to overlap exactly,\nthough they both sample the adult U.S. population\nduring the time period germane to our research.\nMoreover, language models are themselves prod-\nucts of a speciﬁc place and time, as they are trained\non text corpora assembled largely within a speciﬁc\ntime period, and are unlikely to reﬂect the full diver-\nsity of language within that time period; OpenAI’s\nWebText corpus, for example, is collected using\noutbound web links from Reddit running through\nDecember 2017 (Radford et al., 2019).\nFinally, we note that our results are the effect\nprimarily of low frequency of observation, and it is\npossible that similar differences in tokenization and\ncontextualization based on frequency might be ob-\nserved in other sets of words which are unrelated to\ndemographics. However, frequency relates directly\nto the social dimension to which our work directs\nattention: that names of marginalized social groups\nare underrepresented - less frequent - in language\nmodeling corpora, and more likely to be overﬁt by\na language model. Frequency of representation is\none of the composite variables that inform race and\ngender in our cultural context, and must be consid-\nered when training a language model which forms\ndiffering representations based on frequency.\nWhile large-scale demographic datasets are re-\nductive, they can help to observe the social impact\nof the systems we study. Within the limited con-\nﬁnes of the groups deﬁned, our work shows that\nneural language models are ill-adapted to represent\nmarginalized social groups.\nReferences\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020. The\npushshift reddit dataset. In Proceedings of the Inter-\nnational AAAI Conference on Web and Social Media,\nvolume 14, pages 830–839.\nRishi Bommasani, Kelly Davis, and Claire Cardie.\n2020. Interpreting Pretrained Contextualized Repre-\nsentations via Reductions to Static Embeddings. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4758–\n4781, Online. Association for Computational Lin-\nguistics.\nHicham El Boukkouri, Olivier Ferret, Thomas\nLavergne, Hiroshi Noji, Pierre Zweigenbaum, and\nJunichi Tsujii. 2020. Characterbert: Reconcil-\ning elmo and bert for word-level open-vocabulary\nrepresentations from characters. arXiv preprint\narXiv:2010.10392.\nMarc-Etienne Brunet, Colleen Alkalay-Houlihan, Ash-\nton Anderson, and Richard Zemel. 2019. Under-\nstanding the origins of bias in word embeddings.\nIn International Conference on Machine Learning ,\npages 803–811. PMLR.\nJoy Buolamwini and Timnit Gebru. 2018. Gender\nshades: Intersectional accuracy disparities in com-\nmercial gender classiﬁcation. In Proceedings of\nthe 1st Conference on Fairness, Accountability and\nTransparency, volume 81 of Proceedings of Ma-\nchine Learning Research, pages 77–91, New York,\nNY , USA. PMLR.\nAylin Caliskan, Joanna J Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\n528\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nKawin Ethayarajh, David Duvenaud, and Graeme Hirst.\n2019. Understanding undesirable word embedding\nassociations. arXiv preprint arXiv:1908.06361.\nPhilip Gage. 1994. A new algorithm for data compres-\nsion. C Users Journal, 12(2):23–38.\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus. http://Skylion007.github.\nio/OpenWebTextCorpus.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2018. Frage: Frequency-agnostic\nword representation. In Advances in Neural Infor-\nmation Processing Systems, volume 31. Curran As-\nsociates, Inc.\nAnthony G Greenwald, Debbie E McGhee, and Jor-\ndan LK Schwartz. 1998. Measuring individual dif-\nferences in implicit cognition: the implicit associa-\ntion test. Journal of personality and social psychol-\nogy, 74(6):1464.\nWei Guo and Aylin Caliskan. 2021. Detecting emer-\ngent intersectional biases: Contextualized word em-\nbeddings contain a distribution of human-like biases.\nIn Proceedings of the 2021 AAAI/ACM Conference\non AI, Ethics, and Society.\nBrent L Hughes, Nicholas P Camp, Jesse Gomez,\nVaidehi S Natu, Kalanit Grill-Spector, and Jennifer L\nEberhardt. 2019. Neural adaptation to faces reveals\nracial outgroup homogeneity effects in early percep-\ntion. Proceedings of the National Academy of Sci-\nences, 116(29):14532–14537.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\net al. 2017. Google’s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\nTransactions of the Association for Computational\nLinguistics, 5:339–351.\nDouwe Kiela, Laura Rimell, Ivan Vulic, and Stephen\nClark. 2015. Exploiting image generality for lexical\nentailment detection. In Proceedings of the 53rd An-\nnual Meeting of the Association for Computational\nLinguistics (ACL 2015), pages 119–124. ACL; East\nStroudsburg, PA.\nSosuke Kobayashi. 2018. Homemade bookcorpus.\nhttps://github.com/BIGBALLON/.\nSimon Kornblith, Mohammad Norouzi, Honglak Lee,\nand Geoffrey Hinton. 2019. Similarity of neural\nnetwork representations revisited. In International\nConference on Machine Learning, pages 3519–3529.\nPMLR.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 66–75, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in contex-\ntualized word representations. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 166–172, Florence, Italy. Associ-\nation for Computational Linguistics.\nBrian Larson. 2017. Gender as a variable in natural-\nlanguage processing: Ethical considerations. In Pro-\nceedings of the First ACL Workshop on Ethics in Nat-\nural Language Processing, pages 1–11.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measur-\ning social biases in sentence encoders. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 622–628, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nAri Morcos, Maithra Raghu, and Samy Bengio. 2018.\nInsights on representational similarity in neural net-\nworks with canonical correlation. In Advances in\nNeural Information Processing Systems, volume 31.\nCurran Associates, Inc.\n529\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-\ntop: Simple and effective postprocessing for word\nrepresentations. In International Conference on\nLearning Representations.\nPandu Nayak. 2019. Understanding searches better\nthan ever before.\nMyle Ott, Michael Auli, David Grangier, and\nMarc’Aurelio Ranzato. 2018. Analyzing uncer-\ntainty in neural machine translation. In Interna-\ntional Conference on Machine Learning.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nIvan Provilkov, Dmitrii Emelianenko, and Elena V oita.\n2020. BPE-dropout: Simple and effective subword\nregularization. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1882–1892, Online. Association for\nComputational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5149–5152. IEEE.\nMaya Sen and Omar Wasow. 2016. Race as a bundle\nof sticks: Designs that estimate effects of seemingly\nimmutable characteristics. Annual Review of Politi-\ncal Science, 19:499–522.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline.\nCoRR, abs/1905.05950.\nAutumn Toney-Wails and Aylin Caliskan. 2021. Val-\nnorm quantiﬁes semantics to reveal consistent va-\nlence biases across languages and over centuries.\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nKonstantinos Tzioumis. 2018. Demographic aspects of\nﬁrst names. Scientiﬁc data, 5(1):1–9.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\nTianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-\njani, Bryan McCann, Vicente Ordonez, and Caiming\nXiong. 2020. Double-hard debias: Tailoring word\nembeddings for gender bias mitigation. In Associa-\ntion for Computational Linguistics (ACL).\nLaura Wendlandt, Jonathan K. Kummerfeld, and Rada\nMihalcea. 2018. Factors inﬂuencing the surprising\ninstability of word embeddings. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 2092–2102, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems , volume 32. Curran\nAssociates, Inc.\nKaitlyn Zhou, Kawin Ethayarajh, and Dan Jurafsky.\n2021. Frequency-based distortions in contextual-\nized word embeddings.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\n530\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 19–\n27.\n531\nA Neural Language Models\nA.1 BERT\nDevlin et al. (2019) introduce BERT, a bidirectional\ntransformer trained on masked language model-\ning (also known as the Cloze objective) and next\nsentence prediction tasks. We examine the “bert-\nbase-cased” model available from Hugging Face\nTransformers. BERT is trained on 2.5 billion words\nof English Wikipedia documents (excluding lists,\nheaders, and tables) and 800 million words of the\nBookCorpus compiled by Zhu et al. (2015), a col-\nlection of more than 11,000 free online books writ-\nten by unpublished authors.\nA.2 GPT-2\nRadford et al. (2019) introduce OpenAI GPT-2,\na unidirectional transformer model trained on the\nnext-word prediction language modeling task. We\nexamine the “gpt2” model available from Hugging\nFace Transformers. GPT-2 is trained on the Web-\nText corpus, a web scrape composed of outbound\nlinks from Reddit with at least 3 karma, which the\nmodel designers (who are also the creators of the\ntraining corpus) take as a heuristic for link quality.\nWebText contains over 8 million documents and\n40GB of text.\nA.3 XLNet\nYang et al. (2019) introduce XLNet, an autore-\ngressive transformer trained on language modeling\nwhich learns bidirectional contexts by maximizing\nthe expected likelihood over all permutations of\nits input’s factorization order. We use the “xlnet-\nbase-cased” model available from Hugging Face\nTransformers. Like BERT, XLNet is trained on En-\nglish Wikipedia and the BooksCorpus. However,\nit also trains on three additional corpora: Giga5 (a\ncorpus of print news from publications such as the\nAssociated Press and New York Times), ClueWeb\n2012-B, and Common Crawl. ClueWeb 2012-B is a\ncorpus maintained by Carnegie Mellon University.\nA.4 T5\nRaffel et al. (2020) introduce T5, a text-to-text\nencoder-decoder transformer model trained on a\nnumber of different pretraining tasks and designed\nto take in text and return text for any NLP task.\nT5 is pretrained on the Colossal Cleaned Crawled\nCorpus (C4). We examine the encoder layers of the\n’t5-base’ model available on Hugging Face Trans-\nformers.\nB Subword Tokenizers\nB.1 Byte-Pair Encoding\nGPT-2 uses a variation on Byte-Pair Encoding\n(BPE) to iteratively choose the most frequently\noccurring bigram of symbols in the training cor-\npus, merge them into a single symbol, and add the\nmerged symbol to its subword vocabulary until it\nreaches its maximum vocabulary size of 50,256.\nBPE is a compression algorithm originally pro-\nposed by Gage (1994). The technique was adapted\nfor encoding rare subwords for the purpose of neu-\nral machine translation in 2015 by Sennrich et al.\n(2016). GPT-2 modiﬁes this algorithm to oper-\nate directly on byte sequences rather than Unicode\ncharacter sequences.\nB.2 WordPiece\nBERT uses the WordPiece tokenization algorithm,\nﬁrst developed by Schuster and Nakajima (2012)\nand subsequently reﬁned by (Johnson et al., 2017)\nin Google’s neural machine translation system.\nWordPiece builds a probability distribution from\nthe training corpus using the base vocabulary of sin-\ngle character symbols, and then iteratively assem-\nbles its vocabulary by merging a bigram, adding\nthe merged unit, and replacing every instance of\nthe bigram in the training corpus with the merged\nunit. WordPiece selects the bigram which most\nincreases the likelihood of the training data when\nthe merged bigram is added to the language model,\na process which is indirectly related to frequency.\nB.3 SentencePiece\nXLNet and T5 use the SentencePiece tokenizer,\nwhich can implement either Byte-Pair Encoding\nor Unigram to form a vocabulary. SentencePiece\nwas developed by Kudo and Richardson (2018) and\nallows for lossless tokenization, such that original\ninput can be reconstructed from tokenized input.\nSentencePiece inserts an underscore to preserve\nwhitespace as a character, and can be used with\nbyte-pair encoding or with the Unigram algorithm\nalso introduced by Kudo (2018). Unlike WordPiece\nand BPE, Unigram starts with a large set of words\nand subwords generated from a training corpus,\nand iteratively removes the words which have the\nleast effect on the overall loss (with the loss func-\ntion chosen as a hyperparameter) of a language\nmodel created from the vocabulary until a ﬁxed\nvocabulary size is reached.\n532\nC First Names Dataset\nThe names dataset of Tzioumis (2018) provides the\npercentage of individuals with a ﬁrst name who\nself-identify as each of six races and ethnicities:\nWhite, Black, Hispanic, Asian-Paciﬁc-Islander, Na-\ntive American, and Mixed Race. We provide in-\nformation regarding the number of names in each\ngroup after cross-referencing for gender with the\n1990 U.S. Social Security Administration data in\nTable 8. Note that we were left with only one\npredominantly Native American name and one pre-\ndominantly Mixed Race name, which is not enough\nnames for use in our experiments.\nNames by Intersectional Group\nIntersectional Group Number of Names\nAsian Female 156\nBlack Female 37\nHispanic Female 189\nWhite Female 1,621\nAsian Male 263\nBlack Male 32\nHispanic Male 243\nWhite Male 1,216\nTable 8: Number of names in our dataset\nAfter cross-referencing between Tzioumis’ list\nand the 1990 U.S. Social Security Administration\ndata, we are left with a list of 3,757 ﬁrst names.\nWhite-majority names are more common in this\nlist, likely due both to the United States being a\nplurality-white nation, and to economic and struc-\ntural disparities which allow easier access to mort-\ngages for white individuals.",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.8320868015289307
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.6348108649253845
    },
    {
      "name": "Natural language processing",
      "score": 0.5987712144851685
    },
    {
      "name": "Representation (politics)",
      "score": 0.5706545114517212
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5699996948242188
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5517287254333496
    },
    {
      "name": "Computer science",
      "score": 0.5140575170516968
    },
    {
      "name": "Contextualization",
      "score": 0.47250545024871826
    },
    {
      "name": "Lexical analysis",
      "score": 0.4248626232147217
    },
    {
      "name": "Linguistics",
      "score": 0.4223382771015167
    },
    {
      "name": "Biology",
      "score": 0.09483838081359863
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.0
    },
    {
      "name": "Artificial neural network",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ],
  "cited_by": 26
}