{
  "title": "Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection",
  "url": "https://openalex.org/W4385567081",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2097214232",
      "name": "Chenglong Wang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A261194755",
      "name": "Yi Lu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A3120903733",
      "name": "Yongyu Mu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2104869488",
      "name": "Yimin Hu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A1983914940",
      "name": "Tong Xiao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2496766346",
      "name": "Jingbo Zhu",
      "affiliations": [
        "Northeastern University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2112663282",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W3152607317",
    "https://openalex.org/W3169554260",
    "https://openalex.org/W2944958965",
    "https://openalex.org/W3095273266",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W4214717370",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2997006708",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3173417753",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3110846353",
    "https://openalex.org/W4285198027",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2597357081",
    "https://openalex.org/W2739879705",
    "https://openalex.org/W2094387729",
    "https://openalex.org/W2949475445",
    "https://openalex.org/W4302570325",
    "https://openalex.org/W3113622461",
    "https://openalex.org/W3173580669",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3121694563",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W3200808010"
  ],
  "abstract": "Knowledge distillation addresses the problem of transferring knowledge from a teacher model to a student model.In this process, we typically have multiple types of knowledge extracted from the teacher model.The problem is to make full use of them to train the student model.Our preliminary study shows that: (1) not all of the knowledge is necessary for learning a good student model, and (2) knowledge distillation can benefit from certain knowledge at different training steps.In response to these, we propose an actor-critic approach to selecting appropriate knowledge to transfer during the process of knowledge distillation.In addition, we offer a refinement of the training algorithm to ease the computational burden.Experimental results on the GLUE datasets show that our method outperforms several strong knowledge distillation baselines significantly.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6232–6244\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nImproved Knowledge Distillation for Pre-trained Language Models via\nKnowledge Selection\nChenglong Wang1, Yi Lu1∗, Yongyu Mu 1, Yimin Hu1, Tong Xiao1,2†and Jingbo Zhu1,2\n1NLP Lab, School of Computer Science and Engineering,\nNortheastern University, Shenyang, China\n2NiuTrans Research, Shenyang, China\n{clwang1119,yilu001102}@gmail.com\n{xiaotong,zhujingbo}@mail.neu.edu.cn\nAbstract\nKnowledge distillation addresses the problem\nof transferring knowledge from a teacher model\nto a student model. In this process, we typ-\nically have multiple types of knowledge ex-\ntracted from the teacher model. The problem\nis to make full use of them to train the student\nmodel. Our preliminary study shows that: (1)\nnot all of the knowledge is necessary for learn-\ning a good student model, and (2) knowledge\ndistillation can benefit from certain knowledge\nat different training steps. In response to these,\nwe propose an actor-critic approach to selecting\nappropriate knowledge to transfer during the\nprocess of knowledge distillation. In addition,\nwe offer a refinement of the training algorithm\nto ease the computational burden. Experimen-\ntal results on the GLUE datasets show that our\nmethod outperforms several strong knowledge\ndistillation baselines significantly.\n1 Introduction\nPre-trained language models (PLMs) have signif-\nicantly advanced state-of-the-art on various natu-\nral language processing tasks, such as sentiment\nanalysis (Bataa and Wu, 2019; Baert et al., 2020),\ntext classification (Sun et al., 2019a; Arslan et al.,\n2021), and question answering (Yang et al., 2019).\nDespite the remarkable results, PLMs have a large\nnumber of parameters which make them expensive\nfor deployment (Yang et al., 2019).\nTo solve this problem, recent works resort to\nknowledge distillation (KD) (Hinton et al., 2015)\nto compress and accelerate the PLMs (Sun et al.,\n2019b; Sanh et al., 2019; Li et al., 2021b; Liang\net al., 2021). Its key idea is to transfer the knowl-\nedge from a large PLM (i.e., teacher model) into\na lightweight model (i.e., student model) without\na significant performance loss. In this process,\n*This work is done during the internship at Northeastern\nUniversity NLP Lab.\n†Corresponding author.\nwe typically have multiple types of knowledge ex-\ntracted from the teacher model, such as response\nknowledge, feature knowledge, and relation knowl-\nedge (Gou et al., 2021). Recent works mainly fo-\ncus on how the student model learns the transferred\nknowledge more efficiently, such as designing train-\ning schemes (Sun et al., 2019b; Mirzadeh et al.,\n2020; Jafari et al., 2021; Li et al., 2021a) or enrich-\ning task-specific data (Jiao et al., 2020; Liang et al.,\n2021). However, few works consider how to make\nfull use of the multiple types of knowledge into the\ntraining of the student model.\nOur preliminary study (see Section 4.1) shows\nthat not all of the knowledge is necessary for learn-\ning a good student model when conducting distilla-\ntion with diverse knowledge. Furthermore, inspired\nby dynamic KD (Li et al., 2021b), we assume that\nlearning from certain knowledge at different train-\ning steps is beneficial to KD. We conduct probing\nexperiments to verify this assumption in Section\n4.2. Specifically, we repeat KD 200 times and ran-\ndomly select knowledge at each training step. We\nfind that different distilled student models have a\ndistinct performance. For example, the best stu-\ndent model is nearly 10% higher than the worst\none in terms of accuracy on the RTE dataset. In\naddition, it notably exceeds the student model that\nlearns from fixed knowledge. Based on our pre-\nliminary study, we have the following suggestion:\nthe distilled student model could achieve superior\nperformance if it learns appropriate knowledge at\neach training step. This suggestion motivates us to\ninvestigate how to select the appropriate knowledge\nfor the student model during the process of KD.\nBased on the above findings, we propose\nan actor-critic approach to selecting appropriate\nknowledge to transfer at different training steps\n(Bhatnagar et al., 2009). This approach first uses\nan actor-critic algorithm to implement a knowledge\nselection module via a long-term reward optimiza-\ntion. This optimization can consider the influence\n6232\nof knowledge selection on future training steps.\nFurthermore, we develop a multi-phase training\napproach that divides the distillation process into\nmany phases and provides a particular reward at\nthe phase end. Compared to the usual actor-critic\nalgorithm, it can ease the burden of computing re-\nwards. After that, we perform KD by employing\nthe trained knowledge selection module to select\nknowledge at each training step.\nWe test the proposed approach on six GLUE\ndatasets (Wang et al., 2019) which involve question\nanswering, sentiment analysis, and textual entail-\nment. Experimental results show that our method\nsignificantly outperforms the vanilla KD method\n(Hinton et al., 2015) and other competitive KD\nmethods. Notably, results also show our BERT6 (6-\nlayer BERT) student model can achieve more than\n98.5% of the performance of teacher BERT BASE\nwhile keeping much fewer parameters (∼61%). In\naddition, we prove that when armed with data aug-\nmentation, our approach can yield further improve-\nments and be superior to TinyBERT (Jiao et al.,\n2020), leading to a strong baseline with the data\naugmentation.\n2 Related Work\nKnowledge distillation (KD) (Hinton et al., 2015)\nis widely used to compress and accelerate the pre-\ntrained language models (PLMs) (Jiao et al., 2020;\nSun et al., 2020; Wang et al., 2020; Li et al., 2021b).\nIts core idea is to transfer the knowledge from a\nlarge PLM (i.e., teacher model) into a lightweight\nmodel (i.e., student model). Recent works on KD\ncould be classified into three groups. The first\ngroup focused on utilizing various knowledge from\nthe teacher model to distill the student model. For\nexample, multiple teacher models are leveraged to\nprovide a more diverse and accurate knowledge for\nthe student model (Liu et al., 2019a; Yuan et al.,\n2021). Sun et al. (2019b) exploited knowledge\nfrom intermediate layers of the teacher model dur-\ning distillation. Moreover, Weight Distillation (Lin\net al., 2021) transfers the knowledge in parameters\nof the teacher model to the student model. The sec-\nond group tended to design effective strategies to\nfacilitate the student model to learn the knowledge\nfrom different types of knowledge, such as enrich-\ning the task-specific data (Jiao et al., 2020; Liang\net al., 2021) and the two-stage learning framework\n(Turc et al., 2019; Jiao et al., 2020). The third group\nthat has attracted less attention generally explored\nappropriate data and teacher models for student\nmodels in the KD. For example, Li et al. (2021b)\nproposed a data selection strategy that only selects\nsome vital data for the student model according to\nits competency. Yuan et al. (2021) attempted to\nadjust the weights to the multiple teacher models\nduring distillation.\nDifferent from these methods, in this paper, we\nare concerned with how to select appropriate knowl-\nedge for student models during the process of KD.\nTo this end, we train an effective knowledge selec-\ntion model via an actor-critic algorithm and em-\nploy it to select appropriate knowledge to transfer\nat each training step.\n3 Background\n3.1 Knowledge Distillation\nIn KD, the student model learns knowledge from\nthe teacher model by mimicking corresponding be-\nhaviors. In the distillation process, this mimicking\ncan be implemented by minimizing the following\nloss function:\nLKD=\n∑\nx∈X\nLdiff(fS(x),fT(x)) (1)\nwhere Xis the training dataset, xis an input sam-\nple, fS(·) and fT(·) are the functions of describing\nthe behaviors of the student model and the teacher\nmodel, respectively. Ldiff(·) is a loss function that\nevaluates the difference between their behaviors.\nIn the process of KD, due to many behaviors (e.g.,\nextracting features) existing in the student and the\nteacher model, we typically have multiple types\nof knowledge. Previous works mainly focus on\ndesigning elaborate fS(·), fT(·), and Ldiff(·) to\nencourage the student model to learn certain fixed\nknowledge better (Sun et al., 2019b; Jiao et al.,\n2020; Liang et al., 2021). Unlike them, in this\nwork, we make full use of the multiple types of\nknowledge and select appropriate knowledge for\nthe student model during the process of KD.\n3.2 Knowledge Types\nThe student model can learn from various of knowl-\nedge extracted from the teacher model in KD. Ac-\ncording to Gou et al. (2021), the types of knowl-\nedge can often be Response Knowledge (ResK) ,\nFeature Knowledge (FeaK), and Relation Knowl-\nedge (RelK). In addition, we consider a Finetune\nKnowledge (FinK)derived from the training dataset\non compressing PLMs. The overview of these\nknowledge types is following:\n6233\nKnowledge RTE SST-2\nBERT6 BERT3 BERT6 BERT3\nFinK 65.8 58.1 89.7 88.0\nResK 67.1 56.8 90.2 87.5\nFeaK 66.1 57.8 90.4 87.1\nRelK 64.6 58.9 88.9 88.2\nFinK&ResK 65.2 55.6 90.0 88.3\nFinK&ResK&FeaK 64.6 56.7 90.0 87.0\nFinK&ResK&FeaK&RelK 64.5 57.490.7 87.7\nTable 1: Accuracies (%) of distilled student models\non the RTE and SST-2 development sets. The teacher\nmodel is the BERTBASE . The student models are the 6-\nlayer BERT6 and the 3-layer BERT3, respectively. Note\nthat the knowledge is fixed during the process of KD.\n• ResK: ResK refers to the knowledge that the\nstudent model learns by mimicking the neu-\nral response of the last layer of the teacher\nmodel. It includes more information about the\npredicted results (Hinton et al., 2015).\n• FeaK: FeaK denotes the feature representa-\ntion that the student model learns from the out-\nputs of the intermediate layers of the teacher\nmodel. Different from ResK, it contains more\ninformation about the calculation process of\nthe teacher model.\n• RelK: RelK represents the relationships be-\ntween different layers or samples (Yim et al.,\n2017). It focuses more on the relationship\nbetween model layers.\n• FinK: FinK is the knowledge that the student\nmodel learns from ground-truth labels by fine-\ntuning on the training dataset (Hinton et al.,\n2015; Devlin et al., 2019). Here, we also con-\nsider it as a knowledge type in KD.\nIn this work, we explore how to select appropri-\nate knowledge from these types to transfer during\nthe process of KD. To ensure transferring the above\ntypes of knowledge to the student model, we design\nloss functions LResK, LFeaK, LRelK, and LFinK,\nrespectively. The Appendix A presents the design\ndetails of loss functions.\n4 Preliminary Study\nDue to its unique information and character, we\nassume that each type of knowledge has a different\nimpact on KD results. Based on this, we conduct\npreliminary studies to probe the relationship be-\ntween knowledge and KD.\n4.1 Different Knowledge for KD\nWe conduct an experiment using different knowl-\nedge to distill the student models individually. In\nTask Method Sbest Sworst ∆\nRTE\nRandom-All 68.4 58.3 10.1\nRandom-One 67.2 58.9 8.7\nFixed-All 67.1 64.5 2.6\nSST-2\nRandom-All 91.3 86.1 5.2\nRandom-One 90.7 87.9 2.8\nFixed-All 90.7 88.9 1.8\nTable 2: Accuracies (%) of Sbest and Sworst on the de-\nvelopment sets. Fixed-All denotes that we use the same\nknowledge to train the student model at each training\nstep. Its results are taken from Table 1.\nthe experiment, we tune the weights of each type of\nknowledge on the dev set, similar to tuning hyper-\nparameters. Table 1 compares these distilled stu-\ndent models. We can see that the student models\nhave noticeable gaps in terms of accuracy while\nusing different knowledge. It indicates that not all\nknowledge is necessary for learning a good student\nmodel. Interestingly, we can also see that the ap-\npropriate knowledge changes as the student model\nor the dataset changes. We conjecture that factors\nsuch as student capacity and sample complexity\ncould affect transferring knowledge, which is con-\nsistent with Stanton et al. (2021)’s findings.\n4.2 Different Knowledge at Training Steps\nInspired by dynamic KD (Li et al., 2021b), we as-\nsume that the appropriate knowledge may change\ndynamically because the student model and the\nsample keep changing during training. We con-\nduct probe experiments to verify this assumption.\nWe randomly select knowledge for the student\nmodel at each training step. Here, we employ\nthis random strategy for the training steps of all\nepochs (Random-All) and the training steps of the\nfirst epoch (Random-One), respectively. To make\nexperiments more general, both Random-All and\nRandom-One are repeated 200 times.\nWe report the best and worst performance of\nthe distilled student models denoted by Sbest and\nSworst in Table 2. From the results of Random-All,\nwe can see that there is a conspicuous gap in ac-\ncuracy scores between Sbest and Sworst. Notably,\nSbest performs significantly better than Sworst by\n10% on the RTE dataset. We conjecture that Sbest\nmeans the student model is trained with the ap-\npropriate knowledge at most of the training steps,\nwhile Sworst is the opposite. In addition, we notice\nthat Sbest achieves 68.4% accuracy, which signif-\nicantly surpasses all student models with a fixed\nknowledge. It indicates that KD can benefit from\ncertain knowledge at different training steps. This\n6234\nalso motivates us to investigate the method for se-\nlecting appropriate knowledge.\nFor the results of Random-One, there is also a\nremarkable performance difference between Sbest\nand Sworst, though only the training steps of the\nfirst epoch in KD involves random selection of\nknowledge. It shows that knowledge selection at\nthe current training step affects the following train-\ning steps. Therefore, we should consider the future\ninfluence while selecting appropriate knowledge.\n5 Method\n5.1 Overview\nIn this work, our goal is to select appropriate knowl-\nedge to transfer during the process of KD. Consider-\ning the current selection may affect future training\nsteps, we propose the actor-critic approach, which\naddresses the knowledge selection problem via a\nlong-term reward optimization. Figure 1 gives an\noverview of our method. Our method involves\ntwo stages: training a knowledge selection mod-\nule (KSM) and distilling the student model with\nthe trained KSM. In the first stage, we use an actor-\ncritic algorithm consisting of an actor and a critic to\nimplement KSM. The actor outputs an action that\nselects knowledge based on a given state. The critic\nnetwork predicts the action value, i.e., the sum of\nfuture rewards. In the second stage, we employ the\ntrained KSM to select appropriate knowledge and\ntransfer them to the student model.\n5.2 Definitions\nIn this work, we use an actor-critic algorithm to im-\nplement the KSM. Here, we first describe some key\nconcepts of the actor-critic algorithm, including\nstate, action, and reward.\nState. At training step t, we use st to denote the\ncorresponding state. Here, st should comprise suf-\nficient information for selecting appropriate knowl-\nedge. To this end, we use the informative [CLS]\nembeddings of the last layers in the student and\nthe teacher models (Devlin et al., 2019) to achieve\nit. Specifically, for the i-th sample at training step\nt, we use cS\ni and cT\ni to denote the [CLS] embed-\nding of the last layer of the student and the teacher\nmodel, respectively. Then we utilize two trainable\nfeature networks, NS\nfea and NT\nfea, to extract the\nuseful feature vector v:\nvS\ni = NS\nfea(cS\ni), vT\ni = NT\nfea(cT\ni ) (2)\nwhere both NS\nfea and NT\nfea consist of a 2-layer\nmulti-layer perceptron (MLP) network. We con-\ncatenate all extracted feature vectors in given batch\nas st.\nAction. Given the state st, an action at can be\ncreated, which is used to select the appropriate\nknowledge to transfer at training step t. Here, we\npresent soft and hard actions to select appropriate\nknowledge subtly. The soft action determines how\nmuch to learn from each knowledge type, while the\nhard action selects one or more knowledge types\nfor training the student model.\nReward. Here we define the immediate reward\nrt as the cross-entropy loss difference on a devel-\nopment set after the student model is trained with\nselected knowledge. See Table 6 in Appendix for a\ncomparison of different rewards.\n5.3 Knowledge Selection Module\nTo improve the performance of the distillation\nmodel, we need to select the appropriate knowl-\nedge for transferring at different training steps. For\nthis purpose, we implement a knowledge selec-\ntion module via the actor-critic algorithm which\nconsists of an actor and a critic. Here, we utilize\nlong-term rewards to optimize it to consider the\ninfluence of knowledge selection on future training\nsteps.\n5.3.1 Actor and Critic\nActor. In this work, the actor network µθ is com-\nposed of a three-layer MLP with four output neu-\nrons. It takes a state st and outputs the soft action\nwith a Sigmoid function:\nat = Sigmoid(µθ(st)) (3)\nwhere at = {at\n1,at\n2,at\n3,at\n4}contains four real val-\nues belonging to [0,1]. We use at\nm in at to denote\nthe percentage of the selected m-th type of knowl-\nedge*. At training step t, the student model’s loss\nis calculated as follows:\nLsoft\nt = at\n1LFinK+at\n2LResK+at\n3LFeaK\n+at\n4LRelK\n(4)\nIn addition, to directly select one or more types of\nknowledge, we obtain the hard action via a condi-\ntional function g(·):\ng(at\nm) =\n{\n1 atm≥λ\n0 otherwise (5)\n*Here, we treat FinK, ResK, FeaK, and RelK as 1-th, 2-th,\n3-th, and 4-th type of knowledge.\n6235\nStudent Feature\nNetwork\nTeacher Feature\nNetwork\n……\nS\n1c\nS\n2c\nS\nnc\nT\n1c\nT\n2c\nT\nnc ……\nActor\nCriticAction\nAction Value\n(a) the architecture of the KSM\nDifferent Knowledge\nTransfer\nInput\nEncoder \nBlock\n……\nEncoder \nBlock\nOutput\nStudent ModelSelected Knowledge\nKSM\nFinetune Knowledge\nResponse Knowledge\nFeature Knowledge\nRelation Knowledge\nFinetune Knowledge\nResponse Knowledge\nFeature Knowledge\nRelation Knowledge (b) selecting appropriate knowledge for KD via the KSM\nFigure 1: An overview of the proposed method. We use an actor-critic approach to design a knowledge selection\nmodule (KSM), which aims to select the appropriate knowledge to transfer during KD. Besides the soft action\npresented in Figure 1(b), we also design the hard action for the actor module (see Section 5.3.1).\nwhere λ∈[0,1] is a threshold value. The student\nmodel’s loss with the hard action is as follows:\nLhard\nt = g(at\n1)LFinK+g(at\n2)LResK+\ng(at\n3)LFeaK+g(at\n4)LRelK\n(6)\nHere if at\nm ≥λ, m-th type of knowledge is ap-\npropriate for the student model; otherwise, it is\nnot.\nCritic. The critic network Qϕ is also composed\nof a three-layer MLP. It computes the action value\nQϕ(st,at) of a given pair (st,at), where st and at\nare concatenated as the input. The action value is\nan estimation of the sum of rewards earned after the\nactor takes the action at at the state st. Here, we\ndefine the action value as the long-term reward,i.e.,\nthe performance gain of the student model from the\ntraining step tto the training end.\n5.3.2 Optimization\nActor Optimization. Following the deep deter-\nministic policy gradient algorithm (Silver et al.,\n2014), we optimize the long-term reward by up-\ndating the actor parameters via the sampled policy\ngradient:\n∇θJ≈1\nN\nN∑\nt=1\n∇θQϕ(st,at) (7)\nCritic Optimization. At each training step, we\ncan only gain an immediate reward rt. How-\never, the critic provides a long-term reward rather\nthan an immediate reward. Therefore, we employ\nTemporal-Difference (Tesauro, 1991) to approxi-\nmate the actual long-term reward with rt:\nQ∗\nϕ(st,at) =γtrt+Qϕ(st+1,at+1) (8)\nwhere γ is a discount factor (Sutton and Barto,\n2018). Then we can give the optimization objective\nof the critic network through the approximate long-\nterm reward:\nLQ= 1\nN\nN∑\nt=1\nMSE(Qϕ(st,at),Q∗\nϕ(st,at)) (9)\nwhere MSE(·) is the mean squared error loss func-\ntion, and N is the number of training steps in KD.\nFeature Networks Optimization. During train-\ning KSM, we update the student feature network\nwith the actor network and the teacher feature net-\nwork with the critic network, respectively, in order\nto reduce the instability of training KSM.\n5.3.3 Multi-Phase Training\nDue to a large number of training steps in some\ndatasets, computing the reward via the loss on the\ndevelopment set can yield considerable costs dur-\ning training the KSM. For example, training with\n5 epochs on the QQP dataset needs to compute\nthe reward about 50,000 times in an episode. To\nease the computational burden, we propose a multi-\nphase training approach to train the KSM. Specif-\nically, we divide the complete KD process into\nmany phases where each phase contains ktraining\nsteps. After the student model is trained on a phase,\nwe treat the cross-entropy loss difference on a de-\nvelopment set as the phase reward rp. It expresses\nthe sum of all rewards in a phase. Here we can\nuse a phase optimization objective Lp\nQ to train the\ncritic network instead of the LQ:\nLp\nQ= 1\nNp\nNp∑\nj=1\nMSE(rp\nj,\ne(j)∑\nt=b(j)\nˆrt) (10)\nwhere Np is the number of phases in KD, b(j) and\ne(j) are the beginning and end training steps in the\nj-th phase. The ˆrt is an estimated reward computed\nby Eq. 8:\nˆrt = Qϕ(at,st) −Qϕ(at+1,st+1)\nγt (11)\n6236\nAlgorithm 1 Our Method\nInput: the well-trained teacher model MT; the initial stu-\ndent model MS; training dataset X\nOutput: the distilled MS;\n1: %%% the first stage\n2: Train the KSM via Algorithm 2;\n3:\n4: %%% the second stage\n5: for t= 1to N do\n6: compute a state st via Eq. 2;\n7: use an action at = µθ(st) to select appropriate\nknowledge;\n8: train MS via Eq. 4 or Eq. 6;\n9: end for\n10: return MS\nAlgorithm 2 Training of the KSM\nInput: the well-trained teacher model MT; the initial stu-\ndent model MS; training dataset X; the initial KSM (an\nactor µθ, a critic Qϕ, two feature networks NS\nfea and\nNT\nfea)\nOutput: the trained KSM\n1: for episode= 1to Kdo\n2: reset MS;\n3: divide the whole KD process into Np phases;\n4: for j = 1to Np do\n5: for t= b(j) to e(j) do\n6: compute a state st via Eq. 2;\n7: sample an action at = µθ(st);\n8: utilize the actions in the previous phases to\ncompute re\nt via Eq. 12 or Eq. 13;\n9: compute an action value Qϕ(s,a);\n10: train MS via Eq. 4 or Eq. 6;\n11: end for\n12: compute rp\nj via the loss difference;\n13: train the KSM via Eq. 9 and Eq. 10;\n14: end for\n15: end for\n16: return KSM\nWith the help of dividing phases, we can effec-\ntively relieve the computational burden. Generally,\nthe conventional training of the actor-critic algo-\nrithm needs to compute rewards N times, while\nour multi-phase training only calculates rewards\n⌈N/k⌉times.\n5.3.4 Exploration Reward\nTo speed up the training of the KSM, we also design\nan exploration reward re\nt at training step t, which\nencourages the actor to take more different actions\n(Tang et al., 2017). For the soft action, we consider\nthe similarity between actions as re\nt:\nre\nt = α×(1−1\nk\nk∑\nl=1\nSim(at,al)) (12)\nwhere Sim(·) is a cosine similarity function, andal\nis the l-th action in the previous phase. In addition,\nwe calculate exploration reward re\nt for hard action\nbased on its repetition:\nre\nt = α×(1−count(at)\nk ) (13)\nwhere count(at) is the number of times the action\nat was taken in the previous phase, and αis a scale\nfactor. We provide the critic network with an addi-\ntional optimization objective Le\nQ via re\nt computed\nby Eq. 9.\n5.4 Knowledge Distillation\nAlgorithm 1 presents our overall method. In the\nfirst stage, we train a KSM with an actor-critic\nalgorithm. As described in Algorithm 2, we first\ndivide the complete KD process into Np phases\nin an episode (line 3). At each training step in\na phase, we compute a state, an action, and an\nexploration reward (lines 6-9) and train the student\nmodel with selected knowledge (line 10). Then,\nwe compute the phase reward when the phase ends.\nSubsequently, we optimize KSM via the Le\nQ and\nLp\nQ loss objectives computed by the Eq. 9 and\nEq. 10, respectively (line 13). This process iterates\non episodes until the performance of the student\nmodel converges. In the second stage, we adopt the\ntrained KSM to select appropriate knowledge that\nis used to distill the student model.\n6 Experiments\n6.1 Datasets and Settings\nDatasets. Following Sun et al. (2019b), we con-\nduct experiments on six GLUE datasets (Wang\net al., 2019): MNLI (Williams et al., 2018), QQP\n(Chen et al., 2018), QNLI (Rajpurkar et al., 2016),\nSST-2 (Socher et al., 2013), MRPC (Dolan and\nBrockett, 2005), and RTE (Bentivogli et al., 2009).\nSettings. We use BERTBASE as the teacher model\nand the BERT6 and BERT3 as the student models.\nThe details of training settings are shown in Ap-\npendix B. All experiments are repeated three times,\nand we report the average results over three runs\nwith different seeds.\n6.2 Baselines\nWe compare the proposed method with finetune\n(Devlin et al., 2019), vanilla KD (Hinton et al.,\n2015), and other competitive pre-trained model KD\nmethods such as patient knowledge distillation\n(PKD) (Sun et al., 2019b),DistilBERT (Sanh et al.,\n2019), and Dynamic KD (Li et al., 2021b).\nIn addition, we also design Random-Soft and\nRandom-Hard baselines to evaluate the effective-\nness of our method. Random-Soft and Random-\n6237\nMethod Student #Params RTE\n(2.5k)\nMRPC\n(3.7k)\nMNLI-m/mm\n(393k)\nSST-2\n(67k)\nQNLI\n(105k)\nQQP\n(364k) Avg.\nBERTBASE(Teacher) - 109.0M 69.3 87.6/83.5 84.1/83.1 94.3 90.5 71.0/89.2 84.9\nPKD BERT6 67.0M 65.5 85.0/ 79.9 81.5/81.0 92.0 89.0 70.7/88.9 82.5\nDynamic KD BERT6 67.0M - 86.5/- 81.8/81.0 - - - -\nDistilBERT BERT6 67.0M 58.4 86.9/- 82.6/81.3 92.5 88.9 70.1/- -\nFinetune BERT6 67.0M 65.2 85.1/79.2 81.1/79.8 91.7 87.1 69.4/88.2 81.8\nVanilla KD BERT6 67.0M 65.1 85.5/79.8 82.4/81.6 91.4 86.9 70.0/88.4 82.2\nRandom-Hard BERT6 67.0M 65.1 85.2/79.0 82.1/81.3 91.2 86.7 69.8/88.3 82.0\nRandom-Soft BERT6 67.0M 64.8 85.0/79.2 82.2/81.3 91.5 86.5 69.7/88.1 81.9\nOur method (Hard Action)BERT6 67.0M 66.6 87.7 /82.2 82.6/81.8 92.1 89.0 70.5/88.9 83.3\nOur method (Soft Action)BERT6 67.0M 66.8 87.9/82.2 83.1/82.1 92.6 89.3 71.1/89.1 83.6\nPKD BERT3 45.7M 58.2 80.7/ 72.5 76.7/76.3 87.5 84.7 68.1 /87.8 77.7\nRandom-Hard BERT3 45.7M 54.3 80.7/71.8 76.8/76.1 87.4 82.9 66.4/86.4 76.5\nRandom-Soft BERT3 45.7M 55.7 78.9/69.7 77.1/76.3 87.7 82.3 66.4/86.5 76.5\nOur method (Hard Action)BERT3 45.7M 58.4 81.4 /71.5 77.5/76.8 88.4 84.6 68.1/88.0 77.9\nOur method (Soft Action)BERT3 45.7M 58.7 81.9/72.7 78.0/77.3 88.3 85.1 68.7/88.2 78.3\nTable 3: Results from the GLUE test server. The best and second-best results for each group of student models\nare in bold and in italics, respectively. The numbers under each dataset indicate the corresponding number of the\ntraining dataset. For MRPC and QQP, we report F1/Accuracy. We also report the average accuracy for each dataset\nin the “Avg.” column. The results for Dynamic KD are achieved via the Uncertainty-Entropy strategy (Li et al.,\n2021b). The results of DistilBERT and PKD are taken from Jiao et al. (2020) and Sun et al. (2019b), respectively.\nHard denote that the KSM randomly takes a soft\nand hard action, respectively.\n6.3 Main Results\nWe submit our model predictions to the official\nGLUE evaluation server, and the results are summa-\nrized in Table 3. First, compared with all baselines,\nour method can achieve optimal results while dis-\ntilling the student models BERT6 and BERT3 on all\nthe datasets. Second, compared with the Random-\nHard (or Random-soft), we can observe that our\nmethod achieves significant improvements in the\nF1 and the accuracy scores. It demonstrates that\nthe KSM system outperforms the baselines that ran-\ndomly select knowledge to use. Third, the BERT6\nstudent model distilled by our method with soft\naction significantly outperforms the popular vanilla\nKD by a margin of at least 1.4%. We attribute this\nto the fact that KD benefits from selected knowl-\nedge at different training steps. Fourth, our method\nachieves the BERT6 student model with ∼61% pa-\nrameters and achieves a similar performance com-\npared with the teacher model BERT BASE . For ex-\nample, on the QQP dataset, the BERT 6 student\nmodel trained by our method with soft action has\n89.1% accuracy, which only is 0.1% away from\nthe teacher model. In addition, compared with\nthe teacher model BERTBASE , the BERT3 student\nmodel has only 41% parameters while maintaining\n92% performance.\nFurthermore, we further investigate the perfor-\nmance gain on two different actions. From the\n/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017/uni00000014/uni00000019/uni00000014/uni0000001b/uni00000015/uni00000013/uni00000015/uni00000015/uni00000015/uni00000017/uni00000015/uni00000019\n/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048\n/uni00000019/uni00000015\n/uni00000019/uni00000016\n/uni00000019/uni00000017\n/uni00000019/uni00000018\n/uni00000019/uni00000019\n/uni00000019/uni0000001a\n/uni00000019/uni0000001b\n/uni00000019/uni0000001c/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000032/uni00000058/uni00000055/uni00000003/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000003/uni0000000b/uni00000036/uni00000052/uni00000049/uni00000057/uni00000003/uni00000024/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000010/uni00000033/uni0000004b/uni00000044/uni00000056/uni00000048/uni00000003/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000028/uni0000005b/uni00000053/uni0000004f/uni00000052/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047\nFigure 2: Ablation study. We plot the mean accuracy\nof student models distilled with three different seeds on\nthe RTE development set.\nresults, we can find that the soft action performs\nbetter than the hard action, though both can con-\ntribute to our method to achieve an improvement\nover baselines on most of the datasets. One poten-\ntial explanation might be that the soft action space\nis larger than the hard action space. Therefore,\nthe soft action is more likely to explore a better\nknowledge selection than the hard action.\n6.4 Ablation Study\nWe conduct an ablation study to explore the effects\nof the proposed multi-phase training and explo-\nration reward on accuracy and efficiency. Figure\n2 shows the accuracy of student models distilled\nwith the KSM on the RTE development set after re-\nmoving multi-phase training or exploration reward.\nWe can see that using the multi-phase training can\ntrain a good KSM more likely, though the number\n6238\n/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017/uni00000014/uni00000019/uni00000014/uni0000001b/uni00000015/uni00000013/uni00000015/uni00000015/uni00000015/uni00000017/uni00000015/uni00000019\n/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048\n/uni00000019/uni00000015\n/uni00000019/uni00000016\n/uni00000019/uni00000017\n/uni00000019/uni00000018\n/uni00000019/uni00000019\n/uni00000019/uni0000001a\n/uni00000019/uni0000001b\n/uni00000019/uni0000001c/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000039/uni00000044/uni00000051/uni0000004c/uni0000004f/uni0000004f/uni00000044/uni00000003/uni0000002e/uni00000027\n/uni00000032/uni00000058/uni00000055/uni00000003/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000003/uni0000000b/uni00000036/uni00000052/uni00000049/uni00000057/uni00000003/uni00000024/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c\n/uni00000032/uni00000058/uni00000055/uni00000003/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000003/uni0000000b/uni0000002b/uni00000044/uni00000055/uni00000047/uni00000003/uni00000024/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c\nFigure 3: Comparison of the soft and hard actions. Our\nmethod can achieve faster KSM learning with the hard\naction and gain better results with the soft action.\nMethod RTE\n(52k)\nMRPC\n(80k)\nSST-2\n(1,084k)\nQNLI\n(2,237k)\nFinetune 64.8 86.6/81.3 81.8 84.0\nVanilla KD 67.3 88.0/82.8 91.2 87.0\nTinyBERT 70.0 87.3/82.6 93.1 90.4\nOur Method 70.4 88.9/83.9 93.8 90.8\nTable 4: Results of our method with the augmented\ndatasets from the GLUE test server. The results of\nTinyBERT are taken from Jiao et al. (2020).\nof offering a reward is reduced. We conjecture that\nthe underlying reason is that the phase reward may\nbe more accurate and stable than the immediate\nreward at each training step. In addition, we can\nalso observe that without the exploration reward,\nthe KSM fails to explore the better knowledge se-\nlection more quickly.\n6.5 Discussion\nPerformance on Data Augmentation Although\nthe trained KSM can select the appropriate knowl-\nedge, the use of the fewer samples in the training\ndataset fail to provide the opportunity for the stu-\ndent model to learn them (Jiao et al., 2020; Liang\net al., 2021). In such cases, we assume that if armed\nwith the data augmentation, the student model\ncould learn the knowledge assigned by KSM more\nsufficiently, thus achieving better performance. To\nthis end, we augment the training datasets ( i.e.,\nRTE, MRPC, SST-2, and QNLI datasets) about 20\ntimes via the data augmentation procedure (Jiao\net al., 2020). To make a fair comparison, we ini-\ntialize our student model with the released BERT6†\ndistilled via General Distillation (Jiao et al., 2020).\nTable 4 compares our student model (soft action)\nwith vanilla KD, finetune, and TinyBERT.\n†https://huggingface.co/huawei-noah/TinyBERT_\nGeneral_6L_768D\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000036/uni00000057/uni00000058/uni00000047/uni00000048/uni00000051/uni00000057/uni00000003/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055\n/uni00000019/uni00000018/uni00000011/uni00000013\n/uni00000019/uni0000001a/uni00000011/uni00000018\n/uni0000001a/uni00000013/uni00000011/uni00000013\n/uni0000001a/uni00000015/uni00000011/uni00000018\n/uni0000001a/uni00000018/uni00000011/uni00000013\n/uni0000001a/uni0000001a/uni00000011/uni00000018\n/uni0000001b/uni00000013/uni00000011/uni00000013\n/uni0000001b/uni00000015/uni00000011/uni00000018\n/uni0000001b/uni00000018/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000057/uni00000058/uni00000051/uni00000048\n/uni00000039/uni00000044/uni00000051/uni0000004c/uni0000004f/uni0000004f/uni00000044/uni00000003/uni0000002e/uni00000027\n/uni00000032/uni00000058/uni00000055/uni00000003/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000003/uni0000000b/uni00000036/uni00000052/uni00000049/uni00000057/uni00000003/uni00000024/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c\nFigure 4: Performance of distilled student model with\nvarious layers on the MRPC development set.\nThe results show that our method is consistently\nbetter than all baselines. It demonstrates that data\naugmentation can significantly improve the student\nmodels distilled by our method. In addition, com-\npared to the vanilla KD, our method can take ad-\nvantage of the data more efficiently and transfer\nmore knowledge to the student model.\nThis performance on data augmentation empha-\nsizes that our method is orthogonal to the Tiny-\nBERT, as we address the knowledge selection prob-\nlem during KD. It also provides a suggestion that\nour method can be complementary to other KD\nmethods, such as the MixKD (Liang et al., 2021)\nand MiniLM (Wang et al., 2020).\nComparison of Soft Action and Hard Action\nTo intuitively present the effect of different actions\non performance, we compare the soft and hard\nactions on the RTE dataset. Figure 3 shows the\nmean accuracy of the student models distilled with\ndifferent action strategies on the development set.\nIt can be found that our approach can integrate\nseamlessly with different strategies. Furthermore,\nwe notice that the hard action can achieve faster\nKSM learning, while the soft action can achieve\nbetter results. We can draw similar observations\non the results of other datasets, e.g., on a relatively\nlarger dataset QQP (see Figure 5 in Appendix).\nPerformance on Different Student Models To\nexplore the performance of different student mod-\nels, we distill student models with different layers\nand plot the performance to compare baselines (i.e.,\nfinetune and vanilla KD) in Figure 4. The results\nshow that our method consistently outperforms the\nbaselines while distilling the student model with\nvarious layers. It indicates that our method can\nbe adapted to the distillation of different student\nmodels well.\n6239\nA Regularization Perspective on Knowledge Se-\nlection In practice, we consider that the knowl-\nedge selection can act as a regularization which pre-\nvents the co-adaptation (Grisogono, 2006; Sabiri\net al., 2022) in KD, i.e., distilling a student model\nhighly depends on a certain behavior of the teacher.\nIf the distilled student model receives the inap-\npropriate knowledge from the dependent behavior\nof the teacher, it can significantly alter the perfor-\nmance of the student model, which is what might\nhappen with overfitting (Hawkins, 2004; Phaisan-\ngittisagul, 2016). However, knowledge selection\nallows the student model to learn from multiple\ndifferent behaviors properly at each training step\nduring KD. Thus, compared to the traditional KD\nmethods that distill a student model via single or\nfixed knowledge, our method can ease the effect\nfrom the inappropriate knowledge from a behavior\nto prevent the co-adaptation.\nSee more discussion in Appendix C.\n7 Conclusion\nIn this paper, we focus on making full use of the\nmultiple types of knowledge into the distilling of\nthe student model. We have proposed an actor-\ncritic approach to selecting appropriate knowledge\nto transfer during the process of knowledge distil-\nlation. To ease the burden of computing rewards\nduring training, we propose a multi-phase training\napproach and an exploration reward. Our exper-\niments on GLUE datasets show that our method\nsignificantly outperforms several strong knowledge\ndistillation baselines.\nLimitations\nIn this section, we discuss some limitations of this\nwork as follows:\n• We train a model to select appropriate knowl-\nedge to transfer during the process of knowl-\nedge distillation . In this process, training\nthe model is time-consuming and resource-\nintensive. For example, on the RTE dataset,\nif we utilize one TITAN V GPU and set the\ndistillation epoch to 5, it will take 40 minutes\nto train the model.\n• It is difficult to scale the proposed approach\nto a large dataset. The underlying reasons are:\n(i) More training steps in the supersize dataset\nwill make the knowledge selection question\nmore difficult. ( ii) The larger dataset, the\nstate is more complex, which may confuse\nthe knowledge selection module.\nAcknowledgements\nThis work was supported in part by the National\nScience Foundation of China (Nos. 61876035\nand 61732005), the China HTRD Center Project\n(No. 2020AAA0107904), Yunnan Provincial Ma-\njor Science and Technology Special Plan Projects\n(Nos. 202002AD080001 and 202103AA080015),\nNational Frontiers Science Center for Industrial In-\ntelligence and Systems Optimization (Northeastern\nUniversity, China. No. B16009) and the Funda-\nmental Research Funds for the Central Universities.\nWe also thank anonymous reviewers for valuable\nfeedback.\nReferences\nYusuf Arslan, Kevin Allix, Lisa Veiber, Cedric Lothritz,\nTegawendé F Bissyandé, Jacques Klein, and Anne\nGoujon. 2021. A comparison of pre-trained language\nmodels for multi-class text classification in the finan-\ncial domain. In Proceedings of WWW.\nGaétan Baert, Souhir Gahbiche, Guillaume Gadek, and\nAlexandre Pauchet. 2020. Arabizi language models\nfor sentiment analysis. In Proceedings of COLING.\nEnkhbold Bataa and Joshua Wu. 2019. An investiga-\ntion of transfer learning-based sentiment analysis in\nJapanese. In Proceedings of ACL.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In TAC.\nShalabh Bhatnagar, Richard S Sutton, Mohammad\nGhavamzadeh, and Mark Lee. 2009. Natural actor–\ncritic algorithms. Automatica.\nZihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi\nZhao. 2018. Quora question pairs. University of\nWaterloo.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of IWP.\nJianping Gou, Baosheng Yu, Stephen J Maybank, and\nDacheng Tao. 2021. Knowledge distillation: A sur-\nvey. International Journal of Computer Vision.\nAnne-Marie Grisogono. 2006. Co-adaptation. In Com-\nplex Systems, volume 6039, pages 23–37.\n6240\nDouglas M Hawkins. 2004. The problem of overfit-\nting. Journal of chemical information and computer\nsciences, 44(1):1–12.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. Pro-\nceedings of NeurIPS.\nAref Jafari, Mehdi Rezagholizadeh, Pranav Sharma, and\nAli Ghodsi. 2021. Annealing knowledge distillation.\nIn Proceedings of EACL.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinyBERT: Distilling BERT for natural language\nunderstanding. In Proceedings of EMNLP Findings.\nBei Li, Ziyang Wang, Hui Liu, Quan Du, Tong Xiao,\nChunliang Zhang, and Jingbo Zhu. 2021a. Learn-\ning light-weight translation models from deep trans-\nformer. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 35, pages 13217–\n13225.\nLei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou,\nand Xu Sun. 2021b. Dynamic knowledge distillation\nfor pre-trained language models. In Proceedings of\nEMNLP.\nKevin J. Liang, Weituo Hao, Dinghan Shen, Yufan Zhou,\nWeizhu Chen, Changyou Chen, and Lawrence Carin.\n2021. Mixkd: Towards efficient distillation of large-\nscale language models. In Proceedings of ICLR.\nYe Lin, Yanyang Li, Ziyang Wang, Bei Li, Quan Du,\nTong Xiao, and Jingbo Zhu. 2021. Weight distilla-\ntion: Transferring the knowledge in neural network\nparameters. pages 2076–2088. Proceedings of ACL.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019a. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof ACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nSeyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang\nLi, Nir Levine, Akihiro Matsukawa, and Hassan\nGhasemzadeh. 2020. Improved knowledge distil-\nlation via teacher assistant. In Proceedings of AAAI.\nEkachai Phaisangittisagul. 2016. An analysis of the\nregularization between l2 and dropout in single hid-\nden layer neural network. In ISMS, pages 174–179.\nIEEE.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nEMNLP.\nBihi Sabiri, Bouchra El Asri, and Maryem Rhanoui.\n2022. Mechanism of overfitting avoidance tech-\nniques for training deep neural networks. In ICEIS\n(1), pages 418–427.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. ArXiv\npreprint.\nDavid Silver, Guy Lever, Nicolas Heess, Thomas De-\ngris, Daan Wierstra, and Martin A. Riedmiller. 2014.\nDeterministic policy gradient algorithms. In Proceed-\nings of ICML.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of EMNLP.\nSamuel Stanton, Pavel Izmailov, Polina Kirichenko,\nAlexander A Alemi, and Andrew G Wilson. 2021.\nDoes knowledge distillation really work? Proceed-\nings of NeurIPS.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019a. How to fine-tune bert for text classification?\nIn Proceedings of CCL.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019b.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of EMNLP.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of ACL.\nRichard S Sutton and Andrew G Barto. 2018. Reinforce-\nment learning: An introduction. MIT press.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam\nStooke, Xi Chen, Yan Duan, John Schulman, Filip De\nTurck, and Pieter Abbeel. 2017. #exploration: A\nstudy of count-based exploration for deep reinforce-\nment learning. In Proceedings of NeurIPS.\nGerald Tesauro. 1991. Practical issues in temporal dif-\nference learning. Proceedings of NeurIPS, 4.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Proceed-\nings of EMNLP WorkShop.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. Proceedings of NeurIPS,\n33:5776–5788.\n6241\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of NAACL.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nbertserini. In Proceedings of NAACL (Demonstra-\ntions), pages 72–77.\nJunho Yim, Donggyu Joo, Ji-Hoon Bae, and Junmo Kim.\n2017. A gift from knowledge distillation: Fast opti-\nmization, network minimization and transfer learning.\nIn Proceedings of CVPR.\nFei Yuan, Linjun Shou, Jian Pei, Wutao Lin, Ming Gong,\nYan Fu, and Daxin Jiang. 2021. Reinforced multi-\nteacher selection for knowledge distillation. In Pro-\nceedings of AAAI.\n6242\nA Design Details of Learning Each\nKnowledge Type\nA.1 Response Knowledge\nFor learning the response knowledge, we use the\nvanilla KD loss function:\nLResK=\n∑\nx∈B\nKL(σ(fS(x)\nτ ),σ(fT(x)\nτ )) (14)\nwhere KL(·) is the Kullback-Leibler divergence\nfunction, σ(·) is the softmax function and τ is a\ntemperature hyper-parameter (Hinton et al., 2015).\nFor the sample x, fS(x) and fT(x) are the final\noutputs of the student model and the teacher model,\nrespectively.\nA.2 Feature Knowledge\nFor learning the feature knowledge, we utilize the\nPKD-Skip method (Sun et al., 2019b), where the\nstudent model learns the outputs of the assigned\nteacher model’s layers. The corresponding loss\nfunction can be defined by:\nLFeaK=∑\nx∈B\nLS∑\ni=1\n\nhSx,ihSx,i\n2\n− hTx,I(i)hTx,I(i)\n2\n\n2\n(15)\nwhere I(i) is the assigned indexes of the teacher\nlayer for responding the i-th layer of the student\nmodel, LS is the number of layers of the student\nmodel, hS\nx,i and hT\nx,i are the [CLS] embeddings\nof i-th layer of the student model and the teacher\nmodel for the sample x, respectively.\nA.3 Relation Knowledge\nFor learning the relation knowledge, we use the\nflow of solution procedure (FSP) method (Yim\net al., 2017). Specifically, we first define the FSP\nmatrix by:\nG(h1,h2) = h1 ×h⊤2\n|h1| (16)\nwhere h1 and h2 are the [CLS] embeddings. Let\nGS(·) and GT(·) to denote the FSP matrices of the\nstudent model and the teacher model, respectively.\nBased on this two FSP matrices, we can achieve the\nloss function for learning the relation knowledge:\nLRelK=\n∑\nx∈B\nLS−1∑\ni=1\nMSE(GS(hS\nx,i,hS\nx,i+1),\nGT(hT\nx,I(i),hT\nx+1,I(i+1)))\n(17)\nwhere MSE(·) is the mean squared error loss func-\ntion. Here we use the same layer selection strategy\nfor the teacher model as the PKD-Skip method.\nA.4 Finetune Knowledge\nFor learning the finetune knowledge, we use the\nloss function:\nLFinK= −\n∑\nx∈B\n1{y= ˆy}logp(y|x) (18)\nwhere 1{·}is the indicator function, yis the pre-\ndicted label and ˆyis the ground-truth label of the\nsample x.\nB Training Setups\nB.1 Training the KSM\nWe implement the KSM described in Sec. 5 based\non PyTorch‡. Specifically, we use a 2-layer MLP\nto achieve the student and teacher feature networks.\nThe input layer size and output layer size are the\n[CLS] embedding size and 8, respectively. Both\nthe actor network and the critic network are com-\nposed of a 3-layer MLP. The hidden size and output\nlayer size in the actor are 256 and 4. The hidden\nsize and output layer size in the critic are 256 and\n1. In addition, there are six hyper-parameters for\ntraining the KSM. The candidate values for these\nhyper-parameters are introduced in Table 5. Dur-\ning training the KSM, we can select the optimal\nhyper-parameter setups with the best development\nset accuracy.\nHyper-parameter Value\nThreshold valueλ {0.1, 0.2, 0.3}\nNumber of training stepsk {32, 64, 96, 128}\nScale factorα {0.1, 0.2}\nDiscount factorγ 0.98\nLearning rate of the actorϵϕ 0.0002\nLearning rate of the criticϵθ 0.0002\nTable 5: Hyper-parameters for training the KSM.\nB.2 Training the Teacher Model\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size\nas H, and the number of self-attention heads as\nA for BERT. We use the pre-trained language\nmodel BERTBASE (L = 12 , H = 768 , A = 12 ,\nTotalParameters = 110M) (Devlin et al., 2019)\nas the teacher model. We initialize the BERTBASE\nwith the bert-base-uncased§. The teacher mod-\nels are trained with random seeds. In addition, the\nsentence length is 128, and the learning rate is 5e-5.\nWe set the batch size and training epoch to 32 and 5,\n‡https://github.com/pytorch/pytorch\n§https://github.com/google-research/bert\n6243\n/uni00000014 /uni00000015 /uni00000016 /uni00000017\n/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048\n/uni0000001b/uni0000001b\n/uni0000001b/uni0000001c\n/uni0000001c/uni00000013\n/uni0000001c/uni00000014\n/uni0000001c/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000039/uni00000044/uni00000051/uni0000004c/uni0000004f/uni0000004f/uni00000044/uni00000003/uni0000002e/uni00000027\n/uni00000032/uni00000058/uni00000055/uni00000003/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000003/uni0000000b/uni00000036/uni00000052/uni00000049/uni00000057/uni00000003/uni00000024/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c\n/uni00000032/uni00000058/uni00000055/uni00000003/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000003/uni0000000b/uni0000002b/uni00000044/uni00000055/uni00000047/uni00000003/uni00000024/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c\nFigure 5: Comparison of the soft and hard actions on\nthe QQP development set. For the QQP dataset, we can\ntrain a well-performance KSM only across two episodes.\nThis is because a large dataset can provide more oppor-\ntunities for updating the KSM in each episode.\nReward RTE MRPC QNLI Avg.\nLoss 68.4 89.3/84.7 89.0 80.7\nF1 66.9 87.8/82.9 88.1 79.3\nAccuracy 66.7 88.1/83.2 87.3 79.1\nTable 6: Results of different rewards for training KSM.\nrespectively. Note that it is possible to plug-in any\nlarge pre-trained model such as BERT LARGE (De-\nvlin et al., 2019) and RoBERTa (Liu et al., 2019b)\nin our method.\nB.3 Training the Student Model\nWe primarily report results on two student mod-\nels: BERT6 (L = 6 , H = 768 , A = 12 ,\nTotalParameters = 67.0M) and BERT3 (L=\n3, H = 768 , A = 12 , TotalParameters =\n45.7M). We initialize the BERT6 and BERT3 with\nthe bottom 6 and 3 transformer layers of BERTBASE ,\nrespectively. In training the student model, we set\nthe batch size to 32 and the sentence length to 128.\nWe select a well-trained student model through\ntheir scores among the learning rate set of {2e-5,\n3e-5, 5e-5}.\nC Discussion\nPerformance on Different Phase Sizes In Sec.\n5.3.3, we divide the whole KD process into mul-\ntiple phases. Then a question may arise about\nwhether the phase size has an impact on the KSM\nperformance. To probe this question, we run our\nmethod to distill the student model BERT 6 with\ndifferent phase sizes. As shown in Figure 6, we ob-\nserve that the excessive phase size can hurt the per-\nformance of the trained KSM, which we attribute\nto the deficiency of phase rewards. In addition, the\n/uni00000014/uni00000019/uni00000016/uni00000015/uni00000019/uni00000017/uni0000001c/uni00000019/uni00000014/uni00000015/uni0000001b/uni00000015/uni00000018/uni00000019\n/uni00000033/uni0000004b/uni00000044/uni00000056/uni00000048/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048\n/uni0000001b/uni0000001a\n/uni0000001b/uni0000001b\n/uni0000001b/uni0000001c\n/uni0000001c/uni00000013\n/uni0000001c/uni00000014\n/uni0000001c/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000039/uni00000044/uni00000051/uni0000004c/uni0000004f/uni0000004f/uni00000044/uni00000003/uni0000002e/uni00000027\n/uni00000032/uni00000058/uni00000055/uni00000003/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000003/uni0000000b/uni00000036/uni00000052/uni00000049/uni00000057/uni00000003/uni00000024/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c\n/uni00000032/uni00000058/uni00000055/uni00000003/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000003/uni0000000b/uni0000002b/uni00000044/uni00000055/uni00000047/uni00000003/uni00000024/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c\nFigure 6: Performance of the student models distilled\nby our method with different phase sizes.\n/uni00000013/uni00000011/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000014\n/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015/uni00000033/uni0000004b/uni00000044/uni00000056/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047\n/uni00000029/uni00000014/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047\n/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013\n/uni00000033/uni0000004b/uni00000044/uni00000056/uni00000048\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015 /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047\nFigure 7: Phase reward comparison of the different\nrewards in one episode. The loss reward is more stable\nand smooth than the F1 reward and the Accuracy reward.\ntoo small phase size is not beneficial to our method\nbecause its phase reward is similar to an immediate\nreward.\nPerformance on Different Phase Rewards We\ninvestigate the performance of our method while\ntaking the task metrics (i.e., F1 or Accuracy) differ-\nence as the phase reward. As shown in Table 6, we\nobserve that training KSM with a loss reward has\nthe best performance overall. To explore the rea-\nsons for this observation, we further compare the\nphase rewards of different rewards in one episode,\nas shown in Figure 7. The results show that the loss\nreward is more stable and smooth than the F1 and\nAccuracy reward. Based on this, we conclude that\nthe stability and smoothness give the loss reward\nthe ability to train a better KSM.\n6244",
  "topic": "Distillation",
  "concepts": [
    {
      "name": "Distillation",
      "score": 0.8276086449623108
    },
    {
      "name": "Computer science",
      "score": 0.7915686368942261
    },
    {
      "name": "Process (computing)",
      "score": 0.6327285766601562
    },
    {
      "name": "Machine learning",
      "score": 0.573809027671814
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5709962844848633
    },
    {
      "name": "Knowledge transfer",
      "score": 0.47975313663482666
    },
    {
      "name": "Knowledge engineering",
      "score": 0.4579940438270569
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.4462229907512665
    },
    {
      "name": "Knowledge management",
      "score": 0.19850760698318481
    },
    {
      "name": "Chemistry",
      "score": 0.06616050004959106
    },
    {
      "name": "Programming language",
      "score": 0.06113100051879883
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9224756",
      "name": "Northeastern University",
      "country": "CN"
    }
  ]
}