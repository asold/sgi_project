{
  "title": "Part-Guided Relational Transformers for Fine-Grained Visual Recognition",
  "url": "https://openalex.org/W3213639431",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2088959084",
      "name": "Zhao Yifan",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A1937087924",
      "name": "Li Jia",
      "affiliations": [
        "Beihang University",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1973214777",
      "name": "Chen Xiaowu",
      "affiliations": [
        "Beihang University",
        "Peng Cheng Laboratory",
        "State Key Laboratory of Virtual Reality Technology and Systems"
      ]
    },
    {
      "id": "https://openalex.org/A1989693854",
      "name": "Tian Yonghong",
      "affiliations": [
        "Peng Cheng Laboratory",
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035220232",
    "https://openalex.org/W2990495699",
    "https://openalex.org/W1898560071",
    "https://openalex.org/W2955854238",
    "https://openalex.org/W2763070548",
    "https://openalex.org/W6735649984",
    "https://openalex.org/W2741910023",
    "https://openalex.org/W1496650988",
    "https://openalex.org/W6640376812",
    "https://openalex.org/W2963203586",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W1968245656",
    "https://openalex.org/W6792309431",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2162915993",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W6677700107",
    "https://openalex.org/W6769955919",
    "https://openalex.org/W6736321067",
    "https://openalex.org/W56385144",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W1954152232",
    "https://openalex.org/W3034676907",
    "https://openalex.org/W2889469641",
    "https://openalex.org/W2737725206",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6754854709",
    "https://openalex.org/W1994213117",
    "https://openalex.org/W2295107390",
    "https://openalex.org/W6765836972",
    "https://openalex.org/W2998619563",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W6788477181",
    "https://openalex.org/W6792155083",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6783944145",
    "https://openalex.org/W2883888092",
    "https://openalex.org/W2807931652",
    "https://openalex.org/W6755069125",
    "https://openalex.org/W2963090248",
    "https://openalex.org/W2963066927",
    "https://openalex.org/W2883502031",
    "https://openalex.org/W2554320282",
    "https://openalex.org/W2551829638",
    "https://openalex.org/W2895643041",
    "https://openalex.org/W2202499615",
    "https://openalex.org/W2963393555",
    "https://openalex.org/W2963805953",
    "https://openalex.org/W6638677478",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W2798365843",
    "https://openalex.org/W2104657103",
    "https://openalex.org/W2963407932",
    "https://openalex.org/W3009073662",
    "https://openalex.org/W2990520530",
    "https://openalex.org/W2997426000",
    "https://openalex.org/W2773003563",
    "https://openalex.org/W3008809756",
    "https://openalex.org/W3035367622",
    "https://openalex.org/W2986821660",
    "https://openalex.org/W3174336354",
    "https://openalex.org/W2765268259",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3108870912",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2604134068",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4300106950",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2604702198",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W2892035828",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1928906481",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2951852399",
    "https://openalex.org/W1797268635",
    "https://openalex.org/W4299802238",
    "https://openalex.org/W1846799578",
    "https://openalex.org/W2119525058",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2961018736",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W2891951760",
    "https://openalex.org/W3139434170"
  ],
  "abstract": "Fine-grained visual recognition is to classify objects with visually similar appearances into subcategories, which has made great progress with the development of deep CNNs. However, handling subtle differences between different subcategories still remains a challenge. In this paper, we propose to solve this issue in one unified framework from two aspects, i.e., constructing feature-level interrelationships, and capturing part-level discriminative features. This framework, namely PArt-guided Relational Transformers (PART), is proposed to learn the discriminative part features with an automatic part discovery module, and to explore the intrinsic correlations with a feature transformation module by adapting the Transformer models from the field of natural language processing. The part discovery module efficiently discovers the discriminative regions which are highly-corresponded to the gradient descent procedure. Then the second feature transformation module builds correlations within the global embedding and multiple part embedding, enhancing spatial interactions among semantic pixels. Moreover, our proposed approach does not rely on additional part branches in the inference time and reaches state-of-the-art performance on 3 widely-used fine-grained object recognition benchmarks. Experimental results and explainable visualizations demonstrate the effectiveness of our proposed approach.",
  "full_text": "IEEE TRANSACTIONS ON IMAGE PROCESSING 1\nPart-guided Relational Transformers for\nFine-grained Visual Recognition\nYifan Zhao, Jia Li, Senior Member, IEEE, Xiaowu Chen, Senior Member, IEEE,\nand Yonghong Tian, Senior Member, IEEE\nAbstract—Fine-grained visual recognition is to classify objects\nwith visually similar appearances into subcategories, which has\nmade great progress with the development of deep CNNs. How-\never, handling subtle differences between different subcategories\nstill remains a challenge. In this paper, we propose to solve\nthis issue in one uniﬁed framework from two aspects, i.e.,\nconstructing feature-level interrelationships, and capturing part-\nlevel discriminative features. This framework, namely PArt-\nguided Relational Transformers (PART), is proposed to learn\nthe discriminative part features with an automatic part discovery\nmodule, and to explore the intrinsic correlations with a feature\ntransformation module by adapting the Transformer models from\nthe ﬁeld of natural language processing. The part discovery\nmodule efﬁciently discovers the discriminative regions which are\nhighly-corresponded to the gradient descent procedure. Then\nthe second feature transformation module builds correlations\nwithin the global embedding and multiple part embedding,\nenhancing spatial interactions among semantic pixels. Moreover,\nour proposed approach does not rely on additional part branches\nin the inference time and reaches state-of-the-art performance\non 3 widely-used ﬁne-grained object recognition benchmarks.\nExperimental results and explainable visualizations demonstrate\nthe effectiveness of our proposed approach. Code can be found\nat https://github.com/iCVTEAM/PART.\nIndex Terms—Fine-grained visual recognition, Transformers,\nPart discovery, Relationship.\nI. I NTRODUCTION\nF\nINE-grained visual recognition aims to classify and dis-\ntinguish the subtle differences among sub-categories with\nsimilar appearances, which has made great progress in recog-\nnizing an increasing number of categories. Beneﬁting from\nthe development of Convolutional Neural Networks (CNNs),\nperformance on recognizing common object categories, i.e.,\nbirds [1], [2], cars [3], and aircrafts [4], has increased\nsteadily in the last decade. However, further explorations on\ndiscriminative features and representative feature embedding\nstill face great challenges, which also limit the performance\nimprovement on ﬁne-grained recognition tasks.\nExisting methods in solving this challenge can be roughly\ndivided into two predominant groups, considering the different\nlearning manners of ﬁne-grained features. The ﬁrst group\ntackles the ﬁne-grained classiﬁcation problem by generating\nY . Zhao, J. Li, and X. Chen are with the State Key Laboratory of\nVirtual Reality Technology and Systems, School of Computer Science and\nEngineering, Beihang University, Beijing, 100191, China.\nY . Tian is with the Department of Computer Science and Technology,\nPeking University, Beijing, 100871, China.\nJ. Li, X. Chen and Y . Tian are also with the Pengcheng Laboratory,\nShenzhen, 518055, China.\nJ. Li is the corresponding author (E-mail: jiali@buaa.edu.cn).\nFig. 1. Motivation of the proposed approach. a): input image. b): one global\nfeature scope and three automatic part discovery scopes. c): relation trans-\nformation module to construct global relation embedding and local relational\nembedding. Our framework ﬁrst discovers the discriminative local regions and\nthen constructs contextual interactions with relation transformations.\nrich feature representations [5]–[9] or applying auxiliary con-\nstraints [10], [11]. Leading by the bilinear pooling opera-\ntion [5], second-order matrices across different channels are\nwidely adopted by introducing compact homogeneous repre-\nsentations [12], hierarchical organizations [13] and other di-\nmensional reduction operations [14]–[16]. Second-order pool-\ning operations describe the ﬁne-grained object features with\nrich pair-wise correlations between network channels, which\nare regarded as high-dimensional descriptors for discovering\nﬁne-grained features. Beyond the exploitation of second-order\nmatrices, trilinear attention across semantic channels [8], [9]\nare proposed to build global attention as well as maintaining\nthe feature shapes. However, there still remain two major\nproblems in this group of methods: 1) limited by the natural\nﬂaws of CNNs, the constructed correlations are still conducted\nlocally and restricted in channel dimensions, and the long-term\nspatial relationships are still unperceived; 2) methods of the\nsecond group focus on the constraints on feature learning, but\nneglect the discriminative part features, which are necessary\nfor distinguishing near-duplicated samples.\nTo tackle the second problem, methods of the other group\npropose to localize distinct parts for feature enhancement.\nPioneer methods tend to obtain object parts by using part\ndetectors [17], [18] or segmentation parsers [19], [20]. Whilst\npromising results have been achieved by introducing addi-\ntional network branches, annotating part segmentation masks\nor bounding boxes is labor-consuming. Toward this issue,\nrecent approaches [21]–[23] resort to attention mechanisms\narXiv:2212.13685v1  [cs.CV]  28 Dec 2022\nIEEE TRANSACTIONS ON IMAGE PROCESSING 2\nfor exploring auto-emerged parts during the backpropagation\nprocess. For example, Fu et al. [21] proposed a zoom-in strat-\negy to discover the most informative region in a progressive\nmanner. Nevertheless, auto-discovered object parts are usually\nunstable, which would lead to overﬁtting on local regions.\nHence a contextual understanding of global relationships is\nneeded in solving ﬁne-grained classiﬁcation problems.\nTo efﬁciently solve the deﬁciencies of these two groups\nof approaches, we propose to learn the discriminative part\nfeatures and intrinsic correlations in one uniﬁed framework,\nnamely PArt-guided Relational Transformers (PART). To be\nspeciﬁc, our proposed PART is composed of two essential\nmodules, i.e., a part discovery module to investigate discrimi-\nnative local regions, and a relational transformation module to\nconstruct local and contextual correlations. Inspired by the suc-\ncess in building long-term dependencies in Natural Language\nProcessing (NLP), we make an attempt to learn the relation-\nships among high-level features by using Transformers [24]. In\nthis paper, we embed the Transformer model [24] into the nat-\nural design of representative CNN architectures, incorporating\nself-related global relationships of network features with the\nstack of transformation layers. The high-level feature maps\nafter CNN encoders are decomposed as several individual\nvectors by their spatial dimensions. After that, each spatial\nregion has the potential to perceive the contextual information\nof any other region and builds semantic correlations with\neach other, which help the networks understand the holistic\nobjects rather than restricted in local patterns. This novel\nnetwork architecture greatly broadens the receptive ﬁeld of\nconventional CNN architectures, while simultaneously keeps\ntheir spatial distributions by a positional-aware encoding.\nAs shown in Fig. 1, our proposed approach ﬁrst investigates\nthe discriminative regions by generated Class Activation Maps\n(CAMs) [25], exploring several part scopes and one global\nscope in Fig. 1 b). These part regions are located by an auto-\nmatic part discovery module, which maximizes the diversity\nof different semantic parts while ﬁnding discriminative regions\nsimultaneously. With these scopes for object and part discov-\nery, we further propose to learn the local relational embedding\nand global contextual embedding in Fig. 1 c) with masked\nmulti-head attentions. Hence not only the global relationships\nare explored in our approach, but also the locally-connected\nrelations. With these two insightful modules, our approach can\nwell embed the long-term dependencies and generate robust\npart localizations for the accurate classiﬁcation of ﬁne-grained\nspecies. It is worth noting that our proposed PART approach\ndoes not rely on the additional part branches in the inference\ntime, which introduces less computation cost. Moreover, our\nproposed approach does not use additional data annotations\nor multi-crop testing operations, and achieves state-of-the-\nart performances on three public ﬁne-grained recognition\nbenchmarks, i.e., CUB-200-2011 [1], Stanford-Cars [3], and\nFGVC-Aircrafts [4]. Further experimental results and visual-\nized explanations reveal the effectiveness of the two proposed\nmodules.\nIn summary, the contribution of this paper is three-fold:\n1) We propose a novel PArt-guided Relational Transformers\n(PART) framework for ﬁne-grained recognition tasks,\nwhich broadens the long-term dependencies of conven-\ntional CNNs with a part-aware embedding. To the best\nof our knowledge, it is the ﬁrst attempt to handle ﬁne-\ngrained visual recognition tasks with joint CNN and\nTransformer architectures.\n2) We propose a new part discovery module to mine the\ndiscriminative regional features, to robustly handle dif-\nferential part proposals by class activation maps.\n3) We propose a part-guided relational transformation mod-\nule to learn intrinsic high-order correlations and conduct\ncomprehensive experiments to verify the effectiveness of\nour proposed approach, surpassing state-of-the-art models\nby a margin on 3 widely-used benchmarks.\nThe remainder of this paper is organized as follows: Section\nII brieﬂy reviews related works of ﬁne-grained recognition\ntasks and Section III describes how to discover distinct part\nfeatures and explore the part-guided contextual relationship\nin our proposed approach. Qualitative and quantitative experi-\nments are reported in Section IV. Section V ﬁnally concludes\nthis paper.\nII. R ELATED WORK\nEarly ﬁne-grained visual recognition researches [26]–[28]\nfocused on tackling limited labeling data with hand-crafted\nconventional features. For example, Yao et al. [26] proposed\nto use dense sampling strategies and random forest for mining\ndiscriminative features. Moreover, methods [27] using these\nhand-crafted features show its beneﬁts considering the inde-\npendence of human labeling or encoded dictionary features.\nRecent ideas in solving ﬁne-grained visual recognition tasks\nfocus on the exploitation of deep features, which show both\nrobustness and performance boost for handling discriminative\nfeatures. Considering the different constraints of representation\nlearning, we mainly consider two families of methods in this\npaper, i.e., part awareness learning and regularized feature\nrepresentation.\nA. Part awareness learning\nHandling subtle differences using part-level local fea-\ntures has been widely studied in the past decades. Pioneer\nworks [17], [20], [29]–[32] tend to discover the part-level\nfeatures with manual labels and amplify these local repre-\nsentations when obtaining the ﬁnal features. For example,\nZhang et al. [29] proposed a part-based R-CNN network,\nusing part detectors for building pose-normalized representa-\ntions. Krause et al. [33] proposed to detect the object part\nwith an unsupervised co-segmentation and alignment manner,\nenhancing the ﬁnal classiﬁcation abilities. Huang et al. [17]\nintegrated a part detection network in the recognition network\nand developed two streams to encode the part-level and object-\nlevel cues simultaneously. In addition, Weiet al. [31] proposed\nto learn accurate object part masks using an additional FCN\nand then aggregated these masked features with the global\nones. Although promising results have been achieved by using\nstrongly supervised annotations, labeling segmentation masks\nand accurate object parts are still time-and labor-consuming.\nIEEE TRANSACTIONS ON IMAGE PROCESSING 3\nOn the other hand, deep neural networks have the natural\nability in discovering object parts [34] with only weakly\nsupervised labels. Object parts would emerge automatically\nin the feature maps during the gradient learning process.\nBeneﬁting from these observations, recent works [21]–[23],\n[35]–[39] proposed to learn the part-aware features with auto-\ndiscovered part masks. Leading by this motivation, Simon et\nal. [36] proposed a local region discovery model using deep\nneural activation maps, constructing part-level attentions for\nﬁnal representation. Lam et al. [37] proposed to search object\nparts using heuristic function and successor functions. Wei et\nal. [40] proposed a ﬂood-ﬁll method which aggregated these\nfeatures maps as holistic object activations. However, this\noperation only considers the extraction of object features,\nwhich does not reﬂect the part localizations. Peng et al. [41]\nproposed to select image patches as candidates to select the\ninformative regions and extract the feature of these regions\nfor complementation. Wang et al. [39] proposed to regularize\nthe learned feature maps in a sparse representation manner\nand adopted axillary part branches for feature representation\nregularization. Moreover, Ge et al. [18] proposed a bottom-\nup architecture to obtain instance detection and segmentation\nvia weakly supervised object labels. Combing these part\nproposals, a context encoding LSTM is further proposed to\nfuze the sequential part information for image classiﬁcation.\nHowever, these works only considered the localization of part\nregions, while the contextual relationship of these local regions\nis less explored. In this paper, we propose to discover the\ndiscriminative parts and investigate their spatial relations in\none uniﬁed framework.\nB. Regularized feature representation\nThe other family for solving the ﬁne-grained visual cate-\ngorization task is to regularize the feature learning process or\nformulating rich feature relationships. Inspired by the bilinear\npooling operation [5], high-order relationship matrices [7],\n[8], [13], [42], [43] has been widely used for feature rep-\nresentations. For example, Yu et al. [13] proposed to learn\nthe heterogonous feature interaction from different levels\nof feature layers, building compact cross-layer relationships.\nHowever, simply adopting the second-order feature would lead\nto a dimensional disaster for optimization, thus subsequent\nworks proposed to use low-rank presentations [14], feature\nfactorization [15] or Grassmann constraints [16] for building\ncompact homogenous features. These works [14]–[16] greatly\nreduce the computation cost of high order matrices but still\nshow comparable results. Besides the efforts on the second-\norder relationships, the recent network tends to explore the\nthird-order relationships of feature channels. Leading by the\nnon-local [44] design in building global relationships of dif-\nferent channels, several works [8], [9] have been proposed\nto explore the channel-wise relationship by attention mech-\nanisms. Gao et al. [9] proposed to learn the channel-wise\nrelationship by image-level relations and cross-image relations\nby a contrastive constraint.\nThinking the feature representation problem from another\nperspective, other works tend to explore the cross-layer rela-\ntionships [6], [10], [45]–[48] or auxiliary information [49].\nSun et al. [10] proposed a multi-attention multi-constraint\n(MAMC) to regularize the same class focusing on the same\nattention region. Based on this MAMC module, Luo et al. [47]\nproposed to learn relationships between different images and\ndifferent network layers. In addition, Chen et al. [50] proposed\na destruction and reconstruction framework to learn the local\nfeature transformation rules for obtaining robust feature rep-\nresentations. Moreover, an attentive pair relation network [51]\nis proposed to ﬁne the group-wise relationship inter-and intra-\nclasses. Beyond these designs on network architectures, adopt-\ning pair-wise confusions [52] and maximizing the entropy [11]\nalso show effectiveness in improving the ﬁnal representation.\nDifferent from the aforementioned methods, in this work, we\npropose to ﬁnd the feature embedding by two steps, i.e.,\ndiscovering local parts and transforming for relational embed-\nding, which incorporates the advantages of part discovery but\nalso constructs reliable feature regularization.\nC. Vision Transformers\nTransformer architectures [24], [53] have achieved great\nprogress in the ﬁeld of natural language processing. Bene-\nﬁting from its ability for long-term relationship construction,\nseveral works [54], [55] proposed to introduce this architecture\nbased on the convention CNN encoders. For example, Car-\nion etal [54] proposed to localize different object regions with\nmultiple attention regions in transformer layers. Beyond these\nworks, recent ideas [56]–[59] proposed to directly encode\nthe images patches, e.g., 16 ×16 as the basic pattern to\nunderstanding images. As the representative work, ViT [56]\nbuilt a strong pretrained transformer backbone, which greatly\nsurpass the performance of conventional CNNs. However,\nthis encoding manner also introduces huge computation costs.\nSeveral works proposed to introduce the shifted windows [57]\nand local encoding [58] to build hierarchical encoders, which\nexploit the advantages of local understandings in CNNs and\nalso reduce computation costs. Based on these successful back-\nbone encoders, utilizing vision transformers has also shown\ngreat beneﬁts in semantic segmentation [60] and ﬁne-grained\nvisual classiﬁcation [61]. Different from the TransFG [61] with\nViT [56] encoders, our model builds relationships between\nhigh-level CNN features. In this manner, the local patterns\nwith detailed features can be well represented and the contex-\ntual relationships are also constructed simultaneously.\nIII. A PPROACH\nA. Problem Formulation\nOur proposed PART approach for ﬁne-grained visual recog-\nnition is illustrated in Fig. 2. Our ﬁrst key idea in exploring\nﬁne-grained recognition is to build contextual dependencies by\nthe conjunction of successful Transformers with conventional\nConvolutional Neural Networks (CNNs). The second motif is\nto regularize the network embedding be aware of the local\ndiscriminative regions, which is desired for distinguishing\nvisually similar samples. Beyond these designs, one global\nrelation transformation and three local relation transformations\nIEEE TRANSACTIONS ON IMAGE PROCESSING 4\nFig. 2. Overall pipeline of our proposed PArt-guided Relational Transformers (PART) framework, which is composed of a part discovery module and a\nrelational transformation module. The part discovery module automatically generates object parts from class activation maps during the forward propagation\nprocess. And the relational transformation module takes advantages of object parts and constructs a global interaction and S (S=3 for example) local interactions.\nDuring the inference stage, only the global branch is maintained, introducing less computation cost.\nare proposed to learn the crucial attention features by build-\ning regionally dense connections during the gradient descent\nprocess.\nGiven an input image I, the general procedure in optimizing\nﬁne-grained recognition task is to optimize the target E\nbetween prediction and groundtruth label y:\nθ∗= arg min\nθ\nE(Φ(I; θ)⊤wc,y), (1)\nwhere Φ(·; θ) represents feature extractor Φ with learnable\nparameters θ. wc represents the ﬁnal classiﬁcation projection\nvectors. Different from the general classiﬁcation optimizations,\nour proposed network ﬁrst generates two different feature\nembedding, i.e., Xp,Xg ∈RW×H×C. Thus we could local-\nize the object part proposals P with part discovery module\nMpart, i.e., P= Mpart(Xp), and construct high-order cor-\nrelations with Transformers G,Tfor global and local branches.\nBeyond these foundations, we thus make a general as-\nsumption that the object part proposals P share the same\ngroundtruth labels y with the global image I. Hence the\noverall optimization in our framework can be presented as:\nθ∗= arg min\nθ\nE((Φ(I; θb) ⊛ G(Xg; θg))⊤wg,y)+\n∑\np∈P\nλpE((Φ(I; θb) ⊛ Tp(p,Xp; θp))⊤wl,y), (2)\nwhere ⊛ indicates that the latter term extract features from the\nformer one. w{g,l}represent the global and local classiﬁcation\nprojections and λ is the balanced weight. Hence our network\nintroduces the relational transformation constraints and part\nlocalization constraints in ﬁne-grained classiﬁcation tasks. We\nwill elaborate the details of these two modules in Section III-B\nand Section III-C.\nB. Part Discovery Module\nExcavating discriminative features by localizing object parts\nhas been widely explored by previous works [17], [20], [29]–\n[32] with bounding box annotations or segmentation mask\nannotations. In our proposed PART framework, we propose to\nexplore the object parts in a weakly supervised manner, which\ntakes advantage of the class activation maps generated during\nthe gradient descent process. Recent research [34] shows that\nobject parts emerge automatically during the learning of classi-\nﬁcation tasks, i.e., during the training process, network features\ncan be automatically regularized for excavating discriminative\nfeatures and distinguishing near-duplicated objects.\nBased on these ﬁndings, several recent ideas, e.g., RA-\nCNN [21], MA-CNN [46], propose to constrain these acti-\nvation regions into part groups by their similarities. Different\nfrom these works, our part discovery module aims to provide\ndiscriminative region priors P for the transformer module.\nWith these priors, the transformer module is proposed to mine\nthe contextual object clues within each region. To achieve this,\nour proposed part discovery module follows two meaningful\nrules: 1) the highly activated features maps are attached\nwith high priorities during the back-propagation process; 2)\ndifferent activated parts should have the minimum overlap\nfor discriminative part ﬁnding. These rules ensure CAMs are\nnot restricted to local regions and enhance the generalization\nability of deep models. Besides that, our part discovery module\ncan be trained end-to-end in one uniﬁed scheme and does not\ncost additional computation costs for inference.\nGiven the extracted backbone features Xp ∈RW×H×C, we\nIEEE TRANSACTIONS ON IMAGE PROCESSING 5\nAlgorithm 1 Part Discovery Algorithm\nInput: Part feature maps X ∈RW×H×C, Hyperparameters:\nNumber of parts N, Selection range R, IoU threshold th\nOutput: Part Selection Set: P= {p1,p2,..., pN }\n1: Initialize selection set P= φ\n2: while |P|<N do\n3: Calculate part activation scores s of X in Eqn. (3)\n4: Rearrange X by the ranking of s:\nX∗= ActivationSort(X, s)\n5: for x∗\ni ∈X∗ do\n6: Sample Random Activation Rate: ηi ∼N(µ,σ)\n7: ˜xi = Norm(x∗\ni )\n8: p∗\ni = ROICrop( ˜xi ≥ηi):\n9: for pj ∈P do\n10: Calculate bounding box IoU:\nBIoU= BboxIoU(pj,p∗\ni )\n11: if BIoU >th then\n12: break\n13: Add new legal proposals: P= P∪{ p∗\ni }\n14: iter = iter+1\n15: if iter > maxiter and |P|<N then\n16: Select top N parts from X∗and repeat Step 6 ∼8\n17: return Part Selection Set P\ndeﬁne the class activation scores sc of the cth channel:\nvc = 1\nW ×H\nH∑\ni=1\nW∑\nj=1\nXp\ni,j,c,\nsc = vc\n∑C\nk=1(vk + ϵ)\n,\n(3)\nwhere ϵ is set to keep nonzero of the denominator. With this\nscore as the ranking indicator, we thus rearrange the X∗ =\nActivationSort(X, s) and build a part proposal stack to\nselect the most important one from the stack top.\nStarting from this part proposal stack, we ﬁrst sample a\nrandom rate for each part ηc ∼N(µ,σ), enhancing robustness\nfor threshold selection. For each selected feature x∗\nc ∈X, the\npart proposals p∗are localized with the ROICrop operation:\np∗\ni,j,c =\n\n\n\n1\nx∗\ni,j,c−min(x∗\nc)\nmax(x∗c)−min(x∗c) ≥ηc,\n0\nx∗\ni,j,c−min(x∗\nc)\nmax(x∗c)−min(x∗c) <ηc,\n(4)\nThe ﬁnal bounding box regions can be automatically ob-\ntained by calculating the range of coordinates in the x-axis and\ny-axis. However, as an unsupervised part discovery method,\ndifferent feature maps usually localize similar part regions,\nwhich may restrict the learning of discriminative features.\nInspired by the Non-Maximum Suppression operation [62] in\nedge detection, we remove the redundant part proposals by\ncalculating the Intersection over Union (IoU) of the current\nbounding box with the existing boxes in the part selection set\nP. By repeating this process, all the legal part proposals will be\nautomatically added into the ﬁnal selection set until |P|≥ N.\nSpecially, we further apply an iteration terminator if all this\npart selection module could not ﬁnd appropriate object parts.\nThe detailed process is elaborated in Algorithm 1. With this\nproposed part selection module, our approach has the potential\nto ﬁnd discriminative regional features automatically.\nC. Relation Embedding with Transformers\nAs mentioned in Section III-A, one crucial problem in ﬁne-\ngrained recognition is to build contextual dependencies for\ndifferent semantic parts. However, limited by the design of\n2D Convolutional Neural Networks, each feature unit has a\nrestricted receptive ﬁeld which is ﬁxed by the convolutional\nkernels. To solve this problem, we propose to adopt the\nTransformer [24], which has been established as a state-of-\nthe-art transduction model to take advantage of long-term\ncorrelation in semantic sentence understanding. Motivated by\nits succusses in natural language processing, we make an\nattempt to introduce it into the ﬁne-grained recognition task,\nwhich is presented as the transformation operation in Eqn. (2).\nFine-grained recognition with visual transformers. The\nconventional transformers tend to take the input of a whole lan-\nguage sentence and building correlations of different semantic\nwords, which are now used to replace the Recurrent Neural\nNetworks (RNNs) in many NLP tasks to build long-term\ndependencies. Besides its superior performance in relation\nto modeling, embedding this architecture into existing CNN\nbackbones, e.g., ResNet, VGGNet, is thus essential. In our\nPART framework, CNN backbones and transformers play\ndifferent roles in ﬁne-grained visual understanding. Solely\nadopting transformer models may result in a larger receptive\nﬁeld in relational modeling, but leads to a high computation\nburden for network optimization. In addition, keeping the main\nbody of CNNs can also beneﬁt from the pretrained common\nknowledge, e.g., ImageNet [63]. Hence in our approach, we\npropose to make a conjunction of conventional CNNs and\nthe successful Transformer model. The detailed architecture\nis illustrated in Fig. 3.\nPositional encoding. Another drawback in attention-based\nmodeling methods is that structural information would be\nforgotten, especially when transforming CNN feature maps\nX ∈RW×H×C to multiple vectors. When dense connections\nare constructed, their original structural information could\nbe easily lost. Inspired by [24], [54] in keeping position\ninformation, here we adopt the positional encoding E into\nthe original backbone features X. With the positional encoded\nfeatures regardless of part proposals in Fig. 3, the key idea is to\nbuild global relation matrices A ∈RWH ×WH , indicating the\ndense dependencies of each pixel unit to all the other pixels.\nLet i,j be the positions of query and key values, this spatial\ncorrelation can be formulated as:\nAi,j = (Xi,: + Ei,:)WqryW⊤\nkey(Xj,: + Ej,:)⊤, (5)\nwhere W{qry,key} denote the learnable weight of query and\nkey projections. Here we adopt the relative positional encoding\nof Transformer-XL [64] to build different positional encoding\nfor different pixels. We build the 1D learning for relative\nencoding and 2D positional learning for absolute encoding.\nIEEE TRANSACTIONS ON IMAGE PROCESSING 6\nFig. 3. Illustration of the part-guided relational transformation module.\nWhen input backbone features and positional encoding information, this\nmodule builds local transformation embedding with efﬁcient self-attention\nmechanisms with a multi-head ( H1 . . .Hm) encoder manner.\nBy expanding Eqn. (5) and using learnable u,v, this relative\nencoding correlations A can be presented as:\nAi,j = X⊤\ni,:W⊤\nqryWkeyXj,: + X⊤\ni,:W⊤\nqry ˆWkeyRi−j\n+ u⊤W⊤\nkeyXj,: + v⊤ˆWkeyRi−j,\n(6)\nwhere key weights are spilt into Wkey and relative position\nˆWkey respectively. R denotes the sinusoid encoding matrix\nin [24] without learnable weights.\nMulti-head attention with transformers. The relational\nmatrices Ai,j are multiplied with the original feature X,\nresulting in the enhanced feature of one head H:\nH = exp(A:/\n√\nC)∑WH\nj=1 exp(A:,j/\n√\nC)\nXWval. (7)\nAs illustrated in Fig. 3, each transformer unit is composed of\nmultiple transformer heads, i.e., H= {H1,..., Hm}and m\ndenotes the number of attention heads. Employing multiple\nattention heads encourages different projections of query, key\nand value triplets to construct relational embedding. Then\nthese attention heads are concatenated and fused with the\nfeed-forward network, and the ﬁnal output O can be formally\npresented as:\nO = Concat(H1,..., Hm)Wo + bo, (8)\nwhere Wo ∈ RmCh×C denotes the learnable weight for\nlinear transformations, and Ch denotes the dimension of one\nattention head.\nRelations with CNNs. Based on this designment of rela-\ntional embedding transformers, networks have the potential\nto learn the long-term dependencies between any two pixels.\nNotably, this attentive transformer layer and convolutional\nlayers process 2D features in similar ways. In other words, the\ntransformer layers are attached with more relational embed-\nding learning matrices compared to the convolutional layers\nand in extreme cases, the transformers can be degenerated to\nsimulate CNNs. The main theorem from [65] is as follows:\nTheorem 3.1: A transformer layer with m heads and with\nhidden dimension Ch and output dimension Cout can simulate\nconvolutional neural networks with min(Ch,Cout) channels.\nDetailed proof can refer to [65], which is abstracted in\nAppendix A. Theorem 3.1 shows that the transformer modules\nhave the potential to represent existing CNN layers but have\nthe strong potential to explore cross-pixel relations. It means\nthat the CNNs are “restricted” transformers that have ﬁxed\nattentional embedding or limited receptive ﬁelds.\nPart-guided relation embedding. As aforementioned in\nEqn. (2), our proposed optimization framework aims to con-\nstruct global relations as well as discovering discriminative\npart relations simultaneously. Beneﬁting from the strong re-\nlation embedding ability of transformer models, we propose\nto learn the part-level relations by the auto-discovered part\nin Section III-B. We ﬁrst adopt the learned part masks pi ∈P\nas attentive masks before feeding into the transformer layers.\nThus the learned architecture of each attention head can be\npresented as:\nHpi = Attention((E(pi) + Xi,: ⊙pi)), (9)\nwhere ⊙ denotes the Hadamard product for mask region\nselection. We thus concatenate the multiple heads of each\npart following Eqn. (8), forming output Opi of each part pi.\nAfter passing the mask selection operation, a dense connection\nwithin each local part is constructed, which discovers the\nspatial relations while neglects the unnecessary ones.\nMoreover, the part-guided transformer units can be stacked\nin sequence, passing each enhanced feature Opi into the next\nlayer. This operation enlarges the reception ﬁeld of each pixel\nand constructs strong correlations by mixing connections hops:\nOpi\nt = Tpi\nt−1(Opi\nt−1,Et + X,pi),t = 1 ...S, (10)\nwhere S denotes the number of stacked layers of transformer\nmodel T. Similarly, the global relation embedding transformer\nGis deﬁned in similar manners.\nIn our training scheme, we conduct a collaborative learning\nscheme of the global and N part transformation branches\nin Fig. 2. Each branch is supervised with an individual softmax\ncross-entropy loss for minimizing target E in Eqn. (2). Thus\nour overall learning is composed of one global loss for\ncontextual relation learning and N part losses for local feature\ndiscovery. Note that our proposed PART approach resort to\nthe N auxiliary part branches for feature regularization in the\ntraining stage, whilst not relying on these parts in the testing\nstage. Moreover, simply aggregating these features would also\nlead to overﬁtting issues and inferior results. Compared to\nthe ResNet backbone, we only introduce limited learnable\nmatrices for relational embedding and conduct an end-to-end\ntraining with the conjunction of part discovery module and\nrelational embedding module. Our framework greatly allevi-\nates the computation burden and retains the discriminative part\nsimultaneously.\nIV. E XPERIMENTS\nIn this section, we ﬁrst introduce the experimental settings\nwith dataset statistics in Section IV-A and elaborate the imple-\nmentation details and network architectures in Section IV-B.\nSubsequently, the comparison with state-of-the-art methods is\nexhibited in Section IV-C. The detailed performance analyses,\nIEEE TRANSACTIONS ON IMAGE PROCESSING 7\nTABLE I\nCOMPARISON WITH STATE -OF-THE -ART RESULTS ON CUB-200-2011\nDATASET. †: USING ADDITIONAL ANNOTATIONS .\nType Method Backbone Accuracy\nPart\nBased\nPA-CNN†[33] VGG-19 84.3%\nFCAN†[66] ResNet50 84.7%\nRA-CNN [21] VGG-19 85.3%\nMA-CNN [46] VGG-19 86.5%\nInterpret [20] ResNet101 87.3%\nNTS-Net [23] ResNet50 87.5%\nDF-GMM [39] ResNet50 88.8%\nFeature\nBased\nBilinear [5] VGG-16 84.0%\nMAMC [10] ResNet101 86.5%\nMaxEnt [11] DenseNet-161 86.5%\nDFL-CNN [6] VGG-16 86.7%\nPC [52] DenseNet-161 86.9%\nHBP [13] VGG-16 87.1%\nDFL-CNN [6] ResNet50 87.4%\nCross-X [47] ResNet50 87.7%\nDCL [50] ResNet50 87.8%\nTASN [8] ResNet50 87.9%\nACNet [48] ResNet50 88.1%\nAPI-Net [51] ResNet101 88.6%\nPMG [43] ResNet50 89.6%\nJoint PART (Ours) ResNet50 89.6%\nPART (Ours) ResNet101 90.1%\nas well as the explainable visualizations, are presented in Sec-\ntion IV-D.\nA. Experimental Settings\nTo evaluate the effectiveness of our proposed PART ap-\nproach, we conduct experiments on three public popu-\nlar benchmarks, namely Caltech-UCSD Birds (CUB-200-\n2011) [1], Stanford-Cars [3], and FGVC-Aircrafts [4]. CUB-\n200-2011 dataset [1] contains 11,788 images of 200 wild bird\nspecies, which is the most widely used benchmark. Stanford-\nCars dataset [3] includes 16,185 images of 196 car subcate-\ngories. FGVC-Aircraft dataset [4] contains 10,000 images of\n100 classes of ﬁne-grained aircrafts. We follow the standard\ntraining/testing splits as in the original works [1], [3], [4]. For\nevaluations, we use the top-1 evaluation criterion following\nprevious works and only adopt the classiﬁcation label for\nsupervised training without any additional part annotations.\nB. Implementation Details\nFor the implementation of baseline and our proposed PART\nframework, we adopt ResNet-50 and ResNet-101 [67] net-\nworks pretrained on ImageNet [63] as our backbone for fair\ncomparisons. We remove the last bottleneck in stage 4 to\ndirectly get the feature of 512 dimensions. We use the SGD\noptimizer with an initial learning rate of 8e−4 annealed by\n0.1 for every 60 epochs. We adopt the commonly used data\naugmentation techniques, i.e., random cropping and erasing,\nleft-right ﬂipping, and color jittering for robust feature repre-\nsentations. Our model is relatively lightweight and is trained\nend-to-end on two NVIDIA 2080Ti GPUs for acceleration. We\nuse the group-wise sampler with a batchsize of 16, sampling\n4 samples for each class. For hyper-parameter ﬁne-tuning,\nwe randomly select 10% of the training set as validation.\nThe balanced weights λp for multiple part loss functions are\nempirically set as 0.1. The channel dimension for feeding\ninto relation embedding module is C = 512 and the part\nbranch number is set as 4, the IoU threshold is th = 0 .6.\nThe stacked layer number S is set as 3 for the global branch\nand 1 for the local branches. The training and testing protocol\nfollow the state-of-the-art works [47], [50], [51] using random\ncropping of 448×448 in training and only use one-crop during\ninference, which does not rely on the multi-crop features like\nprevious works [21], [23], [46]. Notably, in the inference stage,\nwe only use the global feature for accuracy reports, and the\nlocal branches can be simply omitted for acceleration.\nC. Comparison with State of The Arts\nIn this subsection, we conduct experimental comparisons\nwith state-of-the-art models on three representative bench-\nmarks, i.e., CUB-200-2011 [1], Stanford-Cars [3], and FGVC-\nAircrafts [4].\nComparison on CUB-200-2011 dataset. CUB-200-2011 is\nthe most widely-used dataset, consisting of 200 bird species\nwith visually similar appearances. The classiﬁcation accuracy\ncan be found in Tab. I. Following the aforementioned group\nways in Section I, we divide the state-of-the-art models into\ntwo groups, i.e., feature regularized learning and part-based\nlearning. It can be found that the earlier works, e.g., [33],\n[66] relies on the accurate part localization information, while\nachieving only 84.7% base performance. Hence with the de-\nvelopment of auto-discovered part localization methods [23],\n[46], the classiﬁcation accuracy has been increased by over\n3% without any additional annotations.\nOn the other hand, feature-learning-based algorithms also\nshow advantages in extracting informative knowledge and\nforming efﬁcient representations. Hence our proposed PART\napproach is a joint learning algorithm with collaborative\nfeature relation learning and part feature discovery. Com-\nbining these merits, our proposed method generates state-\nof-the-art results of 89.6% accuracy, which demonstrates the\neffectiveness of our proposed learning framework. Follow-\ning API-Net [51], we extend our model with the deeper\nResNet101 backbones, which achieves 90.1% and is 1.5%\nhigher than [51].\nComparison on FGVC-Aircraft dataset. Similar to the\nbenchmarking on CUB-200-2011 dataset, here we make com-\nparisons on sub-categories of aircrafts in Tab. II. Recent\napproaches, e.g., DCL [50] and Cross-X [47], achieve perfor-\nmance of 93.0% and 92.7%, which is much higher than the\nprevious work [5]. To make fair comparisons with these works,\nwe adopt the lightweight ResNet-50 backbone to achieve the\nstate-of-the-art performance of 94.4%, while previous works\nrely on deep backbones for feature extraction, e.g., [9] with\nResNet-101 and [11], [52] with DenseNet-161. In addition,\nother works [21], [23], [46] adopt the multi-crop inference to\nobtain multi-scale features, introducing additional computation\nIEEE TRANSACTIONS ON IMAGE PROCESSING 8\nTABLE II\nCOMPARISON WITH STATE -OF-THE -ART RESULTS ON FGVC-A IRCRAFT\nDATASET.\nType Method Backbone Accuracy\nPart\nBased\nRA-CNN [21] VGG-19 88.2%\nMA-CNN [46] VGG-19 89.9%\nNTSNet [23] ResNet50 91.4%\nFeature\nBased\nBilinear [5] VGG-16 84.1%\nPC [52] DenseNet-161 89.2%\nMaxEnt [11] DenseNet-161 89.8%\nHBP [13] VGG-16 90.3%\nACNet [48] VGG-16 91.5%\nDFL-CNN [6] ResNet50 92.0%\nACNet [48] ResNet50 92.4%\nCIN [9] ResNet50 92.6%\nCross-X [47] ResNet50 92.7%\nCIN [9] ResNet101 92.8%\nDCL [50] ResNet50 93.0%\nAPI-Net [51] ResNet50 93.0%\nPMG [43] ResNet50 93.4%\nAPI-Net [51] ResNet101 93.4%\nJoint PART (Ours) ResNet50 94.4%\nPART (Ours) ResNet101 94.6%\nburdens. Different from these mentioned issues, our proposed\nPART approach presents robust features with ResNet-50 back-\nbone and one-crop inference.\nComparison on Stanford-Cars dataset. Tab. III exhibits\nthe results on the Stanford-Cars dataset. Stanford-Cars is a\nmuch easier dataset, in which previous methods achieve pre-\nliminary results of over 92.5% accuracy. It can be found that\nmethods on this dataset perform very similar performance,e.g.,\nTASN [8] and HBP [13] of 93.7% performance. While\nobtaining multi-level feature representations [47] and using\nseparate layer initialization strategies [6], high performance as\n94.5% can be achieved. Surprisingly, even on this dataset, our\nmethod can provide a steady improvement compared to state-\nof-the-art results, reaching 95.1% performance. With deep\nbackbones i.e., ResNet101, the performance shows a slight\nimprovement to 95.3% in top-1 accuracy.\nD. Performance Analysis\nIn this subsection, we ﬁrst conduct ablation studies of our\nproposed different modules, and study the manner of positional\nencoding in relation to transformation modules. Then we make\na detailed analysis of the part effects in our framework and\nﬁnally make explanations using visualized comparisons.\nAblation studies on different components. To evaluate\nthe effectiveness of our proposed part discovery module and\nrelation transformation module, we conduct detailed ablation\nstudies on three representative benchmarks [1], [3], [4]. In Tab.\nIV, the ﬁrst line shows the baseline performance on these three\nbenchmarks, using the same data augmentation and training\nhyper-parameters as our ﬁnal model. It can be found that our\nbaseline model surpasses many earlier works and our proposed\nmodules, i.e., relation transformation module MRelation and\npart discovery module MPart can improve the recognition\nTABLE III\nCOMPARISON WITH STATE -OF-THE -ART RESULTS ON STANFORD -CARS\nDATASET. †: ADDITIONAL BOUNDING BOX OR SEGMENTATION\nANNOTATION .\nType Method Backbone Accuracy\nPart\nBased\nPA-CNN†[33] VGG-19 92.8%\nMA-CNN [46] VGG-19 92.8%\nNTSNet [23] ResNet50 93.9%\nFeature\nBased\nBilinear [5] VGG-16 91.3%\nGrassmann [16] VGG-16 92.8%\nPC [52] DenseNet-161 92.9%\nMaxEnt [11] DenseNet-161 93.0%\nMAMC [10] ResNet101 93.0%\nHBP [13] VGG-16 93.7%\nTASN [8] ResNet50 93.7%\nDFL-CNN [6] ResNet50 93.8%\nCIN [9] ResNet50 94.1%\nCIN [9] ResNet101 94.5%\nCross-X [47] ResNet50 94.5%\nDCL [50] ResNet50 94.5%\nAPI-Net [51] ResNet50 94.8%\nAPI-Net [51] ResNet101 95.1%\nPMG [43] ResNet50 95.1%\nJoint PART (Ours) ResNet50 95.1%\nPART (Ours) ResNet101 95.3%\nTABLE IV\nABLATION STUDIES OF OUR DIFFERENT COMPONENTS ON THREE\nBENCHMARKS . MRelation, MPart DENOTES THE PROPOSED RELATION\nTRANSFORMATION MODULE AND PART -DISCOVERY MODULE\nRESPECTIVELY . QUATERNION (x, y1, y2, y3) INDICATES THE HEAD\nNUMBER OF GLOBAL TRANSFORMER IS x AND HEADS OF THREE PARTS\nARE y1, y2 AND y3.\nMRelation MPart CUB-200-2011 FGVC-Aircraft Stanford-Cars\nN/A N/A 85.4% 90.7% 93.4%\n(2,0,0,0) N/A 88.2% 93.5% 94.7%\n(2,1,1,1) ×3 89.2% 94.2% 95.1%\n(4,1,1,1) ×3 89.3% 94.3% 95.1%\n(4,1,1,1) ×4 89.6% 94.4% 95.1%\nability steadily, e.g., from 85.4% to 89.6% (4 parts) on CUB-\n200-2011 dataset. In the second row, we only adopt the relation\ntransformation module with stacked manners, and set the\nhead number of multi-head attention as 2. It can be found\nthat the performance has increased notably by incorporating\nthe part-level features. This veriﬁes that the effectiveness of\nour proposed module, even on the high-performance baseline.\nMoreover, when setting the number of attention heads as 4,\nperformance on these three datasets can be slightly improved.\nEffects of feature encoding. We ﬁrst present the perfor-\nmance of the baseline method in the ﬁrst two rows on the\nCUB benchmark dataset. The ﬁrst row indicates that without\nthe group sampler in Section IV-B of a class-wise balanced\nmanner, baseline model would result in a performance drop\nof over 1%. As a natural drawback in building contextual\ndense relations, the spatial structure information is lost during\nthe vectorization process. The result (with 2 attention heads)\nIEEE TRANSACTIONS ON IMAGE PROCESSING 9\nFig. 4. Class activation visualizations of baseline (ResNet50) and our proposed PART. Left ﬁgures in a) and c) are the weighted summed class activation\nmap of all channels. right ﬁgures in a) and c) are the guided gradient of baseline and our model generated by Grad-CAMs [68]. Images in b) are referred\nchannels generated by our proposed PART.\nTABLE V\nPERFORMANCE ANALYSIS OF FEATURE EXTRACTION AND FEATURE\nENCODING MANNER ON CUB-200-2011 BENCHMARK . MRelation,\nMPart DENOTES THE PROPOSED RELATION TRANSFORMATION MODULE\nAND PART -DISCOVERY MODULE RESPECTIVELY .\nFeature Extraction Encoding Manner Accuracy\nBaseline (w/o group sampler) N/A 84.2%\nBaseline N/A 85.4%\nMRelation N/A 87.5%\nMRelation Absolute Encoding 88.1%\nMRelation Learnable Encoding 88.2%\nMRelation + MPart Learnable Encoding 89.2%\nwithout positional information can be found in the third-row\nof Tab. V. By incorporating the absolute positional encoding\noperation (sinusoid encoding function in [24]), the recognition\naccuracy has been increased from 87.5% to 88.1%. In this\npaper, we adopt relative encoding which uses learned encoding\nparameters to encode the unique positional information for\neach pixel. The result can be found in the ﬁfth row, which\nalso has the potential to simulate any convolutional layers\nas in Theorem 3.1. And ﬁnally our full model with the part\ndiscovery module achieves the best performance in the last\nrow.\nEffects of part discovery. To evaluate the effectiveness of\nthe part discovery module, here we exhibit different hyperpa-\nrameters in the part discovery module in Tab. VI. In our pro-\nTABLE VI\nHYPER PARAMETER EXPERIMENTS ON PART DISCOVERY MODULE ON\nCUB-200-2011 BENCHMARK . |X∗|AND |P|DENOTES THE NUMBER OF\nPARTS IN THE PART STACK AND FINAL SELECTED PARTS RESPECTIVELY . H\nINDICATES THE HEAD NUMBER OF TRANSFORMERS .\n|X∗| N/A 64 64 64 32 64 64 64\n|P| N/A 1 2 3 3 3 4 5\nH 2 2 2 2 2 4 4 4\nAcc. 88.2% 88.8% 89.0% 89.2% 89.1% 89.3% 89.6% 88.8%\nposed algorithm, the part selection is mainly affected by two\nhyper-parameters, i.e., the capacity of the part stack |X∗|and\nthe ﬁnal selected part number |P|. The selected part number\nalso affects the network architecture, deciding the number of\nregularized part-level branches. When applying no auxiliary\npart branches, in Tab. VI, our proposed approach with only\ntransformer modules achieves the performance of 88.2%. With\nthe increase of selected part number |P|, auxiliary supervision\nis also automatically added into the overall framework. It can\nbe found that our approach with more parts (|P|= 4) reaches a\nhigh performance of 89.6%. This indicates the effectiveness of\nusing auxiliary part branches as regularization. However, with\nthe part numbers |P|increasing to be 5, the ﬁnal performance\ndecreases to 88.8%. Discovering more parts would incorporate\nmeaningless background regions into computation and also\nrestrain the learning of global features, which jointly lead\nto an inferior response on class activation maps. Moreover,\nIEEE TRANSACTIONS ON IMAGE PROCESSING 10\nwhen squeezing the capacity of the part stack |X∗|, the part\ndiscovery module would also lead to similar performance,\nwhich veriﬁes the robustness of our proposed method.\nDuring the inference phase, we only rely on the global\nbranches and drop the redundant part features. We also conduct\nexperiments of direct concatenating these branches rather than\nusing them as regularization. The performance of condition\n|P|= 1 in the ﬁrst row of Tab. VI drops to 88.5%. Assuming\nthe same classiﬁcation task of 200 classes, the overall parame-\nters of ResNet50 is 25.6M. Comparing to the full model with\nall 4 parts of 32.9M, our ﬁnal model only requires 28.7M\nparameters for learning, which is implementation-friendly.\nVisualized explanations. We further investigate the class\nactivation maps using state-of-the-art Grad-CAMs [68]. As\nshown in Fig. 4, we present the summation of all channels\nof baseline model (ResNet50) in Fig. 4 a), and our proposed\nPART in c). Interestingly, the baseline model can easily fall\ninto local attentions to distinguish the visually similar samples,\nwhich leads to inferior generalization capabilities. While our\nproposed PART can localize the full object, resulting in a\nrobust recognition ability. Moreover, we visualize the referred\nchannels of our model in Fig. 4 b). It can be found that\ndifferent channels focus on different semantic parts, forming\nthe ﬁnal robust representations. Although these “parts” do not\nstrictly meet the deﬁnition of natural semantics, while still\nresponding to meaningful object regions.\nV. C ONCLUSIONS\nIn this paper, we propose a novel PArt-guided Relational\nTransformers (PART) framework for ﬁne-grained recognition\ntasks. To solve the deﬁciencies in building long-term rela-\ntionships in conventional CNNs, we make the ﬁrst attempt\nto introduce the Transformer architecture into ﬁne-grained\nvisual recognition tasks. Beyond these insights, we further\npresent a new part discovery module to automatically mine\ndiscriminative regions by utilizing the class activation maps\nduring the training process. With these generated part regions,\nwe propose a part-guided transformation module to learn high-\norder spatial relationships among semantic pixels. Our full\nmodel is composed of several local regional transformers and\none global transformer, enhancing the discriminative regions\nwhile maintaining the contextual one. Experimental results\nverify the effectiveness of our proposed modules and our\nproposed PART reaches new state-of-the-art on 3 widely-used\nﬁne-grained recognition benchmarks.\nACKNOWLEDGMENT\nThis work was supported by grants from National\nNatural Science Foundation of China (No.61922006 and\nNo.61825101).\nREFERENCES\n[1] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The\ncaltech-ucsd birds-200-2011 dataset,” 2011.\n[2] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis,\nP. Perona, and S. Belongie, “Building a bird recognition app and\nlarge scale dataset with citizen scientists: The ﬁne print in ﬁne-grained\ndataset collection,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2015, pp. 595–604.\n[3] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3d object representations\nfor ﬁne-grained categorization,” in Proceedings of the IEEE Interna-\ntional Conference on Computer Vision Workshops , 2013, pp. 554–561.\n[4] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi, “Fine-\ngrained visual classiﬁcation of aircraft,” arXiv preprint arXiv:1306.5151,\n2013.\n[5] T.-Y . Lin, A. RoyChowdhury, and S. Maji, “Bilinear cnn models for\nﬁne-grained visual recognition,” in IEEE International Conference on\nComputer Vision (ICCV) , 2015, pp. 1449–1457.\n[6] Y . Wang, V . I. Morariu, and L. S. Davis, “Learning a discriminative ﬁlter\nbank within a cnn for ﬁne-grained recognition,” in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2018, pp. 4148–\n4157.\n[7] L. Zhang, S. Huang, W. Liu, and D. Tao, “Learning a mixture of\ngranularity-speciﬁc experts for ﬁne-grained categorization,” in IEEE\nInternational Conference on Computer Vision (ICCV) , 2019, pp. 8331–\n8340.\n[8] H. Zheng, J. Fu, Z.-J. Zha, and J. Luo, “Looking for the devil in the\ndetails: Learning trilinear attention sampling network for ﬁne-grained\nimage recognition,” inIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019, pp. 5012–5021.\n[9] Y . Gao, X. Han, X. Wang, W. Huang, and M. Scott, “Channel interaction\nnetworks for ﬁne-grained image categorization.” in AAAI Conference on\nArtiﬁcial Intelligence (AAAI) , 2020, pp. 10 818–10 825.\n[10] M. Sun, Y . Yuan, F. Zhou, and E. Ding, “Multi-attention multi-class\nconstraint for ﬁne-grained image recognition,” in European Conference\non Computer Vision (ECCV) , 2018, pp. 805–821.\n[11] A. Dubey, O. Gupta, R. Raskar, and N. Naik, “Maximum-entropy ﬁne\ngrained classiﬁcation,” in Advances in Neural Information Processing\nSystems (NeurIPS), 2018, pp. 637–647.\n[12] Y . Gao, O. Beijbom, N. Zhang, and T. Darrell, “Compact bilinear pool-\ning,” in IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2016, pp. 317–326.\n[13] C. Yu, X. Zhao, Q. Zheng, P. Zhang, and X. You, “Hierarchical bilinear\npooling for ﬁne-grained visual recognition,” in European Conference on\nComputer Vision (ECCV) , 2018, pp. 574–589.\n[14] S. Kong and C. Fowlkes, “Low-rank bilinear pooling for ﬁne-grained\nclassiﬁcation,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017, pp. 365–374.\n[15] Y . Li, N. Wang, J. Liu, and X. Hou, “Factorized bilinear models for\nimage recognition,” in IEEE International Conference on Computer\nVision (ICCV), 2017, pp. 2079–2087.\n[16] X. Wei, Y . Zhang, Y . Gong, J. Zhang, and N. Zheng, “Grassmann\npooling as compact homogeneous bilinear pooling for ﬁne-grained visual\nclassiﬁcation,” in European Conference on Computer Vision (ECCV) ,\n2018, pp. 355–370.\n[17] S. Huang, Z. Xu, D. Tao, and Y . Zhang, “Part-stacked cnn for ﬁne-\ngrained visual categorization,” in IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2016, pp. 1173–1182.\n[18] W. Ge, X. Lin, and Y . Yu, “Weakly supervised complementary parts\nmodels for ﬁne-grained image classiﬁcation from the bottom up,” in\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2019, pp. 3034–3043.\n[19] M. M. Kalayeh, E. Basaran, M. G ¨okmen, M. E. Kamasak, and M. Shah,\n“Human semantic parsing for person re-identiﬁcation,” in IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) , 2018, pp.\n1062–1071.\n[20] Z. Huang and Y . Li, “Interpretable and accurate ﬁne-grained recognition\nvia region grouping,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2020, pp. 8662–8672.\n[21] J. Fu, H. Zheng, and T. Mei, “Look closer to see better: Recurrent atten-\ntion convolutional neural network for ﬁne-grained image recognition,” in\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2017, pp. 4438–4446.\n[22] A. Recasens, P. Kellnhofer, S. Stent, W. Matusik, and A. Torralba,\n“Learning to zoom: a saliency-based sampling layer for neural net-\nworks,” in European Conference on Computer Vision (ECCV) , 2018,\npp. 51–66.\n[23] Z. Yang, T. Luo, D. Wang, Z. Hu, J. Gao, and L. Wang, “Learning\nto navigate for ﬁne-grained classiﬁcation,” in European Conference on\nComputer Vision (ECCV) , 2018, pp. 420–435.\n[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems , 2017, pp. 5998–6008.\n[25] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning\ndeep features for discriminative localization,” in IEEE Conference on\nIEEE TRANSACTIONS ON IMAGE PROCESSING 11\nComputer Vision and Pattern Recognition (CVPR) , 2016, pp. 2921–\n2929.\n[26] B. Yao, A. Khosla, and L. Fei-Fei, “Combining randomization and dis-\ncrimination for ﬁne-grained image categorization,” in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR). IEEE, 2011, pp.\n1577–1584.\n[27] B. Yao, G. Bradski, and L. Fei-Fei, “A codebook-free and annotation-\nfree approach for ﬁne-grained image categorization,” in IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) . IEEE,\n2012, pp. 3466–3473.\n[28] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial\npyramid matching for recognizing natural scene categories,” in IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), vol. 2.\nIEEE, 2006, pp. 2169–2178.\n[29] N. Zhang, J. Donahue, R. Girshick, and T. Darrell, “Part-based r-cnns for\nﬁne-grained category detection,” in European Conference on Computer\nVision (ECCV). Springer, 2014, pp. 834–849.\n[30] X. He and Y . Peng, “Weakly supervised learning of part selection model\nwith spatial constraints for ﬁne-grained image classiﬁcation,” in Thirty-\nﬁrst AAAI conference on artiﬁcial intelligence , 2017, pp. 4075–4081.\n[31] X.-S. Wei, C.-W. Xie, J. Wu, and C. Shen, “Mask-cnn: Localizing parts\nand selecting descriptors for ﬁne-grained bird species categorization,”\nPattern Recognition, vol. 76, pp. 704–714, 2018.\n[32] B. He, J. Li, Y . Zhao, and Y . Tian, “Part-regularized near-duplicate\nvehicle re-identiﬁcation,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2019, pp. 3997–4005.\n[33] J. Krause, H. Jin, J. Yang, and L. Fei-Fei, “Fine-grained recognition\nwithout part annotations,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2015, pp. 5546–5555.\n[34] A. Gonzalez-Garcia, D. Modolo, and V . Ferrari, “Do semantic parts\nemerge in convolutional neural networks?” International Journal of\nComputer Vision, vol. 126, no. 5, pp. 476–494, 2018.\n[35] T. Xiao, Y . Xu, K. Yang, J. Zhang, Y . Peng, and Z. Zhang, “The\napplication of two-level attention models in deep convolutional neural\nnetwork for ﬁne-grained image classiﬁcation,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2015, pp. 842–850.\n[36] M. Simon and E. Rodner, “Neural activation constellations: Unsu-\npervised part model discovery with convolutional networks,” in IEEE\nInternational Conference on Computer Vision (ICCV) , 2015, pp. 1143–\n1151.\n[37] M. Lam, B. Mahasseni, and S. Todorovic, “Fine-grained recognition\nas hsnet search for informative image parts,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2017, pp. 2520–\n2529.\n[38] Y . Ding, Y . Zhou, Y . Zhu, Q. Ye, and J. Jiao, “Selective sparse sampling\nfor ﬁne-grained image recognition,” in IEEE International Conference\non Computer Vision (ICCV) , 2019, pp. 6599–6608.\n[39] Z. Wang, S. Wang, S. Yang, H. Li, J. Li, and Z. Li, “Weakly supervised\nﬁne-grained image classiﬁcation via guassian mixture model oriented\ndiscriminative learning,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2020, pp. 9749–9758.\n[40] X.-S. Wei, J.-H. Luo, J. Wu, and Z.-H. Zhou, “Selective convolutional\ndescriptor aggregation for ﬁne-grained image retrieval,” IEEE Transac-\ntions on Image Processing , vol. 26, no. 6, pp. 2868–2881, 2017.\n[41] Y . Peng, X. He, and J. Zhao, “Object-part attention model for ﬁne-\ngrained image classiﬁcation,” IEEE Transactions on Image Processing ,\nvol. 27, no. 3, pp. 1487–1500, 2017.\n[42] Y . Zhao, K. Yan, F. Huang, and J. Li, “Graph-based high-order relation\ndiscovery for ﬁne-grained recognition,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2021, pp.\n15 079–15 088.\n[43] R. Du, D. Chang, A. K. Bhunia, J. Xie, Z. Ma, Y .-Z. Song, and J. Guo,\n“Fine-grained visual classiﬁcation via progressive multi-granularity\ntraining of jigsaw patches,” in European Conference on Computer Vision\n(ECCV). Springer, 2020, pp. 153–168.\n[44] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\nworks,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7794–7803.\n[45] D. Chang, Y . Ding, J. Xie, A. K. Bhunia, X. Li, Z. Ma, M. Wu,\nJ. Guo, and Y .-Z. Song, “The devil is in the channels: Mutual-channel\nloss for ﬁne-grained image classiﬁcation,” IEEE Transactions on Image\nProcessing, vol. 29, pp. 4683–4695, 2020.\n[46] H. Zheng, J. Fu, T. Mei, and J. Luo, “Learning multi-attention con-\nvolutional neural network for ﬁne-grained image recognition,” in IEEE\nInternational Conference on Computer Vision (ICCV) , 2017, pp. 5209–\n5217.\n[47] W. Luo, X. Yang, X. Mo, Y . Lu, L. S. Davis, J. Li, J. Yang, and S.-N.\nLim, “Cross-x learning for ﬁne-grained visual categorization,” in IEEE\nInternational Conference on Computer Vision (ICCV) , 2019, pp. 8242–\n8251.\n[48] R. Ji, L. Wen, L. Zhang, D. Du, Y . Wu, C. Zhao, X. Liu, and\nF. Huang, “Attention convolutional binary neural tree for ﬁne-grained\nvisual categorization,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2020, pp. 10 468–10 477.\n[49] O. M. Aodha, E. Cole, and P. Perona, “Presence-only geographical priors\nfor ﬁne-grained image classiﬁcation,” in IEEE International Conference\non Computer Vision (ICCV) , 2019, pp. 9595–9605.\n[50] Y . Chen, Y . Bai, W. Zhang, and T. Mei, “Destruction and construction\nlearning for ﬁne-grained image recognition,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2019, pp. 5157–\n5166.\n[51] P. Zhuang, Y . Wang, and Y . Qiao, “Learning attentive pairwise in-\nteraction for ﬁne-grained classiﬁcation,” in The Thirty-Fourth AAAI\nConference on Artiﬁcial Intelligence, AAAI , 2020, pp. 13 130–13 137.\n[52] A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell, and N. Naik,\n“Pairwise confusion for ﬁne-grained visual classiﬁcation,” in European\nConference on Computer Vision (ECCV) , 2018, pp. 70–86.\n[53] K. M. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane,\nT. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser et al. ,\n“Rethinking attention with performers,” in International Conference on\nLearning Representations, 2020.\n[54] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision (ECCV) . Springer, 2020,\npp. 213–229.\n[55] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:\nDeformable transformers for end-to-end object detection,”arXiv preprint\narXiv:2010.04159, 2020.\n[56] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” in 9th International Conference on Learning Representations,\nICLR, Virtual Event, Austria, May 3-7 . OpenReview.net, 2021.\n[57] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” arXiv preprint arXiv:2103.14030 , 2021.\n[58] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z. Jiang, F. E. Tay, J. Feng,\nand S. Yan, “Tokens-to-token vit: Training vision transformers from\nscratch on imagenet,” arXiv preprint arXiv:2101.11986 , 2021.\n[59] A. Srinivas, T.-Y . Lin, N. Parmar, J. Shlens, P. Abbeel, and A. Vaswani,\n“Bottleneck transformers for visual recognition,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2021, pp. 16 519–\n16 529.\n[60] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n“Segformer: Simple and efﬁcient design for semantic segmentation with\ntransformers,” arXiv preprint arXiv:2105.15203 , 2021.\n[61] J. He, J.-N. Chen, S. Liu, A. Kortylewski, C. Yang, Y . Bai, C. Wang,\nand A. Yuille, “Transfg: A transformer architecture for ﬁne-grained\nrecognition,” arXiv preprint arXiv:2103.07976 , 2021.\n[62] A. Rosenfeld and M. Thurston, “Edge and curve detection for visual\nscene analysis,” IEEE Transactions on computers , vol. 100, no. 5, pp.\n562–569, 1971.\n[63] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2009, pp. 248–255.\n[64] Z. Dai, Z. Yang, Y . Yang, J. G. Carbonell, Q. Le, and R. Salakhutdi-\nnov, “Transformer-xl: Attentive language models beyond a ﬁxed-length\ncontext,” in Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , 2019, pp. 2978–2988.\n[65] J.-B. Cordonnier, A. Loukas, and M. Jaggi, “On the relationship between\nself-attention and convolutional layers,” in 8th International Conference\non Learning Representations, ICLR, Addis Ababa, Ethiopia, April 26-30.\nOpenReview.net, 2020.\n[66] X. Liu, T. Xia, J. Wang, Y . Yang, F. Zhou, and Y . Lin, “Fully\nconvolutional attention networks for ﬁne-grained recognition,” arXiv\npreprint arXiv:1603.06765, 2016.\n[67] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” inIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016, pp. 770–778.\n[68] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh,\nand D. Batra, “Grad-cam: Visual explanations from deep networks\nvia gradient-based localization,” in IEEE International Conference on\nComputer Vision (ICCV) , 2017, pp. 618–626.\nIEEE TRANSACTIONS ON IMAGE PROCESSING 12\nAPPENDIX\nPROOF OF THE THEOREM 3.1\nThis Appendix provides a brief proof of theorem 3.1, which\nis derived from previous research [65] and not the main\ncontribution of our paper.\nTheorem A.1: A transformer layer with m heads and with\nhidden dimension Ch and output dimension Cout can simulate\nconvolutional neural networks with min(Ch,Cout) channels.\nProof A.1: A typical transformer layer T is composed of\nmulti-head attention ( MHA) layers with m heads and sev-\neral feed forward layers (multi-layer perceptron). Considering\nEqn. (7) and Eqn. (8), it can be formally presented as:\nT = ((MHA(X)XWval + bo)Wf + bf\n= (Concatm\nh=1(\n∑\nk\nϕ(A(h))kXk)Wh\nval + bo)Wf + bf ,\n(11)\nwhere ϕ denotes the softmax operation and Wf ,bf are\nlearnable weights for feed-forward layers. In this equation,\nthe learnable weights Wh\nval and Wf can be formulated into\none uniﬁed projection of Wh\nproj. In this manner, Eqn. (11)\ncan be further reformulated as:\nT = Concatm\nh=1(\n∑\nk\nϕ(A(h))kXk)Wh\nproj + b, (12)\nConsidering a convolutional layer Conv(X) = XkW + b,\nthe transformer layer would degenerated to a convolutional\nlayer, when the attention value ϕ(A(h))k equals to 1, when\nthe positional biases ∆{1,2} of convolutional kernel centers\nequals to the attention biases k−q. Considering the relative\nencoding in the main manuscript:\nAk,q = X⊤\nk,:W⊤\nqryWkeyXq,: + X⊤\nk,:W⊤\nqry ˆWkeyRk−q\n+ u⊤W⊤\nkeyXq,: + v⊤ˆWkeyRk−q,\n(13)\nAs proved by Cordonnier et al. [65], there exists an attention\nmatrix ϕ(A(h))k with relative encoding to be 1 if the bias in\nconvolutional kernels equals to k−q, otherwise ϕ(A(h))k = 0.\nTo construct this, we set Wkey = Wqry = 0, thus Eqn. (13)\nonly has the last term of v⊤ˆWkeyRk−q.\nIn the above expression, we set ˆWkey = I and assume\n(A(h)\nq )k = v⊤Rk−q = −α||k−q−∆||2 + αc, the softmax\noperation ϕ(A(h)\nq )k yields 1. α and c denotes the coefﬁcient\nand constant respectively. Note that ∆{1,2} ∈Z2 denote the\nbiases of x-axis and y-axis direction.\nWith these values ﬁxed, the limitation of softmax value\nϕ(A(h))k could reach 1. On the other conditions ( k−q̸= ∆),\nits limitation would be zero. Hence theorem 3.1 can be\nconcluded.",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.9057328104972839
    },
    {
      "name": "Computer science",
      "score": 0.7547056674957275
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7209634184837341
    },
    {
      "name": "Embedding",
      "score": 0.6984090209007263
    },
    {
      "name": "Inference",
      "score": 0.6429204344749451
    },
    {
      "name": "Transformer",
      "score": 0.6024335026741028
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5316306352615356
    },
    {
      "name": "Feature extraction",
      "score": 0.47962889075279236
    },
    {
      "name": "Pixel",
      "score": 0.4698214530944824
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.45295771956443787
    },
    {
      "name": "Transformation (genetics)",
      "score": 0.43775707483291626
    },
    {
      "name": "Deep learning",
      "score": 0.4206690788269043
    },
    {
      "name": "Machine learning",
      "score": 0.34993743896484375
    },
    {
      "name": "Natural language processing",
      "score": 0.3246147632598877
    },
    {
      "name": "Engineering",
      "score": 0.07767519354820251
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4392738276",
      "name": "State Key Laboratory of Virtual Reality Technology and Systems",
      "country": null
    }
  ],
  "cited_by": 58
}