{
  "title": "WBI at MEDIQA 2021: Summarizing Consumer Health Questions with Generative Transformers",
  "url": "https://openalex.org/W3167705015",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5017484078",
      "name": "Mario Sänger",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A5041680406",
      "name": "Leon Weber",
      "affiliations": [
        "Humboldt-Universität zu Berlin",
        "Max Delbrück Center"
      ]
    },
    {
      "id": "https://openalex.org/A5055236937",
      "name": "Ulf Leser",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3166358520",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3000014100",
    "https://openalex.org/W2741672218",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W3023167149",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W2996403597",
    "https://openalex.org/W2946119234",
    "https://openalex.org/W2950246755",
    "https://openalex.org/W2793567354",
    "https://openalex.org/W2964061924",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2295951612",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3125468681",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "This paper describes our contribution for the MEDIQA-2021 Task 1 question summarization competition. We model the task as conditional generation problem. Our concrete pipeline performs a finetuning of the large pretrained generative transformers PEGASUS (Zhang et al.,2020a) and BART (Lewis et al.,2020). We used the resulting models as strong baselines and experimented with (i) integrating structured knowledge via entity embeddings, (ii) ensembling multiple generative models with the generator-discriminator framework and (iii) disentangling summarization and interrogative prediction to achieve further improvements.Our best performing model, a fine-tuned vanilla PEGASUS, reached the second place in the competition with an ROUGE-2-F1 score of 15.99. We observed that all of our additional measures hurt performance (up to 5.2 pp) on the official test set. In course of a post-hoc experimental analysis which uses a larger validation set results indicate slight performance improvements through the proposed extensions. However, further analysis is need to provide stronger evidence.",
  "full_text": "Proceedings of the BioNLP 2021 workshop, pages 86–95\nJune 11, 2021. ©2021 Association for Computational Linguistics\n86\nWBI at MEDIQA 2021: Summarizing Consumer Health Questions with\nGenerative Transformers\nMario Sänger∗ ∗, ♣, Leon Weber∗, ♣, †, Ulf Leser♣\n♣Computer Science Department, Humboldt-Universität zu Berlin\n†Max Delbruck Center for Molecular Medicine\n{saengema,weberple,leser}@informatik.hu-berlin.de\nAbstract\nThis paper describes our contribution for the\nMEDIQA-2021 Task 1 question summariza-\ntion competition. We model the task as con-\nditional generation problem. Our concrete\npipeline performs a ﬁnetuning of the large\npretrained generative transformers PEGA-\nSUS (Zhang et al., 2020a) and BART (Lewis\net al., 2020). We used the resulting mod-\nels as strong baselines and experimented with\n(i) integrating structured knowledge via entity\nembeddings, (ii) ensembling multiple genera-\ntive models with the generator-discriminator\nframework and (iii) disentangling summariza-\ntion and interrogative prediction to achieve\nfurther improvements. Our best perform-\ning model, a ﬁne-tuned vanilla PEGASUS,\nreached the second place in the competition\nwith an ROUGE-2-F1 score of 15.99. We\nobserved that all of our additional measures\nhurt performance (up to 5.2 pp) on the ofﬁ-\ncial test set. In course of a post-hoc exper-\nimental analysis which uses a larger valida-\ntion set results indicate slight performance im-\nprovements through the proposed extensions.\nHowever, further analysis is need to provide\nstronger evidence.\n1 Introduction\nThe internet provides a wealth of information on\nhealth topics through specialised websites, forums,\nblogs and social networks. Increasingly, consumers\nare using these information sources to answer their\nmedical and health-related questions. In the course\nof this development, also the consumers’ expecta-\ntions regarding search engine functionalities have\nbecome much more demanding. Instead of reading\nthrough a list of relevant articles returned by a clas-\nsical search engine, short and precise passages are\nnow expected to answer questions. This transfor-\nmation also has an impact on the technologies used\n∗ These authors contributed equally. Author order was\ndetermined by coin ﬂip.\nto fulﬁll the user’s information needs. In particular,\napproaches for automatic questions answering as\nwell as automatic summarization and simpliﬁcation\nof (long) articles has received a lot of attention by\nresearchers in recent years (Allahyari et al., 2017;\nKwiatkowski et al., 2019; Narayan et al., 2018b;\nSee et al., 2017; Weber et al., 2019). This trend\nis also addressed by Task 1 of the MEDIQA 2021\nshared task (Ben Abacha et al., 2021) through in-\nvestigating consumer health-questions asked on the\n(experimental) medical question answering system\nCHiQA1. As we participated only in this task, we\nrefer to it as Shared Task (ST) in the following.\nThe goal of Task 1 was to foster the development\nof new summarization approaches, speciﬁcally de-\nsigned for the challenges of long and potentially\ncomplex consumer health questions. One major\nchallenge of CHiQA is the extraction of the user’s\nmain concern from the question text. The given\nquestions are often lengthy and contain a lot of pe-\nripheral information, which makes automatic pro-\ncessing and answering (much more) difﬁcult. Re-\ncent studies highlight that expert-based summariza-\ntions of such questions can lead to signiﬁcant en-\nhancements of the overall QA process (Ben Abacha\nand Demner-Fushman, 2019). Effective automatic\nsummarization methods could therefore play a key\nrole for improving medical question answering.\nWe contribute to this task by ﬁrst building a base-\nline using the general conditional generation frame-\nwork and then investigating three modiﬁcations to\nsummarize the consumer health questions. Our\nbaseline relies on ﬁnetuning the large pretrained\ngenerative transformers PEGASUS (Zhang et al.,\n2020a) and BART (Lewis et al., 2020). We ex-\nplore three different strategies to improve the per-\nformance of these baseline models, i.e. (i) integrat-\ning structured knowledge via entity embeddings,\n(ii) ensembling multiple generative models with\nthe generator-discriminator framework and (iii) dis-\n1https://chiqa.nlm.nih.gov/\n87\nentangling summarization and question word pre-\ndiction. Our best performing model, a ﬁne-tuned\nvanilla PEGASUS, reached the second place in the\ncompetition. We observed that all measures hurt\nperformance (up to 5.2 pp) on the evaluation set.\nHowever, a post-hoc experimental analysis (see\nSection 3), using a larger validation set, indicates\nslight improvements through the model extensions.\nThe remainder of the paper is organized as fol-\nlows: the next section introduces our baseline and\nthe three extension strategies in detail. Section 3\nhighlights and discusses the experiments and re-\nsults we obtained in our own evaluation as well\nas in the ofﬁcial assessment. The paper concludes\nwhich a summary of the main ﬁndings.\n2 Methods\n2.1 Data & Baselines\nThe shared task provides only an ofﬁcial validation\nand test set as data. For training data, we follow the\ntasks’ organizers suggestion to use the MeQSum\ncorpus which consists of 1,000 consumer health\nquestions and their summaries.\nWe model the summarization task as conditional\ngeneration, in which a model is prompted with\nthe original question and then generates the sum-\nmary in an autoregressive fashion. We base our\nimplementation2 on the huggingface transformers\nlibrary (Wolf et al., 2020) and experiment with the\nincluded pretrained generative transformers bart-\nbase3, bart-large4, pegasus-large5 and pegasus-\nxsum6. pegasus-xsum is a version of PEGASUS\nthat was already ﬁnetuned for summarization on\nthe Xsum dataset (Narayan et al., 2018a). For all\nmodels, we use a learning rate of 3e −5 and train\nfor 10 epochs. We use beam search for decoding\nand tune the search parameters on the validation set.\nWe independently evaluated {1, 10}as the number\nof beams and the {0.7, 0.8, 0.9, 1.0}for the length\npenalty and found 10 and 0.8 to be optimal.\n2Our code is publicly available under https://\ngithub.com/leonweber/bionlp21_summarize\n3https://huggingface.co/facebook/\nbart-base\n4https://huggingface.co/facebook/\nbart-large\n5https://huggingface.co/google/\npegasus-large\n6https://huggingface.co/google/\npegasus-xsum\n2.2 Integration of structured knowledge via\nentity embeddings\nIn initial analyses, we noticed that most question\nsummaries revolve around a few central entities\nsuch as speciﬁc diseases or medications which are\nalmost always mentioned in the source text. Fur-\nthermore, all of the generative transformers that\nwe used were trained on texts from the general do-\nmain, in which such entities presumably are rare.\nWe conjectured that it could be beneﬁcial to explic-\nitly provide entity information to the model. We\napproach this by ﬁrst applying a domain-speciﬁc\nNER model to the source text and then enriching\nthe input embeddings of the transformer with the\nfound entities. Formally, we extend the computa-\ntion of the i’th input embedding in the transformer\nto:\nei = wi + pi + si + ni, (1)\nwhere wi, pi, si are the standard subword, position\nand sequence type embeddings which are initial-\nized with the weights of the pretrained transformer.\nni is a randomly initialized embedding, which rep-\nresents the type of the named entity to which the to-\nken i belongs (including None) and has the same di-\nmensionality as the other transformer embeddings.\nNote, that si is set to zero for transformers which do\nnot use sequence type embeddings such as BART.\nWe experiment with two different NER models:\n(i) HunFlair (Weber et al., 2021), a state-of-the-\nart BioNER tagger and (ii) a custom Flair (Ak-\nbik et al., 2019) model trained on the CHQA cor-\npus (Kilicoglu et al., 2018) consisting of manual\nannotations for the central entities of consumer\nhealth questions. Speciﬁcally, we use the Dis-\nease and Chemical models of HunFlair and the\nPC-harmonization of the CHQA corpus.\n2.3 Ensembling multiple generative\ntransformers\nIn preliminary experiments, we found that ensem-\nbling generative transformers by simply averaging\nthe logits of different models hurt performance.\nThus, we investigate a different strategy for en-\nsembling generative models. We ﬁrst use each\nmodel m of the ensemble to generate n summaries\n{sm1, . . . , smn}conditioned on the original ques-\ntion q and then use a discriminative model to select\nthe question-summary pair with the highest prob-\nability. The n different summaries are generated\nby simply taking the ﬁnal generations of the top-n\n88\nscoring beams. We implement the discriminator as\na BERT (Devlin et al., 2019) model that receives\nboth the original question q and a question sum-\nmary s produced by one of the ensembled models\nand predicts the ROUGE-L-F1 score between both\nROUGE-L-F1(s, q) using a tanh output layer. The\nmodel is trained via an L2-loss. More formally,\nh = BERT[CLS](s, q) (2)\no = 0.5 ·tanh(W ·h + b) (3)\nL= ∥ROUGE-L-F1(s, q) −o∥2, (4)\nwhere BERT[CLS] is the BERT-embedding of\nthe special [CLS] token, W and b are trainable\nparameters and Lis the loss value.\nFor training the discriminator, we require gen-\nerated summaries that are close to the generated\nsummaries on the test data. We cannot simply use\nthe training data of the generators to create the\ntraining data for the discriminator, because we ex-\npect the distributions of the generated summaries\nfor seen and unseen data to be signiﬁcantly differ-\nent. Thus, we split MeQSum training data in a\n75% / 25% fashion and use the ﬁrst chunk for train-\ning the generators and the combination of both to\ntrain the discriminators. The full training process\nis illustrated in Figure 1a.\n2.4 Disentangling summarization and\ninterrogative prediction\nWe observed that the consumer questions cover\ndifferent categories of health-related issues in the\nST data, e.g. possible side-effects of certain drugs,\nsuitable treatments for speciﬁc diseases or food-\nrelated questions. We conjectured that providing\nthe putative category of the question to the summa-\nrization model could guide the generator towards a\nbetter summary. Moreover, we recognized that the\ndifferent categories are aligned to some extent with\nthe interrogative of the target questions summaries.\nBased on these two observations, we designed a\nthird modiﬁcation by creating a separate model to\npredict the putative interrogative, which acts as a\nsurrogate for the different question categories.\nTo this end, we implement a BERT-based classi-\nﬁcation model which gets the original user question\nas input and predicts the interrogative of the target\nquestion summary. We combine the classiﬁcation\nmodel with the output of our baseline method us-\ning a three-step approach: (i) we generate m ques-\ntion summaries using a generative transformer, (ii)\nwe predict the interrogative given the original user\nquestion based on the trained classiﬁcation model\nand (iii) selected the highest ranked candidate ques-\ntions which starts with the predicted interrogative\nas target summary. The process is illustrated in\nFigure 1b. To train the classiﬁcation models we\nuse the data from the MeQSum corpus but just take\nthe ﬁrst word of the summaries as goldstandard\ninterrogative. Because in this model there is no\ndependency between generative and classiﬁcation\nmodels (as opposed to our generator-discriminator\nframework), the classiﬁcation model can be trained\non the complete training data.\n3 Results\n3.1 Evaluation setting\nWe evaluate our models in two different settings.\nSetting 1For our ten submissions to the shared\ntask, we typically use some combination of MeQ-\nSum and the validation data for training. For model\nselection and evaluation of our modiﬁcations, we\nuse the ofﬁcial validation set of the shared task. Fi-\nnally, we report scores of our models on the shared\ntasks’ hidden test set.\nSetting 2While preparing our runs, we noticed\nthat the variance of the results on the validation\nand test set is rather high, which probably has to do\nwith the small amount of validation and test data\n(50 and 100 questions respectively). To evaluate\nthe performance impact of our modiﬁcations in a\nmore stable manner, we devised a second evalua-\ntion setting after the ST submissions were closed.\nFor this, we combine the MeQSum data and the\nshared task validation data in a single dataset and\nthen split it into a train and validation set, reserv-\ning 200 questions for validation, which leaves 850\nquestions for training. We ensure that for each split\nthe ratio of original MeQSum and validation data is\nequal. For each result, we compute three different\nruns with different random seeds and report the\naverage and standard deviation.\nTable 1 highlights the used splits of the two dif-\nferent data settings and provides basic statistics for\nthem. The results for both settings differ signiﬁ-\ncantly and thus, we report results for both settings\nin the following sections. In the ofﬁcial evalua-\ntion of the shared task, the approaches were ranked\naccording to the achieved ROUGE-2-F1 score.\n89\nFigure 1: (a) Training an ensemble of multiple generators together with a discriminator. Resources are depicted as\nyellow rectangles and trained models as green ellipses. (b) Predicting summaries with the interrogative predictor.\nResources are drawn as yellow rectangles and models as green ellipses.\nSetting Split Questions Tokens / Question Tokens / Summary\nMean Min Max Mean Min Max\nSetting 1 Training (MeQSum) 1000 60.78 5 378 10.04 3 26\nValidation 50 64.16 9 234 9.34 4 19\nSetting 2 Training 850 59.60 8 348 9.70 3 26\nValidation 200 66.64 5 378 10.18 3 26\nTable 1: Overview about the data sets and splits used for training and evaluation in Setting 1 and 2. For Setting 2,\nwe use all instances from the ofﬁcial training data (MeQSum) and validation data and randomly assign them to the\ntwo splits. We ensure that for each split the ratio of original training and validation data is equal.\n90\n3.2 Final evaluation results\nOur best performing model achieved a ROUGE-2-\nF1 score of 15.99% on the hidden test set, leading\nto a second place in the competition. However, all\ntop-5 models achieve results that are very close,\nand ranks change when different metrics are used.\nThe top ﬁve of the ofﬁcial leaderboard is repro-\nduced in Table 2. This best performing model is\none of our baselines based on pegasus-large ﬁne-\ntuned on the combination of MeQSum and the ST\nvalidation set. The results of our ten runs on the\nofﬁcial hidden test set together with a description\nof each run can be found in Table 5.\n3.3 Baseline results\nIn preliminary experiments on the ST validation set,\nwe found that pegasus-large works better than bart-\nlarge when the model is ﬁne-tuned on MeQSum\nand evaluated on the ST validation set (ROUGE-\nL-F1 of 33.32 vs. 32.82). Based on this result, we\nopted to select pegasus-large as baseline model for\nour submissions (refer to Section 3.7 for a discus-\nsion of challenges in model selection). In the ofﬁ-\ncial evaluation (i.e. Setting 1) the vanilla pegasus-\nlarge model achieves the best performance of all\nour submitted runs with an ROUGE-2-F1 score of\n15.99 (see Run 1 in Table 5). In a post-hoc anal-\nysis, we noticed that in the consumer questions\nspelling errors for crucial pieces of information\nsuch as diseases are common and that the models\ntend to copy those spelling errors into the summary\nof the question. Thus, our approach probably could\nhave beneﬁted from incorporating a spell-checking\ntool that corrects the spelling errors in the health\nquestions.\nSetting 2 uses the same basic models, but re-\nlies on a different training setup. Table 3 shows\nthe performance scores. The best performance is\nachieved by bart-large with ROUGE-1-, ROUGE-\n2 and ROUGE-L-F1 scores of 52.91, 34.06 and\n49.88. This represents an improvement of 0.55pp\nconcerning ROUGE-2-F1 to the next best model\n(bart-base). In this setting, the BART-based models\nachieve better results than the PEGASUS models.\n3.4 Entity embedding results\nWe evaluate the addition of entity embeddings to a\ngenerative transformer using bart-base. For detect-\ning entities, we experiment with the two different\nNER models HunFlair and a custom Flair model\ntrained on the PC-harmonization (Passonneau and\nCarpenter, 2014) of the CHQA corpus. The results\nfor Setting 2 can be found in Table 3. Adding entity\nembeddings to the input representation improves\nresults consistently, leading to a gain of 0.3pp and\n1.01pp in ROUGE-2-F1 over our bart-base base-\nline. However, we did not observe any gains in our\npreliminary experiments on the ST validation set\nand thus did not evaluate the models with entity\nembeddings in Setting 1. The submission of new\nruns was not possible at the time of writing.\n3.5 Ensemble results\nAll results for the generator-discriminator ensem-\nbles in Setting 1 (on the hidden test set) can be\nfound in Table 5, where each row with Type ’GD’\ncorresponds to one conﬁguration of a generator-\ndiscriminator ensemble. Considering ROUGE-2-\nF1, the best generator-discriminator result (run 7)\nstill performs 1.4 pp worse than our best baseline\nmodel. This run used only one generator based\non pegasus-large to produce ten candidates per\nquestion and a bert-large discriminator to select\nthe most promising summary. The only setting\nin which a generator-discriminator model outper-\nforms our strongest baseline on the hidden test set\nis run 8 which gains 0.2 pp under the BERTScore\nmetric (Zhang et al., 2020b), making it the overall\ntop ranking run of the ST under this metric. This\nrun uses a single pegasus-large generator proposing\nten candidate summaries per question and an en-\nsemble of three different bert-large discriminators.\nIn Setting 2, we observed considerable gains by\nusing an ensemble of bart-base, bart-large, pegasus-\nlarge and pegasus-xsum, while using a single bert-\nbase as the discriminator, using only the most prob-\nable output sequence per model as candidate. Com-\npared to pegasus-large, this conﬁguration leads to\nan improvement of 2.16pp in ROUGE-1-F1, 1.46pp\nin ROUGE-2-F1 and 2.27pp in ROUGE-L-F1.\nWe also investigated the performance ceiling for\nour ensembling approach by evaluating the ensem-\nble under a perfect discriminator, which always\nselects the summary yielding the highest Rouge-L-\nF1 score. Under this setting, our ensemble achieved\na Rouge-2-F1 score of 44.87 which is an improve-\nment of 10.9 pp. This shows the promise of our en-\nsembling approach and suggests that a worthwhile\npath to obtain better results would be to improve\nthe discriminator.\n91\nRank Team name ROUGE-1-F1 ROUGE-2-F1 ROUGE-L-F1 HOLMS BERTScore-F1\n1 damo_nlp (summc) 35.14 16.08 31.31 56.77 68.98\n2 WBI 33.40 15.99 31.49 57.67 69.96\n3 NCUEE-NLP 33.52 15.97 30.90 57.87 69.60\n4 yamr 32.80 15.25 30.38 57.86 68.77\n5 Saama 33.33 15.18 29.50 57.72 69.38\nTable 2: Top ﬁve of the ofﬁcial results for subtask one (ranked by ROUGE-2-F1). All scores are given in percent.\nIn total 23 teams participated in this subtask. Our contribution is displayed in bold. These numbers correspond to\nour evaluation Setting 2.\nModel type Gen. model(s) Add-on ROUGE-1-F1 ROUGE-2-F1 ROUGE-L-F1\nBaseline bart-large - 52.91 ( ±0.91) 34.06 ( ±1.01) 49.88 ( ±0.66)\nbart-base - 52.17 ( ±0.14) 33.49 ( ±0.84) 49.36 ( ±0.32)\npegasus-large - 51.06 ( ±0.78) 32.51 ( ±0.72) 48.28 ( ±0.68)\npegasus-xsum - 51.47 ( ±0.28) 32.65 ( ±0.58) 48.90 ( ±0.30)\nEntity bart-base HunFlair 52.16 ( ±0.45) 33.79 ( ±0.46) 49.24 ( ±0.27)\nembeddings bart-base CHQA ﬂair model 53.17 ( ±1.58) 34.5 ( ±1.30) 50.22 ( ±1.43)\nGenerator-\ndiscriminator\nbart-base\nbart-large\npegasus-large\npegasus-xsum\nbert-base 53.22 ( ±1.81) 33.97 ( ±1.40) 50.55 ( ±1.75)\nInterrogative pegasus-large bert-base 52.11 ( ±0.36) 33.71 ( ±0.85) 49.21 ( ±0.66)\nprediction pegasus-large bio-bert 52.22 ( ±0.60) 33.42 ( ±0.70) 49.26 ( ±0.53)\npegasus-large biomed-roberta 52.66 ( ±0.67) 33.71 ( ±0.81) 49.58 ( ±0.85)\npegasus-large bio-bert\nbiomed-roberta 52.28 (±0.58) 33.47 ( ±0.69) 49.40 ( ±0.67)\nTable 3: Overview of Setting 2 evaluation results. For each experiment, we list the used generative transformer(s)\nand (if applicable) utilized complementary models (Add-on). For entity embeddings add-on models are named\nentity recognition models. In case of the generator-discriminator framework it’s the discriminator model and\nregarding interrogative prediction it deﬁnes the applied classiﬁcation model(s). For each experiment, we compute\nthree different runs with different random seeds and report the average and standard deviation.\n92\n3.6 Interrogative-predictor results\nFor evaluating our interrogative prediction ap-\nproach we experimented with different transformer-\nbased models, pre-trained on either general\ndomain or biomedical data, for classiﬁcation:\nBERT7, BioBERT (Lee et al., 2020) 8, BioMed-\nRoBERTa (Gururangan et al., 2020)9 and multiple\nof these models arranged in an ensemble. All mod-\nels are learned on the training portion (for each\nevaluation setting). For all models we use pegasus-\nlarge as generative model and produce 10 candidate\nsummaries per user question.\nAs shown in Table 3 we observe clear perfor-\nmance improvements of this approach compared to\nthe baseline when evaluated in Setting 2. Here,\nthe best results are achieved with the BioMed-\nRoBERTa model. In this conﬁguration, the model\nachieves a ROUGE-2-F1 score of 33.71 which rep-\nresents an increase of 1.20 pp compared to the\nvanilla pegasus-large result. Again, the results\nachieved in the ofﬁcial evaluation (Setting 1) show\na different picture. In this setting, the usage of an\nensemble of three interrogative classiﬁcation mod-\nels lowers the performance by 2.6 pp (see Run 3 in\nTable 5).\nWe also investigated the accuracy of the inter-\nrogative prediction models. Table 4 highlights the\nachieved accuracy and macro F1-scores of the\nthree models. All models predict the correct in-\nterrogative for only half of the consumer questions.\nAn analysis of the predictions showed that all mod-\nels are biased towards the majority classes, i.e. in-\nterrogatives with a high support in the training data.\nLike in the generative ensemble setting, we fur-\nther checked the potential performance gains of\nthe interrogative prediction using a perfect classi-\nﬁer. For this, we took the gold standard interroga-\ntive and use the ﬁrst generated summary candidate\nwhich starts with this interrogative as prediction. If\nno generated summary starts with the gold interrog-\native we use the highest ranked candidate. Using\nthis selection scheme we reached an ROUGE-2-F1\nscore of 39.72 in Setting 2 which represents an in-\ncrease by 7.21 pp over the baseline pegasus-large\nmodel. Again, this accentuates the suitability of\nthe proposed approach.\n7https://huggingface.co/\nbert-base-cased\n8https://huggingface.co/dmis-lab/\nbiobert-v1.1\n9https://huggingface.co/allenai/\nbiomed_roberta_base\nModel Accuracy F1\nbert-base 0.530 0.103\nbio-bert 0.525 0.095\nbiomed-roberta 0.555 0.228\nTable 4: Overview of the performance of the three in-\nterrogative classiﬁcation models. For each model we\nreport accuracy and macroF1 score. Bold ﬁgures high-\nlight the highest value per column.\n3.7 Discussion of result differences between\nSetting 1 and Setting 2\nTables 2 and 3 reveal enormous performance dif-\nferences between Setting 1 (the ofﬁcial evaluation\nresults) and Setting 2 (our post-hoc experimental\nanalysis). In Setting 1, none of our proposed exten-\nsions leads to consistent quantitative improvements\nof the results and the best performance is achieved\nby an vanilla generative transformer. In contrast in\nSetting 2, we see (at least) slight beneﬁts from all\nthree strategies.\nExplaining these results and differences is difﬁ-\ncult for several reasons. Concerning Setting 2, the\nhigh variance of the results (see Table 3) prevents\na clear conclusion. Results of the methods vary\nwith different random initializations and are also\nquite sensitive to hyperparameter settings. Often\nthe differences of the methods lie within the range\nof the standard deviation making it unclear whether\nthe ﬁndings would hold up in further analysis or\nother contexts.\nRegarding Setting 1, the small size of the eval-\nuation data (only 100 instances) puts any conclu-\nsions about the quality of the proposed methods\ninto question. In Setting 2, we tried to mitigate the\nproblem of small test data by increasing the num-\nber of test instances, however the results remain\nunstable. Furthermore, weaknesses of the ROUGE\nmetric, e.g. handling of synonyms, abbreviations\nor enumerations, must be taken into account in the\nresult interpretation (Schluter, 2017; Kané et al.,\n2019). The automatic evaluation of generated sum-\nmaries remains a research ﬁeld in itself (Zhang\net al., 2020b). In summary, we neither believe that\nthe results from Setting 1 provide strong evidence\nof the extension’s inappropriateness, nor that the\nresults from Setting 2 allow a convincing statement\nabout their positive effects. To this end, further in-\nvestigation is necessary in order to draw deﬁnitive\nconclusions about our proposed modiﬁcations.\n93\nRun Type Description ROUGE-2 HOLMS BERTScore-F1\n1 B pegasus-large ﬁnetuned on MeQSum and validation\ndata\n16.0 57.7 70.0\n2 B pegasus-large ﬁrst ﬁnetuned on MeQSum and then on\nvalidation data\n12.4 55.5 69.3\n3 IP pegasus-large ﬁnetuned on MeQSum and validation\ndata with ensemble of interrogative predictors consist-\ning of two biobert and one biomed-roberta model\n13.4 56.4 69.0\n4 GD Generator ensemble of bart-base, bart-large, pegasus-\nlarge and pegasus-xsum with one candidate summary\nper model and bert-base as discriminator\n11.8 55.5 68.4\n5 B pegasus-xsum ﬁnetuned on MeQSum and validation\ndata\n12.4 55.5 68.7\n6 GD Same conﬁguration as in run 4 but with an ensemble of\ndiscriminators consisting of bert-base, roberta-base and\nbiobert\n11.4 55.4 68.2\n7 GD pegasus-large trained on MeQSum with ten candidate\nsummaries and a bert-large discriminator trained on\nMeQSum to select the best one\n14.6 57.3 69.8\n8 GD Same conﬁguration as in run 7 but with an ensemble\nof three different bert-large discriminators trained on\nMeQSum\n14.2 57.0 70.2\n9 GD Same conﬁguration as in run 7 but the bert-large dis-\ncriminator is trained on MeQSum and validation data\n12.0 55.4 68.9\n10 GD Same conﬁguration as in run 8 but the the discriminators\nare trained on MeQSum and validation data\n12.0 55.4 69.5\nTable 5: Ofﬁcial results for our submitted runs for subtask one. In total we submitted 10 runs. The runs can\nbe categorized according to their type into baseline models (B), models using interrogative prediction (IP) or the\ngenerator-discriminator framework (GD). The highest value per metric is highlighted in bold. This corresponds to\nour evaluation Setting 1.\n94\n4 Conclusion\nIn this work we investigate the large-scale pre-\ntrained generative transformers PEGASUS and\nBART for the task of health-related consumer ques-\ntion summarization in the context of the MEDIQA\n2021 shared task (Task 1). We propose and evalu-\nate three different strategies, i.e. integrating struc-\ntured knowledge via entity embeddings, utilizing\na generator-discriminator framework and apply-\ning interrogative prediction, to extend these strong\nbaseline models. Our best performing model, a\nﬁne-tuned pegasus-large transformer, reaches an\nROUGE-2-F1 score of 15.99 and is ranked second\nplace in the competition. Experimental results for\nour proposed extensions show a mixed picture and\nfurther analysis is needed to assess the quality of\nthese extensions.\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\nFLAIR: An easy-to-use framework for state-of-the-\nart NLP. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n54–59, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMehdi Allahyari, Seyedamin Pouriyeh, Mehdi Asseﬁ,\nSaeid Safaei, Elizabeth D Trippe, Juan B Gutier-\nrez, and Krys Kochut. 2017. Text summariza-\ntion techniques: a brief survey. arXiv preprint\narXiv:1707.02268.\nAsma Ben Abacha and Dina Demner-Fushman. 2019.\nOn the role of question summarization and informa-\ntion source restriction in consumer health question\nanswering. AMIA Summits on Translational Science\nProceedings, 2019:117.\nAsma Ben Abacha, Yassine Mrabet, Yuhao Zhang,\nChaitanya Shivade, Curtis Langlotz, and Dina\nDemner-Fushman. 2021. Overview of the mediqa\n2021 shared task on summarization in the med-\nical domain. In Proceedings of the 20th SIG-\nBioMed Workshop on Biomedical Language Pro-\ncessing, NAACL-BioNLP 2021 . Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360.\nHassan Kané, Yusuf Kocyigit, Pelkins Ajanoh, Ali Ab-\ndalla, and Mohamed Coulibali. 2019. Towards neu-\nral similarity evaluator. In Workshop on Document\nIntelligence at NeurIPS 2019.\nHalil Kilicoglu, Asma Ben Abacha, Yassine Mrabet,\nSonya E. Shooshan, Laritza Rodriguez, Kate Mas-\nterton, and Dina Demner-Fushman. 2018. Seman-\ntic annotation of consumer health questions. BMC\nBioinform., 19(1):34:1–34:28.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics,\n7:453–466.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 7871–7880. Association for Computational\nLinguistics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018a. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018, pages 1797–1807. Association\nfor Computational Linguistics.\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n2018b. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807.\nRebecca J. Passonneau and Bob Carpenter. 2014. The\nbeneﬁts of a model of annotation. Trans. Assoc.\nComput. Linguistics, 2:311–326.\n95\nNatalie Schluter. 2017. The limits of automatic sum-\nmarisation according to rouge. In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume\n2, Short Papers, pages 41–45.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083.\nLeon Weber, Pasquale Minervini, Jannes Münchmeyer,\nUlf Leser, and Tim Rocktäschel. 2019. Nlprolog:\nReasoning with weak uniﬁcation for question an-\nswering in natural language. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6151–6161.\nLeon Weber, Mario Sänger, Jannes Münchmeyer,\nMaryam Habibi, Ulf Leser, and Alan Akbik. 2021.\nHunFlair: an easy-to-use tool for state-of-the-art\nbiomedical named entity recognition. Bioinformat-\nics. Btab042.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and\nPeter J. Liu. 2020a. PEGASUS: pre-training with\nextracted gap-sentences for abstractive summariza-\ntion. In Proceedings of the 37th International Con-\nference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 ofProceedings\nof Machine Learning Research, pages 11328–11339.\nPMLR.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020b. Bertscore:\nEvaluating text generation with BERT. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8461593389511108
    },
    {
      "name": "Discriminator",
      "score": 0.780343770980835
    },
    {
      "name": "Transformer",
      "score": 0.7560417652130127
    },
    {
      "name": "Computer science",
      "score": 0.726081132888794
    },
    {
      "name": "Generative grammar",
      "score": 0.5994061231613159
    },
    {
      "name": "Test set",
      "score": 0.5632346272468567
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5612305402755737
    },
    {
      "name": "Natural language processing",
      "score": 0.4830901622772217
    },
    {
      "name": "Machine learning",
      "score": 0.4673258364200592
    },
    {
      "name": "Pipeline (software)",
      "score": 0.43361347913742065
    },
    {
      "name": "Parsing",
      "score": 0.412203848361969
    },
    {
      "name": "Engineering",
      "score": 0.09843006730079651
    },
    {
      "name": "Programming language",
      "score": 0.09096553921699524
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Detector",
      "score": 0.0
    }
  ]
}