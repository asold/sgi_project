{
  "title": "Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties",
  "url": "https://openalex.org/W4293506705",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5062667199",
      "name": "Alessio Miaschi",
      "affiliations": [
        "Institute for Computational Linguistics “A. Zampolli”",
        "University of Pisa",
        "University of Groningen"
      ]
    },
    {
      "id": "https://openalex.org/A5042129410",
      "name": "Gabriele Sarti",
      "affiliations": [
        "Institute for Computational Linguistics “A. Zampolli”",
        "University of Pisa",
        "University of Groningen"
      ]
    },
    {
      "id": "https://openalex.org/A5057659257",
      "name": "Dominique Brunato⋄",
      "affiliations": [
        "Institute for Computational Linguistics “A. Zampolli”",
        "University of Pisa",
        "University of Groningen"
      ]
    },
    {
      "id": "https://openalex.org/A5084812833",
      "name": "Felice Dell’Orletta⋄",
      "affiliations": [
        "Institute for Computational Linguistics “A. Zampolli”"
      ]
    },
    {
      "id": "https://openalex.org/A5080111149",
      "name": "Giulia Venturi⋄",
      "affiliations": [
        "Institute for Computational Linguistics “A. Zampolli”"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2529194139",
    "https://openalex.org/W2155870214",
    "https://openalex.org/W2906384214",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2773956126",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W2251234095",
    "https://openalex.org/W2995380724",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W1928863833",
    "https://openalex.org/W3198683886",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4313155193",
    "https://openalex.org/W3160858468",
    "https://openalex.org/W3178340556",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3118017018",
    "https://openalex.org/W3113522843",
    "https://openalex.org/W1909180194",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W3035305735",
    "https://openalex.org/W2990188683",
    "https://openalex.org/W2275897210",
    "https://openalex.org/W2806067357",
    "https://openalex.org/W3172296099",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W3100308117",
    "https://openalex.org/W6770372225",
    "https://openalex.org/W2914924671"
  ],
  "abstract": "In this paper, we present an in-depth investigation of the linguistic knowledge encoded by the transformer models currently available for the Italian language. In particular, we investigate how the complexity of two different architectures of probing models affects the performance of the Transformers in encoding a wide spectrum of linguistic features. Moreover, we explore how this implicit knowledge varies according to different textual genres and language varieties.",
  "full_text": " \nIJCoL\nItalian Journal of Computational Linguistics \n8-1 | 2022\nMiscellanea\nProbing Linguistic Knowledge in Italian Neural\nLanguage Models across Language Varieties\nAlessio Miaschi, Gabriele Sarti, Dominique Brunato, Felice Dell’Orletta and\nGiulia Venturi\nElectronic version\nURL: https://journals.openedition.org/ijcol/965\nDOI: 10.4000/ijcol.965\nISSN: 2499-4553\nPublisher\nAccademia University Press\n \nElectronic reference\nAlessio Miaschi, Gabriele Sarti, Dominique Brunato, Felice Dell’Orletta and Giulia Venturi, “Probing\nLinguistic Knowledge in Italian Neural Language Models across Language Varieties”, IJCoL [Online],\n8-1 | 2022, Online since 01 July 2022, connection on 30 August 2022. URL: http://\njournals.openedition.org/ijcol/965 ; DOI: https://doi.org/10.4000/ijcol.965 \nCreative Commons - Attribution-NonCommercial-NoDerivatives 4.0 International - CC BY-NC-ND 4.0\nhttps://creativecommons.org/licenses/by-nc-nd/4.0/\nProbing Linguistic Knowledge in Italian\nNeural Language Models across Language\nV arieties\nAlessio Miaschi ∗\nUniversità di Pisa;\nILC-CNR, Pisa - ItaliaNLP Lab\nGabriele Sarti ∗∗\nCLCG, University of Groningen\nDominique Brunato †\nILC-CNR, Pisa - ItaliaNLP Lab\nFelice Dell’Orletta ‡\nILC-CNR, Pisa - ItaliaNLP Lab\nGiulia V enturi§\nILC-CNR, Pisa - ItaliaNLP Lab\nIn this paper , we present an in-depth investigation of the linguistic knowledge encoded by the\ntransformer models currently available for the Italian language. In particular , we investigate\nhow the complexity of two different architectures of probing models affects the performance of the\nT ransformers in encoding a wide spectrum of linguistic features. Moreover , we explore how this\nimplicit knowledge varies according to different textual genres and language varieties.\n1. Introduction and Motivation\nIn the last few years, the study of Neural Language Models (NLMs) and their repre-\nsentations has become a key research area in the Natural Language Processing (NLP)\ncommunity . Several methods have been devised to obtain meaningful explanations\nregarding how these models are able to capture syntax- and semantic-sensitive phenom-\nena (Belinkov and Glass 2019). Among them, the probing task approach has emerged as\nthe most commonly adopted diagnostic strategy to estimate the mutual information\nshared by a neural network’s parameters and some latent property that the model\ncould have learned to encode in the training procedure. During probing experiments, a\nsupervised model ( probe) is trained to predict the latent information from the network’s\nlearned representations. If the probe does well, we may conclude that the network\neffectively encodes some knowledge related to the selected property . Formally speak-\ning, let f : xi → yi be a neural network model mapping a corpus of input sentences\nX = (x1, . . . , x n) to a set of target labels Y = (y1, . . . , y n) for a learned downstream\n∗ Department of Computer Science, Università di Pisa; Istituto di Linguistica Computazionale “Antonio\nZampolli”, CNR, Pisa - ItaliaNLP Lab. E-mail: ale.miaschi@gmail.com\n∗∗ Center for Language and Cognition, University of Groningen. E-mail: g.sarti@rug.nl\n† Istituto di Linguistica Computazionale “Antonio Zampolli”, CNR, Pisa - ItaliaNLP Lab.\nE-mail: dominique.brunato@ilc.cnr.it\n‡ Istituto di Linguistica Computazionale “Antonio Zampolli”, CNR, Pisa - ItaliaNLP Lab.\nE-mail: felice.dellorletta@ilc.cnr.it\n§ Istituto di Linguistica Computazionale “Antonio Zampolli”, CNR, Pisa - ItaliaNLP Lab.\nE-mail: giulia.venturi@ilc.cnr.it\n© 2022 Associazione Italiana di Linguistica Computazionale\nItalian Journal of Computational Linguistics V olume 8, Number 1\ntask. Assume that each sentence xi is also labeled with some linguistic annotations zi,\nreﬂecting the underlying properties we aim to detect. Let also hl(xi) be the network’s\noutput at the l-th layer given the sentence xi as input. T o estimate the quality of rep-\nresentations hl with respect to property z, a supervised model g : hl(xi) → zi mapping\nrepresentations to property values is trained. W e take such model’s performances as a\nproxy of H(hl(x), z ). In information theoretic terms, the probe is trained to minimize\nentropy H(z|hl(x)), and by doing that it maximizes mutual information between the\ntwo quantities.\n(Alain and Bengio 2017) were among the ﬁrst to use linear probing classiﬁers\nas tools to evaluate the presence of task-speciﬁc information inside neural networks’\nlayers. The approach was later extended to the ﬁeld of NLP by (Conneau et al. 2018)\nand (Zhang and Bowman 2018) inter alia , which evaluated the presence of semantic\nand syntactic information inside sentence embeddings generated by LSTM encoders\n(Hochreiter and Schmidhuber 1997) pretrained on different objectives using probing\ntask suites.\nNowadays, several studies adopt the probing task approach to investigate the\ninner working of state-of-the-art Neural Language Models (NLMs). This approach\ndemonstrated that NLMs representations encode linguistic knowledge in a hierarchical\nmanner (Belinkov et al. 2017; Blevins, Levy , and Zettlemoyer 2018; T enney et al. 2019),\nand can even support the extraction of dependency parse trees (Hewitt and Manning\n2019). (Jawahar , Sagot, and Seddah 2019) investigated the representations learned by\nBERT (Devlin et al. 2019), one of the most prominent NLM, across its layers, showing\nthat lower ones are usually better for capturing surface features, while embeddings from\nhigher layers are better for syntactic and semantic properties. Using a suite of probing\ntasks, (T enney , Das, and Pavlick 2019) deeply explore this behavior showing that the\nlinguistic knowledge encoded by BERT through its 12/24 layers follows the traditional\nNLP pipeline.\nWhile the vast majority of this research is focused on English contextual represen-\ntations, relatively little work has been done to understand the inner working of non-\nEnglish models. The study by (de V ries, van Cranenburgh, and Nissim 2020) represents\nan exception in this context: the authors applied the probing task approach to compare\nthe linguistic competence encoded by a Dutch BERT-based model and multilingual\nBERT (mBERT), showing that earlier layers of mBERT are consistently more informative\nthat earlier layers of the monolingual model. (Guarasci et al. 2021) applied instead the\nstructural probe originally deﬁned by (Hewitt and Manning 2019) on the representa-\ntions of a pre-trained Italian BERT . T esting their approach on different subsets of the\nItalian Universal Dependency T reebank (IUDT), they showed on the one hand that the\nmodel is able to encode properties of syntax especially in its central-upper layers; on\nthe other hand, that such embedded syntactic information can be used to successfully\nperform two speciﬁc syntactic tasks, i.e. prediction of Subject-V erb agreement and\nparsing of null-subject sentences. In (Guarasci et al. 2022), the authors exploited the\nsame methodology to investigate the ability of multilingual BERT to transfer syntactic\nknowledge across the English, French and Italian languages.\nAnother less investigated issue in the previous studies has to do with the design of\nprobing models themselves. Although many studies have focused on multiple trans-\nformer models and diagnostic tasks to probe their inner linguistic competence, few\nworks tested different probing architectures and investigated more in-depth their actual\neffectiveness. Among this few works, (Hewitt and Liang 2019) were the ﬁrst who ob-\nserved that probing tasks might conceal the information about the NLM representation\nbehind the ability of the probe to learn surface patterns in the data. T o test this idea, they\n26\nMiaschi et al. Probing Italian Neural Language Models\nintroduced control tasks , a set of tasks that associate word types with random outputs\nthat can be solved by simply learning regularities. In addition, (Pimentel et al. 2020)\nshowed that more complex probes, in contrast with simple linear models, could produce\ntighter estimates for the actual underlying information.\nStarting from these premises, this paper introduces an approach to NLMs inter-\npretation aimed at carrying out an in-depth investigation of the linguistic knowledge\nimplicitly encoded by 6 Italian monolingual models and multilingual BERT . Besides\nthe focus on Italian, which represents a scarcely considered language in the scenario of\nthe NLM interpretation studies, a further novelty of our approach concerns the broad\nset of probing tasks we took into account, each corresponding to a speciﬁc property\nof sentence structure. In addition, the present study is one of the few that introduces\na still rather under-investigated research issue, i.e. the comparative analysis of how\nand to which extent the different architectures on which the probing model rely on\ninﬂuence the probing accuracy . T o address this point, for each T ransformer , we perform\nthe same suite of probing tasks using both a LinearSVR and a multilayer perceptron\n(MLP), and compare how each probing task’s resolution is differently affected by the\ntwo architectures. Since all experiments were carried out on different sections of the\nItalian Universal Dependency T reebank (Zeman et al. 2019) considered as representative\nof different textual genres and language varieties, we are also able to investigate how\nlinguistic knowledge of NLMs varies according to standard and less or non-standard\nvarieties of the Italian language.\nThe present article is based on, and extends, the work reported in (Miaschi et al.\n2020b).\nContributions. T o the best of our knowledge, this is the ﬁrst study aimed at comparing\nthe linguistic knowledge encoded in the representations of multiple non-English pre-\ntrained transformer models. In particular:\nr we compare the probing performances of 7 pre-trained Italian NLMs\nspanning three models architectures over multiple linguistic features;\nr we investigate how the complexity of the probing classiﬁer impacts its\nability to capture the information encoded in learned representations;\nr we highlight how the implicit knowledge encoded by NLMs during the\ntraining process differs across textual genres and language varieties.\n2. Approach\nT o inspect the inner knowledge of language encoded by the Italian T ransformers, we\nrelied on a suite of 82 probing tasks, each of which consists in predicting the value of a\ngiven feature modeling a speciﬁc linguistic property of the sentence. W e tested two dif-\nferent probing architectures: a LinearSVR and a three-layer feedforward network with\nReLU activations (Multi-layer perceptron, MLP). If the linear architecture is the most\ncommonly used approach to infer information inside NLMs, the MLP was selected to\ninvestigate the presence of nonlinear relations in representations, which could hamper\nthe probing performance of the LinearSVR probe. Regardless of the architecture, the two\nprobing models take as input layer-wise sentence-level representations extracted from\nthe Italian models. These representations are produced for each sentence of different\nsections of the Italian Universal Dependency T reebank (IUDT), version 2.5 (Zeman et\nal. 2019), and used to predict the actual value of each probing feature. Starting from the\n27\nItalian Journal of Computational Linguistics V olume 8, Number 1\nT able 1\nNLMs used in the experiments.\nName T raining data\nBERT Architecture\nMultilingual-BERT Wikipedia\nBERT-base-italian Wikipedia + OPUS (13GB)\nAlBERT o TWIT A (191GB)\nRoBERT a Architecture\nGilBERT o OSCAR (71GB)\nUmBERT o-Commoncrawl OSCAR (69GB)\nUmBERT o-Wikipedia Wikipedia (7GB)\nGPT -2 Architecture\nGePpeTto Wikipedia + ItW AC (14GB)\nresults obtained we performed three complementary investigations. In the ﬁrst one we\ncompared the results obtained by the two probing architectures according to different\ngroups of probing tasks (Section 3.1). Then, we move to compare the linguistic compe-\ntence of the 7 Italian T ransformers (Section 3.2). Finally , the impact of the considered\nlinguistic varieties on the linguistic generalization abilities of the NLMs is discussed in\nSection 3.3.\n2.1 Models and Data\nW e relied on 7 pre-trained Italian models based on three different T ransformer archi-\ntectures: BERT (Devlin et al. 2019), RoBERT a (Liu et al. 2019b) and GPT-2 (Radford\net al. 2019). In particular , we investigated the linguistic competence of: three BERT-\nbased models, i.e. Multilingual-BERT , BERT-base-italian 1 and AlBERT o (Polignano et\nal. 2019), trained respectively on Wikipedia (102 languages), Italian Wikipedia + texts\nfrom the OPUS corpus (Tiedemann and Nygaard 2004) and TWIT A (Basile, Lai, and\nSanguinetti 2018); three RoBERT a-based models, i.e. GilBERT o 2 and two versions of\nUmBERT o3 , trained respectively on OSCAR (Ortiz Suárez, Sagot, and Romary 2019)\n(GilBERT o and UmBERT o-Commoncrawl) and Italian Wikipedia; a GPT-2 based model,\nGePpeTto (De Mattei et al. 2020), trained on Italian Wikipedia + ItW AC (Baroni et al.\n2009). Models statistics are reported in T able 1. Sentence level representations were\ncomputed performing a Mean-pooling operation over the word embeddings provided\nby the models across their layers.\nNLM’s linguistic competences are probed against 5 sections of the Italian Universal\nDependency T reebank (IUDT) representative of different language varieties and textual\ngenres, as shown in T able 2. The considered sections can be categorised in two main\ngroups: a ﬁrst one that includes sentences acquired from documents of diverse nature,\nranging from Wikipedia pages, to newspaper articles, novels, speech transcriptions,\netc., and a second group collecting examples of the social media language, in particular\nof T witter . In the ﬁrst group we included the Italian version of the multilingual T urin\n1 https://github.com/dbmdz/berts\n2 https://github.com/idb-ita/GilBERT o\n3 https://github.com/musixmatchresearch/umberto\n28\nMiaschi et al. Probing Italian Neural Language Models\nT able 2\nSections of the Italian Universal Dependency T reebank (IUDT).\nShort Name T ypes of texts # sent\nParTUT Multi-genre 2,090\nVIT Multi-genre 10,087\nISDT Multi-genre 14,167\nISDT_tanl Newswire 4,043\nISDT_tut Legal/Newswire/Wiki 3,802\nISDT_quest Interrogative sentences 2,162\nISDT_2parole Simpliﬁed Italian news 1,421\nISDT_europarl EU Parliament debates 497\nPoSTWIT A T weets 6,713\nTWITTIRÒ Ironic T weets 1,424\nT otal 35,481\nUniversity Parallel T reebank (ParTUT) (Sanguinetti and Bosco 2015), the V enice Italian\nT reebank (VIT) (Delmonte, Bristot, and T onelli 2007) and Italian Stanford Dependency\nT reebank (ISDT) (Bosco, Montemagni, and Simi 2013), which we considered represen-\ntative of the standard Italian language. The group of treebanks composed of PoSTWIT A\n(Sanguinetti et al. 2018) and TWITTIRÒ (Cignarella, Bosco, and Rosso 2019) was orig-\ninally built to enhance the performances of systems in processing social media texts,\nand in particular , for irony detection purposes. Being representative of a non-standard\nvariety of the Italian language, for our speciﬁc scopes, they are intended to be a quite\nchallenging testbed for probing the linguistic knowledge of NLMs also when they are\ntrained on standard language variety .\nNote that the linguistic abilities of the 7 NLMs were also tested against a number of\nsub-portions of the largest Italian UD treebank, i.e. ISDT . They have been chosen since\nthey are representative of language sub-varieties possibly infrequently seen during the\nNLMs training phase. Accordingly , they can be conceived as a favorite point of view\nto investigate to which extent general-purpose NLMs are robust against less standard\ntexts. For this purpose, in addition to sub-sections including newspapers (ISDT_tanl)\nand miscellaneous documents (ISDT_tut), we considered sub-portions including sen-\ntences in the interrogative form (ISDT_quest), newspaper articles speciﬁcally written to\nbe linguistically simple (ISDT_2parole) and transcriptions of the European parlament\noral debates (ISDT_europarl).\n2.2 Probing features\nThe set of probing tasks consists in predicting the value of a speciﬁc linguistic feature\nautomatically extracted from the manually revised annotation of each sentence of the\nIUDT datasets.\nW e relied on the set described in (Brunato et al. 2020) that includes about 130\nfeatures representative of the linguistic structure underlying a sentence and derived\nfrom raw , morpho-syntactic and syntactic levels of annotation. For the speciﬁc purpose\nof this study , we selected the 82 most frequent features in order to prevent data sparsity\nissues thus making our results reliable.\n29\nItalian Journal of Computational Linguistics V olume 8, Number 1\nT able 3\nProbing Features used in the experiments.\nLinguistic Feature Label\nRaw T ext Properties ( RawT ext)\nSentence Length sent_length\nW ord Length char_per_tok\nV ocabulary Richness ( V ocabulary)\nT ype/T oken Ratio for words and lemmas ttr_form, ttr_lemma\nMorphosyntactic information ( POS)\nDistribution of UD and language–speciﬁc POS upos_dist_*, xpos_dist_*\nLexical density lexical_density\nInﬂectional morphology ( V erbInﬂection )\nInﬂectional morphology of lexical verbs and aux-\niliaries\nverbs_*, aux_*\nV erbal Predicate Structure ( V erbPredicate)\nDistribution of verbal heads and verbal roots verbal_head_dist, verbal_root_perc\nV erb arity and distribution of verbs by arity avg_verb_edges, verbal_arity_*\nGlobal and Local Parsed T ree Structures ( T reeStructure)\nDepth of the whole syntactic tree parse_depth\nA verage length of dependency links and of the\nlongest link\navg_links_len, max_links_len\nA verage length of prepositional chains and distri-\nbution by depth\navg_prep_chain_len, prep_dist_1\nClause length avg_token_per_clause\nOrder of elements ( Order)\nRelative order of subject and object subj_pre, subj_post, obj_post\nSyntactic Relations ( SyntacticDep)\nDistribution of dependency relations dep_dist_*\nUse of Subordination ( Subord)\nDistribution of subordinate clauses subordinate_prop_dist\nA verage length of subordination chains and distri-\nbution by depth\navg_subord_chain_len, subordinate_dist_1\nRelative order of subordinate clauses subordinate_post\nAs shown in T able 3, the considered tasks are intended to probe whether the NLMs\nencode in their representations 9 main aspects of the structure of a sentence. They\nrange from quite simple aspects related to the knowledge of raw text properties (i.e.\nsentence and word length), to the vocabulary richness (in terms of type/token ratio),\nto the distribution of UD and language-speciﬁc Parts-Of-Speech 4 and of inﬂectional\nproperties speciﬁc in particular to verbal predicates (i.e. mood, tense, person). More\nchallenging probing tasks concern the ability to encode complex aspects of sentence\nstructure, including both global structure, such as the depth of the whole syntactic tree,\nand local features. W e paid a speciﬁc attention to testing the models knowledge of the\nsub-trees of the nuclear elements of a sentence. In this respect, we included a group of\nfeatures modelling the verbal predicate structure, e.g. in terms of number of dependents\nof verbal heads, and a group referring to the order of subjects and objects with respect to\ntheir verbal head. In line with the focus on speciﬁc sub-trees, we also considered a group\n4 For the list of UD Parts-Of-Speech refer to https://universaldependencies.org/u/pos/index.html, while\nfor the language-speciﬁc one to http://www .italianlp.it/docs/ISST-T ANL-POStagset.pdf\n30\nMiaschi et al. Probing Italian Neural Language Models\nT able 4\nA verage R2 scores for all the NLMs obtained with the LinearSVR and the MLP probing models.\nBaseline scores for a Linear SVR trained only on sentence length are also reported.\nGroups LinearSVR MLP Baseline\nRawT ext 0.84 0.80 0.50\nV ocabulary 0.70 0.34 0.19\nPOS 0.69 0.68 0.03\nV erbInﬂection 0.50 0.61 0.03\nV erbPredicate 0.32 0.43 0.08\nT reeStructure 0.61 0.64 0.40\nOrder 0.46 0.55 0.06\nSyntacticDep 0.65 0.74 0.04\nSubord 0.49 0.60 0.16\nAllFeatures 0.60 0.64 0.10\nof features capturing the use of subordination in terms of distribution of subordinate\nclauses, of their internal structure and relative order with respect to the main clause.\nW e chose to rely on these linguistic characteristics for two main reasons. Firstly ,\nthey have been shown to be highly predictive when leveraged by traditional learning\nmodels on a variety of classiﬁcation problems where the linguistic information plays a\nfundamental role. In addition, they are multilingual as they are based on the Universal\nDependency formalism for sentence representation, which guarantees the comparative\nencoding of language phenomena across different languages (Nivre 2015). In fact, they\nhave been also used to proﬁle the knowledge encoded in the language representations\nof a pretrained NLM, speciﬁcally the English BERT , and how it changes across layers\n(Miaschi et al. 2020a).\n3. Experiments and Results\nIn this section we report the results of the three different investigations we carried out\nstarting from the probing strategies devised.\n3.1 Comparison of Probing Model Architectures\nOur ﬁrst analysis concerns the comparison of the two considered architectures for\nprobing the linguistic knowledge encoded by the Italian T ransformers. Since many of\nour probing features are strongly related to sentence length, we compared these results\nwith the ones obtained by a baseline corresponding to a LinearSVR model trained\nusing only sentence length as input feature. T able 4 reports average R2 results5 across\nall the layers of all the 7 NLMs obtained with the LinearSVR and the MLP probing\narchitectures, along with baseline scores.\nAs a ﬁrst remark, we notice that both probing architectures outperform the sentence\nlength baseline. This suggests that all NLMs encode a spectrum of phenomena that,\n5 The Coefﬁcient of determination ( R2) is a statistical measure of how close the data are to the ﬁtted\nregression line and corresponds to the proportion of the variance in the dependent variable that is\npredictable from the independent variable(s).\n31\nItalian Journal of Computational Linguistics V olume 8, Number 1\nalthough related to syntagmatic complexity , require a more sophisticated linguistic\nknowledge to be accurately predicted. However , if we compare the results achieved\nby the two architectures on all groups of linguistic phenomena ( AllFeatures), we can see\nthat MLP architecture achieves higher R2 scores. This is speciﬁcally the case of the group\nof features which refer to characteristics of the verb inﬂectional morphology ( V erbIn-\nﬂection ) and structure ( V erbPredicate) and the use of subordination ( Subord), for which\nthe differences between the two architectures is higher . On the contrary , the LinearSVR\nresulted to be more accurate to probe NLMs’ competences of raw text properties, vo-\ncabulary richness and about the distribution of Parts-Of-Speech. Interestingly , the SVR\narchitecture outperforms the MLP by more than .30 R2 points when predicting features\nrelated to vocabulary richness ( V ocabulary). The increase in performances observed for\nthe MLP model on syntactic features can be motivated by the presence of nonlinearities\nin the probing model, which allow the model to capture non-linear relations between\nlearned features. On the other hand, this increase in model capacity seems to hinder the\nperformances of the probe on low level features ( RawT ext, V ocabulary, POS) for which\na simple linear combination can be sufﬁcient. Despite this difference, a comparison of\nthe rankings of linguistic phenomena ordered by decreasing scores for the two probing\nmodels shows that in both cases raw text properties and the distribution of morpho-\nsyntactic categories ( POS) appear in the ﬁrst positions, while the order of subject and\nobject ( Order) and the structure of verbal predicates ( V erbPredicate) are found in the\nlower part of the ranking. This observation suggests that the hierarchy of linguistic\ninformation captured by probing models is preserved, regardless of the architectural\ncomplexity of the probe. As a matter of fact, if we compute the Spearman correlation ( ρ)\nbetween the average scores obtained for the 82 linguistic features with the LinearSVR\nand MLP we obtained a ρ of 0.71, thus indicating a strong correlations between the\nscores obtained with the two probing models.\nIn order to ensure that our probes are actually showing the linguistic generaliza-\ntion abilities of the NLMs rather than learning the linguistic tasks, we also tested the\nprobing models using the control task approach devised in (Hewitt and Liang 2019). W e\nproduced a control version of the IUDT corpus by randomly shufﬂing the linguistic\nfeatures assigned to each sentence and performed the same probing tasks with the two\nprobing classiﬁers for all NLMs representations. The correlation and R2 scores between\nregressors’ predictions and shufﬂed scores were low ( < 0. 05) and comparable for both\nthe SVR and the MLP . These results support the claim that NLMs representations en-\ncode information closely related to linguistic competence and that our probing models\nare not relying on spurious signals unrelated to our linguistic properties to solve the\nregression task.\n3.2 Comparison of Italian T ransformers\nT o investigate to which extent each transformer encodes the considered set of linguistic\nphenomena, we compared the performances achieved by the 7 NLMs, using the two\nprobing architectures. Results are reported in Figure 1, where we can notice that the\n7 T ransformers achieve quite similar results when considering all features as a whole\n(all). Nevertheless, a more in depth analysis highlights a number of small differences.\nNamely , we can see that BERT-base-italian is the ﬁrst and second best model for the MLP\nand SVR architecture respectively; while the least performing model is AlBERT o using\nMLP and, for the SVR probing architecture, UmBERT o model trained on the Italian\nWikipedia.\n32\nMiaschi et al. Probing Italian Neural Language Models\nFigure 1\nLayer-wise average R2 scores obtained by each NLM with the two probing models.\nHowever , this trend does not hold when we analyse the NLMs performances with\nrespect to the encoding of the different groups of linguistic phenomena. For instance, we\ncan notice that, for the two probing architectures, tree structure properties ( T reeStruc-\nture) are predicted more accurately by RoBERT a-style models, i.e. by GilBERT o and\nUmBERT o-Commoncrawl, than by models based on BERT or GPT-2. Only for MLP , this\ncan be similarly observed for the prediction of two other linguistic properties referring\nto sub-trees of the whole syntactic structure of a sentence. Namely , it can be seen that\nGilBERT o and UmBERT o-Commoncrawl are the two best models able to encode the use\n33\nItalian Journal of Computational Linguistics V olume 8, Number 1\nFigure 2\nA verage layerwise R2 scores obtained with the LinearSVR ( top) and the MLP ( bottom) using the\ninternal representations of the 7 NLMs.\nof subordination ( Subord) and the verb predicate structures ( V erbPredicate). Further dif-\nferences in terms of probing architectures can be inspected considering NLMs abilities\nto encode competencies related to vocabulary richness ( V ocabulary): while UmBERT o-\nWikipedia extensively outperforms all the other transformers using the MLP model,\nthe best transformer is BERT-base-italian when these competences are probed with the\nLinearSVR model.\nAdditional observations can be made if we move to the analysis of how NLMs pre-\ndiction abilities change and evolve across layers. As it can be seen in Figure 2, regardless\nof the architecture, for all transformers linguistic competences tend to decrease across\nthe 12 layers. This is in line with previous ﬁndings (Liu et al. 2019a; Miaschi et al. 2020a)\nand it could be due to the fact that transformer layers trade off between task-oriented\n(e.g. Masked Language Modeling) information and general linguistic competence. Such\ndecreasing trend can be speciﬁcally observed for example for the ability to predict\nraw text features, or the distribution of the UD morpho-syntactic categories ( POS)\nand syntactic dependencies ( SyntacticDep): they represent sentence properties mainly\n34\nMiaschi et al. Probing Italian Neural Language Models\nencoded in the ﬁrst layers by all NLMs. On the contrary , we can observe that there is\na number of more complex linguistic features whose knowledge increases consistently\nacross layers, even if it decreases in the output layer . This is the case of features referring\nto structural sentence knowledge, such as the order of subject/object with respect to\nthe verbal head ( Order) and the use of subordination ( Subord). In addition, contrarily\nto what was observed by (de V ries, van Cranenburgh, and Nissim 2020), Multilingual-\nBERT’s linguistic knowledge is not encoded systematically earlier than in monolingual\ntransformers.\nThis perspective of analysis also reveals other differences among the considered\ntransformers which were unseen. By inspecting the trend of the R2 scores across layers,\nwe can for example see that even though GePpeTto has a lower average competence\non verb inﬂection (see Figure 1), it achieves the highest scores in the middle layers.\nOr , even if we previously noted that RoBERT a-style transformers are more able to\npredict features related to the structure of a sentence ( T reeStructure), the highest accuracy\nis achieved by a BERT-style model, i.e. BERT-base-italian, in the -4 layer . A similar\nobservation also concerns the use of subordination and the verb predicate structure:\nthe two groups of features are in general predicted more accurately by GilBERT o and\nUmBERT o-Commoncrawl but the highest R2 scores are achieved by Mulilingual-BERT\nand BERT-base-italian in the -5 and -4 layers.\nFocusing instead on differences between layerwise scores obtained by the two\nprobing architectures, we can clearly notice that the encoding of linguistic knowledge\nshows a quite rough trend for what concerns the results obtained with the MLP . This\nis particularly the case of features belonging to the vocabulary , POS and tree structure\ngroups.\nIf we deepen our investigation and we focus on the linguistic generalization ability\nof the NLMs with respect to each individual feature (see Figure 3), we can clearly\nobserve that the rankings according to R2 scores are quite similar regardless the prob-\ning architecture and the transformer model. It is also interesting to note that, despite\nsome deviations, the distinction into macro-groups of linguistic phenomena seems to\nbe mostly preserved across the rankings. In fact, raw-text features, as well as the dis-\ntributions of POS-tags ( upos_dist_*, xpos_dist_*) and dependency relations ( dep_dist_*),\nare those that were better predicted by the two probing models, while features more\nrelated to the structural information of a sentence, such as the order of elements (e.g.\nsubj_pre, subj_post and obj_post) or the structure of parsed tree (e.g. avg_token_per_clause,\navg_prep_chain_len) achieved lower probing scores. Lower results also concern the pre-\ndiction of the morphological features of lexical and auxiliary verbs, namely for example\ntheir mood ( verb_mood_*) or tense ( verb_tense_*).\nIn line with what observed in Figure 1, we can see that in few cases the linguistic\ncompetence of the AlBERT o model is signiﬁcantly different (lower) from that of the\nother models. The most remarkable case concerns the distribution of punctuation marks\nin general, both at the level of morpho-syntactic category ( upos_dist_PUNCT), depen-\ndency relation ( dep_dist_punct), and more speciﬁcally considering the distribution of\ncommas ( xpos_dist_FF) and balanced punctuation ( xpos_dist_FB). This appears particu-\nlarly evident using MLP as probing architecture and it is possibly related to the typology\nof texts the AlBERT o model was trained on, i.e. T witter . It is well known that social\nmedia represents a non standard language variety , characterised by speciﬁc linguistic\nproperties mostly different from ordinary language (Farzindar and Inkpen 2015), such\nas short sentences where punctuation marks, especially weak ones, are rarely used.\nAccordingly , the low frequency of punctuation in the training corpus possibly yields\nAlBERT o’s reduced generalization abilities with respect to this speciﬁc set of features.\n35\nItalian Journal of Computational Linguistics V olume 8, Number 1\nFigure 3\nA verage R2 scores obtained for each probing features using the two probing architectures tested\nwith the internal representations of the 7 NLMs. Both heatmaps are ordered on the basis of the\nfeature ranking as predicted by the AlBERT o model using the LinearSVR architecture.\n36\nMiaschi et al. Probing Italian Neural Language Models\nFigure 4\nA verage LinearSVM R2 score considering all the UD Italian sentences ( all) and according to the\n10 treebanks previously described.\n3.3 Comparison of Italian Language V arieties\nOur last analysis concerns the impact of the considered Italian language varieties on\nNLMs linguistic abilities. For this purpose, we inspected whether the overall linguistic\ncompetence encoded in the contextual representations of each model changes according\nto the different IUDT sections. The results reported in Figure 4 show that all trans-\nformers, regardless of the probing architecture, achieve lower performance when they\nhave to predict the value of features extracted from treebanks representative of social\nmedia language (PoSTWIT A and TWITTIRÒ) and from the sub-set of ISDT sentences in\nthe interrogative form (ISDT_quest). In both cases, this seems supporting our starting\n37\nItalian Journal of Computational Linguistics V olume 8, Number 1\nintuition that NLMs trained on standard language varieties, represented for example\nby Wikipedia pages, websites or web-crawled documents, may be less robust to non-\nstandard varieties that were possibly unseen, or rarely seen, during the pre-training\nprocess. Quite surprisingly , even if AlBERT o has been trained on T witter data, it obtains\nthe lowest R2 scores also when its internal representations are used to predict the\nfeature values of the two social media Italian treebanks. A possible explanation is\nthat, although PoSTWIT A and TWITTIRÒ contain sentences representative of T witter\nlanguage, these sentences are still quite close to the Italian standard language, in order\nto be compliant with the UD morpho-syntactic and syntactic annotation schema. On\nthe contrary , AlBERT o’s training set is derived from T witter ’s ofﬁcial streaming API\nthat included all possible typologies of sentences.\nIt also worth noting that BERT-base-italian and GePpeTto are the two models\nslightly less affected by the non-standard linguistic peculiarities of the social media\nvariety . As noted in Section 3.2, they represent the two best performing models in\nterms of overall linguistic competence. This may explain why they are more robust\nin the accurate prediction of the features values of all the considered IUDT sections.\nThis holds both with the LinearSVR and MLP probing architecture, even if in the\nlatter case the two versions of UmBERT o achieve comparable or slightly better scores.\nA main exception is represented by the ISDT sub-section including sentences in the\ninterrogative form (ISDT_quest), which, as we noted above, are hardly mastered by\nall models. This is possible due to the fact that interrogative sentences are more likely\nto display a less canonical distribution of morpho-syntactic and syntactic phenomena,\nhence being more difﬁcult to encode effectively . In this case, the transformer based on\nGPT-2, i.e. GePpeTto, results to be the NLM with the highest linguistic knowledge of\nthis type of sentences.\nA further analysis of the impact of language varieties on the ability of NLMs to\nencode the considered group of linguistic phenomena can be appreciated in T able 5. It\nshows, for each probing architecture, the Spearman correlations between the rankings\nof features predicted by all NLMs considering three ISDT sub-sections, i.e. ISDT_tanl,\nISDT_2parole and ISDT_quest, and PoSTWIT A, and ordered by decreasing R2 scores.\nFor each NLM, higher correlations correspond to similar linguistic generalization abil-\nities across the paired treebanks, while lower correlations suggest that the inner repre-\nsentations of the NLM allow predicting effectively diverse linguistic features. As we can\nsee, regardless of the probing architecture, for all NLMs, the highest correlated rankings\nare those obtained comparing ISDT_tanl ( tanl) and PoSTWIT A ( ptw) predicted features.\nEven if it is quite surprising, this result can be explained assuming that the morpho-\nsyntactic and syntactic features of the T witter sentences contained in PoSTWIT A are\nnot so dramatically different from those characterising ISDT_tanl newspaper articles.\nIn fact, among all the IUDT sections considered here they resulted to be the two most\nsimilar treebanks with respect to the distribution of the set of linguistic features reported\nin T able 3. In particular , the main differences concern the distribution of some morpho-\nsyntactic categories (e.g. punctuation, nouns) and main features related to the inﬂec-\ntional morphology of verbs, e.g. the distribution of present tenses, higher in PoSTWIT A\n(51.11% out of the total verb tenses) than in ISDT_tanl (34.95%), or of the past tenses\nthat in the T witter sentences are less than half than in the newspaper ones. Interestingly ,\nthese characteristics belong to the group of features that the NLMs are able to master\nquite accurately , regardless of the language variety . Even if these differences had a neg-\native impact on the overall probing abilities of the PoSTWIT A sentence characteristics,\nas shown in Figure 4, the higher knowledge of these speciﬁc features did not possibly\n38\nMiaschi et al. Probing Italian Neural Language Models\nT able 5\nSpearman correlations between rankings of features as predicted by the 7 NLMs on four sections\nof the IUDT treebank: IUDT_2parole ( 2par), IUDT_tanl ( tanl), IUDT_quest ( quest) and\nIUDT_postwita ( ptw). Highest correlations are bolded, while lowest ones are marked in italics.\nModel Section LinearSVR MLP\n2par tanl quest ptw 2parole tanl quest ptw\nalberto\n2par 1 1\ntanl .72 1 .85 1\nquest .38 .38 1 .62 .56 1\nptw .76 .82 .45 1 .75 .80 .58 1\nbert-base-italian\n2par 1 1\ntanl .68 1 .82 1\nquest .34 .41 1 .62 .47 1\nptw .72 .91 .47 1 .75 .88 .47 1\ngeppetto\n2par 1 1\ntanl .65 1 .80 1\nquest .30 .38 1 .64 50 1\nptw .70 .92 .48 1 .72 .88 .47 1\ngilberto\n2par 1 1\ntanl .61 1 .77 1\nquest .30 .40 1 .58 54 1\nptw .66 .88 .46 1 .69 .82 .49 1\nmbert\n2par 1 1\ntanl .65 1 .76 1\nquest .30 .37 1 .55 .47 1\nptw .71 .90 .45 1 .71 .83 .46 1\numberto-commoncr .\n2par 1 1\ntanl .58 1 .71 1\nquest .28 .33 1 .55 .47 1\nptw .69 .8 .39 1 .65 .75 .35 1\numberto-wikipedia\n2par 1 1\ntanl .57 1 .70 1\nquest - - 1 .50 .44 1\nptw .66 .72 .36 1 .69 .72 .36 1\nhave a great consequence on the ranking of the predicted features, thus yielding quite\nhigh correlations.\nOn the contrary , the lowest correlations can be observed when we compare the\nrankings obtained for the pairs of treebanks containing the set of sentences in the inter-\nrogative form, i.e. ISDT_quest ( quest). Even if the correlation values are slightly higher\nusing MLP , this trend holds for the two probing architectures and for all NLMs. Note\nthat the correlations between the ranking obtained with UmBERT o-Wikipedia for the\npairs ISDT_quest/ISDT_2parole and ISDT_quest/ISDT_tanl are even not statistically\nsigniﬁcant. Let us remind that this is the NLM that achieved the lowest prediction\naccuracy using the LinearSVR probing architecture (see Figure 1). Our intuition is that\nthis may have made it less robust in the prediction of non-standard linguistic forms,\nsuch as interrogative sentences. Similarly to what aforementioned, these results can\nbe explained if we analyse the feature values in the considered treebanks. ISDT_quest\n39\nItalian Journal of Computational Linguistics V olume 8, Number 1\nresulted to be quite different from all the other treebanks particularly with respect to\ncomplex aspects of sentence structure. For example, the canonical order of the nuclear\nelements of a sentence (i.e. subject and object) is largely subverted in sentences in the\ninterrogative form. Thus, they contain a very high percentage of post-verbal explicit\nsubjects (68.69% of the total), half an order of magnitude higher than ISDT_tanl (15.21%)\nand PoSTWIT A (12.63%) and an order of magnitude higher than ISDT_2parole (7.55%).\nSentences in the interrogative form also have a lower percentage of post-verbal objects\n(17.31%), which instead represent the majority of cases in other treebanks, and they\nare characterised by a very low distribution of subordinate clauses in general and in\nparticular of subordinates following the principal clause, i.e. 4% vs. 43% in ISDT_tanl,\n35.78% ISDT_2parole and 44.36%. These and other similar features all concern structural\naspects of a sentence that may have undermined the overall NLM linguistic competence\nthus yielding not only lower probing scores on ISDT_quest but also different feature\nrankings with respect to the other treebanks.\n4. Conclusion\nIn this paper we presented an in-depth comparative investigation of the linguistic\nknowledge encoded in the Italian transformer models. Relying on a suite of 82 prob-\ning features and on two different probing architectures, we performed a number of\ncomplementary investigations all tested on different sections of the Italian Universal\nDependency T reebank (IUDT), representative of diverse textual genres and language\nvarieties.\nFirstly , we showed experimentally how non-linear architectures such as the multi-\nlayer perceptron (MLP) capture a broader range of information encoded in learned rep-\nresentations with respect to their linear counterparts, and as such they can be considered\nmore suitable for studying highly nonlinear models such as NLM. In this sense, our\nresults support the information-theoretic operationalization of probing proposed by (Pi-\nmentel et al. 2020). However , the rankings of this and of the LinearSVR model in terms\nof their probing ability are quite similar . Namely , both are particularly able to probe raw\ntext properties, as well as the distribution of Parts-Of-Speech and dependency relations;\nwhile they obtained lower scores for features referring to the order of subject and object\nwith respect their verbal head and to the verbal predicate structure.\nThe following comparison of the linguistic generalization abilities of the 7 T rans-\nformers showed that if we analyse the results considering all the probing features as a\nwhole few differences can be observed. Similarly to what observed for English (Liu et\nal. 2019a) and Dutch (de V ries, van Cranenburgh, and Nissim 2020), we showed that\nregardless of the probing architecture, for all transformers the internal layers (i.e. -6/-4)\nare the most informative ones and the linguistic competences tend to decrease across\nthe 12 layers. However , contrary to (de V ries, van Cranenburgh, and Nissim 2020) our\nﬁndings reveal that Multilingual-BERT’s linguistic knowledge is not encoded systemat-\nically earlier than in monolingual transformers. More interesting outcomes result when\nwe focus on the embedded knowledge of each group of linguistic characteristics. W e\nnoticed for example that global and local tree structure properties are predicted more\naccurately by RoBERT a-style models, i.e. by GilBERT o and UmBERT o-Commoncrawl,\nthan by models based on BERT or GPT-2. W e obtained additional information when\nwe narrowed our analysis on how NLMs prediction abilities evolve across models’\nlayers, showing for example that the highest competence about the tree structure is\nachieved by a BERT-style model, i.e. BERT-base-italian, in the -4 layer . A more in-depth\ncomparison with respect to the ranking of each individual feature by R2 scores also\n40\nMiaschi et al. Probing Italian Neural Language Models\nrevealed that, even if the 7 T ransfomers are quite similar , a main exception is represented\nby the AlBERT o model. In particular , it showed to have reduced generalization abilities\nconcerning the use of punctuation. Our intuition is that it is possibly related to the\ntypology of texts the AlBERT o model was trained on, i.e. T witter , where punctuation\nmarks are rarely used.\nFinally , we showed that the level of NLMs linguistic competence changes according\nto the diverse linguistic varieties of IUDT . All T ransformers resulted to be less robust\nin the prediction of the linguistic properties characterising sentences representative\nof social media language and of sentences in the interrogative form. This is possible\ndue to the fact that the two types of sentences are characterised by non-canonical\ndistribution of morpho-syntactic and syntactic phenomena, possibly rarely or never\nseen during the training phase. Surprisingly , also the AlBERT o model, even if it was\ntrained on T witter data, achieved very low performances, while on the contrary , BERT-\nbase-italian and GePpeTto are the two models slightly less affected by the non-standard\nlinguistic varieties. Despite both social media and questions seem representing two\nquite challenging testbeds, our in-depth investigation of how each probing feature is\nranked by the NLMs allowed highlighting noteworthy differences. W e observed that\nthe most diverse rankings concern the test on the sentences in the interrogative form,\nwhich result to be characterised by distributions of structural aspects very different from\nother IUDT sections.\nIn terms of present and future research directions, we are currently investigating\nhow the relation between the linguistic knowledge encoded by a NLM positively\naffects the resolution of downstream tasks, following recent works highlighting the\ntendency of pretrained NLMs to lose general linguistic information during the ﬁne-\ntuning process and the connection between encoded linguistic information and models’\ndownstream performances for the English language (Miaschi et al. 2020a; Sarti, Brunato,\nand Dell’Orletta 2021). These connections, which are still sporadically investigated at\nthe moment, can cast a light on the decision process inside NLMs, and ultimately lead\nto an improved understanding and utilization of these systems for real-world usage.\nReferences\nAlain, Guillaume and Y oshua Bengio. 2017. Understanding intermediate layers using linear\nclassiﬁer probes. In Workshop T rack of the Fifth International Conference on Learning\nRepresentations (ICLR 2017) , T oulon, France, April.\nBaroni, Marco, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide\nweb: a collection of very large linguistically processed web-crawled corpora. Language\nresources and evaluation , 43(3):209–226.\nBasile, V alerio, Mirko Lai, and Manuela Sanguinetti. 2018. Long-term social media data\ncollection at the university of Turin. In T ommaso Caselli, Nicole Novielli, Viviana Patti, and\nPaolo Rosso, editors, Proceedings of the Fifth Italian Conference on Computational Linguistics\n(CLiC-it 2018) , volume 2263, pages 1–6, T urin, Italy , December . CEUR W orkshop Proceedings.\nBelinkov , Y onatan and James Glass. 2019. Analysis Methods in Neural Language Processing: A\nSurvey. T ransactions of the Association for Computational Linguistics , 7:49–72, 04.\nBelinkov , Y onatan, Lluís Màrquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass.\n2017. Evaluating layers of representation in neural machine translation on part-of-speech and\nsemantic tagging tasks. In Proceedings of the Eighth International Joint Conference on Natural\nLanguage Processing (V olume 1: Long Papers) , pages 1–10, T aipei, T aiwan, November . Asian\nFederation of Natural Language Processing.\nBlevins, T erra, Omer Levy , and Luke Zettlemoyer . 2018. Deep RNNs encode soft hierarchical\nsyntax. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(V olume 2: Short Papers) , pages 14–19, Melbourne, Australia, July . Association for\nComputational Linguistics.\n41\nItalian Journal of Computational Linguistics V olume 8, Number 1\nBosco, Cristina, Simonetta Montemagni, and Maria Simi. 2013. Converting Italian treebanks:\nT owards an Italian Stanford dependency treebank. In Proceedings of the 7th Linguistic\nAnnotation Workshop and Interoperability with Discourse , pages 61–69, Soﬁa, Bulgaria, August.\nAssociation for Computational Linguistics.\nBrunato, Dominique, Andrea Cimino, Felice Dell’Orletta, Giulia V enturi, and Simonetta\nMontemagni. 2020. Proﬁling-ud: a tool for linguistic proﬁling of texts. In Proceedings of The 12th\nLanguage Resources and Evaluation Conference , pages 7147–7153, Marseille, France, May .\nEuropean Language Resources Association.\nCignarella, Alessandra T eresa, Cristina Bosco, and Paolo Rosso. 2019. Presenting TWITTIRÒ-UD:\nAn Italian Twitter treebank in Universal Dependencies. In Proceedings of the Fifth International\nConference on Dependency Linguistics (Depling, SyntaxFest 2019) , pages 190–197, Paris, France,\nAugust. Association for Computational Linguistics.\nConneau, Alexis, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni.\n2018. What you can cram into a single $&!#* vector: Probing sentence embeddings for\nlinguistic properties. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (V olume 1: Long Papers) , pages 2126–2136, Melbourne, Australia, July .\nAssociation for Computational Linguistics.\nDe Mattei, Lorenzo, Michele Cafagna, Felice Dell’Orletta, Malvina Nissim, and Marco Guerini.\n2020. GePpeTto carves italian into a language model. In Felice Dell’Orletta, Johanna Monti,\nand Fabio T amburini, editors, Proceedings of the Seventh Italian Conference on Computational\nLinguistics (CLiC-it 2020) , pages 136–143, Bologna, Italy (Online), March.\nde V ries, Wietse, Andreas van Cranenburgh, and Malvina Nissim. 2020. What’s so special about\nBERT’s layers? a closer look at the NLP pipeline in monolingual and multilingual models. In\nFindings of the Association for Computational Linguistics: EMNLP 2020 , pages 4339–4350, Online,\nNovember . Association for Computational Linguistics.\nDelmonte, Rodolfo, Antonella Bristot, and Sara T onelli. 2007. VIT - Venice Italian Treebank:\nSyntactic and quantitative features. In Proceedings of the Sixth International Workshop on\nT reebanks and Linguistic Theories , Bergen, Norway , August.\nDevlin, Jacob, Ming-W ei Chang, Kenton Lee, and Kristina T outanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage T echnologies, V olume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis,\nMinnesota, June. Association for Computational Linguistics.\nFarzindar , Atefeh and Diana Inkpen. 2015. Natural Language Processing for Social Media . Synthesis\nLectures on Human Language T echnologies, Morgan & Claypool.\nGuarasci, Raffaele, Stefano Silvestri, Giuseppe De Pietro, Hamido Fujita, and Massimo Esposito.\n2021. Assessing BERT’s ability to learn italian syntax: a study on null-subject and agreement\nphenomena. Journal of Ambient Intelligence and Humanized Computing , pages 1–15.\nGuarasci, Raffaele, Stefano Silvestri, Giuseppe De Pietro, Hamido Fujita, and Massimo Esposito.\n2022. Bert syntactic transfer: A computational experiment on italian, french and english\nlanguages. Computer Speech & Language , 71.\nHewitt, John and Percy Liang. 2019. Designing and interpreting probes with control tasks. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\n2733–2743, Hong Kong, China, November . Association for Computational Linguistics.\nHewitt, John and Christopher D. Manning. 2019. A structural probe for ﬁnding syntax in word\nrepresentations. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language T echnologies, V olume 1 (Long and Short\nPapers), pages 4129–4138, Minneapolis, Minnesota, June. Association for Computational\nLinguistics.\nHochreiter , Sepp and Jürgen Schmidhuber . 1997. Long short-term memory . Neural computation ,\n9(8):1735–1780.\nJawahar , Ganesh, Benoît Sagot, and Djamé Seddah. 2019. What does BERT learn about the\nstructure of language? In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 3651–3657, Florence, Italy , July . Association for Computational\nLinguistics.\nLiu, Nelson F ., Matt Gardner , Y onatan Belinkov , Matthew E. Peters, and Noah A. Smith. 2019a.\nLinguistic knowledge and transferability of contextual representations. In Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational Linguistics:\n42\nMiaschi et al. Probing Italian Neural Language Models\nHuman Language T echnologies, V olume 1 (Long and Short Papers) , pages 1073–1094, Minneapolis,\nMinnesota, June. Association for Computational Linguistics.\nLiu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy , Mike\nLewis, Luke Zettlemoyer , and V eselin Stoyanov . 2019b. RoBERTa: A robustly optimized bert\npretraining approach. arXiv preprint arXiv:1907.11692 .\nMiaschi, Alessio, Dominique Brunato, Felice Dell’Orletta, and Giulia V enturi. 2020a. Linguistic\nproﬁling of a neural language model. In Proceedings of the 28th International Conference on\nComputational Linguistics , pages 745–756, Barcelona, Spain (Online), December . International\nCommittee on Computational Linguistics.\nMiaschi, Alessio, Gabriele Sarti, Dominique Brunato, Felice Dell’Orletta, and Giulia V enturi.\n2020b. Italian transformers under the linguistic lens. In Johanna Monti, Felice Dell’Orletta, and\nFabio T amburini, editors, Proceedings of the Seventh Italian Conference on Computational\nLinguistics (CLiC-it 2020) , Online, March 2021. CEUR.org.\nNivre, Joakim. 2015. T owards a universal grammar for natural language processing. In\nAlexander Gelbukh, editor , Computational linguistics and intelligent text processing , pages 3–16,\nNew Y ork. Springer .\nOrtiz Suárez, Pedro Javier , Benoît Sagot, and Laurent Romary . 2019. Asynchronous pipelines for\nprocessing huge corpora on medium to low resource infrastructures. In Piotr Ba ´ nski, Adrien\nBarbaresi, Hanno Biber , Evelyn Breiteneder , Simon Clematide, Marc Kupietz, Harald Lüngen,\nand Caroline Iliadi, editors, Proceedings of the Workshop on Challenges in the Management of Large\nCorpora (CMLC-7) 2019 , pages 9 – 16, Cardiff, 22nd July . Leibniz-Institut für Deutsche Sprache.\nPimentel, Tiago, Josef V alvoda, Rowan Hall Maudslay , Ran Zmigrod, Adina Williams, and Ryan\nCotterell. 2020. Information-theoretic probing for linguistic structure. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics , pages 4609–4622, Online, July .\nAssociation for Computational Linguistics.\nPolignano, Marco, Pierpaolo Basile, Marco de Gemmis, Giovanni Semeraro, and V alerio Basile.\n2019. Alberto: Italian bert language understanding model for nlp challenging tasks based on\ntweets. In Raffaella Bernardi, Roberto Navigli, and Giovanni Semeraro, editors, Proceedings of\nthe Sixth Italian Conference on Computational Linguistics (CLiC-it 2019) , Bari, Italy , November .\nRadford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever . 2019.\nLanguage models are unsupervised multitask learners. T echnical report.\nSanguinetti, Manuela and Cristina Bosco. 2015. PartTUT: The turin university parallel treebank.\nIn Roberto Basili et al., editor , Harmonization and Development of Resources and T ools for Italian\nNatural Language Processing within the P ARLI Project . Springer , pages 51–69.\nSanguinetti, Manuela, Cristina Bosco, Alberto Lavelli, Alessandro Mazzei, Oronzo Antonelli,\nand Fabio T amburini. 2018. PoSTWIT A-UD: an Italian Twitter treebank in Universal\nDependencies. In Proceedings of the Eleventh International Conference on Language Resources and\nEvaluation (LREC 2018) , Miyazaki, Japan, May . European Language Resources Association\n(ELRA).\nSarti, Gabriele, Dominique Brunato, and Felice Dell’Orletta. 2021. That looks hard:\nCharacterizing linguistic complexity in humans and language models. In Proceedings of the\nWorkshop on Cognitive Modeling and Computational Linguistics , pages 48–60, Online, June.\nAssociation for Computational Linguistics.\nT enney , Ian, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages\n4593–4601, Florence, Italy , July . Association for Computational Linguistics.\nT enney , Ian, Patrick Xia, Berlin Chen, Alex W ang, Adam Poliak, R. Thomas McCoy , Najoung\nKim, Benjamin V an Durme, Samuel R. Bowman, Dipanjan Das, et al. 2019. What do you learn\nfrom context? Probing for sentence structure in contextualized word representations. In\nProceedings of the Seventh International Conference on Learning Representations (ICLR 2019) , New\nOrleans, Louisiana, USA, May .\nTiedemann, Jörg and Lars Nygaard. 2004. The OPUS corpus - parallel and free:\nhttp://logos.uio.no/opus. In Proceedings of the Fourth International Conference on\nLanguage Resources and Evaluation (LREC’04) , Lisbon, Portugal, May . European Language\nResources Association (ELRA).\nZeman, Daniel, Joakim Nivre, Mitchell Abrams, and al. 2019. Universal dependencies 2.5. In\nLINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚF AL) .\nZhang, Kelly and Samuel Bowman. 2018. Language modeling teaches you more than translation\ndoes: Lessons learned through auxiliary syntactic task analysis. In Proceedings of the 2018\n43\nItalian Journal of Computational Linguistics V olume 8, Number 1\nEMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages\n359–361, Brussels, Belgium, November . Association for Computational Linguistics.\n44",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6994231939315796
    },
    {
      "name": "Computer science",
      "score": 0.6416951417922974
    },
    {
      "name": "Linguistics",
      "score": 0.6048020124435425
    },
    {
      "name": "Deep linguistic processing",
      "score": 0.473730206489563
    },
    {
      "name": "Language model",
      "score": 0.4667898118495941
    },
    {
      "name": "Linguistic description",
      "score": 0.4610286355018616
    },
    {
      "name": "Natural language processing",
      "score": 0.45736345648765564
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4427541494369507
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4105263948440552
    },
    {
      "name": "Engineering",
      "score": 0.09054407477378845
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}