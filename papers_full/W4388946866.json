{
  "title": "Comparison between vision transformers and convolutional neural networks to predict non-small lung cancer recurrence",
  "url": "https://openalex.org/W4388946866",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2068099172",
      "name": "Annarita Fanizzi",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A2740233810",
      "name": "Federico Fadda",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A2885387708",
      "name": "Maria Colomba Comes",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A3123701309",
      "name": "Samantha Bove",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A2893536902",
      "name": "Annamaria Catino",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A5113053064",
      "name": "Erika Di Benedetto",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A5111079268",
      "name": "Angelo Milella",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A2020740717",
      "name": "Michele Montrone",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A2571409620",
      "name": "Annalisa Nardone",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A5093326245",
      "name": "Clara Soranno",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A2141776516",
      "name": "Alessandro Rizzo",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A2734761626",
      "name": "Deniz Can Guven",
      "affiliations": [
        "Hacettepe University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2147571854",
      "name": "Domenico Galetta",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A1452643473",
      "name": "Raffaella Massafra",
      "affiliations": [
        "Istituto Tumori Bari"
      ]
    },
    {
      "id": "https://openalex.org/A2068099172",
      "name": "Annarita Fanizzi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2740233810",
      "name": "Federico Fadda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2885387708",
      "name": "Maria Colomba Comes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3123701309",
      "name": "Samantha Bove",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2893536902",
      "name": "Annamaria Catino",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5113053064",
      "name": "Erika Di Benedetto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5111079268",
      "name": "Angelo Milella",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A2020740717",
      "name": "Michele Montrone",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2571409620",
      "name": "Annalisa Nardone",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093326245",
      "name": "Clara Soranno",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2141776516",
      "name": "Alessandro Rizzo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2734761626",
      "name": "Deniz Can Guven",
      "affiliations": [
        "Hacettepe University Hospital",
        "Hacettepe University"
      ]
    },
    {
      "id": "https://openalex.org/A2147571854",
      "name": "Domenico Galetta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1452643473",
      "name": "Raffaella Massafra",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2106787323",
    "https://openalex.org/W2107896071",
    "https://openalex.org/W2575493793",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2763355946",
    "https://openalex.org/W3135096391",
    "https://openalex.org/W2991000712",
    "https://openalex.org/W3205076722",
    "https://openalex.org/W3177894684",
    "https://openalex.org/W4281955677",
    "https://openalex.org/W2436233456",
    "https://openalex.org/W3161491505",
    "https://openalex.org/W4310273766",
    "https://openalex.org/W4367669251",
    "https://openalex.org/W4223435421",
    "https://openalex.org/W3081081614",
    "https://openalex.org/W2809785120",
    "https://openalex.org/W4221112170",
    "https://openalex.org/W2794284562",
    "https://openalex.org/W2910121883",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W4308119975",
    "https://openalex.org/W4313577028",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W4214634256",
    "https://openalex.org/W4312820606",
    "https://openalex.org/W4312847199",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W4310024325",
    "https://openalex.org/W4286355566",
    "https://openalex.org/W4285018316",
    "https://openalex.org/W4301392676",
    "https://openalex.org/W4321375308",
    "https://openalex.org/W4286437542",
    "https://openalex.org/W4361226205",
    "https://openalex.org/W4313889909",
    "https://openalex.org/W4294252561",
    "https://openalex.org/W3166591035",
    "https://openalex.org/W4214842939",
    "https://openalex.org/W2896246551",
    "https://openalex.org/W3134258455",
    "https://openalex.org/W3165409352",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2152575748",
    "https://openalex.org/W4200535752",
    "https://openalex.org/W4226085666",
    "https://openalex.org/W3100321043",
    "https://openalex.org/W4213099919",
    "https://openalex.org/W2884561390"
  ],
  "abstract": "Abstract Non-Small cell lung cancer (NSCLC) is one of the most dangerous cancers, with 85% of all new lung cancer diagnoses and a 30–55% of recurrence rate after surgery. Thus, an accurate prediction of recurrence risk in NSCLC patients during diagnosis could be essential to drive targeted therapies preventing either overtreatment or undertreatment of cancer patients. The radiomic analysis of CT images has already shown great potential in solving this task; specifically, Convolutional Neural Networks (CNNs) have already been proposed providing good performances. Recently, Vision Transformers (ViTs) have been introduced, reaching comparable and even better performances than traditional CNNs in image classification. The aim of the proposed paper was to compare the performances of different state-of-the-art deep learning algorithms to predict cancer recurrence in NSCLC patients. In this work, using a public database of 144 patients, we implemented a transfer learning approach, involving different Transformers architectures like pre-trained ViTs, pre-trained Pyramid Vision Transformers, and pre-trained Swin Transformers to predict the recurrence of NSCLC patients from CT images, comparing their performances with state-of-the-art CNNs. Although, the best performances in this study are reached via CNNs with AUC, Accuracy, Sensitivity, Specificity, and Precision equal to 0.91, 0.89, 0.85, 0.90, and 0.78, respectively, Transformer architectures reach comparable ones with AUC, Accuracy, Sensitivity, Specificity, and Precision equal to 0.90, 0.86, 0.81, 0.89, and 0.75, respectively. Based on our preliminary experimental results, it appears that Transformers architectures do not add improvements in terms of predictive performance to the addressed problem.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:20605  | https://doi.org/10.1038/s41598-023-48004-9\nwww.nature.com/scientificreports\nComparison between vision \ntransformers and convolutional \nneural networks to predict \nnon‑small lung cancer recurrence\nAnnarita Fanizzi 1,8, Federico Fadda 1,8, Maria Colomba Comes 1,8*, Samantha Bove 1*, \nAnnamaria Catino 2, Erika Di Benedetto 3, Angelo Milella 4, Michele Montrone 2, \nAnnalisa Nardone 5, Clara Soranno 1, Alessandro Rizzo 6, Deniz Can Guven 7, \nDomenico Galetta 2 & Raffaella Massafra 1\nNon‑Small cell lung cancer (NSCLC) is one of the most dangerous cancers, with 85% of all new lung \ncancer diagnoses and a 30–55% of recurrence rate after surgery. Thus, an accurate prediction of \nrecurrence risk in NSCLC patients during diagnosis could be essential to drive targeted therapies \npreventing either overtreatment or undertreatment of cancer patients. The radiomic analysis of \nCT images has already shown great potential in solving this task; specifically, Convolutional Neural \nNetworks (CNNs) have already been proposed providing good performances. Recently, Vision \nTransformers (ViTs) have been introduced, reaching comparable and even better performances \nthan traditional CNNs in image classification. The aim of the proposed paper was to compare the \nperformances of different state‑of‑the‑art deep learning algorithms to predict cancer recurrence in \nNSCLC patients. In this work, using a public database of 144 patients, we implemented a transfer \nlearning approach, involving different Transformers architectures like pre‑trained ViTs, pre‑trained \nPyramid Vision Transformers, and pre‑trained Swin Transformers to predict the recurrence of NSCLC \npatients from CT images, comparing their performances with state‑of ‑the‑art CNNs. Although, the \nbest performances in this study are reached via CNNs with AUC, Accuracy, Sensitivity, Specificity, \nand Precision equal to 0.91, 0.89, 0.85, 0.90, and 0.78, respectively, Transformer architectures reach \ncomparable ones with AUC, Accuracy, Sensitivity, Specificity, and Precision equal to 0.90, 0.86, \n0.81, 0.89, and 0.75, respectively. Based on our preliminary experimental results, it appears that \nTransformers architectures do not add improvements in terms of predictive performance to the \naddressed problem.\nNon-small cell lung cancer (NSCLC) represents the most frequent form of lung cancer, treated mainly with sur-\ngery and modern  radiotherapy1–3. Therapeutic approaches for NSCLC patients differ according tothe histological \ncharacteristics of the tumor and the patient’s condition. The treatment path for patients with locally advanced \nNSCLC currently includes chemoradiotherapy possibly followed by immunotherapy. For early-stage patients, \nhowever, surgical resection followed by chemotherapy currently remains the only potentially curative treatment. \nNonetheless, 30–55% of these patients develop post-resection tumor recurrence within the first 5  years2. There-\nfore, the early identification of patients most prone to developing a recurrence is a challenge that is currently still \nopen and would allow clinicians to plan a more accurate therapeutic surveillance plan.\nOPEN\n1Struttura Semplice Dipartimentale Fisica Sanitaria, I.R.C.C.S. Istituto Tumori ‘Giovanni Paolo II’, Viale Orazio Flacco \n65, 70124 Bari, Italy. 2Unità Operativa Complessa di Oncologia Toracica, I.R.C.C.S. Istituto Tumori ‘Giovanni Paolo \nII’, Viale Orazio Flacco 65, 70124 Bari, Italy. 3Unità Operativa Complessa di Oncologia Medica, I.R.C.C.S. Istituto \nTumori ‘Giovanni Paolo II’, Viale Orazio Flacco 65, 70124 Bari, Italy. 4Dipartimento di ElettronicaInformazione e \nBioingegneria, Politecnico di Milano, Via Giuseppe Ponzio, 34, 20133 Milan, Italy. 5Unità Operativa Complessa \ndi Radioterapia, I.R.C.C.S. Istituto Tumori ‘Giovanni Paolo II’, Viale Orazio Flacco 65, 70124 Bari, Italy. 6Unità \nOperativa Complessa di Oncologia Medica ‘Don Tonino Bello’, I.R.C.C.S. Istituto Tumori ‘Giovanni Paolo II’, Viale \nOrazio Flacco 65, 70124 Bari, Italy. 7Department of Medical Oncology, Hacettepe University Cancer Institute, \n06100 Sihhiye, Ankara, Turkey. 8 These authors contributed equally: Annarita Fanizzi, Federico Fadda and Maria \nColomba Comes. *email: m.c.comes@oncologico.bari.it; s.bove@oncologico.bari.it\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:20605  | https://doi.org/10.1038/s41598-023-48004-9\nwww.nature.com/scientificreports/\nSeveral works have been proposed on the prediction of recurrence-free survival and overall survival in \nNSCLC patients. However, the state-of-the-art is lacking of models designed for the early prediction of disease \nrecurrence. Furthermore, although all proposed models show encouraging results, they are still not suitable for \na clinical application, even when they involve genomic-based models which are expensive and time-consuming \nprocedures. In recent years, artificial intelligence has already demonstrated its potential in defining predictive \nand prognostic models. Specifically, the predictive power of radiomic features extracted from biomedical images \nis now well established in the scientific  community4–8.\nRecently, radiomics via Convolutional Neural Networks (CNNs) has been extensively used showing strong \n potential5–20. CNNs can be of two types: custom or pre-trained. In the former, scientists build their own network \nwhich is then trained to execute a specific task; in the latter case, a transfer-learning approach is  used15–20. Net-\nworks are first trained on millions of images of different classes (e.g., ImageNet) in recognizing specific patterns \nlike edges, dots, color gradients, shapes, etc. 21. After that, this gained knowledge is transferred to the specific \nset of images to study. In this work, we adopted only the transfer learning approach. Typically, CNNs consist of \nseveral layers of convolutions and max pooling. When applied to images, the bottom layers (close to the input \nlayer) focus on local simple features like edges, dots, and color gradients; higher layers, instead, combine the \nprevious features into more complex ones and can be used to train Machine Learning models.\nHowever, CNNs require high computational resources; second, they focus more on the entire image instead \nof its portions which could contain the  lesion22, 23.\nIn 2020, the first ViT architecture was introduced and after that, a variety of different architectures \n appeared24–40. Differently from CNNs, ViTs consist of a small number of layers and can decompose the image in \npatches gaining information with the attention  mechanism37–40. They turned out to reach promising performances \neven outperforming traditional  CNNs22, 23, 41–48.\nIn this scenario, in light of innovative algorithms proposed in the literature, the aim of our work was to \ncompare the performances of different state-of-the-art deep learning algorithms to predict disease recurrence \nin NSCLC patients. To the best of our knowledge, the state-of-the-art lacks a comparative study on the classifica-\ntion performances obtained by these two architectural families in relation to the problem of disease recurrence \nprediction evaluated on the same reference dataset. This information would allow us to lay the foundations for \nfuture studies aimed at defining and validating an accurate model of personalized medicine. Therefore, in this \npreliminary work, we used various Transformer architectures to predict NSCLC  recurrence14, 49–52. We used a \npublic database of CT images of 144 NCSLC patients for recurrence classification comparing the performances \nof ViTs and  CNNs53. The paper is organized as follows: in Section “ Results” , Materials and Methods, we intro-\nduce the database of patients and the network architectures; then, in Sections “Discussion and conclusion” and \n“Materials and methods” , Results and Discussion, we present the results of our transfer-learning-based model, \ndiscussing their performances.\nResults\nThe performances of diverse Transformer families are summarized in the radar plot of Figs. 1, 2, and 3: ViTb_32 \nand ViTb16 (Fig. 1a,b), PVT-B1 and PVT-B0 (Fig. 2a,b), Swin-tiny and Swin-small (Fig. 3a,b).\nAmong all the structures evaluated for this family of architectures, PTV_B1 shows the best performance \n(Fig. 2). It was highly performing with an AUC value, accuracy sensitivity, specificity and precision of 0.90 ± 0.04, \n0.86 ± 0.04, 0.81 ± 0.12, 0.89 ± 0.07, and 0.75 ± 0.11 respectively.\nOn the other hand, performances of CNNs are shown in the radar plots of Fig.  4. InceptionV3 (Fig.  4b) \noutperformed the other structures by achieving an AUC value, accuracy, sensitivity, specificity, and precision of \n0.91 ± 0.03, 0.89 ± 0.04, 0.85 ± 0.05, 0.90 ± 0.06 and 0.78 ± 0.10, respectively.\nAs additional result, Fig. 5 shows a histogram of the validation loss values, averaged over all the epochs, folds \nand rounds of cross-validation, for ViTs, PVTs, Swins and CNNs. ViTb_16, PVT-B1, Swin-tiny, and InceptionV3 \nFigure 1.  Radar plots of the performances AUC, Accuracy (Acc), Sensitivity (Sens), Specificity (Spe), and \nPrecision (Pre) of ViTb_32 (a) and ViTb_16 (b). For each metric, the mean value, among all the cross-validation \n20 rounds, is shown with its standard deviation.\n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:20605  | https://doi.org/10.1038/s41598-023-48004-9\nwww.nature.com/scientificreports/\nshow the lowest validation loss in the histogram within their family. The best trade-off between the performances \nachieved and the loss valued was reached by InceptionV3.\nDiscussion and conclusion\nThe a m of the study was to evaluate the performances of different deep learning algorithms for predicting recur-\nrence in NSCLC patients by analyzing baseline CT. Our experimental results showed that ViTb_16, has higher \nperformances, reaching an AUC and Accuracy values of 0.84 ± 0.04 and 0.83 ± 0.05, respectively, against ViTb_32, \nvalues equal to 0.67  ± 0.05 and 0.64  ± 0.08 respectively due to their different architectures. Indeed, ViTb_16 \ndecomposes the input images into patches of size 16 × 16 pixels, while ViTb_32 into patches of size 32 × 32 pixels. \nTherefore, if the patch size is smaller, the transformer encoder’s attention would be higher, bringing to a better \nFigure 2.  Radar plots of the performances AUC, Accuracy (Acc), Sensitivity (Sens), Specificity (Spe), and \nPrecision (Pre) of PVT-B1 (a) and PVT-B0 (b). For each metric, the mean value, among all the cross-validation \n20 rounds, is shown with its standard deviation.\nFigure 3.  Radar plots of the performances AUC, Accuracy (Acc), Sensitivity (Sens), Specificity (Spe), and \nPrecision (Pre) of Swin-tiny (a) and Swin-small (b). For each metric, the mean value, among all the cross-\nvalidation 20 rounds, is shown with its standard deviation.\nFigure 4.  Radar plots of the performances AUC, Accuracy (Acc), Sensitivity (Sens), Specificity (Spe), and \nPrecision (Pre) of three CNNs: ResNet50 (a), InceptionV3 (b), and DenseNet201 (c). For each metric, the mean \nvalue, among all the cross-validation 20 rounds, is shown with its standard deviation.\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:20605  | https://doi.org/10.1038/s41598-023-48004-9\nwww.nature.com/scientificreports/\nclassification. As regards the Swin cases, both Swin-tiny and Swin-small are comparable (AUC = 0.82 ± 0.04 \nand 0.80 ± 0.07; Accuracy = 0.79 ± 0.04 and 0.77 ± 0.04 respectively). The best performances among the consid -\nered Transformers techniques are reached with PVT-B1 with the AUC and Accuracy value of 0.90 ± 0.04 and \n0.86 ± 0.04 respectively. These better performances, among all considered Transformers, could depend on the \nPVT overlapping patch embedding mechanism allowing the Transformer to extract more information from the \nCT image than ViTs and  Swins29. In the end, the best performances of this study are reached via pre-trained CNN \nInceptionV3 with AUC and Accuracy equal to 0.91 ± 0.03 and 0.89 ± 0.04 respectively. Even if CNNs perform \nbest, the considered Transformers ViTs, PVTs and Swins still reach high and comparable performances.\nAs regards the topic of NSCLC classification, we scanned the literature and, to the best of our knowledge, we \nidentified the state-of-the-art works which mainly use clinical features or radiomic ones. The latter can be further \nsplit into handcrafted features or extracted via  CNNs14, 49–52. To the best of our knowledge, we use pre-trained \nViTs, PVTs, and Swins for the first time, for the specific task of NSCLC classification. Table  1 summarizes the \nprincipal results proposed in the state-of-the-art according to the topic of our clinical task.\nS. Hindocha et al. predicted recurrence, recurrence-free survival, and overall survival of NSCLC patients, \nemploying only clinical features from a cohort of 657 patients. As regards the task of recurrence prediction, \nan AUC equals to 0.69 was  reached51. In the work of Wang et al., for example, CT images from a cohort of 157 \nNSCLC patients were analyzed using only handcrafted-radiomic features reaching an a ccuracy equals to 0.8552.\nAs regards NSCLC recurrence radiomic studies based on deep learning models, we mention the works of \nAonpong et al., Kim et al., and Bove et al. 14, 49, 50. In the former, Authors used a subsample of our same radiog-\nenomic database to predict the NSCLC recurrence implementing a genotype-guided radiomic model focusing \non a smaller cohort of 88  patients50. Using various state-of-the-art CNNs, gene expression data were extracted \nfrom CT images achieving an AUC equals to 0.77, and a ccuracy equals to 0.83. In the second one, Kim et al. 49 \nbuilt various ensemble-based prediction models using a database of 326 patients including our one. Clinical data, \nhandcrafted radiomic features, and deep learning radiomic ones were considered and combined with each other. \nThe best performances combining all together were AUC equals to 0.77, and Accuracy equals to 0.73. Finally, \nin the work of Bove et al. a transfer learning approach was implemented extracting radiomic features from the \ncropped CT images, around the tumor area, of our same NSCLC radiogenomic  dataset53 via pre-trained CNNs, \nFigure 5.  Example of the training loss function and validation loss plots as a function of the 30 epochs of \ntraining (a). Histogram of the validation loss values, averaged over all the epochs, rounds, and folds of the cross-\nvalidation for ViTs, PVTs, Swins, and CNNs (b).\nTable 1.  Table of the state-of-the-art performances achieved in previous works about NSCLC recurrence \nprediction.\nN. of patients Dataset Model Performances\nWang et al.51 157 Private Handcrafted Radiomic features based Acc = 0.85\nAonpong et al.50 88 Public CNN + gene-expression based AUC = 0.77\nAcc = 0.83\nKim et al.49 326 Public CNN based + Handcrafted Radiomic based + Clinical based AUC = 0.77\nAcc = 0.73\nHindocha et al.52 657 Private Clinical based AUC = 0.69\nBove et al.14 144 Public CNN based + Clinical based AUC = 0.83\nAcc = 0.79\nOur proposed model Public CNN + Transformer based AUC = 0.91\nAcc = 0.89\nOur proposed model 144 ViT + Transformer based AUC = 0.90\nAcc = 0.86\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:20605  | https://doi.org/10.1038/s41598-023-48004-9\nwww.nature.com/scientificreports/\nreducing the number of radiomic features and combining them with the clinical data of the database. The best \nreached performances consisted of AUC and Accuracy equal to 0.83 and 0.79  respectively14.\nConsidering all the results, in our model pre-trained CNN InceptionV3 seems to outperform the state-of-\nthe-art works on NSCLC recurrence classification topic.\nWe would like to underline that the comparison with the state of the art is purely naïve. Unfortunately, the \nworks proposed in the literature on the same clinical task have often been developed starting from private data-\nsets. Even when they use the same public dataset to which we referred, the authors integrated the public data with \nprivate data (as in the work presented by Kim et al.49), without then differentiating the results obtained, or selected \na subset of data according to certain criteria, which could be compatible with the objective of our work (as for the \nwork presented by Aonpong et al.50). Therefore, it is difficult to make objective comparisons on the same dataset.\nHowever, our model still suffers from some limitations. Indeed, although a data augmentation technique \nhas been used to reinforce the training of the last layers of the pre-trained networks used, the obtained perfor -\nmances are strongly influenced by the retrospective nature and small dimension of the dataset. Specifically, the \nmodel needs to be validated in a more robust manner also using an external validation set, preferably referring \nto a sample of private data, although the use of a public database as is known allows an objective comparison \nof the proposed methods. Therefore, for the future, we intend to collect a larger database of NSCLC patients \nto validate and optimize the proposed models; moreover, we will also evaluate other public dataset to test the \nobtained results. Another possible future direction in the research would include a further investigation of more \nTransformer architectures and their correspondent performances. Moreover, further studies could include both \ncombined deep radiomic and clinical features to train suitable Machine Learning classifiers to predict NSCLC \nrecurrence after years with the help of the Explainable Artificial Intelligence (XAI) to detect the most relevant \nand decisive features for the  prediction54, 55.\nMaterials and methods\nExperimental dataset\nIn our work, we used a public radiogenomics dataset of NSCLC available in the Cancer Imaging Archive \n(TCIA)53. The public database consisted of 211 subjects divided into two sub-cohorts:\n(1) The R01 cohort with 162 patients (38 females and 124 males, age at scan: mean 68, range: 42–86) from Stan-\nford University School of Medicine (69) and Palo Alto Veterans Affairs Healthcare System (93) recruited \nbetween April 7ths 2008 and September 15th, 2012;\n(2) The second AMC cohort consisting of 49 additional subjects (33 females, 16 males, age at scan: mean 67, \nrange 24–80) was retrospectively collected from Stanford University School of Medicine based on the same \ncriteria.\nWe chose to focus only on the (1) sub-cohort R01 because they had both tumor segmentation binary masks \nand the axial CT available. Among the 162 patients of cohort R01, the tumor segmentation mask was not available \nfor 18 patients, so the final number of patients involved in this study is equal to 144, of which 40 (27.78%) with \na recurrence event within eight years from the first diagnosis. For each patient, a CT image in DICOM format \nwas available and was acquired by preoperative CT scans with a thickness of 0.625–3 mm and an X-ray tube cur-\nrent at 124–699 mA at 80–140 KVp. On the other hand, the related segmentations were defined on the axial CT \nimage series by thoracic radiologists with more than five years of experience and adjusted using ePAD  software53.\nBeyond CTs and binary tumor masks, the adopted database includes the following clinical features: Recur -\nrence (values: yes, no), age at histological diagnosis, weight, gender (values: female, male), histology (values: \nadenocarcinoma, squamous cell carcinoma, not otherwise specified), pathological T (values: T1, T2, T3, T4), \npathological N stage (values: N0, N1, N2), histopathological grade (values: G1, G2 and G3), lymphovascular \ninvasion (values: absent, present, not collected) and pleural invasion (values: yes, no)53. All these clinical features \nare listed in Table 2.\nIn this study, the clinical data were not used, and the recurrence feature (yes = 1, no = 0) was chosen as a label \nfor image classification.\nFor each patient, we first detected the segmentation mask with the largest tumour area and found the cor -\nresponding CT slide for the analysis as shown in Fig. 6.\nViTs, PVTs, Swins and CNNs architectures\nAfter detecting the CTs with the largest tumor area, we adopted a deep learning transfer-learning approach \ninvolving pre-trained ViTs, PVTs, Swins, and CNNs. All the analysis steps were performed using Python pro -\ngramming language with Tensorflow-Keras56, 57.\nFirst, the original CT image pixels were normalized in the range [0;1] and then reshaped to the specific input \nsize of the Transformers and CNNs. Then, the whole pre-processed images became the input for the various \nmodels.\nThe usual architecture of state-of-the-art CNNs, shown in Fig. 7, consists of three key elements represented \nby the convolutional layers, the pooling layers, and the fully connected. Once the CNN receives an input image \nsuitably pre-processed, the convolutional layers are the ones dedicated to learning features from the input images, \ninstead, the max-pooling layers are responsible for the reduction of the size of feature maps. At the end of the \nCNN, fully connected layers are added in a stacked way which, via a specific function (e.g., SoftMax or Sigmoid), \nprovides  classification10–20.\nThe architecture of the Transformers, shown in Fig. 8 according to the architecture of A. Dosovitskiy et al., \nis quite different from traditional  CNNs24. ViTs derive from the original transformer model used in the natural \n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:20605  | https://doi.org/10.1038/s41598-023-48004-9\nwww.nature.com/scientificreports/\nlanguage processing (NLP), where the input object consists of one-dimensional word tokens. The input images, of \ntypical size 224 × 224 pixels, of height H, width W, and channels C are divided into smaller patches with number \nN = HW/P2 being P × P the pixel size of the input image. To perform the classification task, ViTs are equipped \nwith an encoder that receives the sequence of embedded picture patches, together with positional data, and a \nlearnable class embedding suspended sequence. The latter is sent to the classification head coupled to the output \nof the encoder. Therefore, the data sequence is the following:\n• Original images are resized to size e.g., 224 × 224, and normalized between [0;1]. They are then decomposed \nin the N patches.\n• The obtained patches are then flattened obtaining a linear patch projection.\n• Learnable embeddings with patch projections are then concatenated. The positional embedding marks the \norder of the single patch in the sequence.\n• The output of the transformer encoder is sent to a Multilayer perceptron head (MLP) that with additional \nlayers of this work, e.g., a Flatten layer, a Batch Normalization layer, a Dense layer with 64 units, another \nBatch Normalization layer, and the final Dense layer with sigmoid function shown in red dashed box of Fig. 3, \nprovide classification.\nIn this study, we performed different experiments using two ViT models: a base model with 16  × 16 image \npatch size (ViTb_16) and a base model with 32 × 32 image patch size (ViTb_32) both consisting of 12 hidden \nTable 2.  Table of the clinical features of the adopted dataset and their distributions. “Nan” means “Not A \nNumber” if the data is missing in the database, “abs” stands for “absolute value”.\nClinical feature Distribution\nRecurrence\n Y es (abs; %) (40; 27.78%)\n No (abs; %) (104; 72.22%)\nAge at histological diagnosis\n Median  [q1;  q3] 69 [64; 76]\nWeight (lbs)\n Median  [q1;  q3] 173.5 [145.13; 198.90]\n Nan (abs; %) (10; 6.94%)\nGender\n Female (abs; %) (36; 25%)\n Male (abs; %) (108; 75%)\nHistology\n Adenocarcinoma (abs; %) (112; 77.77%)\n Squamous cell carcinoma (abs; %) (29; 20.14%)\n Not otherwise specified (abs; %) (3; 2.08%)\nPathological T stage\n T1 (abs; %) (74; 51.39%)\n T2 (abs; %) (49; 34.03%)\n T3 (abs; %) (16; 11.11%)\n T4 (abs; %) (5; 3.47%)\nPathological N stage\n N0 (abs; %) (115; 79.86%)\n N1 (abs; %) (12; 8.33%)\n N2 (abs; %) (17; 11.8%)\nHistopathological grade\n G1 (abs; %) (37; 25.69%)\n G2 (abs; %) (80; 55.56%)\n G3 Poorly differentiated (abs; %) (27; 18.75%)\nLymphovascular invasion\n Absent (abs; %) (121; 84.03%)\n Present (abs; %) (18; 12.5%)\n Not Collected (abs; %) (5; 3.47%)\nPleural invasion\n No (abs; %) (105; 72.92%)\n Y es (abs; %) (39; 27.08%)\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:20605  | https://doi.org/10.1038/s41598-023-48004-9\nwww.nature.com/scientificreports/\n layers22–24. PVTs represent a variant of the original ViTs and as stated by their name, they possess a columnar \npyramid structure similar to traditional  CNNs29, 30. In this work, we adopted the improved version PVTs v2, from \nWang et al. (2022), which introduced the linear complexity attention layer, the overlapping patch embedding \nand convolutional feed-forward network orthogonal to original PVTs. From now on, throughout the text, for the \nsake of simplicity, we will use the term PVT to indicate PVT v2 architecture of Wang et al. 29, 30. We considered \ntwo models of this family: PVT-B0 and PVT-B1. Both consist of four stages characterized by  Ci channel number \nof the output of stage i,  Ri reduction ratio,  Ni head number,  Ei expansion ratio of the feed forward layer, and  Li \nnumber of encoder layers for i = 1–4 hyperparameters. For both  L1–L4 equals 2 whereas  Ci, for i = 1–4, of PVT-B1 \nis double of the correspondent PVT-B029, 30. The Swin Transformer is another Transformer  architecture27. As the \nname states, Shifted Window, the key idea of this type of Transformer is to build a hierarchy starting from small-\nsized patches and gradually merging neighbouring patches into deep Tranformer layers. Between a self-attention \nlayer and the next one, there is a window shift resulting in a new one. We adopted two types of this architecture \nconsisting of the Swin-tiny and Swin-small which provided the best performances. The hyper-parameters of \nthese types of Swins are represented by the channel number C of hidden layers in the first stage being C = 96 for \nboth the Swin-tiny and small and the layer numbers being {2,2,6,2} ({2,2,18,2}) for the tiny one (small)27. For all \nthe analyzed Transformers the ideal image size has been set to 224 × 224 pixels.\nFigure 6.  An example of a binary Mask with the largest tumor area and its corresponding CT are shown. The \nyellow lines mark the tumor area in the CT. For each patient, we detected this correspondence, and the CTs, \nsuitably rescaled in the range [0;1] and with a specific input size, were then used as input for the ViTs, PVTs, \nSwins, and CNNs.\nFigure 7.  Typical architecture of a CNN. It takes the input image, suitably resized, and elaborates it through a \nseries of internal layers consisting of convolutional ones, max-pooling layers, and fully connected layers until \nfinal  classification10–20.\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:20605  | https://doi.org/10.1038/s41598-023-48004-9\nwww.nature.com/scientificreports/\nAs regards traditional CNNs, we used three well-established state-of-the-art CNNs of different families: \nResNet50, DenseNet201, and InceptionV3.\nIn Python Tensorflow-Keras, ResNet50 requires input images of size 224 × 224 pixels with 177 total layers. \nDifferently, InceptionV3 needs input images of 299 × 299 pixels with 313 total layers. In the end, DenseNet201 \naccepts input images of 224 × 224 pixels size with a total of 709  layers56, 57.\nLearning model\nWe built transfer learning models using pre-trained ViTs, PVTs, Swins, and CNNs on the ImageNet natural image \ndataset to train the dataset of NCSLC patients to predict the recurrence  event56–58. The application of transfer \nlearning to ViT, PVT, and Swin architectures consisted in replacing the last layer with the following layers: a flat-\ntening layer plus a batch normalization, one dense layer with Gelu activation function followed by another batch \nnormalization, and the final dense layer as classifier with a sigmoid activation function. The red dashed box in \nFig. 8 shows the added layers. This scheme was also adopted for CNNs replacing the Gelu with the Relu activa-\ntion function for the added dense layer. These new networks were then trained for the image classification task. \nWe implemented a stratified tenfold cross-validation in 20 external rounds on the entire dataset of 144 patients. \nIn each fold of the cross-validation, 90% of the dataset corresponding to 130 elements is used as a training set, \nwhereas the remaining 10%, corresponding to 14 elements, is used as the test set.\nIn this study, all the models were trained for 30 epochs in each fold of the cross-validation with batch size \nequal to 10 elements. Adam optimizer with an initial learning rate of 10–4 was used to optimize the weights of \nthe network. To handle the imbalancing of the dataset, a sigmoid focal cross-entropy was used as loss function \nwith balancing factor α and modulating factor β equal 0.25 and 2.0  respectively59. Considering our database is \nrelatively small, to make our analysis more robust we implemented a data augmentation process, in addition to \nthe transfer-learning approach, using three built-in Keras transformations such as Random Flip, Random Rota-\ntion, and Random  Contrast57. This data augmentation was added as an additional layer in the models.\nAfter the training phase, the model was used to predict the probability scores and then used to compute the \nperformances via the Scikit-learn library  functions58. Performances of classification of NSCLC recurrence for \npre-trained ViTs, PVTs, Swins and CNNs have been evaluated in terms of the Area Under the Curve (AUC), \nAccuracy, Sensitivity, Specificity, and Precision. These metrics are computed in each of the 20 rounds of the \nstratified cross-validation so, in the end, the final performances, of the specific model, are evaluated as an aver-\nage of all the 20 values with their corresponding standard deviation. To better balance these metrics, a Y ouden \nindex test was  performed60.\nData availability\nThe data was obtained from the open-access NSCLC-Radiogenomics dataset publicly available at The Cancer \nImaging Archive (TCIA) database (https:// wiki. cance rimag ingar chive. net/ displ ay/ Public/ NSCLC+ Radio genom \nics). Imaging and the clinical data have been de-identified by TCIA and approved by the Institutional Review \nBoard of the TCIA hosting institution. Ethical approval was reviewed and approved by Washington University \nInstitutional Review Board protocols. Informed consent was obtained from all individual participants included \nin this  study53. The source codes can be found at the following link: https://  github. com/ mcome s92/ NSCLC_ \nVit_ CNN.\nFigure 8.  Proposed architecture of the ViTs starting from Dosovitskiy et al.24. The original input image, suitably \npre-processed, is then decomposed into N patches then flattened obtaining a linear patch projection. Through \nthe Transformer Encoder, these elements are sent to the head of MLP , which provides classification. The yellow \nfinal boxes placed after the MLP , inside the red dashed rectangle, indicate the new added layers of the proposed; \nthese have also been adopted for the CNNs.\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:20605  | https://doi.org/10.1038/s41598-023-48004-9\nwww.nature.com/scientificreports/\nReceived: 3 July 2023; Accepted: 21 November 2023\nReferences\n 1. Jemal, A. et al. Global cancer statistics. CA Cancer J. Clin. 61, 69–90 (2011).\n 2. Chen, Y . Y . et al. Risk factors of postoperative recurrences in patients with clinical stage I NSCLC. World J. Surg. Oncol. 12, 10 \n(2014).\n 3. Scalchi, P . et al. Use of parallel-plate ionization chambers in reference dosimetry of NOV AC and  LIAC® mobile electron linear \naccelerators for intraoperative radiotherapy: A multi-center survey. Med. Phys. 44, 1 (2017).\n 4. LeCun, Y ., Bengio, Y . & Hinton, G. Deep learning. Nature 521, 436–444 (2015).\n 5. Lambin, P . et al. Radiomics: the bridge between medical imaging and personalized medicine. Nat. Rev. Oncol. 14, 749–762 (2017).\n 6. Castiglioni, I. et al. AI applications to medical images: From machine learning to deep learning. Phys. Med. 83, 9–24 (2021).\n 7. Domingues, I. et al. Using deep learning techniques in medical imaging: A systematic review of applications on CT and PET. Artif. \nIntell. Rev. 53, 4093–4160 (2020).\n 8. Bera, K., Braman, N., Gupta, A., Velcheti, V . & Madabhushi, A. Predicting cancer outcomes with radiomics and artificial intel-\nligence in radiology. Nat. Rev. Clin. Oncol. 19, 132–146 (2022).\n 9. Bellotti, R., De Carlo, F ., Massafra, R., de Tommaso, M. & Sciruicchio, V . Topographic classification of EEG patterns in Huntington’s \ndisease. Neurol. Clin. Neurophysiol. 2004, 37 (2004).\n 10. Comes, M. C. et al. Early prediction of neoadjuvant chemotherapy response by exploiting a transfer learning approach on breast \nDCE-MRIs. Sci. Rep. 11, 14123 (2021).\n 11. Massafra, R. et al. Robustness evaluation of a deep learning model on sagittal and axial breast DCE-MRIs to predict pathological \ncomplete response to neoadjuvant chemotherapy. J. Pers. Med. 12, 953 (2022).\n 12. Comes, M. C. et al. Early prediction of a breast cancer recurrence for patients treated with neoadjuvant chemotherapy: A transfer \nlearning approach on DCE-MRIs. Cancers 13, 2298 (2021).\n 13. Comes, M. C. et al. A deep-learning model based on whole slide images to predict disease-free survival in cutaneous melanoma \npatients. Sci. Rep. 12, 20366 (2022).\n 14. Bove, S. et al. A CT-based transfer learning approach to predict NSCLC recurrence: The added-value of peritumoral region. PLoS \nONE 18(5), e0285188 (2023).\n 15. Zhou, J. & Xin, H. Emerging artificial intelligence methods for fighting lung cancer: A survey. Clin. eHealth 5, 19–34 (2022).\n 16. Sakamoto, T. et al. A narrative review of digital pathology and artificial intelligence: Focusing on lung cancer. Transl. Lung Cancer \nRes. 9(5), 2255–2276 (2020).\n 17. Shi, L. et al. Radiomics for response and outcome assessment for non-small cell lung cancer. Technol. Cancer Res. Treat. 17, 1–14 \n(2018).\n 18. Silva, F . et al. Towards machine learning-aided lung cancer clinical routines: Approaches and open challenges. J. Pers. Med. 12, \n480 (2022).\n 19. Voulodimos, A., Doulamis, N., Doulamis, A. & Protopapadakis, E. Deep learning for computer vision: A brief review. Comput. \nIntell. Neurosci. 2018, 7068349 (2018).\n 20. Khan, A., Sohail, A., Zahoora, U. & Qureshi, A. S. A survey of the recent architectures of deep learning neural networks. Artif. \nIntell. Rev. 53, 5455–5516 (2020).\n 21. Russakovsky, O. et al. ImageNet large scale visual recognition challenge. Int. J. Comput. Vis. 115, 211–252 (2015).\n 22. Ayana, G. & Choe, S. W . BUViTNET: Breast ultrasound detection via vision transformers. Diagnostics 12, 2654 (2022).\n 23. Ayana, G. et al. Vision-transformer-based transfer learning for mammogram classification. Diagnostics 13, 178 (2023).\n 24. Dosovitskiy, A., Beyer, L., Kolesnikov, A. et al. An image is worth 16 × 16 words: Transformers for image recognition at scale. arXiv: \n2010. 11929 v2 (2020).\n 25. Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J. & Beyer, L. How to train your ViT? Data, augmentation, and \nregularization in vision transformers. arXiv: 2106. 1027v2 (2022).\n 26. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Proc. Syst. 30, 5998–6008 (2017).\n 27. Liu, Z., Lin, Y ., Cao, Y . et al. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/\nCVF International Conference on Computer Vision 9992–10002 (2021).\n 28. Chen, C. F ., Fan, Q. & Panda, R. CrossVit: Cross-attention multi-scale vision transformer for image classification. arXiv: 2103.  \n14899 v2 (2021).\n 29. Wang, W ., Xie, E., Li, X. et al. Pyramid vision transformers: A versatile backbone for dense prediction without convolutions. In \nProceedings of the IEEE/CVF International Conference on Computer Vision 548–558 (2021).\n 30. Wang, W ., Xie, E., Li, X. & Fan, D. P . PVT v2: Improved baselines with pyramid vision transformers. arXiv: 2106. 13797 v6 (2022).\n 31. d’ Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A., Biroli, G. & Sagun, L. ConVit: Improving vision transformers with soft con-\nvolutional inductive biases. J. Stat. Mech. 114005 (2022).\n 32. Zhou, D., Kang, B., Jin, X. et al. DeepViT: Towards deeper vision transformer. arXiv: 2103. 11886 (2021).\n 33. Heo, B., Yun, S., Han, D., Chun, S., Choe, J. & Oh, S. J. Rethinking spatial dimensions of vision transformers. arXiv: 2103. 16302 v2 \n(2021).\n 34. Touvron, H., Cord, M., Sablayrolles, A. & Synnaeve, G. Going deeper with image transformers. arXiv: 2103. 17239 v2 (2021).\n 35. Yu, W ., Luo, M., Zhou, P . et al. Metaformer is actually what you need for vision. arXiv: 2111. 11418 v3 (2022).\n 36. Tu, Z., Talebi, H., Zhang, H. et al. MaxViT: Multi-axis vision transformer. arXiv: 2204. 01697 v4 (2022).\n 37. Han, K. et al. A survey on vision transformer. IEEE Trans. Pattern Anal. Mach. Intell. 45, 87–110 (2023).\n 38. Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C. & Dosovitskiy, A. Do vision transformers see like convolutional neural net -\nworks? arXiv: 2108. 08810 v2 (2022).\n 39. Lee, S. H., Lee, S. & Song, B. C. Vision transformer for small-size datasets. arXiv: 2112. 13492 v1 (2021).\n 40. Khan, S., Naseer, M., Hayat, M., Zamir, S. W . & Khan, S. M. Transformers in vision: A survey. ACM Comput. Surv. (CSUR) 54, \n1–41 (2022).\n 41. Hutten, N., Meyers, R. & Meisen, T. Vision transformer in industrial visual inspection. Appl. Sci. 12, 11981 (2022).\n 42. Chen, Y . et al. Detection and classification of lung cancer cells using swin transformer. J. Cancer Ther. 13, 464–475 (2022).\n 43. Usman, M., Zia, T. & Tariq, A. Analyzing transfer leaning of vision transformers for interpreting chest radiography. J. Digit. Imaging \n35, 1445–1462 (2022).\n 44. Lian, J. et al. Early state NSCLC patients’ prognostic prediction with multi-information using transformer and graph neural network \nmodel. eLife 11, e80547 (2022).\n 45. Sun, R., Pang, Y . & Li, W . Efficient lung cancer image classification and segmentation algorithm based on an improved swin \ntransformer. Electronics 12, 1024 (2023).\n 46. Chen, X. et al. Transformers improve breast cancer diagnosis from unregistred multi-view mammograms. Diagnostics  12, 1549 \n(2022).\n 47. Prodan, M., Paraschiv, E. & Stanciu, A. Applying deep learning methods for mammography analysis and breast cancer detection. \nAppl. Sci. 13(7), 4272 (2023).\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:20605  | https://doi.org/10.1038/s41598-023-48004-9\nwww.nature.com/scientificreports/\n 48. Moutik, O. et al. Convolutional neural networks or vision transformers: Who will win the race for action recognitions in visual \ndata?. Sensors 23(2), 734 (2023).\n 49. Kim, G., Moon, S. & Choi, J. H. Deep learning with multimodal integration for predicting recurrence in patients with non-small \ncell lung cancer. Sensors 22, 6594 (2022).\n 50. Aonpong, P ., Iwamoto, Y ., Han, X. H., Lin, L. & Chen, Y . W . Genotype-guided radiomics signatures for recurrence prediction of \nnon-small cell lung cancer. IEEE Access 9, 90244–90254 (2021).\n 51. Wang, X., Duan, H. H. & Nie, S. D. Prognostic recurrence analysis method for non-small cell lung cancer based on CT imaging. \nProc. SPIE 11321, 113211T (2019).\n 52. Hindocha, S. et al. A comparison of machine learning methods for predicting recurrence and death after curative-intent radio -\ntherapy for non-small cell lung cancer: Development and validation of multivariable clinical prediction models. Lancet 77, 103911 \n(2022).\n 53. Bakr, S. et al. A radiogenomic dataset of non-small cell lung cancer. Sci. Data 5, 180202 (2018).\n 54. Massafra, R. et al. A clinical decision support system for predicting invasive breast cancer recurrence: Preliminary results. Front. \nOncol. 11, 576007 (2021).\n 55. Amoroso, N. et al. A roadmap towards breast cancer therapies supported by explainable artificial intelligence. Appl. Sci.  11(11), \n4881 (2021).\n 56. Abadi, M., Agarwal, A., Barham, P . et al. TensorFlow: Large-scale machine learning on heterogeneous distributed systems. arXiv: \n1603. 04467 (2016).\n 57. https:// github. com/ keras- team/ keras; https:// pypi. org/ proje ct/ tfimm/.\n 58. Pedregosa, F . et al. Scikit-learn: Machine learning in python. JMLR 12, 2825–2830 (2011).\n 59. Lin, T. Y ., Goyal, P ., Girshick, R., He, K. & Dollar, P . Focal loss for dense object detection. arXiv: 1708. 02002 v2 (2018).\n 60. Y ouden, W . J. Index for rating diagnostic tests. Cancer 3, 32–35 (1950).\nDisclaimer\nThe authors affiliated with Istituto Tumori “Giovanni Paolo II, ” IRCCS, Bari are responsible for the views \nexpressed in this article, which do not necessarily represent the ones of the Institute.\nAuthor contributions\nConceptualization, A.F ., F .F ., and R.M.; methodology, A.F ., F .F ., and R.M; software, F .F ., A.F ., and M.C.C.; valida-\ntion, A.F . and F .F .; formal analysis, A.F ., F .F ., S.B., M.C.C. and R.M.; resources, R.M.; data curation, A.F ., F .F ., S.B., \nM.C.C. and R.M.; writing—original draft preparation, A.F ., F .F ., S.B., M.C.C. and R.M.; writing—review and \nediting, A.F ., F .F ., S.B., M.C.C., A.C., E.D., A.M., M.M., A.N., C.S., D.G. and R.M.; supervision, A.F . and R.M. \nAll authors reviewed the manuscript.\nFunding\nThis work was supported by funding from the Italian Ministry of Health, Ricerca Corrente 2023 Deliberation \nn. 187/2023.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to M.C.C. or S.B.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Convolutional neural network",
  "concepts": [
    {
      "name": "Convolutional neural network",
      "score": 0.7803829908370972
    },
    {
      "name": "Lung cancer",
      "score": 0.6603291630744934
    },
    {
      "name": "Computer science",
      "score": 0.5515062808990479
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4915946424007416
    },
    {
      "name": "Medicine",
      "score": 0.426568865776062
    },
    {
      "name": "Transformer",
      "score": 0.42429736256599426
    },
    {
      "name": "Oncology",
      "score": 0.3421032428741455
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3225556015968323
    },
    {
      "name": "Engineering",
      "score": 0.05790206789970398
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}