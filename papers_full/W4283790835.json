{
  "title": "Enhanced Story Comprehension for Large Language Models through Dynamic Document-Based Knowledge Graphs",
  "url": "https://openalex.org/W4283790835",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4284445922",
      "name": "Berkeley R Andrus",
      "affiliations": [
        "Brigham Young University"
      ]
    },
    {
      "id": "https://openalex.org/A4284445923",
      "name": "Yeganeh Nasiri",
      "affiliations": [
        "Brigham Young University"
      ]
    },
    {
      "id": "https://openalex.org/A4284445924",
      "name": "Shilong Cui",
      "affiliations": [
        "Brigham Young University"
      ]
    },
    {
      "id": "https://openalex.org/A2689571815",
      "name": "Benjamin Cullen",
      "affiliations": [
        "Brigham Young University"
      ]
    },
    {
      "id": "https://openalex.org/A2042243515",
      "name": "Nancy Fulda",
      "affiliations": [
        "Brigham Young University"
      ]
    },
    {
      "id": "https://openalex.org/A4284445922",
      "name": "Berkeley R Andrus",
      "affiliations": [
        "Brigham Young University"
      ]
    },
    {
      "id": "https://openalex.org/A4284445923",
      "name": "Yeganeh Nasiri",
      "affiliations": [
        "Brigham Young University"
      ]
    },
    {
      "id": "https://openalex.org/A4284445924",
      "name": "Shilong Cui",
      "affiliations": [
        "Brigham Young University"
      ]
    },
    {
      "id": "https://openalex.org/A2689571815",
      "name": "Benjamin Cullen",
      "affiliations": [
        "Brigham Young University"
      ]
    },
    {
      "id": "https://openalex.org/A2042243515",
      "name": "Nancy Fulda",
      "affiliations": [
        "Brigham Young University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035850279",
    "https://openalex.org/W3137014599",
    "https://openalex.org/W3016697633",
    "https://openalex.org/W2902391430",
    "https://openalex.org/W2251913848",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W2968154628",
    "https://openalex.org/W2951525151",
    "https://openalex.org/W6750305986",
    "https://openalex.org/W2999854190",
    "https://openalex.org/W6779857854",
    "https://openalex.org/W2131494463",
    "https://openalex.org/W2115613106",
    "https://openalex.org/W2016089260",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2989902860",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2415830263",
    "https://openalex.org/W2772700764",
    "https://openalex.org/W2293453011",
    "https://openalex.org/W3091627448",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W4288027128",
    "https://openalex.org/W4320871337",
    "https://openalex.org/W2963217826",
    "https://openalex.org/W2997876626",
    "https://openalex.org/W2970745243",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3171434230",
    "https://openalex.org/W3035348754",
    "https://openalex.org/W3100714086",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034671305"
  ],
  "abstract": "Large transformer-based language models have achieved incredible success at various tasks which require narrative comprehension, including story completion, answering questions about stories, and generating stories ex nihilo. However, due to the limitations of finite context windows, these language models struggle to produce or understand stories longer than several thousand tokens. In order to mitigate the document length limitations that come with finite context windows, we introduce a novel architecture that augments story processing with an external dynamic knowledge graph. In contrast to static commonsense knowledge graphs which hold information about the real world, these dynamic knowledge graphs reflect facts extracted from the story being processed. Our architecture uses these knowledge graphs to create information-rich prompts which better facilitate story comprehension than prompts composed only of story text. We apply our architecture to the tasks of question answering and story completion. To complement this line of research, we introduce two long-form question answering tasks, LF-SQuAD and LF-QUOREF, in which the document length exceeds the size of the language model's context window, and introduce a story completion evaluation method that bypasses the stochastic nature of language model generation. We demonstrate broad improvement over typical prompt formulation methods for both question answering and story completion using GPT-2, GPT-3 and XLNet.",
  "full_text": "Enhanced Story Comprehension for Large Language Models through Dynamic\nDocument-Based Knowledge Graphs\nBerkeley R Andrus, Yeganeh Nasiri, Shilong Cui, Benjamin Cullen, and Nancy Fulda\nComputer Science Department, Brigham Young University, Provo, Utah, USA\nberkeley.andrus, ynasiri, shilong, bcullen2, nfulda@byu.edu\nAbstract\nLarge transformer-based language models have achieved in-\ncredible success at various tasks which require narrative com-\nprehension, including story completion, answering questions\nabout stories, and generating stories ex nihilo. However, due\nto the limitations of ﬁnite context windows, these language\nmodels struggle to produce or understand stories longer than\nseveral thousand tokens. In order to mitigate the document\nlength limitations that come with ﬁnite context windows, we\nintroduce a novel architecture that augments story process-\ning with an external dynamic knowledge graph. In contrast to\nstatic commonsense knowledge graphs which hold informa-\ntion about the real world, these dynamic knowledge graphs\nreﬂect facts extracted from the story being processed. Our ar-\nchitecture uses these knowledge graphs to create information-\nrich prompts which better facilitate story comprehension than\nprompts composed only of story text. We apply our archi-\ntecture to the tasks of question answering and story com-\npletion. To complement this line of research, we introduce\ntwo long-form question answering tasks, LF-SQuAD and LF-\nQUOREF, in which the document length exceeds the size of\nthe language model’s context window, and introduce a story\ncompletion evaluation method that bypasses the stochastic\nnature of language model generation. We demonstrate broad\nimprovement over typical prompt formulation methods for\nboth question answering and story completion using GPT-2,\nGPT-3 and XLNet.\nIntroduction\nLarge language models such as GPT-2 (Radford et al. 2019)\nhave been used for a variety of story-related tasks includ-\ning story completion (Xu et al. 2020; Guan et al. 2020),\nreading comprehension (Radford et al. 2019; Raffel et al.\n2020), and story generation (Fan, Lewis, and Dauphin 2018;\nRoemmele 2016). However, previous works have mostly fo-\ncused on extremely short stories that ﬁt within the limited\ncontext windows of transformer-based language models. For\nexample, several of these works use the popular ROCSto-\nries dataset (Mostafazadeh et al. 2016) which is composed\nof ﬁve-sentence stories. Using GPT- 2’s tokenization rules,\nthe stories in this dataset have an average token length of\n53.5, much smaller than most transformer context windows.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nWhile language models with ﬁnite context windows are ca-\npable of generating stories of arbitrary length through itera-\ntive text predictions, these longer stories are prone to intro-\nduce information that abandons or contradicts information\npresent earlier in the story. Much work has been done to\nincrease story coherence through external knowledge bases\n(Xu et al. 2020; Guan et al. 2020), but these works have fo-\ncused on making stories consistent with the real world rather\nthan on making stories consistent with themselves.\nA language model’s ability to generate internally consis-\ntent text relies on a property which we term “story compre-\nhension”. This ability is a prerequisite to successfully com-\npleting various tasks including story completion, story gen-\neration, document summarization, question answering, etc.\nIt stands to reason that language models with ﬁnite con-\ntext windows, including transformer-based language mod-\nels, cannot comprehend more text than ﬁts within their con-\ntext windows, putting an upper limit on the story compre-\nhension ability of these models. This poses a problem for\nlong-form text generation applications such as AI Dungeon1\nin which end users have a vested interest in the coherency of\ngenerated stories.\nIn this work we introduce a novel architecture for im-\nproving the story comprehension of large language models\nthrough external knowledge bases. Our approach involves\nextracting facts from a document and constructing a custom\ndynamic knowledge graph. Then, given a story comprehen-\nsion task, we extract and verbalize relevant information from\nthe knowledge graph and incorporate it into information-rich\nprompts for a language model.\nThe primary contributions of this work include:\n• Deﬁning an architecture which interfaces effectively with\na large language model, providing the language model\nwith fact-rich prompts that enhance story comprehension\n• Introducing LF-SQuAD and LF-QUOREF, two novel\nevaluation tasks designed to measure long-form story\ncomprehension\n• Introducing a new evaluation metric for story completion\nthat, unlike previous metrics such as BLEU, does not as-\nsume that a human-written response is the single correct\nanswer\n1https://play.aidungeon.io\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10436\nFigure 1: Visualization of the three tasks accomplished by our architecture. In Knowledge Graph Construction, we use an\nOpen Information Extraction model with rule based post processing to convert a long-form text document into a document-\nspeciﬁc knowledge graph. In Fact Retrieval we ﬁnd which facts are most relevant to some story comprehension task. In Prompt\nFormulation we use few-shot learning with GPT-3 to verbalize extracted facts as natural language and incorporate them into a\nprompt for a large language model.\nWe apply our architecture to two story comprehension\ntasks, question answering and story completion, and eval-\nuate its impact on the performance of three transformer-\nbased language models, GPT-2 (Radford et al. 2019), GPT-3\n(Brown et al. 2020), and XLNet (Yang et al. 2019). Our eval-\nuation tasks require a language model to process and under-\nstand text that exceeds its context window size, and hence\nare not directly comparable to existing tasks and method-\nologies which rely on documents with limited lengths. We\ndemonstrate consistent improvement over a system in which\nno knowledge base is used and language models must make\npredictions using traditional prompts.\nRelated Work\nThere have been several previous works exploring how\nknowledge graphs can augment language models. Guan\net al. (2020) use commonsense knowledge graphs to gen-\nerate data for ﬁne-tuning GPT-2 to enhance the language\nmodel’s ability to generate coherent, non-repetitive stories.\nXu et al. (2020) also use a knowledge graph to ﬁne-tune\nlanguage models for story generation, adding a mechanism\nto extract relevant information from the knowledge graph\nbased on predicted keywords as it was needed. Both works\nuse static pre-constructed knowledge graphs.\nGuu et al. (2020) use an external knowledge base to train a\nlanguage model on open-domain question answering. It uses\na collection of text snippets from Wikipedia as a knowledge\nbase rather than a knowledge graph, and it trains a a neu-\nral knowledge retriever rather than retrieving based on pre-\ndicted keywords.\nIn contrast to these works, our system uses a knowledge\nbase that is dynamic and reﬂects facts that have appeared\nin the story rather than commonsense information. Our fo-\ncus is not on helping the language model comprehend the\nreal world but on helping the language model comprehend\nthe document being processed. In addition, the documents\nwe use to evaluate our system are many times longer than\ndocuments used in directly comparable systems, shifting our\ntasks to a new and largely unexplored domain.\nDynamic knowledge graphs have been used in Dialogue\nState Tracking applications to model participants in a con-\nversation (He et al. 2017; Zhou and Small 2019). Bosselut,\nBras, and Choi (2019) and Bosselut et al. (2019) also aim\nto generate knowledge graphs dynamically, but treat knowl-\nedge graph generation as a language modelling problem for\ndiscovering implicit commonsense relations between enti-\nties, rather than an extraction problem for recording explicit\nfacts speciﬁc to document text.\nThere has also been much work over the years to auto-\nmatically generate knowledge graphs from text documents.\nWang et al. (2018) use a statistical model to predict entity\nrelations from ﬁltered domain-speciﬁc text. Distiawan et al.\n(2019) train a relation extraction model to add new relations\nbetween entities in an existing knowledge graph. Angeli,\nPremkumar, and Manning (2015) use dependency parses to\n10437\nextract tuples including entities and relations. Nayak and Ng\n(2020) focus on extracting overlapping relations, where en-\ntities have multiple relations reﬂected in a single text span.\nAll of these works aim to extract knowledge about the world\nfrom the text, creating one large database that applies gener-\nally or that always describes a speciﬁc domain. Our system,\nconversely, aims to create a knowledge base that describes\na single long-form document, including subjective facts that\nmay not be meaningful or even intelligible out of context.\nParticularly relevant to this paradigm of document-speciﬁc\nknowledge graphs are (Ammanabrolu and Riedl 2019; Am-\nmanabrolu et al. 2020; Adhikari et al. 2020), which use\nknowledge graphs to track agent progress and world state\nin text adventure games. This work uses similar mecha-\nnisms for generating knowledge graphs from text, but we\nuse knowledge graphs to generate language model prompts\nrather than to inform reinforcement learning agents.\nFinally, there are many approaches to story related tasks\nthat do not rely on prevailing transformer-based language\nmodels. These include the introduction of new variants of\nthe transformer architecture (Ainslie et al. 2020), hierarchi-\ncal representation and generation of stories (Li, Luong, and\nJurafsky 2015; Fan, Lewis, and Dauphin 2018), and em-\nbedding facts with a neural network for question answer-\ning (Kumar et al. 2016; Xiong, Merity, and Socher 2016).\nWe emphasize that our purpose is not to compete with these\nother methods but to maximize the performance of existing\ntransformer-based language models on similar tasks in a way\nthat does not require any training, ﬁne-tuning, or labelled\ndata.\nArchitecture\nWe use a dynamic knowledge graph to enhance the coher-\nence of language model output when making document-\nbased predictions as shown in Figure 1.\nOur story comprehension pipeline consists of three steps:\n(1) Knowledge Graph Construction, in which we construct a\nknowledge graph Gthat contains key information extracted\nfrom a natural language document D, (2) Fact Retrieval\nfrom the knowledge graph, in which we retrieve from G\nfacts that are relevant to some document comprehension task\nT, and (3) Prompt Formulation, which includes both the\nsynthesis of knowledge graph facts into free-form text and\nthe concatenation of those facts with an excerpt from the\nstory text and some framing text to form a prompt P. Fur-\nther details can be found in Algorithm 1. Our complete ar-\nchitecture implementation is available in the supplementary\nmaterials for this paper.\nKnowledge Graph Construction\nThe ﬁrst task for our architecture is to generate a knowl-\nedge graph G that contains information extracted from\na text document D containing a story or narrative text.\nOnce constructed, G is composed of facts of the form\n[head;relation;tail ], where headand tailare entities from\nthe document and relationdescribes how those entities are\nrelated. We constructGby, for each sentences2D, extract-\ning 0 or more facts fromsand adding them to G. We extract\nAlgorithm 1: Story Comprehension Pipeline\nInputs\nD= natural language document to be processed\nE= schemaless knowledge extractor (Open IE)\nLM= text generation via language model (GPT-2, GPT-3, XLNet)\n = ﬁlter function for KG facts\n\b = text similarity metric (uses either SBERT or Levenshtein)\n\u0007 = fact verbalization function\nk= language model context length\nquery = the question to be answered, or the ﬁnal paragraph of the\nstory to be completed, query 6\u001aD\nframing text= Additional task-speciﬁc text inserted into prompt\nto focus the language model\nPipeline\n1: G= []\n2: for sentencein Ddo\n3: T = E(sentence)\n4: T = [t for t in T if  (t)]\n5: G= G+ T\n6: end for\n7: similarities= []\n8: for f in Gdo\n9: similarities.append(\b (f, query))\n10: end for\n11: indices= argmax(similarities,n=3)\n12: facts = G[indices]\n13: verbalized\nfacts = [\u0007(f) for f in facts]\n14: kD = k- len(framing text+ verbalized facts + query)\n15: prompt = D[-kD:] + verbalized facts + framing text+\nquery\n16: output text= LM(prompt)\nfacts using the Stanford Open Information Extraction (Open\nIE) model (Angeli, Premkumar, and Manning 2015), which\nseparates natural language sentences into short clauses and\nthen segments clauses into a knowledge graph triple.\nMany applications that use knowledge graphs for natu-\nral language tasks use a schema, such as the knowledge\ngraphs Wikidata (Vrande ˇci´c and Kr ¨otzsch 2014) and Con-\nceptNet (Liu and Singh 2004). Schemas often predeﬁne a\nset of possible relations between entities, which facilitate\nautomatic data analysis or text generation. In our applica-\ntion facts in G will only ever be processed by a language\nmodel, which eliminates the need for a predeﬁned schema.\nWe choose the more ﬂexible approach of using a schema-\nless knowledge graph where the head, relation, and tailof\neach fact can be composed of any natural language found in\nthe text. relationis typically a verb phrase and headis typ-\nically a noun phrase. See Table 3 for examples of extracted\nknowledge graph facts.\nTo mitigate any mistakes made by the Open IE model,\nwe apply several ﬁltering rules before adding each fact to\nG. The purpose of these rules is to eliminate any facts that\nlack a recoverable meaning. We do not add a fact of the form\n[head;relation;tail ] to Gif it follows any of the following\npatterns:\n• tailis identical to head(e.g. [you, just sit, you])\n• head does not contain a noun (e.g. [screamed, howling\nlike, wolf])\n10438\n• the ﬁrst word in tail is identical to the last word in\nrelation(e.g. [i, enjoy playing, playing soccer])\n• head or tail starts with a pronoun or conjunction (e.g.\n[who, lurking, behind a wire], [but, should, be careful])\n• there are no verbs present in the fact (e.g. [breakfast,\nlunch, dinner])\nWe use the NLTK interface for WordNet (Miller 1995) to\ndetermine the part-of-speech of words in each fact.\nFact Retrieval from Knowledge Graph\nGiven a story comprehension taskT and a generated knowl-\nedge graph G, our next step in producing a suitable prompt\nP is to ﬁnd which facts in G are most relevant to T. We\nuse two relevance metrics, one for question answering and\none for story completion. For question answering, where we\nare interested in ﬁnding discrete information about speciﬁc\nstory entities, we determine relevance using the string edit\ndistance between knowledge graph facts f 2 G and the\nquestion T. The most relevant fact for question answering\ncan be deﬁned as\narg min\nf2G\nlev(f;T ) (1)\nwhere levrepresents the Levenshtein distance function.\nFor story completion we are more concerned with se-\nmantic similarity than literal string similarity, so we deter-\nmine relevance using Sentence-BERT (SBERT) (Reimers\nand Gurevych 2019), a variation on the BERT network (De-\nvlin et al. 2018) designed to generate embeddings which fa-\ncilitate semantic comparisons. We ﬁnd the cosine similarity\nbetween the SBERT embeddings of T (in this case the most\nrecent paragraph of the story) and each factf 2G. The most\nrelevant fact for story completion can be deﬁned as\narg max\nf2G\ncos(SB(f);SB(T)) (2)\nwhere cos represents the cosine similarity function and SB\nis the application of the SBERT embedding model.\nWe also try applying SBERT for the question answering\ntask and Levenshtein distance for the story completion task,\nbut ﬁnd that these settings yield inferior performance.\nFor both question answering and story completion we se-\nlect the top 3 most relevant facts fromG. We also experiment\nwith larger numbers but ﬁnd a negligible change in overall\nsystem performance.\nPrompt Formulation\nWhen a knowledge graph schema is used, facts can be con-\nverted to sentences using template rules (Xu et al. 2020;\nGuan et al. 2020). Because we have a schemaless knowl-\nedge graph with open-ended relations, we instead use few-\nshot learning with GPT-3 (Brown et al. 2020) to verbalize\nknowledge graph facts as sentences that can be incorporated\ninto language model prompts. We use the prompt shown in\nFigure 2 to coax GPT-3 into generating well-formed sen-\ntences.\nWe note that while GPT-3 is not currently publicly avail-\nable, other methods of verbalizing knowledge graph facts\nFigure 2: Prompt used to verbalize knowledge graph facts\nthrough few-shot learning with GPT-3. [Fact Head], [Fact\nRelation], and [Fact Tail] are replaced with the three con-\nstituents of the knowledge graph fact being verbalized. Sev-\neral example inputs and outputs are shown in Table 3.\nexist. For example, Agarwal et al. (2021) demonstrate suc-\ncess ﬁne-tuning T5 (Raffel et al. 2020) for knowledge graph\nverbalization, and in many cases even simple concatenation\nof fact constituents produces adequate verbalizations.\nExperiments\nTo test whether our system improves the performance of\nlarge language models, we evaluate it using two tasks that\nrequire document comprehension: question answering and\nstory completion. We hypothesize that our dynamic knowl-\nedge graph will aid the language model in generating more\ncoherent text by capturing facts that would not normally ﬁt\nwithin the language model’s context window if the language\nmodel were working merely on unﬁltered document text.\nWe repeat experiments using three language models:\nGPT-2, GPT-3, and XLNet. These models are selected due\nto their prevalence as generative language models and their\nhigh performance on other document comprehension tasks\n(Radford et al. 2019; Brown et al. 2020; Yang et al. 2019).\nOther large language models such as BERT (Devlin et al.\n2018) and T5 (Raffel et al. 2020) are not trained to perform\nautoregressive text generation and are thus not natural ﬁts\nfor both the question answering and story completion tasks.\nQuestion Answering\nFor the question answering task our goal is to correctly\nanswer questions about documents. The task has been re-\nferred to as ‘Reading Comprehension’ in other works and\nis distinct from the open-domain question answering task,\nin which no document containing the correct answer is pro-\nvided.\nWe evaluate our system’s performance on two new\nquestion answering datasets, LF-SQuAD and LF-QUOREF\n(with LF standing for long-form), which we make available\nin the supplementary materials. These datasets are adapted\nfrom SQuAD (Rajpurkar et al. 2016) and QUOREF (Dasigi\net al. 2019) respectively. The primary difference in these new\ndatasets is that rather than associating questions with para-\ngraphs we associate them with documents. For each ques-\ntion in SQuAD and QUOREF, we reconstruct the original\nsource document and pair the question with the entire doc-\nument rather than with a speciﬁc paragraph. This substan-\ntially increases the difﬁculty of the question answering task,\nparticularly for transformer-based language models that can\nﬁt paragraphs in their context windows but not entire doc-\n10439\nFigure 3: Prompt used for the Question Answering task.\n[Source Text] is replaced with as much of the end of the\ndocument text as will ﬁt in the context window. [Verbal-\nized Facts] is replaced with the most relevant facts from the\nknowledge graph verbalized as sentences. [Question] is re-\nplaced with the question from the dataset. The entire prompt\nis limited to a 1000 token context length. In the baseline ap-\nproach, the [Verbalized Facts] ﬁeld is omitted, which makes\nroom for more document text in the [Source Text] ﬁeld.\numents. In constructing LF-SQuAD and LF-QUOREF we\nalso ﬁlter out any questions that have multiple correct an-\nswers listed to simplify the evaluation process and avoid\nsituations in which answers could be expressed multiple\nways. In these experiments we use an abridged version of\neach dataset to accommodate our compute-heavy task. We\nﬁrst choose the document-question pairs associated with the\nlongest documents and then randomly select 600 document-\nquestion pairs from LF-SQuAD and 568 document-question\npairs from LF-QUOREF. We use the same subset of the\ndatasets in all experiments.\nWe answer questions using the process deﬁned in Algo-\nrithm 1 and the language models listed above. We compare\neach model’s answer to the human labelled answer using an\nF1 score, which measures the amount of overlap in tokens\nused in each answer as in (Rajpurkar et al. 2016). Speciﬁ-\ncally, we calculate F1 by ﬁnding the precision p(percentage\nof tokens in model answer also found in human answer) and\nrecall r (percentage of tokens in human answer also found\nin model answer) and deﬁning F1 as 100 \u00022pr=(p+ r).\nWe compare our results to a more traditional approach\nin which no facts are provided in the prompt and the same\nlanguage models are used. This approach typically provides\nmore of the actual document text to the language model,\nas the maximum prompt length remains the same. We note\nthat it is similar to the question answering approach used\nin (Radford et al. 2019), except that we do not prime the\nmodel with multiple question-answer pairs in order to ﬁt\nmore story text. We were unable to identify any other base-\nline approaches that could be meaningfully applied to the\ntask of answering questions about long form documents via\na pretrained language model.\nStory Completion\nWe make a distinction between story completion, in which\na system reads part of a human-written story and composes\na plausible end or next portion of the story, and story gen-\neration, in which a system composes a story ex nihilo. The\nmechanics of the two tasks are similar and we believe our\nmethods are applicable to both, but in this work we evalu-\nate only on story completion due to a lack of satisfactory\nautomated metrics for story generation.\nWe evaluate system performance using a dataset of Spar-\nknotes summaries of popular novels 2. We use summaries\nrather than stories because they contain highly factual and\ndetail-rich sentences without extra prose or dialogue, mak-\ning them well suited for comprehension tasks. Although the\nsummaries are shorter than the corresponding stories, they\nare still considerably longer than the stories used in compa-\nrable works of which we are aware. For example, the com-\nmonly used ROCStories dataset (Mostafazadeh et al. 2016)\ncontains an average of 53.5 tokens per story using GPT-2’s\ntokenizer, while the Sparknotes summaries contain an aver-\nage of 1166.3 tokens per story. For reference, the maximum\ntotal length of a combined prompt and response is 1024 to-\nkens with GPT-2 and 2048 tokens with GPT-3. In these ex-\nperiments we use a prompt length of 800 to allow sufﬁcient\nspace for the model’s response.\nWe evaluate by processing the ﬁrst paragraph of a human-\nwritten story and using it to predict the text of the second\nparagraph, then processing the ﬁrst two paragraphs and us-\ning them to predict the text of the third paragraph, etc. un-\ntil every paragraph has been predicted based on the pro-\ncessed preceding paragraphs. At each step we construct a\nknowledge graph from all of the story paragraphs seen so\nfar and select knowledge graph facts based on what is rele-\nvant to the most recently processed paragraph. We generate\na prompt consisting of document text with the verbalized\nknowledge graph facts inserted before the last story para-\ngraph. The prompt is limited to a maximum of 800 tokens,\nand the amount of document text is scaled appropriately. As\nwith question answering, we compare our results to a tradi-\ntional approach in which no facts are provided in the prompt,\nwhich typically causes more document text to be provided.\nWe use two automatic methods to evaluate the efﬁcacy of\nour enhanced prompts. Given a language modelL, a prompt\np, and a human-written completion c, the ﬁrst evaluation\nmethod is to generate a new completion ^c= generate(L;p)\nand measure the BLEU score between cand ^c. This method\nis currently the typical evaluation method for story comple-\ntion, but it relies on the faulty assumption that cis the “cor-\nrect answer” and that increased n-gram overlap between c\nand ^ccorrelates with generation quality. In reality, there are\nmany different ways to appropriately complete any given\nstory portion, and most will have little n-gram overlap with\nc. Additionally, ^cis stochastically generated by the language\nmodel, making it difﬁcult to replicate BLEU results. To\novercome these issues with BLEU evaluation, we introduce\na second story-completion evaluation metric based on per-\n2We use the summaries for the 606 novels found at https://www.\nsparknotes.com/lit/ at the time of writing. For each novel we collect\nthe text at https://www.sparknotes.com/lit/[story ID]/summary/. In\ntotal we make predictions on over 4000 paragraphs of text, and we\nmake all summaries available with the supplementary materials.\n10440\nLM Method LF-S F1 LF-Q F1\nGPT-2 Traditional Prompts 8.1 24.0\nOur Pipeline 18.6* 25.4\nGPT-3 Traditional Prompts 15.7 22.4\nOur Pipeline 20.0* 25.7\nXLNet Traditional Prompts 6.6 14.1\nOur Pipeline 12.9* 18.5*\nTable 1: Results of question answering experiments de-\nscribed in Section 4.1. LF-S and LF-Q are LF-SQuAD and\nLF-QUOREF respectively. F1 scores in this context measure\nthe precision and recall between bag-of-words tokens used\nin a human-written and predicted answer. Bolded scores are\nbetter and * indicates a signiﬁcant result (\u000b= 0.01).\nplexity (PPL). Given the same L, p, and cas above, we use\nthe language model to measure the perplexity ofcgiven p, or\nPPL = L( cjp). Note that we are no longer assuming that\ncis the only high quality completion. Rather, we treat cas\none sample from the population of high quality completions\nfor a given story. Thus, if our enhanced prompts consistently\nimprove the probability of generating c in the majority of\ncases, we infer that our method improves the probability of\ngenerating most high-quality completions and successfully\nincreases story comprehension. PPL is also non-stochastic,\nwhich improves the reliability of results. Unlike previous\nmetrics commonly used for evaluating language models, our\nPPL metric speciﬁcally measures the quality of prompts,\nmaking it effective at evaluating this and other automatic\nprompt engineering tasks.\nDue to the nature of the OpenAI API used to access GPT-\n3, we make two adjustments to the typical perplexity calcu-\nlation. First, because the API only gives access to the 100\nmost likely tokens at each generation step, we treat any to-\nken not appearing in the top 100 as having the same like-\nlihood as the 100th most likely token. This occurs for only\n4.4% of tokens with both traditional and enhanced prompts.\nSecond, due to cost and time limitations we run perplexity\ntests on only a subset (roughly 65%) of the Sparknotes test\ncases. Perplexity calculations using the other two language\nmodels are performed in the usual manner.\nModule Analysis\nIn order to better understand our system’s performance we\nmeasure accuracy after each module. We evaluate by mea-\nsuring the frequency with which the correct answer to each\nquestion-answer pair appears in our system at seven points\nin the pipeline: in the original document, after extracting\nknowledge graph facts, after applying ﬁltering rules, after\nselecting the most relevant facts from the knowledge graphs,\nafter verbalizing facts in natural language, after constructing\na prompt, and after generating a response with the language\nmodel. We use GPT-2 as the language model in these experi-\nments. We do not perform this analysis for story completion\nas the literal human-written completion will never appear in\nany portion of the system.\nLM Method BLEU PPL\nGPT-2 Traditional Prompts 0.0620 2.42\nOur Pipeline 0.0622 2.24*\nGPT-3 Traditional Prompts 0.0643* 10.93\nOur Pipeline 0.0455 10.89\nXLNet Traditional Prompts 0.0306* 1.61\nOur Pipeline 0.0254 1.56*\nTable 2: Results of story completion experiments on the\nSparknotes dataset described in Section 4.2. Bolded scores\nare better (higher for BLEU, lower for PPL) and * indicates\na signiﬁcant result (\u000b= 0.01).\nFigure 4: Percentage of correct answers retained by the sys-\ntem after each module of the QA Pipeline on two datasets.\nWhen desired information is lost, it usually happens dur-\ning the fact selection step and when applying the language\nmodel to generate a response. The ﬁltering step loses almost\nno correct answers and the verbalizing step adds correct an-\nswers. Constructing the prompt causes a spike in answer ap-\npearances because it re-incorporates as much of the docu-\nment text as possible given context length constraints.\nResults\nResults for the question answering experiments are found\nin Table 1, and story completion results are found in Ta-\nble 2. For question answering our prompt construction sys-\ntem outperforms traditional prompts on both datasets and\nusing all three language models, and results are statistically\nsigniﬁcant (\u000b = 0.01) in the majority of cases. LF-SQuAD\nhas a longer average document length than LF-QUOREF,\nwhich accounts both for the higher overall accuracy on LF-\nQUOREF and our system’s higher performance relative to\nthe baseline on LF-SQuAD. This underscores the difﬁculty\nof long-form question answering by language models and\nvalidates our system’s performance on the deﬁned task.\nFor story completion our prompts outperform traditional\nprompts on the PPL metric for all three language models,\nincluding signiﬁcant (\u000b = 0.01) improvements using GPT-2\nand XLNet. BLEU scores were less consistent; we achieve\nslightly better BLEU scores than traditional prompts using\n10441\nOriginal Text The age distribution was 6% aged 0–4 years, 14% aged 5–15 years, 4% aged 16–19 years. . .\nKG Fact [head:4% // relation:aged // tail:16-19 years]\nGPT-3 Verbalized 4% of the population aged 16–19 years.\nOriginal Text . . . Milo then leads his friends towards the Valley of Sound, where they meet . . . Dynne, a monster\nmade of smoke.\nKG Fact [head:a monster // relation:made // tail:of smoke]\nGPT-3 Verbalized The monster was made of smoke.\nOriginal Text As the two men eat borscht, a red Russian soup made of beets, Rainsford praises his host’s . . .\nKG Fact [head:the two men // relation:eat // tail:borscht]\nGPT-3 Verbalized The two men ate borscht.\nOriginal Text Perez gets injured and decides to stay behind . . .\nKG Fact [head:Perez // relation:stay // tail:behind]\nGPT-3 Verbalized Perez stayed behind.\nOriginal Text Kovrin is restless and does not sleep much, but he talks a great deal . . .\nKG Fact [head:Kovrin not // relation:sleep // tail:much]\nGPT-3 Verbalized Kovrin doesn’t sleep much.\nTable 3: Examples of original document text, knowledge graph facts extracted from the text, and the corresponding sentences\ngenerated by GPT-3 via few-shot learning. Note that GPT-3 is capable of recovering meaning found in the original text but not\nthe knowledge graph (e.g. the word “population” in the ﬁrst example, which is implied in the original text and omitted in the\nknowledge graph).\nGPT-2 and signiﬁcantly worse (\u000b= 0.01) BLEU scores with\nGPT-3 and XLNet. PPL scores measure the likelihood of\ngenerating the human-written response (and by assumption\nother appropriate responses) given the constructed prompt,\nwhile BLEU scores compare a randomly generated response\nto the human-written one.\nThe results for module-speciﬁc experiments are in Figure\n4. We ﬁnd that the performance of each module is similar on\nLF-SQuAD and LF-QUOREF. Most of the information loss\nthat occurs happens at two points: when selecting relevant\nfacts from the knowledge graph and when generating a ﬁnal\nanswer using the language model. This is not particularly\nsurprising, as these are the two phases where the most total\ninformation is pared out. The ﬁltering step loses almost no\ncorrect answers, despite the fact that it does remove a lot of\ninformation, which means that our ﬁltering rules are effec-\ntive at removing only irrelevant knowledge graph facts. The\nverbalizing step actually increases the percentage of cases\nwhere our system retains the correct answer. This is likely\ndue to cases when the correctcontent is contained in selected\nknowledge graph facts and GPT-3 adds important function\nwords or formatting. We ﬁnd similar trends when we mea-\nsure the percentage of correct answer tokens retained by the\nsystem rather than measuring only the presence of the com-\nplete correct answer.\nConclusion\nIn this work we have demonstrated that a dynamic knowl-\nedge graph containing document-speciﬁc information can\nenhance prompt generation for large language models,\nthereby mitigating the limitations of ﬁnite context lengths\nused by transformer-based language models. We have eval-\nuated our architecture on two story comprehension tasks,\nquestion answering and document completion, and believe\nthat it will be successfully applied to other tasks which re-\nquire comprehension of large documents. We are optimistic\nthat these and similar techniques will allow for more practi-\ncal and effective story generation systems in the future.\nEach part of the presented architecture is independently\ntrained for general purpose language processing, leaving\nmany opportunities for potential domain-speciﬁc improve-\nments. We are particularly interested in developing more tai-\nlored models for retrieving relevant facts from the knowl-\nedge graph and creating a feedback system for ﬁne-tuning\na domain-speciﬁc Open IE model. We also look forward to\nmore sophisticated retrieval methods, including predicting\nwhich knowledge graph facts are likely to be relevant in the\nfuture based on learned storytelling patterns. Even as it cur-\nrently stands with no domain-speciﬁc ﬁne-tuning, our archi-\ntecture is effective at enhancing the story comprehension of\nlarge language models.\nWe are encouraged by the rapid improvements that have\nbeen made in the area of neural story processing, including\nmany signiﬁcant developments in the past two years alone.\nAs researchers collectively have pushed the limits of what\nlarge language models are capable of, context window sizes\nhave proven to be a prohibitive obstacle. This work is an\ninitial attempt at mitigating the weaknesses of ﬁnite-length\ncontext windows, and it indicates great potential for this line\nof research going forward.\n10442\nAcknowledgements\nThis work has been funded in part by Latitude. We would\nlike to thank Nick Walton and Ben Swanson at Latitude for\ntheir mentorship throughout this project, and we would like\nto thank Latitude and OpenAI for providing access to GPT-3\nthrough the OpenAI API.\nReferences\nAdhikari, A.; Yuan, X.; Cˆot´e, M.-A.; Zelinka, M.; Rondeau,\nM.-A.; Laroche, R.; Poupart, P.; Tang, J.; Trischler, A.; and\nHamilton, W. 2020. Learning Dynamic Belief Graphs to\nGeneralize on Text-Based Games. In Larochelle, H.; Ran-\nzato, M.; Hadsell, R.; Balcan, M. F.; and Lin, H., eds.,\nAdvances in Neural Information Processing Systems, vol-\nume 33, 3045–3057. Curran Associates, Inc.\nAgarwal, O.; Ge, H.; Shakeri, S.; and Al-Rfou, R. 2021.\nKnowledge Graph Based Synthetic Corpus Generation for\nKnowledge-Enhanced Language Model Pre-training. In\nProceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 3554–3565.\nAinslie, J.; Ontanon, S.; Alberti, C.; Cvicek, V .; Fisher, Z.;\nPham, P.; Ravula, A.; Sanghai, S.; Wang, Q.; and Yang, L.\n2020. ETC: Encoding Long and Structured Inputs in Trans-\nformers. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing (EMNLP),\n268–284.\nAmmanabrolu, P.; and Riedl, M. 2019. Playing Text-\nAdventure Games with Graph-Based Deep Reinforcement\nLearning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), 3557–3565.\nAmmanabrolu, P.; Tien, E.; Hausknecht, M.; and Riedl,\nM. O. 2020. How to Avoid Being Eaten by a Grue: Struc-\ntured Exploration Strategies for Textual Worlds. arXiv\npreprint arXiv:2006.07409.\nAngeli, G.; Premkumar, M. J. J.; and Manning, C. D. 2015.\nLeveraging Linguistic Structure for Open Domain Informa-\ntion Extraction. In Proceedings of the 53rd Annual Meeting\nof the Association for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), 344–354.\nBosselut, A.; Bras, R. L.; and Choi, Y . 2019. Dynamic\nNeuro-Symbolic Knowledge Graph Construction for Zero-\nshot Commonsense Question Answering. arXiv preprint\narXiv:1911.03876.\nBosselut, A.; Rashkin, H.; Sap, M.; Malaviya, C.; Celikyil-\nmaz, A.; and Choi, Y . 2019. COMET: Commonsense Trans-\nformers for Automatic Knowledge Graph Construction. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, 4762–4779.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language Models are Few-Shot Learners.\narXiv preprint arXiv:2005.14165.\nDasigi, P.; Liu, N. F.; Marasovi ´c, A.; Smith, N. A.; and\nGardner, M. 2019. QUOREF: A Reading Comprehension\nDataset with Questions Requiring Coreferential Reasoning.\nIn Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 5925–5932.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. arXiv preprint arXiv:1810.04805.\nDistiawan, B.; Weikum, G.; Qi, J.; and Zhang, R. 2019. Neu-\nral Relation Extraction for Knowledge Base Enrichment. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, 229–240.\nFan, A.; Lewis, M.; and Dauphin, Y . 2018. Hierarchical\nNeural Story Generation. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), 889–898.\nGuan, J.; Huang, F.; Zhao, Z.; Zhu, X.; and Huang, M. 2020.\nA Knowledge-Enhanced Pretraining Model for Common-\nsense Story Generation. Transactions of the Association for\nComputational Linguistics, 8: 93–108.\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M. 2020.\nRetrieval Augmented Language Model Pre-Training. In In-\nternational Conference on Machine Learning, 3929–3938.\nPMLR.\nHe, H.; Balakrishnan, A.; Eric, M.; and Liang, P. 2017.\nLearning Symmetric Collaborative Dialogue Agents with\nDynamic Knowledge Graph Embeddings. arXiv preprint\narXiv:1704.07130.\nKumar, A.; Irsoy, O.; Ondruska, P.; Iyyer, M.; Bradbury, J.;\nGulrajani, I.; Zhong, V .; Paulus, R.; and Socher, R. 2016.\nAsk Me Anything: Dynamic Memory Networks for Natural\nLanguage Processing. In International Conference on Ma-\nchine Learning, 1378–1387. PMLR.\nLi, J.; Luong, M.-T.; and Jurafsky, D. 2015. A Hierarchi-\ncal Neural Autoencoder for Paragraphs and Documents. In\nProceedings of the 53rd Annual Meeting of the Associa-\ntion for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing (Volume\n1: Long Papers), 1106–1115.\nLiu, H.; and Singh, P. 2004. ConceptNet—A Practical\nCommonsense Reasoning Tool-Kit. BT technology journal,\n22(4): 211–226.\nMiller, G. A. 1995. WordNet: A Lexical Database for En-\nglish. Communications of the ACM, 38(11): 39–41.\nMostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Ba-\ntra, D.; Vanderwende, L.; Kohli, P.; and Allen, J. 2016. A\nCorpus and Cloze Evaluation for Deeper Understanding of\nCommonsense Stories. In Proceedings of the 2016 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\n839–849. San Diego, California: Association for Computa-\ntional Linguistics.\nNayak, T.; and Ng, H. T. 2020. Effective Modeling of\nEncoder-Decoder Architecture for Joint Entity and Relation\n10443\nExtraction. Proceedings of the AAAI Conference on Artiﬁ-\ncial Intelligence, 34(05): 8528–8535.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised\nMultitask Learners. OpenAI blog, 1(8): 9.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe Limits of Transfer Learning with a Uniﬁed Text-to-Text\nTransformer. Journal of Machine Learning Research, 21:\n1–67.\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension\nof Text. In Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, 2383–2392.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sen-\ntence Embeddings Using Siamese BERT-Networks. In Pro-\nceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing. Association for Computa-\ntional Linguistics.\nRoemmele, M. 2016. Writing Stories with Help from Re-\ncurrent Neural Networks. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 30.\nVrandeˇci´c, D.; and Kr ¨otzsch, M. 2014. Wikidata: A Free\nCollaborative Knowledgebase. Commun. ACM, 57(10):\n78–85.\nWang, C.; Ma, X.; Chen, J.; and Chen, J. 2018. Information\nExtraction and Knowledge Graph Construction from Geo-\nscience Literature. Computers & geosciences, 112: 112–\n120.\nXiong, C.; Merity, S.; and Socher, R. 2016. Dynamic Mem-\nory Networks for Visual and Textual Question Answering.\nIn International Conference on Machine Learning, 2397–\n2406. PMLR.\nXu, P.; Patwary, M.; Shoeybi, M.; Puri, R.; Fung, P.; Anand-\nkumar, A.; and Catanzaro, B. 2020. MEGATRON-CNTRL:\nControllable Story Generation with External Knowledge\nUsing Large-Scale Language Models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), 2831–2845.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR. R.; and Le, Q. V . 2019. Xlnet: Generalized autoregressive\npretraining for language understanding. Advances in neural\ninformation processing systems, 32.\nZhou, L.; and Small, K. 2019. Multi-Domain Dialogue State\nTracking as Dynamic knowledge Graph Enhanced Question\nAnswering. arXiv preprint arXiv:1911.06192.\n10444",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8147944211959839
    },
    {
      "name": "Comprehension",
      "score": 0.6623175740242004
    },
    {
      "name": "Question answering",
      "score": 0.5970089435577393
    },
    {
      "name": "Narrative",
      "score": 0.5500959157943726
    },
    {
      "name": "Language model",
      "score": 0.512939453125
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4957498013973236
    },
    {
      "name": "Natural language processing",
      "score": 0.4949728846549988
    },
    {
      "name": "Knowledge graph",
      "score": 0.4939034581184387
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4746197760105133
    },
    {
      "name": "Transformer",
      "score": 0.46676963567733765
    },
    {
      "name": "Architecture",
      "score": 0.4519649147987366
    },
    {
      "name": "Linguistics",
      "score": 0.22865182161331177
    },
    {
      "name": "Programming language",
      "score": 0.13361170887947083
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I100005738",
      "name": "Brigham Young University",
      "country": "US"
    }
  ]
}