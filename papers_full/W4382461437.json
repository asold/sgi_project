{
  "title": "PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor",
  "url": "https://openalex.org/W4382461437",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2128225123",
      "name": "Shun Lu",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2111353027",
      "name": "Yu Hu",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2224476665",
      "name": "Peihao Wang",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2105653060",
      "name": "Yan Han",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2153587335",
      "name": "Jianchao Tan",
      "affiliations": [
        "Kuaishou (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100285767",
      "name": "Jixiang Li",
      "affiliations": [
        "Kuaishou (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2095688776",
      "name": "Sen Yang",
      "affiliations": [
        "Snap (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2105518551",
      "name": "Ji Liu",
      "affiliations": [
        "Meta (United States)",
        "BC Platforms (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2128225123",
      "name": "Shun Lu",
      "affiliations": [
        "Institute of Computing Technology",
        "Intel (United States)",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2111353027",
      "name": "Yu Hu",
      "affiliations": [
        "Institute of Computing Technology",
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2224476665",
      "name": "Peihao Wang",
      "affiliations": [
        "The University of Texas at Austin",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2105518551",
      "name": "Ji Liu",
      "affiliations": [
        "Meta (United States)",
        "BC Platforms (Finland)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2556833785",
    "https://openalex.org/W2767274188",
    "https://openalex.org/W2885820039",
    "https://openalex.org/W2902251695",
    "https://openalex.org/W3179517581",
    "https://openalex.org/W2942263598",
    "https://openalex.org/W3133959206",
    "https://openalex.org/W3041166015",
    "https://openalex.org/W3082519296",
    "https://openalex.org/W2953604046",
    "https://openalex.org/W2991221016",
    "https://openalex.org/W2804047946",
    "https://openalex.org/W2126105956",
    "https://openalex.org/W2980270353",
    "https://openalex.org/W2998030011",
    "https://openalex.org/W2976485264",
    "https://openalex.org/W2932077855",
    "https://openalex.org/W4285600331",
    "https://openalex.org/W6794020371",
    "https://openalex.org/W6770564647",
    "https://openalex.org/W6772715067",
    "https://openalex.org/W6653110446",
    "https://openalex.org/W6746582238",
    "https://openalex.org/W6600669965",
    "https://openalex.org/W2786460182",
    "https://openalex.org/W3213098846",
    "https://openalex.org/W6753701838",
    "https://openalex.org/W3014900121",
    "https://openalex.org/W2624503621",
    "https://openalex.org/W3127389359",
    "https://openalex.org/W2018764772",
    "https://openalex.org/W6681029592",
    "https://openalex.org/W3093858143",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W2938428612",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6747904511",
    "https://openalex.org/W6770616408",
    "https://openalex.org/W6780197768",
    "https://openalex.org/W6640300118",
    "https://openalex.org/W6757204547",
    "https://openalex.org/W2999049229",
    "https://openalex.org/W3133074276",
    "https://openalex.org/W2916118939",
    "https://openalex.org/W2942465004",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2962847160",
    "https://openalex.org/W3118344311",
    "https://openalex.org/W4287725919",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2771751675",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2746314669",
    "https://openalex.org/W2964259004",
    "https://openalex.org/W4232872062",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3176772026",
    "https://openalex.org/W3130159758",
    "https://openalex.org/W2997806280",
    "https://openalex.org/W4287907702",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3153131045",
    "https://openalex.org/W4287725527",
    "https://openalex.org/W2810240468",
    "https://openalex.org/W4297778814",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W2990613095",
    "https://openalex.org/W3047744011",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W2964253930",
    "https://openalex.org/W3173166478",
    "https://openalex.org/W2963830382",
    "https://openalex.org/W2981748264",
    "https://openalex.org/W4295727797",
    "https://openalex.org/W2963415211",
    "https://openalex.org/W3107453328",
    "https://openalex.org/W2142498761",
    "https://openalex.org/W3034202788",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W3094801149",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W2963821229",
    "https://openalex.org/W3132602976",
    "https://openalex.org/W3202742610",
    "https://openalex.org/W4287242089",
    "https://openalex.org/W3041574813",
    "https://openalex.org/W2981406437",
    "https://openalex.org/W3132083952",
    "https://openalex.org/W2951245151",
    "https://openalex.org/W3107893198",
    "https://openalex.org/W4295185264",
    "https://openalex.org/W2888429796"
  ],
  "abstract": "Time-consuming performance evaluation is the bottleneck of traditional Neural Architecture Search (NAS) methods. Predictor-based NAS can speed up performance evaluation by directly predicting performance, rather than training a large number of sub-models and then validating their performance. Most predictor-based NAS approaches use a proxy dataset to train model-based predictors efficiently but suffer from performance degradation and generalization problems. We attribute these problems to the poor abilities of existing predictors to character the sub-models' structure, specifically the topology information extraction and the node feature representation of the input graph data. To address these problems, we propose a Transformer-like NAS predictor PINAT, consisting of a Permutation INvariance Augmentation module serving as both token embedding layer and self-attention head, as well as a Laplacian matrix to be the positional encoding. Our design produces more representative features of the encoded architecture and outperforms state-of-the-art NAS predictors on six search spaces: NAS-Bench-101, NAS-Bench-201, DARTS, ProxylessNAS, PPI, and ModelNet. The code is available at https://github.com/ShunLu91/PINAT.",
  "full_text": "PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor\nShun Lu1, 2, Yu Hu1,2*, Peihao Wang3, Yan Han3, Jianchao Tan4, Jixiang Li4, Sen Yang5, Ji Liu6\n1 Research Center for Intelligent Computing Systems, Institute of Computing Technology, Chinese Academy of Sciences\n2 School of Computer Science and Technology, University of Chinese Academy of Sciences\n3 University of Texas at Austin\n4 Kuaishou Technology.\n5 Snap Inc.\n6 Meta Platforms, Inc.\nflushun19s, huyug@ict.ac.cn, fpeihaowang, yh9442g@utexas.edu, fjianchaotan, lijixiangg@kuaishou.com,\nsyang3@snap.com, ji.liu.uwisc@gmail.com\nAbstract\nTime-consuming performance evaluation is the bottleneck\nof traditional Neural Architecture Search (NAS) methods.\nPredictor-based NAS can speed up performance evaluation\nby directly predicting performance, rather than training a\nlarge number of sub-models and then validating their perfor-\nmance. Most predictor-based NAS approaches use a proxy\ndataset to train model-based predictors efﬁciently but suffer\nfrom performance degradation and generalization problems.\nWe attribute these problems to the poor abilities of existing\npredictors to character the sub-models’ structure, speciﬁcally\nthe topology information extraction and the node feature rep-\nresentation of the input graph data. To address these prob-\nlems, we propose a Transformer-like NAS predictor PINAT,\nconsisting of a partial Permutation INvariance Augmentation\nmodule serving as both token embedding layer and atten-\ntion head, as well as a Laplacian matrix to be the positional\nencoding. Our design produces more representative features\nof the encoded architecture and outperforms state-of-the-art\nNAS predictors on six search spaces: NAS-Bench-101, NAS-\nBench-201, DARTS, ProxylessNAS, PPI, and ModelNet. The\ncode is available at https://github.com/ShunLu91/PINAT.\nIntroduction\nNeural architecture search (NAS) has made great progress\nin many domains by automatically designing effective ar-\nchitectures. One of the key factors behind these progress is\nthe innovation of efﬁcient search methods, such as one-shot\nmethods (Bender et al. 2018; Dong and Yang 2019; Guo\net al. 2020; Chu et al. 2021b), differentiable methods (Liu\net al. 2019a; Xie et al. 2019; Chen et al. 2019; Chu et al.\n2020), and predictor-based methods (Liu et al. 2018; Luo\net al. 2018; Zhang et al. 2019; Shi et al. 2020). In particular,\npredictor-based methods are promising as they can learn an\naccurate mapping from the architecture’s representation to\nits corresponding performance in a pre-deﬁned search space\nwith only a few training samples, and thus signiﬁcantly im-\nproving NAS efﬁciency.\n*Corresponding author.\nCopyright © 2023, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nMost predictor-based methods usually train a perfor-\nmance predictor with a proxy dataset, i.e. architecture-\naccuracy pairs as the training dataset. By utilizing the pre-\ntrained predictor, the performance of arbitrary network ar-\nchitectures in the same search space can be directly queried,\nthus greatly facilitating the search process. As architectures\nare represented by discrete encodings, most predictor-based\nmethods ﬁrst embed the discrete data into a continuous la-\ntent space and then excavate useful features to model the\nmapping relationship. Therefore, how to effectively encode\nthe discrete architecture becomes essential.\nMoreover, in our scenarios, when permuting the operation\nmatrix and adjacency matrix, we can get the same graphs,\nwhich can be referred to as the graph isomorphism (GI) (Le-\nman and Weisfeiler 1968). Generally, how to enable the per-\nmutation invariance properties for the input graph features to\nlet our predictor recognize such architecture graph isomor-\nphism becomes another challenge.\nPrevious works have proposed the sequence-based\nscheme and the graph-based methods to address this issue in\nthe NAS predictor scenario. The former is encoding compu-\ntation ﬂow of graph instead of the graph itself, and the latter\nis usually taking a GNN with a kind of isomorphism prop-\nerty to process architecture graph data directly, for example,\nGraph Isomorphism Networks (Xu et al. 2018). However,\nboth methods suffer from the poor encoding abilities and\nstill struggle to handle architecture permutation invariance.\nIn this paper, we aim to design a more powerful NAS\npredictor while behaving permutation invariance for archi-\ntectural isomorphism. Several recent NAS predictor works\n(Yan et al. 2021; Lu et al. 2021) have proven the effective-\nness of self-attention in encoding architectures with global\npermutation invariance, which can output the same results\nwhen permuting the inputs. And PINE (Gui et al. 2021) pro-\nposed a partial permutation invariance embedding method\nby modeling the dependence of each node to its neighbors,\nand achieved promising results on various tasks. Inspired by\nthe novel combination of the global and the local attention\nin previous works (Chen et al. 2021a; Raghu et al. 2021),\nwe propose to combine the partial and global permutation\ninvariance properties together, to excavate more representa-\ntive features for input graph data.\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n8957\nSpeciﬁcally, to let the predictor better recognize the NAS\narchitecture isomorphism, which is due to the permutation\ninvariance property on the operation and adjacency matrix,\nwe propose a partial permutation invariant token embed-\nding (PITE) and a partial permutation invariant self-attention\n(PISA) by plugging the partial permutation invariance mod-\nule (PIM) into different places of Transformer, to obtain par-\ntial and global permutation invariance augmented architec-\ntural representations. Our method does not need to compute\nmessage ﬂow from the architecture graph or create augmen-\ntation data pairs during training like previous work. The con-\ntributions are summarized as:\n• We propose a Transformer-like NAS predictor PINAT\nwhich consists of a partial permutation invariance mod-\nule (PIM) serving as both token embedding layer (PITE)\nand self-attention head (PISA), as well as a Laplacian\nmatrix based positional encoding.\n• PINATenables partial and global permutation invariance\non graph node features to handle architectural isomor-\nphism for the NAS predictor task, which belongs to the\ngraph regression task, and captures good representations\nfrom discrete architectures in the meantime. Therefore,\nit is especially good at predicting network performance\nwith limited data, and it can help to design good models\non general graph tasks.\n• With the same data splits as prior works, our proposed\nPINAT outperforms recent state-of-the-arts in terms of\nthe ranking performance on NAS-Bench-101 (Ying et al.\n2019) and NAS-Bench-201(Dong and Yang 2020). By\nconducting open-domain search on DARTS (Liu et al.\n2019a) and ProxylessNAS (Cai, Zhu, and Han 2019)\nsearch spaces, our searched CNN architectures reach\nstate-of-the-art performance on CIFAR-10 and ImageNet\ndataset. When searching for GCN architectures as SGAS\n(Li et al. 2020a), PINAT also achieves comparable per-\nformance on PPI and ModelNet (Wu et al. 2015) datasets.\nRelated Works\nNeural architecture search With the widespread use of\ndeep neural networks for various tasks, it is becoming in-\ncreasingly difﬁcult to create cutting-edge hand-crafted de-\nsigns based on expert knowledge. NAS has emerged as a\npromising technique to automatically search for superb ar-\nchitectures. However, due to the unaffordable search cost\nof reinforcement learning-based NAS methods (Baker et al.\n2017a; Zoph and Le 2017), many researchers turn to ef-\nﬁciently search for optimal architectures in a pre-deﬁned\nsearch space. These methods can be roughly divided into\nthree categories: one-shot NAS methods (Bender et al. 2018;\nGuo et al. 2020), differentiable NAS methods (Liu et al.\n2019a; Xie et al. 2019; Chu et al. 2021a), and predictor-\nbased NAS methods (Baker et al. 2017b; Luo et al. 2018;\nZhang et al. 2019; Li et al. 2020b; Shi et al. 2020) which are\ndescribed in the following subsection.\nPredictor-based NAS methods Most predictor-based\nNAS methods follow a two-stage paradigm: (1) train a per-\nformance predictor to model the mapping relationship be-\ntween architectures and their corresponding performance,\n(2) use a heuristic algorithm to search for good architec-\ntures. As architectures are represented by discrete encod-\nings, the effectiveness of the modeling process is of vital im-\nportance. Various encoding tools have been utilized to trans-\nform the discrete representation into a meaningful continu-\nous latent space, such as the embedding matrix (Deng, Yan,\nand Lin 2017; Luo et al. 2018; Zhang et al. 2019; Ning et al.\n2020b; Cheng et al. 2021), GCN (Li, Gong, and Zhu 2021;\nWen et al. 2020; Shi et al. 2020; Chen et al. 2021b), and\nMLPs (Xu et al. 2021). Behind the encoding module, a sim-\nple regressor is employed to predict the performance of the\nencoded architecture. With the pre-trained predictor, prior\nworks have explored Bayes Optimization methods (Zhang\net al. 2019; Yan et al. 2020; Shi et al. 2020; Ru et al. 2021),\nEvolutionary methods (Ning et al. 2020b; Xu et al. 2021)\nor the simple random methods (Deng, Yan, and Lin 2017;\nBaker et al. 2017b; Wen et al. 2020; Luo et al. 2020; Cheng\net al. 2021; Chen et al. 2021b) to search for superb architec-\ntures. Detailed surveys can be referred to (White et al. 2020;\nNing et al. 2020a; White et al. 2021a).\nBesides, CATE (Yan et al. 2021) and TNASP (Lu et al.\n2021) are the most similar works to ours. However, CATE\n(Yan et al. 2021) adopts more Transformer encoder blocks\nwith more parameters and requires more training data, while\nTNASP (Lu et al. 2021) tries to further improve the perfor-\nmance by the proposed Self-evolution framework, which in-\ntroduces more hyper-parameters and consumes more train-\ning time. Moreover, they directly borrow the original Trans-\nformer encoder block which only has global permutation in-\nvariance property, while our method enabled both the global\nand partial permutation invariance property, thus outper-\nforming them obviously on public benchmark datasets.\nPermutation invariance for graphs As mentioned be-\nfore, in our scenarios, when permuting the operation matrix\nand adjacency matrix, we can get the same graphs, which\ncan be referred to as the graph isomorphism (GI) (Leman\nand Weisfeiler 1968). How to enable the permutation in-\nvariance augmentation on the input graph features to let our\npredictor recognize the architecture graph isomorphism be-\ncomes another challenge. Previous works have proposed the\nsequence-based scheme and the graph-based method to ad-\ndress this issue in the NAS predictor scenario. The former\nis to encode the computation ﬂow instead of the graph, and\nthe latter is usually taking GNN with isomorphism property\nto process architecture graph data. BANANAS (White et al.\n2021b) introduced a path-based sequence encoding scheme,\nwhich naturally has the property of permutation invariance\nbut scales exponentially with the network depth. D-V AE\n(Zhang et al. 2019) and GATES (Ning et al. 2020b) simu-\nlated the computation ﬂow by sequentially performing mes-\nsage passing for nodes following a topological ordering of\nthe DAG, which ensures the encoder is invariant to node\npermutations. Arch2vec (Yan et al. 2020) directly utilized\nthe Graph Isomorphism Networks (GINs) (Xu et al. 2018) to\nencode architectures to handle the permutation invariance is-\nsue. Moreover, NAO (Luo et al. 2018) and CATE (Yan et al.\n2021) computationally generated similar data pairs to aug-\nment the encoder training, by assuming similar input graphs\n8958\nRegressor (FC x 2)\nOne-hot\nEncoding\nLaplacian Matrix Operation Vector\nBackbone (Encoder Blocks x 3)\nEmbedding \nMatrix\nPIM for Token\nEmbedding (PITE)\nEncode Raw Inputs\nSelf-attention PIM for Self-\nattention (PISA)\nConcat\nAdd & Norm\nFeed Forward\nAdd & Norm\nFeed Forward\nPermutation Invariance  \nModule (PIM) \nFC Hidden Units \nFinal Prediction \nFC Inputs \nFeed\nForward\nAdd \nFigure 1: Our Transformer-like NAS predictor. We map the information of operations and connections into continuous repre-\nsentation, followed by 3 encoder blocks and a 2-layer fully-connected regressor to derive the ﬁnal prediction.\nshould have similar latent space representations. NASGEM\n(Cheng et al. 2021) and InterpretableNAS (Ru et al. 2021)\nadopted WL-Kernel (Shervashidze et al. 2011) to measure\nthe similarity of two graphs to obtain a similarity loss. These\nfour works are a little related to the permutation invariance\naugmentation purpose.\nTo overcome the aforementioned issues, we propose a\nnovel NAS predictor PINAT, including a novel permutation\ninvariance module (PIM) severing for token embedding and\nself-attention calculation. PINAT produces more representa-\ntive features for architecture graph data, while also ensuring\npartial permutation variance in end-to-end training.\nMethodology\nPreliminary\nIn the context of deep neural networks, we can construct var-\nious neural architectures by assembling numerous operation\nunits (O= fo1;o2;:::;o F g, where F denotes the number of\noperations) with different connections. As long as the can-\ndidate operation corpus Ois ﬁnite, a uniﬁed encoding rule\ncan be applied to deﬁne the candidate operations. For exam-\nple, in neural architecture search, an architecture \u000bwith N\nlayers in a pre-deﬁned search space Scan be represented by\na discrete operator V 2RN\u00021 with selected operation in-\ndices and a deﬁnite connection matrix i.e. adjacency matrix\nA2 RN\u0002N as below,\nV = [o1;o2;:::;o N ]; \u000b = (V;A); \u000b 2S (1)\nwhere oi represents the index of the selected operation from\nthe candidate operation corpus Ofor i-th layer. In this vein,\neach architecture owns an encoding sequence with an ad-\njacency matrix, and thus all of them can be represented by\na uniﬁed encoding rule, which is also regarded as Directed\nAcyclic Graph structure (DAG) in some literature.\nIn light of that, predictor-based NAS methods usually\nadopt an encoder E to transform the discrete architecture\n\u000binto continuous representation, followed by a regressor R\nto predict the architecture’s performance Y, which can be\nformulated as below:\nY= R(E(\u000b)) (2)\nBy learning from a few labeled data, most predictor-based\nNAS methods have the ability to model the potential re-\nlationship between the architecture \u000b and performance Y.\nSuch predictor can be plugged into standard NAS search\nmethods, for example, one kind of heuristic algorithm to\nhelp discover architectures efﬁciently and effectively, thus\nreducing the total search cost.\nPermutation Invariance Module (PIM)\nAs has been frequently mentioned in previous works (Luo\net al. 2018; Zhang et al. 2019; Yan et al. 2020; White et al.\n2021b; Cheng et al. 2021; Ru et al. 2021), the permutation\ninvariance is a necessary and crucial property for input graph\ndata of NAS predictor task. In this paper, we expect to dis-\ncover a fruitful and efﬁcient module design to augment such\nproperty for the NAS predictor without the need of any labo-\nrious and tedious data augmentation as in (Luo et al. 2018;\nYan et al. 2021).\nRecently, CATE (Yan et al. 2021) and TNASP (Lu et al.\n2021) have demonstrated the effectiveness of the self-\nattention module in encoding the architectures. This widely\nused self-attention module proposed by Transformer struc-\nture (Vaswani et al. 2017) has global permutation invariance\nas it computes the pairwise interactions using row-wise com-\nputation functions. On the other hand, PINE (Gui et al. 2021)\nhas introduced a partial permutation invariant graph node\nembedding method via effectively modeling the dependence\nof each node to its neighbors. They apply a neural network to\napproximate the desired partial permutation invariant func-\ntion and achieve promising results on various tasks. Inspired\nby previous works (Chen et al. 2021a; Raghu et al. 2021), we\n8959\nLinear\nAdd\nTransform Matrix \n(Rank=1)\nMatMul Activation\nPermutation Invariance Module\nActivation\n3 ×\nFigure 2: Structure of the permutation invariance module\n(PIM). XVk is the concatenation of feature vectors for all\nneighbors of node Vk. We adopt ReLU function as the acti-\nvation layer inside residual block and ELU for the activation\nlayer outside the residual block. Transform Matrix here is a\nconcatenation of multiple weight matrices with rank 1, fol-\nlowing the original settings in PINE (Gui et al. 2021).\npropose to obtain more representative features by enabling\nboth global and partial permutation invariance.\nSpeciﬁcally, we propose to aggregate permutation invari-\nant features extracted from the self-attention module and the\npartial permutation invariance module (PIM) together. The\ndesign of PIM is inspired by PINE (Gui et al. 2021), where\nthey have proved that such a set function can be approxi-\nmated as at least a four-layer neural network, including a\nlinear layer W1 (with each sub-matrix’s rank equal to 1) per-\nformed on concatenated neighborhood node features XVk , a\nlinear layer W2 to transform the aggregated neighborhood\nfeatures into ﬁnal features for the center node, and activation\nlayers. We propose to adopt a variant of their design: we re-\npeat W1 associated with an additional residual connection\nfor three times, as shown in Fig.2. In this way, our design\ncan approximate a map function PIM(x), whose extracted\nfeatures epim 2RN\u0002M inherently have the partial permu-\ntation invariance property,\nepim = PIM(oneHot(V)) (3)\nwhere Mdenotes the embedding dimension. We further pro-\npose to incorporate this structure into the process of token\nembedding and self-attention to produce more representa-\ntive features.\nPIM for Token Embedding We apply PIM at the token\nprocessing stage, by simply adding the partial permutation\ninvariance augmented node featuresepim into standard node\nembeddings and node positional encodings, noted as PITE.\nAs a result, the combined features will consist of three prop-\nerties: original node identities, topology information, and\npartial permutation invariance, which are sufﬁcient to repre-\nsent the whole graph data. We combine three embedding fea-\ntures together by simple addition as the input feature xpinat\nof subsequent layers,\nxpinat = eop + epos + epim (4)\nwhere eop = B(V) denotes the continuous representation\nof the discrete operation vector extracted by an embedding\nmatrix B 2RF\u0002M , and epos 2RN\u0002M stands for positional\nencodings transformed from the Laplacian matrix of graph.\nPIM for Self-Attention To further augment the permuta-\ntion invariance property during the information passing for\nsuch cascaded stacking structure, we propose to concatenate\nthe partially augmented featureepim together with the global\npermutation invariant feature of the standard self-attention\nhead at each stacked layer. We denote the PIM module for\nself-attention as PISA. The insight is that PIM actually per-\nforms a local self-attention between nodes and their neigh-\nbors. The calculation of the j-th layer is formulated as:\nTj(xj\u00001) = Aggregation( Concat(SA1(xj\u00001); : : : ;SAn(xj\u00001);\nPISA1(xj\u00001); : : : ;PISAm(xj\u00001)))\n(5)\nwhere xj\u00001 = Tj\u00001(xj\u00002), j = 2;:::;n. The number of SA\nheads and PISA heads are denoted bynand m, respectively.\nWe use the operator Aggregation to represent the ensemble\noperation, including linear, element-wise addition, and layer\nnormalization operations, which follow the standard atten-\ntion block. By aggregating the features with global and par-\ntial permutation invariance properties, we get a more distin-\nguishable representation of input architectures, which helps\nin modeling the mapping relationship for the predictor.\nA Transformer-Like NAS Predictor\nBy assembling the proposed modules PITE and PISA, and\nattaching a regression head Rat the end to model the rela-\ntionship between the representative featuresxn and its actual\nperformance, the ensemble model design of our proposed\nPINAT is shown in Fig.1 and can be formulated as below,\n( x1 = eop + epim + epos\nxj = Ti(xj\u00001); j = 2;:::;n\nY = R(xn)\n(6)\nThere are many choices for the regressor and even the\nclassical machine learning methods can be applied here (Wu\net al. 2021) such as Gradient Boosting Regression Tree and\nRandom Forest. To achieve end-to-end training, we choose\ntwo fully connected layers and compute mean square loss\nbetween Yand Yture. In the following, we perform exten-\nsive experiments to verify the effectiveness of our design.\nExperiments\nWe conduct experiments over six public benchmark search\nspaces. We ﬁrst compare the ranking ability of PINAT via\nvarious train-test data splits on NAS-Bench-101 (Ying et al.\n2019) and NAS-Bench-201 (Dong and Yang 2020). Then\nwe search for CNN architectures on CIFAR-10 and Ima-\ngeNet (Krizhevsky et al. 2017) datasets using DARTS (Liu\net al. 2019a) and ProxylessNAS (Cai, Zhu, and Han 2019)\nsearch spaces, respectively. Next, we further search for GCN\narchitectures on PPI and ModelNet10 datasets to compare\nthe performance of the searched GCN with SGAS (Li et al.\n2020a). We provide detailed ablation studies to discuss the\neffect of PITE and PISA, compare our design with the orig-\ninal PINE module, and analyze the better performance of\nPINAT. We provide the ablations of positional encodings,\n8960\nPIM hidden dimensions, SA and PISA heads number, the\napplication of the ranking loss, and more implementation\ndetails about each search space, and the visualization of our\nsearched architectures in the supplementary material.\nRanking Results on NAS Benchmarks\nNAS-Bench-101 (Ying et al. 2019) and NAS-Bench-201\n(Dong and Yang 2020) are widely used benchmarks in re-\ncent years. The former contains 423,624 architectures in to-\ntal and each architecture is comprised of basic layers and\n9 repeated cells, each of which has 7 nodes and 9 edges at\nmost. Each node represents 3 candidate operations and their\nconnection are denoted by the edges. The latter has a differ-\nent search space, where each edge can be selected from 5\ncandidate operations. Architectures are also constructed by\na pre-deﬁned skeleton, i.e. 15 repeated cells and 4 nodes in\neach cell, which results in 15,625 network structures.\nBoth benchmarks provide the validation accuracy and\nthe test accuracy for each architecture, and we utilize the\nformer to train our predictor while using the latter for\nevaluation. To provide an apples-to-apples comparison, we\nfollow the same data splits in TNASP (Lu et al. 2021),\nand noted as S1;S2;S3;S4;S5 on NAS-Bench-101 and\nS0\n1;S0\n2;S0\n3;S0\n4;S0\n5 on NAS-Bench-201, detailed in Tab.10\nof our Supp. We evaluate the ranking performance using\nKendall’s Tau (Sen 1968) between the predicted accuracy\nand the actual accuracy of test samples.\nResults The ranking results of PINAT are shown in Tab.1.\nNoticeably, our PINAT gets the highest scores in all data\nsplits on both benchmarks, surpassing recent state-of-the-art\nmethods such as TNASP (Lu et al. 2021) and GMAE-NAS\n(Jing, Xu, and Li 2022). Such results substantiate the over-\nwhelming advantages of our design, clearly demonstrating\nthat aggregating the features with global and partial permu-\ntation invariance is effective.\nSearch for CNN Architectures on CIFAR-10\nCIFAR-10 is a standard image classiﬁcation dataset, con-\ntaining 50,000 training samples and 10,000 test samples\nof image size 32\u000232\u00023, which are divided into 10 object\nclasses. Based on the CIFAR-10 dataset, DATRS (Liu et al.\n2019a) proposed a cell search space, which is popular and\nadopted by many previous works. We also choose it as one\nof our open-domain experiments.\nWe follow main procedures of CTNAS (Chen et al.\n2021b) and TNASP (Lu et al. 2021) to search for CNN\narchitectures on DARTS (Liu et al. 2019a) search space.\nSpeciﬁcally, we utilize the uniform sampling method (Guo\net al. 2020) to pre-train a supernet on the training set for\n120 epochs and evaluate 1k architectures on the validation\ndataset by inheriting the pre-trained supernet weights to per-\nform efﬁcient inference to get architecture-accuracy pairs\nfor our predictor to learn a mapping relationship. In this way,\nour pre-trained predictor can be used to query any architec-\nture’s performance in this search space and we choose the\nevolutionary algorithm (Deb et al. 2002) to search for op-\ntimal architectures for 100 generations and maintain a pop-\nulation of size 100 in each generation. Finally, we re-train\nNAS-Bench-101 S1 S2 S3 S4 S5\nSPOSy - - 0.196 - -\nFairNASy - - -0.232 - -\nReNASy - - 0.634 0.657 0.816\nRegressionNASy - - 0.430 - -\nNPz 0.391 0.545 0.710 0.679 0.769\nNAOz 0.501 0.566 0.704 0.666 0.775\nArch2Vec? 0.435 0.511 0.561 0.547 0.596\nD-V AE? 0.530 0.549 0.671 0.626 0.698\nGATES? 0.605 0.659 0.666 0.691 0.822\nGraphTrans? 0.330 0.472 0.600 0.602 0.700\nGraphormer? 0.564 0.580 0.596 0.611 0.797\nCTNAS - - 0.751 - -\nTNASP 0.600 0.669 0.752 0.705 0.820\nGMAE-NAS? 0.666 0.697 0.788 0.732 0.775\nPINAT 0.679 0.715 0.801 0.772 0.846\nNAS-Bench-201 S0\n1 S0\n2 S0\n3 S0\n4 S0\n5\nNPz 0.343 0.413 0.584 0.634 0.646\nNAOz 0.467 0.493 0.470 0.522 0.526\nArch2Vec? 0.542 0.573 0.601 0.606 0.605\nGraphTrans? 0.409 0.550 0.594 0.588 0.673\nGraphormer? 0.505 0.630 0.680 0.719 0.776\nTNASP 0.539 0.589 0.640 0.689 0.724\nPINAT 0.549 0.631 0.706 0.761 0.784\nTable 1: Ranking results on NAS-Bench-101 and NAS-\nBench-201. y: results from CTNAS (Chen et al. 2021b). z:\nreported by TNASP (Lu et al. 2021).?: implemented by our-\nselves using their released models.\nour searched architectures with common DARTS strategies,\ni.e. 600 epochs by the SGD optimizer with the initial learn-\ning rate 2.5e-2 and weight decay 3e-4, and use the Cutout\n(DeVries and Taylor 2017) as the data augmentation.\nResults Tab. 2 summarizes the comparison of evaluation\nresults with recent state-of-the-arts and we report the aver-\nage accuracy of three searched architectures and the best ac-\ncuracy to ensure a fair comparison. The ﬁrst row shows the\nperformance of well-known NAS methods, and predictor-\nbased NAS methods are placed in the second row. We can\nsee that PINAT obtains the best accuracy of 97.58 and the av-\nerage accuracy of 97.46\u00060.08, outperforming all the others\nwith similar model parameters and the least GPU days 0.3.\nSearch for CNN Architectures on ImageNet\nTo further validate the robustness of our method, we trans-\nfer the best searched architecture on DARTS to ImageNet\n(Krizhevsky et al. 2017) noted as PINAT-T and perform an-\nother architecture search on ImageNet with a distinct chain-\nstyled search space proposed by ProxylessNAS (Cai, Zhu,\nand Han 2019) namely PINAT-S.\nImageNet is a large-scale dataset with 1.28 million train-\ning images and 50,000 validation images, consisting of\n1,000 classes. We strictly follow the transferring conﬁgu-\nration of DARTS (Liu et al. 2019a) to re-train PINAT-T\nand adopt the same search procedure as mentioned above\n8961\nMethod #P Best Average Cost\n(M) Acc.(%) Acc.(%) (G\u0001D)\nNASNet-A? 3.3 97.35 - 1,800\nAmoebaNet-A 3.2 - 96.66 \u00060.06 3,150\nENAS 4.6 97.11 - 0.5\nGHN 5.7 - 97.16 \u00060.07 0.8\nDARTS 3.3 - 97.24 \u00060.09 4\nPNASy 3.2 - 97.17 \u00060.07 -\nNAO? 10.6 97.52 - 200\nD-V AE - 94.80 - -\nGATES 4.1 97.42 - -\nArch2vec-BO 3.6 97.52 97.44 \u00060.05 100\nGP-NAS 3.9 96.21 - 0.9\nBONAS-D 3.3 97.57 - 10.0\nBANANASz 3.6 - 97.33\u0006 0.07 11.8\nNAS-BOWL 3.7 97.50 97.39 \u00060.08 3\nCATE? 3.5 - 97.45 \u00060.08 3.3\nCTNAS 3.6 - 97.41 \u00060.04 0.3\nTNASP 3.6 97.48 97.43 \u00060.04 0.3\nPINAT 3.6 97.58 97.46 \u00060.08 0.3\nTable 2: Comparison with state-of-the-art NAS methods on\nCIFAR-10 using DARTS search space. Search cost is mea-\nsured by the GPU days. ?: we report the results with simi-\nlar budget to get a fair comparison. y: results from CTNAS\n(Chen et al. 2021b). z: reported by CATE (Yan et al. 2021).\nto search for the PINAT-S. We employ the training strate-\ngies of EfﬁcientNet (Tan and Le 2019) to re-train PINAT-S\nfrom scratch for evaluation. Concretely, we re-train PINAT-\nS for 450 epochs with the batch size of 320. RMSpropTF\noptimizer is adopted with the initial learning rate 0.16 and\nweight decay 1e-5. To prevent over-ﬁtting, we also use the\nAutoAug (Cubuk et al. 2019) and RE (Zhong et al. 2020) to\naugment the training images.\nResults We summarize the results in Tab. 3. PINAT-T con-\nsumes fewer FLOPs and exceeds PNAS (Liu et al. 2018),\nNAO (Luo et al. 2018), Arch2vec-BO (Yan et al. 2020),\nBONAS-D (Shi et al. 2020), BANANAS (White et al.\n2021b), and CATE (Yan et al. 2021). PINAT-S achieves the\nhighest accuracy compared with others and outperforms CT-\nNAS (Chen et al. 2021b) by 0.5 in terms of top-1 accuracy\nwith fewer FLOPs. Such improvement is non-trivial com-\npared with those improvements claimed in previous works.\nSearch for GCN Architectures on PPI\nTo demonstrate the generality of our method, we also adopt\nour predictor to conduct architecture search for GCN archi-\ntectures on Protein-Protein Interactions (PPI) dataset, which\ncollects motif gene sets and immunological signatures as\nfeatures and gene ontology sets as labels from the Molec-\nular Signatures Database (Liberzon et al. 2011). We need to\nspecify each protein role according to the interactions in a\npre-deﬁned graph, which can be regarded as a node classiﬁ-\ncation task. The average number of nodes per graph is 2373,\nwith an average degree of 28.8.\nFollowing SGAS (Li et al. 2020a), we build the supernet\nMethod #P(M) #F(M) Top-1(%) Top-5(%)\nPNAS 5.1 588 74.2 91.9\nNAO 11.4 584 74.3 91.8\nArch2vec-BO 5.2 580 74.5 -\nBONAS-D 4.8 532 74.6 92.0\nBANANAS 5.1 - 73.7 -\nCATE 5.0 - 73.9 -\nPINAT-T 5.2 583 75.1 92.5\nMnasNet-A3 5.2 403 76.7 93.3\nMobileNetV3 5.4 219 75.2 -\nFBNet-C 5.5 375 74.9 -\nProxylessNAS 7.1 465 75.1 92.3\nEfﬁcientNet-B0 5.3 390 76.3 -\nOFA w/ PS #75 - 230 76.9 -\nSPOS 5.4 472 74.8 -\nRLNAS 5.3 473 75.6 92.6\nNP 6.4 536 74.8 -\nCTNAS - 482 77.3 93.4\nTNASP-B 5.1 478 75.5 92.5\nPINAT-S 5.1 452 77.8 93.5\nTable 3: Comparison with SOTAs on ImageNet. Transfer-\nring results of predictor-based NAS methods are shown in\nthe ﬁrst row; Search results of recent NAS methods on the\nProxylessNAS search space are placed in the third row.\nMethod micro-F1(%) #P(M) Cost(G\u0001D)\nGraphSAGE 61.2 0.26 manual\nGeniePath 97.9 1.81 manual\nGAT 97.3\u00060.2 3.64 manual\nDenseMRGCN-14 99.43 53.42 manual\nResMRGCN-28 99.41 14.76 manual\nRandom Search 99.36\u00060.04 23.70 random\nSGAS (Cri.2 avg.) 99.40\u00060.09 25.93 0.003\nSGAS (Cri.2 best) 99.46 29.73 0.003\nPINAT(avg.) 99.47\u00060.01 23.70 0.083\nPINAT(best.) 99.48 21.87 0.083\nTable 4: Comparison with other methods on PPI.\nwith 1 cell and 32 channels, and 10 candidate operations for\neach edge. We randomly sample one path to train the super-\nnet as SPOS (Guo et al. 2020) for 1000 epochs and use the\nsame search procedure as depicted above to search for re-\nmarkable architectures. The discovered cells will be stacked\n5 times with an initial channel size of 512 to build the ﬁ-\nnal architecture. We retrain this architecture for 2000 epochs\nwith the Adam optimizer and report the average micro-F1\nscore of 10 searched cells on the test dataset.\nResults Searched results are shown in Tab. 4. When com-\npared with manually designed networks, PINAT yields the\nhighest micro-F1 than GraphSAGE (Hamilton et al. 2017),\nGeniePath (Liu et al. 2019b), GAT (Veliˇckovi´c et al. 2018),\nDenseMRGCN-14 (Li et al. 2021), and ResMRGCN-28 (Li\net al. 2021). When compared with previous SOTA NAS\nmethods, PINAT outperforms SGAS (Cri.2 best) by 0.02%\n8962\nMethod OA(%) #P(M) Cost(G\u0001D)\n3DmFV-Net 91.6 45.77 manual\nSpecGCN 91.5 2.05 manual\nPointNet++ 90.7 1.48 manual\nPCNN 92.3 8.2 manual\nPointCNN 92.2 0.6 manual\nDGCNN 92.2 1.84 manual\nKPConv 92.9 14.3 manual\nRandom Search 92.65\u00060.33 8.77 0.19\nSGAS (Cri.2 avg.) 92.93\u00060.19 8.87 0.19\nSGASy(Cri.2 best) 92.71(93.07) 3.86 0.19\nPINAT(avg.) 92.87\u00060.12 3.94 0.17\nPINAT(best.) 93.07 3.95 0.17\nTable 5: Comparison with other state-of-the-arts on Model-\nNet40. y: As we can not reproduce the experiments with the\nbig network in SGAS (Li et al. 2020a), we compare with\ntheir best small network. By re-run their open source code,\nwe only got the overall accuracy 92.87% while it appeared\n93.07% in the paper. OA: overall accuracy.\nwith 7.86 M fewer parameters, with only 0.08 more GPU\ndays. We also report the average performance of 10 searched\ncells as SGAS (Li et al. 2020a). Our searched cells are gener-\nally lightweight with fewer parameters but still can achieve\nbetter results in terms of both mean and standard deviation.\nSearch for GCN Architectures on ModelNet\nWe also conduct experiments on the ModelNet (Wu et al.\n2015) to search for GCN architectures. The ModelNet\ndataset mainly contains synthetic object point clouds and\nhas two variants, ModelNet10 and ModelNet40. The for-\nmer has 10 classes and 40 classes are included in the lat-\nter, which contains 9,843 CAD-generated meshes for train-\ning and 2,468 meshes for testing. We follow the pre-deﬁned\nsearch space in SGAS (Li et al. 2020a) to conduct architec-\nture search on ModelNet10 and retrain the searched archi-\ntectures on ModelNet40. As we can not reproduce their ex-\nperiments for the big network, we retrain our searched archi-\ntecture with their small network conﬁguration, speciﬁcally\nstacking 3 cells and setting knearest neighbor to 9.\nResults As shown in Tab. 5, PINAT gets the highest over-\nall accuracy than manually designed networks, i.e. 3DmFV-\nNet (Ben-Shabat et al. 2018), SpecGCN (Wang et al. 2018),\nPointNet++ (Qi et al. 2017), PCNN (Atzmon et al. 2018),\nPointCNN (Li et al. 2018), DGCNN (Wang et al. 2019)\nand KPConv (Thomas et al. 2019). Even compared with\nthe big network reported by SGAS (Li et al. 2020a) (Cri2\navg.), PINAT still obtains comparable performance with less\nsearch cost and fewer than half of the parameters.\nDiscussion\nComparison with the original PINE moduleThe origi-\nnal PINE structure (Gui et al. 2021) is a four-layer network\nwith two linear transformation matrices and two activation\nfunctions. We use a variant design to enhance permutation\nS1 S2 S3 S4 S5\nData split on NAS-Bench-101\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Kendall's Tau\nPIM Variant\nPIM Original\nWithout PIM\n(a) Ablation for the PIM designs\nS1 S2 S3 S4 S5\nData split on NAS-Bench-101\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Kendall's Tau\nPITE+PISA\nOnly PISA\nOnly PITE\nWithout PIM (b) Ablation for PITE and PISA\nFigure 3: Ablation for the PIM designs, and PITE and PISA.\ninvariant features: Adding the residual connection in parallel\nwith the only ﬁrst layer of the original structure to be a Res-\nblock-like module, and repeatedly stacking it for 3 times. We\ncompare these two modules in our predictor on NAS-Bench-\n101 and the results are summarized in Fig.3 (a). We can ob-\nserve that the original PINE module can improve the predic-\ntor’s performance as it introduces the features with partial\npermutation invariance, which is consistent with our previ-\nous analysis. And more improvements emerge when using\nour designed PIM variant, showing that our choice is effec-\ntive for NAS predictors and superior to the original design.\nThe effect of PITE and PISATo analyze the effect of our\nproposed PITE and PISA, we conduct an ablation study for\nthese two modules. As shown in Fig.3 (b), when both PITE\nand PISA modules are disabled, the predictor degenerates to\na totally Transformer-based structure. As long as any one of\nthe two modules is enabled, the ranking performance of the\nNAS predictor can be improved. Moreover, when combin-\ning both modules, we get the most powerful NAS predictor\nPINAT, surpassing others in all data splits.\nBetter performance of PINATMany recent Transformer\nrelated works (Chen et al. 2021a; Raghu et al. 2021) have\ndemonstrated that CNN extracts local features while the\nself-attention module focuses more on the global features.\nCombining both modules together results in better perfor-\nmance in various tasks. When it comes to our situation, local\nfeatures of node neighbors with partial permutation invari-\nance from PIM modules are exactly complementary to the\nglobal features of self-attention modules. Combining these\nfeatures can better represent the topology information from\nboth local and global views, which explains the great perfor-\nmance of our method. We explore the performance of differ-\nent combinations of SA and PISA modules in our Supp.\nConclusion\nIn this paper, we present a Transformer-like NAS predic-\ntor PINAT with PITE and PISA modules, which aggre-\ngates the features with global and partial permutation invari-\nance to produce more meaningful features to represent dis-\ncrete architectures. Extensive experiments on six benchmark\ndatasets show that our method constantly surpasses previous\nstate of the arts, clearly demonstrating our effectiveness. In\nthe future, we will explore the balance between the global\nand local aggregation information of node features to fur-\nther improve our predictor.\n8963\nAcknowledgments\nThis work was supported in part by the National Key R&D\nProgram of China under grant No. 2018AAA0102701 and\nin part by the National Natural Science Foundation of China\nunder Grant No. 62176250.\nReferences\nAtzmon, M.; et al. 2018. Point convolutional neural net-\nworks by extension operators. ACM Trans.\nBaker, B.; Gupta, O.; Naik, N.; and Raskar, R. 2017a. De-\nsigning neural network architectures using reinforcement\nlearning. In ICLR.\nBaker, B.; Gupta, O.; Raskar, R.; and Naik, N. 2017b. Ac-\ncelerating neural architecture search using performance pre-\ndiction. In ICLR.\nBen-Shabat, Y .; et al. 2018. 3dmfv: Three-dimensional point\ncloud classiﬁcation in real-time using convolutional neural\nnetworks. IEEE Robotics and Automation Letters, 3(4):\n3145–3152.\nBender, G.; Kindermans, P.-J.; Zoph, B.; Vasudevan, V .; and\nLe, Q. 2018. Understanding and Simplifying One-Shot Ar-\nchitecture Search. In ICML.\nCai, H.; Zhu, L.; and Han, S. 2019. Proxylessnas: Direct\nneural architecture search on target task and hardware. In\nICLR.\nChen, B.; Li, P.; Li, C.; Li, B.; Bai, L.; Lin, C.; Sun, M.; Yan,\nJ.; and Ouyang, W. 2021a. Glit: Neural architecture search\nfor global and local image transformer. In ICCV.\nChen, X.; Xie, L.; Wu, J.; and Tian, Q. 2019. Progressive\nDifferentiable Architecture Search: Bridging the Depth Gap\nbetween Search and Evaluation. In ICCV.\nChen, Y .; Guo, Y .; Chen, Q.; Li, M.; Wang, Y .; Zeng, W.;\nand Tan, M. 2021b. Contrastive Neural Architecture Search\nwith Neural Architecture Comparators. In CVPR.\nCheng, H.-P.; Zhang, T.; Li, S.; Yan, F.; Li, M.; Chandra,\nV .; Li, H.; and Chen, Y . 2021. Nasgem: Neural architecture\nsearch via graph embedding method. In AAAI.\nChu, X.; Wang, X.; Zhang, B.; Lu, S.; Wei, X.; and Yan, J.\n2021a. Darts-: robustly stepping out of performance collapse\nwithout indicators. In ICLR.\nChu, X.; Zhang, B.; Xu, R.; and Li, J. 2021b. Fairnas: Re-\nthinking evaluation fairness of weight sharing neural archi-\ntecture search. In ICCV.\nChu, X.; Zhou, T.; Zhang, B.; and Li, J. 2020. Fair DARTS:\nEliminating Unfair Advantages in Differentiable Architec-\nture Search. In ECCV.\nCubuk, E. D.; Zoph, B.; Mane, D.; Vasudevan, V .; and Le,\nQ. V . 2019. Autoaugment: Learning augmentation policies\nfrom data. In CVPR.\nDeb, K.; Pratap, A.; Agarwal, S.; and Meyarivan, T. 2002.\nA fast and elitist multiobjective genetic algorithm: NSGA-\nII. IEEE transactions on evolutionary computation, 6(2):\n182–197.\nDeng, B.; Yan, J.; and Lin, D. 2017. Peephole: Predicting\nnetwork performance before training. arXiv:1712.03351.\nDeVries, T.; and Taylor, G. W. 2017. Improved regu-\nlarization of convolutional neural networks with cutout.\narXiv:1708.04552.\nDong, X.; and Yang, Y . 2019. One-shot neural architecture\nsearch via self-evaluated template network. In ICCV.\nDong, X.; and Yang, Y . 2020. NAS-Bench-201: Extending\nthe Scope of Reproducible Neural Architecture Search. In\nICLR.\nGui, S.; Zhang, X.; Zhong, P.; Qiu, S.; Wu, M.; Ye, J.; Wang,\nZ.; and Liu, J. 2021. Pine: Universal deep embedding for\ngraph nodes via partial permutation invariant set functions.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence.\nGuo, Z.; Zhang, X.; Mu, H.; Heng, W.; Liu, Z.; Wei, Y .;\nand Sun, J. 2020. Single Path One-Shot Neural Architecture\nSearch with Uniform Sampling. In ECCV.\nHamilton, W. L.; et al. 2017. Inductive representation learn-\ning on large graphs. In NIPS.\nJing, K.; Xu, J.; and Li, P. 2022. Graph Masked Autoen-\ncoder Enhanced Predictor for Neural Architecture Search.\nIn IJCAI.\nKrizhevsky, A.; et al. 2017. Imagenet classiﬁcation with\ndeep convolutional neural networks. Communications of the\nACM, 60(6): 84–90.\nLeman, A.; and Weisfeiler, B. 1968. A reduction of a graph\nto a canonical form and an algebra arising during this reduc-\ntion. Nauchno-Technicheskaya Informatsiya, 2(9): 12–16.\nLi, G.; M ¨uller, M.; Qian, G.; Perez, I. C. D.; Abualshour,\nA.; Thabet, A. K.; and Ghanem, B. 2021. Deepgcns: Mak-\ning gcns go as deep as cnns. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence.\nLi, G.; Qian, G.; Delgadillo, I. C.; Muller, M.; Thabet, A.;\nand Ghanem, B. 2020a. Sgas: Sequential Greedy Architec-\nture Search. In CVPR.\nLi, W.; Gong, S.; and Zhu, X. 2021. Neural graph embed-\nding for neural architecture search. In AAAI.\nLi, Y .; Bu, R.; Sun, M.; Wu, W.; Di, X.; and Chen, B. 2018.\nPointcnn: Convolution on x-transformed points. InNeurIPS.\nLi, Z.; Xi, T.; Deng, J.; Zhang, G.; Wen, S.; and He, R.\n2020b. Gp-nas: Gaussian process based neural architecture\nsearch. In CVPR.\nLiberzon, A.; Subramanian, A.; Pinchback, R.; Thor-\nvaldsd´ottir, H.; Tamayo, P.; and Mesirov, J. P. 2011. Molec-\nular signatures database (MSigDB) 3.0. Bioinformatics.\nLiu, C.; Zoph, B.; Neumann, M.; Shlens, J.; Hua, W.; Li, L.-\nJ.; Fei-Fei, L.; Yuille, A.; Huang, J.; and Murphy, K. 2018.\nProgressive Neural Architecture Search. In ECCV.\nLiu, H.; et al. 2019a. DARTS: Differentiable Architecture\nSearch. In ICLR.\nLiu, Z.; Chen, C.; Li, L.; Zhou, J.; Li, X.; Song, L.; and Qi,\nY . 2019b. Geniepath: Graph neural networks with adaptive\nreceptive paths. In AAAI.\nLu, S.; Li, J.; Tan, J.; Yang, S.; and Liu, J. 2021. TNASP:\nA Transformer-based NAS Predictor with a Self-evolution\nFramework. In NeurIPS.\n8964\nLuo, R.; Tan, X.; Wang, R.; Qin, T.; Chen, E.; and\nLiu, T.-Y . 2020. Neural architecture search with gbdt.\narXiv:2007.04785.\nLuo, R.; Tian, F.; Qin, T.; Chen, E.; and Liu, T.-Y . 2018.\nNeural architecture optimization. In NeurIPS.\nNing, X.; Li, W.; Zhou, Z.; Zhao, T.; Liang, S.; Zheng, Y .;\nYang, H.; and Wang, Y . 2020a. A surgery of the neural ar-\nchitecture evaluators. arXiv:2008.03064.\nNing, X.; Zheng, Y .; Zhao, T.; Wang, Y .; and Yang, H.\n2020b. A generic graph-based neural architecture encoding\nscheme for predictor-based nas. In ECCV.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017. Pointnet++:\nDeep hierarchical feature learning on point sets in a metric\nspace. In NIPS.\nRaghu, M.; Unterthiner, T.; Kornblith, S.; Zhang, C.; and\nDosovitskiy, A. 2021. Do Vision Transformers See Like\nConvolutional Neural Networks? In NeurIPS.\nRu, B.; Wan, X.; Dong, X.; and Osborne, M. 2021. Inter-\npretable Neural Architecture Search via Bayesian Optimisa-\ntion with Weisfeiler-Lehman Kernels. In ICLR.\nSen, P. K. 1968. Estimates of the regression coefﬁcient based\non Kendall’s tau.Journal of the American statistical associ-\nation.\nShervashidze, N.; Schweitzer, P.; Van Leeuwen, E. J.;\nMehlhorn, K.; and Borgwardt, K. M. 2011. Weisfeiler-\nLehman graph kernels. Journal of Machine Learning Re-\nsearch.\nShi, H.; Pi, R.; Xu, H.; Li, Z.; Kwok, J. T.; and Zhang, T.\n2020. Bridging the gap between sample-based and one-shot\nneural architecture search with bonas. In NeurIPS.\nTan, M.; and Le, Q. 2019. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. In ICML.\nThomas, H.; Qi, C. R.; Deschaud, J.-E.; Marcotegui, B.;\nGoulette, F.; and Guibas, L. J. 2019. Kpconv: Flexible and\ndeformable convolution for point clouds. In ICCV.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio,\nP.; and Bengio, Y . 2018. Graph attention networks. InICLR.\nWang, C.; et al. 2018. Local spectral graph convolution for\npoint set feature learning. In ECCV.\nWang, Y .; Sun, Y .; Liu, Z.; Sarma, S. E.; Bronstein, M. M.;\nand Solomon, J. M. 2019. Dynamic graph cnn for learn-\ning on point clouds. Acm Transactions On Graphics (tog),\n38(5): 1–12.\nWen, W.; Liu, H.; Chen, Y .; Li, H. H.; Bender, G.; and Kin-\ndermans, P. 2020. Neural Predictor for Neural Architecture\nSearch. In ECCV.\nWhite, C.; Neiswanger, W.; Nolen, S.; and Savani, Y . 2020.\nA study on encodings for neural architecture search. In\nNeurIPS.\nWhite, C.; Zela, A.; Ru, R.; Liu, Y .; and Hutter, F. 2021a.\nHow powerful are performance predictors in neural archi-\ntecture search? In NeurIPS.\nWhite, C.; et al. 2021b. Bananas: Bayesian optimization\nwith neural architectures for neural architecture search. In\nAAAI.\nWu, J.; Dai, X.; Chen, D.; Chen, Y .; Liu, M.; Yu, Y .; Wang,\nZ.; Liu, Z.; Chen, M.; and Yuan, L. 2021. Weak NAS Pre-\ndictors Are All You Need. arXiv:2102.10490.\nWu, Z.; Song, S.; Khosla, A.; Yu, F.; Zhang, L.; Tang, X.;\nand Xiao, J. 2015. 3d shapenets: A deep representation for\nvolumetric shapes. In CVPR.\nXie, S.; Zheng, H.; Liu, C.; and Lin, L. 2019. SNAS:\nStochastic Neural Architecture Search. In ICLR.\nXu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2018. How\npowerful are graph neural networks? In ICLR.\nXu, Y .; Wang, Y .; Han, K.; Jui, S.; Xu, C.; Tian, Q.; and Xu,\nC. 2021. Renas: Relativistic evaluation of neural architec-\nture search. In CVPR.\nYan, S.; Song, K.; Liu, F.; and Zhang, M. 2021. CATE:\nComputation-aware Neural Architecture Encoding with\nTransformers. In ICML.\nYan, S.; Zheng, Y .; Ao, W.; Zeng, X.; and Zhang, M. 2020.\nDoes unsupervised architecture representation learning help\nneural architecture search? In NeurIPS.\nYing, C.; Klein, A.; Christiansen, E.; Real, E.; Murphy, K.;\nand Hutter, F. 2019. Nas-bench-101: Towards reproducible\nneural architecture search. In ICML.\nZhang, M.; Jiang, S.; Cui, Z.; Garnett, R.; and Chen, Y .\n2019. D-vae: A variational autoencoder for directed acyclic\ngraphs. In NeurIPS.\nZhong, Z.; Zheng, L.; Kang, G.; Li, S.; and Yang, Y . 2020.\nRandom erasing data augmentation. In AAAI.\nZoph, B.; and Le, Q. V . 2017. Neural Architecture Search\nwith Reinforcement Learning. In ICLR.\n8965",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7069390416145325
    },
    {
      "name": "Graph embedding",
      "score": 0.5527176856994629
    },
    {
      "name": "Machine learning",
      "score": 0.45286130905151367
    },
    {
      "name": "Embedding",
      "score": 0.44837674498558044
    },
    {
      "name": "Bottleneck",
      "score": 0.4323849678039551
    },
    {
      "name": "Transformer",
      "score": 0.4183100461959839
    },
    {
      "name": "Data mining",
      "score": 0.407913476228714
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38502076268196106
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3580867052078247
    },
    {
      "name": "Engineering",
      "score": 0.11425361037254333
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210090176",
      "name": "Institute of Computing Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4401726859",
      "name": "Kuaishou (China)",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210142583",
      "name": "Snap (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210092558",
      "name": "BC Platforms (Finland)",
      "country": "FI"
    },
    {
      "id": "https://openalex.org/I4210114444",
      "name": "Meta (United States)",
      "country": "US"
    }
  ],
  "cited_by": 13
}