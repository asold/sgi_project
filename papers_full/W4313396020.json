{
  "title": "Image and Model Transformation with Secret Key for Vision Transformer",
  "url": "https://openalex.org/W4313396020",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1411898383",
      "name": "Hitoshi Kiya",
      "affiliations": [
        "Tokyo Metropolitan University"
      ]
    },
    {
      "id": "https://openalex.org/A2285232221",
      "name": "Ryota Iijima",
      "affiliations": [
        "Tokyo Metropolitan University"
      ]
    },
    {
      "id": "https://openalex.org/A3182568713",
      "name": "AprilPyone MaungMaung",
      "affiliations": [
        "Tokyo Metropolitan University"
      ]
    },
    {
      "id": "https://openalex.org/A2528483160",
      "name": "Yuma Kinoshita",
      "affiliations": [
        "Tokyo Metropolitan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2810392541",
    "https://openalex.org/W2085762799",
    "https://openalex.org/W1648971828",
    "https://openalex.org/W2050770647",
    "https://openalex.org/W2051267297",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W1673923490",
    "https://openalex.org/W3006602764",
    "https://openalex.org/W4285275243",
    "https://openalex.org/W3134815184",
    "https://openalex.org/W3178705018",
    "https://openalex.org/W3091761040",
    "https://openalex.org/W4200625937",
    "https://openalex.org/W4225775889",
    "https://openalex.org/W4312358089",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2898784076",
    "https://openalex.org/W2689968489",
    "https://openalex.org/W2009460929",
    "https://openalex.org/W122513721",
    "https://openalex.org/W2609045769",
    "https://openalex.org/W2101677434",
    "https://openalex.org/W2023966246",
    "https://openalex.org/W2101647944",
    "https://openalex.org/W2525641875",
    "https://openalex.org/W1501211345",
    "https://openalex.org/W3103565790",
    "https://openalex.org/W3097384281",
    "https://openalex.org/W2970550114",
    "https://openalex.org/W4225880932",
    "https://openalex.org/W3042755074",
    "https://openalex.org/W3106853766",
    "https://openalex.org/W2998555882",
    "https://openalex.org/W3015738389",
    "https://openalex.org/W2237358707",
    "https://openalex.org/W2992607870",
    "https://openalex.org/W2914330473",
    "https://openalex.org/W2946635188",
    "https://openalex.org/W2579318729",
    "https://openalex.org/W2952608669",
    "https://openalex.org/W2796299376",
    "https://openalex.org/W3168768313",
    "https://openalex.org/W2786379237",
    "https://openalex.org/W2806082141",
    "https://openalex.org/W2942140795",
    "https://openalex.org/W2768064608",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W3183804933",
    "https://openalex.org/W4226297238",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2964335264",
    "https://openalex.org/W2971122390",
    "https://openalex.org/W3118030655",
    "https://openalex.org/W3159734327",
    "https://openalex.org/W3107370524",
    "https://openalex.org/W4226363321",
    "https://openalex.org/W3102733833",
    "https://openalex.org/W3110286774"
  ],
  "abstract": "In this paper, we propose a combined use of transformed images and vision transformer (ViT) models transformed with a secret key. We show for the first time that models trained with plain images can be directly transformed to models trained with encrypted images on the basis of the ViT architecture, and the performance of the transformed models is the same as models trained with plain images when using test images encrypted with the key. In addition, the proposed scheme does not require any specially prepared data for training models or network modification, so it also allows us to easily update the secret key. In an experiment, the effectiveness of the proposed scheme is evaluated in terms of performance degradation and model protection performance in an image classification task on the CIFAR-10 dataset.",
  "full_text": "2\nIEICE TRANS. INF. & SYST., VOL.E106–D, NO.1 JANUARY 2023\nINVITED PAPER Special Section on Enriched Multimedia–Advanced Safety, Security and Convenience–\nImage and Model Transformation with Secret Key for Vision\nTransformer\nHitoshi KIYAya), Fellow, Ryota IIJIMAyb), Aprilpyone MAUNGMAUNGyc), Nonmembers,\nand Yuma KINOSHITAyd), Member\nSUMMARY In this paper, we propose a combined use of transformed\nimages and vision transformer (ViT) models transformed with a secret key.\nWe show for the ﬁrst time that models trained with plain images can be\ndirectly transformed to models trained with encrypted images on the basis\nof the ViT architecture, and the performance of the transformed models\nis the same as models trained with plain images when using test images\nencrypted with the key. In addition, the proposed scheme does not require\nany specially prepared data for training models or network modiﬁcation,\nso it also allows us to easily update the secret key. In an experiment, the\neﬀectiveness of the proposed scheme is evaluated in terms of performance\ndegradation and model protection performance in an image classiﬁcation\ntask on the CIFAR-10 dataset.\nkey words:perceptual image encryption, vision transformer, DNN, privacy\npreserving\n1. Introduction\nMachine learning (ML) algorithms have been deployed in\nmany applications including security-critical ones such as\nbiometric authentication, automated driving, and medical\nimage analysis [1], [2]. However, training successful mod-\nels requires three ingredients: a huge amount of data, GPU\naccelerated computing resources, and e ﬃcient algorithms,\nand it is not a trivial task. In fact, collecting images and la-\nbeling them is also costly and will also consume a massive\namount of resources. Therefore, trained ML models have\ngreat business value. Considering the expenses necessary\nfor the expertise, money, and time taken to train a model, a\nmodel should be regarded as a kind of intellectual property\n(IP). In addition, generally, data contains sensitive informa-\ntion and it is di ﬃcult to train a model while preserving pri-\nvacy. In particular, data with sensitive information cannot\nbe transferred to untrusted third-party cloud environments\n(cloud GPUs and TPUs) even though they provide a pow-\nerful computing environment [3]–[9]. Accordingly, it has\nbeen challenging to train /test a ML model with encrypted\nimages as one way for solving these issues [10].\nLearnable image encryption, which has given new so-\nlutions to the above issues, is encryption that allows us not\nManuscript received March 16, 2022.\nManuscript revised July 19, 2022.\nManuscript publicized November 2, 2022.\nyThe authors are with Tokyo Metropolitan University, Hino-\nshi, 191–0065 Japan.\na) E-mail: kiya@tmu.ac.jp\nb) E-mail: iijima-ryota@ed.tmu.ac.jp\nc) E-mail: fugokidi@gmail.com\nd) E-mail: ykinoshita@tmu.ac.jp\nDOI: 10.1587/transinf.2022MUI0001\nonly to generate visually protected images to protect per-\nsonally identiﬁable information included in an image, such\nas an individual or the time and location of the taken pho-\ntograph, but to also apply encrypted images to a ML algo-\nrithm in the encrypted domain [10]. In addition, image en-\ncryption with a secret key, referred to as image transforma-\ntion with a secret key, can embed unique features controlled\nwith the key into images. The use of the unique features\nwas demonstrated to be e ﬀective in applications such as ad-\nversarial defense and model protection [11]–[16]. However,\neven though many image transformation methods with a se-\ncret key have been studied so far for application to such ap-\nplications, no conventional methods can avoid the inﬂuence\nof image transformation. In other words, the use of trans-\nformed images degrades the performance of models com-\npared with models trained with plain images [11]–[16]. In\naddition, if we want to update the key, models have to be\nre-trained by using a new key.\nIn this paper, we show that the use of the vision trans-\nformer (ViT) [17] allows us to reduce the inﬂuence of block-\nwise encryption thanks to its architecture. After that, to\novercome the problems with conventional image transfor-\nmation, we propose a novel framework for ML algorithms\nwith encrypted images that uses ViT.\nIn the framework, a model trained with plain images\nis also transformed with a secret key using the unique fea-\ntures in addition to test images, and the combined use of\nthe transformed model and test images is proposed for us-\ning ML algorithms in the encrypted domain. The proposed\nscheme allows us not only to obtain the same performance\nas models trained with plain images but to also update the\nsecret key easily. In an experiment, the e ﬀectiveness of\nthe proposed scheme is evaluated in terms of performance\ndegradation and model protection performance in an image\nclassiﬁcation task on the CIFAR-10 dataset.\n2. Related Work\nConventional image transformation for machine learning\nand ViT are summarized here.\n2.1 Image Transformation for Machine Learning\nVarious image transformation methods with a secret key,\noften referred to as perceptual image encryption or image\ncryptography, have been studied so far for many applica-\nCopyright c⃝2023 The Institute of Electronics, Information and Communication Engineers\nKIYA et al.: IMAGE AND MODEL TRANSFORMATION WITH SECRET KEY FOR VISION TRANSFORMER\n3\nFig. 1 Applications of perceptual image encryption\ntions. Perceptional encryption can o ﬀer encrypted images\nthat are described as bitmap images, so the encrypted im-\nages can be directly applied to image processing algorithms.\nIn addition, encrypted images can be decrypted even when\nnoise is added to them, although the use of standard encryp-\ntion algorithms such as DES and AES cannot.\nFigure 1 shows typical applications of image transfor-\nmation with a key. Image transformation with a key allows\nus not only to protect visual information on plain images but\nto also embed unique features controlled with the key into\nimages. The use of visually protected images has enabled\nvarious kinds of applications. One of the origins of image\ntransformation with a key is in block-wise image encryp-\ntion schemes for encryption-then-compression (EtC) sys-\ntems [18]–[27]. Image encryption prior to image compres-\nsion is required in certain practical scenarios such as secure\nimage transmission through an untrusted channel provider.\nAn EtC system is used in such scenarios, although the tra-\nditional way of securely transmitting images is to use a\ncompression-then-encryption (CtE) system. Compressible\nencryption methods have been applied to privacy-preserving\ncompression, data hiding, and image retrieval [28]–[30] in\ncloud environments. In addition, visually protected images\nhave been demonstrated to be e ﬀective in fake image detec-\ntion [31] and various learning algorithms [10], [32]–[37].\nIn this paper, we focus on image transformation meth-\nods for machine learning including deep neural networks\n(DNNs), called learnable encryption. Learnable encryption\nenables us to directly apply encrypted data to a model as\ntraining and testing data. Encrypted images have no vi-\nsual information on plain images in general, so privacy-\npreserving learning can be carried out by using visually pro-\ntected images. In addition, the use of a secret key allows\nus to embed unique features controlled with the key into\nimages. Adversarial defense [11], [13], [14], access con-\ntrol [12], [38], [39], and DNN watermarking [14], [40]–[47]\nare carried out with encrypted data using the unique fea-\ntures.\n2.2 Vision Transformer\nThe transformer architecture has been widely used in natu-\nral language processing (NLP) tasks [48]. The vision trans-\nFig. 2 Architecture of ViT [17]\nformer (ViT) [17] has also provided excellent results com-\npared with state-of-the-art convolutional networks. Follow-\ning the success of ViT, a number of isotropic networks\n(with the same depth and resolution across di ﬀerent layers in\nthe network) have been proposed such as MLP-Mixer [49],\nResMLP [50], CycleMLP [51], gMLP [52], vision permuta-\ntor [53], and ConvMixer [54]. In this paper, we focus on ViT\nbecause it utilizes patch embedding and position embedding\n(see Fig. 2). Figure 2 illustrates the architecture of ViT. The\nmain procedure of ViT is given as follows:\n1. Split an image into ﬁxed-size patches, and linearly em-\nbed each of them.\n2. Add position embedding to patch embedding.\n3. Feed the resulting sequence of vectors to a standard\ntransformer encoder.\n4. Feed the output of the transformer to a multi-layer per-\nceptron (MLP), and get a result.\nViT utilizes patch embedding and position embedding. In\nthis paper, we point out that the embedding structure enables\nus to reduce the inﬂuence of block-wise encryption. In patch\nembedding, patches are mapped to vectors, and in position\nembedding, the position information is embedded. If ev-\nery patch (block) is transformed with the same key, pixel\nshuﬄing and bit ﬂipping can be expressed as the operation\nof patch embedding. In addition, block permutation can be\ngiven as the operation of position embedding. The relation\n4\nIEICE TRANS. INF. & SYST., VOL.E106–D, NO.1 JANUARY 2023\nbetween block-wise encryption and the embedding structure\nis demonstrated to avoid the inﬂuence of block-wise image\nencryption.\n3. Image and Model Transformation with Secret Key\nAn image transformation method with a secret key is pro-\nposed here. The method makes it possible to simultaneously\nuse both transformed images and models.\n3.1 Notation\nThe following notations are utilized throughout this paper.\n\u000f W, H, and C denote the width, height, and the number\nof channels of an image, respectively.\n\u000f The tensor x 2[0; 1]C\u0002W\u0002H represents an input color\nimage.\n\u000f The tensor x′ 2 [0; 1]C\u0002W\u0002H represents an encrypted\nimage.\n\u000f M is the block size of an image.\n\u000f Tensors xb and x′b 2 [0; 1]Wb\u0002Hb\u0002pb are a block im-\nage and an encrypted block image, respectively, where\nWb = W\nM is the number of blocks across width W,\nHb = H\nM is the number of blocks across height H, and\npb = M \u0002M \u0002C is the number of pixels in a block.\nWe assume that W and H are divisible by M, so Wb and\nHb are positive integers.\n\u000f A pixel value in a block image ( xb or x′b) is denoted\nby xb(w; h; c) or x′b(w; h; c), where w 2f0; :::; Wb \u00001g,\nh 2f0; :::; Hb \u00001g, and c 2f0; :::; pb \u00001gare indices\ncorresponding to the dimension of xb of x′b.\n\u000f xb(w; h; :) denotes a vector ( xb(w; h; 0); :::; xb(w; h; pb \u0000\n1)) of a tensor xb.\n\u000f xb(:; :; c) denotes a matrix of a tensor xb as given in\n0BBBBBBBBBB@\nxb(0; 0; c) : : : xb(Wb \u00001; 0; c)\n::: : : : :::\nxb(0; Hb \u00001; c) : : : xb(Wb \u00001; Hb \u00001; c)\n1CCCCCCCCCCA\n:\n(1)\n\u000f B is a block in an image, and its dimension is M\u0002M\u0002C.\n\u000f ˆB is a ﬂattened version of block B, and its dimension is\n1 \u00021 \u0002pb.\n\u000f P is the patch size of an image.\n3.2 Overview\nFigure 3 shows the scenario of the proposed scheme, where\nit is assumed that the classiﬁcation model builder is trusted,\nand the classiﬁcation service provider is untrusted. The clas-\nsiﬁcation model builder trains a model by using plain images\nand transforms the trained model with a secret key where the\ntransformation by using secret keys is performed only on the\npreprocessing part of the transformer encoder. The trans-\nformed model is given to the classiﬁcation service provider,\nand the key is sent to a client. The client prepares a trans-\nformed test image with the key and sends it to the provider.\nThe provider applies it to the transformed model to obtain a\nclassiﬁcation result, and the result is sent back to the client.\nNote that the provider has neither a key nor plain im-\nages. The proposed scheme enables us to achieve this sce-\nnario without any performance degradation compared with\nthe use of plain images.\n3.3 Image Transformation\nA block-wise image transformation with a secret key is pro-\nposed for application to test images. As shown in Fig. 4,\nthe procedure of the transformation consists of three steps:\nblock segmentation, block transformation, and block inte-\ngration. To transform an image x, we ﬁrst divide x into\nWb \u0002Hb blocks, as in {B11; B12; :::; BWb Hb\n}. In this paper, we\nassume that the block size of the segmentation is the same\nas the patch size of ViT. Next, each block is ﬂattened, and it\nis concatenated again to obtain a block image xb. Then, xb\nis transformed to x′b in accordance with block transforma-\ntion with key K. Finally, x′b is transformed so that it has the\nsame C \u0002H \u0002W dimensions as those of the original image,\nand encrypted image x′is obtained.\nIn addition, the block transformation is carried out by\nusing the four operations shown in Fig. 4. Details on each\noperation are given below.\nA Block Permutation\n1. Generate a random permutation vector vA =\n(v0; v1; :::; v k; :::; v k′; :::; v Wb\u0002Hb\u00001) that consists of ran-\ndomly permuted integers from 0 to Wb \u0002Hn \u00001 by\nusing a key K1, where k; k′ 2f0; :::; Hb \u0002Wb \u00001g, and\nvk , vk′ if k , k′.\n2. Blocks are permutated to replace xb with x′(1)\nb by using\nvector vA (see Algorithm 1).\nAlgorithm 1Block permutation\nInput: xb; K1\nOutput: x′(1)\nb\nGenerate a vector vA using key K1 .\nyb  (xb[0; 0; :]; xb[1; 0; :]; :::; xb[Wb \u00001; Hb \u00001; :])\ni  0\nwhile i < Hb \u0002Wb do\ny′\nb[i]  yb[vA[i]]\ni  i + 1\nend while\nw  0\nh  0\nwhile h < Hb do\nwhile w < Wb do\nx′(1)\nb [w; h; :]  y′\nb[w + h \u0002Wb]\nw  w + 1\nend while\nh  h + 1\nend while\nreturn x′(1)\nb\nKIYA et al.: IMAGE AND MODEL TRANSFORMATION WITH SECRET KEY FOR VISION TRANSFORMER\n5\nFig. 3 Scenario of proposed scheme\nFig. 4 Procedure of block-wise transformation\nB Pixel Shu ﬄing\n1. Generate a random permutation vector vB =\n(v0; v1; :::; v k; :::; v k′; :::; v pb\u00001) by using a key K2, where\nk; k′2f0; :::; pb \u00001g, and vk , vk′ if k , k′.\n2. Pixels in each block are shu ﬄed by vector vB as (see\nAlgorithm 2).\nx′(2)\nb (w; h; k) = x′(1)\nb (w; h; vk): (2)\nAlgorithm 2Pixel shuﬄing\nInput: x′(1)\nb ; K2\nOutput: x′(2)\nb\nGenerate a vector vB using key K2 .\ni  0\nwhile i < pb do\nx′(2)\nb [:; :; i]  x′(1)\nb [:; :; vB[i]]\ni  i + 1\nend while\nreturn x′(2)\nb\n6\nIEICE TRANS. INF. & SYST., VOL.E106–D, NO.1 JANUARY 2023\nC Bit Flipping\n1. Generate a random binary vector r = (r0; :::; rk; :::; rpb\u00001),\nrk 2f0; 1gby using a key K3. To keep the transforma-\ntion consistent, r is distributed with 50% of “0”s and\n50% of “1”s.\n2. Apply negative-positive transformation on the basis of\nr as\nx′(3)\nb (w; h; k) =\n8>><>>:\nx′(2)\nb (w; h; k) ( rk = 0)\n1 \u0000x′(2)\nb (w; h; k) ( rk = 1):\n(3)\nAlgorithm 3Bit ﬂipping\nInput: x′(2)\nb ; K3\nOutput: x′(3)\nb\nGenerate a vector r using key K3 .\ni  0\nwhile i < pb do\nif r[i] = 0 then\nx′(3)\nb [:; :; i]  x′(2)\nb [:; :; i]\nelse\nx′(3)\nb [:; :; i]  1 \u0000x′(2)\nb [:; :; i]\nend if\ni  i + 1\nend while\nreturn x′(3)\nb\nD Normalization\nVarious normalization methods are widely used to improve\nthe training stability, optimization e ﬃciency, and general-\nization ability of DNNs. In this paper, we use a normal-\nization method to achieve the combined use of transformed\nimages and models.\nFrom Eq. (3), if rk = 0, a pixel x′(3)\nb (w; h; k) is replaced\nwith x′b(w; h; k) as\nx′\nb(w; h; k) =\nx′(3)\nb (w; h; k) \u00001=2\n1=2\n=\nx′(2)\nb (w; h; k) \u00001=2\n1=2 :\n(4)\nIn contrast, if rk = 1, a pixel x′(3)\nb (w; h; k) is replaced as fol-\nlows.\nx′\nb(w; h; c) =\nx′(3)\nb (w; h; c) \u00001=2\n1=2\n= 2x′(3)\nb (w; h; c) \u00001\n= 2(1 \u0000x′(2)\nb (w; h; c)) \u00001\n= 1 \u00002x′(2)\nb (w; h; c)\n= \u0000\nx′(2)\nb (w; h; c) \u00001=2\n1=2\n(5)\nTherefore, bit ﬂipping with normalization can be re-\ngarded as an operation that reverses the positive or negative\nsign of a pixel value. This property allows us to use the\nmodel encryption that will be described later.\nThe above encryption steps are the same as those of a\nnumber of conventional methods [10], [11], but the perfor-\nmance of conventional models is degraded due to the inﬂu-\nence of encryption when the encryption steps are used in\nthe conventional schemes. In contrast, the proposed method\nis demonstrated to avoid the inﬂuence of encryption in this\npaper, which is one of the reasons to apply the algorithm\nwritten in each step. In addition, the encryption steps can\nbe expressed as a linear transform as described in the paper.\nOther encryption steps can also be used under the frame-\nwork of the proposed scheme, if they are expressed as a lin-\near transform such as a random matrix.\n3.4 Model Transformation\nIn model transformation, some parameters in models trained\nwith plain images are transformed by using a secret key. In\nthis paper, a model transformation method is proposed that\ncan achieve the combined use of models and images trans-\nformed with the same key.\nViT utilizes patch embedding and position embedding\n(see Fig. 2), so it has the following two properties.\n1. Patch-order invariance of transformer encoder: the out-\nput of the transformer encoder corresponding to an in-\nput patch is independent of the order of input patches.\n2. Ability to adapt to pixel order by patch embedding:\npatch embedding can be adapted to pixel shu ﬄing and\nbit ﬂipping because they can be expressed as an invert-\nible linear transformation as described below.\nIn the proposed scheme, it is assumed that the patch\nsize P used for patching is the same as the block size used\nfor image encryption, and the number of patches is equal to\nthat of blocks in an image. The transformation of parameters\nin trained models is described below.\nA Position Embedding and Patch Embedding\nIn ViT [17], all segmented patches are ﬂattened. A pixel\nvalue in the ﬂattened patches is given by a pixel value in the\noriginal image as\nxi\np[k] = x[h; w; c];\nh =\n⌊ i \u00001\nW=P\n⌋\nP +\n⌊ k \u00001 mod P2\nP\n⌋\n;\nw = ((i \u00001) mod ( W=P)) P + ((k \u00001) mod P);\nc =\n⌊ k \u00001\nP2\n⌋\n;\nxi\np 2RP2C; i 2f1; 2; :::; Ng; k 2f1; 2; :::; P2Cg\n(6)\nwhere xi\np[k] is a pixel value in the i-th patch, and N =\nKIYA et al.: IMAGE AND MODEL TRANSFORMATION WITH SECRET KEY FOR VISION TRANSFORMER\n7\nHW=P2 is the number of patches. To simplify the discus-\nsion, we assume that H and W are divisible by P.\nThen, in patch embedding, the ﬂattened patches are\nmapped to vectors with dimensions of D by using a ma-\ntrix E 2R(P2C)\u0002D, and in position embedding, the position\ninformation Epos 2R(N+1)\u0002D is embedded into each patch as\nz0 = [xclass; x1\npE; x2\npE; \u0001\u0001\u0001 ; xN\np E] + Epos;\nEpos =\n((\ne0\npos\n)T (\ne1\npos\n)T\n\u0001\u0001\u0001\n(\neN\npos\n)T\n)T\nei\npos 2RD; i = 0; 1; :::; N\n(7)\nwhere xclass is the classiﬁcation token which is the input to\nMLP (see Fig. 2), e0\npos is the information of the classiﬁcation\ntoken.\nThe proposed model transformation is carried out in\naccordance with the above relation.\nB Position Embedding Transformed with Key\nPosition embedding is an operation that embeds position\ninformation into classiﬁcation token and each patch as in\nEq. (7). Let us deﬁne a matrix ˆEpos consisting of position\ninformation of each patch as\nˆEpos =\n((\ne1\npos\n)T (\ne2\npos\n)T\n\u0001\u0001\u0001\n(\neN\npos\n)T\n)T\n;\nei\npos 2RD; i = 1; 2; :::; N:\n(8)\nNote that ˆEpos does not contain information about the\nclassiﬁcation token.\nThe permutation of rows in ˆEpos corresponds to the\nblock permutation in image transformation. Therefore, ˆEpos\ncan be permuted using key K1 used for block permutation,\nand the model can be encrypted. The transformed model\noﬀers a high accuracy for only test images transformed by\nblock permutation with K1. By deﬁning a permutation ma-\ntrix E1 with key K1, the transformation from ˆEpos to ˆE′\npos can\nbe given as follows.\nˆE′\npos = E1 ˆEpos\n=\n((\ne′1\npos\n)T\n\u0001\u0001\u0001\n(\ne′ N\npos\n)T\n)T\n;\ne′i\npos 2fe1\npos; \u0001\u0001\u0001 ; eN\nposg\n(9)\nTherefore, from Eq. (7), the transformed matrix E′\npos is given\nby\nE′\npos =\n((\ne0\npos\n)T (\ne′1\npos\n)T\n\u0001\u0001\u0001\n(\ne′ N\npos\n)T\n)T\n: (10)\nC Patch Embedding Transformed with Key\nIn patch embedding, ﬂattened patches are mapped to vec-\ntors with a dimension of D as in Eq. (7). When the patch\nsize of ViT is equal to the block size of image transforma-\ntion, P2C = pb is satisﬁed. Therefore, the permutation of\nrows in E corresponds to pixel shu ﬄing, so the model can\nbe encrypted with key K2 used for pixel shu ﬄing. The accu-\nracy of the transformed model is high only when test images\nare encrypted by using pixel shu ﬄing with key K2. As well\nas the transformation of Epos, a permutation matrix E2 is de-\nﬁned with key K2, and the transformation from matrix E to\nE′is shown as follows.\nE′= E2E (11)\nIn addition, as shown in Eqs. (4) and (5), bit ﬂipping\nwith normalization can be regarded as an operation that ran-\ndomly inverses the positive /negative sign of a pixel value.\nTherefore, we can encrypt a model by inverting the sign of\nthe rows in matrix E with key K3 used for bit ﬂipping. The\ntransformed model o ﬀers a high accuracy only for test im-\nages transformed by bit ﬂipping with key K3. Using key K3\nto generate the same vector r used in bit ﬂipping, the trans-\nformation from E to E′can be expressed as follows.\nE′(k; :) =\n{E(k; :) ( rk = 0)\n\u0000E(k; :) ( rk = 1) (12)\nwhere E(k; :) and E′(k; :) are k-th rows of matrices E and E′.\nAccordingly, the procedure of the proposed method can\nbe summarized as follows.\nStep 1: Prepare a key set K = fK1; K2; K3g.\nStep 2: Generate E1 with K1, E2 with K2, and r with K3\nStep 3: Transform a model V\u0012 trained with plain images by\nusing E1, E2, and r on the basis of Eqs. (9), (10), and\n(11) as\nV′\n\u0012 = t(V\u0012; fE1; E2; rg); (13)\nwhere t(V\u0012; fE1; E2; rg) is the proposed model transfor-\nmation algorithm, and V′\n\u0012 is a transformed model.\nStep 4: Transform test images with K = fK1; K2; K3g.\n3.5 Properties of Proposed Scheme\nThe proposed method has the following properties.\n\u000f The model performs well only if test images are trans-\nformed with the same key as that used for transforming\nthe model.\n\u000f The proposed scheme does not cause performance\ndegradation, due to the relation\nV′\n\u0012(x′) = V\u0012(x): (14)\n\u000f Model training and encryption are independent. There-\nfore, it is possible to easily update a key.\n4. Experiment and Discussion\nIn an experiment, the e ﬀectiveness of the proposed scheme\n8\nIEICE TRANS. INF. & SYST., VOL.E106–D, NO.1 JANUARY 2023\nwas shown in terms of image classiﬁcation accuracy and\nmodel protection performance.\n4.1 Experiment Setup\nTo conﬁrm the e ﬀectiveness of the proposed method, we\nevaluated the accuracy of an image classiﬁcation task on the\nCIFAR-10 dataset (with 10 classes). The CIFAR-10 consists\nof 60,000 color images (dimension of 3 \u000232 \u000232), where\n50,000 images are for training, 10,000 for testing, and each\nclass contains 6,000 images. Images in the dataset were re-\nsized to 3 \u0002224 \u0002224 to input them to ViT, before applying\nthe proposed encryption algorithm, where the block size was\n16 \u000216.\nWe used the PyTorch [55] implementation of ViT and\nﬁne-tuned a ViT model with a patch size P = 16, which\nwas pre-trained on ImageNet-21k. The ViT model was ﬁne-\ntuned for 5000 epochs. The parameters of the stochastic\ngradient descent (SGD) optimizer were a momentum of 0.9\nand a learning rate value of 0.03.\nIn addition, we used three conventional visual informa-\ntion protection methods (Tanaka’s method [56], the pixel-\nbased encryption method [57], and the GAN-based trans-\nformation method [58]) to compare them with the proposed\nmethod. ResNet-20 was used to validate the e ﬀectiveness\nof the conventional method with reference to [59]. The\nCIFAR-10 was also used for training networks, and the net-\nworks were trained for 200 epochs by using stochastic gra-\ndient descent (SGD) with a weight decay of 0 :0005 and a\nmomentum of 0 :9. The learning rate was initially set to 0 :1,\nand it was multiplied by 0 :2 at 60, 120, and 160 epochs. The\nbatch size was 128.\n4.2 Image Classiﬁcation\nFirst, we evaluated the proposed and conventional methods\nin terms of the accuracy of image classiﬁcation under the\nuse of ViT and ResNet-20. As shown in Table 1, the perfor-\nmance of all conventional methods was degraded compared\nwith the baselines. In contrast, the proposed method did not\ndegrade the performance at all.\n4.3 Model Protection\nNext, we validated whether the proposed method has the\nability to protect models. Table 2 shows the accuracy of\nimage classiﬁcation when encrypted or plain images were\nTable 1 Comparison with conventional methods in terms of classiﬁca-\ntion accuracy\nModel Method Accuracy\nViT Baseline 99.03\nProposed 99.03\nResNet-20 [59] Baseline 91.55\nTanaka[56] 87.02\nPixel-based [57] 86.66\nGAN-based [58] 82.55\ninput to the encrypted model.\nThe encrypted model performed well for test images\nwith the correct key, but its accuracy was not high when\nusing plain test images. The CIFAR-10 dataset consists of\nten classes, so 9 :06 is almost the same accuracy as that when\ntest images are randomly classiﬁed.\nNext, we conﬁrmed the performance of images en-\ncrypted with a di ﬀerent key from that used in the model en-\ncryption. We prepared 100 random keys, and test images\nencrypted with the keys were input to the encrypted model.\nFrom the box plot in Fig. 5, the accuracy of the models was\nnot high under the use of the wrong keys. Accordingly, the\nproposed scheme was conﬁrmed to be robust against a ran-\ndom key attack.\nThe use of a large key space enhances robustness\nagainst various attacks in general. In this experiment, the\nkey space of block permutation, pixel shu ﬄing, and bit ﬂip-\nping (OP, OS, and OF) is given by\nOP = (WbHb)!\n= (WH=M2)! = 196!; (15)\nOS = pb!\n= (M2C)! = 768!; (16)\nand\nOF = pb!\n(pb=2)! \u0001(pb=2)!\n= (M2C)!\n(M2C=2)! \u0001(M2C=2)! = 768!\n384! \u0001384!:\n(17)\nTherefore, the key space of the proposed method is repre-\nsented as follows.\nTable 2 Robustness against use of plain images\nTest Image\nModel Plain Proposed\nBaseline 99.03 -\nProposed 9.06 99.03\nFig. 5 Evaluating robustness against random key attack. Boxes span\nfrom ﬁrst to third quartile, referred to as Q1 and Q3, and whiskers show\nmaximum and minimum values in range of [ Q1 \u00001:5(Q3 \u0000Q1); Q3 +\n1:5(Q3 \u0000Q1)]. Band inside box indicates median. Outliers are indicated as\ndots.\nKIYA et al.: IMAGE AND MODEL TRANSFORMATION WITH SECRET KEY FOR VISION TRANSFORMER\n9\nO = OP \u0002OS \u0002OF ≃28237 (18)\nTypical cipher systems are recommended to have 2 128\nas a key space as in [60]. Accordingly, the key space O is\nsuﬃciently large, so it is di ﬃcult to ﬁnd the correct key by\nrandom key estimation.\n5. Conclusion\nIn this paper, we proposed the combined use of an image\ntransformation method with a secret key and ViT models\ntransformed with the key. The proposed scheme enables\nus not only to use visually protected images but to also\nmaintain the same classiﬁcation accuracy as that of models\ntrained with plain images. In addition, in an experiment, the\nproposed scheme was demonstrated to outperform state-of-\nthe-art methods with perceptually encrypted images in terms\nof classiﬁcation accuracy, and it was also veriﬁed to be ef-\nfective in model protection.\nAcknowledgments\nThis study was partially supported by JSPS KAKENHI\n(Grant Number JP21H01327) and JST CREST (Grant Num-\nber JPMJCR20D3).\nReferences\n[1] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” nature,\nvol.521, no.7553, p.436, 2015.\n[2] X. Liu, Z. Deng, and Y . Yang, “Recent progress in semantic image\nsegmentation,” Artif. Intell. Rev., vol.52, no.2, pp.1089–1106, 2019.\n[3] C.-T. Huang, L. Huang, Z. Qin, H. Yuan, L. Zhou, V . Varadharajan,\nand C.-C.J. Kuo, “Survey on securing data storage in the cloud,”\nAPSIPA Trans. Signal and Information Processing, vol.3, p.e7,\n2014.\n[4] M.-R. Ra, R. Govindan, and A. Ortega, “P3: Toward privacy-\npreserving photo sharing,” 10th USENIX Symposium on Net-\nworked Systems Design and Implementation (NSDI 13), pp.515–\n528, USENIX Association, Lombard, IL, April 2013.\n[5] R. Lagendijk, Z. Erkin, and M. Barni, “Encrypted signal processing\nfor privacy protection: Conveying the utility of homomorphic en-\ncryption and multiparty computation,” IEEE Signal Process. Mag.,\nvol.30, no.1, pp.82–105, Jan. 2013.\n[6] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks\nthat exploit conﬁdence information and basic countermeasures,”\nProc. 22nd ACM SIGSAC Conference on Computer and Communi-\ncations Security, ser. CCS’15, pp.1322–1333, Association for Com-\nputing Machinery, New York, NY , USA, 2015.\n[7] R. Shokri, M. Stronati, C. Song, and V . Shmatikov, “Membership in-\nference attacks against machine learning models,” 2017 IEEE sym-\nposium on security and privacy (SP), pp.3–18, IEEE, 2017.\n[8] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I.J.\nGoodfellow, and R. Fergus, “Intriguing properties of neural net-\nworks,” 2nd Int. Conf. Learning Representations, ICLR 2014, Ban ﬀ,\nAB, Canada, April 14-16, 2014, Conference Track Proceedings, Y .\nBengio and Y . LeCun, eds., 2014.\n[9] R.S.S. Kumar, M. Nystr ¨om, J. Lambert, A. Marshall, M. Goertzel,\nA. Comissoneru, M. Swann, and S. Xia, “Adversarial machine\nlearning-industry perspectives,” 2020 IEEE Security and Privacy\nWorkshops (SPW), pp.69–75, 2020.\n[10] H. Kiya, A.P.M. Maung, Y . Kinoshita, S. Imaizumi, and S. Shiota,\n“An overview of compressible and learnable image transformation\nwith secret key and its applications,” APSIPA Trans. Signal and In-\nformation Processing, vol.11, no.1, e11, 2022. [Online]. Available:\nhttp://dx.doi.org/10.1561/116.00000048\n[11] M. Aprilpyone and H. Kiya, “Block-wise image transformation with\nsecret key for adversarially robust defense,” IEEE Trans. Inf. Foren-\nsics Security, vol.16, pp.2709–2723, 2021.\n[12] A. Maungmaung and H. Kiya, “A protection method of trained CNN\nmodel with a secret key from unauthorized access,” APSIPA Trans.\nSignal and Information Processing, vol.10, p.e10, 2021.\n[13] M. Maung, A. Pyone, and H. Kiya, “Encryption inspired adversar-\nial defense for visual classiﬁcation,” 2020 IEEE Int. Conf. Image\nProcess. (ICIP), 2020, pp.1681–1685.\n[14] A. MaungMaung and H. Kiya, “Ensemble of key-based models: De-\nfense against black-box adversarial attacks,” 2021 IEEE 10th Global\nConference on Consumer Electronics (GCCE), pp.95–98, 2021.\n[15] M. AprilPyone and H. Kiya, “Privacy-preserving image classiﬁca-\ntion using an isotropic network,” IEEE MultiMedia, vol.29, no.2,\npp.23–33, April -June 2022.\n[16] Z. Qi, A. MaungMaung, Y . Kinoshita, and H. Kiya, “Privacy-\npreserving image classiﬁcation using vision transformer,” arXiv\npreprint arXiv:1804.00750, 2022. [Online]. Available: https: //arxiv.\norg/abs/2205.12041\n[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J.\nUszkoreit, and N. Houlsby, “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” Int. Conf. Learning Repre-\nsentations, 2021. [Online]. Available: https: //openreview.net/forum?\nid=YicbFdNTTy\n[18] T. Chuman, W. Sirichotedumrong, and H. Kiya, “Encryption-then-\ncompression systems using grayscale-based image encryption for\njpeg images,” IEEE Trans. Inf. Forensics Security, vol.14, no.6,\npp.1515–1525, June 2019.\n[19] T. Chuman, K. Kurihara, and H. Kiya, “On the security of block\nscrambling-based etc systems against jigsaw puzzle solver attacks,”\n2017 IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP),\npp.2157–2161, 2017.\n[20] J. Zhou, X. Liu, O.C. Au, and Y .Y . Tang, “Designing an e ﬃcient im-\nage encryption-then-compression system via prediction error clus-\ntering and random permutation,” IEEE Trans. Inf. Forensics Secu-\nrity, vol.9, no.1, pp.39–50, Jan. 2014.\n[21] M. Ghonge and K. Nimbokar, “A survey based on designing an e ﬃ-\ncient image encryption-then-compression system,” Int. J. Computer\nApplications, p.8887, 2014.\n[22] T.Y . Liu, K.J. Lin, and H.C. Wu, “ECG data encryption then com-\npression using singular value decomposition,” IEEE J. Biomedical\nand Health Informatics, vol.22, no.3, pp.707–713, May 2018.\n[23] W. Liu, W. Zeng, L. Dong, and Q. Yao, “E ﬃcient compression of\nencrypted grayscale images,” IEEE Trans. Image Process., vol.19,\nno.4, pp.1097–1102, April 2010.\n[24] R. Hu, X. Li, and B. Yang, “A new lossy compression scheme for en-\ncrypted gray-scale images,” 2014 IEEE Int. Conf. Acoust., Speech,\nSignal Process. (ICASSP), pp.7387–7390, 2014.\n[25] M. Johnson, P. Ishwar, V . Prabhakaran, D. Schonberg, and K.\nRamchandran, “On compressing encrypted data,” IEEE Trans. Sig-\nnal Process., vol.52, no.10, pp.2992–3006, Oct. 2004.\n[26] M. Gaata and F.F. Hantoosh, “An e ﬃcient image encryption tech-\nnique using chaotic logistic map and rc4 stream cipher,” Int. J. Mod-\nern Trends in Engineering and Research, vol.3, pp.213–218, 2016.\n[27] O. Watanabe, A. Uchida, T. Fukuhara, and H. Kiya, “An encryption-\nthen-compression system for jpeg 2000 standard,” 2015 IEEE Int.\nConf. Acoust., Speech, Signal Process. (ICASSP), pp.1226–1230,\n2015.\n[28] S. Imaizumi, Y . Izawa, R. Hirasawa, and H. Kiya, “A reversible data\nhiding method in compressible encrypted images,” IEICE Trans.\nFundamentals, vol.E103-A, no.12, pp.1579–1588, Dec. 2020.\n[29] K. Iida and H. Kiya, “Privacy-preserving content-based image re-\ntrieval using compressible encrypted images,” IEEE Access, vol.8,\n10\nIEICE TRANS. INF. & SYST., VOL.E106–D, NO.1 JANUARY 2023\npp.200038–200050, 2020.\n[30] K. Iida and H. Kiya, “An image identiﬁcation scheme of encrypted\njpeg images for privacy-preserving photo sharing services,” 2019\nIEEE Int. Conf. Image Process. (ICIP), pp.4564–4568, 2019.\n[31] S. Liu, Z. Lian, S. Gu, and L. Xiao, “Block shu ﬄing learning\nfor deepfake detection,” arXiv preprint arXiv:2202.02819, 2022.\n[Online]. Available: https: //arxiv.org/abs/2202.02819\n[32] A. Kawamura, Y . Kinoshita, T. Nakachi, S. Shiota, and H. Kiya,\n“A privacy-preserving machine learning scheme using etc images,”\nIEICE Trans. Fundamentals, vol.E103-A, no.12, pp.1571–1578,\nDec. 2020.\n[33] Y . Bandoh, T. Nakachi, and H. Kiya, “Distributed secure sparse\nmodeling based on random unitary transform,” IEEE Access, vol.8,\npp.211762–211772, 2020.\n[34] T. Nakachi, Y . Bandoh, and H. Kiya, “Secure overcomplete dictio-\nnary learning for sparse representation,” IEICE Trans. Inf. & Syst.,\nvol.E103-D, no.1, pp.50–58, Jan. 2020.\n[35] T. Nakachi, Y . Wang, and H. Kiya, “Privacy-preserving pattern\nrecognition using encrypted sparse representations in l0 norm min-\nimization,” ICASSP 2020 - 2020 IEEE Int. Conf. Acoust., Speech,\nSignal Process. (ICASSP), pp.2697–2701, 2020.\n[36] I. Nakamura, Y . Tonomura, and H. Kiya, “Unitary transform-based\ntemplate protection and its application to l2-norm minimization\nproblems,” IEICE Trans. Inf. & Syst., vol.E99-D, no.1, pp.60–68,\nJan. 2016.\n[37] T. Maekawa, A. Kwamura, T. Nakachi, and H. Kiya, “Privacy-\npreserving support vector machine computing using random unitary\ntransformation,” IEICE Trans. Fundamentals, vol.E102-A, no.12,\npp.1849–1855, 2019.\n[38] M. Chen and M. Wu, “Protect your deep neural networks from\npiracy,” 2018 IEEE International Workshop on Inf. Forensics Se-\ncurity (WIFS), pp.1–7, 2018.\n[39] H. Chen, C. Fu, B.D. Rouhani, J. Zhao, and F. Koushanfar, “Deep-\nattest: An end-to-end attestation framework for deep neural net-\nworks,” 2019 ACM/IEEE 46th Annual International Symposium on\nComputer Architecture (ISCA), pp.487–498, Junet 2019.\n[40] Y . Uchida, Y . Nagai, S. Sakazawa, and S. Satoh, “Embedding wa-\ntermarks into deep neural networks,” Proc. 2017 ACM on Int. Conf.\nMultimedia Retrieval, ser. ICMR’17, pp.269–277, Association for\nComputing Machinery, June 2017.\n[41] H. Chen, B.D. Rouhani, C. Fu, J. Zhao, and F. Koushanfar, “Deep-\nmarks: A secure ﬁngerprinting framework for digital rights man-\nagement of deep learning models,” Proc. 2019 Int. Conf. Multime-\ndia Retrieval, ser. ICMR’19, p.105–113, Association for Computing\nMachinery, June 2019.\n[42] B.D. Rouhani, H. Chen, and F. Koushanfar, “Deepsigns: A generic\nwatermarking framework for IP protection of deep learning mod-\nels,” arXiv preprint arXiv:1804.00750, 2018. [Online]. Available:\nhttps://arxiv.org/abs/1804.00750\n[43] L. Fan, K.W. Ng, C.S. Chan, and Q. Yang, “DeepIPR: Deep neu-\nral network intellectual property protection with passports,” IEEE\nTrans. Pattern Anal. Mach. Intell., vol.44, no.10, pp.6122–6139,\nOct. 2021.\n[44] Y . Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, “Turning your\nweakness into a strength: Watermarking deep neural networks by\nbackdooring,” Proc. 27th USENIX Conference on Security Sym-\nposium, ser. SEC’18, pp.1615–1631, USENIX Association, Aug.\n2018.\n[45] J. Zhang, Z. Gu, J. Jang, H. Wu, M.P. Stoecklin, H. Huang, and\nI. Molloy, “Protecting intellectual property of deep neural networks\nwith watermarking,” Proc. 2018 on Asia Conference on Computer\nand Communications Security, ser. ASIACCS’18, pp.159–172, As-\nsociation for Computing Machinery, May 2018.\n[46] S. Sakazawa, E. Myodo, K. Tasaka, and H. Yanagihara, “Visual\ndecoding of hidden watermark in trained deep neural network,”\n2019 IEEE Conf. Multimedia Information Processing and Retrieval\n(MIPR), pp.371–374, 2019.\n[47] E. Le Merrer, P. P ´erez, and G. Tr ´edan, “Adversarial frontier stitching\nfor remote neural network watermarking,” Neural Computing and\nApplications, vol.32, no.13, pp.9233–9244, 2019.\n[48] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language understand-\ning,” Proc. 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Tech-\nnologies, V olume 1 (Long and Short Papers), pp.4171–4186, Asso-\nciation for Computational Linguistics, June 2019.\n[49] I. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T.\nUnterthiner, J. Yung, A.P. Steiner, D. Keysers, J. Uszkoreit, M.\nLucic, and A. Dosovitskiy, “MLP-mixer: An all-MLP architecture\nfor vision,” Advances in Neural Information Processing Systems, A.\nBeygelzimer, Y . Dauphin, P. Liang, and J.W. Vaughan, eds., 2021.\n[50] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby, E.\nGrave, A. Joulin, G. Synnaeve, J. Verbeek, and H. J ´egou, “ResMLP:\nFeedforward networks for image classiﬁcation with data-e ﬃcient\ntraining,” CoRR, vol.abs/2105.03404, 2021.\n[51] S. Chen, E. Xie, C. GE, R. Chen, D. Liang, and P. Luo, “CycleMLP:\nA MLP-like architecture for dense prediction,” Int. Conf. Learning\nRepresentations, 2022. [Online]. Available: https: //openreview.net/\nforum?id=NMEceG4v69Y\n[52] H. Liu, Z. Dai, D. So, and Q.V . Le, “Pay attention to MLPs,” Ad-\nvances in Neural Information Processing Systems, A. Beygelzimer,\nY . Dauphin, P. Liang, and J.W. Vaughan, eds., 2021.\n[53] Q. Hou, Z. Jiang, L. Yuan, M.-M. Cheng, S. Yan, and J. Feng,\n“Vision Permutator: A permutable MLP-like architecture for visual\nrecognition,” IEEE Trans. Pattern Anal. Mach. Intell., 2022.\n[54] A. Trockman and J.Z. Kolter, “Patches are all you need?”\narXiv preprint arXiv:2201.09792, 2022. [Online]. Available:\nhttps://arxiv.org/abs/2201.09792\n[55] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A.\nKopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,\nB. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative\nstyle, high-performance deep learning library,” Advances in Neural\nInformation Processing Systems 32, H. Wallach, H. Larochelle, A.\nBeygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, eds., pp.8024–\n8035, Curran Associates, Inc., 2019.\n[56] M. Tanaka, “Learnable image encryption,” 2018 IEEE Int. Conf.\nConsumer Electronics-Taiwan (ICCE-TW), pp.1–2, 2018.\n[57] W. Sirichotedumrong, T. Maekawa, Y . Kinoshita, and H. Kiya,\n“Privacy-preserving deep neural networks with pixel-based image\nencryption considering data augmentation in the encrypted domain,”\n2019 IEEE Int. Conf. Image Process. (ICIP), pp.674–678, 2019.\n[58] W. Sirichotedumrong and H. Kiya, “A gan-based image transfor-\nmation scheme for privacy-preserving deep neural networks,” 2020\n28th European Signal Processing Conference (EUSIPCO), pp.745–\n749, 2021.\n[59] H. Ito, Y . Kinoshita, M. Aprilpyone, and H. Kiya, “Image to per-\nturbation: An image transformation network for generating visu-\nally protected images for privacy-preserving deep neural networks,”\nIEEE Access, vol.9, pp.64629–64638, 2021.\n[60] B. Schneier and P. Sutherland, Applied Cryptography: Protocols,\nAlgorithms, and Source Code in C, 2nd ed. USA: John Wiley and\nSons, Inc., 1995.\nKIYA et al.: IMAGE AND MODEL TRANSFORMATION WITH SECRET KEY FOR VISION TRANSFORMER\n11\nHitoshi Kiya received his B.E and M.E.\ndegrees from the Nagaoka University of Tech-\nnology in 1980 and 1982, respectively, and his\nDr. Eng. degree from Tokyo Metropolitan Uni-\nversity in 1987. In 1982, he joined Tokyo\nMetropolitan University, where he became a\nFull Professor in 2000. From 1995 to 1996,\nhe attended the University of Sydney, Australia\nas a Visiting Fellow. He is a Fellow of IEEE,\nIEICE, AAIA and ITE. He served as President\nof APSIPA from 2019 to 2020 and as Regional\nDirector-at-Large for Region 10 of the IEEE Signal Processing Society\nfrom 2016 to 2017. He was also President of the IEICE Engineering Sci-\nences Society from 2011 to 2012, and he served there as Editor in-Chief\nfor IEICE Society Magazine and Society Publications. He has been an\nEditorial Board Member of eight journals, including IEEE Trans. on Sig-\nnal Processing, Image Processing, and Information Forensics and Security,\nChair of two technical committees, and Member of nine technical com-\nmittees including the APSIPA Image, Video, and Multimedia Technical\nCommittee (TC) and IEEE Information Forensics and Security TC. He has\norganized a lot of international conferences in such roles as TPC Chair of\nIEEE ICASSP 2012 and as General Co-Chair of IEEE ISCAS 2019. He\nhas received numerous awards, including 12 best paper awards.\nRyota Iijima received his B.C.S degree\nfrom Tokyo Metropolitan University, Japan in\n2022. Since 2022, he has been a Master course\nstudent at Tokyo Metropolitan University. His\nresearch interests include deep neural networks\nand their protection.\nAprilPyone MaungMaung received a BCS\ndegree from the International Islamic University\nMalaysia in 2013 under the Albukhary Founda-\ntion Scholarship, MCS degree from the Univer-\nsity of Malaya in 2018 under the International\nGraduate Research Assistantship Scheme, and\nPh.D. degree from the Tokyo Metropolitan Uni-\nversity in 2022 under the Tokyo Human Re-\nsources Fund for City Diplomacy Scholarship.\nHe is currently working as a Project Assistant\nProfessor in the Tokyo Metropolitan University\nand as a researcher in rinna Co. Ltd. He received an IEEE ICCE-TW Best\nPaper Award in 2016. His research interests are in the area of adversarial\nmachine learning and information security. He is a member of IEEE.\nYuma Kinoshita received his B.Eng.,\nM.Eng., and the Ph.D. degrees from Tokyo\nMetropolitan University, Japan, in 2016, 2018,\nand 2020 respectively. In April 2020, he started\nto work with Tokyo Metropolitan University,\nas a project assistant professor. He moved to\nTokai University, Japan, as an associate profes-\nsor/lecturer in April 2022. His research interests\nare in the area of signal processing, image pro-\ncessing, and machine learning. He is a Member\nof IEEE, APSIPA, IEICE, and ASJ. He received\nthe IEEE ISPACS Best Paper Award, in 2016, the IEEE Signal Processing\nSociety Japan Student Conference Paper Award, in 2018, the IEEE Sig-\nnal Processing Society Tokyo Joint Chapter Student Award, in 2018, the\nIEEE GCCE Excellent Paper Award (Gold Prize), in 2019, and the IWAIT\nBest Paper Award, in 2020. He was a Registration Chair of DCASE2020\nWorkshop.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8674191236495972
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6656360030174255
    },
    {
      "name": "Encryption",
      "score": 0.636244535446167
    },
    {
      "name": "Transformer",
      "score": 0.6249006390571594
    },
    {
      "name": "Key (lock)",
      "score": 0.5723624229431152
    },
    {
      "name": "Computer vision",
      "score": 0.4859248697757721
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.4245131015777588
    },
    {
      "name": "Architecture",
      "score": 0.42140719294548035
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3547371029853821
    },
    {
      "name": "Computer security",
      "score": 0.10739952325820923
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}