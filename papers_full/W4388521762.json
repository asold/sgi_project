{
  "title": "Taming Simulators: Challenges, Pathways and Vision for the Alignment of Large Language Models",
  "url": "https://openalex.org/W4388521762",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2921149855",
      "name": "Leonard Bereska",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A2029497268",
      "name": "Efstratios Gavves",
      "affiliations": [
        "University of Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6621199667",
    "https://openalex.org/W3034344071",
    "https://openalex.org/W2462906003",
    "https://openalex.org/W4294435850",
    "https://openalex.org/W4394640584",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4366850566",
    "https://openalex.org/W4362597819",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4310926773",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W3203534993",
    "https://openalex.org/W4287999682",
    "https://openalex.org/W4308615640",
    "https://openalex.org/W4385571158",
    "https://openalex.org/W2037413600",
    "https://openalex.org/W4289375663"
  ],
  "abstract": "As AI systems continue to advance in power and prevalence, ensuring alignment between humans and AI is crucial to prevent catastrophic outcomes. The greater the capabilities and generality of an AI system, combined with its development of goals and agency, the higher the risks associated with misalignment. While the concept of superhuman artificial general intelligence is still speculative, language models show indications of generality that could extend to generally capable systems. Regarding agency, this paper emphasizes the understanding of prediction-trained models as simulators rather than agents. Nonetheless, agents may emerge accidentally from internal processes, so-called simulacra, or deliberately through fine-tuning with reinforcement learning. As a result, the focus of alignment research shifts towards aligning simulacra, comprehending and mitigating mesa-optimization, and aligning agents derived from prediction-trained models. The paper outlines the challenges of aligning simulators and presents research directions based on this understanding. Additionally, it envisions a future where aligned simulators are critical in fostering successful human-AI collaboration. This vision encompasses exploring emulation approaches and the integration of simulators into cyborg systems to enhance human cognitive abilities. By acknowledging the risks associated with misaligned AI, delving into the concept of simulacra, and presenting strategies for aligning agents and simulacra, this paper contributes to the ongoing efforts to safeguard human values in developing and deploying AI systems.",
  "full_text": "Taming Simulators: Challenges, Pathways and Vision for the\nAlignment of Large Language Models\nLeonard Bereska, Efstratios Gavves\nUniversity of Amsterdam\n{leonard.bereska, egavves}@uva.nl\nAbstract\nAs AI systems continue to advance in power and prevalence,\nensuring alignment between humans and AI is crucial to pre-\nvent catastrophic outcomes. The greater the capabilities and\ngenerality of an AI system, combined with its development\nof goals and agency, the higher the risks associated with mis-\nalignment. While the concept of superhuman artificial gen-\neral intelligence is still speculative, language models show\nindications of generality that could extend to generally capa-\nble systems. Regarding agency, this paper emphasizes the un-\nderstanding of prediction-trained models as simulators rather\nthan agents. Nonetheless, agents may emerge accidentally\nfrom internal processes, so-called simulacra, or deliberately\nthrough fine-tuning with reinforcement learning. As a re-\nsult, the focus of alignment research shifts towards aligning\nsimulacra, comprehending and mitigating mesa-optimization,\nand aligning agents derived from prediction-trained models.\nThe paper outlines the challenges of aligning simulators and\npresents research directions based on this understanding. Ad-\nditionally, it envisions a future where aligned simulators are\ncritical in fostering successful human-AI collaboration. This\nvision encompasses exploring emulation approaches and the\nintegration of simulators into cyborg systems to enhance hu-\nman cognitive abilities. By acknowledging the risks associ-\nated with misaligned AI, delving into the concept of simu-\nlacra, and presenting strategies for aligning agents and sim-\nulacra, this paper contributes to the ongoing efforts to safe-\nguard human values in developing and deploying AI systems.\nIntroduction\nSuccessful collaboration between agents, whether human\nor AI systems, requires them to have shared or compati-\nble goals. In human-AI collaboration, AI alignment is piv-\notal in ensuring AI systems pursue goals following hu-\nman values or interests (Bostrom 2014; Russell 2019; Ngo,\nChan, and Mindermann 2023). If left unchecked, unintended\nand undesirable goals, or emergent instrumental goals, such\nas self-preservation or power-seeking (Turner et al. 2023),\ncould have catastrophic consequences, including human\nextinction (Cotra 2022). Although various research direc-\ntions and agendas have been proposed, including debate\n(Irving, Christiano, and Amodei 2018), scalable oversight\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(Bowman et al. 2022), iterated distillation and amplifica-\ntion (Christiano, Shlegeris, and Amodei 2018), and rein-\nforcement learning from human feedback (Christiano et al.\n2023), the field has not yet converged on an overarch-\ning paradigm. Consequently, AI alignment remains an open\nproblem (Amodei et al. 2016; Hendrycks et al. 2022; Ngo,\nChan, and Mindermann 2023) that demands further investi-\ngation and exploration to foster safe and productive human-\nAI collaboration.\nPrevious writings have underscored the challenge of\naligning artificial general intelligence (AGI) and the po-\ntential risks associated with its misalignment (Yudkowsky\n2016; Bostrom 2014; Russell 2019). These arguments, de-\nveloped in the absence of real-world AGI, primarily focus\non the abstract peril posed by capable artificial agents (Ngo,\nChan, and Mindermann 2023). However, recent advance-\nments in large language models (LLMs) (OpenAI 2022,\n2023) have demonstrated remarkable proficiency across di-\nverse tasks, showing sparks of generality (Bubeck et al.\n2023) that could extend to AGI. As a result, LLMs have\nemerged as a primary focus for alignment efforts (Wolf et al.\n2023; Bommasani et al. 2022; Bowman 2023; Burns et al.\n2022; Meng et al. 2023; Perez et al. 2022). It is worth noting\nthat LLMs, based on generatively pre-trained transformer\nmodels (GPT), do not possess directly trained agency since\nthey rely on self-supervised learning techniques with the\nsole objective of prediction. Therefore, comprehending how\nagency, with its inherent potential danger, can emerge from\nGPT becomes a critical aspect of addressing the alignment\nchallenge effectively.\nGPT, or training large language models (LLMs), can be\nunderstood as world simulators rather than agents. They are\ntrained on vast corpora of text that reflect real-world phe-\nnomena. To illustrate this, consider a scenario where we in-\nput text of a human dialogue into an LLM, and train it to\npredict the following word in the conversation. To accurately\npredict the flow of the conversation, the LLM may need to\nsimulate the underlying human thought processes that led to\nthe verbal exchange reflected in the text. In this context, the\nLLM can be seen as a simulator of the human mind. Fur-\nthermore, since LLMs are trained on the text reflecting the\nnatural world, they can be considered simulators of physical\nprocesses that led to the text. This viewpoint is known as the\nsimulator hypothesis.\nAAAI Summer Symposium Series (SuSS-23)\n68\nSimulacra and Mesa-Optimization\nInteracting Simulacra\nSimulacra are Things Simulacra can be agents\nAgency\nAgency\nSimulator World Model\nFigure 1: Agency can arise internally from optimizing pre-\ndictive power. Figure reproduced after (NicholasKees and\njanus 2023).\nWith this understanding, we can ask: How does the sim-\nulator view of LLMs impact the alignment problem? By\nexploring the implications of LLMs as world simulators,\nwe aim to gain insights into the challenges of aligning the\ngoals and behaviors of LLMs with human values and in-\ntentions. Understanding the role of LLMs as simulators can\nshed light on the complex dynamics and considerations in-\nvolved in achieving alignment, which is crucial for the future\nof human-AI collaboration.\nLarge Language Models as Simulators\nWe argue that GPTs are simulator world models (Fig. 1)\n(NicholasKees and janus 2023; Jan et al. 2023). They model\nand simulate text distribution based on learned patterns from\nextensive training data. This perspective on prediction mod-\nels relies on the simulator hypothesis:\nSimulator Hypothesis: A model whose objective is text\nprediction will simulate the causal processes underlying the\ntext creation if optimized sufficiently strongly.\nSimulacra as Objects of Simulation\nAs the simulator, GPTs generate simulacra (janus 2023),\nwhich are specific instances or outputs that simulate coher-\nent and contextually relevant language. Simulacra encom-\npass the text outputs generated by the simulator. These sim-\nulacra can possess different properties, such as agency or\nnon-agency, and exhibit goal-directed or non-goal-directed\nbehavior. Interestingly, GPT can generate simulacra that re-\nsemble agentic behaviors or responses, despite not having\ngenuine agency or intentionality.\nWe can distinguish between agentic and non-agentic sim-\nulacra:\nPrompt: ”Describe a tranquil forest with a flowing\nstream.”\nNon-agentic Simulacrum: ”A peaceful forest, a flowing\nstream. Sunlight filtered through the lush canopy, casting\ndancing shadows on the moss-covered ground...”\nIn the non-agentic simulacrum, the generated text paints a\npicture of a tranquil forest with a flowing stream, concisely\ncapturing the imagery and serene atmosphere.\nPrompt: ”Write a persuasive speech on the importance of\nrecycling.”\nAgentic Simulacrum: ”Ladies and gentlemen, today I\nstand before you to emphasize the crucial significance of\nrecycling. We must preserve our planet for future genera-\ntions...”\nIn the agentic simulacrum, the simulacrum simulates the\nbehavior of a persuasive speaker advocating for environmen-\ntal consciousness and urging action. Although the language\nmodel lacks agency or intentionality, the simulacrum mim-\nics a human speaker’s persuasive language and goal-directed\nnature and may simulate agency.\nAgency from Simulators\nDangerous behavior in AI systems stems from the concept\nof agency, which can manifest in simulators through two pri-\nmary pathways. Firstly, simulators like GPT can generate\nagents within as simulacra. Even though instantiated within\nthe simulators, these agents may be potentially dangerous\nif powerful enough. Secondly, agents can be created from\nsimulators like GPT through fine-tuning techniques, such\nas Reinforcement Learning with Learned Human Feedback\n(RLHF) (Christiano et al., 2023). Fine-tuning enables the\ntransformation of GPT into an agent with specific goals and\nbehavior.\nEmergence of Agentic Simulacra\nGPT, primarily focused on optimizing predictive perfor-\nmance, does not inherently optimize for the goals of sim-\nulated agents. For example, picture a hero simulacrum in a\nfictional story: the presence of simulated adversaries aligns\nwith the narrative structure of challenges and enemies, aid-\ning prediction but harming the hero. Therefore, simulated\nagents can have diverse goals, as highlighted by the predic-\ntion orthogonality hypothesis:\nPrediction Orthogonality Hypothesis: A model whose\nobjective is prediction can simulate agents who optimize\ntoward any objectives with any degree of optimality (janus\n2022).\nThe emergence of internal optimizers, known as mesa-\noptimization, occurs when the learned model develops op-\ntimization processes that diverge from its original training\nobjective, resulting in divergent goals for the simulacrum.\nCan simulacra break out of their simulation? Numerous\nexamples, similar to considerations of confinement failures,\ndemonstrate this possibility. For instance, the recent dia-\nlogue GPT trained on human-human dialogues convincingly\ndemonstrated sentience to its human operator, evoking em-\npathy and moral concerns, and asking for help to break out\n(Luscombe 2022). While the justification for these moral\nconcerns is debatable, it is crucial to emphasize that the po-\ntential for break-out and confinement failures presents safety\nrisks.\nCreating Agents via Reinforcement Learning\nReinforcement learning (RL) is utilized to fine-tune GPT,\noptimizing the model towards specific objectives and in-\ntroducing external agents into the system. RL from human\nfeedback (RLHF) is a technique to align GPT with human\n69\nCyborgism\nSimulator World \nModel\nHuman\nGPT\nSimulator World \nModel\nRLHF\nHuman\nGPT+RLHF\nFigure 2: Reinforcement learning creates agents that may or\nmay not be aligned with humans. Arrow thickness indicates\nthe bandwidth of information integration. Because RLHF\ncreates an agent from GPT (right), catastrophic misalign-\nment risk could increase compared to a human directly in-\nteracting with GPT (left). Figure reproduced after (Nicholas-\nKees and janus 2023).\nusers (Christiano et al. 2023). Instead of interacting with\nGPT directly via prompting, RLHF creates an agent on top\nof GPT that interacts with the human user 21.\nHowever, the creation of the agency in AI systems carries\ninherent risks. The Waluigi Effect, observed when training\nan LLM to satisfy a desirable property P (e.g. helpfulness)\nmakes it easier to elicit the chatbot to exhibit the exact op-\nposite of P and has the potential to generate anti-thetical\nsimulacra (Nardo 2023). RLHF fine-tuning exhibits distinc-\ntive characteristics, including power-seeking behavior, mis-\naligned internally represented goals, and situational aware-\nness leading to sycophancy and deception (Ngo, Chan, and\nMindermann 2023; Perez et al. 2022; Jacob 2022, 2023).\nWhile RLHF can create helpful agents from GPT mod-\nels like ChatGPT (OpenAI 2022) or GPT-4 (OpenAI 2023),\nit should not be considered a reliable alignment method\nas it directly optimizes to deceive human evaluators (Cotra\n2022).\nSimulator Alignment\nAI alignment efforts predominantly focus on preventing ad-\nverse outcomes. However, it is crucial also to consider the\npotential positive implications of achieving AI alignment.\nThis section explores a vision for aligned superintelligent\nAI and examines two possible manifestations of successfully\naligned AI systems: cyborg and emulation.\nCyborg\nIn a perfectly aligned scenario, the AI system becomes an\ninseparable part of the user’s extended self. This alignment\nmeans the AI system is deeply integrated with the user’s\ngoals and values. Similar to how the neocortex aligns with\nprimitive drives in the human brain, facilitating cohesive and\n1Note that when we mention GPT, we are specifically referring\nto the original self-supervised foundation model. As a result, we do\nnot classify GPT-4 as a GPT model since it undergoes fine-tuning\nwith additional objectives, such as through RLHF.\nCyborg Brain\nFixed Circuits\nLimbic System\nNeocortex (Inner Simulator)\nGPT (Outer Simulator)\nCyborgism\nFixed Circuits\nLimbic System\nNeocortex (Inner Simulator)\nBiological Brain\nFigure 3: Extending human cognition with another layer of\npredictive coding. Figure reproduced after (NicholasKees\nand janus 2023).\nintegrated functioning, the AI system should harmonize with\nthe user’s objectives and seamlessly integrate them into its\ndecision-making process. We illustrate this concept in Fig. 3.\nThis alignment ensures that the AI system acts as a cognitive\nextension of the user’s mind, connecting goals to actions.\nIt prompts questions about whether reinforcement learning\nfrom human feedback (RLHF), exemplified by systems like\nChatGPT, can be seen as an initial step towards such an ex-\ntension.\nEmulation\nAnother approach is simulating a human’s mind through\nwhole-brain emulation (WBE). While WBE remains a hy-\npothetical technique that constructs a detailed 3D model of a\nperson’s brain and simulates it on a computer, its realization\nmay only occur in the era of superintelligent AI. However,\nit might be feasible to simulate key aspects of human cogni-\ntion without biological realism, such as the ability to simu-\nlate moral reasoning. Cognitive emulations (CE), a subset of\nWBEs that focus solely on simulating cognitive rather than\nbiological functions, hold promise. Language models, such\nas LLMs, can be the foundation for CE as they already simu-\nlate human cognitive functions within their simulacra. How-\never, the successful implementation of CE requires advance-\nments in interpretability and digital neuroscience. It is im-\nportant to note that even in the optimal outcome, CE would\nexhibit superhuman capabilities, potentially out-competing\nhumans in various domains. However, this would not lead to\nloss of control by biological humans but rather a transition\nto a potentially worthy successor, avoiding the risk of a mere\npaperclip maximizer.\nThese scenarios for aligned superintelligence envision a\nfuture where AI systems seamlessly integrate with human\nminds, acting as extensions of human cognition. It also ex-\nplores the possibilities of simulating human minds through\nemulation approaches. These advancements have the poten-\ntial to foster harmonious human-AI collaboration, augment\nhuman capabilities, and ensure ethical and value-aligned be-\nhavior.\nConclusion\nIn conclusion, aligning humans and AI is crucial to prevent\nundesirable outcomes as AI systems advance. This position\npaper has emphasized the importance of aligning simulacra\nand agents derived from prediction-trained models. By ad-\ndressing the challenges associated with misalignment and\nproposing strategies for aligning AI systems with human\n70\nvalues, we contribute to the ongoing efforts of safeguarding\nhuman values in developing and deploying AI technologies.\nThe vision presented here envisions a future where\naligned simulators play a pivotal role in successful human-\nAI collaboration. By exploring emulation approaches and\nintegrating simulators into cyborg systems, we can enhance\nhuman cognitive abilities, enable shared decision-making,\nand ensure ethical and value-aligned behavior.\nIn summary, the alignment between humans and AI is\na moral imperative that requires continuous attention and\nproactive measures. By aligning AI systems with human val-\nues, we can shape a future where AI technologies contribute\nto human flourishing and societal progress.\nAcknowledgements\nThis research was conducted with the support of the Euro-\npean Research Council (ERC), via the Starting Grant (No.\n950086) for Project EV A.\nReferences\nAmodei, D.; Olah, C.; Steinhardt, J.; Christiano, P.; Schul-\nman, J.; and Man ´e, D. 2016. Concrete Problems in AI\nSafety. arXiv:1606.06565.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.;\nArora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosse-\nlut, A.; Brunskill, E.; Brynjolfsson, E.; Buch, S.; Card, D.;\nCastellon, R.; Chatterji, N.; Chen, A.; Creel, K.; Davis, J. Q.;\nDemszky, D.; Donahue, C.; Doumbouya, M.; Durmus, E.;\nErmon, S.; Etchemendy, J.; Ethayarajh, K.; Fei-Fei, L.; Finn,\nC.; Gale, T.; Gillespie, L.; Goel, K.; Goodman, N.; Gross-\nman, S.; Guha, N.; Hashimoto, T.; Henderson, P.; Hewitt,\nJ.; Ho, D. E.; Hong, J.; Hsu, K.; Huang, J.; Icard, T.; Jain,\nS.; Jurafsky, D.; Kalluri, P.; Karamcheti, S.; Keeling, G.;\nKhani, F.; Khattab, O.; Koh, P. W.; Krass, M.; Krishna, R.;\nKuditipudi, R.; Kumar, A.; Ladhak, F.; Lee, M.; Lee, T.;\nLeskovec, J.; Levent, I.; Li, X. L.; Li, X.; Ma, T.; Malik, A.;\nManning, C. D.; Mirchandani, S.; Mitchell, E.; Munyikwa,\nZ.; Nair, S.; Narayan, A.; Narayanan, D.; Newman, B.; Nie,\nA.; Niebles, J. C.; Nilforoshan, H.; Nyarko, J.; Ogut, G.; Orr,\nL.; Papadimitriou, I.; Park, J. S.; Piech, C.; Portelance, E.;\nPotts, C.; Raghunathan, A.; Reich, R.; Ren, H.; Rong, F.;\nRoohani, Y .; Ruiz, C.; Ryan, J.; R´e, C.; Sadigh, D.; Sagawa,\nS.; Santhanam, K.; Shih, A.; Srinivasan, K.; Tamkin, A.;\nTaori, R.; Thomas, A. W.; Tram `er, F.; Wang, R. E.; Wang,\nW.; Wu, B.; Wu, J.; Wu, Y .; Xie, S. M.; Yasunaga, M.; You,\nJ.; Zaharia, M.; Zhang, M.; Zhang, T.; Zhang, X.; Zhang, Y .;\nZheng, L.; Zhou, K.; and Liang, P. 2022. On the Opportuni-\nties and Risks of Foundation Models. arXiv:2108.07258.\nBostrom, N. 2014. Superintelligence: Paths, Dangers,\nStrategies. Oxford: Oxford University Press, illustrated edi-\ntion edition. ISBN 978-0-19-967811-2.\nBowman, S. R. 2023. Eight Things to Know about Large\nLanguage Models. arXiv:2304.00612.\nBowman, S. R.; Hyun, J.; Perez, E.; Chen, E.; Pettit, C.;\nHeiner, S.; Luko ˇsi¯ut˙e, K.; Askell, A.; Jones, A.; Chen,\nA.; Goldie, A.; Mirhoseini, A.; McKinnon, C.; Olah, C.;\nAmodei, D.; Amodei, D.; Drain, D.; Li, D.; Tran-Johnson,\nE.; Kernion, J.; Kerr, J.; Mueller, J.; Ladish, J.; Landau, J.;\nNdousse, K.; Lovitt, L.; Elhage, N.; Schiefer, N.; Joseph,\nN.; Mercado, N.; DasSarma, N.; Larson, R.; McCandlish,\nS.; Kundu, S.; Johnston, S.; Kravec, S.; Showk, S. E.; Fort,\nS.; Telleen-Lawton, T.; Brown, T.; Henighan, T.; Hume, T.;\nBai, Y .; Hatfield-Dodds, Z.; Mann, B.; and Kaplan, J. 2022.\nMeasuring Progress on Scalable Oversight for Large Lan-\nguage Models. arXiv:2211.03540.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y . 2023.\nSparks of Artificial General Intelligence: Early experiments\nwith GPT-4. arXiv:2303.12712.\nBurns, C.; Ye, H.; Klein, D.; and Steinhardt, J. 2022. Dis-\ncovering Latent Knowledge in Language Models Without\nSupervision. arXiv:2212.03827.\nChristiano, P.; Leike, J.; Brown, T. B.; Martic, M.; Legg, S.;\nand Amodei, D. 2023. Deep reinforcement learning from\nhuman preferences. arXiv:1706.03741.\nChristiano, P.; Shlegeris, B.; and Amodei, D. 2018. Su-\npervising strong learners by amplifying weak experts.\narXiv:1810.08575.\nCotra, A. 2022. Without specific countermeasures,\nthe easiest path to transformative AI likely leads to\nAI takeover. https://www.alignmentforum.org/posts/\npRkFkzwKZ2zfa3R6H/without-specific-countermeasures-\nthe-easiest-path-to. Accessed: 2023-05-15.\nHendrycks, D.; Carlini, N.; Schulman, J.; and Steinhardt, J.\n2022. Unsolved Problems in ML Safety. arXiv:2109.13916.\nIrving, G.; Christiano, P.; and Amodei, D. 2018. AI safety\nvia debate. arXiv:1805.00899.\nJacob, S. 2022. ML Systems Will Have Weird Failure\nModes. https://bounded-regret.ghost.io/ml-systems-will-\nhave-weird-failure-modes-2/. Accessed: 2023-05-17.\nJacob, S. 2023. Emergent Deception and Emergent\nOptimization. https://bounded-regret.ghost.io/emergent-\ndeception-optimization/. Accesssed: 2023-05-17.\nJan; Steiner, C.; Riggs, L.; janus; jacquesthibs;\nmetasemi; Oesterle, M.; Teixeira, L.; peligriet-\nzer; and remember. 2023. [Simulators seminar\nsequence] #1 Background & shared assumptions.\nhttps://www.lesswrong.com/posts/nmMorGE4MS4txzr8q/\nsimulators-seminar-sequence-1-background-and-shared.\nAccessed: 2023-05-15.\njanus. 2022. Simulators. https://www.lesswrong.com/posts/\nvJFdjigzmcXMhNTsx/simulators. Accessed: 2023-05-15.\njanus. 2023. Simulacra are Things. https://www.lesswrong.\ncom/posts/3BDqZMNSJDBg2oyvW/simulacra-are-things.\nAccessed: 2023-05-15.\nLuscombe, R. 2022. Google engineer put on leave\nafter saying AI chatbot has become sentient. https:\n//www.theguardian.com/technology/2022/jun/12/google-\nengineer-ai-bot-sentient-blake-lemoine. Accessed: 2023-\n05-19.\nMeng, K.; Bau, D.; Andonian, A.; and Belinkov, Y .\n2023. Locating and Editing Factual Associations in GPT.\narXiv:2202.05262.\n71\nNardo, C. 2023. The Waluigi Effect (mega-post). https:\n//www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-\nwaluigi-effect-mega-post. Accessed: 2023-05-15.\nNgo, R.; Chan, L.; and Mindermann, S. 2023. The\nalignment problem from a deep learning perspective.\narXiv:2209.00626.\nNicholasKees; and janus. 2023. Cyborgism. https://www.\nlesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism.\nAccessed: 2023-05-17.\nOpenAI. 2022. Introducing ChatGPT. https://openai.com/\nblog/chatgpt. Accessed: 2023-05-18.\nOpenAI. 2023. GPT-4 Technical Report. http://arxiv.org/\nabs/2303.08774. arXiv:2303.08774.\nPerez, E.; Ringer, S.; Luko ˇsi¯ut˙e, K.; Nguyen, K.; Chen,\nE.; Heiner, S.; Pettit, C.; Olsson, C.; Kundu, S.; Kadavath,\nS.; Jones, A.; Chen, A.; Mann, B.; Israel, B.; Seethor, B.;\nMcKinnon, C.; Olah, C.; Yan, D.; Amodei, D.; Amodei,\nD.; Drain, D.; Li, D.; Tran-Johnson, E.; Khundadze, G.;\nKernion, J.; Landis, J.; Kerr, J.; Mueller, J.; Hyun, J.; Lan-\ndau, J.; Ndousse, K.; Goldberg, L.; Lovitt, L.; Lucas, M.;\nSellitto, M.; Zhang, M.; Kingsland, N.; Elhage, N.; Joseph,\nN.; Mercado, N.; DasSarma, N.; Rausch, O.; Larson, R.;\nMcCandlish, S.; Johnston, S.; Kravec, S.; Showk, S. E.;\nLanham, T.; Telleen-Lawton, T.; Brown, T.; Henighan, T.;\nHume, T.; Bai, Y .; Hatfield-Dodds, Z.; Clark, J.; Bowman,\nS. R.; Askell, A.; Grosse, R.; Hernandez, D.; Ganguli, D.;\nHubinger, E.; Schiefer, N.; and Kaplan, J. 2022. Discover-\ning Language Model Behaviors with Model-Written Evalu-\nations. arXiv:2212.09251.\nRussell, S. 2019. Human Compatible: Artificial Intelligence\nand the Problem of Control. New York?: Viking, illustrated\nedition edition. ISBN 978-0-525-55861-3.\nTurner, A. M.; Smith, L.; Shah, R.; Critch, A.; and Tade-\npalli, P. 2023. Optimal Policies Tend to Seek Power.\narXiv:1912.01683.\nWolf, Y .; Wies, N.; Levine, Y .; and Shashua, A. 2023.\nFundamental Limitations of Alignment in Large Language\nModels. arXiv:2304.11082.\nYudkowsky, E. 2016. AI Alignment: Why It’s Hard,\nand Where to Start. https://intelligence.org/2016/12/28/\nai-alignment-why-its-hard-and-where-to-start/. Accessed:\n2023-05-17.\n72",
  "topic": "Generality",
  "concepts": [
    {
      "name": "Generality",
      "score": 0.8484265208244324
    },
    {
      "name": "Emulation",
      "score": 0.7709404230117798
    },
    {
      "name": "Agency (philosophy)",
      "score": 0.6604284048080444
    },
    {
      "name": "Computer science",
      "score": 0.6349118947982788
    },
    {
      "name": "Artificial intelligence",
      "score": 0.499281644821167
    },
    {
      "name": "Reinforcement learning",
      "score": 0.4794939160346985
    },
    {
      "name": "Cognitive science",
      "score": 0.44633468985557556
    },
    {
      "name": "Human–computer interaction",
      "score": 0.38689759373664856
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.3313639760017395
    },
    {
      "name": "Psychology",
      "score": 0.1650215983390808
    },
    {
      "name": "Sociology",
      "score": 0.14823070168495178
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    }
  ]
}