{
  "title": "LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings",
  "url": "https://openalex.org/W4392637089",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2040022320",
      "name": "Xin Xie",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2149489317",
      "name": "Zhoubo Li",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2108605455",
      "name": "Xiaohan Wang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5111153035",
      "name": "Zekun Xi",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2132377640",
      "name": "Ningyu Zhang",
      "affiliations": [
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4387430414",
    "https://openalex.org/W4221021831",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W4226309857",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4285261975",
    "https://openalex.org/W4385573246",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2250635077",
    "https://openalex.org/W3155001903",
    "https://openalex.org/W3106378800",
    "https://openalex.org/W4221141197",
    "https://openalex.org/W4287889449",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3194836374",
    "https://openalex.org/W3174464510",
    "https://openalex.org/W3117339789",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4285288843",
    "https://openalex.org/W4285172793",
    "https://openalex.org/W2889583850",
    "https://openalex.org/W2883722483",
    "https://openalex.org/W4297847420",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W3176750236",
    "https://openalex.org/W3046075728",
    "https://openalex.org/W4226142803",
    "https://openalex.org/W3166051255",
    "https://openalex.org/W4290944058",
    "https://openalex.org/W2728059831"
  ],
  "abstract": "Xin Xie, Zhoubo Li, Xiaohan Wang, ZeKun Xi, Ningyu Zhang. Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: System Demonstrations. 2023.",
  "full_text": "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of\nthe Asia-Paciﬁc Chapter of the Association for Computational Linguistics: System Demonstrations, pages 25–33\nNovember 1–4, 2023. ©2023 Association for Computational Linguistics\n25\nLambdaKG: A Library for Pre-trained Language Model-Based\nKnowledge Graph Embeddings\nXin Xie1∗, Zhoubo Li1∗, Xiaohan Wang1∗, Zekun Xi1, Ningyu Zhang1 †\n1 Zhejiang University\n{xx2020,zhoubo.li,wangxh07,zhangningyu}@zju.edu.cn,\nhttps://zjunlp.github.io/project/promptkg/\nAbstract\nKnowledge Graphs (KGs) often have two char-\nacteristics: heterogeneous graph structure and\ntext-rich entity/relation information. Text-\nbased KG embeddings can represent entities\nby encoding descriptions with pre-trained lan-\nguage models, but no open-sourced library is\nspecifically designed for KGs with PLMs at\npresent. In this paper, we present LambdaKG,\na library for KGE that equips with many pre-\ntrained language models (e.g., BERT, BART,\nT5, GPT-3) and supports various tasks (e.g.,\nknowledge graph completion, question answer-\ning, recommendation, and knowledge probing).\nLambdaKG is publicly open-sourced1, with a\ndemo video2 and long-term maintenance.\n1 Introduction\nKnowledge Graphs (KGs) encode real-world facts\nas structured data and have drawn significant atten-\ntion from academia, and industry (Zhang et al.,\n2022b). Knowledge Graph Embedding (KGE)\naims to project the relations and entities into a con-\ntinuous vector space, which can enhance knowl-\nedge reasoning abilities and feasibly be applied\nto downstream tasks: question answering (Sax-\nena et al., 2022), recommendation (Zhang et al.,\n2021) and so on (Chen et al., 2022b). Previous\nembedding-based KGE methods, such as TransE\n(Bordes et al., 2013), involved embedding rela-\ntional knowledge into a vector space and subse-\nquently optimizing the target object by applying\na pre-defined scoring function to those vectors. A\nfew remarkable embedding-based KGE toolkits\nhave been developed, such as OpenKE (Han et al.,\n2018), LibKGE (Broscheit et al., 2020), PyKEEN\n(Ali et al., 2021), CogKGE (Jin et al., 2022) and\nNeuralKG (Zhang et al., 2022c). Nevertheless,\n∗ Equal contribution and shared co-first authorship.\n† Corresponding author.\n1Code: https://github.com/zjunlp/PromptKG/tree/\nmain/lambdaKG\n2Video: http://deepke.zjukg.cn/lambdakg.mp4\nthese embedding-based KGE approaches are re-\nstricted in expressiveness regarding the shallow\nnetwork architectures without using any side infor-\nmation (e.g., textual description).\nBy comparison with embedding-based KGE ap-\nproaches, text-based methods incorporate available\ntexts for KGE. With the development of Pre-trained\nLanguage Models (PLMs), many text-based mod-\nels (Xie et al., 2022; Saxena et al., 2022; Kim et al.,\n2020; Markowitz et al., 2022; Chen et al., 2022a;\nLiu et al., 2022) have been proposed, which can\nobtain promising performance and take advantage\nof allocating a fixed memory footprint for large-\nscale real-world KGs. Recently, large language\nmodels (LLMs) (e.g., GPT-3 (Brown et al., 2020),\nChatGPT (OpenAI, 2022)) further demonstrated\nthe ability to perform a variety of natural language\nprocessing (NLP) tasks without adaptation, pro-\nviding potential opportunities of better knowledge\nrepresentations. However, there is no comprehen-\nsive open-sourced library particularly designed\nfor KGE with PLMs at present, which makes it\nchallenging to test new methods and make rigorous\ncomparisons with previous approaches.\nIn this paper, we share with the community a\npre-trained LAnguage Model-BaseD librAry for\nKGEs and applications called LambdaKG (MIT\nLicense), which supports various cutting-edge mod-\nels. Specifically, we equip LambdaKG with both\nsmall PLMs, e.g., BERT (Devlin et al., 2018;\nYao et al., 2019), BART (Lewis et al., 2020; Liu\net al., 2021), T5 (Raffel et al., 2020; Saxena et al.,\n2022); and large PLMs, e.g., GPT-3 (Brown et al.,\n2020), ChatGPT (OpenAI, 2022), by developing\ntwo major paradigms of discrimination-based and\ngeneration-based methods for KGEs. LambdaKG\nsupports factual and commonsense KGs with di-\nverse tasks, including KG completion, question\nanswering, recommendation, and knowledge prob-\ning (LAMA). We will provide maintenance to meet\nnew tasks, new requests and fix bugs.\n26\nModel Hub\nDiscrimination-based\nPKGG\nKGT5\nStAR\nKG-BERT\nSimKGC\nkNN-KGEGenKGC\nGeneration-based\nChatGPT\nGPT-3\nSmall LMsLarge LMs\nTasks\nKGC \nQA \nRec \nLAMA \nUnified KG Encoder\n...\nPre-trained\nLanguage Model\nCore Module\nTrainer&Evaluator\nBag of Tricks\nMetrics\nText description\nPlato:Plato was\nAthenian philosopher\nGraph structure\nlive in\nPlato\nGreek\n...\nFigure 1: The architecture and features of LambdaKG.\n2 System Architecture\nThe overall features & architecture of LambdaKG\nare presented in Figure 1. We will detail two major\ntypes of PLM-based KGE methods (discrimination-\nbased and generation-based) with various PLMs.\nOur design principles are: 1) Core Module\nwith Unified KG Encoder : LambdaKG utilizes\na unified encoder to pack graph structure and text\nsemantics, with convenient Trainer&Evaluator,\nMetric, and Bag of Tricks ; 2) Model Hub :\nLambdaKG is integrated with many cutting-edge\nPLM-based KGE models; 3) Flexible Downstream\nTasks: LambdaKG disentangles KG representa-\ntion learning and downstream tasks.\n2.1 Core Module\n2.1.1 Trainer&Evaluator\nTypically, the training process with LambdaKG\ncan be decomposed into several distinct steps,\nsuch as the forward and backward passes (i.e.,\ntraining_step), logging of intermediate results\n(log), and model evaluation ( evaluate_step).\nOur Trainer class provides a flexible and modular\nframework for training different types of models,\nwith customizable functions to handle various tasks,\nsuch as computing the loss function and updating\nmodel parameters. Moreover, the Trainer class\nallows users to define their own plugins, which can\nbe integrated seamlessly into the training pipeline\nto provide additional functionalities.\n2.1.2 Metric\nWe design the Metric class to evaluate different\nmodels for various tasks. Specifically, we use\nhits@k with k values of 1, 3, 10 and mean rank\n(MR) as the evaluation metrics. Hits@k measures\nthe proportion of correct predictions among the top\nk-ranked results, while MR calculates the average\nrank of the correct answer. We also implement\nBLEU-1 score to evaluate the commonsense KG\ncompletion tasks following Hwang et al. (2021).\n2.1.3 Bag of Tricks\nAll models in the LambdaKG are based on PLMs,\nand we equip a bag of tricks of training techniques\nto improve their performance. In particular, we\nemploy different pluggable modules such as label\nsmoothing and exponential moving average\nto assist in the training of models. We implement\nearly stopping and fast run modules to pre-\nvent overfitting with small data by introducing early\nstopping and automatic verification mechanisms.\nFurthermore, we integrate an off-the-shelf Top-k\nnegative sampling strategy to enhance the\ntraining by selecting the most informative negative\nsamples during the training process.\n2.2 Unified KG Encoder\nSince LambdaKG is based on PLMs, the most\ncritical thing is to convert structural triples into\nplain natural language for PLMs to understand. We\nintroduce a unified KG encoder to represent graph\nstructure and text semantics, supporting different\n27\n Prompt \n1-hop neighbors entity token\nMasked entity description\n1) PLM-based KGEs\n2) Downstream Tasks\nKGC\nQA\nLAMA\nRec\nDiscrimination-based Method\nGeneration-based Method\n Prompt \nEntity\nRelation\nKnowlege Graph Representation\nKnowledge Store\nFigure 2: PLM-based KGEs in LambdaKG and those KGEs can be applied to KGC, QA, recommendation and\nknowledge probing. Entity_t refers to the target tail entity, answer entity, recommended items, and target tail entity\nfor different tasks, which follows the pre-train (obtain the embedding) and fine-tune paradigm (task-specific tuning).\ntypes of PLM-based KGE methods. To encode the\ngraph structure, we sample 1-hop neighbor entities\nand concatenate their tokens as input for implicit\nstructure information. With such a unified KG en-\ncoder, LambdaKG can encode both heterogeneous\ngraph structure and text-rich semantic information.\nFor the discrimination-based method, the input is\nbuilt on the plain text description:\nXhr pair = [CLS] Xh[SEP] Xr [SEP]\nXtail = [CLS] Xt [SEP]. (1)\nwhere Xh, Xr, and Xt refer to the text sequence\nof the head entity, relation, and tail entity, respec-\ntively. Referring to some prompt learning methods\nlike kNN-KGE (Zhang et al., 2022a), we represent\nentities and relations in KG with special tokens\n(See §2.3) and obtain the input as:\nX=[CLS]Xh[Entity h] [SEP]Xr [SEP] [MASK] [SEP],\n(2)\nwhere [Entity h] represents the special token to\nthe head entity.\nFor the generation-based model, we leverage the\ntokens in Xh and Xr to optimize the model with\nthe label Xt. When predicting the head entity,\nwe add a special token [reverse] in the input\nsequence for reverse reasoning.\n2.3 Model Hub\nAs shown in Figure 2 and Table 1, LambdaKG\nconsists of a Model Hub which supports many rep-\nresentative PLM-based KGE methods, mainly fol-\nlow the two major paradigms of discrimination-\nbased methods and generation-based methods as:\nDiscrimination-based methods There are three\nkinds of models based on the discrimination\nmethod: the first one (e.g., KG-BERT (Yao et al.,\n2019), PKGC (Lv et al., 2022)) utilizes a single en-\ncoder to encode triples of KG with text description;\nanother kind of model (e.g., StAR (Wang et al.,\n2021), SimKGC (Wang et al., 2022)) leverages\nsiamese encoder (two-tower models) with PLMs to\nencode entities and relations respectively. For the\nfirst kind, the score of each triple is expressed as:\nScore(h, r, t) = TransformerEnc(Xh, Xr, Xt),\n(3)\nwhere TransformerEnc is the BERT model fol-\nlowed by a binary classifier. However, these mod-\nels have to iterate all the entities calculating scores\nto decide the correct one, which is computation-\nintensive, as shown in Table 1. In contrast, two-\ntower models like StAR (Wang et al., 2021) and\nSimKGC (Wang et al., 2022) usually encode ⟨h, r⟩\nand t to obtain the embeddings. Then, they use a\n28\nModel PLM Support Tasks Complexity\nKGBERT (Yao et al., 2019) MLM KGC O(|L|2|E|2|R|)\nStAR (Wang et al., 2021) MLM KGC O(|L/2|2|E|(1 +|R|))\nSimKGC (Wang et al., 2022) MLM KGC O(|L/2|2|E|(1 +|R|))\nkNN-KGE (Zhang et al., 2022a) MLM KGC, LAMAO(|L|2|E||R|)\nKGT5 (Saxena et al., 2022) Seq2Seq KGC, QA O(|L/2|3|E||R|)\nGenKGC (Xie et al., 2022) Seq2Seq KGC, QA O(|L/2|3|E||R|)\nTable 1: Comparison of different methods based on small PLMs. |L| is the length of the triple description. |L/2|\ncan be seen as the length of entity tokens. |E| and |R| are the numbers of all unique entities and relations in the\ngraph respectively.\nscore function to predict the correct tail entity from\nthe candidates, denoted by:\nScore(⟨h, r⟩, t) = cos(e⟨h,r⟩, et). (4)\nThe final kind of model, e.g., kNN-KGE (Zhang\net al., 2022a), utilizes masked language modeling\nfor KGE, which shares the same architecture as nor-\nmal discrimination PLMs. Note that there are two\nmodules in the normal PLMs: a word embedding\nlayer to embed the token ids into semantic space\nand an encoder to generate context-aware token\nembedding. Here, we take the masked language\nmodel and treat entities and relations as special\ntokens in the “word embedding layer”. As shown\nin Figure 2, the model predicts the correct tail entity\nwith the sequence of the head entity and relation\ntoken and their descriptions. For the entity/relation\nembedding, we freeze the encoder layer, only tun-\ning the entity embedding layer, to optimize the loss\nfunction:\nL = − 1\n|E|\nP\nej∈E\nI(ej = ei) logp\u0000[MASK]= ej | Xi; Θ\u0001,\n(5)\nwhere Θ represents the parameters of the model,\nXi and ei is the description and the embedding of\nentity i.\nGeneration-based methods Generation-based\nmodels formulate KG completion or other KG-\nintensive tasks as sequence-to-sequence generation.\nGiven a triple with the tail entity missing (h, r,?),\nmodels are fed with ⟨Xh, Xr⟩ and then output Xt.\nIn the training procedure, generative models maxi-\nmize the conditional probability:\nScore(h, r, t) =Q|Xt|\ni=1 p(xti|xt1, xt2, ..., xti−1;⟨Xh, Xr⟩).\n(6)\nTo guarantee the consistency of decoding se-\nquential schemas and tokens in KG, GenKGC (Xie\net al., 2022) proposes an entity-aware hierarchical\nGiven head entity and relation, predict the tail\nentity from the candidates: [ 100 candidates ]\nWhat is the genre of Charlie's Angels: FullThrottle? Charlie's Angels: Full Throttle is a\n2003 American action comedy film. The answeris Comedy-GB.\n...\nWhat is the prequel of Charlie's Angels: FullThrottle?\nnatural language\nrationales\nWhat is the < relation >\nof  < head entity >?\nTrain\n Test\ndemonstrations & candidates\nInformation Retrieval\n×5 \nFigure 3: LLM-based KGC. The prompt comprises\nthree components, namely the task description with can-\ndidates, demonstrations, and test information.\ndecoder to constrain Xt. Besides, KGT5 (Saxena\net al., 2022) proposes to pre-train generation-based\nPLMs with text descriptions for KG representation.\nLLMs We further apply the LLMs, namely GPT-\n3 and ChatGPT, to assess their effectiveness in\nKGE (KGC with link prediction). Generative\nLLMs allow the KGC task to be framed as input\nsentences containing header entities and relations,\nmaking it easier for the model to generate sentences\nwith tail entities. A well-designed prompt can im-\nprove the performance of LLMs, and prior studies\nindicate incorporating in-context learning can im-\nprove accuracy and ensure consistent output. Thus,\nwe adopt a similar approach that the prompt com-\nprises three components: task description with can-\ndidates, demonstrations, and test information.\nAs shown in Figure 3, we employ information\nretrieval (BM25) to select the top 100 most relevant\nentities from the training set as candidates. Like-\nwise, the prompt’s demonstrations utilize the top-\n5 most similar instances, which assist the model\nin comprehending the task more effectively. Fur-\nthermore, taking inspiration from the Chain-of-\n29\nThought (CoT) method in reasoning tasks, we uti-\nlize natural language rationales to improve\nthe model’s capacity to reason and explain predic-\ntions, ultimately improving its overall performance\nin KGC tasks. Comparatively, the prompt used for\nChatGPT solely utilizes a few demonstrations and\ntest data with these strategies.\n2.4 Pluggable KGE for Downstream Tasks\nWe introduce the technical details of applying KGE\nto downstream tasks as shown in Figure 2. For\nknowledge graph completion, we feed the model\nwith the textual information ⟨Xh, Xr⟩ of the head\nentity and the relation, then obtain the target tail en-\ntity via mask token prediction. For question answer-\ning, we feed the model with the question written\nin natural language concatenated with a [MASK] to-\nken to obtain the special token of the target answer\n(entity). For recommendation, we take the user’s\ninteraction history as sequential input (Sun et al.,\n2019) with entity embeddings and then leverage\nthe mask token prediction to obtain recommended\nitems. For the knowledge probing task, we adopt\nentity embedding as additional knowledge follow-\ning PELT (Ye et al., 2022).\n3 System Usage\nThe proposed system can be used in three scenar-\nios. First, users can utilize LambdaKG to ob-\ntain PLM-based KGE for knowledge discovery.\nLitModel serves as the training of link prediction\ntask class and fit for all models in Model Hub.\nUsers can choose proper models in ModelModule\nand specific metrics in DataModule to train mod-\nels to obtain the embedding in the KGs. More-\nover, users can utilize LambdaKG PLM-based\nKGE for downstream tasks. We provide various\nprompts to obtain the knowledge (entity) embed-\nding in KGs for downstream tasks. For different\ntasks, we design different base classes for users to\nefficiently implement their own tasks. Finally, we\nprovide an online interactive demo for PLM-based\nKGE at https://zjunlp.github.io/project/\npromptkg/demo.html.\n4 Evaluation\n4.1 Knowledge Graph Completion\nFor the KG completion task with small PLMs,\nwe conduct link prediction experiments on two\ndatasets WN18RR (Dettmers et al., 2018), and\nFB15k-237 (Toutanova et al., 2015). From Table 2,\nTask Dataset Method hits1 MRR\nKG Completion\nWN18RR\nKG-BERT3 4.1 21.6StAR3 24.3 40.1SimKGC 42.5 60.8KGT5 17.9 -GenKGC 39.6 -kNN-KGE 52.5 57.9\nFB15k-237\nKG-BERT3 - -StAR3 20.5 29.6SimKGC 22.6 30.1KGT5 10.8 -GenKGC 19.2 -kNN-KGE 28.0 37.3\nQuestion AnsweringMetaQA\nGT query3 63.3 -PullNet3 65.1 -KGT5 67.8 -\nRecommendationML-20mBERT4Rec3 34.4 47.9LambdaKG 37.3 50.5\nKnowledge Probing\nTREx\nBERT 28.6 37.7RoBERTa 19.9 27.8LambdaKG(RoBERTa) 22.1 29.8\nSquad\nBERT 13.2 23.5RoBERTa 13.4 24.6LambdaKG(RoBERTa) - -\nGoogle RE\nBERT 10.3 17.3RoBERTa 7.6 12.8LambdaKG(RoBERTa) 8.1 14.2\nTable 2: Hits1 and MRR (%) results on KGC, question\nanswering, recommendation and knowledge probing\ntasks. 3 refers to the results from origin papers.\nFB15k-237\n0\n10\n20\n30\n40\n50\n22.6\n13.5\n32.1\nhits1\nSimKGC\n(a)\nATOMIC2020\n46.9\n31.1 29.4\n15.0\nChatGPT\nGPT3 (text-davinci-001)\nGPT3.5 (text-davinci-003)\nCOMET(BART)\nbleu1\n100\n(b)\nATOMIC2020\n0\n20\n40\n60\n80\n100\n53.9\n65.2\n45.2\n86.1\nmanual acc\n(c)\nFigure 4: Results on small and large LMs. (a)\nhit@1 scores on FB15k-237. (b) BLEU-1 scores on\nATOMIC2020. (c) Accuracy scores on ATOMIC2020\nby manual evaluation.\nwe observe that the discrimination-based method\nSimKGC (Wang et al., 2022) (previous state-of-the-\nart) achieves higher performance than other base-\nlines. Generation-based models like KGT5 (Sax-\nena et al., 2022) and GenKGC (Xie et al., 2022)\nalso yield comparable results and show potential\nabilities in KG representation.\nSmall vs. Large LMs We adopt GPT-3/3.5\n(text-davinci-001/003 and ChatGPT) for evalu-\nation and assessment through the interfaces pro-\nvided by OpenAI. The evaluation of ChatGPT\n30\n60\n 40\n 20\n 0 20 40\n  Left:1-1 case            Right:1-n case\nChatGPT\nT ext-Davinci-003\n19.18\n5.56\n56.41\n37.08\nLLMs on KGC task\nFigure 5: hit@1 of ChatGPT and text-davinci-003\nin FB15k-237.\nis conducted on 224 instances, with each rela-\ntion in the test set. As shown in Figure 4(a),\nChatGPT demonstrates better performance, while\ntext-davinci-003 exhibits a slight gap. The ex-\nperiment has reaffirmed the capability of LLMs\nin capturing semantic similarities and regularities\namong entities, thereby allowing for precise predic-\ntions of missing links in knowledge graphs.\nIn cases where one head entity and relation pair\ncorrespond to one or multiple tail entities (1-1 and\n1-n cases), we conducted a detailed analysis. No-\ntably, the model performs significantly better in the\n1-1 case compared to the 1-n case, as illustrated in\nFigure 5. Two potential reasons explain this dispar-\nity: (1) In the 1-1 case, the model demonstrates a\nlower propensity for language understanding devi-\nations. Additionally, ChatGPT’s training utilizes\na larger corpus, enhancing the model to generate\naccurate responses through analysis and reason-\ning. (2) The presence of multiple correspondences\nposes a challenge for the model’s capacity to gener-\nate informative and contextually relevant responses.\nMoreover, current evaluation metrics fail to fully\ncapture the intricacy of the responses necessary to\nproperly handle such questions.\nWe further conduct experiments on common-\nsense KG completion with ATOMIC2020 (Hwang\net al., 2021). As suggested in the paper, we sample\n5,000 test queries to evaluate the models (excluding\nChatGPT). COMET (BART) is fine-tuned through\nsupervised learning and utilizes greedy decoding\nto generate answers. For GPT3 and ChatGPT, we\nprovide each relation with 5 examples of heads and\ntails to construct prompts and evaluate them in a\nzero-shot setting. The results, as shown in Figure\n4(b), demonstrates the BLEU-1 scores on the sam-\npled 5,000 queries, while we sample 115 (5 for\neach relation) queries from the test for ChatGPT.\nThe results indicate that GPT-3 exhibits limited per-\nformance in the system evaluation. After analyzing\nseveral cases, we sample 115 (5 for each relation)\nqueries as a benchmark and apply manual scoring\nto evaluate models. Figure 4(c) depicts the accu-\nracy scores of each model. Our study reveals that\nChatGPT is capable of generating reasonable out-\nputs, but they are quite different from the ground\ntruth, which accounts for the final results.\n4.2 Question Answering\nKG is known to be helpful for the task of ques-\ntion answering. We apply LambdaKG to ques-\ntion answering and conduct experiments on the\nMetaQA dataset. Due to computational resource\nlimits, we only evaluate the 1-hop inference per-\nformance. From Table 2, KGT5 in LambdaKG\nyields the best performance.\n4.3 Recommendation\nFor the recommendation task, we conduct exper-\niments on a well-established version ML-20m 3.\nLinkage of ML-20m and Freebase offered by\nKB4Rec (Zhao et al., 2019) is utilized to ob-\ntain textual descriptions of movies in ML-20m.\nWith movie embeddings pre-trained on these de-\nscriptions, we conduct experiments on sequential\nrecommendation tasks following the settings of\nBERT4Rec (Sun et al., 2019). We notice that\nLambdaKG is confirmed to be effective for the\nrecommendation compared with BERT4Rec.\n4.4 Knowledge Probing\nKnowledge probing (Petroni et al., 2019) exam-\nines the ability of LMs (BERT, RoBERTa, etc.)\nto recall facts from their parameters. We conduct\nexperiments on LAMA using pre-trained BERT\n(bert-base-uncased) and RoBERTa (roberta-base)\nmodels. To prove that entity embedding enhanced\nby KGs helps LMs grab more factual knowledge\nfrom PLMs, we train a pluggable entity embed-\nding module following PELT (Ye et al., 2022). As\nshown in Table 2, the performance boosts while we\nuse the entity embedding module.\n5 Conclusion and Future Work\nWe propose LambdaKG, a library that establishes\na unified toolkit with well-defined modules and\neasy-to-use interfaces to support research on using\nPLMs on KGs. In the future, we will continue to\nintegrate more models and tasks (e.g., dialogue)\ninto the proposed library to facilitate the research\nprogress of the KG.\n3https://grouplens.org/datasets/movielens/20m/\n31\nAcknowledgment\nWe would like to express our heartfelt gratitude\nto the anonymous reviewers for their thoughtful\nand kind comments. This work was supported by\nthe National Natural Science Foundation of China\n(No.62206246), Zhejiang Provincial Natural Sci-\nence Foundation of China (No. LGG22F030011),\nNingbo Natural Science Foundation (2021J190),\nYongjiang Talent Introduction Programme (2021A-\n156-G).\nReferences\nMehdi Ali, Max Berrendorf, Charles Tapley Hoyt, Lau-\nrent Vermue, Sahand Sharifzadeh, V olker Tresp, and\nJens Lehmann. 2021. Pykeen 1.0: A python library\nfor training and evaluating knowledge graph embed-\ndings. J. Mach. Learn. Res., 22:82:1–82:6.\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013, Lake\nTahoe, Nevada, United States, pages 2787–2795.\nSamuel Broscheit, Daniel Ruffinelli, Adrian Kochsiek,\nPatrick Betz, and Rainer Gemulla. 2020. Libkge - A\nknowledge graph embedding library for reproducible\nresearch. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, EMNLP 2020 - Demos,\nOnline, November 16-20, 2020, pages 165–174. As-\nsociation for Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nChen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam.\n2022a. Knowledge is flat: A seq2seq generative\nframework for various knowledge graph completion.\nIn Proceedings of the 29th International Confer-\nence on Computational Linguistics, COLING 2022,\nGyeongju, Republic of Korea, October 12-17, 2022,\npages 4005–4017. International Committee on Com-\nputational Linguistics.\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,\nYunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. 2022b. Knowprompt: Knowledge-\naware prompt-tuning with synergistic optimization\nfor relation extraction. In WWW ’22: The ACM Web\nConference 2022, Virtual Event, Lyon, France, April\n25 - 29, 2022, pages 2778–2788. ACM.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d knowl-\nedge graph embeddings. In Proceedings of the Thirty-\nSecond AAAI Conference on Artificial Intelligence,\n(AAAI-18), the 30th innovative Applications of Arti-\nficial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in Artificial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, pages 1811–1818. AAAI Press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nXu Han, Shulin Cao, Xin Lv, Yankai Lin, Zhiyuan Liu,\nMaosong Sun, and Juanzi Li. 2018. Openke: An\nopen toolkit for knowledge embedding. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2018: Sys-\ntem Demonstrations, Brussels, Belgium, October 31 -\nNovember 4, 2018, pages 139–144. Association for\nComputational Linguistics.\nJena D. Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2021. (comet-) atomic 2020: On sym-\nbolic and neural commonsense knowledge graphs.\nIn Thirty-Fifth AAAI Conference on Artificial Intel-\nligence, AAAI 2021, Thirty-Third Conference on In-\nnovative Applications of Artificial Intelligence, IAAI\n2021, The Eleventh Symposium on Educational Ad-\nvances in Artificial Intelligence, EAAI 2021, Virtual\nEvent, February 2-9, 2021, pages 6384–6392. AAAI\nPress.\nZhuoran Jin, Tianyi Men, Hongbang Yuan, Zhitao He,\nDianbo Sui, Chenhao Wang, Zhipeng Xue, Yubo\nChen, and Jun Zhao. 2022. Cogkge: A knowledge\ngraph embedding toolkit and benchmark for repre-\nsenting multi-source and heterogeneous knowledge.\nIn Proceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2022 -\nSystem Demonstrations, Dublin, Ireland, May 22-27,\n2022, pages 166–173. Association for Computational\nLinguistics.\nBosung Kim, Taesuk Hong, Youngjoong Ko, and\nJungyun Seo. 2020. Multi-task learning for knowl-\nedge graph completion with pre-trained language\nmodels. In Proceedings of the 28th International\nConference on Computational Linguistics , pages\n1737–1743, Barcelona, Spain (Online). International\nCommittee on Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\n32\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nXiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong\nQiu, Mengdi Zhang, Wei Wu, Yuxiao Dong, and Jie\nTang. 2022. Mask and reason: Pre-training knowl-\nedge graph transformers for complex logical queries.\nIn KDD ’22: The 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, Washington,\nDC, USA, August 14 - 18, 2022 , pages 1120–1130.\nACM.\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S.\nYu. 2021. KG-BART: knowledge graph-augmented\nBART for generative commonsense reasoning. In\nThirty-Fifth AAAI Conference on Artificial Intelli-\ngence, AAAI 2021, Thirty-Third Conference on In-\nnovative Applications of Artificial Intelligence, IAAI\n2021, The Eleventh Symposium on Educational Ad-\nvances in Artificial Intelligence, EAAI 2021, Virtual\nEvent, February 2-9, 2021, pages 6418–6425. AAAI\nPress.\nXin Lv, Yankai Lin, Yixin Cao, Lei Hou, Juanzi Li,\nZhiyuan Liu, Peng Li, and Jie Zhou. 2022. Do pre-\ntrained models benefit knowledge graph completion?\nA reliable evaluation and a reasonable approach. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 3570–3581. Association for Computa-\ntional Linguistics.\nElan Markowitz, Keshav Balasubramanian, Mehrnoosh\nMirtaheri, Murali Annavaram, Aram Galstyan, and\nGreg Ver Steeg. 2022. StATIK: Structure and text\nfor inductive knowledge graph completion. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, pages 604–615, Seattle, United States.\nAssociation for Computational Linguistics.\nOpenAI. 2022. Chatgpt: Optimizing language mod-\nels for dialogue. https://openai.com/blog/\nchatgpt/.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language mod-\nels as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2463–2473. Association for\nComputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nApoorv Saxena, Adrian Kochsiek, and Rainer Gemulla.\n2022. Sequence-to-sequence knowledge graph com-\npletion and question answering. In Proceedings of\nthe 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 2814–\n2828. Association for Computational Linguistics.\nFei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin,\nWenwu Ou, and Peng Jiang. 2019. Bert4rec: Se-\nquential recommendation with bidirectional encoder\nrepresentations from transformer. In Proceedings\nof the 28th ACM International Conference on Infor-\nmation and Knowledge Management, CIKM 2019,\nBeijing, China, November 3-7, 2019 , pages 1441–\n1450. ACM.\nKristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\nfung Poon, Pallavi Choudhury, and Michael Gamon.\n2015. Representing text for joint embedding of text\nand knowledge bases. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2015, Lisbon, Portugal,\nSeptember 17-21, 2015, pages 1499–1509. The As-\nsociation for Computational Linguistics.\nBo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying\nWang, and Yi Chang. 2021. Structure-augmented\ntext representation learning for efficient knowledge\ngraph completion. In WWW ’21: The Web Confer-\nence 2021, Virtual Event / Ljubljana, Slovenia, April\n19-23, 2021, pages 1737–1748. ACM / IW3C2.\nLiang Wang, Wei Zhao, Zhuoyu Wei, and Jingming\nLiu. 2022. Simkgc: Simple contrastive knowledge\ngraph completion with pre-trained language models.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 4281–4294. Association for Com-\nputational Linguistics.\nXin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, Hui\nChen, Feiyu Xiong, Mosha Chen, and Huajun Chen.\n2022. From discrimination to generation: Knowl-\nedge graph completion with generative transformer.\nIn Companion of The Web Conference 2022, Virtual\nEvent / Lyon, France, April 25 - 29, 2022 , pages\n162–165. ACM.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\nKG-BERT: BERT for knowledge graph completion.\nCoRR, abs/1909.03193.\nDeming Ye, Yankai Lin, Peng Li, Maosong Sun, and\nZhiyuan Liu. 2022. A simple but effective pluggable\nentity lookup table for pre-trained language models.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n2: Short Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 523–529. Association for Com-\nputational Linguistics.\nNingyu Zhang, Qianghuai Jia, Shumin Deng, Xiang\nChen, Hongbin Ye, Hui Chen, Huaixiao Tou, Gang\n33\nHuang, Zhao Wang, Nengwei Hua, and Huajun Chen.\n2021. Alicg: Fine-grained and evolvable conceptual\ngraph construction for semantic search at alibaba.\nIn KDD ’21: The 27th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, Virtual\nEvent, Singapore, August 14-18, 2021, pages 3895–\n3905. ACM.\nNingyu Zhang, Xin Xie, Xiang Chen, Shumin Deng,\nChuanqi Tan, Fei Huang, Xu Cheng, and Hua-\njun Chen. 2022a. Reasoning through memoriza-\ntion: Nearest neighbor knowledge graph embeddings.\nCoRR, abs/2201.05575.\nNingyu Zhang, Xin Xu, Liankuan Tao, Haiyang Yu,\nHongbin Ye, Shuofei Qiao, Xin Xie, Xiang Chen,\nZhoubo Li, Lei Li, et al. 2022b. Deepke: A\ndeep learning based knowledge extraction toolkit\nfor knowledge base population. In Proceedings of\nEMNLP Demonstration.\nWen Zhang, Xiangnan Chen, Zhen Yao, Mingyang\nChen, Yushan Zhu, Hongtao Yu, Yufeng Huang, Ya-\njing Xu, Ningyu Zhang, Zezhong Xu, Zonggang\nYuan, Feiyu Xiong, and Huajun Chen. 2022c. Neu-\nralkg: An open source library for diverse representa-\ntion learning of knowledge graphs. In SIGIR, pages\n3323–3328. ACM.\nWayne Xin Zhao, Gaole He, Kunlin Yang, Hongjian\nDou, Jin Huang, Siqi Ouyang, and Ji-Rong Wen.\n2019. Kb4rec: A data set for linking knowledge\nbases with recommender systems. Data Intell. ,\n1(2):121–136.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6968116760253906
    },
    {
      "name": "Zhàng",
      "score": 0.6921831369400024
    },
    {
      "name": "Computational linguistics",
      "score": 0.5038241744041443
    },
    {
      "name": "Natural language processing",
      "score": 0.49930262565612793
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48146024346351624
    },
    {
      "name": "Knowledge graph",
      "score": 0.4589596092700958
    },
    {
      "name": "Association (psychology)",
      "score": 0.44368258118629456
    },
    {
      "name": "Graph theory",
      "score": 0.43555474281311035
    },
    {
      "name": "Graph",
      "score": 0.41353821754455566
    },
    {
      "name": "Linguistics",
      "score": 0.34449341893196106
    },
    {
      "name": "Programming language",
      "score": 0.33812814950942993
    },
    {
      "name": "China",
      "score": 0.26006874442100525
    },
    {
      "name": "Theoretical computer science",
      "score": 0.25071173906326294
    },
    {
      "name": "Mathematics",
      "score": 0.11423206329345703
    },
    {
      "name": "History",
      "score": 0.11341387033462524
    },
    {
      "name": "Psychology",
      "score": 0.1082196831703186
    },
    {
      "name": "Philosophy",
      "score": 0.09909096360206604
    },
    {
      "name": "Combinatorics",
      "score": 0.08191666007041931
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    }
  ]
}