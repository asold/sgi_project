{
    "title": "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models",
    "url": "https://openalex.org/W3020268419",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2320201689",
            "name": "Mengjie Zhao",
            "affiliations": [
                "LMU Klinikum",
                "Ludwig-Maximilians-Universität München"
            ]
        },
        {
            "id": "https://openalex.org/A2088064778",
            "name": "Tao Lin",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2127241586",
            "name": "Fei Mi",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2097383200",
            "name": "Martin Jaggi",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2035156685",
            "name": "Hinrich Schütze",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6631349028",
        "https://openalex.org/W2896409484",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2808168148",
        "https://openalex.org/W3037282604",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W2114766824",
        "https://openalex.org/W2523060838",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2899402383",
        "https://openalex.org/W2963384892",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2963012544",
        "https://openalex.org/W4322588869",
        "https://openalex.org/W2970352191",
        "https://openalex.org/W2894740066",
        "https://openalex.org/W2963674932",
        "https://openalex.org/W2170973209",
        "https://openalex.org/W3015233032",
        "https://openalex.org/W3005700362",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W3022969335",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W4288796528",
        "https://openalex.org/W2008652694",
        "https://openalex.org/W2970866842",
        "https://openalex.org/W3103754749",
        "https://openalex.org/W2963959597",
        "https://openalex.org/W2963159690",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4288263223",
        "https://openalex.org/W1597409757",
        "https://openalex.org/W2995463996",
        "https://openalex.org/W2125389748",
        "https://openalex.org/W2788838181",
        "https://openalex.org/W2242818861",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2791091755",
        "https://openalex.org/W2805003733",
        "https://openalex.org/W4322588812",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2911300548",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W2524428287",
        "https://openalex.org/W2613332842",
        "https://openalex.org/W2793333878",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2267635276",
        "https://openalex.org/W3023285645",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2970277060",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2970120757",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W2752201871",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2472331097",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963813662",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2962933129",
        "https://openalex.org/W4288333794",
        "https://openalex.org/W3097861677",
        "https://openalex.org/W2777662428",
        "https://openalex.org/W4295262505",
        "https://openalex.org/W2144578941",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2913190747",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2989499211",
        "https://openalex.org/W4287867774",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2952729433",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2971033911",
        "https://openalex.org/W3104263050",
        "https://openalex.org/W2963247446",
        "https://openalex.org/W2952984539",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W2300242332",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2914120296"
    ],
    "abstract": "We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning.",
    "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2226–2241,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n2226\nMasking as an Efﬁcient Alternative to Finetuning\nfor Pretrained Language Models\nMengjie Zhao†*, Tao Lin‡*, Fei Mi‡, Martin Jaggi‡, Hinrich Schütze†\n† LMU Munich, Germany ‡ EPFL, Switzerland\nmzhao@cis.lmu.de, {tao.lin, fei.mi, martin.jaggi}@epfl.ch\nAbstract\nWe present an efﬁcient method of utilizing pre-\ntrained language models, where we learn selec-\ntive binary masks for pretrained weights in lieu\nof modifying them through ﬁnetuning. Exten-\nsive evaluations of masking BERT, RoBERTa,\nand DistilBERT on eleven diverse NLP tasks\nshow that our masking scheme yields perfor-\nmance comparable to ﬁnetuning, yet has a\nmuch smaller memory footprint when several\ntasks need to be inferred. Intrinsic evaluations\nshow that representations computed by our bi-\nnary masked language models encode informa-\ntion necessary for solving downstream tasks.\nAnalyzing the loss landscape, we show that\nmasking and ﬁnetuning produce models that\nreside in minima that can be connected by a\nline segment with nearly constant test accu-\nracy. This conﬁrms that masking can be uti-\nlized as an efﬁcient alternative to ﬁnetuning.\n1 Introduction\nFinetuning a large pretrained language model like\nBERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019b), and XLNet (Yang et al., 2019) often yields\ncompetitive or even state-of-the-art results on NLP\nbenchmarks (Wang et al., 2018, 2019). Given an\nNLP task, standard ﬁnetuning stacks a linear layer\non top of the pretrained language model and then\nupdates all parameters using mini-batch SGD. Vari-\nous aspects like brittleness (Dodge et al., 2020) and\nadaptiveness (Peters et al., 2019) of this two-stage\ntransfer learning NLP paradigm (Dai and Le, 2015;\nHoward and Ruder, 2018) have been studied.\nDespite the simplicity and impressive perfor-\nmance of ﬁnetuning, the prohibitively large number\nof parameters to be ﬁnetuned, e.g., 340 million in\nBERT-large, is a major obstacle to wider deploy-\nment of these models. The large memory foot-\nprint of ﬁnetuned models becomes more prominent\n* Equal contribution.\nwhen multiple tasks need to be solved – several\ncopies of the millions of ﬁnetuned parameters have\nto be saved for inference.\nRecent work (Gaier and Ha, 2019; Zhou et al.,\n2019) points out the potential of searching neural\narchitectures within a ﬁxed model, as an alternative\nto optimizing the model weights for downstream\ntasks. Inspired by these results, we present mask-\ning, a simple yet efﬁcient scheme for utilizing pre-\ntrained language models. Instead of directly updat-\ning the pretrained parameters, we propose to select\nweights important to downstream NLP tasks while\ndiscarding irrelevant ones. The selection mecha-\nnism consists of a set of binary masks, one learned\nper downstream task through end-to-end training.\nWe show that masking, when being applied to\npretrained language models like BERT, RoBERTa,\nand DistilBERT (Sanh et al., 2019), achieves per-\nformance comparable to ﬁnetuning in tasks like\npart-of-speech tagging, named-entity recognition,\nsequence classiﬁcation, and reading comprehen-\nsion. This is surprising in that a simple subselec-\ntion mechanism that does not change any weights\nis competitive with a training regime – ﬁnetuning\n– that can change the value of every single weight.\nWe conduct detailed analyses revealing important\nfactors and possible reasons for the desirable per-\nformance of masking.\nMasking is parameter-efﬁcient: only a set of 1-\nbit binary masks needs to be saved per task after\ntraining, instead of all 32-bit ﬂoat parameters in\nﬁnetuning. This small memory footprint enables\ndeploying pretrained language models for solving\nmultiple tasks on edge devices. The compactness of\nmasking also naturally allows parameter-efﬁcient\nensembles of pretrained language models.\nOur contributions: (i) We introduce masking,\na new scheme for utilizing pretrained language\nmodels by learning selective masks for pretrained\nweights, as an efﬁcient alternative to ﬁnetuning.\n2227\nWe show that masking is applicable to models like\nBERT/RoBERTa/DistilBERT, and produces perfor-\nmance on par with ﬁnetuning. (ii) We carry out\nextensive empirical analysis of masking, shedding\nlight on factors critical for achieving good perfor-\nmance on eleven diverse NLP tasks. (iii) We study\nthe binary masked language models’ loss landscape\nand language representations, revealing potential\nreasons why masking has task performance compa-\nrable to ﬁnetuning.\n2 Related Work\nTwo-stage NLP paradigm. Pretrained language\nmodels (Peters et al., 2018; Devlin et al., 2019; Liu\net al., 2019b; Yang et al., 2019; Radford et al.,\n2019) advance NLP with contextualized repre-\nsentation of words. Finetuning a pretrained lan-\nguage model (Dai and Le, 2015; Howard and\nRuder, 2018) often delivers competitive perfor-\nmance partly because pretraining leads to a bet-\nter initialization across various downstream tasks\nthan training from scratch (Hao et al., 2019). How-\never, ﬁnetuning on individual NLP tasks is not\nparameter-efﬁcient. Each ﬁnetuned model, typi-\ncally consisting of hundreds of millions of ﬂoating\npoint parameters, needs to be saved individually.\nStickland and Murray (2019) use projected atten-\ntion layers with multi-task learning to improve efﬁ-\nciency of ﬁnetuning BERT. Houlsby et al. (2019)\ninsert adapter modules to BERT to improve mem-\nory efﬁciency. The inserted modules alter the for-\nward pass of BERT, hence need to be carefully\ninitialized to be close to identity.\nWe propose to directly pick parameters appro-\npriate to a downstream task, by learning selective\nbinary masks via end-to-end training. Keeping the\npretrained parameters untouched, we solve several\ndownstream NLP tasks with minimal overhead.\nBinary networks and network pruning. Bi-\nnary masks can be trained using the “straight-\nthrough estimator” (Bengio et al., 2013; Hinton,\n2012). Hubara et al. (2016), Rastegari et al. (2016),\nHubara et al. (2017), inter alia, apply this tech-\nnique to train efﬁcient binarized neural networks.\nWe use this estimator to train selective masks for\npretrained language model parameters.\nInvestigating the lottery ticket hypothesis (Fran-\nkle and Carbin, 2018) of network pruning (Han\net al., 2015a; He et al., 2018; Liu et al., 2019c; Lee\net al., 2019; Lin et al., 2020), Zhou et al. (2019)\nﬁnd that applying binary masks to a neural network\nis a form of training the network. Gaier and Ha\n(2019) propose to search neural architectures for re-\ninforcement learning and image classiﬁcation tasks,\nwithout any explicit weight training. This work\ninspires our masking scheme (which can be inter-\npreted as implicit neural architecture search (Liu\net al., 2019c)): applying the masks to a pretrained\nlanguage model is similar to ﬁnetuning, yet is much\nmore parameter-efﬁcient.\nPerhaps the closest work, Mallya et al. (2018)\napply binary masks to CNNs and achieve good per-\nformance in computer vision. We learn selective\nbinary masks for pretrained language models in\nNLP and shed light on factors important for ob-\ntaining good performance. Mallya et al. (2018)\nexplicitly update weights in a task-speciﬁc classi-\nﬁer layer. In contrast, we show that end-to-end\nlearning of selective masks, consistently for both\nthe pretrained language model and a randomly ini-\ntialized classiﬁer layer, achieves good performance.\nRadiya-Dixit and Wang (2020) investigate ﬁnetun-\ning of BERT by employing a number of techniques,\nincluding what they call sparsiﬁcation, a method\nsimilar to masking. Their focus is analysis of ﬁne-\ntuning BERT whereas our goal is to provide an\nefﬁcient alternative to ﬁnetuning.\n3 Method\n3.1 Background on Transformer and\nﬁnetuning\nThe encoder of the Transformer architecture\n(Vaswani et al., 2017) is ubiquitously used when\npretraining large language models. We brieﬂy re-\nview its architecture and then present our masking\nscheme. Taking BERT-base as an example, each\none of the 12 transformer blocks consists of (i)\nfour linear layers1 WK, WQ, WV, and WAO for\ncomputing and outputting the self attention among\ninput wordpieces (Wu et al., 2016). (ii) two lin-\near layers WI and WO feeding forward the word\nrepresentations to the next transformer block.\nMore concretely, consider an input sentenceX ∈\nRN×d where N is the maximum sentence length\nand dis the hidden dimension size. WK, WQ, and\nWV are used to compute transformations of X:\nK = XWK,Q = XWQ,V = XWV,\n1We omit the bias terms for brevity.\n2228\nand the self attention of X is computed as:\nAttention(K,Q,V) = softmax(QKT\n√\nd\n)V.\nThe attention is then transformed by WAO, and\nsubsequently fed forward by WI and WO to the\nnext transformer block.\nWhen ﬁnetuning on a downstream task like se-\nquence classiﬁcation, a linear classiﬁer layer WT,\nprojecting from the hidden dimension to the output\ndimension, is randomly initialized. Next, WT is\nstacked on top of a pretrained linear layerWP (the\npooler layer). All parameters are then updated to\nminimize the task loss such as cross-entropy.\n3.2 Learning the mask\nGiven a pretrained language model, we do not\nﬁnetune, i.e., we do not update the pretrained\nparameters. Instead, we select a subset of the\npretrained parameters that is critical to a down-\nstream task while discarding irrelevant ones with\nbinary masks. We associate each linear layer Wl\n∈{Wl\nK,Wl\nQ,Wl\nV,Wl\nAO,Wl\nI,Wl\nO}of the l-th\ntransformer block with a real-valued matrix Ml\nthat is randomly initialized from a uniform distri-\nbution and has the same size as Wl. We then pass\nMl through an element-wise thresholding function\n(Hubara et al., 2016; Mallya et al., 2018), i.e., a\nbinarizer, to obtain a binary mask Ml\nbin for Wl:\n(ml\nbin)i,j =\n{ 1 if ml\ni,j ≥τ\n0 otherwise , (1)\nwhere ml\ni,j ∈Ml, i,j indicate the coordinates of\nthe 2-D linear layer and τ is a global thresholding\nhyperparameter.\nIn each forward pass of training, the binary mask\nMl\nbin (derived from Ml via Eq. 1) selects weights in\na pretrained linear layer Wl by Hadamard product:\nˆWl := Wl ⊙Ml\nbin .\nIn the corresponding backward pass of training,\nwith the associated loss functionL, we cannot back-\npropagate through the binarizer, since Eq. 1 is a\nhard thresholding operation and the gradient with\nrespect to Ml is zero almost everywhere. Similar\nto the treatment2 in Bengio et al. (2013); Hubara\n2Bengio et al. (2013); Hubara et al. (2016) describe it as\nthe “straight-through estimator”, and Lin et al. (2020) provide\nconvergence guarantee with error feedback interpretation.\net al. (2016); Lin et al. (2020), we use ∂L( ˆWl)\n∂Ml\nbin\nas a\nnoisy estimator of ∂L( ˆWl)\n∂Ml to update Ml, i.e.:\nMl ←Ml −η∂L( ˆWl)\n∂Ml\nbin\n, (2)\nwhere ηrefers to the step size. Hence, the whole\nstructure can be trained end-to-end.\nWe learn a set of binary masks for an NLP task\nas follows. Recall that each linear layer Wl is\nassociated with a Ml to obtain a masked linear\nlayer ˆWl through Eq. 1. We randomly initialize an\nadditional linear layer with an associated Ml and\nstack it on top of the pretrained language model.\nWe then update each Ml through Eq. 2 with the\ntask objective during training.\nAfter training, we pass each Ml through the\nbinarizer to obtain Ml\nbin, which is then saved for\nfuture inference. Since Ml\nbin is binary, it takes only\n≈3% of the memory compared to saving the 32-\nbit ﬂoat parameters in a ﬁnetuned model. Also,\nwe will show that many layers – in particular the\nembedding layer – do not have to be masked. This\nfurther reduces memory consumption of masking.\n3.3 Conﬁguration of masking\nOur masking scheme is motivated by the obser-\nvation: the pretrained weights form a good ini-\ntialization (Hao et al., 2019), yet a few steps of\nadaptation are still needed to produce competitive\nperformance for a speciﬁc task. However, not every\npretrained parameter is necessary for achieving rea-\nsonable performance, as suggested by the ﬁeld of\nneural network pruning (LeCun et al., 1990; Has-\nsibi and Stork, 1993; Han et al., 2015b). We now\ninvestigate two conﬁguration choices that affect\nhow many parameters are “eligible” for masking.\nInitial sparsity of Ml\nbin. As we randomly initial-\nize our masks from uniform distributions, the spar-\nsity of the binary mask Ml\nbin in the mask initializa-\ntion phase controls how many pretrained parame-\nters in a layer Wl are assumed to be irrelevant to\nthe downstream task. Different initial sparsity rates\nentail different optimization behaviors.\nIt is crucial to better understand how the initial\nsparsity of a mask impacts the training dynamics\nand ﬁnal model performance, so as to generalize\nour masking scheme to broader domains and tasks.\nIn §5.1, we investigate this aspect in detail. In prac-\ntice, we ﬁx τ in Eq. 1 while adjusting the uniform\ndistribution to achieve a target initial sparsity.\n2229\nWhich layers to mask. Different layers of pre-\ntrained language models capture distinct aspects of\na language during pretraining, e.g., Tenney et al.\n(2019) ﬁnd that information on part-of-speech tag-\nging, parsing, named-entity recognition, semantic\nroles, and coreference is encoded on progressively\nhigher layers of BERT. It is hard to know a priori\nwhich types of NLP tasks have to be addressed in\nthe future, making it non-trivial to decide layers to\nmask. We study this factor in §5.2.\nWe do not learn a mask for the lowest embed-\nding layer, i.e., the uncontextualized wordpiece em-\nbeddings are completely “selected”, for all tasks.\nThe motivation is two-fold. (i) The embedding\nlayer weights take up a large part, e.g., almost 21%\n(23m/109m) in BERT-base-uncased, of the total\nnumber of parameters. Not having to learn a se-\nlective mask for this layer reduces memory con-\nsumption. (ii) Pretraining has effectively encoded\ncontext-independent general meanings of words in\nthe embedding layer (Zhao et al., 2020). Hence,\nlearning a selective mask for this layer is unnec-\nessary. Also, we do not learn masks for biases\nand layer normalization parameters as we did not\nobserve a positive effect on performance.\n4 Datasets and Setup\nDatasets. We present results for masking BERT,\nRoBERTa, and DistilBERT in part-of-speech tag-\nging, named-entity recognition, sequence classiﬁ-\ncation, and reading comprehension.\nWe experiment with part-of-speech tagging\n(POS) on Penn Treebank (Marcus et al., 1993),\nusing Collins (2002)’s train/dev/test split. For\nnamed-entity recognition (NER), we conduct ex-\nperiments on the CoNLL-2003 NER shared task\n(Tjong Kim Sang and De Meulder, 2003).\nFor sequence classiﬁcation , the following\nGLUE tasks (Wang et al., 2018) are evaluated:\nStanford Sentiment Treebank (SST2) (Socher et al.,\n2013), Microsoft Research Paraphrase Corpus\n(MRPC) (Dolan and Brockett, 2005), Corpus of\nLinguistic Acceptability (CoLA) (Warstadt et al.,\n2019), Recognizing Textual Entailment (RTE) (Da-\ngan et al., 2005), and Question Natural Language\nInference (QNLI) (Rajpurkar et al., 2016).\nIn addition, we experiment on sequence classiﬁ-\ncation datasets that have publicly available test sets:\nthe 6-class question classiﬁcation dataset TREC\n(V oorhees and Tice, 2000), the 4-class news classi-\nﬁcation dataset AG News (AG) (Zhang et al., 2015),\nand the binary Twitter sentiment classiﬁcation task\nSemEval-2016 4B (SEM) (Nakov et al., 2016).\nWe experiment with reading comprehension\non SWAG (Zellers et al., 2018) using the ofﬁcial\ndata splits. We report Matthew’s correlation coef-\nﬁcient (MCC) for CoLA, micro-F1 for NER, and\naccuracy for the other tasks.\nSetup. Due to resource limitations and in the\nspirit of environmental responsibility (Strubell\net al., 2019; Schwartz et al., 2019), we conduct\nour experiments on the base models: BERT-base-\nuncased, RoBERTa-base, and DistilBERT-base-\nuncased. Thus, the BERT/RoBERTa models we use\nhave 12 transformer blocks (0–11 indexed) produc-\ning 768-dimension vectors; the DistilBERT model\nwe use has the same dimension but contains 6 trans-\nformer blocks (0–5 indexed). We implement our\nmodels in PyTorch (Paszke et al., 2019) with the\nHuggingFace framework (Wolf et al., 2019).\nThroughout all experiments, we limit the max-\nimum length of a sentence (pair) to be 128 after\nwordpiece tokenization. Following Devlin et al.\n(2019), we use the Adam (Kingma and Ba, 2014)\noptimizer of which the learning rate is a hyperpa-\nrameter while the other parameters remain default.\nWe carefully tune the learning rate for each setup:\nthe tuning procedure ensures that the best learn-\ning rate does not lie on the border of our search\ngrid, otherwise we extend the grid accordingly. The\ninitial grid is {1e-5, 3e-5, 5e-5, 7e-5, 9e-5}.\nFor sequence classiﬁcation and reading compre-\nhension, we use [CLS] as the representation of the\nsentence (pair). Following Devlin et al. (2019), we\nformulate NER as a tagging task and use a linear\noutput layer, instead of a conditional random ﬁeld\nlayer. For POS and NER experiments, the represen-\ntation of a tokenized word is its last wordpiece (Liu\net al., 2019a; He and Choi, 2020). Note that a 128\nmaximum length of a sentence for POS and NER\nmeans that some word-tag annotations need to be\nexcluded. Appendix §A shows our reproducibil-\nity checklist containing more implementation and\npreprocessing details.\n5 Experiments\n5.1 Initial sparsity of binary masks\nWe ﬁrst investigate how initial sparsity percentage\n(i.e., fraction of zeros) of the binary mask Ml\nbin in-\nﬂuences performance of a binary masked language\nmodel on downstream tasks. We experiment on\nfour tasks, with initial sparsities in {1%, 3%, 5%,\n2230\n1 5 15 25 35 45 55 65 75 85 95\nInitial Mask Sparsity (%)\n0.0\n0.2\n0.4\n0.6\n0.8Task PerformanceTask\nRTE\nMRPC\nCoLA\nSST2\nFigure 1: Dev set performance of masking BERT when\nselecting different amounts of pretrained parameters.\n10%, 15%, 20%, . . . , 95%}. All other hyperparam-\neters are controlled: learning rate is ﬁxed to 5e-5;\nbatch size is 32 for relatively small datasets (RTE,\nMRPC, and CoLA) and 128 for SST2. Each exper-\niment is repeated four times with different random\nseeds {1, 2, 3, 4}. In this experiment, all trans-\nformer blocks, the pooler layer, and the classiﬁer\nlayer are masked.\nFigure 1 shows that masking achieves decent per-\nformance without hyperparameter search. Specif-\nically, (i) a large initial sparsity removing most\npretrained parameters, e.g., 95%, leads to bad per-\nformance for the four tasks. This is due to the\nfact that the pretrained knowledge is largely dis-\ncarded. (ii) Gradually decreasing the initial sparsity\nimproves task performance. Generally, an initial\nsparsity in 3% ∼10% yields reasonable results\nacross tasks. Large datasets like SST2 are less sen-\nsitive than small datasets like RTE. (iii) Selecting\nalmost all pretrained parameters, e.g., 1% sparsity,\nhurts task performance. Recall that a pretrained\nmodel needs to be adapted to a downstream task;\nmasking achieves adaptation by learning selective\nmasks – preserving too many pretrained parameters\nin initialization impedes the optimization.\n5.2 Layer-wise behaviors\nNeural network layers present heterogeneous char-\nacteristics (Zhang et al., 2019) when being applied\nto tasks. For example, syntactic information is\nbetter represented at lower layers while semantic\ninformation is captured at higher layers in ELMo\n(Peters et al., 2018). As a result, simply masking\nall transformer blocks (as in §5.1) may not be ideal.\nWe investigate the task performance when apply-\ning the masks to different BERT layers. Figure 2\npresents the optimal task performance when mask-\ning only a subset of BERT’s transformer blocks on\nMRPC, CoLA, and RTE. Different amounts and\nindices of transformer blocks are masked: “bottom-\nup” and “top-down” indicate to mask the targeted\namount of transformer blocks, either from bottom\nor top of BERT.\nWe can observe that (i) in most cases, top-down\nmasking outperforms bottom-up masking when ini-\ntial sparsity and the number of masked layers are\nﬁxed. Thus, it is reasonable to select all pretrained\nweights in lower layers, since they capture gen-\neral information helpful and transferable to various\ntasks (Liu et al., 2019a; Howard and Ruder, 2018).\n(ii) For bottom-up masking, increasing the number\nof masked layers gradually improves performance.\nThis observation illustrates dependencies between\nBERT layers and the learning dynamics of masking:\nprovided with selected pretrained weights in lower\nlayers, higher layers need to be given ﬂexibility to\nselect pretrained weights accordingly to achieve\ngood task performance. (iii) In top-down mask-\ning, CoLA performance increases when masking a\ngrowing number of layers while MRPC and RTE\nare not sensitive. Recall that CoLA tests linguistic\nacceptability that typically requires both syntactic\nand semantic information3. All of BERT layers are\ninvolved in representing this information, hence\nallowing more layers to change should improve\nperformance.\n5.3 Comparing ﬁnetuning and masking\nWe have investigated two factors – initial sparsity\n(§5.1) and layer-wise behaviors (§5.2) – that are\nimportant in masking pretrained language models.\nHere, we compare the performance and memory\nconsumption of masking and ﬁnetuning.\nBased on observations in §5.1 and §5.2, we\nuse 5% initial sparsity when applying masking to\nBERT, RoBERTa, and DistilBERT. We mask the\ntransformer blocks 2–11 in BERT/RoBERTa and 2–\n5 in DistilBERT.WP and WT are always masked.\nNote that this global setup is surely suboptimal for\nsome model-task combinations, but our goal is to\nillustrate the effectiveness and the generalization\nability of masking. Hence, conducting extensive\nhyperparameter search is unnecessary.\nFor AG and QNLI, we use batch size 128. For\nthe other tasks we use batch size 32. We search the\noptimal learning rate per task as described in §4,\n3For example, to distinguish acceptable caused-motion\nconstructions (e.g., “the professor talked us into a stupor”)\nfrom inacceptable ones (e.g., “water talked it into red”), both\nsyntactic and semantic information need to be considered\n(Goldberg, 1995).\n2231\n4 6 8 10\n# of masked blocks\n80\n85\nAccuracy\nMasking (bottom-up), 5% sparsity\nMasking (top-down), 5% sparsity\nMasking (bottom-up), 15% sparsity\nMasking (top-down), 15% sparsity\nFine-tuning\n4 6 8 10\n# of masked blocks\n52.5\n55.0\n57.5\nMCC\nMasking (bottom-up), 5% sparsity\nMasking (top-down), 5% sparsity\nMasking (bottom-up), 15% sparsity\nMasking (top-down), 15% sparsity\nFine-tuning\n4 6 8 10\n# of masked blocks\n60\n70\nAccuracy\nMasking (bottom-up), 5% sparsity\nMasking (top-down), 5% sparsity\nMasking (bottom-up), 15% sparsity\nMasking (top-down), 15% sparsity\nFine-tuning\nFigure 2: The impact of masking different transformer blocks of BERT for MRPC (left), CoLA (middle), and\nRTE (right). The number of masked blocks is shown on the x-axis; that number is either masked “bottom-up” or\n“top-down”. More precisely, a bottom-up setup (red) masking 4 blocks means we mask the transformer blocks\n{0,1,2,3}; a top-down setup (blue) masking 4 blocks means we mask the transformer blocks {8,9,10,11}. WP\nand WT are always masked.\nMRPC SST2 CoLA RTE QNLI SEM TREC AG POS NER SW AG\n3.5k 67k 8.5k 2.5k 108k 4.3k 4.9k 96k 38k 15k 113k\nBERT Finetuning86.1±0.8 93.3±0.2 59.6±0.8 69.2±2.7 91.0±0.6 86.6±0.3 96.4±0.2 94.4±0.1 97.7±0.0 94.6±0.2 80.9±1.7\nMasking86.8±1.1 93.2±0.5 59.5±0.1 69.5±3.0 91.3±0.4 85.9±0.5 96.0±0.4 94.2±0.0 97.7±0.0 94.5±0.1 80.3±0.1\nRoBERTaFinetuning89.8±0.5 95.0±0.3 62.1±1.7 78.2±1.1 92.9±0.2 90.2±0.5 96.2±0.4 94.7±0.0 98.1±0.0 94.9±0.1 83.4±0.8\nMasking88.5±1.1 94.5±0.3 60.3±1.3 69.2±2.1 92.4±0.1 90.1±0.1 95.9±0.5 94.5±0.1 98.0±0.0 93.9±0.1 82.1±0.2\nDistilBERTFinetuning85.4±0.5 91.6±0.4 55.1±0.3 62.2±3.0 89.0±0.8 85.9±0.2 95.7±0.6 94.2±0.1 97.6±0.0 94.1±0.1 72.5±0.2\nMasking86.0±0.3 91.3±0.3 53.1±0.7 61.6±1.5 89.2±0.2 86.6±0.6 95.9±0.6 94.2±0.1 97.6±0.0 94.1±0.2 71.0±0.0\nTable 1: Dev set task performances (%) of masking and ﬁnetuning. Each experiment is repeated four times with\ndifferent random seeds and we report mean and standard deviation. Numbers below dataset name (second row) are\nthe size of training set. For POS and NER, we report the number of sentences.\nand they are shown in Appendix §A.4.\nPerformance comparison. Table 1 reports per-\nformance of masking and ﬁnetuning on the dev\nset for the eleven NLP tasks. We observe that ap-\nplying masking to BERT/RoBERTa/DistilBERT\nyields performance comparable to ﬁnetuning. We\nobserve a performance drop 4 on RoBERTa-RTE.\nRTE has the smallest dataset size (train: 2.5k; dev:\n0.3k) among all tasks – this may contribute to the\nimperfect results and large variances.\nOur BERT-NER results are slightly worse than\nDevlin et al. (2019). This may be due to the fact\nthat “maximal document context” is used by Devlin\net al. (2019) while we use sentence-level context\nof 128 maximum sequence length5.\nRows “Single” in Table 2 compare performance\nof masking and ﬁnetuning BERT on the test set of\nSEM, TREC, AG, POS, and NER. The same setup\nand hyperparameter searching as Table 1 are used,\nthe best hyperparameters are picked on the dev set.\nResults from Sun et al. (2019); Palogiannidi et al.\n(2016) are included as a reference. Sun et al. (2019)\n4Similar observations were made: DistilBERT has a 10%\naccuracy drop on RTE compared to BERT-base (Sanh et al.,\n2019); Sajjad et al. (2020) report unstableness on MRPC and\nRTE when applying their model reduction strategies.\n5Similar observations were made: https://github.\ncom/huggingface/transformers/issues/64\nMRPCSST2CoLARTEQNLISEMTREC\nAGPOSNERSWAG\nTask\n250\n500\n750\n1000\n1250\n# of Parameters (million)\nFinetuning\nMasking\n(a) Number of parameters.\nMRPCSST2CoLARTEQNLISEMTREC\nAGPOSNERSWAG\nTask\n1000\n2000\n3000\n4000\n5000\nDevice Storage (MB)\nFinetuning\nMasking (b) Memory consumption.\nFigure 3: The accumulated number of parameters and\nmemory required by ﬁnetuning and masking to solve\nan increasing number of tasks.\nemploy optimizations like layer-wise learning rate,\nproducing slightly better performance than ours.\nPalogiannidi et al. (2016) is the best performing\nsystem on task SEM (Nakov et al., 2016). Again,\nmasking yields results comparable to ﬁnetuning.\nMemory comparison. Having shown that task\nperformance of masking and ﬁnetuning is compa-\nrable, we next demonstrate one key strength of\nmasking: memory efﬁciency. We take BERT-base-\nuncased as our example. Figure 3 shows the ac-\ncumulated number of parameters in million and\nmemory in megabytes (MB) required when an in-\ncreasing number of downstream tasks need to be\nsolved using ﬁnetuning and masking. Masking re-\n2232\nSEM TREC AG POS NERMemory (MB)\nMaskingSingle 12.03 3.30 5.62 2.34 9.85 447\nEnsem. 11.52 3.20 5.28 2.12 9.19 474\nFinetun. Single 11.87 3.80 5.66 2.34 9.85 438\nEnsem. 11.73 2.80 5.17 2.29 9.231752\nSun et al. (2019)n/a 2.80 5.25 n/a n/a n/a\nPalogiannidi et al. (2016)13.80 n/a n/a n/a n/a n/a\nTable 2: Error rate (%) on test set and model size com-\nparison. Single: the averaged performance of four mod-\nels with different random seeds. Ensem.: ensemble of\nthe four models.\nquires a small overhead when solving a single task\nbut is much more efﬁcient than ﬁnetuning when\nseveral tasks need to be inferred. Masking saves a\nsingle copy of a pretrained language model contain-\ning 32-bit ﬂoat parameters for all the eleven tasks\nand a set of 1-bit binary masks for each task. In\ncontrast, ﬁnetuning saves every ﬁnetuned model so\nthe memory consumption grows linearly.\nMasking naturally allows light ensembles of\nmodels. Rows “Ensem.” in Table 2 compare ensem-\nbled results and model size. We consider the en-\nsemble of predicted (i) labels; (ii) logits; (iii) proba-\nbilities. The best ensemble method is picked on dev\nand then evaluated on test. Masking only consumes\n474MB of memory – much smaller than 1752MB\nrequired by ﬁnetuning – and achieves comparable\nperformance. Thus, masking is also much more\nmemory-efﬁcient than ﬁnetuning in an ensemble\nsetting.\n6 Discussion\n6.1 Intrinsic evaluations\n§5 demonstrates that masking is an efﬁcient alter-\nnative to ﬁnetuning. Now we analyze properties\nof the representations computed by binary masked\nlanguage models with intrinsic evaluation.\nOne intriguing property of ﬁnetuning, i.e., stack-\ning a classiﬁer layer on top of a pretrained language\nmodel then update all parameters, is that a linear\nclassiﬁer layer sufﬁces to conduct reasonably ac-\ncurate classiﬁcation. This observation implies that\nthe conﬁguration of data points, e.g., sentences\nwith positive or negative sentiment in SST2, should\nbe close to linearly separable in the hidden space.\nLike ﬁnetuning, masking also uses a linear classi-\nﬁer layer. Hence, we hypothesize that upper layers\nin binary masked language models, even without\nexplicit weight updating, also create a hidden space\nin which data points are close to linearly separable.\nFigure 4 uses t-SNE (Maaten and Hinton, 2008)\n20\n 0 20\n40\n20\n0\n20\n40\nBERT-SST2\nGold Labels\nPositive\nNegative\n40\n 20\n 0 20\n20\n0\n20\n40\nBERT-SST2\nGold Labels\nPositive\nNegative\n20\n 10\n 0 10 20\n20\n10\n0\n10\n20\nROBERTA-SST2\nGold Labels\nPositive\nNegative\n10\n 0 10\n60\n40\n20\n0\n20\n40\nROBERTA-SST2\nGold Labels\nPositive\nNegative\nFigure 4: t-SNE visualization of the representation of\n[CLS] computed by the topmost transformer block in\npretrained (left), ﬁnetuned (top right), and masked (bot-\ntom right) BERT/RoBERTa. We usescikit-learn\n(Pedregosa et al., 2011) and default t-SNE parameters.\nSST2 SEM\nSST2 41.8 -13.4\nSEM 20.0 11.5\n(a) Masking\nSST2 SEM\nSST2 41.8 -10.1\nSEM 18.9 12.2\n(b) Finetuning\nTable 3: Generalization on dev (%) of binary masked\nand ﬁnetuned BERT. Row: training dataset; Column:\nevaluating dataset. Numbers are improvements against\nthe majority-vote baseline: 50.9 for SST2 and 74.4 for\nSEM. Results are averaged across four random seeds.\nto visualize the representation of [CLS] computed\nby the topmost transformer block in pretrained,\nﬁnetuned, and masked BERT/RoBERTa, using the\ndev set examples of SST2. The pretrained mod-\nels’ representations (left) are clearly not separable\nsince the model needs to be adapted to downstream\ntasks. The sentence representations computed by\nthe ﬁnetuned (top right) and the binary masked\n(bottom right) encoder are almost linearly separa-\nble and consistent with the gold labels. Thus, a lin-\near classiﬁer is expected to yield reasonably good\nclassiﬁcation accuracy. This intrinsic evaluation\nillustrates that binary masked models extract good\nrepresentations from the data for the downstream\nNLP task.\n6.2 Properties of the binary masked models\nDo binary masked models generalize? Fig-\nure 4 shows that a binary masked language model\nproduces proper representations for the classiﬁer\nlayer and hence performs as well as a ﬁnetuned\nmodel. Here, we are interested in verifying that\n2233\nFigure 5: Scores s of two sets of masks, trained with\ntwo different tasks, of layer WO in transformer blocks\n2 (left) and 11 (right) in BERT. A large s means that\nthe two masks are dissimilar.\nthe binary masked model does indeed solve down-\nstream tasks by learning meaningful representa-\ntions – instead of exploiting spurious correlations\nthat generalize poorly (Niven and Kao, 2019; Mc-\nCoy et al., 2019). To this end, we test if the binary\nmasked mode is generalizable to other datasets of\nthe same type of downstream task. We use the two\nsentiment classiﬁcation datasets: SST2 and SEM.\nWe simply evaluate the model masked or ﬁnetuned\non SST2 against the dev set of SEM and vice versa.\nTable 3 reports the results against the majority-vote\nbaseline. The ﬁnetuned and binary masked models\nof SEM generalize well on SST2, showing ≈20%\nimprovement against the majority-vote baseline.\nOn the other hand, we observe that the knowl-\nedge learned on SST2 does not generalize to SEM,\nfor both ﬁnetuning and masking. We hypothesize\nthat this is because the Twitter domain (SEM) is\nmuch more speciﬁc than movie reviews (SST2).\nFor example, some Emojis or symbols like “:)” re-\nﬂecting strong sentiment do not occur in SST2, re-\nsulting in unsuccessful generalization. To test our\nhypothesis, we take another movie review dataset\nIMDB (Maas et al., 2011), and directly apply the\nSST2-ﬁnetuned- and SST2-binary-masked- mod-\nels on it. Masking and ﬁnetuning achieve accuracy\n84.79% and 85.25%, which are comparable and\nboth outperform the baseline 50%, demonstrating\nsuccessful knowledge transfer.\nThus, ﬁnetuning and masking yield models with\nsimilar generalization ability. The binary masked\nmodels indeed create representations that contain\nvalid information for downstream tasks.\nAnalyzing masks. We study the dissimilarity be-\ntween masks learned by different BERT layers and\ndownstream tasks. For the initial and trained binary\nmasks Mt,init\nbin and Mt,trained\nbin of a layer trained on\ntask t∈{t1,t2}. We compute:\ns=\nMt1,trained\nbin −Mt2,trained\nbin\n\n1Mt1,trained\nbin −Mt1,init\nbin\n\n1\n+\nMt2,trained\nbin −Mt2,init\nbin\n\n1\n,\nwhere ∥W∥1 = ∑m\ni=1\n∑n\nj=1 |wi,j|. Note that for\nthe same random seed, Mt1,init\nbin and Mt2,init\nbin are\nthe same. The dissimilarity smeasures the differ-\nence between two masks as a fraction of all changes\nbrought about by training. Figure 5 shows that, af-\nter training, the dissimilarities of masks of higher\nBERT layers are larger than those of lower BERT\nlayers. Similar observations are made for ﬁnetun-\ning: top layer weights in ﬁnetuned BERT are more\ntask-speciﬁc (Kovaleva et al., 2019). The ﬁgure\nalso shows that the learned masks for downstream\ntasks tend to be dissimilar to each other, even for\nsimilar tasks. For a given task, there exist differ-\nent sets of masks (initialized with different random\nseeds) yielding similar performance. This observa-\ntion is similar to the results of evaluating the lottery\nticket hypothesis on BERT (Prasanna et al., 2020;\nChen et al., 2020): a number of subnetworks exist\nin BERT achieving similar task performance.\n6.3 Loss landscape\nTraining complex neural networks can be viewed\nas searching for good minima in the highly non-\nconvex landscape deﬁned by the loss function (Li\net al., 2018). Good minima are typically depicted\nas points at the bottom of different locally convex\nvalleys (Keskar et al., 2016; Draxler et al., 2018),\nachieving similar performance. In this section, we\nstudy the relationship between the two minima ob-\ntained by masking and ﬁnetuning.\nRecent work analyzing the loss landscape sug-\ngests that the local minima in the loss landscape\nreached by standard training algorithms can be con-\nnected by a simple path (Garipov et al., 2018; Got-\nmare et al., 2018), e.g., a Bézier curve, with low\ntask loss (or high task accuracy) along the path. We\nare interested in testing if the two minima found by\nﬁnetuning and masking can be easily connected in\nthe loss landscape. To start with, we verify the task\nperformance of an interpolated model W(γ) on\nthe line segment between a ﬁnetuned model W0\nand a binary masked model W1:\nW(γ) =W0 + γ(W1 −W0),0 ≤γ ≤1 .\nWe conduct experiments on MRPC and SST2\nwith the best-performing BERT and RoBERTa\n2234\n0.0 0.2 0.4 0.6 0.8 1.0\n0.84\n0.86\n0.88\n0.90\nAccuracy\nBERT RoBERTa\n0.0 0.2 0.4 0.6 0.8 1.0\n0.920\n0.925\n0.930\n0.935\n0.940\n0.945\n0.950\nAccuracy\nBERT RoBERTa\n0.0 0.2 0.4 0.6 0.8 1.0\n0.70\n0.75\n0.80\n0.85\nAccuracy\nBERT, finetuning\nBERT, masking\n0.0 0.2 0.4 0.6 0.8 1.0\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nBERT, finetuning\nBERT, masking\nFigure 6: Mode connectivity results on MRPC (left)\nand SST2 (right). Top images: dev set accuracy of an\ninterpolated model between the two minima found by\nﬁnetuning (γ=0) and masking (γ=1). Bottom images:\naccuracy of an interpolated model between pretrained\n(γ=0) and ﬁnetuned/masked (γ=1) BERT.\nmodels obtained in Table 1 (same seed and training\nepochs); Figure 6 (top) shows the results of mode\nconnectivity, i.e., the evolution of the task accuracy\nalong a line connecting the two candidate minima.\nSurprisingly, the interpolated models on the\nline segment connecting a ﬁnetuned and a binary\nmasked model form a high accuracy path, indicat-\ning the extremely well-connected loss landscape.\nThus, masking ﬁnds minima on the same connected\nlow-loss manifold as ﬁnetuning, conﬁrming the ef-\nfectiveness of our method. Also, we show in Fig-\nure 6 (bottom) for the line segment between the\npretrained BERT and a ﬁnetuned/masked BERT,\nthat mode connectivity is not solely due to an over-\nparameterized pretrained language model. Bézier\ncurves experiments show similar results, cf. Ap-\npendix §B.\n7 Conclusion\nWe have presented masking, an efﬁcient alternative\nto ﬁnetuning for utilizing pretrained language mod-\nels like BERT/RoBERTa/DistilBERT. Instead of\nupdating the pretrained parameters, we only train\none set of binary masks per task to select criti-\ncal parameters. Extensive experiments show that\nmasking yields performance comparable to ﬁne-\ntuning on a series of NLP tasks. Leaving the pre-\ntrained parameters unchanged, masking is much\nmore memory efﬁcient when several tasks need\nto be solved. Intrinsic evaluations show that bi-\nnary masked models extract valid and generaliz-\nable representations for downstream tasks. More-\nover, we demonstrate that the minima obtained by\nﬁnetuning and masking can be easily connected\nby a line segment, conﬁrming the effectiveness of\napplying masking to pretrained language models.\nOur code is available at: https://github.com/\nptlmasking/maskbert.\nFuture work may explore the possibility of ap-\nplying masking to the pretrained multilingual en-\ncoders like mBERT (Devlin et al., 2019) and XLM\n(Conneau and Lample, 2019). Also, the binary\nmasks learned by our method have low sparsity\nsuch that inference speed is not improved. De-\nveloping methods improving both memory and in-\nference efﬁciency without sacriﬁcing task perfor-\nmance can open the possibility of widely deploying\nthe powerful pretrained language models to more\nNLP applications.\nAcknowledgments\nWe thank the anonymous reviewers for the insight-\nful comments and suggestions. This work was\nfunded by the European Research Council (ERC\n#740516), SNSF grant 200021_175796, as well as\na Google Focused Research Award.\nReferences\nYoshua Bengio, Nicholas Léonard, and Aaron\nCourville. 2013. Estimating or propagating gradi-\nents through stochastic neurons for conditional com-\nputation.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Si-\njia Liu, Yang Zhang, Zhangyang Wang, and\nMichael Carbin. 2020. The lottery ticket hypoth-\nesis for pre-trained bert networks. arXiv preprint\narXiv:2007.12223.\nMichael Collins. 2002. Discriminative training meth-\nods for hidden Markov models: Theory and ex-\nperiments with perceptron algorithms. In Proceed-\nings of the 2002 Conference on Empirical Methods\nin Natural Language Processing (EMNLP 2002) ,\npages 1–8. Association for Computational Linguis-\ntics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems , pages\n7059–7069.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop, pages 177–190. Springer.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In C. Cortes, N. D. Lawrence,\nD. D. Lee, M. Sugiyama, and R. Garnett, editors,\n2235\nAdvances in Neural Information Processing Systems\n28, pages 3079–3087. Curran Associates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nFelix Draxler, Kambis Veschgini, Manfred Salmhofer,\nand Fred A Hamprecht. 2018. Essentially no bar-\nriers in neural network energy landscape. arXiv\npreprint arXiv:1803.00885.\nJonathan Frankle and Michael Carbin. 2018. The lot-\ntery ticket hypothesis: Finding sparse, trainable neu-\nral networks.\nAdam Gaier and David Ha. 2019. Weight agnostic neu-\nral networks. In Advances in Neural Information\nProcessing Systems, pages 5365–5379.\nTimur Garipov, Pavel Izmailov, Dmitrii Podoprikhin,\nDmitry P Vetrov, and Andrew G Wilson. 2018. Loss\nsurfaces, mode connectivity, and fast ensembling of\ndnns. In Advances in Neural Information Process-\ning Systems, pages 8789–8798.\nAdele E Goldberg. 1995. Construction grammar. Wi-\nley.\nAkhilesh Gotmare, Nitish Shirish Keskar, Caiming\nXiong, and Richard Socher. 2018. A closer\nlook at deep learning heuristics: Learning rate\nrestarts, warmup and distillation. arXiv preprint\narXiv:1810.13243.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015a. Learning both weights and connections for\nefﬁcient neural network. In NeurIPS - Advances\nin Neural Information Processing Systems , pages\n1135–1143.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015b. Learning both weights and connections\nfor efﬁcient neural network. In C. Cortes, N. D.\nLawrence, D. D. Lee, M. Sugiyama, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 28, pages 1135–1143. Curran Asso-\nciates, Inc.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 4143–\n4152, Hong Kong, China. Association for Computa-\ntional Linguistics.\nBabak Hassibi and David G. Stork. 1993. Second order\nderivatives for network pruning: Optimal brain sur-\ngeon. In S. J. Hanson, J. D. Cowan, and C. L. Giles,\neditors, Advances in Neural Information Processing\nSystems 5, pages 164–171. Morgan-Kaufmann.\nHan He and Jinho D. Choi. 2020. Establishing Strong\nBaselines for the New Decade: Sequence Tagging,\nSyntactic and Semantic Parsing with BERT. In\nProceedings of the 33rd International Florida Ar-\ntiﬁcial Intelligence Research Society Conference ,\nFLAIRS’20. Best Paper Candidate.\nYang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu,\nand Yi Yang. 2018. Soft ﬁlter pruning for acceler-\nating deep convolutional neural networks. In Inter-\nnational Joint Conference on Artiﬁcial Intelligence\n(IJCAI), pages 2234–2240.\nGeoffrey Hinton. 2012. Neural networks for machine\nlearning.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799,\nLong Beach, California, USA. PMLR.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. 2016. Bina-\nrized neural networks. In D. D. Lee, M. Sugiyama,\nU. V . Luxburg, I. Guyon, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n29, pages 4107–4115. Curran Associates, Inc.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. 2017. Quantized\nneural networks: Training neural networks with low\nprecision weights and activations. The Journal of\nMachine Learning Research, 18(1):6869–6898.\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge No-\ncedal, Mikhail Smelyanskiy, and Ping Tak Peter\nTang. 2016. On large-batch training for deep learn-\ning: Generalization gap and sharp minima. arXiv\npreprint arXiv:1609.04836.\n2236\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4356–4365, Hong Kong, China. Association for\nComputational Linguistics.\nYann LeCun, John S. Denker, and Sara A. Solla. 1990.\nOptimal brain damage. In D. S. Touretzky, editor,\nAdvances in Neural Information Processing Systems\n2, pages 598–605. Morgan-Kaufmann.\nNamhoon Lee, Thalaiyasingam Ajanthan, and\nPhilip HS Torr. 2019. SNIP: Single-shot net-\nwork pruning based on connection sensitivity. In\nICLR - International Conference on Learning\nRepresentations.\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and\nTom Goldstein. 2018. Visualizing the loss landscape\nof neural nets. In Advances in Neural Information\nProcessing Systems, pages 6389–6399.\nTao Lin, Sebastian U. Stich, Luis Barba, Daniil\nDmitriev, and Martin Jaggi. 2020. Dynamic model\npruning with feedback. In International Conference\non Learning Representations.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nZhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang,\nand Trevor Darrell. 2019c. Rethinking the value of\nnetwork pruning. In ICLR - International Confer-\nence on Learning Representations.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(Nov):2579–2605.\nArun Mallya, Dillon Davis, and Svetlana Lazebnik.\n2018. Piggyback: Adapting a single network to mul-\ntiple tasks by learning to mask weights. In The Eu-\nropean Conference on Computer Vision (ECCV).\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional Linguistics, 19(2):313–330.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nPreslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio\nSebastiani, and Veselin Stoyanov. 2016. SemEval-\n2016 task 4: Sentiment analysis in twitter. In\nProceedings of the 10th International Workshop on\nSemantic Evaluation (SemEval-2016) , pages 1–18,\nSan Diego, California. Association for Computa-\ntional Linguistics.\nTimothy Niven and Hung-Yu Kao. 2019. Probing neu-\nral network comprehension of natural language ar-\nguments. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 4658–4664, Florence, Italy. Association\nfor Computational Linguistics.\nElisavet Palogiannidi, Athanasia Kolovou, Fenia\nChristopoulou, Filippos Kokkinos, Elias Iosif, Niko-\nlaos Malandrakis, Haris Papageorgiou, Shrikanth\nNarayanan, and Alexandros Potamianos. 2016.\nTweester at SemEval-2016 task 4: Sentiment anal-\nysis in twitter using semantic-affective model adap-\ntation. In Proceedings of the 10th International\nWorkshop on Semantic Evaluation (SemEval-2016) ,\npages 155–163, San Diego, California. Association\nfor Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Jun-\njie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research ,\n12:2825–2830.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\n2237\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nMatthew E. Peters, Sebastian Ruder, and Noah A.\nSmith. 2019. To tune or not to tune? adapting pre-\ntrained representations to diverse tasks. In Proceed-\nings of the 4th Workshop on Representation Learn-\ning for NLP (RepL4NLP-2019) , pages 7–14, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nSai Prasanna, Anna Rogers, and Anna Rumshisky.\n2020. When BERT plays the lottery, all tickets are\nwinning.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nEvani Radiya-Dixit and Xin Wang. 2020. How ﬁne\ncan ﬁne-tuning be? learning efﬁcient language mod-\nels. volume 108 of Proceedings of Machine Learn-\ning Research, pages 2435–2443, Online. PMLR.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nMohammad Rastegari, Vicente Ordonez, Joseph Red-\nmon, and Ali Farhadi. 2016. Xnor-net: Imagenet\nclassiﬁcation using binary convolutional neural net-\nworks. In European conference on computer vision,\npages 525–542. Springer.\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and\nPreslav Nakov. 2020. Poor man’s bert: Smaller\nand faster transformer models. arXiv preprint\narXiv:2004.03844.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren\nEtzioni. 2019. Green ai.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nAsa Cooper Stickland and Iain Murray. 2019. BERT\nand PALs: Projected attention layers for efﬁcient\nadaptation in multi-task learning. In Proceedings\nof the 36th International Conference on Machine\nLearning, volume 97 of Proceedings of Machine\nLearning Research, pages 5986–5995, Long Beach,\nCalifornia, USA. PMLR.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune bert for text classiﬁcation?\nIn China National Conference on Chinese Computa-\ntional Linguistics, pages 194–206. Springer.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142–147.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nEllen V oorhees and Dawn Tice. 2000. The trec-8 ques-\ntion answering track evaluation. Proceedings of the\n8th Text Retrieval Conference.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, pages 3261–3275.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\n2238\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. SW AG: A large-scale adversar-\nial dataset for grounded commonsense inference. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages 93–\n104, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nChiyuan Zhang, Samy Bengio, and Yoram Singer.\n2019. Are all layers created equal? arXiv preprint\narXiv:1902.01996.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In C. Cortes, N. D. Lawrence, D. D. Lee,\nM. Sugiyama, and R. Garnett, editors, Advances in\nNeural Information Processing Systems 28 , pages\n649–657. Curran Associates, Inc.\nMengjie Zhao, Philipp Dufter, Yadollah\nYaghoobzadeh, and Hinrich Schütze. 2020. Quanti-\nfying the contextualization of word representations\nwith semantic class probing. In Findings of\nEMNLP.\nHattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosin-\nski. 2019. Deconstructing lottery tickets: Zeros,\nsigns, and the supermask. In Advances in Neural\nInformation Processing Systems, pages 3592–3602.\n2239\nA Reproducibility Checklist\nA.1 Computing infrastructure\nAll experiments are conducted on following GPU\nmodels: Tesla V100, GeForce GTX 1080 Ti, and\nGeForce GTX 1080. We use per-GPU batch size\n32. Thus, experiments comparing masking and\nﬁnetuning on QNLI and AG take 4 GPUs and all\nthe other tasks use a single GPU.\nA.2 Number of parameters\nIn §5.3 we thoroughly compare the number of pa-\nrameters and memory consumption of ﬁnetuning\nand masking. Numerical values are in Table 8.\nA.3 Validation performance\nThe dev set performance of Table 2 is covered in\nTable 1. We report Matthew’s correlation coefﬁ-\ncient (MCC) for CoLA, micro-F1 for NER, and\naccuracy for the other tasks. We use the evalu-\nation functions in scikit-learn (Pedregosa\net al., 2011) and seqeval (https://github.\ncom/chakki-works/seqeval).\nA.4 Hyperparameter search\nThe only hyperparameter we searched is learning\nrate, for both masking and ﬁnetuning, according to\nthe setup discussion in §4. The optimal values are\nin Table 4.\nA.5 Datasets\nFor GLUE tasks, we use the ofﬁcial datasets\nfrom the benchmark https://gluebenchmark.\ncom/. For TREC and AG, we download the\ndatasets developed by Zhang et al. (2015), which\nare available at here. Note that this link is pro-\nvided by Zhang et al. (2015) and also used by\nSun et al. (2019). For SEM, we obtain the\ndataset from the ofﬁcial SemEval website:http://\nalt.qcri.org/semeval2016/task4/. For NER,\nwe use the ofﬁcial dataset: https://www.clips.\nuantwerpen.be/conll2003/ner/. We obtain\nour POS dataset from the linguistic data con-\nsortium (LDC). We use the ofﬁcial dataset of\nSWAG (Zellers et al., 2018): https://github.\ncom/rowanz/swagaf/tree/master/data.\nFor POS, sections 0-18 of WSJ are train, sections\n19-21 are dev, and sections 22-24 are test (Collins,\n2002). We use the ofﬁcial train/dev/test splits of all\nthe other datasets.\nTo preprocess the datasets, we use the tokenizers\nprovided by the Transformers package (Wolf\net al., 2019) to convert the raw dataset to the\nformats required by BERT/RoBERTa/DistilBERT.\nSince wordpiece tokenization is used, there is no\nout-of-vocabulary words.\nSince we use a maximum sequence length of\n128, our preprocessing steps exclude some word-\ntag annotations in POS and NER. For POS, after\nwordpiece tokenization, we see 1 sentence in dev\nand 2 sentences in test have more than 126 (the\n[CLS] and [SEP] need to be considered) word-\npieces. As a result, we exclude 5 annotated words\nin dev and 87 annotated words in test. Similarly,\nfor NER (which is also formulated as a tagging task\nfollowing Devlin et al. (2019)), we see 3 sentences\nin dev and 1 sentence in test have more than 126\nwordpieces. As a result, we exclude 27 annotated\nwords in dev and 8 annotated words in test.\nThe number of examples in dev and test per task\nis shown in following Table 5.\nB More on Mode Connectivity\nFollowing the mode connectivity framework pro-\nposed in Garipov et al. (2018), we parameter-\nize the path joining two minima using a Bézier\ncurve. Let w0 and wn+1 be the parameters of\nthe models trained from ﬁnetuning and masking.\nThen, an n-bend Bézier curve connecting w0 and\nwn+1, with ntrainable intermediate models θ =\n{w1,..., wn}, can be represented by φθ(t), such\nthat φθ(0) =w0 and φθ(1) =wn+1, and\nφθ(t) =\nn+1∑\ni=0\n(n+ 1\ni\n)\n(1 −t)n+1−itiwi.\nWe train a 3-bend Bézier curve by minimizing\nthe loss Et∼U[0,1]L(φθ(t)), where U[0,1] is the\nuniform distribution in the interval [0,1]. Monte\nCarlo method is used to estimate the gradient of\nthis expectation-based function and gradient-based\noptimization is used for the minimization. The re-\nsults are illustrated in Figure 7. Masking implicitly\nperforms gradient descent, analogy to the weights\nupdate achieved by ﬁnetuning; the observations\ncomplement our arguments in the main text.\nC More Empirical Results\nEnsemble results of RoBERTa and DistilBERT.\nFollowing Table 6 shows the single and ensemble\nresults of RoBERTa and DistilBERT on the test set\nof SEM, TREC, AG, POS, and NER.\n2240\nMRPC SST2 CoLA RTE QNLI POS NER SW AG SEM TREC AG\nBERT Finetuning 5e-5 1e-5 3e-5 5e-5 3e-5 3e-5 3e-5 7e-5 1e-5 3e-5 3e-5\nMasking 1e-3 5e-4 9e-4 1e-3 7e-4 5e-4 7e-4 1e-4 7e-5 1e-4 5e-4\nRoBERTaFinetuning 3e-5 1e-5 1e-5 7e-6 1e-5 9e-6 3e-5 1e-5 7e-6 9e-6 3e-5\nMasking 3e-4 9e-5 3e-4 3e-4 1e-4 3e-4 3e-4 1e-4 3e-4 5e-4 5e-4\nDistilBERTFinetuning 3e-5 7e-5 3e-5 3e-5 3e-5 3e-5 1e-5 7e-6 1e-5 3e-5 3e-5\nMasking 9e-4 7e-4 9e-4 9e-4 1e-3 7e-4 7e-4 3e-4 3e-4 9e-4 1e-3\nTable 4: The optimal learning rate on different tasks for BERT/RoBERTa/DistilBERT. We perform ﬁnetun-\ning/masking on all tasks for 10 epochs with early stopping of 2 epochs.\n0.0 0.2 0.4 0.6 0.8 1.0\n0.84\n0.85\n0.86\n0.87\nAccuracy\nBezier curves Linear segment\n(a) BERT\n0.0 0.2 0.4 0.6 0.8 1.0\n0.88\n0.89\n0.90\n0.91\nAccuracy\nBezier curves Linear segment (b) RoBERTa\nFigure 7: The accuracy on MRPC dev set, as a function of the point on the curves φθ(γ), connecting the two\nminima found by ﬁnetuning (left, γ=0) and masking (right, γ=1).\nDev Test\nMRPC 408 n/a\nSST2 872 n/a\nCoLA 1,042 n/a\nRTE 277 n/a\nQNLI 5,732 n/a\nSEM 1,325 10,551\nTREC 548 500\nAG 24,000 7,600\nPOS 135,105 133,082\nNER 51,341 46,425\nSW AG 20,006 n/a\nTable 5: Number of examples in dev and test per task.\nFor POS and NER, we report the number of words.\nD Numerical Values of Plots\nD.1 Layer-wise behaviors\nTable 7 details the numerical values of Figure 2.\nSEM TREC AG POS NER\nRoBERTa\nMaskingSingle11.12 3.15 5.06 2.11 11.03\nEnsem.10.54 2.40 4.55 2.11 10.57\nFinetun.Single10.74 3.00 5.10 2.00 10.43\nEnsem.10.74 2.60 4.50 1.96 9.54\nDistilBERT\nMaskingSingle11.89 3.70 5.71 2.39 10.40\nEnsem.11.60 3.00 5.29 2.54 9.86\nFinetun.Single11.94 3.30 5.42 2.39 10.18\nEnsem.11.48 3.00 4.84 2.29 9.74\nTable 6: Error rate (%) on test set of tasks by RoBERTa\nand DistilBERT. Single: the averaged performance of\nfour models with different random seeds. Ensem.: en-\nsemble of the four models.\nD.2 Memory consumption\nTable 8 details the numerical values of Figure 3.\n2241\nMRPC RTE CoLA\nFinetuning (BERT + classiﬁer) 0.861±0.008 0 .692±0.027 0 .596±0.015\nMasking (BERT 00-11 + classiﬁer, initial sparsity 5%) 0.862±0.015 0 .673±0.036 0 .592±0.004\nMasking (BERT 00-11 + classiﬁer, initial sparsity 15%)0.825±0.039 0 .626±0.040 0 .522±0.027\nMasking (BERT 02-11 + classiﬁer, initial sparsity 5%) 0.868±0.011 0 .695±0.030 0 .595±0.010\nMasking (BERT 02-11 + classiﬁer, initial sparsity 15%)0.844±0.024 0 .662±0.021 0 .556±0.012\nMasking (BERT 04-11 + classiﬁer, initial sparsity 5%) 0.861±0.004 0 .705±0.037 0 .583±0.005\nMasking (BERT 04-11 + classiﬁer, initial sparsity 15%)0.861±0.009 0 .669±0.014 0 .553±0.014\nMasking (BERT 06-11 + classiﬁer, initial sparsity 5%) 0.862±0.004 0 .696±0.027 0 .551±0.006\nMasking (BERT 06-11 + classiﬁer, initial sparsity 15%)0.868±0.008 0 .691±0.033 0 .534±0.016\nMasking (BERT 08-11 + classiﬁer, initial sparsity 5%) 0.848±0.016 0 .675±0.034 0 .538±0.014\nMasking (BERT 08-11 + classiﬁer, initial sparsity 15%)0.851±0.009 0 .688±0.022 0 .545±0.005\nMasking (BERT 00-09 + classiﬁer, initial sparsity 5%) 0.859±0.012 0 .683±0.031 0 .589±0.011\nMasking (BERT 00-09 + classiﬁer, initial sparsity 15%)0.820±0.052 0 .604±0.021 0 .514±0.016\nMasking (BERT 00-07 + classiﬁer, initial sparsity 5%) 0.829±0.032 0 .649±0.053 0 .574±0.012\nMasking (BERT 00-07 + classiﬁer, initial sparsity 15%)0.807±0.042 0 .600±0.027 0 .509±0.004\nMasking (BERT 00-05 + classiﬁer, initial sparsity 5%) 0.814±0.033 0 .632±0.058 0 .565±0.027\nMasking (BERT 00-05 + classiﬁer, initial sparsity 15%)0.781±0.032 0 .567±0.030 0 .510±0.025\nMasking (BERT 00-03 + classiﬁer, initial sparsity 5%) 0.791±0.026 0 .606±0.027 0 .535±0.034\nMasking (BERT 00-03 + classiﬁer, initial sparsity 15%)0.776±0.035 0 .600±0.019 0 .527±0.014\nTable 7: Numerical value of the layer-wise behavior experiment. We train for 10 epochs with mini-batch size 32.\nThe learning rate is ﬁnetuned using the mean results on four different random seeds.\nNumber of Parameters Memory Usage (Kilobytes)\nFinetuning Masking Finetuning Masking\nPretrained 109,482,240 437,928.96\nMRPC + 1,536 + 1,536 + 71,368,704 + 1,536 + 6.144 + 6.144 + 8,921.088 + 0.192\nSST2 + 1,536 + 109,482,240 + 71,368,704 + 1,536 + 6.144 + 437,928.96 + 8,921.088 + 0.192\nCoLA + 1,536 + 109,482,240 + 71,368,704 + 1,536 + 6.144 + 437,928.96 + 8,921.088 + 0.192\nRTE + 1,536 + 109,482,240 + 71,368,704 + 1,536 + 6.144 + 437,928.96 + 8,921.088 + 0.192\nQNLI + 1,536 + 109,482,240 + 71,368,704 + 1,536 + 6.144 + 437,928.96 + 8,921.088 + 0.192\nSEM + 1,536 + 109,482,240 + 71,368,704 + 1,536 + 6.144 + 437,928.96 + 8,921.088 + 0.192\nTREC + 4,608 + 109,482,240 + 4,608 + 71,368,704 + 4,608+ 18.432 + 437,928.96 + 18.432 + 8,921.088 + 0.576\nAG + 3,072 + 109,482,240 + 3,072 + 71,368,704 + 3,072+ 12.288 + 437,928.96 + 12.288 + 8,921.088 + 0.384\nPOS + 37,632 + 109,482,240 + 37,632 + 71,368,704 + 37,632+ 150.528 + 437,928.96 + 150.528 + 8,921.088 + 4.704\nNER + 6,912 + 109,482,240 + 6,912 + 71,368,704 + 6,912+ 27.648 + 437,928.96 + 27.648 + 8,921.088 + 0.864\nSW AG + 768 + 109,482,240 + 768 + 71,368,704 + 768+ 3.072 + 437,928.96 + 3.072 + 8,921.088 + 0.096\nTable 8: Model size comparison when applying masking and ﬁnetuning. Numbers are based on BERT-base-\nuncased. Note that our masking scheme enables sharing parameters across tasks: tasks with the same number of\noutput dimension can use the same classiﬁer layer."
}