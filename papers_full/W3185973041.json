{
  "title": "Performance of Czech Speech Recognition with Language Models Created from Public Resources",
  "url": "https://openalex.org/W3185973041",
  "year": 2011,
  "authors": [
    {
      "id": "https://openalex.org/A2133254032",
      "name": "Procházka Václav",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745929260",
      "name": "Pollak Petr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227500874",
      "name": "Zdansky, Jindrich",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749189491",
      "name": "Nouza Jan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2171947673",
    "https://openalex.org/W1545755900",
    "https://openalex.org/W2139545398",
    "https://openalex.org/W1516588155",
    "https://openalex.org/W2114539183",
    "https://openalex.org/W2154507487",
    "https://openalex.org/W2241053157",
    "https://openalex.org/W2140608987",
    "https://openalex.org/W1982637252",
    "https://openalex.org/W2100957046",
    "https://openalex.org/W1525875379",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W61161404"
  ],
  "abstract": "In this paper, we investigate the usability of publicly available n-gram corpora for the creation of language models (LM) applicable for Czech speech recognition systems. N-gram LMs with various parameters and settings were created from two publicly available sets, Czech Web 1T 5-gram corpus provided by Google and 5-gram corpus obtained from the Czech National Corpus Institute. For comparison, we tested also an LM made of a large private resource of newspaper and broadcast texts collected by a Czech media mining company. The LMs were analyzed and compared from the statistic point of view (mainly via their perplexity rates) and from the performance point of view when employed in large vocabulary continuous speech recognition systems. Our study shows that the Web1T-based LMs, even after intensive cleaning and normalization procedures, cannot compete with those made of smaller but more consistent corpora. The experiments done on large test data also illustrate the impact of Czech as highly inflective language on the perplexity, OOV, and recognition accuracy rates.",
  "full_text": null,
  "topic": "Czech",
  "concepts": [
    {
      "name": "Czech",
      "score": 0.9524762630462646
    },
    {
      "name": "Computer science",
      "score": 0.6146912574768066
    },
    {
      "name": "Linguistics",
      "score": 0.43205636739730835
    },
    {
      "name": "Language model",
      "score": 0.4170878529548645
    },
    {
      "name": "Natural language processing",
      "score": 0.39808008074760437
    },
    {
      "name": "Speech recognition",
      "score": 0.3923293352127075
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}