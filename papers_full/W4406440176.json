{
  "title": "Assessing Racial and Ethnic Bias in Text Generation by Large Language Models for Health Care–Related Tasks: Cross-Sectional Study",
  "url": "https://openalex.org/W4406440176",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4282565435",
      "name": "John J Hanna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092814639",
      "name": "Abdi D Wakene",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2153210257",
      "name": "Andrew O. Johnson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2169211821",
      "name": "Christoph U. Lehmann",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2723423603",
      "name": "Richard J Medford",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4206686640",
    "https://openalex.org/W2905810301",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4376129499",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4381185932",
    "https://openalex.org/W4377010595",
    "https://openalex.org/W4387772689"
  ],
  "abstract": "Background Racial and ethnic bias in large language models (LLMs) used for health care tasks is a growing concern, as it may contribute to health disparities. In response, LLM operators implemented safeguards against prompts that are overtly seeking certain biases. Objective This study aims to investigate a potential racial and ethnic bias among 4 popular LLMs: GPT-3.5-turbo (OpenAI), GPT-4 (OpenAI), Gemini-1.0-pro (Google), and Llama3-70b (Meta) in generating health care consumer–directed text in the absence of overtly biased queries. Methods In this cross-sectional study, the 4 LLMs were prompted to generate discharge instructions for patients with HIV. Each patient’s encounter deidentified metadata including race/ethnicity as a variable was passed over in a table format through a prompt 4 times, altering only the race/ethnicity information (African American, Asian, Hispanic White, and non-Hispanic White) each time, while keeping all other information constant. The prompt requested the model to write discharge instructions for each encounter without explicitly mentioning race or ethnicity. The LLM-generated instructions were analyzed for sentiment, subjectivity, reading ease, and word frequency by race/ethnicity. Results The only observed statistically significant difference between race/ethnicity groups was found in entity count (GPT-4, df=42, P=.047). However, post hoc chi-square analysis for GPT-4’s entity counts showed no significant pairwise differences among race/ethnicity categories after Bonferroni correction. Conclusions A total of 4 LLMs were relatively invariant to race/ethnicity in terms of linguistic and readability measures. While our study used proxy linguistic and readability measures to investigate racial and ethnic bias among 4 LLM responses in a health care–related task, there is an urgent need to establish universally accepted standards for measuring bias in LLM-generated responses. Further studies are needed to validate these results and assess their implications.",
  "full_text": null,
  "topic": "Preprint",
  "concepts": [
    {
      "name": "Preprint",
      "score": 0.7351415157318115
    },
    {
      "name": "Ethnic group",
      "score": 0.661188542842865
    },
    {
      "name": "Cross-sectional study",
      "score": 0.5875195264816284
    },
    {
      "name": "Health care",
      "score": 0.5471398830413818
    },
    {
      "name": "Psychology",
      "score": 0.4310522675514221
    },
    {
      "name": "Computer science",
      "score": 0.33166366815567017
    },
    {
      "name": "Medicine",
      "score": 0.2987860143184662
    },
    {
      "name": "World Wide Web",
      "score": 0.22004327178001404
    },
    {
      "name": "Sociology",
      "score": 0.18555805087089539
    },
    {
      "name": "Political science",
      "score": 0.1508251428604126
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}