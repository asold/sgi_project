{
    "title": "Smoothing a tera-word language model",
    "url": "https://openalex.org/W2114539183",
    "year": 2008,
    "authors": [
        {
            "id": "https://openalex.org/A5013522667",
            "name": "Deniz Yüret",
            "affiliations": [
                "Koç University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2111305191",
        "https://openalex.org/W1966812932",
        "https://openalex.org/W2035959783",
        "https://openalex.org/W2132957691",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W605575788",
        "https://openalex.org/W2097927681",
        "https://openalex.org/W2154099718",
        "https://openalex.org/W2097333193",
        "https://openalex.org/W2159399018"
    ],
    "abstract": "Frequency counts from very large corpora, such as the Web 1T dataset, have recently become available for language modeling. Omission of low frequency n-gram counts is a practical necessity for datasets of this size. Naive implementations of standard smoothing methods do not realize the full potential of such large datasets with missing counts. In this paper I present a new smoothing algorithm that combines the Dirichlet prior form of (Mackay and Peto, 1995) with the modified back-off estimates of (Kneser and Ney, 1995) that leads to a 31% perplexity reduction on the Brown corpus compared to a baseline implementation of Kneser-Ney discounting.",
    "full_text": null
}