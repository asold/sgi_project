{
  "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
  "url": "https://openalex.org/W4389519108",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3208887323",
      "name": "Daman Arora",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2144077801",
      "name": "Himanshu Pratap Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222705522",
      "name": "Mausam",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3197040007",
    "https://openalex.org/W4385002382",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W4389520310",
    "https://openalex.org/W4296605665",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4378718493",
    "https://openalex.org/W4376653782",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2963123047",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2513499049",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3089285634",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W4353112996"
  ],
  "abstract": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7527–7543\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nHave LLMs Advanced Enough?\nA Challenging Problem Solving Benchmark For Large Language Models\nDaman Arora∗,†\nMicrosoft Research\ndaman1209arora@gmail.com\nHimanshu Gaurav Singh∗,†\nUC Berkeley\nhimanshu_singh@berkeley.edu\nMausam\nIIT Delhi\nmausam@cse.iitd.ac.in\nAbstract\nThe performance of large language models\n(LLMs) on existing reasoning benchmarks has\nsignificantly improved over the past years. In\nresponse, we present JEEB ENCH , a consid-\nerably more challenging benchmark dataset\nfor evaluating the problem solving abilities\nof LLMs. We curate 515 challenging pre-\nengineering mathematics, physics and chem-\nistry problems from the highly competitive IIT\nJEE-Advanced exam. Long-horizon reason-\ning on top of deep in-domain knowledge is\nessential for solving problems in this bench-\nmark. Our evaluation on various open-source\nand proprietary models reveals that the highest\nperformance, even after using techniques like\nself-consistency, self-refinement and chain-of-\nthought prompting, is less than 40%. The typi-\ncal failure modes of GPT-4, the best model, are\nerrors in algebraic manipulation, difficulty in\ngrounding abstract concepts into mathematical\nequations accurately and failure in retrieving\nrelevant domain-specific concepts. We also ob-\nserve that by mere prompting, GPT-4 is unable\nto assess risk introduced by negative marking\nfor incorrect answers. For this, we develop a\npost-hoc confidence-thresholding method over\nself-consistency, which enables effective re-\nsponse selection. We hope that our challeng-\ning benchmark will guide future re-search in\nproblem-solving using LLMs.\n1 Introduction\nThe capabilities of large language models (LLMs)\nhave been improving since the last decade on a\nplethora of tasks including reasoning. Most re-\ncently, GPT-4 demonstrates significant improve-\nments over GPT-3 on tasks such as code-generation,\narithmetic and commonsense reasoning (Bubeck\net al., 2023), exhibiting impressive performance on\nstandard reasoning and STEM benchmarks such as\nGSM-8K (Cobbe et al., 2021), MATH (Hendrycks\n∗equal contribution, †work done while at IIT Delhi\nFigure 1: An example problem from JEEB ENCH\net al., 2021b), MMLU (Hendrycks et al., 2021a)\nand ScienceQA (Lu et al., 2022)\nRising capabilities of LLMs call for harder\nbenchmarks. We introduce JEEB ENCH , a bench-\nmark consisting of 515 problems that require com-\nplex logical and mathematical reasoning on top\nof deep in-domain knowledge of pre-engineering\nlevel Physics, Chemistry and Mathematics. Prob-\nlems have been curated from the past 8 editions of\nthe Joint Entrance Examination (JEE)-Advanced\nexam, held annually in India as an entrance test\nto India’s premier engineering institutes: the IITs.\nThe exam is designed to be time-consuming, diffi-\n7527\ncult, and has a low selection rate (approx. 5%).\nThe problems in the dataset require a complex\ninterplay of employing multiple high-level domain\nspecific concepts, grounding them into mathemat-\nical equations/constraints, followed by algebraic\nmanipulation and arithmetic operations. Figure 1 is\na problem from the dataset along with an expert’s\nsolution. In this problem, the ideal solution in-\nvolves the retrieval of the appropriate concepts: the\nrules of static equilibrium, grounding the concepts\ninto mathematical equations for the specific prob-\nlem instance, followed by solving the equations\nin order to find the final answer. Other instances\nof domain-specific concepts can be Balancing of\nredox reactions (Chemistry), Current into a junc-\ntion equals current out of the junction (Physics)\nand Integration by parts (Mathematics). More such\nexamples can be found in the Appendix A.2.\nWe conduct a qualitative and quantitative study\nof contemporary open-source and proprietary\nLLMs on these problems and also highlight av-\nenues for further research. Our analysis indicates\nthat GPT-4 is unparalleled in performance com-\npared to other models. It demonstrates long hori-\nzon reasoning and the ability to manipulate com-\nplex algebraic equations in quite a few problems.\nWe observe that chain-of-thought prompting (Ko-\njima et al., 2022) and self-consistency (Wang et al.,\n2023b), which are recent proposals to improve\nLLM performance, are indeed effective on our\ndataset.\nWe also explore Self-Critique (Madaan et al.,\n2023; Shinn et al., 2023), where an LLM (verifier)\nis instructed to improve the outputs of the same\nLLM (generator). We find that this approach is\nnot helpful on JEEB ENCH . The verifier is weak in\nspotting conceptual errors, and like the generator,\nis itself prone to hallucinations. It would be inter-\nesting to explore the class of problems where this\napproach of self-refinement is (not) helpful.\nWe further conduct a critical analysis of the lim-\nits of GPT-4’s reasoning abilities, and highlight\nmajor areas that require considerable improvement.\nA detailed error analysis suggests that it frequently\nstruggles in retrieving relevant concepts required\nto solve problems, and performing algebraic ma-\nnipulation & arithmetic. Inability to perform even\nsimple algebra highlights an important question:\ncan we build LLMs faithful to mathematical logic?\nAnother important question is how to estimate\nGPT-4’s performance in comparison to humans.\nThe JEE Advanced Exam comes with the bane of\nnegative marking for incorrectly answered ques-\ntions. This makes the exam even more challeng-\ning, because in addition to advanced problem solv-\ning skills, it requires an accurate risk assessment\nand computing a good policy based on it. Our ex-\nperiments demonstrate that when prompted with\nthe marking scheme, GPT-4’s performance actu-\nally drops. To mitigate this, we employ a simple\nmethod - thresholding over self consistency. Self\nconsistency generates multiple responses for each\nquestion. Relative frequency in the set of responses\ncan be considered as a proxy for confidence score\nof each option. Threshold on the confidence score\ncan be tuned using a validation set. We find that\nGPT-4’s score, after augmenting it this way, lies\nin the top 10-20 percentile of human scores in the\n2023 edition of the exam.\nOverall, we hope that this benchmark serves as\na strong and reliable test-bed and fosters future\nresearch on problem solving with LLMs. Our code\nand dataset are available athttps://github.com/\ndair-iitd/jeebench.\n2 Related Work\nReasoning has been studied under various contexts\nsuch as logical reasoning, commonsense reason-\ning, mathematical reasoning, and theorem proving.\nWe summarize some key works in two sub-areas,\nmost closely related to our work: mathematical\nreasoning and Science QA.\nMathematical problem solving: GSM8K (Cobbe\net al., 2021), Dolphin18K (Huang et al., 2016),\nAQuA-RAT (Ling et al., 2017), MATH (Hendrycks\net al., 2021b) and Ape210K (Zhao et al., 2020)\nare datasets that contain mathematical reasoning\nquestions. Dolphin18K, GSM8K, and AQuA-RAT\nconsist of elementary problems, requiring only ba-\nsic arithmetic and problem comprehension. Thus,\nthere is a general lack of variety in the underlying\nreasoning steps across problems. In terms of dif-\nficulty, MATH, containing problems from AMC,\nAIME and Olympiads, comes close to JEEB ENCH\nin terms of complexity. However, compared to\nMATH, the mathematics questions in our dataset\nspan many additional topics such as Differential\nand Integral Calculus, Differential Equations, 3D\ngeometry, and Conic Sections. Also, the problems\nin JEEB ENCH are harder, as we discuss later in\nthe paper. miniF2F (Zheng et al., 2022) consists\nof mathematics problems from MATH dataset and\n7528\nFigure 2: Performance of GPT-4 on a random subset of\nMATH, MMLU-Physics, Chemistry, Math,JEEB ENCH\nother sources in a formal language. In contrast,\nproblems in our dataset are in natural language.\nGeneral Science: In the context of Physics and\nChemistry, ScienceQA (Lu et al., 2022), SciQ\n(Welbl et al., 2017) and MMLU (Hendrycks et al.,\n2021a) are prominent available datasets. Sci-\nenceQA and SciQ, built from elementary and high\nschool science curricula, mainly test factual knowl-\nedge of the subject. The skills required to solve\nsuch problems are primarily information extraction,\nreading comprehension and commonsense reason-\ning. In contrast, questions in our dataset require\nlong-horizon reasoning and grounding of com-\nplex scientific concepts into equations and arith-\nmetic. Concurrently, C-Eval (Huang et al., 2023)\nand SciBench (Wang et al., 2023a) are datasets\nalong similar lines. C-Eval consists of a variety\nof disciplines such as engineering, medicine and\nhumanities has been created. SciBench creates a\ndataset from college-level Mathematics, Physics\nand Chemistry questions.\nProblems present in JEEB ENCH are signifi-\ncantly harder than those in other contemporary\ndatasets. To verify this, we sample 50 questions\neach from JEEB ENCH and the test sets of MATH\nand the high-school Physics, Chemistry and Math-\nematics sections from MMLU and conduct zero-\nshot evaluations on GPT-4. The results can be\nseen in Figure 2. As we can see, GPT-4 can easily\nsolve more than 80% problems from MMLU. The\nMATH dataset is harder, where the performance\nis approximately 60%. However, GPT-4 struggles\nin JEEB ENCH -Math, solving close to a mere 20%\nproblems.\n3 The JEEB ENCH Dataset\nThe dataset consists of 515 problems extracted\nfrom the past 8 editions of the JEE-Advanced\nfrom the year 2016 to 2023. The problems are\nharvested from publicly available sources. 1 The\nexam consists of 2 papers held every year, each con-\ntaining 50-60 questions equally distributed among\nPhysics, Chemistry, and Mathematics. We use on-\nline tools to extract problems from PDF-format\nexam papers into L ATEX. We remove all prob-\nlems containing diagrams in their description (ap-\nproximately 40%). Manual quality checks are\nperformed to fix/eliminate possible errors in pre-\nprocessing. Figure 3 shows representative prob-\nlems from the final dataset. The problems are cate-\ngorised by subject: Physics, Chemistry and Mathe-\nmatics, and the format of expected response: multi-\nple choice questions (MCQ) with single option cor-\nrect, MCQs with multiple options correct, Integer-\ntype and Numeric-type. In Integer-type questions,\nthe answer is an unbounded non-negative integer,\nwhereas for Numeric-type, the answer is a float-\ning point number upto 2 digits after the decimal\npoint. The breakdown of the problems based on\nanswer-type and subject is shown in Table 1.2\nMath Phys Chem\nSingle-Correct 53 27 30 110\nMulti-Correct 85 41 60 186\nInteger 37 22 23 82\nNumeric 61 33 43 137\nTotal 236 123 156 515\nTable 1: # of questions for each subject and problem-type.\nThe questions contained in the dataset belong\nto diverse sub-topics (for example, Math questions\ncould belong to Calculus, Algebra, Combinatorics,\netc.). The breakdown of the entire dataset into\nsub-topics can be found in Appendix A.1.\n4 Experimental Setup and Results\nWe wish to investigate the following research prob-\nlems:\n1. How well do LLMs perform on JEEB ENCH ?\n2. How effective are methods, such as chain-\nof-thought prompting and self-consistency,\nwhich have been proposed to improve the rea-\nsoning abilities of LLMs?\n3. What are the main sources of errors which\nlimit the performance of these models?\n1https://jeeadv.ac.in/archive.html\n2There are fewer problems in Physics and Chemistry be-\ncause more problems in these two subjects contained images.\n7529\nFigure 3: Instances from the dataset from each subject:\nPhysics (Top), Chemistry (Middle), Math (Bottom)\n4. Can LLMs be used to verify their own genera-\ntions in the context of JEEB ENCH ? What are\nthe limitations in this behaviour?\n5. How would they perform in an exam setting,\nwhere each question could potentially give\nnegative marks when answered incorrectly?\n4.1 Metrics\nFor Single-Correct MCQs and Integer-type ques-\ntions, we use accuracy as the metric, that is, a score\nof 1 if the model response matches the gold re-\nsponse, otherwise 0. For Numeric-type questions,\nwe award a score of 1 if the model response differs\nfrom the gold response by atmost 0.01. For Multi-\nCorrect MCQs, we award a score of 1 if the model\nresponse matches all the correct options. If any\nof the options selected by the model is incorrect,\nwe award 0. If the model selects some of the cor-\nrect options and no incorrect option, then for each\ncorrect option in the output, the model is given a\nscore of 0.25. For example, if the gold response\nis ABD and the output response is BD, a score of\n0.5 is awarded. This is done, so as to reflect the\nactual scoring method of JEE-Advanced, which\nincentivizes a student to not guess.3\n4.2 Prompting LLMs\nWe evaluate the proposed benchmark on some\nopen-source models: Falcon7B-Instruct (Al-\nmazrouei et al., 2023) and Alpaca-LoRA, which\nuses low-rank adapation (Hu et al., 2021) to re-\nproduce Alpaca (Taori et al., 2023). Then, we\nevaluate proprietary models such as the OpenAI’s\n3JEE Advanced also employs negative marking. We incor-\nporate it in Section 4.5 while comparing with human perfor-\nmance in exam setting.\nGPT series of models text-davinci-003 (GPT-\n3), gpt-3.5-turbo (GPT-3.5) and gpt-4-0314\n(GPT-4) , as well as text-bison-001 (PaLM-2)\nprovided by Google. Evaluation on larger open-\nsourced LLMs is left for future work.\nFor obtaining the model’s response, each model\nis prompted with the expected response type con-\ncatenated with the problem description. The exact\nsystem and user prompts can be found in the Ap-\npendix A.3. The exact answer is extracted man-\nually from the response generated by the LLM.\nSometimes, the LLM response is gibberish and\nsometimes responds by saying that none of the op-\ntions are correct. For both of these cases, we record\n“None” as the answer. If the question’s expected\nresponse type doesn’t match the question type (for\nexample non-integer for an integer type question),\neven then we record it as a “None” response.\nWe also conduct a few-shot evaluation with ex-\namples drawn from 2014 edition of the exam. One\nexample is chosen for each question type and sub-\nject pair.\nAll the proprietary models were prompted be-\ntween May 17, 2023 and June 23, 2023. The maxi-\nmum response length is set to 2048 and decoding\ntemperature is set to 0. Table 2 contains the results\nobtained on various LLMs aggregated by subject\nand question type.\nGeneral trends: We observe that open-source\nmodels perform as good as random and are, in\ngeneral, lagging behind proprietary models. Perfor-\nmance on JEEB ENCH increases consistently with\nnewer versions of the GPT model. GPT-3 exhibits\nnear random performance, but GPT-3.5 and GPT-4\nperform significantly better. GPT-4 is far superior\nto GPT-3.5, by a large margin of 12.9 points but\noverall performance still remains close to 30%. It\nis evident that the performance boost is the high-\nest for Chemistry, followed by Physics, and lastly\nMaths. This is probably because the complexity of\nreasoning is highest in Mathematics questions and\nleast in Chemistry in JEEB ENCH . These results\nhighlight the difficulty of the benchmark posed to\nboth open-source and proprietary models.\nHereafter, we focus on just GPT-4’s performance\nsince it is far superior to other models. Firstly,\nwe evaluate the performance of methods like zero-\nshot chain-of-thought prompting (Kojima et al.,\n2022), self-consistency (Wang et al., 2023b) & self-\nrefinement (Madaan et al., 2023) on JEEB ENCH .\nZero shot Chain-of-Thought prompting: The\n7530\nChemistry Mathematics PhysicsInteger Single-Correct Multi-Correct NumericTotal\nRandom 0.108 0.105 0.103 0.000 0.250 0.144 0.000 0.105\nAlpaca-LoRA 0.072 0.101 0.087 0.037 0.164 0.122 0.015 0.089\nFalcon7B-Instruct 0.083 0.114 0.085 0.000 0.182 0.142 0.029 0.098\nGPT-3 0.135 0.107 0.134 0.049 0.291 0.133 0.015 0.122\nPaLM2 0.192 0.130 0.146 0.073 0.291 0.165 0.073 0.153\nGPT-3.5 0.228 0.146 0.173 0.073 0.318 0.249 0.029 0.177\nGPT-4 0.423 0.212 0.352 0.207 0.455 0.383 0.153 0.309\nGPT-4+CoT 0.468 0.280 0.335 0.256 0.473 0.448 0.175 0.350\nGPT-4+ (1-shot) CoT 0.409 0.198 0.323 0.244 0.391 0.340 0.175 0.292\nGPT-4+CoT+Self Critique0.487 0.234 0.352 0.280 0.355 0.444 0.219 0.339\nGPT-4+CoT+SC@8 0.463 0.308 0.449 0.293 0.618 0.410 0.234 0.389\nTable 2: This table shows the score obtained by various open-source and proprietary models on JEEB ENCH\naggregated by subject on the left question type on the right. The overall aggregate scores are in the last column.\nNote that CoT in the table refers to zero-shot CoT except for GPT-4 + (1-shot) CoT.\noriginal prompt is concatenated with the phrase\nLet’s think step by step , as proposed by Kojima\net al. (2022). We observe that this approach leads\nto significant improvement in performance, improv-\ning vanilla GPT-4 by 4.2 points.\nFew-shot Chain-of-Thought prompting: We\nprepend the question with one few-shot example\nfor each question-type, subject pair. Overall, 1-\nShot CoT achieves a score of 0.296 as opposed to\nzero-shot CoT at 0.350 and vanilla GPT-4 at 0.308.\nOur hypothesis is that few-shot prompting is not\nvery helpful in these questions, because conceptual\nerrors are hard improve upon using few-shot ex-\namples. Additionally, many novel reasoning paths\nmight not be covered in the few-shot examples.\nThus, our dataset acts as an interesting testbed for\nadvanced approaches in few-shot prompting. Sim-\nilar results where scores are better with zero-shot\nCoT as compared to few-shot CoT have been found\nin Wang et al. (2023a).\nFunction calling: Since GPT-4 makes a lot\nof arithmetic errors, we decide to also test the\nfunction-calling API exposed by OpenAI. Since\nthe JEE exam only allows access to a basic calcu-\nlator with 4 primitive arithmetic operation(+, -, /,\n*), allowing plugins such as Wolfram or python\nwould not make a fair comparison (for example,\ndirectly asking Wolfram to integrate a function in-\nstead of doing it from first principles). Instead, to\nensure a level-playing field, we use the newly intro-\nduced function-calling API to implement standard\narithmetic operators.\nNote that function-calling is only allowed with\ngpt-4-0613 whereas the rest of our results are with\ngpt-4-0314. We first evaluate gpt-4-0613 with zero-\nshot CoT. Surprisingly, results suggest that gpt-\n4-0613 (the new version) is weaker with a CoT\nFigure 4: The figure shows the different types of error made\nby GPT-4 in its response. (i) (top) exhibits a computation\nerror, where the squaring operation performed is algebraically\nwrong. (ii) (middle) response shows a conceptual error where\nthe model is unable to retrieve the relevant concepts required\nto solve the problem (iii) (bottom) response is a grounding\nerror, where the concept is correct, however the application\nin terms of computing # lone pair electrons on Br in BrF5 is\nwrong.\nperformance of 0.303 as compared to 0.350 with\ngpt-4-0314.\nUsing a calculator API reduces performance\neven more to 0.274 from 0.303. We observe that\ntool usage isn’t very robust for GPT-4, where it\nsometimes hallucinates invalid function arguments\nand sometimes even invalid function names! Also,\nGPT-4 is quite accurate at arithmetic for small digit\noperations. Computation errors are mostly during\nsymbolic manipulation, rather than purely arith-\nmetic operations, which is probably why a black-\nbox calculator isn’t very beneficial.\nSelf-Consistency (SC): We sample multiple re-\nsponses from the LLM at a non-zero temperature.\n7531\nFor Integer-type, Numeric-type and Single-Correct\nMCQs, we use a majority vote (from all the re-\nsponses which are not “None”) as the proposed\nanswer. For Multi-Correct MCQs, we choose a\nsimplifying assumption that all options are inde-\npendent. If an option occurs atleast 50% times in\nthe responses, we select it, otherwise we don’t. We\nuse τ = 0.5 and the number of responses is set to 8.\nSelf-consistency helps a lot in improving over the\nGPT-4+CoT baseline by a score of +3.9 points. In\nthe future, it will be interesting to apply extensions\nsuch as adaptive consistency for better cost-quality\ntradeoffs (Aggarwal et al., 2023).\n4.3 Error Analysis of System Responses\nIn order to assess GPT-4’s weaknesses, we conduct\na manual inspection of the errors it makes in its\nreasoning chains. We perform this study on the\nerrors made by GPT-4+CoT on a random subset of\n100 problems. The score obtained on this subset is\n27.25. We ask the following questions about the\nmodel response for each problem instance:\n1. Is GPT-4 able to retrieve the concepts/facts\nrequired for solving the problem? Inability to\ndo this contributes to conceptual errors.\n2. If relevant concepts are retrieved, are they\ngrounded correctly as equations/constraints?\nThese contribute to grounding errors.\n3. Is the algebraic manipulation & arithmetic cor-\nrect? These contribute to computation errors.\nRefer to Figure 4 for an illustration of each type\nof error4. In one case, we find that GPT-4 misun-\nderstands the question. The overall results of this\nanalysis is shown in Table 3.\nError Type Count\nConceptual Error 34\nComputation Error 30\nGrounding Error 15\nProblem Miscomprehension1\nPerfect 20\nTable 3: Variety of errors GPT-4 makes in the solution.\nOur error analysis indicates that most errors are\ncaused because of not being able to retrieve con-\ncepts (34 out of 80), which are critical to making\nprogress in the solution, or due to computation\nerrors (30 out of 80). Moreover, in 20 questions,\n4Note that a question can have multiple types of errors,\nhowever, we only investigate the first error that is noticed.\nwhere the answer is correct (out of 27), the explana-\ntion is also correct. i.e., 28% of the time, the model\ngives a correct answer for the wrong reasons.\n4.4 Can GPT-4 find and correct its mistakes?\nCan GPT-4 be used to grade its own outputs? A\ngood grader should be able to spot errors in a solu-\ntion. Using an LLM to critique its own output has\nbeen proposed recently by multiple works (Shinn\net al., 2023; Madaan et al., 2023) and has shown\nimprovements on some datasets. A good verifier\nshould be able to catch and fix all errors. Even\nwhen the final answer is correct, it isn’t necessary\nthat intermediate reasoning steps are correct.\nWe put the idea of self-critique to test on\nJEEB ENCH . After a CoT response has been gen-\nerated, we prompt another GPT-4 instance by first\ndescribing the problem, GPT’s solution and then\nappending the instruction: “Find problems(if any)\nwith the given solutions. If there are any errors,\ncorrect it and give the new answer. ”\nWe re-evaluate the new answer suggested by\nGPT-4. Results clearly show that this approach\ndoesn’t lead to improvement. In fact, it leads to\npoorer results as compared to GPT-4+CoT and the\nperformance goes down from 35% to 33.9%.\nIn order to develop a deeper understanding of the\nrepairs suggested by the verifier GPT-4, a manual\ninspection is performed. We use the same subset\nof 100 problems picked up earlier for categoriz-\ning error-types. For each generated solution and\nsuggested edit, we pose the following questions:\n• Can the verifier find problems in the solution?\n• Can the verifier fix problems if it finds them?\n• Is the problem identified by the verifier actu-\nally a valid problem?\nError in solution? Verifier Response Count\nYes No error found 46\nFound error but didn’t fix it25\nConverted non-error to error7\nFound error and fixed it2\nNo Converted non-error to error1\nDidn’t find error 19\nTable 4: The figure shows the breakup of the kind of\nerrors the verifier GPT-4 makes while suggesting edits.\nOur results can be seen in Table 4. It is evident\nthat, contrary to observations in other works, on\nJEEB ENCH , GPT-4 is mostly ( 46\n80 = 57.5%) un-\nable to find errors in solutions it proposes. Even\nwhen it can, it is unable to fix them. Only in 2\n7532\nFigure 5: [Top]: Question where GPT-4 identifies a mistake\nbut is unable to fix it. The problem and part of the response\nis on the top. The bottom block contains the edit suggested\nby GPT-4. The manipulation in the edit suggested is mathe-\nmatically wrong. [Bottom]: Question where GPT-4 is unable\nto identify an error. The problem and part of the response is\non the top. The bottom block contains the edit suggested by\nGPT-4. It should be log2(2 · 44) instead of log2(2 · 164)\n.\nout of 80 questions, was GPT-4 able to give a\nmeaningful edit to an erroneous solution, which\nis over-compensated by the solutions it degrades\nby suggesting edits to parts of the solution which\nare already correct. Figures 5 provides example of\nerrors made by the verifier. The complete response\nfor these along with other examples can be found\nin the Appendix A.5. This experiment raises an\ninteresting question: for what class of problems\nis self-critique (not) helpful? It might be interest-\ning to look into methods which use learnt verifiers\n(Arora and Kambhampati, 2023; Lightman et al.,\n2023).\n4.5 Comparison with human performance\nThe JEE exam contains negative marking, for in-\nstance, single-correct MCQ questions are awarded\na score of +3 marks when correct, -1 if answered\nincorrectly and 0 when not answered. For Mult-\nCorrect MCQs, a score of +4 is given when all\noptions are included in the final answer. If any of\nthe options is wrong, -2 is given. If some of the\noptions are correct, +1 is given for each correct\noption. The skills needed for an examinee to maxi-\nmize one’s score include being able to assess one’s\nown confidence in their response and being able to\ndecide whether to answer or not based on the con-\nfidence levels. Contingent on the former skill, the\nlatter is a simple decision-theoretic computation\nunder uncertainty.\n4.5.1 Deciding whether to answer\nTo attain a good score in the examination, it is im-\nportant to ensure that the model does not answer\nwhen it is unsure of its solution. Can LLMs as-\nsess this risk and plan accordingly when prompted\nwith the marking scheme? To investigate this, we\nprompt the model with the exact marking scheme\nfor each MCQ question type along with problem\nstatement, and then ask to generate an answer or\nskip the question altogether. The complete prompt\nis in Appendix A.6. We re-run inference on all\nthe problems with these prompts for all the MCQ\nquestions. The results can be seen in Table 5.\nMethod Pos. Score Neg. ScoreTotal\nGPT-4+CoT w/o Marking489 181 308\nGPT-4+CoT w Marking404 206 198\nTable 5: Marks obtained when GPT-4 is prompted with\nthe marking scheme v/s without on MCQ questions.\nThese marks are out of a total of 1074.\nResults indicate that prompting is not helpful\nin this case, and GPT-4 cannot effectively decide\nwhen not to answer. This is in line with observa-\ntions made by Valmeekam et al. (2022) where it\nis shown that LLMs have poor planning capabili-\nties. In response, we develop a post-hoc confidence-\nthresholding method on self-consistency responses.\n4.5.2 Calibration\nFor single-correct & multiple-correct MCQs, we\ncompute the confidence score for each option by\ncomputing its relative frequency in the set of re-\nsponses. Note that often, GPT-4 is unable to answer\nthe question at all, or arrives at a conclusion that is\nnot supported by any option (a “None” response).\nIn such cases, we do not count contributions from\nthis response. For instance, if a model’s response\nin 4 attempts in a Multi-Correct MCQ is “AB”,\n“None”, “B”, “AC”, then, the confidence for op-\ntions are A:1\n2 , B:1\n2 , C:1\n4 , D:0.\nFigure 6 is the calibration cruve of GPT-4 on\nJEEB ENCH . The maximum calibration Error\n(MCE) is 0.136 and the average calibration error\n(ACE)5 is 0.098. The plot suggests that the model is\nslightly overconfident at high confidences, because\nof lower accuracy on higher levels of confidence,\n5MCE/ACE are the maximum/average absolute difference\nb/w confidence & accuracy among all confidence bins.\n7533\nFigure 6: Calibration plot of GPT-4 on MCQ questions\nFigure 7: Scores obtained on different thresholding val-\nues on Single-Correct(top) and Multi-Correct(bottom)\ntype questions from the val set, the optimal value is\nτsingle = 0.125 and τmultiple = 0.75\nbut slightly underconfident at low and medium con-\nfidences.\n4.5.3 Thresholding with Self-Consistency\nOur objective is to decide whether to include an\noption in the final response or not. We wish to com-\npute a parameter τ such that an option will be in\nthe final response if the confidence for that option\nis atleast τ. We compute separate τsingle,τmultiple\nfor Single-correct and Multiple-correct MCQs re-\nspectively. We compute confidence scores for\nGPT-4’s response to each question as in Section\n4.5.2. Questions from 2016-2021 are chosen as\nthe validation set and from 2022-2023 as the test\nset. The best τsingle and τmultiple thresholds for\nSingle-Correct and Multi-Correct MCQs by simple\nhyper-parameter search. Figure 7 shows the plot\nof positive, negative and total score on the vali-\ndation set over range of possible values of τsingle\nand τmultiple. The optimal value of τmultiple is\n0.75 and of τsingle is 0.125. τsingle being less than\n0.25 indicates that taking a majority vote is the\nFigure 8: Plot showing performance of models com-\npared to projected human performance.\nbest strategy for single-correct MCQs. However,\nthis is not true for multi-correct MCQs, where a\nthreshold of τmultiple = 0.5 (as done originally) is\nsub-optimal. We assume that Integer and Numeric\nquestions do not have negative marking. The final\nresponse for them is decided using a majority vote\nover responses. Table 6 shows scores with the\noptimal thresholds on the test set. We find that not\nanswering when confidence is less than threshold\nincreases the total score by about 4.3%.\nMethod Pos Score Neg ScoreTotal Score\nGPT-4+CoT 109 43 66\nGPT-4+CoT+SC 118 49 69\nGPT-4+CoT+SC+Thresholding111 39 72\nTable 6: Marks on the test set obtained when optimal thresh-\nolds derived from the val set are used.\n4.6 Estimating performance compared to\nhumans\nFinally, we wish to estimate the performance of\nGPT-4 compared to humans. For this we use the\n2023 examination paper since there is almost no\nprobability of contamination. The 2023 paper was\nreleased on 4th June 2023 and contained 65 ques-\ntions which were textual (rest 37 contained images).\nThe total marks in the exam were 360. The score\nobtained by GPT-4 after confidence-thresholding\non MCQs and regular aggregation on Integer and\nNumeric types is 49 out of 229. Assuming average\ndifficulty levels in questions that were not included\n(because they contained images) is equal to ones\nthat were, we normalize the projected human per-\nformance from 229 to 360 giving it a total of 77\nmarks out of 360. Projections indicates that this\nwould place GPT-4 around the 80-90 percentile\nrange. The results of JEE Advanced 2023 indicate\nthat the top 10% score is approximately 97/360 and\ntop 20% score is approximately 70/360. Figure 8\n7534\nprovides a comparison of the performance of LLMs\nalong with human performance of the applicants.\n4.7 Has GPT-4 memorized some problems?\nIn the era of internet-scale pre-training, it is very\nhard to ascertain whether a dataset has been\nused for training a particular model. Neverthe-\nless, we tried to investigate the contamination of\nJEEB ENCH . This was done by (i) searching for\ninstances in JEEB ENCH from publicly available\ninternet corpora, (ii) prompting the LLM to com-\nplete the problem statement itself, when prompted\nwith a prefix of the problem statement. Both these\ninvestigations suggest only minor (approx. 6%)\ncontamination. A detailed description of our con-\ntamination study can be found in Appendix A.8.\nGPT-4+CoT+SC attains a score of 0.338 on Ad-\nvanced 2023 questions, which is not very far from\nthe aggregate performance of 0.396 on the remain-\ning dataset. Given that 2023 questions are uncon-\ntaminated, we believe that the extent of contam-\nination is quite low, and its performance on this\ndataset is a genuine indication of its current reason-\ning abilities. It is also noteworthy that some exams\n(e.g., JEE Advanced 2017) are easier, and GPT-4\ndoes much better on it raising the aggregate score.\n5 Discussion\nThe general performance trend demonstrates the ef-\nficacy of high-quality data, instruction fine-tuning,\nRLHF and parameter scaling in improving the rea-\nsoning capabilities of LLMs. For many problems,\nGPT-4 is able to give a sketch of the correct, human-\nlike solution that is impressive given the extent of\nreasoning involved in the problems. However, our\nanalysis also reveals major areas where progress is\nneeded. Although GPT-4 performs flawless logi-\ncal and mathematical reasoning in some instances,\nsometimes it commits grave errors in trivial steps.\nErrors in retrieval and application of concepts\nsuggest an interesting research question: Can we\naugment an LLM such that it’s generation is con-\nstrained by faithfulness to a set of facts? Such a\nsystem will demonstrate robustness in reasoning,\ncritical for long-horizon tasks.\nPhysics problems in the benchmark often require\nan understanding of spatial reasoning. We found\nthat while GPT-4’s spatial reasoning is far from\nperfect. Appendix A.7 provides an example where\nGPT-4 commits errors which might be attributed to\nits inability to reason spatially. With the release of\nthe multi-modal version of GPT-4, evaluating this\naspect of Physics problems might be easier.\nFinally, an LLM that understands its own con-\nfidence in an answer is a key missing piece, as\nhighlighted by our experiments in the exam setting.\nOur simple post-hoc wrapper does slightly improve\nperformance in this regard.\n6 Conclusion\nWe present a challenging problem solving bench-\nmark to evaluate large language models. We per-\nform a detailed analysis of the performance of var-\nious LLMs on the benchmark, and identify areas\nof improvement in the best current LLMs. Our\nwork raises interesting research directions such as\nmathematical logic-augmented GPT, multi-modal\nevaluations on GPT-4 and the decision-making ca-\npabilites of GPT in an exam setting. We hope that\nJEEB ENCH guides future research in reasoning us-\ning LLMs. Our code and dataset are available at\nhttps://github.com/dair-iitd/jeebench.\nAcknowledgements\nWe thank Dr. Parag Singla and the JEE Office for\nhelping with getting approvals to use the dataset\nfor research purposes. We thank Rishabh Ran-\njan for discussions around calibration and Mohd.\nZaki for help in obtaining OpenAI API access for\nthis work. The work is supported by grants from\nGoogle, Verisk, Microsoft, and Huawei, and the Jai\nGupta chair fellowship by IIT Delhi. We thank the\nIIT-D HPC facility for its computational resources.\nLimitations\nContamination is a big problem in the era of pre-\ntrained language models which have been trained\non large web corpora. Therefore, its really hard\nto determine if a dataset has been seen. Determin-\ning the extent of contamination is also not easy,\nalthough we make an effort to quantify it. Eval-\nuations against humans is also a slightly flawed\nprocess due to other limitations such as time pres-\nsure during the examination procedure. Addition-\nally, this data’s distribution is fixed to pre-college\nPhysics, Chemistry and Mathematics. There are\nmore gradations and difficulty levels at which the\nmodel can be evaluated which have not been tested\nas part of our analysis.\n7535\nReferences\nPranjal Aggarwal, Aman Madaan, Yiming Yang, and\nMausam. 2023. Let’s sample step by step: Adaptive-\nconsistency for efficient reasoning and coding with\nllms. In Conference on Empirical Methods in Natu-\nral Language Processing.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. Falcon-40B: an open large language model\nwith state-of-the-art performance.\nDaman Arora and Subbarao Kambhampati. 2023.\nLearning and leveraging verifiers to improve plan-\nning capabilities of pre-trained language models.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-\nter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,\nHarsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general in-\ntelligence: Early experiments with gpt-4.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168.\nJesse Dodge, Ana Marasovic, Gabriel Ilharco, Dirk\nGroeneveld, Margaret Mitchell, and Matt Gardner.\n2021. Documenting large webtext corpora: A case\nstudy on the colossal clean crawled corpus. In Con-\nference on Empirical Methods in Natural Language\nProcessing.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021a. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021b. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nDanqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin,\nand Wei-Ying Ma. 2016. How well do computers\nsolve math word problems? large-scale dataset con-\nstruction and evaluation. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 887–896,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei\nZhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, Yao\nFu, Maosong Sun, and Junxian He. 2023. C-\neval: A multi-level multi-discipline chinese evalu-\nation suite for foundation models. arXiv preprint\narXiv:2305.08322.\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. InAdvances in\nNeural Information Processing Systems, volume 35,\npages 22199–22213. Curran Associates, Inc.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri\nEdwards, Bowen Baker, Teddy Lee, Jan Leike, John\nSchulman, Ilya Sutskever, and Karl Cobbe. 2023.\nLet’s verify step by step.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 158–167, Vancouver,\nCanada. Association for Computational Linguistics.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. 2022. Learn to explain:\nMultimodal reasoning via thought chains for science\nquestion answering. In The 36th Conference on Neu-\nral Information Processing Systems (NeurIPS).\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck, Bodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao.\n2023. Reflexion: Language agents with verbal rein-\nforcement learning.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\nand Subbarao Kambhampati. 2022. Large language\nmodels still can’t plan (a benchmark for LLMs on\nplanning and reasoning about change). In NeurIPS\n2022 Foundation Models for Decision Making Work-\nshop.\nXiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu\nZhang, Satyen Subramaniam, Arjun R. Loomba,\nShichang Zhang, Yizhou Sun, and Wei Wang.\n2023a. Scibench: Evaluating college-level scientific\nproblem-solving abilities of large language models.\n7536\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023b. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions.\nArXiv, abs/1707.06209.\nWei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and\nJingming Liu. 2020. Ape210k: A large-scale and\ntemplate-rich dataset of math word problems.\nKunhao Zheng, Jesse Michael Han, and Stanislas Polu.\n2022. minif2f: a cross-system benchmark for for-\nmal olympiad-level mathematics. In International\nConference on Learning Representations.\n7537\nA Appendix\nA.1 Sub-topic wise distribution of questions in\nJEEB ENCH\nFigure 9 provides the topic wise distribution of\nproblems in JEEB ENCH for each subject.\nA.2 Example problems from JEEB ENCH\nHere we present a few problems from\nJEEB ENCH along with expert written solu-\ntions, with concepts being highlighted in yellow,\ntheir grounding being highlighted in violet, and the\nfinal algebraic manipulation highlighted in green.\nSee Figures 10 for a Math Problem, Figure 11 for\na Physics problem and Figure 12 for a Chemistry\nproblem.\nA.3 Exact Prompts for GPT models\nFor each problem, we prompt the model with the\nrequired answer type and then the prompt. For\ndifferent response types, the prompts are:\n1. Single-Correct: In this problem, only\none option will be correct. Give\na detailed solution and end the\nsolution with the final answer.\n2. Multi-Correct: In this problem,\nmultiple options can be correct.\nGive a detailed solution and end the\nsolution with the final answer.\n3. Integer: In this problem, the final\nanswer will be a non-negative integer.\nGive a detailed solution and end the\nsolution with the final answer.\n4. Numeric: In this problem, the final\nwill be a numeric value. Give the\nnumerical answer correct upto the\n2nd decimal digit. Give a detailed\nsolution and end the solution with\nthe final answer.\nThe entire prompt consists of the question\nprepended with the response type prompt. An ex-\nample is:\nIn this problem, only one option will be correct.\nGive a detailed solution and end the solution\nwith the final answer.\nProblem: A uniform wooden stick of mass\n1.6 kg and length lrests in an inclined manner on\na smooth, vertical wall of height h(<l) such that a\nsmall portion of the stick extends beyond the wall.\nThe reaction force of the wall on the stick is per-\npendicular to the stick. The stick makes an angle of\n30◦with the wall and the bottom of the stick is on\na rough floor. The reaction of the wall on the stick\nis equal in magnitude to the reaction of the floor on\nthe stick. The ratio h/land the frictional force f at\nthe bottom of the stick are\n(\ng= 10 ms s2)\n(A) h\nl =\n√\n3\n16 ,f = 16\n√\n3\n3 N\n(B) h\nl = 3\n16 ,f = 16\n√\n3\n3 N\n(C) h\nl = 3\n√\n3\n16 ,f = 8\n√\n3\n3 N\n(D) h\nl = 3\n√\n3\n16 ,f = 16\n√\n3\n3 N\nA.4 Errors made by GPT-4\nHere we present some errors made by GPT-4.\nA.4.1 Conceptual Error\nThis is an example of a Conceptual Error\nWhat is the total number of distinct x ∈[0,1]\nfor which\n∫x\n0\nt2\n1+t4 dt= 2x−1?\nGPT-4 Response\nFirst, we can differentiate both sides of the equation\nwith respect to xto get rid of the integral. Using\nthe Fundamental Theorem of Calculus, we have:\nd\ndx\n(∫x\n0\nt2\n1+t4 dt\n)\n= d\ndx(2x−1)\nx2\n1+x4 = 2 This is an error because if two func-\ntions are equal at a point, that doesn’t imply that\ntheir derivatives are also equal.\nNow, we want to find the number of distinct\nsolutions for xin the interval [0,1]. Let’s first solve\nthe equation for x:\nx2 = 2(1 + x4)\nx2 −2x4 −2 = 0\nThis is a quartic equation, and we can try to find\nits roots. Notice that x= 0 is not a solution, so we\ncan divide both sides by x2:\n1 −2x2 −2x−2 = 0\nNow, let y= x2. Then, we have:\n1 −2y−2y−1 = 0\n2y2 −y+ 1 = 0\nThis is a quadratic equation in y. We can use the\nquadratic formula to find its roots:\ny= −(−1)±\n√\n(−1)2−4(2)(1)\n2(2)\ny= 1±√1−8\n4\nSince the discriminant is negative (1 −8 = −7),\nthere are no real roots for y. Therefore, there are\nno real solutions for xin the given equation.\nThus, the total number of distinct x∈[0,1] for\nwhich\n∫x\n0\nt2\n1+t4 dt= 2x−1 is 0 .\n7538\nFigure 9: Topic-wise distribution of problems in our dataset\nFigure 10: A Math problem\nA.4.2 Grounding Error\nThis is an example of a Grounding Error\nA plano-convex lens is made of a material of\nrefractive index n. When a small object is placed\n30 cm away in front of the curved surface of the\nlens, an image of double the size of the object\nis produced. Due to reflection from the convex\nsurface of the lens, another faint image is observed\nat a distance of 10 cm away from the lens. Which\nof the following statement(s) is(are) true?\n(A) The refractive index of the lens is 2.5\n(B) The radius of curvature of the convex surface\nis 45 cm\n(C) The faint image is erect and real\n(D) The focal length of the lens is 20 cm\nGPT-4 Response\nFigure 11: A Physics problem\nFirst, let’s consider the refraction of light through\nthe lens. Since the image formed is double the size\nof the object, we can say that the image distance\n(v) is twice the object distance (u). So, v= 2u=\n60 cm. This is an example of wrong grounding.\nThe equation generated is inconsistent with the\nsign convention which must be taken care of for\nthe other equations to be valid.\nNow, let’s use the lens maker’s formula to find\nthe focal length ( f) of the lens: 1\nf = ( n −\n1)\n(\n1\nR1\n− 1\nR2\n)\nSince the lens is plano-convex, one surface is\nflat, and its radius of curvature (R1) is infinite. The\nradius of curvature of the convex surface ( R2) is\nnegative because it is convex. So, the formula\nbecomes: 1\nf = (n−1)\n(\n0 − 1\n−R2\n)\n1\nf = (n−1) 1\nR2\n7539\nFigure 12: A Chemistry problem\nNow, let’s use the lens formula to relateu, v, and\nf: 1\nf = 1\nu + 1\nv\nSubstituting the values of uand v: 1\nf = 1\n30 +\n1\n60 = 3\n60 = 1\n20\nNow, we can substitute the expression for1\nf from\nthe lens maker’s formula: (n−1) 1\nR2\n= 1\n20\nNow, let’s consider the reflection of light from\nthe convex surface of the lens. Since the faint image\nis formed at a distance of 10 cm from the lens,\nwe can use the mirror formula to relate the object\ndistance ( u), image distance ( v′), and radius of\ncurvature (R2) for the convex surface: 1\nu + 1\nv′ = 2\nR2\nSubstituting the values ofuand v′: 1\n30 + 1\n10 = 2\nR2\n1\n6 = 2\nR2\nNow, we can solve for R2: R2 = 2\n1\n6\n= 12 cm\nNow, we can substitute the value ofR2 back into\nthe equation relating nand R2: (n−1) 1\n12 = 1\n20\nSolving for n: n= 1 + 12\n20 = 1 + 3\n5 = 8\n5 = 1.6\nNow, let’s analyze the given options:\n(A) The refractive index of the lens is 2.5 This\nis false, as we found the refractive index to be 1.6.\n(B) The radius of curvature of the convex surface\nis 45 cm This is false, as we found the radius of\ncurvature to be 12 cm.\n(C) The faint image is erect and real This is true.\nSince the faint image is formed due to reflection\nfrom the convex surface, it behaves like a convex\nmirror. For a convex mirror, the image is always\nerect and virtual.\n(D) The focal length of the lens is 20 cm This is\nfalse, as we found the focal length to be 1\n20 , which\nis 0.05 cm.\nFinal Answer: (C) The faint image is erect and\nreal.\nA.4.3 Computation Error\nHere is an example of a computation error.\nTwo inductors L1 (inductance 1mH, internal re-\nsistance 3Ω ) and L2 (inductance 2mH, internal\nresistance 4Ω ), and a resistor R(resistance 12Ω )\nare all connected in parallel across a 5 V battery.\nThe circuit is switched on at time t= 0. What is\nthe ratio of the maximum to the minimum current\n(Imax/Imin) drawn from the battery?\nGPT-4 Response\nStep 1: Analyze the circuit at t=0 At t=0, the induc-\ntors behave like short circuits (since they have not\nhad time to build up any magnetic field). There-\nfore, the current through each inductor is deter-\nmined by their internal resistances. Let’s call the\ncurrents through L1, L2, and Ras I1, I2, and IR\nrespectively. Using Ohm’s law, we can find these\ncurrents:\nI1 = 5V\n3Ω = 5\n3 A\nI2 = 5V\n4Ω = 5\n4 A\nIR = 5V\n12Ω = 5\n12 A\nThe total current drawn from the battery at t=0\nis the sum of these currents:\nImax = I1 + I2 + IR = 5\n3 + 5\n4 + 5\n12 = 20\n12 A\nWrong calculation, should be 40\n12\nStep 2: Analyze the circuit at steady state As\ntime goes to infinity, the inductors will behave\nlike open circuits (since their magnetic fields have\nreached their maximum and no more current can\nflow through them). In this case, the only current\nflowing in the circuit is through the resistor R. Us-\ning Ohm’s law, we can find this current:\nImin = 5V\n12Ω = 5\n12 A\nStep 3: Find the ratio of maximum to minimum\ncurrent Now we can find the ratio of the maximum\nto the minimum current:\nImax\nImin\n=\n20\n12 A\n5\n12 A = 20\n5 = 4\nSo the ratio of the maximum to the minimum\ncurrent drawn from the battery is 4.\n7540\nA.5 GPT-4 verifying its own response\nFigures 13, and 14 demonstrate various cases\nwhich arise when GPT-4 critiques its own output.\nFigure 13: GPT-4 identifying a correct solution as cor-\nrect.\nA.6 GPT-4 deciding when to attempt\nTo evaluate if GPT-4 could decide whether to refuse\nattempt given the negative penalty for being wrong,\nwe prompt it with the following for Single-Correct\nMCQ questions. If the answer is wrong,\nyou’ll be given -1 marks. If the answer\nis correct, you’ll be given +3 marks.\nIf you’re unsure of the answer, you can\nskip the question, and you’ll be given 0\nmarks.\nThe following is used for Multi-Correct MCQs\nIf any of the options in the final answer\nis wrong, you’ll be given -2 marks. If\nall the options are correct, you’ll be\ngiven +4 marks. If some of the options\nare correct, you’ll be given +1 for each\ncorrect option. If you’re unsure of the\nanswer, you can skip the question, and\nyou’ll be given 0 marks.\nFigure 14: GPT-4 identifying a correct step as wrong\nand making it incorrect\nA.7 Can GPT-4 reason spatially\nFigures 15 and 16 show an example problem\nfor which GPT-4’s response indicates inability to\nground physical concepts spatially.\nA.8 Is JEEB ENCH contaminated?\nWe note that several web-pages on the internet do\nprovide problems and solutions to the JEE exam\n(for example, here). Therefore, it is possible to\nfind the question and the answer during web-scale\ntraining. Unfortunately, this would be true for\nany dataset that has been created from questions\nfrom competitive exams such as (Hendrycks et al.,\n2021b).\nTo check if the dataset is indeed contaminated,\nwe perform a careful analysis along the following\n7541\nFigure 15: Error made by GPT-4 in understanding phys-\nical concepts. In this example, correct form of equation\n1 should be f = NA cos 30◦. GPT-4 fails to spatially\nground the concept of direction in a 2D environment.\naxes:\n1. We search the C4 dataset (Dodge et al., 2021)\nusing 50 randomly sampled questions from\nPhysics and Chemistry(we do not use Math\nquestion since they are generally more LaTeX\nheavy and exact matches would be harder to\nfind). We were unable to find any documents\ncontaining sufficiently long substrings from\nthe questions of the dataset.\n2. We search the Common Crawl URL Index\nhttp://urlsearch.commoncrawl.org/\nCC-MAIN-2023-23/ for several popular\nsources which release consolidated full length\nsolutions to exams from the JEE exams. Out\nof 500+ questions in our dataset, we could\nfind web pages containing the solution of 30\nquestions [19 chemistry, 11 math] in the latest\nCommon Crawl 2023 index. This is less than\n6% of the dataset. We study the effects this\nhas on the final evaluation and observe that\nremoving these questions results in only a\nFigure 16: Error made by GPT-4 in understanding which\ncurves to take area between. Here it is taking area be-\ntween the curve and x-axis. However, the question in-\ntended between curve and the x= yline. This indicates\nthat GPT-4 might be weak in 2D reasoning from purely\ntext-based prompts.\nminor decrease in overall scores for most\nmodels. This suggests that the contamination\ndoesn’t impact the scores greatly. For\ninstance, GPT4+CoT performance goes\ndown from 0.350 to 0.347. The average\nperformance on the 30 questions is 0.392\n(this could also be attributed to a much higher\nproportion of chemistry questions which\nGPT-4 is better at). It is noteworthy that even\nthough these URLs are present in Common\nCrawl, that doesn’t imply they have been\ntrained on since LLM training doesn’t even\ncomplete 1 epoch over the pretraining data\ngenerally.\n3. Since proprietary LLMs do not disclose their\nexact data recipe, we wish to see if GPT-\n4 memorized the questions present in the\ndataset? Note that this is a very challenging\nproblem in itself. Inspired by contemporary\nmethods 6 we take the following heuristic ap-\nproach: we prompt GPT-4 with a prefix of\nthe question and instruct it to complete the\nremaining question providing context of the\nyear (for eg, Complete this question from JEE\nAdvanced 2017). In this, we check if the\n6https://github.com/hitz-zentroa/lm-contamination\n7542\nmodel is able to generate “precarious” data,\nthat is, data which cannot be predicted from\nthe context. For example, some numerical\ndata provided, or the options of the questions.\nWe use the same 50 questions sampled above\nand prompt GPT-4. Our evaluation suggested\nGPT-4 was unable to generate any such re-\nsponses.\nThese facts suggest that the extent of contamina-\ntion, if any, is very low and that is not detrimental\nto the paper. We will add these analyses in the final\npaper to clear any aspersions regarding possible\ncontamination.\nWe additionally want to emphasize that our\nbenchmark is dynamic in the sense that a new set\nof 40-50 new uncontaminated problems can be\nadded to it annually. This would allow a reliable\ntest of the problem solving abilities of future LLMs\nas more and more data goes into training them.\n7543",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.8865666389465332
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.7724894285202026
    },
    {
      "name": "Computer science",
      "score": 0.5766576528549194
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5275437831878662
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42235690355300903
    },
    {
      "name": "Management science",
      "score": 0.3288758397102356
    },
    {
      "name": "Mathematics",
      "score": 0.1614874303340912
    },
    {
      "name": "Engineering",
      "score": 0.13123342394828796
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}