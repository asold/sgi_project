{
  "title": "Inferring Implicit Relations in Complex Questions with Language Models",
  "url": "https://openalex.org/W4385573400",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5072216674",
      "name": "U. Katz",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A5065717258",
      "name": "Mor Geva",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A5045872048",
      "name": "Jonathan Berant",
      "affiliations": [
        "Tel Aviv University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2889787757",
    "https://openalex.org/W3172267148",
    "https://openalex.org/W4287393336",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W2949134692",
    "https://openalex.org/W4221143736",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W3175910413",
    "https://openalex.org/W3172335055",
    "https://openalex.org/W2964120615",
    "https://openalex.org/W3105725765",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W3098323839",
    "https://openalex.org/W2963866616",
    "https://openalex.org/W4306753761",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W4287898313",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3212198148",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3202673127",
    "https://openalex.org/W3171639130",
    "https://openalex.org/W3100436891",
    "https://openalex.org/W4287758766",
    "https://openalex.org/W4286903249",
    "https://openalex.org/W3196886373",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3166444100",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2027047406",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3201663597",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3197499505",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2962977085"
  ],
  "abstract": "A prominent challenge for modern language understanding systems is the ability to answer implicit reasoning questions, where the required reasoning steps for answering the question are not mentioned in the text explicitly. In this work, we investigate why current models struggle with implicit reasoning question answering (QA) tasks, by decoupling inference of reasoning steps from their execution.We define a new task of implicit relation inference and construct a benchmark, IMPLICITRELATIONS, where given a question, a model should output a list of concept-relation pairs, where the relations describe the implicit reasoning steps required for answering the question.Using IMPLICITRELATIONS, we evaluate models from the GPT-3 family and find that, while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations.This suggests that the challenge in implicit reasoning questions does not stem from the need to plan a reasoning strategy alone, but to do it while also retrieving and reasoning over relevant information.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2548â€“2566\nDecember 7-11, 2022 Â©2022 Association for Computational Linguistics\nInferring Implicit Relations in Complex Questions with Language Models\nUri Katz1 Mor Geva2 Jonathan Berant1\n1The Blavatnik School of Computer Science, Tel-Aviv University\n2Allen Institute for Artificial Intelligence\n{uri.katz,joberant}@cs.tau.ac.il, morp@allenai.org\nAbstract\nA prominent challenge for modern language un-\nderstanding systems is the ability to answer im-\nplicit reasoning questions, where the required\nreasoning steps for answering the question are\nnot mentioned in the text explicitly. In this\nwork, we investigate why current models strug-\ngle with implicit reasoning question answer-\ning (QA) tasks, by decoupling inference of rea-\nsoning steps from their execution. We define\na new task of implicit relation inference and\nconstruct a benchmark, IMPLICIT RELATIONS ,\nwhere given a question, a model should out-\nput a list of concept-relation pairs, where the\nrelations describe the implicit reasoning steps\nrequired for answering the question. Using IM-\nPLICIT RELATIONS , we evaluate models from\nthe GPT-3 family and find that, while these\nmodels struggle on the implicit reasoning QA\ntask, they often succeed at inferring implicit re-\nlations. This suggests that the challenge in im-\nplicit reasoning questions does not stem from\nthe need to plan a reasoning strategy alone, but\nto do it while also retrieving and reasoning over\nrelevant information.\n1 Introduction\nA longstanding goal of language understanding has\nbeen to develop systems that can reason, i.e., inte-\ngrate multiple pieces of information to reach a con-\nclusion (McCarthy, 1959; Clark et al., 2021). This\nhas sparked interest in question answering (QA)\nbenchmarks that require such reasoning (Welbl\net al., 2018; Yang et al., 2018; Talmor and Berant,\n2018). One particularly challenging case is ques-\ntions that require implicit reasoning, that is, where\nthe evidence for answering the question is not men-\ntioned explicitly. Consider the question â€œDoes\nSanta Claus work during summer?â€. This question\nrequires implicit reasoning since it involves know-\ning when the holiday associated with Santa occurs,\nbut this is not evident from the question.\nRecent advances in QA (Tafjord and Clark, 2021;\nLourie et al., 2021) have steered attention towards\nWould a stool be useful for a \nLusotitan to reach the top of an almond tree?\nâŸ¨Lusotitan , heightâŸ© âŸ¨almond tree , heightâŸ©\nâ€œThe adult Lusotitan \nwas over 25 \nmeters highâ€\nâ€œAlmond trees can \ngrow up to 10 \nmeters highâ€  >\nFALSE\nImplicit \nrelations\nRetrieval / \nReasoning\nAnswer\nQuestion\nFigure 1: An example implicit reasoning question ,\nwhere answering requires inferring implicit relations\nthat are not explicitly mentioned. Besides implicit re-\nlations, answering the question also requires reasoning\nover the relevant retrieved facts. In this work, we focus\non the first step of inferring implicit relations.\nimplicit reasoning QA benchmarks such as STRAT-\nEGY QA (Geva et al., 2021), OPEN CSR (Lin et al.,\n2021), COMMONSENSE QA 2.0 (Talmor et al.,\n2021), CREAK (Onoe et al., 2021), and REAL FP\n(Kalyan et al., 2021), which span a wide range of\ndomains and reasoning skills. Still, implicit rea-\nsoning remains an open challenge, even for large\nlanguage models (LMs) such as GPT-3 and PaLM\n(BIG-bench collab., 2021; Talmor et al., 2021; Rae\net al., 2021; Chowdhery et al., 2022).\nAnswering implicit reasoning questions can be\nviewed as a two-step process: (a) inferring simple\nsub-questions necessary for answering the question,\nand (b) retrieving the relevant knowledge pieces\n(i.e., answering sub-questions) and reasoning over\nthem to derive the answer. Figure 1 illustrates\nthis decoupling. To answer the shown question,\nwe need to use knowledge about the Lusotitan di-\nnosaur and almond trees to infer that the relevant\nsub-questions concern their heights. We refer to the\nrelation height, which is not mentioned in the ques-\ntion as an implicit relation. Once implicit relations\nare inferred, we can retrieve the relevant facts and\ndeduce that the answer is â€˜Falseâ€™, as a Lusotitan is\n2548\nSource Dataset Question and Implicit Relation Pairs Answer\nSTRATEGY QA\nDid the 40th president of the United States forward lolcats to his friends?\nâŸ¨40th president of the United States, year of deathâŸ©, âŸ¨lolcats, year of creationâŸ© False\nCould $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?\nâŸ¨2009 eclipse, numberâŸ©, âŸ¨TIME magazine in 2020, retail priceâŸ© True\nCREAK\nAziz Ansari has never performed in front of a crowd.\nâŸ¨Aziz Ansari, professionâŸ© False\nPantera made music with distorted guitars.\nâŸ¨Pantera, music genreâŸ© True\nCSQA2.0\nNone of the mail in a personâ€™s Yahoo inbox has a stamp on it.\nâŸ¨Yahoo inbox, type of mailboxâŸ© True\nIf you play a cello you cannot join the marching band.\nâŸ¨cello, playing postureâŸ© True\nTable 1: Example annotations of concept-relation pairs from IMPLICIT RELATIONS along with the question source\ndataset and answer. Each source exhibits different facets of implicit reasoning questions.\nmuch higher than an almond tree.\nIn this work, we put a spotlight on implicit rela-\ntions and investigate the ability of language models\nto infer them as a necessary (albeit insufficient) step\nfor answering implicit reasoning questions. We\nfirst define implicit relations, and show that they\ncan be reliably annotated through crowdsourcing\n(example annotated implicit relations are in Fig-\nure 1 and Table 1). To show implicit relations are\ncommon, we curate and annotate implicit reason-\ning questions from three existing datasets, STRAT-\nEGY QA, CREAK , and COMMONSENSE QA 2.0 ,\nwhich results in IMPLICIT RELATIONS , a new eval-\nuation benchmark containing 615 questions and\n2,673 annotations of implicit relations.\nWe use our benchmark to evaluate the ability of\nlarge LMs to infer implicit relations, since they are\nknown to acquire substantial amounts of knowl-\nedge and common sense with scale (Roberts et al.,\n2020; Liu et al., 2021; Smith et al., 2022), but strug-\ngle with implicit reasoning questions. Specifically,\nwe evaluate models from the GPT-3 family using\nin-context learning, where the model is fixed and\nonly a few examples are given as context.\nWe find that large LMs perform well on this task,\nwith a 175B parameter model recovering 0.53-0.59\nof the implicit relations across datasets, outper-\nforming a baseline by 21-40 points. This is robust\nacross methods for sampling in-context examples,\nand even in cross-data scenarios where in-context\nexamples are sampled from a different dataset than\nthe target question. However, inferring implicit\nrelations does not improve accuracy on the down-\nstream QA task, even when gold relations are pro-\nvided. This suggests that the challenge of implicit\nreasoning questions is not primarily due to implicit\nrelation inference, but possibly due to the need to\nalso retrieve information and reason over it.\nTo conclude, in this work we propose the notion\nof implicit relations, and construct the IMPLIC -\nITRELATIONS evaluation benchmark for testing\nthe ability of models to infer them from questions.\nWe evaluate large LMs and show that they infer im-\nplicit relations fairly well, while still falling short\nof answering implicit reasoning questions. Our\nwork facilitates future work on improving implicit\nrelation inference, and sheds light on the factors\nrelevant for developing models that can handle im-\nplicit reasoning. From a broader perspective, our\nwork joins recent community efforts to highlight\nthe ubiquity of missing and implicit elements in\nnatural language (Cheng and Erk, 2018; Pyatkin\net al., 2020; Elazar et al., 2022). 1\n2 Implicit Relations\nWe now define the notion of implicit relations in\nthe context of complex question answering.\nComplex questions are questions that require\nmultiple steps of reasoning in order to be answered\n(Yang et al., 2018; Talmor and Berant, 2018; Welbl\net al., 2018; Khot et al., 2021). For example,\nthe question â€œWas Linnaeus alive when Darwin\npublished Origin of Species?â€ involves fetching\ntwo dates and then comparing them (Figure 2).\nA prominent challenge in complex QA, that at-\ntracted substantial attention recently (Mihaylov\net al., 2018; Khashabi et al., 2020; Lin et al., 2021;\n1Our benchmark IMPLICIT RELATIONS and relevant\ncode can be downloaded from github.com/katzurik/\nImplicitRelations.\n2549\nDid Linnaeus edit \nDarwin's draft of Origin \nof Species?\nWas Linnaeus alive \nwhen Darwin published \nOrigin of Species?\ns1  When did Carl \nLinnaeus pass away?\ns2  When was Origin of \nSpecies first published?\ns3  Is #2 before #1?\nImplicit Reasoning Explicit Reasoning\nDecomposition  D Implicit Relations  I\nâŸ¨Origin of Species ,   \n year publishedâŸ©\nâŸ¨Linnaeus , \n year of deathâŸ©i1\ni2\nRetrieval\nRetrieval\nLogical\nFigure 2: An example of explicit and implicit reasoning\nquestions that share the same question decomposition,\nalong with the implicit relations derived from the re-\ntrieval steps in the decomposition.\nGeva et al., 2021; Yasunaga et al., 2021; Wei et al.,\n2022), is cases where the reasoning steps are im-\nplicit and should be inferred from the question. For\ninstance, â€œDid Linnaeus edit Darwinâ€™s draft of Ori-\ngin of Species?â€ involves the same reasoning steps,\nbut they are not mentioned explicitly. Thus, the\nformer question is an explicit reasoning question,\nwhile the latter is an implicit one (Figure 2).\nWolfson et al. (2020) proposed QDMR as a\nmeaning representation for complex questions,\nwhere a complex question qis decomposed into a\nsequence of mreasoning steps D= (s1,...,s m),\nand each step si corresponds to a simple natural\nlanguage question. Answering the simple questions\none-by-one yields the final answer (see decompo-\nsition in Figure 2). Geva et al. (2021) collected\ndecompositions for implicit reasoning questions as\npart of the STRATEGY QA dataset, where impor-\ntantly, inferring the sequence of reasoning steps\nis challenging due to their implicit nature. In ad-\ndition to generating effective decompositions for\nimplicit reasoning questions, we find an additional\nchallenge in evaluating these decompositions when\nrepresented as a sequence of sub-questions. Specif-\nically, Geva et al. (2021) distinguished two types\nof reasoning steps in a decomposition â€“ retrieval\nsteps, which require retrieval of facts (s1 and s2 in\nFigure 2), and logical steps, which perform logical\nreasoning over previous results (s3 in Figure 2).\nIn this work, we observe that a key ingredient\nin inferring decompositions is to identify the im-\nplicit relations that are necessary for answering the\nquestion. Concretely, each retrieval step in a ques-\ntion decomposition can typically be represented as\nconcept-relation pair âŸ¨c,râŸ©, where cis a sequence\nof tokens from the question that refer to a concept,\nand r is a relation of that concept. For example,\nthe concept cin step s2 in Figure 2 is â€œOrigin of\nSpeciesâ€, and the relation ris its publication year.\nBased on this observation, we provide the follow-\ning definition for implicit relations in complex QA.\nLet qbe an implicit reasoning question, and denote\nby D = (s1,...,s m) its decomposition into a se-\nquence of mreasoning steps. Let {si1 ,...,s in }\nbe the subset of retrieval steps in the decomposi-\ntion D. We define the implicit relations for an-\nswering qas the set I= {âŸ¨c1,r1âŸ©,..., âŸ¨cn,rnâŸ©}of\nconcept-relation pairs, where each concept-relation\npair corresponds to a particular retrieval step.\nIn the next sections, we will use this definition\nto construct a task for probing the ability of models\nto infer implicit relations (Â§3-Â§5), and investigate\nwhy they struggle on the downstream QA task (Â§6).\n3 The I MPLICIT RELATIONS Benchmark\nIn this section, we describe the process of creating\nIMPLICIT RELATIONS , a benchmark for evaluating\nthe ability of models to infer implicit relations.\n3.1 Data Collection\nWe curate questions that require inferring implicit\nrelations from three recent datasets:\nâ€¢ STRATEGY QA (Geva et al., 2021): A dataset of\nyes/no questions that require implicit multi-step\nreasoning. STRATEGY QA (STGQA) questions\ncan be answered from Wikipedia, and are diverse\nin terms of the required reasoning skills and ques-\ntion topic. Large language models, such as GPT-\n3 (Brown et al., 2020), were shown to struggle\non STGQA (BIG-bench collab., 2021).\nâ€¢ CREAK (Onoe et al., 2021): A dataset containing\ntrue/false statements that require common sense\nand knowledge about real-world entities.\nâ€¢ CSQA2 (Talmor et al., 2021): A dataset contain-\ning yes/no questions and true/false statements.\nCSQA2 questions involve generic commonsense\nreasoning. Most questions do not require knowl-\nedge about particular entities.\nExamples from each dataset are shown in Table 1.\nCollecting questions from three sources serves\ntwo purposes. First, it demonstrates that infer-\nring implicit relations is necessary for many ques-\ntion types: in multi-step questions ( STGQA) and\nsingle-step questions (CREAK ), in entity-focused\n2550\nquestions ( CREAK ) and generic common sense\nquestions (CSQA2 ). Second, these datasets were\ncreated using different protocols: STGQA and\nCSQA2 use a model-in-the-loop during data col-\nlection, while CREAK does not. STGQA and\nCREAK ask annotators to author questions freely,\nwhile CSQA2 employs a gamification mechanism.\nHaving questions from different data collection\npipelines increases the likelihood that empirical\nconclusions are not tied to a particular dataset.\nQuestion curation We chose questions that sat-\nisfy two properties: (a) answering the question\nrequires inferring an implicit relation, and (b) the\nquestion is feasible, that is, it can be answered us-\ning real-world facts (provided as part of the bench-\nmark in STGQA and CREAK ) or using generic com-\nmon sense (in CSQA2 ). We sampled examples\nfrom the training set of each of the three datasets\nand kept questions that satisfy the two properties.\nAnnotating implicit relations We use Amazon\nMechanical Turk to annotate implicit relations over\ncurated questions. We qualify 15 crowdworkers to\nidentify concepts, which are token sequences from\nthe question, relevant for answering the question,\nand the corresponding relation for each concept\n(see annotation guidelines in Appendix C). Anno-\ntators can specify up to four concept-relation pairs\nper question, but in practice, 98.9% consist of â‰¤2\npairs. Concepts must be extracted directly from\nthe input questions, and relations are phrased using\nconcise natural language phrases.\nFor STGQA and CREAK , which often require\nuncommon knowledge about entities, we provided\nadditional context from the original data source.\nFor STGQA, we provided facts along with the full\nquestion decomposition. For CREAK , we provided\nan explanation for why the claim is true or false.\nWe collected 5 annotations per example in\nCREAK and CSQA2 , and 3 annotations inSTGQA.\nDue to the availability of facts and question de-\ncompositions, STGQA showed high agreement\nbetween annotators (see Table 2). CREAK and\nCSQA2 showed more variability, and thus, we col-\nlected more annotations per example. To ensure\nquality, we manually verified all examples created\nduring the annotation process, filtering out annota-\ntions that do not fit the task requirements.\n3.2 Data Analysis\nTable 2 provides statistics on the collected data.\nIMPLICIT RELATIONS consists of 615 annotated\nSTGQA C REAK CSQA2\n# of questions 201 205 209\n# of unique concepts 399 309 303\n# of concept-relation pairs 1139 1272 1285\nConcept agreement 87% 71% 74%\nRelation agreement:\nLexical variability 85% 65% 80%\nMultiple reasoning paths 15% 35% 20%\nTable 2: Statistics on IMPLICIT RELATIONS .\nquestions, âˆ¼200 per dataset, where exactly 100\nexamples from each data source are used as a test\nset, and the rest are used as a development set.\nDevelopment set examples are in Appendix A.\nAnnotator Agreement We manually analyzed\n20 random examples from each data source to eval-\nuate agreement between annotators for concepts\nand relations. We declared concept agreement\nwhen at least three annotators identified the same\nconcept in an example. We found that annotators\nagreed on 77% of the concepts, and less than 10%\nof concepts were extracted only by a single annota-\ntor. See Table 2 for break-down by data source.\nAssessing agreement on relations is more chal-\nlenging, since relations are short phrases that can\ndiffer lexically. We marked for each example\nwhether annotated relations differed only lexically\nor due to multiple reasoning strategies for answer-\ning the question. In 76% of the examples, the\nrelations across all annotators were identical or dif-\nfered lexically, e.g., the relations â€œaverage priceâ€\nand â€œcostâ€. In 24% of the examples, multiple rea-\nsoning strategies were used, which would result in\ndifferent reasoning and retrieval steps (see Table 2).\nOverall, our analysis suggests that implicit rela-\ntions can be annotated reliably.\n4 Experimental Setting\nWe now turn to evaluate the ability of large LMs\nto infer implicit relations in questions. To this end,\nwe use examples from IMPLICIT RELATIONS in a\nfew-shot in-context learning setting (Brown et al.,\n2020), where given several input-output examples\nand a test input, the LM is expected to generate the\nrequired output. We focus on this setup following\nthe recent progress in in-context learning, specifi-\ncally for tasks that involve general commonsense\nreasoning (Da et al., 2021; Chowdhery et al., 2022).\n2551\n4.1 Task and Model Specification\nGiven a test question qâˆ—, we prepend k an-\nnotated examples {âŸ¨q(i),I(i)âŸ©}k\ni=1 to it, where\nq(i) is the i-th question in this set and I(i) =\n{âŸ¨c(i)\n1 ,r(i)\n1 âŸ©,..., âŸ¨c(i)\nni ,r(i)\nni âŸ©}is the set of correspond-\ning concept-relation pairs. Specifically, we use the\nfollowing input format:\nQuestion: q1\nImplicit Reasoning: âŸ¨c(1)\n1 ,r(1)\n1 âŸ©,..., âŸ¨c(1)\nn1 ,r(1)\nn1 âŸ©\n...\nQuestion: qk\nImplicit Reasoning: âŸ¨c(k)\n1 ,r(k)\n1 âŸ©,..., âŸ¨c(k)\nnk ,r(k)\nnk âŸ©\nQuestion: qâˆ—\nImplicit Reasoning:\nExample inputs are given in Appendix A. The pre-\nfixes â€œQuestionâ€ and â€œImplicit Reasoningâ€ were\nchosen arbitrarily and remained fixed throughout\nall experiments. For each test question, kexamples\nare randomly sampled from the development set,\nand for each example, a single random annotation\nis selected. In our experiments, we use k= 16.2\nModels We evaluate models from the GPT-3 fam-\nily,3 which are known to exhibit broad factual\nknowledge (Brown et al., 2020). In particular, we\nuse text-davinci-002, a 175B-parameter LM,\nwhich to the best of our knowledge was not trained\non any of the target benchmarks. Furthermore, to\nassess the scaling behaviour of models, we exper-\niment with other models from the GPT-3 family,\nsee Â§5.2 for details. In all experiments, outputs are\npredicted using greedy decoding.\nBaseline To account for correlations originating\nfrom the concepts that appear in the question or rea-\nsoning shortcuts, we define a â€˜Concept-onlyâ€™ base-\nline, where instead of testing whether the model\ncan infer the implicit relations from the full ques-\ntion, we test its ability to infer them from the set\nof gold concepts that appear in the question. For\nthis baseline, we use the same inputs as before, but\nreplace every question qi and test question qâˆ— with\nits set of annotated concepts.\nQuestion: qi â†’ Question: c(i)\n1 ; ...; c(i)\nni\nWhile the identity of the gold concepts provides\nuseful information for inferring implicit relations,\n2We experimented with k âˆˆ{8, 16, 32} but found it does\nnot dramatically change performance, see Appendix B.\n3We use the API at https://openai.com/api/.\nQuestion:     Are two cans of Campbell's Soup a day good for hypertension?\n(1) Concept \nAlignment\nSimilarity: 0.88 > ğœ\n(2) Relation \nMatching\nSimilarity: 0.38 < ğœ\n    \n        âŸ¨hypertension, nutritional causesâŸ© , âŸ¨Campbell's Soup , ingredientsâŸ©\n    Prediction:\n    âŸ¨hypertension , dietary causesâŸ© , âŸ¨campbell's soup , sodium contentâŸ© \nLabels:   \nRelation match = 0.5\nAre two cans of Campbell's Soup a day good for hypertension?\n(1) Concept \nAlignment\nSimilarity: 0.88 > ğœ\n(2) Relation \nMatching\nSimilarity: 0.38 < ğœ\nP:\nLabels:   \nRelation coverage = 0.5\ncampbell's \nsoup\nsodium\ncontent âŸ¨ âŸ©,hypertension dietary \ncauses âŸ¨ âŸ©,\nCampbell's \nSoup ingredientsâŸ¨ âŸ©,hypertension nutritional \ncausesâŸ¨ âŸ©,\nQ:\nG:\n(1)\n(2)\n(3)\nFigure 3: Given predicted concept-relation pairs (P) and\ngold concept-relation pairs (G), we evaluate relations\nby aligning predicted and gold concepts using edit dis-\ntance (1), and computing the cosine similarity between\nmatched relation embeddings (2). Relation coverage is\nthe fraction of gold relations with similarity >Ï„ (3).\nwe expect models that have access to the full ques-\ntion to perform better.\n4.2 Evaluation\nInferring implicit relations involves identifying con-\ncepts and relations. We now define evaluation met-\nrics for this task.\nConcept extraction Our output is a set of conept-\nrelation pairs. Let Cpred be the set of concepts pre-\ndicted by the LM, and let Ci\ngold be the gold set\nannotated by annotator i. Given that annotated con-\ncepts are tokens from the original question, we can\nuse edit-distance to match each predicted concept\nc âˆˆCpred with a concept from Ci\ngold, declaring a\nmatch when the edit distance of the best match\nis above 0.8. Following concept matching, we\ncompute recall and precision in the typical fashion\nand take the max over all annotators. A post-hoc\nmanual analysis validated that no incorrect concept\nmatching occurred due to the use of edit distance.\nRelation coverage Since relations are short\nphrases with high lexical variability, string match-\ning is not a viable strategy. To overcome this, we\nleverage our ability to align concepts and use rela-\ntion embeddings rather than relation strings. Fig-\nure 3 depicts our evaluation procedure for two sets\nof predicted (Rpred) and annotated (Rgold) relations.\nFirst (Figure 3, step 1), we align predicted and\ngold concept-relation pairs, using the concepts (as\ndone for concept evaluation). Then (Figure 3, step\n2), we embed every relation phrase, using the sen-\ntence transformer all-mpnet-base-v2 (Reimers\nand Gurevych, 2019), and compute cosine simi-\nlarity between the embeddings of matched rela-\n2552\nSTGQA C REAK CSQA2\n# of gold pairs 1.9 1.2 1.2\n# of generated pairs 2.0 Â±0.04 1 .2 Â±0.02 1 .2 Â±0.01\nTable 3: The mean number of concept-relation pairs in\nthe development set of each data source. For model-\ngenerated pairs, we report an average over 3 seeds.\nConcept Concept Relation\nRecall Precision Coverage\nSTGQA CO 0.99 Â±0.01 0.95 Â±0.02 0.32 Â±0.02\nFQ 0.97 Â±0.01 0.89 Â±0.02 0.53 Â±0.02\nCREAK CO 1 Â±0.0 0.96 Â±0.0 0.33 Â±0.03\nFQ 0.98 Â±0.0 0.95 Â±0.01 0.54 Â±0.05\nCSQA2 CO 1 Â±0.0 0.98 Â±0.02 0.19 Â±0.02\nFQ 0.93 Â±0.02 0.94 Â±0.01 0.59 Â±0.01\nTable 4: Test set performance for concept-only (CO)\nand full-question (FQ) for all datasets.\ntions (defined it as zero if no relation was matched).\nLast (Figure 3, step 3), we consider a gold relation\nrgold covered if cosine similarity is higher than a\nthreshold Ï„, and compute relation coverage, that\nis, the fraction of gold relations that were covered.\nWith this procedure, we evaluate model predictions\nagainst each annotation and take the maximum as\nthe final relation coverage.\nWe focus on coverage (rather than precision)\nsince we care about whether a model can reveal\nimplicit relations, but predicting additional ones\nis mostly harmless. Moreover, since we use in-\ncontext learning, the average number of concept-\nrelation pairs generated is similar to the average\nnumber of gold concept-relation pairs (Table 3).\nTo set a threshold Ï„ on relation embedding sim-\nilarity, we annotated whether a predicted relation\nis semantically equivalent to a matched gold rela-\ntion for 100 development examples. We choose\nÏ„ = 0.51, which results in a 5% false-positive rate\n(predicting that two different relations are equiva-\nlent) and 12% false-negative rate (predicting that\ntwo equivalent relations are different). All reported\nresults are an average over three random seeds.\n5 Large LMs Can Infer Implicit Relations\nTable 4 shows results on implicit relation inference.\nFirst, the model successfully identifies the relevant\nconcepts in the question, achieving high concept\nrecall and precision across all datasets. This is\nnot limited to named entities but is also achieved\nwhen concepts are more abstract, as in CSQA2\n(Table 1). More importantly, GPT-3 infers the im-\nplicit relations well, achieving relation coverage\nscores of 0.53, 0.54, and 0.59 on STGQA, CREAK\nand CSQA2 , respectively. Moreover, a model ex-\nposed to the full question dramatically outperforms\na model exposed only to the gold concepts by 21,\n21, and 40 points on the three datasets. This in-\ndicates that concepts contain relevant information,\nbut access to the full question allows the LM to\ninfer the reasoning strategy and in turn the implicit\nrelations. We provide examples for predicted vs.\ngold concept-relation pairs along with additional\nqualitative analysis in Appendix A.\nNext, we perform additional experiments to (a)\nfurther substantiate the ability of LMs to infer im-\nplicit relations (Â§5.1), and (b) test the effect of\nmodel scale on performance (Â§5.2).\n5.1 Effect of In-Context Examples\nWhile the aforementioned results are encouraging,\nthere are two potential (non-disjoint) causes for\nthem: (a) the LM â€œunderstandsâ€ the task of im-\nplicit relation inference, or (b) the LM observes in-\ncontext examples and uses them to guess implicit\nrelations for the target question (â€œsoft copyingâ€).\nWe study the effect of these causes.\nSimilar vs. dissimilar in-context examples To\nquantify the effect of in-context examples, rather\nthan choosing them randomly, we use examples\nthat are similar or dissimilar to the target question\nin terms of their implicit relations.\nWe first represent each example as an embed-\nding vector, by (a) concatenating all annotated re-\nlations, i.e., r1,...,r n, and computing a vector\nrepresentation using a sentence transformer, and\n(b) averaging the embedding vectors of all annota-\ntors. Then, for each example, we select two sets of\nin-context examples: (a) Similar: the top- kmost\nsimilar examples (using cosine similarity), and (b)\nDissimilar: we discard the 33% most similar exam-\nples, and randomly sample from the rest. In both\ncases, we use gold implicit relations at test time,\nand thus this experiment is for analysis only.\nTable 5 shows relation coverage for the different\nsets of in-context examples and the fraction of cases\nwhere one of the implicit relations predicted by the\nLM is copied from the in-context examples. When\nDissimilar examples are presented, there is a slight\nperformance degradation, most notably in STGQA.\nHowever, results are still dramatically higher com-\npared to Concept-only. Moreover, the model suc-\nceeds in predicting implicit relations while hardly\n2553\nRelation Copying\nCoverage\nSTGQA\nDissimilar 0.46 Â±0.01 0.01 Â±0.01\nRandom 0.52 Â±0.02 0.13 Â±0.02\nSimilar 0.54 Â±0.02 0.31 Â±0.02\nConcept-only 0.28 Â±0.04 0.10 Â±0.04\nCREAK\nDissimilar 0.60 Â±0.02 0.02 Â±0.02\nRandom 0.59 Â±0.03 0.08 Â±0.02\nSimilar 0.67 Â±0.02 0.33 Â±0.03\nConcept-only 0.34 Â±0.02 0.13 Â±0.02\nCSQA2\nDissimilar 0.63 Â±0.02 0.02 Â±0.02\nRandom 0.67 Â±0.01 0.07 Â±0.02\nSimilar 0.73 Â±0.01 0.21 Â±0.03\nConcept-only 0.20 Â±0.02 0.10 Â±0.02\nTable 5: Development set performance. Controlling\nthe set of in-context examples with Dissimilar, Similar,\nRandom relations, and Concept-only baseline.\ncopying from in-context examples.\nIn the Similar setting, performance increases\nacross all datasets, along with a much higher rate\nof copying. This hints that designing methods for\nretrieving similar prompts can lead to gains in per-\nformance (Rubin et al., 2022).\nTo further investigate the relation between copy-\ning and performance, we label every example for\nwhether the model copies from in-context exam-\nples and the coverage of the inferred implicit rela-\ntions. We then compute the point-biserial correla-\ntion (Tate, 1954) to check if copying is correlated\nwith performance and find that correlation is low\n(<0.1 for all datasets), showing that copying does\nnot explain model performance.\nOverall, this experiment suggests that while mod-\nels can leverage examples from context to improve\nperformance, the LM does more than copy and\nexecute implicit relation inference.\nCross-dataset in-context examples If LMs can\ninfer implicit relations, we should expect high per-\nformance even when in-context examples and tar-\nget questions are taken from different datasets.\nTo test this, we evaluate performance on ques-\ntions from CREAK and CSQA2 when in-context\nexamples originate from all 3 datasets. Testing\non STGQA does not work well because the num-\nber of implicit relations in an example is typically\ntwo, while in CREAK and CSQA2 it is typically\none (see Table 3), and thus the LM output a single\nimplicit relation, leading to poor relation coverage.\nTable 6 shows that, overall, relation coverage re-\nmains high for all sources, suggesting that the LM\nindeed infers implicit relations regardless of the\nInference/Context Concept Concept Relation\nSource Recall Precision Coverage\nCREAK /STGQA 0.97 Â±0.0 0.69 Â±0.01 0.62 Â±0.03\nCREAK /CREAK 0.93Â±0.02 0.91 Â±0.01 0.59 Â±0.03\nCREAK /CSQA2 0.93 Â±0.01 0.90 Â±0.02 0.60 Â±0.0\nCSQA2/S TGQA 0.98 Â±0.01 0.77 Â±0.02 0.62 Â±0.0\nCSQA2/C REAK 0.96Â±0.02 0.94 Â±0.02 0.59 Â±0.02\nCSQA2/CSQA2 0.95 Â±0.01 0.96 Â±0.0 0.67 Â±0.01\nTable 6: Development set performance in the cross-\ndataset setup where inference on example from one\ndataset is done with in-context examples from another\ndataset.\nquestion and reasoning types in the source dataset.\nConcept recall and precision are also relatively\nstable, except when using STGQA for in-context\nexamples, since the model tends to output two\nconcept-relation pairs, reducing precision. Thus,\nthe LM is sensitive tothe number of output concept-\nrelation pairs that appear in in-context examples,\nbut succeeds in inferring implicit relations.\n5.2 Effect of Model Size\nRecent work (Kaplan et al., 2020; Smith et al.,\n2022; Chowdhery et al., 2022) has shown that rea-\nsoning abilities of LMs improve with model size.\nWe evaluate this effect on models from the GPT-3\nfamily: ada, babbage, curie, and davinci, which\nare estimated to have 350M, 1.3B, 6.7B, and 175B\nparameters, respectively (Gao, 2021; Black et al.,\n2022). text-davinci, the model evaluated thus\nfar, is a more recent LM that (Ouyang et al., 2022)\nwas trained differently.4\nTable 7 presents results on STGQA. Increasing\nmodel size improves relation coverage and concept\nrecall, but does not significantly change concept\nprecision. Moving from curie to davinci leads\nto a modest gain in relation coverage. Comparing\nthis to the order of magnitude difference in param-\neters between curie and davinci suggests that\ninferring implicit relations does not explain perfor-\nmance improvement in many reasoning and com-\nmonsense QA benchmarks. The smallest model,\nbabbage, tends to produce structural errors, indi-\ncating it did not properly learn the task.\n4text-davinci has 175B parameters like davinci, but\nits relation coverage on STGQA is higher: 0.43â†’0.52. This\nindicates that its training procedure improves inference of\nimplicit relations.\n2554\nParameters Concept Concept Relation\nRecall Precision Coverage\n350M 0.83 Â±0.01 0.89 Â±0.01 0.21 Â±0.01\n1.3B 0.93 Â±0.01 0.84 Â±0.01 0.37 Â±0.01\n6.7B 0.92 Â±0.01 0.83 Â±0.02 0.42 Â±0.02\n175B 0.97 Â±0.0 0.88 Â±0.0 0.43 Â±0.03\nTable 7: Model size and performance comparison on\nthe development set of STGQA. The relation coverage\nimproves as model size is increased.\nStrategyQA Creak CSQA2\n60\n70\n80\n90Accuracy\nQuestion\nQuestion + predicted\nQuestion + gold\nFigure 4: Test QA accuracy under all conditions, aver-\naged over 7 seeds. Providing the gold implicit relations\ndid not contribute to model performance.\n6 Implicit Relations for QA\nGiven that LMs infer implicit relations well, a nat-\nural question is whether they improve performance\non answering implicit reasoning questions.\nTo examine this, we created three experimental\nsetups: Question + predicted: in-context examples\nare triples of the question, the implicit relations,\nand the True/False answer; the model is given a\nquestion and asked to return the implicit relations\nand the answer. Question + gold: Similar to Ques-\ntion + predicted except that the model is given\nthe target question and gold implicit relations and\nasked to return the answer. Question only: in-\ncontext examples are pairs of questions and an-\nswers, and the model is given a question and asked\nto provide the answer. We report an average over 7\nseeds. See Figure 4 for results.\nOverall, access to either gold or predicted rela-\ntions does not improve accuracy. This suggests\nthat additional capabilities are missing from LMs\nto handle implicit reasoning questions, such as\nretrieval and reasoning. This agrees with work\non chain-of-thoughts prompting (Wei et al., 2022),\nwhich found that adding an explanation of the rea-\nsoning process to a question does not improve per-\nformance on STGQA for both GPT-3 and LaMDA\n(Thoppilan et al., 2022). Nevertheless, recently\nChowdhery et al. (2022) achieved improvements\non STGQA using chain-of-thought prompting, but\nwith the larger 540B-parameter PaLM.\nOn STGQA, adding gold relations (Question +\ngold) does not improve QA performance.5 Addi-\ntionally, no significant differences were observed\nwhen the model inferred implicit relations on its\nown (Question + predicted).6 For CREAK , adding\ngold implicit relations did not improve accuracy\ncompared to Question only, and none of the experi-\nments showed any significant difference. Last, in\nCSQA2 adding gold implicit relations (Question\n+ gold) did not improve the QA performance, but\nwe observed a statistically significant accuracy im-\nprovement of 4.5% when the model inferred the\nimplicit relations (Question + predicted).7\nTo further analyze the results, we computed the\npoint-biserial correlation coefficient between the\nrelation coverage score and the binary outcome\n(correct/incorrect) for each question. We found\nthat relation coverage score and answer accuracy\nare entirely not correlated with a rpb coefficient of\n0.03, âˆ’0.02 and 0.06 for STGQA, CSQA2 and\nCREAK respectively. Overall, our results indicate\nthat inferring implicit relations correctly is not suf-\nficient to answer implicit reasoning questions.\n7 Related Work\nRecent work utilized the ability of large LMs to\ngenerate intermediate reasoning steps for improv-\ning performance on QA tasks (Wei et al., 2022;\nWang et al., 2022; Zelikman et al., 2022; Nye\net al., 2022). Wei et al. (2022) introduced â€˜chain-\nof-thoughtâ€™ prompting to elicit intermediate rea-\nsoning steps along with answers from LMs, which\nimproved performance on several reasoning tasks.\nConversely, we propose a task and benchmark for\nevaluating the ability of LMs to infer the interme-\ndiate reasoning steps themselves.\nPrior work has dealt with reasoning abilities in\nLMs (Talmor et al., 2020; Khashabi et al., 2020;\nGu et al., 2021) by fine-tuning LMs to generate\nadditional knowledge for reasoning questions. We\ncontribute to this effort by evaluating the in-context\nability to infer implicit relations with large LMs.\nImplicit relations are closely related to question\ndecomposition, which have been used in past work\n5paired t-test with p-value > 0.05.\n6paired t-test with p-value > 0.05.\n7paired t-test with p-value < 0.05, Cohenâ€™s d = 1.7.\n2555\nto improve performance on questions that require\nreasoning (Min et al., 2019; Wolfson et al., 2020;\nPerez et al., 2020; Khot et al., 2021). We contribute\nto this research direction by defining implicit re-\nlations pairs, which provide a structured represen-\ntation of the decomposed sub-questions and allow\nus to examine how language models infer reason-\ning steps. Several works in narrative understand-\ning (Rajpurkar et al., 2018; Mostafazadeh et al.,\n2020; Lal et al., 2021) have attempted to assess a\nmodelâ€™s implicit reasoning capabilities using dif-\nferent methods, such as assessing the solution path\nto unanswerable questions and and narrative under-\nstanding through question-answering. Despite their\ndifferent approaches, these studies are relevant to\nour cause.\n8 Conclusion\nWe propose the task of implicit relation inference,\nwhich decouples inference of reasoning steps from\ntheir execution. We introduce IMPLICIT RELA -\nTIONS , a benchmark that includes more than 2,000\nannotated implicit relations. We show large LMs\ncan infer implicit relations across multiple types\nof questions and reasoning skills, but this success\ndoes not translate to an improvement in answer-\ning implicit reasoning questions. Our work sheds\nlight on capabilities missing from large LMs for\naddressing implicit reasoning questions, and pro-\nvides a valuable resource for improving the ability\nof models to infer implicit relations.\nLimitations\nThis research has some limitations, which are typi-\ncal for work on text generation with large language\nmodels.\nFirst, we demonstrated that large LMs can infer\nimplicit relations from complex questions, but we\nalso showed that they may fail to answer those\nquestions correctly. It is unclear how LMs can\nuse implicit relations to improve QA accuracy or\nwhat is the path that leads from inferring implicit\nrelations to actually answering the questions.\nSecond, evaluating relation coverage requires\ncomparing to free texts, and therefore may be prone\nto error. Despite the fact that an analysis performed\nmanually exhibited a high degree of consistency\nwith the automatic one, we cannot guarantee the\nsame result for datasets or parameters that have not\nbeen tested.\nFinally, our research was conducted utilizing\nOpenAIâ€™s GPT-3 family of models, which are not\npublicly available. Despite our best efforts to elim-\ninate confounding factors, there is a lack of trans-\nparency regarding the training methods and the data\ncomposition used for pretraining those models.\nAcknowledgements\nWe thank Itay Levy for useful feedback. This re-\nsearch was partially supported by the Computer Sci-\nence Scholarship granted by the SÃ©phora Berrebi\nFoundation, the Yandex Initiative for Machine\nLearning, and the European Research Council\n(ERC) under the European Union Horizons 2020\nresearch and innovation programme (grant ERC\nDELPHI 802800).\nReferences\nBIG-bench collab. 2021. Beyond the imitation game:\nMeasuring and extrapolating the capabilities of lan-\nguage models. In preparation.\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit,\nLaria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of the ACL Workshop on Challenges & Perspec-\ntives in Creating Large Language Models.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877â€“1901.\nPengxiang Cheng and Katrin Erk. 2018. Implicit ar-\ngument prediction with event knowledge. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long Papers), pages 831â€“840, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2021.\nTransformers as soft reasoners over language. In Pro-\nceedings of the Twenty-Ninth International Confer-\nence on International Joint Conferences on Artificial\nIntelligence, pages 3882â€“3890.\n2556\nJeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and\nAntoine Bosselut. 2021. Analyzing commonsense\nemergence in few-shot knowledge models. arXiv\npreprint arXiv:2101.00297.\nYanai Elazar, Victoria Basmov, Yoav Goldberg, and\nReut Tsarfaty. 2022. Text-based np enrichment.\nTransactions of the Association for Computational\nLinguistics.\nLeo Gao. 2021. On the sizes of openai api models.\nEleutherAI Blog.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346â€“\n361.\nYuling Gu, Bhavana Dalvi, and Peter Clark. 2021.\nDream: Uncovering mental models behind language\nmodels. ArXiv, abs/2112.08656.\nAshwin Kalyan, Abhinav Kumar, Arjun Chan-\ndrasekaran, Ashish Sabharwal, and Peter Clark. 2021.\nHow much coffee was consumed during EMNLP\n2019? fermi problems: A new reasoning challenge\nfor AI. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7318â€“7328, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1896â€“1907, Online. Association\nfor Computational Linguistics.\nTushar Khot, Daniel Khashabi, Kyle Richardson, Peter\nClark, and Ashish Sabharwal. 2021. Text modular\nnetworks: Learning to decompose tasks in the lan-\nguage of existing models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1264â€“1279, Online.\nAssociation for Computational Linguistics.\nYash Kumar Lal, Nathanael Chambers, Raymond\nMooney, and Niranjan Balasubramanian. 2021.\nTellMeWhy: A dataset for answering why-questions\nin narratives. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n596â€“610, Online. Association for Computational Lin-\nguistics.\nBill Yuchen Lin, Haitian Sun, Bhuwan Dhingra, Manzil\nZaheer, Xiang Ren, and William Cohen. 2021. Dif-\nferentiable open-ended commonsense reasoning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4611â€“4625, Online. Association for Computa-\ntional Linguistics.\nJiachen Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter\nWest, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2021. Generated knowledge prompting\nfor commonsense reasoning. ArXiv, abs/2110.08387.\nNicholas Lourie, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Unicorn on rainbow: A\nuniversal commonsense reasoning model on a new\nmultitask benchmark. In AAAI.\nJohn McCarthy. 1959. Programs with common sense.\nIn Proceedings of the Teddington Conference on the\nMechanization of Thought Processes, pages 75â€“91,\nLondon. Her Majestyâ€™s Stationary Office.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381â€“2391, Brussels, Belgium. Association\nfor Computational Linguistics.\nSewon Min, Victor Zhong, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019. Multi-hop reading compre-\nhension through question decomposition and rescor-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6097â€“6109, Florence, Italy. Association for Compu-\ntational Linguistics.\nNasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon,\nDavid Buchanan, Lauren Berkowitz, Or Biran, and\nJennifer Chu-Carroll. 2020. GLUCOSE: Gener-\naLized and COntextualized story explanations. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4569â€“4586, Online. Association for Computa-\ntional Linguistics.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, Charles Sutton, and Augustus Odena.\n2022. Show your work: Scratchpads for interme-\ndiate computation with language models. In Deep\nLearning for Code Workshop.\nYasumasa Onoe, Michael JQ Zhang, Eunsol Choi, and\nGreg Durrett. 2021. CREAK: A dataset for common-\nsense reasoning over entity knowledge. In Thirty-\nfifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2).\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\n2557\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nEthan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun\nCho, and Douwe Kiela. 2020. Unsupervised question\ndecomposition for question answering. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n8864â€“8880, Online. Association for Computational\nLinguistics.\nValentina Pyatkin, Ayal Klein, Reut Tsarfaty, and Ido\nDagan. 2020. QADiscourse - Discourse Relations\nas QA Pairs: Representation, Crowdsourcing and\nBaselines. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 2804â€“2819, Online. Association for\nComputational Linguistics.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you donâ€™t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784â€“789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418â€“5426,\nOnline. Association for Computational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In North American Association for Compu-\ntational Linguistics (NAACL).\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nOyvind Tafjord and Peter Clark. 2021. General-\npurpose question-answering with Macaw. ArXiv,\nabs/2109.02593.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 641â€“651, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020. Leap-of-thought:\nTeaching pre-trained models to systematically rea-\nson over implicit knowledge. In Advances in Neural\nInformation Processing Systems, volume 33, pages\n20227â€“20237. Curran Associates, Inc.\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bha-\ngavatula, Yoav Goldberg, Yejin Choi, and Jonathan\nBerant. 2021. Commonsenseqa 2.0: Exposing the\nlimits of ai through gamification. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 1).\nRobert F Tate. 1954. Correlation between a discrete and\na continuous variable. point-biserial correlation. The\nAnnals of mathematical statistics, 25(3):603â€“607.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam M. Shazeer, Apoorv Kulshreshtha, Heng-\nTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,\nYu Du, Yaguang Li, Hongrae Lee, Huaixiu Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nI. A. Krivokon, Willard James Rusch, Marc Pick-\nett, Kathleen S. Meier-Hellstern, Meredith Ringel\nMorris, Tulsee Doshi, Renelito Delos Santos, Toju\nDuke, Johnny Hartz SÃ¸raker, Ben Zevenbergen, Vin-\nodkumar Prabhakaran, Mark Diaz, Ben Hutchinson,\nKristen Olson, Alejandra Molina, Erin Hoffman-\nJohn, Josh Lee, Lora Aroyo, Ravindran Rajakumar,\nAlena Butryna, Matthew Lamm, V . O. Kuzmina,\nJoseph Fenton, Aaron Cohen, Rachel Bernstein, Ray\nKurzweil, Blaise Aguera-Arcas, Claire Cui, Mar-\nian Croak, Ed Chi, and Quoc Le. 2022. Lamda:\nLanguage models for dialog applications. ArXiv,\nabs/2201.08239.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. ArXiv, abs/2201.11903.\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel.\n2018. Constructing datasets for multi-hop reading\ncomprehension across documents. Transactions of\nthe Association for Computational Linguistics, 6:287â€“\n302.\n2558\nTomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-\nner, Yoav Goldberg, Daniel Deutch, and Jonathan\nBerant. 2020. Break it down: A question understand-\ning benchmark. Transactions of the Association for\nComputational Linguistics, 8:183â€“198.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. QA-GNN:\nReasoning with language models and knowledge\ngraphs for question answering. In Proceedings of\nthe 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 535â€“546, Online.\nAssociation for Computational Linguistics.\nEric Zelikman, Yuhuai Wu, and Noah D Goodman.\n2022. Star: Bootstrapping reasoning with reason-\ning. arXiv preprint arXiv:2203.14465.\n2559\nRelation coverage\nManual evaluation 0.6\nAutomatic evaluation 0.53\nTable 8: Manual and automatic evaluation of 50 exam-\nples from STGQA.\nA Examples and Qualitative Analysis\nA.1 Qualitative analysis of evaluation\nIn addition to the evaluation described in Â§4 and\nused throughout our study, we performed manual\nqualitative analysis to assess the relation coverage\nmetric model outputs. We sampled 50 examples\nrandomly from STGQA together with GPT-3 pre-\ndictions and manually labeled if the implicit rela-\ntions output by the model are semantically correct.\nResults in Table 8 show that our relation cover-\nage was slightly more conservative than our manual\nevaluation â€“ 0.53 vs. 0.6. Out of the 20 examples\nthat we marked as incorrect, the automatic eval-\nuation scored 13 examples (65%) with 0 relation\ncoverage, 6 (30%) examples with partial coverage\n(scored 0.5), which indeed we included a partially\ncorrect prediction but not enough to to cover the\nneeded reasoning process. Only one example out\nof the 20 was a false positive.\nA.2 I MPLICIT RELATIONS Examples\nTables 9,10,11 present examples from each source\ndataset accompanied by the question, gold anno-\ntated implicit relation pairs, predicted pairs gener-\nated by the model and the answer to the question.\n2560\nExamples from STRATEGY QA\nQ: Could Eric Claptonâ€™s children play a regulation game of basketball among themselves?\nG: âŸ¨Eric Clapton, number of childrenâŸ©, âŸ¨regulation game of basketball, number of playersâŸ©\nP: âŸ¨eric clapton, number of childrenâŸ©, âŸ¨regulation game of basketball, number of playersâŸ©\nA: False\nQ: Can you purchase General Motors products at a movie theater?\nG: âŸ¨General Motors, Types of productsâŸ©, âŸ¨Movie theater, products soldâŸ©\nP: âŸ¨general motors, type of productâŸ©, âŸ¨movie theater, type of productâŸ©\nA: False\nQ: Does Rupert Murdochâ€™s alma mater have more history than the USA?\nG: âŸ¨Rupert Murdochâ€™s alma mater, year foundedâŸ©, âŸ¨USA, year foundedâŸ©\nP: âŸ¨rupert murdochâ€™s alma mater, date of foundingâŸ©, âŸ¨usa, date of foundingâŸ©\nA: True\nQ: Do Squidward Tentacles and Alan Greenspan have different musical passions?\nG: âŸ¨Squidward Tentacles, instruments playedâŸ©, âŸ¨Alan Greenspan, instruments playedâŸ©\nP: âŸ¨squidward tentacles, musical instrumentâŸ©, âŸ¨alan greenspan, musical instrumentâŸ©\nA: False\nQ: Is the most expensive color in the world Blue?\nG: âŸ¨Blue, cost of pigmentâŸ©, âŸ¨most expensive color in the world, cost of pigmentâŸ©\nP: âŸ¨blue, colorâŸ©, âŸ¨most expensive color, colorâŸ©\nA: True\nTable 9: Examples of gold annotations (G) and predicted (P) concept-relation pairs from IMPLICIT RELATIONS\nalong with the question (Q) and answer (A) from STRATEGY QA.\nExamples from CREAK\nQ: People celebrate Cinco de Mayo before summer starts.\nG: âŸ¨Cinco de Mayo, dateâŸ©, âŸ¨summer, starting dateâŸ©\nP: âŸ¨cinco de Mayo, dateâŸ©\nA: True\nQ: Arthur C. Clarke wrote a book set in outer space.\nG: âŸ¨Arthur C. Clarke, writing genreâŸ©\nP: âŸ¨arthur c. clarke, book genreâŸ©\nA: True\nQ: The local chamber music company hired a Harp seal to join the group.\nG: âŸ¨Harp seal, musical abilityâŸ©\nP: âŸ¨harp seal, natural habitatâŸ©\nA: False\nQ: Nicole Kidman has never heard of Batman.\nG: âŸ¨Nicole Kidman, filmographyâŸ©\nP: âŸ¨nicole kidman, knowledgeâŸ©\nA: False\nQ: Competitors know how to drive when they are competing in Formula One racing.\nG: âŸ¨Formula One racing, type of sportâŸ©\nP: âŸ¨formula one racing, skills requiredâŸ©\nA: True\nTable 10: Examples of gold annotations (G) and predicted (P) concept-relation pairs from IMPLICIT RELATIONS\nalong with the question (Q) and answer (A) from CREAK .\n2561\nExamples from CSQA2\nQ: Trees are never part of forests.\nG: âŸ¨forests, type of floraâŸ©\nP: âŸ¨trees, locationâŸ©\nA: False\nQ: You can only win a medal at the Olympics if you place in the top 3?\nG: âŸ¨Olympics, medal awarding rulesâŸ©\nP: âŸ¨medal, requirements for winningâŸ©\nA: True\nQ: Is it true that a hospital is capable of treating Dutch Elm disease?\nG: âŸ¨Dutch Elm disease, targeted organismâŸ©, âŸ¨hospital, species treatedâŸ©\nP: âŸ¨hospital, type of diseases treatedâŸ©\nA: False\nQ: at least one letter in the word walnut is q.\nG: âŸ¨walnut, spellingâŸ©\nP: âŸ¨word walnut, lettersâŸ©\nA: False\nQ: Is the temperature usually quite hot before Christmas in the southern hemisphere?\nG: âŸ¨Christmas, monthâŸ©, âŸ¨southern hemisphere, summer monthsâŸ©\nP: âŸ¨southern hemisphere, seasonsâŸ©\nA: True\nTable 11: Examples of gold annotations (G) and predicted (P) concept-relation pairs from IMPLICIT RELATIONS\nalong with the question (Q) and answer (A) from CSQA2.\n2562\nk Concept Concept Relation\nRecall Precision Coverage\nSTGQA\n8 0.94 Â±0.02 0.87 Â±0.01 0.48 Â±0.02\n16 0.97 Â±0.01 0.88 Â±0.01 0.53 Â±0.02\n32 0.97 Â±0.01 0.88 Â±0.01 0.50 Â±0.01\nCREAK\n8 0.94 Â±0.02 0.91 Â±0.02 0.56 Â±0.0\n16 0.93 Â±0.02 0.91 Â±0.01 0.59 Â±0.03\n32 0.95 Â±0.01 0.90 Â±0.02 0.62 Â±0.02\nCSQA2\n8 0.94 Â±0.0 0.95 Â±0.02 0.61 Â±0.02\n16 0.95 Â±0.01 0.96 Â±0.0 0.67 Â±0.01\n32 0.95 Â±0.01 0.96 Â±0.02 0.66 Â±0.02\nTable 12: Development set results for 8/16/32 examples\nin the prompt, averaged for 3 seeds.\nB Number of Prompt Examples\nWe investigate how different number of examples\ninfluence implicit relation inference. We run the\noriginal experiment with kâˆˆ{8,16,32}examples\nin the prompt for 3 random seeds. The results (Ta-\nble 12) show that the number of examples in the\nprompt has little effect on all evaluation metrics,\njustifying our choice to use k = 16in all experi-\nments.\n2563\nC Annotation Task Instruction\nC.1 Task instruction\n2564\nC.2 Task instruction cont.\n2565\nC.3 Concept-relation pairs\n2566",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7422531843185425
    },
    {
      "name": "Inference",
      "score": 0.6955194473266602
    },
    {
      "name": "Question answering",
      "score": 0.6677247285842896
    },
    {
      "name": "Construct (python library)",
      "score": 0.538223385810852
    },
    {
      "name": "Task (project management)",
      "score": 0.5211136937141418
    },
    {
      "name": "Implicit knowledge",
      "score": 0.5152668952941895
    },
    {
      "name": "Relation (database)",
      "score": 0.48093053698539734
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47089865803718567
    },
    {
      "name": "Model-based reasoning",
      "score": 0.43957677483558655
    },
    {
      "name": "Qualitative reasoning",
      "score": 0.4369237422943115
    },
    {
      "name": "Natural language processing",
      "score": 0.4154298007488251
    },
    {
      "name": "Cognitive science",
      "score": 0.32214581966400146
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.2558304965496063
    },
    {
      "name": "Programming language",
      "score": 0.16804072260856628
    },
    {
      "name": "Psychology",
      "score": 0.10547080636024475
    },
    {
      "name": "Data mining",
      "score": 0.08144980669021606
    },
    {
      "name": "Knowledge management",
      "score": 0.07999026775360107
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}