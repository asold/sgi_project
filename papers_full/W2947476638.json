{
  "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
  "url": "https://openalex.org/W2947476638",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5012188239",
      "name": "Yao Hung Hubert Tsai",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2785652606",
      "name": "Shaojie Bai",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2767400205",
      "name": "Paul Pu Liang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2219978048",
      "name": "J. Zico Kolter",
      "affiliations": [
        "Robert Bosch (India)",
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A4212702429",
      "name": "Louis-Philippe Morency",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2031945151",
      "name": "Ruslan Salakhutdinov",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2095176743",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2740550900",
    "https://openalex.org/W2906625520",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2963685106",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2556418146",
    "https://openalex.org/W2886193235",
    "https://openalex.org/W2964216663",
    "https://openalex.org/W1491334068",
    "https://openalex.org/W2963871344",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2184188583",
    "https://openalex.org/W2954330188",
    "https://openalex.org/W2524011860",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2950738719",
    "https://openalex.org/W2883409523",
    "https://openalex.org/W2164587673",
    "https://openalex.org/W2029996593",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2950399211",
    "https://openalex.org/W1966797434",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2964266095",
    "https://openalex.org/W2962931510",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2070353225",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2250539671"
  ],
  "abstract": "Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6558–6569\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n6558\nMultimodal Transformer for Unaligned Multimodal Language Sequences\nYao-Hung Hubert Tsai†∗, Shaojie Bai†∗, Paul Pu Liang†,\nJ. Zico Kolter†‡, Louis-Philippe Morency†, Ruslan Salakhutdinov†\n†Carnegie Mellon University, ‡Bosch Center for AI\n{yaohungt,shaojieb,pliang,zkolter,morency,rsalakhu}@cs.cmu.edu\nAbstract\nHuman language is often multimodal, which\ncomprehends a mixture of natural language,\nfacial gestures, and acoustic behaviors. How-\never, two major challenges in modeling such\nmultimodal human language time-series data\nexist: 1) inherent data non-alignment due\nto variable sampling rates for the sequences\nfrom each modality; and 2) long-range depen-\ndencies between elements across modalities.\nIn this paper, we introduce the Multimodal\nTransformer (MulT) to generically address the\nabove issues in an end-to-end manner with-\nout explicitly aligning the data. At the heart\nof our model is the directional pairwise cross-\nmodal attention, which attends to interactions\nbetween multimodal sequences across distinct\ntime steps and latently adapt streams from one\nmodality to another. Comprehensive experi-\nments on both aligned and non-aligned multi-\nmodal time-series show that our model outper-\nforms state-of-the-art methods by a large mar-\ngin. In addition, empirical analysis suggests\nthat correlated crossmodal signals are able to\nbe captured by the proposed crossmodal atten-\ntion mechanism in MulT.\n1 Introduction\nHuman language possesses not only spoken words\nbut also nonverbal behaviors from vision (facial\nattributes) and acoustic (tone of voice) modali-\nties (Gibson et al., 1994). This rich information\nprovides us the beneﬁt of understanding human\nbehaviors and intents (Manning et al., 2014). Nev-\nertheless, the heterogeneities across modalities of-\nten increase the difﬁculty of analyzing human lan-\nguage. For example, the receptors for audio and\nvision streams may vary with variable receiving\nfrequency, and hence we may not obtain optimal\nmapping between them. A frowning face may re-\nlate to a pessimistically word spoken in the past.\n∗*equal contribution.\nIt’s    huge   sort     of                           spectacle  movie\nVisionLanguageAudio[ ][ [[ [ [] ]] ]]\n[ [[ [[ []] ] ]] ]…… … … … … … … …\nVision-to-Language Alignment Audio-to-Language Alignment \n(Pre-defined Word-level) Alignment\nIt’s    huge   sort     of                           spectacle  movie\nVisionLanguageAudio\n…… … … … … … … …\nVision-to-Language (‘spectacle’) Attention WeightsAudio-to-Language (‘spectacle’) Attention Weights\n(Ours) CrossmodalAttention(uninformative)(eyebrows raise)\n(emphasis) (emphasis)(neutral)\nFigure 1: Example video clip from movie reviews.\n[Top]: Illustration of word-level alignment where video\nand audio features are averaged across the time interval\nof each spoken word. [Bottom] Illustration of cross-\nmodal attention weights between text (“spectacle”) and\nvision/audio.\nThat is to say, multimodal language sequences\noften exhibit “unaligned” nature and require in-\nferring long term dependencies across modalities,\nwhich raises a question on performing efﬁcient\nmultimodal fusion.\nTo address the above issues, in this paper we\npropose the Multimodal Transformer (MulT), an\nend-to-end model that extends the standard Trans-\nformer network (Vaswani et al., 2017) to learn rep-\nresentations directly from unaligned multimodal\nstreams. At the heart of our model is the cross-\nmodal attention module, which attends to the\ncrossmodal interactions at the scale of the entire\nutterances. This module latently adapts streams\nfrom one modality to another (e.g., vision →\nlanguage) by repeated reinforcing one modality’s\nfeatures with those from the other modalities, re-\n6559\ngardless of the need for alignment. In compari-\nson, one common way of tackling unaligned mul-\ntimodal sequence is by forced word-aligning be-\nfore training (Poria et al., 2017; Zadeh et al.,\n2018a,b; Tsai et al., 2019; Pham et al., 2019;\nGu et al., 2018): manually preprocess the vi-\nsual and acoustic features by aligning them to\nthe resolution of words. These approaches would\nthen model the multimodal interactions on the (al-\nready) aligned time steps and thus do not directly\nconsider long-range crossmodal contingencies of\nthe original features. We note that such word-\nalignment not only requires feature engineering\nthat involves domain knowledge; but in practice,\nit may also not always be feasible, as it entails\nextra meta-information about the datasets (e.g.,\nthe exact time ranges of words or speech utter-\nances). We illustrate the difference between the\nword-alignment and the crossmodal attention in-\nferred by our model in Figure 1.\nFor evaluation, we perform a comprehensive set\nof experiments on three human multimodal lan-\nguage benchmarks: CMU-MOSI (Zadeh et al.,\n2016), CMU-MOSEI (Zadeh et al., 2018b), and\nIEMOCAP (Busso et al., 2008). Our experi-\nments show that MulT achieves the state-of-the-\nart (SOTA) results in not only the commonly eval-\nuated word-aligned setting but also the more chal-\nlenging unaligned scenario, outperforming prior\napproaches by a margin of 5%-15% on most of the\nmetrics. In addition, empirical qualitative analysis\nfurther suggests that the crossmodal attention used\nby MulT is capable of capturing correlated signals\nacross asynchronous modalities.\n2 Related Works\nHuman Multimodal Language Analysis. Prior\nwork for analyzing human multimodal language\nlies in the domain of inferring representations\nfrom multimodal sequences spanning language,\nvision, and acoustic modalities. Unlike learning\nmultimodal representations from static domains\nsuch as image and textual attributes (Ngiam et al.,\n2011; Srivastava and Salakhutdinov, 2012), hu-\nman language contains time-series and thus re-\nquires fusing time-varying signals (Liang et al.,\n2018; Tsai et al., 2019). Earlier work used\nearly fusion approach to concatenate input fea-\ntures from different modalities (Lazaridou et al.,\n2015; Ngiam et al., 2011) and showed improved\nperformance as compared to learning from a sin-\ngle modality. More recently, more advanced mod-\nels were proposed to learn representations of hu-\nman multimodal language. For example, Gu et al.\n(2018) used hierarchical attention strategies to\nlearn multimodal representations, Wang et al.\n(2019) adjusted the word representations using ac-\ncompanying non-verbal behaviors, Pham et al.\n(2019) learned robust multimodal representations\nusing a cyclic translation objective, and Dumpala\net al. (2019) explored cross-modal autoencoders\nfor audio-visual alignment. These previous ap-\nproaches relied on the assumption that multimodal\nlanguage sequences are already aligned in the res-\nolution of words and considered only short-term\nmultimodal interactions. In contrast, our proposed\nmethod requires no alignment assumption and de-\nﬁnes crossmodal interactions at the scale of the en-\ntire sequences.\nTransformer Network. Transformer net-\nwork (Vaswani et al., 2017) was ﬁrst introduced\nfor neural machine translation (NMT) tasks,\nwhere the encoder and decoder side each lever-\nages a self-attention (Parikh et al., 2016; Lin\net al., 2017; Vaswani et al., 2017) transformer.\nAfter each layer of the self-attention, the encoder\nand decoder are connected by an additional\ndecoder sublayer where the decoder attends to\neach element of the source text for each element\nof the target text. We refer the reader to (Vaswani\net al., 2017) for a more detailed explanation of\nthe model. In addition to NMT, transformer\nnetworks have also been successfully applied to\nother tasks, including language modeling (Dai\net al., 2018; Baevski and Auli, 2019), semantic\nrole labeling (Strubell et al., 2018), word sense\ndisambiguation (Tang et al., 2018), learning\nsentence representations (Devlin et al., 2018), and\nvideo activity recognition (Wang et al., 2018).\nThis paper absorbs a strong inspiration from\nthe NMT transformer to extend to a multimodal\nsetting. Whereas the NMT transformer focuses\non unidirectional translation from source to tar-\nget texts, human multimodal language time-series\nare neither as well-represented nor discrete as\nword embeddings, with sequences of each modal-\nity having vastly different frequencies. Therefore,\nwe propose not to explicitly translate from one\nmodality to the others (which could be extremely\nchallenging), but to latently adapt elements across\nmodalities via the attention. Our model (MulT)\ntherefore has no encoder-decoder structure, but it\n6560\nCrossmodal\nTransformer\n(V ! L )\n(A ! L) (A ! V ) (V ! A )\nTransformer Transformer Transformer\nPrediction ˆ y\nConcatenation\nConv1D Conv1D Conv1D\n\u0000 \u0000 \u0000\nPositional\nEmbedding\nX L 2 R T L ⇥ d L\nX V 2 R T V ⇥ d V\nX A 2 R T A ⇥ d A\nCrossmodal\nAttention\nSelf\nAttention\nBlocki=1,...,D\nCrossmodal\nTransformer\n(L ! V )\nCrossmodal\nTransformer\n(L ! A )\nMultimodal\nTransformer\nZ L 2 R T L ⇥ 2 d Z V 2 R T V ⇥ 2 d\nZ A 2 R T A ⇥ 2 d\nFigure 2: Overall architecture for MulT on modalities\n(L,V,A ). The crossmodal transformers, which sug-\ngests latent crossmodal adaptations, are the core com-\nponents of MulT for multimodal fusion.\nis built up from multiple stacks of pairwise and\nbidirectional crossmodal attention blocks that di-\nrectly attend to low-level features (while remov-\ning the self-attention). Empirically, we show that\nour proposed approach improves beyond standard\ntransformer on various human multimodal lan-\nguage tasks.\n3 Proposed Method\nIn this section, we describe our proposed Multi-\nmodal Transformer (MulT) (Figure 2) for mod-\neling unaligned multimodal language sequences.\nAt the high level, MulT merges multimodal time-\nseries via a feed-forward fusion process from mul-\ntiple directional pairwise crossmodal transform-\ners. Speciﬁcally, each crossmodal transformer\n(introduced in Section 3.2) serves to repeatedly\nreinforce a target modality with the low-level\nfeatures from another source modality by learn-\ning the attention across the two modalities’ fea-\ntures. A MulT architecture hence models all pairs\nof modalities with such crossmodal transformers,\nfollowed by sequence models (e.g., self-attention\ntransformer) that predicts using the fused features.\nThe core of our proposed model is crossmodal\nattention module, which we ﬁrst introduce in Sec-\ntion 3.1. Then, in Section 3.2 and 3.3, we present\nin details the various ingredients of the MulT ar-\nchitecture (see Figure 2) and discuss the difference\nbetween crossmodal attention and classical multi-\nmodal alignment.\n3.1 Crossmodal Attention\nWe consider two modalities α and β, with two\n(potentially non-aligned) sequences from each of\nthem denoted Xα ∈RTα×dα and Xβ ∈RTβ×dβ,\nrespectively. For the rest of the paper,T(·) and d(·)\nare used to represent sequence length and feature\ndimension, respectively. Inspired by the decoder\ntransformer in NMT (Vaswani et al., 2017) that\ntranslates one language to another, we hypothesize\na good way to fuse crossmodal information is pro-\nviding a latent adaptation across modalities; i.e.,β\nto α. Note that the modalities consider in our pa-\nper may span very different domains such as facial\nattributes and spoken words.\nWe deﬁne the Querys as Qα = XαWQα, Keys\nas Kβ = XβWKβ, and Values as Vβ = XβWVβ,\nwhere WQα ∈ Rdα×dk,WKβ ∈ Rdβ×dk and\nWVβ ∈Rdβ×dv are weights. The latent adapta-\ntion from β to α is presented as the crossmodal\nattention Yα := CMβ→α(Xα,XB) ∈RTα×dv:\nYα=CMβ→α(Xα,Xβ)\n=softmax\n(QαK⊤β√dk\n)\nVβ\n=softmax\n(XαWQαW⊤KβX⊤β√dk\n)\nXβWVβ.\n(1)\nNote that Yα has the same length as Qα (i.e.,\nTα), but is meanwhile represented in the feature\nspace of Vβ. Speciﬁcally, the scaled (by √dk)\nsoftmax in Equation (1) computes a score matrix\nsoftmax (·) ∈RTα×Tβ, whose (i,j)-th entry mea-\nsures the attention given by the i-th time step of\nmodality α to the j-th time step of modality β.\nHence, the i-th time step of Yα is a weighted sum-\nmary of Vβ, with the weight determined by i-th\nrow in softmax(·). We call Equation (1) a single-\nhead crossmodal attention, which is illustrated in\nFigure 3(a).\nFollowing prior works on transform-\ners (Vaswani et al., 2017; Chen et al., 2018;\nDevlin et al., 2018; Dai et al., 2018), we add\na residual connection to the crossmodal atten-\ntion computation. Then, another positionwise\nfeed-forward sublayer is injected to complete\na crossmodal attention block (see Figure 3(b)).\nEach crossmodal attention block adapts directly\nfrom the low-level feature sequence (i.e., Z[0]\nβ in\nFigure 3(b)) and does not rely on self-attention,\nwhich makes it different from the NMT encoder-\ndecoder architecture (Vaswani et al., 2017; Shaw\net al., 2018) (i.e., taking intermediate-level\nfeatures). We argue that performing adaptation\n6561\nsoftmax\n✓\nQ ↵ K >\n\u0000\np\nd k\n◆\nV \u0000 2 R T ↵ ⇥ d v\nCM \u0000 ! ↵ ( X ↵ ,X \u0000 )\nsoftmax\n✓Q↵K>\n\u0000p\ndk\n◆\nQ ↵ 2 R T ↵ ⇥ d k K \u0000 2 R T \u0000 ⇥ d k\nV \u0000 2 R T \u0000 ⇥ d v\nX ↵ 2 R T ↵ ⇥ d ↵ X \u0000 2 R T \u0000 ⇥ d ↵\nW Q ↵ W K \u0000 W V \u0000\nModality ↵ Modality \u0000\n(a) Crossmodal attention CM β→α(Xα, Xβ) between sequences Xα, Xβ\nfrom distinct modalities.\nMulti-head\nPositionwise\nFeed-forward\n⇥ D Layers\nLayer 0\nZ [0]\n↵ Z [0]\n\u0000\nCrossmodal\nTransformer\n(\u0000 ! ↵ )Block i\n( \u0000 ! ↵ )\nCM\u0000!↵(Z[i\u00001]\n\u0000!↵,Z[0]\n\u0000 )\nZ[i\u00001]\n\u0000!↵\nZ [i]\n\u0000 !↵\nZ [D ]\n\u0000 !↵\nQ↵ K\u0000 V\u0000\nLayerNorm\n\u0000\nLayerNorm\n\u0000\nLayerNorm\nAddition\nAddition\n(b) A crossmodal transformer is a deep stack-\ning of several crossmodal attention blocks.\nFigure 3: Architectural elements of a crossmodal transformer between two time-series from modality αand β.\nfrom low-level feature beneﬁts our model to\npreserve the low-level information for each\nmodality. We leave the empirical study for\nadapting from intermediate-level features (i.e.,\nZ[i−1]\nβ ) in Ablation Study in Section 4.3.\n3.2 Overall Architecture\nThree major modalities are typically involved in\nmultimodal language sequences: language ( L),\nvideo ( V), and audio ( A) modalities. We de-\nnote with X{L,V,A} ∈RT{L,V,A}×d{L,V,A} the in-\nput feature sequences (and the dimensions thereof)\nfrom these 3 modalities. With these notations, in\nthis subsection, we describe in greater details the\ncomponents of Multimodal Transformer and how\ncrossmodal attention modules are applied.\nTemporal Convolutions. To ensure that each el-\nement of the input sequences has sufﬁcient aware-\nness of its neighborhood elements, we pass the\ninput sequences through a 1D temporal convolu-\ntional layer:\nˆX{L,V,A}=Conv1D(X{L,V,A},k{L,V,A}) ∈RT{L,V,A}×d\n(2)\nwhere k{L,V,A}are the sizes of the convolutional\nkernels for modalities {L,V,A }, and dis a com-\nmon dimension. The convolved sequences are\nexpected to contain the local structure of the se-\nquence, which is important since the sequences\nare collected at different sampling rates. More-\nover, since the temporal convolutions project the\nfeatures of different modalities to the same di-\nmension d, the dot-products are admittable in the\ncrossmodal attention module.\nPositional Embedding. To enable the se-\nquences to carry temporal information, follow-\ning (Vaswani et al., 2017), we augment positional\nembedding (PE) to ˆX{L,V,A}:\nZ[0]\n{L,V,A}= ˆX{L,V,A}+ PE(T{L,V,A},d) (3)\nwhere PE (T{L,V,A},d) ∈ RT{L,V,A}×d computes\nthe (ﬁxed) embeddings for each position index,\nand Z[0]\n{L,V,A}are the resulting low-level position-\naware features for different modalities. We leave\nmore details of the positional embedding to Ap-\npendix A.\nCrossmodal Transformers. Based on the cross-\nmodal attention blocks, we design the crossmodal\ntransformer that enables one modality for receiv-\ning information from another modality. In the fol-\nlowing, we use the example for passing vision (V)\ninformation to language (L), which is denoted by\n“V →L”. We ﬁx all the dimensions ( d{α,β,k,v})\nfor each crossmodal attention block as d.\nEach crossmodal transformer consists of Dlay-\ners of crossmodal attention blocks (see Figure\n3(b)). Formally, a crossmodal transformer com-\nputes feed-forwardly for i= 1,...,D layers:\nZ[0]\nV→L=Z[0]\nL\nˆZ[i]\nV→L=CM[i],mul\nV→L(LN(Z[i−1]\nV→L),LN(Z[0]\nV )) +LN(Z[i−1]\nV→L)\nZ[i]\nV→L=fθ[i]\nV→L\n(LN(ˆZ[i]\nV→L)) +LN(ˆZ[i]\nV→L)\n(4)\nwhere fθ is a positionwise feed-forward sublayer\nparametrized by θ, and CM [i],mul\nV→L means a multi-\nhead (see (Vaswani et al., 2017) for more details)\nversion of CM V→L at layer i (note: d should be\n6562\n=no attention weightTime\nTime\nTime Time\nModality ↵\nModality \u0000\nExplicit alignment written in the form of a crossmodal attention matrix\nA learned crossmodal attention in MulT\n=little attention weight\nFigure 4: An example of visualizing alignment using\nattention matrix from modality β to α. Multimodal\nalignment is a special (monotonic) case for crossmodal\nattention.\ndivisible by the number of heads). LN means layer\nnormalization (Ba et al., 2016).\nIn this process, each modality keeps updating its\nsequence via low-level external information from\nthe multi-head crossmodal attention module. At\nevery level of the crossmodal attention block, the\nlow-level signals from source modality are trans-\nformed to a different set of Key/Value pairs to in-\nteract with the target modality. Empirically, we\nﬁnd that the crossmodal transformer learns to cor-\nrelate meaningful elements across modalities (see\nSection 4 for details). The eventual MulT is based\non modeling every pair of crossmodal interactions.\nTherefore, with 3 modalities (i.e., L,V,A ) in con-\nsideration, we have 6 crossmodal transformers in\ntotal (see Figure 2).\nSelf-Attention Transformers and Prediction.\nAs a ﬁnal step, we concatenate the outputs from\nthe crossmodal transformers that share the same\ntarget modality to yield Z{L,V,A}∈RT{L,V,A}×2d.\nFor example, ZL = [Z[D]\nV→L; Z[D]\nA→L]. Each of\nthem is then passed through a sequence model to\ncollect temporal information to make predictions.\nWe choose the self-attention transformer (Vaswani\net al., 2017). Eventually, the last elements of the\nsequences models are extracted to pass through\nfully-connected layers to make predictions.\n3.3 Discussion about Attention & Alignment\nWhen modeling unaligned multimodal language\nsequences, MulT relies on crossmodal atten-\ntion blocks to merge signals across modalities.\nWhile the multimodal sequences were (manually)\naligned to the same length in prior works be-\nfore training (Zadeh et al., 2018b; Liang et al.,\nMetric Acc h\n7 Acch\n2 F1h MAEℓ Corrh\n(Word Aligned) CMU-MOSI Sentiment\nEF-LSTM 33.7 75.3 75.2 1.023 0.608\nLF-LSTM 35.3 76.8 76.7 1.015 0.625\nRMFN (Liang et al., 2018) 38.3 78.4 78.0 0.922 0.681\nMFM (Tsai et al., 2019) 36.2 78.1 78.1 0.951 0.662\nRA VEN (Wang et al., 2019) 33.2 78.0 76.6 0.915 0.691\nMCTN (Pham et al., 2019) 35.6 79.3 79.1 0.909 0.676\nMulT (ours) 40.0 83.0 82.8 0.871 0.698\n(Unaligned) CMU-MOSI Sentiment\nCTC (Graves et al., 2006) + EF-LSTM 31.0 73.6 74.5 1.078 0.542\nLF-LSTM 33.7 77.6 77.8 0.988 0.624\nCTC + MCTN (Pham et al., 2019) 32.7 75.9 76.4 0.991 0.613\nCTC + RA VEN (Wang et al., 2019) 31.7 72.7 73.1 1.076 0.544\nMulT (ours) 39.1 81.1 81.0 0.889 0.686\nTable 1: Results for multimodal sentiment analysis on\nCMU-MOSI with aligned and non-aligned multimodal\nsequences. h means higher is better and ℓ means lower\nis better. EF stands for early fusion, and LF stands for\nlate fusion.\n2018; Tsai et al., 2019; Pham et al., 2019; Wang\net al., 2019), we note that MulT looks at the non-\nalignment issue through a completely different\nlens. Speciﬁcally, for MulT, the correlations be-\ntween elements of multiple modalities are purely\nbased on attention. In other words, MulT does not\nhandle modality non-alignment by (simply) align-\ning them; instead, the crossmodal attention en-\ncourages the model to directly attend to elements\nin other modalities where strong signals or rele-\nvant information is present. As a result, MulT can\ncapture long-range crossmodal contingencies in a\nway that conventional alignment could not eas-\nily reveal. Classical crossmodal alignment, on the\nother hand, can be expressed as a special (step di-\nagonal) crossmodal attention matrix (i.e., mono-\ntonic attention (Yu et al., 2016)). We illustrate\ntheir differences in Figure 4.\n4 Experiments\nIn this section, we empirically evaluate the Multi-\nmodal Transformer (MulT) on three datasets that\nare frequently used to benchmark human multi-\nmodal affection recognition in prior works (Pham\net al., 2019; Tsai et al., 2019; Liang et al., 2018).\nOur goal is to compare MulT with prior compet-\nitive approaches on both word-aligned (by word,\nwhich almost all prior works employ) and un-\naligned (which is more challenging, and which\nMulT is generically designed for) multimodal lan-\nguage sequences.\n6563\nMetric Acc h\n7 Acch\n2 F1h MAEℓ Corrh\n(Word Aligned) CMU-MOSEI Sentiment\nEF-LSTM 47.4 78.2 77.9 0.642 0.616\nLF-LSTM 48.8 80.6 80.6 0.619 0.659\nGraph-MFN (Zadeh et al., 2018b) 45.0 76.9 77.0 0.71 0.54\nRA VEN (Wang et al., 2019) 50.0 79.1 79.5 0.614 0.662\nMCTN (Pham et al., 2019) 49.6 79.8 80.6 0.609 0.670\nMulT (ours) 51.8 82.5 82.3 0.580 0.703\n(Unaligned) CMU-MOSEI Sentiment\nCTC (Graves et al., 2006) + EF-LSTM 46.3 76.1 75.9 0.680 0.585\nLF-LSTM 48.8 77.5 78.2 0.624 0.656\nCTC + RA VEN (Wang et al., 2019) 45.5 75.4 75.7 0.664 0.599\nCTC + MCTN (Pham et al., 2019) 48.2 79.3 79.7 0.631 0.645\nMulT (ours) 50.7 81.6 81.6 0.591 0.694\nTable 2: Results for multimodal sentiment analysis on\n(relatively large scale) CMU-MOSEI with aligned and\nnon-aligned multimodal sequences.\n4.1 Datasets and Evaluation Metrics\nEach task consists of aword-aligned (processed in\nthe same way as in prior works) and an unaligned\nversion. For both versions, the multimodal\nfeatures are extracted from the textual (GloVe\nword embeddings (Pennington et al., 2014)), vi-\nsual (Facet (iMotions, 2017)), and acoustic (CO-\nV AREP (Degottex et al., 2014)) data modalities.\nA more detailed introduction to the features is in-\ncluded in Appendix.\nFor the word-aligned version, following (Zadeh\net al., 2018a; Tsai et al., 2019; Pham et al., 2019),\nwe ﬁrst use P2FA (Yuan and Liberman, 2008)\nto obtain the aligned timesteps (segmented w.r.t.\nwords) for audio and vision streams, and we then\nperform averaging on the audio and vision fea-\ntures within these time ranges. All sequences in\nthe word-aligned case have length 50. The pro-\ncess remains the same across all the datasets. On\nthe other hand, for the unaligned version, we keep\nthe original audio and visual features as extracted,\nwithout any word-segmented alignment or man-\nual subsampling. As a result, the lengths of each\nmodality vary signiﬁcantly, where audio and vi-\nsion sequences may contain up to > 1,000 time\nsteps. We elaborate on the three tasks below.\nCMU-MOSI & MOSEI. CMU-MOSI (Zadeh\net al., 2016) is a human multimodal sentiment\nanalysis dataset consisting of 2,199 short mono-\nlogue video clips (each lasting the duration of a\nsentence). Acoustic and visual features of CMU-\nMOSI are extracted at a sampling rate of 12.5 and\n15 Hz, respectively (while textual data are seg-\nmented per word and expressed as discrete word\nembeddings). Meanwhile, CMU-MOSEI (Zadeh\net al., 2018b) is a sentiment and emotion analy-\nsis dataset made up of 23,454 movie review video\nclips taken from YouTube (about 10×the size\nof CMU-MOSI). The unaligned CMU-MOSEI se-\nquences are extracted at a sampling rate of 20 Hz\nfor acoustic and 15 Hz for vision signals.\nFor both CMU-MOSI and CMU-MOSEI, each\nsample is labeled by human annotators with a\nsentiment score from -3 (strongly negative) to 3\n(strongly positive). We evaluate the model per-\nformances using various metrics, in agreement\nwith those employed in prior works: 7-class ac-\ncuracy (i.e., Acc 7: sentiment score classiﬁcation\nin Z ∩[−3,3]), binary accuracy (i.e., Acc 2: pos-\nitive/negative sentiments), F1 score, mean abso-\nlute error (MAE) of the score, and the correlation\nof the model’s prediction with human. Both tasks\nare frequently used to benchmark models’ ability\nto fuse multimodal (sentiment) information (Po-\nria et al., 2017; Zadeh et al., 2018a; Liang et al.,\n2018; Tsai et al., 2019; Pham et al., 2019; Wang\net al., 2019).\nIEMOCAP. IEMOCAP (Busso et al., 2008)\nconsists of 10K videos for human emotion anal-\nysis. As suggested by Wang et al. (2019), 4 emo-\ntions (happy, sad, angry and neutral) were selected\nfor emotion recognition. Unlike CMU-MOSI and\nCMU-MOSEI, this is a multilabel task (e.g., a per-\nson can be sad and angry simultaneously). Its mul-\ntimodal streams consider ﬁxed sampling rate on\naudio (12.5 Hz) and vision ( 15 Hz) signals. We\nfollow (Poria et al., 2017; Wang et al., 2019; Tsai\net al., 2019) to report the binary classiﬁcation ac-\ncuracy and the F1 score of the predictions.\n4.2 Baselines\nWe choose Early Fusion LSTM (EF-LSTM) and\nLate Fusion LSTM (LF-LSTM) as baseline mod-\nels, as well as Recurrent Attended Variation\nEmbedding Network (RA VEN) (Wang et al.,\n2019) and Multimodal Cyclic Translation Net-\nwork (MCTN) (Pham et al., 2019), that achieved\nSOTA results on various word-aligned human\nmultimodal language tasks. To compare the mod-\nels comprehensively, we adapt the connection-\nist temporal classiﬁcation (CTC) (Graves et al.,\n2006) method to the prior approaches (e.g., EF-\nLSTM, MCTN, RA VEN) that cannot be applied\ndirectly to the unaligned setting. Speciﬁcally,\nthese models train to optimize the CTC alignment\n6564\nTask Happy Sad Angry Neutral\nMetric Acc h F1h Acch F1h Acch F1h Acch F1h\n(Word Aligned) IEMOCAP Emotions\nEF-LSTM 86.0 84.2 80.2 80.5 85.2 84.5 67.8 67.1\nLF-LSTM 85.1 86.3 78.9 81.7 84.7 83.0 67.1 67.6\nRMFN (Liang et al., 2018) 87.5 85.8 83.8 82.9 85.1 84.6 69.5 69.1\nMFM (Tsai et al., 2019) 90.2 85.8 88.4 86.1 87.5 86.7 72.1 68.1\nRA VEN (Wang et al., 2019) 87.3 85.8 83.4 83.1 87.3 86.7 69.7 69.3\nMCTN (Pham et al., 2019) 84.9 83.1 80.5 79.6 79.7 80.4 62.3 57.0\nMulT (ours) 90.7 88.6 86.7 86.0 87.4 87.0 72.4 70.7\n(Unaligned) IEMOCAP Emotions\nCTC (Graves et al., 2006) + EF-LSTM 76.2 75.7 70.2 70.5 72.7 67.1 58.1 57.4\nLF-LSTM 72.5 71.8 72.9 70.4 68.6 67.9 59.6 56.2\nCTC + RA VEN (Wang et al., 2019) 77.0 76.8 67.6 65.6 65.0 64.1 62.0 59.5\nCTC + MCTN (Pham et al., 2019) 80.5 77.5 72.0 71.7 64.9 65.6 49.4 49.3\nMulT (ours) 84.8 81.9 77.7 74.1 73.9 70.2 62.5 59.7\nTable 3: Results for multimodal emotions analysis on IEMOCAP with aligned and non-aligned multimodal se-\nquences.\nobjective and the human multimodal objective si-\nmultaneously. We leave more detailed treatment\nof the CTC module to Appendix. For fair compar-\nisons, we control the number of parameters of all\nmodels to be approximately the same. The hyper-\nparameters are reported in Appendix. 1\n4.3 Quantitative Analysis\nWord-Aligned Experiments. We ﬁrst evaluate\nMulT on theword-aligned sequences— the “home\nturf” of prior approaches modeling human multi-\nmodal language (Sheikh et al., 2018; Tsai et al.,\n2019; Pham et al., 2019; Wang et al., 2019). The\nupper part of the Table 1, 2, and 3 show the results\nof MulT and baseline approaches on the word-\naligned task. With similar model sizes (around\n200K parameters), MulT outperforms the other\ncompetitive approaches on different metrics on all\ntasks, with the exception of the “sad” class results\non IEMOCAP.\nUnaligned Experiments. Next, we evaluate\nMulT on the same set of datasets in the unaligned\nsetting. Note that MulT can be directly applied to\nunaligned multimodal stream, while the baseline\nmodels (except for LF-LSTM) require the need of\nadditional alignment module (e.g., CTC module).\nThe results are shown in the bottom part of Ta-\nble 1, 2, and 3. On the three benchmark datasets,\nMulT improves upon the prior methods (some\nwith CTC) by 10%-15% on most attributes. Em-\n1All experiments are conducted on 1 GTX-1080Ti\nGPU. The code for our model and experiments can\nbe found in https://github.com/yaohungt/\nMultimodal-Transformer\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nEpochs\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775Mean Average Error\n(Unaligned) MOSEI Validation Performance\nMulT\nLFLSTM\nCTC+RAVEN\nCTC+MCTN\nCTC+EFLSTM\nFigure 5: Validation set convergence of MulT when\ncompared to other baselines on the unaligned CMU-\nMOSEI task.\npirically, we ﬁnd that MulT converges faster to\nbetter results at training when compared to other\ncompetitive approaches (see Figure 5). In addi-\ntion, while we note that in general there is a per-\nformance drop on all models when we shift from\nthe word-aligned to unaligned multimodal time-\nseries, the impact MulT takes is much smaller than\nthe other approaches. We hypothesize such perfor-\nmance drop occurs because the asynchronous (and\nmuch longer) data streams introduce more difﬁ-\nculty in recognizing important features and com-\nputing the appropriate attention.\nAblation Study. To further study the inﬂuence\nof the individual components in MulT, we per-\nform comprehensive ablation analysis using the\nunaligned version of CMU-MOSEI. The results\nare shown in Table 4.\nFirst, we consider the performance for only\n6565\ndisappointingtooandhorribletoo[sp]\nismoviethisofending\nthe\nunfortunatelybut\nText\nVision\nTime\nTimeEach block is a few video frames\n(Summarized bytemporal convolutions)\nLots of facial motions during this time\n(Waving DVD case angrily)\nFrowned\nImpatient/Angry\nMovie reviewer emphasizes\n“butunfortunately” & frowns\n(With drastic expression changes)\nEmphasized\nFigure 6: Visualization of sample crossmodal attention weights from layer 3 of [V →L] crossmodal transformer\non CMU-MOSEI. We found that the crossmodal attention has learned to correlate certain meaningful words (e.g.,\n“movie”, “disappointing”) with segments of stronger visual signals (typically stronger facial motions or expression\nchange), despite the lack of alignment between original L/V sequences. Note that due to temporal convolution,\neach textual/visual feature contains the representation of nearby elements.\n(Unaligned) CMU-MOSEI\nDescription Sentiment\nAcch\n7 Acch\n2 F1h MAEℓ Corrh\nUnimodal Transformers\nLanguage only 46.5 77.4 78.2 0.653 0.631\nAudio only 41.4 65.6 68.8 0.764 0.310\nVision only 43.5 66.4 69.3 0.759 0.343\nLate Fusion by using Multiple Unimodal Transformers\nLF-Transformer 47.9 78.6 78.5 0.636 0.658\nTemporally Concatenated Early Fusion Transformer\nEF-Transformer 47.8 78.9 78.8 0.648 0.647\nMultimodal Transfomers\nOnly [V, A→ L] (ours) 50.5 80.1 80.4 0.605 0.670\nOnly [L, A→ V ] (ours) 48.2 79.7 80.2 0.611 0.651\nOnly [L, V→ A] (ours) 47.5 79.2 79.7 0.620 0.648\nMulT mixing intermediate-\nlevel features (ours) 50.3 80.5 80.6 0.602 0.674\nMulT (ours) 50.7 81.6 81.6 0.591 0.691\nTable 4: An ablation study on the beneﬁt of MulT’s\ncrossmodal transformers using CMU-MOSEI.).\nusing unimodal transformers (i.e., language, au-\ndio or vision only). We ﬁnd that the language\ntransformer outperforms the other two by a large\nmargin. For example, for the Acc h\n2 metric, the\nmodel improves from 65.6 to 77.4 when compar-\ning audio only to language only unimodal trans-\nformer. This fact aligns with the observations in\nprior work (Pham et al., 2019), where the authors\nfound that a good language network could already\nachieve good performance at inference time.\nSecond, we consider 1) a late-fusion trans-\nformer that feature-wise concatenates the last\nelements of three self-attention transformers;\nand 2) an early-fusion self-attention trans-\nformer that takes in a temporal concatenation of\nthree asynchronous sequences [ ˆXL, ˆXV, ˆXA] ∈\nR(TL+TV+TA)×dq (see Section 3.2). Empirically,\nwe ﬁnd that both EF- and LF-Transformer (which\nfuse multimodal signals) outperform unimodal\ntransformers.\nFinally, we study the importance of individ-\nual crossmodal transformers according to the tar-\nget modalities (i.e., using [V,A →L], [L,A →\nV], or [L,V → A] network). As shown in\nTable 4, we ﬁnd crossmodal attention modules\nconsistently improve over the late- and early-\nfusion transformer models in most metrics on un-\naligned CMU-MOSEI. In particular, among the\nthree crossmodal transformers, the one where\nlanguage(L) is the target modality works best.\nWe also additionally study the effect of adapt-\ning intermediate-level instead of the low-level fea-\ntures from source modality in crossmodal atten-\ntion blocks (similar to the NMT encoder-decoder\narchitecture but without self-attention; see Sec-\ntion 3.1). While MulT leveraging intermediate-\nlevel features still outperform models in other ab-\nlative settings, we empirically ﬁnd adapting from\nlow-level features works best. The ablations sug-\ngest that crossmodal attention concretely beneﬁts\nMulT with better representation learning.\n4.4 Qualitative Analysis\nTo understand how crossmodal attention works\nwhile modeling unaligned multimodal data, we\nempirically inspect what kind of signals MulT\npicks up by visualizing the attention activations.\nFigure 6 shows an example of a section of the\ncrossmodal attention matrix on layer 3 of theV →\n6566\nL network of MulT (the original matrix has di-\nmension TL ×TV; the ﬁgure shows the attention\ncorresponding to approximately a 6-sec short win-\ndow of that matrix). We ﬁnd that crossmodal at-\ntention has learned to attend to meaningful signals\nacross the two modalities. For example, stronger\nattention is given to the intersection of words that\ntend to suggest emotions (e.g., “movie”, “disap-\npointing”) and drastic facial expression changes in\nthe video (start and end of the above vision se-\nquence). This observation advocates one of the\naforementioned advantage of MulT over conven-\ntional alignment (see Section 3.3): crossmodal\nattention enables MulT to directly capture po-\ntentially long-range signals, including those off-\ndiagonals on the attention matrix.\n5 Discussion\nIn the paper, we propose Multimodal Trans-\nformer (MulT) for analyzing human multimodal\nlanguage. At the heart of MulT is the cross-\nmodal attention mechanism, which provides a la-\ntent crossmodal adaptation that fuses multimodal\ninformation by directly attending to low-level fea-\ntures in other modalities. Whereas prior ap-\nproaches focused primarily on the aligned multi-\nmodal streams, MulT serves as a strong baseline\ncapable of capturing long-range contingencies, re-\ngardless of the alignment assumption. Empiri-\ncally, we show that MulT exhibits the best perfor-\nmance when compared to prior methods.\nWe believe the results of MulT on unaligned\nhuman multimodal language sequences suggest\nmany exciting possibilities for its future appli-\ncations (e.g., Visual Question Answering tasks,\nwhere the input signals is a mixture of static and\ntime-evolving signals). We hope the emergence\nof MulT could encourage further explorations on\ntasks where alignment used to be considered nec-\nessary, but where crossmodal attention might be\nan equally (if not more) competitive alternative.\nAcknowledgements\nThis work was supported in part by DARPA\nHR00111990016, AFRL FA8750-18-C-0014,\nNSF IIS1763562 #1750439 #1722822, Apple,\nGoogle focused award, and Samsung. We would\nalso like to acknowledge NVIDIA’s GPU support.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, Jean-\nnette N Chang, Sungbok Lee, and Shrikanth S\nNarayanan. 2008. Iemocap: Interactive emotional\ndyadic motion capture database. Language re-\nsources and evaluation, 42(4):335.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\nAshish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,\nZhifeng Chen, Yonghui Wu, and Macduff Hughes.\n2018. The best of both worlds: Combining recent\nadvances in neural machine translation. In ACL.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2018. Transformer-xl: Language\nmodeling with longer-term dependency.\nGilles Degottex, John Kane, Thomas Drugman, Tuomo\nRaitio, and Stefan Scherer. 2014. Covarepa collabo-\nrative voice analysis repository for speech technolo-\ngies. In ICASSP. IEEE.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nSri Harsha Dumpala, Rupayan Chakraborty, and\nSunil Kumar Kopparapu. 2019. Audio-visual fu-\nsion for sentiment classiﬁcation using cross-modal\nautoencoder. NIPS.\nPaul Ekman. 1992. An argument for basic emotions.\nCognition & emotion, 6(3-4):169–200.\nPaul Ekman, Wallace V Freisen, and Sonia Ancoli.\n1980. Facial signs of emotional experience. Journal\nof personality and social psychology, 39(6):1125.\nKathleen R Gibson, Kathleen Rita Gibson, and Tim In-\ngold. 1994. Tools, language and cognition in human\nevolution. Cambridge University Press.\nAlex Graves, Santiago Fern ´andez, Faustino Gomez,\nand J ¨urgen Schmidhuber. 2006. Connectionist\ntemporal classiﬁcation: Labelling unsegmented se-\nquence data with recurrent neural networks. In\nICML.\nYue Gu, Kangning Yang, Shiyu Fu, Shuhong Chen,\nXinyu Li, and Ivan Marsic. 2018. Multimodal af-\nfective analysis using hierarchical attention strategy\n6567\nwith word-level alignment. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics. Association for Computational\nLinguistics.\niMotions. 2017. Facial expression analysis.\nAngeliki Lazaridou, Nghia The Pham, and Marco Ba-\nroni. 2015. Combining language and vision with\na multimodal skip-gram model. arXiv preprint\narXiv:1501.02598.\nPaul Pu Liang, Ziyin Liu, Amir Zadeh, and Louis-\nPhilippe Morency. 2018. Multimodal language\nanalysis with recurrent multistage fusion. EMNLP.\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sentence\nembedding. arXiv preprint arXiv:1703.03130.\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Prismatic Inc, Steven J. Bethard, and\nDavid Mcclosky. 2014. The stanford corenlp natu-\nral language processing toolkit. In In ACL, System\nDemonstrations.\nJiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan\nNam, Honglak Lee, and Andrew Y Ng. 2011. Multi-\nmodal deep learning. In Proceedings of the 28th in-\nternational conference on machine learning (ICML-\n11), pages 689–696.\nAnkur P Parikh, Oscar T ¨ackstr¨om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. arXiv preprint\narXiv:1606.01933.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532–1543.\nHai Pham, Paul Pu Liang, Thomas Manzini, Louis-\nPhilippe Morency, and Barnabas Poczos. 2019.\nFound in translation: Learning robust joint repre-\nsentations by cyclic translations between modalities.\nAAAI.\nSoujanya Poria, Erik Cambria, Devamanyu Hazarika,\nNavonil Majumder, Amir Zadeh, and Louis-Philippe\nMorency. 2017. Context-dependent sentiment anal-\nysis in user-generated videos. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , vol-\nume 1, pages 873–883.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations.\nImran Sheikh, Sri Harsha Dumpala, Rupayan\nChakraborty, and Sunil Kumar Kopparapu. 2018.\nSentiment analysis using imperfect views from\nspoken language and acoustic modalities. In\nProceedings of Grand Challenge and Workshop on\nHuman Multimodal Language (Challenge-HML) ,\npages 35–39.\nNitish Srivastava and Ruslan R Salakhutdinov. 2012.\nMultimodal learning with deep boltzmann ma-\nchines. In Advances in neural information process-\ning systems, pages 2222–2230.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 5027–5038. Association for\nComputational Linguistics.\nGongbo Tang, Mathias M¨uller, Annette Rios, and Rico\nSennrich. 2018. Why self-attention? a targeted eval-\nuation of neural machine translation architectures.\narXiv preprint arXiv:1808.08946.\nYao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh,\nLouis-Philippe Morency, and Ruslan Salakhutdinov.\n2019. Learning factorized multimodal representa-\ntions. ICLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and\nKaiming He. 2018. Non-local neural networks. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7794–7803.\nYansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang,\nAmir Zadeh, and Louis-Philippe Morency. 2019.\nWords can shift: Dynamically adjusting word rep-\nresentations using nonverbal behaviors. AAAI.\nLei Yu, Jan Buys, and Phil Blunsom. 2016. Online seg-\nment to segment neural transduction. arXiv preprint\narXiv:1609.08194.\nJiahong Yuan and Mark Liberman. 2008. Speaker\nidentiﬁcation on the scotus corpus. Journal of the\nAcoustical Society of America, 123(5):3878.\nAmir Zadeh, Paul Pu Liang, Navonil Mazumder,\nSoujanya Poria, Erik Cambria, and Louis-Philippe\nMorency. 2018a. Memory fusion network for multi-\nview sequential learning. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence.\nAmir Zadeh, Rowan Zellers, Eli Pincus, and Louis-\nPhilippe Morency. 2016. Multimodal sentiment in-\ntensity analysis in videos: Facial gestures and verbal\nmessages. IEEE Intelligent Systems, 31(6):82–88.\nAmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Po-\nria, Erik Cambria, and Louis-Philippe Morency.\n2018b. Multimodal language analysis in the wild:\nCmu-mosei dataset and interpretable dynamic fu-\nsion graph. In ACL.\n6568\nA Positional Embedding\nA purely attention-based transformer network is\norder-invariant. In other words, permuting the or-\nder of an input sequence does not change trans-\nformer’s behavior or alter its output. One solution\nto address this weakness is by embedding the posi-\ntional information into the hidden units (Vaswani\net al., 2017).\nFollowing (Vaswani et al., 2017), we encode the\npositional information of a sequence of length T\nvia the sin and cos functions with frequencies dic-\ntated by the feature index. In particular, we de-\nﬁne the positional embedding (PE) of a sequence\nX ∈RT×d (where T is length) as a matrix where:\nPE[i,2j] = sin\n( i\n10000\n2j\nd\n)\nPE[i,2j+ 1] = cos\n( i\n10000\n2j\nd\n)\nfor i = 1,...,T and j = 0,⌊d\n2 ⌋. Therefore,\neach feature dimension (i.e., column) of PE are\npositional values that exhibit a sinusoidal pat-\ntern. Once computed, the positional embedding is\nadded directly to the sequence so that X+ PE en-\ncodes the elements’ position information at every\ntime step.\nB Connectionist Temporal Classiﬁcation\nConnectionist Temporal Classiﬁcation\n(CTC) (Graves et al., 2006) was ﬁrst pro-\nposed for unsupervised Speech to Text alignment.\nParticularly, CTC is often combined with the\noutput of recurrent neural network, which enables\nthe model to train end-to-end and simultaneously\ninfer speech-text alignment without supervision.\nFor the ease of explanation, suppose the CTC\nmodule now are aiming at aligning an audio\nsignal sequence [a1,a2,a3,a4,a5,a6] with length\n6 to a textual sequence “I am really really happy”\nwith length 5. In this example, we refer to\naudio as the source and texts as target signal,\nnoting that the sequence lengths may be different\nbetween the source to target; we also see that the\noutput sequence may have repetitive element (i.e.,\n“really”). The CTC (Graves et al., 2006) module\nwe use comprises two components: alignment\npredictor and the CTC loss.\nFirst, the alignment predictor is often chosen as\na recurrent networks such as LSTM, which per-\nforms on the source sequence then outputs the\npossibility of being the unique words in the tar-\nget sequence as well as a empty word (i.e., x).\nIn our example, for each individual audio sig-\nnal, the alignment predictor provides a vector of\nlength 5 regarding the probability being aligned to\n[x, ‘I’, ‘am’, ‘really’, ‘happy’].\nNext, the CTC loss considers the negative log-\nlikelihood loss from only the proper alignment for\nthe alignment predictor outputs. The proper align-\nment, in our example, can be results such as\ni) [x, ‘I’, ‘am’, ‘really’, ‘really’, ‘happy’];\nii) [‘I’, ‘am’, x, ‘really’, ‘really’, ‘happy’];\niii) [‘I’, ‘am’, ‘really’, ‘really’, ‘really’, ‘happy’];\niv) [‘I’, ‘I’, ‘am’, ‘really’, ‘really’, ‘happy’]\nIn the meantime, some examples of the subopti-\nmal/failure cases would be\ni) [x, x, ‘am’, ‘really’, ‘really’, ‘happy’];\nii) [‘I’, ‘am’, ‘I’, ‘really’, ‘really’, ‘happy’];\niii) [‘I’, ‘am’, x, ‘really’, x, ‘happy’]\nWhen the CTC loss is minimized, it implies the\nsource signals are properly aligned to target sig-\nnals.\nTo sum up, in the experiments that adopting\nthe CTC module, we train the alignment predic-\ntor while minimizing the CTC loss. Then, ex-\ncluding the probability of blank words, we mul-\ntiply the probability outputs from the alignment\npredictor to source signals. The source signal\nis hence resulting in a pseudo-aligned target sin-\ngal. In our example, the audio signal is then\ntransforming to a audio signal [ a′\n1,a′\n2,a′\n3,a′\n4,a′\n5]\nwith sequence length 5, which is pseudo-aligned\nto [’I’, ’am’, ’really’, ’really’, ’happy’].\nC Hyperparameters\nTable 5 shows the settings of the various MulTs\nthat we train on human multimodal language\ntasks. As previously mentioned, the models are\ncontained at roughly the same sizes as in prior\nworks for the purpose of fair comparison. For hy-\nperparameters such as the dropout rate and number\nof heads in crossmodal attention module, we per-\nform a basic grid search. We decay the learning\nrate by a factor of 10 when the validation perfor-\nmance plateaus.\n6569\nCMU-MOSEI CMU-MOSI IEMOCAP\nBatch Size 16 128 32\nInitial Learning Rate 1e-3 1e-3 2e-3\nOptimizer Adam Adam Adam\nTransformers Hidden Unit Size d 40 40 40\n# of Crossmodal Blocks D 4 4 4\n# of Crossmodal Attention Heads 8 10 10\nTemporal Convolution Kernel Size (L/V/A) (1 or 3)/3/3 (1 or 3)/3/3 3/3/5\nTextual Embedding Dropout 0.3 0.2 0.3\nCrossmodal Attention Block Dropout 0.1 0.2 0.25\nOutput Dropout 0.1 0.1 0.1\nGradient Clip 1.0 0.8 0.8\n# of Epochs 20 100 30\nTable 5: Hyperparameters of Multimodal Transformer (MulT) we use for the various tasks. The “# of Crossmodal\nBlocks” and “# of Crossmodal Attention Heads” are for each transformer.\nD Features\nThe features for multimodal datasets are extracted\nas follows:\n- Language. We convert video transcripts\ninto pre-trained Glove word embeddings\n(glove.840B.300d) (Pennington et al., 2014).\nThe embedding is a 300 dimensional vector.\n- Vision. We use Facet (iMotions, 2017) to in-\ndicate 35 facial action units, which records\nfacial muscle movement (Ekman et al., 1980;\nEkman, 1992) for representing per-frame ba-\nsic and advanced emotions.\n- Audio. We use COV AREP (Degottex et al.,\n2014) for extracting low level acoustic fea-\ntures. The feature includes 12 Mel-frequency\ncepstral coefﬁcients (MFCCs), pitch track-\ning and voiced/unvoiced segmenting fea-\ntures, glottal source parameters, peak slope\nparameters and maxima dispersion quotients.\nDimension of the feature is 74.",
  "topic": "Crossmodal",
  "concepts": [
    {
      "name": "Crossmodal",
      "score": 0.8389158248901367
    },
    {
      "name": "Computer science",
      "score": 0.7412171363830566
    },
    {
      "name": "Gesture",
      "score": 0.6198850870132446
    },
    {
      "name": "Transformer",
      "score": 0.5565226674079895
    },
    {
      "name": "Modalities",
      "score": 0.5390957593917847
    },
    {
      "name": "Pairwise comparison",
      "score": 0.5021367073059082
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.4752640128135681
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4444606900215149
    },
    {
      "name": "Speech recognition",
      "score": 0.4279119074344635
    },
    {
      "name": "Multimodal learning",
      "score": 0.4212820827960968
    },
    {
      "name": "Natural language processing",
      "score": 0.3479972183704376
    },
    {
      "name": "Perception",
      "score": 0.25006216764450073
    },
    {
      "name": "Visual perception",
      "score": 0.13652822375297546
    },
    {
      "name": "Psychology",
      "score": 0.09631741046905518
    },
    {
      "name": "Engineering",
      "score": 0.07579445838928223
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    }
  ]
}