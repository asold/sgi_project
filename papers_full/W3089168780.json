{
  "title": "Bidirectional Representation Learning From Transformers Using Multimodal Electronic Health Record Data to Predict Depression",
  "url": "https://openalex.org/W3089168780",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2347588372",
      "name": "Meng Yi-wen",
      "affiliations": [
        "University of California, Los Angeles",
        "Computational Diagnostics (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2743467098",
      "name": "Speier William",
      "affiliations": [
        "Computational Diagnostics (United States)",
        "University of California, Los Angeles"
      ]
    },
    {
      "id": null,
      "name": "Ong, Michael K.",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A4221473089",
      "name": "Arnold, Corey W.",
      "affiliations": [
        "Computational Diagnostics (United States)",
        "University of California, Los Angeles"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964006392",
    "https://openalex.org/W6773649962",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W1985329916",
    "https://openalex.org/W6778642046",
    "https://openalex.org/W6739651123",
    "https://openalex.org/W2611715281",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2129330384",
    "https://openalex.org/W2039056175",
    "https://openalex.org/W2134431612",
    "https://openalex.org/W1956509155",
    "https://openalex.org/W1845587167",
    "https://openalex.org/W2216133675",
    "https://openalex.org/W6754497374",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6680930200",
    "https://openalex.org/W6685812147",
    "https://openalex.org/W2343760456",
    "https://openalex.org/W3036914761",
    "https://openalex.org/W2174344915",
    "https://openalex.org/W2625625371",
    "https://openalex.org/W2690721124",
    "https://openalex.org/W6685764666",
    "https://openalex.org/W6726186668",
    "https://openalex.org/W2912654919",
    "https://openalex.org/W2951704914",
    "https://openalex.org/W2125674401",
    "https://openalex.org/W2587860706",
    "https://openalex.org/W2809396336",
    "https://openalex.org/W2997494090",
    "https://openalex.org/W6738631160",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6765671411",
    "https://openalex.org/W2016127234",
    "https://openalex.org/W1779612606"
  ],
  "abstract": "Advancements in machine learning algorithms have had a beneficial impact on representation learning, classification, and prediction models built using electronic health record (EHR) data. Effort has been put both on increasing models' overall performance as well as improving their interpretability, particularly regarding the decision-making process. In this study, we present a temporal deep learning model to perform bidirectional representation learning on EHR sequences with a transformer architecture to predict future diagnosis of depression. This model is able to aggregate five heterogenous and high-dimensional data sources from the EHR and process them in a temporal manner for chronic disease prediction at various prediction windows. We applied the current trend of pretraining and fine-tuning on EHR data to outperform the current state-of-the-art in chronic disease prediction, and to demonstrate the underlying relation between EHR codes in the sequence. The model generated the highest increases of precision-recall area under the curve (PRAUC) from 0.70 to 0.76 in depression prediction compared to the best baseline model. Furthermore, the self-attention weights in each sequence quantitatively demonstrated the inner relationship between various codes, which improved the model's interpretability. These results demonstrate the model's ability to utilize heterogeneous EHR data to predict depression while achieving high accuracy and interpretability, which may facilitate constructing clinical decision support systems in the future for chronic disease screening and early detection.",
  "full_text": "  \nAbstract‚Äî Advancements in machine learning algorithms have \nhad a beneficial impact on representation learning, classification, \nand prediction models built using electronic health record (EHR) \ndata. Effort has been put both on increasing model s‚Äô overall \nperformance as well  as improving their interpretability, \nparticularly regarding the decision-making process. In this study, \nwe present a temporal deep learning model to perform \nbidirectional representation learning on EHR sequences with a \ntransformer architecture to predict future diagnosis of \ndepression. This model is able to aggregate five heterogenous and \nhigh-dimensional data sources from the EHR and process them \nin a temporal manner for chronic disease prediction at various \nprediction windows. We applied the current trend of pretraining \nand fine-tuning on EHR data to outperform the current state -of-\nthe-art in chronic disease prediction , and to demonstrate the \nunderlying relation between EHR codes in the sequence .  The \nmodel generated  the highest increases of precision -recall area \nunder the curve (PRAUC) from 0.70  to 0.76 in depression \nprediction compared to the best baseline model. Furthermore, \nthe self-attention weight s in each sequence quantitatively \ndemonstrated the  inner relation ship between various codes, \nwhich improved the model‚Äôs interpretability . These results \ndemonstrate the model‚Äôs ability to utilize heterogeneous EHR \ndata to predict depression  while achieving high accuracy and \ninterpretability, which may facilitate constructing clinical \ndecision support systems  in the future for chronic disease \nscreening and early detection. \n \nIndex Terms‚ÄîClinical decision support, natural language \nprocessing, electronic health record, depression, temporal \nrepresentation and reasoning. \nI. INTRODUCTION \nlectronic health record (EHR) systems have become the \nmain method of documenting patients‚Äô historical medical \nrecords over the last decade [1]. The latest report from the \n \nThis work was supported by the National Heart, Lung, and Blood Institute \n(NIH/NHLBI R01HL141773). \nY. Meng, is with  the Computational Diagnostics Lab, the Department of \nBioengineering, at the University of California Los Angeles, 924 Westwood \nBlvd, Suite 420, CA 90024 USA (e-mail: lanyexiaosa@ucla.edu) \nW. Speier is with the Computational Diagnostics Lab, the Department of \nRadiology at the University of California Los Angeles, 924 Westwood Blvd, \nSuite 420, CA 90024 USA (e-mail: speier@ucla.edu). \nM.K. Ong is with the Department of Medicine at the University of \nCalifornia Los Angeles, 1100 Glendon Avenue, Suite 850, CA 90024 USA (e-\nmail:mong@mednet.ucla.edu). \nC.W. Arnold is with the Computational Diagnostics Lab, the Department \nof Bioengineering, the Department of Radiology and the Department of \nPathology at the University of California Los Angeles, 924 Westwood Blvd, \nSuite 420, CA 90024 USA (e-mail:cwarnold@ucla.edu) \nOffice of the National Coordinator for Health Information \nTechnology (ONC), stated that nearly 84% of hospitals have \nadopted at least a basic EHR system, a nine-fold increase from \n2008 [2]. EHRs are composed of data from different \nmodalities, documented in a sequence for each patient \nencounter, including demographic information, diagnoses, \nprocedures, medications or prescriptions, clinical notes written \nby physicians , images, and laboratory results, which \ncontribute to their high dimensionality and heterogeneity  [3], \n[4]. Deep learning algorithms  enable the usage of E HR data \nnot only as a documenting method for billing purpose s, but \nalso as a source of tremendous amount of data to construct \nclassification or prediction models, which build the foundation \nfor creating clinical decision support systems and personalized \nprecision medicine . However, there is an unsolved challenge \nof achieving high accuracy while providing adequate \nexplanation for models‚Äô decision -making processes. Although \nseveral effort s have attempted to improve model \ninterpretability [5]‚Äì[7], they did not address the problem of  \ndata heterogeneity  that is pervasive in medi cal research  as \nEHRs are often composed of data from various modalities in a \nsequential structure.   \nDepression is one typ ical comorbidity of chronic disease  \nand a major cause of disability worldwide  [8]. It often leads to \na number of adverse outcomes, including increased risk of \nself-harm, premature mortality , and the development of \ncomorbid general medical conditions, such as heart disease, \nstroke, and obesity [9]. Within a year of experiencing \ndepression symptoms, patients are 4.4 times more like ly to \ndevelop major depressive disorder (MDD) [10], a \nheterogeneous spect rum disorder with a variety of onsets, \ntreatment responses, and comorbidities. The economic burden \nof individuals with MDD was $210.5 billion in 2010, which \nhas increased from $173.2 billion in 2005  [11]. Although \ndepression has become highly prevalent and costly, the current \nscreening process used in clinics for patients with high risk of \ndepression only produced a true positive rate of 50% [12]. \nHence, much informatics effort has been put to increase the \naccuracy using machine learning algorithms [13]‚Äì[15].  \nThe goal of this study is to create a model with high \ninterpretability for predicti ng future diagnosis  of depression  \nwhile being  able to accommodate  the heterogeneity of EHR \ndata and process it effectively in a temporal manner . We  \npropose a Bidirectional Representation Learning model with a \nTransformer architecture on Multimodal EHR (BRLTM). This \nBRLTM is  able to aggregate five EHR data modalities: \ndiagnoses, procedure codes, medications,  demographic \nYiwen Meng, William Speier, Member, Michael K. Ong and Corey W. Arnold, Member, IEEE \nBidirectional Representation Learning from \nTransformers using Multimodal Electronic Health \nRecord Data to Predict Depression \nE \ninformation, and clinical notes. It enables modeling EHR data \nwith the widely used  two-stage pretraining and finetuning \napproach, which is  significantly improve d from previous \nworks [4]‚Äì[6], [16] . The transf ormer architecture offers \ngeneralized representation learning on EHR data and the self-\nattention mechanism highly improves the model‚Äôs \ninterpretability by showing a ssociation of various EHR codes \nin sequences quantitatively [17]. Our results also improve the \nperformance of bidirectional learning than forward -only \nmethods in sequence modeling.  This approach could help in \nclinical practice by identifying individuals potentially at risk \nfor developing depression within a specific time interval who \nshould be screened (and potentially treated) for depression. \nII. RELATED WORK \nPerformance on sequential or temporal learning tasks has \nbeen largely advanced by the advent of  recurrent neural \nnetwork (RNN) and its variations , including long-short-term-\nmemory (LSTM) and gated recurrent unit (GRU). Previous \nstudies have  applied these methods on modeling time -series \nmedical data, particularly on EHR data to predict future \ndiagnoses [18], [19]. [5] first added a reverse time attention \nmechanism to RNN for heart fa ilure prediction, which \nimproved the model‚Äôs interpretability by showing the temporal \neffect of events . [20] also achieved improving their model‚Äôs \ninterpretability using self -attention, but only applied  it on \ndiagnosis and procedure codes . [21] was able to predict \nclinical interventions from a deep  neural network using lab \nresults and demographics, but with a smaller number of \nfeatures (34). MiME focused on learning the inner structure of \nan EHR by constructing a hierarchy of diagnosis level, visit \nlevel, and patient level  embeddings [16]. The HCET model \nextended this hierarchical structure  by removing the \nrequirement of linked structure between diagnosis codes and \nprocedure codes and medication while enabling attention on \neach E HR data modality  to increase the model‚Äôs \ninterpretability [4]. However, these models did not provide a \ngeneralized approach to model EHR  data with a two-stage \npretraining and finetuning, nor reveal the latent association on \nevery code instead of each aggregated modality in terms of \nmodel interpretability. \n   Recent developments in natural language processing (NLP) \nprovide a number of potential methods applicable to EHR \ndata. NLP uses sequential learning on word sequences, which \ncan be applied to  EHR data that is comp rised of time series \nsequences from different data modalities. Dipole  exhibited the \npotential of bidirectional learning on E HR data using an RNN \nwith concatenated attention to predict diagnosis of diabetes [6] \nbased on diagnosis and procedure codes . Choi et al. enhanced \ntheir model‚Äôs latent representation learning with a graph \nconvolution transformer (GCT) , but did not  perform \nsequential learning , focusing only on a single  patient \nencounter [22]. The BEHRT model  [23] first realized a two-\nstage transfer learning approach with the BERT model  [24], \nbut only applied it to diagnosis codes with a low dimensional \nfeature space (301). Meanwhile, it merely relied on diagnosis \ncodes as the true label for every disease, which actually \nreduced the prediction sensit ivity or specificity , due to \ninaccuracy and incompleteness in  International Classification \nof Disease ( ICD) codes and they are mostly for billing \npurposes [25]. Thus, our BRLTM model aims to implement a \ngeneralized representation learning method on multimodal and \nhigh dimensional EHR data using the two -stage pretraining \nand finetuning approach, capable of aggregating five EHR \nmodalities. In the meantime, we  provided a feasible approach \nto increase the label accuracy for diagnosis of depression , \nwhich also enhanced the validity of model‚Äôs prediction  \nperformance. Finally, this method also demonstrates the power \nof using incomplete or historical EHR sequences for future \nprediction whereas BERT model takes complete word \nsequences or sentences for downstream classification or text \ngeneration.   \nIII. DATA PREPROCESSING \nWe selected patients based on three primary diagnoses: \nmyocardial infarction (MI), breast cancer, and liver cirrhosis , \nto capture a spectrum of clinical complexity. Generally, MI \nrepresents the least complexity, with acute onset, resolution, \nand straight -forward treatment. Breast cancer is i ncreasingly \ncomplicated in terms of diagnoses and treatment options. \nFinally, a patient with liver cirrhosis may have many sequelae, \ngenerating a complex EHR representation. Patients for this \nwork were identified from our EHR in accordance with an \nInstitutional Review  Board ( IRB) (#14-000204) approved \nprotocol. Each patient visit had EHR data types consisting of \ndiagnosis codes in International Classification of Disease, \nninth revision (ICD -9) form at, procedure codes in Current \nProcedural Terminology (CPT) format, medication lists, \ndemographic information, and clinical notes  represented as \n100 topics  using latent Dirichlet allocation ( LDA) analysis \n[26]‚Äì[28]. EHR data are structured in a tabular format in SQL, \nand transformed in a pandas  data frame for Python coding  \n[29]. All patient records coded with ICD -9 values for MI, \nbreast cancer, or liver cirrhosis between 2006-2013 were \nincluded. Demographics were limited to the patient‚Äôs gender \nand age. Initially, there were 45,208 patients  and after \npreprocessing to remove visits without at least one  ICD-9 \ncode, CP T code, medication , or topic feature  and eliminate \npatients with fewer than two visits , 43,967 patients were \nincluded in the analysis.  More importantly, data after 15 days \nprior to the  time of depression diagnosis was excluded for \ndepressed patients to ensure no data leakage while all data \nwere included for non -depressed ones. We followed the same \ndata selection criteria as in the HCET model [4] with four \nprediction windows prior to  the time of depression diagnosis: \ntwo weeks, three months, six  months, and one year. The \nlength of each data window was restricted to six months \ninstead of patient‚Äôs entire history to avoid bias towards \nTABLE I \nEXAMPLE TOPICS \nTopic \nNum Top five words \n55 heart, cardiomyopathy, oht, failure, vt \n42 liver, cirrhosis, ascites, lactulose, hepatic \n29 value, component, creat, plt, hgb \n27 transplant, tacrolimus, liver, renal, daily \n \npatients with longer medical histories.  For LDA feature \ngeneration, stop words were first removed from the note dat a. \nThe MALLET [30] software package was used to fit the \nmodel with asymmetric priors. The model was fit over 1 ,000 \niterations. Four example topics are shown in Table I.  \n    Patient Health Questionnaire (PHQ -9) scores [31], the most \ncommon way to identify the diagnosis of depression , were not \navailable for this patient cohort during the data collecti on \nperiod. Hence, depression onset was identified by three \nmethods: depression related ICD -9 code s, inclusion of an \nantidepressant drug in a patient‚Äôs medication list , or \nappearance of an antide pressant drug in clinical notes , which \nhas been used previously [4]. \nIV. METHODS \nIn NLP implementations, BERT models process words \nsequentially. This method can be applied to EHR data by \nanalyzing ICD-9 codes, CPT codes, medication lists, and  \ntopics as  code sequences representing a patient‚Äôs visits . Full \nICD-9 codes  are high dimensional that are sparsely \nrepresented with 9 ,285 distinct codes in our dataset. As in \n[19], dimensionality was reduced by grouping codes by the \nthree digits before the decimal point  to reduce its feature \ndimension to 1,131 . Each demographic is added as a n \nindividual feature and repeated for every sequence. Pretraining \nwas conducted through  masked language modeling (MLM)  to \npredict the mask code based on EHR sequences  [24]. This \npretraining is a n unsupervised approach to learn the latent \nstructure of EHR data as the result is able to  show the \nassociation between two code in the sequence quantitatively. \nAfter pretraining, the saved model was added a classification \nhead to finetune for the  downstream task of chronic disease \nprediction. The feature dimension as the unique number of \ncodes for five modalities were listed as follows: \n‚Ä¢ Diagnoses: 1,131 \n‚Ä¢ Procedures: 7,048 \n‚Ä¢ Medications: 4,181 \n‚Ä¢ Demographics: 2 \n‚Ä¢ Topics: 100 \nA.  BRLTM model for EHR representation learning \n     Notation for the BRLTM model is included in Table II. Eq. \n(1) shows  a patient‚Äôs EHR, composed by different visits \nranging from ùëâ1 to ùëâùêø, where L is the length of the hospital \nvisits. Two symbolic tokens ùê∂ùêøùëÜ and ùëÜùê∏ùëÉ are adopted here : \nùê∂ùêøùëÜ denotes the starting point of  the EHR and ùëÜùê∏ùëÉ denotes \nthe separation between two consecutive visits. This formulates \nthe input in Fig. 1. \n \n     ùê∏ùêªùëÖ: (ùê∂ùêøùëÜ, ùëâ1, ùëÜùê∏ùëÉ, ùëâ2, ùëÜùê∏ùëÉ, ‚Ä¶ , ùëâùêø)     (1) \n \nEach of the visit ùëâùë° is comprised of EHR codes ùëã, as shown in \nEq. (2), where the number of codes is ùëöùë°, w hich varies for \neach visit. Every code is from the vocabulary of the dataset :  \nùê∑: diagnosis, ùê∂: procedure, ùëÄ: medication and ùëá: topics. \n \nùëâùë°: (ùëã1, ùëã2, ‚Ä¶ , ùëãùëöùë° ), ùëã ùúñ {ùê∑, ùê∂, ùëÄ, ùëá}     (2) \n \n   The original BERT models has three types of embedding s: \n \nFig. 1. Architecture of the BRLTM model for EHR representation learning . The subscripts show the original value for each embedding. CLS and SEP are \nsymbolic tokens stands for the beginning of EHR and separation of two visits adjacent to each other, respectively. D, C, M, a nd T denote diagnoses, procedures, \nmedications, and topics, respectively. The last row denotes the sum of the five embeddings as the output embedding. \nTABLE II \nNOTATION USED IN THE FORMULATION OF BRLTM \nNotation Definition \nùëâùë° EHR code sequences for tth visit, t ‚àà [1, L] \nL Length of a patient‚Äôs hospital visits \nùëãùëñ One EHR code in patient‚Äôs tth visit, i ‚àà [1, ùëöùë°] \nùëöùë° Number of codes in the tth visit \nùê∑ Vocabulary of diagnosis codes \nùê∂ Vocabulary of CPT codes \nùëÄ Vocabulary of medications \nùëá Vocabulary of topic features \nùê∂ùêøùëÜ Starting point of one patient‚Äôs EHR sequence \nùëÜùê∏ùëÉ  A separation notation to separate codes in two consecutive \nvisits   \n \ntoken, position, and segment [24]. In our BRLTM model, we \ntreated each token embedding as a code embedding and \nextended the model‚Äôs ability to aggregate demographics by \nadding age and gender embeddings, shown in Fig. 1. There are \nfive types of embeddings  which are  summed to generate  the \nfinal output embeddings for training. Data after the depression \ndiagnosis were excluded to avoid data leakage. Any data \nwithin 15 days prior to the diagnosis time  was excluded to \nensure the predictive power  of the model . For non-depressed \npatients, the last time step of the EHR was substituted for the \ndiagnosis time. Code embeddings are composed of four EHR \ndata modalities mentioned in Eq. (2), while the original BERT \nmodel only takes word tokens as code embeddings .  As in the \noriginal BERT model, position and seg ment embedding s \nindicate the position of one code in the full sequence which \ndistinguishes codes in adjacent visits , which is highly \nefficient. Hence, we followed the same structure of them here \nand adopted pre -determined instead of learned encodings for \npositional embedding s to avoid weak learning of positional \nembedding due to high variety in  a patient‚Äôs sequence length. \nThe position embedding s play an important role in sequence \nlearning, equivalent to the recurrent structure in RNN s. \nAnnotating the position of each code in the sequence enables \nthe model to  capture the positional interactions  among EHR \ndata modalities . However, position embeddin gs do not tell \nwhether codes are from the same visit or not. Hence, segment \nembeddings are used  to provide extra information to \ndifferentiate codes in adjacent visits by alternating between \ntwo trainable vectors, depicted as A and B in Fig. 1.  \nAge and gender embeddings are repeated in every position \nof the sequence.  Combining code embedding s with the age \nembedding not only enables the model to use age information \nas a feature,  but also provides temporal information in the \nsequence. As shown Fig. 2, the final embeddings are input into \na bidirectional sequential learning  step with transformer \narchitecture as in the  BERT model  [24]. Hence, t he latent \ncontextual representation of  five data modalities in temporal \nEHR sequences can be efficiently learned from the \naggregation of these five embeddings. This architecture is \ncapable of aggregating multimodal EHR data into a single \nmodel and processing them in a temporal manner, as well as \ninvestigating the inner association contingency between them \nin various visits . In total, the model can perform temporal \nrepresentation learning from a patient‚Äôs EHR. More \nimportantly, it realizes the common  two-stage transfer \nlearning approach on EHR modeling , which has been widely  \nadopted and has achieved outstanding performance in \ncomputer vision [32] as well as NLP [24], [33].  \nB. Pretraining with Masked Language Modeling (MLM) \n    An EHR is composed of multimodal code sequences, which \nis similar to the way  that language is composed of word \nsequences. Hence, we hypothesized that the advantage of  deep \nbidirectional sequential learning in language modeling over \neither a left -to-right model or the shallow concatenation of a \nleft-to-right and a right -to-left model can be transferred to \nEHR modeling . As a consequence , we adopted the same \npretraining approach of MLM from the original BERT paper  \n[24]. Namely, we randomly selected  15% of EHR codes and \nmodified them according to the following procedures:  \n \n‚Ä¢ 80% of the time replace them with [MASK] \n‚Ä¢ 10% of the time replace them with a random EHR \ncode \n‚Ä¢ 10% of the time do nothing and keep them \nunchanged \n \n   This structure in MLM forces the model to learn the \ndistributional contextual representation between EHR codes as \nthe model does not know which codes are masked or which \ncodes have replaced by a random code. EHR modeling is not \naffected significantly because  only 1.5% (10% of 15%) of \ncodes are randomly replaced. This random replacement brings \na small perturbation that distracts the model from learning the \ntrue contextual sequences of the EHR and forces the model to \nidentify the noise and continue learning the overal l temporal \nprogression. We followed the precision score  (true positives \ndivided by predicted positives) at a threshold of 0.5  as the \nmetric to evaluate pre-training MLM task , which was same \napproach as the BERHT model [23]. The average value is \ncalculated over every masked code  and over all patients. We \nfollowed results from previous models [23], [24] with random \nsearch to find the best set of hyperparameters during training. \nIn addition, we conducted an ablation study to investigate the \ncontribution of each data modality by training the model using \nall five modalities first and train ing again after removing \ntopics and CPT codes individually while training without both \nof them  as the last comparison . The data used for MLM is \nshown in the second column of Table III, displaying the total \nTABLE III \nSTATISTICS OF DATASETS FOR TWO TRAINING APPROACHES \nDatasets pretraining finetuning \nPatients with MI 10,616 (2,915  \ndepressed) \n2,943 (1,280 \ndepressed) \nPatients with breast \ncancer \n23,3077 (4,483 \ndepressed) \n5,568 (1,960 \ndepressed) \nPatients with liver \ncirrhosis \n11,757 (2,359 \ndepressed) \n2,218 (772 depressed) \nGender 70.18% female 72.54% female \nAge 65.78 ¬± 14.99, \nmin: 18, max 100 \n68.78 ¬± 15.46, \nmin: 18, max 98 \nSequence length 54.64 ¬± 45.37,  \nmin:2, max: 1,186 \n54.64 ¬± 45.37, \nmin:2, max: 180 \n \n \nFig. 2. Illustration of bidirectional learning with transformer architecture . The \norange squares are final embedding s in Fig. 1, which are the input sequences \nhere. Trm stands for the transformer while the green squares denote the output \nsequence. O denotes the output for each code after learning. \n\nnumber of patients for each of three primary diagnos es, and \nthe number of patients depressed patient  displayed in the \nbrackets. Note that some patients had more than one primary \ndiagnosis. Furthermore, distribution of age, gender and EHR \nsequence length are also displayed here.  \nC. Fine tuning to predict diagnosis of depression \nAfter pretraining to learn the latent contextual \nrepresentation of the EHR, one feed-forward classification \nlayer was added for finetuning on a specific dataset. We \nfollowed the same data selection criteria as in the HCET \nmodel [4] with four prediction windows pr ior to time of \ndepression diagnosis: two weeks, three months, six months , \nand one year. The length of each data window was restricted \nto six months instead of patient‚Äôs entire history to avoi d bias \ntowards patients with longer medical histories.  Patients who \nhad at least one ICD -9, CPT, medication or topic feature in all \nfour time windows were included. After processing data based \non this method, 10,148 patients were selected, where 3,747 \nwere diagnosed with depression. Basic statistics of the data for \nthis prediction task are shown in the third column of Table III. \nFinally, predicting performance of each model was evaluated \nin receiver characteristic area under curve (ROCAUC) and \nPRAUC. \nD. Training Details  \n    The BERT model was implemented  in Pytorch 1.4 and \ntrained on a workstation equippe d with  an Intel Xeon E3 -\n1245, 32 GB RAM and a 12G  NVIDIA TitanX GPU. We \nfollowed the training scheduler with the Adam [34] optimizer \nused the original BERT model [24] and set the warmup \nproportion and weight to 0.01 and 0.1, respectively . The \nGaussian error linear unit ( GELU) rather than  the standard \nReLu was used as the non-linear activation fun ction in the \nhidden layers. Pretraining of MLM used the first dataset with \nthe minibatch of 256 patients for 100 epochs and evaluated at \nevery 20 iteration s. The dataset for Finetuning of the \nprediction task underwent 10 random data splits: 70% training, \n10% validation, and 20% test, and trained with minibatch of \n64 patients for 50 epochs . Dropout of 0.1 was set to both \nhidden size and multi -head attention to address overfitting . \nThe source code and more detailed description of the model is \navailable at https://github.com/lanyexiaosa/brltm. \nE. Baseline models  \nThe following models described in Section II were used to \ncompare the prediction performance to our BRLTM model: \nDipole [6], MiME*  [16], HCET  [4], BERHT [23]. We \nmodified MiME to MiME* as the original MiME model \nrequires external knowledge of linked relation between ICD -9 \ncodes and associated CPT codes and medication lists during \neach visit, which was not applica ble to this dataset. Paired t-\ntest was used to compute the statistical significance when \nTABLE IV \nRESULTS OF PRETRAINING WITH MLM \nData combination All No topic No CPT No \n(topic+CPT) \nVocabulary size 12,460  12,360 5,412 5,312 \nPrecision 0.4248 0.4324 0.4836 0.5086 \nLearning rate 1e-4 1e-4 1e-4 1e-4 \nEmbedding size 216 240 252 264 \nAttention layers  9 9 6 6 \nAttention heads  12 12 12 12 \nIntermediate layer 512 512 256 256 \nAll means using all five modalities.  The result of MLM in BEHRT model \nas the last column is compared here. \n \nFig. 3. Quantitative analysis of self -attention from two patients‚Äô EHR sequences shown in color plots. CLS and SEP represent the beginning of the record and \nseparators between visits, respectively. Topic features are represented as the five most commonly associat ed words. Each example is presented as two identical \ncolumns as the left one represents the code of interest colored in grey while the right one indicates the corresponding associations to  the highlighted code on the \nleft. The intensity of the blue color o n the right column denotes the strength of the attention  score; the deeper blue color suggests higher self -attention score and \nhence the stronger the latent association. \n\ncomparing two models. \nV. RESULTS \nA. Pretraining with MLM and self-attention of code \nsequences \nTable IV presents the results for MLM including the \noptimal hyperparameter setting s on various combination of \nEHR data modalities . According to the result, the precision \nscore raises gradually from 0.4208 to 0.5086 as the vocabulary \nsize decreases by excluding more data modalities. The optimal \nembedding size also follow s this trend as 216 for all data and \n264 for data without topics and CPT . The number of attention \nlayers and the numb er of multi-head has the opposite trend, \nchanging from 9 to 6 and 512 to 256, respectively.  \nAfter training, attention weights for any EHR sequence can \nbe extracted by enabling the output variable in the BERT \nencoder function and better presented via this visualization \ntool: https://github.com/jessevig/bertviz. Fig. 3 (a) and (b) \nexhibits the self -attention weights from two patients‚Äô E HR \nsequences, retrieved from the attention component of the last \nlayer of the BERT model . It was illustrated in Fig. 3(a) that \nthis patient was diagnosed with malignant neoplasm of the \nliver. The self -attention weight indicated its highest \nassociation with the topic feature associated with the words \n‚Äútransplant, tacrolimus, liver, renal, and daily‚Äù and second \nhighest relation to the ICD-9 code for ‚ÄúOrgan or tissue \nreplaced by transplant .‚Äù The topic feature with the words \n‚Äúliver, hepatitis, pain, hcc, and abdominal‚Äù described the fact \nthat the patient was undergoing a liver transplant after the \noriginal diagnosis.  Fig. 3(b) displays an example patient \ndiagnosed initially with an unspecified joint disorder which \nled to a topic feature of ‚Äúpain, knee, hip, fracture, and \nshoulder‚Äù shown later in the EHR sequence. The darker color \nsuggests the stronger association of this topic feature to the \noriginal diagnosis code (diagnostic radiology imaging ) and a \nweaker latent relation to the diagnosis of diabetes and the \nmedication ceftriaxone.  The attention scores demonstrate the \nassociation of the  patient‚Äôs health status with an original \ndiagnosis of joint disorder and a comorbidity of diabetes \ndeveloped later . This matches the meta -analysis that arthritic \npatients have 61% higher odds of having diabetes compared to \nthe population without arthritis [35].  \nB. Comparison of Performance in Depression Prediction  \nTable V shows the ROCAUC and PRAUC from all baseline \nmodels and our BRLTM model at the four prediction \nwindows. The BR LTM model achieved the highest \nperformance in each prediction window  with statistically \nsignificant improvements over the next best model (HCET) . \nBEHRT generated slightly better results than MiME* in the \ntwo shortest time windows, but MiME* reached higher \nnumbers in longer windows . This result follows those \nobserved in HCET where ICD-9 possessed attention weights \nhigher than the average in smaller prediction windows while it \nwas lower than the average in larger windows . The prediction \nperformance was slightly improved with Dipole  which used \nICD-9 and CPT  codes in a  bidirectional l earning metho d. \nFinally, t here is a consistent decrease of accuracy for every \nmodel as the prediction window moves further away from the \ntime of diagnosis. \nC. Prediction performance for each primary diagnosis  \nTable V I displays the  individual results for each of three \nprimary diagnoses in prediction windows of two -weeks and \none-year. Our BR LTM model also achieved the best \nperformance for all three primary diagnoses in both prediction \nwindows. The ROCAUC from all three diseases within  each \nmodel is similar even though the number of patients with \nbreast cancer was substantially higher than the other diseases \n(n=5,568), which indicate s no bias toward any primary \ndiagnosis for the task of predicting diagnosis of depression.  It \nis notable that while the PRAUC for patients with myocardial \ninfarction is relatively higher than other two , the difference in \nthe B RLTM model is relatively small . Dipole generated a \nmean increase of around 0.02 both in ROC AUC and PRAUC \nTABLE V \nCOMPARISON OF PREDICTION PERFORMANCE FOR DIFFERENT MODELS \nPrediction window Two weeks Three months Six months One year \nModels ROCAUC PRAUC ROCAUC PRAUC ROCAUC PRAUC ROCAUC PRAUC \nMiME* \nICD-9+CPT+Medication \n0.76 \n(0.01) \n0.67 \n(0.02) \n0.74 \n(0.01) \n0.64 \n(0.02) \n0.72 \n(0.02) \n0.61 \n(0.01) \n0.70 \n(0.01) \n0.61 \n(0.01) \nBEHRT \nICD-9 \n0.77 \n(0.02) \n0.68 \n(0.01) \n0.75 \n(0.02) \n0.65 \n(0.01) \n0.71 \n(0.01) \n0.61 \n(0.02) \n0.69 \n(0.02) \n0.60 \n(0.02) \nDipole \nICD-9+CPT \n0.78 \n(0.02) \n0.70 \n(0.01) \n0.76 \n(0.02) \n0.67 \n(0.02) \n0.75 \n(0.01) \n0.65 \n(0.02) \n0.74 \n(0.01) \n0.64 \n(0.01) \nHCET  \nAll \n0.81 \n(0.01) \n0.73 \n(0.01) \n0.80 \n(0.01) \n0.70 \n(0.02) \n0.79 \n(0.01) \n0.69 \n(0.01) \n0.78 \n(0.01) \n0.67 \n(0.01) \nBRLTM \nAll \n0.85 ‚Ä† \n(0.02) \n0.78 ‚Ä† \n(0.01) \n0.84 ‚Ä† \n(0.01) \n0.76 ‚Ä† \n(0.01) \n0.83 ‚Ä† \n(0.01) \n0.74 ‚Ä† \n(0.02) \n0.81 ‚Ä† \n(0.01) \n0.73 ‚Ä† \n(0.01) \nValues in parenthesis stand for standard deviations across randomizations and bold values denotes the highest in each column. ‚Ä† indicates the value is \nsignificantly better than that from the best baseline model HCET (p<0.05). The words after each models denotes the input data modalities where all means all \nfive in our dataset. \nacross all diseas es. In addition, HCET achieved better values \nthan Dipole with higher improvement in PRAU C than \nROCAUC. The BRLTM model further improve d the \nperformance from HCET with highest increase of 0.06 in \nROCAUC for MI in the window of two weeks and 0.05 in \nPRAUC for breast cancer and liver cirrhosis in  the one-year \nwindow.  \nFig. 4 contains the confusion matrices individually for three \nprimary diagnos es in the two -week prediction window from \nfour models . T he output probability was calibrat ed using \nisotonic regression [36] using a threshold of 0.5, and numbers \nwere aggregated from 10 -fold cross validation . The class \ndistribution was imbalanced with a  smaller portion of  \ndepressed patients  for each primary diagnosis.  MiME* \nreached a higher portion of false positive than false negative \nand Dipole managed to reduce both numbers slightly. Our \nBRLTM model significantly decreased the false positives by \nalmost 50% from HCET while reducing false negatives by \nroughly 40% for MI and 30% for breast cancer and liver \ncirrhosis. Hence, it achieved outstanding average precision \nand recall of 0.94 and 0.84, respectively, over the three \nprimary diagnoses. \nThe computation time for every model is shown in Table \nVII, where values were averaged over batches for training and \ntesting. According to the result, BRLTM obtained the highest \ncomputation time both in training and test over baseline \nmodels, while MiME* reached the lowest.  \nVI. DISCUSSION \nAccording to results in Table s V and V I, our BRLTM \nmodel sufficiently resolved the data heterogeneity issue by \nrealizing bidirectional sequential learning and enabling the \nsturcture to aggregate multimoal EHR data , which achieved \nthe best performance in predicting future diagnoisis of \ndepression in  four prediction windows. Additionally, the \ncomparison to other models demonstrates the advantage of \nincluding more data modalities for predictive power as the \nBEHRT model only took diagnosis codes in their study . On \nthe other hand, t he better results from Diople than MiME* \nvalidates the advantage of bi directional learning over single \ndirection, as medication was less frequently present than ICD -\n9 and CPT, which Dipole did not take as the input . However, \nits lower performance than HCET, which adopted the forward-\nonly sequnetial  learning, highlights the importa nce to \naggregate topics feature and demographics  as Dipole  only \ninput ICD -9 and CPT codes,  while HCET was capable of \nincluding all five modalities. Meanwhile, these results indicate \nthat each model‚Äôs performance consistently declines as the \nprediction window moves further away from the diagnosis \n \nFig. 4 Confusion matrices for patients separated by three primary diagnosis at \na window of two weeks for four models. The numbers are aggregated together \nwith 10 -fold cross validation. Label 0 means non -depressed while 1 means \ndepressed.  \nTABLE VII \nCOMPUTATION TIME FOR EACH MODEL PER BATCH \nModels Training (s) Testing (s) \nMIME* 0.78 0.57 \nBEHRT 1.55 1.03 \nDipole 1.23 0.89 \nHCET 0.94 0.67 \nBRLTM 2.37 1.83 \n \nTABLE VI \nCOMPARISON OF PREDICTION PERFORMANCE FOR THREE PRIMARY DIAGNOSES \nPrediction \nwindow Two weeks One year \nDiseases Breast cancer MI Liver cirrhosis Breast cancer MI Liver cirrhosis \nModels ROC \nAUC \nPR \nAUC \nROC \nAUC \nPR \nAUC \nROC \nAUC \nPR \nAUC \nROC \nAUC \nPR \nAUC \nROC \nAUC \nPR \nAUC \nROC \nAUC \nPR \nAUC \nMiME* \n \n0.77 \n(0.01) \n0.67 \n(0.02) \n0.75 \n(0.01) \n0.70 \n(0.02) \n0.76 \n(0.02) \n0.67 \n(0.01) \n0.71 \n(0.02) \n0.61 \n(0.01) \n0.69 \n(0.02) \n0.64 \n(0.01) \n0.70 \n(0.01) \n0.61 \n(0.02) \nBERHT 0.78 \n(0.02) \n0.68 \n(0.01) \n0.77 \n(0.01) \n0.71 \n(0.02) \n0.77 \n(0.01) \n0.68 \n(0.02) \n0.70 \n(0.01) \n0.59 \n(0.02) \n0.70 \n(0.01) \n0.62 \n(0.01) \n0.69 \n(0.02) \n0.60 \n(0.02) \nDipole 0.79 \n(0.02) \n0.69 \n(0.02) \n0.78 \n(0.01) \n0.71 \n(0.02) \n0.78 \n(0.02) \n0.69 \n(0.01) \n0.75 \n(0.01) \n0.63 \n(0.02) \n0.74 \n(0.02) \n0.66 \n(0.02) \n0.74 \n(0.01) \n0.63 \n(0.01) \nHCET \n \n0.81  \n(0.01) \n0.73  \n(0.01) \n0.79  \n(0.01) \n0.77  \n(0.01) \n0.80  \n(0.01) \n0.72  \n(0.01) \n0.78  \n(0.01) \n0.67  \n(0.01) \n0.77  \n(0.01) \n0.71  \n(0.01) \n0.77  \n(0.01) \n0.66  \n(0.01) \nBRLTM 0.85  \n(0.01) \n0.76 \n(0.01) \n0.85 \n(0.01) \n0.78  \n(0.01) \n0.84 \n(0.01) \n0.75 \n(0.01) \n0.80 \n(0.01) \n0.72 \n(0.01) \n0.81 \n(0.01) \n0.74 \n(0.01) \n0.80 \n(0.01) \n0.71 \n(0.01) \n \ntime point, which agrees with our expectation tha t records \ncloser to the diagnosis are more likely to contain relevant \ninformation and provide better predictions.  Table IV shows \nthe observation that for a larger vocabul ary size or more data \nmodalities, a smaller embedding size should be used, but the \nnumber of attention layers and intermediate layer si ze should \nbe increased, while no strong perference of the learning rate \nand number of attention heads. The results also approves \nmodel‚Äôs flexible structure of several tunable hyperparameters, \nespeically in attetnion layers, enabling it to process various \ntypes of EHR data which may be collected from different \ninsitutions. \nThe BRLTM model demonstrated another advantage of  \nimproving model‚Äôs interpretability by quantitalively revealing \nthe latent association between code s in the sequence using \nself-attetion and multi -head attetion . More importantly, we \nsucessfully r ealized the common two -stage transfer learning \napporach of pretraining and finetuing  on modeling EHR  data. \nPrivacy issues related to EHR data, restrict the ability of \ninstitutions to share data, which substantially hinders the \ndevelopment in this field. The two -stage approach allows  \ninstitutions with access to large amount of EHR data  to \nprovide the pretrained model as a general EHR feature \nextractor so that  others can take the advantage  by only \nfinetuning the pretrained model on the customized dataset for \nspecific task s [37], [38] . This process benefits EHR \nrepresentation learning  and lays the foundation to allow \nadequate predictive power for model s built on small EHR \ndatasets. Furthermore, the BRLTM model also provides a \ngeneralized architecture that can be adopted with every EHR \nsystem by increasing the vocabulary of code embedding or by \nstacking more embedding layers for addition data modalities \nnot used in this work.  \nThe results from our BRLTM model and the previous \nHCET model both emphasize the critical contribution of \nclinical notes to build predictive models from EHR. We used \ntopic modeling with LDA to extract semantic features of \ntopics, which wa s limited by the bag of words assumption . \nThus, future studies could utilize more recent NLP tools,  such \nas BERT  [24] or GPT -2 [33] to optimize contextual \nrepresentation of clinical  notes, which could further improve \nthe overall performance . In addition , our model‚Äôs predictive \npower was exhibited in predicti ng future diagnosis of \ndepression, which was a binary classification task.  Future \nstudies could expand model‚Äôs robustness in prediction by \nperforming multiclass prediction simultaneously for other \nchronic diseases , such as hypertension, diabetes, and obesity . \nReducing labeling error s is also critical to build  an accura te \nmachine learning  model. Only relying on diagnosis codes as \nthe label of every disease  injects noise in the label, which was \nwidely adopted in previous studies [16], [23] . Instead, we \nadopted three criteria for determining depression diagnosis, \nwhich mitigated the labeling error compared to previous \nstudies. Future prospective studies could continue the effort to \nacquire more precise labels for E HR, such as  administering \nPHQ-9 surveys periodically to increase precision in depression \ndiagnosis. Thus, more robust predictive models could be \nconstructed to track the disease progression as well as early \ndetection, which could provide  more applications for the \nBRLTM model in future work. Finally,  all the results were \nbased on this patient cohort and EHR data, hence conducting \nmore testing on EHR data collected from various cohorts as \nwells as predicting different chronic diseases could bring more \ninsights on the generalizability of BRLTM model. In addition, \nother EHR data modalities such as laboratory results [21] were \nnot included in the BRLTM model as they were unav ailable \nfor collection . Future studies may extend the model to \naggregate more data modalities to further make it as a \ngeneralized model.  \nVII. CONCLUSION \nWe have developed a bidirectional deep learning model  \nBRLTM to perform temporal representation learning  on \nmultimodal E HR data and successfully realized two -stage \npretraining and finetuning.  These effort s contributed to \nsignificant improvement on chronic disease prediction as well \nas advancement of model interpretability by the quanti tative \nanalysis of self -attention weights of  EHR sequences. The \nresults demonstrate the ability of the two -stage transfer \nlearning approach for EHR modeling to overcome limitations \nin the amount of available data and that bidirectional learning \ncan provide superior performance to  unidirectional. This \napproach facilitates the development of clinical decision \nsupport system s for chronic disease prediction, such  as a \nscreening tool for patients at high risk  of depression, thereby \nenabling early intervention. Future works could test the model \nwith more EHR data modalities , increasing the labeling \nprecision of depression diagnosis or improving semantic \nfeature extraction on clinical notes. \nREFERENCES \n[1] G. S. Birkhead, M. Klompas, and N. R. Shah, ‚ÄúUses of electronic \nhealth records for public health surveillance to advance public \nhealth,‚Äù Annu. Rev. Public Health, vol. 36, pp. 345‚Äì359, 2015, doi: \n10.1146/annurev-publhealth-031914-122747. \n[2] S. T. & P. V. Henry, J., Pylypchuk, Y., ‚ÄúAdoption of Electronic \nHealth Record Systems among U.S. Non-Federal Acute Care \nHospitals: 2008-2015,‚Äù ONC Data Brief, no.35., no. 35, pp. 2008‚Äì\n2015, 2016. \n[3] B. Shickel, P. J. Tighe, A. Bihorac, and P. Rashidi, ‚ÄúDeep EHR: A \nSurvey of Recent Advances in Deep Learning Techniques for \nElectronic Health Record (EHR) Analysis,‚Äù IEEE J. Biomed. Heal. \nInformatics, vol. 22, no. 5, pp. 1589‚Äì1604, 2018, doi: \n10.1109/JBHI.2017.2767063. \n[4] Y. Meng, W. Speier, M. Ong, and C. W. Arnold, ‚ÄúHCET‚ÄØ: \nHierarchical Clinical Embedding with Topic Modeling on \nElectronic Health Record for Predicting Depression,‚Äù IEEE J. \nBiomed. Heal. Informatics, 2020, doi: 10.1109/JBHI.2020.3004072. \n[5] E. Choi., M. T. Bahadori, J. A. Kulas, A. Schuetz, W. F. Stewart, \nand J. Sun, ‚ÄúRETAIN: An Interpretable Predictive Model for \nHealthcare using Reverse Time Attention Mechanism,‚Äù Adv. Neural \nInf. Process. Syst. 29 (NIPS 2016), 2016, doi: 10.1063/1.859355. \n[6] F. Ma, R. Chitta, J. Zhou, Q. You, T. Sun, and J. Gao, ‚ÄúDipole: \nDiagnosis prediction in healthcare via attention-based bidirectional \nrecurrent neural networks,‚Äù 2017, doi: 10.1145/3097983.3098088. \n[7] Y. Meng et al., ‚ÄúA Machine Learning Approach to Classifying Self-\nReported Health Status in a Cohort of Patients with Heart Disease \nUsing Activity Tracker Data,‚Äù IEEE J. Biomed. Heal. Informatics, \nvol. 24, no. 3, pp. 878‚Äì884, 2020, doi: 10.1109/JBHI.2019.2922178. \n[8] S. L. James et al., ‚ÄúGlobal, regional, and national incidence, \nprevalence, and years lived with disability for 354 Diseases and \nInjuries for 195 countries and territories, 1990-2017: A systematic \nanalysis for the Global Burden of Disease Study 2017,‚Äù Lancet, vol. \n392, no. 10159, pp. 1789‚Äì1858, 2018, doi: 10.1016/S0140-\n6736(18)32279-7. \n[9] A. Akincigil and E. B. Matthews, ‚ÄúNational rates and patterns of \ndepression screening in primary care: Results from 2012 and 2013,‚Äù \nPsychiatr. Serv., vol. 68, no. 7, pp. 660‚Äì666, 2017, doi: \n10.1176/appi.ps.201600096. \n[10] E. Horwath, J. Johnson, G. L. Klerman, and M. M. Weissman, \n‚ÄúDepressive Symptoms as Relative and Attributable Risk Factors \nfor First-Onset Major Depression,‚Äù Arch. Gen. Psychiatry, vol. 49, \nno. 10, p. 817, Oct. 1992, doi: \n10.1001/archpsyc.1992.01820100061011. \n[11] P. E. Greenberg, A.-A. Fournier, T. Sisitsky, C. T. Pike, and R. C. \nKessler, ‚ÄúThe Economic Burden of Adults With Major Depressive \nDisorder in the United States (2005 and 2010),‚Äù J Clin Psychiatry, \nvol. 76, no. 2, pp. 155‚Äì162, 2015, doi: 10.4088/JCP.14m09298. \n[12] A. J. Mitchell, A. Vaze, S. Rao, and R. Infi, ‚ÄúClinical diagnosis of \ndepression in primary care‚ÄØ: a meta-analysis,‚Äù Lancet, vol. 374, no. \n9690, pp. 609‚Äì619, 2009, doi: 10.1016/S0140-6736(09)60879-5. \n[13] S. H. Huang, P. LePendu, S. V Iyer, M. Tai-Seale, D. Carrell, and \nN. H. Shah, ‚ÄúToward personalizing treatment for depression: \npredicting diagnosis and severity,‚Äù J. Am. Med. Informatics Assoc., \nvol. 21, no. 6, pp. 1069‚Äì1075, Nov. 2014, doi: 10.1136/amiajnl-\n2014-002733. \n[14] H. Jin, S. Wu, and P. Di Capua, ‚ÄúDevelopment of a Clinical \nForecasting Model to Predict Comorbid Depression Among \nDiabetes Patients and an Application in Depression Screening \nPolicy Making,‚Äù Prev. Chronic Dis., vol. 12, pp. 1‚Äì10, 2015, doi: \n10.5888/pcd12.150047. \n[15] J. Zhang, H. Xiong, Y. Huang, H. Wu, K. Leach, and L. E. Barnes, \n‚ÄúM-SEQ: Early detection of anxiety and depression via temporal \norders of diagnoses in electronic health data,‚Äù Proc. - 2015 IEEE \nInt. Conf. Big Data, IEEE Big Data 2015, pp. 2569‚Äì2577, 2015, \ndoi: 10.1109/BigData.2015.7364054. \n[16] E. Choi, C. Xiao, J. Sun, and W. F. Stewart, ‚ÄúMime: Multilevel \nmedical embedding of electronic health records for predictive \nhealthcare,‚Äù in Advances in Neural Information Processing Systems, \n2018, pp. 4547‚Äì4557. \n[17] A. Vaswani et al., ‚ÄúAttention Is All You Need,‚Äù Adv. Neural Inf. \nProcess. Syst., no. Nips, pp. 5998‚Äì6008, 2017. \n[18] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzel, ‚ÄúLearning to \nDiagnose with LSTM Recurrent Neural Networks,‚Äù pp. 1‚Äì18, 2015, \ndoi: 10.14722/ndss.2015.23268. \n[19] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun, \n‚ÄúDoctor AI: Predicting Clinical Events via Recurrent Neural \nNetworks.,‚Äù Proc. Mach. Learn. Healthc. 2016, vol. 56, pp. 301‚Äì\n318, 2016. \n[20] T. Bai, B. L. Egleston, S. Zhang, and S. Vucetic, ‚ÄúInterpretable \nrepresentation learning for healthcare via capturing disease \nprogression through time,‚Äù Proc. ACM SIGKDD Int. Conf. Knowl. \nDiscov. Data Min., pp. 43‚Äì51, 2018, doi: \n10.1145/3219819.3219904. \n[21] H. Suresh, N. Hunt, A. Johnson, L. A. Celi, P. Szolovits, and M. \nGhassemi, ‚ÄúClinical Intervention Prediction and Understanding \nusing Deep Networks,‚Äù arXiv Prepr. arXiv1705.08498v1, pp. 1‚Äì16, \n2017. \n[22] E. Choi et al., ‚ÄúLearning the Graphical Structure of Electronic \nHealth Records with Graph Convolutional Transformer,‚Äù Proc. \nAAAI Conf. Artif. Intell., vol. 34, no. 01, pp. 606‚Äì613, 2020, doi: \n10.1609/aaai.v34i01.5400. \n[23] Y. Li et al., ‚ÄúBEHRT: Transformer for Electronic Health Records,‚Äù \nSci. Rep., vol. 10, no. 1, pp. 1‚Äì17, 2020, doi: 10.1038/s41598-020-\n62922-y. \n[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: Pre-\ntraining of Deep Bidirectional Transformers for Language \nUnderstanding,‚Äù arXiv:11810.04805, 2018. \n[25] W. Q. Wei, P. L. Teixeira, H. Mo, R. M. Cronin, J. L. Warner, and \nJ. C. Denny, ‚ÄúCombining billing codes, clinical notes, and \nmedications from electronic health records provides superior \nphenotyping performance,‚Äù J. Am. Med. Informatics Assoc., vol. 23, \nno. e1, pp. 20‚Äì27, 2016, doi: 10.1093/jamia/ocv130. \n[26] C. Arnold and W. Speier, ‚ÄúA topic model of clinical reports,‚Äù in \nProceedings of the 35th international ACM SIGIR conference on \nResearch and development in information retrieval, 2012, pp. 1031‚Äì\n1032. \n[27] C. W. Arnold, A. Oh, S. Chen, and W. Speier, ‚ÄúEvaluating topic \nmodel interpretability from a primary care physician perspective,‚Äù \nComput. Methods Programs Biomed., vol. 124, pp. 67‚Äì75, 2015. \n[28] W. Speier, M. K. M. K. Ong, and C. W. C. W. Arnold, ‚ÄúUsing \nphrases and document metadata to improve topic modeling of \nclinical reports,‚Äù J. Biomed. Inform., vol. 61, pp. 260‚Äì266, 2016, \ndoi: 10.1016/j.jbi.2016.04.005. \n[29] W. McKinney, ‚Äúpandas: a Foundational Python Library for Data \nAnalysis and Statistics,‚Äù Python High Perform. Sci. Comput., vol. \n14, no. 9, pp. 583‚Äì591, 2011, doi: 10.1002/mmce.20381. \n[30] A. K. McCallum, ‚Äú{MALLET: A Machine Learning for Language \nToolkit},‚Äù 2002. \n[31] M. Kurt Kroenke, MD; Robert L. Spitzer, ‚ÄúThe PHQ-9‚ÄØ: A New \nDepression Measure,‚Äù Psychiatr. Ann., vol. 32, no. 9, pp. 509‚Äì515, \n2002. \n[32] O. Russakovsky et al., ‚ÄúImageNet Large Scale Visual Recognition \nChallenge,‚Äù Int. J. Comput. Vis., vol. 115, no. 3, pp. 211‚Äì252, 2015, \ndoi: 10.1007/s11263-015-0816-y. \n[33] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \n‚ÄúLanguage Models are Unsupervised Multitask Learners,‚Äù OpenAI \nBlog, vol. 1, no. 8, 2019. \n[34] D. P. Kingma and J. L. Ba, ‚ÄúAdam: A method for stochastic \noptimization,‚Äù in 3rd International Conference on Learning \nRepresentations, ICLR 2015, 2015, pp. 1‚Äì15. \n[35] Q. Dong, H. Liu, D. Yang, and Y. Zhang, ‚ÄúDiabetes mellitus and \narthritis: Is it a risk factor or comorbidity?,‚Äù Med. (United States), \nvol. 96, no. 18, pp. 1‚Äì6, 2017, doi: \n10.1097/MD.0000000000006627. \n[36] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, ‚ÄúOn Calibration \nof Modern Neural Networks,‚Äù in 34th International Conference on \nMachine Learning, 2017, pp. 1321‚Äì1330. \n[37] S. Soni and K. Roberts, ‚ÄúEvaluation of Dataset Selection for Pre-\nTraining and Fine-Tuning Transformer Language Models for \nClinical Question Answering,‚Äù in 12th Conference on Language \nResources and Evaluation (LREC 2020), 2020, no. May, pp. 5532‚Äì\n5538. \n[38] D. Liu and T. Miller, ‚ÄúFederated pretraining and fine tuning of \nBERT using clinical notes from multiple silos,‚Äù arXiv:2002.08562, \n2020. \n ",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I4210121124",
      "name": "Computational Diagnostics (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    }
  ]
}