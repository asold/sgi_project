{
    "title": "Utilizing a Pretrained Language Model (BERT) to Classify Preservice Physics Teachers’ Written Reflections",
    "url": "https://openalex.org/W4225279564",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1775197432",
            "name": "Peter Wulff",
            "affiliations": [
                "Heidelberg University of Education",
                "Heidelberg University"
            ]
        },
        {
            "id": "https://openalex.org/A4225349385",
            "name": "Lukas Mientus",
            "affiliations": [
                "University of Potsdam"
            ]
        },
        {
            "id": "https://openalex.org/A2009062913",
            "name": "Anna Nowak",
            "affiliations": [
                "University of Potsdam"
            ]
        },
        {
            "id": "https://openalex.org/A2001169298",
            "name": "Andreas Borowski",
            "affiliations": [
                "University of Potsdam"
            ]
        },
        {
            "id": "https://openalex.org/A1775197432",
            "name": "Peter Wulff",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4225349385",
            "name": "Lukas Mientus",
            "affiliations": [
                "University of Potsdam"
            ]
        },
        {
            "id": "https://openalex.org/A2009062913",
            "name": "Anna Nowak",
            "affiliations": [
                "University of Potsdam"
            ]
        },
        {
            "id": "https://openalex.org/A2001169298",
            "name": "Andreas Borowski",
            "affiliations": [
                "University of Potsdam"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2133132547",
        "https://openalex.org/W4323526630",
        "https://openalex.org/W2614883504",
        "https://openalex.org/W1982382618",
        "https://openalex.org/W2093621717",
        "https://openalex.org/W2043393559",
        "https://openalex.org/W2084341220",
        "https://openalex.org/W2595522556",
        "https://openalex.org/W2913469663",
        "https://openalex.org/W3039916702",
        "https://openalex.org/W3026838079",
        "https://openalex.org/W2593180205",
        "https://openalex.org/W3015242699",
        "https://openalex.org/W6748002556",
        "https://openalex.org/W2086667273",
        "https://openalex.org/W2020746585",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2040940389",
        "https://openalex.org/W2086245846",
        "https://openalex.org/W2007407316",
        "https://openalex.org/W6964356498",
        "https://openalex.org/W2518712911",
        "https://openalex.org/W3104750522",
        "https://openalex.org/W2606089314",
        "https://openalex.org/W6730267373",
        "https://openalex.org/W2123169053",
        "https://openalex.org/W2160053845",
        "https://openalex.org/W2102716846",
        "https://openalex.org/W6902728790",
        "https://openalex.org/W2019600690",
        "https://openalex.org/W6619002981",
        "https://openalex.org/W2170644967",
        "https://openalex.org/W1501215627",
        "https://openalex.org/W4241868989",
        "https://openalex.org/W2124172474",
        "https://openalex.org/W2130395160",
        "https://openalex.org/W2791401923",
        "https://openalex.org/W1969103607",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2150762032",
        "https://openalex.org/W1964459817",
        "https://openalex.org/W2040081664",
        "https://openalex.org/W2250873048",
        "https://openalex.org/W2138984333",
        "https://openalex.org/W1996650435",
        "https://openalex.org/W1995986722",
        "https://openalex.org/W3110648930",
        "https://openalex.org/W2134090198",
        "https://openalex.org/W2009434747",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W1970043627",
        "https://openalex.org/W2000810817",
        "https://openalex.org/W4245298043",
        "https://openalex.org/W4292366075",
        "https://openalex.org/W2053615142",
        "https://openalex.org/W2039031214",
        "https://openalex.org/W2613402814",
        "https://openalex.org/W4235947318",
        "https://openalex.org/W1781458262",
        "https://openalex.org/W3193685060",
        "https://openalex.org/W7000635770",
        "https://openalex.org/W4285530062",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W2594018861",
        "https://openalex.org/W2911994647",
        "https://openalex.org/W27341848",
        "https://openalex.org/W1892968353",
        "https://openalex.org/W2051339053",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3091838623",
        "https://openalex.org/W1985100237",
        "https://openalex.org/W2048431936",
        "https://openalex.org/W2321621029"
    ],
    "abstract": null,
    "full_text": "Vol.:(0123456789)\nInternational Journal of Artificial Intelligence in Education (2023) 33:439–466\nhttps://doi.org/10.1007/s40593-022-00290-6\n1 3\nARTICLE\nUtilizing a Pretrained Language Model (BERT) to Classify \nPreservice Physics Teachers’ Written Reflections\nPeter Wulff1  · Lukas Mientus2 · Anna Nowak2 · Andreas Borowski2 \nAccepted: 18 January 2022 / Published online: 2 May 2022 \n© The Author(s) 2022, corrected publication 2023\nAbstract\nComputer-based analysis of preservice teachers’ written reflections could enable \neducational scholars to design personalized and scalable intervention measures to \nsupport reflective writing. Algorithms and technologies in the domain of research \nrelated to artificial intelligence have been found to be useful in many tasks related to \nreflective writing analytics such as classification of text segments. However, mostly \nshallow learning algorithms have been employed so far. This study explores to what \nextent deep learning approaches can improve classification performance for seg-\nments of written reflections. To do so, a pretrained language model (BERT) was uti-\nlized to classify segments of preservice physics teachers’ written reflections accord-\ning to elements in a reflection-supporting model. Since BERT has been found to \nadvance performance in many tasks, it was hypothesized to enhance classification \nperformance for written reflections as well. We also compared the performance of \nBERT with other deep learning architectures and examined conditions for best per -\nformance. We found that BERT outperformed the other deep learning architectures \nand previously reported performances with shallow learning algorithms for classifi-\ncation of segments of reflective writing. BERT starts to outperform the other mod-\nels when trained on about 20 to 30% of the training data. Furthermore, attribution \nanalyses for inputs yielded insights into important features for BERT’s classifica-\ntion decisions. Our study indicates that pretrained language models such as BERT \ncan boost performance for language-related tasks in educational contexts such as \nclassification.\nKeywords Reflective writing · NLP · Deep learning · Science education\n * Peter Wulff \n peter.wulff@ph-heidelberg.de\n1 Physics Education Research, Heidelberg University of Education, Im Neuenheimer Feld \n560-562, 69120 Heidelberg, Germany\n2 Institute for Physics and Astronomy, University of Potsdam, Karl-Liebknecht-Straße 24/25, \n14476 Potsdam-Golm, Germany\n440 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\nMotivation\nIn their profession, teachers are required to make complex decisions under uncer -\ntainty (Grossman et  al., 2009). Professional growth for teachers is therefore \nrelated to coping with uncertainty and learning from one’s experiences (Clarke & \nHollingsworth, 2002; Kolb, 1984). It has been argued that reflection is an impor -\ntant link to connect personal experiences with theoretical knowledge that helps \nteachers to understand and cope with uncertain situations (Korthagen & Kes-\nsels, 1999). The explicit instruction of reflection was found to be a key ingredi-\nent of effective university-based teacher education programs (Darling-Hammond \net al., 2017). A goal for university-based teacher education programs is thus to \nsupport teachers to become reflective practitioners: professionals who are able to \ncapitalize on practical teaching experiences to grow professionally (Schön, 1987; \nKorthagen, 1999; Zeichner, 2010).\nTo assist preservice teachers in reflecting on essential aspects of their teaching \nexperiences, instructors often require them to write about teaching experiences in \na structured way guided by reflection-supporting models (Lai & Calandra, 2010; \nPoldner et al., 2014; Korthagen & Kessels, 1999). However, the analysis of the \nwritten reflection is underdeveloped and expectations for what a quality reflection \nentails vary from context to context (Buckingham Shum et al., 2017). Moreover, \nfeedback by instructors for the written reflections was found to be rather holistic \nthan analytic (Poldner et al., 2014). This is because content analysis of written \nreflections is a labor-intensive process (Ullmann, 2019).\nComputerized methods in the domain of artificial intelligence research can \nhelp to analyze written reflections (Wulff et al., 2020). For example, supervised \nmachine learning methods can categorize written reflections in reference to \nreflection-supporting models (Ullmann, 2019). Recent research in the domain of \nartificial intelligence research has found that advances in model architectures for \ndeep neural networks such as transformers as pretrained language models can fur -\nther improve classification performance for various language-related tasks (Dev -\nlin et al., 2018). These architectures might well be applicable to reflective writing \nanalytics and hence provide education researchers novel tools to design applica-\ntions that facilitate accurate reflective writing analytics (Buckingham Shum et al., \n2017).\nThe purpose of this study is to explore the possibilities of using transformer-\nbased pretrained language models for the analysis of written reflections. To do \nso, we utilized a pretrained language model to analyze preservice physics teach-\ners’ written reflections that are grounded in a theoretically derived reflection-sup-\nporting model. Written reflections were collected over several years in a teach-\ning placement and labeled by human raters according to the reflection-supporting \nmodel. The labelled texts formed the training data for a transformer-based pre-\ntrained language model called Bidirectional Encoder Representations for Trans-\nformers (BERT) (Devlin et al., 2018). To assess the performance of the pretrained \nlanguage model, several other widely used deep learning architectures for mod-\neling language were fit to the data and compared with BERT’s performance to \n441\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \nclassify segments in the held-out test data. To better understand optimal condi-\ntions for training the models, we compared hyperparameters and dependence \nof classification performance to size of training data between the deep learning \narchitectures. Finally, important input features for BERT’s classification deci-\nsions were explored.\nReflective Writing in Teacher Education\nReflective thinking has been conceptualized as the antidote to intuitive, fast thinking \n(Dewey, 1933; Kahneman, 2012). It has been argued to be a naturally (yet rarely) \noccurring thinking process that relates to meaning-making from experience and can \nbe characterized to be systematic, rigorous, and disciplined (Clarà, 2015; Dewey, \n1933; Rodgers, 2002). Reflection “involves actively monitoring, evaluating, and \nmodifying one’s thinking” (Lin et al., 1999, p. 43). In the context of reflection-sup-\nporting teacher education, Korthagen (2001, p. 58) defines reflection as “the mental \nprocess of trying to structure or restructure an experience, a problem, or existing \nknowledge or insights.” Along these lines, reflective thinking has been characterized \nto include (1) a process with specific thinking activities (e.g., describing, analyzing, \nevaluating), (2) target content (e.g., teaching experience, personal theory, assump-\ntions), and (3) specific goals and reasons for engaging in this kind of thinking (e.g., \nto think differently or more clearly, or justify one’s stance) (Aeppli & Lötscher, \n2016).\nGiven the complexity and uncertainty of the teaching profession, and the chal-\nlenges to transfer formal knowledge into practical teaching knowledge, teachers \noftentimes develop intuitive responses to classroom events that can result in blind \nroutines (Fenstermacher, 1994; Grossman et al., 2009; Korthagen, 1999; Neuweg, \n2007). Unmitigated exposure to complex and uncertain teaching situations over an \nextended period of time can furthermore result in the development of control strate-\ngies and transmissive views of learning (Hascher, 2005; Korthagen, 2005; Loughran \n& Corrigan, 1995). Reflecting one’s teaching actions and professional development \ncan help to integrate the formal knowledge with more practical teaching knowledge \n(Carlson et al., 2019). However, preservice teachers who have not had opportunities \nto explicitly reflect their teaching actions and learning have no scripts for reflective \nthinking, e.g., how to write a reflection (Buckingham Shum et al., 2017; Loughran \n& Corrigan, 1995). Korthagen (1999) noted that preservice teachers have difficulties \nin adequately understanding a problem that a certain experience exposes. Preservice \nteachers also tend to be rather self-centered, with comparably fewer concerns for the \nstudents’ thinking (Chan et al., 2021; Levin et al., 2009). Expert teachers have devel-\noped much more flexibility in their classroom behavior, which relates to more fully \ndeveloped reflective competencies (Berliner, 2001). Expert teachers are particularly \nable to reflect while teaching and utilize their experiences for self-directed profes-\nsional growth (Berliner, 2001; Korthagen, 1999; Schön, 1983).\nReflective university-based teacher education can help preservice teachers \nto expose their knowledge and integrate it with their practical teaching experi-\nences (Abels, 2011; Darling-Hammond et  al., 2017; Lin et  al., 1999). Enacting \n442 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\nreflection-supporting structures in university-based teacher education requires \ninstructors to create spaces where preservice teachers can make authentic and \nscaffolded teaching experiences (Grossman et  al., 2009). School placements were \nargued to enable preservice teachers to act in authentic classroom situations that \nprovide opportunities for reflection (Grossman et al., 2009; Korthagen, 2005; Zeich-\nner, 2010). Reflection-supporting structures in university-based teacher education \nalso entail providing prompts and reflection-supporting models that help preservice \nteachers to engage in reflective thinking (Lin et al., 1999; Mena-Marcos et al., 2013).\nPrompts and reflection-supporting models in university-based teacher education \nprograms are often implemented in the context of written reflections where expert \nfeedback is provided afterwards. Common ways of eliciting reflective thinking \ninclude assignments such as logbooks (Korthagen, 1999), reflective journals (Bain \net  al., 2002), or dialogical and response journals (Roe and Stallman, 1994; Pater -\nson, 1995), oftentimes in the context of school placements. In these approaches, stu-\ndents start a written conversation in the form of a student-centered communication \nin which significant experiences of the learner are explored (Paterson, 1995). Writ-\ning offers several advantages as a way for teachers to reflect. Teachers are allowed \nto think carefully about what they write, have the possibility to rethink and rework \nideas, and establish a clear position statement that can be discussed with others (Pol-\ndner et al., 2014).\nHowever, despite widespread implementation of reflective writing, little quantify-\nable knowledge was gathered on the genre of written reflections. What is the pro-\ntotypical composition of a written reflection? Which aspects of reflective thinking \nare most prevalent within a large collection of written reflections? We argue that \nanswering these questions requires a principled and systematic way of analyzing \nwritten reflections (Buckingham Shum et al., 2017). Moreover, instructors’ feedback \nfor written reflections is often focused on quality rather than addressing the specific \ncontents of the reflection (Poldner et al., 2014). These problems can be partly attrib-\nuted to the large amount of written reflections that require feedback and the lack of \nconceptual precision about the content of the reflection (Aeppli & Lötscher, 2016; \nRodgers, 2002).\nMethods in the context of artificial intelligence research such as natural language \nprocessing (NLP) and machine learning (ML) can provide principled and system-\natic ways for analyzing preservice teachers’ written reflections (Ullmann, 2019), \nbecause, amongst others, they incentivize researchers to scrutinize the assump-\ntions and models informing the generation of written reflections (Breiman, 2001; \nKovanović et al., 2018; Ullmann, 2019).\nAutomated Analysis of Reflective Writing\nNLP and ML methods have been used to analyze reflective writing in various \ndomains such as engineering, business, health, or teacher education (Buckingham \nShum et al., 2017; Luo & Litman, 2015; Ullmann, 2019; Wulff et al., 2020). Com-\nmonly, the analyses include a reflection-supporting model utilized as a framework \n(Ullmann, 2019) to define categories that are to be identified through ML algorithms. \n443\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \nAs such, the raw texts are commonly preprocessed with NLP methods such as: (1) \nremoving redundant and uninformative words (e.g., stopwords), (2) identifying the \nlinguistic role of words (part-of-speech annotation) (Gibson et al., 2016; Ullmann \net al., 2012), or (3) reducing words to their (morphological) base forms (stemming, \nlemmatization) (Ullmann et  al., 2012). Besides these general preprocessing tech-\nniques, more reflection-specific extraction methods have been devised as well. For \nexample, Ullmann et al. (2012) identified self-references such as personal pronouns \nor reflective verbs (e.g., “rethink”, “mull over”) to annotate segments of reflective \ntexts based on pre-defined dictionaries.\nThe preprocessed texts were then forwarded into ML models or if-then-rules in \norder to classify segments (typically sentences) of reflective writing. To perform \nclassification, supervised ML methods were then applied (Carpenter et  al., 2020; \nWulff et al., 2020; Ullmann, 2019). In this context, Buckingham Shum et al. (2017) \nused hand-crafted rules to distinguish between reflective and unreflective sentences. \nThey used overall 30 annotated texts (382 sentences) to train their system. The best \nperformance for this system was a Cohen’s kappa of .43 (as calculated in: Ullmann \n(2019)). Gibson et al. (2016) used 6,090 student reflections and categorized them \ninto weak or strong metacognitive activity (related to reflection). They used part-\nof-speech tagging and dictionary-based methods to represent the texts as features. \nAs calculated by Ullmann (2019), their best performing model received a Cohen’s \nkappa value of .48. Furthermore, Ullmann (2019) used the distinction between \nreflective depth (reflective text versus descriptive text) and reflective breadth (eight \ncategories such as awareness of a problem, future intentions). He used shallow learn-\ning techniques in ML to classify sentences for reflective breadth. Cohen’s kappa \nvalues for the individual categories were ranging from .53 to .85. Finally, Cheng \n(2017) used a latent semantic analysis approach to classify reflective entries in an \ne-portfolio system using the A-S-E-R model (A: analysis, reformulation and future \napplication; S: strategy application - analyze the effectiveness of language learning \nstrategy; E: external influences; R: Report of event or experience). Cohen’s kappa \nvalues for classification performance ranged from .60 to .73.\nA potential advancement in this research was presented by Carpenter et  al. \n(2020). They used word embeddings to represent students’ reflections. Word embed-\ndings are high-dimensional representations of words in vector space that eventually \nencapsulate semantic and syntactic relations between words and resolve issues such \nas synonymy or polysemy (Taher Pilehvar and Camacho-Collados, 2020). Carpen-\nter et al. (2020) used the reflection model by Ullmann (2017) to annotate students’ \nresponses in a game-based science environment, where students played microbiolo-\ngists who had to diagnose the outbreak of a disease. First, the authors ascertained \nthat reflective depth of students’ responses was predictive for their post-test scores. \nThey further showed that using word embeddings (ELMo) with ML algorithms \n(support vector machine) was more performant compared to models that only used \ncount-based representations of input features.\nMost applications for classifying segments of written reflection according to \nreflection-supporting models have utilized rather shallow learning models such \nas logistic regression or naïve Bayes classification. However, modeling language \ndata has intricacies that cannot be captured with these models such as long-range \n444 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\ndependencies. Deep learning architectures have been found to be able to model these \nintricacies. Furthermore, deep learning architectures have the advantage to automat-\nically find efficient representations for textual data, thus exempting researchers of \ntasks such as lemmatization or stopwords removal (Goodfellow et al., 2016).\nDeep Learning Models for Language Modeling\nModeling textual data has generally become more performant with the advent and \napplication of deep neural network architectures (Goldberg, 2017; LeCun et  al., \n2015). Deep neural network architectures mitigated shortcomings with formerly \nemployed bag-of-words language models. Bag-of-words models make the simpli-\nfying assumption that the order in which words appear in a segment is irrelevant \n(Jurafsky and Martin, 2014). Then, modeling the context dependence through word \nembeddings and the sequencing of words through dynamic embeddings was a major \nfacilitator for performance boosts in language modeling (Taher Pilehvar & Cama-\ncho-Collados, 2020).\nOn the downside, increasingly sophisticated deep neural network architectures \nrequired also more training data, because the amount of parameters in the models \nincreased substantially.1 To cope with excessive requirements, methods of transfer \nlearning were developed where researchers make use of previously trained large \ndeep learning models that were trained on massive language corpora such as the \ndump of the Internet or Wikipedia. In analogy to computer vision, where image \nclassification can be boosted through pretrained model weights rather than random \ninitializations of the model weights (Pratt & Thrun, 1997), pretrained model weights \nin language models can form a solid backbone that provides structure for further \ndownstream tasks (Devlin et al., 2018).\nBased on these encouraging findings in ML and NLP research, we postulate that \nthe application of pretrained language models can also advance performance for \nsegment classification in preservice teachers’ written reflections. However, we are \nnot aware of studies that have used deep learning architectures in classification of \nreflective writing. Carpenter et al. (2020) noticed: “Another direction for future work \n[in reflective writing analytics] is to investigate alternative machine learning tech-\nniques for modeling the depth of student reflections, including deep neural architec-\ntures (e.g., recurrent neural networks)” (p. 76).\nIn line with this suggestion, we employed a pretrained language model named \nbidirectional encoder representations for transformers (BERT) that has been found \nto excel in many language-related tasks not specific to written reflections (Devlin \net al., 2018). The following overarching research question guided the present study: \nTo what extent can a pretrained language model (BERT) be utilized in the context of \npreservice teachers’ written reflections in order to classify segments according to the \n1 Note that the famous GPT-3 generative language model that was advanced by OpenAI encompasses \n175 billion parameters (Brown et al., 2020)\n445\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \nelements of a given reflection-supporting model? More specifically, the following \nresearch questions will be answered:\n1. To what extent does a fine-tuned pretrained language model (BERT) outperform \nother deep learning architectures in classification performance of segments in \npreservice teachers’ written reflections according to the elements of a reflection-\nsupporting model?\n2. To what extent is the size of the training data related to the classification perfor -\nmance of the deep learning language models?\n3. What features can best explain the classifiers’ decisions?\nMethod\nPreservice Physics Teachers’ Written Reflections\nIn order to instruct preservice teachers to write a reflection on their teaching experi-\nences, a reflection-supporting model was devised based on an existing reflection-\nsupporting model (Korthagen and Kessels, 1999). Reflection-supporting models \nas scaffolds for written reflections commonly differentiate between several func-\ntional zones (Swales, 1990) of writing that should be addressed to elicit appropriate \nreflection-related thinking processes (Aeppli & Lötscher, 2016; Bain et  al., 1999; \nKorthagen & Kessels, 1999; Poldner et  al., 2014; Ullmann, 2019). According to \nmany models, reflection-related thinking processes start with a recapitulation and \ndescription of the teaching situation. This allows preservice teachers to establish \nevidence on a problem that they noticed (Hatton & Smith, 1995; van Es & Sherin, \n2002). Another important category is the evaluation and analysis of the teaching sit-\nuation (Korthagen & Kessels, 1999). Preservice teachers judge the students’ actions \nand their own actions. Finally, preservice teachers are instructed to devise alterna-\ntives for their actions and conceive consequences for their own professional develop-\nment. Alternatives and consequences are important aspects of a critical reflection \n(Hatton & Smith, 1995; Korthagen & Kessels, 1999), because–in comparison to \nmere analysis–reflection aims at transforming the individual teacher’s experience in \na helpful way for her or his future professional development.\nIn the present study, a reflection-supporting model was used that was developed \nin prior studies (Nowak et al., 2019). In this model, essentials of reflective thinking \nas outlined above are captured in elements that are constitutive of reflective writing, \nas based on experiential learning theory. The model is based on the ALACT-model \nby Korthagen and Kessels (1999). In a nutshell, the model by Korthagen and Kessels \n(1999) outlines a cyclical process for reflection that entails performing the teach-\ning, looking back on essential aspects of the experience, becoming aware of such \naspects, creating alternative methods for action, and trying them out. Nowak et al. \n(2019) adapted this model in the context of written reflections in a school placement \nfor university-based teacher education (see also: Wulff et al., 2020). The model by \nNowak et al. (2019) defines a reflection to include the elements: circumstances of \n446 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\nthe teaching situation, description of the teaching situation, evaluation of students’ \nand teacher actions, devising of alternatives, and derivation of consequences.\nThis model was employed in the present study. Preservice teachers were instructed \nto produce a written reflection on a self-taught lesson in their final teaching place-\nment or on a lesson observed in a video-vignette. The teachers were instructed to 1) \ndescribe the circumstances of their/the lesson, followed by 2) a detailed description \nof events to be reflected on. Then they should 3) evaluate the described event(s) \nand 4) anticipate alternative actions. Finally, they were instructed to 5) outline per -\nsonal consequences for their professional development based on their evaluations. \nThis instruction gave us the opportunity to present teachers a scaffold for reflection \nto eventually compensate the often criticized lack of expectations for what reflection \nactually entails (Buckingham Shum et al., 2017). The simplicity of the model makes \nit accessible to teachers at various degrees of expertise: all are given a framework to \nstructure their writing.\nAll written reflections were collected from preservice physics teachers at two \nmid-sized German universities. Restricting the study to physics teachers was con-\nsidered useful, because teaching expertise is considered domain-specific (Berliner, \n2001). Physics can be viewed as a knowledge-rich domain (Schoenfeld, 2014). Cur-\nrent classroom instruction in physics tends to be dominated by teacher-centered ped-\nagogy, rather than more constructivist, student-centered learning approaches (Fis-\ncher et al., 2010). It can be expected that the development of attention to students’ \nideas and noticing physics-specific learning problems through experiental learning \nas facilitated through reflection can help physics teachers to implement more stu-\ndent-centered instruction.\nWritten self-reflections were collected over the course of three years. Overall, \nN=270 reflections were authored by 92 preservice physics teachers in their final \nteaching placement of their university-based teacher education program. The pre-\nservice teachers were new to the reflection-supporting model. Consequently the \ninstruction was explicit on what was expected of the teachers (see below). The \nteaching placement lasted approximately 15 weeks. Preservice physics teachers who \nreflected on their own lessons reflected on average 8.9, 4.5, and 3.8 times throughout \nthe three consecutive teaching placements where written reflections were collected. \nWe noticed early in the study that 8.9 written reflections were too much for preser -\nvice teachers in a teaching placement lasting only 15 weeks. Consequently, preser -\nvice physics teachers were instructed to write only about six written reflections in \nthe following teaching placements. The overall number of reflections that a teacher \nhad to write was eventually one reason why some reflections were very short. How -\never, we did not expect overall quantity of words to pose problems for the classifica-\ntion task at hand. Even though multiple reflections by the same preservice teachers \nraise issues of intra-individual dependence in the written reflections, we considered \nthe increased size of training data as more relevant given the purpose of our study. \nTeachers participated based on two possible arrangements. In the first, they could \nwrite reflections on their own teaching. In the second, they could reflect on others’ \nteaching as observed in a video vignette. Instruction for the reflection-supporting \nmodel was provided in written form for the preservice teachers who participated the \nvideo vignette arrangement. The instruction for the preservice teachers in the school \n447\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \nplacement was in verbal and written form in the accompanying seminar. In the video \nvignette arrangement, all preservice physics teachers reflected only once on the \nsame 16-minute video clip. The average length (in words) of all written reflections \nwas 667 (SD= 433) words. The minimum length was 72 words and the maximum \nlength 2,808 words. The vocabulary consisted of 12,518 unique words.\nThe goal of this study was to train and evaluate a model that classifies segments \nof preservice physics teachers’ written reflections according to the elements of the \nreflection-supporting model, namely 1) circumstances, 2) description, 3) evalua-\ntion, 4) alternatives, and 5) consequences. A segment as the elementary coding unit \nwas defined as a passage of text that expressed a single idea (e.g., description of \nan action). In the context of written reflection analytics, smaller coding units such \nas sentences were found to be more suitable for classification purposes of written \nreflections as compared to larger units such as whole texts (Ullmann, 2019). Seg-\nment lengths were on average 1.6 (SD=1.2) sentences. Minimum segment length \nwas 1 and maximum segment length was 18 sentences. A sample coded text seg-\nment looked as follows: \n[The purpose of the experiment was to observe different forces and determine \ntheir points of contact.][Code: Circumstances] ... [The experimental situation \nbegan with unfavourable conditions, as due to a shift of the lesson to another \ntime slot, the experiments had to be carried out in a room that was not a Phys-\nics room. However, since they were mechanics experiments, it was still pos-\nsible to carry them out. However, the room in which the lesson took place is \nmuch smaller than the room that was originally planned. Therefore, the experi-\nmental situation was somewhat cramped.][Code: Circumstances] ... [After the \nassignment was announced, some students remained seated because not all \nstudents could experiment at the same time.][Code: Description] ... [One of \nthe experiments involved a trolley that was connected to a weight by a thread \nand a pulley. This experiment was partially modified by the students, contrary \nto the instructions. However, as these experiments were still in line with the \naim of the experiment, no intervention was made. ][Code: Description] ... [The \nexperiment was well received by the students.][Code: Description] ... [The \nstudents worked very purposefully, this went better than expected.][Code: \nEvaluation] ... [Alternatively, the students’ learning time could have been used \nmore effectively by carrying out the experiments as demo experiments. How -\never, this would have eliminated one of the learning objectives of the lesson, \nwhich was in the area of knowledge acquisition.][Code: Alternatives] ... [As \na consequence for me as a teacher, it is noticeable that the planning must be \nmore detailed. Furthermore, reserves must be created in case an experiment \nhas to be cancelled due to a room change or something similar.][Code: Conse-\nquences] ...\nA rater was trained to manually label the written reflections according to the ele-\nments in the reflection-supporting model (elements 1 to 5). The rater first segmented \nthe written reflections. The minimal segmentation unit was mainly on the sentence-\nlevel, with some exception, e.g., when sentences were not meaningful on their own \nor when they were strongly connected. In prior studies, we found that loosening the \n448 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\nsentence-boundary constraint yielded noticeably better interrater agreement (Wulff  \net  al., 2020). Then the rater labeled the segments with one of the five elements \naccording to the definitions of the elements outlined above. Interrater-agreement \nwas ascertained through recoding a subset of the texts (overall N= 8 written reflec-\ntions) by an independent second rater who used the segments from the first rater. \nGiven the comparably simple classification problem and the promising analyses of \ninterrater agreement for this classification problem in prior studies (Nowak et  al., \n2018), determining interrater agreement on such a small subset was considered suf-\nficient in this context. Cohen’s kappa values were above .73 for all elements (Wulff  \net al., 2020). Given the substantial agreement, the first rater labeled the remaining \ntexts. With the final ratings of the texts the proportions of the texts of the elements \nwere calculated to be: circumstances (26%), description (36%), evaluation (23%), \nalternatives (8%), consequences (7%).\nDeep Learning Models\nThe goal for this study was to utilize a transformer-based pretrained language model \n(BERT) to classify segments in preservice physics teachers’ written reflections and \ncompare the performance of the pretrained language model with the performance of \nother deep learning architectures. Before describing the pretrained language model \nin detail, we will elaborate on the other deep learning architectures that were con-\nsidered, and outline some of their potential strengths and weaknesses with regards \nto applications for language analytics. These alternative architectures were feed-\nforward neural networks (FFNN) and long-short-term-memory neural nets (LSTM), \nwhich are widely employed architectures in NLP research (Goldberg, 2017) and \nhave particular advantages in classifying segments of written reflections.\nFFNNs are amongst the simpler deep learning architectures. In their easiest form, \nthey consist of an input layer, a hidden layer, and an output layer which are fully \nconnected. The introduction of the hidden layer makes FFNNs more encompassing \nthan models such as logistic regression. In fact, FFNNs with a single hidden layer \nand non-linear activation functions can approximate any function conceivable (i.e., \nthey are general function approximators) (Goodfellow et  al., 2016; Jurafsky and \nMartin, 2014). One shortcoming of simple FFNNs is that information flows only \nfrom lower layers to the next higher layer, with no information flowing back or side-\nways in one layer. We expect FFNNs to be capable of classifying segments in the \nwritten reflections, because they are similar to more shallow ML algorithms (Wulff  \net  al., 2020). However, they should not be regarded as the most performant deep \nlearning architecture due to their strong assumptions about the information flow. \nFurthermore, ordering of input is not accounted for in FFNNs. Important hyperpa-\nrameters in FFNNs include the size of the hidden layer. In this study, we only con-\nsidered width, not depth of the hidden layer. In addition, inputs in FFNNs are often \nrepresented through embedding vectors. The dimensionality of the embedding vec-\ntors for the inputs can be varied.\nLSTMs overcome the assumption that input order is irrelevant. LSTMs are \nbased on recurrent neural networds (RNNs). RNNs excel at capturing patterns in \n449\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \nsequential inputs, because they allow the output for a previous input to flow into the \nnext prediction (Goldberg, 2017). RNNs can provide a fixed-size representation of a \nsequence of any length, which can be beneficial in tasks where, say, word order mat-\nters (Goldberg, 2017). RNNs can make predictions for the next word dependent on \nall the previous input words. As such, they can be used to encode input into a mean-\ningful representation. A shortcoming in RNNs was identified to be the vanishing \nimportance of long-distance relations (Jurafsky & Martin, 2014). Therefore, RNNs \nare often employed in complex, gated architectures such as LSTMs which encode \nbetter statistical regularities and long-distance relationships in the input sequence \n(Goldberg, 2017). In tasks such as segment classification, the resulting representa-\ntions from the LSTM are further fed into a FFNN that maps the representation to a \nlabel or category (Goldberg, 2017; Jurafsky & Martin, 2014). We expect LSTMs to \nreach better performance for classifying segments of the written reflections because \nwe conjecture that word order in the input sequence is an important additional fea-\nture compared to mere word occurence. Hyperparameters for tuning the LSTM \ninclude hidden dimensionality and input embedding dimensionality, among others. \nWe used bidirectional embeddings for the LSTM model.\nTransformer‑based Pretrained Language Model: BERT\nMore recently, transformer models have been introduced to processing natural lan-\nguage and were found to outperform LSTMs (Devlin et  al., 2018; Vaswani et  al., \n2017). At the heart of transformer models is a so-called attention mechanism which \ncomprises query, key, and value for each input. Transformers have the advantage \nover RNN architectures to better attend to distant parts of a sentence, without local-\nity bias (Taher Pilehvar & Camacho-Collados, 2020). Researchers demonstrated that \nnatural language can be characterized by long-range dependencies (Ebeling & Nei-\nman, 1995; Zanette, 2014), which renders transformer-architectures promising mod-\nels to solve language-related tasks. A more technical advantage is that the compu-\ntations are parallelizable (Taher Pilehvar & Camacho-Collados, 2020). This means \nthat even on a personal computer the speed required for training the model can be \nincreased by outsourcing computations on the graphical processing unit (GPU) that \nis built for similar tasks. A powerful implementation of a transformer-based lan-\nguage model is called BERT (Devlin et  al., 2018). BERT outperformed previous \nmodels in standardized NLP tasks such as question answering, textual entailment, \nnatural language inference learning, and document classification (Conneau et  al., \n2019; Devlin et al., 2018).\nBesides being a transformer architecture, BERT also makes use of transfer learning. \nThe idea of transfer learning is to train a general language representation from large \nlanguage datasets. In the pretraining phase, BERT uses widely available, unlabeled data \nfrom the Internet (Devlin et al., 2018). The learning protocol behind BERT is based on \nmasking randomly chosen input tokens and try to predict them, similar to the cloze-\ntask (Taylor, 1953). Therefore, the model has information from both sides of a masked \ninput token to predict it. BERT is also trained with a next-sentence-prediction task, \nwhich enables the model to encode relationships between sentences. For this task, each \n450 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\ninput sequence gets an extra encoding in a special token (“[CLS]”) that is added to the \nvocabulary. Next-Sentence-Prediction is mainly beneficial for tasks such as natural lan-\nguage inference and question answering (Taher Pilehvar & Camacho-Collados, 2020). \nOverall, training BERT embeddings involves minimizing a loss function between cor-\nrect words, or correct next sentences and predictions. Conditioned on the training data, \nembeddings are then trained which can be utilized for further tasks such as segment \nclassification (Devlin et al., 2018). The model architecture of BERT base (as compared \nto large) comprises 12 encoder layers with 768 units (Devlin et al., 2018; Vaswani et al., \n2017). The hyperparameters are fixed, once a researcher decides to use a particular pre-\ntrained BERT model.\nAfter pretraining, the BERT model can be used in a fine-tuning phase (see Fig. 1). \nIn the fine-tuning phase the pretrained model weights are adjusted to the given task. \nAs such, there is less adjustment necessary when pretrained model weights are used. \nIn addition to the pretrained model weights, the predicted embedding for a segment \nby the BERT model is fed into another simple classification layer that is added to the \npretrained model on top (Devlin et al., 2018; Gnehm & Clematide, 2020).\nIn the present study, we adopted a pretrained German language model that was \ntrained by deepset AI.2 The model weights by deepset AI showed better performance \nFig. 1  Procedure to train, validate, and test the pre-trained BERT model. The other models (FFNN, \nLSTM) were validated similarly, except that no pretrained weights were used\n2 Details on German BERT training procedure: https:// deeps et. ai/ german- bert (accessed: 18 Dec 2020).\n451\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \ncompared to the multilingual BERT models on shared language tasks (Ostendorff  \net al., 2019). This model has a total vocabulary size of 30,000 that is used to repre-\nsent all words that occur in the new learning context (occasionally unknown tokens \nare mapped to a special token, “[UNK]”) and yield a good performance on tasks \nwhile keeping memory consumption and computation resources feasible for a per -\nsonal computer (Ostendorff et al., 2019).\nCross‑validation and Technical Implementation\nIn this study, we used two cross-validation strategies: (1) simple hold-out cross-vali-\ndation to assess predictive performance of the trained models, and (2) iterated k-fold \ncross-validation to account for the small sample and better assess generalizability. \n(1) In the simple hold-out cross-validation 60% of the data (randomly chosen) was \nused as training data, 20% was used as validation data, and another 20% was used \nas held-out test data. The same splits were used for all trained models. (2) Given the \ncomparably small size of our datasets for deep learning applications, iterated k-fold \ncross-validation was additionally considered because it enables evaluation of deep \nlearning models with comparably small datasets as precisely as possible (Chollet, \n2018). Iterated k-fold cross-validation randomly splits the data into training and vali-\ndation subsets. We therefore first concatenated our original training and validation \ndata (not the held-out test data) and then performed iterated k-fold cross-validation \non this new dataset. In our case, we split the entire dataset ten times. For each split, \nthe model performance was assessed. This procedure was iterated ten times. Results \nwere then averaged to reach a final score. We performed this procedure with the \nmost performant models, i.e., after tuning the hyperparameters.\nAll experiments (especially the time measurements) were performed on an Intel \nCore i9-10900K CPU with 3.70GHz and 32GB RAM. Measurements on GPU \nwere performed on a GeForce RTX 3080, 10GB. Furthermore, we used Python 3.8 \n(Python Software Foundation, 2020). To train and evaluate the deep learning mod-\nels we used the Python library pytorch (Paszke et  al., 2019). In particular, the \nAdam optimizer was used in combination with binary cross-entroy loss, which are \nboth implemented in pytorch. The pretrained German BERT model was accessed \nthrough the huggingface-library (Wolf et al., 2020).\nComparing BERT with other Deep Learning Architectures (RQ1)\nIn order to compare BERT with the other deep learning architectures, we fit the \nconsidered models (FFNN, LSTM, and BERT) to the training data and evaluated \nthe performance on the validation data. Furthermore, we deleted 11 of BERT’s 12 \nencoder layers and fitted a reduced BERT model (called: BERT (1)). This gives hints \nas to how important the number of encoder layers in the BERT architecture was for \nclassification performance in this task. The task was to classify the segments of the \nwritten reflections according to the five elements of the reflection-supporting model. \nAfter finding the most performant model through hyperparameter optimization, \n452 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\nthis most performant model was fit on the held-out test data in order to evaluate \ngeneralizability.\nPerformance evaluation of the models for the classification task include precision \n(p), recall (r), and the F1 score. These performance metrics account for the fact that \na classifier should correctly detect labels in the human-labeled data. This is why pre-\ncision and recall both focus on detecting true positives (Jurafsky & Martin, 2014). \nFor a multiway classifier as in the present case, the precision, recall, and F1 score \nwill be reported for each element and the averaged values. Averages are reported for \nmicro (i.e., global calculations for p/r/F1), macro (i.e., averaged p/r/F1 for each ele-\nment, unweighted by support for each element), and weighted (i.e., averaged p/r/F1 \nfor each element, weighted by support for each element). The performance metrics \nare henceforth abbreviated as p/r/F1. We also calculated the Cohen’s kappa value in \norder to compare the classification performance with a widely used metric in educa-\ntional research.\nTo find the most performant models, we conducted a grid search through a hyper-\nparameter space. For all models we varied epochs (values from 3 to 200), batch sizes \n(values from 3 to 50), learning rates (values from  10− 3 to  10− 5), and hidden sizes \n(from 100 to 10000). Appropriate step sizes for all hyperparameters were chosen. \nEpochs indicate the number of times that the procedure of adjusting model weights \nto the training data is performed. Generally, this is done more than one time. How -\never, at some point the adjustment eventually becomes too restricted to the training \ndata so that the model overfits the training data and classification performance on \ntest data suffers. Batch size indicates the number of training samples that are con-\ncurrently fed into the model to produce output. Smaller sizes allow more updates \nand faster convergence, and larger values provide better estimates of the corpus-\nwide gradients (Goldberg, 2017). The learning rate relates to the optimization algo-\nrithms and needs to be adjusted, because too large or too small values will result \nin non-convergence of the model weights or similar problems. Finally, the size of \nthe hidden layer relates to representational capacity. However, larger values are not \nnecessarily better. Since BERT uses a very specific vocabulary of 30,000 tokens, \nFFNNs and LSTMs were fit based on the original BERT vocabulary (names: FFNN \nand LSTM), and on the original training vocabulary (names:  FFNN* and  LSTM*). \nThe reasoning was that the BERT vocabulary might be a generally advantageous \nrepresentation of language data.\nDependence on Size of Training Data (RQ2)\nA rule-of-thumb in image classification states that approximately 1,000 instances \nof a class (e.g., cat or dog) are needed to achieve acceptable classification perfor -\nmance (Mitchell, 2020). In the context of science education assessment, Ha et al. \n(2011), who used a software for scoring students’ evolution explanations (Mayfield \nand Rose, 2010), found that approximately 500 samples were sufficient to clas-\nsify responses under certain circumstances. However, we are not aware of analy -\nses of this kind for dependence of classification performance on the size of training \ndata in written reflections. In particular, it is not clear, for example, if and when \n453\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \nclassification performance for deep learning architectures will converge given the \ncomparably small data sets that are commonly used in discipline-based educational \nresearch. Hence, we performed sensitivity analyses for classification performance \nfor the present data. To do so, random samples from the training data were drawn \ngiven a predefined proportion ranging from 0.05 to 1.00. The analysis was per -\nformed based on the most performant models from RQ1. Classification performance \nwas assessed on the test dataset.\nInterpreting BERT’s Classification Decisions (RQ3)\nInterpretability of deep learning models is important in research contexts in order \nto advance our understanding of the functioning of written reflections (Rose, 2017). \nSundararajan et al. (2017) proposed integrated gradients to attribute importance to \ninput tokens for classification tasks in a deep learning architecture. Integrated gra-\ndients allow each input feature to get an attribution that accounts for the contribu-\ntion to the prediction. Integrated gradients are calculated in reference to a neutral \nbaseline input, such as a blank image in image recognition or a “meaningless” word \nin NLP applications. Gradients can be considered in analogy to products of model \ncoefficients and feature values for a deep network.\nFor language models, the input tokens that are defined by the vocabulary (more \nexactly, the token embedding vectors) are assigned attributions through integrated \ngradients. The attribution values allow researchers to estimate the importance \nof certain tokens for a classification category. The baseline token is chosen to be \na neutral token that ideally has a neutral prediction score. Important hyperparam-\neters include the length of the input vector, which taxes computer memory. In the \npresent study the longest segment was unduly long, so a more reasonable number \nwas chosen (98th percentile of sentence length) which included almost all segments, \nand memory consumption was reasonable for a personal computer. Another hyper -\nparameter that also largely affects allocation of computer memory is the number of \nsteps that are used to calculate the integrated gradients. 50 steps were chosen in the \npresent case, which seemed a large enough number to prevent incorrect calculations \nyet small enough to be reasonable for a personal computer. The captum -library in \nPython was used to perform these calculations (Sundararajan et al., 2017).\nResults\nContrasting BERT with other Deep Learning Language Models to Classify \nSegments in the Written Reflections (RQ1)\nOur first RQ was: “To what extent does a fine-tuned pretrained language model \n(BERT) outperform other deep learning models in classification of segments in \npreservice teachers’ written reflections according to the elements of a reflection-\nsupporting model?” To answer this RQ, it was analyzed to what extent fine-\ntuned BERT outperformed FFNNs and LSTMs. Table  1 depicts the classification \n454 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3Table 1  Classifier performance for best models as assessed on the validation dataset\nFFN FFN* LSTM LSTM* BERT (1) BERT BERT (GPU)\nElement p r F1 p r F1 p r F1 p r F1 p r F1 p r F1 p r F1\nCircumstances 0.70 0.69 0.69 0.70 0.73 0.71 0.78 0.74 0.76 0.74 0.74 0.74 0.91 0.73 0.81 0.88 0.82 0.85 0.88 0.82 0.85\nDescription 0.72 0.69 0.71 0.74 0.66 0.70 0.78 0.78 0.78 0.74 0.84 0.78 0.79 0.86 0.82 0.87 0.88 0.87 0.87 0.88 0.87\nEvaluation 0.47 0.63 0.54 0.58 0.57 0.57 0.65 0.67 0.66 0.69 0.60 0.64 0.69 0.71 0.70 0.76 0.82 0.79 0.76 0.82 0.79\nAlternatives 0.60 0.29 0.39 0.34 0.61 0.44 0.50 0.72 0.59 0.68 0.65 0.67 0.55 0.78 0.64 0.76 0.74 0.75 0.76 0.74 0.75\nConsequences 0.49 0.36 0.41 0.57 0.33 0.42 0.72 0.47 0.57 0.64 0.51 0.57 0.75 0.56 0.64 0.67 0.60 0.63 0.67 0.60 0.63\nMicro 0.63 0.63 0.63 0.63 0.63 0.63 0.72 0.72 0.72 0.72 0.72 0.72 0.77 0.77 0.77 0.82 0.82 0.82 0.82 0.82 0.82\nMacro 0.60 0.53 0.55 0.59 0.58 0.57 0.69 0.67 0.67 0.70 0.67 0.68 0.74 0.73 0.72 0.79 0.77 0.78 0.79 0.77 0.78\nWeighted 0.64 0.63 0.62 0.65 0.63 0.64 0.73 0.72 0.72 0.72 0.72 0.72 0.78 0.77 0.77 0.83 0.82 0.82 0.83 0.82 0.82\n455\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \nperformance of the best models that resulted from the grid search. It can be seen \nthat FFNN’s performed lowest with weighted F1 averages of .62 and .64, respec-\ntively. FFNNs were followed by LSTM’s with weighted F1 averages of .72 and \n.72, respectively. Furthermore, BERT (1) performed .05 points lower compared to \nBERT in weighted F1 average. Overall, BERT was the most performant model to \nclassify segments in the written reflections according to the reflection-supporting \nmodel, with a weighted F1 average of .82. The Cohen’s kappa values for the clas-\nsification performance were: FFNN: 0.48, FFNN*: 0.48, LSTM: 0.56, LSTM*: \n0.58, BERT (1): 0.66, BERT: 0.75.\nThe most performant models were then evaluated through iterated k-fold cross-\nvalidation to yield more robust estimations for generalizability. Note that results \nin Table  1, given that they were performed based on simple hold-out cross-val-\nidation, are likely inflated. The weighted F1 averages (standard deviations) for \niterated k-fold cross-validation were as follows: FFNN: 0.60 (0.07), FFNN*: 0.57 \n(0.17), LSTM: 0.71 (0.02), LSTM*: 0.71 (0.02), BERT (1): 0.76 (0.02), BERT: \n0.81 (0.02). For the FFNN models, performance decreased when averaged over \nthe folds. It can be seen that the standard deviations are comparably large. The \niterated k-fold cross-validation values for the LSTM models approximated the \nfinding for the simple fold cross validation value. The standard deviation was also \nsmaller compared to the FFNN models. Similarly, the values for the BERT mod-\nels also approximate the simple fold cross validation values with a comparably \nsmall standard deviation as well. Hence, LSTM and BERT models seem to gen-\neralize better compared to FFNN models. Again, BERT outperformed all other \nmodels.\nThe final hyperparameters for the best models in the training phase, the over -\nall number of parameters for these models, and the training time can be seen in \nTable 2. It can be seen that all architectures have millions of parameters. LSTMs \nhad the lowest number of overall parameters. The LSTMs had also reasonable \ntraining times. The full BERT model utilized the largest number of parameters. \nConsequently, saving the model takes the most storage on hard-disk (> 400 MB). \nAs expected, training time for BERT could be reduced by a factor of 54 to a more \nmanageable time of 2 minutes by outsourcing training to the GPU.\nTable 2  Hyperparameters, # of parameters, and training time for the best performing models\nepochs batch size learning rate hidden size embed dim # parameters time\nFFN 10 10 0.001 200 50 14 M 0h 04min\nFFN* 200 5 0.0001 200 300 10 M 2h 44min\nLSTM 10 3 0.001 100 300 9 M 0h 31min\nLSTM* 3 50 0.005 100 300 3 M 0h 08min\nBERT (1) 5 50 0.0001 768 768 31 M 0h 12min\nBERT 3 3 1e-05 768 768 109 M 1h 48min\nBERT (GPU) 3 3 1e-05 768 768 109 M 0h 02min\n456 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\nFinally the best overall model was fit on the held-out test data. We chose the full \nfine-tuned BERT model, because this model was the most performant classification \nmodel. When fit to the test data, BERT reached a weighted F1 average of .81. The \nCohen’s kappa value was .74.\nClassification Performance in Reference to Size of Training Data (RQ2)\nOur RQ2 was “To what extent is the size of the training data related to the clas-\nsification performance of the deep learning language models?” In order to analyze \nthis RQ, we systematically varied the size of the training data. For each proportion \na single random draw from the training data was used. Even though multiple draws \nwould raise the precision of the performance estimation, we expected single draws \nto indicate more global trends in the performance. Figure  2 shows the classifica-\ntion performance (as measured through weighted F1 average) with respect to the \nsize of the training data (depicted as proportion of the whole training data). We also \nfit a BERT model where we randomly changed the order of the input tokens. This \nwas done because the BERT model also encodes word position. This allowed us \nto evaluate to what extent word-order was important for classification performance \nin the given task of classifying segments according to elements in the reflection-\nsupporting model.\nFigure 2 indicates that BERT with non-random word-order performs lower for \nsmaller training data sizes compared to FFNN and LSTM, but outperforms all other \nmodels at about 20 to 30% of the size of the entire training data. BERT with ran-\ndom word-order in fact suffers in classification performance compared to BERT \nwith non-random word-order. BERT with random word-order settles at about the \nFig. 2  Analysis of dependence \nof classification performance on \nsize of training data\n\n457\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \nperformance of the LSTM. It is noticeable that all curves flatten by an increasing \nsize of training data.\nExplaining the Classifier Decisions (RQ3)\nFinally, our RQ3 was “What features can best explain the classifiers’ decisions?” \nHere, only the BERT model is considered. Layer-integrated gradients were calcu-\nlated for all inputs in the test data. Layer-integrated gradients were used to assign \nan attribution score to each word/token in the input according to the contribution \nthis word/token added to the prediction of the segment. Since BERT has multiple \nembedding layers, the attribution scores across all embedding dimensions were \nadded together to get a final attribution score for each word/token. Now, we wanted \nto extract the most important words for the elements in the reflection-supporting \nmodel. Therefore, we chose classified segments where true outcome and predicted \noutcome were the same. For each word we then averaged all attributions.\nTable 3 depicts the most important nouns, verbs and adjectives for each of the \nelements. Note that some tokens are split through hashes (#) according to the BERT \ntokenizer (i.e., wordpiece tokenization). This occurred when only part of a word was \nknown to the tokenizer. It hence split the word into existing tokens in the BERT \nvocabulary. Besides those split tokens, most of the other words are well interpret-\nable given their belonging to the respective element of the reflection-supporting \nmodel. Note, for example, the subjunctive mode of verbs in alternatives. Further -\nmore, there are words of appraisal such as “good” in evaluation. Also the nouns in \ncircumstances are somewhat prototypical of that element given that the preservice \nteachers were meant to describe the “class”, state their “goals”, and characterize the \n“lesson”. Other words are unexpected. For example, “independent” is not necessar -\nily related to circumstances. However, sometimes the preservice teachers noted that \nthis was their first lesson “independent” of their mentors or similar.\nDiscussion\nReflective writing analytics uses ML and NLP to extract contents from products \nof reflective thinking such as written reflections. The presented study advances \nresearch in reflective writing analytics by applying a pretrained language model \n(BERT) to boost classification performance for the identification of elements of a \nreflection-supporting model in the segments of written reflections.\nIn this study, segments in preservice physics teachers’ written reflections were \nidentified by human raters according to a reflection-supporting model (Nowak et al., \n2019). A pretrained language model (BERT) was then trained to classify segments, \nbased on human annotations. Training refers to the adjustment of the model weights \nin the pretrained language model to reproduce input-output mappings. The qual-\nity of this mapping is calculated based on a loss function and model weights are \nadjusted through an optimization algorithm. The classification performance of the \npretrained language model on the validation data was then compared to widely used \n458 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3Table 3  Words with highest attribution in each category\n Upper cased letters refer to sentence beginnings, and hashes are used by the BERT tokenizer to split unknown tokens into known tokens\nElement Nouns Auxillaries Verbs Adjectives\nCircumstances goal, class, ##goal, lesson, \n##attemp(t)\nhad, have, were, was, will decided, wanted, determined, \nintended, decided\ngood, tea(ch), physic(al), independent, \ntheoretical\nDescription asked, endeavoured, minutes, ##i(t), \nhad to\nhas been, have been, had, have, will said, came, noticed, put, should repeated, Subsequently, Together, \ndifferent, next\nEvaluation expectation, talk(ed), judge, view, \npleas(ant)\nwas, were, am, had, is expected, perc(eive), found, can, \nhelped\ngood, successful, justifiable, unsure, \nGood\nAlternatives alternative, alternatives, would have \nto, example, need\nwould be, would have, would, would \nbe, would\ncould, could, can, should, secure Alternatively, Better, better, possible, \nmore\nConsequences consequence, feeling, ##me(ans), \ntake, next\nwill, had, am, have, is should, should, see, could, can important, Important, better, personal, \ngood\n459\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \ndeep learning algorithms (FFNN, LSTM). The full BERT model yielded the best \nperformance (weighted F1 average of .82). Afterwards, BERT was fit to the held-\nout test data and achieved a weighted F1 average of .81, which relates to a Cohen’s \nkappa value of .74. This finding is in line with recent research in NLP that indi-\ncates that transformer-based language models such as BERT can boost classifica-\ntion performance in a multitude of language-related tasks and reliably generalize to \nunseen data (Devlin et al., 2018; Taher Pilehvar and Camacho-Collados, 2020). We \nfurthermore showed that the advantage of the full BERT model manifests at about \n20 to 30% of the size of the training data. This resonates with the observation that \nutilizing pretrained language models reduces the need for training data to achieve \ngood classification performance (Devlin et  al., 2018). These findings suggest that \npretrained language models such as BERT might be applicable to a range of tasks \nin reflective writing analytics and beyond where only modest amounts of data are \navailable. Furthermore, the word-order in the segments appears to be relevant for the \nBERT model to outperform LSTMs in classification performance. This might be due \nto the fact that pretraining in BERT involves masking words in the inputs and pre-\ndicting the masked words from the context. Also, BERT uses positional (contextual) \nembeddings for the input tokens that might raise the importance of word order in \nthe input sequence (Taher Pilehvar & Camacho-Collados, 2020). Finally, we sought \nto interpret the classification decision of the full BERT model. Consequently, layer-\nintegrated gradients were used to calculate attributions for the inputs that indicated \nto what extent a token in the input contributed to the classification. The retrieved \nwords with high attributions were well interpretable in terms of the respective ele-\nments in the reflection-supporting model. The integrated gradients seem to provide \na valuable tool to open the black box of deep learning models and better understand \nmodel decisions. For example, mostly positive words in the element of evaluation \ncould indicate that preservice teachers tend to appraise their own teaching, at the \nrisk of reifying their own prior beliefs, which has been reported in prior research \n(Mena-Marcos et al., 2013). As such, integrated gradients could provide researchers \nanalytical tools for written reflections.\nLimitations and Improvements\nSeveral implementation details of the model training and data analysis limit the \ngeneralizability of our findings. For example, we chose specific implementations of \ndata representation, optimization procedure, and loss function. These choices were \npartly based on prior research (Devlin et al., 2018; Wulff et al., 2020). However, we \ncannot exclude the possibility that alternative implementations of data representa-\ntion (e.g., stemming, or stopwords removal), optimization procedure (e.g., stochastic \ngradient descent), or different loss functions (e.g., mean square loss) would have \nresulted in better classification performance. Furthermore, the fine-tuned BERT \nmodel was used with a fixed-size vocabulary of 30,000. Consequently, some words \nwere artificially split into meaningless tokens (e.g., “experiment” to “ex -periment”). \nYet, the word “experiment” is important in physics-related written reflections. New \npretrained models that incorporate the vocabulary presented in the training dataset \n460 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\ncould be advantageous, because more attention could be given to the actual word \nboundaries and structure of the written reflections. This would require BERT to \ninclude these important words in the vocabulary that it is pretrained on. Advances \nin the huggingface library particularly allow for pretraining transformer-based \nlanguage models.\nAnother limitation relates to the segmentation of the written reflections. The seg-\nmentation was done mainly on the basis of sentences as in previous studies (Car -\npenter et al., 2020; Ferreira et al., 2013; Wulff et al., 2020). A stronger theoretical \nfoundation such as discourse theory might improve the segmentation (Stede, 2016). \nFor example, the theory of discourse segmentation can help to split segments based \non principled decisions rather than relatedness or sentence boundaries. Segmenta-\ntion can have a large impact on classification performance (Rosé et al., 2008). Even \nthough methods such as sentence segmentation yielded acceptable results in past \napplications (Wulff et al., 2020), a more solid grounding of the segmentation would \nallow models to better encode dependencies within the language vis-à-vis the ele-\nments of the reflection-supporting model. However, it is also important that sen-\ntence-based segmentation can easily be automated today with high accuracy, which \nmotivates the further use of sentence-based segmentation because unseen data can \nbe segmented without human intervention. At the same time, sentence segmentation \nassumes that all information relevant to classifying a segment into an element lies \nwithin the sentence. This assumption is certainly not always true, given the reviewed \nresearch of long-range dependencies that characterize language (Ebeling & Neiman, \n1995). One way to potentially resolve this issue is through multiple models that \nattend to different aspects of the written reflections.\nFinally, it is important to recognize the limits of language and human commu-\nnication via texts. Assumptions and certain amounts of world knowledge are rarely \nexplicitly mentioned in texts (Jurafsky & Martin, 2014; McNamara et  al., 1996). \nRather, a certain amount of common world knowledge is assumed on the part of \nthe reader (Jurafsky & Martin, 2014; McNamara et al., 1996). Implicit and unstated \nknowledge poses problems to algorithms that take the input at its face value, because \nonly explicitly mentioned facts are used for classification of segments. Furthermore, \nhuman raters eventually resolve ambiguities or coreferences that are not explicitly \nstated in the texts and cannot be used by the computer algorithm. Consequently, \nhuman-computer agreement is likely hampered in certain contexts, and validation \nmethods that rely on human-computer agreement are biased.\nImplications\nGiven these limitations, our findings have implications for educational research \nrelated to written reflections and to the implementations of educational tools that \nmake use of automated analysis of written reflections. Our findings suggest that \npretrained language models can improve classification performance for segments \nof written reflections. On the basis of improved classification performance, various \napplications for automated analysis are conceivable.\n461\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \nFuture research would further probe the features in the BERT model architecture \nand training regime. For example, Ostendorff et al. (2019) employed metadata for \nclassification tasks with BERT. It could be helpful to include author-related covari-\nates such as course grades, years of teaching experiences, etc., to boost classification \nperformance or further assessment. Also, additional text features of coded segments \ncould be considered for classification. For example, spacial position of the seg-\nment in the written reflection and text length are two meaningful choices for further \ncovariates that eventually increase classification performance. In addition, genera-\ntive language models such as GPT-3 have been found to be capable of learning new \ntasks with only few examples (Brown et al., 2020). These capabilities could be uti-\nlized in the context of written reflection analytics. For example, generative language \nmodels could be used to generate consequences given an evaluation of a teaching \nsituation. Certainly, a consequence should attend to the description and evaluation \nof the teaching situation. Advancing this line of research would eventually enable \nthe modeling of complex processes for generating written reflections. The capacity \nof modeling complex processes was attributed to be a major benefit of algorithmic \ndata approaches such as ML as compared to more traditional data models (Breiman, \n2001).\nAn important caveat in the context of reflective writing analytics is the missing \nlink between quality reflection and improved classroom performance (Clarà, 2015). \nIn terms of pedagogical value of written reflections, external validity of the reflec-\ntion-supporting model needs to be investigated. We propose that the trained mod-\nels in this study can help evaluate the effectiveness of written reflections in teacher \neducation programs. With the help of the trained models (BERT in particular), pre-\nservice physics teachers’ reflections can be scalably analyzed with regards to com-\npleteness, frequency, and structuredness of the elements of the reflection-supporting \nmodel. On the basis of a large-scale implementation, relationships between reflec-\ntive writing and variables such as students’ knowledge gains or similar metrics can \nbe quantitatively explored on a broader scale.\nFinally, the presented methods of deep learning can help to build reliable and \nanalytical feedback tools that provide instantaneous feedback for a written reflection. \nGiven the requirements for preservice teachers to learn from their teaching expe-\nriences (Korthagen, 1999), as well as the limitations and cost of human resources \nin university-based teacher education (e.g., Nehm and Härtig2012), such systems \nwould be in great need, especially given their effectiveness for improving learning \noutcomes (Aleven et  al., 2016; Chirikov et  al., 2020; VanLehn, 2011). Pretrained \nlanguage models can play an important role in the development of such feedback \ntools (Chirikov et al., 2020). BERT could be used to automatically classify segments \nin preservice physics teachers’ written reflections and report descriptive statistics to \npreservice teachers and instructors. Descriptive statistics could relate to questions \nsuch as if all elements in the reflection-supporting model were addressed in the text \nand to what extent. Self-reflection is also an important element in intelligent tutoring \nsystems such as AutoTutor or Crystal Island (Carpenter et al., 2020; Graesser et al., \n2005) that use natural language as a means to facilitate learning (Nye et al., 2014). \nCarpenter et al. (2020) successfully used deep contextualized word representations \n(ELMo) to represent and predict students’ reflective depth. The reflection-supporting \n462 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\nmodel and the trained BERT model in our study could be used in addition to these \nfindings to also conceptualize and predict the reflective breadth of students’ writing \nin systems such as Crystal Island.\nTransfer of intelligent tutoring systems to the context of preservice teacher educa-\ntion would hold great potentials to improve experiential learning. Reflective depth \nand breadth could be assessed with pretrained language models (ELMo, BERT). \nPrompts and hints could be provided to preservice teachers that were found to be \neffective in prior studies (Lai and Calandra, 2010). They would relate to teach-\ning experiences by the teachers and provide analytical feedback. For example, it \nis known in reflective writing analytics that teachers tend to be evaluative in their \ndescription of a teaching situation (Mann et  al., 2007). A computer-based tutor -\ning system that is based on the trained BERT model (or the ELMo representations) \ncould identify evaluative sentences within a descriptive paragraph and provide hints \nfor rewriting.\nFunding Open Access funding enabled and organized by Projekt DEAL. This project is part of the \n“Qualitätsoffensive Lehrerbildung,” a joint initiative of the Federal Government and the Länder which \naims to improve the quality of teacher training. The program is funded by the Federal Ministry of Educa-\ntion and Research. The authors are responsible for the content of this publication.\nCode Availability Please contact the first author.\nDeclarations \nConflict of Interests No conflicts of interest.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is \nnot permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen \nses/ by/4. 0/.\nReferences\nAbels, S. (2011). LehrerInnen als ‘Reflective Practitioner’: Reflexionskompetenz für einen demokra-\ntieförderlichen Naturwissenschaftsunterricht, (1. Aufl. ed.). Wiesbaden: VS Verl. für Sozialwiss.\nAeppli, J., & Lötscher, HL (2016). EDAMA - Ein Rahmenmodell für Reflexion. Beiträge zur Lehrerin-\nnen- und Lehrerbildung, 34(1), 78–97.\nAleven, V., McLaughlin, E.A., Glenn, R.A., & Koedinger, K.R. (2016). Instruction based on adaptive \nlearning technologies. In RE. Mayer P.A. Alexander (Eds.) Handbook of Research on Learning and \nInstruction, Educational Psychology Handbook (pp. 522–560). Taylor and Francis, Florence.\nBain, J.D., Ballantyne, R., Packer, J., & Mills, C. (1999). Using journal writing to enhance student teach-\ners’ reflectivity during field experience placements. Teachers and Teaching, 5(1), 51–73.\nBain, J.D., Mills, C., Ballantyne, R., & Packer, J. (2002). Developing reflection on practice through jour -\nnal writing: Impacts of variations in the focus and level of feedback. Teachers and Teaching, 8(2), \n171–196.\n463\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \nBerliner, D.C. (2001). Learning about and learning from expert teachers. International Journal of Educa-\ntional Research, 35, 463–482.\nBreiman, L. (2001). Statistical modeling: The two cultures. Statistical Science, 16(3), 199–231.\nBrown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ..., & Amodei, D. (2020). Lan-\nguage models are few-shot learners. arXiv.\nBuckingham Shum, S., Sándor, Á., Goldsmith, R., Bass, R., & McWilliams, M. (2017). Towards reflec-\ntive writing analytics: Rationale, methodology and preliminary results. Journal of Learning Analyt-\nics, 4(1), 58–84.\nCarlson, J., Daehler, K., Alonzo, A., Barendsen, E., Berry, A., Borowski, A., ..., & Wilson, C.D. (2019). \nThe refined consensus model of pedagogical content knowledge. In A. Hume, R. Cooper, & A. \nBorowski (Eds.) Repositioning pedagogical content knowledge in teachers’ professional knowledge. \nSingapore: Springer.\nCarpenter, D., Geden, M., Rowe, J., Azevedo, R., & Lester, J. (2020). Automated analysis of middle \nschool students’ written reflections during game-based learning. In I.I. Bittencourt, M. Cukurova, \nK. Muldner, R. Luckin, & E. Millán (Eds.) Artificial intelligence in education (pp. 67–78). Cham: \nSpringer International Publishing.\nChan, K.K.H., Xu, L., Cooper, R., Berry, A., & van Driel, J.H. (2021). Teacher noticing in science educa-\ntion: do you see what I see? Studies in Science Education, 57(1), 1–44.\nCheng, G. (2017). Towards an automatic classification system for supporting the development of critical \nreflective skills in L2 learning. Australasian Journal of Educational Technology, 33(4), 1–21.\nChirikov, I., Semenova, T., Maloshonok, N., Bettinger, E., & Kililcec, R.F. (2020). Online education \nplatforms scale college STEM instruction with equivalent learning outcomes at lower cost. Science \nAdvances, 6.\nChollet, F. (2018). Deep learning with Python. Shelter Island, NY: Manning. Retrieved from http:// proqu \nest. safar ibook sonli ne. com/ 97816 17294 433.\nClarà, M. (2015). What is reflection? Looking for clarity in an ambiguous notion. Journal of Teacher \nEducation, 66(3), 261–271.\nClarke, D., & Hollingsworth, H. (2002). Elaborating a model of teacher professional growth. Teaching \nand Teacher Education, 18(8), 947–967.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., ..., & Stoyanov, V. \n(2019). Unsupervised cross-lingual representation learning at scale. arXiv:1911. 02116.\nDarling-Hammond, L., Hammerness, K., Grossman, P.L., Rust, F., & Shulman, L.S. (2017). The design \nof teacher education programs. In L Darling-Hammond J. Bransford (Eds.) Preparing teachers for a \nchanging world. New York: John Wiley & Sons.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional \ntransformers for language understanding. arXiv:1810. 04805.\nDewey, J. (1933). How we think: A restatement of the relation of reflective thinking to the educative pro-\ncess ((New ed.) Ed.). Boston usw.: Heath.\nEbeling, W., & Neiman, A. (1995). Long-range correlations between letters and sentences in texts. Phys -\nica A: Statistical Mechanics and its Applications, 215(3), 233–241.\nFenstermacher, G. (1994). Chapter 1: The Knower and the Known: The Nature of Knowledge in Research \non Teaching. Review of Research in Education 20.\nFerreira, R., de Souza Cabral, L., Lins, R.D., Pereira e Silva, G., Freitas, F., Cavalcanti, G.D., ..., & \nFavaro, L. (2013). Assessing sentence scoring techniques for extractive text summarization. Expert \nSystems with Applications, 40 (14), 5755–5764.\nFischer, H.E., Borowski, A., Kauertz, A., & Neumann, K. (2010). Fachdidaktische Unterrichtsforschung: \nUnterrichtsmodelle und die Analyse von Physikunterricht. Zeitschrift für Didaktik der Naturwis-\nsenschaften, 16, 59–75.\nGibson, A., Kitto, K., & Bruza, P. (2016). Towards the discovery of learner metacognition from reflective \nwriting. Journal of Learning Analytics, 3 (2), 22–36.\nGnehm, A.-S., & Clematide, S. (2020). Text zoning and classification for job advertisements in Ger -\nman, French and English: Proceedings of the Fourth Workshop on Natural Language Processing \nand Computational Social Science. ACL. Retrieved from https:// www. aclweb. org/ antho logy/ 2020. \nnlpcss- 1. 10. pdf.\nGoldberg, Y. (2017). Neural network methods for natural language processing. Morgan and Claypool.\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. Cambridge, Massachusetts and Lon-\ndon, England: MIT Press. http:// www. deepl earni ngbook. org/.\n464 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\nGraesser, A.C., Chipman, P., Haynes, B.C., & Olney, A. (2005). AutoTutor: An intelligent tutoring sys-\ntem with mixed-initiative dialogue. IEEE Transactions on Education, 48(4), 612–618.\nGrossman, P.L., Compton, C., Igra, D., Ronfeldt, M., Shahan, E., & Williamson, P.W. (2009). Teaching \npractice: A cross-professional perspective. Teachers College Record, 111(9), 2055–2100.\nHa, M., Nehm, R.H., Urban-Lurain, M., & Merrill, J.E. (2011). Applying computerized-scoring models \nof written biological explanations across courses and colleges: prospects and limitations. CBE Life \nSciences Education, 10(4), 379–393.\nHascher, T. (2005). Die Erfahrungsfalle. Journal für Lehrerinnen- und Lehrerbildung, 5(1), 39–45.\nHatton, N., & Smith, D. (1995). Reflection in teacher education: Towards definition and implementation. \nTeaching and Teacher Education, 11(1), 33–49.\nJurafsky, D., & Martin, J.H. (2014). Speech and language processing (2 ed., Pearson new internat. ed. \ned.). Harlow: Pearson Education.\nKahneman, D. (2012). Schnelles Denken, langsames Denken. Siedler Verlag.\nKolb, D. (1984). Experiential learning: Experience as the source of learning and development. Englewood \nCliffs, NJ: Prentice Hall.\nKorthagen, F.A. (1999). Linking reflection and technical competence: The logbook as an instrument in \nteacher education. European Journal of Teacher Education, 22(2-3), 191–207.\nKorthagen, F.A. (2001). Linking practice and theory: The pedagogy of realistic teacher education. Mahwah, \nNJ: Erlbaum. http:// www. loc. gov/ catdir/ enhan cemen ts/ fy0634/ 00057 273-d. html.\nKorthagen, F.A. (2005). Levels in reflection: core reflection as a means to enhance professional growth. \nTeachers and Teaching, 11(1), 47–71.\nKorthagen, F.A., & Kessels, J. (1999). Linking theory and practice: Changing the pedagogy of teacher educa-\ntion. Educational Researcher, 28(4), 4–17.\nKovanović, V., Joksimović, S., Mirriahi, N., Blaine, E., Gašević, D., Siemens, G., & Dawson, S. (2018). \nUnderstand students’ self-reflections through learning analytics: LAK ’18, March 7–9, 2018, Sydney, \nNSW, Australia, pp. 389–398.\nLai, G., & Calandra, B. (2010). Examining the effects of computer-based scaffolds on novice teachers’ reflec-\ntive journal writing. Educational Technology Research and Development, 58(4), 421–437.\nLeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444.\nLevin, D.M., Hammer, D., & Coffey, J.E. (2009). Novice teachers’ attention to student thinking. Journal of \nTeacher Education, 60(2), 142–154.\nLin, X., Hmelo, C.E., Kinzer, C., & Secules, T. (1999). Designing technology to support reflection. Educa-\ntional Technology Research and Development, 47(3), 43–62.\nLoughran, J., & Corrigan, D. (1995). Teaching portfolios: A strategy for developing learning and teaching in \npreservice education. Teacher & Teacher Education, 11(6), 565–577.\nLuo, W., & Litman, D. (2015). Summarizing student responses to reflection prompts. Proceedings of the \n2015 Conference on Empirical Methods in Natural Language Processing, 1955–1960.\nMann, K., Gordon, J., & MacLeod, A. (2007). Reflection and reflective practice in health professions educa-\ntion: a systematic review. Advances in Health Sciences Education, 14(4), 595.\nMayfield, E., & Rose, C.P. (2010). An interactive tool for supporting error analysis for text mining: Proceed-\nings of the North American Association for Computational Linguistics (NAACL) HLT 2010: Demon-\nstration Session, Los Angeles, CA, June 2010, 25–28.\nMcNamara, D., Kintsch, E., Butler Songer, N., & Kintsch, W. (1996). Are good texts always better? Interac-\ntions of text coherence, background knowledge, and levels of understanding in learning from text. Cog-\nnition and Instruction, 14(1), 1–43.\nMena-Marcos, J., García-Rodríguez, M.-L., & Tillema, H. (2013). Student teacher reflective writing: What \ndoes it reveal? European Journal of Teacher Education, 36(2), 147–163.\nMitchell, M. (2020). Artificial Intelligence: A guide for thinking humans. Pelican Books.\nNehm, R.H., & Härtig, H. (2012). Human vs. computer diagnosis of students’ natural selection knowledge: \nTesting the efficacy of text analytic software. Journal of Science Education and Technology, 21(1), \n56–73.\nNeuweg, G.H. (2007). Wie grau ist alle Theorie, wie grün des Lebens goldner Baum? LehrerInnenbildung \nim Spannungsfeld von Theorie und Praxis. bwpat 12.\nNowak, A., Kempin, M., Kulgemeyer, C., & Borowski, A. (2019). Reflexion von Physikunterricht [Reflec-\ntion of physics lessons]. In C. Maurer (Ed.) Naturwissenschaftliche Bildung als Grundlage für berufli-\nche und gesellschaftliche Teilhabe: Jahrestagung in Kiel 2018. Regensburg: Gesellschaft für Didaktik \nder Chemie und Physik (p. 838).\n465\n1 3International Journal of Artificial Intelligence in Education (2023) 33:439–466 \nNowak, A., Liepertz, S., & Borowski, A. (2018). Reflexionskompetenz von Praxissemesterstudierenden \nim Fach Physik. In C. Maurer (Ed.) Qualitätsvoller Chemie- und Physikunterricht- normative und \nempirische Dimensionen: Jahrestagung in Regensburg 2017. Universität Regensburg.\nNye, B.D., Graesser, A.C., & Hu, X. (2014). AutoTutor and family: A review of 17 years of natural language \ntutoring. International Journal of Artificial Intelligence in Education, 24(4), 427–469.\nOstendorff, M., Bourgonje, P., Berger, M., Moreno-Schneider, J., Rehm, G., & Gipp, B. (2019). Enriching \nBERT with knowledge graph embeddings for document classification. arXiv:1909. 08402 v1.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ..., & Chintala, S. (2019). PyTorch: \nAn imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygel-\nzimer, d’ Alché-Buc, E. Fox, & R. Garnett (Eds.) Advances in neural information processing systems \n32. http:// papers. neuri ps. cc/ paper/ 9015- pytor ch- an- imper ative- style- high- perfo rmance- deep- learn ing- \nlibra ry. pdf(pp. 8024–8035). Curran Associates, Inc.\nPaterson, B.L. (1995). Developing and maintaining reflection in clinical journals. Nurse Education Today, \n15(3), 211–220.\nPoldner, E., van der Schaaf, M., Simons, P.R.-J., van Tartwijk, J., & Wijngaards, G. (2014). Assessing student teachers’ \nreflective writing through quantitative content analysis. European Journal of Teacher Education, 37(3), 348–373.\nPratt, L., & Thrun, S. (1997). Machine Learning, 28(5).\nPython Software Foundation. (2020). Python Language Reference: version 3.8. http:// www. python. org.\nRodgers, C. (2002). Defining Reflection: Another look at John Dewey and reflective thinking. Teachers Col-\nlege Record, 104(4), 842–866.\nRoe, M.F., & Stallman, A.C. (1994). A comparative study of dialogue and response journals. Teaching and \nTeacher Education, 10(6), 579–588.\nRosé, C., Wang, Y.-C., Cui, Y., Arguello, J., Stegmann, K., Weinberger, A., & Fischer, F. (2008). Analyzing \ncollaborative learning processes automatically: Exploiting the advances of computational linguistics in \ncomputer-supported collaborative learning. International Journal of Computer-Supported Collabora-\ntive Learning, 3(3), 237–271.\nRose, C.P. (2017). A social spin on language analysis. Nature, 545, 166–167.\nSchoenfeld, A.H. (2014). What makes for powerful classrooms, and how can we support teachers in creating \nthem? A story of research and practice, productively intertwined. Educational Researcher, 43(8), 404–412.\nSchön, D.A. (1983). The reflective practitioner: How professionals think in action. New York: Basic Books. \nhttp:// www. loc. gov/ catdir/ enhan cemen ts/ fy0832/ 82070 855-d. html.\nSchön, D.A. (1987). Educating the reflective practitioner: Toward a new design for teaching and learning in \nthe professions, 1st edn. San Francisco, Calif.: Jossey-Bass.\nIn M. Stede (Ed.) (2016). Handbuch Textannotation: Potsdamer Kommentarkorpus 2.0. Potsdam: Univer-\nsitätsverlag Potsdam.\nSundararajan, M., Taly, A., & Yan, Q. (2017). Axiomatic attribution for deep networks: Proceedings of the \n34th International Conference on Machine Learning, Sydney, Australia, PMLR 70.\nSwales, J.M. (1990). Genre analysis: English in academic and research settings. Cambridge: Cambridge \nUniv. Press.\nTaher Pilehvar, M., & Camacho-Collados, J. (2020). Embeddings in natural language processing: Theory and \nadvances in vector representation of meaning. Morgan and Claypool.\nTaylor, W.L. (1953). “Cloze Procedure”: A new tool for measuring readability. Journalism Quarterly.\nUllmann, T.D. (2017). Reflective writing analytics: Empirically determined keywords of written reflection: \nLAK ’17 Proceedings of the Seventh International Learning Analytics & Knowledge Conference, ACM \nInternational Conference Proceeding Series, 163–167.\nUllmann, T.D. (2019). Automated analysis of reflection in writing: Validating machine learning approaches. \nInternational Journal of Artificial Intelligence in Education, 29(2), 217–257.\nUllmann, T.D., Wild, F., & Scott, P. (2012). Comparing automatically detected reflective texts with human \njudgements. In Proceedings of the 2nd workshop on awareness and reflection in technology-enhanced \nlearning (AR-TEL 12), 18 September 2013 (pp. 101–116). Saarbrucken, Germany.\nvan Es, E., & Sherin, M.G. (2002). Learning to notice: scaffolding new teachers’ interpretations of classroom \ninteractions. Journal of Technology and Teacher Education, 10(4), 571–596.\nVanLehn, K. (2011). The relative effectiveness of human tutoring, intelligent tutoring systems, and other \ntutoring systems, (Vol. 46 pp. 197–221).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., ..., & Polosukhin, I. (2017). \nAttention is All you Need: Conference on Neural Information Processing Systems. Advances in Neural \nInformation Processing Systems, 6000–6010.\n466 International Journal of Artificial Intelligence in Education (2023) 33:439–466\n1 3\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ..., & Rush, A.M. (2020). HuggingFace’s \nTransformers: State-of-the-art Natural Language Processing. arXiv.\nWulff, P., Buschhüter, D., Nowak, A., Westphal, A., Becker, L., Robalino, H., & Borowski, A. (2020). Com-\nputer-based classification of preservice physics teachers’ written reflections. Journal of Science Educa-\ntion and Technology.\nZanette, D. (2014). Statistical pattern in written language. arXiv:1412. 3336.\nZeichner, K.M. (2010). Rethinking the connections between campus courses and field experiences in college- \nand university-based teacher education. Journal of Teacher Education, 61(1-2), 89–99.\nAvailability of data and material Please contact the first author.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations."
}