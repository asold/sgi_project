{
  "title": "Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs",
  "url": "https://openalex.org/W4379540146",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2231549614",
      "name": "Peña, Alejandro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742880687",
      "name": "Morales, Aythami",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744776064",
      "name": "Fierrez Julian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226508969",
      "name": "Serna, Ignacio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287080158",
      "name": "Ortega-Garcia, Javier",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4379654929",
      "name": "Puente, Iñigo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4379654930",
      "name": "Cordova, Jorge",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4379654931",
      "name": "Cordova, Gonzalo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4385485553",
    "https://openalex.org/W4379745834",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W2776154048",
    "https://openalex.org/W4281397847",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3209776558",
    "https://openalex.org/W4394637965",
    "https://openalex.org/W3018301625"
  ],
  "abstract": "The analysis of public affairs documents is crucial for citizens as it promotes transparency, accountability, and informed decision-making. It allows citizens to understand government policies, participate in public discourse, and hold representatives accountable. This is crucial, and sometimes a matter of life or death, for companies whose operation depend on certain regulations. Large Language Models (LLMs) have the potential to greatly enhance the analysis of public affairs documents by effectively processing and understanding the complex language used in such documents. In this work, we analyze the performance of LLMs in classifying public affairs documents. As a natural multi-label task, the classification of these documents presents important challenges. In this work, we use a regex-powered tool to collect a database of public affairs documents with more than 33K samples and 22.5M tokens. Our experiments assess the performance of 4 different Spanish LLMs to classify up to 30 different topics in the data in different configurations. The results shows that LLMs can be of great use to process domain-specific documents, such as those in the domain of public affairs.",
  "full_text": "arXiv:2306.02864v2  [cs.AI]  8 Aug 2023\nLeveraging Large Language Models for T opic\nClassiﬁcation in the Domain of Public Aﬀairs\nAlejandro Peña 1[0000− 0001− 6907− 5826], A ythami Morales 1[0000− 0002− 7268− 4785],\nJulian Fierrez 1[0000− 0002− 6343− 5656], Ignacio Serna 1[0000− 0003− 3527− 4071], Javier\nOrtega-Garcia1[0000− 0003− 0557− 1948], Íñigo Puente 2, Jorge Córdova 2, Gonzalo\nCórdova2\n1 BiDA - Lab, Universidad Autónoma de Madrid (UAM), Madrid 280 49, Spain\n2 VINCES Consulting, Madrid 28010, Spain\nAbstract. The analysis of public aﬀairs documents is crucial for citi-\nzens as it promotes transparency, accountability, and info rmed decision-\nmaking. It allows citizens to understand government polici es, participate\nin public discourse, and hold representatives accountable . This is crucial,\nand sometimes a matter of life or death, for companies whose o peration\ndepend on certain regulations. Large Language Models (LLMs ) have the\npotential to greatly enhance the analysis of public aﬀairs d ocuments by\neﬀectively processing and understanding the complex langu age used in\nsuch documents. In this work, we analyze the performance of L LMs in\nclassifying public aﬀairs documents. As a natural multi-la bel task, the\nclassiﬁcation of these documents presents important chall enges. In this\nwork, we use a regex-powered tool to collect a database of pub lic aﬀairs\ndocuments with more than 33K samples and 22.5M tokens. Our exper-\niments assess the performance of 4 diﬀerent Spanish LLMs to classify\nup to 30 diﬀerent topics in the data in diﬀerent conﬁgurations. The r e-\nsults shows that LLMs can be of great use to process domain-sp eciﬁc\ndocuments, such as those in the domain of public aﬀairs.\nKeywords: Domain Adaptation · Public Aﬀairs · T opic Classiﬁcation ·\nNatural Language Processing · Document Understanding · LLM\n1 Introduction\nThe introduction of the T ransfomer model [22] in early 2017 supposed a revolu-\ntion in the Natural Language Domain. In that work, V aswani et al. demonstrated\nthat an Encoder-Decoder architecture combined with an Atte ntion Mechanism\ncan increase the performance of Language Models in several t asks, compared to\nrecurrent models such as LSTM [8]. Over the past few years, th ere has been a sig-\nniﬁcant development of transformer-based language model a rchitectures, which\nare commonly known as Large Language Models (LLM). Its deplo yment sparked\na tremendous interest and exploration in numerous domains, including chatbots\n2 A. Peña, A. Morales, J. Fierrez, et al.\n(e.g., ChatGPT, 3 Bard,4 or Claude 5), content generation [2,16], virtual AI as-\nsistants (e.g., JAR VIS [20], or GitHub’s Copilot 6), and other language-based\ntasks [9][10][11]. These models address scalability chall enges while providing sig-\nniﬁcant language understanding and generation abilities. That deployment of\nlarge language models has propelled advancements in conver sational AI, au-\ntomated content creation, and improved language understan ding across vari-\nous applications, shaping a new landscape of NLP research an d development.\nThere are even voices raising the possibility that most rece nt foundational mod-\nels [1][12][13][21] may be a ﬁrst step of an artiﬁcial genera l intelligence [3].\nLarge language models have the potential to greatly enhance the analysis\nof public aﬀairs documents. These models can eﬀectively pro cess and under-\nstand the complex language used in such documents. By levera ging their vast\nknowledge and contextual understanding, large language mo dels can help to ex-\ntract key information, identify relevant topics, and perfo rm sentiment analysis\nwithin these documents. They can assist in summarizing leng thy texts, catego-\nrizing them into speciﬁc themes or subject areas, and identi fying relationships\nand patterns between diﬀerent documents. Additionally , th ese models can aid\nin identifying inﬂuential stakeholders, tracking changes in public sentiment over\ntime, and detecting emerging trends or issues within the dom ain of public aﬀairs.\nBy leveraging the power of large language models, organizat ions and policymak-\ners can gain valuable insights from public aﬀairs documents , enabling informed\ndecision-making, policy formulation, and eﬀective commun ication strategies. The\nanalysis of public aﬀairs documents is also important for ci tizens as it promotes\ntransparency , accountability , and informed decision-mak ing.\nPublic aﬀairs documents often cover a wide range of topics, i ncluding policy\nissues, legislative updates, government initiatives, soc ial programs, and public\nopinion. These documents can address various aspects of pub lic administration,\ngovernance, and societal concerns. The automatic analysis of public aﬀairs text\ncan be considered a multi-label classiﬁcation problem. Mul ti-label classiﬁcation\nenables the categorization of these documents into multipl e relevant topics, al-\nlowing for a more nuanced understanding of their content. By employing multi-\nlabel classiﬁcation techniques, such as text categorizati on algorithms, public af-\nfairs documents can be accurately labeled with multiple att ributes, facilitating\neﬃcient information retrieval, analysis, and decision-ma king processes in the\nﬁeld of public aﬀairs.\nThis work focuses on NLP-related developments in an ongoing research project.\nThe project aims to improve the automatic analysis of public aﬀairs documents\nusing recent advancements in Document Layout Analysis (DLA ) and Language\nT echnologies. The objective of the project is to develop new tools that allow citi-\nzens and businesses to quickly access regulatory changes th at aﬀect their present\nand future operations. With this objective in mind, a system is being developed\n3 https://openai.com/blog/chatgpt\n4 https://blog.google/technology/ai/bard-google-ai-search-updates/\n5 https://www.anthropic.com/index/introducing-claude\n6 https://github.com/features/preview/copilot-x\nLeveraging Large Language Models in the Domain of Public Aﬀa irs 3\nto monitor the publication of new regulations by public orga nizations The block\ndiagram of the system is depicted in Figure 1. The system is co mposed of three\nmain modules: i) Harvester module based on web scrappers; ii) a Document Lay-\nout Analysis (DLA) module; and iii) a T ext Processing module. The Harvester\nmonitors a set of pre-deﬁned information sources, and autom atically downloads\nnew documents in them. Then, the DLA module conducts a layout extraction\nprocess, where text blocks are characterized and automatic ally classiﬁed, us-\ning Random F orest models, into diﬀerent semantic categorie s. Finally , a T ext\nProcessing module process the text blocks using LLMs techno logy to perfom\nmulti-label topic classiﬁcation, ﬁnally aggregating indi vidual text predictions to\ninfer the main topics of the document.\nThe full system proposed in Figure 1 serves us to adapt LLMs to analyze doc-\numents in the domain of public aﬀairs. This adaptation is bas ed on the dataset\nused in our experiments, generated in collaboration with ex perts in public aﬀairs\nregulation. They annotated over 92K texts using a semi-supervised process that\nincluded a regex-based tool. The database comprises texts r elated to more than\n385 diﬀerent public aﬀairs topics deﬁned by experts.\nF rom all the analysis tool that can be envisioned in the gener al framework\ndepicted in Figure 1, in the present paper we focus in topic cl assiﬁcation, with the\nnecessary details of the Harverster needed to explain our da tasets and interpret\nour topic classiﬁcation results. Other modules such as the L ayout Extractor are\nleft for description elsewhere.\nSpeciﬁcally , the main contributions of this work are:\n– Within the general document analysis system for analyzing p ublic aﬀairs\ndocuments depicted in in Figure 1, we propose, develop, and e valuate a\nnovel functionality for multi-label topic classiﬁcation.\n– W e present a new dataset of public aﬀairs documents annotate d by topic\nwith more than 33K text samples and 22.5M tokens representin g the main\nSpanish legislative activity between 2019 and 2022.\n– W e provide experimental evidence of the proposed multi-lab el topic classi-\nﬁcation functionality over that new dataset using four diﬀe rent LLMs (in-\ncluding RoBER T a [11] and GPT 2 [16]) followed by multiple classiﬁers.\nOur results shows that using a LLM backbone in combination wi th SVM clas-\nsiﬁers suppose an useful strategy to conduct the multi-labe l topic classiﬁcation\ntask in the domain of public aﬀairs with accuracies over 85%. The SVM clas-\nsiﬁcation improves accuracies consistently , even with cla sses that have a lower\nnumber of samples (e.g., less than 500 samples).\nThe rest of the paper is structured as follows: In Section 2 we describe the\ndata collected for this work, including data preprocessing details. Section 3 de-\nscribes the development of the proposed topic classiﬁcatio n functionality . Sec-\ntion 4 presents the experiments and results of this work. Fin ally , Section 5 sum-\nmarizes the main conclusions.\n4 A. Peña, A. Morales, J. Fierrez, et al.\nPublic\nInformation\nSources\nHarvester\nScrapy\n>\nS1\nS2\nSM\nLayout \nExtractor\nRF\nLLM\nRoBERTa, \nGPT2\nLarge Text \nCorpus\nClassifier\nSVM\nPublic Affairs \nExpert Annotation\nDomain \nAdaptation\nTopic List\nText \nProcessing\nDocument \nLayout Analysis\nDocument \nCapturing\nGeneral Purpose Modules\nDomain Modules\n⋮\nFig. 1. Block diagram of an automatic public aﬀairs document analys is system.\nThe white blocks represent general-purpose modules, while the grey blocks represent\ndomain-speciﬁc modules.\n2 Data Collection and Analysis\nThe major decisions and events resulting from the legislati ve, judicial and admin-\nistrative activity of public administrations are public da ta. Is a common practice,\nand even a legal requisite, for these administrations to pub lish this information\nin diﬀerent formats, such as govermental websites or oﬃcial gazettes7. Here, we\nuse a regex-powered tool to follow up parliamentary initiat ives from the Spanish\nParlament, resulting in a legislative-activities text cor pora in Spanish. Parlia-\nmentary initiatives involve a diverse variety of parliamen t interactions, such as\nquestions to the government members, legislative proposal s, etc.\nRaw data were collected and processed with this tool, and com prise initiatives\nranging from November 2019 to October 2022. The data is composed of short\ntexts, which may be annotated with multiple labels. Each lab el includes, among\nothers, topic annotations based on the content of the text. T hese annotations\nwere generated using regex logic based on class-speciﬁc pre deﬁned keywords.\nBoth topic classes and their corresponding keywords were de ﬁned by a group\nof experts in public aﬀairs regulations. It is important to n ote that the same\ntopic (e.g., “ Health Policy ”) can be categorized diﬀerently depending on the\nuser’s perspective (e.g., citizens, companies, governmen tal agencies). W e have\nsimpliﬁed the annotation, adding a ID number depending on th e perspective\nused (e.g., “ Health Policy_1 ” or “ Health Policy_2 ”). Our raw data is composed\nof 450K initiatives grouped in 155 weekly-duration sessions, with a total number\nof topic classes up to 385. Of these 450K samples, only 92.5K were labeled,\nwhich suppose roughly 20.5% of the samples. However, almost half of these are\nannotated with more than one label (i.e. 45.5K, 10.06% of samples), with a\ntotal number of labels of 240K. Figure 2 presents the distribution of the 30\nmost frequent topics in the data, where we can clearly observ e the signiﬁcant\nimbalance between classes. The most frequent topic in the ra w data is “ Healthcare\n7 https://op.europa.eu/en/web/forum\nLeveraging Large Language Models in the Domain of Public Aﬀa irs 5\nFig. 2. Distribution of the top 30 most frequent topics in the raw dat a.\nSituation”, appearing in more then 25K data samples. Other topics, such as\n“ Health Policy”, have an important presence in the data as well. However, on ly\n8 out of these 30 topics reach 5K samples, and only 5 of them are present in at\nleast 10K. This imbalance, along with the bias towards health-relat ed subjects in\nthe most frequent topics, is inherent to the temporal framew ork of the database,\nas the Covid- 19 pandemic situation has dominated signiﬁcant public aﬀairs over\nthe past 3 years. Note that Figure 2 depicts the thirty most fr equent topics,\nwhereas 385 topics are present in the data. T o prevent the eﬀects of major class\nimbalances, we will now focus on the 30 topics of Figure 2.\n2.1 Data Curation\nW e applied a data cleaning process to the raw corpora to gener ate a clean version\nof the labeled data. W e started by removing duplicated texts , along with data\nsamples with less than 100 characters. Some works addressing Spanish models\napplied a similar ﬁltering strategy with a threshold of 200 characters [17,23,19]\nwith the aim of obtaining a clean corpus to pre-train transfo rmer models. Here\nwe set the threshold to 100, as our problem here does not require us to be that\nstrict (i.e., we do not want to train a transformer from scrat ch). Instead, we\ndesired to remove extremely short text, which we qualitativ e assessed that were\nmainly half sentences, while retaining as much data as possi ble. In this sense, we\nﬁlter text samples of any length starting with lowercase, to prevent half sentences\n6 A. Peña, A. Morales, J. Fierrez, et al.\nID T opic #Samples ID T opic #Samples\n1 Healthcare Situation 13561 16 Primary Healthcare_ 1 1425\n2 Health Policy_ 1 12029 17 Sustainability 1370\n3 Health Policy_ 2 8229 18 Wastes 1294\n4 Health Policy_ 3 8111 19 Aids 1216\n5 Industrial Emissions 5101 20 Primary Healthcare_ 2 1189\n6 Covid-19 Pandemic 3298 21 T ourism Oﬀer 1181\n7 T ourism Policy 2209 22 Labor Equity 1074\n8 T ourism Companies 2033 23 Industry 1051\n9 Climate Change_ 1 1930 24 Infrastructures 1029\n10 Vaccination 1924 25 Covid-19 Vaccination 997\n11 Vaccine 1751 26 National Healthcare System 964\n12 Covid-19 Vaccine 1617 27 Climate Change_ 2 886\n13 T ourism Strategy 1533 28 Housing Policy 744\n14 Labor Reform 1529 29 Department of Health_ 1 541\n15 Health Innovation 1469 30 Department of Health_ 2 518\nT able 1.Summary of the parliamentary initiative database after the data cleaning\nprocess, which includes 33,147 data samples with multi-label annotations across 30\ntopics. We include a topic ID, the topic, and the number of sam ples annotated for each\nof them.\nto leak in. W e also identiﬁed bad quality/noisy text samples to start with “CSV”\nor “núm”, so we remove samples based on this rule. Finally , gi ven the existence\nof co-oﬃcial languages diﬀerent from Spanish in Spain (e.g. , Basque, Galician\nor Catalan), which are used by a signiﬁcant percentage of Spa nish citizens, we\nﬁlter data samples from these languages. Due to the lack of re liable language\ndetectors in these co-oﬃcial languages, and the use of some l inguistic, domain-\nspeciﬁc patterns in the parliamentary initiatives, we iden tiﬁed a set of words in\nthese languages and use it to detect and ﬁlter out potential s amples not written\nin Spanish. W e applied this process several times to reﬁne th e set of words.\nAt data sample level, we clean texts by removing excessive wh ite spaces and\ninitiative identiﬁers in the samples. W e then ﬁlter URLs and non-alphanumeric\ncharacters, retaining commonly used punctuation characte rs in Spanish written\ntext (i.e., ()-.¿?¡!_;). After applying all the data curati on process, we obtain a\nmulti-label corpus of 33,147 data samples, with annotations on the 30 topics\ncommented above. T able 1 presents the number of samples per t opic category .\nNote that the number of samples of each topic has signiﬁcantl y decreased com-\npared to the proportions observed in the raw data (see Figure 2). The impact\nof the data curation process is diﬀerent between topics, lea ding to some changes\nin the frequency-based order of the topics. The topic with mo st data samples\nin the curated corpus is still “ Healthcare Situation”, but the number of samples\nannotated with this topic has been reduced by half. On the oth er hand, we have\nseveral topics with less than 1K samples, setting a lower limit of 518.\nLeveraging Large Language Models in the Domain of Public Aﬀa irs 7\n3 Methodology and Models\nAs we previously mentioned in Section 2, the samples in our da taset may present\nmore than one topic label. Hence, the topic classiﬁcation ta sk on this dataset is a\nmulti-label classiﬁcation problem, where we have a signiﬁc ant number of classes\nthat are highly imbalanced. This scenario (i.e., high numbe r of classes, some of\nthem with few data samples, with overlapped subjects betwee n classes) leads\nus to discard a single classiﬁer for this task. Instead of add ressing the problem\nas a multi-label task, we break it into small, binary detecti on tasks, where an\nindividual topic detector is trained for each of the 30 classes in a one vs all\nsetup. This methodology , illustrated in Figure 3, represen ts a big advantage, as\nit provides us a high degree of versatility to select the best model conﬁguration\nfor each topic to deploy a real system. During inference, new data samples can\nbe classiﬁed by aggregating the predictions of the individu al classiﬁers [5].\nThe architecture of the binary topic models is depicted in Fi gure 3. W e use a\ntransformer-based model as backbone, followed by a Neural N etwork, Random\nF orest, or SVM classiﬁer. In this work, we explore diﬀerent t ransformer models,\npretrained from scratch in Spanish by the Barcelona Superco mputing Center in\nthe context of the MarIA project [7]. W e included both encode r and decoder\narchitectures. These model architectures are the followin g:\n– RoBER T a-base . An encoder-based model architecture with 12 layers, 768\nhidden size, 12 attention heads, and 125M parameters.\n– RoBER T a-large . An encoder-based model architecture with 24 layers, 71,024\nhidden size, 16 attention heads, and 334M parameters.\n– RoBER T alex . A version [6] of RoBER T a-base, ﬁne-tuned for the Spanish\nlegal domain.\n– GPT 2-base. A decoder-based model architecture with 12 layers, 768 hidden\nsize, 12 attention heads, and 117M parameters.\nW e listed above the conﬁgurations reported in [7] for the ope n-source mod-\nels available in the the HuggingF ace repository of the model s.8 The RoBER T a\nmodels [11] are versions of BER T models [9], in which an optim ized pre-training\nstrategy and hyperparameter selection was applied, compar ed to the original\nBER T pre-training. The Spanish versions of these models wer e pre-trained fol-\nlowing the original RoBER T a conﬁguration, with a corpus of 570 GB of clean\nSpanish written text. The RoBER T alex model is a ﬁne-tuned ve rsion of Spanish\nRoBER T a-base, trained with a corpus of 8.9 GB of legal text data. On the other\nhand, GPT 2 [16] is a decoder-based model of the GPT family [2][12][13][ 15]. As\nsuch, the model is aimed to generative tasks (note that moder n versions of GPT\nmodels, such as InstructGPT [13] or GPT 4 [12] are ﬁne-tuned to follow human\ninstructions, so they cannot be considered generative mode ls in the same way as\nearlier GPT models), diﬀerent from the RoBER T a family , whic h is specialized\nin text understanding. The version used of GPT 2 was trained using the same\ncorpus as the RoBER T a models. All the models use byte-level B PE tokenizer [16]\n8 https://huggingface.co/PlanTL-GOB-ES\n8 A. Peña, A. Morales, J. Fierrez, et al.\nFig. 3. Proposed multi-label topic classiﬁcation system, in which an individual topic\ndetector is applied to an input text before aggregating all t he predictions, and the\narchitecture of each binary topic classiﬁer.\nwith vocab size of 50,265 tokens, and have the same length for the context win-\ndows, i.e. 512. While left padding is used in the RoBER T a models, right padd ing\nis advisable for the GPT 2 model.\n4 Experiments\nAs exposed in Section 3, due to the nature of the dataset colle cted for this work,\nwe address multi-label topic classiﬁcation by training a bi nary topic classiﬁer\nfor each class (one vs all), and then aggregating the individ ual predictions on a\nversatile way (e.g., providing rank statistics, topics ove r a ﬁxed threshold, etc.).\nHence, our experiments will focus on assessing the performa nce of diﬀerent topic\nclassiﬁers conﬁgurations, and the potential of the newly av ailable Spanish lan-\nguage models in unconstrained scenarios (i.e., multi-labe l political data, with\nsubjective annotations based on private-market interest) . Section 4.1 will evalu-\nate ﬁrst the performance of diﬀerent transformer-based mod els on our dataset,\nand then explore the combination of the best-performance mo del with SVM and\nRandom F orest classiﬁers.\nW e conduct all the experiments using a K-fold cross validati on setup with 5\nfolds, and report mean and average results between folds. W e select T rue Positive\nRate (TPR), and T rue Negative Rate (TNR) as our performance m easures, due\nto the class imbalances in the parliamentary dataset. W e use in our experiments\nLeveraging Large Language Models in the Domain of Public Aﬀa irs 9\nthe models available in the HuggingF ace transformers libra ry9, along with several\nsklearn tools. Regarding the hardware, we conducted the exp eriments in a PC\nwith 2 NVIDIA R TX 4090 (with 24 GB each), Intel Core i 9, 32GB RAM.\n4.1 T opic Classiﬁcation in the Domain of Public Aﬀairs\nRecalling from Figure 3, our topic detector architecture is mainly composed of\ni) a transformer backbone, and ii) a classiﬁer. W e train the transformer mod-\nels with a binary neural network classiﬁcation output layer . F or each topic, we\ntrain the detector using W eighted Cross Entropy Loss to addr ess the class im-\nbalance in a “One vs All” setup. T opic classiﬁers are trained for 5 epochs using\na batch size of 32 samples, and freezing the transformer layers. T able 2 prese nts\nthe results of the topics classiﬁers using the four transfor mer models explored\nin this work (i.e., RoBER T a-base [7], RoBER T a-large [7], Ro BER T alex [6], and\nGPT2-base [7]). W e can observe a general behavior across the RoBE R T a mod-\nels. The classiﬁers trained for the topics with more samples obtain higher TPR\nmeans, close to the TNR mean values. In these cases, the class iﬁers are able to\ndistinguish reasonably well text samples in which the train ed topic is present.\nThese results are, in general, consistent across folds, exh ibiting moderate devia-\ntion values. This behavior degrades from T opic 9 onwards, where the low number\nof samples (i.e., less than 2K) leads to an increase of the TNR to values over 90%\nwith a decay of TPR. However, we can observe some exceptions i n the classiﬁers\nusing RoBER T a-base as backbone (topics 11, 12, 24), where TNR scales to val-\nues close to 100% while preserving TPR performances over 80%. F urthermore,\nRoBER T a-base classiﬁers exhibit better results than the Ro BER T a-large classi-\nﬁers (probably due to the constrained number of samples), an d even than the\nRoBER T alex models. Remember that both RoBER T a-base and RoB ER T alex are\nthe same models, the latter being the RoBER T a-base model wit h a ﬁne-tuning\nto the legal domain that, a priori, should make it more approp riate for the prob-\nlem at hand. Regarding GPT 2-based classiﬁers, we observe similar trends to\nthose of the RoBER T a models, but exhibiting lower performan ces. This is not\nsurprising, as the GPT model was trained for generative purp oses, rather than\ntext understanding like RoBER T a.\nIt’s worth noting here the case of T opic 1, which obtains the lowest TNR\nmean value in all models, with deviation values over 0.15, despite being the topic\nwith more data samples (i.e. a third of the data). W e hypothes ize that the low\nperformances when detecting negative samples is mostly due to the overlap with\nthe rest of the topics, as this topic focuses on general healt hcare-related aspects\n(remember from T able 1 that half of the topics are related wit h healthcare).\nF rom the results presented in T able 2, we can conclude that Ro BER T a-base\nis the best model backbone for our task. Now, we want to assess if a specialized\nclassiﬁer, such as Support V ector Machines (SVM) or Random F orests (RF), can\nbe used to ﬁne tune the performance to the speciﬁc domain. F or these classiﬁers,\nwe used RoBER T a-base as feature extractor to compute 768-dimensional text\n9 https://huggingface.co/docs/transformers/index\n10 A. Peña, A. Morales, J. Fierrez, et al.\nID RoBERT a-b[7] RoBERT a-l[7] GPT2-b [7] RoBERT alex[6]\nTPR TNR TPR TNR TPR TNR TPR TNR\n1 .80.07 .75.19 .78.08 .76.19 .58.14 .60.15 .79.10 .70.18\n2 .87.09 .88.04 .84.11 .86.05 .61.25 .82.05 .83.10 .82.06\n3 .83.08 .87.04 .81.09 .87.04 .65.18 .79.07 .79.09 .84.05\n4 .86.07 .89.03 .83.10 .88.03 .69.17 .79.07 .80.10 .86.04\n5 .76.05 .81.06 .72.07 .81.07 .63.06 .74.09 .67.08 .80.06\n6 .82.05 .87.02 .83.05 .87.03 .67.04 .63.08 .68.06 .83.04\n7 .85.04 .93.03 .83.06 .91.05 .64.08 .78.08 .75.07 .94.03\n8 .82.02 .89.05 .81.03 .88.06 .63.04 .78.07 .69.02 .91.04\n9 .79.10 .90.04 .77.11 .89.06 .58.08 .76.08 .68.07 .91.03\n10 .76.26 .96.03 .67.31 .95.03 .49.42 .91.10 .62.34 .95.04\n11 .89.11 .9802 .72.31 .98.01 .55.44 .93.09 .70.32 .97.03\n12 .88.12 .98.02 .73.30 .98.01 .57.41 .94.09 .72.30 .97.02\n13 .76.09 .89.06 .75.08 .86.07 .33.09 .79.09 .58.14 .91.05\n14 .76.12 .93.03 .72.12 .93.03 .39.13 .81.06 .65.13 .94.02\n15 .61.09 .85.04 .58.10 .86.03 .53.06 .82.05 .54.08 .90.02\n16 .75.03 .90.03 .71.05 .88.04 .43.04 .77.03 .64.05 .91.03\n17 .71.25 .94.06 .64.32 .96.05 .59.31 .93.05 .65.31 .92.05\n18 .62.08 .90.03 .54.10 .85.05 .36.07 .79.07 .5105 .9102\n19 .69.10 .92.02 .69.09 .91.03 .45.11 .86.05 .49.12 .95.01\n20 .7305 .93.02 .73.06 .90.03 .32.04 .86.03 .58.05 .94.02\n21 .67.04 .89.05 .67.06 .86.06 .48.05 .84.05 .45.03 .93.03\n22 .71.05 .95.02 .66.03 .94.02 .40.04 .89.04 .51.03 .97.01\n23 .70.08 .96.02 .57.17 .96.02 .24.17 .96.01 .43.17 .98.01\n24 .83.08 .97.04 .69.11 .98.01 .20.24 .98.01 .55.18 .98.01\n25 .80.16 .97.04 .54.36 .97.04 .44.40 .98.03 .57.34 .97.04\n26 .52.10 .95.01 .48.13 .96.01 .17.03 .96.02 .40.10 .98.01\n27 .72.08 .97.02 .62.08 .97.02 .25.07 .97.01 .56.05 .9801\n28 .44.05 .97.02 .32.15 .96.03 00 10 .20.08 .9901\n29 .46.06 .98.01 .17.04 .990 00 10 .1804 .990\n30 .43.06 .98.01 .15.03 .990 00 10 .15.03 .990\nT able 2.Results of the binary classiﬁcation for each topic (one vs al l), using diﬀerent\ntransformer models with a Neural Network classiﬁer. We repo rt T rue Positive Rate\n(TPR) and T rue Negative Rate (TNR) as meanstd (in parts per unit), computed after\na K-fold cross validation ( 5 folds).\nembeddings from each of the text samples. W e explored two app roaches for\nthese embeddings: i) using the embedding computed for the [CLS] token, and\nii) averaging all the token embeddings (i.e., mean pooling). In the original BER T\nmodel [9], and hence the RoBER T a model, the [CLS] is a special token appended\nat the start of the input, which the model uses during trainin g for the Next\nSentence Prediction objective. Thus, the output for this em bedding is used for\nclassiﬁcation purposes, serving the [CLS] embedding as a te xt representation.\nW e repeated the experiment using both types of representati ons, and end up\nselecting the ﬁrst approach after exhibiting better result s. T able 3 presents the\nresults of the topic models using RoBER T a-base text embeddi ngs together with\na SVM and Random F orest classiﬁer. In all cases, we use a compl exity parameter\nof 1 and RBF kernel for the SVM, and a max depth of 1,000 for the Random\nF orest. W e note that these parameters can be tuned for each to pic to improve\nthe results. The ﬁrst thing we notice in T able 3 is the poor per formance of the\nRF-based classiﬁers, which are the worst among all the conﬁg urations. Almost\nfor all the topics under 2K samples, the TNR saturates to 1, and the TPR tends\nto extremely low values. F rom this, we can interpret that the classiﬁer is not\nLeveraging Large Language Models in the Domain of Public Aﬀa irs 11\nID RoBERT a-b[7] + SVM RoBERT a-b[7] + RF\nTPR TNR TPR TNR\n1 .80.07 .76.20 .70.11 .81.21\n2 .87.09 .88.04 .74.18 .94.03\n3 .83.07 .88.04 .64.18 .97.02\n4 .86.07 .90.02 .67.18 .98.02\n5 .80.05 .80.06 .12.06 .99.01\n6 .85.05 .85.03 .23.04 .990\n7 .90.02 .90.05 .49.06 10\n8 .89.01 .86.07 .33.02 10\n9 .88.07 .87.04 .24.05 10\n10 .84.18 .94.03 .51.41 10\n11 .92.08 .97.02 .57.38 10\n12 .93.07 .98.02 .56.36 10\n13 .87.04 .85.08 .08.02 10\n14 .87.07 .88.04 .14.04 10\n15 .70.08 .80.06 .06.02 10\n16 .89.03 .88.04 .13.05 10\n17 .79.18 .92.06 .5931 10\n18 .78.06 .81.05 .09.03 10\n19 .87.03 .85.03 .14.10 10\n20 .89.03 .90.03 .14.06 10\n21 .88.02 .79.08 .09.02 10\n22 .90.03 .88.03 .16.03 10\n23 .89.04 .89.05 .27.15 10\n24 .90.05 .95.02 .37.23 10\n25 .90.07 .95.04 .41.31 .99.01\n26 .83.06 .89.04 .17.11 10\n27 .91.04 .90.04 .33.04 10\n28 .87.04 .86.06 .06.01 10\n29 .84.06 .89.03 .10.03 10\n30 .85.05 .89.03 .08.03 10\nT able 3.Results of the binary classiﬁcation for each topic (one vs al l), using RoBERT a-\nbase [7] in combination with SVM and Random F orest classiﬁer s. We report T rue\nPositive Rate (TPR) and T rue Negative Rate (TNR) as meanstd (in parts per unit),\ncomputed after a K-fold cross validation ( 5 folds).\nlearning, and just predicting the negative, overrepresent ed class. However, the\nperformance on the topics over 2K samples is far from the one observed for\nthe RoBER T a models of T able 2. This could be expected, as the R F classiﬁer\nis not the best approach to work with input data representing a structured\nvector subspace with semantic meaning, such as text/word em bedding subspaces,\nspecially when the number of data samples is low. On the other hand, the SVM\nperformance clearly surpass all previous conﬁgurations in terms of TPR. While\nthe results are comparable with those of RoBER T a-base with N N for the ﬁrst 5\ntopics, this behavior is maintained for all topics, regardl ess of the number of data\nsamples. Almost all classiﬁers achieve a TPR over 80%, except for topics 15, 17\nand 18. Nevertheless, the results in these topics increase with th e SVM (e.g.,\nfor topic 15, where RoBER T a-base with the NN classiﬁer achieved a TPR mea n\nof 61%, here we obtain a 70%). TNR values are, in general, slightly lower, but\nthis could be caused because in previous conﬁgurations, top ic classiﬁers tend to\nexhibit bias towards the negative class as the number of samp les falls (i.e., similar\nto the behavior of the RF classiﬁer). Interestingly , the hig h deviation observed\nin the T opic 1 TNR appears too in both SVM and RF classiﬁers, which could\n12 A. Peña, A. Morales, J. Fierrez, et al.\nsupport our previous hypothesis. As we commented before, we suspect that an\nhyperparameter tuning could improve even more the SVM resul ts on our data.\n5 Conclusions\nThis work applies and evaluates Large Language Models (LLMs ) for topic classi-\nﬁcation in public aﬀairs documents. These documents are of s pecial relevance for\nboth citizens and companies, as they contain the basis of all legislative updates,\nsocial programs, public announcements, etc. Thus, enhanci ng the analysis of\npublic documents using the recent advances of the NLP commun ity is desirable.\nT o this aim, we collected a Spanish text corpora of public aﬀa irs documents,\nusing a regex-powered tool to process and annotate legislat ive initiatives from the\nSpanish Parlament during a capture period over 2 years. The r aw text corpora\nis composed of more than 450K initiatives, with 92K of them being annotated in\na multi-label scenario with up to 385 diﬀerent topics. T opic classes were deﬁned\nby experts in public aﬀairs regulations. W e preprocess this corpus and generate\na clean version of more than 33K multi-label texts, including annotations for the\n30 most frequent topics in the data.\nW e use this dataset to assess the performance of recent Spani sh LLMs [6][7]\nto perform multi-label topic classiﬁcation in the domain of public aﬀairs. Our ex-\nperiments include text understanding models (three diﬀere nt RoBER T a-based\nmodels [11]) and generative models [16], in combination wit h three diﬀerent\nclassiﬁers (i.e., Neural Networks, Random F orests, and SVM s). The results show\nhow text understanding models with SVM classiﬁers supposes an eﬀective strat-\negy for the topic classiﬁcation task in this domain, even in s ituations where the\nnumber of data samples is limited.\nAs future work, we plan to study in more depth biases and imbal ances [4]\nlike the ones mentioned before presenting Figure 2, and comp ensating them\nwith imbalance-aware machine learning procedures [18]. Mo re recent LLMs can\nbe also tested for this task, including multilingual and ins truction-based models,\nwhich have shown great capacities in multiple NLP tasks, eve n in zero-shot\nscenarios. W e will also continue our research by exploring t he incorporation\nof other NLP tasks (e.g. text summarization, named entity re cognition) and\nmultimodal methods [14] to our framework, with the objectiv e of enhancing\nautomatic analysis of public aﬀairs documents.\n6 Acknowledgments\nThis work was supported by VINCES Consulting under the proje ct VINCESAI-\nARGOS and BBforT AI (PID 2021-127641OB-I00 MICINN/FEDER). The work\nof A. Peña is supported by a FPU F ellowship (FPU 21/00535) by the Spanish\nMIU. Also, I. Serna is supported by a FPI F ellowship from the U AM.\nLeveraging Large Language Models in the Domain of Public Aﬀa irs 13\nReferences\n1. Anil, R., Dai, A.M., Firat, O., Johnson, M., et al.: PaLM 2 t echnical report.\narXiv/2305.10403 (2023)\n2. Brown, T., Mann, B., Ryder, N., Subbiah, M., et al.: Langua ge models are few-shot\nlearners. In: NIPS. vol. 33, pp. 1877–1901 (2020)\n3. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., et al.: Sparks of artiﬁcial\ngeneral intelligence: Early experiments with GPT-4. arXiv /2303.12712 (2023)\n4. DeAlcala, D., Serna, I., Morales, A., Fierrez, J., et al.: Measuring bias in AI models:\nAn statistical approach introducing N-Sigma. In: COMPSAC ( 2023)\n5. Fierrez, J., Morales, A., Vera-Rodriguez, R., Camacho, D .: Multiple classiﬁers in\nbiometrics. Part 1: F undamentals and review. Information F usion 44, 57–64 (2018)\n6. Gutiérrez-F andiño, A., Armengol-Estapé, J., Gonzalez- Agirre, A., Villegas, M.:\nSpanish legalese language model and corpora. arXiv/2110.1 2201 (2021)\n7. Gutiérrez-F andiño, A., Armengol-Estapé, J., Pàmies, M. , Llop, J., et al.: MarIA:\nSpanish language models. Procesamiento del Lenguaje Natur al 68 (2022)\n8. Hochreiter, S., Schmidhuber, J.: Long short-term memory . Neural Computation\n9(8), 1735–1780 (1997)\n9. Kenton, J., Chang, M., Lee, K., T outanova, K.: BERT: Pre-t raining of deep bidirec-\ntional transformers for language understanding. In: NAACL . pp. 4171–4186 (2019)\n10. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., et al.: BART: Denoising\nsequence-to-sequence pre-training for natural language g eneration, translation, and\ncomprehension. In: ACL. pp. 7871–7880 (2020)\n11. Liu, Y., Ott, M., Goyal, N., Du, J., et al.: RoBERTa: A robu stly optimized BERT\npretraining approach. arXiv/1907.11692 (2019)\n12. OpenAI: GPT-4 technical report. T ech. rep. (2023)\n13. Ouyang, L., Wu, J., Jiang, X., Almeida, D., et al.: T raini ng language models to\nfollow instructions with human feedback. NIPS 35, 27730–27744 (2022)\n14. Peña, A., Serna, I., Morales, A., Fierrez, J., et al.: Hum an-centric multimodal ma-\nchine learning: Recent advances and testbed on AI-based rec ruitment. SN Com-\nputer Science (2023)\n15. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I ., et al.: Improving lan-\nguage understanding by generative pre-training. T ech. rep . (2018)\n16. Radford, A., Wu, J., Child, R., Luan, D., et al.: Language models are unsupervised\nmultitask learners. OpenAI blog 1(8), 9 (2019)\n17. Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., et al.: Expl oring the limits of transfer\nlearning with a uniﬁed text-to-text transformer. The Journ al of Machine Learning\nResearch 21(1), 5485–5551 (2020)\n18. Serna, I., Morales, A., Fierrez, J., Obradovich, N.: Sen sitive loss: Improving accu-\nracy and fairness of face representations with discriminat ion-aware deep learning.\nArtiﬁcial Intelligence 305, 103682 (2022)\n19. Serrano, A., Subies, G.and Zamorano, H., Garcia, N., et a l.: RigoBERTa: A state-\nof-the-art language model for spanish. arXiv/2205.10233 ( 2022)\n20. Shen, Y., Song, K., T an, X., Li, D., et al.: HuggingGPT: So lving AI tasks with\nChatGPT and its friends in HuggingF ace. arXiv/2303.17580 ( 2023)\n21. T ouvron, H., Lavril, T., Izacard, G., Martinet, X., et al .: LLaMA: Open and eﬃ-\ncient foundation language models. arXiv/2302.13971 (2023 )\n22. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., et a l.: Attention is all you\nneed. Advances in Neural Information Processing Systems 30 (2017)\n23. Xue, L., Constant, N., Roberts, A., Kale, M., et al.: mT5: A massively multilingual\npre-trained text-to-text transformer. In: NAACL. pp. 483– 498 (2021)",
  "topic": "Transparency (behavior)",
  "concepts": [
    {
      "name": "Transparency (behavior)",
      "score": 0.5712993144989014
    },
    {
      "name": "Government (linguistics)",
      "score": 0.5479171872138977
    },
    {
      "name": "Veterans Affairs",
      "score": 0.5459529161453247
    },
    {
      "name": "Accountability",
      "score": 0.5391433835029602
    },
    {
      "name": "Work (physics)",
      "score": 0.46842870116233826
    },
    {
      "name": "Computer science",
      "score": 0.4566646218299866
    },
    {
      "name": "Public domain",
      "score": 0.4309849143028259
    },
    {
      "name": "Public relations",
      "score": 0.4289543926715851
    },
    {
      "name": "Task (project management)",
      "score": 0.4254511594772339
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4210721552371979
    },
    {
      "name": "Process (computing)",
      "score": 0.4164925515651703
    },
    {
      "name": "Regulatory affairs",
      "score": 0.4114013612270355
    },
    {
      "name": "Political science",
      "score": 0.3665306270122528
    },
    {
      "name": "Public administration",
      "score": 0.3390783369541168
    },
    {
      "name": "Computer security",
      "score": 0.24074268341064453
    },
    {
      "name": "Law",
      "score": 0.13045483827590942
    },
    {
      "name": "Linguistics",
      "score": 0.1273266077041626
    },
    {
      "name": "Management",
      "score": 0.11448070406913757
    },
    {
      "name": "Medicine",
      "score": 0.11157363653182983
    },
    {
      "name": "Engineering",
      "score": 0.09963014721870422
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Theology",
      "score": 0.0
    }
  ]
}