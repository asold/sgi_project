{
    "title": "Large language models (LLMs) in radiology exams for medical students: Performance and consequences",
    "url": "https://openalex.org/W4404029770",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4201640040",
            "name": "Jennifer Gotta",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": null,
            "name": "Quang Anh Le Hong",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A2664617296",
            "name": "Vitali Koch",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A3179456688",
            "name": "Leon D Gruenewald",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A1996236407",
            "name": "Tobias Geyer",
            "affiliations": [
                "University of Rostock"
            ]
        },
        {
            "id": "https://openalex.org/A2151541688",
            "name": "Simon S Martin",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A4209584802",
            "name": "Jan-Erik Scholtz",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A1560651013",
            "name": "Christian Booz",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A2329903176",
            "name": "Daniel Pinto Dos Santos",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A2776985844",
            "name": "Scherwin Mahmoudi",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A2990321649",
            "name": "Katrin Eichler",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A4214214147",
            "name": "Tatjana Gruber-Rouh",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A570067774",
            "name": "Renate Hammerstingl",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A3012528106",
            "name": "Teodora Biciusca",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A2553774041",
            "name": "Lisa Joy Juergens",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A3180584827",
            "name": "Elena Höhne",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A2109339025",
            "name": "Christoph Mader",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A2063757862",
            "name": "Thomas J. Vogl",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A4383307328",
            "name": "Philipp Reschke",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4319301505",
        "https://openalex.org/W2803760365",
        "https://openalex.org/W4318069287",
        "https://openalex.org/W4380995257",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4376640725",
        "https://openalex.org/W3013294478",
        "https://openalex.org/W3213380581",
        "https://openalex.org/W4321351832"
    ],
    "abstract": "Abstract The evolving field of medical education is being shaped by technological advancements, including the integration of Large Language Models (LLMs) like ChatGPT. These models could be invaluable resources for medical students, by simplifying complex concepts and enhancing interactive learning by providing personalized support. LLMs have shown impressive performance in professional examinations, even without specific domain training, making them particularly relevant in the medical field. This study aims to assess the performance of LLMs in radiology examinations for medical students, thereby shedding light on their current capabilities and implications. This study was conducted using 151 multiple-choice questions, which were used for radiology exams for medical students. The questions were categorized by type and topic and were then processed using OpenAIʼs GPT-3.5 and GPT- 4 via their API, or manually put into Perplexity AI with GPT-3.5 and Bing. LLM performance was evaluated overall, by question type and by topic. GPT-3.5 achieved a 67.6% overall accuracy on all 151 questions, while GPT-4 outperformed it significantly with an 88.1% overall accuracy (p&lt;0.001). GPT-4 demonstrated superior performance in both lower-order and higher-order questions compared to GPT-3.5, Perplexity AI, and medical students, with GPT-4 particularly excelling in higher-order questions. All GPT models would have successfully passed the radiology exam for medical students at our university. In conclusion, our study highlights the potential of LLMs as accessible knowledge resources for medical students. GPT-4 performed well on lower-order as well as higher-order questions, making ChatGPT-4 a potentially very useful tool for reviewing radiology exam questions. Radiologists should be aware of ChatGPTʼs limitations, including its tendency to confidently provide incorrect responses.",
    "full_text": "Large language models (LLMs) in radiology exams for medical\nstudents: Performance and consequences\nDie Leistungen von große Sprachmodelle (LLMs) in radiologischen\nStudentenprüfungen: Leistung und Auswirkungen\nAuthors\nJennifer Gotta1 ,Q u a n gA n hL eH o n g1, Vitali Koch1, Leon D. Gruenewald1, Tobias Geyer2, Simon S. Martin1,\nJan-Erik Scholtz1, Christian Booz1, Daniel Pinto Dos Santos1, Scherwin Mahmoudi1, Katrin Eichler1, Tatjana Gruber-Rouh1,\nRenate Hammerstingl1, Teodora Biciusca1, Lisa Joy Juergens1, Elena Höhne1, Christoph Mader1,T h o m a sJ .V o g l1 ,\nPhilipp Reschke1\nAffiliations\n1 Department of Diagnostic and Interventional Radiology,\nGoethe University Frankfurt, Frankfurt am Main, Germany\n2 Institute of Diagnostic and Interventional Radiology,\nPediatric Radiology and Neuroradiology, Rostock\nUniversity Medical Center, Rostock, Germany\nKeywords\nAI, medical, education\nreceived 27.6.2024\naccepted after revision2.10.2024\npublished online 4.11.2024\nBibliography\nRofo 2025; 197: 1057–1067\nDOI 10.1055/a-2437-2067\nISSN 1438-9029\n© 2024. Thieme. All rights reserved.\nGeorg Thieme Verlag KG, Oswald-Hesse-Straße 50,\n70469 Stuttgart, Germany\nCorrespondence\nDr. Philipp Reschke\nDepartment of Diagnostic and Interventional Radiology,\nGoethe University Frankfurt, Theodor-Stern-Kai 7,\n60696 Frankfurt am Main, Germany\nPhilipp.reschke@outlook.de\nSupplementary Material is available at https://doi.org/\n10.1055/a-2437-2067.\nABSTRACT\nPurpose The evolving field of medical education is being\nshaped by technological advancements, including the integ-\nration of Large Language Models (LLMs) like ChatGPT. These\nmodels could be invaluable resources for medical students,\nby simplifying complex concepts and enhancing interactive\nlearning by providing personalized support. LLMs have shown\nimpressive performance in professional examinations, even\nwithout specific domain training, making them particularly\nrelevant in the medical field. This study aims to assess the per-\nformance of LLMs in radiology examinations for medical stu-\ndents, thereby shedding light on their current capabilities\nand implications.\nMaterials and Methods This study was conducted using\n151 multiple-choice questions, which were used for radiology\nexams for medical students. The questions were categorized\nby type and topic and were then processed using OpenAIʼs\nGPT-3.5 and GPT- 4 via their API, or manually put into Perplex-\nity AI with GPT-3.5 and Bing. LLM performance was evaluated\noverall, by question type and by topic.\nResults GPT-3.5 achieved a 67.6 % overall accuracy on all\n151 questions, while GPT-4 outperformed it significantly\nwith an 88.1 % overall accuracy (p < 0.001). GPT-4 demon-\nstrated superior performance in both lower-order and high-\ner-order questions compared to GPT-3.5, Perplexity AI, and\nmedical students, with GPT-4 particularly excelling in higher-\norder questions. All GPT models would have successfully\npassed the radiology exam for medical students at our univer-\nsity.\nConclusion In conclusion, our study highlights the potential\nof LLMs as accessible knowledge resources for medical stu-\ndents. GPT-4 performed well on lower-order as well as high-\ner-order questions, making ChatGPT-4 a potentially very use-\nf u lt o o lf o rr e v i e w i n gr a d i o l o g yexam questions. Radiologists\nshould be aware of ChatGPTʼs limitations, including its tend-\nency to confidently provide incorrect responses.\nKey Points\n▪\nChatGPT demonstrated remarkable performance, achieving\na passing grade on a radiology examination for medical\nstudents that did not include image questions.\n▪ GPT-4 exhibits significantly improved performance\ncompared to its predecessors GPT-3.5 and Perplexity AI\nwith 88 % of questions answered correctly.\n▪\nRadiologists as well as medical students should be aware of\nChatGPTʼs limitations, including its tendency to confidently\nprovide incorrect responses.\nAcademic Radiology\n1057Gotta J et al. Large language models… Rofo 2025; 197: 1057–1067 | © 2024. Thieme. All rights reserved.\nThis document was downloaded for personal use only. Unauthorized distribution is strictly prohibited.\nArticle published online: 2024-11-04\nCitation Format\n▪ Gotta J, Le Hong QA, Koch V et al. Large language models\n(LLMs) in radiology exams for medical students: Perform-\nance and consequences. Rofo2025; 197: 1057–1067\nZUSAMMENFASSUNG\nZiel Das sich entwickelnde Feld der medizinischen Ausbildung\nwird durch technologische Fortschritte geprägt, ein-\nschließlich der Integration von Large Language Models\n(LLMs) wie ChatGPT. Diese Modelle könnten für Medizinstu-\ndenten unschätzbare Ressourcen sein, indem sie komplexe\nKonzepte vereinfachen und das interaktive Lernen durch per-\nsönliche Unterstützung verbessern. Diese Studie zielt darauf\nab, die Leistung von LLMs in radiologischen Prüfungen für\nMedizinstudenten zu bewerten und Einblicke in ihre aktuellen\nFähigkeiten und Auswirkungen zu geben.\nMaterialien und MethodenDiese Studie wurde mit 151 Mul-\ntiple-Choice-Fragen durchgeführt, die für radiologische Prü-\nfungen von Medizinstudenten verwendet wurden. Die Fragen\nwurden nach Typ und Thema kategorisiert und dann mithilfe\nvon OpenAIʼs GPT-3.5 und GPT-4 über deren API verarbeitet\noder manuell in Perplexity AI mit GPT-3.5 und Bing eingege-\nben. Die Leistung der LLMs wurde insgesamt nach Fragetyp\nund nach Thema bewertet.\nErgebnisse GPT-3.5 erreichte eine Gesamtgenauigkeit von\n67,6 % bei allen 151 Fragen, während GPT-4 mit einer Ge-\nsamtgenauigkeit von 88,1% signifikant besser abschnitt\n(p <0,001). GPT-4 zeigte sowohl bei einfachen als auch bei\nk o m p l e x e r e nF r a g e ne i n eü b e r legene Leistung im Vergleich\nzu GPT-3.5, Perplexity AI und Medizinstudenten. Besonders\nhervorzuheben ist, dass GPT-4 bei den komplexeren Fragen\ndeutlich besser abschnitt. Alle GPT-Modelle hätten die radi-\nologische Prüfung für Medizinstudenten an unserer Universi-\ntät erfolgreich bestanden.\nSchlussfolgerung Zusammenfassend hebt unsere Studie das\nPotenzial von LLMs als zugängliche Wissensressourcen für\nMedizinstudenten hervor. GPT-4 schnitt gut bei Fragen niedri-\nger und höherer Ordnung ab, was ChatGPT-4 zu einem poten-\nziell sehr nützlichen Werkzeug für die Überprüfung von radi-\nologischen Prüfungsfragen macht. Radiologen sollten sich\nder Grenzen von ChatGPT bewusst sein, einschließlich seiner\nTendenz, selbstbewusst falsche Antworten zu geben.\nKernaussagen\n▪ ChatGPT zeigte eine bemerkenswerte Leistung und alle\nModelle bestanden die Radiologie-Prüfung für Medizin-\nstudenten ohne Bildfragen.\n▪ GPT-4 erzielte mit einer Gesamtgenauigkeit von 88 % die\nhöchste Punktzahl bei den Radiologie-Prüfungsfragen und\nübertraf damit GPT-3.5, Perplexity AI und Medizinstuden-\nten deutlich.\n▪ Radiologen sowie Medizinstudenten sollten sich der Ein-\nschränkungen von ChatGPT bewusst sein, einschließlich\nseiner Tendenz, selbstsicher falsche Antworten zu geben.\nIntroduction\nThe field of medical education is continually evolving with advance-\nments in technology reshaping the way medical students are train-\ned and assessed. One such technological innovation that has gar-\nnered significant attention in recent years is the integration of\nlarge language models (LLMs) [1]. A significant advantage of LLMs\nsuch as ChatGPT is their ability to provide explanations for solu-\ntions, thereby making it easier for students to understand exam\narchitecture. Learning content can be tailored based on the userʼs\nknowledge level, and the chat function allows interactive learning.\nLLMs, such as ChatGPT, are supported by deep neural net-\nworks and have been trained on vast datasets. These models\nhave profound text analysis and generation capabilities, making\nthem exceptionally promising tools for both medical practice and\neducation [2, 3].\nAs an indispensable component of medical practice, radiology\nnecessitates profound comprehension of intricate imaging stud-\nies and clinical implications. Medical students, on their journey to-\nward becoming proficient healthcare professionals, undergo rig-\norous training and examinations to help them gain the requisite\nskills and knowledge. Although the field of artificial intelligence\nin diagnostic radiology has primarily centered on image analysis,\nthere has been growing enthusiasm surrounding the potential ap-\nplications of LLMs, including ChatGPT, within radiology [4, 5, 6].\nThese applications encompass a wide spectrum, including radiol-\nogy education, assistance in differential diagnoses, computer-\naided diagnosis, and disease classification [5, 6, 7]. If these LLMs\ncan demonstrate accuracy and reliability, they have the potential\nto serve as invaluable resources for learners, enabling rapid re-\nsponses to inquiries and simplification of intricate concepts.\nChatGPT has already undergone investigation regarding its poten-\ntial with respect to streamlining radiology reports and facilitating\nclinical decision-making [8, 9]. Furthermore, LLMs have already\nperformed commendably in a diverse array of professional exam-\ninations, even without specialized domain pretraining [10]. In the\nrealm of medicine, they showed convincing results with respect to\nmedical examinations [11, 12, 13].\nThe aim of this study was to explore and evaluate the perform-\nance of LLMs in radiology examinations for medical students in\norder to provide insight into the present capabilities and implica-\ntions of LLMs.\nMethods\nThis exploratory prospective study was carried out from August to\nOctober 2023. We obtained informed consent from the head of\nthe institute to utilize the instituteʼs own radiology examination\nquestions for medical students.\n1058 Gotta J et al. Large language models… Rofo 2025; 197: 1057–1067 | © 2024. Thieme. All rights reserved.\nAcademic Radiology\nThis document was downloaded for personal use only. Unauthorized distribution is strictly prohibited.\n\nMultiple-Choice Question Selection and Classification\n200 multiple-choice questions, each featuring four incorrect an-\nswers and one correct answer, were identified using the database\nof our radiology institute. These questions were originally de-\nsigned for use in the radiology examination for medical students\nat our hospital. The exclusion criteria comprised questions con-\ntaining images (n =40) and questions with multiple correct\nanswers (n =9). After this selection process, 151 questions\nremained. The questions were then either prompted through\nOpenAIʼs API for GPT-3.5 and GPT-4 or manually pasted into the\nuser interface (UI) of Perplexity AI (GPT 3.5 +Bing). To avoid the\ninfluence of previous responses on the modelʼs output, a new\nChatGPT session was initiated for each query. All questions were\nasked in three separate ChatGPT sessions, and the average per-\nformance was calculated.\nA simple prompt for the question was used in the following\nform:\nQuestion:\n{question text}\nA: {answer A}\nB: {answer B}\nC: {answer C}\nD: {answer D}\nE: {answer E}\nFor the initial prompt we used:\nYou are an expert radiologist. Answer the following multiple-\nchoice question in the form:\n<Single letter (answer)>\n<Text explaining the reason>\nThe outputs were restructured and combined for statistical analy-\nsis. A passing score was considered to be 60 % or above. Addition-\nally, the questions were categorized based on their type as either\nlower- or higher- order thinking questions, along with their sub-\nject matter, as detailed in▶ Table 1. Lower-order thinking encom-\npasses tasks related to remembering and basic understanding,\nwhile higher-order thinking invol ves the application, analysis,\nand evaluation of concepts. The higher-order thinking as well as\nthe lower-order thinking questions were further subclassified by\ntype (description of imaging findings, clinical management, com-\nprehension, knowledge). Each question underwent independent\nclassification by two radiologists. A flowchart of the study design\nis displayed in▶ Fig. 1[14].\nLarge language models (LLMs)\nChatGPT (ChatGPT August 3, 2023 version, OpenAI) and Perplex-\nity AI were used in this study. There are two versions of ChatGPT:\nChatGPT, which is based on GPT-3.5, and ChatGPT Plus, which uti-\nlizes the more advanced GPT-4. In this study we used the two un-\nderlying LLMs directly via the OpenAI API. No specialized radiolo-\n▶ Table 1Performance of LLMs and medical students stratified by question type and topic.\nTotal Students GPT-3.5 GPT-3.5 + Bing GPT-4\nN% N % N % N %\nAll questions 151 115.3 76.3 103 68.2 108 71.5 134 88.7\nBone 26 19.4 74.4 17 65.4 18 69.2 23 88.5\nBreast 5 3.7 73.2 2 40 2 40 4 80\nCardiovascular 13 9.9 76.9 11 84.6 10 76.9 12 92.3\nChest 19 13.6 71.7 14 73.7 14 73.7 17 89.5\nGastrointestinal 16 12.8 80 11 68.8 12 75 15 93.8\nGenitourinary 6 4.2 70 4 66.7 5 83.3 5 83.3\nHead and neck 39 31.1 79.8 28 71.8 29 74.4 34 87.2\nPhysics 11 8.5 77.4 7 63.6 7 63.6 10 90.9\nSystemic 16 12 75.1 9 56.3 11 68.8 14 87.5\nClinical management 37 29.1 78.7 26 70.3 27 72.9 33 89.2\nDescription of imaging\nfindings\n27 20.2 74.8 16 59.3 21 77.8 23 85.2\nDiagnosis 23 17.2 74.7 16 69.6 14 60.8 22 95.7\nComprehension 28 21.3 76.3 21 75 23 82.1 25 89.3\nKnowledge 36 27.5 76.3 24 66.7 23 63.9 31 86.1\nHigher-order 87 66.5 76.4 58 66.7 62 71.3 78 89.7\nLower-order 64 48.8 76.3 45 70.3 46 71.9 56 87.5\n1059Gotta J et al. Large language models… Rofo 2025; 197: 1057–1067 | © 2024. Thieme. All rights reserved.\nThis document was downloaded for personal use only. Unauthorized distribution is strictly prohibited.\n\ngy-specific pretraining was conducted for either of these models.\nIt is important to highlight that GPT-3.5 and GPT-4, being server-\ncontained LLMs, lack the capability to access the internet or exter-\nnal databases for information retrieval. In contrast, Perplexity AI\n(ChatGPT 3.5 +Bing) has the capacity to search the internet.\nMedical students\nThe study included a cohort of 621 medical students who were in\ntheir first clinical semester, typically corresponding to their third\nyear in medical school.\nPrior to entering the clinical phase, the students completed\ntwo years of preclinical education, which included foundational\ncourses in anatomy, physiology, biochemistry, pathology, and\nbasic medical sciences. At the time of the study, the students\nhad completed an introductory course in radiology. However,\ntheir exposure to advanced radiological topics was limited com-\npared to more senior students and residents.\nStatistical analysis\nStatistical analysis was performed using Python (version 3.11).\nThe McNemar test was used to determine the statistical signifi-\ncance of difference regarding the performance of the LLMs. This\nwas also done for subgroups by question type and topic. For over-\nall model performance, we utilized the widely used accuracy\nscore.\nTo quantify the comparative performance of the LLMs and the\nmedical students, we performed an odds ratio analysis. For each\ncomparison, we set up 2 ×2 contingency tables that summarize\nthe number of correct and incorrect answers for the two groups\nbeing compared. Thereafter, we calculated p-values using Fisherʼs\nExact Test. A P-value of less than 0.05 was considered statistically\nsignificant. No correction-for-guessing was performed, since the\npassing score of our exam already accounts for guessing.\nResults\nOverall performance\nThe overall accuracy of GPT-3.5 for all 151 questions was 67.6 %.\nIn contrast, GPT-4 achieved significantly higher accuracy compar-\ned to GPT-3.5 with an overall accuracy of 88.1 % (p< 0.001). No\nsignificant differences were observed between GPT-3.5+Bing\nand GPT-3.5 (p = 0.44). In comparison, the overall accuracy of the\nmedical students was 76%. All LLMs would have passed the radiol-\nogy exam for medical students at our university.▶ Table 1shows\nthe overall performance of the LLMs as well the performance stra-\n▶ Fig. 1Flowchart of the study design. From our initial 200 exam questions, 151 remained after excluding questions with images and questions\nwith more than one correct answer. The questions were then prompted either by OpenAIs API for GPT-3.5 and GPT-4 or manually pasted into the UI\nof Perplexity AI (GPT 3.5 +Bing). The outputs were restructured and combined for statistical analysis. Abbreviations: MC: multiple choice;\nAPI: application programming interface; UI: user interface.\n1060 Gotta J et al. Large language models… Rofo 2025; 197: 1057–1067 | © 2024. Thieme. All rights reserved.\nAcademic Radiology\nThis document was downloaded for personal use only. Unauthorized distribution is strictly prohibited.\n\ntified by question type and topic and▶ Fig. 2shows a question\nthat was answered correctly by all LLMs.\nPerformance by topic\nAmong the subgroups, GPT4 exhibited the highest performance\nin the gastrointestinal category, correctly answering 15 out of\n16 questions, thus achieving an accuracy of 93.75%. Compared\nwith GPT3.5 and Perplexity AI, GPT-4 demonstrated significantly\nsuperior performance with regard to answering questions related\nto bone diseases (p = 0.03). However, subgroup analysis revealed\nno noteworthy variations in performance across the remaining\nsubspecialty groups.\nQuestions answered incorrectly by all models\nA total of seven questions were answered incorrectly by all models\n(Table S1). Among these, two questions pertained to the use of\ncontrast agents in patients with renal insufficiency, while another\nrelated to MRI angiography in patients with a pacemaker.\nThe remaining questions that stumped all models demanded a\nnuanced understanding of specific details or specialized knowl-\n▶ Fig. 2GPT-3.5 /4.0 and Perplexity AI response to one of the questions. All picked the correct answer (option B).A:G P T - 3 . 5B: Perplexity AI;C:G P T - 4 .\n1061Gotta J et al. Large language models… Rofo 2025; 197: 1057–1067 | © 2024. Thieme. All rights reserved.\nThis document was downloaded for personal use only. Unauthorized distribution is strictly prohibited.\n\nedge. For instance, one question pertained to renal scintigraphy,\nwhere the correct response hinged on the knowledge that Tc\n99 m-MAG3 is primarily secreted by proximal renal tubules and,\ntherefore, cannot be used to estimate glomerular filtration rate.\n▶ Fig. 3illustrates a question that was answered incorrectly by all\nLLMs.\nPerformance by question type\nGPT-4 demonstrated significantly superior performance in both\nlower-order and higher-order questions when compared to GPT-3.5\nand Perplexity AI (p = 0.01 and p < 0.001, respectively).\nGPT-4 achieved the best performance across all topics and\ncategories compared to medical students, GPT-3.5, and Perplex-\nity AI (▶ Fig. 4).\nWithin the subgroups, GPT-4 exhibited its highest perform-\nance when responding to higher-order questions related to diag-\n▶ Fig. 3Response to a question answered incorrectly: Please be mindful that large language models (LLMs) frequently use assertive language in\ntheir responses, even when those responses are incorrect. Abbreviations: LLM: large language models.\n1062 Gotta J et al. Large language models… Rofo 2025; 197: 1057–1067 | © 2024. Thieme. All rights reserved.\nAcademic Radiology\nThis document was downloaded for personal use only. Unauthorized distribution is strictly prohibited.\n\nnosis. It provided correct answers for 22 out of 23 questions in this\ncategory, achieving an accuracy of 95.65%.\nIn contrast, GPT-3.5 and Perplexity AI exhibited their highest\nperformance with respect to the lower-order subgroup compre-\nhension with accuracies of 75.00% and 82.41% (▶ Table1 ). Per-\nplexity AI demonstrated the weakest performance in the higher-\norder category diagnosis (60.9%) and in the lower-order category\nknowledge (63.9%), while GPT-3.5 had the weakest performance\nin the higher-order description of imaging findings (59.3 %) and\nthe lower-order category comprehension (75 %). The average med-\nical student achieved a similar performance for lower-order ques-\ntions (76.27%) compared to higher-order questions (76.39%). The\nperformance of the average student was relatively stable across all\nsubgroups. The average student achieved the highest performance\nwith regard to questions related to clinical management with an\naccuracy of 78.7 % and the lowest performance with regard to diag-\nnosis with an accuracy of 74.7% (▶ Table1, ▶ Fig.5, ▶ Fig. 6).\nOdds ratio analysis\nThe odds ratio analysis confirmed that the overall performance of\nGPT-4 was significantly superior to that of GPT-3, Perplexity AI,\nand the medical students. The improved performance was parti-\ncularly notable for higher-order questions, where GPT-4 showed\nthe greatest improvement over the other GPT models and the stu-\ndents. For example, GPT-4 is 4.3 times more likely to correctly\nanswer higher-order thinking questions than GPT-3.5 (p < 0.001).\nFor lower-order thinking questions, while GPT-4 still performed\nbetter, the difference was not statistically significant compared\nto the medical students (▶ Table 2).\nDiscussion\nThe integration of LLMs into various domains has increased re-\nmarkably in recent years, with applications ranging from natural\nlanguage processing to medical diagnostics. In the field of medi-\ncal education, LLMs have shown immense potential to assist and\nenhance the learning experience for students, particularly in radi-\nology – a discipline that demands profound understanding of\ncomplex medical concepts and terminology.\nThe present study provides several important key findings to\nunderstand how advancements in LLM technology can impact\nmedical education. First, in this exploratory prospective study, all\nLLMs would have passed the exam. Second, GPT-4 exhibited sig-\nnificantly better performance than its predecessors GPT-3.5, Per-\nplexity AI, and the medical students with 88 % of the questions an-\nswered correctly. Third, GPT-4 maintained the best performance\nacross all topics and categories compared to the medical stu-\ndents, GPT-3.5, and Perplexity AI. Fourth, the performance\nimprovement was particularly pronounced for higher-order ques-\ntions, where GPT-4 demonstrated the most significant improve-\nment over the other GPT models and the students. Fifth, GPT-4\ndemonstrated the highest perfor mance in the gastrointestinal\n▶ Fig. 4Performance comparison across medical topics: medical students vs. GPT models.\n1063Gotta J et al. Large language models… Rofo 2025; 197: 1057–1067 | © 2024. Thieme. All rights reserved.\nThis document was downloaded for personal use only. Unauthorized distribution is strictly prohibited.\n\ncategory with an accuracy of 93.75%. The prevalence of gastroin-\ntestinal content in training datasets may have contributed to the\nmodelʼs enhanced performance in this domain.\nDespite the ability of Perplexity AI to search the internet, it\ndemonstrated the weakest performance with regard to knowl-\nedge. Internet searches can yield information from a wide range\nof sources, including those that are not peer-reviewed or scientifi-\ncally accurate. Without a sophisticated mechanism to filter and\nprioritize high-quality, reliable sources, the model might incorpo-\nrate inaccurate or outdated information. GPT-4ʼs superior per-\nformance may be attributed to the fact that GPT-4 benefits from\nadvanced model enhancements, including a deeper architecture\nand extensive training.\nChatGPT has demonstrated good performance in a wide range\nof professional examinations, including those in the medical field,\neven without the need for specialized domain pretraining [10, 11,\n12, 13]. For instance, it was applied to the USMLE, where ChatGPT\nachieved accuracy rates exceeding 50% across all examinations\nand surpassing 60 % in certain analyses [11].\n▶ Fig. 5Performance comparison in higher- and lower-order tasks: medical students vs. GPT models.\n1064 Gotta J et al. Large language models… Rofo 2025; 197: 1057–1067 | © 2024. Thieme. All rights reserved.\nAcademic Radiology\nThis document was downloaded for personal use only. Unauthorized distribution is strictly prohibited.\n\nDespite the absence of radiology-specific training, ChatGPT\nperformed commendably. When new LLMs with radiology-specif-\nic pretraining and the ability to process images become publicly\navailable, it will be interesting to see what results can be achieved.\nAs LLM technology continues to advance, radiologists will need\nto gain comprehensive understanding of the performance and re-\nliability of these models and of their evolving role in radiology. The\ndevelopment of applications built on LLMs holds promise for fur-\nther enhancing radiological pra ctice and education, ultimately\nbenefiting both current and future healthcare professionals. How-\never, ChatGPT is designed to discern patterns and associations\namong words within its training data. Consequently, we antici-\npate limitations in cases requiring understanding of the context\nof specialized technical language or specific details and specia-\nlized knowledge, such as radiological terminology used in imaging\ndescriptions, calculations, and classification systems.\nFurthermore, ChatGPT consistently employs confident lan-\nguage in its responses, even when those responses are incorrect.\nThis tendency is a well-documented limitation of LLMs [15]. Even\nwhen the most probable available option may be incorrect,\nChatGPT tends to generate responses that sound convincingly\nhuman-like. Interestingly, increased human likeness in chatbots\nis associated with a higher lev el of trust [16]. Consequently,\nChatGPTʼs inclination to produce plausible yet erroneous respon-\nses presents a significant concern when it serves as the sole source\nof information [17]. This concern is particularly critical with regard\nto individuals who may lack the expertise to discern inaccuracies\nin its assertions, notably novices. As a result, this behavior cur-\nrently restricts the practicality of employing ChatGPT in medical\neducation.\nTo prevent a future where LLMs influence the outcome of med-\nical and radiological exams, several measures can be taken. These\ninclude designing exam questions that necessitate critical think-\ning and the application of knowledge rather than mere recall,\nintegrating practical components or simulations that cannot be\neasily answered by LLMs, ensuring robust exam proctoring and\nmonitoring procedures to detect any suspicious behavior, and\ncontinually updating exam formats and content to stay ahead of\n▶ Fig. 6Performance heatmap across medical topics and cognitive functions.\n1065Gotta J et al. Large language models… Rofo 2025; 197: 1057–1067 | © 2024. Thieme. All rights reserved.\nThis document was downloaded for personal use only. Unauthorized distribution is strictly prohibited.\n\npotential cheating methods involving LLMs. Additionally, empha-\nsizing the importance of genuine learning and skill acquisition can\nhelp maintain the integrity of medical exams amidst technologi-\ncal advancements.\nFurthermore, we identified inconsistencies in ChatGPT ʼs\nresponses. In a subsequent evaluation, GPT-3.5 yielded different\nanswers for five questions, while GPT-4 provided six different\nanswers, but there were no significant differences in accuracy\nbetween the two models. These inconsistencies can be partially\nmitigated by adjusting parameters such as temperature, top-k,\nand top-p settings. Temperature controls the randomness of the\nmodelʼs responses; a lower temperature makes the output more\nfocused and deterministic, while a higher temperature increases\nvariability. Top-k limits the model to considering only the top k\nmost likely next words, thus reducing the chance of less probable\nwords being selected. Top-p adjusts the probability mass, allow-\ning the model to consider the smallest possible set of words\nwhose cumulative probability exceeds a certain threshold p,\nthereby balancing diversity and coherence.\nHowever, this adjustment cannot be made directly through the\nweb interface but can be done, for instance, in the OpenAI play-\nground. Without a nuanced understanding of the influence of\nthese parameters, thereʼs a risk of overestimating or underesti-\nmating LLM capabilities, potentially leading to misleading conclu-\nsions about their effectiveness in educational settings. Moreover,\nthe variability introduced by different parameter settings may re-\nsult in significant fluctuations in LLM performance, thus challen-\nging the generalizability of findings to real-world applications. Fu-\nture research should prioritize comprehensive analyses of the\nimpact of LLM settings on responses to radiology exam questions\nto ensure accurate assessments and to optimize LLM configura-\ntions for educational use in specialized fields.\nFurthermore, it is essential to acknowledge certain limitations.\nFirst, we excluded questions containing images, which are typical-\nly integral to a radiology examination, due to ChatGPTʼsi n a b i l i t y\nto process visual content at the time of this study. To thoroughly\nassess the performance of the LLMs presented in a real-world sce-\nnario, including all question types, further studies are necessary.\nSecond the pass/fail threshold we applied is an approximation,\nas normally a passing score of 60 % or above is standard for all\nwritten components, including those featuring image-based\nquestions. Furthermore, the relatively small number of questions\nin each subgroup within this exploratory study has limited the sta-\ntistical power available for subgroup analyses.\nIn conclusion, our study underscores the potential of LLMs like\nChatGPT as a new and readily accessible knowledge source for\nmedical students. Even without radiology-specific pretraining,\nChatGPT demonstrated remarkab le performance, achieving a\npassing grade on a radiology examination for medical students\nthat did not include images. The model excelled with respect to\nhigher-order as well as lower-order thinking questions. It is crucial\nfor radiologists to be aware of ChatGPTʼs limitations, including its\ntendency to confidently generate inaccurate responses. Presently,\nit cannot be solely relied upon for clinical practice or educational\npurposes. However, ChatGPT presents an exciting opportunity as\na new and readily accessible knowledge source for medical stu-\ndents, offering them a valuable tool to supplement their learning\nand understanding of radiology concepts.\nDeclarations\nWe disclose that the manuscript was proofread by ChatGPT. All\nsections proofread by ChatGPT were meticulously reviewed. Addi-\ntionally, we adhered to data protection regulations, ensuring that\nonly anonymized data was uploaded.\nStatistical analysis was performed using Python (version 3.11).\nChatGPT was utilized to understand and debug the Python code\nand adjust the graphics (▶ Fig.4 , ▶ Fig.5 , ▶ Fig. 6). Specifically,\nthe diagrams were created using the Python code.\nInformed Consent:Not applicable.\nData availability statement:All data and information used are\nincluded in this manuscript.\nConflict of Interest\nC.B. received speaking fees from Siemens Healthineers. The other\nauthors have no potential conflict of interest to disclose.\nReferences\n[1] Introducing ChatGPT [Internet]. [zitiert 6. September 2023]. Verfügbar\nunter: https://openai.com/blog/chatgpt\n[2] Sallam M. The Utility of ChatGPT as an Example of Large Language Models\nin Healthcare Education, Research and Practice: Systematic Review on the\nFuture Perspectives and Potential Limitations [Internet]. medRxiv; 2023\n[zitiert 6. September 2023]. S. 2023.02.19.23286155. Verfügbar unter:\nhttps://www.medrxiv.org/content/10.1101/2023.02.19.23286155v1\n[3] Patel SB, Lam K. ChatGPT: the future of discharge summaries? Lancet\nDigit Health 2023; 5 (3): e107–e108\n[4]\nHosny A, Parmar C, Quackenbush J et al. Artificial intelligence in radiology.\nNat Rev Cancer 2018; 18 (8): 500–510\n[5] Shen Y, Heacock L, Elias J et al. ChatGPT and Other Large Language Models\nAre Double-edged Swords. Radiology 2023; 307 (2): e230163\n▶ Table 2Odds ratio analysis of the performance of LLMs and medical\nstudents.\nComparison Odds\nratio\np-value\nAll questions GPT-4 vs. GPT-3.5 3.4 0.00002\nGPT-4 vs. Medical Students 2.2 0.006\nGPT-4 vs. Perplexity AI 2.9 0.0003\nHigher-order GPT-4 vs. GPT-3.5 4.3 0.0004\nGPT-4 vs. medical students 2.7 0.03\nGPT-4 vs. Perplexity AI 3.5 0.004\nLower-order GPT-4 vs. GPT-3.5 2.9 0.03\nGPT-4 vs. Perplexity AI 2.7 0.047\nGPT-4 vs. medical students 2.2 0.11\n1066 Gotta J et al. Large language models… Rofo 2025; 197: 1057–1067 | © 2024. Thieme. All rights reserved.\nAcademic Radiology\nThis document was downloaded for personal use only. Unauthorized distribution is strictly prohibited.\n\n[6] Wang S, Zhao Z, Ouyang X et al. ChatCAD: Interactive Computer-Aided\nDiagnosis on Medical Image using Large Language Models [Internet].\narXiv; 2023 [zitiert 6. September 2023]. Verfügbar unter: http://arxiv.\norg/abs/2302.07257\n[7] Liu J, Wang C, Liu S. Utility of ChatGPT in Clinical Practice. J Med Internet\nRes 2023; 25: e48568\n[8] Rao A, Kim J, Kamineni M et al. Evaluating ChatGPT as an Adjunct for\nRadiologic Decision-Making [Internet]. medRxiv; 2023 [zitiert 6. Sep-\ntember 2023]. S. 2023.02.02.23285399. Verfügbar unter: https://www.\nmedrxiv.org/content/10.1101/2023.02.02.23285399v1\n[9] Jeblick K, Schachtner B, Dexl J et al. ChatGPT Makes Medicine Easy to\nSwallow: An Exploratory Case Study on Simplified Radiology Reports\n[Internet]. arXiv; 2022 [zitiert 6. September 2023]. Verfügbar unter:\nhttp://arxiv.org/abs/2212.14882\n[10] Choi JH, Hickman KE, Monahan A et al. ChatGPT Goes to Law School\n[Internet]. Rochester, NY; 2023 [zitiert 6. September 2023]. Verfügbar\nunter: https://papers.ssrn.com/abstract= 4335905\n[11] Performance of ChatGPT on USMLE: Potential for AI-assisted medical\neducation using large language models | PLOS Digital Health [Internet].\n[zitiert 6. September 2023]. Verfügbar unter: https://journals.plos.org/\ndigitalhealth/article?id = 10.1371/journal.pdig.0000198\n[12] Gilson A, Safranek CW, Huang T et al. How Does ChatGPT Perform on\nthe United States Medical Licensing Examination? The Implications of\nLarge Language Models for Medical Education and Knowledge Assess-\nment. JMIR Med Educ 2023; 9: e45312\n[13] Bhayana R, Krishna S, Bleakney RR. Performance of ChatGPT on a Radi-\nology Board-style Examination: Insights into Current Strengths and\nLimitations. Radiology 2023; 307 (5): e230582\n[14]\nMongan J, Moy L, Kahn CE. Checklist for Artificial Intelligence in Medical\nImaging (CLAIM): A Guide for Authors and Reviewers. Radiology: Artificial\nIntelligence 2020. doi:10.1148/ryai.2020200029\n[15] Xiao Y, Wang WY. On Hallucination and Predictive Uncertainty in Con-\nditional Language Generation. In: Proceedings of the 16th Conference of\nthe European Chapter of the Association for Computational Linguistics:\nMain Volume [Internet]. Online: Association for Computational Linguis-\ntics; 2021 [zitiert 15. September 2023]. S. 2734–44. Verfügbar unter:\nhttps://aclanthology.org/2021.eacl-main.236\n[16]\nLu L, McDonald C, Kelleher T et al. Measuring consumer-perceived human-\nness of online organizational agents. Computers in Human Behavior 2022;\n128: 107092\n[17] Alkaissi H, McFarlane SI. Artificial Hallucinations in ChatGPT: Implications\nin Scientific Writing. Cureus 2023; 15 (2): e35179\n1067Gotta J et al. Large language models… Rofo 2025; 197: 1057–1067 | © 2024. Thieme. All rights reserved.\nThis document was downloaded for personal use only. Unauthorized distribution is strictly prohibited.\n"
}